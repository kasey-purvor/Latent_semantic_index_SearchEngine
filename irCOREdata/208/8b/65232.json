{"doi":"10.1080\/19415530903522519","coreId":"65232","oai":"oai:dro.dur.ac.uk.OAI2:6424","identifiers":["oai:dro.dur.ac.uk.OAI2:6424","10.1080\/19415530903522519"],"title":"Unobserved but not unimportant : the effects of unmeasured variables on causal attributions.","authors":["Coe,  R."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2009-09-01","abstract":"The objective of the present study was to estimate how much difference the inclusion of plausibly important but unmeasured variables could make to estimates of the effects of educational programmes. Two examples of policy-relevant research in education were identified. A sensitivity analysis using Monte Carlo simulation was conducted to estimate the size of a possible spurious 'effect' that could actually be entirely due to the failure to incorporate a plausible unobserved variable. In both examples the effect size reported in the original study was within the range of possible spurious effects. What appeared to the original researchers to be substantial and unequivocal causal effects were reduced to tiny and uncertain differences when the effects of plausible unobserved differences were taken into account. Evaluators who rely on statistical control should be more cautious in making causal claims, consider possible effects of unmeasured variables and conduct sensitivity analyses. Alternatively, stronger designs should be used","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/65232.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/6424\/1\/6424.pdf","pdfHashValue":"08f759e4a5d1fc227aa56b826bee5d4bad1e9af0","publisher":"Routledge","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:6424<\/identifier><datestamp>\n      2016-07-06T12:51:17Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Unobserved but not unimportant : the effects of unmeasured variables on causal attributions.<\/dc:title><dc:creator>\n        Coe,  R.<\/dc:creator><dc:description>\n        The objective of the present study was to estimate how much difference the inclusion of plausibly important but unmeasured variables could make to estimates of the effects of educational programmes. Two examples of policy-relevant research in education were identified. A sensitivity analysis using Monte Carlo simulation was conducted to estimate the size of a possible spurious 'effect' that could actually be entirely due to the failure to incorporate a plausible unobserved variable. In both examples the effect size reported in the original study was within the range of possible spurious effects. What appeared to the original researchers to be substantial and unequivocal causal effects were reduced to tiny and uncertain differences when the effects of plausible unobserved differences were taken into account. Evaluators who rely on statistical control should be more cautious in making causal claims, consider possible effects of unmeasured variables and conduct sensitivity analyses. Alternatively, stronger designs should be used. <\/dc:description><dc:subject>\n        Selection bias<\/dc:subject><dc:subject>\n         Unobserved variables<\/dc:subject><dc:subject>\n         Causal inference<\/dc:subject><dc:subject>\n         Sensitivity analysis<\/dc:subject><dc:subject>\n         Monte Carlo simulation<\/dc:subject><dc:subject>\n         Education. policy.<\/dc:subject><dc:publisher>\n        Routledge<\/dc:publisher><dc:source>\n        Effective education, 2009, Vol.1(2), pp.101-122 [Peer Reviewed Journal]<\/dc:source><dc:date>\n        2009-09-01<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:6424<\/dc:identifier><dc:identifier>\n        issn:1941-5532<\/dc:identifier><dc:identifier>\n        issn: 1941-5540<\/dc:identifier><dc:identifier>\n        doi:10.1080\/19415530903522519 <\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/6424\/<\/dc:identifier><dc:identifier>\n        http:\/\/dx.doi.org\/10.1080\/19415530903522519 <\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/6424\/1\/6424.pdf<\/dc:identifier><dc:rights>\n        This is an electronic version of an article published in Coe, R. (2009) 'Unobserved but not unimportant : the effects of unmeasured variables on causal attributions.', Effective education., 1 (2). pp. 101-122. Effective education is available online at: http:\/\/www.informaworld.com\/smpp\/content~db=all?content=10.1080\/19415530903522519<\/dc:rights><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["1941-5532","issn:1941-5532","issn: 1941-5540"," 1941-5540"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2009,"topics":["Selection bias","Unobserved variables","Causal inference","Sensitivity analysis","Monte Carlo simulation","Education. policy."],"subject":["Article","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n01 April 2011\nVersion of attached file:\nAccepted Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nCoe, R. (2009) \u2019Unobserved but not unimportant : the effects of unmeasured variables on causal\nattributions.\u2019, Effective education., 1 (2). pp. 101-122.\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1080\/19415530903522519\nPublisher\u2019s copyright statement:\nThis is an electronic version of an article published in Coe, R. (2009) \u2019Unobserved but not unimportant : the effects of\nunmeasured variables on causal attributions.\u2019, Effective education., 1 (2). pp. 101-122. Effective education is available\nonline at: http:\/\/www.informaworld.com\/smpp\/content db=all?content=10.1080\/19415530903522519\nAdditional information:\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\n  \n \nDurham Research Online \n \nDeposited in DRO: \n01 April 2011 \n \nPeer-review status: \nPeer-reviewed \n \nPublication status: \nAccepted for publication version \n \nCitation for published item: \nCoe, R. (2009) 'Unobserved but not unimportant : the effects of unmeasured variables on \ncausal attributions.', Effective education., 1 (2). pp. 101-122. \n \nFurther information on publisher\u2019s website: \nhttp:\/\/dx.doi.org\/10.1080\/19415530903522519 \n \nPublisher\u2019s copyright statement: \nThis is an electronic version of an article published in Coe, R. (2009) 'Unobserved but not \nunimportant : the effects of unmeasured variables on causal attributions.', Effective \neducation., 1 (2). pp. 101-122. Effective education is available online at: \nhttp:\/\/www.informaworld.com\/smpp\/content~db=all?content=10.1080\/19415530903522519. \n \n \n \n \n \n \n \n \n \nUse policy \n \nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior \npermission or charge, for personal research or study, educational, or not-for-profit purposes provided that : \n \n\uf0a7 a full bibliographic reference is made to the original source \n\uf0a7 a link is made to the metadata record in DRO \n\uf0a7 the full-text is not changed in any way \n \nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders. \n \nPlease consult the full DRO policy for further details. \n \nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom \nTel : +44 (0)191 334 2975 | Fax : +44 (0)191 334 2971 \nhttp:\/\/dro.dur.ac.uk \n1 \nUnobserved but not unimportant: The effects of unmeasured \nvariables on causal attributions \n \n \nRobert Coe \n \nSchool of Education and CEM, Durham University, UK \n \nSchool of Education, Durham University, Leazes Road, Durham  DH1 1TA, UK. \nEmail: r.j.coe@dur.ac.uk  \n \n \nObjective: To estimate how much difference the inclusion of plausibly important but \nunmeasured variables could make to estimates of the effects of educational programmes. \nMethods: Two examples of policy-relevant research in education were identified. A \nsensitivity analysis using Monte Carlo simulation was conducted to estimate the size of a \npossible spurious \u2017effect\u2018 that could actually be entirely due to the failure to incorporate a \nplausible unobserved variable. \nResults: In all the examples the effect size reported in the original study was \nwithin the range of possible spurious effects.  \nConclusions: What appeared to the original researchers to be substantial and \nunequivocal causal effects were reduced to tiny and uncertain differences when the \neffects of plausible unobserved differences were taken into account. Evaluators who rely \non statistical control should be more cautious in making causal claims, consider possible \neffects of unmeasured variables and conduct sensitivity analyses. Alternatively, stronger \ndesigns should be used. \n \nKeywords: selection bias, unobserved variables, causal inference, sensitivity analysis, Monte \nCarlo simulation, education policy \n \n \n \nIntroduction \nMany of those who believe that the most secure basis for causal inference in the social \nsciences is the evidence from randomised controlled trials would nevertheless \nconcede that causal attributions can sometimes be made on the basis of other methods, \nand in some cases have to be. The main advantage of random allocation is that all \ndifferences between treatment groups, whether observed or not, are controlled. It does \nnot matter whether we have anticipated every possible relevant factor and either \nexplicitly matched the groups on it or measured it adequately and controlled for it \nstatistically; random allocation ensures that the effects of such factors will be equal in \nall groups (or at least differ only by chance) and will therefore cancel out, within \nknown statistical limits.  \nBy contrast, many analyses of non-randomised designs depend on the \nassumption that any differences are either fully known or irrelevant. If they are fully \nknown, their effects can be modelled and any residual effects thus attributed to the \nmanipulated variable. If they are irrelevant, by definition, we do not need to worry \nabout them. In practice, the assumption underlying many causal claims from non-\nrandomised designs in education seems to be a combination of the two: differences \nmay not be fully known, but they are known well enough that once we have taken \n2 \naccount of what we do know, any remaining unknown, or inadequately captured, \ndifferences can be considered irrelevant.  \nAn exception to this need to either know or ignore unmeasured variables is \noffered by a class of analytical methods that use instrumental variables. These include \nmethods such as two-stage least squares, multiprocess modelling, structural equation \nmodelling and simultaneous equation modelling (Greene, 2003) which have been \nwidely used in econometrics, and sometimes \u2013 though less widely \u2013 applied to \nestimating causal effects in education (eg Steele et al, 2007). These approaches \ndepend on the identification of an \u2017instrument\u2018, I, a variable that is correlated with the \ntreatment, X, but which has no independent effect on the outcome, Y, other than \nthrough its effect on the treatment. For this condition to be satisfied, I must be \nuncorrelated with any unobserved factors that influence Y (other than through X). If \nsuch an instrument can be found, the causal effect of X on Y can be estimated as \nessentially the ratio of the effect of I on Y to the effect of I on X (Winship and \nMorgan, 1999; Gennetian et al, 2002). However, the assumption that I acts only \nthrough X is generally untestable, sometimes implausible, and even small violations \nof it can make a big difference to estimates of causal effects, especially if the \nrelationship between I and X is not strong (Winship and Morgan, 1999, p683; \nHeckman, 1997; Small and Rosenbaum, 2008; Schneider et al, 2005, p48).  \nExplicit discussion of the conditions under which causal claims can be \njustified on the basis of correlational evidence (e.g. Klungel et al., 2004) is perhaps \nmore apparent in health research than in education, though good discussions of a \nrange of approaches relevant to educational research do exist (e.g. Schneider et al, \n2007). A review of methods to control for observed and unobserved confounding in \nnon-randomised studies (Groenwold et al., 2009) concludes that unobserved variables \ncannot be controlled for statistically and implores in its title that \u2017Quantitative \nassessment of unobserved confounding is mandatory in nonrandomized intervention \nstudies\u2018.  \nOther strategies identified by Groenwold et al., and a similar review by \nKlungel et al. (2004), include the use of matching either in data collection or analysis \n(eg propensity score analysis) and multivariate analysis. However, all these methods \nassume that any initial differences between comparison groups are fully known; the \npossible existence of unmeasured (or inadequately measured) differences poses more \nof a threat. In discussing the use of instrumental variables approaches, a technique \nthat, as discussed above, potentially deals with the problem of unobserved factors, \nboth reviews conclude that in practice adequate instrumental variables are very \ndifficult to find in health evaluations. Hence their recommendation, as expressed in \nGroenwold et al.\u2018s (2009) title, is that researchers should conduct sensitivity analyses \nto quantify the extent to which unmeasured variables could produce effects similar to \nthose observed, under a range of plausible assumptions. As Rosenbaum (2004) \nexplains, \nA sensitivity analysis replaces the statement\u2014\u2017association does not imply causation\u2018\u2014 \nby a specific statement about the magnitude of hidden bias that would need to be present \nto explain the associations actually observed. (p10812) \nThe stimulus for the current investigation was research in the field of \neducation that makes causal, policy-relevant claims on the basis of non-randomised \ndesigns. In educational research these designs are widely used in impact evaluation \nstudies and their interpretation seems often to be treated as unproblematic. Indeed, \nthere is considerable opposition in principle from some quarters to the use of \nrandomised trials (e.g. Morrison, 2009, Goldstein, 1987). An interesting comparison \ncan be made with the field of health research, where the randomised controlled trial is \n3 \nwidely accepted as providing the \u2017gold standard\u2018 of evidence of causal effects (eg \nKlungel et al., 2004; Rubin, 2008), although non-randomised designs are also widely \nused in epidemiological studies.  \nNumerous examples of the use of sensitivity analyses to estimate the effects of \nunmeasured variables can be found in health research, but in educational research \nthey are much harder to discover. An early paper by Rosenbaum (1986) applied the \ntechnique to a comparison of the achievement of matched pairs of high school drop-\nouts and controls. More recently, Leow et al. (2004) used this approach to help \ninterpret a comparison on basic skills performance of those who had and had not \ntaken advanced courses.  \nAlthough formulae are available to estimate the size of spurious effects due to \nunmeasured variables, all seem to have limitations for this context. Some (eg Lin et \nal., 1998; Arah et al., 2008) are restricted to the case where the outcome variable is \ndichotomous, as is common in health research. Others (eg Rosenbaum, 1991) require \nthe combined unmeasured variables to be effectively dichotomous, or depend on the \nanalysis of matched pairs (Rosenbaum, 1986). Still others (Pan and Frank, 2003) test \ndichotomous decisions of whether causal effects are significant or not and require \ncomplex calculations to implement. \nInstead, the current study uses Monte Carlo simulation to estimate the possible \neffects of plausible unmeasured variables on estimates of the causal impact of an \neducational programme derived from a non-experimental analysis. Two examples of \npolicy evaluation in education have been chosen to illustrate the method. These \nstudies were not the result of considering a large number of studies and selecting a \nsmall number to make a particular point: on the contrary, the first examples tested \nproved to illustrate the point nicely. A third example, an evaluation of the effects of \nthe Assisted Places Scheme (means-tested payment of fees at private schools; Powers \net al., 2006) was also used, but has been omitted for reasons of space (see Coe, 2009 \nfor an extended report). All the examples were chosen initially because they have \nbeen used to make relatively unequivocal, policy-relevant, causal claims that appeared \nto me on reading them to be rather more confident than seemed justified. The fact that \nI was unable to translate my feeling of unease about their confident assertions into a \nclear critique was the motivation for developing and applying the simulation. \n \nDesign of the simulation \nModel simulated \nThe model simulates a situation where the effect of a binary group membership or \n\u2017treatment\u2018 variable, X, on an outcome measure, Y, is estimated with a set of measured \ncovariates, M, using OLS linear regression: \n \n110 eXMY  \nEquation 1 \nwhere \u03b10 and \u03b11 are regression coefficients and e is the residual error. For simplicity, \nM is taken as a single variable, though this could be thought of as a linear sum of a set \nof covariates. In this case instead of the correlation between M and Y we could talk \nabout the multiple correlation.  \nHowever, the true value of Y is given by: \n4 \n \n210 eUMY  \nEquation 2 \nwhere U is an unmeasured covariate that is associated with both group membership \nand the outcome.  \nIn other words, the outcome is actually determined (within random error, e2) \nby a combination of measured and unmeasured variables. Hence the coefficient of \ngroup membership (\u03b11 in Equation 1) represents a spurious group effect, an artefact of \nthe association between X and U and the failure to include U in the model. The \nresidual, e, is assumed to be N(0,\u03c32). \n \nParameters for the simulation \nWe must allow for all possible inter-correlations among the variables, Y, M and U. In \nother words, \n \npuy Correlation between Outcome, Y, and Unmeasured covariate(s), U. \nqum Correlation between Unmeasured covariate(s), U, and Measured \ncovariate(s), M. \nrym Correlation between Outcome, Y, and Measured covariate(s), M. \n \nWe must also allow the strength of relationship between U and X to vary: \n \nsux Correlation (point-biserial) between Unmeasured covariate(s), U, and \n(binary) Group membership, X. \n \nAnd finally, we want to know what values of the apparent but spurious group effect \nare possible for each combination of these: \n \nE Phantom effect of Group membership, X, on Outcome, Y (ie \u03b11 above). \n \nIn practice, we are likely to know E and rym since these have been estimated in the \n(under-specified) regression model that has been fitted (Equation 1). We need to \nidentify what possible, plausible or likely values of puy, qum and sux could produce \nthese known values even if there is no true effect of X on Y. \n \nSetting up the simulation \nThe simulation was run in SPSS 15.0. Initially, 100,000 cases of five normally \ndistributed N(0,1) random variables (RV1 to RV5) were generated. These were used \nto compute the variables Y, U, M and X with the desired inter-correlations. The \nequations used and the SPSS syntax can be found in the Appendix. Values of puy and \nqum were selected to match plausible estimates of those in the example studies. The \nvalue of sux was allowed to vary by adding a random variable to U with different \n(random) relative weights and allocating X = 1 if the sum was greater than zero, \notherwise X = 0. For each assigned combination of puy, qum and rym, the simulation \nwas run 50 times, each with a different (random) value of sux. For each of these \nsimulations, the value of E, the spurious phantom \u2017effect\u2018 that would appear in a \n5 \nregression model that omitted U, was plotted against sux and a trend line (cubic \npolynomial) fitted (see Figure 1 to Figure 3). \nAt one extreme (sux =0), group membership (X) was purely at random, \nuncorrelated with the unmeasured variable, U. At the other extreme, group \nmembership was entirely determined by the value of U: X = 1 if U > 0, X = 0 \notherwise. Note that even in the latter case of complete dependence, if a Gaussian \nvariable is dichotomised like this the correlation (sux) between U and X is capped at \nabout 0.8. In between these two extremes are varying levels of dependence between \nthe unmeasured variable and group membership. \nOne of the most difficult aspects of interpreting the results of the simulations \nis to estimate a plausible value for this correlation, sux. Table 1 shows a conversion \nbetween values of this correlation, the equivalent standardised effect size and an \ninterpretation based on Rosenthal and Rubin\u2018s (1982) binomial effect size display \n(BESD). This illustrates how if the population were dichotomised at the mean value \non the unmeasured variable, U, the possible percentages of high and low scorers who \nwould be found in each treatment group. Various assumptions are required for this \nconversion, which is inevitably simplistic, but may nevertheless provide some help in \ninterpretation (for further explanation and discussion see Coe, 2002). \n \n \nTable 1: Illustrative interpretations of different values of the correlation, sux, between group \nmembership, X, and the unmeasured variable, U. \n \nCorrelation (point-\nbiserial) between \ngroup membership, X, \nand unmeasured \nvariable, U. \n \nsux \n \nEquivalent \nstandardised \neffect size \n \nd \n \n \nPercentage of those above \naverage on the unmeasured \nvariable, U, who are in \nthe \nintervention \ngroup \nX = 1 \n \nthe  \ncontrol  \ngroup \nX = 0 \n \n0 0.00 50% 50% \n0.1 0.20 55% 45% \n0.2 0.41 60% 40% \n0.3 0.63 65% 35% \n0.4 0.87 70% 30% \n0.5 1.15 75% 25% \n0.6 1.50 80% 20% \n0.7 1.96 85% 15% \n0.8 2.67 90% 10% \n    \n \n \n6 \nSummary of the example studies \n1. The Impact of Study Support \nBackground to the study \nThis study by MacBeath et al (2001) is subtitled \u2017A report of a longitudinal study into \nthe impact of participation in out-of-school-hours learning on the academic \nattainment, attitudes and school attendance of secondary school students\u2018. The report \nanalysed the performance at GCSE and Key Stage 3 (national assessments at age 16 \nand 14) of over 8000 pupils in 52 schools, as well as substantial qualitative data. The \nmain statistical analysis used multiple regression. The report was commissioned and \npublished by the Government Department for Education and Skills. It is still \n(September 2009) available and cited as key support for a number of related policy \ninitiatives (see http:\/\/www.standards.dcsf.gov.uk\/studysupport\/about\/).  \nAnalysis and results \nThe study evaluated a variety of forms of study support including Y10 subject-\nfocussed, Y10 sport, Y10 aesthetic, Y11 subject-focussed, Y11 other, Y10 drop-in \nand Y11 Easter school. Of these the last had the biggest effect on achievement and is \ntherefore taken as an upper bound for the effect. Outcomes included GCSE English, \nGCSE mathematics, average of best 5 GCSE grades, number of A-C passes, Key \nStage 3 average, as well as attitudes and attendance. The first two of these have been \nchosen for this simulation as they are simple, valued outcomes that do not require the \nassumption that all GCSEs are equally difficult and span the range of R\n2\n values \nquoted (63.1% and 70.4% respectively). Covariates in the model are Y9 SATs \n(national tests in English, mathematics and science), gender and individual school \ndummy variables. Regression coefficients cited in the study for the \u2017effects\u2018 of Y11 \nEaster School are equivalent to standardised mean differences of 0.18 for English and \n0.11 for mathematics. \nClaims made by the study \nThe word \u2017impact\u2018 in the title announces the causal claim unequivocally. Almost \nevery sentence in the executive summary (p7) confirms this, for example, \n \n\u2017Study support has effects which are significant and substantial for GCSE performance\u2018 \n\u2017Study support can improve attainment in Maths and English by half a grade\u2018 \n\u2017Participation improves Maths attainment by one third of a level and Science attainment \nby three quarters of a level\u2018 \n\u2017All students who participate benefit from study support\u2018 \n\u2017Participation in study support has a favourable effect on attitudes to school\u2018  \n\u2017participation in some forms of study support has a positive impact on school attendance\u2018 \n\u2017Study support has an impact at whole school level when participation rates are high\u2018 \n \n2. Gifted and Talented support in \u2018Excellence in Cities\u2019 \nBackground to the study \nThe Excellence in Cities  (EiC) initiative was launched in 1999 with the aim of raising \nstandards in inner cities and other urban areas in England (Kendall et al, 2005; see \n7 \nalso DCSF, 2009). There were seven key policy strands, of which \u2017Gifted and \nTalented\u2018 is our focus here.  This strand became national policy before the end of EiC \nand continues to be a key element of government policy for schools in England (see \nhttp:\/\/www.standards.dcsf.gov.uk\/giftedandtalented\/ ). As part of the Gifted and \nTalented (G&T) strand, schools were asked to identify between 5 and 10% of their \nmost able pupils. Schools were given additional funding to support the learning of \nthese pupils, most of which was spent on specialist teaching materials, teacher salaries \nor incentive points, out-of-school activities and supply cover (Kendall et al., 2005, \np91). \nAnalysis and results \nDetailed analysis of the attainments of pupils are presented in a report by Morris and \nRutt (2005). Attainment outcomes included Key Stage 3 levels in mathematics and \nEnglish, as well as average level, total GCSE score, capped 8 (total of the best 8 \nGCSE grades) and average GCSE grade. The highest R\n2\n values in multilevel models \nat each Key Stage were for average KS3 level (66%) and capped 8 GCSEs (80%) \n(p20). Regression coefficients (fixed effects) were estimated for 42 different \nexplanatory variables, including a dummy for G&T designation. Conversion of \ncoefficients of G&T gives standardised effect sizes of 0.30 for average KS3 and 0.22 \nfor capped 8 GCSE. \n \nClaims made by the study \nAlthough some caveats and limitations of the data are pointed out by Kendall et al \n(2005), in the main summaries of the results the causal attribution is clear. For \nexample: \n \n\u2015\u2026 EiC has led to an increase in average attainment\u2016 (p25) \n\u2015\u2026 there was evidence of a positive impact for some specific groups of students\u2016 (p25) \n\u2015This was equivalent to increasing the percentage of pupils achieving level 5 or above by \nbetween 1.1 and 1.9 percentage points\u2016 (p25) \n\u2015\u2026early mentoring (in Year 7) had enabled some pupils to overcome barriers to \nlearning\u2016 (p26) \n\u2015The impact of the [G&T] Strand appeared greater for pupils with lower levels of \nattainment at the end of Key Stage 3.\u2016 (p54) \n\u2015\u2026 the main effects from the quantitative analysis have been in three areas: \n\u2022 in improving levels of attainment in Mathematics at Key Stage 3 (with the greatest \nimpact in the most disadvantaged schools) \n\u2022 for pupils identified as gifted and talented \n\u2022 in improving attendance.\u2016 (p121) \n \nResults of the simulations \n1. The Impact of Study Support \nAssigning values of the inter-correlations p, q, r \nThe R\n2\n values quoted above for this study (MacBeath et al, 2001) correspond to \nmultiple-R values of 0.79 for English and 0.84 for mathematics. Hence an appropriate \nvalue for rym, the correlation between the outcome (GCSE grade) and measured \n8 \ncovariates (KS3 SATs, gender, and school level dummies) is 0.8. This represents a \nhigh level of explained variance for an outcome measure such as GCSE performance. \nIdentifying possible unmeasured variables that might account for the effect is \nto some extent a matter of speculation. One obvious candidate would be \nsocioeconomic status (SES). A meta-analysis by Sirin (2005) found the average \ncorrelation between SES and academic achievement to be around 0.3. However, in \npractice SES measures are often a rather loose and poorly measured proxy for the true \nfamily situation; the use of SES measures that are unrestricted, focus on home \nresources (eg books), collect data directly from parents and relate to specific \noutcomes such as mathematics or verbal measures can increase the correlation to as \nmuch as 0.5.  It certainly seems plausible that SES could influence students\u2018 \nlikelihood of participating in study support activities such as Y11 Easter school: \nstudents from more advantaged backgrounds could well be more likely to take part.  \nOther possible candidates for unmeasured variables in this study include \nmotivation (Ugoroglu and Walberg, 1979, found average correlation with \nachievement of 0.34), self-discipline (found by Duckworth and Seligman, 2005, to \nhave correlations as high as 0.7 with achievement), emotional intelligence (Goleman, \n1998), resilience (Wang et al, 1998), self-esteem (Brookover et al, 1965), educational \naspirations (Sewell and Shah, 1968). All these have been shown to be related to \nacademic achievement and could plausibly influence the chances of a student \nembarking on and sustaining participation in out-of-lesson study support. There is no \nneed for us to consider school-level factors, as the inclusion of school dummy \nvariables will have dealt with any unobserved school-level confounders. However, it \nis possible that within-school variations in teacher-level characteristics could \nconfound the estimate of effects. For example, the level of individual teachers\u2018 \nmotivation, commitment and general instructional quality could influence both GCSE \noutcomes and their students\u2018 willingness to attend an Easter school.  \nTaken individually, several of these factors have been shown to have \ncorrelations of at least 0.3 with achievement. Of course, there may be considerable \noverlap among them, but if all these factors were measured well and combined, it is \nlikely that the overall correlation with achievement would exceed 0.3. Certainly, a \nmultiple correlation of around 0.4 does not seem hard to accept and 0.5 or higher may \neven be defensible.  \nGiven these considerations, the simulation was run with estimates of the \ncorrelation between unmeasured variables (U) and both prior and outcome \nachievement (M and Y, respectively) of 0.3, 0.4 and 0.5.  \nResults \nThe results of the simulations are shown in Figure 1. If the unmeasured variables in \nthe model are assumed to have a correlation with both outcome and prior attainment \nof around 0.3, the artefactual attribution of the true effects of this variable to group \nmembership ranges from zero to an effect size of about 0.10, depending on the \nstrength of the relationship between group membership and the unmeasured variable. \nIf the correlation is 0.4 it ranges from zero to 0.14 and climbs to 0.19 if the correlation \nis assumed to be 0.5.  \n \n9 \n \nFigure 1:  Relationship between sux, the strength of association between unmeasured variable \nand group membership, and the spurious \u2018effect\u2019 , E, that would be attributed to Study Support \nparticipation, under different assumed values of puy and qum (the correlations between the unmeasured \nvariable with outcome and measured covariates respectively), when rym (the correlation between \nmeasured covariates and outcome) is fixed at 0.8. \nInterpretation \nIn the MacBeath et al (2001) study, the effect sizes for \u2017Y11 Easter school\u2018 on GCSE \nEnglish language and on GCSE mathematics were estimated at 0.18 and 0.11 \nrespectively (see above).  \nThe former value (0.18) seems to be right at the top end of what could be an \nartefact of inadequate modelling, under the plausible assumptions outlined above. \nFailure to include variables such as socioeconomic status, motivation or teacher \nquality in the model might have inflated the estimate of the effect of study support in \nthis case, but unless we are willing to assume the maximum plausible correlation \nbetween these unmeasured variables and GCSE English grade, and to posit an \nextremely strong relationship between the unmeasured variables and participation in \nthe Y11 study school, this failure cannot account for the whole of the effect.  \nWe must try to narrow the range of plausible assumed parameters. Estimating \nplausible values for sux, depends on knowing the likely strength of the relationship \nbetween unmeasured variables, such as SES, and participation in Y11 study school.  \nData from the Yellis (Year 11 information system) survey (www.yellisproject.org) \nsuggest correlations of the order of 0.3 between measures of either socioeconomic \nstatus or academic motivation with reported participation in after-school clubs. It \nseems likely that for an activity such as the Easter revision course, the association \nbetween participation and academic motivation could be higher than this. Hence we \nmay assume that sux, the correlation between the unmeasured factors and participation \nin study support, may be in the range 0.2 to 0.5.  For the values of puy and qum , any of \nthe values used in the simulation (0.3, 0.4 and 0.5) seem plausible. With this range of \nlikely parameters, the range of possible values for E, the estimate of the spurious \neffect is from 0.03 to 0.11.  \nEstimates of spurious 'effect' of Study Support\n(R\n2 \n= 0.64, r ym  = 0.8)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\ns ux : Correlation between unmeasured variable(s) and treatment group membership\nA\np\np\na\nre\nn\nt \ne\nff\ne\nc\nt \ns\niz\ne\n, \nE\n r ym = correlation between outcome and measured variable(s)\n p uy  = correlation between outcome and unmeasured variable(s)\n q um  = correlation between measured and unmeasured variable(s)\np uy  = 0.5, q um  = 0.5\np uy  = 0.4, q um  = 0.4\np uy  = 0.3, q um  = 0.3\n10 \nIf we go further and seek a single \u2017best guess\u2018 for E we might take 0.4 as the \nmost likely value for puy and qum and 0.4 as an estimate for sux. This would produce a \nspurious effect size of about 0.07. In this case, the true effect of Y11 Easter school \nstudy support on GCSE English would be the difference between this and the estimate \n(0.18) from the regression model, i.e. about 0.11. \nFor GCSE mathematics, on the other hand, the effect size of 0.11 appears to \nbe just within the range that might plausibly occur as an artefact of underspecifying \nthe model.  Under the same \u2017best guess\u2018 that led to estimating the artefactual effect as \n0.07, the true effect would remain at only 0.04. Although positive, this is a very small \neffect, unlikely to be statistically or educationally significant and, given the \nuncertainty surrounding many of the assumptions made, inevitably subject to a wide \nmargin of error.  \n \n2. Gifted and Talented support in \u2018Excellence in Cities\u2019 \nAssigning values of the inter-correlations p, q, r \nMultiple-R values corresponding to the R\n2\n values cited above are 0.8 for average KS3 \nand 0.9 for capped 8 GCSE. These two values were therefore used for the correlation \nbetween outcome and measured variable, rym.  \nGiven such a high proportion of variance explained in the model, and such a \nwide range of explanatory variables included, it might seem unlikely that any \nunmeasured variable could make much difference to the outcome. However, the \nparticular nature of the G&T designation opens up another type of threat here. If \npupils are identified as G&T on the basis of their attainment, then they are effectively \nselected for \u2017treatment\u2018 on a variable that is highly correlated with the outcome. For \nexample, suppose participation in Year 11 G&T activities (X = 1) was open to those \nwho had performed in the top 10% in a school on their internal end of Y10 exams. If \nwe take these Y10 exam results as the unmeasured variable, U, we would expect very \nhigh correlations between U and X. Plausible values for the correlation between the \nmeasured covariates (prior attainment, FSM status, gender, ethnicity) and unmeasured \n(Y10 exam) are likely to be 0.7 or 0.8, as are correlations between the latter and the \noutcome measure (GCSE). Similar correlations could be expected for KS3, where the \nunmeasured variable would be Y8 exam performance, though as the multiple-R value \nfor KS3 is slightly lower, we may reduce the other correlations accordingly. \nHence for KS3, the simulation was run with rym = 0.8; puy = 0.6 and 0.7; qum = \n0.6 and 0.7. For GCSE, the simulation was run with rym = 0.9; puy = 0.7 and 0.8; qum = \n0.7 and 0.8. \nResults \nThe results of the simulations for the impact of G&T on KS3 are shown in Figure 2 \nand on GCSE in Figure 3.  \n \n11 \n \nFigure 2:  Relationship between sux, the strength of association between unmeasured variable \nand group membership, and the spurious \u2018effect\u2019 , E, that would be attributed to G&T participation, \nunder different assumed values of puy and qum (the correlations between the unmeasured variable with \noutcome and measured covariates respectively), when rym (the correlation between measured \ncovariates and outcome) is fixed at 0.8. \n \n \nFigure 3:  Relationship between sux, the strength of association between unmeasured variable \nand group membership, and the spurious \u2018effect\u2019 , E, that would be attributed to G&T participation, \nunder different assumed values of puy and qum (the correlations between the unmeasured variable with \noutcome and measured covariates respectively), when rym (the correlation between measured \ncovariates and outcome) is fixed at 0.9. \n \n \nEstimates of spurious 'effect' of Gifted & Talented provision at KS3\n(R\n2 \n= 0.64, r ym  = 0.8)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\ns ux : Correlation between unmeasured variable(s) and treatment group membership\nA\np\np\na\nre\nn\nt \ne\nff\ne\nc\nt \ns\niz\ne\n, \nE\np uy  = 0.7, q um  = 0.6\n r ym = correlation between outcome and measured variable(s)\n p uy  = correlation between outcome and unmeasured variable(s)\n q um  = correlation between measured and unmeasured variable(s)\np uy  = 0.7, q um  = 0.7\np uy  = 0.6, q um  = 0.6\np uy  = 0.6, q um  = 0.7\nEstimates of spurious 'effect' of Gifted & Talented provision at GCSE\n(R\n2 \n= 0.81, r ym  = 0.9)\n-0.10\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\ns ux : Correlation between unmeasured variable(s) and treatment group membership\nA\np\np\na\nre\nn\nt \ne\nff\ne\nc\nt \ns\niz\ne\n, \nE\np uy  = 0.8, q um  = 0.7\n r ym = correlation between outcome and measured variable(s)\n p uy  = correlation between outcome and unmeasured variable(s)\n q um  = correlation between measured and unmeasured \nvariable(s)\np uy  = 0.8, q um  = 0.8\np uy  = 0.7, q um  = 0.7\np uy  = 0.7, q um  = 0.8\n12 \nInterpretation \nFor the outcome average KS3 level (with rym = 0.8), it is clear that even with \nrelatively high proportions of variance explained by the model, if an unmeasured \nvariable has high correlation (puy) with the outcome, then quite substantial spurious \neffects can appear. For puy = 0.7 the artefact could be equivalent to a standardised \neffect size as high as 0.45; up to 0.25 if puy = 0.6. Reducing the estimate of the \ncorrelation (qum) between the hypothesised unmeasured variable and the measured \nvariables already in the model also increases the size of the spurious effect. This \nmight correspond to changing the timing of the assessment that is used to determine \nG&T status: if it is close to the baseline assessment, qum would be high, puy would be \nlower; if it is closer to the outcome assessment, qum would be lower, while puy would \nbe high.  \nFor this simulation it seems plausible that the correlation (sux) between G&T \nstatus (X) and the unmeasured Y8 assessment (U) could be very high, since eligibility \nfor G&T might conceivably be entirely dependent on performance (ie sux = 0.8). Even \nif other factors are taken into account in identifying G&T pupils, sux seems unlikely to \nfall below 0.6. Under these assumptions, the actual effect size estimated by the \nmultilevel model (0.30) is just about within the range that could be a pure artefact of \nselecting treatment groups on the basis of an unmeasured variable that is highly \ncorrelated with the outcome: E is between 0.16 and 0.32. If we had to make a best \nguess at a single combination of parameters, we might choose puy = 0.7, qum = 0.7, sux \n= 0.7, giving an estimate of the spurious effect of E = 0.26. The difference between \nthis and the actual estimate is too small to be considered evidence of a genuine effect. \nFor effects on \u2017GCSE capped 8\u2018, the proportion of variance accounted for in \nthe model is even higher (rym = 0.9). Even so, if we are prepared to hypothesise that a \npupil\u2018s designation as G&T could be strongly dependent on an assessment whose \nresults were not available to the evaluators, but that was itself highly correlated with \nthe outcome, then we could still get a substantial spurious effect: at the top end of our \nplausible assumptions, a standardised effect size of 0.40 is possible. Here a \u2017best \nguess\u2018 set of parameters might be puy = 0.8, qum = 0.8, sux = 0.7, giving an estimate of \nthe spurious effect of E = 0.17. Again, this is below the effect size actually estimated \n(0.22) so there may be a remaining positive effect, but the difference (0.05) is \ncertainly small and subject to a good deal of uncertainty.  \nInterestingly in this model, if the correlation (puy) between the outcome and \nthis unmeasured variable falls by just 0.1, other things being equal, we will see a \nnegative spurious effect emerging. Under these conditions the estimated effect size \nmight actually be smaller than the true effect. Hence it seems the size of the spurious \neffect is quite sensitive to small changes in the parameters assumed.   \n \nDiscussion \nTwo examples have been used to illustrate that when regression models are used to \nestimate the effects of participation in a programme by adjusting for known \ncovariates, these estimates can, under reasonable assumptions, be substantially biased \nby the failure to include in the model other factors that may be related to both group \nselection and the outcome. In all the cases considered here, what appeared to the \nresearchers to be a substantial and unequivocal difference interpretable as a causal \neffect either disappears or becomes reduced to a tiny and uncertain difference when \n13 \nthe effect of unobserved differences is taken into account. A summary of the example \nstudies, their results and claims, together with the results of the simulation and its \ninterpretation is presented in Table 2. For these examples it is clear that the \nconclusions of the studies would have been very different had the effects of \nunobserved differences been considered. A number of further points emerge as \nworthy of comment. \nThe first is that even statistical models with a very high proportion of variance \nexplained (R\n2\n) can be subject to these spurious effects. One might have thought that \nR\n2\n values as high as 0.81 (multiple-R of 0.9) would pretty much guarantee that, with \nsuch a small amount of remaining unexplained variance to account for, other factors \ncould not make too much difference. This thought proved to be wrong, however. \nSecond, the size of these spurious effects is quite sensitive to small changes in \nthe assumed values of the parameters in the simulation. For example, we saw that the \nestimate of the effect of G&T provision on GCSE could fall from 0.22 down to -0.06 \njust by changing the estimate of the correlation (puy) between the outcome and the \nunmeasured variable by as little as 0.1, other things being equal (see Figure 3). \nThird, and related to the second, is that choosing reasonable values for the \nassumed parameters is far from easy. As a result, the \u2017best guess\u2018, \u2017likely range\u2018 and \neven \u2017possible range\u2018 of values calculated here for the spurious effect, E, will be \ncontroversial and open to challenge. No doubt some readers will already have thought \nas they read the account of the simulations that the assumptions underpinning them \nwere wrong, or at least open to argument.  \nFourth, the fact that it is both hard and crucial to get the assumptions right is \nnot a reason not to try. Debates are necessary about issues such as what assumptions \nare reasonable, what kinds of unmeasured variables should be considered, what \nranges of their possible correlations with the measured and outcome variables are \nplausible, and what the strength of the relationship might be between these \nunobserved variables and group selection. If there are differences of opinion on these \nmatters then we will be able to see what effect those differences might have on the \nconclusions from the study. Unless we believe that getting a single, simple answer is \nmore important than getting it right, making this uncertainly explicit is a good thing. \nWe must also bear in mind that not to engage in a debate about these assumptions is \nin effect to make a default assumption that all the correlations are zero. \nFifth, it is important to point out that the example studies considered here are \nof generally high quality and excellent in many ways. These are by no means taken \nfrom the bottom end of the quality spectrum of evaluations of educational policy \ninitiatives. Certainly, with regard to technical issues such as sampling, \ninstrumentation, survey execution, complementary mixing of quantitative and \nqualitative data and the sophistication of the methods of statistical analysis used, these \nstudies can claim to be exemplary in at least some respects. Where they are all open to \ncriticism, however, is in their failure to consider other possible causes of the effects \nthey describe. \n \n14 \nTable 2 \nStudy MacBeath et al. (2001) Kendall et al. (2005) \nIntervention \/ \nprogramme \nStudy support (Y11 \nEaster School) \nGifted & Talented provision \nOutcome(s) GCSE \nEnglish;  \nGCSE \nmaths \nKS3 average \nlevel;  \nGCSE \ncapped 8 \nscore \nCovariates KS3 SATs average, \nGender, School type \nPrior attainment, FSM status, \ngender, ethnicity \nR\n2\n in the model 63%; 70% 66%; 80% \nEstimate of the \neffect, from \nregression model \n0.18; 0.11 0.30; 0.22 \nInterpretation given \nby the researchers \n\u2018Study support can \nimprove attainment in \nMaths and English by \nhalf a grade\u2019 \n\u2018Pupils designated as gifted \nand talented had higher \nlevels of attainment at the \nend of Key Stages 3 and 4 \nthan those of otherwise \nsimilar pupils not \ndesignated.\u2019 \nPossible relevant \nunmeasured \nvariable(s) \nSocioeconomic status; \nMotivation; Self-discipline \nAttainment used to identify \nG&T status \nRange of possible \nspurious effects \n 0.0 \u2013 0.19 0.0 \u2013 0.45 -0.06 \u2013 0.40 \nRange of likely \nspurious effects \n 0.04 \u2013 0.11 0.16 \u2013 0.32 0.10 \u2013 0.21 \nBest guess at \nspurious effect \n 0.07 0.26 0.17 \nJustified conclusion, \ntaking account of \nbias due to omitted \nfactors \nPossible small residual \neffect (0.11) on English \nbut pretty much no \ngenuine effect on maths \nAny genuine effect for both \noutcomes is very close to \nzero \n \n \nRecommendations for research \nA number of recommendations for further work emerge from this study. The first is \nthat there are probably other ways such a sensitivity analysis could have been \nconducted. Validating the results of this simulation against other methods of \nachieving the same would be a useful step. If the results prove to be robust, ways of \nmaking this kind of approach easier to conduct with widely available software should \nbe explored. \nSecond, it would not be too difficult to extend the method to other kinds of \nstatistical analyses. In fact, Kendall et al.\u2018s (2005) evaluation used multilevel models \nrather than the simple OLS regression used in the current simulation; it is possible that \na simulation using multilevel models would have arrived at a different result. \nPropensity score matching has become the method of choice for creating well-\n15 \nmatched groups in non-randomised comparisons in health research and other areas. \nThis approach offers significant advantages over the kinds of multivariate regression \nused in the educational examples considered here (Klungel et al., 2004). It would \ncertainly be valuable, though perhaps more difficult, to conduct sensitivity analyses of \nthe results of studies that have used this approach.  \nThirdly, if the two studies considered here are at all typical, there is a need for \neducational researchers to be considerably more cautious in making causal claims on \nthe basis of statistical analyses of the differences between those who have and have \nnot experienced a particular educational programme. It seems that the issues that were \ndebated, the lessons learned and the developments made in epidemiology fifty years \nago in debating whether non-experimental evidence could establish smoking as a \ncause of lung cancer (e.g. Cornfield et al, 1959; see Hill et al, 2003) have passed us \nby: we need to catch up. Educational researchers must also be aware of a range of \npowerful statistical methods for dealing with selection bias due to unobserved \nvariables developed and widely used by econometricians (Green, 2003). Policy \nmakers who use their results may also need to be more cautious and critical of \nresearchers\u2018 claims \u2013 assuming they genuinely want to know whether the policy will \nwork (Pritchett, 2002). \nFourthly, evaluators who use statistical control to evaluate effects of \neducational programmes should give more explicit thought to the possible effects of \nunmeasured variables. At the very least, to acknowledge the possibility that \nsomething other than the programme might be responsible for the difference would be \na start. Better still would be to systematically list possible factors that are either \nunmeasured or inadequately measured and that might be related both to participation \nin the programme and to the outcome. For each factor, an argument should be made \nabout the plausibility of its influence, based on existing evidence about associations \nand theoretical arguments about possible mechanisms. Even better, this argument \nshould include a sensitivity analysis to quantify how big an effect it could plausibly \nhave, under particular, plausible and explicitly stated assumptions. As has been \ndemonstrated here, conducting such a sensitivity analysis is no more difficult than the \nkinds of statistical techniques routinely used by evaluators. \nFinally, the extent to which unmeasured factors can undermine causal claims \nreinforces the case for the use of stronger evaluation designs and analyses. Even if we \nacknowledge that randomised controlled trials (RCTs) are not always appropriate or \npossible, it is almost certainly the case that they could be used more often than they \nare (Torgerson and Torgerson, 2007; Cook, 2003; Slavin, 2008). Furthermore, there \nare non-randomised designs that are considerably stronger than the ones used in the \nexamples here: for example, regression-discontinuity and time-series designs (Shadish \net al., 2002; Cook et al., 2008).  \nOf course, RCTs can be ethically problematic; they can impose restrictions on \nthe representativeness of samples, interventions or contexts; they may have \ninappropriate time-frames; and they do not necessarily solve problems such as \nattrition, implementation fidelity, contamination of treatments or poor outcome \nmeasurement. However, their claim to offer the \u2017gold standard\u2018 of evidence for causal \ninference rests on their power to minimise selection bias. If the threat of this bias is \nnot regarded as a particular problem, as seems to have been the case in the examples \nconsidered here, then the arguments against RCTs may seem convincing. The current \nstudy suggests, though, that the bias arising from unmeasured factors can be a very \nsignificant problem: the interpretation of results and attribution of causality can be \n16 \nentirely overturned. If so, the case for using a random allocation design (RCT) that \ncan eliminate this bias is more compelling. \n \nReferences \nArah OA, Chiba Y and Greenland S (2008) Bias formulas for external adjustment and \nsensitivity analysis of unmeasured confounders. ANNALS OF \nEPIDEMIOLOGY,    Volume: 18,    Issue: 8,    Pages: 637-646. \nBrookover, W.B., LePere, J.M., Hamachek, D.E., Thomas, S., Erickson, E.L., 1965, \nSelf-concept of ability and school achievement II. Co-operative research \nproject no. 1636, Michigan State University \nCoe, R. (2002) It's the effect size, stupid: what effect size is and why it is important. \nPaper presented at the Annual Conference of the British Educational Research \nAssociation, University of Exeter, England, 12-14 September 2002.  \nCoe, R. (2009) \u2017Unobserved but not unimportant: The effects of unmeasured variables \non causal attributions\u2018. Paper presented at 4th Annual Conference on \nRandomised Controlled trials in the Social Sciences, University of York, 14-\n15 Sept 2009. \nCook, T.D. (2003) \u2017Why have educators chosen not to do randomized experiments?\u2018 \nAnnals of the American Academy of Political and Social Sciences, 589, 114-\n149. \nCook, T. D., Shadish,W. R., andWong, V. C. (2008), \u2015Three Conditions Under Which \nExperiments and Observational Studies Produce Comparable Causal \nEstimates: New Findings From Within-Study Comparisons,\u2016 Journal of Policy \nAnalysis and Management, 27, 724\u2013750. \nCornfield J, Haenszel W, Hammond EC, Lilienfeld AM, Shimkin MB, Wynder EL  \n(1959) Smoking and lung cancer: Recent evidence and a discussion of some \nquestions. Journal of The National Cancer Institute    Volume: 22    Issue: 1    \nPages: 173-203. \nDCSF (Department for Children, Schools and Families) (2009) The Standards Site \u2013 \nExcellence in Cities.   \nhttp:\/\/www.standards.dfes.gov.uk\/local\/excellence\/whatis_eic.html  (Accessed \n12.7.09) \nDuckworth, A. L. and Seligman, M. E. P. (2005) Self-Discipline Outdoes IQ in \nPredicting Academic Performance of Adolescents. Psychological Science; \nDec2005, Vol. 16 Issue 12, p939-944. \nGennetian, L.A., Bos, J.M. and Morris, P.A. (2002) Using Instrumental Variables \nAnalysis to Learn More from Social Policy Experiments. Manpower \nDemonstration Research Corporation. \nGoldstein, H. (1987). Multi-level models in Educational and Social Research.: \nLondon : Griffin \nGreene, W.H., (2003) Econometric Analysis, Fifth edition. New Jersey: Prentice Hall. \nGroenwold, R.H.H., Hak, E., and Hoes, A.W. (2009) Quantitative assessment of \nunobserved confounding is mandatory in nonrandomized intervention studies. \nJournal of Clinical Epidemiology, 62, 22-28. \nHeckman, J.J. (1997) Instrumental variables: a study of implicit behavioral \nassumptions used in making program evaluations. Journal of Human \nResources 32, 3, 441\u201362. \n17 \nHill, G., Millar, W. and Connelly, J. (2003) \u2015The Great Debate\u2016: Smoking, Lung \nCancer, and Cancer Epidemiology. Canadian Bulletin of Medical History, 20, \n2, 367-386. \nKendall, L., O\u2018Donnell, L., Golden, S., Ridley, K., Machin, S., Rutt, S., McNally, S., \nSchagen, I., Meghir, C., Stoney, S., Morris, M., West, A., and Noden, P., \n(2005) Excellence in Cities: The National Evaluation of a Policy to Raise \nStandards in Urban Schools 2000-2003. Department for Education and Skills. \nResearch Report RR675A. \nKlungel, O.H., Martens, E.P., Psaty, B.M., Grobbee, D.E., Sullivan, S.D., Stricker, \nB.H.Ch., Leufkens, H.G.M. and de Boer, A. (2004) Methods to assess \nintended effects of drug treatment in observational studies are reviewed. \nJournal of Clinical Epidemiology, 57, 1223-1231. \nLeow, C, Marcus, S, Zanutto, E, and Boruch, R (2004) Effects of advanced course-\ntaking on math and science achievement: Addressing selection bias using \npropensity scores. American Journal of Evaluation, 25, 4, 461-478. \nMacBeath, J., Kirwan, T., Myers, K., McCall, J., Smith, I., McKay, E., Sharp, C., \nBhabra, S., Weindling, D., and Pocklington, K. (2001) The Impact of Study \nSupport: A report of a longitudinal study into the impact of participation in \nout-of-school-hours learning on the academic attainment, attitudes and school \nattendance of secondary school students. Department for Education and Skills \nResearch Report RR273. ISBN 1 84185 521 9 \nMorris, M. and Rutt, S. (2005) Excellence in Cities: Pupil outcomes two years on. \nExcellence in Cities Evaluation Consortium (NFER, LSE, IFS). Available at \nhttp:\/\/www.nfer.ac.uk\/publications\/pdfs\/downloadable\/MLM2005.pdf \n[accessed 24.7.09] \nMorrison, K. (2009) Causation in Educational Research. Abingdon: Routledge. \nPan, W. and Frank, K.A. (2003) A Probability Index of the Robustness of a Causal \nInference. Journal of Educational and Behavioral Statistics, Winter 2003, Vol. \n28, No. 4, pp. 315-337. \nPower, S., Whitty, G., and Wisby, E. (2006) The Educational and Career Trajectories \nof Assisted Place Holders.  A report for the Sutton Trust, July 2006 \nPritchett, L, (2002) It pays to be ignorant: A simple political economy of rigorous \nprogram evaluation. Journal of Policy Reform, 5, 4, 251-269. \nRosenbaum, P. R. (1986) Dropping out of high school in the United States: An \nobservational study.  Journal of Educational Statistics, 11, 3, 207-224. \nRosenbaum P R (1991) Discussing hidden bias in observational studies. Annals of \nInternal Medicine 115: 901\u20135 \nRosenbaum, P. R. (2004) Observational Studies: Overview. International \nEncyclopedia of the Social & Behavioral Sciences, 2004, Pages 10808-10815. \nRosenthal, R, and Rubin, D.B. (1982) \u2017A simple, general purpose display of \nmagnitude of experimental effect.\u2018  Journal of Educational Psychology, 74, \n166-169. \nRubin, D. B. (2008) Comment: The Design and Analysis of Gold Standard \nRandomized Experiments. JOURNAL OF THE AMERICAN STATISTICAL \nASSOCIATION, 103, 484, 1350-1353. \nSchneider, B., Carnoy, M., Kilpatrick, J., Schmidt, W. H., and Shavelson, R. J. \n(2007). Estimating causal effects using experimental and observational \ndesigns (report from the Governing Board of the American Educational \nResearch Association Grants Program). Washington, DC: American \nEducational Research Association. \n18 \nSewell, W.H. and Shah, V.P. (1968) Parents\u2018 education and children\u2018s educational \naspiration and achievements. American Sociological Review, 33, 2. \nShadish, W., Cook, T. D., & Campbell, D. (2002). Experimental and quasi-\nexperimental designs for generalized causal inference. Boston: Houghton \nMiflin. \nSirin, S.R. (2005) Socioeconomic status and academic achievement: A meta-analytic \nreview of research. Review of Educational Research    Volume: 75    Issue: 3    \nPages: 417-453 \nSteele, FA, Vignoles, A & Jenkins, A. (2007) \u2017The effect of school resources on pupil \nattainment: a multilevel simultaneous equation modelling approach\u2018, Journal \nof the Royal Statistical Society: Series A (Statistics in Society), 170 (3), pp. \n801\u2010824. \nWang, M. C., Haertel, G. D., & Walberg, H. J. (1998). Educational resilience \n(Laboratory for Student Success Publication Series No. 11). Philadelphia: \nTemple University Center for Research in Human Development and \nEducation. \nWinship, C. and Morgan, S.L. (1999) The estimation of causal effects from \nobservational data. Annual Review of Sociology, 25, 659-706. \n \n19 \nAppendix \nEquations for the simulation \nWe assume, without loss of generality, that all continuous variables Y, U, M , as well \nas RV1 to 5, are N(0,1). \n \nLet RV1 be the \u2017common part\u2018 of M and U, and the \u2017distinct parts\u2018 be RV2 and RV3 \nrespectively. In other words, for some constants, c and d, \n \nM = c RV1 + d RV2 \nU = c RV1 + d RV3 \n \nThen \nc\n2\n + d\n2\n = 1 \n \nand \nc\n2\n = qum \n \nThen if \ne2 = f RV4 \n \nEquation 2 becomes \n \n4321 1010 fRVRVdRVdRVcY  \n \nCalculating the correlations between this and M and U respectively, using the fact that \nall variables have mean 0 and variance 1, together with the knowledge that the RVs \nare mutually independent, gives a set of equations that can be rearranged to give \n \n12\n0\nq\nprq\n \n \nAnd \n12\n1\nq\nrpq\n \n \n \n20 \nSPSS syntax for the simulation \nThis syntax runs in SPSS V15.0. The values of r, p and q in section 1. a) ii) should be \nset to the desired parameters before each run. For convenience, the same values can \nbe entered into the final line so that a correctly labelled file is saved in the directory \n\u2017D:\\TEMP\\simresults\u2018 (the pathname can be changed as desired). \n \n \n* simulation to show how a missing variable can make a lot of difference . \n \n* Contains 2 macros: \n1 to create a file of simulated data, save the required parameters in a dataset called 'combined' \n2 to run the simulation repeatedly (50 times) and save the output . \n \nset format=f5.3 . \n \n*================================================== \n1. MACRO to run simulation once and output coeffs to SPSS dataset \nusing random values of correlation parameters, p, q, r, s \n=================================================== . \n \nDEFINE !simulate_rand_once () \n \n   ******************************************************************************** \n   1.a) Set up the file   \n   ********************************************************************************* . \n \n * 1.a) i) create random variables ++++++++++++++++++++++  . \n  new file. \n  input program . \n  loop n=1 to 100000 . \n  compute rv1 = rv.norm(0,1) . \n  compute rv2 = rv.norm(0,1) . \n  compute rv3 = rv.norm(0,1) . \n  compute rv4 = rv.norm(0,1) . \n  compute rv5 = rv.norm(0,1) . \n  end case . \n  end loop . \n  end file . \n  end input program . \n  execute . \n  dataset name simulation . \n \n *1.a) ii)  compute random correlation parameters ++++++++++ . \n  compute r = 0.3 . \n  compute p = 0.7 . \n  compute q = 0.3 . \n  If ($casenum=1) #t = rv.uniform(0,1) . \n  If ($casenum>1) #t = #t . \n  compute t = #t . \n  *this just sets all cases to the same random number . \n  execute . \n \n *1.a) iii)  calculate coefficients +++++++++++++++++++++++++ . \n  compute b0 = ( p*q - r)  \/  (q**2 -1)  . \n  compute b1 = ( q*r - p)  \/  (q**2 -1)  . \n  compute c  = sqrt(q) . \n  compute d  = sqrt(1-q) . \n  compute k1 = c*(b0+b1) . \n  compute k2 = d*b0 . \n  compute k3 = d*b1 . \n  compute k4 = sqrt(1-k1**2-k2**2-k3**2) . \n \n *1.a) iv)  compute simulated variables++++++++++++++++++++ . \n  compute M = c*rv1 + d*rv2 . \n  compute U  = c*rv1 + d*rv3 . \n  compute Y  = k1*rv1 + k2*rv2 + k3*rv3 + k4*rv4 . \n  compute X = (t*U + (1-t)*rv5 > 0) . \n  execute . \n \n21 \n  \n22 \n   ******************************************************************************** \n   1. b) Save outputs as 4 SPSS datasets:  \n RSQD, COEFFS, CORREL & RESULTS \n   ********************************************************************************* . \n \n *1.b) i)  Save assigned correlation parameters to 'combined' +++++++++++ . \n  DATASET DECLARE combined. \n  AGGREGATE \n     \/OUTFILE='combined' \n     \/BREAK=p \n    \/q = MEAN(q) \n    \/r = MEAN(r) \n    \/t = MEAN(t) . \n \n *1.b) ii)  Send regression output to 'rsqd' & 'coeffs' ++++++++++ . \n  DATASET DECLARE rsqd. \n  OMS \n     \/SELECT TABLES \n     \/IF COMMANDS = ['Regression'] \n           SUBTYPES = ['Model Summary' ] \n     \/DESTINATION FORMAT = SAV \n   OUTFILE = 'rsqd'  . \n  \n  DATASET DECLARE coeffs. \n  REGRESSION \n    \/MISSING LISTWISE \n    \/STATISTICS COEFF OUTS R ANOVA \n    \/CRITERIA=PIN(.05) POUT(.10) \n    \/NOORIGIN \n    \/DEPENDENT Y \n    \/METHOD=ENTER M X \n    \/OUTFILE=COVB(coeffs) . \n \n *1.b) iii)  Calculate actual correlations and output to 'correl' ++++++++++ . \n  DATASET DECLARE correl. \n  OMS \n     \/SELECT TABLES \n     \/IF COMMANDS = ['Correlations'] \n     \/DESTINATION FORMAT = SAV \n   OUTFILE = 'correl'  . \n  CORRELATIONS \n    \/VARIABLES= Y M U X  \n    \/PRINT=TWOTAIL NOSIG \n    \/MISSING=PAIRWISE . \n \n  OMSEND . \n \n *1.b) iv) Tidy up RSQD, COEFFS, CORREL  ++++++++++ . \n \n  dataset activate RSQD . \n  delete variables command_ to var1 . \n  execute. \n  rename variables (R=multR) . \n \n  dataset activate COEFFS. \n  select if (ROWTYPE_='EST') . \n  execute. \n  delete variables depvar_ to varname_ . \n \n  dataset activate CORREL. \n  select if (var2='Pearson Correlation') . \n  execute. \n  delete variables command_ to label_ var2 var3 . \n  compute s_act=U . \n  compute q_act=lag(M,1) . \n  compute r_act=lag(Y,2) . \n  compute p_act=lag(Y,1) . \n  execute . \n  select if (var1='X') . \n  execute. \n  delete variables var1 to X . \n \n  \n23 \n   ******************************************************************************** \n   1. c) Combine results together and format \n   ********************************************************************************* . \n  dataset activate combined . \n \n  MATCH FILES \/FILE=* \n   \/FILE='rsqd'. \n  EXECUTE. \n  MATCH FILES \/FILE=* \n   \/FILE='correl'. \n  EXECUTE. \n  MATCH FILES \/FILE=* \n   \/FILE='coeffs' . \n  EXECUTE. \n \n  dataset close RSQD . \n  dataset close COEFFS. \n  dataset close correl. \n  dataset close simulation . \n \n  variable labels \n    p 'assigned correlation UNMEASURED with OUTCOME' \n    q 'assigned correlation UNMEASURED with MEASURED' \n    r 'assigned correlation MEASURED with OUTCOME'   \n    t 'assigned strength of relationship of GROUP with UNMEASURED' \n    p_act 'actual correlation UNMEASURED with OUTCOME' \n    q_act 'actual correlation  UNMEASURED with MEASURED' \n    r_act 'actual correlation MEASURED with OUTCOME'   \n    s_act  'actual correlation  UNMEASURED with GROUP' \n    multR 'multiple correlation MEASURED and GROUP with OUTCOME'   \n    CONST_ 'Intercept' \n    M 'coefficient of MEASURED' \n    X 'coefficient of TREATMENT GROUP'  . \n \n  variable width p to X (6) . \n \n \n \n!ENDDEFINE . \n \n \n \n \n \n*================================================ \n2. MACRO  to repeat simulation, paste coeffs into RESULTS and save \n=================================================== . \n \nDEFINE !repeat_sim (reps=!TOKENS(1)  \n \/session=!TOKENS(1) ) \n \n !simulate_rand_once . \n  dataset name results . \n \n !DO !I = 2 !TO !reps . \n  !simulate_rand_once . \n  dataset activate results . \n  ADD FILES \/FILE=* \n    \/FILE='combined'. \n  EXECUTE. \n  dataset close combined . \n  output close all . \n !DOEND . \n \n SAVE OUTFILE=!CONCAT(\"'D:\\TEMP\\simresults\\random_session\",!session,\".sav'\") \n   \/COMPRESSED. \n \n!ENDDEFINE . \n \n \n !repeat_sim reps=50 session=373 . \n \n \n \n"}