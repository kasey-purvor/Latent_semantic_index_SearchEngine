{"doi":"10.1007\/978-3-540-70987-9_23","coreId":"177333","oai":"oai:aura.abdn.ac.uk:2164\/2130","identifiers":["oai:aura.abdn.ac.uk:2164\/2130","10.1007\/978-3-540-70987-9_23"],"title":"The Effectiveness of Personalized Movie Explanations : An Experiment Using Commercial Meta-data","authors":["Tintarev, Nava","Masthoff, Judith"],"enrichments":{"references":[{"id":188692,"title":"Automatically assessing review helpfulness. In: EMNLP","authors":[],"date":"2006","doi":null,"raw":null,"cites":null},{"id":188693,"title":"Confidence displays and training in recommender systems. In:","authors":[],"date":"2003","doi":null,"raw":null,"cites":null},{"id":188690,"title":"D.J.: An empirical study of the influence of user tailoring on evaluative argument effectiveness. In: IJCAI","authors":[],"date":"2001","doi":null,"raw":null,"cites":null},{"id":188688,"title":"Effective explanations of recommendations: Usercentered design. In: Recommender Systems","authors":[],"date":"2007","doi":null,"raw":null,"cites":null},{"id":188685,"title":"Explaining collaborative filtering recommendations. In: Computer supported cooperative work","authors":[],"date":"2000","doi":null,"raw":null,"cites":null},{"id":188684,"title":"Explaining recommendations: Satisfaction vs. promotion. In: Beyond Personalization Workshop,","authors":[],"date":"2005","doi":null,"raw":null,"cites":null},{"id":428763,"title":"Explaining recommendations. In: User Modeling","authors":[],"date":"2007","doi":null,"raw":null,"cites":null},{"id":188686,"title":"Explanation in recommender systems.","authors":[],"date":"2005","doi":null,"raw":null,"cites":null},{"id":188689,"title":"Explanations of recommendations. In: Recommender Systems","authors":[],"date":"2007","doi":null,"raw":null,"cites":null},{"id":188691,"title":"Is seeing believing?: how recommender system interfaces affect users\u2019 opinions.","authors":[],"date":"2003","doi":null,"raw":null,"cites":null},{"id":188687,"title":"User friendly recommender systems. Master\u2019s thesis,","authors":[],"date":"2006","doi":null,"raw":null,"cites":null}],"documentType":{"type":1}},"contributors":["Nejdl, Wolfgang","Kay, Judy","Pu, Pearl","Herder, Eelco","University of Aberdeen, Natural & Computing Sciences, Computing Science"],"datePublished":"2008-07-18","abstract":"Preprin","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":"Springer-Verlag","rawRecordXml":"<record><header><identifier>\n        \n            \n                oai:aura.abdn.ac.uk:2164\/2130<\/identifier><datestamp>\n                2018-01-02T00:54:54Z<\/datestamp><setSpec>\n                com_2164_673<\/setSpec><setSpec>\n                com_2164_370<\/setSpec><setSpec>\n                com_2164_331<\/setSpec><setSpec>\n                com_2164_705<\/setSpec><setSpec>\n                col_2164_674<\/setSpec><setSpec>\n                col_2164_706<\/setSpec>\n            <\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:doc=\"http:\/\/www.lyncode.com\/xoai\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n            \nThe Effectiveness of Personalized Movie Explanations : An Experiment Using Commercial Meta-data<\/dc:title><dc:creator>\nTintarev, Nava<\/dc:creator><dc:creator>\nMasthoff, Judith<\/dc:creator><dc:contributor>\nNejdl, Wolfgang<\/dc:contributor><dc:contributor>\nKay, Judy<\/dc:contributor><dc:contributor>\nPu, Pearl<\/dc:contributor><dc:contributor>\nHerder, Eelco<\/dc:contributor><dc:contributor>\nUniversity of Aberdeen, Natural & Computing Sciences, Computing Science<\/dc:contributor><dc:subject>\nQA76 Computer software<\/dc:subject><dc:subject>\nQA76<\/dc:subject><dc:description>\nPreprint<\/dc:description><dc:date>\n2011-09-19T11:49:07Z<\/dc:date><dc:date>\n2011-09-19T11:49:07Z<\/dc:date><dc:date>\n2008-07-18<\/dc:date><dc:type>\nConference item<\/dc:type><dc:identifier>\nTintarev , N & Masthoff , J 2008 , The Effectiveness of Personalized Movie Explanations : An Experiment Using Commercial Meta-data . in W Nejdl , J Kay , P Pu & E Herder (eds) , Adaptive Hypermedia and Adaptive Web-Based Systems : 5th International Conference, AH 2008, Hannover, Germany, July\/August, 2008, Proceedings . Lecture Notes in Computer Science , vol. 5149 , Springer-Verlag , Berlin, Germany , pp. 204-213 . DOI: 10.1007\/978-3-540-70987-9_23<\/dc:identifier><dc:identifier>\n3540709843<\/dc:identifier><dc:identifier>\n978-3540709848<\/dc:identifier><dc:identifier>\n0302-9743<\/dc:identifier><dc:identifier>\nPURE: 3998151<\/dc:identifier><dc:identifier>\nPURE UUID: 124c8193-f8a5-4208-adf1-3f4d87772623<\/dc:identifier><dc:identifier>\nBibtex: urn:5345fca6ad1331ce2cb9cca04dda7c87<\/dc:identifier><dc:identifier>\nScopus: 52949095792<\/dc:identifier><dc:identifier>\nhttp:\/\/hdl.handle.net\/2164\/2130<\/dc:identifier><dc:identifier>\nhttp:\/\/dx.doi.org\/10.1007\/978-3-540-70987-9_23<\/dc:identifier><dc:language>\neng<\/dc:language><dc:relation>\nAdaptive Hypermedia and Adaptive Web-Based Systems<\/dc:relation><dc:relation>\nLecture Notes in Computer Science<\/dc:relation><dc:publisher>\nSpringer-Verlag<\/dc:publisher>\n<\/oai_dc:dc>\n<\/metadata>\n        <\/record>","journals":[{"title":null,"identifiers":["0302-9743","issn:0302-9743"]}],"language":{"code":"en","id":9,"name":"English"},"relations":["Adaptive Hypermedia and Adaptive Web-Based Systems","Lecture Notes in Computer Science"],"year":2008,"topics":["QA76 Computer software","QA76"],"subject":["Conference item"],"fullText":"The E\ufb00ectiveness of Personalized Movie\nExplanations:\nAn Experiment Using Commercial Meta-data\nNava Tintarev and Judith Mastho\ufb00\nUniversity of Aberdeen,\nAberdeen, U.K.\nAbstract. This paper studies the properties of a helpful and trustwor-\nthy explanation in a movie recommender system. It discuss the results of\nan experiment based on a natural language explanation prototype. The\nexplanations were varied according to three factors: degree of personal-\nization, polarity and expression of unknown movie features. Personalized\nexplanations were not found to be signi\ufb01cantly more E\ufb00ective than non-\npersonalized, or baseline explanations. Rather, explanations in all three\nconditions performed surprisingly well. We also found that participants\nevaluated the explanations themselves most highly in the personalized,\nfeature-based condition.\n1 Introduction\nRecommender systems represent user preferences for the purpose of suggesting\nitems to purchase or examine, i.e. recommendations. Our work focuses on ex-\nplanations of recommended items [1,2,3], explaining how a user might relate to\nan item unknown to them. More concretely, we investigate explanations in the\nmovie domain with the aim of helping users make quali\ufb01ed decisions, i.e. E\ufb00ective\nexplanations. An explanation may be formulated along the lines of \u201cYou might\n(not) like Item A because...\u201d. The justi\ufb01cation following may depend on the\nunderlying recommendation algorithm (e.g. content-based, collaborative-based),\nbut could also be independent. Our approach is algorithm independent, as it\naims to explain a randomly selected item rather than the recommendation. In\nthis way we implicitly di\ufb00erentiate between explaining the way the recommen-\ndation engine works (Transparency), and explaining why the user may or may\nnot want to try an item (E\ufb00ectiveness). In addition, since items are selected ran-\ndomly the explanation can vary in polarity: being positive, neutral or negative.\nThe experiment described in this paper measures how di\ufb00erent explanations\na\ufb00ect the likelihood of trying an item (Persuasion) versus making informed deci-\nsions (E\ufb00ectiveness) and inspiring user trust (Trust). We investigate the e\ufb00ects\ndi\ufb00erent types of explanations have on these three explanation aims.\nAs in a study by Bilgic and Mooney [1], we de\ufb01ne an E\ufb00ective explanation as\none which helps the user make a correct estimate of their valuation of an item.\nPersuasion only re\ufb02ects a user\u2019s initial rating of item, but not their \ufb01nal rating\nW. Nejdl et al. (Eds.): AH 2008, LNCS 5149, pp. 204\u2013213, 2008.\nc\u00a9 Springer-Verlag Berlin Heidelberg 2008\nThe E\ufb00ectiveness of Personalized Movie Explanations 205\nafter trying it. While the user might initially be satis\ufb01ed or dissatis\ufb01ed, their\nopinion may change after exposure. As in [1], E\ufb00ectiveness can be measured by\n(1) the user rating the item on the basis of the explanation, (2) the user trying\nthe item, (3) the user re-rating the item. While it would be preferable if users\ncould actually try the item, in an experimental setting step 2 may be approx-\nimated by e.g. allowing users to read item reviews written by other users. The\nmetric suggested by [1] is optimized when the mean di\ufb00erence between the two\nratings (step 1 - step 3) is close to zero, has a low standard deviation, and there\nis a strong positive correlation between the two ratings. If an explanation helps\nusers make good decisions, getting more (accurate and balanced) information or\ntrying the item should not change their valuation of the item greatly.\nAlthough [1] did not explicitly consider the direction of skew, the di\ufb00erence\nbetween the two ratings may be either positive (over-estimation of the item) or\nnegative (under-estimation). Over-estimation may result in false positives; users\ntrying items they do not end up liking. Particularly in a high investment rec-\nommendation domain such as real-estate, a false positive is likely to result in\na large blow to trust in the system. Under-estimation may on the other hand\nlead to false negatives; users missing items they might have appreciated. If a\nuser recognizes an under-estimation due to previous knowledge or subsequent\nexposure, this may lead to a loss of trust as well. Likewise, an under-estimation\nmay needlessly decrease an e-commerce site\u2019s revenue.\n2 Factors That May Impact Explanation E\ufb00ectiveness\n2.1 Features and Personalization\nUsers like to know what it is about a particular item that makes it worthy (or\nnot) of recommendation. Bilgic and Mooney [1] did not \ufb01nd a signi\ufb01cant result\nfor E\ufb00ectiveness for an \u2018In\ufb02uence based explanation\u2019 which listed other books\npreviously rated highly by the user as in\ufb02uential for the recommendation. Other\nwork surveying a similar type of interface suggests that users would like to know\nthe explicit relationship between a recommended item and similar items used\nto form the recommendation [4]. An explanation based on item features may be\none way to do this, e.g. \u201cYou have rated books with the same author highly in\nthe past.\u201d\nUsing item features also makes it possible to personalize explanations, as\ndi\ufb00erent users may place di\ufb00erent importance on di\ufb00erent feature, and have in-\ndividual tastes with regard to these features (i.e. not everyone has the same\nfavorite actor). The seminal study by Herlocker et al. [2] on explanation inter-\nfaces shows a strong persuasive e\ufb00ect for an explanation interface referring to\na particular movie feature, namely \u201cfavorite actor or actress\u201d. This feature (fa-\nvorite actor\/actress) may be more important to some users than others since a\nhigh variance in acceptance for this type of explanation was found. Qualitative\nfeedback from focus groups also shows that users vary with regard to which\nmovie features they \ufb01nd important[5,6].\n206 N. Tintarev and J. Mastho\ufb00\nIf it is the case that some features are more important for particular users,\nit would seem plausible that explanations that tailor which features to describe\nwould be more Persuasive and E\ufb00ective than explanations with randomly selected\nfeatures, and non-feature based explanations. In the real-estate domain Carenini\nandMoore have shown that user-tailored evaluative arguments (such as \u201cthe house\nhas a good location\u201d for a user who cares about location) increase users\u2019 likelihood\nto adopt a particular house compared to non-tailored arguments [7].\nWhile similar, our work di\ufb00ers from the studies in [7] and [2], which primar-\nily considered the Persuasive power of arguments and explanations, but did not\nstudy E\ufb00ectiveness. Arguably [7] varied the polarity of the evaluative arguments,\nbut given the domain (real-estate) it was di\ufb03cult for them to consider the \ufb01nal\nvaluation of the items. Our aim is therefore to consider how user-tailoring of\nitem features can a\ufb00ect explanation E\ufb00ectiveness, Persuasion as well as Trust.\n2.2 Polarity\nAn explanation may contain both positive and negative information, and in that\nsense may have a polarity in a similar way as a numerical rating of an item. [8]\nshowed that manipulating a rating prediction can alter the user valuation of\na movie, causing either an over- or underestimation. Modifying the polarity of\nan explanation is likely to lead to a similar skew in E\ufb00ectiveness. In the study\nby Herlocker et al [2] participants were most likely to see a movie if they saw\nan explanation interface consisting of a barchart of how similar users had rated\nthe item. This bar chart had one bar for \u201cgood\u201d, a second for \u201cok\u201d and a third\nfor \u201cbad\u201d ratings. A weakness of this result is a bias toward positive ratings in\nthe used dataset1. Bilgic and Mooney [1] later showed that using this type of\nhistogram causes users to overestimate their valuation of the items (books).\nWe have analyzed online movie reviews mined from the Amazon website, to\nsee if we could distinguish the properties of reviews that are considered helpful\n[6]. We found that users were more prone to write positive reviews and that\nnegative reviews were considered signi\ufb01cantly less helpful by other users than\npositive ones. Similar correlations between item rating and review helpfulness\nwere found in other domains such as digital cameras and mobile phones [9]. All\nof this makes us consider whether negative explanations are likely to be found\nless helpful by users, or may instead help mitigate users\u2019 overly optimistic beliefs\nabout items.\n2.3 Certainty\nThe Herlocker et al study [2] considered an interface which looked at recom-\nmendation con\ufb01dence, which portrays to which degree the system has su\ufb03cient\ninformation to make a strong recommendation. Their study did not \ufb01nd that\ncon\ufb01dence displays had a signi\ufb01cant e\ufb00ect on how likely a participant was to\nsee a movie. McNee et al [10] also studied the e\ufb00ect of con\ufb01dence displays on\nuser acceptance. They found that users that were already familiar with their\n1 MovieLens - http:\/\/www.grouplens.org\/node\/12#attachments\nThe E\ufb00ectiveness of Personalized Movie Explanations 207\nrecommender system (MovieLens) were less satis\ufb01ed with the system overall af-\nter being exposed to con\ufb01dence displays. On the other hand, more experienced\nusers also found the con\ufb01dence display more valuable than new users.\nAs part of a larger study comparing di\ufb00erent types of explanation interfaces\nwe held three focus groups (comprising of 23 participants) discussing the con\ufb01-\ndence display used in [2]. We found that many participants found information\nabout con\ufb01dence displays confusing. They did not understand what to do with\nthe con\ufb01dence rating or felt that the system should not make predictions if it\nwas not con\ufb01dent. This raised the question of how lack of con\ufb01dence would\na\ufb00ect explanation E\ufb00ectiveness, Persuasion as well as Trust. In particular, we\nwere curious how users would react to missing information. In real data-sets as\nour data retrieved from Amazon\u2019s e-Commerce Service (ECS), detailed feature\nmeta-data is sometimes missing. Is it better to refrain from presenting these\nitems to users altogether, to talk about another feature which might not be\nas important to the user, or candidly state that the system is missing certain\ninformation?\n2.4 Other Factors\nE\ufb00ectiveness of explanations can also be a\ufb00ected by a number of other factors.\nIf the quality of the information used to form the recommendation or recom-\nmendation accuracy are compromised this is likely to lead to poor E\ufb00ectiveness.\nLikewise, the nature of the recommended object and presentation of the rec-\nommended items are likely to be contributing factors. While these are highly\nrelevant topics, they will not be discussed further in this paper. We conduct a\nstudy where no recommendation engine is used, in a single domain (movies),\nwith all items presented in the same manner (one stand-alone item).\n3 Experiment\nThis experiment is based on a prototype system which dynamically generates\nnatural language explanations2 for movie items based on meta-data retrieved\nfrom Amazon (ECS)3. The aim of this experiment was to see if using movie\nfeatures (e.g. lead actors\/actresses), and personalization could a\ufb00ect the E\ufb00ec-\ntiveness of explanations. We studied if explanation polarity, and clearly stating\nthat some information is missing could a\ufb00ect E\ufb00ectiveness. We also wanted to\nknow whether the e\ufb00ect was the same for Persuasion. When we help users make\ndecisions that are good for them (E\ufb00ectiveness), will they end up buying\/trying\nfewer items (Persuasion)? Likewise, we are interested in the e\ufb00ects these factors\nhave on user Trust.\n2 Realized with simpleNLG, a simple and \ufb02exible natural language generation system\ncreated by Ehud Reiter. See also http:\/\/www.csd.abdn.ac.uk\/\u223c ereiter\/simplenlg\n3 The used meta-data considers the \ufb01nding of focus groups and analysis of online\nmovie reviews [5,11,6] as well as which features are readily available via Amazon\u2019s\nECS e.g. actors, directors, genre, average rating, and certi\ufb01cation (e.g. rated PG).\n208 N. Tintarev and J. Mastho\ufb00\n3.1 Design\nFirst, participants entered their movie preferences: which genres they were in\nthe mood for, which they would not like to see, how important they found movie\nfeatures (elicited in previous studies [6]), and their favourite actors\/directors.\nThe user model in our prototype can weigh the movies\u2019 features, according to\nfeature utility as well as the participant\u2019s genre preferences.\nFifty-nine movies were pre-selected as potential recommendations to partic-\nipants. Thirty are present in the top 100 list in the Internet Movie Database\n(IMDB4) and the other twenty-nine were selected at random, but all were present\nin both the MovieLens 100.000 ratings dataset5 and Amazon.com.\nEach participant evaluated ten recommendations and explanations for movies\nselected at random from the pre-selected set. Note that the explanations tell\nthe user what they might think about the item, rather than how the item was\nselected. Moreover, these explanations di\ufb00er from explanations of recommen-\ndations as they may be negative, positive, or neutral, as the movies shown to\nthe user are selected at random. Since we did not want the users to have any\npre-existing knowledge of the movies they rated, we prompted them to request\na new recommendation and explanation if they felt they might have seen the\nmovie. Next, we followed the experimental design of [1] for each movie:\n1. Participants were shown the title and cover image of the movie and expla-\nnation, and answered the following questions:\n\u2013 How much do you think you would like this movie?\n\u2013 How good do you think the explanation is?\n2. Participants read movie reviews on Amazon.com, care was taken to di\ufb00er-\nentiate between our explanation facility and Amazon.\n3. They re-rated the movie, the explanation and their trust of our system:\n\u201cGiven everything you\u2019ve seen so far how much do you now trust the ex-\nplanation facility in this system?\u201d\nOn all questions, participants selected a value on a Likert scale from 1 (bad)\nto 7 (good), or opted out by saying they had \u201cno opinion\u201d. They could give\nqualitative comments to justify their response. In a between subjects design,\nparticipants were assigned to one of three degrees of personalization:\n1. Baseline: The explanation is neither personalized, nor describes item fea-\ntures. This is a generic explanation that could apply to anyone, e.g. \u201cThis\nmovie is one of the top 100 movies in the Internet Movie Database (IMDB).\u201d\nor \u201cThis movie is not one of the top 100 movies in the Internet Movie\nDatabase (IMDB).\u201d No additional information is supplied about the movie.\n2. Random choice, feature based: The explanation describes item features,\nbut the movie feature is selected at random, e.g. \u201cThis movie belongs to your\npreferred genre(s): Action & Adventure. On average other users rated this\nmovie 4\/5.0\u201d. The feature \u2018average rating\u2019 may not be particularly important\nto the user.\n4 http:\/\/www.imdb.com\n5 http:\/\/www.grouplens.org\/node\/12#attachments\nThe E\ufb00ectiveness of Personalized Movie Explanations 209\n3. Personalize choice, feature based: The explanation describes the item\nfeature that is most important to the participant, e.g. \u201cAlthough this movie\ndoes not belong to any of your preferred genre(s), it belongs to the genre(s):\nDocumentary. This movie stars Liam Neeson your favorite actor(s)\u201d. For\nthis user, the most important feature is leading actors.\nOur previous \ufb01ndings [11,6] suggest that genre information is important to most\nif not all users, so both the second and third condition contain a sentence regard-\ning the movie genre in a personalized way. This sentence notes that the movie\nbelongs to some of the user\u2019s disliked genres (negative polarity), preferred gen-\nres (positive polarity), or lists the genres it belongs to though they are neither\ndisliked nor preferred (neutral polarity). In negative explanations, the movie be-\nlongs to a genre the user dislikes. We do not explicitly state what the user may\nthink of the item, e.g. \u201cYou might like\/dislike this movie\u201d as this is likely to\nbias their rating. Also, there are times when Amazon is missing information.\nAn example explanation for a negative explanation with unknown information\nis: \u201cUnfortunately this movie belongs to at least one genre you do not want to\nsee: Horror. Director information is unknown.\u201d. Seventeen movies lack director\ninformation and their explanations explicitly state that this is missing.\nAlso, a movie may star one of the user\u2019s favorite actors or director in which\ncase this will also be mentioned as a \u201cfavorite\u201d, e.g. \u201cThis movie starts Ben\nKingsley, Ralph Fiennes and Liam Neeson your favorite actor(s).\u201d\nFifty-one students and university sta\ufb00 participated in the experiment. Of\nthese, \ufb01ve were removed based on users\u2019 comments suggesting that they had ei-\nther rated movies for which they had a pre-existing opinion, or Amazon\u2019s reviews\ninstead of our explanations. Of the remaining, 25 were male, 21 female and the\naverage age was 26.5. Participants were roughly equally distributed among the\nthree conditions (14, 17 and 15 respectively).\nWe hypothesize that personalized feature based explanationswill bemoreE\ufb00ec-\ntive than randomchoice feature based explanations and the baseline explanations.\n3.2 Results and Discussion\nTable 1 summarizes the means of all the recorded values.\nTable 1. Means (and StDev) of user ratings and percentage \u201cno opinions\u201d. First rat-\nings are given after viewing the explanation, second ratings after viewing the Amazon\nreviews.\nCondition Movie\nrating1\nMovie\nrating2\nExplanation\nrating1\nExplanation\nrating2\nTrust\nBaseline 3.45 (1.26)\n8.8%\n4.11 (1.85)\n0%\n2.38 (1.54)\n2.2%\n2.85 (1.85) 0% 2.69 (1.94)\n0.7%\nRandom choice 3.85 (1.87)\n7.2%\n4.43 (2.02)\n3.6%\n2.50 (1.62)\n3.0%\n2.66 (1.89)\n3.0%\n2.56 (1.74)\n3.6%\nPersonalized 3.61 (1.65)\n3.1%\n4.37 (1.93)\n0.6%\n3.09 (1.70)\n0.6%\n3.14 (1.99) 0% 2.91 (1.60)\n1.3%\n210 N. Tintarev and J. Mastho\ufb00\nFig. 1. First and second movie ratings - the distribution is considered with regard to\npercentage of ratings in each condition\nEnough to Form an Opinion? Since our explanations are very short we \ufb01rst\nconsidered whether they were su\ufb03cient for the user to form an opinion of the\nmovie. In Table 1 we note the percentage of no-opinions in each condition. We\nsee that this is small though perhaps not negligible. The percentage for the \ufb01rst\nmovie as well as for the \ufb01rst explanation is smallest in the personalized condition.\nIn Figure 1 we consider the actual ratings of the movie. We see that the \ufb01rst\nand second rating of the movie are distributed beyond the mean rating of 4,\nsuggesting that participants are able to form polarized opinions.\nAre Personalized Explanations More E\ufb00ective? Next we considered Ef-\nfectiveness. Similar to the metric described by [1] we consider the mean of the\ndi\ufb00erence between the two movie ratings. Unlike [1] (who considered the signed\nvalues) we consider the absolute, or unsigned, di\ufb00erence between the two ratings\nin Table 2. Independent samples t-tests show no signi\ufb01cant di\ufb00erence between the\nmeans of the three conditions. This suggests that the degree of personalization\nor using item features does not increase explanation E\ufb00ectiveness.\nFigure 2 graphically depicts the signed distribution of E\ufb00ectiveness. We see\nhere that under-estimation is more frequent than overestimation in all three\nconditions. We also note the peak at zero in the random choice, feature based\nTable 2. E\ufb00ectiveness over absolute values with \u201cno-opinions\u201d omitted, and Pearson\u2019s\ncorrelations between the two movie ratings\nCondition m (StDev) Correlation p\nBaseline 1.38 (1.20) 0.427 0.000\nRandom choice 1.14 (1.30) 0.650 0.000\nPersonalized 1.40 (1.21) 0.575 0.000\nThe E\ufb00ectiveness of Personalized Movie Explanations 211\nFig. 2. Distribution of (signed) E\ufb00ectiveness - \u201cno opinions\u201d omitted\ncondition. Around 40% of explanations in this condition are perfectly E\ufb00ective,\ni.e. the di\ufb00erence between the two ratings is zero.\nWe investigated this further and found that the random choice condition has\nsigni\ufb01cantly higher initial ratings than the other two conditions. We compared\nthis condition with the personalized condition to see if there was any factor that\ncould cause this. The percentage of shown movies that were in the top 100 in\nIMDB was comparable, and the distribution of movie titles did not show an\nevident skew. In the personalized condition most participants chose \u201cactors\u201d as\ntheir most preferred movie feature (75%) while the participants in the random\nchoice condition received explanations describing the four movie features in fairly\nequal proportions. The explanations in the random choice condition have fewer\nmentions of favorite actors and directors, more explanations with unknown in-\nformation, and fewer movies in the participants\u2019 preferred genres than in the\npersonalized condition. All of this, except the di\ufb00erence in features mentioned,\nwould be expected to lead to a lower initial rating rather than the found higher\nrating. With regards to features, we speculated that the di\ufb00erence may be due\nto the feature \u201caverage rating by other users\u201d being mentioned more often, as we\nobserved a positive bias of average ratings on the Amazon website. However, we\nfound that mentioning this feature correlated more with low ratings of movies.\nSo, we have not yet found a satisfactory explanation.\nSince [1] did not consider the sign of the di\ufb00erence between the two ratings,\ntheir metric of E\ufb00ectiveness also requires that the two ratings are correlated.\nThis correlation is still interesting for our purposes. Table 2 shows a signi\ufb01cant\nand positive correlation between these two ratings for all three conditions. That\nis, explanations in all three conditions perform surprisingly well.\nExplanations and User Satisfaction. In Table 1 we see that the second set\nof explanation ratings are higher than the \ufb01rst. This may be partly due to some\nparticipants confounding our explanations with the Amazon reviews, thus rating\nour explanation facility higher. The mean rating for trust and explanations is\nlow overall, but users rate the \ufb01rst explanation rating signi\ufb01cantly highest in the\npersonalized condition (independent sample t-tests, p<0.001). This suggests that\nwhile the personalized explanations may not help users make better decisions,\nusers may still be more satis\ufb01ed. This is con\ufb01rmed by the qualitative comments.\nParticipants in the personalized condition appreciated when their preferred fea-\nture was mentioned: \u201c...explanation lists main stars, which attracts me a little\nto watch the movie...\u201d. Participants felt that vital information was missing in\n212 N. Tintarev and J. Mastho\ufb00\nparticular in the random choice condition: \u201c...I indicated that Stanley Kubrick is\none of my favorite directors in one of the initial menus but the explanation didn\u2019t\ntell me he directed this. That would have piqued my interest. The explanation\ndidn\u2019t have this important detail so a loss of trust happened here...\u201d Another\nparticipant in the random choice condition had set actors as the most impor-\ntant feature and left the following comment for an explanation with information\nabout the director: \u201c...not much useful information in the explanation - I do not\nknow many directors, so do not really care who directs a movie.\u201d. In contrast,\nparticipants in the baseline condition expressed that they were dissatis\ufb01ed with\nthe explanation: \u201cNot very helpful explanation even if it is top 100...\u201d\nTrust, Classi\ufb01cation and Completeness. In Table 1 we see that the mean\ntrust is low in all three conditions, but seems best in the personalized feature\nbased condition. Many participants felt that the genres were misclassi\ufb01ed, and\nthat this reduced their trust in the explanation facility. Although the genre clas-\nsi\ufb01cation is automatically retrieved from the Amazon ECS there are two things\nwe could change in our explanations to mitigate these e\ufb00ects. In our prototype\nwhen a movie belongs to any of the users\u2019 favorite genres, only preferred genres\nare mentioned in the explanation even if the movie belongs to other genres as\nwell. Similarly for disliked genres, only these are mentioned. A \ufb01rst improvement\nwould be to mention all the genres a movie belongs to. Secondly, the genre expla-\nnations can be improved by considering even more detailed genre speci\ufb01cation\nsuch as \u201cPeriod Drama\u201d rather than just \u201cDrama\u201d for costume dramas.\nWe received similar feedback for actors, where we only mention the user\u2019s\nfavorite actor in the explanation: \u201cBenicio del Toro is in it, but so are others\nwho aren\u2019t listed and who I really like...\u201d. That is, users might like to hear the\nnames of all the leading actors even if only one is known to be their favorite.\nCertainty and Polarity. None of the seven users for which director infor-\nmation was missing noted this, nor were there any explicit complaints about\nnegative explanations where the movie belonged to a genre the user did not like.\n4 Conclusions\nIn all three conditions participants largely have an opinion of the movie, and in\nall conditions there was more underestimation than overestimation. The mean\nE\ufb00ectiveness deviated ca 1.5 from the optimum discrepancy of zero on a 7 point\nscale (StD < 1.5), regardless of the degree of personalization or whether or not\nthe explanation used features such as actors. In light of this under-estimation\nwe reconsider the fact that movie ratings in general, and their Amazon reviews\nin particular, tend to lean toward positive ratings. If Amazon reviews are overly\npositive, this may have a\ufb00ected our results.\nSince there is no signi\ufb01cant di\ufb00erence between conditions w.r.t. E\ufb00ectiveness\nwe consider the factors that the three conditions share, which is that they all\nexpose the participant to the movie title and movie cover. A number of partici-\npants justify their ratings in terms of the image in their qualitative comments,\nThe E\ufb00ectiveness of Personalized Movie Explanations 213\nin particular for the baseline explanation. So it is fair to assume that at least\nsome participants use the image to form their judgment.\nWe are now planning a repeated experiment accounting for the factors dis-\ncussed in this paper. Firstly, the experiment will consider explanations without\nimages. Secondly, explanations regarding genre and actor will be more detailed\nand complete. Thirdly, a clearer distinction will be made between the personal-\nized and random choice condition. Explanations in the random choice condition\nwill describe all the genres of the movie, but not relate them to the user\u2019s prefer-\nences. Likewise it will list all the lead actors, and the director, but will not relate\nwhether they are the user\u2019s favorites. We will consider alternative sources for\napproximating the user\u2019s true evaluation of the item, or repeat the experiment\nin a domain which does not have as strong a positive bias as Amazon. A \ufb01nal\nevaluation in which participants will view the movie they rate is also planned.\nAcknowledgments. This research was partially supported by the EPSRC plat-\nform grant (EP\/E011764\/1).\nReferences\n1. Bilgic, M., Mooney, R.J.: Explaining recommendations: Satisfaction vs. promotion.\nIn: Beyond Personalization Workshop, IUI (2005)\n2. Herlocker, J.L., Konstan, J.A., Riedl, J.: Explaining collaborative \ufb01ltering recom-\nmendations. In: Computer supported cooperative work (2000)\n3. Mcsherry, D.: Explanation in recommender systems. Arti\ufb01cial Intelligence Re-\nview 24(2), 179\u2013197 (2005)\n4. Hingston, M.: User friendly recommender systems. Master\u2019s thesis, Sydney Uni-\nversity (2006)\n5. Tintarev, N., Mastho\ufb00, J.: E\ufb00ective explanations of recommendations: User-\ncentered design. In: Recommender Systems (2007)\n6. Tintarev, N.: Explanations of recommendations. In: Recommender Systems (2007)\n7. Carenini, G., Moore, D.J.: An empirical study of the in\ufb02uence of user tailoring on\nevaluative argument e\ufb00ectiveness. In: IJCAI (2001)\n8. Cosley, D., Lam, S.K., Albert, I., Konstan, J.A., Riedl, J.: Is seeing believing?: how\nrecommender system interfaces a\ufb00ect users\u2019 opinions. In: CHI (2003)\n9. Kim, S.M., Pantel, P., Chklovski, T., Pennacchiotti, M.: Automatically assessing\nreview helpfulness. In: EMNLP (2006)\n10. McNee, S., Lam, S.K., Guetzla\ufb00, C., Konstan, J.A., Riedl, J.: Con\ufb01dence displays\nand training in recommender systems. In: INTERACT IFIP TC13 (2003)\n11. Tintarev, N.: Explaining recommendations. In: User Modeling (2007)\n"}