{"doi":"10.1098\/rspb.2006.3578","coreId":"136126","oai":"oai:bradscholars.brad.ac.uk:10454\/3564","identifiers":["oai:bradscholars.brad.ac.uk:10454\/3564","10.1098\/rspb.2006.3578"],"title":"Resolving multisensory conflict: a strategy for balancing the costs and benefits of audio-visual integration.","authors":["Roach, N.W.","Heron, James","McGraw, Paul V."],"enrichments":{"references":[],"documentType":{"type":null}},"contributors":[],"datePublished":"2006","abstract":"NoIn order to maintain a coherent, unified percept of the external environment, the brain must continuously combine information encoded by our different sensory systems. Contemporary models suggest that multisensory integration produces a weighted average of sensory estimates, where the contribution of each system to the ultimate multisensory percept is governed by the relative reliability of the information it provides (maximum-likelihood estimation). In the present study, we investigate interactions between auditory and visual rate perception, where observers are required to make judgments in one modality while ignoring conflicting rate information presented in the other. We show a gradual transition between partial cue integration and complete cue segregation with increasing inter-modal discrepancy that is inconsistent with mandatory implementation of maximum-likelihood estimation. To explain these findings, we implement a simple Bayesian model of integration that is also able to predict observer performance with novel stimuli. The model assumes that the brain takes into account prior knowledge about the correspondence between auditory and visual rate signals, when determining the degree of integration to implement. This provides a strategy for balancing the benefits accrued by integrating sensory estimates arising from a common source, against the costs of conflating information relating to independent objects or events","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":null,"rawRecordXml":"<record><header><identifier>\n        \n            \n                oai:bradscholars.brad.ac.uk:10454\/3564<\/identifier><datestamp>\n                2016-08-18T17:15:30Z<\/datestamp><setSpec>\n                com_10454_152<\/setSpec><setSpec>\n                col_10454_6342<\/setSpec>\n            <\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:doc=\"http:\/\/www.lyncode.com\/xoai\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n            \nResolving multisensory conflict: a strategy for balancing the costs and benefits of audio-visual integration.<\/dc:title><dc:creator>\nRoach, N.W.<\/dc:creator><dc:creator>\nHeron, James<\/dc:creator><dc:creator>\nMcGraw, Paul V.<\/dc:creator><dc:subject>\nMultisensory integration,<\/dc:subject><dc:subject>\nAudio-visual conflict<\/dc:subject><dc:subject>\nBayesian modelling<\/dc:subject><dc:description>\nNo<\/dc:description><dc:description>\nIn order to maintain a coherent, unified percept of the external environment, the brain must continuously combine information encoded by our different sensory systems. Contemporary models suggest that multisensory integration produces a weighted average of sensory estimates, where the contribution of each system to the ultimate multisensory percept is governed by the relative reliability of the information it provides (maximum-likelihood estimation). In the present study, we investigate interactions between auditory and visual rate perception, where observers are required to make judgments in one modality while ignoring conflicting rate information presented in the other. We show a gradual transition between partial cue integration and complete cue segregation with increasing inter-modal discrepancy that is inconsistent with mandatory implementation of maximum-likelihood estimation. To explain these findings, we implement a simple Bayesian model of integration that is also able to predict observer performance with novel stimuli. The model assumes that the brain takes into account prior knowledge about the correspondence between auditory and visual rate signals, when determining the degree of integration to implement. This provides a strategy for balancing the benefits accrued by integrating sensory estimates arising from a common source, against the costs of conflating information relating to independent objects or events.<\/dc:description><dc:date>\n2009-09-30T14:00:50Z<\/dc:date><dc:date>\n2009-09-30T14:00:50Z<\/dc:date><dc:date>\n2006<\/dc:date><dc:type>\nArticle<\/dc:type><dc:type>\nNo full-text available in the repository<\/dc:type><dc:identifier>\nRoach, N.W., Heron, J. and McGraw, P.V. (2006).  Resolving multisensory conflict: a strategy for balancing the costs and benefits of audio-visual integration.   Proceedings of the Royal Society of London B: Boiological Sciences. Vol. 273, No. 1598, pp. 2159-2168.<\/dc:identifier><dc:identifier>\n90011766<\/dc:identifier><dc:identifier>\nhttp:\/\/hdl.handle.net\/10454\/3564<\/dc:identifier><dc:language>\nen<\/dc:language><dc:relation>\nhttp:\/\/dx.doi.org\/10.1098\/rspb.2006.3578<\/dc:relation>\n<\/oai_dc:dc>\n<\/metadata>\n        <\/record>","journals":null,"language":null,"relations":["http:\/\/dx.doi.org\/10.1098\/rspb.2006.3578"],"year":2006,"topics":["Multisensory integration,","Audio-visual conflict","Bayesian modelling"],"subject":["Article","No full-text available in the repository"],"fullText":null}