{"doi":"10.1007\/s00779-008-0206-3","coreId":"70131","oai":"oai:eprints.lancs.ac.uk:13043","identifiers":["oai:eprints.lancs.ac.uk:13043","10.1007\/s00779-008-0206-3"],"title":"Supporting Device Discovery and Spontaneous Interaction with Spatial References","authors":["Gellersen, Hans","Fischer, Carl","Guinard, Dominique","Gostner, Roswitha","Kortuem, Gerd","Kray, Christian","Rukzio, Enrico","Streng, Sara"],"enrichments":{"references":[{"id":16329872,"title":"A relative positioning system for co-located mobile devices.","authors":[],"date":"2005","doi":"10.1145\/1067170.1067190","raw":"Hazas M, Kray C, Gellersen H, Agbota H, Kortuem G, and Krohn A. 2005. A relative positioning system for co-located mobile devices. In Proceedings of the 3rd international Conference on Mobile Systems, Applications, and Services (Seattle, USA, June 2005). MobiSys '05. 177-190.","cites":null},{"id":16329882,"title":"An Experimental Comparison of Physical Mobile Interaction Techniques: Touching, Pointing and Scanning.","authors":[],"date":"2006","doi":"10.1007\/11853565_6","raw":"Rukzio E, Leichtenstern K, Callaghan V, Holleis P, Schmidt A, Chin J. 2006. An Experimental Comparison of Physical Mobile Interaction Techniques: Touching, Pointing and Scanning. In Proc. Ubicomp 2006, 87-104.","cites":null},{"id":16329867,"title":"ARIS: an interface for application relocation in an interactive space.","authors":[],"date":"2004","doi":null,"raw":"Biehl JT and Bailey BP. 2004. ARIS: an interface for application relocation in an interactive space. In Proc. of the 2004 Conference on Graphics interface (London, Ontario, Canada, May 2004), 107-116.","cites":null},{"id":16329883,"title":"Context-aware computing applications.","authors":[],"date":"1994","doi":"10.1109\/wmcsa.1994.16","raw":"Schilit, BN, Adams, NI and Want, R. 1994. Context-aware computing applications. In Proceedings of Workshop onMobile Computing Systems and Applications (WMCSA), pages 85\u201390, Santa Cruz, CA, USA, December 1994. IEEE Computer Society.","cites":null},{"id":16329878,"title":"Designing for serendipity: supporting end-user configuration of ubiquitous computing environments.","authors":[],"date":"2002","doi":"10.1145\/778712.778736","raw":"Newman, MW, Sedivy JZ, Neuwirth CM, Edwards WK, Hong JI, Izadi S, Marcelo K and Smith TF. 2002. Designing for serendipity: supporting end-user configuration of ubiquitous computing environments. In Proceedings of the Conference on Designing interactive Systems: Processes, Practices, Methods, and Techniques (London, England, June 25 - 28, 2002). DIS '02. 147-156.","cites":null},{"id":16329870,"title":"Discovery Systems in Ubiquitous Computing.","authors":[],"date":"2006","doi":"10.1109\/mprv.2006.28","raw":"Edwards WK. 2006. Discovery Systems in Ubiquitous Computing. IEEE Pervasive Computing 5, 2 (Apr. 2006), 70-77.","cites":null},{"id":16329871,"title":"Extending Mobile Devices with Spatially Arranged Gateways to Pervasive Services.","authors":[],"date":"2007","doi":null,"raw":"Guinard D, Gellersen H, Streng S. 2007. Extending Mobile Devices with Spatially Arranged Gateways to Pervasive Services. Proc. International Workshop on Pervasive Mobile Interaction Devices (PERMID 2007).","cites":null},{"id":16329869,"title":"Geney: Designing a collaborative activity for the palm handheld computer.","authors":[],"date":"2001","doi":"10.1145\/365024.365303","raw":"Danesh A, Inkpen K, Lau F, Shu K, and Booth K. 2001. Geney: Designing a collaborative activity for the palm handheld computer. In Proceedings of CHI, Conference on Human Factors in Computing Systems (CHI 2001), pages 388\u2013395.","cites":null},{"id":16329866,"title":"Halo: a technique for visualizing off-screen objects.","authors":[],"date":"2003","doi":"10.1145\/642611.642695","raw":"Baudisch, P. and Rosenholtz, R. 2003. Halo: a technique for visualizing off-screen objects. In Proc. SIGCHI Conference on Human Factors in Computing Systems (CHI '03), 481-488.","cites":null},{"id":16329879,"title":"ICrafter: A Service Framework for Ubiquitous Computing Environments.","authors":[],"date":"2001","doi":"10.1007\/3-540-45427-6_7","raw":"Ponnekanti S, Lee B, Fox A, Hanrahan P and Winograd W. 2001. ICrafter: A Service Framework for Ubiquitous Computing Environments. Proc. of the 3rd international conference on Ubiquitous Computing (Atlanta, Georgia, USA, September 2001), 56-75.","cites":null},{"id":16329865,"title":"Implementing a Sentient Computing System.","authors":[],"date":"2001","doi":"10.1109\/2.940013","raw":"Addlesee M, Curwen R, Hodges S, Newman J, Steggles P, Ward A and Hopper A. 2001. Implementing a Sentient Computing System. Computer 34, 8 (Aug. 2001), 50-56.","cites":null},{"id":16329875,"title":"People, places, things: web presence for the real world.","authors":[],"date":"2002","doi":"10.1109\/mcsa.2000.895378","raw":"Kindberg T, Barton J, Morgan J, Becker G, Caswell D, Debaty P, Gopal G, Frid M, Krishnan V, Morris H, Schettino J, Serra B, and Spasojevic M. 2002. People, places, things: web presence for the real world. Mob. Netw. Appl. 7, 5 (Oct. 2002), 365-376.","cites":null},{"id":16329880,"title":"Pick-and-drop: a direct manipulation technique for multiple computer environments,","authors":[],"date":"1997","doi":"10.1145\/263407.263505","raw":"Rekimoto J. 1997. Pick-and-drop: a direct manipulation technique for multiple computer environments, Proceedings of the 10th annual ACM symposium on User interface software and technology, October 14-17, 1997, Banff, Alberta, Canada, 31-39.","cites":null},{"id":16329881,"title":"Proximal Interactions: A Direct Manipulation Technique for Wireless Networking.","authors":[],"date":"2003","doi":null,"raw":"Rekimoto J, Ayatsuka Y, Kohno M, and Oba H. 2003. Proximal Interactions: A Direct Manipulation Technique for Wireless Networking. In Proc. of Interact'2003, 511--518.","cites":null},{"id":16329877,"title":"Security by spatial reference: Using relative positioning to authenticate devices for spontaneous interaction.","authors":[],"date":"2007","doi":"10.1007\/978-3-540-74853-3_12","raw":"Mayrhofer R, Gellersen H, and Hazas M. 2007. Security by spatial reference: Using relative positioning to authenticate devices for spontaneous interaction. In Proc. Ubicomp 2007: 9th International Conference on Ubiquitous Computing (Innsbruck, Austria, Sept. 2007). 199-216.","cites":null},{"id":16329876,"title":"Sensing and visualizing spatial relations of mobile devices.","authors":[],"date":"2005","doi":"10.1145\/1095034.1095049","raw":"Kortuem G, Kray C and Gellersen H. 2005. Sensing and visualizing spatial relations of mobile devices. In Proc. of the 18th Annual ACM Symposium on User interface Software and Technology (Seattle, WA, USA, October 23 - 26, 2005). UIST '05. ACM Press, 93-","cites":null},{"id":16329884,"title":"Show me the way to Monte Carlo: density-based trajectory navigation,","authors":[],"date":null,"doi":"10.1145\/1240624.1240812","raw":"S. Strachan, J. Williamson, R. Murray-Smith, Show me the way to Monte Carlo: density-based trajectory navigation, Proceedings of ACM SIG CHI Conference, San Jose,","cites":null},{"id":16329873,"title":"Synchronous gestures for multiple users and computers.","authors":[],"date":"2003","doi":"10.1145\/964696.964713","raw":"Hinckley K. 2003. Synchronous gestures for multiple users and computers. In Proceedings of UIST 2003, 149\u2013158.","cites":null},{"id":16329874,"title":"System Software for Ubiquitous Computing.","authors":[],"date":"2002","doi":"10.1109\/mprv.2002.993146","raw":"Kindberg T and Fox A. 2002. System Software for Ubiquitous Computing. IEEE Pervasive Computing 1, 1 (Jan. 2002), 70-81.","cites":null},{"id":16329885,"title":"u-photo: Interacting with pervasive services using digital still images.","authors":[],"date":"2005","doi":"10.1007\/11428572_12","raw":"Suzuki G, Aoki S, Iwamoto T, Maruyama D, Koda T, Kohtake N, Takashio K, and Tokuda H. 2005. u-photo: Interacting with pervasive services using digital still images. In Proc. Pervasive 2005, 190\u2013207.","cites":null},{"id":16329868,"title":"Ubiquitous computing and the role of geometry.","authors":[],"date":"2000","doi":"10.1109\/98.878536","raw":"Brumitt B, Krumm J, Meyers B and Shafer S. 2000. Ubiquitous computing and the role of geometry. IEEE Personal Communications, pages 41\u201343, October 2000. 16","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2009","abstract":"The RELATE interaction model is designed to support spontaneous interaction of mobile users with devices and services in their environment. The model is based on spatial references that capture the spatial relationship of a user\u2019s device with other co-located devices. Spatial references are obtained by relative position sensing and integrated in the mobile user interface to spatially visualize the arrangement of discovered devices, and to provide direct access for interaction across devices. In this paper we discuss two prototype systems demonstrating the utility of the model in collaborative and mobile settings, and present a study on usability of spatial list and map representations for device selection","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/70131.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/13043\/1\/gellersen08supporting.pdf","pdfHashValue":"4f95962a19e8e54351ddc3eac535e68b01de9d95","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:13043<\/identifier><datestamp>\n      2018-01-24T02:23:41Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Supporting Device Discovery and Spontaneous Interaction with Spatial References<\/dc:title><dc:creator>\n        Gellersen, Hans<\/dc:creator><dc:creator>\n        Fischer, Carl<\/dc:creator><dc:creator>\n        Guinard, Dominique<\/dc:creator><dc:creator>\n        Gostner, Roswitha<\/dc:creator><dc:creator>\n        Kortuem, Gerd<\/dc:creator><dc:creator>\n        Kray, Christian<\/dc:creator><dc:creator>\n        Rukzio, Enrico<\/dc:creator><dc:creator>\n        Streng, Sara<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        The RELATE interaction model is designed to support spontaneous interaction of mobile users with devices and services in their environment. The model is based on spatial references that capture the spatial relationship of a user\u2019s device with other co-located devices. Spatial references are obtained by relative position sensing and integrated in the mobile user interface to spatially visualize the arrangement of discovered devices, and to provide direct access for interaction across devices. In this paper we discuss two prototype systems demonstrating the utility of the model in collaborative and mobile settings, and present a study on usability of spatial list and map representations for device selection.<\/dc:description><dc:date>\n        2009<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:relation>\n        http:\/\/dx.doi.org\/10.1007\/s00779-008-0206-3<\/dc:relation><dc:identifier>\n        Gellersen, Hans and Fischer, Carl and Guinard, Dominique and Gostner, Roswitha and Kortuem, Gerd and Kray, Christian and Rukzio, Enrico and Streng, Sara (2009) Supporting Device Discovery and Spontaneous Interaction with Spatial References. Personal and Ubiquitous Computing. pp. 255-264. ISSN 1617-4909<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/13043\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1007\/s00779-008-0206-3","http:\/\/eprints.lancs.ac.uk\/13043\/"],"year":2009,"topics":["QA75 Electronic computers. Computer science"],"subject":["Journal Article","PeerReviewed"],"fullText":"1 \nSupporting Device Discovery and Spontaneous \nInteraction with Spatial References \nHans Gellersen1, Carl Fischer1,Dominique Guinard2, Roswitha Gostner1, Gerd \nKortuem1, Christian Kray3, Enrico Rukzio1 and Sara Streng4 \n1Computing Department, Lancaster University, Lancaster, U.K. \n2ETH Zurich \/ SAP Research, Zurich, Switzerland  \n4Informatics Research Institute, University of Newcastle, U.K. \n3Ludwig-Maximilians-Universit\u00e4t, Munich, Germany \nThe RELATE interaction model is designed to support spontaneous interaction of mobile users \nwith devices and services in their environment. The model is based on spatial references that \ncapture the spatial relationship of a user\u2019s device with other co-located devices. Spatial references \nare obtained by relative position sensing and integrated in the mobile user interface to spatially \nvisualize the arrangement of discovered devices, and to provide direct access for interaction across \ndevices. In this paper we discuss two prototype systems demonstrating the utility of the model in \ncollaborative and mobile settings, and present a study on usability of spatial list and map \nrepresentations for device selection. \nSpontaneous interaction, spatial user interface, relative positioning. \nIntroduction \nSpontaneous interaction is a central characteristic associated with mobile and \nubiquitous computing [10]. The principal idea of spontaneous interaction is to \nenable mobile users to associate their personal devices with devices encountered \nin their environment, in order to take advantage of serendipitous interaction \nopportunities [15]. Archetypal examples for spontaneous interaction include use \nof a printer in an unknown environment, interaction with public displays, and data \nexchange between mobile users. The chief concern in developing technologies \nthat support spontaneous interaction is to minimize the effort for discovery of \ninteraction opportunities, for establishing a connection to encountered devices, \nand for the actual interaction across devices.  \nMobile devices are now routinely equipped with wireless networking capability, \noften supporting a variety of technologies to facilitate spontaneous connection \nwith other devices, through widely deployed wireless infrastructures (e.g. WLAN, \nGPRS) as well as through direct peer-to-peer channels (e.g. Bluetooth, Infrared). \nMoreover, many discovery systems have been developed that let devices and \nservices become aware of peers on the network, i.e. aware of their availability and \ntheir capability (Jini, UPnP, etc.) [6]. These advances address spontaneous \ninteraction as an infrastructure challenge, however it is critical to recognise that \ndiscovery and interaction with encountered devices is also a significant user \ninterface and interaction design challenge [11,15]. Discovery systems help mobile \ndevices find and access peer devices, but it remains difficult for their users to \nunderstand: what devices and services are available in their environment; how \nnetwork entities found by their device relate to encountered physical entities; and \nhow the intended interaction can be performed. \nA central problem, from the perspective of a mobile user, is the identification of \ntarget devices for spontaneous interaction. A user will seek to engage in \nspontaneous interaction either by searching for a device that is able to provide a \ndesired service (e.g. a printer), or by searching for a service representing a \nphysically encountered device (e.g. the device of another user). In the first case, \n2 \nthe problem is that devices found on the network will be identified in network \nterms, i.e. by a name and address geared toward their unique identification and \nlocalization in the network as opposed to in the real world. For a user it is not \nstraightforward to map such devices names to actual devices in their environment, \neven if descriptive names are used (\u201cJoe\u2019s Laptop\u201d, \u201cPrinter on D floor\u201d). The \nsecond case poses the inverse problem: a device physically identified for \ninteraction, such as a device in front of the user, does not readily give away how it \nis identified on the network, and how it can be accessed for interaction. \nIn this paper we present work on a spatial interaction model developed to address \nthe problem of device identification for spontaneous interaction. The RELATE \ninteraction model builds on the following principle, as illustrated in Figure 1: \nfrom the perspective of a mobile device, the relative positions of potential target \ndevices are determined, and reflected in the mobile user interface in the form of \nspatial references. Spatial references thus capture the spatial relationship of a \nclient device (the user\u2019s device) with target devices in a visual presentation to the \nuser, for matching what their device discovers on the network with what they see \n\u201cin front of them\u201d. The spatial references are integrated on the user\u2019s device as \nuser interface objects, to further facilitate the use of direct manipulation \ntechniques for interaction with target devices.  \nThe RELATE interaction model can be implemented with any location system \nthat is sufficiently accurate to track spatial relationships of devices surrounding a \nmobile user. Many such systems have been proposed and developed, for instance \nbased on computer vision [4] and ultrasonic tracking [1]. These systems require a \ndeployed infrastructure in order to support device positioning. To overcome \ninfrastructure-dependence, we have, in previously published work, introduced a \nsystem for relative positioning of devices in a peer-to-peer manner [9]. In prior \nwork we have focussed on characterisation of relative positioning accuracy with \n \n \nFig. 1 The RELATE model for spatial interaction is based on relative positioning of potential target \ndevices near a mobile user, and provision of corresponding spatial references in the user\u2019s device \n3 \nRELATE [9], and introduced widgets for development of spatially aware user \ninterfaces [13].  \nIn this paper, we focus on the utility of spatial references for device discovery and \nspontaneous interaction, with three contributions:  \n\u2022 A review of earlier work in which we demonstrated RELATE in the \ncontext of collaboration support for co-located mobile users. \n\u2022 Follow-on research introducing Relate Gateways, a user interface design \nin which interaction shortcuts to nearby devices are arranged as \u2018gateways\u2019 \naround the edge of a mobile user\u2019s device. \n\u2022 A controlled study on usage of spatial information for selection of co-\nlocated devices. \nRelated Work \nEfforts in context-aware and ubiquitous computing over the last years have \nfocused on making knowledge about the physical world available to mobile \ncomputer systems, and spatial knowledge has been of particular concern. As \nobserved by Brumitt et al., the addition of basic geometric knowledge has the \npotential to greatly increase the shared understanding between user and system \n[4]. Use of geo-referenced data in conjunction with spatially aware handheld \ndevices has become a widely investigated topic [7]. Mobile spatial interaction is \nexplored for instance for navigation and wayfinding, access to place-specific \ninformation, and mobile augmented reality; the specific focus our work is the \nfacilitation of spontaneous interaction across co-located devices. \nLocation-awareness for mobile devices has been investigated widely, with focus \non development of infrastructures that track absolute positions of devices, or \nenable devices to directly compute their position on the basis of signals emitted by \nthe infrastructure (e.g. [1, 20]). The RELATE work in contrast is focussed on \nrelative location of devices surrounding a user:  the rationale is that spatial \nrelations rather than absolute positions help a user make sense of devices arranged \naround them. Spatial sensing is used for discovery of near-by devices and services \n(as opposed discovery based on network topology): in this respect our approach is \nrelated to physical discovery mechanisms; these include use of near-field \ncommunication for proximal interaction [18], and of beacons and tags for physical \nidentification of interaction opportunities [12,19]. \nIn our interaction model, relative positions of nearby devices are presented to the \nuser through a visualisation that exposes relative spatial arrangement. In related \nwork, world-in-miniature visualisations have been used to show devices present in \ninteractive spaces, to support interaction and relocation of applications [3,16]. An \ninteractive approach to obtain visual shortcuts to services embedded in an \nenvironment has been developed in the uPhoto system, in which users are \nprovided with a \u201ccamera\u201d to capture images that expose embedded hotlinks to \nservices in the photographed scene [22]. Other user interfaces for user-centric \ndiscovery and association of services have integrated more coarse-grained \nlocation information (e.g. sorting device lists by proximity [20], and browsing \nservices by room location [15]). Though not concerned with cross-device \ninteraction, Halo is a visualisation technique closely related to our work, as it is \nconcerned with indicating the location of off-screen targets [2]; in Halo this is \n4 \nachieved with drawing rings around the target and reaching into the visible screen \narea. \nSpatial references in RELATE not only visualise potential target devices, but also \nsupport direct access across devices. Related techniques for cross-device \ninteraction include: Pick-and-drop, allowing users to pick up an object on one \ncomputer with a stylus and drop it on another nearby computer [17]; GesturePen, \nsupporting selection of co-located devices as interaction target with pointing \ngestures [5]; eSquirt, a point and click technique for metaphorical squirting of \ndata from one device onto another [12]; and Synchronous Gestures for dynamic \ndevice association, for instance bumping together of display devices to create a \nlarger display [10].  \nRELATE Interaction Model and System \nThe RELATE interaction model is designed to support spontaneous interaction of \nmobile users within their immediate environment: the space a user can in principle \noversee and interact with from their current position. The model is aimed to help \nusers understand what devices and services are present, and to support association \nof the user\u2019s mobile device with any of the present devices in a seamless manner. \nThe devices involved can be situated devices such as printers and public displays, \nas well as co-located mobile devices, including personal mobile devices of other \nusers. \nThe interaction model involves the following steps: \n1. A combination of network discovery and spatial sensing is used for \nspatially-bounded discovery of potential target devices around the user\u2019s \nmobile device. \n2. The spatial relationships between the involved devices are tracked and \nmodelled in real-time as spatial references. \n3. Users are provided with a visualisation of available target devices in the in \nthe user interface of their personal device, in a spatial layout that reflects \ndevice locations relative to the user\u2019s device. \n4. Users initiate interaction and communication with a device by selection of \nthe corresponding object in their user interface, using direct manipulation \ntechniques. \nThe combination of network discovery and spatial sensing has two purposes: first, \nto limit discovery to devices that are ready to hand; and secondly, to associate \nnetwork identities (device addresses) with spatial references. The association of \nnetwork identity with a spatial reference is essential as it allows users to resolve \nthe location of potential targets discovered by their device, and vice versa their \ndevices to resolve the network address of a device physically selected by the user. \nThe spatial references involved are in terms of relative position, from an ego-\ncentric perspective. The rationale is that relative positions, in contrast to absolute \npositions, support tasks such as: identifying an encountered device in front of the \nuser; indicating where devices are from the user\u2019s perspective; distinguishing \ndevices by spatial reference (\u201cprinter on the left vs. printer on the right\u201d). \nRELATE-style interaction requires device positioning and tracking in real-time, at \na level of accuracy that supports differentiation of devices co-located within an \nenvironment. The model is not tied to any specific location system, but the choice \n5 \n \n \nFig. 2 Notebook augmented with a Relate sensor dongle for relative positioning of co-located \ndevices. Discovered devices are visualized in a spatial layout in the user interface. \nof system impacts on properties of the supported spatial interaction. We have \ndeveloped a relative positioning system based on bi-directional ultrasonic ranging \n(synchronised over a dedicated RF channel) that has the following properties of \nrelevance to our interaction model: \n1. Spatially-bounded discovery: Ultrasonic ranging is limited to a few metres, \nand ultrasonic signals are contained within rooms. The sensing mechanism \nthus corresponds with our concept of limiting discovery to the immediate \ninteraction range of the user.   \n2. Location-limited channel: The combination of RF and ultrasound in the \nsensing systems provides a location-limited channel that allows users to \nverify the authenticity of a device selected for interaction. In related work \nwe have shown use of this property for securing spontaneous interactions \nagainst attacks on the wireless network [12]. \n3. Infrastructure-less: The sensing system operates in peer-to-peer mode and \nis not reliant on any infrastructure in the environment. It can support \nspontaneous interaction between RELATE-enabled devices in any \nenvironment, indoors and outdoors. \n4. Fine-grained positioning in real-time: The system provides accurate and \nup-to-date readings (performance in a setting with 5 devices in co-planar \narrangement: 90th percentile accuracy of 7 cm for distances and 25 \ndegrees for angle-of-arrival; accurate updates within one second 70% of \nthe time) \nSupporting Collaboration of Co-located Mobile \nUsers \nThe first application explored with the RELATE system was support for \ncollaboration of co-located mobile users. The target scenario was to provide users \nwho come with their mobile devices into a meeting with a spatial interface to \nmore easily interact with the other meeting participants. The meeting support \nconsidered included awareness support to match names of participants with their \nrelative position around the table, chat with spatially selected participants, file \n6 \ntransfer by spatial reference, and virtual connection to a large shared display using \na spatial metaphor. \nSystem Design and Implementation \nWith the meeting support application in mind, RELATE sensing devices were \npackaged as USB add-ons (Relate Dongles) to be readily plugged into standard \nmobile computing devices (notebooks and PDAs). As dongle-equipped mobile \ndevices become co-located, the dongles discover each other and form a wireless \nsensor network for collaborative measurement of their spatial arrangement. Each \nuser\u2019s device acts as a client to the dongle sensor network, and translates \nmeasurements received into a visualization of the positions of the other devices. \nFigure 2 shows a notebook with sensor dongle, and a screen displaying the user\u2019s \ndevice surrounded by other discovered devices. \nA two-dimensional map view was chosen for the visualization of co-located \ndevices, using a relative coordinate system with the local device at the origin. \nDevices are represented by icons spatially arranged in a to-scale representation of \nthe actual device arrangement, with the aim to allows a user to easily map \nbetween display and reality. The map view is implemented as a widget that also \nsupports direct manipulation techniques: selection of one or more of the depicted \ndevices, to specify the target of a command (for example, pinging the device, or \nopening a pop-up window with more information on the device); and drag-and-\ndrop of interface objects such as files onto device icons (for instance to invoke file \ntransfer to another device).  \nFigure 3 provides an illustration of the application, showing a meeting situation \non the left, and the corresponding spatial user interface on the right. Note that the \nuser interface provides an egocentric view of the meeting situation, with the user\u2019s \nown device highlighted for reference. The system supports collaboration by \nproviding awareness of who the meeting participants are; this is done by \nannotating device icons with user names, to allow matching of faces with names. \nThe system further supports initiation of communication (e.g. chat) and document \nexchange via the spatial references in the user interface. To transfer a document to \nanother user\u2019s device, the corresponding file is selected in the user interface and \nmoved with a drag-and-drop operation to the icon that spatially represents the \n  \nFig. 3 Co-located mobile users are provided with an interface that provides spatial references \nto the devices brought into the meeting. The display shown on the right reflects the situation \nshown on the left (captured from the perspective of the user in the foreground). Note \nintegration of the display with a file browser to support remote file transfer by drag and drop.  \n7 \ntarget user. This means, that users do not have to concern themselves with \ncomputer names and IP addresses. Instead they can identify the desired target \ndevice on their screen by mapping the real arrangement of devices to the \ncorresponding layout of icons in the interface, which significantly lowers the bar \nfor spontaneous interaction. \nEvaluation and Observations \nThe application system has been tested and demonstrated in configurations \ninvolving between 3 and 5 mobile devices augmented with Relate dongles. A first \nset of experiments was conducted with 5 notebooks on a 2.4 x 1.6 m surface in an \nindoor office environment, with the primary aim of characterising relative \npositioning accuracy. For this purpose, each notebook was placed at randomly \ngenerated location and orientation on the surface for collection of measurements \nover several minutes. Over one hundred runs of the experiment were performed, \neach with a different randomly generated device arrangement. Half of the \nexperiments involved device arrangements with limited line-of-sight between \nsensor dongles (with three of the possible ten lines-of-sight blocked)1. This was to \ntest the systems ability to compensate for limited line-of-sight with collaborative \nsensing and sharing of measurements.  \nThe sensor performance results are detailed in our prior published work [9]. For \nthe purposes of this paper, we focus on impact of sensor performance on the user \ninterface, observed alongside the above experiment and in a series of interactive \ndemonstrations, in our lab environment and at the Mobisys \u201905 and EWSN \u201906 \nconferences (where smaller setups with three devices were used). We also review \ninformal feedback received from demo participants on the spatial user interface \ndesign. The main insights gained in this respect are: \n\u2022 Sensor data was pre-processed to filter noise prior to visual representation of \nrelative device position (occasional outliers are typical in ultrasonic ranging) \nbut the limited reliability of RF communication for sharing of measurements \nbetween sensor nodes still resulted in significant jitter in the visualisation. \nUsers were clearly sensitive even to small amount of jitter, and found it very \ndistracting when device icons moved although the corresponding real-world \ndevices did not. Also problematic was that loss of measurements for a device \nover more then 10 seconds led the system to assume that the device had \nmoved outside sensing range and left the meeting \u2013 causing further irritation \nin the user experience. \n\u2022 Our demonstrations routinely involved dynamic addition and removal of \ndevices to show discovery and automatic adaptation of the collaborative \nsensing protocol to changing numbers of nodes. The overall positioning \naccuracy of the system decreases when devices are removed as fewer \nmeasurements are available for producing position estimate: this appeared to \nbe counterintuitive for users who expected that performance should increase \nin a less complex setup.  \n\u2022 Our concern had been to map device position as accurately as possible in a \nuser-centred coordinate system, but we discovered that relative accuracy was \n                                                 \n1 Ultrasonic ranging requires direct line-of-sight. In the absence of direct line-of-sight less accurate \ndistance estimates can be achieved indirectly via other sensor nodes \n8 \nmuch more important for usability than absolute accuracy. For example, \nwhen three devices were equally spaced in front of the user, then it was more \nimportant that the user interface reflected the equal spacing then the correct \ndistance or angle between the devices. In all our demonstrations, it was \napparent that users are very sensitive to proportionality. Small relative errors \nare perceived immediately and found confusing. \n\u2022 The two-dimensional map representation, while aiming to provide detail on \ndevice arrangement at a glance, in many instances confused users. The \nmapping between the real-world and flat representation on the screen was not \nalways clear to users, with many expecting a top view of the environment \nwhereas our visualisation was based on front view. In response we have also \nconsidered perspective views in device icon size indicated depth. \n\u2022 A general concern with the map representation was its large footprint on the \nscreen. The demonstrated tasks in our application (awareness, file transfer) \nare typically not in the foreground of user activity in meetings but peripheral \nto activities such as note-taking or browsing of documents. This led us to \nconsider visualisation of spatial references in a more peripheral manner, \nresulting in the RelateGateways design on which we report in the next \nsection.  \nSpatial Discovery and Access to Services \nBuilding on the experience and insight gained with our initial application \ndemonstrator, we have developed a second application based on the RELATE \ninteraction model. The application setting in this case is mobile interaction with \npervasive services in the environment of the user. The targeted functionality is \nsupport for discovery of devices and services in the user\u2019s proximity, for \nidentification and differentiation of devices (\u201cwhich of the two printers does \ncolour \u2013 the one on the left or the one the right?\u201d), and for direct access to \nservices from the user\u2019s mobile device. \nSystem Design and Implementation \nFor this second application we used the dongle hardware from our initial \ndevelopment as plug-in for the mobile device in our scenario. In addition, we \n   \nFig. 4 The Relate gateways interface provides mobile users with a view of services available \nin the environment, arranged as gateways around the edge of the screen based on a compass \nmetaphor. The photo on the right shows a public display as example of a service that is \ndiscovered and accessed through Relate gateways \n9 \ndeveloped stand-alone sensor nodes (Relate Dots) for tagging of devices in the \nenvironment. The dots have no direct connection to the devices they augment but \nare pre-configured to identify \u2018their\u2019 device and the services it provides (with a \nURL for retrieval of a service description, transmitted to any dongle upon mutual \ndiscovery). The sensing protocol is modified as only the mobile device collects \nmeasurements from the sensor network.  \nThe user interface design for this application takes account of the shortcomings \nobserved with the map representation in our initial application, and the limited \nscreen size on mobile devices. In this new design, discovered devices are \nrepresented by Relate Gateways which are arranged around the edge of the screen \nusing a compass metaphor. Figure 4 illustrates the concept on the left, and shows \na snapshot of the demonstrator on the right, with a mobile user device and public \ndisplay as example for a service provided in the environment. \nFigure 5 provides detail on the implementation of the Relate gateways interface. \nGateways represent services in the environment and are arranged around the \nperiphery of the mobile device\u2019s user interface. The position in the interface \nindicates the direction of the device providing the service. For example if the user \nis standing in front of a printer, they will see a printer gateway on the top of their \nscreen. If the printer is on the left, the gateway will appear on the left. As the user \nmoves around, or changes orientation, the positions of gateways are updated. In \nthis way, gateways function as pointers to services. However, gateways are also \naccess points to services, fully integrated with the user interface to support direct \nmanipulation techniques. Users can use gateways in two ways: as target area for \ndrag-and-drop operations that invoke default actions (e.g. dropping a file onto a \nprinter gateway, to invoke it being printed); or as button that can be clicked to \nopen a service menu (e.g. to select an action, or to set options).  \n10 \nThe Relate gateway system supports operation in two different discovery modes. \nIn the scanning mode all devices (and corresponding services) within visibility \nrange (i.e. within line-of-sight, from a sensing perspective) are shown in the \ninterface, to support general discovery and awareness of available services. In \ncontrast, in the conditional mode, devices only become displayed as gateways \nwhen the user is in close enough proximity to directly use the device. For example \na keyboard might be offered as service for text-entry on small mobile devices \u2013 in \nthe scanning mode a user would be shown a corresponding gateway to find out \nthat such as a service is available (and where in their environment), whereas in the \nconditional mode the gateway would only appear if the user places their mobile \ndevice in direct interaction range of the keyboard (which in our system design \nwould be defined as a spatial condition by the service).  \nFormative Study of the Interface Design \nFor assessment of the revised user interface design, we used a Wizard of Oz \napproach, in which displayed relative device positions were provided by a human \noperator. This approach was chosen to focus the study on the usability in principle \nof the gateway concept, factoring out influences caused by potentially fluctuating \nsensor performance (how to accommodate sensor noise and resulting uncertainty \nin the interface design is a separate concern necessitating further study \u2013 informed \nfor instance by work of others on mobile spatial interaction in the presence of \nuncertainty [21]). \nOur study was set up in a larger meeting room extending into an adjacent hallway \nwith an arrangement of three devices\/services as shown in Figure 6: a keyboard \nthat users can select to have keyboard input redirected to their mobile device; a \ndisplay supporting presentation of documents transferred from the mobile device; \nand a printer offering standard printing services. For exploration of spatial \nconditions we defined interaction zones around devices based on user distance, \norientation and movement as illustrated in Figure 6. \n \n   \nFig. 5 Implementation of Relate gateways as widgets arranged at the periphery of the user \ninterface of a mobile device, and examples of gateways to a variety of services. Note that \nrelative position of services is mapped to position around the user interface (orientation) and \nto a distance measure provided in the gateway representation \n11 \n \nFig. 6 Setup of devices and services with their respective interaction zones for exploration of the \nRelateGateways interface concept \n \nThe study was conducted with 15 users recruited in one of the Universities \nparticipating in this research: all students between 21 and 25 years old, mostly \nmale, and all with prior experience with mobile computers (but none had prior \nexposure to the RELATE system and concepts). Participants were given a short \nintroduction and then asked to perform tasks that required interaction with all \nthree deployed services, followed by invitation to more freely explore use of the \nsystem, and specifically use of scanning versus conditional modes of discovery. \nThe main insights gained from the study were as follows: \n\u2022 The gateways interface appeared to be more easily understood than the \nmap view in our prior application. The gateway interface abstracts relative \npositions to points around the edge of a screen and as a result was less \nconfusing then the two-dimensional map representation. \n\u2022 The arrangement of device-representing gateways around the edge of the \nscreen was observed to be practical not only in terms of preserving screen \nreal estate, but also facilitated drag-and-drop to discovered services very \neffectively; positioned around the screen perimeter, gateways were easier \nto locate then icons on a map, and also less susceptible to occlusion by \nother screen content. Users consistently rated drag-and-drop to a remote \ndevice via gateways as very intuitive. \n\u2022 Following exploration of the alternative discovery modes, most \nparticipants suggested they would use the scanning mode when they enter \nan unknown environment, and conditional mode in familiar environment. \nThis suggests that both should be supported but further thought needs to be \ngiven on how to expose clearly in which mode the interface is. \n\u2022 The way in which the system facilitated seamless access to infrastructure \ndevices led some of the users to perceive the mobile device effectively as \nuniversal remote control. While our motivation for the system had been \nfacilitation of spontaneous interaction, it is apparent that the interaction \nmodel can also be effective for interaction over a distance with the devices \nin a familiar environment. \n12 \nControlled Study of Spatial Interfaces for Device \nSelection \nIn addition to exploration of spatial references in the context of application \ndemonstrators we have conducted a controlled study aimed to compare their use \nwith a non-spatial condition for selection of co-located devices. As non-spatial \ncondition we chose an alphabetically sorted listed as common for display of \nnetwork-discovered services, and as spatial conditions a list sorted by device \ndistance and a map view as used in our first demonstrator.  \nOne key advantage of spatial references, as demonstrated, is that they enable \ndevice selection without prior knowledge of device names and addresses. \nHowever for this study we focused on a setting in which device names are \navailable (as label on the device) in order to gain insight into user preference for \nspatial versus non-spatial interface. Our hypotheses for the study were:  \n (H1) Users prefer device selection with spatial references in comparison with \ndevice selection from an alphabetical list. \n(H2) The mental demand is lower using spatial references when compared with an \nalphabetical list.  \nExperiment Design and Procedure \nThe experiment is a within-subject design with one independent variable, the level \nof spatial information provided in the interface: 1) no spatial information, 2) low \nspatial information (spatial list) and 3) detailed spatial information (iconic map). \nThe order of interface presentation and the target devices were randomized for \neach configuration using Latin squares. User satisfaction, mental load and ranking \nof the three interfaces were the primary dependent variables. \nThe computer running the experiment was an OQO model 01, with a 5\" display \nand 800x480 pixel resolution. The three interfaces for the selection task, shown in \nfigure 7, were implemented with HTML and named as follows: a) Alphabetical \nList b) Spatial List and c) Iconic Map. We conducted the study in our \ndepartment\u2019s library and used three notebooks, a projected screen and two printers \nto simulate a multifunctional meeting room. All devices were clearly labelled with \ntheir name to enable devices to be identified and selected without spatial hints. \nParticipants started the experiment on one of two predefined places in the room \nand one interface configuration (both places and interface configurations were \n \nFig. 7 The three interfaces implemented on an OQO handheld for study of spatial references, \nfrom left to right: a) Alphabetical List, b) Spatial List, c) Iconic Map \n13 \ncounterbalanced among subjects). For each trial, the investigator touched one of \nthe devices in the lab in order to show the participant which device to select. \nParticipants then clicked \u2018start\u2019 to bring up the interface, and to select the \nindicated target. For each interface condition, a participant received six trials; the \nfirst two were for warm-up, followed by two each on the predefined participant \npositions. With the re-location of the participant from one position to the other, \nsome devices were also re-arranged to modify the overall configuration. After all \nsix trials, the participants filled in a questionnaire on satisfaction (based on the \nIBM computer usability satisfaction questionnaire) and mental load (using the \nNASA task load index). The procedure was then repeated for the other interface \nconditions, followed by a concluding interview in which users were asked to rank \nthe three interfaces, and to provide general comments. \n9 male and 9 female participants took part in the study. These were employees \nand students from different Departments in our University, with an average age of \nM=30.8 (SD=7.9). Participants rated themselves with M=3.8 (SD=0.83) for their \nexperience with computers and M=3.4 (SD=0.9) for their experience with mobile \ndevices (on a scale from 1=none to 5=expert).  \nResults  \nThe user satisfaction scores showed higher satisfaction for Iconic Map (M=1.67) \nand Alphabetic List (M=1.69) than for Spatial List (M=2.11). The perceived \nmental demand was higher for Spatial List (3.3) than for Iconic Map (3.7) and \nAlphabetic List. The frustration level was also rated higher for Spatial List (3.6) \nthan the other two conditions (4.3). Participants rated their performance toward \ntask accomplishment higher for Iconic Map (1.6) and Alphabetic List (1.6) than \nfor Spatial List (1.9). These results do not fully support our hypotheses as they do \nnot show a significance preference for Iconic Map, and consistently lower rating \nfor Spatial List. Given that device names were clearly visible, list search was \neasier based on alphabetic sorting than distance sorting. Moreover, device \ndistances do not vary much in co-located device settings: the direction at which a \ndevice is seen from the perspective of the user would appear to be much more \nsignificant for matching of interface to real world, than the distance. \nThe ranking results show a significant association between the amount of spatial \ninformation and whether it would be ranked as the most preferred one to use \n\u03c72(4)=14.67, p<0.01. 2\/3 of the participants chose Iconic Map as the most \n \nFig. 8 Final ranking of the three interfaces \n \n14 \npreferred interface for the selection task, as summarised in Figure 8. As reasons \nfor their preference of Iconic Map, participants mentioned for instance \u201cI know \nwhere the devices are\u201d but there were also a few participants who had problems to \nmatch the room with the iconic interface. Only two participants were in favour of \nSpatial List; many participants commented on the order in which devices were \nshown in Spatial List as confusing. \nThe study indicates that if device names are available spatial information still \ntends to be preferred by users but does not add to user satisfaction, and can be \nconfusing if the abstraction is inappropriate. However it has to be noted that in \npractice device names are not as readily available as in our experimental setup. If \ndevice names are not displayed on the device, spatial hints can be expected to add \nsignificantly to usability: they can replace names if the spatial resolution is \nsufficient, or alternatively assist users in name-to-device matching. \nDiscussion \nThe two demonstrator systems we have built illustrate use of the RELATE \ninteraction model and validate its support for spontaneous interaction in different \napplication settings. The user benefits demonstrated are support for discovery and \nsense-making of interaction opportunities in the environment, and access to \ndevices and services in abstraction from device names and addresses. Moreover \nwe have presented a controlled study showing that spatial interfaces can perform \nas well as device lists also when device names are readily available. \nThe interaction model demonstrated is generic and widely applicable to any \nsituation in which users wish to dynamically associate devices. The two \napplications described emphasize interaction with devices that are a priori \nunknown to the mobile user, highlighting access without knowledge of device \nnames; however the model may also useful for interaction across multiple devices \nof the same user, as spatial references can provide an efficient shortcut for tasks \nsuch as file transfer, relocation of applications, or migration of controls. A \nlimitation to application of the model however is its dependence on relative \npositioning support; the model either requires a smart environment that tracks \ndevices, or augmentation of devices with built-in sensors as demonstrated in the \nprototype systems we built. \nThe two applications have been demonstrated at relatively small scale, involving \nonly a few devices around a mobile user. Scalability of the interaction model is a \nconcern from a number of perspectives: space in the user interface to display \nspatial references (i.e. overall screen real estate, and resolution of devices that are \nclose together); human perception of spatial representations (and ability to map \nentities between display and environment when numbers increase); and \nperformance of the sensor system (e.g. longer delay of updates with growing \nnumber of devices). While this has not been tested, it is reasonable to expect that \nusability of the interface concept will decrease quickly with larger number of \ndevices. Mechanisms to address this would include filtering (as already explored \nwith the conditional mode in our second application study) and more advanced \nvisualisation concepts, for instance based on grouping of devices. \nUser feedback obtained with both application systems support that users quickly \nunderstand the spatial mapping employed in the user interface. The gateways \ninterface appeared to be more easily understood, whereas the map view involves a \nmore complex mapping and potentially more confusing movement in the \n15 \ninterface. An interesting insight is specifically the role of proportionality and \nrelative accuracy in the representation of device position \u2013less dominant though in \na layout around the edge of the screen than in a two-dimensional map. \nFinally, a general problem observed pertains to the inherent limitations of sensor \nsystems, in particular their imprecision (precision refers to the quality of a sensor \nto produce the same or similar result with repeated measurement, not to be \nconfused with accuracy). Whereas our initial system design sought to filter noise, \nan alternative approach is to use models that cope with uncertainty.  \nConclusion \nWe have presented an interaction model designed to support interaction of mobile \nusers with devices and services in their immediate environment. The model is \nbased on relative positioning of devices and integration of spatial references in the \nmobile user interface. The contributions of the model are that it supports matching \nof network identity of devices with physical identity; visual discovery of \ninteraction opportunities; and direct access to discovered devices. The model has \nbeen tried and validated in two application systems, demonstrating its versatility \nand support of very common tasks in mobile and ubiquitous computing, such as \ndocument transfer during an encounter of mobile users, and dynamic association \nof a mobile device with a device situated in the environment. A main advantage \ndemonstrated is that spatial references support selection when device names are \nnot available to the user. In a controlled study we have shown that spatial \nreferences can be as effective as name-based device selection also in less likely \nsettings in which device names are readily available for identification of \nencountered devices. \nThe reported work exposes a number of challenges for further investigation. \nThese include: development of further experimental data on the usability of \nspatial references and interface designs such as the Relate Gateways; assessment \nof scalability from both sensing and sense-making perspectives; and the \ninvestigation of spatial interaction and visualisation models that are more robust \nwith respect to limitations of underlying sensing systems. \nAcknowledgements \nThe presented research was supported by the UK Engineering and Physical \nSciences Research Council (grant GR\/S77097\/01, \u201cRelative Positioning of Mobile \nObjects\u201d) and the European Commission (contract 013790, \u201cRELATE\u201d). \nReferences \n1. Addlesee M, Curwen R, Hodges S, Newman J, Steggles P, Ward A and Hopper A. 2001. \nImplementing a Sentient Computing System. Computer 34, 8 (Aug. 2001), 50-56. \n2. Baudisch, P. and Rosenholtz, R. 2003. Halo: a technique for visualizing off-screen \nobjects. In Proc. SIGCHI Conference on Human Factors in Computing Systems (CHI \n'03), 481-488. \n3. Biehl JT and Bailey BP. 2004. ARIS: an interface for application relocation in an \ninteractive space. In Proc. of the 2004 Conference on Graphics interface (London, \nOntario, Canada, May 2004), 107-116. \n4. Brumitt B, Krumm J, Meyers B and Shafer S. 2000. Ubiquitous computing and the role of \ngeometry. IEEE Personal Communications, pages 41\u201343, October 2000. \n16 \n5. Danesh A, Inkpen K, Lau F, Shu K, and Booth K. 2001. Geney: Designing a \ncollaborative activity for the palm handheld computer. In Proceedings of CHI, \nConference on Human Factors in Computing Systems (CHI 2001), pages 388\u2013395. \n6. Edwards WK. 2006. Discovery Systems in Ubiquitous Computing. IEEE Pervasive \nComputing 5, 2 (Apr. 2006), 70-77. \n7. Fr\u00f6hlich, P., Simon, R., Baillie, L., Roberts, J., and Murray-Smith, R. (2007).Mobile \nSpatial Interaction.Extended Abstracts of CHI2007, Conference on Human Factors in \nComputing Systems, San Jos\u00e9, CA, USA \n8. Guinard D, Gellersen H, Streng S. 2007. Extending Mobile Devices with Spatially \nArranged Gateways to Pervasive Services. Proc. International Workshop on Pervasive \nMobile Interaction Devices (PERMID 2007). \n9. Hazas M, Kray C, Gellersen H, Agbota H, Kortuem G, and Krohn A. 2005. A relative \npositioning system for co-located mobile devices. In Proceedings of the 3rd international \nConference on Mobile Systems, Applications, and Services (Seattle, USA, June 2005). \nMobiSys '05. 177-190. \n10. Hinckley K. 2003. Synchronous gestures for multiple users and computers. In \nProceedings of UIST 2003, 149\u2013158.  \n11. Kindberg T and Fox A. 2002. System Software for Ubiquitous Computing. IEEE \nPervasive Computing 1, 1 (Jan. 2002), 70-81. \n12. Kindberg T, Barton J, Morgan J, Becker G, Caswell D, Debaty P, Gopal G, Frid M, \nKrishnan V, Morris H, Schettino J, Serra B, and Spasojevic M. 2002. People, places, \nthings: web presence for the real world. Mob. Netw. Appl. 7, 5 (Oct. 2002), 365-376.  \n13. Kortuem G, Kray C and Gellersen H. 2005. Sensing and visualizing spatial relations of \nmobile devices. In Proc. of the 18th Annual ACM Symposium on User interface Software \nand Technology (Seattle, WA, USA, October 23 - 26, 2005). UIST '05. ACM Press, 93-\n102. \n14. Mayrhofer R, Gellersen H, and Hazas M. 2007. Security by spatial reference: Using \nrelative positioning to authenticate devices for spontaneous interaction. In Proc. Ubicomp \n2007: 9th International Conference on Ubiquitous Computing (Innsbruck, Austria, Sept. \n2007). 199-216. \n15. Newman, MW, Sedivy JZ, Neuwirth CM, Edwards WK, Hong JI, Izadi S, Marcelo K and \nSmith TF. 2002. Designing for serendipity: supporting end-user configuration of \nubiquitous computing environments. In Proceedings of the Conference on Designing \ninteractive Systems: Processes, Practices, Methods, and Techniques (London, England, \nJune 25 - 28, 2002). DIS '02. 147-156. \n16. Ponnekanti S, Lee B, Fox A, Hanrahan P and Winograd W. 2001. ICrafter: A Service \nFramework for Ubiquitous Computing Environments. Proc. of the 3rd international \nconference on Ubiquitous Computing (Atlanta, Georgia, USA, September 2001), 56-75. \n17. Rekimoto J. 1997. Pick-and-drop: a direct manipulation technique for multiple computer \nenvironments, Proceedings of the 10th annual ACM symposium on User interface \nsoftware and technology, October 14-17, 1997, Banff, Alberta, Canada, 31-39.  \n18. Rekimoto J, Ayatsuka Y, Kohno M, and Oba H. 2003. Proximal Interactions: A Direct \nManipulation Technique for Wireless Networking. In Proc. of Interact'2003, 511--518. \n19. Rukzio E, Leichtenstern K, Callaghan V, Holleis P, Schmidt A, Chin J. 2006. An \nExperimental Comparison of Physical Mobile Interaction Techniques: Touching, \nPointing and Scanning. In Proc. Ubicomp 2006, 87-104. \n20. Schilit, BN, Adams, NI and Want, R. 1994. Context-aware computing applications. In \nProceedings of Workshop onMobile Computing Systems and Applications (WMCSA), \npages 85\u201390, Santa Cruz, CA, USA, December 1994. IEEE Computer Society. \n21. S. Strachan, J. Williamson, R. Murray-Smith, Show me the way to Monte Carlo: \ndensity-based trajectory navigation, Proceedings of ACM SIG CHI Conference, San Jose, \n2007. \n17 \n22. Suzuki G, Aoki S, Iwamoto T, Maruyama D, Koda T, Kohtake N, Takashio K, and \nTokuda H. 2005. u-photo: Interacting with pervasive services using digital still images. In \nProc. Pervasive 2005, 190\u2013207. \n"}