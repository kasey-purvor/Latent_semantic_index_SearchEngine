{"doi":"10.4018\/jmbl.2009040104","coreId":"195977","oai":"oai:lra.le.ac.uk:2381\/8123","identifiers":["oai:lra.le.ac.uk:2381\/8123","10.4018\/jmbl.2009040104"],"title":"Meeting the Challenges in Evaluating Mobile Learning: A 3-level Evaluation Framework","authors":["Vavoula, Giasemi N.","Sharples, Mike"],"enrichments":{"references":[{"id":44733626,"title":"73 Copyright \u00a9 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.","authors":[],"date":"2009","doi":null,"raw":"International Journal of Mobile and Blended Learning, 1(2), 54-75, April-June 2009 73 Copyright \u00a9 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.","cites":null},{"id":44733656,"title":"75 Copyright \u00a9 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.","authors":[],"date":"2009","doi":null,"raw":"International Journal of Mobile and Blended Learning, 1(2), 54-75, April-June 2009 75 Copyright \u00a9 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.","cites":null},{"id":44733631,"title":"A Briefing on Key Concepts: Formative and summative, criterion and norm-referenced assessment. LTSN Generic Centre.","authors":[],"date":"2001","doi":null,"raw":"Knight, P. (2001). A Briefing on Key Concepts: Formative and summative, criterion and norm-referenced assessment. LTSN Generic Centre.","cites":null},{"id":44733639,"title":"A Framework for Conceptualising the Impact of","authors":[],"date":"2007","doi":null,"raw":"Price, S., & Oliver, M. (2007). A Framework for Conceptualising the Impact of Technology on Teaching and Learning. Educational Technology & Society, 10(1), 16-27.","cites":null},{"id":44733655,"title":"A lifecycle approach to evaluating MyArtSpace. In","authors":[],"date":"2006","doi":"10.1109\/wmte.2006.261337","raw":"Vavoula, G., Meek, J., Sharples, M., Lonsdale, P. & Rudman, P. (2006a). A lifecycle approach to evaluating MyArtSpace. In S. Hsi, Kinshuk, T. Chan, & D. Sampson (Eds.), Proceedings of 4th International Workshop of Wireless, Mobile and Ubiquitous Technologies in Education (WMUTE 2006 ), Athens, Greece, IEEE Computer Society, (pp. 18-22).","cites":null},{"id":44733650,"title":"A task-centred approach to evaluating a mobile learning environment for pedagogical soundness.","authors":[],"date":"2004","doi":null,"raw":"Taylor, J. (2004). A task-centred approach to evaluating a mobile learning environment for pedagogical soundness. In Proceedings of MLearn2004, London, UK, Learning and Skills Development Agency, (pp. 167-171).","cites":null},{"id":44733645,"title":"A Theory of Learning for the Mobile Age. In","authors":[],"date":"2007","doi":"10.4135\/9781473955011.n4","raw":"Sharples, M., Taylor, J., & Vavoula, G. (2007a). A Theory of Learning for the Mobile Age. In Andrews, R. and Haythornthwaite, C. (Eds.), The Sage Handbook of E-learning Research. London, Sage, (pp. 221-47).","cites":null},{"id":44733633,"title":"Adopting a Lifecycle Approach to the Evaluation of Computers and Information Technology.","authors":[],"date":"2006","doi":null,"raw":"Meek, J. (2006). Adopting a Lifecycle Approach to the Evaluation of Computers and Information Technology. PhD Thesis, School of Electronic, Electrical and Computer Engineering, The University of Birmingham.","cites":null},{"id":44733646,"title":"An Evaluation of MyArtSpace: a Mobile Learning Service for School Museum Trips.","authors":[],"date":null,"doi":null,"raw":"An Evaluation of MyArtSpace: a Mobile Learning Service for School Museum Trips. In Proceedings of mLearn 2007, Melbourne, Australia.","cites":null},{"id":44733637,"title":"Analysis of an informal mobile learning activity based on activity theory.","authors":[],"date":"2007","doi":null,"raw":"Papadimitriou, I., Tselios, N., & Komis, V. (2007). Analysis of an informal mobile learning activity based on activity theory. In Vavoula, G.N., Kukulska-Hulme, A. and Pachler, N., Proceedings of Workshop Research Methods in Informal and Mobile Learning, WLE Centre, Institute of Education, London, UK, (pp. 25-28).","cites":null},{"id":44733617,"title":"Assessment and learning: contradictory or complementary? In","authors":[],"date":"1995","doi":null,"raw":"Boud, D. (1995). Assessment and learning: contradictory or complementary? In P. Knight (Ed.), Assessment for Learning in Higher Education. London, Kogan Page, (pp. 35-48).","cites":null},{"id":44733622,"title":"Building the Virtual State: Information Technology and Institutional Change,","authors":[],"date":"2001","doi":"10.1002\/pam.10127","raw":"Fountain, J. E. (2001). Building the Virtual State: Information Technology and Institutional Change, Brookings Institution Press.","cites":null},{"id":44733628,"title":"Conceptualizing Learning from the Everyday Activities of Digital Kids.","authors":[],"date":"2007","doi":"10.1080\/09500690701494076","raw":"Hsi, S. (2007). Conceptualizing Learning from the Everyday Activities of Digital Kids. International Journal of Science Education, 29(12), 1509 - 1529.","cites":null},{"id":44733643,"title":"Designing Learning Activities with Mobile Technologies. In","authors":[],"date":"2008","doi":"10.4018\/978-1-60566-062-2.ch001","raw":"Ryu, H., & Parsons, D. (2008). Designing Learning Activities with Mobile Technologies. In Ryu, H. and Parsons, D. (Eds.), Innovative Mobile Learning: Techniques 74 International Journal of Mobile and Blended Learning, 1(2), 54-75, April-June 2009 Copyright \u00a9 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. and Technologies, Idea Group Inc (IGI), (pp. 1-20).","cites":null},{"id":44733625,"title":"Emerging research methods for understanding mobile technology use.","authors":[],"date":"2005","doi":"10.3127\/ajis.v13i2.47","raw":"Hagen, P., Robertson, T., Kan, M., & Sadler, K. (2005). Emerging research methods for understanding mobile technology use. In Proceedings of CHISIG, Australia, (pp. 1-10).","cites":null},{"id":44733653,"title":"Evaluating Mobile Learning: Reflections on Current Practice.","authors":[],"date":"2005","doi":null,"raw":"Evaluating Mobile Learning: Reflections on Current Practice. In Proceedings of MLEARN 2005, Cape Town, South Africa.","cites":null},{"id":44733620,"title":"Evaluation of a Mobile Learning Organiser for University Students.","authors":[],"date":"2005","doi":"10.1111\/j.1365-2729.2005.00124.x","raw":"Corlett, D., Sharples, M., Chan, T., & Bull, S. (2005). Evaluation of a Mobile Learning Organiser for University Students. Journal of Computer Assisted Learning, 21, 162-170.","cites":null},{"id":44733640,"title":"Examining mobile phone use in the wild with quasi-experimentation.","authors":[],"date":"2004","doi":null,"raw":"Roto, V., Oulasvirta, A., Haikarainen, T., Kuorelahti, J., Lehmuskallio, H. & Nyyssonen, T. (2004). Examining mobile phone use in the wild with quasi-experimentation.","cites":null},{"id":44733624,"title":"Finding Evidence of Learning in Museum Settings.","authors":[],"date":"1998","doi":null,"raw":"Griffin, J., & Symington, D. (1998). Finding Evidence of Learning in Museum Settings. In Proceedings of Evaluation and Visitor Research Special Interest Group Conference \u2018Visitors Centre Stage: Action for the Future\u2019, Canberra.","cites":null},{"id":44733623,"title":"From ReplayTool to Digital Replay System. In","authors":[],"date":"2007","doi":null,"raw":"Greenhalgh, C., French, A., Tennent, P., Humble, J., & Crabtree, A. (2007). From ReplayTool to Digital Replay System. In Proceedings of e-Social Science Conference, Ann Arbor, Michigan, USA.","cites":null},{"id":44733615,"title":"Harnessing Technology: Next Generation Learning","authors":[],"date":"2008","doi":null,"raw":"BECTA (2008). Harnessing Technology: Next Generation Learning 2008-14. Available online: http:\/\/publications.becta.org.uk\/ display.cfm?resID=38751&page=1835.","cites":null},{"id":44733654,"title":"Have You Got Your PDA With You?... Denials and Accusations.","authors":[],"date":"2007","doi":null,"raw":"Trinder, J., Roy, S., & Magill, J. (2007). Have You Got Your PDA With You?... Denials and Accusations. In G. N.Vavoula, A. Kukulska-Hulme, & N. Pachler, (Eds.), Proceedings of Workshop Research Methods in Informal and Mobile Learning, Institute of Education, London, UK.","cites":null},{"id":44733638,"title":"Health and the Mobile Phone.","authors":[],"date":"2008","doi":"10.1016\/j.amepre.2008.05.001","raw":"Patrick, K., Griswold, W., Raab, F. & Intille, S. (2008). Health and the Mobile Phone. American Journal of Preventive Medicine, 35(2), 177-181.","cites":null},{"id":44733611,"title":"IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.","authors":[],"date":"2009","doi":null,"raw":"In Proceedings of 31st IRIS Scandinavian Information Systems Conference, \u00c5re, Sweden Anastopoulou, S., Sharples, M., Wright, M., Martin, H., Ainsworth, S., Benford, S., 72 International Journal of Mobile and Blended Learning, 1(2), 54-75, April-June 2009 Copyright \u00a9 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.","cites":null},{"id":44733619,"title":"Informality and formality","authors":[],"date":"2003","doi":"10.7788\/ijbe.2005.3132.1.165","raw":"Colley, H., Hodkinson, P., & Malcom, J. (2003). Informality and formality in learning. Learning and Skills Research Centre. 1492\/11\/03\/500.","cites":null},{"id":44733616,"title":"Inside the black box: raising standards through classroom assessment.","authors":[],"date":"1998","doi":"10.1177\/003172171009200119","raw":"Black, P. & Wiliam, D. (1998b). Inside the black box: raising standards through classroom assessment. Phi Delta Kappan, 80, 139-148.","cites":null},{"id":44733632,"title":"Iterative and Incremental Development: A Brief History.","authors":[],"date":"2003","doi":"10.1109\/mc.2003.1204375","raw":"Larman, C., & Basili, V.R. (2003). Iterative and Incremental Development: A Brief History. Computer Supported Cooperative Work, 36(6), 47-56.","cites":null},{"id":44733659,"title":"Learning Bridges: a role for mobile technologies in education.","authors":[],"date":"2007","doi":"10.1109\/wmte.2006.261337","raw":"Vavoula, G., Sharples, M., Rudman, P., Lonsdale, P., & Meek, J. (2007). Learning Bridges: a role for mobile technologies in education. Educational Technology Magazine, XLVII(3), 33-37. Vavoula, G., Sharples, M., Rudman, P., Meek, J., & Lonsdale, P. (forthcoming). Myartspace: Design and evaluation of support for learning with multimedia phones between classrooms and museums. Computers and Education.","cites":null},{"id":44733642,"title":"Managing the Development of Large Software Systems.","authors":[],"date":"1970","doi":null,"raw":"Royce, W. W. (1970). Managing the Development of Large Software Systems. In Proceedings of IEEE WESCON, (pp. 1-9).","cites":null},{"id":44733614,"title":"Manifesto for Agile Software Development. Available online: www.agilemanifesto.org. Accessed on 22\/01\/09.","authors":[],"date":"2001","doi":null,"raw":"Beck, K., Beedle, M., van Bennekum, A., Cockburn, A., Cunningham, W., Fowler, M., Grenning, G., Highsmith, J., Hunt, A., Jeffries, R., Kern, J., Marick, B., Martin, R.C., Mellor, S., Schwaber, K., Sutherland, J., & Thomas, D. (2001). Manifesto for Agile Software Development. Available online: www.agilemanifesto.org. Accessed on 22\/01\/09.","cites":null},{"id":44733652,"title":"Mobile Learning - The Ethical and Legal Challenges.","authors":[],"date":"2004","doi":"10.4018\/978-1-59904-970-0.ch033","raw":"Traxler, J., & Bridges, N. (2004). Mobile Learning - The Ethical and Legal Challenges. In Proceedings of MLEARN 2004, Bracciano, Italy.","cites":null},{"id":44733627,"title":"Motivating the Academically Unmotivated: A Critical Issue for the 21st Centrury.","authors":[],"date":"2000","doi":"10.3102\/00346543070002151","raw":"Hidi, S., & Harackiewicz, J.M. (2000). Motivating the Academically Unmotivated: A Critical Issue for the 21st Centrury. Review of Educational Research, 70(2), 151-179.","cites":null},{"id":44733661,"title":"Museum explorations in physical, personal and virtual space through MyArtSpace.","authors":[],"date":"2006","doi":"10.1109\/wmte.2006.261337","raw":"Vavoula, G., Sharples, M., Rudman, P., Meek, J., Lonsdale, P., & Philips, D. (2006b). Museum explorations in physical, personal and virtual space through MyArtSpace. In Proceedings of mLearn 2006, Banff, Canada.","cites":null},{"id":44733630,"title":"New Techniques for Usability Evaluation of Mobile Systems.","authors":[],"date":"2004","doi":"10.1016\/j.ijhcs.2003.11.001","raw":"Kjeldskov, J., & Stage, J. (2004). New Techniques for Usability Evaluation of Mobile Systems. International Journal of HumanComputer Studies, 60, (pp. 599-620).","cites":null},{"id":44733612,"title":"Proceedings of mLearn 2008: The bridge from text to context,","authors":[],"date":null,"doi":null,"raw":"(Eds.), Proceedings of mLearn 2008: The bridge from text to context, Wolverhampton, UK, University of Wolverhampton, (pp. 12-19).","cites":null},{"id":44733649,"title":"Reconstructing an informal mobile learning experience with multiple data streams.","authors":[],"date":"2007","doi":null,"raw":"Smith, H., Heldt, S., Fitzpatrick, G., Hui Ng, K., Benford, S., Wyeth, P., Walker, K., Underwood, J., Luckin, R., & Good, J. (2007). Reconstructing an informal mobile learning experience with multiple data streams. In Vavoula, G.N., Kukulska-Hulme, A. and Pachler, N., Proceedings of Workshop Research Methods in Informal and Mobile Learning, WLE Centre, Institute of Education, London, UK., (pp. 29-34).","cites":null},{"id":44733647,"title":"Socio-cognitive engineering: a methodology for the design of humancentred technology.","authors":[],"date":"2002","doi":"10.1016\/s0377-2217(01)00118-7","raw":"Sharples, M., Jeffery, N., Du Boulay, J. B. H., Teather, D., Teather, B., & Du Boulay, G. H. (2002). Socio-cognitive engineering: a methodology for the design of humancentred technology. European Journal of Operational Research, 136(2), 310-323.","cites":null},{"id":44733610,"title":"Sustainable Wireless Computing \u2013 How Public Information Systems Can Be Designed to Reduce Exposure.","authors":[],"date":"2008","doi":null,"raw":"Ahonen, M. (2008). Sustainable Wireless Computing \u2013 How Public Information Systems Can Be Designed to Reduce Exposure.","cites":null},{"id":44733621,"title":"The critical incident technique.","authors":[],"date":"1954","doi":"10.1037\/h0061470","raw":"Flanagan, J. C. (1954). The critical incident technique. Psychological Bulletin, 51(4), 327-358.","cites":null},{"id":44733634,"title":"The development, field test and validation of an inventory of scientific attitudes.","authors":[],"date":"1970","doi":"10.1002\/tea.3660070203","raw":"Moore, R. W., & Sutman, F. X. (1970). The development, field test and validation of an inventory of scientific attitudes. Journal of Research in Science Teaching, 7, 85\u201394.","cites":null},{"id":44733644,"title":"The methodology of evaluation. In","authors":[],"date":"1967","doi":"10.4135\/9781848607958.n28","raw":"Scriven, M. (1967). The methodology of evaluation. In R. W. Tyler & R. M. Gagne, & M. Scriven (Eds.), Perspectives of Curriculum Evaluation. Chicago, Rand McNally, (pp. 39-83).","cites":null},{"id":44733636,"title":"The Six Sigma Way: How GE, Motorola, and Other Top Companies are Honing Their Performance.","authors":[],"date":"2000","doi":"10.1108\/tqmm.2002.14.4.263.1","raw":"Pande, P. S., Neuman, R. P., & Cavanagh, R. R. (2000). The Six Sigma Way: How GE, Motorola, and Other Top Companies are Honing Their Performance. McGrawHill.","cites":null},{"id":44733618,"title":"The Uses of PDAs and Smartphones in Informal learning.","authors":[],"date":"2006","doi":"10.1111\/j.1365-2729.2007.00268.x","raw":"Clough, G., & Jones, A. (2006). The Uses of PDAs and Smartphones in Informal learning. In Proceedings of MLearn 2006, Banff, Canada.","cites":null},{"id":44733651,"title":"Towards a Task Model for Mobile Learning: a Dialectical Approach.","authors":[],"date":"2006","doi":"10.1504\/ijlt.2006.010616","raw":"Taylor, J., Sharples, M., O\u2019Malley, C., Vavoula, G., & Waycott, J. (2006). Towards a Task Model for Mobile Learning: a Dialectical Approach. International Journal of Learning Technology, 2(2\/3), (pp. 138-158). Traxler, J. (forthcoming). Mobile Learning Evaluation: The Challenge of Mobile Societies. In G. Vavoula, N. Pachler, & A. Kukulska-Hulme (Eds.), Researching Mobile Learning: Frameworks, Methods and Research Designs. Oxford, Peter Lang.","cites":null},{"id":44733613,"title":"User-Centered Internet Research: The Ethical Challenge.","authors":[],"date":"2004","doi":"10.4018\/978-1-59140-152-0.ch018","raw":"Bakardjieva, M., Feenberg, A., & Goldie, J. (2004). User-Centered Internet Research: The Ethical Challenge. In Buchanan, E.A. (Ed.), Readings in Virtual Research Ethics: Issues and Controversies, Idea Group Inc (IGI), (pp. 338-350).","cites":null},{"id":44733629,"title":"Using the balanced scorecard as a strategic management system. Harvard Business Review,","authors":[],"date":"1996","doi":"10.1177\/097215090100200211","raw":"Kaplan, R. S., & Norton, D. P. (1996). Using the balanced scorecard as a strategic management system. Harvard Business Review, (pp. 75\u201385).","cites":null},{"id":44733635,"title":"What does \u2018impact\u2019 mean in the evaluation of learning technology?","authors":[],"date":"2002","doi":null,"raw":"Oliver, M., & Harvey, J. (2002). What does \u2018impact\u2019 mean in the evaluation of learning technology? Educational Technology & Society, 5(3), 18-26.","cites":null},{"id":44733662,"title":"WP4: A Study of Mobile Learning Practices. MOBIlearn deliverable D4.4. Available online: http:\/\/","authors":[],"date":"2005","doi":null,"raw":"Vavoula, G. N. (2005). WP4: A Study of Mobile Learning Practices. MOBIlearn deliverable D4.4. Available online: http:\/\/","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2009","abstract":"This is the final publisher edited version of the paper published as International Journal of Mobile and Blended Learning, 2009, 1 (2), pp. 54-75.  This version was first published at http:\/\/www.igi-global.com\/Bookstore\/Article.aspx?TitleId=4058, Doi: 10.4018\/jmbl.2009040104.We propose six challenges in evaluating mobile learning: capturing and analyzing learning in context and across contexts, measuring mobile learning processes and outcomes, respecting learner\/participant privacy, assessing mobile technology utility and usability, considering the wider organizational and socio-cultural context of learning, and assessing in\/formality. A three-level framework for evaluating mobile learning is proposed, comprising a micro level concerned with usability, a meso level concerned with the learning experience, and a macro level concerned with integration within existing educational and organizational contexts. The article concludes with a discussion of how the framework meets the evaluation challenges and with suggestions for further extensions","downloadUrl":"http:\/\/www.igi-global.com\/article\/meeting-challenges-evaluating-mobile-learning\/4058","fullTextIdentifier":"https:\/\/lra.le.ac.uk\/bitstream\/2381\/8123\/3\/vavoula%20article-JMBL.pdf","pdfHashValue":"31fff8aaefa01a6b87b6919e4f50c1f9bd4c9c86","publisher":"IGI Global","rawRecordXml":"<record><header><identifier>\n        \n            \n                oai:lra.le.ac.uk:2381\/8123<\/identifier><datestamp>\n                2015-12-02T17:13:24Z<\/datestamp><setSpec>\n                com_2381_3<\/setSpec><setSpec>\n                com_2381_9548<\/setSpec><setSpec>\n                col_2381_51<\/setSpec>\n            <\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:doc=\"http:\/\/www.lyncode.com\/xoai\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n            \nMeeting the Challenges in Evaluating Mobile Learning: A 3-level Evaluation Framework<\/dc:title><dc:creator>\nVavoula, Giasemi N.<\/dc:creator><dc:creator>\nSharples, Mike<\/dc:creator><dc:subject>\nMobile learning evaluation<\/dc:subject><dc:subject>\nlearning context<\/dc:subject><dc:subject>\nevaluation framework<\/dc:subject><dc:subject>\nlearning outcomes<\/dc:subject><dc:subject>\nethics<\/dc:subject><dc:subject>\ninformality and formality of learning<\/dc:subject><dc:subject>\nrequirements for evaluation<\/dc:subject><dc:description>\nThis is the final publisher edited version of the paper published as International Journal of Mobile and Blended Learning, 2009, 1 (2), pp. 54-75.  This version was first published at http:\/\/www.igi-global.com\/Bookstore\/Article.aspx?TitleId=4058, Doi: 10.4018\/jmbl.2009040104.<\/dc:description><dc:description>\nWe propose six challenges in evaluating mobile learning: capturing and analyzing learning in context and across contexts, measuring mobile learning processes and outcomes, respecting learner\/participant privacy, assessing mobile technology utility and usability, considering the wider organizational and socio-cultural context of learning, and assessing in\/formality. A three-level framework for evaluating mobile learning is proposed, comprising a micro level concerned with usability, a meso level concerned with the learning experience, and a macro level concerned with integration within existing educational and organizational contexts. The article concludes with a discussion of how the framework meets the evaluation challenges and with suggestions for further extensions.<\/dc:description><dc:date>\n2010-06-30T09:27:57Z<\/dc:date><dc:date>\n2010-06-30T09:27:57Z<\/dc:date><dc:date>\n2009<\/dc:date><dc:type>\nArticle<\/dc:type><dc:identifier>\nInternational Journal of Mobile and Blended Learning, 2009, 1 (2), pp. 54-75<\/dc:identifier><dc:identifier>\n1941-8647<\/dc:identifier><dc:identifier>\nhttp:\/\/hdl.handle.net\/2381\/8123<\/dc:identifier><dc:identifier>\nhttp:\/\/www.igi-global.com\/article\/meeting-challenges-evaluating-mobile-learning\/4058<\/dc:identifier><dc:identifier>\n10.4018\/jmbl.2009040104<\/dc:identifier><dc:language>\nen<\/dc:language><dc:publisher>\nIGI Global<\/dc:publisher>\n<\/oai_dc:dc>\n<\/metadata>\n        <\/record>","journals":[{"title":null,"identifiers":["1941-8647","issn:1941-8647"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2009,"topics":["Mobile learning evaluation","learning context","evaluation framework","learning outcomes","ethics","informality and formality of learning","requirements for evaluation"],"subject":["Article"],"fullText":"InternatIonal Journal of MobIle \nand blended learnIng\n Special Issue: mLearn2008\n Guest Editorial Preface\ni mLearn2008: \n The Text and the Context\n John Traxler, Learning Lab, UK\n Research Articles\n1 Wildfire Activities: \n New Patterns of Mobility and Learning\n Yrj\u00f6 Engestr\u00f6m, University of Helsinki, Finland\n \n19 Designing Participant-Generated Context into Guided Tours\n Juliet Sprake, Goldsmiths, University of London, UK\n39 Improving Cross-Cultural Awareness and Communication through \n Mobile Technologies\n Adele Botha, Meraka Institute, South Africa\n Steve Vosloo, Stanford University, USA\n John Kuner, Stanford University, USA\n Madelein van den Berg, Meraka Institute, South Africa\n54 Meeting the Challenges in Evaluating Mobile Learning: \n A 3-Level Evaluation Framework \n Giasemi Vavoula, University of Leicester, UK\n\t Mike\tSharples,\tUniversity\tof\tNottingham,\tUK\n76 Ethical Considerations in Implementing Mobile Learning in the \n Workplace\n Jocelyn Wishart, University of Bristol, UK\nTable of Contents\nApril-June 2009, Vol. 1, No. 2\n54   International Journal of Mobile and Blended Learning, 1(2), 54-75, April-June 2009\nCopyright \u00a9 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global\nis prohibited.\naBstraCt\nWe propose six challenges in evaluating mobile learning: capturing and analysing \nlearning in context and across contexts, measuring mobile learning processes and out-\ncomes, respecting learner\/participant privacy, assessing mobile technology utility and \nusability, considering the wider organisational and socio-cultural context of learning, \nand assessing in\/formality. A three-level framework for evaluating mobile learning is \nproposed, comprising a micro level concerned with usability, a meso level concerned \nwith the learning experience, and a macro level concerned with integration within exist-\ning educational and organisational contexts. The article concludes with a discussion \nof how the framework meets the evaluation challenges and with suggestions for further \nextensions.[Article copies are available for purchase from InfoSci-on-Demand.com]\nKeywords: Ethics; Evaluation Framework; Informality and Formality of Learning; \nLearning Context; Learning Outcomes; Mobile Learning Evaluation; Re-\nquirements for Evaluation\nintroDUCtion\nMobile learning is a relatively new \nresearch area, with the first research \nprojects appearing in the second half \nof the 1990s and the first international \nresearch conferences less than a decade \nago. It is a field whose practice has not \nyet been standardised in terms of re-\nsearch frameworks, methods and tools. \nThankfully, mobile learning has a lot of \ncommon ground with related research \nMeeting the Challenges in  \nevaluating Mobile learning:  \na 3-level evaluation framework \nGiasemi Vavoula, University of Leicester, UK\nMike Sharples, University of Nottingham, UK\nInternational Journal of Mobile and Blended Learning, 1(2), 54-75, April-June  2009   55\nCopyright \u00a9 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global \nis prohibited.\nareas including Technology-Enhanced \nLearning (TEL) and Mobile Human-\nComputer Interaction (mobileHCI). \n\u2018Borrowing\u2019 frameworks and methods \nfrom these areas has been common \npractice for early mobile learning \nresearch, providing researchers with \nuseful starting points. \nAs our conceptions and under-\nstanding of mobile learning deepen, \nthese \u2018borrowed\u2019 frameworks and tools \nmight no longer be adequate. We now \nappreciate mobile learning not just as \nlearning that is facilitated by mobile \ntechnology, but also as the processes \nof coming to know through conversa-\ntions and explorations across multiple \ncontexts amongst people and personal \ninteractive technologies (Sharples et \nal. 2007a). Such evolving conceptions \nintroduce challenges to all aspects of \nmobile learning research, including \nevaluation. As the field matures, our \nframeworks and tools need to address \nthese challenges.\nIn this article we summarise six \nchallenges in evaluating mobile learn-\ning: capturing and analysing learning in \ncontext and across contexts, measuring \nthe processes and outcomes of mobile \nlearning, respecting learner\/participant \nprivacy, assessing mobile technology \nutility and usability, considering the \nwider organisational and socio-cultural \ncontext of learning, and assessing \nin\/formality. The article proposes \nan evaluation framework with three \nlevels: a micro level concerned with \nusability, a meso level concerned with \nthe learning experience, and a macro \nlevel concerned with integration within \nexisting educational and organisational \ncontexts. The article demonstrates how \nthis framework has guided data collec-\ntion and analysis in one mobile learning \nevaluation project, and concludes with \na discussion of how it meets the evalu-\nation challenges and with suggestions \nfor further extensions.\nCHallenGe 1: CaPtUrinG \nlearninG ConteXt anD \nlearninG aCross  \nConteXts\nA major task for educational evalua-\ntion is to identify and analyse learning \nwithin and across contexts. For mobile \nlearning, the interest is not only in how \nlearning occurs in a variety of settings, \nbut also how people create new contexts \nfor learning through their interactions \nand how they progress learning across \ncontexts. This poses a significant chal-\nlenge to evaluators of mobile learning. \nIn order to establish, document and \nevaluate learning within and across \ncontexts, a researcher needs to analyse: \nthe physical setting and the layout of the \nlearning space (where); the social set-\nting (who, with whom, from whom); the \nlearning objectives and outcomes (why \nand what); the learning methods and \nactivities (how); the learning progress \nand history (when); and the learning \ntools (how).\nWhen evaluating learning in a tra-\nditional classroom, researchers gener-\nally have access to information about \n56   International Journal of Mobile and Blended Learning, 1(2), 54-75, April-June 2009\nCopyright \u00a9 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global\nis prohibited.\nthese context elements before, during \nand after the learning experience. \nThus, they can inspect the classroom \nand interview the teacher and learners \nin advance of a lesson to discover the \nobjectives, methods, lesson plan and \ntools. To evaluate a school museum \nvisit or field trip, the researcher can visit \nthe site and inspect the lesson plan, but \nwill generally not know in advance the \nroute that each student will take. For \npersonal or family visits to museums or \nother learning sites, neither the objec-\ntives nor the trajectory may be known \nin advance. Learning objectives may \narise as a response to interactions with \nthe environment and learning trails may \nbe guided by curiosity or unplanned \nevents. The learners themselves may \nnot be known in advance, for example \nwhen evaluating the learning experience \nof museum visitors randomly selected at \nthe museum entrance. Personal mobile \nlearning embraces any learning event \nwhere people, individually and collec-\ntively, continually create micro-sites for \nlearning out of the available physical \nand social resources. In considering \nthis generic case, the setting, objec-\ntives, methods and processes may all \nbe unpredictable. \nTable 1 portrays the increasing \nvagueness in moving from evaluating \na classroom lesson, to a school museum \nvisit, to personal or family museum vis-\nits, to personal mobile learning across \nformal and informal settings. Each set \nof context elements requires specific \nevaluation methods, to match the actual \nlearning processes and outcomes to \nexpectations, or to capture contingent \nand unexpected learning events.\nRecent research efforts have fo-\ncused on devising tools and methods \nappropriate for capturing and analysing \nmobile learning contexts. Some ef-\nforts concentrate on implementing \ntechnology-based solutions for data col-\nClassroom School museum visit \nor field trip\nPersonal or family \nvisit\nPersonal mobile \nlearning\nPhysical setting \uf0fe  Conventional and static\n\uf029 Moving around a \nfixed location\n\uf029 Moving around a \nfixed location\n\uf029 Unpredictable & \nchanging\nSocial setting \uf0fe Fixed \uf0fe Pre-arranged \uf0fe Pre-arranged \uf029 Unpredictable and changing\nLearning objectives \nand outcomes \uf0fe Externally set \uf0fe Externally set\n\uf029 Personally set or \ncontingent\n\uf029 Personally set or \ncontingent\nLearning method \nand activities \uf0fe Pre-determined \uf0fe Pre-determined\n\uf029 Pre-determined or \ncontingent\n\uf029 Pre-determined \nor contingent\nLearning progress \nand history \uf0fe Pre-determined\n\uf0fe Pre-determined or \ncontingent \uf029 Mostly contingent \uf029 Contingent\nLearning tools \uf0fe Provided \uf029 Provided by school or museum\n\uf029  P r o v i d e d  & \npersonally owned\n\uf029  Personal  & \nserendipitous \nTable 1. Context elements relevant to the learning researcher\nInternational Journal of Mobile and Blended Learning, 1(2), 54-75, April-June  2009   57\nCopyright \u00a9 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global \nis prohibited.\nlection, such as mobile eye tracking  or \nwearable interaction capture kits (Roto \net al. 2004). Although these have the \nadvantage of capturing accurate data in \ncontext, they have some disadvantages, \nnot least the obtrusiveness of the ap-\nparatus used. Other efforts opt for co-\noperative inquiry-based solutions (Hsi \n2007), such as using learners\u2019 accounts \nof the experience through retrospective \ninterviews, diaries, or attitude surveys \n(Clough & Jones 2006; Vavoula 2005). \nThese have different shortcomings such \nas the accuracy of recall, the degree to \nwhich post-rationalisation skews data, \nand the effect of the participants\u2019 con-\ncern over the image they project. \nIncreasingly, mobile evaluation \ndesigns include mixed methods. These \nare useful not only for validating data, \nbut also for capturing different perspec-\ntives of the learning experience. Thus, \ncollected data might include recorded \nvideo, audio transcripts, observation \nnotes, artefacts produced by the learners, \nand application screenshots. Interpret-\ning such rich collections of data can be \nchallenging too, in terms of assembling \nit into a meaningful, accurate and elabo-\nrate account of the learning experience. \nRelated research addresses the design \nof tools and methods to support the \nsequencing, synchronisation, inter-rela-\ntion and visualisation of evaluation data \n(Greenhalgh et al. 2007; Papadimitriou \net al. 2007; Smith et al. 2007).\nCHallenGe 2: Has anYone \nlearneD anYtHinG?\nA second challenge that faces mobile \nlearning evaluation is the assessment \nof learning processes and outcomes. In \ntraditional learning settings such as the \nclassroom there are well-established and \naccepted methods for the assessment \nof learning activities, such as essay \nwriting, multiple choice tests, open-\nbook exams, and unseen examinations. \nDistinctions have been made between \nformative assessment (aiming to pro-\nvide students with feedback regarding \ntheir progress) and summative assess-\nment (aiming to judge and sum up the \nstudents\u2019 achievements) (Scriven 1967), \nwith formative assessment bearing the \ngreater potential to aid and complement \nteaching and learning (Black & Wiliam \n1998a; 1998b). \nSummative assessment is often used \nas a measure of success of the teaching \nas well as a measure of effectiveness of \nthe learning (Boud 1995), but with many \n(often unresolved) issues regarding the \nreliability and validity of summative \nassessment methods (see Knight 2001 \nfor a discussion of these issues). Despite \nthese difficulties, summative assess-\nment can be meaningful in formal learn-\ning contexts where learning objectives \nand desired outcomes are well specified \nin advance. By contrast with formal \neducation, mobile, informal learning \ncan be both personal and elusive. The \nlearning may be personally initiated and \nstructured, such that it is not possible to \ndetermine in advance where the learn-\n58   International Journal of Mobile and Blended Learning, 1(2), 54-75, April-June 2009\nCopyright \u00a9 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global\nis prohibited.\ning may occur, nor how it progresses \nor what outcomes it produces. It may \nalso be difficult to track the progress \nof learning if it occurs across multiple \nsettings and technologies.\nAn alternative approach is to exam-\nine the experience for evidence which \nmight suggest that productive learning \nis taking place. For example, in the \ncontext of museum learning, Griffin \nand Symington (1998) suggest to watch \nfor instances where learners initiate and \nshow responsibility for their own learn-\ning (e.g. by writing, drawing, or taking \nphotos by choice; deciding where and \nwhen to move), are actively involved in \nlearning (e.g. by absorbed, close exami-\nnation of resources; or persevering with \na task), make links and transfer ideas and \nskills (e.g. by comparing evidence), and \nshare learning with experts and peers \n(e.g. by talking and gesturing; or asking \neach other questions). Adaptations of \nthe Critical Incidents method (Flanagan \n1954) provide one way to achieve this. \nFor example, activities of learners who \nwear radio microphones are videotaped \nat a discrete distance. The evaluators \nthen watch the videotapes to identify \nobservable critical incidents that appear \nto be breakthroughs (indicating produc-\ntive new forms of learning or important \nconceptual change) or breakdowns \n(where a learner is struggling with the \ntechnology, is asking for help, or appears \nto be labouring under a clear misun-\nderstanding). These incidents can be \nassembled into a compilation tape and \nreviewed with the learners for further \nelaboration (Vavoula et al. forthcom-\ning) or analysed as is (Anastopoulou \net al. 2008). \nAnother alternative is to focus on \nlearner perceptions of the learning ex-\nperience rather than learning outcomes \nin terms of cognitive gains. Attitude \nsurveys have been used extensively in \nthe mobile learning literature to measure \nlearner attitudes towards the technology \nand their enjoyment of the experience. \nSince attitudes are closely related to \nintrinsic motivation and learning agency \n(Hidi & Harackiewicz 2000), they can \nbe a reliable predictor of conditions for \neffective learning (though not necessar-\nily of learning outcomes). However, the \nmobile learning community has yet to \nproduce standardised attitude measure-\nment instruments such as those available \nin other fields (e.g. science learning - \nMoore & Sutman 1970).\nInformation useful in assessing \nlearning can also be found in learner-\ncreated artefacts, such as log files of \ncomputer activity or web access, the \nresults of online quizzes, learner-created \nmedia, and personal reflective docu-\nments such as blogs and e-portfolios. \nFurther work is needed to integrate these \ninto a revealing and valid assessment \nof learning. \nThe challenge of assessing learning \nis not unique to mobile learning and is \nnot easily solved. Although a learning \nexperience can be a well defined event \nwith a start and a finish, learning is an \nongoing, lifelong process of personal \ntransformation and, as such, requires \nlongitudinal, historical assessment.\nInternational Journal of Mobile and Blended Learning, 1(2), 54-75, April-June  2009   59\nCopyright \u00a9 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global \nis prohibited.\nCHallenGe 3: an etHiCal \nqUestion\nResearch ethics frameworks have \ngoverned research involving human \nsubjects for decades. With the increas-\ning use of the Internet as the research \nobject and medium, accounts of virtual \nresearch ethics prevailed, along with \nanalysis of differences in the nature \nof ethical issues confronted by virtual \nversus traditional research (Buchanan \n2004). The challenge for mobile learn-\ning evaluation is to translate these issues \ninto ethical guidelines appropriate for \nmobile contexts.\nThe evaluation of mobile learning \npresents particular ethical problems, \nbeyond those routinely associated with \na study of people and technology, i.e. \nensuring. A fundamental need is to \nexplicate the purpose and principles of \nthe evaluation within an ethical frame-\nwork. Ethics can arise from differing \nphilosophical foundations regarding the \nnature of \u2018reality\u2019 and the value of the \nscientific method in validating claims \nby analysis of objective data. \nWithin a modernist framework the \nresearcher strives for objectivity. En-\ngaging in postmodern research involves \nresearchers reflexively asking why they \nare doing this research in this way, what \nit is silent about, what gives it authority, \nand who is \u201cprivileged by it\u201d (Traxler \n& Bridges 2004:204).\nAs studies of learning move out of \nthe classroom into homes and outdoor \nlocations, so the evaluation will need \nto rely more on a combination of data \ncollected automatically by mobile de-\nvices (such as logs of user interaction, \ntime and location) and self-reports from \nlearners. These do not fit naturally to-\ngether, since they exemplify objectivist \nand postmodern approaches to the study \nof learning. For example, a current study \n(as yet unpublished) of children using \nmobile phones to create records of their \ndaily eating habits has found that some \nchildren deliberately avoid photograph-\ning unhealthy food items. This is not \nsimply a matter of treatment of missing \ndata items, but indicates deeper prob-\nlems of children\u2019s self-image, research \nby proxy using mobile devices, intrusion \ninto daily life, power and willing coop-\neration, and the interpretation of silence. \nAs mobile learning grows in scale and \nscope, evaluators must address the reli-\nability of evidence (particularly when \ncollected outside the lab or classroom) \nand difficulties of conducting scien-\ntifically rigorous studies as a basis for \nformulating evidence-based policy1.\nTraxler and Bridges indicate spe-\ncific ethical issues of evaluating mobile \nlearning, including: explaining the \nscope of mobile learning in a succinct \nand appropriate way, gaining informed \nconsent for novel forms of interac-\ntion (such as learning by SMS), and \nidentifying the contribution of learners \nacross multiple devices and contexts. \nOther issues include identifying the \nownership of material collected across \ncontexts (such as field trips), the rights \nof participants to know when and how \nthey are being monitored during their \ndaily lives, and possible health dangers \n60   International Journal of Mobile and Blended Learning, 1(2), 54-75, April-June 2009\nCopyright \u00a9 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global\nis prohibited.\nassociated with regular use of wireless \ntechnologies (Ahonen 2008; Patrick et \nal. 2008).\nObtaining informed consent can be \nproblematic: the previous sections de-\nscribed the vagueness of mobile learning \ncontext and the elusiveness of mobile \nlearning outcomes. When evaluators \nare uncertain of what will constitute \nthe mobile learning experience, how \naccurately can they inform the partici-\npants of what data is sought and why? \nAssuming that a vague description of \nthe requirements for participation is \nacceptable, how can learners consent \nto disclosing information about events \nthey currently do not know when, where \nand under what circumstances will take \nplace?\nEven if the essence of the evalu-\nation is successfully conveyed to the \nparticipants, and they consent to it, there \nare still important issues to consider \nrelating to the degree to which they will \nco-operate in practice \u2013 either in terms \nof disclosing all that might be relevant, \nor in terms of carrying out related prac-\ntical tasks such as synchronising their \nmobile devices as and when requested \n(Trinder et al. 2007).\nFurthermore, when asking partici-\npants to record their own learning (either \nthrough wearing\/carrying recording \nequipment or by keeping a written \nrecord of their learning), we are in es-\nsence making them co-researchers. In \ndoing so, we also need to ensure that \nthey are able themselves to follow the \nethics rules and regulations; and are \nmore legitimate research partners than \ninactive participants (Bakardjieva et \nal. 2004).\nA major challenge then for mobile \nlearning evaluation is to accurately \ninform participants, to ease their partici-\npation, and to build capacity in ethical \nresearch and mobile learning practice \nby providing them with appropriate eth-\nics training. In the process evaluators \nshould ask themselves how much they \nreally need to know, and investigate \nbest practices in safeguarding and dis-\nseminating sensitive personal data.\nCHallenGe 4: MoBile \nteCHnoloGY\nEvaluations of mobile learning often \nreference inherent limitations of mobile \ndevices, such as their small screens, \nshort battery lives, intermittent con-\nnectivity, and associated human factors, \nall of which affect their usability (see \ne.g. Corlett et al. 2005). As the focus of \nresearch shifts from the mobility of the \ntechnology to the mobility of the learner, \nadditional issues arise as learners move \nacross multiple, quickly-dating devices, \nboth personal and public, possibly over \nshort time periods in multiple loca-\ntions. Extracting learning interactions \nfrom this mesh of technology interac-\ntions requires synchronization of data \ncapture and analysis across multiple \ndevices and interfaces. Assessing the \nusability of the mobile technology and \nthe effectiveness of its integration with \nthe mobile learning practice remains a \nhigh priority for evaluation.\nInternational Journal of Mobile and Blended Learning, 1(2), 54-75, April-June  2009   61\nCopyright \u00a9 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global \nis prohibited.\nThus, challenges of mobile human-\ncomputer interaction stemming from \nthe complexity introduced by physi-\ncal movement and changing variables \n(Kjeldskov & Stage 2004) and the small \nscale and ubiquitous nature of mobile \ndevices (Hagen et al. 2005), add to the \nchallenges already facing mobile learn-\ning evaluation.\nCHallenGe 5: seeinG tHe \nBiGGer PiCtUre\nThere is a wealth of literature on the re-\nlation between information technology \nand institutional change (e.g. Fountain \n2001) and methods of performance \nmanagement such as the balanced \nscorecard (Kaplan & Norton 1996) and \nSix Sigma (Pande et al. 2000).  Becta, \nthe UK Government agency leading the \nintroduction of technology in education, \nprovides a strategy for the introduction \nand effective use of technology across \nall education sectors. This describes the \nprogression of an institution in confi-\ndence with technology from \u2018enabled\u2019, \nthrough \u2018capable\u2019 to \u2018confident\u2019 through \nintroduction of appropriate infrastruc-\nture, planning and leadership, learner \naccess to resources, and personalisation \nof learning.  Its Performance Framework \n(BECTA 2008:47) indicates the system-\nic changes needed to achieve the goals \nof improved personalised learning ex-\nperiences. These are: confident system \nleadership and innovation, technology \nconfident effective providers, engaged \nand empowered learners and enabling \ninfrastructure and processes.\nFor Higher Education (HE), Oliver \nand Harvey (2002) suggest four kinds \nof impact of educational technologies: \nimpact on students\u2019 learning, impact on \nindividual academics\u2019 practice, impact \non institution, and national impact. Also \nin the context of HE, Price and Oliver \n(2007) identify three types of impact \nstudies: anticipatory, ongoing and \nachieved. Anticipatory studies relate \nto pre-intervention intentions, opinions \nand attitudes; ongoing studies focus on \nanalysing processes of integration; and \nachieved studies are summative studies \nof technology no longer \u2018novel\u2019. Riley \n(2007) extends this impact framework \nby distinguishing between minor \nmodifications and culturally significant \nchanges in practice, and suggesting that \ndifferent kinds of change will emerge \nover different timescales.\nMobile learning evaluation has \nsimilar issues regarding impact. It \nneeds to examine how learning takes \nplace within a personal, socio-cultural \nand institutional context, to chart the \nprogression of institutions in their \nmaturity of support for learning with \nmobile technology, and examine the \nrelation between personal and insti-\ntutional learning. It needs to address \nhow the immediate learner experience \nwithin these contexts blends with or \nconfronts existing practices to lead to \nnew practices, by analysing this change \nprocess over extended periods of time. \nThese requirements necessitate an ex-\ntended view of the role of evaluation as \n62   International Journal of Mobile and Blended Learning, 1(2), 54-75, April-June 2009\nCopyright \u00a9 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global\nis prohibited.\na continual process of adjustment and \nfine-tuning.\nCHallenGe 6: forMal or \ninforMal?\nMobile learning is often defined in terms \nof the technology that mediates the \nlearning experience: if the technology \nis mobile, so is the learning. Mobility, \nhowever, is not an exclusive property \nof the technology, it also resides in \nthe lifestyle of the learner, who in the \ncourse of everyday life moves from \none context to another, switching loca-\ntions, social groups, technologies and \ntopics; and learning often takes place \ninconspicuously or is crammed in the \nshort gaps between these transitions. \nAlthough this view of learning is in-\nclusive of formal education contexts, \nit is particularly pertinent to everyday, \ninformal learning. \nNevertheless, characterising a learn-\ning experience as formal or informal \ncan be complicated. For example, is the \nlearning of pupils visiting a museum \n(largely considered an informal learning \nsetting) with their school (an irrefut-\nably formal learning setting) a case of \nformal or informal learning? There is \na large literature related to definitions \nof informal learning and related termi-\nnology, a review of which is beyond \nthe scope of this article. However, a \ngeneral tendency seems to be to define \ninformal learning in contrast to formal \nlearning, and formal learning in turn \nto be confined to learning that takes \nplace in educational settings. Colley et \nal. (2003) argue that \u201cseeing informal \nand formal learning as fundamentally \nseparate results in stereotyping and a \ntendency for the advocates of one to \nsee only the weaknesses of the other \u2026 \nIt is more sensible to see attributes of \ninformality and formality as present in \nall learning situations\u201d. They advocate \nfour groups of attributes: those related to \nthe learning process, to the location and \nsetting, to the learning purposes, and to \nthe learning content. They propose that \nattributes of in\/formality are interrelated \nin different ways in different learning \nsituations, and that those attributes and \ntheir interrelationships influence the \nnature and effectiveness of learning in \nany situation.\nUnderstanding such attributes of \nin\/formality and their interrelation-\nships in mobile learning is important \nfor evaluation. It is not only a case \nof analysing pre-existing practices in \nterms of processes, settings, purposes \nand content, but also of capturing how \nthe introduction of mobile learning \npractices, or new ways of supporting \nthem, can change the in\/formality of \nthe learning experience.\nPreCePts for MoBile \nlearninG eValUation\nThe challenges discussed in the previous \nsections translate into a set of basic pre-\ncepts for mobile learning evaluation:\nInternational Journal of Mobile and Blended Learning, 1(2), 54-75, April-June  2009   63\nCopyright \u00a9 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global \nis prohibited.\nP1. Capture and analyse learning in con-\ntext, with consideration of learner \nprivacy (challenges 1, 3)\nP2. Assess the usability of the technol-\nogy and how it affects the learning \nexperience (challenge 4)\nP3. Look beyond measurable cognitive \ngains into changes in the learning \nprocess and practice (challenge 2)\nP4. Consider organisational issues in the \nadoption of mobile learning practice \nand its integration with existing \npractices and understand how this \nintegration affects attributes of in\/\nformality (challenges 5, 6)\nP5. Span the lifecycle of the mobile \nlearning innovation that is evalu-\nated, from conception to full deploy-\nment and beyond (challenges 1-6)\nAs an illustration of how these \nmight guide evaluation in practice, the \nfollowing section presents an evalu-\nation framework for mobile learning \nand its application in the context of the \nMyartspace project. \nM3: a tHree-leVel  \nfraMeWorK for  \neValUatinG MoBile  \nlearninG\nMyartspace supports structured inquiry \nlearning through technology that con-\nnects learning in the classroom with \nlearning in museums and galleries. \nDetailed descriptions of the project \nand the evaluation process and out-\ncomes have been presented elsewhere \n(Sharples et al. 2007b; Vavoula et al. \n2006a; Vavoula et al. 2007; Vavoula et \nal. 2006b). In summary, Myartspace ad-\ndresses the problem of how to connect \na school museum trip with classroom \nactivities of planning and further study. \nIt enables school students to create \ntheir own interpretations of museum \nexhibits through descriptions, images \nand sounds they collect at the museum, \nwhich they then review, reflect upon and \nshare outside the museum. Before the \nvisit, the teacher will typically set an \nopen-ended question that the students \ncan answer by gathering and selecting \nevidence from the museum visit. On \narrival at the museum, students are \ngiven multimedia mobile phones run-\nning custom software which they can \nuse to collect multimedia presentations \nof exhibits, take photos, record sounds, \nor write text comments. This content is \ntransmitted by the phone into a time-\nordered collection on their personal \nweb space. Back at school, the students \ncan organise the material into online \ngalleries to present their findings in the \nclassroom and share with their friends \nand family. \nM3, the evaluation framework de-\nveloped in the context of Myartspace, \nfollowed the Lifecycle approach to \neducational technology evaluation \nproposed by Meek (2006). This places \nevaluation at the centre of the develop-\nment process, from the early stages of \ndesign to a final assessment of the de-\nployed technology in use. The Lifecycle \napproach can be matched to a sequential \nsystems development process (Royce \n64   International Journal of Mobile and Blended Learning, 1(2), 54-75, April-June 2009\nCopyright \u00a9 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global\nis prohibited.\n1970), with evaluations undertaken at \nthe end of each stage, or to iterative (Lar-\nman & Basili 2003) or socio-cognitive \nmethods (Sharples et al. 2002) where \nevaluation activities are undertaken \nat key points that are of most value to \nsupport the design process or inform \nstakeholders, with the outcomes of each \nevaluation guiding the next phase of the \nsystem development or feeding into an \niteration of an earlier phase.\nEvaluation under M3 is conducted \nat three levels:\n1. Micro level: which examines the \nindividual activities of the technol-\nogy users and assesses the usability \nand utility of the educational tech-\nnology system. For Myartspace the \nactivities included collecting ob-\njects through exhibit codes, making \nnotes, contacting people who had \ncollected a particular item, record-\ning audio, and taking pictures.\n2. Meso level: which examines the \nlearning experience as a whole, to \nidentify learning breakthroughs and \nbreakdowns. It also assesses how \nwell the learning experience inte-\ngrates with other related activities \nand experiences. For Myartspace, \nevaluation at this level involved \nexploring whether there was a suc-\ncessful connection between learning \nin the museum and the classroom, as \nwell as identifying critical incidents \nin the museum that reveal new pat-\nterns and forms of learning or where \nlearning activity is impeded. \n3. Macro level: which examines \nthe impact of the new technology \non established educational and \nlearning practices and institutions. \nFor Myartspace this related to the \norganisation of school museum \nvisits. The evaluation at this level \nexamined the appropriation of the \nnew technology by teachers, the \nemergence of new museum prac-\ntices in supporting school visits, \nand how these related to the original \nproject visions.\nThe development of Myartspace \ncomprised four broad phases: (1) \nRequirements analysis, to establish the \nrequirements for the socio-technical \nsystem (the users and their interactions \nwith technology) and specify how \nit would work, through consultation \nwith the different stakeholder groups; \n(2) Design of the user experience and \ninterface; (3) Implementation of the \nservice; and (4) Deployment of the \nservice. These are compatible with an \nAgile Development approach (Beck et \nal. 2001) where requirements can evolve \nthroughout the development process \nto take account of the evaluations \nof usability, learning effectiveness \nand institutional adoption. Thus, \nthe requirements analysis persisted \nthroughout the project lifecycle, and \ncovered all three levels of analysis \n(micro, meso and macro). \nFigure 1 illustrates this gradual \nintroduction of evaluation activities \nat the three framework levels over all \nproject phases. The horizontal axis in \nInternational Journal of Mobile and Blended Learning, 1(2), 54-75, April-June  2009   65\nCopyright \u00a9 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global \nis prohibited.\nFigure 1 depicts time; the change of \nfocus development phase over time \nis shown, as is the persistence of \nrequirements analysis throughout the \nproject lifecycle. The shaded areas \nrepresent activities for requirements \nanalysis (dark gray) and evaluation \nat the three levels during design, \nimplementation and deployment (all \nother shades).\nThe emphasis  on level  of \nrequirements analysis changes during \nthe development process. At the start \nof a project, the requirements analysis \nmust take account of all levels to set \ninitial requirements for an educational \nexperience that integrates technology, \neffective learning and institutional \nsupport. As the project progresses, the \ntechnology matures, so that changes \nto requirements become focused on \nthe learning context and institutional \nadoption. At the end of the project, the \nrequirements have been finalised and \nare evaluated at all levels.\nThe emphasis on level of evaluation \nalso changes during the development \nprocess. Early evaluations at micro \nlevel inform the user interface and \nhuman-technology interactions. Once \nthe technology is robust enough to al-\nlow assessment of educational value, \nevaluation activities at the meso level \nare introduced during the implementa-\ntion phase. Similarly, the macro level \nrequires that the technology is in place \nand used for long enough to establish its \neffects on e.g. school museum visiting \npractice, so evaluation activities at the \nmacro level may be introduced during \nthe deployment phase. \nFigure 1. Evaluation activities at the three levels over the project phases\nMacro \nevaluation \nMeso \nevaluation \nMicro \nevaluation \nTechnology robust enough for \nevaluation of learning  \nService deployed long \nenough to assess impact\nSpecify requirements \nDesign\nImplement\nDeploy \nProject development process \n66   International Journal of Mobile and Blended Learning, 1(2), 54-75, April-June 2009\nCopyright \u00a9 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global\nis prohibited.\nTo establish the value of the service \nat each of the three levels, evaluation \nactivities explore the gap between \nexpectations and reality and uncover \nunforeseen processes and outcomes. \nThis is enacted in two stages of data \ncollection and a stage of analysis:\n\u2022 Stage 1: collect data about what \nis supposed to happen at a level. \nUser expectations at each level \ncan be captured through interviews \nwith users (e.g. teachers, students, \nmuseum staff) and by analysing \ntechnical requirements specifica-\ntions, user documentation, training \nsessions and lesson materials. \n\u2022 Stage 2: collect data about what ac-\ntually happened at a level. The user \nexperience is documented through \nobservations and video and audio \nrecordings to establish the reality \nof technology use for the different \nusers. \n\u2022 Stage 3: examines the gaps be-\ntween user expectations and reality \nthrough a combination of reflective \ninterviews with users and critical \nanalysis of the data collected in \nstages 1 and 2.\nIn summary, M3 follows a Life-\ncycle approach of continuous strategic \nevaluation to guide an agile approach \nto software development and to inform \nstakeholders in the development pro-\ncess. It assesses the evolving design \nand implementation at three levels, of \nusability, educational effectiveness and \ninstitutional adoption. For each level the \nevaluation relates what should happen \n(through interviews with stakeholders \nand examination of documents) to what \nactually happens (through observation \nof user experience) and examines any \ngaps between expectation and reality \nas evidence of a need to modify re-\nquirements, design, implementation, or \ndeployment. These findings guide the \nnext phase of the system development \nor feed into an iteration of an earlier \nphase. \nTable 2 summarises requirements \nanalysis and evaluation activities and \nthe respective data collection methods \nin the Myartspace project at each level \nof M3, for all project phases. \nFurthermore, M3 provided an \nefficient way to structure data analysis \nfor the evaluation of Myartspace, \nallowing the documentation of the \npotential of such a service. Successes \nand failures of the service at all levels \n(micro, meso and macro) were identified, \nalong with inter-level influences. Table \n3 outlines the data analysis processes for \nthe data collected during stages 1 and \n2 of the requirements and evaluation \nactivities.\nFigure 2 gives an illustration of how \nM3 guided data analysis. It presents a \nsample of data snippets that were gath-\nered through observations, interviews, \nand system logs. Snippet 1 shows that \nthe children have created large amounts \nof content by taking photographs, re-\ncording sounds and writing comments. \nThis suggests that at the micro level, \ncreating and collecting items is a quick \nand easy task for them. Examination at \nInternational Journal of Mobile and Blended Learning, 1(2), 54-75, April-June  2009   67\nCopyright \u00a9 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global \nis prohibited.\nTable 2. Methods used for data collection during requirements analysis and \nevaluation activities at each level, for each project phase\ncontinued on following page\nthe meso level reveals that this ease of \nuse may not result in productive learning \nunless it is accompanied by creativity \nand sense of ownership, as exemplified \nin snippet C. This is an example of \nsynergy between the two levels.\nAt the micro level, students are \nnot able to annotate their photographs \nand the audio clips they recorded with \nnotes describing what they were about \nor why they were recorded. Although \nthey could create text notes, such notes \ncould not be directly associated with \nphotos or audio clips. This was puzzling \nfor some students, who expected to be \nable to record this metadata (snippet \nB). Analysis at the meso level revealed \nfrustration back in the classroom when \nstudents were trying to interpret their \ncollections (snippet D). This is an ex-\nample of a micro-level problem that \nmigrates to the meso level, affecting \nthe students\u2019 learning. Possible fixes to \nthis problem can be placed either at the \nmicro-level (e.g. changing the system \nto support annotation of photographs) \nor at the meso-level by giving advice \nto the students in effective techniques \nsuch as reading an exhibit label into the \nphone after taking a photograph of it, \nData collection for requirements and evaluation activities Level Phase\nRequirements analysis\nStage 1: \u2018expectations\u2019 data collection\nScoping study of previous projects and related recommendations\nConsultation workshop on \u2018User Experience\u2019 to establish \nrequirements\nAll Requirements\nStage 2: \u2018reality\u2019 data collection\nData supplied by evaluation analysis All Requirements\nHeuristic Evaluations (examining how system designs compare to \nexpectations re established design heuristics)\nStage 1: \u2018expectations\u2019 data collection\nEstablished design heuristics Micro All\nStage 2: \u2018reality\u2019 data collection\nExperts undertaking heuristic evaluation Micro All\nTechnical testing prior to trials\nStage 1: \u2018expectations\u2019  data collection\nData supplied by system requirements Micro Implement\nStage 2: \u2018reality\u2019 data collection\nSystem performance tests Micro Implement\n68   International Journal of Mobile and Blended Learning, 1(2), 54-75, April-June 2009\nCopyright \u00a9 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global\nis prohibited.\nTable 2. continued\nFull scale user trial (Key Stage 2 class visits Myartspace museum) All Implement\/Deploy\nStage 1: \u2018expectations\u2019  data collection\nExamine system documentation (Teacher\u2019s Pack and Lesson Plans, \nonline help) for descriptions of functionality\nInterview teacher prior to lesson to assess level of knowledge and \nexpectations for functionality\nObserve training sessions at museum and school to document how \nfunctionality is described to teachers\/students.\nStudent questionnaires regarding expectations of system functional-\nity in forthcoming lesson\nMicro Implement\/Deploy\nAnalyse description of educational experience based on Teacher\u2019s \nPack and Lesson Plans\nInterview teachers and museum educators prior to lessons about what \nthey have planned for the students\u2019 learning experience\nObserve teachers and museum educators while presenting learning \nexperience to students in the classroom\/museum\nStudent questionnaires regarding expectations of learning experi-\nence in forthcoming lesson\nMeso Implement\/Deploy\nAnalyse descriptions in service promotion materials, original pro-\nposal, minutes of early project meetings\nInterviews with stakeholders to elicit initial expectations for impact \nof service\nMacro Deploy\nStage 2: \u2018reality\u2019 data collection\nObserve lesson to establish actual teacher and student experience \nof functionality\nInterview teacher after the lesson to clarify experience of func-\ntionality\nQuestionnaire and focus groups with students after the lesson to \ncapture experience of functionality\nMicro Implement\/Deploy\nObserve educational experience in museum\/classroom\n\u2022 Note critical incidents that show new forms of learning or edu-\ncational interaction\n\u2022 Note breakdowns\nInterviews\/focus groups with teachers, museum educators, students \non educational experience in museum\/classroom\nMeso Implement\/Deploy\nReview of press coverage and interviews with stakeholders to docu-\nment impact\/transformations effected by the service Macro Deploy\nsomething that students were actually \nobserved doing.\nA final example comes from the \nanalysis of snippets 2 and A. As men-\ntioned previously, snippet 2 suggests \nthat creating and collecting items is \na quick and easy task at the micro \nlevel. Snippet A, however, suggests \nthat decomposing and interpreting the \ncollected content back in the classroom \ntakes significantly longer, which re-\nsulted in students not managing to go \nthrough all their collected items during \nthe post-visit lesson. This is an example \nof how omitting to resonate the micro \nand meso levels might lead to problems \nin the meso level. Possible fixes to this \nproblem can be placed at any of the three \nInternational Journal of Mobile and Blended Learning, 1(2), 54-75, April-June  2009   69\nCopyright \u00a9 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global \nis prohibited.\nData analysis for requirements and evaluation activities Level Phase\nRequirements analysis\nStage 3: data analysis\nWorkshop to finalise educational and user requirements\nRevisions of requirements in light of evaluation findings\nAll Requirements\nHeuristic Evaluations \nStage 3: data analysis\nAnalysis of expert reports and production of (re)design recommendations Micro All\nTechnical testing prior to trials\nStage 3: data analysis\nAnalysis of performance data against requirements Micro Implement\nFull scale user trial All I m p l e m e n t \/Deploy\nStage 3: data analysis\nCapture expectations-reality gaps in terms of user experience of functional-\nity through reflective interpretation of documentation analysis in the light of \nobservations; through interviews and focus groups with teachers\/students; and \nthrough critical incident analysis with students.\nMicro I m p l e m e n t \/Deploy\nCapture expectations-reality gaps in terms of educational experience through \nreflective interpretation of documentation analysis and observations; through \ninterviews\/focus groups with teachers, students, museum educators; and through \ncritical incident analysis with students.\nMeso I m p l e m e n t \/Deploy\nReflective analysis of expectations-reality gaps in terms of service impact Macro Deploy\nTable 3. Methods used for data analysis during requirements and evaluation \nactivities at each level, for each project phase\nFigure 2. Data snippets gathered through observations (C, D), interviews (A, \nB), and system logs (1, 2)\nGroup average Class\nPhotographs 33 364 \nSounds 11 121 \nWritten comments 7 77 \nCollected objects 7 75 \nTOTAL 58 637 \nObjects created \/ collected during museum visit \n\u201cI expect to be able to record \nwhat pictures are of\u201d \nStudent \n\u201cHow will I know what \nthis photo is about?\u201d \nStudent \n\u201c-It has a code \n- I want to take my \nown picture\u201d \nStudents dialogue \n\u201cA student can effectively \nprocess 5-10 items during a \nsingle post-visit lesson\u201d \nTeacher \n70   International Journal of Mobile and Blended Learning, 1(2), 54-75, April-June 2009\nCopyright \u00a9 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global\nis prohibited.\nlevels. At the micro level, we can enforce \nan upper limit on the number of items a \nstudent can collect; or we might want to \nsimplify the web interface so that online \nprocessing of items takes less time. At \nthe meso level, we might try to educate \nstudents in regulating their collecting \npractices. At the macro level, we might \ntry to influence the teachers\u2019 practice so \nthat they include more than one post-\nvisit lesson in their planning.\nDisCUssion anD  \nConClUsion\nM3 provides a structured format \nto assess usability, educational and \norganisational impact, and their inter-\nrelationships. Table 4 describes how \nit follows the precepts for mobile \nlearning evaluation presented earlier \nin the article.\nThe six challenges in mobile learn-\ning evaluation identified in this article \nare a direct consequence of the complex \nnature of mobile learning as we have \ncome to understand it, as a social rather \nthan technical phenomenon of people \non the move constructing spontaneous \nlearning contexts and advancing through \neveryday life by negotiating knowledge \nand meanings through interactions \nwith settings, people and technology. \nIn this article we construed these chal-\nlenges into precepts for evaluation and \npresented M3 as the implementation \nof one interpretation of these precepts; \nother frameworks previously proposed \nin the literature for the design (Ryu & \nParsons 2008) and\/or evaluation of \nmobile learning (Taylor 2004; Taylor \nPrecept Framework qualities\nP1. Capture and analyse learning in context; with \nconsideration of learner privacy\nIlluminates learning activities and contexts at different \nlevels of detail\nInvolves learners and teachers as informed participants in \nthe evaluation process\nP2. Assess the usability of the technology and how \nit affects the learning experience\nMicro-level (usability) evaluation activities are linked \nwith evaluation activities at the meso and macro levels \n(educational effectiveness and institutional adoption)\nThe focus on interaction puts equal emphasis on the learn-\ners and the technology\nP3. Look beyond cognitive gains into changes in \nthe learning process and practice\nRelates the intended learning processes and outcomes \nto observed activities and examines the gaps between \nexpectation and reality\nP4. Consider organisational issues in the adoption \nof mobile learning practice and its integration with \nexisting practices and understand how this integra-\ntion affects attributes of in\/formality\nCan analyse individual interactions, educational processes \nand organisational change\nCan be applied across formal and informal settings\nP5. Span the lifecycle of the mobile learning in-\nnovation that is evaluated, from conception to full \ndeployment and beyond\nIntegrates with a Lifecycle (Meek 2006) approach to \nevaluation\nTable 4. M3 evaluation framework - fitness for purpose\nInternational Journal of Mobile and Blended Learning, 1(2), 54-75, April-June  2009   71\nCopyright \u00a9 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global \nis prohibited.\net al. 2006; Traxler & Kukulska-Hulme \n2005) can be seen as different interpre-\ntations of the same precepts. We view \nM3\u2019s main contributions to this growing \nbody of knowledge and experience in \nmobile learning design and evaluation \nto be (a) its multi- and cross-level focus \non individual interactions, educational \nprocesses and organisational change; \n(b) the way it combines with Lifecycle \nevaluation approaches to weave require-\nments analysis and evaluation into the \nwhole development cycle, thereby \nemphasising experience-centred over \ntechnology-centred development; and \n(c) its focus on experience gains over \ncognitive gains alone. These qualities \nof M3 allude to Traxler\u2019s (forthcoming) \nproposal for alignment of our modern-\nist conceptions of evaluation with the \npostmodern reality of mobile technolo-\ngies and learning. \nThe application of M3 in the context \nof Myartspace was successful and of-\nfered valuable insights to the project. \nAlthough we believe the framework is \ntransferable to other mobile learning \ncontexts, it needs further development \nto address, for example, contexts with \nhigher ethical concerns, or contexts \nwhere it is challenging to align the \nrequirements analysis and evaluation \nactivities with the objectives and ethos \nof the project. The outcomes of an \nevaluation based on this framework can \nfeed directly into system design, as has \nhappened in the case of Myartspace. \nPerhaps with suitable extensions the \nframework could serve the design \nprocess more directly, guiding mobile \nlearning designers to interpret and \nimplement requirements for learning \nacross self-constructed contexts.\naCKnoWleDGMent\nWe are grateful to Peter Lonsdale, Julia \nMeek and Paul Rudman for their invalu-\nable contribution in the design of the \nevaluation of Myartspace and the de-\nvelopment of the evaluation framework. \nWe would like to thank the Department \nfor Culture, Media and Sport for the \nfunding of Myartspace through Culture \nOnline. Myartspace was designed and \ndeveloped by The Sea (http:\/\/www.the-\nsea.com); the service was branded as \nOOKL in 2007 and is now commercially \navailable (www.ookl.org.uk) and used \nin a number of sites, including the Kew \nGardens in London. Last, but certainly \nnot least, we would like to thank all the \nstudents, teachers, and museum educa-\ntors who took part in our trials, with \nspecial thanks to Bryony Kelly from \nthe D-Day museum for her enthusiasm \nabout Myartspace and her continuous \nsupport during the user trials.\nreferenCes\nAhonen, M. (2008). Sustainable Wireless \nComputing \u2013 How Public Information Sys-\ntems Can Be Designed to Reduce Exposure. \nIn Proceedings of 31st IRIS Scandinavian \nInformation Systems Conference, \u00c5re, \nSweden\nAnastopoulou, S., Sharples, M., Wright, \nM., Martin, H., Ainsworth, S., Benford, S., \n72   International Journal of Mobile and Blended Learning, 1(2), 54-75, April-June 2009\nCopyright \u00a9 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global\nis prohibited.\nCrook, C., Greenhalgh, C., & O\u2019Malley, \nC. (2008). Learning 21st Century Science \nin Context with Mobile Technologies. In \nTraxler, J., Riordan, B. and Dennett, C. \n(Eds.), Proceedings of mLearn 2008: The \nbridge from text to context, Wolverhamp-\nton, UK, University of Wolverhampton, \n(pp. 12-19).\nBakardjieva, M., Feenberg, A., & Goldie, \nJ. (2004). User-Centered Internet Research: \nThe Ethical Challenge. In Buchanan, E.A. \n(Ed.), Readings in Virtual Research Ethics: \nIssues and Controversies, Idea Group Inc \n(IGI), (pp. 338-350).\nBeck, K., Beedle, M., van Bennekum, A., \nCockburn, A., Cunningham, W., Fowler, \nM., Grenning, G., Highsmith, J., Hunt, A., \nJeffries, R., Kern, J., Marick, B., Martin, \nR.C., Mellor, S., Schwaber, K., Sutherland, \nJ., & Thomas, D. (2001). Manifesto for \nAgile Software Development. Available \nonline: www.agilemanifesto.org. Accessed on \n22\/01\/09.\nBECTA (2008). Harnessing Technology: \nNext Generation Learning 2008-14. Avail-\nable online: http:\/\/publications.becta.org.uk\/\ndisplay.cfm?resID=38751&page=1835.\nBlack, P., & Wiliam, D. (1998a). Assess-\nment and Classroom Learning. Assessment \nin Education: Principles, Policy & Prac-\ntice, 5(1), 7-74.\nBlack, P. & Wiliam, D. (1998b). Inside \nthe black box: raising standards through \nclassroom assessment. Phi Delta Kappan, \n80, 139-148.\nBoud, D. (1995). Assessment and learning: \ncontradictory or complementary? In P. \nKnight (Ed.), Assessment for Learning in \nHigher Education. London, Kogan Page, \n(pp. 35-48).\nBuchanan, E. A. (Ed.) (2004). Readings in \nVirtual Research Ethics: Issues and Con-\ntroversies. Idea Group Inc (IGI).\nClough, G., & Jones, A. (2006). The Uses \nof PDAs and Smartphones in Informal \nlearning. In Proceedings of MLearn 2006, \nBanff, Canada. \nColley, H., Hodkinson, P., & Malcom, J. \n(2003). Informality and formality in learn-\ning. Learning and Skills Research Centre. \n1492\/11\/03\/500.\nCorlett, D., Sharples, M., Chan, T., & Bull, \nS. (2005). Evaluation of a Mobile Learn-\ning Organiser for University Students. \nJournal of Computer Assisted Learning, \n21, 162-170.\nFlanagan, J. C. (1954). The critical incident \ntechnique. Psychological Bulletin, 51(4), \n327-358.\nFountain, J. E. (2001). Building the Vir-\ntual State: Information Technology and \nInstitutional Change, Brookings Institu-\ntion Press.\nGreenhalgh, C., French, A., Tennent, P., \nHumble, J., & Crabtree, A. (2007). From \nReplayTool to Digital Replay System. In \nProceedings of e-Social Science Confer-\nence, Ann Arbor, Michigan, USA.\nGriffin, J., & Symington, D. (1998). Finding \nEvidence of Learning in Museum Settings. \nIn Proceedings of Evaluation and Visitor \nResearch Special Interest Group Confer-\nence \u2018Visitors Centre Stage: Action for the \nFuture\u2019, Canberra.\nHagen, P., Robertson, T., Kan, M., & Sadler, \nK. (2005). Emerging research methods for \nunderstanding mobile technology use. In \nProceedings of CHISIG, Australia, (pp. \n1-10).\nInternational Journal of Mobile and Blended Learning, 1(2), 54-75, April-June  2009   73\nCopyright \u00a9 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global \nis prohibited.\nHidi, S., & Harackiewicz, J.M. (2000). Mo-\ntivating the Academically Unmotivated: A \nCritical Issue for the 21st Centrury. Review \nof Educational Research, 70(2), 151-179.\nHsi, S. (2007). Conceptualizing Learning \nfrom the Everyday Activities of Digital \nKids. International Journal of Science \nEducation, 29(12), 1509 - 1529.\nKaplan, R. S., & Norton, D. P. (1996). Us-\ning the balanced scorecard as a strategic \nmanagement system. Harvard Business \nReview, (pp. 75\u201385).\nKjeldskov, J., & Stage, J. (2004). New Tech-\nniques for Usability Evaluation of Mobile \nSystems. International Journal of Human-\nComputer Studies, 60, (pp. 599-620).\nKnight, P. (2001). A Briefing on Key Con-\ncepts: Formative and summative, criterion \nand norm-referenced assessment. LTSN \nGeneric Centre.\nLarman, C., & Basili, V.R. (2003). Itera-\ntive and Incremental Development: A Brief \nHistory. Computer Supported Cooperative \nWork, 36(6), 47-56.\nMeek, J. (2006). Adopting a Lifecycle \nApproach to the Evaluation of Computers \nand Information Technology. PhD The-\nsis, School of Electronic, Electrical and \nComputer Engineering, The University of \nBirmingham.\nMoore, R. W., & Sutman, F. X. (1970). The \ndevelopment, field test and validation of an \ninventory of scientific attitudes. Journal of \nResearch in Science Teaching, 7, 85\u201394.\nOliver, M., & Harvey, J. (2002). What does \n\u2018impact\u2019 mean in the evaluation of learning \ntechnology? Educational Technology & \nSociety, 5(3), 18-26.\nPande, P. S., Neuman, R. P., & Cavanagh, \nR. R. (2000). The Six Sigma Way: How \nGE, Motorola, and Other Top Companies \nare Honing Their Performance. McGraw-\nHill.\nPapadimitriou, I., Tselios, N., & Komis, \nV. (2007). Analysis of an informal mobile \nlearning activity based on activity theory. \nIn Vavoula, G.N., Kukulska-Hulme, A. \nand Pachler, N., Proceedings of Workshop \nResearch Methods in Informal and Mobile \nLearning, WLE Centre, Institute of Educa-\ntion, London, UK, (pp. 25-28).\nPatrick, K., Griswold, W., Raab, F. & Intille, \nS. (2008). Health and the Mobile Phone. \nAmerican Journal of Preventive Medicine, \n35(2), 177-181.\nPrice, S., & Oliver, M. (2007). A Framework \nfor Conceptualising the Impact of Technol-\nogy on Teaching and Learning. Educational \nTechnology & Society, 10(1), 16-27.\nRiley, D. (2007). Educational Technology \nand Practice: Types and Timescales of \nChange. Educational Technology & Society, \n10, 85-93.\nRoto, V., Oulasvirta, A., Haikarainen, T., \nKuorelahti, J., Lehmuskallio, H. & Nyys-\nsonen, T. (2004). Examining mobile phone \nuse in the wild with quasi-experimentation. \nHIIT Technical Report 2004-1. Available \nonline: www.hiit.fi\/publications.\nRoyce, W. W. (1970). Managing the De-\nvelopment of Large Software Systems. \nIn Proceedings of IEEE WESCON, (pp. \n1-9).\nRyu, H., & Parsons, D. (2008). Designing \nLearning Activities with Mobile Technolo-\ngies. In Ryu, H. and Parsons, D. (Eds.), \nInnovative Mobile Learning: Techniques \n74   International Journal of Mobile and Blended Learning, 1(2), 54-75, April-June 2009\nCopyright \u00a9 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global\nis prohibited.\nand Technologies, Idea Group Inc (IGI), \n(pp. 1-20).\nScriven, M. (1967). The methodology \nof evaluation. In R. W. Tyler & R. M. \nGagne, & M. Scriven (Eds.), Perspectives \nof Curriculum Evaluation. Chicago, Rand \nMcNally, (pp. 39-83).\nSharples, M., Taylor, J., & Vavoula, G. \n(2007a). A Theory of Learning for the \nMobile Age. In Andrews, R. and Haythorn-\nthwaite, C. (Eds.), The Sage Handbook of \nE-learning Research. London, Sage, (pp. \n221-47).\nSharples, M., Lonsdale, P., Meek, J., Rud-\nman, P. D., & Vavoula, G. N. (2007b). \nAn Evaluation of MyArtSpace: a Mobile \nLearning Service for School Museum \nTrips. In Proceedings of mLearn 2007, \nMelbourne, Australia. \nSharples, M., Jeffery, N., Du Boulay, J. B. \nH., Teather, D., Teather, B., & Du Boulay, \nG. H. (2002). Socio-cognitive engineering: \na methodology for the design of human-\ncentred technology. European Journal of \nOperational Research, 136(2), 310-323.\nSmith, H., Heldt, S., Fitzpatrick, G., Hui Ng, \nK., Benford, S., Wyeth, P., Walker, K., Un-\nderwood, J., Luckin, R., & Good, J. (2007). \nReconstructing an informal mobile learning \nexperience with multiple data streams. In \nVavoula, G.N., Kukulska-Hulme, A. and \nPachler, N., Proceedings of Workshop \nResearch Methods in Informal and Mobile \nLearning, WLE Centre, Institute of Educa-\ntion, London, UK., (pp. 29-34).\nTaylor, J. (2004). A task-centred approach \nto evaluating a mobile learning environment \nfor pedagogical soundness. In Proceedings \nof MLearn2004, London, UK, Learning \nand Skills Development Agency, (pp. \n167-171).\nTaylor, J., Sharples, M., O\u2019Malley, C., \nVavoula, G., & Waycott, J. (2006). To-\nwards a Task Model for Mobile Learning: \na Dialectical Approach. International \nJournal of Learning Technology, 2(2\/3), \n(pp. 138-158).\nTraxler, J. (forthcoming). Mobile Learn-\ning Evaluation: The Challenge of Mobile \nSocieties. In G. Vavoula, N. Pachler, & A. \nKukulska-Hulme (Eds.), Researching Mo-\nbile Learning: Frameworks, Methods and \nResearch Designs. Oxford, Peter Lang.\nTraxler, J., & Bridges, N. (2004). Mobile \nLearning - The Ethical and Legal Chal-\nlenges. In Proceedings of MLEARN 2004, \nBracciano, Italy.\nTraxler, J., & Kukulska-Hulme, A. (2005). \nEvaluating Mobile Learning: Reflections \non Current Practice. In Proceedings of \nMLEARN 2005, Cape Town, South Af-\nrica.\nTrinder, J., Roy, S., & Magill, J. (2007). \nHave You Got Your PDA With You?...\nDenials and Accusations. In G. N.Vavoula, \nA. Kukulska-Hulme, & N. Pachler, (Eds.), \nProceedings of Workshop Research Meth-\nods in Informal and Mobile Learning, \nInstitute of Education, London, UK.\nVavoula, G., Meek, J., Sharples, M., Lons-\ndale, P. & Rudman, P. (2006a). A lifecycle \napproach to evaluating MyArtSpace. In \nS. Hsi, Kinshuk, T. Chan, & D. Sampson \n(Eds.), Proceedings of 4th International \nWorkshop of Wireless, Mobile and Ubiqui-\ntous Technologies in Education (WMUTE \n2006 ), Athens, Greece, IEEE Computer \nSociety, (pp. 18-22).\nInternational Journal of Mobile and Blended Learning, 1(2), 54-75, April-June  2009   75\nCopyright \u00a9 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global \nis prohibited.\nVavoula, G., Sharples, M., Rudman, P., \nLonsdale, P., & Meek, J. (2007). Learning \nBridges: a role for mobile technologies in \neducation. Educational Technology Maga-\nzine, XLVII(3), 33-37.\nVavoula, G., Sharples, M., Rudman, P., \nMeek, J., & Lonsdale, P. (forthcoming). \nMyartspace: Design and evaluation of sup-\nport for learning with multimedia phones \nbetween classrooms and museums. Com-\nputers and Education.\nVavoula, G., Sharples, M., Rudman, P., \nMeek, J., Lonsdale, P., & Philips, D. \n(2006b). Museum explorations in phy-\nsical, personal and virtual space through \nMyArtSpace. In Proceedings of mLearn \n2006, Banff, Canada.\nVavoula, G. N. (2005). WP4: A Study of \nMobile Learning Practices. MOBIlearn \ndeliverable D4.4. Available online: http:\/\/\nwww.mobilearn.org\/download\/results\/pu-\nblic_deliverables\/MOBIlearn_D4.4_Final.\npdf\u2019\nenDnote\n1 We are grateful to a reviewer for \nbringing this aspect of evaluation \nto our attention.\n"}