{"doi":"10.1080\/0968776030110102","coreId":"14216","oai":"oai:generic.eprints.org:395\/core5","identifiers":["oai:generic.eprints.org:395\/core5","10.1080\/0968776030110102"],"title":"Evaluating complex digital resources","authors":["Blake, Canan","Davies, Clare","Jones, Ann","Morris, Erica","Scanlon, Eileen"],"enrichments":{"references":[{"id":197690,"title":"An evaluation model for a digital library services tool',","authors":[],"date":"2002","doi":"10.1145\/544220.544298","raw":"Dorward, J., Reinke, D., and Recker, M. (2002), 'An evaluation model for a digital library services tool', in Proceedings of the Joint Conference on Digital Libraries, New York: ACM, 322-3.","cites":null},{"id":197695,"title":"Barriers to the use of computer assisted learning',","authors":[],"date":"1982","doi":"10.1111\/j.1467-8535.1982.tb00441.x","raw":"Jones, A. and O'Shea, T. (1982), 'Barriers to the use of computer assisted learning', British Journal of Educational Technology, 3 (13), 207-17.","cites":null},{"id":197689,"title":"CTIGGM (Computers in Teaching Initiative: Geography, Geology and Meteorology)","authors":[],"date":"1998","doi":null,"raw":"CTIGGM (Computers in Teaching Initiative: Geography, Geology and Meteorology) (1998), 'WWW gateway for teaching and learning resources', http:\/\/eee.geog.le.ac.uk\/cti\/ Davies, C. (1998), 'Using digital geographic maps in distance learning', Computers and Learning Research Group Technical Report No. 180, Open University.","cites":null},{"id":1880271,"title":"Enriching accounts of computer supported 14Avr-J Volume 11 Number I collaboration by using video data',","authors":[],"date":"2002","doi":"10.1080\/0968776030110202","raw":"Blake, C. and Scanlon, E. (2002), 'Enriching accounts of computer supported 14Avr-J Volume 11 Number I collaboration by using video data', Research Proceedings of ALT-C Conference, Sunderland, UK, 10-12 September 2002.","cites":null},{"id":197686,"title":"Enriching accounts of computer supported Avr-J Volume 11 Number I collaboration by using video data',","authors":[],"date":"2002","doi":"10.1080\/0968776030110202","raw":null,"cites":null},{"id":1042788,"title":"Establishing Relationships between Networked Technologies and Attainment,","authors":[],"date":"2000","doi":null,"raw":"(2000), Establishing Relationships between Networked Technologies and Attainment, Impact2 Preliminary Report, http:\/\/www.becta.org.uk\/research\/reports\/docs\/ impact2_ prelim1.pdf.","cites":null},{"id":1042784,"title":"Evaluating CAL at the Open University: 15 years on',","authors":[],"date":"1996","doi":"10.1016\/0360-1315(95)00064-x","raw":"Jones, A., Scanlon, E., Tosunoglu, C., Ross, S., Butcher, P., Murphy, P. and Greenberg, J. (1996), 'Evaluating CAL at the Open University: 15 years on', Computers and Education, 26(1-3), 5-15.","cites":null},{"id":1042789,"title":"Evaluating communication and information technologies: a toolkit for practitioners',","authors":[],"date":"1998","doi":null,"raw":"Oliver, M., and Conole, G. (1998), 'Evaluating communication and information technologies: a toolkit for practitioners', Active Learning 8, 3-8.","cites":null},{"id":1042792,"title":"Evaluating information and communication technologies for learning in FE',","authors":[],"date":"2000","doi":"10.3402\/rlt.v8i3.12005","raw":"Scanlon, E., Jones, A., Calder, X, Barnard, J. and Thompson, J. (2000) 'Evaluating information and communication technologies for learning in FE', Educational Technology and Society, 3 (4), 101-7.","cites":null},{"id":1042790,"title":"Evaluating the effectiveness of multimedia computer modules as enrichment exercises for introductory human geography',","authors":[],"date":"1997","doi":"10.1080\/03098269708725408","raw":"Proctor, J. and Richardson, A. (1997), 'Evaluating the effectiveness of multimedia computer modules as enrichment exercises for introductory human geography', Journal of Geography in Higher Education, 21 (1), 41-55. Purves, R., Medycki-Scott, D., Blake, C., Fairbain, D. and Mackaness, W. (forthcoming), 'Delivering customisable e-learning on spatial data to a multidisciplinary audience', submitted to Journal of Geography in Higher Education.","cites":null},{"id":1042786,"title":"How Do Students Learn: The UNCAL Evaluation, occasional publication no 5, Centre for Applied Research in Education,","authors":[],"date":"1977","doi":null,"raw":"Kemmis, S., Atkins, R. and Wright, E. (1977), How Do Students Learn: The UNCAL Evaluation, occasional publication no 5, Centre for Applied Research in Education, University of East Anglia.","cites":null},{"id":197691,"title":"Integrative evaluation: an emerging role for classroom studies of CAL',","authors":[],"date":"1996","doi":"10.1016\/0360-1315(95)00068-2","raw":"Draper, S., Brown, M., Henderson, F. and McAteer, E. (1996), 'Integrative evaluation: an emerging role for classroom studies of CAL', Computers and Education, 26 (1-3), 17-32.","cites":null},{"id":197693,"title":"Now you see it, now you don't: maintaining digital learning objects for the future',","authors":[],"date":"2002","doi":"10.1109\/mc.2002.1016907","raw":"Harvey, R. (2002), 'Now you see it, now you don't: maintaining digital learning objects for the future', E-JIST (e-Journal of Instructional Science and Technology), 5 (2), http:\/\/www.usq.edu.aulelectpuble-jist\/docs\/Vol5%20No2\/Harvey%20-%20Final.pdf.","cites":null},{"id":1042796,"title":"Predicting quality in educational software: evaluating for learning, usability and the synergy between them',","authors":[],"date":"1999","doi":null,"raw":"Squires, D. and Preece, J. (1999), 'Predicting quality in educational software: evaluating for learning, usability and the synergy between them', Interacting with Computers, 11, 467-83.","cites":null},{"id":197687,"title":"Programme Evaluation and Quality: A Comprehensive Guide to Setting up an Evaluation System,","authors":[],"date":"1994","doi":null,"raw":"Calder, J. (1994), Programme Evaluation and Quality: A Comprehensive Guide to Setting up an Evaluation System, London: Kogan Page.","cites":null},{"id":1042785,"title":"Reflections on a model for evaluating learning technologies',","authors":[],"date":"1998","doi":null,"raw":"Jones, A., Scanlon, E. and Blake, C. (1998), 'Reflections on a model for evaluating learning technologies', in Oliver, M. (ed.), Innovation in the Evaluation of Learning Technology, London: University of North London.","cites":null},{"id":197688,"title":"The use and implications of ICT for supporting practical music skills in the school curriculum', paper to be presented at CAL","authors":[],"date":"2003","doi":null,"raw":"Chan, L., Jones, A., Joiner, A. R. and Scanlon, E. (forthcoming), 'The use and implications of ICT for supporting practical music skills in the school curriculum', paper to be presented at CAL 2003, Belfast, April 2003.","cites":null},{"id":1042787,"title":"Two approaches to enable the sharing and reuse of resources across institutions',","authors":[],"date":"2002","doi":null,"raw":"Littlejohn, A. and Campbell, L. (2002), 'Two approaches to enable the sharing and reuse of resources across institutions', Proceedings of Ed-Media, Association for the Advancement of Computers in Education, Denver, Colorado, USA, June 2002. ISCanan Tosunoglu Blake et al Evaluating complex digital resources McFarlane, A., Harrison, C., Somekh, B., Scrimshaw, P., Harrison A. and Lewin, C.","cites":null},{"id":1042795,"title":"Usability and educational software design: editorial',","authors":[],"date":"1999","doi":"10.1016\/s0953-5438(98)00062-9","raw":"Squires, D. (1999), 'Usability and educational software design: editorial', special issue of Interacting with Computers, 11, 463-6.","cites":null},{"id":1042794,"title":"Usability and learning: evaluating the potential of educational software',","authors":[],"date":"1996","doi":"10.1016\/0360-1315(96)00010-3","raw":"Squires, D. and Preece, J. (1996), 'Usability and learning: evaluating the potential of educational software', Computers and Education, 27(1), 15-22.","cites":null},{"id":197694,"title":"Use of TLTP Materials in UK higher education: a HEFCE-commissioned study',","authors":[],"date":"1999","doi":null,"raw":"Haywood, G., Anderson, C., Day, K. and, MacLeod, H. (1999), 'Use of TLTP Materials in UK higher education: a HEFCE-commissioned study', http:\/\/www.flp.ed.ac.uk\/LTRG\/ TLTP.html.","cites":null},{"id":1042793,"title":"Using video to enable effective collaboration between academic and software developers in the design and specification of educational software', paper presented at CALRG","authors":[],"date":"2002","doi":null,"raw":"Shipp, K. (2002), 'Using video to enable effective collaboration between academic and software developers in the design and specification of educational software', paper presented at CALRG Symposium on Video Recorded Data in Educational Research, Open University, April.","cites":null},{"id":197692,"title":"Varying the texture: a study of art, learning and multimedia',","authors":[],"date":"1996","doi":null,"raw":"Durbridge, N. and Stratford, M. (1996), 'Varying the texture: a study of art, learning and multimedia', Journal of Interactive Media in Education (1), http:\/\/www-jime.open.ac.uk\/ 96\/11.","cites":null},{"id":1042791,"title":"Virtual Fieldtrips, WWW site regarding 'virtual' fieldtrips or studies","authors":[],"date":"1997","doi":null,"raw":"Ritter, M. (1997), Virtual Fieldtrips, WWW site regarding 'virtual' fieldtrips or studies in geography, http:\/\/www.uwsp.edu\/acaddept\/geog\/projects\/virtdept\/guidel.html.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2003","abstract":"Squires (1999) discussed the gap between HCI (Human Computer Interaction) and the educational computing communities in their very different approaches to evaluating educational software. This paper revisits that issue in the context of evaluating digital resources, focusing on two approaches to evaluation: an HCI and an educational perspective. Squires and Preece's HCI evaluation model is a predictive model \u2010 it helps teachers decide whether or not to use educational software \u2010 whilst our own concern is in evaluating the use of learning technologies. It is suggested that in part the different approaches of the two communities relate to the different focus that each takes: in HCI the focus is typically on development and hence usability, whilst in education the concern is with the learner and teacher use","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/14216.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/395\/1\/ALT_J_Vol11_No1_2003_Evaluating%20complex%20digital%20res.pdf","pdfHashValue":"ec7024b557f10e3e948514aa6ace3af31faebc23","publisher":"University of Wales Press","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:395<\/identifier><datestamp>\n      2011-04-04T09:10:57Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/395\/<\/dc:relation><dc:title>\n        Evaluating complex digital resources<\/dc:title><dc:creator>\n        Blake, Canan<\/dc:creator><dc:creator>\n        Davies, Clare<\/dc:creator><dc:creator>\n        Jones, Ann<\/dc:creator><dc:creator>\n        Morris, Erica<\/dc:creator><dc:creator>\n        Scanlon, Eileen<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        Squires (1999) discussed the gap between HCI (Human Computer Interaction) and the educational computing communities in their very different approaches to evaluating educational software. This paper revisits that issue in the context of evaluating digital resources, focusing on two approaches to evaluation: an HCI and an educational perspective. Squires and Preece's HCI evaluation model is a predictive model \u2010 it helps teachers decide whether or not to use educational software \u2010 whilst our own concern is in evaluating the use of learning technologies. It is suggested that in part the different approaches of the two communities relate to the different focus that each takes: in HCI the focus is typically on development and hence usability, whilst in education the concern is with the learner and teacher use.<\/dc:description><dc:publisher>\n        University of Wales Press<\/dc:publisher><dc:date>\n        2003<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/395\/1\/ALT_J_Vol11_No1_2003_Evaluating%20complex%20digital%20res.pdf<\/dc:identifier><dc:identifier>\n          Blake, Canan and Davies, Clare and Jones, Ann and Morris, Erica and Scanlon, Eileen  (2003) Evaluating complex digital resources.  Association for Learning Technology Journal, 11 (1).  pp. 4-16.  ISSN 0968-7769     <\/dc:identifier><dc:relation>\n        10.1080\/0968776030110102<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/395\/","10.1080\/0968776030110102"],"year":2003,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"Evaluating complex digital resources\nCanan Tosunoglu Blake* Clare Davies,** Ann Jones,* Erica Morris* and\nEileen Scanlon*\n*Open University, UK **Northwestern University, USA\nemail: C.Tosunoglu@open.ac.uk\nSquires (1999) discussed the gap between HCI (Human Computer Interaction) and the\neducational computing communities in their very different approaches to evaluating\neducational software. This paper revisits that issue in the context of evaluating digital\nresources, focusing on two approaches to evaluation: an HCI and an educational\nperspective. Squires and Preece's HCI evaluation model is a predictive model - it helps\nteachers decide whether or not to use educational software - whilst our own concern is in\nevaluating the use of learning technologies. It is suggested that in part the different\napproaches of the two communities relate to the different focus that each takes: in HCI\nthe focus is typically on development and hence usability, whilst in education the concern\nis with the learner and teacher use.\nIn the paper we consider a recent application of the CIAO! framework, developed at the\nOpen University, to evaluating digital resources. We compare this with an evaluation\nmodel by Dorward, Reinke and Recker (2002): a recent approach from the HCI 'stable'.\nInterestingly, there are several similarities and consistencies between these approaches\nand there are also other recent indications that the communities may be drawing on each\nother's work, in at least some areas.\nForeword\nDavid Squires had an important influence on the work of all the authors and on that of\nthe Computers and Learning Research group (CALRG) at the Open University, where\nfour of the authors are located. Ann first met David some twenty-five years ago when one\nof his roles was working with teachers to develop educational software. Over the years his\nwork has often taken a similar route to the work of the CALRG and we have been\nindebted to his contributions. In the early days of educational 'microcomputer' use (as it\nwas known in the 1980s) David provided invaluable advice to our Micros in Schools project\n4\nALT-] Volume 11 Number I\nat the OU, and we also drew on his work with teachers which developed models and\nguidelines for their review and use of educational software.\nErica felt honoured that David Squires examined her Ph.D. thesis and valued his insightful\ncomments and the discussion he encouraged and supported. In 1996, David ran a British\nComputer Society Human Computer Interaction Special Interest Group meeting to\nexplore the issues of joint concern to educational and HCI evaluators and invited Ann\nalong. This paper and the work reported here was influenced by the lively discussion at the\nworkshop, the special issue of 'Interacting with Computers' that followed and the\ncontinuing debate.\nIntroduction: digital resources for teaching\nThere has recently been a rapid increase in the development and use of digital resources for\nteaching and learning. 'Digital resources' is a broad term: it can include electronic books,\nonline journals, movies, reference texts such as dictionaries as well as audio or image files;\nit is used to cover material created digitally or by scanning analogue resources. In this\npaper we are using the term 'complex digital resources' to mean any or all the above but we\nhave a particular interest in learning environments which combine text and graphics,\nespecially maps.\nThis increase in use has been accompanied by a high level of concern by government and\nother funding bodies both in the UK and the US about the impact of such developments.\nFor example, a recent programme jointly funded by the Joint Information Systems\nCommittee (JISC) in the UK and the National Science Foundation (NSF) in the US\nfocuses on how innovative applications of emerging IT and digital resources may trans-\nform teaching and learning. It is clear that there is an unprecedented level of interest in and\nuse of such digital resources, and a clear political wish to encourage and foster this. Both\nJISC and NSF expressed their wish to promote 'effective use of large scale distributed\ndigital content and advanced networking technologies in the context of the (higher\neducation) classroom'. In particular, they emphasrze the availability of the combination of\nstate-of-the-art digital and Internet-based services as well as digital content available\nglobally for emerging applications in undergraduate education.\nThe particular focus of this paper is the evaluation of such complex digital resources. It is\nonly through such evaluation that we begin to understand whether using such resources\ndoes transform teaching and learning. Evaluations of learning technologies have been a\ncontinuing part of the work of the Computers and Learning Research Group (CALRG) at\nthe Open University for some twenty-five years. Whilst the group has been involved with\nevaluating the use of IT in education in the broadest sense, this paper will focus on digital\nresources and, in particular, a project on digital maps.\nThe outline of this paper is as follows. The next two sections describe the resources that\nhave been developed in this area and their advantages for learning. Next we outline the\nCALRG's approach to evaluation and explain its rationale, and discuss how HCI and\neducational concerns may differ. This section also describes an HCI evaluation model\ndeveloped for evaluating digital library services and compares the two approaches.\nFollowing this we outline how the framework was applied to evaluating an external project\nbefore drawing our conclusions.\n5\nConan Tosunoglu Blake et al Evaluating complex digital resources\nResources for learning with ICT in geography and cognate areas\nWithin geography and cartography, and related subjects such as geology, meteorology,\nenvironmental science and earth science, a range of educational resources has been\ndeveloped (for example, CTIGGM, 1998). The Higher Education Funding Councils' Fund\nfor the Development of Teaching and Learning (FDTL), the Teaching and Learning\nTechnology Programme (TLTP) and the DeLiberations Geography project within the JISC\nElectronic Libraries programme have all funded efforts to increase access to computer-based\nteaching resources for this group of disciplines. For example, with TLTP funding, a group of\nUK earth science departments formed the UK Earth Science Courseware Consortium\n(UKESCC), which developed a set of computer-aided course modules for use by con-\nsortium members and for purchase by non-members. Similarly, seventy-two university and\ncollege geography departments formed a consortium to produce the GeographyCal software\ncourse modules (GeographyCal, see http:llwww.geog.le.ac.uklctilTltplintro.htm), which like\nthe UKESCC resources are aimed at first- or second-year undergraduates. GeographyCal\ncontains modules on map design and on introductory Geographical Information Systems\n(GIS), as well as incorporating digital maps into other modules.\nLess ambitiously, the 'Virtual Fieldtrips' section of the 'Virtual Geography Department' at\nthe University of Wisconsin encourages geographers to follow a simple template to\nproduce a field trip primer for a given area used in their teaching, and to include location\nmaps (Ritter, 1997). The site then acts as a gateway to all such 'virtual field trips', giving\naccess to basic details of and exercises about areas all over the world.\nAnother recent development is the growth of the use of GIS. In the strictest sense, a GIS is\na computer system capable of assembling, storing, manipulating and displaying\ngeographically referenced information, that is, data identified according to their locations.\nMore simply, we can view GIS as the digital equivalent of a map. In the same way that\nindividual maps contain a wealth of information and are used in diverse ways by different\nindividuals and organizations, GIS are also used in diverse applications. Applications\nrange from databases of electricity networks to aid maintenance and supply to displaying\nthe extent of deforestation in the Brazilian Amazon. (See, for example,\nhttp:\/\/www.geo. ed. ac. uklhomelresearchlwhatisgis.html.)\nGiven such diverse applications, GIS are now used by many institutions including local\nand national governments, research institutions, businesses and industry. For example,\nplanning offices might use GIS to keep records of property boundaries and they could be\nused in market analysis where it is necessary to know location of customers, the distance\nthey have to travel, the best places to advertise and location of competitors. The wide\napplication area of GIS requires many subject areas to incorporate the teaching of spatial\nskills and data manipulation into their programmes to assist graduates with career options.\nThese users may not need a deep understanding of all of the elements of spatial data, such\nas its data structures, analysis and visualization, but they need to have sufficient knowledge\nto make appropriate and valid use of these data (Purves, Medycki-Scott, Blake, Fairbain\nand Mackaness, forthcoming).\nThe Open University has begun in recent years to use digital maps not only within\ngeography and related disciplines but also within other less predictable contexts. An early\nexample of this is the use of maps for historical research by students in a fourth-level\n6\nALT-J Volume 11 Number I\ncourse entitled, 'Charles Booth and Social Investigation in Britain 1850-1914'. The CD-\nROM developed for this course includes a section labelled the 'map room', which includes\nmonochrome Ordnance Survey maps as well as the social maps drawn up by Booth and the\nopportunity for students to plot data for themselves. The aim is as much to make students\nquestion the decisions and value systems reflected in the maps as to use them for study of\nthe actual phenomena: Booth's work, like every other cartographer, reflected his own\nagenda and social context. Thus maps are being studied as visual artefacts in their own\nright, with students encouraged to consider critically their context and interpretation, in a\nsimilar vein to OU developments such as-Art Explorer and its successors (Durbridge and\nStratford, 1996). A Research Libraries Support Programme-funded project based at the\nLondon School of Economics has creating free online digital versions of these maps\n{http:llbooth.lse.ac.ukl).\nThe project whose evaluation we consider in this paper, 'e-MapScholar' (http:\/\/edina.ac.uk\/\nprojectslmapscholarf), also arose out of the need to enable students to evaluate and apply\nmap resources appropriately even in non-geographic disciplines. In this case, however, the\nproject arose out of an online data provision service funded by the JISC and managed at\nthe University of Edinburgh: EDINA Digimap. This service provides current Ordnance\nSurvey digital map data to subscribing UK higher education institutions so that academics\nand students can use the data in their projects. Evaluations of the service showed that this\nwas encouraging the use of such data beyond the geographic disciplines for which most of\nthe above learning resources were developed. It was felt that delivering learning resources\nto help non-geographers learn about map use alongside the data would be the best way to\nhelp this audience to appreciate the digital mapping techniques and skills required both\nwithin and beyond the academic environment. The next section will consider the concepts\nand difficulties faced by such learners.\nDigital maps and learning\nDigital maps can provide the learner with a number of advantages over using conventional\npaper-based maps (Davies, 1998): the learner can edit and change the appearance of the\ninformation they contain, has more control over the information and can choose what to\ndisplay. Equally importantly from an educational viewpoint, far more information can be\nmade available to the user than could be fitted onto a paper map.\n\u2022 Learners can hide or display -different combinations of 'layers', showing different\nfeature types or variables, and can thus observe various different views or relationships.\n'Layers' can include a reference grid, text labels and other explanatory features, as well\nas actual geographical entities.\n\u2022 Learners may be given the choice over some or all aspects of the map's appearance:\nsymbolization, categorization, colour, texture, scale, projection, label placement,\ngeneralization and description.\n\u2022 Spatial correlations and other statistical relationships between features or variables can\nbe calculated and displayed, to test whether apparent effects are really significant.\n\u2022 Particular phenomena (such as floods, emigration or erosion) can be modelled and\nanimated to show changes of extent or distribution over time.\n7\nCanan Tosunoglu Blake et al Evaluating complex digital resources\n\u2022 A digital map can be continuous and can be much larger than the screen at a given\nscale: the user can 'zoom out', 'zoom in' and 'pan' across the map to change the area\ndisplayed at any given moment.\n\u2022 A database can be linked to the map so that displayed objects (such as a building) can\nbe selected with a mouse click, and further information displayed (for example, about\nthe building's history or owners) in a pop-up window. The data linked to the map may\ninclude more than simple text records: aerial or other photographs, numeric tables or\nspreadsheets, and hypermedia entities such as video clips or hypertext could also be\nincluded.\nIn other words, besides the visible design of the map, digital map-based multimedia has a\ncomplex information structure. The structure also differs between different digital maps,\neven from the same supplier. For example, the users of EDINA Digimap have to learn that\ndifferent Ordnance Survey datasets are designed for use at different scales of accuracy and\ndetail and cannot be effectively overlaid upon each other (for example, the extra-thick\ngreen line drawn to depict a trunk road at one scale looks nonsensically massive when\noverlaid on a street-scale image; individual buildings appear to be 'swallowed' by it). This\nfocus on the information structure necessitates specific tutoring.\nThe obvious flexibility and depth of information provide potenticil benefits for learners, but\nalso risk misleading or confusing them. The sections that follow describe the CALRG's\napproach to evaluation in order to investigate the extent to which such potential benefits\ncan become reality.\nAn educational approach to evaluation: the CIAO! framework\nAt the Open University (OU) we have developed the CIAO! model to evaluate learning\ntechnology in context. This framework has been applied internally to evaluate learning\ntechnology applications developed at the university and has also been the basis for\nevaluating external projects (for example, Scanlon, Jones, Calder, Barnard and Thompson,\n2000).\nThe framework outlines three dimensions to evaluation: (i) context, (ii) interactions and\n(iii) attitudes and outcomes. The framework is described elsewhere (Jones, Scanlon,\nTosunoglu, Ross, Butcher, Murphy and Greenberg, 1996), as are the issues in evaluating\nlearning technologies that led to the emphasis on these three areas (Jones, Scanlon and\nBlake, 1998). Here we will briefly discuss the reasons for the emphasis on two of the\ndimensions, those of context and interactions, and the implications of this emphasis for\nevaluating digital resources. We will also discuss how such a model relates to other\napproaches to evaluating digital resources, and educational computing and usability issues\nmore generally.\nContext\nIn all our studies we have found it particularly important to pay attention to context. In the\nCIAO! framework, context refers to a number of things, ranging from the wider context,\nsuch as the 'framing' of the teaching (for example, its location and who is involved, and\nwhether the use is individual or collaborative), to a finer grained level (such as the context\nof the digital resource within the course and the components of the resource itself). This\nallows us to focus at a detailed level on how learners use information technology resources\n8\nALT-] Volume 11 Number I\nwithout losing the broader frame in which the learning is situated. It also includes the\nrationale for using or developing the particular application or resource - this is one of the\nmost important aspects as it emphasizes the need to understand the intention of the\ndesigner or educator. Whether such intentions are realized is a matter for the evaluation to\nestablish.\nContext was a major concern in early evaluation literature (for example, Kemmis, Atkins\nand Wright, 1977) and has been emphasized more recently in the evaluation of TLTP\nprojects and the many technology initiatives in the 1990s (for example, Draper, Brown,\nHenderson and McAteer, 1996). Oliver and Conole (1998) also emphasize context,\nalthough here it is part of what they refer to as authenticity which 'describes the notion\nof how closely an evaluation captures the context of an existing course' (Oliver and\nConole, 1998: 4). However, with online learning resources such as those developed in 'e-\nMapScholar', the context of learning will vary unpredictably from classroom-based group\nteaching with formal progress assessments, to lone self-motivated browsing for the sake of\nunderstanding the maps used in a practical project.\nA crucial aspect of context is the designer's rationale in introducing the technology.\nAnalysis and understanding of this pedagogical rationale is essential in determining the\nevaluation questions to be asked. For example, specific educational software may be\ndesigned to help learners understand concepts that are known to be difficult; perhaps by\noffering a different representation, or in subjects such as biology by demonstrating a\ndynamic process (such as relationships between parts of the circulatory system). One way\nto reflect this is to ask the designers and teachers what benefits they are expecting from\nusing learning technology. In cases such as the examples above, this might be that it will\nhelp learners in their understanding.\nWe are not suggesting that the evaluation questions are driven only by the teacher's or\ndesigner's view, but that understanding both is a crucial element. It is also true that learners\nmay benefit in ways that were not anticipated at all by the designers and may have\nexpectations that have not been considered. For example, Chan, Jones, Joiner and Scanlon\n(forthcoming) have been studying learners using Teach Me Piano (TMP), a program\ndesigned to support the learning of generic performance skills. However, learners' specific\nmotivation for using such a program cannot be assumed. It may be that some of the young\nstudents using it have aspirations towards playing their favourite band's current 'hit' -\naspirations that may not be realistic. Such expectations can be revealed through inter-\nviewing and observing the learners. Paying attention to context can also help to bridge the\ngap between the different approaches in HCI and education evaluation. It provides one\nway of taking different perspectives into account; in this case the perspective of the\nsoftware developer whose evaluation concerns may be around HCI issues and the teacher\nwho is concerned with pedagogical issues.\nSquires (1999) elaborated on the different traditions of research and evaluation in\neducational computing and HCI, and discussed ways that each community could benefit\nfrom the work of the other. As part of this, Squires and Preece (1999) developed a\npredictive model of evaluation which recasts an HCI evaluation paradigm in terms of a\nsocio-constructivist view of learning to produce a set of 'learning with software' heuristics.\nThese attend to the integration of usability and learning issues. It is this integration that\n9\nCanon Tosunoglu Blake et al Evaluating complex digital resources\nhas been a particular concern for Squires. However, this is a predictive evaluation model;\nthat is, it allows teachers to decide which software to use with their students. The CIAO!\nframework complements such an approach by concentrating on the software in use by\nlearners, which Squires and Preece have referred to elsewhere (1996) as an interpretive\nmodel. The approach overall is educational. However, we would agree with Squires that it\nis necessary to attend to both usability and educational issues. The framework allows us to\ndo that because it is a range of approaches rather than a prescribed method and so the\nevaluator has the flexibility to choose the most appropriate methods.\nIt is also worth noting that the two concerns should dovetail together, since digital\nresources increasingly integrate the interface with the educational content. In the online\nmapping tutorial in 'e-MapScholar', to give two examples, the user can see separated types\n(layers) of geographical data and examine the meanings of and relationships between map\nsymbols. This prepares students conceptually, as well as procedurally, for handling the\nsame issues of geographical structure and semiotics when using a real GIS to examine their\nown project data.\nCollecting information about the process: tracking interactions\nAs well as valuing context, the CIAO! framework emphasizes studying the learning process\nin addition to learning outcomes. The problems of evaluating innovative technologies in\nlearning through assessing outcomes are well debated in the general literature (for example,\nMcFarlane, Harrison, Somekh, Scrimshaw, Harrison and Lewin, 2000; Jones, Scanlon and\nBlake, 1998). Not surprisingly, the same finding occurs in the literature on using digital\nmaps in learning (Proctor and Richardson, 1997). In any case, evaluation is usually\nintended not just to provide information about success but also to suggest improvements\n(see, for example, Calder, 1994). Hence in addition to any outcome data it is at least as\nimportant to try to understand the learning process.\nThe OU's approach to this has been to look in detail at students' use of the software by\nanalysing their interactions. This can provide information on why and how particular\nelements work (or not), rather than just finding out whether something works. Where\npossible, students are observed working with the software: this has included inviting\nstudents to come to the campus where members of the evaluation team can observe this.\nWhen these sessions have involved software developers, they have found the process of\nwatching learners and their problems to be particularly insightful. In one such formative\nevaluation the software developer was part of a team observing the use of computer-\nassisted learning materials in algebra for undergraduates (Jones, Scanlon and Blake, 1998);\nin this case, she found the process so helpful that she initiated and incorporated such\nevaluation trials in developing other materials and also developed some innovative\ncollaborative development processes (Shipp, 2002). Such interactions can be recorded by\naudio or video, in order to provide protocol data for later analysis. Computer logs can also\nbe collected of all key presses and the routes that students take through the materials.\nMore recently, we have been making increasing use of the Data Capture Suite. This is a\nfacility which can combine video records of each user with a synchronous record of their\ncomputer screen (Blake and Scanlon, 2002). This allows for four different video screens to\nbe displayed at the same time. It might, for example, include two screens showing video of\ntwo participants and a further two screens showing their computer screens. This allows\n10\nALT-] Volume 11 Number I\nobservers to track facial gestures and movements as well as participants' utterances and\ninteractions with the computer. A more detailed account 'of this facility is given in Blake\nand Scanlon (2002).\nA different evaluation model was proposed by Dorwood et al. (2002). This was developed\nfor the evaluation of a digital library services tool, and addresses both process and\noutcome. It views evaluation designs as combinations of different approaches that are the\nmost appropriate for addressing the information needs of developers and other\nstakeholders. In this, it is similar to the CIAO! framework, which is not prescriptive but\nsuggests a range of approaches and methods so that those that are adopted are appropriate\nto the study in hand. In CIAO! the focus on context requires the evaluator to assess what is\nneeded in each particular case. Dorwood et al.'s model was used to evaluate the\nInstructional Architect (IA), which enables users to discover, select, reuse, sequence and\nannotate digital library learning objects. The evaluation process here consisted of a\nnumber of phases. The first was a needs assessment - surveying teachers' use of online\nresources, their perceptions of IA and their needs. This was followed by an expert review of\nthe interface design, both by an internal and external panel. The next phase was an\nevaluation of the prototype design by the target audience, involving observation and a\npost-evaluation focus group interview with the evaluation team.\nOne of the findings from this evaluation phase was that, as the development team were\ninvolved in the evaluation, the problems users faced were very apparent to them and they\ncould work on fixing these immediately. As discussed above, similar benefits arose from\ninvolving software developers in evaluations at the Open University (Jones, Scanlon and\nBlake, 1998). A major part of the evaluation design was a case analysis examining how\nteachers assess, combine and use teaching objects in the classroom. This has a particular\nfocus on understanding the factors that enable or prevent the reuse of digital resources in\neducation. Again, this process incorporated observations, and in the next section we\nconsider the role of observation in tracking interactions in the CIAO! framework.\nApplying the evaluation approach to 'e-MapScholar'\nThe 'e-MapScholar' project aims to develop customizable resources appropriate to the uses\nmade of spatial data by a wide community. These are developed around three themes:\n\u2022 working with digital data;\n\u2022 integrating spatial data; and\n\u2022 visualization.\nAll resources produced allow interactive, customizable learning experiences, enabling\neffective constructivist learning that is related to prior knowledge. Both the tools and\nlearning materials access the Digimap map and data servers in real time.\nLearning materials developed under the 'Working with Digital Map Data' strand include\nconcepts of geographic data (extent, scale and generalization); how objects in the real\nworld are portrayed within Ordnance Survey data; and how the student should select data\nbased on fitness for purpose. The tools developed include simple map querying, reporting\nand measurement functions. Digimap supports the production of high-quality maps based\nCanon 7bsunog\/u Blake et al Evaluating complex digital resources\non Ordnance Survey digital map data. These can be used or customized to illustrate\nconcepts relating to the use of digital map data. However, a major aspect of this work is\nthe deconstruction of the Digimap user interface into components (atoms) which can be\nrecombined into simple interactive client-based tools and embedded within learning\nmaterials to provide an interactive illustration of a concept.\nLearning materials developed under the Data Integration strand focus on developing skills\nin, and understanding of, integrating a variety of external data (census, remote sensing,\nenvironmental) as well as user-generated data (Global Positioning Systems positions, other\nmeasured datasets) with the Ordnance Survey data available through Digimap. A number\nof client-based tools have been developed to interact with the servers, and learners and\nteachers will be able to upload their own data for use against the Ordnance Survey\nbackdrop.\nLearning materials developed in the Data Visualization strand focus on developing skills\nin, and understanding of, 2D and 3D visualization and visual problem-solving techniques.\nAreas addressed include fitness for purpose, collecting data for visual problem-solving and\nworking through the decision-making process. The tools and materials developed can be\nadapted to the learner's own prior knowledge and subject area, and enable the develop-\nment of appropriate skills by participation. The evaluation of these resources includes\nboth a formative and summative component. A summative evaluation is planned for the\nend of 2002 and beginning of 2003 when the resources are fully developed.\nThe formative evaluation is integrated into the development phase of the learning\nresources and, given that the materials are under development, focuses on usability issues\nby adopting techniques such as walk-through and expert and peer review. These lead to the\nfeeding back of comments and recommendations to the development team. The\nproduction of learning resources was planned in such a way that, during the development\nphase, selected resources were being made available to the evaluators. This facilitated\nformative evaluation and meant that the findings could be incorporated in to the design of\nnewly developed resources. This allowed modifications to be tested, in an iterative way, by\nthe evaluators. i\nSuch iterative models of formative evaluation are also used in the Open University in the\ndevelopment of teaching materials. These share a number of features with Dorward et al.'s\nmodel described earlier. We would argue that formative evaluations of resources under\ndevelopment will necessarily have more emphasis on the basic usability aspects such as\nconsistency and clarity, as it is difficult to evaluate their educational impact for two\nreasons. Firstly, un-fixed usability issues 'get in the way', so learners find their attention\ndrawn to difficulties in navigating and using the resources rather than learning from them,\nand secondly it is difficult to simulate using the resource in its intended context until it is\nclose to completion.\nWe have recently carried out a formative evaluation of a module in the 'Working with\nDigital Map Data' workpackage. The formative evaluation took place after a peer review\nby the members of the evaluation team. Following this we tested the basic interface to the\nresources, the usability of navigational aids and the help facility, presentation of maps on\nscreen, usability of tools (for example, a tool to present different layers of a map to the user\nvia automatic and user-controlled buttons, a magnifying glass tool to examine features of a\n12\nALT-] Volume 11 Number I\nmap more closely), time taken to complete the module and general academic content. The\nfindings from this formative evaluation were communicated to the software developers and\nthey incorporated necessary modifications into the newly developed resources.\nDiscussion\nIn considering what can be evaluated and when, it is important to bear in mind the factors that\naffect usability. In our very early ICT evaluation work we proposed a 'Chinese box' model of\nICT adoption and use (Jones and O'Shea, 1982) which views the process of adoption as a\nnumber of barriers to be surmounted. Interestingly this model applies as much now as it did\nthen, and very similar models have been proposed by evaluation teams investigating the take-\nup of ICT resources on a large scale. For example, Haywood, Anderson, Day and MacLeod\n(1999) list a number of contextual features that influence ICT use including institutional and\ndepartmental characteristics. Being mindful of such factors should enable evaluators to select\nthe most appropriate form of evaluation for their particular purpose.\nEarlier we discussed the benefits of involving the development team in observing and\ntracking how users interact with the software, both in making any problems that the users\nfaced apparent and because the developers can then work on fixing any problems\nimmediately. Patton (1997) explores this issue in some detail and discusses, for example,\nwhether particular programs or organizations are ready for evaluation, given that the\n'reality testing' this involves can be both uncomfortable and threatening. In the 'e-\nMapScholar' evaluation, the evaluation and development teams were working at different\nsites. This was a challenge for the evaluation team who needed to persuade the\ndevelopment team of the learners' perspective as the developers did not have the advantage\nof the immediacy of observing users' reactions.\nThis evaluation, as we discussed earlier, studies a resource under development and thus\nfocuses on usability aspects. In this case, as in many development projects, the target\naudience cannot be involved in the evaluation until the final stage, and then practical\nconstraints and timing only allow the evaluators to run one trial with end-users near the\nend of the project. The walkthrough and expert evaluations used by the team did identify\nusability issues but these could not always be fixed quickly by the development team. We\nbelieve that this is partly an issue of timeliness: resources needed to be developed to a tight\ntimescale. By the time the development team had the evaluation findings, they were faced\nwith the tension between 'fixing' the modules that had been evaluated or applying them to\nthe resources they were currently working on. Thus they were engaged in a learning curve,\napplying lessons learnt from the earlier modules to later ones.\nIn evaluating 'e-MapScholar', the emphasis on context alerted the team to the importance\nof the pedagogical context. It could not be assumed that technical terms and specialized\nlanguage introduced in one unit would be remembered and understood in later units, either\nbecause students had skipped the appropriate unit or had forgotten the terms. So it was\nnecessary to remind students about such terms. This issue is particularly important where\nstand-alone learning materials are being developed, for example given the current\nemphasis and interest in re-usable 'learning objects' (such as Littlejohn and Campbell,\n2002; Harvey, 2002). It also points to the need for learners to be engaged in applying the\nterms to help them remember them.\n13\nCanon Tosunoglu Blake et al Evaluating complex digital resources\nAnother contextual issue that emerged was the students' need to appreciate why and when\nthey might want to use a particular feature before being shown how to do it. This\nmotivation could sometimes be provided by the learning context (such as a project they\nwere trying to execute with a real map or through discussion with their tutors about how\nmaps were used in their discipline). Arguably, given that particular users' goals for this\nresource are not known and cannot be anticipated, this kind of context needs to be\nprovided exactly in this way, in situ.\nFinally, early feedback from student evaluators suggested that they sometimes found it\ndifficult to make the connection between the processes and features described in the\nresource and their use in real contexts. For example, although the resources showed how\ndifferent features could be represented on the maps, they did not include photographs of\nsuch features, nor did they include images of people using such resources (for example,\ndigitizing). Again, they emphasized the need to relate what they were learning back to the\ncontext in which such resources were used. At a more general level this is a reminder of the\nimportance of making full and appropriate use of any particular medium. However, with\ncustomizable resources like 'e-MapScholar', teachers would be able to add such context-\nrelevant information and images, as suited their intentions for the unit's use, to overcome\nthe necessarily context-independent nature of the core resource itself.\nConclusions\nBoth the CIAO! framework and the Digital Library Services Tool evaluation model are\nnon-prescriptive frameworks that allow different approaches to be combined according to\nthe needs of the particular evaluation. In applying these approaches to IA and 'e-\nMapScholar1, there are interesting similarities: both involved iterative testing by different\ngroups of participants ranging from experts in the field to, eventually, the end users. Both\ninclude formative evaluations - which we have argued focus on usability issues and are\nneeded for development work. Both frameworks also highlight the value of the developers\nbeing part of the evaluation team in order to appreciate the kinds of problems that end\nusers have and how they use the resources.\nWe would argue that it is important to have a range of instruments\/approaches in tightly\ntimetabled development projects - and the flexibility to replace one with another if the\nplanned evaluation is not possible (for example, if the development is not finished on time).\nOur experience has been that this is not an uncommon situation in evaluating externally\nfunded projects that involve working to very tight timescales.\nWe have also argued that different forms of evaluation are appropriate in different\ncircumstances. For example, when focusing on digital resources that are being developed,-\nusability\/HCI issues are more important, not least because evaluators will not find out\nabout educational value if the resource is not usable. It is more appropriate to focus on\nbroader learning and educational advantages once the usability issues are resolved, and\nonce the resource is embedded in its intended educational context.\nReferences\nBlake, C. and Scanlon, E. (2002), 'Enriching accounts of computer supported\n14\nAvr-J Volume 11 Number I\ncollaboration by using video data', Research Proceedings of ALT-C Conference,\nSunderland, UK, 10-12 September 2002.\nCalder, J. (1994), Programme Evaluation and Quality: A Comprehensive Guide to Setting up\nan Evaluation System, London: Kogan Page.\nChan, L., Jones, A., Joiner, A. R. and Scanlon, E. (forthcoming), 'The use and implications\nof ICT for supporting practical music skills in the school curriculum', paper to be\npresented at CAL 2003, Belfast, April 2003.\nCTIGGM (Computers in Teaching Initiative: Geography, Geology and Meteorology)\n(1998), 'WWW gateway for teaching and learning resources', http:\/\/eee.geog.le.ac.uk\/cti\/\nDavies, C. (1998), 'Using digital geographic maps in distance learning', Computers and\nLearning Research Group Technical Report No. 180, Open University.\nDorward, J., Reinke, D., and Recker, M. (2002), 'An evaluation model for a digital library\nservices tool', in Proceedings of the Joint Conference on Digital Libraries, New York: ACM,\n322-3.\nDraper, S., Brown, M., Henderson, F. and McAteer, E. (1996), 'Integrative evaluation: an\nemerging role for classroom studies of CAL', Computers and Education, 26 (1-3), 17-32.\nDurbridge, N. and Stratford, M. (1996), 'Varying the texture: a study of art, learning and\nmultimedia', Journal of Interactive Media in Education (1), http:\/\/www-jime.open.ac.uk\/\n96\/11.\nHarvey, R. (2002), 'Now you see it, now you don't: maintaining digital learning objects for\nthe future', E-JIST (e-Journal of Instructional Science and Technology), 5 (2),\nhttp:\/\/www.usq.edu.aulelectpuble-jist\/docs\/Vol5%20No2\/Harvey%20-%20Final.pdf.\nHaywood, G., Anderson, C., Day, K. and, MacLeod, H. (1999), 'Use of TLTP Materials in\nUK higher education: a HEFCE-commissioned study', http:\/\/www.flp.ed.ac.uk\/LTRG\/\nTLTP.html.\nJones, A. and O'Shea, T. (1982), 'Barriers to the use of computer assisted learning', British\nJournal of Educational Technology, 3 (13), 207-17.\nJones, A., Scanlon, E., Tosunoglu, C., Ross, S., Butcher, P., Murphy, P. and Greenberg, J.\n(1996), 'Evaluating CAL at the Open University: 15 years on', Computers and Education,\n26(1-3), 5-15.\nJones, A., Scanlon, E. and Blake, C. (1998), 'Reflections on a model for evaluating learning\ntechnologies', in Oliver, M. (ed.), Innovation in the Evaluation of Learning Technology,\nLondon: University of North London.\nKemmis, S., Atkins, R. and Wright, E. (1977), How Do Students Learn: The UNCAL\nEvaluation, occasional publication no 5, Centre for Applied Research in Education,\nUniversity of East Anglia.\nLittlejohn, A. and Campbell, L. (2002), 'Two approaches to enable the sharing and reuse of\nresources across institutions', Proceedings of Ed-Media, Association for the Advancement\nof Computers in Education, Denver, Colorado, USA, June 2002.\nIS\nCanan Tosunoglu Blake et al Evaluating complex digital resources\nMcFarlane, A., Harrison, C., Somekh, B., Scrimshaw, P., Harrison A. and Lewin, C.\n(2000), Establishing Relationships between Networked Technologies and Attainment,\nImpact2 Preliminary Report, http:\/\/www.becta.org.uk\/research\/reports\/docs\/ impact2_\nprelim1.pdf.\nOliver, M., and Conole, G. (1998), 'Evaluating communication and information\ntechnologies: a toolkit for practitioners', Active Learning 8, 3-8.\nPatton, M. (1997), Utilization-Focused Evaluation, London: Sage.\nProctor, J. and Richardson, A. (1997), 'Evaluating the effectiveness of multimedia\ncomputer modules as enrichment exercises for introductory human geography', Journal of\nGeography in Higher Education, 21 (1), 41-55.\nPurves, R., Medycki-Scott, D., Blake, C., Fairbain, D. and Mackaness, W. (forthcoming),\n'Delivering customisable e-learning on spatial data to a multidisciplinary audience',\nsubmitted to Journal of Geography in Higher Education.\nRitter, M. (1997), Virtual Fieldtrips, WWW site regarding 'virtual' fieldtrips or studies in\ngeography, http:\/\/www.uwsp.edu\/acaddept\/geog\/projects\/virtdept\/guidel.html.\nScanlon, E., Jones, A., Calder, X, Barnard, J. and Thompson, J. (2000) 'Evaluating\ninformation and communication technologies for learning in FE', Educational Technology\nand Society, 3 (4), 101-7.\nShipp, K. (2002), 'Using video to enable effective collaboration between academic and\nsoftware developers in the design and specification of educational software', paper\npresented at CALRG Symposium on Video Recorded Data in Educational Research,\nOpen University, April.\nSquires, D. and Preece, J. (1996), 'Usability and learning: evaluating the potential of\neducational software', Computers and Education, 27(1), 15-22.\nSquires, D. (1999), 'Usability and educational software design: editorial', special issue of\nInteracting with Computers, 11, 463-6.\nSquires, D. and Preece, J. (1999), 'Predicting quality in educational software: evaluating for\nlearning, usability and the synergy between them', Interacting with Computers, 11, 467-83.\n"}