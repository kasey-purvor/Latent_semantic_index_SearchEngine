{"doi":"10.1080\/0968776020100109","coreId":"14230","oai":"oai:generic.eprints.org:371\/core5","identifiers":["oai:generic.eprints.org:371\/core5","10.1080\/0968776020100109"],"title":"Does the mode of delivery affect mathematics examination results?","authors":["Fiddes, D.J.","Korabinski, A. A.","McGuire, G. R.","Youngson, M.A.","McMillan, D."],"enrichments":{"references":[{"id":198190,"title":"Assessment in mathematics',","authors":[],"date":"2002","doi":"10.1177\/1469787403004002002","raw":"Beevers, C. E. and Paterson, J. S. (2002), 'Assessment in mathematics', in P. Khan and J. Kyle (eds), Effective Teaching and Learning in Mathematics and its Applications, London: Kogan Page.","cites":null},{"id":198191,"title":"Blueprint for computer-assisted assessment'. Available from: http:\/\/www.caacentre\/bp CALM Group","authors":[],"date":"2001","doi":null,"raw":"Bull, J. and McKenna, C. (2001), 'Blueprint for computer-assisted assessment'. Available from: http:\/\/www.caacentre\/bp CALM Group (2001), 'CUE assessment system'. Available from: http:\/\/www.calm. hw.ac.uk\/cue.html\/ Lawson, D. (2001), 'Computer-aided assessment in relation to learning outcomes'. Available from: http.\/\/ltsn. mathstore. ac. uk\/articles\/maths-caa-series\/ Lee, G. and Weerakoon, P. (2001), 'The role of computer-aided assessment in health professional education: a comparison of student performance in computer-based and paper-and-pen multiple-choice tests', Medical Teacher, 23, 152-7.","cites":null},{"id":198187,"title":"Computer aided assessment in mathematics at Heriot-Watt University',","authors":[],"date":"2000","doi":"10.11120\/msor.2000.00010017","raw":"Beevers, C. E. (2000), 'Computer aided assessment in mathematics at Heriot-Watt University', Maths, Stats and OR Newsletter, 1, 17-19.","cites":null},{"id":198194,"title":"Electrical and electronic engineering assessment network'. Available from: http:\/\/www.e3an.ac.uk\/","authors":[],"date":"2001","doi":null,"raw":"White, S. (2001), 'Electrical and electronic engineering assessment network'. Available from: http:\/\/www.e3an.ac.uk\/","cites":null},{"id":198192,"title":"Introductory Statistics,","authors":[],"date":"1985","doi":null,"raw":"McGhee, J. W. (1985), Introductory Statistics, St Paul, MN: West Publishing.","cites":null},{"id":198189,"title":"Issues of partial credit in mathematical assessment by computer',","authors":[],"date":"1999","doi":"10.3402\/rlt.v7i1.11236","raw":"Beevers, C. E., Youngson, M. A., McGuire, G. R., Wild, D. G. and Fiddes, D. J. (1999), 'Issues of partial credit in mathematical assessment by computer', ALT-J, 7, 26-32.","cites":null},{"id":198188,"title":"Mathematical ability assessed by computer',","authors":[],"date":"1995","doi":"10.1016\/0360-1315(95)00049-6","raw":"Beevers, C. E., McGuire, G. R., Stirling, G. and Wild, D. G. (1995), 'Mathematical ability assessed by computer', Computers and Education 25, 3, 123-32.","cites":null},{"id":198193,"title":"Open testing with a large databank of multiple choice questions',","authors":[],"date":"1999","doi":"10.1093\/teamat\/18.4.159","raw":"68Alt-J Volume 10 Number I Sims-Williams, J. (1999), 'Open testing with a large databank of multiple choice questions', Teaching Mathematics and its Applications, 18, 159-61. (Also http:\/\/www.tal.bris.ac.uk\/).","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2002","abstract":"At present most examinations are delivered on paper but there is a growing trend in many subjects to deliver some or part of these examinations by computer. It is therefore important to know whether there are any differences in the results obtained by candidates sitting examinations taken by computer compared with those obtained by candidates sitting conventional examinations using pen and paper. The purpose of this article is to describe the outcome of a pilot study designed to\u2217 investigate possible causes of any differences in results from the use of different modes of delivery in a mathematics examination. One outcome of this study was that the process of translating examination questions into a format required for use on the computer (but keeping this as a pen and paper test) can have a significant effect on examination results. However, the main conclusion is that changing the medium only has no effect on the results in mathematics examinations","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/14230.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/371\/1\/ALT_J_Vol10_No1_2002_Does%20the%20mode%20of%20delivery%20affe.pdf","pdfHashValue":"d6174bc42959f744288438f0163460d4c7d5ebe5","publisher":"University of Wales Press","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:371<\/identifier><datestamp>\n      2011-04-04T09:12:28Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/371\/<\/dc:relation><dc:title>\n        Does the mode of delivery affect mathematics examination results?<\/dc:title><dc:creator>\n        Fiddes, D.J.<\/dc:creator><dc:creator>\n        Korabinski, A. A.<\/dc:creator><dc:creator>\n        McGuire, G. R.<\/dc:creator><dc:creator>\n        Youngson, M.A.<\/dc:creator><dc:creator>\n        McMillan, D.<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        At present most examinations are delivered on paper but there is a growing trend in many subjects to deliver some or part of these examinations by computer. It is therefore important to know whether there are any differences in the results obtained by candidates sitting examinations taken by computer compared with those obtained by candidates sitting conventional examinations using pen and paper. The purpose of this article is to describe the outcome of a pilot study designed to\u2217 investigate possible causes of any differences in results from the use of different modes of delivery in a mathematics examination. One outcome of this study was that the process of translating examination questions into a format required for use on the computer (but keeping this as a pen and paper test) can have a significant effect on examination results. However, the main conclusion is that changing the medium only has no effect on the results in mathematics examinations.<\/dc:description><dc:publisher>\n        University of Wales Press<\/dc:publisher><dc:date>\n        2002<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/371\/1\/ALT_J_Vol10_No1_2002_Does%20the%20mode%20of%20delivery%20affe.pdf<\/dc:identifier><dc:identifier>\n          Fiddes, D.J. and Korabinski, A. A. and McGuire, G. R. and Youngson, M.A. and McMillan, D.  (2002) Does the mode of delivery affect mathematics examination results?  Association for Learning Technology Journal, 10 (1).  pp. 60-69.  ISSN 0968-7769     <\/dc:identifier><dc:relation>\n        10.1080\/0968776020100109<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/371\/","10.1080\/0968776020100109"],"year":2002,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"Does the mode of delivery affect mathematics\nexamination results?\nD.J. Fiddes* A. A. Korabinski* G. R. McGuire * M.AYoungson* and D. McMillan**\n*Heriot-Watt University **Scottish Qualifications Authority\nemail: g.r.mcguire@hw.ac.uk\nAt present most examinations are delivered on paper but there is a growing trend in many\nsubjects to deliver some or part of these examinations by computer. It is therefore\nimportant to know whether there are any differences in the results obtained by candidates\nsitting examinations taken by computer compared with those obtained by candidates\nsitting conventional examinations using pen and paper. The purpose of this article is to\ndescribe the outcome of a pilot study designed to* investigate possible causes of any\ndifferences in results from the use of different modes of delivery in a mathematics\nexamination. One outcome of this study was that the process of translating examination\nquestions into a format required for use on the computer (but keeping this as a pen and\npaper test) can have a significant effect on examination results. However, the main\nconclusion is that changing the medium only has no effect on the results in mathematics\nexaminations.\nIntroduction\nThere is a growing trend in many subjects to deliver some or part of examinations by\ncomputer (Bull and McKenna, 2001; Lawson, 2001). Indeed, in some areas banks of\nsuitable questions are being assembled to allow future examinations in some\nundergraduate subjects to be taken on paper or by computer (White, 2001; Sims-Williams,\n1999). Very little is known, however, about the effects of the medium in testing basic skills.\nSome studies have been conducted using multiple-choice tests (for example Lee and\nWeerakoon, 2001, in the area of health education) but mathematics examinations taken on\npaper and by computer normally consist of questions which the candidate attempts by\ngiving their answer to each question as a number or more generally as a mathematical\nexpression. An up-to-date review of computer-aided assessment in mathematics is\npresented in a chapter of a recently published book on Effective Teaching and Learning in\nMathematics and its Applications (Beevers and Paterson, 2002).\n60\nAk-J Volume 10 Number I\nExaminations taken by computer have several advantages, not least of which is\ninstantaneous marking and feedback. However, they are different from paper examinations\nin at least two ways:\n\u2022 The questions for a computer test require varying amounts of rewording and\nadjustment in layout compared with the same questions in a paper test in order to write\nthem into a computer assessment package. We shall call this the rewording effect.\n\u2022 In a computer test, students read questions from the screen and answer these questions\nby typing in numbers or more general mathematical expressions at specific places (for\nexample an answer box) on the screen. We shall call this the medium effect.\nThese two effects can create differences in the results of the examination process. If any\ndifferences were found between the marks from a paper test and those from its computer\nversion this might be due to either the rewording or the medium effects, or both. The main\naim of our project was to devise an experiment to separate these two effects and investigate\nthe significance of each.\nSeveral other factors that might lead to differences between candidate performance in a\npaper examination and in one using computer delivery include the motivation of the\ncandidate to do well in the examinations, the level of anxiety felt by the candidate when\nsitting the examinations and the familiarity of the candidate with the assessment software\ndelivering the examinations on computer. The efforts made to deal with these factors\nthrough the timing of the experiment and in preparations for the actual tests of the\nexperiment are outlined later in the section on running the experiment.\nSetting up the experiment\nThe Scottish Qualifications Authority (SQA) Higher Mathematics examination has a high\nuptake in Scotland with virtually all candidates having previously taken examinations from\nthis board. For the experiment, three different test papers were supplied by the SQA, each\ncontaining short response questions typical of those found in Paper 1 of Higher\nMathematics. The questions used in this pilot project covered most topics in the syllabus\nwith the answers to many questions requiring general mathematical expressions as well as\nnumbers. The marking scheme provided by the SQA was that used in the Higher\nMathematics examinations where credit is awarded for each key skill required to be shown\nby a candidate. In this experiment there was no intention to investigate the most appropri-\nate key skills required in questions, as this would introduce more variables into the\nexperiment. The intention was only to compare current examination practice with its\npossible computer replacement, and so the marking scheme provided by the SQA was\nadhered to throughout the experiment and no consideration given as to whether the most\nappropriate key skills were being examined. While the time allowed for each test was thirty\nminutes, it was expected that candidates would be able to complete the paper in less time\nthan this so that, in general, time would not be an issue. We use the abbreviation 'P format'\nfor this type of paper test. The questions in the P format tests were then converted into\ncomputer test questions as required by the CUE assessment package (Beevers, 2000).\nFurther details of the CUE assessment system and online demonstrations of tests can be\nobtained at the CALM project Website (CALM Group, 2001). We use the abbreviation\n'ICT format' for this type of computer test. In order to separate the medium and rewording\n61\nD.J. Fiddes et o\\ Does the mode of delivery affect mathematics examination results?\neffects, a third type of test was produced, called 'RT format', which is the reverse\ntranslation of the computer test and was basically a screen dump of the questions in ICT\nformat. Examples of P format and ICT format questions are shown in Figures 1 and 2.\nPaper Question (P format)\n1. If f[x) = '2x - 3 and g{x) = 2a.-2 - 3 find an expression\nfor g{f{j:)). Write your answer in the form ax2 I fer-l-c .\nFigure I: Example of paper question (P format)\nIf flc) = 2x - 3 and g(x) = 2X2 - 3 then\nWhat are a , \u00a3> and c ?\n= ax2 + \u00a3x + c\n1-1) What is a ?\nYour currently accepted ansvrer:\n: Submit\n1-2) what is b ?\n: ; Submit:\nYour currently accepted answer:\n1-3) what is C ?\nr\nYour currently accepted answer:\n\u2022 Submit\nfigure 2: Example of computer question (Id format)\n62\nAk-J Volume 10 Number I\nAlthough in the examples shown it would have been fairly easy to make the ICT question\nthe same as the P question we wanted to use questions in this project which were reworded\nso that the rewording effect could be investigated. Since the RT format has exactly the same\nwords in each question and exactly the same place to insert the answer, comparison of the\nmarks between the ICT and RT format (both marked in the same way) should determine\nthe significance of the medium effect.\nAs both the P and RT format tests are paper tests, there is no change in the medium.\nHowever in marking the P format, working is taken into account in giving partial credit for\nanswers that are not correct but in which a candidate has shown some of the skills required\nto tackle the question. The same could be done with RT format. Space was provided\nopposite each question for rough working so that in the marking process this working\ncould be taken into account. Therefore the RT paper was marked in two ways. The first\nwas only to mark the answer, which we called RTC marking, since it was exactly how the\nanswers in ICT format tests were marked. The second was called RTW marking since it\nincluded giving partial credit for the rough working. Hence comparison of ICT with RTC\nmarks investigates the medium effect and comparison of P with RTW marks investigates\nthe rewording effect.\nRunning the experiment\nPupils from two schools, 18 from Falkirk High School and 50 from Queensferry High\nSchool were invited to participate in the project. All pupils were in their fifth or sixth year\nof secondary school. There were 40 males and 28 females. Prior to carrying out the\nexperiment, each school was visited, and the pupils were given details of what the project\nentailed and what would be expected of them. To give them some practice with inputting\nmathematical answers, a trial ICT test with 5 questions was set up and the pupils were\ngiven the opportunity to do this test when help was available to answer their queries about\nany aspect of what was required to sit the test. The questions in this trial ICT test were in\ngeneral much easier than Higher standard, but, unlike the questions involved in the\nproject, random parameters were incorporated into the questions. The trial test was\navailable to the pupils from the day of the visit until the day of the experiment so that they\ncould practice as often as they wished beforehand. By introducing random parameters they\nwould get a different test each time they ran the trial. Practice with the trial test would\nallow pupils to minimize any navigational or inputting difficulties they might have during\nthe computer test and help them gain some familiarity with this new type of test. Some\ncandidates took a lot of advantage of this practice, while others did not.\nTo enable a paired statistical analysis to be performed on the results of the experiment\nthree groups of pupils were set up at each of the schools in such a way that each group had\nroughly the same mixture of mathematical ability and gender. Their mathematical ability\nwas estimated from knowledge of their previous SQA examination and Higher preliminary\nexamination results.\nThe actual tests took place during or just after school hours at each school in April 2001.\nThis was in the small window of opportunity between the time when the pupils had\ncovered enough of the material in the Higher syllabus and before the period when their\nSQA examinations started. This was a time when the pupils were motivated to do well in\n63\nD.J. Fiddes et al Does the mode of delivery affect mathematics examination results?\nthe tests, having been encouraged to regard the tests as good revision for their approaching\nHigher examination. Each candidate took each of the three tests, one in each of the\nformats, over a 90-minute period. This was done in such a way that each group sat the three\ntests in different orders and also that, at any time, no group was sitting the same test or\ntaking a test in the same format as any other group. The computer tests were delivered over\nthe Web with the results being marked and stored at Heriot-Watt University as the pupils\nsat the tests. During the computer tests, the candidates did not experience either any\ndifficulty in navigating within a test or any delays when submitting answers.\nPupil feedback\nThe pupils were asked informally by questionnaire what they thought of the tests. Out of a\npossible 68 pupils, 54 took this opportunity to express their views. The results give some\nindication about how the pupils felt about the tests they took in the project.\n\u2022 47 per cent preferred the paper test, 25 per cent the reverse translation test, 4 per cent\nthe computer test with 24 per cent expressing no preference.\n\u2022 4 per cent found the paper test the most stressful, 4 per cent the reverse translation test,\n69 per cent the computer test with 23 per cent finding no difference in stress levels\nbetween the tests.\n\u2022 7 per cent found the computer tests a bit easier than the paper tests, 62 per cent found\nthem a bit harder while 31 per cent found them to be much the same in terms of\ndifficulty.\n\u2022 20 per cent thought that using the computer was a better way to be tested in\nmathematics, 65 per cent thought it was not a better way and 15 per cent felt there was\nno difference.\nAlthough these results were of some interest, they were not used in the subsequent analysis\nof the experiment. The pupils who took part in this project had been used to taking tests\non paper throughout their school careers. Considering how little experience they had with\ncomputer tests, perhaps these views on the computer tests are not surprising.\nMarking and analysis\nAfter the pupils sat the tests they were marked as follows. The P format papers were\nmarked by the SQA as though they had been Higher examinations. The RTW examina-\ntions were marked at Heriot-Watt using the SQA marking scheme. The ICT tests were\nmarked (automatically) by computer, while the RTC examinations were marked at Heriot-\nWatt using the same marking scheme that was used by the computer. The statistical\nanalysis involved two separate comparisons. The first was to investigate the rewording\neffect by comparing P marks with RTW marks. The second was to investigate the medium\neffect by comparing ICT marks with RTC marks.\nWithin both schools each of the three tests was taken in each of its three forms by one of\nthe three groups of pupils. With information available on the ability of the pupils,\nStandard Grade Mathematics results for the Falkirk pupils and Higher Mathematics\npreliminary marks for the Queensferry pupils, matched pairs were constructed. For\n64\nAlt-] Volume 10 Number I\nexample, one Falkirk pair consisted of two male pupils both with grade 1 in Standard\nGrade Mathematics, such that one of the pair sat the P version of Test 1 while the other sat\nthe RTW version of the same test. Whereas one Queensferry pair consisted of two female\npupils scoring 67 and 68, respectively, in their preliminaries, such that one sat the ICT\nversion of test 3 while the other sat the RTC version of the same test. In this way 14\nmatched pairs were created from the Falkirk pupils, 11 being matched for ability and\ngender and 3 for ability but with different genders, and 48 from the Queensferry pupils, all\nbeing matched for ability and gender. This resulted in a total of 62 matched pairs. Due to\nabsences of pupils on the day of the tests a very small number of test marks were not used.\nStatistically a matched pair analysis provides the most efficient use of the data and the\nsample size of 62 was large enough to validate the analysis without the need for any\nassumptions such as requiring a normal distribution (McGhee, 1985).\nThe two statistical analyses were based on the 62 differences in marks for the matched pairs\nof pupils, either (P - RTW) or (ITC - RTC). In each case the null hypothesis was that the\ntrue underlying mean difference was zero against a two-sided alternative hypothesis. A one-\nsample t-test on these differences was performed using the statistical package Minitab with\nthe following results.\nThe rewording effect\nThe mean of the 62 observed differences was -2.3 marks so that RTW marks were greater\non average by 2.3 marks in tests, which were marked out of 21 or 22 (see Table 1).\nN Mean StDev SE Mean\nPMark\nRTW Mark\nDifference\n62\n62\n62\n8.484\n10.774\n-2.290\n4.742\n4.723\n4.194\n0.602\n0.600\n0.533\n95% Cl for mean difference: (-3.355, -1.225)\nT-Test of mean difference = 0 (vs not = 0):T-Value = -4.30 P-Value = 0.0005\nTable I: Paired T-Test and confidence interval for P mark \u2014 RTW mark\nThe observed t-statistic is -4.3 with a probability-value of less than 0.001 to 3 d.p., which is\nhighly significant, giving very strong evidence of a difference between the P marks and the\nRTW marks.\nHowever, it was noted that the P format papers were marked by SQA while the RTW\npapers were marked at Heriot-Watt using the SQA marking scheme. Therefore a potential\nsource of variation between the two sets of marks could have been due to the different\nmarkers and not just the rewording effect. In order to eliminate any such marker effect and\nso isolate the rewording effect the P format papers were remarked at Heriot-Watt by the\nsame marker as for the RTW papers, using the same marking scheme. This resulted in some\nsmall changes and the data were re-analysed to give the results in Table 2. The set of P\nmarks obtained from remarking is referred to as PMY mark in the output.\n65\nD.J. Fiddes et al\nPMY Mark\nRTW Mark\nDifference\nDoes the mode\nN\n62\n62\n62\nof delivery affect mathematics examination results?\nMean\n9.065\n10.774\n-1.710\nStDev\n4.627\n4.723\n4.079\nSE Mean\n0.588\n0.600\n0.518\n95% Cl for mean difference: (-2.745, -0.674)\nT-Test of mean difference = 0 (vs not = 0):T-Value = -3.30 P-Value = 0.002\nTable 2: Paired T-Test and confidence interval for PMY mark - RTW mark\nSo the mean difference is now -1.7 marks and this can be said to be due to the rewording\neffect. The probability value is 0.002, which still provides very strong evidence of a\ndifference due to the rewording effect.\nThese results are illustrated in Figure 3, which gives a histogram of the 62 observed\ndifferences together with a 95 per cent confidence interval for the underlying mean and the\nnull hypothesis mean presented below the histogram. The fact that the null hypothesis (Ho)\nmean is well outside the confidence interval illustrates the very strong evidence of a\ndifference due to the rewording.\n10-\nFr\neq\nue\nnc\ny\nI \nI\nHistogram of the (P - RTW) differences\n(with Ho and 95% t-confidence interval for the mean)\nI \u2022 \u2022:\u2022\u2022-\u2022\u2022\n-\n-\n-\n-\n-\n-\n-\n-\nX Ho\ni i I I I I I I I I\n-12 -10 - 8 - 6 - 4 - 2 0 2 4 6\n(P - RTW) d fferences\nFigure 3\nThe medium effect\nThe mean of the 62 observed differences was only -0.2 marks so that RTC marks were\ngreater on average by 0.2 marks in these tests. In this case this is not a significant difference\n(see Table 3).\nThe probability value is 0.57 showing that the observed differences could easily have\noccurred by chance. Again this is illustrated in Figure 4. This time the null hypothesis mean\n66\nis well inside\nmedium effect\nCMark\nRTC Mark\nDifference\nthe\nN\n62\n62\n62\nconfidence interval,\nMean\n7.479\n7.065\n0.415\nindicating no\nStDev\n5.654\n4.534\n5.738\nevidence of\nSE Mean\n0.718\n0.576\n0.729\nAlt-J Volume 10 Number 1\na difference due to the\n95% Cl for mean difference: (-1.043, 1.872)\nT-Test of mean difference = 0 (vs not = 0):T-Value = 0.57 P-Value = 0.572\nTable 3: PairedT-Test and confidence interval for C mark - RTC mark\nFr\neq\nue\nnc\ny\nO\n \n<7\nI \nO\n1 \n1 \n1\nHistogram of (C - RTC) differences\n(with Ho and 95% t-confidence interval for the mean)\nr\u2014 \u2014\n\u2014\n- T ]\nE\u2014\u20141\ni i i i\n20 -10 0 10\n(C - RTC) differences\nFigure 4\nAdditional comments\nThe average mark over all the tests was 39 per cent, with the individual marks varying from\n0 per cent to 100 per cent. Although an investigation into a gender effect was not the main\npurpose of this investigation, a comparison was possible due to the fact that all but three of\nthe 62 pairs were matched for gender. Accordingly the above analyses were repeated for the\nmale pairs and for the female pairs. The results showed that for both the rewording effect\nand the medium effect the conclusions were the same as above.\nConclusions and directions for further study\nThe difference in marks between P format and RTW format may be due to the rewording,\nthe formatting (in terms of the number of questions on a page) or the fact that the working\nfor the answers in RTW format was not written in a linear way. In P format, the answers\nwere written with one line following the other making it fairly clear where any error\n67\nD.J. Fiddes et al Does the mode of delivery affect mathematics examination results?\noccurred. If two answers were given in RTW format with one correct and one wrong, it was\nnot as clear which answer the pupils had meant as their answer since, for example, both\nanswers may have appeared side by side. Accordingly, pupils may have been given the\nbenefit of the doubt on occasions. This might be a reason why the RTW marks were higher\nthan P marks. A more detailed study may be required to clarify this issue.\nCandidates had little prior experience of computer tests. They were also required to use\ndifferent mathematical symbols during the computer tests when typing answers into the\ncomputer from those with which they were familiar in paper tests. So even with the small\namount of practice candidates may have obtained in the trial test beforehand, many would\nstill have some anxiety when sitting the computer test. Despite all of this, the major\nconclusion drawn from this project is that the medium has no effect on the marks for these\ntests.\nThe CUE assessment system has evolved following a number of educational experiments\nover the last fifteen years. One of its important pedagogic enhancements occurs with the\nintroduction of steps in questions (Beevers, McGuire, Stirling and Wild, 1995). Steps are a\npossible way of providing partial credit in computer tests (Beevers, Youngson, McGuire,\nWild and Fiddes, 1999; Lawson, 2001). There were no steps in any of the questions in the\ncurrent project. A second investigation is planned into the effect of steps on exam results.\nReferences\nBeevers, C. E. (2000), 'Computer aided assessment in mathematics at Heriot-Watt\nUniversity', Maths, Stats and OR Newsletter, 1, 17-19.\nBeevers, C. E., McGuire, G. R., Stirling, G. and Wild, D. G. (1995), 'Mathematical ability\nassessed by computer', Computers and Education 25, 3, 123-32.\nBeevers, C. E., Youngson, M. A., McGuire, G. R., Wild, D. G. and Fiddes, D. J. (1999),\n'Issues of partial credit in mathematical assessment by computer', ALT-J, 7, 26-32.\nBeevers, C. E. and Paterson, J. S. (2002), 'Assessment in mathematics', in P. Khan and\nJ. Kyle (eds), Effective Teaching and Learning in Mathematics and its Applications, London:\nKogan Page.\nBull, J. and McKenna, C. (2001), 'Blueprint for computer-assisted assessment'. Available\nfrom: http:\/\/www.caacentre\/bp\nCALM Group (2001), 'CUE assessment system'. Available from: http:\/\/www.calm.\nhw.ac.uk\/cue.html\/\nLawson, D. (2001), 'Computer-aided assessment in relation to learning outcomes'.\nAvailable from: http.\/\/ltsn. mathstore. ac. uk\/articles\/maths-caa-series\/\nLee, G. and Weerakoon, P. (2001), 'The role of computer-aided assessment in health\nprofessional education: a comparison of student performance in computer-based and\npaper-and-pen multiple-choice tests', Medical Teacher, 23, 152-7.\nMcGhee, J. W. (1985), Introductory Statistics, St Paul, MN: West Publishing.\n68\nAlt-J Volume 10 Number I\nSims-Williams, J. (1999), 'Open testing with a large databank of multiple choice questions',\nTeaching Mathematics and its Applications, 18, 159-61. (Also http:\/\/www.tal.bris.ac.uk\/).\nWhite, S. (2001), 'Electrical and electronic engineering assessment network'. Available\nfrom: http:\/\/www.e3an.ac.uk\/\n69\n"}