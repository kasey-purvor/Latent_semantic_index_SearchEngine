{"doi":"10.1007\/978-3-540-30473-9_5","coreId":"70296","oai":"oai:eprints.lancs.ac.uk:12529","identifiers":["oai:eprints.lancs.ac.uk:12529","10.1007\/978-3-540-30473-9_5"],"title":"Using Cooperative Artefacts as a Basis for Activity Recognition","authors":["Strohbach, Martin","Kortuem, Gerd","Gellersen, Hans","Kray, Christian"],"enrichments":{"references":[{"id":16313862,"title":"Activity Recognition in the Home using Simple and Ubiquitous Sensors.","authors":[],"date":"2004","doi":"10.1007\/978-3-540-24646-6_10","raw":"Tapia, E. M., Intille, S. and Larson, K.: Activity Recognition in the Home using Simple and Ubiquitous Sensors. Proc. Pervasive 2004, Vienna, April 2004.","cites":null},{"id":16313801,"title":"Advanced Interaction in Context.","authors":[],"date":null,"doi":"10.1007\/3-540-48157-5_10","raw":"Schmidt, A., Aidoo, K.A., Takaluoma, A., Tuomela, U., Van Laerhoven, K., Van de Velde, W.: Advanced Interaction in Context. In Proc. of HUC99, Karlsruhe, Germany,","cites":null},{"id":16313799,"title":"Context-aware computing applications.","authors":[],"date":null,"doi":"10.1109\/mcsa.1994.512740","raw":"Schilit, B. Adams, N. and Want, R.: Context-aware computing applications. Proc. WMCSA\u201994.","cites":null},{"id":16313861,"title":"Cooperative Artefacts: Assessing Real World Situations with Embedded Technology. Accepted for Publication in","authors":[],"date":"2004","doi":"10.1007\/978-3-540-30119-6_15","raw":"Strohbach, M., Gellersen, H.-W., Kortuem, G., Kray, C.: Cooperative Artefacts: Assessing Real World Situations with Embedded Technology. Accepted for Publication in Proc. of Ubicomp 2004, Nottingham, UK 2004.","cites":null},{"id":16313797,"title":"CyberCode: Designing Augmented Reality Environments with Visual Tags.","authors":[],"date":"2000","doi":"10.1145\/354666.354667","raw":"Rekimoto J. and Ayatsuka, Y.: CyberCode: Designing Augmented Reality Environments with Visual Tags. Proc. Designing Augmented Reality Environments (DARE 2000), 2000.","cites":null},{"id":16313867,"title":"Designing calm technology.","authors":[],"date":null,"doi":"10.1007\/978-1-4612-0685-9_6","raw":"Weiser, M. and Brown, J. S.: Designing calm technology. PowerGrid Journal 1.01, July","cites":null},{"id":16313752,"title":"E.: Wireless Sensor Networks: A Survey.","authors":[],"date":"2002","doi":"10.1016\/s1389-1286(01)00302-4","raw":"Akyildiz, I. F., Su, W., Sankarasubramaniam, Y., Cayirci, E.: Wireless Sensor Networks: A Survey. In Computer Networks, 38(4), March 2002, pp. 393\u2013422.","cites":null},{"id":16313757,"title":"EasyLiving: Technologies for Intelligent Environments.","authors":[],"date":"2000","doi":"10.1007\/3-540-39959-3_2","raw":"Brumitt, B., Meyers, B., Krumm, J., Kern, A. and Shafer, S. EasyLiving: Technologies for Intelligent Environments. Proc. of HUC 2000, Bristol, UK, Sept. 2000.","cites":null},{"id":16313764,"title":"eSeal - A System for Enhanced Electronic Assertion of Authenticity and Integrity.","authors":[],"date":"2004","doi":"10.1007\/978-3-540-24646-6_19","raw":"Decker, C., Beigl, M., Krohn, A., Robinson, P. and Kubach, U.: eSeal - A System for Enhanced Electronic Assertion of Authenticity and Integrity. In Proc. Of Pervasive 2004, Vienna, Austria, April 2004.","cites":null},{"id":16313792,"title":"et al.: Recognizing Workshop Activity Using Body Worn Microphones and Accelerometers.","authors":[],"date":"2004","doi":"10.1007\/978-3-540-24646-6_2","raw":"Lukowicz, P. et al.: Recognizing Workshop Activity Using Body Worn Microphones and Accelerometers. Proc. Pervasive 2004, Vienna, Austria 2004.","cites":null},{"id":16313762,"title":"Finding a Place for Ubicomp in the Home.","authors":[],"date":"2003","doi":"10.1007\/978-3-540-39653-6_17","raw":"Crabtree, A., Rodden, T., Hemmings, T., Benford, S.: Finding a Place for Ubicomp in the Home. In Proceedings of the 5 th International Conference on Ubiquitous Computing Ubicomp 2003, Seattle, WA ,USA, October 2003.","cites":null},{"id":16313768,"title":"G.D.: A Conceptual Framework and a Toolkit for Supporting the Rapid Prototyping of Context-Aware Applications In","authors":[],"date":"2001","doi":"10.1207\/s15327051hci16234_02","raw":"Dey, A.K., Salber, D. Abowd, G.D.: A Conceptual Framework and a Toolkit for Supporting the Rapid Prototyping of Context-Aware Applications In Human-Computer Interaction (HCI) Journal, Vol. 16 (2-4), 2001, pp. 97-166.","cites":null},{"id":16313774,"title":"H-W.: SmartIts Friends: A Technique for Users to Easily Establish Connections between Smart Artefacts.","authors":[],"date":"2001","doi":"10.1007\/3-540-45427-6_10","raw":"Holmquist, L.E., Mattern, F., Schiele, B., Alahuhta, P., Beigl, M., Gellersen, H-W.: SmartIts Friends: A Technique for Users to Easily Establish Connections between Smart Artefacts. In Proc. Ubicomp 2001, Atlanta, USA, Sept. 2001.  13.  Holmquist, L. E., Antifakos, S., Schiele, B., Michahelles, F., Beigl, M., Gaye, L., Gellersen, H.-W., Schmidt, A., Strohbach, M.: Building Intelligent Environments with Smart-Its. SIGGRAPH 2003, Emerging Technologies exhibition, San Diego USA.","cites":null},{"id":16313804,"title":"H.-W.: Context Acquisition based on Load Sensing.","authors":[],"date":"2002","doi":"10.1007\/3-540-45809-3_26","raw":"Schmidt, A., Strohbach, M., Van Laerhoven, K., Friday, A., Gellersen, H.-W.: Context Acquisition based on Load Sensing. In Proc. of Ubicomp 2002, Gothenburg, Sweden.","cites":null},{"id":16313807,"title":"H.-W.: Ubiquitous interaction - Using surfaces in everyday environments as pointing devices.","authors":[],"date":"2002","doi":"10.1007\/3-540-36572-9_21","raw":"Schmidt, A., Strohbach, M., Van Laerhoven, K., Gellersen, H.-W.: Ubiquitous interaction - Using surfaces in everyday environments as pointing devices. In Lecture Notes in Computer Science (LNCS), Volume 2615, N. Carbonell & C. Stephanidis (Eds.). Springer Verlag, 2002, pp. 263-279.","cites":null},{"id":16313771,"title":"Homepage,","authors":[],"date":null,"doi":null,"raw":"DIY Smart-its Homepage, http:\/\/ubicomp.lancs.ac.uk\/smart-its","cites":null},{"id":16313751,"title":"Implementing a Sentient Computing System.","authors":[],"date":"2001","doi":"10.1109\/2.940013","raw":"Addlesee, M., Curwen, R., Hodges, S., Newman, J., Steggles, P., Ward, A., Hopper, A. Implementing a Sentient Computing System. IEEE Computer 34(5), Aug. 2001, pp. 50-56.","cites":null},{"id":16313811,"title":"Interaction in Pervasive Computing Settings using Bluetooth-enabled Active Tags and Passive RFID Technology together with Mobile Phones.","authors":[],"date":"2003","doi":"10.1109\/percom.2003.1192762","raw":"Siegemund, F. and Fl\u00f6rkemeier, C.: Interaction in Pervasive Computing Settings using Bluetooth-enabled Active Tags and Passive RFID Technology together with Mobile Phones. Proc. IEEE PerCom 2003, March 2003, Fort Worth, USA.","cites":null},{"id":16313754,"title":"Mediacups: Experience with Design and","authors":[],"date":"2001","doi":"10.1016\/s1389-1286(00)00180-8","raw":"Beigl, M., Gellersen H., Schmidt, A. Mediacups: Experience with Design and Use of Computer-Augmented Everyday Artefacts. Computer Networks 35(4), March 2001.","cites":null},{"id":16313777,"title":"On sentences which are true of direct unions of algebras.","authors":[],"date":"1951","doi":"10.2307\/2268661","raw":"Horn, A.: On sentences which are true of direct unions of algebras. Journal of Symbolic Logic, 16, 14-21, 1951.","cites":null},{"id":16313783,"title":"Places, Things: Web Presence for the Real World.","authors":[],"date":"2002","doi":"10.1109\/mcsa.2000.895378","raw":"Kindberg, T., et al.: People, Places, Things: Web Presence for the Real World. In MONET Vol. 7, No. 5, Oct. 2002, Kluwer Publ.","cites":null},{"id":16313753,"title":"Proactive Instructions for Furniture Assembly.","authors":[],"date":"2002","doi":"10.1007\/3-540-45809-3_27","raw":"Antifakos, S., Michahelles F., Schiele, B,: Proactive Instructions for Furniture Assembly. Proc. Ubicomp 2002, Gothenburg, Sweden, Sept. 2002.","cites":null},{"id":16313758,"title":"Reactive Environments: Throwing Away Your Keyboard and Mouse.","authors":[],"date":"1997","doi":"10.1145\/260750.260774","raw":"Cooperstock, J.R. Fels, S. S., Buxton, W. and Smith, K.C. Reactive Environments: Throwing Away Your Keyboard and Mouse. Comm of the ACM 40(9), Sept. 1997.","cites":null},{"id":16313794,"title":"Smart rooms,","authors":[],"date":"1996","doi":"10.1038\/scientificamerican0496-68","raw":"Pentland, A.: Smart rooms, Scientific American, vol. 274, pp. 54-62, 1996.","cites":null},{"id":16313786,"title":"SPECs: Another Approach to Human Context and Activity Sensing Research.","authors":[],"date":"2003","doi":"10.1007\/978-3-540-39653-6_15","raw":"Lamming, M., Bohm, D.: SPECs: Another Approach to Human Context and Activity Sensing Research. In Proceedings of Ubicomp 2003. Seattle, WA, USA, October 2003.","cites":null},{"id":16313780,"title":"The Aware Home: A Living Laboratory for Ubiquitous Computing Research.","authors":[],"date":"1999","doi":"10.1007\/10705432_17","raw":"Kidd, C., Orr, R., Abowd, G., Atkeson, C., Essa, I., MacIntyre, B. Mynatt, E., Starner, T and Newstetter, W.: The Aware Home: A Living Laboratory for Ubiquitous Computing Research. In Proc. Cooperative Buildings, CoBuild\u201999, Pittsburgh, Oct 1999.","cites":null},{"id":16313789,"title":"The Potential of RFID for Movable Asset Management.","authors":[],"date":"2003","doi":"10.1007\/1-84628-321-3_4","raw":"Lampe M. and Strassner M.: The Potential of RFID for Movable Asset Management. Workshop on Ubiquitous Commerce at Ubicomp 2003, Seattle, October 2003","cites":null},{"id":16313864,"title":"Ubiquitous Chip: a Rule-based I\/O Control Device for Ubiquitous Computing.","authors":[],"date":"2004","doi":"10.1007\/978-3-540-24646-6_18","raw":"Terada, T., Tsukamoto, M., Hayakawa, K., Yoshihisa, T., Kishino, Y., Kashitani, A. and Nishio, S.: Ubiquitous Chip: a Rule-based I\/O Control Device for Ubiquitous Computing. In Proc. of Pervasive 2004, Vienna, April 2004.","cites":null},{"id":16313814,"title":"Visual Context awareness in Wearable Computing.","authors":[],"date":"1998","doi":"10.1109\/iswc.1998.729529","raw":"Starner, T., Schiele, B. and Pentland, A.: Visual Context awareness in Wearable Computing. Proc. Intl. Symp. on Wearable Computing (ISWC\u201998), Pittsburgh, Oct. 1998, pp. 50-57.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2004-11","abstract":"Ambient intelligent applications require applications to recognise user activity calmly in the background, typically by instrumentation of environments. In contrast, we propose the concept of Cooperative Artefacts (CAs) to instrument single artefacts that cooperate with each other to acquire knowledge about their situation in the world. CAs do not rely on external infrastructure as they implement their architectural components, i.e. perceptual intelligence, domain knowledge and a rule-based inference engine, on embedded devices. We describe the design and implementation of the CA concept on an embedded systems platform and present a case study that demonstrates the potential of the CA approach for activity recognition. In the case study we track surface-based activity of users by augmenting a table and household goods","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/70296.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/12529\/1\/2004%2DStrohbach%2DUsing_Cooperative_Artifacts.pdf","pdfHashValue":"89f48fde44bb67d49cbd503db52df1928260f542","publisher":"Springer Verlag","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:12529<\/identifier><datestamp>\n      2017-12-09T00:05:52Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D626F6F6B5F73656374696F6E<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Using Cooperative Artefacts as a Basis for Activity Recognition<\/dc:title><dc:creator>\n        Strohbach, Martin<\/dc:creator><dc:creator>\n        Kortuem, Gerd<\/dc:creator><dc:creator>\n        Gellersen, Hans<\/dc:creator><dc:creator>\n        Kray, Christian<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        Ambient intelligent applications require applications to recognise user activity calmly in the background, typically by instrumentation of environments. In contrast, we propose the concept of Cooperative Artefacts (CAs) to instrument single artefacts that cooperate with each other to acquire knowledge about their situation in the world. CAs do not rely on external infrastructure as they implement their architectural components, i.e. perceptual intelligence, domain knowledge and a rule-based inference engine, on embedded devices. We describe the design and implementation of the CA concept on an embedded systems platform and present a case study that demonstrates the potential of the CA approach for activity recognition. In the case study we track surface-based activity of users by augmenting a table and household goods.<\/dc:description><dc:publisher>\n        Springer Verlag<\/dc:publisher><dc:date>\n        2004-11<\/dc:date><dc:type>\n        Contribution in Book\/Report\/Proceedings<\/dc:type><dc:type>\n        NonPeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/12529\/1\/2004%2DStrohbach%2DUsing_Cooperative_Artifacts.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1007\/978-3-540-30473-9_5<\/dc:relation><dc:identifier>\n        Strohbach, Martin and Kortuem, Gerd and Gellersen, Hans and Kray, Christian (2004) Using Cooperative Artefacts as a Basis for Activity Recognition. In: Ambient Intelligence Second European Symposium, EUSAI 2004. Springer Verlag, pp. 49-60.<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/12529\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1007\/978-3-540-30473-9_5","http:\/\/eprints.lancs.ac.uk\/12529\/"],"year":2004,"topics":["QA75 Electronic computers. Computer science"],"subject":["Contribution in Book\/Report\/Proceedings","NonPeerReviewed"],"fullText":"Using Cooperative Artefacts as Basis for Activity \nRecognition \nMartin Strohbach, Gerd Kortuem, Hans-Werner Gellersen, Christian Kray \nComputing Department, Lancaster University, Lancaster LA1 4YR \n{strohbach, kortuem, hwg, kray}@comp.lancs.ac.uk \nAbstract. Ambient intelligent applications require applications to recognise \nuser activity calmly in the background, typically by instrumentation of \nenvironments. In contrast, we propose the concept of Cooperative Artefacts \n(CAs) to instrument single artefacts that cooperate with each other to acquire \nknowledge about their situation in the world. CAs do not rely on external \ninfrastructure as they implement their architectural components, i.e. perceptual \nintelligence, domain knowledge and a rule-based inference engine, on \nembedded devices. We describe the design and implementation of the CA \nconcept on an embedded systems platform and present a case study that \ndemonstrates the potential of the CA approach for activity recognition. In the \ncase study we track surface-based activity of users by augmenting a table and \nhousehold goods. \n1 Introduction \nAmbient Intelligence research aims to create environments that support the needs of \npeople with \u2018calm\u2019 technology. [4, 31]. To this effect many ambient intelligence \nsystems make use of knowledge about activities occurring in the physical \nenvironment to adapt their behaviour. Hence, one of the central research challenges of \nambient intelligence research is how such systems acquire, maintain, and use models \nof their changing environment. Approaches to address this challenge are generally \nbased on instrumentation of devices, physical artefacts or entire environments. \nSpecifically, instrumentation of otherwise non-computational artefacts has been \nshown to play an important role, e.g. for tracking of valuable goods [9, 18, 26], \ndetecting safety critical situations [28], or supporting human memory [17]. In most \naugmented artefact applications artefacts are instrumented to support their \nidentification [18, 21, 26] while perception, reasoning and decision-making is \nallocated in backend infrastructures [1, 6] or user devices [23, 27]. This approach, \nhowever, makes artefacts reliant on supporting infrastructure, and ties applications to \ninstrumented environments. \nIn this paper we introduce the notion of Cooperative Artefacts (CAs) that combine \nsensing, perception, reasoning and communication. Such artefacts are able to \ncooperatively identify, track and interpret activities in dynamically changing \nenvironments without relying on external infrastructure. Cooperative artefacts model \ntheir situation on the basis of domain knowledge, observation of the world, and \nsharing of knowledge with other artefacts. Thus, knowledge about the real world \nbecomes integral with the artefact itself. \nIn section 2 we introduce the notion of Cooperative Artefacts and describe the \nstructure and mechanisms by which CAs cooperate to acquire knowledge and reason \nabout the world. In section 3 we describe an implementation of the CA concept on an \nembedded systems platform based on a case study to demonstrate the potential of our \napproach for activity recognition applications. In the case study an augmented table \nand household goods are used to track surface-based user activities. In section 4 we \ndescribe the interaction between involved artefacts. Finally, we discuss related work \nin section 5 and conclude our paper in section 6. \n2 Cooperative Artefacts  \nCooperative Artefacts (CAs) are self-contained physical entities that are able to \nautonomously observe their environment and reason about these observations, thus \nacquiring knowledge about their world. Most notably they cooperate by sharing their \nknowledge which allows them to acquire more knowledge collectively than each of \nthem could acquire individually. It is a defining property of our approach that world \nknowledge associated with artefacts is stored and processed within the artefact itself. \nAlthough this structure is independent of any particular hardware platform, all \ncomponents are intended to be implemented on low-powered embedded devices with \ninherent resource limitations. Figure 1 depicts the architecture of a Cooperative \nArtefact. \n \nFig. 1. Architecture of a Cooperative Artefact \n\u2022 Sensors. Cooperative Artefacts include sensors which provide measurements of \nphenomena in the physical world. \n\u2022 Perception. The perception component associates sensor measurements with \nmeaning, producing observations that are meaningful in terms of the application \ndomain. \n\u2022 Knowledge base. The knowledge base stores the acquired knowledge of the \nartefact and externalises the artefact knowledge. \n\u2022 Inference. The inference component processes the knowledge of an artefact as \nwell as knowledge provided by other artefacts to infer further knowledge. \n2.1 Structure of the Artefact Knowledge Base  \nThe artefact knowledge base is structured into facts and rules. Facts are the \nfoundation for any decision-making and action-taking within the artefact, and rules \nallow inferring further knowledge based on facts and other rules (table 1). \nTable 1. Knowledge managed by a Cooperative Artefact. \nDomain \nKnowledge \nDomain knowledge built into the artefact, e.g. facts \ndescribing the physical nature of the artefact or general \nworld knowledge.  \nObservational \nKnowledge \nKnowledge describing the situation of an artefact in the \nworld. It is based on facts that result from sensor-based \nobservations. \nInferred \nKnowledge \nRules are used to infer further knowledge based on \npreviously established facts, which may be based on \ndomain knowledge, observation, previous inference, or \nknowledge made available by cooperating artefacts.  \n2.2 Cooperation between Artefacts \nActivity recognition applications will rely on rich knowledge about users and their \nenvironment. It is therefore a desirable feature that artefacts cooperate to maximise \nthe knowledge available about the physical world. Our model for cooperation is that \nartefacts share knowledge. More specifically, knowledge stored in an artefact\u2019s \nknowledge base is made available to other artefacts where they feed into the inference \nprocess generating additional knowledge. Effectively, the artefact knowledge bases \ntaken together form a distributed knowledge base on which the inference processes in \nthe individual artefacts can operate.  \nAlthough different artefacts may be able to observe the same physical \nphenomenon, the acquired knowledge is likely to be incomplete and different between \nobserving artefacts. This is due to the nature and diversity of the used sensors and \nperception algorithms. However, the artefacts can use the distributed knowledge to \nexchange and reason about their knowledge. This leads to a synergetic effect: \ncooperating artefacts are able to acquire more knowledge collectively than each of \nthem could acquire individually. This principle is illustrated in figure 2. \n Fig. 2. Cooperation of artefacts is based on sharing of knowledge \n3 CA Case Study: Tracking Surface-based User Activities  \nRecent research has identified activity centres as areas in domestic settings where \nhuman activity is focused on [8]. This research suggests that the identified activity \ncentres, such as surfaces provided by kitchen tables, should be considered as prime \nsites for technological augmentation. Other research has shown that tagged artefacts \nmay reveal valuable information about users\u2019 activity [17]. Based on this research we \nchose to track surface-based user activities as a case study for Cooperative Artefacts. \nThe basic application idea is to track artefacts on and across tables to infer the user \nactivity.  \n \nFig. 3. CA demonstrator \nThe CA demonstrator (Fig. 3) was developed to show the potential of Cooperative \nArtefacts for activity recognition applications. Glasses and jugs were built as artefacts \nthat are aware whether they are on surface or not. They cooperate with a load sensing \ntable to infer their location on the table and, as a further synergetic effect of \ncooperation, they can infer their filling state. The picture in Fig. 3 shows the table, \nglasses and jugs as it was demonstrated at SIGGRAPH [13]. This setup also included \nchairs to measure the weight distribution of people sitting on them. A graphical \nrepresentation of the position of the glasses and the jug on the table was projected on \na screen in order to display the recognised interactions of visitors with the artefacts. \nThis section describes the implementation of the CA architecture on an embedded \ndevice platform. \n3.1 Artefact Sensors and Perception \nAs a first step, the artefacts involved in the demonstrator require some basic form of \nintelligence, i.e. they need sensors and computing power to implement perception. \nFort this purposes we used the DIY Smart-its platform, an easy to use and highly \ncustomizable hardware platform for wireless sensing applications [11]. The modular \ndesign of the DIY Smart-its allows using a range of different sensors by plugging \nadd-on boards on a basic processing and communication board.  \nWe used an add-on board to interface four industrial load cells that were put under \neach leg of the table. With this technology we can easily augment most tables in an \nunobtrusive way. We implemented a perception algorithm on the microcontroller of \nthe DIY Smart-it to calculate events based on the weight changes on the table. Thus, \nwe are able to recognise when an artefact has been put or removed from the table. In a \nsecond step, we use the load distribution on the four load cells to calculate the \nposition of the artefact on the table surface. Additionally we obtain the weight of the \nartefact that has just been put or removed from the table. Further details about the \nimplementation of context acquisition based on load sensing and its potential \napplications have been published in [24] and [25]. \n \n \nFig. 4. Left: A mini Smart-its with battery attached to a water jug. The FSR sensor is mounted \non the bottom of the jug. The glass was augmented in a similar way. Right: A DIY Smart-it \nwith load add-on board attached to the frame of the table. The black cables connect to the four \nload cells. \nA smaller version of the DIY Smart-its has been used to augment jugs and glasses \nwith force sensitive resistance sensors (FSR sensors). Thus the perception algorithm \nof glasses and jugs can observe if the artefact is put down on a surface or lifted up. \nFigure 4 shows the physical augmentation of the artefacts. \nThus, each artefact is able to make individual observations about its world: \n\u2022 The load table observes the position and weight of artefacts on its top but does \nnot know about their identity, i.e. is it a jug, glass or something else. \n\u2022 Jugs and glasses observe whether they have been put on a surface or lifted up, \nbut they know nothing about their location. \n3.2 Artefact Knowledge Bases \nThese observations are stored and managed in the knowledge bases embedded in the \nindividual artefacts. They contain facts to represent the knowledge and rules to infer \nfurther knowledge. Each of the artefacts implements an inference engine similar to a \nsimple Prolog interpreter that operates on these facts and rules expressed in a subset \nof Horn logic [14]. The inference engine uses backward-chaining with depth first \nsearch as inference algorithm. Compromises in terms of expressiveness and generality \nwere necessary to facilitate the implementation on a micro-controller platform. The \ndata structures for the predicate arguments provide information whether the argument \nrefers to an external artefact which allows the inference engine to acquire knowledge \nfrom other artefacts using a query\/reply protocol.  \nTable 2. Knowledge base of a load table \nDomain \nKnowledge \nconcurrent(<time>,<time>) \nObservational \nKnowledge \nlocation_and_weight(me, x, y, _, w, \n<added\/removed>, <time>) \nRules  \n(R1) location_and_weight(me, X, Y, A, W, added, T2):- \nlocation_and_weight(me, X, Y, _, W, added, T1), \nlocation_and_weight(_, _, _, A, _, added, T2), \nconcurrent(T1,T2). \nTable 3. Knowledge base of a jug\/glass \nDomain \nKnowledge \nconcurrent(<time>,<time>) \nabove_weight_threshold(<weight>) \nbelow_weight_threshold(<weight>) \nObservational \nKnowledge \nlocation_and_weight(_, _, _, me, _, \n<added\/removed>, <time>) \nRules  \n(R2) filling_state(me, full, T2):- \nlocation_and_weight(TABLE, X, Y, _, W, EVENT1, T1), \nlocation_and_weight(_, _, _, me, _, EVENT2, T2), \nconcurrent(T1, T2), above_weight_threshold(W), EVENT1 \n== EVENT2. \n(R3) filling_state(me, empty, T2):- \nlocation_and_weight(TABLE, X, Y, _, W, EVENT1, T1), \nlocation_and_weight(_, _, _, me, _, EVENT2, T2), \nconcurrent(T1, T2), below_weight_threshold(W), EVENT1 \n== EVENT2. \nRules and some facts are specified by the developer. Other facts such as \nlocation_and_weight(<table>, <x>, <y>, <artefact>, <weight>, \n<added\/removed>, <time>) model the observational knowledge acquired by the \nartefacts. This observation indicates the position and weight on a table on which an \nartefact has been added or removed at a certain time. Both kinds of artefacts, the load \ntable and glasses\/jugs, model their observation with the same fact; however with \ndifferent levels of information according to their perception capabilities as described \nin the previous subsection. Table 2 lists the knowledge base of an load table while \nTable 3 lists the knowledge base of an jug or glass. Lowercase letters are constants \nand model a specific value of an argument, while we use the special character \u201c_\u201d to \nindicate an unknown or irrelevant value of an argument. Uppercase letters indicate \nvariables. The special constant me always refers to the artefact that stores the fact or \nrule. Arguments put in brackets are to be replaced with the concrete values for each \nobservation. Rule R1 can be verbalised as follows: \nR1: A table knows location, weight and identity of an artefact on its top if both, the \ntable and the artefact, observe the event of putting the artefact on the table at \napproximately the same time. \nThis rule relies on synchronised time between artefacts as each observation is time-\nstamped to determine time relationships between two observations. \nconcurrent(<time>, <time>) is semantically equivalent to the expression |T1-\nT2|<time_th with an appropriate threshold time_th. The table perception \nalgorithm for the location_and_weight observation takes a few milliseconds \nlonger than the algorithms of the glasses and jugs. Consequently we take T2, the time \nof the glass (or jug respectively) observation as a timestamp for the inferred location \nand weight. Rules R2 and R3 can be verbalised as follows: \nR2 and R3: A glass or jug knows its filling state when it observes the same event at \napproximately the same time as the table. \nRules R2 and R3 are a by-product of our initial goal to obtain location information \nabout artefacts that are put on the table. This is due to the fact that we measured the \nweight distribution to calculate the position of artefacts. These rules also make use of \nadditional domain knowledge: above_weight_threshold(<weight>) and \nbelow_weight_threshold(<weight>) model for each individual artefact a \nweight threshold to determine if a glass or jug is full or empty.  \n4 Tracking User Activities with the CA Demonstrator  \nIn this section we describe a set of actions as they have occurred during the demo \nsetup. We use an initially empty table with which users interact by putting and \nremoving a conventional bottle and the augmented glass on and from the table. The \nglass is initially empty and located on a conventional table. We perform the following \nactions: \nAction 1. Put a conventional bottle in the middle of the table \nAction 2. Put an empty, augmented glass on top left corner of the table \nAction 3. Remove glass from table and fill it with water \nAction 4. Put the glass back on the bottom right corner of the table \nInitially the artefact knowledge bases only contain their respective domain knowledge \nentries (cf. Table 2 and Table 3). The knowledge base of the glass also reflects that it \nis put on the conventional table. After putting the conventional bottle on the table the \nperception component of the table adds the corresponding observation to the \nknowledge base and we obtain the observations as detailed in Table 4. \nTable 4. Observations after action 1 \nGlass Table \nlocation_and_weight(_, _, \n_, me, _, added, 30) \nlocation_and_weight(me, 25, \n25, _, 400, added, 768) \nIn order to detect the changes in the environment and update the screen display, a \nquery for location_and_weight must be sent to the table. The query contains only \none value, the table identifier to which the query should be sent. All other arguments \ncontain variables representing the values we are interested in: \nlocation_and_weight(table, X, Y, A, W, EVENT, TIME) \nWhen the table receives this message it tries to unify the message with the entries in \nthe knowledge base. The inference engine always finds the most specific answer and \ntries to evaluate rule R1. It checks the premises of the rule and unifies the table \nobservation with the entry in the fact base. The external observation \nlocation_and_weight(_, _, _, A, _, added, T2) requires cooperation \nwith an unknown artefact A. Therefore, the inference engine issues a broadcast query \nfor this observation. The glass replies with its observation as detailed in table 3. \nHowever, rule R1 cannot be applied as the timestamps are not close in time. \nTherefore, the table replies with \nlocation_and_weight(me, 25, 25, _, 400, added, 768) \nnot being able to provide information about the actual identity of the bottle. The \nvisualization projected on the screen is now updated showing a question mark for the \nbottle. \nAfter the second action, both the table and the glass add new observations to their \nknowledge bases as shown in Table 5.  \nTable 5. Observations after action 2 \nGlass Table \n location_and_weight(me, 25, \n25, _, 400, added, 768) \nlocation_and_weight(_, _, \n_, me, _, added, 2103) \nlocation and weight(me, 25, \n25, _, 131, added, 2105) \nNote, that the perception component of the glass always updates the knowledge base \nto reflect the latest observed state, i.e. there was an intermediate observation when the \nglass was lifted from the conventional table that is not displayed in the table. Queries \nto the table will now result in replies that provide information about the location of \nthe glass:  \nlocation_and_weight(table, 25, 25, glass, 400, added, 2105) \nThis is a result from applying rule R2 which entails a broadcast query. This query is \nreplied by the glass with the corresponding observation that was made at nearly the \nsame time. The identity of the glass can now be used to query the glass about its \nfilling state: \nfilling_state(glass, FILLING_STATE, T2) \nIn order to answer this query the glass must cooperate with the table: rules R2 and R3 \nrely on the table\u2019s observations. Again, the table issues a broadcast query asking for \nthe observation made by the table. The table replies with his observations and the \nglass replies with the conclusion of rule R3: \nfilling_state(glass, empty, 2103) \nAgain the changes in the environment have been detected and the display can be \nupdated: \n \nFig. 5. Screenshot of the available knowledge after action 2. In the demonstrator \nquestion marks are used to represents unknown artefacts, their size is relative to their \nweight. Here the question mark represents the bottle. \nAfter the third action the knowledge bases of both artefacts are updated. In this state \nqueries to the table always return the location of the unknown artefact (the bottle) as \nthe observation stored in the glass relates to an earlier observation (cf. Table 6). \nTable 6. Observations after action 3 \nGlass Table \nlocation_and_weight(_, _, \n_, me, _, removed, 2876) \nlocation_and_weight(me, 25, \n25, _, 400, added, 1325) \nAs we fill the glass with water no new observations are added to either of the \nknowledge bases. After action 4, cooperation between artefacts is similar as after \naction 2 and we obtain the observations in Table 7: \nTable 7. Observations after action 4 \nGlass Table \n location and weight(me, 25, \n25, _, 400, added, 1325) \nlocation_and_weight(_, _, \n_, me, _, added, 3469) \nlocation_and_weight(me, 120, \n60, _, 400, added, 3472) \n5 Related Work \nOur work is generally related to other ubiquitous computing research concerned with \ninstrumentation of the world and with systems that adapt and react to their \ndynamically changing environment [10, 22, 1]. In contrast to our work, most of these \npreviously reported systems and infrastructures are based on instrumentation of \nlocations (e.g. office [1, 7, 20], home [6, 15, 29], or of users and their mobile devices \n[19, 23, 27]). \nWireless sensor networks are also generally related to our work as they use similar \ntechnology, i.e. wireless nodes with generic sensors [2]. However, they differ mainly \nin one point: functionalities implemented on wireless sensor nodes only include data \nacquisition and routing, i.e. taking measurements from sensors and sending the data to \na backend infrastructure where it is processed and potential models of individual \nnodes are maintained. \nPrevious research has also considered the role of artefacts in addition to locations \nand users, e.g. the Cooltown project provides a digital presence for people, places and \nthings [16], and SPECs is a proximity sensing hardware platform for activity \nrecognition [17]. Closer to our work are systems directly concerned with artefacts and \ntheir situations, e.g. for tracking of moveable assets [18, 26]. Particulary close in spirit \nis the eSeal system in which artefacts are instrumented with embedded sensing and \nperception autonomously monitor their physical integrity [9]. \nSeveral levels of integration of artefacts in ubiquitous computing systems have \nbeen explored, e.g. visual tags [21] and RFID tags [18, 26] to support unique \nidentification. SPECs have been attached to artefacts to capture movement \ninformation of users [17]. Collective behaviour of augmented artefacts has been \nexplored in the context of the Smart-its project, e.g. by integrating different kind of \nsensors in user\u2019s belongings [12], furniture [3], and cups [5]. A more generic \nframework, based on event-condition-action rules (ECA rules), has been provided by \nthe Ubiquitous Chip platform [30]. \n6 Conclusion \nIn this paper we demonstrated the potential of the CA concept for activity recognition \napplications. We have described the design and implementation on an embedded \nsystems platform in the context of a case study in which an augmented table and \nhousehold goods can be used to track surface-based user activities. There are three \ninnovative aspects to be noted: \n\u2022 It is a novel approach to acquire and maintain knowledge on activity and \nchanges in the world, distinct in being entirely embedded in moveable artefacts. \n\u2022 Embedding of generic reasoning capabilities constitutes a new quality of \nembedded intelligence not previously demonstrated for otherwise non-\ncomputational artefacts. \n\u2022 This approach has the potential to leverage activity recognition applications by \nproviding rich knowledge about situations in the world that can be especially \nuseful for deployment in users homes where installing external infrastructure \nmight be critical. \nCurrently we are working on a software framework for embedded devices called \narteFACT that fully implements the CA architecture. Among power efficiency and \nresponsive, our current prototype has also revealed the following two issues that we \nseek to address in our future work: \n\u2022 Activity recognition applications rely on timestamped data and history \ninformation. While we are currently extending our devices with FRAM and \nFlash memory to store history information, it will be crucial to include time \nas a fundamental concept. This will be especially important for querying \nhistory information. We are planning to look into possibilities of using \ntemporal logic in our current implementation of the embedded inference \nengine. \n\u2022 In order to improve the scalability of our architecture, we plan to include \nsubscriptions to changes in the artefact knowledge bases. This will entail to \ninclude forward reasoning as a more effective inference algorithm \nReferences \n1. Addlesee, M., Curwen, R., Hodges, S., Newman, J., Steggles, P., Ward, A., Hopper, A. \nImplementing a Sentient Computing System. IEEE Computer 34(5), Aug. 2001, pp. 50-56. \n2. Akyildiz, I. F., Su, W., Sankarasubramaniam, Y., Cayirci, E.: Wireless Sensor Networks: \nA Survey. In Computer Networks, 38(4), March 2002, pp. 393\u2013422. \n3. Antifakos, S., Michahelles F., Schiele, B,: Proactive Instructions for Furniture Assembly. \nProc. Ubicomp 2002, Gothenburg, Sweden, Sept. 2002.  \n4. Basten, T., Geilen, M., de Groot, H., (eds.): Ambient Intelligence: Impact on Embedded \nSystem Design. Kluwer Academic Publishers, Boston, 2003. \n5. Beigl, M., Gellersen H., Schmidt, A. Mediacups: Experience with Design and Use of \nComputer-Augmented Everyday Artefacts. Computer Networks 35(4), March 2001. \n6. Brumitt, B., Meyers, B., Krumm, J., Kern, A. and Shafer, S. EasyLiving: Technologies for \nIntelligent Environments. Proc. of HUC 2000, Bristol, UK, Sept. 2000. \n7. Cooperstock, J.R. Fels, S. S., Buxton, W. and Smith, K.C. Reactive Environments: \nThrowing Away Your Keyboard and Mouse. Comm of the ACM 40(9), Sept. 1997. \n8. Crabtree, A., Rodden, T., Hemmings, T., Benford, S.: Finding a Place for Ubicomp in the \nHome. In Proceedings of the 5th International Conference on Ubiquitous Computing \nUbicomp 2003, Seattle, WA ,USA, October 2003. \n9. Decker, C., Beigl, M., Krohn, A., Robinson, P. and Kubach, U.: eSeal - A System for \nEnhanced Electronic Assertion of Authenticity and Integrity. In Proc. Of Pervasive 2004, \nVienna, Austria, April 2004. \n10. Dey, A.K., Salber, D. Abowd, G.D.: A Conceptual Framework and a Toolkit for \nSupporting the Rapid Prototyping of Context-Aware Applications In Human-Computer \nInteraction (HCI) Journal, Vol. 16 (2-4), 2001, pp. 97-166. \n11. DIY Smart-its Homepage, http:\/\/ubicomp.lancs.ac.uk\/smart-its \n12. Holmquist, L.E., Mattern, F., Schiele, B., Alahuhta, P., Beigl, M., Gellersen, H-W.: Smart-\nIts Friends: A Technique for Users to Easily Establish Connections between Smart \nArtefacts. In Proc. Ubicomp 2001, Atlanta, USA, Sept. 2001.  \n13. Holmquist, L. E., Antifakos, S., Schiele, B., Michahelles, F., Beigl, M., Gaye, L., \nGellersen, H.-W., Schmidt, A., Strohbach, M.: Building Intelligent Environments with \nSmart-Its. SIGGRAPH 2003, Emerging Technologies exhibition, San Diego USA. \n14. Horn, A.: On sentences which are true of direct unions of algebras. Journal of Symbolic \nLogic, 16, 14-21, 1951. \n15. Kidd, C., Orr, R., Abowd, G., Atkeson, C., Essa, I., MacIntyre, B. Mynatt, E., Starner, T \nand Newstetter, W.: The Aware Home: A Living Laboratory for Ubiquitous Computing \nResearch. In Proc. Cooperative Buildings, CoBuild\u201999, Pittsburgh, Oct 1999.  \n16. Kindberg, T., et al.: People, Places, Things: Web Presence for the Real World. In MONET \nVol. 7, No. 5, Oct. 2002, Kluwer Publ. \n17. Lamming, M., Bohm, D.: SPECs: Another Approach to Human Context and Activity \nSensing Research. In Proceedings of Ubicomp 2003. Seattle, WA, USA, October 2003.  \n18. Lampe M. and Strassner M.: The Potential of RFID for Movable Asset Management. \nWorkshop on Ubiquitous Commerce at Ubicomp 2003, Seattle, October 2003 \n19. Lukowicz, P. et al.: Recognizing Workshop Activity Using Body Worn Microphones and \nAccelerometers. Proc. Pervasive 2004, Vienna, Austria 2004. \n20. Pentland, A.: Smart rooms, Scientific American, vol. 274, pp. 54-62, 1996. \n21. Rekimoto J. and Ayatsuka, Y.: CyberCode: Designing Augmented Reality Environments \nwith Visual Tags. Proc. Designing Augmented Reality Environments (DARE 2000), 2000. \n22. Schilit, B. Adams, N. and Want, R.: Context-aware computing applications. Proc. \nWMCSA\u201994. \n23. Schmidt, A., Aidoo, K.A., Takaluoma, A., Tuomela, U., Van Laerhoven, K., Van de \nVelde, W.: Advanced Interaction in Context. In Proc. of HUC99, Karlsruhe, Germany, \n1999. \n24. Schmidt, A., Strohbach, M., Van Laerhoven, K., Friday, A., Gellersen, H.-W.: Context \nAcquisition based on Load Sensing. In Proc. of Ubicomp 2002, Gothenburg, Sweden. \n25. Schmidt, A., Strohbach, M., Van Laerhoven, K., Gellersen, H.-W.: Ubiquitous interaction \n- Using surfaces in everyday environments as pointing devices. In Lecture Notes in \nComputer Science (LNCS), Volume 2615, N. Carbonell & C. Stephanidis (Eds.). Springer \nVerlag, 2002, pp. 263-279. \n26. Siegemund, F. and Fl\u00f6rkemeier, C.: Interaction in Pervasive Computing Settings using \nBluetooth-enabled Active Tags and Passive RFID Technology together with Mobile \nPhones. Proc. IEEE PerCom 2003, March 2003, Fort Worth, USA. \n27. Starner, T., Schiele, B. and Pentland, A.: Visual Context awareness in Wearable Computing. \nProc. Intl. Symp. on Wearable Computing (ISWC\u201998), Pittsburgh, Oct. 1998, pp. 50-57. \n28. Strohbach, M., Gellersen, H.-W., Kortuem, G., Kray, C.: Cooperative Artefacts: Assessing \nReal World Situations with Embedded Technology. Accepted for Publication in Proc. of \nUbicomp 2004, Nottingham, UK 2004. \n29. Tapia, E. M., Intille, S. and Larson, K.: Activity Recognition in the Home using Simple \nand Ubiquitous Sensors. Proc. Pervasive 2004, Vienna, April 2004. \n30. Terada, T., Tsukamoto, M., Hayakawa, K., Yoshihisa, T., Kishino, Y., Kashitani, A. and \nNishio, S.: Ubiquitous Chip: a Rule-based I\/O Control Device for Ubiquitous Computing. \nIn Proc. of Pervasive 2004, Vienna, April 2004. \n31. Weiser, M. and Brown, J. S.: Designing calm technology. PowerGrid Journal 1.01, July \n1996. \n"}