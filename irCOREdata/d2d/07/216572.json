{"doi":"10.1145\/509907.510005","coreId":"216572","oai":"oai:eprints.lse.ac.uk:31084","identifiers":["oai:eprints.lse.ac.uk:31084","10.1145\/509907.510005"],"title":"The complexity of approximating entropy","authors":["Batu, Tugkan","Dasgupta, Sanjoy","Kumar, Ravi","Rubinfeld, Ronitt"],"enrichments":{"references":[],"documentType":{"type":null}},"contributors":[],"datePublished":"2002","abstract":"We consider the problem of approximating the entropy of a discrete distribution under several models. If the distribution is given explicitly as an array where the i-th location is the probability of the i-th element, then linear time is both necessary and sufficient for approximating the entropy.We consider a model in which the algorithm is given access only to independent samples from the distribution. Here, we show that a &lgr;-multiplicative approximation to the entropy can be obtained in O(n(1+\u03b7)\/&lgr;2 < poly(log n)) time for distributions with entropy \u03a9(&lgr; \u03b7), where n is the size of the domain of the distribution and \u03b7 is an arbitrarily small positive constant. We show that one cannot get a multiplicative approximation to the entropy in general in this model. Even for the class of distributions to which our upper bound applies, we obtain a lower bound of \u03a9(nmax(1\/(2&lgr;2), 2\/(5&lgr;2\u20142)).We next consider a hybrid model in which both the explicit distribution as well as independent samples are available. Here, significantly more efficient algorithms can be achieved: a &lgr;-multiplicative approximation to the entropy can be obtained in O(&lgr;2.Finally, we consider two special families of distributions: those for which the probability of an element decreases monotonically in the label of the element, and those that are uniform over a subset of the domain. In each case, we give more efficient algorithms for approximating the entropy","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":"ACM","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lse.ac.uk:31084<\/identifier><datestamp>\n      2011-01-06T18:03:36Z<\/datestamp><setSpec>\n      74797065733D4445505453:4C53452D4D41<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lse.ac.uk\/31084\/<\/dc:relation><dc:title>\n        The complexity of approximating entropy<\/dc:title><dc:creator>\n        Batu, Tugkan<\/dc:creator><dc:creator>\n        Dasgupta, Sanjoy<\/dc:creator><dc:creator>\n        Kumar, Ravi<\/dc:creator><dc:creator>\n        Rubinfeld, Ronitt<\/dc:creator><dc:subject>\n        QA Mathematics<\/dc:subject><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        We consider the problem of approximating the entropy of a discrete distribution under several models. If the distribution is given explicitly as an array where the i-th location is the probability of the i-th element, then linear time is both necessary and sufficient for approximating the entropy.We consider a model in which the algorithm is given access only to independent samples from the distribution. Here, we show that a &lgr;-multiplicative approximation to the entropy can be obtained in O(n(1+\u03b7)\/&lgr;2 < poly(log n)) time for distributions with entropy \u03a9(&lgr; \u03b7), where n is the size of the domain of the distribution and \u03b7 is an arbitrarily small positive constant. We show that one cannot get a multiplicative approximation to the entropy in general in this model. Even for the class of distributions to which our upper bound applies, we obtain a lower bound of \u03a9(nmax(1\/(2&lgr;2), 2\/(5&lgr;2\u20142)).We next consider a hybrid model in which both the explicit distribution as well as independent samples are available. Here, significantly more efficient algorithms can be achieved: a &lgr;-multiplicative approximation to the entropy can be obtained in O(&lgr;2.Finally, we consider two special families of distributions: those for which the probability of an element decreases monotonically in the label of the element, and those that are uniform over a subset of the domain. In each case, we give more efficient algorithms for approximating the entropy.<\/dc:description><dc:publisher>\n        ACM<\/dc:publisher><dc:date>\n        2002<\/dc:date><dc:type>\n        Book Section<\/dc:type><dc:type>\n        NonPeerReviewed<\/dc:type><dc:identifier>\n          Batu, Tugkan and Dasgupta, Sanjoy and Kumar, Ravi and Rubinfeld, Ronitt  (2002) The complexity of approximating entropy.   In:  Proceedings of the Thiry-Fourth Annual ACM Symposium on Theory of Computing - Stoc '02.  ACM, New York, USA, pp. 678-687.  ISBN 1581134959     <\/dc:identifier><dc:relation>\n        http:\/\/portal.acm.org\/citation.cfm?doid=509907.510005<\/dc:relation><dc:relation>\n        10.1145\/509907.510005<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":null,"relations":["http:\/\/eprints.lse.ac.uk\/31084\/","http:\/\/portal.acm.org\/citation.cfm?doid=509907.510005","10.1145\/509907.510005"],"year":2002,"topics":["QA Mathematics","QA75 Electronic computers. Computer science"],"subject":["Book Section","NonPeerReviewed"],"fullText":null}