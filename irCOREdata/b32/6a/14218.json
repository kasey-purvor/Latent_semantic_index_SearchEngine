{"doi":"10.1080\/0968776020100306","coreId":"14218","oai":"oai:generic.eprints.org:390\/core5","identifiers":["oai:generic.eprints.org:390\/core5","10.1080\/0968776020100306"],"title":"How accurately do instructors judge students\u2019 attitudes online? A measurement of expectations and level of satisfaction with an Online Information Systems masters program","authors":["Macht, Lauren\u2010Nicole","Preece, Jenny"],"enrichments":{"references":[{"id":197651,"title":"Learning Networks: A Field Guide to Teaching and Learning Online,","authors":[],"date":"1995","doi":"10.1080\/14626269609408367","raw":"Harasim, L., Hiltz, S. R., Teles, L. and Turroff, M. (1995), Learning Networks: A Field Guide to Teaching and Learning Online, Cambridge, MA: MIT Press.","cites":null},{"id":197652,"title":"The Virtual Classroom: Learning without Limits via Computer Networks, Human-Computer Interaction Series,","authors":[],"date":"1994","doi":"10.1177\/089443939701500326","raw":"Hiltz, S. R. (1994), The Virtual Classroom: Learning without Limits via Computer Networks, Human-Computer Interaction Series, Norwood, NJ: Ablex Publishing Group.","cites":null},{"id":197653,"title":"Digital diploma mills: the automation of higher education',","authors":[],"date":"1998","doi":"10.5210\/fm.v3i1.569","raw":"Noble, D. (1998), 'Digital diploma mills: the automation of higher education', Educom Review, 33 (3).","cites":null},{"id":197654,"title":"Usability and learning: evaluating the potential of educational software',","authors":[],"date":"1996","doi":"10.1016\/0360-1315(96)00010-3","raw":"Squires, D. and Preece, J. (1996), 'Usability and learning: evaluating the potential of educational software', Computers and Education, 27 (1), 15-22.","cites":null},{"id":197655,"title":"Predicting quality in educational software: evaluating for learning, usability, and the synergy between them',","authors":[],"date":"1999","doi":null,"raw":"Squires, D. and Preece, J. (1999), 'Predicting quality in educational software: evaluating for learning, usability, and the synergy between them', Interacting with Computers: The Interdisciplinary Journal of Human-Computer Interaction, 11 (5), 467-83.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2002","abstract":"In order to run a successful educational program, instructors as well as staff members must constantly review and adapt to the expectations, concerns, demographics and satisfaction level of their student consumers. This study was conducted in order to examine these issues in an online educational setting. First, interviews were given to the program instructors in order to determine their opinions about the students\u2019 expectations and satisfaction levels. This information was then used to create a student survey that assessed the students\u2019 expectations and level of satisfaction. These two sets of results were then compared This comparison revealed that the online instructors did have a good grasp of the online students\u2019 expectations, concerns, demographics and satisfaction level. The only areas where the instructors\u2019 concepts of student views were slightly less accurate was student concerns and student feelings about the program administration, where the instructors overestimated the level of concern the students had about successfully returning to the learning environment and underestimated the students\u2019 satisfaction with the program's administration. This leads us to conclude that, even with the added online factor, instructors strongly understand student expectations, satisfaction levels, demographics and concerns","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/14218.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/390\/1\/ALT_J_Vol10_No3_2002_How%20accurately%20do%20instructors%20.pdf","pdfHashValue":"7c24bee126114e71a46206c7d2ba707cc790579f","publisher":"University of Wales Press","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:390<\/identifier><datestamp>\n      2011-04-04T09:11:13Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/390\/<\/dc:relation><dc:title>\n        How accurately do instructors judge students\u2019 attitudes online? A measurement of expectations and level of satisfaction with an Online Information Systems masters program<\/dc:title><dc:creator>\n        Macht, Lauren\u2010Nicole<\/dc:creator><dc:creator>\n        Preece, Jenny<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        In order to run a successful educational program, instructors as well as staff members must constantly review and adapt to the expectations, concerns, demographics and satisfaction level of their student consumers. This study was conducted in order to examine these issues in an online educational setting. First, interviews were given to the program instructors in order to determine their opinions about the students\u2019 expectations and satisfaction levels. This information was then used to create a student survey that assessed the students\u2019 expectations and level of satisfaction. These two sets of results were then compared This comparison revealed that the online instructors did have a good grasp of the online students\u2019 expectations, concerns, demographics and satisfaction level. The only areas where the instructors\u2019 concepts of student views were slightly less accurate was student concerns and student feelings about the program administration, where the instructors overestimated the level of concern the students had about successfully returning to the learning environment and underestimated the students\u2019 satisfaction with the program's administration. This leads us to conclude that, even with the added online factor, instructors strongly understand student expectations, satisfaction levels, demographics and concerns.<\/dc:description><dc:publisher>\n        University of Wales Press<\/dc:publisher><dc:date>\n        2002<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/390\/1\/ALT_J_Vol10_No3_2002_How%20accurately%20do%20instructors%20.pdf<\/dc:identifier><dc:identifier>\n          Macht, Lauren\u2010Nicole and Preece, Jenny  (2002) How accurately do instructors judge students\u2019 attitudes online? A measurement of expectations and level of satisfaction with an Online Information Systems masters program.  Association for Learning Technology Journal, 10 (3).  pp. 70-82.  ISSN 0968-7769     <\/dc:identifier><dc:relation>\n        10.1080\/0968776020100306<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/390\/","10.1080\/0968776020100306"],"year":2002,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"How accurately do instructors judge students'\nattitudes online? A measurement of expectations\nand level of satisfaction with an Online\nInformation Systems masters program\nLauren-Nicole Macht and Jenny Preece\nUniversity of Maryland, USA\nemail: machtl@umbc.edu, preece@umbc.edu\nIn order to run a successful educational program, instructors as well as staff members\nmust constantly review and adapt to the expectations, concerns, demographics and\nsatisfaction level of their student consumers. This study was conducted in order to\nexamine these issues in an online educational setting. First, interviews were given to the\nprogram instructors in order to determine their opinions about the students' expectations\nand satisfaction levels. This information was then used to create a student survey that\nassessed the students' expectations and level of satisfaction. These two sets of results\nwere then compared This comparison revealed that the online instructors did have a good\ngrasp of the online students' expectations, concerns, demographics and satisfaction level.\nThe only areas where the instructors' concepts of student views were slightly less\naccurate was student concerns and student feelings about the program administration,\nwhere the instructors overestimated the level of concern the students had about\nsuccessfully returning to the learning environment and underestimated the students'\nsatisfaction with the program's administration. This leads us to conclude that, even with\nthe added online factor, instructors strongly understand student expectations, satisfaction\nlevels, demographics and concerns.\nForeword\nDavid Squires was my colleague for almost twenty-five years. During that time I watched\nDavid's stature as an academic grow. David brought depth and rigour to the evaluation of\neducational software and in recent years he introduced techniques and concepts from\n70\nALT-J Volume 10 Number 3\nhuman-computer interactions which added a new dimension to educational computing.\nCollaborating with David was always a pleasure. David's insight, enthusiasm and wit were\nendearing characteristics which I and his colleagues will remember and treasure.\nJenny Preece\nDefinitions\nFor this study, expectations are defined as a preconceived concept of how an aspect of a\nsituation or environment should appear or be carried out. A positive expectation is a type\nof expectation where the individual would be happier if the terms of the expectation were\nmet instead of not met. Level of satisfaction is defined as the measurable assessment of\nhow well an individual's positive expectations are either not being met, being met or being\nsurpassed.\nIntroduction\nUnderstanding student concerns, demographics, expectations and satisfaction levels is\nessential to running a successful educational program. Within any educational program,\ninstructors as well as staff members must constantly determine, review and adapt to these\nstudent variables (Harasim, Hiltz, Teles and Turroff, 1995; Squires and Preece, 1996). This\ntask, while difficult within itself, is made increasingly difficult when the educational\nenvironment is placed online and educators must concern themselves with the usability of\nthe software as well as with content and pedagogy (Squires and Preece, 1999). The\nimportance of this task, however, is not lessened by the online factor.\nIt is because of the importance of student consumers' expectations, concerns, demo-\ngraphics and level of satisfaction within an educational program that this study was\nconducted. A second reason for doing this study is that there is surprisingly little work\nreported that directly examines whether faculty and staff perceive students' satisfaction of\nonline courses or lack thereof accurately. The main goal of this study is to compare the\ninstructors' views of student expectations, concerns, demographics and level of satisfaction\nwith the same variables for the students within the context of one online educational\nprogram. It was our hope that this type of analysis will be done for other programs in other\neducational settings, since it is obviously desirable that instructors should accurately\ninterpret the concerns and feelings of their students.\nThe study occurs in three parts. The first part involves conducting instructor interviews\nand then analysing the results to identify what the instructors think are the students'\nconcerns. The next part entails creating a survey based partly upon the analysis from part\none and administering this survey to students within the program. The data collected from\nthis section of the study is then analysed. The third and final part of this study involves the\ncomparison of the students' data and the instructor data.\nPart one: instructor interviews\nExperimental method\nA set of open-ended interview questions was created in order to access the instructors'\nperceptions of student expectations and satisfaction level within the Online Information\nSystems Masters program. The questions were as follows:\n71\nLauren-Nicole Mocht and Jenny Freeze How accurately do instructors judge students' attitudes online?\n1. What do you think the students expect from the program as a whole?\n2. What do you think the students expect from your course(s)?\n3. What are your general feelings about the students' level of satisfaction with the\nprogram as a whole?\n4. What are your general feelings about the students' level of satisfaction with any\ncourses that you have been involved in?\n5. Is there any part of the program that you feel that the students are mainly satisfied\nwith?\n6. Is there any part of the program that you feel that the students are mainly dissatisfied\nwith?\nParticipants\nFor this part of the study, seven out of the eleven course instructors were interviewed.\nHowever, two interviews had to be conducted through email due to the fact that these\ninstructors were on vacation. All the instructors have extensive experience teaching masters\nlevel courses but one had not taught online before.\nResearch setting\nEach interview took place in the instructor's office. Every session was recorded onto an\naudiotape and notes were taken. Each session lasted between fifteen minutes and one hour,\nwith most averaging forty minutes.\nDiscussion of results\nAll instructors appear to have similar concepts of student expectations and levels of\nsatisfaction. All perceived the students as being either satisfied or very satisfied with the\nprogram as a whole. Many aspects of student level of satisfaction were noted, such as the\nstudents getting quick and timely responses to email, finding the course Website easy to\nuse, participating in many useful interactions, feeling that the course material and the\ncourse itself are useful, and appreciating the program's flexibility of time schedule and\nlocation. It was also noted that there is a very low drop-out rate for the program, which\nalso supports the notion of the students being satisfied. Also, the instructors who have\ntaught an online course within this program mentioned that they are satisfied with the\nexperience, which often suggests that students are also satisfied.\nEven though all instructors saw the students as being satisfied with the program, if not\nvery satisfied, they did mention some aspects of the program that may lower the overall\nstudent level of satisfaction. These consisted of the students dissatisfaction with many\nadministration procedures that were not completed as smoothly as expected,\ndisappointment when course books and other needed materials did not arrive on time,\nfrustration when their course enrolments were not correct or timely, dissatisfaction when\nprogram accounts were not set up fast enough, and when admissions procedures were\nexecuted at what appeared to be the last minute.\nIn conjunction with the view of students generally being satisfied, the instructors also saw\nthe students as having most, if not all, of their expectations met or surpassed. It was noted\n72\nALT-J Volume 10 Number 3\nduring the interviews that many different expectations are involved within each student's\nperception of this program. Also, while students may not share all expectations, many of\nthe students will approach the program with quite similar expectations. Specifically, most\ninstructors indicated that they believed the students to have the following expectations:\n\u2022 to be able to enhance the skills that they already have as well as to acquire new skills;\n\u2022 to gain a basic introduction and familiarization with IT terminology and tools;\n\u2022 to enhance further the students' placements within the job market or to assist with\ncareer advancement;\n\u2022 to gain enrichment, to be able to get assistance from instructors and other staff\nmembers when assistance is needed;\n\u2022 to pass course hurdles without excessive trouble and without wasting time, to receive as\nmuch support as possible;\n\u2022 to complete the course fully without wasting too much time;\n\u2022 to find that the course work is useful instead of wasteful, that an online course should\nbe no harder to complete than an equivalent face-to-face course;\n\u2022 to have as much student-teacher and student-student interaction as possible;\n\u2022 to receive a degree at graduation that has as much status as the equivalent degree from\na face-to-face program, to be able to do everything from a distance;\n\u2022 to be given all needed materials aiid clear instructions of what is expected from them,\nfor the administration procedures to run smoothly, a learning environment filled with\nother committed and competent students;\n\u2022 to be supplied with thorough lecture notes;\n\u2022 to be given sufficient guidance on all projects;\n\u2022 to receive feedback on assignments and work;\n\u2022 to gain an in-depth and wide understanding of current IT;\n\u2022 to learn about industry-related topics so that they can apply them to real-life scenarios,\nto have the administrative procedure be almost invisible;\n\u2022 to have the teachers stick to the syllabi;\n\u2022 to have all accounts set up and ready for use before courses begin;\n\u2022 to have instructors keep their promises;\n\u2022 for the courses to be set up on a flexible time schedule;\n\u2022 to gain hands on or applied experiences.\nInstructors also had similar concepts of the demographics of the student population. Most\nindicated that the majority of the students had some level of computer skills prior to\nentering the program and have been working in the industry for a while. It was also\nbelieved that almost all of the students are returning students, many of whom are older\n73\nLauren-Nicole Macht and Jenny Preece How accurately do instructors judge students' attitudes online?\nand lived out of state. The instructors also believed that most students did not have higher\neducation degrees, especially within high-tech fields.\nSome students' concerns were also noted during the interview process. Getting back into\nthe learning environment and succeeding in it was noted as a major student concern by\nmost of the instructors. Many also saw the online factor, which is new for most students, as\nbeing a cause for student concern.\nLimitations\nEven though all efforts were made to strengthen the soundness of the research design,\nmethods, and findings, some limitations need to be noted. First, even though a 64 per cent\nresponse rate was achieved for the interview participants, the rate was not 100 per cent\nbecause not all instructors were able to participate in the interview process. The response\nrate was more than ample for this kind of research and a good deal of information was\ncollected. Another limitation lies in the fact that two of the instructors were interviewed\nthrough email. While this could not be helped, since the instructors were on vacation and\nour goal was to collect as much information as possible, the research setting could have\nbeen controlled more if these interviews had been conducted in person. Also, when\ninterviews are conducted through email or other distance mediums, information such as\nbody language and reactions is lost. Having such information for these two instructors\ncould have been useful.\nWhile a sufficient number of interview questions were presented to each instructor, the\nquestions were quite vague in nature. The questions were designed in this way because very\nlittle prior research has been done in this area and there was not enough information that\ncould have been used to make the questions more specific. However, if time had permitted,\nit might have been beneficial to interview each instructor a second time with a more\nspecific set of questions, based on the answers from the first set of instructor interviews.\nThis way each instructor could have supplied us with information on each possible\nexpectation and notion of satisfaction.\nPart two: student surveys\nSurvey creation\nThe findings from the first part of this study were used to create a survey that was\nadministered to the online students. The survey contained two parts. The first section of\nthe survey contained a measure of student demographics, a general measure of student\nexpectations as well as whether they are being met, and a single overall measure of level of\nsatisfaction.\nThe questions that measured student demographics were based on the instructors'\nperceptions of student demographics, provided within the interview process. Other\ncommon demographic measures, such as age and gender, were added to the question list as\nwell. These questions were presented in Likert, open-answer and check-box form. The\npurpose of these questions was not only to gain an understanding of the demographics for\nthe students, but also to be able to evaluate how well the instructors' views of student\ndemographics matched actual student demographics.\nThe first part of the survey also dealt with general measures of program expectations and\n74\nALT-J Volume 10 Number 3\nwhether they were being met. Within this part of the survey, the survey user was first asked\nthrough an open-answer question to list any of their expectations that pertained to a\nspecific aspect of the program (such as course software, admissions, instructors, material,\netc.). Then, the user was asked to indicate, through the use of a check box, whether this\nexpectation had been met. Lastly, in open-answer form, the user was asked to discuss why\nthe above answer was given. The purpose of these questions was to gather a list of\nexpectations as determined by the students. This list could then be compared with the list\nof student expectations provided by the instructors. These questions also enabled us to\ngather an unbiased list of student expectations that was not influenced by any of the\nexpectations that the instructors had already listed.\nThe end of the first part of the survey contained a general level of satisfaction measure\nwhere the students could indicate if they were very dissatisfied, dissatisfied, neither\nsatisfied nor dissatisfied, satisfied, or very satisfied with the program. The students were\nalso asked to provide information on why they chose the above answer. The purpose of this\nquestion was to gain a general and almost baseline measure of level of satisfaction from\neach participant before they were asked to think about more specific aspects of satisfaction\nand expectations. The second part of the survey consisted of questions that accessed either\nan aspect of satisfaction or an expectation that had been mentioned by one or more of the\ninstructors during the interview process.\nAll of the questions within this section of the survey were in 'I statement' form. Each\nquestion also had answer choices ranging from 'very dissatisfied' to 'very satisfied'. These\nfive answer choices are presented to the survey user within a Likert scale format.\nIt is important to note that, once survey users had moved on to the second part of the\nsurvey, they were not allowed to change their answers to the questions from the first part of\nthe survey. The survey was purposely designed this way in order to make sure that the\nintegrity of the expectation and overall level of satisfaction questions within the first part\nof the survey was not jeopardized. In other words, we did not want survey users to base\ntheir answers to the questions within the first part of the survey on the expectations and\naspects of satisfaction that are mentioned within the second part of the survey. In order to\nmake sure that this did not occur, it was necessary to make sure that the answers to the\nquestions within the first part of the survey could not be accessed by the survey users after\nthey had viewed the questions within the second part of the survey.\nParticipants\nAll sixty-nine students on the Online Information Systems Masters Program were\ncontacted through email and asked to complete the online survey. Twenty-eight students\nfilled out the survey. However, five of those surveys had to be discarded because the\nanswers were either incomplete or were received too late after the deadline. The total\nnumber of usable surveys was twenty-three, giving a response rate of 33 per cent.\nResearch setting\nThe survey was available online, and was completed by the students in their own time and\ntherefore it was impossible to control the environmental conditions while the students were\ncompleting the survey. The online survey consisted of two separate surveys connected by a\nlink. Survey users could complete each survey part at leisure, although most completed\nboth parts within a one-hour period.\n75\nLauren-Nicole Macht and Jenny Preece How accurately do instructors judge students' attitudes online?\nDiscussion of results\nMuch student demographic data was collected throughout the survey process. It was found\nthat the majority of the online students are males (68 per cent) between the ages of 41 and\n50 (55 per cent) or 31 and 40 (31 per cent). The majority of the online students also live\nwithin the United States (95 per cent), mainly outside Maryland.\nMost online students within this program have a strong background in computer use as\nwell as computer structure and operations (77 per cent). The undergraduate areas of study\nfor these online students are very diverse, ranging from Chemistry and Engineering to\nPsychology and Education, with the majority of the areas of study not falling within a\ncomputer-related field (90 per cent). Most online students reported their highest level of\ndegree earned to be a Bachelor of Science degree (60 per cent), with the next most popular\nlevel of degree being a Bachelor of Arts (23 per cent). In conjunction with that, most\nonline students indicated that they did not hold any computing degrees or certifications\nprior to enrolling in this program (64 per cent). For those who did have computer degrees\nor certifications, their types were diverse, ranging from Information Systems Management\nand an MCSE certification to a BS in Computer Programming and CNE (Novell)\ncertifications. The year that each online student had last taken a college level course varied,\nwith the majority having been out of the learning environment for at least four years (58\nper cent).\nOnline students within this program also varied in their current occupations. Many had\nonly spent between two and six years working within their current job (50 per cent).\nHowever, most had spent either between eleven and twenty (38 per cent) or twenty-one and\nthirty (28 per cent) years working within the job market.\nOnline students within this program seemed to be divided on issues pertaining to program\nconcerns prior to beginning classes. A small majority of students indicated that they were\nconcerned with getting back into a college learning environment successfully (45 per cent),\nalthough many did not think about it (18 per cent). A slight majority also noted that they\nwere at some point concerned with the fact that the program is offered strictly online (55\nper cent), and no students indicated that they did not think about this possible concern.\nThrough this survey process, online students also indicated what expectations they had for\nthe program and its parts. In order to do so, online students were first asked to list their\nexpectations. Students noted such expectations as that all materials should be supplied on\ntime, communication with the instructors should be often and thorough, the course work\nload should be time- and location-flexible, professors should be knowledgeable, adminis-\ntrative staff should be friendly, organized and helpful, and that the degree received at the\nend of this program should be valuable. The large majority of online students also\nindicated that they felt as if their expectations were being met.\nThe online students were then asked to rate their overall level of satisfaction with the\nprogram as a whole. Using the Likert scale mentioned above, the average level of\nsatisfaction rating was a 4.04, which means that the online students were satisfied or very\nsatisfied with this program.\nNext, online students were asked to rate each expectation within the provided expectation\nlist that was collected from the instructor interviews. A reliability analysis was run and\n76\nALT-J Volume 10 Number 3\nalpha was shown to be 0.90. According to this statistic, each question or expectation\nappears to measure the concept of general online student expectations without being\nredundant. A frequency analysis run with this data shows that the average score for each of\nthese expectations was a 4.03 or 'agree'. In other words, the online students agreed that the\nstudent expectation list that the instructors created reflected their own expectations. Within\nthis expectation list, the expectation score ranged from 3.03 or 'neither agree nor disagree'\nto 4.73 or 'strongly agree'. The expectation receiving the lowest rating of 'neither agree of\ndisagree' was that 'Courses should not take up too much of my time'. Some other\nexpectations that received the same rating were 'I expect that I will interact with my peers\non a regular basis throughout the course of this program', 'I assume that through this\nprogram I will be able to gain hands on experience', 'I expect that this program will help\nme to further enhance my placement within the job market', and 'I anticipate that it will be\neasy to pass the hurdles within this program'. The expectation with the highest rating of\n'strongly agree' was 'Instructors do have to provide relevant feedback'. Other expectations\nthat also received some of the highest ratings were 'I assume that I will find the work\nassigned to me within my courses to be useful', 'Instructors and other program staff\nmembers should provide me with help when it is needed', 'Students should receive\nguidance from their instructors on projects', and 'I assume that my program credits and\ngrades will be properly recorded'.\nFinally, the online students noted their ratings of the aspects of satisfaction. A reliability\nanalysis was run and alpha was shown to be 0.92. According to this statistic, each question\nor aspect of satisfaction appears to measure an aspect of general online student level of\nsatisfaction without being redundant. A frequency analysis run with this data shows that\nthe average score for each of these expectations was a 3.83 or 'agree'. In other words,\nstudents seemed to agree that they were satisfied with the aspects of satisfaction as a whole.\nThe scores on each aspect of satisfaction ranged from 3.17 or 'neither agree or disagree' to\n4.26 or 'agree'. The aspect of satisfaction that received the lowest rating of 'neither agree or\ndisagree' reads 'I believe that this program does not demand too much from its students'.\nOther aspects of satisfaction that received the same rating are 'The program courses\nprovide me with just enough information on each topic', 'Instructors and other program\nstaff members are providing me with the proper assistance that I need', and 'I expect that I\nwill not interact with my peers on a regular basis throughout the course of this program'.\nThe aspect of satisfaction that received the highest rating or 'agree' reads: 'I am pleased\nwith the program administration.' Other aspects of satisfaction that also received this\nrating are 'I find the courses within this program to be interesting', \"The materials within\neach course seem appropriate', and 'This program is helping me to learn new skills'.\nLimitations\nEven though one of the main goals of this research process was to strengthen the accuracy\nof this study's design, methods and findings, some limitations still need to be noted. First,\nalthough a response rate of 33 per cent for this type of research is quite acceptable, it\nwould have made this study's findings stronger if the response rate had been higher.\nAnother limitation stems from the fact that the subjects were unsupervised and in a\nlocation of their choice when filling out the survey. Because of this fact, subjects may have\nbeen distracted at times or may have rushed over parts of the survey without caution in\norder to finish quickly. Both of these situations could have lessened the strength of our\n77\nLauren-Nicole Macht and Jenny Preece How accurately do instructors judge students' attitudes online?\nfindings by leaving the researchers with missing and\/or incomplete data or with data filled\nwith mistakes. Even though it is impossible to tell whether or not any of this undesirable\ndata can be found within this study (it is assumed that if present such incorrect data will be\noutweighed by correct data during the analysis), it would have improved this study if the\npossibility of such undesirable data could have been eliminated.\nPart three: comparison\nComparison of results\nIt is apparent that the instructors within the Online Masters Program have a strong and\ntruthful concept of student expectations and satisfaction level, as well as student concerns\nand demographics. This was determined by directly comparing the student responses to the\nsurvey and the instructor interviews. Even though the information collected from the\nonline survey technically applied to the student sample and not the student population as a\nwhole, one can assume that they generally reflect the demographics within the whole\nstudent population. It is based on this that we can make a comparison.\nThe first thing that we compared was the instructors' concepts of student demographics\nand actual student demographics as collected from the survey process. The instructor views\nof student demographics reflected the collected students' demographic information in\nmany respects, such as noting that the majority of students within the program are\nstudents returning to the learning environment from an educational break, are older\nstudents falling within the 31 to 50 years old range, having some level of computer skills\nbefore beginning this program, living outside the state of Maryland, working within the\nindustry for a number of years, and not having higher educational degrees, especially\nwithin a computer-related field. From this comparison it is apparent that the instructors\nhave an excellent grasp of online student demographics.\nStudent concerns were then compared with the instructors' concepts of student concerns.\nInstructors noted that they believed the student population to be concerned with\nsuccessfully getting back into the learning environment and overcoming the differences and\ndifficulties that the online factor can add to learning. Through the survey process, it was\nlearned that online students seem to be very divided on these issues. Most online students\ndid note that they were nervous about how the online factor will affect their learning\nexperience, but a fair number of online students also noted that they were not concerned\nabout that at any point. The majority of the online students indicated that they were not\nconcerned with getting back into the learning environment, with a few students even\nnoting that they did not even consider this concern at all. It can be concluded from this\nsection of the analysis that the instructors did not have a perfect concept of student\nconcerns.\nNext, we compared instructor concepts of student expectations with the student-given\nexpectations. This part of the procedure actually involved two steps: comparing the list of\nstudents' expectations collected from the instructors with those collected by the students in\norder to make sure that the list of expectations used within the second part of the survey\nwas complete; and analysing the answers to the expectations within the second part of the\nsurvey in order to make sure that the listed expectations truly reflected those of the student\npopulation.\n78\nALT-J Volume 10 Number 3\nThe expectations that the students listed on their own turned out to be quite similar to\nthose student expectations that the instructors had previously listed. The few student-given\nexpectations that were unlike those provided by the instructors were irrational, such as\nwanting to receive the degree within a year and wanting to do very little to no work for the\ndegree, and therefore discarded. The students also appeared to agree that the expectations\nthat the instructors came up with reflected their own. This can be seen by the average score\nof 4.03 or 'agree' that the online students assigned to the instructor expectations list. As\nmentioned above, there are some expectations that receive a higher score of 'strongly agree'\nsuch as, 'Instructors do have to provide relevant feedback', 'I assume that I will find the\nwork assigned to me within my courses to be useful', 'Instructors and other program staff\nmembers should provide me with help when it is needed', 'Students should receive\nguidance from their instructors on projects', and 'I assume that my program credits and\ngrades will be properly recorded' while some expectations received a lower score of 'neither\nagree or disagree', like, 'Courses should not take up too much of my time', 'I expect that I\nwill interact with my peers on a regular basis throughout the course of this program', 'I\nassume that through this program I will be able to gain hands on experience', 'I expect that\nthis program will help me to further enhance my placement within the job market', and 'I\nanticipate that it will be easy to pass the hurdles within this program'. It is also important\nto note that no instructor-given positive expectation received an overall score below a three\n('neither agree or disagree'). All of this together suggests that the instructors understand\nstudent expectations well.\nOur next step was to compare aspects of student satisfaction. This was accomplished by\nanalysing the scores that the online students assigned to the aspects of satisfaction that\nwere supplied by the instructors. The average score for the aspects of satisfaction was a\n3.83 or 'agree'. As mentioned above, some aspects of satisfaction, such as, 'I believe that\nthis program does not demand too much from its students', 'The program courses provide\nme with just enough information on each topic', 'Instructors expect that I will not interact\nwith my peers on a regular basis throughout the course of this program', received a lower\nrating of 3 or 'neither agree nor disagree'. Other aspects of satisfaction received a higher\nrating of 4 or 'agree', such as, 'I am pleased with the program administration', 'I find the\ncourses within this program to be interesting', 'The materials within each course seem\nappropriate', and 'This program is helping me to learn new skills'. It is also important to\nnote that no aspect of satisfaction received an overall score less that 'neither agree or\ndisagree' or higher than 'agree'. In other words, students seemed mainly to agree that they\nwere satisfied with the aspects of satisfaction that the instructors noted, suggesting that the\ninstructors do have a fairly good grasp of student concepts of level of satisfaction.\nThere were a few aspects of satisfaction that the instructors believed that the students were\nvery unhappy with, such as the administration and their procedure, the arrival of books\nand the timely creation of accounts. While the students agreed that they were disappointed\nwith the arrival of books and the timely creation of accounts, they noted that they were\nvery pleased with the program's administration. The discrepancy between the students'\nsatisfaction with the program administration and the instructors' concept of that satis-\nfaction leads us to believe that while the instructors have a very good grasp of satisfaction\nwithin parts of the program that relate to their teaching experiences, perhaps they do not\nhave the same strong concept of satisfaction when it comes to aspects of the program that\nthey are often not involved with.\n79\nLauren-Nicole Macht and Jenny Preece How accurately do instructors judge students' attitudes online?\nInstructors' view of the\nstudent response\nStudent response\nDemographics\nConcerns\nExpectations\nAspects of Satisfaction\nOverall Satisfaction\nMost instructors indicated that\nthey believed that the majority of\nstudents are returning from a\nbreak in education, between 31\nand 50 years of age, had some\nlevel of computer skills before\nthis program, live outside\nMaryland, have been working in\nthe industry for a number of\nyears, and do not have degrees of\nhigher education.\nThe majority of the instructors\nnoted that they believe that most\nstudents were concerned about\nsuccessfully returning to the\nlearning environment and about\nsucceeding within an online\ncourse.\nInstructors believed that the\nmajority of students shared many\nexpectations, such as that\ninstructors have to provide\nrelevant feedback, the course\nwork will be useful, instructors\nand other program staff\nmembers will provide assistance,\nand that program credits and\ngrades will be properly recorded.\nInstructors indicated that they felt\nthat the majority of students\nshared the same aspects of\nsatisfaction, such as being pleased\nwith finding the courses\ninteresting, seeing the course\nmaterials to be appropriate, and\nlearning new skills. Instructors\nthought that the students were\nless satisfied with the program\nadministration.\nInstructors noted that they\nbelieved that the students were\neither satisfied or very satisfied\nwith the program as a whole.\nThe average student response\nmatched those suggested by the\ninstructors.\nStudent responses reflected that\nthe majority of students were\nconcerned about the online\nfactor, but were not worried\nabout getting back into the\nlearning environment\nThe majority of students strongly\nagreed with the expectations that\nthe instructors suggested.\nMost of the students agreed with\nthe instructor-determined aspects\nof satisfaction but they were\nmore satisfied with the program's\nadministration than the\ninstructors realized.\nStudents indicated that the\nmajority of them were satisfied\nwith the program as a whole.\nTable I: Summary of results\n80\nALT-J Volume 10 Number 3\nLastly, we compared overall level of satisfaction. During the interview process, all\ninstructors indicated that they perceived the online students as being very satisfied with the\nprogram as a whole. However, while most students did indicate that they were very\nsatisfied, the overall score for level of satisfaction was only a 4.04 or a score of 'satisfied'.\nThis difference indicates that, while instructors had a good concept of student level of\nsatisfaction, their concept was not flawless. However, the overall level of satisfaction score\nand the scores for aspects of satisfaction did correlate significantly, suggesting that the\ninstructors were not too far off. It is a strong possibility that these findings might merely\nreflect the fact that the list of aspects of satisfaction given by the instructors is just not\ncomplete. Table 1 summarizes these reports.\nConclusions and future work\nFrom this study it can be seen that overall the instructors working within the Online\nMasters Program possess a very good understanding of online student expectations, level\nof satisfaction, and demographics. They also possess a good but not flawless under-\nstanding of student concerns and aspects of satisfaction dealing with parts of the program\nwhere the instructors are not often involved. Instructors and students both appear to be\nsatisfied with the program itself. Students also feel that the program is meeting a good\nmajority of their expectations.\nThe online factor has not negatively affected the ability of instructors and other staff\nmembers to determine, review and adapt to the expectations, concerns, demographics and\nsatisfaction level of their students. This is important to determine, since the online factor\neliminated face-to-face communication, which was the most common way that instructors\nand other staff members were informed of student expectations, concerns,- demographics\nand satisfaction levels.\nFor future work in this area, it would be beneficial to all education institutions, especially\nthose with unique factors such as the online element, constantly to assess the level of\nsatisfaction, concerns, demographics and expectations of their consumer students. It is\nonly after knowing this that any program can expect to be the best that it can be. Such self-\nreflection is particularly important for online programs (Hiltz, 1994) in which it is only too\neasy to fall into the trap of 'just putting material on the Web' with little thought about\npedagogic structure. Approaches like this can all too easily lead to digital diploma mills\n(Noble, 1998), particularly when profit forms part of the underlying motive as it has done\nfor many online programs.\nFor this and many institutions it will be beneficial to repeat this type of study in the near\nfuture. This is due to the fact that levels of satisfaction, concerns, expectations and\ndemographics are not constant elements within an institution. In order for this program to\nbe able to continue satisfying its consumers it needs to be able to continue to know what\nthey want from the program and if they feel that their needs are being met. This is true for\nany kind of program but it is particularly so for Masters programs in Information\nTechnology where the curriculum changes rapidly, driven partly by technical developments\nand partly by market forces.\n81\nLauren-Nicole Macht and Jenny Preece How accurately do instructors judge students' attitudes online?\nAcknowledgements\nWe would like to thank all of the UMBC students, staff and faculty who participated in\nour research, Darniet Jennings for assisting us in creating the survey tool, and Noah\nGotteman for assisting with the editing.\nReferences\nHarasim, L., Hiltz, S. R., Teles, L. and Turroff, M. (1995), Learning Networks: A Field\nGuide to Teaching and Learning Online, Cambridge, MA: MIT Press.\nHiltz, S. R. (1994), The Virtual Classroom: Learning without Limits via Computer\nNetworks, Human-Computer Interaction Series, Norwood, NJ: Ablex Publishing Group.\nNoble, D. (1998), 'Digital diploma mills: the automation of higher education', Educom\nReview, 33 (3).\nSquires, D. and Preece, J. (1996), 'Usability and learning: evaluating the potential of\neducational software', Computers and Education, 27 (1), 15-22.\nSquires, D. and Preece, J. (1999), 'Predicting quality in educational software: evaluating for\nlearning, usability, and the synergy between them', Interacting with Computers: The\nInterdisciplinary Journal of Human-Computer Interaction, 11 (5), 467-83.\n82\n"}