{"doi":"10.1107\/s0021889809006840","coreId":"84140","oai":"oai:centaur.reading.ac.uk:1822","identifiers":["oai:centaur.reading.ac.uk:1822","10.1107\/s0021889809006840"],"title":"GDASH: a grid-enabled program for structure solution from powder diffraction data","authors":["Griffin, T. A. N.","Shankland, Kenneth","van de Streek, J. V.","Cole, J."],"enrichments":{"references":[{"id":195286,"title":"http:\/\/www.univaud.com\/hpc\/products\/grid-mp\/. computer programs","authors":[],"date":"2008","doi":null,"raw":null,"cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2009-04","abstract":"The simulated annealing approach to structure solution from powder diffraction data, as implemented in the DASH program, is easily amenable to parallelization at the individual run level. Very large scale increases in speed of execution can therefore be achieved by distributing individual DASH runs over a network of computers. The GDASH program achieves this by packaging DASH in a form that enables it to run under the Univa UD Grid MP system, which harnesses networks of existing computing resources to perform calculations","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/84140.pdf","fullTextIdentifier":"http:\/\/centaur.reading.ac.uk\/1822\/1\/gdash.pdf","pdfHashValue":"086f651e275860f989a38f6b649a89e028091b7c","publisher":"Wiley-Blackwell Publishing, Inc","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:centaur.reading.ac.uk:1822<\/identifier><datestamp>\n      2017-09-25T01:09:44Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><setSpec>\n      6469766973696F6E733D315F3965653563346331:335F3134316638303032:355F3936363261656231:375F6337653038646139<\/setSpec><\/header><metadata><rioxx xmlns=\"http:\/\/www.rioxx.net\/schema\/v2.0\/rioxx\/\" xmlns:ali=\"http:\/\/ali.niso.org\/2014\/ali\/1.0\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:dcterms=\"http:\/\/purl.org\/dc\/terms\/\" xmlns:rioxxterms=\"http:\/\/docs.rioxx.net\/schema\/v2.0\/rioxxterms\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.rioxx.net\/schema\/v2.0\/rioxx\/ http:\/\/www.rioxx.net\/schema\/v2.0\/rioxx\/rioxx.xsd\" ><ali:free_to_read>\n    \n      <\/ali:free_to_read><dc:description>The simulated annealing approach to structure solution from powder diffraction data, as implemented in the DASH program, is easily amenable to parallelization at the individual run level. Very large scale increases in speed of execution can therefore be achieved by distributing individual DASH runs over a network of computers. The GDASH program achieves this by packaging DASH in a form that enables it to run under the Univa UD Grid MP system, which harnesses networks of existing computing resources to perform calculations.<\/dc:description><dc:format>application\/pdf<\/dc:format><dc:identifier>http:\/\/centaur.reading.ac.uk\/1822\/1\/gdash.pdf<\/dc:identifier><dc:language>en<\/dc:language><dc:publisher>Wiley-Blackwell Publishing, Inc<\/dc:publisher><dc:source>0021-8898<\/dc:source><dc:subject>548<\/dc:subject><dc:title>GDASH: a grid-enabled program for structure solution from powder diffraction data<\/dc:title><rioxxterms:author>Griffin, T. A. N.<\/rioxxterms:author><rioxxterms:author>Shankland, Kenneth<\/rioxxterms:author><rioxxterms:author>van de Streek, J. V.<\/rioxxterms:author><rioxxterms:author>Cole, J.<\/rioxxterms:author><rioxxterms:publication_date>2009-04<\/rioxxterms:publication_date><rioxxterms:type>Journal Article\/Review<\/rioxxterms:type><rioxxterms:version>VoR<\/rioxxterms:version><rioxxterms:version_of_record>DOI:10.1107\/S0021889809006840 <\/rioxxterms:version_of_record><\/rioxx><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2009,"topics":["548"],"subject":["548"],"fullText":"computer programs\n356 doi:10.1107\/S0021889809006840 J. Appl. Cryst. (2009). 42, 356\u2013359\nJournal of\nApplied\nCrystallography\nISSN 0021-8898\nReceived 2 October 2008\nAccepted 24 February 2009\n# 2009 International Union of Crystallography\nPrinted in Singapore \u2013 all rights reserved\nGDASH: a grid-enabled program for structure\nsolution from powder diffraction data\nThomas A. N. Griffin,a Kenneth Shankland,a* Jacco van de Streekb\u20212and Jason\nColeb\naSTFC Rutherford Appleton Laboratory, Harwell Science and Innovation Campus, Didcot OX11 0QX, UK, and\nbCambridge Crystallographic Data Centre, 12 Union Road, Cambridge CB2 1EZ, UK. Correspondence e-mail:\nkenneth.shankland@stfc.ac.uk\nThe simulated annealing approach to structure solution from powder diffraction\ndata, as implemented in the DASH program, is easily amenable to\nparallelization at the individual run level. Very large scale increases in speed\nof execution can therefore be achieved by distributing individual DASH runs\nover a network of computers. The GDASH program achieves this by packaging\nDASH in a form that enables it to run under the Univa UD Grid MP system,\nwhich harnesses networks of existing computing resources to perform\ncalculations.\n1. Introduction\nDASH (David et al., 2006) is a computer program for structure\nsolution from powder diffraction data (SDPD) that, since its first\nrelease in 1999, has placed computational efficiency as well as\neffectiveness at the heart of its design. It employs the now widely\nadopted global optimization approach to SDPD and, in particular,\nuses simulated annealing (SA) as its primary optimization method.\nBy its nature, no individual SA run is guaranteed to find a global\nminimum equating to the solved crystal structure in a finite time\nframe and so multiple SA runs are required when tackling an SDPD\nproblem in order to maximize the chances of locating this global\nminimum. For relatively small problems of the order of 15\u201320 degrees\nof freedom, only a small number of runs (\u000150) is typically required in\norder to locate the global minimum several times. For more complex\nproblems, of the order of 30 or more degrees of freedom, the success\nrate (defined as the number of SA runs reaching the global minimum\ndivided by the total number of SA runs and expressed as a percen-\ntage) can fall to only a few percent, and successful structure solution\nmay necessitate several hundred SA runs to be performed. Faced\nwith such a processing load, one can consider invoking fine-grained\nparallelization of the time-consuming calculations in order to speed\nup the evaluation of a single SA run, or coarse-grained paralleliza-\ntion, taking advantage of the fact that each SA run is independent of\nevery other and that there is no requirement for these runs to be\nperformed sequentially, i.e. this is an \u2018embarrassingly parallel\u2019\nproblem. The former approach, utilized mostly on symmetric multi-\nprocessing\/shared memory based computers, has been exploited in\ncrystallography for a variety of demanding calculations (Diederichs,\n2000). The latter approach has also been used to good effect in other\ncrystallographic multisolution-type procedures, such as that\nemployed by Shake and Bake (Miller et al., 2007). Both approaches\nhave their advantages, but as a general rule, if a problem falls into the\ncategory of embarrassingly parallel then the latter approach is\nfavoured, as it generally does not require modification of the core\ncode and can be scaled to a vast number of processors that can be\nresident in computers that are geographically quite distinct.\nWithDASH, we have pursued the latter approach, using the Univa\nUD Grid MP system (Univa UD, 2008) which can harness the spare\nCPU cycles of existing networked computing resources (such as a\ndepartmental PC network) in a well defined manner. It thus has the\npotential to achieve very large scale parallelization without the need\nto invest in new hardware. Furthermore, Grid MP is widely used and\nhas been well tested on other embarrassingly parallel tasks (such as\nprotein\u2013ligand docking), and it has all the features required to enable\nDASH to be easily distributed across a network. DASH (Version 3.1\nonwards) has thus been modified to allow its SA engine to be driven\nby command file, in order to allow it to be run under Grid MP.\n2. GDASH overview\nGDASH is a command-line driven program that takes, as input, files\ngenerated using DASH and submits them to a Grid MP server (a\ncomputer that runs the core Grid MP software) for subsequent\nexecution by DASH on client PCs (computers that run the Grid MP\nMPAgent software) that are in contact with the Grid MP server\n(Fig. 1). It also has the ability to retrieve the results of these DASH\ncalculations from the Grid MP server at any time and amalgamate\nthem into a single results file that is readable by DASH (Fig. 2).\n3. Program description\n3.1. Creation of input files for GDASH\nIn setting up a normal SDPD attempt using DASH, the user\nfollows a well defined set of steps, generating a series of required files\n(e.g. .sdi, .hcv) along the way, culminating in the execution of a\nnumber of sequential SA runs. For a full description of these steps\nand of the function of the various files generated, the reader is\nreferred to David et al. (2006). As of Version 3.1 of DASH, the user\ncan specify the generation of an additional batch file for subsequent\nexecution on a grid-type system (or a multiple-core CPU), as opposed\nto pressing the \u2018Solve\u2019 button to start a standard sequence of runs\n(Fig. 3, top). The batch (.grd) file generated is simply a text listing of\n\u2021 Present address: Avant-garde Materials Simulation, D-79100 Freiburg,\nGermany.\nthe names of the DASH control (.dbf) files that are generated. The\nnumber of .dbf files generated depends upon the total number of SA\nruns requested and the number of SA runs per package (where each\npackage represents a discrete number of SA runs upon which a single\ninstance of DASH will operate), with the number of files equal to the\ntotal number of SA runs divided by the number of runs per package\n(Fig. 3, bottom). Typically, each package will contain only a single SA\nrun, and thus a request for 500 SA runs will generate 500 .dbf files.\nDASH gives the user the option of saving the .grd and .dbf files into a\nnew directory, in order to provide a tidy directory structure and allow\nmultiple sets of grid files to be created if desired. Once the .grd and\n.dbf files have been generated, the user may exit DASH.\n3.1.1. The .dbf file. The .dbf (DASH batch file) file is an ASCII file\ncontaining the necessary information that can be read in by DASH in\norder to set up an SA run. Thus, it contains the locations of the .sdi\n(DASH project file, which lists files needed for the correct setup of\nthe SA) and .zmatrix (molecular structure description) files, and a\nlocation for the output .dash file into which results are written. The\nfile is fully commented with easy-to-interpret control parameter\nnames, e.g. MAXMOVES is the maximum number of SA moves\nallowed in a single SA run. The .dbf file can be edited by hand if\nneeded, but in practice this should rarely be necessary.\n3.1.2. DASH command line execution. If the DASH executable is\ninvoked from the command line with an argument consisting of a .dbf\nfile (e.g. c:\\dash.exe mytestfile.dbf), DASH does not display\nthe usual graphical user interface (GUI) but instead begins asyn-\nchronous execution of the SA run(s) specified in the .dbf file. Upon\ncompletion, the program output is stored in the .dash file specified in\nthe .dbf file. Note that this \u2018suppressed-GUI\u2019 mode of operation is\nessential to enable DASH to operate in a distributed environment\nwhere it will be executing in the background at low priority on PCs\nthat are already in use by other people. Note too that running DASH\nin this mode also brings a small performance gain (\u000110% reduction\nin execution time) relative to the standard mode of operation, as the\nprogram no longer has the overhead of updating the GUI after every\nSA move.\n3.1.3. Submitting jobs using GDASH.GDASH is invoked from the\ncommand line, so the first task is to open a command prompt window\nin the directory that contains the .grd and .dbf files created using\nDASH. Typing gdash at the command prompt returns a brief\nsummary of how to run the program. To submit a job created\npreviously, the user types gdash filename, where filename is the\nname of the .grd file. The program displays the progress of the job\nsubmission in terms of the percentage of data packages transferred to\nthe grid servers. Once all the necessary files have been uploaded to\nthe server, the job is started and a job summary (including a job_id\nnumber) is returned (Fig. 4). Note that at this point, execution of the\njob is now entirely under the control of the grid servers and the user\ncan close the command window on the machine used for job\nsubmission.\n3.1.4. Monitoring and retrieving jobs using GDASH. In order to\nmonitor the progress of a job, the user invokes GDASH from the\ncommand line, with the job_id number as an argument. If the job is\nnot yet complete, it returns the \u2018% completeness\u2019 and offers the\ncomputer programs\nJ. Appl. Cryst. (2009). 42, 356\u2013359 Thomas A. N. Griffin et al. \u0002 Grid-enabled powder structure solution 357\nFigure 3\nThe DASH interface allows for the creation of batch files ready for execution on\nthe grid (top), and also allows the user to specify the total number of SA runs to be\nperformed and how they are to be packaged for execution.\nFigure 2\nThe relationship of DASH and GDASH. DASH produces the batch files that are\nsubmitted to the grid system usingGDASH.GDASH is also used to retrieve results\nfrom the grid system in a format that can be displayed in DASH.\nFigure 1\nAn overview of GDASH operation\noption to download those results already completed (Fig. 5). If it is\ncomplete, results files are downloaded from the server to the user\u2019s\nPC and then merged into a single .dash file automatically. Note that\nthe .dash file is assigned a name that incorporates the date and time of\nsubmission of the job. The .dash file can then be opened using DASH\nin order to examine the results of the job.\n3.1.5. The DASH.mpconfig application configuration file.\nGDASH has the ability to control many of the grid parameters that\nare relevant to DASH jobs and these can all be set in the\nDASH.mpconfig file. A sample configuration file is shown in Table 1.\nThe only parameters that may need to be adjusted with any frequency\nare those pertaining to the timeouts that apply to DASH program\nexecution. By way of example, the default timeout of 36 000 s sets\nlimits of 10 h on both the CPU time and the elapsed wall clock time,\nafter which a job is deemed to have failed.\n4. DASH program performance when invoked using GDASH\nThe performance of DASH running on the Grid MP installation at\nthe ISIS Facility of the STFC Rutherford Appleton Laboratory has\nbeen evaluated using the moderately challenging optimization\nproblem of solving the crystal structure of famotidine form B\n(Shankland et al., 2002; P21\/c, V = 1421 A\u02da\n3, Z0 = 1, 13 degrees of\nfreedom, 1.64 A\u02da resolution) from synchrotron X-ray powder\ndiffraction data. In order to generate easily measurable execution\ntimes, the total number of SA moves per run was set to 1\u0003 107, about\na factor of ten higher than is necessary actually to solve famotidine.\nPerformance tests were carried out on a single PC, a test grid of five\nPCs and the full production Grid MP system, and test results are\nsummarized in Table 2. Fig. 6 shows the progress of the large job\nsubmitted to the production grid. During the initial quiet phase, data\nare transferred from the PC running GDASH to the grid servers,\nwhere 999 workunits are assembled. About 4 min after job invocation\nfrom GDASH, \u0001300 workunits are sent out for execution from the\ngrid servers to grid client machines. Approximately 5 min later the\nfirst results are returned, and from then on a steady stream of results\nis received, with more workunits being sent out to occupy now idle\nCPUs. The entire job, consisting of 999 SA runs, is complete in just\nunder 40 min.\n5. Software and hardware environment\nGDASH itself runs under MS Windows XP (SP2) and MS Windows\nVista. The GDASH installer requires the MS .Net 2.0 Framework (or\nhigher) to be installed; this is present by default in XP SP2 and Vista.\nGDASH only needs to be installed on computers from which grid\nDASH jobs are going to be submitted. The GDASH installer will\ncomputer programs\n358 Thomas A. N. Griffin et al. \u0002 Grid-enabled powder structure solution J. Appl. Cryst. (2009). 42, 356\u2013359\nFigure 4\nGDASH provides feedback on the progress of a job submission and returns the\njob_id that is necessary for subsequent job monitoring.\nFigure 5\nGDASH allows the results of partially completed jobs to be retrieved.\nTable 1\nThe DASH.mpconfig file.\nParameter Value\nGrid_Username grid_power_user\nGrid_Password power_user_password\nResults_per_WU 1\nMax_Concurrent 1\nMax_Error 3\nPriority 10\nWU_Clock_Timeout 36000\nWU_CPU_Timeout 36000\nAppName DASH\nProgName DASH 3.1\nMGSI_FILESVR_URL https:\/\/gridserver.mydomain.com:28443\/mgsi\/filesvr.fcgi\nMGSI_SOAP_URL https:\/\/gridserver.mydomain.com:18443\/mgsi\/rpc_soap.fcgi\nTable 2\nDASH performance on a test SA job under different conditions.\nSingle PC Test grid Production grid\nDASH 3.1 mode GUI Batch Batch\nNumber of SA runs 64 64 999\nNumber of PCs used 1 (using 1\ncore only)\n5 163\nCore2Quad CPU speed 2.4 GHz Four 2.4 GHz\nand one 2.8 GHz\nMixed\u2020\nTotal elapsed time 9 h 24 min 40 min\nNet minutes per single\nSA run\n8.43 0.38 0.04\nRelative speed rating 1 22 211\n\u2020 The production grid consisted of a mixture of Athlon, Duron, P4, Xeon, Core2Duo and\nCore2Quad CPUs with speeds ranging from 1.2 to 3.6 GHz.\nFigure 6\nThe progress of the 999 workunit test job on the production grid, plotted as a\nfunction of time.\nperform this installation but will also perform the installation of\nDASH Version 3.1 onto the Grid MP servers. Furthermore, the\ninstaller sets up all the necessary environment variables that permit\nGDASH to be accessed from the command prompt. Note thatDASH\ndoes not need to be installed onto individual client PCs; the Grid MP\nsystem sends out a copy of DASH (together with a valid licence) to\nclient PCs as part of each workunit.\n5.1. Prerequisites\nIn order to deployGDASH correctly, the user must have a running\nGrid MP system with administrative access rights and access to the\nGrid MP SDK. DASH Version 3.1 or higher must be installed on the\nPC from which the GDASH installer is run and the user must have a\nsite licence (or equivalent demonstration licence) for DASH in order\nto permit execution on client machines.\n6. Documentation and availability\nA GDASH executable can be downloaded free of charge from\nhttp:\/\/www.gdash.info. The download package includes the GDASH\ninstaller and full user documentation in PDF format. Whilst GDASH\nitself does not require a licence, it does require a licenced copy of\nDASH in order to operate.\nWe are grateful to Sravish Sridar of Univa UD for his assistance in\nsetting up the original Grid MP system at the ISIS Facility. We are\nalso extremely grateful to STFC Facilities Business Unit IT Services\nfor their help and cooperation in deploying the Grid MP agent on\nclient PCs throughout the FBU. Thanks are also due to Elna Pidcock\nand Wei Dong of the CCDC, and to Alastair Florence and Norman\nShankland of the University of Strathclyde, for their help in testing\nand validating GDASH.\nReferences\nDavid, W. I. F., Shankland, K., van de Streek, J., Pidcock, E., Motherwell,\nW. D. S. & Cole, J. C. (2006). J. Appl. Cryst. 39, 910\u2013915.\nDiederichs, K. (2000). J. Appl. Cryst. 33, 1154\u20131161.\nMiller, R., Shah, N., Green, M. L., Furey, W. & Weeks, C. M. (2007). J. Appl.\nCryst. 40, 938\u2013944.\nShankland, K., McBride, L., David, W. I. F., Shankland, N. & Steele, G. (2002).\nJ. Appl. Cryst. 35, 443\u2013454.\nUniva UD (2008). http:\/\/www.univaud.com\/hpc\/products\/grid-mp\/.\ncomputer programs\nJ. Appl. Cryst. (2009). 42, 356\u2013359 Thomas A. N. Griffin et al. \u0002 Grid-enabled powder structure solution 359\n"}