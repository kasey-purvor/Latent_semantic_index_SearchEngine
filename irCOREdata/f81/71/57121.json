{"doi":"10.1109\/AVSS.2005.1577244","coreId":"57121","oai":"oai:eprints.lincoln.ac.uk:83","identifiers":["oai:eprints.lincoln.ac.uk:83","10.1109\/AVSS.2005.1577244"],"title":"Scene modelling using an adaptive mixture of Gaussians in colour and space","authors":["Dickinson, Patrick","Hunter, Andrew"],"enrichments":{"references":[{"id":18432692,"title":"A Hierarchical Approach to Robust Background Subtraction Using Color and Gradient Information\u201d,","authors":[],"date":"2002","doi":"10.1109\/motion.2002.1182209","raw":"O. Javed, K. Sha\ufb01que, and M. Shah, \u201cA Hierarchical Approach to Robust Background Subtraction Using Color and Gradient Information\u201d, Proc. of IEEE Workshop on Motion and Video Computing, pp. 2227, 2002.","cites":null},{"id":18432698,"title":"Adaptive Background Mixture Models for Real-Time Tracking,\u201d","authors":[],"date":"1999","doi":"10.1109\/cvpr.1999.784637","raw":"C. Stauffer and W.Grimson, \u201cAdaptive Background Mixture Models for Real-Time Tracking,\u201d Proc. of IEEE Conference on Computer Vision and Pattern Recognition, pp. 246252, 1999.","cites":null},{"id":18432694,"title":"An Improved Adaptive Background Mixture Model for Real-Time Tracking with Shadow Detection\u201d,","authors":[],"date":"2001","doi":"10.1007\/978-1-4615-0913-4_11","raw":"P. KaewTraKulPong and R. Bowden, \u201cAn Improved Adaptive Background Mixture Model for Real-Time Tracking with Shadow Detection\u201d, 2nd European Workshop on Advanced Video-based Surveillance Systems, 2001.","cites":null},{"id":18432689,"title":"Background and Foreground Modeling Using NonParametric Kernel Density Estimation for Visual Surveillance,\u201d","authors":[],"date":"2002","doi":"10.1109\/jproc.2002.801448","raw":"A. Elgammal, R. Duraiswami, D. Harwood, and L.S. Davis, \u201cBackground and Foreground Modeling Using NonParametric Kernel Density Estimation for Visual Surveillance,\u201d Proc. of the IEEE, Vol. 90, No. 7, pp.1151-1163, July 2002.","cites":null},{"id":18432696,"title":"Color Model Selection and Adaptation in Dynamic Scenes,\u201d","authors":[],"date":"1998","doi":"10.1007\/bfb0055684","raw":"Y. Raja, S. J. Mckenna, and S. Gong, \u201cColor Model Selection and Adaptation in Dynamic Scenes,\u201d Proc. 5th European Conference on Computer Vision, pp. 460474, 1998.","cites":null},{"id":18432697,"title":"Generative-Model-Based Tracking by Cluster Analysis of Image Differences,\u201d","authors":[],"date":"2002","doi":"10.1016\/s0921-8890(02)00203-8","raw":"A. Pece, \u201cGenerative-Model-Based Tracking by Cluster Analysis of Image Differences,\u201d Robotics and Autonomous Systems, Vol. 39, Nos. 3-4, pp. 181-194, 2002.","cites":null},{"id":18432690,"title":"Goldberger and A.Mayer, \u201cProbabilistic Space-Time Video Modelling via Piecewise GMM,\u201d","authors":[],"date":"2004","doi":"10.1109\/tpami.2004.1262334","raw":"H.Greenspan, J. Goldberger and A.Mayer, \u201cProbabilistic Space-Time Video Modelling via Piecewise GMM,\u201d IEEE Trans. on Pattern Analysis and Machine Intelligence, Vol. 26, No. 3, pp.384-396, March 2004.","cites":null},{"id":18432688,"title":"Integrated Regionand Pixel-Based Approach to Background Modelling,\u201d","authors":[],"date":"2002","doi":"10.1109\/motion.2002.1182206","raw":"M. Cristani, M. Bicego, and V. Murino, \u201cIntegrated Regionand Pixel-Based Approach to Background Modelling,\u201d Proc. of IEEE Workshop on Motion and Video Computing, pp. 3-8, 2002.","cites":null},{"id":18432691,"title":"Mixture-of-Gaussian Background Models,\u201d","authors":[],"date":"2002","doi":"10.1007\/3-540-47977-5_36","raw":"M.Harville.\u201cAframeworkforHigh-LevelFeedbacktoAdaptive, Per-Pixel, Mixture-of-Gaussian Background Models,\u201d Proc. of 6th European Conference on Computer Vision, Vol. 3, pp. 543-560, 2002.","cites":null},{"id":18432695,"title":"Object Based Segmentation of Video Using Color, Motion and Spatial Information,\u201d","authors":[],"date":"2001","doi":"10.1109\/cvpr.2001.991039","raw":"S. Khan and M. Shah, \u201cObject Based Segmentation of Video Using Color, Motion and Spatial Information,\u201d Proc. of IEEE Conference on Computer Vision and Pattern Recognition, Vol. 2, pp. 746751, 2001.","cites":null},{"id":18432699,"title":"P\ufb01nder: Real-Time Tracking of the Human Body,\u201d","authors":[],"date":"1997","doi":"10.1109\/34.598236","raw":"C. Wren, A. Azarbayejani, T. Darrell, and A. Pentland, \u201cP\ufb01nder: Real-Time Tracking of the Human Body,\u201d IEEE Trans. on Pattern Analysis and Machine Intelligence, Vol. 19, No. 7, pp. 780785, July 1997.","cites":null},{"id":18432700,"title":"Tracking Multiple Humans in Complex Situations,\u201d","authors":[],"date":"2004","doi":"10.1109\/tpami.2004.73","raw":"T.Zhao and R.Nevatia, \u201cTracking Multiple Humans in Complex Situations,\u201d IEEE Trans. on Pattern Analysis and Machine Intelligence Vol. 26, No. 9, pp. 1208-1221, September 2004.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2005-09-15","abstract":"We present an integrated pixel segmentation and region\\ud\ntracking algorithm, designed for indoor environments. Visual monitoring systems often use frame differencing techniques to independently classify each image pixel as either foreground or background. Typically, this level of processing does not take account of the global image structure, resulting in frequent misclassification. \\ud\nWe use an adaptive Gaussian mixture model in colour and space to represent background and foreground regions of the scene. This model is used to probabilistically classify observed pixel values, incorporating the global scene structure into pixel-level segmentation. We evaluate our system over 4 sequences and show that it successfully segments foreground pixels and tracks major foreground regions as they move through the scene","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/57121.pdf","fullTextIdentifier":"http:\/\/eprints.lincoln.ac.uk\/83\/1\/avss2005pld.pdf","pdfHashValue":"9cdd54f9900db0c5f66972fea91b6b6d31a0729d","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lincoln.ac.uk:83<\/identifier><datestamp>\n      2013-03-13T08:21:58Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47373430<\/setSpec><setSpec>\n      74797065733D636F6E666572656E63655F6974656D<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lincoln.ac.uk\/83\/<\/dc:relation><dc:title>\n        Scene modelling using an adaptive mixture of Gaussians in colour and space<\/dc:title><dc:creator>\n        Dickinson, Patrick<\/dc:creator><dc:creator>\n        Hunter, Andrew<\/dc:creator><dc:subject>\n        G740 Computer Vision<\/dc:subject><dc:description>\n        We present an integrated pixel segmentation and region\\ud\ntracking algorithm, designed for indoor environments. Visual monitoring systems often use frame differencing techniques to independently classify each image pixel as either foreground or background. Typically, this level of processing does not take account of the global image structure, resulting in frequent misclassification. \\ud\nWe use an adaptive Gaussian mixture model in colour and space to represent background and foreground regions of the scene. This model is used to probabilistically classify observed pixel values, incorporating the global scene structure into pixel-level segmentation. We evaluate our system over 4 sequences and show that it successfully segments foreground pixels and tracks major foreground regions as they move through the scene.<\/dc:description><dc:date>\n        2005-09-15<\/dc:date><dc:type>\n        Conference or Workshop contribution<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lincoln.ac.uk\/83\/1\/avss2005pld.pdf<\/dc:identifier><dc:identifier>\n          Dickinson, Patrick and Hunter, Andrew  (2005) Scene modelling using an adaptive mixture of Gaussians in colour and space.  In: IEEE Conference on Advanced Video and Signal based Surveillance, 15-15 Sept 2005, Como, Italy.  <\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/AVSS.2005.1577244<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lincoln.ac.uk\/83\/","http:\/\/dx.doi.org\/10.1109\/AVSS.2005.1577244"],"year":2005,"topics":["G740 Computer Vision"],"subject":["Conference or Workshop contribution","PeerReviewed"],"fullText":"Scene Modelling Using An Adaptive Mixture of Gaussians in Colour and Space\nPatrick Dickinson Andrew Hunter\nDepartment of Computing and Informatics\nUniversity of Lincoln\nLincoln LN6 7TS, UK\nAbstract\nWe present an integrated pixel segmentation and region\ntracking algorithm, designed for indoor environments. Vi-\nsual monitoring systems often use frame differencing tech-\nniques to independently classify each image pixel as either\nforeground or background. Typically, this level of process-\ning does not take account of the global image structure,\nresulting in frequent misclassification. We use an adap-\ntive Gaussian mixture model in colour and space to repre-\nsent background and foreground regions of the scene. This\nmodel is used to probabilistically classify observed pixel\nvalues, incorporating the global scene structure into pixel-\nlevel segmentation. We evaluate our system over 4 se-\nquences and show that it successfully segments foreground\npixels and tracks major foreground regions as they move\nthrough the scene.\n1. Introduction\nThe challenge of intelligent video surveillance is to extract\nmeaningful structure from a stream of raw image pixel data.\nA popular approach is to apply a \u201cbottom-up\u201d sequence\nof processing steps to each frame image. Each step rep-\nresents an incremental abstraction of the image data, re-\nsulting in a high-level and context specific interpretation of\nthe scene dynamics. Typically, the first level of process-\ning employs a per-pixel background subtraction technique\nto classify each pixel as either foreground or background.\nMany methods have been proposed. Wren\u2019s Pfinder system\n[14], and Zhao [15] use an adaptive Gaussian to represent\neach pixel\u2019s background colour value. Subsequent values\nare classified by thresholding against this distribution, and\nadaption of the model copes with slow lighting changes. El-\ngammal [2] uses a non-parametric kernel density estimate\ninstead of a Gaussian model. Stauffer [12] introduced a per-\npixel mixture of Gaussians: the popularity of this model\nlies in its ability to represent multiple processes for each\npixel. Many variations and modifications have been devel-\noped, such as [5, 6]. A common feature of these schemes is\nthat pixels are treated as spatially independent processes.\nSpatial correlation of foreground pixels is typically de-\nveloped at the second processing stage, through the cluster-\ning of foreground pixels into homogeneous regions. Such\nsystems have also been well studied and explored. The\nPfinder system [14] uses Gaussian distributions in colour\nand space, and explicitly assigns each to a specific body\npart. Khan [7] uses a similar approach, and further ex-\ntends the feature space to include optical flow values [8].\nNon-probabilistic schemes have also been used success-\nfully, such as connected components algorithms [9, 15].\nThe work presented in this paper is motivated by the fact\nthat background subtraction techniques treat each pixel as\nan independent process: classification is based only on pre-\nvious observation of that pixel\u2019s value. Failure to interpret\nobservations within the context of the higher-level image\nstructure leads to frequent and obvious misclassifications,\nfor example when a background object, or the camera, move\nslightly. Some authors have addressed this issue. Stauffer\u2019s\n[12] model can eliminate misclassification due to cyclical\nmotion in the background (such as moving foliage), but\nnot sporadic movements. Elgammal [2] thresholds against\nadjacent pixel distributions, to eliminate small movements.\nHowever, a more structural approach is warranted. Harville\n[4] proposes a general framework in which high-level deci-\nsions feedback to background subtraction. Cristani [1] suc-\ncessfully integrates a pixel model with a particle filter track-\ning system to implement high-level modulation of low-level\nprocessing.\nOur approach is to dispense with a per-pixel background\nmodel. Instead we model homogenous regions of scene pix-\nels using a 5-dimensional mixture of Gaussians in colour\nand space. Each regional distribution is classified as either\nforeground or background, and each pixel is assigned to one\nregion. Thus each pixel is classified as foreground or back-\nground according to the classification of the distribution to\nwhich it is assigned.\nWe process each new frame by probabilistically assign-\ning each pixel to a distribution. We then re-estimate each\ndistribution\u2019s parameters from the statistics of its assigned\npixels. The result is that if a distribution represents a region\n1\nof pixels which moves or changes over time, then the pa-\nrameters of the distribution are updated to reflect this, and\nthe distribution automatically tracks the region through the\nscene. If changes in the scene are not well represented by\na the model then we add new distributions, which we clas-\nsify as foreground. The background and foreground dis-\ntributions are initialised and developed automatically, and\nrequire no off-line or supervised pre-processing.\nUsing this scheme we are able to directly integrate pixel-\nlevel segmentation and classification with a dynamic struc-\ntural representation of the scene. We are currently using this\nas the basis for developing a visual surveillance system for\nmonitoring individuals in interior environments.\n2. Related Work\nStatistical region modelling without background subtraction\nhas been used by some researchers in computer vision and\nvideo indexing. McKenna [10] developed a tracker in which\nthe colour distribution of a known object is represented as a\nmixture of 2-dimensional Gaussians. This system does not\nexplicitly model spatial features of objects or regions, and\nrequires the off-line learning of the object model.\nGaussian mixture models have been used in the field of\nimage indexing as a method of extracting salient features\nfrom a scene. Greenspan [3] indexes video sequences by\nadding time to the pixel feature vector. This requires that\nthe video sequence is split into sub-sequences, and a model\nbuilt for each. A second processing pass uses the model to\nsegment objects in each frame. Though effective, this ap-\nproach is unsuitable for real-time surveillance applications.\nWork by Pece [11] in tracking objects in monochromatic\nvideo bears conceptual similarity to ours. Pece models the\nscene image as a mixture of components in space and grey-\nscale intensity. The likelihood of each pixel is calculated for\neach cluster, and used to weight the pixel\u2019s contribution to\nthe parameter re-estimation. However, the cluster densities\nare only modelled as Gaussian for the spatial distribution of\nforeground objects.\n3. Our Approach\nWe represent regions of the scene using a mixture of\nGaussian components in 5-dimensional space. The first two\ndimensions are x and y image coordinates, the remaining\nthree are YUV colour values. A pixel value is represented\nin this feature space by a vectorX = [x, y, Y, U, V ]T . Each\ncomponent of the mixture is represented by a distribution\nfunction of the form:\np(X|\u03b8i) = \u03c9i\n1\u221a\n(2pi)d|\u03a3i|\ne\u2212\n1\n2\n(X\u2212\u00b5i)\nT\n\u03a3\n\u22121\ni\n(X\u2212\u00b5i) (1)\nWhere the distribution parameters \u03b8i = {\u03c9i, \u00b5i,\u03a3i} are\nthe weight \u03c9i, mean \u00b5i, and covariance matrix\u03a3i of the ith\ncomponent. The dimensionality, d, is 5. For a mixture of k\ncomponents, the following condition holds:\nk\u2211\ni=1\n\u03c9i = 1 (2)\nGiven a set of model parameters of this form, an ob-\nserved pixel value may be classified by assigning it to the\ncomponent with the maximum posterior probability, Cmap.\nUsing the log likelihood of the pixel value:\nCmap = argmaxi {log(p(X|\u03b8i)} (3)\nWe relax the model slightly by assuming that the spa-\ntial and colour distributions of each component are inde-\npendent and uncorrelated. We may therefore re-express the\ndistribution function in equation (1) as the product of a 2-\ndimensional spatial Gaussian and a 3-dimensional colour\nGaussian, with parameter sets \u03b8si = {\u03c9si , \u00b5si ,\u03a3si} and\n\u03b8ci = {\u03c9\nc\ni , \u00b5\nc\ni ,\u03a3\nc\ni}. Correspondingly, each pixel value is\nexpressed by the spatial vectors Xs = [x, y]T , and colour\nvectorXc = [Y,U, V ]T . Hence, equation (3) becomes:\nCmap = argmaxi {log(p(X\ns|\u03b8si ) + log(p(X\nc|\u03b8ci )} (4)\nWe use each component of our model to represent a ho-\nmogenous region of the scene: that is, a set of pixels with\nsimilar spatial and colour characteristics. Our premise is\nthat such a region is generated by a single corresponding\nprocess, such as part of an object. Components are used\nto represent both foreground and background regions. We\ndefine a background region as one which corresponds to a\nstatic or marginally varying process. This could be an object\nor part of an object which is perceived as static or displaying\ninconsequential movement. A foreground region is one in\nwhich the process is not static: the corresponding object ex-\nhibits significant movement or change over time. Each com-\nponent is explicitly classified as either foreground or back-\nground. Background components are constructed in an ini-\ntialisation phase, and foreground components are detected\nduring frame processing. All components are updated dur-\ning frame processing, to reflect changes in the scene.\nThe remaining significant data structure in our system is\nthe \u201csupport map\u201d, which stores the current component as-\nsignment for each pixel. During frame processing we use\nequation (4) to assign each new pixel to one of the current\ncomponents. If the colour value of the pixel changes sig-\nnificantly it may be assigned to a new component. When\nthis happens, the support map entry for the pixel is changed\ncorrespondingly.\n2\n3.1. Building the Background\nThe background components are constructed from the first\nframe of the sequence. Expectation Maximisation (EM) is\nan established algorithm for estimating a maximum like-\nlihood set of parameters for a Gaussian mixture, given a\nmodel order and data set. Greenspan [3] uses this technique\nto build parameters for short video sections, and utilises the\nMinimum Description Length criteria to estimate an appro-\npriate model order. Although this technique is effective and\nwell principled, we face some problems in using it for a\nreal-time surveillance system. Firstly, EM is computation-\nally expensive. We wish to employ a method which ini-\ntialises quickly to build the background model, and which\nis fast enough to develop the model on a per-frame basis.\nEM methods are unsuitable in this respect. Secondly, we\nneed to be able to adapt the model order dynamically. If\nwe consider that the model order is related to the number\nof underlying processes, then, as objects enter and leave the\nscene, the model order needs to be re-estimated.\nThe technique of splitting and merging components has\nbeen shown to be effective for controlling convergence of\nthe EM algorithm [13]. It has also been used by McKenna\n[10] and by Pece [11] as a technique for dynamically adapt-\ning model order. We use the iterative splitting and merg-\ning of components as the basis for building an initial back-\nground model, and subsequently for developing foreground\ncomponents during frame processing. We find that this\nmethod is computationally manageable, and, by minimis-\ning the variance of the model components, generates a suit-\nable representation of the scene regions. We build the back-\nground model using the following steps:\n1 We initialise the mixture with a single component es-\ntimated from the statistics of the entire image, and set\neach support map entry to this component.\n2 We iteratively select the components with the highest\nspatial and colour variances, and split each into two\nnew components.\n3 Pairs of similar components are merged.\n4 Components which are significantly spatially dis-\nconnected (possibly representing different objects or\nprocesses) are split.\nWe now describe these steps in more detail. Starting with\nan initial component generated by step 1, we split it into a\nset of components by iteratively applying step 2, as follows.\nWe calculate the principle eigenvalue, \u03bbsi and correspond-\ning eigenvector,\u039bsi for each component\u2019s spatial covariance\nmatrix. We select the component Csp = argmaxi {\u03bbsi}. If\nits eigenvalue \u03bbssp > T ssp, where T ssp is a predefined thresh-\nold, then we split component Csp. We create a new compo-\nnent and re-assign to it those pixels which satisfy:\n(Xs \u2212 \u00b5ssp) \u00b7\u039b\ns\nsp > 0 (5)\nThis amounts to placing a separating plane through the\nspatial mean, perpendicular to\u039bssp. The parameters of both\ncomponents are then re-estimated from the statistics of their\nrespective assigned pixel values as follows:\n\u03c9si =\nni\nN\n(6)\n\u00b5si =\n1\nni\nni\u2211\nj=1\nX\ns\nij (7)\nZ\ns\ni = \u00b5\ns\ni\nT\n\u00b5si (8)\n\u03a3\ns\ni =\n\u2211ni\nj=1X\ns\nij\nT\nX\ns\nij\nni\n\u2212 Zsi (9)\nWhere Xsij is the spatial component of the jth pixel as-\nsigned to the ith component; ni is the number of pixels\nassigned to the component; and N is the total number of\npixels in the image. The value of \u00b5si used in equation (8) is\nthe new value calculated using equation (7). We then apply\nthe same selection and splitting procedure in colour space,\nusing a corresponding threshold T csp, to split the component\nwith the highest colour variance. We repeat this process,\nalternating between spatial and colour distributions, until\nreaching a maximum number of components, or until the\nlargest eigenvalues fall below their thresholds.\nWe now merge similar components. If Msi (Xs) is the\nspatial Mahalanobis distance of Xs from \u00b5si , and Mci (Xc)\nis the colour Mahalanobis distance of Xc from \u00b5ci , then a\npair of components is considered suitable for merging if the\nfollowing holds:\nMs1(\u00b5\ns\n2) < T\ns\nmg \u2227 M\ns\n2(\u00b5\ns\n1) < T\ns\nmg \u2227\nMc1(\u00b5\nc\n2) < T\nc\nmg \u2227 M\nc\n2(\u00b5\nc\n1) < T\nc\nmg (10)\nWhere T smg and T cmg are predefined thresholds. We con-\nsider each pair of components, and merge the qualifying\npair with the lowest value of max(Mc1(\u00b5c2), Mc2(\u00b5c1)).\nThis procedure is repeated until no qualifying pairs remain.\nWe next seek to identify components which represent\nspatially disconnected regions, and split them to represent\nthose regions separately. We order the components in de-\nscending value of \u03bbsi , and step through the list. For each,\nwe use a connected components algorithm to determine if it\nrepresents two or more disconnected regions of the support\nmap: if so, we split the largest region away from the rest as\na new component. For reasons of efficiency we implement\nthis at a reduced resolution. We repeat this until no discon-\nnected components are found, or for a maximum number\nof iterations. Finally, when this process is complete, com-\nponents which have a zero or very small weight are culled\nfrom the model.\n3\n3.2. Frame Processing\nOnce the background model has been built we are ready\nto start processing new frames. For each new frame the\nfollowing operations are applied:\n1 The current model is used assign the pixels in the new\nframe, and rebuild the support map.\n2 Existing background and foreground components are\nupdated from the support map.\n3 New foreground components are detected and created.\n4 Foreground components are merged and culled.\nThe current mixture of components is used to assign each\npixel in the new frame using equation (4). Many systems,\nsuch as [12], use a predictive filter to estimate the change\nin position of objects between frames. Instead, we em-\nploy Elgammal\u2019s observation [2] that foreground objects,\nthough moving, will have moved only a relatively short\ndistance between one frame and the next. Thus we use\nonly the previous frame\u2019s observed model to assign pixels:\neach pixel is assigned to Cmap, and its corresponding sup-\nport map entry set to reflect this. During frame process-\ning a minimum likelihood threshold Tmap is applied: if\nlog(p(X|\u03b8Cmap)) < Tmap, the pixel is set as unassigned.\nThe parameters of the existing background and fore-\nground components are now re-estimated. For foreground\ncomponents, equations of the form (6) to (9) are used to re-\nbuild its spatial and colour distributions from its assigned\npixels. For background components we adapt the parame-\nters more slowly. For each component iwe start by calculat-\ning a set of parameter values \u03b8(i,sm) from the support map,\nin the same way as for foreground components. However,\nwe do not substitute these new values directly. Given the\nexisting parameters \u03b8(i,t\u22121), we calculate the new set \u03b8(i,t)\nusing an adaptive learning rate:\n\u03b8(i,t) = \u03b1i\u03b8(i,sm) + (1\u2212 \u03b1i)\u03b8(i,t\u22121) (11)\nThe learning rate \u03b1i is calculated for each component for\neach frame as:\n\u03b1i =\n\u03c9(i,sm)\n\u03c9(i,t\u22121)\n, \u03b1i \u2208 [0, 1] (12)\nWhere \u03c9(i,sm) and \u03c9(i,t\u22121) are the weights from \u03b8(i,sm)\nand \u03b8(i,t\u22121) respectively. Constraining the adaptation in this\nway ensures that if a background component is occluded it\ndoes not adapt too quickly to represent only the visible part.\nIt also helps to prevent the background from over adapting\nto misclassified foreground pixels. It is necessary to renor-\nmalise the component weights at this point, to enforce the\ncondition in equation (2).\nNext we use the support map to detect new foreground\nregions. The map is divided into a grid of resolution 16\u00d716\npixels, and the number of unassigned pixels counted for\neach location. Locations exceeding a threshold density are\nconsidered to correspond to new foreground regions. A sin-\ngle foreground component is built from the unassigned pix-\nels in all such grid locations, using equations of the form\n(6) to (9) to build the spatial and colour distributions. This\nnew component is then split using the same splitting method\nused to build the background.\nRegardless of whether any new components have been\nadded this frame, we test all foreground components for\npossible merging. First, we restrict the spatial and colour\nvariances of each component to pre-defined maximum val-\nues. This helps prevent over adaptation to misclassified\nbackground pixels. We then merge similar components us-\ning the same pair-wise method as was used for the back-\nground model. Finally, we conclude frame processing by\nculling any foreground components which have a zero or\nvery low weight.\n3.3. Modifications to Frame Processing\nWe have made some modifications to the algorithm in or-\nder to improve performance. Firstly we encounter a prob-\nlem using equation (4) during frame updates: the spatial\nvariance \u03a3si for large background components is typically\nvery high. This frequently results in pixels being assigned\nto regions from which they are significantly disconnected.\nIn particular, this hampers detection of new foreground re-\ngions. To resolve this we apply the additional restriction\nthat a pixel may only be assigned to a background compo-\nnent if its spatial likelihood exceeds a predefined threshold\nT slik:\nlog(p(Xs|\u03b8si )) > T\ns\nlik (13)\nWe also make a performance optimisation to the assign-\nment of pixels during frame processing. If a pixel is cur-\nrently assigned to an existing component, its colour value\nremains relatively unchanged, and its likelihood given the\nsame assignment is greater than Tmap then we leave its as-\nsignment unchanged. This significantly reduces processing\ntime. A pixel value is defined as unchanged if each element\nof its YUV colour value is within a threshold deviation from\nthe value first used to assign it.\n4. Experiments\nWe have tested our system on 4 short sequences, each com-\nprising 150-200 frames, and recorded using a DV cam-\ncorder in 720\u00d7576 PAL format. Each sequence shows a\nsingle human figure engaged in some routine activity, such\nas walking to a chair and sitting down. One sequence\nwas filmed in an exterior environment, and the others are\n4\ninterior. Figure 1 shows an example frame from one of\nthe sequences, together with representations of the back-\nground and foreground components, and a binary mask of\nthe foreground pixels. The component representations were\nconstructed by setting each pixel to \u00b5c of the component\nCsmap = argmaxi {log(p(X\ns|\u03b8si )}. Figure 2 shows some\nsimilar example frames from two of the other sequences.\n(a) (b)\n(c) (d)\nFigure 1: A frame from an image Sequence. (a) Captured\nimage. (b) Background components. (c) Foreground com-\nponents. (d) Foreground pixels in the support map.\nThe representation shown in figure 1 is typical in that\nwe can infer a correspondence between foreground compo-\nnents and body parts of the subject. We intend to develop\nour system to use the foreground regions as the basis for a\nhuman body tracking system. We therefore evaluated how\neffectively the foreground components in our system repre-\nsent and track major body parts.\nIn all sequences, the major body parts were represented\nand tracked successfully. In some cases the segmentation\nfailed where part of the foreground coincided with, or was\nimmediately adjacent to, a background region of similar\ncolour. Small body parts, such as hands or feet, were fre-\nquently not represented by a corresponding component, or\nwere not tracked between frames due to sudden large dis-\nplacements.\nThe system proved robust to camera movement. Dur-\ning one of the sequences the camera was unintentionally\nmoved, resulting in a global image translation. The sys-\ntem continued to operate without noticeable corresponding\nmisclassifications. However, we also noted that the slowed\nadaptation of the background sometimes results in the in-\ntroduction of spurious foreground components where there\nis a significant lighting change.\nEach sequence was processed using an Intel Pentium 4\nFigure 2: Example frames from two other sequences.\n(2.8GHz) PC system, and the executable code was devel-\noped in C++. Each sequence took approximately 10 sec-\nonds to initialise. Frame processing time ranged between\n0.94 and 2.70 seconds.\nWe quantify our results in the following tables, which\nwere constructed as follows. For each sequence we identi-\nfied the major body parts. We inspected the representation\nin each frame, in an arrangement similar to figure 1. For\neach body part we categorised the estimated proportion (of\nits pixels) assigned to a corresponding foreground compo-\nnent as either more than 0.75, 0.75-0.5, less than 0.5, or 0. A\nclassification of 0 corresponds to a failure to represent the\nbody part in that frame. For the whole sequence we then\ncalculated the proportion of frames in which each body part\nwas represented by each category. The sequence tables are\nordered from easiest to hardest in terms of perceived diffi-\nculty of segmentation. The lower arms are classified sepa-\nrately in the last two sequences, as the subject was wearing\nshort sleeves.\nResults for Sequence 1\n>0.75 >0.50 <0.50 0.00\nHead & Face 0.95 0.05 0 0\nTorso & Arms 0.92 0.08 0 0\nLegs & Feet 0.73 0.19 0.08 0\nResults for Sequence 2\n>0.75 >0.50 <0.50 0.00\nHead & Face 0.07 0.24 0.57 0.12\nTorso & Arms 0.89 0.11 0 0\nLegs & Feet 1.00 0 0 0\n5\nResults for Sequence 3\n>0.75 >0.50 <0.50 0.00\nHead & Face 0.27 0.04 0.60 0.09\nTorso & Arms 0.52 0.43 0.05 0\nLower Arms 0.24 0.07 0.45 0.24\nLegs & Feet 0.50 0.46 0.02 0.02\nResults for Sequence 4\n>0.75 >0.50 <0.50 0.00\nHead & Face 0.28 0.37 0.13 0.22\nTorso & Arms 0.52 0.43 0.05 0\nLower Arms 0.69 0.21 0.10 0\nLegs & Feet 0.06 0.02 0.24 0.68\n5. Conclusions and Further Work\nWe have presented a novel algorithm which tracks objects\nby modelling global scene structure, and shown that it is ef-\nfective. The execution speed of our algorithm is not quite\nsuitable for real-time processing, but we consider that since\nwe have not yet attempted low-level performance optimisa-\ntion, and that our platform is relatively modest, it is feasible\nto develop a version which runs in real-time. This will be\none objective for further development work.\nWe are currently developing an indoor surveillance sys-\ntem based on our work. To achieve this we need to improve\nthe on-line adaptation of the model, so that we can identify\nspurious foreground components and re-classifying them as\nbackground. We intend to do this by clustering associated\nforeground regions to build models of complete objects. We\nalso wish to investigate using the features of foreground\ncomponents to implement a human body tracking system.\nAcknowledgements\nThis work was supported by an EPSRC CASE studentship\nin conjunction with Nectar Electronics Ltd, Durham, UK.\nReferences\n[1] M. Cristani, M. Bicego, and V. Murino, \u201cIntegrated Region-\nand Pixel-Based Approach to Background Modelling,\u201d Proc.\nof IEEE Workshop on Motion and Video Computing, pp. 3-8,\n2002.\n[2] A. Elgammal, R. Duraiswami, D. Harwood, and L.S.\nDavis, \u201cBackground and Foreground Modeling Using Non-\nParametric Kernel Density Estimation for Visual Surveil-\nlance,\u201d Proc. of the IEEE, Vol. 90, No. 7, pp.1151-1163, July\n2002.\n[3] H.Greenspan, J. Goldberger and A.Mayer, \u201cProbabilistic\nSpace-Time Video Modelling via Piecewise GMM,\u201d IEEE\nTrans. on Pattern Analysis and Machine Intelligence, Vol. 26,\nNo. 3, pp.384-396, March 2004.\n[4] M. Harville. \u201cA framework for High-Level Feedback to Adap-\ntive, Per-Pixel, Mixture-of-Gaussian Background Models,\u201d\nProc. of 6th European Conference on Computer Vision, Vol.\n3, pp. 543-560, 2002.\n[5] O. Javed, K. Shafique, and M. Shah, \u201cA Hierarchical Ap-\nproach to Robust Background Subtraction Using Color and\nGradient Information\u201d, Proc. of IEEE Workshop on Motion\nand Video Computing, pp. 2227, 2002.\n[6] P. KaewTraKulPong and R. Bowden, \u201cAn Improved Adap-\ntive Background Mixture Model for Real-Time Tracking with\nShadow Detection\u201d, 2nd European Workshop on Advanced\nVideo-based Surveillance Systems, 2001.\n[7] S. Khan and M. Shah, \u201cTracking People in the Presence of\nOcclusion,\u201d Asian Conference on Computer Vision, 2000.\n[8] S. Khan and M. Shah, \u201cObject Based Segmentation of Video\nUsing Color, Motion and Spatial Information,\u201d Proc. of IEEE\nConference on Computer Vision and Pattern Recognition,\nVol. 2, pp. 746751, 2001.\n[9] S. McKenna, S. Jabri, Z. Duric, A. Rosenfeld, and H. Wech-\nsler, \u201cTracking Groups of People,\u201d Computer Vision and Im-\nage Understanding, Vol. 80, No. 1, pp. 4256, October 2000.\n[10] Y. Raja, S. J. Mckenna, and S. Gong, \u201cColor Model Selec-\ntion and Adaptation in Dynamic Scenes,\u201d Proc. 5th European\nConference on Computer Vision, pp. 460474, 1998.\n[11] A. Pece, \u201cGenerative-Model-Based Tracking by Cluster\nAnalysis of Image Differences,\u201d Robotics and Autonomous\nSystems, Vol. 39, Nos. 3-4, pp. 181-194, 2002.\n[12] C. Stauffer and W.Grimson, \u201cAdaptive Background Mixture\nModels for Real-Time Tracking,\u201d Proc. of IEEE Conference\non Computer Vision and Pattern Recognition, pp. 246252,\n1999.\n[13] N. Ueda, R. Nakano, Z. Ghahramani, and G.E. Hinton,\n\u201cSMEM algorithm for mixture models,\u201d Neural Computation,\nVol. 12, No. 9, pp. 2109-2128, September 2000.\n[14] C. Wren, A. Azarbayejani, T. Darrell, and A. Pentland,\n\u201cPfinder: Real-Time Tracking of the Human Body,\u201d IEEE\nTrans. on Pattern Analysis and Machine Intelligence, Vol. 19,\nNo. 7, pp. 780785, July 1997.\n[15] T.Zhao and R.Nevatia, \u201cTracking Multiple Humans in Com-\nplex Situations,\u201d IEEE Trans. on Pattern Analysis and Ma-\nchine Intelligence Vol. 26, No. 9, pp. 1208-1221, September\n2004.\n6\n"}