{"doi":"10.1080\/0968776042000259555","coreId":"14175","oai":"oai:generic.eprints.org:609\/core5","identifiers":["oai:generic.eprints.org:609\/core5","10.1080\/0968776042000259555"],"title":"Easing the transition from paper to screen: an evaluatory framework for CAA migration","authors":["McAlpine, Mhairi"],"enrichments":{"references":[{"id":1878242,"title":"Accessibility of SQA assessments in Portable Document Format. Project report to Scottish Qualifications Authority.","authors":[],"date":"2003","doi":null,"raw":"Nisbet (2003) Accessibility of SQA assessments in Portable Document Format. Project report to Scottish Qualifications Authority. CALL Centre, University of Edinburgh McAlpine & Ware (2003) Introducing computer assisted assessment in Scotland: laying the foundations for an integrated approach, 29th International Association for Educational Assessment Conference, Manchester, 5\u201310 October.","cites":null},{"id":1878238,"title":"Assessing complex problem-solving performances: the NAEP problem solving in technology-rich environments project, Advisory Council for New Technologies in Education, Cambridge,","authors":[],"date":"2004","doi":null,"raw":"Bennett, Jenkins, Persky & Weiss (2004) Assessing complex problem-solving performances: the NAEP problem solving in technology-rich environments project, Advisory Council for New Technologies in Education, Cambridge, 10 February.","cites":null},{"id":1878237,"title":"Assessment using the QCA \u2018Rules Base\u2019 with process-based data, Advisory Council for New Technologies in Education,","authors":[],"date":"2004","doi":null,"raw":"Ripley (2004) Assessment using the QCA \u2018Rules Base\u2019 with process-based data, Advisory Council for New Technologies in Education, Cambridge, 10 February.","cites":null},{"id":1878243,"title":"Computer assisted assessment (CAA) of proof = proof of CAA\u2014new approaches to computer assessment for higher level learning,","authors":[],"date":"2001","doi":null,"raw":"McCabe (2001) Computer assisted assessment (CAA) of proof = proof of CAA\u2014new approaches to computer assessment for higher level learning, 6th International Conference on Technology in Mathematics Teaching, ICTMT-6, Klagenfurt, Austria.248 M. McAlpine McKenna & Bull (2000) Quality assurance of computer assisted assessment: practical and strategic issues, Quality Assurance in Education, 8(1), 24\u201331.","cites":null},{"id":1878244,"title":"Computer based testing of medical knowledge,","authors":[],"date":"2003","doi":null,"raw":"Mitchell, Aldridge, Williamson & Broomhead (2003) Computer based testing of medical knowledge, 7th International Computer Assisted Assessment Conference, Loughborough, 8\u20139 July.","cites":null},{"id":1878239,"title":"How the internet will help large-scale assessment reinvent itself, Education Policy Analysis Archives,","authors":[],"date":"2001","doi":null,"raw":"Bennett (2001) How the internet will help large-scale assessment reinvent itself, Education Policy Analysis Archives, 9(5).","cites":null},{"id":1878235,"title":"Piloting summative web assessment in secondary education,","authors":[],"date":"2003","doi":null,"raw":"Ashton, Schofield & Woodger (2003) Piloting summative web assessment in secondary education, 7th International Computer Assisted Assessment Conference, Loughborough, 8\u20139 July.","cites":null},{"id":1878240,"title":"Special arrangements for using ICT in SQA assessments. Project report to Scottish Qualifications Authority.","authors":[],"date":"2002","doi":null,"raw":"Nisbet (2002) Special arrangements for using ICT in SQA assessments. Project report to Scottish Qualifications Authority. CALL Centre, University of Edinburgh.","cites":null},{"id":1878236,"title":"The e-assessment tail: new opportunities? New challenges? The uses of ICT","authors":[],"date":"2003","doi":null,"raw":"Ripley (2003) The e-assessment tail: new opportunities? New challenges? The uses of ICT in assessment, Beyond the Exam Conference, Bristol, 19 November.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2004","abstract":"Computer assisted assessment is becoming more and more common through further and higher education. There is some debate about how easy it will be to migrate current assessment practice to a computer enhanced format and how items which are currently re-used for formative purposes may be adapted to be presented online. This paper proposes an evaluatory framework to assess and enhance the practicability of large-scale CAA migration for existing items and assessments. The framework can also be used as a tool for exposing compromises between delivery mechanism and validity-exposing the limits of validity of modified paper based assessments and highlighting the crucial areas for transformative assessments","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/14175.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/609\/1\/ALT_J_Vol12_No3_2004_Easing%20the%20transition%20from%20pap.pdf","pdfHashValue":"df7196de096a9243aca9ffa48ce89f49dd066734","publisher":"Taylor and Francis Ltd","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:609<\/identifier><datestamp>\n      2011-04-04T08:52:11Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/609\/<\/dc:relation><dc:title>\n        Easing the transition from paper to screen: an evaluatory framework for CAA migration<\/dc:title><dc:creator>\n        McAlpine, Mhairi<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        Computer assisted assessment is becoming more and more common through further and higher education. There is some debate about how easy it will be to migrate current assessment practice to a computer enhanced format and how items which are currently re-used for formative purposes may be adapted to be presented online. This paper proposes an evaluatory framework to assess and enhance the practicability of large-scale CAA migration for existing items and assessments. The framework can also be used as a tool for exposing compromises between delivery mechanism and validity-exposing the limits of validity of modified paper based assessments and highlighting the crucial areas for transformative assessments.<\/dc:description><dc:publisher>\n        Taylor and Francis Ltd<\/dc:publisher><dc:date>\n        2004<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/609\/1\/ALT_J_Vol12_No3_2004_Easing%20the%20transition%20from%20pap.pdf<\/dc:identifier><dc:identifier>\n          McAlpine, Mhairi  (2004) Easing the transition from paper to screen: an evaluatory framework for CAA migration.  Association for Learning Technology Journal, 12 (3).  pp. 231-248.  ISSN 0968-7769 (print)\/1741-1629 (online)     <\/dc:identifier><dc:relation>\n        10.1080\/0968776042000259555<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/609\/","10.1080\/0968776042000259555"],"year":2004,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"ALT-J, Research in Learning Technology\nVol. 12, No. 3, September 2004\nISSN 0968\u20137769 (print)\/ISSN 1741\u20131629 (online)\/04\/030231\u201318\n\u00a9 2004 Association for Learning Technology\nDOI: 10.1080\/0968776042000259555\nEasing the transition from paper to \nscreen: an evaluatory framework for \nCAA migration\nMhairi McAlpine*\nScottish Qualifications Authority, Glasgow, UK\nTaylor and Francis LtdCALT120304.sgm10.1080\/ 968776042000259555ALT-J, Research in Learning Technology0968 7769 (pri t)\/1741-1629 (onli e)Original Article2 04ssoci tion for Learning Techno ogy23 000Sept mber 20 4Mha r McAl i eScottish Qualifi atio s AgencyGlasgowMhairi.McAlpine@sqa.org.uk\nComputer assisted assessment is becoming more and more common through further and higher\neducation. There is some debate about how easy it will be to migrate current assessment practice to\na computer enhanced format and how items which are currently re-used for formative purposes may\nbe adapted to be presented online. This paper proposes an evaluatory framework to assess and\nenhance the practicability of large-scale CAA migration for existing items and assessments. The\nframework can also be used as a tool for exposing compromises between delivery mechanism and\nvalidity\u2014exposing the limits of validity of modified paper based assessments and highlighting the\ncrucial areas for transformative assessments.\nBackground\nAll holders of assessment materials are currently investigating what the impact of\nICT will be on their future practice. There is an acknowledgement that the tradi-\ntional manner of assessment will change, however as yet no clear vision of what will\nreplace it. Bennett (2001) outlined the major changes that assessment would\nundergo in response to the changing technology. Ripley (2003) has built on this idea\nto refine the three models of change which will dominate in the migration from\npaper-based to screen-based assessment. Figure 1 gives an illustrated summary of\nthe Ripley model.\nFigure 1. The Ripley modelThere is, however, an issue in how we get from a mass system of testing to an indi-\nvidualized assessment structure, quite apart from the changes that the introduction of\nICT will create. While the introduction of ICT is, for many, a time to rip up the rule\nbook and start again\u2014there is a need to take the practitioners along with the technol-\nogy, starting from where we are now and introducing change slowly and incrementally\n*Scottish Qualifications Authority, Glasgow, UK. Email: Mhairi.McAlpine@sqa.org.uk\n232 M. McAlpine\nF\nig\nur\ne \n1.\nT\nhe\n R\nip\nle\ny \nm\nod\nel\nAn evaluatory framework for CAA migration 233\nand ensuring that the cultural shift between paper-based and computer-assisted\nassessment is supported\u2014and above all that the confidence in assessment systems is\nretained.\nThis paper looks at how existing systems of assessment and collections of items can\nbe put online and how these can be evaluated to appreciate the difficulties and chal-\nlenges that this transition presents. By acknowledging where compromises have been\nmade and differences created, there is an awareness of the limitations of the technol-\nogy. This can expose where validity risks are being compromised for the sake of the\nassessment format. This proposed framework also can be used as a means to prioritize\ndevelopments and make explicit where the likely challenges in certain types of devel-\nopments may be. Used in this manner, it can be an important tool for implementers\nof large-scale CAA developments to manage change from paper-based to on-screen\ndelivery.\nMost UK developments are currently in the first or second stages of the transition\nprocess, looking at how they can adapt their current assessment practice to an on-\nscreen delivery format. Although this might seem a little un-ambitious, especially\nwhen compared with the more radical online assessment methodologies being devel-\noped, it is a necessary evolutionary step to engender confidence in computer delivery\nwithout too much of a radical change in the assessment format.\nThe Scottish Qualifications Authority is currently in the process of exploring the\npotential of CAA (McAlpine & Ware, 2003). It is anxious to avoid the fragmented\napproach that McKenna and Bull (2000) report has characterized the development\nof CAA in higher education in the UK and the resulting difficulty in achieving\nsustained systematic innovation across the education system. To that end it has\nactively sought partnership in its CAA activities with its stakeholders, and is involved\nin setting up the infrastructure which underpins CAA systems, and putting in place\nthe processes of change which will ease the transition for all involved. We work in\npartnership with our centres and the rest of the Scottish Educational community and\nare keen that we are aware of what we are expecting of them through this time of\ninnovation, and are doing all that we can to support them as they make this transition\nwith us.\nIntroduction\nSix subjects were chosen to form the medium of this pilot. These were English,\nMaths, a science (Chemistry), a humanity (History), an art (Music) and a modern\nforeign language (French). All were chosen from the external assessment component\nof the Higher. These are summative terminal assessments which are typically done by\nmore able students in their fifth year of High School in what is considered the main\ndeterminant of entry to Scottish Universities.\nThe question papers varied in a number of respects: the time allocated to the exam-\nination; the number of questions; the response that the questions demanded; the\nappearance of items, and the supporting material associated with the papers. It was\nconsidered that the major change issues with a move from paper based to on-screen\n234 M. McAlpine\nassessment would be the response type that an item in the paper expected, and the\ninclusion of any stimulus material which is currently given on paper.\nEach item in each of the question papers was considered\u2014looking at any stimulus\nassociated with the items, the type of response input that the item required, and the\ntype of marking required by the item.\nClassification of ease of migration\nIn order to facilitate decisions of which types of items should be considered suitable\nfor straightforward translation, which for modification and which should be consid-\nered from a transformative standpoint, A coding was established to identify which\ntypes of responses; input mechanisms and stimulus material were able to be directly\ntranslated into an computer based format. A classification scheme was developed to\nidentify the extent of the challenge (Table 1).\nAs can be seen from Table 1, classes one and two are most suitable for direct on-\nscreen migration, classes three and four were possible to implement with some\nconsideration, but may well be more suited to a modified form, while classes five and\nsix were not available for direct translation and may require the kind of third stage\ntransformative work.\nTable 1. Classification of ease of migration\n1 Currently widely available\nThere should be only trivial issues to resolve. Immediate implementation is feasible.\n2 Currently available, but requires refinement\nSome minor decisions may have to be taken about how exactly it is implemented. Immediate \nimplementation is possible, however small amounts of work, or consideration of issues may have to \nbe given to ensure long-term success.\n3 Currently available, but needs development for operational use\nSubstantial decisions may have to be made about the technology used or the manner in which it is \nimplemented. It may require investment to ensure that it is of the standard which we would require.\n4 Limited availability and requires development\nDecisions would have to be made about how it is developed and to what extent. Implementation \nwill not be possible until the development is complete. It will require some investment for \noperational use.\n5 Potential availability with commitment to development\nThis technology may well be at the beta stage or only available as a trial version. Substantial decision \nwould have to be made about how exactly it is implemented and in what form, it will require \ninvestment both to finalize the technology and to make it operationally available.\n6 Not currently available without significant commitment and investment\nThere is no reliable method of doing this at the present time. Experimental projects are at an early \nstage or have not reached satisfactory conclusions. In order to implement this operationally, \nsignificant resources would have to be deployed to ensure its success.\nAn evaluatory framework for CAA migration 235\nDescription and classification of items by attribute\nStimulus\nFrom the papers selected, five types of stimulus material were identified: diagram\/\ngraphs; photo\/drawing; quote; aural cue and formula. Each question was classified\naccording to which type of stimulus was associated with it, the most numerous minor-\nity (43.7%) of the items had no stimulus material associated with it, while only one\nquestion had more than one associated stimulus (Table 2).\nEach stimulus code was taken in turn to consider how difficult it would be to\nmigrate that type of stimulus to a computer format. Table 3 details this classification\ntogether with some analysis of how it was reached. Issues which were considered\nincluded how difficult it would be to present through computer, how difficult it would\nbe to access it, how candidates with special needs might be affected by this method\nof delivery, any special pieces of software which might be required to enable this, what\nthe \u2018industry\u2019 tended to use for the delivery of this type of material, and alternative\nways that it might be presented, including some evaluation of these methods.1 To\nconstruct this classification, a number of approaches were used, including a literature\nreview, consideration of software known to the author and consultations with those\ninvolved with practical projects involving CAA. It does not claim however to be a\ndefinitive account of all available technologies.\nResponse type\nFrom the papers selected, six types of response type were identified: numeric\nresponses; algebraic responses; lexical responses; diagrammatic responses; closed\nresponses and selected responses. Each question was classified according to which\ntype of response was expected from it. The most numerous grouping of responses\nwere lexical responses (class 3), accounting for 59.6% of the responses required.\nCategories two and three were further divided by the length of response expected, as\nin these response types that is a significant factor influencing the ability of the\ncomputer to automatically mark the item (Table 4).\nAs with stimulus, each response type was taken in turn to consider how difficult\nit would be to migrate that type to a computer format. Two issues were consid-\nered, how difficult the response type was for a candidate to enter an answer into\nTable 2. Count of stimulus by code\nStimulus code 0 1 2 3 4 5 Total\nCounta 131 41 6 84 31 8 300\n% of items 43.7 13.7 2.0 28.0 10.3 2.7 100\n% of items with stimulus - 24.2 3.6 49.7 18.3 4.7 100\naNote that one question had both a Photo and Quote stimulus.\n236 M. McAlpine\nTable 3. Description and classification of stimulus types\nCode and description Stimulus classification\nCode 1: Diagram\/Graph\nItems with images where the information \nin the stimulus was essential to the \nanswering of the question\u2014thus an \naccurate reproduction of the image would \nneed to be rendered on computer in order \nnot to disadvantage candidates.\nClass 2\nThere are a number of standard image formats \navailable that diagrams and graphs can be rendered \nin. Most CAA engines and VLEs will accept these \nforms, however the display mechanism may cause \nsubtle variations, which may affect question quality.\nFurthermore the capabilities of the machines which \nthe candidates attempt the question may affect the \nrendering.\nCode 2: Photo\/Drawing\nItems with images where the information \nin the stimulus was impressionistic on the \nanswering of the question. Thus so long as \nthe image was visible and retained its \nmeaning it would be an acceptable \nrendering.\nClass 1\nThere are a number of standard image formats \navailable that photos and drawings can be rendered \nin. Most CAA engines and VLEs will accept these \nforms.\nAgain the capabilities of the machines which the \ncandidates attempt the question on may affect the \nrendering\u2014and although this might not be so critical \nas in the above example, it may bias results towards \nbetter resourced centres and candidates.\nCode 3: Quote\nItems with text stimulus of a few words to \na sentence or two. As with the above this \ncould be rendered as an image, however it \nis assumed that the underlying coding style \nis textual.\nClass 1\nMost CAA engines and VLEs will accept textual \nstimulus material.\nCode 4: Aural Cue\nItems which required candidates to listen \nto something before responding.\nClass 3\nThere are a number of rendering mechanisms for \naural stimulus material, however the implementation \nof this into live examinations is surrounded by \ntechnical and practical issues which would have to \nbe resolved prior to live use.\nSome of these issues include \n\u25cf the sound quality which, as with the image \nrendering, may be affected by the specification \nof the machine on which the candidate is being \nexamined;\n\u25cf the candidates control over the music playing \u2013 \ncan they play it themselves, or would the \ncomputer play it for them in the manner that \nthe invigilator currently does;\nAn evaluatory framework for CAA migration 237\nthe computer, and how difficult it was to enable automatic marking of that type of\nquestion. Table 4 details these classifications together with some analysis of how\nthey were reached. Issues which were considered for input purposes included\nspecial characters, free-input, specialist notation and some consideration of acces-\nsibility issues. Issues which were considered for marking purposes included the\navailability of technology that could enable computer-based marking of these types\nof questions, any minor changes that could be made to the questions to make\nthem easier to mark on computer, and the reliability of the marking. As with stim-\nulus, it does not claim to be a definitive account of all available technologies, but\nbased on existing practice, known issues, currently available software and\npublished literature.\nVisualizing the papers\nUsing the stimulus class and the highest of the marking class and input class, a view\nof how easy each of the papers would be to migrate to CAA were established. A\nshort consultation exercise in which method of visualisation could most easily\ncommunicate the essential information indicated that bubble graphs were favoured\nover the two alternative methods put forward (stacked area graphs and luminosity\nsquares).\nTable 3. Continued\nCode and description Stimulus classification\n\u25cf how candidates might have access to the aural \ncues at different times without disturbing other \ncandidates in the room;\n\u25cf may a computer be able to get round some of \nthe problems that candidates with SEN may \nhave in accessing certain part of the \nexamination? (e.g. through increased \namplification etc.).\nCode 8: Formula\nThis code was used to demarcate stimulus \nwhich was presented in a standard subject \nspecific form (in this case using \nmathematical notation and chemical \nnotation). These could, in theory at least, \nbe presented as an image and fall into code \n1, however essential information would be \nlost which should be retained to maximize \nthe usage of the question (not least in \nquestion generation).\nClass 5\nThere are a number of ways that chemical and \nmathematical formulae can be represented on \ncomputer. These include LaTeX; MathML and \nChemML as well as a variety of plug-ins. None of \nthese, (except LaTeX, which is an imperfect partial \nsolution), can be adequately rendered on the \nmajority of CAA engines or VLEs\u2014this would cause \na significant problem should the meaning behind the \nformulae have to be retained.\n238 M. McAlpine\nTable 4. Description and classification of response types\nCode and \ndescriptions Input classification Marking classification\nCode 1: Numeric\nItems where a \nnumerical answer \nwas required.\nClass 1\nIn the case of most numbers, input is \nfairly straightforward and can be \ndone using standard notation on a \nstandard keyboard\u2014additional \ncharacters which would be required \nin addition to digits would be \u2018-\u2019 \n(negative numbers) \u2018\/\u2019 (fractions), \u2018.\u2019 \n(decimals), \u2018i\u2019 (imaginary and \ncomplex numbers), \u2018e\u2019 (2.11) and \u2018\u03c0\u2019 \n(3.14). The only one which causes \nsignificant issues and is not found on \nstandard keyboard is \u2018\u03c0\u2019. Most CAA \nengines accept numeric data, input \nissues should be minimal.\nClass 2\nThere are a variety of ways that \nnumerical questions may be marked. \nSometimes a precise answer is \nrequired\n(e.g. what is 1\/2 expressed as a decimal?\nANS = 0.5 only)\nand other times more than one \nrepresentation may be acceptable\n(e.g. If 8 cakes are shared among 10 \npeople how many do each get?\nANS = 0.8 or 4\/5 or \n8\/10)\nThus there would have to be the \nfacilities available for the evaluation \nof the answer and an understanding \nof numerically equivalent forms as \nwell as the facility to limit the \nacceptance of equivalent forms in \ncertain cases.\nThere are a number of CAA systems \nwhich have both of these capabilities \nand although tweaking them to the \nprecise requirements may require \nsome work, this should not be a \nsignificant limiting factor.\nCode 2: Algebraic\nItems where an \nalgebraic answer \n(i.e. one including \nunknowns, \ntypically \nrepresented by \nletters) was \nrequired.\nFor marking \npurposes it is \nseparated into \nsections 2a which \nrequires only one \nline of input (the \nanswer expected \nClass 3\nWhere unknowns are represented by \nletters as is common\u2014this should \nnot pose a problem as they are found \non a standard keyboard. At lower \nlevels unknowns may be represented \nin other forms (e.g. stars or question \nmarks) which may prove more \nchallenging.\nAlgebraic answers can quickly \nbecome complex and may require \nthe whole range of algebraic notation \navailable. This might include (but \nnot limited to) complicated \nfractions, integrals, logs and powers. \nThere are a number of ways of \n2a\u2014Simple Algebraic: Class 3\nAs with numeric answers, algebraic \nanswers may have a number of \nacceptable equivalent representations \nhowever the question may limit the \nnumber of acceptable forms. \nAccepting too many equivalents may \ncompromise the question, especially \nwhere the question is designed to test \ntheir ability to manipulate algebraic \nexpressions. Thus, as with the \nnumeric questions, there would have \nto be the facilities available for \nevaluation of the answer and an \nunderstanding of numerically \nequivalent forms as well as the facility \nAn evaluatory framework for CAA migration 239\nTable 4. Continued\nCode and \ndescriptions Input classification Marking classification\nwould be a \nformula) and 2b \nthose which \nwould require \nmore than one \nline of input (the \nexpected answer \nwould be a proof).\ninputting these as well as standard \ntext input, such as selection from a \nmenu (as in MS Word Equation \nEditor); special codes (as in LaTeX); \nor typing in characters in an \nappropriate order. Many CAA \nengines are able to accept algebraic \ndata, however the quality of their \ninput mechanisms vary (especially \nfor more complex expressions) and \ninput issues may challenge validity if \nthey are not adequately considered.\nto limit the acceptance of equivalent \nforms in certain cases.\nWhere input issues were affecting the \nquality of the answers, there would \nhave to be recognition of common \nerrors which had been caused by \ninput difficulties. This would best be \nrecognized at the input stage \u2013 thus it \nmight require an intermediate \nevaluation of the answer given \nchecking for common input errors \n(e.g. x2 computer asks if that is x2; 2x \nor \u2018times 2\u2019).\nClass 2b\u2014Proof: Class 4\nThere are a variety of packages \navailable which allow for input of \nalgebraic expressions longer than one \nline, in some cases however this \nwould have to be coded as separate \nanswers. The CUE system in use with \nthe Pass-IT trials, allows supporting \n\u2018steps\u2019 to be accessed and used when \ncandidates request them (this would \nbe of particular relevance in the case \nof codes 1a and 2a where the answer \nitself is of a different form, however \nthe proof may allow access to the \npartial credit available)\u2014although \nit could be effectively insisted \nupon.\nThere would be a number of \ndifficulties associated with proofs, \nparticularly as there may well be no \none correct proof, but a variety of \nanswers which may legitimately gain \nthe available marks. McCabe (2001) \nhas suggested the objectification of \nproof questions to assess this type of \nlearning and suggests a variety of \nways which various CAA engines \nhave approached this. All in all, this \nwould be a problematic area and one \nwhich would require further \nconsideration.\n240 M. McAlpine\nTable 4. Continued\nCode and \ndescriptions Input classification Marking classification\nCode 3 Textual \nResponse\nItems where the \nresponse expected \nwas textual. This \nmay also include \nnon-lexical \nanswers (such as a \ntelephone number \nor date).\nThe code is \ndivided up into \nfive subsections \ndependant on the \nlength of response \nexpected for \nanalysing the \npotential for \ncomputer \nenhanced \nmarking.\nCode 2a examines \nsingle word \nresponses (which \nmay on occasions \ncover a short \nphrase or non-\nlexical responses \nsuch as a date), \ncode 2b covers \nshort responses, \nranging from a \nfew words to a \nsentence; code 2c \ncovers expected \nresponses ranging \nfrom a sentence to \na paragraph. Code \n2d, codes \nextended \nresponses where \nbetween one and \nthree would be \nexpected and \ncode 3e covers\nClass 1\nThe input mechanisms for this \nresponse type would be fairly \nstraightforward\u2014involving the \nstandard characters on the \nkeyboard\u2014although this may \ninclude numbers and other \ncharacters (such as \u2018\u00a3\u2019; \u2018\/\u2019 etc.). This \nwould be accepted by all CAA \nengines\u2014although any unusual \ncharacters which were likely to be \nused in an assessment would have to \nbe flagged and considered.\nAs the length of response demanded \ngrows it would have to be considered \nwhether there were sufficient \nelements in place in the case of a \nsystems failure and whether there \ncould be checks built into the system \nto ensure that no input was lost.\nCode 2a: Single Word: Class 2\nAlmost all standard CAA packages \nmark single word responses. \nProblems may well occur, especially \nwith less able candidates where \nspelling is poor, compromising the \ncomputer\u2019s ability to recognize the \nanswer, or where there are a number \nof synonyms which would be equally \nacceptable. These can be \ncircumvented by entering a variety of \nalternative answers, including \ncommon spelling mistakes, which \nshould also be marked as correct. \nAlternatively, for spelling errors, a \nformulaic interpretation can catch \nunusual errors, however this must be \nmonitored carefully to ensure that the \nnet is not being cast too wide.\nChanging the format of the question \nmay be an option worth consideration \nin some cases\u2014there are questions \nwould lend themselves to \nobjectification (perhaps through pull-\ndown menus or drag and drop). Real-\ntime spell checks may also assist \ncandidates enter a response which was \nrecognisable to the computer, with \nmis-spelt words being highlighted to \nthe candidate for revision \u2013 suggesting \nalternatives may however \ncompromise the validity of the test.\nUltimately it should be fairly \nstraightforward to computer mark \nsingle word response, however there \nmay need to be an element of human \nmarking back up, or question re-\ndesign to ensure the reliability of the \nsystem. These types of items could \nprobably be migrated with minimal \ndifficulty.\nCode 3b: Short Response: Class 3\nThese responses tend to be relatively \nfactually based\u2014where the mark key\nAn evaluatory framework for CAA migration 241\nTable 4. Continued\nCode and \ndescriptions Input classification Marking classification\nessay responses \nwhere a response \nover two \nparagraphs would \nbe demanded.\nis determined very much by the \ncontent of the response, rather than \nby its construction or style. These \nshould thus not present too much of a \nchallenge to most CAA systems \nalthough the methods of obtaining a \nreliable marking schema which can be \nentirely computer driven may be \nlaborious and time consuming. For \nsmall entry subjects, the process of \ncreating the algorithms for computer \nmarking may negate the benefits of \non-line marking unless progress is \nmade in this area. The technology to \nmake this possible is certainly \navailable, however some advances \nwould help to make it a desirable \ninnovation.\nCode 3c\u2014Short Answer: Class 3\nMost CAA packages accept short \nanswer responses, however the \naccuracy of the marking varies in its \nreliability. Michell et al. (2003) have \nsuggested that there are systems \navailable which after human \nmoderation can mark at 99.4% \naccuracy overall. In a trial all items \nwere marked at over 93% accuracy \nand with 98.1% of items over 95% \naccuracy. For more problematic \nitems these could be redesigned to \nease marking. They would still \nrequire a level of human moderation \n(figs above are post moderation) but \nthis is a hopeful development. As with \nthe above there is some issue with the \ntime which may be needed to create \nand moderate the marking scheme, \nhowever as these question-types are \nmore difficult to mark by human \nmarkers, this may not be such an \nissue in this instance.\nThere are a few issues still to be \nresolved with these items however it \nlooks as if reliable marking of short \nresponse questions may appear soon.\n242 M. McAlpine\nTable 4. Continued\nCode and \ndescriptions Input classification Marking classification\nCode 3d\u2014Extended Response: \nClass 4\nThis type of response would be best \nmarked in a manner similar to that \ndescribed below - and would have \nsimilar problems and challenges \nassociated with it\u2014although it might \nbe imagined that the problems \nassociated with essays would be \nreduced as the size of the material was \nreduced, this may not be the case and \nfurther investigation into the \ntechnologies available would have to \nbe performed.\nCode 3e: Essay\u2014Class 4\nThere are packages on the market \nwhich are designed to automatically \nmark essay responses. The most \nwidely known and used is the e-rater \nsystem from Education Testing \nServices (ETS). These have problems \nassociated with them, and it is not \nclear whether they would be accepted \nby markers and teachers. The \ndevelopments in this area tend to \ncome from the US, and are heavily \ninfluenced by US assessment \npractices, which may create \nchallenges when migrating the \ntechnology to a Scottish context.\nCode 9: Diagram\nItems which \nrequire the \ncandidate to draw \nsomething which \nis then evaluated.\nClass 5\nThis would be a difficult one to \nimplement without significantly \nchanging the question or providing \nvery specialist hardware, although \nquestions did vary in their input \ncomputerisation difficulty. Much \nthough would have to be given to \nwhat the question was actually \ndesigned to assess, and how this \ncould be put onto computer without \ncompromising the nature of the \nassessment. Some methods of input \nare already available\u2014such as the \nClass 5\nMuch of the possibility of CAM \nwould be determined by the input \nmechanism used. Where specific \ntools were used, there might be less \ndifficulty in establishing a marking \nmechanism, however where a more \ngeneric input mechanism was used \nthis may prove more challenging.\nThere are already CAA packages \nwhich do allow some element of \nautomatic marking of questions \nrequiring a diagrammatic response, \nhowever most significantly change the\nAn evaluatory framework for CAA migration 243\nTable 4. Continued\nCode and \ndescriptions Input classification Marking classification\ndemand of the questions in doing so. \nThe pass-IT trials included one such \nquestions, however it is unclear \nwhether the question was indeed \nequivalent to the paper based form or \nan alternative way of assessing the \nsame skills.\nA full review and evaluation of the \narea would have to be undertaken.\nCode 10: Cloze \nResponse\nItems where there \nis a body of \nmaterial with \nmissing pieces \ntogether with a \nchoice of pieces to \ncomplete the \nmaterial. \nCandidates are \nasked to insert \nthese pieces at the \nappropriate \npoints.\nClass 2\nAlthough the fundamental \ncomputerisation of a cloze response \nis quite unproblematic, there are a \nnumber of minor issues. The format \nof answering may be changed on a \ncomputer\u2014including say drag and \ndrop or scrolling and there may also \nbe other methods of input. This may \nindeed add to the questions \nreliability by slightly altering the \ninput mechanism to get rid of \nexternalities such as spelling ability.\nClass 1\nAlthough there will be trivial issues \naround spelling (where candidates are \nexpected to type their response in)\u2014\nthese marking issues suffer from the \nsame kind of difficulties as single \nword response items (code 3a). Most \nother marking issues will be similar to \nthose with paper based clozes.\nCode 11: Selected \nResponse\nItems which \nrequired \ncandidates to \nmake a choice out \nof a number of \npossible given \nanswers.\nClass 1\nInput for these types of questions \nshould not pose any particular \nproblem and could be implemented \nin a number of ways. Indeed \ncomputerisation of selected response \nitems can add to the validity in a \nnumber of cases by changing the \ninput mechanism (e.g. to a hotspot) \nrather than the traditional A\/B\/C\/D \nresponse.\nClass 1\nTechniques for marking selected \nresponse items on computer are well \nestablished. This should pose no \nparticular problems. There may be \nissues where these items migrate from \nthere traditional form to newer less \nwell tested input mechanisms (e.g. \nhotspots) however these can be \navoided until confidence in their \nreliability can be ensured.\nTable 5. Count of response type by code\nResponse type 1 2a 2b 3a 3b 3d 3e 3f 4 5 6 Total\nCounta 8 3 36 11 45 25 38 71 13 3 48 300\n% of items 2.7 1.0 12.0 3.7 15.0 8.3 12.7 23.7 4.3 1.0 16.0 100\na Note that one question was classified as both 1and 4, one as both 3 and 5 and one as both 5 and 9.\n244 M. McAlpine\n1. Papers suitable for direct migration\nChemistry Paper 1\nOnly chemistry paper 1 was really suitable for\ndirect migration. This was a multiple choice\npaper and, as can be seen from the graph\nabove, very few of the questions pose any\ndifficulty at all in migration to a computer\nbased format. This examination could\nmigrate practically instantly with very little\nmodification.\n2. Papers which may be suitable for modification\nMusic Paper 2\nThis paper has some challenges surrounding\nthe stimulus material that it uses and indeed the\nstimulus is the biggest problem. The response\ntypes used for the majority of questions do not\npose significant issues and the difficulties inher-\nent in some of them could be circumvented.\nEnglish Paper 1\nIn this English paper, the response type is caus-\ning difficulties in migration although there are\nno stimulus issues. The questions are all fairly\nsimilar, suggesting that technical developments\nare needed to overcome the difficulties faced.\nChanging the response type may challenge\nvalidity unless carefully studied.\nFrench Paper 1\nThis paper has some technical challenges asso-\nciated with the response type used, although\nthese are not insurmountable. Given the unifor-\nmity of the response types, hastening migration\nthrough adaptation may pose challenges to the\nvalidity of the assessment.\nAn evaluatory framework for CAA migration 245\nFrench Paper 2\nThere are some issues to be overcome in this\npaper both through the stimulus used and the\nresponse types used, although again these are\nnot insurmountable. Whether these can be\ndone whilst maintaining the quality of the\nassessment needs to be viewed in a wider\ncontext.\n3. Papers where some of the paper may have to be rethought for CAA\nChemistry Paper 2\nThe variety of difficulty levels and technical\ndifficulties which need to be overcome, mean\nthat this might not be a suitable paper for\nmigration until a significant number of the\ntechnical challenges have been overcome.\nShould early migration be seen as desirable,\ncompromises and adaptations to the response\ntypes used may be necessary. Migration of\nsome of the items may challenge validity.\nMaths Paper 1\nThis paper poses significant challenges to\nmigrate. There may be some room to adapt the\nresponse types in particular and certain parts of\nstimulus could be presented in a more migrate-\nable format, however there are a number of\nitems which would have to change significantly\nin order to be computerized.\n4. Papers that may require a transformative approach\nEnglish Paper 2\nAs with the above, the response type used in\nthis paper is not conducive to migration. Tech-\nnical developments are needed to enable migra-\ntion. This may be an area in which the efforts\ninvolved in paper to screen migration may not\nbe worth the transitional results.\n246 M. McAlpine\nMaths Paper 2\nThis paper also has significant challenges to\nmigration. There are few issues with the\nstimulus required, however the response types\nused are not conducive to migration. The\nresponse types used would have to be recon-\nsidered should migration be desired in the\nshort term.\nHistory Paper 1\nAs with the English papers this history paper\nhas significant problems associated with the\nresponse types used. Using more migrateable\nresponse types would significantly alter the\ncharacter of the exam.\nHistory Paper 2\nAs with paper 1, considerable technical devel-\nopments are needed to enable these types of\nitems, and adaptation and changing response\ntypes would significantly alter the character of\nthe exam.\nConclusions\nThis methodology suggests a mechanism whereby papers and subjects which are\nbeing considered for migration to CAA can be compared in their suitability for online\ndelivery taking into account the wide variations in response types and stimuli which\nare found across papers and subjects. Furthermore it also gives an indication to what\nextent the migration to computer-assisted formats for assessment may pose chal-\nlenges to both reliability and validity, and at the same time open up opportunities for\nre-thinking the methods of assessment in those areas. The subjects which cannot\neasily be migrated to a CAA format are perhaps the ones most amenable to the type\nof transformative assessment talked about by Ripley and Bennett, and the most prom-\nising for emerging technologies \u2013 however it must be considered to what extent this\nAn evaluatory framework for CAA migration 247\nis indicative of SQA\u2019s assessment practices and how can be allocated to the curricular\nareas themselves.\nWhile the conclusions that can be drawn from only six subjects, at one level are\nlimited, the classification framework is now in place to rapidly construct comparable\nindicators for other subjects and other levels. This will give us an indication of what\nissues need to be tackled in each subject, how significant a problem they are and (for\ninstitutions which hold large numbers of paper-based items which they would wish to\nmove to an online format) suggest an order in which migration can commence.\nOne of the weaknesses of this project was that there was insufficient review of the\nemerging item marking and display technologies. It was outside the scope of this\nstudy to perform the type of comprehensive review which would be required and the\nclassifications are given to the items should be interpreted with that caveat. A compre-\nhensive study of emerging CAA technologies is long overdue and would greatly\ninform the sector, not least by ensuring that anyone using this methodology for\nmigrating paper based items to a CAA format has a robust system in place by ensuring\nthat the classification was as accurate as possible.\nNotes\n1. It should be noted that these codings are given on the basis of current knowledge of the author\nand are not based on any external categorisation. This may lead to inaccuracies in classification\nwhere technology has progressed beyond the author\u2019s awareness. Where an accurate classifica-\ntion is required it is recommended that a thorough review is undertaken and that further\nprogress and development in the area is monitored.\nReferences\nAshton, Schofield & Woodger (2003) Piloting summative web assessment in secondary education,\n7th International Computer Assisted Assessment Conference, Loughborough, 8\u20139 July.\nRipley (2003) The e-assessment tail: new opportunities? New challenges? The uses of ICT in\nassessment, Beyond the Exam Conference, Bristol, 19 November.\nRipley (2004) Assessment using the QCA \u2018Rules Base\u2019 with process-based data, Advisory Council for\nNew Technologies in Education, Cambridge, 10 February.\nBennett, Jenkins, Persky & Weiss (2004) Assessing complex problem-solving performances: the NAEP\nproblem solving in technology-rich environments project, Advisory Council for New Technologies\nin Education, Cambridge, 10 February.\nBennett (2001) How the internet will help large-scale assessment reinvent itself, Education Policy\nAnalysis Archives, 9(5).\nNisbet (2002) Special arrangements for using ICT in SQA assessments. Project report to Scottish\nQualifications Authority. CALL Centre, University of Edinburgh.\nNisbet (2003) Accessibility of SQA assessments in Portable Document Format. Project report to\nScottish Qualifications Authority. CALL Centre, University of Edinburgh\nMcAlpine & Ware (2003) Introducing computer assisted assessment in Scotland: laying the foun-\ndations for an integrated approach, 29th International Association for Educational Assessment\nConference, Manchester, 5\u201310 October.\nMcCabe (2001) Computer assisted assessment (CAA) of proof = proof of CAA\u2014new approaches\nto computer assessment for higher level learning, 6th International Conference on Technology in\nMathematics Teaching, ICTMT-6, Klagenfurt, Austria.\n248 M. McAlpine\nMcKenna & Bull (2000) Quality assurance of computer assisted assessment: practical and strategic\nissues, Quality Assurance in Education, 8(1), 24\u201331.\nMitchell, Aldridge, Williamson & Broomhead (2003) Computer based testing of medical knowl-\nedge, 7th International Computer Assisted Assessment Conference, Loughborough, 8\u20139 July.\n"}