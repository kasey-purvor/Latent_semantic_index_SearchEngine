{"doi":"10.1117\/12.646644","coreId":"66638","oai":"oai:dro.dur.ac.uk.OAI2:648","identifiers":["oai:dro.dur.ac.uk.OAI2:648","10.1117\/12.646644"],"title":"Cosmic cookery : making a stereoscopic 3D animated movie.","authors":["Holliman,  N.","Baugh,  C.","Frenk,  C.","Jenkins,  A. R.","Froner,  B.","Hassaine,  D.","Helly,  J.","Metcalfe,  N.","Okamoto,  T."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":["Woods, Andrew J.","Dodgson, Neil A.","Merritt, John O.","Bolas, Mark T.","McDowall, Ian E."],"datePublished":"2006-02-02","abstract":"This paper describes our experience making a short stereoscopic movie visualizing the development of structure in\\ud\nthe universe during the 13.7 billion years from the Big Bang to the present day. Aimed at a general audience for\\ud\nthe Royal Society's 2005 Summer Science Exhibition, the movie illustrates how the latest cosmological theories\\ud\nbased on dark matter and dark energy are capable of producing structures as complex as spiral galaxies and\\ud\nallows the viewer to directly compare observations from the real universe with theoretical results. 3D is an\\ud\ninherent feature of the cosmology data sets and stereoscopic visualization provides a natural way to present the\\ud\nimages to the viewer, in addition to allowing researchers to visualize these vast, complex data sets.\\ud\nThe presentation of the movie used passive, linearly polarized projection onto a 2m wide screen but it was\\ud\nalso required to playback on a Sharp RD3D display and in anaglyph projection at venues without dedicated\\ud\nstereoscopic display equipment. Additionally lenticular prints were made from key images in the movie. We\\ud\ndiscuss the following technical challenges during the stereoscopic production process; 1) Controlling the depth\\ud\npresentation, 2) Editing the stereoscopic sequences, 3) Generating compressed movies in display speci\u00afc formats.\\ud\nWe conclude that the generation of high quality stereoscopic movie content using desktop tools and equipment\\ud\nis feasible. This does require careful quality control and manual intervention but we believe these overheads\\ud\nare worthwhile when presenting inherently 3D data as the result is signi\u00afcantly increased impact and better\\ud\nunderstanding of complex 3D scenes","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/66638.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/648\/1\/648.pdf","pdfHashValue":"2026185665b1edf0ae594147fbcb4691e3a21767","publisher":"SPIE","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:648<\/identifier><datestamp>\n      2017-03-10T15:37:01Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Cosmic cookery : making a stereoscopic 3D animated movie.<\/dc:title><dc:creator>\n        Holliman,  N.<\/dc:creator><dc:creator>\n        Baugh,  C.<\/dc:creator><dc:creator>\n        Frenk,  C.<\/dc:creator><dc:creator>\n        Jenkins,  A. R.<\/dc:creator><dc:creator>\n        Froner,  B.<\/dc:creator><dc:creator>\n        Hassaine,  D.<\/dc:creator><dc:creator>\n        Helly,  J.<\/dc:creator><dc:creator>\n        Metcalfe,  N.<\/dc:creator><dc:creator>\n        Okamoto,  T.<\/dc:creator><dc:description>\n        This paper describes our experience making a short stereoscopic movie visualizing the development of structure in\\ud\nthe universe during the 13.7 billion years from the Big Bang to the present day. Aimed at a general audience for\\ud\nthe Royal Society's 2005 Summer Science Exhibition, the movie illustrates how the latest cosmological theories\\ud\nbased on dark matter and dark energy are capable of producing structures as complex as spiral galaxies and\\ud\nallows the viewer to directly compare observations from the real universe with theoretical results. 3D is an\\ud\ninherent feature of the cosmology data sets and stereoscopic visualization provides a natural way to present the\\ud\nimages to the viewer, in addition to allowing researchers to visualize these vast, complex data sets.\\ud\nThe presentation of the movie used passive, linearly polarized projection onto a 2m wide screen but it was\\ud\nalso required to playback on a Sharp RD3D display and in anaglyph projection at venues without dedicated\\ud\nstereoscopic display equipment. Additionally lenticular prints were made from key images in the movie. We\\ud\ndiscuss the following technical challenges during the stereoscopic production process; 1) Controlling the depth\\ud\npresentation, 2) Editing the stereoscopic sequences, 3) Generating compressed movies in display speci\u00afc formats.\\ud\nWe conclude that the generation of high quality stereoscopic movie content using desktop tools and equipment\\ud\nis feasible. This does require careful quality control and manual intervention but we believe these overheads\\ud\nare worthwhile when presenting inherently 3D data as the result is signi\u00afcantly increased impact and better\\ud\nunderstanding of complex 3D scenes.<\/dc:description><dc:subject>\n        Stereoscopic animation<\/dc:subject><dc:subject>\n         Scientific visualization<\/dc:subject><dc:subject>\n         Cosmology<\/dc:subject><dc:subject>\n         Astronomy<\/dc:subject><dc:subject>\n         3D video editing<\/dc:subject><dc:subject>\n         3D video coding<\/dc:subject><dc:subject>\n         3D display<\/dc:subject><dc:subject>\n         Human factors<\/dc:subject><dc:subject>\n         Rendering.<\/dc:subject><dc:publisher>\n        SPIE<\/dc:publisher><dc:source>\n        Woods, Andrew J. & Dodgson, Neil A. & Merritt, John O. & Bolas, Mark T. & McDowall, Ian E. (Eds.). (2006). Stereoscopic displays and virtual reality systems XIII : 16-19 January, 2006, San Jose, California, USA. Bellingham, WA: SPIE, pp. 605505, Proceedings of SPIE(6055)<\/dc:source><dc:contributor>\n        Woods, Andrew J.<\/dc:contributor><dc:contributor>\n        Dodgson, Neil A.<\/dc:contributor><dc:contributor>\n        Merritt, John O.<\/dc:contributor><dc:contributor>\n        Bolas, Mark T.<\/dc:contributor><dc:contributor>\n        McDowall, Ian E.<\/dc:contributor><dc:date>\n        2006-02-02<\/dc:date><dc:type>\n        Book chapter<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:648<\/dc:identifier><dc:identifier>\n        issn:0277-786X<\/dc:identifier><dc:identifier>\n        doi:10.1117\/12.646644<\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/648\/<\/dc:identifier><dc:identifier>\n        https:\/\/doi.org\/10.1117\/12.646644<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/648\/1\/648.pdf<\/dc:identifier><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["issn:0277-786X","0277-786x"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2006,"topics":["Stereoscopic animation","Scientific visualization","Cosmology","Astronomy","3D video editing","3D video coding","3D display","Human factors","Rendering."],"subject":["Book chapter","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n21 August 2009\nVersion of attached file:\nPublished Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nHolliman, N. and Baugh, C. and Frenk, C. and Jenkins, A. and Froner, B. and Hassaine, D. and Helly, J. and\nMetcalfe, N. and Okamoto, T. (2006) \u2019Cosmic cookery : making a stereoscopic 3D animated movie.\u2019, in\nProceedings of SPIE : Stereoscopic displays and virtual reality systems XIII. Bellingham, WA: SPIE, p.\n605505.\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1117\/12.646644\nPublisher\u2019s copyright statement:\nAdditional information:\nN. Holliman, C. Baugh, C. Frenk, A. Jenkins, B. Froner, D. Hassaine, J. Helly, N. Metcalfe, T. Okamoto, \u2019Cosmic\ncookery : making a stereoscopic 3D animated movie., Proceedings of SPIE : Stereoscopic displays and virtual reality\nsystems XIII, A. J. Woods, N. A. Dodgson, J. O. Merritt, M. T. Bolas, I. E. McDowell, Editors, 6055, 605505, (2006).\nCopyright 2006 Society of Photo-Optical Instrumentation Engineers. One print or electronic copy may be made for\npersonal use only. Systematic reproduction and distribution, duplication of any material in this paper for a fee or for\ncommercial purposes, or modification of the content of the paper are prohibited.\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\nCosmic Cookery: Making a Stereoscopic 3D Animated Movie\nNick Hollimana, Carlton Baughb, Carlos Frenkb, Adrian Jenkinsb, Barbara Fronera, Djamel\nHassainea, John Hellyb, Nigel Metcalfeb, Takashi Okamotob\naDepartment of Computer Science; bDepartment of Physics,\nUniversity of Durham, United Kingdom\nABSTRACT\nThis paper describes our experience making a short stereoscopic movie visualizing the development of structure in\nthe universe during the 13.7 billion years from the Big Bang to the present day. Aimed at a general audience for\nthe Royal Society\u2019s 2005 Summer Science Exhibition, the movie illustrates how the latest cosmological theories\nbased on dark matter and dark energy are capable of producing structures as complex as spiral galaxies and\nallows the viewer to directly compare observations from the real universe with theoretical results. 3D is an\ninherent feature of the cosmology data sets and stereoscopic visualization provides a natural way to present the\nimages to the viewer, in addition to allowing researchers to visualize these vast, complex data sets.\nThe presentation of the movie used passive, linearly polarized projection onto a 2m wide screen but it was\nalso required to playback on a Sharp RD3D display and in anaglyph projection at venues without dedicated\nstereoscopic display equipment. Additionally lenticular prints were made from key images in the movie. We\ndiscuss the following technical challenges during the stereoscopic production process; 1) Controlling the depth\npresentation, 2) Editing the stereoscopic sequences, 3) Generating compressed movies in display specific formats.\nWe conclude that the generation of high quality stereoscopic movie content using desktop tools and equipment\nis feasible. This does require careful quality control and manual intervention but we believe these overheads\nare worthwhile when presenting inherently 3D data as the result is significantly increased impact and better\nunderstanding of complex 3D scenes.\nKeywords: Stereoscopic Animation, Scientific Visualization, Cosmology, Astronomy, 3D Video Editing, 3D\nVideo Coding, 3D Display, Human Factors, Rendering\n1. INTRODUCTION\nOur understanding of the development of structure in the universe is increasing through a combination of\nimproved observations, theoretical modeling and simulation. These methods produce huge data sets that on\ntheir own are difficult to present to general audiences. The aim of producing the Cosmic Cookery movie was to\npresent this information to a general audience in a comprehensible way using advanced 3D visualization.\nThe requirements for the Cosmic Cookery exhibit were to create stereoscopic video and multiscopic images for\nlarge scale stereoscopic projection, medium sized multiscopic lenticular prints and small scale auto-stereoscopic\ndisplay. This became a significant test of the level of support for stereoscopic image creation in rendering software,\nediting tools and compression standards.\nAs we have recently produced new stereo image rendering tools based on novel algorithms first reported at\nthis conference1\u20133 we were interested to compare these with traditional approaches to stereo image generation\nusing verging and parallel cameras. The wide range of rendering software we needed required all these methods\nto be adopted and enabled a practical comparison of the different approaches.\nIn the following section we introduce technical background regarding the target displays and the stereoscopic\ncamera control methods adopted. Details relating to the cosmological simulations and astronomical observations\nwe are visualizing are presented when we describe the production of the individual sections of the movie.\nFurther author information: Send correspondence to Nick Holliman.\nEmail: n.s.holliman@durham.ac.uk, Telephone: +44 191 334 4287, Web: http:\/\/www.durham.ac.uk\/n.s.holliman\/\n2. BACKGROUND\n2.1. Displays\nThe choice of 3D display technologies was driven in part by availability of the displays but also by availability\nof the software tools to render, edit and distribute the visualizations in the required formats. While large-scale\nauto-multiscopic displays would have been an attractive option for public presentation their current scarcity and\nlack of software support ruled this option out.\nThe primary target display for the movie was an Inition Duality S2 system, a portable 2m wide stereo-\nscopic projection screen using twin projectors and linear polarization. This was driven at a stereo resolution of\n1024x768x2 from standard PC hardware using a dual channel nVidia Quadro card.4\nA second, interactive, display was placed on a pedestal allowing sections of the movie to be played back\nindividually from a web page interface. To support this feature a Sharp LL151D auto-stereoscopic display was\nused. This was driven with pre-interlaced images at a stereo resolution of 512x768x2 using a single channel from\nan nVidia Quadro card.4\nFinally two large 1mx0.9m auto-multiscopic lenticular prints were used to show 3D views from the finale\nof the movie. Each print required the generation of 27 source images, each individual view at a resolution of\napproximately 1024x768, giving a multiscopic resolution of 1024x768x27.\n2.2. Stereo Camera Settings\nOne of the most discussed issues regarding stereoscopic image capture is the stereo camera settings required,\nspecifically what camera separation to use. Two common approaches are the verging camera model and the\nparallel camera model; these are widely recommended in stereoscopic graphics and photography books.5\nLeft camera\nRight camera\na\nZ\u2019\nLeft camera\nRight camera\na\nZ\u2019\nFigure 1. Stereoscopic camera models; left diagram shows a verging camera arrangement; right diagram shows a parallel\ncamera arrangement.\nThe verging camera arrangement is shown in Figure 1. This is often a preferred approach as it is easy to\nsetup and requires little post-processing of images for synthetic and photographic cameras using symmetrical\nfrustum. If the cameras are rotated to verge at the zero disparity plane Z \u2032 then objects in the scene in this plane\nwill appear to the viewer to be in the plane of the display. However, this rotation generates opposite keystone\ndistortion5 in the left and right images which in turn produces false vertical disparities in the stereo pair. The\nresult is images that are subjectively harder to view and quantitatively distort stereoscopic space.6\nA parallel camera arrangement, also shown in Figure 1, avoids the problem of keystone distortion by capturing\nthe left and right images using coplanar image(film) planes. To ensure infinity is behind the screen surface the\ncamera frustum should be asymmetrical such that the cameras have coincident field width at the zero disparity\nplane Z \u2032. This can be achieved in systems without asymmetric frustum by capturing a symmetric view and\ncropping to the required region of the image. Using the parallel camera model avoids vertical disparity but, in\ncommon with the verging camera model, it does relatively little to help the image producer control total apparent\ndepth. The result is for both approaches there is a need to engage in repeated trial and error, adjusting camera\nseparation until a comfortable image is created.\nA confusion regarding the utility of verging vs parallel camera setup is often brought about as the eyes verge\nwhen viewing a scene. However, the aim of image generation for most 3D displays is not to generate images for\nthe retina but for a display surface that is then viewed by the eyes from a distance. Hence the requirement is to\ncapture each view for the same image plane since they are displayed in the same display plane.\nA second confusion is that it is often assumed that camera separation should be the same as average eye\nseparation. Again this would be true if we were generating images to write directly onto the retina. But in almost\nevery 3D display, including head mounted displays, we are generating images to view on a remote display plane.\nThe human factors of viewing 3D displays like this are such that it is rarely the case that camera separation\nshould equal eye separation; more typically camera separation should be less than eye separation.\nA growing number of independent studies7\u201310 investigating the depth range that 3D displays can reproduce\nhave shown there are limits regarding how much depth can be viewed in-front and behind the display plane.\nThe limits vary between displays and they are thought to arise in part because of the mismatch between focus\nand vergence requirements compared to viewing the real world. On a 3D display wherever the eyes verge to fuse\ndepth behind and in-front of the screen, the eye\u2019s accommodation system must work to keep the image at the\nphysical display screen distance in focus. This is thought to be a situation unlike the real world where vergence\nand accommodation systems work together in a flexible but inter-linked arrangement.\nAs a result of the lack of direct control over perceived depth in existing stereo camera control methods and\nthe growing weight of evidence from human factors research, we have over the last five years developed a range\nof new camera models for stereo image generation. These are based on two assumptions; first that the content\ncreator wants to frame a view before taking a stereo picture of that view; secondly that the content creator\nwants the resulting picture to be guaranteed to be comfortable to view on a target 3D display. The result has\nbeen methods that, from a given viewpoint, map depth from scene space to a display space in a controlled\nmanner.1, 2, 9\nMonoscopic\nCamera\nN\u2019 n\u2019 f\u2019 F\u2019\nTotal scene depth\nROI\nZ\u2019 Left\nCamera\nN\u2019 n\u2019 f\u2019 F\u2019\nTotal scene depth\nROI\nRight\nCamera\nZ\u2019\na\nFigure 2. New stereoscopic camera models allow the content creator to position a camera view and define the depth\nrange they wish to capture as shown in the diagram on the left. The stereoscopic cameras shown in the diagram on the\nright are then calculated automatically to capture the required scene depth range.\nThis new depth region mapping approach to stereo image generation is illustrated in Figure 2. The content\ncreator simply positions a camera to frame the desired view of the scene and measures (usually automatically)\nthe total range of depth in the scene as seen by the camera. The stereo camera separation is then calculated\nautomatically with a guarantee that the viewer looking at the image on the target 3D display will not see\nperceived depth outside defined limits. There is then no need for trial and error capture to adjust camera\nseparation.\nWe have implemented a recent depth region mapping algorithm1 in the inThreeD3 plug-in for 3ds Max11\nwhich supports animated scenes. One of our goals in this paper is to test this approach and compare it with\nexisting methods. In the following sections we discuss in turn each of the individual sections of the Cosmic\nCookery movie; in particular we highlight the rendering tools we used and discuss the different issues the varying\nstereo support in these tools raised during the movie production process.\n3. FLIGHT OUT OF THE SOLAR SYSTEM\nFigure 3. Our journey into space and time begins at home on Earth.\nThe movie is constructed to begin and end at the Earth providing a reference point for the viewer as we fly\nfar out in space and time. In this first sequence, Figure 3, we see the Earth and then fly out passing Jupiter and\nits giant red spot before sweeping on past Saturn where the rings and moons become clearly visible in 3D.\nAfter hunting for software supporting suitable 3D models of the Solar System we chose Celestia.12 Celestia\ncan generate an animation and capture a sequence of still images along an animation path. Unfortunately\nCelestia\u2019s animation output is not stereo capable and therefore we had to capture left and right viewpoints\nseparately. We used trial and error to set the separation for a pair of verging cameras and then kept the same\nfixed camera separation for the entire sequence. This caused artificial vertical disparity and does not allow us to\nadjust the available depth to track the key object in the scene. However, since the main object of interest is in\nthe screen center the vertical disparity is minimized.\nAs the left and right images in this sequence were generated as separate screen captures, we had to process\nthe entire sequence to crop images to the required size and then produce the side by side format required for the\nmovie. We used automated scripts in Adobe Photoshop13 to size and crop images individually before merging\nthe separate left and right images together into the side-by-side format using the batch processing mode of\nStereoPhoto Maker.14\n4. FROM THE MILKY WAY TO VIRGO\nThe Sun is one of the hundred thousand million stars that make up the Milky Way and as we leave the Solar\nsystem we see a spectacular view of our own spiral galaxy. Then traveling across intergalactic space, we encounter\nthe large and small Magellanic clouds, two miniature galaxies attached to the Milky Way. Eventually we arrive\nat the Virgo cluster, at the center of Virgo lies a large galaxy called M87 which is home to a massive black hole\nweighing more than three thousand million times the weight of the Sun.\nThis sequence was rendered by the Visualization group at NCSA, UIUC, using their PartiView15 software\ntool. PartiView allows the stereo camera separation to be modified along a flight path on a frame by frame basis.\nA verging camera model is available providing control of focal point.\nTrans-Atlantic communication of the movie was achieved using ftp and JPEG compressed still sequences.\nAlthough using lossy compression degrades image quality and is not ideal for production, the substantial reduction\nin file size made this worthwhile when communicating by ftp. Stills captured from an MPEG preview movie\nproved a useful tool for feedback on depth adjustments. During this process the left and right image sequences\nwere merged using StereoPhoto Maker working in batch mode to create side-by-side image pairs. The result was\ncoded and previewed in stereo using QuickTime Pro16 and the nVidia Quadro horizontal span mode so that left\nand right views appeared on the left and right channels driving the projectors.\n5. FLIGHT THROUGH THE 2DF SURVEY\nFigure 4. The 2dF Galaxy Redshift Survey, this fly through contains some of the highest disparities in the animation as\ngalaxies rapidly fly past the viewer.\nThe 2dF Galaxy Redshift Survey (2dFGRS)17 was designed to measure redshifts for approximately 250,000\ngalaxies and maps our cosmic neighborhood far beyond the Virgo cluster. This sequence begins with a fly through\nof the galaxies, see Figure 4, and then pans out to show the whole survey. As the camera zooms out the view\nshows how galaxies clump together in a rich variety of structures, revealing an intricate cosmic web of filaments\nand clusters.\nThe sequence was generated at Durham from the 2dFGRS data and a series of galaxy images using the NCSA\nPartiView15 software. This involved converting the observation data to a 3D model with galaxy images and then\nplanning an animation path through the sequence. The stereoscopic camera separation was then defined and\ntuned throughout the sequence to give the best depth effect within a reasonable comfortable range. Because\nPartiView presents an interactive graphics interface to the user it is possible to preview flight paths in real time\nand this allows faster iteration when making creative decisions and tuning the stereo.\nIn this sequence certain fast moving objects briefly exceed what we usually consider to be a comfortable\ndepth range when they are flying rapidly off the screen. This produces a dramatic effect but is used sparingly\nas we suspect were we to repeatedly over-drive the vergence muscles in this way it would result in eye fatigue.\nPartiView output images are saved as separate left and right files using the ppm file format. This required\nbatch conversion, using Adobe PhotoShop, to PNG and then merging into the side-by-side format for the final\nproduction using StereoPhoto Maker.\n6. THE MILLENNIUM SIMULATION\nThis section of the movie, shown in Figure 5, illustrates the simulated dark matter distribution in the universe\nat the present time, using data from the Millennium Simulation.18 The structure of the dark matter looks like\nthe observed cosmic web seen in the previous sequence reflecting the statistically similar nature of the real and\nsimulated universes. These vast concentrations of dark matter are believed to surround real galaxies; the gravity\nof the dark matter holding the galaxies together. Dark matter is thought to consist of particles significantly\ndifferent from those that make up the atoms of stars, planets and people.\nThe Millennium Simulation represents a landmark in computational cosmology following the gravitational\ninteraction between more than 1010 particles in a volume larger than that mapped by the 2dFGRS which featured\nFigure 5. An image of the Millennium simulation illustrating the distribution of dark matter at the present day.\nin Section 5. This sequence shows approximately 5 \u2217 106 particles or 0.05% of the total. The analysis of such\na massive simulation has posed a new set of challenges for cosmologists, resulting in shifts in their research\nmethodology. Visualizations such as the one described here are becoming a valuable research tool in addition to\nproviding striking images for outreach programmes.\nIn order to render these frames a computing cluster of 16 Sun SPARC processors was used running custom\nrendering software18 over several days. The software supported verging stereoscopic cameras and again required\nseveral trial runs to establish a reasonable perceived depth range. With camera control limited to a single\nsetting of camera separation a compositional balance was required to illustrate the dark matter halo at the end\nin sufficient 3D depth while not producing excessive depth in the smaller clusters flying out of the screen towards\nthe viewer early in the sequence.\n7. THE BIG BANG AND INFLATION\nFigure 6. A still from the middle of the Big Bang sequence illustrating the creation of the universe as it grew from a\npinpoint to the size of an orange, before stereoscopically cross-fading to the show the CMB radiation; the evidence still\nmeasurable today from the Big Bang.\nHere we visualize the Big Bang; when a feature the size of a pin-point formed our universe in a process known\nas vacuum fluctuation. This was followed by inflation; a period of rapid expansion, lasting a tiny fraction of a\nsecond. Following which the universe was approximately the size of a large orange and the forces and particles of\nmatter we are familiar with began to form.The sequence ends showing the fluctuations in the Cosmic Microwave\nBackground (CMB), measurable today as the fossil record of the Big Bang illustrated in Figure 6.\nIn this sequence we planned a pinpoint to travel towards the viewer as the commentary began to explain the\norigin of cosmic structures. We then planned an explosion to illustrate the Big Bang and inflation followed by\nthe appearance of the illustrative orange cross fading to show the CMB. If we had rendered the sequence with a\nstandard stereo depth mapping most of the stereoscopic depth would have been used in the pinpoint section and\nthe explosion and subsequent action would have been left stereoscopically flat. Instead we used our inThreeD\nplug-in for 3dsMax allowing us to manipulate the depth mapping ensuring most of the available stereoscopic\ndepth was allocated to the explosion and the remaining part of the sequence.\nThe new control provided by the three region depth mapping implemented in the inThreeD plug-in provided\na clear benefit in composing the depth presentation of this sequence. In addition it required significantly fewer\ntrial renderings to adjust the depth effect since we knew the mapping by design. An unexpected benefit of the\nplug-in was the ability to directly generate correctly cropped side-by-side image sequences as output. This then\nrequired no time to batch process the images to change format, cropping or image size and they could be directly\nimported into QuickTime Pro and Adobe Premiere for movie creation.\nOverall the time savings using the inThreeD plug-in to render this sequence allowed us to spend significantly\nmore time considering creative rather than technical aspects of production. This was in contrast to the other\ntools we used where we spent significant time tuning detailed technical aspects of the stereo production.\n8. DARK MATTER HALOS\nFigure 7. In this sequence we see dark matter condensing due to gravitational attraction to form dark matter halos.\nThis simulation shows the formation of dark matter structure in the universe around one dark matter halo,19\nillustrated in Figure 7. These halos form due to the small fluctuations in the early universe and the action of\ngravity over time. As we see in the next sequence within dark matter halos galaxies form as matter is sucked in\nover billions of years.\nOutput from the halo simulation software consisted of 900 files of volumetric data each containing 2563 values\nwith a total data size of 57 Gbytes. We chose Kitware\u2019s VolView20 as the rendering software for this sequence\nas it was able to use floating point voxel data and produces high quality volume renderings. However, VolView\nhad two drawbacks; it was designed for single file manipulation rather than batch processing and it was not able\nto save stereo images to file.\nFortunately KitWare were able to supply scripts enabling VolView to process the large sequences of data files\nand output left and right stereo images to file. These allowed us to batch process the animation with a total\nrendering time for the sequence of the order of two days. Without this support manual loading and rendering of\nthe 900 individual files would have been an infeasible task in the time available.\nStereoscopic camera control in VolView is provided simply by setting a vergence angle and therefore repeated\ntrial and error renderings were needed to ensure the stereoscopic depth was acceptable throughout the sequence.\nTherefore vertical disparity is a potential issue, however as the sequence was primarily centered in view vertical\ndisparity is minimized. Since VolView rendering was to the on-screen buffer the image resolution was defined\nby window size. This combined with the output of separate left and right files required post-processing to crop\nand re-size images and merge them to form combined left\/right side-by-side pairs. Both Adobe Photoshop and\nStereoPhoto Maker batch mode processing tools were used to do this.\n9. SPIRAL GALAXY FORMATION\nFigure 8. Simulated behaviour of gas and stars as a spiral galaxy forms.\nIn this sequence we again turn back the clock 13 billion years to the beginning of the universe and now\nexamine what happens in the inner region of a dark matter halo. Over time matter collects drawn together by\ngravity, it cools and as the density increases myriad stars form. In particular this simulation illustrates how the\ngas and stars can merge to form a spiral galaxy,21 as shown in Figure 8.\nAs with the previous sequence there were a series of 2563 simulation output files; 850 in total. Like the last\nsequence we used VolView to render these and relied on the scripts supplied by Kitware to handle batch rendering\nand stereo image output. Post-processing using Adobe Photoshop and StereoPhoto Maker was required to obtain\nthe final side-by-side stereo pair. With rendering times for this sequence exceeding two days the stereo camera\ncontrol available in VolView and consequent trial and error approach to adjusting camera separation to obtain\na good depth presentation proved time consuming.\n10. FINALE\nFigure 9. The simulated galaxy, left, and real galaxy, right, are shown together to allow a direct visual comparison.\nThe life of a galaxy, from its microscopic beginnings in the early universe to its present day splendor can\nbe recreated in a computer. This final sequence, Figure 9, shows both a simulated galaxy and a volumetric\nrendering of a real galaxy side-by-side. The sequence zooms into the two galaxies and then rotates them to\nprovide a visual comparison between simulation and reality.\nRendering this sequence using VolView involved rendering both the rotation sequence for the real galaxy\nand a separate sequence for the simulated galaxy. Each sequence was then scaled individually in both left and\nright views to achieve the zoom-in. This required loading the four image sequences individually and then scaling\neach image sequence up to full size using built in Adobe Premiere animation tools. Ensuring the scaling was\nconsistent for each of the four sub-images required some care.\nOverall the editing and post-processing for this sequence was surprisingly time-consuming, particularly given\nthe relative simplicity of the individual tasks required. The renaming software tool MRename22 became of crucial\npractical significance to ensure file names were compatible as we transferred them between the different tools\nused.\n11. WORK FLOW AND TOOLS\nSimulation data \nconversion to \nvolume data\nVolume render\nL\/R Frames\nCropping\/Resizing\nMerge L\/R Frames\nQuickTime Raw\nAVI Uncompressed\nSimulation\nSoftware\nVolView + \nCustom Scripts\nPhotoshop\nMRename\nStPhtMkr\n31Mb\n57Gb\nQuickTimePro\nRecord\nVoice Over\nAudio\nMP3 FilesPremiere\nBSplayer\nCopyright\nFree Music\nFinals\n234Mb\n4Gb\nPreview Frame Re-naming\nModeling and \nAnimation\nRendering\nStereo frames\nQuickTime Raw\nUncompressed AVI\n3dsmax\ninThreeD\n3ds plug-in\nQuickTimePro\nRecord\nVoice Over\nAudio\nMP3 FilesPremiere\nBSplayer\nCopyright\nFree Music\nFinals\nPreview\nFigure 10. A sketch of the work flow for the dark matter halo sequence on the left and the Big Bang sequence on the\nright.\nThe work-flow for each sequence varied depending on the degree to which the rendering systems supported\nstereoscopic rendering and output. In addition edits to a sequence typically required manual intervention and\nbatch processing.\nThe work flow in two different sequences is shown in Figure 10. For the dark matter halo the volume\nrenderer, while supporting stereo, rendered to screen resolution and produced as output separate left and right\nimage files. These images required re-naming, cropping and re-sizing before merging into a side-by-side left\/right\npair. This then allowed preview movies to be created from the PNG format image files using QuickTime Pro.\nAs the stereoscopic camera separation was only controlled approximately using verging cameras this process was\nrepeated more than once to tune perceived depth before the final version was produced.\nFor the Big Bang sequence we used the InThreeD plug-in which allowed direct control over the stereoscopic\neffect and produced as output the correct size side-by-side pairs. As a result not only was the stereo effect easier\nto control there was no need for any image processing before the preview movie could be created. The extra time\nthis saved allowed us to spend more time on modeling and planning the composition of depth in this sequence.\n12. COMPRESSION AND PLAYBACK\nThe main goal both in production and playback was to have the best possible image quality and where the\nrendering tools allowed we used lossless image coding formats; either PNG or TIFF. When these were not\navailable JPEG was used at the highest quality setting. Our aim was to avoid multiple lossy compression steps\nas files were edited and re-edited several times in some sequences.\nEach sequence after rendering and editing was stored as a set of numbered stereo images in side-by-side\nformat, using either lossless PNG or high quality JPEG when this was not possible. For preview movies we used\nQuickTime Pro to read in the numbered sequence and playback the resulting preview movie in realtime. This\nran on an nVidia Quadro graphics card running in horizontal span mode across dual DVI outputs driving our\nBARCO Gemini passive stereo dual projector and preview monitors. QuickTime worked well for early previews\nof individual sequences, when the preview was small enough to fit in system memory (2Gbytes), but the full\nmovie was too large to playback this way on a standard PC.\nFor the production version of the movie we switched to use an Inition 3DVidBox, containing a high speed\nRAID array and dual DVI nVidia Quadro cards again driving passive dual channel projection. This allowed us\nto playback the entire movie (47Gbytes) as a single uncompressed AVI file at 2x1024x768 at 25fps. The final edit\nto create this version of the movie was undertaken using Adobe Premiere, this brought together the individual\nsequences, titles and sound track. Real time playback of the full production version of the movie was achieved\nusing the BSplayer software tool.23\nSubsequently we have used Premiere and StereoPhoto Maker to generate anaglyph versions of the movie for\nplayback on 2D displays. These have required some care with compression standards and we chose QuickTime\nusing MPEG4 coding and also a lossy AVI format. It has however not been straight-forward to find a single\ncompression standard that works well on the range of platforms we have ported the movie to (MAC, PC, DVD-\nVideo). Monoscopic versions of the movie have been simple to generate using Premiere to crop the sequence to\nremove the right channel images.\n12.1. Editing\nTools for editing stereoscopic images are not widely available and often lack the batch processing technologies\nsuitable for movie generation. A range of tools were therefore used in post-rendering edits and movie production.\nEach of the rendering systems were used to generate sequences of still stereoscopic images. Some of the\nrendering tools were able to generate side-by-side pairs at the required resolution 1024x768x2 but many generated\nnon-standard resolutions and separate files for the left and right views. This required tools capable of batch\ncropping or extending images to the required size and often this also required splitting and\/or merging of stereo\npairs. To achieve this Adobe Photoshop and Stereo PhotoMaker were both important in the production process.\nOnce the images were formatted correctly we used QuickTimePro to generate rushes for previews and then\nfinally Adobe Premiere to merge the sequences into a single movie file.\nA crucial tool was file renaming software used to rename sequences so that the various automated file readers\nread them in the correct order. For this we used MRename22 which had a flexible and powerful interface based\non a regular grammar.\n13. SUPPORTING ADDITIONAL DISPLAY TYPES\nWe have already described support for passive stereoscopic projection, anaglyph displays and monoscopic (2D)\nsequences. In addition several clips from the movie were formatted using custom software in the colour column\ninterleaved format required for the Sharp RD3D display. This interleaved each pair in the required format and\nsaved them using lossless PNG files. These could then be read in as a numbered sequence and played back in\nQuickTime Pro, the PNG lossless compression retained the interleaving pattern.\nIn all cases we assumed the largest display used was the 2m projection screen and the scaling of the images to\nfit smaller displays would reduce disparity to remain within comfortable limits. The images were not recalculated\nfor each smaller display and would have benefited from this had sufficient time been available.\nWe also created two 1mx0.9m lenticular prints of the simulated and real spiral galaxies for the exhibition.\nThe prints were produced by David Burder at 3D Images Ltd. but the images were rendered at Durham again\nusing VolView. A series of twenty seven images were created at one degree rotation intervals and merged into a\nsingle layered Photoshop file. As these had to use simple camera rotation around a point to generate the views\nvertical disparity is present in the stereoscopic image. However, as the content is centered in the image this\nartifact is minimized.\n14. CONCLUSIONS\nOur first aim was to produce a 3D movie that would visualize current theories and observations describing the\ncreation of cosmic structures in the universe and in particular the formation of matter into galaxies. The movie\nwas well received by the general public at its first showing at the Royal Society Summer Science Exhibition in July\n2005. It has subsequently been successful in attracting attention to and generating substantial discussion about\nresearch into the universe and its origins at many national and international venues, including the American\nAstronomical Society meeting in January 2006.\nTechnically we used a wide range of 3D techniques and tools to produce the movie and associated mate-\nrial. Underlying this we had a constant requirement to retain the best possible stereo quality throughout the\nproduction for the viewer; three specific issues concerned us:\nCamera control for comfortable depth In most of the sequences we had to use traditional stereoscopic\ncamera methods to set camera parameters; requiring a trial and error approach to depth composition. We\nalso used our new methods; based in human factors research and allowing precise control over the depth\npresentation in an image. Both approaches worked well but the new methods gave more control and were\nmuch faster to use; freeing more time to consider scene design and the creative use of the stereo effect.\nImage editing Very few image and movie editing tools are stereo aware and this resulted in a need to use\na range of tools to edit and format the image sequences. Of these four tools became essential; Adobe\nPhotoshop, Stereo PhotoMaker, MRename and Adobe Premiere. We would particularly highlight Stereo\nPhotoMaker as pointing the way to general purpose cross-platform stereo image editing.\nCompression During production we worked whenever possible with lossless image formats such as PNG and\nTIFF and always used side-by-side full resolution images. When JPEG was the only available output format\nfrom a renderer we saved images at the highest quality and converted to a lossless format for editing. For\npresentation of the movies our main platform is a 3DVidBox allowing us to playback uncompressed AVI\nvideo at full 2x1024x768 resolution. We have also successfully created and distributed lossy QuickTime\nand AVI versions of the movie in side-by-side, anaglyph and monoscopic formats.\nWe were careful to control the final depth effect in the movie and tuned the effect to a 2m diagonal screen\nviewed from about 2m. It is very clear when we view this enlarged on our 3m screen that the depth starts to\npush the limits of acceptable comfort. This confirms human factors work in the literature and predictions from\nthe viewing geometry. In addition the reduced sizes of the movie for desktop display work but show less depth\nthan they ideally could. We believe it remains an open question how portability can be successfully managed if\n3D movies become a widespread consumer commodity\nIt is worth noting that without nVidia introducing comprehensive driver support and multi-screen windows\nmanagement technology we would find stereo displays very much more difficult to use. We would encourage\nindustry to enhance this support in three specific ways:\nCapture Enable real time capture of the pixel stereo format fed to the display. This would allow direct capture\nof real time graphics in a display specific format, e.g. colour column interleaved, for subsequent playback\nas a movie, or:\nMovie support Enable the side-by-side format we used to play back to the left and right stereo buffers (instead\nof the left and right screens). Then the stereo pixel format required for a specific display device could be\ngenerated in real time by the graphics driver and the side-by-side movie format could become standard\nacross all stereoscopic platforms.\nLeft\/Right standards In our recent experience left\/right standards for stereo are urgently needed. It is still\nfeasible to show these reversed and still hard for non-experts (and experts) to detect this problem.\nThe Cosmic Cookery project was a significant production effort allowing us to exercise a wide range of\nstandards and tools for animated stereoscopic movie generation. We believe we have shown that this is a feasible\nprocess using current desktop tools. We have been encouraged that the anticipated benefits of our research into\nstereoscopic camera control have been demonstrated in this project.\nACKNOWLEDGMENTS\nWe would like to acknowledge the support and help of a wide range of people, funding agencies and companies\nin the production of the Cosmic Cookery movie and Royal Society exhibits. In addition to those mentioned\nin the text we would like to thank; for the real galaxy volume data and colour schemes Richard Bower and\nSimon Morris at Durham University, for rendering the Milky Way to Virgo sequence Bob Patterson and Stuart\nLevy at NCSA UIUC, for his software and advice on rendering the Millennium Simulation data Volker Springel\nat MPA Garching, for advice on conveying science to real people Pete Edwards, for systems support Lydia\nHeck, for anaglyph conversions summer students of the Institute for Computational Cosmology including Diego\nHartasanchez, for substantial equipment loans and advice for the exhibition Inition Limited particularly Stuart\nCupit, for Premiere advice and creating the final edit of the movie Phil Brown at Media55, for laboratory space\nand equipment access the eScience Research Institute Durham University, for additional financial support the\nDean of the Science Faculty Durham University.\nREFERENCES\n1. N. Holliman, \u201cMapping perceived depth to regions of interest in stereoscopic images,\u201d in Stereoscopic Displays and\nApplications XV, Proceedings of SPIE 5291, 2004.\n2. N. Holliman, \u201cSmoothing region boundaries in variable depth mapping for real time stereoscopic images,\u201d in Stereo-\nscopic Displays and Applications XVI, Proceedings of SPIE 5664A, 2005.\n3. B.Froner and N. Holliman, \u201cImplementating an improved stereoscopic camera model,\u201d in Eurographics Theory and\nPractice of Computer Graphics, ISBN: 3-905673-56-8, 2005.\n4. nVidia Corporation, \u201cnVidia Quadro FX.\u201d http:\/\/www.nvidia.com\/.\n5. D. McAllister, ed., Stereo computer graphics and other true 3D technologies, Princeton University Press, 2003. ISBN\n0-691-08741-5.\n6. D. Diner and D. Fender, Human engineering in stereoscopic viewing devices, Plenum Press, 1993. ISBN 0-306-44667-7.\n7. Y. Yeh and L. Silverstein, \u201cLimits of fusion and depth judgements in stereoscopic color displays,\u201d Human Factors\n1(32), 1990.\n8. A. Woods, T. Docherty, and R. Koch, \u201cImage distortions in stereoscopic video systems,\u201d Proceedings of SPIE 1915,\n1993.\n9. G. Jones, D. Lee, N. Holliman, and D. Ezra, \u201cControlling perceived depth in stereoscopic images,\u201d in Stereoscopic\nDisplays and Virtual Reality Systems VIII, Proceedings of SPIE 4297A, 2001.\n10. E. W. Jin, M. E. Miller, S. Endrikhovski, and C. D. Cerosaletti, \u201cCreating a comfortable stereoscopic viewing\nexperience: effects of viewing (accommodative) distance and field of view on fusional range,\u201d in Stereoscopic Displays\nand Applications XVI, Proceedings of SPIE 5664A, 2005.\n11. Autodesk Corporation, \u201cAutodesk 3ds Max.\u201d http:\/\/usa.autodesk.com\/.\n12. C. Laurel, \u201cCelestia.\u201d http:\/\/www.shatters.net\/celestia\/.\n13. Adobe Corporation, \u201cAdobe Photoshop.\u201d http:\/\/www.adobe.com\/products\/photoshop\/main.html.\n14. M. Suto, \u201cStereoPhoto Maker.\u201d http:\/\/stereo.jpn.org\/eng\/.\n15. S. Levy, \u201cPartiView.\u201d http:\/\/niri.ncsa.uiuc.edu\/partiview\/.\n16. Apple Corporation, \u201cQuickTime Pro.\u201d http:\/\/www.apple.com\/quicktime\/download\/win.html.\n17. M. Colless, G.B.Dalton, S. Maddox, W. Sutherland, P. Norberg, S. Cole, J. Bland-Hawthorn, T. Bridges, R. Cannon,\nC. Collins, W. Couch, N. Cross, K. Deeley, R. D. Propris, S. Driver, G. Efstathiou, R. Ellis, C. Frenk, K. Glazebrook,\nC. Jackson, O. Lahav, I. Lewis, S. Lumsden, D. Madgwick, J. Peacock, B. Peterson, I. Price, M. Seaborne, and\nK. Taylor, \u201cThe 2df galaxy redshift survey: Spectra and redshifts,\u201d Mon.Not.Roy.Astron.Soc 328, 2001.\n18. V. Springel, S. White, A. Jenkins, C. Frenk, N. Yoshida, L. Gao, J. Navarro, R. Thacker, D. Croton, J. Helly,\nJ. Peacock, S. Cole, P. Thomas, H. Couchman, A. Evrard, J. Colberg, and F. Pearce, \u201cSimulations of the formation,\nevolution and clustering of galaxies and quasars,\u201d Nature 435(7042).\n19. J. Navarro, E. Hayashi, C. Power, A. Jenkins, C. Frenk, S. White, V. Springel, J. Stadel, and T. Quinn, \u201cThe inner\nstructure of lcdm haloes - iii. universality and asymptotic slopes,\u201d Mon.Not.Roy.Astron.Soc 349, 2004.\n20. Kitware Corporation, \u201cVolView.\u201d http:\/\/www.kitware.com\/products\/volview.html.\n21. T. Okamoto, V. Eke, , C. Frenk, and A. Jenkins, \u201cEffects of feedback on the morphology of galaxy discs,\u201d\nMon.Not.Roy.Astron.Soc 363, 2005.\n22. Rajko, \u201cMRename.\u201d http:\/\/fly.cc.fer.hr\/\u02dcrajko\/MRename\/.\n23. Boris, \u201cBSplayer.\u201d http:\/\/www.bsplayer.org\/.\n"}