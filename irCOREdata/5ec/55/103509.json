{"doi":"10.1007\/978-3-540-74913-4","coreId":"103509","oai":"oai:epapers.bham.ac.uk:32","identifiers":["oai:epapers.bham.ac.uk:32","10.1007\/978-3-540-74913-4"],"title":"Neuroevolution of Agents Capable of Reactive and Deliberative Behaviours in Novel and Dynamic Environments","authors":["Robinson, Edward","Ellis, Timothy","Channon, Alastair"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2007-09-04","abstract":"Both reactive and deliberative qualities are essential for a good action selection mechanism. We present a model that embodies a hybrid of two very different neural network architectures inside an animat: one that controls their high level deliberative behaviours, such as the selection of sub-goals, and one that provides reactive and navigational capabilities. Animats using this model are evolved in novel and dynamic environments, on complex tasks requiring deliberative behaviours: tasks that cannot be solved by reactive mechanisms alone and which would traditionally have their solutions formulated in terms of search-based planning. Significantly, no a priori information is given to the animats, making explicit forward search through state transitions impossible. The complexity of the problem means that animats must first learn to solve sub-goals without receiving any reward. Animats are shown increasingly complex versions of the task, with the results demonstrating, for the first time, incremental neuro-evolutionary learning on such tasks","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/103509.pdf","fullTextIdentifier":"http:\/\/epapers.bham.ac.uk\/32\/1\/ecal2007.pdf","pdfHashValue":"3bf94025550ede74712356aa3c7eca8ec52ece65","publisher":"Springer Berlin \/ Heidelberg","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:epapers.bham.ac.uk:32<\/identifier><datestamp>\n      2011-07-09T15:00:51Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D48:4831<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Neuroevolution of Agents Capable of Reactive and Deliberative Behaviours in Novel and Dynamic Environments<\/dc:title><dc:creator>\n        Robinson, Edward<\/dc:creator><dc:creator>\n        Ellis, Timothy<\/dc:creator><dc:creator>\n        Channon, Alastair<\/dc:creator><dc:subject>\n        H Social Sciences (General)<\/dc:subject><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        Both reactive and deliberative qualities are essential for a good action selection mechanism. We present a model that embodies a hybrid of two very different neural network architectures inside an animat: one that controls their high level deliberative behaviours, such as the selection of sub-goals, and one that provides reactive and navigational capabilities. Animats using this model are evolved in novel and dynamic environments, on complex tasks requiring deliberative behaviours: tasks that cannot be solved by reactive mechanisms alone and which would traditionally have their solutions formulated in terms of search-based planning. Significantly, no a priori information is given to the animats, making explicit forward search through state transitions impossible. The complexity of the problem means that animats must first learn to solve sub-goals without receiving any reward. Animats are shown increasingly complex versions of the task, with the results demonstrating, for the first time, incremental neuro-evolutionary learning on such tasks.<\/dc:description><dc:publisher>\n        Springer Berlin \/ Heidelberg<\/dc:publisher><dc:date>\n        2007-09-04<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/epapers.bham.ac.uk\/32\/1\/ecal2007.pdf<\/dc:identifier><dc:relation>\n        public<\/dc:relation><dc:relation>\n        http:\/\/epapers.bham.ac.uk\/32\/1.hassmallThumbnailVersion\/ecal2007.pdf<\/dc:relation><dc:relation>\n        http:\/\/dx.doi.org\/10.1007\/978-3-540-74913-4<\/dc:relation><dc:identifier>\n        Robinson, Edward and Ellis, Timothy and Channon, Alastair (2007) Neuroevolution of Agents Capable of Reactive and Deliberative Behaviours in Novel and Dynamic Environments. Advances In Artificial Life, 4648. pp. 345-354. ISSN 0302-9743<\/dc:identifier><dc:relation>\n        http:\/\/epapers.bham.ac.uk\/32\/<\/dc:relation><dc:language>\n        English<\/dc:language><dc:contributor.sponsor>\n        Partially funded by EPSRC Studentship<\/dc:contributor.sponsor><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["public","http:\/\/epapers.bham.ac.uk\/32\/1.hassmallThumbnailVersion\/ecal2007.pdf","http:\/\/dx.doi.org\/10.1007\/978-3-540-74913-4","http:\/\/epapers.bham.ac.uk\/32\/"],"year":2007,"topics":["H Social Sciences (General)","QA75 Electronic computers. Computer science"],"subject":["Article","PeerReviewed"],"fullText":"Neuroevolution of Agents Capable of Reactive\nand Deliberative Behaviours in Novel and\nDynamic Environments\nEdward Robinson, Timothy Ellis and Alastair Channon\nSchool of Computer Science, University of Birmingham, Birmingham, B15 2TT, UK\nedd@edd-robinson.net, t.s.ellis@cs.bham.ac.uk, alastair@channon.net\nAbstract. Both reactive and deliberative qualities are essential for a\ngood action selection mechanism. We present a model that embodies\na hybrid of two very di\u000berent neural network architectures inside an\nanimat: one that controls their high level deliberative behaviours, such as\nthe selection of sub-goals, and one that provides reactive and navigational\ncapabilities. Animats using this model are evolved in novel and dynamic\nenvironments, on complex tasks requiring deliberative behaviours: tasks\nthat cannot be solved by reactive mechanisms alone and which would\ntraditionally have their solutions formulated in terms of search-based\nplanning. Signi\fcantly, no a priori information is given to the animats,\nmaking explicit forward search through state transitions impossible. The\ncomplexity of the problem means that animats must \frst learn to solve\nsub-goals without receiving any reward. Animats are shown increasingly\ncomplex versions of the task, with the results demonstrating, for the \frst\ntime, incremental neuro-evolutionary learning on such tasks.\nKeywords: Arti\fcial Life, Neural Networks, Incremental Evolution, Re-\nactive and Deliberative Systems, Novel and Dynamic Environments.\n1 Introduction\nIn this paper we present work showing animats that use neural networks to dis-\nplay high level deliberative decision making whilst retaining reactive qualities.\nDeliberative planning has traditional roots in \\Good Old Fashioned Arti\fcial\nIntelligence\" (GOFAI) as a search-based method for the design of behaviour\nsystems. There are issues however with its application in dynamic and novel\nenvironments. Reactive models of action selection on the other hand can be\nvery successful in dealing with unpredictable and dynamic environments. How-\never, since these systems generally have only a short look-ahead the individual\ncomplexity of behaviour that can emerge is limited. Both reactive and deliber-\native qualities are essential for a good action selection mechanism: deliberative\nmechanisms for long term goal seeking and reactive capabilities for dealing with\nunforeseen events [1, 2].\nA complex problem has been designed to demonstrate our model, which we\nhave called the \u2018river-crossing task\u2019 or RC task. In this problem an animat must\n2cross a river by building a bridge made out of stones collected from locations\nin a 2D grid-world environment. Importantly, animats are evolved to solve this\nproblem without any task-speci\fc information. Animats are embodied with two\nvery di\u000berent neural networks. The \frst acts as a deliberative style decision net-\nwork: it makes high level choices about the sub-goals that need to be achieved,\ngiven current internal and external states. The actions that the animats choose\nmay for example be \u2018head to the nearest stone\u2019, \u2018avoid the traps\u2019 or \u2018head to\nthe resource\u2019. Once an animat has made a decision, the second (reactive) neural\nnetwork acts as a navigation tool, taking care of low level actions, such as which\ndirection to move in next. In the RC environment there are several classes of\nobjects that the animats can interact with. Grass objects take up most environ-\nmental space; the animat can place other objects onto them. Stones are movable\nobjects: they can be picked up and dropped on grass or water. If one is dropped\non water then the water object is converted into a grass object. Water objects\nare dangerous. If an animat moves onto one and does not place a stone down\nthen the animat drowns. Traps are lethal to animats, which die if they move onto\none. Resource objects o\u000ber rewards to animats, if they can reach them. None\nof this information is given a priori to the animats, ruling out the possibility of\nexplicit forward search through state transitions.\nPayton [3] used gradient \felds to represent the state-space of a problem and\nas an internalised plan. Unlike more traditional search based models, gradient\n\felds can be generated e\u000eciently, and do not su\u000ber from the same local-minima\nproblems as other wave based mechanisms such as potential \felds [4]. However,\nthe gradient \felds approach does not deal well with changing environments and\nso is often coupled with a Brooks inspired [5] subsumption architecture [3]. An-\nother issue with gradient \felds is that they have to be laboriously constructed.\nWe describe in the next section a biologically inspired gradient based model\nwhich does not su\u000ber from local minima nor any of the other problems associated\nwith other gradient based models. It is computationally e\u000ecient and simple to\ninitialise. We also describe a decision network which is designed to allow animats\nto manipulate the navigation model. We show experimental results in section 3\nand conclude in Section 4.\n2 The Model\nThe movements of the animats in the environment are dictated by a shunt-\ning model introduced in [6, 7]. Yang and Meng were interested in motion plan-\nning models that could react quickly in real-time, allowing a robot or robot-\nmanipulator to perform collision-free motion planning.\nNeural networks have been used extensively and successfully for robot control\nproblems. Often controllers specify a robot\u2019s behaviour based upon sensory input\nfrom the environment; this makes them good for dynamic environments, which\nare likely to change continuously. The model in [6, 7] uses neural networks in\na very di\u000berent way. Instead of using the network to specify behaviour by, for\nexample, mapping the actuators of the robot to the outputs of the network, the\n3network\u2019s activation landscape itself directly speci\fes the robot\u2019s movements\nthrough the environment.\nTheir model consists of a neural network composed of an n-dimensional lat-\ntice of neurons \u2018, where each neuron represents a possible state of the system.\nTherefore any system that can be fully described by a set of discrete states can\nbe represented. This is referred to as the \u2018con\fguration space\u2019 of the robot [8].\nIn the case of the simulated robot in their studies, the con\fguration space was\nthe discretised 2D Cartesian workspace. Each neuron is connected to a subset of\nthe lattice|<i \u001a \u2018. This subset is called the receptive \feld, and represents all\nthe states that are reachable from the current state. It is useful to note that if,\nas in Yang and Meng\u2019s simulated robot example, the state space is simply the\n2D coordinates of the environment that we wish the agent to navigate, there is\nalways a simple one-to-one relationship between neurons and locations.\nThe transition function used to specify inter-neuron dynamics is based on\nthe \u2018shunting equation\u2019, inspired by Hodgkin and Huxley [9] and Grossberg [10].\nYang and Meng designed two versions of this transition function: one which\nhelped to control activity saturation in the network, and a simpler one which\ndid not. In our study we found that the more elaborate transition function was\nnot necessary, since out model did not develop saturation problems; the function\nis shown in equation 1.\ndxi\ndt\n= \u2212Axi + Ii +\nkX\nj=1\nwij [xj ]+ : (1)\nAlpha (A) represents the passive decay rate, which determines the degree to\nwhich each neuron\u2019s activity diminishes towards an idle state. The function [x]+\ncan be described as max(0; x). The connection weight (or synapse strength) wi;j\nbetween neurons is simply speci\fed as the Euclidean distance between the cell\nand its neighbour within the receptive \feld. k is the receptive \feld size and is\nset to 8 to represent the direct neighbours in a 2d grid-world environment. Iota\n(I) is equal to E in the case of a target, and \u2212E for an obstacle, where E is a\nlarge integer.\nOnce the network is con\fgured, and the targets and obstacles established,\nneural activity can be used to navigate a robot by gradient ascent. At each\ntime-step the robot looks at the level of activity in each grid-cell that it is\nconnected to (its neighbours), because they are all the states that it can reach,\nand picks the one with the highest value. As a result of the network\u2019s dynamics,\npositive activity entered at neurons that map to targets propagates through\nthe network, whilst negative activity that is inputted into neurons mapping to\nobstacles cannot propagate globally. Due to the nature of the leaky integrator\nfunction in the model, the activity contribution from a target decreases with\ndistance from that target source, leaving a trail of activity back to the target.\nThis system therefore allows a robot to navigate a dynamic environment,\navoiding obstacles and heading towards targets. Using the shunting network to\ncontrol movement throughout the environment means that high level actions\n4such as \u2018head to the target whilst avoiding obstacles\u2019 can be carried out flaw-\nlessly and quickly. The highly dynamic nature of the network means that when\nthe environment changes (e.g. an obstacle moves), a new activity path can be\ngenerated quickly. In the RC task there are four di\u000berent classes of objects (Re-\nsource, Stone, Water and Trap) that have Iota values associated with them.\nGrass states are considered empty states to allow activity to flow through the\nenvironment. For target acquisition and obstacle avoidance there are two types\nof value: positive and negative. In our implementation of the model however, we\nallow a class to be speci\fed with no value at all. Setting Iota to 0 for a class\nmeans that no external activity will be inputted into any of the neurons in that\nclass. The animat will ignore the object present at that location, and may or\nmay not pass over it while moving through the environment.\n2.1 The Decision Network\nThe outputs of the decision network are used to set the Iota values for the object\nclasses. Using this network, the animat can manipulate the activity landscape in\nthe shunting network in a way that allows it to string together multiple actions\nin parallel to create more complex behaviours.\nFig. 1. The decision network controller. The output neurons are P = pick up\/put down;\nR = resource; S = stone; W = water; T = trap. The input neurons are g = grass; r =\nresource; s = stone; w = water; t = trap.\nThe decision network is a feed-forward neural network with a single hidden\nlayer of four neurons (\fgure 1). The input layer represents the current state\nof the animat, or more precisely, the object class situated on and the carrying\nstatus of the animat. The inputs are single values of 1 or 0 and they feed through\nweighted links into the hidden layer neurons, where tan activation functions are\napplied to the summed input values. The neurons in the output layer represent\nIota values for the object classes needing them (four). Output neurons have tan\nactivation functions and two \fxed thresholds. Neurons with activations over 0.3\nor under -0.3, after being processed through the activation function, output 1\nand -1 respectively. Any activation values in the range [\u22120:3; 0:3] resolve to 0.\nOutput neurons then, have three possible outputs: -1, 0 or 1. The Iota values\nof all the objects (except grass) in the environment are set based upon the\n5output of the decision network neuron associated with that class. If a neuron\nhas a negative output then all of the objects in that class will have a negative\nIota value (-15 in our simulations). Similarly, a positive neuron output sets all\nthe objects in that class with positive Iota values (+15 in our simulations).\nFinally, a neuron with an output of zero sets the objects in that class with Iota\nvalues of 0. Having an Iota value of 0 means that the objects have no external\nactivity inputted into them: their activation values in the shunting network will\nbe solely based upon those of their neighbours. To get a clear understanding\nof the purpose and ability of the decision network, two examples of resulting\nshunting network activity landscapes are shown in \fgure 2. Each landscape is\ndi\u000berent because of the Iota values of object classes. In the \frst landscape for\nexample, the Iota values result in positive activity propagating from the resource\nneuron, through the rest of the network attracting the animats, while negative\nactivity repels animats from the traps.\nFig. 2. A typical environment (left) and two activity landscapes (middle, right). Iota\nvalues in the \frst (middle) landscape a are: resource = 15; stone = 0; water = 0; trap\n= -15. In the right landscape b, resource = 15; stone = 15; water = -15; trap = -15.\nEnvironment legend: stone = small square; resource = circle; trap = cross; water =\nfour dots.\nLandscape b represents the same environment, but with di\u000berent Iota values\nfor object classes, and so the animat acts di\u000berently. Activity from the resource\nwould no longer be able to propagate through the river, but since the stones have\npositive activities, the animat moves to the nearest one (still avoiding traps). One\nof the output neurons on the decision network is not used to provide an Iota value\nfor motion. Instead, its output is used to make a decision about whether or not\nto pick up or drop stones: the other actions an animat can take in our system. If\nthe output is positive then the animat will attempt to pick up whatever object\nit is currently situated on in the grid-world. If negative then the animat will\nattempt to drop an object.\n62.2 Evolution of Decision Networks\nWe used a steady-state genetic algorithm (GA) to search the weight-space, with\n\ftness based upon animat performance in evaluation. Tournament selection was\nused for each iteration, with three animats evaluated and the worst performer\nreplaced by a new o\u000bspring created from a combination of the other two.\nAn animat has a set of chromosomes: one for each neuron in its decision\nnetwork. Each chromosome contains the floating point values for the weights of\nits neuron\u2019s input connections. For each of an o\u000bspring\u2019s (output and hidden)\nneurons, there is a probability Pwhole = 0:95 that the corresponding chromosome\nwill be inherited from just one parent; which parent this is copied from is then\nchosen at random. Otherwise (probability Pmix = 0:05) the o\u000bspring will instead\ninherit a new chromosome whose genes are a mixture of both parents\u2019 versions of\nthe same chromosome, combined by single-point crossover. Finally, each weight\nhas a probability of Pmut = 0:001 of having a mutation value from N(0; 0:4)\nadded to it. All mutations are bounded to within [\u22121; 1].\n3 Experimentation\nFor each experiment, a population of 250 animats is initialised with random chro-\nmosomes, hence with random decision network weights. Animats are evaluated\nsingularly on the RC task in a 20x20 cell grid-world. They are only rewarded\nwhen they reach a resource state: if they fail to reach it then their \ftness is zero.\nSimilarly, if the animat performs any action that leads it to either drowning\nin the river or moving onto a trap, the evaluation task ends and the animat\u2019s\n\ftness is zero. Animats are placed randomly to the left of the river and stones\nand traps are distributed randomly inside the environment. The animat\u2019s de-\ncision network inputs are updated whenever the animat\u2019s state changes; then\nthe shunting model updates the activity landscape with new Iota values and\nthe animat moves to the neighbouring cell with the highest activity. If the pick\nup\/put down neuron is activated then the animat will attempt to pick up\/put\ndown whatever it is on or carrying. At each iteration of the GA three animats are\nrandomly selected. Each animat is evaluated on the same randomly generated\nenvironment to ensure fairness; a new environment is randomly generated in the\nnext iteration of the GA.\nFirst we tested animats by evaluating them on one environment that con-\ntained a river two cells wide. Due to the problem complexity, and because the\nanimats had to learn to solve sub-problems before even receiving any \ftness,\nall of the population\u2019s individuals had zero \ftness and search was random. To\novercome this problem we exposed the animats to three trials of increasing di\u000e-\nculty. Figure 3 shows three randomly generated versions of the three maps used.\nThe randomly positioned objects are the traps and stones: the same river and\nresource locations are always used. The \frst map that the animats are tested on\nalready has a completed bridge. To solve this task the animats simply have to\nlearn to avoid traps and the river and get to the resource. To solve the second\nand third maps an animat has to build a bridge, with the bridge needing to be\n7smaller (one cell wide) for the second map than the third (two cells wide)1. This\nsystem provides a route for incremental evolutionary learning and can be seen as\na simpli\fcation of a more complex general world-environment in which animats\nencounter opportunities for reward, of varying degrees of di\u000eculty.\nFig. 3. Examples of the three environments that animats are evaluated in.\n3.1 Experimental Results\nFor each task the animats solved out of the three they received a score of 100. If\nthey failed a task they were awarded a score of 0 only for the task they failed. In\neach simulation, data showing how many versions of each task had been solved\nin the previous 250 tournaments was collected in 250 tournament intervals. Since\nthere were three animats taking part in each tournament, the maximum number\nof tasks solved in this period, for each map, was 750. Table 1 shows results from\n15 simulations. Once 80% (ie. 600) of animats tested per iteration could solve\nall versions of the task they were shown, the simulation was stopped.\nIn all simulations the animats quickly evolved behaviours that could be used\nto solve the simplest map, where they needed only to know how to avoid dangers\nlike water and traps. They didn\u2019t need to interact with stones to solve this task,\nmaking the process simple. The next two maps were substantially harder to solve,\nand the time taken to solve them reflects this. Results showed that contrary to\nprevious observations: the third map was not harder to solve than the second,\neven though it required a speci\fc strategy. The second intermediate map could\n1 Observations of the simulation showed that animats were solving a map with a single\ncelled river by picking up stones and dropping them on the river indiscriminately.\nThey were attracted to both the resource and the river; since the river was closer,\nthey would deposit a stone on the nearest river cell. Although once they had done\nthis they could complete the task, because they were still attracted to the river they\nwould often keep depositing stones there. Eventually they would deposit enough\nstones to create a gap so large that the activity from the resource attracted them\nenough for them to reach it. Using a deeper river stopped animats for being rewarded\nfor developing this \u2018brute force\u2019 behaviour.\n8Table 1. The mean, best and worst number of tournaments needed for 80% of the\nanimats evaluated in a 250-iteration period to have solved each map.\nMap Mean Best Worst Stdev\n1 5700 4000 8250 1203.4\n2 99084.6 13000 437750 139542.4\n3 99083.3 13000 437750 139544.4\nbe solved in two ways: the \u2018brute force\u2019 approach (above)1 or the \u2018correct\u2019 way,\ndescribed below. Since the third map could only be solved in the correct way,\nanimats that learnt this behaviour also solved the second map in the same way;\nthis accounts for the similar (usually identical) time taken to solve both harder\nmaps. However, the exclusion of either of these maps in the learning process\nwould have lead to sub-optimal behaviour evolving.\nFig. 4. Results from the simulation which solved all the tasks in the fastest time. Map\n1 was learnt in around 6000 tournaments, while maps 2 and 3 took 13000 tournaments\neach.\nThe observed \u2018correct\u2019 behaviour for solving the harder maps can be described\nas follows: When the animats are on grass and not carrying anything they head\nto the nearest stone, whilst avoiding the river and traps. Once they reach a stone\nthey pick it up; they are then situated on grass, but now carrying a stone. Next,\nthey adjust the shunting model so they are attracted to the resource, ignoring\nthe river\/other stones and avoiding traps. Once they reach the river they deposit\na stone; they are now back to being on grass and not carrying. If activity can\npropagate from the resource to the animat (because of a completed bridge) they\nhead to the resource. Otherwise they return to a stone and repeat.\nThe RC problem is particularly di\u000ecult because of the lack of reward avail-\nable leading to long periods of stasis between solving the easy and the harder\nmaps. During this time the animats phenotypic behaviour does not change by\n9much; they keep solving the easy task and failing the harder ones. It is possible\nthat selection pressures are preventing change from taking place gradually as\none might expect. Animats beginning to learn to solve the harder tasks, for ex-\nample by starting to pick up stones, may disrupt and forget previous behaviours\ncausing them to fail the simple task - most likely due to sharing connections for\nthe di\u000berent behaviours [11]. Figure 4 shows the results of the simulation with\nthe quickest time to solve all tasks, and although it is the fastest, the nature of\nthe graph is the same for all runs: the simpler task is learnt quickly, then there is\na stasis period until by chance animats are born that can solve the harder tasks.\nThis advantageous behaviour then quickly propagates through the environment\nuntil the vast majority can solve all tasks.\nFig. 5. A dynamic environment: In Part B the wall is about to move and block the\nanimats path; instantly the animat starts building in a new direction and crosses suc-\ncessfully (Part C)\nOne advantage of the shunting model as shown in [6] was that it was highly\nreactive to a dynamic environment. We tested this ability in our system by\nevaluating animats that had been evolved to solve the three maps. Animats\nwere shown the map in part A of Figure 5. The map has a larger river than they\nhave been shown during evolution. Also, it has a movable wall of traps. The\nanimat begins building a bridge through the shortest route of the river (part\nB); once the animat gets halfway across however the wall is moved, blocking\nthe animats path (part C). Without hesitation the animat continues to build its\nbridge in the new required direction and navigates across the bridge.\n4 Conclusions and Future Work\nWe have developed and presented a model that allows animats to develop com-\nplex behaviours such as building a bridge in the RC task, by using an incremental\napproach. Through the use of an adapted version of Yang and Meng\u2019s shunting\nmodel animats manipulate an activity landscape by utilising a decision network.\nThis approach allows high level behaviours such as \u2018\fnd a stone without drown-\ning or falling into traps\u2019 to be carried out without any further requirements from\n10\nthe animat other than the evolved desire to do so. Further, animats that are only\nshown the simpler three maps used in the evolutionary process can solve a novel\nand dynamic version of the task. Due to the lack of hard-coded constraints, this\nmodel could be used with many di\u000berent environments without needing to make\nchanges.\nOne draw-back of the current model is that it requires incremental versions\nof the task to be shown to the animat. Using a larger environment could allow\nthese tasks to be situated in the same realm, but we chose to implement them\nas separate worlds. This approach leads to problems as more complex environ-\nments are constructed. To address this issue, future work will include methods\nfor allowing the animats to generate intrinsic motivations, which has been shown\nto be imperative in mental development [12]. The intrinsic motivation will en-\ncourage an animat to solve sub-components of complex problems without the\nneed of an outside critic guiding them.\nReferences\n1. Tyrrell, T.: Computational Mechanisms for Action Selection. PhD thesis, Univer-\nsity of Edinburgh (1993)\n2. Benjamin, M.R.: Virtues and limitations of multifusion based action selection.\nIn: Agents \u201900: The Fourth International Conference on Intelligent Agents. (2000)\n23{24\n3. Payton, D.W., Rosenblatt, J.K., Keirsey, D.M.: Plan guided reaction. IEEE Trans.\non Systems, Man, and Cybernetics 20(6) (1990) 1370{1382\n4. Koren, Y., Borenstein, J.: Potential \feld methods and their inherent limitations\nfor mobile robot navigation. In: IEEE Int. Conf. on Robotics and Automation.\n(1991) 1398{1404\n5. Brooks, R.A.: A robust layered control system for a mobile robot. IEEE J. Robot.\nand Auto. 2(3) (1986) 14{23\n6. Yang, S.X., Meng, M.: An e\u000ecient neural network approach to dynamic robot\nmotion planning. Neural Networks 13(2) (2000) 143{148\n7. Yang, S.X., Meng, M.: An e\u000ecient neural network method for real-time motion\nplanning with safety consideration. Robotics and Autonomous Systems 32(2-3)\n(2000) 115{128\n8. Schultz, A.C.: Adapting the evaluation space to improve global learning. In Belew,\nR., Booker, L., eds.: Proceedings of the Fourth International Conference on Genetic\nAlgorithms, San Mateo, CA, Morgan Kaufman (1991) 158{164\n9. Hodgkin, A.L., Huxley, A.F.: A quantitative description of membrane current and\nits application to conduction and excitation in nerve. Journal of Physiology. 116\n(1952) 500{544\n10. Grossberg, S.: Nonlinear neural networks: Principles, mechanisms, and architec-\ntures. Neural Networks 1 (1988) 17{61\n11. Seipone, T., Bullinaria, J.: The evolution of minimal catastrophic forgetting in\nneural systems. In Mahwah, N., ed.: Twenty-Seventh Annual Conference of the\nCognitive Science Society, 1991-1996, Lawrence Erlbaum Associates (2005)\n12. White, R.W.: Motivation reconsidered: The concept of competence. Psychological\nReview 66(5) (1959) 297{333\n"}