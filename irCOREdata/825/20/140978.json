{"doi":"10.1016\/j.jprocont.2007.10.012","coreId":"140978","oai":"oai:dspace.lib.cranfield.ac.uk:1826\/3071","identifiers":["oai:dspace.lib.cranfield.ac.uk:1826\/3071","10.1016\/j.jprocont.2007.10.012"],"title":"Nonlinear system identification for predictive control using continuous time\nrecurrent neural networks and automatic differentiation.","authors":["Al Seyab, Rihab Khalid Shakir","Cao, Yi"],"enrichments":{"references":[{"id":37923311,"title":"ADOL-C: A Package for the Automatic Dierentiation of Algorithms written in C\/C++&quot;,","authors":[],"date":"1996","doi":"10.1145\/229473.229474","raw":"Griewank A., David J., Jean U., \\ADOL-C: A Package for the Automatic Dierentiation of Algorithms written in C\/C++&quot;, ACM Transactions on Mathematical Software, vol. 22(2), pp. 131 { 167, 1996.","cites":null},{"id":37923278,"title":"An Identi Approach to Nonlinear State Space Model for Industrial Multivariable Model Predictive Control&quot;,","authors":[],"date":"1998","doi":"10.1109\/acc.1998.703517","raw":"Zhao H., Guiver J., and Sentoni G., \\An Identication Approach to Nonlinear State Space Model for Industrial Multivariable Model Predictive Control&quot;, Proceeding of the American Control Conference, Philadelphia, Pennsylvania, 1998.","cites":null},{"id":37923319,"title":"and the proposed NMPC,","authors":[],"date":null,"doi":null,"raw":"and the proposed NMPC, 4T = 1 min. . . . . . . . . . . 40 9 Evaporator performance using the present NMPC at setpoint changes plus random disturbances test. (a){(c) Measured outputs with setpoints. (d){(f) Manipulated variables. (g){(j) Disturbances, 4T = 1 min. . . . . . . . . . . . . . . . . . . . 41 32Steam             T100 F100              P100 Evaporator Condensate Feed F1, X1, T1 F3 Separator P2, L2 Product F2, X2, T2 T201 T200 F200 Cooling water Condenser","cites":null},{"id":37923304,"title":"Applied Process Control-A Case Study&quot;,","authors":[],"date":"1989","doi":null,"raw":"Newell R. B. and P. L. Lee, \\Applied Process Control-A Case Study&quot;, Prentice Hall, Englewood Clis, NJ, 1989. 29[28] Garces F., Kambhampati C., and Warwick K., \\Dynamic Recurrent Neural Networks for Identication of a Multivariable Nonlinear Evaporator Systems&quot;, Proceedings DYCONS'99, World Scientic, 1999.","cites":null},{"id":37923284,"title":"Approximation of Discrete-Time Statespace Trajectories using Dynamic Recurrent Neural Networks&quot;,","authors":[],"date":"1995","doi":"10.1109\/icnn.1995.488134","raw":"Jin L., Nikiforuk P. , Gupta M., \\Approximation of Discrete-Time Statespace Trajectories using Dynamic Recurrent Neural Networks&quot;, IEEE Transactions on Automatic Control, vol. 40(7), pp. 1266-1270, 1995.","cites":null},{"id":37923279,"title":"Approximation of Dynamical Systems by Continous time Recurrent Neural Networks&quot;,","authors":[],"date":"1993","doi":"10.1016\/s0893-6080(05)80125-x","raw":"Funahashi K. L. and Nakamura Y., \\Approximation of Dynamical Systems by Continous time Recurrent Neural Networks&quot;, Neural Networks, vol. 6, pp. 183 { 192, 1993.","cites":null},{"id":37923289,"title":"Approximation of Nonautonomouous Dynamic Systems by Continuous Time Recurrent Neural Networks&quot;,","authors":[],"date":"2000","doi":"10.1109\/ijcnn.2000.857815","raw":"Kambhampati C., Garces F., Warwick K., \\Approximation of Nonautonomouous Dynamic Systems by Continuous Time Recurrent Neural Networks&quot;, Proceeding of the IEEE-INNS-ENNS International Joint Conference on Neural Networks IJCNN 2000, vol. 1, pp. 64 { 69, 2000.","cites":null},{"id":37923283,"title":"Arti Neural Networks for Nonlinear Process Identi and Control&quot;, Nonlinear Process","authors":[],"date":"1997","doi":null,"raw":"Su H. T. and McAvoy T. J., \\Articial Neural Networks for Nonlinear Process Identication and Control&quot;, Nonlinear Process Control, M. A. Henson and D. E. Seborg (eds) Prentic Hall, NJ, pp. 371 { 428, 1997.","cites":null},{"id":37923315,"title":"Correlation based model validity tests for nonlinear models&quot;,","authors":[],"date":"1986","doi":null,"raw":"Billings S. A. and Voon W. S. F., \\Correlation based model validity tests for nonlinear models&quot;, Int. J. Control, vol. 44, pp. 235{244, 1986.","cites":null},{"id":37923285,"title":"Dynamic Recurrent Neural Network for System Identi and Control&quot;,","authors":[],"date":"1995","doi":"10.1049\/ip-cta:19951873","raw":"Delgado A., Kambhampati C., Warwick K., \\Dynamic Recurrent Neural Network for System Identication and Control&quot;, IEE Proc. Control Theory Appl., vol. 142(4), pp. 307-314. 1995.","cites":null},{"id":37923305,"title":"Dynamic Recurrent Neural Networks for Feedback Linearization of a Multivariable Nonlinear Evaporator Systems&quot;,","authors":[],"date":"2000","doi":null,"raw":"Garces F., Kambhampati C., and Warwick K., \\Dynamic Recurrent Neural Networks for Feedback Linearization of a Multivariable Nonlinear Evaporator Systems&quot;, In Review for UKACC International Conference Control, 2000.","cites":null},{"id":37923297,"title":"Evaluating Gradients in Optimal Control: Continuous Adjoint Versus Automatic Dierentiation&quot;,","authors":[],"date":"2004","doi":"10.1023\/b:jota.0000041731.71309.f1","raw":"Griesse R., Walther A., \\Evaluating Gradients in Optimal Control: Continuous Adjoint Versus Automatic Dierentiation&quot;, Journal of Optimization Theory and Applications, vol. 122(1), pp. 63 { 86, 2004.","cites":null},{"id":37923309,"title":"Evauating Derivatives&quot;,","authors":[],"date":"2000","doi":null,"raw":"Griewank A., &quot;Evauating Derivatives&quot;, SIAM, Philadelphia, PA, 2000.","cites":null},{"id":37923303,"title":"Formulation of nNonlinear Model Predictive Control using Automatic Dierentiation&quot;,","authors":[],"date":"2005","doi":null,"raw":"Cao Y.,\\A Formulation of nNonlinear Model Predictive Control using Automatic Dierentiation&quot;, Journal of Process Control, vol. 15, pp. 851 { 858, 2005.","cites":null},{"id":37923294,"title":"Genetic Algorithms in Search, Optimization and Machine Learning&quot;,","authors":[],"date":"1989","doi":null,"raw":"Goldberge D. E., \\Genetic Algorithms in Search, Optimization and Machine Learning&quot;, Reading, MA:Addision-Wesley, 1989.","cites":null},{"id":37923282,"title":"Identi and Control of Dynamical Systems using Neural Networks&quot;,","authors":[],"date":"1990","doi":"10.1109\/72.80202","raw":"Narendra K. S. and Parthasarathy K., &quot;Identication and Control of Dynamical Systems using Neural Networks&quot;, IEEE Trans, Neural Networks, vol. 1(1), pp. 4 { 26, 1990.","cites":null},{"id":37923293,"title":"Improvement of the Backpropagation Algorithm for Training Neural Networks&quot;,","authors":[],"date":"1990","doi":"10.1016\/0098-1354(90)87070-6","raw":"Leonard J. A. and Kramer M. A., \\Improvement of the Backpropagation Algorithm for Training Neural Networks&quot;, Computers and Chemical Eng., vol. 14, pp. 337 { 341, 1990. 28[20] Marquardt D., \\An Algorithm for least-Square Estimation of Nonlinear Parameters&quot; SIAM J. Appl. Math., vol. 11 , pp. 431 { 441 , 1963.","cites":null},{"id":37923288,"title":"Inverse Model Control using Recurrent Networks&quot;,","authors":[],"date":"2000","doi":"10.1016\/s0378-4754(99)00116-0","raw":"Kambhampati C., Craddock R. J., Tham M., Warwick K., \\Inverse Model Control using Recurrent Networks&quot;, Mathematics and Computers in Simulation, vol. (51), pp. 181-199, 2000.","cites":null},{"id":37923292,"title":"Learning Internal Representiations by Error Propagation&quot;,","authors":[],"date":"1986","doi":"10.1016\/b978-1-4832-1446-7.50035-2","raw":"Rumelhart D. E., Hinton G. E., and Williams R. J., \\Learning Internal Representiations by Error Propagation&quot;, in Prallel Distributed Processing, D. E. Rumelhart and J. L. MeClelland, Eds., Cambrige MA:MIT Press, 1986.","cites":null},{"id":37923306,"title":"Linear System Theory and Design&quot;, third ed.,","authors":[],"date":"1999","doi":null,"raw":"Chen C. T., \\Linear System Theory and Design&quot;, third ed., Oxford University Press, New York, 1999.","cites":null},{"id":37923280,"title":"Model Predictive Control of an Industrial Packed Reactors using Neural Networks&quot;,","authors":[],"date":"1995","doi":"10.1016\/0959-1524(95)95942-7","raw":"Temeng H., Schenelle P., and McAvoy T., \\Model Predictive Control of an Industrial Packed Reactors using Neural Networks&quot;, J. Proc. Control, vol. 5(1), pp. 19 { 28, 1995. 26[5] Tan Y., and Cauwenberghe A., \\Nonlinear One Step Ahead Control using Neural Networks:Control Strategy and Stability Design&quot;, Automatica, vol. 32(12), pp. 1701 { 1706, 1996.","cites":null},{"id":37923281,"title":"Neural Networks for Control&quot;,","authors":[],"date":"1990","doi":"10.1017\/s0263574700000746","raw":"Miller W. T., Sutton R. S., and Werbos P. J., &quot;Neural Networks for Control&quot;, MIT Press, Cambridge, MA, 1990.","cites":null},{"id":37923302,"title":"Nonlinear Model Predictive Control using Automatic Dierentiation&quot;,","authors":[],"date":"2003","doi":null,"raw":"Cao Y., and A-Seyab R., \\Nonlinear Model Predictive Control using Automatic Dierentiation&quot;, European Control Conference (ECC 2003), Cambridge, UK, 2003, p. in CDROM.","cites":null},{"id":37923291,"title":"Nonlinear System Identi and Model Reduction using Arti Neural Networks&quot;,","authors":[],"date":"2003","doi":"10.1016\/s0098-1354(03)00137-6","raw":"Prasad V., and Bequette B. W., \\Nonlinear System Identication and Model Reduction using Articial Neural Networks&quot;, Computers and Chemical Engineering, vol. 27, pp. 1741-1754, 2003.","cites":null},{"id":37923295,"title":"Obtaining Sensitivity Information in Dynamic Optimization Problems Solved by the Sequential Approach&quot;,","authors":[],"date":"1999","doi":"10.1016\/s0098-1354(99)00010-1","raw":"Storen S., Hertzberg T., \\Obtaining Sensitivity Information in Dynamic Optimization Problems Solved by the Sequential Approach&quot;, Computer and Chemical Engineering, vol. 23, pp. 807 { 819, 1999.","cites":null},{"id":37923316,"title":"Predictive control with constraints&quot;,","authors":[],"date":"2002","doi":"10.1109\/cacsd.2002.1036925","raw":"Maciejowski J. M., \\Predictive control with constraints&quot;, Prentice Hall, Harlow, England, 2002.","cites":null},{"id":37923286,"title":"Progress in Supervised Neural Networks&quot;,","authors":[],"date":"1993","doi":"10.1109\/79.180705","raw":"Hush D. R. and Horne B. G., \\Progress in Supervised Neural Networks&quot;, IEEE Sig. Process, Mag., vol. 1, pp. 8 { 39, 1993.","cites":null},{"id":37923307,"title":"Reverse Accumulation and Accurate Rounding Error Estimatres for Taylor Series&quot;,","authors":[],"date":"1992","doi":"10.1080\/10556789208805508","raw":"B. Christianson, \\Reverse Accumulation and Accurate Rounding Error Estimatres for Taylor Series&quot;, Optimization Methods and Software, vol. 1, pp. 81 { 94, 1992.","cites":null},{"id":37923263,"title":"Selecting Nonlinear Model Structures for Computer Control: Review&quot;,","authors":[],"date":"2003","doi":"10.1016\/s0959-1524(02)00022-7","raw":"Pearson R. K., \\Selecting Nonlinear Model Structures for Computer Control: Review&quot;, Journal of Process Control, vol. 13, pp. 1-26, 2003.","cites":null},{"id":37923296,"title":"Sensitivity Analysis of Linearly Implicit Dierential-Algebraic Systems by One-step Extrapolation&quot;,","authors":[],"date":"2004","doi":"10.1016\/j.apnum.2003.07.001","raw":"Schlegel M., Marquardt W., Ehrig R., Nowak U., \\Sensitivity Analysis of Linearly Implicit Dierential-Algebraic Systems by One-step Extrapolation&quot;, Applied Numerical Mathematics, vol. 48, pp. 83 { 102, 2004.","cites":null},{"id":37923313,"title":"Solving via Automatic Dierentiation and Rational Prediction&quot;, in: Griths","authors":[],"date":"1995","doi":null,"raw":"Griewank A., \\ODE Solving via Automatic Dierentiation and Rational Prediction&quot;, in: Griths D., Watson G. (Eds.), Numerical Analysis 1995, vol. 344 of Pitman Research Nots in Mathematics Series, AddisonWesley., Reading, MA, 1995. 30[35] Zhang J., and Morris J., \\Recurrent neuro{fuzzy networks for nonlinear process modeling&quot;, IEEE Trans. on Neural Networks, vol. 10(2), pp. 313{ 326, 1999.","cites":null},{"id":37923318,"title":"Stability analysis of constrained nonlinear model predictive control with terminal weighting&quot;, submitted to International Journal of Robust and Nonlinear Control,","authors":[],"date":"2007","doi":"10.1002\/asjc.486","raw":"Chen, W.-H. and Cao, Y., \\Stability analysis of constrained nonlinear model predictive control with terminal weighting&quot;, submitted to International Journal of Robust and Nonlinear Control, 2007. 31List of Figures 1 Evaporator System . . . . . . . . . . . . . . . . . . . . . . . . 33 2 Recurrent Neural Network Structure . . . . . . . . . . . . . . 34 3 Training data set, Inputs, 4T = 0:2 min. . . . . . . . . . . . . 35 4 Training data set, Outputs, 4T = 0:2 min. . . . . . . . . . . . 36 5 Validation data set, Outputs, 4T = 0:05 min. . . . . . . . . . 37 6 Validation tests\/Autocorrelation coe. for error, 4T = 0:05 min. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 7 Validation tests\/Cross-correlation coef. of U, 4T = 0:05 min. 39 8 Evaporator performance at setpoints ramp changes using LMPC","cites":null},{"id":37923287,"title":"State-Space Neural Network, Properties and Application&quot;,","authors":[],"date":"1998","doi":"10.1016\/s0893-6080(98)00074-4","raw":"Zamarreno J. M. and Vega P., \\State-Space Neural Network, Properties and Application&quot;, Neural Networks, vol. 11, pp. 1099 { 1112, 1998. 27[13] Pearlmutter B. A., \\Gradient Calculations for Dynamic Recurrent Neural Networks: A Survey&quot;, IEEE Transactions on Neural Networks, 1995.","cites":null},{"id":37923290,"title":"The Relative Order and Inverses of Recurrent Networks&quot;,","authors":[],"date":"1996","doi":"10.1016\/0005-1098(95)00098-4","raw":"Kambhampati C., Manchanda S., Delgado A., Green G., Warwick K., Tham M., \\The Relative Order and Inverses of Recurrent Networks&quot;, Automatica, vol. 32(1), pp. 117-123, 1996.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2008-07-01T00:00:00Z","abstract":"In this paper, a continuous time recurrent neural network (CTRNN) is developed\nto be used in nonlinear model predictive control (NMPC) context. The neural\nnetwork represented in a general nonlinear state-space form is used to predict\nthe future dynamic behavior of the nonlinear process in real time. An efficient\ntraining algorithm for the proposed network is developed using automatic\ndifferentiation (AD) techniques. By automatically generating Taylor\ncoefficients, the algorithm not only solves the differentiation equations of the\nnetwork but also produces the sensitivity for the training problem. The same\napproach is also used to solve the online optimization problem in the predictive\ncontroller. The proposed neural network and the nonlinear predictive controller\nwere tested on an evaporation case study. A good model fitting for the nonlinear\nplant is obtained using the new method. A comparison with other approaches shows\nthat the new algorithm can considerably reduce network training time and improve\nsolution accuracy. The CTRNN trained is used as an internal model in a\npredictive controller and results in good performance under different operating\nconditions","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/140978.pdf","fullTextIdentifier":"http:\/\/dx.doi.org\/10.1016\/j.jprocont.2007.10.012","pdfHashValue":"8f3beee623b17c5595de11903725b637c6f824fb","publisher":"Elsevier Science B.V., Amsterdam.","rawRecordXml":"<record><header><identifier>\noai:dspace.lib.cranfield.ac.uk:1826\/3071<\/identifier><datestamp>2011-11-14T12:49:18Z<\/datestamp><setSpec>hdl_1826_19<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>Nonlinear system identification for predictive control using continuous time\nrecurrent neural networks and automatic differentiation.<\/dc:title><dc:creator>Al Seyab, Rihab Khalid Shakir<\/dc:creator><dc:creator>Cao, Yi<\/dc:creator><dc:subject>Nonlinear system<\/dc:subject><dc:subject>System identification<\/dc:subject><dc:subject>Predictive control<\/dc:subject><dc:subject>Recurrent neural network<\/dc:subject><dc:subject>Automatic differentiation<\/dc:subject><dc:description>In this paper, a continuous time recurrent neural network (CTRNN) is developed\nto be used in nonlinear model predictive control (NMPC) context. The neural\nnetwork represented in a general nonlinear state-space form is used to predict\nthe future dynamic behavior of the nonlinear process in real time. An efficient\ntraining algorithm for the proposed network is developed using automatic\ndifferentiation (AD) techniques. By automatically generating Taylor\ncoefficients, the algorithm not only solves the differentiation equations of the\nnetwork but also produces the sensitivity for the training problem. The same\napproach is also used to solve the online optimization problem in the predictive\ncontroller. The proposed neural network and the nonlinear predictive controller\nwere tested on an evaporation case study. A good model fitting for the nonlinear\nplant is obtained using the new method. A comparison with other approaches shows\nthat the new algorithm can considerably reduce network training time and improve\nsolution accuracy. The CTRNN trained is used as an internal model in a\npredictive controller and results in good performance under different operating\nconditions.<\/dc:description><dc:publisher>Elsevier Science B.V., Amsterdam.<\/dc:publisher><dc:date>2011-11-13T23:22:40Z<\/dc:date><dc:date>2011-11-13T23:22:40Z<\/dc:date><dc:date>2008-07-01T00:00:00Z<\/dc:date><dc:type>Article<\/dc:type><dc:identifier>R.K. Al Seyab, Y. Cao, Nonlinear system identification for predictive control\nusing continuous time recurrent neural networks and automatic differentiation,\nJournal of Process Control, Volume 18, Issue 6, July 2008, Pages 568-581<\/dc:identifier><dc:identifier>0959-1524<\/dc:identifier><dc:identifier>http:\/\/dx.doi.org\/10.1016\/j.jprocont.2007.10.012<\/dc:identifier><dc:identifier>http:\/\/dspace.lib.cranfield.ac.uk\/handle\/1826\/3071<\/dc:identifier><dc:language>en_UK<\/dc:language><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["issn:0959-1524","0959-1524"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2008,"topics":["Nonlinear system","System identification","Predictive control","Recurrent neural network","Automatic differentiation"],"subject":["Article"],"fullText":"Nonlinear System Identification for Predictive\nControl using Continuous Time Recurrent\nNeural Networks and Automatic\nDifferentiation.\nR K Al Seyab Y Cao \u2217\nCranfield University, UK\nKeywords: Nonlinear System, System Identification, Predictive Control,\nRecurrent Neural Network, Automatic Differentiation.\nAbstract\nIn this paper, a continuous time recurrent neural network (CTRNN)\nis developed to be used in nonlinear model predictive control (NMPC)\ncontext. The neural network represented in a general nonlinear state-\nspace form is used to predict the future dynamic behavior of the non-\nlinear process in real time. An efficient training algorithm for the\nproposed network is developed using automatic differentiation (AD)\ntechniques. By automatically generating Taylor coefficients, the al-\ngorithm not only solves the differentiation equations of the network\nbut also produces the sensitivity for the training problem. The same\napproach is also used to solve the online optimization problem in the\npredictive controller. The proposed neural network and the nonlin-\near predictive controller were tested on an evaporation case study. A\ngood model fitting for the nonlinear plant is obtained using the new\nmethod. A comparison with other approaches shows that the new\nalgorithm can considerably reduce network training time and improve\nsolution accuracy. The CTRNN trained is used as an internal model in\na predictive controller and results in good performance under different\noperating conditions.\n\u2217To whom correspondence should be addressed (y.cao@cranfield.ac.uk).\n1 Introduction\nModel Predictive Control (MPC) is proving its continuous success in indus-\ntrial applications particularly in the presence of constraints and varying op-\nerating conditions, thereby allowing processes to operate at the limits of their\nachievable performance. The basic control strategy in MPC is the selection\nof a set of future control moves (control horizon) and minimize a cost func-\ntion based on the desired output trajectory over a prediction horizon with\na chosen length. This requires a reasonably accurate internal model, that\ncaptures the essential nonlinearities of the process under control, to predict\nmulti-step ahead dynamic behavior [1].\nIn many reported applications of MPC, a linear model is assumed. How-\never, MPC based on linear models, often results in poor control performance\nfor highly nonlinear processes because of the inadequateness of a linear model\nto predict dynamic behavior of a nonlinear process. There is therefore, a\nstrong requirement of a good fitting model for NMPC applications.\nIn many practical applications, a restrict mathematical model based on\nphysical principles is either unknown or too complicated to be used for con-\ntrol. In this case, nonlinear system identification is an inevitable step in a\nNMPC project. Possibly, it is also the most costly and time consuming part\nof the project [2]. Therefore, an efficient and effective approach of nonlinear\nsystem identification is critical to the success of NMPC.\nUnlike linear system identification, there is no uniform way to parame-\nterize general nonlinear dynamic systems. Among existing techniques, the\nuniversal approximation properties of neural networks makes them a powerful\n2\ntool for modelling nonlinear systems [3]. The structure of neural networks\nmay be classified as feedforward and recurrent. Most of the publications\nin nonlinear system identification use feedforward neural networks (FFNNs)\nwith backpropagation or its other variations for training, for example [4, 5].\nThe main drawback of this approach is that it can only provide predictions\nfor a predetermined finite number of steps, in most cases, only one step. This\ndrawback makes such models not well suitable for predictive control, where\nvariable multi-step predictions are desired.\nRecurrent neural networks (RNNs) on the other hand are capable of pro-\nviding long range predictions even in the presence of measurements noise [8].\nTherefore, RNN models are better suited for NMPC. RNNs with internal\ndynamics are adopted in several recent works. Models with such networks\nare shown [3, 9], to have the capability of capturing various plant nonlin-\nearities. They have been shown more efficient than FFNNs in terms of the\nnumber of neurons required to model a dynamic system [10, 11]. In addition,\nthey are more suitable to be represented in state-space format, which is quite\ncommonly used in most control algorithms [12].\nIn this work, a continuous time version of the recurrent neural networks\n(CTRNNs) in state-space form is used as the internal model of NMPC. The\ncontinuous time RNN brings further advantages and computational efficiency\nover the discrete formulation even if at the end both are represented on the\ncomputer using only discrete values [13]. Using a discrete time RNNs causes\na great dependence of the resulting models on the sampling period used in the\nprocess and no information is given about the model trajectories between the\nsampling instants. The sampling period used with CTRNNs, on the other\n3\nhand, can be varied without the need for re-training [14, 15].\nThe main difficulty with recurrent neural networks is their training [13,\n16, 17]. Various training strategies have been suggested in the literature,\nsuch as the backpropagation method [18], the conjugate gradient method\n[19], Levenberg-Marquardt optimization [20], or methods based on genetic\nalgorithm (GAs) [21]. To solve the nonlinear optimization problem associated\nwith CTRNN training, the calculation of a large number of dynamic sensitiv-\nity equations is required. Depending on the number of sensitivity equations\ninvolved, the sensitivity calculation could take more than 90 percent of the\ntotal computation time required for solving a training problem. Hence, sen-\nsitivity calculation is a bottleneck in training CTRNNs. Ways to find the\nsensitivity of a dynamic system [22] are: perturbation, sensitivity equations,\nand adjoint equations. In a perturbation approach, finite difference (FD)\nis used to approximate derivatives. Hence at least N perturbations to the\ndynamic system are needed to get the solution of a N -parameter sensitivity\nproblem [22]. Alternatively, sensitivity can also be obtained by simultane-\nously solving the original ordinary differential equations (ODEs) together\nwith nN sensitivity equations, where n is the number of states [23]. Fi-\nnally, sensitivity can be calculated by solving n adjoint equations (in reverse\ndirection).\nRecently, the automatic differentiation (AD) techniques have been ap-\nplied to tackle the dynamic optimization problem [24]. In our previous work,\n[25], a first-order approximation was derived using AD to simplify the dy-\nnamic sensitivity equations associated with a NMPC problem so that com-\nputation efficiency was improved. In most published work using AD for\n4\ndynamic optimization, AD has only been used to generate low (first and\/or\nsecond) order derivatives. Recently, AD techniques have been used to solve\nODEs and sensitivity equations using high-order Taylor series in a NMPC\nformulation [26]. In this work, the approach of [26] is extended to solve both\nthe CTRNN training and associated NMPC control problems to speed up\ncalculations and to increase efficiency. Both training and NMPC algorithms\nare applied to an evaporator process [27]. The network training time is signif-\nicantly reduced by using the new algorithm comparing with other methods.\nUsing the trained CTRNN as its internal model, the NMPC controller gives\nsatisfactory control performance at different operating conditions.\nThe paper is organized as follows. After the introduction, a CTRNN\ntraining algorithm is discussed in section 2. The details of the MPC algorithm\nare presented in section 3. Section 4 dedicates to the evaporator case study\nincluding its nonlinear system identification using CTRNN, the predictive\ncontroller design and simulation results. In section 5 some conclusions are\ndrawn from the work.\n5\n2 CTRNN Training\n2.1 Neural network model\nIt has been proven that CTRNNs are able to approximate trajectories gen-\nerated by nonlinear dynamic systems given by:\nx\u02d9 = f(x, u) (1)\ny = g(x)\nA key to the approximating capabilities of this type of networks is the use of\nhidden neurons, [10, 28, 29]. There are many types of neural networks from\nmulti-layer perceptrons (MLP) to radial basis functions (RBF), which can\nbe constructed as recurrent networks to approximate the nonlinear system\n(1). The training algorithm to be discussed is suitable for any kind networks.\nHence, the CTRNN to be considered is represented in the following general\nform.\n\u02d9\u02c6x(t) = f\u02c6(x\u02c6(t), u(t), \u03b8) (2)\ny\u02c6(t) = Cx\u02c6(t)\nwhere u(t) \u2208 Rnu is the external input, y\u02c6 \u2208 Rny the network output, x\u02c6 \u2208 Rnx\u02c6\nthe network\u2019s state vector, \u03b8 \u2208 Rn\u03b8 the network parameter vector and the\n6\noutput matrix C is fixed as\nC =\n[\nIny\u00d7ny , \u2205ny\u00d7(nx\u02c6\u2212ny)\n]\n(3)\ni.e. outputs are the first ny states of the networks.\nA particular example of (2), which will be used for the case study later,\nis shown in Figure 2, where a MLP network is adopted to construct the\nrecurrent neural network of (2).\n2.2 CTRNN sensitivity calculation using AD\nThe definition of the sensitivity is the variation of the network output against\nthe variation of \u03b7, where \u03b7 \u2208 Rn\u03b7 represents the general parameters, \u03b7 = \u03b8\nin training cases, and \u03b7 = u(t) in NMPC, whilst in other cases, \u03b7 may also\ninclude the initial state, x\u02c6(0). Assume the function f\u02c6 is d-time continu-\nously differentiable. Then, the sensitivity can be calculated by taking partial\nderivative for both sides of equations (2):\nx\u02d9\u03b7(t) = fxx\u03b7(t) + f\u03b7 (4)\ny\u03b7(t) = Cx\u03b7(t)\nwhere, x\u03b7 := \u2202x\u02c6\/\u2202\u03b7, y\u03b7 := \u2202y\u02c6\/\u2202\u03b7, fx := \u2202f\u02c6\/\u2202x\u02c6, and f\u03b7 := \u2202f\u02c6\/\u2202\u03b7.\nEquation (4) is a linear time-varying system with initial condition, x\u03b7(0) =\n\u2202x\u02c6(0)\/\u2202\u03b7. Generally, system (4) has no analytical solution although it can be\nrepresented in a state-transition matrix form [30]. The dynamic sensitivity\n7\nfunction x\u03b7 can be calculated using different method as mentioned earlier.\nNumerically, equation (4) can be solved together with the state equation (2)\nusing a differential equation solver. The total number of differential variables\nto be solved at each time instant is nx\u02c6 \u00d7 (1 + n\u03b7). Depend on the size of a\nnetwork, this number of differential variables could growth so large that the\ncalculation causes a significant burden on network training. To tackle this\nproblem, the sensitivity calculation method proposed in [26] is extended for\nCTRNNs.\nTo solve differential equations (2) and (4), an integration step has to\nbe determined. Normally, the integration step should be shorter than the\nsampling period to get accurate results. However, for the approach devel-\noped here, the accuracy can be maintained by adjusting the Taylor series\norder, d. Moreover, for the identification problem, there is no information\navailable between two sampling points to compare integration results if a\nshorter integration step is adopted. Therefore, for simplicity and efficiency,\nthe integration step is selected to be the same as the sampling period in this\nwork.\nUsing normalized time, \u03c4 = t\/h, where h is the sampling period, the right-\nhand-side of the state equation becomes z(x\u02c6(\u03c4), \u03b7(\u03c4)) := hf\u02c6(x\u02c6(\u03c4), \u03b7(\u03c4)) and\nthe solution interval is 0 \u2264 \u03c4 \u2264 1 for each integration step. Consider x\u02c6(\u03c4)\nand \u03b7(\u03c4) are given by the truncated Taylor series:\nx\u02c6(\u03c4) = x\u02c6[0] + x\u02c6[1]\u03c4 + \u00b7 \u00b7 \u00b7+ x\u02c6[d]\u03c4 d (5)\n\u03b7(\u03c4) = \u03b7[0] + \u03b7[1]\u03c4 + \u00b7 \u00b7 \u00b7+ \u03b7[s]\u03c4 s, s \u2264 d (6)\n8\nwith coefficients x\u02c6[i] \u2208 Rnx\u02c6 , and \u03b7[i] \u2208 Rn\u03b7 given as follows respectively:\nx\u02c6[i] = (i!)\n\u22121\u2202\nix\u02c6(\u03c4)\n\u2202\u03c4 i\n|\u03c4=0 (7)\n\u03b7[i] = (i!)\n\u22121\u2202\ni\u03b7(\u03c4)\n\u2202\u03c4 i\n|\u03c4=0 (8)\nLet v = [\u03b7T[0] \u00b7 \u00b7 \u00b7 \u03b7T[s]]T , then, z(\u03c4) = z(x\u02c6(\u03c4), v) can be expressed by a Taylor\nexpansion:\nz(\u03c4) = z[0] + z[1]\u03c4 + \u00b7 \u00b7 \u00b7+ z[d]\u03c4 d + O(\u03c4 d+1) (9)\nwhere coefficients z[j] is given as;\nz[j] = (j!)\n\u22121\u2202\njz(\u03c4)\n\u2202\u03c4 j\n|\u03c4=0 (10)\nFrom the chain rule, z[j] is uniquely determined by the coefficient vectors, x\u02c6[i]\nand v with i \u2264 j, i.e.\nz[j] \u2261 z[j](x\u02c6[0], x\u02c6[1], \u00b7 \u00b7 \u00b7 , x\u02c6[j], v) (11)\nNevertheless, inherently, functions z[j] are also d-time continuously differen-\ntiable and their derivatives satisfy the identity [31];\n\u2202z[j]\n\u2202x\u02c6[i]\n=\n\u2202z[j\u2212i]\n\u2202x\u02c6[0]\n:= A[j\u2212i],x\u02c6 \u2261 A[j\u2212i],x\u02c6(x\u02c6[0], x\u02c6[1], \u00b7 \u00b7 \u00b7 , x\u02c6[j\u2212i], v) (12)\n\u2202z[j\u2212i]\n\u2202v\n:= A[j\u2212i],v \u2261 A[j\u2212i],v(x\u02c6[0], x\u02c6[1], \u00b7 \u00b7 \u00b7 , x\u02c6[j\u2212i], v) (13)\n9\nwhere, A[j]x \u2208 Rnx\u02c6\u00d7nx\u02c6 , j = 0, \u00b7 \u00b7 \u00b7 , d, and A[j]v \u2208 Rnx\u02c6\u00d7sn\u03b7 , j = 0, \u00b7 \u00b7 \u00b7 , d are\nalso the Taylor coefficients of the Jacobian path, i.e.;\n\u2202z\n\u2202x\u02c6[0]\n= A[0]x + A[1]x\u03c4 + \u00b7 \u00b7 \u00b7+ A[d]x\u03c4 d + O\u03c4 d+1 (14)\n\u2202z\n\u2202v\n= A[0]v + A[1]v\u03c4 + \u00b7 \u00b7 \u00b7+ A[d]v\u03c4 d + O\u03c4 d+1 (15)\nAD techniques provide an efficient way to calculate these coefficients vectors,\nz[j] and matrices A[i] [32]. For example, with the software package, ADOL-C\n[33, 34], using the forward mode of AD all Taylor coefficient vectors for a\ngiven degree, d can be calculated simultaneously, whilst the matrices, A[i]\ncan be obtained using the reverse mode of AD. The run time and memory\nrequirement associated with these calculations grow only as d2.\nUsing AD for the CTRNN system (2), the Taylor coefficients of x\u02c6(\u03c4) can\nbe iteratively determined from x\u02c6[0] and v [26]:\nx\u02c6[k+1] =\n1\nk + 1\nz[k](x\u02c6[0], \u00b7 \u00b7 \u00b7 , x\u02c6[k], v), k = 0, \u00b7 \u00b7 \u00b7 , d\u2212 1 (16)\ny\u02c6[k] = Cx\u02c6[k], k = 0, \u00b7 \u00b7 \u00b7 , d (17)\nThen, by applying AD to (16), the partial derivatives are obtained and par-\ntitioned as;\nA[k] =\n[\nA[k]x | A[k]v\n]\n:=\n[\n\u2202z[k]\n\u2202x\u02c6[0]\n| \u2202z[k]\n\u2202v\n]\n, (18)\nThe total derivatives are accumulated from these partial derivatives as fol-\n10\nlows:\nB[k] =\n[\nB[k]x | B[k]v\n]\n:=\n[\ndx\u02c6[k]\ndx\u02c6[0]\n| dx\u02c6[k]\ndv\n]\n=\n1\nk\n(\nA[k\u22121] +\n\u2211k\u22121\nj=1 A[k\u2212j\u22121]xB[j]\n)\n, k = 1, \u00b7 \u00b7 \u00b7 , d (19)\nNote, B[0] =\n[\nI | B[0]v\n]\n, where B[0]v := \u2202x\u02c6[0]\/\u2202v. In summary, the\nsolutions of system (2) at t = h are;\nx\u02c6(h) =\nd\u2211\ni=0\nx\u02c6[i], y\u02c6(h) = Cx\u02c6(h) (20)\nwhilst their sensitivities to initial value, x\u02c6[0] and coefficients v are,\nBx(h) :=\ndx\u02c6(h)\ndx\u02c6[0]\n=\nd\u2211\ni=0\nB[i]x = I +\nd\u2211\ni=1\nB[i]x (21)\nBv(h) :=\ndx\u02c6(h)\ndv\n=\nd\u2211\ni=0\nB[i]v = B[0]v +\nd\u2211\ni=1\nB[i]v (22)\nDx(h) :=\ndy\u02c6(h)\ndx\u02c6[0]\n= CBx(h) (23)\nDv(h) :=\ndy\u02c6(h)\ndv\n= CBv(h) (24)\n2.3 Network training algorithm\nTraining produces the optimal connection weights for the networks by min-\nimizing a quadratic cost function of the errors between the neural network\noutput and the plant output over the entire set of samples. Among many\nnetwork training algorithms, Levenberge-Marquardt (LM) algorithm [20] is\n11\nknown to be a robust and fast gradient method because of its second-order\nconverging speed without having to compute the Hessian matrix. For this\nreason, the LM algorithm is combined with the sensitivity algorithm using\nAD described above for the dynamic network training.\nFirstly, assume the dynamic system (1) is initially at steady-state. By\nintroducing a set of random inputs to the system, the outputs of the plant are\ncollected with the inputs for N sampling points at sampling rate h. Then,\nthe unknown network parameters \u03b8 are estimated from the input-output data\nset by minimizing the sum of squared approximation errors, i.e.\nmin\n\u03b8\n\u03a6 = min\n\u03b8\n1\n2\nN\u2211\ni=0\neTi ei (25)\nwhere, ei is the error between the actual plant output and the network output\nat i-th sampling point which is a function of the model parameter vector given\nby:\nei \u2261 ei(\u03b8) = y\u02c6(ti, \u03b8)\u2212 y(ti), i = 1, 2, \u00b7 \u00b7 \u00b7 , N (26)\nLet:\nE(\u03b8) =\n[\neT1 \u00b7 \u00b7 \u00b7 eTN\n]T\n(27)\nThe nyN \u00d7 n\u03b8 Jacobian matrix of E is defined as\nJ(\u03b8) :=\n\u2202E(\u03b8)\n\u2202\u03b8\n(28)\n12\nThen, the gradient of \u03a6 is J(\u03b8)E(\u03b8), whilst the Hessian of \u03a6 can be approx-\nimated as JT (\u03b8)J(\u03b8). The training algorithm based on the nonlinear least\nsquare approach of Levenberg Marquandt [20] is:\n\u03b8k+1 = \u03b8k \u2212\n[\nJ(\u03b8)TJ(\u03b8) + \u00b5I\n]\u22121\nJ(\u03b8)TE(\u03b8) (29)\nwhere, \u03b8k+1 is an updated vector of weights and biases, \u03b8k the current weights\nand biases, and I the identity matrix. When the scalar \u00b5 is zero, this is a\nquasi-Newton approach, using the approximate Hessian matrix, JTJ . When\n\u00b5 is large, it is equivalent to a gradient descent method with a small step size.\nQuasi-Newton method is faster and more efficient when \u03a6 is near the error\nminimum. In this way, the performance function \u03a6 will always be reduced\nat each iteration of the algorithm.\nThe Jacobian matrix can be partitioned into N blokes as:\nJ(\u03b8) = [JT1 (\u03b8) \u00b7 \u00b7 \u00b7 JTN(\u03b8)]T (30)\nwhere each block is an ny \u00d7 n\u03b8 matrix as:\nJi(\u03b8) =\n\u2202ei(\u03b8)\n\u2202\u03b8\n=\n\u2202y\u02c6(ti, \u03b8)\n\u2202\u03b8\n= C\n\u2202x\u02c6(ti, \u03b8)\n\u2202\u03b8\n(31)\nFor accurate and fast calculation of the sensitivity equations required for\nthe Jacobian matrix above, the method described in the previous section is\nadopted here. Since \u03b8 is a constant vector, v = \u03b8.\nFor given v, x\u02c6(k + 1) := x\u02c6(tk+1), and y\u02c6(k) := y\u02c6(tk) are iteratively de-\ntermined from x\u02c6(0) = [yT (0), 01\u00d7(nx\u02c6\u2212ny)]\nT using (20). Then the value of\n13\nJk(\u03b8) = dy\u02c6(k)\/dv can be calculated using (19) and (21) \u2013 (24) as:\nBv(0) = 0 = B[0]v(0) (32)\nBx(k) = I +\nd\u2211\ni=1\nB[i]x(k \u2212 1) (33)\nBv(k) = Bv(k \u2212 1) +\nd\u2211\ni=1\nB[i]v(k \u2212 1) = B[0]v(k) (34)\nDv(k) = CBv(k) = Jk(\u03b8) (35)\nHence, with AD, the nonlinear training problem can be efficiently solved\nusing the LM method.\n2.4 Model Validation\nMany model validity tests for nonlinear models have been developed [35], for\nexample, the Akaike information criterion (AIC), the statistical \u03c72 tests, the\npredicted squared error criterion, and the higher\u2013order correlation tests.\nThe most common method of validation is to investigate the residual\n(prediction errors) by cross validation on a test data set. Here, validation\nis done by carrying out a number of tests on correlation functions, includ-\ning autocorrelation function of the residual and cross-correlation function\nbetween controls and residuals. If the identified model based on CTRNN\nis adequate, the prediction errors should satisfy the following conditions of\n14\nhigh-order correlation tests [36]:\nRee(\u03c4) = E[e(t\u2212 \u03c4)e(t)] = \u03b4(\u03c4), \u2200\u03c4 (36)\nRue(\u03c4) = E[u(t\u2212 \u03c4)e(t)] = 0, \u2200\u03c4 (37)\nwhere Rxz(\u03c4) indicates the cross-correlation function between x(t) and z(t),\ne is the model residual. These tests look into the cross-correlation amongst\nmodel residuals and inputs. These test are normalized to be within a range of\n\u00b11 so that the tests are independent of signal amplitude and easy to interpret\n[36]. The significance of the correlation between variables is indicated by a\nconfidence interval. For a sufficiently large data set with length N , the\n95% confidence bounds are approximately \u00b11.96\/\u221aN . If these correlation\ntests are satisfied (within the confidence limits) then the model residuals are\na random sequence and are not predictable from the model inputs. This\nprovides additional evidence of the validity of the identified model.\n3 Nonlinear predictive control algorithm\nOnce the CTRNN has been trained, the network can be used as an internal\nmodel of a predictive controller. The recurrent neural network generates\nprediction of future process outputs over a specified prediction horizon P ,\n15\nwhich allows the following performance criterion to be minimized:\nmin\nu\u2264uk\u2264u\nk=0,...,M\u22121\n\u03d5 =\n1\n2\nP\u2211\nk=1\neTy,kQey,k +\nM\u2211\nk=1\n\u2206uTkR\u2206uk (38)\ns.t. \u02d9\u02c6x(t) = f\u02c6(x\u02c6(t), u(t)), t \u2208 [t0, tP ] (39)\ny\u02c6(t) = Cx\u02c6(t) + d(t) (40)\nx\u02c6(t0) = x\u02c60, x\u02c6k := x\u02c6(t0 + kh)\nuk := u(tk) = u(t), t \u2208 [tk, tk+1]\ney,k := y\u02c6k \u2212 rk, k \u2208 [1, P ]\n\u2206uk := uk+1 \u2212 uk, k \u2208 [1,M ]\nuk = uM\u22121, k \u2208 [M,P \u2212 1]\nwhere, M and P are the control and prediction horizons respectively, Q \u2208\nRny\u00d7ny and R \u2208 Rnx\u02c6\u00d7nx\u02c6 are the weighting matrices for the output error and\nthe control signal changes respectively, rk \u2208 Rny is the output reference vector\nat tk, d is a virtual disturbance estimated at the current time and used to\nreduce the model-plant mismatch, u and u are constant vectors determining\nthe input constraints as element-by-element inequalities.\nThe prediction horizon [t0, tP ] is divided into P intervals, t0, t1, \u00b7 \u00b7 \u00b7 , tP\nwith ti+1 = ti + hi and\n\u2211P\u22121\ni=0 hi = tP \u2212 t0. For piecewise constant control,\nassume the optimal solution to (38) is u(t) \u2261 u(tk) = u[0](k) for tk \u2264 t \u2264 tk+1,\nk = 0, \u00b7 \u00b7 \u00b7 , P \u2212 1. Then, only the solution in the first interval is to be\nimplemented and whole procedure will be repeated at next sampling instant.\nLet v \u2208 RM\u00d7nu be defined as v := [uT[0](0) \u00b7 \u00b7 \u00b7uT[0](M \u2212 1)]T . Problem (38)\nis a standard nonlinear programming problem (NLP) which can be solved\n16\nby any modern NLP solvers. To efficiently solve the online optimization\nproblem of the predictive controller the same gradient calculation strategy\nof the NMPC approach proposed by [26] is used.\nA simple method is used to estimate the initial value of the model states\nrequired to solve the optimization problem at each sample time. In this\nmethod, the new states are updated from the old values using the dynamic\nequation (39). Also, the state estimate error was reduced further by adding\nthe virtual disturbance d to the output. No terminal penalty is used in this\nwork and a good tuning of h, P , M , Q, and R was found enough to ensure\nthe close-loop stability for the case study in different operation conditions.\n4 Case Study \u2013 An Evaporator Process\nThis case study is based on the forced-circulation evaporator described by\nNewell and Lee [27], and shown in Figure 1. In this process, a feed stream\nenters the process at concentration X1 and temperature T1, with flow rate F1.\nIt is mixed with recirculating liquor, which is pumped through the evaporator\nat a flow rate F3. The evaporator itself is a heat exchanger, which is heated\nby steam flowing at rate F100 with entry temperature T100 and pressure P100.\nThe mixture of feed and recirculating liquor boil inside the heat exchanger,\nand the resulting mixture of vapour and liquid enters a separator where the\nliquid level is L2 . The operating pressure inside the evaporator is P2. Most\nof the liquid from the separator becomes the recirculating liquor. A small\nproportion of it is drawn off as product, with concentration X2, at a flow rate\nF2 and temperature T2. The vapour from the separator flows into a condenser\n17\nat flow rate F4 and temperature T3, where it is condensed by cooling water\nflowing at rate F200, with entry temperature T200 and exist temperature T201.\nThe nominal values of the system variables are given in Table 1, while the\nfirst-principle model equations are available in [27].\n4.1 System identification using CTRNN\nThe evaporator system has been adopted as a case study for system identifica-\ntion using CTRNN by a group of researchers [15], where a Genetic Algorithm\n(GA) based approach was used to train the CTRNN as it was believed that\n\u201cthe implementation of gradient-based training algorithms is computation-\nally expensive\u201d. However, only a short period (5 minutes) of data with simple\ninput signals (step changes) was used to train the network and another short\nperiod (7.5 minutes) was adopted for model validation. In this work, it is\nto be demonstrated that with the approach developed above the gradient-\nbased algorithm is not computationally expensive any more comparing with\nthe GA based approach since the new training algorithm is able to handle a\nmuch longer period (500 minutes) of data with much more complicated input\nsignals (random pulses) for training and validation.\nThe evaporator is approximated using a continuous-time recurrent MLP\n18\nnetwork with one hidden layer as shown in Figure 2:\nxh(t) = \u03c3s (Wxx\u02c6(t) +Wuu(t) + b1) (41)\n\u02d9\u02c6x(t) = W2xh(t) + b2\ny\u02c6(t) = Cx\u02c6(t)\nwhere, Wx \u2208 Rnh\u00d7nx\u02c6 , Wu \u2208 Rnh\u00d7nu , andW2 \u2208 Rnx\u02c6\u00d7nh are connection weights,\nb1 \u2208 Rnh and b2 \u2208 Rnx\u02c6 are bias vectors, whilst each element of the vector\n\u03c3s(\u00b7) \u2208 Rnh represents the sigmoid-tanh function as the neural activation\nfunction, i.e.\n\u03c3s(n) =\n2\n1 + e\u22122n\n\u2212 1 (42)\nThe parameter vector is \u03b8 =\n[\nvec(Wx)\nT vec(Wu)\nT bT1 vec(W2)\nT bT2\n]T\n\u2208\nRn\u03b8 , where n\u03b8 = nx\u02c6\u00d7 (nh+1)+nh\u00d7 (nx\u02c6+nu+1). The identification scheme\nassumes that the plant model equations are unknown and the only available\ninformation is the input-output data which is generated through various runs\nof the first principle model of the plant given by [27]. Two different structures\nof the CTRNN are studied to model the process. The first network (Network\n1) was trained with nx\u02c6 = ny = 3, and nh = 8 (n\u03b8 = 83), while the second\none (Network 2) was trained with nx\u02c6 = 5 and nh = 8 (n\u03b8 = 117). The\ntraining was carried out repetitively over the data collected within a fixed\ntime interval of 500 minutes and sampled at every 0.2 minutes. The inputs\ntraining data was a random pulses with a different amplitude and durations\n19\nwith the range chosen to cover all the region of operation of the plant (see\nFigure 3). Another set of data at sampling time 0.05 minutes is randomly\ngenerated from the plant to be used for network validation. The output data\nare corrupted with a normally distributed zero mean noise with variance 5%\nof the steady state values of the output variables. The initial values of the\nfirst ny network states were chosen equal to the steady state values of the\nsimulated plant outputs while the (nx\u02c6 \u2212 ny) remains were equal to zeros.\nTo demonstrate the CTRNN capability for evaporator model approxi-\nmation, the simulated plant output and the trained neural networks output\nare compared in figures 4 and 5. A good model fitting is observed for both\nnetworks with approximately similar accuracy with the training data. In\nterms of model validation, Network 1 is better than Network 2 as shown in\nFigure 5. This means increase the network state dimension does not neces-\nsarily improve the model fitting. Sometimes, networks with high order could\ninclude undesirable eigenvalues which may induce an unstable or poor perfor-\nmance. Therefore, Network 1 is chosen as the internal model of the predictive\ncontroller for accurate and fast online calculations. Also, the validation re-\nsults show the capability of the network to approximate the simulated plant\noutput with a sampling time less than that used for training, without the\nneed to re-train the network. In fact, this is one of the most important\nadvantages of CTRNNs over discrete-time recurrent networks.\nAlso, as a confidence test of the resulting model, the correlation\u2013based\nmodel validation results for the CTRNN model can be calculated according\nto equations (36)\u2013(37) and shown in figures 6 and 7 respectively. The dotted\nlines in each plot are the 95% confidence bounds (\u00b11.96\/\u221a500). It can be\n20\nseen that only a small number of points are outside the bounds. This demon-\nstrates that the model can be considered as being adequate for modelling this\nplant.\nTo solve the training problem, a total nx\u02c6 \u00d7 N \u00d7 n\u03b8 = 3 \u00d7 2000 \u00d7 83 =\n498000 sensitivity variables have to be calculated in addition to the original\n3 ordinary differential equations (ODEs) of Network 1 while for Network 2,\nthe number of sensitivity is 5 \u00d7 2000 \u00d7 117 = 1170000. To demonstrate the\nefficiency of the new algorithm, it is compared with the traditional sensitivity\nequation integrating approach using a typical numerical ODE solver, the\nMATLAB function ode15s.\nTo compare computation time associated with a given accuracy, a refer-\nence solution is produced by using ode15s solver and setting the error toler-\nance to 10\u221214. Then with four tolerance settings, (10\u22123, 10\u22126, 10\u22128, 10\u221210)\nand four different Taylor series orders (3, 6, 8, 10), computation time and\naccuracy against the reference solutions using two different approaches are\ncompared in Table 2. A third network (Network 3) with new configurations\n(nx = 4, nh = 15, and sensitivity variables number = 4 \u00d7 2000 \u00d7 169 =\n1352000) has been trained and the results are given in Table 2 for com-\nparison. Note that the computation time in Table 2 is the time required to\ncalculate the cost function and the sensitivity variables over one optimization\niteration whilst the error term in the same table is the maximum absolute\nerror against the reference solution. The table shows that training algorithm\nusing AD perform better than the traditional sensitivity approach in both\nefficiency and accuracy. It can be seen that the order of Taylor series plays an\nimportant role in error control. Increase the order by a few number, the error\n21\nwould be reduced by a number of orders magnitude without increasing too\nmuch computation time. However, using traditional approaches, significant\ncomputation time may have to be traded off for a reduction in computation\nerror. A way to determine an appropriate order of Taylor series for a given\nerror tolerant was suggested in [26]. It is worth to mention that a successful\ntraining would require thousands of iterations. If the accuracy of ODE solver\nis lower, it would require even more iterations to get a converged solution.\nTherefore, the time comparison listed in Table 2 suggests a massive efficiency\nimprovement in network training achieved by the proposed approach.\nAll tests are done on a Windows XP PC with an Inetl Pentium-4 processor\nrunning at 3.0 GHz. Note that, the proposed algorithm is implemented in C\nusing ADOL-C and interfaced to MATLAB via a mex warp.\n4.2 Evaporator predictive control\nEffective control of the evaporator system using traditional PID controllers\nwas not very successful especially for large setpoint changes [27]. Predictive\ncontrol was also considered by a number of workers. Linear model predic-\ntive control (LMPC) was demonstrated to be not sufficient to fully control\nthis process for an excessive range of variation (see control objectives given\nbellow)[37] (see Figure 8). A nonlinear MPC strategy based on successive\nlinearization solution to control this process under a large setpoint change\ncondition was proposed by Maciejowski [37]. A good performance was ob-\nserved after re-linearizing the nonlinear process model after every few steps.\nHowever, disturbances have not been considered there. In this paper, the\n22\nNMPC algorithms described in section 3 is applied to control the process for\nsetpoint tracking and disturbance rejection tests described as follows;\nThe control objective of the case study is;\n1. Track setpoint ramp changes of X2 from 25% to 15% and P2 from 50.5\nkPa to 70 kPa.\n2. Track setpoint changes as above when unmeasured disturbances, F1,\nX1, T1 and T200 are varied within \u00b120% of their nominal values.\nThe control system is configured with three manipulated variables, F2, P100\nand F200 and three measurements, L2, X2 and P2. All manipulated variables\nare subject to a first order lag with a time constant equal to 0.5 minutes and\nsaturation constraints, 0 \u2264 F2 \u2264 4, 0 \u2264 P100 \u2264 400 and 0 \u2264 F200 \u2264 400.\nTo tune control horizon M , prediction horizon P , and sampling time h,\ninitially let P = M = 1 min, and h = 1 min. By varying M (and assuming\nP = M) from 1 to 15 min, a stable performance is obtained which satisfies\nall control specifications for 1 \u2264 M \u2264 20 min. When M \u2265 10 min, the\nimprovement on the system performance is negligible but computation time\nincreases. Therefore M = 4 min is selected. The same steps are used to\nchoose a suitable prediction horizon P , a reasonable range from the minimum\nvalue (P = M = 1) min to P = 40 min has been tested. A stable response\nwithout any constraints violation is detected within range 1 \u2264 P min. No\nperformance improvement can be observed when P \u2265 7 min. Therefore\nP = 7 min is chosen to ensure that both the system stability and satisfactory\ncontrol performance achieved within a reasonable computation time.\nThe weighting matrix, Q = diag(Q0, \u00b7 \u00b7 \u00b7 , Q0), where Q0 is diagonal and\n23\ninitially set to be the inverse of the output error bounds. After online tuning,\nthe final values are:\nQ0 =\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1000 0 0\n0 500 0\n0 0 200\n\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb (43)\nAlso, the input weighting matrix R = diag(R0, \u00b7 \u00b7 \u00b7 , R0), where R0 is diagonal\nand set to I.\nBy using piecewise constant input, the result NLP problem has nu \u00d7\nM = 12 degrees of freedom. To solve the NLP problem of the NMPC, a\ntotal (nx\u02c6 \u00d7 P ) \u00d7 (nu \u00d7 M) = 252 sensitivity variables have to calculated\nin addition to original 3 ODEs of the neural network. In this work, the\nsensitivity equations are solved using the sensitivity algorithm of [26].\n4.3 Simulation Results\nSimulation results of all tests above are shown in figures 8 and 9. The effi-\nciency and the stability of the proposed CTRNN based NMPC during set-\npoint ramp test has been proved in contrast with the LMPC [37] as shown\nin Figure 8. Also, it can be seen from the results given in Figure 9 that mea-\nsured outputs follow the setpoints quite well without any input constraints\nviolation in spite of the existence of severe unmeasured disturbances.\nTo test the controller sensitivity to the sampling time, simulations have\nalso been done by varying h from 0.5 min to 2 min. A stable performance\nwithout constraints violation at all tests are also obtained. Knowing that\n24\nthe recurrent neural network (Network 1) which is trained at h = 0.2 min is\nused as the controller internal model at all the above tests.\nA detailed stability analysis for nonlinear model predictive control of the\nevaporator has been done [38], where using the new stability measure de-\nveloped, a concrete conclusion had been obtained, i.e. the NMPC of the\nevaporator is asymptotically stable around the nominal steady state for any\npositive definite state weighting matrix, Q. The work also provided a way to\ncalculate the stability region around the nominal steady state. According to\n[38], it can be shown that the NMPC described in this work is always stable.\n5 Conclusion\nThis paper demonstrates the reliability of artificial neural networks in pro-\ncess control. An efficient algorithm has been proposed to train continuous-\ntime recurrent neural networks to approximate nonlinear dynamic systems\nso that the trained network can be used as the internal model for a nonlinear\npredictive controller. The new training algorithm is based on the efficient\nLevenberge Marquardt method combined with an efficient and accurate tool:\nautomatic differentiation. The dynamic sensitivity equations and the ODEs\nof the recurrent neural network are solved accurately and simultaneously via\nAD. Big time saving to solve sensitivity equations with a higher accuracy\nare observed using the new algorithm compared with a traditional method.\nAlso, the trained networks with a different model orders show the capability\nto approximated the multivariable nonlinear plant at different sampling time\nwithout the need to re-train the networks. The results show that, the choice\n25\nof the network order is also very important to get a good model fitting and\nstable performance. Based on the identified neural network model, a NMPC\ncontroller has been developed. The similar strategy that used in the network\ntraining has been used to solve the online optimization problem of the predic-\ntive controller. The capability of the new nonlinear identification algorithm\nand NMPC algorithm are demonstrated via an evaporator case study with\nsatisfactory results.\nReferences\n[1] Pearson R. K., \u201cSelecting Nonlinear Model Structures for Computer Con-\ntrol: Review\u201d, Journal of Process Control, vol. 13, pp. 1-26, 2003.\n[2] Zhao H., Guiver J., and Sentoni G., \u201cAn Identification Approach to Non-\nlinear State Space Model for Industrial Multivariable Model Predictive\nControl\u201d, Proceeding of the American Control Conference, Philadelphia,\nPennsylvania, 1998.\n[3] Funahashi K. L. and Nakamura Y., \u201cApproximation of Dynamical Sys-\ntems by Continous time Recurrent Neural Networks\u201d, Neural Networks,\nvol. 6, pp. 183 \u2013 192, 1993.\n[4] Temeng H., Schenelle P., and McAvoy T., \u201cModel Predictive Control of\nan Industrial Packed Reactors using Neural Networks\u201d, J. Proc. Control,\nvol. 5(1), pp. 19 \u2013 28, 1995.\n26\n[5] Tan Y., and Cauwenberghe A., \u201cNonlinear One Step Ahead Control us-\ning Neural Networks:Control Strategy and Stability Design\u201d, Automatica,\nvol. 32(12), pp. 1701 \u2013 1706, 1996.\n[6] Miller W. T., Sutton R. S., and Werbos P. J., \u201dNeural Networks for\nControl\u201d, MIT Press, Cambridge, MA, 1990.\n[7] Narendra K. S. and Parthasarathy K., \u201dIdentification and Control of Dy-\nnamical Systems using Neural Networks\u201d, IEEE Trans, Neural Networks,\nvol. 1(1), pp. 4 \u2013 26, 1990.\n[8] Su H. T. and McAvoy T. J., \u201cArtificial Neural Networks for Nonlinear\nProcess Identification and Control\u201d, Nonlinear Process Control, M. A.\nHenson and D. E. Seborg (eds) Prentic Hall, NJ, pp. 371 \u2013 428, 1997.\n[9] Jin L., Nikiforuk P. , Gupta M., \u201cApproximation of Discrete-Time State-\nspace Trajectories using Dynamic Recurrent Neural Networks\u201d, IEEE\nTransactions on Automatic Control, vol. 40(7), pp. 1266-1270, 1995.\n[10] Delgado A., Kambhampati C., Warwick K., \u201cDynamic Recurrent Neu-\nral Network for System Identification and Control\u201d, IEE Proc. Control\nTheory Appl., vol. 142(4), pp. 307-314. 1995.\n[11] Hush D. R. and Horne B. G., \u201cProgress in Supervised Neural Networks\u201d,\nIEEE Sig. Process, Mag., vol. 1, pp. 8 \u2013 39, 1993.\n[12] Zamarreno J. M. and Vega P., \u201cState-Space Neural Network, Properties\nand Application\u201d, Neural Networks, vol. 11, pp. 1099 \u2013 1112, 1998.\n27\n[13] Pearlmutter B. A., \u201cGradient Calculations for Dynamic Recurrent Neu-\nral Networks: A Survey\u201d, IEEE Transactions on Neural Networks, 1995.\n[14] Kambhampati C., Craddock R. J., Tham M., Warwick K., \u201cInverse\nModel Control using Recurrent Networks\u201d, Mathematics and Computers\nin Simulation, vol. (51), pp. 181-199, 2000.\n[15] Kambhampati C., Garces F., Warwick K., \u201cApproximation of Non-\nautonomouous Dynamic Systems by Continuous Time Recurrent Neu-\nral Networks\u201d, Proceeding of the IEEE-INNS-ENNS International Joint\nConference on Neural Networks IJCNN 2000, vol. 1, pp. 64 \u2013 69, 2000.\n[16] Kambhampati C., Manchanda S., Delgado A., Green G., Warwick K.,\nTham M., \u201cThe Relative Order and Inverses of Recurrent Networks\u201d,\nAutomatica, vol. 32(1), pp. 117-123, 1996.\n[17] Prasad V., and Bequette B. W., \u201cNonlinear System Identification and\nModel Reduction using Artificial Neural Networks\u201d, Computers and\nChemical Engineering, vol. 27, pp. 1741-1754, 2003.\n[18] Rumelhart D. E., Hinton G. E., and Williams R. J., \u201cLearning Internal\nRepresentiations by Error Propagation\u201d, in Prallel Distributed Process-\ning, D. E. Rumelhart and J. L. MeClelland, Eds., Cambrige MA:MIT\nPress, 1986.\n[19] Leonard J. A. and Kramer M. A., \u201cImprovement of the Back-\npropagation Algorithm for Training Neural Networks\u201d, Computers and\nChemical Eng., vol. 14, pp. 337 \u2013 341, 1990.\n28\n[20] Marquardt D., \u201cAn Algorithm for least-Square Estimation of Nonlinear\nParameters\u201d SIAM J. Appl. Math., vol. 11 , pp. 431 \u2013 441 , 1963.\n[21] Goldberge D. E., \u201cGenetic Algorithms in Search, Optimization and Ma-\nchine Learning\u201d, Reading, MA:Addision-Wesley, 1989.\n[22] Storen S., Hertzberg T., \u201cObtaining Sensitivity Information in Dynamic\nOptimization Problems Solved by the Sequential Approach\u201d, Computer\nand Chemical Engineering, vol. 23, pp. 807 \u2013 819, 1999.\n[23] Schlegel M., Marquardt W., Ehrig R., Nowak U., \u201cSensitivity Analysis\nof Linearly Implicit Differential-Algebraic Systems by One-step Extrap-\nolation\u201d, Applied Numerical Mathematics, vol. 48, pp. 83 \u2013 102, 2004.\n[24] Griesse R., Walther A., \u201cEvaluating Gradients in Optimal Control: Con-\ntinuous Adjoint Versus Automatic Differentiation\u201d, Journal of Optimiza-\ntion Theory and Applications, vol. 122(1), pp. 63 \u2013 86, 2004.\n[25] Cao Y., and A-Seyab R., \u201cNonlinear Model Predictive Control using\nAutomatic Differentiation\u201d, European Control Conference (ECC 2003),\nCambridge, UK, 2003, p. in CDROM.\n[26] Cao Y.,\u201cA Formulation of nNonlinear Model Predictive Control using\nAutomatic Differentiation\u201d, Journal of Process Control, vol. 15, pp. 851\n\u2013 858, 2005.\n[27] Newell R. B. and P. L. Lee, \u201cApplied Process Control-A Case Study\u201d,\nPrentice Hall, Englewood Cliffs, NJ, 1989.\n29\n[28] Garces F., Kambhampati C., and Warwick K., \u201cDynamic Recurrent\nNeural Networks for Identification of a Multivariable Nonlinear Evapora-\ntor Systems\u201d, Proceedings DYCONS\u201999, World Scientific, 1999.\n[29] Garces F., Kambhampati C., and Warwick K., \u201cDynamic Recurrent\nNeural Networks for Feedback Linearization of a Multivariable Nonlinear\nEvaporator Systems\u201d, In Review for UKACC International Conference\nControl, 2000.\n[30] Chen C. T., \u201cLinear System Theory and Design\u201d, third ed., Oxford Uni-\nversity Press, New York, 1999.\n[31] B. Christianson, \u201cReverse Accumulation and Accurate Rounding Error\nEstimatres for Taylor Series\u201d, Optimization Methods and Software, vol. 1,\npp. 81 \u2013 94, 1992.\n[32] Griewank A., \u201dEvauating Derivatives\u201d, SIAM, Philadelphia, PA, 2000.\n[33] Griewank A., David J., Jean U., \u201cADOL-C: A Package for the Auto-\nmatic Differentiation of Algorithms written in C\/C++\u201d, ACM Transac-\ntions on Mathematical Software, vol. 22(2), pp. 131 \u2013 167, 1996.\n[34] Griewank A., \u201cODE Solving via Automatic Differentiation and Ratio-\nnal Prediction\u201d, in: Griffiths D., Watson G. (Eds.), Numerical Analysis\n1995, vol. 344 of Pitman Research Nots in Mathematics Series, Addison-\nWesley., Reading, MA, 1995.\n30\n[35] Zhang J., and Morris J., \u201cRecurrent neuro\u2013fuzzy networks for nonlinear\nprocess modeling\u201d, IEEE Trans. on Neural Networks, vol. 10(2), pp. 313\u2013\n326, 1999.\n[36] Billings S. A. and Voon W. S. F., \u201cCorrelation based model validity\ntests for nonlinear models\u201d, Int. J. Control, vol. 44, pp. 235\u2013244, 1986.\n[37] Maciejowski J. M., \u201cPredictive control with constraints\u201d, Prentice Hall,\nHarlow, England, 2002.\n[38] Chen, W.-H. and Cao, Y., \u201cStability analysis of constrained nonlinear\nmodel predictive control with terminal weighting\u201d, submitted to Interna-\ntional Journal of Robust and Nonlinear Control, 2007.\n31\nList of Figures\n1 Evaporator System . . . . . . . . . . . . . . . . . . . . . . . . 33\n2 Recurrent Neural Network Structure . . . . . . . . . . . . . . 34\n3 Training data set, Inputs, 4T = 0.2 min. . . . . . . . . . . . . 35\n4 Training data set, Outputs, 4T = 0.2 min. . . . . . . . . . . . 36\n5 Validation data set, Outputs, 4T = 0.05 min. . . . . . . . . . 37\n6 Validation tests\/Autocorrelation coeff. for error, 4T = 0.05\nmin. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n7 Validation tests\/Cross-correlation coef. of U, 4T = 0.05 min. 39\n8 Evaporator performance at setpoints ramp changes using LMPC\n[37] and the proposed NMPC, 4T = 1 min. . . . . . . . . . . 40\n9 Evaporator performance using the present NMPC at setpoint\nchanges plus random disturbances test. (a)\u2013(c) Measured out-\nputs with setpoints. (d)\u2013(f) Manipulated variables. (g)\u2013(j)\nDisturbances, 4T = 1 min. . . . . . . . . . . . . . . . . . . . 41\n32\nSteam             T100\nF100              P100\nEvaporator\nCondensate\nFeed\nF1, X1, T1\nF3\nSeparator\nP2, L2\nProduct\nF2, X2, T2\nT201\nT200  \nF200\nCooling\nwaterCondenser\nFigure 1: Evaporator System\n33\n)(\u02c6 ty\u222b\n)(\u22c5s\u03c3\n)(\u22c5s\u03c3\n)(\u22c5s\u03c3\n)( tu\n)(\u02c6 tx\n)(\u22c5s\u03c3\n)(\u02c6 tx&\nC\nM\n2W\nInput  layer               Hidden layer                         Output   layer                  \n1b\n2b\nx\nu\nW\nW\nFigure 2: Recurrent Neural Network Structure\n34\n0 50 100 150 200 250 300 350 400 450 500\n0\n1\n2\n3\n4\nF 2\n,\n \nkg\n\/m\nin\n0 50 100 150 200 250 300 350 400 450 500\n50\n100\n150\n200\n250\n300\nP 1\n00\n,\n \nkP\na\n0 50 100 150 200 250 300 350 400 450 500\n0\n50\n100\n150\n200\n250\n300\nF 2\n00\n,\n \nkg\n\/m\nin\ntime, minutes\nFigure 3: Training data set, Inputs, 4T = 0.2 min.\n35\n0 50 100 150 200 250 300 350 400 450 500\n0\n1\n2\n3\n4\n5\n6\nL 2\n,\n \nm\n0 50 100 150 200 250 300 350 400 450 500\n0\n10\n20\n30\n40\n50\n60\nX 2\n,\n \n%\n0 50 100 150 200 250 300 350 400 450 500\n20\n30\n40\n50\n60\n70\n80\n90\nP 2\n,\n \nkP\na\ntime, minutes\nActual output\nNetwork 1 output\nNetwork 2 output\nFigure 4: Training data set, Outputs, 4T = 0.2 min.\n36\n0 50 100 150 200 250 300 350 400 450 500\n0\n1\n2\n3\n4\n5\n6\n7\nL 2\n,\n \nm\n0 50 100 150 200 250 300 350 400 450 500\n0\n10\n20\n30\n40\n50\n60\nX 2\n,\n \n%\n0 50 100 150 200 250 300 350 400 450 500\n20\n30\n40\n50\n60\n70\n80\n90\nP 2\n,\n \nkP\na\ntime, minutes\nActual output\nNetwork 1 output\nNetwork 2 output\nFigure 5: Validation data set, Outputs, 4T = 0.05 min.\n37\n0 5 10 15 20 25\n\u22121\n\u22120.5\n0\n0.5\n1\n0 5 10 15 20 25\n\u22121\n\u22120.5\n0\n0.5\n1\n0 5 10 15 20 25\n\u22121\n\u22120.5\n0\n0.5\n1\nlag\nR e\n1e\n1(\u03c4\n)\nR e\n2e\n2(\u03c4\n)\nR e\n3e\n3(\u03c4\n)\nNetwork 1 \nNetwork 2 \nFigure 6: Validation tests\/Autocorrelation coeff. for error, 4T = 0.05 min.\n38\n\u221220 \u221210 0 10 20\n\u22121\n\u22120.5\n0\n0.5\n1\n\u221220 \u221210 0 10 20\n\u22121\n\u22120.5\n0\n0.5\n1\n\u221220 \u221210 0 10 20\n\u22121\n\u22120.5\n0\n0.5\n1\nlag\n\u221220 \u221210 0 10 20\n\u22121\n\u22120.5\n0\n0.5\n1\n\u221220 \u221210 0 10 20\n\u22121\n\u22120.5\n0\n0.5\n1\n\u221220 \u221210 0 10 20\n\u22121\n\u22120.5\n0\n0.5\n1\nlag\nR U\ne 1\n(\u03c4)\nR U\ne 1\n(\u03c4)\nR U\ne 2\n(\u03c4)\nR U\ne 2\n(\u03c4)\nR U\ne 3\n(\u03c4)\nR U\ne 3\n(\u03c4)\nNetwork 1 Network 2 \nR\nu\n3\ne\n1\n Ru1e1 \nR\nu\n2\ne\n1\n \nFigure 7: Validation tests\/Cross-correlation coef. of U, 4T = 0.05 min.\n39\n0.8\n1\n1.2\n1.4\n1.6\nL 2\n,\n \nm\n14\n16\n18\n20\n22\n24\n26\nX 2\n,\n \n%\n0 20 40 60 80 100\n50\n55\n60\n65\n70\n75\nP 2\n,\n \nkP\na\ntime, min.\n0\n1\n2\n3\n4\nF 2\n,\n \nkg\n\/m\nin\n0\n100\n200\n300\n400\nP 1\n00\n,\n \nkP\na\n0 20 40 60 80 100\n0\n100\n200\n300\n400\ntime, min.\nF 2\n00\n,\n \nkg\n\/m\nin\nLMPC\nSetpoint\nNMPC\nOutput Variables Input Variables\nFigure 8: Evaporator performance at setpoints ramp changes using LMPC\n[37] and the proposed NMPC, 4T = 1 min.\n40\n0.8\n1\n1.2\n1.4\nL2\n, m\na\n10\n15\n20\n25\n30\nX2\n, %\nb\n50\n60\n70\n80\nP2\n, k\nPa\nc\n0\n1\n2\n3\n4\nF2\n, k\ng\/\nm\nin\nd\n0\n100\n200\n300\n400\nP1\n00\n, k\nPa\ne\n0\n100\n200\n300\n400\nF2\n00\n, k\ng\/\nm\nin\nf\n8\n9\n10\n11\n12\nF1\n, k\ng\/\nm\nin\ng\n4\n4.5\n5\n5.5\n6\nX1\n, %\nh\n0 20 40 60 80 100\n30\n35\n40\n45\n50\ntime, min\nT1\n, o\nC\ni\n0 20 40 60 80 100\n20\n25\n30\ntime, min\nT2\n00\n, o\nC\nj\nFigure 9: Evaporator performance using the present NMPC at setpoint\nchanges plus random disturbances test. (a)\u2013(c) Measured outputs with set-\npoints. (d)\u2013(f) Manipulated variables. (g)\u2013(j) Disturbances, 4T = 1 min.\n41\nTable 1: Evaporator Variables and Values\nVariables Description Nominal value Units\nF1 Feed flowrate 10 kg\/min\nF2 Product flowrate 2.0 kg\/min\nF3 Circulating flowrate 50 kg\/min\nF4 Vapor flowrate 8.0 kg\/min\nF5 Condensate flowrate 8 kg\/min\nX1 Feed composition 5.0 %\nX2 Product composition 25 %\nT1 Feed temperature 40.0 %\nT2 Product temperature 84.6\noC\nT3 Vapor temperature 80.6\noC\nL2 Separator level 1.0 m\nP2 Operator pressure 50.5 kPa\nF100 Steam flowrate 9.3 kg\/min\nT100 Steam temperature 119.9\noC\nP100 Steam pressure 194.7 kPa\nQ100 Heat duty 339 kW\nF200 Cooling water flowrate 208 kg\/min\nT200 Inlet C. W. temperature 25.0\noC\nT201 Outlet C. W. temperature 46.1\noC\nQ200 Condenser duty 307 kW\n42\nTable 2: Computing Time and Accuracy Comparison\nTraditional Sensitivity Approach\nTolerance Network 1 Network 2 Network 3\nTime, ms Error Time, ms Error Time, ms Error\n10\u22123 13.859 0.555 48.328 4.7175 67.641 0.5851\n10\u22126 45.046 0.0153 257.454 0.1351 459.046 0.0135\n10\u22128 69.437 4.0183\u00d710\u22124 434.547 8.973\u00d710\u22124 740.688 2.1924\u00d710\u22124\n10\u221210 77.906 1.1103\u00d710\u22128 580.125 1.3316\u00d710\u22125 838.563 9.6209\u00d710\u22129\nADOL\u2013C Software\nOrder Network 1 Network 2 Network 3\nTime, ms Error Time, ms Error Time, ms Error\n3 2.609 3.276\u00d710\u22125 3.11 1.396\u00d710\u22124 4.157 1.9759\u00d710\u22125\n6 4.031 2.095\u00d710\u221210 5.281 1.7862\u00d710\u22129 6.844 7.5623\u00d710\u22129\n8 5.207 1.136\u00d710\u221213 6.875 1.2301\u00d710\u221212 9.234 6.7502\u00d710\u221211\n10 6.813 8.881\u00d710\u221216 8.875 5.6843\u00d710\u221214 12.609 7.9543\u00d710\u221214\n43\n"}