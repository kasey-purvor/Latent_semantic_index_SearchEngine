{"doi":"10.1016\/j.engappai.2011.01.006","coreId":"51959","oai":"oai:eprints.lincoln.ac.uk:3832","identifiers":["oai:eprints.lincoln.ac.uk:3832","10.1016\/j.engappai.2011.01.006"],"title":"A controlled migration genetic algorithm operator for hardware-in-the-loop experimentation","authors":["Gladwin, D.","Stewart, P.","Stewart, J."],"enrichments":{"references":[{"id":655165,"title":"A Comparative Analysis of Selection Schemes Used in Genetic Algorithms. In","authors":[],"date":"1991","doi":"10.1016\/b978-0-08-050684-5.50008-2","raw":"Goldberg, D. E. (1991). A Comparative Analysis of Selection Schemes Used in Genetic Algorithms. In D. E. Goldberg, Genetic Algorithms: Foundations of Genetic Algorithms (pp. 69-93). San Mateo, CA, USA: Morgan Kauffmann.","cites":null},{"id":660745,"title":"A Comparative Study of SteadyState and Generational Genetic Algorithms.","authors":[],"date":"1996","doi":"10.1007\/bfb0032791","raw":"Vavak, F. and Fogarty, T. C. (1996). A Comparative Study of SteadyState and Generational Genetic Algorithms. Selected Papers from AISB Workshop on Evolutionary Computing (pp. 297-304). AISB.","cites":null},{"id":659601,"title":"A selective migration parallel multi-objective genetic algorithm,","authors":[],"date":"2010","doi":"10.1109\/CCDC.2010.5499013","raw":"Qiu T. and Ju G. (2010), A selective migration parallel multi-objective genetic algorithm, Control and Decision Conference (CCDC), 2010 Chinese, 463 - 467.","cites":null},{"id":659831,"title":"A study of control parameters affecting online performance of genetic algorithms for function optimisation.","authors":[],"date":"1989","doi":null,"raw":"Schaffer, J. D., Caruana, R. A., Eshelman, L. J. and Das, R. (1989). A study of control parameters affecting online performance of genetic algorithms for function optimisation. Proceedings of the 3rd International Conference on Genetic Algorithms (pp. 51-60). Los Altos CA: Morgan Kaufman.","cites":null},{"id":660457,"title":"Adaptive Mutation Rate Control Schemes in Genetic Algorithms.","authors":[],"date":"2002","doi":"10.1109\/cec.2002.1007058","raw":"Thierens, D. (2002). Adaptive Mutation Rate Control Schemes in Genetic Algorithms. IEEE International Conference on E-Commerce, Evolutionary Computation. 1, pp. 980-985. Piscataway: IEEE.","cites":null},{"id":653458,"title":"Adaptive Selection Methods for Genetic Algorithms. In Eribaum (Ed.),","authors":[],"date":"1987","doi":null,"raw":"Baker, J. E. (1987). Adaptive Selection Methods for Genetic Algorithms. In Eribaum (Ed.), Proceedings of the 2nd International Conference on Genetic Algorithms and their Application, (pp. 14-21). Cambridge MA.","cites":null},{"id":18441802,"title":"Adaptive Steady State Genetic Algorithm for scheduling university exams,","authors":[],"date":"2010","doi":"10.1109\/icnit.2010.5508555","raw":"AlSharafat W.S. and AlSharafat, M.S. (2010), Adaptive Steady State Genetic Algorithm for scheduling university exams, Networking and Information Technology (ICNIT), 2010 International Conference on, 70-74.","cites":null},{"id":653665,"title":"An Adaptive Genetic Algorithm Based on Population Diversity Strategy, Genetic and Evolutionary Computing,","authors":[],"date":"2009","doi":"10.1109\/wgec.2009.67","raw":"Chen L. (2009), An Adaptive Genetic Algorithm Based on Population Diversity Strategy, Genetic and Evolutionary Computing, 2009. WGEC \u201909. 3rd International Conference on, 93-96.","cites":null},{"id":653875,"title":"An Analysis of the Behaviour of a Class of Genetic Adaptive Systems.","authors":[],"date":"1975","doi":null,"raw":"DeJong, K. A. (1975). An Analysis of the Behaviour of a Class of Genetic Adaptive Systems. University of Michigan, Department of Computer and Communication Science. Ann Arbor: University of Michigan.","cites":null},{"id":654161,"title":"Are Genetic Algorithms Function Optimizers? Parallel Problem Solving from Nature PPSN2.","authors":[],"date":"1992","doi":null,"raw":"DeJong, K. A. (1992). Are Genetic Algorithms Function Optimizers? Parallel Problem Solving from Nature PPSN2. Elsevier.","cites":null},{"id":654961,"title":"Continuous variable neighbourhood search algorithm based on evolutionary metaheuristic components: a scalability test.","authors":[],"date":"2009","doi":"10.1109\/isda.2009.68","raw":"Garcia-Martinez C. and Lozano H. (2009), Continuous variable neighbourhood search algorithm based on evolutionary metaheuristic components: a scalability test. 9th Int. Conf. on Intelligent Systems Design and Applications, 1074-1079.","cites":null},{"id":658943,"title":"Cooperative Control of Mobile Sensor Networks: Adaptive Gradient Climbing in a Distributed Environment.","authors":[],"date":"2004","doi":"10.1109\/tac.2004.832203","raw":"Petter, O., Fiorelli, E. and Leonard, N. E. (2004). Cooperative Control of Mobile Sensor Networks: Adaptive Gradient Climbing in a Distributed Environment. IEEE Transactions on Automatic control , 49 (8), 1292-1302.","cites":null},{"id":660173,"title":"Crossover or Mutation.","authors":[],"date":"1992","doi":"10.1016\/b978-0-08-094832-4.50020-9","raw":"Spears, W. M. (1992). Crossover or Mutation. In Whitley (Ed.), Foundations of Genetic Algorithms -2 (pp. 221-237). San Mateo, CA, USA: Morgan Kauffmann.","cites":null},{"id":659993,"title":"Dominance based multiobjective simulated annealing,","authors":[],"date":"2008","doi":"10.1109\/TEVC.2007.904345","raw":"Smith K.I., Eversen R.M., Murphy C. and Misra R. (2008), Dominance based multiobjective simulated annealing, IEEE Trans. Evolutionary Computation 12 (3), 323-342.","cites":null},{"id":658494,"title":"Ensuring population diversity in genetic algorithms: A technical note with application to the cell formation problem,","authors":[],"date":"2006","doi":"10.1016\/j.ejor.2006.02.012","raw":"Nsakandaa A.L., Wilson L., Price B., Moustapha D. and Marc G. (2006), Ensuring population diversity in genetic algorithms: A technical note with application to the cell formation problem, European Journal of Operational Research, 178, (2), 634-638.","cites":null},{"id":658736,"title":"Evaluation of motor characteristics for hybrid electric vehicles using the hardware-in-the-loop concept.","authors":[],"date":"2005","doi":"10.1109\/tvt.2005.847228","raw":"Oh, S. C. (2005). Evaluation of motor characteristics for hybrid electric vehicles using the hardware-in-the-loop concept. IEEE Transactions on Vehicular Technology , 54 (3), 817-824.","cites":null},{"id":654394,"title":"Evolutionary Computation,","authors":[],"date":"2006","doi":"10.1109\/cec.2006.1688348","raw":"Eiben A.E., Schut, M.C. and de Wilde, A.R. (2006), Evolutionary Computation, 2006. CEC 2006. IEEE Congress on, 477 - 482.","cites":null},{"id":654748,"title":"Evolutionary Computation: Towards a New Philosophy of Machine Intelligence.","authors":[],"date":"2006","doi":"10.1016\/s0303-2647(97)81064-7","raw":"Fogel, D. B. (2006). Evolutionary Computation: Towards a New Philosophy of Machine Intelligence. New York: IEEE Press.","cites":null},{"id":656687,"title":"Experience With optimizers in Structural Design.","authors":[],"date":"1994","doi":null,"raw":"Keane, A. J. (1994). Experience With optimizers in Structural Design. In I. C. Parmee (Ed.), Proceedings of the Conference on Adaptive Computing in Engineering Design and Control \u201994, (pp. 14-27). Plymouth, UK.","cites":null},{"id":656897,"title":"Genetic Algorithm Optimisation of Multi-Peak Problems: Studies in Convergence and Robustness.","authors":[],"date":"1995","doi":"10.1016\/0954-1810(95)95751-q","raw":"Keane, A. J. (1995). Genetic Algorithm Optimisation of Multi-Peak Problems: Studies in Convergence and Robustness. International Journal of Arti\ufb01cial Intelligence in Engineering , 9 (2), 75-83.","cites":null},{"id":655405,"title":"Genetic Algorithms, Noise and the Sizing of Populations.","authors":[],"date":"1992","doi":"10.1016\/b978-0-08-094832-4.50014-3","raw":"Goldberg, D. E., Deb, K. and Clark, J. (1992). Genetic Algorithms, Noise and the Sizing of Populations. Complex Systems , 6, 333-362.","cites":null},{"id":657576,"title":"Genetic Algorithms: Concepts and Designs.","authors":[],"date":"1999","doi":null,"raw":"Mann, K. F., Tang, K. S., Kwong, S. and Halang, W. A. (1999). Genetic Algorithms: Concepts and Designs. London: Springer-Verlag.","cites":null},{"id":656443,"title":"Genetic Local Search for Multiple Objective Combinatorial Optimisation.","authors":[],"date":"1998","doi":null,"raw":"Jaskiewiscz, A. (1998). Genetic Local Search for Multiple Objective Combinatorial Optimisation. Technical Report RA-GL4\/98, Poznan University, Institute of Computing Science, Poznan.","cites":null},{"id":660967,"title":"GENITOR: A Different Genetic Algorithm.","authors":[],"date":"1998","doi":null,"raw":"Whitely, D. and Kauth, J. (1998). GENITOR: A Different Genetic Algorithm. Rocky Mountain Conference on Arti\ufb01cial Intelligence. Denver.","cites":null},{"id":656271,"title":"Hardware-in-the-loop simulation for the design and testing of engine-control systems.","authors":[],"date":"1999","doi":"10.1016\/s0967-0661(98)00205-6","raw":"Isermann, R., Schaffnit, J. and Sinsel, S. (1999). Hardware-in-the-loop simulation for the design and testing of engine-control systems. Control Engineering Practice , 5 (7), 643-653.","cites":null},{"id":658000,"title":"How Genetic Algorithms Really Work. Mutation and Hill-Climbing. In","authors":[],"date":"1992","doi":null,"raw":"Muhlenbein, H. (1992). How Genetic Algorithms Really Work. Mutation and Hill-Climbing. In R. Manner and R. Manderick (Ed.), Parallel Problem Solving from Nature PPSN II, (pp. 15-25). Amsterdam.","cites":null},{"id":657800,"title":"How to Solve It: Algorithms for Engineering Systems.","authors":[],"date":"2006","doi":"10.1007\/978-3-662-04131-4_14","raw":"Michalewicz, Z. and Fogel, D. B. (2006). How to Solve It: Algorithms for Engineering Systems. Cambridge, UK: Cambridge University Press.","cites":null},{"id":660421,"title":"Improved Decision Support for Engine-in-the-Loop Experimental Design Optimisation. IMechE Part D - Automobile Engineering ,","authors":[],"date":"2010","doi":"10.1243\/09544070JAUTO1213","raw":"Stewart, P. G., Gladwin, D., Stewart, J., Chen, R. and Winward, E. (2010). Improved Decision Support for Engine-in-the-Loop Experimental Design Optimisation. IMechE Part D - Automobile Engineering , In Press.","cites":null},{"id":655635,"title":"Interactive genetic algorithms with large population size, Evolutionary Computation,","authors":[],"date":"2008","doi":"10.1109\/cec.2008.4631016","raw":"Gong D., Yuan J. and Ma X. (2008), Interactive genetic algorithms with large population size, Evolutionary Computation, 2008. CEC 2008. (IEEE World Congress on Computational Intelligence). IEEE Congress on, 1678 - 1685.","cites":null},{"id":659192,"title":"Novel directional gradient descent searches for fast block motion estimation.","authors":[],"date":"2009","doi":"10.1109\/tcsvt.2009.2020320","raw":"Po L.M., Ng K.H., Cheung K.-W., Wong K.-H., Uddin Y. and Ting C.-W. (2009), Novel directional gradient descent searches for fast block motion estimation. IEEE Trans. Circuits and Signals for Video Technology, 19 (8), 1189-1195.","cites":null},{"id":653245,"title":"On Optimal Population Size of Genetic Algorithms.","authors":[],"date":"1992","doi":"10.1109\/cmpeur.1992.218485","raw":"Alander, T. (1992). On Optimal Population Size of Genetic Algorithms. IEEE International Conference on Computer Systems and Software Engineering (pp. 65-70). IEEE.9","cites":null},{"id":659401,"title":"Promoting diversity using migration strategies in distributed genetic algorithms,","authors":[],"date":"2005","doi":"10.1109\/cec.2005.1554910","raw":"Power, D., Ryan C. and Azad, R.M.A. (2005), Promoting diversity using migration strategies in distributed genetic algorithms, Evolutionary Computation, 2005. The 2005 IEEE Congress on, 1831 - 1838 Vol. 2.","cites":null},{"id":656068,"title":"Robust yaw stability controller design and hardware in the loop testing for a road vehicle.","authors":[],"date":"2009","doi":"10.1109\/tvt.2008.925312","raw":"Gouvenc B.A., Gouvenc L. and Karaman S. (2009). Robust yaw stability controller design and hardware in the loop testing for a road vehicle. IEEE Trans. Veh. Tech. 58 (2), 551-571.","cites":null},{"id":653336,"title":"Self-Adaptation in Genetic Algorithms.","authors":[],"date":"1992","doi":null,"raw":"Back, T. (1992). Self-Adaptation in Genetic Algorithms. Proceedings of the 1st European Conference on Arti\ufb01cial Life, (pp. 263-271).","cites":null},{"id":655833,"title":"Stochastic optimal control and analysis of stability of networked control systems with long delay.","authors":[],"date":"2003","doi":"10.1016\/s0005-1098(03)00196-1","raw":"Hu, S. and Zhu, Q. (2003). Stochastic optimal control and analysis of stability of networked control systems with long delay. Automatica , 39 (11), 1877-1884.","cites":null},{"id":657316,"title":"Study on convergence of selfadaptive and multi-population composite Genetic Algorithm, Machine Learning and Cybernetics,","authors":[],"date":"2009","doi":"10.1109\/icmlc.2009.5212122","raw":"Liu L.-M., Wang N.-P. and Li F.-C. (2009), Study on convergence of selfadaptive and multi-population composite Genetic Algorithm, Machine Learning and Cybernetics, 2009 International Conference on, 2680 -2685.","cites":null},{"id":657101,"title":"The Sixth World Congress on,","authors":[],"date":"2006","doi":"10.1109\/wcica.2006.1713930","raw":"Li N and Ye F. (2006), Optimal Design of Discrete Structure with Directed Mutation Genetic Algorithms, Intelligent Control and Automation, 2006. WCICA 2006. The Sixth World Congress on, 3663 - 3667.","cites":null},{"id":658277,"title":"Transition and convergence properties of genetic algorithms applied to \ufb01tness functions perturbed concurrently by additive and multiplicative noise.","authors":[],"date":"2009","doi":"10.1109\/cec.2009.4983276","raw":"Nakama T. (2009), Transition and convergence properties of genetic algorithms applied to \ufb01tness functions perturbed concurrently by additive and multiplicative noise. 2009 IEEE Congress on Evolutionary Computation, 2662-2669.","cites":null},{"id":654633,"title":"Varying the Probability of Mutation in the Genetic Algorithm.","authors":[],"date":"1989","doi":null,"raw":"]Fogarty, T. C. (1989). Varying the Probability of Mutation in the Genetic Algorithm. Proceedings of the 3rd International Conference on Genetic Algorithms, (pp. 104-109).","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2011-06","abstract":"In this paper, we describe the development of an extended migration operator, which combats the negative effects of noise on the effective search capabilities of genetic algorithms. The research is motivated by the need to minimize the num- ber of evaluations during hardware-in-the-loop experimentation, which can carry a significant cost penalty in terms of time or financial expense. The authors build on previous research, where convergence for search methods such as Simulated Annealing and Variable Neighbourhood search was accelerated by the implementation of an adaptive decision support operator. This methodology was found to be effective in searching noisy data surfaces. Providing that noise is not too significant, Genetic Al- gorithms can prove even more effective guiding experimentation. It will be shown that with the introduction of a Controlled Migration operator into the GA heuristic, data, which repre- sents a significant signal-to-noise ratio, can be searched with significant beneficial effects on the efficiency of hardware-in-the- loop experimentation, without a priori parameter tuning. The method is tested on an engine-in-the-loop experimental example, and shown to bring significant performance benefits","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/51959.pdf","fullTextIdentifier":"http:\/\/eprints.lincoln.ac.uk\/3832\/1\/controlled_migration.pdf","pdfHashValue":"d3939ca3e03cb0e1169a8f60f72fbf9c0a0849f0","publisher":"Elsevier","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lincoln.ac.uk:3832<\/identifier><datestamp>\n      2013-12-04T21:35:35Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47373030<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lincoln.ac.uk\/3832\/<\/dc:relation><dc:title>\n        A controlled migration genetic algorithm operator for hardware-in-the-loop experimentation<\/dc:title><dc:creator>\n        Gladwin, D.<\/dc:creator><dc:creator>\n        Stewart, P.<\/dc:creator><dc:creator>\n        Stewart, J.<\/dc:creator><dc:subject>\n        G700 Artificial Intelligence<\/dc:subject><dc:description>\n        In this paper, we describe the development of an extended migration operator, which combats the negative effects of noise on the effective search capabilities of genetic algorithms. The research is motivated by the need to minimize the num- ber of evaluations during hardware-in-the-loop experimentation, which can carry a significant cost penalty in terms of time or financial expense. The authors build on previous research, where convergence for search methods such as Simulated Annealing and Variable Neighbourhood search was accelerated by the implementation of an adaptive decision support operator. This methodology was found to be effective in searching noisy data surfaces. Providing that noise is not too significant, Genetic Al- gorithms can prove even more effective guiding experimentation. It will be shown that with the introduction of a Controlled Migration operator into the GA heuristic, data, which repre- sents a significant signal-to-noise ratio, can be searched with significant beneficial effects on the efficiency of hardware-in-the- loop experimentation, without a priori parameter tuning. The method is tested on an engine-in-the-loop experimental example, and shown to bring significant performance benefits.<\/dc:description><dc:publisher>\n        Elsevier<\/dc:publisher><dc:date>\n        2011-06<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lincoln.ac.uk\/3832\/1\/controlled_migration.pdf<\/dc:identifier><dc:identifier>\n          Gladwin, D. and Stewart, P. and Stewart, J.  (2011) A controlled migration genetic algorithm operator for hardware-in-the-loop experimentation.  Engineering Applications of Artificial Intelligence, 24  (4).   pp. 586-594.  ISSN 0952-1976  <\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1016\/j.engappai.2011.01.006<\/dc:relation><dc:relation>\n        10.1016\/j.engappai.2011.01.006<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lincoln.ac.uk\/3832\/","http:\/\/dx.doi.org\/10.1016\/j.engappai.2011.01.006","10.1016\/j.engappai.2011.01.006"],"year":2011,"topics":["G700 Artificial Intelligence"],"subject":["Article","PeerReviewed"],"fullText":"1A Controlled Migration Genetic Algorithm Operator\nfor Hardware-in-the-Loop Experimentation.\nD. Gladwin,a, P. Stewartb and J. Stewartb\naDepartment of Electronic and Electrical Engineering, University of Sheffield. Mappin St. Sheffield S1 3JD U.K.;\nbSchool of Engineering, University of Lincoln, Lincoln LN6 7TS, UK. Corresponding Author:\npstewart@lincoln.ac.uk\nAbstract\u2014In this paper, we describe the development of an\nextended migration operator, which combats the negative effects\nof noise on the effective search capabilities of genetic algorithms.\nThe research is motivated by the need to minimize the num-\nber of evaluations during hardware-in-the-loop experimentation,\nwhich can carry a significant cost penalty in terms of time or\nfinancial expense. The authors build on previous research, where\nconvergence for search methods such as Simulated Annealing\nand Variable Neighbourhood search was accelerated by the\nimplementation of an adaptive decision support operator. This\nmethodology was found to be effective in searching noisy data\nsurfaces. Providing that noise is not too significant, Genetic Al-\ngorithms can prove even more effective guiding experimentation.\nIt will be shown that with the introduction of a Controlled\nMigration operator into the GA heuristic, data, which repre-\nsents a significant signal-to-noise ratio, can be searched with\nsignificant beneficial effects on the efficiency of hardware-in-the-\nloop experimentation, without a priori parameter tuning. The\nmethod is tested on an engine-in-the-loop experimental example,\nand shown to bring significant performance benefits.\nI. INTRODUCTION\nFOR the design, implementation and testing of systems,which are complex and\/or difficult to represent to a suf-\nficiently high degree of accuracy in simulation; it is common\npractice to adopt a hardware-in-the-loop approach, where some\nof the control loop components are real items of hardware\n[18], [19]. In this way, major systems components (such\nas engines in automotive applications) can be evaluated and\ncontrol systems designed without the expense and complexities\nof a whole system empirical development programme [30].\nHowever, the utilization of real hardware introduces the sig-\nnificant issue of sensor and measurement noise. The authors\nhave previously conducted research into the development of\ntechniques to improve the performance of several standard\nsearch heuristics such as gradient descent [31], [32], variable\nneighbourhood search [12] and simulated annealing [36] in\nsupporting hardware-in-the-loop search in internal combustion\nengine development [38]. This produced a methodology, which\nmakes use of the data evaluated by the heuristic during the\nsearch, and utilizes this to produce response surfaces. These\nresponse surfaces are used to generate probability surfaces to\nprovide the search heuristic with weighted stochastic decision\nsupport (WSDS) (figure 1).\nThis operator supports the heuristic and guides the exper-\nimental process to predicted areas of interest in the search\nFig. 1. Decision Support Architecture\nspace. Basic gradient descent, Simulated Annealing (SA) and\nVariable Neighbourhood Search (VNS) were supplemented\nby the WSDS methodology, and performance compared to\nthe basic form of the heuristics. The supplemented heuristics\nwere shown to have significantly improved performance when\nsearching over increasingly noisy surfaces.\nIt would be expected that Genetic Algorithms (GAs) should\nbe effective in noisy environments, and out-perform basic\nheuristics [6]. The GA allows for variance in fitness values,\nand providing that noise isnt overwhelming, this is effective,\nsince the GA doesnt discard useful information too quickly. In\ncomparison, local search may not identify improving moves\nor local optima without a priori information related to the\nnature of the noise. For this reason, GAs are studied in this\npaper. In comparative studies conducted at the time, an initial\nexperimental investigation into the performance of GAs was\ncarried out, resulting in a lower performance level than was\nanticipated. This motivated the current work, which addresses\nthe application of GAs to real-life experimental decision\nsupport applications\nGAs have been shown to be compromised when directing\nsearch over significantly rugged surfaces [15], [28], such as\nthose applications discussed in this work. As the amount of\nnoise inherent in the surface increases, it is likely that the\nnumber of local optima increases and, unless there is sufficient\ndiversity within the populations of the GA, this often causes\nthe GA to converge on these local optima, rather than the\nglobal, optimal solution [14]. Diversity is important in genetic\nalgorithms [29], as crossing over a homogeneous population\n2does not yield new solutions [10]. The parameters of a GA\ncan be improved for such problems, for example using a high,\nor directed mutation rate [23], [27], larger population sizes\n[1], [11], [16] or by suitable selection techniques [8], [13]. A\npriori knowledge is typically required to set these parameters,\nalthough solutions such as adapting the parameters throughout\nthe search using deterministic control schemes have been\nproduced [4], [5], [9]. However, a most important aspect to\nbe considered here is that for many Hardware in the Loop\napplications, a priori knowledge is not available, and the search\nspace can be considered unknown and unseen.\nAnother possible degree-of-freedom in GA implementation\nis Mutation, which is used to maintain the diversity of the\nentire population by changing individuals bit by bit with a\nsmall probability pm[0, 1], termed mutation rate. There is\nmuch debate whether high or low mutation rates should be\nused and whether these should be static or adaptive. A high\nmutation rate increases the level of exploration creating a\nmore diverse population according to [26], which is desirable\nfor more complex combinatorial problems. However, there\nhave been many proposed static mutation probabilities which\nare derived from experience or by trial-and-error. De Jong\nsuggested pm = 0.001 in [6], with Schaffer et al extending\nthis to a range of [0.001, 0.005] [35]. Bck used Schaffers\nresults in [3] to propose that the mutation rate should be\nset according to population size and length of individuals,\ngiving pm = 1.75\/(N \u2217 L1\/2), where N is the population\nsize and L denotes the length of individuals. Mhlenbein [27]\nrecommended that pm = 1 \/ L is an acceptable mutation rate\nand should be generally optimal. There is, however, evidence,\nboth empirical [9] for learning control rules, and theoretical\n[3] that the optimal rate of mutation is not only different for\nevery problem but will vary with evolutionary time according\nto the state of the search and the nature of the landscape\nbeing searched. Work by Thierens [39] proposes two simple\nadaptive mutation rate control schemes called constant gain\nand declining. Thierens compares these to fixed mutation rates,\nand other known self-adaptive mutation rates showing that\nthey perform favourably in terms of performance with no\ninitial parameters to configure. Qiu [34] proposes a new multi-\nobjective evolutionary algorithm, called selective migration\nparallel genetic algorithm (SMPGA) in which a new migration\nstrategy develops a searching population and a elite population\nevolve at the same time to keep and improve the convergence\nand diversity of the Pareto optimal set. Power [33] incorporates\na diversity guided selection mechanism, selecting a diverse set\nof individuals for migration from the evolving populations, and\nreports good performance.\nNone of the cited methods report activity in noisy environ-\nments, and many require a priori knowledge of the problem\ndomain. In particular, adaptive mutation schemes require con-\nsiderable a priori knowledge and subsequent parameter tuning.\nThe application domain in which we are working, in particular,\nthe automotive and aerospace sectors have, in general, rugged\nor noisy search surfaces, with little a priori information. Often,\nthe experimental evaluations of the controller are expensive,\nand hence it is preferred to use a methodology which requires\na minimum of parameter tuning to achieve convergence.\nGiven the prior success of weighted operators in raising the\nperformance of local heuristics, and associated with no a priori\ntuning requirement, this approach will be investigated in this\npaper in conjunction with migration operators, which have\nbeen shown recently to have significant potential in this kind\nof application area.\nII. RANDOM MIGRATION OPERATORS\nIn this section, we introduce the random migration operator\nbased upon the migration operator that is used in multi-deme\n(multiple population) GAs [24], [25], and apply this to single-\ndeme GAs supported by a decision support operator to yield a\nnovel operator called controlled migration. It should be noted\nthat controlled migration is equally applicable to the multi-\ndeme case although this is not investigated here. Multi-deme\nGAs make use of the migration operator to pass individuals be-\ntween sub populations according to a pre determined migration\nrate and migration interval. During a search, sub populations\nwill receive a new individual from another sub population\nthat could be from anywhere in the global search space. The\nindividual that is received is likely to have been evolved in a\nsub population that may be converging towards an alternative\noptimum, thus creating diversity in the receiving population.\nIn a single-deme (single population) GA, a similar scheme can\nbe applied where random individuals are introduced into each\ngeneration from the global search space, thus introducing an\nalternative source of diversity. A typical GA will use either\nthe incremental\/steady state genetic algorithm (IGA) model\n[2], [41] or the generational genetic algorithm (GGA) [7],\n[40]. Here we use the GGA that batch replaces an entire\npopulation each generation, as opposed to the IGA which in\ntypical applications only replaces one individual at a time.\nFigure (2) represents the GGA methodology that is applied in\nthis section.\nFig. 2. GGA architecture\nTo insert random individuals into a generation, the following\nchanges are necessary: In step 4 select fewer individuals that\n3are required to create the next population; step 7 is then\naltered to insert the processed individuals from step 6 into\na new generation, and to also introduce randomly generated\nindividuals termed migrants to maintain the population size\n(figure 3). The term migration rate defines the number of\nmigrants to insert into the new population, and hence the\nnumber of individuals to select in step 4 will be equal to the\noriginal population size less the migration rate.\nFig. 3. New generation compiled of processed individuals and random\nmigrants\nFor the development of this methodology, a realistic data\nsurface with multiple local minima, plateaus and one global\nminimum, representative of real-life experimental combinato-\nrial surfaces is considered. Later in this paper, the developed\nmethodology will be applied to a real-life hardware-in-the-\nloop experimental application. Inspection of the experimental\nsurface (figure 22) reveals the fundamental similarities of this\nkind of real problem to the development surfaces presented\nhere. The standard MATLAB peaks surface (figure 4) de-\nscribes a combinatorial process in two variables (equation 1):\nFig. 4. Smooth algorithm development fitness landscape: peaks0\ny = 3 (1\u2212 x1)2 .exp\n(\u2212x21 \u2212 x22)\n\u2212 10\n(x1\n5\n\u2212 x31 \u2212 x52\n)\n.exp\n(\u2212x21 \u2212 x22)\n\u2212 1\n3\n.exp\n(\u2212(x1 + 1)2 \u2212 x22) (1)\nIn order to investigate the effects of noise, progressively\nlarger amounts of Gaussian noise are added to the smooth\nsurface (peaks0) to give peaks 1,2,3 (figures 5, 6, 7). For the\nGA, performance is degraded by the number of local minima\nin the search space. Local optima are formed in this case\nby two mechanisms. The first mechanism is the underlying\nshape of the search space. Essentially, higher order functions\ntend to create more complex shapes with more local minima.\nMeasurement or Process noise adds numerous local minima\nto the underlying surface. The magnitude of the noise is given\nas a fraction of the range of values of this input array. The\naddition of the noise is achieved by utilising the R function\njitter written by Werner Stahel and Martin Maechler, ETH\nZurich. The jitter function adds a small amount of Gaussian\n(white) or uniform noise to a vector, matrix or N-D array.\nFig. 5. Rugged algorithm development fitness landscape: peaks1\nFig. 6. Rugged algorithm development fitness landscape: peaks2\nThe development surfaces have increasing levels of Gaus-\nsian noise imposed on the Peaks0 surface according to:\n\u2022 Peaks1 mean 0.1189, variance 0.0836\n\u2022 Peaks2 mean 0.2842, variance 0.3705\n\u2022 Peaks3 mean 1.7277, variance 0.7648\nIn order to examine the effectiveness of the method, another\nsearch space is introduced, namely the bump problem [21],\nwhich is a smooth surface comprising many peaks, all of a\n4Fig. 7. Rugged algorithm development fitness landscape: peaks3\nsimilar size. Also the optimal value is defined adjacent to a\nconstraint boundary. The Bump problem is defined as:\nmax\nabs\n(\u2211n\ni=k cos\n4 (xi)\u2212 2\n\u220fn\ni=1 cos\n2 (xi)\n)\u221a\u2211n\ni=1 ix\n2\ni\n(2)\nfor: 0 < xi < 10, i = 1, ..., n\nsubject to\n\u220fn\ni=1 xi < 0.75 and\n\u2211n\ni=1 xi < 15n\/2\nstarting from: xi = 5, i = 1, ..., n\nwhere the xi are the variables (in Radians) in the range\n0 to 10 subject to two constraints, and n is the number of\ndimensions.\nIt has been noted that these features render it relatively\ndifficult for most optimisers to deal with [22], (Figures 8, 9).\nFig. 8. Contour map for two-variable bumps function\nUsing this methodology with the GGA parameters as de-\nclared in Table (figure(10)), the range of surfaces Peaks0\nto Peaks3 and Bumps are searched to identify the global\nminimum. The GGA is run 100 times per surface, producing\nFig. 9. Two-variable bumps function surface\nmean results to negate the effects of the inherently stochastic\nheuristic.\nFig. 10. GGA parameters used for search\nFig. 11. Effects of increasing random migrants across the test surfaces with\na population size of 20, (upper value: mean, lower value: worst case)\nTable (figure(11)) shows the effect on computations of\ninserting random migrants into a population of size 20 for\na range of migration rates. A computation is counted as each\nevaluation of an individual. It shows that introducing random\nmigration for complex surfaces such as peaks3 and the bump\nyields a considerable decrease in the number of computations\ncompared to having no migrants. Figure 12 and figure 13\n5illustrate the effects of the different migration rates across\nthese surfaces, clearly showing that increasing the migration\nrate reduces computations until a critical point where the\nsearch starts to degrade. A justification for this observation\nis that as the migration rate increases, then so does the\ndiversity of the population with only a small number of highly\nranked individuals surviving. As the migration rate nears the\npopulation size, then the search is comparable to a random\nsearch. Observing the results from the less complex surfaces\nit can also be seen that a critical point also exists, albeit to a\nlesser degree, where a random migration rate is present that\nincreases the performance of the GA (Figure 14)).\nFig. 12. Mean computations on peaks3 surface showing the effects of varying\nthe number of random migrants for population of size 20\nFig. 13. Mean computations on bump surface showing the effects of the\nnumber of random migrants for population size of 20\nThe results show how introducing random migration into\nsingle-deme GAs can increase diversity, and hence lead to\ndramatic search improvements, particularly on rugged or com-\nplex surfaces. However, it is apparent that there is a critical\nmigration rate that varies according to the complexity of the\nsurface. A high random migration rate leads to excessive\ndiversity analogous to high mutation rates, where previous\nFig. 14. Mean computations for peaks0, peaks1 and peaks2 showing the\neffects of the number of random migrants for population size of 20\nwork has shown a similar effect [37], [42]. It can be seen\nthat low to mid random migration rates are a good trade-\noff between performance gains for complex surfaces, whilst\nminimising additional computation requirements for less com-\nplex surfaces. As with other genetic operators, the introduction\nof random migration has introduced another parameter that\nfor improved effectiveness would require a priori knowledge\nof the surface to set a random migration rate. However, the\nnext section will show that by applying decision support to\nrandom migration, it is possible to minimise penalties for\nhigher random migration rates on less complex surfaces.\nIII. CONTROLLED MIGRATION\nWe have discussed earlier in this paper how Genetic Algo-\nrithms have been shown to be compromised when directing\nsearch over noisy evaluation surfaces. As the amount of noise\ninherent in the surface increases in experimental applications\nwith additive process, measurement and sensor noise, it is\nlikely that the number of local optima increases and, this often\ncauses the GA to converge on these local optima. We have\nshown in the previous section that the introduction of a random\nmigration operator can reduce the steps to convergence of a\nGA presented with noisy evaluation surfaces. The authors have\npreviously produced a methodology, which makes use of the\ndata evaluated by the heuristic during the search, and utilizes\nthis to produce response surfaces. These response surfaces are\nused to generate probability surfaces to provide the search\nheuristic with weighted stochastic decision support (WSDS).\nSince this methodology has shown excellent results when\napplied with other heuristics, in this section, we aim to com-\nbine the beneficial effects of migration and weighted decision\nsupport with the aim of achieving even higher performance\nlevels when searching noisy surfaces. A Weighted Stochastic\nDecision Support (WSDS) Operator method introduced in [38]\nis applied to random migration to create a novel operator\ntermed controlled migration.\nThe methodology updates itself with data as it is gathered,\nto map areas of potential interest to direct experimentation\n6based upon previous results. It is an extremely compact and\ntractable representation, based upon polynomial response sur-\nfaces, which retains a generalized approximation of the search\nspace. The method approximates the incoming and historical\ndata with a polynomial function, often a second order of the\nform\n\u03b7 = \u03b20 +\nk\u2211\nj=1\n\u03b2jxj +\nk\u2211\nj=1\n\u03b2jjx\n2\nj +\n\u2211\ni<j\n\u2211\n\u03b2ijxixj (3)\nIt may be necessary to employ an approximating function\ngreater than two, based upon standard Taylor series expan-\nsion. In this paper, a standard second order approximation\nis employed. The parameter set is estimated by least squares\nregression analysis. With n < k, an observed response\ny1, Y2, ..., yn\nis associated with regression variables such that xij denotes\nthe ith observation of variable xj . Assuming that the error term\n\u000f has E(\u000f) = 0 and V ar(\u000f) = \u03c32 and the \u000fi are uncorrelated\nvariables. The model can now be expressed in terms of the\nobservations\nyi = \u03b20 + \u03b21xj1 + \u03b22xj2 + ...+ \u03b2kxjk + \u000fj\nj = 1, 2, ..., n (4)\nThe \u03b2 coefficients in (4) are chosen such that the sum of\nthe squares of the errors \u000fi are minimized via the least squares\nfunction\nL =\nn\u2211\nj=1\n\u000f2i =\nn\u2211\nj=1\n\uf8eb\uf8edyi \u2212 \u03b20 \u2212 n\u2211\nj=1\n\u03b2jxij\n\uf8f6\uf8f8 (5)\nThus, as data from the experimental results are gathered\nunder the direction of the GA, it is possible to generate a\nsurface approximation for the system under consideration.\nSince the true system response surface is unknown, this\nrepresents the current view of the likely response. It is this\npolynomial which forms the basis for the controlled migration.\nThe y values are normalized according to\nynorm = 1\u2212\n(\ny \u2212 (max(y) +min(y))\/2\n(max(y)\u2212min(y))\/2 + 1\n)\n\/2 (6)\nwhich yields a surface over the search space bounded\nbetween zero and one, where increasing value represents\nincreasing interest, inferred from previous evaluations. These\nmonotonically increasing values correspond to co-ordinates in\na probability space from which migrants are chosen according\nto random selection, with probability of being chosen based\non relative value in the probability space.\nFigure 15 illustrates how the WSDS is integrated into\nthe GGA methodology, with the additional steps coloured\nin red. During the first generation, the evaluation results\nfrom each individual in the population are used collectively\nto provide the data to fit the normalised response surface\nto. This response surface is then used to create the WSDS\nsurface as defined in the accompanying paper. According to\nFig. 15. GGA methodology with addition of WSDS random immigrants\nthe controlled migration rate, a number of migrants are then\nprobabilistically selected from the WSDS surface and inserted\ninto the new generation, along with the individuals processed\nby the standard GGA operators. This procedure is repeated\nfor each generation, with the evaluation of each individual\nfeeding into the data used to update the normalised response\nsurface, thus the probability of selection of the next controlled\nmigrants are based statistically on the results of the previous\ngenerations.\nUsing the GGA parameters previously presented, the ex-\nperiment from the previous section is repeated replacing the\nrandom migrants with controlled migrants using the prescribed\nmethod for a range of controlled migration rates. Based on the\nresults of the WSDS methodology applied to gradient descent\nmethods, a 2nd order support surface is chosen. The search is\nconducted running the GGA on each surface 100 times to yield\nthe mean and maximum number of computations as shown in\ntable (figure(16)).\nFig. 16. Effects of increasing controlled migrants across the test surfaces\nwith a population size of 20\nFrom the results it is immediately evident that using the\ncontrolled migration gives a 35% improvement in perfor-\nmance at migration rates of interest compared to using random\n7migration. Figure 17 and figure 18 illustrate this improvement\nusing the total mean and max computations respectively across\nall the surfaces. Comparing the totals across all surfaces is\njustified, as although the more complex surfaces contribute\nmost to the improvements observed, there are no significant\ndeclines in performance for less complex surfaces. Moreover,\ncontrolled migration appears to minimise penalties for higher\nmigration rates on the basic surfaces (figure 19 and figure 20).\nThis further endorses the generality of controlled migration as\na viable operator to speed-up GA searches, as whilst it appears\nto cause no significant detrimental affect, it can provide a\nmajor performance boost on such surfaces as the bump (figure\n21).\nFig. 17. Comparison of random migration vs. controlled migration for total\nmean computations across all surfaces\nFig. 18. Comparison of random migration vs. controlled migration for total\nmaximum computations across all surfaces\nIV. EXPERIMENTAL HARDWARE-IN-THE-LOOP\nAPPLICATION\nThe method, which in the previous section was applied\nto test surfaces, is now applied to an experimental auto-\nmotive combinatorial search. It is desired to identify the\nFig. 19. Comparison of random migration vs. controlled migration for mean\ncomputation on peaks0\nFig. 20. Comparison of random migration vs. controlled migration for mean\ncomputation on peaks\nmaximum power output of an experimental single-cylinder\nspark-ignition engine operating under a novel control regime.\nThe programme conducts peak power experiments under\ncombinatorial conditions to identify global peak power for\nsubsequent design procedures [38]. In order to evaluate the\nrelative performance of the Controlled Migration method, the\nengine was characterized by exhaustive designed experiment,\nshown in figure 22.\nIn the experimental series, both standard and controlled mi-\ngration, GAs were utilized to guide the search for a maximum\npower point. Due to the stochastic nature of the Metaheuristics,\neach method was run 100 times in order to produce mean\nperformance evaluations. A comparison was also performed\nbetween population sizes of 20 (Figure 23) and 40 (Figure\n24).\nIn both cases, at significant migration performance, the GA\nrunning a controlled migration policy outperforms the standard\nmigration operator.\nV. CONCLUSIONS\nThis paper introduces a novel decision support method-\nology based upon response surfaces. The method had been\n8Fig. 21. Comparison of random migration vs. controlled migration for mean\ncomputations on bump\nFig. 22. Engine experimental map for peak cylinder pressure for given\nthrottle, spark and injection settings\npreviously applied to add decision support to the previously\nrandom jumps of gradient descent methods commonly used\nin combinatorial experimentation [20]. The response surfaces\nare generated through exploitation of evaluated data that is\nperformed during the search, valuable additional information\nwhich should not be discarded. These response surfaces are\ntransformed into normalised contours of the search area,\nproviding weighted stochastic decision support for migration.\nThe decision support methodology was applied to GAs, first\ninvestigating a mechanism for the introduction of a decision\nsupported operator. Migration in single-deme GAs of random\nindividuals was investigated as an alternative to mutation as\na means of maintaining diversity in each generation. Random\nmigration is then demonstrated to provide substantial improve-\nments in the efficiency of a GA when faced with more complex\nor rugged surfaces that contain many local optima. Moreover,\nthis migration operator provides the mechanism for which to\napply decision support, and is introduced as controlled migra-\ntion. Through a comparison with random migration, controlled\nmigration is shown to provide an improvement in required\ncomputations by up to a factor of two. With both migration\nFig. 23. Experimental results for 20 population\nFig. 24. Experimental results for 40 population\noperators, it is apparent that there is a critical migration rate\nthat varies according to the complexity of the surface. A\nhigh migration rate leads to excessive diversity analogous to\nhigh mutation rates. A low migration rate, whilst providing\nminimum risk for computation penalties for simple surfaces,\ndoes not exploit the benefits attainable when applied to more\ncomplex surfaces. However, using controlled migration over\nrandom migration is shown to allow higher migration rates,\nwhilst minimising the detrimental effects on simple surfaces.\nThis can be explained as whilst exploring simple surfaces, the\ndecision support surface is more likely to provide a reliable\nestimate as to where the minimum or maximum lies. Using\nhigh controlled migration rates, it is more probable that good\ncandidate individuals are chosen. In the context of hardware-\nin-the-loop experimentation, the methodology has the potential\nfor significant cost savings, since each experimental evaluation\nin the search has associated expense in terms of time and\nhardware costs.\nREFERENCES\n[1] Alander, T. (1992). On Optimal Population Size of Genetic Algorithms.\nIEEE International Conference on Computer Systems and Software\nEngineering (pp. 65-70). IEEE.\n9[2] AlSharafat W.S. and AlSharafat, M.S. (2010), Adaptive Steady State\nGenetic Algorithm for scheduling university exams, Networking and\nInformation Technology (ICNIT), 2010 International Conference on, 70-\n74.\n[3] Back, T. (1992). Self-Adaptation in Genetic Algorithms. Proceedings of\nthe 1st European Conference on Artificial Life, (pp. 263-271).\n[4] Baker, J. E. (1987). Adaptive Selection Methods for Genetic Algorithms.\nIn Eribaum (Ed.), Proceedings of the 2nd International Conference on\nGenetic Algorithms and their Application, (pp. 14-21). Cambridge MA.\n[5] Chen L. (2009), An Adaptive Genetic Algorithm Based on Population\nDiversity Strategy, Genetic and Evolutionary Computing, 2009. WGEC\n\u201909. 3rd International Conference on, 93-96.\n[6] DeJong, K. A. (1975). An Analysis of the Behaviour of a Class\nof Genetic Adaptive Systems. University of Michigan, Department\nof Computer and Communication Science. Ann Arbor: University of\nMichigan.\n[7] DeJong, K. A. (1992). Are Genetic Algorithms Function Optimizers?\nParallel Problem Solving from Nature PPSN2. Elsevier.\n[8] Eiben A.E., Schut, M.C. and de Wilde, A.R. (2006), Evolutionary\nComputation, 2006. CEC 2006. IEEE Congress on, 477 - 482.\n[9] ]Fogarty, T. C. (1989). Varying the Probability of Mutation in the Genetic\nAlgorithm. Proceedings of the 3rd International Conference on Genetic\nAlgorithms, (pp. 104-109).\n[10] Fogel, D. B. (2006). Evolutionary Computation: Towards a New Philos-\nophy of Machine Intelligence. New York: IEEE Press.\n[11] Furutani, H., Fujimaru, T., Yu-an Zhang and Sakamoto, M. (2007),\nEffects of Population Size on Computational Performance of Genetic\nAlgorithm on Multiplicative Landscape, Natural Computation, 2007.\nICNC 2007. Third International Conference on, 488 - 496.\n[12] Garcia-Martinez C. and Lozano H. (2009), Continuous variable neigh-\nbourhood search algorithm based on evolutionary metaheuristic compo-\nnents: a scalability test. 9th Int. Conf. on Intelligent Systems Design\nand Applications, 1074-1079.\n[13] Goldberg, D. E. (1991). A Comparative Analysis of Selection Schemes\nUsed in Genetic Algorithms. In D. E. Goldberg, Genetic Algorithms:\nFoundations of Genetic Algorithms (pp. 69-93). San Mateo, CA, USA:\nMorgan Kauffmann.\n[14] Goldberg, D. E. (1989). Genetic Algorithms in Search, Optimization and\nMachine Learning. Reading, MA: Addison-Wesley.\n[15] Goldberg, D. E., Deb, K. and Clark, J. (1992). Genetic Algorithms,\nNoise and the Sizing of Populations. Complex Systems , 6, 333-362.\n[16] Gong D., Yuan J. and Ma X. (2008), Interactive genetic algorithms\nwith large population size, Evolutionary Computation, 2008. CEC 2008.\n(IEEE World Congress on Computational Intelligence). IEEE Congress\non, 1678 - 1685.\n[17] Hu, S. and Zhu, Q. (2003). Stochastic optimal control and analysis of\nstability of networked control systems with long delay. Automatica , 39\n(11), 1877-1884.\n[18] Gouvenc B.A., Gouvenc L. and Karaman S. (2009). Robust yaw stability\ncontroller design and hardware in the loop testing for a road vehicle.\nIEEE Trans. Veh. Tech. 58 (2), 551-571.\n[19] Isermann, R., Schaffnit, J. and Sinsel, S. (1999). Hardware-in-the-loop\nsimulation for the design and testing of engine-control systems. Control\nEngineering Practice , 5 (7), 643-653.\n[20] Jaskiewiscz, A. (1998). Genetic Local Search for Multiple Objective\nCombinatorial Optimisation. Technical Report RA-GL4\/98, Poznan Uni-\nversity, Institute of Computing Science, Poznan.\n[21] Keane, A. J. (1994). Experience With optimizers in Structural Design.\nIn I. C. Parmee (Ed.), Proceedings of the Conference on Adaptive Com-\nputing in Engineering Design and Control \u201994, (pp. 14-27). Plymouth,\nUK.\n[22] Keane, A. J. (1995). Genetic Algorithm Optimisation of Multi-Peak\nProblems: Studies in Convergence and Robustness. International Journal\nof Artificial Intelligence in Engineering , 9 (2), 75-83.\n[23] Li N and Ye F. (2006), Optimal Design of Discrete Structure with\nDirected Mutation Genetic Algorithms, Intelligent Control and Automa-\ntion, 2006. WCICA 2006. The Sixth World Congress on, 3663 - 3667.\n[24] Liu L.-M., Wang N.-P. and Li F.-C. (2009), Study on convergence of self-\nadaptive and multi-population composite Genetic Algorithm, Machine\nLearning and Cybernetics, 2009 International Conference on, 2680 -\n2685.\n[25] Mann, K. F., Tang, K. S., Kwong, S. and Halang, W. A. (1999). Genetic\nAlgorithms: Concepts and Designs. London: Springer-Verlag.\n[26] Michalewicz, Z. and Fogel, D. B. (2006). How to Solve It: Algorithms\nfor Engineering Systems. Cambridge, UK: Cambridge University Press.\n[27] Muhlenbein, H. (1992). How Genetic Algorithms Really Work. Mutation\nand Hill-Climbing. In R. Manner and R. Manderick (Ed.), Parallel\nProblem Solving from Nature PPSN II, (pp. 15-25). Amsterdam.\n[28] Nakama T. (2009), Transition and convergence properties of genetic\nalgorithms applied to fitness functions perturbed concurrently by ad-\nditive and multiplicative noise. 2009 IEEE Congress on Evolutionary\nComputation, 2662-2669.\n[29] Nsakandaa A.L., Wilson L., Price B., Moustapha D. and Marc G. (2006),\nEnsuring population diversity in genetic algorithms: A technical note\nwith application to the cell formation problem, European Journal of\nOperational Research, 178, (2), 634-638.\n[30] Oh, S. C. (2005). Evaluation of motor characteristics for hybrid electric\nvehicles using the hardware-in-the-loop concept. IEEE Transactions on\nVehicular Technology , 54 (3), 817-824.\n[31] Petter, O., Fiorelli, E. and Leonard, N. E. (2004). Cooperative Control of\nMobile Sensor Networks: Adaptive Gradient Climbing in a Distributed\nEnvironment. IEEE Transactions on Automatic control , 49 (8), 1292-\n1302.\n[32] Po L.M., Ng K.H., Cheung K.-W., Wong K.-H., Uddin Y. and Ting C.-W.\n(2009), Novel directional gradient descent searches for fast block motion\nestimation. IEEE Trans. Circuits and Signals for Video Technology, 19\n(8), 1189-1195.\n[33] Power, D., Ryan C. and Azad, R.M.A. (2005), Promoting diversity\nusing migration strategies in distributed genetic algorithms, Evolutionary\nComputation, 2005. The 2005 IEEE Congress on, 1831 - 1838 Vol. 2.\n[34] Qiu T. and Ju G. (2010), A selective migration parallel multi-objective\ngenetic algorithm, Control and Decision Conference (CCDC), 2010\nChinese, 463 - 467.\n[35] Schaffer, J. D., Caruana, R. A., Eshelman, L. J. and Das, R. (1989). A\nstudy of control parameters affecting online performance of genetic al-\ngorithms for function optimisation. Proceedings of the 3rd International\nConference on Genetic Algorithms (pp. 51-60). Los Altos CA: Morgan\nKaufman.\n[36] Smith K.I., Eversen R.M., Murphy C. and Misra R. (2008), Dominance\nbased multiobjective simulated annealing, IEEE Trans. Evolutionary\nComputation 12 (3), 323-342.\n[37] Spears, W. M. (1992). Crossover or Mutation. In Whitley (Ed.), Foun-\ndations of Genetic Algorithms -2 (pp. 221-237). San Mateo, CA, USA:\nMorgan Kauffmann.\n[38] Stewart, P. G., Gladwin, D., Stewart, J., Chen, R. and Winward, E.\n(2010). Improved Decision Support for Engine-in-the-Loop Experimen-\ntal Design Optimisation. IMechE Part D - Automobile Engineering , In\nPress.\n[39] Thierens, D. (2002). Adaptive Mutation Rate Control Schemes in\nGenetic Algorithms. IEEE International Conference on E-Commerce,\nEvolutionary Computation. 1, pp. 980-985. Piscataway: IEEE.\n[40] Vavak, F. and Fogarty, T. C. (1996). A Comparative Study of Steady-\nState and Generational Genetic Algorithms. Selected Papers from AISB\nWorkshop on Evolutionary Computing (pp. 297-304). AISB.\n[41] Whitely, D. and Kauth, J. (1998). GENITOR: A Different Genetic Al-\ngorithm. Rocky Mountain Conference on Artificial Intelligence. Denver.\n[42] Zhang Y., Sakamoto, M. and Furutani, H. (2009), Effects of String\nLength and Mutation Rate on Success Probability of Genetic Algorithm,\nNatural Computation, 2009. ICNC \u201909. Fifth International Conference\non, 211 - 216.\n"}