{"doi":"10.1080\/0968776042000259546","coreId":"14177","oai":"oai:generic.eprints.org:608\/core5","identifiers":["oai:generic.eprints.org:608\/core5","10.1080\/0968776042000259546"],"title":"Implementation of computer assisted assessment: lessons from the literature","authors":["Sim, Gavin","Holifield, Phil","Brown, Martin"],"enrichments":{"references":[{"id":1879502,"title":"A briefing on key concepts formative and summative, criterion and norm-referenced assessment (York, LTSN Generic Centre).","authors":[],"date":"2001","doi":null,"raw":"Knight, P. (2001) A briefing on key concepts formative and summative, criterion and norm-referenced assessment (York, LTSN Generic Centre).","cites":null},{"id":1879547,"title":"A linguistic perspective on multiple choice questioning,","authors":[],"date":"2000","doi":"10.1080\/713611429","raw":"Paxton, M. (2000) A linguistic perspective on multiple choice questioning,  Assessment and Evaluation in Higher Education, 25(2), 109\u2013119.","cites":null},{"id":1879579,"title":"A note on multiple choice exams, with respect to students\u2019 risk preference and confidence,","authors":[],"date":"2001","doi":"10.1080\/02602930120052413","raw":"Walker, D. M. & Thompson, J. S. (2001) A note on multiple choice exams, with respect to students\u2019 risk preference and confidence,  Assessment and Evaluation in Higher Education, 26(3), 261\u2013267.","cites":null},{"id":1879500,"title":"A practical look at delivering assessment to BS7988 recommendations,","authors":[],"date":"2002","doi":null,"raw":"Kleeman, J. & Osborne, C. (2002) A practical look at delivering assessment to BS7988 recommendations, Proceedings of the 6th International Computer Assisted Assessment Conference (Loughborough, Loughborough University), 163\u2013170.","cites":null},{"id":1879511,"title":"A secure on-line submission system,","authors":[],"date":"1999","doi":"10.1002\/(sici)1097-024x(19990710)29:8<721::aid-spe257>3.3.co;2-s","raw":"Luck, M. and Joy, M. (1999) A secure on-line submission system,  Software\u2014Practice and Experience, 29(8), 721\u2013740.","cites":null},{"id":1879480,"title":"A survey of assessment methods employed in UK higher education programmes for HCI courses,","authors":[],"date":"2004","doi":null,"raw":"Graham, D. (2004) A survey of assessment methods employed in UK higher education programmes for HCI courses, Proceedings of the 7th HCI Educators Workshop (Preston, LTSN), 66\u201369.","cites":null},{"id":1879525,"title":"A taxonomy for computer-based assessment of problem solving,","authors":[],"date":"2002","doi":"10.1016\/s0747-5632(02)00020-1","raw":"Mayer, R. E. (2002) A taxonomy for computer-based assessment of problem solving, Computers in Human Behaviour, 18(6), 623\u2013632.","cites":null},{"id":1879451,"title":"A taxonomy for learning, teaching, and assessing. A revision of blooms taxonomy of educational objectives","authors":[],"date":"2001","doi":null,"raw":"Anderson, L. W. & Krathwohl, D. R. (2001) A taxonomy for learning, teaching, and assessing. A revision of blooms taxonomy of educational objectives (New York, Longman).","cites":null},{"id":1879587,"title":"Accessibility and computer-based assessment: a whole new set of issues? in:","authors":[],"date":"2002","doi":null,"raw":"Wiles, K. (2002) Accessibility and computer-based assessment: a whole new set of issues? in: L. Phipps, A. Sutherland & J. Seale (Eds) Access all areas: disability, technology and learning (Oxford and York, ALT\/TechDis), 61\u201366.","cites":null},{"id":1879482,"title":"Allowing for guessing and for expectations from the learning outcomes in computer-based assessments,","authors":[],"date":"2002","doi":null,"raw":"Harper, R. (2002) Allowing for guessing and for expectations from the learning outcomes in computer-based assessments, Proceedings of the 6th International Computer Assisted Assessment Conference (Loughborough, Loughborough University), 139\u2013150.","cites":null},{"id":1879463,"title":"Alternative marking schemes for on-line multiple choice tests,","authors":[],"date":"1999","doi":null,"raw":"Bush, M. (1999) Alternative marking schemes for on-line multiple choice tests, Proceedings of the 7th Annual Conference on the Teaching of Computing (Belfast, Elsevier).","cites":null},{"id":1879474,"title":"An XML question bank using Microsoft Office,","authors":[],"date":"2002","doi":null,"raw":"Daly, J. (2002) An XML question bank using Microsoft Office, Proceedings of the 6th International Computer Assisted Assessment Conference (Loughborough, Loughborough University), 107\u2013118.","cites":null},{"id":1879512,"title":"Assessing activity-based learning for a networked course,","authors":[],"date":"2002","doi":"10.1111\/1467-8535.00295","raw":"Macdonald, J. & Twining, P. (2002) Assessing activity-based learning for a networked course, British Journal of Educational Technology, 33(5), 603\u2013618.","cites":null},{"id":1879453,"title":"Assessing the use of a new QTI assessment tool within Physics,","authors":[],"date":"2003","doi":null,"raw":"Bacon, R. A. (2003) Assessing the use of a new QTI assessment tool within Physics, Proceedings of the 7th International Computer Assisted Assessment Conference (Loughborough, Loughborough University), 33\u201344.","cites":null},{"id":1879494,"title":"Assessing using grade-related criteria: a single currency for universities? Assessment and Evaluation","authors":[],"date":"2003","doi":"10.1080\/0260293032000066254","raw":"Horney, W. (2003) Assessing using grade-related criteria: a single currency for universities? Assessment and Evaluation in Higher Education, 28(4), 435\u2013454.","cites":null},{"id":1879519,"title":"Assessment for learning: the Triads assessment of learning outcomes project and the development of a pedagogically friendly computer-based assessment system, in: D. O\u2019Hare & D. Mackenzie (Eds) Advances in computer aided assessment","authors":[],"date":"2004","doi":null,"raw":"Mackenzie, D., O\u2019Hare, D., Paul, C., Boyle, A., Edwards, D., Willimas, D. & Wilkins, H. (2004) Assessment for learning: the Triads assessment of learning outcomes project and the development of a pedagogically friendly computer-based assessment system, in: D. O\u2019Hare & D. Mackenzie (Eds) Advances in computer aided assessment (Birmingham, SEDA), 11\u201325.","cites":null},{"id":1879470,"title":"Assessment methods in social work education: a review of the literature,","authors":[],"date":"2002","doi":"10.1080\/02615470220126471","raw":"Crisp, B. R. (2002) Assessment methods in social work education: a review of the literature, Social Work Education, 21(2), 259\u2013269.","cites":null},{"id":1879567,"title":"At the coal face: experience of computer based exams,","authors":[],"date":"2003","doi":null,"raw":"Sealey, C., Humphries, P. & Reppert, D. (2003) At the coal face: experience of computer based exams,  Proceedings of the 7th International Computer Assisted Assessment Conference (Loughborough, Loughborough University), 357\u2013376.","cites":null},{"id":1879466,"title":"Automated essay marking for both content and style,","authors":[],"date":"1999","doi":null,"raw":"Christie, J. R. (1999) Automated essay marking for both content and style, Proceedings of the 3rd Annual Computer Assisted Assessment Conference (Loughborough, Loughborough University).","cites":null},{"id":1879460,"title":"Automated evaluation of essays and short answers,","authors":[],"date":"2001","doi":null,"raw":"Burstein, J., Leacock, C. & Swartz, R. (2001) Automated evaluation of essays and short answers, Proceedings of the 5th International Computer Assisted Assessment Conference  (Loughborough, Loughborough University).","cites":null},{"id":1879459,"title":"Blueprint for computer-assisted assessment (Loughborough,","authors":[],"date":"2001","doi":"10.4324\/9780203464687","raw":"Bull, J. & McKenna, C. (2001) Blueprint for computer-assisted assessment  (Loughborough, Loughborough University).","cites":null},{"id":1879581,"title":"CAA in UK HEIs\u2014the state of the art?","authors":[],"date":"2003","doi":null,"raw":"Warburton, B. & Conole, G. (2003) CAA in UK HEIs\u2014the state of the art? Proceedings of the 7th International Computer Assisted Assessment Conference  (Loughborough, Loughborough University), 433\u2013441.","cites":null},{"id":1879530,"title":"CAA scoring strategies for partial credit and confidence levels,","authors":[],"date":"2003","doi":null,"raw":"McCabe, M. & Barrett, D. (2003) CAA scoring strategies for partial credit and confidence levels, Proceedings of the 7th International Computer Assisted Assessment Conference  (Loughborough, Loughborough University), 209\u2013219.","cites":null},{"id":1879493,"title":"Careless about privacy,","authors":[],"date":"2003","doi":"10.1016\/s0167-4048(03)00403-6","raw":"Hindle, S. (2003) Careless about privacy, Computers and Security, 22(4), 284\u2013288.","cites":null},{"id":1879590,"title":"Chat as media in exams,","authors":[],"date":"2002","doi":"10.1007\/978-0-387-35596-2_55","raw":"Wiltfelt, C., Philipsen, P. E. & Kaiser, B. (2002) Chat as media in exams,  Education and Information Technologies, 7(4), 343\u2013349.","cites":null},{"id":1879458,"title":"Code of practice for the use of information technology (IT) in the delivery of assessment","authors":[],"date":null,"doi":null,"raw":"BS7988 (2002) Code of practice for the use of information technology (IT) in the delivery of assessment Bull, J., Conole, G., Davis, H. C., White, S., Danson, M. & Sclater, N. (2002) Rethinking assessment through learning technologies,  Proceedings of the ASCILITE 2002  (Auckland, UNITEC), 1\u201312.","cites":null},{"id":1879465,"title":"Cognitive test anxiety and academic performance,","authors":[],"date":"2002","doi":"10.1006\/ceps.2001.1094","raw":"Cassady, J. C. & Johnson, R. E. (2002) Cognitive test anxiety and academic performance, Contemporary Educational Psychology, 27(2), 270\u2013295.","cites":null},{"id":1879527,"title":"Comprehension and workload differences for VDT and paper-based reading,","authors":[],"date":"2001","doi":"10.1016\/s0169-8141(01)00043-9","raw":"Mayes, D. K., Sims, V. K. & Koonce, J. M. (2001) Comprehension and workload differences for VDT and paper-based reading,  International Journal of Industrial Ergonomics,  28(6), 367\u2013378.","cites":null},{"id":1879497,"title":"Computer aided assessment using WebCT,","authors":[],"date":"2000","doi":null,"raw":"Jefferies, P., Constable, I., Kiely, B., Richardson, D. & Abraham, A. (2000) Computer aided assessment using WebCT,  Proceedings of the 4th International Computer Assisted Assessment Conference (Loughborough, Loughborough University).","cites":null},{"id":1879548,"title":"Computer assisted assessment and disabilities,","authors":[],"date":"2001","doi":null,"raw":"Phipps, L. & McCarthy, D. (2001) Computer assisted assessment and disabilities, Proceedings of the 5th International Computer Assisted Assessment Conference (Loughborough, Loughborough University).","cites":null},{"id":1879488,"title":"Computer assisted assessment implementing CAA in FE sector in Scotland: question types (Glenrothes,","authors":[],"date":"2002","doi":null,"raw":"Herd, G. & Clark, G. (2002) Computer assisted assessment implementing CAA in FE sector in Scotland: question types (Glenrothes, Glenrothes College).","cites":null},{"id":1879577,"title":"Computer based assessment systems evaluation via the ISO90126 quality model,","authors":[],"date":"2002","doi":null,"raw":"Valenti, S., Cucchiarelli, A. & Panti, M. (2002) Computer based assessment systems evaluation via the ISO90126 quality model, Journal of Information Technology Education, 1(3), 157\u2013175.","cites":null},{"id":1879573,"title":"Computer-assisted assessment: suggested guidelines for an institutional strategy, Assessment and Evaluation","authors":[],"date":"1998","doi":"10.1080\/0260293980230305","raw":"Stephens, D., Bull, J. & Wade, W. (1998) Computer-assisted assessment: suggested guidelines for an institutional strategy,  Assessment and Evaluation in Higher Education,  23(3), 283\u2013294.","cites":null},{"id":1879596,"title":"Computer-based assessment: quality assurance issues, the hub of the wheel, Assessment and Evaluation","authors":[],"date":"2003","doi":"10.1080\/0260293032000130243","raw":"Zakrzewski, S. & Steven, S. (2003) Computer-based assessment: quality assurance issues, the hub of the wheel, Assessment and Evaluation in Higher Education, 28(6), 609\u2013623.","cites":null},{"id":1879536,"title":"Computer-based testing. Building the foundation for future assessments (Mahwah, Lawrence Erlbaum Associates).","authors":[],"date":"2002","doi":null,"raw":"Mills, C. N., Potenza, M. T., Fremer, J. J. & Ward, W. C. (2002) Computer-based testing. Building the foundation for future assessments (Mahwah, Lawrence Erlbaum Associates).","cites":null},{"id":1879504,"title":"Computerised adaptive testing,","authors":[],"date":"2002","doi":"10.1111\/1467-8535.00296","raw":"Latu, E. & Chapman, E. (2002) Computerised adaptive testing, British Journal of Educational Technology, 33(5), 619\u2013622.","cites":null},{"id":1879588,"title":"Constructing accessible CBA: minor works or major renovations?","authors":[],"date":"2003","doi":null,"raw":"Wiles, K. & Ball, S. (2003) Constructing accessible CBA: minor works or major renovations? Proceedings of the 7th International Computer Assisted Assessment Conference  (Loughborough, Loughborough University), 445\u2013451.","cites":null},{"id":1879483,"title":"Correcting computer-based assessment for guessing,","authors":[],"date":"2003","doi":"10.1046\/j.0266-4909.2002.00001.x","raw":"Harper, R. (2003) Correcting computer-based assessment for guessing,  Journal of Computer Assisted Learning, 19(1), 2\u20138.","cites":null},{"id":1879454,"title":"Correcting grade deflation caused by multiple-choice scoring,","authors":[],"date":"2000","doi":"10.1080\/002073900287147","raw":"Baranchik, A. & Cherkas, B. (2000) Correcting grade deflation caused by multiple-choice scoring, International Journal of Mathematical Education in Science and Technology, 31(3), 371\u2013380.","cites":null},{"id":1879584,"title":"Creating large scale test banks: a briefing for participative discussion and agendas,","authors":[],"date":"2000","doi":null,"raw":"White, S. & Davis, H. C. (2000) Creating large scale test banks: a briefing for participative discussion and agendas, Proceedings of the 4th International Computer Assisted Assessment Conference (Loughborough, Loughborough University).","cites":null},{"id":1879555,"title":"Designing online quiz questions to assess a range of cognitive skills,","authors":[],"date":"2002","doi":null,"raw":"Reid, N. (2002) Designing online quiz questions to assess a range of cognitive skills, Proceedings of the World Conference on Educational Multimedia, Hypermedia and Telecommunications (Denver, AACE), 1625\u20131630.","cites":null},{"id":1879533,"title":"Development of computerised assessment (TRIADS)","authors":[],"date":"2004","doi":null,"raw":"McLaughlin, P. J., Fowell, S. L., Dangerfield, P. H., Newton, D. J. & Perry, S. E. (2004) Development of computerised assessment (TRIADS) in an undergraduate medical school, in: D. O\u2019Hare & D. Mackenzie (Eds)  Advances in computer aided assessment  (Birmingham, SEDA), 25\u201332.","cites":null},{"id":1879594,"title":"Does grading method influence honours degree classification? Assessment and Evaluation","authors":[],"date":"2002","doi":"10.1080\/02602930220138624","raw":"Yorke, M., Barnett, G., Bridges, P., Evanson, P., Haines, C., Jenkins, D., Knight, P., Scurry, D., Stowell, M. & Woolf, H. (2002) Does grading method influence honours degree classification? Assessment and Evaluation in Higher Education, 27(3), 269\u2013279.","cites":null},{"id":1879521,"title":"Electronic security is a continous process,","authors":[],"date":"2003","doi":"10.1016\/s1361-3723(03)01013-3","raw":"Mason, S. (2003) Electronic security is a continous process, Computer Fraud and Security, 2003(1), 13\u201315.","cites":null},{"id":1879517,"title":"Empirical prediction of the measurement scale and base level \u2018Guess Factor\u2019 for advanced computer-based assessment,","authors":[],"date":"2002","doi":null,"raw":"Mackenzie, D. & O\u2019Hare, D. (2002) Empirical prediction of the measurement scale and base level \u2018Guess Factor\u2019 for advanced computer-based assessment, Proceedings of the 6th International Conference of Computer Aided Assessment (Loughborough, Loughborough University), 187\u2013201.","cites":null},{"id":1879478,"title":"Essentials of educational measurement (Englewood Cliffs,","authors":[],"date":"1972","doi":"10.1177\/001316447203200337","raw":"Ebel, R. L. (1972) Essentials of educational measurement (Englewood Cliffs, Prentice-Hall).226 G. Sim et al.","cites":null},{"id":1879550,"title":"Evaluating the costs and benefits of Changing to CAA,","authors":[],"date":"2000","doi":null,"raw":"Pollock, M. J., Whittington, C. D. & Doughty, G. F. (2000) Evaluating the costs and benefits of Changing to CAA, Proceedings of the 4th International Computer Assisted Assessment Conference (Loughborough, Loughborough University).","cites":null},{"id":1879471,"title":"Experience of using computer assisted assessment in engineering mathematics,","authors":[],"date":"2001","doi":"10.1016\/s0360-1315(01)00034-3","raw":"Croft, A. C., Danson, M., Dawson, B. R. & Ward, J. P. (2001) Experience of using computer assisted assessment in engineering mathematics, Computers and Education, 37(1), 53\u201366.","cites":null},{"id":1879507,"title":"Exploring the use of multimedia examination formats in undergraduate teaching: results from the fielding testing,","authors":[],"date":"2001","doi":"10.1016\/s0747-5632(01)00008-5","raw":"Liu, M., Papathanasiou, E. & Hao, Y. (2001) Exploring the use of multimedia examination formats in undergraduate teaching: results from the fielding testing,  Computers in Human Behaviour, 17(3), 225\u2013248.","cites":null},{"id":1879457,"title":"Finding appropriate methods to assure quality computer-based assessment development","authors":[],"date":"2003","doi":null,"raw":"Boyle, A. & O\u2019Hare, D. (2003) Finding appropriate methods to assure quality computer-based assessment development in UK higher education, Proceedings of the 7th International Computer Assisted Assessment Conference, (Loughborough, Loughborough University), 67\u201382.","cites":null},{"id":1879476,"title":"Flexibility and the technology of computer aided assessment,","authors":[],"date":"1998","doi":null,"raw":"Dowsing, R. D. (1998) Flexibility and the technology of computer aided assessment, Proceedings of the ASCILITE 1998 (Wollongong, University of Wollongong), 163\u2013171.","cites":null},{"id":1879562,"title":"Glare from monitors measured with subjective scales and eye movements,","authors":[],"date":"1999","doi":"10.1016\/s0141-9382(98)00055-9","raw":"Schenkman, B., Fukuda, T. & Persson, B. (1999) Glare from monitors measured with subjective scales and eye movements, Displays, 20, 11\u201321.","cites":null},{"id":1879558,"title":"Improving student performance through computer-based assessment: insights from recent research,","authors":[],"date":"2002","doi":"10.1080\/0260293022000009348","raw":"Ricketts, C. & Wilks, S. (2002b) Improving student performance through computer-based assessment: insights from recent research, Assessment and Evaluation in Higher Education, 27(5), 475\u2013479.","cites":null},{"id":1879473,"title":"Introductory programming, problem solving and computer assisted assessment,","authors":[],"date":"2002","doi":null,"raw":"Daly, C. & Waldron, J. (2002) Introductory programming, problem solving and computer assisted assessment,  Proceedings of the 6th International Computer Assisted Assessment Conference, (Loughborough, Loughborough University), 95\u2013106.","cites":null},{"id":1879495,"title":"Is the test constructor a facet?","authors":[],"date":"2003","doi":"10.1191\/0265532203lt244oa","raw":"Jafarpur, A. (2003) Is the test constructor a facet? Language Testing, 20(1), 57\u201387.","cites":null},{"id":1879485,"title":"It\u2019s pretty difficult to fail; the reluctance of lecturers to award a fail grade, Assessment and Evaluation","authors":[],"date":"2003","doi":"10.1080\/0260293032000066209","raw":"Hawe, E. (2003) It\u2019s pretty difficult to fail; the reluctance of lecturers to award a fail grade, Assessment and Evaluation in Higher Education, 28(4), 371\u2013382.","cites":null},{"id":1879479,"title":"Keeping the wolves from the door, wolves in sheep clothing, that is,","authors":[],"date":"2000","doi":null,"raw":"Frohlich, R. (2000) Keeping the wolves from the door, wolves in sheep clothing, that is, Proceedings of the 4th International Computer Assisted Assessment Conference  (Loughborough, Loughborough University).","cites":null},{"id":1879545,"title":"Linking on-line assessment in mathematics to cognitive skills,","authors":[],"date":"2002","doi":null,"raw":"Paterson, J. S. (2002) Linking on-line assessment in mathematics to cognitive skills, Proceedings of the 6th International Computer Assisted Assessment Conference (Loughborough, Loughborough University), 295\u2013306.","cites":null},{"id":1879538,"title":"MCQ: an interactive computer program for multiple-choice self testing,","authors":[],"date":"1979","doi":"10.1016\/0307-4412(79)90049-9","raw":"Morgan, M. R. J. (1979) MCQ: an interactive computer program for multiple-choice self testing, Biochemical Education, 7(3), 67\u201369.","cites":null},{"id":1879472,"title":"Missing paper sparks exam reprint","authors":[],"date":"2003","doi":null,"raw":"Curtis, P. (2003) Missing paper sparks exam reprint (London, Guardian).","cites":null},{"id":1879523,"title":"On-line formative assessment item banking and learning support,","authors":[],"date":"2001","doi":null,"raw":"Maughan, S., Peet, D. & Willmot, A. (2001) On-line formative assessment item banking and learning support, Proceedings of the 5th International Computer Assisted Assessment Conference (Loughborough, Loughborough University).","cites":null},{"id":1879540,"title":"On-line multimedia assessment for K-4 students,","authors":[],"date":"2003","doi":null,"raw":"Nugent, G. (2003) On-line multimedia assessment for K-4 students,  Proceedings of the World Conference on Educational Multimedia, Hypermedia and Telecommunications (Hawaii, AACE), 1051\u20131057.","cites":null},{"id":1879487,"title":"Online marking of essay-type assignments,","authors":[],"date":"2003","doi":null,"raw":"Heinrich, E. & Wang, Y. (2003) Online marking of essay-type assignments, Proceedings of the World Conference on Educational Multimedia Hypermedia and Telecommunications (Hawaii, AACE), 768\u2013772.","cites":null},{"id":1879468,"title":"Paper-based versus computer-based assessment: key factors associated with test mode effect,","authors":[],"date":"2002","doi":"10.1111\/1467-8535.00294","raw":"Clariana, R. & Wallace, P. (2002) Paper-based versus computer-based assessment: key factors associated with test mode effect, British Journal of Educational Technology, 33(5), 593\u2013602.","cites":null},{"id":1879452,"title":"Pilot summative web assessment in secondary education,","authors":[],"date":"2003","doi":null,"raw":"Ashton, H. S., Schofield, D. K. & Woodger, S. C. (2003) Pilot summative web assessment in secondary education, Proceedings of the 7th International Computer Assisted Assessment Conference (Loughborough, Loughborough University), 19\u201329.","cites":null},{"id":1879529,"title":"Principles of assessment","authors":[],"date":"2002","doi":null,"raw":"McAlpine, M. (2002) Principles of assessment (Luton, CAA Centre).","cites":null},{"id":1879532,"title":"Quality assurance of computer-assisted assessment: practical and strategic issues,","authors":[],"date":"2000","doi":"10.1108\/09684880010312659","raw":"McKenna, C. & Bull, J. (2000) Quality assurance of computer-assisted assessment: practical and strategic issues, Quality Assurance in Education, 8(1), 24\u201331.","cites":null},{"id":1879461,"title":"Quantifying the effects of chance in multiple choice and true\/false tests: question selection and guessing of answers,","authors":[],"date":"2001","doi":"10.1080\/02602930020022273","raw":"Burton, R. F. (2001) Quantifying the effects of chance in multiple choice and true\/false tests: question selection and guessing of answers, Assessment and Evaluation in Higher Education, 26(1), 41\u201350.","cites":null},{"id":1879506,"title":"Question and test interoperability: an update on national and international developments,","authors":[],"date":"2001","doi":null,"raw":"Lay, S. & Sclater, N. (2001) Question and test interoperability: an update on national and international developments, Proceedings of the 5th International Computer Assisted Assessment Conference (Loughborough, Loughborough University).","cites":null},{"id":1879514,"title":"Recent developments in the triartite interactive assessment delivery system (TRIADS),","authors":[],"date":"1999","doi":null,"raw":"Mackenzie, D. (1999) Recent developments in the triartite interactive assessment delivery system (TRIADS),  Proceedings of the 3rd Annual Computer Assisted Assessment Conference (Loughborough, Loughborough University).","cites":null},{"id":1879576,"title":"Remote electronic examinations: student experience,","authors":[],"date":"2002","doi":"10.1111\/1467-8535.00290","raw":"Thomas, P., Price, B., Paine, C. & Richards, M. (2002) Remote electronic examinations: student experience, British Journal of Educational Technology, 33(5), 537\u2013549.","cites":null},{"id":1879569,"title":"Special Educational Needs and Disability Act 2001, http:\/\/www.hmso.gov.uk\/ acts\/acts2001\/20010010.htm (Accessed 10","authors":[],"date":"2001","doi":null,"raw":"SENDA (2001), Special Educational Needs and Disability Act 2001,  http:\/\/www.hmso.gov.uk\/ acts\/acts2001\/20010010.htm (Accessed 10 March 2004) Sim, G., Malik, N. A. & Holifield, P. (2003) Strategies for large-scale assessment: an institutional analysis of research and practice in a virtual university, Proceedings of the 7th International Computer Assisted Assessment Conference (Loughborough, Loughborough University), 379\u2013390.","cites":null},{"id":1879462,"title":"Statistical modelling of multiple-choice and true\/false tests: ways of considering, and of reducing, the uncertainties attributed to guessing, Assessment and Evaluation","authors":[],"date":"1999","doi":"10.1080\/0260293990240404","raw":"Burton, R. F. & Miller, D. J. (1999) Statistical modelling of multiple-choice and true\/false tests: ways of considering, and of reducing, the uncertainties attributed to guessing, Assessment and Evaluation in Higher Education, 24(4), 399\u2013411.","cites":null},{"id":1879490,"title":"Students in higher education institutions 1994\/95","authors":[],"date":"1995","doi":null,"raw":"HESA (1995) Students in higher education institutions 1994\/95 (London, HMSO).","cites":null},{"id":1879491,"title":"Students in higher education institutions 2001\/02","authors":[],"date":"2002","doi":null,"raw":"HESA (2002) Students in higher education institutions 2001\/02 (London, HMSO).","cites":null},{"id":1879560,"title":"Students\u2019 perception of the learning benefits of computer-assisted assessment: a case study in electronic engineering, in:","authors":[],"date":"1999","doi":null,"raw":"Sambell, K., Sambell, A. & Sexton, G. (1999) Students\u2019 perception of the learning benefits of computer-assisted assessment: a case study in electronic engineering, in: S. Brown, J. Bull & P. Race (Eds)  Computer-assisted assessment in higher education  (Birmingham, SEDA), 179\u2013191.","cites":null},{"id":1879552,"title":"Stumping e-rator: challenging the validity of automated essay scoring,","authors":[],"date":"2002","doi":"10.1016\/s0747-5632(01)00052-8","raw":"Powers, D. E., Burstein, J. C., Chodorow, M., Fowles, M. E. & Kukich, K. (2002) Stumping e-rator: challenging the validity of automated essay scoring, Computers in Human Behaviour, 18(2), 103\u2013134.","cites":null},{"id":1879467,"title":"Summary of question styles. Available online at: http:\/\/www.derby.ac.uk\/ciad\/ ciastyles.html (accessed 30","authors":[],"date":"2003","doi":null,"raw":"CIAD (2003) Summary of question styles. Available online at: http:\/\/www.derby.ac.uk\/ciad\/ ciastyles.html (accessed 30 June 2003).","cites":null},{"id":1879456,"title":"Taxonomy of educational objectives: the classification of educational goals. Handbook 1. Cognitive domain (New York, Longman).Implementation of computer assisted assessment 225","authors":[],"date":"1956","doi":"10.1177\/001316445601600310","raw":"Bloom, B. S. (1956) Taxonomy of educational objectives: the classification of educational goals. Handbook 1. Cognitive domain (New York, Longman).Implementation of computer assisted assessment 225 Bloom, B. S., Hastings, J. T. & Madaus, G. F. (1971) Handbook on formative and summative evaluation of student learning (New York, McGraw-Hill Books).","cites":null},{"id":1879585,"title":"Technical and security issues, in:","authors":[],"date":"1999","doi":null,"raw":"Whittington, D. (1999) Technical and security issues, in: S. Brown, J. Bull & P. Race (Eds) Computer assisted assessment in higher education (Birmingham, SEDA), 21\u201328.","cites":null},{"id":1879563,"title":"Technologies for online interoperable assessment,","authors":[],"date":"2003","doi":null,"raw":"Sclater, N., Davis, H. C., White, S. A., Conole, G. C. & Danson, M. (2003) Technologies for online interoperable assessment, Proceedings of the CAL03 (Belfast, Elsevier).","cites":null},{"id":1879592,"title":"Test construction (Columbus, Charles E.","authors":[],"date":"1960","doi":"10.1177\/001316446102100125","raw":"Wood, D. A. (1960) Test construction (Columbus, Charles E. Merrill Books).","cites":null},{"id":1879553,"title":"The art of assessing,","authors":[],"date":"1995","doi":null,"raw":"Race, P. (1995) The art of assessing, The New Academic, 4(3).","cites":null},{"id":1879509,"title":"The introduction of computer based testing on an engineering technology course, Assessment and Evaluation","authors":[],"date":"1996","doi":"10.1080\/0260293960210107","raw":"Lloyd, D., Martin, J. G. & McCaffery, K. (1996) The introduction of computer based testing on an engineering technology course, Assessment and Evaluation in Higher Education, 21(1), 83\u201390.Implementation of computer assisted assessment 227 Loewenberger, P. & Bull, J. (2003) Cost-effectiveness analysis of computer-based assessment, ALT-J, 11(2), 23\u201345.","cites":null},{"id":1879477,"title":"The legibility of screen formats: are three columns better than one?","authors":[],"date":"1997","doi":"10.1016\/s0097-8493(97)00048-4","raw":"Dyson, M. C. & Kipping, G. J. (1997) The legibility of screen formats: are three columns better than one? Computers and Graphics, 21(6), 703\u2013712.","cites":null},{"id":1879469,"title":"The use of formative quizzez for deep learning,","authors":[],"date":"1998","doi":null,"raw":"Cox, K. & Clark, D. (1998) The use of formative quizzez for deep learning, Computers and Education, 30(3), 157\u2013167.","cites":null},{"id":1879484,"title":"The use of PGP to provide secure email delivery of CAA results,","authors":[],"date":"2002","doi":null,"raw":"Hatton, S., Boyle, A., Byrne, S. & Wooff, C. (2002) The use of PGP to provide secure email delivery of CAA results,  Proceedings of the 6th International Computer Assisted Assessment Conference (Loughborough, Loughborough University), 149\u2013160.","cites":null},{"id":1879574,"title":"Theoretical foundations of multimedia","authors":[],"date":"1999","doi":"10.1145\/347634.347643","raw":"Tannenbaum, R. S. (1999) Theoretical foundations of multimedia (New York, W.H. Freeman).","cites":null},{"id":1879475,"title":"There\u2019s no confidence in multiple-choice testing,","authors":[],"date":"2002","doi":null,"raw":"Davies, P. (2002) There\u2019s no confidence in multiple-choice testing,  Proceedings of the 6th International Computer Assisted Assessment Conference  (Loughborough, Loughborough University), 119\u2013132.","cites":null},{"id":1879559,"title":"Towards principled practice in evaluation: learning from instructors\u2019 dilemmas in evaluating graduate students,","authors":[],"date":"2002","doi":"10.1016\/s0191-491x(02)00042-1","raw":"Sabar, N. (2002) Towards principled practice in evaluation: learning from instructors\u2019 dilemmas in evaluating graduate students, Studies in Educational Evaluation, 28(4), 329\u2013345.","cites":null},{"id":1879516,"title":"TRIADS experiences and developments. A panel discussion,","authors":[],"date":"2002","doi":null,"raw":"Mackenzie, D., Hallam, B., Baggott, G. & Potts, J. (2002) TRIADS experiences and developments. A panel discussion,  Proceedings of the 6th International Computer Assisted Assessment Conference (Loughborough, Loughborough University).","cites":null},{"id":1879565,"title":"User requirements of the \u2018ultimate\u2019 online assessment engine,","authors":[],"date":"2003","doi":"10.1016\/s0360-1315(02)00132-x","raw":"Sclater, N. & Howie, K. (2003) User requirements of the \u2018ultimate\u2019 online assessment engine, Computers and Education, 40(3), 285\u2013306.","cites":null},{"id":1879499,"title":"Using computer aided assessment to test higher level learning outcomes,","authors":[],"date":"2002","doi":null,"raw":"King, T. & Duke-Williams, E. (2002) Using computer aided assessment to test higher level learning outcomes,  Proceedings of the 5th International Computer Assisted Assessment Conference (Loughborough, Loughborough University).","cites":null},{"id":1879571,"title":"Using computer-assisted assessment: time saver or sophisticated distractor?","authors":[],"date":"1994","doi":null,"raw":"Stephens, D. (1994) Using computer-assisted assessment: time saver or sophisticated distractor? Active Learning, 1, 11\u201315.","cites":null},{"id":1879464,"title":"Using computer-based tests,","authors":[],"date":"1997","doi":"10.1080\/0968776970050105","raw":"Callear, D. & King, T. (1997) Using computer-based tests, ALT-J, 5(1), 27\u201332.","cites":null},{"id":1879455,"title":"Using multimedia in large-scale computer-based testing programs,","authors":[],"date":"1999","doi":"10.1016\/s0747-5632(99)00024-2","raw":"Bennett, R. E., Goodman, M., Hessinger, J., Kahn, H., Liggett, J., Marshall, G. & Zack, J. (1999) Using multimedia in large-scale computer-based testing programs,  Computers in Human Behaviour, 15(3), 283\u2013294.","cites":null},{"id":1879542,"title":"Wading through treacle: CAA at the University of Bristol,","authors":[],"date":"2001","doi":null,"raw":"O\u2019Leary, R. & Cook, J. (2001) Wading through treacle: CAA at the University of Bristol, Proceedings of the 5th International Computer Assisted Assessment Conference (Loughborough, Loughborough University).","cites":null},{"id":1879544,"title":"WebCT and online assessment: the best thing since SOAP?","authors":[],"date":"2003","doi":null,"raw":"Pain, D. & Le Heron, J. (2003) WebCT and online assessment: the best thing since SOAP? Educational Technology and Society, 6(2), 62\u201371.228 G. Sim et al.","cites":null},{"id":1879557,"title":"What factors affect students opinions of computer-assisted assessment?","authors":[],"date":"2002","doi":null,"raw":"Ricketts, C. & Wilks, S. (2002a) What factors affect students opinions of computer-assisted assessment?  Proceedings of the 6th International Computer Assisted Assessment Conference (Loughborough, Loughborough University), 307\u2013316.","cites":null},{"id":1879481,"title":"Writing test items to evaluate higher order thinking (Needham Heights, Allyn &","authors":[],"date":"1996","doi":null,"raw":"Haladyna, T. M. (1996) Writing test items to evaluate higher order thinking (Needham Heights, Allyn & Bacon).","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2004","abstract":"This paper draws attention to literature surrounding the subject of computer-assisted assessment (CAA). A brief overview of traditional methods of assessment is presented, highlighting areas of concern in existing techniques. CAA is then defined, and instances of its introduction in various educational spheres are identified, with the main focus of the paper concerning the implementation of CAA. Through referenced articles, evidence is offered to inform practitioners, and direct further research into CAA from a technological and pedagogical perspective. This includes issues relating to interoperability of questions, security, test construction and testing higher cognitive skills. The paper concludes by suggesting that an institutional strategy for CAA coupled with staff development in test construction for a CAA environment can increase the chances of successful implementation","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/14177.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/608\/1\/ALT_J_Vol12_No3_2004_Implementation%20of%20computer%20ass.pdf","pdfHashValue":"f217804736f7a442415e46a2634d632cca8d6a47","publisher":"Taylor and Francis Ltd","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:608<\/identifier><datestamp>\n      2011-04-04T09:05:37Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/608\/<\/dc:relation><dc:title>\n        Implementation of computer assisted assessment: lessons from the literature<\/dc:title><dc:creator>\n        Sim, Gavin<\/dc:creator><dc:creator>\n        Holifield, Phil<\/dc:creator><dc:creator>\n        Brown, Martin<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        This paper draws attention to literature surrounding the subject of computer-assisted assessment (CAA). A brief overview of traditional methods of assessment is presented, highlighting areas of concern in existing techniques. CAA is then defined, and instances of its introduction in various educational spheres are identified, with the main focus of the paper concerning the implementation of CAA. Through referenced articles, evidence is offered to inform practitioners, and direct further research into CAA from a technological and pedagogical perspective. This includes issues relating to interoperability of questions, security, test construction and testing higher cognitive skills. The paper concludes by suggesting that an institutional strategy for CAA coupled with staff development in test construction for a CAA environment can increase the chances of successful implementation.<\/dc:description><dc:publisher>\n        Taylor and Francis Ltd<\/dc:publisher><dc:date>\n        2004<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/608\/1\/ALT_J_Vol12_No3_2004_Implementation%20of%20computer%20ass.pdf<\/dc:identifier><dc:identifier>\n          Sim, Gavin and Holifield, Phil and Brown, Martin  (2004) Implementation of computer assisted assessment: lessons from the literature.  Association for Learning Technology Journal, 12 (3).  pp. 215-229.  ISSN 0968-7769 (print)\/1741-1629 (online)     <\/dc:identifier><dc:relation>\n        10.1080\/0968776042000259546<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/608\/","10.1080\/0968776042000259546"],"year":2004,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"ALT-J, Research in Learning Technology\nVol. 12, No. 3, September 2004\nISSN 0968\u20137769 (print)\/ISSN 1741\u20131629 (online)\/04\/030215\u201315\n\u00a9 2004 Association for Learning Technology\nDOI: 10.1080\/0968776042000259546\nImplementation of computer assisted \nassessment: lessons from the literature\nGavin Sim*, Phil Holifield & Martin Brown\nUniversity of Central Lancashire, UK\nTaylor and Francis LtdGALT120303.sgm10.1080\/ 968776042000259546ALT-J, Research in Learning Technology0968 7769 (pri t)\/1741-1629 (onli e)Original Article2 04ssoci tion for Learning Techno ogy23 000Sept mber 20 4vinSimDepartm n of Computing, Computing and Technology BuildingUniversity of Central LancashirePrestonPR1 2HEgrsim@uclan.ac.uk\nThis paper draws attention to literature surrounding the subject of computer-assisted assessment\n(CAA). A brief overview of traditional methods of assessment is presented, highlighting areas of\nconcern in existing techniques. CAA is then defined, and instances of its introduction in various\neducational spheres are identified, with the main focus of the paper concerning the implementa-\ntion of CAA. Through referenced articles, evidence is offered to inform practitioners, and direct\nfurther research into CAA from a technological and pedagogical perspective. This includes issues\nrelating to interoperability of questions, security, test construction and testing higher cognitive\nskills. The paper concludes by suggesting that an institutional strategy for CAA coupled with staff\ndevelopment in test construction for a CAA environment can increase the chances of successful\nimplementation.\nIntroduction\nThis paper presents evidence that the more traditional methods of assessment within\nuniversities have their limitations. As a result of these limitations and also the\ncontinued increase in the use of technology to deliver curriculum, the gap between\nassessment methods and learning is widening.\nStudents entering higher education directly from schools and colleges are likely to\nhave been exposed to Information Technology as part of the UK National Curricu-\nlum. Pilot studies conducted within schools for the delivery of summative assessment\nvia the web (Ashton et al., 2003; Nugent, 2003) and for basic key skills tests in both\nLearn Direct and army centres (Sealey et al., 2003) indicate that CAA can success-\nfully assess students and provide timely feedback regarding class and individual\nprogress. There is also empirical evidence to suggest students find CAA an acceptable\nassessment technique (Sambell et al., 1999; Croft et al., 2001; Ricketts & Wilks,\n2002a). Therefore it could be argued that for many students CAA may become a\n*Corresponding author: Department of Computing, Computing and Technology Building,\nUniversity of Central Lancashire, Preston PR1 2HE, UK. Email: grsim@uclan.ac.uk\n216 G. Sim et al.\nmore widely used method of assessment in schools, Further education and Universi-\nties. Many universities are now using technology in their assessment strategies\n(Stephens & Mascia, 1997) and by examining the literature, lessons can be learned\nto facilitate the successful implementation of computer-assisted assessment.\nMethodology\nA comprehensive literature review was conducted primarily using online resources,\nalthough library resources were also used. The searching centred on the databases\nIngenta, AACE, Science Direct and Conference Proceedings such as the International\nComputer Assisted Assessment Conference. Key word searching was problematic and\ntime consuming, for example conducting a search using \u2018computer assessment\u2019 would\nproduces a divergent array of articles in excess of one thousand. Other terminology\nused in the search included \u2018computer based testing, computer-based assessment,\ncomputer aided assessment and e-assessment\u2019. Browsing through the contents of entire\njournals such as \u2018Assessment and Evaluation in Higher education\u2019 was also adopted.\nAssessment in general\nAcademic assessment can be administered through various techniques. Fifty varied\ntechniques have been identified and used within higher education for assessment\npurposes (Knight, 2001); the most commonly used are exams and essays (Graham,\n2004). However this does not include all the methods now available within CAA\npackages for example incorporate questions that make use of multimedia. New\nassessment techniques will continue to emerge as technology and teaching methods\nchange and develop, therefore continuing research will be required to determine the\neffectiveness and appropriateness of these methods.\nEach form of assessment presents its own difficulties, whether computer based or\ntraditional. Essays present the problem of double marking, in one study both markers\nagreed only 52% of the time (Powers et al., 2002). Additionally there are the prob-\nlems with cheating as Internet sites offer custom-written and off the shelf essays\n(Crisp, 2002). It has been suggested that exams tend to encourage surface learning\n(Race, 1995) and may cause increased anxiety resulting in significantly lower scores\n(Cassady & Johnson, 2002). The multiple choice question (MCQ) styles are used in\nboth offline and CAA exams and raise a number of concerns, for example, grade\ndeflation by not enabling partial credit (Baranchik & Cherkas, 2000), poorly designed\nquestions (Paxton, 2000; Jafarpur, 2003) and guessing (Burton, 2001). However the\nadvantages of using computers to deliver MCQ for lecturers include automated\nmarking (Pollock et al., 2000) and for formative purposes the students have the\nopportunity to study at their own pace, repeat questions and receive instant feedback\n(Loewenberger & Bull, 2003). It is the potential advantages of CAA that has driven\nresearch into ways to overcome the difficulties.\nUltimately in an academic environment the marks from summative assessment are\naccumulated to award an overall grade and there are concerns over comparability across\nImplementation of computer assisted assessment 217\nsubject domains. It has been suggested that the scientific subjects produce more First\nClass Degrees than the humanities because of the nature of the marking criteria in\nusing the full range of marks and subjectivity is eliminated from the equation where\nthere is a predefined correct answer (Yorke et al., 2002; Horney, 2003). These findings\nwould appear to be further corroborated by the Higher education Statistics Agency\n(HESA) figures. Of the students graduating from UK universities in 2001\/02 in Math-\nematical Science 25.5% passed with a First Class Degree, compared to 10.4% in\nHumanities (HESA, 2002) and this trend was also evident in other years for example,\n1994\/95 (HESA, 1995). CAA, like mathematics and some science subjects, also tends\nto use the full range of marks therefore the trend towards a high proportion of First\nClass Degrees may occur in other subject domains adopting this technique in the future.\nThere is pressure on lecturers not to fail students, and one study found that in\nprofessional subjects there is a tendency to leave the award of a fail to the next assessor\n(Hawe, 2003). Lecturers are confronted with emotional and ethical dilemmas when\na close working relationship is formed, increasing their reluctance to award a fail\n(Sabar, 2002). The emotional and subjectivity issues that are evident in human\ncentred marking may be removed via automatic marking offered by CAA software.\nIt is important to recognize that some of these issues discussed are still prevalent in\nCAA along with new challenges. Adopting a diverse assessment strategy may lead to\na fairer assessment of the student (Race, 1995).\nComputer-assisted assessment defined\nFrom the literature there is a lack of universal consent regarding the terminology and\nits definition, however, Bull and McKenna (2001) argue that computer-assisted\nassessment is the common term for the use of computers in the assessment of\nstudents and the other terminology tend to focus on the activities. Therefore the\ndefinition of CAA used in this review will be that: CAA encompasses the use of\ncomputers to deliver, mark or analyse assignments or exams.\nVariations in CAA\nWithin higher education institutions the application of CAA has occurred in a\nnumber of varied ways, these include, adaptive testing (Latu & Chapman, 2002; Mills\net al., 2002), analysis of the content of discussion boards (Macdonald & Twining,\n2002; Wiltfelt et al., 2002), automated essay marking (Christie, 1999; Burstein et al.,\n2001), delivery of exam papers (Sim et al., 2003) and objective testing (Walker &\nThompson, 2001; Pain & Le Heron, 2003). These methods vary considerably\nhowever the focus of this review of research will centre on the issues relating to\nimplementing objective tests via CAA.\nTesting cognitive skills with CAA\nThere is concern in the literature relating to CAA and its ability to test higher cognitive\nskills across subject domains (Daly & Waldron, 2002; Paterson, 2002). The higher\n218 G. Sim et al.\ncognitive skills are often associated with \u2018Analysis, Synthesis, and Evaluation\u2019 as defined\nin Bloom\u2019s Taxonomy (Bloom, 1956). However, a revised taxonomy takes into consid-\neration the \u2018Knowledge Dimension\u2019 (Anderson & Krathwohl, 2001) and this has also\nbeen used in CAA research for classification of questions (King & Duke-Williams,\n2002; Mayer, 2002).\nPaterson (2002) indicated that it is not feasible to test the higher-level cognitive\nskills using CAA within mathematics. Bloom states that in the majority of instances\nSynthesis and Evaluation promote divergent thinking and answers cannot be\ndetermined in advance (Bloom et al., 1971). Heinrich and Wang (2003) argue that\nobjective testing is still not sophisticated enough to examine complex content and\nthinking patterns. However, other research in linguistics and computer programming\nconcluded that the higher-level skills can be assessed via CAA through innovative\napproaches (Cox & Clark, 1998; Reid, 2002). In the study by Reid (2002) a new\nlanguage was devised and students were required to apply linguistic techniques in\norder to answer MCQ. It has been suggested that CAA tests of higher-level skills are\nmore complex and costly to produce (Dowsing, 1998) and this may be because more\ninnovative approaches are needed.\nQuestion styles\nObjective testing has been used within assessment for over forty years (Wood, 1960)\nand computer programs delivering MCQ date back to the 1970s (Morgan, 1979).\nMore sophisticated question styles have emerged enabling more diverse assessment\nmethods. The question styles delivered by the TRIADS software developed at Derby\nUniversity are evidence of this evolution, offering 17 question styles in 1999\n(Mackenzie, 1999) and 39 in 2003 (CIAD, 2003). However, staff at the University\nof Liverpool using TRIADS found that this presented an additional problem, as they\nwere unfamiliar with the new question styles and lacked confidence in writing suitable\nquestions (McLaughlin et al., 2004). Staff development in writing suitable questions\nand guidelines can be used to overcome these problems. For example, generic\nguidelines developed by Haladyna (1996), Herd and Clark (2002) present examples\nof the various questions styles used in further education whilst examples used within\nhigher education can be found at http:\/\/www.caacentre.ac.uk.\nAlthough there are a large number of possible formats for CAA questions, it is\npossible to classify them into four distinct groups based on the human interaction\ntechnique required (CIAD, 2003). These groups are defined as point and click, move\nobject, text entry and draw object.\nPoint and click\nPoint and click questions include Multiple Choice (MCQ) and Multiple Response\n(MRQ) items, which have both been used within assessment practise for a consider-\nable time and as a result are often transformed into CAA (Ricketts & Wilks, 2002b).\nEbel (1972) suggests that any understanding or ability that can be tested by means of\nImplementation of computer assisted assessment 219\nany other technique, for instance essays, can also be tested by MCQ. More complex\nMCQ questions can be devised through assertion reasoning resulting in the testing of\nhigher cognitive skills (Bull & McKenna, 2001). Both MCQ and MRQ have inherent\nproblems, such as reliance on true and false style questions which students might\nperceive to be unfair (Wood, 1960). Davies also argues that the quality of MCQ is\ndependent on the quality of the distracter and not the question (Davies, 2002).\nMove object\nMove object style questions focus on the movement of objects to predetermined\npositions on the screen. They are a variation of the MCQ format and are good for\nassessing students understanding of relationships (Bull & McKenna, 2001). For\nexample in computing they could be used for the labelling of entity relationship\ndiagrams or in linguistics students could be presented with a poem and move the\nhighlighted words to the appropriate word class. One problem is that when the\nnumber of moveable objects is equal to the number of targets, if a student knows all\nbut one answer they will automatically get full marks (Wood, 1960).\nText entry\nText entry questions consist of input of short predefined answers, such as factual\nknowledge or syntax in computer programming. An advantage of this format is that\nstudents must supply the correct answer removing the possibility of guessing (Bull &\nMcKenna, 2001) and this style has been found to be the most demanding format for\nstudents (Reid, 2002). There are problems associated with text entry within some\nsubject domains such as mathematics, as mathematical expressions cannot easily be\nincluded in most commercial software (Croft et al., 2001; Paterson, 2002). Another\nproblem associated with this question style is that the answer may be marked\nincorrect due to spelling mistakes and the time saving element may be reduced if\nlecturers need to manually check for spelling errors.\nDraw object\nThis is associated with drawing simple objects or lines. For example, students may be\nrequired to plot graphs which can be automatically marked. This style of question is\na high discriminator between strong and weak candidates (Mackenzie, 1999). There\nis little evidence in the literature concerning the effectiveness of this format, but this\nmight be due to the fact that commercial software such as Questionmark and I-Assess\ndo not have this style in their templates.\nInteroperability and question banks\nQuestion banks which are authored and peer reviewed by academics are emerging,\nsuch as the Electrical and Electronic Engineering Assessment Network who\n220 G. Sim et al.\ndeveloped a database of questions in electrical and electronic engineering (Bull et al.,\n2002). One such bank will typically require 5000 questions making it unfeasible for a\nsingle institution to develop (Maughan et al., 2001). Constructing high quality\nquestions is difficult, time consuming and expensive (Sclater et al., 2003) and issues\narise in the interoperability of questions between CAA Software (Lay & Sclater,\n2001). There are several international standards established to enable interoperability\nof questions between software applications (Herd & Clark, 2003). These specifica-\ntions are based on metadata structure for questions and their grouping together.\nUnless these interoperability standards are developed and utilized question banks will\nhave a limited life, as they cannot be used on a variety of delivery platforms (White &\nDavis, 2000). Systems are emerging that are IMS-QTI compliant (Instructional\nManagement Systems \u2013 Question and Test Interoperability Specification) to facilitate\nthe exchange of questions (Daly, 2002; Bacon, 2003). The Centre for Educational\nTechnology Interoperability Standards (www.cetis.ac.uk) offers comprehensive\nresources and information on the issues concerning interoperability which may help\ndirect further research.\nGuessing\nA number of the question styles associated with CAA can lead to artificially high\nmarks through guessing (Bush, 1999), which has implications for setting the pass\nmark of the test. For example, setting a pass mark of 40% based on assessment of\ntrue\/false answers would be inappropriate, as guessing alone would give an average of\n50% (Harper, 2002). The problems of guessing may be addressed through various\nmarking schemes, such as post test correction (Bull & McKenna, 2001), negative\nmarking (Bush, 1999), increasing the number of questions or combining the results\nfrom several tests (Burton & Miller, 1999) or increasing the number of distracters and\nthe pass mark (Mackenzie & O\u2019Hare, 2002). It has been suggested that negative\nmarking is not generally implemented in the UK (McAlpine, 2002) and that post test\ncorrection is only suitable with a single question style because the formulae would\nvary depending on the number of distracters (Harper, 2003).\nStatistical analysis has resulted in various methods being developed to assist in test\nconstruction in order to reduce the effects of guessing. An empirical marking\nsimulator to assist in scoring and test construction based on a base level guess factor\nhas been developed (Mackenzie & O\u2019Hare, 2002), this program examines the mark\ndistribution and measurement scale for a set of random answers, enabling tutors to\nestablish the effects of guessing on their assessment. Also statistics to award a score\nfor partial credit through a formula based on a mean uneducated guessers score has\nbeen investigated (McCabe & Barrett, 2003). This allows MCQ to be unconstrained,\nsimilar to MRQ styles, enabling students to provide more than one answer and their\nscore is weighted depending on the number of choices. For example, an MCQ with\none correct answer, four possible options and a score of 3, if a student includes the\ncorrect answer by selecting 2 options they would only score 2 (2=3-1). Davies used a\ncombination of predetermining the students\u2019 confidence in answering the question\nImplementation of computer assisted assessment 221\nprior to seeing the distracters and negative marking, resulting in students perceiving\nthis to be a fairer test of their abilities (Davies, 2002).\nThere is lack of evidence that any one specific technique generates more accurate\nresults than any other. It could be argued that these techniques are unnecessary if the\ntests are well constructed (Bull & McKenna, 2001).\nAccessibility\nUK institutions now have to comply with the Special Educational Needs and\nDisability Act when preparing both teaching and assessment material (SENDA,\n2001). The number of students in UK higher education registering a disability in\n2000 was 22,290 and this has implications for CAA (Phipps & McCarthy, 2001). For\nexample, a student with dyslexia may exert more cognitive resources in interpreting\nthe question, therefore, ensuring the language is appropriate is a necessity (Wiles &\nBall, 2003). In addition extra time may be required to complete the test which may\nnecessitate the publishing of two different assessments, one with a longer duration.\nFeedback from one dyslexic student regarding CAA indicated that they thought it\nprovided a more level playing field in which they can demonstrate their knowledge\n(Jefferies et al., 2000). Students with visual or physical impairment may struggle to\nanswer move object and draw object style questions without the aid of assistive\ntechnology, they may need specially adapted input software and hardware such as,\ntouch screens, eyegaze systems or speech browsers.\nThere are guidelines for general teaching, however there is little evidence that\nguidelines for inclusive and accessible design in CAA are emerging (Wiles, 2002). For\nexample, when multimedia elements, such as video are used within the assessment,\nit may necessitate the provision of an alternative paper-based version for students\nwith sensory impairment. The introduction of an alternative, in this instance paper,\nposes the problem of ensuring comparability (Bennett et al., 1999). When identical\ntests are presented on a computer and paper they are not comparable (Clariana &\nWallace, 2002) because there are numerous variables that impact on student\u2019s\nperformance when questions are presented on a computer. These variables include\nthe monitor (Schenkman et al., 1999), the way text is displayed on screen (Dyson and\nKipping, 1997), reading from a monitor is slower than paper (Mayes et al., 2001) and\nthe problems of obtaining a feel for the exam when only a single question is presented\n(Liu et al., 2001). The Web Accessibility Initiative (http:\/\/www.w3c.org\/WAI\/) has\nproduced useful guidelines for promoting online accessibility which may be\napplicable to CAA but this initiative does not address the issue of comparability\nbetween questions.\nInstitutional strategies for the adoption of CAA\nThe greatest barrier to the adoption of CAA by academics is lack of time, to both\ndevelop questions and learn the software (Warburton & Conole, 2003). This may\nhave contributed to the fact that the adoption of CAA has usually resulted from the\n222 G. Sim et al.\nimpetus of enthusiastic individuals rather than strategic decisions (O\u2019Leary & Cook,\n2001; Daly & Waldron, 2002). The perceived benefits of CAA of freeing lecturers\u2019\ntime can be illusive if no institutional strategy or support is offered (Stephens, 1994),\nsuccessful implementation may be left to chance (Stephens et al., 1998) and CAA\nmay be developed in an anarchic fashion (McKenna & Bull, 2000). Research\nconducted at the University of Portsmouth indicate that there is no time saving\nbenefit for courses with less than twenty students (Callear & King, 1997). In order to\nutilize the features within software packages staff training and development is\nnecessary (Boyle & O\u2019Hare, 2003) and this may not be feasible without institutional\nsupport.\nInstitutions adopting CAA are faced with the difficulty of evaluating and deciding\nupon the most appropriate CAA software. Without an institutional strategy,\nindividual departments may adopt their own systems (O\u2019Leary & Cook, 2001). This\nresults in students having to cope with a number of different user interfaces and CAA\nformats, increased licence costs and problems offering administrative and technical\nsupport. Even if an institution has a clear strategy there are also problems in\ndetermining the selection criteria for software used to deliver assessment and there is\na lack of analysis within the literature (Valenti et al., 2002). Sclater and Howie (2003)\ncontributed to this literature by defining the ultimate online assessment engine. This\nwas achieved through a process of examining the user requirements of the system,\nestablishing the stakeholders and their functional requirements. This research may\naid institutions identify their needs and establish an appropriate evaluation\nmethodology.\nThe following guidelines for an institutional strategy have been formulated by\nLoughborough University and the University of Luton: establish a coordinated\nCAA management policy for CAA unit(s) and each discipline on campus; establish\na CAA unit; establish CAA discipline groups\/committees; provide funding; organize\nstaff development programmes; establish evaluation procedures; identify technical\nissues; establish operational and administrative procedures (Stephens et al., 1998).\nBS7988 is a new British Standard Code of practice that has been introduced\ngoverning the use of information technology in the delivery of assessments\n(BS7988, 2002). The guidelines have various implications for the delivery of assess-\nments, for example, it is recommended that students take a break after 1.5 hours\nwhich has an impact on the invigilation process. If this recommendation is followed,\nprocedures need to be established to prevent collusion between students during the\nbreak or the tests need to be split into two separate sections. One of the difficulties\nfor many institutions using CAA arises through the lack of resources to accommo-\ndate large cohorts of students sitting the exam simultaneously (Mackenzie et al.,\n2004). This problem can be alleviated through institutional support and therefore,\nto fully utilize the benefits of CAA an institutional strategy would appear necessary\nto increase the chance of successful implementation. These benefits are evident\nwithin a number of institutions with strategies, such as, Ulster (Stevenson et al.,\n2002), Derby (Mackenzie et al., 2002), Coventry (Lloyd et al., 1996) and\nLoughborough (Croft et al., 2001).\nImplementation of computer assisted assessment 223\nSecurity\nThe move from traditional teaching environments and examination settings presents\nadditional issues relating to security. Frohlich (2000) states that in traditional\nenvironments it is possible to ensure the security of the exam papers and scripts, this\nincludes the transportation to and from the exam venue. However, even under this\nsystem breaches in security do occur, for example AQA had to replace 500,000\nEnglish and English Literature exam papers after a box had been tampered with\n(Curtis, 2003).\nTannenbaum (1999) defines security in computer systems as consisting of\nprocedures to ensure that individuals cannot access material for which they do not\nhave authorisation. This is essential within a CAA environment as questions and\nstudent details are stored in a database and usually the test data is sent over a local\nnetwork or the Internet. Before computers were connected to the Internet it was rela-\ntively easy to have effective security measures (Mason, 2003), but transmission of\nsensitive data over an insecure network requires additional security measure to be\nimplemented.\nEncryption techniques can be used to ensure the security of the questions and\nanswers when transmitting data over the Internet (Sim et al., 2003). To increase\nsecurity, examinations can be loaded on to the server at the last minute (Whittington,\n1999). If email is used to submit results there is a potential risk due to the lack of\nauthentication (Hatton et al., 2002). Four security requirements have been identified\nby Luck and Joy, these being: all submissions must be logged, it must be verified that\na stored document used for the assessment is the same as the one used by the student,\na feedback mechanism must inform students that their submission has been received\nand the identity of the student must be established (Luck & Joy, 1999).\nWith the majority of CAA software students and administrators are required to\nhave passwords which is often the weakest link in terms of protection (Hindle, 2003).\nAlthough an unlikely event, students could get access to the administrator password\nand change their results or gain access to the questions. Other concerns include\nauthentication and invigilation of the students, which can be are particularly\nproblematic in remote locations (Thomas et al., 2002). At present students enrolled\non distance learning courses overseas need to sit exams in a specific location such as,\nthe British Council Offices to enable authentication and invigilation. Research is\nbeing conducted to overcome these problems but unless solutions are found,\ngeographical barriers will remain as students need access to the test centres.\nDuring the test computers need to be locked down, removing the possibility of\naccessing other content and secure browsers have been developed to enable this such\nas, Questionmark Secure (Kleeman & Osborne, 2002). There are operational risks\nassociated with CAA that have security implications such as the server crashing and\nthese risks need to be identified and procedures established to minimize them\n(Zakrzewski & Steven, 2003).\nThere are software standards for security for example, the British Standards on\nInformation Security Management BS7799, which has also been adopted as an\n224 G. Sim et al.\nInternational Standard IS17799. In addition, when data from the test has been\ncollected institutions within the UK should abide by the Data Protection Act 1998\n(Mason, 2003). If security measures are in place there is no evidence to suggest that\nthe integrity of the examination is more compromised by delivery over the Internet\nthan by paper.\nConclusion\nThe implementation of CAA from a technical and pedagogical perspective is a\ncomplex process. The first, and perhaps the most important, lesson that can be\nlearned is that an institutional strategy would seem to greatly increase the chances of\nsuccess. There are recommendations that have been made to assist policy makers\nformulate an effective strategy. Without institutional support implementing security\nprocedures may be more problematic, such as locking down PCs. However,\nauthentication and invigilation in remote locations is still an issue that has yet to be\nfully resolved.\nThe other important lesson that can be learned is in relation to staff development\nand training in test construction within a CAA environment. Focused staff develop-\nment may help alleviate a number of issues, such as guessing, testing various cognitive\nskills, using appropriate question styles and accessibility. The emergence of question\nbanks may also address these issues depending on their level of interoperability.\nAnother issue is that whilst there are guidelines relating to accessible online content\nthere are still no formal guidelines relating to CAA.\nThe reliance on a single method of assessment is problematic and a diverse\nassessment strategy is usually necessary. Within an environment of increasing student\nnumbers and a reduction of staff to student ratio, CAA would appear to be a partial\nsolution. This study has highlighted the issues surrounding the implementation of\nCAA to both inform and direct further research in the field.\nReferences\nAnderson, L. W. & Krathwohl, D. R. (2001) A taxonomy for learning, teaching, and assessing. A\nrevision of blooms taxonomy of educational objectives (New York, Longman).\nAshton, H. S., Schofield, D. K. & Woodger, S. C. (2003) Pilot summative web assessment in\nsecondary education, Proceedings of the 7th International Computer Assisted Assessment Conference\n(Loughborough, Loughborough University), 19\u201329.\nBacon, R. A. (2003) Assessing the use of a new QTI assessment tool within Physics, Proceedings of\nthe 7th International Computer Assisted Assessment Conference (Loughborough, Loughborough\nUniversity), 33\u201344.\nBaranchik, A. & Cherkas, B. (2000) Correcting grade deflation caused by multiple-choice scoring,\nInternational Journal of Mathematical Education in Science and Technology, 31(3), 371\u2013380.\nBennett, R. E., Goodman, M., Hessinger, J., Kahn, H., Liggett, J., Marshall, G. & Zack, J. (1999)\nUsing multimedia in large-scale computer-based testing programs, Computers in Human\nBehaviour, 15(3), 283\u2013294.\nBloom, B. S. (1956) Taxonomy of educational objectives: the classification of educational goals. Hand-\nbook 1. Cognitive domain (New York, Longman).\nImplementation of computer assisted assessment 225\nBloom, B. S., Hastings, J. T. & Madaus, G. F. (1971) Handbook on formative and summative evalu-\nation of student learning (New York, McGraw-Hill Books).\nBoyle, A. & O\u2019Hare, D. (2003) Finding appropriate methods to assure quality computer-based\nassessment development in UK higher education, Proceedings of the 7th International Computer\nAssisted Assessment Conference, (Loughborough, Loughborough University), 67\u201382.\nBS7988 (2002) Code of practice for the use of information technology (IT) in the delivery of assessment\nBull, J., Conole, G., Davis, H. C., White, S., Danson, M. & Sclater, N. (2002) Rethinking\nassessment through learning technologies, Proceedings of the ASCILITE 2002 (Auckland,\nUNITEC), 1\u201312.\nBull, J. & McKenna, C. (2001) Blueprint for computer-assisted assessment (Loughborough,\nLoughborough University).\nBurstein, J., Leacock, C. & Swartz, R. (2001) Automated evaluation of essays and short answers,\nProceedings of the 5th International Computer Assisted Assessment Conference (Loughborough,\nLoughborough University).\nBurton, R. F. (2001) Quantifying the effects of chance in multiple choice and true\/false tests: ques-\ntion selection and guessing of answers, Assessment and Evaluation in Higher Education, 26(1),\n41\u201350.\nBurton, R. F. & Miller, D. J. (1999) Statistical modelling of multiple-choice and true\/false tests:\nways of considering, and of reducing, the uncertainties attributed to guessing, Assessment and\nEvaluation in Higher Education, 24(4), 399\u2013411.\nBush, M. (1999) Alternative marking schemes for on-line multiple choice tests, Proceedings of the\n7th Annual Conference on the Teaching of Computing (Belfast, Elsevier).\nCallear, D. & King, T. (1997) Using computer-based tests, ALT-J, 5(1), 27\u201332.\nCassady, J. C. & Johnson, R. E. (2002) Cognitive test anxiety and academic performance, Contem-\nporary Educational Psychology, 27(2), 270\u2013295.\nChristie, J. R. (1999) Automated essay marking for both content and style, Proceedings of the\n3rd Annual Computer Assisted Assessment Conference (Loughborough, Loughborough Univer-\nsity).\nCIAD (2003) Summary of question styles. Available online at: http:\/\/www.derby.ac.uk\/ciad\/\nciastyles.html (accessed 30 June 2003).\nClariana, R. & Wallace, P. (2002) Paper-based versus computer-based assessment: key factors\nassociated with test mode effect, British Journal of Educational Technology, 33(5), 593\u2013602.\nCox, K. & Clark, D. (1998) The use of formative quizzez for deep learning, Computers and Educa-\ntion, 30(3), 157\u2013167.\nCrisp, B. R. (2002) Assessment methods in social work education: a review of the literature, Social\nWork Education, 21(2), 259\u2013269.\nCroft, A. C., Danson, M., Dawson, B. R. & Ward, J. P. (2001) Experience of using computer\nassisted assessment in engineering mathematics, Computers and Education, 37(1), 53\u201366.\nCurtis, P. (2003) Missing paper sparks exam reprint (London, Guardian).\nDaly, C. & Waldron, J. (2002) Introductory programming, problem solving and computer assisted\nassessment, Proceedings of the 6th International Computer Assisted Assessment Conference,\n(Loughborough, Loughborough University), 95\u2013106.\nDaly, J. (2002) An XML question bank using Microsoft Office, Proceedings of the 6th International\nComputer Assisted Assessment Conference (Loughborough, Loughborough University), 107\u2013118.\nDavies, P. (2002) There\u2019s no confidence in multiple-choice testing, Proceedings of the 6th\nInternational Computer Assisted Assessment Conference (Loughborough, Loughborough\nUniversity), 119\u2013132.\nDowsing, R. D. (1998) Flexibility and the technology of computer aided assessment, Proceedings of\nthe ASCILITE 1998 (Wollongong, University of Wollongong), 163\u2013171.\nDyson, M. C. & Kipping, G. J. (1997) The legibility of screen formats: are three columns better\nthan one? Computers and Graphics, 21(6), 703\u2013712.\nEbel, R. L. (1972) Essentials of educational measurement (Englewood Cliffs, Prentice-Hall).\n226 G. Sim et al.\nFrohlich, R. (2000) Keeping the wolves from the door, wolves in sheep clothing, that is,\nProceedings of the 4th International Computer Assisted Assessment Conference (Loughborough,\nLoughborough University).\nGraham, D. (2004) A survey of assessment methods employed in UK higher education\nprogrammes for HCI courses, Proceedings of the 7th HCI Educators Workshop (Preston, LTSN),\n66\u201369.\nHaladyna, T. M. (1996) Writing test items to evaluate higher order thinking (Needham Heights,\nAllyn & Bacon).\nHarper, R. (2002) Allowing for guessing and for expectations from the learning outcomes in\ncomputer-based assessments, Proceedings of the 6th International Computer Assisted Assessment\nConference (Loughborough, Loughborough University), 139\u2013150.\nHarper, R. (2003) Correcting computer-based assessment for guessing, Journal of Computer\nAssisted Learning, 19(1), 2\u20138.\nHatton, S., Boyle, A., Byrne, S. & Wooff, C. (2002) The use of PGP to provide secure email\ndelivery of CAA results, Proceedings of the 6th International Computer Assisted Assessment\nConference (Loughborough, Loughborough University), 149\u2013160.\nHawe, E. (2003) It\u2019s pretty difficult to fail; the reluctance of lecturers to award a fail grade,\nAssessment and Evaluation in Higher Education, 28(4), 371\u2013382.\nHeinrich, E. & Wang, Y. (2003) Online marking of essay-type assignments, Proceedings of the World\nConference on Educational Multimedia Hypermedia and Telecommunications (Hawaii, AACE),\n768\u2013772.\nHerd, G. & Clark, G. (2002) Computer assisted assessment implementing CAA in FE sector in Scotland:\nquestion types (Glenrothes, Glenrothes College).\nHerd, G. & Clark, G. (2003) CAA implementation in the FE Sector in Scotland (Glenrothes,\nGlenrothes College).\nHESA (1995) Students in higher education institutions 1994\/95 (London, HMSO).\nHESA (2002) Students in higher education institutions 2001\/02 (London, HMSO).\nHindle, S. (2003) Careless about privacy, Computers and Security, 22(4), 284\u2013288.\nHorney, W. (2003) Assessing using grade-related criteria: a single currency for universities? Assess-\nment and Evaluation in Higher Education, 28(4), 435\u2013454.\nJafarpur, A. (2003) Is the test constructor a facet? Language Testing, 20(1), 57\u201387.\nJefferies, P., Constable, I., Kiely, B., Richardson, D. & Abraham, A. (2000) Computer aided\nassessment using WebCT, Proceedings of the 4th International Computer Assisted Assessment\nConference (Loughborough, Loughborough University).\nKing, T. & Duke-Williams, E. (2002) Using computer aided assessment to test higher level learn-\ning outcomes, Proceedings of the 5th International Computer Assisted Assessment Conference\n(Loughborough, Loughborough University).\nKleeman, J. & Osborne, C. (2002) A practical look at delivering assessment to BS7988\nrecommendations, Proceedings of the 6th International Computer Assisted Assessment Conference\n(Loughborough, Loughborough University), 163\u2013170.\nKnight, P. (2001) A briefing on key concepts formative and summative, criterion and norm-referenced\nassessment (York, LTSN Generic Centre).\nLatu, E. & Chapman, E. (2002) Computerised adaptive testing, British Journal of Educational\nTechnology, 33(5), 619\u2013622.\nLay, S. & Sclater, N. (2001) Question and test interoperability: an update on national and\ninternational developments, Proceedings of the 5th International Computer Assisted Assessment\nConference (Loughborough, Loughborough University).\nLiu, M., Papathanasiou, E. & Hao, Y. (2001) Exploring the use of multimedia examination\nformats in undergraduate teaching: results from the fielding testing, Computers in Human\nBehaviour, 17(3), 225\u2013248.\nLloyd, D., Martin, J. G. & McCaffery, K. (1996) The introduction of computer based testing on\nan engineering technology course, Assessment and Evaluation in Higher Education, 21(1), 83\u201390.\nImplementation of computer assisted assessment 227\nLoewenberger, P. & Bull, J. (2003) Cost-effectiveness analysis of computer-based assessment,\nALT-J, 11(2), 23\u201345.\nLuck, M. and Joy, M. (1999) A secure on-line submission system, Software\u2014Practice and\nExperience, 29(8), 721\u2013740.\nMacdonald, J. & Twining, P. (2002) Assessing activity-based learning for a networked course,\nBritish Journal of Educational Technology, 33(5), 603\u2013618.\nMackenzie, D. (1999) Recent developments in the triartite interactive assessment delivery\nsystem (TRIADS), Proceedings of the 3rd Annual Computer Assisted Assessment Conference\n(Loughborough, Loughborough University).\nMackenzie, D., Hallam, B., Baggott, G. & Potts, J. (2002) TRIADS experiences and\ndevelopments. A panel discussion, Proceedings of the 6th International Computer Assisted\nAssessment Conference (Loughborough, Loughborough University).\nMackenzie, D. & O\u2019Hare, D. (2002) Empirical prediction of the measurement scale and base\nlevel \u2018Guess Factor\u2019 for advanced computer-based assessment, Proceedings of the 6th Interna-\ntional Conference of Computer Aided Assessment (Loughborough, Loughborough University),\n187\u2013201.\nMackenzie, D., O\u2019Hare, D., Paul, C., Boyle, A., Edwards, D., Willimas, D. & Wilkins, H. (2004)\nAssessment for learning: the Triads assessment of learning outcomes project and the\ndevelopment of a pedagogically friendly computer-based assessment system, in: D. O\u2019Hare &\nD. Mackenzie (Eds) Advances in computer aided assessment (Birmingham, SEDA), 11\u201325.\nMason, S. (2003) Electronic security is a continous process, Computer Fraud and Security, 2003(1),\n13\u201315.\nMaughan, S., Peet, D. & Willmot, A. (2001) On-line formative assessment item banking and\nlearning support, Proceedings of the 5th International Computer Assisted Assessment Conference\n(Loughborough, Loughborough University).\nMayer, R. E. (2002) A taxonomy for computer-based assessment of problem solving, Computers in\nHuman Behaviour, 18(6), 623\u2013632.\nMayes, D. K., Sims, V. K. & Koonce, J. M. (2001) Comprehension and workload differences\nfor VDT and paper-based reading, International Journal of Industrial Ergonomics, 28(6),\n367\u2013378.\nMcAlpine, M. (2002) Principles of assessment (Luton, CAA Centre).\nMcCabe, M. & Barrett, D. (2003) CAA scoring strategies for partial credit and confidence levels,\nProceedings of the 7th International Computer Assisted Assessment Conference (Loughborough,\nLoughborough University), 209\u2013219.\nMcKenna, C. & Bull, J. (2000) Quality assurance of computer-assisted assessment: practical and\nstrategic issues, Quality Assurance in Education, 8(1), 24\u201331.\nMcLaughlin, P. J., Fowell, S. L., Dangerfield, P. H., Newton, D. J. & Perry, S. E. (2004)\nDevelopment of computerised assessment (TRIADS) in an undergraduate medical school, in:\nD. O\u2019Hare & D. Mackenzie (Eds) Advances in computer aided assessment (Birmingham,\nSEDA), 25\u201332.\nMills, C. N., Potenza, M. T., Fremer, J. J. & Ward, W. C. (2002) Computer-based testing. Building\nthe foundation for future assessments (Mahwah, Lawrence Erlbaum Associates).\nMorgan, M. R. J. (1979) MCQ: an interactive computer program for multiple-choice self testing,\nBiochemical Education, 7(3), 67\u201369.\nNugent, G. (2003) On-line multimedia assessment for K-4 students, Proceedings of the World\nConference on Educational Multimedia, Hypermedia and Telecommunications (Hawaii, AACE),\n1051\u20131057.\nO\u2019Leary, R. & Cook, J. (2001) Wading through treacle: CAA at the University of Bristol, Proceedings\nof the 5th International Computer Assisted Assessment Conference (Loughborough, Loughborough\nUniversity).\nPain, D. & Le Heron, J. (2003) WebCT and online assessment: the best thing since SOAP? Educa-\ntional Technology and Society, 6(2), 62\u201371.\n228 G. Sim et al.\nPaterson, J. S. (2002) Linking on-line assessment in mathematics to cognitive skills, Proceedings of\nthe 6th International Computer Assisted Assessment Conference (Loughborough, Loughborough\nUniversity), 295\u2013306.\nPaxton, M. (2000) A linguistic perspective on multiple choice questioning, Assessment and\nEvaluation in Higher Education, 25(2), 109\u2013119.\nPhipps, L. & McCarthy, D. (2001) Computer assisted assessment and disabilities, Proceedings of\nthe 5th International Computer Assisted Assessment Conference (Loughborough, Loughborough\nUniversity).\nPollock, M. J., Whittington, C. D. & Doughty, G. F. (2000) Evaluating the costs and benefits of\nChanging to CAA, Proceedings of the 4th International Computer Assisted Assessment Conference\n(Loughborough, Loughborough University).\nPowers, D. E., Burstein, J. C., Chodorow, M., Fowles, M. E. & Kukich, K. (2002) Stumping\ne-rator: challenging the validity of automated essay scoring, Computers in Human Behaviour,\n18(2), 103\u2013134.\nRace, P. (1995) The art of assessing, The New Academic, 4(3).\nReid, N. (2002) Designing online quiz questions to assess a range of cognitive skills, Proceedings of\nthe World Conference on Educational Multimedia, Hypermedia and Telecommunications (Denver,\nAACE), 1625\u20131630.\nRicketts, C. & Wilks, S. (2002a) What factors affect students opinions of computer-assisted\nassessment? Proceedings of the 6th International Computer Assisted Assessment Conference\n(Loughborough, Loughborough University), 307\u2013316.\nRicketts, C. & Wilks, S. (2002b) Improving student performance through computer-based assess-\nment: insights from recent research, Assessment and Evaluation in Higher Education, 27(5),\n475\u2013479.\nSabar, N. (2002) Towards principled practice in evaluation: learning from instructors\u2019 dilemmas\nin evaluating graduate students, Studies in Educational Evaluation, 28(4), 329\u2013345.\nSambell, K., Sambell, A. & Sexton, G. (1999) Students\u2019 perception of the learning benefits of\ncomputer-assisted assessment: a case study in electronic engineering, in: S. Brown, J. Bull\n& P. Race (Eds) Computer-assisted assessment in higher education (Birmingham, SEDA),\n179\u2013191.\nSchenkman, B., Fukuda, T. & Persson, B. (1999) Glare from monitors measured with subjective\nscales and eye movements, Displays, 20, 11\u201321.\nSclater, N., Davis, H. C., White, S. A., Conole, G. C. & Danson, M. (2003) Technologies for\nonline interoperable assessment, Proceedings of the CAL03 (Belfast, Elsevier).\nSclater, N. & Howie, K. (2003) User requirements of the \u2018ultimate\u2019 online assessment engine,\nComputers and Education, 40(3), 285\u2013306.\nSealey, C., Humphries, P. & Reppert, D. (2003) At the coal face: experience of computer\nbased exams, Proceedings of the 7th International Computer Assisted Assessment Conference\n(Loughborough, Loughborough University), 357\u2013376.\nSENDA (2001), Special Educational Needs and Disability Act 2001,  http:\/\/www.hmso.gov.uk\/\nacts\/acts2001\/20010010.htm (Accessed 10 March 2004)\nSim, G., Malik, N. A. & Holifield, P. (2003) Strategies for large-scale assessment: an institutional\nanalysis of research and practice in a virtual university, Proceedings of the 7th International\nComputer Assisted Assessment Conference (Loughborough, Loughborough University), 379\u2013390.\nStephens, D. (1994) Using computer-assisted assessment: time saver or sophisticated distractor?\nActive Learning, 1, 11\u201315.\nStephens, D., Bull, J. & Wade, W. (1998) Computer-assisted assessment: suggested guide-\nlines for an institutional strategy, Assessment and Evaluation in Higher Education, 23(3),\n283\u2013294.\nStephens, D. & Mascia, J. (1997) Results of a survey into the use of computer-assisted assessment in\ninstitutions of higher education (Loughborough University). Available online at: http:\/\/\nwww.lboro.ac.uk\/service\/ltd\/flicaa\/downloads\/survey.pdf (accessed 18 June 2004).\nImplementation of computer assisted assessment 229\nStevenson, A., Sweeny, P., Greenan, K. & Alexander, S. (2002) Integrating CAA within the\nUniversity of Ulster, Proceedings of the 6th International Computer Assisted Assessment Conference\n(Loughborough, Loughborough University), 329\u2013340.\nTannenbaum, R. S. (1999) Theoretical foundations of multimedia (New York, W.H. Freeman).\nThomas, P., Price, B., Paine, C. & Richards, M. (2002) Remote electronic examinations: student\nexperience, British Journal of Educational Technology, 33(5), 537\u2013549.\nValenti, S., Cucchiarelli, A. & Panti, M. (2002) Computer based assessment systems evaluation\nvia the ISO90126 quality model, Journal of Information Technology Education, 1(3), 157\u2013175.\nWalker, D. M. & Thompson, J. S. (2001) A note on multiple choice exams, with respect to\nstudents\u2019 risk preference and confidence, Assessment and Evaluation in Higher Education,\n26(3), 261\u2013267.\nWarburton, B. & Conole, G. (2003) CAA in UK HEIs\u2014the state of the art? Proceedings of the 7th\nInternational Computer Assisted Assessment Conference (Loughborough, Loughborough\nUniversity), 433\u2013441.\nWhite, S. & Davis, H. C. (2000) Creating large scale test banks: a briefing for participative discus-\nsion and agendas, Proceedings of the 4th International Computer Assisted Assessment Conference\n(Loughborough, Loughborough University).\nWhittington, D. (1999) Technical and security issues, in: S. Brown, J. Bull & P. Race (Eds)\nComputer assisted assessment in higher education (Birmingham, SEDA), 21\u201328.\nWiles, K. (2002) Accessibility and computer-based assessment: a whole new set of issues? in:\nL. Phipps, A. Sutherland & J. Seale (Eds) Access all areas: disability, technology and learning\n(Oxford and York, ALT\/TechDis), 61\u201366.\nWiles, K. & Ball, S. (2003) Constructing accessible CBA: minor works or major renovations?\nProceedings of the 7th International Computer Assisted Assessment Conference (Loughborough,\nLoughborough University), 445\u2013451.\nWiltfelt, C., Philipsen, P. E. & Kaiser, B. (2002) Chat as media in exams, Education and\nInformation Technologies, 7(4), 343\u2013349.\nWood, D. A. (1960) Test construction (Columbus, Charles E. Merrill Books).\nYorke, M., Barnett, G., Bridges, P., Evanson, P., Haines, C., Jenkins, D., Knight, P., Scurry, D.,\nStowell, M. & Woolf, H. (2002) Does grading method influence honours degree classifica-\ntion? Assessment and Evaluation in Higher Education, 27(3), 269\u2013279.\nZakrzewski, S. & Steven, S. (2003) Computer-based assessment: quality assurance issues, the hub\nof the wheel, Assessment and Evaluation in Higher Education, 28(6), 609\u2013623.\n"}