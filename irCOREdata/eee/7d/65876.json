{"doi":"10.1007\/978-3-540-30138-7_27","coreId":"65876","oai":"oai:dro.dur.ac.uk.OAI2:4321","identifiers":["oai:dro.dur.ac.uk.OAI2:4321","10.1007\/978-3-540-30138-7_27"],"title":"How explicit are the barriers to failure in safety arguments?","authors":["Smith, S. P.","Harrison, M. D.","Schupp, B. A."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":["Heisel, M.","Liggesmeyer, P.","Wittmann, S."],"datePublished":"2004-09-24","abstract":"Safety cases embody arguments that demonstrate how safety properties of a system are upheld. Such cases implicitly document the barriers that must exist between hazards and vulnerable components of a system. For safety certification, it is the analysis of these barriers that provide confidence in the safety of the system. The explicit representation of hazard barriers can provide additional insight for the design and evaluation of system safety. They can be identified in a hazard analysis to allow analysts to reflect on particular design choices. Barrier existence in a live system can be mapped to abstract barrier representations to provide both verification of barrier existence and a basis for quantitative measures between the predicted barrier behaviour and performance of the actual barrier. This paper explores the first stage of this process, the binding between explicit mitigation arguments in hazard analysis and the barrier concept. Examples from the domains of computer-assisted detection in mammography and free route airspace feasibility are examined and the implications for system certification are considered","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/65876.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/4321\/1\/4321.pdf","pdfHashValue":"b65414cd5f6483067862301d76236b00f2c70e0b","publisher":"Springer","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:4321<\/identifier><datestamp>\n      2017-03-08T12:21:54Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        How explicit are the barriers to failure in safety arguments?<\/dc:title><dc:creator>\n        Smith, S. P.<\/dc:creator><dc:creator>\n        Harrison, M. D.<\/dc:creator><dc:creator>\n        Schupp, B. A.<\/dc:creator><dc:description>\n        Safety cases embody arguments that demonstrate how safety properties of a system are upheld. Such cases implicitly document the barriers that must exist between hazards and vulnerable components of a system. For safety certification, it is the analysis of these barriers that provide confidence in the safety of the system. The explicit representation of hazard barriers can provide additional insight for the design and evaluation of system safety. They can be identified in a hazard analysis to allow analysts to reflect on particular design choices. Barrier existence in a live system can be mapped to abstract barrier representations to provide both verification of barrier existence and a basis for quantitative measures between the predicted barrier behaviour and performance of the actual barrier. This paper explores the first stage of this process, the binding between explicit mitigation arguments in hazard analysis and the barrier concept. Examples from the domains of computer-assisted detection in mammography and free route airspace feasibility are examined and the implications for system certification are considered.<\/dc:description><dc:publisher>\n        Springer<\/dc:publisher><dc:source>\n        Heisel, M. & Liggesmeyer, P. & Wittmann, S. (Eds.). (2004). Computer safety, reliability, and security : 23rd International Conference, SAFECOMP 2004, Potsdam, Germany, September 21-24, 2004 ; proceedings. Berlin: Springer, pp. 325-337, Lecture notes in computer science(3219)<\/dc:source><dc:contributor>\n        Heisel, M.<\/dc:contributor><dc:contributor>\n        Liggesmeyer, P.<\/dc:contributor><dc:contributor>\n        Wittmann, S.<\/dc:contributor><dc:date>\n        2004-09-24<\/dc:date><dc:type>\n        Book chapter<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:4321<\/dc:identifier><dc:identifier>\n        issn:0302-9743<\/dc:identifier><dc:identifier>\n        issn: 1611-3349<\/dc:identifier><dc:identifier>\n        doi:10.1007\/978-3-540-30138-7_27<\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/4321\/<\/dc:identifier><dc:identifier>\n        https:\/\/doi.org\/10.1007\/978-3-540-30138-7_27<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/4321\/1\/4321.pdf<\/dc:identifier><dc:rights>\n        The final publication is available at Springer via http:\/\/dx.doi.org\/10.1007\/978-3-540-30138-7_27<\/dc:rights><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["0302-9743"," 1611-3349","issn: 1611-3349","issn:0302-9743"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2004,"topics":[],"subject":["Book chapter","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n02 February 2010\nVersion of attached file:\nAccepted Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nSmith, S. P. and Harrison, M. D. and Schupp, B. A. (2004) \u2019How explicit are the barriers to failure in safety\narguments ?\u2019, in Computer safety, reliability, and security : 23rd International Conference, SAFECOMP 2004,\nPotsdam, Germany, September 21-24, 2004 ; proceedings. Berlin: Springer, pp. 325-337. Lecture notes in\ncomputer science. (3219).\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1007\/b100227\nPublisher\u2019s copyright statement:\nThe original publication is available at www.springerlink.com\nAdditional information:\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\n  \nDurham Research Online \n \nDeposited in DRO: \n02 February 2010 \n \nPeer-review status: \nPeer-reviewed \n \nPublication status: \nAccepted for publication version \n \nCitation for published item: \nSmith, S. P. and Harrison, M. D. and Schupp, B. A. (2004) 'How explicit are the barriers to \nfailure in safety arguments ?', in Computer safety, reliability, and security : 23rd International \nConference, SAFECOMP 2004, Potsdam, Germany, September 21-24, 2004 ; proceedings. \nBerlin: Springer, pp. 325-337. Lecture notes in computer science. (3219). \n \nFurther information on publishers website: \nhttp:\/\/dx.doi.org\/10.1007\/b100227 \n \nPublisher\u2019s copyright statement: \nThe original publication is available at www.springerlink.com \n \n \n \n \n \n \n \n \n \n \n \nUse policy \n \nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior \npermission or charge, for personal research or study, educational, or not-for-profit purposes provided that : \n \n\uf0a7 a full bibliographic reference is made to the original source \n\uf0a7 a link is made to the metadata record in DRO \n\uf0a7 the full-text is not changed in any way \n \nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders. \n \nPlease consult the full DRO policy for further details. \n \nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom \nTel : +44 (0)191 334 2975 | Fax : +44 (0)191 334 2971 \nhttp:\/\/dro.dur.ac.uk \nHow explicit are the barriers to failure\nin safety arguments?\nShamus P. Smith?, Michael D. Harrison??, and Bastiaan A. Schupp\nDependability Interdisciplinary Research Collaboration,\nDepartment of Computer Science,\nUniversity of York, York YO10 5DD,\nUnited Kingdom.\n{Shamus.Smith, Michael.Harrison, Bastiaan.Schupp}@cs.york.ac.uk\nAbstract. Safety cases embody arguments that demonstrate how safety\nproperties of a system are upheld. Such cases implicitly document the\nbarriers that must exist between hazards and vulnerable components of\na system. For safety certification, it is the analysis of these barriers that\nprovide confidence in the safety of the system.\nThe explicit representation of hazard barriers can provide additional in-\nsight for the design and evaluation of system safety. They can be identi-\nfied in a hazard analysis to allow analysts to reflect on particular design\nchoices. Barrier existence in a live system can be mapped to abstract\nbarrier representations to provide both verification of barrier existence\nand a basis for quantitative measures between the predicted barrier be-\nhaviour and performance of the actual barrier. This paper explores the\nfirst stage of this process, the binding between explicit mitigation argu-\nments in hazard analysis and the barrier concept. Examples from the\ndomains of computer-assisted detection in mammography and free route\nairspace feasibility are examined and the implications for system certifi-\ncation are considered.\n1 Introduction\nBarriers are often complex socio-technical systems: a combination of technical,\nhuman and organisational measures that prevent or protect against an adverse\neffect. Barriers for safety critical systems include physical representations, for\nexample a mechanical guard on an electronic throttle [1], as well as beliefs, such\nas confidence in system safety based on conformance to applied standards. A no\nsmoking sign is a typical example of a barrier as a complex system. Although\nthe sign aims to prevent fire from cigarettes, it is not just the sign. The barrier\nincludes awareness of how smoking may cause fires, awareness of the significance\n? Now at the Department of Computer Science, University of Durham, Durham DH1\n3LE, shamus.smith@durham.ac.uk\n?? Now at the Informatics Research Institute, University of Newcastle Upon Tyne,\nNewcastle Upon Tyne, NE1 7RU, michael.harrison@ncl.ac.uk\n2of the sign, the sign\u2019s visibility, training of the smokers, and its relation to other\nbarrier systems such as an installed smoke alarm and sprinkler system [19].\nBarriers embody both abstract and concrete representations of properties\ncommonly argued in a safety case. Kelly et al. [11] defines a safety case as the\ndocument, or set of documents, presenting the argument that a system is ac-\nceptably safe to operate in a given context. Such cases implicitly document the\nbarriers that must exist between hazards and hazardous states and vulnerable\ncomponents of a system. For certification it is the verification of these barriers\nthat provide confidence in the safety of the system. However, explicit represen-\ntations of such barriers are commonly absent from safety case documentation\nand the associated arguments for compliance to particular standards.\nExplicit barrier description in hazard analysis can provide insight throughout\nthe development of safety critical systems and in addition aid safety certification\nby documenting barrier development through design to implementation in a live\nsystem. For example if there is a hazard mitigation that an interlock1 inhibits\nsome type of behaviour, this may feature as evidence in a safety case. It should be\npossible to prove that it is in place in the live system and that its performance\ncan be accessed and compared to predicted performance in the initial hazard\nanalysis.\nThis paper investigates the binding of explicit mitigation arguments in hazard\nanalysis to the barrier concept. Identifying explicit barriers early in system devel-\nopment can allow informed decision making through design and implementation\nphases of a system\u2019s development. The remainder of this paper is as follows.\nSection 2 describes barriers in relation to risk reduction in design and imple-\nmentation. Section 3 presents an overview of barriers in the context of hazard\nanalysis. The use of explicit barriers to highlight hazard and barrier properties\nare exemplified in two case studies in Sections 4 and 5. Section 6 overviews the\nuse of explicit barriers for certification. Section 7 presents conclusions.\n2 Risk reduction and barriers\nRisk reduction is a key factor in the design of safety critical systems and in as-\nsessment of their operational safety. It is achieved either by preventing hazards or\nby protecting against hazards. Prevention typically involves design modifications\nof the total system, including for example operating procedures. Protection in-\nvolves the design of additional systems, which embody barriers that fend against\nadverse events, damage or harm [19]. Barriers represent the diverse physical and\norganisational measures that are taken to prevent a target from being affected\nby a potential hazard [10, pg 359]. A barrier is an obstacle, an obstruction, or\na hindrance that may either (i) prevent an action from being carried out or an\nevent from taking place, or (ii) prevent or lessen the impact of the consequences,\nlimiting the reach of the consequences or weakening them in some way [9].\n1 An interlock is a mechanism which ensures that potentially hazardous actions are\nonly performed at times when they are safe [22].\n3The concepts and terminology related to barriers or safety features vary\nconsiderably [7], for example Hollnagel [9] presents a classification of barrier\nsystems based on four main categories:\n1. Material barriers physically prevent an action from being carried out or the\nconsequences of a hazard from spreading. For example a fence or wall.\n2. Functional barriers impede an action from being carried out, for instance\nthe use of an interlock.\n3. Symbolic barriers require an act of interpretation in order to achieve their\npurpose. For example a give way sign indicates a driver should give way but\ndoes not actively enforce\/stop non-compliance.\n4. Immaterial barriers are not physically present or represented in the situa-\ntion, but depend on the knowledge of the user to achieve their purpose. For\nexample the use of standards.\nThis paper makes no commitment to the terminology of barriers and in-\nstead focuses on the presence of barriers, in whatever form, in the context of a\nhazardous event or action. The pre- and post-condition states of a hazard are\nrepresented by preventive and protective barriers respectively. Therefore the use\nof barriers, either for the prevention of hazards or the protection from hazardous\neffect, is considered to be part of the process of hazard analysis.\n3 Hazard analysis and barriers\nHazard analysis is at the heart of any safety programme [13, pg 287]. It is a\nnecessary first step before hazards can be eliminated or controlled through design\nor operational procedures. Within hazard analysis, descriptive arguments2 are\nimplicitly used to justify prevention arguments of identified hazards.\nPrevious work has demonstrated that explicit mitigation arguments allow\nan analyst to reflect on the mitigations present and constitute an initial step to\nprocesses such as argument reuse in hazard analysis [20, 21]. In addition, explicit\narguments document the reasoning being applied in an analysis session. If such\ndecisions are lost, evaluation of the analysis and certification can be problematic.\nMitigation arguments to hazards are implicitly described in terms of bar-\nriers. Barriers against hazards may take a variety of forms for example proce-\ndures, training, human action, as well as, systems and components that prevent\naccidents or provide mitigation of consequences and constitute barriers against\ninjury [14, pg A-1].\nAlthough a range of methods have been developed to support systematic\nhazard analysis, for example, HAZOP (Hazard and Operability Studies) [12],\nFMEA (Failure Modes and Effect Analysis) [3] and THEA (Technique for Human\nError Assessment) [15], such methods stop short of explicitly defining barriers.\nThe explicit representation of barriers is a step towards defining a semantics of\n2 Descriptive arguments can be considered as informal arguments in contrast to more\nquantitative, numeric arguments.\n4safety arguments and allows analysts to reflect on the hazards being mitigated\nand the associated implications for design and implementation of safe systems\nso that risk reduction techniques can be more effectively implemented.\nIn Sections 4 and 5, two existing hazard analyses will be examined and the ex-\nplicit barriers inherent in the analysis identified. Barrier implications are drawn\nout and areas of concern for both the hazard analysis and any associated design\nare highlighted. The case studies are the proposed design of a computer-aided\ndetection tool (CADT) for mammography and the feasibility of eight-state free\nroute airspace.\n4 Computer-aided mammography example\nThe UK Breast Screening Programme is a national service that involves a number\nof screening clinics, each with two or more radiologists. Initial screening tests are\nby mammography, where one or more X-ray films (mammograms) are taken by\na radiographer. Each mammogram is then examined for evidence of abnormality\nby two experienced radiologists [8]. A decision is then made as to whether to\nrecall a patient for further tests because there is suspicion of cancer [23]. Within\nthe screening process it is desirable to achieve the minimum number of false\npositives (FPs), so that fewer women are recalled for further tests unnecessarily,\nand the maximum true positive (TP) rate, so that few cancers will be missed [8].\nUnfortunately the radiologists\u2019 task is a difficult one because the small number\nof cancers is hidden among a large number of normal cases. Also the use of two\nexperienced radiologists, for double readings, makes this process labour intensive.\nComputer-based image analysis techniques are being explored to enable a\nsingle radiologist to achieve performance that is equivalent or similar to that\nachieved by double readings [2, 8]. Computer-aided detection systems can provide\nradiologists with a useful \u201csecond opinion\u201d [24]. The case study in this section\ninvolves the introduction of a CADT as an aid in screening mammograms. When\na CADT is used, the radiologist initially views the mammogram and records a\nrecall decision. The CADT marks a digitised version of the X-ray film with\n\u201cprompts\u201d that the radiologist should examine. The proposed procedure is that\nthe radiologist records a decision before looking at the CADT prompted x-ray\nfilm. A final decision on a patient\u2019s recall is then taken by the human radiologist\nbased on the original decision and the examination of the marked-up X-ray. A\nsummary of this process can be seen in Figure 1 (from [23]).\nA system based on the model shown in Figure 1 has been investigated to\nidentify the undesirable consequences that may arise. An incorrect recall deci-\nsion resulting from a misdiagnosis of cancer is an example of such an consequence.\nThe general argument for safe use involves a number of argument legs covering\nthree main activities namely (i) human analysis of the X-ray, (ii) CADT anal-\nysis of the X-ray and (iii) the recall decision by the human, based on a review\nof their original analysis and the CADT analysis. A HAZOP [12] style analysis\nfor the system was completed by a team including the authors [21]. HAZOP is\ndescribed as a technique of imaginative anticipation of hazards and operation\n5Fig. 1. Model for person using computerised aid for reading mammograms in breast\nscreening.\nproblems [16, pg43]. It is a systematic technique that attempts to consider events\nin a system or process exhaustively. The output from this process was a HA-\nZOP table summarising cause, consequence and protection relations to hazards\nidentified in the proposed system (see Table 3 in Appendix A for four example\nHAZOP rows).\nThe CADT HAZOP contained 105 HAZOP rows and 61 hazards that re-\nquired mitigation. In total 99 barriers were identified in the mitigation arguments\nof the 61 identified hazards. Typical implied barriers included human oriented\nbarriers such as \u201cstaff training\u201d, environmental conditions, for example \u201croom\nlayout\u201d, and system components, for example \u201cbar codes on x-rays\u201d. The barri-\ners were identified through the examination of the mitigation arguments present\nin the HAZOP. Each barrier was considered independent as validating true inde-\npendence between the associated mitigation arguments is non-trivial and outside\nthe scope of this paper. By examining these barriers further insight into the im-\nplications of the hazard mitigation can be derived in the context of the proposed\nsystem. The following sections present several views on the nature of barriers\nidentified in post HAZOP analysis. However, it should be noted that these are\nnot necessarily an exhaustive set of the barrier properties or implications for\nsafety.\n4.1 Preventive vs. protective barriers\nIt is common for hazard mitigations to be considered in terms of independence\nand diversity. The belief that a hazard has been mitigated may be given a higher\nlevel of confidence if multiple diverse arguments are present. Also the nature of\nthe associated barrier in the context of the initiating hazard event is also of\nrelevance. Classifying preventive and protective barriers highlights this consid-\neration. For example if a hazard has only preventive barriers there is no fault\ntolerance in the system, as provided by protective barriers.\nIn the mammography analysis there are 7 examples of protective barriers and\n92 examples of preventive barriers. Two of the protective barriers and 15 of the\npreventive barriers are unique. Therefore the majority of the barrier protection\nin this system is based on preventive barriers. This has implications for the\n6fault tolerance of the system as the failure of preventive barriers may lead to a\npotentially hazardous system state not anticipated by the designers.\n4.2 Barrier frequency and type\nCommonly there is not a one-to-one relation between hazards and barriers. One\nhazard may be protected against by several barriers (see Section 4.3) and one\nbarrier may feature in the mitigation arguments of several hazards. A barrier\nmitigation with a number of high consequence hazards will require greater reli-\nability as more of the system safety will be dependent on it. This is particularly\nthe case if a single barrier is the only defence to a hazard (see Section 4.3). In ad-\ndition there may be cost-benefit tradeoffs between barriers. Expensive barriers,\nin terms of physical cost, time to implement and\/or ongoing maintenance, that\nprovide protection against a single hazard may be less desirable than alternative\nbarrier solutions that provide protection from multiple hazards. Such knowledge\ncan provide justification for particular design decisions.\nTable 1. Eight most common barriers in the mammography analysis\nBarrier Frequency Barrier Frequency\nStaff training 23 Good practice following 19\nCADT reliability 12 Timetable enforcement 7\nSafety culture 6 Bar codes on x-rays 6\nUser experience 6 CADT testing 5\nTable 1 shows the eight most common barriers in the mammography analysis.\nThe top two most common barriers are human oriented and together contribute\n42% of the barriers for this example. This may seem surprising considering this\nsystem is a technology based solution to a labour intensive process. Even in this\ncomputer-based system there is reliance on appropriate training in the mitigation\nof hazards. Also these human oriented barriers operate when the system is live\nand are therefore prone to performance variation and other human-error issues\n(see [17]). Technology based barriers, e.g. \u201cCADT reliability\u201d, \u201cbar codes on\nx-rays\u201d and \u201cCADT testing\u201d, contribute 23% of the barriers. From a total of\n17 unique barriers identified in the hazard analysis, barriers in the top eight\nrepresent 85% of the total barriers. Identifying the barriers that have the most\nimpact can allow developers to focus their efforts.\nIn addition to the occurrence of particular barriers in this case study, the\nfrequency of demand of barriers significantly modifies the predicted risk. Expec-\ntations on how often a barrier will be expected to be active, and not fail, will\ndetermine how critical it is to the system it is protecting. However, the analysis\nmaterial discussed in this paper does not provide details of such expectations\nand will therefore not be discussed here further.\n74.3 Barriers per hazard\nAccidents happen because barriers fail and hazards are present. Hollnagel [9]\nobserves that accidents are frequently characterised in terms of the events and\nconditions that led to the final outcome or in terms of the barriers that have\nfailed. As a consequence, redundancy is a common feature in the safety aspects\nof dependable systems. In particular, redundancy is used to prevent the failure of\na single component causing the failure of a complete system - a so-called single-\npoint failure [22, pg 132]. Identifying potential single-point failures is essential\nfor determining problem areas in a system\u2019s reliability. Hazards with only single\nbarriers, and in particular single preventive barriers, represent a significant threat\nto system safety. In addition, identifying multiple barriers does not necessarily\nimply greater prevention or tolerance properties. Barrier interdependence will\ncompromise any diversity based arguments if combined dependability between\nbarriers results in single-point failure situations. A common preventive barrier\npair in the mammography example is the use of \u201cstaff training\u201d and \u201cgood\nprocedure following\u201d which are clearly interrelated.\nIn the mammography analysis 33 hazards are protected against by single\nbarriers, 14 hazards by double barriers, 12 hazards by triple barriers and 2\nhazards by quadruple barriers. Therefore 54% of the barriers in this analysis\nsuffer from potential single-point failures. Of the single-point failure barriers 5\nare protective barriers and 29 are preventive barriers. This reinforces the barrier\nbias demonstrated in Section 4.1. In this case the additional 2 protective barriers\nexamples are double barriers with, the same, one protective (\u201cbar codes on x-\nray\u201d) and one preventive (\u201cgood procedure following\u201d) barrier each. In this case\nindependence can be observed informally between a technology based barrier\nand a human oriented barrier. There is a need to determine such independence\nif accurate predictions of barrier performance are to be generated.\n5 Airspace route feasibility example\nEurocontrol\u2019s European Air Traffic Management Programme requires a safety\nassessment to be performed for \u201call new systems and changes to existing sys-\ntems.\u201d[5]. Therefore a safety assessment was commissioned for the eight-states3\nfree route airspace concept. The overriding aim of the concept was to obtain\nbenefits in terms of safety, capacity, flexibility and flight efficiency by removing\nthe constraints imposed by the fixed route structure and by optimising the use\nof more airspace [6, pg xiii]. The principal safety objective was to ensure that\nfree route airspace operations are at least as safe as the current fixed route oper-\nations. A functional hazard assessment was completed to determine how safe the\nvarious functions of the system need to be in order to satisfy the safety policy\nrequirements. This assessment investigated each function of the proposed system\nand identified ways in which it could fail (i.e. the hazards) [6, pg 10].\n3 Belgium, Denmark, Finland, Germany, Luxembourg, The Netherlands, Norway and\nSweden.\n8This hazard assessment has been examined in a similar manner to that de-\nscribed in Section 4 (see Table 4 in Appendix A for three example hazard as-\nsessment rows). Although the two cases are not directly comparable, examining\nthe explicit barriers present in the airspace route provides insight into the iden-\ntification of barriers as both a design tool and possible analysis metric. Analysis\nis based on the mitigations associated with the new hazards introduced by the\nimplementation of free route operations and ignores existing mitigations in the\nprevious system.\nThe functional hazard assessment contains 105 rows of which 69 contained\nnew hazards that required mitigation. Newly identified hazards are not mitigated\nby existing mitigating factors in the system. The output of the hazard assess-\nment was a set of safety requirements for the proposed free route environment.\nIn total 128 barriers can be identified in the safety requirements. For example\nassessment 210 in Table 4 of Appendix A contains four existing mitigating fac-\ntors and four proposed barriers described as safety requirements. Other implied\nbarriers in this case study include human oriented barriers such as \u201ccontroller\ntraining\u201d, environmental conditions, for example \u201cairspace design\u201d, and system\ncomponents, for example \u201cMTCD4 system usage\u201d. The following sections are\nindicative of the set of barrier properties and of their implications for safety.\n5.1 Preventive vs. protective barriers\nNo protective barriers and 128 preventive barriers were identified in the free\nroute airspace example. The majority consist of the enforcement or review of\ndifferent operating procedures. Other barriers include controller and pilot train-\ning and monitoring system technology. Twenty two different preventive barriers\ncan be identified as unique barrier forms. All of the barrier protection is based\non preventive barriers here, which has implications for the fault tolerance of the\nsystem.\n5.2 Barrier frequency and type\nTable 2 shows the eight most common barriers in the airspace analysis. The\ntwo barriers that appear most common in the hazard analysis are technological\nsystems and together contribute 39% of the barriers. In Table 2 technological\nsystems represent 48% of the total barriers and the human oriented barriers\nrepresent 24%. From a total of 22 unique barriers identified in the analysis,\nthose in Table 2 represent 84% of all the barriers in this hazard analysis.\n5.3 Barriers per hazard\nIn this analysis 28 hazards are protected against by single barriers, 31 hazards\nby double barriers, 10 hazards by triple barriers and 3 hazards by quadruple\nbarriers. Therefore 22% of the barriers in this analysis suffer from potential\n4 Medium Term Conflict Detection.\n9Table 2. Eight most common barriers in the airspace analysis\nBarrier Frequency Barrier Frequency\nMONA (MONitoring Aid) system 32 MTCD system 18\nController training 18 Free Route Airspace 15\ncontingency procedures\nAirspace design 8 Review procedures 8\nTransfer procedure 5 Area Proximity Warning 4\n(APW) system\nsingle-point failures. Although this is less than in the CADT for mammography\nexample it represents a considerably percentage of the barriers proposed in this\nassessment. As with the CADT analysis (see Section 4), each barrier was con-\nsidered independent and determining independence between barriers is outside\nthe scope of this paper.\n6 Explicit barriers for certification\nStorey [22] notes three typical aspects to the certification of safety-critical sys-\ntems:\n1. A demonstration that all important hazards have been identified and dealt\nwith, and that the integrity of the system is appropriate for the application.\n2. Evidence of compliance with some particular standard.\n3. A rigorous argument to support the claim that the system is sufficiently safe\nand will remain so throughout its life.\nExplicit barrier definition through the development phases of a safety-critical\nsystem form a traceable hazard mitigation link in the associated documentation.\nBarriers identified via hazard analysis will require representation in any design\nrationale and associated safety case used to assure system safety. In addition\nwhether hazard mitigations, as represented by abstract barriers in a design, are\nin fact present and functioning in a live system can be determined. Therefore\nthe explicit representation of barriers highlights the hazard mitigations that are\nin place and their continuing performance.\nThere is little information on final implementation and performance of the\ncase studies described in this paper. However, they can be examined in the\ncontext of the proposed designs. This allows designers to reflect on the identified\nbarriers and their influence on any future certification.\nUser training as a preventive barrier has played a considerable part in the\nmitigation of hazards in both the CADT for mammography and the free route\nairspace examples. Verification that appropriately qualified staff are part of the\nhuman-machine system would therefore be required. This may require the in-\ntroduction of additional barriers, such as qualification checking, confirmation of\naccreditation of training schemes and continuous assessment of actual perfor-\nmance.\n10\nThe majority of barriers in the free route airspace example were based on\nthe development and implementation of future products, for example, the review\nand definition of good operating procedures in particularly hazardous situations\nand the deployment of proposed traffic monitoring technology. It is likely that\nthese barriers would feature predominantly in any safety case based in part on\nthis hazard assessment. Verification of the existence of these procedures and their\nacceptance in the organisational structure of the domain would be required. Also\nthe barriers indicating the use of the new traffic monitoring technology (MONA)\nprovides a minimum level of functionality for the deployed system. Therefore the\nperformance between any predicted barrier behaviour, commonly presented as\nevidence as part of a safety case, and the actual barrier behaviour in the live\nsystem can provide a quantitative measure of barrier reliability for certification\npurposes.\n7 Conclusions\nBarriers are important for the understanding and prevention of accidents and are\nan intrinsic part of safety-critical systems. They feature implicitly throughout a\nsystem\u2019s development life-cycle. In additional to having physical presense in a\nlive system, they provide a representation for safety concerns in hazard analysis,\ndesign decisions, safety case construction and certification.\nIn this paper several views on the explicit representation of barriers have\nbeen presented. These aid the understanding of hazards as represented in the\nanalysis of safety-critical systems. Reflecting on the choice and nature of barriers\nis an essential part of constructing more dependable systems. Two case studies\nhave been examined and the implication of barriers in the context of a hazard\nanalysis have been defined. The process of hazard mitigation in a design can be\ndocumented by considering barriers explicitly. In addition, this process provides\na framework for a quantitative measure of barriers as part of the certification\nprocess.\nAnalysing and defining barrier descriptions is a time consuming process which\nwould be aided considerably by a barrier notation and tool support. The authors\nare currently investigating the use of the Hazard-Barrier-Target model [18] and\nthe Safety Modelling Language [19] as the next step to incorporating explicit\nbarriers in safety-critical system development. This is ongoing work.\n8 Acknowledgements\nThis work was supported in part by the UK EPSRC DIRC project [4], GR\/N13999\nand by the ADVISES research training network, GR\/N 006R02527.\nReferences\n1. Stephen Barker, Ian Kendall, and Anthony Darlison. Safety cases for software-\nintensive systems: an industrial experience report. In Peter Daniel, editor, 16th\n11\nInternational Conference on Computer Safety, Reliability and Security (SAFE-\nCOMP 97), pages 332\u2013342. Springer, 1997.\n2. Caroline R. M. Boggis and Susan M. Astley. Computer-assisted mammographic\nimaging. Breast Cancer Research, 2(6):392\u2013395, 2000.\n3. B. S. Dhillon. Failure modes and effects analysis - bibliography. Microelectronics\nand Reliability, 32(5):719\u2013731, 1992.\n4. DIRC - Interdisciplinary Research Collaboration on Dependability of Computer-\nBased Systems, http:\/\/www.dirc.org.uk [last access 6\/06\/2003], 2003.\n5. European air traffic management programme safety policy, November 1995.\nSAF.ET1.ST01.1000-POL-01-00, Edition 1.0.\n6. Eurocontrol. Safety assessment of the free route airspace concept: Feasibility phase.\nWorking Draft 0.3, European Organisation for the Safety of Air Navigation, Oc-\ntober 2001. 8-States Free Route Airspace Project.\n7. Lars Harms-Ringdahl. Investigation of barriers and safety functions related to\naccidents. In Proceedings of the European Safety and Reliability Conference ESREL\n2003, Maastricht, The Netherlands, 2003.\n8. Mark Hartswood and Rob Proctor. Computer-aided mammography: A case study\nof error management in a skilled decision-making task. In Chris Johnson, ed-\nitor, Proceedings of the first workshop on Human Error and Clinical Systems\n(HECS\u201999). University of Glasgow, April 1999. Glasgow Accident Analysis Group\nTechnical Report G99-1.\n9. Erik Hollnagel. Accidents and barriers. In J-M Hoc, P Millot, E Hollnagel, and\nP. C. Cacciabue, editors, Proceedings of Lex Valenciennes, volume 28, pages 175\u2013\n182. Presses Universitaires de Valenciennes, 1999.\n10. C. W. Johnson. Failure in Safety-Critical Systems: A Handbook of Accident and\nIncient Reporting. University of Glasgow Press: Glasgow, Scotland, October 2003.\nISBN 0-85261-784-4.\n11. T. P. Kelly, I. J. Bate, J. A. McDermid, and A. Burns. Building a preliminary\nsafety case: An example from aerospace. In 1997 Australian Workshop of Industrial\nExperience with Safety Critical Systems, Sydney, Australia, 1997. ACS.\n12. Trevor Kletz. Hazop and Hazan: Identifying and Assessing Process Industrial Haz-\nards. Institution of Chemical Engineers, third edition, 1992. ISBN 0-85295-285-6.\n13. Nancy G. Leveson. Safeware: System Safety and Computers. Addison Wesley,\n1995.\n14. P. Neogy, A. L. Hanson, P. R. Davis, and T. E. Fenstermacher. Hazard and barrier\nanalysis guidance document. Technical Report EH-33, Department of Engery,\nOffice of Operating Experience Analysis and Feedback, USA, November 1996. Rev\n0.\n15. Steven Pocock, Michael Harrison, Peter Wright, and Paul Johnson. THEA - a\ntechnique for human error assessment early in design. In Michitaka Hirose, editor,\nHuman-Computer Interaction: INTERACT\u201901, pages 247\u2013254. IOS Press, 2001.\n16. David. J. Pumfrey. The Principled Design of Computer System Safety Analysis.\nPhD thesis, Department of Computer Science, The University of York, 2000.\n17. James Reason. Human Error. Cambridge University Press, Cambridge, 1990.\n18. Bastiaan A. Schupp, Saul M. Lemkowitz, and Hans J. Pasman. Application of\nthe Hazard-Barrier-Target (HBT) model for more effective design for safety in a\ncomputer-based technology management environment. In CCPS ICW: Making\nProcess Safety Pay: The Business Case, pages 287\u2013316. AIChE\/CCPS, 2001.\n19. Bastiaan A. Schupp, Shamus P. Smith, Peter C. Wright, and Louis H. J. Goossens.\nIntegrating human factors in the design of safety critical systems: A barrier based\n12\napproach. In Proceedings of IFIP 13.5 Working Conference on Human Error,\nSafety and Systems Development (HESSD 2004). Forthcoming, 2004.\n20. Shamus P. Smith and Michael D. Harrison. Improving hazard classification through\nthe reuse of descriptive arguments. In Cristina Gacek, editor, Software Reuse:\nMethods, Techniques, and Tools (ICSR-7), volume 2319 of Lecture Notes in Com-\nputer Science (LNCS), pages 255\u2013268, Berlin, 2002. Springer.\n21. Shamus P. Smith and Michael D. Harrison. Reuse in hazard analysis: Identification\nand support. In Stuart Anderson, Massimo Felici, and Bev Littlewood, editors,\nComputer Safety, Reliability and Security (SAFECOMP 2003), volume 2788 of\nLecture Notes in Computer Science (LNCS), pages 382\u2013395, Berlin, 2003. Springer.\n22. Neil Storey. Safety-Critical Computer Systems. Addison-Wesley, 1996.\n23. L. Strigini, A. Povyakalo, and E. Alberdi. Human-machine diversity in the use of\ncomputerised advisory systems: a case study. In IEEE International Conference\non Dependable Systems and Networks (DSN 2003), pages 249\u2013258. IEEE, 2003.\nSan Francisco, U.S.A.\n24. Bin Zheng, Ratan Shah, Luisa Wallance, Christiane Hakim, Marie A. Ganott,\nand David Gur. Computer-aided detection in mammography: An assessment of\nperformance on current and prior images. Academic Radiology, 9(11):1245\u20131250,\nNovember 2002. AUR.\n1\n3\nA Raw hazard analysis fragments\nTable 3. Fragment of HAZOP for the CADT for mammography design\nRef Item Guideword Cause Consequence\/Implication Indication\/Protection\n1.1.1a Make initial decision Wrong Radiologist inexperience Wrong detection result Training\n...\n1.1.1.1g Examine x-ray Repeat X-rays out of order Mixed up detection and Barcoding on x-ray and\npatient record patient record. Srict procedure\n...\n1.2a Process digital x-ray Omit System failure No CADT image. CADT reliability\nReliance on human decision\n...\n1.3.3a Record decision Omit Operator lapse Loss of records Interlock to force form\ncompletion\nTable 4. Fragment of safety assessment for the free route airspace concept\nTask Function ID Failure Operational Existing mitigating Proposed Free Route\nCondition Consequences factors safety requirement\nHandling Conflict 210 Controller fails Potential Controller training. Pilot MTCDa.\naircraft identification to identify conflict collision risk awareness of other Controller training.\ntraffic. STCAb, TCASc Airspace design.\nProcedure review.\nHandling Conflict 211 Controller unable to Potential Controller training. Pilot MTCD. Airspace design.\naircraft identification make timely collision risk awareness of other traffic Controller training.\nidentification of conflict STCA, TCAS. Transfer procedures\nHandling Conflict 212 Controller mistakenly Extra workload Controller training. Traffic MTCD.\naircraft identification identifies conflict monitoring Controller training.\nwhen none existed\na Medium Term Conflict Detection system.\nb Short Term Conflict Alert system.\nc Traffic Alert Collision Avoidance System.\n"}