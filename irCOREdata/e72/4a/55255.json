{"doi":"10.1243\/09544070JAUTO1213","coreId":"55255","oai":"oai:eprints.lincoln.ac.uk:2182","identifiers":["oai:eprints.lincoln.ac.uk:2182","10.1243\/09544070JAUTO1213"],"title":"Improved decision support for engine-in-the-loop experimental design optimization","authors":["Gladwin, D.","Stewart, P.","Stewart, J.","Chen, R.","Winward, E."],"enrichments":{"references":[{"id":18445415,"title":"Real time simulation and control systems design by the response surface methodology and designed experiments.","authors":[],"date":"2003","doi":"10.1080\/00207720310001640287","raw":"Stewart, P., Fleming, P. J., and MacKenzie, S. A. Real time simulation and control systems design by the response surface methodology and designed experiments. Int. J. Systems Sci., 2003, 34(14\u201315),","cites":null},{"id":18445416,"title":"Design of robust fuzzy-logic control systems by multiobjective evolutionary methods with hardware in the loop.","authors":[],"date":"2004","doi":"10.1016\/j.engappai.2004.03.003","raw":"Stewart, P., Stone, D. A., and Fleming, P. J. Design of robust fuzzy-logic control systems by multiobjective evolutionary methods with hardware in the loop. IFAC J. Engng Applic. Artif. Intell., 2004,","cites":null},{"id":18445417,"title":"Response surface methodology: process and product optimization using designed experiments,","authors":[],"date":"1995","doi":"10.2307\/1270613","raw":"Myers, R. H. and Montgomery, D. C. Response surface methodology: process and product optimization using designed experiments, 1995 (John Wiley, New York).","cites":null},{"id":18445418,"title":"How to solve it, algorithms for engineering systems,","authors":[],"date":"2006","doi":"10.1007\/978-3-662-04131-4_14","raw":"Michalewicz, Z. and Fogel, D. B. How to solve it, algorithms for engineering systems, 2006 (Cambridge University Press, Cambridge).","cites":null},{"id":18445419,"title":"Design optimization of composites using genetic algorithms and failure mechanism based failure criterion. Composite Structs,","authors":[],"date":"2008","doi":"10.1016\/j.compstruct.2007.05.005","raw":"Narayana Naik, G., Gopalakrishnan, S., and Ganguli, R. Design optimization of composites using genetic algorithms and failure mechanism based failure criterion. Composite Structs, 2008, 83(4),","cites":null},{"id":18445420,"title":"Engine-in-the-loop experimental design optimization 217 JAUTO1213","authors":[],"date":"1997","doi":null,"raw":"\u2013367. Engine-in-the-loop experimental design optimization 217 JAUTO1213 Proc. IMechE Vol. 224 Part D: J. Automobile Engineering6 Mladenovic, M. and Hansen, P. Variable neighbourhood search. J. Comput. Ops Res., 1997, 24(11),","cites":null},{"id":18445421,"title":"Comparison of metaheuristic algorithms for clustering rectangles.","authors":[],"date":"1990","doi":"10.1016\/s0360-8352(99)00099-6","raw":"Burke, E. and Kendall, G. Comparison of metaheuristic algorithms for clustering rectangles. J. Comput. Ind. Engng, 1990, 37(1), 383\u2013386.","cites":null},{"id":18445423,"title":"Tabu search and finite convergence.","authors":[],"date":"2002","doi":"10.1016\/s0166-218x(01)00263-3","raw":"Glover, F. and Hanfi, S. Tabu search and finite convergence. Appl. Math., 2002, 119(1\u20132), 3\u201336.","cites":null},{"id":18445424,"title":"Solving the unconstrained optimisation problem by a variable neighbourhood search.","authors":[],"date":null,"doi":"10.1016\/j.jmaa.2006.06.025","raw":"Toksari, M. D. and Guner, E. Solving the unconstrained optimisation problem by a variable neighbourhood search. J. Math. Analysis and Applic.,","cites":null},{"id":18445425,"title":"Applied optimization: formulation and algorithms for engineering systems,","authors":[],"date":"2006","doi":"10.1017\/cbo9780511610868.006","raw":"Baldick, R. Applied optimization: formulation and algorithms for engineering systems, 2006 (Cambridge University Press, Cambridge, UK).","cites":null},{"id":18445426,"title":"Optimization by simulated annealing.","authors":[],"date":"1983","doi":"10.1126\/science.220.4598.671","raw":"Kirkpatrick, S., Gelatt, C. D., and Vecchi, M. P. Optimization by simulated annealing. Science, New Ser., 1983, 220(4598), 671\u2013680.","cites":null},{"id":18445428,"title":"Search methodologies: introductory tutorials in optimisation and decision support techniques,","authors":[],"date":"2005","doi":null,"raw":"Burke, E. and Kendall, G. Search methodologies: introductory tutorials in optimisation and decision support techniques, 2005 (Springer, New York).","cites":null},{"id":18445431,"title":"The response surface methodology for rapid prototyping of real-time control systems.","authors":[],"date":null,"doi":"10.1109\/acc.2002.1025308","raw":"Stewart, P. and Fleming, P. J. The response surface methodology for rapid prototyping of real-time control systems. In Proceedings of the 2002 American Control Conference, Anchorage, Alaska, USA,","cites":null},{"id":18445435,"title":"Experiences with optimizers in structural design.","authors":[],"date":"1994","doi":null,"raw":"Keane, A. J. Experiences with optimizers in structural design. In Proceedings of the First International Conference on Adaptive computing in engineering design and control (Ed. I. C. Parmee), Plymouth, Devon, UK, 21\u201322 September 1994, pp.","cites":null},{"id":18445436,"title":"Genetic algorithm optimization of multi-peak problems: studies in convergence and robustness.","authors":[],"date":"1995","doi":"10.1016\/0954-1810(95)95751-q","raw":"Keane, A. J. Genetic algorithm optimization of multi-peak problems: studies in convergence and robustness. Artif. Intell. Engng, 1995, 9(2), 75\u201383.","cites":null},{"id":18445437,"title":"Quasi-constant volume (QCV) spark ignition combustion.","authors":[],"date":"2009","doi":"10.4271\/2009-01-0700","raw":"Chen, R. Quasi-constant volume (QCV) spark ignition combustion. SAE paper 2009-01-0700, 2009.","cites":null},{"id":18445438,"title":"Introduction to internal combustion engines, 3rd edition,","authors":[],"date":"1999","doi":null,"raw":"Stone, R. Introduction to internal combustion engines, 3rd edition, 1999 (Macmillan, London).","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-02","abstract":"Experimental optimization with hardware in the loop is a common procedure in engineering and has been the subject of intense development, particularly when it is applied to relatively complex combinatorial systems that are not completely understood, or where accurate modelling is not possible owing to the dimensions of the search space. A common source of difficulty arises because of the level of noise associated with experimental measurements, a combination of limited instrument precision, and extraneous factors. When a series of experiments is conducted to search for a combination of input parameters that results in a minimum or maximum response, under the imposition of noise, the underlying shape of the function being optimized can become very difficult to discern or even lost. A common methodology to support experimental search for optimal or suboptimal values is to use one of the many gradient descent methods. However, even sophisticated and proven methodologies, such as simulated annealing, can be significantly challenged in the presence of noise, since approximating the gradient at any point becomes highly unreliable. Often, experiments are accepted as a result of random noise which should be rejected, and vice versa. This is also true for other sampling techniques, including tabu and evolutionary algorithms.\\ud\nAfter the general introduction, this paper is divided into two main sections (sections 2 and 3), which are followed by the conclusion. Section 2 introduces a decision support methodology based upon response surfaces, which supplements experimental management based on a variable neighbourhood search and is shown to be highly effective in directing experiments in the presence of a significant signal-to-noise ratio and complex combinatorial functions. The methodology is developed on a three-dimensional surface with multiple local minima, a large basin of attraction, and a high signal-to-noise ratio.\\ud\nIn section 2, the methodology is applied to an automotive combinatorial search in the laboratory, on a real-time engine-in-the-loop application. In this application, it is desired to find the maximum power output of an experimental single-cylinder spark ignition engine operating under a quasi-constant-volume operating regime. Under this regime, the piston is slowed at top dead centre to achieve combustion in close to constant volume conditions.\\ud\nAs part of the further development of the engine to incorporate a linear generator to investigate free-piston operation, it is necessary to perform a series of experiments with combinatorial parameters. The objective is to identify the maximum power point in the least number of experiments in order to minimize costs. This test programme provides peak power data in order to achieve optimal electrical machine design.\\ud\nThe decision support methodology is combined with standard optimization and search methods \u2013 namely gradient descent and simulated annealing \u2013 in order to study the reductions possible in experimental iterations. It is shown that the decision support methodology significantly reduces the number of experiments necessary to find the maximum power solution and thus offers a potentially significant cost saving to hardware-in-the-loop experi- mentation","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/55255.pdf","fullTextIdentifier":"http:\/\/eprints.lincoln.ac.uk\/2182\/1\/Gladwin.pdf","pdfHashValue":"947a0cdeb5bb31a75bef7fb1fde3d9a8716364c4","publisher":"Sage \/ IMechE","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lincoln.ac.uk:2182<\/identifier><datestamp>\n      2014-07-17T11:11:19Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47313530<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F48:6A6163735F48333330<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F48:6A6163735F48313330<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lincoln.ac.uk\/2182\/<\/dc:relation><dc:title>\n        Improved decision support for engine-in-the-loop experimental design optimization<\/dc:title><dc:creator>\n        Gladwin, D.<\/dc:creator><dc:creator>\n        Stewart, P.<\/dc:creator><dc:creator>\n        Stewart, J.<\/dc:creator><dc:creator>\n        Chen, R.<\/dc:creator><dc:creator>\n        Winward, E.<\/dc:creator><dc:subject>\n        G150 Mathematical Modelling<\/dc:subject><dc:subject>\n        H330 Automotive Engineering<\/dc:subject><dc:subject>\n        H130 Computer-Aided Engineering<\/dc:subject><dc:description>\n        Experimental optimization with hardware in the loop is a common procedure in engineering and has been the subject of intense development, particularly when it is applied to relatively complex combinatorial systems that are not completely understood, or where accurate modelling is not possible owing to the dimensions of the search space. A common source of difficulty arises because of the level of noise associated with experimental measurements, a combination of limited instrument precision, and extraneous factors. When a series of experiments is conducted to search for a combination of input parameters that results in a minimum or maximum response, under the imposition of noise, the underlying shape of the function being optimized can become very difficult to discern or even lost. A common methodology to support experimental search for optimal or suboptimal values is to use one of the many gradient descent methods. However, even sophisticated and proven methodologies, such as simulated annealing, can be significantly challenged in the presence of noise, since approximating the gradient at any point becomes highly unreliable. Often, experiments are accepted as a result of random noise which should be rejected, and vice versa. This is also true for other sampling techniques, including tabu and evolutionary algorithms.\\ud\nAfter the general introduction, this paper is divided into two main sections (sections 2 and 3), which are followed by the conclusion. Section 2 introduces a decision support methodology based upon response surfaces, which supplements experimental management based on a variable neighbourhood search and is shown to be highly effective in directing experiments in the presence of a significant signal-to-noise ratio and complex combinatorial functions. The methodology is developed on a three-dimensional surface with multiple local minima, a large basin of attraction, and a high signal-to-noise ratio.\\ud\nIn section 2, the methodology is applied to an automotive combinatorial search in the laboratory, on a real-time engine-in-the-loop application. In this application, it is desired to find the maximum power output of an experimental single-cylinder spark ignition engine operating under a quasi-constant-volume operating regime. Under this regime, the piston is slowed at top dead centre to achieve combustion in close to constant volume conditions.\\ud\nAs part of the further development of the engine to incorporate a linear generator to investigate free-piston operation, it is necessary to perform a series of experiments with combinatorial parameters. The objective is to identify the maximum power point in the least number of experiments in order to minimize costs. This test programme provides peak power data in order to achieve optimal electrical machine design.\\ud\nThe decision support methodology is combined with standard optimization and search methods \u2013 namely gradient descent and simulated annealing \u2013 in order to study the reductions possible in experimental iterations. It is shown that the decision support methodology significantly reduces the number of experiments necessary to find the maximum power solution and thus offers a potentially significant cost saving to hardware-in-the-loop experi- mentation.<\/dc:description><dc:publisher>\n        Sage \/ IMechE<\/dc:publisher><dc:date>\n        2010-02<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lincoln.ac.uk\/2182\/1\/Gladwin.pdf<\/dc:identifier><dc:identifier>\n          Gladwin, D. and Stewart, P. and Stewart, J. and Chen, R. and Winward, E.  (2010) Improved decision support for engine-in-the-loop experimental design optimization.  Proceedings of the IMechE, Part D: Automobile Engineering, 224  (2).   pp. 201-218.  ISSN 0954-4070  <\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1243\/09544070JAUTO1213<\/dc:relation><dc:relation>\n        10.1243\/09544070JAUTO1213<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lincoln.ac.uk\/2182\/","http:\/\/dx.doi.org\/10.1243\/09544070JAUTO1213","10.1243\/09544070JAUTO1213"],"year":2010,"topics":["G150 Mathematical Modelling","H330 Automotive Engineering","H130 Computer-Aided Engineering"],"subject":["Article","PeerReviewed"],"fullText":"Improved decision support for engine-in-the-loop\nexperimental design optimization\nD Gladwin1, P Stewart2*, J Stewart2, R Chen3, and E Winward3\n1Department of Electronic and Electrical Engineering, University of Sheffield, Sheffield, UK\n2School of Engineering, University of Lincoln, Lincoln, UK\n3Department of Aeronautical and Automotive Engineering, Loughborough University, Loughborough, UK\nThe manuscript was received on 16 March 2009 and was accepted after revision for publication on 29 July 2009.\nDOI: 10.1243\/09544070JAUTO1213\nAbstract: Experimental optimization with hardware in the loop is a common procedure in\nengineering and has been the subject of intense development, particularly when it is applied to\nrelatively complex combinatorial systems that are not completely understood, or where\naccurate modelling is not possible owing to the dimensions of the search space. A common\nsource of difficulty arises because of the level of noise associated with experimental\nmeasurements, a combination of limited instrument precision, and extraneous factors. When\na series of experiments is conducted to search for a combination of input parameters that\nresults in a minimum or maximum response, under the imposition of noise, the underlying\nshape of the function being optimized can become very difficult to discern or even lost. A\ncommon methodology to support experimental search for optimal or suboptimal values is to\nuse one of the many gradient descent methods. However, even sophisticated and proven\nmethodologies, such as simulated annealing, can be significantly challenged in the presence\nof noise, since approximating the gradient at any point becomes highly unreliable. Often,\nexperiments are accepted as a result of random noise which should be rejected, and vice versa.\nThis is also true for other sampling techniques, including tabu and evolutionary algorithms.\nAfter the general introduction, this paper is divided into two main sections (sections 2 and 3),\nwhich are followed by the conclusion. Section 2 introduces a decision support methodology\nbased upon response surfaces, which supplements experimental management based on a\nvariable neighbourhood search and is shown to be highly effective in directing experiments in\nthe presence of a significant signal-to-noise ratio and complex combinatorial functions. The\nmethodology is developed on a three-dimensional surface with multiple local minima, a large\nbasin of attraction, and a high signal-to-noise ratio.\nIn section 2, the methodology is applied to an automotive combinatorial search in the\nlaboratory, on a real-time engine-in-the-loop application. In this application, it is desired to\nfind the maximum power output of an experimental single-cylinder spark ignition engine\noperating under a quasi-constant-volume operating regime. Under this regime, the piston is\nslowed at top dead centre to achieve combustion in close to constant volume conditions.\nAs part of the further development of the engine to incorporate a linear generator to\ninvestigate free-piston operation, it is necessary to perform a series of experiments with\ncombinatorial parameters. The objective is to identify the maximum power point in the least\nnumber of experiments in order to minimize costs. This test programme provides peak power\ndata in order to achieve optimal electrical machine design.\nThe decision support methodology is combined with standard optimization and search\nmethods \u2013 namely gradient descent and simulated annealing \u2013 in order to study the reductions\npossible in experimental iterations. It is shown that the decision support methodology\nsignificantly reduces the number of experiments necessary to find the maximum power\n*Corresponding author: School of Engineering, University of Lincoln, Brayford Pool, Lincoln, LN6 7TS, UK.\nemail: pstewart@lincoln.ac.uk\n201\nJAUTO1213 Proc. IMechE Vol. 224 Part D: J. Automobile Engineering\nsolution and thus offers a potentially significant cost saving to hardware-in-the-loop experi-\nmentation.\nKeywords: experimental decision support, variable neighbourhood search, gradient descent,\nsimulated annealing, hardware in the loop\n1 INTRODUCTION\nAn experimental search with hardware or process in\nthe loop is a common procedure in, for example, the\nengineering and pharmaceutical industries. These\nprocedures are often applied to combinatorial pro-\nblems during (in the engineering case, for example)\nthe development of new hardware systems or con-\ntrol [1, 2]. The systems under test can be described\nas a set of dependent variables that vary according\nto some functions of independent variables. In this\ncase, there does not exist a complete specification\nof the function relating the variables. This implies\nthat there is no accurate a priori knowledge of the\nfundamental cause and effect present in the system.\nThus, for an example linear function, the values in\nthe coefficients matrix would be unknown.\nCommonly, it is required to identify the sets of\nindependent variables that maximize or minimize\nthe dependent variables [3]. To obtain the necessary\ninformation to have a confident estimate of the\nparameters, it is possible to vary the independent\nparameters over successive trials (designed experi-\nments) and to measure the corresponding depen-\ndent variables. In order to examine this relationship\nfully, a large number of trials is often required to\nidentify the location of the desired response. How-\never, real-world problems are difficult to solve by\nthis methodology for a number of reasons [4].\n1. The number of possible solutions in the experi-\nmental space is so large as to preclude an exhau-\nstive search for the best (or acceptable) answer.\n2. The evaluation function that describes the solu-\ntions is extremely noisy and\/or complex.\n3. The cost of conducting an experiment at many\npoints in the search space may be prohibitive in\nterms of time taken and\/or resources used.\nThese constraints motivate the use of gradient\ndescent (GD) methods in order to provide the\ndecision support to direct the search and to mini-\nmize the number of experiments conducted. Other\nmetaheuristics, such as genetic algorithms [5], are\napplicable to this class of problem, but are relatively\ndifficult to implement and tune because of the\nnumber of parameters associated with this techni-\nque in comparison with GD methods. These meth-\nods are based upon the statistics of the neighbour-\nhood around a given point, thus relying on local\ninformation at each step. However, basic GD meth-\nods provide only locally optimum solutions whose\nvalues depend on selection of the starting point [6].\nThere have been many metaheuristic methods\ndeveloped to increase the efficiency of the experi-\nmental search, such as simulated annealing (SA)\n[7], tabu search [8], genetic algorithms [2], and vari-\nable neighbourhood search (VNS) [9].\nAs the nature of the experimental surface is an\nunknown, it is important to utilize methodologies\nthat require the minimum number of parameters to\nbe \u2018tuned\u2019 in order to conduct an effective search.\nWith this caveat in mind, simple GD, SA, and VNS\nwill be considered in this paper since, in most of\ntheir varieties, implementation is simple and basic\ntuning rules are readily available, making them\ncommonly used heuristics in the experimental\ncommunity.\nIt has been noted in the literature that the per-\nformance of metaheuristics such as SA are to some\nextent compromised when directing search over\nsignificantly noisy surfaces [4]. This paper describes\nthe implementation of a weighted stochastic deci-\nsion support (WSDS) operator based on response\nsurfaces (RSs) which supports the heuristic and\nguides the experimental process to predicted areas\nof interest in the search space. Basic GD, SA, and\nVNS are supplemented by this methodology, and\nperformance is compared with the basic form of the\nmetaheuristics. The supplemented metaheuristics\nare shown to have significantly improved perfor-\nmance when searching noisy environments.\n1.1 Scope of this paper\nThis paper is primarily concerned with the develop-\nment and assessment of a novel problem-solving\nmethodology for practical engineering applications,\nand hence it is helpful to define the scope of re-\nsearch presented here. This paper is primarily con-\ncerned with the following:\n(a) experimentation on real engineering problems;\n202 D Gladwin, P Stewart, J Stewart, R Chen, and E Winward\nProc. IMechE Vol. 224 Part D: J. Automobile Engineering JAUTO1213\n(b) problems that contain inherently noisy data and\nprocesses;\n(c) decision support to reduce experimentation\ntime;\n(d) applying a decision support operator to com-\nmon search methodologies.\nIt is important to note that the emphasis here is on\nthe reduction of experimentation time by decision\nsupport and hence does not consider the following:\n(a) performance comparison of heuristics andmeta-\nheuristics;\n(b) metaheuristic tuning methodologies;\n(c) \u2018toy\u2019 problems and surfaces.\nThus, the paper is concerned with the problem of\nfinding a result in an unseen noisy search space,\nwhere every individual evaluation of a point in the\nsearch space is expensive in terms of time and\/or\nresources.\n2 ALGORITHM DEVELOPMENT STUDY\n2.1 Search methodologies\nA comprehensive description of GD-based methods\ncan be found in reference [4]. In this section, the\nimplementations of the algorithms are described.\nThe class of problems addressed in this paper are in\ngeneral of a minimization type [10, 11]; i.e. it is\ndesired to minimize a function f(x) over choices of x\nthat lie in the feasible set S such that\nf \u0001~min\nx [ s\nf x\u00f0 \u00de \u00f01\u00de\nThus, \u2019x* [ S, such that f *5 f(x*) and x [ S) f(x*)(\nf(x). Set S is the constraint set, f * is the minimum of\nthe problem, while x* is the minimizer. Minimiza-\ntion problems are considered in this paper; however,\nconversion of the method to maximization is a trivial\ntask.\n2.1.1 Gradient descent\nA sequence of intermediate values are successively\ngenerated by the algorithm. First, an initial random\nguess is made and then it is successively improved.\nIn general, none of the iterates exactly solves the\nproblem; therefore, a termination criterion is in-\ncluded that, when satisfied, will cause the algorithm\nto terminate with a suitable approximation to the\nexact solution. This is particularly applicable to real-\nworld noisy surfaces. Iterative hill descent can be\ndescribed with the general form of recursion\nxnz1~xnzan Dxn, n~0, 1, 2, . . . \u00f02\u00de\nwhere\nx05 initial guess\nn5 iteration counter\nxn5 value of the iterate at the nth iteration\nan [R+5 step size\nDxn [Rn5 step direction\nan Dxn5update to add to the current iterate xn to\nobtain new iterate xn + 1\nIn the case of minimization, the step direction is\nchosen so as to reduce the objective f *. If x\u02c6 [R, then\nthe vector Dx [R is called a descent direction for f at\nx\u02c6 if \u2019a\u00afR++ such that\n0va\u00a1\u0001a[f x^za Dx\u00f0 \u00devf x^\u00f0 \u00de \u00f03\u00de\nand Dx is a descent direction for f at x\u02c6 if the objective\nis smaller than f(x\u02c6) at points along the line segment\nx\u02c6 + a Dx for a. 0 and a( a\u00af. There are some caveats\nassociated with GD methods.\n1. The methods usually terminate at solutions which\nare only locally optimal.\n2. No information is apparent as to how the dis-\ncovered local optima deviates from the global\nminima or other local minima.\n3. The optimum obtained depends on the original\nconfiguration.\n4. In general it is not possible to calculate an upper\nbound for computation time.\nGD thus exploits the best opportunities for impro-\nvements, but neglects to explore a large search space.\nIn contrast, random search where points are sam-\npled from S with equal probability explores thorou-\nghly, but forgoes local exploitation. Thus, most GD\nmethods execute a random \u2018jump\u2019 at local minima,\nto balance exploration with exploitation.\n2.1.2 Variable neighbourhood search\nThe VNS algorithm implemented in this paper\nsystematically exploits the idea of neighbourhood\nchange in the descent to minima [17], and attempts\nto balance local exploitation with global exploration.\nIt is simply an implementation of the basic GD\nmethod described in the previous section; however,\nin this case the step length an is variable rather than\nfixed. A number of variations have been reported,\nwith both lengthening step length [6] and reducing\nstep length [9]. In this case, reducing the step length\nEngine-in-the-loop experimental design optimization 203\nJAUTO1213 Proc. IMechE Vol. 224 Part D: J. Automobile Engineering\nis implemented by a static schedule [13] using a step\ndecrementation function given by\nCkz1~ack, k~0, 1, 2, . . . \u00f04\u00de\nThe initial step length is usually chosen to be sig-\nnificant with respect to the search variable ranges,\nwhere a is chosen to be a positive constant greater\nthan 1. The final value is fixed, generally related to\nthe smallest feasible measurement or control incre-\nment of the variables. As with GD, a random \u2018jump\u2019\nis implemented to escape local minima.\n2.1.3 Simulated annealing\nThe implementation of SA used in this paper is\nbased again on GD, accepting improvements in cost\nin traversing the search space; however, depending\non a control parameter c, it will accept deteriorations\nto a limited extent to escape local minima. Initially,\nat large values of c, large deteriorations will be\naccepted; as c decreases, smaller deteriorations are\naccepted; finally, as the value of c approaches 0,\nno deteriorations are accepted. The probability of\naccepting deteriorations is achieved by comparing\nthe value of exp {[f(i)2 f(j)]\/c} with a random num-\nber generated from a uniform distribution in the\ninterval [0, 1]. In this case, the rate in decrease in the\nparameter c is achieved by implementing the VNS\ndecrementation function.\n2.2 Weighted stochastic decision support\noperator\nLocal search methods, such as GD, execute a random\njump at local optima or other predefined termina-\ntion metric based upon the implementation. SA-type\nmethodologies typically execute a random jump at\nthe termination of the cooling schedule if the global\nminimum or some upper bound of acceptable per-\nformance has not been reached. Obviously, with\nunknown experimental functions, the exact value of\nthe global maximum will not be known; however, it\nis common for the designer or experimenter to have\nan \u2018acceptable\u2019 performance metric in mind when\nstarting the experimental procedure that will act as\na termination criterion.\nTabu search has been shown to be a particularly\neffective metaheuristic by directing the experimen-\ntal search \u2018jumps\u2019 away from areas that have been\nfound to be unproductive. However, this does not\ntake advantage of the previously gathered data with\nrespect to the possibility of \u2018predicting\u2019 promis-\ning areas of search. The RS methodology has been\nshown to be an effective tool in approximating com-\nplex and noisy functions for real-time control [1, 14]\nand thus would appear to be a useful tool for direct\nexperimentation based upon past results.\nThe RS methodology is a technique that was\ninitially developed to optimize process control and\nexperimentation by the application of designed ex-\nperiments in order to characterize the relationship\nbetween the system variables and outputs [3]. The\nrelationship between the response variable of inter-\nest, y, and the predictor variables (j1, j2,\u2026, jk)\nprovides a description of the system of the form\ny~g j1, j2, . . . , jk\u00f0 \u00deze \u00f05\u00de\nwhere e represents the model error and includes\nmeasurement error and other variables such as back-\nground noise. The error will be assumed to have\na normal distribution with zero mean and variance\ns2. In general, the experimenter approximates the\nsystem function g with an empirical model of the\nform\ny~f j1, j2, . . . , jk\u00f0 \u00deze \u00f06\u00de\nwhere f is a polynomial of arbitrary order (generally,\nfirst or second order in the process control industry).\nThis is the empirical or RS model. The variables are\nknown as natural variables since they are expressed\nin physical units of measurement. The natural\nvariables are transformed into coded variables x1,\nx2, \u2026, xk, which are dimensionless, zero mean, and\nthe same standard deviation, in order to minimize\nthe effects of outliers, sparse, or unevenly distributed\ndata, which are all likely in practical experimental\napplications. This approach is standard practice in\nthe literature and in industry [3]. The response\nfunction now becomes\ng~f x1, x2, . . . , xk\u00f0 \u00de \u00f07\u00de\nThe successful application of RSs relies on the\nidentification of a suitable approximation for f. This\nwill often be a first-order model of the form\ng~b0zb1x1zb2x2z \u0002 \u0002 \u0002zbkxk \u00f08\u00de\nor a second-order model of the form\ng~b0z\nXk\nj~1\nbjxjz\nXk\nj~1\nbjjx\n2\njz\nX\nivj\nX\nbijxixj \u00f09\u00de\nIt may also be necessary to employ an approximat-\n204 D Gladwin, P Stewart, J Stewart, R Chen, and E Winward\nProc. IMechE Vol. 224 Part D: J. Automobile Engineering JAUTO1213\ning function greater than an order of two, based on\nthe standard Taylor series expansion. The set of\nparameters can be estimated by regression analysis\nbased upon the experimental data. The method of\nleast squares is typically used to estimate the\nregression coefficients. With n, k on the response\nvariable available, giving y1, y2, \u2026 yn, each observed\nresponse will have an observation on each regression\nvariable, with xij denoting the ith observation of\nvariable xj. Assuming that the error term e has\nE(e)5 0 and Var(e)5 s2 and that the ei are uncorre-\nlated random variables, the model can now be\nexpressed in terms of the observations\nyi~b0zb1xi1zb2xi2z \u0002 \u0002 \u0002zbkxikzei, i~1, 2, . . . ,n\n\u00f010\u00de\nThe coefficients b in equation (10) are chosen such\nthat the sum of the squares of the errors ei are\nminimized via the least-squares function\nL~\nXn\ni~1\ne2i~\nXn\ni~1\nyi{b0{\nXn\nj~1\nbjxij\n !2\n\u00f011\u00de\nThe model can be more usefully expressed in matrix\nform as\ny~Xbze \u00f012\u00de\nwhere\ny~\ny1\ny2\n:\n:\n:\nyn\n2\n666666666664\n3\n777777777775\n, X~\n1 x11 x12 . . . x1k\n1 x21 x22 . . . x1k\n: : : : :\n: : : : :\n: : : : :\n1 xn1 xn2 . . . xnk\n2\n666666666664\n3\n777777777775\n,\nb~\nb0\nb1\n:\n:\n:\nbn\n2\n666666666664\n3\n777777777775\n, e~\ne1\ne2\n:\n:\n:\nen\n2\n666666666664\n3\n777777777775\n\u00f013\u00de\nIt is now necessary to find a vector b of least-squares\nestimators which minimizes the expression\nL~\nXn\ni~1\ne2i~e\u2019e~ y{Xb\u00f0 \u00de\u2019 y{Xb\u00f0 \u00de \u00f014\u00de\nand yields the least-squares estimator of b which is\nb~ X\u2019X\u00f0 \u00de{1X\u2019y \u00f015\u00de\nand, finally, the fitted regression model is\ny^~Xb, e~y{y^ \u00f016\u00de\nwhere e is the vector of residual errors of the model.\nThe RS method can thus be implemented upon\neither simulated or actual experimental results to\nderive a polynomial expression describing the rela-\ntionship between the causal inputs and resulting\noutputs of the dynamic systems under considera-\ntion.\nAs data from the experimental results are gathered\nunder the direction of the metaheuristics, it is pos-\nsible to generate a surface approximation for the system\nunder consideration.\n2.2.1 WSDS method\nThe WSDS method in its basic form is encapsulated\nin the following pseudo-code:\n1. WHILE\n2. run meta-heuristic to global minimum (or accep-\ntable value)\n3. END\n4. ELSE\n5. add new path data to old path data\n6. fit normalized RS to old path data\n7. generate WSDS surface\n8. perform weighted jump\n9. END\nThe response surface can take any arbitrary order;\nfor descriptive purposes, a second-order support sur-\nface in two variables is described.\nIf the metaheuristic finds a global or acceptable\nminimum, then the procedure terminates. Other-\nwise, a random jump is made to escape the local\nminima. In this case, the most recent search data\n(traveldata) is added to a data history file (e1, e2,\nyresp), where e1 and e2 are the natural variables, and\nyresp is the response. Thus\ne1~ e1; traveldata :, 1\u00f0 \u00de\u00bd \u0003\ne2~ e2; traveldata :, 2\u00f0 \u00de\u00bd \u0003\nyresp~ yresp; traveldata :, 3\u00f0 \u00de\u00bd \u0003\n\u00f017\u00de\nThe natural variables e1 and e2 are transformed into\nEngine-in-the-loop experimental design optimization 205\nJAUTO1213 Proc. IMechE Vol. 224 Part D: J. Automobile Engineering\ndimensionless zero-mean coded variables x1 and x2.\nAs the surface is unknown, a priori knowledge of the\nminimum and maximum of the natural variables is\nnot possible. Hence, they vary dynamically as new\ndata arrives according to\nx1~e1{\nmax e1\u00f0 \u00dezmin e1\u00f0 \u00de\u00bd \u0003=2\nmax e1\u00f0 \u00de{min e1\u00f0 \u00de\u00bd \u0003=2\nx2~e2{\nmax e2\u00f0 \u00dezmin e2\u00f0 \u00de\u00bd \u0003=2\nmax e2\u00f0 \u00de{min e2\u00f0 \u00de\u00bd \u0003=2\n\u00f018\u00de\nFor a representative second-order support surface,\nthe variable matrix X is thus\nX~\nx0 x1 x2 x\n2\n1 x\n2\n2 x1x2\n. . . . . . . . . . . . . . . . . .\n\u0001 \u0002\n\u00f019\u00de\nThe variable matrix shown in equation (19) has a\ncross-coupling term x1x2 which is optional and is\ndependent on the surface being fitted. The coded\nestimated coefficients of the RS are calculated accor-\nding to equation (15), which are converted to natural\ncoefficients by inverting the process in equation (18).\nFor the second-order cross-coupled approximation\nunder examination, the estimated natural coeffi-\ncients are calculated from\nThe natural coefficients are now recombined into the\nRS polynomial\ny~b0zb1e1zb2e2zb3e\n2\n1zb4e\n2\n2zb5e1e2 \u00f021\u00de\nwhere [e1 e2] is a matrix of coordinate values for the\nsearch space, and y is the corresponding predicted\nresponse value array. Since the \u2018true\u2019 system re-\nsponse surface is unknown, this predicted array\nrepresents the current view of the likely response. It\nis this polynomial that forms the basis for the\nweighted jump from local minima. The y values are\nnormalized according to\nynorm~1{ y{\nmax y\u00f0 \u00dezmin y\u00f0 \u00de\u00bd \u0003=2\nmax y\u00f0 \u00de{min y\u00f0 \u00de\u00bd \u0003=2z1\n\u0003 \u0004\u0005\n2 \u00f022\u00de\nwhich yields an RS over the search space bounded\nfrom zero to one, where increasing value represents\nincreasing interest inferred from previous searches.\nThese monotonically increasing values correspond\nto coordinates in a probability space from which\nthe next jump coordinates are chosen according to\na random number generation, with probability of\nbeing chosen based on relative value in the support\nspace. Thus, the probability of selection of the next\njump point is based statistically on the results of\nprevious searches.\nb^0~b0zb1\n{max e1\u00f0 \u00dezmin e1\u00f0 \u00de\u00bd \u0003=2\nmax e1\u00f0 \u00de{min e1\u00f0 \u00de\u00bd \u0003=2 zb2\n{max e2\u00f0 \u00dezmin e2\u00f0 \u00de\u00bd \u0003=2\nmax e2\u00f0 \u00de{min e2\u00f0 \u00de\u00bd \u0003=2 zb3\nmax e1\u00f0 \u00dezmin e1\u00f0 \u00de\u00bd \u0003=2\nmax e1\u00f0 \u00de{min e1\u00f0 \u00de\u00bd \u0003=2\n\u0003 \u00042\nzb4\nmax e2\u00f0 \u00dezmin e2\u00f0 \u00de\u00bd \u0003=2\nmax e2\u00f0 \u00de{min e2\u00f0 \u00de\u00bd \u0003=2\n\u0003 \u00042\nzb5\nmax e1\u00f0 \u00dezmin e1\u00f0 \u00de\u00bd \u0003=2f g max e2\u00f0 \u00dezmin e2\u00f0 \u00de\u00bd \u0003=2f g\nmax e1\u00f0 \u00de{min e1\u00f0 \u00de\u00bd \u0003=2f g max e2\u00f0 \u00de{min e2\u00f0 \u00de\u00bd \u0003=2f g\nb^1~\nb1\nmax e1\u00f0 \u00de{min e1\u00f0 \u00de\u00bd \u0003=2{2b3\nmax e1\u00f0 \u00dezmin e1\u00f0 \u00de\u00bd \u0003=2\nmax e1\u00f0 \u00de{min e1\u00f0 \u00de\u00bd \u0003=2f g2\n{b5\nmax e2\u00f0 \u00dezmin e2\u00f0 \u00de\u00bd \u0003=2\nmax e1\u00f0 \u00de{min e1\u00f0 \u00de\u00bd \u0003=2f g max e2\u00f0 \u00de{min e2\u00f0 \u00de\u00bd \u0003=2f g\nb^2~\nb2\nmax e2\u00f0 \u00de{min e2\u00f0 \u00de\u00bd \u0003=2{2b4\nmax e2\u00f0 \u00dezmin e2\u00f0 \u00de\u00bd \u0003=2\nmax e2\u00f0 \u00de{min e2\u00f0 \u00de\u00bd \u0003=2f g2\n{b5\nmax e1\u00f0 \u00dezmin e1\u00f0 \u00de\u00f0 \u00de=2\nmax e1\u00f0 \u00de{min e1\u00f0 \u00de\u00bd \u0003=2f g max e2\u00f0 \u00de{min e2\u00f0 \u00de\u00bd \u0003=2f g\nb^3~\nb3\nmax e1\u00f0 \u00de{min e1\u00f0 \u00de\u00bd \u0003=2f g2\nb^4~\nb4\nmax e2\u00f0 \u00de{min e2\u00f0 \u00de\u00bd \u0003=2f g2\nb^5~\nb5\nmax e1\u00f0 \u00de{min e1\u00f0 \u00de\u00bd \u0003=2f g max e2\u00f0 \u00de{min e2\u00f0 \u00de\u00bd \u0003=2f g\n\u00f020\u00de\n206 D Gladwin, P Stewart, J Stewart, R Chen, and E Winward\nProc. IMechE Vol. 224 Part D: J. Automobile Engineering JAUTO1213\n2.2.2 Development surfaces\nFor the development of this methodology, a realistic\nnoisy surface with multiple local minima, plateaux,\nand one global minimum was considered for algo-\nrithm development. The standard MATLAB \u2018peaks\u2019\nsurface (Fig. 1) describes a combinatorial process in\ntwo variables as\ny~3 1{x1\u00f0 \u00de2 exp {x21{x22\n\u0006 \u0007\n{10\nx1\n5\n{x31{x\n5\n2\n\b \t\nexp {x21{x\n2\n2\n\u0006 \u0007\n{\n1\n3\nexp { x1z1\u00f0 \u00de2{x22\nh i\n\u00f023\u00de\nIn order to investigate the effects of noise, progres-\nsively larger amounts of Gaussian noise are added to\nthe smooth surface (peaks 0) to give peaks 1 (Fig. 2),\npeaks 2 (Fig. 3), and peaks 3 (Fig. 4).\nFor the search heuristics presented here, perfor-\nmance is degraded by the number of local optima in\nthe search space. Local optima are formed by two\nmechanisms. The first mechanism is the underlying\n\u2018shape\u2019 of the search space. Basically, higher order\nfunctions tend to create more complex shapes with\nmore local minima. Measurement or process noise\nadds numerous local minima to the underlying sur-\nface. Hence, in the paper, the noise added to the\nfundamental search space peaks 0 is defined by a\nstatistical mean and variance in the text. As ex-\npected, performance degrades with increasing noise.\nAn example of a custom-designed surface, namely\n\u2018bumps\u2019, is also considered. This surface is standard\nin the literature and presents complexity in terms of\nthe number of local minima and the proportion of\nthe area of local minima to global minima.\nThe magnitude of the noise is given as a fraction of\nthe range of values of this input array. The addition\nof the noise is achieved by utilizing the R function\nFig. 1 Noise-free algorithm development fitness land-\nscape: peaks 0\nFig. 2 Noisy algorithm development fitness landscape:\npeaks 1\nFig. 3 Noisy algorithm development fitness landscape:\npeaks 2\nFig. 4 Noisy algorithm development fitness landscape:\npeaks 3\nEngine-in-the-loop experimental design optimization 207\nJAUTO1213 Proc. IMechE Vol. 224 Part D: J. Automobile Engineering\n\u2018jitter\u2019 written by Werner Stahel and Martin Maech-\nler (Eidgeno\u00a8ssische Technische Hochschule Zurich,\nZurich, Switzerland).\nThe development surfaces are designated peaks 0\nto peaks 3 with increasing levels of noise imposed on\nthe clean peaks 0 surface according to the following:\n(a) peaks 1: mean, 0.1189; variance, 0.0836;\n(b) peaks 2: mean, 0.2842; variance, 0.3705;\n(c) peaks 3: mean, 1.7277; variance, 0.7648.\nThis paper does not set out to compare the\nperformance of search methodologies, but to in-\nvestigate the benefits that can be obtained by inte-\ngrating elements of decision support into the pro-\ncess. The parameters associated with each method\nwere tuned in order to obtain a reasonably rapid\nconvergence to an acceptable solution. Based upon\nthe range of noise levels between peaks 0 and peaks\n3 surfaces, a stop criterion of 0.5 was found to be an\nacceptable trade-off between performance and con-\nvergence time. This stop criterion was applied to all\nthe heuristics. Step sizes and, where appropriate,\ncooling schedules were also tuned to give acceptable\nperformance. This was particularly important, in\norder to give results with reasonable statistical signi-\nficance; each method\u2013surface combination was run\n100 times to achieve the mean values. This approach\nwas adopted because a significant feature of these\nheuristics is the stochastic nature of aspects of the\nsearch.\nThe GD method was run with a fixed step size of\n0.2, the VNS method had an initial step size of 0.1,\ndividing by 2 at each step to a final step size of 0.01,\nand finally the SA method had a step size of 0.1, with\na cooling schedule of 0.5 per step from an initial\ntemperature of 10 to a final temperature of 0.001.\nThe parameters for the search methods are given in\nTable 1.\nIn order to examine the method\u2019s effectiveness,\nanother search space was introduced, namely the\n\u2018bump\u2019 problem [15, 16], which is a smooth surface\nconsisting of many peaks, all of similar sizes. Also,\nthe optimal value is defined adjacent to a constraint\nboundary. It has been noted that these features\nrender it relatively difficult for most optimizers to\ndeal with.\nThe bump problem is defined as\nmax\nabs\nPn\ni~1 cos\n4 xi\u00f0 \u00de{2Pni~1 cos2 xi\u00f0 \u00de\n\n \u000b\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiPn\ni~1 ix\n2\n1\nq\n8><\n>:\n9>=\n>; \u00f024\u00de\nfor\n0vxiv10, i~1, . . . ,n\nsubject to\nP\nn\ni~1\nxiw0:75,\nXn\ni~1\nxiv\n15n\n2\nstarting from\nxi~5, i~1, . . . ,n\nwhere xi are the variables (in radians) in the range\nfrom 0 to 10, subject to two constraints, and n is the\nnumber of dimensions. The highly contoured sur-\nface that this function produces is shown in Fig. 5.\nIn this case, a two-dimensional surface has been\nchosen. The global optimum is defined by the pro-\nduct constraint.\nThe surface of the function in two variables is\nshown in Fig. 6. The parameters of the heuristics\nTable 1 Search parameters: peaks\nMethod Stop criterion Step size Schedule\nGD Minimum+ 0.5 0.2 0\nVNS Minimum+ 0.5 4R 0.01 0.5\nSA Minimum+ 0.5 0.1 10R 0.001(0.5)\nFig. 5 Contour map for a two-variable bump function\n208 D Gladwin, P Stewart, J Stewart, R Chen, and E Winward\nProc. IMechE Vol. 224 Part D: J. Automobile Engineering JAUTO1213\nwere tuned slightly to reflect this new surface and\nare given in Table 2.\n2.3 Results\nEach of the peaks surfaces was searched by each of\nthe heuristics both with and without stochastic\nsupport. In addition, a variety of support surfaces\nof different orders were examined.\nEach instance of surface type\u2013with support\u2013with-\nout support\u2013order\u2013cross-coupling was run 100 times\nproducing mean results to negate the effects of the\ninherently stochastic methods. In the search meth-\nods presented here, each step is associated by a\nnumber of computations, evaluating the surround-\ning points (with positions specified by step size). In\nthe comparisons of search performance, the total\nnumber of computations to the stop criterion are\npresented, together with the maximum number of\nsteps that occurred in the worst-case search.\n2.3.1 Gradient descent\nThe GD methodology was applied to the five surf-\naces initially without any stochastic support, result-\ning in the performance presented in Table 3.\nIn the case of the methodologies presented in this\npaper, a trade-off in performance was made during\nthe initial tuning of the search parameters. In the\ncase of GD, the main parameter is step size, which\nwas adjusted to give an acceptable computation to\nconvergence on the \u2018worst\u2019 surfaces, i.e. peaks 3 and\nbump. This was done to make the problem compu-\ntationally tractable as each run was carried out\nmultiple times. The effect of this tuning is to give a\nrelatively poor performance on the smooth peaks 0\nsurface which could have been improved consider-\nably; however, as tuning and performance compar-\nison between methods is not at the core of this\npaper, this pragmatic approach was deemed accep-\ntable. As would be expected, mean and worst-case\ncomputations deteriorate as the level of complexity\nor noise increases, and the heuristic reaches more\nand more local minima above the level of the stop\ncriterion.\nA typical unsupported GD search on the peaks 2\nsurface is shown in Fig. 7 and illustrates a typical\noperation, with local terminations, stochastic jumps,\nand stop criterion at approximately (3, 1).\nThe GD heuristic was then run on the same sur-\nfaces, with stochastic decision support ranging from\nfirst- to fifth-order surfaces. The results of the experi-\nments are given in Table 4.\nAn example of a stochastic third-order support\nsurface for the bump function is shown in Fig. 8,\nwith the contour lines denoting the probability of the\nnext jump. This particular support map is shown\nafter three jumps. The structure and value of the\nFig. 6 Two-variable bump function surface\nTable 2 Search parameters: bump\nMethod Stop criterion Step size Schedule\nGD Minimum+ 0.05 0.1 0\nVNS Minimum+ 0.5 4R 0.001 0.5\nSA Minimum+ 0.05 0.1 10R 0.001(0.5)\nTable 3 GD performance: no support\nSurface Mean computations Worst-case computations\nPeaks 0 327 565\nPeaks 1 450 679\nPeaks 2 1087 7614\nPeaks 3 2750 16 698\nBump 6561 29 268\nFig. 7 Peaks 2: GD; no support; search example (com-\nputations, 953; steps, 101; stochastic jumps, 19)\nEngine-in-the-loop experimental design optimization 209\nJAUTO1213 Proc. IMechE Vol. 224 Part D: J. Automobile Engineering\nprobability contours change on the basis of new\ninformation available after every search.\nFigure 9 shows the corresponding bump function\ncontour map with the searches that have been\ndirected by the stochastic support operator. The\nstop criterion is satisfied after the final search\nreaches a point near (1.25, 0.5).\n2.3.2 Variable neighbourhood search\nThe VNS was applied to the surfaces without any\nstochastic support, resulting in the performance\npresented in Table 5.\nThe heuristic was then run on the same surfaces,\nwith stochastic decision support ranging from first-\nto fifth-order surfaces. The results of the experi-\nments are given in Table 6.\n2.3.3 Simulated annealing\nFinally, SA was applied to the surfaces without any\nstochastic support, resulting in the performance\npresented in Table 7.\nThe heuristic was then run on the same surfaces,\nwith stochastic decision support ranging from first-\nto fifth-order surfaces. The results of the experi-\nments are given in Table 8.\n2.4 Discussion\nFigures 10 and 11 present a comparison of the three\nsearch methodologies for the five surfaces. In all the\nfigures presented in this section, the x axis repre-\nsents increasingly noisy surfaces from peaks 0 to\npeaks 3, followed by the complex surface G2-inv. For\nboth mean and worst-case computations, simple\nTable 4 GD performance: with support (upper value, mean; lower value, worst case)\nSurface\nValue for the following\nFirst order Second order Third order Fourth order Fifth order\nPeaks 0 244 234 176 136 168\n1597 1361 646 599 633\nPeaks 1 217 237 254 269 376\n985 1080 1278 1416 2298\nPeaks 2 573 896 1009 669 1679\n6883 4947 10 397 5379 30 367\nPeaks 3 1649 2435 2395 1204 2611\n6171 4287 32 402 6669 12 203\nBump 1668 1682 2014 2063 3051\n10 023 7237 6688 11 109 13 097\nFig. 8 Bump surface: GD; third-order stochastic sup-\nport surface after three jumps\nFig. 9 Bump surface: GD; search example with third-\norder support (computations, 479; steps, 54;\nstochastic jumps, 6; start point, blue; finishing\npoint, green). The search order is numbered\nTable 5 VNS performance: no support\nSurface Mean computations Worst-case computations\nPeaks 0 723 2701\nPeaks 1 936 2896\nPeaks 2 1479 12 919\nPeaks 3 3087 63 587\nBump 2502 98 784\n210 D Gladwin, P Stewart, J Stewart, R Chen, and E Winward\nProc. IMechE Vol. 224 Part D: J. Automobile Engineering JAUTO1213\nGD outperforms the other methodologies on the\npeaks surface. As predicted, SA performs increasing-\nly poorly with increasing noise but shows a better\nperformance than VNS on the complex G2_inv sur-\nface. No conclusions will be drawn concerning the\nrelative methods used here, as the prime motiva-\ntion of this research is to develop a generic decision\nsupport to increase the performance of generalized\nsearch methods.\nNext, the performance of the search methods with\nstochastic decision support will be considered.\nFigures 12 to 14 present a performance compar-\nison for each method supported by a range of\nsupport surfaces from first to fifth order. The first\nobservation of note is that the effectiveness of the\ndecision support is dependent on its order, and\nhence its level of fit to the data surface being esti-\nmated. This will be investigated in future research, to\nincorporate an \u2018adaptive ordered\u2019 fit that optimizes\nthe benefit of the support method. In the case of GD\n(Fig. 12), excluding fifth-order support, the suppor-\nted searches deliver increasing comparative levels\nof performance with increasing noise and comple-\nxity.\nIn the case of the VNS (Fig. 13), all the supported\nsearches outperform unsupported searches in the\npresence of increasing noise; however, this does not\nhold true for the complex G2_inv surface. Finally, in\nthe case of SA (Fig. 14), supported searches outper-\nform unsupported searches only in the cases of high\nTable 6 VNS: with support (upper value, mean; lower value, worst case)\nSurface\nValue for the following\nFirst order Second order Third order Fourth order Fifth order\nPeaks 0 660 334 507 453 454\n2201 1505 1793 1785 2683\nPeaks 1 545 421 482 694 632\n3770 2226 1869 2681 2505\nPeaks 2 730 880 974 699 788\n6883 4947 10 397 5379 30 367\nPeaks 3 2448 1959 1913 1313 1487\n35 661 16 450 13 682 8965 11 941\nBump 4125 4358 4961 8550 6619\n18 903 15 572 18 423 54 847 39 062\nTable 7 SA search performance: no support\nSurface Mean computations Worst-case computations\nPeaks 0 402 1937\nPeaks 1 597 2386\nPeaks 2 1542 10 184\nPeaks 3 8134 40 868\nBump 9922 40 610\nTable 8 SA: with support (upper value, mean; lower value, worst case)\nSurface\nValue for the following\nFirst order Second order Third order Fourth order Fifth order\nPeaks 0 474 545 293 328 494\n2564 3669 1129 1667 2307\nPeaks 1 644 822 853 716 644\n2614 4384 3981 4573 3002\nPeaks 2 1515 2214 1530 2179 2747\n6151 21 859 12 312 22 382 49 213\nPeaks 3 3838 4558 5014 3899 7804\n18 250 35 506 44 304 40 157 62 188\nBump 3042 5235 4522 4977 4834\n10 557 31 227 17 355 20 882 18 181\nFig. 10 Mean computations comparison of unsup-\nported search methods on the five surfaces\nEngine-in-the-loop experimental design optimization 211\nJAUTO1213 Proc. IMechE Vol. 224 Part D: J. Automobile Engineering\nlevels of noise or high levels of complexity. At this\npoint in the development of the methodology, it can\nbe concluded that the WSDS component makes a\nsignificant improvement to search efficiency, parti-\ncularly with noisy real-world data. As with every-\nthing associated with search heuristics, there is no\nuniversal methodology, evidenced by the fact that\nunsupported SA outperforms the supported instan-\nces on the bump surface.\n3 ENGINE-IN-THE-LOOP EXPERIMENTAL\nAPPLICATION\nIn this section, a decision support system for hard-\nware-in-the-loop optimization is investigated. The\nmethod, which has previously been developed on\ntest surfaces in simulation, is applied to an auto-\nmotive combinatorial search in the laboratory, on\na real-time engine in the loop application. It is\ndesired to find the maximum power output of an ex-\nperimental single-cylinder spark ignition (SI) engine\noperating under a quasi-constant-volume (QCV)\noperating regime. Under this regime, the piston is\nslowed at top dead centre (TDC) to achieve combus-\ntion in close to constant-volume conditions.\nAs part of the further development of the engine to\nincorporate a linear generator to investigate free\nFig. 12 Comparison of GD method with no support\nwith first- to fifth-order supported searches\nFig. 13 Comparison of VNS method with no support\nwith first- to fifth-order supported searches\nFig. 11 Worst-case computations comparison of un-\nsupported search methods on the five surfaces\nFig. 14 Comparison of SA method with no support\nwith first- to fifth-order supported searches\n212 D Gladwin, P Stewart, J Stewart, R Chen, and E Winward\nProc. IMechE Vol. 224 Part D: J. Automobile Engineering JAUTO1213\npiston operation, it is necessary to perform a series\nof experiments with combinatorial parameters. The\nobjective is to identify the maximum power point in\nthe least number of experiments in order to mini-\nmize costs. This test programme provides peak\npower data in order to achieve optimal electrical\nmachine design.\nThe decision support methodology is combined\nwith standard optimization and search methods,\nnamely GD and SA, in order to study the reductions\npossible in experimental iterations. It is shown that\nthe decision support methodology significantly re-\nduces the number of experiments necessary to find\nthe maximum power solution and thus offers a\npotentially significant cost saving to hardware-in-\nthe-loop experimentation.\nIn this section, GD and SA are supplemented by\nthe decision support methodology and applied to\nthe real-life task of identifying the peak power\noperating point of a novel internal combustion\nengine experimental rig and control methodology.\nThe supplemented methods are compared with the\nbasic methodologies and offer considerable savings\nin experimental effort.\n3.1 Engine-in-the-loop operation\nThe engine employed in this research, as shown in\nFig. 15, is a purposely converted single-cylinder re-\nsearch engine. Its combustion chamber, the head\nand the piston, are based on the GM Family One 1.8-l\nengine architecture, of four-valve type with a bore of\n80.5mm and a stroke of 88.2mm. The bottom part\nof the engine is converted from a standard four-\ncylinder engine block. The combustion chamber\nhas been lifted significantly from the bottom. The\nextension in between is reserved for further future\nwork on free-piston engines. The purpose of the\nwork described in this paper is to identify the peak\npower of the engine under a QCV regime [17], in\norder to design optimally a linear motor\u2013generator\nthat will replace the extension tube.\nDuring operation of conventional internal com-\nbustion (IC) engines, the piston can only reciprocate\ncontinuously between TDC and bottom dead centre\n(BDC) at a frequency proportional to the engine\nspeed. The chemical reaction process associated\nwith combustion events, however, essentially takes a\nfixed time to complete, which is relatively indepen-\ndent of the engine speed. In order to maximize the\nwork obtained from the heat energy released by\ncombustion, the air\u2013fuel mixture has to be ignited\nprior to the piston reaching TDC, and the ignition\ntiming should be adjusted according to the engine\nspeed and the quality of the air\u2013fuel mixture.\nClearly, the early stage of the heat release before\nthe piston reaches TDC results in negative work.\nDuring the combustion event, the piston movement\nis defined by the crank rotation, so that truly\nconstant-volume heat release is not achievable.\nFurther, to scavenge the burned gas efficiently, the\nexhaust valve has to be opened well before BDC,\nwhile the pressure of the burned gas is still high.\nThus, a large portion of the thermal energy is ex-\npelled into the exhaust, which further reduces the\nengine efficiency. The ideal scenario is to initiate\nand complete the combustion event while the piston\nremains at or close to the TDC position in order to\nachieve the maximum thermal potential and elimin-\nate the negative work which results with early ignition,\nand to extend the expansion stroke further in order\nto use the thermal energy fully as well as to pro-\nvide sufficient time for post-combustion reactions,\nthereby reducing partial burned emissions. One\npractical method of achieving such an optimization\nwithout changing the engine design and sacrificing\nengine performance in series-hybrid applications is\nto reduce the engine crank rotation velocity signi-\nficantly at the TDC position to provide sufficient\ntime for combustion to be completed and then to\naccelerate the engine during the compression and\nFig. 15 The QCV experimental engine\nEngine-in-the-loop experimental design optimization 213\nJAUTO1213 Proc. IMechE Vol. 224 Part D: J. Automobile Engineering\nexpansion phases to maintain the high average\ncrank speed to deliver the high power output. This\nwill then generate a new combustion cycle which is\nbetween the combustion cycle of a conventional IC\nengine and the ideal Otto constant-volume com-\nbustion cycle. This can be therefore be called a\nQCV combustion cycle, as illustrated in Fig. 16.\nTheoretically, the series-hybrid power train en-\nables a higher efficiency and power output from IC\nengine configurations [18], while the QCV concept\noffers an even greater potential for higher combus-\ntion efficiency. In order to investigate the QCV com-\nbustion concept, a proof-of-principle engine system\nhas been developed, as shown in Fig. 17. The sys-\ntem consists of a high torque-to-inertia ratio, high-\nbandwidth permanent-magnet brushless a.c. electric\nmachine, a single-cylinder research SI engine, and a\ncontrol system.\n3.1.1 Piston trajectory\nFor a proof-of-principle system, control was im-\nplemented on the electrical machine that could\ndeliver a sinusoidal crank velocity with defined\naverage and magnitude quantities. An example of a\nsimple variable-crank-velocity profile was selected; a\nsinusoidal wave velocity form at an average speed of\n600 r\/min with a wave magnitude of \u00a1200 r\/min\nhas been employed in the study. Figure 18 shows the\ntheoretical variable crank rotation velocity at various\ncrank positions in comparison with equivalent\nconventional constant-speed data. The piston TDC\nposition is 0u and 360u.\nThe residual time at TDC has been extended, while\nthe residual time at BDC is reduced, as shown by the\nsolid curve. This offers longer time for the combus-\ntion event to complete at the TDC region which\ndelivers higher combustion efficiency than in the\nconventional case. Figure 19 shows the fired cylinder\npressure of the conventional cycle at 600 r\/min and\nthe QCV cycle at a sinusoidal speed of 600 r\/min\naverage with \u00a1200 r\/min amplitude at a normalized\ncycle time. The engine was 5.75 per cent throttled in\nthe conventional cycle and 5.3 per cent for the\nQCV cycle. The fuel-injection pulse width for both\nscenarios were the same with a length of 5.65ms.\nThe SI timing of the conventional cycle was opti-\nmized at 10.2u crank angle (CA) before top dead\ncentre (BTDC). For the QCV cycle, it was optimized\nat 9.8 uCA BTDC. Clearly, the QCV cycle uses a later\noptimized ignition timing, but produces a later but\nhigher peak cylinder pressure, which further leads to\nan overall higher in-cylinder pressure during the\nexpansion stroke.\nThe work produced by a combustion engine is an\nintegration of the pressure over an engine cycle.\nClearly, the higher expansion pressure of the QCV\ncycle can produce higher work than its conventionalFig. 16 Typical pressure\u2013volume diagram\nFig. 17 Schematic diagram of the QCV electrical power system\n214 D Gladwin, P Stewart, J Stewart, R Chen, and E Winward\nProc. IMechE Vol. 224 Part D: J. Automobile Engineering JAUTO1213\ncounterpart. Overall, the pressure integral of the\nQCV cycle is 11 per cent higher than that of the\nconventional cycle.\n3.2 Decision support and search methods\nFor the next stage of the development of this\nexperimental rig, and the main exposition of the\napplication of the decision support methodology,\nit is necessary to find the combinations of mean\ncrankshaft velocity and sinusoidal amplitude at each\nthrottle setting and injection timing that deliver\nthe highest peak cylinder pressure (and hence\npower). These data are required for the design of a\nlinear generator that will be built into the rig to\ninvestigate free-piston operation. As an example, the\napplication of the experimental decision support\noperator will be examined to find the maximum\npower operating point of the engine at the throttle\nand injection settings given earlier in the section. In\norder to evaluate the relative performance of the\ndecision support method, the engine was character-\nized by an exhaustive search, which is shown in\nFig. 20.\nGD-based methods will be used to find the com-\nbination of mean crankshaft velocity versus crank-\nshaft sinusoidal perturbation that produces the\nhighest peak cylinder pressure.\nFigure 21 shows a typical hardware-in-the-loop\nsearch using a simple GD method. The method uses\na total of 41 stochastic jumps, and a total of 311 steps\nto find a solution within 0.02 bar of the maximum\nidentified by exhaustive search. Under the same\nexperimental parameters, a simple GD search with\nWSDS is performed.\nFig. 19 Peak cylinder pressure at firing; QCV versus\nconventional operation\nFig. 21 Example of a simple GD search with no WSDS\n(terminating coordinates (516, 280); x, average\ncrankshaft speed (r\/min); y, sinusoidal pertur-\nbation (r\/min))\nFig. 20 Engine experimental map for peak cylinder\npressure under QCV operation\nFig. 18 Conventional constant and variable sinusoidal\ncrank velocity (CAD, crank angle (deg))\nEngine-in-the-loop experimental design optimization 215\nJAUTO1213 Proc. IMechE Vol. 224 Part D: J. Automobile Engineering\nIn Fig. 22, the decision support surface based\nupon previous searches is shown. This particular\nsurface is stopped at fulfilment of the termination\ncriterion. The associated searches are shown in\nFig. 23.\nIn this case, the termination criterion was achi-\neved in 24 jumps and 126 steps. Apart from the im-\nprovement in search efficiency, the effect of the\nweighted surface on the stochastic jumps can be\nobserved. Search activity increases around areas of\nhigher interest based upon previous searches.\n3.3 Discussion\nFor simplicity of analysis here, the decision support\nRSs applied to the experimental data were fixed\nsecond order, although further research will inves-\ntigate the efficacy of adaptive schemes. GD and SA\nwere run both with and without support. Since the\nterminal value had been identified by exhaustive\nsearch, the relaxation of termination value was also\nvaried to investigate its influence (Fig. 24).\nIrrespective of the search algorithm, WSDS sup-\nport reduces the mean number of searches by a\nsignificant amount and, in the case of the tightest\nstop criterion, by approximately 75 per cent.\nIt is important to remember that this is not a\ncomparative study of search heuristics, but an ob-\nservation of the advantages that can be gained by\nextending simple search heuristics with a simple\nRS-based decision support operator. It can be seen\nthat, in all cases, the WSDS extension significantly\nimproves the performance of the search methodol-\nogies. The improvement in performance of all the\nsearches as the stop criterion is relaxed reflects one\nof the basic effects of experimental noise. As noise\nor the \u2018tightness\u2019 of the stop criterion to the global\nminimum increases, then these methodologies be-\ncome increasingly sensitive to the step size and\nstarting point. In other words, the noise dominates\nthe search.\nAs can be seen from the typical support surface in\nFig. 22, there is an overall correlation between the\n\u2018area of interest\u2019 and the actual experimental surface;\nhowever, this would be improved by higher-order\nRSs (section 2). Future work will include the develop-\nment of an adaptive WSDS, which will choose the\nbest RS method representation based on the fit to\nthe received data. Of future interest will also be the\nintegration of certain aspects of tabu search [8]. After\na certain number of searches, it might be advanta-\nFig. 22 WSDS support for experimental search\nFig. 23 Example of a simple GD search with WSDS\n(terminating coordinates (515, 279))\nFig. 24 Mean experimental steps to termination ver-\nsus relaxation of stop criterion\n216 D Gladwin, P Stewart, J Stewart, R Chen, and E Winward\nProc. IMechE Vol. 224 Part D: J. Automobile Engineering JAUTO1213\ngeous to weaken areas with low confidence, and\nto strengthen the areas with high confidence. This\nwould have the effect of higher concentration of\njumps away from low-confidence areas, a corollary\nwith tabu search. This will also be researched at a\nlater date.\n4 CONCLUSION\nA method has been presented to add decision\nsupport to the previously random jumps of GD\nmethods commonly used in combinatorial experi-\nmentation. Significant promise has been shown in\nthe performance improvements of even the simplest\nGD method. However, as with all search method-\nologies, there is no universal solution. The basic\nheuristics themselves have been shown to perform\nbetter on some surfaces than others, which is to be\nexpected. In a similar way, the support method has\nbeen shown to be more effective in certain surface\u2013\nmethod\u2013support order combinations.\nIn section 3 of this paper, the support method-\nologies were applied to a real-life combinatorial\nexperiment conducted in an engine laboratory, to\nassess their performance in a realistic environment.\nSignificant improvements in search efficiency was\nexhibited by the WSDS methodology, even under a\nrelatively tight stop criterion.\nFuture work will address implementing an adap-\ntive order support surface, and an adaptive search\nhybrid (effectively a hyperheuristic) to maximize the\neffectiveness of this method. In particular, an ada-\nptive heuristic search parameter will be investigated.\nA decision support methodology has been pre-\nsented to support hardware-in-the-loop experimen-\ntation. Since the majority of this type of experimen-\ntation is performed on plant with an unknown\nresponse, it is general practice to utilize simple\nheuristics such as GD or SA. Genetic algorithms have\nalso been shown to be effective, and these will be\nthe subject of future study in terms of decision sup-\nport. The methodology as presented in this paper\nhas shown itself to be effective in dealing with \u2018un-\nknown\u2019 search spaces and relatively \u2018untuned\u2019 search\nheuristics.\nA decision support operator has been presented,\nwhich uses the past history of searches to construct\na \u2018confidence map\u2019 based upon the RS methodol-\nogy. This confidence map influences the stochastic\njumps of the heuristics towards areas of increased\ninterest. In line with the philosophy of the heuristics,\nthe decision support operator is extremely simple to\nimplement.\nIn section 2, the performance of the decision\nsupport operator was investigated on a series of test\nsurfaces with various levels of noise present. The\noperator was shown to be highly effective in re-\nducing the number of steps to termination of the\nchosen heuristics.\nIn section 3, the methodology has been applied\nto the task of finding the maximum power point of\na single-cylinder engine under a novel operating\nregime at a defined operating point. The two chosen\nheuristics were GD and SA. In both cases, the search\nperformance was significantly improved by decision\nsupport. With respect to the experimental applica-\ntion under consideration in this paper, the identified\npeak pressures with predicted peak power profiles\nallowed the design of a linear motor\u2013generator for\nthe project, under reduced experimental time to\nextract the necessary data and, as such, the meth-\nodology is shown to be advantageous.\nACKNOWLEDGEMENTS\nTechnical support and the hardware used in this\nresearch was provided by the Engineering and\nPhysical Sciences Research Council Project Zero-\nConstraint Free Piston Energy Converter (Grant GR\/\nS97507\/01) and Lotus Engineering Ltd UK.\nF Authors 2010\nREFERENCES\n1 Stewart, P., Fleming, P. J., and MacKenzie, S. A.\nReal time simulation and control systems design by\nthe response surface methodology and designed\nexperiments. Int. J. Systems Sci., 2003, 34(14\u201315),\n837\u2013850.\n2 Stewart, P., Stone, D. A., and Fleming, P. J. Design\nof robust fuzzy-logic control systems by multi-\nobjective evolutionary methods with hardware in\nthe loop. IFAC J. Engng Applic. Artif. Intell., 2004,\n70(3), 275\u2013284.\n3 Myers, R. H. andMontgomery, D. C. Response sur-\nface methodology: process and product optimiza-\ntion using designed experiments, 1995 (John Wiley,\nNew York).\n4 Michalewicz, Z. and Fogel, D. B. How to solve it,\nalgorithms for engineering systems, 2006 (Cam-\nbridge University Press, Cambridge).\n5 Narayana Naik, G., Gopalakrishnan, S., and Gang-\nuli, R. Design optimization of composites using\ngenetic algorithms and failure mechanism based\nfailure criterion. Composite Structs, 2008, 83(4),\n354\u2013367.\nEngine-in-the-loop experimental design optimization 217\nJAUTO1213 Proc. IMechE Vol. 224 Part D: J. Automobile Engineering\n6 Mladenovic, M. and Hansen, P. Variable neigh-\nbourhood search. J. Comput. Ops Res., 1997, 24(11),\n1097\u20131100.\n7 Burke, E. and Kendall, G. Comparison of meta-\nheuristic algorithms for clustering rectangles. J.\nComput. Ind. Engng, 1990, 37(1), 383\u2013386.\n8 Glover, F. and Hanfi, S. Tabu search and finite\nconvergence. Appl. Math., 2002, 119(1\u20132), 3\u201336.\n9 Toksari, M. D. and Guner, E. Solving the uncon-\nstrained optimisation problem by a variable neigh-\nbourhood search. J. Math. Analysis and Applic.,\n2007, 328(2), 1178\u20131187.\n10 Baldick, R. Applied optimization: formulation and\nalgorithms for engineering systems, 2006 (Cam-\nbridge University Press, Cambridge, UK).\n11 Kirkpatrick, S., Gelatt, C. D., and Vecchi, M. P.\nOptimization by simulated annealing. Science,\n1983, 220, 671\u2013680.\n12 Burke, E. and Kendall, G. Search methodologies:\nintroductory tutorials in optimisation and decision\nsupport techniques, 2005 (Springer, New York).\n13 Kirkpatrick, S., Gelatt, C. D., and Vecchi, M. P.\nOptimization by simulated annealing. Science, New\nSer., 1983, 220(4598), 671\u2013680.\n14 Stewart, P. and Fleming, P. J. The response surface\nmethodology for rapid prototyping of real-time\ncontrol systems. In Proceedings of the 2002 Amer-\nican Control Conference, Anchorage, Alaska, USA,\n8\u201310 May 2002, pp. 3343\u20133348 (IEEE, New York).\n15 Keane, A. J. Experiences with optimizers in\nstructural design. In Proceedings of the First\nInternational Conference on Adaptive computing\nin engineering design and control (Ed. I. C. Parmee),\nPlymouth, Devon, UK, 21\u201322 September 1994, pp.\n14\u201327 (University of Plymouth, Plymouth, UK).\n16 Keane, A. J. Genetic algorithm optimization of\nmulti-peak problems: studies in convergence and\nrobustness. Artif. Intell. Engng, 1995, 9(2), 75\u201383.\n17 Chen, R. Quasi-constant volume (QCV) spark igni-\ntion combustion. SAE paper 2009-01-0700, 2009.\n18 Stone, R. Introduction to internal combustion en-\ngines, 3rd edition, 1999 (Macmillan, London).\n218 D Gladwin, P Stewart, J Stewart, R Chen, and E Winward\nProc. IMechE Vol. 224 Part D: J. Automobile Engineering JAUTO1213\n"}