{"doi":"10.1111\/j.1467-9469.2009.00685.x","coreId":"216502","oai":"oai:eprints.lse.ac.uk:30992","identifiers":["oai:eprints.lse.ac.uk:30992","10.1111\/j.1467-9469.2009.00685.x"],"title":"The Dantzig selector in Cox's proportional hazards model","authors":["Antoniadis, Anestis","Fryzlewicz, Piotr","Letu\u00e9, Fr\u00e9d\u00e9rique"],"enrichments":{"references":[{"id":17342797,"title":"A generalized Dantzig selector with shrinkage tuning.","authors":[],"date":"2009","doi":"10.1093\/biomet\/asp013","raw":"James, G. & Radchenko, P. (2009). A generalized Dantzig selector with shrinkage tuning. Biometrika 96, 323\u2013337.","cites":null},{"id":17342814,"title":"A note on path-based variable selection in the penalized proportional hazards model.","authors":[],"date":"2008","doi":"10.1093\/biomet\/asm083","raw":"Zou, H. (2008). A note on path-based variable selection in the penalized proportional hazards model. Biometrika 95, 241\u2013247. Corrresponding author: Anestis Antoniadis Laboratoire Jean Kuntzmann, D\u00e9partement de Statistique, Universit\u00e9 Joseph Fourier, B.P. 53 38041 Grenoble CEDEX 9, France e-mail : Anestis.Antoniadis@imag.fr","cites":null},{"id":17342812,"title":"Adaptive Lasso for Cox\u2019s proportional hazards model.","authors":[],"date":"2007","doi":"10.1093\/biomet\/asm037","raw":"Zhang, H. H. & Lu, W. (2007). Adaptive Lasso for Cox\u2019s proportional hazards model. Biometrika 94, 691\u2013703.","cites":null},{"id":17342792,"title":"Assessment and comparison of prognostic classi\ufb01cation schemes for survival data. Statistics in Medicine 18, 2529\u20132545.Dantzig selector fox","authors":[],"date":"1999","doi":"10.1002\/(sici)1097-0258(19990915\/30)18:17\/18<2529::aid-sim274>3.3.co;2-x","raw":"Graf, E., Schmoor, C., Sauerbrei, W. & Schumacher, M. (1999). Assessment and comparison of prognostic classi\ufb01cation schemes for survival data. Statistics in Medicine 18, 2529\u20132545.Dantzig selector fox Cox\u2019s model 27 Gui, J. & Li, H. (2005). Threshold gradient descent method for censored data regression with applications in pharmacogenomics. Paci\ufb01c Symposium on Biocomputing 10, 272\u2013 283.","cites":null},{"id":17342798,"title":"Associations between gene expressions in beast cancer and patient survival.","authors":[],"date":"2002","doi":"10.1007\/s00439-002-0804-5","raw":"Jenssen, T., Kuo, W., Stokke, T. & Hovig, E. (2002). Associations between gene expressions in beast cancer and patient survival. Human Genetics 111, 411\u2013420.","cites":null},{"id":17342796,"title":"Bayesian variable selection for the Cox regression model with missing covariates.","authors":[],"date":"2008","doi":"10.1007\/s10985-008-9101-5","raw":"Ibrahim, J., Chen, M.-H. & Kim, S. (2008). Bayesian variable selection for the Cox regression model with missing covariates. Lifetime Data Analysis 14, 496\u2013520.","cites":null},{"id":17342791,"title":"Bayesian variable selection method for censored survival data.","authors":[],"date":"1998","doi":"10.2307\/2533672","raw":"Faraggi, D. & Simon, R. (1998). Bayesian variable selection method for censored survival data. Biometrics 54, 1475\u20131485.","cites":null},{"id":17342802,"title":"Covariate selection for the semiparametric additive risk model.","authors":[],"date":"2009","doi":"10.1111\/j.1467-9469.2009.00650.x","raw":"Martinussen, T. & Scheike, T. H. (2009b). Covariate selection for the semiparametric additive risk model. Scandinavian Journal of Statistics .Dantzig selector fox Cox\u2019s model 28 Marx, B. D. (1996). Iteratively reweighted partial least squares estimation for generalized linear regression. Technometrics 38, 374\u2013381.","cites":null},{"id":17342781,"title":"Cox\u2019s regression model for counting processes: a large sample study.","authors":[],"date":"1982","doi":"10.1214\/aos\/1176345976","raw":"Andersen, P. K. & Gill, R. D. (1982). Cox\u2019s regression model for counting processes: a large sample study. Annals of Statistics 10, 1100\u20131120.","cites":null},{"id":17342810,"title":"Crossvalidated Cox regression on microarray gene expression data.","authors":[],"date":"2006","doi":"10.1002\/sim.2353","raw":"van Houwelingen, H., Bruinsma, T., A., H., Van\u2019t Veer, L. J. & Wessels, L. (2006). Crossvalidated Cox regression on microarray gene expression data. Statistics in Medicine 25, 3201\u20133216.","cites":null},{"id":17342788,"title":"Ef\ufb01cient computation of subset selection probabilities with application to Cox regression.","authors":[],"date":"1994","doi":"10.2307\/2337133","raw":"Delong, D., Guirguis, G. & So, Y. (1994). Ef\ufb01cient computation of subset selection probabilities with application to Cox regression. Biometrika 81, 607\u2013611.","cites":null},{"id":17342795,"title":"Ef\ufb01cient estimation for the Cox model with interval censoring.","authors":[],"date":"1996","doi":"10.1214\/aos\/1032894452","raw":"Huang, J. (1996). Ef\ufb01cient estimation for the Cox model with interval censoring. Annals of Statistics 24, 540\u2013568.","cites":null},{"id":17342807,"title":"Empirical processes with applications to statistics.","authors":[],"date":"1986","doi":"10.1137\/1.9780898719017","raw":"Shorack, G. R. & Wellner, J. A. (1986). Empirical processes with applications to statistics. Wiley, New York.","cites":null},{"id":17342799,"title":"Equivalence of several methods for ef\ufb01cient best subsets selection in generalized linear models.","authors":[],"date":"1995","doi":"10.1016\/0167-9473(94)00030-m","raw":"Jovanovic, B. D., Hosmer, D. & Buonaccorsi, J. P. (1995). Equivalence of several methods for ef\ufb01cient best subsets selection in generalized linear models. Computational Statistics and Data Analysis 20, 59\u201364.","cites":null},{"id":17342809,"title":"Exponential inequalities for martingales, with application to maximum likelihood estimation for counting processes.","authors":[],"date":"1995","doi":"10.1214\/aos\/1176324323","raw":"van de Geer, S. (1995). Exponential inequalities for martingales, with application to maximum likelihood estimation for counting processes. Annals of Statistics 23.","cites":null},{"id":17342805,"title":"Linking expression data with patient survival times using partial least squares.","authors":[],"date":"2002","doi":"10.1093\/bioinformatics\/18.suppl_1.s120","raw":"Park, P., Tian, L. & Kohane, I. (2002). Linking expression data with patient survival times using partial least squares. Bioinformatics 18, 120\u2013127.","cites":null},{"id":17342794,"title":"Matrix analysis.","authors":[],"date":"1985","doi":"10.1017\/cbo9780511810817","raw":"Horn, R. A. & Johnson, C. R. (1985). Matrix analysis. Cambridge University Press, Cambridge, UK.","cites":null},{"id":17342804,"title":"Partial least squares Cox regression for genome-wide data.","authors":[],"date":"2008","doi":"10.1007\/s10985-007-9076-7","raw":"Nyg\u00e5rd, S., Borgan, \u00d8., Lingj\u00e6rde, O. & St\u00f8rvold, H.-L. (2008). Partial least squares Cox regression for genome-wide data. Lifetime Data Analysis 14, 179\u2013195.","cites":null},{"id":17342803,"title":"Partial least squares proportional hazard regression for application to DNA microarray survival data.","authors":[],"date":"2002","doi":"10.1093\/bioinformatics\/18.12.1625","raw":"Nguyen, D. V. & Rocke, D. M. (2002). Partial least squares proportional hazard regression for application to DNA microarray survival data. Bioinformatics 18, 1625\u2013 1632.","cites":null},{"id":17342784,"title":"Partial least squares: A versatile tool for the analysis of high-dimensional genomic data.","authors":[],"date":"2007","doi":"10.1093\/bib\/bbl016","raw":"Boulesteix, A. & Strimmer, K. (2007). Partial least squares: A versatile tool for the analysis of high-dimensional genomic data. Brie\ufb01ngs in Bioinformatics 8, 24\u201332.","cites":null},{"id":17342806,"title":"Piecewise linear regularized solution paths.","authors":[],"date":"2007","doi":"10.1214\/009053606000001370","raw":"Rosset, S. & Zhu, J. (2007). Piecewise linear regularized solution paths. Annals of Statistics 35, 1012\u20131030.","cites":null},{"id":17342785,"title":"Predicting survival from microarray data - a comparative study.","authors":[],"date":"2007","doi":"10.1093\/bioinformatics\/btm305","raw":"B\u00f8velstad, H., Nyg\u00e5rd, S., St\u00f8rvold, H., Aldrin, M., Borgan, \u00d8., Frigessi, A. & Lingj\u00e6rde, O. C. (2007). Predicting survival from microarray data - a comparative study. Bioinformatics 23, 2080\u20132087.","cites":null},{"id":17342782,"title":"Prediction by supervised principal components.","authors":[],"date":"2006","doi":"10.1198\/016214505000000628","raw":"Bair, E., Hastie, T., Paul, D. & Tibshirani, R. (2006). Prediction by supervised principal components. Journal of the American Statistical Association 101, 119\u2013137.Dantzig selector fox Cox\u2019s model 26 Barlow, W. E. & Prentice, R. L. (1988). Residuals for relative risk regression. Biometrika 75, 65\u201374.","cites":null},{"id":17342793,"title":"Regression modeling strategies: With applications to linear models, logistic regression, and survival analysis.","authors":[],"date":"2001","doi":"10.1093\/ije\/31.3.699","raw":"Harrell, F. E. (2001). Regression modeling strategies: With applications to linear models, logistic regression, and survival analysis. Springer, New York.","cites":null},{"id":17342783,"title":"Simultaneous analysis of Lasso and Dantzig selector.","authors":[],"date":"2009","doi":"10.1214\/08-aos620","raw":"Bickel, P., Ritov, Y. & Tsybakov, A. (2009). Simultaneous analysis of Lasso and Dantzig selector. Annals of Statistics 37, 1705\u20131732.","cites":null},{"id":17342787,"title":"Smoothing noisy data with spline functions.","authors":[],"date":"1979","doi":"10.1007\/bf01404567","raw":"Craven, P. & Wahba, G. (1979). Smoothing noisy data with spline functions. Numer. Math. 31.","cites":null},{"id":17342780,"title":"Statistical models based on counting processes.","authors":[],"date":"1993","doi":"10.1007\/978-1-4612-4348-9_9","raw":"Andersen, P. K., Borgan, \u00d8., Gill, R. D. & Keiding, N. (1993). Statistical models based on counting processes. Springer, New York.","cites":null},{"id":17342800,"title":"Sup-norm convergence rate and sign concentration property of Lasso and Dantzig estimators.","authors":[],"date":"2008","doi":"10.1214\/08-ejs177","raw":"Lounici, K. (2008). Sup-norm convergence rate and sign concentration property of Lasso and Dantzig estimators. Electronic Journal of Statistics 2, 90\u2013102.","cites":null},{"id":17342790,"title":"Sure independence screening for ultrahigh dimensional feature space (with discussion).","authors":[],"date":"2008","doi":"10.1111\/j.1467-9868.2008.00674.x","raw":"Fan, J. & Lv, J. (2008). Sure independence screening for ultrahigh dimensional feature space (with discussion). Journal of the Royal Statistical Society Series B 70, 849\u2013911.","cites":null},{"id":17342811,"title":"Survival prediction using gene expression data: a review and comparison. Computational Statistics and Data Analysis 53, 1590\u20131603.Dantzig selector fox Cox\u2019s model 29 Van\u2019t","authors":[],"date":"2009","doi":"10.1016\/j.csda.2008.05.021","raw":"van Wieringen, D., Kun, D., Hampel, R. & Boulesteix, A.-L. (2009). Survival prediction using gene expression data: a review and comparison. Computational Statistics and Data Analysis 53, 1590\u20131603.Dantzig selector fox Cox\u2019s model 29 Van\u2019t Veer, L. J., Dai, H., Van de Vijver, M. J., He, Y., Hart, A., Mao, M., Peterse, H., Van der Kooy, K., Marton, M., Witteveen, A., Schreiber, G., Kerkhoven, R., Roberts, C., Linsley, P., Bernards, R. & Friend, S. (2002). Gene expression pro\ufb01ling predicts clinical outcome of breast cancer. Nature 415, 530\u2013536.","cites":null},{"id":17342801,"title":"The Aalen additive hazards model with high dimensional regressors.","authors":[],"date":"2009","doi":"10.1007\/s10985-009-9111-y","raw":"Martinussen, T. & Scheike, T. H. (2009a). The Aalen additive hazards model with high dimensional regressors. [L]ifetime Data Analysis 15, 330\u2013342.","cites":null},{"id":17342786,"title":"The Dantzig selector: Statistical estimation when p is much larger than n.","authors":[],"date":"2007","doi":"10.1214\/009053607000000532","raw":"Cand\u00e8s, E. & Tao, T. (2007). The Dantzig selector: Statistical estimation when p is much larger than n. Annals of Statistics 35, 2313\u20132351.","cites":null},{"id":17342808,"title":"The lasso method for variable selection in the Cox model.","authors":[],"date":"1997","doi":"10.1002\/(sici)1097-0258(19970228)16:4<385::aid-sim380>3.0.co;2-3","raw":"Tibshirani, R. (1997). The lasso method for variable selection in the Cox model. Statistics in Medicine 16, 385\u2013395.","cites":null},{"id":17342789,"title":"Variable selection for Cox\u2019s proportional hazards model and frailty model.","authors":[],"date":"2002","doi":"10.2495\/iceee20140241","raw":"Fan, J. & Li, R. (2002). Variable selection for Cox\u2019s proportional hazards model and frailty model. Annals of Statistics 30, 74\u201399.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-12","abstract":"The Dantzig selector (DS) is a recent approach of estimation in high-dimensional linear regression models with a large number of explanatory variables and a relatively small number of observations. As in the least absolute shrinkage and selection operator (LASSO), this approach sets certain regression coefficients exactly to zero, thus performing variable selection. However, such a framework, contrary to the LASSO, has never been used in regression models for survival data with censoring. A key motivation of this article is to study the estimation problem for Cox's proportional hazards (PH) function regression models using a framework that extends the theory, the computational advantages and the optimal asymptotic rate properties of the DS to the class of Cox's PH under appropriate sparsity scenarios. We perform a detailed simulation study to compare our approach with other methods and illustrate it on a well-known microarray gene expression data set for predicting survival from gene expressions","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/216502.pdf","fullTextIdentifier":"http:\/\/eprints.lse.ac.uk\/30992\/1\/The_Dantzig_selector_in_Cox%27s_proportional_hazards_model%28lsero%29.pdf","pdfHashValue":"b27b70b44a7a3ea10dc47465b449085ffbc6226a","publisher":"John Wiley","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lse.ac.uk:30992<\/identifier><datestamp>\n      2011-11-25T10:57:04Z<\/datestamp><setSpec>\n      74797065733D4445505453:4C53452D5354<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lse.ac.uk\/30992\/<\/dc:relation><dc:title>\n        The Dantzig selector in Cox's proportional hazards model<\/dc:title><dc:creator>\n        Antoniadis, Anestis<\/dc:creator><dc:creator>\n        Fryzlewicz, Piotr<\/dc:creator><dc:creator>\n        Letu\u00e9, Fr\u00e9d\u00e9rique<\/dc:creator><dc:subject>\n        HA Statistics<\/dc:subject><dc:description>\n        The Dantzig selector (DS) is a recent approach of estimation in high-dimensional linear regression models with a large number of explanatory variables and a relatively small number of observations. As in the least absolute shrinkage and selection operator (LASSO), this approach sets certain regression coefficients exactly to zero, thus performing variable selection. However, such a framework, contrary to the LASSO, has never been used in regression models for survival data with censoring. A key motivation of this article is to study the estimation problem for Cox's proportional hazards (PH) function regression models using a framework that extends the theory, the computational advantages and the optimal asymptotic rate properties of the DS to the class of Cox's PH under appropriate sparsity scenarios. We perform a detailed simulation study to compare our approach with other methods and illustrate it on a well-known microarray gene expression data set for predicting survival from gene expressions.<\/dc:description><dc:publisher>\n        John Wiley<\/dc:publisher><dc:date>\n        2010-12<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lse.ac.uk\/30992\/1\/The_Dantzig_selector_in_Cox%27s_proportional_hazards_model%28lsero%29.pdf<\/dc:identifier><dc:identifier>\n          Antoniadis, Anestis and Fryzlewicz, Piotr and Letu\u00e9, Fr\u00e9d\u00e9rique  (2010) The Dantzig selector in Cox's proportional hazards model.  Scandinavian Journal of Statistics, 37 (4).  pp. 531-552.  ISSN 0303-6898     <\/dc:identifier><dc:relation>\n        http:\/\/onlinelibrary.wiley.com\/journal\/10.1111\/(ISSN)1467-9469<\/dc:relation><dc:relation>\n        10.1111\/j.1467-9469.2009.00685.x<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lse.ac.uk\/30992\/","http:\/\/onlinelibrary.wiley.com\/journal\/10.1111\/(ISSN)1467-9469","10.1111\/j.1467-9469.2009.00685.x"],"year":2010,"topics":["HA Statistics"],"subject":["Article","PeerReviewed"],"fullText":" \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nAnestis Antoniadis, Piotr Fryzlewicz and Fr\u00e9d\u00e9rique Letu\u00e9 \nThe Dantzig selector in Cox's proportional \nhazards model \n \n \n \nArticle (Accepted version) \n(Unrefereed) \n \n \n \n \nOriginal citation: \nAntoniadis, Anestis and Fryzlewicz, Piotr and Letu\u00e9, Fr\u00e9d\u00e9rique (2010) The Dantzig selector in \nCox's proportional hazards model.  Scandinavian journal of statistics, 37 (4). pp. 531-552. ISSN \n0303-6898 \nDOI:  10.1111\/j.1467-9892.2008.00586.x \n \n\u00a9 2010 John Wiley & Sons \n \nThis version available at: http:\/\/eprints.lse.ac.uk\/30992\/ \nAvailable in LSE Research Online: November 2011 \n \nLSE has developed LSE Research Online so that users may access research output of the \nSchool. Copyright \u00a9 and Moral Rights for the papers on this site are retained by the individual \nauthors and\/or other copyright owners. Users may download and\/or print one copy of any \narticle(s) in LSE Research Online to facilitate their private study or for non-commercial research. \nYou may not engage in further distribution of the material or use it for any profit-making activities \nor any commercial gain. You may freely distribute the URL (http:\/\/eprints.lse.ac.uk) of the LSE \nResearch Online website. \n \nThis document is the author\u2019s final accepted version of the journal article. There may be \ndifferences between this version and the published version.  You are advised to consult the \npublisher\u2019s version if you wish to cite from it. \nTHE DANTZIG SELECTOR IN COX\u2019S\nPROPORTIONAL HAZARDS MODEL\nRunning headline : Dantzig selector fox Cox\u2019s model\nAnestis Antoniadis,\nLaboratoire Jean Kuntzmann, D\u00e9partement de Statistique,\nUniversit\u00e9 Joseph Fourier, France\nPiotr Fryzlewicz,\nDepartment of Statistics, London School of Economics, UK\nand\nFr\u00e9d\u00e9rique Letu\u00e9,\nLaboratoire Jean Kuntzmann, D\u00e9partement de Statistique,\nUniversit\u00e9 Pierre Mend\u00e8s France, France.\nOctober 6, 2009\n1\nDantzig selector fox Cox\u2019s model 2\nAbstract\nThe Dantzig Selector is a recent approach to estimation in high-dimensional\nlinear regression models with a large number of explanatory variables and a\nrelatively small number of observations. As in the least absolute shrinkage\nand selection operator (LASSO), this approach sets certain regression coefficients\nexactly to zero, thus performing variable selection. However, such a framework,\ncontrary to the LASSO, has never been used in regression models for survival data\nwith censoring. A key motivation of this article is to study the estimation problem\nfor Cox\u2019s proportional hazards function regression models using a framework that\nextends the theory, the computational advantages and the optimal asymptotic rate\nproperties of the Dantzig selector to the class of Cox\u2019s proportional hazards under\nappropriate sparsity scenarios. We perform a detailed simulation study to compare\nour approach to other methods and illustrate it on a well-known microarray gene\nexpression data set for predicting survival from gene expressions.\nKey words: Dantzig selector, generalized linear models, Lasso, penalized partial\nlikelihood, proportional hazards model, variable selection\n1 Introduction\nAn objective of survival analysis is to identify the risk factors and their risk\ncontributions. Often, many covariates are collected and, to reduce possible modelling\nbias, a large parametric model is built. An important and challenging task is then\nvariable selection which is a form of model selection in which the class of models\nunder consideration is represented by subsets of covariate components to be included\nin the analysis. Variable selection methods are well developed in linear regression\nsettings and in recent years many of them have been extended to the context of\ncensored survival data analysis. They include best-subset selection (Jovanovic et al.,\nDantzig selector fox Cox\u2019s model 3\n1995), stepwise selection (Delong et al., 1994), asymptotic procedures based on score\ntests, Wald tests and other approximate chi-squared testing procedures (Harrell, 2001),\nbootstrap procedures (Graf et al., 1999) and Bayesian variable selection (Faraggi &\nSimon, 1998; Ibrahim et al., 2008). However, theoretical properties of these methods\nhave not been fully validated (Fan & Li, 2002).\nA family of penalized partial likelihood methods, such as the LASSO (Tibshirani,\n1997) and the smoothly clipped absolute deviation method (SCAD, Fan & Li, 2002)\nwere proposed for Cox\u2019s proportional hazards model. By shrinking some regression\ncoefficients to zero, these methods select important variables and estimate the\nregression model simultaneously. The LASSO estimator does not possess the oracle\nproperties (Fan & Li, 2002). The SCAD estimator has better theoretical properties than\nthe LASSO, but the nonconvex form of its penalty makes its computation challenging\nin practice, and the solutions may suffer from numerical instability (see Zou, 2008).\nAn adaptive LASSO method based on a penalized partial likelihood with adaptively\nweighted L1 penalties on regression coefficients developed by Zhang & Lu (2007)\nenjoys the oracle properties of the SCAD estimator but the optimization problem\ncannot be efficiently solved by standard algorithms.\nRecently, Cand\u00e8s & Tao (2007) proposed the Dantzig selector (DS, for short) for\nperforming model fitting for linear regression models where the number of variables\ncan be much larger than the sample size but the set of coefficients is sparse, i.e. most\nof the coefficients are zero. Unlike most other procedures such as the LASSO and the\nSCAD, whichminimize the sum of squared errors subject to a penalty on the regression\ncoefficients, the Dantzig Selector minimizes the L1 norm of the coefficients subject to a\nconstraint on the error terms. As with the LASSO, SCAD or the adaptive LASSO, this\napproach sets certain coefficients exactly to zero, thus performing variable selection.\nHowever, unlike the other methods, standard linear programming methods can be\nused to compute the solution to the Dantzig selector, providing a computationally\nefficient algorithm. Also, the resulting estimated coefficients enjoy near-optimal `2\nnon-asymptotic error bounds. Hence, the Dantzig selector appears to be an appealing\nDantzig selector fox Cox\u2019s model 4\nestimation procedure for sparse linear regression models and this motivates us to\nextend its theoretical and computational advantages to the class of semi-parametric\nCox\u2019s proportional hazards models. The proposed method uses the partial likelihood\nfunction as an overall loss function. Since it compares favourably with other methods,\nas indicated in our simulation study, we view it as a useful addition to the toolbox of\nestimation and prediction methods for the widely used Cox\u2019s model.\nIncidentally, after this work was completed, more recent work by Martinussen &\nScheike (2009b) has appeared, where the Dantzig selector is studied in the setting\nof a semiparametric Aalen additive hazards model using, instead of Cox\u2019s partial\nlikelihood, an appropriate least-squares criterion as a loss function.\nThe paper is organized as follows. The usual survival data setup for (generalized)\nCox\u2019s regression model with time-independent covariates is introduced in Section 2,\nrecalling the basic ideas of Cox\u2019s original proportional model for the hazard rates.\nIn particular, we briefly recall in this section the appropriate framework needed to\nrepresent this model in a martingale notation based on theories of counting processes\n(see e.g. Andersen & Gill, 1982). In Section 3, after outlining the approach behind the\nDantzig Selector for linear regression models, we introduce our Dantzig Selector for\nproportional hazards (PH) models and develop a computationally efficient algorithm\nfor computing the estimator. Section 3 also contains our main assumptions and\ntheoretical results concerning the estimator, the main result relating to its l2 error,\nin analogy with Cand\u00e8s and Tao\u2019s (Cand\u00e8s & Tao, 2007) results for linear models.\nIn Section 4, we present a simulation study comparing the proposed approach with\nvarious competitors, while in Section 5 we present an application of our method on a\nwell-knownmicroarray gene expression data set, used previously for similar purposes\nin the literature (B\u00f8velstad et al., 2007). Some conclusions are given in Section 6. Proofs\nof main and intermediate results are in the Appendix.\nR software implementing our Dantzig selector for survival data can be downloaded\nfrom\nhttp:\/\/stats.lse.ac.uk\/fryzlewicz\/dscox\/ds_cox.html.\nDantzig selector fox Cox\u2019s model 5\n2 Notation and preliminaries\nIn order to fix the notation we consider the usual survival data setup. The\nreader unfamiliar with the concepts described in this Section is referred to the book\nby Andersen et al. (1993). The survival time X is assumed to be conditionally\nindependent of a censoring time U given the p-dimensional vector of covariates\nZ = (Z1,Z2, . . . ,Zp)T so that the construction of the partial likelihood is justified.\nWe observe n i.i.d. copies (X\u02dci,Di,Zi), i = 1, . . . , n, of the right censored survival\ntime X\u02dc = min(X,U) and the censoring indicator D = I[X \u2264 U] = I[X\u02dc = X].\nThe covariates are assumed to be bounded: there exists a positive constant C such\nthat sup1\u2264j\u2264p |Zj| \u2264 C. This assumption is fully justified in the fixed design case,\nand is used in our theoretical calculations regarding the performance of our estimator.\nHowever, we emphasise that in practice, our computational algorithm makes no use\nof either the assumption itself or the (possibly unknown) value of the constant C.\nThus this assumption should not be viewed as restrictive, even in the random design\ncase. We also note that in cases where Zj represent gene expressions measured on a\nmicroarray, they are naturally bounded by virtue of the measurement process.\nIn the following we will denote by Z the n \u00d7 p matrix whose generic term Zij is\nthe ith observed value of the jth covariate Zj, and the ith row of Z will be denoted by\nzTi . For simplicity, we will also assume that there are no tied failure times; suitable\nmodifications of the partial likelihood exist for the case of predictable and locally\nbounded covariate processes and for the case of ties (see Andersen et al., 1993). Most\noften in the literature, proportional hazards models are formulated using random\nvariables (as opposed to stochastic processes), and the implied statistical methods are\nbased on maximum (partial) likelihoods. However, we prefer studying such problems\nin terms of the theory of counting processes, since time and random phenomena\noccurring in time play an essential role in survival analysis. Moreover, this counting\nprocess approach has been facilitated by the work of Andersen & Gill (1982) and\npermits us to use martingale convergence results in a unified way to demonstrate\nDantzig selector fox Cox\u2019s model 6\ntheoretical properties of our approach.\nIn the counting process setup, we can represent the observed data as follows.\nThe regression model for survival data, described above, is linked to the multivariate\ncounting process N = (N1, . . . ,Nn) of the form, Ni(t) = I(X\u02dci \u2264 t,Di = 1) where the\nNi\u2019s are independent copies of the single-jump counting process N(t) = I(X\u02dc \u2264 t,D =\n1) which registers whether an uncensored failure (or death) has occurred by time t.\nLet Y(t) = I[X\u02dc \u2265 t] be the corresponding \u201cat risk\u201d indicator. Define the filtration\nFt = F0 \u2228 {N(u),Y(u); u \u2264 t}, where F0 = \u03c3(Z). Under the true probability measure\nP on F = Ft, the counting processes Ni(t) have intensity processes \u03bbi(t, zi) and under\nthe Cox regression model, the conditional intensities \u03bbi(t, zi) of Ni given Zi = zi for t\nrestricted to a fixed time interval [0, \u03c4] are\n\u03bbi(t, zi) = Yi(t)\u03b10(t) exp(zTi \u03b20) (1)\nwhere \u03b10 is the baseline hazard function and \u03b20 is the unknown vector of regression\ncoefficients. For flexibility of fit, the baseline hazard function is left unspecified and\nour setting is therefore semiparametric. This, in particular, means that\nMi(t) = Ni(t)\u2212\n\u222b t\n0\n\u03bbi(u, zi)du, t \u2208 [0, \u03c4],\nare independent Ft square-integrable martingales under P with compensator Vi(t) =\u222b t\n0 \u03bbi(u, zi)du. In particular, we have\n\u3008Mi,Mi\u3009(t) =\n\u222b t\n0\n\u03bbi(u, zi)du = Vi(t).\nUnder the above notation, the (rescaled by 1\/n) Cox\u2019s partial loglikelihood function\nis given by\nl(\u03b2) =\n1\nn\nn\n\u2211\ni=1\nzTi \u03b2\n\u222b \u03c4\n0\ndNi(u)\u2212\n\u222b \u03c4\n0\nlog\n(\nn\n\u2211\ni=1\nYi(u) exp(zTi \u03b2)\n)\ndN\u00af(u)\nn\n,\nwhere dN\u00af(u) = d\u2211ni=1 Ni(u). Let Sn(\u03b2, u) = \u2211\nn\ni=1 Yi(u) exp(z\nT\ni \u03b2). Then\nl(\u03b2) =\n1\nn\nn\n\u2211\ni=1\nzTi \u03b2\n\u222b \u03c4\n0\ndNi(u)\u2212\n\u222b \u03c4\n0\nlog (Sn(\u03b2, u))\ndN\u00af(u)\nn\n.\nDantzig selector fox Cox\u2019s model 7\nDefine the first and second order partial derivative of Sn(\u03b2, u) with respect to \u03b2:\nS1n(\u03b2, u) =\nn\n\u2211\ni=1\nYi(u) exp(zTi \u03b2)zi and S\n2\nn(\u03b2, u) =\nn\n\u2211\ni=1\nYi(u) exp(zTi \u03b2)z\n\u2297\n2\ni , (2)\nwhere z\n\u2297\n2 = zzT. The maximum likelihood estimator of \u03b2 in Cox\u2019s model, is found as\nthe solution to the score equation U(\u03b2\u02c6) = 0, where the score process U(\u03b2) is defined\nby\nU(\u03b2) =\n\u2202l(\u03b2)\n\u2202\u03b2\n=\n1\nn\nn\n\u2211\ni=1\n\u222b \u03c4\n0\n(zi \u2212 E(u, \u03b2))dNi(u),\nwith E(u, \u03b2) = S\n1\nn(\u03b2,u)\nSn(\u03b2,u)\n. In particular, for the true parameter \u03b2 = \u03b20, we have:\n(U(\u03b20))j =\n(\n\u2202l(\u03b2)\n\u2202\u03b2 j\n)\n\u03b20\n=\n1\nn\nn\n\u2211\ni=1\nZij\n\u222b \u03c4\n0\ndMi(u)\u2212\n\u222b \u03c4\n0\nS1n(\u03b20, u)\nSn(\u03b20, u)\ndM\u00af(u)\nn\n,\nwhere dM\u00af(u) = d\u2211ni=1 Mi(u). Thus the score process evaluated at the true parameter\n\u03b2 = \u03b20 is itself a martingale and this fact, together with standard regularity\nassumptions, facilitates the study of the asymptotic properties of the MLE estimator\nof the vector of regression coefficients.\nIn practice, not all the covariates (components of Z) may contribute to the prediction\nof survival outcomes: some components of \u03b2 in the true model may be zero. Our\nDantzig selection procedure, described in the next section, works under this \u201csparsity\u201d\nassumption and produces consistent and easily computable estimates of the relevant\ncoefficients.\n3 Dantzig selector for Cox\u2019s regression model\nTheoretical properties of LASSO and SCAD for Cox\u2019s proportional hazard model have\nbeen investigated in literature. These penalized partial likelihood methods may be\nviewed, in an asymptotic sense, as instances of iteratively re-weighted least squares\nprocedures by transferring the objective functions involved in the optimization into\nasymptotically equivalent least-squares problems. Indeed, as noted by Wang & Leng\nDantzig selector fox Cox\u2019s model 8\n(2007), when p is fixed and is smaller that n, using the asymptotic theory for the MLE\nestimator \u03b2\u02dc of \u03b2 in a standard Cox\u2019s regression model, the negative log-likelihood\nfunction can be replaced locally by a Taylor series expansion at \u03b2\u02dc leading to a least\nsquares penalized criterion which is updated iteratively (LASSO Estimation via Least\nSquares Approximation (LSA)). As shown by Wang & Leng (2007), their resulting\nLSA estimators are often asymptotically as efficient as oracle as long as the number\nof components p remains fixed and the tuning parameters are chosen appropriately.\nIn our case, we do not want to restrict ourselves to the standard p < n setup, but we\nwould also like to examine the case where pmay growwith, and exceed, n, i.e. the case\nof a (fast) growing dimension of the predictor. This is indeed part of our motivation\nfor proposing the Dantzig selector. However, in order to justify the algorithm that\nnumerically implements our procedure, we will make some use of the above remarks\nabout LSA.\n3.1 Dantzig selector for linear regression\nThe Dantzig Selector (Cand\u00e8s & Tao, 2007)) was designed for linear regression models\nY = Z\u03b2+ e, (3)\nwith a large p but a sparse set of coefficients, i.e. where most of the regression\ncoefficients \u03b2 j are zero. For the linear regression model given by (3), the Dantzig\nSelector estimate, \u03b2\u02c6, is defined as the solution to\nmin\n\u03b2\u2208B\n\u2016\u03b2\u20161 subject to |ZjT(Y\u2212 Z\u03b2)| \u2264 \u03bb, j = 1, . . . , p, (4)\nwhere \u2016 \u00b7 \u20161 is the L1 norm, Zj is the jth column of Z, \u03bb is a tuning parameter and\nB represents the set of possible values for \u03b2, usually taken to be a subset of Rp. The\nL1 norm minimization produces coefficient estimates that are exactly zero in a similar\nfashion to the LASSO and hence can be used as a variable selection tool. In this setup Zj\nis assumed to be norm one which is rarely the case in practice. However, this difficulty\nis easily resolved by reparameterizing (3) such that the Zj\u2019s do have norm one.\nDantzig selector fox Cox\u2019s model 9\nNotice that for Gaussian error terms, (4) can be rewritten as,\nmin\n\u03b2\u2208B\n\u2016\u03b2\u20161 subject to |`\u2032j(\u03b2)| \u2264 \u03bb\/\u03c32, j = 1, . . . , p, (5)\nwhere `\u2032j is the partial derivative of the log likelihood function with respect to \u03b2 j and\n\u03c32 = Var(ej). Hence, an intuitive motivation for the Dantzig Selector, as also observed\nby James & Radchenko (2009), is that, for \u03bb = 0, the solution to (5) will return the\nmaximum likelihood estimator. For \u03bb > 0, the Dantzig Selector searches for the \u03b2\nwith the smallest L1-norm that is within a given distance of the maximum likelihood\nsolution, i.e. the sparsest \u03b2 that is still reasonably consistent with the observed data.\nNotice that even for p > n, where the likelihood equation will have infinite possible\nsolutions, this approach can still hope to identify a unique solution, provided \u03b2 is\nsparse, because it is only attempting to locate the sparsest \u03b2 close to the peak of the\nlikelihood function.\nThe Dantzig Selector has two main advantages. The first is that (4) can be\nformulated as a standard linear programming problem. The second main advantage\nis theoretical. Cand\u00e8s and Tao (2007) proved tight non-asymptotic bounds on the error\nin the estimator for \u03b2, a result which has recently attracted a lot of attention since it\ndemonstrated that the L2-error in estimating \u03b2 was within a factor of log p of that one\ncould achieve if the true model were known. More precisely, suppose that that ei are\ni.i.d. N(0, \u03c32) variables and that \u03b2 has at most S non-zero components. Assume also\nthat a Uniform Uncertainty Principle (UUP) condition holds on the design matrix, i.e.\nsuppose that the Gram matrix \u03a8 = 1nZ\nTZ is such that \u03a8ii = 1 for all i = 1, . . . , p\nand maxi 6=j |\u03a8i,j| \u2264 13\u03b1S for some \u03b1 > 1 (see Lounici, 2008). Then for any a \u2265 0 and\n\u03bb = \u03c3\n\u221a\n2(1+ a)(log p)\/n, the Dantzig selector estimator satisfies\n\u2016\u03b2\u02c6\u2212 \u03b2\u201622 \u2264 (1+ a) \u00b7 C \u00b7 S \u00b7 \u03c32 \u00b7 (log p)\/n, (6)\nwith probability close to 1. Even if we knew ahead of time which \u03b2 j\u2019s were non-\nzero, under the same conditions on the design Gram matrix, it would still be the\ncase that \u2016\u03b2\u02c6 \u2212 \u03b2\u201622 grew at the rate of S \u00b7 \u03c32\/n. Hence the rate is optimal up to a\nDantzig selector fox Cox\u2019s model 10\nfactor of log p, and we only pay a small price for adaptively choosing the significant\nvariables. As mentioned before, equation (6) holds for Gaussian errors with a linear\nregression model. Our purpose is to extend the Dantzig estimator, algorithm and the\nabove theoretical bounds to the general class of Cox\u2019s proportional hazards regression\nmodels introduced in Section 2. To our knowledge this is the first time that bounds of\nthis form have been proposed for such models.\n3.2 Survival Dantzig Selector\nWe have already observed that for Gaussian errors in a linear regression model, the\ninner product between the jth covariate and the vector of residuals, ZjT(Y \u2212 Z\u03b2)\nis proportional to the jth component `\u2032j(\u03b2) of the score vector. Hence, the Dantzig\noptimization criteria given by (4) and (5) can be extended to the class of Cox\u2019s PH\nregression models in a natural fashion by computing the solution \u03b2\u02c6 of\nmin\n\u03b2\u2208IRp\n\u2016\u03b2\u20161 subject to \u2016U(\u03b2)\u2016\u221e \u2264 \u03b3, (7)\nwhere \u03b3 \u2265 0 and U(\u03b2) is the score process. Note that such a solution exists because\nthe negative of the loglikelihood is a convex function of \u03b2. We will call the resulting\nprocedure the Survival Dantzig Selector (SDS for short). The purpose of this subsection\nis to show that, under appropriate assumptions on the information matrix of the\ncorresponding point process, the resulting SDS estimator maintains all the important\nproperties of the Dantzig selector.\nIn order to prove our main results we will partially proceed along similar lines to\nCand\u00e8s and Tao\u2019s (2007) original result on the DS and wewill need for that the fact that\n\u2016\u03b2\u02c6\u20161 \u2264 \u2016\u03b20\u20161. However, while for Gaussian errors in a sparse linear regression model,\nsuch an inequality is \u201cautomatic\u201d (it follows from obvious concentration properties of\ncentered Gaussian measures), this is not the case in our general point process setup,\nand, indeed, it is implied by Lemma 1 stated below and proved in Section 7. The\nnumber of predictors p = pn is allowed to grow (fast) with the sample size n.\nDantzig selector fox Cox\u2019s model 11\nLemma 1 Assume that the dimension of predictor in Cox\u2019s PH model satisfies pn = O(n\u03be),\nn \u2192 \u221e, for some 1 < \u03be. Assume also that the number S of effective predictors, i.e. the number\nof \u03b20 j,n 6= 0 is independent of n and finite (S-sparsity of \u03b20). Let \u03b3 = \u03b3n,p =\n\u221a\n(1+a) log pn\u221a\nn for\nsome a > 0. Under the additional assumptions that\n\u2022 the baseline hazard function in eq. (1) is such that\n\u222b\n\u03b10(u)du < +\u221e\n\u2022 sup1\u2264i\u2264n sup1\u2264j\u2264pn |Zij| \u2264 C,\nit follows that\nP{\u2016U(\u03b20)\u2016\u221e \u2265 \u03b3n,p} \u2264 pn exp\n(\n\u2212 n\u03b3\n2\np,n\n2(2C\u03b3p,n + K)\n)\n= O\n(\nn\u2212a\u03be\n)\n,\nwith K > 0 a suitable constant. It follows that, as n \u2192 \u221e, with probability tending to 1, the\ntrue \u03b20 is admissible for problem (7) , i.e. \u2016U(\u03b20)\u2016\u221e < \u03b3 and in particular \u2016\u03b2\u02c6\u20161 \u2264 \u2016\u03b20\u20161.\nRemark 1 The scaling 1\/\n\u221a\nn in \u03b3n,p in the above lemma comes from the scaling 1\/n we chose\nin the log-likelihood. This choice is also made by Bickel et al. (2009) and Lounici (2008). Note\nalso that the result of Lemma 1 is taken for granted in the extension of the DS to the class of\ngeneralized linear models derived by James & Radchenko (2009), but it is not automatically\ntrue. Finally, note that we allow for a large predictor dimension relative to the sample size\nn as long as \u03be > 1 and the S-sparsity assumption holds. The other assumptions about the\nboundedness of the predictor variables and the baseline hazard are standard under Cox\u2019s PH\nmodel (Andersen et al., 1993).\nIn order to obtain error bounds on the components selected by our Survival Dantzig\nSelector, we introduce a few definitions that are closely related to those from Cand\u00e8s &\nTao (2007).\nGiven an n\u00d7 p matrix A and an index set T \u2282 {1, . . . , p} we will write AT for the\nn\u00d7 |T|matrix constructed by extracting the columns of A corresponding to the indices\nin T. The quantities defined below depend on A but this will be omitted to simplify\nDantzig selector fox Cox\u2019s model 12\nthe notation. If this dependency is needed we will denote them with a superscript A.\nAs in Cand\u00e8s & Tao (2007), for any integer S \u2264 p, \u03b4S is the largest quantity such that\n\u2016ATc\u201622 \u2265 \u03b4S\u2016c\u201622\nfor all subsets T with |T| \u2264 S and all vectors c of length |T|. If A is an orthonormal\nmatrix, then \u2016ATc\u20162 = \u2016c\u20162 for all T, c and hence \u03b4S = 1. If some columns of A are\nlinearly dependent then for a certain T and c, \u2016ATc\u20162 = 0 and hence \u03b4S = 0.\nIf S+ S\u2032 \u2264 p, we also define \u03b8S,S\u2032 as the smallest quantity such that\n|(ATc)TAT\u2032c\u2032| \u2264 \u03b8S,S\u2032\u2016c\u20162\u2016c\u2032\u20162\nfor all disjoint subsets T and T\u2032 with |T| \u2264 S and |T\u2032| \u2264 S\u2032 and all corresponding\nvectors c and c\u2032. Note that when the columns of A are orthogonal then \u03b8S,S\u2032 = 0.\nBefore stating our main result, we recall that the p \u00d7 p observed \u201cinformation\u201d\nmatrix up to time \u03c4 corresponding to Cox\u2019s proportional model is given by (see e.g.\nAndersen & Gill, 1982):\nJ(\u03b2, \u03c4) = Jn(\u03b2, \u03c4) =\n\u222b \u03c4\n0\n[\nS2n\nSn\n(\u03b2, u)\u2212 (S\n1\nn\nSn\n)\u22972(\u03b2, u)]dN\u00afn(u)\nn\n,\nwith notation as in (2). For a fixed sparsity parameter S, as n tends to infinity, it tends\nin probability (see Theorem VII.2.2 in Andersen et al., 1993) to the p\u00d7 pmatrix of rank\nS\nI(\u03b2, \u03c4) =\n\u222b \u03c4\n0\n[\ns2\ns\n(\u03b2, u)\u2212 ( s\n1\ns\n)\u22972(\u03b2, u)]s(\u03b2, u)\u03b10(u)du,\nwhere s(\u03b2, u) = E(Sn(\u03b2, u)\/n), s1(\u03b2, u) = E(S1n(\u03b2, u)\/n), s2(\u03b2, u) = E(S2n(\u03b2, u)\/n).\nFinally, when derivatives defining s(\u03b2, u), s1(\u03b2, u) and s2(\u03b2, u) are computed only\nwith respect to the components of the true S-dimensional vector \u03b20, the true S \u00d7 S\ninformation matrix, not be confused with the p\u00d7 p matrix I(\u03b20, \u03c4) (of rank S) which\nis the asymptotic limit of J(\u03b20, \u03c4), will be denoted by I(\u03b20, \u03c4). Applying Theorem\n7.2.6 of Horn & Johnson (1985) with k = 2, we will denote hereafter V1\/2 the unique\n(semi)definite positive square root matrix of a (semi)definite positive matrix V.\nLet \u03b3 = \u03b3n,p be a tuning parameter. We now state our main theoretical result in\nTheorem 1 below. The proof is in the Appendix.\nDantzig selector fox Cox\u2019s model 13\nTheorem 1 Suppose that the true vector of coefficients \u03b20 \u2208 Rp is a nonzero S-sparse\ncoefficient vector with S independent of n, such that the coefficients \u03b4 and \u03b8 for the matrix\nI1\/2(\u03b20, \u03c4) obey \u03b8S,2S < \u03b42S. Assume that the assumptions used in Lemma 1 hold and let \u03b2\u02c6 be\nthe estimate from the SDS using tuning parameter \u03b3 = \u03b3n,p with \u03b3n,p as in Lemma 1. Then,\nas long as the information matrix I(\u03b20, \u03c4) is positive definite at \u03b20, we have:\nP\n(\n\u2016\u03b2\u02c6\u2212 \u03b20\u201622 > 64S(\n\u03b3\n\u03b42S \u2212 \u03b8S,2S )\n2\n)\n\u2264 O(n\u2212a\u03be).\nThe assumptions of Theorem 1 are similar to the assumption \u03b4+ \u03b8 < 1 made for the\nDantzig selector in standard linear models by Cand\u00e8s & Tao (2007) and the assumption\n\u2206K > 0 made for sparse generalized linear models by James & Radchenko (2009). The\npositive-definiteness of I(\u03b20, \u03c4) is classical in survival analysis (condition VII.2.1 of\nAndersen et al., 1993). While one appealing property of the DS is the fact that the error\nbound can be established for set-ups satisfying the UUP (the condition \u03b8S,2S < \u03b42S from\nTheorem 1), in our case this leads to a condition on the \u201cinformation matrix\" I(\u03b20, \u03c4)\nrather than directly on the design as in the standard least squares regression setting.\nThe verifiability of the UUP condition is also an issue, whatever setting one\nconsiders, since, as dimensionality grows, the UUP condition becomes more and more\ndifficult to satisfy as important predictors can be highly correlated with some other\nunimportant predictors. In summary, assessing this condition in our case is not easy\nand the difficulty is twofold: firstly, the condition is formulated on the asymptotic\nmatrix I and not on the observable finite-sample matrix J . Secondly, to verify the\ncondition, even if we are prepared to work with the estimated information and with an\na priori upper bound on the sparsity parameter S, we need to spend an \u201cexponential\u201d\namount of time verifying the two inequalities that define the UUP condition stated\nin Theorem 1, especially when p is large. It is certainly of interest to relax the UUP\nconditionwhenworkingwith a concrete data-set and oneway to reach an upper bound\non p that achieves this is to extend and use a concept similar to that of sure screening\nproposed recently by Fan & Lv (2008) in the linear regression setting to reduce high\ndimensionality to a relatively smaller scale, possibly below the sample size. We do not\nDantzig selector fox Cox\u2019s model 14\npursue this in this paper.\nThe above theorem depends on the rate at which p is allowed to increase with\nthe number of observations n. Under the usual regularity assumptions for our point\nprocess (similar to those of Andersen & Gill (1982), Theorem 4.1) our choice of the\nthreshold \u03b3 leads to an optimal (a rate that is similar to the one obtained for the\nclassical Dantzig selector in linear models by Cand\u00e8s & Tao (2007)), up to a log p factor,\nsquared error bound for the SDS estimator \u03b2\u02c6, provided that S remains small. Under\nsuch conditions the SDS will give accurate results even for values of p that are larger\nthan n.\n3.3 An algorithm for computing the SDS\nIn this section, we propose an iterative weighted Dantzig selector algorithm for\ncomputing the SDS solution for a given value of \u03b3.\nNote that the constraints in (7) are non-linear, so linear programming software\ncannot directly be used to compute the SDS solution. As noted in the Introduction,\nin a standard GLM setting, an iterative weighted least squares algorithm is usually\nused to solve the system of score equations. More precisely, given a current estimate\nfor \u03b2\u02c6, an adjusted dependent variable is computed, and a new estimate for \u03b2 is then\ncomputed using weighted least squares. This procedure is iterated until \u03b2\u02c6 converges.\nFor more details the reader is referred to McCullagh & Nelder (1989). An analogous\niterative approach works well in computing the SDS solution. We can describe it as\nfollows.\nFor any fixed \u03b3:\n1. At the (k + 1)st iteration, compute the gradient vector U(\u03b2\u02c6(k)) and the Hessian\nmatrix J(\u03b2\u02c6(k), \u03c4), where (k) denotes the corresponding estimate from the kth\niteration. Consider the unique square root of the matrix J(\u03b2\u02c6(k), \u03c4), i.e. J(\u03b2\u02c6(k), \u03c4) =\nA2(k), and set the pseudo response vector Y = (A(k))\n\u2212{J(\u03b2\u02c6(k), \u03c4)\u03b2\u02c6(k \u2212U(\u03b2\u02c6(k))},\nwhere V\u2212 denotes the Moore-Penrose generalized inverse of V. This amounts to\nDantzig selector fox Cox\u2019s model 15\napproximating Cox\u2019s partial likelihood at the current estimate by the quadratic\nform\n1\n2\n(Y\u2212 A(k)\u03b2)T(Y\u2212 A(k)\u03b2). (8)\n2. Re-parameterize A(k) say to A?(k) such that its columns have norm one andmodify\naccordingly Y to Y? to produce the SDS estimate of \u03b2 at the original scale.\n3. Use Candes and Tao\u2019s (2007) Dantzig selector to compute \u03b2\u02c6\n(k+1)\nusing Y? as the\nresponse and A?(k) as the design matrix.\n4. Repeat steps 1 through 3 until convergence.\nNote that numerical implementation of step 3 requires only a linear programming\nalgorithm.\nThe above approach is closely related to the standard GLM approach and relies\nupon an appropriate linearization, equivalent to making a quadratic approximation of\nthe partial log-likeilhood in the censored case instead of the usual log-likelihood in the\nstandard GLM approach. Another difference between the standard GLMmethodology\nand our algorithm is that the (linear) Dantzig selector is used in step 3.\nThis algorithm gives exact zeros for some coefficients and it converges quickly\nbased on our empirical experience. However, as with the standard GLM iterative\nalgorithm, there is no theoretical proof that the algorithm is guaranteed to converge to\nthe global minimizer of (7). Especially in the case n < p, instead of using a Moore-\nPenrose inverse for the possibly semi-positive definite matrix A(k) in the previous\nalgorithm, we could have used, as it is done in ridge regression, the square root of\nthe positive definite matrix J(\u03b2\u02c6(k), \u03c4) + \u00b5Ip for a small \u00b5 > 0.\nTo estimate the tuning parameter \u03b3, we use generalized cross-validation (Craven &\nWahba (1979)). Let \u03bd = \u03b3\u22121 and V(\u03b2\u02c6) be the diagonal matrix with diagonal entries\n1\/\u03b2\u02c62i when \u03b2\u02c6\n2\ni > 0 and 1 when \u03b2\u02c6i = 0. At convergence, the minimizer of (8) in\nstep 1 can be approximated by the ridge solution (J(\u03b2\u02c6, \u03c4) + \u03bdV(\u03b2\u02c6))\u22121ATY. Therefore,\nthe number of effective parameters in the SDS estimator can be approximated by\nDantzig selector fox Cox\u2019s model 16\np(\u03bd) = tr\n(\n(J(\u03b2\u02c6, \u03c4) + \u03bdV(\u03b2\u02c6))\u22121 J(\u03b2\u02c6, \u03c4)\n)\nand the generalized cross-validation function\nis GCV(\u03bd) = \u2212`(\u03b2\u02c6)\/[n(1\u2212 p(\u03bd)\/n)2]. If \u03bd\u02c6 minimizes GCV(\u03bd) then \u03b3 is chosen to be\n1\/\u03bd\u02c6. We used the above algorithm both in the simulation study and in the real data\nanalysis, reported below.\n4 Simulation study\nIn this section, we present the results of a simulation study conducted to evaluate the\nperformance of the SDS in comparisonwith three other approaches which include both\nstate-of-the-art and classical methods. To keep the scope of the study manageable,\nwe only included a limited number of methods in our comparison. We feel that the\ncurrent selection covers the spectrum of existingmethods reasonably well: one of them\nis similar to the Lasso but better (Gui & Li, 2005), the other one is known to be an\nexcellent predictor while the third one is simple and standard. We briefly describe\nbelow the methods to which the comparisons with SDS are made, namely Partial Cox\nregression with one or two retained components (PLS Cox), Cox regression with the\nsubset of 20 \u201cbest\u201d genes (Cox20) and the threshold gradient descent procedure (TGD)\nfor the Cox model by Gui & Li (2005).\nPartial Cox Regression. Nguyen & Rocke (2002) proposed the use of the partial least\nsquares (PLS) algorithm for the prediction of survival with gene expression.\nThis method, however, does not handle the censoring aspect of the survival\ndata properly. PLS for Cox regression handling censoring has been developed\nfor analyzing genome-wide data in Nyg\u00e5rd et al. (2008). We adopted the\nclosely related approach of Park et al. (2002) in which the full likelihood for\nCox\u2019s model is reformulated as the likelihood of a Poisson model, i.e. a\ngeneralized linear model (GLM). This reformulation enables application of the\niteratively reweighted partial least squares (IRPLS) procedure for GLM (Marx,\n1996). We used the implementation of the PLS algorithm of Park et al. (2002) in\nDantzig selector fox Cox\u2019s model 17\nR provided by Boulesteix & Strimmer (2007) where the PLS components depend\nsolely on the gene expressions. The interpretation of components is generally\nnot straightforward, especially if the number of genes that contribute to the\ncomponent becomes large. Aside from this difficulty, PLS components may be\nexcellent survival time predictors. We should also mention here a recent proposal\nbyMartinussen & Scheike (2009a) implementing a partial least squares regression\nfor additive hazards model via the so-called Krylov sequence.\nCox with univariate gene selection. Possibly the most straightforward and intuitive\napproach to handling high-dimensional data consists of carrying out univariate\ngene selection and using the obtained (small) subset of genes as covariates in\nthe standard Cox model. Such an approach was adopted by Jenssen et al. (2002)\nand van Wieringen et al. (2009). We order genes based on the p-value obtained\nusing Wald\u2019s test in univariate Cox regression and, similarly to van Wieringen\net al. (2009), we select a pre-fixed number of genes (20 in the present study) rather\nthan genes whose p-values fall below a threshold. This ensures having a set of\ngenes of a convenient size for any training set. A partial justification for selecting\n20 covariates comes from the work of vanWieringen et al. (2009), which indicates\nthat using more covariates may lead to more variable results. Furthermore, the\nunivariate Cox regression model is estimated based on the training data only,\nwhich is a universally recommended approach.\nTGD Cox. The threshold gradient descent procedure for the Cox regression analysis\nin the high-dimensional and low-sample size setting approximates the Lasso or\nLARS estimates, while selecting more relevant genes, which is also the reason\nwhy we did not include Lasso directly in our simulation study. The method is\ndescribed in Gui & Li (2005). The approach has two parameters but they rarely\nneed to be tuned, and can instead be chosen by minimizing a cross-validated\npartial likelihood. The complete method, including the dimensional reduction\nand the ability to capture correlated genes, is discussed in details in the above\nDantzig selector fox Cox\u2019s model 18\ncited paper and implemented as an R script available upon request from the\nauthors.\nThe methods are compared in a simulation study. As in van Wieringen et al.\n(2009), two artificial data sets are used. In the first data set the survival times are\ngenerated independently of the gene expression data. Its results give an indication\nof the performance of the tested algorithms when there is no predictive power in the\nexpression data. The other simulated data set was introduced by Bair et al. (2006), also\nfor evaluation purposes.\nDesign of artificial data sets\nEach artificial data set used in the simulation study consists of p = 500 variables and\nn = 100 samples. The survival times and covariate values are distributed as follows.\nData set 1: The columns of the design matrix are samples from a multivariate\nnormal distribution with a given non-diagonal covariation matrix. The survival and\ncensoring times (with censoring probability 1\/3) are exponentially distributed. They\nare independent from each other as well as from the covariates data. Hence, there is\nno prediction power in the covariates.\nData set 2: Following Bair et al. (2006) the covariate data are distributed as:\nlog(Zij) =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f3\n3+ eij if i \u2264 n\/2, j \u2264 30\n4+ eij if i > n\/2, j \u2264 30\n3.4+ eij if j > 30\nwhere the eij are drawn from a standard normal distribution. The survival and\ncensoring times (with censoring probability 1\/3) are generated from an accelerated\nfailure model in which only the values of covariates 1 to 30 (with additional noise)\ncontribute. In other words, only the first 30 covariates determine the survival.\nOne of the desirable goals in estimating a survival model via Cox\u2019s proportional\nhazards is to design procedures that predict well. In high dimensions it is essential to\nDantzig selector fox Cox\u2019s model 19\nregularize the model in some fashion in order to attain good predictive risk. However,\nas noted in van Wieringen et al. (2009), it is not straightforward to evaluate or compare\nprediction methods in the presence of censoring. The standard mean-squared-error or\nmisclassification rate criteria used in regression or classification cannot be applied to\ncensored survival times. In the simulations, we used three measures to evaluate the\nprediction of the compared methods: the p-value (likelihood ratio test) of Bair et al.\n(2006), which is in fact the probability of drawing the observed data under the null-\nhypothesis that the covariates have no effect on survival (the lower the p-value, the\nmore probable that the null hypothesis is not true); a goodness-of-fit measure for the\nproportional hazard model based on the variance of the martingale residuals proposed\nby Barlow & Prentice (1988) (the smaller the better); and the integrated Brier-Score\nintroduced by Graf et al. (1999). The values of the Brier-Score are between 0 and\n1 and good predictions result in small Brier-Scores. A detailed description of these\nmeasures is given in van Wieringen et al. (2009). The first two measures are based on\nthe Cox model, while the Brier score uses the predicted survival curves, which can be\nderived via other approaches. For applying the evaluation measures to our prediction\nmethods, we simply extract the predicted median survival time from the predicted\nsurvival curves and use it as a predictor in a univariate Cox model. This approach,\nthough possibly suboptimal, allows to compare all the prediction methods with these\nthree evaluation measures.\nAn alternative goal is accurate parameter estimation. Indeed, one of the most basic\ndesirable properties of an estimator is consistency and Theorem 1 shows that our DS\nis consistent in the high dimensional setting. Therefore, our simulation study also\nfocuses on the properties of the estimate of the coefficient vector under squared error\nloss. Moreover, when the underlying model is sparse, a natural requirement is to ask\nthat the estimator should correctly identify the relevant variables. Our simulation\nresults specify the false positive rate, F+, i.e. the proportion of unrelated variables\nincorrectly included in the selected model, and the false negative rate, i.e. the proportion\nof variables with non-zero coefficients left out of the model.\nDantzig selector fox Cox\u2019s model 20\nSimulation results\nThe data sets described above were generated 50 times, and randomly split into\ntraining and test sets with a 7:3 ratio. The survival prediction methods were applied\nto the training sets, and the test set was then used for calculation of the evaluation\nmeasures (p-value, variance of martingale residuals and Brier score as implemented\nin the R package ipred). The hyperparameters needed for the TGD and the DS\nmethods were determined by cross-validation on the training sample. The number\nof PLS components in the PLS algorithm is usually determined by some sort of cross\nvalidation procedure. However, there is no simple such criterion to use with the\npartial likelihood of Cox\u2019s regression model. In our simulations, we tried retaining\none, two and three components, but the results showed that retaining more than two\ncomponents, at least in the examples we studied, was not advisable.\nThe results are plotted and summarized in the figures and tables that follow.\nFigures 1, 2 and 3 show evaluation measure boxplots for the results of each of the\nfive methods. The boxplots are grouped by method: two boxplots for the two artificial\ndata sets per method. The coding of the methods underneath the boxplots is explained\nin Tables 1, 2 and 3 which also contain the summary statistics of the results for the\nthree evaluation measures. The median and IQR are given to match the characteristic\nfeatures of the boxplots.\nFigure 1 and its caption here\nTable 1 and its caption here\nFigure 2 and its caption here\nTable 2 and its caption here\nFigure 3 and its caption here\nTable 3 and its caption here\nDantzig selector fox Cox\u2019s model 21\nWith respect to the variance of the martingale residuals, no method clearly stands\nout. They all perform more or less alike. Hence, the variance of the martingale\nresiduals is not very discriminative as an evaluation measure for survival prediction\nmethods.\nThe smaller the Brier score, the better the survival prediction. Focusing on the\nsecond data set where the expression data contains predictive information on survival,\nwe observe that PLS1, PLS2 and DS have a similar good performance. Exceptions are\nthe Cox with 20 genes method and the TGD Cox regression, which do not perform so\nwell, even falling behind the simple Cox regression with univariate feature selection.\nA closer look at this method revealed that for data set 2 sometimes no features were\nselected, leading to poor evaluation measures. We believe this is partially due to the\nchoice of the tuning parameters in the cross-validation, forcing the method to choose\nbetween either the maximum (no features included) or a value that leads to a poor\nprediction.\nSo far, we have focused on finding the best prediction rule for the time to an adverse\nevent using all the available covariates measurements. However, if we bear in mind\nthat in many studies, the main focus is on finding a small subset of the covariates that\nare the most important for predicting survival, we find the survival Dantzig selector\nvery interesting, as it also is a variable selection method. Note that the SDS selector\npicked on average as few as 20 genes (median over the 50 splits) for the second data\nset and as few as 6 genes for the first data set.\nMore precisely, regarding the accuracy of the estimators of the vector of coefficients\n(excluding PLS which is only useful as a dimension reduction technique for\nprediction), the same simulation scenario as that described above was used to generate\nthe two data sets but, this time, with no split into training and test sets since we only\nwished to evaluate the properties of the estimators. Once again the hyperparameters\nneeded for the TGD and the DS methods were determined by cross-validation on\nthe simulated sample. We fitted each method 50 times for each simulation scenario.\nFor each method (Cox20, TGD and DS) and simulation we computed three statistics,\nDantzig selector fox Cox\u2019s model 22\naveraged over 50 runs: the false positive rate, the false negative rate and the median\nroot mean squared error between the estimated coefficients and the truth. Table 4\nprovides the results.\nTable 4 and its caption here\nIn comparison to Cox20 and TGD, DS had the lowest false positive rates and similar\nor lower false negative rates. One notes also that the mean squared error is more\npronounced with data set 2, which we believe is partly explained by the log p factor in\nthe upper bound in Theorem 1.\n5 Analysis of a real-life data set\nIn this section, we compare the performance of the prediction methods on a real-life\ndata set from survival gene expression data. As in Van\u2019t Veer et al. (2002), we have used\na smaller version (78 patients) of a well known real-life data set, namely the Dutch\nbreast cancer data which was analyzed first by van Houwelingen et al. (2006) and\nused by B\u00f8velstad et al. (2007) and consisting of survival times and gene expression\nmeasurements from 295 women. The expression levels of p = 4919 genes were\navailable for this study (consisting of 78 patients). In order to evaluate the methods we\ndivided the data set randomly into two parts; a training set of about 2\/3 of the patients\nused for estimation and a test set of about 1\/3 of the patients used for evaluation or\ntesting of the prediction capability of the estimated model. The split was done 50 times\nand in such a way that the proportion of censored observations in the original data set\nwas respected. The results are plotted and summarized in the following figures and\ntables.\nAs shown in the simulations, the variance of the martingale residuals was not\nhighly discriminative as an evaluation measure for survival prediction. Bearing this\nin mind, for this real-data case we only used the p-values and the Brier scores as\nevaluation measures of predictive performance. Figures 5.4 and 5.5 show boxplots for\nDantzig selector fox Cox\u2019s model 23\nthe evaluation measures of the results for each of the five methods. Table 5 and Table 6\ncontain the summary statistics of the results for the two evaluation measures over the\n50 iterations. The median and IQR are given to match the characteristic features of the\nboxplots.\nFigure 4 and its caption here\nTable 5 and its caption here\nFigure 5 and its caption here\nTable 6 and its caption here\nWith respect to the variance of the martingale residuals, as for the simulation, no\nmethod clearly stands out. Both the boxplots in Figure 5.5 and Table 6 indicate that\nthe PLS based methods and the Dantzig selector have the smallest Brier score, with the\nDantzig selector also having the smallest IQR. Remembering that the PLS components\nare built out of a combination of genes, the Dantzig selector is therefore preferable in\nterms of interpretability for the breast cancer data set.\n6 Conclusions\nWe compared our Dantzig selector method for survival data to several previously\npublished methods for predicting survival and applied it to some simulated data and\nalso to a survival study based on microarray data. Our method performed well in\nsimulations and for real data in comparison with the competitors. Another important\nadvantage of the Dantzig selector is that it selects a subset of the genes to use as\npredictors. The PLS based method, which had a comparable predicting power, by\ncontrast, require the use of all (or a large number) of the genes.\nWe close with a few further remarks. We acknowledge that previous work (Lounici\n(2008); James & Radchenko (2009)) established links between the Dantzig selector and\nDantzig selector fox Cox\u2019s model 24\nLASSO for linearmodels, also as variable selectors. We note that establishing a possible\nsimilar connection between the two procedures in Cox\u2019s model appears challenging\nand is out of scope of the present work. It is also unclear to us whether or how it is\npossible to rapidly compute entire solution paths for the Survival Dantzig Selector;\nwe note that generalised path algorithms for penalised optimisation problems for loss\nfunctions different from least-squares are not obvious to construct or known to exist\n(Rosset & Zhu (2007)).\nAs pointed out by a reviewer, it may be desirable to retain some covariates in\nthe model, based on information from previous studies. In that case, the conditional\nintensities (1) can be re-written as\n\u03bbi(t, zi,wi) = Yi(t)\u03b10(t) exp(zTi \u03b20 +w\nT\ni \u03b70) (9)\nwhere the wi are q-dimensional covariates whose corresponding parameters \u03b70 are\nto be estimated in a standard way (i.e. via maximum likelihood), and the zi are pn-\ndimensional covariates among which a selection should be done. In this set-up, it is\nconvenient to keep q fixed, whereas pn can possibly go to infinity with n. We wish to\nestimate \u03b70 in a classical way and \u03b20 in a sparse way, using a DS-type algorithm. In\nthis set-up, our estimator can be adapted by calculating the solution \u03b8\u02dc = (\u03b2\u02dcT, \u03b7\u02dcT)T of\nmin\n\u03b8\u2208IRp+q\n\u2016\u03b2\u20161 + \u2016\u03b7\u20161 subject to \u2016U\u03b2(\u03b8)\u2016\u221e \u2264 \u03b3 and \u2016U\u03b7(\u03b8)\u2016\u221e = 0, (10)\nwhere \u03b8 = (\u03b2T, \u03b7T)T and where U\u03b2 (resp. U\u03b7) is the p-dimensional (resp. q-\ndimensional) vector of derivatives of the log-partial likelihood corresponding to the\n\u03b2 components (resp. \u03b7 components). The only change in our algorithm would occur in\nstep 3 whilst executing the linear programming algorithm: the parameter \u03b3 is simply\nput to 0 for the components we wish to retain in the model, which ensures that the\nsolution for those components is in fact a marginal maximum likelihood solution. We\nleave a more thorough study of this issue for future work.\nWe close with some advice for data analysts, based on our practical experience with\nhigh dimensionality in Cox\u2019s model. When faced with a particular real data set with\nDantzig selector fox Cox\u2019s model 25\na fixed n and p, our advice is to first conduct a pilot simulation study by simulating\nartificial data sets with these values of n and p, and with the number of significant\ncovariates which we believe is of the order of the corresponding number from the real\ndata set. For each replicate, our advice is then to run the chosen algorithm (this applies\nnot only to our Dantzig Selector, but also to other methods) and assess its performance\nacross a number of replicates. Satisfactory performance would offer us reassurance\nthat the method would also perform well for the real data set. If performance is\nunsatisfactory, our advice is to perform univariate gene deletion until p is reduced\nenough for the tested method to offer satisfactory performance.\nAcknowledgements\nPiotr Fryzlewicz would like to thank Anestis Antoniadis for his hospitality while\nvisiting the Department of Statistics, LJK to carry out this work. Financial support\nform the IAP research network Nr. P6\/03 of the Belgian government (Belgian Federal\nScience Policy) is gratefully acknowledged. The authors thank Anne-Laure Boulesteix\nfor kindly providing the R code for the PLS Cox regression described in Boulesteix\n& Strimmer (2007) and the simulation designs and also Gareth James for kindly\nproviding his Dantzig selector R code implementation for GLM. We thank the Editor,\nAssociate Editor and two Referees for stimulating reports.\nReferences\nAndersen, P. K., Borgan, \u00d8., Gill, R. D. & Keiding, N. (1993). Statistical models based on\ncounting processes. Springer, New York.\nAndersen, P. K. & Gill, R. D. (1982). Cox\u2019s regression model for counting processes: a\nlarge sample study. Annals of Statistics 10, 1100\u20131120.\nBair, E., Hastie, T., Paul, D. & Tibshirani, R. (2006). Prediction by supervised principal\ncomponents. Journal of the American Statistical Association 101, 119\u2013137.\nDantzig selector fox Cox\u2019s model 26\nBarlow, W. E. & Prentice, R. L. (1988). Residuals for relative risk regression. Biometrika\n75, 65\u201374.\nBickel, P., Ritov, Y. & Tsybakov, A. (2009). Simultaneous analysis of Lasso and Dantzig\nselector. Annals of Statistics 37, 1705\u20131732.\nBoulesteix, A. & Strimmer, K. (2007). Partial least squares: A versatile tool for the\nanalysis of high-dimensional genomic data. Briefings in Bioinformatics 8, 24\u201332.\nB\u00f8velstad, H., Nyg\u00e5rd, S., St\u00f8rvold, H., Aldrin, M., Borgan, \u00d8., Frigessi, A. &\nLingj\u00e6rde, O. C. (2007). Predicting survival from microarray data - a comparative\nstudy. Bioinformatics 23, 2080\u20132087.\nCand\u00e8s, E. & Tao, T. (2007). The Dantzig selector: Statistical estimation when p is much\nlarger than n. Annals of Statistics 35, 2313\u20132351.\nCraven, P. & Wahba, G. (1979). Smoothing noisy data with spline functions. Numer.\nMath. 31.\nDelong, D., Guirguis, G. & So, Y. (1994). Efficient computation of subset selection\nprobabilities with application to Cox regression. Biometrika 81, 607\u2013611.\nFan, J. & Li, R. (2002). Variable selection for Cox\u2019s proportional hazards model and\nfrailty model. Annals of Statistics 30, 74\u201399.\nFan, J. & Lv, J. (2008). Sure independence screening for ultrahigh dimensional feature\nspace (with discussion). Journal of the Royal Statistical Society Series B 70, 849\u2013911.\nFaraggi, D. & Simon, R. (1998). Bayesian variable selection method for censored\nsurvival data. Biometrics 54, 1475\u20131485.\nGraf, E., Schmoor, C., Sauerbrei, W. & Schumacher, M. (1999). Assessment and\ncomparison of prognostic classification schemes for survival data. Statistics in\nMedicine 18, 2529\u20132545.\nDantzig selector fox Cox\u2019s model 27\nGui, J. & Li, H. (2005). Threshold gradient descent method for censored data regression\nwith applications in pharmacogenomics. Pacific Symposium on Biocomputing 10, 272\u2013\n283.\nHarrell, F. E. (2001). Regression modeling strategies: With applications to linear models,\nlogistic regression, and survival analysis. Springer, New York.\nHorn, R. A. & Johnson, C. R. (1985). Matrix analysis. Cambridge University Press,\nCambridge, UK.\nHuang, J. (1996). Efficient estimation for the Coxmodel with interval censoring. Annals\nof Statistics 24, 540\u2013568.\nIbrahim, J., Chen, M.-H. & Kim, S. (2008). Bayesian variable selection for the Cox\nregression model with missing covariates. Lifetime Data Analysis 14, 496\u2013520.\nJames, G. & Radchenko, P. (2009). A generalized Dantzig selector with shrinkage\ntuning. Biometrika 96, 323\u2013337.\nJenssen, T., Kuo, W., Stokke, T. & Hovig, E. (2002). Associations between gene\nexpressions in beast cancer and patient survival. Human Genetics 111, 411\u2013420.\nJovanovic, B. D., Hosmer, D. & Buonaccorsi, J. P. (1995). Equivalence of\nseveral methods for efficient best subsets selection in generalized linear models.\nComputational Statistics and Data Analysis 20, 59\u201364.\nLounici, K. (2008). Sup-norm convergence rate and sign concentration property of\nLasso and Dantzig estimators. Electronic Journal of Statistics 2, 90\u2013102.\nMartinussen, T. & Scheike, T. H. (2009a). The Aalen additive hazards model with high\ndimensional regressors. [L]ifetime Data Analysis 15, 330\u2013342.\nMartinussen, T. & Scheike, T. H. (2009b). Covariate selection for the semiparametric\nadditive risk model. Scandinavian Journal of Statistics .\nDantzig selector fox Cox\u2019s model 28\nMarx, B. D. (1996). Iteratively reweighted partial least squares estimation for\ngeneralized linear regression. Technometrics 38, 374\u2013381.\nMcCullagh, P. &Nelder, J. (1989). Generalized linear models. Chapman andHall, London,\n2nd edn.\nNguyen, D. V. & Rocke, D. M. (2002). Partial least squares proportional hazard\nregression for application to DNAmicroarray survival data. Bioinformatics 18, 1625\u2013\n1632.\nNyg\u00e5rd, S., Borgan, \u00d8., Lingj\u00e6rde, O. & St\u00f8rvold, H.-L. (2008). Partial least squares\nCox regression for genome-wide data. Lifetime Data Analysis 14, 179\u2013195.\nPark, P., Tian, L. & Kohane, I. (2002). Linking expression data with patient survival\ntimes using partial least squares. Bioinformatics 18, 120\u2013127.\nRosset, S. & Zhu, J. (2007). Piecewise linear regularized solution paths. Annals of\nStatistics 35, 1012\u20131030.\nShorack, G. R. & Wellner, J. A. (1986). Empirical processes with applications to statistics.\nWiley, New York.\nTibshirani, R. (1997). The lasso method for variable selection in the Cox model.\nStatistics in Medicine 16, 385\u2013395.\nvan de Geer, S. (1995). Exponential inequalities for martingales, with application to\nmaximum likelihood estimation for counting processes. Annals of Statistics 23.\nvan Houwelingen, H., Bruinsma, T., A., H., Van\u2019t Veer, L. J. &Wessels, L. (2006). Cross-\nvalidated Cox regression on microarray gene expression data. Statistics in Medicine\n25, 3201\u20133216.\nvan Wieringen, D., Kun, D., Hampel, R. & Boulesteix, A.-L. (2009). Survival prediction\nusing gene expression data: a review and comparison. Computational Statistics and\nData Analysis 53, 1590\u20131603.\nDantzig selector fox Cox\u2019s model 29\nVan\u2019t Veer, L. J., Dai, H., Van de Vijver, M. J., He, Y., Hart, A., Mao, M., Peterse, H.,\nVan der Kooy, K., Marton, M., Witteveen, A., Schreiber, G., Kerkhoven, R., Roberts,\nC., Linsley, P., Bernards, R. & Friend, S. (2002). Gene expression profiling predicts\nclinical outcome of breast cancer. Nature 415, 530\u2013536.\nWang, H. & Leng, C. (2007). Unified LASSO estimation by least squares approximation.\nJournal of American Statistical Association 102, 1039\u20131048.\nZhang, H. H. & Lu, W. (2007). Adaptive Lasso for Cox\u2019s proportional hazards model.\nBiometrika 94, 691\u2013703.\nZou, H. (2008). A note on path-based variable selection in the penalized proportional\nhazards model. Biometrika 95, 241\u2013247.\nCorrresponding author:\nAnestis Antoniadis\nLaboratoire Jean Kuntzmann, D\u00e9partement de Statistique,\nUniversit\u00e9 Joseph Fourier, B.P. 53\n38041 Grenoble CEDEX 9, France\ne-mail : Anestis.Antoniadis@imag.fr\n7 Appendix: Proofs\nThis section is devoted to the proofs of our main theoretical results stated in the paper.\nProof of Lemma 1. We have to control P(\u2016U(\u03b20)\u2016\u221e < \u03b3) as n, p \u2192 \u221e. That is, we are\nstudying the event\nsup\nj\n\u2223\u2223\u2223\u2223\u2223 1n n\u2211i=1\n\u222b \u03c4\n0\ndMi(u)\n[\nn\n\u2211\nk=1\n{\nZij \u2212 Zkj\n}\nwk(\u03b20, u)\n]\u2223\u2223\u2223\u2223\u2223 \u2265 \u03b3,\nwhere\nwk(\u03b2, u) =\nexp(zTk \u03b2)Yk(u)\n\u2211l exp(zTl \u03b2)Yl(u)\n.\nDantzig selector fox Cox\u2019s model 30\nNote that the wk(\u03b2, u), u \u2208 [0, \u03c4) are nonegative and sum to one. Let\ngn,i,j(u) =\nn\n\u2211\nk=1\n(Zij \u2212 Zkj)wk(\u03b2, u).\nNote that gn,i,j(u) inherits from Yk(u) all measurability properties, so it is a predictable\nprocess. Thus, for each i, j,\n\u222b \u03c4\n0 gn,i,j(u)dMi(u) is a martingale, which implies that\nMn,j = 1n \u2211\nn\ni=1\n\u222b \u03c4\n0 gn,i,j(u)dMi(u) is a martingale. We use now Lemma 2.1 from van de\nGeer (1995), which comes from Shorack & Wellner (1986). For that purpose, we need\nto compute the quantities \u2206Mn,j(u) (magnitude of a jump in Mn,j if it occurs at time u)\nand Vn,j(u) (the variation process of Mn,j(u)).\nSince the jumps of the processes Mi do not occur at the same time and are all of\nmagnitude one, we have\n|\u2206Mn,j(u)| \u2264 sup\n1\u2264i\u2264n\n\u2016gn,i,j\u2016\u221e\nn\n\u2264 sup\ni,j,k\n|Zi,j \u2212 Zk,j|\nn\nn\n\u2211\nk=1\nwk(u) \u2264 2 sup\nj\n\u2016zj\u20162\nn\n=\n2C\nn\n.\nFor the variation process, we use the fact that\n\u3008\n\u222b \u03c4\n0\nHudMu,\n\u222b \u03c4\n0\nH\u2032udM\u2032u\u3009 =\n\u222b \u03c4\n0\nHuH\u2032ud\u3008M,M\u2032\u3009u,\nwhere H, H\u2032 are square integrable predictable processes, and M and M\u2032 are square\nintegrable martingales. Since the Mi are independent, we have\nVn,j(\u03c4) =\n1\nn2\nn\n\u2211\ni=1\n\u222b \u03c4\n0\ng2n,i,j(u)d\u3008Mi,Mi\u3009u\n=\n1\nn2\nn\n\u2211\ni=1\n\u222b \u03c4\n0\ng2n,i,j(u) exp(z\nT\ni \u03b20)Yi(u)\u03b10(u)du\n\u2264 4\nn2\n\u2016zj\u201622 sup\nu\u2208[0,\u03c4]\n{Sn(\u03b20, u)}\u2016\u03b10\u20161.\nWe have\nsup\nu\u2208[0,\u03c4]\n{Sn(\u03b20, u)} \u2264\nn\n\u2211\ni=1\nexp(zTi \u03b20) \u2264 n exp(S\u2016\u03b20\u2016\u221e sup\nj\n\u2016zj\u20162) = O(n),\nso that Vn,j(\u03c4) \u2264 Kn for a suitable constant K. We will now use the exponential\nDantzig selector fox Cox\u2019s model 31\ninequality from Shorack & Wellner (1986).\nP\n(\nsup\nj\n\u2223\u2223\u2223\u2223\u2223 1n n\u2211i=1\n\u222b \u03c4\n0\ndMi(u)\n[\nn\n\u2211\nk=1\n{\nZij \u2212 Zkj\n}\nwk(\u03b20, u)\n]\u2223\u2223\u2223\u2223\u2223 \u2265 \u03b3\n)\n\u2264\n\u2211\nj\nP\n(\u2223\u2223\u2223\u2223\u2223 1n n\u2211i=1\n\u222b \u03c4\n0\ndMi(u)\n[\nn\n\u2211\nk=1\n{\nZij \u2212 Zkj\n}\nwk(\u03b20, u)\n]\u2223\u2223\u2223\u2223\u2223 \u2265 \u03b3\n)\n=\n\u2211\nj\nP\n(\u2223\u2223\u2223\u2223\u2223 1n n\u2211i=1\n\u222b \u03c4\n0\ndMi(u)\n[\nn\n\u2211\nk=1\n{\nZij \u2212 Zkj\n}\nwk(\u03b20, u)\n]\u2223\u2223\u2223\u2223\u2223 \u2265 \u03b3 \u2229Vn,j(\u03c4) \u2264 Kn\n)\n\u2264\np exp\n(\n\u2212 n\u03b3\n2\n2(2C\u03b3+ K)\n)\n.\nOur choice of \u03b3 allows us to conclude.\nProof of Theorem 1. To prove the result, we will also need the following Lemma\nwhich we state with no proof since it is a straightforward generalization of Lemma 3.1\nin Cand\u00e8s & Tao (2007).\nLemma 2 Let A be an n \u00d7 p matrix and suppose T0 \u2282 {1, . . . , p} is a set of cardinality S.\nFor a vector h \u2208 Rp, let T1 be the S\u2032 largest positions of h outside of T0 and put T01 = T0 \u222a T1.\nThen\n\u2016hT01\u20162 \u2264\n1\n\u03b4S+S\u2032\n\u2016ATT01Ah\u20162 +\n\u03b8S\u2032,S+S\u2032\n\u03b4S+S\u2032(S\u2032)1\/2\n\u2016hTc0\u20161\n\u2016h\u201622 \u2264 \u2016hT01\u201622 + (S\u2032)\u22121\u2016hTc0\u201621.\nTo prove the Theorem we need to establish that \u2016U(\u03b20)\u2016\u221e \u2264 \u03b3 implies that\n\u2016\u03b2\u02c6\u2212 \u03b20\u201622 \u2264 64S( \u03b3\u03b42S\u2212\u03b8S,2S )2. Assume that \u2016U(\u03b20)\u2016\u221e \u2264 \u03b3 where\n\u2016U(\u03b20)\u2016\u221e = sup\nj\n\u2223\u2223\u2223\u2223\u2223 1n n\u2211i=1\n\u222b \u03c4\n0\ndMi(u)\n[\nn\n\u2211\nk=1\n{\nZij \u2212 Zkj\n}\nwk(u)\n]\u2223\u2223\u2223\u2223\u2223 .\nRecall here that for any consistent estimator \u03b2\u02dc of \u03b20, we may write:\nJ(\u03b2\u02dc, \u03c4)\u2212 I(\u03b20, \u03c4) =\n\u222b \u03c4\n0\n(Vn(\u03b2\u02dc, u)\u2212 v(\u03b2\u02dc, u))dN\u00af(u)n (11)\n+\n\u222b \u03c4\n0\n(v(\u03b2\u02dc, u)\u2212 v(\u03b20, u))\ndN\u00af(u)\nn\n(12)\n+\n\u222b \u03c4\n0\nv(\u03b20, u)\ndM\u00af(u)\nn\n(13)\n+\n\u222b \u03c4\n0\nv(\u03b20, u)(\nSn(\u03b20, u)\nn\n\u2212 s(\u03b20, u))\u03b10(u)du, (14)\nDantzig selector fox Cox\u2019s model 32\nwhere Vn(\u03b2, u) =\nS2n\nSn (\u03b2, u)\u2212 (\nS1n\nSn )\n\u22972(\u03b2, u) and v(\u03b2, u) = s2s (\u03b2, u)\u2212 ( s\n1\ns )\n\u22972(\u03b2, u). Since\n\u03b20 is a nonzero S-sparse vector with S independent of n and since the true information\nmatrix I(\u03b20, \u03c4) is positive definite at \u03b20, for any \u03b2\u2217 in an Euclidian ball Br = B(\u03b20, r)\ncentered at \u03b20 and of radius at most r = 8\n\u221a\nS \u03b3\u03b42S\u2212\u03b8S,2S , the regularity conditions of\nTheorem 3.4 in Huang (1996) hold and it follows that\nsup\n\u03b2\u02dc\u2208Br\n\u2016J(\u03b2?, \u03c4)\u2212 I(\u03b20, \u03c4)\u2016\u221e = OP(n\u22121\/2) (15)\nas n tends to \u221e.\nDefine h = \u03b2\u02c6 \u2212 \u03b20 and let T0 be the support of \u03b20. According to Lemma 1, we\nhave \u2016\u03b2\u02c6\u20161 \u2264 \u2016\u03b20\u20161 and this inequality implies that \u2016hTc0\u20161 \u2264 \u2016hT0\u20161, which yields, by\nCauchy inequality,\n\u2016hTc0\u20161 \u2264 \u2016hT0\u20161 \u2264 S1\/2\u2016hT0\u20162. (16)\nBy assumption, we have \u2016U(\u03b20)\u2016\u221e \u2264 \u03b3 and by construction of the estimator,\n\u2016U(\u03b2\u02c6)\u2016\u221e \u2264 \u03b3. Adding up the two inequalities (triangle inequality)\n\u2016U(\u03b2)\u2212U(\u03b2\u02c6)\u2016\u221e \u2264 2\u03b3\nBy Andersen & Gill (1982), formula (2.6), we have, Taylor-expanding the left hand side\nof the above, \u2225\u2225\u2225J(\u03b2\u2217, \u03c4)(\u03b2\u02c6\u2212 \u03b20)\u2225\u2225\u2225\n\u221e\n\u2264 2\u03b3, (17)\nwhere \u03b2\u2217 lies within the segment between \u03b2\u02c6 and \u03b20.\nNow, using our remark (15) on the behavior of the matrix I(\u03b20, \u03c4) at the\nneighborhood of \u03b20 we have\u2225\u2225\u2225I(\u03b20, \u03c4)(\u03b2\u02c6\u2212 \u03b20)\u2225\u2225\u2225\n\u221e\n\u2264\n\u2225\u2225\u2225(J(\u03b2\u2217, \u03c4)\u2212 I(\u03b20, \u03c4))(\u03b2\u02c6\u2212 \u03b20)\u2225\u2225\u2225\n\u221e\n+\n\u2225\u2225\u2225J(\u03b2\u2217, \u03c4)(\u03b2\u02c6\u2212 \u03b20)\u2225\u2225\u2225\n\u221e\n\u2264 Dn\u22121\/2\n\u2225\u2225\u2225\u03b2\u02c6\u2212 \u03b20\u2225\u2225\u2225\n1\n+ 2\u03b3,\n\u2264 4\u03b3,\nfor n large enough, since\n\u2225\u2225\u2225\u03b2\u02c6\u2212 \u03b20\u2225\u2225\u2225\n1\n\u2264\n\u2225\u2225\u2225\u03b2\u02c6\u2225\u2225\u2225\n1\n+ \u2016\u03b20\u20161 \u2264 2 \u2016\u03b20\u20161. Hence, if A =\nI(\u03b20, \u03c4)1\/2 denotes the squared root of the (semi)definite positive matrix I(\u03b20, \u03c4), we\nDantzig selector fox Cox\u2019s model 33\nhave\n\u2016AAh\u2016\u221e \u2264 4\u03b3.\nThis, again by Cauchy inequality, implies \u2016ATT01Ah\u20162 \u2264 4(S+ S\u2032)1\/2\u03b3. Take S\u2032 = S.\nBy the first inequality of Lemma 2 and inequality (16), we have\n\u2016hT01\u20162 \u2264\n4\n\u03b42S\n(2S)1\/2\u03b3+\n\u03b8S,2S\n\u03b42SS1\/2\nS1\/2\u2016hT0\u20162\n\u2264 4\n\u03b42S\n(2S)1\/2\u03b3+\n\u03b8S,2S\n\u03b42S\n\u2016hT01\u20162.\nRearranging for \u2016hT01\u20162, we get\n\u2016hT01\u20162\n(\n1\u2212 \u03b8S,2S\n\u03b42S\n)\n\u2264 4\n\u03b42S\n(2S)1\/2\u03b3\n\u2016hT01\u20162 \u2264\n4\n\u03b42S \u2212 \u03b8S,2S (2S)\n1\/2\u03b3.\nBy the second inequality of Lemma 2 and inequality (16), we have\n\u2016h\u201622 \u2264 \u2016hT01\u201622 + S\u22121S\u2016hT0\u201622 \u2264 2\u2016hT01\u201622 \u2264 64S(\n\u03b3\n\u03b42S \u2212 \u03b8S,2S )\n2,\nwhich completes the proof of the Theorem.\nDantzig selector fox Cox\u2019s model 34\nFigures, Tables and Captions\ncox pls1 pls2 tgd ds\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\nMethods\nl\nll\nl\nll\nl\nll\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\ndata set 1\ndata set 2\np values_\nFigure 1: Box plots of the p-values for each method over the 50 simulations of each data set.\nThe lower the p-value, the more probable is that the covariates have predictive power.\nDantzig selector fox Cox\u2019s model 35\nMethod Coded as Data set Median IQR\nCox regression with 20 best genes COX ds1 0.563 0.445\nCox regression with 20 best genes COX ds2 0.437 0.555\nPLS Cox (1 comp) PLS1 ds1 0.516 0.399\nPLS Cox (1 comp) PLS1 ds2 0.031 0.099\nPLS Cox (2 comp) PLS2 ds1 0.626 0.412\nPLS Cox (2 comp) PLS2 ds2 0.495 0.483\nTGD Cox regression TGD ds1 0.404 0.121\nTGD Cox regression TGD ds2 0.028 0.084\nDantzig Selector DS ds1 0.438 0.492\nDantzig Selector DS ds2 0.027 0.084\nTable 1: Results for the simulated data sets: p-values.\nCOX PLS1 PLS2 TGD DS\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\nVariance of Martingale Residuals\nMethods\nl l\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\ndata set 1\ndata set 2\nFigure 2: Box plots of the variance of martingale residuals for each method over the 50\nsimulations of each data set.\nDantzig selector fox Cox\u2019s model 36\nMethod Coded as Data set Median IQR\nCox regression with 20 best genes COX ds1 0.617 0.106\nCox regression with 20 best genes COX ds2 0.606 0.104\nPLS Cox (1 comp) PLS1 ds1 0.650 0.144\nPLS Cox (1 comp) PLS1 ds2 0.620 0.165\nPLS Cox (2 comp) PLS2 ds1 0.635 0.138\nPLS Cox (2 comp) PLS2 ds2 0.571 0.156\nTGD Cox regression TGD ds1 0.618 0.121\nTGD Cox regression TGD ds2 0.639 0.150\nDantzig Selector DS ds1 0.636 0.114\nDantzig Selector DS ds2 0.631 0.120\nTable 2: Results for the simulated data sets: variance of martingale residuals.\nl\nll\nl\nl\nCOX PLS1 PLS2 TGD DS\n0.\n0\n0.\n1\n0.\n2\n0.\n3\n0.\n4\n0.\n5\n0.\n6\nBrier score\nMethods\nl\nl\n0.\n0\n0.\n1\n0.\n2\n0.\n3\n0.\n4\n0.\n5\n0.\n6\ndata set 1\ndata set 2\nFigure 3: Box plots of the Brier prediction score for each method over the 50 simulations of\neach data set.\nDantzig selector fox Cox\u2019s model 37\nMethod Coded as Data set Median IQR\nCox regression with 20 best genes COX ds1 0.169 0.052\nCox regression with 20 best genes COX ds2 0.150 0.054\nPLS Cox (1 comp) PLS1 ds1 0.135 0.049\nPLS Cox (1 comp) PLS1 ds2 0.106 0.040\nPLS Cox (2 comp) PLS2 ds1 0.157 0.055\nPLS Cox (2 comp) PLS2 ds2 0.120 0.043\nTGD Cox regression TGD ds1 0.264 0.107\nTGD Cox regression TGD ds2 0.191 0.082\nDantzig Selector DS ds1 0.156 0.060\nDantzig Selector DS ds2 0.109 0.037\nTable 3: Results for the simulated data sets: Brier scores (the lower the better).\nMethod Coded as Data set RMSE F+ F\u2212\nCox regression with 20 best genes COX ds1 0.0905 0.04 0.00\nCox regression with 20 best genes COX ds2 2.2832 0.00 0.12\nTGD Cox regression TGD ds1 0.0188 0.08 0.00\nTGD Cox regression TGD ds2 2.2889 0.02 0.11\nDantzig Selector DS ds1 0.0163 0.03 0.00\nDantzig Selector DS ds2 2.2798 0.00 0.04\nTable 4: Results for the simulated data sets: median root mean squared error, false\npositive rate F+ and false negative rate F\u2212 for three different methods using 50\nsimulations with p = 0 (data-set ds1) and p = 30 (data-set ds2).\nDantzig selector fox Cox\u2019s model 38\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nCox20 PLS1 PLS2 TGD DS\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\np\u2212values\nMethods\nFigure 4: Box plots of the p-values for each method over the 50 simulations for the Breast\nCancer data set.\nMethod Median IQR\nCOX 0.139 0.406\nPLS1 0.082 0.181\nPLS2 0.094 0.217\nTGD 0.027 0.120\nDS 0.141 0.194\nTable 5: Results for the Breast Cancer data: p-values over the 50 splits.\nDantzig selector fox Cox\u2019s model 39\nl\nl\nl\nl\nl\nl\nCox20 PLS1 PLS2 TGD DS\n0.\n1\n0.\n2\n0.\n3\n0.\n4\n0.\n5\n0.\n6\nBrier Score\nMethods\nFigure 5: Box plots of the Brier prediction score for each method over the 50 simulations for\nthe Breast Cancer data set.\nMethod Median IQR\nCOX 0.263 0.113\nPLS1 0.199 0.052\nPLS2 0.215 0.047\nTGD 0.246 0.093\nDS 0.230 0.045\nTable 6: Results for the Breast Cancer data: Brier scores over the 50 splits.\n"}