{"doi":"10.1109\/TCE.2008.4560175","coreId":"102352","oai":"oai:epubs.surrey.ac.uk:1811","identifiers":["oai:epubs.surrey.ac.uk:1811","10.1109\/TCE.2008.4560175"],"title":"Joint source and channel coding for 3D video with depth image - Based rendering","authors":["Kamolrat, B","Fernando, WAC","Mrak, M","Kondoz, A"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2008-05","abstract":null,"downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:epubs.surrey.ac.uk:1811<\/identifier><datestamp>\n      2017-10-31T14:03:14Z<\/datestamp><setSpec>\n      74797065733D61727469636C65<\/setSpec><setSpec>\n      6469766973696F6E733D656E67616E64706879736963616C736369656E636573:656C656374726F6E6963656E67696E656572696E67:63637372<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/epubs.surrey.ac.uk\/1811\/<\/dc:relation><dc:title>\n        Joint source and channel coding for 3D video with depth image - Based rendering<\/dc:title><dc:creator>\n        Kamolrat, B<\/dc:creator><dc:creator>\n        Fernando, WAC<\/dc:creator><dc:creator>\n        Mrak, M<\/dc:creator><dc:creator>\n        Kondoz, A<\/dc:creator><dc:publisher>\n        IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC<\/dc:publisher><dc:date>\n        2008-05<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/epubs.surrey.ac.uk\/1811\/1\/fulltext.pdf<\/dc:identifier><dc:identifier>\n          Kamolrat, B, Fernando, WAC, Mrak, M and Kondoz, A  (2008) Joint source and channel coding for 3D video with depth image - Based rendering   IEEE T CONSUM ELECTR, 54 (2).  pp. 887-894.      <\/dc:identifier><dc:relation>\n        10.1109\/TCE.2008.4560175<\/dc:relation><dc:language>\n        EN<\/dc:language><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/epubs.surrey.ac.uk\/1811\/","10.1109\/TCE.2008.4560175"],"year":2008,"topics":[],"subject":["Article","PeerReviewed"],"fullText":" Joint Source and Channel Coding  \nfor 3D Video with Depth Image - Based Rendering \nB. Kamolrat, W.A.C. Fernando, Senior Member, IEEE, M. Mrak and A. Kondoz, Member, IEEE \n \nAbstract \u2014 Following recent commercial availability of \nautostereoscopic 3D displays that allow 3D visual data to be \nviewed without the use of special headgear or glasses, it is \nanticipated that the applications of 3D video will increase \nrapidly in near future. In this paper we propose a joint source \nchannel coding scheme for depth image-based rendering \nbased 3D video coding. We considered different source and \nchannel coding rates to find the optimum coding performance \nunder a given channel bit rate for a WiMAX based \ncommunication channel. When the optimum bit allocation \ncombination for color and depth image sequences are found, \ndifferent protection levels have been considered for coding \nboth image sequences. Finally, an optimum protection levels \nare proposed for the best video quality.1 \n \nIndex Terms \u2014 3D video, video coding, image coding. \nI. INTRODUCTION \nThe research on stereoscopic video has received high interest \nover the past decade in order to provide viewers with more \nrealistic vision than traditional 2D video. Instead of using left \nand right views to represent 3D video, a new technique, known \nas depth image-based rendering (DIBR) [1] , represent 3D video \nbased on a monoscopic video and associated per-pixel depth \ninformation (simply called color and depth maps). While the \ncolor consists of three components - Y, U and V as in the \ntraditional video applications, the depth maps video uses only \none component to store the depth information of objects within \nthe scene related to the camera position. The advantage of such \na scheme is, it can capture the stereoscopic sequences more \neasily compared to the traditional left and right view techniques. \nEven though, 3D video can provide more impressive video than \nconventional 2D video, in the past, there were many factors \nsuch as huge bandwidth requirement and the discomfort due to \nspecial devices needed to observe 3D effect preventing 3D \nvideo from success in commercial services. \nWith recent advances in both digital video compression and \ndigital transmission technologies like H.264\/AVC and \nWiMAX, the real time 3D video transmission application such \nas 3D television (3D TV) and 3D-conference can be realized. \nH.264\/AVC is a video compression standard, also known as \nMPEG-4 Part 10 - Advanced Video Coding (AVC) [2]. It has \nmany new features to achieve significant improvement in \ncoding efficiency comparing to previous standards. Besides, it \n \n1This work was supported in part by the European Commission IST FP6 \nprogram, under VISNET II project, a European Network of Excellence. \nAuthors are with the Centre for Communication System Research, \nUniversity of Surrey, Guildford, GU2 7XH, UK (e-mail: {B.Kamolrat, \nW.Fernando, M.Mark, A.Kondoz}@surrey.ac.uk). \nprovides more flexibility for application to a wide variety of \nnetwork environments by applying a concept of Network \nAdaptation Layer (NAL). The high data rate and Quality of \nService (QoS) provided by WiMAX technology make it \nattractive to multimedia applications, such as video telephony, \nvideo gaming, and video broadcasting. IEEE WiMAX \n802.16e standard, [3], [4], also referred to as Wireless-MAN, \nis capable to support data rate up to 70 Mbps. The WiMAX \nstandard features Low-Density-Parity-Check (LDPC) codes as \nan optional channel coding scheme. LDPC codes are linear \nblock codes first introduced by Gallager in 1963 [5]. The \nadvantage of LDPC codes is that they can approach the \nchannel capacity over a very large code length. For example, \nfor the code length of one million bits, the performance of \nLDPC is only 0.0045 dB lower from the maximum theoretical \nchannel capacity [6]. \nHighly compressed H.264\/AVC video bit-streams are very \nsensitive to channel errors [7]. To further improve the \nperformance of 3D video transmission over wireless channel \nwhich is considered as bandwidth limited and error prone, \njoint source channel coding (JSCC) is an effective method to \novercome such challenges [8]-[12]. The main concept of \nJSCC is that both the source coding and channel coding are \nadapted according to channel conditions in order to minimize \nthe distortion. Distortion in video communication can be \nseparated into two major types. The first type is the \nquantization distortion introduced by a lossy source encoding \nand the second type is caused by channel noise. These \ndistortions are simply called \u201csource distortion\u201d and \u201cchannel \ndistortion\u201d denoted by DS and DC, respectively. The overall \ndistortion, DTotal, is equal to DS + DC. A popular measure the \ndistortion is a mean square error (MSE). Source distortion \nrefers to MSE between the decoded frame from uncorrupted \nbit-stream at the transmitter and the original one. Channel \ndistortion refers to the MSE between the decoded video frame \nat the receiver and the transmitted frame. The overall picture \ndistortion at the receiver end can be defined as the MSE \nbetween the received video frame and the original one. In this \npaper we are trying to minimize the effect of these two \ndistortions using a JSCC approach. \nThe rest of the paper is organized as follows. Some related \nwork is briefly discussed in section 2 together with the basic \nconcept of DIBR. The concept of bit stream organization in \nH.264\/AVC is summarized in section 3. The 3D video \ntransmission framework and experimental results for JSCC are \npresented in section 4. Finally, the conclusions are \nsummarized in section 5. \nContributed Paper\nManuscript received April 15, 2008 0098 3063\/08\/$20.00 \u00a9 2008 IEEE\nB. Kamolrat et al.: Joint Source and Channel Coding for 3D Video with Depth Image - Based Rendering 887\nAuthorized licensed use limited to: University of Surrey. Downloaded on April 13,2010 at 14:32:44 UTC from IEEE Xplore.  Restrictions apply. \n II. RELATED WORK \nSince 3D TV is believed by many to be a new generation of \nTV broadcasting, the Moving Picture Expert Group (MPEG) \ndeveloped a compression technology for stereoscopic video \nsequences as part of the MPEG-2 standard [13]. This \napproach is based on the multiview profile (MVP) which can \nbe regarded as an extension of the temporal scalability tool. It \nencodes the left-eye view as a based layer and right-eye view \nas an enhancement layer. Even though this scheme provides \nbackwards-compatibility to conventional 2D digital TV \nservices, it has not been applied to any commercial services so \nfar. \nThe latest 3D TV broadcasting approach has been proposed \nby the European Information Society Technologies (IST) \nproject \u2018Advanced Three-Dimensional Television System \nTechnologies\u2019 (ATTEST), [14]. The 3D representation used in \nthis approach, based on monoscopic video and associated per-\npixel depth information, is called DIBR. The concept of the \nATTEST 3D TV can be separated into five sections: 3D \ncontent creation, 3D coding, transmission, virtual view \nsynthesis and 3D display. At the coding part, monoscopic \nvideo is encoded by MPEG-2 and the depth information is \nencoded with more efficient codec such and MPEG-4. \nSubsequently, the encoded data streams are transmitted over \ndigital video broadcasting (DVB) network. At the receiver, the \nreceived data streams are synthesized and displayed by a 3D \ndisplay. The conventional digital TV can display 2D video \nwhile depth information is ignored. \nThe main advantage of DIBR technique compared to \ntraditional representation of 3D video with the left-right views \nis that it provides high quality 3D video with smaller \nbandwidth required for transmission. This is because the depth \nmap can be coded more efficiently than the two streams of \nmonoscopic views if correlations and properties of the depth \nmap are identified properly. Examples of color and depth map \nframes of the \u201cOrbi\u201d video test sequence are shown in Figure \n1. While the color is stored in the same way as normal 2D \nvideo, the depth map is stored using only one component. \nWhen 8-bit component depth is used, 256 different depth \nvalues are associated to the pixels of the depth map.  \n \n  \n  \n(a) color (b) depth \n  \nFigure 1 Frame from the \"Orbi\" test sequence. \n \nConceptually, DIBR utilizes a depth frame to generate 2 \nvirtual views from the same reference (original) view, one for \nthe left eye and the other one for the right eye [15]. This \nprocess can be described by a following 2-step procedure. \nFirstly, the original image points are re-projected into the 3D \ndomain, utilizing the respective depth values. Thereafter, these \nintermediate space points are projected into the image plane of \na virtual camera located at the required viewing position. This \n2-step procedure is usually referred to as \u201c3D image warping\u201d \nin the computer graphics literature. The virtual views \ngeneration process is shown in Figure 2. In this process the \noriginal image points at locations (x, y) are transferred to new \nlocations (xL, y) and (xR, y) for left and right view, \nrespectively. This process is defined with: \n \n 1 1\n2\nx c\nL\nc\ntx x\nZ Z\n\u03b1 \u239b \u239e\u22c5= + \u2212\u239c \u239f\u239d \u23a0\n (1) \n \n 1 1\n2\nx c\nR\nc\ntx x\nZ Z\n\u03b1 \u239b \u239e\u22c5= \u2212 \u2212\u239c \u239f\u239d \u23a0\n (2) \n \nwhere \u03b1x is the focal length of the reference camera expressed \nin multiples of the pixel width and tC is the distance between \nthe left and right virtual cameras. ZC is the convergence \ndistance located at the zero parallax setting (ZPS) plane and Z \ndenotes the depth value of each pixel in the reference view. \nNotes that the y component is constant since the virtual \ncameras used to capture the virtual views (left-right) are \nassumed to be located at the same horizontal plane. \nThe quality of virtual views depends on the quality of \nreceived color and depth map. In a video transmission system \nthe impairment of synthesized frames depends on both the \ncompression and transmission processes. \n \n \n tC \nLeft Virtual Cam. \nCapturing image \npoint  (xL, y) \n \nReference Cam. \nCapturing image \npoint  (x, y) \nRight Virtual Cam. \nCapturing image \npoint  (xR, y) \n \nZPS \nX \nZ \nZPS ZC \n\u03b1x \n \nFigure 2 Virtual view generation in DIBR process. \n \nIII. VIDEO TRANSMISSION \nEven though new communication technologies provide \nsufficient bit rate to support video communication \napplications, the bit rate will always be a scarce resource in \nwireless transmission environments due to physical bandwidth \n888 IEEE Transactions on Consumer Electronics, Vol. 54, No. 2, MAY 2008\nAuthorized licensed use limited to: University of Surrey. Downloaded on April 13,2010 at 14:32:44 UTC from IEEE Xplore.  Restrictions apply. \n and power limitations. To reduce the amount of transmitted \ndata, the efficient video compression techniques are necessary. \nH.264\/AVC is a new video coding standard expected to be \nused by many applications in the near future since it provides \nbetter performance than prior video coding standards like \nH.263 and MPEG-4. However, the highly compressed data is \nvery vulnerable to channel errors. Most efficient video coding \ntechniques are usually based on variable length coded (VLC) \nwhere short code words are assigned to the highly probable \nvalues and long code words to the less probable ones. Only \none error bit can lead to the whole frame to be dropped due to \ndesynchronization between the encoder and the decoder at the \nreceiver. This phenomenon is known as error propagation. \nTherefore, to prevent error propagation through frame, the \nconcept of slice is adopted where a frame is subdivided into \nseveral slices. If errors occur within a slice, the current slice is \ndropped and the decoder will search for the starting point of \nnext slice and continue the decoding process. Therefore, in the \nnoisy environment it is desirable to use frame segmentation \ninto slices. On the other hand, each slice has a large overhead \nto indicate the starting point of each slice. The tradeoff \nbetween robustness to errors and overhead bits is needed to be \ncarefully considered when designing the video transmission \nsystem. Generally, the decoded frame is used as a reference \nframe for the following frames; therefore the impairment in \nthe reconstructed frame can propagate to successive frames. \nTo reduce the channel errors, error control mechanisms are \nnormally used and the popular techniques used in data \ntransmission system are Automatic Repeat request (ARQ) and \nForward Error Correction (FEC). Due to the long delay of \nARQ, FEC has been commonly suggested for real-time \napplication. However, FEC incurs constant transmission \noverhead even when the channel is loss free. To achieve the \nefficient bandwidth usage, especially over the limited \nbandwidth of the wireless link, the JSCC technique based on \nFEC has been introduced. \nThe concept of JSCC is that source and channel coding are \njointly adjusted. This concept enables minimization of the \ndistortions since the distortion in video transmission can be \nseparated into two types: source distortion and channel \ndistortion. The source distortion does not depend only on a \ngiven source-coding bit rate, but also on characteristics of the \ninput videos and the data representation scheme which is \nemployed by the coding algorithm. In case of H.264\/AVC, the \nbit rate and video quality are adjusted by selection of suitable \nquantization parameter (QP). QP regulates strength of  \nquantization and its value is selected in the rate-distortion \noptimization process. In order to meet specific bit rate \nrequirements, QP also has to be adjusted depending on the \nunderlying video content [16]. The channel distortion is due to \ntransmitted data corrupted by noise over channel. \nOne of the design goals of H.264\/AVC is to allow coded \nvideo to be integrated to all current protocol and multiplex \narchitectures. The H.264\/AVC consists of two conceptually \ndifferent layers: Video Coding Layer (VCL) and Network \nAdaptation Layer (NAL) [17]. The VCL is designed to \nrepresent the content of the video data whereas the NAL is \nresponsible for packaging and conveying data in a manner \nappropriate to transmission channels. Encoded video data and \nparameter sets are sent from the VCL to the NAL and \nencapsulated into units called NAL units. The format of NAL \nunit is shown in Figure 3. \n \n \nNAL UNIT \nNAL header RBSP Trailing bits \n \nFigure 3 NAL unit format. \n \nA NAL unit consists of a 1-byte NAL header, a variable \nbyte length Raw Byte Sequence Payload (RBSP) and a \npayload trailing bits. The NAL header is used to indicate the \ntype of the NAL unit. Compressed video data and parameter \nsets are stored in the RBSP. The payload trailing bits are used \nto adjust the payload to become a multiple of bytes. \nThe interface between VCL and NAL is a slice layer. A \nslice is a group of macroblocks (MB) that does not need any \ninformation from other slices to be encoded or decoded. In \nvideo transmission, the order in which the NAL units have to \nbe sent is constant. The first NAL unit to be sent is the \nSequence Parameter Set (SPS) followed by the Picture \nParameter Set (PPS). Both SPS and PPS include parameters \nwhich set in the encoder configuration for all pictures in the \nvideo sequence, for example: entropy coding mode flag, \nnumber of reference index, weighted prediction flag, picture \nwidth in MB, picture height in MB and number of reference \nframes. The next NAL unit is the Instantaneous Decoder \nRefresh (IDR). After receiving this NAL unit type all the \nbuffers are deleted. Following the IDR frame is the NAL unit \ntype slice. Figure 4 shows NAL units order in the case when \nslice mode 0 is selected and no data partitioning is used. As \nSPS and PPS are used for all pictures to reconstruct, they are \nthe most sensitive to errors following with IDR. The NAL unit \ntype slice is the least sensitive to errors. \n \n SPS PPS IDR SLICE SLICE \n \nFigure 4 Order of NAL unit. \n \nB. Kamolrat et al.: Joint Source and Channel Coding for 3D Video with Depth Image - Based Rendering 889\nAuthorized licensed use limited to: University of Surrey. Downloaded on April 13,2010 at 14:32:44 UTC from IEEE Xplore.  Restrictions apply. \n  \nFigure 5 Simulation model. \n \nIV. SIMULATION MODEL AND EXPERIMENTAL RESULTS \nIn this section, we explain the simulation model that we use \nto encode DIBR based 3D video data and the considered \nJSCC scheme for the video data. The overall system model is \nillustrated in Figure 5. \nAt the transmission side (Tx), color and depth map are \nseparately compressed by H.264\/AVC encoders and then \nprotected by LDPC codes. The output bit streams are \nrearranged to get single output at the multiplexer. \nSubsequently, the output from multiplexer is transmitted by \nWiMAX over a Rayleigh fading channel. At the receiver (Rx), \nreceived data stream is separated back to 2 data streams before \ndecoded by LDPC and H.264\/AVC decoders, respectively. At \nthe end of the process, color and depth map are reconstructed.  \nIn this system the quality of received video is evaluated for \nleft and right views that are synthesized from color and depth \nmap. The average peak signal to noise ration (PSNR) of \nreconstructed left and right views is measured comparing to \nthe original left and right views, Figure 6. \nIn order to maximize the system performance, the bit rate \nratio between color and depth map has to be optimized. In the \nfollowing section this process is explained. \n \n \n \nFigure 6 Average PSNR for a video sequence is obtained from the left and \nright views. \n \nA. Evaluation of bit rate allocation between color and depth \nmap \nTo maximize the quality of 3D video when the total bit \nbudget is fixed, optimized bit allocation is needed for both \ncolor and depth maps. In conventional approaches, where 8 \nbits per pixel are used for the depth map, a small change in the \npixel value for the depth map does not have much impact on \nthe overall 3D video quality. \nA suitable bit rate ratio between color and depth map, when \nthe bit budget is fixed, has been found experimentally. Color \nand depth map are encoded by H.264\/AVC. Several 3D video \nsequences are considered. However, the \u201cOrbi\u201d and \n\u201cInterview\u201d video sequences are presented in this paper with \n25 fps, CIF resolution (352 \u00d7 288), 8-bits per each colour and \ndepth component and there are 30 P-frames between I-frame. \nThe bit rate of depth map is varied between 10 - 90 % of the \ntotal bit rate and the remaining bit rate is assigned to the color. \nThe total bit rate is varied from 200 kbps to 1 Mbps. Then the \ncolor and depth map are used to generate left and right videos. \nFinally, the average PSNR of left and right video output is \nmeasured by referencing to the left and right video generated \nfrom original color and depth map. The results are \nsummarized in Figure 7. The average PSNR of left and right \nviews are illustrated in Figure 7 a)-d), while the graphs in \nFigure 7 e) and f) show average result for both views. \nThe results suggests that if the total bit rate (color and depth \nmap) is fixed, the highest decoding quality is obtained when \nthe percentage of depth map bit rate to the total bit rate is \nabout 20 %. This observation is used in the proposed system. \nB. Simulation Results and Discussion \nIn this subsection the effect of channel coding on \ntransmission of DIBR based 3D video is investigated. \n \n890 IEEE Transactions on Consumer Electronics, Vol. 54, No. 2, MAY 2008\nAuthorized licensed use limited to: University of Surrey. Downloaded on April 13,2010 at 14:32:44 UTC from IEEE Xplore.  Restrictions apply. \n  \n10 20 30 40 50 60 70 80 90\n24\n26\n28\n30\n32\n34\n36\n38\n40\nInterview Left-View\n( Rd \/ Rtotal ) x 100 [%]\nP\nS\nN\nR\n [d\nB\n]\n \n \nRtotal = 1000 kbps\nRtotal = 900 kbps\nRtotal = 800 kbps\nRtotal = 700 kbps\nRtotal = 600 kbps\nRtotal = 500 kbps\nRtotal = 400 kbps\nRtotal = 300 kbps\nRtotal = 200 kbps\n(BD \/ BT+D) \u00d7 100% [ ]  \n10 20 30 40 50 60 70 80 90\n26\n28\n30\n32\n34\n36\n38\n40\n42\nOrbi Left-View\n( Rd \/ Rtotal ) x 100 [%]\nP\nS\nN\nR\n [d\nB\n]\n \n \nRtotal = 1000 kbps\nRtotal = 900 kbps\nRtotal = 800 kbps\nRtotal = 700 kbps\nRtotal = 600 kbps\nRtotal = 500 kbps\nRtotal = 400 kbps\nRtotal = 300 kbps\nRtotal = 200 kbps\n(BD \/ BT+D) \u00d7 100% [%]  \n(a) Interview, left-view (b) Orbi, left-view \n  \n10 20 30 40 50 60 70 80 90\n24\n26\n28\n30\n32\n34\n36\n38\n40\n42\n44\nInterview Right-View\n( Rd \/ Rtotal ) x 100 [%]\nP\nS\nN\nR\n [d\nB\n]\n \n \nRtotal = 1000 kbps\nRtotal = 900 kbps\nRtotal = 800 kbps\nRtotal = 700 kbps\nRtotal = 600 kbps\nRtotal = 500 kbps\nRtotal = 400 kbps\nRtotal = 300 kbps\nRtotal = 200 kbps\n(BD \/ BT+D) \u00d7 100% [ ]  \n10 20 30 40 50 60 70 80 90\n26\n28\n30\n32\n34\n36\n38\n40\n42\n44\nOrbi Right-View\n( Rd \/ Rtotal ) x 100 [%]\nP\nS\nN\nR\n [d\nB\n]\n \n \nRtotal = 1000 kbps\nRtotal = 900 kbps\nRtotal = 800 kbps\nRtotal = 700 kbps\nRtotal = 600 kbps\nRtotal = 500 kbps\nRtotal = 400 kbps\nRtotal = 300 kbps\nRtotal = 200 kbps\n(BD \/ BT+D) \u00d7 100% [%]  \n(c) Interview, right-view (d) Orbi, right-view \n  \n10 20 30 40 50 60 70 80 90\n24\n26\n28\n30\n32\n34\n36\n38\n40\n42\nInterview\n( Rd \/ Rtotal ) x 100 [%]\nA\nve\nra\nge\n P\nS\nN\nR\n [d\nB\n]\n \n \nRtotal = 1000 kbps\nRtotal = 900 kbps\nRtotal = 800 kbps\nRtotal = 700 kbps\nRtotal = 600 kbps\nRtotal = 500 kbps\nRtotal = 400 kbps\nRtotal = 300 kbps\nRtotal = 200 kbps\n(BD \/ BT+D) \u00d7 100% [%]  \n10 20 30 40 50 60 70 80 90\n26\n28\n30\n32\n34\n36\n38\n40\n42\n44\nOrbi\n( Rd \/ Rtotal ) x 100 [%]\nA\nve\nra\nge\n P\nS\nN\nR\n [d\nB\n]\n \n \nRtotal = 1000 kbps\nRtotal = 900 kbps\nRtotal = 800 kbps\nRtotal = 700 kbps\nRtotal = 600 kbps\nRtotal = 500 kbps\nRtotal = 400 kbps\nRtotal = 300 kbps\nRtotal = 200 kbps\n(BD \/ BT+D) \u00d7 10 % [%]  \n(e) Interview, average results for left and right views  (f) Orbi, average results for left and right views \n  \nFigure 7 Decoding quality of reconstructed sequences for different coding conditions. \n \nThe total bit budget Btotal for transmission is fixed to 2 Mbps. \nThe coding rates of color and depth map are denoted as RT and \nRD, respectively. The relation between the total bit budget and \nthe color bit rate BT and the depth map bit rate BD is: \n \n T D\ntotal\nT D\nB B B\nR R\n+ \u2248  (3) \nWhen the ratio of the color and depth map bit rates is set to \nthe optimized values of the total source coding rate BT+D the \ntotal bit rate is: \n \n 0.8 0.2T D T D\ntotal\nT D\nB B B\nR R\n+ +\u22c5 \u22c5+ \u2248  (4) \nIn this experiment, each frame of \u201cOrbi\u201d and \u201cInterview\u201d \ntest sequences is subdivided into 18 slices. Color and depth \nmap data streams are protected by the LDPC code with coding \nrates of 1\/2, 2\/3, 3\/4 and 5\/6. With all combinations, the 16 \nprotection schemes are available. The WiMAX-LDPC is used \nB. Kamolrat et al.: Joint Source and Channel Coding for 3D Video with Depth Image - Based Rendering 891\nAuthorized licensed use limited to: University of Surrey. Downloaded on April 13,2010 at 14:32:44 UTC from IEEE Xplore.  Restrictions apply. \n to convey color and depth map data streams over wireless \nchannel which has the characteristic of Rayleigh fading. \nWiMAX frame size is set to 1056 bits. The data stream is \nmodulated with 64-QAM. At the receiver, the data stream is \ndemodulated by log-MAP algorithm and decoded by sum-\nproduct decoding algorithm with maximum iteration set to 50. \nMoreover, it is assumed that all errors can be perfectly \ndetected and if an error is detected within slice after channel \ndecoder, the whole slice is simply dropped. Finally, the error \nconcealment mode 1 of the reference software JM version 10 \n[18] is applied by simply copying the pixel area from the \nreference frame that is at the same location as the macroblock \nin the current image. \nThe results are expressed by the PSNR of reconstructed \nvideos for different channel signal-to-noise ratio SNRs. The \nprotection levels are indicated by coding rate of color and \ndepth map where \u201cX and Y\u201d means RT equals X and RD equals \nY. \n \n8 10 12 14 16 18 20\n15\n20\n25\n30\n35\n40\n45\n50\nInterview (Colour)\nEb\/N0 [dB]\nP\nS\nN\nR\n [d\nB\n]\n \n \n1\/2 and 1\/2\n1\/2 and 2\/3\n1\/2 and 3\/4\n1\/2 and 5\/6\n2\/3 and 1\/2\n2\/3 and 2\/3\n2\/3 and 3\/4\n2\/3 and 5\/6\n3\/4 and 1\/2\n3\/4 and 2\/3\n3\/4 and 3\/4\n3\/4 and 5\/6\n5\/6 and 1\/2\n5\/6 and 2\/3\n5\/6 and 3\/4\n5\/6 and 5\/6\nEb \/ 0 [d ] \n \n8 10 12 14 16 18 20\n20\n25\n30\n35\n40\n45\n50\nOrbi (Colour)\nEb\/N0 [dB]\nP\nS\nN\nR\n [d\nB\n]\n \n \n1\/2 and 1\/2\n1\/2 and 2\/3\n1\/2 and 3\/4\n1\/2 and 5\/6\n2\/3 and 1\/2\n2\/3 and 2\/3\n2\/3 and 3\/4\n2\/3 and 5\/6\n3\/4 and 1\/2\n3\/4 and 2\/3\n3\/4 and 3\/4\n3\/4 and 5\/6\n5\/6 and 1\/2\n5\/6 and 2\/3\n5\/6 and 3\/4\n5\/6 and 5\/6\nEb \/ 0 [d ] \n \n(a) (b) \n  \n8 10 12 14 16 18 20\n15\n20\n25\n30\n35\n40\n45\n50\nInterview (Depth Map) \nEb\/N0 [dB]\nP\nS\nN\nR\n [d\nB\n]\n \n \n1\/2 and 1\/2\n1\/2 and 2\/3\n1\/2 and 3\/4\n1\/2 and 5\/6\n2\/3 and 1\/2\n2\/3 and 2\/3\n2\/3 and 3\/4\n2\/3 and 5\/6\n3\/4 and 1\/2\n3\/4 and 2\/3\n3\/4 and 3\/4\n3\/4 and 5\/6\n5\/6 and 1\/2\n5\/6 and 2\/3\n5\/6 and 3\/4\n5\/6 and 5\/6\nEb \/ 0 [d ] \n \n8 10 12 14 16 18 20\n20\n25\n30\n35\n40\n45\nEb\/N0 [dB]\nP\nS\nN\nR\n [d\nB\n]\nOrbi (Depth Map)\n \n \n1\/2 and 1\/2\n1\/2 and 2\/3\n1\/2 and 3\/4\n1\/2 and 5\/6\n2\/3 and 1\/2\n2\/3 and 2\/3\n2\/3 and 3\/4\n2\/3 and 5\/6\n3\/4 and 1\/2\n3\/4 and 2\/3\n3\/4 and 3\/4\n3\/4 and 5\/6\n5\/6 and 1\/2\n5\/6 and 2\/3\n5\/6 and 3\/4\n5\/6 and 5\/6\nEb \/ 0 [ ] \n \n(c) (d) \n  \n8 10 12 14 16 18 20\n15\n20\n25\n30\n35\n40\n45\n50\nInterview\nEb\/N0 [dB]\nA\nve\nra\nge\n P\nS\nN\nR\n [d\nB\n]\n \n \n1\/2 and 1\/2\n1\/2 and 2\/3\n1\/2 and 3\/4\n1\/2 and 5\/6\n2\/3 and 1\/2\n2\/3 and 2\/3\n2\/3 and 3\/4\n2\/3 and 5\/6\n3\/4 and 1\/2\n3\/4 and 2\/3\n3\/4 and 3\/4\n3\/4 and 5\/6\n5\/6 and 1\/2\n5\/6 and 2\/3\n5\/6 and 3\/4\n5\/6 and 5\/6\nEb \/ N0 [dB] \nInterview (Left-Right Views) \n \n8 10 12 14 16 18 20\n20\n25\n30\n35\n40\n45\nEb\/N0  [dB]\nA\nve\nra\nge\n P\nS\nN\nR\n [d\nB\n]\nOrbi (Left-Right Views) \n \n \n1\/2 and 1\/2\n1\/2 and 2\/3\n1\/2 and 3\/4\n1\/2 and 5\/6\n2\/3 and 1\/2\n2\/3 and 2\/3\n2\/3 and 3\/4\n2\/3 and 5\/6\n3\/4 and 1\/2\n3\/4 and 2\/3\n3\/4 and 3\/4\n3\/4 and 5\/6\n5\/6 and 1\/2\n5\/6 and 2\/3\n5\/6 and 3\/4\n5\/6 and 5\/6\nEb \/ N0 [dB] \n \n(e) (f) \n  \nFigure 8 The quality of reconstructed 3D video bit streams when the levels of protection are varied. \n \n892 IEEE Transactions on Consumer Electronics, Vol. 54, No. 2, MAY 2008\nAuthorized licensed use limited to: University of Surrey. Downloaded on April 13,2010 at 14:32:44 UTC from IEEE Xplore.  Restrictions apply. \n Figure 8 presents the PSNR of color, depth and the average \nleft and right views for different channel SNRs. The results \nsuggest that the PSNR of reconstructed left and right views is \ndominated by the quality of color images. For example, in the \ncase of \u201c1\/2 and 5\/6\u201d even though the depth map is not \navailable until the SNR of 14 dB, the average PSNR of left \nand right views is about 26 dB (in SNR range of 10 dB to \n13 dB). On the other hand, in case of \u201c5\/6 and 1\/2\u201d the \naverage PSNR of reconstructed left and right views is very \nlow or not available at SNR below 16 dB since no or very low \nquality of color images is available. The effect of this \nphenomenon on visual quality is presented in Figure 9 for the \n15th frame of color, depth map, left-view and right-view In \ncase of \u201c1\/2 and 5\/6\u201d at SNR 11 dB even though no depth \nmap is available, the visual quality of synthesized left and \nright views is acceptable. However, when displaying them on \n3D display, viewers cannot perceive the sense of depth due to \nthe absence of depth information. In the case of \u201c5\/6 and 12\u201d \nat SNR 15 dB, where the quality of color is low but the quality \nof depth map is excellent, the quality of reconstructed left and \nright views is very low. When displaying them on 3D display, \neven though the video quality is not good, viewer can perceive \nthe sense of depth.  \nWhen the protection level is varied, the system performance \ncan be summarized as follow. The \u201c1\/2 and 1\/2\u201d protection \nscheme has the best performance in low SNR region since it \nprovides the strongest protection level. However, at high SNR \nregion the \u201c5\/6 and 5\/6\u201d which provides the lowest protection, \ncan obtain the highest PSNR due to low redundant bits. \nTherefore, to maximize the system performance both the \nchannel condition and the source coding scheme have to be \nconsidered. Such JSCC approach maximizes the decoding \nquality since it enables selection of the appropriate protection \nlevel according to channel conditions. \nIn the final experiment the impact of the number of slices \nper frame to the overall system performance is investigated. A \nsingle frame of \u201c1\/2 and 1\/2\u201d coding pair is separated into 36, \n18, 12 and 6 slices per frame. The experiment results are \nshown in Figure 10. \nFor low SNRs, the application of 36 slices per frame \nachieves the best result. For example, at SNR of 10 dB with \nsource coding with 36 slices per frame the video quality is \napproximately 0.9 dB higher than in the case of 18 slices per \nframe, 3 dB higher than in the case of 12 slices per frame and \n7 dB higher than in the case of 6 slices per frame. However, at \nerror free region the PSNR values for source coding schemes \nwith 36, 18, 12 and 6 slices per frame are saturated at \n40.28 dB, 41.59 dB, 41.79 dB and 41.93 dB, respectively. It is \nclear that at high channel SNRs, the 6 slices per frame is the \nbest option since it has the minimum rate of overhead bits. As \nmentioned before, the immunity to channel error can be \nenhanced by splitting a single frame into many slices with a \ntradeoff of redundant bits. \n \n \n \n \n(a) original frames \n \n  \nnot  \navailable \n \n \n(b) decoded frames with coding setting \"1\/2 and 5\/6\" \n \n \n \n \n \n(c) decoded frames with coding setting \"5\/6 and 1\/2\" \n \nFigure 9 Decoded frames for different channel code rates. \n \n8 9 10 11 12 13 14 15 16\n20\n25\n30\n35\n40\n45\nEb\/N0 [dB]\nP\nS\nN\nR\n [d\nB\n]\nOrbi\n \n \n6 Slices\n12 Slices\n18 Slices\n36 Slices\nEb\/N0 [dB]  \nFigure 10 The performance of \u201c1\/2 and 1\/2\u201d when frame is spited into 36, \n18, 12 and 6 slices. \nV. CONCLUSIONS \nIn this paper, we consider optimized bit allocation and \nJSCC for DIBR based 3D video to achieve high compression \nefficiency and robustness to channel errors. Instead of using \nleft and right views to represent 3D video, the monoscopic \nvideo and associated per-pixel depth information are used to \ngain better compression. Experimental results clearly show \nthat the quality of 3D video is dominated by quality of the \ncolor. Besides, when they are encoded by H.264\/AVC, the \nratio of color to depth map bit rate is found to be suitable \nwhen depth map bit rate is about 20 % to the total source \ncoding bit rate. This result comes up with the fact that the \ncomplexity of depth map is much lower than color leading to \nsignificantly redundant information that can be compressed. \nB. Kamolrat et al.: Joint Source and Channel Coding for 3D Video with Depth Image - Based Rendering 893\nAuthorized licensed use limited to: University of Surrey. Downloaded on April 13,2010 at 14:32:44 UTC from IEEE Xplore.  Restrictions apply. \n When channel coding is used, results show that stronger \nchannel coding at lower bit rates produce higher video quality \nand at high bit rates, weaker channel codes produces better \npicture quality. Results also suggest that quality of the depth \nimage does not have a big impact on final picture quality and \ntherefore, a lower channel code can be used to code the depth \nsequence.  \nREFERENCES \n[1] Fehn, C., \"A 3D-TV system based on video plus depth information,\" \nSignals, Systems and Computers, 2003. Conference Record of the \nThirty-Seventh Asilomar Conference on , vol.2, no., pp. 1529-1533 \nVol.2, 9-12 Nov. 2003  \n[2] \"Draft ITU-T recommendation and final draft international standard of \njoint video specification (ITU-T Rec. H.264\/ISO\/IEC 14 496-10 AVC,\" \nin Joint Video Team (JVT) of ISO\/IEC MPEG and ITU-T VCEG, \nJVTG050, 2003. \n[3] IEEE 802.16. Worldwide Interoperability for Microwave Access, \nwww.wimaxforum.org. \n[4] IEEE 802.16e. Air interface for fixed and mobile broadband wireless \naccess systems. IEEE P802.16e\/D12 Draft, Oct. 2005. \n[5] R. G. Gallager, \"Low-Density Parity-Check Codes,\" M.I.T. Press, \nCambridge, Massachusetts, 1963. \n[6] S. Y. Chung, G. D. Forney, T. J. Richardson, and R. Urbanke, \"On the \nDesign of Low-Density Parity-Check Codes within 0.0045 dB of the \nShannon Limit,\" IEEE Comm., vol. 5, pp. 58-60, February 2001. \n[7] L. Hanzo, P. J. Cherriman, and J. Stereit, Wireless Video \nCommunications Second to Third Generation Systems and Beyond, New \nYork: IEEE PRESS, 2001. \n[8] A. H. Murad and T. E. Fuja, \"Joint Source-Channel Decoding of \nVariable-Length Encoded Sources,\" in Proc. IEEE Information Theory \nWorkshop 1998, pp. 94-95, June 1998. \n[9] M. J. Ruf and J. W. Modestino, \"Rate-Distortion Performance for Joint \nSource and Channel Coding of images \" IEEE Trans. on Image \nProcessing, pp. 305-320, 1999. \n[10] J. Cai and C. W. Chen, \"Operational Rate-Distortion Design for Joint \nSource and Channel Coding over Noisy Channels,\" Proc. IEEE Wireless \nCommunications and Networking Conference 1999, September 1999. \n[11] M. Bystrom and J. W. Modestino, \"Combined Source-Channel Coding \nSchemes for Video Transmission over an Additive White Gaussian \nNoise Channel,\" IEEE in Comm., vol. 18, pp. 880-890, 2000. \n[12] J. Xu, Q. Zhang, W. Zhu, X. G. Xia, and Y. Q. Zhang, \"Optimal Joint \nSource-Channel Bit Allocation for MPEG-4 Fine Granularity Scalable \nVideo over OFDM System,\" in Proc. of the 2003 International \nSymposium on Circuits and Systems, vol. 2, pp. 360-363, May 2003. \n[13] H. Imaizumi and A. Luthra, \"MPEG-2 Multiview Profile\", Three-\nDimensional Television, Video, and Display Technologies, pp. 169-181, \n2002. \n[14] W. A. IJsselsteijn, P. J. H. Seutiens, and L. M. J. Meesters, \"State-of-the-\nArt in Human Factors and Quality Issues of Stereoscopic Broadcast \nTelevision,\" Technical Report D1, IST-2001-34396 ATTEST 2002. \n[15] C. Fehn, R. Rarre, and S. Pastoor, \u201cInteractive 3-DTV\u2014Concepts and \nKey Technologies,\u201d Proc. IEEE, vol. 94, no.3, pp. 524-538, 2006 \n[16] D.-K. Kwon, M.-Y. Shen, and C. C. J. Kuo, \"Rate Control for H.264 \nVideo with Enhanced Rate and Distortion Models,\" IEEE Trans. on \nCircuits and Systems for Video Technology, vol. 17, pp. 517-529, 2007. \n[17] A. Ksentini, A. Gueroui, and M. Naimi, \"Improving H.264 video \ntransmission in 802.11e EDCA,\". in Proc. of 14th International \nConference on Computer Communications and Networks, ICCCN 2005, \npp. 381-386, 2005. \n[18] H.264\/AVC JM Refernce Software, http:\/\/iphome.hhi.de\/suehring\/tml\/ \n \nB. Kamolrat received the BSc in Electrical Engineering \nfrom Royal Thai Naval Academy (RTNA), Thailand in \n2000. He had served for the Royal Thai Navy (RTN) \nfrom 2000 to 2003 before he got a scholarship from the \nRTN to further his study at the University of Newcastle \nupon Tyne, Uk in 2004 and received MSc degree \n(Distinction) in Communication and Signal Processing \nin 2005. Currently he is a research student completing \nhis PhD in the Centre for Communication Systems Research (CCSR) in the \nUniversity of Surrey, UK. His research interests are in 3D video coding and \nvideo communications.  \n \nW.A.C. Fernando received the B.Sc. Engineering \ndegree (First class) in Electronic and \nTelecommunications Engineering from the University \nof Moratuwa, Sri Lanka in 1995 and the MEng degree \n(Distinction) in Telecommunications from Asian \nInstitute of Technology (AIT), Bangkok, Thailand in \n1997. He completed his PhD at the Department of \nElectrical and Electronic Engineering, University of \nBristol, UK in February 2001. Currently, he is a senior lecture in signal \nprocessing at the University of Surrey, UK. Prior to that, he was a senior \nlecturer in Brunel University, UK and an assistant professor in AIT. His \ncurrent research interests include Distribute Video Coding (DVC), 3D video \ncoding, intelligent video encoding for wireless communications, OFDM and \nCDMA for wireless channels, channel coding and modulation schemes for \nwireless channels. He has published more than 150 international papers on \nthese areas. He is a senior member of IEEE and a fellow of the HEA, UK. He \nis also a member of the EPSRC College. \n \nM. Mrak received the BSc and MSc degrees in \nelectrical engineering from University of Zagreb, \nCroatia, in 2001 and 2003, respecitively, and the PhD \ndegree from Queen Mary University of London, UK, \nin 2006. From 2001 to 2003 she was a research \nassistant at the University of Zagreb. In 2002 she was \na visitor in the Image Processing Department, Heinrich-Hertz-Institut, \nGermany. From 2004 to 2007 she was a member of Multimedia and Vision \nResearch Group at Queen Mary University of London where she was a work-\npackage leader at EU FP6 integrated project aceMedia. In 2007 she joined the \nCentre for Communication System Research, University of Surrey, UK, as a \nresearch fellow. Her main research interests are visual data processing, video \nanalysis, video coding and adaptive transforms for image coding. She acts as \nreviewer for several international conferences and journals, and she ia author \nof more than 50 publications. Dr Mrak is the recipient of the principal's award \nfor best student work and has received two awards and the bronze medal from \nthe Faculty of Electrical Engineering and Computing in Zagreb for the \nexemplary success during study. She was awarded a scholarship from DAAD \nin 2002. She has been a member of the organizing\/technical committee of \nseveral conferences, including IET VIE 2008 and CBMI 2008, co-chair of \nworkshop on \"Scalable coded media beyond compression\" and she served as \nsession chair and organiser of conference special sessions. \n \nA. Kondoz studied B.Sc. (Hons.) degree in \nengineering, and M.Sc. degree in telematics in 1983 \nand 1984, respectively, before receiving his Ph.D. \ndegree in communication in 1986. He became a \nLecturer in 1988, a Reader in 1995, and then in 1996, a \nProfessor in Multimedia Communication Systems and \ndeputy director of Center for Communication Systems \nResearch (CCSR), University of Surrey, Guildford, U.K. He has over 250 \npublications, including two books on low-bit-rate speech coding and several \nbook chapters, and seven patents. He has graduated more than 50 Ph.D. \nstudents in the areas of speech\/image and signal processing and wireless \nmultimedia communications, and has been a consultant for major wireless \nmedia terminal developers and manufacturers. Prof. Kondoz has been awarded \nseveral prizes, the most significant of which are The Royal Television \nSocieties\u2019 Communications Innovation Award and The IEE Benefactors \nPremium Award. Prof. Kondoz is also a director of Mulsys Ltd. a University \nof Surrey spin-off company marketing worlds first secure GSM \ncommunication system through the GSM voice channel. \n \n \n894 IEEE Transactions on Consumer Electronics, Vol. 54, No. 2, MAY 2008\nAuthorized licensed use limited to: University of Surrey. Downloaded on April 13,2010 at 14:32:44 UTC from IEEE Xplore.  Restrictions apply. \n"}