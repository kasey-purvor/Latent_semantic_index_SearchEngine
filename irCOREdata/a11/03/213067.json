{"doi":"10.1007\/BF02523391","coreId":"213067","oai":"oai:eprints.lse.ac.uk:25830","identifiers":["oai:eprints.lse.ac.uk:25830","10.1007\/BF02523391"],"title":"Forecasting non-stationary time series by wavelet process modelling","authors":["Fryzlewicz, Piotr","van Bellegem, S\u00e9bastien","von Sachs, Rainer"],"enrichments":{"references":[{"id":17293934,"title":"Adaptive covariance estimation of locally stationary processes.","authors":[],"date":"1998","doi":"10.1214\/aos\/1030563977","raw":"Mallat, S., Papanicolaou, G. and Zhang, Z. (1998). Adaptive covariance estimation of locally stationary processes. Ann. Statist., 26, 1{47.","cites":null},{"id":17293907,"title":"Asymptotic statistical inference for nonstationary processes with evolutionary spectra. In","authors":[],"date":"1996","doi":"10.1007\/978-1-4612-2412-9_11","raw":"Dahlhaus, R. (1996a). Asymptotic statistical inference for nonstationary processes with evolutionary spectra. In P. Robinson and M. Rosenblatt (Eds.), Athens conference on applied probability and time series analysis (Vol. 2). Springer, New York.","cites":null},{"id":17293941,"title":"Automatic statistical analysis of bivariate nonstationary time series.","authors":[],"date":"2001","doi":"10.1198\/016214501753168244","raw":"Ombao, H., Raz, J., von Sachs, R. and Malow, B. (2001). Automatic statistical analysis of bivariate nonstationary time series. J. Amer. Statist. Assoc., 96, 543{560.","cites":null},{"id":17293936,"title":"Contributions to the evolutionary spectral theory.","authors":[],"date":"1989","doi":"10.1111\/j.1467-9892.1989.tb00014.x","raw":"M elard, G. and Herteleer-De Schutter, A. (1989). Contributions to the evolutionary spectral theory. J. Time Ser. Anal., 10, 41{63.","cites":null},{"id":17293942,"title":"El Ni~ no, La Ni~ na and the southern oscillation. San Diego:","authors":[],"date":"1990","doi":null,"raw":"Philander, S. (1990). El Ni~ no, La Ni~ na and the southern oscillation. San Diego: Academic Press.","cites":null},{"id":17293943,"title":"Evolutionary spectra and non-stationary processes.","authors":[],"date":"1965","doi":null,"raw":"Priestley, M. (1965). Evolutionary spectra and non-stationary processes. J. Roy. Statist. Soc. Ser. B, 27, 204{237.","cites":null},{"id":17293913,"title":"Fitting time series models to nonstationary processes.","authors":[],"date":"1997","doi":"10.1214\/aos\/1034276620","raw":"26Dahlhaus, R. (1997). Fitting time series models to nonstationary processes. Ann. Statist., 25, 1{37.","cites":null},{"id":17293944,"title":"Forecasting economic time series using models of nonstationarity (Discussion","authors":[],"date":"2002","doi":"10.1016\/j.ijforecast.2003.10.002","raw":"Van Bellegem, S. and von Sachs, R. (2002). Forecasting economic time series using models of nonstationarity (Discussion paper No. 0227). Institut de statistique, UCL. (ftp:\/\/www. stat.ucl.ac.be\/pub\/papers\/dp\/dp02\/dp0227.ps) 27List of Figures 1 These simulated examples demonstrate the idea of a sparse representation of the local (co)variance. The left-hand column shows an example of a smooth time-varying variance function of a TM process. The example on the right hand side is constructed in such a way that the local variance function c(z;0) is constant over time. In this example, the only deviation from stationarity is in the covariance structure. The simulations, like all throughout the article, use Gaussian innovations jk and Haar wavelets. . . . . . . . . . . . . . . . . 29 (a) Theoretical wavelet spectrum equal to zero everywhere except scale 2 where S2(z) = 0:1 + cos2(3z + 0:25). . . . . . . . . . . . . . . . . . 29 (b) Theoretical wavelet spectrum S2(z) = 0:1+cos2(3z+0:25), S1(z) = 0:1 + sin 2(3z + 0:25) and Sj(z) = 0 for j 6= 1;2. . . . . . . . . . . 29 (c) A sample path of length 1024 simulated from the wavelet spectrum dened in (a). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 (d) A sample path of length 1024 simulated from the wavelet spectrum dened in (b). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 2 The wind anomaly data (910 observations from March 1920 to December 1995). 30 (a) The wind anomaly index (in cm\/s). The two vertical lines indicate the segment shown in Figure 2(b). . . . . . . . . . . . . . . . . . . . . . . . 30 (b) Comparison between the one-step-ahead prediction in our model (dashed lines) and AR (dotted lines). . . . . . . . . . . . . . . . . . . . . . . . . 30 3 The last observations of the wind anomaly series and its 1- up to 9-step-ahead forecasts (in cm\/s). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 (a) 9-step-ahead prediction using LSW modelling . . . . . . . . . . . . . . 31 (b) 9-step-ahead prediction using AR modelling . . . . . . . . . . . . . . . 31","cites":null},{"id":17293901,"title":"Forecasting multifractal volatility.","authors":[],"date":"2001","doi":"10.1016\/s0304-4076(01)00069-0","raw":"Calvet, L. and Fisher, A. (2001). Forecasting multifractal volatility. J. Econometrics, 105, 27{58.","cites":null},{"id":17293928,"title":"Modelling and forecasting  log-returns as locally stationary wavelet processes (Research Report).","authors":[],"date":"2002","doi":null,"raw":"Fry zlewicz, P. (2002). Modelling and forecasting nancial log-returns as locally stationary wavelet processes (Research Report). Department of Mathematics, University of Bristol. (http: \/\/www.stats.bris.ac.uk\/pub\/ResRept\/2002.html) Grillenzoni, C. (2000). Time-varying parameters prediction. Ann. Inst. Statist. Math., 52, 108{122.","cites":null},{"id":17293916,"title":"Non-linear wavelet estimation of timevarying autoregressive processes.","authors":[],"date":"1999","doi":"10.2307\/3318448","raw":"Dahlhaus, R., Neumann, M. H. and von Sachs, R. (1999). Non-linear wavelet estimation of timevarying autoregressive processes. Bernoulli, 5, 873{906.","cites":null},{"id":17293930,"title":"Numerical analysis.","authors":[],"date":"1991","doi":"10.1007\/978-1-4612-0599-9","raw":"Kress, R. (1991). Numerical analysis. New York: Springer.","cites":null},{"id":17293910,"title":"On the Kullback-Leibler information divergence of locally stationary processes. Stochastic Process.","authors":[],"date":"1996","doi":"10.1016\/0304-4149(95)00090-9","raw":"Dahlhaus, R. (1996b). On the Kullback-Leibler information divergence of locally stationary processes. Stochastic Process. Appl., 62, 139{168.","cites":null},{"id":17293933,"title":"Recursive estimation and adaptive forecasting in ARIMA models with time varying coecients. In Applied Time Series Analysis,","authors":[],"date":"1980","doi":"10.1016\/b978-0-12-256420-8.50020-4","raw":"Ledolter, J. (1980). Recursive estimation and adaptive forecasting in ARIMA models with time varying coecients. In Applied Time Series Analysis, II (Tulsa, Okla.) (pp. 449{471). New York-London: Academic Press.","cites":null},{"id":17293921,"title":"Ten lectures on wavelets.","authors":[],"date":"1992","doi":"10.1137\/1.9781611970104","raw":"Daubechies, I. (1992). Ten lectures on wavelets. Philadelphia: SIAM.","cites":null},{"id":17293940,"title":"The SLEX model of a non-stationary random process.","authors":[],"date":"2002","doi":null,"raw":"Ombao, H., Raz, J., von Sachs, R. and Guo, W. (2002). The SLEX model of a non-stationary random process. Ann. Inst. Statist. Math., 54, 171{200.","cites":null},{"id":17293898,"title":"Time series: Theory and methods","authors":[],"date":"1991","doi":"10.1007\/978-1-4419-0320-4","raw":"Brockwell, P. J. and Davis, R. A. (1991). Time series: Theory and methods (Second ed.). Springer, New York.","cites":null},{"id":17293905,"title":"Time-invariant de-noising.","authors":[],"date":"1995","doi":"10.1007\/978-1-4612-2544-7_9","raw":"Coifman, R. and Donoho, D. (1995). Time-invariant de-noising. In A. Antoniadis and G. Oppenheim (Eds.), Wavelets and Statistics (Vol. 103, pp. 125{150). New York: Springer-Verlag.","cites":null},{"id":17293896,"title":"Wavelet methods for continuous-time prediction using representations of autoregressive processes in Hilbert spaces.","authors":[],"date":"2002","doi":"10.1016\/s0047-259x(03)00028-9","raw":"Antoniadis, A. and Sapatinas, T. (2002). Wavelet methods for continuous-time prediction using representations of autoregressive processes in Hilbert spaces. J. Multivariate Anal. (Under revision) Berkner, K. and Wells, R. (2002). Smoothness estimates for soft-threshold denoising via translationinvariant wavelet transforms. Appl. Comput. Harmon. Anal., 12, 1{24.","cites":null},{"id":17293939,"title":"Wavelet processes and adaptive estimation of evolutionary wavelet spectra.","authors":[],"date":"2000","doi":"10.1111\/1467-9868.00231","raw":"Nason, G. P., von Sachs, R. and Kroisandt, G. (2000). Wavelet processes and adaptive estimation of evolutionary wavelet spectra. J. Roy. Statist. Soc. Ser. B, 62, 271{292.","cites":null},{"id":17293938,"title":"Wavelets in time series analysis.","authors":[],"date":"1999","doi":"10.1098\/rsta.1999.0445","raw":"Nason, G. P. and von Sachs, R. (1999). Wavelets in time series analysis. Phil. Trans. Roy. Soc.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2003-12","abstract":"Many time series in the applied sciences display a time-varying second order structure. In this article, we address the problem of how to forecast these nonstationary time series by means of non-decimated wavelets. Using the class of Locally Stationary Wavelet processes, we introduce a new predictor based on wavelets and derive the prediction equations as a generalisation of the Yule-Walker equations. We propose an automatic computational procedure for choosing the parameters of the forecasting algorithm. Finally, we apply the prediction algorithm to a meteorological time series","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/213067.pdf","fullTextIdentifier":"http:\/\/eprints.lse.ac.uk\/25830\/1\/Forecasting_non-stationary_time_series_by_wavelet_process_modelling%28lsero%29.pdf","pdfHashValue":"a4b8ab58035ee963c16faad64f0ba25f34c7d8e3","publisher":"Springer Netherlands","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lse.ac.uk:25830<\/identifier><datestamp>\n      2011-11-25T11:16:03Z<\/datestamp><setSpec>\n      74797065733D4445505453:4C53452D5354<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lse.ac.uk\/25830\/<\/dc:relation><dc:title>\n        Forecasting non-stationary time series by wavelet process modelling<\/dc:title><dc:creator>\n        Fryzlewicz, Piotr<\/dc:creator><dc:creator>\n        van Bellegem, S\u00e9bastien<\/dc:creator><dc:creator>\n        von Sachs, Rainer<\/dc:creator><dc:subject>\n        HA Statistics<\/dc:subject><dc:description>\n        Many time series in the applied sciences display a time-varying second order structure. In this article, we address the problem of how to forecast these nonstationary time series by means of non-decimated wavelets. Using the class of Locally Stationary Wavelet processes, we introduce a new predictor based on wavelets and derive the prediction equations as a generalisation of the Yule-Walker equations. We propose an automatic computational procedure for choosing the parameters of the forecasting algorithm. Finally, we apply the prediction algorithm to a meteorological time series.<\/dc:description><dc:publisher>\n        Springer Netherlands<\/dc:publisher><dc:date>\n        2003-12<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lse.ac.uk\/25830\/1\/Forecasting_non-stationary_time_series_by_wavelet_process_modelling%28lsero%29.pdf<\/dc:identifier><dc:identifier>\n          Fryzlewicz, Piotr and van Bellegem, S\u00e9bastien and von Sachs, Rainer  (2003) Forecasting non-stationary time series by wavelet process modelling.  Annals of the Institute of Statistical Mathematics, 55 (4).  pp. 737-764.  ISSN 0020-3157     <\/dc:identifier><dc:relation>\n        http:\/\/www.springer.com\/statistics\/journal\/10463<\/dc:relation><dc:relation>\n        10.1007\/BF02523391<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lse.ac.uk\/25830\/","http:\/\/www.springer.com\/statistics\/journal\/10463","10.1007\/BF02523391"],"year":2003,"topics":["HA Statistics"],"subject":["Article","PeerReviewed"],"fullText":" \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nPiotr Fryzlewicz, S\u00e9bastien van Bellegem and Rainer von \nSachs \nForecasting non-stationary time series by \nwavelet process modelling \n \n \n \nArticle (Accepted version) \n(Unrefereed) \n \n \nOriginal citation: \nFryzlewicz, Piotr and van Bellegem, S\u00e9bastien and von Sachs, Rainer (2003) Forecasting non- \nstationary time series by wavelet process modelling.  Annals of the Institute of Statistical \nMathematics, 55 (4). pp. 737-764. ISSN 0020-3157 \nDOI:  10.1007\/BF02523391 \n \n\u00a9 2003 The Institute of Statistical Mathematics \n \nThis version available at: http:\/\/eprints.lse.ac.uk\/25830\/ \nAvailable in LSE Research Online: November 2011 \n \nLSE has developed LSE Research Online so that users may access research output of the \nSchool. Copyright \u00a9 and Moral Rights for the papers on this site are retained by the individual \nauthors and\/or other copyright owners. Users may download and\/or print one copy of any \narticle(s) in LSE Research Online to facilitate their private study or for non-commercial research. \nYou may not engage in further distribution of the material or use it for any profit-making activities \nor any commercial gain. You may freely distribute the URL (http:\/\/eprints.lse.ac.uk) of the LSE \nResearch Online website. \n \nThis document is the author\u2019s final accepted version of the journal article. There may be \ndifferences between this version and the published version.  You are advised to consult the \npublisher\u2019s version if you wish to cite from it. \nForecasting non-stationary time series\nby wavelet process modelling\nP. Fryz\u00b4lewicz1 S. Van Bellegem2, 4, \u2217 R. von Sachs3, 4\nDecember 16, 2002\nAbstract\nMany time series in the applied sciences display a time-varying second order struc-\nture. In this article, we address the problem of how to forecast these non-stationary\ntime series by means of non-decimated wavelets. Using the class of Locally Station-\nary Wavelet processes, we introduce a new predictor based on wavelets and derive the\nprediction equations as a generalisation of the Yule-Walker equations. We propose\nan automatic computational procedure for choosing the parameters of the forecasting\nalgorithm. Finally, we apply the prediction algorithm to a meteorological time series.\nKeywords: Local stationarity, non-decimated wavelets, prediction, time-modulated pro-\ncesses, Yule-Walker equations.\nRunning head: Forecasting non-stationary processes by wavelets\n1University of Bristol, Department of Mathematics, Bristol, UK. E-mail: P.Z.Fryzlewicz@bristol.ac.uk\n2Research Fellow of the National Fund for Scientific Research (F.N.R.S.). Universite\u00b4 catholique de Lou-\nvain, Institut de statistique, Louvain-la-Neuve, Belgium. E-mail: vanbellegem@stat.ucl.ac.be\n3Universite\u00b4 catholique de Louvain, Institut de statistique, Louvain-la-Neuve, Belgium. E-mail:\nvonsachs@stat.ucl.ac.be\n4Financial support from the contract \u2018Projet d\u2019Actions de Recherche Concerte\u00b4es\u2019 nr. 98\/03-217 of the\nBelgian Government and from the IAP research network No. P5\/24 of the Belgian State (Federal Office for\nScientific, Technical and Cultural Affairs) are gratefully acknowledged.\n* Corresponding author. Address for correspondence: Universite\u00b4 catholique de Louvain, Institut de\nstatistique, Voie du Roman Pays, 20, B-1348 Louvain-la-Neuve, Belgium. Fax: +32 10 47.30.32\n1\n1 Introduction\nIn a growing number of fields, such as biomedical time series analysis, geophysics, telecom-\nmunications, or financial data analysis, to name but a few, explaining and inferring from\nobserved serially correlated data calls for non-stationary models of their second order struc-\nture. That is, variance and covariance, or equivalently the spectral structure, are likely to\nchange over time.\nIn this article, we address the problem of whether and how wavelet methods can help\nin forecasting non-stationary time series. Recently, Antoniadis and Sapatinas (2002) used\nwavelets for forecasting time-continuous stationary processes. The use of wavelets has proved\nsuccessful in capturing local features of observed data. There arises a natural question of\nwhether they can also be useful for prediction in situations where too little homogeneous\nstructure at the end of the observed data set prevents the use of classical prediction methods\nbased on stationarity. Obviously, in order to develop a meaningful approach, one needs to\ncontrol this deviation from stationarity, and hence one first needs to think about what kind\nof non-stationary models to fit to the observed data. Let us give a brief overview of the\nexisting possibilities.\nCertainly the simplest approach consists in assuming piecewise stationarity, or approxi-\nmate piecewise stationarity, where the challenge is to find the stretches of homogeneity opti-\nmally in a data-driven way (Ombao et al., 2001). The resulting estimate of the time-varying\nsecond order structure is, necessarily, rather blocky over time, so some further thoughts on\nhow to cope with these potentially artificially introduced discontinuities are needed. To name\na few out of the many models which allow a smoother change over time, we cite the following\napproaches to the idea of \u201clocal stationarity\u201d: the work of Mallat et al. (1998), who impose\nbounds on the derivative of the Fourier spectrum as a function of time, and the approaches\nwhich allow the coefficients of a parametric model (such as AR) to vary slowly with time\n(e.g. Me\u00b4lard and Herteleer-De Schutter (1989), Dahlhaus et al. (1999) or Grillenzoni (2000)).\nThe following fact is a starting point for several other more general and more non-parametric\napproaches: every covariance-stationary process Xt has a Crame\u00b4r representation\n(1.1) Xt =\n\u222b\n(\u2212pi,pi]\nA(\u03c9) exp(i\u03c9t)dZ(\u03c9), t \u2208 Z,\nwhere Z(\u03c9) is a stochastic process with orthonormal increments. Non-stationary processes\nare defined by assuming a slow change over time of the amplitude A(\u03c9) (Priestley (1965),\nDahlhaus (1997), Ombao et al. (2002)). All the above models are of the \u201ctime-frequency\u201d\ntype as they use, directly or indirectly, the concept of a time-varying spectrum, being the\nFourier transform of a time-varying autocovariance.\nThe work of Nason, von Sachs and Kroisandt (2000) adopts the concept of local sta-\ntionarity but replaces the aforementioned spectral representation with respect to the Fourier\nbasis by a representation with respect to non-decimated (or translation-invariant) wavelets.\nWith their model of \u201cLocally Stationary Wavelet\u201d (LSW) processes, the authors introduce\na time-scale representation of a stochastic process. The representation allows for a rigorous\ntheory of how to estimate the wavelet spectrum, i.e. the coefficients of the resulting repre-\nsentation of the local autocovariance function with respect to autocorrelation wavelets. This\ntheory parallels the one developed by Dahlhaus (1997), where rescaling the time argument\nof the autocovariance and the Fourier spectrum makes it possible to embed the estimation\nin the non-parametric regression setting, including asymptotic considerations of consistency\n2\nand inference. Nason et al. (2000) also propose a fast and easily implementable estimation\nalgorithm which accompanies their theory.\nAs LSW processes are defined with respect to a wavelet system, they have a mean-square\nrepresentation in the time-scale plane. It is worth recalling that many time series in the ap-\nplied sciences are believed to have an inherent \u201cmultiscale\u201d structure (e.g. financial log-return\ndata, see Calvet and Fisher (2001)). In contrast to Fourier-based models of nonstationarity,\nthe LSW model offers a multiscale representation of the (local) covariance (see Section 2).\nThis representation is often sparse, and thus the covariance may be estimated more easily\nin practice. The estimator itself is constructed by means of the wavelet periodogram, which\nmimicks the structure of the LSW model and is naturally localised.\nGiven all these benefits, it seems appropriate to us to use the (linear) LSW model to\ngeneralise the stationary approach of forecasting Xt by means of a predictor based on the\nprevious observations up to time t \u2212 1. While the classical linear predictor can be viewed\nas based on a non-local Fourier-type representation, our generalisation uses a local wavelet-\nbased approach.\nThe paper is organised as follows: Section 2 familiarises the reader with the general\nLSW model, as well as with the particular subclass of time-modulated processes. These are\nstationary processes modulated by a time-varying variance function, and have proved useful,\nfor instance, in modelling financial log-return series (Van Bellegem and von Sachs (2002)).\nIn the central Section 3, we deal with the theory of prediction for LSW processes, where\nthe construction of our linear predictor is motivated by the approach in the stationary case,\ni.e. the objective is to minimise the mean-square prediction error (MSPE). This leads to\na generalisation of the Yule-Walker equations, which can be solved numerically by matrix\ninversion or standard iterative algorithms such as the innovations algorithm (Brockwell and\nDavis, 1991), provided that the non-stationary covariance structure is known. However, the\nestimation of a non-stationary covariance structure is the main challenge in this context, and\nthis issue is addressed in Section 4. In the remainder of Section 3, we derive an analogue of\nthe classical Kolmogorov formula for the theoretical prediction error, and we generalise the\none-step-ahead to h-step-ahead prediction.\nSection 4 deals with estimation of the time-varying covariance structure. We discuss some\nasymptotic properties of our estimators based on the properties of the corrected wavelet\nperiodogram, which is an asymptotically unbiased, but not consistent, estimator of the\nwavelet spectrum. To achieve consistency, we propose an automatic smoothing procedure,\nwhich forms an integral part of our new algorithm for forecasting non-stationary time series.\nThe algorithm implements the idea of adaptive forecasting (see Ledolter (1980)) in the LSW\nmodel. In Section 5 we apply our algorithm to a meteorological time series.\nWe close with a conclusions section and we present our proofs in two appendices. Ap-\npendix A contains all the results related to approximating the finite-sample covariance struc-\nture of the non-stationary time series by the locally stationary limit. In Appendix B, we\nshow some relevant basic properties of the system of autocorrelation wavelets, and provide\nthe remaining proofs of the statements made in Section 3 and 4.\n2 Locally Stationary Wavelet processes\nLSW processes are constructed by replacing the amplitude A(\u03c9) in the Crame\u00b4r representation\n(1.1) with a quantity which depends on time (this ensures that the second-order structure\nof the process changes over time), as well as by replacing the Fourier harmonics exp(i\u03c9t)\n3\nwith non-decimated discrete wavelets \u03c8jk(t), j = \u22121,\u22122, . . ., k \u2208 Z. Here, j is the scale\nparameter (with j = \u22121 denoting the finest scale) and k is the location parameter. Note that\nunlike decimated wavelets, for which the permitted values of k at scale j are restricted to the\nset {c2\u2212j, c \u2208 Z}, non-decimated wavelets can be shifted to any location defined by the finest\nresolution scale, determined by the observed data (k \u2208 Z). As a consequence, non-decimated\nwavelets do not constitute bases for `2 but overcomplete sets of vectors. The reader is referred\nto Coifman and Donoho (1995) for an introduction to non-decimated wavelets.\nBy way of example, we recall the simplest discrete non-decimated wavelet system: the\nHaar wavelets. They are defined by\n\u03c8j0(t) = 2\nj\/2\nI{0,1,...,2\u2212j\u22121\u22121}(t)\u2212 2j\/2I{2\u2212j\u22121 ,...,2\u2212j\u22121}(t) for j = \u22121,\u22122, . . . and t \u2208 Z ,\nand \u03c8jk(t) = \u03c8j0(t\u2212 k) for all k \u2208 Z, where IA(t) is 1 if t \u2208 A and 0 otherwise.\nWe are now in a position to quote the formal definition of an LSW process from Nason,\nvon Sachs and Kroisandt (2000).\nDefinition 1. A sequence of doubly-indexed stochastic processes Xt,T (t = 0, . . . , T \u22121) with\nmean zero is in the class of LSW processes if there exists a mean-square representation\n(2.1) Xt,T =\n\u22121\u2211\nj=\u2212J\n\u221e\u2211\nk=\u2212\u221e\nwj,k;T \u03c8jk(t) \u03bejk,\nwhere {\u03c8jk(t)}jk is a discrete non-decimated family of wavelets for j = \u22121,\u22122, . . . ,\u2212J , based\non a mother wavelet \u03c8(t) of compact support and J = \u2212min{j : Lj 6 T} = O(log(T )), where\nLj is the length of support of \u03c8j0(t). Also,\n1. \u03bejk is a random orthonormal increment sequence with E\u03bejk = 0 and Cov (\u03bejk, \u03be`m) =\n\u03b4j` \u03b4km for all j, `, k,m; where \u03b4j` = 1 if j = ` and 0 otherwise;\n2. For each j 6 \u22121, there exists a Lipschitz-continuous function Wj(z) on (0, 1) possessing\nthe following properties:\n\u2022 \u2211\u22121j=\u2212\u221e |Wj(z)|2 <\u221e uniformly in z \u2208 (0, 1) ;\n\u2022 there exists a sequence of constants Cj such that for each T\n(2.2) sup\nk=0,...,T\u22121\n\u2223\u2223\u2223\u2223wj,k;T \u2212Wj ( kT\n)\u2223\u2223\u2223\u2223 6 CjT ;\n\u2022 the constants Cj and the Lipschitz constants Lj are such that\n\u2211\u22121\nj=\u2212\u221eLj(Cj +\nLjLj) <\u221e.\nLSW processes are not uniquely determined by the sequence {wjk;T}. However, Nason et\nal. (2000) develop a theory which defines a unique spectrum. This spectrum measures the\npower of the process at a particular scale and location. Formally, the evolutionary wavelet\nspectrum of an LSW process {Xt,T}t=0,...,T\u22121, with respect to \u03c8, is defined by\n(2.3) Sj(z) = |Wj(z)|2 , z \u2208 (0, 1)\nand is such that, by definition of the process, Sj(z) = limT\u2192\u221e |wj,[zT ];T |2 for all z in (0, 1).\n4\nRemark 1 (Rescaled time). In Definition 1, the functions {Wj(z)}j and {Sj(z)}j are\ndefined on the interval (0, 1) and not on {0, . . . , T \u2212 1}. Throughout the paper, we refer to\nz as the rescaled time. This idea goes back to Dahlhaus (1997), who shows that the time-\nrescaling permits an asymptotic theory of statistical inference for a time-varying Fourier\nspectrum. The rescaled time is related to the observed time t \u2208 {0, . . . , T \u22121} by the natural\nmapping t = [zT ], which implies that as T \u2192 \u221e, functions {Wj(z)}j and {Sj(z)}j are\nsampled on a finer and finer grid. Due to the rescaled time concept, the estimation of the\nwavelet spectrum {Sj(z)}j is a statistical problem analogous to the estimation of a regression\nfunction (see also Dahlhaus (1996a)).\nIn the classical theory of stationary processes, the spectrum and the autocovariance\nfunction are Fourier transforms of each other. To establish an analogous relationship for\nthe wavelet spectrum, observe that the autocovariance function of an LSW process can be\nwritten as\ncT (z, \u03c4) = Cov\n(\nX[zT ],T , X[zT ]+\u03c4,T\n)\nfor z \u2208 (0, 1) and \u03c4 in Z, and where [ \u00b7 ] denotes the integer part of a real number. The next\nresult shows that this covariance tends to a local covariance as T tends to infinity. Let us\nintroduce the autocorrelation wavelets as\n\u03a8j(\u03c4) =\n\u221e\u2211\nk=\u2212\u221e\n\u03c8jk(0) \u03c8jk(\u03c4) , j < 0, \u03c4 \u2208 Z.\nSome useful properties of the system {\u03a8j}j<0 can be found in Appendix B. By definition,\nthe local autocovariance function of an LSW process with evolutionary spectrum (2.3) is\ngiven by\n(2.4) c (z, \u03c4) =\n\u22121\u2211\nj=\u2212\u221e\nSj(z)\u03a8j (\u03c4)\nfor all \u03c4 \u2208 Z and z in (0, 1). In particular, the local variance is given by the multiscale\ndecomposition\n(2.5) \u03c32(z) = c(z, 0) =\n\u22121\u2211\nj=\u2212\u221e\nSj(z)\nas \u03a8j(0) = 1 for all scales j.\nProposition 1 (Nason et al. (2000)). Under the assumptions of Definition 1, if T \u2192\u221e,\nthen |cT (z, \u03c4)\u2212 c (z, \u03c4)| = O (T\u22121) uniformly in \u03c4 \u2208 Z and z \u2208 (0, 1).\nNote that formula (2.4) provides a decomposition of the autocovariance structure of the\nprocess over scales and rescaled-time locations. In practice, it often turns out that spectrum\nSj(z) is only significantly different from zero at a limited number of scales (Fryz\u00b4lewicz, 2002).\nIf this is the case, then the local autocovariance function c(z, \u03c4) has a sparse representation\nand can thus be estimated more easily.\nRemark 2 (Stationary processes). A stationary process with an absolutely summable\nautocovariance function is an LSW process (Nason et al., 2000, Proposition 3). Stationarity\n5\nis characterised by a wavelet spectrum which is constant over rescaled time: Sj(z) = Sj for\nall z \u2208 (0, 1).\nRemark 3 (Time-modulated processes). Time-modulated (TM) processes constitute a\nparticularly simple class of non-stationary processes. A TM process Xt,T is defined as\n(2.6) Xt,T = \u03c3\n(\nt\nT\n)\nYt,\nwhere Yt is a zero-mean stationary process with variance one, and the local standard deviation\nfunction \u03c3(z) is Lipschitz continuous on (0, 1) with the Lipschitz constant D. Process Xt,T\nis LSW if\n\u2022 the autocovariance function of Yt is absolutely summable (so that Yt is LSW with a\ntime-invariant spectrum {SYj }j);\n\u2022 and if the Lipschitz constants LXj = D(SYj )1\/2 satisfy the requirements of Definition 1.\nIf these two conditions hold, then the spectrum Sj(z) of Xt,T is given by the formula\nSj(z) = \u03c3\n2(z)SYj . The local autocorrelation function \u03c1(\u03c4) = c(z, \u03c4)\/c(z, 0) of a TM pro-\ncess is independent of z.\nHowever, the real advantage of introducing general LSW processes lies in their ability to\nmodel processes whose both variance and autocorrelation function vary over time. Figure\n1 shows simulated examples of LSW processes in which the spectrum is only non-zero at a\nlimited number of scales. A sample realisation of a TM process is plotted in Figure 1(c),\nand Figure 1(d) shows a sample realisation of an LSW process which cannot be modelled as\na TM series.\nFigure 1 here\n3 The predictor and its theoretical properties\nIn this section, we define and analyse the general linear predictor for non-stationary data\nthat are modelled to follow the LSW process representation given in Definition 1.\n3.1 Definition of the linear predictor\nGiven t observations X0,T , X1,T , . . . , Xt\u22121,T of an LSW process, we define the h-step-ahead\npredictor of Xt\u22121+h,T by\n(3.1) X\u02c6t\u22121+h,T =\nt\u22121\u2211\ns=0\nb\n(h)\nt\u22121\u2212s;T Xs,T ,\nwhere the coefficients b\n(h)\nt\u22121\u2212s;T are such that they minimise the Mean Square Prediction Error\n(MSPE). The MSPE is defined by\nMSPE(X\u02c6t\u22121+h,T , Xt\u22121+h,T ) = E\n(\nX\u02c6t\u22121+h,T \u2212Xt\u22121+h,T\n)2\n.\n6\nThe predictor (3.1) is a linear combination of doubly-indexed observations where the\nweights need to follow the same doubly-indexed framework. This means that as T \u2192 \u221e,\nwe augment our knowledge about the local structure of the process, which allows us to\nfit coefficients b\n(h)\nt\u22121\u2212s;T more and more accurately. The double indexing of the weights is\nnecessary due to the non-stationary nature of the data. This scheme is different to the\ntraditional filtering of the data Xs,T by a linear filter {bt}. In particular, we do not assume\nthe (square) summability of the sequence bt because (3.1) is a relation which is written in\nrescaled time.\nThe following assumption holds in the sequel of the paper.\nAssumption 1. If h is the prediction horizon and t is the number of observed data, then\nwe set T = t+ h and we assume h = o(T ).\nRemark 4 (Prediction domain in the rescaled time). With this assumption, the last\nobservation of the LSW process is denoted by Xt\u22121,T = XT\u2212h\u22121,T , while X\u02c6T\u22121,T is the last\npossible forecast (h steps ahead). Consequently, in the rescaled time (see Remark 1), the\nevolutionary wavelet spectrum Sj(z) can only be estimated on the interval\n(3.2)\n[\n0, 1\u2212 h+ 1\nT\n]\n.\nThe rescaled-time segment\n(3.3)\n(\n1\u2212 h + 1\nT\n, 1\n)\naccommodates the predicted values of Sj(z). With Assumption 1, the estimation domain\n(3.2) asymptotically tends to [0, 1) while the prediction domain (3.3) shrinks to an empty set\nin the rescaled time. Thus, Assumption 1 ensures that asymptotically, we acquire knowledge\nof the wavelet spectrum over the full interval [0, 1).\n3.2 Prediction in the wavelet domain\nThere is an interesting link between the above definition of the linear predictor (3.1) and\nanother, \u201cintuitive\u201d definition of a predictor in the LSW model. For ease of presentation,\nlet us suppose the forecasting horizon is h = 1, so that T = t + 1. Given observations up\nto time t \u2212 1, a natural way of defining a predictor of Xt,T is to mimic the structure of the\nLSW model itself by moving to the wavelet domain. The empirical wavelet coefficients are\ndefined by\ndjk;T =\nt\u22121\u2211\ns=0\nXs,T \u03c8jk(s)\nfor all j = \u22121, . . . ,\u2212J and k \u2208 Z. Then, the one-step-ahead predictor is constructed as\n(3.4) X\u02c6t,T =\n\u22121\u2211\nj=\u2212J\n\u2211\nk\u2208 \u0000\ndjk;T a\n(1)\njk;T \u03c8jk(t) ,\nwhere the coefficients a\n(1)\njk have to be estimated and are such that they minimise the MSPE.\nThis predictor (3.4) may be viewed as a projection of Xt,T on the space of random variables\n7\nspanned by {dj,k;T |j = \u22121, . . . ,\u2212J and k = 0, . . . , T \u2212 1}.\nIt turns out that due to the redundancy of the non-orthogonal wavelet system {\u03c8jk(t)},\nthe predictor (3.4) does not have a unique representation: there exists more than one solution\n{a(1)jk } minimising the MSPE, but each solution gives the same predictor (expressed as a\ndifferent linear combination of the redundant functions {\u03c8jk(t)}). One can easily verify this\nobservation by considering, for example, the stationary process Xs =\n\u2211\u221e\nk=\u2212\u221e \u03c8\u22121k(s)\u03b6k,\nwhere \u03c8\u22121 is the non-decimated discrete Haar wavelet at scale \u22121 and \u03b6k is an orthonormal\nincrement sequence.\nIt is not surprising that the wavelet predictor (3.4) is related to the linear predictor (3.1)\nby\nb\n(1)\nt\u2212s;T =\n\u22121\u2211\nj=\u2212J\n\u2211\nk\u2208 \u0000\na\n(1)\njk;T \u03c8jk(t) \u03c8jk(s).\nBecause of the redundancy of the non-decimated wavelet system, for a fixed sequence b\n(1)\nt\u2212s;T ,\nthere exists more than one sequence a\n(1)\njk;T such that this relation holds. For this reason, we\nprefer to work directly with the general linear predictor (3.1), bearing in mind that it can\nalso be expressed as a (non-unique) projection onto the wavelet domain.\n3.3 One-step ahead prediction equations\nIn this subsection, we consider a forecasting horizon h = 1 (so that T = t + 1) and want\nto minimise the mean square prediction error MSPE(X\u02c6t;T , Xt;T ) with respect to b\n(1)\nt\u2212s;T . This\nquadratic function may be written as\nMSPE(X\u02c6t;T , Xt;T ) = b\n\u2032\nt\u03a3t;T bt ,\nwhere bt is the vector (b\n(1)\nt\u22121;T , . . . , b\n(1)\n0;T ,\u22121) and \u03a3t;T is the covariance matrix ofX0;T , . . . , Xt;T .\nHowever, the matrix \u03a3t;T depends on w\n2\njk;T which cannot be estimated, as they are not\nidentifiable (recall that the representation (2.1) is not unique due to the redundancy of\nthe system {\u03c8jk}). The next proposition shows that the MSPE may be approximated by\nb\n\u2032\ntBt;T bt, where Bt;T is a (t + 1)\u00d7 (t+ 1) matrix whose (m,n)\u2212th element is given by\n\u22121\u2211\nj=\u2212J\nSj\n(\nn+m\n2T\n)\n\u03a8j(n\u2212m) ,\nand can be estimated by estimating the (uniquely defined) wavelet spectrum Sj. We first\nconsider the following assumptions on the evolutionary wavelet spectrum.\nAssumption 2. The evolutionary wavelet spectrum is such that\n\u221e\u2211\n\u03c4=0\nsup\nz\n|c(z, \u03c4)| <\u221e,(3.5)\nC1 := ess inf\nz,\u03c9\n\u2211\nj<0\nSj(z)|\u03c8\u02c6j(\u03c9)|2 > 0,(3.6)\nwhere \u03c8\u02c6j(\u03c9) =\n\u2211\u221e\ns=\u2212\u221e \u03c8j0(s) exp(i\u03c9s).\n8\nNote that if (3.5) holds, then\n(3.7) C2 := ess sup\nz,\u03c9\n\u2211\nj<0\nSj(z)|\u03c8\u02c6j(\u03c9)|2 <\u221e.\nAssumption (3.5) ensures that for each z, the local covariance c(z, \u03c4) is absolutely summable,\nso the process is short-memory (in fact, Assumption (3.5) is slightly stronger than that, for\ntechnical reasons). Assumption (3.6) and formula (3.7) become more transparent when we\nrecall that for a stationary process Xt with spectral density f(\u03c9) and wavelet spectrum\nSj, we have f(\u03c9) =\n\u2211\nj Sj|\u03c8\u02c6j(\u03c9)|2 (the Fourier transform of equation (2.4) for stationary\nprocesses). In this sense, (3.6) and (3.7) are \u201ctime-varying\u201d counterparts of the classical\nassumptions of the (stationary) spectral density being bounded away from zero, as well as\nbounded from above.\nProposition 2. Under Assumptions (3.5) and (3.6), the mean square one-step-ahead pre-\ndiction error may be written as\n(3.8) MSPE(X\u02c6t;T , Xt;T ) = b\n\u2032\ntBt;T bt (1 + oT (1)) .\nMoreover, if {b(1)s;T} are the coefficients which minimise b\u2032tBt;T bt, then {b(1)s;T} solve the fol-\nlowing linear system\n(3.9)\nt\u22121\u2211\nm=0\nb\n(1)\nt\u22121\u2212m;T\n{ \u22121\u2211\nj=\u2212J\nSj\n(\nn+m\n2T\n)\n\u03a8j(m\u2212 n)\n}\n=\n\u22121\u2211\nj=\u2212J\nSj\n(\nt+ n\n2T\n)\n\u03a8j(t\u2212 n)\nfor all n = 0, . . . , t\u2212 1.\nThe proof of the first result can be found in Appendix A (see Lemma 5) and uses standard\napproximations of covariance matrices of locally stationary processes. The second result is\nsimply the minimisation of the quadratic form (3.8) and the system of equations (3.9) is\ncalled the prediction equations. The key observation here is that minimising b\u2032t\u03a3t;T bt is\nasymptotically equivalent to minimising b\u2032tBt;T bt. Bearing in mind the relation of formula\n(2.4) between the wavelet spectrum and the local autocovariance function, the prediction\nequations can also be written as\n(3.10)\nt\u22121\u2211\nm=0\nb\n(1)\nt\u22121\u2212m;T c\n(\nn+m\n2T\n,m\u2212 n\n)\n= c\n(\nn+ t\n2T\n, t\u2212 n\n)\n.\nThe following two remarks demonstrate how the prediction equations simplify in the case of\ntwo important subclasses of locally stationary wavelet processes.\nRemark 5 (Stationary processes). If the underlying process is stationary, then the local\nautocovariance function c(z, \u03c4) is no longer a function of two variables, but only a function\nof \u03c4 . In this context, the prediction equations (3.10) become\nt\u22121\u2211\nm=0\nb\n(1)\nt\u22121\u2212m c(m\u2212 n) = c(t\u2212 n)\nfor all n = 0, . . . , t \u2212 1, which are the standard Yule-Walker equations used to forecast\n9\nstationary processes.\nRemark 6 (Time-modulated processes). For the processes considered in Remark 3\n(equation (2.6)), the local autocovariance function has a multiplicative structure: c(z, \u03c4) =\n\u03c32(z)\u03c1(\u03c4). Therefore, for these processes, prediction equations (3.10) become\nt\u22121\u2211\nm=0\nb\n(1)\nt\u22121\u2212m;T\u03c3\n2\n(\nn+m\n2T\n)\n\u03c1(m\u2212 n) = \u03c32\n(\nn+ t\n2T\n)\n\u03c1(t\u2212 n).\nWe will now study the inversion of the system (3.9) in the general case, and the stability\nof the inversion. Denote by Pt the matrix of this linear system, i.e.\n(Pt)nm =\n\u22121\u2211\nj=\u2212J\nSj\n(\nn +m\n2T\n)\n\u03a8j(m\u2212 n)\nfor n,m = 0, . . . , t\u2212 1. Using classical results of numerical analysis (see for instance Kress\n(1991, Theorem 5.3)) the measure of this stability is given by the so-called condition number,\nwhich is defined by cond (Pt) = \u2016Pt\u2016 \u2016P\u22121t \u2016. It can be proved along the lines of Lemma 3\n(Appendix A) that, under Assumptions (3.5) and (3.6), cond (Pt) 6 C1 C2.\n3.4 The prediction error\nThe next result generalises the classical Kolmogorov formula for the theoretical one-step-\nahead prediction error (Brockwell and Davis, 1991, Theorem 5.8.1). It is a direct modification\nof a similar result stated by Dahlhaus (1996b, Theorem 3.2(i)) for locally stationary Fourier\nprocesses.\nProposition 3. Suppose that Assumptions (3.5) and (3.6) hold. Given t observations\nX0,T , . . . , Xt\u22121,T of the LSW process {Xt,T} (with T = t + 1), the one-step ahead mean\nsquare prediction error \u03c32\nospe\nin forecasting X\u02c6t,T is given by\n\u03c32ospe = exp\n{\n1\n2pi\n\u222b pi\n\u2212pi\nd\u03c9 ln\n[ \u22121\u2211\nj=\u2212\u221e\nSj\n(\nt\nT\n)\n|\u03c8\u02c6j(\u03c9)|2\n]}\n(1 + oT (1)) .\nNote that due to Assumption (3.6), the sum\n\u2211\nj Sj(t\/T )|\u03c8\u02c6j(\u03c9)|2 is strictly positive,\nexcept possibly on a set of measure zero.\n3.5 h-step-ahead prediction\nThe one-step-ahead prediction equations have a natural generalisation to the h-step-ahead\nprediction problem with h > 1. The mean square prediction error can be written\nMSPE(X\u02c6t+h\u22121,T , Xt+h\u22121,T ) = E\n(\nX\u02c6t+h\u22121,T \u2212Xt+h\u22121,T\n)2\n= b\u2032t+h\u22121\u03a3t+h\u22121;Tbt+h\u22121,\nwhere \u03a3t+h\u22121;T is the covariance matrix of X0,T , . . . , Xt+h\u22121,T and bt+h\u22121 is the vector\n(b\n(h)\nt\u22121, . . . , b\n(h)\n0 , b\n(h)\n\u22121 , . . . , b\n(h)\n\u2212h), with b\n(h)\n\u22121 , . . . , b\n(h)\n\u2212h+1 = 0 and b\n(h)\n\u2212h = \u22121. Like before, we approx-\nimate the mean square error by b\u2032t+h\u22121Bt+h\u22121;T bt+h\u22121, where Bt+h\u22121;T is a (t+ h)\u00d7 (t+ h)\n10\nmatrix whose (m,n)-th element is given by\n\u22121\u2211\nj=\u2212J\nSj\n(\nn+m\n2T\n)\n\u03a8j(n\u2212m) .\nProposition 4. Under Assumptions (3.5) and (3.6), the mean square prediction error may\nbe written as\nMSPE(X\u02c6t+h\u22121;T , Xt+h\u22121;T ) = b\u2032t+h\u22121Bt+h\u22121;T bt+h\u22121 (1 + oT (1)) .\n4 Prediction based on data\nHaving treated the prediction problem from a theoretical point of view, we now address the\nquestion of how to estimate the unknown time-varying second order structure in the system\nof equations (3.9). In Subsection 4.3, we propose a complete algorithm for forecasting non-\nstationary time series using the LSW framework.\n4.1 Estimation of the time-varying second-order structure\nOur estimator of the local autocovariance function c(z, \u03c4), with 0 < z < t\/T , is constructed\nby estimating the unknown wavelet spectrum Sj(z) in the multiscale representation (2.4).\nLet us first define the function J(t) = \u2212min{j : Lj 6 t}. Following Nason et al. (2000) we\ndefine the wavelet periodogram as the sequence of squared wavelet coefficients djk;T , where j\nand k are scale and location parameters, respectively:\nIj(k\/T ) = d\n2\njk;T =\n(\nt\u22121\u2211\ns=0\nXs,T \u03c8jk(s)\n)2\n, \u2212J(t) 6 j 6 \u22121, k = Lj \u2212 1, . . . , t\u2212 1 .\nNote that as \u03c8jk is only nonzero for s = 0, . . . ,Lj \u2212 1, the estimator Ij(k\/T ) is a function of\nXt,T for t 6 k. At the left edge, we set Ij(k\/T ) = Ij((Lj \u2212 1)\/T ) for k = 0, . . . ,Lj \u2212 2.\nFrom this definition, we define our multiscale estimator of the local variance function\n(2.5) as\n(4.1) c\u02dc\n(\nk\nT\n, 0\n)\n=\n\u22121\u2211\nj=\u2212J\n2j Ij\n(\nk\nT\n)\n.\nThe next proposition concerns the asymptotic behaviour of the first two moments of this\nestimator.\nProposition 5. The estimator (4.1) satisfies\nE c\u02dc\n(\nk\nT\n, 0\n)\n= c\n(\nk\nT\n, 0\n)\n+O\n(\nT\u22121 log(T )\n)\n.\n11\nIf, in addition, the increment process {\u03bejk} in Definition 1 is Gaussian and (3.5) holds, then\nVar c\u02dc\n(\nk\nT\n, 0\n)\n= 2\n\u22121\u2211\ni,j=\u2212J\n2i+j\n(\u2211\n\u03c4\nc(k\/T, \u03c4)\n\u2211\nn\n\u03c8in(\u03c4)\u03c8jn(0)\n)2\n+ O(T\u22121).\nRemark 7 (Time-modulated processes). For Gaussian time-modulated processes con-\nsidered in Remark 3 (formula (2.6)), the variance of estimator (4.1) reduces to\nVar c\u02dc\n(\nk\nT\n, 0\n)\n= 2\u03c34(k\/T )\n\u22121\u2211\ni,j=\u2212J\n2i+j\n(\u2211\n\u03c4\n\u03c1(\u03c4)\n\u2211\nn\n\u03c8in(\u03c4)\u03c8jn(0)\n)2\n+O(T\u22121),(4.2)\nwhere \u03c1(\u03c4) is the autocorrelation function of Yt (see equation (2.6)). If Xt,T = \u03c3(t\/T )Zt,\nwhere Zt are i.i.d. N(0, 1), then the leading term in (4.2) reduces to (2\/3)\u03c3\n4(k\/T ) for all\ncompactly supported wavelets \u03c8. Other possible estimators of the local variance for time-\nmodulated processes, as well as an empirical study of the explanatory power of these models\nas applied to financial time series, may be found in Van Bellegem and von Sachs (2002).\nRemark 8. Proposition 5 can be generalised for the estimation of c(z, \u03c4) for \u03c4 6= 0. Define\nthe estimator\n(4.3) c\u02dc\n(\nk\nT\n, \u03c4\n)\n=\n\u22121\u2211\nj=\u2212J\n( \u22121\u2211\n`=\u2212J\nA\u22121j` \u03a8`(\u03c4)\n)\nIj\n(\nk\nT\n)\n, k = 0, . . . , t\u2212 1, \u03c4 6= 0,\nwhere the matrix A = (Aj`)j,`<0 is defined by\n(4.4) Aj` := \u3008\u03a8j,\u03a8`\u3009 =\n\u2211\n\u03c4\n\u03a8j(\u03c4) \u03a8`(\u03c4) .\nNote that the matrix Aj` is not simply diagonal due to the redundancy in the system of\nautocorrelation wavelets {\u03a8j}. Nason et al. (2000) proved the invertibility of A if {\u03a8j} is\nconstructed using Haar wavelets. If other compactly supported wavelets are used, numerical\nresults suggest that the invertibility of A still holds, but a complete proof of this result has\nnot been established yet. Using Lemma 8, it is possible to generalise the proof of Proposition\n5 for Haar wavelets to show that\nE c\u02dc\n(\nk\nT\n, \u03c4\n)\n= c\n(\nk\nT\n, \u03c4\n)\n+O\n(\nT\u22121\/2\n)\nfor \u03c4 6= 0 and, if Assumption (3.5) hold and if the increment process {\u03bejk} in Definition 1 is\nGaussian, then\nVar c\u02dc\n(\nk\nT\n, \u03c4\n)\n= 2\n\u22121\u2211\ni,j=\u2212J\nhi(\u03c4)hj(\u03c4)\n{\u2211\n\u03c4\nc\n(\nk\nT\n, \u03c4\n)\u2211\nn\n\u03c8in(\u03c4)\u03c8jn(0)\n}2\n+O\n(\nT\u22121 log2(T )\n)\nfor \u03c4 6= 0, where hj(\u03c4) =\n\u2211\u22121\n`=\u2212J A\n\u22121\nj` \u03a8`(\u03c4).\nThese results show the inconsistency of the estimator of the local (co)variance, which\nneeds to be smoothed w.r.t. the rescaled time z (i.e. c\u02dc(\u00b7, \u03c4) needs to be smoothed for all\n12\n\u03c4). We use standard kernel smoothing where the problem of the choice of the bandwidth\nparameter g arises. The goal of Subsection 4.3 is to provide a fully automatic procedure for\nchoosing g.\nTo compute the linear predictor in practice, we invert the generalised Yule-Walker equa-\ntions (3.10) in which the theoretical local autocovariance function is replaced by the smoothed\nversion of c\u02dc(k\/T, \u03c4). However, in equations (4.1) and (4.3), our estimator is only defined for\nk = 0, . . . , t\u2212 1 while the prediction equations (3.10) require the local autocovariance up to\nk = t (for h = 1). This problem is inherent to our non-stationary framework. We denote the\npredictor of c(t\/T, \u03c4) by c\u02c6(t\/T, \u03c4) and, motivated by the slow evolution of the local autoco-\nvariance function, propose to compute c\u02c6(t\/T, \u03c4) by the local smoothing of the (unsmoothed)\nestimators {c\u02dc(k\/T, \u03c4), k = t \u2212 1, . . . , t \u2212 \u00b5}. In practice, the smoothing parameter \u00b5 for\nprediction is set to be equal to gT , where g is the smoothing parameter (bandwidth) for\nestimation. They can be obtained by the data-driven procedure described in Subsection 4.3.\n4.2 Future observations in rescaled time\nFor clarity of presentation, we restrict ourselves (in this and the following subsection) to the\ncase h = 1.\nIn remarks 1 and 4, we recalled the mechanics of rescaled time for non-stationary pro-\ncesses. An important ingredient of this concept is that the data come in the form of a trian-\ngular array whose rows correspond to different stochastic processes, only linked through the\nasymptotic wavelet spectrum sampled on a finer and finer grid. This mechanism is inherently\ndifferent to what we observe in practice, where, typically, observations arrive one by one and\nneither the values of the \u201cold\u201d observations, nor their corresponding second-order structure,\nchange when a new observation arrives.\nOne way to reconcile the practical setup with our theory is to assume that for an observed\nprocess X0, . . . , Xt\u22121, there exists a doubly-indexed LSW process Y such that Xk = Yk,T for\nk = 0, . . . , t\u2212 1. When a new observation Xt arrives, the underlying LSW process changes,\ni.e. there exists another LSW process Z such that Xk = Zk,T+1 for k = 0, . . . , t. An essential\npoint underlying our adaptive algorithm of the next subsection is that the spectra of Y and\nZ are close to each other, due to the above construction and the regularity assumptions\nimposed by Definition 1 (in particular, the Lipschitz continuity of Sj(z)).\nThe objective of our algorithm is to choose appropriate values of certain nuisance pa-\nrameters (see the next subsection) in order to forecast Xt from X0, . . . , Xt\u22121. Assume that\nthese parameters have been selected well, i.e. that the forecasting has been successful. The\ncloseness of the two spectra implies that we can also expect to successfully forecast Xt+1 from\nX0, . . . , Xt using the same, or possibly \u201cneighbouring\u201d, values of the nuisance parameters.\nBearing in mind the above discussion, we introduce our algorithm with a slight abuse of\nnotation: we drop the second subscript when referring to the observed time series.\n4.3 Data-driven choice of parameters\nIn theory, the best one-step-ahead linear predictor of Xt,T is given by (3.1), where bt =\n(b\n(1)\nt\u22121\u2212s;T )s=0,...,t\u22121 solves the prediction equations (3.9). In practice, each of the t components\nof the vector bt is estimated using our estimator of the local autocovariance function based\non observations X0,T , . . . , Xt\u22121,T . Hence, we have to find a balance between the estimation\n13\nerror, potentially increasing with t, and the prediction error which is a decreasing function\nof t.\nAs a natural balancing rule which works well in practice, we suggest to choose a number\np such that the \u201cclipped\u201d predictor\n(4.5) X\u02c6\n(p)\nt,T =\nt\u22121\u2211\ns=t\u2212p\nb\n(1)\nt\u22121\u2212s;TXs,T\ngives a good compromise between the theoretical prediction error and the estimation er-\nror. The construction (4.5) is reminiscent of the classical idea of AR(p) approximation for\nstationary processes.\nWe propose an automatic procedure for selecting the two nusiance parameters: the order\np in (4.5) and the bandwidth g, necessary to smooth the inconsistent estimator c\u02dc(z, \u03c4) using\na kernel method. The idea of this procedure is to start with some initial values of p and\ng and to gradually update these parameters using a criterion which measures how well the\nseries gets predicted using a given pair of parameters. This type of approach is in the spirit\nof adaptive forecasting (Ledolter, 1980).\nSuppose that we observe the series up to Xt\u22121 and want to predict Xt, using an ap-\npropriate pair (p, g). The idea of our method is as follows. First, we move backwards by\ns observations and choose some initial parameters (p0, g0) for predicting Xt\u2212s from the ob-\nserved series up to Xt\u2212s\u22121. Next, we compute the prediction of Xt\u2212s using the pairs of\nparameters around our preselected pair (i.e. (p0 \u2212 1, g0 \u2212 \u03b4), (p0, g0 \u2212 \u03b4), . . . , (p0 + 1, g0 + \u03b4)\nfor a fixed constant \u03b4). As the true value of Xt\u2212s is known, we are able to use a preset\ncriterion to compare the 9 obtained prediction results, and we choose the pair corresponding\nto the best predictor (according to this preset criterion). This step is called the update of\nthe parameters by predicting Xt\u2212s. In the next step, the updated pair is used as the ini-\ntial parameters, and itself updated by predicting Xt\u2212s+1 from X0, . . . , Xt\u2212s. By applying\nthis procedure to predict Xt\u2212s+2, Xt\u2212s+3, . . . , Xt\u22121, we finally obtain an updated pair (p1, g1)\nwhich is selected to perform the actual prediction.\nMany different criteria can be used to compare the quality of the pairs of parameters at\neach step. Denote by X\u02c6t\u2212i(p, g) the predictor of Xt\u2212i computed using pair (p, g), and by\nIt\u2212i(p, g) the corresponding 95% prediction interval based on the assumption of Gaussianity:\n(4.6) It\u2212i(p, g) =\n[\n\u22121.96\u03c3\u02c6t\u2212i(p, g) + X\u02c6t\u2212i(p, g) , 1.96\u03c3\u02c6t\u2212i(p, g) + X\u02c6t\u2212i(p, g)\n]\n,\nwhere \u03c3\u02c62t\u2212i(p, g) is the estimate of MSPE(X\u02c6t\u2212i(p, g), Xt\u2212i) computed using formula (3.8) with\nthe remainder neglected. The criterion which we use in the simulations reported in the next\nsection is to compute \u2223\u2223Xt\u2212i \u2212 X\u02c6t\u2212i(p, g)\u2223\u2223\nlength(It\u2212i(p, g))\nfor each of the 9 pairs at each step of the procedure and select the updated pair as the one\nthat minimises this ratio.\nWe also need to choose the initial parameters (p0, g0) and the number s of data points at\nthe end of the series which are used in the procedure. We suggest that s should be set to the\nlength of the largest segment at the end of the series which does not contain any apparent\nbreakpoints observed after a visual inspection. To avoid dependence on the initial values\n14\n(p0, g0), we suggest to iterate the algorithm a few times, using (p1, g1) as the initial value for\neach iteration. We propose to stop when the parameters (p1, g1) are such that at least 95%\nof the observations fall into the prediction intervals.\nIn order to be able to use our procedure completely on-line, we do not have to repeat the\nwhole algorithm. Indeed, when observation Xt becomes available, we only have to update\nthe pair (p1, g1) by predicting Xt, and we directly obtain the \u201coptimal\u201d pair for predicting\nXt+1.\nThere are, obviously, many possible variants of our algorithm. Possible modifications\ninclude, for example, using a different criterion, restricting the allowed parameter space for\n(p, g), penalising certain regions of the parameter space, or allowing more than one parameter\nupdate at each time point.\nWe have tested our algorithm on numerous examples, and the following section presents\nan application to a real data set. A more theoretical study of this algorithm is left for future\nwork.\n5 Application of the general predictor to real data\nEl Nin\u02dco is a disruption of the ocean atmosphere system in the tropical Pacific which has\nimportant consequences for the weather around the globe. Even though the effect of El\nNin\u02dco is not avoidable, research on its forecast and its impacts allows specialists to attenuate\nor prevent its harmful consequences (see Philander (1990) for a detailed overview). The\neffect of the equatorial Pacific meridional reheating may be measured by the deviation of the\nwind speed on the ocean surface from its average. It is worth mentioning that this effect is\nproduced by conduction, and thus we expect the wind speed variation to be smooth. This\nlegitimates the use of LSW processes to model the speed. In this section, we study the wind\nspeed anomaly index, i.e. its standardised deviation from the mean, in a specific region of the\nPacific (12-2N, 160E-70W). Modelling this anomaly helps to understand the effect of El Nin\u02dco\neffect in that region. The time series composed of T = 910 monthly observations is avail-\nable free of charge at http:\/\/tao.atmos.washington.edu\/data sets\/eqpacmeridwindts.\nFigure 2(a) shows the plot of the series.\nFigure 2 here\nThroughout this section, we use Haar wavelets to estimate the local (co)variance. Having\nprovisionally made a safe assumption of the possible non-stationarity of the data, we first\nattempt to find a suitable pair of parameters (p, g) which will be used for forecasting the\nseries. By inspecting the acf of the series, and by trying different values of the bandwidth,\nwe have found that the pair (7, 70\/T ) works well for many segments of the data; indeed, the\nsegment of 100 observations from June 1928 to October 1936 gets predicted very accurately in\none-step-ahead prediction: 96% of the actual observations are contained in the corresponding\n95% prediction intervals (formula (4.6)).\nHowever, the pair (7, 70\/T ) does not appear to be uniformly well suited for forecasting\nthe whole series. For example, in the segment of 40 observations between November 1986\nand February 1990, only 5% of the observations fall into the corresponding one-step-ahead\nprediction intervals computed using the above pair of parameters. This provides strong\nevidence that the series is non-stationary (indeed, if it was stationary, we could expect to\n15\nobtain a similar percentage of accurately predicted values in both segments). This further\njustifies our approach of modelling and forecasting the series as an LSW process.\nMotivated by the above observation, we now apply our algorithm, described in the pre-\nvious section, to the segment of 40 observations mentioned above, setting the initial param-\neters to (7, 70\/T ). After the first iteration along the segment, the parameters drift up to\n(14, 90\/T ), and 85% of the observations fall within the prediction intervals, which is indeed\na dramatic improvement over the 5% obtained without applying our adaptive algorithm.\nIn the second pass, we set the initial values to (14, 90\/T ), and obtain a 92.5% coverage\nby the one-step-ahead prediction intervals, with the parameters drifting up to (14, 104\/T ).\nIn the last iteration, we finally obtain a 95% coverage, and the parameters get updated to\n(14, 114\/T ). We now have every reason to believe that this pair of parameters is well suited\nfor one-step-ahead prediction within a short distance of February 1990. Without performing\nany further updates, we apply the one-step-ahead forecasting procedure to predict, one by\none, the eight observations which follow February 1990, the prediction parameters being\nfixed at (14, 114\/T ). The results are plotted in Figure 2(b), which also compares our results\nto those obtained by means of AR modelling. At each time point, the order of the AR\nprocess is chosen as the one that minimises the AIC criterion, and then the parameters are\nestimated by means of the standard S-Plus routine. We observe that for both models, all of\nthe true observed values fall within the corresponding one-step-ahead prediction intervals.\nHowever, the main gain obtained using our procedure is that the prediction intervals are\non average 17.45% narrower in the case of our algorithm. This result is not peculiar to AR\nmodelling as this percentage is also similar in comparison with other stationary models, like\nARMA(2,10), believed to accurately fit the series. A similar phenomenon has been observed\nat several other points of the series.\nFigure 3 here\nWe end this section by applying our general prediction method to compute multi-step-\nahead forecasts. Figure 3 shows the 1- up to 9-step-ahead forecasts of the series, along with\nthe corresponding prediction intervals, computed at the end of the series (December 1995).\nIn Figure 3(a), the LSW model is used to construct the forecast values, with parameters\n(10, 2.18) chosen automatically by our adaptive algorithm described above. Figure 3(b)\nshows the 9-step-ahead prediction based on AR modelling (here, AR(2)). The prediction in\nFigure 3(a) looks \u201csmoother\u201d because it uses the information from the whole series. This\ninformation is averaged out, whereas in the LSW forecast, local information is picked up at\nthe end of the series, and the forecasts look more \u201cjagged\u201d.\n6 Conclusion\nIn this paper, we have given an answer to the pertinent question, asked by time series analysts\nover the past few years, of whether and how wavelet methods can help in forecasting non-\nstationary time series. To develop the forecasting methodology, we have considered the\nLocally Stationary Wavelet (LSW) model, which is based on the idea of a localised time-\nscale representation of a time-changing autocovariance function. This model includes the\nclass of second-order stationary processes and has several attractive features, not only for\nmodelling, but also for estimation and prediction purposes. Its linearity and the fact that\nthe time-varying second order quantities are modelled as smooth functions, have enabled\n16\nus to formally extend the classical theory of linear prediction to the whole class of LSW\nprocesses. These results are a generalisation of the Yule-Walker equations and, in particular,\nof Kolmogorov\u2019s formula for the one-step-ahead prediction error.\nIn the empirical prediction equations the second-order quantities have to be estimated,\nand this is where the LSW model proves most useful. The rescaled time, one of the main\ningredients of the model, makes it possible to develop a rigorous estimation theory. Moreover,\nby using well-localised non-decimated wavelets instead of a Fourier based approach, our\nestimators are able to capture the local time-scale features of the observed non-stationary\ndata very well (Nason and von Sachs, 1999).\nIn practice, our new prediction methodology depends on two nuisance parameters which\narise in the estimation of the local covariance and the mean-square prediction error. More\nspecifically, we need to smooth our inconsistent estimators over time, and to do so, we have to\nchoose the bandwidth of the smoothing kernel. Moreover, we need to reduce the dimension of\nthe prediction equations to avoid too much inaccuracy of the resulting prediction coefficients\ndue to estimation errors. We have proposed an automatic computational procedure for\nselecting these two parameters. Our algorithm is in the spirit of adaptive forecasting as it\ngradually updates the two parameters basing on the success of prediction. This new method\nis not only essential for the success of our whole prediction methodology, it also seems to\nbe promising in a much wider context of choosing nuisance parameters in non-parametric\nmethods in general.\nWe have applied our new algorithm to a meteorological data set. Our non-parametric\nforecasting algorithm shows interesting advantages over the classical parametric alternative\n(AR forecasting). Moreover, we believe that one of the biggest advantages of our new\nalgorithm is that it can be successfully applied to a variety of data sets, ranging from financial\nlog-returns (Fryz\u00b4lewicz (2002), Van Bellegem and von Sachs (2002)) to series traditionally\nmodelled as ARMA processes, including in particular data sets which are not, or do not\nappear to be, second-order stationary. The S-Plus routines implementing our algorithm, as\nwell as the data set, can be downloaded from the associated web page\nhttp:\/\/www.stats.bris.ac.uk\/~mapzf\/flsw\/flsw.html\nIn the future, we intend to derive the theoretical properties of our automatic algorithm\nfor choosing the nuisance parameters of the adaptive predictor. Finally, our approach offers\nthe attractive possibility to use the prediction error for model selection purposes. LSW\nprocesses are constructed using a fixed wavelet system, e.g. Haar or another Daubechies\u2019\nsystem. It is clear that we can compare the fitting quality of each such model by comparing\nits prediction performance on the observed data. In the future, we intend to investigate this\nin more detail in order to answer the question, left open by Nason et al. (2000), of which\nwavelet basis to use to model a given series.\n7 Acknowledgements\nThe authors would like to thank Rainer Dahlhaus, Christine De Mol, Christian Hafner, Guy\nNason and two anonymous referees for stimulating discussions and suggestions which helped\nto improve the presentation of this article. They are also grateful to Peter Brockwell and\nBrandon Whitcher for their expertise and advice in analysing the wind data of Section 5.\nSe\u00b4bastien Van Bellegem and Rainer von Sachs would like to express their gratitude to the\n17\nDepartment of Mathematics, University of Bristol, and Piotr Fryz\u00b4lewicz \u2014 to the Institut de\nstatistique, Universite\u00b4 catholique de Louvain, and SVB, for their hospitality during mutual\nvisits in 2001 and 2002. RvS and SVB were funded from the National Fund for Scientific\nResearch \u2013 Wallonie, Belgium (F.N.R.S.), by the contract \u201cProjet d\u2019Actions de Recherche\nConcerte\u00b4es\u201d no. 98\/03-217 of the Belgian Government and by the IAP research network No.\nP5\/24 of the Belgian State (Federal Office for Scientific, Technical and Cultural Affairs).\nPF was funded by the Department of Mathematics at the University of Bristol, Universities\nUK, Unilever Research, and Guy Nason.\nA Theoretical properties of the predictor\nLet Xt;T = (X0;T , . . . , Xt\u22121;T )\u2032 be a realisation of an LSW process. In this appendix, we study\nthe theoretical properties of the covariance matrix \u03a3t;T = E(Xt;T X\n\u2032\nt;T ). As we need upper\nbounds for the spectral norms \u2016\u03a3t;T\u2016 and \u2016\u03a3\u22121t;T\u2016, we base the following results and their\nproofs on methods developed in Dahlhaus (1996b, Section 4) for approximating covariance\nmatrices of locally stationary Fourier processes. However, in our setting these methods need\nimportant modifications. The idea is to approximate \u03a3t;T by overlapping block Toeplitz\nmatrices along the diagonal.\nThe approximating matrix is constructed as follows. First, we construct a coverage of\nthe time axis [0, T ). Let L be a divisor of T such that L\/T \u2192 0, and consider the following\npartition of the time axis:\nP0 =\n{\n[0, L), [L, 2L), . . . , [T \u2212 L, T )\n}\n.\nThen, consider another partition of the time axis, which is a shift of P0 by \u03b4 < L:\nP1 =\n{\n[0, \u03b4), [\u03b4, L+ \u03b4), [L+ \u03b4, 2L + \u03b4), . . . , [T \u2212 L+ \u03b4, T )\n}\n.\nIn what follows, assume that L is a multiple of \u03b4 and that \u03b4\/L \u2192 0 as T tends to infinity.\nAlso, consider the partition of the time axis which is a shift of P1 by \u03b4:\nP2 =\n{\n[0, 2\u03b4), [2\u03b4, L+ 2\u03b4), [L+ 2\u03b4, 2L+ 2\u03b4), . . . , [T \u2212 L+ 2\u03b4, T )\n}\nand, analogously, define P3,P4, . . . up to PM where M = (L\/\u03b4) \u2212 1. Consider the union\nof all these partitions P = {P0,P1, . . . ,PM}, which is a highly redundant coverage of the\ntime axis. Denote by P the number of intervals in P, and denote the elements of P by Mp,\np = 1, . . . , P .\nFor each p, we fix a point \u03bdp in Mp and consider matrix D\n(p) defined by:\nD(p)nm =\n\u2211\nj<0\nSj\n(\u03bdp\nT\n)\n\u03a8j(n\u2212m)In,m\u2208Mp\nwhere In,m\u2208Mp means that we only include those n,m that are in Mp. Observe that each \u03bdp\nis contained exactly in L\/\u03b4 segments. The following lemma concerns the approximation of\n18\n\u03a3t;T by matrix D defined by\nDnm =\n\u03b4\nL\nP\u2211\np=1\nD(p)nm.\nLemma 1. Assume that (3.5) holds. If L\u2192\u221e, \u03b4\/L\u2192 0 and L2\/T \u2192 0 as T \u2192\u221e, then\nx\n\u2032 (\u03a3t;T \u2212D) x = x\u2032xoT (1).\nProof. Define matrix \u03a3\n(p)\nt;T by\n(\n\u03a3\n(p)\nt;T\n)\nnm\n= (\u03a3t;T )nm In,m\u2208Mp. Straightforward calculations\nyield\n(A.1) x\u2032 (\u03a3t;T \u2212D) x = x\u2032\n[\n\u03b4\nL\nP\u2211\np=1\n(\u03a3\n(p)\nt;T \u2212D(p))\n]\nx + RestT\nwhere\nRestT =\nT\n\u03b4\n\u22121\u2211\nn,m=0\nmin\n(\n|n\u2212m| \u03b4\nL\n, 1\n) \u03b4\u22121\u2211\nu,s=0\nxn\u03b4+u (\u03a3t;T )n\u03b4+u,m\u03b4+s xm\u03b4+s.\nLet us first bound this remainder. Replace (\u03a3t;T )nm by\n\u2211\nj Sj((n + m)\/2T )\u03a8j(n \u2212m) and\ndenote b(k) := supz |\n\u2211\nj Sj(z)\u03a8j(k)| = supz |c(z, k)|. We have\n|RestT | 6 2x\u2032x\nT\n\u03b4\n\u22121\u2211\nd=1\nmin\n(\nd\n\u03b4\nL\n, 1\n) d\u03b4\u2211\nk=(d\u22121)\u03b4+1\nb(k) + Rest\u2032T\n6 2x\u2032x\n\uf8eb\uf8ed\u03b4 +\u221aL\nL\n\u221e\u2211\nk=1\nb(k) +\n\u2211\nk>\n\u221a\nL\nb(k)\n\uf8f6\uf8f8 + Rest\u2032T\nand the main term in the above is oT (1) since L \u2192 \u221e and \u03b4\/L \u2192 0 as T \u2192 \u221e, and by\nassumption (3.5). Let us now turn to the remainder Rest\u2032T . We have\nRest\u2032T 6\nT\u22121\u2211\nn,m=0\n\u2223\u2223\u2223\u2223\u2223xnxm\u2211\nj,k\n(\nw2jk;T \u2212 Sj\n(\nn +m\n2T\n))\n\u03c8j,k(m)\u03c8j,k(n)\n\u2223\u2223\u2223\u2223\u2223\nwhich may be bounded as follows using the definition of an LSW process, and the Lipschitz\nproperty of Sj:\nRest\u2032T 6 O(T\n\u22121)\n\u2211\nj\n(Cj + LjLj)\n\u2211\nk\n\uf8eb\uf8ed k\u2211\nn=k\u2212Lj+1\n|xn\u03c8j,k(n)|\n\uf8f6\uf8f82\n6 O(T\u22121)x\u2032x\n\u2211\nj\n(Cj + LjLj)Lj 6 O(T\u22121)x\u2032x\nby assumption of the Lemma.\n19\nLet us finally consider the main term in (A.1). We have\nx\n\u2032\n(\n\u03b4\nL\nP\u2211\np=1\n\u03a3\n(p)\nt;T \u2212D(p)\n)\nx 6\n\u03b4\nL\nP\u2211\np=1\n\u2211\njk\n\u2223\u2223\u2223w2jk;T \u2212 Sj (\u03bdpT )\u2223\u2223\u2223\n(\u2211\nu\n\u03c8j,k(u)xuIu\u2208Mp\n)2\n6 O(T\u22121)\n\u03b4\nL\nP\u2211\np=1\n\u2211\njk\n(\u2211\nn\nx2nIn\u2208Mp\n)\u2211\nj\nCj + Lj(Lj + L)\n= O(T\u22121)x\u2032x\n\u2211\nj\n(Cj + Lj(Lj + L))(Lj + L)(A.2)\nwhere the last equality holds because, by construction, each xn is contained in exactly L\/\u03b4\nsegments of the coverage. Since we assumed that L2\/T \u2192 0 as T \u2192\u221e, we obtain the result.\n\u0003\nLemma 2. Assume that (3.5) holds and there exists a t\u2217 such that xu = 0 for all u 6\u2208\n{t\u2217, . . . , t\u2217 + L}. Then for each t0 \u2208 {t\u2217, . . . , t\u2217 + L},\n(A.3) x\u2032\u03a3t,T x =\n\u2211\nj\nSj\n(\nt0\nT\n)\u2211\nk\n(\nt\u2217+L\u2211\nu=t\u2217\nxu\u03c8j,k(u)\n)2\n+ x\u2032xO\n(\nL2\nT\n)\n.\nProof. Identical to the part of the proof of Lemma 1 leading to the bound for the main term,\ni.e. formula (A.2). \u0003\nIn what follows, the matrix norm \u2016M\u2016 denotes the spectral norm of the matrix M , i.e.\nmax{\u221a\u03bb : \u03bb is the eigenvalue of M \u2032M}. If M is symmetric and nonnegative definite, by\nstandard theory we have\n\u2016M\u2016 = sup\n\u2016 \u0000 \u20162\n2\n=1\nx\n\u2032\nMx \u2016M\u22121\u2016 =\n(\ninf\n\u2016 \u0000 \u20162\n2\n=1\nx\n\u2032\nMx\n)\u22121\n.(A.4)\nLemma 3. Assume that (3.5) holds. The spectral norm \u2016\u03a3t;T\u2016 is bounded in t. Also, if\n(3.6) holds, then the spectral norm \u2016\u03a3\u22121t;T\u2016 is bounded in t.\nProof. Lemma 1 implies\n\u2016\u03a3t;T\u2016 = sup\n\u2016 \u0000 \u20162\n2\n=1\n\u03b4\nL\nP\u2211\np=1\n\u2211\nj<0\nSj\n(\u03bdp\nT\n)\u2211\nk\n(\u2211\nn\nxn\u03c8j,k\u2212nIn\u2208Mp\n)2\n+ oT (1)\nusing Parseval formula, we have\n= sup\n\u2016 \u0000 \u20162\n2\n=1\n\u03b4\n2piL\nP\u2211\np=1\n\u222b pi\n\u2212pi\nd\u03c9\n\u2211\nj<0\nSj\n(\u03bdp\nT\n) \u2223\u2223\u2223\u03c8\u02c6j(\u03c9)\u2223\u2223\u22232\n\u2223\u2223\u2223\u2223\u2223\u2211\nn\nxn exp(\u2212i\u03c9n)In\u2208Mp\n\u2223\u2223\u2223\u2223\u2223\n2\n+ oT (1)\n6 ess sup\nz,\u03c9\n\u2211\nj\nSj(z)\n\u2223\u2223\u2223\u03c8\u02c6j(\u03c9)\u2223\u2223\u22232 sup\n\u2016 \u0000 \u20162\n2\n=1\n\u2016x\u201622 + oT (1) = ess sup\nz,\u03c9\n\u2211\nj\nSj(z)\n\u2223\u2223\u2223\u03c8\u02c6j(\u03c9)\u2223\u2223\u22232 + oT (1)\nwhich is bounded by (3.5) (as (3.5) implies (3.7)). Using (A.4) with M = \u03a3t;T , the bound-\nedness of \u2016\u03a3\u22121t;T\u2016 is shown in exactly the same way. \u0003\n20\nProof of Proposition 3. The proof uses Lemmas 1 to 3 and is along the lines of Dahlhaus\n(1996b, Theorem 3.2(i)). The idea is to reduce the problem to a stationary situation by\nfixing the local time at \u03bdp. Then, the key point is to use the following relation between\nthe wavelet spectrum of a stationary process and its classical Fourier spectrum. If Xt is a\nstationary process with an absolutely summable autocovariance and with Fourier spectrum\nf(\u00b7), then its wavelet spectrum is given by\nSj =\n\u2211\n`\nA\u22121j`\n\u222b\nd\u03bb f(\u03bb)|\u03c8\u02c6`(\u03bb)|2(A.5)\nfor any fixed non-decimated system of compactly supported wavelets {\u03c8jk}. We refer to\nDahlhaus (1996b, Theorem 3.2(i)) for details. \u0003\nWe will now study the approximation of \u03a3t;T by Bt;T .\nLemma 4. Under the assumptions of Proposition 2 and 4,\nMSPE(X\u02c6t+h\u22121;T , Xt+h\u22121;T ) = b\u2032t+h\u22121Bt+h\u22121;T bt+h\u22121 + b\n\u2032\nt+h\u22121bt+h\u22121 oT (1)\nand, in particular,\nMSPE(X\u02c6t;T , Xt;T ) = b\n\u2032\ntBt;T bt + b\n\u2032\ntbt oT (1)\nProof. By the definition of an LSW process, we have |wjk;T |2 = Sj((n+m)\/T )+(Cj +Lj|k\u2212\nn\u2212m|)O(T\u22121). Therefore,\nb\n\u2032\nt+h\u22121\u03a3t+h\u22121;Tbt+h\u22121 =\n\u2211\njk\nt+h\u22121\u2211\nn,m=0\nbnbm\u03c8jk(n)\u03c8jk(m)|wjk;T |2\n=\n\u2211\njk\nt+h\u22121\u2211\nn,m=0\nbnbm\u03a8j(n\u2212m)Sj\n(\nn +m\n2T\n)\n+ Rest1(A.6)\nWe bound Rest1 as follows:\n|Rest1 | 6 O(T\u22121)\n\u2211\njk\nt+h\u22121\u2211\nn,m=0\n(\u2223\u2223\u2223k \u2212 (n+m\n2\n) \u2223\u2223\u2223Lj + Cj) |bnbm\u03c8jk(n)\u03c8jk(m)|.\nIf Lj denotes the length of support of \u03c8j, we have 0 6 k\u2212n, k\u2212m 6 Lj and so k\u2212(n+m)\/2 6\nLj such that\n|Rest1 | 6 O(T\u22121)\n\u2211\njk\nt+h\u22121\u2211\nn,m=0\n(LjLj + Cj) |bnbm\u03c8jk(n)\u03c8jk(m)|\n6 O(T\u22121)b\u2032t+h\u22121bt+h\u22121\n\u2211\nj\nLj (LjLj + Cj) = b\u2032t+h\u22121bt+h\u22121oT (1) by assumption.\nFinally, by Assumption (3.5), (A.6) yields the result. \u0003\n21\nLemma 5. Under the assumptions of Proposition 4, we have\nb\n\u2032\nt+h\u22121\u03a3t+h\u22121;Tbt+h\u22121 = b\n\u2032\nt+h\u22121Bt+h\u22121;T bt+h\u22121 (1 + oT (1))\nProof of Lemma 5. By Lemma 4, we have b\u2032t+h\u22121\u03a3t+h\u22121;T bt+h\u22121 = b\n\u2032\nt+h\u22121Bt+h\u22121;T bt+h\u22121 +\nb\n\u2032\nt+h\u22121bt+h\u22121 oT (1) By Lemma 3, the inverse of \u03a3t;T is bounded in T and, by standard\nproperties of the spectral norm, we have\nb\n\u2032\nt+h\u22121bt+h\u22121 6 b\n\u2032\nt+h\u22121\u03a3t+h\u22121;T bt+h\u22121 \u2016\u03a3\u22121t+h\u22121;T\u2016\nfor all sequences bt+h\u22121. The above gives\nb\n\u2032\nt+h\u22121\u03a3t+h\u22121;T bt+h\u22121 6 b\n\u2032\nt+h\u22121Bt+h\u22121;T bt+h\u22121 + b\n\u2032\nt+h\u22121\u03a3t+h\u22121;T bt+h\u22121 \u2016\u03a3\u22121t+h\u22121;T\u2016 oT (1)\nwhich is equivalent to\nb\n\u2032\nt+h\u22121\u03a3t+h\u22121;Tbt+h\u22121 6 b\n\u2032\nt+h\u22121Bt+h\u22121;T bt+h\u22121\n(\n1\u2212 \u2016\u03a3\u22121t+h\u22121;T\u2016 oT (1)\n)\u22121\nfor large T . On the other hand, we have\nb\n\u2032\nt+h\u22121\u03a3t+h\u22121;T bt+h\u22121 > b\n\u2032\nt+h\u22121Bt+h\u22121;T bt+h\u22121\n(\n1 + \u2016\u03a3\u22121t;T\u2016 oT (1)\n)\u22121\nwhich implies the result. \u0003\nB Estimation of the local autocovariance function\nIn this section, we study the properties of the estimator of the local autocovariance. We\nfirst show some relevant properties of the autocorrelation function \u03a8j(\u03c4) and the matrix A\ndefined in (4.4).\nLemma 6. 1. The system {\u03a8j(\u03c4), j = \u22121,\u22122, . . .} is linearly independent.\n2. Denote by \u03a8(\u03c4) the wavelet autocorrelation function of a continuous wavelet \u03c8, i.e.\n\u03a8(\u03c4) =\n\u222b\ndu\u03c8(u)\u03c8(u\u2212 \u03c4), \u03c4 \u2208 Z.\nWe have\n\u03a8j(\u03c4) = \u03a8\n(\n2j|\u03c4 |)\nfor all j = \u22121,\u22122, . . . and \u03c4 \u2208 Z.\nThe proof of the first result can be found in Nason et al. (2000, Theorem 1). For the\nproof of the second result, see, for example, Berkner and Wells (2002, Lemma 4.2).\nLemma 7.\n\u2211\u22121\nj=\u2212\u221e 2\nj \u03a8j(\u03c4) = \u03b40(\u03c4).\n22\nProof. Using Lemma 6 and Parseval\u2019s formula,\n\u22121\u2211\nj=\u2212\u221e\n2j\u03a8j(\u03c4) =\n\u22121\u2211\nj=\u2212\u221e\n2j\u03a8\n(\n2j |\u03c4 |) = \u22121\u2211\nj=\u2212\u221e\n\u222b \u221e\n\u2212\u221e\nd\u03c9 |\u03c8\u02c6(2\u2212j\u03c9)|2 exp(i\u03c9\u03c4)\n=\n\u22121\u2211\nj=\u2212\u221e\n\u222b 2pi\n0\nd\u03c9\n\u2211\nk\u2208 \u0000\n\u2223\u2223\u2223\u03c8\u02c6 (2\u2212j(\u03c9 + 2kpi))\u2223\u2223\u22232 exp(i\u03c9\u03c4).(B.1)\nDenote by m0(\u03be) the trigonometric polynomial which corresponds to the construction of\nwavelet \u03c8 and its corresponding scaling function \u03c6 (Daubechies, 1992, Theorem 6.3.6). We\nmay write\u2211\nk\u2208 \u0000\n\u2223\u2223\u2223\u03c8\u0302 (2\u2212j (\u03c9 + 2kpi))\u2223\u2223\u22232 = \u2211\nk\u2208 \u0000\n\u2223\u2223m0 (2\u2212j\u22121\u03c9 + 2\u2212j\u22121k2pi + pi)\u2223\u22232 \u2223\u2223\u2223\u03c6\u0302 (2\u2212j\u22121\u03c9 + 2\u2212j\u22121k2pi)\u2223\u2223\u22232\nand, using the 2pik-periodicity of m0,\n=\n\u2223\u2223m0 (2\u2212j\u22121\u03c9 + pi)\u2223\u22232\u2211\nk\u2208 \u0000\n\u2223\u2223\u2223\u03c6\u0302 (2\u2212j\u22121\u03c9 + 2\u2212j\u22121k2pi)\u2223\u2223\u22232\n=\n\u2223\u2223m0 (2\u2212j\u22121\u03c9 + pi)\u2223\u22232\u2211\nk\u2208 \u0000\n\u2223\u2223m0 (2\u2212j\u22122\u03c9 + 2\u2212j\u22122k2pi)\u2223\u22232 \u2223\u2223\u2223\u03c6\u0302 (2\u2212j\u22122\u03c9 + 2\u2212j\u22122k2pi)\u2223\u2223\u22232\n=\n\u2223\u2223m0 (2\u2212j\u22121\u03c9 + pi)\u2223\u22232 \u2223\u2223m0 (2\u2212j\u22122\u03c9)\u2223\u22232\u2211\nk\u2208 \u0000\n\u2223\u2223\u2223\u03c6\u0302 (2\u2212j\u22122\u03c9 + 2\u2212j\u22122k2pi)\u2223\u2223\u22232 .\nBy similar transformations, we finally arrive at\n=\n\u2223\u2223m0 (2\u2212j\u22121\u03c9 + pi)\u2223\u22232 \u2212j\u220f\nn=2\n\u2223\u2223m0 (2\u2212j\u2212n\u03c9)\u2223\u22232\u2211\nk\u2208 \u0000\n\u2223\u2223\u2223\u03c6\u0302 (\u03c9 + k2pi)\u2223\u2223\u22232\n= (2pi)\u22121\n\u2223\u2223m0 (2\u2212j\u22121\u03c9 + pi)\u2223\u22232 \u2212j\u220f\nn=2\n\u2223\u2223m0 (2\u2212j\u2212n\u03c9)\u2223\u22232\n= (2pi)\u22121\n\u2223\u22231\u2212m0 (2\u2212j\u22121\u03c9)\u2223\u22232 \u2212j\u22122\u220f\n`=0\n\u2223\u2223m0 (2`\u03c9)\u2223\u22232 .\nUsing (B.1), we obtain\n\u22121\u2211\nj=\u2212\u221e\n2j\u03a8j(\u03c4) = (2pi)\n\u22121\n\u222b 2pi\n0\n\u22121\u2211\nj=\u2212\u221e\nd\u03c9 exp(i\u03c4\u03c9)\n\u2223\u22231\u2212m0 (2\u2212j\u22121\u03c9)\u2223\u22232 \u2212j\u22122\u220f\n`=0\n\u2223\u2223m0 (2`\u03c9)\u2223\u22232 .\n23\nExpanding the telescopic sum over j, we get\n\u22121\u2211\nj=\u2212\u221e\n\u2223\u22231\u2212m0 (2\u2212j\u22121\u03c9)\u2223\u22232 \u2212j\u22122\u220f\nl=0\n\u2223\u2223m0 (2l\u03c9)\u2223\u22232 = 1\u2212 lim\nj\u2192\u2212\u221e\n\u2212j\u22121\u220f\nl=0\n\u2223\u2223m0 (2l\u03c9)\u2223\u22232\n= 1\u2212\n+\u221e\u220f\nl=0\n\u2223\u2223m0 (2l\u03c9)\u2223\u22232 .\nThus, we obtain\n\u22121\u2211\nj=\u2212\u221e\n2j\u03a8j(\u03c4) =\n1\n2pi\n\u222b 2pi\n0\nd\u03c9 exp (i\u03c4\u03c9)\n{\n1\u2212\n+\u221e\u220f\n`=0\n\u2223\u2223m0(2`\u03c9)\u2223\u22232\n}\n= \u03b40(\u03c4)\u2212 1\n2pi\n\u222b 2pi\n0\nd\u03c9 exp (i\u03c4\u03c9)\n+\u221e\u220f\n`=0\n\u2223\u2223m0(2`\u03c9)\u2223\u22232 .(B.2)\nNow, it remains to prove that the second term in (B.2) is equal to zero. By definition,\nm0(\u03c9) = 2\n\u22121\/2\u22112N\u22121\nn=0 hne\n\u2212in\u03c9, where {hk}k\u2208 \u0000 is the low pass quadrature mirror filter used in\nthe construction of Daubechies\u2019 compactly supported continuous time wavelet \u03c8 (Daubechies,\n1992, Section 6.4). We have\n1\n2pi\n\u222b 2pi\n0\nd\u03c9 exp (i\u03c4\u03c9)\nL\u220f\n`=0\n\u2223\u2223m0(2`\u03c9)\u2223\u22232 = L\u220f\n`=0\n2\u2212`\n2N\u22121\u2211\nn,m=0\nhnhm\u03b40(n\u2212m)\nwhich clearly tends to 0 as L tends to infinity. \u0003\nLemma 8. Matrix A defined in (4.4) has the following properties:\n\u22121\u2211\nj=\u2212\u221e\n2jAj` = 1.(B.3)\nIf, in addition, A is constructed using Haar wavelets, then\n\u22121\u2211\n`=\u2212\u221e\n|A\u22121j` | 6 C \u00b7 2j\/2(B.4)\n\u22121\u2211\n`=\u2212\u221e\nA\u22121j` = 2\nj(B.5)\nfor all j < 0, where C is a constant.\nProof. (B.3) is a straightforward corollary of Lemma 7. To prove (B.4), we introduce the\nauxiliary matrix \u0393 = D\u2032AD, where D = diag(2j\/2)j<0 is diagonal, i.e. \u0393j` = 2j\/2Aj`2`\/2.\nNason et al. (2000, Theorem 2) show that the spectral norm of \u0393\u22121 is bounded for Haar\nwavelets. Therefore, we obtain (B.4) as\n\u2211\u22121\n`=\u2212\u221e |A\u22121j` | =\n\u2211\u22121\n`=\u2212\u221e 2\nj\/22`\/2|\u0393\u22121j` | 6 C \u00b7 2j\/2. To\nprove (B.5), observe that if Xt,T is a white noise, then its classical Fourier spectrum is f(\u03bb) =\n(2pi)\u22121. On the other hand, white noise is an LSW process such that\n\u2211\nj Sj\u03a8j(\u03c4) = \u03b40(\u03c4)\n24\nwhich implies that Sj = 2\nj (Lemma 7). (B.5) then follows from the following property: If Xt\nis the wavelet spectrum of a stationary process with absolute summable autocovariance and\nwith Fourier spectrum f , then its wavelet spectrum is given by Sj =\n\u2211\n`A\n\u22121\nj`\n\u222b\nd\u03bbf(\u03bb)|\u03c8`(\u03bb)|2\nand, moreover,\n\u222b\nd\u03bb|\u03c8\u02c6`(\u03bb)|2 = 2pi. \u0003\nProof of Proposition 5. We will first show\n(B.6)\ncov\n(\u2211\ns\nXs,T\u03c8i,k(s),\n\u2211\ns\nXs,T\u03c8j,k(s)\n)\n=\n\u2211\n\u03c4\nc(k\/T, \u03c4)\n\u2211\nn\n\u03c8i,n(\u03c4)\u03c8j,n(0) +O(2\n\u2212(i+j)\/2T\u22121).\nWe have\ncov\n(\u2211\ns\nXs,T\u03c8i,k(s),\n\u2211\ns\nXs,T\u03c8j,k(s)\n)\n=\n\u2211\nl,u\n(\nSl\n(\nk\nT\n)\n+O\n(\nCl + Ll(u\u2212 k)\nT\n))\u2211\ns,t\n\u03c8l,s(u)\u03c8j,k(s)\u03c8l,t(u)\u03c8i,k(t).\nUsing Lj = O(M2\u2212j) in the first step, and the Cauchy inequality in the second one, we\nbound the reminder as follows:\u2223\u2223\u2223\u2223\u2223\u2211\nl,u\nO\n(\nCl + Ll(u\u2212 k)\nT\n)\u2211\ns,t\n\u03c8l,s(u)\u03c8j,k(s)\u03c8l,t(u)\u03c8i,k(t)\n\u2223\u2223\u2223\u2223\u2223 \u2264\n\u2211\nl\nCl +MLl(2\n\u2212l + min(2\u2212i, 2\u2212j))\nT\n\u2211\nu\n\u2223\u2223\u2223\u2223\u2223\u2211\ns,t\n\u03c8l,s(u)\u03c8j,k(s)\u03c8l,t(u)\u03c8i,k(t)\n\u2223\u2223\u2223\u2223\u2223 \u2264\u2211\nl\nCl +MLl(2\n\u2212l + 2\u2212i\/22\u2212j\/2)\nT\n(Alj)\n1\/2(Ali)\n1\/2 =\n2\u2212(i+j)\/2\nT\n{\u2211\nl\n(Cl +MLl2\n\u2212l)2(i+j)\/2(Alj)1\/2(Ali)1\/2 +\n\u2211\nl\nMLl(Alj)\n1\/2(Ali)\n1\/2\n}\n=\n2\u2212(i+j)\/2\nT\n{I + II}.\nBy formula (B.3),\nI \u2264\n\u2211\nl\n(Cl +MLl2\n\u2212l)(2iAli + 2jAlj) \u2264\n\u2211\nl\n(Cl +MLl2\n\u2212l)2\n\u2211\ni\n2iAli \u2264 D1.\nAs\n\u2211\ni Li2\n\u2212i < \u221e, we must have Li \u2264 C2i so\n\u2211\ni LiAij \u2264 C again by (B.3). This and the\nCauchy inequality give\nII \u2264 2M\n(\u2211\nl\nLlAli\n)1\/2(\u2211\nl\nLlAlj\n)1\/2\n\u2264 D2.\nThe bound for the reminder is therefore O(2\u2212(i+j)\/2T\u22121). For the main term, straightforward\n25\ncomputation gives\u2211\nl,u\nSl\n(\nk\nT\n)\u2211\ns,t\n\u03c8l,s(u)\u03c8j,k(s)\u03c8l,t(u)\u03c8i,k(t) =\n\u2211\n\u03c4\nc(k\/T, \u03c4)\n\u2211\nn\n\u03c8i,n(\u03c4)\u03c8j,n(0),\nwhich yields formula (B.6). Using Lemma 7 and (B.6) with i = j, we obtain\nE(c\u02dc(k\/T, 0)) =\n\u22121\u2211\nj=\u2212J\n2j\n{\u2211\n\u03c4\nc(k\/T, \u03c4)\u03a8j\u03c4 +O(2\n\u2212j\/T )\n}\n=\n\u2211\n\u03c4\nc(k\/T, \u03c4)\u03b40(\u03c4) +O(log(T )\/T ) = c(k\/T, 0) +O(log(T )\/T ),\nwhich proves the expectation. For the variance, observe that, using Gaussianity, we have\ncov\n(\nIi\n(\nk\nT\n)\n, Ij\n(\nk\nT\n))\n= 2\n(\u2211\n\u03c4\nc(k\/T, \u03c4)\n\u2211\nn\n\u03c8i,n(\u03c4)\u03c8j,n(0) +O(2\n\u2212(i+j)\/2T\u22121)\n)2\n= 2\n(\u2211\n\u03c4\nc(k\/T, \u03c4)\n\u2211\nn\n\u03c8i,n(\u03c4)\u03c8j,n(0)\n)2\n+O(2\u2212(i+j)\/2T\u22121),(B.7)\nprovided that (3.5) holds. Using (B.7), we finally obtain\n(B.8) Var(c\u02dc(k\/T, 0)) = 2\n\u22121\u2211\ni,j=\u2212J\n2i+j\n(\u2211\n\u03c4\nc(k\/T, \u03c4)\n\u2211\nn\n\u03c8i,n(\u03c4)\u03c8j,n(0)\n)2\n+O(T\u22121).\n\u0003\nReferences\nAntoniadis, A. and Sapatinas, T. (2002). Wavelet methods for continuous-time prediction using\nrepresentations of autoregressive processes in Hilbert spaces. J. Multivariate Anal. (Under\nrevision)\nBerkner, K. and Wells, R. (2002). Smoothness estimates for soft-threshold denoising via translation-\ninvariant wavelet transforms. Appl. Comput. Harmon. Anal., 12, 1\u201324.\nBrockwell, P. J. and Davis, R. A. (1991). Time series: Theory and methods (Second ed.). Springer,\nNew York.\nCalvet, L. and Fisher, A. (2001). Forecasting multifractal volatility. J. Econometrics, 105, 27\u201358.\nCoifman, R. and Donoho, D. (1995). Time-invariant de-noising. In A. Antoniadis and G. Oppen-\nheim (Eds.), Wavelets and Statistics (Vol. 103, pp. 125\u2013150). New York: Springer-Verlag.\nDahlhaus, R. (1996a). Asymptotic statistical inference for nonstationary processes with evolu-\ntionary spectra. In P. Robinson and M. Rosenblatt (Eds.), Athens conference on applied\nprobability and time series analysis (Vol. 2). Springer, New York.\nDahlhaus, R. (1996b). On the Kullback-Leibler information divergence of locally stationary pro-\ncesses. Stochastic Process. Appl., 62, 139\u2013168.\n26\nDahlhaus, R. (1997). Fitting time series models to nonstationary processes. Ann. Statist., 25, 1\u201337.\nDahlhaus, R., Neumann, M. H. and von Sachs, R. (1999). Non-linear wavelet estimation of time-\nvarying autoregressive processes. Bernoulli, 5, 873\u2013906.\nDaubechies, I. (1992). Ten lectures on wavelets. Philadelphia: SIAM.\nFryz\u00b4lewicz, P. (2002). Modelling and forecasting financial log-returns as locally stationary wavelet\nprocesses (Research Report). Department of Mathematics, University of Bristol. (http:\n\/\/www.stats.bris.ac.uk\/pub\/ResRept\/2002.html)\nGrillenzoni, C. (2000). Time-varying parameters prediction. Ann. Inst. Statist. Math., 52, 108\u2013122.\nKress, R. (1991). Numerical analysis. New York: Springer.\nLedolter, J. (1980). Recursive estimation and adaptive forecasting in ARIMA models with time\nvarying coefficients. In Applied Time Series Analysis, II (Tulsa, Okla.) (pp. 449\u2013471). New\nYork-London: Academic Press.\nMallat, S., Papanicolaou, G. and Zhang, Z. (1998). Adaptive covariance estimation of locally\nstationary processes. Ann. Statist., 26, 1\u201347.\nMe\u00b4lard, G. and Herteleer-De Schutter, A. (1989). Contributions to the evolutionary spectral theory.\nJ. Time Ser. Anal., 10, 41\u201363.\nNason, G. P. and von Sachs, R. (1999). Wavelets in time series analysis. Phil. Trans. Roy. Soc.\nLond. A, 357, 2511\u20132526.\nNason, G. P., von Sachs, R. and Kroisandt, G. (2000). Wavelet processes and adaptive estimation\nof evolutionary wavelet spectra. J. Roy. Statist. Soc. Ser. B, 62, 271\u2013292.\nOmbao, H., Raz, J., von Sachs, R. and Guo, W. (2002). The SLEX model of a non-stationary\nrandom process. Ann. Inst. Statist. Math., 54, 171\u2013200.\nOmbao, H., Raz, J., von Sachs, R. and Malow, B. (2001). Automatic statistical analysis of bivariate\nnonstationary time series. J. Amer. Statist. Assoc., 96, 543\u2013560.\nPhilander, S. (1990). El Nin\u02dco, La Nin\u02dca and the southern oscillation. San Diego: Academic Press.\nPriestley, M. (1965). Evolutionary spectra and non-stationary processes. J. Roy. Statist. Soc. Ser.\nB, 27, 204\u2013237.\nVan Bellegem, S. and von Sachs, R. (2002). Forecasting economic time series using models of\nnonstationarity (Discussion paper No. 0227). Institut de statistique, UCL. (ftp:\/\/www.\nstat.ucl.ac.be\/pub\/papers\/dp\/dp02\/dp0227.ps)\n27\nList of Figures\n1 These simulated examples demonstrate the idea of a sparse representation of\nthe local (co)variance. The left-hand column shows an example of a smooth\ntime-varying variance function of a TM process. The example on the right\nhand side is constructed in such a way that the local variance function c(z, 0)\nis constant over time. In this example, the only deviation from stationarity is\nin the covariance structure. The simulations, like all throughout the article,\nuse Gaussian innovations \u03bejk and Haar wavelets. . . . . . . . . . . . . . . . . 29\n(a) Theoretical wavelet spectrum equal to zero everywhere except scale \u22122\nwhere S\u22122(z) = 0.1 + cos2(3piz + 0.25pi). . . . . . . . . . . . . . . . . . 29\n(b) Theoretical wavelet spectrum S\u22122(z) = 0.1+cos2(3piz+0.25pi), S\u22121(z) =\n0.1 + sin2(3piz + 0.25pi) and Sj(z) = 0 for j 6= \u22121,\u22122. . . . . . . . . . . 29\n(c) A sample path of length 1024 simulated from the wavelet spectrum\ndefined in (a). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n(d) A sample path of length 1024 simulated from the wavelet spectrum\ndefined in (b). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n2 The wind anomaly data (910 observations from March 1920 to December 1995). 30\n(a) The wind anomaly index (in cm\/s). The two vertical lines indicate the\nsegment shown in Figure 2(b). . . . . . . . . . . . . . . . . . . . . . . . 30\n(b) Comparison between the one-step-ahead prediction in our model (dashed\nlines) and AR (dotted lines). . . . . . . . . . . . . . . . . . . . . . . . . 30\n3 The last observations of the wind anomaly series and its 1- up to 9-step-ahead\nforecasts (in cm\/s). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n(a) 9-step-ahead prediction using LSW modelling . . . . . . . . . . . . . . 31\n(b) 9-step-ahead prediction using AR modelling . . . . . . . . . . . . . . . 31\n28\nFigures\nRescaled Time\nSca\nles\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n(a) Theoretical wavelet spectrum equal to zero\neverywhere except scale \u22122 where S\n\u22122(z) = 0.1+\ncos2(3piz + 0.25pi).\nRescaled Time\nSca\nles\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n(b) Theoretical wavelet spectrum S\n\u22122(z) = 0.1+\ncos2(3piz + 0.25pi), S\n\u22121(z) = 0.1 + sin\n2(3piz +\n0.25pi) and Sj(z) = 0 for j 6= \u22121,\u22122.\n0 200 400 600 800 1000\n-\n4\n-\n2\n0\n2\n(c) A sample path of length 1024 simulated from\nthe wavelet spectrum defined in (a).\n0 200 400 600 800 1000\n-\n4\n-\n2\n0\n2\n(d) A sample path of length 1024 simulated from\nthe wavelet spectrum defined in (b).\nFigure 1: These simulated examples demonstrate the idea of a sparse representation of\nthe local (co)variance. The left-hand column shows an example of a smooth time-varying\nvariance function of a TM process. The example on the right hand side is constructed in\nsuch a way that the local variance function c(z, 0) is constant over time. In this example,\nthe only deviation from stationarity is in the covariance structure. The simulations, like all\nthroughout the article, use Gaussian innovations \u03bejk and Haar wavelets.\n29\n1920 1930 1940 1950 1960 1970 1980 1990\n-\n20\n0\n-\n10\n0\n0\n10\n0\n(a) The wind anomaly index (in cm\/s). The two\nvertical lines indicate the segment shown in Fig-\nure 2(b).\n1987 1988 1989 1990\n-\n10\n0\n-\n50\n0\n50\n(b) Comparison between the one-step-ahead pre-\ndiction in our model (dashed lines) and AR (dot-\nted lines).\nFigure 2: The wind anomaly data (910 observations from March 1920 to December 1995).\n30\n1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997\n-\n15\n0\n-\n10\n0\n-\n50\n0\n50\n10\n0\n(a) 9-step-ahead prediction using LSW modelling\n1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997\n-\n15\n0\n-\n10\n0\n-\n50\n0\n50\n10\n0\n(b) 9-step-ahead prediction using AR modelling\nFigure 3: The last observations of the wind anomaly series and its 1- up to 9-step-ahead\nforecasts (in cm\/s).\n31\n"}