{"doi":"10.1007\/s11207-006-0272-5","coreId":"136539","oai":"oai:bradscholars.brad.ac.uk:10454\/4092","identifiers":["oai:bradscholars.brad.ac.uk:10454\/4092","10.1007\/s11207-006-0272-5"],"title":"Automatic Short-Term Solar Flare Prediction Using Machine Learning and Sunspot Associations.","authors":["Qahwaji, Rami S.R.","Colak, Tufan"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2007","abstract":"YesIn this paper, a machine-learning-based system that could provide automated short-term solar flare prediction is presented. This system accepts two sets of inputs: McIntosh classification of sunspot groups and solar cycle data. In order to establish a correlation between solar flares and sunspot groups, the system explores the publicly available solar catalogues from the National Geophysical Data Center to associate sunspots with their corresponding flares based on their timing and NOAA numbers. The McIntosh classification for every relevant sunspot is extracted and converted to a numerical format that is suitable for machine learning algorithms. Using this system we aim to predict whether a certain sunspot class at a certain time is likely to produce a significant flare within six hours time and if so whether this flare is going to be an X or M flare. Machine learning algorithms such as Cascade-Correlation Neural Networks (CCNNs), Support Vector Machines (SVMs) and Radial Basis Function Networks (RBFN) are optimised and then compared to determine the learning algorithm that would provide the best prediction performance. It is concluded that SVMs provide the best performance for predicting whether a McIntosh classified sunspot group is going to flare or not but CCNNs are more capable of predicting the class of the flare to erupt. A hybrid system that combines a SVM and a CCNN is suggested for future use.EPSR","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/136539.pdf","fullTextIdentifier":"https:\/\/bradscholars.brad.ac.uk\/bitstream\/10454\/4092\/2\/qahwaji_paper.pdf","pdfHashValue":"bb29c5d15725782d7547bb2c0c7402bc88fae860","publisher":"Springer","rawRecordXml":"<record><header><identifier>\n        \n            \n                oai:bradscholars.brad.ac.uk:10454\/4092<\/identifier><datestamp>\n                2016-09-09T14:51:56Z<\/datestamp><setSpec>\n                com_10454_413<\/setSpec><setSpec>\n                col_10454_6345<\/setSpec>\n            <\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:doc=\"http:\/\/www.lyncode.com\/xoai\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n            \nAutomatic Short-Term Solar Flare Prediction Using Machine Learning and Sunspot Associations.<\/dc:title><dc:creator>\nQahwaji, Rami S.R.<\/dc:creator><dc:creator>\nColak, Tufan<\/dc:creator><dc:subject>\nSolar imaging; Space weather; Image processing; Machine learning; Sunspots; Solar cycle data; Prediction; Sunspot groups; McIntosh classification<\/dc:subject><dc:description>\nYes<\/dc:description><dc:description>\nIn this paper, a machine-learning-based system that could provide automated short-term solar flare prediction is presented. This system accepts two sets of inputs: McIntosh classification of sunspot groups and solar cycle data. In order to establish a correlation between solar flares and sunspot groups, the system explores the publicly available solar catalogues from the National Geophysical Data Center to associate sunspots with their corresponding flares based on their timing and NOAA numbers. The McIntosh classification for every relevant sunspot is extracted and converted to a numerical format that is suitable for machine learning algorithms. Using this system we aim to predict whether a certain sunspot class at a certain time is likely to produce a significant flare within six hours time and if so whether this flare is going to be an X or M flare. Machine learning algorithms such as Cascade-Correlation Neural Networks (CCNNs), Support Vector Machines (SVMs) and Radial Basis Function Networks (RBFN) are optimised and then compared to determine the learning algorithm that would provide the best prediction performance. It is concluded that SVMs provide the best performance for predicting whether a McIntosh classified sunspot group is going to flare or not but CCNNs are more capable of predicting the class of the flare to erupt. A hybrid system that combines a SVM and a CCNN is suggested for future use.<\/dc:description><dc:description>\nEPSRC<\/dc:description><dc:date>\n2009-12-14T16:06:17Z<\/dc:date><dc:date>\n2009-12-14T16:06:17Z<\/dc:date><dc:date>\n2007<\/dc:date><dc:type>\nArticle<\/dc:type><dc:type>\nAccepted Manuscript<\/dc:type><dc:identifier>\nQahwaji RSR and Colak T (2007) Automatic Short-Term Solar Flare Prediction Using Machine Learning and Sunspot Associations. Solar Physics. 241(1): 195-211.<\/dc:identifier><dc:identifier>\n90001377<\/dc:identifier><dc:identifier>\n90001377<\/dc:identifier><dc:identifier>\nhttp:\/\/hdl.handle.net\/10454\/4092<\/dc:identifier><dc:language>\nen<\/dc:language><dc:relation>\nhttp:\/\/dx.doi.org\/10.1007\/s11207-006-0272-5<\/dc:relation><dc:publisher>\nSpringer<\/dc:publisher>\n<\/oai_dc:dc>\n<\/metadata>\n        <\/record>","journals":null,"language":null,"relations":["http:\/\/dx.doi.org\/10.1007\/s11207-006-0272-5"],"year":2007,"topics":["Solar imaging; Space weather; Image processing; Machine learning; Sunspots; Solar cycle data; Prediction; Sunspot groups; McIntosh classification"],"subject":["Article","Accepted Manuscript"],"fullText":" The University of Bradford Institutional \nRepository \nhttp:\/\/bradscholars.brad.ac.uk \nThis work is made available online in accordance with publisher policies. Please refer to the \nrepository record for this item and our Policy Document available from the repository home \npage for further information. \nTo see the final version of this work please visit the publisher\u2019s website. Access to the \npublished online version may require a subscription. \nLink to original published version: http:\/\/dx.doi.org\/10.1007\/s11207-006-0272-5 \nCitation: Qahwaji RSR and Colak T (2007) Automatic Short-Term Solar Flare Prediction Using \nMachine Learning and Sunspot Associations. Solar Physics. 241(1): 195-211. \nCopyright statement: \u00a9 2007 Kluwer Academic Publishers. Full-text reproduced in accordance \nwith the publisher\u2019s self-archiving policy. \n \n1 \nAUTOMATIC SHORT-TERM SOLAR FLARE \nPREDICTION USING MACHINE LEARNING AND \nSUNSPOT ASSOCIATIONS \n \nR. QAHWAJI and T. COLAK   \nDepartment of Electronic Imaging and Media Communications \n University of Bradford, Richmond Road, Bradford BD7 1DP, England, U.K. \n(e-mail: r.s.r.qahwaji@brad.ac.uk, t.colak@bradford.ac.uk) \n \nAbstract. In this paper, a machine learning-based system that could provide automated short-term solar \nflares prediction is presented. This system accepts two sets of inputs: McIntosh classification of sunspot \ngroups and solar cycle data. In order to establish a correlation between solar flares and sunspot groups, \nthe system explores the publicly available solar catalogues from the National Geophysical Data Centre \n(NGDC) to associate sunspots with their corresponding flares based on their timing and NOAA numbers. \nThe McIntosh classification for every relevant sunspot is extracted and converted to a numerical format \nthat is suitable for machine learning algorithms. Using this system we aim to predict if a certain sunspot \nclass at a certain time is likely to produce a significant flare within six hours time and whether this flare is \ngoing to be an X or M flare. Machine learning algorithms such as Cascade-Correlation Neural Networks \n(CCNN), Support Vector Machines (SVM) and Radial Basis Function Networks (RBFN), are optimised \nand then compared to determine the learning algorithm that would provide the best prediction \nperformance. It is concluded that SVM provides the best performance for predicting if a McIntosh \nclassified sunspot group is going to flare or not but CCNN is more capable of predicting the class of the \nflare to erupt. A hybrid system that combines SVM and CCNN is suggested for future use. \n1. Introduction \nSolar activity is the driver of space weather. Thus it is important to be able to predict the \nviolent eruptions such as coronal mass ejections (CME) and solar flares (Koskinen et \nal., 1999). An efficient space weather forecasting system should analyse the recent solar \ndata and provide reliable predictions of forthcoming solar activities and their possible \neffects on Earth. Space weather is defined by the U.S. National Space Weather Program \n(NSWP) as \u201cconditions on the Sun and in the solar wind, magnetosphere, ionosphere, \nand thermosphere that can influence the performance and reliability of space-borne and \nground-based technological systems and can endanger human life or health\u201d. The \nimportance of understanding space weather is increasing for two reasons. First the way \nsolar activity affects life on Earth. Second we rely more and more on communications \nand power systems, both of which are vulnerable to space weather.  \nSpace weather forecasting is an emerging science that is facing many challenges. Some \nof these challenges are the automated definition, classification and representation of \nsolar features and the establishing of an accurate correlation between the occurrence of \nsolar activities (e.g., solar flares and CME) and solar features (e.g., sunspots, filaments \nand active regions) observed in various wavelengths. \nIn order to predict solar activities we need to use real-time, high-quality data and data \nprocessing techniques (Wang et al., 2003) . Solar flares research has shown that flares \nare mostly related to sunspots and active regions (Liu et al., 2005; Zirin and Liggett, \n1987; Shi and Wang, 1994). In this paper we aim to design an automated system that \nuses machine learning algorithms to provide efficient short-term prediction of \nsignificant flares. The training of machine learning algorithms is based on associating \nsunspots with their corresponding flares and on the numerical simulation of the solar \n2 \ncycle. In the near future we intend to extend the work reported in this paper and \ncombine it with automated image processing algorithms that will extract solar images in \nreal-time mode. Solar features will be extracted and classified in real-time and then fed \nto the machine learning algorithms reported here to provide real-time automated short-\nterm prediction of solar flares.  \nThere have been noticeable developments recently in solar imaging and the automated \ndetection of various solar features, by: Gao et al. (2002), Turmon et al. (2002), Shih and \nKowalski (2003), Benkhalil et al. (2003), LefebvreandRozelot (2004) and \nQahwajiandColak (2006a). Despite the recent advances in solar imaging, machine \nlearning and data mining have not been widely applied to solar data. However, there is \nno machine learning algorithm that is known to provide the \u201cbest\u201d learning performance \nespecially in the solar domain. One of the early attempts to predict solar activity was \nreported in Calvo et al. (1995). However in Calvo et al. (1995) solar activity was \nrepresented in terms of the Wolf number, not in terms of flares or CMEs. A recent \nsurvey describing the imaging techniques used for solar feature recognition is presented \nin Zharkova et al. (2005). Borda et al. (2002) described a method for the automatic \ndetection of solar flares using the Multi-Layer Perceptron (MLP) with back-propagation \ntraining rule. A supervised learning technique that required a large number of iterations \nwas used. Qu et al. (2003) compared the classification performance for features \nextracted from solar flares using Radial Basis Function Networks (RBFN), Support \nVector Machines (SVM) and MLP methods. Each flare is represented using nine \nfeatures. However, these features provide no information about the position, size and \nclassification of solar flares. Neural Networks (NNs) were used in Zharkova and \nSchetinin (2003) for filament recognition in solar images and in Qahwaji and Colak \n(2006a) NNs were used after image segmentation to verify the regions of interest, which \nwere solar filaments. \nThere is also some work reported on the correlation between sunspot classifications and \nflare occurrences and prediction. NOAA Space Environment Laboratory designed an \nexpert system called THEO (McIntosh, 1990) that uses flare rates of sunspot groups \nclassified by the McIntosh scheme, dynamical properties of sunspot growth, rotation, \nshear and previous flare activity, for assigning a 24 hour probabilities for X-ray flares. \nThe system was subjective because the forecast decision rules were incorporated by a \nhuman expert and had not been evaluated statistically. In Sammis et al. (2000), eight \nyears of active region observations from the US Air Force\/Mount Wilson data set, \nsupplied by the NOAA World Data CenterCentre were studied to explore the relation \nbetween \u03b4 sunspots and large flares. They confirmed K\u00fcnzel (1960)\u2019s findings that \nregions classified as \u03b2\u03b3\u03b4 produce more and larger flares than other regions of \ncomparable size. In particular, they concluded that each region larger than 1000 \nmillionths of hemisphere and classified as \u03b2\u03b3\u03b4 had nearly 40% probability of producing \nX1 class flares or even bigger flares. Bornmann and Shaw (1994) used multiple linear \nregression analysis to derive the effective solar flare contributions of each McIntosh \nclassification parameters. They concluded that the original seventeen McIntosh \nClassification parameters can be reduced to ten and still reproduce the observed flare \nrates. Gallagher et al. (2002) provided a method for finding the flaring probability of an \nactive region in a given day using Poisson statistics and historical averages of flare \nnumbers for McIntosh classifications. Wheatland (2004) introduced a Bayesian \napproach to flare prediction. This approach can be used to improve or refine the initial \nprediction of big flares during a subsequent period of time using the flaring record of an \nactive region together with phenomenological rules of flare statistics. To construct the \nflaring records for an active region, all the flares (whether significant or not) are \n3 \nconsidered. This method was tested on limited dataset (i.e., ten flaring days) but the \nauthor made interesting points on how it can be adapted to handle real-time data. More \nrecently, Ternullo et al. (2006) presented a statistical analysis carried out by correlating \nsunspot group data collected at the INAF-Catania Astrophysical Observatory with data \nof M- and X- class flares obtained by the GOES-8 satellite in the soft X-ray range \nduring the period from January 1996 till June 2003. It was found that the flaring \nprobability of sunspots increases slightly with the sunspot age as the flaring sunspots \nhave longer life times compared to non-flaring ones. These previous works demonstrate \nthat there is a clear correlation between sunspot classification and the flaring \nprobability. However, this relation hasn\u2019t been verified on large-scale data using \nautomated machine learning algorithms. In this work, we aim to design a fully \nautomated system that can analyse all the reported flares and sunspots for the last \nfourteen years to explore the degree of correlation and provide an accurate prediction \nusing time-series data. The work reported in this paper is a first step toward constructing \na real-time, automated, web compliant system that will extract the latest solar images in \nreal-time and provide automated prediction for the occurrence of big flares within six \nhours time period.   \n \n This paper is organised as follows: the solar data used in this paper are described in \nSection 2. Machine learning algorithms are discussed in Section 3. The implementation \nof the system and experimental results are reported in Section 4. Finally, the concluding \nremarks are given in Section 5. \n2. Data \nIn this work, we have used data from the publicly available sunspot group catalogue and \nthe solar flare catalogue. Both catalogues are provided by the National Geophysical \nData Centre (NGDC)1. NGDC keeps record of data from several observatories around \nthe world and holds one of the most comprehensive publicly available databases for \nsolar features and activities. \nFlares are classified according to their X-ray brightness in the wavelength range from \none to eight Angstroms. C, M, and X class flares can affect earth. C-class flares are \nmoderate flares with few noticeable consequences on Earth (i.e., minor geomagnetic \nstorms). M-class flares are large; they generally cause brief radio blackouts that affect \nEarth's polar regions by causing major geomagnetic storms. X-class flares can trigger \nplanet-wide radio blackouts and long-lasting radiation storms. The NGDC flare \ncatalogue (Figure 1) provides information about dates, starting and ending times for \nflare eruptions, location, X-ray classification, and the NOAA number for the active \nregions that are associated with the detected flares.  \nThe NGDC sunspot catalogue (Figure 1) holds records of many solar observatories \naround the world that have been tracking sunspot regions and supplying their date, time, \nlocation, physical properties, and classification. Two classification systems exist for \nsunspots: McIntosh and Mt. Wilson. McIntosh classification depends on the size, shape \nand spot density of sunspots, while the Mt. Wilson classification (Hale et al., 1919) is \nbased on the distribution of magnetic polarities within spot groups (Greatrix and Curtis, \n1973). In this study we used the McIntosh classification which is the standard for the \ninternational exchange of solar geophysical data. It is a modified version of the Zurich \nclassification system developed by Waldmeir. The general form of the McIntosh \n                                                 \n1 ftp:\/\/ftp.ngdc.noaa.gov\/STP\/SOLAR_DATA\/, last access: 2006 \n4 \nclassification is Zpc where, Z is the modified Zurich class, p is the type of spot, and c is \nthe degree of compactness (distribution of sunspots) in the interior of the group.  \nThese two catalogues are analyzed by a C++ computer program that we have created, in \norder to associate sunspots with their corresponding flares using their timing and \nlocation information. This computer program provides training and testing data sets for \nmachine learning algorithms using these associations. More information is provided \nabout the program and data sets in section 4. \n3. Machine Learning Algorithms \nIn this work, Cascade-Correlation Neural Networks (CCNN), Support Vector Machines \n(SVM) and Radial Basis Function Networks (RBFN) are used and compared for flare \nprediction. CCNN and RBFN are used because of their efficient performance in \napplications involving classification and time-series prediction (Frank et al., 1997). In \nour previous work Qahwaji and Colak (2006b) the performance of several NN \ntopologies and learning algorithms was compared and it was concluded that CCNN \nprovides better association between solar flares and sunspot classes. It is one of the \nrecent trends in machine learning to compare the performance of SVM and neural \nnetworks. The work reported in Acir and Guzelis (2004), Pal and Mather (2004), Huang \net al. (2004), and Distante et al. (2003) though not related to the prediction of solar \nactivities, has shown that SVM have outperformed neural networks in the classification \ntasks for different applications. Similar performance for SVM were reported for flares \ndetection in Qu et al. (2003), where statistical features were extracted from solar images \nand were trained using MLP, RBFN and SVM for automatic flares detection. Many \nexisting references, such as Qu et al. (2003), provide detailed description for SVM and \nRBF. However, the same is not true for CCNN, so we have decided to provide more \ndetails for CCNN. \n3.1. CASCADE-CORRELATION NEURAL NETWORKS (CCNN) \nThe training of Backpropagation neural networks is considered to be slow because of \nthe step-size problem and the moving target problem (Fahlmann and Lebiere, 1989). To \novercome these problems cascade neural networks were developed. The topology of \nthese networks is not fixed. The supervised training begins with a minimal network \ntopology. New hidden nodes are added gradually to create a multi-layer structure. The \nnew hidden nodes are added to maximize the magnitude of the correlation between the \nnew node\u2019s output and the residual error signal we are trying to eliminate. The weights \nof every new hidden node are fixed and never changed later, hence making it a \npermanent feature-detector in the network. This feature detector can then produce \noutputs or create other more complex feature detectors (Fahlmann and Lebiere, 1989) .  \nIn CCNN, the number of input nodes is determined based on the input features, while \nthe number of output nodes is determined based on the number of different output \nclasses. The learning of CCNN starts with no hidden nodes. The direct input-output \nconnections are trained using the entire training set with the aid of the backpropagation \nlearning algorithm. Hidden nodes are then added gradually and every new node is \nconnected to every input node and to every pre-existing hidden node. Training is carried \nout using the training vector and the weights of the new hidden nodes are adjusted after \neach pass (Fahlmann and Lebiere, 1989) .  \nHowever, a major problem facing these networks is over-fitting the training data, \nespecially when dealing with real-world problems (Smieja, 1993). Over-fitting usually \n5 \noccurs if the training data are characterized by many irrelevant and noisy features \n(Schetinin, 2003). On the other hand, the Cascade-Correlation architecture has several \nadvantages. Firstly, it learns very quickly, at least 10 times faster than Back-propagation \nalgorithms (Shet et al., 2005). Secondly, the network determines its own size and \ntopology and it retains the structures it has built even if the training set changes \n(Fahlmann and Lebiere, 1989). Thirdly, it requires no back-propagation of error signals \nthrough the connections of the network (Fahlmann and Lebiere, 1989). Finally, this \nstructure is useful for incremental learning in which new information is added to the \nalready trained network (Shet et al., 2005).   \n3.2. SUPPORT VECTOR MACHINE (SVM) \nSVMs are becoming powerful tools for solving a variety of learning and function \nestimation problems. Compared to neural networks, SVMs have a firm statistical \nfoundation and are guaranteed to converge to a global minimum during training. They \nare also considered to have better generalisation capabilities than neural networks. \nSVMs have been shown to outperform neural networks in a number of applications (Qu \net al., 2003; Acir and Guzelis, 2004; Pal and Mather, 2004; Huang et al., 2004,; \nDistante et al., 2003). SVM are a learning algorithm that is based on the statistical \nlearning theory. It was developed by Vapnik (1999) and uses the structural risk \nminimisation principle. Given an input vector, the SVM classifies it into one of two \nclasses. This is done by seeking the optimal separating hyperplane that provides \nefficient separation for the data and maximises the margin, i.e. it maximises the distance \nbetween the closest vectors in both classes to the hyper plane. This is of course with the \nassumption that the data are linearly separable. For the case when the data are not \nlinearly separable, SVM will map the data into a higher dimensional feature space \nwhere data can become linearly separable. This is done by using kernel functions. More \ndetails on SVM can be found in Qu et al. (2003) and Vapnik (1999) \n3.3. RADIAL BASIS FUNCTIONS NETWORKS (RBFN) \nRBFN are powerful interpolation techniques that can be efficiently applied in \nmultidimensional space. The RBFN approach to classification is based on curve fitting. \nLearning is achieved when a multi-dimensional surface is found that can provide \noptimum separation of multi-dimensional training data. In general, RBFN can model \ncontinuous functions with reasonable accuracy. The radial basis functions are the set of \nfunctions provided by the hidden nodes that constitute an arbitrary \u201cbasis\u201d for the input \npatterns (Qu et al., 2003). \nThe training of the RBFN is usually simpler and shorter compared to other NNs. This is \none of major advantages for using RBFN. However, greater computation and storage \nrequirements for classification of inputs are usually required after the network is trained \n(Sutton and Barto, 1998). \nRBFN apply a mixture of supervised and unsupervised learning modes. The layer from \ninput nodes to hidden nodes is unsupervised, while supervised learning exists in the \nlayer from hidden nodes to output nodes. Non linear transformation exists from input to \nhidden space, while linear transformation exists from the hidden to the output space \n(Bishop, 1995). More details on the theory and implementation of RBFN can be found \nin Qu et al. (2003)and Sutton and Barto (1998) \n6 \n \n4. Implementation and Results \n4.1 ASSOCIATING SUNSPOTS AND FLARES \n \nAs illustrated previously, a C++ platform was created to access the NGDC sunspots and \nflares catalogues and to automatically associate all the X and M class flares with their \ncorresponding sunspot groups. The correspondence was determined based on the \nlocation (i.e., same NOAA number) and timing information (maximum six hours time \ndifference between the erupting flare and its classified sunspot). After finding all the \nassociations, a numerical dataset is created for the machine learning algorithms.  \nThe whole process is described in the following steps: \n \n\u2022 Parse flare data from NGDC soft X-ray flare catalogues. \n\u2022 Parse sunspot group data from NGDC sunspot catalogues. \n\u2022 If an X- or M-class flare is found then try to identify its associated sunspot as \nfollows: \no If the NOAA number of the flare is specified in the flare catalogue then, \n\uf0a7 Search the sunspot catalogue to find the sunspot groups with the \nmatching NOAA number with flare. \n\uf0a7 Find the time difference between the flare eruption time and the \nextracted sunspots time.  \n\uf0a7 If the time difference is less than six hours then the flare record \nand the sunspot record are marked as being ASSOCIATED.  \n\uf0a7 If there are more than one sunspot group associated with a flare \nwithin six hours then choose the one with minimum time \ndifference. \n\u2022 Get the sunspot groups that are not associated with any flare. \no If no solar flares anywhere on the sun occur within one day after the \nclassification of this sunspot group then it is marked as \nUNASSOCIATED. \n\u2022 Record all ASSOCIATED groups and their corresponding flare classes (An \nexample for ASSOCIATED groups and flares is given in Figure 1).  \n\u2022 Record UNASSOCIATED groups.  \n\u2022 Create the data sets using McIntosh classes of groups and flare classes (Table I). \n \n4.2 CREATING THE INITIAL DATA SET    \nWe need to define appropriate numerical representations for the ASSOCIATED and \nUNASSOCIATED groups that were extracted earlier, because most machine learning \nalgorithms deal with numbers. This means that we need to convert the McIntosh \nclassification of every sunspot to a corresponding numerical value. To do so we will go \nback to the basics of McIntosh classification which is represented as Zpc, as explained \nearlier. There are seven classes under the Z part of classification (i.e., A, B, C, D, E, F, \nH), six classes under the p part (i.e., x, r, s, a, h, k) and four classes under the c part (i.e., \nx, o, i, c). Hence, three numerical values are used to represent every sunspot group. To \nfind these values the McIntosh classification for the sunspot group is divided into its \nthree components and the class order is divided by the total number of classes for each \n7 \ncomponent. As for the flare representation, two numerical values are used. The first \nvalue represents whether a significant flare occurs or not. This value becomes 0.9 if a \nsignificant flare occurs otherwise it is 0.1. Also if the flare is an X-class flare, it is \nrepresented by 0.9 and if it is an M-class flare, it is represented by 0.1. The numerical \nconversions of data sets are explained in more details in Table I.   \nIn this work, we have processed all the data of sunspots and flares for the periods from \n01 Jan1992 till 31 Dec 2005. Our software has analysed the data related to 29343 flares \nand 110241 sunspot groups and has managed to associate 1425 X- or M-class flares \nwith their corresponding sunspot groups. To complete our data set we have processed \nthe NGDC catalogues again to choose 1457 UNASSOCIATED sunspot groups based \non the rules mentioned previously. Hence, the total number of samples used for our \ntraining and testing sets for machine learning is 2882.  \nIt is worth mentioning here that the total number of reported X- or M-class flares in \nNGDC flares catalogues is 1870 which is 6.3% of the total numbers of flares in this \ncatalogue. However, out of these 1870 flares, only 1425 flares have NOAA numbers \nand are included in our study.  Veronig et al. (2002) has reported that about 10% of all \nsoft X-ray flares are X- or M-class flares based on studying all flares between 1976 and \n2000. Comparing these findings with ours reveals that there is a 3.7% difference. The \ndata used for Veronig et al. (2002) is more comprehensive as it consisted of  24 years \ndata with three solar maxima, while we have extracted all the available NGDC data \nwhich is fourteen years data with only one solar maximum. Veronig et al. (2002) \ncommented that \u201cM- and X-class flares are less frequent and occur primarily during \ntimes of maximum solar activity\u201d which may explain the 3.7% difference between the \ntwo findings. \n4.3 THE INITIAL TRAINING AND TESTING PROCESS \n4.3.1. Jack-Knife Technique \nFor our study, all the machine learning training and testing experiments are carried out \nwith the aid of the Jack-knife technique (Fukunaga, 1990). This technique is usually \nimplemented to provide a correct statistical evaluation for the performance of the \nclassifier when implemented on a limited number of samples. This technique divides the \ntotal number of samples into two sets: a training set and a testing set. Practically, a \nrandom number generator decides which samples are used for the training of the \nclassifier and which are kept for testing it. The classification error depends mainly on \nthe training and testing samples. For a finite number of samples, the error counting \nprocedure can be used to estimate the performance of the classifier (Fukunaga, 1990). In \neach experiment, 80% of the samples were randomly selected and used for training \nwhile the remaining 20% were used for testing. Hence, the number of training samples \nis 2305, while 577 samples are used for the testing of the classifier.  \n4.3.2. Initial Training and Testing \nTo test the suitability of our data set for solar flare prediction, we started our \nexperiments with training and testing of CCNN with our initial data set.  This data set is \ndivided into inputs and outputs as described in Section 4.2. The input set of our training \ndata consisted of three numerical values representing the McIntosh classification of the \nsunspot. The output set consisted of two nodes representing whether a flare is likely to \noccur and whether it is an X- or M-class flare. Several experiments based on the Jack-\n8 \nknife technique were carried out and we found that the prediction rate for flares in the \nbest case scenario was 72.9%. This indicated that a correlation existed between the \ninput and output sets, but it is not high enough to provide reliable prediction of solar \nactivities. It soon became obvious that our training data do not enable the classifier to \ncreate accurate discriminations between the input and output data. We concluded that \nusing the McIntosh classification on its own is not enough and more discriminating data \nneed to be added to the input data. Hence, we decided to associate the classified \nsunspots with the sunspot cycle. The new input is computed by finding the time of the \nsunspot with respect to the solar cycle. Our decision to include the timing information \nseemed to be supported by the fact that the rise and fall of solar activity coincides with \nthe sunspot cycle (Pap et al., 1990) and by the fact that the M- and X- class flares \nmostly occur during times of maximum solar activity (Veronig et al., 2002). When the \nsolar cycle is at a maximum, plenty of large active regions exist and many solar flares \nare detected. The number of active regions decreases as the Sun approaches the \nminimum part of its cycle (Pap et al,. 1990). More details are provided in the section to \nfollow.  \n4.3.3. Sunspot Cycle  \nTo determine the association between the classified sunspot group and their location on \nthe solar cycle, it is important to use a mathematical model that could simulate the \nbehaviour of the solar cycle. The model introduced by Hathaway et al. (1994), which is \nshown in Equation (1), can serve this purpose. This equation enables us to estimate the \nsunspot number for any given date and it enables the prediction of future values as well.  \n                                   (1) \n \nIn Equation (1), parameter \u2018a\u2019 represents the amplitude and is related to the rise of the \ncycle minimum, \u2018b\u2019 is related to the time in months from minimum to maximum; \u2018c\u2019 \ngives the asymmetry of the cycle; and \u2018to\u2019 denotes the starting time. More information \nabout the derivation of Equation (1) can be found in Hathaway et al. (1994).  \nTo verify the accuracy of this model we have applied this model to the dates from 01 \nJan 1992 till 31 Dec 2005 and the average of the sunspot numbers for each month is \nshown in Figure 2. Using our software, we have extracted the sunspot data for the same \nperiod and found the actual average of monthly sunspot numbers. This value is also \nshown in Figure 2. Comparing both plots, it is obvious that Hathaway\u2019s model provides \nreliable modelling for the rise and fall of the solar cycle. An alternative method to \nsimulate the solar cycle could be the NN model introduced by Calvo et al. (1995). It is \nworth mentioning that in Lantos (2006) it is shown that all solar cycle prediction \nmethods give, at least for one of the cycles, an error larger than 20 %, especially when \ncycles 13 to 23 are considered. Although Hathaway\u2019s model represents the evolution of \nthe solar cycle in a slightly rough manner, we used this model because of its easy \nimplementation for the existing and future solar cycles. In addition, Hathaway\u2019s model \nprovides an automated numerical simulation for the behaviour of the solar cycle that can \nbe used for studying the past, present and future evolution of the cycle. This is \nparticularly important for any automated prediction system as certain sunspots group \nclasses are more likely to flare at certain times. Hence, Hathaway\u2019s model is integrated \ninto the training and testing phases of our machine learning algorithms. After the \nmachine learning algorithms are trained, the timing information for any future sunspot \ncan be simulated and fed to them to provide real-time flares prediction.  \n9 \n \n4.3.4. The Final Data Set \nAfter using Hathaway\u2019s model, the final training data set can be constructed. For each \nsample, the training data set consists of six numerical values: four input values and two \noutput values. The first three input values represent the McIntosh classification of \nsunspots, while the last numerical value represents the simulated and normalized \nsunspot number based on Hathaway\u2019s model. The simulated Hathaway\u2019s value is \nnormalised by dividing it by the actual or estimated number of sunspots at solar maxima \nto obtain a value between zero and one. The output set consists of two values: The first \nvalue is used to predict whether the sunspot is going to produce a significant flare or \nnot. For performance evaluation purposes, this value will be referred to as the \u201cCorrect \nFlare Prediction (CFP)\u201d value. The second value is used to determine whether the \npredicted flare is an X or M flare, and it will be referred to as the \u201cCorrect Flare Type \nPrediction (CFTP)\u201d.  The final input and output data set is given in Table II. Also some \nexamples from the data set are provided in Table III. \n4.4. FINAL TRAININGS AND TESTINGS FOR COMPARISONS \nIn order to determine which machine learning algorithm is more suitable for our \napplication, the prediction performance of CCNN, SVM and RBFN needs to be \ncompared. However, these learning algorithms must be optimised before the actual \ncomparison can take place. Learning algorithms are optimised to ensure that their best \nperformances are achieved. In order to find the best parameters and\/or topologies for the \nthree learning algorithms initial training and testing experiments are applied. The results \nof these experiments are used to determine the optimum parameters and topology for \nevery machine learning algorithm before it can be compared with other learning \nalgorithms.  \n4.4.1 Optimising the CCNN for final comparison \nIn Qahwaji and Colak (2006b), it was proven that CCNN provides the optimum neural \nnetwork performance for processing solar data in catalogues. However, many hidden \nnodes and just one hidden layer were used for training the network in Qahwaji and \nColak (2006b). To simplify the topology of CCNN more experiments with two hidden \nlayers are concluded in this work by changing the number of hidden nodes in each layer \nfrom one to ten.  At the end we managed to compare 100 different CCNN topologies \nbased on the best CFP and CFTP. As can be seen from Figures 3 and 4, a CCNN with \nsix hidden nodes in the first layer and four hidden nodes in the second layer gives the \nbest results for CFP and CFTP. \n4.4.2 Optimising the SVM for final comparison \nFor our work, the SVM experiments are carried out using the mySVM2 program. To \noptimise the performance of SVM the optimal kernel and its parameters need to be \ndetermined empirically. There are no known guidelines to choose them (Manik et al., \n2004). In our work, many kernels were tested, such as the dot, polynomial, neural, \nradial, and Anova kernels, in order to choose the kernel that provides the best \nclassification performance. After applying all these kernels we have concluded that the \n                                                 \n2 Ruping, S. 2004  http:\/\/www-ai.informatik.uni-ortmund.de\/SOFTWARE\/MYSVM\/index.html. \n \n10 \nAnova kernel, which is shown in Equation (2), provides better results compared to all \nother kernels. Our findings match the findings of Manik et al. (2004). .  \n \ni i\ni\n( , ) = ( exp(- ( - )))  dk x y \u03b3 x y\u2211                                    (2) \nIn Equation (2), the sum of exponential functions in x and y directions defines the \nAnova kernel. The shape of the kernel is controlled by the parameters \u03b3 (gamma) and d \n(exponential degree). To complete our SVM optimisation we need to determine these \nparameters for the Anova kernel. Two sets of experiments are carried out to determine \nthese values. In the first experiments the value of \u03b3 is fixed at 50 and several SVM \ntraining experiments are concluded while changing the value of d from one to ten. For \nevery new experiment the value of d is incremented and the prediction performance is \nrecorded. The results for CFP and CFTP for different d values are shown in Figure 5. \nThis figure shows that saturation is reached as the value of d exceeds eight. In the \nsecond experiments d is fixed at five and the value of \u03b3 is changed from 10 to 100. \nFigure 6 shows the results for CFP and CFTP for different \u03b3 values. The best results are \nreached for \u03b3 = 85. Based on the above, the Anova kernel is used with its d and \u03b3 \nparameters set to 8 and 85 consecutively.  \n4.4.3 Optimising the RBFN for final comparison \n A standard RBFN has a feed-forward structure consisting of three layers, an input \nlayer, a nonlinear hidden layer and a linear output layer (Broomhead and Lowe, 1988). \nWe have used MATLAB\u2019s RBFN tool for our experiments. This tool adds nodes to the \nhidden layer of a RBFN until it reaches the specified learning goal, which is usually the \nmean squared error value. For our case this value is made equal to 0.001. To optimise \nthe RBFN, we have run many simulations by increasing the number of hidden nodes \nfrom 3 to 60 in order to determine the optimum RBFN topology. Figure 7 shows the \ncomparison for RBFN simulations with different number of hidden nodes. It is clear that \nthe optimum number of hidden nodes is between 20 and 40 nodes. Hence, we have \ndecided to use 30 hidden nodes for our future experiments. \n  \n4.5. COMPARING THE PERFORMANCES \nAfter successfully optimising CCNN, SVM and RBFN, we are able to compare the \nprediction performance for the three learning algorithms. As illustrated previously, our \nCCNN has two hidden layers with six and four hidden nodes, our SVM uses the Anova \nkernel with d and \u03b3 parameters set to 8 and 85 respectively and our RBFN has 30 hidden \nnodes. All the training and testing experiments are carried out based on the statistical \nJack-knife technique. For every experiment the Jack-knife technique is applied once to \nobtain the training and testing sets. For every experiment the same data sets are shared \nbetween CCNN, SVM and RBFN. Ten experiments are carried out and the results for \nCFP, CFTP, and training times are shown in Table IV. \nAs it can be seen from Table IV, CCNN requires more training times compared to SVM \nand RBFN. On the other hand, SVM is very fast in training and on average it provides \nthe best performance for CFP, which is 93.07%. The best performance for CFTP is \nprovided by CCNN, which on average is 88.02%. It is worth noting that RBFN \noutperforms SVM for CFTP as well. Figure 8 and Figure 9 shows the graphical \ncomparison of CFP and CFTP performances for the three learning algorithms.  \n \n11 \n5. Conclusions and Future Research \nIn this paper, an empirical evaluation for three of the top machine learning algorithms \nfor the automated prediction of solar flares is carried out. CCNN, RBFN and SVM are \ncompared based on their topology, training times and most importantly their prediction \naccuracy. We used the publicly available solar catalogues from the National \nGeophysical Data Centre to associate the reported occurrences of M and X solar flares \nwith the relevant sunspots that were classified manually and exist in the same NOAA \nregion prior to flares occurrence.  \nWe have investigated all the reported flares and sunspots between 01 Jan 1992 and 31 \nDec 2005. Our software has managed to associate 1425 M and X soft X-ray flares with \ntheir corresponding sunspot groups. These 1425 flares were produced by 446 different \nsunspot groups. Among these groups, NOAA region 10808 produced 30, NOAA region \n10375 produced 29 and NOAA region 9393 produced 27 of the major flares. Our \nsoftware was used to extract these findings and it can be used for statistical-based \nanalysis of different aspects connected to flares occurrence.  \nOur practical findings on the associations show that there is a direct relation between the \neruptions of flares and certain McIntosh classes of sunspots such as Ekc, Fki and Fkc. \nOur findings are in accordance with McIntosh (1990), Warwick (1966), and Sakurai \n(1970). However we believe that our work is the first to introduce a fully automated \ncomputer platform that could verify this relation using machine learning. In its current \nversion, our system can provide a few hours prediction in advance (i.e., up to six hours \nin advance) by analysing the latest sunspots data. \nWe compared different machine learning algorithms to obtain the best prediction results \nfor solar flares. Erupting flares were associated with their corresponding sunspot groups \nwith McIntosh classifications. To increase the accuracy of prediction, a mathematical \nmodel representing the sunspot cycle based on the work of Hathaway et al. (1994), is \nimplemented to automatically simulate the solar cycle. The simulated solar cycle and \nclassified sunspots are converted to the appropriate numerical formats and used to train \nCCNN, RBFN and SVM machine learning systems, to predict automatically whether a \nsignificant flare will occur and whether it is going to be an X- or M-class flare.  \nIn this research, SVM provides the best results for predicting if a sunspot group with \nMcIntosh classification is going to flare or not but CCNN is more capable of predicting \nthe class of the flare to erupt. We have reached the conclusion that a hybrid system \nsimilar to the one shown in Figure 10, which combines both SVM and CCNN, will give \nbetter results for flare prediction. In this hybrid system, the inputs will be fed to SVM \nengine to decide if the sunspot group is going to produce a solar flare or not. Depending \non the outcome of the SVM engine, CCNN is used to predict the type of the flare to \nerupt. \nWe believe that this work is a first step toward constructing a fully automated and web-\ncompliant platform that would provide short-term prediction for solar flares. In this \npaper, we managed to extract the experts\u2019 knowledge which is embedded in the \ncatalogues of flares and sunspots and managed to represent this knowledge using \nassociation and learning algorithms. However, the creation of an automated and \nobjective system means that the system should be able to accept the latest solar images, \ndetect solar features (i.e., sunspots, active regions, etc), provide the McIntosh \nclassification for the detected features and then feed these classifications to our hybrid \nsystem to obtain the short-term prediction for solar activities. This would require the use \nof advanced image processing techniques and we are currently working on this.  \nAnother future direction is to incorporate the findings of Ternullo et al. (2006) and \nmodify our association software to extract the age of every associated sunspot. A flaring \nprobability that increases with the age of this sunspot can be added to our input features. \n12 \nThis could help in increasing the prediction performance even further. The association \nsoftware can also be modified to study the development of flaring sunspots. Sunspots \nare dynamic objects and they evolve with time. Most of the existing prediction models \ndon\u2019t take this evolution into consideration when predicting flares (Wheatland, 2004). A \nthird direction is to combine the Mt Wilson classification of sunspots with McIntosh \nclassification. This is inspired by the findings of Sammis et al. (2000), were it was \nproven that there is a general trend for large sunspots to produce large flares, but the \nmost important factor is the magnetic properties of these sunspots. A serious problem \nfacing these future directions is that the observations of sunspots are not made at fixed \ntime intervals. We need to find a solution to this problem to improve the consistency of \nthese data and make it more suitable for machine learning algorithms.  \n \nAcknowledgment \nThis work is supported by an EPSRC Grant (GR\/T17588\/01), which is entitled \u201cImage \nProcessing and Machine Learning Techniques for Short-Term Prediction of Solar \nActivity\u201d. \n \nReferences \nAcir, N. & Guzelis, C.: 2004, Expert Systems with Applications 27, 451. \nBenkhalil, A., Zharkova, V., Ipson, S., and Zharkov, S.: 2003, in H Holstein and F. \nLabrosse (ed.), AISB'03 Symposium on Biologically-Inspired Machine Vision, Theory \nand Application, University of Wales, Aberystwyth, p. 66. \nBishop, C.M.: 1995, Neural Networks for Pattern Recognition, Oxford University \nPress, London, p.164. \nBorda, R.A.F., Mininni, P.D., Mandrini, C.H., Gomez, D.O., Bauer, O.H., and Rovira, \nM.G.: 2002, Solar Phys. 206, 347. \nBornmann, P.L. and Shaw, D.: 1994, Solar Phys. 150, 127. \nBroomhead, D.S. and Lowe, D.: 1988, Complex Systems 2, 321. \nCalvo, R.A., Ceccatto, H.A., and Piacentini, R.D.: 1995, Astrophys. J. 444, 916. \nDistante, C., Ancona, N., and Siciliano, P.: 2003, Sensors and Actuators B: Chemical  \n88, 30. \nFahlmann, S.E. and Lebiere, C.: 1989. in D. S. Touretzky(ed.), Advances in Neural \nInformation Processing Systems 2 (NIPS-2) , Morgan Kaufmann, Denver, Colorado, \nUSA, p. 524. \nFrank, R.J., Davey, N., and Hunt, S.P.: 1997. J. Intelligent and Robotic Systems, 31, p. \n91. \nFukunaga, K.: 1990, Introduction to Statistical Pattern Recognition, Academic Press, \nNew York, p. 220. \nGallagher, P.T., Moon, Y.J., and Wang, H.M.: 2002, Solar Phys. 209, 171. \nGao, J.L., Wang, H.M., and Zhou, M.C.: 2002, Solar Phys. 205, 93. \n13 \nGreatrix, G.R. and Curtis, G.H.: 1973, Observatory 93, 114. \nHale, G.E., Ellerman, F., Nicholson, S.B., and Joy, A.H. 1919, Astrophys. J. 49, 153. \nHathaway, D., Wilson, R.M., and Reichmann, E.J.: 1994, Solar Phys. 151, 177. \nHuang, Z., Chen, H.C., Hsu, C.J., Chen, W.H., and Wu, S.S.: 2004, Decision Support \nSystems 37, 543. \nKoskinen, H., Eliasson, L., Holback, B., Andersson, L., Eriksson, A., M\u00e4lkki, A., \nNorberg, O., Pulkkinen, T., Viljanen, A., Wahlund, J.-E., and Wu, J.-G.: 1999. Space \nWeather and Interactions with Spacecraft, Finnish Meteorological Institute Reports \n1999-4. \nK\u00fcnzel, H.: 1960, Astron. Nachr. 285, 271. \nLantos, P.: 2006, Solar Phys. 236, 199. \nLefebvre, S. and Rozelot, J.P.: 2004, Solar Phys. 219, 25. \nLiu, C., Deng, N., Liu, Y., Falconer, D., Goode, P.R., Denker, C., and Wang, H.M.: \n2005, Astrophys. J. 622, 722. \nManik, A., Singh, A., and Yan, S.: 2004, in E.G. Berkowitz (ed.), Fifteenth Midwest \nArtificial Intelligence and Cognitive Sciences Conference, Omnipress, Chicago. \nMcIntosh, P.S.: 1990, Solar Phys. 125, 251. \nPal, M. and Mather, P.M.: 2004, Future Generation Computer Systems 20, 1215. \nPap, J., Bouwer, S., and Tobiska, W.:1990, Solar Phys. 129, 165. \nQahwaji, R. and Colak, T.: 2006a, Int. J. Computers and Their Appl. 13, 9. \nQahwaji, R. & Colak, T. 2006b, in H.W. Chu, J. Aguilar, N. Rishe, J. Azoulay (ed.), \nThe Third International Conference on Cybernetics and Information Technologies, \nSystems and Applications, International Institude of Informatics and Systemics, \nOrlando, p.192. \nQu, M., Shih, F.Y., Jing, J., and Wang, H.M.: 2003, Solar Phys. 217, 157. \nSakurai, K.: 1970, Planet Space Sci. 18, 33. \nSammis, I., Tang, F., and Zirin, H.: 2000, Astrophys. J. 540, 583. \nSchetinin, V.: 2003, Neural Processing Lett. 17, 21. \nShet, R.N., Lai, K.H., A.Edirisinghe, E., and Chung, P.W.H.: 2005, Lecture Notes in \nComputer Science, 3523, 343. \nShi, Z.X. and Wang, J.X.: 1994, Solar Phys. 149, 105. \nShih, F.Y. and Kowalski, A.J.: 2003, Solar Phys. 218, 99. \nSmieja, F.J.: 1993, Circuits, Systems and Signal Processing 12, 331. \n14 \nSutton, R.S. and Barto, A.G.: 1998, Reinforcement Learning: An Introduction , MIT \nPress, Cambridge, Massachusetts, p.193. \nTernullo, M., Contarino, L., Romano, P., and Zuccarello, F.: 2006, Astron. Nachr. 327, \n36. \nTurmon, M., Pap, J.M., and Mukhtar, S.: 2002, Astrophys. J. 568, 396. \nVapnik, V.: 1999, The Nature of Statistical Learning Theory, Springer-Verlag, New \nYork, p. 138. \nVeronig, A., Temmer, M., Hanslmeier, A., Otruba, W., and Messerotti, M.: 2002, \nAstron. Astrophys. 382, 1070. \nWang, H., Qu, M., Shih, F., Denker, C., Gerbessiotis, A., Lofdahl, M., Rees, D. & \nKeller, C.: 2003, Bulletin of the AAS , 36, 755. \nWarwick, C.S.: 1966, Astrophys. J. 145, 215. \nWheatland, M.S.: 2004, Astrophys. J. 609, 1134. \nZharkova, V., Ipson, S., Benkhalil, A., and Zharkov, S.: 2005, Artificial Intelligence \nReview, 23, 209. \nZharkova, V. and Schetinin, V.: 2003, in L. C. Jain, R. J. Howlett, V. Palade (ed.), \nProceedings of the Seventh International Conference on Knowledge-Based Intelligent \nInformation and Engineering Systems KES'03, Springer, Oxford, U.K., p.148. \nZirin, H. and Liggett, M.A.: 1987, Solar Phys. 113, 267. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n15 \n \n \n \n \n \n \n \n \n \n \n \nFigure 1: Parts from NGDC flare and sunspot catalogues for 02\/Nov\/2003showing an associated flare and \nsunspot region. \n \nFigure 2: Comparison of the actual and simulated \u2018monthly average sunspot number\u2019 between 01 Jan \n1992 and 31 Dec 2005. \n \nFigure 3: Comparison of CCNNs with different number of hidden nodes for CFP. \n \nFigure 4: Comparison of CCNNs with different number of nodes for CFTP. \n \nFigure 5: Comparison of SVM with different degree values. \n \nFigure 6: Comparison of SVM with different gamma values. \n \nFigure 7: Comparison of RBFN with different number of maximum hidden nodes. \n \nFigure 8: Graphical comparison of experiment results for Correct Flare Prediction (CFP) using different \nlearning algorithms. \n \nFigure 9: Graphical comparison of experiment results for Correct Flare Type Prediction (CFTP) using \ndifferent learning algorithms. \n \nFigure 10: Suggested hybrid flare prediction system. \n \nTable I:  Inputs and output values for initial machine training process.  \nInputs Outputs \nMcIntosh classes \nFlare \n=0.9 \n \nNo \nflare= \n0.1 \nNo\/ \nM-class \nflare = \n0.1 \n \nX-class \nFlare = \n0.9 \nA= 0.10 \nH= 0.15 \nB= 0.30 \nC= 0.45 \nD= 0.60 \nE= 0.75 \nF= 0.90 \nX= 0 \nR=0.10 \nS=0.30 \nA=0.50 \nH=0.70 \nK=0.90 \nX=0 \nO=0.10 \nI=0.50 \nC=0.90 \n \n16 \nTable II:  Inputs and output values for final machine training process.  \nInputs Outputs \nMcIntosh classes \nNormalized \naverage \nsunspot \nnumber from \nHathaway \nfunction \nFlare \n=0.9 \n \nNo \nflare= \n0.1 \nNo\/ \nM-class \nflare = \n0.1 \n \nX-class \nFlare = \n0.9 \nA= 0.10 \nH= 0.15 \nB= 0.30 \nC= 0.45 \nD= 0.60 \nE= 0.75 \nF= 0.90 \nX= 0 \nR=0.10 \nS=0.30 \nA=0.50 \nH=0.70 \nK=0.90 \nX=0 \nO=0.10 \nI=0.50 \nC=0.90 \n \n \nTable III:  Some example input and output values used for machine training process: Data 1 is \nrepresenting a  X-class flare detected on 02\/Nov\/2003 at 17:39 on NOAA region 10486 which is \nclassified as EKC at 15:45. Data 2 is representing a M-class flare detected on 02\/May\/2003 at 02:47 on \nNOAA region 10345 which is classified as DAO at 00:45. Data 3 is representing NOAA region 7566 \nwhich is classified as CSO at 08:40 and no corresponding flares detected at that day.  \nData \nno Inputs Outputs \n1 E K C Normalized sunspot number Flare X-class 0.75 0.90 0.90 0.145165 0.9 0.9 \n2 D A O Normalized sunspot number Flare M-class 0.60 0.50 0.10 0.189032 0.9 0.1 \n3 C S O Normalized sunspot number No flare No flare 0.45 0.30 0.10 0.100992 0.1 0.1 \n \nTable IV:  Experiments and results using Jack-knife technique. (CFP= Correct Flare Prediction, CFTP= \nCorrect Flare Type Prediction, TT=Training Time in seconds). \n  CCNN SVM RBFN  \nExperiments TT. CFP CFTP  TT.  CFP CFTP  TT. CFP CFTP \n1 32.14 92.72 88.91 0.05 93.41 82.50 13.94 89.77 85.96 \n2 31.83 93.07 88.39 0.03 94.28 83.54 13.41 89.95 85.44 \n3 31.72 90.47 86.31 0.05 91.33 82.15 13.75 88.21 84.06 \n4 31.94 93.24 89.43 0.03 93.07 81.98 13.92 90.64 86.83 \n5 31.73 92.20 88.73 0.05 94.11 86.48 13.70 88.39 84.92 \n6 31.75 90.12 85.96 0.02 93.07 84.40 13.34 89.25 85.10 \n7 31.63 92.20 89.25 0.03 93.93 84.92 13.34 90.47 87.52 \n8 32.00 91.85 88.91 0.06 93.93 84.92 13.38 90.47 87.52 \n9 32.00 91.16 88.39 0.03 92.20 84.23 13.66 88.21 85.62 \n10 31.78 88.91 85.96 0.05 91.33 82.15 13.66 89.08 85.96 \nAverage 31.85 91.59 88.02 0.04 93.07 83.73 13.61 89.45 85.89 \n \n \n \n"}