{"doi":"10.1214\/13-BA814","coreId":"68160","oai":"oai:eprints.lancs.ac.uk:33244","identifiers":["oai:eprints.lancs.ac.uk:33244","10.1214\/13-BA814"],"title":"An adaptive sequential Monte Carlo sampler","authors":["Fearnhead, Paul","Taylor, Benjamin M."],"enrichments":{"references":[{"id":874500,"title":"A genetic algorithm tutorial.","authors":[],"date":"1994","doi":null,"raw":null,"cites":null},{"id":873923,"title":"A Markov chain Monte Carlo version of the genetic algorithm differential evolution: easy Bayesian computing for real parameter spaces.","authors":[],"date":"2006","doi":null,"raw":null,"cites":null},{"id":865679,"title":"A sequential particle filter method for static models.","authors":[],"date":"2002","doi":null,"raw":null,"cites":null},{"id":864931,"title":"A tutorial on adaptive MCMC.","authors":[],"date":"2008","doi":null,"raw":null,"cites":null},{"id":868583,"title":"Adaptive direction sampling.","authors":[],"date":"1994","doi":null,"raw":null,"cites":null},{"id":865379,"title":"Adaptive importance sampling in general mixture classes.","authors":[],"date":"2008","doi":null,"raw":null,"cites":null},{"id":865959,"title":"Adaptive methods for sequential importance sampling with application to state space models.","authors":[],"date":"2008","doi":null,"raw":null,"cites":null},{"id":872009,"title":"Adaptively scaling the Metropolis algorithm using expected squared jumped distance. To appear: Statistica Sinica.","authors":[],"date":"2010","doi":null,"raw":null,"cites":null},{"id":869053,"title":"An adaptive Metropolis algorithm.","authors":[],"date":"1998","doi":null,"raw":null,"cites":null},{"id":871680,"title":"Annealed importance sampling.","authors":[],"date":"2001","doi":null,"raw":null,"cites":null},{"id":870869,"title":"Blind deconvolution via sequential imputations.","authors":[],"date":"1995","doi":"10.2307\/2291068","raw":null,"cites":null},{"id":867589,"title":"Computational methods for complex stochastic systems: A review of some alternatives to MCMC.","authors":[],"date":"2008","doi":null,"raw":null,"cites":null},{"id":864639,"title":"Controlled MCMC for optimal sampling.","authors":[],"date":"2001","doi":null,"raw":null,"cites":null},{"id":873387,"title":"Dealing with label switching in mixture models.","authors":[],"date":"2000","doi":null,"raw":null,"cites":null},{"id":871538,"title":"Equation of state calculations by fast computing machines.","authors":[],"date":"1953","doi":"10.1063\/1.1699114","raw":null,"cites":null},{"id":872531,"title":"Examples of adaptive MCMC.","authors":[],"date":"2009","doi":null,"raw":null,"cites":null},{"id":872791,"title":"Exponential convergence of Langevin distributions and their discrete approximations.","authors":[],"date":"1996","doi":null,"raw":null,"cites":null},{"id":867790,"title":"Finite Mixture and Markov Switching Models.","authors":[],"date":"2006","doi":null,"raw":null,"cites":null},{"id":869944,"title":"Inference for Levy driven stochastic volatility models via adaptive SMC.","authors":[],"date":"2008","doi":"10.1111\/j.1467-9469.2010.00723.x","raw":null,"cites":null},{"id":869633,"title":"Interacting sequential Monte Carlo samplers for trans-dimensional simulation.","authors":[],"date":"2008","doi":null,"raw":null,"cites":null},{"id":866264,"title":"Learn from thy neighbor: Parallel-chain and regional adaptive MCMC.","authors":[],"date":"2009","doi":null,"raw":null,"cites":null},{"id":868287,"title":"Markov Chain Monte Carlo in Practice.","authors":[],"date":"1995","doi":null,"raw":null,"cites":null},{"id":868029,"title":"Markov chain Monte Carlo: Stochastic simulation for Bayesian inference","authors":[],"date":"2006","doi":null,"raw":null,"cites":null},{"id":867319,"title":"MCMC, sufficient statistics and particle filters.","authors":[],"date":"2002","doi":null,"raw":null,"cites":null},{"id":866810,"title":"Minimum variance importance sampling via population Monte Carlo.","authors":[],"date":"2005","doi":null,"raw":null,"cites":null},{"id":874186,"title":"Mixture models, Monte Carlo, Bayesian updating and dynamic models.","authors":[],"date":"1993","doi":null,"raw":null,"cites":null},{"id":869298,"title":"Monte Carlo sampling methods using Markov chains and their applications.","authors":[],"date":"1970","doi":null,"raw":null,"cites":null},{"id":868822,"title":"Novel approach to nonlinear\/non-Gaussian Bayesian state estimation.","authors":[],"date":"1993","doi":null,"raw":null,"cites":null},{"id":865155,"title":"On adaptive Markov chain Monte Carlo algorithms.","authors":[],"date":"2005","doi":null,"raw":null,"cites":null},{"id":870186,"title":"On population-based simulation for static inference.","authors":[],"date":"2007","doi":"10.1007\/s11222-007-9028-9","raw":null,"cites":null},{"id":872213,"title":"Optimal scaling for various Metropolis-Hastings algorithms.","authors":[],"date":"2001","doi":null,"raw":null,"cites":null},{"id":873084,"title":"Optimal scaling of the random walk Metropolis on elliptically symmetric unimodal targets.","authors":[],"date":"2009","doi":null,"raw":null,"cites":null},{"id":870374,"title":"Optimization by simulated annealing.","authors":[],"date":"1983","doi":"10.1126\/science.220.4598.671","raw":null,"cites":null},{"id":873651,"title":"Particle filters for state-space models with the presence of unknown static parameters.","authors":[],"date":"2002","doi":null,"raw":null,"cites":null},{"id":871325,"title":"Rejection control and sequential importance sampling.","authors":[],"date":"1998","doi":"10.2307\/2669846","raw":null,"cites":null},{"id":870441,"title":"Sequential imputations and Bayesian missing data problems.","authors":[],"date":"1994","doi":"10.2307\/2291224","raw":null,"cites":null},{"id":871096,"title":"Sequential Monte Carlo methods for dynamic systems.","authors":[],"date":"1998","doi":"10.2307\/2669847","raw":null,"cites":null},{"id":870674,"title":"Sequential Monte Carlo Methods in Practice, Chapter 10: Combined Parameter and State Estimation in Simulation-Based Filtering.","authors":[],"date":"2001","doi":null,"raw":null,"cites":null},{"id":867057,"title":"Sequential Monte Carlo Methods in Practice.","authors":[],"date":"2001","doi":null,"raw":null,"cites":null},{"id":866547,"title":"Sequential Monte Carlo samplers.","authors":[],"date":"2006","doi":null,"raw":null,"cites":null}],"documentType":{"type":0}},"contributors":[],"datePublished":"2013","abstract":"Sequential Monte Carlo (SMC) methods are not only a popular tool in the analysis of state\u2013space models, but offer an alternative to Markov chain Monte Carlo (MCMC) in situations where Bayesian inference must proceed via simulation. This paper introduces a new SMC method that uses adaptive MCMC kernels for particle dynamics. The proposed algorithm features an online stochastic optimization procedure to select the best MCMC kernel and simultaneously learn optimal tuning parameters. Theoretical results are presented that justify the approach and give guidance on how it should be implemented. Empirical results, based on analysing data from mixture models, show that the new adaptive SMC algorithm (ASMC) can both choose the best MCMC kernel, and learn an appropriate scaling for it. ASMC with a choice between kernels outperformed the adaptive MCMC algorithm of Haario et al. (1998) in 5 out of the 6 cases considered","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/68160.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/33244\/1\/fearnhead.pdf","pdfHashValue":"f3fe9f06605ed87ce3652d9b05da47059d07cc3b","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:33244<\/identifier><datestamp>\n      2018-01-24T03:00:59Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        An adaptive sequential Monte Carlo sampler<\/dc:title><dc:creator>\n        Fearnhead, Paul<\/dc:creator><dc:creator>\n        Taylor, Benjamin M.<\/dc:creator><dc:subject>\n        QA Mathematics<\/dc:subject><dc:description>\n        Sequential Monte Carlo (SMC) methods are not only a popular tool in the analysis of state\u2013space models, but offer an alternative to Markov chain Monte Carlo (MCMC) in situations where Bayesian inference must proceed via simulation. This paper introduces a new SMC method that uses adaptive MCMC kernels for particle dynamics. The proposed algorithm features an online stochastic optimization procedure to select the best MCMC kernel and simultaneously learn optimal tuning parameters. Theoretical results are presented that justify the approach and give guidance on how it should be implemented. Empirical results, based on analysing data from mixture models, show that the new adaptive SMC algorithm (ASMC) can both choose the best MCMC kernel, and learn an appropriate scaling for it. ASMC with a choice between kernels outperformed the adaptive MCMC algorithm of Haario et al. (1998) in 5 out of the 6 cases considered.<\/dc:description><dc:date>\n        2013<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/33244\/1\/fearnhead.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1214\/13-BA814<\/dc:relation><dc:identifier>\n        Fearnhead, Paul and Taylor, Benjamin M. (2013) An adaptive sequential Monte Carlo sampler. Bayesian Analysis, 8 (2). pp. 411-438. ISSN 1931-6690<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/33244\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1214\/13-BA814","http:\/\/eprints.lancs.ac.uk\/33244\/"],"year":2013,"topics":["QA Mathematics"],"subject":["Journal Article","PeerReviewed"],"fullText":"Bayesian Analysis (2013) 8, Number 2, pp. 411{438\nAn Adaptive Sequential Monte Carlo Sampler\nPaul Fearnhead * and Benjamin M. Taylor \u0084\nAbstract. Sequential Monte Carlo (SMC) methods are not only a popular tool\nin the analysis of state{space models, but o\u000ber an alternative to Markov chain\nMonte Carlo (MCMC) in situations where Bayesian inference must proceed via\nsimulation. This paper introduces a new SMC method that uses adaptive MCMC\nkernels for particle dynamics. The proposed algorithm features an online stochas-\ntic optimization procedure to select the best MCMC kernel and simultaneously\nlearn optimal tuning parameters. Theoretical results are presented that justify\nthe approach and give guidance on how it should be implemented. Empirical re-\nsults, based on analysing data from mixture models, show that the new adaptive\nSMC algorithm (ASMC) can both choose the best MCMC kernel, and learn an\nappropriate scaling for it. ASMC with a choice between kernels outperformed\nthe adaptive MCMC algorithm of Haario et al. (1998) in 5 out of the 6 cases\nconsidered.\nKeywords: Adaptive MCMC, Adaptive Sequential Monte Carlo, Bayesian Mixture\nAnalysis, Optimal Scaling, Stochastic Optimization\n1 Introduction\nSequential Monte Carlo (SMC) is a class of algorithms that enable simulation from\na target distribution of interest. These algorithms are based on de\fning a series of\ndistributions, and generating samples from each distribution in turn. SMC was initially\nused in the analysis of state-space models. In this setting there is a time{evolving hidden\nstate of interest, inference about which is based on a set of noisy observations (Gordon\net al. 1993; Liu and Chen 1998; Doucet et al. 2001; Fearnhead 2002). The sequence of\ndistributions is de\fned as the set of posterior distributions of the state at consecutive\ntime-points given the observations up to those time points. More recent work has looked\nat developing SMC methods that can analyse state-space models which have unknown\n\fxed parameters. Such methods introduce steps into the algorithm to allow the support\nof the sample of parameter values to change over time, for example by using ideas from\nkernel density estimation (Liu and West 2001), or Markov chain Monte Carlo (MCMC)\nmoves (Gilks and Berzuini 1999; Storvik 2002; Fearnhead 2002).\nMost recently, SMC methods have been applied as an alternative to MCMC for\nstandard Bayesian inference problems (Neal 2001; Chopin 2002; Del Moral et al. 2006;\nFearnhead 2008). In this paper the focus will be on methods for sampling from the\nposterior distribution of a set of parameters of interest. SMC methods for this class of\ntargets introduce an arti\fcial sequence of distributions running from the prior to the\n*Department of Mathematics and Statistics, Lancaster University, UK. p.fearnhead@lancaster.ac.uk\n\u0084(Corresponding Author) Faculty of Health and Medicine, Lancaster University, UK.\nb.taylor1@lancaster.ac.uk\n\u00a9 2013 International Society for Bayesian Analysis DOI:10.1214\/13-BA814\n412 An Adaptive Sequential Monte Carlo Sampler\nposterior, and sample recursively from these using a combination of Importance Sam-\npling and MCMC moves. This approach to sampling has been demonstrated empirically\nto often be more e\u000bective than using a single MCMC chain (Jasra et al. 2007, 2008a).\nThere are heuristic reasons for why this may true in general: the annealing of the target\nand spread of samples over the support means that SMC is less likely to be become\ntrapped in posterior modes.\nSimply invoking an untuned MCMC move within an SMC algorithm would likely\nlead to poor results because the move step would not be e\u000bective in combating sample\ndepletion. The structure of SMC means that at the time of a move there is a sample\nfrom the target readily available, this can be used to compute posterior moments and\ninform the shape of the proposal kernel as in Jasra et al. (2008b); however, further\nre\fnements can lead to even better performance. Such re\fnements include the scaling\nof estimated target moments by an optimal factor, see Roberts and Rosenthal (2001)\nfor example. For general targets and proposals no theoretical results for the choice of\nscaling exist, and this has led to the recent popularity of adaptive MCMC (Haario et al.\n1998; Andrieu and Robert 2001; Roberts and Rosenthal 2009; Craiu et al. 2009; Andrieu\nand Thoms 2008). In this paper the idea of adapting the MCMC kernel within an SMC\nalgorithm will be explored.\nTo date there has been little work at adapting SMC methods. Exceptions include the\nmethod of Jasra et al. (2008b), whose method assumes a likelihood tempered sequence\nof target densities (see Neal (2001)) and the adaptation procedure both chooses this\nsequence online and computes the variance of a random walk proposal kernel used\nfor particle dynamics. Cornebise et al. (2008) also considers adapting the proposal\ndistribution within SMC for state-space models. Assuming that the proposal density\nbelongs to a parametric family with parameter \u0012, their method proceeds by simulating\na number of realisations for each of a range of values of \u0012 and selecting the value that\nminimises the empirical Shannon entropy of the importance weights; new samples are\nthen re{proposed using this approximately optimal value. Further related work includes\nthat of Douc et al. (2007) and Capp\u0013e et al. (2008) on respectively population Monte\nCarlo and adaptive importance sampling and also Schafer and Chopin (2013).\nThe aims of this paper are to introduce a new adaptive SMC algorithm (ASMC)\nthat automatically tunes MCMC move kernels and chooses between di\u000berent proposal\ndensities and to provide theoretical justi\fcation of the method. The algorithm is based\non having a distribution of kernels and their tuning parameters at each iteration. Each\ncurrent sample value, called a particle, is moved using an MCMC kernel drawn from\nthis distribution. By observing the expected square jumping distance (Craiu et al. 2009;\nSherlock and Roberts 2009) for each particle it is possible to learn in some sense which\nMCMC kernels are mixing better. The information thus obtained can then be used to\nupdate the distribution of kernels. The key assumption of the new approach is that the\noptimal MCMC kernel for moving particles does not change much over the iterations\nof the SMC algorithm; we note that this assumption is more intrinsic to SMC methods\nand not con\fned to our proposed algorithm. As will be discussed and shown empirically\nin Section 5, this can often be achieved by appropriate parameterisation of a family of\nMCMC kernels.\nP. Fearnhead and Benjamin M. Taylor 413\nThe structure of the paper is as follows. In the next section, the model of interest\nwill be introduced and followed by a review of MCMC and SMC approaches. Then in\nSection 3, the new adaptive SMC will be presented. Guidelines on implementing the\nalgorithm as well as some theory on the convergence will be presented in Section 4. In\nSection 5 the method will be evaluated using simulated data. The results show that\nthe proposed method can successfully choose both an appropriate MCMC kernel and\nan appropriate scaling for the kernel. The paper ends with a discussion.\n2 Model\nThe focus of this article will be on Bayesian inference for parameters, \u0012, from a model\nwhere independent identically distributed data is available. Note that the ideas behind\nthe proposed adaptive SMC algorithm can also be applied to the non i.i.d. case, see\nSection 6. Let \u0019(\u0012) denote the prior for \u0012 and \u0019(yj\u0012) the probability density for the\nobservations. The aim will be to calculate the posterior density,\n\u0019(\u0012jy1:n) \/ \u0019(\u0012)\nnY\ni=1\n\u0019(yij\u0012); (1)\nwhere, here and throughout, \u0019 will be used to denote a probability density, and y1:n\nmeans y1; : : : ; yn.\nIn general, \u0019(\u0012jy1:n) is analytically intractable and so to compute posterior func-\ntionals of interest, for example expectations, Monte Carlo simulation methods are often\nemployed. Sections 2.1 and 2.2 provide a brief description of two such Monte Carlo\napproaches.\n2.1 MCMC\nAn MCMC transition kernel, Kh, is a probability law governing the transition between\nstates of a discrete Markov chain with some stationary distribution of interest, for\nexample a posterior. Kh comprises a proposal kernel, here and throughout denoted\nqh (the subscript h indicates dependence on a tuning parameter) and an acceptance\nratio that depends on the target and, in general, the proposal densities (see Gilks et al.\n(1995); Gamerman and Lopes (2006) for reviews of MCMC methodology). The most\ngenerally applicable MCMC method is Metropolis{Hastings, described in Algorithm 1\n(Metropolis et al. 1953; Hastings 1970).\nProbably the simplest MH algorithm is random walk Metropolis (RWM). The pro-\nposal kernel for RWM is a symmetric density centred on the current state, the most\ncommon example being a multivariate normal, qh(\u0012\n(i\u00001); ~\u0012) = N (~\u0012; \u0012(i\u00001); h2\u0006^\u0019), where\n\u0006^\u0019 is an estimate of the target covariance. Both the values of \u0006^\u0019 and h are critical\nto the performance of the algorithm. If \u0006^\u0019 does not accurately estimate the posterior\ncovariance matrix, then the likely directions of the random walk moves will probably\nbe inappropriate. On the other hand, a value of h that is too small will lead to high\nacceptance rates, but the samples will be highly correlated. If h is too large then the\n414 An Adaptive Sequential Monte Carlo Sampler\nAlgorithm 1 Metropolis{Hastings Algorithm (Metropolis et al. 1953; Hastings 1970)\n1: Start with an initial sample, \u0012(0), drawn from any density, \u00190.\n2: for j = 1; 2; : : : do\n3: Propose a move to a new location, ~\u0012, by drawing a sample from qh(\u0012\n(i\u00001); ~\u0012).\n4: Accept the move (i.e., set \u0012(i) = ~\u0012) with probability,\nmin\n(\n1;\n\u0019(~\u0012jy1:n)\n\u0019(\u0012(i\u00001)jy1:n)\nqh(~\u0012; \u0012\n(i\u00001))\nqh(\u0012(i\u00001); ~\u0012)\n)\n; (2)\nelse set \u0012(i) = \u0012(i\u00001).\n5: end for\nalgorithm will rarely move, which in the worst case scenario could lead to a degenerate\nsample.\nThese observations on the ro^le of h point to the idea of an optimal scaling, a h\nsomewhere between the extremes that promotes the best mixing of the algorithm. In\nthe case of elliptically symmetric unimodal targets, an optimal random walk scaling\ncan sometimes be computed numerically; this class of targets includes the Multivariate\nGaussian (Sherlock and Roberts 2009). Other theoretical results include optimal accep-\ntance rates which are derived in the limit as the dimension of \u0012, d ! 1 (see Roberts\nand Rosenthal (2001) for examples of targets and proposals). In general however, there\nare no such theoretical results.\nOne way of circumventing the need for analytical optimal scalings is to try to learn\nthem online (Andrieu and Robert 2001; Atchad\u0013e and Rosenthal 2005), this can include\nboth learning a good scaling, h, and estimating the target covariance, \u0006^\u0019 (Haario et al.\n1998). Recent research in adaptive MCMC has generated a number of new algorithms\n(see for example Andrieu and Thoms (2008); Roberts and Rosenthal (2009); Craiu\net al. (2009)), though some care must be taken to ensure that the resulting chain has\nthe correct ergodic distribution.\n2.2 Sequential Monte Carlo\nSequential Monte Carlo (SMC) is a class of simulation{based methods for sampling\nfrom a target density of interest (see Doucet et al. (2001); Del Moral et al. (2006)\nfor a review). The main idea behind SMC is to introduce a sequence of densities\nleading from the prior to the target density of interest and to iteratively update an\napproximation to these densities. For the application considered here, it is natural to\nde\fne these densities as \u0019t(\u0012) = \u0019(\u0012jy1:t) for t = 1; : : : ; n; this `data tempered' schedule\nwill be used in the sequel. The approximations to each density are de\fned in terms of a\ncollection of particles, \u0012\n(j)\nt , together with their respective weights, w\n(j)\nt , for j = 1; : : : ;M ,\nproduced so that asM !1, Monte Carlo sums converge almost surely to their `correct'\nP. Fearnhead and Benjamin M. Taylor 415\nexpectations: PM\nj=1 w\n(j)\nt \u0010(\u0012\n(j)\nt )PM\ni=1 w\n(i)\nt\na.s.\u0000! E\u0019t(\u0012t)[\u0010(\u0012t)];\nfor all \u0019t{integrable functions, \u0010. Such a collection of particles and their respective\nweights will be denoted by f\u0012(j)t ; w(j)t gMj=1; notice that we do not assume the sum of\nthe weights is equal to one. Furthermore, we will often write f\u0012(j)t ; w(j)t gMj=1 \u0018 \u0019t(\u0012t) or\nf\u0012(j)t ; w(j)t gMj=1 \u0018 \u0019t to make this explicit. When each particle, or sample, has weight\n1=M , we will sometimes write f\u0012(j)t ; 1=MgMj=1 and other times f\u0012(j)t gMj=1.\nOne step of an SMC algorithm can involve importance reweighting, resampling and\nmoving the particles via an MCMC kernel (Gilks and Berzuini 1999; Chopin 2002). For\nconcreteness, this paper will focus on the iterated batch importance sampling (IBIS)\nalgorithm of Chopin (2002).\nThe simplest way to update the particle approximation in model (1) is to let \u0012\n(j)\nt =\n\u0012\n(j)\nt\u00001 and w\n(j)\nt = w\n(j)\nt\u00001\u0019(ytj\u0012(j)t ). However such an algorithm will degenerate for large t, as\neventually only one particle will have non-negligible weight. With IBIS, resample{move\nsteps (sometimes referred to here as simply `move steps') are introduced to alleviate\nthis. In a move step, the particles are \frst resampled so that the expected number of\ncopies of particle \u0012\n(j)\nt is proportional to w\n(j)\nt . This process produces multiple copies of\nsome particles. In order to create particle diversity, each resampled particle is moved\nby an MCMC kernel. The MCMC kernel is chosen to have stationary distribution \u0019t.\nThe resulting particles are then assigned a weight of 1=M .\nThe decision of whether to apply a resample-move step within IBIS is based on the\ne\u000bective sample size (ESS, see Kong et al. (1994); Liu and Chen (1998)). The ESS is a\nmeasure of variability of the particle weights; using this to decide whether to resample\nis justi\fed by arguments within Liu and Chen (1995) and Liu et al. (1998). Full details\nof IBIS are given in Algorithm 2.\nWhilst the focus of this article is on the IBIS algorithm, the ideas presented here\ncan be applied to more general SMC algorithms (Del Moral et al. 2006). The use\nof an MCMC-Kernel move within IBIS is itself probably the most common approach\nfor implementing SMC in practice (and corresponds to the implementation described\nin Section 3.3.2.3 of Del Moral et al. (2006)). Furthermore, the adaptive method de-\nveloped in Section 3 immediately applies to algorithms that use a di\u000berent sequence\nof target distributions, for example likelihood tempering (Neal 2001). The likelihood\ntempered target sequence is \u0019t(\u0012) = \u0019(\u0012)\u0019(y1:nj\u0012)\u0018t , where f\u0018tg is a sequence of real\nnumbers starting at 0 (the prior) and ending on 1 (the posterior). The focus here is\non data tempering, \u0019t(\u0012) = \u0019(\u0012jy1:t), because for the application considered in Section\n5.2, calculating the likelihood has a cost which increases linearly with the number of\nobservations.\nThe SMC algorithm of Del Moral et al. (2006) also implements moves at each iter-\nation of the algorithm. However the approach in this article is closely related, and is\n416 An Adaptive Sequential Monte Carlo Sampler\nAlgorithm 2 Chopin's IBIS algorithm\n1: Initialise from the prior f\u0012(j)0 ; w(j)0 gMj=1 \u0018 \u00190.\n2: for t = 1; : : : ; n do\n3: Reweight w\n(j)\nt = w\n(j)\nt\u00001\u0019t(\u0012\n(j)\nt\u00001)=\u0019t\u00001(\u0012\n(j)\nt\u00001). Result: f\u0012(j)t\u00001; w(j)t gMj=1 \u0018 \u0019t.\n4: if particle weights not degenerate (see text) then\n5: f\u0012(j)t ; w(j)t gMj=1  f\u0012(j)t\u00001; w(j)t\u00001gMj=1\n6: t! t+ 1.\n7: else\n8: Resample: let K = fk1; : : : ; kMg \u0012 f1; : : : ;Mg be the resampling indices, then\nf\u0012(k)t\u00001; 1=Mgk2K \u0018 \u0019t. Relabel: kj  j, the jth resampling index so that\nf\u0012(j)t\u00001; 1=MgMj=1 \u0018 \u0019t.\n9: Move via \u0019t{invariant MCMC kernel. Result: f\u0012(j)t ; 1=MgMj=1 \u0018 \u0019t.\n10: end if\n11: end for\nequivalent to using a sequence of targets, ~\u0019j(\u0012) = \u0019(\u0012jy1:tj ), where tj is the iteration\nwhere IBIS resamples for the jth time. IBIS is thus similar to SMC, with ~\u0019j(\u0012) as\nthe sequence of targets. As such, IBIS itself can be viewed as choosing the sequence\nof targets in an adaptive way based on the closeness of successive targets, ~\u0019j\u00001(\u0012) and\n~\u0019j(\u0012), as measured by the variability of the importance weights (Del Moral et al. 2010).\nThe e\u000eciency of an algorithm such as IBIS depends on the mixing properties of\nthe associated MCMC kernel. Within SMC there is the advantage of being able to use\nthe current set of particles to help tune an MCMC kernel. For example, the weighted\nparticles can give an estimate of the posterior covariance matrix, which can be used\nwithin a random walk proposal. However even in this case, the proposal variance still\nneeds to be appropriately scaled (Roberts and Rosenthal 2001; Sherlock and Roberts\n2009). In the next section the new adaptive SMC procedure will be introduced. The\nnew algorithm can learn an appropriate tuning for the MCMC kernel, and can also be\nused to choose between a set of possible kernels.\n3 The Adaptive SMC Sampler\nFirst consider the case where the move step in the IBIS algorithm involves one type of\nMCMC kernel. Let \u0019t be an arbitrary continuous probability density (the target) and\nKh;t a \u0019t{invariant MCMC kernel with tuning parameter, h. The parameter h is to be\nchosen to maximise the following utility function,\ng(t)(h) =\nZ\n\u0019t(\u0012t\u00001)Kh;t(\u0012t\u00001; \u0012t)\u0003(\u0012t\u00001; \u0012t)d\u0012t\u00001d\u0012t; (3)\n= E [\u0003(\u0012t\u00001; \u0012t)] ;\nwhere \u0003(\u0012t\u00001; \u0012t) > 0 is a measure of mixing of the chain. Most MCMC adaptation\ncriteria can be viewed in this way (Andrieu and Thoms 2008). Note that for simplicity\nP. Fearnhead and Benjamin M. Taylor 417\nof presentation, \u0003 only depends on the current and subsequent state, though the idea\nreadily extends to more complex cost functionals, for example involving multiple tran-\nsitions of the MCMC chain. The function g(t) is the average performance of the chain\nwith respect to \u0003, which would normally be some measure of mixing. A computationally\nsimple measure of mixing is the expected square jumping distance (ESJD). Maximising\nthe ESJD is equivalent to minimising the lag-1 autocorrelation; this measure is often\nused within adaptive MCMC, see for example Sherlock and Roberts (2009) and Pasarica\nand Gelman (2010).\nIn the following it will be assumed that the proposal distribution can depend on\nquantities calculated from the current set of particles (for example estimates of the\nposterior variance), but this will be suppressed in the notation. The main idea of\nASMC is to use the observed instances of \u0003(\u0012t\u00001; \u0012t) to help choose the best h. The\ntuning parameter will be treated as an auxiliary random variable. At time-step t the\naim is to derive a density for the tunings, \u0019(t)(h); note this should not be confused with\nthe target densities on the parameters, \u0019t(\u0012). If a move step is invoked at this time, a\nsample of M realisations from \u0019(t)(h), denoted fh(j)t gMj=1, will be drawn and `allocated'\nto particles at random.\nWhen moving the jth resampled particle, the tuning parameter h\n(j)\nt will be used\nwithin the proposal distribution. Let \u0012\n(j)\nt\u00001 be the jth resampled particle (see step 8 of\nAlgorithm 2). In moving this particle, ~\u0012\n(j)\nt is drawn from qh(j)t\n(\u0012\n(j)\nt\u00001; \u0001 ), and accepted\nwith probability \u000b\nh\n(j)\nt\n(\u0012\n(j)\nt\u00001; ~\u0012\n(j)\nt ), given by (2). If the proposed particle is accepted then\n\u0012\n(j)\nt =\n~\u0012\n(j)\nt otherwise \u0012\n(j)\nt = \u0012\n(j)\nt\u00001.\nWe recommend that low-variance resampling methods such as residual, or strati\fed\nsampling be used for resampling both the particles as well as the tunings (Whitley 1994;\nKitagawa 1996; Liu et al. 1998; Carpenter et al. 1999).\nIn practice, we use a Rao-Blackwellised, unbiased estimate of the utility function,\ng(h\n(j)\nt ),\n~\u0003(\u0012\n(j)\nt\u00001; ~\u0012\n(j)\nt ) = \u000bh(j)t\n(\u0012\n(j)\nt\u00001; ~\u0012\n(j)\nt )\u0003(\u0012\n(j)\nt\u00001; ~\u0012\n(j)\nt ): (4)\nThe approach in this paper is to use the observed ~\u0003(\u0012\n(j)\nt\u00001; ~\u0012\n(j)\nt ) to update the distri-\nbution \u0019(t)(h) to a new distribution \u0019(t+1)(h) in a way that moves towards values of h\nwith a higher ESJD. In particular each h\n(j)\nt will be assigned a weight, f(~\u0003(\u0012\n(j)\nt\u00001; ~\u0012\n(j)\nt )),\nfor some function f : R+ ! R+. The new density of scalings will be de\fned,\n\u0019(t+1)(h) \/\nMX\nj=1\nf(~\u0003(\u0012\n(j)\nt\u00001; ~\u0012\n(j)\nt ))R(h\u0000 h(j)t ); (5)\nwhere R(h\u0000h(j)t ) is a density for h which is centred on h(j)t . Simulating from \u0019(t+1)(h)\nis achieved by \frst resampling the h\n(j)\nt s with probabilities proportional to their weight\nand then adding noise to each resampled value; the distribution of this noise is given\n418 An Adaptive Sequential Monte Carlo Sampler\nby R( \u0001 ). If there is no resampling at step t then \u0019(t+1)(h) = \u0019(t)(h). In practice, the\nscheme can be initiated with an arbitrary distribution \u0019(h).\nIn computing a new density of the tunings, a function f will be used to weight the\nh\n(j)\nt s. The function f should be increasing with ~\u0003, so that more weight is placed on\ntunings that produce bigger moves. The speci\fc choice of f considered in this paper is\na simple linear weighting scheme,\nf(~\u0003) = a+ ~\u0003; a \u0015 0: (6)\nTheoretical justi\fcation for this choice is given in the next section. This approach is\nsimilar in spirit to that of genetic algorithms (Jennison and Sheehan 1995).\nThe motivation for adding noise to the resampled h{values is to avoid the distri-\nbutions \u0019(t)(h) degenerating too quickly to a point-mass on a single value. It can be\nviewed as a form of kernel density estimation, and as such it is natural to allow the\nvariance of the noise to depend on the variance of \u0019(t)(h) and the number of particles,\nM . Asymptotic results for kernel density estimation suggest that this variance should\ndecrease to 0 as M increases. Similar ideas are used in dynamic SMC methods for\ndealing with \fxed parameters, for example West (1993); Liu and West (2001).\nAn initial collection of tuning parameters is drawn from a density, \u0019(0)(h). In the\nexamples considered here, this density was taken to be a uniform density on an ap-\npropriate support. For adaptive random walk kernels, an appropriate support may be\nconstructed quickly by using ideas similar to those in Section 5.1 to compute g(h) (de-\n\fned in (3)) for a Gaussian of the appropriate dimension, and then choosing a wide\ninterval around the mode.\nOne assumption of the proposed approach is that a good choice of h at one time-step\nwill be a good choice at nearby time-steps. Note that this is based on an implicit as-\nsumption within SMC that successive targets are similar (see Chopin (2002); Del Moral\net al. (2006) for example). Furthermore, using estimates of posterior variances within\nthe proposal distribution can also help ensure that good values of h at one time-step will\nbe a good choice at nearby time-steps. Some theoretical results concerning this matter\nwill be presented in Section 4. We note that for improved performance, but at an ad-\nditional computational cost, an MCMC move step could be applied at any iteration of\nthe algorithm.\nTo choose between di\u000berent types of MCMC kernel is now a relatively straight-\nforward extension of the above. Assume there are nK di\u000berent MCMC kernels, each\nde\fned by a proposal distribution qh;i, where i 2 f1; : : : ; nKg. Instead of just resam-\npling the tuning parameters after the particle resampling step, now both the kernels and\ntheir associated parameters are resampled. The algorithm learns a set of distributions,\n\u0019(t)(h; i), for the pair of kernel type and associated tuning parameter. Each particle is\nassigned a random kernel type and tuning drawn from this distribution, with the pair,\n(h\n(j)\nt\u00001; i\n(j)\nt\u00001), associated with \u0012\n(j)\nt\u00001. The algorithm proceeds by weighting this pair based\nP. Fearnhead and Benjamin M. Taylor 419\non the observed ~\u0003(\u0012\n(j)\nt\u00001; ~\u0012\n(j)\nt ) values as before, and updating the distribution,\n\u0019(t)(h; i) \/\nMX\nj=1\nf(~\u0003(\u0012\n(j)\nt\u00001; ~\u0012\n(j)\nt ))R(h\u0000 h(j)t\u00001)\u000ei(j)t\u00001(i) (7)\nwhere \u000e\ni\n(j)\nt\u00001\n(i) is a point mass on i = i\n(j)\nt\u00001.\nThe method is described in detail below, see Algorithm 3. Within the speci\fc im-\nplementation described, the pairs, f(h(j)t ; i(j)t )gMj=1, sampled from \u0019(t)(h; i) are allocated\nto particles randomly immediately after the resample{move step at iteration t. These\npairs are then kept until the next time a resample{move step is called.\nAlgorithm 3 The Adaptive SMC algorithm. Here, \u00190( \u0001 ); : : : ; \u0019n( \u0001 ) are an arbitrary\nsequence of targets; an MCMC kernel is assumed for particle dynamics.\n1: Initialise from the prior f\u0012(j)0 ; w(j)0 gMj=1 \u0018 \u00190.\n2: Draw a selection of pairs of MCMC kernels with associated tuning parameters,\nf(h(j)0 ;K(j)h;0)gMj=1 \u0011 f(h(j)0 ; i(j)0 )gMj=1 \u0018 \u0019(h; i), and attach one to each particle arbi-\ntrarily.\n3: for t = 1; : : : ; n do\n4: Reweight w\n(j)\nt = w\n(j)\nt\u00001\u0019t(\u0012\n(j)\nt\u00001)=\u0019t\u00001(\u0012\n(j)\nt\u00001). Result: f\u0012(j)t\u00001; w(j)t gMj=1 \u0018 \u0019t.\n5: if particle weights not degenerate (see text) then\n6: f\u0012(j)t ; w(j)t gMj=1  f\u0012(j)t\u00001; w(j)t\u00001gMj=1\n7: f(h(j)t ;K(j)h;t)gMj=1  f(h(j)t\u00001;K(j)h;t\u00001)gMj=1\n8: t! t+ 1.\n9: else\n10: Resample: let K = fk1; : : : ; kMg \u0012 f1; : : : ;Mg be the resampling indices, then\nf\u0012(k)t\u00001; 1=Mgk2K \u0018 \u0019t. Relabel: kj  j, the jth resampling index so that\nf\u0012(j)t\u00001; 1=MgMj=1 \u0018 \u0019t. DO NOT resample kernels or tuning parameters at this\nstage.\n11: Move \u0012\n(j)\nt\u00001 via the \u0019t{invariant MCMC kernel, K\n(j)\nh;t , and tuning parameter\nh\n(j)\nt\u00001, denote the proposed new particle as ~\u0012\n(j)\nt and accepted\/rejected particle\nas \u0012\n(j)\nt . Result: f\u0012(j)t ; 1=MgMj=1 \u0018 \u0019t.\n12: To obtain f(h(j)t ;K(j)h;t)gMk=1 \u0011 f(h(j)t ; i(j)t )g, sample M times from (7). Allocate\nthe new selection to particles at random.\n13: end if\n14: end for\nThe new method treats the tuning parameters as auxiliary random variables, but\nthis is not the only way to choose good tuning parameters. One option would be to\ndirectly optimise g(t)(h) at each iteration, however this function is typically intractable\n(except for a small subset of targets and MCMC kernels). Another option would be to\nuse stochastic optimisation at each move step in the style of Andrieu and Thoms (2008).\n420 An Adaptive Sequential Monte Carlo Sampler\n4 Theoretical Results\nIn this section the proposed algorithm will be justi\fed by a series of theoretical results;\nguidance as to how it should best be implemented will also be given. The results\npresented here apply in the limit as the number of particles, M !1.\nWe assume a multivariate tuning parameter, h, and that there are \fnitely many\ntypes of MCMC kernel to choose from. In this section, we further assume that the\nvariance of the kernel R( \u0001 ) in (5) is 0. Lastly, we assume that the choice of MCMC\nkernels can be represented as a random variable I, taking values in a \fnite set, i 2 I :=\nfi1; : : : ; inKg. The extension to a countable choice of kernels is trivial, but irrelevant\nfrom a practical point of view. For notational simplicity we assume h can take values\nin the same set H regardless of the value i, though generalising this is trivial.\nFor a slight notational simpli\fcation, the criterion \u0003 will be used, rather than ~\u0003\n(as suggested in algorithm 3); this does not a\u000bect the validity of any of the arguments,\nwhich also hold for ~\u0003. The section is split into two parts.\nFirstly, in section 4.1, it is of interest to examine what happens to the distribution\nof (h; i)s after one step of reweighting and resampling; this result will lead to a criterion\nfor the choice of weight function that guarantees MCMC mixing improvement with\nrespect to \u0003. In section 4.2, the sequential improvement of (h; i)s will be considered\nover many steps of the ASMC algorithm and with a changing target. General conditions\nfor convergence of ASMC to the optimal kernel and tuning parameter will be provided.\n4.1 One Step Improvement and Weighting Function\nIn this section and in the relevant proofs, it is appropriate to temporarily drop the t\nsuperscript, e.g., g(t) \u0011 g, h(j)t \u0011 h(j), \u0012t\u00001 \u0011 \u0012, \u0012t \u0011 \u00120, and \u0019t\u00001(\u0012) \u0011 \u0019(\u0012). For a\ngiven number of particles M we will have that the \u0012 are drawn from a density \u0019[M ](\u0012)\nwhich is the SMC approximation to \u0019(\u0012).\nTo study the e\u000bect of reweighting and resampling on the distribution of the hs and\nis, suppose that currently f(h(j); i(j))gMj=1 iid\u0018 \u0019(HjI)\u0019(I), the joint pdf of a random\nvariable, (H; I). The weight attached to any pair (h; i) is random. Therefore, since\nthese pairs are assigned to particles independently of the value of the particle, the weight\nhas mean,\nw[M ](h; i) =\nZ\n\u0019[M ](\u0012)K\n(i)\nh (\u0012; \u0012\n0)f(\u0003(\u0012; \u00120))d\u0012d\u00120:\nNow if we de\fne a weight based on the conditional expection of f(\u0003) when \u0012 \u0018 \u0019(\u0012):\nw(h; i) = E\u0002;\u00020jH;I [f(\u0003)jH = h; I = i] =\nZ\n\u0019(\u0012)K\n(i)\nh (\u0012; \u0012\n0)f(\u0003(\u0012; \u00120))d\u0012d\u00120 (8)\nthen standard SMC results give that under regularity conditions, we will have that as\nM ! 1 that w[M ](h; i) ! w(h; i) in probability, see Crisan (2001) and Del Moral\n(2004).\nP. Fearnhead and Benjamin M. Taylor 421\nThe following proposition, which is used repeatedly in subsequent results, shows how\nreweighting and resampling a\u000bects \u0019(h; i).\nProposition 1. Suppose \u0012 \u0018 \u0019[M ](\u0012) and f(h(j); i(j))gMj=1 iid\u0018 \u0019(HjI)\u0019(I), the joint pdf\nof a random variable, (H; I), independent of \u0012. Let w(h; i) be the weighting function\nde\fned as in (8). Assume as M !1 we have w[M ](h; i)! w(h; i) in probability; also\nsuppose that\nP\ni2I\nR\nH w(h; i)\u0019(h; i)dh > 0 and is \fnite.\nThen in the limit as M ! 1, the distribution of the reweighted and subsequently\nresampled (h; i)s is,\n\u0019?(h; i) =\nw(h; i)\u0019(h; i)P\ni2I\nR\nH w(h; i)\u0019(h; i)dh\n: (9)\nProof: See Appendix 1. 2\nSince ASMC uses a selection of kernels each with a selection of hs, it is appropriate\nas a starting point to look for conditions under which their distribution is improved.\nIt would be desirable if, over \u0019?(h; i), the objective function would on average take a\nhigher value, for then the new distribution would on average perform better with respect\nto \u0003 than the old. This criterion can be stated in mathematical form: conditions on f\nare sought for which,X\ni2I\nZ\nH\n\u0019?(h; i)g(h; i)dh \u0015\nX\ni2I\nZ\nH\n\u0019(h; i)g(h; i)dh\nwhere\ng(h; i) =\nZ\n\u0019(\u0012)K\n(i)\nh (\u0012; \u0012\n0)\u0003(\u0012; \u00120)d\u0012d\u00120:\nLemma 1. Assuming g is \u0019(h; i){integrable, in the limit as M !1,\nE\u0019?(h;i)[g(h; i)] \u0015 E\u0019(h;i)[g(h; i)]\n() cov\u0019(h;i)[g(h; i); w(h; i)] \u0015 0:\n(10)\nThat is, provided there is positive correlation between the objective function g(h; i) and\nthe weighting function, w(h; i), the new distribution of (h; i)s will on average perform\nbetter (on g(h; i)) with respect to \u0003 than the old.\nProof: The result is obtained by expanding de\fnitions in (10):\nE\u0019?(h;i)[g(h; i)] \u0015 E\u0019(h;i)[g(h; i)];\n() E\u0019(h;i)[w(h; i)g(h; i)] \u0015 E\u0019(h;i)[w(h; i)]E\u0019(h;i)[g(h; i)];\n() cov\u0019(h;i)[g(h; i); w(h; i)] \u0015 0:\n2\nAlthough this result does not directly yield a general form for f , it does give a simple\ncriterion that must be ful\flled by any candidate function. An immediate corollary gives\nmore concrete guidance:\n422 An Adaptive Sequential Monte Carlo Sampler\nCorollary 1. A linear weighting scheme, f(\u0003) = a+ \u0003, where a \u0015 0, satis\fes (10).\nProof: This is trivially veri\fed using the linearity property of the covariance. 2\nA consequence of this lemma is that the ASMC algorithm with linear weights will\nlead to sequential improvement with respect to \u0003 under very weak assumptions on the\ntarget and initial density for (h; i). A linear weighting scheme may at \frst glance seem\nsub{optimal, and that it should be possible to learn (h; i) more quickly using a function\nf(\u0003) that increases at a super{linear rate. It is conjectured that such functions will not\nalways guarantee an improvement in the distribution of (h; i). For example consider\nf(\u0003) = \u00032, where the weighting function takes the form, w(h; i) = g(h; i)2 + V[\u0003jH =\nh; I = i]. Because of the V[\u0003jH = h; I = i] term, which may be large for values of (h; i)\nwhere g(h; i) is small, it is no longer true that cov\u0019(h;i)[g(h; i); w(h; i)] \u0015 0 in general.\n4.2 Convergence Over a Number of Iterations\nThe goal of this section is to provide a theoretical result concerning the ability of\nASMC to update the distribution of (h; i)s with respect to a sequence of targets,\n\u00191(\u00121); : : : ; \u0019n(\u0012n). To simplify notation, it will be assumed that a move occurs at\neach iteration of the algorithm. The result can be extended to the case where moves\noccur intermittently, providing they incur in\fnitely often in the limit as the number of\ndata points goes to in\fnity.\nDe\fne a set of functions, fg(t)(h; i) : A ! R\u00150gnt=1,\ng(t)(h; i) =\nZ\n\u0019t(\u0012t\u00001)K\n(i)\nh;t(\u0012t\u00001; \u0012t)\u0003(\u0012t\u00001; \u0012t)d\u0012t\u00001d\u0012t;\nwhere A = H\u0002 I; and for each t, Kh;t is a \u0019t{invariant MCMC kernel. We assume for\neach (h; i) 2 A that g(t)(h; i) is integrable for all t.\nFor a linear weighting scheme,\n\u0019(t)(h; i) \/ \u0019(h; i)\ntY\ns=1\n(a+ g(s)(h; i)):\nBelow it will be shown that as t!1 if the sequence of functions, fg(t)(h; i)g, converges\nquickly enough to a \fxed function, g(h; i), and if g has a unique global maximum,\n(hopt; iopt), then \u0019\n(t)(h) will converge to a point mass on (hopt; iopt).\nThe main assumption of this theorem regards the convergence of the sequence of\nfunctions fg(t)(h; i)g, condition (11) below. The existence of a limiting g(h; i) informs\nthe choice of parameterisation for the MCMC kernel. Though this assumption may seem\nrestrictive, the key to understanding the utility of the theorem in practice is that the\nproposal should be adapted to suit the changing target in some way independent of the\ntuning parameter. For example, standard Bayesian asymptotics (Ghosal 1999) suggest\nthat if \u00120 is the true parameter value and \u0006t is the posterior variance after t observations,\nthen the posterior for \u0006\n\u00001=2\nt (\u0012t\u0000 \u00120) will converge to a standard Gaussian distribution.\nP. Fearnhead and Benjamin M. Taylor 423\nIn a random walk kernel, for example, the variance should therefore be parameterised\nas h2\u0006^t, where \u0006^t is an estimate of the posterior variance given t observations. This\nchoice of parametrisation should mean that g(t)(h) converges to the the expected square\njump distance for a standard Gaussian target, given RWM with proposal variance hId,\nwhere Id is the d\u0002 d identity matrix. The assumption is also linked to the idea that a\ngood value of (h; i) for the target at time t is required to be a good value at times later\non. As mentioned above, the motivation behind SMC is that successive targets should\nbe similar. These issues will be explored empirically in the next section.\nTheorem 1. Let \u0019(h; i) = \u0019(hji)\u0019(i) be the initial density for the tuning parameter\nwith support A = H \u0002 I and a > 0. De\fne, as above,\n\u0019(t)(h; i) \/ \u0019(h; i)\ntY\ns=1\n(a+ g(s)(h; i)):\nSuppose there exists a function (random variable) g : A ! R\u00150 such that\nsup\nA\njg(t) \u0000 gj \u0014 kgt\u0000\u000b; \u000b 2 (0; 1); kg 2 R>0: (11)\nFurthermore, suppose g has a unique global maximum, (hopt; iopt) 2 A, with the property\nthat for iopt there exists an open set around hopt in which g is continuous.\nThen as t ! 1, \u0019(t)(h; i) tends to a Dirac mass centred on the optimal pair of\nkernel and associated scaling, (hopt; iopt).\nProof: See Appendix 2. 2\n5 Results\nThis section is organised as follows. In Section 5.1, the convergence of h to an optimal\nscaling will be demonstrated empirically using a linear Gaussian model. Then in Section\n5.2 the problem of Bayesian mixture analysis will be introduced. In Sections 5.3 and\n5.4 the proposed method will be evaluated in simulation studies using the example of\nBayesian mixture posteriors as de\fning the sequence of targets of interest.\nAs per Sherlock and Roberts (2009), the expected (Mahalanobis) square jumping\ndistance will be considered as an MCMC performance criterion:\n\u0003(\u0012t\u00001; \u0012t) = (\u0012t\u00001 \u0000 \u0012t)T \u0006^\u00001\u0019t (\u0012t\u00001 \u0000 \u0012t);\nwhere \u0012t\u00001 and \u0012t are two points in the parameter space and \u0006^\u0019t is an empirical estimate\nof the target covariance obtained from the current set of particles.\nTwo di\u000berent MCMC kernels will be considered; these are de\fned by the following\ntwo proposals:\nqrw(\u0012t\u00001; ~\u0012t) = N (\u0012t\u00001; h2\u0006^\u0019t);\nqlw(\u0012t\u00001; ~\u0012t) = N (\u000b\u0012t\u00001 + (1\u0000 \u000b)\u0016\u0012t; h2\u0006^\u0019t);\n424 An Adaptive Sequential Monte Carlo Sampler\nwhere \u0016\u0012t and \u0006^\u0019t are respectively estimates of the target and covariance and in the\nlatter, h 2 (0; 1] and \u000b = p1\u0000 h2. The \frst of these is a random{walk proposal. The\nsecond is based upon a method for updating parameter values in Liu and West (2001),\nhere named the `Liu\/West' proposal. The Liu\/West proposal has mean shrunk towards\nthe mean of the target and the imposed choice of \u000b =\np\n1\u0000 h2 sets the mean and\nvariance of proposed particles to be the same as that of the current particles. Note\nthat if the target is Gaussian, then this proposal can be shown to be equivalent to a\nLangevin proposal (Roberts and Tweedie 1996).\n5.1 Convergence of h\nIt is of interest to examine some examples of g(h) and demonstrate convergence of\none of the proposed algorithms to the optimal scaling and kernel in the context of a\nchoice between two candidate proposal densities. In this section, we take as an example\nthe g(h) arising from (1) a Gaussian proposal and (2) a t proposal, both exploring a\nGaussian target and with mixing criterion \u0003 being the squared jumping distance.\nThe results in this section are based on 100 observations simulated from a 5{\ndimensional standard Gaussian density, y1:100\niid\u0018 N (0; I5). The observation variance\nwas assumed to be known and therefore the probability model, or likelihood, was spec-\ni\fed as,\n\u0019(yj\u0012) = N (y; \u0012; I5):\nThe prior on the unknown parameter, \u0012, the vector of means, was set to N (0; 5I5).\nASMC with a random walk proposal was used to generate M = 2000 particles from\nthe posterior. We allowed the algorithm to choose between two potential random walk\nkernels: a multivariate Gaussian or a multivariate t. We \fxed the multivariate t random\nwalk to have 3 degrees of freedom, giving the algorithm the option to choose a heavy-\ntailed proposal (which as will be seen in this case, is sub-optimal). Resampling was\ninvoked when the ESS dropped below M=2 and no noise was added to the hs after\nresampling. The initial distribution for h was chosen to be uniform on (0; 10) for both\nproposal kernels. We note that for the target in consideration here, the sequence of\nfunctions fg(t)g for either of the proposal kernels does not change much since each\nintermediate target is exactly Gaussian and each proposal is scaled by the approximate\nvariance of the target. The optimum scaling for the Gaussian proposal kernel, hopt, was\ncomputed using 1-dimensional numerical integration and Theorem 1 of Sherlock and\nRoberts (2009).\nThe left plot in Figure 1 shows g(h) for this target explored by the Gaussian proposal\nkernel (black) and the multivariate t proposal (green). This plot was produced by\nsimulation using standard Metropolis Hastings MCMC for a range of potential values\nfor h; the expected square jumping distance was computed empirically from 10000\nsamples. The plot shows that of the two proposals, we expect the random walk to be\nchosen as it has the higher g(h), we further note that the optimal value of h for the\nGaussian proposal is slightly larger than the optimal value of h for the t proposal.\nThe right plot illustrates several features of the adaptive algorithm: the resampling\nP. Fearnhead and Benjamin M. Taylor 425\nfrequency, when the multivariate t kernel is rejected as sub-optimal, that the algorithm\ndoes indeed converge to the true optimal scaling for the Gaussian random walk and\nthe approximate rate of this convergence. Note also that just before rejecting the t\nproposal, the method had approximately converged to the best scaling in that case as\nwell { slightly lower than the value for the Gaussian proposal, as expected.\nFigure 1: Left plot: g(h) for a 5{dimensional Gaussian target, explored with a random\nwalk Metropolis algorithm (Gaussian proposal in black and t proposal in green) and with\nESJD as the optimization criterion. Right plot: convergence of h for the same density\nbased on 100 simulated observations; the horizontal line is the approximately optimal\nscaling, 1:06; the dashed line indicates at each iteration the proportion of proposal\nkernels that were Gaussian random walks.\n5.2 Bayesian Mixture Analysis\nThe ability of the ASMC algorithm to learn MCMC tuning parameters in more compli-\ncated scenarios is now evaluated using simulated data from a range of mixture likelihoods\n(for a complete review of this topic, see Fruhwirth-Schnatter (2006)). Let p1; : : : ; pr > 0\nbe such that\nPr\ni=1 pi = 1. Let N ( \u0001 ;\u0016; v) denote the normal density function with mean\n\u0016 and variance v. Let \u0012 = fp1:r\u00001; v1:r; \u00161:rg.\nThe likelihood function for a single observation, yi, is\n\u0019(yij\u0012) =\nrX\nj=1\npjN (yi;\u0016j ; vj): (12)\nThe prior \u0012 was multivariate normal, on a transformed space using the generalised logit\nscale for the weights, log scale for variances, and leaving the means untransformed.\nThe components of \u0012 were assumed independent a priori; the priors were log(pj=pr) \u0018\nN (0; 12), log(vj) \u0018 N (\u00001:5; 1:32) and \u0016j \u0018 N (0; 0:752), where j = 1; : : : ; r \u0000 1 in\n426 An Adaptive Sequential Monte Carlo Sampler\nRW\fxed Random walk ordered by means, with h chosen based on the the-\noretical results for Gaussian targets (Roberts and Rosenthal 2001;\nSherlock and Roberts 2009).\nRWadaptive Adaptive random walk MCMC ordered on means with uniform\nprior on h.\nLWmean Adaptive Liu\/West type proposal ordered by means.\nLWvariance Adaptive Liu\/West proposal ordered by variances.\nKmix Adaptive choice between random walk ordered by means,\nLiu\/West ordered using means and a Liu\/West ordered on vari-\nances.\nTable 1: Details of algorithms compared in the simulation study.\nthe case of the weights and j = 1; : : : ; r for the means and variances. The MCMC\nmoves within the SMC algorithm were performed in the transformed space, with the\nappropriate inverse transformed values to compute the likelihood in (12).\nAn issue with mixture models is that for the above choice of prior, the likelihood\nand posterior are invariant to permutation of the component labels (Stephens 2000).\nAs a result the posterior distribution has a multiple of r! modes, corresponding to each\npossible permutation. One way of overcoming this problem is by introducing a constraint\non the parameters, such as labelling the components so that \u00161 < \u00162 < \u0001 \u0001 \u0001 < \u0016r, or so\nthat v1 < v2 < \u0001 \u0001 \u0001 < vr. In the MCMC literature, constraints such as these are often\nimposed post{processing, see for example Celeux et al. (2000). This choice will a\u000bect\nthe empirical moments of the resulting posterior and hence the proposal distribution\nof the MCMC kernel { both the random walk and Liu\/West proposals depend on the\nposterior covariance, the latter also depending on the mean. In particular if there is\na choice of ordering whereby the posterior is closer to Gaussian, then this is likely to\nlead to better mixing of the MCMC kernels. This phenomenon motivates the idea that\nit is also possible to choose between orderings on the parameter vector, which will be\ninvestigated in the sequel.\n5.3 Details of Implementation of ASMC\nIn analysing the simulated data, a number of SMC and ASMC algorithms were com-\npared. These correspond to using the MCMC kernels shown in Table 1. In each case\nthe reference to ordering relates to how the component labels were de\fned, and thus\na\u000bect the estimate of the posterior mean and covariance used.\nThe above methods were also compared with the adaptive MCMC algorithm of\nHaario et al. (1998), denoted AMCMC. The speci\fc implementation is as follows.\nThe prior densities were identical to those for ASMC, the parameter vector was ordered\nby means and the random walk tuning was computed using the approximately optimal\nGaussian scaling given by h = 2:4=\np\n3r \u0000 1. AMCMC was run for 12000 iterations for\nthe 5 dimensional datasets (datasets 1{4 in Section 5.4) and for 30000 iterations for the\nP. Fearnhead and Benjamin M. Taylor 427\n8 dimensional datasets (datasets 5 and 6 in Section 5.4): these values were chosen so as\nto approximately match the number of likelihood computations involved between the\nASMC and AMCMC methods. The burn{in period was set to half of the number of\niterations and the method was initialised by a draw from the prior. There was an initial\nnon{adaptive phase, lasting 1000 iterations, where the proposal kernel was scaled by the\nprior covariance and after which scaling was via estimates of the posterior covariance\ncomputed from the chain to{date, this was updated every 100 iterations.\nFor the ASMC algorithms, the initial distribution of hs was chosen to be uniform\non (0; 2] for the random walk and on (0; 1] for the Liu\/West proposal. In the case of\nthe random walk, this range of hs can be justi\fed by considering the optimal scaling for\na random walk Metropolis on a multivariate Gaussian target in 5 dimensions namely\n2:38=\np\n5 = 1:06 (and decays with increasing dimension as O(d\u00001=2)). For the Liu\/West,\nh must be in (0; 1].\nIn each case we chose R (see Equation 5) to be a Gaussian kernel with variance\n0:0152. A sensitivity analysis showed that changing the variance of the noise slightly\ndid not a\u000bect the conclusions of this research. The parameter for the linear h-weighting\nscheme was a = 0. If any h was perturbed below zero, a small value, 1 \u0002 10\u00006, was\nimputed and similarly for the Liu\/West approach, any h perturbed above 1 was replaced\nby 1.\nThe number of particles was set to M = 2000 for the 2{mixture datasets and M =\n5000 for the 3{mixture datasets. Each algorithm was run 100 times on each dataset\nwith the order of observations randomised each time. For the MCMC based methods\nan ESS tolerance ofM=2 was used, as in Jasra et al. (2007). Resampling of the particles\nwas via residual sampling (Whitley 1994; Liu et al. 1998), but multinomial sampling\nwas used in selecting hs. For ease of computing posterior quantities of interest, each of\nthe above algorithms was forced to resample and move on the last iteration.\nTo compare the performance of di\u000berent methods, a measure of the accuracy of the\nestimated predictive density was used. This is advantageous because it is invariant to\nre{labelling of the mixture components { the alternative of comparing the accuracy\nof posterior marginals is compromised by the so-called label switching problem. The\nchosen accuracy measure was the variability of the predictive density (VPD) and was\ncalculated as follows. Each run of the algorithm produces a weighted particle set, from\nwhich an estimate of E[\u0019(y(i)jy1:n)] can be obtained at 100 points, fy(i) : i = 1; : : : ; 100g,\nequi-spaced between -2.5 and 2.5. For each i, the 100 simulation runs produce 100\nrealisations of E[\u0019(y(i)jy1:n)]; let y^(i;j) be the estimate of y(i) obtained from run j. The\nVPD measure used in this paper is\nmeani[varj(y^\n(i;j))];\nwhere meani is the mean over the is and varj is the variance of the estimates of y\n(i)\nobtained from the 100 simulations. The VPD gives an indication of the global variability\nof the predictive density across the simulations. In the tables, the relative VPD is used,\nwhich gives a scale{free comparison between methods. The SMC\/ASMC algorithm\nwith a relative VPD of 1 is the reference algorithm and has the smallest VPD of the\n428 An Adaptive Sequential Monte Carlo Sampler\nDataset 1: 0:5N (y;\u00000:25; 0:52) + 0:5N (y; 0:25; 0:52)\nDataset 2: 0:5N (y; 0; 12) + 0:5N (y; 0; 0:12)\nDataset 3: 0:3N (y;\u00001; 0:52) + 0:7N (y; 1; 0:52)\nDataset 4: 0:5N (y;\u00000:75; 0:12) + 0:5N (y; 0:75; 0:12)\nDataset 5: 0:35N (y;\u00000:1; 0:12) + 0:3N (y; 0; 0:52) + 0:35N (y; 0:1; 12)\nDataset 6: 0:25N (y;\u00000:5; 0:12) + 0:5N (y; 0; 0:22) + 0:25N (y; 0:5; 0:12)\nTable 2: Details of likelihoods, \u0019(yj\u0012), in the simulation study.\nSMC\/ASMC methods; larger values indicate higher VPDs. For the AMCMC methods,\nthe predictive densities were computed using all available samples, i.e., with 6000 for the\n2{mixture datasets and 15000 for the 3{mixture datasets. For the SMC\/ASMC methods\na Rao{Blackwellised version of the predictive density was computed using all current and\nproposed particles available from the last iteration (that is, using 4000\/10000 sample\npoints respectively for the 2\/3{mixture datasets).\n5.4 Results\n100 realisations were simulated from the following likelihoods in Table 2. This choice\nof datasets in combination with the selection of MCMC kernels allows several hypothe-\nses to be tested empirically. Firstly, by comparing the performance of RW\fxed with\nRWadaptive in these cases, it is possible to see whether anything is lost or gained by\nadapting the proposal kernel. Secondly, the impact of the di\u000berent kernel orderings\non MCMC mixing will become apparent by considering the performance of LWmean\nand LWvariance in these settings. Datasets 3, 4 and 6 have well `separated' means and\nsimilar variances, so one might expect algorithms ordering by means to perform better;\nwhereas datasets 2 and 5 have well separated variances and similar means, so perhaps\nthe algorithms ordering by variances might do well here. Thirdly, the Kmix algorithm\nshould be able to choose the best ordering and it is of interest to compare the results\nfrom this algorithm with an adaptive version of the individual kernels.\nThe simulation results from these datasets are presented in Table 3. These give both\nthe relative VPD for each method, but also an estimated mean ESJD for each method.\nThe mean number of likelihood evaluations for the SMC algorithms in datasets 1{\n6 were respectively: 5.65 \u0002105, 9.17\u0002105, 9.18\u0002105, 9.12\u0002105, 2.65\u0002106, 2.40 \u0002106;\nthere was little variability between the individual algorithms. In comparison, AMCMC\nused 1.2\u0002105 likelihood evaluations for datasets 1{4 and 3\u0002106 likelihood evaluations\nfor the others.\nAs would be hoped, a very strong correlation between lower VPD and higher ESJD\nis evident for the SMC\/ASMC algorithms. This empirically supports the use of ESJD\nas the chosen criterion for adapting the MCMC kernels.\nThere is relatively little di\u000berence across scenarios between the \fxed and adaptive\nrandom walk methods. Furthermore, the adaptive random walk settles on a similar\nP. Fearnhead and Benjamin M. Taylor 429\nTable 3: Rel. VPD is relative VPD, JD is the mean square jumping distance, Acc is\nthe mean \fnal acceptance probability, h is the mean \fnal scaling by kernel and Propn is\nthe mean \fnal kernel proportions. The kernels `LWm' and `LWv' indicate respectively\na Liu\/West proposal ordering on means or variances.\nDataset 1\nMethod Rel.\nVPD\nJD Acc. h Propn\nLWvariance 1 1.869 0.3 0.941\nLWmean 1.189 1.818 0.32 0.956\nKmix 1.258 1.845 0.317 LWm 0.963\nLWv 0.958\nLWm 0.785\nLWv 0.215\nRWadaptive 2.391 0.708 0.21 0.946\nAMCMC 2.396 0.575 0.13 1.073\nRW\fxed 3.414 0.641 0.18 1.064\nDataset 2\nLWvariance 1 9.139 0.873 0.978\nKmix 2.843 9.023 0.854 LWm 0.984\nLWv 0.978\nLWm 0.005\nLWv 0.995\nAMCMC 28.333 0.197 0.019 1.073\nLWmean 112.23 1.869 0.129 0.969\nRWadaptive 188.094 0.77 0.134 0.584\nRW\fxed 219.907 0.596 0.041 1.064\nDataset 3\nLWmean 1 6.38 0.792 0.98\nKmix 1.54 6.378 0.806 LWm 0.979 LWm 1\nAMCMC 7.465 0.847 0.146 1.073\nRW\fxed 40.538 1.124 0.277 1.064\nRWadaptive 45.739 1.057 0.369 1.045\nLWvariance 148.827 0.737 0.064 0.966\nDataset 4\nLWmean 1 7.132 0.875 0.98\nKmix 1.099 7.127 0.877 LWm 0.979 LWm 1\nAMCMC 24.024 0.462 0.057 1.073\nRWadaptive 48.606 1.143 0.274 1.086\nRW\fxed 51.919 1.167 0.298 1.064\nLWvariance 1096.167 0.632 0.027 0.961\nDataset 5\nAMCMC 0.883 0.356 0.04 0.849\nKmix 1 2.258 0.234 LWm 0.964\nLWv 0.971\nLWm 0.044\nLWv 0.956\nLWvariance 1.151 2.284 0.183 0.971\nLWmean 2.792 1.007 0.092 0.961\nRWadaptive 4.923 0.847 0.205 0.435\nRW\fxed 5.187 0.56 0.055 0.84\nDataset 6\nLWmean 1 4.099 0.277 0.972\nKmix 1.018 3.994 0.363 LWm 0.973 LWm 1\nAMCMC 1.556 0.211 0.04 0.849\nRW\fxed 3.244 0.996 0.429 0.84\nRWadaptive 3.259 0.93 0.192 0.693\nLWvariance 3.951 1.951 0.13 0.944\n430 An Adaptive Sequential Monte Carlo Sampler\nscaling as the \fxed scaled version in datasets 3 and 4, whereas in datasets 1, 2, 5 and\n6, RWadaptive settles to values below RW\fxed. In datasets 1, 2, 4 and 5, the adaptive\nRW outperformed the \fxed equivalent (though the di\u000berence was negligible in datasets\n4 and 5); this is likely due to the fact that the covariance was not a good estimate and\nthe adaptive version of the algorithm was able to rescale to compensate for this. In\ndatasets 3 and 6, the \fxed random walk marginally outperformed the adaptive.\nThe `correctly ordered' sequential Liu\/West algorithms considerably outperform\nthose using RW kernels in all six datasets and the incorrectly ordered versions per-\nform worse or as poorly as the RW. For the Liu\/West proposals, the h selected in each\ndataset was very close to 1: this special value corresponds to an independence kernel\nin the form of a moment{matched Gaussian approximation of the target. This is of in-\nterest as, in combination with the high acceptance rates of between 80{87% in datasets\n2{4, it suggests that the `correct' ordering makes the target, ostensibly a very complex\ndensity function, approximately Gaussian in these cases.\nThe Kmix algorithm is able to choose between orderings; the advantages of this are\nclearly evidenced in the results, as it selects the best ordering in each case, with the\nexception of dataset 1 (where the means and variances are both similar). The Kmix\nsampler settles almost unanimously on one ordering above the others. These results\nshow empirically that there is not much di\u000berence in using a single (correctly chosen)\nkernel compared with using a selection of kernels.\nThe performance of AMCMC was surpassed in all cases by the Kmix algorithm ex-\ncepting dataset 5, where AMCMC was the best performing algorithm. In this latter\ncase and in dataset 6, neither AMCMC nor the SMC\/ASMC algorithms performed well.\nAMCMC outperformed RWadaptive in each case apart from dataset 1, where the dif-\nference was small. However, the results show the average jumping distance of the kernel\nused in the ASMC algorithm was greater than that of AMCMC in all cases, suggesting\nASMC is able to adapt better to well-mixing kernels. To make this comparison more\nclear, two MCMC algorithms were run on each data-set, one using the \fnal kernel found\nby AMCMC and one using a kernel based on the ASMC run, with the \fnal estimated\ncovariance matrix and the \fnal mean value of the tuning parameter. The resulting\nMCMC algorithms performed very similarly in 3 cases (VPD of the two MCMC algo-\nrithms within 10% of each other) and the kernel found by ASMC performed better in\nthe other 3 (VPD reduced by 30%, 40% and 80%).\nThe R and C functions for the mixture example are available from the corresponding\nauthor.\n6 Discussion\nThis paper introduces a new method for automatically tuning and choosing between\ndi\u000berent MCMC kernels. Where MCMC based SMC code already exists, adapting the\nhs would be a relatively straightforward means of enhancing performance, the main\ne\u000bort being in calculating the ratio of the proposed particles in the accept\/reject step.\nP. Fearnhead and Benjamin M. Taylor 431\nProbably the most important conclusion from the simulation studies presented is that\nthere is not much lost in terms of performance in the adaption process { the Kmix al-\ngorithm performed comparably to the respective best performing individual component\nand the adaptive random walk Metropolis performed similarly to the \fxed, approxi-\nmately optimally scaled version.\nAlthough the method as presented has assumed that i.i.d. observations are available\nfrom the likelihood, the ASMC algorithm readily extends to the case of a dependent\nsequence. In this case the tth target density is given by,\n\u0019t \/ \u0019(\u0012)\u0019(y1j\u0012)\ntY\ni=2\n\u0019(yijy1:i\u00001; \u0012); (13)\nthe extension to general sequences of target densities, f\u0019igni=1, including (13), being\nimmediate and implied by the choice of notation in Algorithm 3.\nThe main assumption of ASMC is that a good h at time t is likely also to perform well\nat time t+1. One piece of evidence that supports this assumption is that the resampling\nfrequency decreases with an increasing number of observations (Chopin 2002). This\nimplies that, although \u00191 and \u00192 may be quite di\u000berent, \u00191001 and \u00191002 are likely to\nbe less so, provided that the data provides su\u000ecient information on the parameters.\nAs mentioned earlier in the text, the assumption of similar successive target densities\nis also required for the e\u000eciency of the non{adaptive version (Chopin 2002; Del Moral\net al. 2006).\nASMC can be easily extended by considering other proposal densities. For example\nit is possible to formulate a t{distributed version of the Liu\/West proposal, allowing\nfor heavier tailed proposals, the heaviness of which can be selected automatically by\nadaptively choosing the number of degrees of freedom; this t{based proposal includes\nthe Liu\/West as a special case. Other interesting algorithms can be formulated using\nDE proposals (Ter Braak 2006) (which generalises the snooker algorithm of Gilks et al.\n(1994)) or regional MCMC proposals (Roberts and Rosenthal 2009; Craiu et al. 2009)\n{ both of which appeal strongly to the particle structure of the new method.\nReferences\nAndrieu, C. and Robert, C. (2001). \\Controlled MCMC for Optimal Sampling.\" Tech-\nnical report, Universit\u0013e Paris{Dauphine. 412, 414\nAndrieu, C. and Thoms, J. (2008). \\A tutorial on adaptive MCMC.\" Statistics and\nComputing , 18(4): 343{373. 412, 414, 416, 419\nAtchad\u0013e, Y. and Rosenthal, J. (2005). \\On adaptive Markov chain Monte Carlo algo-\nrithms.\" Bernoulli, 11(5): 815{828. 414\nCapp\u0013e, O., Douc, R., Guillin, A., Marin, J.-M., and Robert, C. P. (2008). \\Adaptive\nimportance sampling in general mixture classes.\" Statistics and Computing , 18(4):\n447{459. 412\n432 An Adaptive Sequential Monte Carlo Sampler\nCarpenter, J., Cli\u000bord, P., and Fearnhead, P. (1999). \\Improved particle \flter for\nnonlinear problems.\" Radar, Sonar and Navigation, IEEE Proceedings, 146(1): 2 {7.\n417\nCeleux, G., Hurn, M., and Robert, C. P. (2000). \\Computational and Inferential Dif-\n\fculties with Mixture Posterior Distributions.\" Journal of the American Statistical\nAssociation, 95(451): 957{970. 426\nChopin, N. (2002). \\A sequential particle \flter method for static models.\" Biometrika,\n89(3): 539{552. 411, 415, 418, 431\nCornebise, J., Moulines, E., and Olsson, J. (2008). \\Adaptive methods for sequential\nimportance sampling with application to state space models.\" Statistics and Com-\nputing , 18(4): 461{480. 412\nCraiu, R. V., Rosenthal, J., and Yang, C. (2009). \\Learn From Thy Neighbor: Parallel-\nChain and Regional Adaptive MCMC.\" Journal of the American Statistical Associ-\nation, 104(488): 1454{1466. 412, 414, 431\nCrisan, D. (2001). \\Particle Filters { A Theoretical perspective.\" In Sequential Monte\nCarlo methods in practice, chapter 2, 17{42. Springer. 420\nDel Moral, P. (2004). Feynman-Kac Formulae. Genealogical and interacting particle\nsystems with applications. Springer. 420\nDel Moral, P., Doucet, A., and Jasra, A. (2006). \\Sequential Monte Carlo samplers.\"\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 68(3):\n411{436. 411, 414, 415, 418, 431\n| (2010). \\An Adaptive Sequential Monte Carlo Method for Approximate Bayesian\nComputation.\"\nURL http:\/\/www.cs.ubc.ca\/%7Earnaud\/delmoral_doucet_jasra_smcabc.pdf\n416\nDouc, R., Guillin, A., Marin, J.-M., and Robert, C. P. (2007). \\Minimum variance im-\nportance sampling via Population Monte Carlo.\" ESAIM: Probability and Statistics,\n11: 427{447. 412\nDoucet, A., de Freitas, N., and Gordon, N. (eds.) (2001). Sequential Monte Carlo\nMethods in Practice. Springer{Verlag New York. 411, 414\nFearnhead, P. (2002). \\MCMC, su\u000ecient statistics and particle \flters.\" Journal of\nComputational and Graphical Statistics, 11: 848{862. 411\n| (2008). \\Computational Methods for Complex Stochastic Systems: A Review of\nSome Alternatives to MCMC.\" Statistics and Computing , 18: 151{171. 411\nFruhwirth-Schnatter, S. (2006). Finite Mixture and Markov Switching Models. Springer.\n425\nP. Fearnhead and Benjamin M. Taylor 433\nGamerman, D. and Lopes, H. F. (2006). Markov Chain Monte Carlo: Stochastic Sim-\nulation for Bayesian Inference (2nd ed.). Chpman & Hall\/CRC. 413\nGhosal, S. (1999). \\Asymptotic Normality of Posterior Distributions in High Dimen-\nsional Linear Models.\" Bernoulli, 5(2): 315{331. 422\nGilks, W. and Berzuini, C. (1999). \\Following a moving target { Monte Carlo inference\nfor dynamic Bayesian models.\" Journal of the Royal Statistical Society, Series B,\n63(1): 127{146. 411, 415\nGilks, W., Richardson, S., and Spiegelhalter, D. (eds.) (1995). Markov Chain Monte\nCarlo in Practice. Chapman & Hall\/CRC. 413\nGilks, W. R., Roberts, G. O., and George, E. I. (1994). \\Adaptive Direction Sampling.\"\nJournal of the Royal Statistical Society. Series D (The Statistician), 43(1): 179{189.\n431\nGordon, N. J., Salmond, D. J., and Smith, A. F. M. (1993). \\Novel approach to\nnonlinear\/non-Gaussian Bayesian state estimation.\" Radar and Signal Processing,\nIEEE Proceedings F, 140(2): 107{113. 411\nHaario, H., Saksman, E., and Tamminen, J. (1998). \\An Adaptive Metropolis algo-\nrithm.\" Bernoulli, 7: 223{242. 411, 412, 414, 426\nHastings, W. K. (1970). \\Monte Carlo sampling methods using Markov chains and their\napplications.\" Biometrika, 57(1): 97{109. 413, 414\nJasra, A., Doucet, A., Stephens, D. A., and Holmes, C. C. (2008a). \\Interacting se-\nquential Monte Carlo samplers for trans-dimensional simulation.\" Computational\nStatistics & Data Analysis, 52(4): 1765{1791. 412\nJasra, A., Stephens, D. A., Doucet, A., and Tsagaris, T. (2008b). \\Inference\nfor Levy driven Stochastic Volatility Models via Adaptive SMC.\" http:\/\/www.\ntheodorostsagaris.com\/svvg-DAS.pdf. 412\nJasra, A., Stephens, D. A., and Holmes, C. C. (2007). \\On population-based simulation\nfor static inference.\" Statistics and Computing , 17(3): 263{279. 412, 427\nJennison, C. and Sheehan, N. (1995). \\Theoretical and Empirical Properties of the Ge-\nnetic Algorithm as a Numerical Optimizer.\" Journal of Computational and Graphical\nStatistics, 4(4): 296{318. 418\nKitagawa, G. (1996). \\Monte Carlo Filter and Smoother for Non-Gaussian Nonlinear\nState Space Models.\" Journal of Computational and Graphical Statistics, 5(1): 1{25.\n417\nKong, A., Liu, J. S., and Wong, W. H. (1994). \\Sequential Imputations and Bayesian\nMissing Data Problems.\" Journal of the American Statistical Association, 89(425):\n278{288. 415\n434 An Adaptive Sequential Monte Carlo Sampler\nLiu, J. and West, M. (2001). Sequential Monte Carlo Methods in Practice, chapter 10:\nCombined Parameter and State Estimation in Simulation-Based Filtering. Springer{\nVerlag New York. 411, 418, 424\nLiu, J. S. and Chen, R. (1995). \\Blind Deconvolution Via Sequential Imputations.\"\nJournal of the American Statistical Association, 90: 567{576. 415\n| (1998). \\Sequential Monte Carlo Methods for Dynamic Systems.\" Journal of the\nAmerican Statistical Association, 93(443): 1032{1044. 411, 415\nLiu, J. S., Chen, R., and Wong, W. H. (1998). \\Rejection Control and Sequential\nImportance Sampling.\" Journal of the American Statistical Association, 93(443):\n1022{1031. 415, 417, 427\nMetropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., and Teller, E.\n(1953). \\Equation of State Calculations by Fast Computing Machines.\" The Journal\nof Chemical Physics, 21(6): 1087{1092. 413, 414\nNeal, R. (2001). \\Annealed Importance Sampling.\" Statistics and Computing , 11(2):\n125{139. 411, 412, 415\nPasarica, C. and Gelman, A. (2010). \\Adaptively scaling the Metropolis algorithm using\nexpected squared jumped distance.\" Statistica Sinica, 20: 343{364. 417\nRoberts, G. and Rosenthal, J. (2001). \\Optimal Scaling for Various Metropolis-Hastings\nAlgorithms.\" Statistical Science, 16(4): 351{367. 412, 414, 416, 426\nRoberts, G. O. and Rosenthal, J. S. (2009). \\Examples of adaptive MCMC.\" Journal\nof Computational and Graphical Statistics, 18(2): 349{367. 412, 414, 431\nRoberts, G. O. and Tweedie, R. L. (1996). \\Exponential Convergence of Langevin\nDistributions and Their Discrete Approximations.\" Bernoulli, 2(4): 341{363. 424\nSchafer, C. and Chopin, N. (2013). \\Sequential Monte Carlo on large binary sampling\nspaces.\" Statistics and Computing , 23: 163{184. 412\nSherlock, C. and Roberts, G. (2009). \\Optimal scaling of the random walk Metropolis\non elliptically symmetric unimodal targets.\" Bernoulli, 15(3): 774{798. 412, 414,\n416, 417, 423, 424, 426\nStephens, M. (2000). \\Dealing with label switching in mixture models.\" Journal of the\nRoyal Statistical Society, Series B, 62(4): 795{809. 426\nStorvik, G. (2002). \\Particle \flters for state-space models with the presence of unknown\nstatic parameters.\" IEEE Transactions on Signal Processing , 50: 281{289. 411\nTer Braak, C. J. F. (2006). \\A Markov Chain Monte Carlo version of the genetic\nalgorithm Di\u000berential Evolution: easy Bayesian computing for real parameter spaces.\"\nStatistics and Computing , 16(3): 239{249. 431\nP. Fearnhead and Benjamin M. Taylor 435\nWest, M. (1993). \\Mixture models, Monte Carlo, Bayesian updating and dynamic\nmodels.\" Computing Science and Statistics, 24: 325{333. 418\nWhitley, D. (1994). \\A genetic algorithm tutorial.\" Statistics and Computing , 4: 65{85.\n417, 427\nAppendix 1: Proof of Proposition 1\nRecall that we use \u0019 to denote a probability density. Let \u0003(j) = \u0003(\u0012(j); \u00120(j)), i.e.,\nthe observed \u0003 for the jth particle and I denote the indicator function. The collection\nf(h(j); i(j)); 1=MgMj=1 is an iid sample from \u0019(H; I). Let\nWj =\nf(\u0003(j))PM\ni=1 f(\u0003\n(i))\nbe a set of weights and de\fne a discrete random variable (H?; I?), which takes value\n(h(j); i(j)) with probability Wj . For any B = HB \u0002 IB \u0012 H\u0002 I,\nIPr[(H?; I?) 2 B] =\nMX\nj=1\nWjI[(h(j); i(j)) 2 B];\n=\n1\nM\nPM\nj=1 f(\u0003\n(j))I[(h(j); i(j)) 2 B]\n1\nM\nPM\ni=1 f(\u0003\n(i))\n:\nNow we wish to use the strong law of large numbers for the numerator and denominator.\nFor this we need to know the expectations of these, which can be calculated using the\nproperties of conditional expectation in terms of w[M ](h(j); i(j)), with for example,\nE\u0019[M](\u0012)K(\u0012;\u00120)ff(\u0003)I[(H?; I?) 2 B]g = EH?;I?fw[M ](H; I)I[(H; I) 2 B]g:\nThus in the limit as M !1, using also that w[M ](h; i)! w(h; i) we get\f\f\f\f\f\n1\nM\nPM\nj=1 f(\u0003\n(j))I[(h(j); i(j)) 2 B]\n1\nM\nPM\ni=1 f(\u0003\n(i))\n\u0000\nP\nk2IB\nR\ns2HB w(s; k)\u0019(s; k)dsP\ni2I\nR\nh2H w(h; i)\u0019(h; i)dh\n\f\f\f\f\f\n\u0014\n\f\f\f\f\f\n1\nM\nPM\nj=1 f(\u0003\n(j))I[(h(j); i(j)) 2 B]\n1\nM\nPM\ni=1 f(\u0003\n(i))\n\u0000\nP\nk2IB\nR\ns2HB w\n[M ](s; k)\u0019(s; k)dsP\ni2I\nR\nh2H w\n[M ](h; i)\u0019(h; i)dh\n\f\f\f\f\f\n+\n\f\f\f\f\f\nP\nk2IB\nR\ns2HB w\n[M ](s; k)\u0019(s; k)dsP\ni2I\nR\nh2H w\n[M ](h; i)\u0019(h; i)dh\n\u0000\nP\nk2IB\nR\ns2HB w(s; k)\u0019(s; k)dsP\ni2I\nR\nh2H w(h; i)\u0019(h; i)dh\n\f\f\f\f\f\n! 0;\nas required. 2\n436 An Adaptive Sequential Monte Carlo Sampler\nAppendix 2: Proof of Theorem 1\nThe of this theorem proceeds in two parts. We start by observing that \u0019(n)(h; i) =\n\u0019(h; i) expfnfng where,\nfn(h; i) =\n1\nn\nnX\nt=1\nlog(a+ g(t)(h; i)):\nIn the \frst part, the following results will be proved:\n\u0088 There exists a function, f : A ! R\u00150, such that sup(h;i)2A jfn \u0000 f j \u0014 kfn\u0000\u000b.\n\u0088 (hopt; iopt) is the unique global maximum of f .\n\u0088 There exists an open set of h around hopt in which f(hopt; iopt) is continuous.\nIn the second part of the proof, these results will be used to show that as n ! 1,\n\u0019(n)(h; i) approaches a Dirac mass centred on (hopt; iopt).\nPart 1\nClaim that f(h; i) = log(a+ g(h; i)). It is easy to show that as g(h; i) \u0015 0,\nsup\n(h;i)2A\nj(a+ g(t))=(a+ g)\u0000 1j \u0014 klt\u0000\u000b;\nwhere kl = kg=a.\nUsing log(x) \u0014 x\u0000 1 we have, that for any (h; i) 2 A\nlog\n\u001a\na+ g(t)\na+ g\n\u001b\n\u0014 a+ g\n(t)\na+ g\n\u0000 1 \u0014 klt\u0000\u000b:\nPut km = 2kl and c = \u0000(1=2) log(1=2), noting that c 2 (0; 1). Since the function\n\r(x) = 1 \u0000 x \u0000 expf\u00002xg is increasing on [0; c], we have \r(x) \u0015 0 on this interval, as\n\r(0) = 0. Hence by re-arranging and taking logs in the inequality \r(x) \u0015 0, we have\nlog(1\u0000x) \u0015 \u00002x for any x 2 [0; c] and so provided that t > (kl=c)1=\u000b, for all (h; i) 2 A,\nlog\n\u001a\na+ g(t)\na+ g\n\u001b\n\u0015 log(1\u0000 klt\u0000\u000b) \u0015 \u00002klt\u0000\u000b = \u0000kmt\u0000\u000b:\nThe preceding arguments show that for all t > (kl=c)\n1=\u000b,\nsup\n(h;i)2A\n\f\f\f\flog\u001aa+ g(t)a+ g\n\u001b\f\f\f\f = sup\n(h;i)2A\nj log(a+ g(t))\u0000 log(a+ g)j \u0014 kmt\u0000\u000b:\nP. Fearnhead and Benjamin M. Taylor 437\nPut t? = d(kl=c)1=\u000be and ct? =\nPt?\u00001\nt=1\n\f\flog(a+ g(t))\u0000 log(a+ g)\f\f < 1 then for all\n(h; i) 2 A,\njfn \u0000 log(a+ g)j \u0014 1\nn\nnX\nt=1\n\f\f\flog(a+ g(t))\u0000 log(a+ g)\f\f\f ;\n\u0014 ct?\nn\n+\nkm\nn\nnX\nt=t?\nt\u0000\u000b;\n\u0014 ct?\nn\n+\nkm\nn\nZ n\n0\nt\u0000\u000bdt;\n=\nct?\nn\n+\nkm\n1\u0000 \u000bn\n\u0000\u000b;\n= ct?n\n\u0000\u000b +\nkm\n1\u0000 \u000bn\n\u0000\u000b; since 0 < \u000b < 1;\n< kfn\n\u0000\u000b;\nwhere kf = ct? + km=(1\u0000 \u000b) as required.\nThe continuity and strict monotonicity of the logarithm and the assumptions on g\nimply that (hopt; iopt) is the unique global maximum of f and also that for iopt there\nexists an open set around hopt in which f is continuous.\nPart 2\nIn this part, the properties of f will be used to show that for any set containing\n(hopt; iopt) as n!1, the probability that (H; I) belongs to that set tends to 1.\nBy the uniqueness of (hopt; iopt) and local continuity of f about this point, there\nexists an \u000f > 0 such that for all i 6= iopt\nf(hopt; iopt)\u0000max\nh\nf(h; i) > \u000f:\nLet \u0016X denote the complement of X in A. LetH0 \u001a H be any set containing hopt. By\nvirtue of the global uniqueness of (hopt; iopt) and continuity of f( \u0001 ; iopt) around hopt,\nthere exists an open set H1 \u001a H0 also containing hopt on which g( \u0001 ; iopt) is concave and\nwith the property, infh2H1 f(h; iopt) \u0015 sup(h)2 \u0016H1 f(h; iopt). Such a set exists precisely\nbecause g( \u0001 ; iopt) is locally continuous about the global maximum.\nFor any H1 we can de\fne an H2 \u001a H1 such that there exists \u000f1 < \u000f2 with\nsup\nh2H2\nff(hopt; iopt)\u0000 f(h; iopt)g = \u000f1;\ninf\nh2 \u0016H1\nff(hopt; iopt)\u0000 f(h; iopt)g = \u000f2:\nFurthermore for any H0 we can choose H1 and H2 such that \u000f > \u000f2.\n438 An Adaptive Sequential Monte Carlo Sampler\nConsider the probability of H 2 H0 and I = iopt after n updates,\nIPr[H 2 H0 \\ I = iopt] > IPr[H 2 H1 \\ I = iopt]\n=\nR\nH1 \u0019\n(n)(h; iopt)dhP\ni2I\nR\nH \u0019\n(n)(h; i)dh\n;\n=\nR\nH1 \u0019(h; iopt) expfnfn(h; iopt)gdhP\ni2I\nR\nH \u0019(h; i) expfnfn(h; i)gdh\n:\nNow we can obtain a lower bound for the numeratorZ\nH1\n\u0019(h; iopt) expfnfn(h; iopt)gdh \u0015\nZ\nH2\n\u0019(h; iopt) expfnfn(h; iopt)gdh\n\u0015\nZ\nH2\n\u0019(h; iopt) expfnf(h; iopt)\u0000 kfn1\u0000\u000bgdh\n\u0015\nZ\nH2\n\u0019(h; iopt) expfnf(hopt; iopt)\n\u0000n\u000f1 \u0000 kfn1\u0000\u000bgdh:\nA similar argument gives an upper bound for the di\u000berence between the denominator\nand the numeratorX\ni2I\n\u0012Z\nH\n\u0019(h; i) expfnfn(h; i)gdh\n\u0013\n\u0000\nZ\nH1\n\u0019(h; iopt) expfnfn(h; iopt)gdh\n\u0014\nX\ni2I\nZ\nH\n\u0019(h; i) expfnf(hopt; iopt)\u0000 n\u000f2 + kfn1\u0000\u000bgdh:\nThus we have R\nH1 \u0019(h; iopt) expfnfn(h; iopt)gdhP\ni2I\n\u0000R\nH \u0019(h; i) expfnfn(h; i)gdh\n\u0001\u0000 RH1 \u0019(h; iopt) expfnfn(h; iopt)gdh\n\u0015 expfn(\u000f2 \u0000 \u000f1) + 2kfn1\u0000\u000bg\nZ\nH2\n\u0019(h; iopt)dh:\nThis tends to in\fnity as n ! 1 since \u000f2 \u0000 \u000f1 > 0. Therefore IPr[(H; I) 2 H0 \u0002\nfioptg] ! 1 as n ! 1. Since the choice of H0 3 hopt was arbitrary, it may be\nmade in\fnitesimally small and still, after enough iterations of the sampler IPr[(H; I) 2\nH0 \u0002 fioptg] ! 1. This implies that \u0019(n)(h; i) tends in distribution to a Dirac mass\ncentred on (hopt; iopt) and establishes the claim. 2\nAcknowledgments\nThe authors wish to thank the anonymous reviewers of this article, whose feedback has helped\nto improve it.\n"}