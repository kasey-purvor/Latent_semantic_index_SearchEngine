{"doi":"10.1080\/0968776030110104","coreId":"14214","oai":"oai:generic.eprints.org:397\/core5","identifiers":["oai:generic.eprints.org:397\/core5","10.1080\/0968776030110104"],"title":"Profiling and understanding student information behaviour: Methodologies and meaning","authors":["Rowley, Jennifer"],"enrichments":{"references":[{"id":451932,"title":"A conceptual framework for understanding e-service quality: implications for future research and managerial practice', Marketing Sciences Institute Working Paper.","authors":[],"date":"2000","doi":null,"raw":"Zeithaml, V., Parasuraman, A. and Malhotra, A. (2000), 'A conceptual framework for understanding e-service quality: implications for future research and managerial practice', Marketing Sciences Institute Working Paper.","cites":null},{"id":197599,"title":"A study of the use of electronic information systems by higher education students in the UK',","authors":[],"date":"2002","doi":"10.1108\/eum0000000006949","raw":"Armstrong, C., Fenton, R. Lonsdale, R., Stoker, D., Thomas, R. and Urquhart, C. (2002), 'A study of the use of electronic information systems by higher education students in the UK', Program, 35 (3), 241-62.","cites":null},{"id":451236,"title":"An exploration of flow during Internet use',","authors":[],"date":"2001","doi":"10.1108\/10662240110695070","raw":"Rettie, R. (2001), 'An exploration of flow during Internet use', Internet Research, 11 (2), 103-14.","cites":null},{"id":197600,"title":"Australian Bureau of Statistics","authors":[],"date":"1999","doi":"10.1007\/978-1-4471-3052-9_8","raw":"Australian Bureau of Statistics (1999), Use of the Internet by Householders, Australia: ABS.","cites":null},{"id":1042809,"title":"Customer adoption of e-service: an experimental study',","authors":[],"date":"2001","doi":"10.1108\/09564230110387542","raw":"De Ruyter, K., Wetzels, M. and Kleijnen, M. (2001), 'Customer adoption of e-service: an experimental study', International Journal of Service Industry Management, 12 (2), 184-208.","cites":null},{"id":451239,"title":"E-service and the consumer',","authors":[],"date":"2001","doi":null,"raw":"Rust, R. and Lemon, K. (2001), 'E-service and the consumer', International Journal of Electronic Commerce, 5 (3), 85-101.","cites":null},{"id":451237,"title":"Encouraging and facilitating the use of EIS',","authors":[],"date":"2001","doi":null,"raw":"Rowley, J. (2001a), 'Encouraging and facilitating the use of EIS', in Proceedings of EUNIS 2001: The Changing Universities: The Role of Technology, Berlin, March 2001, 270-2.","cites":null},{"id":451238,"title":"Encouraging and facilitating the use of electronic information services (EIS)', Educational Developments,","authors":[],"date":"2001","doi":null,"raw":"Rowley, J. (2001b), 'Encouraging and facilitating the use of electronic information services (EIS)', Educational Developments, February, 12-14.","cites":null},{"id":1042810,"title":"Evaluation of the Distributed National Electronic Resource (DNER)', details available at: http:\/\/www.mmu.ac.uk\/h-ss\/cerlim\/projects\/edner.htm","authors":[],"date":"2001","doi":null,"raw":"EDNER (2001), 'Evaluation of the Distributed National Electronic Resource (DNER)', details available at: http:\/\/www.mmu.ac.uk\/h-ss\/cerlim\/projects\/edner.htm Eskola, E. (1999), 'University students' information seeking behaviour in a changing learning environment: how are students' information needs, seeking and use affected by new teaching methods?', Information Research, 4, (3), available at: www.shef.ac.uk\/~is\/ publications\/infres\/isicleeskola.html Graphic, Visualisation and Usability Centre (1999), 'Ninth WWW user survey', available at: www.gvu.gatech.edu\/user_survey.","cites":null},{"id":1042803,"title":"Influence of computer attitude and self-efficacy on IT usage behaviour',","authors":[],"date":"2000","doi":"10.4018\/joeuc.2001010103","raw":"38ALT-] Volume 11 Number I Chau, P. (2000), 'Influence of computer attitude and self-efficacy on IT usage behaviour', Journal of End User Computing, 13 (1), 26-34.","cites":null},{"id":197604,"title":"Information and digital literacies: a review of concepts',","authors":[],"date":"2001","doi":"10.1108\/eum0000000007083","raw":"Bawden, D. (2001), 'Information and digital literacies: a review of concepts', Journal of Documentation, 57 (2), 218-59.","cites":null},{"id":451931,"title":"Information Skills in Higher Education, Task Force on Information Skills,","authors":[],"date":"1999","doi":null,"raw":"SCONUL (1999), Information Skills in Higher Education, Task Force on Information Skills, London: SCONUL. Urquhart, C. (forthcoming), 'Critical incident technique and explicating interviewing in studies of information behaviour', Library and Information Science Research.","cites":null},{"id":197605,"title":"Integrating attitudinal theories to understand and predict use of technology-based self-service: the Internet as an illustration',","authors":[],"date":"2001","doi":"10.1108\/eum0000000006092","raw":"Bobbitt, L. and Dabholkar, P. (2001), 'Integrating attitudinal theories to understand and predict use of technology-based self-service: the Internet as an illustration', International Journal of Service Industry Management, 12 (5), 423-51.","cites":null},{"id":197601,"title":"JUBILEE: monitoring user information behaviour in the electronic age',","authors":[],"date":"2000","doi":"10.1108\/10650750010354148","raw":"Banwell, L. and Gannon-Leary, P. (2000), 'JUBILEE: monitoring user information behaviour in the electronic age', OCLC Systems and Services, 14 (4), 189-93.","cites":null},{"id":1042811,"title":"Learning online: student behaviour in a virtual campus',","authors":[],"date":"1999","doi":null,"raw":"Griffiths, L., Ashworth, J., Ward, H. and Marsden, P. (1999), 'Learning online: student behaviour in a virtual campus', Virtual University Journal, 2 (4), 76-86.","cites":null},{"id":1042808,"title":"Leveraging the corporate library through Web user training',","authors":[],"date":"1999","doi":"10.1108\/01435129910285163","raw":"Detlor, B. (1999), 'Leveraging the corporate library through Web user training', Library Management, 20 (7), 393-401.","cites":null},{"id":197602,"title":"Meaningful measures for individuals' realities: evidence form the JUBILEE project',","authors":[],"date":"2001","doi":null,"raw":"Banwell, L. and Gannon-Leary, P. (2001), 'Meaningful measures for individuals' realities: evidence form the JUBILEE project', in Proceedings of Fourth Northnmbria International Conference on Libraries and Information Services, Pittsburgh, 12-16 August 2001.","cites":null},{"id":451235,"title":"Measuring consistency of Web page design and its effect on performance and satisfaction',","authors":[],"date":"2000","doi":"10.1080\/001401300184332","raw":"Ozok, A. and Salvendy, G. (2000), 'Measuring consistency of Web page design and its effect on performance and satisfaction', Ergonomics, 43 (4), 23-34.","cites":null},{"id":451234,"title":"Measuring the customer experiences in online environments: a structural modelling approach',","authors":[],"date":"2000","doi":"10.1287\/mksc.19.1.22.15184","raw":"Novak, D., Hoffman, T. and Yung, Y.-F. (2000), 'Measuring the customer experiences in online environments: a structural modelling approach', Marketing Science, 19 (1), 22-43.","cites":null},{"id":1042802,"title":"Networked learning',","authors":[],"date":"2001","doi":"10.1108\/00220410110803828","raw":"Brophy, P. (2001), 'Networked learning', Journal of Documentation, 57 (1), 130-56.","cites":null},{"id":451233,"title":"Profiling and understanding student information behaviour: methodologies and meaning","authors":[],"date":"2000","doi":"10.3402\/rlt.v11i1.11265","raw":"39Jennifer Rowley Profiling and understanding student information behaviour: methodologies and meaning Lubans, J. (2000), 'Study 4: Internet use (February 2000) among 3rd year students at Duke University', Durham, North Carolina, USA, available at: http:\/\/www.lib.duke.edu\/lubans\/ study4b.html Mackie, M. and Burton, P. (1999), 'The use and effectiveness of the eLib subject gateways: a preliminary investigation', Program, 33 (4), 327-37.","cites":null},{"id":197597,"title":"Report, available at http: \/\/www.mmu.ac.uk\/h-ss\/cerlim\/projects\/agora\/htm","authors":[],"date":"2000","doi":null,"raw":"AGORA (2000), Report, available at http: \/\/www.mmu.ac.uk\/h-ss\/cerlim\/projects\/agora\/htm Armstrong, C., Barker, A., Everitt, J., Fenton, R., Lonsdale, R., Stoker, D., Thomas, R.","cites":null},{"id":451240,"title":"Studying the value of library and information services, Part II',","authors":[],"date":"1997","doi":"10.1002\/(sici)1097-4571(199706)48:6<543::aid-asi7>3.3.co;2-o","raw":"Saracevic, T. and Kantor, P. (1997), 'Studying the value of library and information services, Part II', Journal of the American Society for Information Science, 48 (6), 543-63.","cites":null},{"id":1042807,"title":"The effect of the Web on undergraduate citation behaviour 1996-1999',","authors":[],"date":"2001","doi":"10.1002\/1532-2890(2000)9999:9999<::aid-asi1069>3.0.co;2-p","raw":"Davies, P. and Cohen, S. (2001), 'The effect of the Web on undergraduate citation behaviour 1996-1999', Journal of the American Society for Information Sciences and Technology, 52 (4), 309-14.","cites":null},{"id":1042804,"title":"The EQUINOX project and the development of performance indicators for the electronic library',","authors":[],"date":"1999","doi":null,"raw":"Clarke, Z. (1999), 'The EQUINOX project and the development of performance indicators for the electronic library', in Proceedings of the Third Northumbria Informational Conference on Performance Measurement in Libraries and Information Services, 27-31 August, England, 145-8.","cites":null},{"id":451231,"title":"The impact of personality and approaches to learning on information behaviour',","authors":[],"date":"2000","doi":null,"raw":"Heinstrom, J. (2000), 'The impact of personality and approaches to learning on information behaviour', Information Research, 5 (3), available at: www.shef.ac.uk\/~is\/ publications\/infres\/paper78.html IMPEL2 (2001) http:\/\/is.unn.ac.uk\/imri\/projects\/completed_research\/electrnic_libraries\/ impel\/IMPEL2.htm Kucuk, S. and Arslan, M. (2000), 'A cross cultural comparison of consumers' acceptance of the Web marketing facilities', Journal of Euromarketing, 9 (3), 27-44.","cites":null},{"id":197598,"title":"The JISC usage surveys: trends in electronic information services (JUSTEIS) project: supply and demand in higher education',","authors":[],"date":"2001","doi":null,"raw":"and Urquhart, C. (2001), 'The JISC usage surveys: trends in electronic information services (JUSTEIS) project: supply and demand in higher education', Library and Information Briefings, 106\/7, 1-18.","cites":null},{"id":197596,"title":"The role of innovation characteristics and perceived voluntariness in the acceptance of information technologies',","authors":[],"date":"1997","doi":"10.1111\/j.1540-5915.1997.tb01322.x","raw":"Agarwal, R. and Prasad, J. (1997), 'The role of innovation characteristics and perceived voluntariness in the acceptance of information technologies', Decision Sciences, Summer (3), 557-83.","cites":null},{"id":451232,"title":"Towards an understanding of the behavioural intention to use a web site',","authors":[],"date":"2000","doi":"10.1016\/s0268-4012(00)00005-0","raw":"Lin, J. and Lu, H. (2000), 'Towards an understanding of the behavioural intention to use a web site', International Journal of Information Management, 20 (3), 197-209.","cites":null},{"id":197603,"title":"Usability testing of an academic library web site: a case study',","authors":[],"date":"2001","doi":"10.1016\/s0099-1333(01)00180-x","raw":"Battleson, B., Booth, A. and Weinthrop, J. (2001), 'Usability testing of an academic library web site: a case study', Journal of Academic Librarianship, 27 (3), 188-98.","cites":null},{"id":1042806,"title":"User issues',","authors":[],"date":"1998","doi":null,"raw":"Davies, C. (1998), 'User issues', in ELINOR: Electronic Library Project, British Library Research and Innovation Centre Reports, London: Bowker-Saur for the British Library, 71-85.","cites":null},{"id":1042805,"title":"Uses and perceptions of the World Wide Web in an information seeking environment',","authors":[],"date":"2000","doi":"10.1177\/096100060003200302","raw":"Dalgleish, A. and Hall, R. (2000), 'Uses and perceptions of the World Wide Web in an information seeking environment', Journal of Librarianship and Information Science, 32 (3), 104-16.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2003","abstract":"This paper draws on work conducted under the Joint Information Systems Committee (JISC) User Behaviour Monitoring and Evaluation Framework to identify a range of issues associated with research design that can form a platform for enquiry about knowledge creation in the arena of user behaviour. The Framework has developed a multidimensional set of tools for profiling, monitoring and evaluating user behaviour. The Framework has two main approaches: one, a broad\u2010based survey which generates both a qualitative and a quantitative profile of user behaviour, and the other a longitudinal qualitative study of user behaviour that (in addition to providing in\u2010depth insights) is the basis for the development of the EIS (Electronic Information Services) Diagnostic Toolkit. The strengths and weaknesses of the Framework approach are evaluated. In the context of profiling user behaviour, key methodological concerns relate to: representativeness, sampling and access, the selection of appropriate measures and the interpretation of those measures. Qualitative approaches are used to generate detailed insights. These include detailed narratives, case study analysis and gap analysis. The messages from this qualitative analysis do not lend themselves to simple summarization. One approach that has been employed to capture and interpret these messages is the development of the EIS Diagnostic Toolkit. This toolkit can be used to assess and monitor an institution's progress with embedding EIS into learning processes. Finally, consideration must be given to integration of insights generated through different strands within the Framework","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/14214.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/397\/1\/ALT_J_Vol11_No1_2003_Profiling%20and%20understanding%20st.pdf","pdfHashValue":"1fb474286fde6ae097d5dc3ae38e562461488663","publisher":"University of Wales Press","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:397<\/identifier><datestamp>\n      2011-04-04T09:10:26Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/397\/<\/dc:relation><dc:title>\n        Profiling and understanding student information behaviour: Methodologies and meaning<\/dc:title><dc:creator>\n        Rowley, Jennifer<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        This paper draws on work conducted under the Joint Information Systems Committee (JISC) User Behaviour Monitoring and Evaluation Framework to identify a range of issues associated with research design that can form a platform for enquiry about knowledge creation in the arena of user behaviour. The Framework has developed a multidimensional set of tools for profiling, monitoring and evaluating user behaviour. The Framework has two main approaches: one, a broad\u2010based survey which generates both a qualitative and a quantitative profile of user behaviour, and the other a longitudinal qualitative study of user behaviour that (in addition to providing in\u2010depth insights) is the basis for the development of the EIS (Electronic Information Services) Diagnostic Toolkit. The strengths and weaknesses of the Framework approach are evaluated. In the context of profiling user behaviour, key methodological concerns relate to: representativeness, sampling and access, the selection of appropriate measures and the interpretation of those measures. Qualitative approaches are used to generate detailed insights. These include detailed narratives, case study analysis and gap analysis. The messages from this qualitative analysis do not lend themselves to simple summarization. One approach that has been employed to capture and interpret these messages is the development of the EIS Diagnostic Toolkit. This toolkit can be used to assess and monitor an institution's progress with embedding EIS into learning processes. Finally, consideration must be given to integration of insights generated through different strands within the Framework.<\/dc:description><dc:publisher>\n        University of Wales Press<\/dc:publisher><dc:date>\n        2003<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/397\/1\/ALT_J_Vol11_No1_2003_Profiling%20and%20understanding%20st.pdf<\/dc:identifier><dc:identifier>\n          Rowley, Jennifer  (2003) Profiling and understanding student information behaviour: Methodologies and meaning.  Association for Learning Technology Journal, 11 (1).  pp. 28-40.  ISSN 0968-7769     <\/dc:identifier><dc:relation>\n        10.1080\/0968776030110104<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/397\/","10.1080\/0968776030110104"],"year":2003,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"Profiling and understanding student information\nbehaviour: methodologies and meaning\nJennifer Rowley\nEdge Hill College of Higher Education, UK\nemail: rowleyj@edgehill.ac.uk\nThis paper draws on work conducted under the Joint Information Systems Committee\n(JISC) User Behaviour Monitoring and Evaluation Framework to identify a range of\nissues associated with research design that can form a platform for enquiry about\nknowledge creation in the arena of user behaviour. The Framework has developed a\nmultidimensional set of tools for profiling, monitoring and evaluating user behaviour. The\nFramework has two main approaches: one, a broad-based survey which generates both a\nqualitative and a quantitative profile of user behaviour, and the other a longitudinal\nqualitative study of user behaviour that (in addition to providing in-depth insights) is the\nbasis for the development of the EIS (Electronic Information Services) Diagnostic\nToolkit. The strengths and weaknesses of the Framework approach are evaluated. In the\ncontext of profiling user behaviour, key methodological concerns relate to: representative-\nness, sampling and access, the selection of appropriate measures and the interpretation of\nthose measures. Qualitative approaches are used to generate detailed insights. These\ninclude detailed narratives, case study analysis and gap analysis. The messages from this\nqualitative analysis do not lend themselves to simple summarization. One approach that\nhas been employed to capture and interpret these messages is the development of the EIS\nDiagnostic Toolkit. This toolkit can be used to assess and monitor an institution's\nprogress with embedding EIS into learning processes. Finally, consideration must be given\nto integration of insights generated through different strands within the Framework.\nIntroduction\nThe interaction between students and electronic resources in support of the learning\nprocess is important for all agencies involved in the learning process, including lecturers,\nlibraries and learners.\nUser behaviour in networked environments has been explored for a number of different\npurposes and from different disciplinary bases. Both commercial and public sector\n28\nAw-] Volume I! Number I\norganizations are interested in user response to Web environments, and user, consumer,\nemployee and learner behaviour has attracted and continues to attract considerable\ninterest. The purposes for which investigation into user behaviour in Web-based\nenvironments has been pursued include:\n\u2022 Profiling user behaviour - in this context it is necessary to understand the key\nparameters that define an appropriate and useful profile, or to think about those aspects\nof behaviour that it makes sense to measure (for example, Agarwal and Prasad, 1997;\nAustralian Bureau of Statistics, 1999; Chau, 2000; Graphic Visualisation and Usability\nCenter, 1999-; Griffith, Ashworth, Ward and Marsden, 1999; Lin and Lu, 2000).\n\u2022 Pursuit of enhancements in service delivery. Service delivery is often evaluated in terms\n\u2022 of the match between the service user's expectation and perceptions of the service\nexperience or episode. In order to enhance service delivery it is typically necessary to\nmanage both operational details and user expectations (for example, Bobbitt and\nDabholker, 2001; Dalgleish and Hall, 2000; De Ruyter, Wetzels and Kleijnen, 2001;\nNovak, Hoffman and Yung, 2000; Zeithaml, Parasuraman and Malhotra, 2000; Rust\nand Lemon, 2001).\n\u2022 Performance measurement, in pursuit of understanding whether the tools available for\nlearning are fit for the purpose for which they are intended. This requires a definition of\nthe purpose of the tools, and the selection of performance indicators and performance\nmeasures that reflect the key purpose (for example, Clarke, 1999; IMPEL2, 2001;\nSaracevic and Kantor, 1997)\n\u2022 Monitoring trends and change, perhaps to note the effect of the injection of additional\nresources and facilities, or alternatively to justify or assess the need for continued\ninjection of resources; such resources might include technology or training (for\nexample, Detlor, 1999; Lubans, 2000).\n\u2022 To inform the design of electronic information environments, both in relation to\nusability and Website design, but also in the evaluation of specific projects or tools to\nprovide access to information resources, such as those under the eLib or JISC\nDistributed National Electronic Resource (DNER) funded initiatives (for example,\nBattleson, Booth and Weinthrop, 2001; Davies, 1998; Kucuk and Arslan, 2000; Mackie\nand Burton, 1999; Ozok and Salvendy, 2000; Rettie, 2001; AGORA, 2000; EDNER,\n2001).\n\u2022 To develop an enhanced theoretical understanding of learning in electronic\ninformation environments in order to support policy initiatives concerning learning (for\nexample, Bawden, 2001; Davies and Cohen, 2001 as summarized by Brophy, 2001;\nEskola, 1999; Heinstrom, 2000; SCONUL, 1999).\nOne of the very real problems in developing a theoretical understanding of user behaviour\nis drawing together the outcomes from numerous, often small-scale studies conducted with\ndifferent groups, asking different questions for different purposes, and emerging from\ndifferent disciplinary traditions. It is important to seek to understand the relationships\nbetween these studies performed for different purposes and within different contexts. This\narticle takes a step in this direction by using the opportunity afforded by the large-scale\nand multidimensional JISC User Behaviour Monitoring and Evaluation Framework to\n29\nJennifer Rowley Profiling and understanding student information behaviour: methodologies and meaning\nexplore some of the common methodological considerations that influence understanding\nof user behaviour. Aspects of the Framework are explained as a basis for comment on\nwider-ranging methodological issues. It is not, however, the purpose of this study to report\nin detail on either the methodology or the findings of the Framework. For those interested\nin this perspective a range of reports on the Framework are available through the JISC\nAssist Website {www.jisc.assist.ac.uk) and the articles that have emerged from the\nFramework (such as Armstrong et al, 2001,2002; Banwell and Gannon Leary, 2000,2001;\nRowley, 2001a, 2001b).\nIn 1999, JISC responded to the need to profile, monitor and evaluate user behaviour by\nestablishing the User Behaviour Monitoring and Evaluation Framework. The Framework\nspecifically focuses on the development of a longitudinal profile of the use of electronic\ninformation services and the development of an understanding of the triggers and barriers\nthat affect such use. The monitoring activities within the Framework take place on an\nannual cycle, initially supported for three years. The two strands that relate to user\nbehaviour are Strands A and D. Strand A is an annual survey that seeks to measure and\nevaluate the overall awareness, uptake, usage and usefulness of information technologies\nand information services in higher education in the United Kingdom. The survey was\nconducted by critical incident-based interviews, e-mail and paper-based questionnaires,\nand is part of the JUSTEIS project at the University of Aberystwyth. Strand D is a linked\nprogramme of ongoing, longitudinal monitoring of the information behaviour, needs and\nopportunities for specific academic and student communities, and for academics and\nstudents in general. The study has been conducted through the JUBILEE project at the\nUniversity of Northumbria. The Framework has been co-ordinated by the JISC Scientific\nAdviser. One of the strengths of this strand-based approach is that the mixed qualitative\nand quantitative methodology generates a rich picture and a set of deliverables that have\nthe potential to address the perspectives of, and to be of value to, a variety of different\naudiences. This paper draws upon the work within the Framework to reflect upon\nmethodological issues associated with understanding user, and more specifically student,\ninformation behaviour. In undertaking this reflective critique of our work, we reveal the\ncomplexity associated with understanding user behaviour.\nThe article starts with a review of the research design for the Framework. The core of the\npaper explores a range of methodological issues that arise from the two main projects,\nJUSTEIS and JUBILEE. Finally, the issue of integration is discussed. Using the\nperspective of this major project, the article raises a range of issues associated with\nresearch design that can form a platform for enhanced knowledge creation and\nunderstanding in the arena of user behaviour.\nResearch design\nThe underlying philosophy that determined the overall design of the Framework embraced\nseveral of the lenses that could be applied to understand user behaviour. This multiple-\nperspective approach involved some inherent contradictions. For example, Strand D seeks\nto identify the factors that influenced user behaviour through a grounded theory approach\nbased on semi-structured interviews and other contexts in which users were encouraged to\ndescribe and analyse their attitudes and behaviour. Yet in parallel, and therefore in the\nabsence of summative insights from Strand D, Strand A needed to design a general survey\n30\nALT-J Volume 11 Number I\non the use of EIS. This general survey needed to identify questions for inclusion in both\nquestionnaires and interviews.\nWhilst Strand A was focused on the delivery of a profile, and Strand D on an under-\nstanding of user behaviour, ultimately the strands share many questions. Key questions for\nboth strands are:\n\u2022 Which EIS do users use?\n\u2022 How do they use EIS?\n\u2022 What do they use EIS for?\n\u2022 What influences their use of EIS?\n\u2022 Are differences in the use of EIS discernible between different groups?\n\u2022 Is the use of EIS changing, and what will be the impact of that change on learning\nprocesses?\nWith shared questions at the foundation of both projects there is scope for overlap and\nconflict in relation to the contexts of the fieldwork, the design of the data collection\ninstruments, the conceptual frameworks and the project outcomes. This potential basis for\nconflict and duplication needs to be converted into creative synergy.\nA particularly valuable but challenging aspect of the,Framework is the requirement to\ngenerate longitudbal data about the changes in user behaviour. This involves the\nidentification of 'measures' that can be reassessed on an annual basis. It also implies a\ncommitment to the design of an approach that has a life beyond the current funding and\ncontractual arrangements associated with the research. Ideally, such work needs to have a\nten-year time frame but sustaining such a programme presents all sorts of financial,\noperational and logistical dilemmas. However, without such a commitment all that can be\nachieved is a series of loosely linked snapshots of user behaviour on the basis of which it is\ndifficult to generalize and to develop a sufficiently rich picture to inform both practice and\npolicy-making.\nThe Framework also needs to evolve as factors in the environment change. Changes can be\nanticipated in policy agendas affecting the composition of the student community and\npriorities in relation to elements of and the nature of the student experience, technological\ninnovation, and the range of EIS that are available to users.\nThe design of the Framework as a set of separate projects, overviewed and coordinated\n(but not administered or managed) by a third party, has a number of strengths and some\nchallenges. These are of interest because they are likely to be experienced by other large\nprojects on user behaviour. Any overarching project management structure impacts on the\noutcomes of the project, and is relevant to the methodology for the Framework as a whole.\nThe strengths and challenges are summarized below.\nStrengths\n1. The mixed qualitative and quantitative methodology generates a rich picture and a set\nof deliverables that have the potential to address the perspectives of, and to be of value\nto, a variety of different audiences.\n31\nJennifer Rowley Profiling and understanding student information behaviour: methodologies and meaning\n2. The approach draws on many project team members, with diverse competencies,\ndisciplinary allegiances, methodological perspectives, views of priorities and networks.\nThis presents a rich resource, which has the potential to be extremely creative.\n3. Project teams learn from identification with a larger team; knowledge and insights can\nbe created through the virtual team, and valuable research networks are strengthened.\n4. The work of any one project team comes under close scrutiny from another project\nteam that is working on a parallel project; thus research is submitted to critical\nformative evaluation.\nChallenges\n1. Identification of the most effective approaches to integration, and the interleaving of\nintegration with work on projects, taking into account that the project teams have been\nspecifically funded by JISC to complete their specific projects.\n2. Managing integration at an operational level, in relation to issues such as sampling and\nthe design of data collection instruments.\n3. Managing integration at a strategic level, in terms of creating ownership of a\ncontinuing review of what has been achieved overall and the extent to which this\nmatches the original objectives of the Framework, whilst accommodating changes in\nthe sector that might impact on those objectives.\n4. The difficulties associated with bringing the project teams together on a reasonably\nregular basis, when they are both busy.\nProfiling user behaviour\nThe questionnaires and interview schedules developed under JUSTEIS have been used in\nboth further and higher education amongst a range of different categories of users. They\nare a useful resource for universities wishing to profile user behaviour. Such questionnaires\nand interview schedules can generate a descriptive profile of user behaviour amongst a user\ncommunity. They, for example, provide answers to questions such as whether postgraduate\nstudents make use of JISC services or the purpose for which students are making use of\nEIS.\nIf data is collected at annual or biannual intervals they can be used to profile trends and\nchanges in behaviour. Such changes may offer a perspective on the effects of enhanced or\nchanged information services or the design of learning experiences on EIS use. For\nexample, the results of the First Cycle revealed that 'search engines', 'e-mail', 'OPAC (Own\ninstitution)' and 'Local EIS' were the top four services used by students. In Second Cycle\nresults, 'Search engines' and 'e-mail' remained in top position, but 'other Web EIS' and\n'Own HEI Website' had replaced 'OPAC (Own institution)' and 'Local EIS' in top\nposition.\nSimilarly, if data is collected from different groups it is possible to profile behaviour\nspecifically in relation to disciplines or different categories of students (such as\nundergraduate Years 1, 2, 3 and postgraduate). It is, for instance, evident that patterns of\nuse of EIS vary between disciplines, with the Pure and Applied Sciences cluster making\nnoticeably more use of JlSC-negotiated services than other disciplinary clusters and the\n32\nALT-] Volume 11 Number I\nPure and Applied Social Sciences cluster making comparatively more use of organizational\nWebsites.\nThere are, however, a number of dilemmas associated with a broad-based survey. These are\nperhaps more acute in a sector-wide survey than might be the case for a survey within an\ninstitution or studying a specific, more narrowly defined, group of users, but they apply for\nall such surveys, and are therefore important in understanding, interpreting and utilizing\ndata. In the interests of simplification, the discussion here tends to take something of a\nquantitative bent. These issues are also significant in research that is pursuing answers to\n'why' and 'how' questions. Issues for consideration include: representativeness, sampling\nand access; the selection of appropriate measures; and interpreting data.\nRepresentativeness, sampling and access\nGood surveys include responses from a randomly selected, or at the least an unbiased,\nsample of the population. The first challenge is to decide how the sample should be\nconstituted. The JUSTEIS sampling methodology illustrates the potential for complexity\nin this process. First a sample of institutions was taken in order to ensure that a range of\ninstitutional types was included. A two-way categorization of higher education institutions\n(HEIs) was used, based on size of HEI in terms of undergraduate student numbers and\ninstitutional type. The next level of analysis used the multi-stage cluster sampling approach\nto provide a range of departments, evenly split amongst disciplinary clusters. This process\nshould have led to the selection of fifty departments. Continuing with this level of rigour,\nthe team initially sought lists of student e-mail addresses, from which they could select\nstudents on a random quota basis to ensure representation of different groups of students.\nThis rigorous approach proved unsatisfactory on at least two levels:\n\u2022 Some departments selected could not participate, often due to faculty or departmental\nrestructuring.\n\u2022 Many departments did not have .lists of students' e-mail addresses. Where these were\nheld centrally, different sections of the university were responsible for them in different\ninstitutions, and access by appropriate agencies proved problematic. Data protection\nwas also raised as an issue by some institutions. Finally, even when researchers\nmanaged to gather e-mail addresses, response rates from e-mails were poor.\nAdjustments were made to the list of departments selected. Response rates from students\nwere enhanced by making contact with key informants in departments, and through them\nacademic staff, who then facilitated access to groups of students.\nAccordingly, although the group of users engaging in the survey remained relatively large\nand spread across numerous departments, it became 'lumpy' with stronger representation\nfrom some institutions, departments and disciplinary clusters. Although the JUBILEE\nsampling was more focused because representativeness was not intended to derive from a\nbroad-based sample, difficulty in accessing students again had the result of generating\nunevenness in the data collected. Access and participation issues eroded the theoretical\nsampling procedures. Limited and uneven response rate is always likely to be a limitation\nof such surveys and this needs to be acknowledged in the interrogation and intelligent use\nof survey data.\n33\nJennifer Rowley Profiling and understanding student information behaviour: methodologies and meaning\nSelection of appropriate measures\nHow can user behaviour be measured? What are the performance indicators of appropriate\nuser behaviour? The JUSTEIS project posed these questions in terms of:\n\u2022 the purposes for which users used EIS;\n\u2022 the types of EIS used;\n\u2022 influences on student use of EIS; and\n\u2022 student information skills education and training.\nData in response to the first two questions had the potential to be analysed according to\nthe category of user (undergraduate, postgraduate, academic and research staff, and\nlibrary and information services staff), and according to disciplinary group. Major report-\ning was chosen to be by category of user since this appeared to create the most significant\ndifferentials between the behaviour of the different groups. Comparison across years has\nbeen largely in terms of these categories. This has made it possible to appreciate the main\npurposes for which EIS are used: for assignment or lab report, or for background reading\nfor personal research. Jhese uses are common to both undergraduates and postgraduates,\nbut postgraduates make greater use of a range of specialized and often discipline-specific\nEIS. Purposes for which staff use EIS are more diverse, but top of the list is student\nadministration, followed by bibliography or reference checking, and planning leisure\nactivity.\nAll questions required the development of specific sub-categories, such as the types of\npurposes and the types of EIS used. Different categories might have generated different\ninsights. For example, the team needed to decide whether e-mail and OPACs were relevant\nEIS and what constituted relevant purposes within the remit of the study.\nOne important factor was the terms that were used to describe EIS categories to respondents:\nmuch of the terminology that the researcher might find useful for analysis is meaningless to\nmany users. Although, for example, the project sponsors (JISC) were interested to know\nwhether users were using a JlSC-negotiated service, the average user would be totally\nunaware of the provenance or licensing arrangements associated with specific services.\nAccordingly free form responses were analysed using the EIS taxonomy developed for the\npurpose. All such taxonomies can be contested and whichever taxonomy is adopted,\nresearchers need to perform post hoc categorization of users' responses during data analysis.\nAnother choice that had to be made related to reporting of use. The JUSTEIS team decided\nnot to ask users to rank or rate frequency of use, but rather to provide information in\nrelation to a recent 'critical incident of information use' and to identify the 'critical success\nfactors'. The critical incident technique encourages respondents to tell the story of the\ninformation-seeking incident in their own terms, and can be used to explore the antecedents,\nthe purposes of the search, the processes involved and outcomes (Urquhart, forthcoming).\nThe critical success factors technique is useful in determining the individual's priorities and\nthe contribution that information can make in the context of those priorities.\nInterpreting data\nAnalysis and interpretation of the data collected depends upon survey objectives. A\nspecific focus might, for example, be to monitor the relative use of EIS between first-year\n34\nALT-] Volume 11 Number I\nand second year undergraduates. Another study might be interested in the relative use of\nacademic gateways and search engines. At the generic level at which the Framework has\noperated, the most pressing questions in relation to interpretation are of the type:\n\u2022 What is the significance of the fact that 46.9 per cent of undergraduate students cited\nassignment or lab report as a purpose for using EIS, whereas only 9.8 per cent cited it\nfor seminars or class presentation, only 6.5 per cent for dissertation or thesis and 6.8\nper cent for online shopping?\n\u2022 What are the implications of a rank ordering of types of EIS used by undergraduates\nwhich, for example, shows search engines (68.5 per cent) at the top of the list, with e-\nmail (63.8 per cent) a close-run second, and JlSC-negotiated services (8.5 per cent) as\nthe least cited?\nUnderlying any interpretation is the matter of benchmarking - what would be a reasonable\nlevel of citation of specific sources? Should students undertaking different courses differ in\nthe extent and nature of the use that they make of the different types of EIS?\nBenchmarking demands refinement of expectations regarding user information behaviour.\nUnderstanding and changing user behaviour\nBoth JUSTEIS and JUBILEE have qualitative dimensions. For JUSTEIS, interviews\nprovided richer data than questionnaires. Interviews are key to understanding behaviour\nand act as a means of identifying the early emergence of changes, such as the rapid\nemergence of Google as a 'universal' search tool for students, or the use of Web text\nmessaging services.\nIn JUBILEE, rich interview and focus group data has been used to generate and inform:\n\u2022 detailed narratives, in the respondents' own words, of EIS user behaviour;\n\u2022 case study analysis of the use of EIS at the different sites;\n\u2022 the gap analysis that exposes and explains gaps that exist between the expectations of\nuser behaviour held by different groups; and\n\u2022 the EIS Diagnostic Toolkit, which can be used on an institutional or disciplinary basis\nto monitor progress with the engagement of EIS.\nOne of the real challenges of the qualitative approach is to lift from a morass of anecdotal\nevidence some insights, tools and models that are useful to academics, managers and\nlibrary and information professionals who need to work together to formulate and\npromote learning experiences that prepare students for continued learning in a digital\nenvironment. JUBILEE has sought to generate and analyse rich data in such a way that\npromotes understanding of user behaviour, as well as to create tools that can be used as the\nbasis for management agendas for change and the monitoring and evaluating of change.\nOver the three cycles of the Framework, work has progressed towards the development of\nthe EIS Diagnostic Toolkit. The Toolkit is a benchmarking tool that provides an action\nplan for HE managers to support them in the development of engagement with EIS at\nboth at individual and institutional levels. The Toolkit is based on the characterization of\nuser-based success criteria in relation to EIS. Evidence from Cycle 1 was used to identify\n35\nJennifer Rowley Profiling and understanding student information behaviour: methodologies and meaning\nthe prerequisites for individual success in information-searching, and these have been used\nto generate the seven themes. The themes are:\n\u2022 access to information;\n\u2022 resource base;\n\u2022 user skills;\n\u2022 EIS and course design and delivery;\n\u2022 EIS and student learning;\n\u2022 quality assurance; and\n\u2022 seamlessness (of access to EIS).\nThese themes have been developed in subsequent cycles, and form the basis of the structure\nfor the toolkit. At the conclusion of Cycle 1, three example matrices were generated that\nillustrated how the themes can be used to profile disciplines:\n\u2022 A matrix for one discipline (Business Studies) at six sites, showing the variations\nbetween sites in that discipline.\n\u2022 Evidence of the three Cycle 1 target disciplines at one site, showing the variations\nbetween disciplines at the same site.\n\u2022 Drilling down into one theme, showing Cycle 1 evidence of poor, intermediate and\ngood practice from all sites.\nIn addition, as in subsequent cycles, the themes were used to summarize and characterize\nthe qualitative evidence captured from the JUBILEE sites, and also to draft profiles of the\ncase study sites.\nDuring Cycle 2, taking work conducted by the research team at Northumbria for UKOLN\n(http:\/\/www.ukoln.ac.uk\/serviceslelib\/papers\/supporting\/) as a departure point, a model of\ngeneric development stages was developed. These stages are summarized in Table 1.\nDuring Cycles 2 and 3 evidence has been presented of each of the different stages of\nStage\n1 Baseline\nStarting point, status quo\n2 Change\nPoint at which there is recognition of a need to change'\n3 Congruence\nStage where the vision is starting to be implemented\n4 Embedding\nAppropriate partnerships are developing from congruence and are accepted as part of the culture\n5 Full integration\nAll the diverse elements are assimilated, signifying maturity and the ability to fully exploit potential\navailable\nTable I: EIS DiagnosticToolkit - generic development stages\n36\nALT-] Volume 11 Number I\ndevelopment in the use of EIS. The final form of the toolkit manual will be multi-layered.\nThe generic form of the toolkit will overlay the characterization of situations in different\ndisciplines and sites. This form of presentation will permit institutions to tailor the\nilluminative material contained in it to their own situations, at institution, discipline or\nindividual levels.\nIntegration\nThe original conceptualization of the Framework was as a series of interlinked strands.\nThe mechanism for integration of these strands, and indeed the more general\nmethodological issue of how the picture generated by qualitative and quantitative\napproaches might be integrated to produce a holistic picture, was left undefined. In\naddition, it is an extremely challenging task to use the data collection methodologies\nwithin the Framework in such a way as to generate insights or tools that might support\neach of the potential groups of stakeholders as they seek to develop their particular\ncontribution to the advancement of a digital learning environment and community.\nIntegration within the Framework has developed through the three cycles of the\nFramework. This process is proposed as one approach to the integration of perspectives on\nuser behaviour derived from different methodological bases. In Cycle 1, the primary focus\nwas on project data collection and access methods. This generated frameworks for\nunderstanding, organizing and interpreting data at a project level. In Cycle 1 the\nFramework was a loosely-linked set of projects within an overarching set of objectives.\nDuring Cycle 2 the data collection approaches were tested and refined and further data was\ncollected to populate data sets. In terms of integration, with the project methodologies\nreasonably established and stable, and a strong understanding of the type of issues that\naffect EIS, project teams had a stronger platform from which to understand the\nrelationship between the separate projects. Cycle 3 provided a further opportunity to\npopulate data sets, culminating in a cohesive set of deliverables (Table 2).\nCycle Key Activities Level of Integration\n1 Design and testing of project data collection Loosely linked set of projects with an overarching\nand access methodologies, leading to the set of objectives.\ngeneration of frameworks for understanding\nthe data at project level.\n2 Further refinement of data collection and Partial integration based on enhanced\naccess methodologies. Further population of understanding of the relationships between projects,\ndata sets.\n3 Further population of data sets. Refinement Full integration, evidenced through a holistic set of\nof project data collection, and analysis to deliverables.\ngenerate a holistic picture of user behaviour.\nTable 2: Phases of integration in framework methodolog\/\nConclusion\nThe User Behaviour Monitoring and Evaluation Framework is an ambitious attempt to\nprovide a methodology for creating multidimensional insights into the nature of user\n37\nJennifer Rowley Profiling and understanding student information behaviour: methodologies and meaning\ninformation behaviour. This article has sought to draw out the challenges associated with\nachieving meaningful insights into user information behaviour. These challenges do not\ninvalidate data that has been collected and analysed, but do encourage a critical\nperspective on research methodologies, and the interpretation of the outcomes and\nfindings from research projects. The design decisions made during the development of the\nFramework have been outlined; these reflect some common dilemmas for researchers,\nmanagers and other information and education professionals who seek to understand user:\nbehaviour. Some researchers have avoided the dilemmas outlined in this article by focusing\nmore narrowly on specific EIS, or upon specific learning communities. Whilst such\ndisconnected studies offer useful isolated insights, any attempt to conduct research that can\ninform general policy and practice in higher education needs to be informed by an\nunderstanding of the issues associated with profiling, interpreting and changing user\ninformation behaviour.\nReferences\nAgarwal, R. and Prasad, J. (1997), 'The role of innovation characteristics and perceived\nvoluntariness in the acceptance of information technologies', Decision Sciences, Summer\n(3), 557-83.\nAGORA (2000), Report, available at http: \/\/www.mmu.ac.uk\/h-ss\/cerlim\/projects\/agora\/htm\nArmstrong, C., Barker, A., Everitt, J., Fenton, R., Lonsdale, R., Stoker, D., Thomas, R.\nand Urquhart, C. (2001), 'The JISC usage surveys: trends in electronic information services\n(JUSTEIS) project: supply and demand in higher education', Library and Information\nBriefings, 106\/7, 1-18.\nArmstrong, C., Fenton, R. Lonsdale, R., Stoker, D., Thomas, R. and Urquhart, C. (2002),\n'A study of the use of electronic information systems by higher education students in the\nUK', Program, 35 (3), 241-62.\nAustralian Bureau of Statistics (1999), Use of the Internet by Householders, Australia:\nABS.\nBanwell, L. and Gannon-Leary, P. (2000), 'JUBILEE: monitoring user information\nbehaviour in the electronic age', OCLC Systems and Services, 14 (4), 189-93.\nBanwell, L. and Gannon-Leary, P. (2001), 'Meaningful measures for individuals' realities:\nevidence form the JUBILEE project', in Proceedings of Fourth Northnmbria International\nConference on Libraries and Information Services, Pittsburgh, 12-16 August 2001.\nBattleson, B., Booth, A. and Weinthrop, J. (2001), 'Usability testing of an academic library\nweb site: a case study', Journal of Academic Librarianship, 27 (3), 188-98.\nBawden, D. (2001), 'Information and digital literacies: a review of concepts', Journal of\nDocumentation, 57 (2), 218-59.\nBobbitt, L. and Dabholkar, P. (2001), 'Integrating attitudinal theories to understand and\npredict use of technology-based self-service: the Internet as an illustration', International\nJournal of Service Industry Management, 12 (5), 423-51.\nBrophy, P. (2001), 'Networked learning', Journal of Documentation, 57 (1), 130-56.\n38\nALT-] Volume 11 Number I\nChau, P. (2000), 'Influence of computer attitude and self-efficacy on IT usage behaviour',\nJournal of End User Computing, 13 (1), 26-34.\nClarke, Z. (1999), 'The EQUINOX project and the development of performance indicators\nfor the electronic library', in Proceedings of the Third Northumbria Informational\nConference on Performance Measurement in Libraries and Information Services, 27-31\nAugust, England, 145-8.\nDalgleish, A. and Hall, R. (2000), 'Uses and perceptions of the World Wide Web in an\ninformation seeking environment', Journal of Librarianship and Information Science, 32\n(3), 104-16.\nDavies, C. (1998), 'User issues', in ELINOR: Electronic Library Project, British Library\nResearch and Innovation Centre Reports, London: Bowker-Saur for the British Library,\n71-85.\nDavies, P. and Cohen, S. (2001), 'The effect of the Web on undergraduate citation\nbehaviour 1996-1999', Journal of the American Society for Information Sciences and\nTechnology, 52 (4), 309-14.\nDetlor, B. (1999), 'Leveraging the corporate library through Web user training', Library\nManagement, 20 (7), 393-401.\nDe Ruyter, K., Wetzels, M. and Kleijnen, M. (2001), 'Customer adoption of e-service: an\nexperimental study', International Journal of Service Industry Management, 12 (2),\n184-208.\nEDNER (2001), 'Evaluation of the Distributed National Electronic Resource (DNER)',\ndetails available at: http:\/\/www.mmu.ac.uk\/h-ss\/cerlim\/projects\/edner.htm\nEskola, E. (1999), 'University students' information seeking behaviour in a changing\nlearning environment: how are students' information needs, seeking and use affected by\nnew teaching methods?', Information Research, 4, (3), available at: www.shef.ac.uk\/~is\/\npublications\/infres\/isicleeskola.html\nGraphic, Visualisation and Usability Centre (1999), 'Ninth WWW user survey', available\nat: www.gvu.gatech.edu\/user_survey.\nGriffiths, L., Ashworth, J., Ward, H. and Marsden, P. (1999), 'Learning online: student\nbehaviour in a virtual campus', Virtual University Journal, 2 (4), 76-86.\nHeinstrom, J. (2000), 'The impact of personality and approaches to learning on\ninformation behaviour', Information Research, 5 (3), available at: www.shef.ac.uk\/~is\/\npublications\/infres\/paper78.html\nIMPEL2 (2001) http:\/\/is.unn.ac.uk\/imri\/projects\/completed_research\/electrnic_libraries\/\nimpel\/IMPEL2.htm\nKucuk, S. and Arslan, M. (2000), 'A cross cultural comparison of consumers' acceptance\nof the Web marketing facilities', Journal of Euromarketing, 9 (3), 27-44.\nLin, J. and Lu, H. (2000), 'Towards an understanding of the behavioural intention to use a\nweb site', International Journal of Information Management, 20 (3), 197-209.\n39\nJennifer Rowley Profiling and understanding student information behaviour: methodologies and meaning\nLubans, J. (2000), 'Study 4: Internet use (February 2000) among 3rd year students at Duke\nUniversity', Durham, North Carolina, USA, available at: http:\/\/www.lib.duke.edu\/lubans\/\nstudy4b.html\nMackie, M. and Burton, P. (1999), 'The use and effectiveness of the eLib subject gateways:\na preliminary investigation', Program, 33 (4), 327-37.\nNovak, D., Hoffman, T. and Yung, Y.-F. (2000), 'Measuring the customer experiences in\nonline environments: a structural modelling approach', Marketing Science, 19 (1), 22-43.\nOzok, A. and Salvendy, G. (2000), 'Measuring consistency of Web page design and its\neffect on performance and satisfaction', Ergonomics, 43 (4), 23-34.\nRettie, R. (2001), 'An exploration of flow during Internet use', Internet Research, 11 (2),\n103-14.\nRowley, J. (2001a), 'Encouraging and facilitating the use of EIS', in Proceedings of EUNIS\n2001: The Changing Universities: The Role of Technology, Berlin, March 2001, 270-2.\nRowley, J. (2001b), 'Encouraging and facilitating the use of electronic information services\n(EIS)', Educational Developments, February, 12-14.\nRust, R. and Lemon, K. (2001), 'E-service and the consumer', International Journal of\nElectronic Commerce, 5 (3), 85-101.\nSaracevic, T. and Kantor, P. (1997), 'Studying the value of library and information services,\nPart II', Journal of the American Society for Information Science, 48 (6), 543-63.\nSCONUL (1999), Information Skills in Higher Education, Task Force on Information\nSkills, London: SCONUL.\nUrquhart, C. (forthcoming), 'Critical incident technique and explicating interviewing in\nstudies of information behaviour', Library and Information Science Research.\nZeithaml, V., Parasuraman, A. and Malhotra, A. (2000), 'A conceptual framework for\nunderstanding e-service quality: implications for future research and managerial practice',\nMarketing Sciences Institute Working Paper.\n40\n"}