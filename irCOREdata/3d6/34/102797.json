{"doi":"10.1109\/ICMLC.2006.258538","coreId":"102797","oai":"oai:epubs.surrey.ac.uk:2335","identifiers":["oai:epubs.surrey.ac.uk:2335","10.1109\/ICMLC.2006.258538"],"title":"Detection and Management of Concept Drift","authors":["Mak, Lee-Onn","Krause, Paul"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2006","abstract":"The ability to correctly detect the location and derive the\\ud\ncontextual information where a concept begins to drift is\\ud\nessential in the study of domains with changing context. This paper proposes a Top-down learning method with the\\ud\nincorporation of a learning accuracy mechanism to efficiently detect and manage context changes within a large dataset. With the utilisation of simple search operators to perform convergent search and JBNC with a graphical viewer to derive context information, the identified hidden context are shown with the location of the disjoint points, the contextual attributes that contribute to the concept drift, the graphical output of the true relationships between these attributes and\\ud\nthe Boolean characterisation which is the context","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":"Institute of Electrical and Electronics Engineers","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:epubs.surrey.ac.uk:2335<\/identifier><datestamp>\n      2017-10-31T14:05:12Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><setSpec>\n      6469766973696F6E733D656E67616E64706879736963616C736369656E636573:656C656374726F6E6963656E67696E656572696E67:63637372<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/epubs.surrey.ac.uk\/2335\/<\/dc:relation><dc:title>\n        Detection and Management of Concept Drift<\/dc:title><dc:creator>\n        Mak, Lee-Onn<\/dc:creator><dc:creator>\n        Krause, Paul<\/dc:creator><dc:description>\n        The ability to correctly detect the location and derive the\\ud\ncontextual information where a concept begins to drift is\\ud\nessential in the study of domains with changing context. This paper proposes a Top-down learning method with the\\ud\nincorporation of a learning accuracy mechanism to efficiently detect and manage context changes within a large dataset. With the utilisation of simple search operators to perform convergent search and JBNC with a graphical viewer to derive context information, the identified hidden context are shown with the location of the disjoint points, the contextual attributes that contribute to the concept drift, the graphical output of the true relationships between these attributes and\\ud\nthe Boolean characterisation which is the context.<\/dc:description><dc:publisher>\n        Institute of Electrical and Electronics Engineers<\/dc:publisher><dc:date>\n        2006<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/epubs.surrey.ac.uk\/2335\/1\/SRF002343.pdf<\/dc:identifier><dc:identifier>\n          Mak, Lee-Onn and Krause, Paul  (2006) Detection and Management of Concept Drift   Proceedings of the Fifth International Conference on Machine Learning and Cybernetics.  pp. 3486-3491.      <\/dc:identifier><dc:relation>\n        http:\/\/ieeexplore.ieee.org\/xpls\/abs_all.jsp?arnumber=4028674&tag=1<\/dc:relation><dc:relation>\n        10.1109\/ICMLC.2006.258538<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/epubs.surrey.ac.uk\/2335\/","http:\/\/ieeexplore.ieee.org\/xpls\/abs_all.jsp?arnumber=4028674&tag=1","10.1109\/ICMLC.2006.258538"],"year":2006,"topics":[],"subject":["Article","PeerReviewed"],"fullText":"Proceedings of  the Fifth International Conference on Machine Learning and Cybernetics, Dalian, 13-16 August 2006 \n1-4244-0060-0\/06\/$20.00\u00a92006 IEEE \n3486 \nDETECTION & MANAGEMENT OF CONCEPT DRIFT \nLEE-ONN MAK, PAUL KRAUSE \nDepartment of Computing, School of Electronics & Physical Sciences, University of Surrey, UK \nE-MAIL: l.mak@surrey.ac.uk, p.krause@surrey.ac.uk \nAbstract: \nThe ability to correctly detect the location and derive the \ncontextual information where a concept begins to drift is \nessential in the study of domains with changing context. This \npaper proposes a Top-down learning method with the \nincorporation of a learning accuracy mechanism to efficiently \ndetect and manage context changes within a large dataset. \nWith the utilisation of simple search operators to perform \nconvergent search and JBNC with a graphical viewer to derive \ncontext information, the identified hidden context are shown \nwith the location of the disjoint points, the contextual \nattributes that contribute to the concept drift, the graphical \noutput of the true relationships between these attributes and \nthe Boolean characterisation which is the context. \nKeywords: \nConcept drift; context; context derivation; Bayesian \nNetwork Classifiers \n1. Introduction \n The ability to correctly detect the location and derive \nthe contextual information where a concept begins to drift \nis essential in the study of domains with changing context. \nFor example, in the specific case of studying virus \nbehaviour, marketing profiles, medical diseases and more, \nthe correct location of changes can help to produce \ninformation that justifies and explains the change. \nIn the area of detecting and managing context changes, \nwe extend the work of Widmer (METAL) [6] and Harries \n(SPLICE) [2] and resolve some limitations exhibited by \nthese two systems. METAL(B) uses a Na\u00efve Bayes \nclassifier [1] as the base learner and a statistical method to \nidentify the contextual attributes. Due to the limitations of \nthe Na\u00efve Bayes classifier (the assumption that all attributes \nare conditionally independent), this classifier can only be \nused as a black box classifier. The outcome of the learning \nprocess is a graph of drifting locations. The characteristics \nor information at the points of concept drift are not clearly \nshown.  \nSPLICE is an offline learner that uses the C4.5 \nalgorithm to perform the initial clustering. The \ndisadvantage of SPLICE is that SPLICE can have poor \nconvergence and has no proof of convergence in some \ndomains. In addition, due to the use of clustering techniques, \nSPLICE has no notion of overlapping contexts. Further, \nSPLICE provides no information about the properties of the \nidentified hidden context. \nThis paper proposes a Top-down learning method with \nthe incorporation of a learning accuracy mechanism to \nefficiently detect and manage context changes within a \nlarge dataset. With the utilization of simple search operators \nto perform convergent search and JBNC with a graphical \nviewer to derive context information, the identified hidden \ncontext are shown with the location of the disjoint points, \nthe contextual attributes that contribute to the concept drift, \nthe graphical output of the true relationships between these \nattributes and the Boolean characterisation which is the \ncontext. \n2. Learning with Single Context \nA stable concept is a concept that holds true for some \nperiod of time. For each collection of data, there exists a \nhidden context. The group of data instances that contributes \nto the concept can be considered as a data cluster where the \ninfra similarity is high. Therefore, the self-accuracy should \nbe 100% accurate if no noise is present in the concept. To \nhandle noise or irrelevant instances, an allowable noise \nlimit is used in a self-accuracy test. \nFor example, the STAGGER dataset consists of 3 data \nsets and each dataset has a hidden context. If we join the 3 \ndatasets and learn with JBNC_SFAND [5], the \nself-accuracy for the total 79 instances is 64.6% with 28 \nmisclassified instances. However, if the datasets are learned \nseparately, we achieve 100% for each dataset. \nTherefore, this paper is predicated on the principle that \nthe self-accuracy of a dataset that has a single context \nshould be 100%, or slightly less in the presence of noise. If \nthe dataset consists of multiple contexts, the learning \nalgorithm will be confused, with a corresponding reduction \nin the learning accuracy. \nAuthorized licensed use limited to: University of Surrey. Downloaded on May 17,2010 at 10:17:50 UTC from IEEE Xplore.  Restrictions apply. \nProceedings of  the Fifth International Conference on Machine Learning and Cybernetics, Dalian, 13-16 August 2006 \n3487 \n3. Context Derivation \nTo illustrate the method in deriving context, we give a \nbrief description of the techniques. For full details of the \nexample used, please refer to [4]. \nJBNC [3], a Java toolkit for training, testing and \napplying Bayesian Network classifiers, was designed and \nwritten by Sacha [5].  \nIn this paper, JBNC is selected as the underlying \nlearner as it overcomes the limitation of Na\u00efve Bayes \nclassifier [4], in producing a precise network structure, \nwhich is useful in formulating context. The JBNC SAND \noperator discards attributes that are not determined to be \ndependent on the class variable before applying the \nAUGMENTER operator. The graphical outcome is a set of \nrelevant attributes, the arcs that linked the attributes, and \nthe attributes\u2019 value with reference to the probability tables. \nIn this paper, context is represented by a Boolean \ncharacterisation, using Boolean operators to represent the \nconjunctive or disjunctive nature of the learnt \nrepresentation. There can be \u201cAND\u201d, \u201cOR\u201d or both \nconditions in representing the underlying context of the \ndataset. From the graphical output of the learnt Bayesian \nnetwork structures, the identification of the conjunction \n\u201cAND\u201d and disjunction \u201cOR\u201d relationship is as follows: \n \n \n \n \n \n                                                               \n \n             (a)                                (b) \nFigure 1. (a) The \u201cAND\u201d relationship between attributes, \n   (b) The \u201cOR\u201d relationship between attributes \n \nIn Figure 1a, the Bayesian network can model a \nconcept represented as \u201cA AND B\u201d. From the diagram, \nthere are arcs connected from the class node to each \nattribute node. To portray the \u201cAND\u201d relationship, there is \nno direct arc from attribute A to attribute B or vice versa. \nThis means that there is no dependency between the \nattribute nodes of A and B. This captures the following \ncases: if C is true, then A is true; if C is true, then B is true. \nThat is, if C is true, then both A and B are true. \nHowever, if the preferred representation of a concept is \n\u201cA OR B\u201d (Figure 1b), then we must place a direct \ndependency between the attribute nodes of A and B. This is \ncapturing the following cases: if A is false, then B must be \ntrue; if B is false, then A must be true.  \n \n \n \n \n \n \nFigure 2. The learnt network structure of Weather dataset \n \nFigure 2 shows the learnt network structure of the \nWeather datasets. From the figure, there are arcs that link \nHumidity to Outlook and Outlook to Windy which \nsymbolises the \u201cOR\u201d relationship. There is no direct link \nbetween Humidity and Windy which symbolises the \n\u201cAND\u201d relationship. From the probability tables in [4], the \nderived context represented by Figure 2 is (Humidity = \nNormal AND Windy = False) OR Outlook = Overcast.  \n4. Brief Description of Process Flow \nTo detect concept drift and identify context \ninformation, we present an efficient method which \novercomes the limitation of SPLICE and METAL. \n \nFigure 3. The process of detecting concept drift \n \nFigure 3 illustrates the general process of detecting \nconcept drift over time. The datasets that feed to the \nlearning system are collected over time and may contain \nhidden changes of context. The learning system seeks to \ndetect the \u201cdisjoint\u201d points within the dataset where the \nconcept begins to drift. This method uses the \nJBNC_SFAND classifier, the main search operators and the \nrefinement operators to detect the actual disjoint points \nwithin the dataset. The outcomes of the learning are the \nclusters of data instances, which are delimited by the \nlocation of the disjoint points, and their Boolean \ncharacterisations which represent the context. \nThere are 2 learning methods created in our research \nwork in [4]. For this paper, we focus the presentation on the \n\u201cTop-down\u201d learning method. \nAuthorized licensed use limited to: University of Surrey. Downloaded on May 17,2010 at 10:17:50 UTC from IEEE Xplore.  Restrictions apply. \nProceedings of  the Fifth International Conference on Machine Learning and Cybernetics, Dalian, 13-16 August 2006 \n3488 \n5. Search Operators \nThe main task of the learning algorithm is to \nefficiently search for the disjoint point where the concept \nbegins to drift. This section proposes 3 operators to perform \nthe main and refinement searches.  \nAs the main search operator, the rate reduction \noperator seeks to achieve quick convergence to the \napproximate disjoint point within a large dataset. The \noperator performs an instance reduction based on a \nuser-specified rate. The reduction rate ranges from 0 to \n100%. For example, if there are 100 instances in the dataset, \nafter the rate reduction of 50%, the leftover instances will \nbe 1 to 50 and the instances from 51 to 100 are removed.  \nThe misclassified removal operator searches the \napproximate disjoint point by removing the number of \nmisclassified instances from the present instances that \nproduce the network structure. This operator activates after \nthe main operator has completed the initial searching. The \noperator is used to remove the outliers or irrelevant \ninstances that do not belong to the present group of \ninstances that formulate that concept. The condition to \nactivate this operator is that the present stage of learning \nhas achieved accuracy below the allowable noise limit but \nthe previous stage had achieved error rate greater than the \nallowable noise limit. \nThe step operator refines the result by making a step \nforward search from the approximate disjoint point. This \noperator is activated after the main operator or the \nmisclassified removal operator has completed the \nconvergent search. This operator is used to perform a single \nstep or user-specified step to reach the actual disjoint point. \n6. Top-down Method \nThe Top-down learning method uses the various \noperators mentioned above to search for the actual disjoint \npoints within a dataset according to a user-specified \nreduction rate. By \u201cTop-down\u201d we mean that the whole \ndataset is involved in the initial learning and the subsequent \nreduction of instances is done from the end of the dataset. \nTable 1 shows the learning algorithm. \nFrom Figure 4, the search begins with the total \ninstances n. At this step, the JBNC_SFAND classifier is \ncalled to learn the dataset and to perform a self-accuracy \ntest. If the misclassification rate (error) is greater than the \nallowable limit (allow), the algorithm engages the rate \nreduction operator to perform instance reduction operation. \nAt each iteration, the dataset is reduced by r% where r is \nthe user-specified reduction rate. For example, the dataset \nafter reduction at iteration 1 is r x n instances. At the next \niteration, the dataset is further reduced to r x r x n \ninstances. \n \nTable 1. The Top-down Learning Algorithm \nInput: \n\u2022 Dataset with instances n \n\u2022 Allowable error rate, allow \n\u2022 User-specified rate reduction r \n\u2022 Step increment 1 \n \nAlgorithm: \nStage 1: Convergent Search \n\u2022 Begins with dataset of instances n \n\u2022 Use JBNC_SFAND to classify the dataset and \nperform self-accuracy test \n\u2022 If error > allow, call rate reduction operator \n\u2022 If error < allow, proceed to Stage 2 \nStage 2: Refinement Search \n\u2022 Perform check 1 \no Last iteration step m, error rate <= allow \no Previous iteration step m-1, error > \nallow \n\u2022 If check 1 is TRUE, call misclassified removal \noperator \n\u2022 If error < allow, proceed to Stage 3 \nStage 3: Fine-tuning Search \n\u2022 Perform check 2 \no Last iteration step m, error < allow \n\u2022 If check 2 is TRUE, call step operator \n\u2022 If error > allow, stop the search \n\u2022 Output \u201cContext x is found at instances y\u201d \nStage 4: Data Removal \n\u2022 Remove instances y from the beginning of \ninstances n \n\u2022 If n \u2013 y > 0, goto Stage 1 \n\u2022 If n \u2013 y = 0, stop the whole process \n \nOutput: \n\u2022 Location of stable concepts\u2019 disjoint points \n\u2022 Graphical output of learnt Bayesian network \nstructures \n\u2022 Boolean characterization of the hidden context \n \n \nFor the refinement search, in Figure 4, the rate \nreduction operator, with much iteration, reaches the \napproximate disjoint point by performing instances \nreduction. However, the actual disjoint point is located \nsomewhere after the approximate disjoint point. The \nmisclassified removal operator is used at this point to \nAuthorized licensed use limited to: University of Surrey. Downloaded on May 17,2010 at 10:17:50 UTC from IEEE Xplore.  Restrictions apply. \nProceedings of  the Fifth International Conference on Machine Learning and Cybernetics, Dalian, 13-16 August 2006 \n3489 \nfurther converge to the right location. The misclassified \nremoval operator attempts to reach the actual disjoint point \nby removing the number of misclassified instances from the \ndataset. This process brings the location point closer to the \nactual disjoint point but not the actual point. The iteration \nceases when the error rate goes below the allowable noise \nlimit. \n \nFigure 4. The Top-down searching process \n \nTo \u201cfine-tune\u201d to the actual disjoint point, the step \noperator is employed to search upwardly in a step-by-step \nmanner to reach the actual disjoint point. At each iteration, \nthe dataset is increased by 1 instance or a user-specified \nnumber of instances which is greater than 1. Once the error \nrate exceeds the allowable noise limit, iteration ceases and \nthe actual disjoint point is the number of instances \nbelonging to the previous step. \n \n \n \nFigure 5. The searching for next context with the removal \nof a instances \n \nWith the final search performed by the step operator, \nthe actual disjoint point for Context 1 is found. To begin the \nnext context search, the pointer line starts to move up to \ninstances a as shown in Figure 5 where a is the 1st disjoint \npoint. This means that the instances that belong to Context \n1 are removed from the actual dataset. The next search is \ndone with instances n-a. The algorithm performs a test to \nensure that the number of instances for the next search is \ngreater than zero. If the test is true, the process repeats stage \n1 to 3. The search iteration continues until the actual \ndisjoint point is found with b instances. The search process \ncontinues as long as the number of instances is greater than \nzero. If the number of instances equals to zero, the search \nprocess ceases. \nTo ensure good accuracy in locating the right disjoint \npoints, the parameters used in the learning have to be \nhandled efficiently.  \nThe allowable noise limit is used to control the level of \nnoise present in the dataset. No one knows a priori how \nmuch noise is present in the dataset that is being tested. \nUsually, the dataset is tested a few times with the learning \nmethod to investigate the noise level. The location of the \ndisjoint points varies according to the allowable noise limit \nused in the search. After each round of testing, the \nallowable noise limit needs to be adjusted accordingly.  \nThe reduction rate specified by the user has to be \nrealistic. The recommended values for the reduction rate are \nfrom 20% to 50%. The smaller the reduction, the more \naccurately and confidently the approximate disjoint points \nare located. \nFinally, the step increment for the refinement search is \nrecommended to be 1. If the increment is too large, the \nactual disjoint point cannot be found. With an increase of 1 \ninstance, the actual disjoint point can be located easily. \n7. Example with STAGGER dataset \nIn this section, the STAGGER dataset is used to verify \nthe Top-down learning method. This simple dataset consists \nof 79 instances and 3 hidden contexts where 26 instances \nbelong to context 1, 27 instances belong to context 2 and 26 \ninstances belong to context 3. \n \nSearching method: Top-down \nJBNC: SFAND, LC, STAGGER Dataset \nAllowable noise tolerance = 0 \nReduction rate = 50% \n*NG means \u201cNo Good\u201d \nSearch No 1.  \nTotal instances = 79 \nSte\np \nAction Inst Accuracy\/ \nMisclassified \nRemk\n1  79 65% \/ 28 NG \n2 Reduce 50% 40 82.5% \/ 7 NG \n3 Reduce 50% 20 100% \/ 0 OK \n \n \n \nAuthorized licensed use limited to: University of Surrey. Downloaded on May 17,2010 at 10:17:50 UTC from IEEE Xplore.  Restrictions apply. \nProceedings of  the Fifth International Conference on Machine Learning and Cybernetics, Dalian, 13-16 August 2006 \n3490 \nRefinement Search: \n4 Remove 7 inst \nfrom 40 inst in \nstep 2 \n33 84.9% \/ 5 NG \n5 Remove 5 inst \nfrom 33 inst in \nstep 4 \n28 92.9% \/ 2 NG \n6 Remove 2 inst \nfrom 28 inst in \nstep 5 \n26 100% \/ 0 OK \nFine-Tuning Search: \n7 Add 1 inst to \n26 inst in step \n6 \n27 96.3% \/ 1 NG \nStep 6: misclassified rate is 0 <= allowable \n*Context 1 found with <26> instances at <100%> \naccuracy*  \n**Remove 26 instances from dataset**  \nSearch No 2. \nTotal instances = 79-26 = 53 \nSte\np \nAction Inst Accuracy\/ \nMisclassified \nRemk\n1  53 56.6% \/ 23 NG \n2 Reduce 50% 27 100% \/ 0 OK \nRefinement Search: \n3 Remove 23 \ninst from 53 \ninst in step 1 \n30 93.3% \/ 2 NG \nFine-Tuning Search: \n4 Add 1 inst to \n27 inst in step \n2 \n28 96.4% \/ 1 NG \nStep 2: misclassified rate is 0 <= allowable \n*Context 2 found with <27> instances at <100%> \naccuracy*  \n**Remove 27 instances from dataset** \nSearch No 3. \nTotal instances = 53-27 = 26 \nSte\np \nAction Inst Accuracy\/ \nMisclassified \nRemk\n1  26 100% \/ 0 ok \nStep 1: misclassified rate is 0 <= allowable \n*Context 3 found with <26> instances with <100%> \naccuracy*  \n**Remove 26 instances from dataset** \n***Total Instances = 0*** \n****End of contexts search**** \n \nNetwork Structures Output & Derived Context \nConcept drifts from instances 1 to 26, 27 to 53, 54 to 79 \nwith context information as: \n \n \n \n \n \n8. Managing context Accuracy \nIn order to manage context accuracy in learning \nreal-life and noisy datasets, the irregularities occurring \nduring learning have to be handled efficiently. The \nTop-down learning method might experience that the main \noperator fails to converge to the approximate disjoint point \nwith the possible scenarios: \n\u2022 The error rate remains above the allowable \nnoise limit throughout the entire search.  \n\u2022 The search operators have missed the \napproximate disjoint point. \nUnder these situations, the learning algorithm will \nbehave as if an irregularity has occurred and the iteration \nceases. Based on the error rate collected at each learning \nstep, the algorithm engages the interpolation operator to \nperform a further search for the approximate disjoint point.  \nThe interpolation operator is a special operator that is \nonly activated when irregularities occur during the search. \nThis operator uses the equation: (x + y)\/2 where x and y are \nthe number of instances at step n and n- 1. Both error rates \nat these steps are greater than the allowable noise limit. The \niteration continues as long as the error rate exceeds the \nallowable noise limit. \n9. Further Experiments \nNext, to verify the performance of the proposed \nlearning method in detecting concept drift in a real-life \nenvironment; we apply them to the Music Chord datasets. \nIn [4], the learning method had successfully applied to both \nMusic and Vowel datasets. For further results and \nexplanation of both processes, please refer to [4].  \nWidmer [6] uses the Music dataset to predict online \nwhat chord should accompany the next note in a given \nmelody. The task is to correctly predict one of three classes: \ntonic harmony, dominant or other. The data used are the \nmelodies of Franz Schubert\u2019s German Mass; a collection of \n8 songs of varying length. After putting all the songs \ntogether for the context search, there are 553 melody notes \nContext 1: -> \ncolor = green or \nshape = circular \n \nContext 2: ->  \nsize = medium or \nsize = large \nContext 3:     \n(size = small) or \n(color = red and \nshape = square) \nAuthorized licensed use limited to: University of Surrey. Downloaded on May 17,2010 at 10:17:50 UTC from IEEE Xplore.  Restrictions apply. \nProceedings of  the Fifth International Conference on Machine Learning and Cybernetics, Dalian, 13-16 August 2006 \n3491 \nin total.  \nFirst, the dataset is tested if only one context lies \nwithin the entire dataset. If there is more than one context \npresent in the dataset, the dataset is tested by parts by the \nrate reduction operator in a Top-down manner. With a 50% \nrate reduction, the 553 instances used in search 1 was tested, \nand then followed by 227, 139, 70 and 35 until the error \nrate is within the allowable noise limit. Search 1, 2, 4, 5, 6, \n7 and 8, with the exception of 3, have a straightforward \nerror convergent.  \nAfter the rate reduction operator reaches the initial \ndisjoint point, the misclassified removal operator is used to \nbring the location closer to the actual disjoint point. The \nmisclassified removal operator removes the misclassified \ninstances from the present instances. Search 1, 3, 4 and 7 \nhave a straightforward processing. Search 2 and 6 need \nmore iteration before reaching the approximate disjoint \npoint. Hence, the misclassified removal operator is a \npowerful tool to remove redundant instances and \nconvergence is assured.  \nThe Top-down method handles the irregularity that \noccurred during the search efficiently by engaging the \ninterpolation operator. In search 3, after much iteration \nperformed by the rate reduction operator, the approximate \ndisjoint point cannot be obtained as the error rate begins to \ndiverge after the initial convergence. The main search by \nthe rate reduction operator has overlooked the initial \ndisjoint point. By employing the interpolation operator, \nremedial work is performed and the tentative point of \nconvergence is found. Later, with the misclassified removal \noperator, the approximate disjoint point is found. \n \nTable 2. The comparison of the searched locations closeness \nto the actual results \n \n   \n \n \n \n \n \n \n \nFrom the results produce from search 1 to 8, we can \nobserve that the search algorithms are simple and \nconvergence can be achieved easily. With the incorporation \nof learning accuracy mechanisms, the irregularities can be \novercome easily.  \nTable 2 shows the result of the search for disjoint \npoints as compared to the actual locations.  \n10. Conclusions & Discussions \n With the development and integration of algorithms, \nthis paper proposes a rich learning package to identify \ncontext and detect the locations where the concepts begin to \ndrift. So far, the learning method produced encouraging \nresults with the Weather and Music datasets. \nAs compared with the METAL learning system, the \nJBNC_SFAND classifier outperforms the Na\u00efve Bayes \nclassifier in producing a precise and correct network \nstructure where the context information can be identified \neasily. With the use of the nodes discarding facility, the \nJBNC provides the relevant or contextual attributes after \nlearning without using any external statistical method as in \nMETAL. \nThe learning method has also resolved some of the \nlimitations demonstrated by the SPLICE learning system. \nSPLICE suffers from convergence problem due to the use \nof clustering techniques. The Top-down method efficiently \nconverged to the right disjoint point with the use of simple \noperators. The simple operators had also been demonstrated \nto be effective on the real-world Music dataset. With the \nuse of allowable noise limits, the Top-down method \naddressed the issue of overlapping instances. The locations \nthat separated different Music Chord contexts are close to \nthe actual results (Table 2). With the use of JBNC and the \ngraphical viewer, the properties of the identified hidden \ncontext are shown with the location of the disjoint point, the \ncontextual attributes that contribute to the concept drift, the \ngraphical output of the true relationships between these \nattributes and the Boolean characterisation which is the \ncontext. \nReferences \n[1] R. O. Duda and P. E. Hart, Pattern Classification and Scene \nAnalysis, John Wiley & Sons, New York, 1973. \n[2] M. B. Harries, C. Sammut and K. Horn, \u201cExtracting Hidden \nContext\u201d, Machine Learning, Vol 32, pp. 101-126, 1998. \n[3] JBNC, Bayesian Network Classifier Toolbox, \nhttp:\/\/jbnc.sourceforge.net. \n[4] Lee-Onn Mak, \u201cIdentification and Management of Context \nwith A Bayesian Network Classifier\u201d, Doctoral dissertation, \nDepartment of Computing, School of Electronics & Physical \nScience, University of Surrey, July, 2004. \n[5] J. P. Sacha, L. Goodenday, K. J. Cios, \u201cBayesian Learning \nFor Cardiac SPECT Image Interpretation\u201d, Artificial \nIntelligence in Medicine, Vol 26, pp. 109-143, 2002. \n[6] Gerhard Widmer, \u201cTracking Context Changes through \nMeta-Learning\u201d, Machine Learning, Vol 27, No. 3, pp. \n259-286, 1997. \nChord Actual Searched Diff \n1 63 62 1 \n2 133 139 6 \n3 206 212 6 \n4 248 254 6 \n5 293 329 36 \n6 379 368 -11 \n7 440 456 16 \n8 553 553 0 \nAuthorized licensed use limited to: University of Surrey. Downloaded on May 17,2010 at 10:17:50 UTC from IEEE Xplore.  Restrictions apply. \n"}