{"doi":"10.1007\/s00453-007-9081-y","coreId":"65740","oai":"oai:dro.dur.ac.uk.OAI2:4935","identifiers":["oai:dro.dur.ac.uk.OAI2:4935","10.1007\/s00453-007-9081-y"],"title":"On the stability of dynamic diffusion load balancing.","authors":["Berenbrink,  P.","Friedetzky,  T.","Martin,  R."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2008-02-01","abstract":"We consider the problem of dynamic load balancing in arbitrary (connected) networks on n nodes. Our load generation model is such that during each round, n tasks are generated on arbitrary nodes, and then (possibly after some balancing) one task is deleted from every non-empty node. Notice that this model fully saturates the resources of the network in the sense that we generate just as many new tasks per round as the network is able to delete. We show that even in this situation the system is stable, in that the total load remains bounded (as a function of n alone) over time. Our proof only requires that the underlying \u201ccommunication\u201d graph be connected. (It of course also works if we generate less than n new tasks per round, but the major contribution of this paper is the fully saturated case.) We further show that the upper bound we obtain is asymptotically tight (up to a moderate multiplicative constant) by demonstrating a corresponding lower bound on the system load for the particular example of a linear array (or path). We also show some simple negative results (i.e., instability) for work-stealing based diffusion-type algorithms in this setting","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/65740.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/4935\/1\/4935.pdf","pdfHashValue":"1de9d7c2eff51b30661af43790e9970fb9e5fd8a","publisher":"Springer","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:4935<\/identifier><datestamp>\n      2011-09-13T13:09:13Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        On the stability of dynamic diffusion load balancing.<\/dc:title><dc:creator>\n        Berenbrink,  P.<\/dc:creator><dc:creator>\n        Friedetzky,  T.<\/dc:creator><dc:creator>\n        Martin,  R.<\/dc:creator><dc:description>\n        We consider the problem of dynamic load balancing in arbitrary (connected) networks on n nodes. Our load generation model is such that during each round, n tasks are generated on arbitrary nodes, and then (possibly after some balancing) one task is deleted from every non-empty node. Notice that this model fully saturates the resources of the network in the sense that we generate just as many new tasks per round as the network is able to delete. We show that even in this situation the system is stable, in that the total load remains bounded (as a function of n alone) over time. Our proof only requires that the underlying \u201ccommunication\u201d graph be connected. (It of course also works if we generate less than n new tasks per round, but the major contribution of this paper is the fully saturated case.) We further show that the upper bound we obtain is asymptotically tight (up to a moderate multiplicative constant) by demonstrating a corresponding lower bound on the system load for the particular example of a linear array (or path). We also show some simple negative results (i.e., instability) for work-stealing based diffusion-type algorithms in this setting. <\/dc:description><dc:publisher>\n        Springer<\/dc:publisher><dc:source>\n        Algorithmica, 2008, Vol.50(3), pp.329-350 [Peer Reviewed Journal]<\/dc:source><dc:date>\n        2008-02-01<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:4935<\/dc:identifier><dc:identifier>\n        issn:0178-4617<\/dc:identifier><dc:identifier>\n        issn: 1432-0541<\/dc:identifier><dc:identifier>\n        doi:10.1007\/s00453-007-9081-y<\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/4935\/<\/dc:identifier><dc:identifier>\n        http:\/\/dx.doi.org\/10.1007\/s00453-007-9081-y<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/4935\/1\/4935.pdf<\/dc:identifier><dc:rights>\n        The original publication is available at www.springerlink.com<\/dc:rights><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["issn: 1432-0541","0178-4617"," 1432-0541","issn:0178-4617"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2008,"topics":[],"subject":["Article","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n24 October 2008\nVersion of attached file:\nAccepted Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nBerenbrink, P. and Friedetzky, T. and Martin, R. (2008) \u2019On the stability of dynamic diffusion load\nbalancing.\u2019, Algorithmica., 50 (3). pp. 329-350.\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1007\/s00453-007-9081-y\nPublisher\u2019s copyright statement:\nThe original publication is available at www.springerlink.com\nAdditional information:\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\n Use policy \n \nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without \nprior permission or charge, for personal research or study, educational, or not-for-profit purposes \nprovided that : \n \n\u0083 a full bibliographic reference is made to the original source \n\u0083 a link is made to the metadata record in DRO \n\u0083 the full-text is not changed in any way \n \nThe full-text must not be sold in any format or medium without the formal permission of the copyright \nholders.  \n \nPlease consult the full DRO policy for further details. \n \nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom \nTel : +44 (0)191 334 2975 | Fax : +44 (0)191 334 2971 \nhttp:\/\/dro.dur.ac.uk \nDurham Research Online \n Deposited in DRO:\n24 October 2008\nVersion of attached file:\nAccepted\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nBerenbrink, P. and Friedetzky, T. and Martin, R. (2008) 'On the stability of dynamic diffusion load balancing.',\nAlgorithmica, 50 (3), pp.\u0000329-350.\nFurther information on publishers website:\nhttp:\/\/dx.doi.org\/10.1007\/s00453-007-9081-y\nPublishers copyright statement:\nThe original publication is available at www.springerlink.com\nOn the Stability of Dynamic Diffusion Load Balancing\u2217\nPetra Berenbrink\u2020\nSimon Fraser University\nSchool of Computing Science\nTom Friedetzky\nUniversity of Durham\nDepartment of Computer Science\nRussell Martin\u2021\nUniversity of Liverpool\nDepartment of Computer Science\nAbstract\nWe consider the problem of dynamic load balancing in arbitrary (connected) networks on\nn nodes. Our load generation model is such that during each round, n tasks are generated on\narbitrary nodes, and then (possibly after some balancing) one task is deleted from every non-\nempty node. Notice that this model fully saturates the resources of the network in the sense that\nwe generate just as many new tasks per round as the network is able to delete. We show that\neven in this situation the system is stable, in that the total load remains bounded (as a function\nof n alone) over time. Our proof only requires that the underlying \u201ccommunication\u201d graph be\nconnected. (It of course also works if we generate less than n new tasks per round, but the major\ncontribution of this paper is the fully saturated case.) We further show that the upper bound\nwe obtain is asymptotically tight (up to a moderate multiplicative constant) by demonstrating\na corresponding lower bound on the system load for the particular example of a linear array\n(or path). We also show some simple negative results (i.e., instability) for work-stealing based\ndiffusion-type algorithms in this setting.\n1 Introduction\nThe use of parallel and distributed computing is established in many areas of science, technology,\nand business. One of the most crucial parameters of parallel machines is the efficient utilization of\nresources. Of greatest importance here is an even distribution of the workload among the processors.\nIn particular applications exposing some kind of \u201cirregularity\u201d require the use of a load balancing\nmechanism.\nA well known and much studied load balancing approach is the so-called diffusion load bal-\nancing, first introduced by Cybenko and Boillat ([11], [10]). The algorithm works in synchronized\nrounds. The basic idea is that in every round, every processor p balances load with all its neighbors\n(independently, i.e., pair-wise). Let `p be the load of p and `q the load of some of p\u2019s neighbor\nq, and let dp denote the degree of the vertex p. One popular method in the discrete setting has\np transferring max{0, b(`p \u2212 `q)\/(max{dp, dq} + 1)c} tasks to q in a given round. Some of many\nadvantages of diffusion-type algorithms are the locality (no global knowledge regarding the overall\n\u2217A preliminary version of this paper entitled \u201cDynamic diffusion load balancing\u201d was published in Proc. 32nd\nInternational Colloquium on Automata, Languages and Programming (ICALP\u201905), Lecture Notes in Computer Science\n3580, Springer-Verlag, pp. 1386\u20131398.\n\u2020Supported by NSERC Discovery Grant 250284-2002.\n\u2021Supported by EPSRC grant \u201cDiscontinuous Behaviour in the Complexity of Randomized Algorithms.\u201d\n1\nload situation, or, in fact, anything except the strict neighborhood of any vertex is needed), its\nsimplicity, and its neighborhood preservation (tasks tend to stay close to the processors where they\nare generated, which may help to maintain small communication overhead).\nThe diffusion load balancing algorithm has been thoroughly analyzed for static scenarios, where\neach processor has some initial number of tasks, and the objective is to distribute this load evenly\namong the processors as quickly as possible. Much work has been done under the assumption that\nevery edge is only allowed to forward one task per round [15, 17, 21] or when a constant number of\ntasks can be passed by each processor [16]. We refer to these scenarios as token distribution prob-\nlems. In addition, [12, 14] have studied the diffusion algorithm where tasks can be split arbitrarily,\nwhile [13] assumes that they are indivisible. Muthukrishnan, Ghosh, and Schultz [19] examined\nfirst- and second-order schemes for \u201ccoarse\u201d load balancing (the goal being to reduce large dis-\ncrepancies between the loads on vertices). Rabani, Sinclair, and Wanka [22] also considered the\nproblem of coarse balancing. They model the load balancing process by a suitable Markov chain,\nand show this model is accurate until a certain threshold discrepancy is reached.\nIn contrast to the static case of load balancing and token distribution, in the dynamic setting\nduring each round new tasks are generated (in some manner) on the set of processors, load is\nbalanced amongst neighbors, then tasks are deleted from non-empty processors.\nMuch past work has studied the dynamic token distribution problem. Muthukrishnan and\nRajaraman [20] studied a dynamic version where processors can forward a single task in each\nround. They assume an adversarial load generation model. The adversary is allowed to generate\nand to delete tokens from the network in every round. The simple and elegant algorithm they\nconsider is due to [1]: A node sends a task to its neighbor if the load difference between them is\nat least 2\u2206 + 1, where \u2206 is the maximum degree of the underlying graph. They show that the\nsystem is stable if the load change in every subset S of the nodes minus a|S| is at most (1\u2212 \u000f)e(S)\nfor \u000f > 0. Here e(S) is the number of outgoing edges of S and a is the change in the average load.\nTheir system is said to be stable if the deviation of the load of any processor from the average load\ncan be bounded. Muthukrishnan and Rajaraman left open the question whether the system is also\nstable for \u000f = 0.\nAnshelevich, Kempe, and Kleinberg [4] gave a positive result for token distribution when \u000f = 0.\nThey showed that under the above load generation model no processor has more than average load\n\u00b1(2\u2206+1) \u00b7n. Anshelevich, et al. also showed how their result can be generalized for edges that can\nforward c tokens per time step. A node sends min{c, \u03c1} tasks to its neighbor if the load difference\nis at least 2\u2206c+ \u03c1. In this setting no processor has more than average load \u00b1(2\u2206 + 1)c \u00b7 n as long\nas the load change in every subset S of the nodes minus a|S| is at most c \u00b7 e(S). Additionally,\nthey showed that a generalization of the algorithm is stable for two distinct types of jobs, and they\nextended their results to related flow problems.\nIn [6, 7] Awerbuch and Leighton use a variant of the token distribution model under the as-\nsumption that tokens can be split into arbitrarily sized parts. They use a \u201cbalancing\u201d algorithm\nto approximate the multi-commodity flow problem with capacitated edges. Their method is an\niterative approach where flow is queued at the vertices of the graph. In each step, the commodity\nwhich has the largest excess is shipped from one vertex to another, and then new flow is injected\ninto the system. In this balancing process, edge capacities must always be respected. These edge\ncapacities are analogous to the restrictions on the number of tasks that can be passed over any\nsingle edge in the token distribution problems. Furthermore, their model does not actually allow\nfull use of those edge capacities, which is similar to the case in [20] where \u000f > 0 was required to\nensure stability. The work in [2] and [5] expands the results of Awerbuch and Leighton for packet\nrouting, but again in these cases only a constant number of tasks can be moved across any edge in\n2\na single time step.\nClearly the condition that processors can forward only a single task (or a constant number) per\nedge in each round significantly restricts the number and distribution of tasks that can be generated\non (or deleted from) processors in each round and still obtain a stability result. Thus, in the results\nof [20] and [4] some dependence on the quantity e(S) (or some measure of the \u201cedge expansion\u201d)\nis to be expected.\nAnagnostopoulos, Kirsch, and Upfal [3] consider the setting where there are no restrictions on\nthe number of tasks balanced between processors in a time step, and they allow a broad range of\ninjection models. Their protocol is similar to that studied in [16] for a static setting, but is not\nthe typical diffusion load balancing procedure. In their setting, in each step nodes are matched\nrandomly with adjacent neighbors and matched nodes equalize their load. Hence, every processor\nis only involved in a single load balancing action. They show that the system is stable as long as at\nmost wn\u03bb tasks (in expectation) are generated in a time interval of length w, where \u03bb < 1. Their\nproof method unfortunately cannot be generalized to the case of full saturation when \u03bb = 1, which\nis the main focus of this paper.\nIn a different approach in the dynamic setting, Berenbrink, Friedetzky, and Mayr [9] consider\na load balancing scheme that uses a \u201ccollision protocol\u201d (see also [18]) to resolve load balancing\nrequests amongst lightly- and heavily-loaded processors. They show stability with this balancing\nprotocol and a variety of randomized task generation models where each processor receives, in\nexpectation, strictly less than one newly generated task per round.\n1.1 Our Results\nIn this paper we present the first analysis of the simple diffusion scheme for the dynamic load\nbalancing problem that allows full saturation of the resources. We assume that n new tasks are\ngenerated per round and, after load balancing, every non-empty processor deletes one task each\nround. (With small modifications our proofs will carry through to the case when we generate at\nmost n tasks per round.) In contrast to [4] and [20], the newly generated tasks may be arbitrarily\ndistributed among the nodes of the network, regardless of any \u201cedge expansion\u201d type of condition\nas in those models. For example, the tasks may always be generated on the same processor, or all\ntasks may be generated on one processor but the processor can change from round to round, or\nalternatively, the tasks may be allocated at random each round. Note that, obviously, without load\nbalancing the total number of tasks in the system may grow unboundedly with time (in the worst\ncase, we generate n new tasks per step but delete only one).\nWe show that the system of processors is stable under the diffusion load balancing scheme and\nthe generation model described above. By stable, we mean that the total load in the system does\nnot grow with time. In particular, we show that the total system load can be upper-bounded by\nO(\u2206n3), where \u2206 denotes the maximum degree of the network. Furthermore, we present a simple,\nasymptotically matching lower bound when the network is a path.\nOur technique also captures a different scenario, similar to that in [4, 20], where stability is\ndefined in terms of deviation of any processor\u2019s load from the average. In this scenario we have\ntwo separate phases, the first where tasks are generated on and\/or deleted from nodes, and the\nsecond where tasks are then balanced amongst nodes. Let L\u00aft(S) denote the total load of the nodes\nin the set S after the task generation\/deletion phase, and Lt(S) denote the total load of S after\nthe balancing step at time t. Assume that the generation\/deletion phase satisfies the following\ncondition:\nL\u00aft(S)\u2212 Lt\u22121(S) \u2264 (avg(t)\u2212 avg(t\u2212 1)) \u00b7 |S|+ \u03c1 for every subset S,\n3\nwhere avg(t) denotes the average system load in step t. Then the total load of S can be bounded\nby |S| \u00b7 avg(t) + 5\u2206n\u03c1.\nFor both proofs of our results we use a potential function. Although the potential function we\nuse looks similar to the one used in [4], the proof technique is very different. The proof method\nin [4] very much relies upon the restriction of their generation\/deletion model, where the number\nof tasks inserted into\/deleted from a set S is bounded by a function of e(S), the number of edges\nthat join the set S to its complement S\u00af. This, together with the bounded capacities on the edges of\nthe graph, allows for a direct analysis of how the loads of sets might change in a single step of their\nprocess. The arbitrary distribution of tasks in our generation model and the unrestricted capacity\nof the edges in our network (i.e. unknown bounds on load transferred into a set S in a single step)\ndoes not allow us to directly obtain similar results, so we need a different proof to show stability\nunder our model.\nIn the final part of our paper we discuss a different method of load balancing, one which is\ncommonly referred to as work stealing. In this framework, processors that are empty after task\ngeneration will balance with processors that are not empty, but no other balancing actions are\npermitted. We show that for this work-stealing protocol there are graphs for which the system\ncannot be stable for a significant class of generation parameters. These results show that restricting\nbalancing actions to empty processors is not sufficient in general.\nIn contrast, Berenbrink, Friedetzky, and Goldberg [8] showed stability of a work stealing algo-\nrithm under a load generation model that is similar to many of those already mentioned. They\nconsider a flexible distribution of n generators among the nodes of the network, where each gener-\nator is allowed to generate a task with probability strictly smaller than one. In this setting a very\nsimple, parameterized work-stealing algorithm achieves stability (in our sense) for a wide range of\nparameters. The important point to note is that their model applies only when the set of processors\n(and their communication linkages) forms a complete graph, and their results only hold for the case\nwhere strictly less than n tasks (in expectation) are generated during any time step.\nOur model is defined in the next section, and the formal definition of the diffusion approach to\nload balancing is given following that.\n1.2 Our Model\nOur parallel system is modeled by a connected graph G = (V,E). The nodes V of the graph\nmodel our processors P = {P1, . . . , Pn} , and the edges E model the underlying communication\nstructure. If two nodes are connected with each other, this means that the processors modeled by\nthe nodes can communicate directly. For us, this means that they are allowed to exchange tasks.\nNodes not connected by an edge have to communicate via message passing. Furthermore, let \u2206\nbe the maximum degree of the graph. We assume that each processor maintains a queue in which\nyet-to-be-processed tasks are stored. One round looks as follows:\n1. n generators are arbitrarily distributed over the processors, and each generator generates one\ntask at the beginning of every time round. For 1 \u2264 i \u2264 n, let kti = j if generator i is allocated\nto processor Pj in round t, and kti = 0 if the generator is not allocated to any processor in\nthat round.\n2. Every processor balances its load with some or all its neighbors in the network (according to\na well-defined scheme for doing this operation).\n3. Every non-empty processor deletes one task.\n4\nLet \u02c6`ti be the load of Pi directly after the load deletion phase in round t. A system is called\nstable if the number of tasks L\u02c6t(P) = \u2211ni=1 \u02c6`ti that are in the system at the end of round t does not\ngrow with time, i.e. the total load L\u02c6t(P) is bounded by a number that might depend on n, but not\non the time t.\nWe will mainly focus on one load balancing method called the diffusion approach. Every pro-\ncessor is allowed to balance its load with all its neighbors. As mentioned previously, we briefly\nconsider a second approach in Section 4 where only empty processors are allowed to take load from\ntheir non-empty neighbors. We call this second method the work stealing approach.\nDiffusion approach. We begin with a detailed description of the first approach, an integral\nvariant of the First-Order Diffusion scheme from [19]. Let \u00af`ti be the load of processor Pi directly\nbefore the load balancing phase, and `ti the load directly after the load balancing phase. Let \u03b1\nt\ni,j\nbe the load that is to be sent from Pi to Pj in round t for (i, j) \u2208 E (\u03b1ti,j = 0 otherwise). Recall\nthat di denotes the degree of vertex i. Then \u03b1i,j and `i are calculated as follows:\n\u03b1ti,j := max\n{\n0,\n\u230a\n\u00af`t\ni \u2212 \u00af`tj\n2 max{di, dj}\n\u230b}\n`ti := \u00af`\nt\ni \u2212\n\u2211\n(i,j)\u2208E\n\u03b1ti,j +\n\u2211\n(j,i)\u2208E\n\u03b1tj,i.\nTo compute \u02c6`ti, the load of processor Pi after load deletion, it remains to subtract one if `\nt\ni > 0,\nthus\n\u02c6`t\ni := max{0, `ti \u2212 1}.\nOne of the \u201cstandard\u201d diffusion approaches divides \u00af`ti\u2212 \u00af`tj by max{di, dj}+1 instead of 2 max{di, dj}.\nWe need the change for our analysis.\nWe will now very briefly introduce our contributions. In Section 2, we prove Theorem 2.1, which\nstates that we can upper-bound the total system load by O(\u2206n3). This generalizes the results of\n[4] to the case of unbounded edge capacities and, hence, analyzes the standard diffusion approach.\nTheorem 3.2 in Section 3 provides an asymptotically matching lower bound, showing that our upper\nbound is tight, up to a multiplicative constant. In Section 4 we discuss the problem of combining the\ndiffusion-approach with the work-stealing approach and show that certain assumptions necessarily\nlead to instability.\n2 Analysis of the Dynamic Diffusion Algorithm\nIn this section we will show that the diffusion approach yields a stable system. Moreover, we are\nable to upper bound the maximum load that will be in the system by O(\u2206n3). Throughout, we\nassume that n \u2265 2 and \u2206 \u2265 2.\nIn order to clarify the exposition, we first recall the notation we have already defined:\n\u2022 \u00af`ti denotes the load of processor Pi after we have generated tasks at the start of round t, but\nbefore load is balanced,\n\u2022 `ti is the load of processor Pi immediately after the load balancing phase, and\n\u2022 \u02c6`ti is the load of processor Pi after the task deletion phase of round t (i.e. at the very end of\nround t).\n\u2022 We will also use notation like L\u00aft(S) = \u2211i:Pi\u2208S \u00af`ti for a subset S \u2286 P, with similar definitions\nfor Lt(S) and L\u02c6t(S).\n5\nWith this notation, our main result about the diffusion approach to load balancing is\nTheorem 2.1 Let n \u2265 2 denote the number of processors in the system, and an upper bound on\nthe number of tasks that are generated during each time round. Let \u2206 \u2265 2 denote the maximum\ndegree of the connected graph G that specifies the communication linkages in the network. Then,\nstarting with an empty system, for all t \u2265 1 we have\nL\u02c6t(P) =\nn\u2211\ni=1\n\u02c6`t\ni \u2264 2\u2206n2(n+ 1).\nWe will prove this theorem by first giving a series of preliminary results. The proof of Theo-\nrem 2.1 uses a similar potential function as the one that was used in [4] (though what follows is\nvery different). This idea is to prove an invariant that for all t \u2265 1, every subset S \u2286 P satisfies\nthe following inequality:\nL\u02c6t(S) \u2264\nn\u2211\ni=n\u2212|S|+1\ni \u00b7 (4\u2206) \u00b7 n. (1)\nThen, Inequality (1) will immediately imply Theorem 2.1 (by taking S = P). We will often have\noccasion to refer to the right hand side of Inequality (1) for many sets, so to make our proofs that\nfollow easier to read, we define the following function f : {1, . . . , n} \u2192 N in this way\nf(k) =\nn\u2211\ni=n\u2212k+1\ni \u00b7 (4\u2206) \u00b7 n. (2)\nDefinition 2.2 In what follows, we will refer to sets as being bad after load generation in round t,\nor after the load balancing phase of round t, etc., meaning that the load of the set at that particular\ntime violates Inequality (1). For example, if we say that a set S is bad after load generation in\nround t, we mean that L\u00aft(S) > f(|S|).\nConversely, we will also refer to a set as being good (after load generation, or load balancing,\netc.) if it satisfies Inequality (1) (at the time in question).\nThe first lemma states that if we consider any (non-empty) set S at the end of round t, there\nmust have existed a set S\u2032 so that the load of S\u2032 before load balancing was at least as large as the\nload of S after load balancing, i.e. L\u00aft(S\u2032) \u2265 Lt(S) \u2265 L\u02c6t(S). The fact that might not be obvious is\nthat we can assert that the two sets contain the same number of processors. This is the statement\nof the following lemma.\nLemma 2.3 Let \u2205 6= S \u2286 P denote an arbitrary subset of processors. There exists a set |S\u2032| such\nthat\n1. |S\u2032| = |S|, and\n2. L\u00aft(S\u2032) \u2265 Lt(S).\nProof: The claim is clear if S = P, since in this case we have Lt(P) \u2265 L\u02c6t(P) and L\u00aft(P) = Lt(P).\nTaking S\u2032 = P then satisfies the conclusions of the theorem.\nSo we suppose that S is not the entire set of processors. In this case let Sin = {v : v \u2208\nS and \u2203w 6\u2208 S such that \u03b1twv > 0)}. In other words, Sin is the subset of S consisting of processors\nthat received tasks from outside of S during load balancing.\n6\nCase 1: Sin = \u2205. This case is essentially the same as when S = P. Since no processors in S\nreceived load from outside of S, the elements of S can only exchange load among themselves or\nsend load to processors outside of S. Then it is clear that L\u00aft(S) \u2265 Lt(S), so taking S\u2032 = S again\nsatisfies the desired conclusions.\nCase 2: Sin 6= \u2205. Let R = {w : w 6\u2208 S and \u2203v \u2208 Sin such that \u03b1twv > 0}. In other words, R is\nthe set of nodes not in S that pushed tasks into S during load balancing. The main idea of what\nfollows is that we are going to swap some elements of R for elements of Sin on a one-for-one basis\nto find the set S\u2032 we desire. More formally, let Lin =\n\u2211\nw\u2208R,v\u2208Sin \u03b1\nt\nwv denote the total flow from R\nto S during load balancing. We aim to find sets Rk \u2286 R and Sk \u2286 Sin with\n1. |Rk| = |Sk| = k, and\n2. L\u00aft(Rk) \u2265 Lt(Sk) + Lin + (flow from Sk to S\\Sk).\nThen we will take S\u2032 = (S\\Sk) \u222a Rk. Our choice of the set Rk guarantees that S\u2032 will satisfy\nL\u00aft(S\u2032) \u2265 Lt(S), since the elements of Rk account for all flow that enters S during load balancing,\nplus all flow that passes from elements in Sk to elements in S\\Sk as well.\nTo do this, let E1 = {(w, v) : w \u2208 R, v \u2208 Sin, \u03b1twv > 0}. Consider an edge e1 = (w1, v1) \u2208 E1\nwhere \u03b1te1 is largest. From the definition of \u03b1\nt\nw1v1 , we see that\n\u00af`t\nw1 \u2265 2 max{dw1 , dv1}\u03b1tw1v1 + \u00af`tv1 .\nThe key observation is that by choosing the largest edge, the expression \u00af`tw1 accounts for all possible\nload that v1 could have received during load balancing, and all tasks that w1 pushes into the set S\ntoo. Note also that the quantity \u00af`tw1 includes the number of tasks that v1 might happen to pass to\nother elements in S, since this is included in the term \u00af`tv1 . We set R1 := {w1} and S1 := {v1}, and\nE2 := E1\\ ({(w1, v\u2032) : v\u2032 \u2208 Sin} \u222a {(w\u2032, v1) : w\u2032 \u2208 R}).\nThen, we iteratively apply this argument, namely take a largest edge e2 = (w2, v2) \u2208 E2.\n(Note that w2 6= w1 and v2 6= v1.) The choice of largest edge then allows us to swap w2 for\nv2, again accounting for all tasks that w2 pushes into S during load balancing, all tasks that v2\nreceives, and any tasks that v2 passes to other elements in S. Then, we add w2 to R1, i.e. set\nR2 := R1 \u222a {w2}, add v2 to S1, so S2 := S1 \u222a {v2}, and delete the appropriate set of edges from\nE2. Thus, E3 := E2\\ ({(w2, v\u2032) : v\u2032 \u2208 Sin} \u222a {(w\u2032, v2) : w\u2032 \u2208 R}).\nWe continue to iterate this procedure, selecting an edge with largest \u03b1twv value, and performing\nan exchange as before, until we finish step k with a set Ek+1 = \u2205. It is possible that this procedure\nterminates at a step when Rk = R or Sk = Sin (or both), or with one or both of Rk, Sk being\nproper subsets of their respective sets. In any case, we have constructed sets Rk and Sk (each with\nk \u2264 min{|Sin|, |R|} elements), so that by taking S\u2032 = (S\\Sk) \u222a Rk, this set S\u2032 satisfies the two\nconditions of the theorem. \u0003\nFrom the previous lemma, we see that we have proven an inequality about the load of the sets\nof highest loaded processors, before and after load balancing (which, of course, need not be equal\nto each other). Thus we can conclude the following result:\nCorollary 2.4 For i \u2208 [n], let M\u00af ti denote a set of i largest loaded processors before load balancing\n(in round t). Also let M ti denote a corresponding set of i largest loaded processors after load\nbalancing. Then L\u00aft(M\u00af ti ) \u2265 Lt(M ti ).\nWe also conclude another result from Lemma 2.3.\nCorollary 2.5 Fix i \u2208 {1, . . . , n}. Suppose that every subset with i processors is good after the\nload generation phase of round t. Then, after the load balancing phase (and thus after the task\ndeletion phase), every subset with i processors is still good. (Of course, provided that M\u00af ti is good\nafter load generation, we actually get the same conclusion from Corollary 2.4.)\n7\nOur next result tells us that if a set is made bad by load generation, then the load balancing\nand deletion phases are sufficient to make that set good again.\nLemma 2.6 Suppose that at the end of round t, every set S \u2286 P satisfies (1). Further, suppose that\nafter the load generation phase in round t+ 1, there is some set S \u2286 P such that L\u00aft+1(S) > f(|S|).\nThen, at the end of round t+ 1, S again satisfies Inequality (1).\nProof: If there is more than one set S such that L\u00aft+1(S) > f(|S|), we may apply the argument\nthat follows to each, so we fix one of the possible sets S. Suppose that x \u2208 {1, . . . n} denotes the\nnumber of tasks that were injected into this set during load generation in round t+ 1.\nWe first show that\nif Pj \u2208 S then \u00af`t+1j \u2265 (n\u2212 |S|+ 1)(4\u2206)n\u2212 x. (3)\nIn the case when S = {Pi} for some i (that is, |S| = 1), this statement is clear, since we must have\n\u00af`t\ni > n(4\u2206)n to violate Inequality (1).\nWhen |S| \u2265 2 we can prove (3) by contradiction. So assume that some Pj \u2208 S satisfies\n\u00af`t+1\nj < (n \u2212 |S| + 1)(4\u2206)n \u2212 x. Since S was good before load generation, but not after, we know\nthat L\u00aft+1(S)\u2212 f(|S|) > 0. Then, using that L\u00aft+1(S\\Pj) = L\u00aft+1(S)\u2212 \u00af`t+1j , and our assumption on\n\u00af`t+1\nj , we conclude\nL\u00aft+1(S\\Pj) > L\u00aft+1(S)\u2212 (n\u2212 |S|+ 1)(4\u2206)n+ x\nL\u00aft+1(S\\Pj)\u2212 f(|S\\Pj |) > L\u00aft+1(S)\u2212 f(|S\\Pj |)\u2212 (n\u2212 |S|+ 1)(4\u2206)n+ x\nL\u00aft+1(S\\Pj)\u2212 f(|S\\Pj |) > L\u00aft+1(S)\u2212 f(|S|) + x > x.\nSince we injected x tasks into S during the load generation phase of round t + 1, we know that\nL\u00aft+1(S\\Pj) \u2264 L\u02c6t(S\\Pj) + x. Putting this together with our last inequality above, we see that\nL\u02c6t(S\\Pj) + x\u2212 f(|S\\Pj |) \u2265 L\u00aft+1(S\\Pj)\u2212 f(|S\\Pj |) > x\n=\u21d2 L\u02c6t(S\\Pj)\u2212 f(|S\\Pj |) > 0.\nThis is a contradiction to the assumption stated in the hypothesis that all sets satisfied (1) at the\nend of round t. Hence, we conclude what we wanted to show, namely Inequality (3).\nIf S = P, then our lemma follows immediately. In this case, the lower bound in (3) is also a\nlower bound on the load of each processor after the load balancing phase, i.e. `ti \u2265 (4\u2206)n\u2212 n > 0\nfor all Pi (since x = n when S = P). Thus, each processor will delete one task during the deletion\nphase. Since we injected at most n tasks into the system and deleted n tasks, the set S = P again\nsatisfies (1), and we are done.\nSo, we now assume that S 6= P. Then, in a similar manner as before, we can show\nif Pj 6\u2208 S, then \u00af`t+1j \u2264 (n\u2212 |S|)(4\u2206)n+ n. (4)\nTo see this, again assume the contrary, so that some Pj 6\u2208 S satisfies \u00af`t+1j > (n \u2212 |S|)(4\u2206)n + n.\nThen we have the following inequalities\nL\u02c6t(S \u222a Pj) + n \u2265 L\u00aft+1(S \u222a Pj) (5)\nL\u00aft+1(S \u222a Pj)\u2212 f(|S \u222a Pj |) > L\u00aft+1(S)\u2212 f(|S|) + n. (6)\n8\nInequality (5) holds simply because we insert n tasks into the system, and Inequality (6) follows\nby breaking up the difference on the left hand side into constituent parts, and using our assumption\nabout \u00af`t+1j . These inequalities together imply\nL\u02c6t(S \u222a Pj)\u2212 f(|S \u222a Pj |) + n \u2265 L\u00aft+1(S)\u2212 f(|S|) + n (7)\nL\u02c6t(S \u222a Pj)\u2212 f(|S \u222a Pj |) \u2265 L\u00aft+1(S)\u2212 f(|S|) > 0. (8)\nThe final inequality in (8) comes from our assumption that L\u00aft+1(S) > f(|S|). Of course, (8) violates\nthe hypothesis of the theorem stating that all sets satisfied Inequality (1) at the end of round t.\nHence, we obtain the upper bound on the load of elements not in S, as expressed in (4).\nThe rest of this lemma is a simple calculation. We first note that no load will be passed from\nP\\S into S during the load balancing phase because of the load differences in the processors. Then,\nsince our network G is connected, there must be an edge (i, j) with Pi \u2208 S and Pj 6\u2208 S. Using our\nbounds (3) and (4) for \u00af`ti and \u00af`\nt\nj , respectively, we find that\n\u03b1t+1ij \u2265\n\u00af`t+1\ni \u2212 \u00af`t+1j\n2 max{di, dj} \u2212 1 \u2265\n4\u2206n\u2212 n\u2212 x\n2\u2206\n\u2212 1 \u2265 2n\u2212 n\n\u2206\n\u2212 1 \u2265 3\n2\nn\u2212 1.\nThe last two inequalities use the facts that x \u2264 n and \u2206 \u2265 2. We see this final ratio is at least\nn (with our assumption that n \u2265 2). Hence, during round t + 1, at most n tasks were injected\ninto the set S during load generation, and at least n tasks were removed from S during the load\nbalancing phase (and none were inserted into S during this phase). Therefore, after load balancing\n(and thus also after the task deletion phase) S again satisfies Inequality (1). \u0003\nLemma 2.6 tells us that if a set is made bad by the load generation phase, then the load balancing\nand deletion phases are sufficient to make this set good. The essential task that remains to be shown\nis that load balancing cannot, in some way, change a good set into a bad one. Corollary 2.5 tells\nus half the story. We need a little more to cover all possible sets.\nLemma 2.7 Suppose that at all sets are good at the end of round t, but that after load generation\nin round t+ 1, there exists a bad set S with |S| = i. Then after load balancing and deletion, there\nexists no bad set with i processors.\nProof: Without loss of generality, we can assume that S = M\u00af ti , the largest i processors.\nLemma 2.6 tells us that S is not bad at the end of round t + 1. We therefore have to show\nthat we do not somehow change a good set (of i processors) into a bad set during the load bal-\nancing phase. This proof is similar in flavor to that of Lemma 2.3, except that the argument is\nsomewhat more delicate in this case.\nSince we injected at most n tasks into the set S to change S from a good set into a bad set,\nwe know that L\u00aft+1(S) \u2212 n \u2264 f(|S|). Our goal now is to show that any set S\u2032 of i processors will\nsatisfy Lt+1(S\u2032) \u2264 L\u00aft+1(S)\u2212 n, meaning that S\u2032 is good after load balancing.\nSo with this mind, fix some set S\u2032 where |S\u2032| = i. We assume that S\u2032 6= S, otherwise by\nLemma 2.6 there is nothing to prove. Define the following sets:\nScommon = S \u2229 S\u2032 Sold = S\\Scommon Snew = S\u2032\\Scommon.\nWe note that |Snew| = |Sold| \u2265 1. From our previous argument in Lemma 2.6, we know that the\nload difference (after generation, but before balancing) of any pair of processors, one from S and\none from P\\S, is at least 4\u2206n\u22122n. In order to show our result, we will consider the load balancing\n9\nactions of round t + 1 in three stages. We first compute (and fix) the values of \u03b1t+1i,j . Then we\nproceed this way:\nStage 1. Internal load balancing actions among processors of S, and among processors of P\\S.\nAfter this stage, the load difference between a pair of processors, one from S and one from P\\S is\nstill at least 4\u2206n\u2212 2n.\nStage 2. Processors in Sold balance with those in Snew. This can only move load from Sold to Snew\nbecause of the high load difference between processors of these two sets.\nStage 3. All remaining load balancing actions are performed. Which ones remain? Because there\nare no balancing actions from Snew \u2286 P\\S into Scommon \u2286 S, the only remaining ones are\n(a) Scommon to Snew,\n(b) Sold to P\\(S\u2032 \u222a Sold), and\n(c) Scommon to P\\(S\u2032 \u222a Sold).\nThe balancing actions of (a) and (b) do not change the load of S\u2032 = Scommon \u222a Snew, and those of\n(c) can only decrease the load of S\u2032. Hence, if we can show the load of S\u2032 after Stage 2 is at most\nL\u00aft+1(S)\u2212 n, then we get the conclusion we want.\nTo this end, let L1(Snew) denote the load of Snew after Stage 1, and L2(Snew) the load after\nStage 2 (and similarly for other sets Sold, S, etc.). Let A =\n\u2211\nj\u2208Sold,k\u2208Snew \u03b1\nt+1\nj,k denote the total\nload transferred during Stage 2 from Sold to Snew, and let B denote the load that remains in Sold\nafter Stage 2. We note the following equations hold:\nL2(S\u2032) = L2(S) + L2(Snew)\u2212 L2(Sold)\nL1(Sold) = A+B\nL2(Sold) = B\nL2(Snew) = L1(Snew) +A\nL2(S) = L1(S)\u2212A.\nAll of these equations together imply that\nL2(S\u2032) = L1(S)\u2212A+ L1(Snew) +A\u2212B\n= L1(S) + L1(Snew)\u2212B\n= L1(S) + L1(Snew) +A\u2212 L1(Sold).\nSince Stage 1 did not change the total load of S (so L1(S) = L\u00aft+1(S)), if we can show that\nL1(Snew) +A\u2212 L1(Sold) \u2264 \u2212n (9)\nwe obtain our desired result. Having arrived at the crux of the problem, we now demonstrate\nInequality (9).\nFirst note that if, in fact, there are no edges from Sold to Snew, then A = 0. In this case, if we\npair the vertices from Sold with those from Snew, then Inequality (9) follows immediately using the\nfact that the load difference of processors in Sold and Snew is at least 4\u2206n\u2212 2n.\nSuppose there is at least one edge from Sold to Snew. Because of the load difference of processors\nin Sold and Snew, we see that any edge for which \u03b1t+1j,k is positive, we in fact have that \u03b1\nt+1\nj,k \u2265 n.\nConsider the subgraph G\u2032 that consists of processors in Sold and Snew and edges which were\nused to pass load from Sold to Snew during Stage 2. Choose an edge from G\u2032 such that the value of\n10\n\u03b1t+1j,k is maximized. Assume (for simplicity) that j = 1 and k = 2. As in Lemma 2.3, we conclude\nthat \u00af`t+11 \u2265 2 max{d1, d2}\u03b1t+11,2 + \u00af`t+12 . Define A1,2 =\n\u2211\nk\u2208Snew \u03b1\nt+1\n1,k +\n\u2211\nj\u2208Sold \u03b1\nt+1\nj,2 , the total flow\nout of P1 (into Snew) and into P2 (from Sold). Since \u03b1t+11,2 has maximum value over edges, we see\nthat \u00af`t+11 \u2265 2 max{d1, d2}\u03b1t+11,2 + \u00af`t+12 \u2265 A1,2 + \u00af`t+12 . Hence, we see that \u00af`t+12 + A1,2 \u2212 \u00af`t+11 \u2264 0.\nIndeed, if at least one of P1 and P2 has degree strictly smaller than \u2206 in G\u2032, this difference is\nsmaller than or equal to \u2212n, which is what we want on the right hand side of Inequality (9)!\nIn either case, consider the subgraph G\u2032\u2032 obtained from G\u2032 by deleting the processors P1, P2,\nand all edges adjacent to them. As before, if there are no edges, we can pair the remaining\nprocessors however we like, and then we get the desired inequality. Otherwise, if we can show that\nL1(Snew\\P2) + (A\u2212A1,2)\u2212 L1(Sold\\P1) \u2264 \u2212n we again have shown Inequality (9).\nThe point is that we can proceed in an inductive manner as before, until we either find a pair\nPj \u2208 Sold, Pk \u2208 Snew where Pj sent load to Pk during Stage 2 and one of Pj and Pk has degree (in\nthe remaining subgraph of G\u2032) that is strictly less than \u2206 (in which case \u00af`t+1k +Aj,k \u2212 \u00af`t+1j \u2264 \u2212n),\nor we obtain a subgraph that has processors remaining, but no edges (and in this case we pair\nup the remaining processors however we like, and the large load difference between processors in\nthe two sets gives us Inequality (9)). Whatever occurs, we can pair up processors in a one-to-one\nfashion to prove Inequality (9), and thus, our lemma. \u0003\nNow we are prepared to prove our main result.\nProof: [Theorem 2.1]\nWe prove this theorem by induction on t. Inequality (1) holds when t = 1, for however we inject\nthe first n tasks into the system, all sets are good at the end of the first round.\nSo assume that at the end of round t, all sets are good. Fix i \u2208 {1, . . . , n}. If all sets of i\nprocessors are good after the load generation phase, then from Corollary 2.5 they are all good\nat the end of round t + 1. If there is some bad set of i processors after load generation, then\nLemmas 2.6 and 2.7 show that all sets of size i are still good at the end of round t+ 1.\nFinally, it is not possible that during load balancing a (good or bad) set of i processors will\nlead to the creation of a bad set of j( 6= i) processors. For suppose there is some bad set of j( 6= i)\nprocessors at the end of round t+ 1. Lemma 2.3 tells us that there must exist a set of j processors\nthat was bad before the load balancing phase, but then Lemmas 2.6 and 2.7 again tell us that there\nis no bad set of j processors at the end of round t+ 1, a contradiction to our assumption that there\nwas a bad set of j processors at the end of the round. \u0003\nOn the first glance it might look as if the our proof strategy is overly complicated and that\nthere is a much simpler proof. In the course of proving our result, we show that there is a gap\nof 4n\u2206 tasks between a processor in the bad set S and a processor outside of the bad set before\nbalancing whenever S is bad after balancing. Hence, at least n tasks were sent away from S in\nthis step and the invariant could not have been violated by S. But unfortunately it is possible to\ncreate a different bad set of processors during load balancing (possibly with a different number of\nprocessors), and we have to discount this case too. Hence, we have to show that if we can find a\nbad set after load balancing, then there was another bad set S\u2032 before load balancing, which leads\nus to a contradiction through our series of lemmas above.\n3 A Matching Lower Bound\nIn this section we provide a simple example that asymptotically matches the upper bound from\nSection 2. Fix some n \u2265 3 and consider the linear array G = (V,E) with V = {P0, . . . , Pn\u22121}\nand E = {(Pi, Pi+1)|0 \u2264 i < n \u2212 1}. Furthermore, suppose that during every time step, n new\n11\ntasks are generated on processor Pn\u22121. The idea of the proof essentially follows from a few simple\nobservations, which we state without formal proof.\nObservation 3.1\n1. The system is periodic since it is stable and thus there is a finite number of possible con-\nfigurations it can be in, i.e., there is a \u201crun-in\u201d phase during which load is being built up\n(essentially, load is being distributed from processor Pn\u22121 to all other processors), followed by\nperiodical behavior (notice that we consider a strictly deterministic system).\n2. Another obvious fact is that once the system has finished the initial run-in phase, every\nprocessor must delete one task in every round. If that were not the case, the system could not\npossibly be stable (we would delete strictly fewer tasks that are generated per period, i.e., the\nsystem load would increase by at least one during every period).\n3. Suppose the period length is T . Then we see that once the system is periodic, during any\nT rounds, processor Pi (i > 0) must send exactly T \u00b7 i many tasks to processor Pi\u22121 (some\nof which gets spread to the other processors Pi\u22122, . . . , P0), because that is just the number of\ntasks that processors Pi\u22121, . . . , P0 delete in T rounds. In other words, on average processor\nPi sends i many tasks during any of those rounds (it does, in fact, send exactly i tasks to\nprocessor Pi\u22121, thus T = 1; more about that later).\n4. In our setting, load will never be sent toward processor Pn\u22121.\nTheorem 3.2 below implies that the preceding analysis of our algorithm is tight up to a multiplicative\nconstant, because the line graph has maximum degree \u2206 = 2, and thus we have an upper bound\nof O(n3) on the system load.\nTheorem 3.2 The system described above on the linear array is stable with a total steady-state\nsystem load of \u0398(n3).\nProof: We begin by showing that processor Pi will never send more than i tasks to processor\nPi\u22121; the proof is by induction on time. The claim is trivially true in round 1. Let \u03b1ti = \u03b1\nt\ni,i\u22121\ndenote the number of tasks that processor Pi sends to processor Pi\u22121 in round t. (We may extend\nthe definition to \u03b1tn = n and \u03b1\nt\n0 = 0 for all t.) Suppose the claim holds for some t \u2212 1 > 1, i.e.,\n\u03b1t\u22121i \u2264 i for all i \u2208 {1, . . . , n \u2212 1}. Let `ti denote the load of processor Pi before the balancing\nstep in round t, 0 \u2264 i < n. From Observation 3.1 (2), for large enough values of t we have\n`ti = `\nt\u22121\ni + \u03b1\nt\u22121\ni+1 \u2212 \u03b1t\u22121i \u2212 1 and `ti\u22121 = `t\u22121i\u22121 + \u03b1t\u22121i \u2212 \u03b1t\u22121i\u22121 \u2212 1. Using the facts that\n\u03b1t\u22121i =\n\u230a\n`t\u22121i \u2212 `t\u22121i\u22121\n4\n\u230b\nand\n`t\u22121i \u2212 `t\u22121i\u22121\n4\n\u2264\n\u230a\n`t\u22121i \u2212 `t\u22121i\u22121\n4\n\u230b\n+\n3\n4\n,\nwe can conclude that\n\u03b1ti =\n\u230a\n`ti \u2212 `ti\u22121\n4\n\u230b\n\u2264 `\nt\ni \u2212 `ti\u22121\n4\n=\n(`t\u22121i + \u03b1\nt\u22121\ni+1 \u2212 \u03b1t\u22121i \u2212 1)\u2212 (`t\u22121i\u22121 + \u03b1t\u22121i \u2212 \u03b1t\u22121i\u22121 \u2212 1)\n4\n=\n`t\u22121i \u2212 `t\u22121i\u22121\n4\n+\n\u03b1t\u22121i+1 \u2212 2\u03b1t\u22121i + \u03b1t\u22121i\u22121\n4\n\u2264\n\u230a\n`t\u22121i \u2212 `t\u22121i\u22121\n4\n\u230b\n+\n3\n4\n+\n\u03b1t\u22121i+1 \u2212 2\u03b1t\u22121i + \u03b1t\u22121i\u22121\n4\n12\n= \u03b1t\u22121i +\n3\n4\n+\n\u03b1t\u22121i+1 \u2212 2\u03b1t\u22121i + \u03b1t\u22121i\u22121\n4\n=\n2\u03b1t\u22121i + \u03b1\nt\u22121\ni+1 + \u03b1\nt\u22121\ni\u22121\n4\n+\n3\n4\n\u2264 2i+ (i+ 1) + (i\u2212 1)\n4\n+\n3\n4\n= i+\n3\n4\n.\nFrom the above we know that processor Pi will never send more than i tasks to processor Pi\u22121\nduring each round (i.e. \u03b1ti \u2264 i since fractional tasks are not allowed in our model). However, in\norder to obtain stability, at least i tasks on average are necessary. Thus, we can conclude that\nonce the system is \u201crun-in\u201d, processor Pi will always send i tasks to processor Pi\u22121, i.e., the system\nis in fact periodic with period length T = 1. Clearly, there are many possible fixed points with\nthis property. However, since we are interested in a lower bound, we pick the one with smallest\ntotal load, i.e., the one in which processor P0 is empty at the end of a round, receives one task\nfrom processor P1 in the next round, deletes it, and so on. Since a load difference of 2\u2206i = 4i\nimplies i tasks being sent, this means that, directly before balancing, the load of processor Pi is\u2211i\nj=0 4j = 2i(i + 1), and thus the total system load is\n\u2211n\u22121\ni=0 2i(i + 1) = (2n\n3 \u2212 2n)\/3. Together\nwith the upper bound of 2\u2206n2(n+ 1) = 4n2(n+ 1) from Theorem 2.1 we get the statement of the\ntheorem. \u0003\n4 Some Instability Results for Work Stealing\nIn this section we will consider a variation of our load balancing process where we may transfer tasks\nto empty processors only. This approach is similar to the diffusion approach, only the computation\nof the \u03b1ti,j is different. The value of \u03b1\nt\ni,j , the load that is sent from Pi to Pj , is larger than zero iff\nPj is empty (and Pi non-empty). This method is referred to as work stealing.\n\u03b1ti,j =\n{\nb \u00af`ti\u2206+1c : \u00af`tj = 0 and Pj is adjacent to Pi\n0 : otherwise\nNote that the bounds below also hold when we divide by 2\u2206 instead of \u2206+1. We use the above\ndefinition as worst case assumption. In [8] the authors showed that simple work stealing yields\na stable system. They assumed that there are at most (1 \u2212 \u000f)n new tasks generated per round,\nfor some \u000f \u2208 (0, 1]. The important point to note is that in [8], the processor communication links\ncorrespond to a complete graph on n vertices. Here we will see that the work stealing method can\nfail (in the sense that the total load is unbounded over time) if the graph is no longer the complete\ngraph.\nWe consider the line network. In a line, we have an edge between node Pi and Pi+1 for 1 \u2264 i \u2264\nn\u2212 1. Hence, the maximum degree is 2.\nObservation 4.1 Assume we have n processors connected as a line and n generators are all on\nprocessor 1. Then the diffusion work stealing system is not stable.\nProof: Let us assume the system is in a state where P2 is empty and P1 has k tasks directly\nbefore the balancing. Then it will transfer k\/3 tasks to P2 during the load balancing step. It is\neasy to see that it will take at least\nt =\nk\n3(n\u2212 1) +\nn\u22122\u2211\ni=1\ni =\nk\n3(n\u2212 1) +\nn2 \u2212 4n\u2212 3\n2\n13\ntime steps until P2 is empty again. To see that, assume that all other processors are empty. Then\nit takes n \u2212 2 steps until load will reach Pn, it takes n \u2212 3 time steps until load will reach Pn\u22121,\nand so on. In the meantime, the load of P1 increases by t(n\u2212 1) tasks. Thus, the load of P1 after\nt steps is at least\nk \u2212 k\n3\n+\n(\nk\n3(n\u2212 1) +\nn2 \u2212 4n\u2212 3\n2\n)\n(n\u2212 1) \u2265 k.\nThis shows that the load of P1 increases between any two consecutive balancing actions. \u0003\nIn a similar manner, under adversarial injection schemes, it is easy to show that the work\nstealing protocol will not be stable for many classes of graphs, even under a randomized injection\npattern. For example, we can simply define the process in a way such that the expected load of a\nprocessor increases between two load balancing actions.\nThe next observation shows that already very small networks are not stable under adversarial\ninjections.\nObservation 4.2 Assume we have a network with a pair of nodes u and v that are not connected\nby an edge. Let assume that the degree of u is not larger that the degree of v, and let \u03b4 be degree\nof u. Then the work stealing system is not stable under an adversarial load generation scheme that\ngenerates \u03b4 + 2 tasks per round.\nProof: Simply allocate 2 generators on node u and one generator on every of the \u03b4 neighbors of\nu. Then none of the neighbors will ever balance with u and the load of u will increase by one per\nround. \u0003\nSimilar to the observation above it is easy to show that the system is not stable under a wide class\nof randomized injection patterns. Define the process in a way that the expected load of u increases\nbetween two load balancing actions.\n5 A Different Model for Task Generation\/Deletion\nIn this section we define a load generation model similar to [20] and [4]. Rather than bounding\nthe total number of tasks that are generated per round, we bound the load change in any subset\nof the processors. During each round, tasks can be added or deleted from processors, subject to\nthe restriction in Inequality (10) below. The processors then balance load amongst themselves as\nbefore.\nIn the following, \u00af`ti (respectively, L\u00af\nt(S)) denotes the load of processor Pi (resp. the total load\nof all processors in set the S) after we have generated and deleted tasks, and `ti (resp. L\nt(S)) is the\nload of processor Pi (resp. the total load of all processors in the set S) immediately after the load\nbalancing phase. Let avg(t) be the average load of the processors in round t after load generation\nand deletion, i.e. avg(t) = 1n \u00b7\n\u2211n\ni=1\n\u00af`t\ni. Again, L\nt(P) denotes the total system load at the end of\nstep t. One round looks now as follows:\n1. Tasks are generated and deleted according to the following generation restriction:\nL\u00aft(S)\u2212 Lt\u22121(S) \u2264 |S| \u00b7 (avg(t)\u2212 avg(t\u2212 1)) + n for every subset S. (10)\n2. Every processor balances its load with some or all its neighbors in the network using the\ndiffusion operation defined in Section 1.2.\n14\nWe can show the following result.\nTheorem 5.1 Let n \u2265 2 denote the number of processors in the system. Let \u2206 \u2265 2 denote the\nmaximum degree of the connected graph G that specifies the communication linkages in the network.\nAssume the load generation and deletion fulfills the generation restriction in (10). Then, starting\nwith an empty system, for all t \u2265 1 and all S \u2286 P we have\nLt(S) \u2264 |S| \u00b7 avg(t) + 5\u2206n3.\nFurthermore, the maximum number of tasks per processor is avg(t) + 5\u2206n2.\nProof: The proof of this theorem follows the proof of Theorem 2.1. Here, we will concentrate\non the parts that have to be changed compared to that proof. We redefine f as follows.\nf(k) =\nn\u2211\ni=n\u2212k+1\ni \u00b7 (5\u2206) \u00b7 n. (11)\nOur new invariant is\nLt(S) \u2264 |S| \u00b7 avg(t) + f(|S|) = |S| \u00b7 avg(t) +\nn\u2211\ni=n\u2212|S|+1\ni \u00b7 (5\u2206) \u00b7 n. (12)\nSimilar to the previous section, we call a set S bad if Lt(S) > |S|\u00b7avg(t)+f(|S|), and good otherwise.\nSince Lemma 2.3, Corollary 2.5, and Corollary 2.4 only depend on the load balancing scheme and\nnot on the underlying load generation and deletion, they still can be applied. Because Lemma 2.6\ndepends on the actual load of the processors and, therefore, on the load generation model, we have\nto adjust it. The new version is presented below.\nLemma 5.2 Suppose that at the end of round t, every set S \u2286 P satisfies (12). Further, suppose\nthat after the load generation and deletion phase in round t+ 1, there is some set S \u2286 P such that\nL\u00aft+1(S) > |S| \u00b7avg(t+1)+f(|S|). Then, at the end of round t+1, S again satisfies Inequality (12).\nProof: We only consider the parts of the proof that are different from the proof of Lemma 2.6.\nWe first show that\nif Pj \u2208 S then \u00af`t+1j \u2265 (n\u2212 |S|+ 1)(5\u2206)n+ avg(t+ 1)\u2212 n. (13)\nIn the case when S = {Pi} for some i (that is, |S| = 1), this statement is clear, since we must have\n\u00af`t+1\ni > n(5\u2206)n+ avg(t+ 1) to violate Inequality (12).\nAs in Lemma 2.6, when |S| \u2265 2 we can prove (13) by contradiction. So assume that some Pj \u2208 S\nsatisfies \u00af`t+1j < (n\u2212|S|+1)(5\u2206)n+avg(t+1)\u2212n. Since S was good before load generation, but not\nafter, we know that L\u00aft+1(S)\u2212f(|S|) > |S|\u00b7avg(t+1). Then, using that L\u00aft+1(S\\Pj) = L\u00aft+1(S)\u2212 \u00af`t+1j ,\nand our assumption on \u00af`t+1j , we conclude\nL\u00aft+1(S\\Pj) > L\u00aft+1(S)\u2212 (n\u2212 |S|+ 1)(5\u2206)n\u2212 avg(t+ 1) + n\nL\u00aft+1(S\\Pj)\u2212 f(|S\\Pj |) > L\u00aft+1(S)\u2212 f(|S\\Pj |)\u2212 (n\u2212 |S|+ 1)(5\u2206)n\u2212 avg(t+ 1) + n\nL\u00aft+1(S\\Pj)\u2212 f(|S\\Pj |) > L\u00aft+1(S)\u2212 f(|S|)\u2212 avg(t+ 1) + n > (|S| \u2212 1) \u00b7 avg(t+ 1) + n.\nOn the other hand, Inequality (10) tells us that\nL\u00aft+1(S\\Pj) \u2264 Lt(S\\Pj) + (|S| \u2212 1) \u00b7 (avg(t+ 1)\u2212 avg(t)) + n.\n15\nPutting this together with our last inequality above, we see that\nLt(S\\Pj) + (|S| \u2212 1) \u00b7 (avg(t+ 1)\u2212 avg(t)) + n\u2212 f(|S\\Pj |) \u2265 L\u00aft+1(S\\Pj)\u2212 f(|S\\Pj |)\n\u2265 (|S| \u2212 1) \u00b7 avg(t+ 1) + n\n=\u21d2 Lt(S\\Pj)\u2212 f(|S\\Pj |) > (|S| \u2212 1) \u00b7 avg(t).\nThis is a contradiction to the hypothesis that all sets satisfied (12) at the end of round t. Hence,\nwe conclude what we wanted to show, namely Inequality (13).\nWhen S = P, then our lemma follows immediately since the load of S is exactly n \u00b7 avg(t+ 1).\nhence, we can assume that S 6= P. Then, in a similar manner as before, we can show\nif Pj 6\u2208 S, then \u00af`t+1j \u2264 (n\u2212 |S|)(5\u2206)n+ avg(t+ 1) + n. (14)\nTo see this, again assume the contrary, so that some Pj 6\u2208 S satisfies\n\u00af`t+1\nj > (n\u2212 |S|)(5\u2206)n+ avg(t+ 1) + n.\nThen we have the following inequalities\nLt(S \u222a Pj) + n+ (avg(t+ 1)\u2212 avg(t))(|S|+ 1) \u2265 L\u00aft+1(S \u222a Pj) (15)\nL\u00aft+1(S \u222a Pj)\u2212 f(|S \u222a Pj |) > L\u00aft+1(S)\u2212 f(|S|) + avg(t+ 1) + n. (16)\nInequality (15) is due to the generation restriction. Inequality (16) follows by breaking up the\ndifference on the left hand side into constituent parts, and using our assumption about \u00af`t+1j . These\ninequalities together imply\nLt(S \u222a Pj)\u2212 f(|S \u222a Pj |) + (avg(t+ 1)\u2212 avg(t))(|S|+ 1) + n\n\u2265 L\u00aft+1(S)\u2212 f(|S|) + avg(t+ 1) + n (17)\nLt(S \u222a Pj)\u2212 f(|S \u222a Pj |) + (avg(t+ 1)\u2212 avg(t))(|S|+ 1)\n\u2265 L\u00aft+1(S)\u2212 f(|S|) + avg(t+ 1) > 0 (18)\nLt(S \u222a Pj)\u2212 f(|S \u222a Pj |) + (avg(t+ 1)\u2212 avg(t))(|S|+ 1) \u2265 |S + 1| \u00b7 avg(t+ 1) (19)\nL\u02c6t(S \u222a Pj)\u2212 f(|S \u222a Pj |) \u2265 |S + 1| \u00b7 avg(t). (20)\nInequality in (18) comes from our assumption that L\u00aft+1(S) > |S| \u00b7 avg(t + 1) + f(|S|). Again, we\nhave a contradiction and obtain the upper bound on the load of elements not in S, as expressed\nin (14).\nAgain, we have a load difference of at least 5\u2206n between processors on S and processors not in\nS. The rest of this lemma is a simple calculation and can be done similar to the one in Lemma 5.2.\n\u0003\nLemma 2.7 only depends on the load difference of the processors and is still valid under the\nnew load generation and deletion model. We have only to show that we still have\nL\u00aft+1(S)\u2212 n \u2264 |S| \u00b7 avg(t+ 1) + f(|S|),\ni.e. if we subtract n from the load of set S after load generation and deletion, set S is good again.\nThis can be done as follows. Due to the generation restriction, we know that the load generated in\n16\nS is upper bounded by |S| \u00b7 (avg(t + 1)\u2212 avg(t)) + n. We know that Lt(S) \u2264 |S| \u00b7 avg(t) + f(|S|)\nsince S was good at the end of round t. This gives us\nL\u00aft+1(S) \u2264 Lt(S) + |S| \u00b7 (avg(t+ 1)\u2212 avg(t)) + n\n\u2264 |S| \u00b7 avg(t) + f(|S|) + |S|(avg(t+ 1)\u2212 avg(t)) + n\nL\u00aft+1(S) \u2264 |S| \u00b7 avg(t+ 1) + f(|S|) + n\nL\u00aft+1(S)\u2212 n \u2264 |S| \u00b7 avg(t+ 1) + f(|S|).\nAlso, the remainder of the proof of Theorem 5.1 can be done similar to the proof of Theorem 2.1.\n\u0003\n5.1 Further Extensions\nWe can easily generalize our results to other load generation processes, and the proofs of the\nfollowing results are much like those of Theorem 5.1.\nTheorem 5.3 Let n \u2265 2 denote the number of processors in the system. Let \u2206 \u2265 2 denote the\nmaximum degree of the graph G that specifies the communication linkages in the network. Assume\nthe load generation and deletion fulfills the generation restriction\nL\u00aft(S)\u2212 Lt\u22121(S) \u2264 |S| \u00b7 (avg(t)\u2212 avg(t\u2212 1)) +K.\nThen, starting with an empty system, for all t \u2265 1 and all S \u2286 P we have\nLt(S) \u2264 |S| \u00b7 avg(t) + 5\u2206n2K.\nFurthermore, the maximum number of tasks per processor is avg(t) + 5\u2206nK.\nProof: Similar to the proof of Theorem 5.1. All we have to do is to define\nf(k) =\nn\u2211\ni=n\u2212k+1\ni \u00b7 (5\u2206) \u00b7K.\n\u0003\nFurthermore, we can improve our results to a load generation model where the imbalance that\nwe allow to be generated in any set depends on the number of outgoing edges.\nTheorem 5.4 Let n \u2265 2 denote the number of processors in the system. Let \u2206 \u2265 2 denote the\nmaximum degree of the graph G that specifies the communication linkages in the network. Let e(S)\nbe the number of outgoing edges of the set S. Assume the load generation and deletion fulfills the\ngeneration restriction\nL\u00aft(S)\u2212 Lt\u22121(S) \u2264 |S| \u00b7 (avg(t)\u2212 avg(t\u2212 1)) +K \u00b7 e(S).\nThen, starting with an empty system, for all t \u2265 1 and all S \u2286 P we have\nLt(S) \u2264 |S| \u00b7 avg(t) + 5\u2206n2K.\nFurthermore, the maximum number of tasks per processor is avg(t) + 5\u2206nK.\n17\nProof: Similar to the proof of Theorem 5.1. All we have to do is to define\nf(k) =\nn\u2211\ni=n\u2212k+1\ni \u00b7 (5\u2206) \u00b7K.\n\u0003\nAcknowledgements\nThe authors wish to thank the anonymous referee for valuable comments.\nReferences\n[1] W. Aiello, B. Awerbuch, B. Maggs, and S. Rao. Approximate load balancing on dynamic and asyn-\nchronous networks. Proceedings of the 25th Annual ACM Symposium on Theory of Computing (STOC\n1993), pp. 632\u2013641.\n[2] W. Aiello, E. Kushilevitz, R. Ostrovsky, and A. Rosen. Adaptive packet routing for bursty adversarial\ntraffic. J. Computer and Systems Sciences 60 (2000), pp. 482\u2013509.\n[3] A. Anagnostopoulos, A. Kirsch, and E. Upfal. Stability and efficiency of a random local load balancing\nprotocol. Proceedings of the 44th Annual IEEE Symposium on Foundations of Computer Science\n(FOCS 2003), pp. 472\u2013481.\n[4] E. Anshelevich, D. Kempe, and J. Kleinberg. Stability of load balancing algorithms in dynamic\nadversarial systems. Proceedings of the 34th Annual ACM Symposium on Theory of Computing (STOC\n2002), pp. 399\u2013406.\n[5] B. Awerbuch, P. Berenbrink, A. Brinkmann, and C. Scheideler. Simple routing strategies for adversar-\nial systems. Proceedings of the 42nd Annual IEEE Symposium on Foundations of Coumputer Science\n(FOCS 2001), pp. 158\u2013167.\n[6] B. Awerbuch and T. Leighton. A simple local control algorithm for multi-commodity flow. Proceedings\nof the 34th Annual IEEE Symposium on Foundations of Computer Science (FOCS 1993), pp. 459\u2013468.\n[7] B. Awerbuch and T. Leighton. Improved approximation algorithms for the multi-commodity flow\nproblem and local competitive routing in dynamic networks. Proceedings of the 26th Annual ACM\nSymposium on Theory of Computing (STOC 1994), pp. 487\u2013496.\n[8] P. Berenbrink, T. Friedetzky, and L.A. Goldberg. The natural work-stealing algorithm is stable. SIAM\nJ. Computing 32 (2003), pp. 1260\u20131279.\n[9] P. Berenbrink , T. Friedetzky, and E.W. Mayr. Parallel continuous randomized load balancing. Pro-\nceedings of the 10th Annual ACM Symposium on Parallel Algorithms and Architectures (SPAA\u201998),\npp.192\u2013201.\n[10] J.E. Boillat. Load balancing and Poisson equation in a graph. Concurrency: Practice and Experiences\n2 (1990), pp. 289\u2013313.\n[11] G. Cybenko. Dynamic load balancing for distributed memory multiprocessors. J. Parallel and Dis-\ntributed Computing 7 (1989), pp. 279\u2013301.\n[12] R. Diekmann, A. Frommer, and B. Monien. Efficient schemes for nearest neighbor load balancing. J.\nParallel Computing 25 (1999), pp. 789\u2013812.\n[13] R. Elsa\u00a8sser and B. Monien. Load balancing of unit size tokens and expansion properties of graphs.\nProceedings of the 15th Annual ACM Symposium on Parallel Algorithms and Architectures (SPAA\n2003), pp. 266\u2013273.\n[14] R. Elsa\u00a8sser, B. Monien, and R. Preis. Diffusion schemes for load balancing on heterogeneous networks.\nTheory of Computing Systems 35 (2002), pp. 305\u2013320.\n18\n[15] B. Ghosh, F.T. Leighton, B.M. Maggs, S. Muthukrishnan, C.G. Plaxton, R. Rajaraman, A.W. Richa,\nR.E. Tarjan, and D. Zuckerman. Tight analyses of two local load balancing algorithms. Proceedings\nof the 27th Annual ACM Symposium on Theory of Computing (STOC 1995), pp. 548\u2013558.\n[16] B. Ghosh and S. Muthukrishnan. Dynamic load balancing by random matchings. J. Computer and\nSystems Science 53 (1996), pp. 357\u2013370.\n[17] F.M. auf der Heide, B. Oesterdiekhoff, and R. Wanka. Strongly adaptive token distribution. Algo-\nrithmica 15 (1996), pp. 413\u2013427.\n[18] F.M. auf der Heide, C. Scheideler, and V. Stemann. Exploiting storage redundancy to speed up\nrandomized shared memory simulations. Theoretical Computer Science 162 (1996) pp. 245\u2013281.\n[19] S. Muthukrishnan, B. Ghosh, and M.H. Schultz. First- and second-order diffusive methods for rapid,\ncoarse, distributed load balancing. Theory of Computing Systems 31 (1998), pp. 331\u2013354.\n[20] S. Muthukrishnan and R. Rajaraman. An adversarial model for distributed load balancing. Proceedings\nof the 10th Annual ACM Symposium on Parallel Algorithms and Architectures (SPAA 1998), pp. 47\u2013\n54.\n[21] D. Peleg and E. Upfal. The token distribution problem. SIAM J. Computing 18 (1989), pp. 229\u2013243.\n[22] Y. Rabani, A. Sinclair, and R. Wanka. Local divergence of Markov chains and the analysis of\niterative load-balancing schemes. Proceedings of the 39th Annual IEEE Symposium on Foundations\nof Computer Science (FOCS 1998), pp. 694\u2013703.\n19\n"}