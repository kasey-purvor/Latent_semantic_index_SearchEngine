{"doi":"10.1007\/978-3-540-73110-8_21","coreId":"135583","oai":"oai:bradscholars.brad.ac.uk:10454\/3156","identifiers":["oai:bradscholars.brad.ac.uk:10454\/3156","10.1007\/978-3-540-73110-8_21"],"title":"A Toolkit for Multimodal Interface Design: An Empirical Investigation","authors":["Rigas, Dimitrios I.","Alsuraihi, M."],"enrichments":{"references":[],"documentType":{"type":null}},"contributors":[],"datePublished":"2007","abstract":"NoThis paper introduces a comparative multi-group study carried out to investigate the use of multimodal interaction metaphors (visual, oral, and aural) for improving learnability (or usability from first time use) of interface-design environments. An initial survey was used for taking views about the effectiveness and satisfaction of employing speech and speech-recognition for solving some of the common usability problems. Then, the investigation was done empirically by testing the usability parameters: efficiency, effectiveness, and satisfaction of three design-toolkits (TVOID, OFVOID, and MMID) built especially for the study. TVOID and OFVOID interacted with the user visually only using typical and time-saving interaction metaphors. The third environment MMID added another modality through vocal and aural interaction. The results showed that the use of vocal commands and the mouse concurrently for completing tasks from first time use was more efficient and more effective than the use of visual-only interaction metaphors","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":null,"rawRecordXml":"<record><header><identifier>\n        \n            \n                oai:bradscholars.brad.ac.uk:10454\/3156<\/identifier><datestamp>\n                2016-07-25T16:41:21Z<\/datestamp><setSpec>\n                com_10454_413<\/setSpec><setSpec>\n                col_10454_6345<\/setSpec>\n            <\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:doc=\"http:\/\/www.lyncode.com\/xoai\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n            \nA Toolkit for Multimodal Interface Design: An Empirical Investigation<\/dc:title><dc:creator>\nRigas, Dimitrios I.<\/dc:creator><dc:creator>\nAlsuraihi, M.<\/dc:creator><dc:subject>\nSpeech recognition<\/dc:subject><dc:subject>\nText-to-speech<\/dc:subject><dc:subject>\nInterface design<\/dc:subject><dc:subject>\nUsability<\/dc:subject><dc:subject>\nLearnability<\/dc:subject><dc:subject>\nEffectiveness<\/dc:subject><dc:subject>\nEfficiency<\/dc:subject><dc:subject>\nSatisfaction<\/dc:subject><dc:subject>\nVisual<\/dc:subject><dc:subject>\noral<\/dc:subject><dc:subject>\nAural<\/dc:subject><dc:subject>\nMultimodal<\/dc:subject><dc:subject>\nAuditory-icons<\/dc:subject><dc:subject>\nEarcons<\/dc:subject><dc:subject>\nSpeech<\/dc:subject><dc:subject>\nVoice-instruction<\/dc:subject><dc:description>\nNo<\/dc:description><dc:description>\nThis paper introduces a comparative multi-group study carried out to investigate the use of multimodal interaction metaphors (visual, oral, and aural) for improving learnability (or usability from first time use) of interface-design environments. An initial survey was used for taking views about the effectiveness and satisfaction of employing speech and speech-recognition for solving some of the common usability problems. Then, the investigation was done empirically by testing the usability parameters: efficiency, effectiveness, and satisfaction of three design-toolkits (TVOID, OFVOID, and MMID) built especially for the study. TVOID and OFVOID interacted with the user visually only using typical and time-saving interaction metaphors. The third environment MMID added another modality through vocal and aural interaction. The results showed that the use of vocal commands and the mouse concurrently for completing tasks from first time use was more efficient and more effective than the use of visual-only interaction metaphors.<\/dc:description><dc:date>\n2009-07-28T07:02:09Z<\/dc:date><dc:date>\n2009-07-28T07:02:09Z<\/dc:date><dc:date>\n2007<\/dc:date><dc:type>\nArticle<\/dc:type><dc:type>\nNo full-text available in the repository<\/dc:type><dc:identifier>\nRigas, D. and Alsuraihi, M. (2007). A Toolkit for Multimodal Interface Design: An Empirical Investigation. Lecture Notes in Computer Science. Vol. 4552, pp. 196-205.<\/dc:identifier><dc:identifier>\n90006945<\/dc:identifier><dc:identifier>\nhttp:\/\/hdl.handle.net\/10454\/3156<\/dc:identifier><dc:language>\nen<\/dc:language><dc:relation>\nhttp:\/\/dx.doi.org\/10.1007\/978-3-540-73110-8_21<\/dc:relation>\n<\/oai_dc:dc>\n<\/metadata>\n        <\/record>","journals":null,"language":null,"relations":["http:\/\/dx.doi.org\/10.1007\/978-3-540-73110-8_21"],"year":2007,"topics":["Speech recognition","Text-to-speech","Interface design","Usability","Learnability","Effectiveness","Efficiency","Satisfaction","Visual","oral","Aural","Multimodal","Auditory-icons","Earcons","Speech","Voice-instruction"],"subject":["Article","No full-text available in the repository"],"fullText":null}