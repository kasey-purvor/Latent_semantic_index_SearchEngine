{"doi":"10.1016\/j.ssci.2005.02.006","coreId":"138127","oai":"oai:dspace.lib.cranfield.ac.uk:1826\/1402","identifiers":["oai:dspace.lib.cranfield.ac.uk:1826\/1402","10.1016\/j.ssci.2005.02.006"],"title":"Reducing mid-air collision risk in controlled airspace: Lessons from hazardous incidents.","authors":["Brooker, Peter"],"enrichments":{"references":[{"id":38106108,"title":"Air Accidents Investigation Branch","authors":[],"date":"2004","doi":null,"raw":"AAIB [UK Air Accidents Investigation Branch} (2004). Website. http:\/\/www.dft.gov.uk\/stellent\/groups\/dft_control\/documents\/contentservertemplate\/df t_index.hcst?n=5161&l=1 Amalberti, R. (2000). The paradoxes of almost totally safe transportation systems. Safety Science, 1, 1-16.","cites":null},{"id":38106130,"title":"Airworthiness Authorities","authors":[],"date":"2000","doi":null,"raw":"JAA [Joint Airworthiness Authorities] (2000). Advisory Joint Material relating to JAR 25 Large Aeroplanes. AMJ 25.1309. Change 15. Joint Airworthiness Authorities.","cites":null},{"id":38106123,"title":"Aviation Authority","authors":[],"date":"2003","doi":null,"raw":"CAA [Civil Aviation Authority] (2003). The Mandatory Occurrence Reporting Scheme. CAP 382. CAA, London.  http:\/\/www.caa.co.uk\/docs\/33\/CAP382.pdf Chambers (2001).  Chambers 21st. Century Dictionary.  Chambers Harrap, Edinburgh. http:\/\/www.chambersharrap.co.uk\/chambers\/chref\/chref.py\/main?query=hazard&title=21st.","cites":null},{"id":38106132,"title":"Causal Analysis of Aircraft Accidents Computer Safety, Reliability and Security,","authors":[],"date":"2000","doi":"10.1007\/3-540-40891-6_30","raw":"Ladkin, P. B. (2000). Causal Analysis of Aircraft Accidents Computer Safety, Reliability and Security, Proceedings of the 19th International Conference, SAFECOMP 2000, Lecture","cites":null},{"id":38106126,"title":"Designing for Situation Awareness. Taylor and Francis,","authors":[],"date":"2003","doi":"10.1201\/9780203485088","raw":"Endsley, M.R., Bolte, B. and Jones, D.G. (2003). Designing for Situation Awareness. Taylor and Francis, London.","cites":null},{"id":38106127,"title":"ESARR 2 Guidance to ATM Safety Regulators: Severity Classification Scheme for Safety Occurrences in ATM.","authors":[],"date":"1999","doi":null,"raw":"Eurocontrol SRC (1999). ESARR 2 Guidance to ATM Safety Regulators: Severity Classification Scheme for Safety Occurrences in ATM. EAM 2\/GUI 1. Eurocontrol, Brussels. http:\/\/www.eurocontrol.int\/src\/documents\/deliverables\/esarr2_awareness_package\/eam2gui 1e10ri.pdf Eurocontrol SRC (2002). ESARR 2 Mapping between the Eurocontrol Severity Classification Scheme & the ICAO Airprox Severity Scheme. EAM 2\/GUI 3. Eurocontrol, Brussels.","cites":null},{"id":38106112,"title":"Federal Bureau of Aircraft Accidents Investigation","authors":[],"date":"2004","doi":null,"raw":"BFU [German Federal Bureau of Aircraft Accidents Investigation] (2004). Investigation Report \u2018\u00dcberlingen Mid-air collision\u2019. AX001-1-2\/02. http:\/\/www.bfuweb.de\/berichte\/02_ax001efr.pdf Bird, F. (1974). Management Guide to Loss Control. Institute Press, Atlanta, Georgia.","cites":null},{"id":38106121,"title":"Future Air Traffic Management: Quantitative En Route Safety Assessment Part 2 \u2013 New Approaches.","authors":[],"date":"2002","doi":"10.1017\/s037346330200187x","raw":"Brooker, P. (2002). Future Air Traffic Management: Quantitative En Route Safety Assessment Part 2 \u2013 New Approaches. Journal of the Institute of Navigation 55(3), 363-379.","cites":null},{"id":38106128,"title":"Integra Safety Metrics.","authors":[],"date":"2004","doi":null,"raw":"Eurocontrol (2004). Integra Safety Metrics. http:\/\/www.eurocontrol.int\/care\/integra\/safety_metric.htm   26 Graham, R., Hoffman, E., Pusch, C. and Zeghal K. (2003). Absolute versus Relative Navigation: Theoretical Considerations from an ATM Perspective. 5 th Eurocontrol\/FAA ATM R&D Seminar, Budapest, Hungary Greenwell, W. S., Knight, J. C. and Strunk, E. A. (2003). Risk-based Classification of Incidents. Second Workshop on the Investigation and Reporting of Incidents and Accidents (IRIA 2003). http:\/\/shemesh.larc.nasa.gov\/iria03\/p03-greenwell.pdf Hale, S. and Law, M. (1989). Simultaneous Operation of Conflict Alert and ACAS II in UK En-Route Airspace. DORA Report 8914, CAA.","cites":null},{"id":38106131,"title":"Notes on the Foundations of System Safety and Risk.","authors":[],"date":"1998","doi":null,"raw":"Ladkin, P. B. (1998). Notes on the Foundations of System Safety and Risk. RVS-Bk-00-01. RVS Group, Faculty of Technology, University of Bielefeld.  http:\/\/www.rvs.unibielefeld.de\/publications\/books\/safetyNotes.pdf.","cites":null},{"id":38106110,"title":"One safe sky for Europe \u2013 A revolution","authors":[],"date":"2003","doi":null,"raw":"Baumgartner, M. (2003). One safe sky for Europe \u2013 A revolution in European ATM.  The Controller, July, 8-12.","cites":null},{"id":38106129,"title":"Results of ACAS II Safety Analysis. ICAO Secondary Surveillance Radar Improvements and Collision Avoidance Systems Panel","authors":[],"date":"1993","doi":null,"raw":"Harrison, D. (1993). Results of ACAS II Safety Analysis. ICAO Secondary Surveillance Radar Improvements and Collision Avoidance Systems Panel (SICASP\/5) HSE [Health and Safety Executive] (1992 and 1999). The Tolerability of Risk from Nuclear Power Stations. HMSO; Reducing Risks, Protecting People. HSE Books.","cites":null},{"id":38106124,"title":"Taming Human Error with a Systems Approach.","authors":[],"date":"2003","doi":null,"raw":"Courteney, H. and Newman, T. (2003). Taming Human Error with a Systems Approach. FSF\/IASS Conference, Washington, USA.","cites":null},{"id":38106125,"title":"The Theory of Stochastic Processes.","authors":[],"date":"1977","doi":"10.2307\/3617428","raw":"Cox, D. R. and Miller, H. D. (1977). The Theory of Stochastic Processes.  Chapman & Hall, London.","cites":null},{"id":38106122,"title":"Why the Eurocontrol Safety Regulation Commission Policy on Safety Nets and Risk Assessment is Wrong.","authors":[],"date":"2004","doi":"10.1017\/s0373463304002735","raw":"Brooker, P. (2004). Why the Eurocontrol Safety Regulation Commission Policy on Safety Nets and Risk Assessment is Wrong. Journal of the Institute of Navigation 57(2), 231-243. (in press).","cites":null}],"documentType":{"type":0.5555555556}},"contributors":[],"datePublished":"2005-11","abstract":"The collection and analysis data on hazardous air traffic management (ATM) incidents is an important task. Expert judgement about such incidents needs to be carried out within a systematic and consistent safety framework. The mark of the genuine safety expert is to be able to ask the right questions concerning potential accidents.\n\nHazards and risks are not \u2018facts\u2019 or \u2018events\u2019 that \u2018exist\u2019, but rather judgements made about conditional futures and their consequences. A hazardous situation is one in which the outcome was not \u2018system controlled\u2019, with some potential outcomes having significant negative consequences. System controls in this sense cover all the means by which the system is held stable (=defended) against the potential negative consequences.\n\nThe ATM system can be (over-) simplified to consist of three structural system layers acting as the system controls: Planning (pre-operational), Operation (the flight in progress), and Alert (the ground and air protection enabled by conflict alert systems, on which the controller\/pilot will act). A hazardous event is one in which a high degree of conflict between aircraft is observed plus a low confidence that the remaining system layers would generally provide the necessary corrective action","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/138127.pdf","fullTextIdentifier":"http:\/\/hdl.handle.net\/1826\/1402","pdfHashValue":"14ed575b546e40b7c5558db2440a2653ca97f818","publisher":"Elsevier","rawRecordXml":"<record><header><identifier>\noai:dspace.lib.cranfield.ac.uk:1826\/1402<\/identifier><datestamp>2008-06-05T12:39:54Z<\/datestamp><setSpec>hdl_1826_19<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>Reducing mid-air collision risk in controlled airspace: Lessons from hazardous incidents.<\/dc:title><dc:creator>Brooker, Peter<\/dc:creator><dc:subject>Collision risk<\/dc:subject><dc:subject>Air traffic control<\/dc:subject><dc:subject>Incidents<\/dc:subject><dc:description>The collection and analysis data on hazardous air traffic management (ATM) incidents is an important task. Expert judgement about such incidents needs to be carried out within a systematic and consistent safety framework. The mark of the genuine safety expert is to be able to ask the right questions concerning potential accidents.\n\nHazards and risks are not \u2018facts\u2019 or \u2018events\u2019 that \u2018exist\u2019, but rather judgements made about conditional futures and their consequences. A hazardous situation is one in which the outcome was not \u2018system controlled\u2019, with some potential outcomes having significant negative consequences. System controls in this sense cover all the means by which the system is held stable (=defended) against the potential negative consequences.\n\nThe ATM system can be (over-) simplified to consist of three structural system layers acting as the system controls: Planning (pre-operational), Operation (the flight in progress), and Alert (the ground and air protection enabled by conflict alert systems, on which the controller\/pilot will act). A hazardous event is one in which a high degree of conflict between aircraft is observed plus a low confidence that the remaining system layers would generally provide the necessary corrective action.<\/dc:description><dc:publisher>Elsevier<\/dc:publisher><dc:date>2007-01-31T15:39:19Z<\/dc:date><dc:date>2007-01-31T15:39:19Z<\/dc:date><dc:date>2005-11<\/dc:date><dc:type>Postprint<\/dc:type><dc:format>245270 bytes<\/dc:format><dc:format>application\/pdf<\/dc:format><dc:identifier>Peter Brooker, Reducing mid-air collision risk in controlled airspace: Lessons from hazardous incidents, Safety Science, Volume 43, Issue 9, November 2005, Pages 715-738.<\/dc:identifier><dc:identifier>0925-7535<\/dc:identifier><dc:identifier>http:\/\/hdl.handle.net\/1826\/1402<\/dc:identifier><dc:identifier>http:\/\/dx.doi.org\/10.1016\/j.ssci.2005.02.006<\/dc:identifier><dc:language>en<\/dc:language><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["issn:0925-7535","0925-7535"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2005,"topics":["Collision risk","Air traffic control","Incidents"],"subject":["Postprint"],"fullText":" 1\nTo be published in Safety Science \nPost Referees Review \n@ 19 November 2004 \n \n \nREDUCING MID-AIR COLLISION RISK IN CONTROLLED AIRSPACE: \nLESSONS FROM HAZARDOUS INCIDENTS \n \nPeter Brooker \n \nCranfield University \n \nCopyright \u00a9 Cranfield University 2004 \nABSTRACT \nThe collection and analysis data on hazardous air traffic management (ATM) \nincidents is an important task.  Expert judgement about such incidents needs to be \ncarried out within a systematic and consistent safety framework.  The mark of the \ngenuine safety expert is to be able to ask the right questions concerning potential \naccidents.   \n \nHazards and risks are not \u2018facts\u2019 or \u2018events\u2019 that \u2018exist\u2019, but rather judgements made \nabout conditional futures and their consequences.  A hazardous situation is one in \nwhich the outcome was not \u2018system controlled\u2019, with some potential outcomes having \nsignificant negative consequences\u201d.  System controls in this sense cover all the \nmeans by which the system is held stable (= defended) against the potential \nnegative consequences. \n \nThe ATM system can be (over-) simplified to consist of three structural system layers \nacting as the system controls: Planning (pre-operational), Operation (the flight in \nprogress), and Alert (the ground and air protection enabled by conflict alert systems, \non which the controller\/pilot will act).  A hazardous event is one in which a high \ndegree of conflict between aircraft is observed plus a low confidence that the \nremaining system layers would generally provide the necessary corrective action.   \n1. INTRODUCTION \nAviation\u2019s safety track record demonstrates that it is an industry whose people are \nfocused on continuous improvement.  The prime safety goal of the air traffic \nmanagement (ATM) of en route commercial flights is to reduce the risk of mid-air \ncollisions.  Safety has improved to such an extent that collisions are now rare, so \ncollecting data on hazardous ATM Incidents has therefore always been seen as an \nimportant task.  This incident data, collected consistently, can be viewed as a key \nindicator of the \u2018health\u2019 of the ATM system.  Incidents can provide insights into the \nfrequency of known error and failure types, and also enable new types to be \ndetected.  Analyses of the processes and characteristics of incidents provide insights \n 2\ninto potential system design weaknesses.  Moreover, planned changes to the system \nto improve safety can be tested against such incidents to demonstrate that risk is \nbeing reduced.  [NB: \u2018risk\u2019 is often used in safety analyses as a combination of \nfrequency of occurrence and its severity: in the following, the accidents always have \nthe same severity \u2013 an accident with many fatalities.] \n \nA large variety of crucial \u2013 and intrinsically difficult \u2013 safety questions can be asked \nabout ATM incidents: \nWhich incidents should be judged the most important to ATM system safety?   \nWhich incidents give most guidance about potential accidents?   \nIn what ways should incidents be categorised and analysed to help pinpoint \nkey safety issues?   \nAre \u2018minor\u2019 incidents of any safety importance? \nHow should the relevant importance of different incidents be assessed or \nweighted to provide a true picture of the health of the ATM safety system?   \nThis paper attempts to make a start \u2013 no more than that \u2013 in answering these kinds \nof questions.  Interpretation of incident data needs a systematic and consistent \nsafety framework.  Without such a framework, it is difficult to gain the desired \ninsights into system design weaknesses that have the potential for accidents.  There \nis little point in collecting and categorising a great deal of data, unless it is analysed \nin a way that systematically reveals the safety lessons that help to reduce the \nlikelihood of potential future accidents.   \n \nThe aim here is to try to ensure that expert judgement about ATM incidents can be \ncarried out within a systematic and consistent safety framework, rather than \nproducing formulaic prescriptions.  The focus is how the system can be improved \nrather than the whys of causes.  System jargon is avoided: the approach is mainly \nthrough an \u2018ordinary language\u2019 analysis of safety terms and logic, plus some \nmetaphors to describe the key features of a safe ATM system.   \n \nThe following text consists of six sections: \n2. The Nature of Hazardous ATM Events \n3. ATM System Layers and Risk Probabilities \n4. Airproxes, Hazards and System Layers \n5. Discussion \n6. Conclusions \n2. THE NATURE OF HAZARDOUS ATM EVENTS \nThere are several different \u2013 and complementary \u2013 ATM incident systems in use in \nthe UK.  Airproxes derive from pilot and controller reports, and are the province of \nthe UK Airprox Board [UKAB] (1999- ).  The Civil Aviation Authority\u2019s (CAA) Safety \nRegulation Group (SRG) has a Mandatory Occurrence Reporting scheme, which \ncovers all kinds of aviation incident not just ATM-related ones (CAA, 2003).  National \n 3\nair traffic Services Ltd (NATS) records inter alia, data on Short Term Conflict Alert \n(STCA) and Separation Monitoring Function (SMF) monitoring (eg NATS (2004)). \n \nWhat actually is an ATM incident?  To start this analysis, consider a frequently used \nphrase about ATM incidents: \u2018hazardous\u2019.  A dictionary definition is on the lines of: \nhazardous: \u201canything which might cause an accident, create danger, etc\u201d \nThis is a complex statement.  It has three important elements: \n\u201cmight\u201d \u2013 this is a future conditional statement, referring to a possible future \n\u201ccause\u201d \u2013 some chain of related events is being considered \n\u201caccident\/danger\u201d \u2013 there are significant negative consequences \nThus, when something is referred to as being hazardous, this is actually a statement \nabout something that will happen (or is the process of happening) that could have \nsignificant negative penalties to the participant(s) or other individuals. \n \nSome examples help to illustrate these different elements (these will also be used in \nthe further development of the concept).  To walk a tightrope would be hazardous \nbecause an inexperienced person would be likely to fall of it (but not so for an \nexperienced and healthy circus performer).  Driving a car on an icy road around a \ncorner could be hazardous because the driver\u2019s knowledge about the road\u2019s friction \nwould not be as good as in normal conditions, and so the driver might fail to steer the \nvehicle safely.  It would be hazardous for a pilot to adopt a markedly non-standard \nphraseology when communicating with a controller because this could lead to a \nmisunderstanding about the route the aircraft should follow, and hence possibly \nproduce a flight path conflicting with other aircraft. \n \nThus, the word hazardous cannot properly be used about events that have already \noccurred.  There is no conditionality about past events: the tightrope was walked, the \ncar was driven, etc.  What people are often commenting on is the hazard involved in \nsome kind of similar event in the future.  Thus, walking a tightrope is a hazardous \nthing for people to do because the next person to attempt it may fall off \u2013 or perhaps \nthe person after that, etc.  Sometimes, cars that are to be driven on icy roads will \nlead to an accident.  Sometimes, there will be occasions when poor voice discipline \nby aircrew can lead to a mid-air collision.   \n \nNot all future things presently viewed as hazardous will actually turn out to have \nnegative consequences \u2013 some non-acrobats will be able to walk the tightrope, etc.  \nThe likelihood of the negative consequences will depend on factors such as the \nparticipant\u2019s skills, the environment and safety mechanisms.  Thus, the tightrope \nwalking will be much more difficult for the average 90-year-old non-acrobat, and the \npilot\u2019s poor communication may not lead to danger if the controller detects the choice \nof the wrong course.  Sometimes, the negative consequences are avoided by a \ndeterministic (ie definitely present) measure, eg the traffic police might have set a \nspecial speed limit for the icy road and are monitoring every driver.  In other \ninstances (eg the controller monitoring the aircraft), there will be probabilistic \nelements involved, so that, on some proportion of occasions, the negative penalties \nwill occur and the rest of the occasions it will not. \n 4\n \nThis suggests a definition of a past \u2018hazardous situation\u2019: \nhazardous situation: \u201cone in which the outcome was not \u2018system controlled\u2019, \nwith some potential outcomes having significant negative consequences\u201d \nThe phrase \u2018system controlled\u2019 means something like: \nsystem controlled: \u201cthe ability to determine the outcome against reasonably \nforeseen changes and variations of system parameters, such as the abilities \nof the participant(s), the environment (in the largest sense), and the safety \nmechanisms in place. \nSystem controls in this sense cover all the means \u2013 the effective feedbacks \u2013 by \nwhich the system is held stable (= defended) against the potential negative \nconsequences: designers, pilots, controllers, software engineers, etc \u2013 not just the \noperational controller.  What is preventing an unsafe system state from persisting?   \n \nA failure of system control covers both of these kinds of mistake, where the \nmechanisms make the situation worse, and when they essentially fail to intervene \n(eg a conflict alert system could fail either by putting the aircraft into more danger or \nby not alerting).  The new phrase here is \u2018reasonably foreseen\u2019.  This means that the \nassessment of hazard is not to be carried out against \u2018unreasonable\u2019 system \nparameters.  What is unreasonable is a matter for debate and convention.  In the \nexample of the driver on the icy road, if the driver had a great deal of experience of \ndriving in such conditions, then it might be reasonable for him or her to assume that \nthe road friction was no worse than he had previously encountered.  Phrased \nanother way, to say that a past situation was hazardous implies that if some of the \nsystem parameters applicable during that situation had been \u2018slightly different\u2019 or \n\u2018reasonably perturbed\u2019 then significant negative consequences would have resulted.  \nSomething was hazardous because of what might have happened.   \n \nThis idea, that system control is a key thought, has already been adopted in safety \nassessment of operational ATM systems.  To quote (ARIBAa): \n\u201cIn general, a (sub)process is named controllable if it has the property that its \nbehaviour can always be steered from an undesirable sequence of events to \nanother, more desirable, one.  Opposite to controllable (sub)processes are \n(sub)processes that have the property that once the initial state is known, the \nfuture evolution of the (sub)process is completely determined.  This type of \n(sub)process is named autonomous.\u201d \n \nBut is this the correct understanding of a concept such as \u2018hazardous incident\u2019?  This \n\u2018ordinary language\u2019 approach seems to be consistent with the dictionary definitions of \nrelated words \u2013 Appendix A.  The definitions in Appendix A share the underlying \ningredients detected in the word \u2018hazardous\u2019.  Thus, the future conditional tense for \nis implied by phrases such as \u2018likely to\u2019, \u2018possibility\u2019, and \u2018depending on chance\u2019.  \nHazards and risks are not \u2018actual\u2019 things \u2013 \u2018facts\u2019 or \u2018events\u2019 that somehow \u2018exist\u2019 \u2013 \nbut rather judgements made about conditional futures and their consequences, given \na lack of information about current system parameters and further events.  This \n 5\njudgement (or perception or opinion) is about the degree of possibility of some \nunpleasant state of things that may come into existence at some future time.   \n \nThere is no such thing as \u2018actual risk\u2019 unless it is interpreted in this way: combining \nthe definitions above only produces sense if there is this kind of interpretation.  [A \nsimple test is to try to explain a putative conceptual phrase such as \u2018actual risk\u2019 to \nsomeone \u2013 what kind of thing would it consistently describe?  The logical \nimplications of terms such as \u2018hazard\u2019 have generated considerable interest by \nsafety and computing researchers, eg Ladkin (1998).] \n \nThus, the conclusion is that, whenever attempts are made to classify incidents in \nterms of risk, these should be 'what if' exercises.  The central message is that \nhazards and risks are judgements, which implies the need to put in place a \nframework and processes that ensure that the experts are asked to make the most \nvaluable judgements in safety terms.  The implication of this to ATM incident analysis \nis picked up again in latter sections: first, a new description of the ATM system is \nneeded in the next section.   \n3. ATM SYSTEM LAYERS AND RISK PROBABILITIES \nFor an ATM System, the \u2018system control\u2019, as described in the previous section, can \nbe the responsibility of a new concept: \u2018system layers\u2019.  The present ATM system \nhas evolved over the decades.  It has several distinct components in its operational \nand technical concept.  In roughly their date order of introduction, these are.   \nControllers and pilots \u2013 people are an integral part of the whole system. \nFormal Safety Rules \u2013 for the control of traffic, including the minimum \nseparation to be permitted between aircraft. \nRadio Telephony \u2013 communication between controllers and pilots (only \nintroduced shortly before World War II). \nControlled Airspace \u2013 the creation of sectors, volumes of airspace, each \nhandled by a controller team, with a small number of routes; civil commercial \ntraffic is separated from general aviation and military aircraft.   \nFlight Progress Strips \u2013 paper strips, generated by the flight plan computer \nand kept on plastic holders in ordered racks, which are used to record the \ndetails of a flight \u2013 being changed to electronic versions. \nRadar \u2013 processed Secondary Surveillance Radar (SSR) is now used, with \nthe displayed aircraft symbols complemented by callsign and height \ninformation, passed down from aircraft transponders.  [Not is use in oceanic \nairspace \u2013 ATC is supplied with position reports from the on-board navigation \nsystems.] \nComputer Processing \u2013 of radar and flight data. \nHigh Quality Aircraft Navigation \u2013 progressively improved from VOR\/DME \nto Inertial Navigation Systems through to satellite-based aids such GPS. \n 6\nSTCA \u2013 the computer processing system has the facility for analysing SSR \ntracks to predict if aircraft might come into close proximity in the near future \nand, if they do, warn the controller by flashing a message on his radar screen. \nAirborne Collision Avoidance System ACAS \u2013 On board collision \navoidance system based on detection of other aircraft in the vicinity carrying \nSSR transponders.  These tell the pilot of nearby traffic \u2013 TA (Traffic Advisory) \n\u2013 and aircraft coming into conflict \u2013 RA (Resolution Advisory).  RAs tell the \npilot to climb or descend as appropriate to take it out of risk.  [NB: ACAS is the \ngeneric term \u2013 used here.  TCAS \u2013 Traffic Alert and Collision Avoidance \nSystem \u2013 is a commercially available version of ACAS.]  \n \nFor present purposes, these can be grouped as follows: \n \nSafe Route Design \nFormal Safety Rules \nControlled Airspace \nTechnical Infrastructure \nRadio Telephony \nRadar  \nHigh Quality Aircraft \nNavigation \nComputer Processing \nFlight Progress Strips \nPlanning Controller \nFlying Pilot \nControlling Controller \nGround Protection  \nSTCA \nController and Pilot \nAir Protection  \nACAS \nPilot \nFigure 1. ATM System Components - illustrative \nFigure 2 then shows, in a very abstract and simplified fashion, the dynamical transit \nof a typical flight \u2013 in ATM system terms \u2013 through these structural system layers.  \nThe three pre-operational \u2013 Planning \u2013 layers have been grouped together because \nthey are highly related, eg separation minima depend on suitable equipment being \navailable, while the controller has to work within the safety constraints using the \nequipment.  The picture is very simplified: for example, flight scheduling and flow \nmanagement should be included in Planning.  The Operation Layers cover the \nactivities of pilots and controllers while the flight is in progress.  The Alert Layer is the \nground and air protection enabled by STCA and ACAS, on which the controller\/pilot \nwill act.  The assumption that a flight goes through these layers in one sequence is \ntoo simple; in reality, there are complex feedback loops. \n \n \n \n 7\n \n \nFigure 2. The ATM system layers \u2013 highly simplified, without \u2018loops\u2019 \nBut what do these layers actually accomplish in terms of system risks?  The answer \nis that they act systematically to reduce mid-air collision risk by \u2018shuffling the risk \npack\u2019.  The purpose of the system layers is to change the statistical distribution of \nrisks.  A formal structure is imposed by the Planning Layers, next the Operation \nLayer should then eliminate inherent conflicts (but note that the Planning Layer does \nnot produce conflict free paths from departure to landing), and then the Alert Layer \nwarns the controller and pilot about impending conflicts.   \n \nFigure 3 illustrates the risk structure \u2013 the frequencies of flights having particular \n\u2018degree of conflict\u2019 (Dc) values \u2013 of a large number of flights being processed by \nthese layers, measured at two of the \u2018slice\u2019 points I to IV in the transit shown in \nFigure 2.  [The term \u2018degree of conflict\u2019 is used because of similarities with related \nphrases adopted by researchers assessing STCA and ACAS.] \n \nSa\nfe\n ro\nut\nei\nng\n \nTe\nch\nni\nca\nl \nIn\nfr\nas\ntr\nuc\ntu\nre\n \nPl\nan\nni\nng\n \nC\non\ntr\nol\nlin\ng \nG\nro\nun\nd \nPr\not\nec\ntio\nn \n \nFl\nyi\nng\n \nPlanning Operation Alert \nA\nir \nPr\not\nec\ntio\nn A flight\u2019s \ntransit \nin system \nterms \nI II III IV \n 8\n \nFigure 3. Statistical distribution of Dc in a large population of flights \nThe full line curve A shows a comparatively risky set of flights: some have very high \nvalues \u2013 off the scale \u2013 and the number of flights with zero Dc value is not that high.  \nThe dashed curve B is a much better profile: there is a maximum Dc value and most \nflights at near to zero Dc value.  The job of the ATM system layers is to improve type \nA distributions so that they become type Bs: fewer high degrees of conflict and more \nof the lower degrees of conflict). \n \nThe \u2018starting population\u2019 of flights in Figure 3 could be all those that operated on a \ngiven day in a given airspace.  But most of the pairs of flights fly at different times in \ndifferent geographical areas.  As these aircraft are too far apart in time and\/or space \never to pose a risk to each other, they are excluded from the analysis \u2013 they have \n\u2018zero degree of conflict\u2019 (or most precisely, \u2018negligible\u2019 in any practical sense).  The \nbulk of flights are to be found in the very long tail near to this \u2018zero degree of conflict\u2019.  \n(For a given traffic pattern, there is an upper limit to the number of conflicts: it is \ncertainly bounded above \u2013 by all aircraft being instantaneously in conflict with all \nother aircraft.)  Those significant conflicts that exist are comparatively few in number, \nand are generally confined to those flights operating at about the same time and in \nparticularly heavily loaded airspace sectors.   \n \nOn what kind of scale is this, so far vaguely described, degree of conflict Dc to be \nmeasured?  It needs to indicate that the aircraft flightpaths, as known at the time the \ndistribution is measured, will lead to the aircraft being in close proximity.  It is a \nmeasure of hazard potential.  Note that this measurement takes no account of the \naction of the remaining system layers.  The choice of a scale for Dc is essentially \narbitrary: Figure 3 shows a choice in which the full line curve is unbounded (eg for \nhigh values of Dc could correspond to the inverse of the projected closest point of \napproach (CPA) distance).   \n \nDegree of \nconflict Dc \n\u2013 measured \nat a transit \npoint \nNumber of conflicts \nA \n5.1\n 9\nIf the statistical distribution is rescaled by using something like a logarithmic \ntransformation of the number of flights \u2013 so as to compress the horizontal scale \n(indicated by the zigzag on the horizontal axis) \u2013 then the action of the layers looks \nlike Figure 4.   \n \n \nFigure 4. System layers changing the shape of the Conflict Distribution \nEach of the Layers acts to reduce the frequency of the high conflict events \u2013 they \ntransform the conflict pattern from the initial I to the final IV.  A beneficial change \nthen increases the rate of descent at the left and pushes the intercept on the Dc = 0 \naxis to the right.    \n \nHow do the system layers work?  The language of System Theory as applied to \nsafety distinguishes between tightly versus loosely coupled systems (Weick, 1976; \nPerrow, 1984):  \nThe sub-components of a tightly coupled system have immediate impacts on \neach other.  Tightly coupled systems can survive failures, if that failure has \nbeen anticipated and provided for in the original design.  Thus, tightly coupled \nsystems must therefore be designed to anticipate all the failure modes and \nproviding safety features for continued operation and recovery.  These kinds \nof designs can usually be modelled quantitatively, and their performance can \nbe validated against what happens in the real world, eg by studying the kinds \nof accidents, incidents, failures and errors that occur.   \nLoosely coupled systems have flexibility in the timing, nature or intensity of \nresponses.  They accommodate failures through adaptive responses.  There \nis some \u2018play\u2019 in the (negative) feedback loops\u2014a little over-correction, then \nsome under correction.  Such systems are adaptable and error tolerant, but \ncan have long reaction times.  Loosely coupled designs use much more \ncomplex information sources \u2013 eg through human visualization and situational \nawareness, so they tend to be open and continually interacting with the \noutside environment.   \nPlanning Operation Alert \nI II III IV \n 10\nIn summary, in safety terms, loosely coupled systems can accommodate \nshocks, failures, and pressures for change without destabilization, while tightly \ncoupled systems respond more rapidly to perturbations \u2013 but the response \ncan be disastrous.  \n \nThe ATM system layers have elements of both types of coupling.  The Planning and \nAlert Layers tend to be more tightly coupled, because their action should be \n\u2018programmed\u2019.  The Operation Layer tends to be more loosely coupled, because \npilots and controllers make strategic and tactical decisions.  Their decision-making \ndoes however reflect their training, so they will tend to do similar things in similar \nsituations and (eg) highly skilled pilots will make fewer strategic\/decision errors \n(Wiegmann and Shappell, 2001).   \n \nHowever, each layer acts to transforms the distribution in a probabilistic fashion, \nrather than a deterministic one in which specific events necessarily follow particular \ncauses.  A particular aircraft configuration or routeing should be changed by the \nlayers into a different one, which probably has a lower degree of conflict.  But, on \nsome occasions, the pilot or controller will choose to do something that does not \nimprove the Dc value.  A collision can occur if the system is in the hazardous state \nand the remaining system layers do not correct this.  Thus, on a small proportion of \ncases, eg when an aircraft has just manoeuvred considerably and the aircraft pair is \nsuddenly very close, ACAS might recommend an inappropriate course of action. \n \nNote the difference between this kind of system metaphor and other kinds of system \nperspectives, such as those of Bird (1974), Rasmussen (1990) and Reason (1990).  \nThe approach here is foreshadowed in Rasmussen: \u201cThe causal tree found by an \naccident analysis is only a record of one past case, not a model of the involved \nrelational structure\u2026We should be fighting types, not individual tokens\u2026\u201d  In \nReason\u2019s descriptive models of system safety defences, the probabilistic aspects are \nmodelled by the number and size of holes in defensive layers (eg Shappell and \nWiegmann, 2001).  The metaphor used here has the advantage that it shows clearly \nhow the degree of conflict can reduce and increase during the sequence of an \naircraft\u2019s transit through the ATM system.  Human error, in the widest sense, remains \na major element in this causal chain, as it does for aviation in general (Courteney \nand Newman, 2003). \n \nPhilosophical models of \u2018Probabilistic Causation\u2019 have a long history, eg Sosa and \nTooley (1993).  Examples of relevant aviation-related work are by Ladkin (2000), \nwhich provides a different \u2018logic-orientated\u2019 perspective, and by Greenwell et al \n(2003).  There is a huge, multidisciplinary literature on this topic. \n \nATM layered concepts are the subject of research current interest.  A very thought-\nprovoking paper is by Graham et al (2003), which inter alia proposes the uses of a \nloop picture to explain the nature of information flow and decision-making between \nATM layers.  Graham et al trace back the idea of a layered approach to Villiers \n(1968).  Interestingly, the French title of Villiers\u2019 paper uses the phrase \u201cla m\u00e9thode \ndes filtres\u201d, thus giving filtering as a useful metaphor. \n \n 11\n \nFigure 5. Examples of transition paths \nFigure 5 shows a \u2018cut-down\u2019 version of Figure 4 with some examples of conflict level \ntransformations: \na [full line] A comparatively high conflict level is reduced markedly by the \nplanning Layers.  The conflict level then hardly changes: the Operation \nLayers do not increase the Dc, the Alert Layers are not required, and so the \nend result is a low conflict level. \nb [dashed line] This path shows a very high conflict level reduced to a \nmoderate conflict level by the Planning Layers.  Then some kind of \nOperation problem increases the Dc, sufficient the Alert Layer to come into \naction and reduce the conflict markedly. \nc \u2013 [dotted line] In this transition path the low Dc at the initial stage is reduced \neven further by the Planning Layers.  But the Operation Layers then \nincrease the conflict level considerably.  The Alert Layers turn out to be \nineffective, so the Dc is very high. \nThe key point is that the Dc value can rise and fall through the sequence of system \nlayers.  There are obviously many different possibilities, eg these transition paths do \nnot show a case in which the Planning Layers increase the risk.  These could be \nvery significant, given that the route and airspace structure is probably more likely to \nincrease the number of aircraft on conflicting paths.  This also serves to highlights an \nexample of interaction between the layers.  The main purpose of route structures for \nATM could be considered to be helping controllers to identify conflicts (ie by forcing \ncrossing points at particular locations etc), and hence improving the effectiveness of \nthe Operational layer. \n \nIt is one thing describing such a probabilistic process as a metaphor, but this needs \nto be turned into something that is quantitative.  For present purposes, all that is \nrequired is a simple categorisation of \u2018errors\u2019 made by the system layers.  These are \nof two types so, using analogous language to statistical hypothesis testing: \nType 1 error \u2013 failing to reduce the Dc when it is high (\u03b1) \nType 2 error \u2013 increasing the Dc when it is low (\u03b2) \nPlanning Operation Alert \na \nb \nc \n 12\n \nNext, it is necessary to categorise the Dc distributions in two bands: hazard potential \nand safe:  \nThe \u2018hazard potential\u2019 band comprises flights for which there is a high degree \nof risk.   \nAircraft in the \u2018safe\u2019 band are not in any risk now \u2013 but could be if the further \nsafety layers did not act appropriately.   \nThe proportions of hazard potential and safe flights are h and s, with their sum \nadding to unity.  (Obviously, a much more complex banding is possible, with h1, h2, \nh3\u2026representing a finer-structured grouping, but for present purposes the two bands \nare sufficient.)  Thus, probabilities \u03b1 and \u03b2 are respectively the proportion of \nsituations with hazard potential that remain in this state and the proportion of safe \ninstances that move to having hazard potential.   \n \nAs an aside, it is possible to view the system layer process as a Markov Chain (eg \nCox and Miller, 1977) or as a Bayesian network (eg Neil et al, 2003).  These offer \nnew ways of thinking \u2013 in rational and quantitative terms \u2013 about ATM safety and the \nactions of the system layers.  This kind of analytical approach (compare Wiegmann \nand Shappell, 2003) may be more productive for modelling than (eg) Reason\u2019s \ndescriptive ideas.   \n4. AIRPROXES, HAZARDS AND SYSTEM LAYERS \nThe material of the previous two sections can now be used to analyse Airproxes.  .  \nAirproxes are chosen here because they are publicly available and well documented \n\u2013 Airprox statistics are often publicised as a \u2018gold standard\u2019 of UK ATM safety.  It \nmust be stressed that other safety incident data \u2013 eg the CAA\u2019s MORS data, and \nNATS SMF data and other databases would no doubt be at least equally valid ATM \nsafety incident sources. \n \nAirprox Category Description \nCAT A - Risk of Collision:  \n \nThe risk classification of an aircraft proximity in \nwhich serious risk of collision has existed. \nCAT B - Safety Not Assured:  The risk classification of an aircraft proximity in \nwhich the safety of the aircraft may have been \ncompromised. \nCAT C - No risk of Collision:  \n \nThe risk classification of an aircraft proximity in \nwhich no risk of collision has existed. \nRisk Not determined:  \n \nThe risk classification of an aircraft proximity in \nwhich insufficient information was available to \ndetermine the risk involved or inconclusive or \nconflicting evidence precluded such \ndetermination. \nFigure 6. ICAO Doc 4444 AIRPROX Severity Scheme (eg Eurocontrol, 2002) \n 13\nFigure 6 shows the internal (ICAO) definitions for Airprox categories (eg Eurocontrol \nSRC, 2002).  Note the use of the word \u2018existed\u2019, ie at some point there was a \nperception\/judgement of hazard.  Next, Figure 7 illustrates a generic Airprox. \n \n \nFigure 7. Sequence for a possible Airprox \u2013 see text for explanation \nFigure 7 illustrates the time sequence for a possible Airprox \u2013 the black aircraft might \ncollide with the grey one: three \u2018snapshots\u2019 in time are shown.  A problem of some \nkind is the precursor of a possible Airprox \u2013 it could be a miscommunication and a \nfailure to note a relevant piece of information.  This problem could lead to a mid-air \ncollision within a few minutes if nothing further is done.  At some point, this problem \nbegins, but this is probably not immediately detected by the aircrew or ATC.  Later, \npotential consequences of the problem are detected: this could be at any point from \nthe initiation of the problem to an automatic alert to the pilot\/controller.  Action then \ntakes place.  This action might be a new instruction to the pilot, a manoeuvre by the \npilot, or just close monitoring by ATC (if the aircrafts\u2019 projected flightpaths do not in \nfact lead to close proximity, then new instructions would not be necessary \u2013 or even \ndesirable).  Finally, the two aircraft reach their CPA.  This whole process is an \nAirprox. \n \nNote that the reporting of such an occurrence as a potential Airprox need not be a \nstatement by the reportee that the situation was \u2018hazardous\u2019.  It need only be a view \nby the reportee that \u2018this kind of aircraft configuration should not have occurred if the \nrules are followed properly\u2019.  To be hazardous in the terms used here, the \noccurrence must satisfy: \nOn the information available and without further action\/intervention, the \nflightpaths, allowing for typical variabilities, would produce a close proximity \nbetween the aircraft; and \nIn these circumstances, the remaining system layers would not prevent the \ndangerous CPA. \nTime \nproblem \nbegins \nproblem \ndetected \nClosest point \nof approach\nAction \n 14\nExpressed in a different way: \nA hazardous event is one in which there is a high degree of conflict plus a low \nconfidence that the remaining system layers would generally provide the \nnecessary corrective action. \nOne test is to judge the probability that the remaining system layers would deal with \nthe conflict in a straightforward fashion, in other words estimate the effects of the \nremaining \u03b1 and \u03b2 parameters.  What is the probability that the remaining system \nlayers will fail to resolve this high degree of conflict?  Turning the question around, if \nit had not been detected, is it highly probable that the system layers would have \nconverted to \u2018safe\u2019?  The expert judgement\/estimate that has to be made about an \nATM incident (such as an Airprox) is of the chance that the remaining system layers \nwill not resolve the situation safely.  This assessment might be made using \nsomething like the ARIBA (1999b) approach (which attempted to build an accident \nrisk tolerability\/acceptability matrix for air traffic operations on HSE (UK Health and \nSafety Executive, 1992 and 1999) lines).   \n5. DISCUSSION \nThis discussion section consists of seven largely independent sub-sections, mainly \nexploring some of the questions raised in the Introduction.   \n5.1 Real-Life Analyses \n5.2 Which incidents should be judged the most important to ATM system \nsafety? \n5.3 Are \u2018minor\u2019 incidents of any importance? \n5.4 The Value of the System Layer Concept \n5.5 Examples of Airproxes \n5.6 The Right Lessons from Mid-air Collisions? \n5.7 NATS Safety Significant Events \n5.1 Real-Life Analyses \nWhat are really the questions about ATM incidents that need to be answered by \nsafety managers and regulators?  What kinds of action should they take?  How can \nthese people demonstrate that they have done their job properly?  Certainly, one \nway of knowing what one should do is to contemplate the consequences of not doing \ncertain things.  This implies that they must act reasonably and rationally \u2013 but they \nmust be active rather than passive (ie ATM incidents must be analysed from the \nviewpoint of \u2018looking for trouble\u2019).   \n \n 15\nHowever, the public\u2019s and air traveller\u2019s concerns must surely be focused on the \nsafety levels achieved in the real world, rather than on speculations.  Why should \nthey care about \u2018what might have been in some theoretical alternative universe\u2019 \u2013 \ncomposed of \u2018what ifs\u2019 \u2013 in which things had been \u2018slightly different\u2019?  So is it valid or \nsensible to examine these \u2018what ifs\u2019?  \n \nThe answer is that it is necessary for ATM incident analysts to think in this kind of \nway.  This is not some nugatory or over-sophisticated \u2018icing on the cake\u2019.  The mark \nof the genuine safety expert is to be able to ask the right questions about incidents \nconcerning potential accidents.  The messages from an incident do not simply \n\u2018announce themselves\u2019 to the analyst: they provide the raw material for his or her \nproductive thoughts about system safety.   \n \nWhen aviation systems fail, they often do so fail in apparently complex and \nunpredictable ways (Amalberti, 2000; Wickens, 2003): the aviation expert is \nsomeone who perceives the fundamental causal and system factors.  Such people \nare rare individuals \u2013 the kind of structured thinking outlined here is intended to \nsupplement their expertise.   \n \n[If an expert had been able to predict the failure mode of the Comet aircraft \u2013 that \nmetal fatigue concentrated at the corners of the aircraft's windows would probably \ncause catastrophic crashes (three in 1953) \u2013 then the UK\u2019s commercial aviation \nindustry would have followed a very different path.  One aspect of the tragedy that is \nrelevant here was the absence of an earlier related minor incident that could have \nwarned the designers of this kind of failure mode.  How much worse would a \ncatastrophe be viewed by relatives and the public if it could have reasonably been \nprevented by learning from an earlier incident\u2019s characteristics?] \n \nOne way of recognising the need to examine \u2018what ifs\u2019, is to envisage an ATM \nsystem covering a large area.  For simplicity, assume a constant amount of traffic \nevery year, that the underlying \u2018safety culture\u2019 and regulatory framework is \nunchanging.  The nature of ATM incidents in such a scenario will vary in an infinite \nnumber of ways: there will be changes in aircraft types and their flight departure \ntimes, runway usage, meteorological conditions, choices made by controllers, etc, \nwill lead to different kinds of incidents occurring at different times from year to year.  \nHowever, the \u2018pattern\u2019 of these incidents \u2013 the causal nature of these incidents and \nthe action of the system layers \u2013 will be much the same from year to year; the \nobserved Dc values are effectively a \u2018sample\u2019 from this much larger population of \npotentially feasible conflict patterns.   \n \nThe expert is the person who can comprehend the underlying patterns revealed by \nincidents.  He or she can then rationally extrapolate those observed over many years \nto identify the most probable accident in a coming year. \n 16\n5.2 Which incidents should be judged the most important to ATM system \nsafety?   \nThe most important incidents to ATM system safety are surely those in which only \nthe Alert layer prevents a collision, because these represent failures of the previous \nsystem layers.  Was the incident reported subsequent to an Alert?  If so, the \nprotection against mid-collision could have depended on the chance relation \nbetween the aircraft trajectories relied \u2013 the lack of such an event could have been \nwholly fortuitous.  The analyst could therefore ask: \u201cIf one of the aircraft departure \ntimes had been (say) up to 30 seconds different, would there have been a collision?\u201d \n \nThere has been important work on the performance of the Alert layer (ie which relate \nto its \u03b1 and \u03b2 parameters).  As part of ICAO panel studies, Harrison (1993), reported \ncalculations and simulation results that: \nFor every 100 critical Airproxes and under a variety of assumptions about \ntraffic awareness and pilot responses (eg neglecting the use that a pilot might \nmake of TA information prior to an RA \u2013 a very cautious assumption): \n94 will be resolved safely \n6 will not be resolved \n4 new Airproxes will be induced because non-critical encounters are \nconverted into critical ones \nMcLaughlin (1999) has produced some current estimates on the benefits of ACAS. \nHale and Law (1989) provides an examination Simultaneous Operation of Conflict \nAlert and ACAS II in UK En-Route Airspace.   \n5.3 Are \u2018minor\u2019 incidents of any importance? \nATM incidents do not need to be deemed hazardous to be informative or to generate \naction.  The decision-maker needs to have in mind something like the HSE\u2019s ALARP \nphilosophy (Appendix B; HSE, 1992\/1999).  Given that the system is very safe, \nspecific safety management measures \u2013 changes to procedures, equipment, \nsoftware \u2013 should be implemented as long as such is reasonably practicable.  This \ngenerally implies some kind of analysis of costs and benefits.  To give an example \n(edited text from NATS, 2004): \n\u201cIn November 2002, an Airprox involving a Virgin 747 and a Delta 767, in \nwhich track data blocks for the two aircraft were inadvertently swapped on-\nscreen, which was subsequently assessed by the UKAB as category C (no \nrisk of collision).  Immediately after this particular incident, an instruction was \nissued to controllers reminding them of the correct procedure to follow when \nindividual track data blocks are re-positioned, in order to prevent a repeat of \nthese events.  In 2004, NATS changed its software, further improving the \nlegibility of track data blocks.  The change ensures that, whenever a data \nblock is moved, it will be automatically linked (by way of a strut on the screen) \nto the aircraft target to which it belongs.\u201d \n 17\nThus, key questions are: \u201cIs there evidence of a systematic design flaw?  Can it be \ncorrected without disproportionate cost?  Are international agreements an issue?\u201d \n5.4 The Value of the System Layer Concept \nThe safety of the system is the product of the effectiveness of the system layers.  \nThe questions are obvious ones.  Did they operate as they were planned?  Did a \nfailure in the earlier system layers produce an aircraft pair configuration and\/or \ncircumstances that might not generally be corrected by the remaining layers? \n \nIf the incident is a consequence of a known category of system layer failure, is the \nfrequency of this type of failure (= the \u03b1 value) increasing over time?  If so, what is \nbeing done to reduce the rate by \u2018tweaking\u2019 the system layers to improve their \ncapture of such failures?  Actions\/decisions include such things as additional training \nfor pilots or controllers, patches to airspace designs, tailoring of STCA\/ACAS, etc. \n \nIt is not sufficient to focus on the extent to which separation minima are breached, or \nthe nature of actions by the pilot\/controller.  These are vital issues, but they must be \nseen in the context of the system control provided by the system layers.  A low CPA \nmay represent little hazard if, in those particular circumstances, both pilots could see \nthe other aircraft visually and on their ACAS displays.  A larger CPA for an event not \ndetected by either pilots or controller, and subject to a very late ACAS alert because \nof manoeuvring aircraft, is a much more serious matter in system safety terms.  The \npilot\/controller may have taken no action because there was insufficient time for it to \nbe effective of because they judged that it might make a bad situation even worse.  \nAn STCA alert and\/or a pilot\/controller action represent the system controls \u2013 the \nsystem layers \u2013 functioning effectively, rather than a necessarily hazardous situation. \n \nIt almost goes without saying that monitoring the frequency of failures \u2013 the \u03b1 and \u03b2 \nparameters \u2013 is vitally important.  For example, the ATM provider might monitor \n\u2018Safety Separation Breaches\u2019, composed of incidents in which separation was \nsignificantly breached (rather than a minor infringement by a fraction of a nm) or in \nwhich the controller had to act on a STCA alert (Brooker, 2004).   \n \nAnother way in which this kind of model is useful is to consider how the different \nlayers vary in different types of ATM operation.  For instance, in Oceanic operations \nthe operational layer is comparatively weak compared with radar control, but the \nplanning layer is generally very effective in reducing the degree of conflict \u2013 but see \nthe next sub-section.   \n5.5 Examples of Airproxes \nIt is worth examining half a dozen examples of Airproxes to back up the arguments \nof the preceding text.  None of these incidents was rated as deserving the (highest) \nAirprox category A.  But to what extent do they provide important lessons about \npotential accidents?  Some of these particular incidents were chosen because the \nUKAB text used or implied the word \u2018fortuitous\u2019.  The following text \u2013 Figure 8 \u2013 is of \n 18\ncourse heavily summarised from the original material [NB: 127\/99 is incident number \n127 in 1999, etc and the aircraft are A and B]: \n \nAirprox 127\/99 Recorded Separation was 1100 ft \nContext & Issues Warnings 1.7.1.1.1.1 Event, Monitoring & Intervention \nExtremely busy \nperiod, issues \nabout display of \ndata blocks \nOutside STCA \nparameters.  \nACAS RAs to both \naircraft \nController had issued a descent clearance that would \nhave led aircraft A to descend through the level of \naircraft B, which he had inadvertently not taken into \naccount. \n \n \nAirprox 29\/00 Recorded Separation at CPA was 0.6 NM and 600 ft. \nContext & Issues Warnings Event, Monitoring & Intervention \nFlight deck \nprocedure error, \nfatigue \nACAS RA both to \nA and B. \nSTCA activated \nbefore ACAS \npassed to ATC \nAircraft descended below its cleared level.  Neither pilot \nsaw the other aircraft.  ATC occupied with other traffic, \ndid not spot high descent rate.  Board agreed that any \nseparation was to a large degree fortuitous\u2026with \ndifferent geometry \u2026could have been considerably \nmore serious. \n \nAirprox 54\/00 Recorded Separation at CPA was 12 NM \nContext & Issues Warnings Event, Monitoring & Intervention \nIn oceanic \nairspace, period of \ndata processing \n(FDPS) manual \nreversion \nNo warnings Neither pilot saw the other aircraft.  \u2026the SCACC \nDomestic Controller\u2026detect[ed] the conflict from a \nroutine scanning of her radar display.\u2026an ATCO \nmember said that, in his opinion, separation in this case \nwas fortuitous.  had the encounter taken place further \nwest, outside the cover of domestic radar, the \nconfliction would have remained undetected and the \noutcome might have been more serious\u2026 \n \nAirprox 145\/00 Recorded Separation was 0 45 NM and 400 ft \nContext & Issues Warnings Event, Monitoring & Intervention \nPrevious military \nfighter formation, \nsevere weather, \nATC preoccupied \nwith other conflict, \nworkload  \nACAS RA to A \nonly.  ATC at \nairport not STCA \nequipped.  LATCC \nSTCA alert \nATC did not maintain standard separation between \naircraft A and B.  Pilot A, because of weather, saw B \nonly after the ACAS alert.  Pilot B heard the ACAS alert \non RT and saw A subsequently.  ATC was not aware of \nconflict until advised of ACAS alert. \n \nAirprox 164\/03 Recorded Separation was 3.7 NM and 500 ft \nContext & Issues Warnings 1.7.1.1.1.2 Event, Monitoring & Intervention \nNo apparent ATC \ncausal factors, \nnon-standard \nphraseology \nSTCA alerted \nafter the aircraft \nhad received an \nACAS RA.   \nAircraft A crew descended below their cleared level into \nconflict with aircraft B.   \n \nAirprox 184\/03 Recorded Separation was 3.4 NM and 600 ft \nContext & Issues Warnings 1.7.1.1.1.3 Event, Monitoring & Intervention \n 19\nController with an \ninexperienced \ntrainee, combined \nsector \u2013 \u2018busy\u2019 but \nwithin \ncapabilities\u2019.   \nSTCA activated \nand shortly \nafterwards an \nACAS RA climb \nwas issued.   \nThe aircraft B crew read back the wrong heading and \nlevel instructions, which went undetected by the \ncontroller.  The controller said he had no reason to \ndoubt that the aircraft would not comply with the issued \nclearance.   \nFigure 8. Edited text from Airprox Reports \nEach of these incidents exhibits the potential for a more serious incident or even an \naccident.  Most of them show the ATM system getting into an operational state in \nwhich recovery was not assured by subsequent system defensive processes, or \nwhere the failure\/error occurred at such a late stage that the remaining safety \ndefensive layers had a large element of chance.  Each of them should therefore be \nmarked as having more safety significance than incidents in which (say) STCA gave \na very early warning and the controller was able to resolve the problem before any \nkind of ACAS warning was given. \n \nSeveral of the incidents show well-known Human Factors aspects.  Airprox 127\/99 is \nan example of a failure of prospective memory (eg Loft et al, 2003).  Airprox 29\/00 is \nan example of an aircrew procedure error with fatigue as a factor.  Airprox 145\/00 is \na loss of the necessary situation awareness (eg Endsley et al, 2003), given a \ncomplex set of circumstances.  Communications procedures are well known to be a \nsource of potential problems, eg Airprox 184\/03. \n \nThe oceanic incident, Airprox 54\/00 is perhaps an example of a (currently?) inherent \nATM system design limitation.  The North Atlantic region ATM system does not have \nradar coverage, hence it does not have STCA available.  The necessary safety is \ndelivered by the use of large, essentially procedural separation minima, Mach \nNumber flying techniques, and positional reports every 10 degrees of longitude.  \nACAS is a critical safety defensive layer.   \n \nSome defensive system layers do not function for certain types of manoeuvre.  If the \naircrew or the controller mistakenly climb or descend an aircraft, then a potential \nconflict may not be picked up in time for STCA to alert, so the hazard is reduced by a \ncombination of ACAS and the pilot\u2019s see-and-avoid, eg Airproxes 164\/2003 and \n127\/99.   \n \nEquipment failures and mal-functions are comparatively rare these days compared \nwith Human Factors issues (in the broadest sense).  Airprox 54\/00 notes a case of \nFDPS manual reversion, which could affect considerably the oceanic safety layers.  \nAirprox 222\/02, referred to in the previous sub-section, arose because of a confusing \ndisplay of overlapping track data blocks and aircraft symbols.  This led the controller \nto confuse the relative positions of the two aircraft so that one was descended into \nconflict with the other.   \nA caveat needs to be entered about the degree to which the safety system layers \ncan be fully understood  solely from this kind of incident data.  In particular, incidents \nrarely give much insight into the working of the Planning layer.  The Planning layer is \nnot designed to eliminate all conflicts (except, as already noted, in procedural control \nin oceanic systems), so the fact that aircraft may be in conflict is not an incident until \n 20\nthe Operational layer fails to detect and resolve the conflict.  This implies the need to \ncollect other precursor information.  How many conflicts of different types are being \nsolved routinely by the existing safety system?   \n5.6 The Right Lessons from Mid-air Collisions? \nEuropean mid-air collisions in controlled airspace are rare.  Their characteristics can \nbe analysed in exactly the same way to incidents.  On 1 July 2002, two ACAS-\nequipped aircraft collided over the Swiss-German border at \u00dcberlingen.  One \nimportant feature of the accident was that the flight crew of one aircraft did not follow \nthe ACAS alert, but followed instead the ATC instruction.  Guidance material now \nstresses that pilots should follow alerts and that controllers should not attempt to \nmodify the flight path of an aircraft responding to an alert.   \n \nThe full official report on this tragedy has recently been published (BFU, 2004).  \nThere have already been attempts to analyse the causal factors involved, eg Nunes \nand Laursen (2004) identify six \u2018contributing factors\u2019: Single Man Operations, \nDowngraded Radar [STCA], Dual Frequency Responsibility, Phone System, ACAS, \nCorporate Culture.   \n \n \nFigure 9. Components of ATM System safety \nOne clear message from \u00dcberlingen is that it is not only the providers who must take \nresponsibility for preventing accidents.  In Figure 9, ATC is air traffic control, CNS is \nthe communications, navigation and surveillance systems, DP is all the data \nprocessing and information flows.  These can be termed system \u2018Guardians\u2019, as \ndistinct from the various service providers.  The Government sets up the regulatory \nregime; the airspace policy has to recognise the needs to accommodate all users: \ncommercial flights, military flights and general aviation.  The safety regulator has to \nbe confident that everything works safely: \u201cTo require enterprises to take proper \naccount of the hazards to which they expose people\u201d.  The regulator cannot always \nbelieve what it is told by or reads in documents by the service provider.  The \n \n  \nCNS\/DP \nATC \nTCAS \nSafety \nRegulation & \nAirspace Policy\nGovernment \nPROVIDERS \nGUARDIANS \n 21\nregulator\u2019s job does not end there, for example, it must ensure that the system \ndelivers the right kinds of safety-related training.   \n \nThis multiplicity of factors indicates the unusual nature of the \u00dcberlingen accident.  A \npicture of the mid-air collision on the lines of Figure 5 would show a very high conflict \nlevel at every point in the diagram, with the final failure to act correctly on the ACAS \nwarning raising the conflict level to the actual collision event.  But if the accident had \nnot occurred \u2013 if the aircraft had, by providence, missed by several hundred feet \u2013 \nwould the same kinds of international actions have been taken by regulators and \nproviders?  These questions just make an even stronger case for international \nlearning from the collection and thorough analysis of incidents.  This learning has to \nidentify those incidents that reveal inherent system control flaws or regulatory \ngaps\/inconsistencies and offer ways of dealing with them.   \n5.7 NATS Safety Significant Events \nAirprox classifications represent an example of what might be termed a \u2018Board of \nInquiry\u2019 perspective, exemplified in UK aviation by the work of the AAIB (2004), \nwhich examines accidents and incidents.  The AAIB\u2019s remit is stated as:  \n\u201cThe fundamental purpose of investigating accidents is to determine the \ncircumstances and causes of the accident\u2026It is not to apportion blame or \nliability\u201d \nThis appears straightforward \u2013 but the hidden text is: \n\u201cwith a view to the preservation of life and the avoidance of accidents in the \nfuture\u201d, \nwhich actually coincides with a central message here about asking rational and \nprofessional \u2018what if\u2019 questions. \n \nSafety managers and analysts working for ATC providers do attempt this kind of \nthing.  An example is NATS\u2019 work on the Safety Significant Event (SSE) scheme, \nwhich has been under development since the mid-1990s [NB: the author participated \nin the early work].  Eurocontrol has recently given support to this kind of approach for \ncategorising incidents (eg Eurocontrol, 2004).   \n \nSSEs in a radar control environment are defined relative to bands:  \nBand 1: Separation \u2264 66% of the prescribed separation.   \nBand 2: Separation > 66% of the prescribed separation.   \nSSEP: A Band 2 incident, which involved aircraft losing separation or potentially losing \nseparation with another aircraft where there was a possible ATC error.   \nSSE4: A Band 1 incident which was detected and resolved in the most effective and timely \nmanner by the controller who was providing the service when the incident occurred, and no \nsystems failures or procedures affected the resolution.   \nSSE3: A Band 1 event which was detected and resolved by ATC but:  \no it was not resolved by the controller who was providing the service when the event \n 22\nwas initiated; or  \no it was detected by colleague warning, STCA or pilot query; or  \no it was not resolved in either a timely manner; or  \no it was not resolved in an effective manner; or \no systems or procedures failures affected the resolution. \nSSE2: A Band 1 event, which was resolved by the pilot\/other. \nSSE1: A Band 1 event for which no timely\/effective pilot\/other action was taken to resolve the \nevent (providence) or there was a high risk that the action taken would not have been \nsuccessful. \nFigure 10. SSE classification re \u2018Loss of separation in a radar control environment\u2019 \nFigure 10 shows how incident data on breaches of separation is classified according \nto the SSE scheme.  The similarity to the kinds of ideas explored here is in the use of \nwords such as \u2018providence\u2019 and \u2018action taken would not have been successful\u2019. \n \nRecent discussions by the author with NATS experts suggest that the SSE scheme \nhas been judged very successful in safety management terms, in particular that it \nhas helped the organisation to focus on key safety issues.  Unfortunately, there has \nbeen little published by NATS in the safety literature on how the scheme is used.  \nSearches on the web and on academic electronic databases (in particular the British \nLibrary's Electronic Table of Contents \u2018zetoc\u2019) reveal very few open-literature \ntechnical documents about the use of SSEs.  One that is very much worth \nmentioning is the work by Neil et al (2003), which addresses the same kind of \nproblem as that examined here.   \n6. CONCLUSIONS \nThe aim is to try to ensure that analysis and expert judgement about ATM incidents \ncan be carried out within a systematic and consistent safety framework.  Hazards \nand risks are not \u2018facts\u2019 or \u2018events\u2019 that \u2018exist\u2019, but rather judgements made about \nconditional futures and their consequences.  This judgement (or perception or \nopinion) is about the degree of possibility of some unpleasant state of things that \nmay come into existence at some future time.  A hazardous situation is not one in \nwhich aircraft happened to be close, but rather one in which the outcome was not \n\u2018system controlled\u2019, with some potential outcomes having significant negative \nconsequences.  System controls in this sense cover all the means by which the \nsystem is held stable (= defended) against the potential negative consequences. \n \nThe ATM system can be (over-) simplified to consist of three structural system layers \nacting as the system controls: Planning (pre-operational), Operation (the flight in \nprogress), and Alert (the ground and air protection enabled by STCA and ACAS, on \nwhich the controller\/pilot will act).  Each Layer acts to reduce the frequency of high \nconflict events in a probabilistic fashion.  ATM safety improvements correspond to \nmonitoring and acting upon two probabilities \u03b1 and \u03b2 for each system layer: these \nare respectively the proportion of hazardous situations that remain hazardous and \nthe proportion of safe instances that become hazardous.   \n \n 23\nA hazardous event is one in which a high degree of conflict between aircraft is \nobserved plus a low confidence that the remaining system layers would generally \nprovide the necessary corrective action.  The expert judgement\/estimate that has to \nbe made about the hazardousness of an ATM incident (such as an Airprox) is of the \nlikelihood that the remaining system layers will not resolve the situation safely.   \n \nIt is necessary for ATM incident analysts to think in this kind of way \u2013 and indeed for \ntheir managers to ensure that they have enough time to think.  It is not enough just \nfor the analysts to be intelligent, knowledgeable and energetic.  Just \u2018keeping to the \nfacts\u2019 could fail to anticipate the future.  The mark of the genuine safety expert is to \nbe able to ask the right questions concerning potential accidents \u2013 to show \nimagination and system insight.  The messages from an incident do not simply \n\u2018announce themselves\u2019 to the analyst.  However, they do provide the raw material for \nproductive thoughts and wise judgements about system safety.  Thus, ATM safety \nanalysts need first to get data to determine the general effectiveness of the different \nsafety layers, and then to be able to determine the differences in layer performance \nfor different varieties of event. \n \nReturning to the kinds of specific safety questions that can be asked about ATM \nincidents, some answers can be attempted: \n \nWhich incidents should be judged the most important to ATM system safety?  \nThe most important ones are those in which only the final stages of the Alert \nlayer prevented a collision, because this represents a failure of the previous \nsystem layers.  The protection against mid-collision could have depended on \nthe chance \u2013 fortuitous \u2013 relation between the aircraft trajectories relied.  \u201cIf \none of the aircraft departure times had been (say) up to 30 seconds different, \nwould there have been a collision?\u201d  Would there have been enough time to \nanalyse, decide and act successfully?   \n \nWhich incidents give most guidance about potential accidents?  The crucial \nincidents are those in which the system got into a state in which recovery was \nnot assured by subsequent system defensive processes, or where the \nfailure\/error occurred at such a late stage that the remaining safety defensive \nlayers had a large element of chance.  Was an accident prevented by design \nor by chance?  Were, for example, the system defensive layers there (but did \nnot function to specification), weakened (through other elements in the system \nenvironment), or absent (perhaps because of regulatory flaws\/gaps)? \n \nIn what ways should incidents be categorised and analysed to help pinpoint \nkey safety issues?  Additional categorisation schemes probably do not add \nmuch to safety improvement!  Describing, in ever greater detail, the symptoms \nof a patient\u2019s illness is much less important than finding a cure for that illness.  \nThe important thing is to highlight flaws in system controllability by using \nsomething like the transition path picture; and to identify from this what might \nbe possible solutions (or to recognise openly the inherent limits of present \nATM system design, technology and operation). \n \n 24\nAre \u2018minor\u2019 incidents of any safety importance?  Yes.  Incidents that are minor, \nin terms of the \u2018Board of Inquiry view\u2019 of the actual event, can be very \ninformative or even generate decisive action.  Is there evidence of a \nsystematic design flaw?  Can this flaw be corrected without disproportionate \ncost?  Are international agreements an issue?  Does the incident reveal \nreadily correctable flaws in regulatory instructions or training? \n \nHow should the relevant importance of different incidents be assessed or \nweighted to provide a true picture of the health of the ATM safety system?  \nThe vital need is international learning from incident reports.  ATM systems in \ndeveloped countries use much the same equipment and operating concepts, \nso that \u2018ATM health\u2019 is (at least) that of the European system.  This learning \nhas to identify those incidents that reveal inherent system control flaws or \nregulatory gaps\/inconsistencies \u2013 and offer ways of dealing with them.  This \nsafety evolution has to be an international process, not a national one.  Safety \nexperts must use all the information they can get to improve the ATM system, \nwhich strongly supports the need for international openness about safety data \nand its analysis. \n \nThe key safety management question, for both Providers and Guardians, to bear in \nmind is: \u201cIf \u00dcberlingen had been a severe incident not an accident, would the same \nsafety lessons have been learned or pursued so vigorously?  Would it have been put \nto one side as a \u2018unique event\u2019?\u201d   \nACKNOWLEDGEMENTS \nThis work was in part supported by a research grant by the Civil Aviation Authority\u2019s \nSafety Regulation Group (SRG).  I would like to thank SRG staff; Ian Parker, the \nHead of NATS Safety Management Development for updating me on work in NATS; \nand both he and Mike Shorthose of Helios Technology Ltd for their comments on \nearlier drafts.  I would like to thank the Director of the UKAB and his colleagues for \ntheir help.  I would also like to thank the referees for their insightful, indeed \nmotivational, comments. \n 25\nREFERENCES \nAAIB [UK Air Accidents Investigation Branch} (2004). Website. \nhttp:\/\/www.dft.gov.uk\/stellent\/groups\/dft_control\/documents\/contentservertemplate\/df\nt_index.hcst?n=5161&l=1 \nAmalberti, R. (2000). The paradoxes of almost totally safe transportation systems. Safety \nScience, 1, 1-16. \nARIBA [ATM system safety criticality Raises Issues in Balancing Actors responsibility] \n(1999a). WP4 Final Report: Human Operators Controllability of ATM Safety. European \nCommission Project ARIBA\/NLR\/WP4\/FR.  http:\/\/www.nlr.nl\/public\/hosted-\nsites\/ariba\/rapport4\/index.htm  \nARIBA (1999b) WP6 Final Report Part II: Safety Cases for a new ATM operation. European \nCommission Project ARIBA\/NLR\/WP6\/FR-II.  http:\/\/www.nlr.nl\/public\/hosted-\nsites\/ariba\/rapport6\/part2\/title.htm. \nBaumgartner, M. (2003). One safe sky for Europe \u2013 A revolution in European ATM.  The \nController, July, 8-12. \nBFU [German Federal Bureau of Aircraft Accidents Investigation] (2004). Investigation \nReport \u2018\u00dcberlingen Mid-air collision\u2019. AX001-1-2\/02. http:\/\/www.bfu-\nweb.de\/berichte\/02_ax001efr.pdf \nBird, F. (1974). Management Guide to Loss Control. Institute Press, Atlanta, Georgia. \nBrooker, P. (2002). Future Air Traffic Management: Quantitative En Route Safety \nAssessment Part 2 \u2013 New Approaches. Journal of the Institute of Navigation 55(3), 363-379. \nBrooker, P. (2004). Why the Eurocontrol Safety Regulation Commission Policy on Safety \nNets and Risk Assessment is Wrong. Journal of the Institute of Navigation 57(2), 231-243.  \n(in press). \nCAA [Civil Aviation Authority] (2003). The Mandatory Occurrence Reporting Scheme. CAP \n382. CAA, London.  http:\/\/www.caa.co.uk\/docs\/33\/CAP382.pdf \nChambers (2001).  Chambers 21st. Century Dictionary.  Chambers Harrap, Edinburgh. \nhttp:\/\/www.chambersharrap.co.uk\/chambers\/chref\/chref.py\/main?query=hazard&title=21st. \nCourteney, H. and Newman, T. (2003). Taming Human Error with a Systems Approach. \nFSF\/IASS Conference, Washington, USA. \nCox, D. R. and Miller, H. D. (1977). The Theory of Stochastic Processes.  Chapman & Hall, \nLondon. \nEndsley, M.R., Bolte, B. and Jones, D.G. (2003). Designing for Situation Awareness. Taylor \nand Francis, London. \nEurocontrol SRC (1999). ESARR 2 Guidance to ATM Safety Regulators: Severity \nClassification Scheme for Safety Occurrences in ATM. EAM 2\/GUI 1. Eurocontrol, Brussels. \nhttp:\/\/www.eurocontrol.int\/src\/documents\/deliverables\/esarr2_awareness_package\/eam2gui\n1e10ri.pdf \nEurocontrol SRC (2002). ESARR 2 Mapping between the Eurocontrol Severity Classification \nScheme & the ICAO Airprox Severity Scheme. EAM 2\/GUI 3. Eurocontrol, Brussels. \nEurocontrol (2004). Integra Safety Metrics. \nhttp:\/\/www.eurocontrol.int\/care\/integra\/safety_metric.htm \n 26\nGraham, R., Hoffman, E., Pusch, C. and Zeghal K. (2003). Absolute versus Relative \nNavigation: Theoretical Considerations from an ATM Perspective. 5th Eurocontrol\/FAA ATM \nR&D Seminar, Budapest, Hungary \nGreenwell, W. S., Knight, J. C. and Strunk, E. A. (2003). Risk-based Classification of \nIncidents. Second Workshop on the Investigation and Reporting of Incidents and Accidents \n(IRIA 2003). http:\/\/shemesh.larc.nasa.gov\/iria03\/p03-greenwell.pdf \nHale, S. and Law, M. (1989). Simultaneous Operation of Conflict Alert and ACAS II in UK \nEn-Route Airspace. DORA Report 8914, CAA. \nHarrison, D. (1993). Results of ACAS II Safety Analysis. ICAO Secondary Surveillance \nRadar Improvements and Collision Avoidance Systems Panel (SICASP\/5) \nHSE [Health and Safety Executive] (1992 and 1999). The Tolerability of Risk from Nuclear \nPower Stations. HMSO; Reducing Risks, Protecting People. HSE Books. \nJAA [Joint Airworthiness Authorities] (2000). Advisory Joint Material relating to JAR 25 Large \nAeroplanes. AMJ 25.1309. Change 15. Joint Airworthiness Authorities. \nLadkin, P. B. (1998). Notes on the Foundations of System Safety and Risk. RVS-Bk-00-01. \nRVS Group, Faculty of Technology, University of Bielefeld.  http:\/\/www.rvs.uni-\nbielefeld.de\/publications\/books\/safetyNotes.pdf. \nLadkin, P. B. (2000). Causal Analysis of Aircraft Accidents Computer Safety, Reliability and \nSecurity, Proceedings of the 19th International Conference, SAFECOMP 2000, Lecture \nNotes in Computer Science No. 1943, Springer-Verlag, 2000. At: http:\/\/www.rvs.uni-\nbielefeld.de\/cms\/publications\/records \nLoft, S., Humphreys, M. and Neal, A. (2004).  Prospective memory in air traffic control. In, \nEdkins, G. and Pfister, P. (Eds.), Innovation and consolidation in aviation. Aldershot, UK \nAshgate.  \nMcLaughlin, M. (1999). Predicting the Effect of TCAS II on Safety. Air Traffic Control \nQuarterly, 7(1), 1-18. \nNational Air Traffic Services Ltd [NATS] (2004). Safety Website. \nhttp:\/\/www.nats.co.uk\/about\/safety.html \nNATS (2004). Press notice, NATS responds to media comments on 2002 airprox. \nhttp:\/\/www.nats.co.uk\/news\/index.html  \nNeil, M., Malcolm, B. and Shaw, R. (2003). Modelling an Air Traffic Control Environment \nUsing Bayesian Belief Networks. 21st International System Safety Conference, Ottawa, \nOntario, Canada.  \nNunes, A. & Laursen, T. (2004). Identifying the factors that led to the Ueberlingen mid-air \ncollision: implications for overall system safety. Proceedings of the 48th  Annual48th Annual \nChapter Meeting of the Human Factors and Ergonomics Society, New Orleans, LA, USA. \nhttp:\/\/www.aviation.uiuc.edu\/UnitsHFD\/conference\/humfac04\/nuneslaur.pdf. \nPerrow, C. (1984). Normal Accidents: Living with High-Risk Technologies. Basic Books, New \nYork. \nRasmussen, J. (1990). Human error and the problem of causality in analysis of accidents.  \nPhilosophical Transactions of the Royal Society B 327, 449-462. \nReason, J. (1990). Human Error. Cambridge University Press, Cambridge UK. \nReview of the General Concept of Separation Panel (RGCSP) (1995). Working Group A \nMeeting: Summary of Discussions and Conclusions. (1995). ICAO. \n 27\nShappell, S. A. and Wiegmann, D. (2001). Applying Reason: The Human Factors Analysis \nand Classification System (HFACS). Human Factors and Aerospace Safety 1, 59-86.  \nSosa, E. and Tooley, M. (1993) editors. Causation. Oxford Readings in Philosophy. Oxford \nUniversity Press, Oxford. \nUK Airprox Board. (1999 onwards - biannual). Analysis of Airprox in UK Airspace. \nwww.ukab.org.uk. \nVilliers, J. (1968). Perspectives for Air Traffic Control for Advanced Phases of Automation - \nthe Method of Layers (in French: Perspectives pour le contr\u00f6le de la circulation a\u00e9rienne \ndans les phases avanc\u00e9es d'automatisation - la m\u00e9thode des filtres), Navigation n\u00b0 61, \nJanuary. \nWeick, K.E. (1976). Educational organizations as loosely coupled systems. Administrative \nScience Quarterly, 21(1), 1-19. \nWickens, C. D. (2001). Attention to Safety and the Psychology of Surprise. 11th International \nSymposium on Aviation Psychology. Columbus, Ohio. \nhttp:\/\/www.aviation.uiuc.edu\/UnitsHFD\/conference\/Osukeynote01.pdf \nWiegmann, D. and Shappell, S. A. (2001); Applying the Human Factors Analysis and \nClassification System (HFACS) to the analysis of commercial aviation accident data. Paper \nat the 11th International Symposium on Aviation Psychology. Ohio State University, \nColumbus.  \nWiegmann, D. and Shappell, S. A. (2003). A Human Error Approach to Aviation Accident \nAnalysis: The Human Factors Analysis and Classification System. Ashgate Publishing \nCompany. \n 28\nAPPENDIX A \n \n \nWORD DEFINITIONS RELEVANT TO HAZARDOUS EVENTS \n \nFigure A1 sets out a group of word definitions relevant to hazardous events.  These \nare extracted from a well-established dictionary (Chambers, 2001); the deleted text \nis irrelevant material (eg that a metal box used to store valuables is a \u2018safe\u2019).  [The \nwords selected here also include some relating to factual evidence \u2013 \u2018actual\u2019 and \n\u2018existing\u2019, which will be used later here.]   \n \nWord Definition \u2013 Relevant Extracts \nchance noun  1 the way that things happen unplanned and unforeseen.  \n2 fate or luck; fortune.  3 an unforeseen and unexpected \noccurrence\u2026 \ndanger noun 1 a situation or state in which someone or something may \nsuffer harm, an injury or a loss\u2026 2 something that may \ncause harm, injury or loss.  3 a possibility of something \nunpleasant happening.  [from French dangier power, \ntherefore 'power to harm'] \ndangerous adjective likely or able to cause harm or injury \nhazard noun 1 a risk of harm or danger.  2 something which is likely to \ncause harm or danger\u2026 4 chance; accident \nhazardous adjective 1 very risky; dangerous.  2 depending on chance; \nuncertain \nrisk noun 1 the chance or possibility of suffering loss, injury, \ndamage, etc; danger.  2 someone or something likely to \ncause loss, injury, damage, etc\u2026 \nsafe adjective \n \n1 free from danger or harm.  2 unharmed.  3 giving \nprotection from danger or harm; secure\u2026 4 not dangerous \nor harmful\u2026 5 involving no risk of loss; assured\u2026 7 \ncautious \nsafety noun 1 the quality or condition of being safe\u2026 \n  \nactual adjective \n \n1 existing as fact; real.  2 not imagined, estimated or \nguessed.  3 current; present.  [from French actuel, \nmeaning 'demonstrated by one's actions'] \nexist verb (existed, \nexisting) \n1 to be, especially to be present in the real world or \nuniverse rather than in story or imagination.  2 to occur or \nbe found\u2026 \nstraightforward \nadjective \n1 without difficulties or complications; simple\u2026 \n \nFigure A1. Dictionary Definitions of Hazard-related terms (Chambers (2001) \n \n 29\nAPPENDIX B \n \n \nHSE RISK ASSESSMENT \n \nThe Health and Safety Executive (HSE, 1992, 1999) version of risk assessment is \nillustrated in Figure B1 \n \n \nRisk reduction \nRegardless of cost \n \nRelevant Good Practice \nPlus \nRisk reduction \nMeasures \nPlus \nGross \nDisproportion \n \n \nRelevant Good \nPractice \n \n \n \n \n \n \nFigure B1.  ALARP Approach (taken from HSE) \n \n \nRisk is classified by the HSE as being in one of three categories: intolerable, \ntolerable if ALARP, and broadly acceptable (\u2018negligible\u2019 in some variants) (Figure \nB1).  Note that the boundary lines between the risk categories negligible, tolerable, \nand intolerable need to be specified; they are not automatically set.   \n \nA checklist of (simplified) HSE definitions is: \nALARP principle The principle that no risk in the tolerability region can be \naccepted unless reduced \u2018As Low As Reasonably Practicable\u2019. \nbroadly acceptable risk A risk which is generally acceptable without further \nreduction.   \nintolerable risk A risk which cannot be accepted and must be reduced.   \ntolerability region A region of risk which is neither high enough to be \nIntolerable \nTolerable if ALARP \nBroadly Acceptable \n 30\nunacceptable nor low enough to be broadly acceptable.  Risks in this region \nmust be reduced ALARP.   \n \nThe decision processes are: \n\u2022 If a system\u2019s risk falls into the intolerable category, then action must be \ntaken to redress this.  If this is not possible, the system should be halted or \nnot implemented.   \n\u2022 If a system\u2019s risk falls into the tolerable category, it must be proven that it \nis low as reasonably practicable within that region for the system to be \nconsidered acceptable.  Thus, showing a system is ALARP means \ndemonstrating that any further risk reduction in the tolerable zone is either \nimpracticable or \u2018grossly disproportionate\u2019 (ie it can be shown that the cost \nof the measure is far in excess of any benefit to be gained).   \n\u2022 If a system\u2019s risk falls into the negligible category, no action is required \nother than monitoring to ensure that the negligible risk is maintained.   \n \nAn examination of \u2018what people actually do in aviation\u2019 suggests that the message is \nthat the regulators nearly always operate in the \u2018tolerable if ALARP\u2019 region \u2013 it \ncannot be \u2018intolerable\u2019 because flights would therefore have to stop or being tightly \nconstrained.  It cannot be broadly acceptable because there are always incidents \nand accidents that the public expects regulators to examine to see if things can be \ntightened up.  In the middle region, the regulators usually deal with \nprocedural\/organizational changes, which are often of low cost (ie tend to involve \nneither substantial infrastructure capital investment nor dramatically changed \noperational concepts). \n \nThus, in the ALARP region, specific safety management measures should be \ndefined (eg safety monitoring, safety improvement projects, etc.) as long as such is \nreasonably practicable.   \n"}