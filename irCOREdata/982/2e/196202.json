{"doi":"10.1137\/090752365","coreId":"196202","oai":"oai:lra.le.ac.uk:2381\/8366","identifiers":["oai:lra.le.ac.uk:2381\/8366","10.1137\/090752365"],"title":"Quasi-Monte Carlo Method for Infinitely Divisible Random Vectors via Series Representations","authors":["Imai, Junichi","Kawai, Reiichiro"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-06-24","abstract":"An infinitely divisible random vector without Gaussian component admits representations of shot noise series. Due to possible slow convergence of the series, they have not been investigated as a device for Monte Carlo simulation. In this paper, we investigate the structure of shot noise series representations from a simulation point of view and discuss the effectiveness of quasi-Monte Carlo methods applied to series representations. The structure of series representations in nature tends to decrease their effective dimension and thus increase the efficiency of quasi-Monte Carlo methods, thanks to the greater uniformity of low-discrepancy sequence in the lower dimension. We illustrate the effectiveness of our approach through numerical results of moment and tail probability estimations for stable and gamma random variables","downloadUrl":"http:\/\/scitation.aip.org\/getabs\/servlet\/GetabsServlet?prog=normal&id=SJOCE3000032000004001879000001&idtype=cvips&gifs=yes.","fullTextIdentifier":"https:\/\/lra.le.ac.uk\/bitstream\/2381\/8366\/3\/SCE001879%5b1%5d.pdf","pdfHashValue":"0c61888d704c85f3e03b4fa0bf5cc1543f4e9320","publisher":"Society for Industrial and Applied Mathematics (SIAM)","rawRecordXml":"<record><header><identifier>\n        \n            \n                oai:lra.le.ac.uk:2381\/8366<\/identifier><datestamp>\n                2016-04-13T15:32:23Z<\/datestamp><setSpec>\n                com_2381_445<\/setSpec><setSpec>\n                com_2381_9549<\/setSpec><setSpec>\n                col_2381_3823<\/setSpec>\n            <\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:doc=\"http:\/\/www.lyncode.com\/xoai\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n            \nQuasi-Monte Carlo Method for Infinitely Divisible Random Vectors via Series Representations<\/dc:title><dc:creator>\nImai, Junichi<\/dc:creator><dc:creator>\nKawai, Reiichiro<\/dc:creator><dc:subject>\nquasi-Monte Carlo method<\/dc:subject><dc:subject>\neffective dimension<\/dc:subject><dc:subject>\ngamma process<\/dc:subject><dc:subject>\nmoment estimation<\/dc:subject><dc:subject>\ntail probability estimation<\/dc:subject><dc:subject>\nPoisson process<\/dc:subject><dc:subject>\nshot noise<\/dc:subject><dc:description>\nAn infinitely divisible random vector without Gaussian component admits representations of shot noise series. Due to possible slow convergence of the series, they have not been investigated as a device for Monte Carlo simulation. In this paper, we investigate the structure of shot noise series representations from a simulation point of view and discuss the effectiveness of quasi-Monte Carlo methods applied to series representations. The structure of series representations in nature tends to decrease their effective dimension and thus increase the efficiency of quasi-Monte Carlo methods, thanks to the greater uniformity of low-discrepancy sequence in the lower dimension. We illustrate the effectiveness of our approach through numerical results of moment and tail probability estimations for stable and gamma random variables.<\/dc:description><dc:date>\n2010-08-09T13:19:40Z<\/dc:date><dc:date>\n2010-08-09T13:19:40Z<\/dc:date><dc:date>\n2010-06-24<\/dc:date><dc:type>\nArticle<\/dc:type><dc:identifier>\nSIAM Journal on Scientific Computing, 2010, 32 (4), pp. 1879-1897.<\/dc:identifier><dc:identifier>\n1064-8275<\/dc:identifier><dc:identifier>\nhttp:\/\/epubs.siam.org\/doi\/abs\/10.1137\/090752365<\/dc:identifier><dc:identifier>\nhttp:\/\/hdl.handle.net\/2381\/8366<\/dc:identifier><dc:identifier>\n10.1137\/090752365<\/dc:identifier><dc:language>\nen<\/dc:language><dc:rights>\nThis paper was published as SIAM Journal on Scientific Computing, 2010, 32 (4), pp. 1879-1897.  It is also available from http:\/\/scitation.aip.org\/getabs\/servlet\/GetabsServlet?prog=normal&id=SJOCE3000032000004001879000001&idtype=cvips&gifs=yes.  This paper appears here with the permission of SIAM.  Doi: 10.1137\/090752365<\/dc:rights><dc:publisher>\nSociety for Industrial and Applied Mathematics (SIAM)<\/dc:publisher>\n<\/oai_dc:dc>\n<\/metadata>\n        <\/record>","journals":[{"title":null,"identifiers":["issn:1064-8275","1064-8275"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2010,"topics":["quasi-Monte Carlo method","effective dimension","gamma process","moment estimation","tail probability estimation","Poisson process","shot noise"],"subject":["Article"],"fullText":" \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nCopyright \u00a9 by SIAM. Unauthorized reproduction of this article is prohibited. \nSIAM J. SCI. COMPUT. c\u00a9 2010 Society for Industrial and Applied Mathematics\nVol. 32, No. 4, pp. 1879\u20131897\nQUASI-MONTE CARLO METHOD FOR INFINITELY DIVISIBLE\nRANDOM VECTORS VIA SERIES REPRESENTATIONS\u2217\nJUNICHI IMAI\u2020 AND REIICHIRO KAWAI\u2021\nAbstract. An infinitely divisible random vector without Gaussian component admits repre-\nsentations of shot noise series. Due to possible slow convergence of the series, they have not been\ninvestigated as a device for Monte Carlo simulation. In this paper, we investigate the structure\nof shot noise series representations from a simulation point of view and discuss the effectiveness of\nquasi-Monte Carlo methods applied to series representations. The structure of series representations\nin nature tends to decrease their effective dimension and thus increase the efficiency of quasi-Monte\nCarlo methods, thanks to the greater uniformity of low-discrepancy sequence in the lower dimen-\nsion. We illustrate the effectiveness of our approach through numerical results of moment and tail\nprobability estimations for stable and gamma random variables.\nKey words. quasi-Monte Carlo method, effective dimension, gamma process, moment estima-\ntion, tail probability estimation, Poisson process, shot noise\nAMS subject classifications. 60E07, 65C05, 65D30, 65B10\nDOI. 10.1137\/090752365\n1. Introduction. An infinitely divisible random vector without Gaussian com-\nponent admits representations of shot noise series. Such series representations have\nplayed an important role in theories such as the tail probability of stable random vec-\ntors and have also been studied in the applied literature, known as \u201cshot noise.\u201d (See,\nfor example, Vervaat [28] and the references therein.) Series representations involving\nPoisson arrival times are given by Ferguson and Klass [6] for real independent incre-\nment processes without Gaussian component and with positive jumps. The theory\nof stable processes and their applications are expanded, due to LePage [16], on se-\nries representation of stable random vectors. The simulation of nonnegative infinitely\ndivisible random variables is considered, and their series representations as a special\nform of generalized shot noise is developed in Bondesson [3]. The same approach is\nused in Rosin\u00b4ski [21] as a general pattern for series representations of Banach space\nvalued infinitely divisible random vectors.\nInfinitely divisible random vectors consist of three independent components: a con-\nstant, a centered normal random vector, and a Poisson component. The generation\nof the Gaussian component is standard and will not be discussed in this paper. The\nPoisson component is governed in full by the so-called Le\u00b4vy measure. If the Le\u00b4vy\nmeasure is infinite, then the Poisson component consists of infinitely many jumps. It\nis obviously impossible to generate infinitely many jumps on a computer. Possible\nsimulation methods can be summarized as follows.\n(i) Direct generation: When an explicit density function, or an alternative exact\nsimulation method, is available, a simulation should be straightforward in principle.\n\u2217Received by the editors March 11, 2009; accepted for publication (in revised form) April 12,\n2010; published electronically June 24, 2010. This work was supported by the Japan Society for the\nPromotion of Science through Grants-in-Aid for Scientific Research (18710126 and 20740059).\nhttp:\/\/www.siam.org\/journals\/sisc\/32-4\/75236.html\n\u2020Faculty of Science and Technology, Keio University, Yokohama 223-8522, Japan (jimai@ae.keio.\nac.jp).\n\u2021Department of Mathematics, University of Leicester, Leicester LE1 7RH, UK (reiichiro.kawai@\ngmail.com).\n1879\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nCopyright \u00a9 by SIAM. Unauthorized reproduction of this article is prohibited. \n1880 JUNICHI IMAI AND REIICHIRO KAWAI\nStable, exponential, gamma, geometric, and negative binomials are in this category.\nTo simulate random variables from more complicated distributions, a numerical inver-\nsion method is proposed in Ho\u00a8rmann and Leydold [7] to numerically approximate the\ndensity functions with piecewise Hermite functions. Moreover, this method is applied\nin Imai and Tan [13, 14] to the simulation of variates from infinitely divisible ran-\ndom distributions of practical interest, such as generalized hyperbolic, normal inverse\nGaussian, and Meixner distributions.\n(ii) Generation with series representation: Series representation provides perfect\nand often easy simulation of infinitely divisible random vectors. A disadvantage of\nthis method is that some series may converge at an extremely slow rate. Then, one\nmight need a huge number of terms to reach a desired accuracy of the approximation.\nOn the other hand, with ever increasing computational speed, a slow convergence may\nno longer cause a serious practical issue. (See, for example, Kawai [15] and Madan\nand Yor [17] for simulation use of series representations.)\n(iii) Generation from compound Poisson: If a neighborhood of the origin of the\nLe\u00b4vy measure is discarded or replaced by its mean value, then the remainder cor-\nresponds to a compound Poisson random vector with a constant shift. It converges\nto its true random vector as the intensity of the discarded part of the Le\u00b4vy measure\ndecreases. The compound Poisson component can be simulated in a precise manner.\nThe aforementioned series representation provides a consistent way to construct such\napproximations. However, when the discarded part of the Le\u00b4vy measure is too in-\ntense, the corresponding component may produce a substantial error. In such cases,\none may often approximate the discarded component by a Gaussian random vector\nwith an appropriate small variance, as investigated in Asmussen and Rosin\u00b4ski [2].\nThis approximation complements a method through series representations when the\nseries converges slowly.\nThe main purpose of this paper is to discuss the effectiveness of the quasi-Monte\nCarlo (QMC) method applied to shot noise series representations. We aim at moment\nestimations as well as tail probability estimations, rather than a pointwise approxima-\ntion. It is well known that the QMC method could produce more accurate estimations\nthan the Monte Carlo (MC) method since a low-discrepancy sequence in the QMC\nmethod has an asymptotically superior convergence rate compared to random num-\nbers used in the MC method. The practical advantage of the QMC method, on the\nother hand, can be justified by a notion of effective dimension of the problem, first\nintroduced in Caflisch, Morokoff, and Owen [5]. It is demonstrated that the QMC\nmethod works better than the MC method for nominally high-dimensional problems\nas long as their effective dimensions are small. In this paper, we demonstrate that\nthe structure of series representations tends to decrease their effective dimension in\nnature and thus increase the efficiency of the QMC method. Since the QMC method\nmakes the best use of a greater uniformity of low-discrepancy sequence in the lower\ndimension, it is natural to expect the applicability of the QMC method to series rep-\nresentations of infinitely divisible random vectors. We illustrate the effectiveness of\nour approach through numerical results of moment and tail probability estimations\nfor stable and gamma random variables.\nThe rest of this paper is organized as follows. Section 2 briefly summarizes back-\nground material on series representations of infinitely divisible random vectors with\na view towards simulation. In section 3, we discuss the effectiveness of the QMC\nmethod applied to Poisson interarrival times in series representations and review the\nconcept of cumulative explanatory ratio. In section 4, we provide numerical results\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nCopyright \u00a9 by SIAM. Unauthorized reproduction of this article is prohibited. \nQUASI-MONTE CARLO FOR SERIES REPRESENTATIONS 1881\nto illustrate the effectiveness of the QMC method applied to series representations of\nstable and of gamma random variables. Finally, section 5 concludes.\n2. Series representation of infinitely divisible random vectors. Let us\nbegin this section with the notation which will be used throughout this paper. We\ndenote by N the collection of positive integers, by Rd the d-dimensional Euclidean\nspace with the norm \u2016 \u00b7 \u2016, Rd0 := Rd \\ {0}, R+ := (0,+\u221e), and by B(Rd0) the Borel\n\u03c3-field of Rd0. We denote by\nL\n= the identity in law. We say that an infinitely divisible\nrandom vectorX in Rd is generated by the triplet (\u03b3,A, \u03bd) if its characteristic function\nis given by\nE\n[\nei\u3008y,X\u3009\n]\n= exp\n[(\ni\u3008y, \u03b3\u3009 \u2212 1\n2\n\u3008y,Ay\u3009+\n\u222b\nRd0\n(ei\u3008y,z\u3009 \u2212 1\u2212 i\u3008y, z\u3009\u0000(0,1](\u2016z\u2016))\u03bd(dz)\n)]\n,\nwhere \u03b3 \u2208 Rd, A is a symmetric nonnegative-definite d \u00d7 d matrix, and \u03bd is a Le\u00b4vy\nmeasure on Rd0, that is, a \u03c3-finite measure satisfying\n\u222b\nRd0\n(\u2016z\u20162 \u2227 1)\u03bd(dz) < +\u221e.\nThroughout this paper, we consider only infinitely divisible laws without Gaussian\ncomponent, that is, A \u2261 0. Moreover, we denote by {\u0393k}k\u2208N the arrival times of a\nstandard Poisson process and by {Ek}k\u2208N a sequence of independent and identically\ndistributed (i.i.d.) exponential random variables with unit mean.\nLet us review generalities on the series representation of infinitely divisible random\nvectors with a view towards simulation. Our discussion is essentially parallel to the\nso-called inverse Le\u00b4vy measure method [6, 16]. Notice first that the random variable\u2211+\u221e\nk=1 \u0393k\u0000(\u0393k \u2208 [0, T ]) is infinitely divisible with Le\u00b4vy measure \u03bd(dz) = dz defined\non (0, T ]. Recall also that the epochs of an inhomogeneous Poisson process on [0, T ]\nwith intensity h(t) can be generated by H(\u03931), H(\u03932), . . ., where H(t) := inf{u \u2208\n[0, T ] :\n\u222b u\n0 h(s)ds < t}, provided that\n\u222b T\n0 h(s)ds < +\u221e. Therefore, by regarding the\nintensity h(t) as a Le\u00b4vy measure (\u201con state space\u201d rather than on time), we deduce\nthat\n\u2211+\u221e\nk=1H(\u0393k)\u0000(\u0393k \u2208 [0, T ]) is an infinitely divisible random variable with Le\u00b4vy\nmeasure \u03bd(dz) = h(z)dz defined on (0, T ]. Notice here that the definition of H(t)\nimplicitly assumes that the Le\u00b4vy measure \u03bd has a compact support. Moreover, the\ncondition\n\u222b T\n0 h(s)ds < +\u221e implies that the Le\u00b4vy measure is finite. We can extend\nthis formulation to an infinite Le\u00b4vy measure on R+, simply by redefining the inverse\nfunction H as running down from the infinity rather than up the other way, that is,\n(2.1) H(r) := inf\n{\nu > 0 :\n\u222b +\u221e\nu\nh(s)ds < r\n}\n,\nand compute\n\u2211+\u221e\nk=1 H(\u0393k), where {\u0393k}k\u2208N is no longer restricted to lie in [0, T ]. (See\nAsmussen and Glynn [1].) A series representation of stable random vectors can be\nderived in closed form through this inverse Le\u00b4vy measure method as follows.\nExample 2.1 (stable random vector). An infinitely divisible random vector X in\nR\nd, without Gaussian component, is called stable if its Le\u00b4vy measure is given by\n\u03bd\u03b1(B) =\n\u222b\nSd\u22121\n\u222b\nR+\n\u0000B(r\u03be)\ndr\nr1+\u03b1\n\u03c3(d\u03be), B \u2208 B(Rd0),\nwhere \u03b1 \u2208 (0, 2) is the stability index and where \u03c3 is a finite positive measure on\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nCopyright \u00a9 by SIAM. Unauthorized reproduction of this article is prohibited. \n1882 JUNICHI IMAI AND REIICHIRO KAWAI\nSd\u22121. Its characteristic function is given by\n(2.2)\nE\n[\nei\u3008y,X\u3009\n]\n= exp\n[\ni\u3008y, \u03b7\u3009+\n\u222b\nRd0\n(ei\u3008y,z\u3009 \u2212 1\u2212 i\u3008y, z\u3009\u0000(0,1](\u2016z\u2016))\u03bd\u03b1(dz)\n]\n=\n\u23a7\u23aa\u23aa\u23aa\u23a8\u23aa\u23aa\u23a9\nexp\n[\ni\u3008y, \u03c4\u03b1\u3009 \u2212 c\u03b1\n\u222b\nSd\u22121\n|\u3008y, \u03be\u3009|\u03b1\n(\n1\u2212 i tan \u03c0\u03b1\n2\nsgn\u3008y, \u03be\u3009\n)\n\u03c3(d\u03be)\n]\nif \u03b1 \t= 1,\nexp\n[\ni\u3008y, \u03c41\u3009 \u2212 \u03c02\n\u222b\nSd\u22121\n(\n|\u3008y, \u03be\u3009|+ i 2\n\u03c0\n\u3008y, \u03be\u3009 ln |\u3008y, \u03be\u3009|\n)\n\u03c3(d\u03be)\n]\nif \u03b1 = 1,\nwhere c\u03b1 = |\u0393(\u2212\u03b1) cos \u03c0\u03b12 | and where \u03c4\u03b1 = \u03b7\u2212 11\u2212\u03b1\n\u222b\nSd\u22121 \u03be\u03c3(d\u03be) when \u03b1 \t= 1 and \u03c41 =\n\u03b7\u2212(1\u2212\u03b3) \u222bSd\u22121 \u03be\u03c3(d\u03be), with \u03b3(= 0.5772 . . .) being the Euler constant. Let {Uk}k\u2208N be\na sequence of i.i.d. random vectors in Sd\u22121 with common distribution \u03c3(d\u03be)\/\u03c3(Sd\u22121),\nand set\n\u03b7 =\n\u23a7\u23a8\u23a9\n1\n1\u2212 \u03b1\n\u222b\nSd\u22121\n\u03be\u03c3(d\u03be) if \u03b1 \t= 1,\n0 if \u03b1 = 1,\nz0 =\n\u23a7\u23a8\u23a90 if \u03b1 \u2208 (0, 1),\u222b\nSd\u22121\n\u03be\u03c3(d\u03be)\/\u03c3(Sd\u22121) if \u03b1 \u2208 [1, 2)\nand\n\u03bb =\n\u23a7\u23aa\u23aa\u23a8\u23aa\u23aa\u23a9\n0 if \u03b1 \u2208 (0, 1),\n\u03c3\n(\nSd\u22121\n) (\n\u03b3 + ln\n(\n\u03c3\n(\nSd\u22121\n)))\nif \u03b1 = 1,(\n\u03b1\n\u03c3(Sd\u22121)\n)\u22121\/\u03b1\n\u03b6\n(\n1\n\u03b1\n)\nif \u03b1 \u2208 (1, 2),\nwhere \u03b6 denotes the Riemann zeta function. Then, it holds that\n(2.3)\n(\n\u03b1\n\u03c3(Sd\u22121)\n)\u22121\/\u03b1 +\u221e\u2211\nk=1\n(\n\u0393\n\u22121\/\u03b1\nk Uk \u2212 k\u22121\/\u03b1z0\n)\n+ \u03bbz0\nL\n= X.\nThe above centering constants are obtained in Proposition 5.5 of Rosin\u00b4ski [22].\nEvery infinitely divisible random vector admits a series representation through\nthe inverse Le\u00b4vy measure method. In most cases, however, the tail inverse of Le\u00b4vy\nmeasure, that is, the function H(r) defined by (2.1), is not available in closed form,\nwith exceptions such as the stable random vector of Example 2.1 and the layered\nstable random vector of Houdre\u00b4 and Kawai [9]. To obtain a closed form, some al-\nternative methods have been proposed, for example, the thinning method and the\nrejection method of [21]. Each of those methods can be considered as a special case\nof the so-called generalized shot noise method of [3, 21], which we describe as follows.\nAssume that Le\u00b4vy measure \u03bd can be decomposed as\n(2.4) \u03bd(B) =\n\u222b\nR+\nP (H(r, U) \u2208 B) dr, B \u2208 B(Rd0),\nwhere U is a random variable taking values in a suitable space U and where H :\nR+ \u00d7 U \n\u2192 Rd0 is such that, for each u \u2208 U , r \n\u2192 \u2016H(r, u)\u2016 is nonincreasing. Then,\nan infinitely divisible random vector X generated by the triplet (0, 0, \u03bd) is identical\nin law to the shot noise series representation\n(2.5) X\nL\n=\n+\u221e\u2211\nk=1\n[H(\u0393k, Uk)\u2212 ck] ,\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nCopyright \u00a9 by SIAM. Unauthorized reproduction of this article is prohibited. \nQUASI-MONTE CARLO FOR SERIES REPRESENTATIONS 1883\nwhere {Uk}k\u2208N is a sequence of i.i.d. copies of the random variable U , and {ck}k\u2208N\nis a sequence of constants defined by ck := E[H(\u0393k, U)\u0000(\u2016H(\u0393k, U)\u2016 \u2264 1)]. Here, the\nsequence {Uk}k\u2208N is independent of {\u0393k}k\u2208N. Let us present two concrete examples\nfor which the inverse Le\u00b4vy measure method provides no explicit representations and\nthe generalized shot noise series method based upon (2.4) is required.\nExample 2.2 (gamma random variable). An infinitely divisible random variable\nX is gamma if it is characterized by the characteristic function\nE\n[\neiyX\n]\n= exp\n[\u222b\nR+\n(\neiyz \u2212 1) ae\u2212bz\nz\ndz\n]\n,\nwhere a > 0 and b > 0. The inverse Le\u00b4vy measure representation provides a shot\nnoise series representation\n1\nb\n+\u221e\u2211\nk=1\nEi\u22121\n(\n\u0393k\na\n)\nL\n= X,\nwhere Ei(x) :=\n\u222b +\u221e\nx\nu\u22121e\u2212udu is the exponential integral function. However, this\nrepresentation is not directly useful since neither the function Ei(x) nor its inverse\ncan be given in closed form. On the other hand, an explicit representation is given\nin [3] by\n(2.6)\n+\u221e\u2211\nk=1\ne\u2212\u0393k\/a\nVk\nb\nL\n= X,\nwhere {Vk}k\u2208N is a sequence of i.i.d. exponential random variables with unit mean.\nThere are yet other explicit representations available through the thinning method\nand the rejection method of [21].\nThe availability of different representations implies nonuniqueness of the decom-\nposition (2.4). For example, the representation (2.6) is based on the decomposition\u222b\nR+\n\u0000B(z)a\ne\u2212bz\nz\ndz =\n\u222b\nR+\n(\u222b\nR+\n\u0000\n(\ne\u2212\nr\na\nv\nb\n\u2208 B\n)\ne\u2212vdv\n)\ndr,\nwhere\n\u222b\nR+\n\u0000(\u00b7 \u2208 B)e\u2212vdv acts as the standard exponential law. Observe that for each\nv \u2208 R+ the random sequence {e\u2212\u0393k\/av\/b}k\u2208N is almost surely decreasing in k, while\n{e\u2212\u0393k\/aVk\/b}k\u2208N is not because of the additional random sequence {Vk}k\u2208N.\nThe next example presents a series representation of more intricate structure.\nExample 2.3 (CGMY random variable). The CGMY random variable is charac-\nterized by a Le\u00b4vy measure in the form\n\u03bd(dz) = C|z|\u22121\u2212Y\n[\ne\u2212G|z|\u0000(z < 0) + e\u2212M|z|\u0000(z > 0)\n]\ndz,\nwhere C > 0, G > 0, M > 0, and Y < 2. (See Carr et al. [4] for more details.) If Y \u2208\n(0, 2), then it is in a subclass of the tempered stable random vector of Rosin\u00b4ski [22].\nAn explicit representation is given in [22] by\n+\u221e\u2211\nk=1\n[(\nY \u0393k\n2C\n)\u22121\/Y\n\u2227 VkJ1\/Yk |Zk|\n]\nZk\n|Zk| + \u03b3,\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nCopyright \u00a9 by SIAM. Unauthorized reproduction of this article is prohibited. \n1884 JUNICHI IMAI AND REIICHIRO KAWAI\nwhere {Vk}k\u2208N is a sequence of i.i.d. exponential random variables with unit mean,\n{Jk}k\u2208N is a sequence of i.i.d. uniform random variables on [0, 1], {Zk}k\u2208N is a se-\nquence of i.i.d. random variables taking values \u2212G\u22121 and M\u22121 with equal probabil-\nity, and \u03b3 is a suitable constant. Here, all the random sequences {\u0393k}k\u2208N, {Vk}k\u2208N,\n{Jk}k\u2208N, and {Zk}k\u2208N are mutually independent. This representation involves three\nadditional sequences of i.i.d. random variables. Thus, four integrals are involved in\nthe decomposition (2.4). (Three other representations are available in closed form\nand will be discussed in [11]. See also, for example, [15, 17], where the above series\nrepresentation is used for simulation.)\nAs can be seen in the above examples, series representations for an infinite Le\u00b4vy\nmeasure necessarily have a form of infinite sum as well. To deal with such infinite\nseries towards simulation, we need to truncate the infinite series up to a certain finite\npoint. A straightforward approach is the truncation to a finite number of terms of\nthe series\n(2.7) Xn :=\nn\u2211\nk=1\n[H(\u0393k, Uk)\u2212 ck] .\nAn alternative approach to the infinite series is the truncation to a finite time span\nof the Poisson process\n(2.8) X(n) :=\n\u2211\n{k\u2208N:\u0393k\u2264n}\n[H(\u0393k, Uk)\u2212 ck] .\nIn connection with the fact that the function H(r, v) is nonincreasing in r, when the\ntruncation level n is large enough, it is fairly reasonable to expect that the above two\ntruncations (2.7) and (2.8) behave in a similar manner.\nThe common and key building block for simulation of series representations is\nsampling of the epochs of a standard Poisson process. This can be generated itera-\ntively as a successive summation of i.i.d. exponential random variables:\n(2.9) {\u03931,\u03932,\u03933, . . .} L=\n{\n1\u2211\nk=1\nEk,\n2\u2211\nk=1\nEk,\n3\u2211\nk=1\nEk, . . .\n}\n.\nThe exponential random variables {Ek}k\u2208N act as interarrival times of a standard\nPoisson process and can easily be generated as Ek = \u2212 ln Jk, where {Jk}k\u2208N is a\nsequence of i.i.d. uniform random variables on [0, 1]. The truncation (2.8) provides an\ninteresting interpretation of discarded small jumps. Assume, for simplicity, that the\nfunction H has the form \u2016H(r, u)\u2016 = h(r), where h is a nonincreasing function from\nR+ to R+. Then, for each n \u2208 N such that h(n) < 1, the Ito\u02c6\u2013Wiener isometry yields\nthe L2(\u03a9)-estimate of the discarded small jump component\nE\n[\u2225\u2225X \u2212X(n)\u2225\u22252] = E\n\u23a1\u23a3(\u222b 1\n0\n\u222b\n\u2016z\u2016\u2264h(n)\nz(\u03bc\u2212 \u03bd)(dz, ds)\n)2\u23a4\u23a6 = \u222b\n\u2016z\u2016\u2264h(n)\n\u2016z\u20162\u03bd(dz),\nwhere \u03bc is a Poisson random measure on Rd0 \u00d7 [0, 1] whose compensator is the Le\u00b4vy\nmeasure \u03bd. This discarded small jump component may be replaced with a Gaussian\nrandom vector under some mild condition. (For details, see [2].)\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nCopyright \u00a9 by SIAM. Unauthorized reproduction of this article is prohibited. \nQUASI-MONTE CARLO FOR SERIES REPRESENTATIONS 1885\n3. Variance contribution of the lower dimension. As mentioned in the in-\ntroduction, the advantage of the QMC method is attributed to its potentially superior\nconvergence rate of O(N\u22121 lndN), while that of the MC method is O(N1\/2), where N\nand d denote, respectively, the number of iterations and the dimension. Nevertheless,\nseveral studies point out that this theoretical higher asymptotic convergence rate of\nthe QMC is not achievable in practice for high-dimensional problems. To resolve the\nabove puzzle and to explain a practical advantage of the QMC method, especially\nwhen we deal with shot noise series representations, let us review in this section the\nanalysis of variance (ANOVA) decomposition and the concept of effective dimension.\nWe refer the reader to Wang and Fang [27] for more details.\nLet the set A := {1, 2, . . . , d} denote the coordinate axes of the d-dimensional\nunit cube [0, 1)d. Then, for each subset S \u2286 A, we define by |S| its cardinality and by\nA\\S its complementary set. Each point in [0, 1)d is denoted by x := (x1, . . . , xd)\u2032, and\nxS denotes the |S|-vector of components xk for k \u2208 S. Let f be a function mapping\nfrom [0, 1)d to R, and consider a problem of computing the following integral:\nI (f) :=\n\u222b\n[0,1)d\nf(x)dx.\nWe can employ the MC and QMC methods for approximating the integral by taking\na sample mean, that is,\nQn(f) :=\n1\nn\nn\u2211\nk=1\nf\n(\nx(k)\n)\n,\nwhere the sequence {x(k)}k\u2208N corresponds to a sequence of i.i.d. replications in the MC\nmethod, while it is generated from a low-discrepancy sequence in the QMC method.\nIt is well known that the MC error is of order O(n1\/2) for square integrable functions\nand is independent of the dimension d. On the other hand, the Koksma\u2013Hlawka\ninequality provides the QMC error bound by\n|I (f)\u2212Qn (f)| \u2264 VHKD\u2217\n(\n{x(k)}k=1,...,n\n)\n,\nwhere D\u2217({x(k)}k=1,...,n) denotes the star discrepancy of the sequence {x(k)}k=1,...,n\nand VHK is the variation in the sense of Hardy and Krause.\nUnder the mild condition that f is a square integrable function, the ANOVA\ndecomposition expresses the integrand f as a sum of 2d additive functions:\nf(x) =\n\u2211\nS\u2286A\nfS(x),\nwhere the function fS , which depends only on the component of x in the dimension\nS, is defined recursively by\nfS(x) :=\n\u222b\n[0,1)A\\S\nf(y)dyA\\S \u2212\n\u2211\nS\u2032\u2282S\nfS\u2032(x),\nwith the usual convention f\u2205(x) :=\n\u222b\n[0,1)d\nf(y)dy = I(f). Note that the ANOVA\ndecomposition is orthogonal, namely,\n\u222b\nfS(x)fS\u2032 (x)dx = 0, for S \t= S \u2032. Let \u03c32(f) and\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nCopyright \u00a9 by SIAM. Unauthorized reproduction of this article is prohibited. \n1886 JUNICHI IMAI AND REIICHIRO KAWAI\n\u03c32S(f) denote, respectively, the variance of f and fS , that is, \u03c3\n2(f) :=\n\u222b\n[0,1)d\n(f(x) \u2212\nI(f))2dx, and \u03c32S(f) :=\n\u222b\n[0,1)S [fS(x)]\n2dx, whenever |S| > 0. By using the above\nvariance decomposition, we define the total variance for the subset S by\n(3.1) DS(f) :=\n\u2211\nS\u2032\u2286S\n\u03c32S\u2032(f).\nWe are now in a position to define the effective dimension in the truncation sense by\nthe smallest integer d\u2032 satisfying\n(3.2) D{1,2,...,d\u2032}(f) \u2265 p\u03c32(f),\nwhere p is a quantile of the variance. In other words, the first d\u2032 dimensions capture\nmore than p = 95%, say, of the total variations, even though the nominal dimension\nof f may be very large.\nThe efficiency of the QMC is intricately related to the effective dimension. Fix\nN \u2208 N, and consider a low-discrepancy point set P := {x(k)}k=1,...,N , where each\npoint x(k) \u2208 [0, 1)d is used for approximation of the integral I(f). Then, the QMC\nerror satisfies the bound\n|I(f)\u2212QN (f)| \u2264\n\u2211\n|S|>0\nDS,N (PS) \u2016fS\u2016 ,\nwhere PS indicates the projection of the point set P on [0, 1)|S|, where DS,N (PS) is\nthe discrepancy of PS consisting of N points, and where \u2016f\u2016 denotes the variation of\nthe function f . The above inequality indicates that the QMC error bound associates\nwith the uniformity of all the projections PS and all the low-dimensional structures of\nfS in an explicit manner. Although the success of the QMC method relies on a greater\nuniformity of low-discrepancy sequences than random sequences, the uniformity is not\nalways preserved for every component and projections with a finite number of points.\nIn fact, it is well known that the uniformity of low-discrepancy sequences decreases\nas the dimension increases. Nevertheless, as claimed in Wang and Fang [27], the\nQMC method can still be more effective than the MC method, in particular, for\nfunctions with a low effective dimension in the truncation sense. This observation can\nbe justified by decomposing the bound as\n|I(f)\u2212QN(f)| \u2264\n\u2211\nS\u2286{1,...,d\u2032}\nDS,N (PS) \u2016fS\u2016+\n\u2211\n{S:S\u2229(A\\{1,...,d\u2032}) \u000f=0}\nDS,N (PS) \u2016fS\u2016 ,\nprovided that the effective dimension of the function f is given by d\u2032. This inequality\nindicates that when d\u2032 is small the discrepancy of all the low-dimensional projections of\nlow-discrepancy point sets PS are much smaller relative to those of the random point\nsets. This means that the first summation is much smaller for the QMC method\nthan for the MC method. As the dimension increases further, the uniformity of low-\ndiscrepancy point sets may deteriorate, resulting in a higher value of DS,N (PS). Yet\nthe second sum may not be significant in the sense that the quantity \u2016fS\u2016 is often\nsmall. Consequently, the QMC method can be more effective than the MC method\nwhen the function f has a low effective dimension.\nLet us next illustrate why the structure of series representations suits this effective\ndimension argument. There are two primal aspects: the domination of {\u0393k}k\u2208N by a\nlower dimension of the interarrival times {Ek}k\u2208N, and the tails of Le\u00b4vy measure.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nCopyright \u00a9 by SIAM. Unauthorized reproduction of this article is prohibited. \nQUASI-MONTE CARLO FOR SERIES REPRESENTATIONS 1887\nFor simplicity, we focus on the function H(r) defined by (2.1). In view of (2.9), the\nfirst few terms of {H(\u0393k)}k\u2208N are expressed solely by the first few terms of {Ek}k\u2208N as\nwell. For a simple illustration, fix \u03bb > 1 and consider the infinite sum of a decreasing\nsequence,\n(3.3)\n+\u221e\u2211\nk=1\n\u03bb\u2212k+1\u0393k\nL\n=\n+\u221e\u2211\nk=1\n\u03bb\u2212k+1\nk\u2211\nl=1\nEl =\n+\u221e\u2211\nl=1\nEl\n+\u221e\u2211\nk=l\n\u03bb\u2212k+1 =\n+\u221e\u2211\nl=1\n\u03bb\u2212l+2\n\u03bb\u2212 1 El,\nwhere the interchange of two summations can be justified by the Fubini theorem due\nto the almost sure positivity of the summands, including the case where both sides\nare infinite. To see the impact of the first exponential E1, we compare (3.3) to its\ncounterpart, with all the rest of {Ek}k\u22652 being set to be degenerate, in terms of the\nfirst and the second moments, that is,\n(3.4)\nE\n[\u22111\nk=1\n\u03bb\u2212k+2\n\u03bb\u22121 Ek\n]\nE\n[\u2211+\u221e\nk=1\n\u03bb\u2212k+2\n\u03bb\u22121 Ek\n] = 1\u2212 1\n\u03bb\nand\nVar\n(\u22111\nk=1\n\u03bb\u2212k+2\n\u03bb\u22121 Ek\n)\nVar\n(\u2211+\u221e\nk=1\n\u03bb\u2212k+2\n\u03bb\u22121 Ek\n) = 1\u2212 1\n\u03bb2\n.\nHere, a greater \u03bb assigns a more significant role to the lower dimension of the inter-\narrival times {Ek}k\u2208N. Roughly speaking, this tends to happen with a faster decay\nof the function H(r) in r, that is, less activity near the origin of Le\u00b4vy measure.\nNext, concerning the Le\u00b4vy measure tails, recall that most of the Le\u00b4vy measure\ntails tends to be expressed by the first few terms of the sequence {H(\u0393k)}k\u2208N. The\nLe\u00b4vy measure tails are closely related to moments of its associated infinitely divisible\nrandom vector in such a way that for a submultiplicative, locally bounded, measur-\nable function f : Rd \n\u2192 [0,+\u221e) the expectation E[f(X)] is well defined if and only if\u222b\n\u2016z\u2016>1 f(z)\u03bd(dz) is finite. (For example, the functions \u2016x\u2016k \u2228 1 and e\u3008c,x\u3009 are submul-\ntiplicative. See Theorem 25.3 and its surrounding arguments of Sato [24] for details.)\nIn this respect, the simulation of the largest jumps through a systematic generation\nof the lower dimension of the interarrival times {Ek}k\u2208N is expected to contribute to\nimprovements in both precision and convergence when computing expectations.\nThe effectiveness of the QMC method may be increased by combining this struc-\nture with the superior uniformity of a low-discrepancy sequence in earlier dimensions.\nAccordingly, it is expected that the QMC method may work better than the MC\nmethod. The efficiency of the QMC method is often measured through the ratio\nDS(f)\n\u03c32(f)\n,\nwhich we refer to as the cumulative explanatory ratio (CER), where DS(f) is defined\nby (3.1). In other words, this represents the proportion of the variance captured by\nthe first d dimensions. In view of the inequality (3.2), it is possible that the effective\ndimension is given by d when the CER becomes larger than p. In general, it is difficult\nto provide an explicit expression for DS(f). However, as shown in Sobol\u2019 [26] and [27],\nthis can be computed via the formula\nDS(f) =\n\u222b\n[0,1)2d\u2212|S|\nf(x)f\n(\nxS , yA\\S\n)\ndxdy \u2212 [I(f)]2 ,\nwhere x := (xS , xA\\S) and y := (yS , yA\\S). This allows us to estimate D{1,2,...,d} by\nthe MC method. Let us now translate this into our framework. For simplicity, we\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nCopyright \u00a9 by SIAM. Unauthorized reproduction of this article is prohibited. \n1888 JUNICHI IMAI AND REIICHIRO KAWAI\nrestrict ourselves to an infinitely divisible random variable; that is, the function f here\nmaps R to R. Denote by {E\u2032k}k\u2208N an i.i.d. copy of the interarrival times {Ek}k\u2208N,\nand write U := {Uk}k\u2208N and, for each n \u2208 N,\nEn := {E1, . . . , En, E\u2032n+1, E\u2032n+2, . . .},\nwhere E0 := {Ek}k\u2208N. Then, for natural numbers n and N such that n \u2264 N , define,\nin view of (2.9),\nXf (En,U) := f\n\u239b\u239d N\u2211\nk=1\n\u23a1\u23a3H\n\u239b\u239dk\u2227n\u2211\nl=1\nEl +\nk\u2211\nl=(k\u2227n+1)\nE\u2032l , Uk\n\u239e\u23a0\u2212 ck\n\u23a4\u23a6\u239e\u23a0\nL\n= f\n(\nN\u2211\nk=1\n[H(\u0393k, Uk)\u2212 ck]\n)\n.\nThen, the CER with respect to the first n interarrival times is defined by\n(3.5) (CER)n :=\nCov(Xf (En,U), Xf (E0,U))\nVar(Xf (E0,U))\n.\nWe note that the sequence {Uk}k\u2208N appears in common on both sides of the covari-\nance operator. As a toy example, the CER of the random variable (3.3) is given by\n(CER)n = 1\u2212 \u03bb\u22122n, since\nCov\n(\nn\u2211\nk=1\n\u03bb\u2212k+2\n\u03bb\u2212 1 Ek +\n+\u221e\u2211\nk=n+1\n\u03bb\u2212k+2\n\u03bb\u2212 1 E\n\u2032\nk,\n+\u221e\u2211\nk=1\n\u03bb\u2212k+2\n\u03bb\u2212 1 Ek\n)\n= Var\n(\nn\u2211\nk=1\n\u03bb\u2212k+2\n\u03bb\u2212 1 Ek\n)\n.\n4. Numerical illustrations. In this section, we provide numerical results of\nestimations of moments and tail probabilities for the stable and gamma random vari-\nables presented in Examples 2.1 and 2.2. Let us begin by formulating five statistics\nwe will report in the following numerical examples for discussion. Ideally, we are able\nto estimate \u03bc := E[f(X)], where X =\n\u2211+\u221e\nk=1[H(\u0393k, Uk) \u2212 ck] and f is a function of\ninterest; for example, f(x) = x2 for estimation of the second moment. In reality, as\ndiscussed, we need to truncate infinite shot noise series up to a finite number of terms;\nthat is, we instead estimate \u03bcN := E[f(XN )], where XN =\n\u2211N\nk=1[H(\u0393k, Uk) \u2212 ck].\nClearly, limN\u2191+\u221e \u03bcN = \u03bc. Now, let {X(k1,k2)N }(k1,k2)\u2208N2 be an array of i.i.d. random\nvariables such that X\n(1,1)\nN\nL\n= XN . For k2 = 1, . . . , N2, we compute the sample mean\n\u03bc\n(k2)\nN := N\n\u22121\n1\n\u2211N1\nk1=1\nf(X\n(k1,k2)\nN ). Then, we compute average, standard error, root\nmean squared error (RMSE), and relative error defined, respectively, by\n(average) =\n1\nN2\nN2\u2211\nk2=1\n\u03bc\n(k2)\nN =: \u03bc\u02dcN ,\n(standard error) =\n(\n1\nN2(N2 \u2212 1)\nN2\u2211\nk2=1\n(\n\u03bc\n(k2)\nN \u2212 \u03bc\u02dcN\n)2)1\/2\n,(4.1)\n(RMSE) :=\n(\n1\nN2\nN2\u2211\nk2=1\n(\n\u03bc\n(k2)\nN \u2212 \u03bc\n)2)1\/2\n,\n(relative error) :=\n\u2223\u2223\u2223\u2223 \u03bc\u02dcN \u2212 \u03bc\u03bc\n\u2223\u2223\u2223\u2223 .\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nCopyright \u00a9 by SIAM. Unauthorized reproduction of this article is prohibited. \nQUASI-MONTE CARLO FOR SERIES REPRESENTATIONS 1889\nNote that we consider only examples for which the limiting value \u03bc is available. Note\nalso that the RMSE and the relative error are affected by both the estimator accuracy\nand the finite truncation of infinite shot noise series. The fifth statistic is the effective\ndimension by estimating the CER (3.5).\nIn our numerical experiments, we fix N1 = 4096 and N2 = 30 and use a scram-\nbled version of Sobol\u2019 sequences [25], proposed in Owen [20]. The reason for N1 =\n4096 = 212 is because the Sobol\u2019 sequence, which is a (t,m, s)-net in base 2, attains\na better uniformity when sample size is in the power of 2 (see Niederreiter [18]).\nFurthermore, we employ the Latin supercube sampling method of Owen [19] to avoid\ngenerating more than 50-dimensional Sobol\u2019 sequences. For example, 500-dimensional\nlow-discrepancy sequences are constructed as 10 sets of 50-dimensional (randomized)\nlow-discrepancy sequences. This randomization scheme makes full sense of the stan-\ndard error (4.1) and has often been employed, for example, in [12, 13] for error analysis\nunder the uniformity of low-discrepancy sequence in the randomized QMC (RQMC)\nframework.\nFor comparison, we employ the same procedure for estimating those reference\nnumbers with the MC method, except that low-discrepancy sequences are replaced\nby pseudorandom sequences. All the experiments are implemented on the computer\nplatform Intel Xeon(R) CPU 3.00GHz with 3.25GB memory. It takes around 437\nseconds to generate and store 4096 \u00d7 1000 random variables in the MC simulation.\nMeanwhile, in the RQMC method, it takes around 3593 seconds to implement 4096\nsets of 1000-dimensional Latin supercube sampling sequences. Therefore, relative to\nthe plain MC experiments, implementation of the RQMCmethod increases computing\ntime by roughly a factor of 8.\n4.1. One-sided stable random variable. Let X be a stable random variable\nin R with series representation\n(4.2) X :=\n+\u221e\u2211\nk=1\n(\n\u03b1\u0393k\nb\n)\u22121\/\u03b1\n,\nwhere \u03b1 \u2208 (0, 1). This corresponds to d = 1 and \u03c3(d\u03be) = b\u03b4{1}(d\u03be) in the formulation\n(2.3). In our numerical experiments, we use the truncation (2.7) up to a finite number\nof terms\nXN :=\nN\u2211\nk=1\n(\n\u03b1\u0393k\nb\n)\u22121\/\u03b1\n,\nwhere the truncation level N is supposed to be set sufficiently large so that XN is\nclose enough to the infinite sum X . As a reference of the closeness, consider a different\ntruncation X(N) up to a finite time span of a standard Poisson process in the spirit\nof (2.8). As discussed earlier, the L2(\u03a9)-distance is given by\n(4.3) E\n[\u2223\u2223X \u2212X(N)\u2223\u22232] = b\n2\u2212 \u03b1\n(\n\u03b1N\nb\n)1\u22122\/\u03b1\n+\n(\nb\n1\u2212 \u03b1\n)2(\n\u03b1N\nb\n)2\u22122\/\u03b1\n,\nwhose decay is of O(N2\u22122\/\u03b1) as N \u2191 +\u221e. This implies that the truncation X(N)\nconverges in L2(\u03a9) to the infinite sum X at a slower rate for a greater \u03b1.\nFirst, we examine the effectiveness of the RQMC method applied to Poisson\ninterarrival times in moment estimation, where moments are given in closed form\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nCopyright \u00a9 by SIAM. Unauthorized reproduction of this article is prohibited. \n1890 JUNICHI IMAI AND REIICHIRO KAWAI\n(Example 25.10 and Example 24.12 of Sato [24]); for \u03b7 \u2208 (\u2212\u221e, \u03b1),\n(4.4) E [X\u03b7] =\n(\nb\u0393(1\u2212 \u03b1)\n\u03b1\n)\u03b7\/\u03b1\n\u0393(1\u2212 \u03b7\/\u03b1)\n\u0393(1\u2212 \u03b7) .\nAs can be seen in (4.2) and (4.4), the parameter b serves only as a constant scale.\nHence, without loss of generality, we fix b = 1. Our simulation results are based upon\nparameter sets \u03b1 = {0.1, 0.4, 0.7} and \u03b7 = {\u22120.2,+\u03b1\/4}. Note that X\u03b7 is square\nintegrable under those parameter sets and thus the variance Var(X\u03b7) = E[X2\u03b7] \u2212\n(E[X\u03b7])2 is well defined.\nTable 1\nMoment estimation for one-sided stable random variables.\nAverage Standard error RMSE Relative error\n\u03b1 E[X\u22120.2] N MC RQMC MC RQMC MC RQMC MC RQMC\n10 0.018974 0.019078 1.21E-04 7.35E-06 5.62E-05 3.40E-06 0.53% 0.02%\n0.1 0.019075 100 0.019037 0.019077 1.26E-04 7.34E-06 5.80E-05 3.39E-06 0.20% 0.01%\n1000 0.019003 0.019077 1.13E-04 7.34E-06 5.23E-05 3.39E-06 0.38% 0.01%\n10 0.503474 0.503948 6.34E-04 3.33E-06 4.02E-04 3.18E-04 0.65% 0.74%\n0.4 0.500238 100 0.501221 0.500367 6.93E-04 2.72E-06 3.30E-04 1.11E-05 0.20% 0.03%\n1000 0.499978 0.500237 6.54E-04 3.24E-06 3.02E-04 1.50E-06 0.05% 0.00%\n10 0.690219 0.690498 4.66E-04 2.27E-06 3.70E-03 3.71E-03 6.66% 6.71%\n0.7 0.647097 100 0.661708 0.660954 4.26E-04 2.84E-06 1.27E-03 1.19E-03 2.26% 2.14%\n1000 0.651962 0.651980 3.98E-04 2.42E-06 4.55E-04 4.18E-04 0.75% 0.75%\nAverage Standard error RMSE Relative error\n\u03b1 E[X\u03b1\/4] N MC RQMC MC RQMC MC RQMC MC RQMC\n10 2.183678 2.182594 2.36E-03 2.31E-04 1.09E-03 1.07E-04 0.04% 0.01%\n0.1 2.182717 100 2.178075 2.182595 2.48E-03 2.31E-04 1.21E-03 1.07E-04 0.21% 0.01%\n1000 2.181600 2.182595 2.26E-03 2.31E-04 1.04E-03 1.07E-04 0.05% 0.01%\n10 1.590911 1.590189 1.65E-03 1.64E-04 7.80E-04 2.41E-04 0.12% 0.17%\n0.4 1.592868 100 1.589506 1.592699 1.67E-03 1.64E-04 8.24E-04 7.70E-05 0.21% 0.01%\n1000 1.592077 1.592785 1.54E-03 1.64E-04 7.15E-04 7.60E-05 0.05% 0.01%\n10 1.487895 1.487339 1.38E-03 1.44E-04 5.29E-03 5.30E-03 3.96% 3.99%\n0.7 1.549229 100 1.525699 1.528252 1.30E-03 1.44E-04 2.10E-03 1.80E-03 1.52% 1.35%\n1000 1.540968 1.541631 1.19E-03 1.44E-04 8.96E-04 6.54E-04 0.53% 0.49%\nNumerical results of moment estimations are presented in Table 1. To illustrate\nthe differences between average and analytical value, both RMSEs and relative errors\nare also reported. Each relative error is given by a percentile of the difference between\nthe estimated average and the analytical value. The speed of convergence depends\non the parameter value \u03b1. The relative errors indicate that each average value tends\nto be very close to its limiting value even when N is very small for smaller \u03b1, while\nit is slower for larger \u03b1. This observation is consistent with the speed of the L2(\u03a9)-\nconvergence presented in (4.3). Most importantly, we can assert that the RQMC\nmethod produces a smaller standard error than the MC method by a factor in the\nrange of 10\u2013200. Recall that, in plain MC simulation, 100-times more replications\nare required, in principle, to obtain a 10-times more accurate estimate. In comparing\nthe RMSE values, it can also be observed that each moment estimate via the RQMC\nmethod tends to converge to its true limiting value E[X\u03b7] much faster than the MC\nmethod. The difference in relative error is mainly attributed to an inefficiency of the\nMC method; that is, the sample size 4096 \u00d7 30 is not sufficient for estimating the\naverage. In fact, this also asserts the superiority of the RQMC method.\nNote that the standard errors of the RQMC method are almost invariant under\nan increase of the numerical dimension N . We conjecture that Var(X10), Var(X100),\nand Var(X1000) are close to each other. If so, the effective dimension remains small\neven in the high nominal-dimensional problem such as N = 1000.\nLet us next examine the cumulative explanatory ratio (CER)n to analyze the\nimpact of the RQMC method in terms of effective dimension. Each CER is estimated\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nCopyright \u00a9 by SIAM. Unauthorized reproduction of this article is prohibited. \nQUASI-MONTE CARLO FOR SERIES REPRESENTATIONS 1891\nthrough (3.5) by a plain MC method with 1000000 replications. By computing this\nquantity, we examine the variance contribution of the first n dimensions of {Ek}k\u2208N.\nWe report numerical results in Table 2, where all the numbers are rounded off to the\nnearest integer. (Hence, \u201c100%\u201d may be any number greater than 99.49%, not neces-\nsarily exactly 100%.) Here, we present only results of the case where the truncation\nlevel is set at N = 1000 to avoid overloading this section with nonessential details.\n(Let us note that we have obtained very similar results even for smaller N \u2019s.) Let us\nemphasize that the CERs achieve almost 100% at the first dimension. In other words,\nthe effective dimension with p = 0.99 is equal to one without additional variance\nreduction techniques, even when dealing with the high truncation level N = 1000.\nThis indicates that the RQMC method remains very effective even with a very high\ntruncation level at which the RQMC method often ruins its effectiveness against a\nplain MC method. Those CER results are also consistent with the superiority of the\nRQMC method in terms of standard error and support our claim that the RQMC\nmethod has an advantage over the MC method for shot noise series representations\nof infinitely divisible laws.\nTable 2\n(CER)n of moment estimation for one-sided stable random variables.\n(CER)n\n\u03b7 \u03b1 1 2 3 4 5\n0.1 100% 100% 100% 100% 100%\n\u22120.2 0.4 98% 100% 100% 100% 100%\n0.7 96% 99% 99% 100% 100%\n0.1 100% 100% 100% 100% 100%\n+\u03b1\/4 0.4 100% 100% 100% 100% 100%\n0.7 99% 100% 100% 100% 100%\nNext, we examine the effectiveness of the RQMC method in the probability tail\nasymptotics (Property 1.2.15 of Samorodnitsky and Taqqu [23]):\n(4.5) lim\n\u03bb\u2191+\u221e\n\u03bb\u03b1P(X > \u03bb) =\nb\n\u03b1\n.\nWe again fix b = 1, and the parameter sets tested are (\u03b1, \u03bb) = (0.1, 1035), (0.4, 1010),\nand (0.7, 105). We first present results of the CER in Table 3. Let us emphasize that\nthey are all exactly 100% without rounding operations, unlike the results presented\nin Table 2. This is not surprising at all, since, as discussed in [23], the first term of\nthe series (4.2) dominates the tail probability behavior in such a way that\n\u03bb\u03b1P\n((\n\u03b1\u03931\nb\n)\u22121\/\u03b1\n> \u03bb\n)\n= \u03bb\u03b1P\n(\nV1 <\nb\n\u03b1\n\u03bb\u2212\u03b1\n)\n= \u03bb\u03b1\n(\n1\u2212 e\u2212 b\u03b1\u03bb\u2212\u03b1\n)\n\u2192 b\n\u03b1\nas \u03bb \u2191 +\u221e. In addition, recall that the first exponential E1 also appears in all the\nremaining terms of the series, not only in the first term. This fact provides a stronger\nreason for the result (CER)1 = 100%.\nNumerical results of tail probability estimation are presented in Table 4. Note that\nb\/\u03b1 is the limiting value in terms of both the truncation level N and the threshold \u03bb.\nOn the one hand, we can observe through the standard errors and the RMSEs that the\nRQMC method yields estimates with very small variance, regardless of the truncation\nlevel N . Clearly, this fact is attributed to the great uniformity of 4096 samples\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nCopyright \u00a9 by SIAM. Unauthorized reproduction of this article is prohibited. \n1892 JUNICHI IMAI AND REIICHIRO KAWAI\nTable 3\n(CER)n of probability tail estimation for one-sided stable random variables.\n(CER)n\n(\u03b1, \u03bb) 1 2 3 4 5\n(0.1, 1035) 100% 100% 100% 100% 100%\n(0.4, 1010) 100% 100% 100% 100% 100%\n(0.7, 105) 100% 100% 100% 100% 100%\nTable 4\nTail probability estimation for one-sided stable random variables.\n(\u03b1, \u03bb) = (0.1, 1035) Average Standard error RMSE Relative error\nb\/\u03b1 N MC RQMC MC RQMC MC RQMC MC RQMC\n1 10.47402 10.01079 4.54E-01 2.57E-02 2.13E-01 1.19E-02 4.74% 0.11%\n10 10 10.01079 10.01079 5.29E-01 2.57E-02 2.44E-01 1.19E-02 0.11% 0.11%\n100 10.52548 10.01079 4.16E-01 2.57E-02 1.97E-01 1.19E-02 5.25% 0.11%\n(\u03b1, \u03bb) = (0.4, 1010) Average Standard error RMSE Relative error\nb\/\u03b1 N MC RQMC MC RQMC MC RQMC MC RQMC\n1 2.929687 2.522786 5.16E-01 8.14E-02 2.40E-01 3.76E-02 17.19% 0.91%\n2.5 10 2.278646 2.522786 5.85E-01 8.14E-02 2.70E-01 3.76E-02 8.85% 0.91%\n100 2.522786 2.522786 3.79E-01 8.14E-02 1.75E-01 5.01E-03 0.91% 0.91%\n(\u03b1, \u03bb) = (0.7, 105) Average Standard error RMSE Relative error\nb\/\u03b1 N MC RQMC MC RQMC MC RQMC MC RQMC\n1 1.749958 1.415407 2.03E-01 5.34E-02 9.73E-02 2.46E-02 22.50% 0.92%\n1.428571 10 1.492612 1.415407 2.48E-01 5.34E-02 1.14E-01 2.46E-02 4.48% 0.92%\n100 1.569816 1.415407 1.55E-01 5.34E-02 7.23E-02 2.46E-02 9.89% 0.92%\nin the first dimension, although they are scrambled. It seems that the uniformity\ncontributes to stabilizing the number of samples falling beyond the threshold \u03bb. In\nthe MC experiments, on the other hand, different (independent) 4096 replications are\nsampled for different simulation rounds. Hence, as seen in Table 4, estimates tend to\nfluctuate more through the MC method.\n4.2. Gamma random variable. Let {Vk}k\u2208N be a sequence of i.i.d. exponential\nrandom variables with unit mean, independent of {\u0393k}k\u2208N. We have seen in Example\n2.2 that for a > 0 and b > 0 the random variable\n(4.6) X :=\n+\u221e\u2211\nk=1\ne\u2212\u0393k\/a\nVk\nb\nhas a gamma distribution. It is straightforward that, for \u03b7 \u2208 (\u2212a,+\u221e),\nE [X\u03b7] = b\u2212\u03b7\n\u0393(a+ \u03b7)\n\u0393(a)\nand that, for \u03bb > 0,\nP(X > \u03bb) =\n1\n\u0393(a)\n\u222b +\u221e\nb\u03bb\nza\u22121e\u2212zdz,\nwhich can be computed by using an incomplete gamma function in standard math\npackages. As is the case with the one-sided stable random variable, the parameter b\nserves only as a constant scale. Hence, without essential loss of generality, we fix\nb = 1.\n4.2.1. RQMCmethod only for Poisson interarrival times. First, we apply\nthe RQMC method to the Poisson interarrival times {Ek}k\u2208N, while the independent\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nCopyright \u00a9 by SIAM. Unauthorized reproduction of this article is prohibited. \nQUASI-MONTE CARLO FOR SERIES REPRESENTATIONS 1893\nexponential random variables {Vk}k\u2208N are still governed by the plain MC method.\nNumerical results of moment estimation are presented in Table 5. The truncation\nlevel is set at N = 1000 throughout the experiments. In short, those results indicate\nthat the RQMC method improves the plain MC method in standard error only by a\nfactor in the range of 2\u20134. (We have confirmed that even much smaller truncation\nlevels, say N = 50, provide very similar results.) The improvements here are not as\nappealing as in the case of one-sided stable random variables examined in section 4.1.\nTable 5\nMoment estimation for gamma random variables; RQMC method for {Ek}k\u2208N and MC method\nfor {Vk}k\u2208N.\nAverage Standard error RMSE Relative error\na E[X\u22120.2] MC RQMC MC RQMC MC RQMC MC RQMC\n0.5 1.687812 1.683850 1.688163 4.16E-03 1.63E-03 1.95E-03 7.53E-04 0.23% 0.02%\n1.0 1.164230 1.163710 1.164274 1.03E-03 4.04E-04 4.75E-04 1.86E-04 0.04% 0.00%\n1.5 1.012687 1.012363 1.013300 6.01E-04 3.24E-04 2.78E-04 1.58E-04 0.03% 0.06%\nAverage Standard error RMSE Relative error\na E[X1] MC RQMC MC RQMC MC RQMC MC RQMC\n0.5 0.5 0.497839 0.499543 2.61E-03 1.48E-03 1.22E-03 6.84E-04 0.43% 0.09%\n1.0 1.0 0.998212 0.998301 3.44E-03 1.83E-03 1.59E-03 8.54E-04 0.18% 0.17%\n1.5 1.5 1.498708 1.496747 3.98E-03 2.55E-03 1.84E-03 1.21E-03 0.09% 0.22%\nThe effectiveness of the RQMC method is somewhat \u201cwatered down\u201d by the\npurely random element {Vk}k\u2208N. To confirm this conjecture, consider the random\nvariable\n(4.7) Y :=\n+\u221e\u2211\nk=1\ne\u2212\u0393k\/a\n1\nb\n;\nthat is, all the {Vk}k\u2208N in (4.6) are frozen at 1. We can prove that the random variable\nY of (4.7) is an infinitely divisible random variable with Le\u00b4vy measure \u03bd(dz) = a\/zdz\ndefined only on (0, 1\/b) and E[Y ] = a\/b. As a matter of course, this random variable is\nnot directly comparable with gamma random variables in terms of estimator efficiency\nsince they have different distributions. For example, their variances are not identical,\nwhile their means coincide. Nevertheless, the series representation (4.7) provides a\nuseful intuition as to how much the exponential random variables {Vk}k\u2208N in (4.6)\nhave ruined the effectiveness of the RQMC method applied to {Ek}k\u2208N. To avoid\noverloading this section with nonessential details, we give in Table 6 only numerical\nresults of moment estimation of E[Y ] = a\/b. For a better comparison with the results\npresented in Table 5, we also fix b = 1 and the truncation level N = 1000. Those\nresults indicate a sufficient efficiency of the RQMC method against the MC method.\nBased upon the standard errors, the RQMC method provides more accurate values\nby a factor in the range of 37\u201367, and similar results are observed in the RMSE\ncomparison.\nTable 6\nMoment estimation for random variable (4.7); RQMC method for {Ek}k\u2208N.\nAverage Standard error RMSE Relative error\na E[Y ] MC RQMC MC RQMC MC RQMC MC RQMC\n0.5 0.5 0.499489 0.500043 1.22E-03 1.83E-05 5.65E-04 9.19E-06 0.10% 0.01%\n1.0 1.0 0.999498 1.000058 1.78E-03 4.51E-05 8.19E-04 2.14E-05 0.05% 0.01%\n1.5 1.5 1.499562 1.500042 2.24E-03 5.95E-05 1.03E-03 2.77E-05 0.03% 0.00%\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nCopyright \u00a9 by SIAM. Unauthorized reproduction of this article is prohibited. \n1894 JUNICHI IMAI AND REIICHIRO KAWAI\n4.2.2. Improvement through application of RQMC method to every\nrandom sequence. In generating gamma variates in section 4.2.1, we generated\n{Ek}k\u2208N using a low-discrepancy sequence, while a random sequence is used to gen-\nerate {Vk}k\u2208N. The previous example of (4.7) indicates that the additional i.i.d.\nexponential variates {Vk}k\u2208N in (4.6) have blurred the effectiveness of the RQMC\nmethod applied to {Ek}k\u2208N. In this respect, we might be able to enhance the accu-\nracy of simulation by appropriately assigning earlier dimensions of low-discrepancy\nsequences to {Ek}k\u2208N and {Vk}k\u2208N so that the effective dimension is decreased. From\nthe ANOVA decomposition, it is evident that the performance of the RQMC method\nis affected by how to assign each dimension of low-discrepancy sequence. To see this,\nlet us compare the following four schemes to examine estimator efficiency of the MC\nand RQMC methods.\nScheme I. Plain MC method: Both sequences {Ek}k\u2208N and {Vk}k\u2208N are generated\nby i.i.d. random variables.\nScheme II. Hybrid: We use the RQMC method only for {Ek}k\u2208N, while we use\nthe MC method for {Vk}k\u2208N.\nScheme III. RQMC(k, k + N): Prepare a 2N -dimensional (randomized) low-\ndiscrepancy sequence. The first N -dimensional points are assigned to {Ek}k\u2208N, while\nthe second N -dimensional points are assigned to {Vk}k\u2208N. More precisely, we set\n(Ek, Vk) \u2190 (LDk, LDk+N ) for k = 1, . . . , N , where LDk indicates the kth component\nof low-discrepancy sequence.\nScheme IV. RQMC(2k \u2212 1, 2k): Every odd component of the 2N -dimensional\nsequence is used for {Ek}k\u2208N, while every even component is used for {Vk}k\u2208N. More\nprecisely, we set (Ek, Vk) \u2190 (LD2k\u22121, LD2k) for k = 1, . . . , N .\nNumerical results of moment estimation for gamma variates are reported in\nTable 7. As in the previous examples, the truncation level is fixed at N = 1000.\nAll the numerical results support the effectiveness of the RQMC method (Schemes II,\nIII, and IV) over the plain MC method (Scheme I). The effectiveness of the RQMC\nmethod is also observed by comparing Schemes II and III, where {Vk}k\u2208N is gener-\nated, respectively, by the MC and the RQMC methods. Amongst the four schemes,\nScheme IV yields the most accurate estimates. The superiority of Scheme IV can be\nexplained in terms of effective dimension. We present in Table 8 a CER of moment\nestimation for gamma random variables. To distinguish from (CER)n of (3.5), we\ndefine an appropriate CER for Scheme IV as follows. Let {V \u2032k}k\u2208N be an i.i.d. copy\nof {Vk}k\u2208N and also be independent of {Ek}k\u2208N. Define, for n \u2208 N,\nUn := {V1, . . . , Vn, V \u2032n+1, V \u2032n+2, . . .},\nwhere U0 := {Vk}k\u2208N. Then, define a CER, slightly different from (3.5), by\n(4.8) (CER)(m,n) :=\nCov(Xf (Em,Un), Xf (E0,U0))\nVar(Xf (E0,U0))\n.\nRecall that in computing (CER)n via (3.5) a common i.i.d. random sequence U0 is\nplugged into both sides of the covariance operator.\nWe present in Table 8 numerical results of (CER)n and (CER)(m,n), again with\nrounding operations. The results provide clear evidence that Scheme IV significantly\nincreases the CER with the first few dimensions, relative to Scheme III. Surprisingly,\na simple use of the RQMC method without additional variance reduction techniques\ncan capture almost 90% of the total variance only with the first five dimensions out\nof as many as 1000 nominal dimensions if the sequence is assigned in a smart way.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nCopyright \u00a9 by SIAM. Unauthorized reproduction of this article is prohibited. \nQUASI-MONTE CARLO FOR SERIES REPRESENTATIONS 1895\nTable 7\nMoment estimation for gamma random variables through four schemes.\na E[X\u22120.2] Scheme Average Standard error RMSE Relative error\n0.5 1.687812\nI 1.683850 4.16E-03 1.95E-03 0.23%\nII 1.688163 1.63E-03 7.53E-04 0.02%\nIII 1.688572 1.56E-03 7.22E-04 0.05%\nIV 1.685838 5.68E-04 3.12E-04 0.12%\n1.0 1.164230\nI 1.163710 1.03E-03 4.75E-04 0.04%\nII 1.164274 4.04E-04 1.86E-04 0.00%\nIII 1.164073 2.74E-04 1.27E-04 0.01%\nIV 1.164230 8.90E-05 4.10E-05 0.00%\n1.5 1.012687\nI 1.012363 6.01E-04 2.78E-04 0.03%\nII 1.013300 3.24E-04 1.58E-04 0.06%\nIII 1.012563 1.57E-04 7.31E-05 0.01%\nIV 1.012661 3.16E-05 1.47E-05 0.00%\na E[X1] Scheme Average Standard error RMSE Relative error\n0.5 0.5\nI 0.497839 2.61E-03 1.22E-03 0.43%\nII 0.499543 1.48E-03 6.84E-04 0.09%\nIII 0.502280 8.86E-04 4.52E-04 0.46%\nIV 0.499953 7.07E-05 3.28E-05 0.01%\n1.0 1.0\nI 1.163710 1.03E-03 4.75E-04 0.04%\nII 1.164274 4.04E-04 1.86E-04 0.00%\nIII 1.164073 2.74E-04 1.27E-04 0.01%\nIV 1.164230 8.90E-05 4.10E-05 0.00%\n1.5 1.5\nI 1.498708 3.98E-03 1.84E-03 0.09%\nII 1.496747 2.55E-03 1.21E-03 0.22%\nIII 1.502389 1.07E-03 5.33E-04 0.16%\nIV 1.499717 1.90E-04 9.10E-05 0.02%\nTable 8\nEstimated CER for the moment estimations presented in percentage.\n(CER)n (CER)(m,n)\n\u03b7 a 1 2 3 4 5 (1,0) (1,1) (2,1) (2,2) (3,2)\n\u22120.2\n0.5 90% 90% 91% 95% 95% 90% 95% 99% 100% 100%\n1.0 67% 75% 76% 76% 77% 67% 80% 91% 95% 98%\n1.5 51% 63% 66% 67% 68% 51% 66% 81% 89% 93%\n1\n0.5 39% 48% 50% 50% 50% 39% 79% 88% 96% 97%\n1.0 33% 45% 48% 49% 50% 33% 66% 77% 89% 92%\n1.5 28% 41% 46% 49% 49% 28% 57% 69% 82% 87%\n5. Concluding remarks. In this paper, we have proposed an effective applica-\ntion of QMC methods to shot noise series representations of infinitely divisible random\nvectors. We have observed that the largest jumps consisting of an infinitely divisible\nrandom vector are expressed only by the first few terms of the series and that this\nstructure tends to decrease the effective dimension and thus increase the efficiency of\nthe QMC method, thanks to the greater uniformity of low-discrepancy sequence in\nthe lower dimension. We have illustrated the effectiveness of our approach through\nnumerical results of moment and tail probability estimations for stable and gamma\nrandom variables. We expect that our results support a simulation use of shot noise\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nCopyright \u00a9 by SIAM. Unauthorized reproduction of this article is prohibited. \n1896 JUNICHI IMAI AND REIICHIRO KAWAI\nseries representations of various infinitely divisible laws with ever increasing compu-\ntational speed.\nLet us close by describing some related research topics. First, we have discussed\nthat an explicit representation is rarely available through the inverse Le\u00b4vy measure\nmethod, while some other methods may provide a representation in closed form, as\nseen in Examples 2.2 and 2.3. Nevertheless, additional random sequences are required\nfor such representations and ruin the effectiveness of the QMC method applied to\nPoisson interarrival times, as demonstrated in sections 4.2.1 and 4.2.2. In particu-\nlar, the representation for CGMY random variables in Example 2.3 contains so many\nadditional random sequences that our approach in section 4.2.2 would no longer be ef-\nfectively applicable. To address this issue, the authors have recently developed in [10]\na numerical method for implementing the inverse Le\u00b4vy measure method on a com-\nputer. This is worthwhile for two reasons: (i) the effectiveness of the QMC method\nis not watered down by other random sequences, and (ii) in principle, any infinitely\ndivisible random vector can then be simulated with shot noise series representation.\nSecond, it is well known that the series representation can be extended to Le\u00b4vy\nprocesses on a compact time interval simply by scattering all the jumps uniformly on\nthe time interval, that is,\n{Xt : t \u2208 [0, T ]} L=\n{\n+\u221e\u2211\nk=1\n[H(\u0393k, Uk)\u0000(Tk \u2208 [0, t])\u2212 tck] : t \u2208 [0, T ]\n}\n,\nwhere {Tk}k\u2208N is a sequence of i.i.d. uniform random variables on [0, T ]. This rep-\nresentation is known to be useful in, for example, simulation of trajectories of Le\u00b4vy\nprocesses, weak approximation of Le\u00b4vy-driven stochastic differential equations, and\nsimulation of fractional Le\u00b4vy motions (see, for example, Houdre\u00b4 and Kawai [8]). Here,\nQMC techniques can also be applied to the jump timings {Tk}k\u2208N, not only to the\njump size driving {\u0393k}k\u2208N, just as in section 4.2.2.\nAcknowledgment. Part of this work was carried out while RK was based at\nthe Center for the Study of Finance and Insurance, Osaka University, Japan.\nREFERENCES\n[1] S. Asmussen and P. W. Glynn, Stochastic Simulation, Springer-Verlag, New York, 2007.\n[2] S. Asmussen and J. Rosin\u00b4ski, Approximations of small jumps of Le\u00b4vy processes with a view\ntowards simulation, J. Appl. Probab., 38 (2001), pp. 482\u2013493.\n[3] L. Bondesson, On simulation from infinitely divisible distributions, Adv. in Appl. Probab., 4\n(1982), pp. 855\u2013869.\n[4] P. Carr, H. Geman, D. B. Madan, and M. Yor, The fine structure of asset returns: An\nempirical investigation, J. Business, 75 (2002), pp. 303\u2013325.\n[5] R. Caflisch, W. Morokoff, and A. Owen, Valuation of mortgaged-backed securities using\nBrownian bridges to reduce effective dimension, J. Comput. Finance, 1 (1997), pp. 27\u201346.\n[6] T. S. Ferguson and M. J. Klass, A representation of independent increment processes with\nGaussian components, Ann. Math. Statist., 43 (1972), pp. 1634\u20131643.\n[7] W. Ho\u00a8rmann and J. Leydold, Continuous random variate generation by fast numerical in-\nversion, ACM Trans. Model. Comput. Simul., 13 (2003), pp. 347\u2013362.\n[8] C. Houdre\u00b4 and R. Kawai, On fractional tempered stable motion, Stochastic Process. Appl.,\n116 (2006), pp. 1161\u20131184.\n[9] C. Houdre\u00b4 and R. Kawai, On layered stable processes, Bernoulli, 13 (2007), pp. 252\u2013278.\n[10] J. Imai and R. Kawai, Numerical Inverse Le\u00b4vy Measure Method for Infinite Shot Noise Series\nRepresentation, http:\/\/sites.google.com\/site\/reiichirokawai\/ (2010).\n[11] J. Imai and R. Kawai, On Finite Truncation of Infinite Shot Noise Series Representation of\nTempered Stable Laws, http:\/\/sites.google.com\/site\/reiichirokawai\/ (2010).\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nCopyright \u00a9 by SIAM. Unauthorized reproduction of this article is prohibited. \nQUASI-MONTE CARLO FOR SERIES REPRESENTATIONS 1897\n[12] J. Imai and K. S. Tan, A general dimension reduction technique for derivative pricing, J.\nComput. Finance, 10 (2006), pp. 129\u2013155.\n[13] J. Imai and K. S. Tan, An accelerating quasi-Monte Carlo method for option pricing under\nthe generalized hyperbolic Le\u00b4vy process, SIAM J. Sci. Comput., 31 (2009), pp. 2282\u20132302.\n[14] J. Imai and K. Tan, Dimension reduction approach to simulating exotic options in a Meixner\nLe\u00b4vy market, IAENG Int. J. Appl. Math., 39 (2009), pp. 265\u2013275.\n[15] R. Kawai, An importance sampling method based on the density transformation of Le\u00b4vy pro-\ncesses, Monte Carlo Methods Appl., 12 (2006), pp. 171\u2013186.\n[16] R. LePage, Multidimensional infinitely divisible variables and processes. II, in Probability\nin Banach Spaces, III, Lecture Notes in Math. 860, Springer-Verlag, Berlin, New York,\nHeidelberg, 1981, pp. 279\u2013284.\n[17] D. Madan and M. Yor, Representing the CGMY and Meixner Le\u00b4vy processes as time changed\nBrownian motions, J. Comput. Finance, 12 (2008), pp. 27\u201347.\n[18] H. Niederreiter, Random Number Generation and Quasi-Monte Carlo Methods, SIAM, Phil-\nadelphia, 1992.\n[19] A. Owen, Latin supercube sampling for very high-dimensional simulations, ACM Trans. Model.\nComput. Simul., 8 (1998), pp. 71\u2013102.\n[20] A. Owen, Randomly permuted (t,m, s)-nets and (t, s)-sequences, in Monte Carlo and Quasi-\nMonte Carlo Methods in Scientific Computing, H. Niederreiter and P. J. S. Shiue, eds.,\nSpringer-Verlag, New York, 1995, pp. 299\u2013317.\n[21] J. Rosin\u00b4ski, Series representations of Le\u00b4vy processes from the perspective of point processes, in\nLe\u00b4vy Processes\u2014Theory and Applications, O.-E. Barndorff-Nielsen, T. Mikosch, and S. I.\nResnick, eds., Birkha\u00a8user, Boston, 2001, pp. 401\u2013415.\n[22] J. Rosin\u00b4ski, Tempering stable processes, Stochastic Process. Appl., 117 (2007), pp. 677\u2013707.\n[23] G. Samorodnitsky and M. S. Taqqu, Stable Non-Gaussian Random Processes, Chapman &\nHall, New York, 1994.\n[24] K. Sato, Le\u00b4vy Processes and Infinitely Divisible Distributions, Cambridge University Press,\nCambridge, UK, 1999.\n[25] I. M. Sobol\u2019, Distribution of points in a cube and integration nets, Uspehi Mat. Nauk., 21\n(1966), pp. 271\u2013272.\n[26] I. M. Sobol\u2019, Global sensitivity indices for nonlinear mathematical methods and their Monte\nCarlo estimates, Math. Comput. Simul., 55 (2001), pp. 271\u2013280.\n[27] X. Wang and K. T. Fang, The effective dimension and quasi-Monte Carlo integration, J.\nComplexity, 19 (2003), pp. 101\u2013124.\n[28] W. Vervaat, On a stochastic difference equation and representation of nonnegative infinitely\ndivisible random variables, Adv. in Appl. Probab., 11 (1979), pp. 750\u2013783.\n"}