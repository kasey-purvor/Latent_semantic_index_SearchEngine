{"doi":"10.1117\/12.585882","coreId":"71275","oai":"oai:eprints.lancs.ac.uk:4366","identifiers":["oai:eprints.lancs.ac.uk:4366","10.1117\/12.585882"],"title":"Particle Filtering with Multiple Cues for Object Tracking in Video Sequences","authors":["Brasnett, P.","Mihaylova, L.","Canagarajah, N.","Bull, D."],"enrichments":{"references":[],"documentType":{"type":null}},"contributors":[],"datePublished":"2005-01-16","abstract":"In this paper we investigate object tracking in video sequences by using the potential of particle filtering to process features from video frames. A particle filter (PF) and a Gaussian sum particle filter (GSPF) are developed based upon multiple information cues, namely colour and texture, which are described with highly nonlinear models. The algorithms rely on likelihood factorisation as a product of the likelihoods of the cues. We demonstrate the advantages of tracking with multiple independent complementary cues compared to tracking with individual cues. The advantages are increased robustness and improved accuracy. The performance of the two filters is investigated and validated over both synthetic and natural video sequences","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":"SPIE","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:4366<\/identifier><datestamp>\n      2018-01-24T05:51:12Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D626F6F6B5F73656374696F6E<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Particle Filtering with Multiple Cues for Object Tracking in Video Sequences<\/dc:title><dc:creator>\n        Brasnett, P.<\/dc:creator><dc:creator>\n        Mihaylova, L.<\/dc:creator><dc:creator>\n        Canagarajah, N.<\/dc:creator><dc:creator>\n        Bull, D.<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        In this paper we investigate object tracking in video sequences by using the potential of particle filtering to process features from video frames. A particle filter (PF) and a Gaussian sum particle filter (GSPF) are developed based upon multiple information cues, namely colour and texture, which are described with highly nonlinear models. The algorithms rely on likelihood factorisation as a product of the likelihoods of the cues. We demonstrate the advantages of tracking with multiple independent complementary cues compared to tracking with individual cues. The advantages are increased robustness and improved accuracy. The performance of the two filters is investigated and validated over both synthetic and natural video sequences.<\/dc:description><dc:publisher>\n        SPIE<\/dc:publisher><dc:date>\n        2005-01-16<\/dc:date><dc:type>\n        Contribution in Book\/Report\/Proceedings<\/dc:type><dc:type>\n        NonPeerReviewed<\/dc:type><dc:relation>\n        http:\/\/dx.doi.org\/10.1117\/12.585882<\/dc:relation><dc:identifier>\n        Brasnett, P. and Mihaylova, L. and Canagarajah, N. and Bull, D. (2005) Particle Filtering with Multiple Cues for Object Tracking in Video Sequences. In: SPIE Proceedings. SPIE, pp. 430-441.<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/4366\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1117\/12.585882","http:\/\/eprints.lancs.ac.uk\/4366\/"],"year":2005,"topics":["QA75 Electronic computers. Computer science"],"subject":["Contribution in Book\/Report\/Proceedings","NonPeerReviewed"],"fullText":"Copyright 2005 Society of Photo-Optical Instrumentation Engineers. This paper was published in Proc. of\nImage and Video Communications and Processing within the SPIE Symp. on Electronic Imaging and is made\navailable as an electronic reprint with permission of SPIE. One print or electronic copy may be made for personal\nuse only. Systematic or multiple reproduction, distribution to multiple locations via electronic or other means,\nduplication of any material in this paper for a fee or for commercial purposes, or modification of the content of\nthe paper are prohibited.\nParticle Filtering with Multiple Cues\nfor Object Tracking in Video Sequences\nPaul Brasnett, Lyudmila Mihaylova, Nishan Canagarajah and David Bull\nUniversity of Bristol, Department of Electrical and Electronic Engineering,\nMerchant Venturers Building, Woodland Road, UK\nABSTRACT\nIn this paper we investigate object tracking in video sequences by using the potential of particle filtering to process\nfeatures from video frames. A particle filter (PF) and a Gaussian sum particle filter (GSPF) are developed based\nupon multiple information cues, namely colour and texture, which are described with highly nonlinear models.\nThe algorithms rely on likelihood factorisation as a product of the likelihoods of the cues. We demonstrate the\nadvantages of tracking with multiple independent complementary cues compared to tracking with individual\ncues. The advantages are increased robustness and improved accuracy. The performance of the two filters is\ninvestigated and validated over both synthetic and natural video sequences.\nKeywords: particle filtering, Bayesian methods, tracking in video sequences, colour, texture, Gaussian sum\nparticle filtering\n1. INTRODUCTION\nObject tracking is required in many vision applications such as human-computer interfaces, video communica-\ntion\/compression, road traffic control, security and surveillance systems. Often the goal is to obtain a record of\nthe trajectory of moving single or multiple targets over time and space. Object tracking in video sequences is a\nchallenging task because of the large amount of data used and the common requirement for real-time computa-\ntion. Moreover, most of the models encountered in visual tracking are nonlinear, non-Gaussian, multi-modal or\nany combination of these.\nIn this paper we focus on particle filtering techniques for tracking, since they have recently proven to be\na powerful and reliable tool for nonlinear systems.1\u20133 Particle filtering is a promising technique because of\nits inherent property to allow fusion of different sensor data, to account for different uncertainties, to cope\nwith data association problems when multiple targets are tracked with multiple sensors and to incorporate\nconstraints. There are still issues which must be properly addressed if successful algorithms are to be developed.\nThese include the degeneracy problem (when all but one particle has significant normalised weight), appropriate\nchoice of proposal distributions, likelihood evaluation and computational complexity for efficient implementation.\nHere we look at the likelihood evaluation, specifically comparing single and multiple independent cues. Particle\nfiltering has been applied to various fields and has other names including the CONDENSATION algorithm4 and\nbootstrap filter.1 All these algorithms fall into the category of sequential Monte Carlo techniques since they\nkeep track of the state through sample-based representation of probability density functions over time.\nDifferent tracking algorithms are proposed for video sequences and their particularities are mainly application\ndependent. Many of them rely on a single cue, which can be chosen according to the application context. A\ncue-selection approach is proposed in Ref. 5 which is embedded in a hierarchical vision-based tracking algorithm.\nWhen the target is lost, layers cooperate to perform a rapid search for the target and continue tracking. Another\napproach, called democratic integration6 implements cues concurrently where all vision cues are complementary\nand contribute simultaneously to the overall result. Robustness and generality are major features of this approach.\nColour cues are a significant part of many tracking techniques,7\u201312 in combination with shape,7 motion and\nsound.8 Colour cues provide a weak model and as such are very unrestrictive about the objects they can be\nFurther author information: (Send correspondences to P.B.)\nE-mail: P.B.: paul.brasnett@bristol.ac.uk, E-mail: L.M.: mila.mihaylova@bristol.ac.uk\nused to represent. The greatest weakness of colour cues is the ambiguity in scenes with objects or regions with\ncolour features similar to those of the object of interest. As shown in Ref. 8, by fusing colour and motion cues\nthis ambiguity can be considerably reduced if the object is moving. The motivation for multiple cues comes\nfrom the inability of a single cue to fully describe the object and therefore its inability to achieve accurate and\nrobust results. For example in Ref. 8 colour is used as the main cue and motion and sound cues are proposed to\ncomplement the colour because they can provide a greater degree of discrimination.\nIn Ref. 9 a general framework is introduced for the integration of multiple cues, driven by the idea that\ndifferent cues are suitable for different conditions. Activating and suppressing them dynamically, may increase\nthe performance of the system.\nSo far, texture cues have not been widely used for video based tracking purposes. Furthermore, they have\nnot been applied to tracking with particle filtering techniques. In this paper we show that colour and texture\ncomplement each other and provide reliable performance. Additionally to colour and texture, other cues such as\ngradient, edges, motion and illumination can be added when necessary to avoid ambiguities.\nAdaptive colour and texture segmentation for tracking moving objects is proposed in Ref. 10. An autobinomial\nGibbs Markov random field is used for modelling the texture whilst colour is modelled by a 2D Gaussian\ndistribution. In this paper texture is represented using a wavelet decomposition which is a different approach to\nRef. 10 and in the framework of particle filtering. In Ref. 10 segmentation-based tracking with a Kalman filter\nis considered instead of feature-based tracking proposed here.\nThe paper is organised as follows. Section 2 states the problem of object tracking using video sequences within\na sequential Monte Carlo framework. Section 3 introduces the motion model and cues used for the tracking.\nSection 4 describes a PF algorithm we developed based on fusing colour and texture information cues. Section\n5 presents a GSPF algorithm for object tracking in video sequences. In Section 6 the filters\u2019 performance is\ninvestigated and validated over different scenarios. We show the advantage of fusing multiple cues compared to\ncolour-based tracking only and texture-based tracking only by synthetic and natural video sequences. Finally,\nSect. 7 discusses the results and open issues for future research.\n2. SEQUENTIAL MONTE CARLO FRAMEWORK\nThe aim of sequential Monte Carlo estimation is to evaluate the posterior probability density function (PDF)\np(xk|Zk) of the state vector xk given a set Zk = {z1, . . . ,zk} of sensor measurements at a time k.\nThe Monte Carlo approach relies on a sample-based construction to represent the state PDF. Multiple\nparticles (samples) of the state are generated, each one associated with a weight which characterises the quality\nof a specific particle. An estimate of the variable of interest is obtained by the weighted sum of particles. Two\nmajor stages can be distinguished : prediction and update. During the prediction each particle is modified\naccording to the state model of the region of interest in the video frame, including the addition of random noise\nin order to simulate the effect of the noise on the state. In the update stage, each particle\u2019s weight is re-evaluated\nbased on the new data. A resampling procedure deals with the elimination of particles that have small weights\nand replication of the particles with larger weights.\n3. MOTION AND LIKELIHOOD MODELS\n3.1. Motion Model\nFor the purpose of tracking an object in video we initially choose a region which defines the object. The shape\nof this region is fixed a priori and in our case we choose a rectangular box characterised by the state vector\nx = (x, x\u02d9, x\u00a8, y, y\u02d9, y\u00a8)\u2032, with x and y denoting the pixel location of the top, left corner of the rectangle, with\nvelocities x\u02d9 and y\u02d9, respectively, and accelerations x\u00a8 and y\u00a8. Note that the dimensions of the rectangle are fixed\nthrough the sequence. The motion model used is the constant acceleration model13\nxk+1 = Fxk + \u0393vk, vk \u223d N (0,Q), (1)\nwhere F = diag(F ,F ), \u0393 = (\u0393\u2032,\u0393\u2032),\nF =\n\uf8eb\n\uf8ed\n1 T 12T 2\n0 1 T\n0 0 1\n\uf8f6\n\uf8f8 , \u0393 =\n\uf8eb\n\uf8ed\n1\n2T 2\nT\n1\n\uf8f6\n\uf8f8 ,\nand T is the sampling interval. The Gaussian system noise v = (v\u2032x,v\u2032y)\u2032 has a covariance matrix Q =\ndiag(Qx,Qy), Qx = V \u03c32x, Qy = V \u03c32y, with\nV =\n\uf8eb\n\uf8ed\n1\n4T 4 12T 3 12T 21\n2T 3 T 2 T1\n2T 2 T 1\n\uf8f6\n\uf8f8 ,\nwhere \u03c3x and \u03c3y are the standard deviations of the noises respectively in x and y coordinates. It is suggested\nthat suitable values for \u03c32v are in the range 0.5am \u2264 \u03c3v \u2264 am, with am being the maximum acceleration.13\n3.2. Likelihood Models\nUnder the assumption that the features being used as cues are independent, the overall likelihood is a product\nof the likelihoods of the separate cues, in our case colour and texture, as shown in equation (2)\nL(zk|xk) = Lcolour(zcolour,k|xk)Ltexture(ztexture,k|xk). (2)\nwhere zk denotes the measurement vector, composed by the measurement vector zcolour,k from the colour cue\nand the vector ztexture,k from the texture cue. Sections 3.3 and 3.4 give more details for the particular cues\nconsidered. To combine colour and texture features in a Bayesian framework we need to make the assumption\nof conditional independence.\n3.3. Colour Cue\nThe colour likelihood model has to be defined in a way to favor candidate colour histograms close to the reference\nhistogram. The histogram, hcx = (hc1,x, . . . , hcB,x), for a region Sx corresponding to a state x is given by\nhci,x = CN\n\u2211\nu\u01ebSx\n\u03b4i(bcu), i = 1 . . . B, (3)\nwhere bcu \u2208 {1 . . . B} is the histogram bin index associated with the intensity at pixel location u = (x, y) in\nchannel c of the colour image yc and CN is a normalising constant such that\n\u2211B\ni=1 hci,x = 1. We use 8x8x8 bin\nhistograms in the three channels of red, green, blue (RGB) colour space.8\nA distance metric which is appropriate to make decisions about the closeness of two histograms h\u02c61 =\u2211\nc\u2208{R,G,B} h\nc\nx1 and h\u02c62 =\n\u2211\nc\u2208{R,G,B} h\nc\nx2 is the Bhattacharyya similarity distance12, 14, 15\nd(h\u02c61, h\u02c62) =\n\u221a\n1 \u2212 \u03c1(h\u02c61, h\u02c62), (4)\nwhere \u03c1(h\u02c61, h\u02c62) is the Bhattacharyya coefficient,\n\u03c1(h\u02c61, h\u02c62) =\nm\u2211\ni=1\n\u221a\nh\u02c61,ih\u02c62,i. (5)\nThe larger the coefficient \u03c1(h\u02c61, h\u02c62) is, the more similar the distributions are. The Bhattacharyya distance\nvalues are within the interval [0, 1]. For two identical normalised histograms we obtain d = 0 (\u03c1 = 1) indicating\na perfect match.\nBased on this distance, the colour likelihood model can be defined by8\nLcolour(zcolour|x) \u221d exp\n\uf8eb\n\uf8ed\u2212\n\u2211\nc\u2208{R,G,B}\nd2(hcx,hcref )\/2\u03c32C\n\uf8f6\n\uf8f8 , (6)\nwhere the standard deviation \u03c3C specifies the Gaussian noise in the measurements, hcx is the current histogram\nof the target, and hcref is the reference histogram. Small Bhattacharyya distances correspond to large weights\nin the particle filter.\n3.4. Texture Cue\nTexture is an appealing feature to use as a basis for an observation model because of its intuitive definition.\nQualitatively, texture can be described by terms such as fine, coarse, grained and smooth. Although there is no\nunique definition of texture, it is generally agreed that texture describes the spatial arrangements of pixel levels\nin an image, which may be stochastic or periodic, or both.16 When a texture is viewed from a distance it may\nappear to be fine, however, when viewed from close up it may appear to be coarse.\nTexture properties are analysed by different techniques such as statistical methods, spatial frequency methods,\nstructural methods and fractal-based methods. The method for feature extraction we have chosen is the spatial-\nfrequency method of wavelets which implements a discrete wavelet transform.\n3.4.1. The discrete wavelet transform\nThe discrete wavelet transform (DWT)17, 18 decomposition performs a series of subband filtering operations on\nthe original image region. The lower frequency subband then forms the input image for the next level of the\ntransform (Fig. 1a). It should be noted that the resulting transformed image has the same dimensions as the\noriginal image due to a subsampling stage between each stage of decomposition. The wavelet transform provides\na representation containing different frequencies and different resolutions in frequency and time. In this work\nthe Haar wavelets are used as the filter due to their simplicity and compactness. Other wavelet transforms can\nbe applied and allow for an improved overall performance.\n3.4.2. Texture analysis using wavelets\nThe coefficients of each channel in the DWT contain information about the spatial and spatial-frequency infor-\nmation of the input image (see Fig. 1b). The LH channel (channel 9) for example contains the lower subband of\nthe horizontal frequencies and the higher subband of the vertical frequencies, this channel is therefore dominated\nby horizontal edge information.\nWe now describe the process of generating a feature vector representing the texture of a region. Firstly, to\nremove the effect of the input image mean we subtract it from the image before we take the wavelet transform.\nThen a three-level DWT decomposition is applied to generate ten channels (Fig. 1). The texture for the mth\nchannel xm is then represented by the l1-norm\nem =\n1\nMP\nM\u2211\ni=1\nP\u2211\nj=1\n|x(i, j)| , (7)\nwhere M and P are the dimensions of the channel and x(i, j) is the wavelet coefficient at location i,j. The\ntexture of the region using this scheme is represented by a ten-element feature vector\nt = {e1, e2, . . . , e10}. (8)\nBefore we use the feature vector to represent the texture PDF we must ensure that the values are in the\nrange [0, 1] and are normalised. To do this we mean shift by min{t, 0} and then normalise the vector, where\nmin{x} is the smallest value in x. We notate the mean-shifted, normailsed feature vector as t\u00af. It is now possible\nto determine the distance between two texture feature vectors.\n(a) Two-level wavelet transform\n1\r\n4\r3\r\n2\r\n5\r\n7\r6\r\n8\r\n10\r9\r\nLL\r HL\r\nLH\r HH\r\n(b) The ten channels of a three-level\ndiscrete wavelet transform\nFigure 1. Discrete wavelet transform\nA measure, d, is defined using the Bhattacharyya coefficient characterising the difference (distance) between\ntwo normalised texture feature vectors t\u00af1 and t\u00af2.\nd(t\u00af1, t\u00af2) =\n\u221a\n1 \u2212 \u03c1(t\u00af1, t\u00af2), (9)\nwhere the Bhattacharyya coefficient, \u03c1(t\u00af1, t\u00af2), is defined as\n\u03c1(t\u00af1, t\u00af2) =\nm\u2211\ni=1\n\u221a\nt\u00af1,it\u00af2,i. (10)\nThe texture likelihood can then be defined in a similar way to the colour likelihood introduced in Sect. 3.3.\nLtexture(ztexture|x) \u221d exp\n(\n\u2212d2(t\u00afx, t\u00afref )\/2\u03c32t\n)\n, (11)\nwhere \u03c3t is the standard deviation of the Gaussian texture noise, t\u00afx is the texture histogram of the current\nframe, and t\u00afref is the reference texture histogram.\n4. A PARTICLE FILTER ALGORITHM FOR OBJECT TRACKING USING\nMULTIPLE CUES\nWithin the Bayesian framework, the conditional PDF p(xk+1|Zk) is recursively updated according to the pre-\ndiction step\np(xk+1|Zk) =\n\u222b\np(xk+1|xk)p(xk|Zk)dxk, (12)\nand the update step\np(xk+1|Zk+1) =\np(zk+1|xk+1)p(xk+1|Zk)\np(zk+1|Zk)\n, (13)\nwhere p(zk+1|Zk) is a normalising constant. The recursive update of p(xk+1|Zk+1) is proportional to\np(xk+1|Zk+1) \u221d p(zk+1|xk+1)p(xk|Zk). (14)\nUsually, there is no simple analytical expression for propagating p(xk+1|Zk+1) through (14) so we can use\nnumerical methods.\nIn the particle filter approach a cloud of N weighted particles, drawn from the posterior conditional PDF, is\nused to map integrals to discrete sums. The posterior p(xk+1|Zk+1) is approximated by\np\u02c6(xk+1|Zk+1) \u2248\nN\u2211\nl=1\nW\u0302 (l)k+1\u03b4(xk+1 \u2212 x\n(l)\nk+1), (15)\nwhere \u03b4 is the delta-Dirac function and W\u0302 (l)k are the normalised importance weights. New weights are calculated,\nputting more weight on particles that are important according to the posterior PDF (15).\nThe random samples {x(l)k }Nl=1 are drawn from p(xk+1|Zk+1). It is often impossible to sample from the\nposterior density function p(xk|Zk). This difficulty is circumvented by making use of the importance sampling\nfrom a known proposal distribution p(xk+1|xk).\nThe developed particle filter algorithm is given in Table 1. Note that the resampling step is included in order\nto avoid the problem of degeneracy, when only one particle has significant normalised weight.\nTable 1 The particle filter with color and texture cues\nInitialisation\n1. k = 0, for l = 1, . . . , N , generate samples {x(l)0 } from the initial distribution p(x0).\nPrediction Step\n2. For k = 1, 2, . . . , l = 1, . . . , N , sample x(l)k+1 \u223c p(xk+1|x\n(l)\nk ), the motion model (1).\nMeasurement Update: evaluate the importance weights\n3. On the receipt of a new measurement, compute the weights\nW (l)k+1 \u221d L(zk+1|x\n(l)\nk+1). (16)\n4. Normalise the weights, W\u0302 (l)k+1 =\nW (l)k+1\n\u2211N\nl=1 W\n(l)\nk+1\n. The likelihood L(zk+1|x(l)k+1) is calculated from Eq. (2).\nOutput\n5. A collection of samples, from which the approximate posterior distribution is computed, as follows\np\u02c6(xk+1|Zk+1) =\nN\u2211\nl=l\nW\u0302 (l)k+1\u03b4(xk+1 \u2212 x\n(l)\nk+1).\n6. The posterior mean E[xk+1|Zk+1] is computed using the collection of samples (particles)\nx\u02c6k+1 = E[xk+1|Zk+1] =\nN\u2211\nl=1\nW\u0302 (l)k+1x\n(l)\nk+1. (17)\nSelection step (resampling)\n7. Multiply\/ suppress samples x(l)k+1 with high\/ low importance weights W\u0302\n(l)\nk+1, in order to obtain N new random\nsamples approximately distributed according to p(x(l)k+1|Zk+1).\n8. Set k = k + 1 and return to step 2.\n5. A GAUSSIAN SUM PARTICLE FILTER WITH MULTIPLE CUES\nThe GSPF19, 20 is a recently developed filtering technique in which the state filtering and prediction probability\ndensity functions are approximated by finite mixtures of Gaussian components. When the object model and\nobservation model noises are non-Gaussian, they can also be represented as Gaussian mixtures. The GSPF is\nessentially a bank of Gaussian particle filters.21 The GSPF reduces the estimation problem solution to updating\na Gaussian mixture, where mean, covariance, and weights are tracked with each new observation.\nThe filtering state PDF can be approximated as follows\np(xk+1|zk+1) =\nG\u2211\ni=1\nN (xk+1;\u00b5xk+1,i,\u03a3xk+1,i), (18)\nby G components having mean and covariances respectively \u00b5xk+1,i and \u03a3xk+1,i.\nThe GSPF algorithm with multiple information cues is given in Table 2, note that G is the number of Gaussian\ncomponents.\nTable 2 The GSPF with color and texture cues\nInitialisation\n1. k = 0, for i = 1, . . . , G, initialise \u00b5x0,i,\u03a3x0,i, W\u03020,i =\n1\nG .\nTime update\nFor k = 1, 2, . . .,\n2. For i = 1, . . . , G, obtain samples from N (xk; \u00b5xk,i,\u03a3xk,i) and denote them as {x\n(j)\nk,i}Nj=1 for N particles.\n3. For i = 1, . . . , G, j = 1, . . . , N , sample x(j)k+1,i \u223c p(xk+1,i|x\n(j)\nk+1,i), the motion model (1).\n4. For i = 1, . . . , G, update weights Wk+1,i = W\u0302k,i.\n5. For i = 1, . . . , G, from {x(j)k+1,i}Nj=1 obtain sample mean \u00b5\u00afxk+1,i and sample variance \u03a3\u00afxk+1,i by averaging over N\nparticles.\nThe time updated (prediction) density can now be approximated as\np(xk+1|Zk) =\nG\u2211\ni=1\nWk+1,iN (xk+1; \u00b5\u00afxk+1,i, \u03a3\u00afxk+1,i). (19)\nMeasurement update\n6. For i = 1, . . . , G, obtain samples from the distribution N (xk+1; \u00b5\u00afxk+1,i, \u03a3\u00afxk+1,i) and denote them as {x\n(j)\nk+1,i}Nj=1.\n7. For i = 1, . . . , G, j = 1, . . . , N compute the weights \u03c9(j)k+1,i \u221d L(zk+1|x\n(j)\nk+1,i). The likelihood L is calculated from\nEq. (2).\n8. For i = 1, . . . , G, j = 1, . . . , N estimate the mean and covariance by\n\u00b5xk+1,i =\n\u2211N\nj=1\u03c9\n(j)\nk+1,ix\n(j)\nk+1,i\u2211N\nj=1\u03c9\n(j)\nk+1,i\n, (20)\n\u03a3xk+1,i =\n\u2211N\nj=1 \u03c9\n(j)\nk+1,i(x\n(j)\nk+1,i \u2212 \u00b5xk+1,i)(x\n(j)\nk+1,i \u2212 \u00b5xk+1,i)\n\u2032\n\u2211N\nj=1 \u03c9\n(j)\nk+1,i\n. (21)\n9. For i = 1, . . . , G, update the weights\nWk+1,i = W\u0302k,i\n\u2211N\ni=1 \u03c9\n(j)\nk,j\u2211G\ni=1\n\u2211N\nj=1 \u03c9\n(j)\nk,j\n, (22)\n10. Normalise the weights\nW\u0302k+1,i =\nWk+1,i\u2211G\ni=1 Wk+1,i\n. (23)\nOutput\n11. Calculate the state estimate x\u02c6k+1 = E(xk+1|Zk+1) and covariance \u03a3\u02c6xk+1 = E[(xk+1 \u2212 x\u02c6k+1)(xk+1 \u2212 x\u02c6k+1)]\u2032\nx\u02c6k+1 =\nG\u2211\ni=1\nW\u0302k+1,i\u00b5k+1,i, (24)\n\u03a3\u02c6xk+1 =\nG\u2211\ni=1\nW\u0302k+1,i[\u03a3xk+1,i + (x\u02c6k+1 \u2212 \u00b5k+1,i)(x\u02c6k+1 \u2212 \u00b5k+1,i)\u2032]. (25)\nResampling\n12. Resample the weights W\u0302k+1,i by the residual resampling algorithm. Other schemes such as the one described in\nRef. 2 can also be applied.\n13. For g = 1, . . . , G draw a number j \u2208 {1, . . . , G} with probabilities proportional to W\u0302k+1,1, . . . , W\u0302k+1,G and let{\n\u00b5k,g,\u03a3xk+1,g\n}\n=\n{\n\u00b5k,j ,\u03a3xk+1,j\n}\n, set W\u0302k+1,i = 1G .\n14. Set k = k + 1 and return to step 2.\nDuring the resampling step of the GSPF mixands with insignificant weights are discarded, whilst mixands\nwith significant weights are duplicated. Notice that the resampling procedure applied to the GSPF with multiple\ncues slightly differs from the described in Ref. 19 because of the fact that we are not representing the target noise\nas a Gaussian mixture. In the GSPF mixing components are resampled whilst in the PF the resampling step\napplies to particles. In the implemented GSPF resampling is performed at each time step. Since resampling is\nintroducing some bias, the calculation of the estimates in both PF and GSPF is performed before the resampling\nstep.\n6. EXPERIMENTAL RESULTS\nThis section evaluates two different ideas presented by this paper: i) the performance of colour cues, texture\ncues and combined colour and texture cues, ii) the performance of the PF and GSPF with multiple cues.\n6.1. Colour and Texture Cues\nTo evaluate the performance of the colour, texture and combined colour and texture cues we first present results\nfrom artificial sequences with known object motion obtained by 100 independent Monte Carlo runs in Sect. 6.1.1.\nThen in Sect. 6.1.2 we present results from natural sequences obtained by using the particle filter.\n6.1.1. Synthetic sequences\nTo generate the synthetic sequences we use textures taken from the Brodatz set,22 as seen in Fig. 2. The\nbackground to a particular sequence is taken as one of the textures and then the object to be tracked is a\nsubregion of a texture overlaid on the background as illustrated in Fig. 3.\nIf we know the correct location of the target object, xi, in each frame i = 1, . . . , Nf and R is the number of\nrealisations, the root mean squared error (RMSE) at a given frame is23\nRMSEx =\n\u221a\u221a\u221a\u221a 1\nR\nR\u2211\ni=0\n(xi \u2212 x\u02c6i)2, (26)\n(a) Texture 1 (b) Texture 2 (c) Texture 3 (d) Texture 4\nFigure 2. Textures, taken from the Brodatz texture book,22 used to create foreground and background objects in the\nartificial sequences.\nRMSExy = (RMSEx + RMSEy)\/2. (27)\nWe use the RMSExy defined in Eq. (27) to further define an error value for the sequence, Eseq\nEseq =\n(\u2211\nRMSExy\n)\n\/Nframes. (28)\nThis mean error can then characterise the tracking performance through an entire sequence. The mean error is\npresented for a set of synthetic sequences in Table 3.\nSynthetic Frame\r\nBackground Texture\r\nObject Texture\r\nFigure 3. Synthetic sequence creation\n0 10 20 30 40 50\n0\n0.5\n1\n1.5\n2\n2.5\n3\nFrame\nR\nM\nS\nE\nx\ny\nColour and Texture Cues\nColour Cues\nTexture Cues\n(a)\n0 10 20 30 40 50\n0\n0.5\n1\n1.5\n2\n2.5\n3\nFrame\nR\nM\nS\nE\nx\ny\nColour and Texture Cues\nColour Cues\nTexture Cues\n(b)\nFigure 4. RMSExy for colour, texture and combined colour and texture cues (from 100 runs). The tracked object in\nFigure (a) has different colour and texture to the background. Figure (b) is from a sequence where the texture of the\nbackground and of the object are the same. The colour of the object and of the background are very similar.\n(a) Frame 1 (b) Frame 35 (c) Frame 73\n(d) Frame 1 (e) Frame 35 (f) Frame 73\n(g) Frame 1 (h) Frame 35 (i) Frame 73\nFigure 5. Tracking an object with camera induced motion: results from the first, intermediate and final frames. Figs.\n(a)-(c) are generated using colour cues only, (d)-(f) use texture cues only, (g)-(i) use combined colour and texture cues.\nTable 3. Eseq of particle filter with texture feature cues for synthetic sequences. The features used are shown in Fig. 2\nBackground Texture\nObject Texture 1 2 3 4\n1 0.081 0.053 0.088 0.049\n2 1.145 0.636 1.023 1.493\n3 0.149 0.465 0.199 0.154\n4 4.593 4.881 0.983 1.772\nThe results for experiments with each cue are presented in Figure 4 with the RMSExy over 55 frames. Two\ndifferent synthetic sequences were used; i) the target object has texture and colour distinct to the background\n(Fig. 4a) and ii) the target object has similar colour and texture to the background (Fig. 4b). It can be seen\nthat for the first sequence (Fig. 4a) colour cues track the object with a much lower degree of accuracy than the\ntexture or combined cues. Texture provides good tracking results but the most accurate results are observed\nwhen the colour and texture cues are combined.\nA different outcome is noticeable in the second sequence (Fig. 4b). Here the weak model of the colour cues\nperforms significantly better than the texture cues. The colour cue is able to track the object through most\nof the sequence, although it does loose it in the last couple of frames. The texture cue looses the object at\naround frame 15 because of the similarity between the background and the target object. It can be seen that\nthe combined cues provide a good track that is more accurate and robust than the two cues individually.\n0 10 20 30 40 50\n0\n0.5\n1\n1.5\n2\n2.5\n3\nFrame\nR\nM\nS\nE\nx\ny\nPF\nGSPF\n(a) Different Background and Object\nTextures and Colours\n0 10 20 30 40 50\n0\n0.5\n1\n1.5\n2\n2.5\n3\nFrame\nR\nM\nS\nE\nx\ny\nPF\nGSPF\n(b) Similar Background and Object Tex-\ntures and Colours\nFigure 6. RMSExy for the particle filter and Gaussian sum particle filter for two artificial sequences. The results were\nobtained from using colour and textured cues combined.\n6.1.2. Natural sequences\nIt is important to investigate the tracking performance over natural sequences as well. Results with a natural\nsequence are given in Fig. 5. By using only colour cues we do not track the object accurately as is demonstrated\nby Fig. 5b. Texture cues are more accurate but do occasionally drift away from the target, as in Fig. 5f. When\ncolour and texture are combined we see consistently more accurate and robust results.\n6.2. Comparison of Particle Filter and Gaussian Sum Particle Filter\nHere we present results of experiments carried out to compare the performance of the PF and the GSPF algo-\nrithms with multiple cues. In order to assess both algorithms under the same conditions, we chose N = GM\nwhere N is the number of particles in the particle filter, G is the number of Gaussian mixing components in the\nGSPF and M is the number of particles per component in the GSPF. Therefore the two particle filters both\nhave the same total number of particles. Both filters were given the same noise covariances. It can be seen in\nFig. 6 that the performance, in terms of RMSExy of the two particle filters is very similar.\n7. CONCLUSIONS AND FUTURE WORK\nThis paper has presented a particle filter and a Gaussian sum particle filter for tracking. Both algorithms track\nobjects in video by fusing multiple cues. Colour and texture features are the cues described here, other possible\ncues include motion, edges and illumination. The advantages of combined colour and texture cues compared to\ntexture only and colour only demonstrated here are: i) improved accuracy and ii) improved robustness. The\nbenefits of fusing multiple independent cues is shown to apply to both filters.\nCurrent and future areas for research include the investigation of other cues, different fusion schemes, improved\nproposal distributions, automatic detection and recovery from a lost track, tracking multiple objects and online\nadaption of the target model.\nACKNOWLEDGMENTS\nThis work has been conducted with the UK MOD Data and Information Fusion Defence Technology Centre\nunder project DIF DTC 2.2.\nREFERENCES\n1. N. Gordon, D. Salmond, and A. Smith, \u201cA novel approach to nonlinear \/ non-Gaussian Bayesian state\nestimation,\u201d IEE Proceedings on Radar and Signal Processing 40, pp. 107\u2013113, 1993.\n2. A. Doucet, S. Godsill, and C. Andrieu, \u201cOn sequential Monte Carlo sampling methods for Bayesian filtering,\u201d\nStatistics and Computing 10(3), pp. 197\u2013208, 2000.\n3. M. Arulampalam, S. Maskell, N. Gordon, and T. Clapp, \u201cA tutorial on particle filters for online\nnonlinear\/non-Gaussian Bayesian tracking,\u201d IEEE Trans. on Signal Proc. 50(2), pp. 174\u2013188, 2002.\n4. M. Isard and A. Blake, \u201cCondensation \u2013 conditional density propagation for visual tracking,\u201d International\nJournal of Computer Vision 28(1), pp. 5\u201328, 1998.\n5. K. Toyama and G. Hager, \u201cIncremental focus of attention for robust vision-based tracking,\u201d International\nJournal of Computer Vision 35(1), pp. 45\u201363, 1999.\n6. J. Triesch and C. von der Malsburg, \u201cDemocratic integration: Self-organized integration of adaptive cues,\u201d\nNeural Computation 13(9), pp. 2049\u20132074, 2001.\n7. C. Shen, A. van den Hengel, and A. Dick, \u201cProbabilistic multiple cue integration for particle filter based\ntracking,\u201d in Proceedings of the VIIth Digital Image Computing : Techniques and Applications, C. Sun, H.\nTalbot, S. Ourselin, T. Adriansen, Eds., 10-12 Dec. 2003.\n8. P. Pe\u00b4rez, J. Vermaak, and A. Blake, \u201cData fusion for tracking with particles,\u201d Proceedings of the IEEE 92,\npp. 495\u2013513, March 2004.\n9. M. Spengler and B. Schiele, \u201cTowards robust multi-cue integration for visual tracking,\u201d Machine Vision and\nApplications 14(1), pp. 50\u201358, 2003.\n10. E. Ozyildiz, N. Krahnsto\u00a8ver, and R. Sharma, \u201cAdaptive texture and color segmentation for tracking moving\nobjects,\u201d Pattern Recognition 35, pp. 2013\u20132029, October 2002.\n11. P. Pe\u00b4rez, C. Hue, J. Vermaak, and M. Ganget, \u201cColor-based probabilistic tracking,\u201d in 7th European Con-\nference on Computer Vision Vol. 2350 of Lecture Notes in Computer Science, pp. 661\u2013675, (Denmark), May\n2002.\n12. K. Nummiaro, E. Koller-Meier, and L. V. Gool, \u201cAn adaptive color-based particle filter,\u201d Image and Vision\nComputing 21(1), pp. 99\u2013110, 2003.\n13. Y. Bar-Shalom and X. Li, Estimation and Tracking: Principles, Techniques and Software, Artech House,\n1993.\n14. A. Bhattacharayya, \u201cOn a measure of divergence between two statistical populations defined by their prob-\nability distributions,\u201d Bulletin of the Calcutta Mathematical Society 35, pp. 99\u2013110, 1943.\n15. D. Comaniciu, V. Ramesh, and P. Meer, \u201cKernel-based object tracking,\u201d IEEE Transactions on Pattern\nAnalysis and Machine Intelligence 25(5), pp. 564\u2013577, 2003.\n16. R. Porter, Texture Classification and Segmentation, PhD Thesis, University of Bristol, Center for Commu-\nnications Research, 1997.\n17. E. J. Stollnitz, T. D. DeRose, and D. H. Salesin, \u201cWavelets for computer graphics: A primer part 1,\u201d IEEE\nComputer Graphics and Application 15, pp. 76\u201384, May 1995.\n18. E. J. Stollnitz, T. D. DeRose, and D. H. Salesin, \u201cWavelets for computer graphics: A primer part 2,\u201d IEEE\nComputer Graphics and Application 15, pp. 75\u201385, July 1995.\n19. J. H. Kotecha and P. M. Djuric\u00b4, \u201cGaussian sum particle filtering,\u201d IEEE Transactions on Signal Processing\n51, pp. 2602\u20132612, Oct. 2003.\n20. J. H. Kotecha and P. M. Djuric\u00b4, \u201cGaussian sum particle filtering for dynamic state space models,\u201d in\nProceedings of the ICASSP, (Salt Lake City, Utah, USA), 2001.\n21. J. H. Kotecha and P. M. Djuric\u00b4, \u201cGaussian particle filtering,\u201d IEEE Transactions on Signal Processing 51,\npp. 2592\u20132601, Oct. 2003.\n22. P. Brodatz, Textures: A Photographic Album for Artists and Designers, Dover Publications, 1966.\n23. Y. Bar-Shalom, X. R. Li, and T. Kirubarajan, Estimation with Applications to Tracking and Navigation,\nJohn Wiley & Sons, 2001.\n"}