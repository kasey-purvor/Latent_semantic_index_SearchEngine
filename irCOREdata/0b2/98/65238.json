{"doi":"10.1080\/01411920903018182","coreId":"65238","oai":"oai:dro.dur.ac.uk.OAI2:6405","identifiers":["oai:dro.dur.ac.uk.OAI2:6405","10.1080\/01411920903018182"],"title":"The assessment revolution that has passed England by : Rasch measurement.","authors":["Panayides, P.","Robinson, C.","Tymms, P."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-08-01","abstract":"Assessment has been dominated by Classical Test Theory for the last half century although the radically different approach known as Rasch measurement briefly blossomed in England during the 1960s and 1970s. Its open development was stopped dead in the 1980s, whilst some work has continued almost surreptitiously. Elsewhere Rasch has assumed dominance. The purpose of this article is to discuss the major criticisms of the Rasch model, which led to its rejection by some, and to give responses to these criticisms whilst encouraging social scientists to appreciate its strengths. The original breakthrough by Georg Rasch in 1960 has been developed and extended to address every reasonable observational situation in the social sciences","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/65238.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/6405\/1\/6405.pdf","pdfHashValue":"edab2de48d6580f58cc7e5dbba60e5c84f9c98f7","publisher":"Routledge","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:6405<\/identifier><datestamp>\n      2016-07-06T12:48:50Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        The assessment revolution that has passed England by : Rasch measurement.<\/dc:title><dc:creator>\n        Panayides, P.<\/dc:creator><dc:creator>\n        Robinson, C.<\/dc:creator><dc:creator>\n        Tymms, P.<\/dc:creator><dc:description>\n        Assessment has been dominated by Classical Test Theory for the last half century although the radically different approach known as Rasch measurement briefly blossomed in England during the 1960s and 1970s. Its open development was stopped dead in the 1980s, whilst some work has continued almost surreptitiously. Elsewhere Rasch has assumed dominance. The purpose of this article is to discuss the major criticisms of the Rasch model, which led to its rejection by some, and to give responses to these criticisms whilst encouraging social scientists to appreciate its strengths. The original breakthrough by Georg Rasch in 1960 has been developed and extended to address every reasonable observational situation in the social sciences.<\/dc:description><dc:publisher>\n        Routledge<\/dc:publisher><dc:source>\n        British educational research journal, 2010, Vol.36(4), pp.611-626 [Peer Reviewed Journal]<\/dc:source><dc:date>\n        2010-08-01<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:6405<\/dc:identifier><dc:identifier>\n        issn:0141-1926<\/dc:identifier><dc:identifier>\n        issn: 1469-3518<\/dc:identifier><dc:identifier>\n        doi:10.1080\/01411920903018182 <\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/6405\/<\/dc:identifier><dc:identifier>\n        http:\/\/dx.doi.org\/10.1080\/01411920903018182 <\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/6405\/1\/6405.pdf<\/dc:identifier><dc:rights>\n        This is an electronic version of an article published in Panayides, P. and Robinson, C. and Tymms, P. (2010) 'The assessment revolution that has passed England by : Rasch measurement.', British educational research journal., 36 (4). pp. 611-626.\\ud\nBritish educational research journal is available online at: http:\/\/www.informaworld.com\/smpp\/content~db=all?content=10.1080\/01411920903018182<\/dc:rights><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["issn: 1469-3518","0141-1926"," 1469-3518","issn:0141-1926"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2010,"topics":[],"subject":["Article","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n01 March 2011\nVersion of attached file:\nAccepted Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nPanayides, P. and Robinson, C. and Tymms, P. (2010) \u2019The assessment revolution that has passed England by\n: Rasch measurement.\u2019, British educational research journal., 36 (4). pp. 611-626.\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1080\/01411920903018182\nPublisher\u2019s copyright statement:\nThis is an electronic version of an article published in Panayides, P. and Robinson, C. and Tymms, P. (2010) \u2019The\nassessment revolution that has passed England by : Rasch measurement.\u2019, British educational research journal., 36 (4).\npp. 611-626. British educational research journal is available online at:\nhttp:\/\/www.informaworld.com\/smpp\/content db=all?content=10.1080\/01411920903018182\nAdditional information:\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\n 1 \nBritish Educational Research Journal \nVol. X, No. X, Month 200X, pp. 000-000 \n \nThe Assessment Revolution that has passed England by: \nRasch Measurement \n \nPanayiotis Panayides\n*\n, Colin Robinson\n#\n & Peter Tymms\n*\n \n*\nDurham University, UK \n# \nFreelance \n \n \n \nCorresponding Author:  \nPeter Tymms \nCEM Centre \nUniversity of Durham \nMountjoy Research Centre 4 \nStockton Road, Durham DH1 3UZ \nUK \nEmail: Peter.Tymms@cem.dur.ac.uk \n \n 2 \n \n \n \n \nThe Assessment Revolution that has passed England by: \nRasch Measurement \n 3 \nAbstract \n \nKeywords:  \nAssessment has been dominated by Classical Test Theory for the last half century although \nthe radically different approach known as Rasch measurement briefly blossomed in England \nduring the 1960s and 70s. Its open development was stopped dead in the 80s whilst some \nwork has continued almost surreptitiously. Elsewhere Rasch has assumed dominance. The \npurpose of this article is to discuss the major criticisms of the Rasch model, which led to its \nrejection by some, and to give responses to these criticisms whilst encouraging social \nscientists to appreciate its strengths. The original breakthrough by Georg Rasch in 1960 has \nbeen developed and extended to address every reasonable observational situation in the social \nsciences. \n \n \n \n \n \n \n \n \n \n \n \n 4 \nIntroduction \nThis paper starts with an historical perspective of assessment developments in England in the \n70s and 80s. This outlines how traditional approaches to the analysis of test data were shown \nto be inadequate for the purposes at hand and how a new methodology was adopted and \nextended. This new approach was stopped abruptly but was continued elsewhere. The paper \nthen outlines the theoretical basis of the Classical and revolutionary approaches and goes on \nto examine the criticisms which led to the abandonment of the Rasch approach to \nmeasurement. \nDevelopments in the 70s and 80s \nIn the 1970s in the UK, there was great interest in the evaluation of the effectiveness of the \neducation system and particularly trends of performance over time. Since the 1940s, reading \nhad been assessed on a regular basis using unchanging standardised reading tests. Although \nfor many years the pupils\u201f scores on the test seemed to rise, in the early part of the decade \nthey appeared to have declined. An investigation by Start and Wells (1972) suggested that the \nchange might be caused by the test becoming dated and therefore results were no longer \ncomparable. The use of such tests to show trends over long periods of time was called into \nquestion and new ways of monitoring the system were needed. \n \nThe response was the establishment of the Assessment of Performance Unit (APU) which \nargued that, in order to discover what was being taught and how effectively it was being \nlearnt \u2013 \u201ca broad balanced picture of pupils\u201f performance\u201d (APU 1979), it would be necessary \nto have extensive assessments. Even within a single curriculum area, these would need to \ncover a range of content \u2013 e.g. science would need to span at least biology, chemistry, and \nphysics. Similarly a wide range of different assessment types would be necessary, including \n 5 \nwritten, oral and practical tests. It was estimated that in order to cover the full range of the \nscience curriculum alone it would be necessary to have 36 hours of assessment. Obviously \nthis was impossible. What was needed was a system of assessments which could cover the \ncurriculum adequately but in which an individual student would take only a relatively small \nsubset. If these assessments could then be put together on a single scale, it would be possible \nto draw conclusions about what topics were being taught well and which were not. \n \nExperience in the USA was considered and the National Assessment of Educational Progress \n(NAEP), which had been set up in the late 60s, proved to be of particular interest. Clare \nBurstall, Deputy Director at the National Foundation for Educational Research (NFER), and \nBrian Kay, Head of the APU reported: \n \n \u201cSince NAEP had been given the task of assessing changes in the educational \nachievement of pupils and young adults in four different age groups which, \ntogether, made up a population of about 37 million, there was really never any \npossibility that a \u201eblanket\u201f approach to assessment could have been adopted. In \naddition, it had been agreed that no student in the \u201ein-school\u201f samples should be \nasked to give more than one class period of his time to the assessment \nprogramme. This meant, in effect, that no student could be given more than one \npackage of the exercises prepared for use in any given cycle of assessment.\u201d \n(Burstall and Kay 1978 p 35).  \n \nThe approach favoured by NAEP was matrix sampling \u2013 small groups of randomly selected \npupils taking small groups of test items. \n \n 6 \nThe problem was how to deal with the data. Classical Test Theory could only compare items \nif they were all taken by the same group \u2013 or very closely matched groups. To design \nequivalent tests across such a wide area would be almost impossible. \n \nThe Examinations and Tests Research Unit (ETRU) was set up at the NFER by the Schools \nCouncil in 1964 and, in 1966, a pilot study was commissioned into \u201cthe feasibility of \nestablishing banks or libraries of examination questions or items suitable for measuring the \nachievement of 16-year-olds taking examinations in various subjects\u201d. (Wood & Skurnik \n1969 p 1). The principal focus of these item banks were to be the delivery of school-based \nassessment as part of the newly devised Certificate of Secondary Education (CSE) and the \napproach proposed was based upon the procedures then used for Mode 3 examinations in \nwhich teachers devised both the syllabus and its assessment.  \n \nThe statistical analyses were based upon Classical Test Theory, but in an appendix to the \nWood and Skurnik report, Bruce Choppin described a method of arriving at sample-free \nestimates using pairwise comparisons of all the items in a test. (Choppin 1969 pp 134-140). \nThis was based upon the model proposed in 1960 by the Danish mathematician, Georg Rasch, \nwho had devised it originally for reading tests. A great deal of work on extending the model \ninto the wider educational sector was being done in the USA by Ben Wright and his \nassociates at the Measurement, Evaluation, Statistics and Assessment (MESA) unit at the \nUniversity of Chicago. Bruce Choppin was one of a number of those who took on board the \nRasch approach and disseminated it widely. Others included David Andrich and Geoff \nMasters who developed the procedures in Australia. They took the procedure much further \nthan Rasch had envisaged: \u201cI do not expect this model to hold at all if applied to items \nbelonging to different fields of mathematics.\u201d (Rasch 1969 p 100)  \n 7 \n \nThe APU had two problems for which the Rasch model was seen as a potential solution. The \nfirst was to provide a means of comparing the difficulties of items used in different contexts \nand taken by different groups of pupils across a wide range of attainments. This would give \nthe necessary information about the overall achievements of children across the curriculum as \na whole. The second was to provide a metric that would allow changes in performance to be \ncompared at different points in time. The first surveys were carried out in 1978 and were \ncontinued until 1988. \n \nIn 1978, the NFER decided to make use of the approaches developed for the APU to create a \nparallel bank of items that could be used by Local Education Authorities and schools to create \ncustom built tests that matched their own curriculum but which could also be compared with \nnational data to provide a check on comparative standards. The LEAs\u201f and Schools\u201f Item \nBank (LEASIB) was seen as a potential replacement for the numerous standardised tests. The \nfirst Head of LEASIB was Alan Willmott, previously Principal Research Officer in the \nExaminations and Tests Research Unit. Both APU and LEASIB fell under Bruce Choppin\u201fs \noverall leadership.  \n \nThe LEASIB process was simple and had been expounded in Choppin\u201fs appendix. A bank of \nitems would be developed, extensively trialed and calibrated using the Rasch model. A user \nwould be presented with a range of items in the appropriate curriculum area. Selection would \nbe based on a wide variety of features, depending on what the purpose of the test might be. \nFor example it might be appropriate to assess a particular aspect of mathematics one year and \na different one the next. Other item characteristics, such as difficulty, could be used to design \ntailored tests. It was certainly not intended to be a random selection of items but would cover \n 8 \nareas that one might expect students to have covered at that stage in their career. The \ncandidates would then take the test and their responses used to calculate an estimate of their \nabilities, but their data would also be used to refine the information stored in the bank. \n \nThe APU mathematics surveys used Rasch measurement for its first five consecutive years \nstarting in 1977. However, Choppin saw that the use of Rasch measurement was already \nunder attack: \u201cThere are also growing doubts in my mind as to whether the APU is going to \nbe allowed to monitor change except in one or two rather trivial aspects. APU activity in \nitself appears to be controversial even before we have any results. There are statisticians \nadvising the DES that monitoring performance over time is impossible \u2026\u201d (Choppin 1981). \nAs a result of the criticisms, two seminars were convened by the DES, The Rasch Model in \n1980 and Monitoring over time in 1981. Foremost amongst the critics was Harvey Goldstein \n(see for example Goldstein 1979). \n \nOne of Choppin\u201fs supporters, John M Linacre asserts \u201cUnder Choppin's supervision British \npsychometrics could have led the world (to the great benefit of British students, teachers, and \npolicy makers). Instead the entrenched interests condemned Britain to a 60 year regression.\u201d \n(Linacre 1995).  \n \nThe rejection of Rasch measurement within NFER caused great concern amongst staff who \nwrote to the NFER Board complaining that insufficient attention was being given to its own \nstaff \u2013 particularly Tony James, its chief statistician, and Alan Willmott. Nevertheless, both \nthe NFER and the APU bowed under the pressure and Rasch was abandoned as a means of \ntracking changes over time although it did continue to be used to link data within years. Bruce \nChoppin resigned in 1981 and Alan Willmott the following year. LEASIB was discontinued, \n 9 \nthough its bank of items continued to be used by the NFER-Nelson Publishing Company as \nthe basis of custom made tests. Meanwhile Rasch measurement continued to thrive in other \nparts of the world, notably Australia and the USA, where its theoretical base was considerably \nexpanded. The major international assessments (TIMSS, PIRLS and PISA) all use Rasch \nmeasurement or some Item Response Theory approach. \n \nSuch was the impact of the 1981 events that the British Educational Research Journal has not \npublished a paper mentioning Rasch since Preece (1980). \n \nThe two testing theories \nFor much of the middle part of the twentieth century Classical Test Theory dominated the \napproach to testing across the world although it was well know that there were problems with \nit. A number of different approaches were developed. Amongst them Item Response Theory \n(IRT) was of major importance and in its simplest form, the so-called one parameter model \nequates to the approach taken by Rasch. But whilst Rasch purists think in terms of creating \ninstruments for measurement others think in terms of modeling the data using IRT. \n \nIn the following sections the various approaches are outlined. This includes a description of \nthe distinct approach taken by Rasch measurement and an outline of the criticisms leveled it \ntogether with responses to those criticisms. \n 10 \nClassical Test Theory (CTT) \nCTT starts with the model, X = T + e, where X is the observed score of an examinee on the \ntest, T the true score (which is conceptualized as the hypothetical average score resulting from \nmany repetitions of the test or alternate forms of the instrument) and e the error. \n \nThe model has the following assumptions:  \n(i) T is constant, changes in X are due to error  \n(ii) Errors are random and they do not correlate with T or with each other. \n \nThese assumptions together with the theoretical definition that: reliability is the proportion of \nvariation in observed scores attributable to true scores (i.e. xxr  = variance of true \nscores\/variance of observed scores) have led to the formulae for the reliability and the \nstandard error of measurement: \n2\n2\n1\nx\nxx\nS\nS\nr \uf065\uf02d\uf03d    and   SEM = xxx rS \uf02d1 , where \n2\nxS  is the variance of the group\u201fs \nobserved scores, \n2\n\uf065S  is the error variance and SEM is the standard error of measurement. \n \nIn item analysis, psychometricians employing CTT use two basic indices, item difficulty and \nitem discrimination. Item difficulty is calculated by dividing the mean score of the item by the \nmaximum possible score. If items have only one correct answer, which is worth one mark, \nthen this index represents the percentage of examinees responding correctly.  \n \nThe index of discrimination (D) for any item is the difference of the averages of two groups \nof examinees (the high and the low scorers) for the specific item, divided by the maximum \n 11 \npossible score on the item. The precise composition of the two groups varies from study to \nstudy but the basic definition remains. \n \nThe item-total correlation coefficient can also be used as an index of discrimination. The \nhigher the correlation between the scores on a particular item and the total score on all other \nitems, the better discriminator the item is.  \n \nHambleton, Swaminathan and Rogers (1991) identify the following limitations of CTT: \n\uf0b7 Ability scores of individuals are item dependent (i.e. they depend on the item \ndifficulties). \n\uf0b7 The item statistics (difficulty, discrimination, reliability) are examinee dependent. \nDiscrimination indices as well as reliability estimates tend to be higher in \nheterogeneous examinee groups than in homogeneous ones. \n\uf0b7 No information is available about how examinees of specific abilities might perform \non a certain test item. \n\uf0b7 Equal measurement error is assumed for all examinees (this measurement error is item \ndependent). \n\uf0b7 Classical item indices are not invariant across subpopulations (i.e. different subgroups \nof the sample of examinees give different item statistics). \nFurther, as Anastasi and Urbina (1997) note: \n\u201cItem difficulty clearly depends on the ability of the group of test takers. This \naffects also the distribution of scores. In high ability groups the distribution is \nnegatively skewed whereas in low ability groups it is positively skewed. It is \npreferable to add\/revise or delete items so that the score distribution in the \ntarget group is approximately Normal\u201d. (pp 177-178) \n 12 \nItem Response Theory (IRT) \nIRT provides alternative models to CTT with the following desirable features: \n\uf0b7 Item characteristics are not group dependent. \n\uf0b7 Scores describing examinees\u201f abilities are not test dependent. \n\uf0b7 A measure of precision for each ability score is produced. \n\uf0b7 The probability that an examinee of any ability will answer items of any difficulty \ncorrectly is estimated. \n \nAs noted earlier the simplest form of IRT corresponds to the approach taken by Rasch (1960) \nand is sometimes referred to as the one-parameter IRT model. It deals with all the issues set \nout by Hambleton et al. (1991), and has some important advantages over other IRT models, \nwhich are discussed later. \n \nThe Rasch approach \nA pupil may be given a test item which he or she could easily solve, and yet get it wrong. \nSimilarly, a pupil may be given a test item which is too difficult and yet solve it. Rasch (1960) \nsaw that \u201cWe can never know with certainty how a pupil will react to a problem, but we may \nsay whether he has a good or a poor chance of solving it\u201d (Rasch, 1960, p 11). This \nrealisation led him to shift from deterministic models to probabilistic models; ones in which \n\u201cthe possible behaviour of a pupil is described by means of a probability that he solves the \ntask\u201d (Rasch, 1960, p 11). He also saw that, the probability for a right answer must only be \ngoverned by the candidate\u201fs ability (\u03b8) and the item\u201fs difficulty (b). \n  \n\u201cThe ability of the person and the difficulty of the item must be considered to be \njoined or conjoint in all analyses of responses and a principle of relativity with \n 13 \nrespect to the item must underlie the task of measurement. This principle \novercomes the problems that were raised in earlier decades and that claimed that \nmeasurement was not possible in the social and behavioral sciences.\u201d \n      (Keeves and Alagumalai, 1999, p 25) \n \nRasch set out the following formula for dichotomously scored performances: \n DifficultyAbility\nfailureofobability\nsuccessofobability\n\uf02d\uf03d\uf0f7\uf0f7\n\uf0f8\n\uf0f6\n\uf0e7\uf0e7\n\uf0e8\n\uf0e6\nPr\nPr\nln  \nThen with simple mathematical steps he deduced the formula for a person\u201fs probability of \nscoring 1 rather than 0 on item i: \n \n)exp(1\n)exp(\n)(\ni\ni\ni\nb\nb\nP\n\uf02d\uf02b\n\uf02d\n\uf03d\n\uf071\n\uf071\n\uf071  \nwhere \u03b8 is the ability of the person and bi the difficulty of item i. \n \nRasch based his model on three key assumptions: unidimensionality, local independence and \ninvariance which are discussed later. \n \nThe basic model is applicable to tests with dichotomous items which can be marked as right \n(1) or wrong (0). But many tests and questionnaire involve items which are not simply right \nare wrong and the basic model has been extended to deal with such polytomous items.  If the \ntest has a single type of item with the same number of marks available then the Rating Scale \nModel (RSM) applies. This is widely used for the analyses of Likert scales, even though the \noriginal intention of Andrich (1978), who developed it, was to use it in the evaluation of \nwritten essays.   \n 14 \nIf the marks allocated to items vary, then the Partial Credit Model, developed by Masters \n(1982), is appropriate. Situations where the Partial Credit Model is applicable are discussed \nby Bode (2004). \n \nMore complex IRT models \nIRT models are sometimes extended to take into account the differing discriminations of each \nitem. To do this an additional parameter (\u03b1) is added to the basic equation making it a \u201ctwo-\nparameter\u201d (2-P) model. A further refinement produces the \u201cthree-parameter\u201d (3-P) model in \nwhich a guessing parameter (c), called by Hambleton et al. (1991, p 17) a  pseudo-chance-\nlevel parameter, is added.  \n \nComparing the 2-P and 3-P models with Rasch measurement \nWright (1983) argues that fundamental measurement in the social sciences is obtainable only \nthrough the Rasch approach and, in comparing Rasch with the 2-P and 3-P models, states: \n \n\u201cIf measurement is our aim, nothing can be gained by chasing extra item \nparameters like c and a. We must seek, instead, for items which can be managed \nby an observation process in which any potentially misleading disturbances \nwhich might be blamed on variation in possible c\u201fs and a\u201fs can be kept slight \nenough not to interfere with the maintenance of a scale stability sufficient for the \nmeasuring job at hand. \u2026 Only the Rasch process can maintain units that \nsupport addition and so produce results that qualify as fundamental \nmeasurement.\u201d \n         (Wright, 1983, p  7) \n \n 15 \nFurthermore, the Rasch approach is the only one which uses the raw score as the sufficient \nstatistic for estimating item difficulty or person ability. That is, the sufficient statistic for \nestimating person ability is the sum or count of the correct responses for a person over all \nitems. In the other two models the sufficient statistic for ability estimation includes other \nparameters that must be estimated simultaneously.   \n \nIn comparing the 2-parameter and 3-parameter models with the Rasch approach it is \nimportant to distinguish between measurement and modeling.  If the purpose is to construct a \ngood measure then the items and the test should be constrained to the principles of \nmeasurement. If on the other hand the purpose is to model some test data then the model \nwhich fits the data best should be chosen. Rasch corresponds to the principles of measurement \nwhereas other IRT models correspond to modeling. Fischer and Molenaar (1995) state that: \n \n\u201cThey (the 2-p and 3-p models) make less stringent assumptions (than the \nRasch model), and are therefore easier to use as a model for an existing test. \nOn the other hand, they typically pose more problems during parameter \nestimation, fit assessment and interpretation of results. Whenever possible, it is \nthus recommended to find a set of items that satisfies the Rasch model rather \nthan find an IRT model that fits an existing item set.\u201d \n      (Fischer and Molenaar, 1995, p 5) \n \nLinacre (1996) adds to the above that allowing or parameterising discrimination or guessing, \nwhich are sample dependent indices, limits the meaning of the measures to just that subset of \nitems and persons producing these particular data. This prevents any general inferences over \nall possible items probing that construct among all possible relevant persons.  \n 16 \n \nA further important difference is the sample sizes required for the calibrations. The use of the \n2-P or 3-P models requires larger samples of persons for calibrations. Thissen and Wainer \n(1982) determined the number of persons with normally distributed abilities necessary to \nproduce an item difficulty that was accurate to one decimal place (i.e. s.e = 0.05). For the \nRasch model approximately 2500 persons were needed whereas, for the 2-P model \napproximately 7500 and for the 3-P model approximately 67000.  \nApplications of the models \nRasch measurement has been applied in very diverse situations and six examples are outlined \nbelow: \n\uf0b7 Prieto, Roset and Badia (2001) have explored the Spanish version of the assessment of \nGrowth hormone deficiency in adults and confirmed its unidimensionality and \nconstruct validity using the Rasch approach. \n\uf0b7 Bond and Fox (2001) describe how data from Piagetian interviews have been analysed \nusing the Rasch approach to give fresh insights. \n\uf0b7 Massof and Fletcher (2001) have evaluated the validity of, and suggested \nimprovements to, the visual functioning questionnaire which is designed to assess \nhealth-related quality of life of patients with visual impairment.  \n\uf0b7 Chen, Bezruczko and Ryan-Henry (2006), driven by the need of health and social \nagencies to have systematic means of describing mothers\u201f effectiveness in caregiving \nfor their adult children with intellectual disabilities, have used Rasch analyses.  \n\uf0b7 Myford and Wolfe (2002) examined a procedure for identifying and resolving \ndiscrepancies in examiners\u201f ratings. \n 17 \n\uf0b7 Lamprianou (2006) investigated the stability of two marker characteristics across tests: \n(a) severity and (b) consistency of marking.  \n \nThe above selection of applications of Rasch measurement shows the diversity of situations in \nwhich this approach can be used productively over and above the usual assessments of ability \nin educational tests, the positioning of persons on the latent trait line in psychological tests \nand the identification of aberrant response patterns in tests or psychometric scales. \n \nRasch\u2019s different approach to the data-model relationship  \nAlthough the exponential models were known by the time Rasch worked with them he did not \nuse them in the traditional way. As Andrich (2004) notes, the reason that Rasch\u201fs model turns \nthe traditional data-model relationship upside down is that the model does not describe any \ndata. \u201cThe model renders in mathematical, and most importantly from a practical and applied \nprospective, testable form, the requirements of measurement\u201d (p 172). Andrich is referring to \nthe requirements of invariant comparisons and quotes Rasch (1961) summarising those \nrequirements: \n\u201cThe comparison between two stimuli should be independent of which \nparticular individuals were instrumental for the comparison; and it should also \nbe independent of which other stimuli within the considered class were or \nmight have been compared. \nSymmetrically, a comparison between two individuals should be independent of \nwhich particular stimuli within the class considered were instrumental for \ncomparison; and it should also be independent of which other individuals were \nalso compared on the same or some other occasion.\u201d (Andrich, 2004, p 173) \n 18 \n \nAndrich (2004) argues that this is fundamentally a different approach to the data-model \nrelationship. He equates the new approach to a paradigm shift of the type identified by Kuhn \n(1962) and draws parallels with other paradigm shifts and the criticisms that they drew from \n\u201cexperts\u201d at the time only to become orthodox later. \nCriticisms of the Rasch model \nGoldstein (1979) outlined several criticisms of the Rasch model as did Dickson and Kohler \n(1996) when commenting on the appropriateness of Rasch measurement being used for \ntransforming the responses of patients to the Functional Independence Measure (FIM) items \nfrom the ordinal scale to an interval one. (FIM records the severity of disability of \nrehabilitation patients). Others including Divgi (1986, 1989) Whitely and Dawis (1974) and \nWhitely (1977) have also criticised the Rasch approach. But between them, Goldstein (1979) \nand Dickson and Kohler (1996) cover the majority of the points and it was primarily \nGoldstein\u201fs criticisms that led to a severe reduction in the use of Rasch in the UK. \n \nThe major criticisms are outlined below and discussed.  \nCriticism 1: Unidimensionality \nGoldstein\u201fs (1979) first criticism, and probably the most frequently occurring one, refers to \nthe assumption of unidimensionality and more precisely to the fact that in order to fit the \nRasch model the items must \u201crelate only to one underlying dimension of ability\u201d (p 214). He \ndifferentiates the Rasch approach from factor analysis (as methods for detecting the \ndimensionality of data) noting that in factor analysis \u201cthe dimensionality or number of factors \n 19 \nis studied in the analysis itself\u201d (p 214), implying the superiority of factor analysis. Dickson \nand Kohler (1996) also criticise the requirement of a one-dimensional latent space. \nResponse to criticism 1 \nSince Goldstein\u201fs article, many  psychometricians (see for example Hambleton et al., 1991; \nKeeves and Masters, 1999; Smith, 2004; Wright and Linacre, 1989) have made it clear that \nunidimensionality does not implicitly mean only one factor or dimension but rather the \npresence of a dominant dimension with the possible presence of minor dimensions.  \n \nHambleton (1993) writes \u201cthe unidimensionality assumption cannot be strictly met because \nthere are always other cognitive, personality and test-taking factors that affect test \nperformance, at least to some extent\u201d (p 150). Possible factors include test motivation, test \nanxiety, speed of performance, test sophistication, reading proficiency and other cognitive \nskills.  \n \n Linacre (1998) concurs noting that the presence of more than one dimension in the data does \nnot necessarily imply substantive multidimensionality. Extra dimensions may reflect different \nperson response styles or different item content area. For example, items on subtraction may \ndefine a different dimension than items on addition in a simple mathematics test for young \nchildren. Multidimensionality can also be an artifact of test construction. For example, \nincluding the identical item several times in a test produces a subset of highly intercorrelated \nitems which may define an extra dimension. On the other hand, the use of different response \nmechanisms across items (multiple-choice, constructed-response, rating scales) introduces \nunmodeled variation which can be attributed to a dimension of \u201eitem type\u201f.  \n 20 \nMultidimensionality becomes a real concern when the response patterns indicate the presence \nof two or more dimensions so disparate that it is no longer clear what latent dimension the \nRasch dimension operationalises. \n \nAs far as factor analysis is concerned, Linacre (1998) showed that Rasch analysis followed by \nprincipal components analysis of standardized residuals was always more effective at both \nconstructing measures and identifying multidimensionality than direct factor analysis of the \noriginal response-level data.  \n \nPrincipal components analysis of the standardized residuals is based on the specification of \n\u201elocal independence\u201f, which is an assumption of the Rasch model. This asserts that, after the \ncontribution of the measures to the data has been removed, what is left is random, normally \ndistributed noise. Therefore the standardized residuals are modeled to have unit normal \ndistributions which are independent and so uncorrelated. This is testable. If the resulting \ncommon factors explain nothing more than random noise across items, then the data conform \nto the Rasch model. The existence of substantive common factors, however, would indicate \ndeparture from unidimensionality. \n \nCriticism 2: The use of probabilities \nDickson and Kohler (1996), in listing the shortcomings of the Rasch model, claim that \u201eany \nsystem of measurement based on probabilities must necessarily be imprecise\u201f (p 161). \n 21 \nResponse to criticism 2 \nAll measurement is made with error and an explicit acknowledgement that this is so can allow \nthe researcher to express test success in probability terms. The Rasch model does not \nintroduce probabilities or imprecision into the data, on the contrary, it capitalises on their \npresence in the data to construct a measurement system. \n \nCriticism 3: The absence of distributional descriptions \nDickson and Kohler (1996) criticize also the fact that no description of the sample distribution \nexists in Rasch analysis. \nResponse to criticism 3 \nThe Rasch model does not need to assume anything about the distribution of the sample. This \nis a strength and means that it can reveal the underlying distribution and is not dependent on \nassumptions about hypothesised distributions. \nCriticism 4: Constancy of item difficulties \nGoldstein (1979) refers to the fact that the relative difficulty of the items in a test is the same \nfor all individuals. He states: \u201cHence, even if we were satisfied that a test tapped only one \ndimension of ability, in order to use the Rasch model we would also require that, despite \ndifferent experiences, learning sequences etc., the difficulty order of items was the same for \nevery individual\u201d (p 214), implying that because of different experiences, learning sequences \netc. the difficulty order could not be the same for everyone. \n \nDickson and Kohler (1996) also criticise the assumption. \n 22 \nResponse to criticism 4 \nBoth Goldstein and Dickson and Kohler are referring to the property of invariance. This basic \nprinciple of order (or invariance) is not only an assumption of the Rasch model, but also the \nfundamental requirement for measurement. \n \nLinacre (1996) argues that this is a virtue and not a flaw of the model. \n\u201cConstant item parameters imply a constant construct. Different item parameters \nacross samples of the relevant population imply that the construct has changed. \nThen measures cannot be compared across samples, and we are reduced to a \nvague notion of what we are measuring.\u201d (Linacre, 1996, p 513) \n \nRasch, was not the first to require the same kind of invariance in social measurement. L. L. \nThurnstone and L. Guttman, two of the most significant people in this field, both articulated \nthis requirement and according to Andrich (2004) \n\u201cThis leads to another reason that the Rasch models can be subtle. Because the \nproperty of invariance is built into a mathematical model, it is possible to study \nthe consequences of the requirements of invariance by mathematical \nderivations.\u201d  (Andrich, 2004, p 174) \n \nAlthough invariance is a requirement of Rasch models, and of measurement, it is not an \nassumption for an analysis, in that one can test its veracity.  \nCriticism 5: Local independence \nA different criticism refers to the assumption of local independence, which according to \nGoldstein (1979, p 214) means that \u201cfor any individual, the response to an item is completely \n 23 \nindependent of his or her response to any other item\u201d, again implying that this is not easy to \nfind in practice. \nResponse to criticism 5 \nWhat the assumption essentially means is that previous items should not give hints, clues, \ninsights or guidance for the solution of other items. Such an assumption is more like common \nsense, and can easily be met by experienced test constructors.  Athanasou and Lamprianou \n(2002), give an example of an item with sub-questions in simple arithmetic calculations. \n \n\u201cThere are 18 flowers in John\u201fs garden.  \n(a) If he plants 6 flowers more, how many flowers will there be in total? Answer \u2026\u2026\u2026. \n(b) If you need double the number of flowers, how many flowers will you need?  \nAnswer \u2026\u2026\u2026\u2026\u201d \n \nThese two parts of the item cannot be treated as different and independent. If a pupil is not in a \nposition to find the answer to the first part, he\/she will not find the answer to the second part \neven if he\/she is able to double a number correctly. This would be a valid criticism of an item \nbanking system in which items are randomly selected for the test. However, with test \nconstructors involved in the development, this is one aspect that would be checked. \nCriticism 6: Symmetry between items difficulties and individual abilities \nGoldstein (1979) also notes that the Rasch model \u201cseems to imply a symmetry between item \ndifficulties and individual abilities \u2026 In reality, however, this is not the case\u201d (p 215) \n 24 \nResponse to criticism 6 \nThis appears to be a misunderstanding by Goldstein. The reference is presumably to the \ngraphical representation known as the item-person map which often appears to be \nsymmetrical. But the Rasch approach does not require such symmetry.  \nCriticism 7: Items need to be equally discriminating \nDickson and Kohler (1996) refer to the assumption that the Rasch model requires items to \nhave equal discriminating power. An extension to that is Goldstein\u201fs (1979) argument that \nintroducing a discrimination parameter makes the model more flexible and it is no longer \nnecessary to have a constant relative difficulty between items. Although he acknowledges the \nincrease in \u201etechnical problems\u201f he states that \u201cBecause of its greater flexibility we can expect \nthe model to have a better chance than model (3) (the Rasch model) of fitting a set of test \nscores.\u201d (Goldstein, 1979, p 215) \nResponse to criticism 7 \nAs noted earlier the aim of measurement should not be to accommodate the test data but to \nsatisfy the requirements of measurement. The aim is to measure, not to model. The 2-P model, \nwhich introduces a discrimination parameter, seeks to fit a model to the data not vice versa.  \n \nRasch measurement needs items to have discriminations that are equal enough to be regarded \nas the same. In practice, according to Linacre (1996), unequal discrimination is diagnostic of \nvarious types of item malfunction and misinformation. Allowing or parameterising \ndiscrimination, which is a sample-dependent index, limits the meaning of the measures to just \nthat subset of items and persons producing this particular set of data. This prevents any general \ninferences over all possible items probing that construct among all possible relevant persons. \n 25 \nCriticism 8: The model is not perfect \nDickson and Kohler (1996) criticise the Rasch model in that no item fits the model \nexactly. \nResponse to criticism 8 \nThe idea that the world is not perfect is not new. We use circles to approximate all sorts of \nround shapes and straight lines to describe objects that are not perfectly straight. If we were to \nstop investigations when things were not perfect we would do nothing.   \n \nA nice way of viewing the criticism comes from Andrich\u201fs (2004) paper where he argues that \nthe Rasch approach, instead of simply describing data, provide the opportunity to understand \ndata by the exposure of anomalies which is the prime function of measurement. The reason \nwhy the approach can be used this way is that it formalizes conditions of invariance, which \nlead to properties of measurement. Thus, when the data deviate from the Rasch it deviates \nfrom the requirements of measurement. \n \nSimilarly Linacre (1996) does not see non-fitting data as a criticism of Rasch measurement but \nof the data. He concludes (p 512) that \u201cusually, if the data have any meaning at all, they can \nbe segmented into meaningful subsets that do fit the Rasch model and do support inferences\u201d, \nimplying that even if the data are not unidimensional, when grouped appropriately (separating \nthe dimensions) they will separately fit the Rasch model.  \nCriticism 9: All people do not fit the model \nWith regard to the persons\u201f response patterns and whether meaningful inferences can be made \nfrom these response patterns, Dickson and Kohler (1996) comment that they have seen people \n 26 \nwho could climb stairs (success on a difficult item) but not being able to swallow (failing an \neasy item). The implied question in their argument is \u201ehow can one make a meaningful \ninference from such a performance?\u201f \nResponse to criticism 9 \nAgain, when data do not fit the model they provide interesting anomalies to be investigated \nand to challenge the supposed scale. These anomalies are predicted by the Rasch approach to \noccur occasionally. \nConcluding remarks \nThe Rasch approach has turned the traditional relationship between data and analysis upside \ndown. To consider blaming the data rather than the model when there is a mismatch between \nthem is a considerable shift from the traditional, statistical way of thinking. Most of the \ncriticisms of the model have originated from this new approach to the data-model \nrelationship. \n \nWright and Mok (2004) state that in order to construct inferences from observation a model \nwith certain characteristics should be used. It must: \n\uf0b7 Produce linear measures \n\uf0b7 Overcome missing data \n\uf0b7 Give estimates of precision \n\uf0b7 Have devices of detecting misfit, and \n\uf0b7 The parameters of the object being measured and of the measurement instrument must \nbe separable. \n \n 27 \nOnly the family of Rasch measurement models does this.  \nFinally we quote, as does Linacre (1996), from a New York Times Editorial writing about a \ntheory of corporate finance: \n\u201cThat is the true test of a brilliant theory, says a member of the Nobel Economics \ncommittee. What first is thought to be wrong is later shown to be obvious. People \nsee the world as they are trained to see it, and resist contrary explanations. That\u201fs \nwhat makes innovation unwelcome and discovery almost impossible. \nAn important scientific innovation rarely makes its way by gradually winning \nover and converting its opponents. \u2026 What does happen is that its opponents \ngradually die out and that the growing generation is familiarised with the (new) \nidea from the beginning. No wonder that the most profound discoveries are often \nmade by the young or the outsider, neither of whom has yet learned to ignore the \nobvious or live with the accepted wisdom.\u201d \n      \u201cNaked Orthodoxy\u201d (October 17, 1985) \n \n 28 \nReferences \nAnastasi, A. & Urbina, S. (1997) Psychological Testing, 7\nth\n ed. (New Jersey, Prentice \nHall). \nAndrich, D. (1978) A rating formulation for ordered response categories. \nPsychometrika, 43, 561 \u2013 573.  \nAndrich, D. (2004) Understanding resistance to the data-model relationship in Rasch\u201fs \nparadigm: A reflection for the next generation, in: E. V. Smith, Jr and R. M. Smith (Eds) \nIntroduction to Rasch measurement (Minnesota, JAM Press).  \nAPU (1979) Science Progress Report 1977-78. Assessment of Performance Unit, \nLondon.  \nAthanasou, J. & Lamprianou, I. (2002) A teacher\u201fs guide to assessment (Sydney, \nSocial Science Press).  \nBode, R. K. (2004) Partial Credit Model and Pivot Anchoring, in: Smith, E. V and \nSmith, R. M. (Eds) Introduction to Rasch Measurement (pp. 279-295) (Minnesota: JAM \nPress).  \nBond, T.G. & Fox, C. M. (2001) Applying the Rasch Model: Fundamental \nMeasurement in the Human Sciences (New Jersey, Lawrence Erlbaum Associates). \nBurstall, C. and Kay, B. (1978) Assessment \u2013 the American experience. Assessment of \nPerformance Unit (Assessment of Performance Unit, London) \nChen, S. P. C., Bezruczko, N. & Ryan-Henry (2006) Rasch analysis of a new \nconstruct: Functional caregiving for adult children with intellectual disabilities. Journal of \nApplied Measurement, 7(2), 141 \u2013 159. \n 29 \nChoppin, B. H. (1969) An Item bank Using Sample-free Calibration, in: R. Wood,  \nand L. S. Skurnik,  (Eds) Item Banking. (Slough, NFER) \nChoppin, B. (1981) \u201cIs Education Getting Better?\u201d BERA Presidential Address, \nUniversity College, Cardiff, 1980, British Educational Research Journal 7(1), 1981 \nDickson, H. G. & Kohler, F. (1996) The multi-dimensionality of the FIM motor items \nprecludes an interval scaling using Rasch analysis. Scandinavian Journal of Rehabilitation \nMedicine. 26, 159 \u2013 162. \nDivgi, D. R. (1986) Does the Rasch model really work for multiple choice items? Not \nif you look closely. Journal of Educational Measurement. 23 (4), 283 \u2013 298. \nDivgi, D. R. (1989) Reply to Andrich and Henning. Journal of Educational \nMeasurement. 26 (3), 295 \u2013 299. \nFischer, G. H. & Molenaar, I. W (1995) Rasch Models. Foundations, Recent \ndevelopments and Applications. (New York, Springer-Verlag New York Inc.).  \nGoldstein, H. (1979). Consequences of using the Rasch model for educational \nassessment. British Educational Research Journal. 5(2), 211 \u2013 220. \nHambleton, R. K. (1993) Principles and selected applications of Item Response \nTheory, in: R.L Linn, (Ed), Educational Measurement. (3\nrd\n ed.) 13-104. (Phoenix, Oryx \nPress). \nHambleton, R. K., Swaminathan, H. & Rogers, H. J. (1991) Fundamentals of Item \nResponse Theory.  (California, SAGE Publications, Inc). \nKeeves, J. P., & Alagumalai, S. (1999) New approaches to measurement, in: G. N. \nMasters and J.P. Keeves (Eds) Advances in measurement in educational research and \nassessment (Amsterdam, Pergamon). \n 30 \nKeeves, J. P., & Masters, G. N. (1999) Issues in educational measurement, in: G. N. \nMasters and J.P. Keeves (Eds) Advances in measurement in educational research and \nassessment (Amsterdam, Pergamon). \nKuhn, T (1962) The Structure of Scientific Revolutions University of Chicago Press  \nLamprianou, I. (2006) The stability of marker characteristics across tests of the same \nsubject and across subjects, Journal of Applied Measurement, 7(2), 195 \u2013 205. \nLinacre, J.M. (1995) Bruce Choppin: Visionary  Available online at: \nhttp:\/\/www.rasch.org\/rmt\/rmt84e.htm    (accessed 19 November 2007) \nLinacre, J. M. (1996) The Rasch Model cannot be \u201cDisproved\u201d.  Rasch Measurement \nTransactions, 10(3) 512-514.  \nLinacre, J. M. (1998) Detecting multidimensionality: Which residual data-type works \nbest? Journal of Outcome Measurement, 2(3), 266-283. \nMassof, R. W. & Fletcher, D. C. (2001) Evaluation of the NEI visual functioning \nquestionnaire as an interval measure of visual ability in low vision, Vision Research, 41(3), \n397 \u2013 413. \nMasters, G. N. (1982) A Rasch model for partial credit scoring. Psychometrika, 47, \n149-174.  \nMyford, C. M. & Wolfe, E. W. (2002) When raters disagree, then what: Examining a \nthird rating discrepancy resolution procedure and its utility for identifying unusual patterns of \nratings, Journal of Applied Measurement, 3(3), 300 \u2013 324.  \nPreece, P. (1980). On rashly rejecting Rasch: a response to Goldstein. British \nEducational Research Journal 6(209-211). \n 31 \nPrieto, L., Roset, M. & Badia, X. (2001) Rasch measurement in the Assessment of \nGrowth hormone Deficiency in adult patients, Journal of Applied Measurement, 2(1), 48 \u2013 64. \nRasch, G. (1960) Probabilistic models for some intelligence and attainment tests. \n(Reprinted in 1980 with a forward and afterward by Benjamin D. Wright) (Chicago, MESA \nPress).  \nRasch, G (1969) personal communication quoted in: R. Wood and L. S. Skurnik,  \n(1969) Item Banking. (Slough, NFER) \nSmith, Jr., E. V. (2004) Detecting and evaluating the impact of multidimensionality \nusing item fit statistics and principal components analysis of residuals, in: E. V Smith Jr, and \nR. M. Smith (Eds) Introduction to Rasch Measurement (Minnesota, JAM Press).  \nStart, K.B. & Wells, B.K. (1972) The Trend of Reading Standards,  NFER. \nThissen, D. & Wainer, H. (1982) Some standard errors in item response theory. \nPsychometrika, 47, 397 \u2013 412.  \nWhitely, S. E. & Dawis, R. V. (1974) The nature of the objectivity with the Rasch \nmodel. Journal of Educational Measurement. 11(3), 163 \u2013 178. \nWhitely, S. E. (1977) Models, meaning and misunderstandings: some issues in \napplying Rasch\u201fs theory. Journal of Educational Measurement. 14(3), 227 \u2013 235. \nWillmott, A.S. & Fowles, D.E. (1974) The objective interpretation of test performance \nthe Rasch model applied. (Windsor, NFER Publishing Co Ltd).  \nWood, R. and Skurnik, LS. (1969) Item Banking A method for producing school-\nbased examinations and nationally comparable grades. (NFER, Slough).  \nWright, B. D. (1977) Solving measurement problems with the Rasch model. Journal \nof Educational Measurement, 14(2), 97 \u2013 116. \n 32 \nWright, B.D. (1983) Fundamental measurement in social science and education. \nMESA Psychometric Laboratory. Available online at http:\/\/www.rasch.org\/memo33a.htm \n(accessed 24 October 2006) \nWright, B. D. (1989) Rasch model for counting right answers: Raw scores as \nsufficient statistics. Rasch Measurement Transactions, 3(2), 62. Available online at \nhttp:\/\/www.rasch.org\/rmt\/rmt32e.htm   (accessed 28 October 2006)    \nWright, B. D. & Linacre, J. M. (1989) Observations are always ordinal; \nmeasurements, however, must be interval. Archives of Physical Medicine and Rehabilitation, \n70, 857-860. Available online at http:\/\/www.rasch.org\/memo44.htm  (accessed 24 October \n2006) \nWright, B. D. & Mok, M. M. C (2004) An Overview of the Family of Rasch \nMeasurement Models. In E. V. Smith Jr. and R. M. Smith. Introduction to Rasch \nMeasurement (Minnesota, JAM Press).  \n \n"}