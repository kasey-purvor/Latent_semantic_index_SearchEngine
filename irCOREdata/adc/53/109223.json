{"doi":"10.1177\/0894439309332305","coreId":"109223","oai":"oai:aura.abdn.ac.uk:2164\/2197","identifiers":["oai:aura.abdn.ac.uk:2164\/2197","10.1177\/0894439309332305"],"title":"e-Social Science and Evidence-Based Policy Assessment : Challenges and Solutions","authors":["Edwards, Peter","Farrington, John H.","Mellish, Chris","Philip, Lorna J.","Chorley, Alison H.","Hielkema, Feikje","Pignotti, Edoardo","Reid, Richard","Polhill, J. Gary","Gotts, Nick M."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":["University of Aberdeen, Natural & Computing Sciences, Computing Science","University of Aberdeen, PolicyGrid II","University of Aberdeen, Geosciences, Geography & Environment"],"datePublished":"2009-11-01","abstract":"Peer reviewedPreprin","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":null,"rawRecordXml":"<record><header><identifier>\n        \n            \n                oai:aura.abdn.ac.uk:2164\/2197<\/identifier><datestamp>\n                2018-01-02T00:03:35Z<\/datestamp><setSpec>\n                com_2164_320<\/setSpec><setSpec>\n                com_2164_319<\/setSpec><setSpec>\n                com_2164_318<\/setSpec><setSpec>\n                com_2164_673<\/setSpec><setSpec>\n                com_2164_370<\/setSpec><setSpec>\n                com_2164_331<\/setSpec><setSpec>\n                com_2164_687<\/setSpec><setSpec>\n                com_2164_372<\/setSpec><setSpec>\n                com_2164_705<\/setSpec><setSpec>\n                col_2164_321<\/setSpec><setSpec>\n                col_2164_674<\/setSpec><setSpec>\n                col_2164_688<\/setSpec><setSpec>\n                col_2164_706<\/setSpec>\n            <\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:doc=\"http:\/\/www.lyncode.com\/xoai\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n            \ne-Social Science and Evidence-Based Policy Assessment : Challenges and Solutions<\/dc:title><dc:creator>\nEdwards, Peter<\/dc:creator><dc:creator>\nFarrington, John H.<\/dc:creator><dc:creator>\nMellish, Chris<\/dc:creator><dc:creator>\nPhilip, Lorna J.<\/dc:creator><dc:creator>\nChorley, Alison H.<\/dc:creator><dc:creator>\nHielkema, Feikje<\/dc:creator><dc:creator>\nPignotti, Edoardo<\/dc:creator><dc:creator>\nReid, Richard<\/dc:creator><dc:creator>\nPolhill, J. Gary<\/dc:creator><dc:creator>\nGotts, Nick M.<\/dc:creator><dc:contributor>\nUniversity of Aberdeen, Natural & Computing Sciences, Computing Science<\/dc:contributor><dc:contributor>\nUniversity of Aberdeen, PolicyGrid II<\/dc:contributor><dc:contributor>\nUniversity of Aberdeen, Geosciences, Geography & Environment<\/dc:contributor><dc:subject>\neScience<\/dc:subject><dc:subject>\nnatural language<\/dc:subject><dc:subject>\nprovenance<\/dc:subject><dc:subject>\nsemantic web<\/dc:subject><dc:subject>\nevidence-based policy<\/dc:subject><dc:subject>\nH Social Sciences<\/dc:subject><dc:subject>\nH<\/dc:subject><dc:description>\nPeer reviewed<\/dc:description><dc:description>\nPreprint<\/dc:description><dc:date>\n2011-12-05T16:26:02Z<\/dc:date><dc:date>\n2011-12-05T16:26:02Z<\/dc:date><dc:date>\n2009-11-01<\/dc:date><dc:type>\nJournal article<\/dc:type><dc:identifier>\nEdwards , P , Farrington , J H , Mellish , C , Philip , L J , Chorley , A H , Hielkema , F , Pignotti , E , Reid , R , Polhill , J G & Gotts , N M 2009 , ' e-Social Science and Evidence-Based Policy Assessment : Challenges and Solutions ' Social Science Computer Review , vol 27 , no. 4 , pp. 553-568 . DOI: 10.1177\/0894439309332305<\/dc:identifier><dc:identifier>\n0894-4393<\/dc:identifier><dc:identifier>\nPURE: 1593062<\/dc:identifier><dc:identifier>\nPURE UUID: 25a63526-2458-4226-8046-0282ed91f508<\/dc:identifier><dc:identifier>\nScopus: 70350511860<\/dc:identifier><dc:identifier>\nhttp:\/\/hdl.handle.net\/2164\/2197<\/dc:identifier><dc:identifier>\nhttp:\/\/dx.doi.org\/10.1177\/0894439309332305<\/dc:identifier><dc:language>\neng<\/dc:language><dc:relation>\nSocial Science Computer Review<\/dc:relation><dc:format>\n16<\/dc:format>\n<\/oai_dc:dc>\n<\/metadata>\n        <\/record>","journals":[{"title":null,"identifiers":["0894-4393","issn:0894-4393"]}],"language":{"code":"en","id":9,"name":"English"},"relations":["Social Science Computer Review"],"year":2009,"topics":["eScience","natural language","provenance","semantic web","evidence-based policy","H Social Sciences","H"],"subject":["Journal article"],"fullText":"eSocial Science and Evidence-Based \nPolicy Assessment: Challenges and \nSolutions \nP Edwards1, J Farrington2, C Mellish1, L Philip2, A H Chorley1, F \nHielkema1, E Pignotti1, R Reid1, J G Polhill3 & N M Gotts3 \n1 School of Natural & Computing Sciences, University of Aberdeen \n2 School of Geosciences, University of Aberdeen \n3 The Macaulay Institute, Craigiebuckler, Aberdeen. \n{p.edwards, j.farrington, c.mellish, l.philip, a.h.chorley, f.hielkema, e.pignotti, r.reid} \n@abdn.ac.uk \n{g.polhill, n.gotts}@macaulay.ac.uk \n \nAbstract \nThe PolicyGrid project is exploring the role of Semantic Grid technologies to support eSocial \nScience, with particular emphasis on tools to facilitate evidence-based policy making. In this \npaper we discuss the challenges associated with construction of a provenance framework to \nsupport evidence-based policy assessment. We then discuss ourSpaces, a virtual research \nenvironment for eSocial Science that uses the Web2.0 paradigm as well as Semantic Grid \ntechnologies, and which provides researchers with facilities for management of digital \nresources.  We describe a novel approach for the creation and presentation of metadata, before \nexploring how policy-oriented use of social simulation can be integrated with our approach. \nIntroduction \nThe UK government has defined policy making as \u201cthe process by which governments \ntranslate their political vision into programmes and actions to deliver \u2018outcomes\u2019 \u2013 desired \nchanges in the real world\u201d (UK Cabinet Office, 1999).  An important element of twenty-first \ncentury policy making, at all levels, is a move away from opinion-based decision making to \ndecision making that is grounded in evidence and subject to rigorous evaluation.  Evidence is \nused at various stages of policy making, from the design of new policies to the evaluation and \nreview of existing policy.  An evidence base supports transparency and accountability in the \npolicy decision making process. The evidence used in policy making is derived from a range \nof sources, and academic researchers from across the social sciences make a notable \ncontribution to the evidence base for many areas of public policy:  they produce \u2018new\u2019 \nknowledge by undertaking primary research or conducting further analysis of existing data, \nthey produce reviews and critical appraisals of existing research and are often commissioned \nto review and evaluate specific policies.   \nThe PolicyGrid project1, a collaborative venture between computer scientists and social \nscientists interested in policy-related research, is investigating how social scientists can be \nsupported in their policy assessment activities through the application of eSocial Science tools \nand techniques, in particular the application of Semantic Grid (De Roure et al. 2005) \ntechnologies.  The term eScience has come to refer to the use of advanced computing \napproaches to support scientific research, while eSocial Science reflects the extension of these \nideas to support social science. The various activities being undertaken in the PolicyGrid \nproject and their application to evidence based policy assessment (hereafter EBPA) will be \ndescribed in this paper.  The remainder of this introductory section of the paper will introduce \nthe concept of EBPA, describe what some of the requirements for evidence to be used in \npolicy making are and, in so doing, identify some of the challenges for eSocial Science tools. \nEvidence Based Policy Assessment is associated with all aspects of policy-making and is \nrequired, in the UK, of national and local government.  Best practice principles for ex ante \nappraisal of policy and ex post evaluation of policy are set out in the UK Government\u2019s \nGreen Book (HM Treasury, 2003).  Our use of the term assessment embraces both appraisal \nand evaluation.  The Green Book\u2019s primary concern is economic assessment whereby the \ncosts and benefits of policy options are estimated, a process which includes formal \nconsideration of \u2018non-market\u2019 policy impacts.  Economic assessment requires that: \n\u2022 Reports should provide sufficient evidence to support their conclusions and \nrecommendations; \n\u2022 There should be an easy audit trail to allow decision makers to understand the \nassumptions underlying conclusions and recommendations; \n\u2022 There should be sufficient information to support any later evaluation.  \n(HM Treasury, 2003, para 2:14, our emphasis) \nTwo generic types of policy evaluation are recognised in the UK Government\u2019s performance \nmanagement system (UK Cabinet Office Strategy Unit, 2003-5 \u2013 The Magenta Book \u2013 p4; \nDavies, 2004 p19):  formative or process evaluation (how, why and under what conditions \ndoes a policy intervention \u2018work\u2019 or \u2018fail to work\u2019?) and summative or impact evaluation \n(what impact does a policy intervention have?).  These types of policy evaluation may overlap \nand interact and, importantly, recognise that policy evaluation can benefit from using a wide \nrange of evidence, not just (economic) cost-benefit analysis.  Useful evidence for policy \nevaluation can be the data and\/or the conclusions from previous research, and new material \ncollected to supplement existing evidence, to fill in gaps in existing evidence, or to create \ncompletely new evidence.  Policy makers are encouraged to critically appraise the evidence \nthey work with.  For example, they should ask: \n\u2022 Was the evidence sifted and graded for quality? \n\u2022 Were the inclusions and exclusion criteria explicit? \n\u2022 Is the evidence easy to understand? \n\u2022 Has the strength of the evidence been assessed? \n                                                 \n1\n  http:\/\/www.policygrid.org\/ \nThe nature of research questions, the primary data collection methods used, secondary source \nmaterials and the various analytical and interpretative techniques employed set limits on the \nreliability, replicability, generalisability and overall validity of conclusions that can be drawn \nfrom a piece of research that academic researchers may produce for use in policy assessment.  \nTo illustrate some of these points further, consider a research study which has the goal of \nidentifying the socio-economic, cultural and political changes which could bring about a \nreduction in carbon-intensive energy demand from the household sector; and the policy \ninstruments which could be employed to facilitate such changes, The study examines the \nsimilarities, differences and interactions between cities and their functionally associated rural \nareas, and a number of emerging initiatives, both grassroots and top-down, from an \ninterdisciplinary social science viewpoint. The project involves both qualitative and \nquantitative fieldwork, and agent-based simulation modelling. It addresses policy \ndevelopment from the perspective of multiple disciplines, working closely with associated \npolicy-makers and stakeholders. Outputs are expected to be a sequence of policy briefs.  \nWhat forms of computational support would assist the researchers involved in the work \noutlined above? Perhaps the most significant of these is the issue of management of evidence. \nThis refers to both the digital artefacts associated with the project, and their associated \nprovenance2.  Simmham et al. (2005, p31) defined data provenance as \u201cone kind of metadata \n[which] pertains to the derivation history of a data product starting from its original sources\u201d.  \nA number of computational provenance models have previously been reported in the eScience \nliterature, in a variety of areas including life sciences (e.g. myGrid - Stevens et al, 2003), \nchemistry (e.g. CombeChem - Taylor et al. 2006) and High Energy Physics (Branco & \nMoreau, 2006).  What are the particular issues that arise in developing a provenance model for \nevidence-based policy research? The ability to capture details about the resources \n(questionnaires, datasets, simulation experiments) and tasks (surveys, interviews, model runs) \nthat comprise an evidence-base is crucial if one wishes to satisfy the requirements set out in \nthe Green Book and the Magenta Book. Additionally, such a framework must provide support \nfor the creation of audit trails to allow evidence to be assessed\/validated, and must ultimately \ninclude mechanisms for creating policy arguments and options from the evidence base. Such a \nprovenance model would facilitate better communication and collaboration between members \nof a project team, while at the same time enhancing the \u2018interface\u2019 between the producers of \nevidence (e.g. academic researchers) and the users of that evidence (e.g. policy makers). \nThe existence of a framework for management of evidence of the kind suggested above raises \nsignificant challenges in terms of the design of appropriate user-interfaces and tools for \nresearchers. How do we support users who wish to create, query and browse provenance \ninformation without them needing to understand the intricacies of specific metadata formats? \nAny solution should acknowledge (at least in part) the conventions adopted by existing Web-\nbased archives for social science research, such as Intute and the UK Data Archive. Intute: \nsocial sciences3 is an online service that provides access to Web resources for education and \nresearch, that have been evaluated and selected by subject specialists. Users can browse the \ndatabase by topic (offering several thesauri for use), search by keyword (and in advanced \nmode search by resource type, topic and keyword), and suggest new additions. Intute also \ncontains additional services such as a blog, events, timelines, newsround, etc.  The UK Data \n                                                 \n2\n  A variety of terms are used in the computer science literature to describe the origins, analysis, transformations, \nassumptions and conclusions drawn from data, including lineage, pedigree, and genealogy.  In this paper, and the \nwider PolicyGrid project, we are using the term provenance.   \n3\n  http:\/\/www.intute.ac.uk\/socialsciences\/ \nArchive4 offers searches on different criteria (title, geographic coverage, time period, creator, \netc) with regular expressions\/key words. Users can deposit data by submitting a form, which \nis then processed by a human agent who interprets the information on the form into a \ndescription in a standard format. The existence of an evolving evidence-base should become a \npowerful tool to support interactions between project co-workers and external stakeholders. \nHow do we develop collaboration tools which support (amongst others): resource discovery, \nsharing and informed re-use of evidence, tracking of the development of evidence trails and \nultimately policy arguments, participation by representatives of multiple disciplines? \nThe exemplar study above highlights another important challenge if we are to produce support \nfor EBPA activities: How best to integrate a range of evidence types and methodologies, \nspanning not just qualitative and quantitative research methods, but also the emerging field of \nsocial simulation? This will necessitate development of appropriate resource and task \ndescriptions within the provenance model, and in addition, some mechanism for encoding \nsimulation experiment specifications in a form suitable for their re-use.  \nIn the following sections, after briefly introducing the main concepts of the Semantic Grid, we \ndescribe how the PolicyGrid project interprets these challenges in terms of technical \nrequirements, and the overall form of the solutions that the project proposes. \nThe Semantic Grid  \nGrid computing (Foster et al. 2001) plays an important role in delivering many eScience \napplications. Such technologies facilitate access to computing hardware, storage and \napplication services, and at the same time ensure effective management of quality of service \nissues, including security and authentication. While Grid technologies provide much of the \ninfrastructure backbone required to deliver eScience solutions, there is still a need for \nenhancements to improve ease-of-use, automate service\/data discovery and integration, \ndeliver collaboration tools, and so on. The Semantic Grid was conceived as \u201can extension of \nthe current Grid in which information and services are given well-defined meaning, better \nenabling computers and people to work in cooperation.\u201d In others words, by describing \nresources and services available on the Grid through the use of a semantic data model, new \nforms of software could be created to meet the needs of the end-user (scientific) community. \nIn earlier work (Pignotti et al. 2005, Polhill et al. 2007) we argued that such an approach has \nsignificant potential within eSocial Science in helping deliver the vision of a more 'human-\ncentred' Grid (Anderson, 2003) which facilitates tasks such as collaboration, shared \nexperimentation, and annotation of resources. Furthermore, we demonstrated how Semantic \nGrid techniques could be employed to capture qualitative scientific arguments, supported by a \nmix of quantitative and qualitative data and results. \n \nThe adoption of metadata and ontologies to describe resources is central to the Semantic Grid. \nMetadata are (computer-readable) data about data - they are used to describe resources (such \nas documents, datasets or web pages). As such they facilitate the understanding, use and \nmanagement of data. A frequently used medium to present metadata is RDF5 (Resource \nDescription Framework), a metadata model used to make predicate(subject, object) \nstatements about resources (known as RDF-triples), e.g.: \n \n                                                 \n4\n  http:\/\/www.data-archive.ac.uk\/ \n5\n  http:\/\/www.w3.org\/RDF\/ \n<j.0:Person rdf:ID=\u201cF1\u201d> \n<j.0:Name>John Smith<\/j.0:Name> \n<\/j.0:Person> \n \nThis RDF-fragment states that there is a Person whose Name is \u2018John Smith\u2019. Person is a \nclass in an ontology (defined by the namespace \u2018j.0\u2019); Name is a property of that class. \u2018John \nSmith\u2019 is an individual of this class. Ontologies are thus data models that represent a set of \nconcepts (e.g. Person) within a domain and the relationships between those concepts and\/or \ndatatypes (such as strings). Such ontologies are commonly described using the Web Ontology \nLanguage (OWL6), a vocabulary for providing descriptions about classes and properties of \nRDF resources. The query language SPARQL\u0001 supports complex RDF queries containing \noptional patterns, disjunctions, etc.  \nFrom the beginning of our interactions, social scientists expressed a fear of \u2018being trapped in \nthe ontology' due to the contested nature of many concepts within the social sciences. Others \n(Edwards et al. 2006) have noted that as social science concepts emerge from debate and are \nopen to indefinite modification through debate, vocabularies also tend to be imprecise  (e.g. \nthere is no precise definition of \u2018anti-social behaviour') and mutable (vocabularies tend to \nchange over time to reflect shifts in understanding of social reality). It has recently been \nargued (Gruber, 2006) that technologies such as OWL and RDF should act as a \u2018substrate for \ncollective intelligence' - in other words that the community-driven approach to creation and \nmanagement of content now increasingly popular on the Web8 should be integrated with these \ntechnologies. Within PolicyGrid we are adopting an approach which supports dynamic, \ncommunity-driven evolution of metadata (Guy & Tonkin, 2006) within a framework provided \nby a series of OWL ontologies. Our approach is similar in form to Gruber's suggestion of \nintegrating unstructured user contributions (tags) into a structured framework (ontology). We \nbelieve that it provides social scientists with a flexible and open-ended means of describing \nresources, whilst at the same time providing a context for those assertions through more \nstructured concepts. Details of the ontologies are given in the next section on provenance. \nPermitted values for many of the datatype properties within the ontologies are of type \u2018string' \nand it is here that users are permitted to enter tags; as users describe their resources, an \nunderlying folksonomy is constructed which can be used to guide others towards popular tag \nchoices. These folksonomies are kept separate because different values apply to different \nproperties; a property HasCountry has rather different tags associated with it than \nHasSamplingMethod. When a user selects a property, its folksonomy is presented in the \nform of a \u2018tag cloud\u2019 (see Figure 6 - middle), which contains the most frequently used tags for \nthis property; the user is able to select a term from the emerging community vocabulary or is \nfree to use their own. \n \nFigure 1 provides a schematic overview of the PolicyGrid software architecture. Several \naspects of this diagram are discussed elsewhere in this paper, but certain key components will \nbe introduced here. To manage the digital artefacts utilised during the research process, we \nuse a digital object repository; this is based upon the highly scalable Fedora9 open-source \ntechnology. Digital resources stored within Fedora are associated with metadata managed by \n                                                 \n6\n  http:\/\/www.w3.org\/TR\/owl-features\/ \n7\n  http:\/\/www.w3.org\/TR\/rdf-sparql-query\/ \n8\n  Often referred to as Web2.0. \n9\n  http:\/\/www.fedora-commons.org\/ \nan instance of the Sesame10 RDF triple-store framework; this provides support for RDF for \nstorage, inferencing and querying. Together with a relational database (used to manage tag \ndata) these repositories provide the platform for all other PolicyGrid software and services. \nThe box labelled \u2018Grid services\u2019 appearing in Figure 1 refers to a set of social simulation \nmodels (and other computational services) that have been deployed onto the Grid, and which \ncan be invoked through an appropriate user-interface. The ontologies layer in Figure 1 \nprovides the conceptual framework within which the various software services, tools and user-\ninterfaces interoperate. In the next section we describe three of the ontologies which define \nour provenance model. \n \nDigital Object\nRepository\nMetadata\nRepository\nRelational\nDatabase\nGrid\nServices\nArgumentation\nTool\nQualitative Analysis\nTool\nWorkflow\nTool\nOntologies\nVirtual Research Environment\nMetadata Interface\n \n \nFigure 1: PolicyGrid software architecture. \nProvenance  \nProvenance applied to eSocial Science should provide information about how data were \ncreated and provide information about the context of the data.  Such context could include, for \nexample, an account of the characteristics of secondary data and why it was used; an account \nof data collection methods, including who collected the data, from whom, when and where; an \naccount of the analytical\/interpretative process including who was involved and any \nassumptions that were made about the data; and a record of what conclusions were drawn, \nhow data were written up and what dissemination has taken place.  In EBPA, context is \nparticularly important because of the need to evaluate the quality (and thus the reliability) of \ndata, the robustness of analysis, the generalisability and the validity of findings, conclusions \nand\/or recommendations.  Context also helps when data are reused, particularly when that \nanalysis is not performed by those who created the data and or conducted the initial analysis.   \nParallels may be drawn between some research activities in the natural and social sciences, \nnotably the application of statistical and modelling techniques to large quantitative data sets.  \nHowever, qualitative research is arguably as important to the social science community as \nquantitative research, the policy community has become increasingly receptive to qualitative \ndata in the UK and a \u2018mixed methods\u2019 approach is frequently adopted in policy-related \nresearch.  A provenance model for EBPA in the social sciences should not only draw upon \nlessons learnt from the development of provenance architectures for eScience, it should also \nbe capable of handling quantitative, qualitative and mixed methods approaches.   \n                                                 \n10\n  http:\/\/www.openrdf.org\/ \nA provenance system that tracks evidence-conclusion chains in any type of social science \nresearch potentially raises philosophical, ethical and legal issues.  Social scientists working in \nthe policy field must not only conduct their research in an ethically appropriate manner, they \nshould be able to produce, through transparent processes, a robust system for the analysis of \ndata which forms evidence of policy \u2018success\/failure\u2019.  Legal issues relating to research \nmaterials, for example, provisions under data protection, copyright, intellectual property rights \nand freedom of information legislation (c.f. Bishop, 2005; Parry & Mauthner, 2004; 2005) \nalso apply.  For some qualitative researchers each data collection event is unique and \nunreplicable; the narrative content is singular and not capable of analysis by anyone other than \nthe researcher who collected the data.  Others view data as being capable of analysis by others, \nand also subscribe to extension through generalisation. Can, or should, both positions be \ncaptured by a provenance model?  Further, epistemologically informed positions surrounding \nthe secondary use of qualitative data (Mauthner et al. 1998; Fielding, 2004) can affect \nattitudes towards and the practicalities of archiving data and allowing other researchers access \nto source materials, regardless of whether those researchers be from the policy (government), \nacademic or consultancy communities.  Notwithstanding these concerns, in the context of UK \ngovernment practice there is a case for developing a provenance model to support the range of \ndata used in social science research.   \nThe challenge, then, is to track the evidence-gathering and evidence-analysis\/conclusion \nprocess for qualitative and quantitative research in the social sciences, consider the issue of \ndata re-use for EBPA and be sensitive to epistemological concerns which express an \nuneasiness about reusing qualitative data in particular.  The PolicyGrid schema for an e-based \nprovenance system was first described in Chorley et al. (2007).  The model defines \nprovenance as a description of how a resource came about, capturing information about the \nactivities performed to create that resource.  For example, a quantitative data set is associated \nwith the questionnaire that elicited the data.  Contextual information, such as the date that a \nquestionnaire was administered and the response rate, can also be recorded.  Formally \nrecording such information allows questions such as \u2018how was the evidence derived\u2019 to be \nanswered, providing evidence that would allow a third party to ascertain the robustness, or \ntruthfulness of the data collection process.  Information about data analysis and interpretative \nprocesses could also be captured because the provenance model allows the researcher to \nrecord what they do.  For example, if working with interview transcripts a description of the \nprocess by which thematic codes were developed could be recorded (in effect, these would be \nthe memos or other analytical notes routinely made during qualitative analysis but not \nroutinely recorded beyond the researcher\u2019s own informal notes).  If these interview transcripts \nwere re-used a second layer of analytical information could be added if the second researcher \ndrew additional, or different, conclusions from the data.  How the provenance information is \ncollated and recorded by the user (researcher) is described in the next section of this paper. \nTo develop our provenance model, we began by analysing the descriptions (available online) \nof the various datasets held in the UK Data Archive. The descriptions were used to identify \nthe concepts necessary to characterise a social science resource. The result of this initial \nexercise was an OWL ontology which modelled objects such as Document and Person, but \navoided abstract domain concepts such as Rural_accessibility and Poverty. More \ninformation about this initial ontology can be found in Chorley et al. (2007). Evaluation of \nthis ontology (Hielkema et al. 2007) concluded that it did not match the subject\u2019s model of \nresource descriptions closely enough to enable them to easily create satisfactory descriptions. \nWe needed a revised model of provenance that was both more comprehensive and contained \nless ambiguous or duplicate properties, and which enabled users to describe resources in a \nmanner with which they were comfortable. \nWe thus set out to develop a revised set of ontologies that could be used by all tools and \nservices that PolicyGrid is developing, while at the same time retaining a degree of \ncompatibility with the UK Data Archive schema. In a series of interviews with social science \nresearchers, we listed the possible resource types that might be deposited, and the information \nthat should be recorded about them including the details of the method(s) by which they were \ngenerated\/revised\/analysed. \nThe PolicyGrid provenance model comprises three different ontologies: Utility11, Resource12 \nand Task13. The Utility ontology is used to describe utility items such as projects and persons, \nwhile the Resource ontology describes resources, including information such as title, author, \naccess rights and dates of creation and\/or publication. The Task ontology is used to describe, \nfor example, a specific data collection method, including details such as the time and location \nof an interview and the format the interview took; dissemination tasks, such as conference \npresentations, the final report and academic papers; or the preparation of a literature review. \nThe three ontologies are separate but compatible, so that the description of a resource and the \nprocess through which it was created uses elements from them all.  Using the example of the \ndescription of an interview transcript, Figure 2 depicts parts of all three ontologies and how \nthey interoperate; the notation used in the figure is as follows: dotted ovals represent a \nresource, solid ovals represent tasks, and rectangles are properties.  An interview transcript is \na resource that is associated with a particular interview task, which could itself have produced \nother resources, e.g. a recording of the interview and interviewer\u2019s written notes.  These tasks \nand resources all have properties such as DateOfDeposit or DateOfInterview.  They will \nalso have properties describing roles, such as the interview transcript was_deposited_by a \nperson and an interview task was_conducted_by a person. Person and Address are \nconcepts from the Utility ontology, and are represented in Figure 2 by an oval with a double \noutline. \nPerson\nPerson\nInterview Transcript\nPerson\nDeposited By\nInterview\nCreated By\nRecording\nHas Factual Notes\nTranscript Of\nHas Recording\nFactual Notes\nHas Access \nContact Person\nHas Interviewer\nName\nEmail\nHas keywords\nAccess Conditions\nDate of Deposit\nDate of Deposit\nHas keywords\nAccess Conditions\nDate of Deposit\nHas keywords\nAccess Conditions\nQuestions\nLocation\nDate of Collection\nHas Address\nAddress\n \nFigure 2: Using the Resource, Task and Utility ontologies to \ndescribe an interview transcript. \nIn order to assess this approach to provenance a series of case study discussions with social \nscientists were conducted. In each case, interviews were used to elicit full descriptions of a \nprevious (completed) research project, and these descriptions then analysed to determine if the \nontologies were capable of capturing the provenance record. During each interview a \ndiagrammatic representation of the project and the relationships between resources and tasks \nwas created on a whiteboard. An example of such a record is reproduced below in Figure 3; \n                                                 \n11\n  http:\/\/www.policygrid.org\/utility.owl \n12\n  http:\/\/www.policygrid.org\/resource.owl \n13\n  http:\/\/www.policygrid.org\/task.owl \nthis characterises a project which developed a novel approach to contingent valuation - the CV \nMarket Stall (Philip and Macmillan, 2005).   \n \nFigure 3:  Whiteboard image of description of the CV market stall case study. \nPublications and other written documents associated with the case study projects were \nreviewed after each interview to ensure that all aspects of the project were represented and, \nalong with the interview notes, were used to prepare a diagrammatic representation of the \nproject which was used to elicit feedback.  Figure 4 reproduces an extract from such a \ndiagrammatic representation of the CV Market Stall project.  Tasks are depicted as boxes \n(labelled with the tasks details) and resources are depicted as circles (labelled with the \nresource type).   \nDesign Questionnaire\nR\nList of participants\nR\nQuestionnaire\nR\nCompleted \nSurvey\nR\nInterviewer Notes\nR\nRecruitment Document, \nInitial Call, Final \nArrangements\nIdentification of\nData Sources\n(Recruitment)\nQuestionnaire\nSurvey\nTelephone \nInterview\nCoded and Entered\nTranscribed Coded\nR\nTranscribed \nInterviewer Notes\nT TT\nT\nT\nTT\nR\nT\nResource\nTask\nR\nCoded and \nTranscribed \nInterviewer Notes\n \nFigure 4:  Provenance extract from the CV market stall case study. \nFigure 4 demonstrates how various tasks and their associated resources can be captured using \nthe PolicyGrid provenance model.  The top row of the diagram shows how participants were \nrecruited.  The second row illustrates the various stages associated with the background \nquestionnaire and the third row records the various stages associated with the telephone \ninterviews.  The findings from this and the other case studies confirmed that the ontologies \ncould capture the metadata required to document the provenance record and thus provide an \naudit trail for research activities.  In the next section we describe user-interface tools that have \nbeen designed to allow social scientists to utilise the provenance model to enhance their \nresearch activities. \nUser-interface Issues \nAs outlined earlier, there are many challenges to be addressed if we are to build usable eSocial \nScience tools to support those conducting evidence-based policy research. Managing the  \nresources associated with an EBPA activity can be an onerous task, especially when that \nactivity involves a large (possibly multi-disciplinary) effort. While the provenance model \ndescribed above forms a part of the solution, appropriate user-interface tools are also essential. \nSemantic Grid technologies are powerful but complex, yet user interfaces must be designed to \nsupport researchers to deliver the services they require without them becoming expert in, or \neven having to be aware of, the underlying infrastructure.  \nA number of existing Web-based user-interfaces or portals aim to facilitate access to eScience \nservices; these virtual research environments (VREs) include myExperiment (De Roure et al. \n2007) and SciSpace14. myExperiment is a collaborative environment which supports the \npublishing of experimental workflows and other digital objects. It allows scientists to share, \nre-use and re-purpose workflows by supporting many of the features seen in social networking \nsites, including tagging of resources and commentaries. SciSpace, on the other hand, has been \ndesigned as a social networking service for scientists, with tools such as blogs, wikis, and \ntagging. It also ensures that such networks can remain private, allowing small groups of \nresearchers to develop\/share ideas. Inspired by these and other developments, we are currently \ndeveloping a VRE to support social scientists conducting evidence-based policy research; this \nenvironment (ourSpaces) owes much to social networking sites such as MySpace and \nFacebook (see Figure 5). Support is provided in the prototype version for the following: social \nnetworking (messaging, collaboration invites, etc.); resource management (upload, search, \nannotation); creation of project \u2018spaces\u2019 (allowing project teams to manage membership, \naggregate digital artefacts, organise activities according to project stages); privacy controls; \npublishing of blogs and wikis; execution\/monitoring of simulation workflows. Each of these \nactivities is enabled by the rich and pervasive metadata infrastructure described earlier. \nourSpaces enables users to upload and describe resources themselves, rather than through a \nthird party. How can we elicit metadata from, and present metadata to, social scientists? RDF-\ntriples are difficult to read and write: even experts make mistakes producing them with a text \neditor. Tools that use graphical representations (e.g. CREAM: Handschuh et al. 2001) are \nmore effective, but for users unused to complex graphical presentations or ontologies they can \nbe difficult to interpret. Petre (1995) argues that graphical readership is an acquired skill. She \ndescribes experiments into reading comprehension of various graphical and textual \nrepresentations which showed that graphical representations are significantly slower than text. \nNovices in particular suffered from mis-readings and confusion. We argue that for many \nusers, viewing natural language descriptions of metadata would most likely be both faster and \nless error-prone than viewing a graphical representation. Finding and describing resources are \ntasks peripheral to research, and not all researchers who need to do this are experts in reading \nand writing metadata.  \n                                                 \n14\n  http:\/\/www.scispace.net\/ \n  \nFigure 5: The ourSpaces virtual research environment. \nExisting natural language approaches for providing access to databases include menu-\ntechniques in which the user constructs an acceptable sentence by selecting words from a \nseries of menus (Bernstein & Kaufmann, 2006), and controlled languages (Schwitter & \nTillbrook, 2004). Such techniques severely restrict what can be said, limiting the language and \noften making it stilted, so that there is a small learning curve before the user knows which \nstructures are permitted. However, some limitation on what can be expressed is inevitable, \nbecause a system that can parse every utterance the user creates is outside the scope of the \ncurrent state-of-the-art. \nIn order to maintain full expressivity and maximise usability, we have elected to use \nWYSIWYM (What You See Is What You Meant; Power et al. 1998). This is a natural \nlanguage generation approach developed for providing access to databases or other \nknowledge structures, where the system generates a feedback text for the user that is based on \na semantic representation. This representation is edited directly by the user by manipulating \nthe feedback text. As the text is generated by the system and does not have to be parsed, we do \nnot have to restrict what can be said, so the language retains its expressivity and the user does \nnot need to learn what is acceptable input. LIBER15 (Language Interface for Browsing and \nEditing RDF), a metadata interface for creating RDF (editing), SPARQL (querying) and \nviewing existing metadata (browsing) is shown in Figure 6. \n                                                 \n15\n  http:\/\/roc.csd.abdn.ac.uk:8091\/liber\/ \n Top:  Selecting a menu item from the \u2018interview\u2019 anchor. \n \n \nMiddle: Providing \u2018location\u2019, prompted by the tag cloud. \n \n \nBottom: The updated feedback text. \n \nFigure 6: Creating metadata using the LIBER metadata interface. \n We have evaluated LIBER\u2019s usability and usefulness through a series of qualitative and \nquantitative experiments (Hielkema et al. 2007; 2008). The most recent experiment was \nconducted during the Fourth International eSocial Science Conference, and involved eight \nsubjects, most of whom came from a social science background and six of whom had not used \nontologies before. After viewing a four-minute introduction video, they were handed a \nresource description and asked to find the resource in the archive using LIBER\u2019s querying and \nbrowsing components. Most subjects succeeded in this task despite their unfamiliarity with \nLIBER, although the experiment emphasized that accessing metadata is a much more complex \ntask than key-word searching, and that for simple queries the current interface is overly \ncomplex. An earlier evaluation study of the metadata creation component showed that \nsubjects did find LIBER useful for the more complex task of creating descriptions. \nUntil now, all evaluation involving LIBER has been with it operating as a stand-alone web \nservice. LIBER is currently being integrated fully into ourSpaces, so that information about \nthe user\u2019s current activities and other resources contained in ourSpaces can be automatically \nadded to the user\u2019s descriptions and queries. This should reduce the burden on the user, while \nimproving the quality and completeness of the metadata descriptions. Our aim is to integrate \nthe existing support within ourSpaces for simple key word searches through tag clouds, with \nthe much richer querying and metadata creation facilities provided by LIBER. \nIntegrating Social Simulation  \nSocial simulation has been proposed as a \u2018third way\u2019 for social science, both formal and \ndescriptive in its representations of reality (Moss and Edmonds, 2005). It focuses on explicit \nrepresentation of individuals and their actions, and so has theoretical roots in complex \nadaptive systems (Holland and Miller, 1991). Agent-based social simulation (ABSS) \nmodelling, in which decision-makers (individuals, firms, states) are explicitly represented \nwithin the simulation model, has considerable potential use in policy-related applications. \nNevertheless, actual policy-related uses have been relatively infrequent. There are a number of \nkey challenges concerning how ABSS models are used, which need to be overcome before \nthis can be expected to change. \nCalibration, validation, and subsequent policy-oriented use of agent-based models all require \nthe model to be applied to data; often, this will be data of several different types, probably \nfrom different sources \u2013 and may include both quantitative and qualitative social science data. \nWith respect to all these stages, a considerable number of simulation runs will be required, \nalong with extensive post-processing using statistical and graphical software, and probably \npre-processing of data as well. It is therefore vital to keep track of the simulation runs \nperformed, the inputs and outputs, and the purpose of each set of runs. This should be done in \na way that makes it easy for either the original investigators, other researchers, or \npolicymakers to check and understand the investigative process. Moreover, the research \nprocess is very unlikely to be a linear one: results of simulation runs at any stage may suggest \nproblems with the data or model, leading to revisions, or additional forms of post-simulation \nstatistical and graphical processing. Since social simulation modelling is often only a part of a \nlarger project including empirical social science and theoretical development, the complexity \nof the relationships between the simulation model and its scientific and policy context can be \nenormous. \nWith PolicyGrid we are attempting to address some of these challenges through the adoption \nof scientific workflow technologies (Pennington, 2007); these provide a user-friendly \nenvironment for scientists to create and execute experiments from a pool of available data and \ncomputational services. However, one weakness of current workflow languages such as \nMoML16 , BPEL17 and Scufl18 is that while they use representations which make the steps of \nthe experimental method explicit, they do not provide support for capturing the information \nabout goals and contextual conditions vital in understanding an experiment. We are thus \nexploring how such workflow languages can be extended, by capturing a higher-level \ndescription of the experimental process: the scientist\u2019s intent. To illustrate, Figure 7 presents a \nworkflow created using the Kepler 19 tool and used for the calibration and validation of a \nland-use model (Polhill et al. 2001). The workflow is itself embedded into a wider research \nactivity, involving use of quantitative data to establish values for model parameters, and \nqualitative data drawn from semi-structured interviews to influence decision algorithms \nwithin the model. The goal and constraint shown on the right hand side of Figure 7 cannot be \nconveniently embedded in a workflow description. In addition to ensuring that there is support \nwithin our provenance model for the resources and tasks associated with simulation \nexperiments, we have developed a rule-based framework (Pignotti et al. 2008) for capturing \nscientist\u2019s intent.  The rules, written in SWRL20  (Semantic Web Rule Language), act upon \nmetadata generated from workflow activities (e.g. inputs, outputs, execution). Details of the \nintent are kept separate from the operational workflow (itself a digital artefact), to avoid \nembedding and thus obscuring, important provenance information within the workflow \nrepresentation. \n \nFigure 7: An example workflow with associated intent. \nThe constraint in Figure 7 (top-right) aims to save computational resources by terminating the \nexploration of a parameter-set that does not add value to the results. The constraint: \u2018if in any \nof the 50 runs, one land manager owns more than half of the land, ignore this parameter-set\u2019 \ncan be represented by combining rules with the metadata available from the simulation \nservice. The main challenges we faced were to represent scientist\u2019s intent so that: \n \n\u2022 It is meaningful to the researcher, e.g. providing information about the context of an \nexperiment so that the results can be interpreted; \n                                                 \n16\n http:\/\/ptolemy.eecs.berkeley.edu\/projects\/summaries\/00\/moml.html \n17\n  http:\/\/www.ibm.com\/developerworks\/library\/ws-bpel\/ \n18\n http:\/\/www.cs.man.ac.uk\/~witherd5\/taverna-site\/scufl\/index.html \n19\n  http:\/\/kepler-project.org\/ \n20\n http:\/\/www.w3.org\/Submission\/SWRL\/ \nGoal: Obtain at least one \nmatch where the real data \nfalls within 95% \nconfidence interval of the \nmodel value. \nConstraint: If in any of the \n50 runs, one land manager \nowns more than half of the \nland, ignore this parameter-\nset. \n\u2022 It can be reasoned about by a software application, e.g. to use the intent information to \ncontrol, monitor or annotate workflow execution; \n\u2022 It can be re-used (the same intent may apply to different workflows); \n\u2022 It can be used to further enhance the provenance record (documenting the process that \nled to some result).  \nDiscussion \nWe have developed a provenance model able to support descriptions of a range of social \nscience resource types, and methods. It satisfies the requirements set out in the UK \nGovernment\u2019s Green Book, by facilitating the creation of an audit trail, and making explicit \nthe context within which conclusions were drawn. At present, our model only captures those \nstages of the EBPA process concerned with activities such as planning, data collection, \nanalysis and dissemination of results; it does not yet support the creation of (alternative) \npolicy arguments. We are therefore beginning to turn our attention to the issue of support for \nconstruction of policy arguments using the evidence represented using the provenance model. \nArgument schemes (Bex et al. 2003) seem to offer an appropriate method for construction and \nvalidation of such arguments. \nOur approach to provenance requires rich metadata descriptions to be created about resources, \nprojects, data collection tasks, etc. It is unrealistic to expect a user to create descriptions that \nare complete in all necessary aspects; such a task would be far too time-consuming. The \nLIBER metadata interface tool supports creation of RDF using natural language. Although \nthis interface has many positive features, it is clear to us that more needs to be done to exploit \nthe context that the ourSpaces VRE can provide to automatically suggest parts of the metadata \ndescription. For instance, if a user uploads a transcript into a project space while using \nourSpaces, the system should infer that the resource was produced by that user during that \nproject, during the data collection stage, and suggest possible authors, interviewers, dates, \nlocations, etc. using the information already held in the metadata repository. In this way the \nuser only has to specify in what way the transcript differs from the other resources in the \nproject, while the other (larger) part of its description is created automatically. Similarly, \nLIBER may be too complex for many simple queries. Tag clouds and keyword searching may \nbe more appropriate in such situations, with the power of the metadata interface being \nreserved for complex searches, such as those requiring secondary data from a certain time, \nplace and period.  \nWe envision ourSpaces becoming a central workplace for the social scientist, rather than \u2018yet \nanother tool we must login to\u2019. As many activities as possible should be integrated into this \nenvironment, which must be adaptable to the user\u2019s own preferences. We have already \nimplemented a means for the user to configure and personalise aspects of their own personal \nworkspace, or to tailor a project space to their needs, e.g. by only displaying relevant resource \ntypes and tools. In the coming months we intend to evaluate ourSpaces and LIBER further, by \nobserving user behaviour in real-life usage scenarios, rather than in an artificial laboratory \nsetting. \nThe policy-oriented use of simulation models requires that the experimental methodology be \naccessible to investigators, other researchers and policymakers. Through our scientist\u2019s intent \nframework we aim to make social simulation experiments more transparent, by providing a \ncloser connection between workflow experiments and the goals and constraints of the \nresearcher. While this approach provides important additional provenance information for the \nexperiment, we argue that its use should also facilitate management of workflow execution. \nWe are currently assessing our approach using a series of case-studies which aim to explore \nour intent representation in terms of its expressiveness, and its ability to enhance reusability of \nworkflow experiments.  \nThis paper has demonstrated that, drawing upon the experience of eScience, there is potential \nto apply the concept of provenance to eSocial Science. A provenance model capable of \nsupporting a range of quantitative and qualitative data types and analytical\/interpretative \nmethods has being developed which allows users to record whatever they do: it is thus capable \nof accommodating the different styles of quantitative and qualitative research (at least in the \nEBPA context), may be capable of furthering the application of a mixed methods approach in \nthe social sciences because contextual information demonstrates how useful combining \ndifferent types of data can be, and it facilitates the re-use of data. Encouraging social scientists \nto work with the \u2018provenance concept\u2019 has the potential to support methodological rigour and \noverall good research practice because it will support and encourage researchers to be \nreflective and reflexive at all stages of their research. However, challenges remain. How \nreceptive will qualitative researchers be to formally recording the analytical\/interpretative \nprocess? Will all social scientists be willing to record their data and methods of working in the \ndetail required for reuse? The next stage for the PolicyGrid team is therefore to test the \nprovenance model (using the ourSpaces VRE), a process which no doubt will answer some \nquestions and raise more. \nThe technologies described in this paper were not designed to support interdisciplinary \nactivity; rather, the assumption made throughout has been that we were supporting small, \nsingle discipline teams. However, many of the problems facing society (e.g. climate change) \nwill require interdisciplinary collaborations in order to address the societal and scientific \nimplications. How will our provenance model need to change to support interdisciplinary \nevidence bases? As well as appropriate resource and task metadata for other domains (e.g. \necology) issues surrounding interoperation of social science and natural science evidence \ntypes need to be explored. Some of the issues likely to be encountered when integrating such \nevidence include: how to make explicit constraints on sharing and re-use of evidence, the role \nof shared terminology within an interdisciplinary project, granularity of evidence types (single \nsample result vs. macro level survey outcome), temporal scale (data gathered over minutes vs. \nmonths), spatial context (data gathered at specific geospatial coordinates vs. a focus group \nexercise conducted at administrative region level). Investigation of these will no doubt \nhighlight many more challenges. \nAcknowledgements \nThe PolicyGrid project is funded by the UK Economic and Social Research Council, under \ntheir eSocial Science programme; award reference: RES-149-25-1027. We would like to \nextend our thanks to all those social scientists that have participated in evaluation of our tools \nto date, and wish to acknowledge the important contribution they have made to the \ndevelopment of our ideas. \nReferences \nAnderson, A.H. (2003): \u2018Human centred design and grid technologies\u2019, Economic & Social \nResearch Council Discussion Paper. \nBernstein, A. and Kaufmann, E. (2006): \u2018Gino - a guided input natural language ontology \neditor\u2019, International Semantic Web Conference 2006, pp. 144\u2013157. \nBex, F., Prakken, H., Reed, C. and Walton, D. (2003): \u2018Towards a formal account of \nreasoning about evidence: argumentation schemes and generalisations\u2019, Artificial \nIntelligence & Law, 11 (2-3). \nBishop, L. (2005): \u2018Protecting respondents and enabling data sharing: reply to Parry and \nMauthner\u2019, Sociology 39, pp. 333-336. \nBranco, M. and Moreau, L. (2006): \u2018Enabling provenance on large scale e-Science \napplications\u2019, Proceedings of the International Provenance and Annotation Workshop \n(IPAW'06), Volume 4145 of Lecture Notes in Computer Science, pp.55-63.  \nChorley, A., Edwards, P., Preece, A. and Farrington, J. (2007): \u2018Tools for tracing evidence in \nsocial science\u2019, Proceedings of the Third International Conference on eSocial Science, \nAnn Arbor, Michigan, USA. \nDe Roure, D., Jennings, N. and Shadbolt, N. (2005): \u2018The semantic grid: past, present and \nfuture\u2019, Proceedings of the IEEE 93 (3), pp. 669\u2013681. \nDe Roure, D., Goble, C. and Stevens, R. (2007):  \u2018Designing the myExperiment virtual \nresearch environment for the social sharing of workflows\u2019, e-Science 2007 - Third IEEE \nInternational Conference on e-Science and Grid Computing, Bangalore, India, 10-13 \nDecember 2007, pp. 603-610. \nEdwards, P., Aldridge, J. and Clarke, K. (2006):  \u2018A tree full of leaves: description logic and \ndata documentation\u2019, Proceedings of the Second International Conference on e- Social \nScience.  \nFielding, N. (2004): \u2018Getting the most from archived qualitative data:  epistemological, \npractical and professional obstacles\u2019, International Journal of Social Research \nMethodology 7 (1), pp. 97-104. \nFoster, I., Kesselman, C. and Tuecke, S. (2001): \u2018The anatomy of the grid: enabling scalable \nvirtual organizations.\u2019 International J. Supercomputer Applications 15 (3), pp 200-222. \nGruber, T. (2006): \u2018Where the social web meets the semantic web\u2019. In Cruz, I.; Decker, S.; \nAllemang, D.; Preist, C.; Schwabe, D.; Mika, P.; Uschold, M.; and Aroyo, L., eds., \nProceedings of the 5th International Semantic Web Conference, ISWC 2006, volume \n4273, 994. Athens, GA, USA: Springer Lecture Notes in Computer Science.  \nGuy, M., and Tonkin, E. (2006): \u2018Folksonomies: tidying up tags?\u2019 D-Lib Magazine 12 (1).  \nHandschuh, S., Staab, S. and Maedche, A. (2001): \u2018CREAM: creating relational metadata \nwith a component-based, ontology-driven annotation framework\u2019, K-CAP \u201901: \nProceedings of the 1st international conference on Knowledge capture, ACM Press, New \nYork, NY, USA, pp. 76\u201383. \nHielkema, F., Edwards, P., Mellish, C. and Farrington, J. (2007): \u2018A Flexible Interface to \nCommunity-Driven Metadata\u2019, Proceedings of the Third International Conference on \neSocial Science, Ann Arbor, Michigan, USA. \nHielkema, F., Mellish, C. and Edwards, P. (2008): \u2018Evaluating an Ontology-Driven \nWYSIWYM Interface\u2019, Proceedings of the Fifth International Conference on Natural \nLanguage Generation (INLG2008), Columbus, Ohio, USA. \nHM Treasury (2003): \u2018The green book: a guide to appraisal and evaluation\u2019, HM Treasury, \nLondon. \nHolland, J. H. and Miller, J. H. (1991): \u2018Artificial adaptive agents in economic theory\u2019, The \nAmerican Economic Review 81 (2), pp. 365-370. \nMoss, S. and Edmonds, B. (2005): \u2018Towards good social science\u2019, Journal of Artificial \nSocieties and Social Simulation 8 (4), 13. \nMauthner, N.S., Parry, O. and Backett-Milburn, K. (1998): \u2018The data are out there, or are \nthey?  Implications for archiving and revisiting qualitative data\u2019, Sociology 32 (4), pp. \n733-745. \nParry, O. and Mauthner, N.S. (2004): \u2018Whose data are they anyway?  Practical, legal and \nethical issues in archiving qualitative research data\u2019, Sociology 38 (1), pp. 139-152. \nParry, O. and Mauthner, N.S. (2005): \u2018Back to basics:  who reuses qualitative data and why?\u2019, \nSociology 39 (2), pp. 337-342.  \nPennington, D. (2007): \u2018Supporting large-scale science with workflows\u2019, Proceedings of the \n2nd workshop on Workflows in support of large-scale science, High Performance \nDistributed Computing 2007. \nPetre, M. (1995): \u2018Why Looking isn\u2019t always Seeing: Readership Skills and Graphical \nProgramming\u2019, Communications of the ACM 38 (6), pp. 33\u201344. \nPhilip, L.J. and Macmillan, D.C. (2005): \u2018Exploring values, context and perceptions in \nContingent Valuation studies:  the CV Market Stall Technique and Willingness to Pay for \nWildlife Conservation\u2019, Journal of Environmental Planning and Management 48 (2), pp. \n257-274. \nPignotti, E., Edwards, P., Preece, A., Polhill, G., Gotts, N. (2005): \u2018Semantic support for \ncomputational land-use modelling\u2019, Proceedings of the Fifth IEEE International \nSymposium on Cluster Computing and Grid (CCGrid2005), IEEE Press. Volume 2, pp. \n840\u2013847. \nPignotti, E., Edwards, P., Preece, A., Polhill, J.G. and Gotts, N. (2008): \u2018Enhancing workflow \ndescriptions with a semantic description of scientific intent\u2019, Proceedings of Fifth \nEuropean Semantic Web Conference, Springer-Verlag, pp. 644-658. \nPolhill, J.G., Gotts, N. and Law, A. (2001): \u2018Imitative versus non-imitative strategies in a land \nuse simulation.\u2019 Cybernetics and Systems, 32 (1), pp. 285\u2013307. \nPolhill, J.G., Pignotti, E., Gotts, N., Edwards, P. and Preece, A. (2007): \u2018A semantic grid \nservice for experimentation with an agent-based model of land-use change\u2019. Journal of \nArtificial Societies and Social Simulation 10 (2) 2. \nPower, R., Scott, D. and Evans, R. (1998): \u2018What you see is what you meant: direct \nknowledge editing with natural language feedback\u2019, Proceedings of the Thirteenth \nEuropean Conference on Artificial Intelligence, Brighton, UK, 1998. \nSchwitter, R. and Tilbrook, M. (2004): \u2018Controlled natural language meets the semantic web\u2019, \nProceedings of the Australasian Language Technology Workshop 2004. \nSimmhan, Y.L., Plale, B. and Gannon, D. (2005): \u2018A survey of data provenance in e-Science\u2019, \nACM SIGMOD Record 34(3), pp. 31-36. \nStevens, R., Robinson, A., and Goble, C. (2003): \u2018my-Grid: Personalised bioinformatics on \nthe information gGrid\u2019, Bioinformatics 19 (1), pp. 302\u2013304. \nTaylor, K., Essex, J. W., Frey, J. G., Mills, H. R., Hughes, G., and Zaluska, E. J. (2006): \u2018The \nsemantic grid and chemistry: experiences with CombeChem\u2019, Journal of Web Semantics \n4 (2), pp. 84\u2013101. \nUK Cabinet Office Strategy Unit (2003-5): \u2018The magenta book\u2019, Government Chief Social \nResearcher\u2019s Office, London. \nUK Cabinet Office (1999): \u2018Modernising government\u2019, White paper, HM Stationary Office, \nCm 4310, March 1999. \n \n"}