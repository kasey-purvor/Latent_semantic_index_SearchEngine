{"doi":"10.1016\/j.imavis.2006.07.017","coreId":"71506","oai":"oai:eprints.lancs.ac.uk:819","identifiers":["oai:eprints.lancs.ac.uk:819","10.1016\/j.imavis.2006.07.017"],"title":"Sequential Monte Carlo tracking by fusing multiple cues in video sequences","authors":["Brasnett, P","Mihaylova, L"],"enrichments":{"references":[{"id":16344010,"title":"1, http:\/\/www.bbc.co.uk\/calc\/radio1\/ ,&quot; index.shtml,","authors":[],"date":"2005","doi":null,"raw":"\\BBC Radio 1, http:\/\/www.bbc.co.uk\/calc\/radio1\/ ,&quot; index.shtml, 2005, Creative Archive Licence.","cites":null},{"id":16344002,"title":"A \u00aflter design technique for steerable pyramid image transforms,&quot;","authors":[],"date":"1996","doi":"10.1109\/icassp.1996.547763","raw":"A. Karasaridis and E. P. Simoncelli, \\A \u00aflter design technique for steerable pyramid image transforms,&quot; in Proceedings of the ICASSP, Atlanta, GA, May 1996.","cites":null},{"id":16344008,"title":"A study on Bayes feature fusion for image classi\u00afcation,&quot;","authors":[],"date":"2003","doi":"10.1109\/cvprw.2003.10090","raw":"X. Shi and R. Manduchi, \\A study on Bayes feature fusion for image classi\u00afcation,&quot; in Proceedings of the IEEE Workshop on Statistical Analysis in Computer Vision, vol. 8, June 2003.","cites":null},{"id":16343964,"title":"A tutorial on particle \u00aflters for online nonlinear\/non-Gaussian Bayesian tracking,&quot;","authors":[],"date":"2002","doi":"10.1109\/78.978374","raw":"M. Arulampalam, S. Maskell, N. Gordon, and T. Clapp, \\A tutorial on particle \u00aflters for online nonlinear\/non-Gaussian Bayesian tracking,&quot; IEEE Transactions on Signal Processing, vol. 50, no. 2, pp. 174{188, 2002.","cites":null},{"id":16343977,"title":"Adaptive texture and color segmentation for tracking moving objects,&quot;","authors":[],"date":"2002","doi":"10.1016\/s0031-3203(01)00181-9","raw":"E. Ozyildiz, N. Krahnst\u00c4 over, and R. Sharma, \\Adaptive texture and color segmentation for tracking moving objects,&quot; Pattern Recognition, vol. 35, pp. 2013{2029, October 2002.","cites":null},{"id":16343965,"title":"An adaptive color-based particle \u00aflter,&quot;","authors":[],"date":"2003","doi":"10.1016\/s0262-8856(02)00129-4","raw":"K. Nummiaro, E. Koller-Meier, and L. V. Gool, \\An adaptive color-based particle \u00aflter,&quot; Image and Vision Computing, vol. 21, no. 1, pp. 99{110, 2003.","cites":null},{"id":16344006,"title":"Bayesian fusion of colour and texture segmentations,&quot;","authors":[],"date":"1999","doi":"10.1109\/iccv.1999.790351","raw":"R. Manduchi, \\Bayesian fusion of colour and texture segmentations,&quot; in Proc. of the IEEE International Conference on Computer Vision, September 1999.","cites":null},{"id":16343978,"title":"Colorbased probabilistic tracking,&quot;","authors":[],"date":"2002","doi":null,"raw":"P. P\u00b6 erez, C. Hue, J. Vermaak, and M. Ganget, \\Colorbased probabilistic tracking,&quot; in Proceedings of the 7th European Conference on Computer Vision. Vol. 2350, LNCS, Copenhagen, Denmark, May 2002, pp. 661{675.","cites":null},{"id":16343995,"title":"Condensation - conditional density propogation for visual tracking,&quot;","authors":[],"date":"1998","doi":null,"raw":"M. Isard and A. Blake, \\Condensation - conditional density propogation for visual tracking,&quot; International Journal of Computer Vision, vol. 29, no. 1, pp. 5{28, 1998.","cites":null},{"id":16343962,"title":"Condensation { conditional density propagation for visual tracking,&quot;","authors":[],"date":"1998","doi":null,"raw":"M. Isard and A. Blake, \\Condensation { conditional density propagation for visual tracking,&quot; International Journal of Computer Vision, vol. 28, no. 1, pp. 5{28, 1998. 11[2] N. Gordon, D. Salmond, and A. Smith, \\A novel approach to nonlinear \/ non-Gaussian Bayesian state estimation,&quot; IEE Proceedings-F, vol. 140, pp. 107{113, April 1993.","cites":null},{"id":16343973,"title":"Data fusion for tracking with particles,&quot;","authors":[],"date":"2004","doi":"10.1109\/jproc.2003.823147","raw":"P. P\u00b6 erez, J. Vermaak, and A. Blake, \\Data fusion for tracking with particles,&quot; Proceedings of the IEEE, vol. 92, no. 3, pp. 495{513, March 2004.","cites":null},{"id":16343985,"title":"Democratic integration: Self-organized integration of adaptive cues,&quot;","authors":[],"date":"2001","doi":"10.1162\/089976601750399308","raw":"J. Triesch and C. von der Malsburg, \\Democratic integration: Self-organized integration of adaptive cues,&quot; Neural Computation, vol. 13, no. 9, pp. 2049{2074, 2001.","cites":null},{"id":16344009,"title":"Estimation with Applications to Tracking and Navigation.","authors":[],"date":"2001","doi":"10.1002\/0471221279","raw":"Y. Bar-Shalom, X. R. Li, and T. Kirubarajan, Estimation with Applications to Tracking and Navigation. John Wiley and Sons, 2001.","cites":null},{"id":16343980,"title":"Hybrid Monte Carlo \u00afltering: Edgebased tracking people,&quot;","authors":[],"date":"2002","doi":"10.1109\/motion.2002.1182228","raw":"E. Poon and D. Fleet, \\Hybrid Monte Carlo \u00afltering: Edgebased tracking people,&quot; in Proceedings of the IEEE Workshop on Motion and Video Computing, Orlando, Florida, Dec. 2002, pp. 151{158.","cites":null},{"id":16343982,"title":"Incremental focus of attention for robust vision-based tracking,&quot;","authors":[],"date":"1999","doi":"10.1109\/cvpr.1996.517073","raw":"K. Toyama and G. Hager, \\Incremental focus of attention for robust vision-based tracking,&quot; International Journal of Computer Vision, vol. 35, no. 1, pp. 45{63, 1999.","cites":null},{"id":16344007,"title":"Integration of color, edge, shape and texture features for automatic region-based image annotation and retrieval,&quot;","authors":[],"date":"1998","doi":"10.1117\/1.482605","raw":"E. Saber and A. M. Tekalp, \\Integration of color, edge, shape and texture features for automatic region-based image annotation and retrieval,&quot; Journal of Electronic Imaging (Special Issue), vol. 7, no. 3, pp. 684{700, 1998.","cites":null},{"id":16343994,"title":"Kalman Filtering and Neural Networks.","authors":[],"date":"2001","doi":"10.1002\/0471221546.ch7","raw":"E. Wan and R. van der Merwe, Kalman Filtering and Neural Networks. Wiley Publishing, September 2001, chapter 7. the Unscented Kalman \u00aflter 7, pp. 221{280.","cites":null},{"id":16343969,"title":"Kernel-based object tracking,&quot;","authors":[],"date":"2003","doi":null,"raw":"||, \\Kernel-based object tracking,&quot; IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 25, no. 5, pp. 564{577, 2003.","cites":null},{"id":16344011,"title":"Mean shift: A robust approach toward feature space analysis,&quot;","authors":[],"date":"2002","doi":"10.1109\/34.1000236","raw":"D. Comaniciu and P. Meer, \\Mean shift: A robust approach toward feature space analysis,&quot; IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 22, no. 5, pp. 603{619, 2002.","cites":null},{"id":16344005,"title":"Multivariate Density Estimation: Theory, Practice and Visualization, ser. Probability and Mathematical Statistics.","authors":[],"date":"1992","doi":"10.1002\/9780470316849.ch3","raw":"D. Scott, Multivariate Density Estimation: Theory, Practice and Visualization, ser. Probability and Mathematical Statistics. John Wiley and Sons, 1992.","cites":null},{"id":16344004,"title":"On a measure of divergence between two statistical populations de\u00afned by their probability distributions,&quot;","authors":[],"date":"1943","doi":null,"raw":"A. Bhattacharayya, \\On a measure of divergence between two statistical populations de\u00afned by their probability distributions,&quot; Bulletin of the Calcutta Mathematical Society, vol. 35, pp. 99{110, 1943.","cites":null},{"id":16343963,"title":"On sequential Monte Carlo sampling methods for Bayesian \u00afltering,&quot;","authors":[],"date":"2000","doi":null,"raw":"A. Doucet, S. Godsill, and C. Andrieu, \\On sequential Monte Carlo sampling methods for Bayesian \u00afltering,&quot; Statistics and Computing, vol. 10, no. 3, pp. 197{208, 2000.","cites":null},{"id":16343989,"title":"Particle \u00afltering with multiple cues for object tracking in video sequences,&quot; in","authors":[],"date":"2005","doi":"10.1117\/12.585882","raw":"P. Brasnett, L. Mihaylova, N. Canagarajah, and D. Bull, \\Particle \u00afltering with multiple cues for object tracking in video sequences,&quot; in Proc. of SPIE's 17th Annual Symposium on Electronic Imaging, Science and Technology, V. 5685, 2005, pp. 430{441.","cites":null},{"id":16343971,"title":"Probabilistic multiple cue integration for particle \u00aflter based tracking,&quot; in","authors":[],"date":"2003","doi":null,"raw":"C. Shen, A. van den Hengel, and A. Dick, \\Probabilistic multiple cue integration for particle \u00aflter based tracking,&quot; in Proc. of the VIIth Digital Image Computing: Techniques and Applications. C. Sun, H. Talbot, S. Ourselin, T. Adriansen, Eds., 10-12 Dec. 2003.","cites":null},{"id":16343967,"title":"Real-time tracking of non-rigid objects using mean shift,&quot;","authors":[],"date":"2000","doi":"10.1109\/cvpr.2000.854761","raw":"D. Comaniciu, V. Ramesh, and P. Meer, \\Real-time tracking of non-rigid objects using mean shift,&quot; in Proceedings of the 1st Conference on Computer Vision and Pattern Recognition. Hilton Head, SC, 2000, pp. 142{149.","cites":null},{"id":16343991,"title":"Recursive Bayesian estimation: Navigation and tracking applications,&quot;","authors":[],"date":"1999","doi":null,"raw":"N. Bergman, \\Recursive Bayesian estimation: Navigation and tracking applications,&quot; Ph.D. dissertation, Link\u00c4 oping University, Link\u00c4 oping, Sweden, 1999.","cites":null},{"id":16343987,"title":"Robust online appearance models for visial tracking,&quot;","authors":[],"date":"2003","doi":"10.1109\/tpami.2003.1233903","raw":"A. Jepson, D. Fleet, and T. F. El-Maraghi, \\Robust online appearance models for visial tracking,&quot; IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 25, no. 10, pp. 1296{1311, 2003.","cites":null},{"id":16343992,"title":"Sequential Monte Carlo methods for dynamic systems,&quot;","authors":[],"date":"1998","doi":"10.2307\/2669847","raw":"J. Liu and R. Chen, \\Sequential Monte Carlo methods for dynamic systems,&quot; Journal of the American Statistical Association, vol. 93, no. 443, pp. 1032{1044, 1998. [Online]. Available: citeseer.nj.nec.com\/article\/liu98sequential.html","cites":null},{"id":16343999,"title":"Texture Classi\u00afcation and Segmentation, PhD Thesis,","authors":[],"date":"1997","doi":null,"raw":"R. Porter, Texture Classi\u00afcation and Segmentation, PhD Thesis, University of Bristol, Center for Communications Research, 1997.","cites":null},{"id":16344001,"title":"The design and use of steerable \u00aflters,&quot;","authors":[],"date":"1991","doi":null,"raw":"W. Freeman and E. Adelson, \\The design and use of steerable \u00aflters,&quot; in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 13, 1991.","cites":null},{"id":16343974,"title":"Towards robust multicue integration for visual tracking,&quot;","authors":[],"date":"2003","doi":"10.1007\/s00138-002-0095-9","raw":"M. Spengler and B. Schiele, \\Towards robust multicue integration for visual tracking,&quot; Machine Vision and Applications, vol. 14, no. 1, pp. 50{58, 2003.","cites":null},{"id":16343997,"title":"Unifying low-level and high-level tracking in a stochastic framework,&quot;","authors":[],"date":"1998","doi":"10.1007\/bfb0055711","raw":"||, \\Icondensation: Unifying low-level and high-level tracking in a stochastic framework,&quot; in Proceedings of the 5th European Conference on Computer Vision, vol. 1, 1998, pp. 893{908.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2007-08","abstract":"This paper presents visual cues for object tracking in video sequences using particle filtering. A consistent histogram-based framework is developed for the analysis of colour, edge and texture cues. The visual models for the cues are learnt from the first frame and the tracking can be carried out using one or more of the cues. A method for online estimation of the noise parameters of the visual models is presented along with a method for adaptively weighting the cues when multiple models are used. A particle filter (PF) is designed for object tracking based on multiple cues with adaptive parameters. Its performance is investigated and evaluated with synthetic and natural sequences and compared with the mean-shift tracker. We show that tracking with multiple weighted cues provides more reliable performance than single cue tracking","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/71506.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/819\/2\/image_vc_accepted.pdf","pdfHashValue":"b040b71d087bf4dfbf5978ebf55c360f5462ca5b","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:819<\/identifier><datestamp>\n      2018-01-24T03:16:56Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Sequential Monte Carlo tracking by fusing multiple cues in video sequences<\/dc:title><dc:creator>\n        Brasnett, P<\/dc:creator><dc:creator>\n        Mihaylova, L<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        This paper presents visual cues for object tracking in video sequences using particle filtering. A consistent histogram-based framework is developed for the analysis of colour, edge and texture cues. The visual models for the cues are learnt from the first frame and the tracking can be carried out using one or more of the cues. A method for online estimation of the noise parameters of the visual models is presented along with a method for adaptively weighting the cues when multiple models are used. A particle filter (PF) is designed for object tracking based on multiple cues with adaptive parameters. Its performance is investigated and evaluated with synthetic and natural sequences and compared with the mean-shift tracker. We show that tracking with multiple weighted cues provides more reliable performance than single cue tracking.<\/dc:description><dc:date>\n        2007-08<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/819\/2\/image_vc_accepted.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1016\/j.imavis.2006.07.017<\/dc:relation><dc:identifier>\n        Brasnett, P and Mihaylova, L (2007) Sequential Monte Carlo tracking by fusing multiple cues in video sequences. Image and Vision Computing, 25 (8). pp. 1217-1227. ISSN 0262-8856<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/819\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1016\/j.imavis.2006.07.017","http:\/\/eprints.lancs.ac.uk\/819\/"],"year":2007,"topics":["QA75 Electronic computers. Computer science"],"subject":["Journal Article","PeerReviewed"],"fullText":"SequentialMonteCarloTrackingby\nFusingMultipleCues inVideoSequences\nPaul Brasnett a, Lyudmila Mihaylova \u2217,b, David Bull a, Nishan Canagarajah a\naDepartment of Electrical and Electronic Engineering, University of Bristol, Bristol BS8 1UB, UK\nbDepartment of Communication Systems, Lancaster University, Lancaster LA1 4WA, UK\nAbstract\nThis paper presents visual cues for object tracking in video sequences using particle filtering. A consistent histogram-based\nframework is developed for the analysis of colour, edge and texture cues. The visual models for the cues are learnt from the\nfirst frame and the tracking can be carried out using one or more of the cues. A method for online estimation of the noise\nparameters of the visual models is presented along with a method for adaptively weighting the cues when multiple models are\nused. A particle filter (PF) is designed for object tracking based on multiple cues with adaptive parameters. Its performance\nis investigated and evaluated with synthetic and natural sequences and compared with the mean-shift tracker. We show that\ntracking with multiple weighted cues provides more reliable performance than single cue tracking.\nKeywords \u2013 particle filtering, tracking in video se-\nquences, colour, texture, edges, multiple cues, Bhat-\ntacharyya distance\n1 Introduction\nObject tracking is required in many vision applications\nsuch as human-computer interfaces, video communi-\ncation\/compression, road traffic control, security and\nsurveillance systems. Often the goal is to obtain a record\nof the trajectory of one or more targets over time and\nspace. Object tracking in video sequences is a challeng-\ning task because of the large amount of data used and\nthe common requirement for real-time computation.\nMoreover, most of the models encountered in visual\ntracking are nonlinear, non-Gaussian, multi-modal or\nany combination of these.\nIn this paper we focus on Monte Carlo methods (par-\nticle filtering) for tracking in video sequences. Particle\nfiltering, also known as the Condensation algorithm [1]\nand bootstrap filter [2], has recently been proven to be\na powerful and reliable tool for nonlinear systems [2\u20134].\nParticle filtering is a promising technique because of its\ninherent property to allow fusion of different sensor data,\n\u2217 Corresponding author\nEmail addresses: paul.brasnett@bristol.ac.uk\n(Paul Brasnett), mila.mihaylova@lancaster.ac.uk\n(Lyudmila Mihaylova).\nto account for different uncertainties, to cope with data\nassociation problems when multiple targets are tracked\nwith multiple sensors and to incorporate constraints.\nThey keep track of the state through sample-based rep-\nresentation of probability density functions. Here we de-\nvelop a particle filtering technique for object tracking\nin video sequences by visual cues. Further, methods are\npresented for combining the cues if they are assumed to\nbe independent. By comparing results from single-cue\ntracking with multiple cues we show that multiple com-\nplementary cues can improve the accuracy of tracking.\nThe features and their parameters are adaptively chosen\nbased on appropriately defined distance function. The\ndeveloped particle filter together with a mixed dynamic\nmodel enables recovery after a partial or full loss.\nDifferent algorithms have been proposed for visual track-\ning and their particularities are mainly application de-\npendent. Many of them rely on a single cue, which can\nbe chosen according to the application context, e.g. in [5]\na colour-based particle filter is developed. The colour-\nbased particle filter has been shown [5] to outperform the\nmean-shift tracker proposed in [6,7] in terms of reliabil-\nity, at the price of increased computational time. How-\never, both the particle filtering and mean shift tracking\nmethods have real-time capabilities. Colour cues form\na significant part of many tracking algorithms [5, 8\u201312],\nthe advantage of colour is that it is a weak model and\nis therefore unrestrictive about the type of objects be-\ning tracked. The main problem for tracking with colour\nalone occurs when the region around the target object\nPreprint submitted to Image and Vision Computing\ncontains objects with similar colour. When the region is\ncluttered in this way a single cue does not provide reliable\nperformance because it fails to fully model the target.\nStronger models have been used but they rely on off-line\nlearning and modelling of foreground and background\nmodels [1,13]. Multiple-cue tracking providesmore infor-\nmation about the object and hence there is less opportu-\nnity for clutter to influence the result. In [9] colour cues\nare combinedwithmotion and sound cues to provide bet-\nter results. Motion and sound are both intermittent cues\nand therefore cannot always be relied upon. Colour and\nshape cues are used in [8], where shape is described using\na parameterised rectangle or ellipse. The cues are com-\nbined by weighting each one based upon the performance\nin previous frames. A cue-selection approach to optimise\nthe use of the cues is proposed in [14] which is embedded\nin a hierarchical vision-based tracking algorithm. When\nthe target is lost, layers cooperate to perform a rapid\nsearch for the target and continue tracking. Another ap-\nproach, called democratic integration [15] implements\ncues concurrently where all vision cues are complemen-\ntary and contribute simultaneously to the overall result.\nThe integration is performed through saliency maps and\nthe result is a weighted average of saliency maps. Ro-\nbustness and generality are major features of this ap-\nproach resulting from the combination of the cues. Adap-\ntion schemes are sometimes used to handle changes to\nthe appearance of the object being tracked [16]. Care-\nful design has to make a trade off between adaption to\nrapidly changing appearance with adapting too quickly\nto incorrect regions. The work here does not involve an\nadaption scheme but an existing scheme (e.g. [16]) could\nbe adopted within the framework. In this paper we also\nshow that tracking with multiple weighted cues provides\nmore reliable and accurate results. A framework is sug-\ngested for combining colour, texture and edge cues to\nprovide robust and accurate tracking without the need\nfor extensive off-line modelling. It is an extension and\ngeneralisation of the results reported in [17], with colour\nand texture only. A comparison is presented in [17] of a\nparticle filter with a Gaussian sum particle filter, work-\ning separately with colour, with texture, and both with\ncolour and texture features. Adaptive colour and texture\nsegmentation for tracking moving objects is proposed\nin [11]. Texture is modelled by an autobinomial Gibbs\nMarkov random field, whilst colour is modelled by a two-\ndimensional Gaussian distribution. In our paper texture\nis represented using a steerable pyramid decomposition\nwhich is a different approach to [11] but is related to the\nwork in [16]. Additionally, in [11] segmentation-based\ntracking with a Kalman filter is considered instead of\nfeature-based tracking proposed here.\nThe paper is organised as follows. Section 2 states the\nproblem of visual tracking within a sequential Monte\nCarlo framework and presents the particle filter (PF)\nbased on single or multiple information cues. Section 3\nintroduces the model of the region of interest. Section 4\ndescribes the cues and their likelihoods used for the\ntracking process. Methods for adaptively changing the\ncues\u2019 noise parameters and adaptively weighting the cues\nare presented. The overall likelihood function of the par-\nticle filter represents a product of the separate cues. Sec-\ntion 5 investigates the particle filter performance and\nvalidates it over different scenarios. We show the ad-\nvantages of fusing multiple cues compared to single-cue\ntracking using synthetic and natural video sequences. A\ncomparison with the mean-shift algorithm is performed\nover natural video sequences and their computational\ntime is characterised. Results are also presented for par-\ntial and full occlusions. Finally, Section 6 discusses the\nresults and open issues for future research.\n2 Sequential Monte Carlo Framework\nThe aim of sequential Monte Carlo estimation is to eval-\nuate the posterior probability density function (pdf)\np(xk|Zk) of the state vector xk \u2208 Rnx , with dimension\nnx, given a set Zk = {z1, . . . , zk} of sensor measure-\nments up to time k. The Monte Carlo approach relies on\na sample-based construction to represent the state pdf.\nMultiple particles (samples) of the state are generated,\neach one associated with a weight W (`)k which charac-\nterises the quality of a specific particle `, ` = 1, 2, . . . , N .\nAn estimate of the variable of interest is obtained by\nthe weighted sum of particles. Two major stages can be\ndistinguished : prediction and update. During prediction,\neach particle is modified according to the state model\nof the region of interest in the video frame, including\nthe addition of random noise in order to simulate the\neffect of the noise on the state. In the update stage, each\nparticle\u2019s weight is re-evaluated based on the new data.\nAn inherent problem with particle filters is degeneracy,\nthe case when only one particle has a significant weight.\nAn estimate of the measure of degeneracy [18] at time k\nis given as\nNeff =\n1\u2211N\n`=1(W\n(`)\nk )\n. (1)\nIf the value of Neff is below a user defined threshold\nNthres a resampling procedure can help to avoid degen-\neracy by eliminating particles with small weights and\nreplicating particles with larger weights.\n2.1 AParticle Filter for Object TrackingUsingMultiple\nCues\nWithin the Bayesian framework, the conditional pdf\np(xk+1|Zk) is recursively updated according to the\nprediction step\np(xk+1|Zk) =\n\u222b\nRnx\np(xk+1|xk)p(xk|Zk)dxk (2)\n2\nand the update step\np(xk+1|Zk+1) = p(zk+1|xk+1)p(xk+1|Z\nk)\np(zk+1|Zk)\n(3)\nwhere p(zk+1|Zk) is a normalising constant. The recur-\nsive update of p(xk+1|Zk+1) is proportional to\np(xk+1|Zk+1) \u221d p(zk+1|xk+1)p(xk+1|Zk). (4)\nTable 1: The Particle Filter with Multiple Cues\nInitialisation\n(1) For ` = 1, . . . , N , generate samples {x(`)0 } from the\nprior distribution p(x0). Set initial weights\nW\n(`)\n0 = 1\/N .\nFor k = 0, 1, 2, . . . ,\nPrediction Step\n(2) For ` = 1, . . . , N , sample\nx\n(`)\nk+1 \u223c p(xk+1|x(`)k )\nfrom the dynamic model presented in Section 3.\nMeasurement Update: evaluate the importance\nweights\n(3) Compute the weights\nW\n(`)\nk+1 \u221dW (`)k L(zk+1|x(`)k+1).\nbased on the likelihood L(zk+1|x(`)k+1) given in Sec-\ntion 4.\n(4) Normalise the weights, W\u0302 (`)k+1 =\nW\n(`)\nk+1\u2211N\n`=1\nW\n(`)\nk+1\n.\nOutput\n(5) The state estimate x\u02c6k+1 is the probabilistically av-\neraged sum\nx\u02c6k+1 =\nN\u2211\n`=1\nW\u0302\n(`)\nk+1x\n(`)\nk+1.\n(6) Estimate the effective number of particles Neff\nNeff =\n1\u2211N\n`=1(W\u0302\n(`)\nk+1)2\nIf Neff \u2264 Nthres then perform resampling\nResampling Step\n(7) Multiply\/suppress samplesx(`)k+1 with high\/ low im-\nportance weights W\u0302 (`)k+1.\n(8) For ` = 1, . . . , N , set W (`)k+1 = W\u0302\n(`)\nk+1 = 1\/N .\nUsually, there is no simple analytical expression for prop-\nagating p(xk+1|Zk+1) through (4) so numerical meth-\nods are used.\nIn the particle filter approach, a set of N weighted\nparticles, drawn from the posterior conditional pdf, is\nused to map integrals to discrete sums. The posterior\np(xk+1|Zk+1) is approximated by\np\u02c6(xk+1|Zk+1) \u2248\nN\u2211\n`=1\nW\u0302\n(`)\nk+1\u03b4(xk+1 \u2212 x(`)k+1) (5)\nwhere W\u0302 (`)k are the normalised importance weights. New\nweights are calculated, putting more weight on particles\nthat are important according to the posterior pdf (5).\nIt is often impossible to sample directly from the pos-\nterior density function p(xk+1|Zk+1). This difficulty is\ncircumvented by making use of the importance sampling\nfrom a known proposal distribution p(xk+1|xk). The par-\nticle filter is given in Table 1. The residual resampling\nalgorithm described in [19, 20] is applied at step (7).\nThis is a two step process making use of the sampling-\nimportance-resampling scheme.\n3 Dynamic Models\nThe considered model for the moving object provides\ninvariance to different motions, such as translations, ro-\ntations, and to changes in the object size. This allows to\ncover the different types of motion of the object, also the\ncase when the object size varies considerably (the ob-\nject get closer to the camera or moves far away from it)\nand hence ensures reliable performance of the PF. In our\nparticular implementation two generic models are used.\nWe adopted a constant velocity model for the transla-\ntional motion and the random walk model for the rota-\ntion and scaling. A mixed dynamic motion model is also\npresented which allows more than one model to be used\nfor dealing with occlusions.\nFor the purpose of tracking an object in video sequences\nwe initially choose a region which defines the object.\nThe shape of this region is fixed a priori and here is a\nrectangular box.\nDenote by (x, y) the coordinates of the centre of the rect-\nangular region, by \u03b8 the angle through which the region\nis rotated, and by s the scale, (x\u02d9, y\u02d9) are the respective\nvelocity components.\n3.1 Constant Velocity Model for Translational Motion\nThe translational motion of the region of interest in\nx direction can be described by a constant velocity\n3\nmodel [21]\nxk+1 = Fxk +wk, wk v N (0,Q), (6)\nwhere the state vector is x = (x, x\u02d9)T . The matrix F\nF =\n(\n1 T\n0 1\n)\n,\ndescribes the dynamics of the state over time and T is the\nsampling interval. The system noisewk is assumed to be\na zero-mean white Gaussian sequence, wk v N (0,Q),\nwith the covariance matrix\nQ =\n(\n1\n4T\n4 1\n2T\n3\n1\n2T\n3 T 2\n)\n\u03c32 (7)\nand \u03c3 is the noise standard deviation.\n3.2 RandomWalk Model for Rotational Motion and for\nthe Scale\nA random walk model propagates the state x = (\u03b8, s)T\nby\nxk+1 = xk +wk, (8)\nwhere wk v N (0,Q) is a zero-mean Gaussian noise,\nwith covariancematrixQ = diag{\u03c32\u03b8 , \u03c32s }, describing the\nuncertainty in the state vector.\n3.3 Multi-Component State\nThe motion of the object being tracked is described us-\ning the translation (x, y), rotation (\u03b8) and scaling (s)\ncomponents. The translation components are modelled\nusing the constant velocity model (6) and the rotation\nand scaling components are modelled using the random\nwalk model (8). The full augmented state of the region\nis then given as\nx = (x, x\u02d9, y, y\u02d9, \u03b8, s)T . (9)\nThe dynamics of the full state can then be modelled as\nxk+1 = Gxk +wk, wk v N (0,Q), (10)\nwhere the matrix G has the form\nG =\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nF 02\u00d72 02\u00d71 02\u00d71\n02\u00d72 F 02\u00d71 02\u00d71\n01\u00d72 01\u00d72 1 0\n01\u00d72 01\u00d72 0 1\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 . (11)\nThe covariance matrix of the zero-mean Gaussian is\nQ =\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nQx 02\u00d72 02\u00d71 02\u00d71\n02\u00d72 Qy 02\u00d71 02\u00d71\n01\u00d72 01\u00d72 \u03c32\u03b8 0\n01\u00d72 01\u00d72 0 \u03c32s\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 , (12)\nwhere Qx is the covariance matrix of the constant-\nvelocity model for the x component (7), Qy is the co-\nvariance matrix of the y component and \u03c32\u03b8 and \u03c3\n2\ns are\nthe covariances for the rotation (\u03b8) and scaling (s).\n3.4 Mixed Dynamic Model\nA mixed-dynamic model allows the system to be de-\nscribed throughmore than one dynamicmodel [16,22,23]\nand provides abilities to the tracking algorithm to cope\nwith occlusions. Here, we make use of two models: the\nconstant velocity model as given in Section 3.1 and the\nother is the reinitialisationmodel described below.When\nthe object is occluded the tracker might lose it temporar-\nily. In this case the reininialisation model that ensures\nuniform spread of particles along the image guarantees\nrobustness. The new location of the object is recovered\nafter processing the information from separate cues in\nthe measurement update step.\nSamples are generated from the constant velocity model\nwith probability j, set a priori, and from the reinitialisa-\ntion model with probability 1\u2212j. Hence, from the mixed\nmodel samples are generated by the following steps:\n(1) Generate a number \u03b3 \u2208 [0, 1) from a uniform dis-\ntribution U\n(2) If \u03b3 > j, then sample from the constant velocity\nmodel (6)\n(3) else use the reinitialisation model\np(xk+1|xk) \u223c U(0,xmax). (13)\nwhere xmax is a vector with the maximum allowed values\nfor the state vector components. This is repeated until\nthe required number of samples are obtained.\n4 Likelihood Models\nThis section describes how we model the separate cues\nof the rectangular region Sx, surrounding the moving\nobject and the likelihood models of the cues. One of\nthe particularities of tracking moving objects in video\nsequences compared to other tracking problems, such\nas tracking of airborne targets with radar data, is that\nthere are no measurement models in explicit form. The\nestimated state variables of the object are connected to\n4\nfeatures of the video sequences. Practically, the likeli-\nhood models of the features provide information about\nthe changes in the motion of the object.\nAll of the models are based on histograms. Histograms\nhave the useful property that they allow some change in\nthe object appearance without changing the histogram.\n4.1 Colour Cue\nColour cues are flexible in the type of object that they\ncan be used to track. However, the main drawbacks of\ncolour cues are:\n\u2022 the effect of other similar coloured regions and\n\u2022 the lack of discrimination with respect to rotation (ob-\nvious on Fig. 1).\nA histogram, hx = (h1,x, . . . , hBC ,x), for a region Sx\ncorresponding to a state x is given by\nhi,x =\n\u2211\nu\u2208Sx\n\u03b4i(bu), i = 1 . . . BC (14)\nwhere \u03b4i is the Kronecker-delta function at the bin index\ni, bu \u2208 {1, . . . , BC} is the histogram bin index associated\nwith the intensity at pixel location u = (x, y) and BC is\nthe number of bins in each colour channel. The histogram\nis normalised such that\n\u2211BC\ni=1 hi,x = 1.\nA histogram is constructed for each channel in the colour\nspace. For example, we use 8x8x8 bin histograms in the\nthree channels of red, green, blue (RGB) colour space [9],\nother colour spaces could be used to improve robustness\nto illumination or appearance changes.\n4.2 Texture Cue\nAlthough there is no unique definition of texture, it is\ngenerally agreed that texture describes the spatial ar-\nrangements of pixel levels in an image, which may be\nstochastic or periodic, or both [24]. Texture can be qual-\nitatively characterised such as fine, coarse, grained and\nsmooth. When a texture is viewed from a distance it may\nappear to be fine, however, when viewed from close up\nit may appear to be coarse.\nThe texture description used for this work is based on\nsteerable pyramid decomposition [25]. The first deriva-\ntive filter as developed in [26] is steered to 4 orientations\nat two scales (subsampled by a factor of two). A his-\ntogram is then constructed for each of the 8 bandpass\nfilter outputs\nti,x =\n\u2211\nu\u2208Sx\n\u03b4i(tu), i = 1, . . . , BT (15)\n(a) Frame 1\n(b) Frame 40\n(c) Frame 70\nFig. 1. Colour cues provide a flexible model for tracking but\nlack discrimination with respect to rotation. The time board\nis tracked with colour cues, it can be seen that at frame 40\n(b) the region has rotated, this has got worse by frame 70 (c).\nwhere tu \u2208 {1, . . . , BT } is the histogram bin index asso-\nciated with the steerable filter output \u03b8\u02c6 at pixel location\nu, withBE number of bins. The histogram is normalised\nsuch that\n\u2211BE\ni=1 ei,x = 1.\n4.3 Edge Cue\nEdge cues are useful for modelling the structure of the\nobject to be tracked. The edges are described using a his-\ntogram based on the estimated edge direction. Given an\nimage region Sx the intensity of the pixels in that region\nare I(Sx). The edge images are constructed by estimat-\ning the gradients \u2202I\u2202x and\n\u2202I\n\u2202y in the x and y directions\nrespectively by Prewitt operators. The edge strength m\n5\nand direction \u03b8 are then approximated as\nm(u) =\n\u221a\n\u2202I\n\u2202x\n+\n\u2202I\n\u2202y\n, \u03b8(u) = tan\u22121\n(\n\u2202I\n\u2202y\n\/\n\u2202I\n\u2202x\n)\n. (16)\nThe edge direction is filtered to include only edges with\nmagnitude above a predefined threshold\n\u03b8\u02c6(u) =\n{\n\u03b8(u), m(u) > threshold\n0, otherwise.\n(17)\nA histogram ei,x of the edge directions \u03b8\u02c6 is then con-\nstructed\nei,x =\n\u2211\nu\u2208Sx\n\u03b4i(bu), i = 1, . . . , BE (18)\nwhere bu \u2208 {1, . . . , BE} is the histogram bin index as-\nsociated with the thresholded edge gradient \u03b8\u02c6 at pixel\nlocation u, with BE number of bins. The histogram is\nnormalised such that\n\u2211BE\ni=1 ei,x = 1.\n4.4 Weighted Histograms\nThe above histograms discard all information about the\nspatial arrangement of the features in the image. An\nalternative approach that incorporates the pixel distri-\nbution can help to give better performance [5]. More\nspecifically we can give greater weighting to pixels in the\ncenter of the image region. This weighting can be done\nthrough the use of a convex and monotonically decreas-\ning kernel, for example the Epanechnikov kernel [5] or\nthe elliptical Gaussian function\nK(u) =\n1\n2pi\u03c1x\u03c1y\nexp\n(\n\u2212 (x\u2212 x\u02c6)\n2\n2\u03c12x\n+\n(y \u2212 y\u02c6)2\n2\u03c12y\n)\n(19)\nwhere the values \u03c12x and \u03c12y control the spatial signifi-\ncance of the weighting function in the x and y directions\nand the centre pixel in the target region is at (x\u02c6, y\u02c6). This\nkernel can be used to weight the pixel when extracting\nthe histogram\nhi,x =\n\u2211\nu\u2208Sx\nK (u) \u03b4i(bu), i = 1 . . . BC (20)\nThe histogram is normalised so that\n\u2211BC\ni=1 hi,x = 1.\n4.5 Distance Measure\nThe Bhattacharyya measure [27] has been used previ-\nously for colour cues [5, 7] because it has the important\nproperty that \u03c1(p, p) = 1. In the case here the distri-\nbutions for each cue are represented by the respective\nhistograms [28]\n\u03c1(href,htar) =\nB\u2211\ni=1\n\u221a\nhref,ihtar,i, (21)\nwhere two normalised histograms htar and href describe\nthe cues for a target region defined in the current frame\nand a reference region in the first frame respectively. The\nmeasure of the similarity between these two distributions\nis then given by the Bhattacharyya distance\nd(href,htar) =\n\u221a\n1\u2212 \u03c1(href,htar). (22)\nThe larger the measure \u03c1(href,htar) is, the more similar\nthe distributions are. Conversely, for the distance d, the\nsmaller the value the more similar the distributions (his-\ntograms) are. For two identical normalised histograms\nwe obtain d = 0 (\u03c1 = 1) indicating a perfect match.\nBased on (22) a distance D2 for colour can be defined\nthat takes into account all of the colour channels\nD2(href,htar) =\n1\n3\n\u2211\nc\u2208{R,G,B}\nd2(hcref,h\nc\ntar) (23)\nthe distance D2 for the edges is equal to d2 since there\nis only one component. The distance D2 for texture is\nD2(href,htar) =\n1\n8\n\u2211\n\u03c9\u2208{1,...,8}\nd2(h\u03c9ref,h\n\u03c9\ntar) (24)\nwhere \u03c9 is the channel in the steerable-pyramid decom-\nposition.\nThe likelihood function for the cues can be defined by [9]\nL(z |x) \u221d exp\n(\n\u2212D\n2(href,hx)\n2\u03c32\n)\n(25)\nwhere the standard deviation \u03c3 specifies the Gaussian\nnoise in the measurements. Note that small Bhat-\ntacharyya distances correspond to large weights in the\nparticle filter. The choice of an appropriate value for \u03c3 is\nusually left as a design parameter, a method for setting\nand adapting the value is proposed in Section 4.6.\n4.6 Dynamic Parameter Setting\nFigure 2 shows the likelihood surface for a single cue,\napplied to one frame, with different values of \u03c3. It can\nbe seen that as \u03c3 is varied the likelihood surface changes\nsignificantly. The likelihood becomes more discriminat-\ning as the value of \u03c3 is decreased. However, if \u03c3 is too\nsmall and there has been some change in the appearance\nof the object, due to noise, then the likelihood function\n6\nmay have all values near to or equal to zero. The value\nof the noise parameter \u03c3 has a major influence on the\nproperties of the likelihood (25). Typically, the choice of\nthis value is left as a design parameter to be determined\nusually by experimentation. For a well constrained prob-\nlem, such as face tracking, analysis can be performed\noff-line to determine an appropriate value. However, if\nthe algorithm is to be used to track a priori unknown\nobjects, it may not be possible to determine one value\nfor all objects. To overcome the problems of choosing an\nappropriate value for \u03c3, an adaptive scheme is presented\nhere which aims to maximise the information available\nin the likelihood, using the Bhattacharyya distance d.\nWe define the minimum squared distance D2l,min as the\nminimum distance D2 of the set of distance calculated\nfor all particles with a particular cue l (l = 1, 2, . . . , L)\nwhere L is the number of cues. Rearranging the likeli-\nhood yields\nlog(L) = \u2212D\n2\n2\u03c32\n(26)\nfrom which we can get\n\u03c3 =\n\u221a\n2\n2\n\u221a\n\u2212D\n2\nl,min\nlog(L) . (27)\nFor example if the choice is made to maximise the in-\nformation by setting \u03c3 to give a maximum likelihood\nlog(L) = \u22121 (L \u2248 0.36) then \u03c3 = (\n\u221a\n2D2l,min)\/2.\n4.7 Multiple Cues\nThe relationship between different cues has been treated\ndifferently by different authors. For example, [29] makes\nthe assumption that colour and texture are not indepen-\ndent. However, other works [11, 30] assume that colour\nand texture cues are independent. For the purposes of\nimage classification the independence assumption be-\ntween colour and texture is applied for feature fusion in\na Bayesian framework [31].There is generally agreement\nthat in practice colour and texture and colour and edges\ndo combine well together.\nWe assume that the considered cue combinations, colour\nand texture and colour and edges are independent. With\nthis assumption the overall likelihood function of the\nparticle filter represents a product of the likelihoods of\nthe separate cues\nLfused(zk|xk) =\nL\u220f\nl=1\nLl(zl,k|xk)\u00b2l (28)\nThe cues are adaptively weighted by weighting coeffi-\ncients \u00b2l, zk denotes the measurement vector, composed\nof the measurement vectors zl,k from the lth cue for\nl = 1, . . . , L.\n(a) \u03c3 = 0.30\n(b) \u03c3 = 0.17\nFig. 2. The effect of the \u03c3 value on the cues. The results\nshown are for the colour cue, however, similar results apply\nfor texture and edges. It can be seen that as \u03c3 decreases\nthere is more discrimination in the likelihood. As \u03c3 becomes\nsmaller the likelihood tends to zero. A method for dynamic\nsetting of \u03c3 is presented in Section 4.6\n4.8 Adaptively Weighted Cues\nA method is presented here which takes account of the\nBhattacharyya distance (22) to give some significance to\nthe likelihood obtained for each cue based on the current\nframe. This is different to previous works which use the\nperformance of the cues over the previous frames [9], not\ntaking into account information from the latest measure-\nments. This allows an estimate to be made for \u00b2l in (28).\nUsing the smallest value of the distance measure D2l,min\nfor each cue the weight for each cue l is determined by\n\u00b2\u02c6l =\n1\nD2l,min\n, l = 1, . . . , L. (29)\nThe weights are then normalised such that\n\u2211L\nl=1 \u00b2l = 1\n\u00b2l =\n\u00b2\u02c6l\u2211L\nl=1 \u00b2\u02c6l\n, l = 1, . . . , L. (30)\n7\n0 5 10 15 20 25 30 35 40 45 50\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\nFrames\nR\nM\nSE\nxy\nN\nA\u03c3\nWH\nWH & A\u03c3\nFig. 3. Results from: 1) nonadaptive cues (N), 2) adaptive\ncues with automatic setting of \u03c3 (A\u03c3), 3) Gaussian weighting\nkernel for the histogram (WH), 4) both WH & A\u03c3.\n5 Experimental Results\nThis section evaluates the performance of : i) the colour,\ntexture and edge cues ii) combined cues and iii) the\nmixed-state model in sequences with occlusion.\nThe combined root mean squared error (RMSE) [32]\nRMSExy =\n\u221a\u221a\u221a\u221a 1\nR\nR\u2211\ni=0\n(xi \u2212 x\u02c6i)2 + (yi \u2212 y\u02c6i)2 (31)\nof the pixel coordinates (xi, yi) to their estimates (x\u02c6i, y\u02c6i)\nis the measure used to evaluate the performance of the\ndeveloped technique in each frame i = 1, . . . , Nf over\nR = 100 independent Monte Carlo realisations.\n5.1 Dynamic \u03c3 and Weighted Histograms\nTwo techniques were given in this paper to improve the\nperformance of the cues: i) automatic setting of the\nnoise parameters \u03c3 for the likelihood (Section 4.6) and\nii) weighting of the pixels in the histogram extraction\nprocess (Sections 4.1, 4.2 and 4.3). The effect of these\ntechniques can be seen in Fig. 3, where the RMSExy is\nshown for 100 realisations each with N = 500 particles,\nNthresh = N\/2 for tracking in a synthetic sequence us-\ning colour cues. Four different implementations are com-\npared: 1) nonadaptive cues (N), 2) adaptive cues with\nautomatic setting of \u03c3 (A\u03c3), 3) Gaussian weighting ker-\nnel for the histogram (WH), 4) and both WH & A\u03c3.\nThe automatic setting of \u03c3 and the use of a Gaussian\nweighting kernel for the histogram both provide an im-\nprovement. The smallest error is seen when the Gaus-\nsian weighting kernel for the histogram is combined with\nautomatic setting of \u03c3 (WH & A\u03c3).\n5.1.1 Single Cues\nThree very different tracking scenarios are used to high-\nlight some of the strengths and weaknesses of the in-\ndividual cues. All of the results are obtained using 500\nparticles. The first sequence is a wildlife problem which\ninvolves tracking a penguin [33] moving across a snowy\nbackground, Fig. 4. As is common in wildlife scenarios\nthe colour of the object to be tracked is similar to the\nbackground. The particle filter with colour cues (Fig. 4\n(a)-(c)) is distracted by similar coloured background re-\ngions. Both the particle filter with edge cues (Fig. 4 (d)-\n(f)) and with texture cues (Fig. 4 (g)-(i)) perform much\nbetter and track the penguin successfully.\n(a) Frame 1 (b) Frame 37 (c) Frame 61\n(d) Frame 1 (e) Frame 35 (f) Frame 61\n(g) Frame 1 (h) Frame 37 (i) Frame 61\nFig. 4. Tracking a penguin against a snowy background. The\ncolour cues (a)-(c) are distracted by the snowy background.\nUsing either the edge (d)-(f) or the texture cues (g)-(i) pro-\nvides an improvement.\nThe second sequence is a car tracking [33] problem with\nthe car undergoing a significant and rapid change in\nscale. It can be seen that despite the change in scale both\nthe particle filters with colour (Fig. 5 (a)-(c)) and edge\ncues (Fig. 5 (d)-(f)) are able to keep track of the full state\nof the car. However, the texture cues (Fig. 5 (g)-(i)) fail\nbecause of the change in appearance and distractions in\nthe background.\nThe final example from single cues is an example of\ntracking a logo, in a longer sequence, that is undergo-\ning translation, rotation and some small amount of scale\nchange. All three cues are able to keep track of the lo-\ncation of the sequence but the particle filter with colour\ncues (Fig. 6 (a)-(c)) does not provide accurate state in-\nformation for the rotation of the object. The particle fil-\n8\n(a) Frame 1 (b) Frame 26 (c) Frame 91\n(d) Frame 1 (e) Frame 26 (f) Frame 91\n(g) Frame 1 (h) Frame 26 (i) Frame 91\nFig. 5. Tracking a car, as it moves away it undergoes a signifi-\ncant change in scale. The colour (a)-(c) and edge (d)-(f) cues\nboth cope with the scale change. After the original object\nhas undergone some change the texture cues get distracted\nby the background.\nter with texture cues (Fig. 6 (d)-(f)) provides better in-\nformation about the rotation of the object and the par-\nticle filter with edge cues (Fig. 6 (g)-(i)) provides the\nmost accurate result of the three.\n5.1.2 Comparison with Mean-Shift Tracker\nAn alternative tracking technique that has received a\nconsiderable amount of research interest recently is the\nmean-shift tracker [6,34]. A comparison between the vi-\nsual particle filter presented here and the mean-shift\ntracker is particularly relevant because they are both\nbased on histogram analysis. The mean-shift tracker is a\nmode-finding technique that locates the local minimum\nof the posterior density function. Based on the mean-\nshift vector, received as an estimation of the gradient of\nthe Bhattacharyya function, the new object state esti-\nmate is calculated. The mean-shift algorithm was imple-\nmented with Epanechnikov kernel [6].\nA comparison between the results of the particle filter\nand the mean-shift tracker can be seen in Figure 7, both\nalgorithms use colour cues. The mean-shift tracker is un-\nable to track successfully as the hand is moved in front of\nthe face. The particle filter is slightly distracted by the\nface, however, it is successfully tracks the hand due to\nthe fact that it can maintain a multi-modal distribution\nfor a number of frames whereas the mean-shift tracker\nis not able to. This illustrates the superiority of the par-\nticle filter with respect to the mean-shift tracker in the\npresence of ambiguous situations.\n(a) Frame 1 (b) Frame 105 (c) Frame 290\n(d) Frame 1 (e) Frame 105 (f) Frame 290\n(g) Frame 1 (h) Frame 105 (i) Frame 290\nFig. 6. Logo tracking as it undergoes translation, rotation\nand mild scale change. The colour (a)-(c) cues are able to\nlocate the object but it does not successfully capture the\nobject rotation. The texture cues (g)-(i) provide more accu-\nrate information about the rotation and the edge cues (d)-(f)\nprovide the most accurate information about the rotation.\n(a) Frame 1 (b) Frame 66 (c) Frame 196\n(d) Frame 1 (e) Frame 66 (f) Frame 196\nFig. 7. Tracking a hand using colour cues to compare the\nperformance of the mean-shift tracker with the particle filter.\nThe mean shift(a)-(c) tracker gets distracted by the face and\ndoes not recover. The particle filter (d)-(f) is distracted by\nthe face but because it is able to maintain a multi-modal\ndistribution it is able to recover.\nThe results presented here were obtained from Matlab\nimplementation, where the particle filter takes in the or-\nder of 10 times longer than the mean shift tracker. The\nPF with colour cue was also implemented in C++ soft-\nware on a standard PC computer (with Pentium CPU\nand 2.66 GHz) and has shown abilities to process 25-30\nframes per second with particles in the range from 500 to\n100. This result shows the applicability of the PF to real-\ntime problems. The same algorithm, ran with the Mat-\n9\nlab code needs 8 times more computational time than\nits C++ version.\n5.1.3 Multiple Cues\nFrom the results presented in the previous section it can\nbe seen that no single cue can provide accurate results\nunder all conditions. In this section we look at the per-\nformance change when combining the cues.\nFirstly, the behaviour of the cue weighting scheme in-\ntroduced in Section 4.7 is explored with an example as\nshown in Fig. 8. In (d) the change in weights from edge to\ncolour can be seen. At the start of the sequence the edges\nprovide more accurate results and is therefore given a\nhigher weighting. As the players turn around the edges in\nthe scene change and therefore the model learnt from the\nfirst frame become less reliable. In contrast the colour of\nthe region does not change significantly and so becomes\nrelatively more reliable and is given a higher weighting.\nIn the previous section the PF with the colour cue failed\nto track the penguin successfully (Fig. 4), we now look\nat how the performance of the PF is effected if the colour\ncues are combined with the edge and texture informa-\ntion. It can be seen in Fig. 9 that the previous perfor-\nmance of the edge and texture cues is maintained even\nwhen the colour cues are combined with them. In a\nhand tracking scenario the particle filter with edge cues\n(Fig. 10 (a)-(c)) fails but the particle filter with colour\ncues (Fig. 10 (d)-(f))is successful. This is due to the fact\nthat the particle filter can maintain multi-modal pos-\nterior distributions. The particle filter with combined\ncolour and edge cues (Fig. 10 (g)-(i)) successfully tracks\nthe hand through the entire sequence.\n5.2 Occlusion Handling and Handling the Changeable\nWindow Size\nIt is important that the tracking process is robust to\nboth partial occlusions and is able to recover after a full\nocclusion. The sequence shown in Fig. 11 contains full\nocclusions from which the tracker successfully recovers.\nThis is due to the mixed-state model described in Sec-\ntion 3.4 that provides the ability to recover when the\nobject undergoes full occlusion or re-enters the frame af-\nter leaving. Additionally, the use of the Gaussian kernel\nenhances the accuracy when features are extracted from\nthe frame.\n6 Conclusions and Future Work\nThis paper has presented a sequential Monte Carlo tech-\nnique for object tracking in a broad range of video se-\nquences with visual cues. The visual cues, colour, edge,\nand texture, form the likelihood of the developed par-\nticle filter. A method for automatic dynamic setting of\n(a) Frame 1\n(b) Frame 16\n(c) Frame 26\n \n(d) Weights of each cue through the sequence\nFig. 8. The particle filter is run with adaptively weighted\ncolour and edge cues to track the football player\u2019s helmet.\n(a)-(c) show that the helmet is successfully tracked. The\nweighting assigned to the cues is shown in (d).\nthe noise parameters of the cues is proposed to allow\nmore flexibility in the object tracking. Multiple cues are\ncombined for tracking, which has been shown to make\nthe particle filter more able to accurately and robustly\ntrack a range of objects. These techniques can be fur-\nther extended with other visual cues such as motion and\n10\n(a) Frame 1 (b) Frame 37 (c) Frame 61\n(d) Frame 1 (e) Frame 35 (f) Frame 61\nFig. 9. The same sequence as in Fig 4 for which colour track-\ning failed. (a)-(c) show the results from tracking with com-\nbined colour and edge cues, (d)-(f) from colour and texture\ncues. It can be seen that combining the colour cues with ei-\nther edge and texture cues provides accurate tracking.\n(a) Frame 1 (b) Frame 66 (c) Frame 196\n(d) Frame 1 (e) Frame 66 (f) Frame 196\n(g) Frame 1 (h) Frame 66 (i) Frame 196\nFig. 10. Hand tracking as it undergoes translation, rotation\nand mild scale change. The colour (a)-(c) cues track the\nobject, although some distraction is caused by the face. The\nedge cues (d)-(f) get distracted by the edge information in\nthe light. Combining colour and edges (g)-(i) provides more\naccurate tracking of the hand.\nnon-visual cues.\nThe developed particle filter is compared with the mean-\nshift algorithm and its reliability is shown also in the\npresence of ambiguous situations. Nevertheless that the\nparticle filter is more time consuming than the mean-\nshift algorithm, it runs comfortably in real time.\nCurrent and future areas for research include the inves-\n(a) Frame 1\n(b) Frame 40\n(c) Frame 168\nFig. 11. In this sequence the serve speed board is being\ntracked, it undergoes both partial and full occlusion. The\nresults show that the cues are resilient to partial occlusion,\nsee (b) and (c). Using the mixed-state model the tracker is\nable to recover following a full occlusion.\ntigation of alternative data fusion schemes, improved\nproposal distributions, tracking multiple objects and\nonline adaption of the target model.\nAcknowledgements\nThe authors are grateful to the financial support by the\nUK MOD Data and Information Fusion Defence Technology\nCentre for this work.\nReferences\n[1] M. Isard and A. Blake, \u201cCondensation \u2013 conditional density\npropagation for visual tracking,\u201d International Journal of\nComputer Vision, vol. 28, no. 1, pp. 5\u201328, 1998.\n11\n[2] N. Gordon, D. Salmond, and A. Smith, \u201cA novel approach\nto nonlinear \/ non-Gaussian Bayesian state estimation,\u201d IEE\nProceedings-F, vol. 140, pp. 107\u2013113, April 1993.\n[3] A. Doucet, S. Godsill, and C. Andrieu, \u201cOn sequential Monte\nCarlo sampling methods for Bayesian filtering,\u201d Statistics\nand Computing, vol. 10, no. 3, pp. 197\u2013208, 2000.\n[4] M. Arulampalam, S. Maskell, N. Gordon, and T. Clapp, \u201cA\ntutorial on particle filters for online nonlinear\/non-Gaussian\nBayesian tracking,\u201d IEEE Transactions on Signal Processing,\nvol. 50, no. 2, pp. 174\u2013188, 2002.\n[5] K. Nummiaro, E. Koller-Meier, and L. V. Gool, \u201cAn adaptive\ncolor-based particle filter,\u201d Image and Vision Computing,\nvol. 21, no. 1, pp. 99\u2013110, 2003.\n[6] D. Comaniciu, V. Ramesh, and P. Meer, \u201cReal-time tracking\nof non-rigid objects using mean shift,\u201d in Proceedings of the\n1st Conference on Computer Vision and Pattern Recognition.\nHilton Head, SC, 2000, pp. 142\u2013149.\n[7] \u2014\u2014, \u201cKernel-based object tracking,\u201d IEEE Transactions on\nPattern Analysis and Machine Intelligence, vol. 25, no. 5,\npp. 564\u2013577, 2003.\n[8] C. Shen, A. van den Hengel, and A. Dick, \u201cProbabilistic\nmultiple cue integration for particle filter based tracking,\u201d in\nProc. of the VIIth Digital Image Computing: Techniques and\nApplications. C. Sun, H. Talbot, S. Ourselin, T. Adriansen,\nEds., 10-12 Dec. 2003.\n[9] P. Pe\u00b4rez, J. Vermaak, and A. Blake, \u201cData fusion for tracking\nwith particles,\u201d Proceedings of the IEEE, vol. 92, no. 3, pp.\n495\u2013513, March 2004.\n[10] M. Spengler and B. Schiele, \u201cTowards robust multi-\ncue integration for visual tracking,\u201d Machine Vision and\nApplications, vol. 14, no. 1, pp. 50\u201358, 2003.\n[11] E. Ozyildiz, N. Krahnsto\u00a8ver, and R. Sharma, \u201cAdaptive\ntexture and color segmentation for tracking moving objects,\u201d\nPattern Recognition, vol. 35, pp. 2013\u20132029, October 2002.\n[12] P. Pe\u00b4rez, C. Hue, J. Vermaak, and M. Ganget, \u201cColor-\nbased probabilistic tracking,\u201d in Proceedings of the 7th\nEuropean Conference on Computer Vision. Vol. 2350, LNCS,\nCopenhagen, Denmark, May 2002, pp. 661\u2013675.\n[13] E. Poon and D. Fleet, \u201cHybrid Monte Carlo filtering: Edge-\nbased tracking people,\u201d in Proceedings of the IEEEWorkshop\non Motion and Video Computing, Orlando, Florida, Dec.\n2002, pp. 151\u2013158.\n[14] K. Toyama and G. Hager, \u201cIncremental focus of attention\nfor robust vision-based tracking,\u201d International Journal of\nComputer Vision, vol. 35, no. 1, pp. 45\u201363, 1999.\n[15] J. Triesch and C. von der Malsburg, \u201cDemocratic\nintegration: Self-organized integration of adaptive cues,\u201d\nNeural Computation, vol. 13, no. 9, pp. 2049\u20132074, 2001.\n[16] A. Jepson, D. Fleet, and T. F. El-Maraghi, \u201cRobust online\nappearance models for visial tracking,\u201d IEEE Trans. on\nPattern Analysis and Machine Intelligence, vol. 25, no. 10,\npp. 1296\u20131311, 2003.\n[17] P. Brasnett, L. Mihaylova, N. Canagarajah, and D. Bull,\n\u201cParticle filtering with multiple cues for object tracking in\nvideo sequences,\u201d in Proc. of SPIE\u2019s 17th Annual Symposium\non Electronic Imaging, Science and Technology, V. 5685,\n2005, pp. 430\u2013441.\n[18] N. Bergman, \u201cRecursive Bayesian estimation: Navigation\nand tracking applications,\u201d Ph.D. dissertation, Linko\u00a8ping\nUniversity, Linko\u00a8ping, Sweden, 1999.\n[19] J. Liu and R. Chen, \u201cSequential Monte Carlo methods\nfor dynamic systems,\u201d Journal of the American Statistical\nAssociation, vol. 93, no. 443, pp. 1032\u20131044, 1998. [Online].\nAvailable: citeseer.nj.nec.com\/article\/liu98sequential.html\n[20] E. Wan and R. van der Merwe, Kalman Filtering and Neural\nNetworks. Wiley Publishing, September 2001, chapter 7. the\nUnscented Kalman filter 7, pp. 221\u2013280.\n[21] Y. Bar-Shalom and X. Li, Estimation and Tracking:\nPrinciples, Techniques and Software. Artech House, 1993.\n[22] M. Isard and A. Blake, \u201cCondensation - conditional density\npropogation for visual tracking,\u201d International Journal of\nComputer Vision, vol. 29, no. 1, pp. 5\u201328, 1998.\n[23] \u2014\u2014, \u201cIcondensation: Unifying low-level and high-level\ntracking in a stochastic framework,\u201d in Proceedings of the 5th\nEuropean Conference on Computer Vision, vol. 1, 1998, pp.\n893\u2013908.\n[24] R. Porter, Texture Classification and Segmentation, PhD\nThesis, University of Bristol, Center for Communications\nResearch, 1997.\n[25] W. Freeman and E. Adelson, \u201cThe design and use of steerable\nfilters,\u201d in IEEE Transactions on Pattern Analysis and\nMachine Intelligence, vol. 13, 1991.\n[26] A. Karasaridis and E. P. Simoncelli, \u201cA filter design\ntechnique for steerable pyramid image transforms,\u201d in\nProceedings of the ICASSP, Atlanta, GA, May 1996.\n[27] A. Bhattacharayya, \u201cOn a measure of divergence between\ntwo statistical populations defined by their probability\ndistributions,\u201d Bulletin of the Calcutta Mathematical\nSociety, vol. 35, pp. 99\u2013110, 1943.\n[28] D. Scott, Multivariate Density Estimation: Theory, Practice\nand Visualization, ser. Probability and Mathematical\nStatistics. John Wiley and Sons, 1992.\n[29] R. Manduchi, \u201cBayesian fusion of colour and texture\nsegmentations,\u201d in Proc. of the IEEE International\nConference on Computer Vision, September 1999.\n[30] E. Saber and A. M. Tekalp, \u201cIntegration of color, edge,\nshape and texture features for automatic region-based image\nannotation and retrieval,\u201d Journal of Electronic Imaging\n(Special Issue), vol. 7, no. 3, pp. 684\u2013700, 1998.\n[31] X. Shi and R. Manduchi, \u201cA study on Bayes feature fusion for\nimage classification,\u201d in Proceedings of the IEEE Workshop\non Statistical Analysis in Computer Vision, vol. 8, June 2003.\n[32] Y. Bar-Shalom, X. R. Li, and T. Kirubarajan, Estimation\nwith Applications to Tracking and Navigation. John Wiley\nand Sons, 2001.\n[33] \u201cBBC Radio 1, http:\/\/www.bbc.co.uk\/calc\/radio1\/ ,\u201d\nindex.shtml, 2005, Creative Archive Licence.\n[34] D. Comaniciu and P. Meer, \u201cMean shift: A robust approach\ntoward feature space analysis,\u201d IEEE Transactions on\nPattern Analysis and Machine Intelligence, vol. 22, no. 5,\npp. 603\u2013619, 2002.\n12\n"}