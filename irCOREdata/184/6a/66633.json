{"doi":"10.1117\/12.586712","coreId":"66633","oai":"oai:dro.dur.ac.uk.OAI2:655","identifiers":["oai:dro.dur.ac.uk.OAI2:655","10.1117\/12.586712"],"title":"Smoothing region boundaries in variable depth mapping for real time stereoscopic images.","authors":["Holliman,  N. S."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":["Woods, Andrew J.","Bolas, Mark T.","Merritt, John O.","McDowall, Ian E."],"datePublished":"2005-03-22","abstract":"We believe the need for stereoscopic image generation methods that allow simple, high quality content creation\\ud\ncontinues to be a key problem limiting the widespread up-take of 3D displays. We present new algorithms for\\ud\ncreating real time stereoscopic images that provide increased control to content creators over the mapping of\\ud\ndepth from scene to displayed image.\\ud\nPreviously we described a Three Region, variable depth mapping, algorithm for stereoscopic image generation.\\ud\nThis allows different regions within a scene to be represented by different ranges of perceived depth in the final\\ud\nimage. An unresolved issue was that this approach can create a visible discontinuity for smooth objects crossing\\ud\nregion boundaries. In this paper we describe two new Multi-Region algorithms to address this problem: boundary\\ud\nsmoothing using additional sub-regions and scaling scene geometry to smoothly vary depth mapping.\\ud\nWe present real time implementations of the Three-Region and the new Multi-Region algorithms for OpenGL\\ud\nto demonstrate the visual appearance of the results. We discuss the applicability and performance of each\\ud\napproach for rendering real time stereoscopic images and propose a simple modification to the standard graphics\\ud\npipeline to better support these algorithms","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/66633.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/655\/1\/655.pdf","pdfHashValue":"1890c92dbc65f527b3479075ac2a800ee6e33025","publisher":"SPIE","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:655<\/identifier><datestamp>\n      2017-03-10T15:35:11Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Smoothing region boundaries in variable depth mapping for real time stereoscopic images.<\/dc:title><dc:creator>\n        Holliman,  N. S.<\/dc:creator><dc:description>\n        We believe the need for stereoscopic image generation methods that allow simple, high quality content creation\\ud\ncontinues to be a key problem limiting the widespread up-take of 3D displays. We present new algorithms for\\ud\ncreating real time stereoscopic images that provide increased control to content creators over the mapping of\\ud\ndepth from scene to displayed image.\\ud\nPreviously we described a Three Region, variable depth mapping, algorithm for stereoscopic image generation.\\ud\nThis allows different regions within a scene to be represented by different ranges of perceived depth in the final\\ud\nimage. An unresolved issue was that this approach can create a visible discontinuity for smooth objects crossing\\ud\nregion boundaries. In this paper we describe two new Multi-Region algorithms to address this problem: boundary\\ud\nsmoothing using additional sub-regions and scaling scene geometry to smoothly vary depth mapping.\\ud\nWe present real time implementations of the Three-Region and the new Multi-Region algorithms for OpenGL\\ud\nto demonstrate the visual appearance of the results. We discuss the applicability and performance of each\\ud\napproach for rendering real time stereoscopic images and propose a simple modification to the standard graphics\\ud\npipeline to better support these algorithms.<\/dc:description><dc:subject>\n        Stereoscopic cameras<\/dc:subject><dc:subject>\n         Graphics pipeline<\/dc:subject><dc:subject>\n         Human factors<\/dc:subject><dc:subject>\n         Computer graphics<\/dc:subject><dc:subject>\n         Multi-View<\/dc:subject><dc:subject>\n         3D display.<\/dc:subject><dc:publisher>\n        SPIE<\/dc:publisher><dc:source>\n        Woods, Andrew J. & Bolas, Mark T. & Merritt, John O. & McDowall, Ian E. (Eds.). (2005). Stereoscopic displays and virtual reality systems XII. Bellingham, WA: SPIE, pp. 281-292, Proceedings of SPIE(5664)<\/dc:source><dc:contributor>\n        Woods, Andrew J.<\/dc:contributor><dc:contributor>\n        Bolas, Mark T.<\/dc:contributor><dc:contributor>\n        Merritt, John O.<\/dc:contributor><dc:contributor>\n        McDowall, Ian E.<\/dc:contributor><dc:date>\n        2005-03-22<\/dc:date><dc:type>\n        Book chapter<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:655<\/dc:identifier><dc:identifier>\n        issn:0277-786X<\/dc:identifier><dc:identifier>\n        doi:10.1117\/12.586712<\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/655\/<\/dc:identifier><dc:identifier>\n        https:\/\/doi.org\/10.1117\/12.586712<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/655\/1\/655.pdf<\/dc:identifier><dc:rights>\n        N. S. Holliman, \u201cSmoothing region boundaries in variable depth mapping for real time stereoscopic images,\u201d Proceedings of SPIE : Stereoscopic displays and virtual reality systems XII, A. J. Woods, J. O. Merritt, I. E. McDowell, Editors, 5664, 281-292 (2005).\\ud\n\\ud\nCopyright 2005 Society of Photo-Optical Instrumentation Engineers. One print or electronic copy may be made for personal use only. Systematic reproduction and distribution, duplication of any material in this paper for a fee or for commercial purposes, or modification\\ud\nof the content of the paper are prohibited.<\/dc:rights><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["issn:0277-786X","0277-786x"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2005,"topics":["Stereoscopic cameras","Graphics pipeline","Human factors","Computer graphics","Multi-View","3D display."],"subject":["Book chapter","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n21 August 2009\nVersion of attached file:\nPublished Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nHolliman, N. S. (2005) \u2019Smoothing region boundaries in variable depth mapping for real time stereoscopic\nimages.\u2019, in Proceedings of SPIE : Stereoscopic displays and virtual reality systems XII. Bellingham WA:\nSPIE, pp. 281-292.\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1117\/12.586712\nPublisher\u2019s copyright statement:\nN. S. Holliman, Smoothing region boundaries in variable depth mapping for real time stereoscopic images, Proceedings\nof SPIE : Stereoscopic displays and virtual reality systems XII, A. J. Woods, J. O. Merritt, I. E. McDowell, Editors,\n5664, 281-292 (2005). Copyright 2005 Society of Photo-Optical Instrumentation Engineers. One print or electronic\ncopy may be made for personal use only. Systematic reproduction and distribution, duplication of any material in this\npaper for a fee or for commercial purposes, or modification of the content of the paper are prohibited.\nAdditional information:\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\nSmoothing Region Boundaries in Variable Depth Mapping for\nReal Time Stereoscopic Images\nNick Holliman\nDepartment of Computer Science, University of Durham, Durham, United Kingdom\nABSTRACT\nWe believe the need for stereoscopic image generation methods that allow simple, high quality content creation\ncontinues to be a key problem limiting the widespread up-take of 3D displays. We present new algorithms for\ncreating real time stereoscopic images that provide increased control to content creators over the mapping of\ndepth from scene to displayed image.\nPreviously we described a Three Region, variable depth mapping, algorithm for stereoscopic image generation.\nThis allows different regions within a scene to be represented by different ranges of perceived depth in the final\nimage. An unresolved issue was that this approach can create a visible discontinuity for smooth objects crossing\nregion boundaries. In this paper we describe two new Multi-Region algorithms to address this problem: boundary\nsmoothing using additional sub-regions and scaling scene geometry to smoothly vary depth mapping.\nWe present real time implementations of the Three-Region and the new Multi-Region algorithms for OpenGL\nto demonstrate the visual appearance of the results. We discuss the applicability and performance of each\napproach for rendering real time stereoscopic images and propose a simple modification to the standard graphics\npipeline to better support these algorithms.\nKeywords: Stereoscopic Cameras, Graphics Pipeline, Human Factors, Computer Graphics, Multi-View, 3D\nDisplay\n1. INTRODUCTION\nThe perceived depth seen in a stereoscopic image varies with a wide range of parameters and, even if the display\ndevice is ideally aligned and has zero crosstalk, it can be difficult to compose the depth presentation as desired.\nPartly this is due to well-known human factors considerations; there is a limit to the range of usable perceived\ndepth before the binocular image becomes diplopic.1\u20133 However, even within this range existing stereoscopic\nimage generation methods provide limited control over the mapping between perceived depth in the displayed\nimage and the original scene depth.\nTo address this problem we previously presented the Three-Region algorithm4; this allows different regions of\nthe scene to be mapped to different ranges of perceived depth on a target 3D display. The benefit is a particular\nregion of interest in the scene can be given additional depth representation in the stereoscopic image. The\naim is to retain the scene outside the region of interest, rather than clip it or disguise it by blurring because\nin many applications it is important to see context around the region of interest. For example, in a scientific\nvisualization a particular feature can then be highlighted in depth or in computer games a game character can\nbe given preferred depth representation. The approach can be compared to the \u201dfocus+context\u201d methods used\nin visualization5 allowing the \u201dfocus\u201d to be varied in the perceived depth dimension.\nA potential problem with the Three-Region algorithm occurs when smooth objects cross the boundaries\nbetween two adjacent regions. If there is a significant change in perspective projection between the two regions\nthere can be a visible discontinuity at the boundary. In this paper we specifically address this problem and\nconsider how a Multi-Region approach can be used to smooth the visual transition at region boundaries. We\npresent two new Multi-Region algorithms, one using linear perspective projection and an alternative using scene\nscaling to control perceived depth. In addition we describe versions of all three algorithms using asymmetric\ncamera frusta so that we can implement them using real time graphics in OpenGL.6\nFurther author information: (Send correspondence to Nick Holliman.)\nEmail: n.s.holliman@durham.ac.uk, Telephone: +44 191 334 1761, Web: http:\/\/www.durham.ac.uk\/n.s.holliman\/\n2. BACKGROUND\nScene depth range\nPerceived depth range\nPhysical display\nVirtual display\n(a)\nScene depth range\nPerceived depth range\nRegion of interest\n(b)\nNR FR\nFigure 1. Stereoscopic image capture maps a scene depth range to a display perceived depth range: (a) Single Region\nalgorithms map a range of scene depth to perceived depth on a target display as a whole. Some algorithms7 guarantee\nthe result of this mapping even when the viewers head is moving. (b) The Three Region algorithm4 provides additional\ncontrol allowing the available stereoscopic depth range to be split preferentially between three regions. A defined region\nof interest can then be given the best available stereo depth representation.\nMuch recent work in stereoscopic image generation7, 8 has implicitly or explicitly investigated Single Region\ndepth mapping, taking a scene as a whole and mapping it to a target perceived depth range on a 3D display, as\nin figure 1(a). The most recent algorithms are able to guarantee the maximum depth seen in an image. These\nalgorithms also provide guarantees about the depth seen in images for real time head-tracked displays and do\nnot have the shearing and warping effects often associated with head movement and stereo images.\nThere has been relatively little reported work extending this to allow the limited available perceived depth\nto be focused on a particular region of interest within the scene. Our Three-Region algorithm,4 illustrated\nin figure 1(b), implements this using different perspective camera projection to alter the depth mapping from\ndefined ranges of scene depth to defined ranges of perceived display depth. A similar concept using scene scaling\nis presented in9 however key details of the approach were not reported and no results were presented.\n3. THREE-REGION ALGORITHM FOR ASYMMETRIC CAMERA FRUSTUM\nThe Three Region algorithm previously reported4 was designed for use with graphics systems that support\nsymmetric camera frusta, such as the ray tracing program POVRay. A consequence of using symmetric frustum\nis the need to compute image regions not required in the final stereoscopic image pair. Image cropping regions\nmust then be calculated and applied to all the left and right partial images making up the stereoscopic image\npair.\nThe need to calculate and apply cropping is a small overhead in the POVRay implementation where rendering\ntimes are measured in seconds or minutes. It becomes more of a disadvantage for real time systems where the extra\nrendering and image manipulation required will directly impact maximum achievable frame rates. Fortunately,\ngraphics APIs such as OpenGL support asymmetric camera frustum allowing us to render only the regions of\nthe stereoscopic images we need. We therefore derive a modified Three Region algorithm for use with graphics\nsystems supporting asymmetric camera frustum.\nRecall4 that we consider two distinct geometries; the geometry defining the relationship between the viewer\nand the display and the geometry of the scene and camera. The three-region algorithm maps three defined\nregions, Near Region (NR), Region of Interest (ROI) and Far Region (FR) in scene depth onto corresponding\ndefined ranges of geometric perceived depth gpd in the displayed image. This mapping meets the constraint\nthat points on the region boundaries are projected to coincident positions and hence depth in the image pair,\nwhichever region they are considered to belong to.\nFor each defined region we calculate, as previously, a camera separation that maps scene depth range to the\ntarget perceived depth range and use this to calculate an appropriate camera frustum as below. The notation\nfollows that defined in our earlier paper.4\n3.1. Region of Interest Mapping\nWe derive the camera parameters to generate the left and right partial images for the ROI using asymmetric\ncamera frustum. The distance to the location in the scene of the zero disparity plane (ZDP), z\u2032, the width of the\nfrustum w\u2032 at that point and the camera separation, a\u2032 are calculated as previously reported.4 We additionally\nassume the height of the frustum at the ZDP, h\u2032, is simply w\u2032 scaled by the aspect ratio of the intended final\nimage.\nWe need the six extents for each frustum,6 the left\u2032, right\u2032, bottom\u2032, top\u2032, near\u2032 and far\u2032 limits, plus the\ncameras from\u2032, to\u2032 and up\u2032 vectors. In the following we assume a stereoscopic camera with a look from point\ndisplaced about the origin looking in the direction of the positive z-axis.\na\u2019\nw\u2019\nF\u2019\nf\u2019\nz\u2019\nn\u2019\nN\u2019\nstereoscopic camera positions\nfrustum width.\nmonoscopic camera position\nt\u2019m\nROI\u2019NR\u2019 FR\u2019\nZero disparity plane.\na\u2019\nle\nft\n\u2019\nri\ng\nh\nt\u2019\nasymmetric frustum outline.\nFigure 2. The scene geometry highlighting the asymmetric camera frustum for the right camera capturing the ROI.\nReferring to figure 2 we find the extents for the ROI right camera frustum. We find the magnitude of the left\nand right extents by similar triangles using the known frustum width w\u2032 at the ZDP.\nleft\u2032 = \u2212n\n\u2032\nz\u2032\n(w\u2032 + a\u2032) right\u2032 =\nn\u2032\nz\u2032\n(w\u2032 \u2212 a\u2032) (1)\nThe bottom\u2032 and top\u2032 extents are equal as the frustum is vertically symmetrical and can be found using similar\ntriangles and the height of the frustum h\u2032, while the near\u2032 and far\u2032 extents of the frustum are simply the given\nextents of the ROI:\nbottom\u2032 = top\u2032 =\nn\u2032\nz\u2032\n(h\u2032) near\u2032 = n\u2032 far\u2032 = f \u2032 (2)\nFinally the ROI right camera location and orientation are:\nfrom\u2032 = [\u2212a\u2032, 0, 0] to\u2032 = [\u2212a\u2032, 0, z\u2032] up\u2032 = [0, 1, 0] (3)\nThe frustum for the ROI left camera can be found by symmetry.\na\u2019 w\u2019\nF\u2019\nf\u2019\nz\u2019\nn\u2019\nN\u2019\nstereoscopic camera positions\nfrustum width.\nmonoscopic camera position\nt\u2019m\nROI\u2019NR\u2019 FR\u2019\nZero disparity plane.\na\u2019\nle\nft\n\u2019\nri\ng\nh\nt\u2019\nasymmetric frustum outline, ROI\u2019 & NR\u2019.\nNR camera positions\na\u2019\u2019\nri\ng\nh\nt\u2019\n\u2019\nle\nft\n\u2019\u2019\nFigure 3. The scene geometry highlighting the asymmetric camera frustum for the right camera capturing the Near-\nRegion.\n3.2. Near Region Mapping\nThe calculation for the stereo camera separation for the near region, a\u2032\u2032 remains as previously reported.4\nTo find the camera frustum parameters to generate the left and right partial images for the NR consider the\ngeometry of figure 2. By inspection we know the NR frustum width at, n\u2032, as this is where the NR meets the\nROI, as shown in figure 3 . To find the left\u2032\u2032 and right\u2032\u2032 extents for the NR frustum we simply project the ROI\nresults left\u2032 and right\u2032 onto the N \u2032 plane, adjusting for the change in camera separation. Hence the magnitude\nof the left and right extents for the Near Region right camera frustum, left\u2032\u2032 and right\u2032\u2032 are given by:\nleft\u2032\u2032 = \u2212N\n\u2032\nn\u2032\n(|left\u2032| \u2212 a\u2032 + a\u2032\u2032) right\u2032\u2032 = N\n\u2032\nn\u2032\n(|right\u2032|+ a\u2032 \u2212 a\u2032\u2032) (4)\nThe bottom\u2032\u2032 and top\u2032\u2032 extents are equal as the frustum is vertically symmetrical and can be found using\nsimilar triangles and the height of the frustum h\u2032 projected to the N \u2032 plane. The near\u2032\u2032 and far\u2032\u2032 extents of the\nfrustum are simply the given extents of the near region:\nbottom\u2032\u2032 = top\u2032\u2032 =\nN \u2032\nz\u2032\n(h\u2032) near\u2032\u2032 = N \u2032 far\u2032\u2032 = n\u2032 (5)\nFinally the Near Region right camera location and orientation are:\nfrom\u2032\u2032 = [\u2212a\u2032\u2032, 0, 0] to\u2032\u2032 = [\u2212a\u2032\u2032, 0, (N \u2032 + n\u2032)\/2] up\u2032\u2032 = [0, 1, 0] (6)\nThe Near Region left camera frustum and location can be found by symmetry.\n3.3. Far Region Mapping\nThe Far Region frustum is derived in a similar manner to the Near Region. The magnitude of the left and right\nextents for the Far Region right camera frustum, left\u2032\u2032\u2032 and right\u2032\u2032\u2032, are found by projecting the known frustum\nwidth, w\u2032, at z\u2032 adjusted by the camera separation onto the plane at f \u2032 and are given by:\nleft\u2032\u2032\u2032 = \u2212\n(\nf \u2032\nz\u2032\n(w\u2032 + a\u2032)\n)\n\u2212 a\u2032 + a\u2032\u2032\u2032 right\u2032\u2032\u2032 =\n(\nf \u2032\nz\u2032\n(w\u2032 \u2212 a\u2032)\n)\n+ a\u2032 \u2212 a\u2032\u2032\u2032 (7)\na\u2019\nw\u2019\nF\u2019\nf\u2019\nz\u2019\nn\u2019\nN\u2019\nstereoscopic camera positions\nfrustum width.\nmonoscopic camera position\nt\u2019m\nROI\u2019NR\u2019 FR\u2019\nZero disparity plane.\na\u2019\nle\nft\n\u2019\nri\ng\nh\nt\u2019\nasymmetric frustum outline.\nFR camera positions\na\u2019\u2019\u2019\nle\nft\n\u2019\u2019\u2019\nri\ng\nh\nt\u2019\n\u2019\u2019\nFigure 4. The scene geometry highlighting the asymmetric camera frustum for the right camera capturing the Far\nRegion.\nThe bottom\u2032\u2032\u2032 and top\u2032\u2032\u2032 extents are equal as the frustum is vertically symmetrical and can be found using\nsimilar triangles and the height of the frustum h\u2032 projected to the f \u2032 plane. The near\u2032\u2032\u2032 and far\u2032\u2032\u2032 extents of the\nfrustum are simply the given extents of the far region.\nbottom\u2032\u2032\u2032 = top\u2032\u2032\u2032 =\nf \u2032\nz\u2032\n(h\u2032) near\u2032\u2032\u2032 = f \u2032 far\u2032\u2032\u2032 = F \u2032 (8)\nFinally the Far Region right camera location and orientation are:\nfrom\u2032\u2032\u2032 = [\u2212a\u2032\u2032\u2032, 0, 0] to\u2032\u2032\u2032 = [\u2212a\u2032\u2032\u2032, 0, (f \u2032 + F \u2032)\/2] up\u2032\u2032\u2032 = [0, 1, 0] (9)\nThe Far Region left camera frustum and location can be found by symmetry.\n3.4. Implementation\nThe Three-Region algorithm described above can now be implemented in OpenGL. The left and right images\nin each stereo pair are formed by rendering three partial images of the three regions. The partial images are\nrendered in far to near order so that as they overwrite each other and the occlusions in the final image pair are\ncorrectly accounted for.\n4. MULTI-REGION ALGORITHM FOR ASYMMETRIC CAMERA FRUSTUM\nWe anticipated that a drawback of the Three-Region algorithm in certain scenes is that would introduce a C1\ndiscontinuity at the region boundaries. That is, while we guarantee the surface is continuous across boundaries,\nthe surface normals are not. In scenes which have smooth objects crossing a region boundary this can result in\na visible artifact; a kink can be seen as the object crosses a region boundary.\nIn addition when there is a significant difference in depth compression between two adjacent regions a locally\nhigh disparity gradient can be introduced into the stereoscopic image where none previously existed. It is\nknown10 that the effect of this can be to destroy the stereo effect in the area local to the high gradient and this\nis visually distracting. A possible solution is to introduce a more gradual change in perspective projection at\nregion boundaries. Ideally, this should be continuous however within the constraints of current real time graphics\npipelines we need to find a solution using linear perspective projection.\nScene depth range\nPerceived depth range\nRegion of interestn1n2 f1 f2\nFigure 5. The Multi Region algorithm maps multiple scene regions to corresponding perceived depth regions.\nThe solution we describe below is the Multi-Region algorithm for asymmetric camera frustum. This extends\nthe Three-Region approach enabling multiple additional regions to be inserted as required to smooth the tran-\nsition between regions, see figure 5. We follow the same approach, with a Region of Interest surrounded by\nmultiple near and far regions of the scene however the method is general and any number of key regions in the\nscene may be defined.\nThe input to the new algorithm is an array of scene depth boundaries and a corresponding array of display\nperceived depth boundaries. We then need to calculate the camera frustum and separation that maps each scene\nregion to the available gpd defined for the display. The output is a set of arrays, holding for each region: the left\nand right camera frustum, look from point, look to point and up vector. We first consider the ROI mapping.\n4.1. Region of Interest Mapping\nWe assume the ROI is the central region with a number of near and far regions to either side.\nIf there are nRegions mapped from scene to display there will by definition nRegions+1 boundaries defining\nthem. We use a single shared subscript index, i, for identifying both regions and region boundaries. This is\ninitially set to the index number for the ROI region, i = roiIndex.\nThe region boundaries for the current region i are then:\nni = displayBoundariesi fi = displayBoundariesi+1 n\u2032i = sceneBoundariesi f\n\u2032\ni = sceneBoundariesi+1\n(10)\nThe distance to the location in the scene of the zero disparity plane (ZDP), z\u2032, the width of the frustum w\u2032 at\nthat point and the camera separation, now a\u2032i, are calculated as previously reported.\n4 We additionally assume\nthe height of the frustum at the ZDP, h\u2032, is simply w\u2032 scaled by the aspect ratio of the intended final image.\nAs described in the previous section for the Three-Region algorithm we find the left and right extents for the\nROI right camera frustum.\nrightCamLeft\u2032i = \u2212\nn\u2032i\nz\u2032\n(w\u2032 + a\u2032i) rightCamRight\n\u2032\ni =\nn\u2032i\nz\u2032\n(w\u2032 \u2212 a\u2032i) (11)\nThe bottom\u2032 and top\u2032 extents are equal as the frustum is vertically symmetrical and can be found using\nsimilar triangles and the height of the frustum h\u2032. The near\u2032 and far\u2032 extents of the frustum are simply the\ngiven extents of the ROI.\nrightCamBottom\u2032i = rightCamTop\n\u2032\ni =\nn\u2032\nz\u2032\n(h\u2032) rightCamNear\u2032i = n\n\u2032 rightCamFar\u2032i = f\n\u2032 (12)\nFinally the ROI right camera location and orientation are:\nrightCamFrom\u2032i = [\u2212a\u2032, 0, 0] rightCamTo\u2032i = [\u2212a\u2032, 0, z\u2032] rightCamUp\u2032i = [0, 1, 0] (13)\nThe frustum for the ROI left camera can be found by symmetry. Note we retain a record of the camera\nseparations for each region as we calculate them as they are reused in calculations for subsequent regions.\n4.2. Mapping for all Near regions\nThe near regions are those closer to the camera than the region containing the ZDP. We can determine the\nfrustum and camera location for each of the Near Regions iteratively working from furthest to nearest.\nFor each iteration of i from i = roiIndex\u2212 1 to i = 0 repeat the following:\nFor easy comparision with previous definitions we rename the region boundaries for the current region, i, as:\nNi = displayBoundariesi ni = displayBoundariesi+1 N \u2032i = sceneBoundariesi n\n\u2032\ni = sceneBoundariesi+1\n(14)\nWe first calculate the camera separation for the current Near Region, a\u2032i using the method previously re-\nported.4\nWe can then find the magnitude of the left extent for the Near Region right camera frustum, rightCamLeft\u2032i,\nusing our knowledge of the left extent of the adjacent camera frustum rightCamLeft\u2032i+1 and the adjacent camera\nseparation a\u2032i+1. Similarly the right extent can be found:\nrightCamLeft\u2032i = \u2212\nN \u2032i\nn\u2032i\n(|rightCamLeft\u2032i+1|\u2212a\u2032i+1+a\u2032i) rightCamRight\u2032i =\nN \u2032i\nn\u2032i\n(|rightCamRight\u2032i+1|+a\u2032i+1\u2212a\u2032i)\n(15)\nThe rightCamBottom\u2032i and rightCamTop\n\u2032\ni extents are equal as the frustum is vertically symmetrical and can\nbe found using similar triangles and the height of the frustum h\u2032 projected to the N \u2032i plane. The rightCamNear\n\u2032\ni\nand rightCamFar\u2032i extents of the frustum are simply the given extents of the current near region.\nrightCamBottom\u2032i = rightCamTop\n\u2032\ni =\nN \u2032i\nz\u2032\n(h\u2032) rightCamNear\u2032i = N\n\u2032\ni rightCamFar\n\u2032\ni = n\n\u2032\ni (16)\nFinally the current Near Region right camera location and orientation are:\nrightCamFrom\u2032i = [\u2212a\u2032i, 0, 0] rightCamTo\u2032i = [\u2212a\u2032i, 0, (N \u2032i + n\u2032i)\/2] rightCamUp\u2032i = [0, 1, 0] (17)\nThe frustum and camera parameters for the left camera for the current Near Region i can be found by\nsymmetry.\nOnce we have repeated this iteration over all the Near Regions their frustum and camera parameters will\nhave been calculated and stored with the ROI frustum for later use.\n4.3. Mapping for all Far regions\nThe Far Regions are those further from the camera than the region containing the ZDP. We can determine the\nfrustum and camera location for each of the Far Regions iteratively working from furthest to nearest.\nFor each iteration of i from i = roiIndex+ 1 to i = nRegions we repeat the following:\nFor easy comparison with previous definitions we rename the region boundaries for the current region, i, as:\nfi = displayBoundariesi Fi = displayBoundariesi+1 f \u2032i = sceneBoundariesi F\n\u2032\ni = sceneBoundariesi+1\n(18)\nWe then calculate the camera separation for the current Far Region, a\u2032i using the method previously reported.\n4\nWe can then find the magnitude of the left extent for the Far Region right camera frustum, rightCamLeft\u2032i,\nusing our knowledge of the left extent of the adjacent camera frustum rightCamLeft\u2032i\u22121 and the adjacent camera\nseparation a\u2032i\u22121.\nrightCamLeft\u2032i = \u2212\n(\nf \u2032i \u2217 rightCamLeft\u2032i\u22121\nsceneBoundaries\u2032i\u22121\n)\n\u2212 a\u2032i\u22121 + a\u2032i (19)\nSimilarly the right extent can be found.\nrightCamRight\u2032i =\n(\nf \u2032i \u2217 rightCamRight\u2032i\u22121\nsceneBoundaries\u2032i\u22121\n)\n+ a\u2032i\u22121 \u2212 a\u2032i (20)\nThe rightCamBottom\u2032i and rightCamTop\n\u2032\ni extents are equal as the frustum is vertically symmetrical and can\nbe found using similar triangles and the height of the frustum h\u2032 projected to the f \u2032 plane. The rightCamNear\u2032i\nand rightCamFar\u2032i extents of the frustum are simply the given extents of the far region.\nrightCamBottom\u2032i = rightCamTop\n\u2032\ni =\nf \u2032i\nz\u2032\n(h\u2032) rightCamNear\u2032i = f\n\u2032\ni rightCamFar\n\u2032\ni = F\n\u2032\ni (21)\nFinally the Far Region right camera location and orientation are:\nrightCamFrom\u2032i = [\u2212a\u2032i, 0, 0] rightCamTo\u2032i = [\u2212a\u2032i, 0, (f \u2032i + F \u2032i )\/2] rightCamUp\u2032i = [0, 1, 0] (22)\nEach of the Far Region left camera frustum and locations can be found by symmetry.\n5. MULTI-REGION ALGORITHM USING SCENE SCALING\nPrevious work9 has discussed using scene scaling instead of perspective projection to control the perceived depth\nseen on the display. The idea is to scale the scene to fit defined depth boundaries before it is projected. The\nresults can be anticipated to be a scene compressed in depth with a significantly altered perspective depth\ncompared to the original monoscopic camera view, or the Multi-Region algorithm.\nWe derive the Multi-Region algorithm using scene scaling below. The algorithm operates in a similar manner\nto the Multi-Region algorithm for perspective projection described above. The scene is clipped into ranges but\nrather than using a different perspective projection for each region the regions are scaled by a pre-calculated\nfactor using the OpenGL ModelView transformation matrix. For clarity we describe an algorithm that assumes\nunit vertical and horizontal scaling and we are simply concerned with the depth scaling from scene to display,\nthese can be easily calculated if required.\n5.1. Mapping for All Regions\nIn this algorithm the mapping for all nRegions can be calculated identically as the camera position and field of\nview remain constant for all regions. The parameters that change for each region are the camera frustum, and\nthe scaling from scene to display space.\nWe start by initialising parameters for the ROI region, setting i = roiIndex. For convenience we define the\nfollowing:\nni = displayBoundariesi fi = displayBoundariesi+1 n\u2032i = sceneBoundariesi f\n\u2032\ni = sceneBoundariesi+1\n(23)\nThe ZDP can then be assumed to be at the center of the ROI region in display space:\nz\u2032 = (fi + ni)\/2.0; (24)\nGiven the monoscopic camera field of view, t\u2032m the width and the height of the image plane at the ZDP are\nthen:\nw\u2032 = z\u2032 \u2217 tan(t\u2032m\/2.0) h\u2032 = w\u2032 \u2217 3.0\/4.0 (25)\nAs we have assumed unit scaling between scene and display, except in depth, then the camera separation for\nall regions is the eye separation e:\na\u2032 = e (26)\nNow we can find the frustum for each region of the scene from nearest to furthest.\nFor each iteration of i from i = 0 to i = nRegions\u2212 1 we repeat the following:\nLet the region boundaries for the current region i be:\nni = displayBoundariesi fi = displayBoundariesi+1 n\u2032i = sceneBoundariesi f\n\u2032\ni = sceneBoundariesi+1\n(27)\nThen the scaling we need to apply to the current region so that it maps into the available perceived depth\nrange is:\nscalingi = (fi \u2212 ni)\/(f \u2032i \u2212 n\u2032i) (28)\nThe left and right frustum extents for the right camera are given by the following:\nrightCamLeft\u2032i = \u2212\nni\nz\u2032\n(w\u2032 + a\u2032) rightCamRight\u2032i =\nni\nz\u2032\n(w\u2032 \u2212 a\u2032) (29)\nThe bottom and top extents are equal as the frustum is vertically symmetrical and can be found using similar\ntriangles and the height of the frustum h\u2032. The near and far extents of the frustum are the given extents of the\ncurrent region:\nrightCamBottom\u2032i = rightCamTop\n\u2032\ni =\nni\nz\u2032\n(h\u2032) rightCamNear\u2032i = n\n\u2032 rightCamFar\u2032i = f\n\u2032 (30)\nFinally the right camera location and orientation are:\nrightCamFrom\u2032i = [\u2212a\u2032, 0, 0] rightCamTo\u2032i = [\u2212a\u2032, 0, z\u2032] rightCamUp\u2032i = [0, 1, 0] (31)\nThe left camera frustum and locations can be found by symmetry.\n5.2. Implementation\nA side effect of scaling the scene before rendering is also to scale the surface normals used for shading. These\ncan be partially corrected by re-normalizing them to unit length using OpenGL extensions. Ideally the surface\nnormals need re-calculating each time the region scaling changes and OpenGL can support this.\n6. REAL TIME IMPLEMENTATION\nFour image generation algorithms have been implemented in OpenGL using asymmetric frusta; Single Region,7\nThree Region,4 Multi Region using perspective and Multi Region using scaling. This allows a direct comparison\nof results obtained from applying each method to the same test scene.\nThe Three and Multi Region algorithms were implemented using multi-pass rendering, rendering each frustum\nin turn from far to near. For example, to create the final image for the right channel in the Multi Region algorithm\nwith nRegions = 5 requires five partial image renderings, overlaid one on another. The partial images for the\nleft and right views are illustrated in figure 6.\nThe scaling of each partial image from its frustum to the render window is handled transparently as the\nrender window size in pixels is kept the same for each partial rendering. No cropping is required because of the\nuse of the asymmetric frusta.\nThe algorithms were implemented and tested using a 3GHz Pentium PC with an nVidia Quadro FX 4000\ndual DVI graphics card. The PC drives a BARCO Gemini display; two passive circularly polarised projectors,\nwith a 2.4m PASCAD low-crosstalk screen. A benefit of the FX4000 is the driver\u2019s ability to handle a wide range\nof display types via the standard OpenGL quad-buffer mechanism; hence we were also able to view the results\nusing a Sharp LL151 desktop display and a SeeReal C-i display. An implementation for demonstration on the\nSharp RD3D laptop display used the Sharp interlacing libraries in place of the nVidia drivers.\nFigure 6. The partial images that make up the Multi-Region rendering using five sub-regions. The upper row shows the\nleft images and the lower row the right images.\n7. RESULTS\nThe test scene consists of four, long, thin ellipsoids stretching from the furthest to nearest scene depth limits,\nfour far spheres, a central larger sphere and a central ring of smaller spheres with a pattern of alternate depth\ndifferences. The central ring of spheres acts as the region of interest and we would expect the pattern of depth\ndifferences in the ring of spheres to be better represented when rendered with the Three and Multi Region\nalgorithms compared to the Single Region algorithm. Additionally we would anticipate the long ellipsoids to\nhave visible artifacts at region boundaries if rendered using the Three Region algorithm and less visible artifacts\nusing the Multi Region algorithms. The results can be seen by free fusing the image pairs in figure 7, these\nimages can also be downloaded from the author\u2019s web site.\nFigure 7. The results of applying four different depth mapping algorithms to the same test scene. Top left is the Single\nRegion algorithm. Top right is the Three Region algorithm. Bottom left is the Multi Region algorithm using perspective\nprojection. Bottom right is the Multi Region algorithm using scene scaling.\nAs anticipated depth in the region of interest is reproduced well enough to clearly distinguish the depth\ndifferences in the ring of spheres using the Three and Multi Region algorithms whereas these differences are\nsignificantly less clear using the Single Region algorithm. In addition there are shading and disparity artifacts\nat region boundaries in the projection of the long ellipsoids using the Three Region algorithm and these are\nreduced by the Multi Region algorithm using perspective.\nThe Multi Region algorithm using scaling produces a quite different projection of the scene having scaled\nthe scene geometry before projection. This significantly changes the image of the scene and it is application\ndependent as to whether it is preferable to the other approaches.\n500\n520\n540\n560\n580\n600\n620\n640\n320 370 420 470 520 570 620 670 720 770 820\n500\n520\n540\n560\n580\n600\n620\n640\n320 370 420 470 520 570 620 670 720 770 820\n500\n520\n540\n560\n580\n600\n620\n640\n320 370 420 470 520 570 620 670 720 770 820\n500\n520\n540\n560\n580\n600\n620\n640\n320 370 420 470 520 570 620 670 720 770 820\na) Single-region depth mapping\nThe image creator only controls \nthe end points of the mapping.\nThe ROI receives little of the \navailable perceived depth (gpd) in\nthe displayed image.\nb) Three-region depth mapping\nThe image creator has control of \nthe boundaries of each region.\nThe ROI can be allocated most of \nthe perceived depth (gpd) in the \ndisplayed image.\ngpd\nROI\nc) Multi-region using perspective\nHere the additional regions are \nused to smooth the rate of change \nof disparity at the boundaries of \nthe ROI.\nd) Multi-region using scaling\nWhile this appears to have a \nsimilar mapping to c) the visual \neffect over the whole image is \nsignificantly different.\ngpd\ngpd\ngpd\nFigure 8. The depth transfer functions, from scene to display, for four stereoscopic image generation algorithms applied\nto the test scene. Scene depth of [320, 820] along the horizontal axis is mapped onto perceived depth of [500, 640] along\nthe vertical axis. The two axes cross at the point defined by the middle of the scene and the display plane. Note, in case\na) this is not the ZDP which is shifted by the mapping outside the ROI.\nThe graphs in figure 8 illustrate the depth mapping from the scene to the displayed image for each of the\nfour algorithms. These are plots along a line from the center of the viewers eye position through the center of\nthe displayed image.\n8. DISCUSSION\nWe anticipated the Three Region algorithm would have problems for the long thin objects oriented towards the\ncamera and that at region boundaries a visible shading artifact would be present. This turns out to be the case\nand these are visible in the images illustrated in figure 7. The new Multi Region algorithm using perspective\ndoes help overcome the problem by smoothing the transition in depth from the ROI to the adjacent regions.\nThis difference can also be seen comparing the graphs in figure 8 b) and c).\nA second effect the Multi Region algorithm helps overcome is the steep disparity gradient the Three Region\nalgorithm can introduce near region boundaries, as shown in figure 8 b). Such steep disparity gradient is known\nto have significant local affect on Panum\u2019s Area.10 By smoothing the transition between regions the Multi\nRegion algorithm can be used to reduce the local disparity gradient to subjectively acceptable levels.\nFor the scene scaling Multi Region algorithm the scene is scaled using the ModelView transform avoiding\nadding additional load to the CPU. However, the normals must be re-generated, as the shape of the geometry has\nbeen changed, before the vertex lighting calculations have been performed. This requires additional computation\ncompared to the Multi Region algorithm using perspective.\nOverall, additional computational overhead is required by all Three and Multi Region algorithms. Each stereo\nimage pair requires nRegions rendering passes and although geometry is clipped so that only relevant geometry\nis rendered all the scene must be passed from CPU to GPU for each rendering pass. For static scenes retained\ndisplay lists could help reduce this overhead by storing the geometry locally in the graphics card.\nFor improved performance it would be possible to parallelize the computation of the partial images using\nmultiple pipelines with a shared z-buffer to resolve the depth order in the final composed image. A better solution\nwould be to modify the graphics pipeline to allow it to store multiple projection matrices and choose which to\napply based on individual vertex depth. The scene would then only need to be sent from CPU to GPU once and\nframe rates could be maintained for all algorithms at limited additional expense in pipeline complexity.\n9. CONCLUSIONS\nWe have considered four algorithms for drawing stereoscopic images; two new algorithms, Multi Region using\nperspective and Multi Region using scaling and two existing algorithms, Single Region and Three Region. We\nderived versions of the algorithms for asymmetric camera frustum allowing us to implement them using OpenGL\nreal time graphics on a range of 3D displays.\nOur aim in developing the new Multi Region algorithm using perspective was to remove visual artifacts\nwe saw at region boundaries when using the existing Three Region algorithm for rendering long thin objects\noriented towards the camera. The new algorithms have demonstrated two benefits; a reduction in the visible\nshading artifact at region boundaries and smoothing the local disparity gradient at region boundaries allowing\nus to retain a deep Panum\u2019s Area and avoid loss of fusion. The additional computational cost of using the Multi\nRegion algorithm is not prohibitive and we have suggested simple changes to the real time graphics pipeline that\nwould make the overhead minimal.\nWe investigated a version of the Multi Region algorithm using scene scaling to control depth mapping as this\nhad been an approach suggested in the literature.9 However, it is not clear what practical benefit this algorithm\nhas as it requires additional work to correct vertex normals and significantly alters the perspective projection of\nthe scene. It may also require adjustments to texture coordinates.\nIn conclusion, we believe the new Multi Region algorithm using perspective has benefits for the content\ncreator allowing improved control over the depth composition in stereoscopic images and improved stereoscopic\nimage quality compared to existing methods. The algorithm allows a region of interest to be highlighted in depth\nproviding stereoscopic \u201dfocus+context\u201d for interacting with complex scenes.\nREFERENCES\n1. A. Woods, T. Docherty, and R. Koch, \u201cImage distortions in stereoscopic video systems,\u201d Proceedings of SPIE 1915,\n1993.\n2. Y. Yeh and L. Silverstein, \u201cLimits of fusion and depth judgements in stereoscopic color displays,\u201d Human Factors\n1(32), 1990.\n3. N. Holliman, \u201c3D Display Systems.\u201d to appear; Handbook of Opto-electronics, IOP Press, Spring 2005, ISBN 0-\n7503-0646-7.\n4. N. Holliman, \u201cMapping perceived depth to regions of interest in stereoscopic images,\u201d in Stereoscopic Displays and\nVirtual Reality Systems XI, Proceedings of SPIE 5291, 2004.\n5. C. Ware, Information visualization, perception for design, Morgan Kaufmann, 1999. ISBN 0-1-55860-511-8.\n6. D. Schreiner, M. Woo, J. Neider, and T. Davis, eds., OpenGL Programming Guide, Version 1.4, Fourth Edition,\nAddison-Wesley Pub Co, 1993. ISBN 0321173481.\n7. G. Jones, D. Lee, N. Holliman, and D. Ezra, \u201cControlling perceived depth in stereoscopic images,\u201d in Stereoscopic\nDisplays and Virtual Reality Systems VIII, Proceedings of SPIE 4297A, 2001.\n8. Z. Wartell, Stereoscopic Head-Tracked Displays: Analysis and Development of Display Algorithms. PhD thesis,\nGeorgia Institute of Technology, 2001.\n9. S. Williams and R. Parrish, \u201cNew computational control techniques and increased understanding for stereo 3D\ndisplays,\u201d Proceedings of SPIE 1256, 1990.\n10. P. Burt and B. Julesz, \u201cA disparity gradient limit for binocular fusion,\u201d Perception 9, 1980.\n"}