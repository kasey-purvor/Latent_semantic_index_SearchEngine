{"doi":"10.1214\/10-BA524","coreId":"10653","oai":"oai:dro.dur.ac.uk.OAI2:8086","identifiers":["oai:dro.dur.ac.uk.OAI2:8086","10.1214\/10-BA524"],"title":"Galaxy formation : a Bayesian uncertainty analysis.","authors":["Vernon,  Ian","Goldstein,  Michael","Bower,  Richard G."],"enrichments":{"references":[{"id":651346,"title":"Bayesian inference for the uncertainty distribution of computer model outputs.&quot;","authors":[],"date":"2002","doi":"10.1093\/biomet\/89.4.769","raw":null,"cites":null},{"id":651169,"title":"Diagnostics for Gaussian process emulators.&quot;","authors":[],"date":"2008","doi":"10.1198\/TECH.2009.08019","raw":null,"cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-12-01","abstract":"In many scientific disciplines complex computer models are used to understand the behaviour of large scale physical systems. An uncertainty anal- ysis of such a computer model known as Galform is presented. Galform models the creation and evolution of approximately one million galaxies from the begin- ning of the Universe until the current day, and is regarded as a state-of-the-art model within the cosmology community. It requires the specification of many in- put parameters in order to run the simulation, takes significant time to run, and provides various outputs that can be compared with real world data. A Bayes Linear approach is presented in order to identify the subset of the input space that could give rise to acceptable matches between model output and measured data. This approach takes account of the major sources of uncertainty in a consistent and unified manner, including input parameter uncertainty, function uncertainty, observational error, forcing function uncertainty and structural uncertainty. The approach is known as History Matching, and involves the use of an iterative suc- cession of emulators (stochastic belief specifications detailing beliefs about the Galform function), which are used to cut down the input parameter space. The analysis was successful in producing a large collection of model evaluations that exhibit good fits to the observed data","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/10653.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/8086\/1\/8086.pdf","pdfHashValue":"000cd4701d1bdbe249d2ac4d1092654b3cf94e11","publisher":"International Society for Bayesian Analysis","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:8086<\/identifier><datestamp>\n      2011-04-06T09:26:02Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Galaxy formation : a Bayesian uncertainty analysis.<\/dc:title><dc:creator>\n        Vernon,  Ian<\/dc:creator><dc:creator>\n        Goldstein,  Michael<\/dc:creator><dc:creator>\n        Bower,  Richard G.<\/dc:creator><dc:description>\n        In many scientific disciplines complex computer models are used to understand the behaviour of large scale physical systems. An uncertainty anal- ysis of such a computer model known as Galform is presented. Galform models the creation and evolution of approximately one million galaxies from the begin- ning of the Universe until the current day, and is regarded as a state-of-the-art model within the cosmology community. It requires the specification of many in- put parameters in order to run the simulation, takes significant time to run, and provides various outputs that can be compared with real world data. A Bayes Linear approach is presented in order to identify the subset of the input space that could give rise to acceptable matches between model output and measured data. This approach takes account of the major sources of uncertainty in a consistent and unified manner, including input parameter uncertainty, function uncertainty, observational error, forcing function uncertainty and structural uncertainty. The approach is known as History Matching, and involves the use of an iterative suc- cession of emulators (stochastic belief specifications detailing beliefs about the Galform function), which are used to cut down the input parameter space. The analysis was successful in producing a large collection of model evaluations that exhibit good fits to the observed data.<\/dc:description><dc:subject>\n        Computer models<\/dc:subject><dc:subject>\n         Uncertainty analysis<\/dc:subject><dc:subject>\n         Model discrepancy<\/dc:subject><dc:subject>\n         History matching<\/dc:subject><dc:subject>\n         Bayes linear analysis<\/dc:subject><dc:subject>\n         Galaxy formation<\/dc:subject><dc:subject>\n         Galform. <\/dc:subject><dc:publisher>\n        International Society for Bayesian Analysis<\/dc:publisher><dc:source>\n        Bayesian analysis, 2010, Vol.05(04), pp.619-670 [Peer Reviewed Journal]<\/dc:source><dc:date>\n        2010-12-01<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:8086<\/dc:identifier><dc:identifier>\n        issn:1936-0975<\/dc:identifier><dc:identifier>\n        issn: 1931-6690<\/dc:identifier><dc:identifier>\n        doi:10.1214\/10-BA524 <\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/8086\/<\/dc:identifier><dc:identifier>\n        http:\/\/ba.stat.cmu.edu\/vol05is04.php<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/8086\/1\/8086.pdf<\/dc:identifier><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":[" 1931-6690","issn: 1931-6690","1936-0975","issn:1936-0975"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2010,"topics":["Computer models","Uncertainty analysis","Model discrepancy","History matching","Bayes linear analysis","Galaxy formation","Galform."],"subject":["Article","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n06 April 2011\nVersion of attached file:\nPublished Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nVernon, Ian and Goldstein, Michael and Bower, Richard G. (2010) \u2019Galaxy formation : a Bayesian uncertainty\nanalysis.\u2019, Bayesian analysis., 05 (04). pp. 619-670.\nFurther information on publisher\u2019s website:\nhttp:\/\/ba.stat.cmu.edu\/vol05is04.php\nPublisher\u2019s copyright statement:\nAdditional information:\nThis was an invited discussion paper for Bayesian Analysis.\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\nBayesian Analysis (2010) 5, Number 4, pp. 619\u2013670\nGalaxy Formation: a Bayesian Uncertainty\nAnalysis\nIan Vernon\u2217, Michael Goldstein\u2020 and Richard G. Bower\u2021\nAbstract. In many scientific disciplines complex computer models are used to\nunderstand the behaviour of large scale physical systems. An uncertainty anal-\nysis of such a computer model known as Galform is presented. Galform models\nthe creation and evolution of approximately one million galaxies from the begin-\nning of the Universe until the current day, and is regarded as a state-of-the-art\nmodel within the cosmology community. It requires the specification of many in-\nput parameters in order to run the simulation, takes significant time to run, and\nprovides various outputs that can be compared with real world data. A Bayes\nLinear approach is presented in order to identify the subset of the input space that\ncould give rise to acceptable matches between model output and measured data.\nThis approach takes account of the major sources of uncertainty in a consistent\nand unified manner, including input parameter uncertainty, function uncertainty,\nobservational error, forcing function uncertainty and structural uncertainty. The\napproach is known as History Matching, and involves the use of an iterative suc-\ncession of emulators (stochastic belief specifications detailing beliefs about the\nGalform function), which are used to cut down the input parameter space. The\nanalysis was successful in producing a large collection of model evaluations that\nexhibit good fits to the observed data.\nKeywords: computer models, uncertainty analysis, model discrepancy, history\nmatching, Bayes linear analysis, galaxy formation, galform\n1 Introduction\nCurrent theories of cosmology suggest that the Universe began in a hot, dense state\napproximately 13 billion years ago, and that it has been expanding rapidly ever since.\nHowever, observations of galaxies imply that there must exist far more matter in the\nUniverse than the visible matter that makes up stars, planets and us. This is referred\nto as \u2018Dark Matter\u2019 and understanding its nature and role in the evolution of galaxies is\none of the most important problems in modern cosmology. The Galform group, based\nat the Institute of Computational Cosmology, Durham University, is the world leading\ngroup in the study of Galaxy Formation in the presence of Dark Matter (see Bower et al.\n(2006) and references therein). Over the last 13 years, they have developed a detailed\ncomputer model, known as Galform, which simulates the creation and evolution of\n\u2217Department of Mathematical Sciences, Durham University, Science Laboratories, Durham, UK,\nmailto:i.r.vernon@durham.ac.uk\n\u2020Department of Mathematical Sciences, Durham University, Science Laboratories, Durham, UK,\nmailto:michael.goldstein@durham.ac.uk\n\u2021Department of Physics, Durham University, Science Laboratories, Durham, UK, mailto:r.g.\nbower@durham.ac.uk\nc\u00a9 2010 International Society for Bayesian Analysis DOI:10.1214\/10-BA524\n620 Galaxy Formation: a Bayesian Uncertainty Analysis\napproximately one million galaxies from the beginning of the Universe until the present\nday. The simulation produces various physical features of each of the galaxies which\ncan be compared to observed galaxy survey data.\nThe Galform model requires many input parameters to be specified in order to run\nthe simulation. It is therefore necessary to explore the input parameter space and find\nthe set of all input configurations that give rise to acceptable matches between model\noutput and observed data. As the model run time is significant, this is a challenging\ntask. Further, even to assess what constitutes an acceptable match, we must consider\nall of the uncertainties that are involved in the comparison between model and reality,\nincluding input parameter uncertainty, function uncertainty, observational error, forcing\nfunction uncertainty and structural uncertainty. Such a detailed level of uncertainty\nquantification has never been attempted for such a cosmological model.\nThis case study describes a collaboration between members of the Statistics group\nand the Galform group, at Durham, to carry out such an uncertainty analysis for Gal-\nform. Our aim is to identify all choices of input parameters that generate consistent\nphysical models in the sense that they would yield sufficiently good matches to certain\nimportant features of observational data, when we have taken into account all relevant\nsources of uncertainty. In particular, it is of fundamental interest to know whether this\nset of acceptable inputs is non-empty.\nIn order to treat all uncertainties in a consistent and unified manner, we use general\ntechniques related to the Bayesian treatment of uncertainty for computer models for\nlarge scale physical systems. In addition to the uncertainty associated with the Galform\nfunction itself, we elicit all of the other sources of uncertainty which must be addressed\nin order to make meaningful comparisons between Galform output and observations.\nOur approach is based on the construction of an emulator for Galform, this being\na stochastic function that represents our beliefs about the behaviour of the simulator.\nWe use the emulator and the model uncertainties to define implausibility measures over\nthe input parameter space for Galform, based on a Bayes Linear analysis. We exclude\nregions of input space by imposing cutoffs on our implausibility measures. We proceed\niteratively, making function evaluations over the full range of the input space, emulating\nGalform over this space, using implausibility measures to remove a part of the space,\nmaking a further collection of evaluations of Galform in the reduced space, re-emulating\nwithin the reduced space, re-evaluating our implausibility measures over this subspace\nand therefore removing a further portion of the space and continuing in this fashion.\nWe performed this cycle four times, in each case making a substantial further reduction\nto the allowable input space. We then made a final set of runs to check that we did\nhave many acceptable matches between Galform output and observations over a range\nof input parameter choices within the final reduced space.\nThis is a significant contribution toward understanding the Galform model, as pre-\nviously no knowledge of the shape and extent of the acceptable region of input space\nexisted. Further, the previous best matches to the primary data set of interest were not\ncompatible with other secondary, but important, observational data sets. Our analysis\ndemonstrates that, by making realistic assessments of structural uncertainty, we are\nI. Vernon, M. Goldstein and R. G. Bower 621\nindeed able to simultaneously match data sets that were previously thought to be in-\ncompatible, contradicting authors who suggested that the Universe is \u201canti-hierarchical\u201d\n(see section 2), and that such a match is impossible. Thus this work should be viewed\nas consistent with the hypothesis that galaxies formed in the presence of large amounts\nof Dark Matter, and in particular via hierarchical merging.\nThis collaboration began in an informal fashion. Members of the statistics group\nwere interested in applying various techniques that they had developed for the analysis\nof large scale computer models. The Galform group offered the use of their model and\nsome of their computing facilities. Over time, it became clear that such an analysis was\na useful tool for understanding various scientific issues related to the model, and merited\na serious collaborative effort to pursue these questions. This account is a description of\nthe results of the collaboration, described more or less as it has evolved.\nThe Case Study paper is structured as follows. In section 2 we discuss the physical\nmotivation for the study of galaxy evolution and give a general description of the Gal-\nform model. Section 3 describes the Computer Model methodology that we will employ,\nand highlights all the relevant uncertainties that must be considered. In section 4 we\ndescribe the construction of the Wave 1 emulator. In section 5 we assess all remaining\nuncertainties relevant to the analysis and in section 6 we perform the first iteration\nof the History Matching process. Section 7 deals with the remaining iterations, and\nthe results are reported in section 8. We conclude with discussions regarding physical\ninsight gained in section 9.\n2 A universe full of galaxies\nThe night sky is full of stars. Yet the stars that are visible to the human eye are only\nan unimaginably tiny fraction of the stars in the universe as a whole. Equipped with\ntelescopes, astronomers have discovered that at great distances beyond our own galaxy\nlie millions of millions of other galaxies, each with their own populations of stars.\nBecause of the finite speed of light, such distant galaxies are seen when the universe\nwas much younger. Astronomers can use this time delay to observe the build up and\nformation of galaxies. The most distant galaxies identified to date are seen only 109\nyears after the big bang, when the universe was less than 1\/10th of its current age. Such\nobservations reveal some puzzling results: they suggest that a large proportion of the\nmost massive galaxies are present quite early in the history of the universe. This is in\nseeming contradiction to the theoretical predictions of the popular Cold Dark Matter\nmodel, which suggests galaxies form through a process of \u2018hierarchical aggregation\u2019:\nsmall galaxies form early in the history of the universe, building larger and larger galaxies\nthrough gravitational collapse and collision. Explaining this apparent contradiction is a\nkey motivation in the development of the Galform model described below. See Appendix\nA for more details, and for an introduction to galaxy formation.\n622 Galaxy Formation: a Bayesian Uncertainty Analysis\n2.1 Modelling Galaxy Formation: The GALFORM Model\nThere are essentially two approaches to modelling the formation of galaxies (see appen-\ndices A and B). The first is the \u201cnumerical simulation\u201d method, a simple and direct\napproach which uses fundamental physical equations only. This suffers greatly from res-\nolution issues and cannot model many critical features of galaxy formation, for example\nthe winds produced by stars at their deaths, or the formation and effect of black holes.\n\u201cSemi-analytic modelling\u201d represents the alternative approach. Rather than tackling\nthe whole problem in a single numerical integration, it is broken down into its separate\ncomponents or modules. For example, one component of the model is the growth and\nmerging of dark matter haloes (each visible galaxy is thought to be contained within\na massive halo of dark matter, which dominates the galaxy\u2019s motion). This can be\ncomputed by running a numerical calculation known as the Millennium simulation that\nonly includes mass and the force of gravity (see section 2.2). As the various processes\ninvolved in galaxy formation are not precisely understood, the modules include a number\nof uncertain, adjustable input parameters x, the exploration of which is the main goal\nof this case study. Because of the intrinsic complexity of the galaxy formation problem,\n\u201csemi-analytic models\u201d currently offer the best avenue for progress.\nGalform is a world-leading example of such a semi-analytic galaxy formation model\n(see Bower et al. (2006)). The principle modules within the Galform code track: [1] the\ngravitational collapse and build-up of dark matter haloes; [2] the cooling and accretion of\ngas; the formation of stars, stellar evolution and \u201cfeedback\u201d from supernova explosions;\n[3] galaxy mergers and instabilities in stellar disks; [4] the formation of black holes\nand the associated feedback; [5] the effects due to re-ionisation of the universe by the\nultra-violet radiation field.\nThe computer code for each section implements astrophysically motivated algo-\nrithms, each process drawing on the inputs provided by each of the other modules. The\nmodules link together to form a network of non-linear equations that are integrated in\ntime to trace the evolving properties of the galaxy population. In total the model uses\nover 50,000 lines of computer code. Further details are described in appendix B, while\nBaugh (2006) presents an introduction to the internal workings of the code.\n2.2 Galform and Dark Matter\nIn order to run, Galform requires a forcing function1 that represents the positions,\nand subsequent collisions, of all lumps of dark matter containing galaxies, at all times\n(referred to as the \u201cmerger histories of the Dark Matter Haloes\u201d). This information is\nextracted from the Millennium simulation (a large Dark matter simulation described in\nappendix B), and, with it, Galform can model the far more complicated behaviour of\nbaryonic (i.e. normal) matter. It is the baryonic matter that is responsible for the more\n1 We use the term \u2018forcing function\u2019 in the differential equation sense, referring to a function that\nappears in a network of differential equations and which depends on time only. Knowledge of this\nfunction is required before one can attempt to numerically solve or integrate the system of equations.\nI. Vernon, M. Goldstein and R. G. Bower 623\nl\nl l l l l l l l l l l l l l l l l l l l l l l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l l l\n14 16 18 20 22\n\u2212\n6\n\u2212\n5\n\u2212\n4\n\u2212\n3\n\u2212\n2\n\u2212\n1\nbj Luminosity Function Wave 1\nbj Luminosity\nlo\ng(N\no. \nG\nal\nax\nie\ns \npe\nr u\nni\nt V\no\nlu\nm\ne)\nl\nl\nl l\nl l l l l l l l l l l l l l l l l l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\n18 20 22 24 26\n\u2212\n6\n\u2212\n5\n\u2212\n4\n\u2212\n3\n\u2212\n2\n\u2212\n1\nK Luminosity Function Wave 1\nK Luminosity\nlo\ng(N\no. \nG\nal\nax\nie\ns \npe\nr u\nni\nt V\no\nlu\nm\ne)\nFigure 1: The bj (left) and K (right) Luminosity Functions giving the (log) number\nof galaxies per unit volume, binned by luminosity. The data are shown as the black\npoints, along with 2 sigma intervals representing all relevant uncertainties identified\nin section 5. The coloured lines are the Galform outputs from 993 Wave 1 runs of the\nmodel described in section 4.2, none of which were found to be acceptable as they didn\u2019t\npass sufficiently close to all of the observed data points. The vertical lines show the 7\noutputs chosen for emulation also described in section 4.2.\nintricate processes involved in galaxy formation.\nAs the Millennium simulation covers a substantial volume (1.63 billion light years\ncubed), its results are split into 512 sub-volumes, each of which can be used as a forcing\nfunction to the Galform model. The run time for one evaluation of the Galform model on\na single sub-volume is approximately 30 minutes. The Galform group provided shared\naccess to a cluster of 256 processors. Previous attempts by the cosmologists to calibrate\nGalform focussed on the first 40 sub-volumes out of 512, and we follow this approach\nhere while taking account of the uncertainty this generates.\n2.3 Inputs\nEach module of Galform has associated input parameters, which define the workings\nof the module. The Galform model has a total of 17 inputs that relate to various\nuncertain physical processes involved in galaxy formation. We denote the vector of 17\ninput parameters as x. In order to run the code, the astrophysicist must specify values\nfor each of these input parameters. Some parameters are quite well defined by numerical\nexperiments or targeted observational data, but others are highly uncertain.\nAll 17 inputs x, along with their considered ranges, and associated modules are\nshown in table 1. Also shown are the variables that are initially considered, and those\nvaried in Wave 1 of our analysis: this will be discussed in section 4.3.\n624 Galaxy Formation: a Bayesian Uncertainty Analysis\nInput symbol min max Initial Varied in Process\nParameters Variables W1 (x[B]) Modelled\nvhotdisk Vhot,disk 100 550 x x SNe feedback\nvhotburst Vhot,burst 100 550 x x \u00b7\nalphahot \u03b1hot 2 3.7 x \u00b7\nalphareheat \u03b1reheat 0.2 1.2 x x \u00b7\nalphacool \u03b1cool 0.2 1.2 x x AGN feedback\nepsilonEdd \u000fEdd 0.004 0.05 \u00b7\nepsilonStar \u000f? 10 1000 x x Star Formation\nalphastar \u03b1? -3.2 -0.3 \u00b7\nyield pyield 0.02 0.05 x \u00b7\ntdisk tdisk 0 1 \u00b7\nstabledisk fstab 0.65 0.95 x x Disk stability\ntau0mrg fdf 0.8 2.7 Galaxy Mergers\nfellip fellip 0.1 0.35 \u00b7\nfburst fburst 0.01 0.15 \u00b7\nFSMBH Fbh 0.001 0.01 \u00b7\nVCUT vcut 20 50 Reionisation\nZCUT zcut 6 9 \u00b7\nTable 1: Table of Parameter Ranges (which were converted to -1 to 1 for the analysis),\nincluding the initial variables considered and those that are possibly active and analysed\nin Wave 1 (referred to as x[B]). Parameters are grouped by physical process.\n2.4 Outputs\nGalform provides several different sets of output data related to various physical char-\nacteristics of the simulated galaxies. Observational data of differing degrees of accuracy\nare available for comparison with the Galform model output, the most important of\nthese being the bj and K Luminosity Functions. These Luminosity functions give the\n(log) number of galaxies per unit volume, binned with respect to luminosity. We con-\nsider two types of luminosity function: \u201cbj\u201d and \u201cK\u201d which correspond to blue and\ninfrared wavelengths of light, and which are more representative of younger and older\ngalaxies respectively (but not exclusively: most galaxies emit measurable amounts of\nboth wavelengths of light). Figure 1 shows the bj and K luminosity functions in the\nleft and right panels respectively. The 993 model runs used in wave 1 are given by the\ncoloured lines (see section 4.2). Observed data is given by the black dots, with 2\u03c3 error\nbars representing all the uncertainties described in section 5. Such data was gathered by\nthe 2dFGRS sky survey (Colless et al. (2001)): telescopes sweep across sectors of the sky\nmeasuring hundreds of thousands of galaxies and their properties. In order to convert\nthe data into the form given in figure 1 (current day, absolute luminosities), the data\nmust be heavily processed to correct for several factors, for example, the redshifting of\nlight emitted by galaxies due to their velocity away from Earth, and the time delay in\nreceiving such light which implies we are effectively measuring distant galaxies billions\nI. Vernon, M. Goldstein and R. G. Bower 625\nof years in the past (this is discussed in section 5.2). See Norberg et al. (2002) and Cole\net al. (2001) for the details of such processing for the bj and K data respectively.\nThe Luminosity Function data set represents the most accurately measured observa-\ntional data available and is seen as the benchmark by which models of galaxy formation\nare judged (Norberg et al. (2002)). Even if a particular galaxy formation model performs\nwell with respect to other data sets, if it does not match the Luminosity function to an\nacceptable level then that model will be discarded. For these reasons, it was decided\nto focus our analysis on identifying the regions of input space that give rise to matches\nbetween the model output and the bj and K observed luminosity functions. Additional\ndata sets could then be used at a later date to restrict the input space further.\n3 Uncertainty Analysis for Computer Simulators\n3.1 Uncertainty in complex models\nOur aim is to identify that region of the input space of the Galform simulator for\nwhich certain aspects of Galform output match closely to observations. As such, this\nstudy falls within the general area of the analysis of uncertainty arising when we study\ncomplex physical systems by means of mathematical models typically implemented as\ncomputer simulators. The general version is as follows. A computer simulator f takes as\ninput the vector x, which represents certain physical properties of a system of interest.\nThe simulator output vector, f(x), corresponds to certain aspects of the behaviour\nof the system. For given inputs, this behaviour is determined by equations embodying\nthe relevant theoretical knowledge relating system properties to system behaviour. This\napproach is common to many areas of science. We can talk of an emergent methodology\nbecause, despite the enormous differences between each of the individual models, all such\nproblems of physical modelling confront a similar collection of basic uncertainties.\n[1] Parameter uncertainty. We do not know the appropriate values of the in-\nputs x to the simulator. In some cases, we may not even know whether there is any\nappropriate choice for the inputs. Galform is a case in point. If we have misrepresented\nthe underlying physics, for example if the role of Dark Matter is not supported by\nobservational evidence, then the meaning of the model and the interpretation of the pa-\nrameters will be called into question. In particular, were we to discover that there were\nno choices of inputs for which Galform output matched observations in our universe,\nthen that might provide part of the evidence which would call the current account of\ncosmology into question.\n[2] Simulator uncertainty. For any choice of inputs, x, the output of the Galform\ncode f(x) is a deterministic computer function. However, many computer simulators\nare very expensive, in time and resources, to evaluate, for any choice of inputs. In\npractice, it is appropriate to consider that the output values of such a simulator are\nunknown except at the input choices at which the simulator has been evaluated. An\nimportant stage in the analysis, therefore, is the construction of a statistical represen-\ntation or emulator for the simulator. The emulator represents our uncertainty about\n626 Galaxy Formation: a Bayesian Uncertainty Analysis\nthe value of the function at each possible input choice, and therefore acts both as an\napproximation to the function and as an assessment of the uncertainty introduced by\nthe approximation. Much of the literature on computer experiments is concerned\nwith efficient methods for building emulators; see for example Sacks et al. (1989); Sant-\nner et al. (2003); Currin et al. (1991). For our Galform investigations, we have made\nmany evaluations of the simulator. Even so, emulation has proved essential in extending\nour uncertainty description from the function evaluations to the remainder of the input\nspace.\n[3] Structural uncertainty. However carefully we have constructed our model,\nthere will always be a difference between the system and the simulator. Inevitably,\nthere will be simplifications in the physics, based on features that are too complicated\nfor us to include, features that we do not know that we should include, mismatches\nbetween the scales on which the model and the system operate, and simplifications and\napproximations in solving the equations determining the system. Often, understanding\nthis structural uncertainty will be one of the most challenging aspects of the analysis.\n[4] Observational error. This uncertainty arises when we match our model to\nsystem observations. Often, in complex physical systems, the observations are them-\nselves somewhat indirect, being assessed on the basis of extensive preprocessing based\non various additional theoretical constructs, that are external to the theory underlying\nand represented by, the computer model. Further, the measurements may not directly\ncorrespond to the outputs of the simulator and therefore require an extra layer of in-\nterpretation and analysis before the model predictions and the system observations can\nbe compared. The observational error in Galform is of a particularly complex form,\nrequiring considerable processing to transform the system observations to a comparable\nspatio-temporal resolution to the simulator outputs.\n[5] Initial condition and forcing function uncertainty. This corresponds to\nall of the other aspects of the simulator which need to be specified before the model may\nbe evaluated. For example, the Galform simulator requires a full spatial specification of\nthe arrangement of Dark Matter at all times in the development of the universe, and so\nwe need to account for the uncertainty introduced as we do not know this configuration.\nOften such specifications are too complex to be treated in the same manner as parameter\nuncertainty, as is discussed by House et al. (2009).\nIn this study, we will describe how we address each of these sources of uncertainty\nfor the Galform project. We aim to be careful and thorough, but we must also recognise\nthat, for a complex model such as Galform, uncertainty modelling is a process which\nis similar in many ways to the physical modelling process on which we are building.\nQuantifications of uncertainty depend on complex scientific judgements over which dif-\nferent experts may have different views. Further, expert knowledge is held collectively\nover a wide community of experimenters, observationalists, theoreticians and modellers.\nTherefore, it is as misleading to talk of a definitive assessment of the uncertainty asso-\nciated with Galform as it would be to talk of a definitive form for the Galform model\nitself as experts would not currently agree on the precise form the Galform code should\ntake. Assessment of uncertainty is an ongoing process for models which are, themselves,\nI. Vernon, M. Goldstein and R. G. Bower 627\nundergoing continuous development. Our account documents one iteration in this on-\ngoing process, albeit one for which the uncertainty analysis is carried out to a much\ngreater level of detail than is usual in this field (or indeed in most analyses of complex\nphysical models in any area of application of which we are aware).\n3.2 Linking the simulator with the system\nWe now introduce the general structure describing the relationship between the sim-\nulator and the physical system. We describe this link for the Galform simulator, but\nthe ingredients are common to many simulator analyses. We denote by z the vector\nof observations that we use for this study. Our choice for z will be the observed num-\nbers of galaxies of various degrees of luminosity, assessed separately for younger and for\nolder galaxies and expressed on the log scale. We describe the relationship between the\nobservations, z, and the vector of true physical system values, y, as\nz = y + \u000fobs (1)\nwhere \u000fobs is the experimental error, which we judge to be uncorrelated with y (\u000fobs\nrepresents uncertainty of type [4] in section 3.1).\nIs the theoretical understanding of Galaxy formation, as embodied in Galform, con-\nsistent with observations z? Galform is represented as a function, which maps the inputs\nx to the outputs f(x). The theoretical description involves the notion that when we\nevaluate Galform at the actual system properties, x\u2217 say, then we should reproduce the\nactual system behaviour y. This does not mean that we would expect perfect agreement\nbetween f(x\u2217) and y. Although Galform is a highly sophisticated simulator, it still of-\nfers a necessarily simplified account of the evolution of galaxies, and approximates the\nnumerical solutions to the governing equations. The simplest way to view the difference\nbetween f\u2217 = f(x\u2217) and y is to express this as\ny = f\u2217 + \u000fmd, (2)\nwhere we consider that \u000fmd is uncorrelated with f\u2217 and with x\u2217. Expressing our judge-\nments about the likely size of the model discrepancy, \u000fmd, determines how close a fit\nbetween model output, f\u2217, and observation y we require for an acceptable level of con-\nsistency between theory and observation. \u000fmd represents uncertainties [3] and [5] from\nsection 3.1. Note that \u000fmd is a vector of length equal to the number of observations\nconsidered, and may have a rich covariance structure across the different outputs (see\nsection 5.1).\nWe search for choices of input x for which the output f(x) is sufficiently close to\ny that we would declare the observed output to be compatible with the predictions of\nthe model, when we allow for model discrepancy. In practice, all that we can compare\nis f(x) and z, which we do by combining (1) and (2). Achieving an acceptable match,\nfor a particular input choice x, does not mean that the model is \u201ccorrect\u201d or that\na parameter choice which achieves the match corresponds to the \u201ctrue\u201d value of the\nparameters, but simply that this version of the model will have met the challenge of\n628 Galaxy Formation: a Bayesian Uncertainty Analysis\nreproducing an important observational aspect of the galaxy formation study within\nour agreed tolerance level. Similarly, identifying the whole collection of possible choices\nof inputs x which achieve an acceptable match and the subsequent analysis of such a\ncollection, is informative as regards the model and the galaxy formation process itself,\nas is discussed in section 8.\nThe form (2) is simple and intuitive, and is widely used in computer modelling\nstudies. In our case, this corresponds to the natural approach in which we ask whether\nwe could view Galform, with appropriate choice of inputs, as adequately reproducing the\nobserved universe, within the tolerance set by the model discrepancy. In this account, we\ntherefore ignore all of those additional aspects of our uncertainty modelling which would\ncorrespond to a more sophisticated analysis of model discrepancy, based, for example,\non informed expert judgements as to the ways in which the Galform simulator is likely to\nevolve over the coming years. A detailed specification of such features would potentially\nbe highly insightful, and might result in a much richer correlation structure across the\nelements of the discrepancy vector; see Goldstein and Rougier (2009). However, as we\nshall describe, it is challenging even to make a meaningful order of magnitude assessment\nof discrepancy variation, and so, as a first uncertainty quantification for Galform, we\nchose to focus on the most important large scale components of uncertainty.\n3.3 Bayes Linear Analysis\nIn this case study, we follow the Bayes linear approach to uncertainty quantification and\nanalysis. This approach is relatively simple in terms of belief specification and analysis,\nas it is based only on mean, variance and covariance specifications which, following de\nFinetti, we take as primitive; see De Finetti (1974, 1975). The appropriate updating\nrules for expectations and variances for a vector B, given a vector D are\nED[B] = E(B) + Cov(B,D)Var(D)\u22121(D \u2212 E(D)), (3)\nVarD[B] = Var(B)\u2212 Cov(B,D)Var(D)\u22121Cov(D,B). (4)\nED[B] and VarD[B] are termed the adjusted mean and variance of B given D (Goldstein\n(1999); Goldstein and Wooff (2007)).\nIn this formulation, the probability of an event is the expectation of the correspond-\ning indicator function. Conditional expectation may be viewed as the special case of\nbelief adjustment where we base the adjustment on the indicator functions for a collec-\ntion of events which constitute a partition. There are many areas of similarity between\nfull Bayes and Bayes linear analyses. In particular, a full Gaussian specification for all\nof the relevant quantities would lead to similar updating formulae. For a detailed treat-\nment, see Goldstein and Wooff (2007). An overview of the approach is given in Goldstein\n(1999).\nThere are two basic interpretations that we may give for a Bayes linear analysis. The\nfirst view arises as, when we attempt to carry out a full Bayesian analysis, we may face\nI. Vernon, M. Goldstein and R. G. Bower 629\ntwo kinds of difficulty. Firstly, the probability specification that we may be required to\nmake in order to describe fully all of the uncertainties for the problem may be extremely\ncomplex and subtle, and secondly the full Bayesian analysis may be technically very\nchallenging to carry out and highly non-robust, as the posterior judgments may depend\non aspects of the prior specification which we are unable to specify with confidence,\ngiven constraints of time, resource and knowledge.\nIn such circumstances, the Bayes linear analysis that we propose may be viewed\nas a pragmatic compromise to this full analysis. The task of probability specification\nis simplified as we only need to give means, variances and covariances for all of the\nrandom quantities involved in the analysis. The subsequent analysis is simplified as,\nrather than carrying out a full posterior assessment given the observations, we may\ncarry out a Bayes linear update, as determined by equations (3) and (4). The adjusted\nexpectation for B given D is the best linear fit for B, using the elements of D, in terms\nof minimising expected squared error loss - this minimum expected loss is the adjusted\nvariance. This minimisation depends only on the second order specification that we\nhave made. We may expand the Bayes linear analysis by using whichever collection of\nfunctions of the elements of D that we wish, in order to assess the adjusted expectation,\nas long as, for each chosen function of the elements of D, we are prepared to assess\nthe full corresponding second order specification. If we introduce all functions of the\nelements of D, then we are effectively adjusting B by the partition on D, and we retrieve\nthe conditional Bayesian expectation given the observed outcome for D.\nIn many problems, the extra effort and complications involved in assessing and\nanalysing the full probabilistic structures over the complex and high dimensional input\nand output spaces which are required in order to carry out the full Bayes analysis may\nnot be rewarded by a corresponding improvement in accuracy. For example, in our\nexperience, it is usually reasonable to suppose that we can elicit expert judgements\nabout the order of magnitude of quantities such as model discrepancy terms which\nare sufficient for us to make variance and covariance assessments across the various\ncomponents of the model. The qualitative structure that we impose in order to make\nsuch elicitations is based on relations such as (1) and (2), which impose the requirement\nthat the two terms on the right hand side of each equation are uncorrelated. This already\nis a strong assertion, and we might well be reluctant to extend this to a judgement of full\nprobabilistic independence between the corresponding terms. Therefore, we may prefer\nthat our analysis should only depend on those properties which follow directly from this\nspecified lack of correlation, rather than relying on an infinite number of further joint\northogonality constraints as required by full probabilistic independence.\nEven were we able to make a meaningful full probabilistic elicitation for the problem\ndescribed in this paper, then, due to the iterative nature of our approach, which repeat-\nedly reduces the volume of the space which we are exploring by imposing a series of\nvery complicated and highly non-linear constraints, it would be enormously challenging\neven to construct a tractable Bayesian MCMC analysis. In contrast, the Bayes linear\nanalysis is comparatively straightforward and is sufficient to achieve the study objective\nof identifying a class of good matches to observed history. Therefore, while it would be\nof interest to compare our results with the full Bayes version of the calculations that\n630 Galaxy Formation: a Bayesian Uncertainty Analysis\nwe will describe, the technical challenges in doing so would be very great, and this is\nquite apart from the inherent non-robustness of the iterative version of the full Bayes\nanalysis, due to the extremely complex, multi-modal form of the likelihood function.\nHaving said this, if such a full Bayes analysis were feasible, it would contain additional\ninformation about all of the relationships within the problem compared to the Bayes\nlinear approach, and so would allow more detailed inferences to be drawn.\nThe reason that the Bayes linear approach is successful as a surrogate for the full\nBayes analysis derives from the second, and more fundamental, interpretation of the\nmeaning and value of this analysis. We consider that a Bayesian analysis has value\nlargely because we view it as the appropriate way to combine expert judgement and\nobservations to give appropriate posterior judgements. However, there are two problems\nwith this view. Firstly, due to the complexity of the problems that we face, often we\nare unable to make a prior specification which adequately represents our true state of\nuncertainty, and so the posterior analysis unavoidably inherits this lack of accuracy.\nSecondly, the operation of conditioning, itself, does not offer a complete description as\nto how judgements should be modified given new evidence. This is a larger issue than we\nhave space to address here. For those interested in the fundamental reasons why we view\nthe Bayes linear analysis as appropriate for complex uncertainty problems, under partial\nbelief specification, we refer to the discussion in section 4 of Goldstein (2006) where\nthe temporal relationships between Bayes linear adjustment and posterior beliefs are\ndescribed and the foundational properties of standard Bayesian approaches are shown to\nbe inherited from their status as Bayes linear adjustments. The role of such foundational\nconsiderations in the interpretation of the Bayes linear analysis for problems arising in\nthe study of large computer models is discussed in detail in Goldstein (2010) where\nissues arising in the treatment of the Galform model are used as an illustration.\n3.4 Emulation\nWe are interested in the behaviour of Galform over the whole of its specified input\nspace. The substantial run time and the high dimensional input space combine to make\ndirect exploration by model runs alone infeasible. We express our beliefs about Galform\noutputs at all locations in the input space by constructing an emulator (see uncertainty\ntype [2] in section 3.1). An emulator is a stochastic belief specification for a determin-\nistic function (Craig et al. (1996, 1997); O\u2019Hagan (2006); Oakley and O\u2019Hagan (2002);\nConti et al. (2009); Higdon et al. (2004), and for a cosmology application Heitmann\net al. (2009)). The emulator is much faster to evaluate than the simulator, so that we\nmay explore the input space using the emulator, while taking into account the extra\nuncertainty that we have introduced by substituting emulator for simulator evaluations.\nWe construct our emulator for output i of the function f(x) to have the form\nfi(x) =\n\u2211\nj\n\u03b2ij gij(x) + ui(x), (5)\nwhere B = {\u03b2ij} are unknown scalars, gij are known deterministic functions of x and\nu(x), uncorrelated with B, is a weakly stationary stochastic process with constant vari-\nI. Vernon, M. Goldstein and R. G. Bower 631\nance. The regression term on the right hand side of equation (5) expresses the global\nbehaviour of the function while u(x) represents localised deviations from this global\nbehaviour near to x.\nIn the Bayes Linear approach, the emulator specification requires a mean vector and\na variance matrix for B and values for the mean, variance and correlation function of\nu. A simple specification for u(x) is to suppose, for each x, that ui(x) has zero mean\nwith constant variance and where Corr(ui(x), ui(x\u2032)) is a function of \u2016x\u2212 x\u2032\u2016.\nWith high dimensional input spaces, it is common to find, for any output, fi say,\nthat a subset x[i] of the inputs has the most influence in explaining the variation in the\nvalue of fi(x), where x[i] varies with i. We reform the emulator as\nfi(x) =\n\u2211\nj\n\u03b2ij gij(x[i]) + ui(x[i]) + wi(x), (6)\nwhere ui(x[i]) has zero mean, and covariance structure given by Cov(ui(x[i]), ui(x\u2032[i])) =\n\u03c32ui exp(\u2212||x[i]\u2212x\u2032[i]||2\/\u03b82i ) (this is the commonly used Gaussian form, see section 4.3 for\nmore details). Here wi(x) is a \u201cnugget term\u201d with constant variance \u03c32wi over x, zero\nmean and Cov(w(x), w(x\u2032)) = 0 for x 6= x\u2032. The collection x[i] is often called the active\nvariables for fi, and wi(x) expresses all of the variation in f(x) which arises if we view\nthe emulator f(x) simply as a function of x[i].\nWe can use the emulator to evaluate the expectation and variance of the function,\nfor any input x and the covariance between the values of f at any pair of points x, x\u2032.\nFrom (6), these are\n\u00b5i(x) = E(fi(x)) =\n\u2211\nj\nE(\u03b2ij) gij(x[i]), (7)\n\u03bai(x, x\u2032) = Cov(fi(x), fi(x\u2032)) (8)\n= Cov(\n\u2211\nj\n\u03b2ij gij(x[i]),\n\u2211\nj\n\u03b2ij gij(x\u2032[i])) + \u03c3\n2\nui exp(\u2212||x[i] \u2212 x\u2032[i]||2\/\u03b82i ) + \u03c32wiIx,x\u2032\nwhere Ix,x\u2032 = 1 if x = x\u2032, and zero otherwise. In the Bayes linear approach this is all\nthat is required to be able to update our beliefs in terms of expectation and variances,\nabout the model output fi(x) at a new, unevaluated input point x, given a set of model\nevaluations, as we now describe.\nSay we have performed n model evaluations over some space filling design. We write\nthe locations of the n runs in input space as x(k) with k = 1, .., n where each x(k) rep-\nresents the vector of inputs for the kth run. Similarly x(k)[i] is defined to be the vector of\nActive Variable inputs for the kth run. We define Di = (fi(x(1)), fi(x(2)), ..., fi(x(n)))T ,\nthat is the column vector of the n evaluation outputs for output i, the prior expectation\nof which (E(Di)) can be found using equation (7). Replacing the random quantity B in\nthe Bayes Linear update equation (3) with the unknown output fi(x) at input x, and\n632 Galaxy Formation: a Bayesian Uncertainty Analysis\nreplacing D with Di, gives the adjusted expectation EDi [fi(x)] to be:\nEDi [fi(x)] = E(fi(x)) + Cov(fi(x), Di)Var(Di)\n\u22121(Di \u2212 E(Di))\n=\n\u2211\nj\nE(\u03b2ij) gij(x[i]) + t(x)TA\u22121(Di \u2212 E(Di)), (9)\nwhere, by equation (7), t(x) = (\u03bai(x, x(1)), \u03bai(x, x(2)), ..., \u03bai(x, x(n)))T = Cov(fi(x), Di)T\nis the column vector of covariances between the new and known points, and A is\nthe matrix of covariances between known points: an n \u00d7 n matrix with elements\nAjk = \u03bai(x(j), x(k)). The Adjusted Variance VarDi [fi(x)] can similarly be found from\nequations (4) and (7) giving:\nVarDi [fi(x)] = Var(fi(x))\u2212 Cov(fi(x), Di)Var(Di)\u22121Cov(Di, fi(x)),\n= Var(\n\u2211\nj\n\u03b2ij gij(x[i])) + \u03c32ui + \u03c3\n2\nwi \u2212 t(x)TA\u22121t(x). (10)\nThe adjusted expectation and variance, EDi [fi(x)] and VarDi [fi(x)], represent our up-\ndated beliefs about the output of the Galform function f(x) at input x given a set of n\nmodel runs Di, and are used in the implausibility measures described in section 3.5.\nThere is some debate in the computer experiment literature as to whether it is\npreferable to put a lot of effort into constructing the regression terms in the emulator\nor whether it is better to construct a simple mean function and to place more weight on\nthe residual process u(x). We prefer, where possible, to put as much detail as is feasible\ninto the mean function, for the following reasons.\nFirstly, many physical models, and Galform in particular, exhibit strong and physi-\ncally interpretable monotonicities which are naturally expressed through the mean func-\ntion. Secondly, it is easier for the expert to assess whether the emulator formulation\nis consistent with informed scientific judgement about the behaviour of the function if\na large proportion of the variability is expressed through regression terms. Thirdly, if\nmuch of the structure of the emulator is encoded in the regression function, then this\nsimplifies various of the calculations that we need to make when comparing the model\nto observations and suggests very cheap approximations to calculations which would\notherwise be very expensive. Finally, in our experience, the form of local process, u(x),\ncan be difficult to assess, even with large numbers of function evaluations. Partly, this is\nbecause there is a fundamental confounding between the location of the mean function,\nthe size of the residual variance (\u03c32ui +\u03c3\n2\nwi), and the strength of the residual correlation,\nparameterised by \u03b8i. Partly, also, this is because any form of correlation function that\nwe fit necessarily approximates the different degrees of smoothness of the function across\ndifferent areas of the input space, and many methods of estimating smoothness param-\neters are potentially non-robust when applied to processes which do not fit exactly to\nthe assumptions that are used to generate the fitting algorithms. Therefore, we prefer\nto model as much of the variation in the function as we can by the regression form, to\nreduce the residual variance as much as is feasible, and then to be fairly conservative in\nchoosing the length of correlation \u03b8i that we shall impose.\nI. Vernon, M. Goldstein and R. G. Bower 633\nIn general computer experiments, we choose our form for the emulator by a com-\nbination of expert judgement based on physical intuition and experience with earlier\nversions of the model and, where appropriate, by preliminary experiments with fast\napproximate version of the simulator. In our case, we were able to make a collection of\nevaluations of the simulator, based on a Latin Hypercube design, which was sufficiently\nlarge to allow us to fit the emulator directly from our functional evaluations. Therefore\nwe proceeded as follows, for each output that we chose to emulate.\nFirstly, we carried out statistical model fitting, given the collection of runs, to se-\nlect the deterministic functions gij , to assess the values of the coefficients B and to\nassess the residual variance and covariance function, u(x) and, where appropriate, to\nidentify active subsets x[i]. We then used equations (9) and (10) to update our beliefs\nabout the function f(x), obtaining the adjusted expectation and variance EDi [fi(x)]\nand VarDi [fi(x)], for any input of interest x. We then checked that the form of the\nemulator was physically meaningful. Finally, we carried out a diagnostic analysis on\nour emulator. We will give details of these stages below.\n3.5 History Matching\nThe aim of this study is to estimate the set of input values X \u2217 for which the evaluation\nof f(x) gives an acceptable match to the observations z, and to obtain a substantial\ncollection of realised evaluations of the function which actually do yield acceptable\nmatches and which may then be used to explore the match between other aspects of\nthe Galform output and the corresponding observational information.\nWe refer to the process of identifying the collection X \u2217 as history matching. This\nterminology is common in various applications (e.g. Raftery et al. (1995)), and in\nparticular in oil reservoir modelling (Craig et al. (1996, 1997); Cumming and Goldstein\n(2009)), where it refers to the process of adjusting the inputs to a simulator of an oil\nreservoir until the output closely reproduces features such as the historical oil production\nand pressure profiles at all of the wells. The emphasis on identifying all of the possible\nmatches to observation is ours. (Pragmatically, reservoir engineers often stop when a\nfew matches, or even just one, have been obtained.)\nHistory matching may be compared to model calibration in which we suppose there\nis a single \u201ctrue but unknown\u201d value x\u2217 and our objective is to make probabilistic\nstatements as to this value, based on a prior specification for x\u2217, the collection of\nmodel evaluations and the observed history (Kennedy and O\u2019Hagan (2001); Higdon\net al. (2008); Goldstein and Rougier (2006)). Calibration and history matching are\nthematically related, but fundamentally different. For example, calibration will always\nresult in a proper posterior distribution over the input space, while history matching\nmight lead to the conclusion that the collection of acceptable matches was empty. It\nwould be of great interest to find that X \u2217 was empty in the Galform study, as that\nmight suggest possible defects in the general theory underlying the simulation process.\nHowever, in this study, we do find a collection of good fits to the observations. Our\nview is that history matching, as a form of model checking, is always of interest for\n634 Galaxy Formation: a Bayesian Uncertainty Analysis\nassessing computer models and calibration sometimes is. Even when we wish to carry\nout a model calibration, it is often good practice first to carry out a history match,\npartly to see whether such a match is achievable, and partly to reduce the size of the\ninput space over which the calibration exercise will need to be performed.\nOur approach to history matching is based on the assessment of certain implausibility\nmeasures (Craig et al. (1996, 1997)). An implausibility measure is a function defined\nover the input space which, when large, suggests that the match between model and\nsystem would exceed our stated tolerance. We may build this up as follows, for a\nsingle output fi(x), where i labels the output. For a given choice, x\u2217, we would like\nto assess whether the output fi(x\u2217) differs from the system value yi by more than the\ntolerance that we allow in terms of model discrepancy. Therefore, we would assess the\nstandardised distance\n(yi \u2212 fi(x\u2217))2\nVar(\u000fmd:i)\nIn practice, we cannot observe yi and so we must compare fi(x\u2217) with the observation\nz, introducing measurement error, with corresponding standardised distance\n(zi \u2212 fi(x\u2217))2\nVar(\u000fmd:i) + Var(\u000fobs:i)\n(11)\nHowever, for most values of x, we are not able to evaluate f(x) so we use the emulator\nand compare zi with E(fi(x)). Therefore, the implausibility function is defined as\nI2(i)(x) =\n(E(fi(x))\u2212 zi)2\nVar(E(fi(x))\u2212 zi) =\n(E(fi(x))\u2212 zi)2\nVar(fi(x)) + Var(\u000fmd:i) + Var(\u000fobs:i)\n(12)\nWhen I(i)(x) is large, this suggests that, even given all the uncertainties present in\nthe problem, we would be unlikely to view as acceptable the match between model\noutput and observed data were we to run the model at input x. Therefore, we consider\nthat choices of x for which I(i)(x) is large can be discarded as potential members of\nthe set X \u2217. We discard regions of the input space by imposing suitable cutoffs on the\nimplausibility function in that we discard x unless I(i)(x) < c. The choice of cutoff c\ncomes from consideration of the fraction of space removed, and from general unimodality\narguments, as follows. Regarding the individual univariate Implausibility Measures\nI(i)(x), if we consider that for fixed x the appropriate distribution of (E(fi(x\u2217))\u2212 zi) is\nboth unimodal and continuous, then we can use the 3\u03c3 rule (Pukelsheim 1994) which\nimplies quite generally that if x = x\u2217, then I(i)(x) < 3 with a probability of greater than\n0.95. This result applies even, for example, for highly skew or heavy tailed distributions.\nValues higher than 3 would suggest that the point x could be discarded.\nIn our comparisons, we have a separate implausibility function, given by equation\n(12), for each output that we use for history matching. We may choose to make some\nintuitive combination of the individual implausibility functions as a basis of eliminating\nportions of the input space. The simplest of these is obtained by maximising I(i)(x)\nover the considered outputs and we hence define the Maximum Implausibility Measure\nIM (x) = max\ni\nI(i)(x). (13)\nI. Vernon, M. Goldstein and R. G. Bower 635\nThis measure is used in later waves of our analysis and it represents a major part of the\ndefinition of an acceptable match. It is, however, sensitive to problems concerning the\ninaccuracies of individual emulators, and so we define the Second and Third Maximum\nImplausibility Measures I2M (x) and I3M (x) as:\nI2M (x) = max\ni\n( {I(i)(x)} \\ IM (x) ), (14)\nI3M (x) = max\ni\n( {I(i)(x)} \\ {IM (x), I2M (x)} ), (15)\nthat is defining I2M (x) and I3M (x) to be the second and third highest value out of the\nset of univariate measures I(i)(x) respectively. These were used in the first wave as they\nwere considered relatively safe measures in that they were less sensitive to the possibility\nthat one of the emulators was inaccurate. We also construct the natural multivariate\nanalogue of the implausibility, for later waves, which takes the form:\nI(x) = (z \u2212 E(f(x)))T (Var(z \u2212 E(f(x))))\u22121(z \u2212 E(f(x))) (16)\nThe multivariate form is more effective for screening the input space, but it does require\ncareful consideration of the covariance structure for the various quantities.\nHistory matching is an iterative process. We begin by emulating Galform over the\nwhole input space. We evaluate our implausibility measures over the whole space and\nremove from the space all input choices for which the implausibility measure is large. We\nthen re-sample within the remaining input space, denoted X1, and re-emulate Galform\nwithin this reduced space. This is termed refocusing, and we proceed to employ this\nprocess iteratively as represented by the following algorithm. At each iteration or Wave:\n1. A design for a set of runs over the current non-implausible volume Xi is created,\nusing a latin hypercube design with a rejection strategy based on each of the\npreceding implausibility measures.\n2. These runs are used to construct a more accurate emulator defined only over the\ncurrent non-implausible volume Xi.\n3. The implausibility measures are then recalculated over Xi, using the new emulator.\n4. Cutoffs are imposed on the Implausibility measures and this defines a new, smaller\nnon-implausible volume Xi+1 which should satisfy X \u2217 \u2282 Xi+1 \u2282 Xi.\n5. Unless the emulator variance is now small in comparison to the other sources of\nuncertainty, or unless computational resources are exhausted, return to step 1.\n6. Generate a large number of acceptable runs from the final non-implausible volume.\nThe reasons that we may hope to further reduce the acceptable space at each iteration\nare firstly that we produce a higher relative density of runs at each stage, so that emu-\nlation is more effective, secondly that we may expect the function to become smoother\nand so easier to emulate as we reduce the area of the input space, and thirdly because,\nwhen we have accounted for much of the uncertainty related to the most important\nactive variables, then variables which did not account for much of the variability in\nthe original emulation may take on larger importance and therefore allow us to resolve\n636 Galaxy Formation: a Bayesian Uncertainty Analysis\nmore of the uncertainty of the function. In this study, we refocused four times, and\nthen carried out a fifth set of evaluations which produced a large number of runs which\ngave good matches to observations. This continued refocusing is very useful, but it also\nbrings its own complications, as the only way in which we can determine whether an\ninput value lies within our retained collection of potential history matches is by ap-\nplying each implausibility function in turn and seeing whether each such evaluation is\nsmall enough for the input choice to be retained. This raises practical computational\nissues, which makes it important to have fast approximate methods to screen the input\nspace, and also raises basic questions about practical visualisation methods to help us\nto represent and interpret the shape of the input space which we have retained.\nAll of these complications reflect the enormous difficulty of carrying out a fully\nBayesian history matching exercise over a corresponding number of waves to that of our\nstudy. Rather than constructing the full probabilistic edifice, we have identified certain\nkey aspects of the subjective judgements relating to the function, the model discrepancy\nand the observational error and used these to construct an event with low subjective\nprobability for x \u2208 X \u2217 and much higher, though not explicitly evaluated, probability\notherwise, which we have used to progressively filter the input space. The successes of\nthis method, for example in this study we do identify a rich space of acceptable fits,\nsuggests that we are indeed exploiting the probabilistic judgments in a meaningful way,\nbut this does raise the basic question as to whether there is some tractable intermediary\nbetween our version of history matching and the full Bayesian solution that would be\neven more effective in achieving our goals.\n4 First Wave Analysis\n4.1 General Designs for Computer Model Experiments\nWe have to explore the high-dimensional input space of the Galform model, which takes\na significant amount of time to run. Therefore the design for the set of inputs where\nmodels will be evaluated is very important: (Currin et al. 1991; Sacks et al. 1989;\nSantner et al. 2003). The design should be space-filling (to maximise coverage of the\nspace), and approximately orthogonal (where possible) as we will be fitting polynomials\nto the outputs when constructing the emulator. Various designs have been discussed in\nthe Computer Model literature (Santner et al. 2003), with a popular choice being the\nMaximin Latin hypercube design. An n point Latin Hypercube design is constructed\nby dividing the range of each of the input variables into n equal intervals. Points are\nplaced so that one point will occupy each of the n intervals, for each input variable.\nMaximin Latin Hypercube designs are constructed by generating many Latin Hypercube\ndesigns and selecting the one that has the maximum \u2018minimum distance\u2019 between points.\nThey are approximately orthogonal designs and suffer no projection issues as any lower\ndimensional projection remains a Latin Hypercube.\nI. Vernon, M. Goldstein and R. G. Bower 637\n4.2 The Wave 1 Design\nThe first stage in the collaboration concerned History Matching using a smaller number\nof input variables than were present in the full Galform model, in order to demonstrate\nthe methodology in a simplified version of the problem. As the collaboration progressed\nwe extended our aims to include an analysis of the full model with all 17 input param-\neters. This evolution in priorities has had an impact on the general structure of the\nanalysis, as will be noticeable from the initial design choices described here.\nWhen considering the initial design, expert judgements were used to identify a subset\nof the 17 inputs which would have either significant effects on the bj and K luminosity\nfunction outputs, or be of physical interest to the cosmologists (expert judgements\nin this study were made by Richard Bower). These 6 inputs are shown in the \u2018Initial\nVariables\u2019 column of table 1. When the Galform project began, it was impossible to run\nthe model while varying more than 11 input parameters simultaneously due to technical\nissues with the code. Therefore, we constructed two maximin Latin Hypercube designs:\nthe first over the 6 inputs identified as important, and the second over the 11 inputs\nthought to be less significant. An initial analysis of the first set of runs, suggested\nthat acceptable matches could, most likely, only be found for extremely low values of\nthe 5th input parameter epsilonStar, with the Galform function decreasing rapidly at\nsuch values. This made intuitive sense as the relevant physical process is dependent\nupon the inverse of epsilonStar (see appendix B). We therefore reparameterised this\ninput as epsilonStar\u22121 for all subsequent analysis. Comparison of the variance of the\noutputs in each data set implied that one parameter (alphahot) out of the 11 initially\ndiscarded inputs, had a clearly significant effect on the luminosity functions, and after\ncareful consultation, this input was promoted into the active group. At this point, the\ncosmologists requested that the parameter \u201cyield\u201d also be promoted, as recent physical\nevidence had suggested that the value assigned to this parameter in previous analyses\n(0.02) was too low, and hence the cosmologists were interested in finding acceptable\nmatches with a higher yield value. This meant that for the Wave 1 analysis the inputs\nwere now divided into a group of 8 possibly active and 9 inactive variables respectively,\nas is shown in table 1.\nNext, we constructed two 1000 point Latin Hypercube designs: the first over the\n8 possibly active variables, and the second over the 9 inactive variables. The first of\nthese was used to construct the Wave 1 emulator (see the next section), and the second\nwas required to assess the uncertainty due to the set of 9 inactive parameters (see\nsection 5.1). Due to runs crashing (for computational reasons), only 993 of the first\nbatch of runs were completed, while all 1000 of the second batch finished successfully.\nFor illustration, Figure 2 shows the main effects plots for the bj outputs at luminosity\n17, for the first batch of 993 runs against the 8 possibly active input parameters. Note\nthe clear effect of inputs vhotdisk and alphahot (one of the promoted inputs): these\nalong with epsilonStar, alphareheat and vhotburst were eventually chosen as the active\nvariables for this output (see section 4.3).\nTo perform a History Match for Galform, we do not need to analyse every output of\nthe model. At each stage, we remove parts of the parameter space if the outputs fail to\n638 Galaxy Formation: a Bayesian Uncertainty Analysis\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nlll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nll\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl l l\nl\nl\nl\nl\nl\nll\nl\nl\nll\nl\nll\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nll\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nll\nl\nl l\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl l\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nlll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nll l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\n100 200 300 400 500\n\u2212\n2.\n2\n\u2212\n1.\n8\n\u2212\n1.\n4\nvhotdisk\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nlll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl l\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl l\nl l\nl\nl\nl\nl\nll\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nll\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl l\nl ll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl ll\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl l\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nll\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\n0.2 0.4 0.6 0.8 1.0 1.2\n\u2212\n2.\n2\n\u2212\n1.\n8\n\u2212\n1.\n4\nalpha_reheat\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nll l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl ll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl l l\nl\nl\nl\nl\nll\nl\nl\nl l\nl\nl\nl\nl\nl\nll\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl l\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl l\nl\nl\nl\nll\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nll\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\n0.2 0.4 0.6 0.8 1.0 1.2\n\u2212\n2.\n2\n\u2212\n1.\n8\n\u2212\n1.\n4\nalpha_cool\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nlll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl l\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nll\nl l\nl\nl\nll\nl\nl\nl l\nl\nl\nl\nl\nll\nl\nl\nl\nl l\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nlll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl l\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nll\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl ll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\n100 200 300 400 500\n\u2212\n2.\n2\n\u2212\n1.\n8\n\u2212\n1.\n4\nvhotburst\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nlll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl ll\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nll\nl\nl\nl\nl\nl\nll l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl ll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl ll\nl\nl\nl\nl\nl l\nl\nl\nll\nl\nl\nl\nll\nl l\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl l\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\n0 200 400 600 800\n\u2212\n2.\n2\n\u2212\n1.\n8\n\u2212\n1.\n4\nepsilon_Star\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nll l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl l\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nll\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nll\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nll\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl l\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nlll\nl\nl\nl\nl\nl l\nl\nl\nll\nl\n0.65 0.75 0.85 0.95\n\u2212\n2.\n2\n\u2212\n1.\n8\n\u2212\n1.\n4\nstabledisk\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl l l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl ll\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl l l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl ll\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl l\nl l\nl\nl\nl\nl ll\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl l\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl l\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nll\nl\nll l\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nll\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\n2.0 2.5 3.0 3.5\n\u2212\n2.\n2\n\u2212\n1.\n8\n\u2212\n1.\n4\nalphahot\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl ll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl ll\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nll\nll\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nll\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl ll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl l\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nll\nl\nl\nll\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nll\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\n0.020 0.030 0.040 0.050\n\u2212\n2.\n2\n\u2212\n1.\n8\n\u2212\n1.\n4\nyield\nFigure 2: Main effects plots found by plotting the 993 bj outputs (corresponding to\nluminosity = 17, i.e. the first vertical line in the bj luminosity plot of figure 1) obtained\nfrom the Wave 1 runs, against the 993 values of each of the 8 possibly active inputs.\nNote the clear effect of inputs vhotdisk and alphahot.\nmatch a carefully chosen subset of the observations. At the final stage, we will need to\ncheck that our acceptable matches are also in adequate agreement with those features of\nthe output which haven\u2019t been used to achieve the history match. Therefore, we chose\na subset of 7 of the outputs that are straightforward to emulate at a sufficient accuracy,\nare informative regarding the inputs in that they can be used to discard large regions of\nthe input space, and that captured the main features of the luminosity function. These\nare shown as vertical lines in figure 1 along with the full bj and K luminosity outputs\nfrom the first batch of 993 runs over the 8 active parameters. The specific luminosity\nvalues of each of the 7 outputs are given in the top row of table 2. In later waves more\noutputs were used.\n4.3 The Wave 1 Emulator\nWe now describe the construction of the 7 univariate emulators corresponding to the\n7 luminosity outputs identified in the previous section. As we have many runs, we\nconstruct our emulators using a combination of data analytic techniques, checked against\nphysical intuition, and using the Bayes linear update discussed in section 3.3.\nThe collection of 17 input parameters was split into a group of 8 possibly active\nparameters (xB in table 1) and a group of 9 inactive parameters (xBc). 993 runs for\neach of the first 40 sub-volumes were completed from a Latin Hypercube design over\nthe group xB , and these were used to construct the wave 1 emulators. The quantity\nof interest is the mean output over the first 40 sub-volumes (see section 2.2). Writing\nI. Vernon, M. Goldstein and R. G. Bower 639\nf\n(j)\ni (x) as the ith output from the jth sub-volume, we define:\nfi(x) =\n1\n40\n40\u2211\nj=1\nf\n(j)\ni (x). (17)\nWe emulate fi(x) using only the xB inputs. We add the uncertainty due to sampling\nonly 40 sub-volumes, and the uncertainty due to the remaining 9 parameters xBc in\nsection 5.1. We use the following form for the emulator of each fi(xB) similar to that\nof equation (6),\nfi(xB) =\n\u2211\nj\n\u03b2ij gij(x[Ai]) + ui(x[Ai]) + wi(xB), (18)\nwhere the active variables x[Ai] are a subset of xB , and as in section 3.4, ui and wi have\nzero prior expectation, Cov(ui(x[Ai]), ui(x\n\u2032\n[Ai]\n)) = \u03c32ui exp(\u2212||x[Ai] \u2212 x\u2032[Ai]||2\/\u03b82i ), while\nVar(wi(xB)) = \u03c32wi and Cov(w(xB), w(x\n\u2032\nB)) = 0 for x 6= x\u2032.\nThe selection of the set of active variables x[Ai] for each output i is described in\ndetail in appendix C.1, and the results are shown in table 2. It was found that 5 active\nvariables could explain sufficient amounts of the variance of each fi(x). In appendix C.1\nwe also describe the remaining technical procedures involved in emulator construction:\nchoosing the functions gij , assessing the regression coefficients \u03b2ij and the Gaussian\nprocess parameters \u03c3ui , \u03c3wi and \u03b8i. Table 2 shows the adjusted R\n2 corresponding to\nthe polynomial part of the emulator which gives a good indication of the amount of\nvariance of fi(x) that is explained. Note that at this stage, we only require a relatively\nsimple emulator in order to make an initial reduction of the input space, while leaving\nthe construction of more detailed emulators to subsequent waves of the analysis.\nOutput bj 17 bj 21 bj 22.25 K 21 K 22.25 K 24.75 K 25.75\nvhotdisk x x x x x x x\naReheat x x x x x x x\nalphacool x x x x\nvhotburst x x x x x x x\nepsilonStar x x x\nstabledisk x x x x\nalphahot x x x\nyield\nAdj R2 0.92 0.59 0.70 0.87 0.75 0.72 0.80\nTable 2: Wave 1 Active variables and adjusted R2 for the bj and K luminosity emulators.\nOnce the above emulator covariance specifications have been made, we can use the\n993 wave 1 model runs to update our beliefs, in terms of expectations and variances,\nabout the value of the Galform function fi(x) at a new input point x using the Bayes\nlinear update equations (9), (10), as is described in section 3.4. This gives the adjusted\n640 Galaxy Formation: a Bayesian Uncertainty Analysis\nexpectation and variance EDi [fi(x)] and VarDi [fi(x)] which are used in all subsequent\nimplausibility measures.\nEmulator construction should be performed in conjunction with physical considera-\ntions of the model in question. The emulator should reproduce, to a reasonable degree\nof accuracy, the outputs of the model, and should therefore share the physical features\nof the model. Careful expert assessment regarding the choice of the active variables and\nthe form of the polynomial fit for each output was made to ensure that the emulators\nwere consistent with insight into the physical interpretation of the model. For example,\nthe polynomial for the first bj output has large (negative) contributions from terms in-\nvolving vhotdisk and alphahot including a strong interaction between them. Both these\nparameters are used in the SNe feedback module of the Galform model (see table 1 and\nappendix B), and increasing either will decrease the luminosity function at the faint\nend. They are known to interact in the model, and therefore the form of the terms in\nthe polynomial that they feature in makes physical sense.\n4.4 Emulator Diagnostics\nWhen constructing an emulator, it is essential to perform diagnostics to ascertain\nwhether the emulator is sufficiently accurate for the desired task (Bastos and O\u2019Hagan\n2008). At each wave of the analysis, and for each emulator, we performed several types\nof diagnostic test including: examining the residuals from the polynomial fits; evaluating\n200 diagnostic runs of the model (at each wave) and analysing the emulator\u2019s predictive\ndiagnostics for these runs; and examining the implausibility measure diagnostics (as\nshown in figure 5 and discussed in section 6.1). At each wave the emulators were found\nto be sufficiently accurate to allow substantial reduction of the input space.\n5 Quantification of Uncertainty\nWe now discuss the assessment of all of the remaining uncertainties relevant to linking\nthe Galform Model to the real Universe. These uncertainties can be divided into two\nclasses. The first corresponds to the Model Discrepancy which describes the possible\ndeficiencies of the model, and the second to observational errors.\n5.1 Model Discrepancy\nAs with most complex models of physical systems, modelling assumptions and approx-\nimate solutions to known physical equations imply that Galform\u2019s output will only be\nan approximation to what would occur in the real Universe. Further, Galform does not\nmodel specific galaxies that exist within our Universe: instead it simulates around a\nmillion galaxies from a \u2018possible\u2019 universe that should share statistical properties with\nour own. These statistical properties will also suffer from approximations inherent in\nthe Galform modelling process. The model discrepancy \u000fmd links the system y to the\nmodel output evaluated at the actual system properties f\u2217 = f(x\u2217) via the equation\nI. Vernon, M. Goldstein and R. G. Bower 641\nl\nl l l l l l l l l l l l l l l l l l l l l l l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l l l\n14 16 18 20 22\n\u2212\n6\n\u2212\n5\n\u2212\n4\n\u2212\n3\n\u2212\n2\n\u2212\n1\nbj Luminosity: Inactive Variables\nbj Luminosity\nlo\ng(N\no. \nGa\nlax\nies\n pe\nr u\nnit\n V\nolu\nme\n)\nl\nl l l l l l l l l l l l l l l l l l l l l l l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l l l\n14 16 18 20 22\n\u2212\n6\n\u2212\n5\n\u2212\n4\n\u2212\n3\n\u2212\n2\n\u2212\n1\nbj Luminosity: Dark Matter\nbj Luminosity\nlo\ng(N\no. \nGa\nlax\nies\n pe\nr u\nnit\n V\nolu\nme\n)\nFigure 3: Left panel: the bj luminosity outputs from a sample of 500 runs of the model\nwhere only the 9 inactive parameters have been varied. Green and black lines represent\nthe model output when tdisk is off or on respectively. It can be seen that varying the\ninactive parameters causes a small variance in the model output compared to the 8\nactive parameters (the effects of which are shown in figure 1). Right panel: The bj\nluminosity function output of the first 40 sub-volumes of the Dark Matter simulation,\nfor two (blue and red) Wave 1 runs. This source of uncertainty was treated as a model\ndiscrepancy term, judged to have constant variance across all runs.\ny = f\u2217 + \u000fmd. We decompose \u000fmd into three uncorrelated contributions:\n\u000fmd = \u03a6IA + \u03a6DM + \u03a6E . (19)\nwhere \u03a6IA represents the discrepancy due to the nine inactive parameters, \u03a6DM is the\ndiscrepancy due to the unknown Dark Matter configuration of the real Universe and\n\u03a6E summarises the structural deficiencies of the full Galform model itself. The first\ntwo contributions can be assessed using additional runs of the model, while the third\nrequires expert assessment as we describe in the next three sections.\nUncertainty Due to Inactive Variables: \u03a6IA\nAs we were unable to run the Galform model while varying all 17 inputs simultaneously,\nwe did not model the effect of the remaining 9 inactive variables in detail (a problem that\nwas resolved before Wave 4 occurred). Therefore, we treat the effect of the 9 variables\nas initially contributing an extra term \u03a6IA to the model discrepancy; a term which is\ndropped in the Wave 4 analysis. For the first three waves, we are essentially running\na reduced model (using only 8 inputs), and therefore must use \u03a6IA to account for the\nfact that the Galform model output may not match the observed data due to incorrect\nsettings used for the remaining 9 inputs. For waves 1 to 3 these 9 inputs were set to\ntheir default values in the Galform code: values that were deemed physically reasonable\nby the cosmologists.\nQuantification of \u03a6IA was performed as follows. We judged that there was no overall\na priori bias due to the extra 9 inputs and set E(\u03a6IA) = 0. (These variables have already\n642 Galaxy Formation: a Bayesian Uncertainty Analysis\nbeen screened for main effects, in section 4.2.) As we had 1000 runs across the 9 inactive\nvariables (with the original 8 inputs set at their default values) over the first 40 sub-\nvolumes, we took the mean of the first 40 sub-volumes for each of these runs, and set\nthe Var(\u03a6IA) to be equal to the sample variance of the collection of 1000 means. We are\ntreating as negligible any interactions between the 9 inactive variables and the choice\nof subvolume, and with the 8 original variables. In figure 3 (left panel), we show the\nfirst 500 out of the set of 1000 runs performed across these 9 inputs, with the 8 active\nvariables set at the default value (which corresponds to the cosmologists\u2019 best match: a\nrun which is borderline acceptable according to our matching criteria). Figure 4 (bottom\npanel) compares the standard deviation of all uncertainties discussed in this section, at\nevery point on the bj luminosity function graph given in figure 1, and shows\n\u221a\nVar(\u03a6IA)\nas a light blue line (the K luminosity function has similar uncertainties which we do not\nshow here). The three bj points that were chosen for emulation are given by the black\ndashed lines.\nNote the similarity between the nugget term, wi(xB), in equation (18), and the model\ndiscrepancy term given by \u03a6IA. Both are treated as independent of x, have expectation\nzero and constant variance. This greatly simplifies subsequent calculations and allows\na straightforward reduction of the input space in the first wave. In subsequent waves,\nwe model these effects in more detail.\nDark Matter Uncertainty: \u03a6DM\nWe now assess the uncertainty due to the unknown Dark Matter configuration of the\nreal Universe. The Millennium Simulation provides 512 possible forcing functions, each\nrepresenting a possible configuration of dark matter to be used by the Galform model.\nWe perform runs using only the first 40 sub-volumes out of the 512, to facilitate com-\nparison with previous studies. While using more sub-volumes would be more accurate,\nthe extra run time would allow fewer model evaluations. We have therefore emulated\nthe mean of the function output over these 40 sub-volumes given by fi(x) (equation 17).\nFigure 3 (right panel) shows the luminosity output from the first 40 sub-volumes for\ntwo runs of the model (given by the collection of red and blue lines).\nThe processing of the observational data and associated errors has effectively elevated\nthe data to represent the density of galaxies as measured over a much larger volume of\nthe Universe than is defined by the 512 sub-volumes of the Galform model. We take this\nvolume to be effectively infinite and represent the uncertainty due to analysing the mean\nof only 40 sub-volumes as the model discrepancy term \u03a6DM . We assessed \u03a6DM by first\njudging that there was no overall bias a priori and setting E(\u03a6DM ) = 0. We then used\nthe outputs f (j)i (x) for each of the 40 sub-volumes for the 993 runs performed in Wave\n1 to derive an approximate value for the variance of \u03a6DM as follows. For each of the\n993 runs we calculated the standard error of the mean output over 40 sub-volumes, and\naveraged this over all 993 runs. This was done for each of the 7 outputs. While this is a\nrelatively straightforward assessment, given the important simplifying judgement that\n\u03a6DM is independent of x, it was felt that this captured the main source of uncertainty\nwithout going into detail that would be unwarranted at this stage of the analysis. A\nI. Vernon, M. Goldstein and R. G. Bower 643\nmore careful treatment would model the outputs of the sub-volumes individually, as has\nbeen performed in House et al. (2009), using exchangeable computer model techniques.\nTo check that the first 40 sub-volumes are representative of the full set of 512, we ran\na design of 100 runs at the same x input locations as the first 100 runs of the original\nWave 1 design, but choosing 40 random sub-volumes out of the set of 512 instead of\nthe first 40. We found that the variance across the random 40 sub-volumes was not\nsignificantly different from the original 40 and so did not alter the assessment for the\nVar(\u03a6DM ) described above. The size of \u03a6DM for all bj luminosity outputs is shown\nas the dark blue line in figure 4 (bottom panel). Note that the relative size of \u03a6DM is\nsmall compared to other sources of uncertainty, so that it was considered unnecessary\nto model its effect in more detail at this stage.\nFull Galform Model Discrepancy: \u03a6E\nThe model discrepancy term \u03a6E is a 7 vector, the components of which need to be\nassessed from expert judgements. In the first wave of our analysis we perform only a\nunivariate analysis of each of the 7 outputs, hence we required a univariate assessment\nof each component. In waves 3 and 4, multivariate analyses were performed and hence a\nmore detailed assessment was required. We describe here the full multivariate elicitation.\nAs we are employing a Bayes Linear analysis, we only require specification of expec-\ntations, variances and covariances over all quantities of interest. Subjective assessment\nof E(\u03a6E) and Var(\u03a6E) is still a difficult task. Expert assessment for beliefs regard-\ning deficiencies of the model was that discrepancy judgements were symmetric in that\nE(\u03a6E) = 0. For the multivariate case, assessment of Var(\u03a6E) was required which is\nnow a 7x7 matrix. The structure of this matrix came from Richard\u2019s opinion as to the\ndeficiencies of the model as follows.\nFor Galform, there are two major physical defects that can be identified. The first\nis the possibility that the model has too much (or too little) mass in the simulated\nuniverse, possibly due to incorrect choices for the cosmological parameters used in the\nMillennium simulation (see section 2.2). This would lead to the 7 luminosity outputs all\nbeing too high (or too low), and would lead to positive correlation between all outputs in\nthe Var(\u03a6E) matrix. The second possible defect is that the model incorrectly calculates\nthe colour of the galaxies, due to inaccurate modelling of stellar populations or dust.\nThis would lead to an apparent increase\/decrease in the number of red galaxies and\ndecrease\/increase in the number of blue galaxies. This is represented as contributing a\nsmaller negative correlation between the bj and K luminosity outputs. To respect the\nsymmetries of these possible defects, the multivariate Model Discrepancy was parame-\n644 Galaxy Formation: a Bayesian Uncertainty Analysis\nterised in the following (3+4)x(3+4) block form:\nVar(\u03a6E) = a2\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n1 b b c c c c\nb 1 b c c c c\nb b 1 c c c c\nc c c 1 b b b\nc c c b 1 b b\nc c c b b 1 b\nc c c b b b 1\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n(20)\nwhere now a2 is the univariate variance of the model discrepancy; b is the correlation\nbetween outputs of the same luminosity graph (either bj or K luminosity) and c is the\ncross graph correlation. While Richard was satisfied with the form of the parameterisa-\ntion of Var(\u03a6E) as given by equation (20), he was cautious about specifying values for\na, b and c. He was, however, willing to provide the following ranges:\n3.76\u00d7 10\u22122 < a < 7.52\u00d7 10\u22122, 0.4 < b < 0.8, 0.2 < c < b. (21)\nThis assessment involved examining the difference between Galform and a competing\nmodel of similar complexity, consideration of the above possible physical defects to\nthe model, and from his previous years of experience coding and running such galaxy\nformation models. The maximum value of a = 7.52\u00d7 10\u22122 is shown as the black line in\nfigure 4 (bottom panel), where\n\u221a\nVar(\u000fmd:i) = a, for each i.\nAfter the initial assessment we constructed an elicitation tool in order for Richard\nto confirm that his specification agreed with his intuition regarding the outputs of the\nluminosity function. A picture of this elicitation tool is shown in figure 4 (top panel),\nand it possesses the following features. The top two panels of the tool show the bj and K\nluminosity functions, with observational data points in black, error bars representing all\nuncertainties, dotted lines giving the 11 outputs of interest (additional outputs were used\nin later waves), and constructed (or fictitious) luminosity model output given by the red\nlines. This elicitation tool allows the user to experiment with various possible luminosity\nfunctions and see the corresponding values for the two implausibility functions IM (x)\nand I(x) (see section 3.5 for definitions of these measures). Most importantly, the\nvalues of the multivariate model discrepancy parameters a, b and c can be adjusted.\nThis allowed Richard to experiment with different specifications of a, b and c and to see\nthe response of the implausibility measures. This is useful for the expert to get a feel for\nthe behaviour of a multivariate implausibility measure, understand the ramifications of\nthe structure of Var(\u03a6E) and also to check that intuitively acceptable runs would not\nbe ruled out by the current specification.\nObviously it is possible to build in far more structure into Var(\u03a6E) if required. The\naim here was to account for the main sources of model discrepancy, while maintaining\na relatively simple structure of the Var(\u03a6E), as the more detailed the structure, the\nmore difficult eliciting expert information becomes. As we have ranges for a, b and c,\nwe will incorporate this into our analysis by performing a sensitivity analysis, and rule\nout parts of the input space only if they fail certain implausibility cutoffs for all values\nof a, b and c within the above ranges.\nI. Vernon, M. Goldstein and R. G. Bower 645\nl l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l\n14 16 18 20 22\n0.\n00\n0.\n05\n0.\n10\n0.\n15\n0.\n20\n0.\n25\n0.\n30\nLuminosity\nO\nne\n S\nta\nnd\nar\nd \nDe\nvi\nat\nio\nn\nl\nl\nl l\nl\nl l l l l l l l l l l l l l l l l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l l\nl l l l l l l l l l l l l l l l l l l l l l l l l l l\nl l l\nl l\nl l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l\nl l l l l l l l l l l l l l l l\nl l l l\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l l l l l l l l l l l l l l l l l l\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl l l l l l l l l l\nl\nl\nl l\nl\nl\nl l l l l l l l l l l l l l l l\nl\nl\nl\nl\nl\nl\nl\nl l l l l l l l l l l l l l l l l l l l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\n9 Inactive\n40 Subvolumes\nExpert MD\nLum Zero Point\nk+e\nNormalisation\nProduction\nCombined\nFigure 4: Top panel: the Elicitation Tool used to confirm the multivariate model dis-\ncrepancy assessment represented by equations (20) and (21). It allows the expert to\nconstruct and adjust fictitious luminosity functions, and to explore the response of the\nimplausibility measures to changes in a, b and c (see section 7.3). Bottom panel: the sd\nof each contribution from the various sources of uncertainty for the full range of the bj\nluminosity function (the x-axis is the same as figure 1). The vertical lines represent the\nthree bj outputs chosen for emulation in Wave 1. The green line represents the total\nuncertainty due to all contributions, and it is this value that is used in all bj luminosity\nplots such as figure 1. The K luminosity results are similar.\n5.2 Observational Errors\nThe generation of the observational data shown as the black points in figure 1, is an ex-\ntremely intricate task. It involves data from several sky surveys, which is processed using\n646 Galaxy Formation: a Bayesian Uncertainty Analysis\nboth information from various simulations and additional theoretical and experimental\nknowledge related to the evolution of the Universe. Due to this, the observational errors\n\u000fobs defined in equation (1) are complex. Due to space limitations we only summarise\nthe four contributions to Var(\u000fobs) here; see Cole et al. (2001) for more details.\nThe Luminosity Zero Point Error - this is derived from the difficulty of defining\nthe Luminosity Zero Point: that is the point on the x-axis of the luminosity graph (see\nfigure 1) corresponding to a galaxy of \u2018zero\u2019 brightness. This results in a correlated\nerror on every output point (grey line in figure 4 (bottom panel)).\nThe k+e error - a perfectly correlated error on all output points due to necessary\ncorrections for two effects (i) Galaxies being so far away it takes light billions of years\nto reach us and (ii) Galaxies moving away from us so quickly their light is redshifted\n(purple line in figure 4 (bottom panel)).\nThe Normalisation Error - The data on galaxies comes from measurements made\nin our local vicinity and it is possible that we live in a relatively under\/over populated\npart of the Universe. This error attempts to account for this using theoretical knowledge\nabout variation in mass density in the Universe on large scales (yellow line in figure 4\n(bottom panel)).\nGalaxy Production Error - Bright\/faint galaxies can be measured up to relatively\nlarge\/short distances from our Milky Way. This error represents the uncertainty due to\nthis effect and uses assumptions as to the shape of the mean luminosity function (red\nline in figure 4 (bottom panel)).\nIt is clear that significant contributions to the observational errors come from un-\ncertainties related to the processing of the data (i.e. the k + e, Normalisation and\nProduction Errors). These are distinct from measurement errors and are derived from\ncomplex theoretical and modelling uncertainties, and hence could be referred to as\nmodel discrepancy terms as opposed to observational errors. However, the calculations\ninvolved in determining these errors are intricate and rely upon specialist knowledge of\nAstronomy. Although it would be desirable to disentangle some of these errors, due to\ntime constraints it was felt that this was impractical at the current stage.\n6 First Wave History Match\n6.1 History Matching via Implausibility\nHistory Matching is the process of identifying X \u2217 by iteratively discarding values of x,\nby the application of cutoffs to the Implausibility Measures. In Wave 1, we used the\nmeasures I2M (x) and I3M (x) to discard values of x that do not satisfy both:\nI2M (x) < Icut2 and I3M (x) < Icut3, (22)\nwhere Icut2 and Icut3 are the corresponding implausibility cutoffs. The choices made\nfor the individual cutoffs come from a combination of examination of diagnostics (such\nas shown in figure 5), consideration of the amount of space cut out, and unimodality\nI. Vernon, M. Goldstein and R. G. Bower 647\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl l\nl\nl\nl\nl\nl\nl\nll\nll\nl\nl\nl\nl\nl\nl\nll\nl ll\nl\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nll l\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nll\nl\nl\nl\nl\nl\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\n0 1 2 3 4 5 6\n0\n2\n4\n6\n8\n10\n12\nSecond Max Imp\nI_2M(x)\nD\nat\na \nI_\nM\n(x)\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nll\nl l\nl\nl\nl\nl\nl\nll\nl l\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl l\nl\nl\nl\nl\nl\nll\nl l\nll\nll l\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl l\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl l\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\n0 1 2 3 4 5 6\n0\n2\n4\n6\n8\n10\n12\nThird Max Imp\nI_3M(x)\nD\nat\na \nI_\nM\n(x)\nFigure 5: Implausibility diagnostics for the Wave 1 univariate emulators. Plots show\nthe maximum data implausibility IdataM (x) calculated using actual runs, against the im-\nplausibility measures I2M (x) (left panel) and I3M (x) (right panel) which are calculated\nusing the emulator. The vertical lines show the cutoffs imposed at this Wave, with the\nred points belonging to parts of the input space deemed implausible.\narguments based on Pukelsheim\u2019s 3-sigma rule as discussed in section 3.5. While the\nunimodal argument suggests using cutoffs of 3 or higher (depending on the correlation\nbetween outputs), consideration of figure 5 shows that this might be unnecessarily con-\nservative. In response to this we choose cutoffs of Icut2 = 2.7 and Icut3 = 2.3 (shown as\nvertical lines in figure 5), recognising the fact that we want to balance a conservative\ncutoff with the amount of space that can be removed at Wave 1. These cutoffs resulted\nin approximately 85.1 percent of the input space being ruled out due to the Wave 1\nanalysis. Note that, as discussed at the end of section 5.1 and also in section 7.3, we\neffectively perform a sensitivity analysis on \u03a6E by only ruling out inputs that do not\nsatisfy the cutoffs for all values of a, b and c within the ranges given in equation (21).\nFor the univariate cases discussed here, this is equivalent to setting a to its maximum\nvalue as I(i)(x) is monotonically decreasing with increasing a.\nFigure 5 shows diagnostic plots regarding the choice of cutoffs Icut2 and Icut3. It\nshows the maximum data implausibility IdataM (x) (that is the implausibility evaluated\nat a known run, given by equation (11)) across the 7 outputs for a latin hypercube of\n200 diagnostic runs (y-axis), against I2M (x) (left panel) and I3M (x) (right panel). The\nvertical lines are the cutoffs that will be imposed, implying that the red points would\nbe discarded. Note that most points are some distance above the diagonal y = x line,\nsuggesting that IdataM (x) will generally be higher than I2M (x) and I3M (x) as expected.\nAlso note that the discarded points do indeed have high IdataM (x) (significantly higher\nthan the 2.7 cutoff shown as a horizontal line), and hence suggest the space cutout\nin Wave 1 does not contain any inputs of interest. We test the sensitivity of such\ndiagnostics and of the fraction of space cut out, to different values of cutoffs, before\ndefinite choices are made.\nIn figure 6 we show various 2-dimensional projections (top 3 panels) of values of the\nImplausibility Measures, with red areas representing high implausibility and green areas\nlow, which were constructed as follows. For each plot we evaluated the emulator at a set\n648 Galaxy Formation: a Bayesian Uncertainty Analysis\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n100 200 300 400 500\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nWave 1 Implausibility\nvhotdisk\na\nlp\nha\n_r\neh\nea\nt\n0\n1\n2\n3\n4\n0.2 0.4 0.6 0.8 1.0 1.2\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nWave 1 Implausibility\nalpha_reheat\na\nlp\nha\n_c\noo\nl\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n100 200 300 400 500\n2.0\n2.5\n3.0\n3.5\nWave 1 Implausibility\nvhotdisk\na\nlp\nha\nho\nt\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n100 200 300 400 500\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nWave 1 Depth Plot\nvhotdisk\na\nlp\nha\n_r\neh\nea\nt\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.2 0.4 0.6 0.8 1.0 1.2\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nWave 1 Depth Plot\nalpha_reheat\na\nlp\nha\n_c\noo\nl\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n100 200 300 400 500\n2.0\n2.5\n3.0\n3.5\nWave 1 Depth Plot\nvhotdisk\na\nlp\nha\nho\nt\nFigure 6: The top three panels give Wave 1 minimised implausibility projection plots:\nthe red region indicates high implausibility for all values of the remaining inputs: here\ninput points will be discarded. The bottom three panels give the \u2018optical depth\u2019 plots:\nthese show the fraction of the hidden 5 dimensional volume (spanned by the remaining\nactive variables) that satisfies the implausibility cutoff, at that grid-point.\nof inputs specifically designed to produce a 2-dimensional projection in the appropriate\ninput plane. For example, in the top left panel the projection is in the vhotdisk -\nalphareheat plane, and the emulator was evaluated on a (2d grid)x(5d latin hypercube)\ndesign, where the 2d grid was over the vhotdisk - alphareheat plane (and of size 152)\nwhile the latin hypercube was defined over the remaining 5 active inputs at Wave 1\n(and was of size 1500). For each point on the grid, we then minimised the implausibility\nover the corresponding 1500 points at that grid location, the results of which provide\nthe plots shown. This allows the following interpretation: a red area in one of these\nimplausibility projection plots implies that even given all relevant uncertainties, and all\npossible choices for the other input parameters, it is highly unlikely that an acceptable\nmatch will be found at this point in the vhotdisk - alphareheat plane (for example).\nSuch plots present serious computational complications as a large number of emulator\nevaluations are required for each projection. To generate these plots we have used novel\nBayes Linear calculations that exploit both the cross-product symmetry of the emulator\ndesign, and similar symmetries that occur in the emulator update equations (9) and (10)\ndue to the covariance structure of equation (24). These calculations greatly improve\nefficiency, and we will report on these techniques in detail elsewhere.\nThe bottom 3 panels of figure 6 show depth projection plots: these are constructed by\ncalculating at each grid point, the fraction of the corresponding 1500 points of the latin\nhypercube that survive the implausibility cutoffs, given by equation (22). This gives\nI. Vernon, M. Goldstein and R. G. Bower 649\ninformation as to the \u2018optical depth\u2019 of the 7 dimensional non-implausible volume when\nobserved in a direction perpendicular to the vhotdisk - alphareheat plane (for example).\nThey provide complimentary information to the implausibility projections. Consider\nthe middle top and bottom panels of figure 6, where the implausibility projection (top\npanel) shows that non-implausible choices of alphareheat and alphacool exist over much\nof the alphareheat-alphacool plane. The depth plot demonstrates that the majority of\nthe non-implausible volume is found at low values of alphareheat. These images give\nphysical insights into the nature of the Galform model: in the top right panel of figure 6\nwe see that simultaneously low values of both vhotdisk and alphahot are ruled out, and\nthat high values of both these parameters are possibly preferred. These parameters\nare involved in the same Galform module: that of Feedback from Supernovae (see\nequation (23) and appendix B), and increasing their size should increase the amount of\nmaterial expelled from certain galaxies as opposed to being used to form stars. This\nwill reduce the luminosity function at the faint end, and, as most of the Wave 1 runs\nare higher than the observed data, it makes physical sense that parameter choices that\nlower the luminosity function will be preferred. These physical features are also seen in\nthe polynomial terms for the outputs bj 17 and K 21 (which are at the faint end of the\nluminosity function), specifically we find large and negative coefficients for the vhotdisk,\nalphahot and their interaction terms. The Wave 1 emulators are quite approximate, so\nthere is a limit as to the physical insight they, and the corresponding implausibility\nmeasures, can provide.\nEquation (22) defines a volume of input space X1 that we refer to as non-implausible\nafter Wave 1, projections of which are shown in figure 5. We now refocus by running\nthe Galform model within this volume, and repeat the above process of emulation,\nconstructing implausibility measures and imposing cutoffs. We have gone through four\niterations or waves, as described below.\n7 Analysis of Waves 2 - 4\n7.1 Wave 2 to 4: Design, New Outputs and Emulation\nWe apply the refocussing technique iteratively, and here we describe the designs and\nemulators used in waves 2 to 4. The design for the set of Wave 2 model evaluations\nwas derived as follows. We first constructed a large maximin Latin Hypercube design\ncontaining 9500 points defined over the 8 dimensional input space corresponding to the\n8 input variables explored in Wave 1. We then used the Wave 1 emulator and Implau-\nsibility measures to evaluate the implausibility of each proposed point in the design.\nAny points that did not satisfy the implausibility cutoffs, as given by equation (22),\nwere discarded from further analysis. This left a design of 1414 points which were then\nevaluated using the Galform model, the results of which were used to construct the\nWave 2 emulator. The Wave 3 design of 1620 points was constructed in a similar man-\nner. Between Waves 3 and 4, the problems preventing simultaneous varying of all 17\nparameters in the Galform model were resolved. Hence, the Wave 4 design came from\na large latin hypercube defined over the full 17 dimensional input space. Again, only\n650 Galaxy Formation: a Bayesian Uncertainty Analysis\nWave Runs Act IM I2M I3M IMV % Space\n1 993 5 - 2.7 2.3 - 14.9 %\n2 1414 8 - 2.7 2.3 - 5.9 %\n3 1620 8 - 2.7 2.3 26.75 1.6 %\n4 2011 10 3.2 2.7 2.3 26.75 0.26 %\nTable 3: The fraction of parameter space deemed non-implausible after each wave of\nemulation. Column 1: the wave; Column 2: the number of model runs used to construct\nthe emulator; Column 3: the number of Active Variables; Column 4-7: the implausibility\nthresholds; Column 8: the fraction of the parameter space deemed non-implausible.\npoints that satisfied all of the previous 3 wave\u2019s implausibility cutoffs remained in the\ndesign, leaving a total of 2011 points. The number of design points was deliberately\nincreased at each wave in anticipation of fitting more complex polynomials.\nAs the input space has been reduced after the Wave 1 analysis, it became easier\nto emulate model outputs. Therefore more outputs become informative regarding the\ninput space, and warrant inclusion in the analysis. Consideration of the 1414 Wave 2\nruns led to 4 additional outputs being included, the bj outputs with luminosity 18.75, 20\nand 21.75, and the K output with luminosity 23.5. These are shown in figures 12 and 13\nalong with the original 7 outputs, as the dotted vertical lines. For each wave, emulation\nproceeded in a similar manner to Wave 1, with univariate emulators being used in all\nwaves, and multivariate emulators used in waves 3 and 4. The details of the construction\nof these emulators are given in appendices C.2 and C.3. Table 3 summarises the number\nof runs used at each wave, number of active variables required and space remaining. At\neach wave, cluster analysis was performed to check that the non-implausible volume was\nsimply connected (which was found to be the case), as separate emulators would have\nbeen required for unconnected volumes.\n7.2 Comparing Emulators\nAt each wave, emulator accuracy increases. It is instructive to compare the emulators,\nto understand which features lead to this improvement. As the Wave 4 emulator involves\nall 17 input parameters, we leave discussion of it until section 8.1.\nFigure 7 (left panel) shows the estimated value of the residual standard deviation\n\u03c3ui for each of the first three waves, for all 11 emulated outputs (for completeness we\nshow all 11 outputs for Wave 1 even though 4 of these were not considered at that\nstage). There are significant drops in \u03c3ui from Wave 1 to 2 across all outputs, with even\nmore substantial drops from Wave 2 to Wave 3, especially for the K luminosity outputs\n(outputs 7 to 11). The right panel of figure 7 shows the adjusted R2 for each of the 11\nemulators, for each of the 3 waves. It shows the improvement in percentage of output\nvariance explained in Waves 2 and 3 compared to that of Wave 1. Note that although\nthe Wave 3 adjusted R2 is sometimes below that of Wave 2, this is to be expected:\nI. Vernon, M. Goldstein and R. G. Bower 651\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\n2 4 6 8 10\n0.\n0\n0.\n1\n0.\n2\n0.\n3\n0.\n4\nResidual Sigma: Wave 1:3\nOutput\nSi\ngm\na\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nWave 1 Sigma\nWave 2 Sigma\nWave 3 Sigma\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\n2 4 6 8 10\n0.\n6\n0.\n7\n0.\n8\n0.\n9\n1.\n0\nAdjusted R^2: Waves 1:3\nOutput\nAd\njus\nted\n R\n^2\nl\nl l\nl\nl l l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nWave 1 AdjR\nWave 2 AdjR\nWave 3 AdjR\nFigure 7: Plots showing the residual standard deviation \u03c3 for waves 1 to 3 (left panel)\nand the Adjusted R2 for wave 1 to 3 (right panel). In each panel, the first 6 connected\npoints correspond to the bj outputs chosen for emulation, the later 5 connected points\nare the K outputs (shown as vertical lines in figures 12 and 13 respectively).\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\n2 4 6 8 10\n0.\n6\n0.\n7\n0.\n8\n0.\n9\n1.\n0\nR^2 and Adjusted R^2: Waves 2 and 3\nOutput\nAd\njus\nted\n R\n^2\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nl\nl\nl\nWave 2 R^2\nWave 2 Adjusted R^2\nWave 3 R^2\nWave 3 Adjusted R^2\nl\nl\nl l\nl\nl\nl\nl\nl\nl\nl\n2 4 6 8 10\n0.\n4\n0.\n5\n0.\n6\n0.\n7\n0.\n8\n0.\n9\n1.\n0\nW2 and W3 R^2 for Diagnostic Runs\nOutput\nAd\njus\nted\n R\n^2\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l\nl\nl\nWave 2 R^2\nWave 3 R^2\nFigure 8: Left panel gives a plot showing the R2 (open points) and adjusted R2 (solid\npoints) of the Wave 2 polynomial when used to predict the outputs of the Wave 3\nruns (in red). Also shown are the corresponding Wave 3 polynomial R2 (open points)\nand adjusted R2 (solid points) in green. Note the large difference between red and\ngreen points. Right panel: shows the fairer comparison of the R2 of the Wave 2 and 3\npolynomials when used to predict 204 Wave 3 diagnostic runs. First 6 connected points:\nthe bj outputs, last five: the K outputs, as in figure 7.\nas the variance of the Wave 3 run outputs is less than that of the Wave 2 runs, the\nWave 3 emulators may not be able to explain more of this variance than their Wave 2\ncounterparts, even though they are more accurate.\nFurther confirmation of the difference between the Wave 2 and 3 polynomials is\ngiven by figure 8. As the Wave 2 and 3 polynomials have been fitted using highly\nnon-orthogonal designs of input points, it is not trivial to compare their polynomial\ncoefficients directly, in order to determine any differences between them. In figure 8\n(left panel) we show the R2 and adjusted R2 of the Wave 2 polynomial calculated\nusing the Wave 3 runs (in red). Also shown are the R2 and adjusted R2 of the Wave\n3 polynomial calculated with the same Wave 3 runs (in green). Note the dramatic\n652 Galaxy Formation: a Bayesian Uncertainty Analysis\ndifference in variance explained between the red and green points. This demonstrates\nthat the two sets of polynomials are substantially different. While this comparison is not\nstrictly fair (as the Wave 3 points were used to fit the Wave 3 polynomial), equivalent\npolynomials would be expected to have much smaller differences in their R2 values.\nTo highlight this point, figure 8 (right panel) shows the R2 of the Wave 2 and Wave\n3 polynomials calculated using a set of 204 Wave 3 diagnostic runs. Again a clear\ndifference between the explanatory power of the two polynomials can be seen. This\nsuggests that the emulators are picking up new features of the model at each wave\nthrough improved polynomial fits: a natural feature as we build more structure into the\nmean function, as opposed to the Gaussian process residual.\n7.3 Implausibility Measures and Space Reduction\nTable 3 summarises which of the four implausibility measures IM (x), I2M (x), I3M (x)\nand I(x) were used in each of the four Waves, along with the implausibility cutoffs that\nwere imposed. Note that the multivariate cutoff IMV , employed at Wave 3, was chosen\nto be equal to 26.75, the critical value of 0.995 from a chi squared distribution with 11\ndegrees of freedom. This cutoff was employed in a conservative manner as follows. The\nexpert asserted ranges on a, b and c which parameterise Var(\u03a6E) ((20),(21)). Therefore,\ninputs x were only discarded due to the multivariate measure I(x) if I(x) > IMV for all\nvalues of a, b and c within their specified ranges (see Vernon and Goldstein (2009)).\nFigure 9 shows the progression of implausibility and optical depth plots, in the\nvhotdisk and alphacool plane, for Waves 1 to 3. Note that the size of the non-implausible\nregion decreases with each wave as expected, occupying a volume of 14.9%, 5.9% and\n1.6% respectively. Even though the non-implausible volume occupies a small part of\nthe input space, it still covers a large part of the two dimensional projection.\n8 Results of Wave 4 and 5\n8.1 Wave 4\nThe Wave 4 emulator gives an accurate description of the non-implausible region of\ninput parameter space X \u2217. Visualising this region is a difficult task, as it is a com-\nplicated object in a ten-dimensional space. We here confine our analysis to useful two\ndimensional projections of the space. Figure 10 shows the minimised Implausibility pro-\njections (below the diagonal) and optical depth plots (above the diagonal) corresponding\nto all possible pairs of active variables. The plots above the diagonal have been trans-\nposed to have the same orientation as those below the diagonal for ease of comparison.\nFigure 10 highlights many features of the Galform model, which are of great interest to\nthe cosmologists. It suggests that acceptable fits can be found over large ranges of the\ninput parameters. It also demonstrates clear relationships between certain parameters,\nfor example, the positive correlation between vhotdisk and alphareheat: if one input\nis increased, then the second should be increased to compensate. This make physical\nI. Vernon, M. Goldstein and R. G. Bower 653\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n100 200 300 400 500\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nWave 1 Implausibility\nvhotdisk\na\nlp\nha\n_c\noo\nl\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n100 200 300 400 500\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nWave 2 Implausibility\nvhotdisk\na\nlp\nha\n_c\noo\nl\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n100 200 300 400 500\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nWave 3 Implausibility\nvhotdisk\na\nlp\nha\n_c\noo\nl\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n100 200 300 400 500\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nWave 1 Depth Plot\nvhotdisk\na\nlp\nha\n_c\noo\nl\n0.00\n0.05\n0.10\n0.15\n0.20\n100 200 300 400 500\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nWave 2 Depth Plot\nvhotdisk\na\nlp\nha\n_c\noo\nl\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n100 200 300 400 500\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nWave 3 Depth Plot\nvhotdisk\na\nlp\nha\n_c\noo\nl\nFigure 9: The top three panels give Wave 1, 2 and 3 implausibility projection plots:\nthe red region indicates high implausibility where input points will be discarded. Note\nthat the yellow and green regions occupy only 15%, 5.9% and 1.6% of the input space\nrespectively (the non-implausible region), even though they take up much larger areas\nof the 2-dimensional projection. The bottom three panels give the depth plots, showing\nthe fraction of the hidden 6-dimensional volume that satisfies the implausibility cutoff,\nat that grid-point.\nsense as both these parameters are involved with feedback from supernovae: vhotdisk\nis related to the gas blown out of a galaxy due to supernovae while alphareheat reg-\nulates the time taken for this gas to return. Similarly, there exist a strong negative\ncorrelation between vhotdisk and alphahot: another input related to supernovae feed-\nback. Figure 10 also shows which parameters influence the luminosity functions, and are\ntherefore constrained, and which parameters do not. Inputs related to the Reionisation\nand Galaxy Mergers modules of the Galform function (see table 1 and appendix B) are\nall inactive save tau0mrg (fdf), which only has a subtle impact. Therefore the physical\nprocesses represented by these modules can be concluded to have little impact on the\nluminosity function. There are many more physical interpretations that can be obtained\nfrom this analysis. For example, by applying principal component analysis to a set of\npoints belonging to the non-implausible region, several approximate linear relationships\nbetween groups of variables can be obtained (see Bower et al. (2010)).\n8.2 Wave 5\nAfter the Wave 4 analysis, we ran a final batch of 2000 model evaluations within the\nnon-implausible region defined by the Wave 4 emulator. We refer to these as Wave 5\n654 Galaxy Formation: a Bayesian Uncertainty Analysis\n0.00\n0.01\n0.02\n0.03\n0.04\n1.5\n2.0\n2.5\n3.0\n3.5\nFigure 10: All Wave 4 Implausibility (below diagonal) and Optical Depth (above diag-\nonal) projections. Compare the Implausibility plots with the Wave 5 runs of figure 11.\nruns. These runs were evaluated to see if we could determine whether the set X \u2217 was\nnon-empty. and if so to check that a significant volume of the non-implausible region\ndid indeed correspond to acceptable runs (and therefore that another wave of analysis is\nnot required), and to generate a large set of realised acceptable runs for the cosmologists\nto use to perform provisional explorations of other output data sets.\nFigure 11 shows the two-dimensional projections of these Wave 5 runs, coloured\nusing the data implausibility (that is the implausibility without any emulator variance).\nThe colour scale is the same as that of figure 10 to allow direct comparison. It can be\nseen that we do indeed find a large number of acceptable runs: 306 of the 2000 Wave\n5 runs satisfied the implausibility cutoffs, with approximately 800 more runs within\n10 percent of the cutoff boundary. This is expected as the surface area of a complex\n10-dimensional object can be large compared to its volume. The acceptable runs do\nspan a large range in several of the inputs, as was suggested by the Wave 4 analysis:\na fact that was a surprise to the cosmologists. In general the Wave 5 runs are in good\nagreement with the Wave 4 analysis, suggesting that the Wave 4 emulator is of sufficient\nI. Vernon, M. Goldstein and R. G. Bower 655\nFigure 11: The Wave 5 runs coloured by the data implausibility, consistent with fig 10.\naccuracy. For this reason, and due to the large number of acceptable runs obtained,\nwe concluded that another wave of analysis was unnecessary. The acceptable runs were\nused to perform provisional explorations of additional outputs of the Galform model, as\ndescribed in Bower et al. (2010).\nTo illustrate the improvement in the model runs from Wave 1 to Wave 5, figures 12\nand 13 show the first 500 model runs bj and K outputs from Waves 1, 2, 3 and the\n\u2018good\u2019 runs from Wave 5, defined as those that satisfy IM (x) < 2.5. It can be seen\nthat a large number of acceptable runs have been found, which are acceptable across\nall outputs of interest, not just the 11 used for the emulation process.\n9 Conclusion\nIn this Case Study we have presented the results of an uncertainty analysis of the galaxy\nformation model known as Galform. The main aim was to identify the set of inputs\nthat would give rise to an acceptable match between model output and observed data,\n656 Galaxy Formation: a Bayesian Uncertainty Analysis\nl\nl l l l l l l l l l l l l l l l l l l l l l l l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l l l\n14 16 18 20 22\n\u2212\n6\n\u2212\n5\n\u2212\n4\n\u2212\n3\n\u2212\n2\n\u2212\n1\nbj Luminosity Function Wave 1\nbj Luminosity\nlo\ng(N\no. \nGa\nlax\nies\n pe\nr u\nnit\n V\nolu\nme\n)\nl\nl l l l l l l l l l l l l l l l l l l l l l l l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l l l\n14 16 18 20 22\n\u2212\n6\n\u2212\n5\n\u2212\n4\n\u2212\n3\n\u2212\n2\n\u2212\n1\nbj Luminosity Function Wave 2\nbj Luminosity\nlo\ng(N\no. \nGa\nlax\nies\n pe\nr u\nnit\n V\nolu\nme\n)\nl\nl l l l l l l l l l l l l l l l l l l l l l l l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l l l\n14 16 18 20 22\n\u2212\n6\n\u2212\n5\n\u2212\n4\n\u2212\n3\n\u2212\n2\n\u2212\n1\nbj Luminosity Function Wave 3\nbj Luminosity\nlo\ng(N\no. \nGa\nlax\nies\n pe\nr u\nnit\n V\nolu\nme\n)\nl\nl l l l l l l l l l l l l l l l l l l l l l l l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl l l l\n14 16 18 20 22\n\u2212\n6\n\u2212\n5\n\u2212\n4\n\u2212\n3\n\u2212\n2\n\u2212\n1\nbj Luminosity Function Wave 5\nbj Luminosity\nlo\ng(N\no. \nG\nal\nax\nie\ns \npe\nr u\nni\nt V\no\nlu\nm\ne)\nFigure 12: The bj Luminosity function output for the first 500 runs of Waves 1, 2 and\n3 (top left, top right and bottom left panels respectively). The colours represent the\nmaximum implausibility IM (x) and are consistent with the colour scale of figures 10 and\n11. Bottom right panel: the Wave 5 runs that satisfy IM (x) < 2.5. (Note the tighter\nerror bars compared to previous waves as \u03a6IA has been dropped).\ntaking into account all of the major uncertainties present in such a situation.\nThis analysis can be seen as a demonstration of the power of the iterative refocussing\ntechnique in addressing a difficult and important problem: difficult in the sense that\nGalform is a complex model with a significant run time, and with a large number of\nactive parameters many of which exhibit intricate interactions; important in that Gal-\nform is a state-of-the-art model, and that the results we present provide insight into the\nphysics of galaxy formation for the cosmology community. At each iteration, improved\nfits for the emulators are obtained, and new features of the model are seen (section 7.2).\nThis iterative strategy leads to a collection of emulators that are increasingly accurate\nover regions of the input space of increasing interest. It is hard to see how such an\naccurate description of the non-implausible region of input space could be obtained in\none step, without requiring an infeasibly large number of model evaluations. As the\nnon-implausible region is so small (less that 0.26% of the initial space), it is clearly\nbeneficial to perform a History Match before attempting any form of fully Bayesian\ncalibration.\nWhat improvements could have been made to this project? We have had the benefit\nof substantial computational resources, courtesy of the Galform group. This has allowed\nrelatively large numbers of runs to be performed at each wave of the analysis, when it\nmay have been possible to obtain broadly similar results using fewer evaluations. Also,\nI. Vernon, M. Goldstein and R. G. Bower 657\nl\nl\nl l\nl l l l l l l l l l l l l l l l l l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\n18 20 22 24 26\n\u2212\n6\n\u2212\n5\n\u2212\n4\n\u2212\n3\n\u2212\n2\n\u2212\n1\nK Luminosity Function Wave 1\nK Luminosity\nlo\ng(N\no. \nGa\nlax\nies\n pe\nr u\nnit\n V\nolu\nme\n)\nl\nl\nl l\nl l l l l l l l l l l l l l l l l l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\n18 20 22 24 26\n\u2212\n6\n\u2212\n5\n\u2212\n4\n\u2212\n3\n\u2212\n2\n\u2212\n1\nK Luminosity Function Wave 2\nK Luminosity\nlo\ng(N\no. \nGa\nlax\nies\n pe\nr u\nnit\n V\nolu\nme\n)\nl\nl\nl l\nl l l l l l l l l l l l l l l l l l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\n18 20 22 24 26\n\u2212\n6\n\u2212\n5\n\u2212\n4\n\u2212\n3\n\u2212\n2\n\u2212\n1\nK Luminosity Function Wave 3\nK Luminosity\nlo\ng(N\no. \nGa\nlax\nies\n pe\nr u\nnit\n V\nolu\nme\n)\nl\nl\nl l\nl l l l l l l l l l l l l l l l l l\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\n18 20 22 24 26\n\u2212\n6\n\u2212\n5\n\u2212\n4\n\u2212\n3\n\u2212\n2\n\u2212\n1\nK Luminosity Function Wave 5\nK Luminosity\nlo\ng(N\no. \nG\nal\nax\nie\ns \npe\nr u\nni\nt V\no\nlu\nm\ne)\nFigure 13: The K luminosity version of figure 12. Note the disparity at luminosity \u2264\n19 for the Wave 5 runs (bottom right panel) is due to the limited resolution of the Dark\nMatter simulation (see Bower et al. (2006)) and so is not considered to be of interest.\ncertain simplifying assumptions used when assessing the Model Discrepancy could have\nbeen dropped. For example, the assumption that the effect of the Dark Matter forcing\nfunction \u03a6DM was independent of x, has been addressed in House et al. (2009), where\nGalform models with different Dark Matter configurations are treated as exchangeable\ncomputer models. This is a particular aspect of a more general treatment of model\ndiscrepancy known as Reification (Goldstein and Rougier (2009)).\nThe identification of the non-implausible region shown in figure 10 provides several\nimmediate physical insights into the Galform model, e.g. the relations between certain\ninputs, the ranges of feasible values for the inputs, as well as identifying which inputs are\nnot restricted by the luminosity function, all of which are of significant scientific interest.\nHowever, there may be several physical features that are hard to obtain from simple 2- or\neven 3-dimensional projections, or from linear analyses such as PCA (Bower et al. 2010).\nVisualising the complexities of the full 10-dimensional volume efficiently is a difficult\ntask (even using packages such as Ggobi (www.ggobi.org)), but must be addressed in\norder to extract the full information provided by the emulators. This is made even more\ndifficult by the fact that although the emulators are very fast to evaluate, they are still\nnot fast enough to completely cover a (possibly complex) 10-dimensional object. We\nhave developed efficient emulator designs and calculation routines for high-dimensional\nvisualisation purposes and will report on these elsewhere.\nThe set of Wave 5 evaluations provided a large number of realised acceptable runs\nfor use by the cosmologists in provisionally exploring further Galform outputs. Several\n658 Galaxy Formation: a Bayesian Uncertainty Analysis\nFigure 14: 5 new outputs of the Galform model describing galaxy disk sizes, TF relation,\ngas metallicity, gas mass to LB and BH mass. The cosmologists best fit is in red, with\na group of the best Wave 5 runs in green. Already we have found better simultaneous\nfits to these additional data sets.\nexamples of such output datasets describing various galaxy properties (disk sizes, TF\nrelation, gas metallicity, gas mass to LB and BH mass), along with corresponding ob-\nserved data (the black points) are shown in figure 14 (see Bower et al. (2010)). The\nsingle red line represents the cosmologists\u2019 single best run prior to this analysis, and the\ngreen lines are ten of the best Wave 5 runs. We found many runs that were substantially\nbetter fits to the luminosity functions than had ever been seen previously by the cos-\nmologists, and as figure 14 shows, have already found several runs that are an improved\nmatch to these other output data sets. The next step in this ongoing collaboration is to\napply the emulation and History Matching procedures outlined in this report to these\nnew output data sets, in order to understand their impact on the input space, and to\ndetermine which regions of input space will provide acceptable matches to all possible\noutputs.\nAcknowledgments\nIRV and MG acknowledge the support of the Basic Technology initiative as part of the Man-\naging Uncertainty for Complex Models project. IRV acknowledges the support of an EPSRC\nmobility fellowship. RGB acknowledges the support of a Durham-University Christopherson-\nKnott Fellowship. We would like to thank the Durham Semi-analytical group based at the\nInstitute for Computational Cosmology, Physics Department, Durham University for access to\nthe Galform model and to their computational resources.\nI. Vernon, M. Goldstein and R. G. Bower 659\nReferences\nBastos, T. S. and O\u2019Hagan, A. (2008). \u201cDiagnostics for Gaussian process emulators.\u201d Techno-\nmetrics, 51: 425\u2013438. 640\nBaugh, C. M. (2006). \u201cA primer on hierarchical galaxy formation: the semi- analytical ap-\nproach.\u201d Rept. Prog. Phys., 69: 3101\u20133156. 622, 663, 664\nBower, R. G., Benson, A. J., et al. (2006). \u201cThe broken hierarchy of galaxy formation.\u201d\nMon.Not.Roy.Astron.Soc., 370: 645\u2013655. 619, 622, 657, 664\nBower, R. G., Vernon, I., Goldstein, M., et al. (2010). \u201cThe Parameter Space of Galaxy\nFormation.\u201d Mon.Not.Roy.Astron.Soc., 407: 2017\u20132045. 653, 655, 657, 658\nCole, S. et al. (2001). \u201cThe 2dF Galaxy Redshift Survey: Near Infrared Galaxy Luminosity\nFunctions.\u201d Mon. Not. Roy. Astron. Soc., 326: 255\u2013273. 625, 646, 664, 665\nColless, M. et al. (2001). \u201cThe 2dF Galaxy Redshift Survey: Spectra and redshifts.\u201d\nMon.Not.Roy.Astron.Soc., 328: 1039\u20131066. 624\nConti, S., Gosling, J. P., Oakley, J. E., and O\u2019Hagan, A. (2009). \u201cGaussian process emulation\nof dynamic computer codes.\u201d Biometrika, 96: 663\u2013676. 630\nCraig, P. S., Goldstein, M., Seheult, A. H., and Smith, J. A. (1996). \u201cBayes linear strategies\nfor history matching of hydrocarbon reservoirs.\u201d In Bernardo, J. M., Berger, J. O., Dawid,\nA. P., and Smith, A. F. M. (eds.), Bayesian Statistics 5, 69\u201395. Oxford, UK: Clarendon\nPress. 630, 633, 634, 667\n\u2014 (1997). \u201cPressure matching for hydrocarbon reservoirs: a case study in the use of Bayes\nlinear strategies for large computer experiments.\u201d In Gatsonis, C., Hodges, J. S., Kass,\nR. E., McCulloch, R., Rossi, P., and Singpurwalla, N. D. (eds.), Case Studies in Bayesian\nStatistics, volume 3, 36\u201393. New York: Springer-Verlag. 630, 633, 634\nCressie, N. (1991). Statistics for Spatial Data. New York: Wiley. 667\nCumming, J. A. and Goldstein, M. (2009). \u201cBayes linear uncertainty analysis for oil reservoirs\nbased on multiscale computer experiments.\u201d In O\u2019Hagan, A. and West, M. (eds.), Handbook\nof Bayesian Analysis. Oxford, UK: Oxford University Press. 633\nCurrin, C., Mitchell, T., Morris, M., and Ylvisaker, D. (1991). \u201cBayesian prediction of deter-\nministic functions with applications to the design and analysis of computer experiments.\u201d\nJournal of the American Statistical Association, 86(416): 953\u2013963. 626, 636\nDe Finetti, B. (1974). Theory of Probability, volume 1. London: Wiley. 628\n\u2014 (1975). Theory of Probability , volume 2. London: Wiley. 628\nGoldstein, M. (1999). \u201cBayes linear analysis.\u201d In Kotz, S. et al. (eds.), Encyclopaedia of\nStatistical Sciences, 29\u201334. Chichester: Wiley. 628\n\u2014 (2006). \u201cSubjective Bayesian Analysis: Principles and Practice.\u201d Bayesian Analysis, 1(3):\n403\u2013420. 630\n660 Galaxy Formation: a Bayesian Uncertainty Analysis\n\u2014 (2010). \u201cExternal Bayesian analysis for computer simulators.\u201d In Bernardo, J. M., Bayarri,\nM. J., Berger, J. O., Dawid, A. P., Heckerman, D., Smith, A. F. M., and West, M. (eds.),\nTo appear in Bayesian Statistics 9. Oxford University Press. 630\nGoldstein, M. and Rougier, J. C. (2006). \u201cBayes linear calibrated prediction for complex\nsystems.\u201d Journal of the American Statistical Association, 101(475): 1132\u20131143. 633\n\u2014 (2009). \u201cReified Bayesian modelling and inference for physical systems (with Discussion).\u201d\nJournal of Statistical Planning and Inference, 139(3): 1221\u20131239. 628, 657\nGoldstein, M. and Wooff, D. A. (2007). Bayes Linear Statistics: Theory and Methods. Chich-\nester: Wiley. 628\nHeitmann, K., Higdon, D., et al. (2009). \u201cThe Coyote Universe II: Cosmological Models and\nPrecision Emulation of the Nonlinear Matter Power Spectrum.\u201d Astrophys. J., 705(1): 156\u2013\n174. 630\nHigdon, D., Gattiker, J., Williams, B., and Rightley, M. (2008). \u201cComputer Model Calibration\nUsing High-Dimensional Output.\u201d Journal of the American Statistical Association, 103(482):\n570\u2013583. 633\nHigdon, D., Kennedy, M., Cavendish, J. C., Cafeo, J. A., and Ryne, R. D. (2004). \u201cCombining\nfield data and computer simulations for calibration and prediction.\u201d SIAM Journal on\nScientific Computing , 26(2): 448\u2013466. 630\nHouse, L., Goldstein, M., and Vernon, I. (2009). \u201cExchangeable Computer Models.\u201d MUCM\nTechnical Report 10\/01, submitted to Journal of the Royal Statistical Society, Series B. 626,\n643, 657\nKennedy, M. C. and O\u2019Hagan, A. (2001). \u201cBayesian calibration of computer models.\u201d Journal\nof the Royal Statistical Society, Series B, 63(3): 425\u2013464. 633\nNorberg, P., Cole, S., et al. (2002). \u201cThe 2dF Galaxy Redshift Survey: The bJ -band galaxy\nluminosity function and survey selection function.\u201d Mon.Not.Roy.Astron.Soc., 336: 907\u2013934.\n625\nOakley, J. and O\u2019Hagan, A. (2002). \u201cBayesian inference for the uncertainty distribution of\ncomputer model outputs.\u201d Biometrika, 89(4): 769\u2013784. 630\nO\u2019Hagan, A. (2006). \u201cBayesian analysis of computer code outputs: A tutorial.\u201d Reliability\nEngineering and System Safety , 91: 1290\u20131300. 630\nPukelsheim, F. (1994). \u201cThe three sigma rule.\u201d The American Statistician, 48: 88\u201391. 634\nRaftery, A. E., Givens, G. H., and Zeh, J. E. (1995). \u201cInference from a deterministic population\ndynamics model for bowhead whales (with Discussion).\u201d Journal of the American Statistical\nAssociation, 90: 402\u2013430. 633\nRougier, J. C. (2008). \u201cEfficient emulators for multivariate deterministic functions.\u201d Journal\nof Computational and Graphical Statistics, 17(4): 827\u2013843. 668, 669\nSacks, J., Welch, W. J., Mitchell, T. J., and Wynn, H. P. (1989). \u201cDesign and analysis of\ncomputer experiments.\u201d Statistical Science, 4(4): 409\u2013435. 626, 636\nI. Vernon, M. Goldstein and R. G. Bower 661\nSantner, T. J., Williams, B. J., and Notz, W. I. (2003). The Design and Analysis of Computer\nExperiments. New York: Springer-Verlag. 626, 636, 667\nSpergel, D. N. et al. (2003). \u201cFirst Year Wilkinson Microwave Anisotropy Probe (WMAP)\nObservations: Determination of Cosmological Parameters.\u201d Astrophys. J. Suppl., 148: 175\u2013\n194. 664\nSpringel, V. et al. (2005). \u201cSimulating the joint evolution of quasars, galaxies and their large-\nscale distribution.\u201d Nature, 435: 629\u2013636. 663\nVernon, I. and Goldstein, M. (2009). \u201cBayes Linear Analysis of Imprecision in Computer\nModels, with Application to Understanding Galaxy Formation.\u201d In Augustin, T., Coolen,\nF. P. A., Moral, S., and Troffaes, M. C. M. (eds.), ISIPTA\u201909: Proceedings of the Sixth\nInternational Symposium on Imprecise Probability: Theories and Applications, 441\u2013450.\nDurham, UK: SIPTA. 652\nAppendix A: A Guide to Galaxy Formation\nThe aim of galaxy formation studies is to understand why the universe appears as it does.\nWe wish to explain the characteristic properties of galaxies, such as their distribution of\nluminosities, colours and ages. As we will describe below, the present problem is not so\nmuch to understand why galaxies form, but to understand why they are relatively few\nand far between. By understanding this, we hope also to explain why galaxy formation\nappears to proceed very differently to that expected in the simplest theories. The basic\ningredients have been in place for some time (the force of gravity and radiative cooling\nof baryonic matter), but we are only now beginning to understand how the formation of\ngalaxies is regulated. The surprising result is that the black holes (the densest known\nobjects in the universe) appear to play a key role in this.\nGalaxy Formation - a Beginners Guide\nSo how do galaxies form? Why is the universe filled with such objects? In principle,\nit is a straightforward consequence of the dominance of the gravitational force. Since\nall matter makes a positive contribution to the gravitational force, the clumping of the\nuniverse\u2019s mass is a runaway process. As the condensations of matter become denser,\nthey become more effective as attractors. These matter concentrations are referred to\nas haloes. The observational evidence shows that most of this mass, however, is not\nnormal, \u201cbaryonic\u201d, matter (that you and I are made from) and that the universe is\ndominated by \u201cCold Dark Matter\u201d (CDM): massive particles that interact very weakly.\nThe CDM particles may be associated with super-symmetric extensions of the standard\nmodel of particle physics. Recent observations have also shown that a vacuum energy\ncontribution is required.\nThe CDM particles explain the collapse and growth of the gravitating dark matter\nhaloes, but to describe the formation of the luminous galaxies, we must turn to the\nastrophysics of the baryonic matter. As the baryons are pulled together by the collapse of\n662 Galaxy Formation: a Bayesian Uncertainty Analysis\nthe dark matter halo, they heat up and start to resist further compression. The baryonic\ngas (but not the collisionless dark matter) radiate this energy and cool leading to a\nrun-away contraction that is only stopped by the conservation of angular momentum.\nThe baryons form thin, cold spinning disks of gas. Further condensing leads to the\nformation of stars, and empirical measurements show that the rate of formation of\nstars is proportional to the surface density of gas (for current theoretical models, this\nempirical calibration is entirely sufficient).\nIn this scenario, small haloes are able to convert almost all their baryonic component\ninto stars, but this does not accurately reflect the universe we live in. In contrast to our\ninitial model, the fraction of the baryonic material that is observed to form into stars is\nrather small, only about 10% of the total baryonic content of the universe. The origin\nof this discrepancy is a key cosmological puzzle, and astronomers appeal to \u201cfeedback\u201d\nto resolve the discrepancy: somehow the formation of stars must inject energy that\nprevents further gas cooling. One of the key aims of the GALFORM project is to\nidentify the feedback schemes that are needed to account for the observed universe.\nIn small galaxies, we believe that the primary regulation mechanism is supernovae:\nthe energetic explosions that massive stars undergo at the end of their life. In weak\ngravitational potentials, these are capable of driving gas out of the galaxy.\nThe strength and importance of feedback is best assessed by comparing the observed\ngalaxy mass function (the numbers of galaxies in a given mass per unit volume) with the\nhalo mass function. If star formation were uniformly efficient, there would be a constant\noffset between the two. However, a comparison shows that they differ dramatically in\nshape: the dark matter mass function has far more small haloes than are observed\nto host dwarf galaxies in the universe and lack a sharp cut-off at high masses. While\nsupernovae may solve the problem with faint galaxies, it cannot explain the sharp cutoff\nat high masses. Of the solutions proposed, the current front runner is a form of feedback\nassociated with the accretion of gas on to black holes.\nBlack holes are tiny compared to galaxies, their size (measured as their Schwarzschild\nradius or radius of their event horizon) is only 1.5 \u00d7 108 km. It is surprising that an\nobject so small can heat a volume with radius 1011 times larger. Yet this is just what is\nobserved in clusters of galaxies. Clusters are gravitationally bound systems containing\n1000s of galaxies and 1015 solar masses of (largely) dark matter. Gas at the centres of\nthese systems is dense enough that it should cool, promoting the formation of stars in\nthe central object. Yet, little cooling is observed. Instead these systems host a powerful\nradio galaxy \u2014 a galaxy with a central black hole (or AGN) that is the source of a jet\nof magnetised high energy plasma. Although the details are not yet clear, relativistic\nparticle jets from the black hole are capable of replacing the energy that is lost as cooling,\nkeeping the central gas hot and starving the central galaxy of fuel for star formation.\nThe frequency of the discovery of such objects is also remarkable - they seem to occur\neverywhere the runaway cooling process would generate a problem. It is now widely\naccepted that it provides an essential ingredient for models that explain the formation\nof galaxies.\nI. Vernon, M. Goldstein and R. G. Bower 663\nModelling Galaxy Formation\nThere are essentially two approaches to modelling the formation of galaxies. These are\nusually referred to as \u201cnumerical simulation\u201d and \u201csemi-analytic modelling\u201d.\nThe idea of \u201cnumerical simulation\u201d is simple and direct. A powerful computer\nis programmed with the fundamental physical equations that describe the growth of\nfluctuations of dark matter, the hydrodynamical response of the intergalactic gas and\nits loss of energy through key atomic cooling processes. However, the equations are\nmissing some key components of galaxy formation physics and massively over-produce\nthe abundance of stars. Unfortunately, such codes have no hope of directly following\nthe formation of stars or the winds they may generate at their death, and are many\nmore orders of magnitude from being able to track the formation of black holes or the\nprocesses that generate the jets that regulate the formation of bright galaxies.\n\u201cSemi-analytic modelling\u201d represents the alternative approach. Rather than tackling\nthe whole problem in a single numerical integration, we break it down into its separate\ncomponents. Of course, we must make some level of approximation by doing this, but we\nhope to create a model that encompasses the main physical processes with a minimum\nof complexity. For example, one component of the model is the growth and merging of\ndark matter haloes. This can be computed through an analytic approximation or by\nrunning a numerical calculation that only includes the force of gravity. In terms of the\nbehaviour of the dark matter, this approximation is extremely good. We must then add\ncomponents to describe such features as the collapse and cooling of gas; the formation\nof stars; the growth of black holes; merging of galaxies; the feedback effect of supernova\nexplosions and jets from black holes, and then link them together through a network\nof interactions. Adding further components complicates the model but may improve\nits physical realism and ability to match the data. Each component is based on the\nresults of a targeted set of simulations - or, failing this, on physically plausible scaling\nrelations. In many cases, however, the physical process is not completely understood\nor characterised: to cope with this we introduce a number of parameters to account for\nthis uncertainty. The result is a network of equations (or algorithms) whose behaviour\nis driven by the underlying growth and merging of the dark matter haloes, and whose\nresponse is governed by a number of adjustable input parameters. Because of the\nintrinsic complexity of the galaxy formation problem, \u201csemi-analytic models\u201d currently\noffer the best avenue for progress.\nAppendix B: Galform - Physical Details\nWe now outline some relevant technical details of the GALFORM code. For an extended\ndescription and discussion of the Galform implementation see Baugh (2006). In essence,\nthe model consists of a set of modules, each having associated input parameters.\n1. Dark matter merger trees. These are extracted from the \u201cMillennium\u201d dark\nmatter simulation (Springel et al. (2005)). This is a full numerical simulation of the\ngrowth of dark matter structures in the universe from cosmological initial conditions.\n664 Galaxy Formation: a Bayesian Uncertainty Analysis\nThe initial spectrum of density fluctuations is set to be consistent with the WMAP satel-\nlite observations of the cosmic microwave background (Spergel et al. (2003)). The subse-\nquent evolution involves solving the gravitational N-body problem for a collection of 1010\nparticles. The computations took several months on state of the art super-computers at\nthe Max Planck Society\u2019s Rechenzentrum in Munich, Germany. Fortunately, this part of\nthe model need only be solved once, and the main part of the GALFORM code can then\nbe applied to populate the dark matter haloes with galaxies. This approach improves\naccuracy over previous analytic approximations to gravitational structure growth, but\nmeans that we must fix the cosmological parameters for our model. In future, improved\nanalytic modelling of the merger trees will allow us to include the uncertainty in the\ncosmological parameters. For now, cosmological parameters are fixed to the canonical\nyear 3 observations of WMAP in which \u2126b = 0.045, \u2126M = 0.25, \u039b = 0.75 and \u03c38 = 0.9\nat the present day. The model assumes H0 = 0.73, although we quote luminosities and\nspace densities in term of h = H0\/100kms\u22121 so that this dependence is explicit.\n2. Gas Accretion and Cooling. As dark matter haloes grow, the gas that they\ncontain cools and flows to the centre. This occurs at different rates depending on the\nmass of the halo, and the rate at which the halo mass grows. The supply of gas is\ndetermined by computing the mass of gas for which the cooling timescale is less than\nthe halo, and the mass of gas which has had sufficient time to cool and fall to the centre\n(Cole et al. 2001; Baugh 2006). The newer version of the code (referred to as B06),\nwhich is considered in this case study, made several important advances (Bower et al.\n(2006)). One of these is to emphasise the distinction between haloes for which the gas\nsupply is limited by the rate of cooling (henceforth \u201chydrostatic\u201d haloes) and those\nhaloes for which the free-fall timescale is the limiting factor (henceforth \u201crapid cooling\u201d\nhaloes). In the B06 model, it is assumed that energy from the central black hole can\nonly offset the cooling in hydrostatic haloes. The parameter \u03b1cool determines the exact\nratio of timescales at which this distinction is made.\n3. Star Formation. As the hot gas cools or is accreted by a halo, it builds up a\nreservoir of cold gas in the central galaxy. This gas provides the fuel for the formation of\nfurther stars. The code assumes that the star formation rate is related to the dynamical\ntimescale of the galaxy, and its mass of gas, giving\nm\u02d9\u2217 = \u000f?\n(\nmcold\n\u03c4disk\n)( vdisk\n200kms\u22121\n)\u03b1?\nwhere m\u02d9\u2217 is the star formation rate, mcold is the mass of cold gas, \u03c4disk is the disk\ndynamical time and vdisk is the disk rotation speed. \u03b1? and \u000f? are parameters that\ncontrol the rate of star formation and its dependence on galaxy mass. In B06, an\nadditional mode of star formation is also considered. If the disk becomes too massive, it\nbecomes susceptible to warps that grow, funnelling gas to the centre of the galaxy. Such\nsecular evolution may generate many of the bulges that are observed. In the model it\nis assumed that instabilities occur if the disk\u2019s gravity exceeds the stabilising gravity\nof the halo. The threshold at which this occurs is set by the parameter fstab, at which\npoint the disk stars are added to the galaxy\u2019s bulge and the disk gas is consumed in a\nburst of star formation.\nI. Vernon, M. Goldstein and R. G. Bower 665\n4. Feedback - from supernovae. Soon after the most massive stars form, they\nexplode in powerful supernova explosions. These are thought to be responsible for\npreventing the efficient formation of stars in small galaxies - as the stars form, gas is\ndriven out of the system by the supernovae. We model feedback from supernovae by\nassuming that the ratio of material expelled from the galaxy into the halo to that formed\ninto stars is given by the ratio \u03b2, where\n\u03b2 = (vdisk\/vhot)\n\u2212\u03b1hot (23)\nwhere vhot and \u03b1hot are poorly constrained parameters. We allow vhot to take different\nvalues for quiescent and burst star formation which we denote as Vhot,burst and Vhot,disk.\nThe gas that is driven out of galaxies flows into the halo, but does not immediately\nbecome available for cooling. The timescale on which the gas becomes available is\ndetermined by the parameter \u03b1reheat. If this is unity, and cooling is efficient, ejected gas\nwill be allowed to fall back into the galaxy on the dynamical timescale.\n5. Galaxy mergers. When dark haloes collide, the galaxies at their centres do\nnot immediately merge. Rather their relative motion slowly decays due to dynamical\nfriction. This process is discussed extensively in Cole et al. (2001). The merging time is\nset by an overall normalisation parameter fdf . If the time since the halo was accreted is\nless than the merging time, the galaxy from the \u201csatellite\u201d galaxy orbits inside the larger\none. Such satellite galaxies do not collect any gas from the halo, and so star formation\nquickly subsides as the cold gas reservoir is exhausted. If the time since accretion\nexceeds the merging timescale, the galaxy mergers with the central galaxy in the parent\nhalo. If the mass ratio of the galaxies exceeds fellip, this can cause disturbance to the\nunderlying galaxy, transforming it from a spiral type galaxy to an elliptical one. This\nmorphological transformation may be associated with a burst of star formation. If the\nmass ratio exceeds fburst, there is no morphological transformation, but a burst of star\nformation still occurs.\n6. Black holes and their feedback. The model assumes that black holes grow\nthrough three distinct channels: (i) by black hole - black hole mergers when the parent\ngalaxies merge; (ii) by accretion of gas that is funnelled to the galaxy centre during\nbursts of star formation (these being driven either by mergers or disk instabilities);\n(iii) by diffuse gas accretion from hydrostatic haloes (i.e., as a result of \u201cradio mode\u201d\nfeedback). The star burst driven accretion results in luminous quasars, but the current\nmodel assumes that these events do not contribute to the feedback. The parameter\nFbh controls the amount of gas that is accreted by the black hole in these events. The\nfeedback from \u201cradio mode\u201d accretion is, however, of key importance. The mass growth\nof the black hole is determined from the energy output required to counter-balance\ncooling of the halo, i.e. we implicitly assume that the mass accretion rate increases until\nthe net cooling rate decreases to zero. However, accretion onto black holes, although\nan abundant source of energy has limits. We limit the maximum energy output to be\nless than \u000fEddLEdd where LEdd is the Eddington luminosity of the black hole and \u000fEdd\nis an adjustable parameter. Current models for black hole accretion suggest that \u000fEdd\nis of order 1%.\n666 Galaxy Formation: a Bayesian Uncertainty Analysis\n7. Reionisation At very early times, the majority of gas in the universe is neu-\ntral (and the universe is opaque to ultra-violet light). As stars and quasars form in\nabundance, the universe quickly ionizes. This creates an additional form of heating\nthat may be extremely important in very low-mass galaxies. The details of this process\nare very important for understanding the paucity of dwarf galaxies that orbit in the\nmilky-way halo. However, we are here concentrating on the properties of much more\nmassive systems where these effects are less significant and it is sufficient to parame-\nterise this process by two parameters, zcut and vcut. Here, zcut defines the redshift at\nwhich re-ionisation occurs: at lower redshifts, gas cooling is prevented in haloes with\ncircular velocity below vcut.\nAppendix C: Construction of the Wave 1-4 Emulators.\nUnivariate Emulation: Wave 1\nIn section 4.3 we discuss the construction of the wave 1 emulator (see equation 6), and\nhere we describe in detail the procedures involved in this process, namely, active variable\nselection, choice of gij functions, assessment of the regression coefficients \u03b2ij and the\nGaussian process parameters \u03c3ui , \u03c3wi and \u03b8i.\nIn choosing the set of active variables x[Ai] for each output i the aim is to explain\na large amount of the variance of fi(x) using as few variables as possible. For each of\nthe 7 outputs, we used the 993 wave 1 runs to initially reduce the set xB by backwards\nstepwise elimination, starting with a model containing the 8 linear terms. At this stage\nindividual inputs were discarded in turn based upon the size of their main effect. Before\nan input would be discarded, a third order polynomial was fitted to see the extent of\nvariance explained with the current set of active variables. It was found that 5 active\nvariables could explain satisfactory amounts of the variance of fi(x) for each output i\n(see table 2), based on the adjusted R2 of the polynomial fits. In each case, more than\n5 variables yielded little extra benefit (compared to the increase in the size of the input\nspace), while less than 5 led to substantially worse fits.\nOnce the set of active variables x[Ai] has been determined, the full set of regression\nterms gij(x[Ai]) can be chosen. This was done by forward stepwise selection starting\nwith a model containing the linear terms in the active variables, and adding possible\nterms from the full 3rd order polynomial in the active variables, using standard stepwise\nroutines in R, based on criteria such as AIC. When the regression terms have been chosen\nfor each output fi(x), estimates for the B = {\u03b2ij} coefficients can be obtained using\nOrdinary Least Squares, assuming uncorrelated errors. We have a sufficiently large\ncollection of model evaluations that such data analytic techniques will result in small\nvariances on the regression coefficients and generally acceptable results from OLS fitting.\nTherefore, we would expect such results to overwhelm prior judgements. However, any\nsubstantial contradictions between the data and the qualitative form of such judgements\nrequires further investigation.\nAs the ui(x[Ai]) represent local deviations from the regression surface, there will\nI. Vernon, M. Goldstein and R. G. Bower 667\nbe a correlation between ui at neighbouring values of x[Ai], which we must specify.\nVarious choices are available, each of which involves parameters related to the width\nand shape of the correlation function. Estimation of these parameters can be a difficult\ntask. However, these parameters are representations of our subjective assessment of the\nsmoothness of the function and precise assessment of them is not necessarily meaningful,\nand nor is it required in order to construct an emulator of sufficient accuracy for our\nneeds. Here we choose the following Gaussian covariance structure:\nCov(ui(x[Ai]), ui(x\n\u2032\n[Ai]\n)) = \u03c32ui exp(\u2212||x[Ai] \u2212 x\u2032[Ai]||2\/\u03b82i ), (24)\nwhere \u03c32ui is the point variance at any given x[Ai], \u03b8i is the correlation length parameter\nthat controls the strength of correlation between two separated points in the input\nspace (for points a distance \u03b8 apart, the correlation will be exactly exp(\u22121)), and\n|| \u00b7 || is the Euclidean norm. As wi(xB) represents all the remaining variation in the\ninactive variables, it is often small and we treat it as uncorrelated random noise with\nVar(wi(xB)) = \u03c32wi . We consider the point variances of these two processes to be\nproportions of the overall residual variance of the computer model given the emulator\ntrend, \u03c32i , and write \u03c3\n2\nui = (1 \u2212 wi)\u03c32i and \u03c32wi = wi\u03c32i for some small wi. Various\ntechniques for estimating the correlation length and parameters \u03b8i and wi from the data\nare available (for example variograms (Cressie (1991)), REML (Santner et al. (2003));\nhowever, these estimation procedures can often be non-robust as the output from a\ncomputer model rarely behaves exactly like an actual Gaussian Process. An alternative\nis to specify the \u03b8i parameters a priori (Craig et al. 1996) followed by an approximate\nassessment of the nugget term wi, which is the approach we adopt here.\nWe may provide approximate order of magnitude values for the correlation length\nparameters \u03b8i, by appealing to the heuristic that the regression residuals may be viewed\nas deriving from a polynomial of order one higher than the fitted polynomial, as they\ncorrespond to the first order of terms which are neglected by the regression fit. Here\nthis implies that values of \u03b8i should be chosen corresponding to the shape of a 4th order\npolynomial. In such a case, we would not want the correlation length to be greater\nthan the average distance between roots of a 4th order polynomial: approximately\n0.25 of the range of the input. Alternatively it can be argued that there should be\npositive correlation between outputs at the turning points and the adjacent roots of\nthe polynomial, and that the correlation length must therefore be greater than this\ndistance: approximately 0.125 of the range of the input. This argument tends to give\nmore conservative (i.e. smaller) specifications for the correlation length compared to\nmaximum likelihood or variogram methods. As we have scaled all inputs to the range\n[\u22121, 1], this argument suggests that a working estimate of \u03b8i might lie between 0.25 and\n0.5, and therefore we selected the same value for all \u03b8i of 0.35, checked by emulator\ndiagnostics discussed in section 4.4.\nThe value of the nugget parameter wi represents the proportion of residual variance\ndue to the inactive variables. We obtained a working assessment of wi by examining\nthe variance explained by the inactive variables for each of the seven outputs, and\ncomparing this to the residual variance from the active variable polynomial fit. These\nconsiderations led to a conservative value of 0.2 for all wi acknowledging a reasonable\n668 Galaxy Formation: a Bayesian Uncertainty Analysis\ncontribution from the inactive variables at each output. Provided conservative choices\nare made and are combined with analysis of the emulator diagnostics, such specifications\nlead to emulators of sufficient accuracy for the task of providing a first stage reduction of\nthe input space, while avoiding the complex and often misleading problem of estimating\nsuch parameters from the data alone. At this stage, we only require a relatively simple\nemulator in order to make an initial reduction of the input space, while leaving the\nconstruction of more detailed emulators to subsequent waves of the analysis.\nUnivariate Emulation: Waves 2 to 4\nThe Wave 2 to 4 univariate emulators were constructed using similar methods as were\nused in Wave 1, as described in detail in section 4.3. Here we give a summary of their\nconstruction, highlighting the differences with the Wave 1 case.\nRecall that for Waves 1-3 we only explored 8 of the input parameters, which were\nthe set of proposed active variables described in section 4.2 and shown in table 1, with\nthe effect of the remaining 9 inputs being described by the model discrepancy term\n\u03a6IA (see section 5.1). The selection of Wave 2 and 3 Active Variables proceeded as\nfor Wave 1, and it was found that all 8 input parameters were required as active in\nthese cases. Therefore, the only difference to the form of the Wave 1 emulator given by\nequation (6), is that now there is no nugget term wi(xB). The selection and fitting of\nthe polynomial terms was performed as in section 4.2 and appendix C.1, and a similar\nGaussian covariance function to equation (24) was assumed. In Wave 4, it was found\nthat improved polynomial fits could be obtained using 10 active variables, composed of\nthe 8 variables used in Wave 1-3 (and given in table 1) with the addition of the inputs\nalphastar and tau0mrg. The remaining 7 inputs were found to have little impact on the\n11 luminosity function outputs considered. As the effect of all 17 inputs are represented\nby the Wave 4 emulator, the \u03a6IA model discrepancy term (representing the 9 previously\ninactive variables) was dropped at this stage. Table 3 summarises the number of runs\nused at each wave, along with the number of active variables required.\nMultivariate Emulation: Waves 3 and 4\nIn Waves 1 and 2 univariate emulators were used, which allow only the use of univariate\nimplausibility measures to reduce the input space. Therefore, at Wave 3 we constructed\na multivariate emulator in order to develop the corresponding multivariate implausibility\nmeasure I(x) introduced in section 3.5. I(x) will be of use as it measures different\naspects of the model output compared to the univariate implausibility measures, namely\nit is sensitive to the shape of the luminosity functions.\nConstructing a tractable multivariate emulator can be a challenging task. An emu-\nlator that utilizes a weakly stationary process (such as ui(x) in equation (24)) suffers\nfrom what is referred to as the (nq)3 problem (Rougier (2008)), where n is the number\nof model evaluations and q is the number of outputs to be emulated. The process of up-\ndating our beliefs about the emulator given the n model evaluations generally requires\nI. Vernon, M. Goldstein and R. G. Bower 669\nthe inverting of a matrix of size nq \u00d7 nq, a computation that scales as (nq)3. At Wave\n4 say we have n = 2011 and q = 11, leading to a problematic matrix inversion of size\n22121. However, by specifying covariance structures of suitably symmetric form this\nproblem can be avoided.\nThe wave 3 emulator has the same form as that of wave 2, where again we use all\n8 inputs as Active Variables (that is x[Ai] = xB), and we consider the same set of 11\noutputs. Again the gij(xB) and \u03b2ij terms were chosen by model selection techniques\nand OLS fitting respectively: we compare these polynomials to those of previous waves\nin section 7.2. We then assume the following separable multivariate covariance structure\nfor the process ui(xB):\nCov(ui(xB), uj(x\u2032B)) = \u03a3ij exp(\u2212||x[B] \u2212 x\u2032[B]||2\/\u03b82), (25)\nwhere the i and j indices denote each of the 11 outputs, \u03a3 is an 11\u00d711 covariance matrix\nand note we have removed the i index on \u03b8 as we have assumed the same correlation\nlength for each output. We assess the matrix \u03a3 by taking the covariance matrix of the\n11 sets of residuals from each of the polynomials. The separable form of equation (25)\nallows the above problematic matrix to be written as a direct product, which greatly\nsimplifies the calculation of its inverse, as we can invert each component of the direct\nproduct individually. See Rougier (2008) for further discussions regarding calculations\nfor multivariate emulators.\nThe construction of a multivariate emulator allows the use of a Multivariate Implau-\nsibility measure which can be defined as (using equation (16)):\nI2(x) = (E(f(x))\u2212 z)T (Var(f(x)) + Var(\u000fmd) + Var(\u000fobs))\u22121(E(f(x))\u2212 z). (26)\nI(x) is a useful measure to consider as it captures the shape of the luminosity function\noutput. It will allow the discarding of inputs corresponding to runs that satisfy the\nunivariate matching criteria and hence are close to the data points, but that have an\nunphysical shape in either the bj or K luminosity function.\n670 Galaxy Formation: a Bayesian Uncertainty Analysis\n"}