{"doi":"10.1080\/0968776000080207","coreId":"14258","oai":"oai:generic.eprints.org:332\/core5","identifiers":["oai:generic.eprints.org:332\/core5","10.1080\/0968776000080207"],"title":"Revisiting objective tests: A case study in integration at honours level","authors":["Maclaran, Pauline","Sangster, Alan"],"enrichments":{"references":[{"id":1880531,"title":"67Pauline Maclaran and Alan Songster Revisiting objective tests: a case study in integration at honours level","authors":[],"date":"1997","doi":"10.1080\/0968776000080207","raw":"67Pauline Maclaran and Alan Songster Revisiting objective tests: a case study in integration at honours level Valley, K. (1997), 'Learning styles and courseware design', ALT-J, 5 (2) , 42-51.","cites":null},{"id":1043083,"title":"A computer-aided continuous assessment system',","authors":[],"date":"1996","doi":"10.3402\/rlt.v4i2.9968","raw":"Turton, B. C. H. (1996), 'A computer-aided continuous assessment system', ALT-J, 5 (2), 48-60.","cites":null},{"id":198821,"title":"A pedagogical framework for embedding C&IT into the curriculum',","authors":[],"date":"1998","doi":"10.1080\/0968776980060202","raw":"Conole, G. and Oliver, M. (1998), 'A pedagogical framework for embedding C&IT into the curriculum', ALT-J, 6 (2), 4-16.","cites":null},{"id":1043086,"title":"Deconstructing deep and surface: towards a critique of phenomenology',","authors":[],"date":"1997","doi":null,"raw":"Webb, G. (1997), 'Deconstructing deep and surface: towards a critique of phenomenology', Higher Education, 33, 195-212.","cites":null},{"id":1043082,"title":"Designing software to maximize learning',","authors":[],"date":"1996","doi":"10.3402\/rlt.v4i3.9974","raw":"Somekh, B. (1996), 'Designing software to maximize learning', ALT-J, 4 (3), 4-16.","cites":null},{"id":198823,"title":"Guidelines for Promoting Effective Learning","authors":[],"date":"1992","doi":null,"raw":"Entwhistle, N., Thompson, S. and Tait, H. (1992), Guidelines for Promoting Effective Learning in Higher Education, Centre for Research on Learning and Instruction: Edinburgh.","cites":null},{"id":1043079,"title":"How to Write Multiple Choice Questions,","authors":[],"date":"1997","doi":null,"raw":"Kehoe, J. (1997), How to Write Multiple Choice Questions, http:\/\/quizplease.com\/qhe\/p\/qphowto5.htm.","cites":null},{"id":198824,"title":"Instructor control in an automated environment: a reconsideration with empirical evidence',","authors":[],"date":"1992","doi":"10.1080\/09639289200000047","raw":"Fogarty, T. J. and Goldwater, P. M. (1992) 'Instructor control in an automated environment: a reconsideration with empirical evidence', Accounting Education, 1 (4), 219-310.","cites":null},{"id":198825,"title":"Learning styles: individualizing computer-based learning environments',","authors":[],"date":"1995","doi":"10.3402\/rlt.v3i2.9610","raw":"Groat, A. and Musson, T. (1995), 'Learning styles: individualizing computer-based learning environments', ALT-J, 6 (2), 53-62.","cites":null},{"id":198822,"title":"Marketing, Third European Edition,","authors":[],"date":"1997","doi":null,"raw":"Dibb, S, Simpkin, L., Pride, W. M. and Ferrell, O. C. (1997), Marketing, Third European Edition, Houghton Mifflin: Boston.","cites":null},{"id":1043080,"title":"Methodological constants in courseware design',","authors":[],"date":"1997","doi":"10.1111\/1467-8535.00015","raw":"Persico, D. (1997), 'Methodological constants in courseware design', British Journal of Educational Technology, 28 (2), 111-24.","cites":null},{"id":1043081,"title":"Objective tests, learning to learn, and learning styles',","authors":[],"date":"1995","doi":"10.1080\/09639289600000015","raw":"Sangster, A. (1995), 'Objective tests, learning to learn, and learning styles', Accounting Education, 5 (2), 131-46.","cites":null},{"id":1043084,"title":"Revisiting objective tests: a case study in integration at honours level","authors":[],"date":"1997","doi":"10.1080\/0968776000080207","raw":null,"cites":null},{"id":198817,"title":"Taxonomy of Educational Objectives: The Classification of Educational Goals,","authors":[],"date":"1956","doi":"10.1177\/001316446502500324","raw":"Bloom, B. S., Englehart, M. B., Frost, E. J., Hill, W. H. and Krathwohl, D. R. (1956), Taxonomy of Educational Objectives: The Classification of Educational Goals, Longman: New York.","cites":null},{"id":1043087,"title":"Teaching and learning on the Internet',","authors":[],"date":"1995","doi":null,"raw":"Whalley, B. (1995), 'Teaching and learning on the Internet', Active Learning, 2, 25-9.","cites":null},{"id":198820,"title":"The Marketing Learning Centre,","authors":[],"date":"1998","doi":null,"raw":"Catterall, M. and Ibotsen, P. (1998), The Marketing Learning Centre, http:\/\/www.busmgt.ulst.ac.uk\/h_mifflin\/.","cites":null},{"id":1043085,"title":"Thought and Language,","authors":[],"date":"1986","doi":"10.1017\/s0033291700026155","raw":"Vygotsky, L. (1986), Thought and Language, MIT Press: Cambridge, MA.","cites":null},{"id":198818,"title":"Towards a Theory of Instruction,","authors":[],"date":"1966","doi":"10.1177\/019263656605030929","raw":"Bruner, J. S. (1966), Towards a Theory of Instruction, The Belknap Press of Harvard University Press: Cambridge MA.","cites":null},{"id":1043078,"title":"Using computer based learning materials on the undergraduate programme',","authors":[],"date":"1997","doi":null,"raw":"Keady, H. (1997), 'Using computer based learning materials on the undergraduate programme', Nottingham Business School Teaching and Learning Group Conference, 5, 65-74.","cites":null},{"id":198816,"title":"Using information technology to add value to management education',","authors":[],"date":"1997","doi":"10.2307\/257035","raw":"Alavi, M., Yoo, Y. and Vogel, D. R. (1997), 'Using information technology to add value to management education', Academy of Management Journal, 40 (6), 1310-33.","cites":null},{"id":198819,"title":"Using IT for assessment: going forward',","authors":[],"date":"1994","doi":null,"raw":"Bull, J. (1994), 'Using IT for assessment: going forward', Active Learning, No. 1.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2000","abstract":"This paper examines the background to computer\u2010assisted assessment and shows how certain misconceptions or \u2018myths\u2019 have arisen around its use. It then discusses an actual implementation of computerized multiple\u2010choice question (MCQ) tests, addressing both the main theoretical issues, and the practicalities of the design and administration process. It confirms that honours\u2010level learning can be appropriately assessed using summative computer\u2010based objective tests, not just in the eyes of the adopting academic, but also in the eyes of the students. Care should, however, be taken to adopt a flexible implementation that is responsive to unforeseen problems","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/14258.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/332\/1\/ALT_J_Vol8_No2_2000_Revisiting%20objective%20tests_%20A%20.pdf","pdfHashValue":"9100bfae024458f761944a310b6212d875526821","publisher":"University of Wales Press","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:332<\/identifier><datestamp>\n      2011-04-04T09:15:12Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/332\/<\/dc:relation><dc:title>\n        Revisiting objective tests: A case study in integration at honours level<\/dc:title><dc:creator>\n        Maclaran, Pauline<\/dc:creator><dc:creator>\n        Sangster, Alan<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        This paper examines the background to computer\u2010assisted assessment and shows how certain misconceptions or \u2018myths\u2019 have arisen around its use. It then discusses an actual implementation of computerized multiple\u2010choice question (MCQ) tests, addressing both the main theoretical issues, and the practicalities of the design and administration process. It confirms that honours\u2010level learning can be appropriately assessed using summative computer\u2010based objective tests, not just in the eyes of the adopting academic, but also in the eyes of the students. Care should, however, be taken to adopt a flexible implementation that is responsive to unforeseen problems.<\/dc:description><dc:publisher>\n        University of Wales Press<\/dc:publisher><dc:date>\n        2000<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/332\/1\/ALT_J_Vol8_No2_2000_Revisiting%20objective%20tests_%20A%20.pdf<\/dc:identifier><dc:identifier>\n          Maclaran, Pauline and Sangster, Alan  (2000) Revisiting objective tests: A case study in integration at honours level.  Association for Learning Technology Journal, 8 (2).  pp. 58-68.  ISSN 0968-7769     <\/dc:identifier><dc:relation>\n        10.1080\/0968776000080207<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/332\/","10.1080\/0968776000080207"],"year":2000,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"Revisiting objective tests: a case study in\nintegration at honours level\nPauline Maclaran* and Alan Sangster**\n*Department of Marketing De Montfort University\n**The Open University Business School\nemail: a.sangster@open.ac.uk\nThis paper examines the background to computer-assisted assessment and shows how\ncertain misconceptions or 'myths' have arisen around its use. It then discusses an actual\nimplementation of computerized multiple-choice question (MCQ) tests, addressing both\nthe main theoretical issues, and the practicalities of the design and administration\nprocess. It confirms that honours-level learning can be appropriately assessed using\nsummative computer-based objective tests, not just in the eyes of the adopting academic,\nbut also in the eyes of the students. Care should, however, be taken to adopt a flexible\nimplementation that is responsive to unforeseen problems.\nIntroduction\nMultiple-choice questions (MCQ) are widely accepted in the United States as a mode of\nassessment in undergraduate courses. In an environment of ever increasing class sizes, they\nprovide a faster way to assess large groups of students, whilst also providing a way to\nmeasure deep understanding. However, in the UK there is scepticism from lecturers about\nthe use of MCQ tests and, perhaps more significantly, from students themselves. This\nfrequently means that someone using MCQs as a means of assessment will find both a lack\nof support from colleagues and resistance from students.\nWhen MCQ tests are computerized, the introduction of this additional non-traditional\nelement in the assessment process can compound this situation. This paper seeks to\nprovide evidence to support anyone considering using this mode of assessment. A\ndescription is given of a case study of the use of computerized multiple-choice summative\nassessment for a class of 260 honours-level undergraduate students taking a marketing\nmodule.\n58\nALT-J Volume 8 Number 2\nAn overview of computer-assisted assessment\nMuch computer-assisted assessment (CAA) relies on the incorporation of MCQ tests. The\npreferred term for MCQs is 'objective tests', although they are, of course, only as objective\nas the person who designs them (Brown, Bull, and Pendlebury, 1997). Objective testing is a\nbroader and more appropriate term given that much MCQ software incorporates more\nthan just selection of one answer from four or five choices. This is certainly the case with\nthe Question Mark Designer program used for this present case study.\nOne of the most common misconceptions that has occurred in relation to this type of\ntesting is that it is less about student learning and more about lecturer convenience. It is\nfrequently considered to be, at best, a practical way to manage larger classes and, at worst,\nan easy option for lazy colleagues. This 'easy option' myth has arisen mainly because of the\nemphasis throughout the 1980s and early 1990s oh the use of computer-assisted learning\n(CAL) to manage larger classes. This criticism certainly has some foundation, for objective\ntesting does provide a more efficient way to respond to larger class sizes. However, when\neffectively and appropriately integrated into the student learning experience, it not only has\nsound pedagogical validity, it can take as much, if not more, effort to administer than\ntraditional approaches to assessing large classes. Appropriately applied, objective testing is\ncertainly not the 'quick-fix solution' that some suggest (Bull, 1994).\nMore recently, there have been an increasing number of studies that investigate\npedagogical issues such as:\n\u2022 learning styles in relation to computer-based learning (Groat and Musson, 1995;\nSangster, 1995; Valley, 1997);\n\u2022 methodologies for integrating CAL into the curriculum (Persico, 1997; Conole and\nOliver, 1998);\n\u2022 the relationship of theories of learning to courseware design (Somekh, 1996).\nThese studies emphasize the importance of a holistic approach to computer-assisted\nlearning that considers overall educational goals, and that is designed as part of a\ncomprehensive learning plan and not just a stand-alone or ad hoc add-on.\nA further benefit of learning technologies, such as CAA, is that they offer alternative\navenues of tuition to students (Keady, 1997). This flexibility is particularly useful for\nstudents at a distance, or those who require additional support. It also allows and\nencourages independent learning. With so much emphasis on a progression from high\nteacher support levels to increased student empowerment (Whalley, 1995) CAL can enable\nstudents to learn material at their own pace and in their own preferred ways.\nIntegrating MCQs into the curriculum\nStudents tend to focus on those aspects of a course that they perceive will contribute to\ntheir overall marks (Brown et al, 1997). One major benefit of objective tests arises from\ntheir ability to test breadth of knowledge. Course developers who take advantage of this\nfeature will often find their students read more widely around the subject than they might\ndo in a conventionally delivered module in the same subject. The introduction of\ncomputer-based objective tests as a primary mode of course assessment on the B.Sc.\n59\nPauline Madaran and Alan Songster Revisiting objective tests: a case study in integration at honours level\nManagement marketing module was devised to achieve three goals:\n\u2022 to encourage the development of a breadth of subject knowledge in the students;\n\u2022 to encourage the students to take responsibility for their own learning;\n\u2022 to encourage them to utilize a Website entitled the Marketing Learning Centre\n(Catterall and Ibotsen, 1998) accompanying their core textbook (Dibb, Simpkin, Pride\nand Ferrell, 1997).\nThe Website (Figure 1) offers the following learning support: a bank of MCQs to\naccompany each chapter, Internet exercises to learn about marketing organizations and\ncase companies, links with Canadian and US Marketing Learning Centres and a glossary\nof terms.\nur las \u2022\u00bb\u2022\u00ab! l*rlm~U)rmar Menrt 6\u00a7\nIntro4t\u00bbcfaon jj\nJ. :-.-.- , \\\ninternet Exercises MCQ's\n^^^ss^^^^B^^^^*\nSection \u00a3jk Glossary\nFigure \/: The Marketing Learning Centre Webpage.\nOne resource of the Marketing Learning Centre that the students particularly appreciated\nwas the links to the Web pages of companies such as Kelloggs, Lego, Disney, and\nMcDonald's. In accessing these sites students were able to study how the companies were\nincorporating their Websites into their overall marketing strategy. Moreover, these exercises\nprovided a starting point for students to begin to explore the Web and discover other links\nfor themselves.\nStudents were allocated a supervised one-hour computer laboratory session each week,\nworking in groups of around sixty students. In these sessions, they accessed the Marketing\nLearning Centre and worked through the learning support materials in whatever manner\n60\nAdJ-J Volume 8 Number 2\nthey chose. They could work with each other or alone, in sequential order if they wished,\nor they could alternate between topics and relevant Websites, etc. The students were also\nexpected to spend an additional four to six hours per week on the material outside the\nformal class sessions.\nThis flexibility in the way the students were encouraged to approach their learning was\nimportant, particularly to acknowledge the social aspects of learning as put forward by\nBruner (1966) and Vygotsky (1986). Those that needed support from their tutor also\nbenefited from the greater individual access to the tutor resulting from the increased\nfreedom this undirected approach to student learning in the computer labs gave the tutor\nto have one-to-one contact as individual problems arose. A third benefit arising from the\ndesign of this module was the confirmatory support students could obtain from the bank\nof MCQ tests in the Marketing Learning Centre (see Figure 2). This formative assessment\nprovided quick feedback on their progress, feedback that would have otherwise not been so\npromptly available, particularly given the large class size.\n\u2022araxrtaltn ta tStSttfc* nsB aHMhaOuM (kutppts* at a* xbs* \u2022*\u2022\u00ab&\naniuuw !\u201e , \u201e__\u201e,\nIMXXX\nA3 ' \"\nFigure 2: An example of the Marketing Learning Centre MCQ tests.\nA crucial factor in motivating the students was that their learning would be tested by two\nsummative computer-assisted objective tests, in the sixth and tenth weeks of the twelve-\nweek course. These were specifically designed to reflect the majority of topics covered\nduring the two hours a week of lectures and one hour a week of computer sessions. Thus,\nthe design of the course was such that the two objective tests played a pivotal role in\ndriving the learning of the students.\n61\nPauline Madarvn and Alan Songster Revisiting objective tests: a case study in integration at honours level\nFurthermore, because these tests were assessing specific knowledge about the principles of\nmarketing and their applications, the final written examination was designed to take a\nmore contextual and holistic approach than would otherwise have been appropriate. The\nexamination comprised two compulsory questions. The first was to put together a mini-\ncase study to illustrate a marketing-orientated company. The second was to discuss how the\nInternet could be used in a firm's marketing strategy. Students were advised early in the\ncourse of the nature of these questions, ensuring they had an incentive both to pay\nattention to the various case studies they encountered and to make use of other material\nthat they accessed through the Web, either directly or via the Marketing Learning Centre. It\ncan be seen, therefore, that both the formative MCQs and the summative objective tests\nwere very much designed as an integrative formative part of the overall student learning\nexperience in this module.\nDesign of MCQs using Question Mark Designer\nThe software program used for the design of the objective tests was Question Mark\nDesigner. It allows for a flexible approach to question design and offers a variety of\nquestion types and formats, including the ability to introduce graphic images. The main\nquestion types include explanation, multiple choice, text match, multiple response, numeric\nand selection. There are different formats within each of these categories. For example,\nwithin 'selection' there are options for true\/false\/don't know, agree\/disagree, ranking, and\nmatching statements. This variety of forms is important if different levels of learning are to\nbe integrated through this approach into the assessment process.\nIn terms of Bloom's taxonomy of learning (Bloom, Englehart, Frost, Hill and Krathwoh,\n1956) - knowledge, comprehension, application, analysis, synthesis and evaluation -\nteachers often ask questions only in the 'knowledge' category. Kehoe (1997), for example,\nestimates that this occurs 80-90 per cent of the time and suggests that a higher-order level\nof questions should be used, particularly in the writing of MCQs.\nNot surprisingly, as the main purpose of the Marketing Learning Centre is to support a\nbuild-up of knowledge around topics in the textbook, the question banks it contains fall\nmainly into the 'knowledge' category, i.e. they rely on remembering, memorizing, and\nrecognizing. As a result, while they are very useful in supporting and enhancing student\nlearning, their scope is too limited for them to be suitable for use as summative assessment\non a module of this type. For summative assessment, it was seen as important in the\ncontext of this honours-level module to avoid a predominance of question types and\nformats that require only 'knowledge' responses. This meant moving away from\nconventional multiple-choice and text match designs towards the introduction of question\ntypes that encourage deeper levels of analysis and evaluation such as 'multiple response' or\n'selection' type questions. The latter type has one particularly useful format called 'Web'\nthat allows the presentation of information, for example a mini-case study or detailed\ndescription of a company or event. Questions can then test how this information has been\ncomprehended, analysed, synthesized and evaluated, thereby extending question categories\nas described in Bloom taxonomy.\nThis also obviates one of the principal criticisms levelled at MCQs, that they will encourage\nsurface learning in place of a deeper approach (Entwistle, Thompson and Tait, 1992). This is\n62\nfill-} Volume 6 Number 2\na contentious point. Webb (1997) goes so far as to suggest that the deep-surface dichotomy is\nlittle more than a ship of convenience and argues that it is a question of personal perspective\nand culture as to whether 'deep' is better than 'surface', or vice versa. Whatever the merits of\nthese arguments, there is no reason why MCQs cannot be designed to test at the higher levels\nof Bloom's taxonomy and, therefore, encourage learning beyond the level generally\nconsidered 'surface' (see, for example, Fogarty and Goldwater, 1992).\nApart from scope in question design, Question Mark Designer provides good facilities for\nquestion evaluation. Once a test has been completed, the program will provide collated\nscores and a comprehensive question and answer analysis. Each question can be analysed\nin terms of its facility and ability to discriminate between students. For example, the\nanalysis places questions on a continuum running from zero to one. Any question lying too\nclose to either end of the scale is either too difficult (close to zero) or too easy (close to\none). Questions can then be revised in the light of this analysis.\nThere are other useful program features, such as randomization of questions, several\nfeedback alternatives, restrictions on test time, password control, and options for\nmodifications in scoring, including negative marking.\nMain implementation difficulties\nThe first test in the sixth week comprised thirty questions to be answered in twenty\nminutes. The tests were held in the students' normal computer sessions and took place over\na one-week period. Overall, there were four test sessions with around sixty-five students in\neach. The difficulties that were experienced during the actual implementation of the first\ntest can be broadly divided into two categories - preparing the students for the tests and\nthe process management of the taking of the tests.\nStudent preparation\nInitially students were unconvinced that objective tests offered a valid way to assess their\nknowledge. Some time had to be spent ensuring that students would 'buy-in' to this type of\nassessment. This was done in several ways:\n\u2022 Some of the early resistance disappeared as students gained experience of testing their\nknowledge through the Marketing Learning Centre. They liked being able to obtain\nimmediate feedback on their progress. Where they worked in a group, a competitive\naspect - albeit light-hearted - was evident.\n\u2022 The integrative aspects of the course were explained in detail to the students, and\nfrequently repeated. The fact that studying for these summative MCQ assessments\nwould also help them prepare for the end-of-module written examination meant that\nthe examination would be less stressful than usual and this was seen as a bonus by\nmany of the students. In addition, the knowledge that they would ultimately have the\nchance to express themselves in a more traditional, essay-style format offered them\nfurther reassurance that they were not being disadvantaged by this non-traditional\napproach to their assessment.\n\u2022 To ensure that the students were prepared for the more complex question types used in\nthe summative objective tests, they were given demo disks with sample Question Mark\nDesigner tests to practice upon.\n63\nPauline Modaran and Nan Songster Revisiting objective tests: a case study in integration at honours level\n\u2022 The introduction of negative marking for certain questions helped allay fears that\nlazy students could just be lucky on the day. This was introduced in respect of questions\nthat required true or false answers and therefore offered a 50 per cent chance of a\nguessed answer being correct. A 'don't know' category was included that scored 0\nmarks. In the event that true or false were wrongly selected, the candidate was awarded\nminus 1.\n\u2022 Negative marking is a controversial aspect of objective design (Turton, 1996). In this\ninstance it was introduced on seven questions each with four statements to be\nevaluated. When the impact of this negative marking was analysed, it was found that\nno student had lost more than three marks as a result of its being used. Overall,\nnegative marking reduced the mean mark for the first summative objective test by 1 per\ncent.\nDespite all this preparation, the students were insufficiently prepared for the actual\nquestion formats of the first test. Many students appeared unable to move from the\n'knowledge' type questions in the Marketing Learning Centre to the summative objective\ntest's more demanding level of questions.\nTime pressure was an additional factor that caused concern and for which they had no\nprevious practice. While around a third of the class scored highly (at the upper-second\/first\nclass honours level) approximately a quarter failed the test altogether. Many of those in the\nmiddle range (lower-second\/third class categories) felt they should have achieved higher\nscores.\nThe spreading panic and loss of confidence in the tests was only averted by introducing a\nthird test and announcing that only their two best marks would count towards their final\nmark. This alleviated fears and meant that those who had done badly had a possibility of\ndiscounting their first mark altogether. With hindsight this notion of 'two out of three'\nwould have been a good idea in any case and might have assisted the initial 'buying in'\nprocess.\nIn addition, the demo MCQs the students had been given to practise using the testing\nsoftware were generic ones provided with the software. This did not appear to have had the\ndesired effect of ensuring familiarity with the testing software, possibly because of the lack\nof relevance of the subject matter to their course. To address this, disks containing three\nshort objective tests in marketing were created and provided to the students. Again with\nhindsight, these would also have been useful in the early stages to encourage an early\nflexibility in question responses and to stop the rut effect in student thinking caused by their\nextensive exposure to the knowledge-focused MCQs at the Marketing Learning Centre.\nProcess management\nWith such a large class, the process management of the first test was problematic and had\nseveral implications for the administration of the subsequent two tests. The four sessions\nwith sixty-five students were spread through the week, from Tuesday to Friday. Four\nhelpers were recruited to assist and supervise students. This was important to ensure that\nstudents did not copy the test to pass on to a friend. In the inevitable flurry of arrivals and\ndepartures, it still proved difficult to monitor students closely at the start and the end of\nthe test.\n64\nALT-J Volume 8 Number 2\nIn addition, students found it intimidating to be treated in the regimental that their\nnumbers necessitated, particularly by other staff with whom they were not always familiar.\nFurthermore, when results from Friday's class (the last test) were compared with Tuesday's\n(the first), a 5 per cent increase across the range of marks was noted. Although the time\npressure of the questions ensured that few were remembered, there was the worry that\nstudents who took the test early were being disadvantaged, even if only in terms of the\ntime to prepare for the test.\nFor subsequent tests it was decided to assess all students on the same day in groups\nconsisting of 25-30 students in a smaller computer laboratory. Ten sessions were run at\nforty-minute intervals throughout the day and this proved highly successful from the\nprocess management side. A disc was placed in each computer to record the scores of the\nten or so students who used each PC. This saved time not only in copying the original test\nonto disks but also in collating the scores afterwards (a maximum of thirty disks versus\nsixty-five for the first test). It also meant that students would be given a computer number\nin advance of the test and go straight to their allocated seat upon their arrival in the\ncomputer lab.\nFurthermore, the tests were permanently ready to run and this reduced the risk of disk\ncopying that had been a major concern in the first test. The atmosphere in the lab was\ngreatly improved from that in the first test. Students appeared more relaxed and they\nseemed reassured by the fact that their module lecturer conducted all the administration of\nthe test. The same approach was also adopted for the third objective test.\nStudent performance\nThe relative performance of the students in the three tests and in their exam can be seen in\nTable 1.\nNumber of students\n60+\n40-59\n<40\nMean (per cent)\nTest 1\n255\n81\n114\n60\n51.0\nTest 2\n246\n233\n13\n0\n81.8\nTest 3\n179\n72\n93\n14\n56.2\nOverall\n257\n217\n36\n4\n71.0\nExam\n251\n49\n201\n1\n52.8\nOverall\n251\n164\n85\n2\n62.6\nTable I: Student performance.\nThere was clearly a problem with the level of mark attained in the second objective test. In\norder to restore students' confidence, it had been made appreciably easier than the first\nobjective test and the negative marking dropped. Students commented that, as a result of\ntheir experiences of the first test, they had worked extremely hard for the second test. The\ncombination of these two factors gave rise to the abnormally high marks. The third\nobjective test was set to the same standard as the first test and the profile of the students\nacross the three reported grades clearly shifted away from the fail band that had been so\nevident in the first test. Unfortunately, the second test had a significant impact on the\noverall score for the tests and this, in turn, resulted in a very skewed overall mark for the\n65\nPauline Madaran and Alan Songster Revisiting objective tests: a case study in integration at honours level\ncourse, despite the exam average being at approximately the same level as the averages for\nthe first and second tests.\nObviously, finding the right level of test when objective tests are used in this way, is a non-\ntrivial task. If it is too high, the students may lose confidence and may even withdraw from\nthe course. If it is too low, marks across the entire course may be skewed and a final mark\nawarded that is hot merited on performance across the course as a whole.\nThe marks achieved by the students on their other modules were similarly around 62 per\ncent on average, and the average mark achieved by students taking the module in the\nprevious year was virtually the same at 61.9 per cent. Consequently, it was decided not to\nnormalize or otherwise adjust the marks. In future, as both the level of test and the 'better'\nquestions have been identified, this is not a problem that is likely to recur. However, it did\ngive cause for concern among some colleagues who were already uncomfortable at an\nhonours-level module being assessed in this way.\nConclusion\nOverall, students deemed the approach to the module a success, with some 68 per cent in\ntheir module evaluation forms mentioning the computer learning as one of the aspects\nthey most liked about it. Similarly, they viewed the summative tests as being an appropriate\nform of assessment and one that had the added benefit of encouraging them to learn\nmaterial as the module progressed instead of in a cramming exercise at the end of the\nmodule. However, the problems encountered illustrate the point made by Alavi, Yoo and\nVogel (1997) that the integration of information technology into management education is\nnot trivial; and that effective use requires a departure from traditional pedagogical models.\nThere are a number of lessons to be learned from this case study when preparing to adopt\na similar approach:\n\u2022 Prepare students to accept the concept of summative objective tests.\n\u2022 Prepare students for computer-based summative objective tests by providing them with\nopportunities to gain knowledge and experience of using the software on domain-\nrelevant questions before they have their first summative test.\n\u2022 Integrate the tests into the overall design of the course.\n\u2022 Ensure that different learning outcomes are tested by the examination.\n\u2022 Experiment with question design and remember the learning levels you are testing.\n\u2022 Plan the process management of the tests carefully and be prepared to alter it swiftly if\nproblems arise.\n\u2022 Above all, this case study confirms that computer-based objective tests can be used\nappropriately and effectively for summative testing at honours level.\n66\nALT-) Volume 8 Number 2\nReferences\nAlavi, M., Yoo, Y. and Vogel, D. R. (1997), 'Using information technology to add value to\nmanagement education', Academy of Management Journal, 40 (6), 1310-33.\nBloom, B. S., Englehart, M. B., Frost, E. J., Hill, W. H. and Krathwohl, D. R. (1956),\nTaxonomy of Educational Objectives: The Classification of Educational Goals, Longman:\nNew York.\nBrown, G., Bull, J. and Pendlebury, M. (1997), Assessing Student Learning in Higher\nEducation, Routledge: London.\nBruner, J. S. (1966), Towards a Theory of Instruction, The Belknap Press of Harvard\nUniversity Press: Cambridge MA.\nBull, J. (1994), 'Using IT for assessment: going forward', Active Learning, No. 1.\nCatterall, M. and Ibotsen, P. (1998), The Marketing Learning Centre,\nhttp:\/\/www.busmgt.ulst.ac.uk\/h_mifflin\/.\nConole, G. and Oliver, M. (1998), 'A pedagogical framework for embedding C&IT into the\ncurriculum', ALT-J, 6 (2), 4-16.\nDibb, S, Simpkin, L., Pride, W. M. and Ferrell, O. C. (1997), Marketing, Third European\nEdition, Houghton Mifflin: Boston.\nEntwhistle, N., Thompson, S. and Tait, H. (1992), Guidelines for Promoting Effective\nLearning in Higher Education, Centre for Research on Learning and Instruction:\nEdinburgh.\nFogarty, T. J. and Goldwater, P. M. (1992) 'Instructor control in an automated\nenvironment: a reconsideration with empirical evidence', Accounting Education, 1 (4),\n219-310.\nGroat, A. and Musson, T. (1995), 'Learning styles: individualizing computer-based\nlearning environments', ALT-J, 6 (2), 53-62.\nKeady, H. (1997), 'Using computer based learning materials on the undergraduate\nprogramme', Nottingham Business School Teaching and Learning Group Conference, 5,\n65-74.\nKehoe, J. (1997), How to Write Multiple Choice Questions,\nhttp:\/\/quizplease.com\/qhe\/p\/qphowto5.htm.\nPersico, D. (1997), 'Methodological constants in courseware design', British Journal of\nEducational Technology, 28 (2), 111-24.\nSangster, A. (1995), 'Objective tests, learning to learn, and learning styles', Accounting\nEducation, 5 (2), 131-46.\nSomekh, B. (1996), 'Designing software to maximize learning', ALT-J, 4 (3), 4-16.\nTurton, B. C. H. (1996), 'A computer-aided continuous assessment system', ALT-J, 5 (2),\n48-60.\n67\nPauline Maclaran and Alan Songster Revisiting objective tests: a case study in integration at honours level\nValley, K. (1997), 'Learning styles and courseware design', ALT-J, 5 (2) , 42-51.\nVygotsky, L. (1986), Thought and Language, MIT Press: Cambridge, MA.\nWebb, G. (1997), 'Deconstructing deep and surface: towards a critique of phenomenology',\nHigher Education, 33, 195-212.\nWhalley, B. (1995), 'Teaching and learning on the Internet', Active Learning, 2, 25-9.\n68\n"}