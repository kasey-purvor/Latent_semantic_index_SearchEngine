{"doi":"10.1111\/j.1467-9868.2008.00661.x","coreId":"70825","oai":"oai:eprints.lancs.ac.uk:10281","identifiers":["oai:eprints.lancs.ac.uk:10281","10.1111\/j.1467-9868.2008.00661.x"],"title":"Particle filters for partially-observed diffusions.","authors":["Fearnhead, Paul","Papaspiliopoulos, O.","Roberts, Gareth O."],"enrichments":{"references":[{"id":16394647,"title":"A new factorisation of di\u00aeusion measure and \u00afnite sample path constructions.","authors":[],"date":null,"doi":null,"raw":"| (2005b) A new factorisation of di\u00aeusion measure and \u00afnite sample path constructions. Submitted.","cites":null},{"id":16394708,"title":"A new technique for simulating the likelihood of stochastic di\u00aeerential equations.","authors":[],"date":"2002","doi":"10.1111\/1368-423x.t01-1-00075","raw":"Nicolau, J. (2002) A new technique for simulating the likelihood of stochastic di\u00aeerential equations. The Econometrics Journal, 5, 91{103.","cites":null},{"id":16394656,"title":"An improved particle \u00aflter for nonlinear problems.","authors":[],"date":"1999","doi":"10.1049\/ip-rsn:19990255","raw":"Carpenter, J., Cli\u00aeord, P. and Fearnhead, P. (1999) An improved particle \u00aflter for nonlinear problems. IEE proceedings-Radar, Sonar and Navigation, 146, 2{7.","cites":null},{"id":16394667,"title":"An introduction to sequential Monte Carlo methods. In Sequential Monte Carlo methods in practice,","authors":[],"date":"2001","doi":"10.1007\/978-1-4757-3437-9_1","raw":"Doucet, A., de Freitas, N. and Gordon, N. (2001) An introduction to sequential Monte Carlo methods. In Sequential Monte Carlo methods in practice, Stat. Eng. Inf. Sci., 3{14. New York: Springer.","cites":null},{"id":16394700,"title":"Bayesian analysis of single-molecule experimental data.","authors":[],"date":"2005","doi":"10.1111\/j.1467-9876.2005.00509.x","raw":"Kou, S. C., Xie, X. S. and Liu, J. S. (2005) Bayesian analysis of single-molecule experimental data. J. Roy. Statist. Soc. Ser. C, 54, 469{506.","cites":null},{"id":16394671,"title":"Bayesian sequential inference for nonlinear multivariate di\u00aeusions.","authors":[],"date":"2006","doi":"10.1007\/s11222-006-9392-x","raw":"Golightly, A. and Wilkinson, D. J. (2006) Bayesian sequential inference for nonlinear multivariate di\u00aeusions. Statistics and Computing, 16, 323{338.","cites":null},{"id":16394666,"title":"Branching and interacting particle systems. Approximations of Feymann-Kac formulae with applicationc to non-linear \u00afltering.","authors":[],"date":"2000","doi":"10.1007\/bfb0103798","raw":"27Del Moral, P. and Miclo, L. (2000) Branching and interacting particle systems. Approximations of Feymann-Kac formulae with applicationc to non-linear \u00afltering.","cites":null},{"id":16394659,"title":"Central limit theorem for sequential Monte Carlo methods and its application to Bayesian inference.","authors":[],"date":"2004","doi":"10.1214\/009053604000000698","raw":"Chopin, N. (2004) Central limit theorem for sequential Monte Carlo methods and its application to Bayesian inference. The Annals of Statistics, 32, 2385{2411.","cites":null},{"id":16394643,"title":"Closed-form likelihood expansions for multivariate di\u00aeusions. Working paper, available from http:\/\/www.princeton.edu\/\u00bbyacine\/research.htm.","authors":[],"date":"2004","doi":null,"raw":"A\u00c4 \u00b3t-Sahalia, Y. (2004) Closed-form likelihood expansions for multivariate di\u00aeusions. Working paper, available from http:\/\/www.princeton.edu\/\u00bbyacine\/research.htm.","cites":null},{"id":16394662,"title":"Estimation of the coe\u00b1cients of a di\u00aeusion from discrete observations.","authors":[],"date":"1986","doi":"10.1080\/17442508608833428","raw":"Dacunha-Castelle, D. and Florens-Zmirou, D. (1986) Estimation of the coe\u00b1cients of a di\u00aeusion from discrete observations. Stochastics, 19, 263{284.","cites":null},{"id":16394651,"title":"Exact and e\u00b1cient likelihood{based inference for discretely observed di\u00aeusions (with discussion).","authors":[],"date":"2006","doi":"10.1111\/j.1467-9868.2006.00552.x","raw":"Beskos, A., Papaspiliopoulos, O., Roberts, G. O. and Fearnhead, P. (2006b) Exact and e\u00b1cient likelihood{based inference for discretely observed di\u00aeusions (with discussion). Journal of the Royal Statistical Society, Series B.","cites":null},{"id":16394654,"title":"Exact simulation of di\u00aeusions.","authors":[],"date":"2005","doi":"10.1214\/105051605000000485","raw":"Beskos, A. and Roberts, G. O. (2005) Exact simulation of di\u00aeusions. Annals of Applied Probability, 15, 2422{2444.","cites":null},{"id":16394711,"title":"Filtering via simulation: auxiliary particle \u00aflters.","authors":[],"date":"1999","doi":"10.2307\/2670179","raw":"Pitt, M. K. and Shephard, N. (1999) Filtering via simulation: auxiliary particle \u00aflters. J. Amer. Statist. Assoc., 94, 590{599.","cites":null},{"id":16394675,"title":"Inference and \u00afltering for partially observed di\u00aeusion processes via sequential Monte Carlo. Working paper available from http:\/\/www.stat.lsa.umich.edu\/\u00bbionides\/pubs\/WorkingPaper-\u00aflters.pdf.","authors":[],"date":"2003","doi":null,"raw":"Ionides, E. (2003) Inference and \u00afltering for partially observed di\u00aeusion processes via sequential Monte Carlo. Working paper available from http:\/\/www.stat.lsa.umich.edu\/\u00bbionides\/pubs\/WorkingPaper-\u00aflters.pdf.","cites":null},{"id":16394661,"title":"Interacting particle systems approximations of the Kushner-Stratonovich equation.","authors":[],"date":"1999","doi":"10.1239\/aap\/1029955206","raw":"Crisan, D., Del Moral, P. and Lyons, T. J. (1999) Interacting particle systems approximations of the Kushner-Stratonovich equation. Advances in Applied Probability, 31, 819{838.","cites":null},{"id":16394663,"title":"Kalman-Bucy \u00afltering for linear systems driven by the Cox process with shot noise intensity and its application to the pricing of reinsurance contracts.","authors":[],"date":"2005","doi":"10.1239\/jap\/1110381373","raw":"Dassios, A. and Jang, H.-W. (2005) Kalman-Bucy \u00afltering for linear systems driven by the Cox process with shot noise intensity and its application to the pricing of reinsurance contracts. J. Appl. Probab., 42, 93{107.","cites":null},{"id":16394658,"title":"Likelihood based inference for di\u00aeusion driven state space models.","authors":[],"date":"2006","doi":null,"raw":"Chib, S., Pitt, M. K. and Shephard, N. (2006) Likelihood based inference for di\u00aeusion driven state space models. Submitted.","cites":null},{"id":16394668,"title":"Modeling term structures of defaultable bonds.","authors":[],"date":"1999","doi":"10.1093\/rfs\/12.4.687","raw":"Du\u00b1e, D. and Singleton, K. J. (1999) Modeling term structures of defaultable bonds. The Review of Financial Studies, 12, 687{720.","cites":null},{"id":16394676,"title":"Monte Carlo \u00aflter and smoother for non-Gaussian nonlinear state space models.","authors":[],"date":"1996","doi":"10.2307\/1390750","raw":"Kitagawa, G. (1996) Monte Carlo \u00aflter and smoother for non-Gaussian nonlinear state space models. Journal of Computational and Graphical Statistics, 5, 1{25.","cites":null},{"id":16394703,"title":"Monte Carlo \u00aflters:Algorithms and theoretical analysis.","authors":[],"date":"2005","doi":null,"raw":"K\u00c4 unsch, H. R. (2005) Monte Carlo \u00aflters:Algorithms and theoretical analysis. Annals of Statistics, 33, 1983{2021.","cites":null},{"id":16394645,"title":"Monte Carlo maximum likelihood estimation for discretely observed di\u00aeusion processes.","authors":[],"date":"2005","doi":"10.1214\/07-aos550","raw":"26Beskos, A., Papaspiliopoulos, O. and Roberts, G. O. (2005a) Monte Carlo maximum likelihood estimation for discretely observed di\u00aeusion processes. Submitted.","cites":null},{"id":16394673,"title":"Novel approach to nonlinear\/nonGaussian Bayesian state estimation.","authors":[],"date":"1993","doi":"10.1049\/ip-f-2.1993.0015","raw":"Gordon, N., Salmond, D. and Smith, A. F. M. (1993) Novel approach to nonlinear\/nonGaussian Bayesian state estimation. IEE proceedings-F, 140, 107{113.","cites":null},{"id":16394669,"title":"Numerical techniques for maximum likelihood estimation of continuous-time di\u00aeusion processes.","authors":[],"date":"2002","doi":"10.1198\/073500102288618397","raw":"Durham, G. B. and Gallant, A. R. (2002) Numerical techniques for maximum likelihood estimation of continuous-time di\u00aeusion processes. J. Bus. Econom. Statist., 20, 297{338. With comments and a reply by the authors.","cites":null},{"id":16394664,"title":"On the stability of interactin processes with applications to \u00afltering and genetic algorithms.","authors":[],"date":"2001","doi":null,"raw":"Del Moral, P. and Guionnet, A. (2001) On the stability of interactin processes with applications to \u00afltering and genetic algorithms. Ann. Inst. of H. Poincar\u00b6 e Probab. Statist.","cites":null},{"id":16394660,"title":"Particle \u00aflters - a theoretical perspective.","authors":[],"date":"2001","doi":"10.1007\/978-1-4757-3437-9_2","raw":"Crisan, D. (2001) Particle \u00aflters - a theoretical perspective. In Sequential Monte Carlo Methods in Practice (eds. A. Doucet, N. de Freitas and N. gordon), 17{41. Springer{ Verlag; New York.","cites":null},{"id":16394649,"title":"Retrospective exact simulation of di\u00aeusion sample paths with applications.","authors":[],"date":null,"doi":"10.3150\/bj\/1165269151","raw":"| (2006a) Retrospective exact simulation of di\u00aeusion sample paths with applications. Bernoulli, 12, 1077{1098.","cites":null},{"id":16394705,"title":"Sequential Monte Carlo methods for dynamic systems.","authors":[],"date":"1998","doi":"10.2307\/2669847","raw":"Liu, J. S. and Chen, R. (1998) Sequential Monte Carlo methods for dynamic systems. J.","cites":null},{"id":16394670,"title":"The econometrics of ultra-high-frequency data.","authors":[],"date":"2000","doi":"10.1111\/1468-0262.00091","raw":"Engel, R. F. (2000) The econometrics of ultra-high-frequency data. Econometrica, 68, 1{22.","cites":null},{"id":16394665,"title":"The Monte-Carlo method for \u00afltering with discrete-time observations. Probab. Theory Related Fields,","authors":[],"date":"2001","doi":"10.1007\/pl00008786","raw":"Del Moral, P., Jacod, J. and Protter, P. (2001) The Monte-Carlo method for \u00afltering with discrete-time observations. Probab. Theory Related Fields, 120, 346{368.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2008-09","abstract":"In this paper we introduce novel particle filters for a class of partially-observed continuous-time dynamic models where the signal is given by a multivariate diffusion process. We consider a variety of observation schemes, including diffusion observed with error, observation of a subset of the components of the multivariate diffusion and arrival times of a Poisson process whose intensity is a known function of the diffusion (Cox process). Unlike currently available methods, our particle filters do not require approximations of the transition and\/or the observation density using time-discretisations. Instead, they build on recent methodology for the exact simulation of the diffusion process and the unbiased estimation of the transition density as described in Beskos et al. (2006). In particular, we introduce the Generalised Poisson Estimator, which generalises the Poisson Estimator of Beskos et al. (2006). Thus, our filters avoid the systematic biases caused by time-discretisations and they have significant computational advantages over alternative continuous-time filters. These advantages are supported theoretically by a central limit theorem","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/70825.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/10281\/1\/Fearn_Papa_Robe_rev2.pdf","pdfHashValue":"4c4f8054d76f27e1bd7493ba58752aad4d313f43","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:10281<\/identifier><datestamp>\n      2018-01-24T00:01:11Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Particle filters for partially-observed diffusions.<\/dc:title><dc:creator>\n        Fearnhead, Paul<\/dc:creator><dc:creator>\n        Papaspiliopoulos, O.<\/dc:creator><dc:creator>\n        Roberts, Gareth O.<\/dc:creator><dc:subject>\n        QA Mathematics<\/dc:subject><dc:description>\n        In this paper we introduce novel particle filters for a class of partially-observed continuous-time dynamic models where the signal is given by a multivariate diffusion process. We consider a variety of observation schemes, including diffusion observed with error, observation of a subset of the components of the multivariate diffusion and arrival times of a Poisson process whose intensity is a known function of the diffusion (Cox process). Unlike currently available methods, our particle filters do not require approximations of the transition and\/or the observation density using time-discretisations. Instead, they build on recent methodology for the exact simulation of the diffusion process and the unbiased estimation of the transition density as described in Beskos et al. (2006). In particular, we introduce the Generalised Poisson Estimator, which generalises the Poisson Estimator of Beskos et al. (2006). Thus, our filters avoid the systematic biases caused by time-discretisations and they have significant computational advantages over alternative continuous-time filters. These advantages are supported theoretically by a central limit theorem.<\/dc:description><dc:date>\n        2008-09<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:relation>\n        http:\/\/dx.doi.org\/10.1111\/j.1467-9868.2008.00661.x<\/dc:relation><dc:identifier>\n        Fearnhead, Paul and Papaspiliopoulos, O. and Roberts, Gareth O. (2008) Particle filters for partially-observed diffusions. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70 (4). pp. 755-777. ISSN 1369-7412<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/10281\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1111\/j.1467-9868.2008.00661.x","http:\/\/eprints.lancs.ac.uk\/10281\/"],"year":2008,"topics":["QA Mathematics"],"subject":["Journal Article","PeerReviewed"],"fullText":"Particle Filters for Partially Observed Diffusions\nPaul Fearnhead\u2217, Omiros Papaspiliopoulos\u2020and Gareth O. Roberts\u2020\nOctober 4, 2007\nAbstract\nIn this paper we introduce a novel particle filter scheme for a class of partially-\nobserved multivariate diffusions. We consider a variety of observation schemes, in-\ncluding diffusion observed with error, observation of a subset of the components of\nthe multivariate diffusion and arrival times of a Poisson process whose intensity is a\nknown function of the diffusion (Cox process). Unlike currently available methods,\nour particle filters do not require approximations of the transition and\/or the obser-\nvation density using time-discretisations. Instead, they build on recent methodology\nfor the exact simulation of the diffusion process and the unbiased estimation of the\ntransition density as described in Beskos et al. (2006b). We introduce the Generalised\nPoisson Estimator, which generalises the Poisson Estimator of Beskos et al. (2006b).\nA central limit theorem is given for our particle filter scheme.\nKeywords : Continuous-time particle filtering, Exact Algorithm, Auxiliary Variables, Cen-\ntral Limit Theorem, Cox Process\n1 Introduction\nThere is considerable interest in using diffusion processes to model continuous-time phe-\nnomena in many diverse scientific disciplines. These processes can be used to model directly\nthe observed data and\/or to describe unobserved processes in a hierarchical model. This\npaper focuses on estimating the path of the diffusion given partial information about it.\nWe develop novel particle filters for analysing a class of multivariate diffusions which are\npartially observed at a set of discrete time-points.\n\u2217Department of Mathematics and Statistics, Lancaster University, U.K., email:\np.fearnhead@lancaster.ac.uk\n\u2020Department of Statistics, Warwick University, U.K., email: O.Papaspiliopoulos@warwick.ac.uk,\nGareth.O.Roberts@warwick.ac.uk\nThe authors acknowledge helpful comments from the editor and referees.\n1\nParticle filtering methods are standard Monte-Carlo methods for analysing partially-\nobserved discrete-time dynamic models (Doucet et al., 2001). They involve estimating\nthe filtering densities of interest by a swarm of weighted particles. The approximation\nerror decreases as the number of particles, N , increases. However, filtering for diffusion\nprocesses is significantly harder than for discrete-time Markov models since the transition\ndensity of the diffusion is unavailable in all but a few special cases. In many contexts even\nthe observation density is intractable. Therefore, the standard propagation\/weighting\/re-\nsampling steps in the particle filter algorithm cannot be routinely applied.\nTo circumvent these complications, a further approximation, based on a time-discretisation\nof the diffusion, has been suggested (see for example Crisan et al., 1999; Del Moral et al.,\n2001). The propagation of each particle from one observation time to the next is done by\nsplitting the time increment into M , say, pieces and performing M intermediate simula-\ntions according to an appropriate Gaussian distribution. As M gets large this Gaussian\napproximation converges to the true diffusion dynamics. In this framework the compu-\ntational cost of the algorithm is of order M \u00d7 N , and the true filtering distributions are\nobtained as both M and N increase.\nOur approach does not rely on time-discretisation, but builds on recent work on the\nExact Algorithm for the simulation of diffusions (Beskos and Roberts, 2005; Beskos et al.,\n2006a, 2005b) and on the unbiased estimation of the diffusion transition density (Beskos\net al., 2006b, 2005a). This algorithm can be used in a variety of ways to avoid time\ndiscretisations in the filtering problem. The potential of the Exact Algorithm in the filtering\nproblem was brought up in the discussion of Beskos et al. (2006b), see the contributions\nby Chopin, Ku\u00a8nsch, and in particular Rousset and Doucet who also suggest the use of a\nrandom weight particle filter in this context.\nOne possibility is simply to use the Exact Algorithm to propagate the particles in the\nimplementation of the Gordon et al. (1993) bootstrap particle filter, thus avoiding entirely\nthe M intermediate approximate simulations between each pair of observation times. We\ncall this the Exact Propagation Particle Filter (EPPF). Where possible, a better approach\nis to adapt the Exact Algorithm to simulate directly from (a particle approximation to)\nthe filtering density using rejection sampling; we term this the Exact Simulation Particle\nFilter (ESPF).\nHowever, our favoured method goes in a different direction. We work in the framework\nof the auxiliary particle filter of Pitt and Shephard (1999), where particles are propagated\nfrom each observation time to the next according to a user-specified density and then are\nappropriately weighted to provide a consistent estimator of the new filtering distribution.\nDue to the transition density being unavailable, the weights associated with each particle\nare intractable. However, our approach is to assign to each particle a random positive\nweight which is an unbiased estimator of the true weight. We call this the Random Weight\nParticle Filter (RWPF). Our algorithm yields consistent estimates of the filtering distribu-\ntions. The replacement of the weights in a particle filter by positive unbiased estimators is\nan interesting possibility in more general contexts than the one considered in this paper.\n2\nIndeed, in Section 3.2 we show that this approach amounts to a convenient augmentation\nof the state with auxiliary variables.\nThe construction of the unbiased estimators of the weights is one of the main contri-\nbutions of this paper, and it is of independent interest. This is based on an extension\nof the Poisson Estimator of Beskos et al. (2006b), which we call the Generalised Poisson\nEstimator. This estimator is guaranteed to return positive estimates (unlike the Poisson\nEstimator) and its efficiency (in terms of variance and computational cost) can be up to\norders of magnitude better than the Poisson Estimator. Optimal implementation of the\nPoisson and the Generalised Poisson estimators is thoroughly investigated theoretically\nand via simulation.\nAll three time-discretisation-free particle filters we introduce are easy to implement,\nwith the RWPF being the easiest and the most flexible to adapt to contexts more general\nthan those considered here. A simulation study is carried out which shows that the RWPF\nis considerably more efficient than the ESPF which is more efficient than the EPPF. We also\nprovide a theoretical result which shows that our filters can have significant computational\nadvantages over time-discretisation methods. We establish a Central Limit Theorem (CLT)\nfor the estimation of expectations of the filtering distributions using either of the EPPF,\nESPF and the RWPF. This is an extension of the results of Chopin (2004). The CLT\nshows that, for a fixed computational cost K, the errors in the particle approximation of\nthe filtering distributions decrease as K\u22121\/2 in our methods, whereas it is known that the\nrate is K\u22121\/3 or slower in time-discretisation methods.\nThe main limitation of the methodology presented here is the requirement that the\nstochastic differential equation specifying the underlying diffusion process can be trans-\nformed to one with orthogonal diffusion matrix, and gradient drift. Although this frame-\nwork excludes some important model types (such as stochastic volatility models) it incor-\nporates a wide range of processes which can model successfully many physical processes.\nOn the other hand, our methods can handle a variety of discrete-time observation schemes.\nIn this paper we consider three schemes: noisy observations of a diffusion process, observa-\ntion of a subset of the components of a multivariate diffusion, and arrival times of a Poisson\nprocess whose intensity is stochastic and it is given by a known function of a diffusion.\nThe paper is organised as follows. Section 2 introduces the model for the underlying\ndiffusion and the necessary notation, the observation schemes we consider and the simulated\ndata sets on which we test our proposed methods. Section 3 introduces the RWPF and\nstates the CLT. Section 4 introduces the main tool required in constructing the RWPF,\nthe Generalised Poisson Estimator (GPE). Several theoretical results are established for\nthe GPE, and a simulation study is performed to assess its performance. Section 5 is\ndevoted in the empirical investigation of the performance of the different particle filters\nwe introduce. Several implementation issues are also discussed. Section 6 closes with a\ndiscussion on extensions of the methodology and the appendices contain technical results\nand proofs.\n3\n2 Signal, data and assumptions\nLet the signal be modelled by a d-dimensional diffusion process\ndXs = \u03b1(Xs) ds+ dBs , s \u2208 [0, t] . (1)\nWe assume throughout the paper that the drift is known. Our approach requires some\nassumptions which we summarize in this paragraph: i) \u03b1 is continuously differentiable in\nall its arguments, ii) there exists a function A : Rd \u2192 R such that \u03b1(u) = \u2207A(u), and\niii) there exists l > \u2212\u221e such that \u03c6(u) := (\u2016\u03b1(u)\u20162 +\u22072A(u))\/2 \u2212 l \u2265 0. Among these\nlast three conditions i) and iii) are weak and the strictest is ii), which in the ergodic case\ncorresponds to X being a time-reversible diffusion.\nThe transition density of (1) is typically intractable but a useful expression is available\n(see for example Beskos et al., 2006b; Dacunha-Castelle and Florens-Zmirou, 1986)\npt(xt | x0) = Nt(xt \u2212 x0) exp{A(xt)\u2212 A(x0)\u2212 lt}E\n[\nexp\n{\n\u2212\n\u222b t\n0\n\u03c6(Ws)ds\n}]\n. (2)\nIn this expression Nt(u) denotes the density of the d-dimensional normal distribution with\nmean 0 and variance tId evaluated at u \u2208 Rd, and the expectation is taken w.r.t. a\nBrownian bridge, Ws, s \u2208 [0, t], with W0 = x0 and Wt = xt. Note that the expectation\nin this formula typically cannot be evaluated.\nThe data consist of partial observations y1, y2, . . . , yn, at discrete time-points 0 \u2264 t1 <\nt2 < \u00b7 \u00b7 \u00b7 < tn. We consider three possible observation regimes:\n(A) Diffusion observed with error. The observation yi, is related to the signal at time ti\nvia a known density function f(yi|xti). This model extends the general state-space\nmodel by allowing the signal to evolve continuously in time. There is a wide range\nof applications which fit in this framework, see Doucet et al. (2001) for references.\n(B) Partial Information. At time ti we observe yi = \u03b6(Xti) for some non-invertible known\nfunction \u03b6(\u00b7). For example we may observe a single component of the d-dimensional\ndiffusion. In this model type f(yi|xti) = 1 for all xti for which \u03b6(xti) = yi.\n(C) Cox Process. In this regime the data consist of the observation times ti which are\nrandom and are assumed to be the arrivals of a Poisson process of rate \u03bd(Xs), for\nsome known function \u03bd. Such models are popular in insurance (Dassios and Jang,\n2005) and finance (Engel, 2000; Duffie and Singleton, 1999), and they have recently\nbeen used to analyse data from single molecule experiments (Kou et al., 2005).There\nis a significant difference between this observation regime and the two previous ones.\nTo have notation consistent with (A) and (B) we let yi = ti denote the time of the ith\nobservation; and define the likelihood f(yi | xti\u22121 ,xti) to be the probability density\n4\nthat the next observation after ti\u22121 is at time ti. This density can be obtained by\nintegrating\n\u03bd(Xs) exp\n{\n\u2212\n\u222b ti\nti\u22121\n\u03bd(Xs)ds\n}\n, (3)\nw.r.t. the distribution of (Xs, s \u2208 (ti\u22121, ti)) conditionally on Xti\u22121 = xti\u22121 ,Xti = xti .\nThe distribution of this conditioned process has a known density w.r.t. the Brownian\nbridge measure and it is given in Lemma 1 of Beskos et al. (2006b). We can thus\nshow that the density of interest is\n\u03bd(xti)Nti\u2212ti\u22121(xti \u2212 xti\u22121)\npti\u2212ti\u22121(xti | xti\u22121)\nexp{A(xti)\u2212A(xti\u22121)}E\n[\nexp\n{\n\u2212\n\u222b ti\nti\u22121\n(\u03c6(Ws) + \u03bd(Ws))ds\n}]\n,\n(4)\nwhere expectation is with respect to the law of a Brownian Bridge from xti\u22121 to xti .\nWe take a Bayesian approach, and assume a prior distribution for X0. Our interest\nlies in the online calculation of the filtering densities, the posterior densities of the signal\nat time ti given the observations up to time ti, for each 1 \u2264 i \u2264 n. While these densities\nare intractable, we propose a particle filter scheme to estimate recursively these densities\nat each observation time-point. As we point out in Section 6, our approach allows the\nestimation of the filtering distribution of the continuous time path (Xs, ti\u22121 < s < ti).\nA more flexible model for the signal is a diffusion process Z which solves a more general\nSDE than the one we have assumed in (1):\ndZs = b(Zs) ds+\u03a3(Zs)dBs , s \u2208 [0, t] . (5)\nIn contrast with (1), (5) allows the diffusion coefficient to be state-dependent. Our methods\ndirectly apply to all such processes provided there is an explicit transformation Zs 7\u2192\n\u03b7(Zs) =: Xs, where X solves an SDE of the type (1); the implied drift \u03b1 can be easily\nexpressed in terms of b and \u03a3 via Ito\u02c6\u2019s formula and it will have to satisfy the conditions we\nhave already specified. In model (A) the likelihood becomes f(yi | \u03b7\u22121(Xti)), in model (B)\nthe data are yi = \u03b6(\u03b7\n\u22121(Xti)) and in model (C) the Poisson intensity is \u03bd(\u03b7\n\u22121(Xs)), where\n\u03b7\u22121 denotes the inverse transformation. Therefore, the extension of our methodology\nto general diffusions is straightforward when d = 1; under mild conditions (5) can be\ntransformed to (1) by \u03b7(Zs) =\n\u222b Zs\nu\u2217 \u03a3(z)\n\u22121 dz, for some arbitrary u\u2217 in the state space of\nthe diffusion. Moreover, the drift of the transformed process will typically satisfy the three\nconditions we have specified. However, the extension is harder in higher dimensions. The\nnecessary transformation is more complicated when d > 1 and it might be intractable or\neven impossible (A\u0131\u00a8t-Sahalia, 2004). Even when such a transformation is explicit it might\nimply a drift for X which violates condition ii). Nevertheless, many physical systems can\nbe successfully modeled with diffusions which can be transformed to (1).\nOur particle filtering methods will be illustrated on two sets of simulated data:\n5\nExample 1: Sine diffusion observed with error. The signal satisfies\ndXs = sin(Xs)ds+ dBs , (6)\nand the data consist of noisy observations, yi \u223c N(Xti , \u03c32). Figure 1(top) shows a simula-\ntion of this model with \u03c3 = 0.2. In this case\n\u03c6(u) = (sin(u)2 + cos(u) + 1)\/2 . (7)\nThis process is closely related to Brownian motion on a circle. It is convenient as an\nillustrative example since discrete-time skeletons can be easily simulated from this process\nusing the most basic form of the Exact Algorithm (EA1 in Beskos et al., 2006a, R-code is\navailable on request by the authors).\nExample 2: OU-driven Cox Process. The second data set consists of the arrival times\nof a Poisson process, yi = ti, whose intensity is given by \u03bd(Xs), s \u2265 0, where\n\u03bd(x) = a+ \u03b2|x|,\nand X is an Ornstein-Uhlenbeck (OU) process,\ndXs = \u2212\u03c1Xsds+ dBs .\nThe OU process is stationary with Gaussian marginal distribution, N(0, 1\/(2\u03c1)). Thus,\nan interpretation for this model is that the excursions of X increase the Poisson intensity,\nwhereas a corresponds to the intensity when X is at its mean level. An example data set is\nshown in Figure 3; where we have taken a = 0, \u03b2 = 20, \u03c1 = 1\/2. Although the transition\ndensity of the OU process is well-known,\nXt | X0 = x0 \u223c N\n(\ne\u2212\u03c1tx0,\n1\n2\u03c1\n(1\u2212 e\u22122\u03c1t)\n)\n,\nthe observation density f(yi+1 | xti , xti+1) is intractable.\nExamples 1 and 2 are examples of observation regimes (A) and (C) respectively. We\nwill show that observation regime (B) can be handled in a similar fashion as (A), so we\nhave not included an accompanying example.\n3 Random weight particle filter\nAs in Section 2 we will denote the observation at time ti by yi, and pt(\u00b7 | \u00b7) will denote the\nsystem transition density over time t (see Equation 2). We will write \u2206i = ti+1 \u2212 ti, and\nthe filtering densities p(xti|y1:i) will be denoted by pii(xti), where by standard convention\ny1:i = (y1, . . . , yi). To simplify notation, when we introduce weighted particles below, we\nwill subscript both particles and weights by i rather than ti.\n6\nOur aim is to recursively calculate the filtering densities pii(xti). Basic probability\ncalculations yield the following standard filtering recursion for these densities\npii+1(xti+1) \u221d\n\u222b\nf(yi+1|xti ,xti+1)p\u2206i(xti+1|xti)pii(xti)dxti . (8)\nParticle filters approximate pii(xti) by a discrete distribution, denoted by p\u02c6ii(xti), whose sup-\nport is a set of N particles, {x(j)i }Nj=1, with associated probability weight {w(j)i }Nj=1. Substi-\ntuting p\u02c6ii(xti) for pii(xti) in (8), yields a (continuous density) approximation to pii+1(xti+1),\np\u02dcii+1(xti+1) \u221d\nN\u2211\nj=1\nw\n(j)\ni f(yi+1|x(j)i ,xti+1)p\u2206i(xti+1|x(j)i ). (9)\nThe aim of one iteration of the particle filter algorithm is to construct a further particle\n(discrete distribution) approximation to p\u02dcii+1(xti+1).\nWe can obtain such a particle approximation via importance sampling, and a general\nframework for achieving this is given by the auxiliary particle filter of Pitt and Shephard\n(1999). We choose a proposal density of the form\nN\u2211\nj=1\n\u03b2\n(j)\ni q(xti+1|x(j)i , yti+1) . (10)\nChoice of suitable proposals, i.e. choice of the \u03b2\n(j)\ni s and q, is discussed in the analysis of\nour specific applications in Section 5.\nTo simulate a new particle at time ti+1 we (a) simulate a particle x\n(k)\ni at time i, where\nk is a realisation of a discrete random variable which takes the value j \u2208 {1, 2, . . . , N} with\nprobability \u03b2\n(j)\ni ; and (b) simulate a new particle at time ti+1 from q(xti+1|x(k)i , yi+1). The\nweight assigned to this pair of particles (x\n(k)\ni ,xti+1) is proportional to\nw\n(k)\ni f(yi+1|x(k)i ,xti+1)p\u2206i(xti+1|x(k)i )\n\u03b2\n(k)\ni q(xti+1|x(k)i , yi+1)\n. (11)\nThis is repeatedN times to produce the set of weighted particles at time ti+1,\n{\n(x\n(j)\ni+1, w\n(j)\ni+1)\n}N\nj=1\n,\nwhich gives an importance sampling approximation to pii+1(xti+1). Renormalising the\nweights is possible but does not materially affect the methodology or its accuracy. Im-\nprovements on independent sampling in step (a) can be made: see the stratified sampling\nideas of Carpenter et al. (1999). The resulting particle filter has good theoretical properties\nincluding consistency (Crisan, 2001) and central limit theorems for estimates of posterior\nmoments (Del Moral and Miclo, 2000; Chopin, 2004; Ku\u00a8nsch, 2005), as N \u2192 \u221e. Un-\nder conditions relating to exponential forgetting of initial conditions, particle filter errors\nstabilise as n\u2192\u221e (Del Moral and Guionnet, 2001; Ku\u00a8nsch, 2005).\n7\nThe difficulty with implementing such a particle filter when the signal X is a diffusion\nprocess is that the transition density p\u2206i(xti+1|x(k)i ) which appears in (11) is intractable for\nmost diffusions of interest, due to the expectation term in (2). Furthermore, for observation\nmodel (C) (but also for more general models), the likelihood term f(yi+1|x(k)i ,xti+1) given\nin (4) cannot be calculated analytically.\nWe circumvent these problems by assigning each new particle a random weight which is\na realisation of a random variable whose mean is (11). The construction and simulation of\nthis random variable is developed in Section 4, and it is based on the particular expression\nfor the transition density in (2). The replacement of the weights by positive unbiased\nestimators is an interesting possibility in more general contexts than the one considered\nin this paper. Indeed, in Section 3.2 we show that this approach amounts to a convenient\naugmentation of the state with auxiliary variables.\n3.1 Simulation of weights\nIn all models the weight associated with the pair (x\n(k)\ni ,xti+1) equals\nhi+1(x\n(k)\ni ,xti+1 , yi+1)\u00b5g(x\n(k)\ni ,xti+1 , ti, ti+1) (12)\nwhere hi+1 is a known function, and for 0 < u < t,\n\u00b5g(x, z, u, t) := E\n[\nexp\n{\n\u2212\n\u222b t\nu\ng(Ws)ds\n}]\n,\nwhere the expectation is taken w.r.t. a d-dimensional Brownian bridgeW, starting at time\nu from Wu = x and finishing at time t at Wt = z.\nModels (A) and (B) : For these model types\nhi+1(x\n(k)\ni ,xti+1 , yi+1) =\nw\n(k)\ni f(yi+1|xti+1)N\u2206i(xti+1 \u2212 x(k)i ) exp{A(xti+1)\u2212 A(x(k)i )}\n\u03b2\n(k)\ni q(xti+1|x(k)i , yi+1)\n,\nand g = \u03c6. In model type (B) the proposal distribution q(xti+1|x(k)i , yi+1) should be chosen\nto propose only values of xti+1 such that \u03b6(xti+1) = yi+1; then f(yi+1|xti+1) = 1.\nModel (C): A synthesis of (2), (4) and (11), with g = \u03c6+ \u03bd gives\nhi+1(x\n(k)\ni ,xti+1 , yi+1) =\nw\n(k)\ni \u03bd(xti+1)N\u2206i(xti+1 \u2212 x(k)i ) exp{A(xti+1)\u2212 A(x(k)i )}\n\u03b2\n(k)\ni q(xti+1|x(k)i , yi+1)\n,\nSection 4 shows how to construct for each pair of (x, z) and times (u, t), with u < t, ad-\nditional auxiliary variables V, and a function r(v,x, z, u, t) \u2265 0, with the property that\n8\nE[r(V,x, z, u, t) | x, z] = \u00b5g(x, z, u, t). The auxiliary variables are simulated according to\nan appropriate conditional distribution Qg( \u00b7 | x, z, u, t), and r is easy to evaluate. Our\nmethod replaces in the weight the intractable term \u00b5g with its unbiased estimator r.\nRandom Weight Particle Filter (RWPF)\nPF0 Simulate a sample x\n(1)\n0 , . . . ,x\n(N)\n0 from p(x0), and set w\n(j)\n0 = 1\/N .\nFor i = 0, . . . , n\u2212 1, for j = 1, . . . , N :\nPF1 calculate the effective sample size of the {\u03b2(k)i }, ESS = (\n\u2211N\nk=1(\u03b2\n(k)\ni )\n2)\u22121; if ESS < C,\nfor some fixed constant C, simulate ki,j from p(k) = \u03b2\n(k)\ni , k = 1, . . . , N and set\n\u03b4\n(j)\ni+1 = 1; otherwise set ki,j = j and \u03b4\n(j)\ni+1 = \u03b2\n(j)\ni ;\nPF2 simulate x\n(j)\ni+1 from q(xti+1|x(ki,j)i , yi+1);\nPF3 simulate vi+1 \u223c Qg( \u00b7 | x(ki,j)i ,xti+1 , ti, ti+1);\nPF4 assign particle x\n(j)\ni+1 a weight\nw\n(j)\ni+1 = \u03b4\n(j)\ni+1hi+1(x\n(ki,j)\ni ,x\n(j)\ni+1, yi+1)r(vi+1,x\n(ki,j)\ni ,xti+1 , ti, ti+1) . (13)\nNotice that this algorithm contains a decision as to whether or not resample particles\nbefore propagation in step PF1, with decision being based on the ESS of the \u03b2\n(j)\ni . The\nconstant C can be interpreted as the minimum acceptable effective sample size. (See Liu\nand Chen 1998 for the rationale of basing resampling on such a condition.) Whether or\nnot resampling occurs will affect the weight given to the new sets of particles, and this is\naccounted for by different values of \u03b4\n(j)\ni+1 in PF1. Optimally, the resampling for step PF1\nwill incorporate dependence across the N samples; for example the stratified sampling\nscheme of Carpenter et al. (1999) or the residual sampling of Liu and Chen (1998).\n3.2 An equivalent formulation via an augmentation of the state\nIn the previous section we described a generic sequential Monte Carlo scheme where the\nexact weights in the importance sampling approximation of the filtering distributions are\nreplaced by positive unbiased estimators. We now show that this scheme is equivalent to\napplying an ordinary auxiliary particle filter to a model with richer latent structure. We\ndemonstrate this equivalent representation for model types (A) and (B), since an obvious\nmodification of the argument establishes the equivalence for model type (C).\nAccording to our construction, conditionally on Xti , Xti+1 , ti and ti+1, Vi+1 is inde-\npendent of Vj and Xtj for any j different from i, i+ 1. Additionally, it follows easily from\nthe unbiasedness and positivity of r that, conditionally on Xti = x, r(vi+1,x,xti+1 , ti, ti+1)\n9\nis a probability density function for (Xti+1 ,Vi+1) with respect to the product measure\nLeb(dz)\u00d7Qg(dv | x, z, ti, ti+1), where Leb denotes the Lebesgue measure.\nConsider now an alternative discrete-time model with unobserved states (Zi,Vi), i =\n1, . . . , n, Zi \u2208 Rd, with a non-homogeneous Markov transition density\npi+1(zi+1,vi+1 | zi,vi) = r(vi+1, zi, zi+1, ti, ti+1) ,\n(this density is with respect to Leb \u00d7 Qg) and observed data yi with observation density\nf(yi+1 | zi, zi+1). By construction the marginal filtering distributions of Zi in this model\nare precisely pii(xti), i.e. the filtering densities in (8). Consider an auxiliary particle filter\napplied to this model where we choose with probability \u03b2\n(j)\ni each of the existing particles\n(z\n(j)\ni ,v\n(j)\ni ), and generate new particles according to the following proposal\n(zi+1,vi+1) \u223c q(zi+1 | z(k)i , yi+1)Qg(dvi+1 | z(k)i , zi+1, ti, ti+1)Leb(dzi+1) ,\nwhere q is the same proposal density as in (10). The weights associated with each particle\nin this discrete-time model are tractable and are given by (13). Therefore, the weighted\nsample\n{\n(z\n(j)\ni+1, w\n(j)\ni+1)\n}N\nj=1\nis precisely a particle approximation to pii+1(xti+1), and RWPF is\nequivalent to an auxiliary particle filter on this discrete-time model whose latent structure\nhas been augmented with the auxiliary variables Vi.\nThis equivalent representation sheds light on many aspects of our method. Firstly, it\nmakes it obvious that it is inefficient to average more than one realization of the positive\nunbiased estimator of \u00b5g per particle. Instead it is more efficient to generate more particles\nwith only one realization of the estimator simulated for each pair of particles.\nSecondly, it illustrates that RWPF combines the advantages of the bootstrap and the\nauxiliary particle filter. Although it is easy to simulate from the probability distribution\nQg (as described in Section 4), it is very difficult to derive its density. (with respect to an\nappropriate reference measure). Since theVis are propagated according to this measure, its\ncalculation is avoided. This is an appealing feature of the bootstrap filter which propagates\nparticles without requiring analytically the system transition density. On the other hand\nthe propagation of the Zis is done via a user-specified density which incorporates the\ninformation in the data.\nThirdly, it suggests that the RWPF will have similar theoretical properties with auxil-\niary particle filters applied to discrete-time models. This is explored in Section 3.3.\n3.3 Theoretical properties\nConsider estimation of the posterior mean of some function \u03d5 of the state at time ti,\nE[\u03d5(xti)|y1:i]. A natural approach to the investigation of particle filter effectiveness is to\nconsider the limiting behaviour of the algorithm as N \u2192 \u221e. For the standard auxiliary\n10\nparticle filter, Chopin (2004) introduces a central limit theorem (CLT) for estimation of\nthis type of expectations. This CLT applies directly to both EPPF and the ESPF.\nIn Appendix F we extend the result of Chopin (2004) and give a further necessary\ncondition on the random weights in RWPF under which a CLT still holds. This extra\ncondition is (C2). The expression for the variance of the estimator of E[\u03d5(xti)|y1:i] obtained\nwith RWPF differs from the expression in the standard case (i.e. when the weights are\nknown) by an extra term caused by the randomness in the weights (see Equations 27\u201329\nand the comment on Theorem 3 in Appendix F for further details). The ready adaptation\nof Chopin\u2019s approach is facilitated by the observation that the RWPF can be re-expressed\nas a standard particle filter for the an augmented state (see Section 3.2).\nOne important consequence of this CLT is that the errors in estimating E[\u03d5(xti)|y1:i]\nare of order N\u22121\/2. Previous filtering methods for the diffusion problems we consider are\nbased on (i) discretising time and introducing M intermediate time points between each\nobservation time; (ii) using an Euler, or higher order, approximation to the diffusion (1);\nand (iii) applying a particle, or other, filter to this approximate discrete time model. See\nfor example Crisan et al. (1999). Results giving the order of the errors in one such scheme\nare given by Del Moral et al. (2001). For Models such as (A) and (B) the errors are of order\nN\u22121\/2 provided that the number of intermediate time steps M between each observation\nincreases at a rate N1\/2. Thus for fixed computational cost K \u221d MN the errors decrease\nat a rate K\u22121\/3. For models such as (C), where the likelihood depends on the path of the\nstate between two successive observations, the rate at which errors decrease will be slower,\nfor example K\u22121\/4 (Del Moral et al., 2001), or K\u22121\/6 (Theorem 1.1 of Crisan et al., 1999).\n4 Generalised Poisson Estimators\nWe have already motivated the need for the simulation of a positive unbiased estimator of\nE [E] where E =: exp\n{\n\u2212\n\u222b t\n0\ng(Ws)ds\n}\n, (14)\nwhere the expectation is taken w.r.t. a d-dimensional Brownian bridge W. In this section\nwe introduce a methodology for deriving such estimators, and provide theoretical and\nsimulation results regarding the variance of the suggested estimators. These results are of\nindependent interest beyond particle filtering, so we present our methodology in a general\nway, where g is an arbitrary function assumed only to be continuous on Rd. We assume\nthat W0 = x and Wt = z, for arbitrary x, z \u2208 Rd and t > 0. By the time-homogeneity\nproperty of the Brownian bridge our methodology extends to the case where the integration\nlimits change to u and u+ t, for any u > 0.\nBeskos et al. (2006b) proposed an unbiased estimator of (14), the Poisson Estimator:\nPE: e(\u03bb\u2212c)t\u03bb\u2212\u03ba\n\u03ba\u220f\nj=1\n[\nc\u2212 g(W\n\u03c8j\n)\n]\n; (15)\n11\n\u03ba is a Poisson random variable with mean \u03bbt, the \u03c8js are uniformly distributed on [0, t],\nand c \u2208 R, \u03bb > 0 are arbitrary constants. (Here and below we assume that the empty\nproduct, i.e. when \u03ba = 0, takes the value 1.) The two main weaknesses of the PE are\nthat it may return negative estimates and that its variance is not guaranteed to be finite.\nBoth of these problems are alleviated when g is bounded. However this is a very restrictive\nassumption in our context. Therefore, here, we introduce a collection of unbiased and\npositive estimators of (14) which generalise the PE. The methods we consider allow c and\n\u03bb depend on W, and permit \u03ba to have a general discrete distribution. Firstly, we need to\nbe able to simulate random variables LW and UW with\nLW \u2264 g(Ws) \u2264 UW, for all s \u2208 [0, t], (16)\nand to be able to simulate Ws at any s, given the condition implied by (16). For un-\nbounded g this is non-trivial. However, both of these simulations have become feasible\nsince the introduction of an efficient algorithm in Beskos et al. (2005b). An outline of the\nconstruction is given in Appendix A.\nLet UW and LW satisfy (16) and \u03c8j, j \u2265 1, be a sequence of independent uniform\nrandom variables on [0, t]. Then, (14) can be re-expressed as follows,\nE\n[\ne\u2212UWt exp\n{\u222b t\n0\n(UW \u2212 g(Ws))ds\n}]\n= E\n[\ne\u2212UWt\n\u221e\u2211\nk=0\n1\nk!\n(\u222b t\n0\n(UW \u2212 g(Ws))ds\n)k]\n= E\n[\ne\u2212UWtE\n[ \u221e\u2211\nk=0\ntk\nk!\nk\u220f\nj=1\n(UW \u2212 g(W\u03c8j)) | UW, LW\n]]\n= E\n[\ne\u2212UWt\nt\u03ba\n\u03ba!p(\u03ba | UW, LW)\n\u03ba\u220f\nj=1\n(UW \u2212 g(W\u03c8j))\n]\n,\n(17)\nwhere \u03ba is a discrete random variable with conditional probabilities P[\u03ba = k | UW, LW] =\np(k | UW, LW). The second equality in the above argument is obtained using dominated\nconvergence and Fubini\u2019s theorem (which hold by positivity of the summands).\nWe can derive various estimators of (14) by specifying p(\u00b7 | UW, LW). The family of all\nsuch estimators will be called the Generalised Poisson Estimator (GPE):\nGPE: e\u2212UWt\nt\u03ba\n\u03ba!p(\u03ba | UW, LW)\n\u03ba\u220f\nj=1\n(UW \u2212 g(W\u03c8j)) . (18)\nThe following Theorem (proved in Appendix B) gives the optimal choice for p(\u00b7 | UW, LW).\n12\nTheorem 1. The conditional second moment of the Generalised Poisson Estimator given\nUW and LW, is:\ne\u22122UWt\n\u221e\u2211\nk=0\ntk\np(k | UW, LW)k!2E\n[(\u222b t\n0\n(UW \u2212 g(Ws))2ds\n)k\n| UW, LW\n]\n. (19)\nIf\n\u221e\u2211\nk=0\ntk\/2\nk!\nE\n[(\u222b t\n0\n(UW \u2212 g(Ws))2ds\n)k\n| UW, LW\n]1\/2\n<\u221e , (20)\nthen the second moment is minimised by the choice\np(k | UW, LW) \u221d t\nk\/2\nk!\nE\n[(\u222b t\n0\n(UW \u2212 g(Ws))2ds\n)k\n| UW, LW\n]1\/2\n, (21)\nwith minimum second moment given by\uf8eb\uf8ede\u2212UWt \u221e\u2211\nk=0\ntk\/2\nk!\nE\n[(\u222b t\n0\n(UW \u2212 g(Ws))2ds\n)k\n| UW, LW\n]1\/2\uf8f6\uf8f82 <\u221e , for almost all UW, LW .\n(22)\nWhilst the right-hand side of (21) cannot be evaluated analytically, it can guide a suitable\nchoice of p(\u00b7 | UW, LW). If W were known, the optimal proposal is Poisson with mean\n\u03bbW :=\n(\nt\n\u222b t\n0\n(UW \u2212 g(Ws))2ds\n)1\/2\n. (23)\nWe will discuss two possible ways that (23) can be used to choose a good proposal.\nA conservative approach takes p(\u00b7 | UW, LW) to be Poisson with mean (UW\u2212LW)t (an\nupper bound of \u03bbW). We call this estimator GPE-1. An advantage of GPE-1 is that its\nsecond moment is bounded above by E[e\u22122LWt]. Thus, under mild and explicit conditions\non g, which are contained in the following theorem (proved in Appendix C), the variance\nof the estimator is guaranteed to be finite.\nTheorem 2. A sufficient condition for GPE-1 to have finite variance is that\ng(u1, . . . , ud) \u2265 \u2212\u03b4\nd\u2211\ni=1\n(1 + |ui|), for all ui \u2208 R, 1 \u2264 i \u2264 d, \u03b4 \u2265 0.\n13\nSince \u03bbW is stochastic, an alternative approach is to introduce a (exogenous) random\nmean and assume that p(\u00b7|UW, LW) is Poisson with this random mean. For tractability we\nchoose the random mean to have a Gamma distribution, when p(\u00b7 | UW, LW) becomes a\nnegative-binomial distribution:\nGPE-2: e\u2212UWt\nt\u03ba\u0393(\u03b2)(\u03b2 + \u03b3\nW\n)\u03b2+\u03ba\n\u0393(\u03b2 + \u03ba)\u03b2\u03b2\u03b3\u03ba\nW\n\u03ba\u220f\nj=1\n[\nUW \u2212 g(W\u03c8j )\n]\n, (24)\nwhere \u03b3\nW\nand \u03b2 denote the mean and the dispersion parameter respectively of the negative\nbinomial. Since the negative-binomial has heavier tails than the Poisson Estimator, GPE-\n2 will have finite variance whenever there exists a PE with finite variance. On the other\nhand, big efficiency gains can be achieved if \u03b3\nW\nis chosen to be approximately E[\u03bbW |\nUW, LW]. There is a variety of ad-hoc methods which can provide a rough estimation of\nthis expectation. Applying Jensen\u2019s inequality to exchange the integration with the square\npower in (23), and subsequently approximating E[ g(Ws) | UW, LW ] by g(E[Ws]), suggests\ntaking\n\u03b3\nW\n= tUW \u2212\n\u222b t\n0\ng\n(\nx\nt\u2212 s\nt\n+ y\ns\nt\n)\nds > 0 . (25)\nA simulation study (part of which is presented in Section 4.1 below) reveals that this choice\nworks very well in practice and the GPE-2 has up to several orders of magnitude smaller\nvariance than the PE or the GPE-1. The integral can usually be easily evaluated, otherwise\na crude approximation can be used.\nWe have confined our presentation to the case where the expectation in (14) is w.r.t.\nthe Brownian bridge measure. Nevertheless, as pointed out in Beskos et al. (2006b) the\nPE can be constructed in exactly the same way when the expectation is taken w.r.t. an\narbitrary diffusion bridge measure, as long as exact skeletons can be simulated from this\nmeasure. The GPE can also be implemented in this wider framework, provided that the\nprocess W can be constructed to satisfy (16).\n4.1 Simulation study\nWe consider a smooth bounded test function g(u) = (sin(u)2+cos(u)+1)\/2. This has been\nchosen in view of Example 1. The function g is periodic, with period 2pi. In [0, 2pi] it has\nlocal minima at 0 and 2pi, global minimum at pi and maxima at pi\/3 and 5pi\/3. Since g is\nbounded by 9\/8 we can construct a PE which returns positive estimates by setting c \u2265 9\/8.\nUnder this constraint, Beskos et al. (2006b) argued that a good choice is c = \u03bb = 9\/8.\nSimulation experiments suggested that the performance of the GPE-2 is quite robust to the\nchoice of the dispersion parameter \u03b2. We have fixed it in our examples to \u03b2 = 10. Table\n1 summarizes estimates of the variance of the estimators based on 104 simulated values.\nWe see that GPE-2 can be significantly more efficient than PE, in particular when taking\ninto account E[\u03ba]. In general, the performance of PE is sensitive to the choice of c and \u03bb.\n14\nEstimator x = 0, z = 0 x = 0, z = pi x = pi, z = pi\nvariance PE 0.202 0.200 0.027\nGPE-1 4.21\u00d7 10\u22123 0.208 0.034\nGPE-2 2.08\u00d7 10\u22123 0.220 0.033\nVar(E) 3.74\u00d7 10\u22125 3.27\u00d7 10\u22123 4.72\u00d7 10\u22123\nE[\u03ba] PE 1.118 1.126 1.121\nGPE-1 0.130 1.091 0.744\nGPE-2 0.119 0.329 0.735\nTable 1: Monte Carlo estimates of the variance of four estimators of (14) where g(u) =\n(sin(u)2 + cos(u) + 1)\/2. For comparison we give also var(E). We also report an estimate\nof E[\u03ba]. We consider three different pairs of starting and ending points (x, z) and time\nincrement t = 1. The estimates in the table were obtained from a sample of 104 realisations.\nGPE-1 is typically less efficient than GPE-2. Table 1 also gives the value of Var(E) which\ntakes significantly smaller values (by a couple of orders of magnitude) than any of PE,\nGPE-1 or GPE-2, illustrating the efficiency cost of these auxiliary variable constructions\nin absolute terms.\nWe have also investigated how the efficiency of the PE and GPE-2 varies with the\ntime increment t and in particular for small t (results not shown). These empirical results\nsuggest that the coefficient of variation of the errors of both PE and GPE-2 are O(t\u03b4) for\nsome \u03b4 > 0; but that the value of \u03b4 differs for the two estimators. In the cases that we\ninvestigated, the GPE-2 appears to have a faster rate of convergence than PE.\nThe results of this simulation study have been verified for other functions g (results not\nshown). We have experimented with differentiable (e.g. g(u) = u) and non-differentiable\n(e.g. g(u) = |u|) unbounded functions. In these cases it is impossible to design a PE\nwhich returns positive estimates w.p.1. Again, we have found that the GPE-2 performs\nsignificantly better than the PE.\nIt is important to mention that alternative Monte Carlo methods exist which yield\nconsistent but biased estimates of (14). One such estimator is obtained by replacing the\ntime-integral in (14) with a Riemann approximation based on a number,M say, of interme-\ndiate points. This technique is used to construct a transition density estimator in Nicolau\n(2002) and effectively underlies the transition density estimator of Durham and Gallant\n(2002) (when the diffusion process has constant diffusion coefficient). The approach of\nDurham and Gallant (2002) has been used in MCMC and filtering applications (Golightly\nand Wilkinson, 2006; Chib et al., 2006; Ionides, 2003). In the filtering context it provides\nan alternative to RWPF, where the weights are approximated. It is not the purpose of\nthis paper to carry out a careful comparison of RWPF with such variants. However, as\nan illustration we present a very small scale comparison in the context of estimating the\ntransition density, pt(z | x), of (6) for t = 1 and x, z as in Table 1. We compare 4 meth-\n15\nEstimator x = 0, z = 0 x = 0, z = pi x = pi, z = pi\nPE 1.25 0.93 0.17\nGPE-2 0.13 0.78 0.2\nDG-1 0.5 0.45 0.3\nDG-5 0.28 0.19 0.22\nTable 2: Monte Carlo estimates based on 104 realisations of the root mean square error\ndivided by the true value of 4 estimators of pt(z | x), of (6) for t = 1 and various x, z. As\ntrue value we take the estimate produced by averaging the estimations given by GPE-2.\nThe number of intermediate points used for each estimator are 1 and 5 for DG-1 and DG-5\nrespectively; the number of Brownian bridge simulations for PE and GPE-2 are given in\nTable 1 (E[\u03ba]).\nods. Two are based on (2) and use the PE and the GPE-2 to generate estimators of the\nexpectation. The other two, DG-1 and DG-5 are two implementation of the Durham and\nGallant (2002) estimator, with 1 and 5 respectively intermediate points. We compare the\nmethods in terms of their root mean square error divided by the true value (i.e. the coef-\nficient of variation). As the true value we used the estimate of the GPE-2. The results of\nthe comparison are presented in Table 2. Notice that DG-1 and DG-5 simulate many more\nvariables than GPE-2 to construct their estimates.\n5 Comparison of particle filters on the simulated data\nWe now demonstrate the performance of the different particle filters we have presented on\nthe two examples introduced in Section 2.\n5.1 Analysis of the sine diffusion\nWe first consider analysing the sine diffusion of Example 1. The simulated data is shown\nin Figure 1(top). We compare four implementations of the particle filter each of which\navoids time-discretisations by using methodology based on the Exact Algorithm (EA) for\nsimulating diffusions: i) EPPF, which uses EA for implementing a bootstrap filter, ii)\nESPF, which adapts EA to simulate by rejection sampling from the filtering densities, iii)\nRWPF1, an implementation of RWPF using PE (see Table 1) to simulate the weights, iv)\nRWPF2, an implementation of RWPF using GPE-2 to simulate the weights. Details on\nthe implementation of EPPF and ESPF are given in Appendix D.\nIn this simple example ESPF is more efficient than EPPF, since it has the same compu-\ntational cost, but it is proposing from the optimal proposal distribution. However, we have\nefficiently implemented ESPF exploiting several niceties of this simple model, in particular\nthe Gaussian likelihood and the fact that the drift is bounded. In more general models\n16\n0 20 40 60 80 100\n\u2212\n1\n0\n1\n2\n3\n4\n5\n(a)\nTime\nX _\nt\n0 20 40 60 80 100\n\u2212\n0 .\n5\n0 .\n0\n0 .\n5\n(b)\nTime\nR\ne s\ni d\nu a\nl\nFigure 1: Top: A realisation of the sine diffusion (black line) on [0, 100]; 100 observations\nat unit time intervals (blue circles); mean of the filtering distribution of the diffusion at the\nobservation times obtained by RWPF2 with N = 1, 000 particles (red circles). Bottom: the\ndifference between observed data and filtered means (circles) and 90% credible intervals\n(red dashed lines) from RWPF2. (Whilst for clarity they are shown for all times, the\ncredible intervals were only calculated at the observation times.)\nimplementation of ESPF can be considerably harder and its comparison with EPPF less\nfavorable due to smaller acceptance probabilities.\nIn this context where \u03c6 is bounded one can speed up the implementation of GPE-2 with\npractically no loss of efficiency by replacing UW in (24) and (25) by 9\/8 which is the upper\nbound of \u03c6. In this case, there is no need to simulate UW and LW. We have implemented\nthis simplification in the RWPF2.\nAlgorithms EPPF, RWPF1-2 used the stratified re-sampling algorithm of Carpenter\net al. (1999), with re-sampling at every iteration. For RWPF1-2 we chose the proposal\ndistribution for the new particles based on the optimal proposal distribution obtained if\nthe sine diffusion is approximated by the Ozaki discretisation scheme (details in Appendix\nE). For EPPF we chose the \u03b2\n(k)\ni s to be those obtained from this approximation.\nThe number of particles used in each algorithm was set so that each filter had com-\nparable CPU cost, which resulted in 500, 500, 910 and 1000 particles used respectively\nfor each algorithm. For these numbers of particles, EPPF and ESPF on average required\n17\n0 20 40 60 80 100\n0 .\n0\n0 .\n2\n0 .\n4\n0 .\n6\n0 .\n8\n1 .\n0\n1 .\n2\ntime\nR\ne l\na t\ni v\ne  \nE f\nf i c\ni e\nn c\ny\nFigure 2: Relative efficiency of the 4 particle filter algorithms at estimating the filtering\nmean E[Xti|y1:i]. Each line gives the relative efficiency of one algorithm compared to\nRWPF2 (black: RWPF2, green: RWPF1, red: ESPF, blue: EPPF). See text for details.\nthe proposal of 1360 particles and required 675 Brownian bridge simulations within the\naccept-reject step (iii) at each iteration of the algorithm. By comparison RWPF1 and\nRWPF2 simulated respectively 910 and 1000 particles and required on average 1025 and\n850 Brownian bridge simulations to generate the random weights at each iteration.\nNote that the comparative CPU cost of the four algorithms, and in particular that\nof EPPF and ESPF as compared to RWPF1-2 depends on the underlying diffusion path.\nThe acceptance probabilities within EPPF and ESPF depend on the values of x\nki,j\ni and\nxti+1 , and get small when both these values are close to 0(mod 2pi). (in the long run the\ndiffusion will visit these regions infrequently and will stay there for short periods.) Thus,\nsimulated paths which spent more (or less) time in this region of the state-space would\nresult in EPPF and ESPF having a larger (respectively smaller) CPU cost.\nWe compared the four filters based on the variability of estimates of the mean of the\nfiltering distribution of the state across 500 independent runs of each filter. Results are\ngiven in Figure 2, while output from one run of RWPF2 is shown in Figure 1. The com-\nparative results in Figure 2 are for estimating the mean of the filtering distribution at each\niteration (similar results were obtained for various quantiles of the filtering distribution).\nThey show RWPF2 performing best with an average efficiency gain of 15% over RWPF1,\n50% over ESPF and 200% over EPPF. Interpretation of these results suggest that (for\nexample) ESPF would be required to run with N = 750 (taking 1.5 times the CPU cost\nfor this data set) to obtain comparable accuracy with RWPF2.\nVarying the parameters of the model and implementation of the algorithms will affect\n18\nthe relative performance of the algorithms. In particular increasing (or decreasing) \u03c32,\nthe variance of the measurement error, will increase (respectively decrease) the relative\nefficiency of EPPF relative to the other filters. Similar results occur as \u2206i is decreased\n(respectively increased). The relative performance of the other three algorithms appears to\nbe more robust to such changes. We considered implementing EPPF with \u03b2\n(k)\ni = w\n(k)\ni ; and\nalso using an Euler rather than an Ozaki approximation of the sine diffusion to construct\nthe proposal distribution for RWPF1-2, but neither of these changes had any noticeable\neffect on the performance of the methods. We also considered re-sampling less often, setting\nC = N\/4 in step PF1 of the RWPF algorithm (so re-sampling when the effective sample\nsize of the \u03b2\n(j)\ni s was less than N\/4) and this reduced the performance of the algorithms\nsubstantially (by a factor of 2 for RWPF1-2).\nWe also investigated the effect of increasing the amount of time, \u2206, between observa-\ntions. To do this we used the above data taking (i) every 10th; or (ii) every 20th time-point.\nTo measure the performance of the filter for these different scenarios we used the Effec-\ntive Sample Size (ESS) of Carpenter et al. (1999). ESS is calculated based on the variance\nof estimates of posterior means across independent runs of the filter, but this variance is\ncompared to the posterior variance to give some measure of how many independent draws\nfrom the posterior would produce estimators of the same level of accuracy. We focus on\nestimates of the posterior mean of the state at observation times; and if s2 is the sample\nvariance of the particle filter\u2019s estimate of E[Xti|y1:i] across 100 independent runs, and \u03c3\u02c62\nis an estimate of Var[Xti|y1:i], then the ESS is \u03c3\u02c62\/s2. Note that comparing filters by their\nESS is equivalent to comparing filters based on the variance of the estimators.\nTable 3 gives ESS values for the different values of \u2206. We see that the ESS values\ndrops dramatically as \u2206 increases, and the filter is inefficient for \u2206 = 20. This drop\nin performance is due to the large variability of the random weights in this case. The\nvariability of these weights is due to (a) the variability of\nexp\n{\n\u2212\n\u222b ti+1\nti\ng(Ws)ds\n}\n, (26)\nacross different diffusion paths; and (b) the Monte Carlo variability in estimating this for a\ngiven path. To evaluate what amount is due to (a), we tried a particle filter that estimates\n(26) numerically by simulating the Brownian Bridge at a set of discrete time points (for\nthis example we sampled values every 1\/2 time unit) and then using these to numerically\nevaluate the integral. This approach is closely related to the importance sampling approach\nof Durham and Gallant (2002); Nicolau (2002), see Section 4.1. The results for this filter\nare also given in Table 3 (note the ESS values ignore any bias introduced through this\nnumerical approximation), and we again see small ESS values, particularly for \u2206 = 20.\nThis filter\u2019s performance is very similar to the RWPF, which suggests that the Monte Carlo\nvariability in (b) is a small contributor to the poor performance of the RWPF in this case.\nFinally we tried introducing pseudo observations at all integer time-intervals where cur-\nrently no observation is made. The RWPF is then run as above, but with no likelihood\n19\nFilter \u2206 = 10 \u2206 = 20\nRWPF2 73 5\nDiscretisation 80 12\npseudoRWPF2 923 933\nTable 3: Comparison of filter\u2019s mean ESS values for different time intervals between obser-\nvations (\u2206). Results are for the Random Weight Particle Filter using GPE-2 (RWPF2), a\nfilter that numerically approximates the weight through discretising the diffusion process\n(Discretisation), and the RWPF after introducing uninformative observations at unit time\nintervals (pseudoRWPF2).\ncontribution to the weight at the time-points where there are these uninformative obser-\nvations. The idea is that now \u2206 = 1, so that the variance of the random weights is well-\nbehaved, but we still have adaptation of the path of the diffusion in the unit time-interval\nprior to an observation to take account of the information in that observation. Results are\nagain shown in Table 3, and the ESS values are very high (and close to the optimal value,\nthat of the number of particles, 1000). Note that the computational cost is only roughly\ndoubled by adding these extra pseudo observations; as the total computational cost for\nthe simulation of the Brownian bridge is unchanged. These results are reasonably robust\nto the choice of how frequently to introduce these uninformative observations (results not\nshown).\n5.2 Analysis of the Cox process\nWe now consider applying the random weight particle filter (RWPF) to Example 2 from\nSection 2, the OU-driven Cox process. The data we analysed is given in Figure 3(top).\nIt is either impossible or difficult to adapt the other two EA-based particle filters (the\nEPPF and the ASPF) to this problem. For instance we cannot implement EPPF as the\nlikelihood function is not tractable. As such we just focus on the efficiency of the RWPF\nin estimating the filtering distribution of |Xt|.\nOur implementation of the RWPF was based on proposing particles from the prior\ndistribution, so \u03b2\n(k)\ni = w\n(k)\ni and q(xti+1|xji , yi+1) is just the OU transition density p(xti+1|xji ).\nWe simulated the random weights by GPE-2. We calculated the filtering density at each\nobservation time, and also at 56 pseudo-observation times chosen so that the maximum\ntime difference between two consecutive times for which we calculated the filtering density\nwas 0.1. This was necessary to avoid the number of Brownian bridge simulations required\nto simulate the weights being too large for long inter-observation times, and also to control\nthe variance of the random weights (see above). The likelihood function for these non-\nobservation times is obtained by removing \u03bd(xti) from (4).\nWe set the number of particles to 1, 000 and resampled when the ESS of the \u03b2\n(j)\ni s was\n20\n0 2 4 6 8 10\n0 .\n0\n1 .\n0\n2 .\n0\n3 .\n0\n(a)\nTime\n0 2 4 6 8 10\n2 0\n0\n6 0\n0\n1 0\n0 0\n(b)\ntimes\nE S\nS B\nB 1\n. 1\nFigure 3: Top: Simulation from the Cox process of Example 2 and results from anal-\nysis by the RWPF. The path of the absolute of the underlying diffusion (black line);\nobserved arrival times (green dashes); filter estimates from the RWPF (red circles); and\n90% credible interval for the absolute of the diffusion (red dashed line). (Whilst for clarity\nthey are shown for all time, the credible intervals were only calculated at and apply for\ntimes where filter estimates are shown.) Bottom: ESS of the RWPFs weights (defined as\n(\n\u2211N\nj=1w\n(j)\ni )\n2\/\n\u2211N\nj=1(w\n(j)\ni )\n2) over time. The dramatic increases in the effective sample sizes\ncorrespond to re-sampling times.\nless then 100 (C = N\/10 in step PF1 of the algorithm in Section 3). Whilst results for the\nsine diffusion suggest that this will result in an algorithm that re-samples too infrequently,\nwe chose to have a low threshold so that we could monitor the performance of the particle\nfilter by how the ESS of the particle filter weights decay over time. The results of one run\nof this filter are shown in Figure 3(top). The computational efficiency of this method can\nbe gauged by Figure 3 (bottom) where the ESS of the w\n(j)\ni s is plotted over time.\n6 Discussion\nWe have described how recent methods for the exact simulation of diffusions and the\nunbiased estimation of diffusion exponential functionals can be used within particle filters,\nso that the resulting particle filters avoid the need for time-discretisation. Among the\n21\napproaches we have introduced special attention was given to RWPF which implements\nan auxiliary particle filter, but simulates the weights that are allocated to each particle.\nWe showed that this methodology is equivalent to an auxiliary particle filter applied to\nappropriately expanded model. We expect that this methodology will have interesting\napplications to different models than those considered in this paper, which however involve\nintractable dynamics or likelihoods.\nWe have focused on the filtering problem, estimating the current state given observa-\ntions to date. However, extensions to prediction are trivial \u2013 merely requiring the ability\nto simulate from the state equation, which is possible via the EA algorithms. It is also\nstraightforward to use the idea of Kitagawa (1996), where each particle stores the history\nof its trajectory, to get approximations of the smoothing density (the density of the state\nat some time in the past given the observations to date).\nNote that while particles store values of the state only for each observation time, it\nis straightforward to fill in the diffusion paths between these times to produce inferences\nabout the state at any time. A particle approximation to the distribution of (Xs, ti\u22121 <\ns < ti), conditionally on the data y1:i can be constructed using the current set of weighted\nparticles {(x(j)i\u22121,x(j)i )}Nj=1 with weights {w(j)i }, as follows. Firstly we need to introduce\nsome notation; we denote by x\n(j)\ni\u22121|i the value of the particle at time ti\u22121 from which the jth\nparticle at time ti is descended. The particle approximation is given by a set of weighted\npaths {(xs, ti\u22121 < s < ti)(j)}Nj=1 with weights {w(j)i }. Each path is a diffusion bridge\nstarting from x\n(j)\ni\u22121|i and finishing at x\n(j)\ni and it can be simulated using EA, as described\nin Beskos et al. (2006a) and Beskos et al. (2005b). In observation regimes (A) and (B)\nthe EA is applied to simulate a diffusion bridge with density w.r.t. the Brownian bridge\nmeasure given by exp{\u2212 \u222b ti\nti\u22121\n\u03c6(Xs)ds}, whereas in regime (C) the corresponding density\nis exp{\u2212 \u222b ti\nti\u22121\n(\u03c6(Xs) + \u03bd(Xs))ds}. This representation can be directly exploited to draw\ninferences for any function of a finite skeleton of X in-between observation times.\nAppendix A: The layered Brownian motion\nThe algorithm proposed in Beskos et al. (2005b) starts by creating a partition of the\nsample space of W for the given W0 = x and Wt = y. Writing x = (x1, . . . , xd), for a\nuser-specified constant a >\n\u221a\nt\/3, a sequence of subsets of Rd is formed as Aj = {u =\n(u1, . . . , ud) : min(xti , yi) \u2212 ja < ui \u2264 max(xti , yi) + ja}, j \u2265 0, where \u222ajAj = Rd. This\nsequence defines a partition of the sample space of the form \u222a\u221ej=1Dj, where a path belongs\nto Dj if and only if the path has exceeded the bounds determined by Aj\u22121 but not the\nbounds determined by Aj. In Beskos et al. (2005b) it is shown how to simulate the random\nvariable which determines which of the Djs W belongs to, and how to simulate W at\nany collection of times conditional on this random variable, the layered Brownian bridge\nconstruction. Since g is assumed continuous, knowing W \u2208 Dj is sufficient to determine\nUW and LW which satisfy (16). In fact, in the simplified setting where g is bounded, as in\n22\nthe sine diffusion of Example 1, the layered Brownian bridge construction can be avoided\nsince it is easy to choose UW and LW independently of W.\nAppendix B: Proof of Theorem 1\nI :=\nt\u03ba\n\u03ba!p(\u03ba | UW, LW)\n\u03ba\u220f\nj=1\n(UW \u2212 g(W\u03c8j)) .\nThen, (19) is established as follows:\nE[I2 | UW, LW] = E[E[I2 | \u03ba,W] ] = E\n[\nt2\u03ba\n(\u03ba!p(\u03ba | UW, LW))2\n(\u222b t\n0\n(UW \u2212 g(Ws))2\nt\nds\n)\u03ba]\n= E\n[\nt\u03ba\n(\u03ba!p(k | UW, LW))2E\n[(\u222b t\n0\n(UW \u2212 g(Ws))2ds\n)\u03ba\n| UW, LW, \u03ba\n]]\n=\n\u221e\u2211\nk=0\ntk\np(k | UW, LW)k!2E\n[(\u222b t\n0\n(UW \u2212 g(Ws))2ds\n)k\n| UW, LW\n]\n.\nFubini\u2019s theorem and dominated convergence are used above (valid since the integrands\nare positive a.s.). (22) is obtained using the following result (which can be easily proved\nusing Jensen\u2019s inequality). Let fi > 0 for i = 1, 2, . . .. Then the sequence of pis which\nminimize\n\u2211\u221e\ni=0 fi\/pi under the constraint\n\u2211\npi = 1 is given by pi =\n\u221a\nfi\/\n\u2211\u221a\nfi.\nAppendix C: Proof of Theorem 2\nGPE-1\u2264 e\u2212LWt so that the result holds if E[e\u2212LWt] <\u221e, where the expectation is w.r.t. a\nd-dimensional Brownian bridge from x at time 0 to y at time t. However\nE[e\u2212LWt] =\n\u222b \u221e\n0\nP[e\u2212LW > w]dw\n=\n\u222b \u221e\n0\nP[LW < \u2212 logw]dw \u2264\n\u222b \u221e\n0\nP[\u03b4\nd\u2211\ni=1\n(1 +Mi) > logw]dw\nwhere Mi = sup0\u2264s\u2264t |Wi| using the the growth bound in Theorem 2. Furthermore,\u222b \u221e\n0\nP[\u03b4\nd\u2211\ni=1\n(1 +Mi) > logw]dw \u2264\n\u222b \u221e\n0\nd\u2211\ni=1\nP[\u03b4(1 +Mi) > d\n\u22121 logw]dw\n=\n\u222b \u221e\n0\nd\u2211\ni=1\nP[Mi > (d\u03b4)\n\u22121 logw \u2212 1]dw .\n23\nIt remains therefore to bound the d integrals on the right hand side of this expression. How-\never from the Bachelier-Levy formula for hitting times for Brownian motion and bridges,\nP[Mi > v] \u2264 exp\n{\u22122(v \u2212max{xi, yi})2\/t}+ exp{\u22122(min{xi, yi + v})2\/t}\nand so\nP[Mi > (d\u03b4)\n\u22121 logw \u2212 1] \u2264 exp{\u22122((d\u03b4)\u22121 logw \u2212 1)\u2212max{xi, yi})2\/t}\n+ exp{\u22122(min{xi, yi + (d\u03b4)\u22121 logw \u2212 1)})2\/t}\nwhich recedes like w\u2212k logw as w \u2192\u221e thus concluding the proof.\nAppendix D: EPPF and ESPF for Example 1\nEPPF generates the new particles according to the following procedure:\n(i) choose one of the current particles x\n(ki,j)\ni , where particle j is chosen w.p. \u03b2\n(j)\ni ;\n(ii) propose xti+1 from Normal with mean x\n(ki,j)\ni , and variance \u2206i;\n(iii) accept this proposal w.p. exp(\u2212 cos(xti+1)\u2212 1); if proposal is rejected return to (i).\n(iv) accept this proposal with probability\nE\n[\nexp\n{\n\u2212\n\u222b \u2206i\n0\n\u03c6(Ws)ds\n}]\n,\nwhere expectation is with respect to the law of a Brownian Bridge from W0 = x\n(ki,j)\ni\nand W\u2206i = xti+1 , and \u03c6 is given in (7). If the proposal is rejected return to (i),\notherwise xti+1 is the new particle at time ti+1 with weight wi+1 = f(yi+1|xi+1).\n(iv) is performed using retrospective sampling as described in Beskos et al. (2006a).\nESPF proceeds as above but with steps (i) and (ii) replaced by the step\n(i\u2019) propose (x\n(ki,j)\ni , xti+1) according to the density proportional to\nexp\n\uf8f1\uf8f4\uf8f2\uf8f4\uf8f3\u2212 cos(x(ki,j)i )\u2212\n(\nyi+1 \u2212 x(ki,j)i\n)2\n2(\u03c32 + (\u2206i))\n\uf8fc\uf8f4\uf8fd\uf8f4\uf8fe exp\n{\n\u2212(xti+1 \u2212 \u03b7)\n2\n2\u03c4 2\n}\nwhere \u03b7 = (\u03c32 +\u2206i)\n\u22121(x(ki,j)i \u03c3\n2 +\u2206iyi+1), and \u03c4 = \u03c3\n2\u2206i\/(\u03c3\n2 +\u2206i).\nThe algorithm is repeated until N values for xti+1 are accepted, each with weight 1\/N .\n24\nAppendix E: Proposal Distribution for Example 1\nConsider a diffusion satisfying SDE (1), with d = 1 for simplicity. The Ozaki approximation\nof this SDE is based on a first order Taylor expansion of the drift about some value x. For\nthe sine diffusion of Example 1, we get the following approximating SDE\ndX\u02dcs = \u2212 cos(x)[x\u2212 tan(x)\u2212 X\u02dcs] ds+ dBs.\nSo X\u02dcs\u2212 (x\u2212 tan(x)) is an OU process as defined in Example 2 with \u03c1 = cos(x) and \u03c3 = 1.\nTo calculate q(xti+1|x(j)i , yi+1) we compute the product of the transition density given by\nthe Ozaki approximation about x = x\n(j)\ni and the likelihood function f(yi+1|xti+1). Defining\n\u03c4 2 = (1\u2212exp{\u22122 cos(x(j)i )\u2206i})\/(2 cos(x(j)i )), and \u03b7 = x(j)i \u2212tan(x(j)i )(1\u2212exp{\u2212 cos(x(j)i )\u2206i})\nwe get that q(xti+1|x(j)i , yi+1) is Normal with mean (\u03b7\u03c32 + yi+1\u03c4 2)\/(\u03c4 2\u03c32) and variance\n\u03b72\u03c4 2\/(\u03b72 + \u03c4 2). Furthermore we calculate \u03b2\n(j)\ni \u221d w(j)i N\u03c42+\u03c32(yi+1 \u2212 \u03b7).\nAppendix F: Central Limit Theorem\nFor notational simplicity, we consider a special case of our particle filter, chosen to resemble\nthose considered in Chopin (2004). We choose our proposal density for time ti+1 to have\n\u03b2j = w\n(j)\ni ; and we assume iid sampling of X\n(j)\nti in step PF1. The particle filter of Chopin\n(2004) splits up simulating particles at time ti+1 into (i) a resampling of particles at time ti;\nand (ii) a propagation of each of these particles to time ti+1. Our assumption of iid sampling\nis equivalent to the multinomial resampling case of Chopin (2004). (The conditions for the\ncentral limit theorem are the same if the residual sampling methods of Liu and Chen (1998),\nbut the variances differ.) For simplicity we consider observation model (A) or (B), though\nthe result extends easily to observation model (C).\nLet \u03b8\n(j)\ni = (x\n(j)\nti , x\n(ki,j)\nti\u22121 ), where ki,j is the index sampled in step PF1 when simulating\nthe j particle at time ti and \u03b8\n(j)\ni is the jth particle at time ti together with the particle\nat time ti\u22121 from which it is descended. Also let E\u03b8i denote conditional expectation given\n\u03b8i. Similarly, let \u00b5i(\u03b8i) = \u00b5g(xi\u22121, xi, ti\u22121, ti), and denote by Ri the unbiased estimator of\n\u00b5i(\u03b8i), i.e. E[Ri] = \u00b5i(\u03b8i). An important quantity is \u03c32i (\u03b8i) = Var(Ri).\nWe define Ei[\u03d5] and Vari(\u03d5) to be the posterior mean and variance of an arbitrary\nfunction \u03d5(\u03b8) at time i, and consider Particle Filter estimates of Ei[\u03d5]. Let p\u02dcii(\u03b8i) be the\ndensity p(xti\u22121|y1:i\u22121)q(xti|xti\u22121). Finally define Eqi [\u03d5] and Varqi(\u03d5) to be shorthand for\nthe conditional expectation and variance of \u03d5(\u03b8i) with respect to q(xti|xti\u22121) (which are\nfunctions of xti\u22121). We denote \u2016 \u00b7 \u2016 to be the Euclidean norm and define recursively \u03a6i to\nbe the set of measurable functions \u03d5 such that for some \u03b4 > 0 Ep\u02dcii [\u2016hiRi\u03d5\u20162+\u03b4] < \u221e, and\nthat the function xti\u22121 7\u2192 Eqi [hi\u00b5i\u03d5] is in \u03a6i\u22121.\nTheorem 3. Consider a function \u03d5; define V\u02dc0 = Varp\u02dci(x0)(\u03d5), and by induction:\nV\u02dci(\u03d5) = V\u02c6i\u22121 {Eqi [\u03d5]}+ Ei\u22121 {Varqi(\u03d5)} , for i > 0, (27)\n25\nVi(\u03d5) =\nV\u02dci {\u00b5ihi \u00b7 (\u03d5\u2212 Ei[\u03d5])}+ Ep\u02dcii((\u03d5\u2212 Ei[\u03d5])2\u03c32i h2i )\nEp\u02dcii(\u00b5ihi)\n2\n, for i \u2265 0, (28)\nV\u02c6i(\u03d5) = Vi(\u03c6) + Vari(\u03c6), for i \u2265 0. (29)\nThen if for all i (C1) xti 7\u2192 1 belongs to \u03a6i; (C2) Ep\u02dcii [h2i\u03c32i ] <\u221e; and (C3) Ep\u02dcii [\u03c3i\u03d5hi]2+\u03b4 <\n\u221e for some \u03b4 > 0; then for any \u03d5 \u2208 \u03a6i, Ei[\u03d5] and Vi(\u03d5) are finite and we have the following\nconvergence in distribution as the number of particles, N , tends to infinity:\nN1\/2\n{\u2211N\nj=1w\n(j)\ni \u03d5(x\n(j)\nti )\u2211N\nj=1w\n(j)\ni\n\u2212 Ei[\u03d5]\n}\n\u2192 N (0, Vi(\u03d5))\nComment Equations (27)\u2013(29) refer to the changes in variance of the weighted particles\ndue to the propagation, weighting and resampling stages at iteration i. Only (28) differs\nfrom the respective result in Chopin (2004), and this is due to the second term on the\nright-hand side, which represents the increase in variance due to the randomness of the\nweights. Condition C1 is taken from Chopin (2004) and applies to standard particle filters;\nconditions C2 and C3 are new and are conditions bounding the variance of the random\nweights which ensures that Vi(\u03d5) is finite.\nProof. We adapt the induction proof in Chopin (2004), considering in turn the propagation,\nweighting and resampling steps Our filter differs from the standard particle filter only in\nterms of the weighting step; and therefore we need only to adapt the result of Lemma A2\nin Chopin (2004). In fact, (27) and (29) are identical to the corresponding quantities in\nChopin (2004), therefore it remains to show (28). We define the constant K = Ep\u02dcii [Rihi]\nand \u03d5\u2217 = Rihi(\u03d5\u2212 Ei(\u03d5))\/K. Within the enlarged signal space framework, we can apply\nEquation (4) of Chopin (2004), to give:\nVi(\u03d5) = V\u02dci (\u03d5\n\u2217) = V\u02c6i\u22121 {Eqi [\u03d5\u2217]}+ Ei\u22121 {Varqi(\u03d5\u2217)} .\nNow we can calculate Eqi [\u03d5\u2217] by first taking expection over the auxiliary variables (condi-\ntional on \u03b8i). This gives Eqi [\u03d5\u2217] = Eqi [\u00b5ihi(\u03d5\u2212 Ei(\u03d5))\/K]. Similarly we get\nVarqi(\u03d5\n\u2217) = Varqi(E[Rihi(\u03d5\u2212 Ei(\u03d5))\/K]) + Eqi(Var[Rihi(\u03d5\u2212 Ei(\u03d5))\/K]) (30)\n= Varqi(\u00b5ihi(\u03d5\u2212 Ei(\u03d5))\/K) + Eqi(\u03c32i h2i (\u03d5\u2212 Ei(\u03d5))2\/K2). (31)\n(Here the expectation and variance in (30) are w.r.t. the auxiliary variables). Combining\nthese results gives (28). The regularity conditions (C1) - (C3) translate directly also.\nReferences\nA\u0131\u00a8t-Sahalia, Y. (2004) Closed-form likelihood expansions for multivariate diffusions. Work-\ning paper, available from http:\/\/www.princeton.edu\/\u223cyacine\/research.htm.\n26\nBeskos, A., Papaspiliopoulos, O. and Roberts, G. O. (2005a) Monte Carlo maximum like-\nlihood estimation for discretely observed diffusion processes. Submitted.\n\u2014 (2005b) A new factorisation of diffusion measure and finite sample path constructions.\nSubmitted.\n\u2014 (2006a) Retrospective exact simulation of diffusion sample paths with applications.\nBernoulli, 12, 1077\u20131098.\nBeskos, A., Papaspiliopoulos, O., Roberts, G. O. and Fearnhead, P. (2006b) Exact and\nefficient likelihood\u2013based inference for discretely observed diffusions (with discussion).\nJournal of the Royal Statistical Society, Series B.\nBeskos, A. and Roberts, G. O. (2005) Exact simulation of diffusions. Annals of Applied\nProbability, 15, 2422\u20132444.\nCarpenter, J., Clifford, P. and Fearnhead, P. (1999) An improved particle filter for non-\nlinear problems. IEE proceedings-Radar, Sonar and Navigation, 146, 2\u20137.\nChib, S., Pitt, M. K. and Shephard, N. (2006) Likelihood based inference for diffusion\ndriven state space models. Submitted.\nChopin, N. (2004) Central limit theorem for sequential Monte Carlo methods and its ap-\nplication to Bayesian inference. The Annals of Statistics, 32, 2385\u20132411.\nCrisan, D. (2001) Particle filters - a theoretical perspective. In Sequential Monte Carlo\nMethods in Practice (eds. A. Doucet, N. de Freitas and N. gordon), 17\u201341. Springer\u2013\nVerlag; New York.\nCrisan, D., Del Moral, P. and Lyons, T. J. (1999) Interacting particle systems approx-\nimations of the Kushner-Stratonovich equation. Advances in Applied Probability, 31,\n819\u2013838.\nDacunha-Castelle, D. and Florens-Zmirou, D. (1986) Estimation of the coefficients of a\ndiffusion from discrete observations. Stochastics, 19, 263\u2013284.\nDassios, A. and Jang, H.-W. (2005) Kalman-Bucy filtering for linear systems driven by the\nCox process with shot noise intensity and its application to the pricing of reinsurance\ncontracts. J. Appl. Probab., 42, 93\u2013107.\nDel Moral, P. and Guionnet, A. (2001) On the stability of interactin processes with appli-\ncations to filtering and genetic algorithms. Ann. Inst. of H. Poincare\u00b4 Probab. Statist.\nDel Moral, P., Jacod, J. and Protter, P. (2001) The Monte-Carlo method for filtering with\ndiscrete-time observations. Probab. Theory Related Fields, 120, 346\u2013368.\n27\nDel Moral, P. and Miclo, L. (2000) Branching and interacting particle systems. Approxi-\nmations of Feymann-Kac formulae with applicationc to non-linear filtering.\nDoucet, A., de Freitas, N. and Gordon, N. (2001) An introduction to sequential Monte\nCarlo methods. In Sequential Monte Carlo methods in practice, Stat. Eng. Inf. Sci.,\n3\u201314. New York: Springer.\nDuffie, D. and Singleton, K. J. (1999) Modeling term structures of defaultable bonds. The\nReview of Financial Studies, 12, 687\u2013720.\nDurham, G. B. and Gallant, A. R. (2002) Numerical techniques for maximum likelihood\nestimation of continuous-time diffusion processes. J. Bus. Econom. Statist., 20, 297\u2013338.\nWith comments and a reply by the authors.\nEngel, R. F. (2000) The econometrics of ultra-high-frequency data. Econometrica, 68,\n1\u201322.\nGolightly, A. and Wilkinson, D. J. (2006) Bayesian sequential inference for nonlinear mul-\ntivariate diffusions. Statistics and Computing, 16, 323\u2013338.\nGordon, N., Salmond, D. and Smith, A. F. M. (1993) Novel approach to nonlinear\/non-\nGaussian Bayesian state estimation. IEE proceedings-F, 140, 107\u2013113.\nIonides, E. (2003) Inference and filtering for partially observed diffusion\nprocesses via sequential Monte Carlo. Working paper available from\nhttp:\/\/www.stat.lsa.umich.edu\/\u223cionides\/pubs\/WorkingPaper-filters.pdf.\nKitagawa, G. (1996) Monte Carlo filter and smoother for non-Gaussian nonlinear state\nspace models. Journal of Computational and Graphical Statistics, 5, 1\u201325.\nKou, S. C., Xie, X. S. and Liu, J. S. (2005) Bayesian analysis of single-molecule experi-\nmental data. J. Roy. Statist. Soc. Ser. C, 54, 469\u2013506.\nKu\u00a8nsch, H. R. (2005) Monte Carlo filters:Algorithms and theoretical analysis. Annals of\nStatistics, 33, 1983\u20132021.\nLiu, J. S. and Chen, R. (1998) Sequential Monte Carlo methods for dynamic systems. J.\nAmer. Statist. Assoc., 93, 1032\u20131044.\nNicolau, J. (2002) A new technique for simulating the likelihood of stochastic differential\nequations. The Econometrics Journal, 5, 91\u2013103.\nPitt, M. K. and Shephard, N. (1999) Filtering via simulation: auxiliary particle filters. J.\nAmer. Statist. Assoc., 94, 590\u2013599.\n28\n"}