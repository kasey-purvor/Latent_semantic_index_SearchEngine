{"doi":"10.1109\/TSMCB.2003.817053","coreId":"71461","oai":"oai:eprints.lancs.ac.uk:899","identifiers":["oai:eprints.lancs.ac.uk:899","10.1109\/TSMCB.2003.817053"],"title":"An approach to online identification of Takagi-Sugeno fuzzy models","authors":["Angelov, Plamen","Filev, Dimitar"],"enrichments":{"references":[{"id":16349483,"title":"A fuzzy logic based approach ot qualitative modeling,\u201d","authors":[],"date":"1993","doi":"10.1109\/tfuzz.1993.390281","raw":"M. Sugeno and M. Yasukawa, \u201cA fuzzy logic based approach ot qualitative modeling,\u201d IEEE Trans. Fuzzy Syst., vol. 1, pp. 7\u201331, Feb. 1993.","cites":null},{"id":16349459,"title":"A general regression neural network,\u201d","authors":[],"date":"1991","doi":"10.1109\/72.97934","raw":"D. Specht, \u201cA general regression neural network,\u201d IEEE Trans. Neural Networks, vol. 2, pp. 568\u2013576, Nov. 1991.","cites":null},{"id":16349489,"title":"A reinforcement learning-based architecture for fuzzy logic control,\u201d","authors":[],"date":"1992","doi":"10.1016\/0888-613x(92)90020-z","raw":"H. R. Berenji, \u201cA reinforcement learning-based architecture for fuzzy logic control,\u201d Int. J. Approx. Reasoning, vol. 6, pp. 267\u2013292, 1992.","cites":null},{"id":16349501,"title":"A resource allocation network for function interpolation,\u201d","authors":[],"date":"1991","doi":"10.1162\/neco.1991.3.2.213","raw":"J. Platt, \u201cA resource allocation network for function interpolation,\u201d Neural Computat., vol. 3, pp. 213\u2013225, 1991.","cites":null},{"id":16349494,"title":"A rule-based adaptive PID controller,\u201d in","authors":[],"date":"1988","doi":"10.1109\/cdc.1988.194374","raw":"K. L. Anderson, G. L. Blackenship, and L. G. Lebow, \u201cA rule-based adaptive PID controller,\u201d in Proc. 27th IEEE CDC\u201988, 1988, pp. 564\u2013569.","cites":null},{"id":16349470,"title":"A self-learning fuzzy logic controller using genetic algorithms with reinforcements,\u201d","authors":[],"date":"1997","doi":"10.1109\/91.618280","raw":"C. K. Chiang, H.-Y. Chung, and J. J. Lin, \u201cA self-learning fuzzy logic controller using genetic algorithms with reinforcements,\u201d IEEE Trans. Fuzzy Syst., vol. 5, pp. 460\u2013467, Aug. 1997.","cites":null},{"id":16349498,"title":"Adaptive Control.","authors":[],"date":"1989","doi":"10.1007\/bfb0042931","raw":"K. J. Astroem and B. Wittenmark, Adaptive Control. Reading, MA: Addison-Wesley, 1989.","cites":null},{"id":16349490,"title":"An effective neuro-fuzzy paradigm for machinery condition health monitoring,\u201d in","authors":[],"date":"1999","doi":"10.1109\/cca.1999.801205","raw":"G. G. Yen and P. Meesad, \u201cAn effective neuro-fuzzy paradigm for machinery condition health monitoring,\u201d in Proc. IEEE Int. Joint Conf. IJCNN\u201999, Washington, DC, 1999, pp. 1567\u20131572.","cites":null},{"id":16349467,"title":"ANFIS: adaptive network-based fuzzy inference systems,\u201d","authors":[],"date":"1993","doi":"10.1109\/21.256541","raw":"J. S. R. Jang, \u201cANFIS: adaptive network-based fuzzy inference systems,\u201d IEEE Trans. Syst., Man Cybern., vol. 23, pp. 665\u2013685, May\/June 1993.","cites":null},{"id":16349473,"title":"Automatic generation of fuzzy rule-based models from data by genetic algorithms,\u201d","authors":[],"date":"2001","doi":"10.1007\/978-3-7908-1829-1_4","raw":"P. P. Angelov, V. I. Hanby, R. A. Buswell, and J. A. Wright, \u201cAutomatic generation of fuzzy rule-based models from data by genetic algorithms,\u201d in Advances in Soft Computing, R. John and R. Birkenhead, Eds. Heidelberg, Germany: Springer-Verlag, 2001, pp. 31\u201340.","cites":null},{"id":16349505,"title":"biological motivation,\u201d in Methodologies for the Conception, Design and Application of Soft Computing,","authors":[],"date":"1998","doi":null,"raw":"N.Kasabov,\u201cEvolvingfuzzyneuralnetworks\u2014algorithms,applications and biological motivation,\u201d in Methodologies for the Conception, Design and Application of Soft Computing, T. Yamakawa and G. Matsumoto, Eds, Singapore: World Scientific, 1998, pp. 271\u2013274.","cites":null},{"id":16349486,"title":"Cluster validity with fuzzy sets,\u201d","authors":[],"date":"1974","doi":"10.1080\/01969727308546047","raw":"J. Bezdek, \u201cCluster validity with fuzzy sets,\u201d J. Cybern., vol. 3, no. 3, pp. 58\u201371, 1974.","cites":null},{"id":16349499,"title":"DENFIS: dynamic evolving neural-fuzzy inference system and its application for time-series prediction,\u201d","authors":[],"date":"2002","doi":"10.1109\/91.995117","raw":"N. K. Kasabov and Q. Song, \u201cDENFIS: dynamic evolving neural-fuzzy inference system and its application for time-series prediction,\u201d IEEE Trans. Fuzzy Syst., vol. 10, pp. 144\u2013154, Apr. 2002.","cites":null},{"id":16349463,"title":"Essentials of Fuzzy Modeling and Control.","authors":[],"date":"1994","doi":"10.1145\/222267.1065840","raw":"R. R. Yager and D. P. Filev, Essentials of Fuzzy Modeling and Control. New York: Wiley, 1994.","cites":null},{"id":16349479,"title":"Evolving fuzzy rule-based controllers using","authors":[],"date":"1996","doi":"10.1016\/0165-0114(95)00196-4","raw":"B. Carse, T. C. Fogarty, and A. Munro, \u201cEvolving fuzzy rule-based controllers using GA,\u201d Fuzzy Sets Syst., vol. 80, pp. 273\u2013294, 1996.","cites":null},{"id":16349478,"title":"Evolving Rule-Based Models: A Tool for Design of Flexible Adaptive Systems.","authors":[],"date":"2002","doi":"10.1007\/978-3-7908-1794-2_3","raw":"P. P. Angelov, Evolving Rule-Based Models: A Tool for Design of Flexible Adaptive Systems. Heidelberg, Germany: Springer-Verlag, 2002.","cites":null},{"id":16349502,"title":"Evolving self-organizing maps for online learning,dataanalysisandmodeling,\u201dinProc.IJCNN\u20192000NeuralNetworks, Neural Comput.: New Challenges Perspectives New Millennium,","authors":[],"date":"2000","doi":"10.1109\/ijcnn.2000.859364","raw":"D. Deng and N. Kasabov, \u201cEvolving self-organizing maps for online learning,dataanalysisandmodeling,\u201dinProc.IJCNN\u20192000NeuralNetworks, Neural Comput.: New Challenges Perspectives New Millennium, vol. VI, S.-I. Amari, C. L. Giles, M. Gori, and V. Piuri, Eds., New York, NY, 2000, pp. 3\u20138.","cites":null},{"id":16349497,"title":"Fast learning in networks of locally-tuned processing units,\u201d","authors":[],"date":"1989","doi":"10.1162\/neco.1989.1.2.281","raw":"J. Moody and C. J. Darken, \u201cFast learning in networks of locally-tuned processing units,\u201d Neural Computat., vol. 1, pp. 281\u2013294, 1989.","cites":null},{"id":16349491,"title":"Fuzzy clustering with a fuzzy covariancematrix,\u201dinProc.IEEEControlDecisionConf.,SanDiego,CA,","authors":[],"date":"1979","doi":null,"raw":"D. E. Gustafson and W. C. Kessel, \u201cFuzzy clustering with a fuzzy covariancematrix,\u201dinProc.IEEEControlDecisionConf.,SanDiego,CA, 1979, pp. 761\u2013766.","cites":null},{"id":16349482,"title":"Fuzzy identification of systems and its application to modeling and control,\u201d","authors":[],"date":"1985","doi":"10.1016\/b978-1-4832-1450-4.50045-6","raw":"T. Takagi and M. Sugeno, \u201cFuzzy identification of systems and its application to modeling and control,\u201d IEEE Trans. Syst., Man Cybern., vol. 15, pp. 116\u2013132, 1985.","cites":null},{"id":16349485,"title":"Fuzzy model identification based on cluster estimation,\u201d","authors":[],"date":"1994","doi":"10.1109\/fuzzy.1994.343644","raw":"S. L. Chiu, \u201cFuzzy model identification based on cluster estimation,\u201d J. Intel. Fuzzy Syst., vol. 2, pp. 267\u2013278, 1994.","cites":null},{"id":16349496,"title":"Fuzzy modeling and identification,\u201d","authors":[],"date":"1996","doi":"10.1007\/978-94-011-4868-9_4","raw":"R. Babuska, \u201cFuzzy modeling and identification,\u201d Ph.D. thesis, Univ. of Delft, Delft, The Netherlands, 1996.","cites":null},{"id":16349509,"title":"Identification of evolving fuzzy rule-based models,\u201d","authors":[],"date":"2002","doi":"10.1109\/tfuzz.2002.803499","raw":"P. Angelov and R. Buswell, \u201cIdentification of evolving fuzzy rule-based models,\u201d IEEE Trans. Fuzzy Syst., vol. 10, pp. 667\u2013677, Oct. 2002. Plamen P. Angelov (M\u201999) received the Ph.D. degree from the Bulgarian Academy of Science, Bulgaria, in 1993. Since June 2003, he has been a Lecturer in the Department of Communications Systems, Lancaster University, Lancaster, U.K. He was a Research Fellow in Loughborough University, U.K., from 1998 to 2003. He has been Visiting Research Fellow in CESAME, Catholic University of Louvain, Belgium, in 1997,HannoverUniversityandHKI,Jena,Germany,from 1995to 1996.Heauthored the monograph Evolving Rule based Models: A Tool for Design of Flexible Adaptive Systems (Heidelberg, Germany: Springer, 2002). His research has been funded by the EC, ASHARE, Research Councils of UK (EPSRC), Germany (DAAD and DFG), Belgium, Italy (CNR), Bulgaria (NFSI). His research interests are in intelligent data processing, particularly in evolving rule-based models, self-organizing and autonomous systems, evolutionary algorithms, optimization and optimal control in a fuzzy environment. Dr. Angelov has been in the program and organizing committees of several conferences, including IFSA-2003, GECCO-2002, RASC-2002, FUBEST\u201994 and \u201996, BioPS\u201994, \u201995, \u201997. Dimitar P. Filev (M\u201995\u2013SM\u201997) received the PhD. degree in electrical engineering from the Czech Technical University, Czechoslovakia, in 1979. HeisaStaffTechnicalSpecialistandaManageroftheKnowledgeBasedSystems and Control Department with Advanced Manufacturing Technology Development, Ford Motor Company specializing in industrial intelligent systems and technologies for control, diagnostics and decision making. Prior to joining Ford, he was Professor of information systems and Senior Research Associate at the Machine Intelligence Institute, Iona College, and Associate Professor at theBulgarianAcademyofSciences.Heisconducting researchincontrol theory and applications, modeling of complex systems, and intelligent modeling and control. He has published three books and over 150 articles in refereed journals and conference proceedings. He holds nine U.S. patents. Dr.Filevreceivedthe1995AwardforExcellencefromMCBUniversityPress andthreeHenryFordTechnologyAwards.HeisanAssociateEditoroftheIEEE TRANSACTIONS. ON FUZZY SYSTEMS, the International Journal of General Systems, and the International Journal of Approximate Reasoning.","cites":null},{"id":16349487,"title":"Improving the interpretability of TSK fuzzy models by combining global and local learning,\u201d","authors":[],"date":"1998","doi":"10.1109\/91.728447","raw":"J. Yen, L. Wang, and C. W. Gillespie, \u201cImproving the interpretability of TSK fuzzy models by combining global and local learning,\u201d IEEE Trans. Fuzzy Syst., vol. 6, pp. 530\u2013537, Nov. 1998.","cites":null},{"id":16349484,"title":"Intelligent control for automotive manufacturing-rule based guided adaptation,\u201d in","authors":[],"date":"2000","doi":"10.1109\/iecon.2000.973164","raw":"D. P. Filev, T. Larsson, and L. Ma, \u201cIntelligent control for automotive manufacturing-rule based guided adaptation,\u201d in Proc. IEEE Conf. IECON\u201900, Nagoya, Japan, Oct. 2000, pp. 283\u2013288.","cites":null},{"id":16349475,"title":"Learning of a fuzzy control rule base using messy genetic algorithms,\u201d","authors":[],"date":"1996","doi":null,"raw":"F. Hoffmann and G. Pfister, \u201cLearning of a fuzzy control rule base using messy genetic algorithms,\u201d in Studies in Fuzziness and Soft Computing, F. Herrera and J.L. Verdegay, Eds. Heidelberg, Germany: Physica Verlag, 1996, vol. 8, pp. 279\u2013305.","cites":null},{"id":16349493,"title":"Learning of fuzzy rules by mountain clustering,\u201d in","authors":[],"date":"1993","doi":"10.1117\/12.165030","raw":"R. R. Yager and D. P. Filev, \u201cLearning of fuzzy rules by mountain clustering,\u201d in Proc. SPIE Conf. Applicat. Fuzzy Logic Technol., Boston, MA, 1993, pp. 246\u2013254.","cites":null},{"id":16349492,"title":"Mathematical analysis of fuzzy classifiers,\u201d","authors":[],"date":"1997","doi":"10.1007\/bfb0052854","raw":"F. Klawon and P. E. Klement, \u201cMathematical analysis of fuzzy classifiers,\u201d Lect. Notes Comp. Sci., vol. 1280, pp. 359\u2013370, 1997.","cites":null},{"id":16349465,"title":"Operating regime approach to nonlinearmodelingandcontrol,\u201dinMultipleModelApproachestoModeling","authors":[],"date":null,"doi":null,"raw":"T. A. Johanson and R. Murray-Smith, \u201cOperating regime approach to nonlinearmodelingandcontrol,\u201dinMultipleModelApproachestoModeling and Control, R. Murray-Smith and T. A. Johanson, Eds. Hants, U.K.: Taylor Francis, pp. 3\u201372.","cites":null},{"id":16349480,"title":"P.P.Angelov,V.I.Hanby,andJ.A.Wright,\u201cHVACsystemssimulation: a self-structuring fuzzy rule-based approach,\u201d","authors":[],"date":"2000","doi":null,"raw":"P.P.Angelov,V.I.Hanby,andJ.A.Wright,\u201cHVACsystemssimulation: a self-structuring fuzzy rule-based approach,\u201d Int. J. Architectural Sci., vol. 1, no. 1, pp. 49\u201358, 2000.","cites":null},{"id":16349495,"title":"Rule-base guided adaptation for mode detection in process control,\u201d in","authors":[],"date":"2001","doi":"10.1109\/nafips.2001.944753","raw":"D. P. Filev, \u201cRule-base guided adaptation for mode detection in process control,\u201d in Proc. Joint 9th IFSA World Congr.\/20th NAFIPS Annu. Conf., Vancouver, BC, Canada, July 2001, pp. 1068\u20131073.","cites":null},{"id":16349488,"title":"Self-constructingfuzzy neural network speed controller for permanent-magnet synchronous motor drive,\u201d","authors":[],"date":"2001","doi":"10.1109\/91.963761","raw":"F.-J. Lin, C.-H. Lin,andP.-H.Shen, \u201cSelf-constructingfuzzy neural network speed controller for permanent-magnet synchronous motor drive,\u201d IEEE Trans. Fuzzy Syst., vol. 9, pp. 751\u2013759, Oct. 2001.498 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART B: CYBERNETICS, VOL. 34, NO. 1, FEBRUARY 2004","cites":null},{"id":16349481,"title":"Self-tuning fuzzy modeling with adaptive membership function, rules, and hierarchical structure based on genetic algorithm,\u201d","authors":[],"date":"1995","doi":"10.1016\/0165-0114(94)00280-k","raw":"K. Shimojima, T. Fukuda, and Y. Hasegawa, \u201cSelf-tuning fuzzy modeling with adaptive membership function, rules, and hierarchical structure based on genetic algorithm,\u201d Fuzzy Sets Syst., vol. 71, pp. 295\u2013309, 1995.","cites":null},{"id":16349461,"title":"System Identification, Theory for the User.","authors":[],"date":"1987","doi":"10.1109\/mra.2012.2192817","raw":"L. Ljung, System Identification, Theory for the User. Englewood Cliffs, NJ: Prentice-Hall, 1987.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2004-02","abstract":"An approach to the online learning of Takagi-Sugeno (TS) type models is proposed in the paper. It is based on a novel learning algorithm that recursively updates TS model structure and parameters by combining supervised and unsupervised learning. The rule-base and parameters of the TS model continually evolve by adding new rules with more summarization power and by modifying existing rules and parameters. In this way, the rule-base structure is inherited and up-dated when new data become available. By applying this learning concept to the TS model we arrive at a new type adaptive model called the Evolving Takagi-Sugeno model (ETS). The adaptive nature of these evolving TS models in combination with the highly transparent and compact form of fuzzy rules makes them a promising candidate for online modeling and control of complex processes, competitive to neural networks. The approach has been tested on data from an air-conditioning installation serving a real building. The results illustrate the viability and efficiency of the approach. The proposed concept, however, has significantly wider implications in a number of fields, including adaptive nonlinear control, fault detection and diagnostics, performance analysis, forecasting, knowledge extraction, robotics, behavior modeling","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/71461.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/899\/1\/SMC%2DB_paper.pdf","pdfHashValue":"18b4db8a705a65c277912fc13647b844c1de54d0","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:899<\/identifier><datestamp>\n      2018-01-24T03:19:18Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        An approach to online identification of Takagi-Sugeno fuzzy models<\/dc:title><dc:creator>\n        Angelov, Plamen<\/dc:creator><dc:creator>\n        Filev, Dimitar<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        An approach to the online learning of Takagi-Sugeno (TS) type models is proposed in the paper. It is based on a novel learning algorithm that recursively updates TS model structure and parameters by combining supervised and unsupervised learning. The rule-base and parameters of the TS model continually evolve by adding new rules with more summarization power and by modifying existing rules and parameters. In this way, the rule-base structure is inherited and up-dated when new data become available. By applying this learning concept to the TS model we arrive at a new type adaptive model called the Evolving Takagi-Sugeno model (ETS). The adaptive nature of these evolving TS models in combination with the highly transparent and compact form of fuzzy rules makes them a promising candidate for online modeling and control of complex processes, competitive to neural networks. The approach has been tested on data from an air-conditioning installation serving a real building. The results illustrate the viability and efficiency of the approach. The proposed concept, however, has significantly wider implications in a number of fields, including adaptive nonlinear control, fault detection and diagnostics, performance analysis, forecasting, knowledge extraction, robotics, behavior modeling.<\/dc:description><dc:date>\n        2004-02<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/899\/1\/SMC%2DB_paper.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/TSMCB.2003.817053<\/dc:relation><dc:identifier>\n        Angelov, Plamen and Filev, Dimitar (2004) An approach to online identification of Takagi-Sugeno fuzzy models. IEEE Transactions on Systems, Man and Cybernetics, Part B: Cybernetics, 34 (1). pp. 484-498. ISSN 1083-4419<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/899\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1109\/TSMCB.2003.817053","http:\/\/eprints.lancs.ac.uk\/899\/"],"year":2004,"topics":["QA75 Electronic computers. Computer science"],"subject":["Journal Article","PeerReviewed"],"fullText":"484 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART B: CYBERNETICS, VOL. 34, NO. 1, FEBRUARY 2004\nAn Approach to Online Identification of\nTakagi-Sugeno Fuzzy Models\nPlamen P. Angelov, Member, IEEE, and Dimitar P. Filev, Senior Member, IEEE\nAbstract\u2014An approach to the online learning of Takagi\u2013Sugeno\n(TS) type models is proposed in the paper. It is based on a novel\nlearning algorithm that recursively updates TS model structure\nand parameters by combining supervised and unsupervised\nlearning. The rule-base and parameters of the TS model con-\ntinually evolve by adding new rules with more summarization\npower and by modifying existing rules and parameters. In this\nway, the rule-base structure is inherited and up-dated when\nnew data become available. By applying this learning concept\nto the TS model we arrive at a new type adaptive model called\nthe Evolving Takagi\u2013Sugeno model (ETS). The adaptive nature\nof these evolving TS models in combination with the highly\ntransparent and compact form of fuzzy rules makes them a\npromising candidate for online modeling and control of complex\nprocesses, competitive to neural networks. The approach has been\ntested on data from an air-conditioning installation serving a real\nbuilding. The results illustrate the viability and efficiency of the\napproach. The proposed concept, however, has significantly wider\nimplications in a number of fields, including adaptive nonlinear\ncontrol, fault detection and diagnostics, performance analysis,\nforecasting, knowledge extraction, robotics, behavior modeling.\nIndex Terms\u2014Online recursive identification, rule-base adapta-\ntion, Takagi\u2013Sugeno models.\nI. INTRODUCTION\nTAKAGI\u2013SUGENO models have recently become a pow-erful practical engineering tool for modeling and control\nof complex systems. They form a natural transition between\nconventional and rule-based control by expanding and gener-\nalizing the well-known concept of gain scheduling. While the\ngain-scheduling [24] paradigm is based on the assumption of\nlocal approximation of a nonlinear system by a collection of\nlinear models, the TS models utilize the idea of linearization in\na fuzzily defined region of the state space. Due to the fuzzy re-\ngions, the nonlinear system is decomposed into a multi-model\nstructure consisting of linear models that are not necessarily in-\ndependent [4].\nThe TS model representation often provides efficient and\ncomputationally attractive solutions to a wide range of control\nproblems introducing a powerful multiple model structure that\nis capable to approximate nonlinear dynamics, multiple oper-\nating modes and significant parameter and structure variations.\nManuscript received May 24, 2002; revised December 5, 2002. This work\nused data generated from the ASHRAE Project RP1020. This paper was rec-\nommended by Associate Editor L. O. Hall.\nP. P. Angelov is with the Department of Communications Systems, Lancaster\nUniversity, Bailrigg, Lancaster LA1 4YR, U.K. (e-mail: p.angelov@lan-\ncaster.ac.uk).\nD. P. Filev is with the Ford Motor Co., Detroit, MI 48239 USA (e-mail:\ndfilev@ford.com).\nDigital Object Identifier 10.1109\/TSMCB.2003.817053\nThe methods for learning TS models from data are based on\nthe idea of consecutive structure and parameter identification\n[3], [14]. Structure identification includes estimation of the focal\npoints of the rules (antecedent parameters) by fuzzy clustering.\nWith fixed antecedent parameters, the TS model transforms into\na linear model. Parameters of the linear models associated with\neach of the rule antecedents are obtained by pseudo-inversion\nor by applying the recursive least square (RLS) method [16],\n[28]. Alternatively, the antecedent parameters can be consid-\nered as initial estimates only and the structure and parameters\ncan be further optimized by back-propagation [20] or genetic\nalgorithm [12]. These methods, however, suppose that all the\ndata is available at the start of the process of training. There-\nfore, they are appropriate for offline applications only. Their use\nin online algorithms is only possible for the price of re-training\nthe whole model structure and parameters with iterative and\ntime-consuming procedures such as back-propagation [5], ge-\nnetic algorithms [6]\u2013[8], [10]\u2013[12] or other nonlinear search\ntechniques [1], [21], [29].\nAlthough some objects, including biotechnological pro-\ncesses, building thermal systems, etc. have relatively slow\ndynamics, making such re-training possible, it is difficult to\ncharacterize it as adaptation, especially in respect to the model\nstructure. It is in fact, a procedure where completely new\nmodels are repeatedly generated given the new data. The fact\nthat fuzzy models are still not adaptive, while in many practical\nproblems the control object or the environment is changing\nsignificantly is an important obstacle in their design, which is\nstill unresolved [19].\nFor continuous online learning of the TS models a develop-\nment of a new online clustering method responsible for model\nstructure (rule base) learning online is needed. This requires re-\ncursive calculation of the informative potential of the data [9],\nwhich represents a spatial proximity measure used to define the\nfocal points of the rules (antecedent parameters). If suppose that\nthe model structure evolves similarly to the model parameters,\nthough much slower, then we need suitable new algorithms for\nonline clustering and recursive parameter estimation with this\nassumption. The purpose of this paper is to present such algo-\nrithms and the results of their application to a number of test\ncases both simulated and real.\nRecently, rule-bases [9], [15], [27] and neural networks [20],\n[31] with evolving structure have been developed. Rule-base of\nInitial Conditions, RBIC [15], Intelligent Model Bank [27] and\nthe Self-constructing fuzzy-neural network controller [20] are\nprimarily oriented to control applications. They use different\nmechanism of rules update based on the distance to certain\nrule center [15], [27], [31] or the error in previous steps [20].\n1083-4419\/04$20.00 \u00a9 2004 IEEE\nANGELOV AND FILEV: APPROACH TO ONLINE IDENTIFICATION 485\nEvolving rule-based models [9] use the informative po-\ntential of the new data sample (accumulated spatial proximity\ninformation) as a trigger to update the rule-base, which ensures\ngreater generality of the structural changes. Outliers have no\nchance to become rule centers. It also ensures that the rules are\nmore general (that they are able to describe a larger number of\ndata samples) from time of their initialization. In addition, the\nmechanism of rule-base modification (replacement of a less\ninformative rule with a more informative one) is considered\nin [9]. It is also based on the informative potential and is\nmore conservative than the replacement used in [15], [27],\n[31] ensuring a gradual change of the rule-base structure and\ninheritance of the structural information. models generate\na new rule if there is significant new information present in\nthe data collected. The evolution mechanism takes care of\nthe replacement of existing rules based on the accumulated\nmeasure of the spatial proximity of all the data samples. If the\ninformative potential of the new data sample is higher than the\naverage potential of the existing rules it is added to the rule\nbase. If the new data, which is accepted as a focal point of a\nnew rule is too close to a previously existing rule then the old\nrule is replaced by the new one.\nThe appearance of a new rule indicates a region of the data\nspace that has not been covered by the initial training data. This\ncould be a new operating mode of the plant or reaction to a new\ndisturbance. In reality, many regimes and process states cannot\nbe practically included into the training data set (such as faulty\nprocess behavior), but states close to them could well appear\nduring the process run [22].\nIt is important to note that learning could start without a priori\ninformation and only one data sample. This interesting feature\nmakes the approach potentially very useful in adaptive control,\nrobotic, diagnostic systems and as a tool for knowledge acqui-\nsition from data.\nThe concept of modeling [9] is further developed here in\nrespect to online identification of ETS models. Recursive pro-\ncedures for calculation of the informative potential of the new\ndata and of the consequence parameters are introduced, which\nremove the need of the time moving window considered in [9].\nThis feature is vitally important for real-time applications.\nThe rest of the paper is organized as follows. The problem\nof identification of TS models is presented in Section II. Two\nalternative ways (globally and locally optimal) of calculation of\nthe consequent parameters are presented. The new approach for\nonline learning ETS models is presented in the next Section III.\nIn Section IV the essential stages of the procedure are defined\nand systematically described. Section V studies experimental\nresults considering a real air-conditioning engineering problem.\nConcluding remarks are given in Section VI.\nII. TS FUZZY MODEL AND THE PROBLEM OF ITS\nIDENTIFICATION\nFuzzy model identification has its roots in the pioneering pa-\npers of Sugeno and his coworkers [13], [14] and is associated\nwith the so-called Takagi-Sugeno (TS) fuzzy models\u2014a special\ngroup of rule-based models with fuzzy antecedents and func-\ntional consequents that follow from the Takagi\u2013Sugeno\u2013Kang\nreasoning method:\n(1)\nwhere denotes the fuzzy rule; is the number of fuzzy\nrules; is the input vector; ; denotes\nthe antecedent fuzzy sets, ; is the output of the\nlinear subsystem; are its parameters, .\nThe TS model paradigm [13] can be considered as a gener-\nalization of the gain-scheduling concept. Instead of linearizing\nstrictly at an operating point it utilizes the idea of linearization\nin a fuzzily defined region of the space. The fuzzy regions are\nparameterized and each region is associated with a linear sub-\nsystem. Owing to the fuzzily defined antecedents, the nonlinear\nsystem forms a collection of loosely coupled multiple linear\nmodels. The degree of firing of each rule is proportional to the\nlevel of contribution of the corresponding linear model to the\noverall output of the TS model. For Gaussian-like antecedent\nfuzzy sets\n(2)\nwhere and is a positive constant, which defines the\nspread of the antecedent and the zone of influence of the\nmodel (radius of the neighborhood of a data point); too large a\nvalue of leads to averaging, too small a value\u2014to over-fitting;\nvalues of can be recommended ) [16]; is the\nfocal point of the rule antecedent.\nThe firing level of the rules are defined as Cartesian product\nor conjunction of respective fuzzy sets for this rule\n(3)\nThe TS model output is calculated by weighted averaging of\nindividual rules\u2019 contributions\n(4)\nwhere is the normalized firing level of the\nrule; represents the output of the linear model;\n, , is the vector of parameters of\nthe linear model; is the expanded data vector.\nGenerally, the problem of identification of a TS model is di-\nvided into two sub-tasks [3], [13], [16].\ni) Learning the antecedent part of the model (1), which con-\nsists of determination of the focal points of the rules, i.e.,\nthe centers ( ; ) and spreads of the mem-\nbership functions.\nii) Learning the parameters of the linear subsystems ( ;\n; ) of the consequents.\nA. Learning Rule Antecedents by Data Space Clustering\nFirst sub-task can be solved by clustering the input-output\ndata space . The Subtractive Clustering method\n[16], Fuzzy C-means [17], and the Gustafson\u2013Kessel clustering\n486 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART B: CYBERNETICS, VOL. 34, NO. 1, FEBRUARY 2004\nmethod [23] are among the well-established methods for\nlearning the antecedent parameters offline in a batch-processing\nlearning mode when all the input-output data is available.\nThe procedure called subtractive clustering [16] is an im-\nproved version of the so-called mountain clustering approach\n[25]. It uses the data points as candidate prototype cluster cen-\nters. The capability of a point to be a cluster center is evaluated\nthrough its potential\u2014a measure of the spatial proximity be-\ntween a particular point and all other data points\n(5)\nwhere denote the potential of the data point and where\nis the number of training data).\nAs seen from (5) the value of the potential is higher for a\ndata point that is surrounded by a large number of close data\npoints. Therefore, it is reasonable to establish such a point to\nbe the center of a cluster [24]. The potential of all other data\npoints is reduced by an amount proportional to the potential of\nthe chosen point and inversely proportional to the distance to\nthis center. The next center is found also as the data point with\nthe highest (after this subtraction) potential. The procedure is\nrepeated until the potential of all data points is reduced below a\ncertain threshold.\nThe procedure of the subtractive clustering includes the fol-\nlowing steps [16].\n1) Initially, the data point with the highest potential is chosen\nto be the first cluster center\n(6)\nwhere denotes the potential of the first center.\n2) The potential of all other points are then reduced by an\namount proportional to the potential of the chosen point\nand inversely proportional to the distance to this center\n(7)\nwhere denotes the potential of the center;\n; ; where is a positive constant,\ndetermining the radius of the neighborhood that will\nhave measurable reductions in the potential because of\nthe closeness to an existing center; recommended value\nof is [16].\n3) Two boundary conditions are defined: lower\nand upper threshold, determined as a function\nof the maximal potential called the \u201creference\u201d potential\n. A data point is chosen to be a new cluster center,\nand respectively center of a rule, if its potential is higher\nthan the upper threshold.\n4) If the potential of a point lies between the two boundaries,\nthe shortest of the distances between the new can-\ndidate to be a cluster center and all previously found\ncluster centers is decisive. The following inequality, ex-\npress the trade-off between the potential value and the\ncloseness to the previous centers\n(8)\nThis approach has been used for initial estimation of the an-\ntecedent parameters in fuzzy identification. It relies on the idea\nthat each cluster center is representative of a characteristic be-\nhavior of the system [16]. The resulting cluster centers are used\nas parameters of the antecedent parts defining the focal points\nof the rules of the model.\nB. Learning Parameters of Linear Subsystems\nFor fixed antecedent parameters the second sub-task, estima-\ntion of the parameters of the consequent linear models can be\ntransformed into a least squared problem [2]. This is accom-\nplished by eliminating the summation operation in (4) and re-\nplacing it with an equivalent vector expression of\n(9)\nwhere is a vector composed of the linear\nmodel parameters; is a vector\nof the inputs that are weighted by the normalized firing levels\nof the rules.\nFor a given set of input-output data , ,\nthe vector of linear model parameters minimizing the objective\nfunction is\n(10)\nwhere ;\n, can be estimated by the recursive least squares algo-\nrithm (called also the Kalman filter) [13], [16]\n(11)\n(12)\nwith initial conditions and , where is a large\npositive number; C is a co-variance matrix;\nis an estimation of the parameters based on data samples.\nAlternatively, the objective function (10) can be written in\nvector form as\n(10a)\nwhere the matrix and vector Y are formed by , and ,\n.\nThen the vector minimizing (10a) could be obtained by the\npseudo-inversion\n(13)\nThe objective functions (10), (10a) are globally optimal, but this\ndoes not guarantee locally adequate behavior of the sub-models\nthat form the TS model [18]. Locally meaningful sub-models\nANGELOV AND FILEV: APPROACH TO ONLINE IDENTIFICATION 487\ncould be found using the locally weighted objective function\n[18], [28]\n(14)\nwhere matrix X is formed by ; ; matrix\nis a diagonal matrix with as its elements in the main\ndiagonal.\nAn approximate solution minimizing the cost function (14)\ncan be obtained by assuming the linear subsystems are loosely\ncoupled with levels of interaction expressed by the weights\n. Then (14) can be regarded as a sum of cost functions\nwhere\n(15)\nThe solutions that minimize the weighted least square prob-\nlems expressed by the objective functions can be obtained\nby applying a weighted pseudo-inversion [18], [28]\n(16)\nAlternatively, a set of solutions to individual cost functions\n(vectors \u2019s) can be recursively calculated through the\nweighted RLS (wRLS) algorithm. In this case, a wRLS algo-\nrithm that minimizes each of the cost functions is applied\nto the linear subsystem associated with each rule (see the\nAppendix for the detailed derivation)\n(17)\n(18)\nwith initial conditions and .\nAs seen from (17), (18) when the normalized firing weight\nof certain rule is equal to 1 the wRLS algorithm transforms\ninto RLS (11), (12) based on this rule only . For the\nrule for which the normalized firing level is 0 for a certain\ntime step the parameters and the co-variance\nmatrix stay unchanged ( ; ). When\nthe update of the co-variance matrix and parameters\nare weighted by the normalized firing level.\nIII. ONLINE LEARNING OF TS MODELS\nIn Online mode, the training data are collected continuously,\nrather than being a fixed set. Some of the new data reinforce and\nconfirm the information contained in the previous data. Other\ndata, however, bring new information, which could indicate a\nchange in operating conditions, development of a fault or simply\na more significant change in the dynamic of the process [9].\nThey may posses enough new information to form a new rule\nor to modify an existing one. The value of the information they\nbring is closely related to the information the data collected so\nfar already possesses. The judgement of the informative poten-\ntial and importance of the data is made based on their spatial\nproximity, which corresponds to operating conditions, possibly\nseasonal variations or different faults.\nonline learning of ETS models includes online clustering\nunder assumption of a gradual change of the rule-base and\nmodified (weighted) recursive least squares. Due to the evo-\nlution of the model structure, the number of fuzzy rules is\nexpected to grow. This is, however, significantly slower than\nthe growth of the size of the data vectors, because the potential\nis inversely proportional to the number of the data points (5).\nA. Online Clustering Approach\nThe online clustering procedure starts with the first data point\nestablished as the focal point of the first cluster. Its coordinates\nare used to form the antecedent part of the fuzzy rule (1) using\nfor example Gaussian membership functions (2). Any other type\nof membership functions could also be used instead. Its potential\nis assumed equal to 1.\nStarting from the next data point onwards the potential of the\nnew data points is calculated recursively. As a measure of the\npotential, we use a Cauchy type function of first order\n(19)\nwhere denotes the potential of the data point cal-\nculated at time ; , denotes projection of the dis-\ntance between two data points ( and ) on the axis ( for\nand on the axis for ).\nThis function is monotonic and inversely proportional to the\ndistance and enables recursive calculation, which is important\nfor online implementation of the learning algorithm. Addition-\nally, we do not subtract a specified amount from the highest po-\ntential, but update all the potentials after a new data point is\navailable online.\nPotential of the new data sample is recursively calculated as\nfollows (see the Appendix for details)\n(20)\nwhere ; ;\n; .\nParameters and in (20) are calculated from the current\ndata point , while and are recursively updated as\n; .\nAfter the new data are available in online mode, they influ-\nence the potentials of the centers of the clusters ( , ),\nwhich are respective to the focal points of the existing rules\n( , ). The reason is that by definition the potential\ndepends on the distance to all data points, including the new\nones (the sum in the denominator by in (19) has an increasing\nnumber of components). The recursive formula for update of the\n488 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART B: CYBERNETICS, VOL. 34, NO. 1, FEBRUARY 2004\npotentials of the focal points of the existing clusters can easily\nbe derived from (19) (see the Appendix for details)\n(21)\nwhere is the potential at time of the cluster center,\nwhich is a prototype of the rule. Potentials of the new data\npoints are compared to the updated potential of the centers of\nthe existing clusters.\nIf the potential of the new data point is higher than the poten-\ntial of the existing centers then the new data point is accepted as\na new center and a new rule is formed with a focal point based\non the projection of this center on the axis ( ;\n). The rationale is that in this case the new data point\nis more descriptive, has more summarization power than all the\nother data points. It should be noted that the condition to have\nhigher potential is a very strong one. The reason is that with\nthe growing number of data, their concentration is usually de-\ncreasing except in the cases some new important region of data\nspace reflecting a new operating regime [4] or new condition\nappears. In such cases a new rule is formed, while outlying data\nare automatically rejected because their potential is significantly\nlower due to their distance from the other data. This property\nof the proposed approach is very promising for fault detection\nproblems.\nIf in addition to the previous condition (the potential of the\nnew data point is higher that the potential of all the previously\nexisting centers) the new data point is close to an old center\n(22)\nthen the new data point replaces this center .\nThis mechanism for rule-base adaptation called modification en-\nsures a replacement of a rule with another one built around the\nprojection of the new data point on the axis .\nIt should be noted that using the potential instead of the dis-\ntance to a certain rule center only [15], [27], [31] for forming the\nrule-base results in rules that are more informative and a more\ncompact rule-base. The reason is that the spatial information and\nhistory are not ignored, but are part of the decision whether to\nupgrade or modify the rule-base.\nThe proposed online clustering approach ensures an evolving\nrule-base by dynamically upgrading and modifying it while in-\nheriting the bulk of the rules ( of the rules are preserved\neven when a modification or an upgrade take place), Fig. 1.\nB. Online Recursive Estimation of Consequence\nParameters of ETS\nThe problem of increasing size of the training data is handled\nby RLS (11), (12) for the globally optimal case and wRLS (17),\n(18) for the locally optimal case. They, however, are based on the\nassumption of a constant\/unchanged rule base (fixed antecedent\nparameters). Under this assumption, the optimization problems\n(10), (10a) and (14) are linear in parameters. In ETS, however,\nthe rule-base is assumed to be gradually evolving. Therefore, the\nnumber of rules as well as the parameters of the antecedent part\nFig. 1. Schematic representation of the rule-base evolution based on the data\nsamples potential (M\u2014modification\/replacement); U\u2014up-grade of a rule).\nwill vary, though the changes are normally significantly more\nrare than the time step (the change in the data set vector).\nBecause of this evolution, the normalized firing strengths\nof the rules will change. Since this effects all the data\n(including the data collected before time of the change) the\nstraightforward application of the RLS (11), (12) or wRLS\n(17), (18) is not correct. A proper resetting of the co-variance\nmatrices and parameters initialization of the Kalman filter\n(RLS) is needed at each time a rule is added to and\/or removed\nfrom the rule base [2].\nWe propose to estimate the co-variance matrices and param-\neters of the new rule as a weighted average of the re-\nspective co-variance and parameters of the remaining rules.\nThis is possible, since the approach of rule-base innovation, we\nconsider concerns one rule only the other rules remain un-\nchanged.\n1) Global Parameter Estimation: The ETS model is used\nfor online prediction of the output based on the past inputs\n(23)\nThe following Kalman filter procedure is applied\n(24)\n(25)\nwith initial conditions\n(26)\nWhen a new rule is added to the rule-base, the Kalman filter is\nreset in the following way.\ni) Parameters of the new rule are determined by the\nweighted average of the parameters of the other rules.\nThe weights are the normalized firing levels of the\nexisting rules . The idea is to use the existing centers\nas a rule-base to approximate the initialization of the pa-\nrameters of the new rule by a weighted sum. Parameters\nof the other rules are inherited from the previous step\n(27)\nANGELOV AND FILEV: APPROACH TO ONLINE IDENTIFICATION 489\nwhere\n(27a)\nii) Co-variance matrices are reset as\n(28)\nwhere is an element of the co-variance matrix (\n; );\nis a coefficient.\nIn this way, the part of the co-variance matrix associated with\nthe new rule (last columns and last rows)\nis initialized as usual (with a large number in its main di-\nagonal and co-variance matrices respective for the rest of the\nrules (from 1 to ) are updated by multiplication of (28). The\nrationale for this is that the correction the co-variance matrices\nneeds, to approximate the role the new, rule would\nhave if it was in the rule-base from the beginning, can be repre-\nsented by (see the Appendix).\nWhen a rule is replaced by another one, which has antecedent\nparameter close to the rule being replaced, then parameters and\nco-variance matrices are inherited from the previous time step.\n2) Local Parameter Estimation: The local parameter esti-\nmation is based on the wRLS\n(29)\n(30)\nwith initial conditions\n(31)\nIn this case, the co-variance matrices are separate for each\nrule and have smaller dimensions ( ;\n). Parameters of the newly added rule are determined as\nweighted average of the parameters of the rest rules by (27a).\nParameters of the other rules are inherited ( ;\n).\nWhen a rule is replaced by another rule, which have close\nantecedent parameter (center) then parameters of all rules are\ninherited ( ; ).\nThe co-variance matrix of the newly added rule is initialized\nby\n(32)\nThe co-variance matrices of the rest rules are inherited (\n; ).\nIV. PROCEDURE FOR RULE-BASE EVOLUTION IN ETS MODELS\nThe recursive procedure for online learning of ETS models,\nintroduced in this paper, includes the following stages.\n1) Stage 1: Initialization of the rule-base structure (an-\ntecedent part of the rules).\n2) Stage 2: At the next time step reading the next data\nsample.\n3) Stage 3: Recursive calculation of the potential of each new\ndata sample to influence the structure of the rule-base.\n4) Stage 4: Recursive up-date of the potentials of old centers\ntaking into account the influence of the new data sample.\n5) Stage 5: Possible modification or up-grade of the\nrule-base structure based on the potential of the new data\nsample in comparison to the potential of the existing\nrules\u2019 centers (focal points).\n6) Stage 6: Recursive calculation of the consequent param-\neters.\n7) Stage 7: Prediction of the output for the next time step by\nthe ETS model.\nThe execution of the algorithm continues for the next time step\nfrom stage 2. It should be noted that the first output to be pre-\ndicted is .\nStage 1. The rule-base could contain one single rule only,\nbased, for example, on the first data sample. Then\n(33)\nwhere is the first cluster center; is focal point of the first\nrule being a projection of on the axis .\nIn principle, the rule-base could be initialized by existing ex-\npert knowledge. Generally, however, it could be based on the\noff-line identification approaches, described in Section II. In this\ncase\n(34)\nwhere denotes the number of rules defined initially off-line.\nStages 2 to 7 are performed online. They form the distinctive\ncharacteristics of the proposed approach.\nStage 2. At the next time step the new data\nsample is collected.\nAt stage 3 the potential of each new data sample is recursively\ncalculated by (20). The use of already calculated values and\nleads to significant time and calculation savings because\n(19) is normally calculated from large matrices (the number of\ntraining data in online mode is continuously growing). At the\nsame time, they have accumulated information regarding the\nspatial proximity of all previous data.\nAt stage 4 the potentials of the focal points (centers) of the\nexisting clusters\/rules are recursively updated by (21).\nAt stage 5 the potential of the new data sample is compared to\nthe updated potential of existing centers and a decision whether\nto modify or up-grade the rule-base is taken.\na)\nIF (the potential new data point\nis higher than the potential of the\n490 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART B: CYBERNETICS, VOL. 34, NO. 1, FEBRUARY 2004\nexisting centers: ;\n)\nAND [the new data point is close to\nan old center (22)]\nTHEN the new data point re-\nplaces it.\nIn this case, the new data point is used as a prototype of\na focal point (let us suppose that it has index\n(35)\nConsequence parameters and co-variance matrices are in-\nherited from the rule to be replaced\n(36)\nIt should be noted that when a rule is replaced by another\nrule the weights are changing according to (4) and\nthe summation in the denominator in (4) should change.\naddends in this summation are the same and only\none change. Moreover, since the new center is close to the\nreplaced one by definition (22), this change is marginal.\nThe disturbance caused to the RLS by this change could\nbe ignored, because the Kalman filter is able to cope with\nthis disturbance starting from the existing estimations of\nthe parameters and co-variance matrices. This is also illus-\ntrated by the experimental results (next section).\nb)\nELSE IF (the potential of the new\ndata point is higher than the po-\ntential of the existing centers:\n; )\nTHEN it is added to the rule-base\nas a new rule\u2019s center.\nIn this case, the new data point becomes a prototype of\na focal point of a new rule\n(37)\nConsequence parameters and co-variance matrices are\nreset by (27)\u2013(28) or (32), respectively, for the global or\nlocal estimation.\nEND IF\nAt Stage 6 parameters of the consequence are recursively up-\ndated by RLS (24), (25) with initializations (26) for globally op-\ntimal parameters or by wRLS (29), (30) with initializations (31)\nfor locally optimal parameters.\nIn the first case the cost function (10) is minimized, which\nguarantees globally optimal values of the parameters, while in\nthe second case the locally weighted cost function (15) is mini-\nmized and locally meaningful parameters are obtained.\nAt Stage 7 the output for the next time step is predicted\nby (23).\nThe algorithm continues from stage 2 by reading the next data\nsample at the next time step.\nFig. 2. Block-diagram of the online identification of ETS models.\nA graphical representation of the algorithm that realizes the\nproposed approach is demonstrated in Fig. 2. All steps are non-\niterative.\nUsing the approach, a transparent, compact and accurate\nmodel can be found by rule base evolution based on experi-\nmental data with the simultaneous recursive estimation of the\nfuzzy set parameters. It is interesting to note that the rate of\nupgrade with new rules does not lead to an excessively large\nrule base in comparison to [15], [20], [27], [31]. The reason\nfor this is that the condition for the new data point to have\nhigher potential (19), (20) than the focal points of rules of all\nexisting rules is a hard requirement. Additionally, the possible\nproximity of a candidate center to the already existing focal\npoints leads to just a replacement of the existing focal point, i.e.\nmodification of its coordinates without enlarging the rule-base\nsize.\nV. EXPERIMENTAL RESULTS\nThe new algorithm has been tested on the data from a\nfan-coil sub-system of an air-conditioning system serving a real\nbuilding. Training data were collected on August 3, and August\n19, 1998 (courtesy of ASHRAE for the use of data, generated\nfrom the ASHRAE funded research project RP1020).\nANGELOV AND FILEV: APPROACH TO ONLINE IDENTIFICATION 491\nFig. 3. Experimental set-up.\nFig. 4. Absolute error in prediction the valve position using global parameter\nestimation.\nThe ETS model of the position of the valve controlling\nthe water flow rate to a fan-coil sub-system has been consid-\nered (Fig. 3). The model makes online prediction of the valve\nposition steps ahead (the step in this realization is one\nminute). The present value of is one of the inputs of the model\nconsidered, while the other inputs are the present and past (one\nstep back) values of the air inlet and supply to the zone\ntemperature\n(38)\n(39)\nThe coil cools the warm air that flows on. The cool air is used\nto maintain comfortable conditions in an occupied Zone. One of\nthe principle loads on the coil is generated due to the supply of\nambient air required to maintain a minimum standard of indoor\nair quality.\nThe results of the online modeling using the global identifi-\ncation criteria (10) are shown in Fig. 4.\nThe model evolves to five rules and, respectively, five linear\nsub-models with six parameters each. The centers of the mem-\nbership functions describing the fuzzy sets of the antecedent part\nof the rules are tabulated in Table I.\nThe RMS error is 0.015 59 and calculations take a fraction of\na second for each new data point. The parameters are estimated\nin real time by the RLS (24), (25). Their evolution is depicted\nin Fig. 5\nTABLE I\nFUZZY SETS OF THE ANTECEDENTS\nFig. 5. Evolution of parameters of the linear sub-models (global estimation).\nIt is interesting to note that in time instants of adding new\nrule the changes of the parameters by (27), (27a) are not drastic\n(Fig. 5). More significant sudden changes occur in the norm of\nthe co-variance matrix because of the resetting (28) seen from\nFig. 6.\nThe ETS model has evolved to the same 5 rules when the\nlocal identification (14) is applied. In addition, the RMS error is\nmarginally higher (0.016 04) (see Figs. 7\u20139).\nThe evolution of the parameters in this case is smoother and\nthe parameters are locally more transparent.\nA similar problem of modeling temperature difference across\nthe cooling coil (Fig. 3) has been considered. The following\nmeasurements have been used:\n1) flow rate of the air entering the coil ;\n2) moisture content of the air entering the coil ;\n3) temperature of the chilled water ;\n4) control signal to the valve .\nThe temperature difference (drop) across the coil is pre-\ndicted in real time\n(40)\n(41)\nThe data is from a full-scale air-conditioning test facility and\ncover two months (May and August) over two seasons (summer\nand spring). The data was collected with the system operating\n492 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART B: CYBERNETICS, VOL. 34, NO. 1, FEBRUARY 2004\nFig. 6. Evolution of the norm of the co-variance (global estimation).\nFig. 7. Absolute error in prediction of the valve position using locally optimal\nlinear models.\nunder normal conditions on days in August and May respec-\ntively.\nThe proposed approach demonstrates that it is possible to\nbuild ETS model online from data of one season (summer) and\nthen successfully to use this model making gradual changes\nto its structure and parameters for another season (spring).\nThe RMS error is about half a degree centigrade (0.522 74 ;\nnondimensional error index is 0.091 85). The model upgrades\nits structure to four rules with a gradual evolution.\nThe results are similar for the global and local estimation.\nThe RMS error in local estimation is 0.656 57 [Fig. 10(b)].\nThe centers of the antecedent part at the end of the estimation\nare tabulated in Table II (they are the same for both type of\nestimation).\nThe robustness of the eTS model has been tested by consid-\nering 25% additive normally distributed random noise to the\ncontrol signal to the valve, flow rate of the air entering the coil,\nand the moisture content of the air entering the coil. The error\nin the prediction of the temperature difference across the coil\nwas higher, but in the same order of magnitude (\n; ). As it is seen in Fig. 10(c) there\nare high frequency components in the output prediction, because\nthe output from the eTS model is a function of the inputs, most\nFig. 8. Evolution of parameters of the linear sub-models (local estimation).\nFig. 9. Evolution of the norm of the co-variance matrices (local estimation).\nof which have been noisy. One can find a more smooth predic-\ntion by proper tuning of the radius of influence of the clusters\n[parameter r in (2)]. It can significantly decrease the effect of\nthe noise\u2014in a noisy environment a larger radius of influence\nwill prevent the algorithm from creating new clustering cen-\nters due to noise. In a limiting case in a very noisy environ-\nment the algorithm will end up with just a few cluster centers\nand vice versa. This is illustrated by the result, which have been\nachieved by increasing the value of the radius in this experiment\nfrom 0.3 to 0.5. It resulted in reduction of the RMSE to 0.6213\n. A further reduction of the radius to 0.8 lead\nto ; and only three rules. We\nbelieve that a straightforward upgrade of the proposed algorithm\ncan be obtained by adding a set of application specific rules au-\ntomatically adjusting the radius of influence to the noise level.\nIt can be mentioned that the cluster centers are weighted by\nthe frequencies (the number data points belonging with a high\ndegree of membership to the particular cluster). This practically\nexcludes the chance of an outlier to become a cluster center and\nto influence the output of the model. In addition, the cluster cen-\nters with low potential are periodically replaced. These intrinsic\nANGELOV AND FILEV: APPROACH TO ONLINE IDENTIFICATION 493\n(a) (b)\n(c)\nFig. 10 (a) Prediction of the temperature difference across a coil (global criteria). (b) Prediction of the temperature difference across a coil (local criteria). (c)\nPrediction of the temperature difference across a coil in the presence of random noise.\nTABLE II\nCENTERS OF THE ANTECEDENT\nmechanisms of the ETS model design acts as a safeguard to the\nnoisy data, which is illustrated in this example (see Figs. 11 and\n12).\nThe proposed approach has been tested on a benchmark\nproblem: the Mackey\u2013Glass chaotic time series prediction and\nthe results compared to those generated by alternative tech-\nniques for online learning TS models, published in references\n[31] and [34]. The chaotic time series is generated from the\nMackey\u2013Glass differential delay equation defined by [16], [31]\nThe aim is using the past values of to predict some future\nvalue of . We assume , and the value of the\nsignal 85 steps ahead is predicted (same as in [31])\nbased on the values of the signal at the current moment, 6, 12,\nand 18 steps back\nThe validation data set consists of 500 data samples. The same\nnondimensional error index (NDEI) defined as the ratio of the\nroot mean square error over the standard deviation of the target\ndata is used as in [31]\u2013[34] to compare model performance.\nThe results summarized in Table III and Fig. 13 show that\nthe new approach can yield a compact model with favorably\ncomparable NDEI. It should be noted that TS models with lower\nNDEI have been reported in [31]\u2013[34]. The number of rules\n(nodes or units) of these models is, however, in the range of\na thousand which significantly undermines their transparency\nan interpretability. ETS model has evolved in online mode to\n113 transparent rules with . It should also be\nnoted that similar approach to the one presented in this paper,\n494 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART B: CYBERNETICS, VOL. 34, NO. 1, FEBRUARY 2004\n(a)\n(b)\nFig. 11 Evolution of parameters of consequent part (global criteria).\nbut using nonrecursive moving window [35] yields for the same\nproblem evolving to 35 rules.\nIn order to test the robustness of the eTS model a 5% random\nnoise has been added to the standard Mackey\u2013Glass time series.\nThe eTS model has evolved to 124 rules with different centers\nand the . From Fig. 13(d) and Fig. 13(f) it\ncan be seen that initially the error is higher, but the TS model\nquickly up-grades its structure and reduces the error. Fig. 13(e)\nillustrates the evolution of the parameters of the first six fuzzy\nrules, which is similar to the case when no noise is considered\nin the data.\nVI. CONCLUSION\nAn approach to online identification of ETS models is pro-\nposed in the paper. It is computationally effective, as it does\nnot require re-training of the whole model. It is based on re-\ncursive, noniterative building of the rule base by unsupervised\n(a)\n(b)\nFig. 12 (a) Evolution of the norm of the co-variance (global estimation). (b)\nEvolution of the norm of the co-variance (local estimation).\nTABLE III\nCOMPARISON OF ETS WITH OTHER EVOLVING MODELS\nlearning. The rule-based model evolves by replacement or up-\ngrade of rules and parameter estimation.\nThe adaptive nature of this model in addition to the highly\ntransparent and compact form of fuzzy rules makes them\na promising candidate for online modeling and control of\ncomplex processes competitive to neural networks. The main\nadvantages of the approach are:\nANGELOV AND FILEV: APPROACH TO ONLINE IDENTIFICATION 495\n(a) (b)\n(c) (d)\n(e) (f)\nFig. 13 (a) Prediction (85 steps ahead) of the Mackey-Glass chaotic time series by eTS model. (b) Evolution of parameters of linear sub-models for the first six\nfuzzy rules. (c) Evolution of the norm of the co-variance. (d) Prediction of the noisy Mackey-Glass chaotic time series by eTS model. (e) Evolution of parameters\nof linear sub-models for the first six fuzzy rules (noisy data). (f) Absolute error in prediction of the noisy Mackey-Glass chaotic time series by eTS model.\n496 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART B: CYBERNETICS, VOL. 34, NO. 1, FEBRUARY 2004\n1) it can develop\/evolve an existing model when the data\npattern changes, while inheriting the rule base;\n2) it can start to learn a process from a single data sample and\nimprove the performance of the model predictions online;\n3) it is noniterative and recursive and hence computation-\nally very effective (the time necessary for calculation is a\nfraction of a second for a new data sample using a stan-\ndard PC).\nThe proposed concept, has wide implications for many fields,\nincluding nonlinear adaptive control, fault detection and diag-\nnostics, performance analysis of dynamical systems, time-se-\nries and forecasting, knowledge extraction, intelligent agents,\nbehavior and modeling. The results illustrate the viability, effi-\nciency and the potential of the approach when used with a lim-\nited amount of initial information, especially important in au-\ntonomous systems and robotics. Future implementation in var-\nious engineering problems is under consideration.\nAPPENDIX\nA. WEIGHTED RECURSIVE LEAST SQUARES ALGORITHM\nThe wRLS algorithm could be derived from the weighted\npseudo-inversion (16) expressing the matrices as sums and re-\ngrouping the components in a similar way as RLS is derived\nfrom LS [2], [30]\n(A1)\nwhere is the co-variance matrix.\nUsing this expression for the estimate based on data and\nregrouping we have\n(A2)\nSubstituting (A2) in (A1) and using the matrix inversion Lemma\n(Lemma 3.1 from [30], p. 65) we arrive at\n(A3)\nwhich is equivalent to (17). In a similar way, from the definition\nof the co-variance matrix for the estimation based on data we\nhave\n(A4)\nwhich is equivalent to (18).\nB. RECURSIVE POTENTIALS CALCULATION\nStarting form the formula of the potential (19) and expressing\nthe projections of the distances in an explicit form for the time\nstep we have\n(A5)\nRegrouping we have (A6) shown at the bottom of the page,\nwhich is equivalent to (20).\nC. RECURSIVE UPDATE OF FOCAL POINT\u2019S POTENTIAL\nIf a data point is accepted to be the focal point of a cluster\/rule\nat time ( ; for ) then its potential is\ncalculated according to (19) as\n(A7)\nWe can re-order expressing the sums explicitly\n(A8)\nAt the next time step the potential have to be updated in\norder to accommodate the influence of the new data on this\ncenter\n(A9)\nBy substituting (A8) into (A9) we have\n(A10)\nBy regrouping, we arrive at (A11) shown at the top of the next\npage, which is equivalent to (21).\nD. CO-VARIANCE MATRIX UPDATE\nLet us introduce a vector of inputs that are weighted by the\nnonnormalized firing levels of the rules similarly to the nota-\ntions used in (10) as\n(A12)\n(A6)\nANGELOV AND FILEV: APPROACH TO ONLINE IDENTIFICATION 497\n(A11)\nThen it is obvious that\n(A12a)\nFrom the expression for the Kalman filter for the update of the\nco-variance matrices (12) we have\n(A13)\nor expressing the history until time in an explicit way\n(A14)\nwhere ; ;\nLet us suppose that the rule added at\nthe step had been added from the beginning. Then the\nco-variance matrix at time would be\nor\n(A15)\nwhere ; . It can be seen that\nadding a rule at time step results in a corruption of the co-vari-\nance matrix, which is expressed in an increase of the denomi-\nnator of the part subtracted from . It should be noted\nthat the values of and are strongly less than 1 (because\nthey are quadratic forms of membership functions). could be\na big number since it is a quadratic form of the input data multi-\nplied by the co-variance matrix. is bigger then since it is a\nsum of positive membership functions, while is only one\nMF. F is also bigger than if . There-\nfore, the role of the addends would be more significant only if\nall values of (for all past time steps) tend to 0 or the co-vari-\nance matrix tends to zero. The practical tests with a number of\nfunctions illustrate that the corruption of the covariance matrix\nby the addition of a new rule is marginal.\nWe approximate this (normally small) influence by an in-\nverse mean of average type of correction. The logic is following.\nFrom (A14), (A15) we have that the corrupted co-variance ma-\ntrix is a function of the original one\n(A16)\nAn approximation of the function could be the inverse squared\nmean, since the role of the corruption will decrease with increase\nof and this is a squared dependence\n(A17)\nwhich is equivalent to (25).\nREFERENCES\n[1] D. Specht, \u201cA general regression neural network,\u201d IEEE Trans. Neural\nNetworks, vol. 2, pp. 568\u2013576, Nov. 1991.\n[2] L. Ljung, System Identification, Theory for the User. Englewood\nCliffs, NJ: Prentice-Hall, 1987.\n[3] R. R. Yager and D. P. Filev, Essentials of Fuzzy Modeling and Con-\ntrol. New York: Wiley, 1994.\n[4] T. A. Johanson and R. Murray-Smith, \u201cOperating regime approach to\nnonlinear modeling and control,\u201d in Multiple Model Approaches to Mod-\neling and Control, R. Murray-Smith and T. A. Johanson, Eds. Hants,\nU.K.: Taylor Francis, pp. 3\u201372.\n[5] J. S. R. Jang, \u201cANFIS: adaptive network-based fuzzy inference sys-\ntems,\u201d IEEE Trans. Syst., Man Cybern., vol. 23, pp. 665\u2013685, May\/June\n1993.\n[6] C. K. Chiang, H.-Y. Chung, and J. J. Lin, \u201cA self-learning fuzzy logic\ncontroller using genetic algorithms with reinforcements,\u201d IEEE Trans.\nFuzzy Syst., vol. 5, pp. 460\u2013467, Aug. 1997.\n[7] P. P. Angelov, V. I. Hanby, R. A. Buswell, and J. A. Wright, \u201cAuto-\nmatic generation of fuzzy rule-based models from data by genetic al-\ngorithms,\u201d in Advances in Soft Computing, R. John and R. Birkenhead,\nEds. Heidelberg, Germany: Springer-Verlag, 2001, pp. 31\u201340.\n[8] F. Hoffmann and G. Pfister, \u201cLearning of a fuzzy control rule base using\nmessy genetic algorithms,\u201d in Studies in Fuzziness and Soft Computing,\nF. Herrera and J.L. Verdegay, Eds. Heidelberg, Germany: Physica\nVerlag, 1996, vol. 8, pp. 279\u2013305.\n[9] P. P. Angelov, Evolving Rule-Based Models: A Tool for Design of Flex-\nible Adaptive Systems. Heidelberg, Germany: Springer-Verlag, 2002.\n[10] B. Carse, T. C. Fogarty, and A. Munro, \u201cEvolving fuzzy rule-based con-\ntrollers using GA,\u201d Fuzzy Sets Syst., vol. 80, pp. 273\u2013294, 1996.\n[11] P. P. Angelov, V. I. Hanby, and J. A. Wright, \u201cHVAC systems simulation:\na self-structuring fuzzy rule-based approach,\u201d Int. J. Architectural Sci.,\nvol. 1, no. 1, pp. 49\u201358, 2000.\n[12] K. Shimojima, T. Fukuda, and Y. Hasegawa, \u201cSelf-tuning fuzzy mod-\neling with adaptive membership function, rules, and hierarchical struc-\nture based on genetic algorithm,\u201d Fuzzy Sets Syst., vol. 71, pp. 295\u2013309,\n1995.\n[13] T. Takagi and M. Sugeno, \u201cFuzzy identification of systems and its appli-\ncation to modeling and control,\u201d IEEE Trans. Syst., Man Cybern., vol.\n15, pp. 116\u2013132, 1985.\n[14] M. Sugeno and M. Yasukawa, \u201cA fuzzy logic based approach ot quali-\ntative modeling,\u201d IEEE Trans. Fuzzy Syst., vol. 1, pp. 7\u201331, Feb. 1993.\n[15] D. P. Filev, T. Larsson, and L. Ma, \u201cIntelligent control for automotive\nmanufacturing-rule based guided adaptation,\u201d in Proc. IEEE Conf.\nIECON\u201900, Nagoya, Japan, Oct. 2000, pp. 283\u2013288.\n[16] S. L. Chiu, \u201cFuzzy model identification based on cluster estimation,\u201d J.\nIntel. Fuzzy Syst., vol. 2, pp. 267\u2013278, 1994.\n[17] J. Bezdek, \u201cCluster validity with fuzzy sets,\u201d J. Cybern., vol. 3, no. 3,\npp. 58\u201371, 1974.\n[18] J. Yen, L. Wang, and C. W. Gillespie, \u201cImproving the interpretability\nof TSK fuzzy models by combining global and local learning,\u201d IEEE\nTrans. Fuzzy Syst., vol. 6, pp. 530\u2013537, Nov. 1998.\n[19] EUNITE: EUropean network on intelligent technologies for smart adap-\ntive systems, p. 4, 2000.\n[20] F.-J. Lin, C.-H. Lin, and P.-H. Shen, \u201cSelf-constructing fuzzy neural net-\nwork speed controller for permanent-magnet synchronous motor drive,\u201d\nIEEE Trans. Fuzzy Syst., vol. 9, pp. 751\u2013759, Oct. 2001.\n498 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART B: CYBERNETICS, VOL. 34, NO. 1, FEBRUARY 2004\n[21] H. R. Berenji, \u201cA reinforcement learning-based architecture for fuzzy\nlogic control,\u201d Int. J. Approx. Reasoning, vol. 6, pp. 267\u2013292, 1992.\n[22] G. G. Yen and P. Meesad, \u201cAn effective neuro-fuzzy paradigm for ma-\nchinery condition health monitoring,\u201d in Proc. IEEE Int. Joint Conf.\nIJCNN\u201999, Washington, DC, 1999, pp. 1567\u20131572.\n[23] D. E. Gustafson and W. C. Kessel, \u201cFuzzy clustering with a fuzzy co-\nvariance matrix,\u201d in Proc. IEEE Control Decision Conf., San Diego, CA,\n1979, pp. 761\u2013766.\n[24] F. Klawon and P. E. Klement, \u201cMathematical analysis of fuzzy classi-\nfiers,\u201d Lect. Notes Comp. Sci., vol. 1280, pp. 359\u2013370, 1997.\n[25] R. R. Yager and D. P. Filev, \u201cLearning of fuzzy rules by mountain clus-\ntering,\u201d in Proc. SPIE Conf. Applicat. Fuzzy Logic Technol., Boston,\nMA, 1993, pp. 246\u2013254.\n[26] K. L. Anderson, G. L. Blackenship, and L. G. Lebow, \u201cA rule-based\nadaptive PID controller,\u201d in Proc. 27th IEEE CDC\u201988, 1988, pp.\n564\u2013569.\n[27] D. P. Filev, \u201cRule-base guided adaptation for mode detection in process\ncontrol,\u201d in Proc. Joint 9th IFSA World Congr.\/20th NAFIPS Annu.\nConf., Vancouver, BC, Canada, July 2001, pp. 1068\u20131073.\n[28] R. Babuska, \u201cFuzzy modeling and identification,\u201d Ph.D. thesis, Univ. of\nDelft, Delft, The Netherlands, 1996.\n[29] J. Moody and C. J. Darken, \u201cFast learning in networks of locally-tuned\nprocessing units,\u201d Neural Computat., vol. 1, pp. 281\u2013294, 1989.\n[30] K. J. Astroem and B. Wittenmark, Adaptive Control. Reading, MA:\nAddison-Wesley, 1989.\n[31] N. K. Kasabov and Q. Song, \u201cDENFIS: dynamic evolving neural-fuzzy\ninference system and its application for time-series prediction,\u201d IEEE\nTrans. Fuzzy Syst., vol. 10, pp. 144\u2013154, Apr. 2002.\n[32] J. Platt, \u201cA resource allocation network for function interpolation,\u201d\nNeural Computat., vol. 3, pp. 213\u2013225, 1991.\n[33] D. Deng and N. Kasabov, \u201cEvolving self-organizing maps for online\nlearning, data analysis and modeling,\u201d in Proc. IJCNN\u20192000 Neural Net-\nworks, Neural Comput.: New Challenges Perspectives New Millennium,\nvol. VI, S.-I. Amari, C. L. Giles, M. Gori, and V. Piuri, Eds., New York,\nNY, 2000, pp. 3\u20138.\n[34] N. Kasabov, \u201cEvolving fuzzy neural networks\u2014algorithms, applications\nand biological motivation,\u201d in Methodologies for the Conception, De-\nsign and Application of Soft Computing, T. Yamakawa and G. Mat-\nsumoto, Eds, Singapore: World Scientific, 1998, pp. 271\u2013274.\n[35] P. Angelov and R. Buswell, \u201cIdentification of evolving fuzzy rule-based\nmodels,\u201d IEEE Trans. Fuzzy Syst., vol. 10, pp. 667\u2013677, Oct. 2002.\nPlamen P. Angelov (M\u201999) received the Ph.D. degree from the Bulgarian\nAcademy of Science, Bulgaria, in 1993.\nSince June 2003, he has been a Lecturer in the Department of Communica-\ntions Systems, Lancaster University, Lancaster, U.K. He was a Research Fellow\nin Loughborough University, U.K., from 1998 to 2003. He has been Visiting\nResearch Fellow in CESAME, Catholic University of Louvain, Belgium, in\n1997, Hannover University and HKI, Jena, Germany, from 1995 to 1996. He au-\nthored the monograph Evolving Rule based Models: A Tool for Design of Flex-\nible Adaptive Systems (Heidelberg, Germany: Springer, 2002). His research has\nbeen funded by the EC, ASHARE, Research Councils of UK (EPSRC), Ger-\nmany (DAAD and DFG), Belgium, Italy (CNR), Bulgaria (NFSI). His research\ninterests are in intelligent data processing, particularly in evolving rule-based\nmodels, self-organizing and autonomous systems, evolutionary algorithms, op-\ntimization and optimal control in a fuzzy environment.\nDr. Angelov has been in the program and organizing committees of several\nconferences, including IFSA-2003, GECCO-2002, RASC-2002, FUBEST\u201994\nand \u201996, BioPS\u201994, \u201995, \u201997.\nDimitar P. Filev (M\u201995\u2013SM\u201997) received the PhD. degree in electrical engi-\nneering from the Czech Technical University, Czechoslovakia, in 1979.\nHe is a Staff Technical Specialist and a Manager of the Knowledge Based Sys-\ntems and Control Department with Advanced Manufacturing Technology De-\nvelopment, Ford Motor Company specializing in industrial intelligent systems\nand technologies for control, diagnostics and decision making. Prior to joining\nFord, he was Professor of information systems and Senior Research Associate\nat the Machine Intelligence Institute, Iona College, and Associate Professor at\nthe Bulgarian Academy of Sciences. He is conducting research in control theory\nand applications, modeling of complex systems, and intelligent modeling and\ncontrol. He has published three books and over 150 articles in refereed journals\nand conference proceedings. He holds nine U.S. patents.\nDr. Filev received the 1995 Award for Excellence from MCB University Press\nand three Henry Ford Technology Awards. He is an Associate Editor of the IEEE\nTRANSACTIONS. ON FUZZY SYSTEMS, the International Journal of General Sys-\ntems, and the International Journal of Approximate Reasoning.\n"}