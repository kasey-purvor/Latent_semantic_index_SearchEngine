{"doi":"10.1109\/IROS.2008.4650701","coreId":"55855","oai":"oai:eprints.lincoln.ac.uk:1679","identifiers":["oai:eprints.lincoln.ac.uk:1679","10.1109\/IROS.2008.4650701"],"title":"An adaptive appearance-based map for long-term topological localization of mobile robots","authors":["Dayoub, Feras","Duckett, Tom"],"enrichments":{"references":[{"id":18439127,"title":"A robot exploration and mapping strategy based on a semantic hierarchy of spatial representations. Toward Learning Robots.","authors":[],"date":"1993","doi":"10.1016\/0921-8890(91)90014-c","raw":"B. Kuipers and Y. T. Byun. A robot exploration and mapping strategy based on a semantic hierarchy of spatial representations. Toward Learning Robots. MIT Press, Cambridge, Massachusetts, page 4763, 1993.","cites":null},{"id":18439140,"title":"Appearance-based place recognition for topological localization. In","authors":[],"date":"2000","doi":"10.1109\/robot.2000.844734","raw":"I. Ulrich and I. Nourbakhsh. Appearance-based place recognition for topological localization. In Proc. IEEE International Conference on Robotics and Automation (ICRA), 2000.","cites":null},{"id":18439146,"title":"Auxiliary particle \ufb01lter robot localization from high-dimensional sensor observations.","authors":[],"date":"2002","doi":"10.1109\/robot.2002.1013331","raw":"N. Vlassis, B. Terwijn, and B. Krose. Auxiliary particle \ufb01lter robot localization from high-dimensional sensor observations. In Proc. IEEE International Conference on Robotics and Automation (ICRA), 2002.","cites":null},{"id":18439118,"title":"Human memory: A proposed system and its control processes.","authors":[],"date":"1968","doi":"10.1016\/b978-0-12-121050-2.50006-5","raw":"R.C. Atkinson and R.M. Shiffrin. Human memory: A proposed system and its control processes. In K.W. Spence & J.T. Spence (Eds.), The Psychology of Learning and Motivation, 2:89\u2013195, 1968.","cites":null},{"id":18439133,"title":"Image-based Monte Carlo localisation with omnidirectional images.","authors":[],"date":"2004","doi":"10.1016\/j.robot.2004.05.003","raw":"E. Menegatti, M. Zoccarato, E. Pagello, and H. Ishiguro. Image-based Monte Carlo localisation with omnidirectional images. Robotics and Autonomous Systems, 48(1):17\u201330, 2004.","cites":null},{"id":18439144,"title":"Incremental Topological Mapping Using Omnidirectional Vision. In","authors":[],"date":"2006","doi":"10.1109\/iros.2006.282583","raw":"C. Valgren, A. Lilienthal, and T. Duckett. Incremental Topological Mapping Using Omnidirectional Vision. In Proc. IEEE International Conference on Intelligent Robots and Systems (IROS), 2006.","cites":null},{"id":18439139,"title":"Localization of mobile robots with omnidirectional vision using particle \ufb01lter and iterative sift. In","authors":[],"date":"2005","doi":"10.1016\/j.robot.2006.04.018","raw":"H. Tamimi, H. Andreasson, A. Treptow, T. Duckett, and A. Zell. Localization of mobile robots with omnidirectional vision using particle \ufb01lter and iterative sift. In Proc. European Conference on Mobile Robots (ECMR), 2005.","cites":null},{"id":18439137,"title":"Mobile Robot Localization and Mapping with Uncertainty using Scale-Invariant Visual Landmarks.","authors":[],"date":"2002","doi":"10.1177\/027836402128964611","raw":"S. Se, D. Lowe, and J. Little. Mobile Robot Localization and Mapping with Uncertainty using Scale-Invariant Visual Landmarks. The International Journal of Robotics Research, 21(8):735, 2002.","cites":null},{"id":18439129,"title":"Object recognition from local scale-invariant features. In","authors":[],"date":"1999","doi":"10.1109\/iccv.1999.790410","raw":"D.G. Lowe. Object recognition from local scale-invariant features. In Proc. IEEE International Conference on Computer Vision (ICCV), 1999.","cites":null},{"id":18439121,"title":"Omnidirectional Vision Based Topological Navigation.","authors":[],"date":"2007","doi":"10.1002\/rob.10130","raw":"T. Goedem\u00b4 e, M. Nuttin, T. Tuytelaars, and L. Van Gool. Omnidirectional Vision Based Topological Navigation. International Journal of Computer Vision, 74(3):219\u2013236, 2007.","cites":null},{"id":18439123,"title":"Omnivisionbased probabilistic self-localization for a mobile shopping assistant continued.","authors":[],"date":"2003","doi":"10.1109\/iros.2003.1248857","raw":"H.M. Gross, A. Koenig, C. Schroeter, and H.J. Boehme. Omnivisionbased probabilistic self-localization for a mobile shopping assistant continued. In Proc. IEEE International Conference on Intelligent Robots and Systems (IROS), 2003.","cites":null},{"id":18439148,"title":"ose. From images to rooms.","authors":[],"date":"2007","doi":"10.1016\/j.robot.2006.12.005","raw":"Z. Zivkovic, O. Booij, and B. Kr\u00a8 ose. From images to rooms. Robotics and Autonomous Systems, 55(5):411\u2013418, 2007.","cites":null},{"id":18439131,"title":"Robust wide-baseline stereo from maximally stable extremal regions.","authors":[],"date":"2004","doi":"10.5244\/c.16.36","raw":"J. Matas, O. Chum, M. Urban, and T. Pajdla. Robust wide-baseline stereo from maximally stable extremal regions. Image and Vision Computing, 22(10):761\u2013767, 2004.","cites":null},{"id":18439135,"title":"Scalable recognition with a vocabulary tree.","authors":[],"date":"2006","doi":"10.1109\/cvpr.2006.264","raw":"D. Nister and H. Stewenius. Scalable recognition with a vocabulary tree. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2006.","cites":null},{"id":18439119,"title":"SURF: Speeded up robust features. In","authors":[],"date":"2006","doi":"10.1007\/11744023_32","raw":"H. Bay, T. Tuytelaars, and L. Van Gool. SURF: Speeded up robust features. In Proc. European Conference on Computer Vision (ECCV), 2006.","cites":null},{"id":18439120,"title":"Topological mapping, localization and navigation using image collections.","authors":[],"date":"2007","doi":"10.1109\/iros.2007.4399123","raw":"C. Fraundorfer, F. Engels and D. Nister. Topological mapping, localization and navigation using image collections. In Proc. IEEE International Conference on Intelligent Robots and Systems (IROS), 2007.","cites":null},{"id":18439138,"title":"Video Google: a text retrieval approach to object matching in videos.","authors":[],"date":"2003","doi":"10.1109\/iccv.2003.1238663","raw":"J. Sivic and A. Zisserman. Video Google: a text retrieval approach to object matching in videos. In Proc. IEEE International Conference on Computer Vision (ICCV), 2003.","cites":null},{"id":18439125,"title":"Vision based topological Markov localization. In","authors":[],"date":"2004","doi":"10.1109\/robot.2004.1308033","raw":"J. Kosecka and F. Li. Vision based topological Markov localization. In Proc. IEEE International Conference on Robotics and Automation (ICRA), 2004.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2008-09-22","abstract":"This work considers a mobile service robot which uses an appearance-based representation of its workplace as a map, where the current view and the map are used to estimate the current position in the environment. Due to the nature of real-world environments such as houses and offices, where the appearance keeps changing, the internal representation may become out of date after some time. To solve this problem the robot needs to be able to adapt its internal representation continually to the changes in the environment. This paper presents a method for creating an adaptive map for long-term appearance-based localization of a mobile robot using long-term and short-term memory concepts, with omni-directional vision as the external sensor","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/55855.pdf","fullTextIdentifier":"http:\/\/eprints.lincoln.ac.uk\/1679\/1\/IROS08.pdf","pdfHashValue":"4d65e2f2f71b535ce20df6ebb8c1221a84f98e03","publisher":"IEEE","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lincoln.ac.uk:1679<\/identifier><datestamp>\n      2015-11-06T21:05:17Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47373030<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47343030<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47373630<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47373430<\/setSpec><setSpec>\n      74797065733D636F6E666572656E63655F6974656D<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lincoln.ac.uk\/1679\/<\/dc:relation><dc:title>\n        An adaptive appearance-based map for long-term topological localization of mobile robots<\/dc:title><dc:creator>\n        Dayoub, Feras<\/dc:creator><dc:creator>\n        Duckett, Tom<\/dc:creator><dc:subject>\n        G700 Artificial Intelligence<\/dc:subject><dc:subject>\n        G400 Computer Science<\/dc:subject><dc:subject>\n        G760 Machine Learning<\/dc:subject><dc:subject>\n        G740 Computer Vision<\/dc:subject><dc:description>\n        This work considers a mobile service robot which uses an appearance-based representation of its workplace as a map, where the current view and the map are used to estimate the current position in the environment. Due to the nature of real-world environments such as houses and offices, where the appearance keeps changing, the internal representation may become out of date after some time. To solve this problem the robot needs to be able to adapt its internal representation continually to the changes in the environment. This paper presents a method for creating an adaptive map for long-term appearance-based localization of a mobile robot using long-term and short-term memory concepts, with omni-directional vision as the external sensor.<\/dc:description><dc:publisher>\n        IEEE<\/dc:publisher><dc:date>\n        2008-09-22<\/dc:date><dc:type>\n        Conference or Workshop contribution<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lincoln.ac.uk\/1679\/1\/IROS08.pdf<\/dc:identifier><dc:identifier>\n          Dayoub, Feras and Duckett, Tom  (2008) An adaptive appearance-based map for long-term topological localization of mobile robots.  In: International Conference on Intelligent Robots and Systems 2008, 22-26 September 2008, Nice, France.  <\/dc:identifier><dc:relation>\n        10.1109\/IROS.2008.4650701<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lincoln.ac.uk\/1679\/","10.1109\/IROS.2008.4650701"],"year":2008,"topics":["G700 Artificial Intelligence","G400 Computer Science","G760 Machine Learning","G740 Computer Vision"],"subject":["Conference or Workshop contribution","PeerReviewed"],"fullText":"An Adaptive Appearance-based Map for Long-Term Topological\nLocalization of Mobile Robots\nFeras Dayoub and Tom Duckett\nDepartment of Computing and Informatics\nUniversity of Lincoln\nLN6 7TS Lincoln, United Kingdom\n{fdayoub,tduckett}@lincoln.ac.uk\nAbstract\u2014 This work considers a mobile service robot which\nuses an appearance-based representation of its workplace as a\nmap, where the current view and the map are used to estimate\nthe current position in the environment. Due to the nature of\nreal-world environments such as houses and offices, where the\nappearance keeps changing, the internal representation may\nbecome out of date after some time. To solve this problem\nthe robot needs to be able to adapt its internal representation\ncontinually to the changes in the environment. This paper\npresents a method for creating an adaptive map for long-term\nappearance-based localization of a mobile robot using long-\nterm and short-term memory concepts, with omni-directional\nvision as the external sensor.\nI. INTRODUCTION\nFor a mobile robot to be able to work with people\nin their everyday environment it is essential to have the\nability to localize itself using its internal representation\nof the environment. At the same time the robot needs to\nmaintain this representation in response to the dynamics of\nthe environment. Most of the work in mobile robot mapping\nconsiders only how to acquire the initial representation of\nthe environment, but there has been very little work on how\nto update the map during long-term operation in changing\nenvironments.\nThe existing methods in mobile robot localization can\ngenerally be classified into two types: geometric localization,\nwhich aims to estimate and track the absolute position of a\nrobot inside the map [12], and topological localization, which\nuses an appearance-based model of the environment [7]. In\nthe latter approach, the map represents the environment as a\ngraph where the nodes of this graph correspond to places in\nthe real environment.\nTopological localization has gained increasing attention in\nthe last few years, especially the methods based on vision\nsensors. Recently a special type of camera, omnidirectional\ncameras, has become more popular. The omnidirectional\ncamera with its 360o field of view has various advantages\nover a standard camera. The robot can sense the whole\nsurrounding environment in one snapshot regardless of its\nheading. Places can be recognized using fewer images and\nlandmarks can be tracked over long distances.\nMost of the existing work on visual localization and\nmapping assumes that the environment where the robot\nworks is static (e.g., [4], [5], [15]). However, this assumption\ndoes not hold for many real environments. For example, the\nappearance of a room in a house is not static over time: new\nobjects are sometimes added, existing objects like pictures or\ncarpets may be changed or moved, and old objects may be\nremoved. Some of these changes occur very often, such as\nmoving chairs and cushions, etc. These transient variations\nneed to be excluded from the long-term representation of the\nappearance of the environment.\nIn this work we use local features extracted from\npanoramic images to represent the appearance of a node in a\ntopological map. Adopting concepts of short-term and long-\nterm memory inspired by biological systems [1], our method\nupdates the group of feature points for the reference image\nof a particular place. Thus the reference images are adapted\nto represent the information about the new appearance of the\nlocation (note that adaptation of the topology in the map is\nnot considered in this work).\nThe rest of the paper is structured as follows. Section II\ndiscusses previous work on appearance-based mapping and\nlocalization. Section III describes our method for adaptive\nrepresentation of the nodes in a topological map. Section\nIV presents the experiments and results obtained. Finally we\ndraw conclusions and discuss future work in section V.\nII. BACKGROUND\nAn early approach for appearance-based mapping and\nlocalization was published in [15] where the operational area\nof the robot is represented as a graph. The nodes in this\ngraph represent distinctive places and the edges represent\nthe transitions between places. This approach consists of\ntwo stages: off-line and on-line. In the off-line stage the\nrobot is driven through its operational area to learn a model\nof the environment, by taking a sequence of images in\ncertain places and then creating a topological map from these\nimages. In the on-line stage the robot uses the map to find\nits current position, i.e. the node which is most similar to\nthe current view.\nMany researches use various methods to create the map\nautomatically without the need to label the images. Recently,\nZivkovic et al. [18] formalized the topological mapping\nproblem as an approximate solution for a graph cut problem.\nValgren et al. [16] defined the problem as incremental\nspectral clustering. Goedeme et al. [4] applied Dempster-\nShafer probabilistic theory to topological map construction\nin environments with self-similarities.\nDuring the localization stage the robot has to find the\nnode with the most similar appearance to the current view.\nTo solve this problem probabilistic methods such as Monte\nCarlo Localization [14] and Hidden Markov Models [6] can\nbe used. These methods are based on a recursive Bayesian\nfilter, which estimates the current position of the robot in the\nmap given the observations. The probability of the current\nstate xt given the sequence of observations Zt = {z1, ..., zt}\nup to time t is\nP (xt|Zt) = P (zt|xt)P (xt|Z\nt\u22121)\nP (zt|Zt\u22121) , (1)\nwhere the sensor model P (zt|xt) is calculated based on the\nsimilarity between the current view and the nodes in the map.\nTwo different approaches to measure the similarity be-\ntween images have been presented in the literature: global\nand local methods. The global methods capture global\nproperties of the image using approaches based on colour\nhistograms [5], principal component analysis (PCA) [17],\nFourier transform [10], etc. The local methods extract local\nproperties from the image and produce a group of land-\nmark features. Local feature including SIFT [8], SURF [2],\nMSER [9], etc., are used to find the similarity between\nimages. The local methods have been shown to be more\nreliable and robust to illumination and viewpoint changes,\nthanks to the feature descriptors that are built using a\nlocal region around selected feature points. Each feature is\ndescribed by a high-dimensional vector representation, which\nhas high invariance to image translation, scaling and rotation,\nand partial invariance to illumination changes and affine\nprojection.\nUsing local image feature descriptors, the similarity be-\ntween two images can be measured by finding the correspon-\ndences between the features in the two images. This can be\ndone by finding the closest feature in the feature descriptor\nspace. However, the method can be time consuming if the\nnumber of images in the map is large and the search is\ndone linearly. To speed up the matching process, a Kd-Tree\nof the feature descriptors from all the images in the map\ncould be used. Using a text retrieval approach, Sivic and\nZisserman [13] presented a very fast retrieval system using\na visual vocabulary. Nister and Stewenius [11] extended\nthe ability of the system by using hierarchical K-means\nclustering to improve the performance, so that Fraundorfer\net al. [3] were able to implement global localisation in real-\ntime.\nIII. THE METHOD\nIn the presented topological localization methods, the map\ncould become out-of-date after some time in a changing\nenvironment. A naive solution to this problem would be\nsimply to replace the image representation for each node in\nthe topological map from time to time, in order to reflect the\nchanged appearance of the corresponding location. Provided\nthat the robot is correctly localized, this approach would\nenable the robot to remove out-of-date information from the\nmap. However, it could also remove useful features due to\ntemporary occlusions, and could lead to catastrophic results\nin the case of localization errors. A better solution would be\nto update the image representation of a node incrementally,\nby gradually adding information about new stable features\nin the environment, while removing information about fea-\ntures that no longer exist. In our approach, each node is\nrepresented by a group of features (using SURF in these\nexperiments, though other features could be used). This\ngroup of features is updated over time by adding persistent\nnew features and removing older ones that are no longer\nused.\nThe question here is how the system should choose which\nfeatures to add and which features to remove from the stored\nimage representation for a particular node? To answer this\nquestion we will adopt an information processing model\n(see Fig. 1) based on the multi-store model of human\nmemory proposed by Atkinson and Shiffrin [1]. This model,\nwhich forms the basis of modern memory theories, divides\nhuman memory into three stores:\n\u2022 sensory memory,\n\u2022 short-term memory (STM),\n\u2022 long-term memory (LTM).\nThe sensory memory contains information perceived by\nSensory Memory\nShort term Memory\nLong term Memory\nInp\nut\n Info\nrm\natio\nn\nA\ntte\nntio\nn\nForgetting\nR\netrie\nval\nT\nra\nn\nsfe\nr\nF\no\nrg\netting\nRehearsal Loop\nFig. 1. The Information Processing Model.\nthe senses, and selective attention determines what infor-\nmation moves from sensory memory to short-term memory.\nThrough the process of rehearsal, information in STM can\nbe committed to LTM to be retained for longer periods of\ntime. In return, the knowledge stored in LTM affects our\nperception of the world, and influences what information we\nattend to in the environment.\nApplying these concepts to our approach for topological\nmapping, the sensory memory will contain the features\nextracted from the current image. Then an attentional mech-\nanism selects which information to move to STM, which\nis used as an intermediate store where new observations\nare kept for a short time. Over this time the system uses\na rehearsal mechanism to select features that are more stable\nfor transfer to LTM. In order to limit the overall storage\nrequirements and adapt to changes in the environment, the\nsystem also contains a recall mechanism that forgets unused\nfeature points in LTM by removing these features from the\nnode. LTM is used in turn by the attentional mechanism for\nselecting the new sensory information to update the map.\nA. Recall, Rehearsal, Transfer\nWe assume that an initial map of the whole environment\nhas already been created by the robot, e.g. using an existing\nalgorithm for topological mapping of static environments.\n(In this work we selected the places by hand.) We model\nthe world as a set of discrete places. In our experiments,\nomni-directional vision is used to provide the features for\nlocalization and mapping. Each place has two memory\nstores: STM, which is a temporary stage, and LTM, which\nprovides the reference views in the map used for self-\nlocalization. We assume that the robot is able to self-localize\nby matching features extracted from the current view to the\nstored reference views, though the self-localization does not\nneed to be perfect (we measure the effects of noise and self-\nlocalization error in our experiments). The purpose of our\nalgorithm presented here is to maintain up-to-date reference\nviews for the nodes in the map, using recall and rehearsal\nconcepts inspired by human memory.\nTo initialise the map, the image data from the robot\u2019s first\ntour of the environment is used. One panoramic image is\nselected to represent each node in the map. For each node,\nlocal features are extracted using the SURF algorithm [2],\nresulting in approximately 500 features per node in our\nAlgorithm 1 Update the reference view.\nDefinitions:\nCrrNode: The reference view of the current node.\nCrrView: The current view for the current node.\nCrrSTM: The current STM for the current node.\nSTMlng: The maximum number of states in the STM.\nLTMlng: The maximum number of states in the LTM.\nnewFP: The difference between the CrrView and CrrNode.\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013\nfor (every visit to the node ) {\nnewFP = recall( CrrNode , CurrView , LTMlng )\nrehearse( CrrSTM, newFP , STMlng )\n}\nV1 V2 VnVn-1\nSTMSM\nLTM\nForget\nHit Hit\nMiss\nMiss\nMiss\nFig. 2. The rehearsal stage in the STM.\nAlgorithm 2 The rehearsal stage in the STM.\nfor (every feature in CrrSTM ){\nif (feature in newFP){\nMove the feature to the next state.\nif (feature state > STMlng){\nMove the feature to the CrrNode.\nRemove the feature from CrrSTM.\n}\nelse if (feature in the first state){\nRemove the feature from CrrSTM.\nelse\nReset the feature to the first state.\n}\n}\n}\nfor (every feature in newFP ){\nif (feature was not in CrrSTM){\nAdd the feature to CrrSTM in the first state.\n}\n}\nexperiments. These features are used directly to initialise\nLTM, while STM for each node is initially assigned to be\nempty.\nThereafter, every time the robot visits an existing node, the\nfollowing steps are carried out. Feature points are extracted\nfrom the current view, using the SURF algorithm. Self-\nlocalization is carried out by comparing the current features\nto the reference features of each node (LTM) to estimate the\ncurrent node. In our case, we apply global localization by\nplace recognition, although any appropriate self-localization\nalgorithm could be applied, e.g. Markov localization. After\nlocalization, the current features are used in the recall stage\nfor updating the LTM of the current node. Only new features\nwhich do not match any feature in LTM are used in the\nrehearsal stage. Algorithm 1 describes the two main stages;\n(1) recall, where the difference in appearance between the\nreference and current views is computed, and (2) rehearsal,\nwhere this difference is used to update STM and commit\npersistent new features of the location to LTM.\nAlgorithm 2 shows the rehearsal process for a stored\nfeature in STM, which is also represented as a finite state\nmachine in Fig. 2. This stage represents what Atkinson and\nSchiffrin called rehearsal in their memory model (Fig. 1), i.e.\nS1 S2 SmSm-1\nSTM\nLTM\nForget\nMiss Miss\nHit\nHit\nHit\nFig. 3. The recall stage in the LTM.\nAlgorithm 3 The recall stage in the LTM.\nnewFP = []\nfor (every feature in CrrNode){\nif (feature in CrrView){\nRest the feature to the first state.\nelse\nMove the feature to the next state.\n}\nif (feature state > LTMlng){\nRemove the feature from CrrNode.\n}\n}\nfor (every feature in CrrView ){\nif (feature not in CrrNode){\nAdd the feature to newFP.\n}\n}\nreturn newFP\nthe process of continually recalling information into the STM\nin order to memorise it. In order to transfer a feature point\nfrom STM to LTM the feature has to be seen frequently\nin that node. Features enter STM from sensory memory\nand must progress through several intermediate states (V1\nto Vn) before transfer to LTM. Every time the robot visits\nthe node and finds the feature (\u201chit\u201d), the state of the feature\nis moved closer to LTM. However if the feature is missing\nfrom the current view (\u201cmiss\u201d), it is returned to the first\nstate (V1) or forgotten if it is already there. This policy\nmeans that spurious features should be quickly forgotten,\nwhile persistent features will be transferred to LTM.\nAlgorithm 3 shows the recall process for a stored feature\nin LTM, which is also represented as finite state machine\nin Fig. 3. This process first involves updating the LTM by\nmatching the reference view to the current view. In order to\nremain in the LTM, a feature has to be seen occasionally in\nthat node. In contrast to rehearsal, features enter LTM from\nSTM and must progress through several intermediate states\n(S1 to Sm) before being forgotten. Stored features which\nhave been seen in the current view are reset to the first state\n(S1), while the state of features which have not been seen is\nprogressed, and a feature point that passes through all states\nwithout a \u201chit\u201d is forgotten. Finally, recall returns the list of\nnew features that were not already present in the LTM.\nIV. EXPERIMENTS AND RESULTS\nTo investigate our method of updating the reference views\nin a topological map, we conducted two experiments. In\nthe first experiment we tested the system for a single node\nrepresented by a view of an office room. In the second exper-\niment we used an image data set recorded over approximately\n9 weeks from eight places in the students\u2019 restaurant of\nthe University of Lincoln. Our experimental platform is an\nActivMedia P3-AT robot equipped with a GigE progressive\ncamera (Jai TMC-4100GE, 4.2 megapixels) with a curved\nmirror from 0-360.com. Using the camera with the mirror we\nobtain high-resolution omnidirectional images. The images\nin this shape have high order distortions which can affect the\nscale and rotation invariance of feature matching. To reduce\nthese effects and to reduce the complexity of the required\nfeature descriptor, the images are unwrapped to panoramic\nimages using a simple transformation. The transformation of\nthe output coordinates (xp,yp) to coordinates of the omni-\ndirectional image (xo,yo) can be written as\nxo = cos (\nxp\n2piRO\n+ offset) \u2217 (RI + yp) + centerX , (2)\nyo = sin (\nxp\n2piRO\n+ offset) \u2217 (RI + yp) + centerY , (3)\nwhere RO, RI are radii of the outer and inner border of\nthe omni-directional image. The parameters centerX and\ncenterY specify the circle center. The last parameter offset\ndefines the origin of the panoramic image.\nFor local feature extraction we use the SURF algorithm.\nThis algorithm extracts local features from the scale-space\nof the image based on the Hessian matrix and approximates\nthe second order Gaussian derivatives with box filters. A\nfast non-maximum suppression algorithm is also used. The\nresulting algorithm has a good performance in the extraction\nprocess and a high accuracy. For more details, see [2].\nAfter the extraction stage, the algorithm creates vector\ndescriptors for the extracted features using information from\nthe local surrounding area. This algorithm can create several\ntypes of descriptors. In our experiments we use the U-SURF\ndescriptor (rotation invariance is removed) with descriptor\nlength 64, taking into account that the robot is moving on a\nplane and that rotation invariance is not required for place\nrecognition.\nTo find the similarity score between two groups of feature\npoints, we use the number of corresponding features Mij\nbetween the two groups based on a nearest neighbour (NN)\nmatching schema using the value 0.7 as a threshold between\nthe nearest and second-nearest neighbour, following [2]. The\nsimilarity score between group Gi and another group Gj can\nbe defined as\nSij =\nMij\nKi\n\u2217 100, (4)\nwhere Ki is the number of features in Gi.\nA. Long-term update of a single node\nTo illustrate how the changing appearance of the en-\nvironment affects the similarity score, we carried out an\n0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 105 110\n5 \n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n55\n60\n65\n70\n75\n80\n85\n90\n95\n100\nVisit sequence \nS i m\ni l a\nr i t y\n \n \nusing static view\nusing adaptive view\nFig. 4. The similarity score between the reference view and the current\nview of the node during 105 visits using the static and the adaptive reference\nview.\nexperiment where images were recorded over time at a\nsingle location. Objects in the environment were manipulated\nmanually during this experiment. We made three types of\nchanges. One is the temporary changes such a having a\nperson standing in the node during the visit or moving chairs,\netc. A second one is to add new objects to become part of\nthe appearance of this node or to change the arrangement of\nsome objects in the node. The third type is when we removed\nobjects from the node permanently.\nFig. 4 shows how the similarity score between the refer-\nence view and the current view of the node changed during\n105 visits to the node, using the static and the adaptive\nreference view. In this experiment the STM had 4 stages and\nthe LTM had 5 stages. As we can see the similarity score\nis changing from visit to visit due to two factors. The first\none is occlusion, which happens when a part of the image\nis blocked by a person near by, and the second factor is the\nchanging appearance.\nIn the static reference view scenario, the two factors have\nan impact on the similarity score, which drops below 35% in\nsome visits (e.g. visit 57). But when the adaptive reference\nview is used the effect of the second factor is reduced, which\ngives a high similarity score when the visit is occlusion free\nand a good similarity score in the cases when an occlusion\nhappened.\nB. Long-term topological localization\nIn the second experiment we created a topological ap-\npearance map of the students\u2019 restaurant in the University\nof Lincoln by taking eight omni-directional images from\neight different places to form the reference views for the\nnodes. This restaurant is used for various student activities\nand between these events the place is generally returned to\nits normal appearance by the restaurant staff but with some\ndifferences.\nOver a period of approximately 9 weeks we visited the\neight locations 18 times and recorded images for the places\nin the map. Fig. 5 shows two panoramic images for the\nsame place recorded at different times. Using 144 images\ngenerated from these visits we tested our method for adapting\nTABLE I\nLONG-TERM TOPOLOGICAL LOCALIZATION TEST.\nCorrect global localization%\nTest Static Map Adaptive Map\nMean Std Mean Std\nNo noise or occlusion 95.83 - 98.61 -\n50% occlusion, No noise 93.42 1.25 98.41 0.83\nAdded noise, No occlusion 89.79 1.89 97.25 0.95\n25% occlusion + noise 88.02 1.89 96.21 1.73\n50% occlusion + noise 85.60 2.15 93.75 2.41\nthe reference views inside the map by using a Monte Carlo\nsimulation technique. We used a global localization method\nbased on place recognition using the similarity between the\ncurrent and the reference views (winner-takes-all). Local-\nization failures were an integral part of this experiment,\ni.e., in the case of incorrect place recognition the image\nrepresentation for the wrong node would be updated. Monte\nCarlo simulation was used to simulate occlusion and added\nnoise due to illumination changes, etc. in the current view.\n100 trials were used for every test to evaluate the localization\nperformance. We carried out five different tests using the\nrestaurant dataset with 4 stages in the LTM and 2 stages in\nthe STM. Results from these tests are illustrated in Table I.\nIn the first one we tested the localization performance for\nthe 144 images without any simulated occlusion or added\nnoise. In the second test we simulated occlusion by removing\n50% of randomly chosen extracted features from the current\nview before each of the 144 localization attempts. In the\nthird experiment we tested the localization performance\nwith noise in the matching schema by adding Gaussian\nnoise (\u00b5=0,\u03c3=0.1) to the distance threshold between the\nnearest and second nearest neighbour. By adding this noise\nsome of the true matches will be missed and some of the\nfalse matches will be counted. In the fourth experiment we\ncombined the two factors: 25% of randomly chosen extracted\nfeatures were removed from the current view before the\nlocalization stage then the Gaussian noise was also added\ninto the matching schema. In the last one, 50% of randomly\nchosen extracted features were removed then the Gaussian\nnoise was added.\nThe observed performance differences between static and\nadaptive mapping were tested using Student\u2019s t-test and\nshown to be statistically significant (p < 0.01). The results\nshow that the adaptive map yields better localization perfor-\nmance due to its better representation of the real appearance\nof the environment. In this experiment, each node contained\nan average of 395.3\u00b164.5 features in LTM and 416.4\u00b150.0\nfeatures in STM, meaning that the approach should scale well\nto large environments.\nV. CONCLUSIONS AND FUTURE WORKS\nThis paper introduced a complimentary component for\ntopological localization methods that use features extracted\nfrom images to represent the appearance of the nodes in\nthe map. It updates the reference views of the nodes and\ntracks the changing appearance of the environment, while\nFig. 5. Two panoramic views from the same place at different times.\nthe robot is working over long periods of time. To achieve\nthis we adopted short-term and long-term memory concepts\nto adapt the reference views of the nodes in response to\nthe dynamics of the environment. To test our method, we\nconducted two experiments. The first one was in an office\nroom where we manually changed the appearance of the\nplace. For the second experiment we used data recorded from\na real dynamic environment (a students\u2019 restaurant) over 9\nweeks. In both experiments the method gave improved results\nover static mapping.\nIn this work, the number of the stages in LTM and STM\nwere determined empirically based on the recorded sensor\ndata. As a future work, the number of the stages could be\nlearned depending on the dynamics of the real environment.\nThe attention mechanism could be improved by adding real-\ntime tracking of features in the scene to filter out spurious\nfeatures due to noise or temporary occlusion. The adaptive\ncapability of the map could be further extended to the\ntopological level, by making the robot able to add or remove\nnodes and links from the map.\nREFERENCES\n[1] R.C. Atkinson and R.M. Shiffrin. Human memory: A proposed system\nand its control processes. In K.W. Spence & J.T. Spence (Eds.), The\nPsychology of Learning and Motivation, 2:89\u2013195, 1968.\n[2] H. Bay, T. Tuytelaars, and L. Van Gool. SURF: Speeded up robust\nfeatures. In Proc. European Conference on Computer Vision (ECCV),\n2006.\n[3] C. Fraundorfer, F. Engels and D. Nister. Topological mapping,\nlocalization and navigation using image collections. In Proc. IEEE\nInternational Conference on Intelligent Robots and Systems (IROS),\n2007.\n[4] T. Goedeme\u00b4, M. Nuttin, T. Tuytelaars, and L. Van Gool. Omnidirec-\ntional Vision Based Topological Navigation. International Journal of\nComputer Vision, 74(3):219\u2013236, 2007.\n[5] H.M. Gross, A. Koenig, C. Schroeter, and H.J. Boehme. Omnivision-\nbased probabilistic self-localization for a mobile shopping assistant\ncontinued. In Proc. IEEE International Conference on Intelligent\nRobots and Systems (IROS), 2003.\n[6] J. Kosecka and F. Li. Vision based topological Markov localization.\nIn Proc. IEEE International Conference on Robotics and Automation\n(ICRA), 2004.\n[7] B. Kuipers and Y. T. Byun. A robot exploration and mapping strategy\nbased on a semantic hierarchy of spatial representations. Toward\nLearning Robots. MIT Press, Cambridge, Massachusetts, page 4763,\n1993.\n[8] D.G. Lowe. Object recognition from local scale-invariant features.\nIn Proc. IEEE International Conference on Computer Vision (ICCV),\n1999.\n[9] J. Matas, O. Chum, M. Urban, and T. Pajdla. Robust wide-baseline\nstereo from maximally stable extremal regions. Image and Vision\nComputing, 22(10):761\u2013767, 2004.\n[10] E. Menegatti, M. Zoccarato, E. Pagello, and H. Ishiguro. Image-based\nMonte Carlo localisation with omnidirectional images. Robotics and\nAutonomous Systems, 48(1):17\u201330, 2004.\n[11] D. Nister and H. Stewenius. Scalable recognition with a vocabulary\ntree. In Proc. IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2006.\n[12] S. Se, D. Lowe, and J. Little. Mobile Robot Localization and\nMapping with Uncertainty using Scale-Invariant Visual Landmarks.\nThe International Journal of Robotics Research, 21(8):735, 2002.\n[13] J. Sivic and A. Zisserman. Video Google: a text retrieval approach to\nobject matching in videos. In Proc. IEEE International Conference\non Computer Vision (ICCV), 2003.\n[14] H. Tamimi, H. Andreasson, A. Treptow, T. Duckett, and A. Zell. Lo-\ncalization of mobile robots with omnidirectional vision using particle\nfilter and iterative sift. In Proc. European Conference on Mobile\nRobots (ECMR), 2005.\n[15] I. Ulrich and I. Nourbakhsh. Appearance-based place recognition for\ntopological localization. In Proc. IEEE International Conference on\nRobotics and Automation (ICRA), 2000.\n[16] C. Valgren, A. Lilienthal, and T. Duckett. Incremental Topological\nMapping Using Omnidirectional Vision. In Proc. IEEE International\nConference on Intelligent Robots and Systems (IROS), 2006.\n[17] N. Vlassis, B. Terwijn, and B. Krose. Auxiliary particle filter robot\nlocalization from high-dimensional sensor observations. In Proc. IEEE\nInternational Conference on Robotics and Automation (ICRA), 2002.\n[18] Z. Zivkovic, O. Booij, and B. Kro\u00a8se. From images to rooms. Robotics\nand Autonomous Systems, 55(5):411\u2013418, 2007.\n"}