{"doi":"10.1109\/TMI.2004.830524","coreId":"55619","oai":"oai:eprints.lincoln.ac.uk:1909","identifiers":["oai:eprints.lincoln.ac.uk:1909","10.1109\/TMI.2004.830524"],"title":"A fast model-free morphology-based object tracking algorithm","authors":["Owens, Jonathan","Hunter, Andrew","Fletcher, Eric"],"enrichments":{"references":[{"id":18437504,"title":"A Real-Time System for Video Surveillance of Unattended Outdoor Environments.","authors":[],"date":"1998","doi":"10.1109\/76.728411","raw":"Foresti, G.L.: A Real-Time System for Video Surveillance of Unattended Outdoor Environments. IEEE Trans. Circuits and Systems for Vid Tech, Vol. 8, No. 6 (1998)","cites":null},{"id":18437509,"title":"A.: A Statistically-based Newton Method for Pose Refinement.","authors":[],"date":"1998","doi":"10.1016\/s0262-8856(98)00098-5","raw":"Pece, A., Worral, A.: A Statistically-based Newton Method for Pose Refinement. Image and Vision Computing, Vol. 16, No. 8 (1998)","cites":null},{"id":18437510,"title":"A.: Application of the Self-Organising Map to Trajectory Classification.","authors":[],"date":"2000","doi":"10.1109\/vs.2000.856860","raw":"Owens, J., Hunter, A.: Application of the Self-Organising Map to Trajectory Classification. IEEE Third International Workshop on Visual Surveillance (2000)","cites":null},{"id":18437506,"title":"An Integrated Traffic and Pedestrian Model-Based Vision System.","authors":[],"date":"1997","doi":null,"raw":"Remagnino, P., Baumberg, A., Grove, T., Hogg, D., Tan, T., Worral, A., Baker, K.: An Integrated Traffic and Pedestrian Model-Based Vision System. Proc. BMVC, Vol. 2 (1997)","cites":null},{"id":18437515,"title":"Comparison of Background Extraction Based Intrusion Detection Algorithms.","authors":[],"date":"1996","doi":"10.1109\/icip.1996.559548","raw":"Makarov, A.: Comparison of Background Extraction Based Intrusion Detection Algorithms. IEEE Int. Conf. Image Processing (1996)","cites":null},{"id":18437512,"title":"E.: Novelty Detection in Video Surveillance Using Hierarchical Neural Networks.","authors":[],"date":"2000","doi":"10.1007\/3-540-46084-5_202","raw":"Owens, J., Hunter, A., Fletcher, E.: Novelty Detection in Video Surveillance Using Hierarchical Neural Networks. Proc. ICANN (to appear) (2000)","cites":null},{"id":18437514,"title":"Event Detection and Analysis from Video Streams.","authors":[],"date":"2001","doi":"10.1109\/34.946990","raw":"Medioni, G., Cohen, I., Bremond, F., Hongeng, S., Nevatia, R.: Event Detection and Analysis from Video Streams. IEEE Trans. PAMI, Vol. 23, No. 8 (2001)","cites":null},{"id":18437507,"title":"G.D.: Probabilistic Data Association Methods for Tracking Complex Visual Objects.","authors":[],"date":"2001","doi":"10.1109\/34.927458","raw":"Rasmussen, C., Hager, G.D.: Probabilistic Data Association Methods for Tracking Complex Visual Objects. IEEE Trans. PAMI, Vol. 23, No. 6 (2001)","cites":null},{"id":18437513,"title":"Image Processing System for Pedestrian Monitoring Using Neural Classification of Normal Motion Patterns.","authors":[],"date":"1999","doi":"10.1177\/002029409903200902","raw":"Boghossian, B.A., Velastin, S.A.: Image Processing System for Pedestrian Monitoring Using Neural Classification of Normal Motion Patterns. Meas. and Control, Vol. 32, No. 9 (1999)","cites":null},{"id":18437508,"title":"K.D.: Visual Surveillance Using Deformable Models of Vehicles.","authors":[],"date":"1997","doi":"10.1016\/s0921-8890(97)83348-9","raw":"Ferryman, J.M., Worral, A.D., Sullivan, G.D., Baker, K.D.: Visual Surveillance Using Deformable Models of Vehicles. Robotics and Autonomous Sys., Vol. 19, No. 3-4 (1997)","cites":null},{"id":18437505,"title":"Learning and Classification of Suspicious Events for Advanced Visual-Based Surveillance. In: Foresti,","authors":[],"date":null,"doi":"10.1007\/978-1-4615-4327-5_8","raw":"Foresti, G.L., Roli, F.: Learning and Classification of Suspicious Events for Advanced Visual-Based Surveillance. In: Foresti, G.L., M\u00e4h\u00f6nen, P., Regazzoni, C.S. (eds): Multimedia Video-Based Surveillance Systems: Requirements. Kluwer Academic Publishers","cites":null},{"id":18437502,"title":"Multimedia Video-Based Surveillance Systems \u2013 Requirements, Issues and Solutions.","authors":[],"date":null,"doi":"10.1007\/978-1-4615-4327-5","raw":"Foresti, G.L., M\u00e4h\u00f6nen, P., Regazzoni, C.S. (eds): Multimedia Video-Based Surveillance Systems \u2013 Requirements, Issues and Solutions. Kluwer Academic Publishers","cites":null},{"id":18437516,"title":"Real-Time Vision-Based Detection of Waiting Pedestrians.","authors":[],"date":"1997","doi":"10.1006\/rtim.1997.0080","raw":"Kehtarnavaz, N., Rajkotwala, F.: Real-Time Vision-Based Detection of Waiting Pedestrians. Real-Time Imaging, Vol. 3 (1997)","cites":null},{"id":18437503,"title":"W.E.L.: Learning Patterns of Activity Using Real-Time Tracking.","authors":[],"date":"2000","doi":"10.1109\/34.868677","raw":"Stauffer, C., Grimson, W.E.L.: Learning Patterns of Activity Using Real-Time Tracking. IEEE Trans. PAMI, Vol. 22, No. 8 (2000)","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2002-09","abstract":"This paper describes the multiple object tracking component of an automated CCTV surveillance system. The system tracks objects, and alerts the operator if unusual trajectories are discovered. Objects are detected by background differencing. Low contrast levels can present problems, leading to poor object segmentation and fragmentation, particularly on older analogue surveillance networks. The model-free tracking algorithm described in this paper\\ud\naddresses object fragmentation, and the object merging that occurs when proximate objects segment to the same connected component","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/55619.pdf","fullTextIdentifier":"http:\/\/eprints.lincoln.ac.uk\/1909\/1\/bmvc2002f.pdf","pdfHashValue":"ef6395ed1cdcd5a8deb60e89b63eaac59c69f316","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lincoln.ac.uk:1909<\/identifier><datestamp>\n      2013-03-13T08:32:57Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47373630<\/setSpec><setSpec>\n      74797065733D636F6E666572656E63655F6974656D<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lincoln.ac.uk\/1909\/<\/dc:relation><dc:title>\n        A fast model-free morphology-based object tracking algorithm<\/dc:title><dc:creator>\n        Owens, Jonathan<\/dc:creator><dc:creator>\n        Hunter, Andrew<\/dc:creator><dc:creator>\n        Fletcher, Eric<\/dc:creator><dc:subject>\n        G760 Machine Learning<\/dc:subject><dc:description>\n        This paper describes the multiple object tracking component of an automated CCTV surveillance system. The system tracks objects, and alerts the operator if unusual trajectories are discovered. Objects are detected by background differencing. Low contrast levels can present problems, leading to poor object segmentation and fragmentation, particularly on older analogue surveillance networks. The model-free tracking algorithm described in this paper\\ud\naddresses object fragmentation, and the object merging that occurs when proximate objects segment to the same connected component.<\/dc:description><dc:date>\n        2002-09<\/dc:date><dc:type>\n        Conference or Workshop contribution<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lincoln.ac.uk\/1909\/1\/bmvc2002f.pdf<\/dc:identifier><dc:identifier>\n          Owens, Jonathan and Hunter, Andrew and Fletcher, Eric  (2002) A fast model-free morphology-based object tracking algorithm.  In: British Machine Vision Conference 2002, 2nd-5th September 2002, University of Cardiff.  <\/dc:identifier><dc:relation>\n        http:\/\/www.bmva.ac.uk\/bmvc\/2002\/<\/dc:relation><dc:relation>\n        10.1109\/TMI.2004.830524<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lincoln.ac.uk\/1909\/","http:\/\/www.bmva.ac.uk\/bmvc\/2002\/","10.1109\/TMI.2004.830524"],"year":2002,"topics":["G760 Machine Learning"],"subject":["Conference or Workshop contribution","PeerReviewed"],"fullText":"  1\nA Fast Model-Free Morphology-\nBased Object Tracking Algorithm \n \nJonathan Owensa & Andrew Hunterb \naSchool of Computing & Technology \nUniversity of Sunderland, UK \njonathan.owens@sunderland.ac.uk\nbDepartment of Computer Science \nDurham University, UK \nandrew1.hunter@durham.ac.uk\n \n \nAbstract \n \nThis paper describes the multiple object tracking component of an automated \nCCTV surveillance system. The system tracks objects, and alerts the operator \nif unusual trajectories are discovered. Objects are detected by background \ndifferencing. Low contrast levels can present problems, leading to poor object \nsegmentation and fragmentation, particularly on older analogue surveillance \nnetworks. The model-free tracking algorithm described in this paper \naddresses object fragmentation, and the object merging that occurs when \nproximate objects segment to the same connected component. \n \n1   Introduction \n \nAutomated visual surveillance aims to provide an attention-focussing filter to enable \nan operator to make an optimum decision whenever an unusual event occurs [1]. This is \nachieved by directing the operator\u2019s attention only to those events classified as unusual. \nThe backbone of such systems typically comprises something like the processing \npipeline shown in figure 1. \n \nImage\nAcquisition\nObject\nSegmentation\nObject\nTracking\nObject\nClassification\nBehaviour\nClassification\nScene\nDescription\n \n \nFigure 1.   Typical image processing pipeline for automated video surveillance \n \nThe blocks outlined in bold are dealt with in this paper, focussing on the object \ntracking module, which must deal with the uncertainty of object segmentation. This \nuncertainty is manifest when moving objects are segmented by background \ndifferencing, where it is common for the segmented object to fragment due to parts of \nthe object matching the greyscale of the background. This problem is exacerbated when \nCCTV system managers wish to implement modern automated surveillance techniques \non top of the existing surveillance infrastructure. Older cameras are typically low \n  2\nresolution, monochrome, analogue devices with CCD arrays of low dynamic range, \nproducing images of low contrast. \nEven complex multimodal background representations cannot successfully segment \nobjects if these closely match the background. Typical object tracking algorithms \nemploy the Kalman filter or some other predictor-corrector iterative algorithm for \ndealing with uncertainty in the tracking plane [2], [3], [4], [5], [6]. When the uncertainty \nof object segmentation is too great, the tracking algorithm relies on the predictor step, \nand the state of the tracked object is updated using the internal model, rather than the \nobserved measurements. It is an implicit assumption in typical background differencing \nand tracking algorithms that segmentation is successful at maintaining objects \nholistically, without fragmentation.  \nWhen objects merge in the binary difference image, predictor-corrector algorithms \nmay use partial image evidence to update an object state, but rely more heavily on the \ninternal model of object parameters, and do so until such time as the objects separate \nand can be tracked individually. This problem can be overcome, together with other \nphenomena such as occlusion, with an explicit model fit to tracked objects [5], [7], [8]. \nIn this paper, a simple, fast object tracking algorithm is described which attempts to \nmaintain the morphology of tracked objects, given the evidence provided by the \nsegmentation block of the pipeline. This algorithm is part of a hybrid novelty detection \nsystem [9], [10]. The overall philosophy of the system is that it should be self-\norganising, requiring no user defined models of scene elements, object forms or object \nmotion. A self-organising map is used to measure the novelty of a vector describing \nlocal motion, while a hierarchical network classifies the global pattern of  object motion. \n \n2   Summary of Algorithm \n \nThe method of choice for moving object segmentation in most tracking algorithms is \nbackground differencing. Methods based on the calculation of optic flow are \ncomputationally intensive and use a raw image feature match to maintain a track if the \nobject stops moving [11], [12]. The multiple gaussian per pixel representation used in \n[2] is robust, but entails a huge computational cost. To keep the background generator \ncomputable in real-time on non-specialised hardware, we employ a simple low-pass \nfilter [13], [14]. \nThe CCTV images are obtained from an analogue, monochrome camera at a \nresolution of 640x480 pixels, with a colour range of 256 grey values. The low-pass filter \nmethod is able to cope with slow changes in luminance, such as the movement of \nshadows cast by static objects. At time t, the difference and background images are \ncalculated as follows: \n                                           \n\u0001\n\u0002\n\u0003 >\u2212\n=\notherwise    0\n),(),(    1\n),(\nTcrBcrI\ncr\u03b4                                        (1) \n                                )1()1()()( \u2212\u00d7\u2212+\u00d7= tBtItB \u03b2\u03b2    if    0),( =cr\u03b4                           (2)  \nWhere r and c are the row and column subscripts of a single pixel, \u03b4 is the difference \nimage, I is the input image, and B is the background image. If the difference \u03b4(r,c) is \ngreater than the threshold, T = 12, the pixel is labelled as foreground, while background \npixels are modified by expression 2, where \u03b2=0.1 and controls the rate of the \nbackground update. Sufficiently rapid luminance changes cause background \ndifferencing to fail. To guard against this eventuality, the system counts the total \n  3\nnumber of pixels assigned to the foreground and if this is greater than 60% of the image, \nthe entire background image is reset and tracking is restarted. \nNoise is removed by applying a morphological \u201copening\u201d operator to the difference \nimage. The pixels that remain classified as foreground are collected into 4-connected \ncomponents and assigned unique identities. Examples of pedestrians and their \nsegmented silhouettes are shown in figure 2. Along with an identity, each object has an \nassociated feature vector, the elements of which are area, width, height and a histogram \nof the greyscale distribution of object pixels. This feature vector is used to match objects \nfrom frame to frame, as described in detail in the following section. (In this paper, the \nterm \u2018silhouette\u2019 describes a single connected component, as shown in the two images in \nthe central column in figure 2). \n \nFigure 2   Examples of partially segmented silhouettes and the objects they delineate. \n3   Object Tracking \n \nThe object tracker described here is a purely measurement based object-to-silhouette \nmatching algorithm with morphological manipulation that deals with uncertainty in the \nsegmentation algorithm. The philosophy of the tracking algorithm is motivated by \ngeneral top-down assumptions of the types of unusual behaviour and normal activity \nthat the system will have to deal with. Based on a typical car park scene, shown in \nfigure 2, the following assumptions are made: \n1. Activity of pedestrians is of primary interest; vehicles are tracked but their \nactivity will not be passed to the \u201cbehaviour\u201d classifier modules. \n2. Vehicle crime, the main form of novel behaviour that is of interest given the \nmonitored scene, is most likely to be carried out by independent pedestrians. \n3. Pedestrians entering the scene in a group, i.e. with very similar temporal and \nspatial origins with respect to the tracking plane, are likely to have a common \norigin and destination. Hence, tracking the centroid of the group will give a \nreasonable approximation to individual trajectories within the group. \n4. Pedestrians with differing spatial and temporal origins may also have differing \nintended destinations. A distinct history for each object should be maintained, \neven if the silhouettes of such objects merge. \nBased on the above assumptions, the tracker attempts to track objects in the form in \nwhich they are initially segmented. Therefore, the system will try to maintain the \ntracking of distinct objects, even if their segmented silhouettes merge with those of \nother objects. This entails the use of a silhouette-partitioning function to separate \nmerged silhouettes prior to the best-match process. \nThe algorithm will also try to maintain the overall morphology of a group, which \nentails the use of a silhouette-combiner which attempts to maintain the track of a group \n  \nas a single entity. This function serves a two-fold purpose. If a group of pedestrians is \nbeing tracked, a global track on the whole group will be maintained by merging any \ngroup members who temporarily separate from the group silhouette. On the other hand, \nif the segmentation of an object fails and it becomes fragmented into distinct \ncomponents, the silhouette-combiner will attempt to gather the fragments together until \nthe best match with the object is achieved (see figure 2). \nAfter the background differencing step, the binary image consists of silhouettes, \nwhere a number of silhouettes may correspond to a single object (fragmentation), or a \nsingle silhouette may \u201ccover\u201d more than one object (merging). Associated with each \nobject and silhouette is a feature vector, fi = [a,w,h,g] where g is a 16 element vector of \nthe greyscale histogram covered by silhouette i (fig. 3c,d), a is the area, or number of \npixels making up the silhouette and w and h are the width and height of the minimum \nbounding rectangle (fig.3a). \n \n \n \nFigu\nwidth\ngrey\n \nCent\nobject a\nobject Q\n            \nwhich is\nand wid\ndifferen\nbe no o\nthe algo\nto track\nproceed\npreferen\nfragmen\nStep\ncalculat\n            \nwhere d\nand S, a\nvector i\nthe Euc\ndenomin\n  (a)\nre 3   (a) Object delinea\n calculated from bina\nscale histogram (d). \nral to the algorithm d\nnd silhouette feature \n and silhouette S is de\n          [ QaSQd \u2212=),(\n a four element vecto\nth of the object and si\nce vector. When track\nbjects to which the ne\nrithm will jump to step\ned objects instantiate \ns by trying to mat\ntially matching silho\ntation and merging. \n 1 \u2013 Na\u00efve Match: \ned, and is given by the\n                                    \nk(Q,S) is the kth elem\nnd fk(Q) is the feature\nn the denominator of e\nlidean length of the v\nator have the effect of(b)\nted by minimum boun\nry silhouette. (c) Se\nescribed below is t\nvectors. The differe\nfined as \nSQSQS wwhha \u2212\u2212 ,,\nr comprised of the ab\nlhouette and the sca\ning begins (or after t\nwly segmented silho\n 8, where sufficiently\nnew entries in the o\nch silhouettes cons\nuettes to establish\nThe \u201ccost\u201d of every\n scalar value \n  \u0001= k\nk\nk\nji\nQ\nSQc\n(\n),(\nf\nd\nent of the difference\n vector of object Q. \nxpression (2) is trans\nector. The elements\n scaling the values o(c)\nding rectangle. (b) Area, h\ngmented object used to\nhe concept of \u201cdifferen\nnce between the featu\n( ) ( )]SQSQ gggg \u2212\u2022\u2212,\nsolute differences of th\nlar length of the greysca\nhe scene has been empt\nuettes can be compared\n large silhouettes that a\nbject list. Otherwise, t\nervatively to existing \ned objects, new objec\n object to silhouette a\ni\nji\nQ\nS\n)(\n),                             \n vector calculated betw\nThe histogram element \nformed into a scalar va\n of the object feature \nf the difference vector, a(d)4\neight and \n calculate \nce\u201d between \nre vectors of \n                (3) \ne area, height \nle histogram \ny), there will \n; in this case, \nre unmatched \nhe algorithm \nobjects, by \nts, handling \nssignment is \n                (4) \neen object Q \nin the feature \nlue by taking \nvector in the \nssuming that \n  5\nthe within population coefficients of variation are roughly equal for the separate feature \nvector elements. \nThe match matrix is initialised by assigning silhouettes to objects, on a per object \nbasis, where a single silhouette may be matched to more than one object. The elements \nof match matrix M are set where expression 6 is satisfied: \n                                             \n100\n001\n010\n1\n0\n10\n\u0001\n\u0002\u0003\u0002\u0002\u0002\n\u0001\n\u0001\n\u0001\nn\nm\nQ\nQ\nQ\nSSS\nM =\n                                                  (5) \n                                      ( )},{minarg      where1 kjkji SQciM ==                                    (6) \nwhere a non-zero entry indicates a match between object Q and silhouette S, and there \nare n objects and m silhouettes. \nAt the same time the initial match matrix is being constructed, a valid-match matrix \nis calculated based on an object\u2019s location and search radius around that position. A \nvalid search radius, r (80 pixels) establishes a limit on the matches that can be evaluated \nlater on in the algorithm, i.e. the merging function will only consider merging \nsilhouettes within radius r. Hence we have a matrix V, the same size as M which has \nnon-zero elements corresponding to possible matches. After the initial unconstrained, \nna\u00efve match, inappropriate matches are easily removed by the logical AND of the valid-\nmatch and match matrices, thus \n                                                             VMM \u2227=:                                                           (7) \nIn an ideal situation, the na\u00efve match would be enough to match objects to the \nsegmented silhouettes. The remaining steps of the algorithm are designed to address the \nerrors  that may arise from object fragmentation and merging. \nStep 2 \u2013 Remove Duplicate Matches: As the na\u00efve match is allocated by choosing \nthe lowest cost match per object, there exists the potential for match conflicts, where a \nsilhouette is initially matched with more than one object. At this stage, objects are \nallocated to one of two classes, transient or non-transient. Transient objects have only \nbeen instantiated for one frame \u2013 an object must find a silhouette match over two frames \nbefore it is classified as non-transient. If there are match conflicts, silhouette matches to \ntransient object are removed if these overlap with matches to non-transient objects. This \nstep makes it less likely that false object will interfere with the tracking of real objects. \nFalse objects may be attributed to noise or interaction of the object with the \nenvironment, such as reflections on vehicles. \nStep 3 \u2013 Evaluate Possible Merges: Where match conflicts arise between non-\ntransient objects, the objects are combined by treating the separate objects as a single \nmacro-object, \u0398, and calculating a single feature vector accordingly. The differences of \nthe objects assigned to the disputed silhouette are combined as follows, \n                                                        \u0001= k k\nSUM SQ ),(dd                                                  (8) \nwhich is an element-wise summation over the k objects matched to silhouette S. The \ncost of the macro-object-to-silhouette match is compared to dSUM in the following \nexpression \n                                            \u0001\n\u2212\u0398\n=\u0398\nj SUM\nj\nSUM\njj SSc\nd\ndd ),(\n),(                                            (9) \n  6\nwhere the summation is across the i elements of the difference vectors. The denominator \nhas the property of scaling the elements so the sum is not dominated by the larger \nelements, as discussed above, and the scalar value c(\u0398,S) will be greater than zero if the \nmacro-object match increases the feature difference, and below zero if the match is \nimproved. If c(\u0398,S) is less than zero, the conflicting matches are retained and will be \ndealt with at a later stage. Otherwise, the match with the lowest cost is retained and the \nother objects are reallocated to the next best matches available, i.e. to those silhouettes \nnot already assigned to non-transient objects, in a greedy incremental search. \nStep 4 \u2013 Remove Duplicate Matches: Non-transient object matches modified in the \nlast part of step 3 may have been allocated to silhouettes matched to transient objects. \nThis is permitted because established objects take priority over transient objects as it is \npossible these may simply be a product of a patch of noise in the last frame. Transient \nobject matches that conflict with the relocated non-transient objects are removed. Each \nsilhouette allocated to a non-transient object is labelled as \u201csecurely matched\u201d, and the \ndifference vectors, d(Q,S), recalculated ready for the next step. \nStep 5 \u2013 Partition Merged Silhouettes: Here we apply the first of the \nmorphological refinement algorithms; the silhouette-partitioning function is applied to \nresolve silhouettes matched to more than one non-transient object. If a duplicated match \ngot past step 3, it is likely that the silhouettes of the objects have merged and require \nseparating to allow the tracker to maintain a separate track of each object. \nSilhouette Partition Function: Given the (x,y) co-ordinates of each pixel in the \nsilhouette, the sum of least squares linear regression line, y=a+bx can be calculated \ndirectly from the following expressions: \n                                               ( )( ) ( )( )\n( )22\n2\n\u0001\u0001\n\u0001\u0001\u0001\u0001\n\u2212\n\u2212\n=\nxxn\nxyxxy\na                                            (10) \n                                                    ( )( )\n( )22 \u0001\u0001\n\u0001\u0001\u0001\n\u2212\n\u2212\n=\nxxn\nyxxyn\nb                                                (11) \nwhere the summation is applied to all pixels in the silhouette. \nEach pixel is projected onto the linear regression line giving a histogram of the \nsilhouette\u2019s distribution of mass along the line. The silhouette is divided by placing \npartition lines at intervals along the regression line, lr. To calculate the partition points, \nthe sizes of the objects (at t-1) participating in the split are listed in according to their \nrelative positions along the x-axis. Given n objects, there will be n-1 partitions p, based \non their distribution of \u201cmass\u201d among the n objects. If the left-most extent of the \nsilhouette along the regression line is the origin, and the right-most extent is unity, the \npartitions pm will lie in the range {0,1}, \n                                                    \n)11(    \n1\n1\n\u2212\u2264\u2264=\n\u0001\n\u0001\n=\n= nm\na\na\np n\nk\nk\nm\nj\nj\nm\n                                          (12) \nwhere pm is partition m, aj is the area of object j and n is the total number of objects \nparticipating in the split. Moving from left to right along the silhouette regression line \nhistogram, the pm ratios are used to place the partition points relative to the total mass of \nthe merged silhouette. Each pixel can now be labelled according to its projection onto \nthe regression line (figure 4). Calculating the partition intervals with expression (12) \nassumes that the overlap between the participating objects is not significant. Large \noverlaps will mean the partitions are offset with an error that increases as we progress \nfrom right to left along lr, as illustrated in figure 5. \n  7\n \nFigure 4   Example of silhouette partitioning. The linear regression line is shown in \nwhite, and the resulting partition is illustrated by the three grey levels mapping the \npartitioned objects. \n \nFigure 5   If the objects to be partitioned are heavily overlapping, the partitioning \nfunction may have trouble making the split; here the figure on the right is distorted. \n \nBased on the pixel labels, a feature vector is calculated for each sub-silhouette. The \ndifferences between the feature vectors of the participating objects and the new \npartitioned silhouettes are calculated and the cost evaluated with the following \nexpression \n                                          \u0001\n\u2212\u03a6\n=\u03a6\nj\nj\njj\nSQ\nSQQ\nQc\n),(\n),(),(\n),(\nd\ndd                                       (13) \nWhere the sum is across the j elements of the difference vectors, d(Q,S) is the difference \nbetween the feature vectors of object Q and un-partitioned silhouette S and d(Q,\u03a6) is the \nfeature difference between object Q and one of the partitioned sub-silhouettes, \u03a6. The \npartition is accepted if at least one object has a c(Q ,\u03a6) value below zero, indicating that \nit\u2019s match has been improved. Between the participating objects and sub-silhouettes, the \nnew matches are assigned on a lowest difference basis, as in the na\u00efve match performed \nin step 1. \nThe object match matrix M is adjusted to accommodate the new silhouettes and \nrevised match assignments, and these can take part in the subsequent steps in exactly the \nsame way as unmodified silhouettes. \nStep 6 \u2013 Merge Fragmented Silhouettes: Here, the possibility of object \nfragmentation is addressed, in which an object may appear as several separate \nsilhouettes in the binary difference image. Non-transient objects already matched to a \nsingle silhouette combine this with any other silhouettes lacking a secure match within \nthe valid search radius, the feature vector of the combined silhouette, \u03a8, treating the \nseparate silhouettes as a single entity. By recalculating a single feature vector for \nseparate silhouettes, a fragmented silhouette may be recombined provided the \ncombination improves the match to the tracked object. The cost of a new match is \nevaluation with the expression \n  8\n                                             \u0001\n\u2212\u03a8\n=\u03a8\nj\nj\njj\nSQ\nSQQ\nQc\n),(\n),(),(\n),(\nd\ndd                                       (14) \nAs with expressions 6 and 10, if c(Q,\u03a8) is below zero the combination is accepted and \nthe silhouettes are merged into a single entry in the silhouette list. This process \ncontinues until all unallocated silhouette fragments within the valid match radius have \nbeen considered. \nStep 7 \u2013 Refine Transient Matches: The so-called transient objects may have been \ninstantiated over a patch of noise in the previous frame, or they may be genuinely new \nobject entering the field of view. A cost-reducing feature combination step is performed \nacross these objects as in step 6, i.e. at this stage only transient objects are examined and \nmay only be combined with other transient objects. This priority given to persistent \nobjects is one way to reduce the susceptibility of the overall system to short-lived noise \nand temporary object fragmentation. \nStep 8 \u2013 Update Objects: Given the match matrix M, the object lists are updated. \nObjects without a match are removed and each unassigned silhouette of sufficient size \ninstantiates a new object in the list. The size criterion helps to prevent persistent noise, \nwhich is usually comprised of small image patches, from instantiating an object list \nentry. \n \n5   Object Based Reference Update \n \nThe stationarity of non-pedestrian objects is determined to assist in maintaining a \nvalid reference image. When a object is stationary for >16 frames (i.e. 4 seconds at the \n4Hz sampling interval) the object is inserted into the background image, pedestrians \ntypically sway even when standing, so inserted objects are typically parked vehicles. \nThe previously determined minimum bounding rectangle of the silhouette is used to \ndefine the region of the input that is copied to the background. \nOnce an object has been inserted into the background, its object list entry is \ntransferred to a \u201crecently-inserted-object\u201d list, and foreground objects, i.e. pedestrians, \ncan now be tracked as they pass in front of or exit the vehicle. If the event was a \u201cdrop-\noff\u201d, rather than a parking event, the vehicle will subsequently move away from its \npreviously stationary position, leaving a \u201chole\u201d in the background. The negative object \nwill be detected as being stationary, and the centroid can be compared to those in the \n\u201crecently-inserted-object\u201d list. If the distance between the stationary object centroid and \na list entry is below a threshold, the object is inserted immediately into the background, \nthereby patching the \u201chole\u201d as quickly as possible.  \nThis stationary object reference update is useful because of the assumption that the \nsystem will only submit pedestrian activity to the novelty detection components, thereby \ndictating that tracking localises pedestrians at the expense of tracking other objects. \n \n6   Performance of the Object Tracker \n \nThe object tracking algorithm was evaluated with respect to the monitored scene as \ninterpreted by a human observer, the overall description of which could be called the \n\u2018operator perceived activity\u2019 (OPA). The operator looked for discrepancies between \nactual activity and that \u201cperceived\u201d by the tracker. \nThe system was evaluated on 3 days of live video from 8:00am to 10:00am, \ncomprising a total of 6 hours, spanning a range of activity levels, from peak activity to \nrelatively quiescent periods. The tracker performance is shown in table 1. The left side \n  9\nof the table summarises results for pedestrian events, and the right shows the vehicle \nevents. There were a total of 311 separate events, 264 of which were tracked perfectly, \ni.e. 84.9% correct. Three examples of poorly segmented objects successfully tracked are \nshown in figure 6. The instances where there was a discrepancy between the tracker and \nthe OPA are discussed below. \n \n Table 1   Performance of the object tracker, comparing the number of vehicles and \npedestrians detected by the object tracker (columns) with the actual events as \ndefined by an operator (rows). \n  \n \nFigure 6   Examples of fragmented objects that were successfully tracked. \nCorrectly tracked events lie down the main diagonal of both sections of the table; \ne.g. there were 120 instances where a single pedestrian was correctly tracked. The 27 \nentries where one pedestrian was present but two pedestrians were tracked refers to the \nsituation where a pedestrian fragmented and one segment was momentarily tracked as a \nseparate pedestrian \u2013 this situation was temporary and the extra transient track did not \ninterfere with the tracking of the true object. The 3 instances where a pedestrian was \npresent but was not tracked (cell {1,0} in the left of table 1) was due to excessively poor \nsegmentation, which meant there were no fragments large enough to instantiate an \nobject. \nThe 9 instances of pedestrians being tracked when in fact there were none (cell {0,1} \nin the left of table 1) was due temporary regions generated by phenomena such as \nreflections of pedestrians on vehicle windows, detached shadows from vehicles or \nelongated fragments of vehicles. The single instance where a vehicle was incorrectly \ntracked (cell {1,2} in the right of table 1) was due to a neatly segmented vehicle giving \nrise to two vehicle sized objects which were tracked separately. It should be noted that \npedestrian activity is not submitted to the novelty detection networks until the pedestrian \nhas been tracked coherently for approximately 3 seconds, so the entries in the left side \nof table 1 lying above the main diagonal, showing tracking false pedestrians, were not \npassed on to the novelty detectors. From the point of view of activity classification, \nsignificant tracking errors were those lying below the main diagonal in the left side of \ntable 1. These were instances where the tracker \u201clost\u201d the track on one or more \npedestrians, thereby rendering them \u201cinvisible\u201d to the novelty detectors. Therefore, \nconsidering only those table entries lying on or below the main diagonal, out of 132 \nseparate pedestrian events, 125 were successfully passed to the classifier stages of the \nsurveillance system, a success rate of 94.6%. \n Number of Pedestrians Tracker \nNumber of Vehicles \nTracker \n 0 1 2 3  0 1 2 3 \n0  9 1 0 0  0 0 0 \n1 3 120 27 2 1 0 139 1 0 \n2 0 3 4 0 2 0 0 0 0 \nOPA \n3 0 0 1 1 3 0 0 0 0 \n  10\n \n7   Discussion \n \nThe tracking algorithm described in section 4 is able to combine object fragments to \nallow tracking when segmentation is poor. The tracking algorithm attempts to maintain \nobjects in the form in which they were first instantiated, which is achieved by means of \ntwo morphological operators. As shown in table 1, sometimes the fragmentation of \nobjects is so bad that a perfect track cannot be maintained. However, this can be dealt \nwith by the next highest module in the processing pipeline. Indeed, by accepting the \nmotion data only from objects that have been in existence for over a given period, the \nnovelty detection modules [9], [10], can prevent transient false object tracking from \ngenerating false alarms. \nThe algorithm is able to track poorly segmented objects on the basis of form only, \nand no prior models of size, shape or texture are needed. This is consistent with the \noverall strategy of a self-organising system, were objects are tracked and their behaviour \nis classified without a priori knowledge built into the system. The algorithm is \nextremely fast, as the elements used during the match process are simple macroscopic \nfeatures such as silhouette size, width, height and the greyscale histogram. \n \nReferences \n \n1. Foresti, G.L., M\u00e4h\u00f6nen, P., Regazzoni, C.S. (eds): Multimedia Video-Based Surveillance \nSystems \u2013 Requirements, Issues and Solutions. Kluwer Academic Publishers \n2. Stauffer, C., Grimson, W.E.L.: Learning Patterns of Activity Using Real-Time Tracking. \nIEEE Trans. PAMI, Vol. 22, No. 8 (2000) \n3. Foresti, G.L.: A Real-Time System for Video Surveillance of Unattended Outdoor \nEnvironments. IEEE Trans. Circuits and Systems for Vid Tech, Vol. 8, No. 6 (1998) \n4. Foresti, G.L., Roli, F.: Learning and Classification of Suspicious Events for Advanced \nVisual-Based Surveillance. In: Foresti, G.L., M\u00e4h\u00f6nen, P., Regazzoni, C.S. (eds): \nMultimedia Video-Based Surveillance Systems: Requirements. Kluwer Academic Publishers \n5. Remagnino, P., Baumberg, A., Grove, T., Hogg, D., Tan, T., Worral, A., Baker, K.: An \nIntegrated Traffic and Pedestrian Model-Based Vision System. Proc. BMVC, Vol. 2 (1997) \n6. Rasmussen, C., Hager, G.D.: Probabilistic Data Association Methods for Tracking Complex \nVisual Objects. IEEE Trans. PAMI, Vol. 23, No. 6 (2001) \n7. Ferryman, J.M., Worral, A.D., Sullivan, G.D., Baker, K.D.: Visual Surveillance Using \nDeformable Models of Vehicles. Robotics and Autonomous Sys., Vol. 19, No. 3-4 (1997) \n8. Pece, A., Worral, A.: A Statistically-based Newton Method for Pose Refinement. Image and \nVision Computing, Vol. 16, No. 8 (1998) \n9. Owens, J., Hunter, A.: Application of the Self-Organising Map to Trajectory Classification. \nIEEE Third International Workshop on Visual Surveillance (2000) \n10. Owens, J., Hunter, A., Fletcher, E.: Novelty Detection in Video Surveillance Using \nHierarchical Neural Networks. Proc. ICANN (to appear) (2000) \n11. Boghossian, B.A., Velastin, S.A.: Image Processing System for Pedestrian Monitoring Using \nNeural Classification of Normal Motion Patterns. Meas. and Control, Vol. 32, No. 9 (1999) \n12. Medioni, G., Cohen, I., Bremond, F., Hongeng, S., Nevatia, R.: Event Detection and \nAnalysis from Video Streams. IEEE Trans. PAMI, Vol. 23, No. 8 (2001) \n13. Makarov, A.: Comparison of Background Extraction Based Intrusion Detection Algorithms. \nIEEE Int. Conf. Image Processing (1996) \n14. Kehtarnavaz, N., Rajkotwala, F.: Real-Time Vision-Based Detection of Waiting Pedestrians. \nReal-Time Imaging, Vol. 3 (1997) \n"}