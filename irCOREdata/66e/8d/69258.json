{"doi":"10.1002\/sim.4014","coreId":"69258","oai":"oai:eprints.lancs.ac.uk:27756","identifiers":["oai:eprints.lancs.ac.uk:27756","10.1002\/sim.4014"],"title":"Direct effects testing : a two-stage procedure to test for effect size and variable importance for correlated binary predictors and a binary response.","authors":["Sperrin, Matthew","Jaki, Thomas"],"enrichments":{"references":[{"id":978342,"title":"A Language and Environment for Statistical Computing. R Foundation for Statistical Computing,","authors":[],"date":"2008","doi":null,"raw":"R Development Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria, 2008. URL http:\/\/www.R-project.org. ISBN 3-900051-07-0.","cites":null},{"id":976097,"title":"Asymptotics for lasso-type estimators.","authors":[],"date":"2000","doi":null,"raw":"K. Knight and W. Fu. Asymptotics for lasso-type estimators. Ann. Stat., 28:1356\u20131378, 2000.","cites":null},{"id":974134,"title":"Better subset regression using the nonnegative garrote.","authors":[],"date":"1995","doi":null,"raw":"L. Breiman. Better subset regression using the nonnegative garrote. Technometrics, 37: 373\u2013384, 1995. F. Bunea, A. Tsybakov, and M. Wegkamp. Sparsity oracle inequalities for the lasso. Elec.","cites":null},{"id":978165,"title":"Causality.","authors":[],"date":"2009","doi":null,"raw":"J. Pearl. Causality. Cambridge University Press, second edition, 2009.","cites":null},{"id":16663630,"title":"Centralizing the non-REFERENCES 27 central chi-square: a new method to correct for population strati\ufb01cation in case-control association studies.","authors":[],"date":"2006","doi":null,"raw":"P. Gorroochurn, G. A. Heiman, S. E. Hodge, and D. A. Greenberg. Centralizing the non-REFERENCES 27 central chi-square: a new method to correct for population strati\ufb01cation in case-control association studies. Genetic Epidemiology, 30:277\u2013289, 2006.","cites":null},{"id":975407,"title":"Centralizing the nonREFERENCES 27 central chi-square: a new method to correct for population stratification in case-control association studies.","authors":[],"date":"2006","doi":null,"raw":null,"cites":null},{"id":978557,"title":"Coronary risk factor screening in three rural communities.","authors":[],"date":"1983","doi":null,"raw":"J. Rousseauw, J. du Plessis, A. Benade, P. Jordaan, J. Kotze, P. Jooste, and J. Ferreira. Coronary risk factor screening in three rural communities. South African Medical Journal, 64:430\u2013436, 1983.","cites":null},{"id":974124,"title":"GenABEL: genome-wide SNP association analysis,","authors":[],"date":"2008","doi":null,"raw":"Y. Aulchenko and M. Struchalin. GenABEL: genome-wide SNP association analysis, 2008. R package version 1.4-0.","cites":null},{"id":976915,"title":"Generalized Linear Models.","authors":[],"date":"1989","doi":null,"raw":"P. McCullagh and J. A. Nelder. Generalized Linear Models. Chapman & Hall, 1989.","cites":null},{"id":974619,"title":"Genomic control for association studies.","authors":[],"date":"1999","doi":null,"raw":"B. Devlin and K. Roeder. Genomic control for association studies. Biometrics, 55 no. 4: 997\u20131004, 1999.","cites":null},{"id":974853,"title":"Genomic control, a new approach to genetic-based association studies. Theoretical Population Biology,","authors":[],"date":"2001","doi":null,"raw":"B. Devlin, K. Roeder, and L. Wasserman. Genomic control, a new approach to genetic-based association studies. Theoretical Population Biology, 60:155\u2013166, 2001.","cites":null},{"id":977174,"title":"Hierarchical testing of variable importance.","authors":[],"date":"2008","doi":null,"raw":"N. Meinshausen. Hierarchical testing of variable importance. Biometrika, 95(2):265\u2013278, 2008.","cites":null},{"id":979143,"title":"High dimensional variable selection.","authors":[],"date":"2009","doi":"10.1214\/08-AOS646","raw":"L. Wasserman and K. Roeder. High dimensional variable selection. Ann. Stat., 2009. In press.","cites":null},{"id":978615,"title":"Inference and missing data.","authors":[],"date":"1976","doi":"10.1093\/biomet\/63.3.581","raw":"D. B. Rubin. Inference and missing data. Biometrika, 63:581\u2013592, 1976.","cites":null},{"id":975127,"title":"Least angle regression.","authors":[],"date":"2004","doi":null,"raw":"B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Ann. Stat., 32 (2):407\u2013499, 2004. J. Fan and J. Lv. Sure independence screening for ultra-high dimensional feature space. J.","cites":null},{"id":977926,"title":"On the lasso and its dual.","authors":[],"date":"1998","doi":null,"raw":"M. R. Osborne, B. Presnell, and B. A. Turlach. On the lasso and its dual. Technical Report 98\/1, Dept. Statistics, Univ. Adelaide, 1998.","cites":null},{"id":978904,"title":"Regression shrinkage and selection via the lasso.","authors":[],"date":"1996","doi":"10.1111\/j.1467-9868.2011.00771.x","raw":"R. Tibshirani. Regression shrinkage and selection via the lasso. J. R. Stat. Soc. B, 58(1): 267\u2013288, 1996.","cites":null},{"id":975820,"title":"Ridge regression.","authors":[],"date":"1988","doi":null,"raw":"A. Hoerl and R. Kennard. Ridge regression. In Encyclopedia of Statistical Sciences, pages 129\u2013136. New York: Wiley, 1988.","cites":null},{"id":16663646,"title":"Seminar f\u00a8 ur Statistik,","authors":[],"date":"2008","doi":null,"raw":"Technical report, Seminar f\u00a8 ur Statistik, ETH Z\u00a8 urich, 2008.","cites":null},{"id":977677,"title":"Seminar fu\u00a8r Statistik,","authors":[],"date":"2008","doi":null,"raw":null,"cites":null},{"id":976360,"title":"Simple improvements on Corn\ufb01eld\u2019s approximation to the mean of a noncentral hypergeometric random variable.","authors":[],"date":"1984","doi":null,"raw":"B. Levin. Simple improvements on Corn\ufb01eld\u2019s approximation to the mean of a noncentral hypergeometric random variable. Biometrika, 71:630\u2013632, 1984.","cites":null},{"id":979413,"title":"Soft modeling by latent variables: the nonlinear iterative partial least squares approach.","authors":[],"date":"1975","doi":null,"raw":"H. Wold. Soft modeling by latent variables: the nonlinear iterative partial least squares approach. New York: Academic Press, 1975.","cites":null},{"id":976650,"title":"Sparse su\ufb03cient dimension reduction.","authors":[],"date":"2007","doi":null,"raw":"L. Li. Sparse su\ufb03cient dimension reduction. Biometrika, 94(3):603\u2013613, 2007. W. Massey. Principal components regression with exploratory statistical research. J. Am.","cites":null},{"id":977428,"title":"Stability selection.","authors":[],"date":null,"doi":null,"raw":"N. Meinshausen and P. B\u00a8 uhlmann. Stability selection. Technical report, Seminar f\u00a8 ur Statistik, ETH Z\u00a8 urich, 2008.REFERENCES 28 N. Meinshausen, L. Meier, and P. B\u00a8 uhlmann. P-values for high-dimensional regression.","cites":null},{"id":979533,"title":"The adaptive lasso and its properties.","authors":[],"date":null,"doi":"10.1198\/016214506000000735","raw":"H. Zou. The adaptive lasso and its properties. J. Amer. Stat. Assoc., 101(476):1418\u20131429, 2006.REFERENCES 29 H. Zou and T. Hastie. Regularization and variable selection via the elastic net. J. R. Stat.","cites":null},{"id":974383,"title":"The Dantzig selector: statistical estimation when p is much larger than n.","authors":[],"date":"2007","doi":null,"raw":"E. Candes and T. Tao. The Dantzig selector: statistical estimation when p is much larger than n. Ann. Stat., 35(6):2313\u20132351, 2007.","cites":null},{"id":975567,"title":"The Elements of Statistical Learning.","authors":[],"date":"2001","doi":null,"raw":"T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer, 2001.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-10-30","abstract":null,"downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/69258.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/27756\/1\/det%2Dtechrep%2D06%2D10.pdf","pdfHashValue":"47c0b0b4d04aa22b38eb7c69a2db709c86be25d2","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:27756<\/identifier><datestamp>\n      2018-01-24T02:51:22Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Direct effects testing : a two-stage procedure to test for effect size and variable importance for correlated binary predictors and a binary response.<\/dc:title><dc:creator>\n        Sperrin, Matthew<\/dc:creator><dc:creator>\n        Jaki, Thomas<\/dc:creator><dc:subject>\n        QA Mathematics<\/dc:subject><dc:date>\n        2010-10-30<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        NonPeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/27756\/1\/det%2Dtechrep%2D06%2D10.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1002\/sim.4014<\/dc:relation><dc:identifier>\n        Sperrin, Matthew and Jaki, Thomas (2010) Direct effects testing : a two-stage procedure to test for effect size and variable importance for correlated binary predictors and a binary response. Statistics in Medicine, 29 (24). pp. 2544-2556. ISSN 1097-0258<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/27756\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1002\/sim.4014","http:\/\/eprints.lancs.ac.uk\/27756\/"],"year":2010,"topics":["QA Mathematics"],"subject":["Journal Article","NonPeerReviewed"],"fullText":"Direct Effects Testing: A Two-Stage procedure to Test for\nEffect Size and Variable Importance for Correlated Binary\nPredictors and a Binary Response\nM. SPERRIN T. JAKI\nJune 10, 2009\nAbstract\nIn applications such as medical statistics and genetics, we encounter situations where\na large number of highly correlated predictors explain a response. For example, the\nresponse may be a disease indicator and the predictors may be treatment indicators or\nsingle nucleotide polymorphisms (SNPs). Constructing a good predictive model in such\ncases is well studied. Less well understood is how to recover the \u2018true sparsity pattern\u2019,\nthat is finding which predictors have direct effects on the response, and indicating the\nstatistical significance of the results. Restricting attention to binary predictors and\nresponse, we study the recovery of the true sparsity pattern using a two-stage method\nthat separates establishing the presence of effects from inferring their exact relationship\nwith the predictors. The uncertainty in the relationship between the predictors and the\nrecovered effects is represented by a discrete distribution giving the likelihood of the\neffect originating from each of a collection of predictors. Simulations and a real data\n1\n1. Introduction 2\napplication demonstrate the method discriminates well between associations and direct\neffects. Comparisons with lasso based methods demonstrate favourable performance of\nthe proposed method.\nKeywords: Contingency table; Direct effect; High Dimensional; Lasso; Noncentral hy-\npergeometric distribution; Sparsity.\n1 Introduction\nIt is commonplace in applications of statistics to encounter situations in which a large\nnumber of predictors are available to explain a response. Consider the classical regression\nY = X\u03b2 + \u03b5, (1)\nwhere Y is an n \u00d7 1 response vector explained by an n \u00d7 p design matrix X through\nan unknown p\u00d7 1 coefficient vector \u03b2 with n\u00d7 1 noise vector \u03b5. Having a large number of\npredictors, p, possibly even p > n, should intuitively be beneficial, as we are maximising\nthe information available to explain the response. From the perspective of producing a\ngood predictive model, this is true, and many methods are available for this objective, such\nas principal component regression [Massey, 1965], partial least squares [Wold, 1975], ridge\nregression [Hoerl and Kennard, 1988] and more recent methods such as sparse sufficient\ndata reduction [Li, 2007].\nIn this paper we are interested in recovering the so-called \u2018true sparsity pattern\u2019 [Wasser-\nman and Roeder, 2009], in which we search for a subset of predictors deemed to have a \u2018direct\n1. Introduction 3\neffect\u2019 on the response \u2014 that is an effect that is causally attributed to the predictor in\nquestion rather than being due to the correlation of the predictor with other important\npredictors. We wish to find a sparse solution to the regression given in Equation (1) and in\nparticular carry out significance tests of variable importance. The lasso [Tibshirani, 1996]\nis a very popular sparse estimator, where sparsity is induced by applying an L1 penalty\nto the size of the vector \u03b2. It is computationally fast thanks to the least angle regression\nalgorithm (LARS) of Efron et al. [2004]. Other possibilities for sparse estimation include\nsubset selection [Breiman, 1995], the Dantzig selector [Candes and Tao, 2007] and sure in-\ndependence screening [Fan and Lv, 2008]. For the lasso, much work has been carried out\nconcerning consistency in terms of sparse pattern recovery [see for example Knight and Fu,\n2000, Zou, 2006, Bunea et al., 2007].\nUntil recently, it has not been possible to reliably ascertain significance of parameters\nincluded in a sparse model, that is to test for variable importance. Although standard errors\nof lasso parameters are available [Tibshirani, 1996, Osborne et al., 1998] these are difficult to\ninterpret because of the discontinuity of the sampling distribution of the parameters. In the\nsituation where the predictors in the model are not too highly correlated, recent methods\nthat address this include the \u2018screen and clean\u2019 method [Wasserman and Roeder, 2009,\nMeinshausen et al., 2008], and stability selection [Meinshausen and Bu\u00a8hlmann, 2008]. Such\nmethods are also appropriate when, in the highly correlated predictor case, it is satisfactory\nto recover predictors that are correlated with those that are truly causal. However, carrying\nout significance tests in the presence of multicollinearity is, according to Meinshausen [2008],\np266, \u2018in some sense ill-posed\u2019.\n1. Introduction 4\nThere are many situations, however, when multicollinearity can be serious, and we\nare interested nevertheless in recovering the true sparsity pattern, along with ascertaining\nthe significance of our result. For example, in genomewide association studies we study a\nnumber of sites on the genome called single nucleotide polymorphisms (SNPs) which are\nhighly correlated with each other. We would like to identify exact regions on the genome\nthat influence the risk of disease, so that appropriate interventions can be considered. The\nproblem of multicollinearity can be seen by considering a group J of highly correlated\npredictors, one of which has a true non-zero regression coefficient (or direct effect). Then\nthe lasso tends to select one variable from J , but there is no stability in which variable\nis selected. This is noted by Zou and Hastie [2005], who propose as a solution the \u2018elastic\nnet\u2019, which modifies the lasso by adding an L2 penalty, that promotes inclusion of all the\npredictors in the group J . Whilst this improves the sensitivity of recovering the sparsity\npattern, this is at the expense of inclusion of a potentially large number of noise predictors in\nthe model, and effect sizes becoming difficult to interpret because they are \u2018shared\u2019 amongst\nthe correlated predictors. Such an approach is useful, for example, in the recovery of gene\nnetworks, but not for the true sparsity recovery problem considered here. Meinshausen\n[2008] adopts a hierarchical approach, in which he looks for significance at the level of\ngroups of variables, rather than the level of individual variables. This is sensible, since in\nthe case of the group J of highly correlated predictors, it can be easy to identify that at least\none member of the group has a direct effect, but difficult or impossible to identify which\nmember(s) of the group have the effect. However, the method relies upon the selection of an\nappropriate hierarchical clustering regime, and it is apparent that the results will depend\n1. Introduction 5\nupon the clustering method chosen.\nIn this paper we introduce a two-stage method that allows separation of the two inherent\nkinds of uncertainty: presence of an effect (sufficiently large to be deemed significant) and\nwhich predictor(s) the effect is allied to. The application of the method is to \u2018fine mapping\u2019\nproblems \u2014 those where the correlation is particularly high \u2014 and in particular may violate\nthe standard correlation structure assumptions relied upon by other methods for consistency\nresults [see Meinshausen and Bu\u00a8hlmann, 2008, for a summary of these assumptions and\nfurther references]. Consequently, our method makes no claims about consistency of variable\nselection. Instead, the idea is to acknowledge uncertainty about which predictor is the source\nof a given effect by providing probabilities that a direct effect arises from each of a collection\nof predictors. Currently, we restrict attention to binary predictors and response. The key\nelement of the method is a novel recasting of the regression problem as\nZ = EM + \u000f, (2)\nwhere Z is a p \u00d7 1 vector constructed to represent the marginal association of each\npredictor with the response, M is an unknown p \u00d7 1 vector containing the direct effect of\neach predictor with the response, and E is a p \u00d7 p effect matrix constructed to translate\nthe direct effects into the observed associations, by considering the correlation structure of\nthe predictors. These objects are formally defined in Section 2. We estimate M via lasso\nregression [Tibshirani, 1996], where Z is taken as the response and E the design matrix, to\ngive a collection of direct effects that are coherent with the observed association structure\nof all the predictors with the response. We then separately consider the uncertainty of M\n1. Introduction 6\nin terms of the size of the effect, and which predictor is linked to the effect. The main\nadvantage of considering the regression in Equation (2) rather than Equation (1) is that,\nunder some assumptions, distributions for the effect size, not influenced by multicollinearity,\nare readily available. The output of the method is then a collection of significant direct\neffects, each with a probability distribution expressing the uncertainty in the associated\npredictor across a set of predictors. We call the method direct effect testing (DET).\nThe method is similar in spirit to Meinshausen [2008] in that we identify significant\neffects but acknowledge uncertainty about the specific predictors involved, but here we are\nable to specify relative confidence in each predictor being the origin of a given effect. Also,\nwhilst the method of Meinshausen [2008] can be considered a \u2018top down\u2019 approach, starting\nout with large clusters, and gradually moving down the hierarchy to smaller clusters, our\nmethod works in the opposite direction, since we test on an individual predictor level for\neffects, then generate a cluster that contains potential predictors for the true origin of a\ngiven effect.\nIn the remainder of the manuscript, we formally define the methodology in Section 2,\nbefore we investigate its behaviour on simulated data in Section 3 and real data in Section\n4. We conclude with a summary and discussion in Section 5.\n2. Method 7\nTable 1: Notation for a 2\u00d72 contingency table for a binary response Y and binary predictor\nXj\nObserved Counts\nY = 0 Y = 1 Total\nXj = 0 aj bj t0j\nXj = 1 cj dj t1j\nTotal s r n\n2 Method\n2.1 Definitions and Notation\nSuppose we are interested in a binary response Y , and its relationship to a set of p binary\npredictorsX = (X1, . . . , Xp). Consider the situation where we have n complete observations\nof the form (yi, xi1, . . . , xip) \u2208 {0, 1}p+1, i = 1, . . . , n. Table 1 gives further notation that\nwill be used. Without loss of generality we assume in the sequel that cor(Xj , Y ) \u2265 0,\nj = 1, . . . , p, reversing the binary coding for Xj whenever this does not hold.\nWe will use the language of graph theory to introduce the concept of direct effects \u2014\nsee, for example, Pearl [2009] for an introduction. Consider an undirected graph G = (V, E)\nwith vertex set V and edge set E . Let the vertices correspond to the p+ 1 binary variables\n{Y = X0, X1, . . . , Xp}, and the edges correspond to dependencies between the vertices.\nWith the understanding that X0 = Y , the edge (j1, j2) is absent if and only if Xj1 and Xj2\n2.1 Definitions and Notation 8\nare conditionally independent given all the other variables, i.e.\nXj1 \u22a5\u22a5 Xj2 |X\u2212(j1,j2),\nwhere X\u2212(j1,j2) means all the variables except Xj1 and Xj2 .\nWe are interested in dependencies between the variables X and the response Y . A path\nis an unbroken sequence of edges through the graph; two variables are connected if there\nexists a path between them. If Xj and Y are connected they are not independent, and\nhence associated.\nA hypothesis test of this association is\nHj0 : Xj is not associated with Y ,\nHj1 : Xj is associated with Y .\nSince all the variables are binary, the null hypothesis Hj0 implies that the count aj in the\ncontingency table (Table 1) is distributed according to a hypergeometric distribution with\nmean \u00b50j and variance \u03c320j , given by\n\u00b50j =\nst0j\nn\n,\n\u03c320j =\nrst0jt1j\nn2(n\u2212 1) .\nTherefore, such a hypothesis test can be carried out using Fisher\u2019s exact test for each\nXj , j = 1, . . . , p.\nTwo variables are adjacent if there exists an edge between them (i.e. they are connected\nby a path of length one). If Xj and the response Y are adjacent we say there is a direct\n2.1 Definitions and Notation 9\neffect between Xj and Y . If there exists a path of length two between Xj and Y , we say\nthere is an indirect effect between Xj and Y . We ignore any path of length greater than\ntwo. There may be numerous indirect effects between Xj and Y , and direct and indirect\neffects can co-exist.\nA hypothesis test of a direct effect is\nH\u02dcj0 : Xj is not directly affecting Y ,\nH\u02dcj1 : Xj is directly affecting Y .\nRegardless of which of the above hypotheses apply, the count aj is distributed according\nto Fisher\u2019s noncentral hypergeometric distribution [McCullagh and Nelder, 1989] with, say,\nmean \u00b5\u02dc\u03c9j and variance \u03c3\u02dc2\u03c9j , under H\u02dc\nj\n\u03c9, \u03c9 = 0, 1. Under H\u02dc\nj\n1 the noncentrality of the\ndistribution is allowed to include a potential direct effect between Xj and Y , but under\nH\u02dcj0 the noncentrality accounts for indirect effects only. For convenience we will drop the\nsubscript \u03c9 when we talk about the noncentral hypergeometric distribution in general.\nThe mean, \u00b5\u02dcj , of Fisher\u2019s noncentral hypergeometric distribution is available when the\nnoncentrality of the distribution is known [McCullagh and Nelder, 1989]. In this application,\nhowever, the noncentrality is not known as it depends on the potential association of each\npredictor Xj with Y . Once the mean is known, the variance can be approximated by [Levin,\n1984]:\n\u03c3\u02dc2j \u2248\nngh\n(n\u2212 1)(t0jh+ t1jg) ,\ng = \u00b5\u02dcj(t0j \u2212 \u00b5\u02dcj), h = (s\u2212 \u00b5j)(\u00b5j + t1j \u2212 s). (3)\nThe mean of the noncentral distribution \u00b5\u02dcj can be written as the sum of the standard\n2.2 Noncentrality Model 10\nhypergeometric mean \u00b50j and some function of the noncentrality. We propose modelling\nthe noncentrality part explicitly as a linear combination of the direct and indirect effects\nbetween Xj and Y .\n2.2 Noncentrality Model\nThe first stage in constructing the direct effect testing model is to estimate the direct and\nindirect effects in a coherent framework that reflects the correlation structure of the dataset.\nLet zj = \u03c3\u221210j (aj \u2212 \u00b50j), so that zj is the count aj standardized to have zero mean and unit\nvariance under Hj0 . Either aj or zj could be used to test an association hypothesis between\nXj and Y .\nNext, define the event Ck = {Only predictor Xk has a direct effect with Y }. Under Ck,\nany path in the graph G from Y to any Xj with j 6= k must pass through Xk \u2014 we say Y\nis separated from X\u2212k. As a consequnce,\nY \u22a5\u22a5 Xj |Xk\nfor all j 6= k. Let ekj = E(zj | Ck, zk), which is the regression function of zj on zk, under the\ncondition Ck. Then clearly e\nj\nj = zj for each j, while for the general case, straightforward\nalgebra (see Appendix) can be used to derive\nekj = \u03c3\n\u22121\n0j\n{\nn\n(\n\u03b30,0ak\nak + bk\n+\n\u03b30,1ck\nck + dk\n)\n\u2212 \u00b50j\n}\n, (4)\nwhere\n\u03b3\u03c91,\u03c92 =\n1\nn\nn\u2211\ni=1\nI(xik = \u03c91, xij = \u03c92), (5)\nwith I(\u00b7) denoting the indicator function.\n2.2 Noncentrality Model 11\nWe now model zj , j = 1, . . . , p, as a linear combination of its indirect effect induced by\nthe other predictors, its own direct effect, and the residual noise, which can be written as\nzj =\np\u2211\nk=1\nmke\nk\nj + \u000fj , (6)\ngiven in vector form in Equation (2). In Equation (6), each mk denotes the direct effect\nbetween predictor Xk and Y . We expect most of these to be zero, and mk 6= 0 means that\npredictor Xk has a direct effect on the response Y . Also, if mk = 0 this corresponds to the\ntruth of the hypothesis H\u02dck0 . Since all mk, k = 1, . . . , p are unknown we will estimate them\nby m\u02c6k, k = 1, . . . , p via lasso regression [Tibshirani, 1996], using the least angle regression\nalgorithm [Efron et al., 2004]. In order to choose the constraint on the lasso, note that\nbecause E(\u000fj) = 0 for each j,\np\u2211\nj=1\nvar(\u000fj) = E\n(\n\u000f2j\n)\n=\np\u2211\nj=1\n\u03c3\u02dc21j\n\u03c320j\n. (7)\nWe therefore select the point on the lasso path where\n\u2211p\nj=1 \u000f\n2\nj is equal to its expectation\n(Equation 7). The noncentral variance \u03c3\u02dc21j depends upon the current noncentrality estimate,\nhence is recalculated for every step along the lasso path. We make no assumption about\nthe presence or absence of direct effects at this stage \u2014 this is controlled by the estimate\nM\u02c6 .\nThe model (6) is not homoskedastic because var(\u000fj) = \u03c32j \/\u03c3\n2\n0j , so the variances depend\non the size of the noncentrality of each predictor Xj . However, scaling by the standard\ndeviation under each Hj0 provides some stability. Furthermore, the more severe the non-\ncentrality of Xj , the smaller its variance tends to be, so there will not be points that exert\nexcessive leverage on the linear model due to large variances. Also, the \u000fjs are not inde-\n2.3 Stage One \u2014 Hypothesis Testing for Effect Size 12\npendent, they are partially determined through the correlation structure of the predictors.\nClassical regression carried out in a situation of non-independent errors leads to coefficient\nestimates that are still unbiased, but are unlikely to be the best linear unbiased estimator.\nIdeally, we would like to carry out the hypothesis tests (H\u02dcj0 , H\u02dc\nj\n1) to establish whether\nor not a direct effect exists between Xj and Y , for each j = 1, . . . , p. For a given j, if\nwe knew the direct effects on the other predictors, M\u2212j , we could calculate the indirect\neffect between Xj and Y , and hence the noncentrality of the noncentral hypergeometric\nnull distribution. Then, the distribution of aj under H\u02dc\nj\n0 would have mean \u00b5\u02dc0j and variance\n\u03c3\u02dc20j where\n\u00b5\u02dc0j = \u00b50j + \u03c30j\n\u2211\nk 6=j\nmke\nk\nj .\nThis comes about by taking the central mean \u00b50j , and estimating the null noncentrality\nparameter as a linear combination of all the indirect effects between Xj and Y , and then\n\u03c3\u02dc20j is estimated via Equation (3). Thus any remaining association can be attributed to a\ndirect effect.\nUnfortunately, we only have an estimate M\u02c6 , and hence we cannot carry out the above\nhypothesis tests explicitly. We therefore resort to a two-stage procedure in which we separate\nthe uncertainty in M\u02c6 into effect size uncertainty and predictor assignment uncertainty.\n2.3 Stage One \u2014 Hypothesis Testing for Effect Size\nRecall that for a set J of highly correlated predictors, one of which has a direct effect, the\nlasso tends to select one variable from the group, but there is no stability in which variable\nis selected. Therefore, the coefficient estimate m\u02c6j assigned to predictor Xj can be used to\n2.4 Stage Two \u2014 Uncertainty in Direct Effect Predictor Assignment 13\nestimate the size of the corresponding effect, but we must bear in mind that Xj may not be\nthe actual predictor from which the effect originates \u2014 it may be one of its neighbours in\nJ . We test for significance of the size of the effect assigned by the lasso to each predictor\nusing a Fisher\u2019s noncentral hypergeometric null distribution with the estimate M\u02c6 plugged\nin. Denoting the resulting mean by \u02c6\u02dc\u00b50j and the variance by \u02c6\u02dc\u03c320j ,\n\u02c6\u02dc\u00b50j = \u00b50j + \u03c30j\n\u2211\nk 6=j\nm\u02c6ke\nk\nj ,\nand again the variance is estimated via Equation (3). The test statistic is then calculated\nas\nT =\nzj \u2212 \u02c6\u02dc\u00b50j\n\u02c6\u02dc\u03c30j\n,\nand this can either be tested against the relevant non-central hypergeometric distribution,\nor provided the margins of the contingency table are sufficiently large, an approximation to\na standard normal distribution is possible. It is interesting that m\u02c6j = 0 could still lead to\nthe effect assigned by the lasso to Xj being deemed significant. The multiple testing issue\narising at this point can be addressed using one\u2019s favourite method of error control \u2014 we\nhave simply used a Bonferroni correction in this paper.\n2.4 Stage Two \u2014 Uncertainty in Direct Effect Predictor Assignment\nSuppose predictor Xj has a direct effect on the response Y , but is highly correlated with\npredictor Xk. Then by chance it may happen that cor(Xk, Y ) > cor(Xj , Y ), and thus the\nlasso wrongly identifies the effect on predictor Xk [see also Zou and Hastie, 2005]. For\neach detected effect, we therefore identify a class of predictors from which each effect could\n2.4 Stage Two \u2014 Uncertainty in Direct Effect Predictor Assignment 14\ntruly have originated. Moreover, we allocate a probability to each predictor in this class\nmeasuring the likelihood that the effect originated from that predictor. Returning to the\ngraph theory analogy, in the first stage we have established the number of edges originating\nfrom the response Y , and roughly where each edge leads. We now acknowledge uncertainty,\nover a small set of vertices, for each edge.\nWhen an effect is declared, in stage one, on a predictor Xk, we generate a class {Xj :\nj \u2208 J } of predictors highly correlated with Xk (including Xk itself). Then for each j \u2208 J\nwe would like to calculate pj|k = pr(Xj true direct effect|Xk declared direct effect).\nTo proceed we use the result that\npj|k \u221d odds(Xk declared DE|Xj true DE, Xj or Xk declared DE)\n\u00d7pr(Xj declared DE|Xj true DE)pr(Xj true DE), (8)\nwhere DE stands for direct effect. A proof is given in the Appendix. We make three\nassumptions in the sequel:\n1. The set J covers all reasonable predictors, in that pj|k is negligible for any j \/\u2208 J .\nWe discuss the choice of J at the end of this section.\n2. Each predictor is a-priori equally likely to be responsible for a direct effect on Y .\n3. For each j \u2208 J , pr(Xj declared DE|Xj true DE) is the same. In other words the\nsensitivity of the method does not depend on which predictor happens to possess the\neffect.\n2.4 Stage Two \u2014 Uncertainty in Direct Effect Predictor Assignment 15\nThese assumptions allow us to calculate pj|k for each j \u2208 J as\npj|k =\nodds(Xk declared DE|Xj true DE, Xj or Xk declared DE)\u2211\nl\u2208J odds(Xk declared DE|Xl true DE, Xl or Xk declared DE)\n. (9)\nWe now outline the procedure for calculating the right hand side of Equation (9). Sup-\npose an effect has been observed in stage one between Xk and Y . Let \u03b2k be the size of the\ndirect effect, measured as the change in the estimated effect size if Xk were changed from\n{Xk = 0} to {Xk = 1}, but all other variables X\u2212k were held constant, and let \u03b1k be the\nbaseline effect size under {Xk = 0}, with the other variables unchanged, so that\n\u03b2k = pr(Y{Xk=1} = 1)\u2212 pr(Y{Xk=0} = 1),\n\u03b1k = pr(Y{Xk=0} = 1). (10)\nWe estimate \u03b1k and \u03b2k using the association measure zk, with the indirect effects re-\nmoved,\n\u03b1\u02c6k =\nt0k \u2212 \u00b5k \u2212 \u03c3k(zk \u2212\n\u2211\nk 6=j mke\nk\nj )\nt0k\n,\n\u03b2\u02c6k =\nr \u2212 t0k + \u00b5k + \u03c3k(zk \u2212\n\u2211\nk 6=j mke\nk\nj )\nt1k\n\u2212 \u03b1\u02c6k, (11)\nsee the Appendix for further details.\nSuppose that Xj has a true direct effect on Y , but this effect has been detected, in stage\none, on predictor Xk. The effective number of observations that we can use to distinguish\nbetween Xj and Xk as the origin of the effect is given by\nNE(j, k) = n(\u03b3(0,1) + \u03b3(1,0)),\ni.e. when the two predictors take different values. Evidence towards Xj rather than Xk truly\npossessing direct effect, the \u2018truth\u2019 in this case, occurs when (Xj , Xk, Y ) = (0, 1, 0) or (1, 0, 1).\n2.4 Stage Two \u2014 Uncertainty in Direct Effect Predictor Assignment 16\nSuppose this happens ET (j, k) times. Evidence towards predictor k rather than predictor j\nhaving a direct effect, the incorrect conclusion, occurs when (Xj , Xk, Y ) = (0, 1, 1) or (1, 0, 0).\nSuppose this happens EF (j, k) times. It is clearly possible to observe EF (j, k) > ET (j, k),\nand is particularly likely for small \u03b2j , small n or large correlation between Xj and Xk, re-\nsulting in the aforementioned scenario, that Xk is wrongly detected as possessing the direct\neffect.\nUsing straightforward algebra (see Appendix),\nPEF (j,k) = pr [(Xj,Xk,Y) = (0, 1, 1) or (1, 0, 0) | Xj 6= Xk]\n=\n\u03b3(1,0)\u03b1k + \u03b3(0,1)(1\u2212 \u03b1k \u2212 \u03b2k)\n\u03b3(1,0) + \u03b3(0,1)\n, (12)\nwith \u03b3(\u03c91, \u03c92) as in Equation (5). For intuition, note that if we assume t0j = t0k this\nreduces to\nP\u02c6EF (j,k) =\n1\u2212 \u03b2k\n2\n.\nIt follows that\nEF (j, k) \u223c Binomial(NE(j, k), PEF (j,k)), (13)\nso we can use this to calculate, for each j \u2208 J ,\npr{EF (j, k) > ET (j, k)} = pr(EF > NE\/2).\nHowever, note the equality of events\n{EF (j, k) > ET (j, k)} = {k declared DE | j true DE, j or k declared DE} (14)\nthat comes about as a consequence of the behaviour of the lasso. Hence, recalling Equation\n(9), this gives us a mechanism to calculate pj|k for j \u2208 J .\n3. Simulated Data 17\nThere are various ways that J could be chosen. A cut-off value of \u03c1 could be found\nso that, where \u03c1jk is the correlation between Xj and Xk, pr{EF(j, k) > ET(j, k)} is small\nfor \u03c1\u02c6jk < \u03c1, i.e. Xj is very unlikely to be the true causal predictor associated with Xk.\nAlternatively, one could fix the size of J to, say, the ten predictors that are most highly\ncorrelated withXk; or in the spirit of Meinshausen [2008], one could consider using clustering\nalgorithms to select J . In the subsequent work, we adopt the first approach, and choose\n\u03c1 such that pr{EF(j, k) \u2265 ET(j, k) | \u03c1\u02c6jk < \u03c1} \u2264 0 \u00b701. Practically, provided conservative\nbounds are selected when choosing J the choice of the set is not important. Indeed, one\ncould simply allow J to contain all the predictors, in this case those predictors that are not\nhighly correlated with Xk would turn out to have a negligible probability of containing the\ntrue direct effect.\n3 Simulated Data\nWe will now evaluate direct effect testing on the \u2018ge03d2\u2019 dataset taken from the \u2018GenABEL\u2019\npackage [Aulchenko and Struchalin, 2008] in R [R Development Core Team, 2008]. This\ndataset contains n = 897 subjects, with p = 7480 SNPs measured on each subject. We\nrestrict our attention to dominant effects of the SNPs so that, in the usual coding of 0, 1\nor 2, we translate all the 2s to 1s. We select two disjoint subsets of the data (subsetting\non SNPs not observations), one with p = 2000 to study the p > n case, and the other with\np = 400 to look at the p < n case.\nWe study DET by simulating binary responses on the data, with various relationships\nto the binary predictors. Throughout this section we select the significance level for stage\n3. Simulated Data 18\none of DET via a Bonferroni correction to achieve a family-wise error rate of 0.05. We\nrecord two kinds of finds from the DET method \u2014 a \u2018primary find\u2019 (pfind) means that\na true causal predictor is identified by the first stage of the method, whilst a \u2018secondary\nfind\u2019 (sfind) means that a true causal predictor is contained in the set J associated with\na significant direct effect, and has a probability of at least 0.1 of being a direct effect. A\n\u2018false find\u2019 (ffind) occurs when a significant direct effect is found but there are no associated\nprimary or secondary finds.\nWe compare DET with a standard logistic regression with a lasso penalty, where the\nstrength of the penalty is chosen via BIC. We define a \u2018find\u2019 under the standard lasso\noccurring when a true causal predictor is assigned a non-zero coefficient. Significance testing\nis not appropriate because the relatively small sample size coupled with the multicollinearity\nof the dataset means that we do not find coefficients that are significantly different from\nzero. A lasso \u2018false find\u2019 (ffind) occurs when a non-zero coefficient is assigned to a non-\ncausal predictor. We additionally compare with the \u2018screen and clean\u2019 (S&C) method of\nWasserman and Roeder [2009], where the strength of the penalty in the \u2018screen\u2019 stage is\nchosen via BIC. In Wasserman and Roeder [2009] cross validation is used to determine\nthe penalty for the \u2018screen\u2019 stage \u2014 this leads to more variables being carried forward to\nthe \u2018clean\u2019 stage, compared with BIC, and hence more true and false finds. Due to the\nhigh multicollinearity in this particular dataset, the increase in false finds was particularly\ndamaging for both the lasso and the \u2018screen and clean\u2019 methods, so using BIC seemed to\ngive more favourable results for these methods. The significance level for the \u2018screen\u2019 stage\nof the screen and clean procedure is again chosen to achieve a family-wise error rate of 0.05.\n4. Heart Disease Data 19\nIt must be noted that, for the lasso and screen and clean methods, a find is usually\ndeclared to have occurred when a non-zero co-efficient is found on a predictor highly corre-\nlated with the causal predictor. However we are considering the case when it is of interest\nto recover the causal predictor exactly.\nFor each of the p > n and p < n cases, we carry out 100 independent simulations,\nwhere in each case, causal predictor(s) are randomly selected, and a response is simulated\nvia various relationships to these causal predictor(s). We study here cases of one and two\ncausal predictors, with effect sizes of 10% and 20%. Table 2 gives the results for the p > n\ncase and Table 3 gives the results for the p < n case. The number of finds made by lasso\nand DET are very similar, despite DET implementing a stringent significance test and lasso\nmerely reporting non-zero coefficients. In addition, the lasso makes a larger number of false\nfinds in general. The screen and clean method achieves similar false find control to DET,\nbut this is at the expense of a far smaller number of true finds.\n4 Heart Disease Data\nWe now illustrate the method on a real dataset. The Coronary Risk-Factor Study [Rousseauw\net al., 1983] was carried out in three rural areas in South Africa, in the White Cape region,\nwhere incidence of heart disease is particularly high. A subset of the study is analysed\nextensively in Hastie et al. [2001]. In this subset a binary response is measured, whether\nor not the subject has heart disease, and 160 cases and 302 controls are collected. Each\nsubject has nine measurements taken as predictors. These are \u2018sbp\u2019 (systolic blood pres-\nsure); \u2018tobacco\u2019 (cumulative tobacco); \u2018ldl\u2019 (low density lipoprotein cholesterol); \u2018adiposity\u2019;\n4. Heart Disease Data 20\nTable 2: Comparison of lasso and DET finds for p > n case for various effect sizes, for one\nand two causal predictors\nEffect 0.2 0.1 (0.2,0.2) (0.1,0.1)\nLasso finds 36 1 95 5\nLasso ffinds 24 5 45 11\nS&C finds 17 0 28 0\nS&C ffinds 10 7 12 2\nDET pfinds 37 2 80 6\nDET sfinds 7 1 17 0\nDET finds 44 3 97 6\nDET ffinds 11 4 19 5\n\u2018famhist\u2019 (family history of heart disease); \u2018typea\u2019 (type-A behaviour); \u2018obesity\u2019; \u2018alcohol\u2019\n(current alcohol consumption); and \u2018age\u2019 (age at onset, or age of testing for controls). To\nillustrate DET, we have dichotomized the predictors where necessary, by setting a single\nthreshold level, at an appropriate point where possible: for example, the \u2018obesity\u2019 predictor\nmeasures Body Mass Index (BMI) so we we have used 30 as the cut-off point, since persons\nwith a BMI exceeding 30 are classed as obese.\nWe then carry out five analyses on the dichotomized data: the standard single predictor\nassociation test, a standard logistic regression, a logistic regression with lasso penalty, the\nscreen and clean method and the direct effect testing method. Results of the single predictor\ntest, the logistic regression and the screen and clean method are given in Table 4. For the\n4. Heart Disease Data 21\nTable 3: Comparison of lasso and DET finds for p < n case for various effect sizes, for one\nand two causal predictors\nEffect 0.2 0.1 (0.2,0.2) (0.1,0.1)\nLasso finds 53 6 115 11\nLasso ffinds 27 3 51 9\nS&C finds 25 2 48 5\nS&C ffinds 9 3 13 4\nDET pfinds 49 3 82 13\nDET sfinds 7 0 15 1\nDET finds 56 3 97 14\nDET ffinds 10 4 17 4\nscreen and clean method, some variables are \u2018dropped\u2019 at the screen stage, so they do not\nhave associated p-values. For the lasso method, four non-zero coefficients were identified\n\u2014 on \u2018tobacco\u2019, \u2018ldl\u2019, \u2018famhist\u2019 and \u2018age\u2019. For the direct effect testing method, four direct\neffects were found at the Bonferroni significance level of 0.0056, and the details are in Table\n5. The probabilities in Table 5 do not always sum to one, due to rounding and exclusion of\npredictors with low (< 0\u00b701) probabilities, using the cut-off rule specified in Section 2.4.\nTo summarize the findings of the DET analysis, we are virtually certain that \u2018age\u2019,\n\u2018famhist\u2019 and \u2018tobacco\u2019 have a direct effect on heart disease, this is reflected in the small\np-values in both the logistic regression and the single predictor analysis. There is a possible\nfourth direct effect, and \u2018tobacco\u2019 re-appears as a possible predictor to possess this direct\n4. Heart Disease Data 22\nTable 4: Comparing p-values calculated via the standard single predictor test and a logistic\nregression, for the heart disease data\nCovariate Single Predictor Logistic Regression S& C\nage 1\u00b71\u00d7 10\u221211 9\u00b70\u00d7 10\u22124 3\u00b77\u00d7 10\u22123\nfamhist 4\u00b78\u00d7 10\u22129 1\u00b71\u00d7 10\u22125 7\u00b70\u00d7 10\u22123\nldl 4\u00b74\u00d7 10\u22127 6\u00b79\u00d7 10\u22122 3\u00b74\u00d7 10\u22122\nadiposity 4\u00b74\u00d7 10\u22126 2\u00b74\u00d7 10\u22121 dropped\ntobacco 3\u00b72\u00d7 10\u22127 1\u00b70\u00d7 10\u22121 8\u00b73\u00d7 10\u22122\ntypea 2\u00b73\u00d7 10\u22121 4\u00b73\u00d7 10\u22122 dropped\nsbp 8\u00b71\u00d7 10\u22124 2\u00b78\u00d7 10\u22121 dropped\nalcohol 1\u00b73\u00d7 10\u22121 7\u00b70\u00d7 10\u22121 dropped\nobesity 1\u00b73\u00d7 10\u22121 3\u00b73\u00d7 10\u22121 dropped\n4. Heart Disease Data 23\nTable 5: Details from direct effect testing method for heart disease data\nDirect Effect p-value Location Probability\n4\u00b75\u00d7 10\u22128 age 1\n3\u00b70\u00d7 10\u22126 famhist 1\n2\u00b71\u00d7 10\u22124 tobacco 1\n1\u00b73\u00d7 10\u22123 tobacco 0\u00b764\nldl 0\u00b731\nage 0\u00b702\ntypea 0\u00b702\nadiposity 0\u00b701\n5. Discussion 24\neffect. We interpret this as either evidence that the direct effect is elsewhere so that \u2018ldl\u2019\nbecomes the most likely origin for the fourth direct effect; an interaction effect; or evidence\nthat this fourth direct effect is in fact a false positive.\n5 Discussion\nIn this paper we have introduced, for binary predictors and response, a method that sep-\narates the testing for the presence of a direct effect and the selection of the predictor that\nproduces the effect. This allows, in the first stage, direct effect hypothesis tests to be car-\nried out in the presence of highly correlated predictors without suffering multicollinearity\nissues. The uncertainty in the assignment of a direct effect to a predictor, caused by the\nmulticollinearity, is taken into account in the second stage, so that the method gives a set\nof predictors that could represent each direct effect, with probabilities on each predictor\nin the set. We demonstrate that the method works effectively to find single and multi-\nple direct effects, and compares very favourably with the lasso. Whilst similar methods are\navailable [Meinshausen, 2008], DET is unique in offering a probabilistic assessment of which\npredictors could be associated with the detected effect.\nThe second stage of the method can be viewed from a Bayesian perspective, by relaxing\nassumption 2 given in Section 2.4, and instead placing a discrete prior on pr(Xj true DE).\nThe enforcement of assumption 2 corresponds to a uniform prior.\nThe method easily handles missing data, provided we use the missing completely at\nrandom assumption [Rubin, 1976]. Since we deal with cell counts only, values, i.e. a spe-\ncific xij , that are missing at any point can be excluded from the count, and therefore no\n5. Discussion 25\nimputation is required. The column totals in Table 1 would then depend on j so we would\nreplace s by sj , and so forth.\nOne of the shortcomings of the method is that it does not allow for multiple levels of\nthe predictor variables. One way to address this issue is by introducing multiple binary\npredictors for a single discrete predictor. For example, consider a three level predictor Xj ,\ntaking values 0,1 or 2. Then we introduce two binary predictors, Xj1 andXj2 . CodeXj1 = 1\nif Xj \u2265 1 and Xj1 = 0 if Xj = 0; and code Xj2 = 1 if Xj = 2 and Xj2 = 0 if Xj \u2264 1. A\nmore general extension that makes use of the multivariate hypergeometric distribution will\nbe investigated in the future.\nAnother interesting point for future investigation are the connections of the introduced\nmethod to Genomic Control [Devlin and Roeder, 1999, Devlin et al., 2001] and Delta Cen-\ntralisation [Gorroochurn et al., 2006], which are methods used to account for subpopula-\ntion structure or other unobserved confounding effects in a dataset, particularly applied in\ngenetic contexts. This is achieved by assuming the better known noncentral \u03c72 null distri-\nbution in tests of association, with a noncentrality parameter \u03bd that is common to all tests.\nThis begs the question of whether the direct effect testing method can be used in a similar\ncontext, and whether additional power is gained by allowing for a different noncentrality\nparameter for each test.\nThe most important generalization required for this method, perhaps, is to allow for\ncontinuous predictors and response. Whether this is possible remains an open question \u2014\nwe envisage that the main difficulties would be calculation of the matrix E, and whether it\nis possible to perform parametric hypothesis testing in this case.\nREFERENCES 26\nReferences\nY. Aulchenko and M. Struchalin. GenABEL: genome-wide SNP association analysis, 2008.\nR package version 1.4-0.\nL. Breiman. Better subset regression using the nonnegative garrote. Technometrics, 37:\n373\u2013384, 1995.\nF. Bunea, A. Tsybakov, and M. Wegkamp. Sparsity oracle inequalities for the lasso. Elec.\nJourn. Stat., 1:169\u2013194, 2007.\nE. Candes and T. Tao. The Dantzig selector: statistical estimation when p is much larger\nthan n. Ann. Stat., 35(6):2313\u20132351, 2007.\nB. Devlin and K. Roeder. Genomic control for association studies. Biometrics, 55 no. 4:\n997\u20131004, 1999.\nB. Devlin, K. Roeder, and L. Wasserman. Genomic control, a new approach to genetic-based\nassociation studies. Theoretical Population Biology, 60:155\u2013166, 2001.\nB. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Ann. Stat., 32\n(2):407\u2013499, 2004.\nJ. Fan and J. Lv. Sure independence screening for ultra-high dimensional feature space. J.\nRoy. Stat. Soc., 70:849\u2013911, 2008.\nP. Gorroochurn, G. A. Heiman, S. E. Hodge, and D. A. Greenberg. Centralizing the non-\nREFERENCES 27\ncentral chi-square: a new method to correct for population stratification in case-control\nassociation studies. Genetic Epidemiology, 30:277\u2013289, 2006.\nT. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer,\n2001.\nA. Hoerl and R. Kennard. Ridge regression. In Encyclopedia of Statistical Sciences, pages\n129\u2013136. New York: Wiley, 1988.\nK. Knight and W. Fu. Asymptotics for lasso-type estimators. Ann. Stat., 28:1356\u20131378,\n2000.\nB. Levin. Simple improvements on Cornfield\u2019s approximation to the mean of a noncentral\nhypergeometric random variable. Biometrika, 71:630\u2013632, 1984.\nL. Li. Sparse sufficient dimension reduction. Biometrika, 94(3):603\u2013613, 2007.\nW. Massey. Principal components regression with exploratory statistical research. J. Am.\nStatist. Ass., 60:234\u2013246, 1965.\nP. McCullagh and J. A. Nelder. Generalized Linear Models. Chapman & Hall, 1989.\nN. Meinshausen. Hierarchical testing of variable importance. Biometrika, 95(2):265\u2013278,\n2008.\nN. Meinshausen and P. Bu\u00a8hlmann. Stability selection. Technical report, Seminar fu\u00a8r Statis-\ntik, ETH Zu\u00a8rich, 2008.\nREFERENCES 28\nN. Meinshausen, L. Meier, and P. Bu\u00a8hlmann. P-values for high-dimensional regression.\nTechnical report, Seminar fu\u00a8r Statistik, ETH Zu\u00a8rich, 2008.\nM. R. Osborne, B. Presnell, and B. A. Turlach. On the lasso and its dual. Technical Report\n98\/1, Dept. Statistics, Univ. Adelaide, 1998.\nJ. Pearl. Causality. Cambridge University Press, second edition, 2009.\nR Development Core Team. R: A Language and Environment for Statistical Com-\nputing. R Foundation for Statistical Computing, Vienna, Austria, 2008. URL\nhttp:\/\/www.R-project.org. ISBN 3-900051-07-0.\nJ. Rousseauw, J. du Plessis, A. Benade, P. Jordaan, J. Kotze, P. Jooste, and J. Ferreira.\nCoronary risk factor screening in three rural communities. South African Medical Journal,\n64:430\u2013436, 1983.\nD. B. Rubin. Inference and missing data. Biometrika, 63:581\u2013592, 1976.\nR. Tibshirani. Regression shrinkage and selection via the lasso. J. R. Stat. Soc. B, 58(1):\n267\u2013288, 1996.\nL. Wasserman and K. Roeder. High dimensional variable selection. Ann. Stat., 2009. In\npress.\nH. Wold. Soft modeling by latent variables: the nonlinear iterative partial least squares\napproach. New York: Academic Press, 1975.\nH. Zou. The adaptive lasso and its properties. J. Amer. Stat. Assoc., 101(476):1418\u20131429,\n2006.\nREFERENCES 29\nH. Zou and T. Hastie. Regularization and variable selection via the elastic net. J. R. Stat.\nSoc. B, 67(2):301\u2013320, 2005.\nAppendix\nDerivation of Equation 4\nFirst, by definition\nekj = E(zj | Ck, zk) = \u03c3\u22121j\n{\nnpr(Xj = 0,Y = 0 | Ck, zk)\u2212 \u00b5j\n}\n,\nwhere the observation index i is suppressed. Assumption Ck means that Xj is conditionally\nindependent of Y , given Xk, since Xk separates X\u2212k from Y in the graph. Using this\nconditional independence, we then find\npr(Xj = 0,Y = 0 | Ck, zk) = pr(Xj = 0,Y = 0 | Xk = 0,Ck, zk)pr(Xk = 0 | Ck, zk)\n+pr(Xj = 0,Y = 0 | Xk = 1,Ck, zk)pr(Xk = 1 | Ck, zk)\n= pr(Xj = 0 | Xk = 0,Ck, zk)pr(Y = 0 | Xk = 0,Ck, zk)pr(Xk = 0 | Ck, zk)\n+pr(Xj = 0 | Xk = 1,Ck, zk)pr(Y = 0 | Xk = 1,Ck, zk)pr(Xk = 1 | Ck, zk),\n(15)\nWe proceed by collecting the first and third terms from each of the above lines, to give\nREFERENCES 30\npr(Xj = 0,Y = 0 | Ck, zk) = pr(Xj = 0,Xk = 0 | Ck, zk)pr(Y = 0 | Xk = 0,Ck, zk)\n+pr(Xj = 0,Xk = 1 | Ck, zk)pr(Y = 0 | Xk = 1,Ck, zk)\n=\n\u03b30,0ak\nak + bk\n+\n\u03b30,1ck\nck + dk\n,\nwhere the last line follows from deriving (ak, bk, ck, dk) from zk, which we have conditioned\non throughout.\nDerivation of Equation (8)\nWrite pj|k = pr(Xj true|Xk dec.), abbreviating in the obvious way. Now by Bayes\u2019 Theorem,\npj|k \u221d pr(Xk dec.|Xj true)pr(Xj true).\nBut\npr(Xk dec.|Xj true) = pr(Xk dec.|Xj true, Xk or Xj dec.)pr(Xk or Xj dec.|Xj true)\n= pr(Xk dec.|Xj true, Xk or Xj dec.)\n{\npr(Xk dec.|Xj true) + pr(Xj dec.|Xj true)\n}\n,\nthen re-arranging gives\npr(Xk dec.|Xj true) = pr(Xk dec.|Xj true, Xk or Xj dec.)pr(Xj dec.|Xj true)1\u2212 pr(Xk dec.|Xj true, Xk or Xjdec.) .\nSo that\npj|k \u221d odds(Xk dec.|Xj true, Xk or Xj dec.)pr(Xj dec.|Xj true)pr(Xj true)\nas required.\nREFERENCES 31\nDerivation of Equation (11)\nReferring to Table 1, if we were interested in the size of the association between Xk and Y ,\nwe would estimate this as\nPr(Y = 1 | Xk = 0) = bk\nt0k\n=\nt0k \u2212 ak\nt0k\n=\nt0k \u2212 \u00b5k \u2212 \u03c3kzk\nt0k\n,\nwhere we replace ak by the noncentrality model of Equation (6). Removing the indirect\neffect part,\n\u2211\nj 6=kmje\nj\nk then immediately yields \u03b1\u02c6k in Equation (11).\nIn order to find the expression for \u03b2\u02c6k, consider\nPr(Y = 1 | Xk = 1) = dk\nt1k\n=\nr \u2212 t0k + ak\nt1k\n=\nr \u2212 t0k + \u00b5k + \u03c3kzk\nt1k\n.\nRemoving the indirect effect part and subtracting \u03b1\u02c6k then yields the desired result.\nDerivation of Equation (12)\nRecall that we assume a true direct effect between Xj and Y . We then find\nPEF (j,k) = pr {(Xj,Xk,Y) = (0, 1, 1) or (1, 0, 0) | Xj 6= Xk}\n=\npr {(Xj,Xk,Y) = (0, 1, 1)}+ pr {(Xj,Xk,Y) = (1, 0, 0)}\npr {(Xj,Xk) = (0, 1)}+ pr {(Xj,Xk) = (1, 0)]}\n=\n\u03b3(1,0)\u03b1k + \u03b3(0,1)(1\u2212 \u03b1k \u2212 \u03b2k)\n\u03b3(1,0) + \u03b3(0,1)\n,\nREFERENCES 32\nwhere the last line is obtained by writing pr (Xj,Xk,Y) = pr (Y | Xj,Xk) pr (Xj,Xk), and\nusing Equation (10) for the conditional probabilities of Y .\n"}