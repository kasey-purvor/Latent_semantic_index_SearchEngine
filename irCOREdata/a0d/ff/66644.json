{"doi":"10.1137\/S0097539701399551","coreId":"66644","oai":"oai:dro.dur.ac.uk.OAI2:637","identifiers":["oai:dro.dur.ac.uk.OAI2:637","10.1137\/S0097539701399551"],"title":"The natural work-stealing algorithm is stable.","authors":["Berenbrink,  P.","Friedetzky,  T.","Goldberg,  L. A."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2003-01","abstract":"In this paper we analyze a very simple dynamic work-stealing algorithm. In the work-generation model, there are n (work) generators. A generator-allocation function is simply a function from the n generators to the n processors. We consider a fixed, but arbitrary, distribution $\\cal D$ over generator-allocation functions. During each time step of our process, a generator-allocation function h is chosen from $\\cal D$, and the generators are allocated to the processors according to h. Each generator may then generate a unit-time task, which it inserts into the queue of its host processor. It generates such a task independently with probability $\\lambda$. After the new tasks are generated, each processor removes one task from its queue and services it. For many choices of $\\cal D$, the work-generation model allows the load to become arbitrarily imbalanced, even when $\\lambda < 1$. For example, $\\cal D$ could be the point distribution containing a single function h which allocates all of the generators to just one processor. For this choice of $\\cal D$, the chosen processor receives around $\\lambda n$ units of work at each step and services one. The natural work-stealing algorithm that we analyze is widely used in practical applications and works as follows. During each time step, each empty processor (with no work to do) sends a request to a randomly selected other processor. Any nonempty processor having received at least one such request in turn decides (again randomly) in favor of one of the requests. The number of tasks which are transferred from the nonempty processor to the empty one is determined by the so-called work-stealing functionf. In particular, if a processor that accepts a request has $\\ell$ tasks stored in its queue, then $f(\\ell)$ tasks are transferred to the currently empty one. A popular work-stealing function is $f(\\ell)=\\lfloor \\ell\/2\\rfloor$, which transfers (roughly) half of the tasks. We analyze the long-term behavior of the system as a function of $\\lambda$ and f. We show that the system is stable for any constant generation rate $\\lambda < 1$ and for a wide class of functions f. Most intuitively sensible functions are included in this class (for example, every monotonically nondecreasing function f which satisfies $0 \\leq f(\\ell)\\leq \\ell\/2$ and $f(\\ell)=\\omega(1)$ as a function of $\\ell$ is included). Furthermore, we give upper bounds on theaverage system load (as a function of f and n). Our proof techniques combine Lyapunov function arguments with domination arguments, which are needed to cope with dependency.\\u","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/66644.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/637\/1\/637.pdf","pdfHashValue":"7c922ba3959af1a1bdb6dab6c6c754ca043d5c26","publisher":"Society for Industrial and Applied Mathematics","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:637<\/identifier><datestamp>\n      2011-06-15T15:54:54Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        The natural work-stealing algorithm is stable.<\/dc:title><dc:creator>\n        Berenbrink,  P.<\/dc:creator><dc:creator>\n        Friedetzky,  T.<\/dc:creator><dc:creator>\n        Goldberg,  L. A.<\/dc:creator><dc:description>\n        In this paper we analyze a very simple dynamic work-stealing algorithm. In the work-generation model, there are n (work) generators. A generator-allocation function is simply a function from the n generators to the n processors. We consider a fixed, but arbitrary, distribution $\\cal D$ over generator-allocation functions. During each time step of our process, a generator-allocation function h is chosen from $\\cal D$, and the generators are allocated to the processors according to h. Each generator may then generate a unit-time task, which it inserts into the queue of its host processor. It generates such a task independently with probability $\\lambda$. After the new tasks are generated, each processor removes one task from its queue and services it. For many choices of $\\cal D$, the work-generation model allows the load to become arbitrarily imbalanced, even when $\\lambda < 1$. For example, $\\cal D$ could be the point distribution containing a single function h which allocates all of the generators to just one processor. For this choice of $\\cal D$, the chosen processor receives around $\\lambda n$ units of work at each step and services one. The natural work-stealing algorithm that we analyze is widely used in practical applications and works as follows. During each time step, each empty processor (with no work to do) sends a request to a randomly selected other processor. Any nonempty processor having received at least one such request in turn decides (again randomly) in favor of one of the requests. The number of tasks which are transferred from the nonempty processor to the empty one is determined by the so-called work-stealing functionf. In particular, if a processor that accepts a request has $\\ell$ tasks stored in its queue, then $f(\\ell)$ tasks are transferred to the currently empty one. A popular work-stealing function is $f(\\ell)=\\lfloor \\ell\/2\\rfloor$, which transfers (roughly) half of the tasks. We analyze the long-term behavior of the system as a function of $\\lambda$ and f. We show that the system is stable for any constant generation rate $\\lambda < 1$ and for a wide class of functions f. Most intuitively sensible functions are included in this class (for example, every monotonically nondecreasing function f which satisfies $0 \\leq f(\\ell)\\leq \\ell\/2$ and $f(\\ell)=\\omega(1)$ as a function of $\\ell$ is included). Furthermore, we give upper bounds on theaverage system load (as a function of f and n). Our proof techniques combine Lyapunov function arguments with domination arguments, which are needed to cope with dependency.\\ud\n<\/dc:description><dc:subject>\n        Work-stealing<\/dc:subject><dc:subject>\n         Dynamic<\/dc:subject><dc:subject>\n         Stability.<\/dc:subject><dc:publisher>\n        Society for Industrial and Applied Mathematics<\/dc:publisher><dc:source>\n        SIAM journal on computing, 2003, Vol.32(5), pp.1260-1279 [Peer Reviewed Journal]<\/dc:source><dc:date>\n        2003-01<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:637<\/dc:identifier><dc:identifier>\n        issn:0097-5397<\/dc:identifier><dc:identifier>\n        issn: 1095-7111<\/dc:identifier><dc:identifier>\n        doi:10.1137\/S0097539701399551<\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/637\/<\/dc:identifier><dc:identifier>\n        http:\/\/dx.doi.org\/10.1137\/S0097539701399551<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/637\/1\/637.pdf<\/dc:identifier><dc:rights>\n        \u00a9 2003 Society for Industrial and Applied Mathematics<\/dc:rights><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["issn:0097-5397","0097-5397","issn: 1095-7111"," 1095-7111"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2003,"topics":["Work-stealing","Dynamic","Stability."],"subject":["Article","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n07 October 2008\nVersion of attached file:\nPublished Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nBerenbrink, P. and Friedetzky, T. and Goldberg, L. A. (2003) \u2019The natural work-stealing algorithm is stable.\u2019,\nSIAM journal on computing., 32 (5). pp. 1260-1279.\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1137\/S0097539701399551\nPublisher\u2019s copyright statement:\n2003 Society for Industrial and Applied Mathematics\nAdditional information:\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\nTHE NATURAL WORK-STEALING ALGORITHM IS STABLE\u2217\nPETRA BERENBRINK\u2020 , TOM FRIEDETZKY\u2020 , AND LESLIE ANN GOLDBERG\u2021\nSIAM J. COMPUT. c\u00a9 2003 Society for Industrial and Applied Mathematics\nVol. 32, No. 5, pp. 1260\u20131279\nAbstract. In this paper we analyze a very simple dynamic work-stealing algorithm. In the\nwork-generation model, there are n (work) generators. A generator-allocation function is simply a\nfunction from the n generators to the n processors. We consider a fixed, but arbitrary, distribution\nD over generator-allocation functions. During each time step of our process, a generator-allocation\nfunction h is chosen from D, and the generators are allocated to the processors according to h. Each\ngenerator may then generate a unit-time task, which it inserts into the queue of its host processor.\nIt generates such a task independently with probability \u03bb. After the new tasks are generated, each\nprocessor removes one task from its queue and services it. For many choices of D, the work-generation\nmodel allows the load to become arbitrarily imbalanced, even when \u03bb < 1. For example, D could be\nthe point distribution containing a single function h which allocates all of the generators to just one\nprocessor. For this choice of D, the chosen processor receives around \u03bbn units of work at each step\nand services one. The natural work-stealing algorithm that we analyze is widely used in practical\napplications and works as follows. During each time step, each empty processor (with no work to do)\nsends a request to a randomly selected other processor. Any nonempty processor having received\nat least one such request in turn decides (again randomly) in favor of one of the requests. The\nnumber of tasks which are transferred from the nonempty processor to the empty one is determined\nby the so-called work-stealing function f . In particular, if a processor that accepts a request has\n\u0006 tasks stored in its queue, then f(\u0006) tasks are transferred to the currently empty one. A popular\nwork-stealing function is f(\u0006) = \u0002\u0006\/2\u0003, which transfers (roughly) half of the tasks. We analyze the\nlong-term behavior of the system as a function of \u03bb and f . We show that the system is stable for any\nconstant generation rate \u03bb < 1 and for a wide class of functions f . Most intuitively sensible functions\nare included in this class (for example, every monotonically nondecreasing function f which satisfies\n0 \u2264 f(\u0006) \u2264 \u0006\/2 and f(\u0006) = \u03c9(1) as a function of \u0006 is included). Furthermore, we give upper bounds\non the average system load (as a function of f and n). Our proof techniques combine Lyapunov\nfunction arguments with domination arguments, which are needed to cope with dependency.\nKey words. work-stealing, dynamic, stability\nAMS subject classification. 60J20\nDOI. 10.1137\/S0097539701399551\n1. Introduction. Load balancing is the process of distributing load among a set\nof processors. There are two main approaches to distributed load balancing, namely,\nsender-initiated strategies, in which processors may decide to give away tasks, and\nreceiver-initiated strategies (which are often referred to as work-stealing), in which\nprocessors may request extra work. In both cases, the decision to transfer tasks is\ntypically threshold based. That is, it is based on having too many or too few tasks in\none\u2019s own queue.\nIn recent years, there has been a lot of work devoted to rigorously analyzing\nload balancing, most of which concentrates on sender-initiated approaches or related\n\u2217Received by the editors December 12, 2001; accepted for publication (in revised form) February\n13, 2003; published electronically August 6, 2003. This work was partially supported by EPSRC grant\nGR\/M96940 \u201cSharper Analysis of Randomised Algorithms: A Computational Approach\u201d and the\nFuture and Emerging Technologies Programme of the EU under contracts IST-1999-14186 (ALCOM-\nFT) and IST-1999-14036 (RAND-APX). A preliminary version of this article was published in the\nProceedings of the 42nd Annual Symposium on Foundations of Computer Science (FOCS\u201901), IEEE\nComputer Society Press, Los Alamitos, CA, 2001.\nhttp:\/\/www.siam.org\/journals\/sicomp\/32-5\/39955.html\n\u2020School of Computing Science, Simon Fraser University, Burnaby, BC, V5A 1S6, Canada (petra@\ncs.sfu.ca, tkf@cs.sfu.ca).\n\u2021Department of Computer Science, University of Warwick, Coventry CV4 7AL, UK (leslie@dcs.\nwarwick.ac.uk).\n1260\nTHE NATURAL WORK-STEALING ALGORITHM IS STABLE 1261\nallocation processes such as balls-into-bins games. However, it appears that many\npractitioners prefer the receiver-initiated approach (work-stealing) because this ap-\nproach appears to work better for their applications. The efficiency of work-stealing\nprobably helps to explain the success of Leiserson et al.\u2019s language Cilk [10], a lan-\nguage for multithreaded parallel programming which uses work-stealing in its kernel.\nThere are numerous examples of practical applications of work-stealing. In [17], Feld-\nmann, Mysliwietz, and Monien investigate the behavior of parallel MIN\/MAX-tree\nevaluation in the context of parallel game (chess) programs employing work-stealing\nstrategies. In [14], Decker introduces VDS (virtual data space), a load-balancing sys-\ntem for irregular applications that makes use of work-stealing and other strategies. In\n[23], Mahapatra and Dutt use work-stealing for parallel branch and bound algorithms.\nDespite the practical usefulness of work-stealing, there are not many known theo-\nretical results about its performance. Most existing theoretical work on load balancing\nassumes a rather well-behaved system. For instance, most work on sender-initiated\nload balancing uses a work-generation model in which each processor generates at\nmost a constant number of new tasks per step. In balls-into-bins games, each ball\n(task) chooses its bin (processor) uniformly at random (u.a.r.), which also yields a\nrelatively balanced system.\nIn this paper we analyze a simple and fully distributed work-stealing algorithm.\nOur work-generation model allows for an arbitrary placement of n so-called generators\namong the set of n processors. Each generator generates a new task with a certain\nprobability \u03bb at each time step. In the extreme case, there can be one processor being\nhost to n generators. In this case the one processor has an expected increase of \u03bbn\u22121\ntasks per step, whereas all other processors do not generate tasks at all.\nOur load-balancing algorithm follows a very simple and natural work-stealing\napproach. At each time step, each empty processor sends a request to one randomly\nchosen other processor. Each nonempty processor having received at least one such\nrequest selects one of them randomly. Now each empty processor P whose request is\naccepted by a processor Q \u201csteals\u201d f(\u0006) tasks from Q, where \u0006 denotes Q\u2019s load.\nOur results are concerned mostly with the stability of the system. A system is\nsaid to be unstable if the system load (the sum of the load of all processors) grows\nunboundedly with time. We present both negative and positive results, depending on\nthe work-stealing function f . First we show that if the work-stealing function is \u03c9(1)\nas a function of the load (i.e., f(\u0006) = \u03c9(1) as a function of \u0006), then the system is\nstable (provided f is monotonically nondecreasing and satisfies 0 \u2264 f(\u0006) \u2264 \u0006\/2). This\nresult still holds if we put an upper bound on the amount of work that can be \u201cstolen\u201d\nby a single request. That is, for an upper bound hz which is independent of \u0006 (but\ndepends on n) and will be defined in Lemma 2, the work-stealing function defined by\nf \u2032(\u0006) = min(f(\u0006), f(hz)) is also stable. The value hz depends upon the function f ,\nbut it need not be very large. For the function f(\u0006) = \u0003\u0006\/2\u0004, the value hz is bounded\nfrom above by a polynomial in n. See section 3 for details. Our stability results are\ncomplemented by a straightforward lower bound: The system is unstable if f(\u0006) is\ntoo small, for example, if f(\u0006) < \u03bbn \u2212 1. Finally, we provide upper bounds on the\nsystem load in a stationary system (again, depending on f).\n1.1. New results. Before we state our results, we introduce our model and the\nwork-stealing algorithm.\nThe model. We start with a collection of n synchronized processors, connected\nby some network topology. During each time step, every processor may send a request\n1262 P. BERENBRINK, T. FRIEDETZKY, AND L. A. GOLDBERG\n(requesting extra work) to at most one other processor, and any processor receiving\nmore than one such request accepts at most one of them.\nIn our model, we have n generators. A generator-allocation function is a function\nfrom the n generators to the n processors. We consider a fixed, but arbitrary, distri-\nbution D over generator-allocation functions. During each time step of our process,\na generator-allocation function h is chosen from D, and the generators are allocated\nto the processors according to h. Each generator may then generate a unit-time task\nwhich it inserts into the queue of its host processor. It generates such a task indepen-\ndently with probability \u03bb \u2208 [0, 1]. After the new tasks are generated, each processor\nremoves one task from its queue and services it. We assume constant service time for\nall tasks.\nIn the absence of a load-balancing mechanism, many choices of D allow the load\nto become arbitrarily imbalanced, even when \u03bb < 1.\nThe algorithm. The work-stealing algorithm is very simple and natural. During\neach time step, each empty processor (with no work to do) sends a request to one other\nprocessor, which is chosen independently and u.a.r. Each nonempty processor that\nreceived at least one request selects one of these independently and u.a.r. Then each\nempty processor whose request was accepted \u201csteals\u201d tasks from the other processor.\nThe number of tasks which are transferred from the nonempty processor to the\nempty one is determined by the so-called work-stealing function f . In particular, if a\nprocessor that accepts a request has \u0006 tasks stored in its queue, then f(\u0006) tasks are\ntransferred to the currently empty one. A popular work-stealing function is f(\u0006) =\n\u0003\u0006\/2\u0004, which transfers (roughly) half of the tasks.\nThe results. Recall that a system is said to be stable if the system load (the\nsum of the load of all processors) does not grow unboundedly with time. Obviously,\nstability for large arrival rates is one of the most desirable features of load-balancing\nalgorithms.\nIn Theorem 10 we show that, given a suitable work-stealing function, our algo-\nrithm yields a stable system for any constant arrival rate \u03bb < 1 and any distribution\nof the generators. Most intuitively sensible work-stealing functions are suitable (for\nexample, every monotonically nondecreasing function f(\u0006) which is \u03c9(1) as a func-\ntion of \u0006 and satisfies 0 \u2264 f(\u0006) \u2264 \u0006\/2 is suitable). The rough requirement (for f to\nbe suitable) is that for some finite value \u03a6f (which may depend upon n) and some\nz = O(log n) and T = \u0398(logn), we may apply f to \u03a6f z times and the resulting value\nis still at least 2T . (That is, fz(\u03a6f ) \u2265 2T .) Our stability result still holds if we put\nan appropriate upper bound on the amount of work that can be stolen by a single\nrequest. Details are given in section 3.\nIn Theorem 12 we provide upper bounds on the expected system load as well as\ncorresponding tail bounds. The upper bounds are described in terms of \u03a6f and n.\nFor many natural work-stealing functions f , \u03a6f is at most a polynomial in n, so the\nsystem-load bounds are polynomial in n. For example, \u03a6f is at most a polynomial\nin n for the natural work-stealing function f(\u0006) = \u0003\u0006\/2\u0004.\nFinally, in Theorem 22, we classify some work-stealing functions that do not result\nin a stable system. For example, the system is unstable if f(\u0006) < \u03bbn\u2212 1.\nThe proofs. Since the proofs are technical, we briefly introduce the underlying\nidea. We model our system by a discrete-time, countable Markov chain in which states\nare tuples giving the number of tasks currently allocated to each processor. The stan-\ndard method for determining whether such a Markov chain is ergodic (i.e., whether it\nTHE NATURAL WORK-STEALING ALGORITHM IS STABLE 1263\nhas a stationary distribution) is to find an appropriate Lyapunov function [11, 16, 25]\n(a potential function with an appropriate drift). Foster\u2019s theorem (see Theorem 2.2.3\nof [16]) shows that the chain is ergodic if and only if there is a positive Lyapunov\nfunction which is expected to decrease by a positive amount from every state except\nsome finite subset of the state space. For many computer science applications, it\nis apparently prohibitively difficult to find such a one-step Lyapunov function, even\nwhen one is known to exist. Thus, multiple-step analysis is used [21, 18, 3]. We use\nthe multiple-step extension of Foster\u2019s theorem due to Fayolle, Malyshev, and Men-\nshikov (see Lemma 7). The technical difficulty of our proof arises because of the lack\nof independence as the system evolves over multiple steps. To derive our results, we\nstudy the behavior of a different and simpler Markov chain. The new Markov chain\ndoes not dominate the original chain forever, but we show that it does dominate the\noriginal chain for a sufficiently long period of time, and this enables us to prove that\nthe original chain is ergodic. The proof of ergodicity, together with a martingale\nbound of [8], gives us our bound on the stationary behavior of the chain.\n1.2. Known results. Most known theoretical results on load balancing are for\nunconditional algorithms (which perform load balancing every few steps, regardless\nof the system state) or for sender-initiated approaches. First, there has been a lot of\nwork on static problems, in which the number of jobs to be serviced is fixed and may\neven be known in advance. For these results, see [4, 34, 13, 2, 5].\nIn our paper, we work on dynamic load balancing, in which tasks are generated\nover time. We will now describe previous work on this problem. In [1], Adler, Beren-\nbrink, and Schro\u00a8der consider a process in which m jobs arrive in each round to n\nservers and each server is allowed to remove one job per round. They introduce a\nsimple algorithm in which each job chooses between 2 random servers. They show\nthat, provided m \u2264 n\/6e, no job is likely to wait more than O(log log n) rounds. In\n[12] the authors analyze several dynamic balls-into-bins games with deletion.\nIn [26], Mitzenmacher presents a new differential-equations approach for analyz-\ning both static and dynamic load-balancing strategies. He demonstrates the approach\nby providing an analysis of the supermarket model: jobs (customers) arrive as a Pois-\nson stream of rate \u03bbn, \u03bb < 1, at a collection of n servers. Each customer chooses\nd servers independently and u.a.r. and waits for service at the one with the shortest\nqueue. The service time of the customers is exponentially distributed with mean 1.\nMitzenmacher achieves results on the expected time that a customer spends in the\nsystem. Furthermore, he shows that for any time interval of fixed length, the maxi-\nmum system load is likely to be at most log log n\/ log d+O(1). In [35] Vvedenskaya,\nDobrushin, and Karpelevich independently present similar results. For related results,\nsee [27, 30, 28].\nIn [33], Rudolph, Slivkin-Allalouf, and Upfal present a simple distributed load-\nbalancing strategy. They consider a work-generation model in which, at every time\nstep, the load change of any processor due to local generation and service is bounded\nby some constant. The balancing strategy works as follows. Each processor stores its\ntasks in a local queue. Whenever a processor accesses its local queue, the processor\nperforms a balancing operation with a probability inversely proportional to the size\nof its queue. The balancing operation examines the queue size of a randomly chosen\nprocessor and then equalizes their load. They show that the expected load of any\nprocessor at any point of time is within a constant factor of the average load.\nIn [22], Lu\u00a8ling and Monien use a similar work-generation model. A processor\ninitiates a load-balancing action if its load has changed by a constant factor since\n1264 P. BERENBRINK, T. FRIEDETZKY, AND L. A. GOLDBERG\nits last balancing action. They show that the expected load difference between any\ntwo processors is bounded by a constant factor. They also bound the corresponding\nvariance.\nIn [6, 7] the authors introduce and investigate the performance of certain ran-\ndomized load-balancing algorithms under stochastic and adversarial work-generation\nmodels. They consider two different work-generation models. In the first model, in\neach step, each processor generates a task with some probability p < 1, and then each\nnonempty processor services a task with probability p(1 + \u0014) for \u0014 > 0. In the second\nmodel, each processor is allowed to change its load in each step, provided that the load\nis only increased or decreased by at most a fixed constant amount. With high prob-\nability, the algorithms balance the system load up to additive terms of O(log log n)\nand O((log log n)2), respectively. In particular, in the first model, the maximum load\nof any processor can be upper bounded by one of these terms (depending on the al-\ngorithm), whereas in the second model, the maximum load of any processor can be\nupper bounded by the average load plus O(log log n). The algorithms and analysis of\n[6, 7] are fundamentally different from the one considered here. In particular, their al-\ngorithms are sender-initiated, i.e., overloaded processors seek to distribute their load.\nMoreover, their algorithms are considerably more complicated than ours.\nThere is relatively little existing theoretical work on receiver-initiated approaches.\nThe interesting thing is that this approach seems to be highly efficient in practice\n(much more than, say, \u201cgive-away-if-overloaded\u201d), but there are no (or hardly any)\nrigorous theoretical results.\nIn [29], Mitzenmacher applies his differential-equations approach in order to ana-\nlyze several randomized work-stealing algorithms in a dynamic setting. In contrast to\nour work, he assumes that every processor has a Poisson generating process with rate\n\u03bb < 1. Hence, in contrast to our generation model, the load is generated in a more-or-\nless balanced fashion and the system is stable even without any work-stealing. Each\ntask has an exponentially distributed service time with mean 1. He models a number\nof work-stealing algorithms with differential equations and compares the equations\nwith each other in order to predict which strategies will be most successful. For each\nset of equations, he shows that the queue-lengths decrease more quickly than for a set\nof equations which models the process with no work-stealing.\nIn [9], Blumofe and Leiserson analyze a scheduling strategy for strict multithreaded\ncomputations. A multithreaded computation consists of a set of threads, each of which\nis a sequential ordering of unit-time tasks. During a computation, a thread can spawn\nother threads, which are stored in a local queue. They present a work-stealing algo-\nrithm in which every idle processor tries to steal a thread from a randomly chosen\nother processor. The analysis shows that the expected time to execute such a multi-\nthreaded computation on P processors is O(T1\/P + T\u221e), where T1 denotes the min-\nimum sequential execution time of the multithreaded computation, and T\u221e denotes\nthe minimum execution time with an infinite number of processors. Furthermore,\nthey estimate the probability that the execution time is increased by an additional\nfactor. In [15], Fatourou and Spirakis develop an algorithm for k-strict multithreaded\ncomputations. In this case, all data dependencies of a thread are directed to ancestors\nin at most k higher levels.\n2. The work-stealing process. Suppose that we have n processors and n gen-\nerators, which create work for the processors. Each processor keeps a queue of jobs\nwhich need to be done. The evolution of the system can be described by a Markov\nchain X. The state Xt after step t is a tuple (Xt(1), . . . , Xt(n)) in which Xt(i) rep-\nTHE NATURAL WORK-STEALING ALGORITHM IS STABLE 1265\nresents the length of the ith processor\u2019s queue after step t. Initially, all of the queues\nare empty, so the start state is (0, . . . , 0).\nLet N = {1, . . . , n}. Let P = {h | N \u2192 N} be the set of all generator-allocation\nfunctions. When generators are allocated according a particular function h \u2208 P, h(i) is\ndesignated as the host of the ith generator. The Markov chainX has three parameters.\nD is an arbitrary distribution over P. A new generator-allocation function is selected\nfrom D during each step of the chain. The parameter \u03bb governs the rate at which\njobs are generated\u2014each generator creates a job during each step independently with\nprobability \u03bb and adds the job to the queue of its current host. Finally, the function f\nis the work-stealing function. In section 3, we will state the properties that f must\nsatisfy for our analysis. Figure 1 describes the transition from state Xt to state Xt+1.\n1. Choose the generator-allocation function ht from D.\n2. Each generator generates a new job independently with probability \u03bb.\nIt adds the job to the queue of its current host. In particular, the ith\nprocessor updates the size of its queue from Xt(i) to X\n\u2032\nt(i), where X\n\u2032\nt(i) is\ndefined to be Xt(i) plus the sum of |h\u22121t (i)| independent Bernoulli random\nvariables with mean \u03bb.\n3. Each processor with an empty queue chooses a request destination u.a.r.\nfrom N . Formally, rt(i) is defined to be 0 if X\n\u2032\nt(i) > 0. Otherwise, rt(i) is\nchosen u.a.r. from N .\n4. Each processor which receives a request chooses a recipient and allocates\nsome of its load to give away to the recipient. Formally, we start by setting\nj+t (i) = j\n\u2212\nt (i) = 0 for all i \u2208 N . Then every k \u2208 N for which r\u22121t (k) is\nnonempty chooses \u0006 u.a.r. from r\u22121t (k) and sets j\n+\nt (\u0006) = j\n\u2212\nt (k) = f(X\n\u2032\nt(k)).\n5. The work is shared and then each queue processes one job. Formally, for\nall i, Xt+1(i) is set to max(0, X\n\u2032\nt(i) + j\n+\nt (i)\u2212 j\u2212t (i)\u2212 1).\nFig. 1. The Markov chain X. The transition from Xt to Xt+1.\n3. Work-stealing functions. In this section, we state the properties that the\nwork-stealing function, f , must satisfy for our analysis.\nDefinition 1. We assume that for a positive constant \u03b4, the arrival rate \u03bb is at\nmost 1\u2212 \u03b4. Let c be a constant which is sufficiently large with respect to \u03b4\u22121 (see the\nproof of Lemma 14) and let \u03b1 = 4e(c+1). Let z = \r\u03b1 lg n\u000e. Let \u03bd = n\u22122+n\u2212\u03b1 and let\nT = \r 2c1\u2212\u03bb\/(1\u2212\u03bd) lg n\u000e. Note that T is positive as long as \u03bd < \u03b4, and we will consider\nvalues of n for which this is true. Let g be the function given by g(\u0006) = f(\u0006\u2212 T ). We\nwill use the function g in our analysis of the work-stealing process. Suppose that a\nprocessor has \u0006 units of work in its queue. If no units of work are generated or stolen\nduring T steps, it will then have \u0006\u2212T units. Finally, it may give away f(\u0006\u2212T ) = g(\u0006)\nunits to another processor which requests work. Let \u03a6f be the smallest integer such\nthat, for all j \u2208 {0, . . . , z}, gj(\u03a6f\/n) \u2265 2T , where gj(y) denotes the j-fold application\nof function g to argument y. That is, g0(y) = 1, g1(y) = g(y), g2(y) = g(g(y)), and\nso on. If no such integer \u03a6f exists, say \u03a6f =\u221e. Informally, \u03a6f is a quantity that is\nso large that if we start with \u03a6f units of work and focus on the (at least \u03a6f\/n) units\nof work in some particular queue and allow this work to be stolen up to z times, the\nquantity of work remaining at every processor involved is at least 2T . This idea will\nbe made more precise later.\n1266 P. BERENBRINK, T. FRIEDETZKY, AND L. A. GOLDBERG\nWe require the work-stealing function f to satisfy the following properties.\nProperty 1. 0 \u2264 f(\u0006) \u2264 \u0006\/2.\nProperty 2. f(\u0006) is monotonically nondecreasing in \u0006.\nProperty 3. \u03a6f is finite.\nProperties 1 and 2 are natural and easy to understand. We conclude this section\nby showing that many natural work-stealing functions which satisfy Properties 1 and 2\nalso satisfy Property 3. We start with a general lemma and then conclude with\nparticular examples.\nLemma 2. Suppose that the work-stealing function f satisfies Properties 1 and 2.\nLet h0 = 2T . Suppose that there are positive integers h1, . . . , hz satisfying f(hi\u2212T ) \u2265\nhi\u22121. Then \u03a6f \u2264 nhz.\nProof. We wish to show that for all j \u2208 {0, . . . , z}, gj(hz) \u2265 2T . Since f satisfies\nProperty 1, the condition f(hi \u2212 T ) \u2265 hi\u22121 implies that hi\u22121 \u2264 hi. Therefore, for\nany j \u2208 {0, . . . , z}, hz\u2212j \u2265 h0 \u2265 2T . Thus, it suffices to prove gj(hz) \u2265 hz\u2212j , which\nwe will do by induction on j with base case j = 0. For the inductive step, note that\ngj+1(hz) = f(g\nj(hz)\u2212 T ) \u2265 f(hz\u2212j \u2212 T ) \u2265 hz\u2212(j+1),\nwhere the first inequality uses the monotonicity of f (Property 2) and the inductive\nhypothesis.\nCorollary 3. Suppose that the work-stealing function f satisfies Properties 1\nand 2. Suppose that f(\u0006) = \u03c9(1) as a function of \u0006. Then f satisfies Property 3.\nProof. Since f(\u0006) = \u03c9(1), the function gets arbitrarily big and the values h1, . . . , hz\nin Lemma 2 exist.\nCorollary 3 demonstrates that having f(\u0006) = \u03c9(1) is sufficient in the sense that\nthis, together with Properties 1 and 2, implies Property 3. Having f(\u0006) = \u03c9(1) is not\nnecessary though, as the following observation shows.\nObservation 4. Suppose that the work-stealing function f satisfies Properties 1\nand 2. Let h0 = 2T . Suppose (as in Lemma 2) that there are positive integers\nh1, . . . , hz satisfying f(hi \u2212 T ) \u2265 hi\u22121. Let f \u2032 be the work-stealing function given by\nf \u2032(\u0006) = min(f(\u0006), f(hz)). Then f \u2032 satisfies Properties 1\u20133 and has \u03a6f \u2032 \u2264 nhz.\nWe end the section by giving an upper bound for \u03a6f when f is a member of a\npopular class of work-stealing functions.\nLemma 5. Let f(\u0006) = \u0003\u0006\/r\u0004 for some r \u2265 2. This function satisfies Properties\n1\u20133 and satisfies\n\u03a6f \u2264 n(2T + 2r)(2r)z.\nProof. We use Lemma 2. Let hi = (2T + 2r)(2r)\ni\nfor i \u2208 {1, . . . , z}. Then for\ni \u2208 {1, . . . , z},\nf(hi \u2212 T ) = f((2T + 2r)(2r)i \u2212 T )\n\u2265 (2T + 2r)(2r)\ni\nr\n\u2212 T\nr\n\u2212 1\n=\n(2T + 2r)(2r)\ni\n2r\n+\n(2T + 2r)(2r)\ni\n2r\n\u2212 2T\n2r\n\u2212 2r\n2r\n\u2265 (2T + 2r)(2r)i\u22121\n\u2265 hi\u22121.\nTHE NATURAL WORK-STEALING ALGORITHM IS STABLE 1267\nRemark 6. The value \u03a6f corresponding to the function f in Lemma 5 is bounded\nfrom above by a polynomial in n. To see this, note that the multiplier in the definition\nof T is\n1\n1\u2212 \u03bb1\u2212\u03bd\n\u2264 1\n1\u2212 1\u2212\u03b41\u2212\u03bd\n=\n1\u2212 \u03bd\n\u03b4 \u2212 \u03bd \u2264\n1\n\u03b4 \u2212 \u03bd \u2264\n2\n\u03b4\n,\nwhere the last inequality assumes \u03bd \u2264 \u03b4\/2, which is true if n is sufficiently large with\nrespect to the constant \u03b4\u22121.\n4. Upper bounds. In this section we prove that the system is stable for every\nwork-stealing function satisfying Properties 1\u20133 in section 3. Our analysis does not de-\npend upon the particular distribution D which governs the allocation of generators\u2014\nthe analysis works for an arbitrary distribution.\nAs already outlined in section 1, the basic idea is the following. The Markov chain\nX models our system. Since this chain is difficult to analyze directly, we introduce a\nsecond chain Z and investigate properties of Z instead. Then, using a coupling, we\nrelate the results to chain X itself.\nTo put it very informally and nonrigorously, the core idea is to show that during\nan interval of length T = O(log n) not too many requests are sent. Since in our model\nnot sending a request means servicing a task, we can show that in this case the system\nload decreases. Obviously, the crux is bounding the number of requests during the\ninterval. Informally, this is done by assuming (for contradiction) that there are many\nrequests during the interval, say at least R. Since the system load is initially high,\nthere is at least one processor, processor P , which initially has a high load. This\nimplies that after around R\u2032 < R requests, we can view most of the requests that\nhave been accepted in a tree with P at the root, and the leaves being processors that\neither directly or indirectly received a portion of P \u2019s initial load. By showing that\n(i) there are many leaves, and (ii) the tree does not grow very deep, we can conclude\nthat after R\u2032 requests, there are many processors having a large load (at least T ), and\nnone of them will send a request during the next T steps. Hence, we can contradict\nthe assumption that R requests get sent during the interval. Of course, this kind of\nproof-by-contradiction is invalid if we want to avoid conditioning the random variables\nduring the T steps, so we have to do things more carefully.\n4.1. Background. We start with some brief definitions regarding Markov chains.\nFor more details, see [19]. The Markov chains that we consider are time-homogeneous\n(transition probabilities do not change over time) and irreducible (every state is reach-\nable from every other) and aperiodic (the gcd of the lengths of valid paths from state i\nto itself is 1). An irreducible aperiodic Markov chain (\u03a5t) is said to be recurrent if,\nwith probability 1, it returns to its start state. That is, it is recurrent if\nPr(\u03a5t = \u03a50 for some t \u2265 1) = 1.\nOtherwise, it is said to be transient. It is said to be positive recurrent or ergodic if\nthe expected time that it takes to return to the start state is finite. In particular, let\nTret = min{t \u2265 1 | \u03a5t = \u03a50}.\nThe chain is said to be positive recurrent if E[Tret] < \u221e. A positive recurrent chain\nhas a unique stationary distribution \u03c0. When we analyze the Markov chain X we will\nuse the following generalization of Foster\u2019s theorem, due to Fayolle, Malyshev, and\nMenshikov (Theorem 2.2.4 of [16]).\n1268 P. BERENBRINK, T. FRIEDETZKY, AND L. A. GOLDBERG\nLemma 7 (Foster; Fayolle, Malyshev, Menshikov [16]). A time-homogeneous ir-\nreducible aperiodic Markov chain \u03b6 with a countable state space \u2126 is positive recurrent\nif and only if there exists a positive function \u03a6(x), x \u2208 \u2126, a number \u03be > 0, a positive\ninteger-valued function \u03b2(x), x \u2208 \u2126, and a finite set C \u2032 \u2286 \u2126 such that the following\ninequalities hold:\nE[\u03a6(\u03b6t+\u03b2(\u03b6t))\u2212 \u03a6(\u03b6t) | \u03b6t = x] \u2264 \u2212\u03be\u03b2(x), x \u0011\u2208 C \u2032,(1)\nE[\u03a6(\u03b6t+\u03b2(\u03b6t)) | \u03b6t = x)] <\u221e, x \u2208 C \u2032.(2)\nWe also use the following Chernov\u2013Hoeffding inequalities. The first of these is a\nspecial case of Theorem 4.2 of [31] and the second is taken from Theorem 5.7 of [24].\nLemma 8 (Chernov). Let Z1, . . . , Zs be independent Bernoulli trials with Pr(Zi =\n1) = p. Let Z\u0302 =\n\u2211s\ni=1 Zi. Then for any \u03c1 in (0, 1], Pr(Z\u0302 < (1\u2212\u03c1)sp) \u2264 exp(\u2212sp\u03c12\/2)).\nLemma 9 (Hoeffding). Let Z1, . . . , Zs be independent random variables with\nai \u2264 Zi \u2264 bi for suitable constants ai, bi and all 1 \u2264 i \u2264 s. Also let Z\u0302 =\n\u2211s\ni=1 Zi.\nThen for any t > 0,\nPr\n(|Z\u0302 \u2212 E(Z\u0302)| \u2265 t) \u2264 exp(\u22122t2 \/ s\u2211\ni=1\n(bi \u2212 ai)2\n)\n.\n4.2. Results. Our Markov chain X is time-homogeneous, irreducible, and ape-\nriodic. Its state space is countable. Therefore, it satisfies the initial conditions of\nLemma 7. We will prove the following theorem.\nTheorem 10. Let \u03b4 be a positive constant and \u03bb an arrival rate which is at\nmost 1 \u2212 \u03b4. Let f be a work-stealing function satisfying Properties 1\u20133 in section 3.\nThen for every n which is sufficiently large with respect to \u03b4\u22121, the Markov chain X\nis positive recurrent.\nTheorem 10 guarantees that the Markov chain X has a stationary distribution \u03c0.\nThe next theorem is concerned with the value of the total system load in the stationary\ndistribution. Recall from Definition 1 that \u03bd = n\u22122 + n\u2212\u03b1. Our next theorem uses\nthe following additional definitions.\nDefinition 11. Let \u0014 be (1 \u2212 \u03bb\/(1 \u2212 \u03bd))\/4. Let \u03a6(Xt) be the system load after\nstep t. That is, \u03a6(Xt) =\n\u2211n\ni=1Xt(i).\nTheorem 12. Let \u03b4 be a positive constant and \u03bb an arrival rate which is at most\n1\u2212 \u03b4. Let f be a work-stealing function satisfying Properties 1\u20133 in section 3. Then\nfor every n which is sufficiently large with respect to \u03b4\u22121,\nE\u03c0[\u03a6(Xt)] \u2264 \u03a6f + 2nT\/\u0014+ nT,\nand for any nonnegative integer m,\nPr\u03c0[\u03a6(Xt) > \u03a6f + 2nTm+ nT ] \u2264 exp(\u2212 ln(1 + \u0014)(m+ 1)).\n4.3. A simpler Markov chain. Let C be the set of states x with \u03a6(x) < \u03a6f .\nIn this section we define a simpler Markov chain Z that will be used in order to\nanalyze the Markov chain X with a start state x \u0011\u2208 C.\nThe state space of Z is more complicated than the state space of X, but in some\nsense the information contained in a state of Z is less precise than the information\ncontained in a state of X. In particular, a state Zt consists of a tuple\n(Lt(1), . . . , Lt(n), Yt(1), . . . , Yt(n)).\nTHE NATURAL WORK-STEALING ALGORITHM IS STABLE 1269\nThe variable Lt(i) gives a crude indication of the load at processor i after step t. In\nany initial state that we will consider, exactly one processor (which we will call Jx)\nwill have L0(Jx) large. All other processors will have L0(i) = 0. Informally, these\nvariables will have the following role for a processor i. Let t be the first time step\nduring which processor i steals some of the work that originally sat at processor Jx.\nFor t\u2032 \u2264 t, the variable Yt\u2032(i) denotes the load of processor i and Lt\u2032(i) = 0. For\nt\u2032 > t, the variable Lt\u2032(i) is positive and Yt\u2032(i) = 0. The exact value of Lt\u2032(i) gives an\nindication of how many times the work that processor i acquired at step t has been\nsplit (and, therefore, of how long it will last).\nWe will be observing the evolution of the Markov chainX starting at a state X0 =\nx with \u03a6(x) \u2265 \u03a6f . This condition guarantees that for some i \u2208 N , X0(i) \u2265 \u03a6f\/n.\nLet Jx be the smallest such i. In the following, we will be paying special attention to\na load which originates at processor Jx. Thus, in the Markov chain Z, the state Z0\nwhich corresponds to X0 is defined as follows. L0(Jx) = 2\nz and Y0(Jx) = 0. For all\ni \u0011= Jx, L0(i) = 0 and Y0(i) = X0(i). For convenience, we will say that a processor i\nis \u201cheavily loaded\u201d in state Zt if and only if Lt(i) > 0. Thus, Jx is the only processor\nwhich is deemed to be \u201cheavily loaded\u201d in Z0. Note that the state Z0 is strictly a\nfunction of x. We will refer to this state as Z(x). The transition from Zt to Zt+1 is\ndescribed in Figure 2. It may look surprising at first that the \u201cheavy load\u201d parameter\nLt(k) is halved every time a heavily loaded processor transfers load. This halving\nallows us to study the dissemination of load from Jx without considering the many\ndependent events.\nLet R\u2032t be the set of requests made during the transition from Zt to Zt+1. (This\ntransition is referred to as \u201cstep t+ 1.\u201d) That is, R\u2032t = |{i | r\u2032t(i) > 0}|. Let \u03c4 \u2032 be the\nsmallest integer such that R\u20320 + \u00b7 \u00b7 \u00b7 + R\u2032\u03c4 \u2032\u22121 \u2265 cn lg n. Let \u03a8 be the smallest integer\nsuch that, for some i, L\u03a8(i) = 1. Intuitively, L\u03a8(i) = 1 means that i has received\nload (directly or indirectly) from Jx (so it is \u201cheavily loaded\u201d), but this load has been\n1. Choose the generator-allocation function ht from D.\n2. If Lt(i) > 0, then Y\n\u2032\nt (i) is defined to be 0 (just like Yt(i)). Otherwise, Y\n\u2032\nt (i)\nis defined to be Yt(i) plus the sum of |h\u22121t (i)| Bernoulli random variables\nwith mean \u03bb.\n3. r\u2032t(i) is defined to be 0 except when Lt(i) = 0 and Y\n\u2032\nt (i) = 0. In this case,\nr\u2032t(i) is chosen u.a.r. from N .\n4. Start by setting\nj+t (i) = j\n\u2212\nt (i) = l\n\u2212\nt (i) = l\n+\nt (i) = 0\nfor each i \u2208 N . Then every k \u2208 N for which r\u2032t\u22121(k) is nonempty chooses \u0006\nu.a.r. from r\u2032t\n\u22121\n(k) and sets l+t (\u0006) = l\n\u2212\nt (k) = Lt(k)\/2 and j\n+\nt (\u0006) = j\n\u2212\nt (k) =\nf(Y \u2032t (k)).\n5. For all i \u2208 N , Lt+1(i) is set to Lt(i) + l+t (i)\u2212 l\u2212t (i). If Lt+1(i) > 0, then\nYt+1(i) = 0. Otherwise, Yt+1(i) is set to\nmax(0, Y \u2032t (i) + j\n+\nt (i)\u2212 j\u2212t (i)\u2212 1).\nFig. 2. The transition from Zt to Zt+1.\n1270 P. BERENBRINK, T. FRIEDETZKY, AND L. A. GOLDBERG\nsplit many times (it has been split z times, in fact). The following lemma shows that,\nwith high probability, there are no such i and \u03a8 if at most cn log n requests are sent.\nLemma 13. Suppose x \u0011\u2208 C. Run Markov chain Z starting at Z0 = Z(x). Then\nPr(\u03a8 \u2264 \u03c4 \u2032) \u2264 n\u2212\u03b1.\nProof. Since at most n requests are sent in a single step, the total number of\nrequests sent during steps 1, . . . , \u03c4 \u2032 is at most (c+ 1)n lg n.\nRecall the construction of Z(x) from the beginning of section 4.3. In particular,\nthere is one \u201cheavily loaded\u201d processor, Jx, with L0(Jx) = 2\nz. Every other processor i\nhas L0(i) = 0.\nImagine that the value L0(Jx) = 2\nz corresponds to a collection of 2z tokens which\ninitially sit at processor Jx. The value Lt(k) gives the number of tokens which sit\nat processor k following step t. This is always a power of 2. If Lt(k) > 1, then the\ninstruction l+t (\u0006) = l\n\u2212\nt (k) = Lt(k)\/2 in step 3 of the transition from Zt to Zt+1 splits\nthe collection of tokens sitting at processor k and transfers half of these tokens to\nprocessor \u0006. The event \u03a8 \u2264 \u03c4 \u2032 occurs if and only if some token has its group split z\ntimes during steps 1, . . . , \u03c4 \u2032.\nWhat is the probability that a given token has its group split z times? This is at\nmost (\n(c+ 1)n lg n\nz\n)\nn\u2212z \u2264\n(\ne(c+ 1) lgn\nz\n)z\n.\nThe probability that there exists a token which has its group split z times is thus\nat most (\n2e(c+ 1) lgn\nz\n)z\n\u2264\n(\n2e(c+ 1)\n\u03b1\n)\u03b1 lgn\n= n\u2212\u03b1.\nThe next lemma shows that, with high probability, the number of requests sent\nduring the observed T time steps is less than cn log n. This means that we have very\nlittle idle time during this period, which in turn implies the decrease of the system\nload (as we will see later).\nLemma 14. Suppose x \u0011\u2208 C. Run Markov chain Z starting at Z(x).\nPr(\u03c4 \u2032 \u2264 T ) \u2264 n\u22122.\nProof. Recall that R\u2032t is the number of requests during the transition from Zt\nto Zt+1. In particular, R\n\u2032\nt = |{i | Lt(i) = 0 \u2227 Y \u2032t (i) = 0}|. R\u2032t is a random variable\nwhich depends only upon the state Zt and upon the host-distribution function ht.\nIn particular, every processor i with Lt(i) = 0 and Yt(i) = 0 contributes 1 to R\n\u2032\nt\nindependently with probability (1\u2212 \u03bb)|h\u22121t (i)| and contributes 0 to R\u2032t otherwise.\nTo make the conditioning clear, we will let R\u2032(s, h) be the random variable whose\ndistribution is the same as that of R\u2032t, conditioned on Zt = s and ht = h.\nBy Lemma 9,\nPr\n(\n|R\u2032(s, h)\u2212 E(R\u2032(s, h))| \u2265 cn lg n\n8T\n)\n\u2264 exp\n(\n\u22122\n(\ncn lg n\n8T\n)2\n\/n\n)\n.\nLet \u03c3 denote exp(\u22122( cn lgn8T )\n2\n\/n). Note that \u03c3 is exponentially small in n. (This\nfollows from the definition of T .)\nTHE NATURAL WORK-STEALING ALGORITHM IS STABLE 1271\nSay that (s, h) is \u201cdangerous\u201d if\nE(R\u2032(s, h)) \u2265 (cn lg n)\/(4T ).\nNote that if (Zt, ht) is not dangerous, then, with probability at least 1\u2212\u03c3, the number\nof requests during the transition from Zt to Zt+1 is at most\n(cn lg n)\/(4T ) + (cn lg n)\/(8T ) \u2264 (cn lg n)\/(2T ).\nNow suppose that (s, h) is dangerous and let k be any processor. Then\nPr(r\u2032t\n\u22121\n(k) = \u2205 | Zt = s \u2227 ht = h) \u2264 \u03c3 +\n(\n1\u2212 1\nn\n) cn lgn\n8T\n\u2264 \u03c3 +\n(\n1\u2212 1\nn\n) \u03b4n\n18\n\u2264 1\u2212 \u03b3\nfor a small positive constant \u03b3 which depends upon \u03b4 (but not upon c or n). Let Mt\ndenote the number of heavily loaded processors during step t, i.e.,\nMt = |{i | Lt(i) > 0}|.\nLet \u03bet denote the number of heavily loaded processors during step t that don\u2019t\nget requested, i.e.,\n\u03bet = |{k | Lt(k) > 0 \u2227 r\u2032t\u22121(k) = \u2205}|.\nIf (s, h) is dangerous, then\nE[\u03bet | Zt = s \u2227 ht = h] \u2264 (1\u2212 \u03b3)Mt.\nThus by Markov\u2019s inequality,\nPr\n(\n\u03bet \u2265 (1\u2212 \u03b3\/2)Mt | Zt = s \u2227 ht = h\n) \u2264 1\u2212 \u03b3\n2\u2212 \u03b3 .\nIf \u03bet < (1 \u2212 \u03b3\/2)Mt, then at least (\u03b3\/2)Mt of the Mt heavily loaded processors\ngive away work, so Mt+1 \u2265 (1 + \u03b3\/2)Mt. We say that the step following on from a\ndangerous state is \u201cuseful\u201d if this occurs. We have just seen that for every dangerous\nstate (s, h), the probability that the next step is useful is at least \u03b32\u2212\u03b3 .\nThus, if we have D dangerous states during some time interval, the number\nof useful steps following them dominates (from above) the sum of D independent\nBernoulli random variables with probability p = \u03b32\u2212\u03b3 . Applying Lemma 8 with D =\n(2\/p) log1+\u03b3\/2(n) and \u03c1 = 1\/2, we find that the probability that this sum is less than\nlog1+\u03b3\/2(n) is at most exp(\u2212 log1+\u03b3\/2(n)\/4). This means that if we have D dangerous\nstates, then the probability that there are at least log1+\u03b3\/2(n) useful steps following\nthem is at least 1\u2212 exp(\u2212 log1+\u03b3\/2(n)\/4).\nNow, if there are actually at least log1+\u03b3\/2(n) useful steps during steps 1\u2013t,\nMt+1 = n, so there can be no further dangerous states. We conclude that with\nprobability at least 1 \u2212 exp(\u2212 log1+\u03b3\/2(n)\/4) there are at most D dangerous steps\never.\nIf we make c sufficiently large with respect to \u03b3, then D < c lg n\/2.\n1272 P. BERENBRINK, T. FRIEDETZKY, AND L. A. GOLDBERG\nNow we have that, except for probability exp(\u2212 log1+\u03b3\/2(n)\/4), the dangerous\nsteps contribute fewer than cn lg n\/2 requests (ever). Furthermore, except for proba-\nbility at most \u03c3T , the nondangerous steps contribute at most cn lg n\/2 requests during\nthe first T steps. Thus, the probability that \u03c4 \u2032 \u2264 T is at most\nexp(\u2212 log1+\u03b3\/2(n)\/4) + \u03c3T \u2264 n\u22122.\nLemmas 13 and 14 imply the following.\nCorollary 15. Suppose x \u0011\u2208 C. Run Markov chain Z starting at Z(x).\nPr(T < \u03c4 \u2032 < \u03a8) \u2265 1\u2212 n\u2212\u03b1 \u2212 n\u22122.\nLemma 16. Suppose x \u0011\u2208 C. Run Markov chain Z starting at Z(x). For any\nt \u2264 \u03a8 and any i \u2208 N , either Lt(i) = 0 or, for some j \u2208 {0, . . . , z}, Lt(i) = 2j.\nProof. The lemma is proved by induction on t with the base case t = 0. Consider\nthe assignment\nLt+1(i) = (Lt(i)\u2212 l\u2212t (i)) + l+t (i)\nin the transition from Zt to Zt+1 in Figure 1. If the second term in the expression,\nl+t (i), is greater than zero, then it is equal to Lt(k)\/2 for some k with r\n\u2032\nt(i) = k, so\nLt(i) = 0. The first term in the expression, Lt(i) \u2212 l\u2212t (i), is either Lt(i) or Lt(i)\/2.\nThus, either Lt+1(i) is Lt(i) or it is Lt(k)\/2 for some k. Using the terminology from\nthe proof of Lemma 13, Lt+1(i) = 2\nz\u2212m means that the tokens that sit at processor i\nafter step t+ 1 have had their group split m times. Since t \u2264 \u03a8, m \u2264 z.\n4.4. Proof of Theorem 10. Our first task is to relate the Markov chain X to\nthe simpler Markov chain Z. Recall the definitions of \u03c4 \u2032, R\u2032t, and \u03a8 from section 4.3.\nLet Rt be the set of requests made during the transition from Xt to Xt+1. That is,\nRt = |{i | rt(i) > 0}|. Let \u03c4 be the smallest integer such that R0+\u00b7 \u00b7 \u00b7+R\u03c4\u22121 \u2265 cn lg n.\nLemma 17. If x \u0011\u2208 C, then\nPr(\u03c4 \u2264 T | X0 = x) \u2264 \u03bd.\nProof. A (Markovian) coupling1 of the Markov chains X and Z is a stochastic\nprocess (Xt, Zt) such that (Xt), considered marginally, is a faithful copy of X, and\n(Zt), considered marginally, is a faithful copy of Z. We will describe a coupling\nstarting from state (x, Z(x)). That is, in our coupling, X0 = x and Z0 = Z(x). The\ncoupling will have the property that for all t \u2264 min(T, \u03c4 \u2032,\u03a8) and all i,\nr\u2032t(i) = rt(i).(3)\nFrom (3), we can conclude that whenever the Z chain satisfies T < \u03c4 \u2032 < \u03a8, the\ncoupled X chain satisfies T < \u03c4 . Thus,\nPr(T < \u03c4 | X0 = x) \u2265 Pr(T < \u03c4 \u2032 < \u03a8 | Z0 = Z(x)),\nso the lemma follows from Corollary 15.\nTo give the details of the coupling, we will use the notation in Figures 1 and 2.\nRecall from Definition 1 that g is the function given by g(y) = f(y \u2212 T ), where f is\n1The word \u201ccoupling\u201d is normally used in reference to combining two copies of the same Markov\nchain, so we are using the word in a slightly nonstandard way.\nTHE NATURAL WORK-STEALING ALGORITHM IS STABLE 1273\nthe work-stealing function which is guaranteed to satisfy gj(\u03a6f\/n) \u2265 2T for a finite\n\u03a6f and for all j \u2208 {0, . . . , z}.\nOur coupling will satisfy the following invariants for any i \u2208 N and any t \u2264\nmin(T, \u03c4 \u2032,\u03a8):\n(1) r\u2032t(i) = rt(i),\n(2) Lt(i) = 0 implies Xt(i) = Yt(i), and\n(3) Lt(i) = 2\nj implies Xt(i) \u2265 gz\u2212j(\u03a6f\/n)\u2212 t.\nAs we observed above, our objective is to describe a coupling that satisfies invari-\nant (1). The other invariants will help us to show that our constructed coupling is\nindeed a coupling in the sense that the marginal distributions are correct. The pur-\npose of the third invariant is to ensure that, in the chain X, a node will not become\nempty soon if the corresponding node in chain Z is heavily loaded.\nThe coupling is as follows. We start with X0 = x and Z0 = Z(x). Recall the\nconstruction of Z(x) from section 4.3. In particular, L0(Jx) = 2\nz and Y0(Jx) = 0. For\nevery other i, L0(i) = 0 and Y0(i) = x(i). Invariants (2) and (3) are satisfied for t = 0\nsince X0(Jx) \u2265 \u03a6f\/n.\nNow the transition from (Xt, Zt) to (Xt+1, Zt+1) is given as follows. In part 1\nof the transition, the same generator-allocation function ht is chosen for both chains.\nThe X \u2032t(i) variables are defined in part 2 of the transition from Xt to Xt+1. In part 2\nof the coupled transition from Zt to Zt+1, we set Y\n\u2032\nt (i) = 0 if Lt(i) > 0. Otherwise, we\nset Y \u2032t (i) = X\n\u2032\nt(i). Note that, since invariant (2) held after step t, the Y\n\u2032\nt (i) variables\nare set according to the correct marginal distribution. The rt(i) variables are defined\nin part 3 of the transition from Xt to Xt+1. In part 3 of the coupled transition from\nZt to Zt+1, we set r\n\u2032\nt(i) = rt(i). To show that the marginal distribution is correct, we\nobserve that if Lt(i) = 0, then we defined Y\n\u2032\nt (i) to be X\n\u2032\nt(i). Thus, the r\n\u2032\nt(i) variables\nare assigned correctly. On the other hand, if Lt(i) > 0, then, by Lemma 16, Lt(i) = 2\nj\nfor some j \u2208 {0, . . . , z} so by invariant (3), Xt(i) \u2265 gz\u2212j(\u03a6f\/n) \u2212 t \u2265 2T \u2212 t > 0.\nThus, X \u2032t(i) > 0, and r\n\u2032\nt(i) is defined correctly. In part 4 of the transition from Xt\nto Xt+1, we do the following. For every k \u2208 N for which r\u22121t (k) is nonempty, we\nchoose \u0006 u.a.r. from r\u22121t (k). Since r\n\u2032\nt\n\u22121\n(k) = rt\n\u22121(k), we can make the same choice\nfor k in part 4 of the transition from Zt to Zt+1.\nWe need to prove that the coupling maintains invariants (1), (2), and (3). Invari-\nant (1) (the one that we actually want) is by construction. Invariant (2) is not too dif-\nficult. Lemma 16 shows that all of the variables Lt(i) are nonnegative. Furthermore,\nthe analysis in the proof of Lemma 16 reveals that Lt+1(i) = 0 implies Lt(i) = 0. (To\nsee this, recall that Lt+1(i) = (Lt(i)\u2212 l\u2212t (i))+ l+t (i). The second of these terms is non-\nnegative, and the first is either Lt(i) or Lt(i)\/2.) Thus, whenever we have Lt+1(i) = 0\nwe have Yt(i) = Xt(i), and in the coupling we get Y\n\u2032\nt (i) = X\n\u2032\nt(i). In part 5 of the tran-\nsition fromXt toXt+1 we setXt+1(i) = max(0, X\n\u2032\nt(i)+j\n+\nt (i)\u2212j\u2212t (i)\u22121), and in part 5\nof the transition from Zt to Zt+1, we set Yt+1(i) = max(0, Y\n\u2032\nt (i) + j\n+\nt (i)\u2212 j\u2212t (i)\u2212 1).\nThus we need only argue that the j+t (i) and j\n\u2212\nt (i) variables get the same values in\nboth copies. The j\u2212t (i) variable is the same since Y\n\u2032\nt (i) = X\n\u2032\nt(i). The j\n+\nt (i) variables\nare positive only if processor i made a request, namely, rt(i) = r\n\u2032\nt(i) = k, for some k.\nSince Lt+1(i) = 0, we know Lt(k) = 0. Hence, Y\n\u2032\nt (k) = X\n\u2032\nt(k), and the j\n+\nt (i) values\nare indeed the same.\nFinally, we need to prove that the coupling maintains invariant (3). Suppose that\nLt+1(i) = 2\nj . We wish to show that Xt+1(i) \u2265 gz\u2212j(\u03a6f\/n)\u2212 (t+ 1). First, suppose\n1274 P. BERENBRINK, T. FRIEDETZKY, AND L. A. GOLDBERG\nLt(i) = 0. In this case, Lt+1(i) = Lt(k)\/2, where rt(i) = k. Then\nXt+1(i) \u2265 f(Xt(k))\u2212 1\n\u2265 f(gz\u2212j\u22121(\u03a6f\/n)\u2212 t)\u2212 1\n\u2265 f(gz\u2212j\u22121(\u03a6f\/n)\u2212 T )\u2212 1\n= gz\u2212j(\u03a6f\/n)\u2212 1\n\u2265 gz\u2212j(\u03a6f\/n)\u2212 (t+ 1),\nwhere the first inequality follows from the transition in Figure 1 and the second\ninequality follows from the facts that invariant (3) held after step t and that f is\nmonotonically nondecreasing (Property 2 in section 3). The third inequality also\nfollows from the fact that f is monotonically nondecreasing.\nSecond, suppose Lt(i) = 2\nj . In this case r\u22121t (i) is empty, so\nXt+1(i) \u2265 Xt(i)\u2212 1 \u2265 gz\u2212j(\u03a6f\/n)\u2212 t\u2212 1.\nFinally, suppose Lt(i) = 2\nj+1. (To see that these are the only cases, namely, that\nLt(i) \u2208 {0, 2j , 2j+1}, see the proof of Lemma 16.) In this case r\u22121t is nonempty, so\nXt+1(i) \u2265 Xt(i)\u2212 f(Xt(i))\u2212 1.\nSince f satisfies f(\u0006) \u2264 \u0006\/2 (Property 1 in section 3), we have\nXt+1(i) \u2265 f(Xt(i))\u2212 1,\nwhich is the same as the first case.\nThe next lemma shows that the load has an appropriate drift when \u03c4 > T .\nLemma 18. If x \u0011\u2208 C, then\nE[\u03a6(XT ) | (X0 = x) \u2227 (\u03c4 > T )] \u2264 \u03a6(x)\u2212 2\u0014nT.\nProof. Let At be the number of new jobs that arrive in the system during the\ntransition from Xt to Xt+1. Namely,\nAt =\n\u2211\ni\u2208N\n(X \u2032t(i)\u2212Xt(i)).\nLet Y = A0 + \u00b7 \u00b7 \u00b7+AT\u22121. Splitting E[Y | X0 = x] into two conditional expectations,\nconditioned on whether or not \u03c4 > T , we find\nE[Y | (X0 = x) \u2227 (\u03c4 > T )]\n=\nE[Y | X0 = x]\u2212 Pr(\u03c4 \u2264 T | X0 = x)E[Y | (X0 = x) \u2227 (\u03c4 \u2264 T )]\nPr(\u03c4 > T | X0 = x) .\nBy Lemma 17, the denominator is at least 1 \u2212 \u03bd. The numerator is at most E[Y |\nX0 = x], which is \u03bbnT , since during each of the T steps each of the n generators\ngenerates a new job independently with probability \u03bb. Thus,\nE[Y | (X0 = x) \u2227 (\u03c4 > T )] \u2264 \u03bbnT\n1\u2212 \u03bd .\nTHE NATURAL WORK-STEALING ALGORITHM IS STABLE 1275\nIf \u03c4 > T , then the number of jobs serviced during steps 1\u2013T is at least nT\u2212cn lg n.\n(If a processor does not make a request, then it certainly services a job.) Thus, the\nquantity\nE[\u03a6(XT ) | (X0 = x) \u2227 (\u03c4 > T )]\nis at most the initial load, \u03a6(x), plus the expected number of arrivals, which we have\nseen above is at most \u03bbnT1\u2212\u03bd , minus the expected number of services, which is at least\nnT \u2212 cn lg n. Putting all of this together, we get\nE[\u03a6(XT ) | (X0 = x) \u2227 (\u03c4 > T )] \u2264 \u03a6(x)\u2212\n(\n1\u2212 \u03bb\n1\u2212 \u03bd\n)\nnT + cn lg n\n\u2264 \u03a6(x)\u2212 1\u2212\n\u03bb\n1\u2212\u03bd\n2\nnT\n= \u03a6(x)\u2212 2\u0014nT,\nwhere the second inequality uses the definition of T in Definition 1 and the equality\nuses the definition of \u0014 in Definition 11.\nLemma 19. Suppose that n is sufficiently large with respect to \u03b4\u22121. If x \u0011\u2208 C,\nthen\nE[\u03a6(XT ) | X0 = x] \u2264 \u03a6(x)\u2212 \u0014nT.\nProof.\nE[\u03a6(XT ) | X0 = x] = Pr(\u03c4 > T | X0 = x)E[\u03a6(XT ) | (X0 = x) \u2227 \u03c4 > T ]\n+ Pr(\u03c4 \u2264 T | X0 = x)E[\u03a6(XT ) | (X0 = x) \u2227 \u03c4 \u2264 T ].\nBy Lemma 18, this is at most\nPr(\u03c4 > T | X0 = x)(\u03a6(x)\u22122\u0014nT )+Pr(\u03c4 \u2264 T | X0 = x)E[\u03a6(XT ) | (X0 = x)\u2227 \u03c4 \u2264 T ].\nSince at most n messages arrive per step, this is at most\nPr(\u03c4 > T | X0 = x)(\u03a6(x)\u2212 2\u0014nT ) + Pr(\u03c4 \u2264 T | X0 = x)(\u03a6(x) + nT ).\nThis can be rearranged as\n\u03a6(x)\u2212 (1\u2212 Pr(\u03c4 \u2264 T | X0 = x))(2\u0014nT ) + Pr(\u03c4 \u2264 T | X0 = x)(nT )\n= \u03a6(x)\u2212 2\u0014nT + Pr(\u03c4 \u2264 T | X0 = x)(2\u0014nT + nT ).\nBy Lemma 17, this is at most\n\u03a6(x)\u2212 2\u0014nT + \u03bd(2\u0014nT + nT ) = \u03a6(x)\u2212 \u0014nT \u2212 (\u0014nT \u2212 \u03bd2\u0014nT \u2212 \u03bdnT ).\nThe lemma follows from the fact that\n\u03bd \u2264 \u0014\n2\u0014+ 1\n,(4)\nwhich is true, provided that n is sufficiently large with respect to \u03b4\u22121. To establish\n(4), refer to Definitions 1 and 11. If n is sufficiently large, then \u03bd \u2264 \u03b4\/2, so\n4\u0014 = 1\u2212 \u03bb\n1\u2212 \u03bd \u2265 1\u2212\n1\u2212 \u03b4\n1\u2212 \u03bd =\n\u03b4 \u2212 \u03bd\n1\u2212 \u03bd \u2265\n\u03b4\n2\n.\nAlso,\n\u03bd \u2264 \u03b4\/8\n2(\u03b4\/8) + 1\n\u2264 \u0014\n2\u0014+ 1\n.\nCombining Lemmas 19 and 7, we get a proof of Theorem 10.\n1276 P. BERENBRINK, T. FRIEDETZKY, AND L. A. GOLDBERG\n4.5. Proof of Theorem 12. The proof of Theorem 12 uses the following theo-\nrem, which is Theorem 1 of [8].\nLemma 20 (Bertsimas, Gamarnik, and Tsitsiklis [8]). Consider a time-homogen-\neous Markov chain \u03b6 with a countable state space \u2126 and stationary distribution \u03c0\u2032. If\nthere is a positive function \u03a6(x), x \u2208 \u2126, a number \u03be > 0, and a number \u03b2 \u2265 0 such\nthat\nE[\u03a6(\u03b6t+1)\u2212 \u03a6(\u03b6t) | \u03b6t = x] \u2264 \u2212\u03be, \u03a6(x) > \u03b2,(5)\nand\n|\u03a6(\u03b6t+1)\u2212 \u03a6(\u03b6t)| \u2264 \u03bdmax,(6)\nand, for any x,\nPr[\u03a6(\u03b6t+1) > \u03a6(\u03b6t) | \u03b6t = x] \u2264 pmax(7)\nand\nE\u03c0\u2032 [\u03a6(\u03b6t)] <\u221e,(8)\nthen for any nonnegative integer m,\nPr\u03c0\u2032 [\u03a6(\u03b6t) > \u03b2 + 2\u03bdmaxm] \u2264\n(\npmax\u03bdmax\npmax\u03bdmax + \u03be\n)m+1\nand\nE\u03c0\u2032 [\u03a6(\u03b6t)] \u2264 \u03b2 + 2pmax(\u03bdmax)\n2\n\u03be\n.\nLet Wi = \u03a6(XiT ) for i \u2208 {0, 1, 2, . . . }. Lemma 19 shows that the process\nW0,W1, . . . behaves like a supermartingale above \u03a6f . That is, it satisfies (5) with\n\u03be = \u0014nT and \u03b2 = \u03a6f . In itself, this does not imply that E[Wt] is bounded (see\nPemantle and Rosenthal\u2019s paper [32] for counterexamples). However, we also have\n|Wt+1 \u2212Wt| \u2264 nT(9)\nfor any t. That is, (6) is satisfied with \u03bdmax = nT . This implies (for example, by\nTheorem 1 of [32] or by Theorem 2.3 of [20]) that E\u03c0[Wt] is finite (so (8) is satisfied).\nLemma 20 can now be applied with pmax = 1 to get\nE\u03c0[Wt] \u2264 \u03a6f + 2nT\/\u0014,(10)\nand for any nonnegative integer m,\nPr\u03c0[Wt > \u03a6f + 2nTm] \u2264\n(\nnT\nnT + \u0014nT\n)(m+1)\n.(11)\nThe theorem now follows from the observation that for 0 \u2264 j < T ,\n\u03a6(XiT+j) \u2264 \u03a6(XiT ) + nj.\nTHE NATURAL WORK-STEALING ALGORITHM IS STABLE 1277\n5. Lower bounds. In this section we give the straightforward lower bound,\nwhich shows that the system is not stable for unsuitable work-stealing functions. Of\ncourse we have to put some restrictions on D in order to obtain instability. For\nexample, if D is the point distribution containing a single function h which allocates\none generator to each processor, then the system will be stable even without any\nwork-stealing.\nThe proof of our lower bound uses the following lemma, which is Theorem 2.2.7\nof [16].\nLemma 21 (Fayolle, Malyshev, and Menshikov [16]). An irreducible aperiodic\ntime-homogeneous Markov chain \u03b6 with countable state space \u2126 is transient if there\nis a positive function \u03a6 with domain \u2126 and there are positive constants C, d, and \u03be\nsuch that\n1. there is a state x with \u03a6(x) > C, and a state x with \u03a6(x) \u2264 C,\n2. E[\u03a6(\u03b61)\u2212 \u03a6(\u03b60) | \u03b60 = x] \u2265 \u03be for all x with \u03a6(x) > C, and\n3. if |\u03a6(x) \u2212 \u03a6(y)| > d, then the probability of moving from x to y in a single\nmove is 0.\nIf we use k = 1 in the statement of the following theorem, we find that the system\nis unstable if f(\u0006) < \u03bbn\u2212 1.\nTheorem 22. Let \u03b4 be a positive constant and \u03bb an arrival rate which is at\nmost 1 \u2212 \u03b4. Suppose that D contains a single generator-allocation function h which\ndistributes the n generators equally among some set of k processors. Suppose that for\nall \u0006, f(\u0006) \u2264 j(n). Then the Markov chain X is transient if\nk \u00b7 (j(n) + 1) < \u03bbn.\nProof. This theorem can be proven easily using Lemma 21. Recall that the start\nstate X0 is (0, . . . , 0) (all queues are initially empty). First, we bound the amount of\nwork that can be done during any given step. When a processor steals work, it only\ngets enough work for at most j(n) rounds. Since each processor gives work to only\none other processor per round, and there are at most k processors with generators,\nat most j(n)k processors without generators have work to do during any given step.\nThus, at most (j(n) + 1)k tasks can be done during any step. The expected load\nincrease of the system during a step is \u03bbn. Using Lemma 21 with \u03a6 as the system\nload, it is easy to see that the system is transient if k(j(n) + 1) < \u03bbn.\n6. Conclusions. We have analyzed a very simple work-stealing algorithm, which\nis successfully being used in practical applications. In this paper we have analyzed\nits performance for a wide range of parameters. We have shown that it is stable for\nany constant generation rate \u03bb < 1 and a wide class of work-stealing functions f . On\nthe other hand, we have shown that for every \u03bb > 0 there is a class of unsuitable\nwork-stealing functions, for which it is not stable. Finally, we have derived upper\nbounds on the system load when the system is stable.\nIt would be interesting to know whether there is a nice characterization of the\nclass of functions that lead to stability. It would also be interesting to know how far\nour upper bounds on system load are from the truth. We suspect that the system\nload is actually much smaller than our upper bounds indicate, but it would be useful\nto have rigorous experimental results.\nAcknowledgments. We thank Hesham Al-Ammal for useful discussions. We\nalso thank the referees, both of whom provided many helpful comments.\n1278 P. BERENBRINK, T. FRIEDETZKY, AND L. A. GOLDBERG\nREFERENCES\n[1] M. Adler, P. Berenbrink, and K. Schro\u00a8der, Analyzing an infinite parallel job allocation\nprocess, in Proceedings of the 6th European Symposium on Algorithms (ESA\u201998), Lecture\nNotes in Comput. Sci., Springer-Verlag, New York, 1998, pp. 417\u2013428.\n[2] M. Adler, S. Chakrabarti, M. Mitzenmacher, and L. Rasmussen, Parallel randomized load\nbalancing, in Proceedings of the 27th Symposium on Theory of Computing (STOC\u201995),\nACM Press, New York, 1995, pp. 234\u2013247.\n[3] H. Al-Ammal, L. A. Goldberg, and P. MacKenzie, An improved stability bound for binary\nexponential backoff, Theory Comput. Systems, 34, (2001), pp. 229\u2013244.\n[4] Y. Azar, A. Z. Broder, A. R. Karlin, and E. Upfal, Balanced allocations (extended ab-\nstract), in Proceedings of the 26th Symposium on Theory of Computing (STOC\u201994), ACM\nPress, New York, 1994, pp. 593\u2013602.\n[5] P. Berenbrink, A. Czumaj, A. Steger, and B. Vo\u00a8cking, Balanced allocations: The heav-\nily loaded case, in Proceedings of the 32th ACM Symposium on Theory of Computing\n(STOC\u201900), ACM Press, New York, 2000, pp. 745\u2013754.\n[6] P. Berenbrink, T. Friedetzky, and E. W. Mayr, Parallel continuous randomized load bal-\nancing, in Proceedings of the 10th Symposium on Parallel Algorithms and Architectures\n(SPAA \u201998), ACM Press, New York, 1998, pp. 192\u2013201.\n[7] P. Berenbrink, T. Friedetzky, and A. Steger, Randomized and adversarial load balanc-\ning, in Proceedings of the 11th Symposium on Parallel Algorithms and Architectures\n(SPAA\u201999), ACM Press, New York, 1999, pp. 175\u2013184.\n[8] D. Bertsimas, D. Gamarnik, and J. N. Tsitsiklis, Performance of multiclass Markovian\nqueueing networks via piecewise linear Lyapunov functions, Ann. Appl. Probab., 11 (2001),\npp. 1384\u20131428.\n[9] R. Blumofe and C. Leiserson, Scheduling multithreaded computations by work stealing, in\nProceedings of the 35th Symposium on Foundations of Computer Science (FOCS\u201994), IEEE\nComputer Society Press, Los Alamitos, CA, 1994, pp. 356\u2013368.\n[10] R. D. Blumofe, C. F. Joerg, B. C. Kuszmaul, C. E. Leiserson, K. H. Randall, and\nY. Zhou, Cilk: An efficient multithreaded runtime system, J. Parallel Distrib. Comput.,\n37 (1996), pp. 55\u201369.\n[11] A. Borovkov, Ergodicity and Stability of Stochastic Processes, John Wiley and Sons, New\nYork, 1998.\n[12] R. Cole, A. Frieze, B. Maggs, M. Mitzenmacher, A. Richa, R. Sitaraman, and E. Up-\nfal, On balls and bins with deletions, in Proceedings of the 2nd International Workshop\non Randomization and Approximation Techniques in Computer Science (RANDOM \u201998),\nLecture Notes in Comput. Sci., Springer-Verlag, New York, 1998, pp. 145\u2013158.\n[13] A. Czumaj and V. Stemann, Randomized allocation processes, in Proceedings of the 38th\nSymposium on Foundations on Computer Science (FOCS\u201997), IEEE Computer Society\nPress, Los Alamitos, CA, 1997, pp. 194\u2013203.\n[14] T. Decker, Virtual Data Space\u2014Load balancing for irregular applications, Parallel Comput.,\n26 (2000), pp. 1825\u20131860.\n[15] P. Fatourou and P. Spirakis, Scheduling algorithms for strict multithreaded computations,\nin Proceedings of the International Symposium on Symbolic and Algebraic Computation\n(ISSAC\u201996), Lecture Notes in Comput. Sci., Springer-Verlag, New York, 1996, pp. 407\u2013416.\n[16] G. Fayolle, V. Malyshev, and M. Menshikov, Topics in the Constructive Theory of Count-\nable Markov Chains, Cambridge University Press, Cambridge, UK, 1995.\n[17] B. M. Feldmann, P. Mysliwietz, and B. Monien, Studying overheads in massively parallel\nmin\/max-tree evaluation, in Proceedings of the 5th Symposium on Parallel Algorithms and\nArchitectures, ACM Press, New York, 1994, pp. 94\u2013103.\n[18] L. Goldberg and P. MacKenzie, Analysis of practical backoff protocols for contention reso-\nlution with multiple servers, J. Comput. Systems Sci., 58 (1999), pp. 232\u2013258.\n[19] G. Grimmet and D. Stirzaker, Probability and Random Processes, 2nd ed., Oxford University\nPress, Oxford, 1992.\n[20] B. Hajek, Hitting time and occupation time bounds implied by drift analysis with applications,\nAdv. Appl. Probab., 14 (1982), pp. 502\u2013525.\n[21] J. Ha\u02dastad, T. Leighton, and B. Rogoff, Analysis of backoff protocols for multiple access\nchannels, SIAM J. Comput., 25 (1996), pp. 740\u2013774.\n[22] R. Lu\u00a8ling and B. Monien, A dynamic distributed load balancing algorithm with provable good\nperformance, in Proceedings of the 5th Symposium on Parallel Algorithms and Architec-\ntures (SPAA\u201993), ACM Press, New York, 1993, pp. 164\u2013172.\n[23] N. R. Mahapatra and S. Dutt, Adaptive quality equalizing: High-performance load balancing\nTHE NATURAL WORK-STEALING ALGORITHM IS STABLE 1279\nfor parallel branch and bound across applications and computing systems, in Proceedings\nof the Joint IEEE Parallel Processing Symposium\/Symposium on Parallel and Distributes\nProcessing, IEEE Computer Society Press, Los Alamitos, CA, 1998, pp. 796\u2013800.\n[24] C. McDiarmid, On the method of bounded differences, in Surveys in Combinatorics, London\nMath. Soc. Lecture Note Ser. 141, Cambridge University Press, Cambridge, UK, 1989,\npp. 148\u2013188.\n[25] S. Meyn and R. Tweedie, Markov Chains and Stochastic Stability, Springer-Verlag, London,\n1993.\n[26] M. Mitzenmacher, Density dependent jump Markov processes and applications to load bal-\nancing, in Proceedings of the 37th Symposium on Foundations of Computer Science\n(FOCS\u201996), IEEE Computer Society Press, Los Alamitos, CA, 1996, pp. 213\u2013222.\n[27] M. Mitzenmacher, The Power of Two Random Choices in Randomized Load Balancing, Ph.D.\nthesis, Graduate Division, University of California at Berkeley, 1996.\n[28] M. Mitzenmacher, On the analysis of randomized load balancing schemes, in Proceedings of\nthe 9th Symposium on Parallel Algorithms and Architectures (SPAA\u201997), ACM Press, New\nYork, 1997, pp. 292\u2013301.\n[29] M. Mitzenmacher, Analysis of load stealing models based on differential equations, in Proceed-\nings of the 10th Symposium on Parallel Algorithms and Architectures (SPAA\u201998), ACM\nPress, New York, 1998, pp. 212\u2013221.\n[30] M. Mitzenmacher, A. Richa, and R. Sitaraman, The power of two randomized choices: A\nsurvey of techniques and results, in Handbook of Randomized Computing, Kluwer Aca-\ndemic, Dordrecht, The Netherlands, 2001, pp. 255\u2013305.\n[31] R. Motwani and P. Raghavan, Randomized Algorithms, Cambridge University Press, Cam-\nbridge, UK, 1995.\n[32] R. Pemantle and J. S. Rosenthal, Moment conditions for a sequence with negative drift to\nbe uniformly bounded in lr, Stochastic Process. Appl., 82 (1999), pp. 143\u2013155.\n[33] L. Rudolph, M. Slivkin-Allalouf, and E. Upfal, A simple load balancing scheme for task\nallocation in parallel machines, in Proceedings of the 3rd Symposium on Parallel Algo-\nrithms and Architectures (SPAA \u201991), ACM Press, New York, 1991, pp. 237\u2013245.\n[34] B. Vo\u00a8cking, How asymmetry helps load balancing, in Proceedings of the 40th Symposium on\nFoundations of Computer Science (FOCS\u201900), IEEE Computer Society Press, Los Alamitos,\nCA, 2000, pp. 131\u2013140.\n[35] N. D. Vvedenskaya, R. L. Dobrushin, and F. I. Karpelevich, Queueing system with selec-\ntion of the shortest of two queues: An asymptotic approach, Problems Inform. Transmis-\nsion, 32 (1996), pp. 15\u201327.\n\n\n"}