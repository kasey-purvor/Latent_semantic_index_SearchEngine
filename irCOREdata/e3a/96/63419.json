{"doi":"10.1016\/S0098-3004(00)00041-8","coreId":"63419","oai":"oai:nora.nerc.ac.uk:2401","identifiers":["oai:nora.nerc.ac.uk:2401","10.1016\/S0098-3004(00)00041-8"],"title":"Geoscience after IT: Part F. Familiarization with quantitative analysis","authors":["Loudon, T.V."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":["Loudon, T.V."],"datePublished":"2000-04","abstract":"Numbers, measurement and calculation extend our view of the world. Statistical methods describe the properties of sets of quantitative data, and can test models (particularly the model that observed relationships arose by chance) and help us to draw conclusions. Links between spatial and quantitative methods, through coordinate geometry and matrix algebra, lead to graphical representations for visualizing and exploring relationships. Multivariate statistics tie into visualization to look at pattern among many properties","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/63419.pdf","fullTextIdentifier":"http:\/\/nora.nerc.ac.uk\/2401\/1\/Part_F.pdf","pdfHashValue":"fc154d1358746ab158a9f26a1af1d1d9b2fec56c","publisher":"Pergamon Elsevier-Science, Oxford","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:nora.nerc.ac.uk:2401<\/identifier><datestamp>\n      2012-11-22T11:48:18Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D5338<\/setSpec><setSpec>\n      7375626A656374733D5339<\/setSpec><setSpec>\n      7375626A656374733D533130<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/nora.nerc.ac.uk\/id\/eprint\/2401\/<\/dc:relation><dc:title>\n        Geoscience after IT: Part F. Familiarization with quantitative analysis<\/dc:title><dc:creator>\n        Loudon, T.V.<\/dc:creator><dc:subject>\n        Computer Science<\/dc:subject><dc:subject>\n        Data and Information<\/dc:subject><dc:subject>\n        Earth Sciences<\/dc:subject><dc:description>\n        Numbers, measurement and calculation extend our view of the world. Statistical methods describe the properties of sets of quantitative data, and can test models (particularly the model that observed relationships arose by chance) and help us to draw conclusions. Links between spatial and quantitative methods, through coordinate geometry and matrix algebra, lead to graphical representations for visualizing and exploring relationships. Multivariate statistics tie into visualization to look at pattern among many properties.<\/dc:description><dc:publisher>\n        Pergamon Elsevier-Science, Oxford<\/dc:publisher><dc:contributor>\n        Loudon, T.V.<\/dc:contributor><dc:date>\n        2000-04<\/dc:date><dc:type>\n        Publication - Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/nora.nerc.ac.uk\/id\/eprint\/2401\/1\/Part_F.pdf<\/dc:identifier><dc:identifier>\n         \n\n  Loudon, T.V..  2000  Geoscience after IT: Part F. Familiarization with quantitative analysis.   Computers & Geosciences, 26 (3, Sup). A41-A50.  https:\/\/doi.org\/10.1016\/S0098-3004(00)00041-8 <https:\/\/doi.org\/10.1016\/S0098-3004(00)00041-8>     \n <\/dc:identifier><dc:relation>\n        http:\/\/www.elsevier.com\/wps\/find\/journaldescription.cws_home\/398\/description#description<\/dc:relation><dc:relation>\n        doi:10.1016\/S0098-3004(00)00041-8<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/nora.nerc.ac.uk\/id\/eprint\/2401\/","http:\/\/www.elsevier.com\/wps\/find\/journaldescription.cws_home\/398\/description#description","doi:10.1016\/S0098-3004(00)00041-8"],"year":2000,"topics":["Computer Science","Data and Information","Earth Sciences"],"subject":["Publication - Article","PeerReviewed"],"fullText":"<<<Back to Table of Contents       \nOn to Part G: Familiarization with spatial analysis>>> \n \n \nGeoscience after IT: Part F \n \nFamiliarization with quantitative analysis \n \nT. V. Loudon \nBritish Geological Survey, West Mains Road, Edinburgh EH9 3LA, U.K. \ne-mail: v.loudon@bgs.ac.uk \n \n \nPostprint of article in Computers & Geosciences, 26 (3A) April 2000, pp. A41-A50 \n \nAbstract - Numbers, measurement and calculation extend our view of the world. \nStatistical methods describe the properties of sets of quantitative data, and can test \nmodels (particularly the model that observed relationships arose by chance) and help \nus to draw conclusions. Links between spatial and quantitative methods, through \ncoordinate geometry and matrix algebra, lead to graphical representations for \nvisualizing and exploring relationships. Multivariate statistics tie into visualization to \nlook at pattern among many properties. \n \nKey Words - Statistics, matrix algebra, visualization, multivariate analysis. \n \n \n1. Background \n \nComputing, in the sense of calculation, is a small part of IT applications in \ngeoscience. But, even in traditional aspects of geology, the quantitative representation \nof spatial entities and models is important. To understand why, we need to look at \nsome basic mathematical concepts, and see how numbers relate to properties and \nprograms to physical processes. Readers with a mathematical background may prefer \nto skip at least the first part of this chapter, and those without may wish to skip detail \nirrelevant to them. \n \n2. Measurement and number \n \nReal numbers form a continuous sequence in an exact order. Given any two numbers, \nsay 1.415 and 1.416, you can find as many numbers as you need between them, for \nexample 1.4151 or 1.4150032. You can compare any two numbers to see if they are \nequal or if one is larger or smaller than the other. This leads to a scale of \nmeasurement. The continuous sequence of numbers is analogous to situations in the \nreal world. For example, you can compare the thickness of any two beds of sandstone \nto see if they are the same or if one is thicker or thinner than the other. To avoid \ncarrying sandstone around, you can measure their thicknesses by comparison with a \nstandard tape marked in millimeters. Measurements enable comparisons between any \ntwo bed thicknesses. You can measure more than one property. For example, you \ncould also measure the maximum grain size for the beds of known thickness and \ncompare the two sets of numbers (bed thickness and grain diameter), pair by pair.  \nLoudon, T.V., 2000. Geoscience after IT: Part F  (postprint, Computers & Geosciences, 26(3A)) \nWe can use numbers in several other ways that rely on different aspects of their \nproperties and so must be handled differently (see Krumbein and Graybill, 1965 or \nDavis, 1973 for a fuller account). We can assign numbers quite arbitrarily as object \nidentifiers. An identification number, such as an accession number (numbers from a \ngiven range issued in sequence), need bear no relationship to the object\u2019s properties. \nIndeed it is easier to issue unique identifiers by separating identification from \ndescription. A program or a user who knows the identifier can find the object by \nconsulting a numerical index. Integer numbers are appropriate for identifiers. \n \nThe subject classification on a book or library shelf, such as a UDC number, is rather \nless arbitrary. Similar numbers apply to related subjects, and the number hierarchy \n(hundreds, tens, units) reflects subject subdivisions (part H section 2). Numbers with \ndecimal fractions are convenient in book classification. One can insert additional \nsubdivisions without limit, simply by adding more digits after the decimal point. By \nshelving books or arranging object identifiers in numerical order we bring together \nthose on the same subject for convenient searching or browsing. A sequence of \nnumbers can represent a strict order of categories. Typical ordered categories are \nMohs\u2019 scale of mineral hardness and the Richter scale of earthquake intensity. The \nlarger the number, the higher the value, but the steps between successive values are \nnot equal. \n \nMeasurement compares a property of some object with a standard scale. Intervals are \nequal, although the zero value may be quite arbitrary. For example, most scales of \ntemperature, unlike the Kelvin scale, place zero at a convenient but arbitrary point. It \nwould therefore be foolish to say that 20oC is twice as hot as 10oC.  Nevertheless, we \ncan reasonably say that the increase of temperature from 0o to 10o is half that from 0o \nto 20o. Other physical properties, such as length, have an obvious and unique zero \nvalue, and there is no difficulty in adding, subtracting, multiplying and dividing those \nquantitative measurements.  \n \nThe number field can then lead to a useful model at a deeper level than categorization. \nEquations can mimic real physical relationships. For instance, physicists can write \nequations describing the relationships of temperature with the pressure and volume of \na closed body of gas. Equations imply the ability to calculate and maybe predict. \nAspects of physical systems have direct analogs in well-known arithmetic operations. \nThis astonishing correspondence between the physical world and mathematics is the \nbasis for mathematical modeling (F 3).  \n \nThe quantitative approach introduces a new mode of thinking. Instead of seeing the \nsubject of investigation as a set of discrete objects, such as formations and rock types, \nwe view it as a continuum, with characteristics that we can measure and compare as \nthey vary from place to place.  Gravity and aeromagnetic surveys, satellite imagery, or \nregional geochemical studies of stream sediments are examples. If objects are seen as \n\u2018things\u2019 represented by nouns, and processes resemble verbs, then quantitative \nmeasurements are more akin to adjectives, describing the properties or composition of \nthe objects. \n \nIt is tempting to wonder how far this mode of thinking can extend. Could we, for \ninstance, replace our rather arbitrary classifications of geological objects by a more \nquantitative view where we measure continual change. This is considered further in J \nLoudon, T.V., 2000. Geoscience after IT: Part F  (postprint, Computers & Geosciences, 26(3A)) \n2.3, but classification is basic to science (J 2.1) and descriptions with adjectives and \nno nouns have little meaning. I argue later (L 6.3) that while, with IT support, the \nscope of quantitative studies will surely continue to expand, different modes of \nthought are complementary, each adding to the overall understanding. The more \nimportant role of IT may be to ensure that information of all kinds is readily available \nto the investigators. If this is correct, the scientist (or a multidisciplinary team) needs \nto understand and use an appropriate combination of methods and modes of thought.  \n \nBefore collecting measurements, it makes sense to consider their intended \napplications. This is the next topic. For detail, see Griffiths (1967), Krumbein and \nGraybill (1965), Davis (1973) or Swan and Sandilands (1995). \n \n3. Descriptive statistics \n \nWe can manipulate numbers with simple operations of addition, subtraction, \nmultiplication and division. They take us beyond individual comparisons to the \nproperties of entire sets of measurements, and to general statements about \nrelationships, say between grain size and bed thickness. Statistics (the branch of \nmathematics that deals with collecting, analyzing, interpreting and presenting \nnumerical data) addresses these topics. One requirement is to characterize a set of \nmeasurements, like bed thickness, by fewer numbers that reflect the properties of the \nset as a whole. Important statistics (the measures or values calculated using the \nscience of statistics) include the average value, also known as the mean. It is \ncalculated by adding the measurements together and dividing the total by the number \nof measurements. We can measure the spread of values around the mean by the \nvariance (the mean squared deviation from the mean) or by its square root - the \nstandard deviation.  \n \nStatistics leads on from the description of a single variable, that is, a set of \nmeasurements of a single property, to explore the relationships between pairs of \nvariables measured at the same point, such as bed thickness and grain size. An \nobvious approach would be to multiply each pair of measurements together and take \ntheir average (the mean cross-product). But the mean and standard deviation of each \nvariable would greatly affect the result, and these have been measured already. \nInstead, we can standardize each variable by subtracting the mean from each value \nand dividing the result by the standard deviation. The mean cross-product of the \ntransformed variables is known as the correlation coefficient, which has a value \nsomewhere between +1 and -1. The extreme values are 1 if the bed thickness \nincreases precisely as the grain size increases, and -1 if one decreases precisely as the \nother increases. The value is 0 if one variable shows no relationship to the other.  \n \nThere are two general points here. One is that statistics measure different properties \nseparately. Having calculated the mean, we remove its effects in calculating the next \nproperty, the standard deviation. We remove the effects of both in calculating the \ncorrelation coefficient. As a consequence, we can compare variation in a sequence of \nthick beds with that in a sequence of thin beds, and can judge whether the correlation \nof bed thickness and grain size is more pronounced in sandstone or in siltstone.  \n \nThe other general point is that we are not dealing with sharply defined relationships. \nIf we had measured the properties a few millimeters away, or made twice as many \nLoudon, T.V., 2000. Geoscience after IT: Part F  (postprint, Computers & Geosciences, 26(3A)) \nmeasurements, the results would have been different. If the processes of deposition \nhad changed, with stronger currents, deeper water or different grain composition, the \nresults would again differ. Statistical methods can measure the uncertainties of \nsampling and imperfect knowledge of the process. Their success depends on the skill \nwith which the data are sampled and analyzed and on appreciation of the subject \nmatter.  \n \n \n \nFig. 1. Calculation of simple statistics with a spread-sheet. The total thickness of Pleistocene and \nRecent sediments were recorded at twenty boreholes, together with the thickness of peat in each. \nSimple statistics were calculated with a Microsoft Excel spreadsheet to examine the frequency \ndistributions and their statistical correlation. See also Fig. 3. \n \nStatistics are normally calculated by computer, particularly if the datasets are large. \nGood programs are readily available. The most flexible, although not the easiest to \nuse, are subroutine libraries. The computer program normally calculates the values of \nstatistical parameters using mathematical shortcuts. However, for teaching or \nexploratory purposes, spreadsheets and bar charts show intermediate steps and their \nLoudon, T.V., 2000. Geoscience after IT: Part F  (postprint, Computers & Geosciences, 26(3A)) \neffects on the individual items. For instance, they can show the original measurements \nconverted to standard deviations from the mean (columns D and E of Fig. 1) and the \nuser can examine the measurements in a local framework that may clarify \nrelationships. \n \nSome statistical programs help the user by providing an account of each method, a \ndescription of the algorithm, and examples of its use. The examples are unlikely to \nrefer to geoscience, but it can be helpful to take an example as a template, and replace \nits variables and data with your own. An excellent range of textbooks is available on \nstatistical methods and their applications. I have no plans to add to them, but do wish \nto point out the place of such techniques in geoscience investigations and to indicate \nsome assumptions that constrain their application. The calculations of mean and \nstandard deviation make no such assumptions. Their interpretation, however, raises \nmany questions. The properties of the sets of beds constitute the population (D 4), as \nopposed to the actual measurements, which constitute a sample of the population. \nSampling theory helps to clarify the link, so that conclusions about the population can \nbe drawn from the sample, if appropriate sampling procedures (D 4) have been \nfollowed. \n \nCircumstances determine the appropriateness of statistics. For example, an average \nthickness calculated from 49 siltstone beds and one very much thicker conglomerate \nbed would not be helpful. The result would alter greatly if we arbitrarily included \nanother thick bed. A better procedure would be to study the thickness of conglomerate \nseparately. Statistical measures make sense only for a clearly defined and coherent \npopulation. The frequency distribution, that is the pattern of relative frequencies of \neach measured value, can be examined on a bar chart or frequency plot (Fig. 1). \nIdeally, the frequencies are greatest in the center and fall off on either side to give the \nsymmetrical bell-shaped frequency distribution of the so-called normal distribution \n(Fig. 2). A surprising number of actual distributions approximate to this, perhaps after \na simple transformation such as replacing the original values by their logarithms (F 5). \nIt then makes sense to describe the distribution as a whole with a few numbers, such \nas mean and standard deviation. Otherwise, robust statistics, described in most \nmodern statistics texts, offer a less complete means of description but make fewer \nassumptions. \n \n \nFig. 2. Bell-shaped frequency curve of a normal distribution. The values of a variable that are deflected \nfrom their expected value (the mean) by many random events, might be expected to show this type of \ndistribution. \n \nWith some assumptions about the distribution and sampling scheme, it is possible, for \nexample, to calculate the likely population mean and variance from the sample, and \nthe probability of their lying within a particular range. A technique known as analysis \nLoudon, T.V., 2000. Geoscience after IT: Part F  (postprint, Computers & Geosciences, 26(3A)) \nof variance can show how mean values relate to sources of variation. For instance, if \nthe ratio of Ca to Mg were determined in a number of samples, it could be of interest \nto see how it varied between formations, or between lithologies, or between analytical \nlaboratories. With a carefully designed investigation, analysis of variance might be \nable to separate out the effects of each. Examination of the frequency distribution may \nhowever suggest a more complex situation, such as populations of different \ncharacteristics being sampled together. Descriptive statistics could then mislead by \nobscuring the real complexity. \n \nPresumably measurements are made in order to draw some conclusions or to check \nsome hypothesis. The conclusions must refer to something beyond the measurements \nthemselves. Not \u201chere are fifty beds that I have measured\u201d, but rather \u201cthese beds are \nnoticeably thicker than their counterparts farther east, and the beds are thicker and the \ngrain size coarser towards the base of the succession\u201d. Hypotheses about directions of \nsediment movement or deepening of the basin might in turn have prompted an interest \nin these findings. The hypotheses must be linked to the more general concepts in \nwhich they are embedded, and may lead to a mathematical model. \n \nThe analogy between the number field and the measurement of properties extends to \nthe mathematical model - an analogy between mathematical operations (operating on \nthe numbers) and physical processes (affecting the properties). Thus, adding together \nthe thickness of beds in a vertical section is equivalent to finding their total thickness, \na reflection, perhaps, of the total deposition of sediment at that point. Dividing the \ntotal by the number of beds to find the average is equivalent to recreating the original \nnumber of beds, but all of the same thickness. If nothing else, this may remind us that \nthere is nothing magical about calculation. However, mathematical operations can \nmimic aspects of quite complex physical operations, often surprisingly well. \n \nIf you develop one or more quantitative models before or during data collection, you \ncan statistically compare the predictions of the models with the observed data to see if \nthey conflict. This somewhat negative view is characteristic of scientific argument. If \nthe observed values lie within the range of expected values, they give no indication \nthat the model fails to match reality, and the model may in consequence be accepted. \nAcceptance is always tentative, for there may be other models that would also fit and \nwould be more realistic in other ways. Quantitative models can help to investigate \nsimple aspects of the process, such as: is it likely that the data reflect purely random \nevents? Griffiths (1967) showed how salutary this approach can be. They can also be \ndesigned to throw light on the deep structures of the process (Wendebourg and \nHarbaugh, 1997). Science always seeks to disprove, and if data conflict with the \npredictions of the model, this could be taken as disproof of the model\u2019s validity.  \n \nThe random model, in which events proceed on the basis of chance alone, is widely \nused in statistics. It addresses the question of whether the results of analysis might \nmerely be a consequence of random variation. If this can be ruled out, the argument \nfor an alternative explanation is strengthened. A statistical model may have a \ndeterministic element, giving precise values calculated from the mathematical model \nrepresenting the physical process. It may also include a random element, reflecting the \nunpredicted, chance events (although as pointed out in J 2.3, some non-linear \ndeterministic systems are inherently unpredictable). The random element makes it \npossible to specify not only the most likely values of the outcome of the model, but \nLoudon, T.V., 2000. Geoscience after IT: Part F  (postprint, Computers & Geosciences, 26(3A)) \nalso a range within which the values are likely to occur, taking random effects into \naccount. \n \nIf the investigation is more than simply an initial gathering of ideas, and the data will \nbe widely shared, then the investigator should describe the procedures and state how \nobservations and measurements were made (D 4). Another scientist following the \nsame sampling scheme and procedures would not expect to obtain identical data, but \nwould expect the overall statistics to match within the calculated margin of error. An \naccount of the sampling scheme can also help others to decide how far to rely on the \nresults. By invoking statistical arguments, the process of geoscience investigation and \nreasoning can be made more rigorous and repeatable. \n \nQuantitative methods have another role to play in geoscience. Measurements may be \nmade in an exploratory way, without clear ideas about which of a range of hypotheses \nis being tested, but with hopes of unearthing a familiar pattern or detecting an \ninteresting relationship (Tukey, 1977). Many geoscience investigations are concerned, \nnot with the operation of physical systems, but with geological events and the \nenvironment in which they occurred. Sets of measurements may throw light on spatial \npatterns and interrelationships that guide the formulation of hypotheses. In \ngeoscience, data are commonly displayed as a map or set of maps. Computer \nvisualization studies (Cleveland, 1993, Gallagher, 1995) address the wider question of \nhow to display data to detect pattern (G 2). They normally start with quantitative data \nand explore graphical means of conveying their significance to the human eye and \nbrain. Field mapping faces the same problem of detecting pattern among a host of \ninterrelated properties. It too can be seen as a form of spatial visualization. Statistical \nmethods can then help in another way. They may offer concise summaries (like the \nmean) and reveal relationships that otherwise might not be noticed. They are unlikely \nto offer rigorous proof of the conclusions, but might point you towards a conclusion \nthat you could test or support by other lines of argument more familiar in geoscience. \n \n4. Matrix algebra and spatial data \n \nThe development of coordinate geometry by Descartes in the early 17th century \ngave the basis for spatial visualization of quantitative data, but also meant that spatial \ndata could be brought within a quantitative framework. Position in space is measured \nby coordinates - distances from a zero point known as the origin along each of a set \nof axes at right angles. In consequence, spatial data can be manipulated, analyzed and \nmanaged on the computer. A range of computer techniques can be applied to \ninformation that would normally be recorded on maps and cross-sections. Those of us \nwho think more readily in pictures than in numbers can make use of the \ncorrespondence between numbers and position, and between algebraic and geometric \noperations, as an easy way to gain an understanding of statistical methods. Many \ngeoscientists may find it easier to visualize quantitative techniques as manipulation of \npoints in variable space rather than as manipulations of numbers. \n \nThe link between computation and space, between algebra and geometry, is perhaps \nmost obvious in matrix algebra, which enables a sequence of related operations to be \nwritten as one operation. Matrix algebra is an extensive and complex study in its own \nright, and is widely used in quantitative geoscience. Most computer users who are not \nprogrammers themselves, can understand the results without understanding the details \nLoudon, T.V., 2000. Geoscience after IT: Part F  (postprint, Computers & Geosciences, 26(3A)) \nof the method. A few words of explanation here may make the process less \nmysterious to those who lack the mathematical background. \n \nA table of quantitative values, such as those set out in a spreadsheet, can be regarded \nas a matrix. A single row of the matrix can be referred to as a row vector, and a \nsingle column as a column vector. The individual values or elements of the matrix are \nreferred to in the spreadsheet by a letter indicating the column and a number \nindicating the row. In matrix algebra, the notation is slightly different, with the row \nand column both indicated by numbers. Letters are used, not to designate a specific \ncolumn, but as placeholders that can be replaced by any number. Algebraic statements \nusing the placeholder (or index) are thus quite general, and not tied to any particular \nnumeric values. The matrix as a whole can be referred to by a name, in algebra \nusually a capital letter in bold type. The element has the same name, in lower case \nwith the row and column numbers as suffixes. Thus the element xij is in row i and \ncolumn j of the matrix X. A typical notation in a programming language is X(i,j) \nwhere X is a name that could be several characters in length.  \n \nMatrices of the same size, that is, the same number of rows and columns, can be \nadded. Z=X+Y means that each zij = xij + yij . Subtraction follows the same pattern. \nMultiplication, Z = X.Y, requires that the number of columns in X is equal to the \nnumber of rows in Y. The element zij  is found by adding the products of the elements \nin the ith row of X with the corresponding elements in the jth column of Y.  \n zij = xi1.y1j + xi2.y2j . . . xin.ynj   \nThe ellipsis (. . .) indicates continuation of the preceding series in the same way, and n \nis the number of columns in X. \n \nA problem frequently encountered in statistics and in some geophysical topics is that \nof solving a set of simultaneous equations, which could be written as A = x, where x \nis a column vector. In matrix notation, the general solution is A-1 . Thus matrix \nalgebra is useful where each new value is dependent on several existing values in \nsome systematic way. It provides a more compact and so more powerful notation than \nwriting out each individual operation. \n \nReturning to coordinate geometry, let us suppose that x, y and z are variables holding \nthe latitude, longitude and elevation of a point in space. They can first be brought to \nthe same units, say meters from an origin at mean sea level on the lower left corner of \na map sheet. A set of points within the map sheet, say where samples had been \ncollected, could be numbered with an index i, which takes the values 1, 2, 3 . . . n. As \nsimilar types of measurement are made for each sample, it is convenient to refer to \nthem all in the same way. Each has an x, y and z value to indicate its location. To \nidentify each sample, it can be given a suffix; thus the ith sample is located at xi, yi, zi. \nThe three values together, placed in brackets (xi, yi, zi), form a vector, in the sense of a \nset of numbers referring to the properties of an object. In this case, because the \nelements of the vector are geometrical coordinates, (xi, yi, zi) also denotes a vector in \nthe geometric sense - a line from the origin (0,0,0) with length, orientation and \ndirection.  \n \nAs x, y, and z, once they are measured in the same units, refer to similar things, it is \nconvenient to refer to them with the same letter, say x1, x2, x3 (or xj, where j=1,2,3). \nThe values of x then have two suffixes, and the values can be arranged as a table with \nLoudon, T.V., 2000. Geoscience after IT: Part F  (postprint, Computers & Geosciences, 26(3A)) \nrows i, numbered from 1 to n and columns j, numbered from 1 to 3. In Fortran, the \nmatrix is referred to by the more general term of an array. It is one-dimensional if it \nhas one suffix, two-dimensional if it has two, and so on, whether or not this is its \ngeometric dimension. The geometric operations that might be applied to these vectors \nare described in G 4. Their algebraic equivalents may involve changing each of the \nvalues of each row vector (xi1, xi2, xi3) in a way that depends on all three of the current \nvalues. If the corresponding values after the operation are called y, then we can write  \n yi1 = axi1 + bxi2 + cxi3 \nwith similar equations for yi2 and yi3 , making three in all. Rather than referring to the \nconstants, such as a, b and c, with separate letters, they can be seen as a matrix in their \nown right with three rows and three columns, say T. The transformation of the entire \ndata matrix X  to new values Y can then be written as Y = XT. Some important \ngeometric operations have equivalents in matrix algebra that can be implemented on a \ncomputer system. As described in G 4, the transformation matrix T can represent \nfamiliar operations of moving objects about and changing their shape. It is a basic tool \nin creating computer maps and multidimensional spatial models. \n \n5. Multivariate statistics \n \nThe link between numbers and space, between algebra and geometry, works in both \ndirections. Spatial features can be represented by numbers; quantitative data points \ncan be visualized as a cloud of dots. They can be manipulated in a space where the \ncoordinate axes, at right angles to one another, are marked off in the units of \nmeasurement of the individual variables. The units refer to any measured property, \nsuch as bed thickness, grain size, gravity or uranium content. There is no limit to the \nnumber of axes, but we have trouble visualizing more than three at a time, as we \nappear to live in a three-dimensional world. Visualization may help us to understand \nthe statistical relationships of a set of variables (see Cook, 1998). Statistics is \nconcerned not just with single variables and comparison of different sets of \nmeasurements of the same variable, but also with the relationships between different \nproperties of the same objects. This leads to techniques of multivariate analysis (a \nvariate is a variable that exhibits some random variation). \n \nGiven a set of quantitative data, say, a collection of measurements of fossil shells, \nstatistical methods are a guide to possible conclusions. Many different properties \ncould be measured on each shell, such as length, breadth, thickness, length of hinge-\nline, and number of growth lines. We might wish to investigate how the properties are \nrelated to one another. We might also need some means of measuring the \ncharacteristics of the set of measurements as a whole, to compare them with another \nset from a different locality. The task has moved from comparing individual \nmeasurements to that of comparing aggregates of measurements, that is, a set of \ndistinct items gathered together.  \n \nA starting point, however, is to look at the characteristics of each variate in terms of \nstatistics such as the mean and standard deviation (F 3). Each variate can then be \nstandardized to zero mean and unit standard deviation. This frame of reference may \nmake it easier to compare their relative variation. The cloud of standardized data \npoints is centered on the origin and has equal spread or dispersion along each axis. \nMeasures, such as skewness and kurtosis, based on the third and fourth powers of the \ndeviations from the mean, can be calculated to assess the symmetry and shape of the \nLoudon, T.V., 2000. Geoscience after IT: Part F  (postprint, Computers & Geosciences, 26(3A)) \nfrequency distribution of each variable (F 3). However, there is no substitute for their \nvisual inspection with a bar chart, histogram or scatter diagram.  \n \nSome frequency distributions are quite unevenly distributed about the mean, such as \nthe grain size of sediments or thickness of beds in a vertical section. A log \ntransformation, which compresses the scale at the higher end, can bring the \ndistribution to a more tractable form. Many other transformations are possible, and \nmay be justified simply because the subsequent analysis is more straightforward. It is \nmore satisfying if there is a physical justification for the transformation. For example, \nif an organism doubles in size each year, the distribution of size at random times will \nbe logarithmic, reflecting a multiplicative rather than an additive process. Replacing \nthe original measurements by their logarithms converts the numbers to a simple \nstraight-line distribution. \n \nStatistical reasoning, as opposed to description, tends to assume that variates \napproximately follow the so-called normal distribution - the familiar bell-shaped \ncurve of Fig. 2. Under a number of assumptions, it is possible to compare the actual \nsample with that expected from a random set of events, and determine the likelihood \nof this \u201cnull hypothesis\u201d being incorrect. A number of excellent textbooks, including \nsome for geoscientists (for example, Davis 1973), give fuller information on these \npowerful methods. \n \nThe relationship between two variates can be measured by the correlation coefficient \n(F 3), or by a regression equation. The regression equation predicts the value of one \nvariable (y) from that of another (x). The regression equation describes a line, using \nthe formula y=a+bx, selecting the values of a and b to minimize the sums of squares \nof deviations between the measured and calculated values of y (see Fig. 3). The \naverage squared deviation is a measure of the closeness of fit of the data to the line. \nThe line that best fits the data could be regarded as a mathematical model of the \nrelationship. As before, transformations that give the individual distributions a shape \nlike the normal distribution are helpful. The null hypothesis of no correlation can be \ntested, given a number of assumptions, and only if it fails is there a need for a \nscientific explanation of the relationship. \n \nThe situation becomes more interesting where several variates are measured on the \nsame items. You may be able to visualize the data as a cloud of dots in n dimensions, \neach dot representing one item, and each axis representing the standardized \nmeasurements for one variate. This is easiest with 2 or 3 variates, but the calculations \nare similar in higher dimensions. Each axis is regarded as independent of the others, \nwhich in geometry means that they are shown at right angles to one another. View the \ncloud in any two dimensions, and a correlation may be apparent that can be measured \nby the correlation coefficient. The correlation coefficients can be calculated for each \npair of variates separately and shown as an n x n matrix, where n is the number of \nvariates. There may be underlying processes that affect many of the variates together. \nPossibly there are several such processes affecting different variates in different ways. \nThe matrix of correlation coefficients may throw light on the structure of the data \nwhich in turn might suggest underlying causes.  \n \nLoudon, T.V., 2000. Geoscience after IT: Part F  (postprint, Computers & Geosciences, 26(3A)) \n \n \nFig. 3. Line of best fit between two variables. The data of Fig. 1 are plotted here to examine their \ncorrelation, and a regression line added. The chart was prepared from the spreadsheet with Microsoft \nExcel software. \n \nOne procedure for analyzing the correlation matrix is known as principal component \nanalysis.  It simply rotates the cloud of points in n dimensions (try visualizing it in \nthree), until the largest variance is along one axis (the first principal axis), the greatest \nremaining variability along the second, and so on. The least variance is that around \nthe final principal axis. The number of principal axes is the same as the original \nnumber of variates. They are still at right angles, but have been rotated together, as \ndescribed, to a new orientation. When the points are referred to the new frame of \nreference (the principal axes), the new variates are known as the principal \ncomponents. It is likely that most of the total variance will be accounted for by the \nfirst few components. If the remainder show no interesting pattern and appear merely \nto reflect random effects, they can be disregarded. The result from the principal \ncomponent analysis (PCA) program is thus a smaller set of new variates and a \nstatement of how much of the variance each represents. The relative contribution of \neach of the original variates to each principal component is defined. The challenge for \nthe geoscientist is to decide whether the principal components reflect underlying \ncausal processes, or if not, how they might be accounted for. For example, the \nmeasurements of many aspects of the size and shape of fossil shells might be related \nto a few features of the environment like wave energy, nutrient availability, depth and \nclarity of water. This approach has been extended and elaborated as factor analysis \n(see Reyment, J\u00f6reskog, 1993). \n \nAs well as the correlation coefficient between two variables, we noted the regression \nequation as an alternative way of looking at the relationship. This again is not limited \nto two variates. An equation y = a + bx1 + cx2 + . . . + gxn  representing a straight line \nin n dimensions, can be fitted to n variates x1 to xn, so that the value of the selected \nvariable y can be predicted from all the x variates, minimizing the total sum of squares \nof differences between its measured values and the values predicted by the equation. \nLoudon, T.V., 2000. Geoscience after IT: Part F  (postprint, Computers & Geosciences, 26(3A)) \nUnlike PCA, which treats all variates alike, multiple regression focuses on one \nvariate, with all the others seen as contributing to its variation. \n \nAs an aside, if the number of terms in a regression equation is greater than the number \nof data points, additional information is required to give a unique equation. Methods \nof linear programming achieve this by introducing an objective function that must \nbe maximized to yield the greatest benefit to the system. This has applications in \nallocating raw materials to different products, as in models that allocate chemical \nelements to the mineral constituents of a crystallizing igneous rock.  The more usual \nstatistical case is overspecified, with many more data points than terms in the \nequation, and the least-squares criterion just mentioned, or a similar alternative, is \nused to arrive at a unique surface. \n \n \n \nFig. 4. Generation of polynomial curves. The basis functions for a cubic polynomial, and the combined \ncurve from adding them together, are shown in A. In B, the coefficients are altered to give a different \ncurve, which is still smooth, has the same number of inflection points, and heads for plus or minus \ninfinity at each end. \nLoudon, T.V., 2000. Geoscience after IT: Part F  (postprint, Computers & Geosciences, 26(3A)) \n \n \nThe form of the regression equation suggests how it is possible to fit curves other than \nstraight lines to a relationship between two variables x and y. From the values of x, it \nis a straightforward task to calculate x2, x3, . . . xn. We can then write an equation \nsimilar to that given above:  \n y = a + bx + cx2 + . . . + gxn  \nIf we look at a graph of the powers of x (Fig. 4), we see that adding them in different \nproportions can generate quite complicated curves. \n \n \n \nFig. 5. Complex periodic curve. The upper diagram shows how sine waves can be combined to \napproximate even awkward shapes, such as a square wave. A single wave offers a first approximation, \nwhich can be improved by combining it with appropriately weighted harmonics, shown individually in \nthe lower diagram. \n \nMany natural sequences are periodic, retracing a sequence again and again, like \nrotations of the Earth around the Sun. This type of sequence can be mimicked \nmathematically by a series in which, instead of an x value increasing along a line, we \ntake an angle \u03b8 measured out by a radius rotating around a circle from 0 to 360o \nLoudon, T.V., 2000. Geoscience after IT: Part F  (postprint, Computers & Geosciences, 26(3A)) \nrepeatedly. As it rotates, the sine of the angle \u03b8 changes from 1 to 0 to -1 and back \nagain. A complex periodic curve (see Fig. 5) can be generated by taking, not powers \nof x, but sines of multiples of the angle \u03b8: \n y = a + bsin\u03b8  + csin2\u03b8 + . . . + gsinn\u03b8 \nThe power series and the sine and cosine series have the mathematical property that \nsuccessive terms have a smaller influence as the series continues. They are therefore \nsuitable for approximating to an arbitrary curve. The slope and curvature at any point \nare readily calculated from the equations, and the form of the power series is suited to \nstatistical calculation. They are not appropriate, however, for extrapolating the \nrelationship beyond the data points. The periodic curve repeats itself indefinitely, and \nthe power series heads off to infinity. \n \nStatistical tests (to which power series are well suited) can measure the \u201cgoodness of \nfit\u201d, that is, how well the curve fits the data, compared with expectations from a \nrandom relationship. The test is based on a number of assumptions, notably that a \nrandom sample has been obtained from an underlying distribution that is close to the \nnormal, bell-shaped, curve. It follows that the sample is expected to be drawn from a \nsingle, homogeneous population. There are situations where we suspect that this is not \nthe case. Our sample could have been drawn, without our knowing it, from \npopulations that were formed in different ways by different processes, and they might \nhave quite different properties. It is convenient, therefore, to have a means of \nsearching for different groups within the dataset.  \n \nCluster analysis does this by looking for the most similar items in a dataset, \ncombining them as one item, looking for the next most similar items, and so on until \nall the items in the dataset are combined. The similarity of two items, or its opposite, \ncan be measured in various ways, such as the distance between them in standardized \nvariate space. If the dataset is homogeneous, the clustering will proceed uniformly. If \nthere are a number of natural groups or clusters, then the clustering is more likely to \nproceed with sudden breaks. Closely similar items are brought together, followed by a \nbreak before the next most similar items are found. This is a hierarchical process with \nclusters of clusters amalgamating as bigger clusters. Further examination of the \ncharacteristics of each cluster may suggest why they fall into groups (different \nspecies, different environments, different weathering, and so on). Cluster analysis can \npoint to the existence of non-homogeneous populations, and lead to better analysis. If \nit is known before the investigation that several groups are present, discriminant \nanalysis (see Davis, 1973) provides equations for assigning new items to appropriate \ngroups. Techniques of this kind are used in numerical taxonomy, where \nmeasurements of sets of properties are the basis for classification. They may not be \nappropriate where objects are classified on the basis of an underlying qualitative \nmodel, as is often the case in geoscience (J 2.3). \n \nMost multivariate techniques can simply be regarded as arithmetic transformations of \na set of numbers, and do not necessarily require any underlying assumptions. The \nresults may suggest ideas to a geoscientist who can then proceed to test them by other \nmeans. Calculating the descriptive statistics is then purely an exploratory exercise. \nVisualization, by displaying patterns through interactive graphics, follows this \napproach (see Cleveland, 1993). However, statistical tests of significance, and indeed \nany conclusions that depend on the numbers themselves, almost certainly imply some \nassumptions. Perhaps the most important and the most difficult requirement is to \nLoudon, T.V., 2000. Geoscience after IT: Part F  (postprint, Computers & Geosciences, 26(3A)) \nLoudon, T.V., 2000. Geoscience after IT: Part F  (postprint, Computers & Geosciences, 26(3A)) \nensure that the items recorded (the sample) are truly representative of the population \nabout which the conclusions are drawn. This applies of course not just to quantitative \nmeasurements but to any observation of the natural world. There is, however, a \ndanger that in the course of carrying out the complex manipulations of the data, \noriginal constraints and limitations are forgotten. The subject matter is all important. \n \n6.References \n \nCleveland, W.S., 1993. Visualizing Data. Hobart Press, Summit, New Jersey, 360pp. \n \nCook, R. D., 1998. Regression Graphics: Ideas for Studying Regressions through \nGraphics. Wiley, New York. 349 p. \n \nDavis, John C., 1973. Statistics and Data Analysis in Geology: with Fortran \nPrograms. Wiley, New York, 550pp. \n \nGallagher, R.S. (Ed), 1995. Computer Visualization, Techniques for Scientific and \nEngineering Analysis. CRC Press, Boca Raton, 312pp. \n \nGriffiths, J. C., 1967. Scientific Method in Analysis of Sediments. McGraw-Hill, New \nYork, 508pp. \n \nKrumbein, W.C., Graybill, F.A., 1965. An Introduction to Statistical Models in \nGeology. McGraw-Hill Inc., New York, 475pp. \n \nReyment, R.A., J\u00f6reskog, K.G. (Eds.), 1993. Applied Factor Analysis in the Natural \nSciences. Cambridge University Press, New York, 371pp. \n \nSwan, A.R.H., Sandilands, M., 1995. Introduction to Geological Data Analysis. \nBlackwell Science, Oxford, 446pp. \n \nTukey, J.W., 1977. Exploratory Data Analysis. Addison-Wesley, Reading, Mass., \n499pp. \n \nWendebourg, J., Harbaugh, J.W., 1997. Simulating Oil Entrapment in Clastic \nSequences. Computer Methods in the Geosciences, 16. Pergamon, Oxford, 199pp. \n \n \n \n \n \nDisclaimer: The views expressed by the author are not necessarily those of the British \nGeological Survey or any other organization. I thank those providing examples, but should \npoint out that the mention of proprietary products does not imply a recommendation or \nendorsement of the product. \n \n \n<<<Back to Table of Contents       \nOn to Part G: Familiarization with spatial analysis>>> \n \n"}