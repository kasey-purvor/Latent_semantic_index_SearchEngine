{"doi":"10.1109\/CBMI.2008.4564926","coreId":"102798","oai":"oai:epubs.surrey.ac.uk:2343","identifiers":["oai:epubs.surrey.ac.uk:2343","10.1109\/CBMI.2008.4564926"],"title":"Dynamic layout of visual summaries for scalable video","authors":["Calic, J","Mrak, M","Kondoz, A"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2008","abstract":"The paper brings a novel method for generating visual\\ud\nsummaries of scalable videos. The generated summaries can\\ud\ndynamically adapt to requirements defined by display size,\\ud\nuser's needs or channel limitations. It utilises compressed\\ud\ndomain features coupled with efficient contour evolution\\ud\nalgorithm in order to generate a scale space of temporal\\ud\nvideo descriptors. The layout of the visual summary is created using an efficient graph clustering technique and a fast discrete optimisation algorithm, enabling dynamic video\\ud\nsummarisation in real-time. The experimental results show\\ud\ngood scalability of the dynamic layout and highly efficient\\ud\ngeneration of visual summaries","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:epubs.surrey.ac.uk:2343<\/identifier><datestamp>\n      2017-10-31T14:05:13Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><setSpec>\n      6469766973696F6E733D656E67616E64706879736963616C736369656E636573:656C656374726F6E6963656E67696E656572696E67:63637372<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/epubs.surrey.ac.uk\/2343\/<\/dc:relation><dc:title>\n        Dynamic layout of visual summaries for scalable video<\/dc:title><dc:creator>\n        Calic, J<\/dc:creator><dc:creator>\n        Mrak, M<\/dc:creator><dc:creator>\n        Kondoz, A<\/dc:creator><dc:description>\n        The paper brings a novel method for generating visual\\ud\nsummaries of scalable videos. The generated summaries can\\ud\ndynamically adapt to requirements defined by display size,\\ud\nuser's needs or channel limitations. It utilises compressed\\ud\ndomain features coupled with efficient contour evolution\\ud\nalgorithm in order to generate a scale space of temporal\\ud\nvideo descriptors. The layout of the visual summary is created using an efficient graph clustering technique and a fast discrete optimisation algorithm, enabling dynamic video\\ud\nsummarisation in real-time. The experimental results show\\ud\ngood scalability of the dynamic layout and highly efficient\\ud\ngeneration of visual summaries.<\/dc:description><dc:date>\n        2008<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/epubs.surrey.ac.uk\/2343\/1\/SRF002381.pdf<\/dc:identifier><dc:identifier>\n          Calic, J, Mrak, M and Kondoz, A  (2008) Dynamic layout of visual summaries for scalable video   Proccedings of International Workshop on Content-Based Multimedia Indexing.  pp. 46-50.      <\/dc:identifier><dc:relation>\n        http:\/\/ieeexplore.ieee.org\/search\/srchabstract.jsp?tp=&arnumber=4564926&queryText%3DDynamic+layout+of+visual+summaries+for+scalable+video%26openedRefinements%3D*%26searchField%3DSearch+All<\/dc:relation><dc:relation>\n        10.1109\/CBMI.2008.4564926<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/epubs.surrey.ac.uk\/2343\/","http:\/\/ieeexplore.ieee.org\/search\/srchabstract.jsp?tp=&arnumber=4564926&queryText%3DDynamic+layout+of+visual+summaries+for+scalable+video%26openedRefinements%3D*%26searchField%3DSearch+All","10.1109\/CBMI.2008.4564926"],"year":2008,"topics":[],"subject":["Article","PeerReviewed"],"fullText":"The work presented was developed within VISNET II, a European Network of \nExcellence, funded under the European Commission IST FP6 programme. \nDYNAMIC LAYOUT OF VISUAL SUMMARIES FOR SCALABLE VIDEO \n \nJanko \u0106ali\u0107, Marta Mrak and Ahmet Kondoz \n \nI-Lab, Centre for Communication System Research \nUniversity of Surrey, Guildford, United Kingdom \n{j.calic, m.mrak, a.kondoz}@surrey.ac.uk \n \n \nABSTRACT \n \nThe paper brings a novel method for generating visual \nsummaries of scalable videos. The generated summaries can \ndynamically adapt to requirements defined by display size, \nuser's needs or channel limitations. It utilises compressed \ndomain features coupled with efficient contour evolution \nalgorithm in order to generate a scale space of temporal \nvideo descriptors. The layout of the visual summary is cre-\nated using an efficient graph clustering technique and a fast \ndiscrete optimisation algorithm, enabling dynamic video \nsummarisation in real-time. The experimental results show \ngood scalability of the dynamic layout and highly efficient \ngeneration of visual summaries. \n \nIndex Terms\u2014 discrete optimisation, video analysis, \nvideo summarisation, scalable video coding \n \n1. INTRODUCTION \n \nBeing one of the central requirements of Future Internet \nMultimedia, real-time interactivity and responsiveness of \nthe multimedia interfaces is a prime goal in multimedia \nmanagement research. Nowadays, user experience is limited \nto a rather unidirectional delivery of temporal media, deliv-\nered over the Internet or stored locally. This problem emer-\nges especially in the case of large data repositories. In order \nto facilitate intuitive and responsive interaction with large \nmultimedia collections, the issues of system efficiency and \nusability need to be addressed. \nThe work presented in this paper introduces an efficient \nsystem for large-scale video summarisation that exploits \ncompressed-domain analysis of scalable video. Aimed at \nresponsive and intuitive browsing interfaces for large video \ndatabases, the system generates visual representation of \nvideo data in a form of a comic-like summary with low \nlatency, thus making a shift towards more user-centred \nsummarisation and browsing of large video collections by \naugmenting user's interaction with the content rather than \nlearning the way users create related semantics. \n \nIn generating the visual summaries, presented algorithm \nfollows the narrative structure of comics, linking the tempo-\nral flow of video sequence with the spatial position of panels \nin a comic strip. This approach differentiates our work from \nthe typical reverse storyboarding [1] or video summarisation \napproaches. Although there have been attempts to utilise the \nform of comics as a medium for visual summarisation of \nvideos [2] [3], the high complexity of these algorithms hind-\nered summarisation at the larger scale or with low process-\ning latency. In order to overcome the demanding complexity \nconstraints, the proposed system utilises video analysis al-\ngorithm that uses compressed-domain hierarchical motion \ninformation, coupled with a fast discrete optimisation algor-\nithm for creation of the comic-like layout of extracted key-\nframes. Avoiding full video decoding, the analysis algor-\nithm decompresses only the motion information from tar-\ngeted temporal decomposition level of the scalable video. \nThe motion activity metric is chosen for capturing intensity \nof action since it is highly correlated to human perception \n[4]. Using a fast and robust geometrical curve simplification \nalgorithm, a set of most representative key-frames is ex-\ntracted as a visual summary of the analysed video. In order \nto generate an intuitive and yet compact video browsing \ninterface, our approach introduces a novel solution based on \ndynamic programming (DP). In addition, the presented al-\ngorithm applies a new approach to the estimation of key-\nframe sizes in the final layout by exploiting an efficient \ngraph clustering methodology coupled with a specific cost \nfunction that balances between good content representability \nand discovery of unanticipated content. The evaluation re-\nsults compared to existing methods of video summarization \nshowed substantial improvements in terms of algorithm ef-\nficiency. \nThe utilisation of scalable video coding technology is \npresented in Section 2 followed by the description of an \nalgorithm for fast selection of the most representative key-\nframes. Section 3 introduces a novel method for forming of \nvideo summaries that optimises generated visual representa-\ntion to available spatial resources. The results of the algor-\nithms presented are given in Section 4, followed by the final \nconclusions. \n \n978-1-4244-2044-5\/08\/$25.00 c\u00a92008 IEEE. 46 CBMI 2008\nAuthorized licensed use limited to: University of Surrey. Downloaded on May 17,2010 at 15:06:19 UTC from IEEE Xplore.  Restrictions apply. \n2. COMPRESSED DOMAIN KEY-FRAME \nEXTRACTION \n \nThe proposed video analysis is performed utilising a motion \nactivity measure. The motion activity descriptor is a stan-\ndard tool for capturing intensity of action correlated to hu-\nman perception [4]. In our work, the extraction of motion \nactivity descriptor is performed from the layered bit-streams \nin scalable video. \nThe algorithmic complexity of the activity measure \ncomputation is very low due to its direct extraction form the \ncompressed domain data. The motion information available \nfrom the compressed video is obtained using only partial \ndecoding. The final selection of key-frames is based on ev-\naluation of the motion activity metric, comparing the values \nof the metric for neighbouring frames and the overall video \nsequence activity. \n \n2.1. Compressed domain video analysis \n \nThe activity metric for each frame is extracted from associ-\nated motion information generated in the process of video \ncoding. Decoding of motion information without complete \nvideo decoding is a low-complexity process, which can be \nfurther simplified, if the layered video, such as scalable \ncoded video [5] [6] [7], is used. This is due to the underlying \nhierarchical layered structure of the compressed stream. For \nbit-streams that are encoded using motion compensated \ntemporal filtering [5] the motion information can be ob-\ntained for each compensated frame from different levels of \ntemporal filtering. Lower temporal bit-stream layers consist \nof the data related to more distant frames. While in the con-\ntext of compression the coding efficiency drops when the \nframes are more distant, those frames are still close enough \nfor analysis. Therefore the analysis for key-frame selection \ncan be performed from lower bit-stream layers.  \nIn order to obtain the motion activity metric for the ob-\nserved frame, a variance of magnitude of motion vectors is \ncalculated for all temporal prediction modes. For bidirec-\ntional prediction the motion vector magnitudes are averaged \nand treated as the magnitudes for unidirectional prediction. \nThe overall activity measure, \u03b1t for a frame at time position t \nis computed for all motion compensated frames at the low-\nest accessible bit-stream layer which is then used in key-\nframe selection algorithm described in the following section. \n \n2.2. Discrete contour evolution and key-frame extraction \n \nIn order to generate a scalable temporal descriptor that fa-\ncilitates dynamic extraction of key-frames from the se-\nquence, the activity metric needs to be simplified in a way \nthat spurious and small changes are discarded without any \ninfluence to the main features of the metrics curve. A \nmethod called Discrete Curve Evolution (DCE) [8] effi-\nciently achieves this requirement: it leads to the simplifica-\ntion of curve complexity with no peak rounding effects and \nno dislocation of relevant features. The curve evolution pro-\ncess is guided by a relevance measure K, which is stable \nwith respect to noisy deformations, and is given as: \n \n Ki = |(ai - ai - 1) \u00b7 (ti - ti - 1)| + |(ai + 1 - ai) \u00b7 (ti + 1 - ti)|, (1) \n \nwhere ai are motion activity values at time indices ti at the \nparticular DCE simplification stage. Initial values of ai cor-\nrespond to \u03b1i, i.e. to the motion activity metrics for each \nanalysed frame. The relevance measure Ki is proportional to \na change of area below the motion activity curve caused by \nthe removal of the point i on the curve. At each stage of \nsimplification the relevance measure Ki is iteratively up-\ndated. The optimal complexity level of the temporal descrip-\ntor is calculated as:  \n \nKopt =  \u2013 log(\u03ba) \u22c5 \u03c3K (2) \n \nwhere is the mean of all K at different simplification \nstages, \u03c3K is its standard deviation, while the parameter \u03ba \ncontrols the sensitivity of the event detection and is driven \nby the application requirements. Key-frame positions are \ndetermined by the local minima in the temporal descriptor at \nthe scale where K equals Kopt. Being located at the local \ntroughs of motion activity, the key-frames will have maxi-\nmum probability of avoiding motion blur and other artefacts \ndue to object motion or camera work. In addition, the most \nrepresentative information will be conveyed by the key-\nframes in areas with no camera work, since the cameraman \ntends to focus on the main object of interest using a static \ncamera. In case the level of detail required cannot be \nachieved by using DCE simplification, i.e. initial cost of \nDCE simplification is too high, algorithm switches to a \nhigher layer of scalable video and generates more detailed \nmetric for a given section of video. \n \n3. GRAPH BASED FRAME CLUSTERING \n \nHaving the user's experience at the centre of our browsing \ninterface design task, our main aim is to generate an intui-\ntive video summary by conveying the significance of a shot \nfrom analysed videos via the size of its key-frame represen-\ntation. In our case, the objective is to clearly present visual \ncontent that is dominant throughout the analysed section of \nthe video, as well as to highlight some unanticipated con-\ntent. \nFollowing the approach described above, where the size \nof a key-frame represents its summarisation significance, a \ncost function C(i) that represent the desired frame size in the \nfinal layout is generated, where C(i) \u2208 [0, 1] for i = 1,..., N,  \nand N is the number of extracted key-frames for a given \nsequence. In order to evaluate the cost function in a way that \nwill support the user\u2019s visual experience of the final layout, \nthe clustering based on perceptual similarity is used. More \n47\nAuthorized licensed use limited to: University of Surrey. Downloaded on May 17,2010 at 15:06:19 UTC from IEEE Xplore.  Restrictions apply. \nspecifically, an efficient graph based clustering method [9] \nthat utilises 18 \u00d7 3 \u00d7 3 HSV colour histogram is applied. \nThis approach enables unsupervised analysis of inherent \nstructure of the key-frame data and it copes well with non-\nlinearity of cluster shapes. The exploited graph-clustering \nalgorithm defines a predicate for measuring the evidence for \na boundary between two clusters using a graph-based repre-\nsentation of the image set. Nodes in the constructed graph \nare the extracted key-frame images, while their histogram \ndifference values are assigned to graph edges. An important \ncharacteristic of the method is its ability to preserve detail in \nlow-variability clusters while ignoring detail in high-\nvariability image sets. This algorithm runs in time nearly \nlinear to the number of graph edges, and though we have \ntaken into consideration fully connected image set, due to \nrelatively small number of images the processing is very \nfast. \n The main objective in the process of analysing the inher-\nent structure of the key-frame dataset is to avoid data de-\npendent parameterisation, achieve low algorithm complexity \nand cope well with high non-linearity of underlying data \nclusters. In our previous work, we have utilised a specific \nunsupervised spectral clustering approach [10], but the high \ncomplexity of the eigen-decomposition stage of the affinity \nmatrix hindered overall efficiency for large datasets.  \n To achieve algorithm complexity nearly linear to the \nnumber of key-frames, a specific graph based clustering \nalgorithm is utilised [9]. Although initially formulated in the \nimage segmentation context, this algorithm can be extended \nto a more generic dataset scenario. Its ability to preserve \ndetail in low-variability clusters while ignoring detail in \nhigh-variability regions maintains notion of global features \nof the dataset in the process of making greedy decisions \nlocally.  \n Following a common approach to graph based image \nclustering, this method forms edges of a graph G=(V,E), \nwhere each image corresponds to a node vi\u2208V in the graph, \nand certain images are connected by undirected edges \n(vi,vj)\u2208E. Weights on each edge w(vi,vj) measure the dis-\nsimilarity between the two corresponding images.  \n The graph node grouping is defined by a graph predicate \nD(c1,c2), which evaluates if the two regions c1 and c2 should \nstay disconnected by comparing inter and intra regional dif-\nferences: \n \n \n\u20ac \nD(c1 ,c2) :Ext (c1,c2) > mInt(c1,c2)  (3) \n \nThe internal difference of a component c is defined as the \nlargest weight in the minimum spanning tree of the compo-\nnent. The joint internal difference measure mInt(c1,c2) is \ngiven as: \n \n \n\u20ac \nmInt = min(Int (c1) +\u03c4 (c1),Int (c2) +\u03c4 (c2))  (4) \n \nThe threshold function \u03c4(c)=k\/|c|, where k is some constant \nparameter and |c| denotes the size of c, controls the degree to \nwhich the difference between the two components must be \ngreater than their internal differences. The intra component \ndifference is defined as the minimal weight edge connecting \nthe two components. The technique adaptively adjusts the \nmerging criterion based on the degree of variability in \nneighbouring regions of the dataset.  \n The node grouping is iteratively repeated until there is \nno more component merging. In order to represent the \ndominant content of the selected section of video, each \ncomponent is represented with a frame closest to the com-\nponent\u2019s centre of the mass. Therefore the highest cost func-\ntion C(i)=1 is assigned for d=0, where d is the distance of \nthe key-frame closest to the centre of component and \u03c3i is ith \nframe's component variance. Other members of the compo-\nnent are given values: \n \n \n\u20ac \nC (i) =\u03b1 \u22c5 1\u2212 e\n\u2212\nd( i)2\n2\u22c5\u03c3i2\n\uf8eb \n\uf8ed \n\uf8ec \n\uf8ec \n\uf8f6 \n\uf8f8 \n\uf8f7 \n\uf8f7 \n\u22c5hmax  (5) \n \nThe cost function is scaled to have a maximum value hmax in \norder to be normalised to frame sizes available in the final \nlayout. Parameter \u03b1 can take values \u03b1\u2208[0,1], and in our case \nis chosen empirically to be 0.7. In Figure 1, a range of dif-\nferent cost dependency curves are depicted for values \n\u03b1\u2208{0.5-1.0} and hmax=1. The value of \u03b1 controls the bal-\nance between the importance of the cluster centre and the \noutliers.  \n \n \n \nFig. 1. Importance measure assigned to the frames depend-\ning on their distance from the cluster centre \n \nBy doing this, cluster outliers (i.e. cutaways, establishing \nshots, etc.) are presented as more important and attract more \nattention of the user than key-frames concentrated around \nthe cluster centre. This grouping around the cluster centres \nis due to common repetitions of similar content in raw video \nrushes, often adjacent in time. To avoid the repetition of \ncontent in the final summary, a set of similar frames is rep-\nresented by a larger representative, while the others are as-\nsigned a lower cost function value. \n48\nAuthorized licensed use limited to: University of Surrey. Downloaded on May 17,2010 at 15:06:19 UTC from IEEE Xplore.  Restrictions apply. \n \n4. GENERATING VISUAL SUMMARY \n \nThe main task of the layout module is to generate a \nvisual summary that optimally follows the values of the cost \nfunction by using only frame sizes available in comic-like \npanel templates. The precision of approximation depends \nupon the maximum height of a panel hmax, which gives \ngranularity of the solution. For a given hmax, a set of panel \ntemplates is generated, assigning a vector of frame sizes to \neach template.  \nFor a given cost function C(i), there will be a finite num-\nber of frame-size values \u0398(i) in the final layout. An example \nof a single-row layout approximation for hmax=4 is depicted \nin Figure 2, comparing the values of the cost function C(i) \nwith the achieved values of frame sizes \u0398(i). The templates \nfollow the narrative structure of a comic book, while main-\ntaining the original aspect ratio of images forming the panel \n[10]. \n \nFig. 2. Approximation of the cost function C(i) by frame \nsizes \u0398(i) in the final layout for a single row layout. \n \nSince the aim here is to optimally utilise the available \nspace given the required sizes of images, this is a problem of \ndiscrete optimisation. However, unlike thoroughly explored \ndiscrete optimisation methods like stock cutting or bin pack-\ning [11], there is a non-linear transformation layer of panel \ntemplates between the error function and available re-\nsources. Therefore, a sub-optimal solution using dynamic \nprogramming is proposed. It follows a typical structure of \nthe DP algorithm by efficiently finding the solution to an \noptimisation problem in case the variables in the evaluation \nfunction are not interrelated simultaneously. Although there \nis an indirect dependency between non-adjacent panels due \nto the fact that the width of the last panel is directly depend-\nent upon the sum of widths of previously used panels, by \nintroducing specific corrections to the DP error function \n[12] the sub-optimal solution often achieves optimal results. \nThis correction assigns additional cost if the layout needs \nresizing in order to fit to the required width. \n \n4. EXPERIMENTAL RESULTS \n \nThe evaluation experiments were conducted using the \nTRECVID 2006 evaluation content. This content is pro-\nvided as the benchmarking material for evaluation of video \nretrieval systems. The videos were transcoded to scalable \nvideo format, generating 5 temporal layers and using 4 lev-\nels of temporal filtering. Therefore the motion information \nassociated to the lowest temporal level corresponds to every \n16th (24) frame. Motion activity metrics computed from this \nlowest layer are used to initialise the DCE stage of the key-\nframe selection algorithm.  \nThe results depicted in the Figure 3 represent a scale \nspace at four stages of the DCE simplification process, ap-\nplied to the motion activity metric \u03b10 which corresponds to \nthe video bit-stream layer with the lowest available frame-\nrate (top of Figure 3). The frame numbers in Figure 3 are \nrelated to the original sequence while the number of actual \nsamples of the motion activity metric is 16 times lower. \nGradual removal of less important features of the metric \ncurve is performed using DCE, Figure 3, where i in ai de-\nnotes the DCE simplification stage. The bottom curve a3 \nfrom Figure 3 is used to select the key-frames at the loca-\ntions of curve troughs. The key-frames are then used for \nmodelling of a summary. \n \n \nFig. 3. Scale space of the temporal descriptor generated by \nthe DCE algorithm for the sequence summaries in Figure 2. \nAn example of the final layout obtained from the lay-\nered bit-streams using proposed layout algorithm is pre-\nsented in Figure 5. The results are obtained from the same \nset of key-frames targeting different heights of panels. One \ncan observe that the spatial compression of the layouts de-\npends on selected height hmax, since the larger hmax values \ntend to allow more compact representations.  \nWithout loosing the notion of temporal structure as well \nas representing every detail of the content, the resulting \nvideo summary achieves spatial compression ration of 2 \/ 5 \nwhile producing visually pleasant experience for the user. \nThe layout algorithm complexity is evaluated by comparing \nthe processing speed with the methods that utilise comic-\nlike narrative structure in video summarisation, which are \npresented in [2] and [3]. Firstly, both methods have been \nproved unfeasible for summaries with more than 100 \nframes. The layout processing times for of the algorithms \npresented in [2] TOR and in [3] TFS, compared to the pro-\nposed method TDP are numerically given in Table 1. From \nthe results shown it can be observed that the utilised method \n49\nAuthorized licensed use limited to: University of Surrey. Downloaded on May 17,2010 at 15:06:19 UTC from IEEE Xplore.  Restrictions apply. \nachieves linear complexity, as depicted in Figure 4. There-\nfore, this method proves to be suitable for fast summarisa-\ntion targeting responsive browsing interfaces. \n \nFig. 4. Dependency of the layout duration TDP on the num-\nber of frames in the summary \nTable 1. Layout duration in seconds for a given number of \nframes N. \nN 25 75 125 150 1000 2500 \nTOR 0.03 0.16 1.8 X X X \nTFS 0.03 0.57 200 X X X \nTDP 0.04 0.13 0.32 0.44 1.07 4.20 \n \n5. CONCLUSIONS \n \nThe experimental results show that the utilisation of the lay-\nered bit-stream and compressed domain motion features, \ncoupled with a fast and robust curve simplification, the sys-\ntem efficiently extracts the most representative set of key-\nframes. Exploiting the narrative structure of comics and \nusing its well-known intuitive rules, visual summaries are \ngenerated in a user centred way. Not only does this ap-\nproach improves the processing time of the summarisation \ntask, but it enables new functionalities of visualisation for \nlarge-scale video archives, such as real-time interaction and \nrelevance feedback. \n \n6. REFERENCES \n \n[1] R. Dony, J. Mateer, J. Robinson, \"Techniques for automated \nreverse storyboarding,\" IEE Proc. Vision, Image and Signal \nProcessing, Vol. 152, No. 4, pp. 425 - 436, 2005. \n[2] S. Uchihashi, J. Foote, A. Girgensohn, J. Boreczky, \"Video \nmanga: generating semantically meaningful video sum-\nmaries,\" Proc. 7th ACM Int'l Conference on Multimedia, pp. \n383 - 392, 1999. \n[3] A. Girgensohn, \"A fast layout algorithm for visual video \nsummaries,\" Proc Int'l Conference on Multimedia and Expo, \nVol. 2, pp. 77 - 80, 2003. \n[4] Video Mining, A Rosenfeld, D. Doermann, D. DeMenthon \n(Editors), Kluwer Academic Publishers, 2003. \n[5] N. Adami, A. Signoroni, R. Leonardi, \"State-of-the-art and \ntrends in scalable video compression with wavelet-based ap-\nproaches,\" IEEE Trans. on Circ. and Sys. for Video Tech., \nVol. 17, Iss. 9, pp. 1238 - 1255, Sept. 2007. \n[6] M. Mrak, N. Sprljan, E. Izquierdo, \"Motion estimation in \ntemporal subbands for quality scalable motion coding,\" Elec-\ntronics Letters, No. 41, pp. 1050 \u2013 1051, 2005. \n[7] H. Schwarz, D. Marpe, T. Wiegand, \"Overview of the scalable \nvideo coding extension of the H.264\/AVC standard,\" IEEE \nTrans. on Circ. and Sys. for Video Tech., Vol. 17, Iss. 9, pp. \n1103 - 1120, Sept. 2007. \n[8] L. J. Latecki, R. Lakamper, \"Convexity rule for shape decom-\nposition based on discrete contour evolution,\" Computer Vi-\nsion and Image Understanding, Vol. 73, pp. 441\u2013454, 1999. \n[9] P. F. Felzenszwalb and D. P. Huttenlocher, \"Efficient Graph-\nBased Image Segmentation\", International Journal of Com-\nputer Vision, Volume 59, Number 2, September 2004. \n[10] J. Calic and N. W. Campbell, \"Compact Visualisation of \nVideo Summaries,\" EURASIP Journal on Advances in Signal \nProcessing, vol. 2007, Article ID 19496, 2007. \n[11] A. Lodi, S. Martello, M. Monaci, \u201cTwo-dimensional packing \nproblems: A survey,\" European Journal of Operational Re-\nsearch, Vol. 141, Iss. 2, pp. 241 - 252, 2002. \n[12] J. Calic, D. P. Gibson, and N. W. Campbell, \"Efficient layout \nof comic-like video summaries,\" IEEE Trans. on Circ. and \nSys. for Video Tech., Vol. 17, Iss. 7, pp. 931 - 936, July 2007. \n \n \nhmax = 1 \nhmax = 2 \nhmax = 3 \nhmax = 4  \na) Resulting layouts using panels with different heights \n \nb) Enlarged layout from hmax = 3 \nFig. 5. Resulting video summaries for different panel heights. \n50\nAuthorized licensed use limited to: University of Surrey. Downloaded on May 17,2010 at 15:06:19 UTC from IEEE Xplore.  Restrictions apply. \n"}