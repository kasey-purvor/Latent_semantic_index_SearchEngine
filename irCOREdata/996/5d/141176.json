{"doi":"10.1109\/TASE.2009.2038170","coreId":"141176","oai":"oai:dspace.lib.cranfield.ac.uk:1826\/6877","identifiers":["oai:dspace.lib.cranfield.ac.uk:1826\/6877","10.1109\/TASE.2009.2038170"],"title":"Health-state estimation and prognostics in machining processes","authors":["Camci, Fatih","Chinnam, R. B."],"enrichments":{"references":[{"id":37928697,"title":"A model for reasoning about persistence and causation.","authors":[],"date":"1989","doi":"10.1111\/j.1467-8640.1989.tb00324.x","raw":"T. Dean and K. Kanazawa, A model for reasoning about persistence and causation. Artificial Intelligence, 93(1\u20132), 1\u201327, 1989","cites":null},{"id":37928678,"title":"A novel approach to equipment health management based on auto-regressive hidden semi-Markov model (AR-HSMM), Science In","authors":[],"date":"2008","doi":"10.1007\/s11432-008-0111-4","raw":"M. Dong, A novel approach to equipment health management based on auto-regressive hidden semi-Markov model (AR-HSMM), Science In China Series F-Information Sciences , 51(9) (2008), pp. 1291-1304","cites":null},{"id":37928663,"title":"A novel approach to fault diagnostics and prognostics,","authors":[],"date":"2003","doi":"10.1109\/robot.2003.1241660","raw":"C. Kwan, X. Zhang, R. Xu, L. Haynes, A novel approach to fault diagnostics and prognostics, Proceedings of ICRA '03: IEEE International Conference on Robotics and Automation. 1(3), September 2003, 604-609","cites":null},{"id":37928677,"title":"A segmental hidden semi-Markov model (HSMM) -based diagnostics and prognostics framework and methodology,","authors":[],"date":"2007","doi":"10.1016\/j.ymssp.2006.10.001","raw":"M. Dong, D. He, A segmental hidden semi-Markov model (HSMM) -based diagnostics and prognostics framework and methodology, Mechanical Systems and Signal Processing, 21 (2007), 2248-2266","cites":null},{"id":37928686,"title":"A summary of methods applied to tool condition monitoring in drilling,","authors":[],"date":"2002","doi":"10.1016\/s0890-6955(02)00040-8","raw":"E. Jantunen, A summary of methods applied to tool condition monitoring in drilling, International Journal of Machine Tools & Manufacture, 42 (2002), pp. 997-1010","cites":null},{"id":37928688,"title":"A tutorial on hidden Markov models and selected applications in speech recognition,","authors":[],"date":"1989","doi":"10.1109\/5.18626","raw":"L.R. Rabiner, A tutorial on hidden Markov models and selected applications in speech recognition, Proceedings of IEEE, 1989, 77, 257-285.","cites":null},{"id":37928695,"title":"A Unifying Review of Linear Gaussian Models,","authors":[],"date":"1999","doi":"10.1162\/089976699300016674","raw":"S. Roweis & Z. Ghahramani, A Unifying Review of Linear Gaussian Models, Neural Computation 11(2), 1999, 305-345","cites":null},{"id":37928671,"title":"Advanced diagnostics and prognostics for gas turbine risk assessment,","authors":[],"date":"2000","doi":"10.1109\/aero.2000.877909","raw":"M.J. Roemer and G.J. Kacprzynski, Advanced diagnostics and prognostics for gas turbine risk assessment, Proceedings of the 2000 IEEE Aerospace Conference, Big Sky, Montana, March 18-25, 2000.","cites":null},{"id":37928674,"title":"Al Ani, Condition-based maintenance of machines using hidden","authors":[],"date":"2000","doi":"10.1006\/mssp.2000.1309","raw":"C. Bunks, D. McCarthy, T. Al Ani, Condition-based maintenance of machines using hidden Markov model, Mechanical Systems and Signal Processing, 14(4), July 2000, 597-612","cites":null},{"id":37928676,"title":"Autonomous diagnostics and prognostics in machining processes through competitive learning-driven HMMbased clustering,","authors":[],"date":null,"doi":"10.1080\/00207540802232930","raw":"R. B. Chinnam, P. Baruah, Autonomous diagnostics and prognostics in machining processes through competitive learning-driven HMMbased clustering, International Journal of Production Research, 1-20, in press, iFirst, doi: 10.1080\/00207540802232930","cites":null},{"id":37928679,"title":"Autonomous diagnostics and prognostics through competitive learning driven HMM-based clustering,","authors":[],"date":"2003","doi":"10.1109\/ijcnn.2003.1223951","raw":"R. B. Chinnam, P. Baruah, Autonomous diagnostics and prognostics through competitive learning driven HMM-based clustering, Proceedings of the International Joint Conference on Neural Networks, July 2003, 2466- 2471","cites":null},{"id":37928661,"title":"Autonomous, Process Monitoring, Diagnostics, and Prognostics Using Support Vector Machines and Hidden Markov Models,","authors":[],"date":"2005","doi":null,"raw":"F. Camci, Autonomous, Process Monitoring, Diagnostics, and Prognostics Using Support Vector Machines and Hidden Markov Models, PHD Dissertation, Wayne State University, 2005","cites":null},{"id":37928694,"title":"Belief networks, hidden Markov models, and Markov random fields: a unifying view,","authors":[],"date":"1998","doi":"10.1016\/s0167-8655(97)01050-7","raw":"P. Smyth, Belief networks, hidden Markov models, and Markov random fields: a unifying view, Pattern Recognition Letters 18(11-13), 1998, 1261-1268","cites":null},{"id":37928707,"title":"Covariates and Random Effects in a Gamma Process Model with Application to Degradation and Failure,","authors":[],"date":"2004","doi":"10.1023\/b:lida.0000036389.14073.dd","raw":"J. Lawless and M. Crowder, Covariates and Random Effects in a Gamma Process Model with Application to Degradation and Failure, Journal Lifetime Data Analysis, 10(3), (2004), pp. 213-227","cites":null},{"id":37928711,"title":"Data Fusion for Developing Predictive Diagnostics for Electromechanical Systems,","authors":[],"date":"2001","doi":"10.1201\/9781420038545.ch23","raw":"C.S. Byington and A.K. Garga, Data Fusion for Developing Predictive Diagnostics for Electromechanical Systems, Handbook of Multisensor Data Fusion, D.L. Hall and J. Llinas eds., CRC Press, FL: Boca Raton, 2001.","cites":null},{"id":37928689,"title":"Data-driven extensions to HMM statistical dependencies.","authors":[],"date":"1998","doi":null,"raw":"J. Blimes., Data-driven extensions to HMM statistical dependencies. International Conference on Speech Language Processing, 1998.","cites":null},{"id":37928699,"title":"Dynamic Bayesian Network: Representation, Inference,","authors":[],"date":"2002","doi":null,"raw":"K. Murphy, Dynamic Bayesian Network: Representation, Inference, and Learning, PhD. Dissertation, University of California, Berkeley, 2002.","cites":null},{"id":37928703,"title":"Enhancement of Physics-of-Failure Prognostic Models with System Level Features,","authors":[],"date":"2002","doi":"10.1109\/aero.2002.1036131","raw":"G. J. Kacprzynski et al., Enhancement of Physics-of-Failure Prognostic Models with System Level Features, Proceedings of the 2000 IEEE Aerospace Conference, Big Sky, MT, March 2002.","cites":null},{"id":37928691,"title":"Hidden Markov Models: A Guided Tour,","authors":[],"date":"1988","doi":"10.1109\/icassp.1988.196495","raw":"A.B. Poritz., Hidden Markov Models: A Guided Tour, ICASSP, 1988","cites":null},{"id":37928693,"title":"Hierarchical hidden Markov model, Wikipedia, The Free Encyclopedia,","authors":[],"date":"2008","doi":null,"raw":"Wikipedia Contributors, Hierarchical hidden Markov model, Wikipedia, The Free Encyclopedia, 8 April 2008, 20:59 UTC, <http:\/\/en.wikipedia.org\/w\/index.php?title=Hierarchical_hidden_Mar kov_model&oldid=204300061> [accessed 8 April 2008]","cites":null},{"id":37928680,"title":"HMMs for diagnostics and prognostics in machining processes,","authors":[],"date":"2005","doi":"10.1080\/00207540412331327727","raw":"P. Baruah, P and R.B. Chinnam, R.B., HMMs for diagnostics and prognostics in machining processes, International Journal of Production Research, 43(6), March 2005, 1275-1293","cites":null},{"id":37928732,"title":"Information Theory, Inference and Learning Algorithms,","authors":[],"date":"2003","doi":"10.1017\/s026357470426043x","raw":"D.J.C. MacKay, Information Theory, Inference and Learning Algorithms, 2003 (Cambridge University Press). Fatih Camci is an Assistant Professor in the Department of Computer Engineering at Fatih University, Istanbul Turkey. He worked as senior project engineer at Impact Technologies in Rochester, NY for two years with experience in the development of diagnostic\/prognostic strategies for naval ship systems and military aircraft applications. Camci received his PhD in Industrial Engineering from Wayne State University. He received his B.S. degree in Computer Engineering from Istanbul University (Turkey) and M.S. degree in Computer Engineering from Fatih University (Turkey). He has been involved in development of optimum maintenance scheduling tools and diagnostics\/prognostic methods for naval ship systems. His expertise includes computational intelligence methods such as neural networks, fuzzy logic, support vector machines, etc, and optimization methods such as linear, quadratic optimization, and genetic algorithms, etc. Ratna Babu Chinnam is an Associate Professor in the Department of Industrial & Manufacturing Engineering at Wayne State University (U.S.A.). He received his B.S. degree in Mechanical Engineering from Mangalore University (India) in 1988 and the M.S. and Ph.D. degrees in Industrial Engineering from Texas Tech University (U.S.A.) in 1990 and 1994, respectively. He is the author of over 75 technical publications in the areas of Intelligent Quality Engineering,18 Condition-Based Maintenance Systems, Operations Management, and Computational Intelligence. He is currently the associate editor for the International Journal of Modeling and Simulation. He regularly serves on several international conference planning committees such as IJCNN, ANNIE, and IASTED\u2019s NIC. His past research is mostly funded by such agencies as the National Science Foundation. He carried out extensive collaborative research with Ford Motor Company, DaimlerChrysler, Chrysler, General Dynamics, and consulted for such companies as Sirius, Energy Conversion Devices, and Tecton. He is a member of Alpha Pi Mu, INFORMS, and the North American Manufacturing Research Institute.","cites":null},{"id":37928685,"title":"Knowledge-based diagnosis of drill conditions,","authors":[],"date":"1993","doi":"10.1007\/bf00123967","raw":"S. Y. Hong, Knowledge-based diagnosis of drill conditions, Journal of Intelligent Manufacturig, vol. 4, pp. 233-241 1993","cites":null},{"id":37928683,"title":"Manes Torque control for a form tool drilling operation,","authors":[],"date":"1999","doi":"10.1109\/87.736745","raw":"R. J. Furness, T. Tsao, J. S. Rankin, M. J. Muth, and K. W. Manes Torque control for a form tool drilling operation, IEEE Transactions on Control Systems and Technology, Vol. 7, 1999, pp. 22-30","cites":null},{"id":37928673,"title":"N-channel hidden Markov models for combined stressed speech classification and recognition,","authors":[],"date":"1999","doi":"10.1109\/89.799692","raw":"B.D. Womack, J.H.L. Hansen, N-channel hidden Markov models for combined stressed speech classification and recognition, IEEE Transactions on Speech and Audio Processing, 7(6), Nov 1999, 668-677","cites":null},{"id":37928681,"title":"Neural Network Based Online Detection of Drill Breakage in Micro Drilling Process,","authors":[],"date":"2002","doi":"10.1109\/iconip.2002.1199036","raw":"L. Fu, Ling S., Neural Network Based Online Detection of Drill Breakage in Micro Drilling Process, Proc. 9th International Conference on Neural Information Processing (ICONIP\u201902), 2002 pp. 2054-2058","cites":null},{"id":37928687,"title":"Neural networks : A comprehensive foundation, 2nd Ed., Upper Saddle River,","authors":[],"date":"1999","doi":null,"raw":"S. Haykin, Neural networks : A comprehensive foundation, 2nd Ed., Upper Saddle River, New York: Prentice Hall, 1999.","cites":null},{"id":37928672,"title":"Noise-compensated hidden Markov models,","authors":[],"date":"2000","doi":"10.1109\/89.861372","raw":"I. Sanches, Noise-compensated hidden Markov models, IEEE Transactions on Speech and Audio Processing, 8(5), Sep 2000, 533-540","cites":null},{"id":37928662,"title":"Pervasive Internet Report, Approaching Zero Downtime: The Center for Intelligent Maintenance Systems,","authors":[],"date":null,"doi":null,"raw":"Harbor Research Pervasive Internet Report, Approaching Zero Downtime: The Center for Intelligent Maintenance Systems, April","cites":null},{"id":37928709,"title":"Prognostic Enhancements to Diagnostic Systems for Improved Condition-Based Maintenance,","authors":[],"date":"2002","doi":"10.1109\/aero.2002.1036120","raw":"C.S. Byington, M.J. Roemer, and T. Galie, Prognostic Enhancements to Diagnostic Systems for Improved Condition-Based Maintenance, Proceedings of the 2000 IEEE Aerospace Conference, Big Sky, MT, March 2002.","cites":null},{"id":37928670,"title":"Prognostics, The Real Issues Involved With Predicting Life Remaining,","authors":[],"date":"2000","doi":"10.1109\/aero.2000.877920","raw":"S. J. Engel, B. J. Gilmartin, K. Bongort, A. Hess, Prognostics, The Real Issues Involved With Predicting Life Remaining, IEEE Aerospace Conference Proceedings, 6, 2000, 457-469.","cites":null},{"id":37928675,"title":"Quasi-stationary distributions for stochastic processes with an absorbing state,","authors":[],"date":"2002","doi":"10.1088\/0305-4470\/35\/5\/303","raw":"R. Dickman, R. Vidigal, Quasi-stationary distributions for stochastic processes with an absorbing state, Journal of Physics A: Mathematical and General, (35), 2002, 1147-1166","cites":null},{"id":37928682,"title":"Real Time Monitoring of Tool Wear Using Multiple Modeling Method,","authors":[],"date":"2001","doi":"10.1109\/iemdc.2001.939388","raw":"H. M. Ertunc, K. A. Loparo, E. Ozdemir, H. Ocak, Real Time Monitoring of Tool Wear Using Multiple Modeling Method, Proc. IEEE International Conference Electric Machines and Drives, 2001. IEMDC 2001. pp. 687- 691","cites":null},{"id":37928684,"title":"Sensing of drill wear and prediction of drill life,","authors":[],"date":"1977","doi":"10.1115\/1.3439211","raw":"K. Subramanian and N. H. Cook, Sensing of drill wear and prediction of drill life, ASME Journal of Engineering for Industry, Vol. 99, pp. 295-301, 1977","cites":null},{"id":37928715,"title":"Stochastic Modeling of Fatigue Crack Dynamics for Online Failure Prognostics,","authors":[],"date":"1996","doi":"10.1109\/87.508893","raw":"A. Ray, S. Tangirala, Stochastic Modeling of Fatigue Crack Dynamics for Online Failure Prognostics, IEEE Transactions on Control Systems Technology, 4(4), July 1996 443-451.","cites":null},{"id":37928701,"title":"The Bayes Net Toolbox for","authors":[],"date":"2001","doi":null,"raw":"K. P. Murphy, The Bayes Net Toolbox for Matlab, Computing Science and Statistics, 33, 2001.","cites":null},{"id":37928692,"title":"The hierarchical hidden Markov model: Analysis and applications,","authors":[],"date":"1998","doi":null,"raw":"S. Fine, Y. Singer, and N. Tishby, The hierarchical hidden Markov model: Analysis and applications, Machine Learning 32, 1998, 41-62.","cites":null},{"id":37928705,"title":"Using Degradation Measures to Estimate a Time-to-Failure Distribution,","authors":[],"date":"1993","doi":"10.2307\/1269661","raw":"C. Joseph Lu and William Q. Meeker, Using Degradation Measures to Estimate a Time-to-Failure Distribution, Technometrics, 35(2), 1993, pp. 161-174","cites":null},{"id":37928713,"title":"Using phase space reconstruction of tract parameter drift in a nonlinear system,","authors":[],"date":"1997","doi":"10.1121\/1.418806","raw":"J.P. Cusumano, D. Chelidze, and N.K. Hecht, Using phase space reconstruction of tract parameter drift in a nonlinear system, Proceedings of the ASME 16th Biennial Conference on Mechanical Vibrations and Noise, Symposium on Time-Varying Systems and Structures, September 14-17, 1997.","cites":null},{"id":37928665,"title":"Validation of Helicopter Nominal and Faulted Conditions Using Fleet Data sets,","authors":[],"date":"1999","doi":null,"raw":"K. Maynard, C. S. Byington, G. W. Nickerson, and M. V. Dyke, Validation of Helicopter Nominal and Faulted Conditions Using Fleet Data sets, Proceedings of the International Conference on Condition Monitoring, UK, 1999 129 \u201314117","cites":null},{"id":37928690,"title":"What HMMs can do,","authors":[],"date":"2002","doi":null,"raw":"J. Blimes., What HMMs can do, Technical Report: UWEETR-2002-03, 2002.","cites":null},{"id":37928717,"title":"Yindong Ji Donghua Zhou, Real-time Reliability Prediction for a Dynamic System Based on the Hidden Degradation Process Identification,","authors":[],"date":"2008","doi":"10.1109\/tr.2008.916882","raw":"Zhengguo Xu Yindong Ji Donghua Zhou, Real-time Reliability Prediction for a Dynamic System Based on the Hidden Degradation Process Identification, IEEE Transactions on Reliability, 57(2), 2008, pp. 230-242","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-07-02T00:00:00Z","abstract":"Failure mechanisms of electromechanical systems usually involve several degraded\nhealth-states. Tracking and forecasting the evolution of health-states and\nimpending failures, in the form of remaining-useful-life (RUL), is a critical\nchallenge and regarded as the Achilles' heel of condition-based-maintenance\n(CBM). This paper demonstrates how this difficult problem can be addressed\nthrough Hidden Markov models (HMMs) that are able to estimate unobservable\nhealth-states using observable sensor signals. In particular, implementation of\nHMM based models as dynamic Bayesian networks (DBNs) facilitates compact\nrepresentation as well as additional flexibility with regard to model structure.\nBoth regular HMM pools and hierarchical HMMs are employed here to estimate\nonline the health-state of drill-bits as they deteriorate with use on a CNC\ndrilling machine. Hierarchical HMM is composed of sub-HMMs in a pyramid\nstructure, providing functionality beyond an HMM for modeling complex systems.\nIn the case of regular HMMs, each HMM within the pool competes to represent a\ndistinct health-state and adapts through competitive learning. In the case of\nhierarchical HMMs, health-states are represented as distinct nodes at the top of\nthe hierarchy. Monte Carlo simulation, with state transition probabilities\nderived from a hierarchical HMM, is employed for RUL estimation. Detailed\nresults on health-state and RUL estimation are very promising and are reported\nin this paper. Hierarchical HMMs seem to be particularly effective and efficient\nand outperform other HMM methods from literature","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/141176.pdf","fullTextIdentifier":"https:\/\/dspace.lib.cranfield.ac.uk\/bitstream\/1826\/6877\/1\/Health_State_Estimation_and_Prognostics-2010.pdf","pdfHashValue":"951165a09c642a8524710f34ca3015a200c61715","publisher":"IEEE","rawRecordXml":"<record><header><identifier>\noai:dspace.lib.cranfield.ac.uk:1826\/6877<\/identifier><datestamp>2012-02-03T11:23:30Z<\/datestamp><setSpec>hdl_1826_24<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>Health-state estimation and prognostics in machining processes<\/dc:title><dc:creator>Camci, Fatih<\/dc:creator><dc:creator>Chinnam, R. B.<\/dc:creator><dc:subject>Condition-based-maintenance , diagnostics , dynamic Bayesian networks , health-\nstate estimation , hidden Markov models , prognostics , remaining-useful-life<\/dc:subject><dc:description>Failure mechanisms of electromechanical systems usually involve several degraded\nhealth-states. Tracking and forecasting the evolution of health-states and\nimpending failures, in the form of remaining-useful-life (RUL), is a critical\nchallenge and regarded as the Achilles' heel of condition-based-maintenance\n(CBM). This paper demonstrates how this difficult problem can be addressed\nthrough Hidden Markov models (HMMs) that are able to estimate unobservable\nhealth-states using observable sensor signals. In particular, implementation of\nHMM based models as dynamic Bayesian networks (DBNs) facilitates compact\nrepresentation as well as additional flexibility with regard to model structure.\nBoth regular HMM pools and hierarchical HMMs are employed here to estimate\nonline the health-state of drill-bits as they deteriorate with use on a CNC\ndrilling machine. Hierarchical HMM is composed of sub-HMMs in a pyramid\nstructure, providing functionality beyond an HMM for modeling complex systems.\nIn the case of regular HMMs, each HMM within the pool competes to represent a\ndistinct health-state and adapts through competitive learning. In the case of\nhierarchical HMMs, health-states are represented as distinct nodes at the top of\nthe hierarchy. Monte Carlo simulation, with state transition probabilities\nderived from a hierarchical HMM, is employed for RUL estimation. Detailed\nresults on health-state and RUL estimation are very promising and are reported\nin this paper. Hierarchical HMMs seem to be particularly effective and efficient\nand outperform other HMM methods from literature.<\/dc:description><dc:publisher>IEEE<\/dc:publisher><dc:date>2012-01-23T23:05:18Z<\/dc:date><dc:date>2012-01-23T23:05:18Z<\/dc:date><dc:date>2010-07-02T00:00:00Z<\/dc:date><dc:type>Article<\/dc:type><dc:identifier>F. Camci,  R.B. Chinnam, Health-state estimation and prognostics in machining processes, IEEE Transactions on Automation Science and Engineering, Volume 7, Issue 3, 2010, Pages 581 - 597.<\/dc:identifier><dc:identifier>1545-5955<\/dc:identifier><dc:identifier>http:\/\/dx.doi.org\/10.1109\/TASE.2009.2038170<\/dc:identifier><dc:identifier>http:\/\/dspace.lib.cranfield.ac.uk\/handle\/1826\/6877<\/dc:identifier><dc:language>en_UK<\/dc:language><dc:rights>(c) 2010 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other users, including reprinting\/ republishing this material for advertising or promotional purposes, creating new collective works for resale or redistribution to servers or lists, or reuse of any copyrighted components of this work in other works.<\/dc:rights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["1545-5955","issn:1545-5955"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2010,"topics":["Condition-based-maintenance , diagnostics , dynamic Bayesian networks , health-\nstate estimation , hidden Markov models , prognostics , remaining-useful-life"],"subject":["Article"],"fullText":"1\uf020Abstract\u2014Failure mechanisms of electro-mechanical systems\nusually involve several degraded health-states. Tracking and\nforecasting the evolution of health-states and impending failures,\nin the form of remaining-useful-life (RUL), is a critical challenge\nand regarded as the Achilles\u2019 heel of condition-based-\nmaintenance (CBM). This paper demonstrates how this difficult\nproblem can be addressed through Hidden Markov models\n(HMMs) that are able to estimate unobservable health-states\nusing observable sensor signals. In particular, implementation of\nHMM based models as dynamic Bayesian networks (DBNs)\nfacilitates compact representation as well as additional flexibility\nwith regard to model structure. Both regular HMM pools and\nhierarchical HMMs are employed here to estimate on-line the\nhealth-state of drill-bits as they deteriorate with use on a CNC\ndrilling machine. Hierarchical HMM is composed of sub-HMMs\nin a pyramid structure, providing functionality beyond an HMM\nfor modeling complex systems. In the case of regular HMMs,\neach HMM within the pool competes to represent a distinct\nhealth-state and adapts through competitive learning. In the case\nof hierarchical HMMs, health-states are represented as distinct\nnodes at the top of the hierarchy. Monte Carlo simulation, with\nstate transition probabilities derived from a hierarchical HMM,\nis employed for RUL estimation. Detailed results on health-state\nand RUL estimation are very promising and are reported in this\npaper. Hierarchical HMMs seem to be particularly effective and\nefficient and outperform other HMM methods from literature.\nNote to Practitioners \u2014 Today\u2019s high competitive environment\nforces industry to decrease operating & support cost, whose one\nof the most contributing factors is maintenance and repair cost.\nThus, industry is interested not only in the identification of\nfailures, but also in identification of failure states, their\nprogression and forecasting. This paper presents health state\nestimation and remaining useful life prediction in machining\nprocesses with a case study on drilling processes.\nIndex Terms \u2014 Condition-based-maintenance, diagnostics,\nhealth-state estimation, prognostics, remaining-useful-life,\ndynamic Bayesian networks, hidden Markov models\nI. INTRODUCTION\nondition-Based-Maintenance (CBM) is a maintenance\ntechnology that employs such tasks as monitoring,\n\uf020Re-revised manuscript received in August 2009. This work was supported\nin part by NSF DMI Grant 0300132 and TUBITAK (The Scientific and\nTechnological Research Council of Turkey) under project number 108M275.\nF. Camci, Assistant Professor of Computer Engineering Department at\nFatih University, Istanbul Turkey 34500 (e-mail: fcamci@fatih.edu.tr).\nR. B. Chinnam, Associate Professor of Industrial and Manufacturing\nEngineering Department at Wayne State University, Detroit, MI 48202 USA\n(corresponding author, phone: (313) 577.4846, fax: (313) 578.5902, e-mail:\nr_chinnam@wayne.edu).\nclassification, and forecasting to increase system readiness and\nsafety while reducing costs attributed to reduced maintenance\nand inventory, increased capacity, and enhanced logistics and\nsupply chain performance [1]. Unlike time-based preventive\nmaintenance and corrective maintenance practices, CBM aims\nto avoid both unnecessary maintenance actions as well as\nmachine failures. The Center for Intelligent Maintenance\nSystems estimates that $35 billion per year would be saved in\nthe United States alone if CBM technology were widely\nemployed [2].\nThe failure mechanisms of electro-mechanical systems\nusually involve several degraded health-states. For example, a\ntiny change in a bearing\u2019s position could cause a small nick in\nthe bearing, which could cause scratches in the bearing race in\ntime, which then could cause additional nicks, which could\nthen lead to complete bearing failure [3]. Tracking and\nforecasting the health-state of a machine is very critical for\ndetecting, identifying, and localizing the failure as well as\ncarrying out proper maintenance. Hence, employing effective\ndiagnostic and prognostic algorithms\/methods is an important\nprerequisite for widespread deployment of CBM [4].\nDiagnostics is the process of identifying and localizing the\nmachine failure, and determining its primary cause and\nseverity, whereas prognostics is the process of estimating the\nremaining-useful-life (RUL) [5]. Diagnostics is, in essence, a\nclassification problem, and there are many methods proposed\nand implemented in the literature that attempt to resolve this\nproblem; this is in much contrast to prognostics, which is\nessentially a forecasting problem. However, most diagnostic\nalgorithms have limited potential in that they cannot detect\nfailure modes in a timely manner. See [1] for a thorough\nreview of popular diagnostic algorithms and methods.\nDiagnosing effectively the earliest stages of a failure, even\nif the machine is serving its intended function, is not only\nimportant but is a prerequisite for prognostics. It is logical to\ndiagnose these health-states through their effects on observed\nsensor signals since we are unable to \u2018observe\u2019 real health-\nstates. The primary challenge then within diagnostics is to\nachieve high classification accuracy in identifying health-\nstates given sensory signals such as vibration, current,\ntemperature, etc.\nPrognostics is a dynamic process that evolves in time from\nthe moment the machine is first used until it fails. RUL should\nnot be confused with expected life expectancy, which is the\n\u2018mean-time-to-failure\u2019 of an average machine\/component [6].\nExpected life expectancy is the average life of similar\ncomponents\/machines or a family of machines, while RUL is\nthe time-to-failure of a specific machine, which is being\nFatih Camci and Ratna B. Chinnam\nHealth-State Estimation and Prognostics in\nMachining Processes\nC\n2monitored. Prognostics is far more difficult than diagnostics,\nand is currently regarded the Achilles\u2019 heel of CBM [1].\nDespite considerable advances in sensing hardware,\ncommunications, information technologies, and software\nalgorithms, equipment health monitoring and diagnostics are\nstill largely reserved for only the most critical system\ncomponents and have not found their place in the mainstream\n[7]. Worse, there exist no robust prognostics methods (for\npredicting remaining-useful-life of existing assets), a vital\nenabler of condition-based maintenance, for even the most\ncritical system components. The aim of the models presented\nin this article is to squarely address this issue in the domain of\nmachining processes (one of the most common family of\nmanufacturing processes), in particular, for efficient and\neconomical replacement of cutting tools.\nThis paper employs Hidden Markov Models (HMMs),\nwhich characterize doubly embedded stochastic processes\nwith an underlying stochastic process that can be observed\nthrough another stochastic process and have been successful in\ntackling such difficult tasks as automatic speech recognition\n(ASR) [8] ,[9]. The tasks of both ASR and equipment\ndiagnostics have many commonalities. Speech signals are\nquasi-stationary, and so are the sensory signals such as\nmachine vibration [10]. Quasi-stationary signals in some sense\nterminate in an absorbing state and show stationary behavior\nin any reasonable time scale [11]. In addition, words should be\nrecognized in automatic speech recognition, although they are\nspoken by different speakers, whereas health-states should be\nrecognized in diagnostics although machine behavior can be\nquite different due to such factors as manufacturing and\nassembly variation, operating\/maintenance history, and aging.\nBeyond the aforementioned commonalities, implementation of\nHMMs for diagnostics is more difficult than their\nimplementation in speech recognition. For example, the\nnumber of phonemes is a relatively small finite set in ASR\n(resulting in sound and word libraries), a notion that is neither\nobserved nor justified in machine diagnostics. In addition, S\/N\nratios tend to be far better in speech signals unlike machine\nsensor signals. Nevertheless, speech signal only remains\nstationary over intervals of approximately 10ms. In\ncomparison, machine vibration signals remain stationary on\ntime scales of many seconds and even minutes [10].\nRegular HMMs were implemented for health-state\nestimation in the literature [3], [10] ,[12]-[16]. However,\nregular HMMs tend to be limited in their ability to represent\ncomplex systems. More importantly, in the absence of\n\u2018labeled\u2019 health-state sensor signal examples, the unsupervised\nlearning process for diagnostic applications is computationally\ntedious, for it involves such methods as competitive learning\n[15]. In addition, regular HMMs do not have intrinsic\ntransition probabilities between health-states since each HMM\nrepresents a distinct \u2018health-state\u2019. Hence, they require\nadditional methods to calculate health-state transition\nprobabilities to be utilized in RUL estimation. Thus, effective\nprognostics could not be carried out using regular HMMs.\nWhile some have indicated the potential for using log-\nlikelihoods of health-state HMMs for calculating RUL [12],\n[14], they rely on mostly empirical regression models. The\naim of this paper is to be able to obtain effective diagnostics\nand prognostics results by overcoming aforementioned\ndifficulties (inability to represent complex systems,\ncomputational difficulty, and lack of health-state transition\nprobability for RUL estimation).\nIn this article, we present the implementation of hierarchical\nHMMs as dynamic Bayesian networks for health-state and\nremaining-useful-life estimation. The main contribution of this\npaper is to be able to obtain effective diagnostics and\nprognostics results for machining processes (e.g., drilling\nprocess) using Hierarchical Hidden Markov Model (HHMM).\nHierarchical HMM, a variant of a HMM that is composed of\nseveral sub-HMMs in a pyramid structure, strengthens the\nability of an HMM to jointly represent multiple health-states\nalong with their state transition properties, facilitating\nestimation of RUL.\nThe paper is organized as follows: Section II gives the\nproblem description, while section III discusses hidden\nMarkov Models (HMM) and dynamic Bayesian networks\n(DBN) and their implementations for health-state estimation\n(diagnostics) and RUL estimation (prognostics). Section IV\npresents results from a drill-bit application of regular HMMs\nand HHMMs for health-state estimation and RUL estimation\nusing HHMM and Monte-Carlo simulation. Finally, Section V\noffers some concluding remarks.\nII. PROBLEM DESCRIPTION\nA. Drilling Process\nDrilling process is one of the most commonly used\nmachining processes in industry [17], [18]. Up to 50% of all\nmachining operations in US involve drilling [19]. In addition,\naround 40% of metal removal operations in the aerospace\nindustry involve the drilling process [20]. For example,\nproduction of a typical small jet fighter requires about 245,000\nholes to be drilled [21].\nTool breakage and\/or excessive wear may cause fatal\ndefects in the product. Quality of drilled holes is crucial for\nsome 60% of rejected parts are often attributable to poor hole\nquality [18]. Drill-bit, as a cutting tool, is one of the main\ncomponents that affects the hole quality. Thus, early detection\nof drill-bit breakage and\/or excessive wear is important and\nhas been studied extensively [17]-[21]. While there is a large\nbody of literature targeting monitoring and diagnostics of\ndrilling processes, there is very little work related to drill-bit\nprognostics.\nIt is generally agreed that the most appropriate sensor\nsignals for drill-bit monitoring and diagnostics are longitudinal\nthrust-force and torque signals [17],[18]. For example, a\ncareful review of diagnostics methods that employ 11 different\nsensor signals for monitoring drill-bits reveals that the most\ncommonly used and valuable signals are indeed thrust-force\nand torque signals [22]. For these reasons, we too employ\nthese same sensor signals for monitoring the drilling process.\n3Figure 1: Experimental setup for capturing thrust-force\nand torque degradation signals during drilling process.\nFig. 1 illustrates the drilling process along with a data\nacquisition system. The signals collected during the actual\ndrilling process (from the time the drill-bit enters the work\npiece until it exits the work piece from the other side), labeled\n\u2018hole signal vector(s)\u2019 will be used for monitoring the drill-bit\nhealth. Thus, the life of the drill-bit can be discretely modeled\nin terms of number of holes successfully drilled by the bit\nrather than actual drilling time.\nB. Problem Formulation\nWhen used, drill-bits undergo deterioration, and in the\nprocess, go through several \u2018health-states\u2019 (say \u2018brand new\u2019\nstate to the \u2018failure\u2019 state as do humans from infancy to death).\nThe fundamental problem this paper targets is two-fold:\nidentification of current drill-bit state (diagnostics \u2013 health-\nstate estimation) and remaining useful life (prognostics \u2013\nnumber of additional holes the drill-bit can successfully drill\nfrom the \u2018current\u2019 state to \u2018failure\u2019 state).\n1) Diagnostics \u2013 Health State Estimation\nHealth-states of the drill-bit cannot be clearly observed due\nto the nature of the process. In most industrial settings, it is\nvery difficult if not impossible to stop and physically\nobserve\/assess the drill-bit state after every hole. However, the\neffects of the health-states can be observed indirectly through\nthe signals under observation (i.e., thrust-force and torque).\nSince the identification of the health-states cannot be\ndeterministic due to the variations within the process, material\nand other external factors, they will be represented with\nprobabilities. The diagnostics problem can be formulated as\nfollows:\n\uf028 \uf029\uf028 \uf0291:arg max | , 1...\ni\nt i t\nS\nHS P X S O i i N\uf03d \uf03d \uf022 \uf03d (1)\nwhere tX is health-state variable at time t , 1:tO is the\nobserved signal vector(s) from time 1 to t , iS is the health-\nstate i , N is the number of possible distinct health-states, and\nHS is the identified health-state with highest probability.\nFigure 2: Sensor signals from a typical hole drilling cycle\nrevealing different sub-state signatures.\nEq. (1) would be adequate for diagnostics if the\nobservations within a hole given the health-state were\nstationary (having static statistical properties). Unfortunately,\nthrust-force and torque signals are non-stationary during the\nhole generation. When the drill-bit enters the material, the\nthrust-force and torque signals increase rapidly. Later, they\nbecome relatively smooth with increasing trend, partially due\nto increase in friction between the side wall of the partially\ncompleted hole and the rotating drill-bit. Once the drill-bit\nproduces the hole (goes through the other side of the work\npiece), the thrust-force and torque signals decrease rapidly in\namplitude. In other words, there exist other states, labeled\n\u2018sub-states\u2019, within the hole that affect the observations (their\namplitudes and signatures) besides overall drill-bit \u2018health-\nstates\u2019. Fig. 2, a sample plot of sensor signals from a single\nhole, clearly reveals these sub-states. In addition, health-states\nnot only affect observations, but also affect the sub-states and\ntheir transitions. Hence, the monitoring algorithms have to\nexplicitly account for transitions between both the sub-states\nwithin a hole as well as overall health-states for effective\ndiagnostics.\nThe diagnostics formulation can be re-written to handle the\nsub-states as follows:\n\uf028 \uf029\uf028 \uf0291 21: 1:arg max | , , 1...\ni\nt i t t\nS\nHS P X S O X i i N\uf03d \uf03d \uf022 \uf03d (2)\n2\n1:tX : Sub-states from time 1 to t\n1\ntX : Health-state at time t\nNote that these formulations help us characterize the\nproblem, but do not present a solution. These formulations\nmay not be solvable directly and may need several other\nformulations to represent the relationships. For example, the\nsub-states 21:tX are not observable and have to be estimated\nusing inference algorithms.\n2) Prognostics \u2013 Estimation of Remaining Useful Life\nHere, the goal is to estimate the number of additional holes\nthe drill-bit can successfully drill from the current state to\nfailure state. This can be formulated as follows:\n1 1 1 1\n1 1: , ,..., ,t t t n t nFind n X i X j X l X f\uf02b \uf02b \uf02d \uf02b\uf03d \uf03d \uf03d \uf03d (3)\nStandardized Torque\nStandardized Thrust-force\nRapid\nIncrease\nTrend\nRapid\nDecrease\nHOLE\nSTART\nHOLE\nEND\n4where t , 1t \uf02b , 2t \uf02b ,\u2026, t n\uf02b represent the distinct hole\nsequence ( t denotes current hole and t n\uf02b the last\nsuccessfully drilled hole prior to failure), , ,..., ,i j l f are the\nhealth-state transitions, f the failure state ( ...f l j i\uf03e \uf0b3 \uf0b3 \uf0b3 ),\nand n is a random number representing the number of\nremaining drilling cycles or holes.\nNext section discusses the models to be employed for\naddressing the formulations from Eqns. (2) and (3).\nIII. MODELING BACKGROUND\nTwo different modeling approaches that employ hidden\nMarkov models (HMMs) will be presented here to solve the\ndiagnostic and prognostic formulations discussed above (i.e.,\nEqns. (2) and (3)). First approach employs a \u2018set\u2019 of HMMs\nwhereas the second approach employs a single Hierarchical-\nHMM (HHMM).\nA. Hidden Markov Models (HMM)\nThis section discusses hidden Markov models and their\napplication to drill-bit health-state estimation using\n\u2018competitive learning\u2019 [23]. Readers familiar with HMMs can\nskip the first sub-section and move to the second sub-section,\nwhich gives the application details.\n1) HMM Description\nIn state-space modeling, a stochastic system can be\ndescribed as being in one of a finite number of states at any\ntime. The system evolves through the states according to a set\nof probabilities associated with each state as demonstrated in\nFig. 3. The model is called a hidden Markov model if states\nare not observable (hidden) and are assumed to be causing the\nobservations. The system behavior depends on the current\nstate and predecessor states. A special case, first-order HMM,\nassumes that only the current state is responsible for producing\nthe observations. In the remainder of this paper, HMM implies\na first-order HMM.\nTo better understand HMM, let us consider an urn and ball\nsystem with 3 urns and a different number of colored balls in\neach urn [24]. An urn is selected randomly; then, a ball is\nchosen from this urn, its color is recorded, and it is placed\nback into the urn it is chosen from. In the next step, a new urn\nis selected, and a ball is chosen from this urn and recorded.\nThis process is repeated a finite number of times resulting in a\nfinite observation sequence of ball colors. Now, assume that\nthe system is in a different room and handled by someone else\nso that we don\u2019t see the selected urns. The only event\nobservable to us is the colors of the selected balls. Obviously,\nthe simplest HMM representation of this system corresponds\nto states being urns with different color probabilities for each\nurn.\nFigure 3: A Markov chain with 6 states and state transition probabilities\n(arrows represent non-zero state transition probabilities).\nThere are several elements to an HMM: number of\nstates ( )N , observations, state transition probability\ndistribution, observation probability distribution, and initial\nstate distribution. tX denotes the state at time t and tO\ndenotes observation at time t , which might either be a\ndiscrete symbol {1,..., }tO L\uf0ce or a feature vector from L\ndimensional space, LtO R\uf0ce . State transition probability\ndistribution models the probability of being in state i at time\nt , given that it is in state j in time 1t \uf02d , and is denoted as\n, 1{ } ( | )i j t tA P X i X j\uf061 \uf02d\uf03d \uf03d \uf03d \uf03d . Observation probability\ndistribution defines the probability of observing k at time t\ngiven the state i , denoted as { ( )} ( | )i t tB b k P O k X i\uf03d \uf03d \uf03d \uf03d .\nThese distributions are either mass functions in the case of\ndiscrete observations or specified using a parametric model\nfamily -commonly Gaussian- in the case of continuous\nobservations. Initial state distribution is the probability of\nbeing in state i at 0t \uf03d and is denoted as 1( ) ( )i P X i\uf070 \uf03d \uf03d .\nGenerally, \uf028 \uf029, ,A B\uf06c \uf070\uf03d is used to specify a HMM. The rest of\nthis paper employs the Gaussian observation model.\nThere are three basic problems of interest to be solved given\nthe above model specifications.\n\uf0b7 How to compute the probability of obtaining the\nobservation sequence 1 2 ... TO O O O\uf03d given the model \uf06c ? (i.e.\n\uf028 \uf0291 2 ... |TP O O O \uf06c =?) The forward-backward (FB) algorithm\n[25],[26] is commonly used for this since it is more efficient\nthan the direct evaluation method.\n\uf0b7 How to identify the most likely state sequence that\nmight produce the observation sequence? The Baum-Welch\nalgorithm, also called the Expectation Maximization (EM)\nalgorithm, uses both the forward and backward procedures to\nsolve this problem [27].\n\uf0b7 How to adjust or learn the parameters of \uf06c in order\nto maximize the likelihood of the given observation sequence?\nThe EM algorithm solves this problem as well.\nThese three problems are tightly linked and studied\nextensively in the literature. The standard HMM solution to\nthese problems requires an exponential number of parameters\nto specify the transition and observation models since it\ncalculates the Cartesian product of the state-spaces of each\nexample. This means requiring excessive amounts of data to\nlearn the model (high sample complexity) and exponential\ntime for inference (high computational complexity). For\nexample, the FB algorithm cycle takes \uf028 \uf0292NO Tk operations.\nFor more detailed information about HMMs, see [24].\n2) HMMs for Drilling Process Diagnostics with\nCompetitive Learning\nAs mentioned in section II.B.1, the thrust-force and torque\nsignals are non-stationary within a hole. In other words, there\nexist several sub-states within a hole such as \u2018rapid increase\u2019,\n\u2018trend\u2019, and \u2018rapid decrease\u2019 states as displayed in Fig. 2. In\nstandard HMM modeling, the state of the HMM ( tX )\nS1\nS5\nS2\nS6\nS4\nS3\n5represents these states in the hole. Given that HMM states are\nmodeling the sub-states within a hole, a single HMM is\nincapable of modeling the different overall \u2018health-states\u2019\nwitnessed during the life of a typical drill-bit. Thus, we\nemploy a set of HMMs, hoping that at the end of the\n(competitive learning based) training process, each HMM\nwithin the set will represent a distinct health-state.\nIdentification of current health-state given observations of a\nhole is performed as a competition among HMMs. Under\ncompetitive learning [23], HMMs (undergoing learning)\ncompete to represent the health-state dynamics present within\neach training example, i.e., sensor signals from a hole,\npresented one at a time in random order to the HMM pool.\nThe HMM with the highest fitness, i.e. highest log-likelihood\nvalue, is identified as a winner and is considered the\nrepresentative of the current health-state. In general, only the\nHMM winner is allowed to further learn using the current\ntraining example (however, literature offers other learning\nschemes that might allow few nearest neighbors of the HMM\nwinner to learn as well). This process is repeated for all holes\nof all drill-bits that are being used for training. Training\ncontinuous until predefined number of iterations or\nconvergence is achieved (see [23] for more details). Fig. 4\nfurther illustrates this competitive learning based training\nprocedure for the HMM pool. Once trained, for actual on-line\ndiagnostics, the health-state of any given hole sensor signal\nvector is defined by the trained HMM with the highest log-\nlikelihood value as illustrated in Fig. 5.\nFigure 4: Procedure for training HMM pool through competitive learning for\nhealth-state diagnostics.\nFigure 5: Health-state inference with regular HMMs.\nB. Hierarchical Hidden Markov Models (HHMM)\nHierarchical HMM (HHMM), proposed by Fine et al [28],\nis an extension of an HMM that is designed to model\nhierarchical structures for sequential data. In the HHMM, each\nstate is considered to be a self contained probabilistic model.\nMore precisely, each state of the HHMM is itself an HMM (or\neven a HHMM). When a state in an HHMM is activated, it\nwill activate its own probabilistic model, i.e. it will activate\none of the states of the underlying HHMM, which in turn may\nactivate its underlying HHMM and so on [29]. The process is\nrepeated until a special state, called a \u2018production state\u2019, is\nactivated. Only the production states emit observations in the\nusual HMM sense.\nFig. 6 provides an illustrative example of this hierarchical\nstructure of hidden-states. In the figure, 1tX represents the\ntht\ntop-state and 2kX represents the\nthk sub-state. The states that\ndo not directly emit observations symbols are called hidden or\n\u2018internal states\u2019 ( 1tX and\n2\nkX are hidden states). The transition\nof a hidden state from one to another within the same level is\ncalled \u2018horizontal transition\u2019. For example, under 11X ,\ntransition from 21X to\n2\n2X is an example of horizontal\ntransition. After the last state is reached in sub-states, a\n\u2018vertical transition\u2019 is allowed. For example, after sub state\ntransitions reach the last state 23X indicating that the hole has\nended, health-state transition from 11X to\n1\n2X may occur in the\nexample from Fig. 6.\nFigure 6: Hierarchical representation of states in HHMM:\nTop states ( 1iX ) and Sub-states (\n2\njX )\nTrained\nHMM1\nTrained\nHMM2\nTrained\nHMMN\n\u2026\nLog-likelihood\nCalculation\nLog-likelihood\nCalculation\nLog-likelihood\nCalculation\nSelect\nMaximum\nGet data from the hole whose health-\nstate needs to be determined\nHealth-state represented by the\nHMM with highest log-likelihood\nThrust-Force & Torque Signals\nSelect data from a random\n\u2018hole\u2019 for learning\nHMM1 HMM2 HMMN\u2026\nLog-likelihood\nCalculation\nLog-likelihood\nCalculation\nLog-likelihood\nCalculation\nIdentify winner (with\nmaximum Log-likelihood)\nTrain winner HMM\nGet data from all\ntraining drill-bits\nData\nleft?\nConve\nrged?\nNo YesNo\nYes\nend\nThrust-Force\n& Torque Signals\nHMM\nPOOL\nCOMPETITIVE\nLEARNING\n1\n1X\n1\n2X\n2\n1X\n2\n2X\n2\n1X 22X\n2\n3X\n2\n3X\n6The methods for estimating the HHMM parameters and\nmodel structure are more complex than for the HMM, see [28]\nfor more details. Dynamic Bayesian Network (DBN) can more\nefficiently represent HMMs with the added flexibility of\nimplementing different variants of HMM such as HHMM.\nReaders who are familiar with DBNs may skip the next two\nsub-sections and move to the sub-section on HHMM for\ndrilling process diagnostics.\n1) Dynamic Bayesian Network (DBN)\nGraph and probability theories are the basis of Bayesian\nnetworks, also known as Graphical Models, that combine the\nvisual representation of variables (i.e., nodes) and conditional\nprobabilities representing their cause-effect relationships. The\nconditional probability distribution of a node variable depends\nonly on the parents of the node and is independent of its\nancestors (i.e., parent\u2019s parents) given its parents. This is\ncalled the factorization property, and it dramatically reduces\nthe number of model parameters.\nWhile Bayesian networks are effective in their\nrepresentation, they are limited to modeling \u2018static\u2019\nrelationships. However, sequential data arises in many areas of\nscience and engineering, calling for modeling of dynamic\nprocesses. Dynamic Bayesian networks (DBNs) are state-\nspace models that are effective for modeling these types of\nstochastic processes, and are more general and expressive than\nHMMs, where hidden variables are discrete, and Kalman\nFilter Models (KFMs), where the hidden variables are\ncontinuous [30], [31]. While state-space in HMMs consists of\na single discrete random variable, DBN can represent the\nhidden-state in terms of a set of random variables. While KFM\nrequires all the condition probability distributions to be linear-\nGaussian, DBN allows arbitrary CPDs. In addition, HMMs\nand KFMs have a restricted topology, whereas, DBN allows\nmuch more general graph structures. Note that the term\n\u2018dynamic\u2019 in DBN means we are modeling a dynamic system,\nand does not mean that the Bayesian network graph structure\nchanges over time. DBNs are thus designed to model\nprobability distributions of hidden variables (states) that\nevolve in time by using sequenced observed variables that are\ngenerated by these hidden-states [32]. DBN structure consists\nof different levels such as observation level ( O nodes -\nshaded) and hidden-state level ( X nodes \u2013 not shaded), as\nillustrated in Fig. 7. Observation in time t ( )tO is generated\nby hidden-state tX and previous observation 1( )tO \uf02d . Once the\nstructure is designed, conditional probability distributions (i.e.,\ninitial probability distribution 1( )P X i\uf03d , state transition\ndistribution 1( | )t tP X X \uf02d , and the observation distribution\n|( )t tP O X ) of each node given its parents are learned during\nthe \u2018training\u2019 process. It is also assumed here that transition\nand observation functions do not change over time. In this\nwork, the observation distribution is assumed to be continuous\nand Gaussian as represented in (4).\n\uf028 \uf029 \uf028 \uf0292| ~ ,t t i iP O X i N \uf06d \uf073\uf03d (4)\nEven though a variable from different time instants could be\npotentially represented as distinct variables in the DBN\nnetwork, the difficulty in representation and computation is\nobvious even from problems with limited time history (e.g., 10\ndistinct variables will be required to represent 1 variable in 10\ntime instants). Hence, variables in different time slices that\nhave the same parental structure are represented as one\nvariable in DBNs to obtain compact representation. For\nexample, in Fig, 7, hidden-states in all time slices except the\ninitial time slice (i.e., 2,3,...t n\uf03d but not 1t \uf03d ) have the same\nparental structure, in which each variable has a previous\nhidden-state as the sole parent, and are represented as one\nvariable tX . Similarly, observable variables in all time (i.e.,\n1,2,...t n\uf03d ) are represented as O , since they have a hidden-\nstate of the same time slice as the parent. Representation of\nHMM as a DBN thus requires just three variables (i.e.,\n1X , tX , and O ; one for the initial hidden variable, one for\nother hidden variables, and one for the observable,\nrespectively). The goal of a DBN acting as a HMM is to infer\nthe hidden-state given the observation sequence, which can be\nrepresented more precisely as 1:( | )t tP X i O\uf03d . Fig. 7\nillustrates the implementation of HMM as a DBN with shaded\nnodes representing observable variables and blank nodes\nrepresenting hidden variables.\nFigure 7: Representation of HMM as a dynamic Bayesian network (DBN).\n2) Implementation of HHMM as DBN\nIn representing HHMM as a DBN, as illustrated in Fig. 8,\nvariables in all levels are represented by a node in each\ntime slice. Top-state 1( )tX is replicated in Fig. 8 for each\nsub-state in Fig. 6 as well as the observation state, which\nrepresents the observed variable. As illustrated in Fig. 8,\nthe hidden-states in top- and sub-state levels \u2018cause\u2019 the\nobservation and top-level states also cause the sub-level\nstate (\u2018cause\u2019 is represented as an arrow). By replicating\ntop-states for each sub-state, we encounter the danger of\nlosing the hierarchical structure. In order to maintain the\nhierarchical structure, \u2018top level\u2019 state transitions are only\nallowed if the \u2018lower level\u2019 state reaches the last possible\nstate. In Fig. 6 example, top-state cannot be 12X unless\nsub-state reaches 23X . This can be achieved through a\nbinary control variable F, allowing a top-level state\nchange only if the control variable F=1. The control\nvariable F is allowed to take a value of 1 only if the lower\nlevel reaches its last possible state. In other words, 2 1tX \uf02d\naffects 1tX indirectly through 1tF \uf02d .\n1\ntX can change only if\n1 1tF \uf02d \uf03d . 1tF \uf02d becomes 1 only if\n2\n1tX \uf02d reaches the last\nstate. Hence, six variables (i.e., initial lower and top level\nhidden-states 11X ,\n2\n1X ; lower and top level hidden-states\nother than initial case 1tX ,\n2\ntX ; control state F ; and\nobserved state O ) are necessary to represent the this\nHHMM as a DBN.\n1X tX\nO O\n1X\n1O\n1t\n2X\n2O\n2t\n3X\n3O\n3t\nHidden States\nObserved signal\nTime sequence\n\u2026\n\u2026\n\u2026\n7Figure 8: DBN representation of HHMM from Fig. 6.\nd\ntX denotes node X in time t at level d\nThe conditional probabilities of variables in the first\/initial\ntime slice are initial distributions and written in (5) and (6):\n\uf028 \uf0291 11 ( )P X j j\uf070\uf03d \uf03d (5)\n\uf028 \uf029 \uf028 \uf0292 1 21 1| jP X i X j i\uf070\uf03d \uf03d \uf03d (6)\nHere, 1( )j\uf070 denotes the initial top-level state distribution and\n\uf028 \uf029\n2\nj i\uf070 the probability of sub-state being in state i , given that\nthe upper state is in state j .\nGiven that variable F plays the key role of sustaining the\nhierarchical structure in the rest of the time slices, the\nconditional probabilities of top and lower hidden layers (other\nthan initial ones) are based on F. If F is \u2018on\u2019 (i.e., F=1), it is a\nvertical transition (i.e., lets an upper level state change; e.g.,\nfrom 11X to\n1\n2X ); otherwise, it is a horizontal transition (i.e.,\ntransition to a state in the sub-state level under the same top\nlevel state; from 21X to\n2\n2X ). In the top level, the conditional\nprobability distribution is the top level state transition\nprobability ( \uf028 \uf0291 ,A i j ) if the control variable F is \u2018on\u2019;\notherwise, the top level state is not allowed a transition as\nformulated in (7). In the sub-state level, conditional\nprobability can be drawn from either the initial distribution or\nthe transition distribution depending on the state of the binary\ncontrol variable F as written in (8). The probability of turning\ncontrol variable F \u2018on\u2019 is equal to the probability of transition\nto the last state as stated in (9). In addition, (10) gives the\nconditional distribution of the observed state.\n\uf028 \uf0291 1 1 1| ,t t tP X j X i F f\uf02d \uf02d\uf03d \uf03d \uf03d \uf03d\n\uf028 \uf029\n1\n1 if 0 &\n0 if 0 &\n, if 1\nf i j\nf i j\nA i j f\n\uf0ec \uf03d \uf03d\n\uf0ef\n\uf03d \uf0b9\uf0ed\n\uf0ef\n\uf03d\uf0ee\n(7)\n\uf028 \uf029\n1 ,A i j denotes transition probability from top state i to j\nin the top level of the hierarchy.\n\uf028 \uf0292 2 11 1| , ,t t t tP X j X i F f X k\uf02d \uf02d\uf03d \uf03d \uf03d \uf03d \uf03d\n\uf028 \uf029\n\uf028 \uf029\n2\n2\n, if 0\nif 1\nk\nk\nA i j f\nj f\uf070\n\uf0ec \uf03d\uf0ef\n\uf0ed\n\uf03d\uf0ef\uf0ee\n(8)\n\uf028 \uf029\n2 ,kA i j denotes transition probability from state i to j\ngiven that the high level state is in k and \uf028 \uf0292k j\uf070 the initial\nsub-state level distribution given that high level state is in k .\n\uf028 \uf029 \uf028 \uf0291 2 21| , ,t t t kP F X k X i A i l\uf03d \uf03d \uf03d \uf03d (9)\nwhere l is the last state.\n\uf028 \uf029 \uf028 \uf0291 2 2| , ~ ,, ,i j i jt t i jP O X X Nt \uf06d \uf073\uf03d \uf03d (10)\nThe conditional probabilities are learned during the training\nprocess using the training dataset. See [33] for more detailed\ninformation about DBNs.\n3) HHMM for Drilling Process Diagnostics\nRealization of a DBN involves three phases: Designing,\nTraining, and Testing as represented in Fig. 9. During the\nDesign phase, the structure of the DBN (i.e., hidden and\nobservable variables and their cause-effect relationships in the\nform of a directed acyclic graph) and the conditional\nprobabilities are defined. Design phase includes the\nidentification of number health-states and sub-states within a\nhole. In other words, number of different i and j values in\nEqns. (5) and (6). These numbers are typically identified\nusing subject domain expertise and\/or trial-error and will be\ndiscussed further in section IV.\nIn the training phase, drill-bits are divided into two groups:\none for training, other for testing. Given the observation\nsequence, parameters 1( )j\uf070 , \uf028 \uf0292j i\uf070 , \uf028 \uf029\n2 ,kA i j , ,i j\uf06d , and\n2\n,i j\uf073\n(as displayed in Eqns. (5)-(10)) are learned using the training\ndataset. A hole is randomly selected from training dataset (any\nhole from any drill bit in the training dataset) and DBN\n(HHMM in our case) is trained with the data sequence of the\nselected hole. The techniques for learning the mentioned\nparameters in DBN given the data sequence are mostly\nstraightforward extensions of the techniques for learning BNs,\nand often involve expectation maximization, such as the\nBaum-Welch (EM) method. This training process is repeated\nfor all holes to be used for training, presenting them in random\norder. Training continues until convergence or predefined\nnumber of iterations are reached. Fig. 10.a illustrates this\ntraining of HHMM. Note that the toolbox developed by Kevin\nMurphy (Bayesian network toolbox\nhttp:\/\/www.cs.ubc.ca\/~murphyk\/Software\/BNT\/bnt.html [34]),\nemployed for this work, has ready functions for training,\nwhich get the training data as input and performs the training\nand returns the trained HHMM with estimated conditional\nprobability parameters.\n1\n1X\n1\ntX\nO O\n2\n1X\n2\ntX\nF\nSub-state level\nObserved\nTop-state level\nControl Variable\nH\nidden\nStates\n8Figure 9: DBN application flowchart.\nFigure 10: HHMM training and inference illustration.\nOnce training is complete, the testing phase (i.e., inference)\ninvolves estimation of the most likely hidden-state sequence\ngiven the observation sequence from all drill-bits in training\nand testing datasets, albeit separately. While exact inference of\nhidden-states in DBNs is possible through the forward-\nbackward algorithm and others, it is NP-hard (computationally\nintractable). Fortunately, there are a variety of deterministic\nand stochastic approximate inference methods that are\ncomputationally efficient while offering reasonable\nperformance. The BNT toolbox mentioned above also has\nfunctions for inference. See [28],[33] for more detailed\ninformation about methods used in training and testing phases\nof BNs and DBNs.\n4) Prognostics - RUL Estimation\nThe primary task of prognostics is to estimate the\nremaining-useful-life (RUL) of the equipment. In the context\nof CBM, RUL can be defined as the operational hours or\nnumber of cycles to be completed before a system or\ncomponent will require replacement or maintenance. Given\nthe uncertainty of prognostics, it is also best to characterize\nRUL as a probability distribution.\nThe literature on prognostics is extremely sparse. In general,\nprognostics methods can be broadly grouped into two\ncategories: physics-based and empirical-based. Even though\nphysics-based prognostics models have been attempted for a\nvariety of mechanical systems and sub-systems with some\nsuccess [35], [36] and might give better results than empirical-\nbased models, they are machine specific and much more\nexpensive to implement. For these reasons, physics-based\nmethods pose significant barriers to widespread deployment of\nCBM practices.\nEmpirical prognostic methods can be grouped into three\ncategories. The first approach, evolutionary prognostics,\ninvolves the trending of key features combined with simplistic\nthresholds set from past experience and the analysis of the\nchange rate from the current condition to the known failure in\nthe feature space. In [37], failure is defined as specified level\nof degradation (i.e., threshold) and various degradation models\nare used to estimate RUL, the time to reach the threshold.\nSimilarly, a Gamma process model is used for degradation\nmodeling in [38]. However, many systems are not simple\nenough to set thresholds to the features for failure states. The\nsecond approach [39] is to utilize statistical regression models\nand\/or computational intelligence methods such as neural\nnetworks to model known failure degradation paths in the\nfeature space. However, these models are not very promising,\nespecially for long-term forecasting, which is a necessity for\nestimation of RUL. The third approach, future state\nestimation, estimates a state vector that represents the\nequipment health condition from a brand new state to failure\nby employing subspace and non-linear dynamic methods [40].\nThese methods forecast the progression of health-states of the\nmachine from the current state (estimated by diagnostic\nmethods) to the failure state by employing transition\nprobabilities between states and time spent in each state [35],\n[40], [41]. The proposed method for RUL estimation would\nqualify as a state-estimator-driven prognostic method. Kalman\nand Alpha-Beta-Gamma tracking filters are also examples of\nthis approach [42]. In [43], health-states are predicted using a\nhidden degradation process identification method. However,\nthis method is claimed to be in its infancy and applicable to\nnon-complex conditions for now [43].\nAs state-estimator-driven prognostic method, HMM is used\nin [12], [13], [16]. In [16], the health-states for each hole are\ndefined by the user and the estimated transition from one\nhealth-state to another is compared with the transition defined\nby the user. However, supervised learning concept may be\nmisleading in cases where ground truth information is not\navailable. In most health-state estimation problems, the true\nhealth-state is not known or impractical to collect. Thus,\nauthors of [16] have presented an unsupervised learning\nconcept in [12], in which HMM models are employed for\nRUL estimation. In particular, they employ a competitive\nlearning method for health-state estimation. They also\nproposed three different RUL estimation methods. In the first\nmethod (method 1), state transition probability distributions\nSelect data from a random\n\u2018hole\u2019 for learning\nHHMM Train HHMM\nGet data from all\ntraining drill-bits\nData\nleft?\nConve\nrged?\nNo YesNo\nYes\nend\nThrust-Force\n& Torque Signals\n(a) HHMM Training\nGet data from the hole whose\nhealth-state will be determined\nTrained\nHHMM\nProbabilities of top states\n(health-state) of HHMM\n(b) HHMM Inference\nDBN Inference\nDBN Design Phase\nConditional Prob.\nSpecification\nTraining Phase\nStructure Design\nTesting Phase\nObservation\nSequence(s)\nfor Testing\n\uf0b7 Hidden-state estimation with likelihood\n\uf0b7 Transition probabilities\nObservation\nSequence(s)\nfor Training\n9are estimated. In methods 2 and 3, health-state log-likelihood\nvalues are estimated using polynomial and quadratic\nregression methods, respectively, and RUL is calculated based\non estimated log-likelihood values. In the HHMM method\npresented in this paper, transition probabilities learnt during\nthe training process are used for RUL estimation. The results\nof the presented method will be compared with the results in\n[12].\nIn [13], hidden semi-Markov model is employed for a\npump-system. Some of these same authors also proposed an\nauto-regressive hidden semi-Markov model [14] for\ndiagnostics and prognostics of hydraulic pumps. Even though\ndetailed information about diagnostics results is given in [13],\nthe prognostics results are limited (only mean and covariance\nof one RUL estimation is reported).\nOur proposed method is a future state estimator and\nemploys Monte-Carlo simulation based on the health-state\ntransition probabilities captured by the HHMM in order to\ncharacterize the RUL. Since we employed \u2018left-to-right\u2019\nHHMM, only forward transition (from left to right) is allowed.\nIn other words, health-state of a drill-bit may get worse or stay\nthe same, but cannot get better as time progress. Health-state\ntransition probability is calculated as follows:\n\uf028 \uf029\n\uf028 \uf0291 1 1\n1 1\n, if\n| , 1\n0 otherwiset t t\nA i j i j\nP X j X i F\uf02d \uf02d\n\uf0a3\uf0ec\n\uf03d \uf03d \uf03d \uf03d \uf0ed\n\uf0ee\n(11)\nwhere \uf028 \uf029,A i j denotes transition probability from state i to\nstate j .\nAs can be seen from Eqn. (11), the transition probability\ndepends on the current state at (t-1) and possible future state at\n(t). The time spent in the current state does not affect the\ntransition probability. Given that the proposed HHMM\nstructure does not explicitly incorporate any state duration\ndensity, the assumption is that the state durations follow an\nexponential distribution. Future research will consider explicit\nstate duration density modeling.\nGiven the current health-state and all possible future health-\nstates and transition probabilities from a HHMM, RUL can be\nlooked upon as the answer to the following question: How\nmany transitions need to be made to go from the current\nhealth-state to the failure state? In the drill-bit monitoring\napplication, number of transitions from the current state to the\nfailure state corresponds to number of holes to be successfully\ndrilled before the failure of the drill-bit, since a transition from\na health-state to another one (or itself) occurs in each hole.\nHierarchical structure of the HHMM does not allow multiple\ntransitions in a hole (consider the transition of top-states and\ncontrol variable F).\nIt is generally the case that any unit or system under\nconsideration might endure multiple successful operational\nhours\/cycles in the \u2018failure state\u2019 as identified by the HHMM.\nHowever, in the interest of not causing a real failure and\nallowing time for replacement\/maintenance, we define as\nfailure an \u2018entry\u2019 into the failure state. Given the learnt\nHHMM model and the intrinsic state-transition probability\nmatrices, the proposed approach naturally characterizes RUL\nas a probability distribution, utilizing the frequency table\ngenerated by Monte-Carlo simulation. As is the case with any\nMonte-Carlo simulation-based estimation, large sample sizes\nare critical for accuracy.\nThe process is illustrated for a hypothetical system with five\nhealth-states and some non-zero state transition probabilities\nin Fig. 16. According to the figure, the system is currently in\nthe second health-state with the fifth health-state defined as\nthe failure state. RUL, given the current state, is the number of\ntransitions necessary to reach the failure state (i.e., state #5)\nfrom the current state (i.e., state #2) by transitioning according\nto health-state transition probabilities. RUL distribution can be\nobtained by simulating the transition process several times,\nyielding, distinct RUL values.\nFigure 11: Illustration of equipment health-state\ntransition paths (including self-transitions).\n1: Brand new equipment, 5: Failure state, 2: Current health-state\nIV. IMPLEMENTATION AND RESULTS\nThe proposed health-state and RUL estimation methods\nwere applied to a drilling process, the most popular industrial\nmachining process. The intent was to estimate on-line (in a\nnon-intrusive way) the health-state of the drill-bit and RUL to\nfacilitate timely replacement of the bit in order to avoid any\nfailures within the work-piece and\/or premature replacement.\nDrill-bits are normally subject to gradual wear along the\ncutting lips and the chisel edge, which leads to a series of\ntransitions in health-states from a \u2018brand new\u2019 state to a\n\u2018failure state\u2019 [15]. The objective of on-line estimation of\nhealth-states and RUL was achieved through the processes\ndisclosed below.\nThe experimental setup (see Fig. 1) consisted of a HAAS\nVF-1 CNC Machine, a workstation with LabVIEW software\nfor signal processing, a Kistler 9257B piezo-dynamometer for\nmeasuring thrust-force and torque, and a NI PCI-MIO-16XE-\n10 card for data acquisition. Twelve drill-bits were used for\nthe experiment, and each was operated until it reached a state\nof physical failure. Thrust-force and torque sensor signals\nwere employed for health-state estimation given their strong\ncorrelation with the health condition of the drill-bit [17], [18].\nStainless steel bars with a thickness of 0.25 inches were used\nas specimens for tests. The drill-bits used consisted of high-\nspeed twist drill-bits with two flutes and were operated under\nthe following conditions without any coolant: feed-rate of 4.5\ninches-per-minute (ipm) and spindle-speed of 800 revolutions-\nper-minute (rpm). The thrust-force and torque data were\ncollected for each hole from the time instant the drill-bit\npenetrated the work piece through the time instant the drill-bit\nprotruded from the other side of the work piece. The data was\ncollected at 250 Hz, considered adequate to capture cutting\ntool dynamics in terms of thrust-force and torque. The number\nof data points collected for a hole changed between 380 and\n460. Data from each hole was binned to 24 RMS (root mean\nsquare) values. For illustrative purposes, data collected from\ndrill-bit #5 are depicted in Fig. 11. Given the substantial\n1 2 3 4 5\n10\ndifference in the amplitudes of the thrust and torque signals,\nnormalization (i.e., shifting the means to zero and scaling the\nstandard deviations to unity) seems to generally help speed up\nthe DBN parameter estimation process during learning. It\ninvolves calculating the mean thrust force (a scalar) and\nstandard deviation of the thrust force (another scalar) of data\nfrom all holes of all training dataset drill-bits. We then scale\nthe thrust force signal values by subtracting the mean and\ndividing the difference by the standard deviation (essentially a\nshifting and scaling operation). The procedure is repeated for\ntorque signals as well. The established mean and standard\ndeviation parameters of thrust force and torque signals are\nthen employed for scaling signals from the testing dataset as\nwell.\nFigure 12: Trust-force and torque data from drill-bit #5.\nIn order to evaluate the effectiveness of the proposed\nmethods, health-state classification \u2018accuracy\u2019 needs to be\nassessed. Reliable information is essential for calculation of\nideal classification accuracy. For the given experiment,\ntrustworthy information consists of the actual health-state of\ndrill-bits, which might be identified as a function of wear and\ntear on the cutting lips or the chisel edge at the end of each\ndrilling cycle, an extremely tedious proposition. However, this\nevaluation process between drilling cycles causes the drill-bit\nto cool down and lead to a weak representation of true\ndegradation under actual operating conditions. In addition,\nmany real world applications lack accurate information about\nthe health-state of the equipment.\nIn the absence of accurate and reliable information, we have\nevaluated the proposed method in two ways: first evaluation\ninvolves health-state estimation performance and the second\nevaluation is based on RUL estimation performance. As for\nhealth-state estimation performance (i.e., diagnostics), three\ncriteria are employed: number of reverse health-state jumps,\nuniformity, and health-state resolution. Reverse health-state\njump denotes the case when the diagnostic model indicates\nthat the drill-bit is revisiting a prior health-state after\ntransitioning to a different health-state. For example, if the\ndiagnostic model indicates that a particular drill-bit has\nreached a state of failure during on-line monitoring and then\nlater indicates that the drill-bit is once again in an operational\nhealth-state, we have a reverse health-state jump. Uniformity\nmeasures the consistent presence of the different health-states\nin all the drill-bits used for the training process and during on-\nline monitoring. Each health-state needs to be visited by most,\nif not all, drill-bits for generalization of health-state\nestimation. This is a reasonable measure for this experiment\nand may not be appropriate for systems that fail due to\nmultiple failure mechanisms. The last criterion, health-state\nresolution, ensures adequate resolution between a brand new\ndrill-bit and a complete failure. This is measured here by\ncounting the number of identified health-states. As for RUL\nestimation performance evaluation, we compare the estimated\nRUL, which is calculated using the estimated health-states,\nwith true or witnessed RUL and will be discussed in\nprognostics section.\nBoth regular and hierarchical HMMs (as described in\nsection III) were implemented for diagnostics and prognostics\nusing data from nine drill-bits for training and three for testing\nand are discussed in the next sub-section. The application is\nimplemented using MATLAB based on Kevin Murphy\u2019s\nBayesian network toolbox, available at\nhttp:\/\/www.cs.ubc.ca\/~murphyk\/Software\/BNT\/bnt.html [34].\nA. Health State Estimation with Competitive Learning of\nHMMs\nAs outlined in section III.A.2, in the competitive learning\ncase, several HMMs (HMM1, HMM2, etc.), each of which is\neventually expected to represent a distinct health-state, are\ncreated. A data sequence (i.e., 24 data points) for a random\nhole (among all training drill-bit data) was selected and the\nprobability of obtaining this data sequence given the HMM\nmodel is calculated for all HMMs (1st basic problem\nmentioned in HMM section). Log-likelihood value is the log-\nprobability of obtaining the data sequence given the HMM\ncalculated using the Forward-Backward algorithm. In general,\nthe resulting likelihood in HMM is represented as log-\nlikelihood values, since the exact distribution of the likelihood\nratio is very difficult to determine in testing nested\nhypotheses, as in [15], [24], [44]. The HMM with the highest\nlog-likelihood value is labeled the \u2018winner\u2019, inferring that the\ndrill-bit is in the health-state represented by the winner. The\nselected data sequence is used for further incremental training\nof the winning HMM alone. Standard training methods\nreferred in section II were used. This unsupervised\ncompetitive learning method is expected to lead each health-\nstate to be represented by a distinct HMM, since an HMM that\nlearned similar data sequences will have higher likelihood\nvalues during the competition.\nInitialization is an important issue for building HMMs for\nhealth-state estimation in terms of reducing training times and\nimproving diagnostic performance. In this investigation, the\nfirst and last holes of all drill-bits were used for initialization\nof the \u2018first\u2019 and \u2018last\u2019 HMM models in the pool, respectively.\nThen, one hole from each drill-bit is selected in such a way\nthat the selected holes are as far away from each other as\npossible to initialize the remaining HMMs. For example, with\nfour HMMs in the competitive learning pool and data\navailable from 22 holes from drill-bit #1, holes 1, 8, 15, and\n22 were used to initialize HMM1, HMM2, HMM3, and HMM4,\nrespectively. After initialization, HMMs were trained through\npure competitive learning as explained in detail in section\nIII.A.2. Data sequences from all training holes were presented\nin a random order to the HMM pool in each epoch of the\ncompetitive learning process. The learning process is\nterminated when reverse jumps (i.e., classification error) per\nepoch reach zero or when a pre-specified maximum number of\nepochs are reached.\nTime\nThrust-force \u2013\nNewtons\nTorque \u2013\nNewton meters\nA hole\n11\nHere is a note of caution regarding the selection of number\nof hidden-states for each HMM prior to competitive learning.\nIn our experiments, we varied the number of hidden-states\nwithin each HMM by starting with two states and increasing it\nuntil the non-stationarity is captured adequately. As with most\nstatistical models where measures of goodness-of-fit improve\nwith model complexity, so is the case with HMMs. As we\nincrease the number of hidden-states, so does the log-\nlikelihood [44]. However, as is the case with any model, we\nprefer simpler models to complex models, justified by the\nprinciple of Occam\u2019s razor [45]. Thus, we need to evaluate the\nperformance of HMMs by increasing the number of hidden-\nstates and stop when the rate of improvement in the log-\nlikelihood begins to diminish. If the number of hidden-states is\nincreased too much, they can also overlap, compromising the\ndistinction between the different sub-states, and in turn, the\nhealth-states. In our experiments, we did not see any\nsignificant improvement in performance beyond four hidden-\nstates per HMM.\nFor the given datasets, best results based on the\naforementioned evaluation criteria (i.e., number of reverse\nhealth-state jumps, uniformity, and health-state resolution)\nwere obtained when the HMM pool was initialized with 4\nHMMs. This selection process would of course vary from\napplication to application and should be based on subject\ndomain expertise and desired health-state resolution. In our\ncurrent application of monitoring drill-bits, 4 distinct health-\nstates are considered adequate, with the understanding that\nonce trained, HMMs can be used for health-state diagnostics\non-line to replace the drill-bit in a timely fashion (i.e.,\nreplacement at transition from the third health-state to the last\nhealth-state or failure state). After proper initialization, the\ncompetitive learning has been performed. As described in\nsection III.A.1, each trained HMM can be fully characterized\nby \uf028 \uf029, ,A B\uf06c \uf070\uf03d , where, ,{ }i jA \uf061\uf03d 1( | )t tP X i X j\uf02d\uf03d \uf03d \uf03d\ndenotes the state-transition matrix,\n{ ( )} ( | )i t tB b k P O k X i\uf03d \uf03d \uf03d \uf03d the observation distribution\ngiven the state, and 1( ) ( )i P X i\uf070 \uf03d \uf03d the initial state\ndistribution. All our studies adopted the standard Gaussian\nobservation process assumption during modeling.\nGiven that each HMM is initialized with 4 hidden or sub-\nstates, each trained HMM (supposedly representing a distinct\nhealth-state) yields 4 Gaussian distribution observation models\n(one for each hidden state). Fig. 12 plots the mean (denoted by\n\u2018+\u2019 sign) and co-variance structure of the observation\ndistributions given the hidden-states for all four of the health-\nstate HMMs that underwent the competitive learning process,\nsuper imposed on data from the complete life history of drill-\nbit #1. The orientation of the co-variance structure ellipse is\nbased on the covariance matrix, and is scaled to pass through\nthe 1\uf073\uf0b1 points on the two principal component axes. In each\nof the sub-plots, data sequences are also color coded based on\ntheir similarity to the distinct health-state HMMs (green for\ndata sequences that closely match the particular health-state\nHMM and yellow for those with low similarity \u2013 meaning\nsimilar to one of the other three health-state HMMs).\nAs can be seen from Fig. 13, the thrust-force and torque\nsignals in 2D space look similar to an ellipsoid. All sub-plots\nare showing data from all holes of drill-bit #1, however, with\ncolor coding. Holes represented (under competition) by each\nof the four distinct health-state HMMs are coded green in the\nsub-plots, with sup-plots (a), (b), (c), and (d) corresponding to\nhealth-states represented by HMM1, HMM4, HMM3, and\nHMM2 (from brand new state to failure) respectively. Each\nsub-plot also shows in red the centers as well as the constant\ndensity contours representing the covariance structures of the\nsub-health-states of the corresponding trained HMM. The\nlocation of the center of the ellipse corresponds to the mean\nvector of the observable bi-variate Gaussian density, and the\nmajor and minor axes of the constant density contours\nrepresent the eigen-vectors of the covariance matrix. For\nexample, Fig. 13(a) shows the four centers and the covariance\nstructure constant-density contours for HMM1. As drill-bit\nwears, the blunt cutting edges increase the thrust-force and\ntorque signal amplitudes. The green ellipsoid data pattern is\nsmall for HMM1 (see Fig. 13a), whereas it moves out\nproducing a bigger ellipsoid like pattern as the drill-bit gets\ncloser to failure health-state. Tracking the location of the four\nsub-states of the four 4 distinct HMMs, there is evidence that\nthe HMMs are attempting to represent different health-states\nor life stages of the drill-bit. The figure illustrates that the data\nmoves from the interior to the exterior as the drill-bit is used,\nand sub-states of HMMs can represent this movement.\n-2 0 2 4\n-2\n-1\n0\n1\n2\n3\n4\nTorque\nTh\nru\nst\nHMM 1\n-2 0 2 4\n-2\n-1\n0\n1\n2\n3\n4\nTorque\nTh\nru\nst\nHMM 4\n-2 0 2 4\n-2\n-1\n0\n1\n2\n3\n4\nTorque\nTh\nru\nst\nHMM 3\n-2 0 2 4\n-2\n-1\n0\n1\n2\n3\n4\nTorque\nTh\nru\nst\nHMM 2\ndata not represented by HMM\ndata represented by HMM\nMean given the state\nCovariance given the state\nFigure 13: Mean and co-variance structure of sub-states of the four health-\nstate HMMs superimposed on data from drill-bit #1.\nAfter the HMMs were trained, health-states of drill-bits for\nall holes are identified hole by hole. The data for the\ncorresponding hole is fed to all HMMs and the one with\nhighest log-likelihood value is defined as the representative of\nthe health-state, as illustrated in Fig. 5. The log-likelihood\nvalues of the four HMMs for drill-bit #1 are plotted in Fig. 14.\nThe x-axis gives the life of the drill-bits in terms of holes and\nthe y-axis gives the log-likelihood values for the four health-\nstate HMMs. As can be seen from the figure, the log-\nlikelihood values are highest for HMM1, HMM4, HMM3, and\nHMM2, as we go from brand new to failure states,\nrespectively. Meaning, HMM2 learned the \u2018failure state\u2019\nduring the competitive learning process, whereas HMM1\nlearned the \u2018perfectly healthy state\u2019. Note that we arbitrarily\nlabeled the randomly initialized HMMs in order from 1 to 4\nprior to the competitive learning process. This order, as\nindicated, is arbitrary and has no impact on the final solution.\n(a)\n(c) (d)\n12\nWhat matters is the result of the competitive learning process.\nAs indicated above, it is the log-likelihood plots from the\ntraining and testing cycles (past competitive learning) that\nreveal the true order of the health-states and the corresponding\nHMMs. It is not important if the failure state is represented by\nHMM1 or HMM2 as long as it is always represented by the\nsame HMM both during training phase and the testing phase,\nindicating successful learning.\n0 5 10 15 20 25\n-600\n-500\n-400\n-300\n-200\n-100\n0\nLife of Drill bit (holes)\nLo\ng-\nlik\nel\nih\noo\nd\nLog-likelihoods of HMMs for Drill bit 1\nHMM1\nHMM2\nHMM3\nHMM4\nFigure 14: Log-likelihood trajectories of the four health-state HMMs\npast competitive learning for data from drill-bit #1.\nThe health-state estimation results from competitive\nlearning are given in Table 1 for all twelve drill-bits. Each\ndrill-bit is represented in a row (total of 12 rows for 12 drill-\nbits) with various numbers of successfully drilled holes\nrepresented in columns. An empty cell denotes that the\ncorresponding drill-bit in the row failed before reaching the\ncorresponding hole in the column. The number in each cell\ndenotes the HMM number that represents the health of the\ndrill-bit in the corresponding row and hole in the\ncorresponding column. HMM1 represents the \u2018good\u2019 health-\nstate and HMM4, HMM3, and HMM2 represents health-states\ncloser to failure respectively, HMM2 representing the health-\nstate just before the failure-state. Different colors (gray levels\nin black-white print) are used for ease of visualization. As can\nbe seen from the table, the trained HMMs were successful in\nrepresenting the health-states without yielding any reverse\njumps, and in addition, majority of drill-bits visited or\nunderwent all the distinct health-states.\nIn general, the number of HMMs within the competitive\nlearning pool and the number of hidden-states allowed within\neach HMM should be optimized. Too many or too few HMM\nhidden-states lead to data over-fitting (hence poor\nperformance on the testing set) or poor performance (even\nwithin the training set), respectively [15]. For our drill-bit\nmonitoring application, it was noticed that when less than four\nstates are used, the reverse jumps increase (i.e., classification\naccuracy decreases), whereas when more than four HMMs are\nused, some of the HMMs either couldn\u2019t win any competition\nor tended to represent a part of a health-state. For example, in\nthe case of 5 HMMs used for training, both HMM 3 and 5\nrepresented the final failure state (i.e., the last hole of all drill-\nbits).\nTable 1: Health-state estimation\/labeling results for data from all holes of all\ndrill-bits using HMM pool competitive learning.\n(1: brand new, 4: used, 3::excessively used, 2: close to failure)\nHoles 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\nD\nri\nll-\nbi\nts\n1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 3 3 3 2\n2 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 3 2\n3 1 1 1 1 1 1 1 1 1 1 1 1 4 3 2\n4 1 1 1 1 1 1 2\n5 1 1 1 1 1 1 1 1 1 1 4 4 2\n6 1 1 1 1 1 1 1 2\n7 1 1 1 1 1 1 1 1 1 1 4 4 3 2\n8 1 1 1 1 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 3 3 2\n9 1 1 1 1 1 1 1 1 1 1 1 1 1 4 3 2\n10 1 1 1 1 1 1 1 1 2 2\n11 1 1 1 1 1 1 1 4 2\n12 1 1 1 1 1 1 1 1 1 4 4 4 4 4 3 3 2\nB. Hierarchical Hidden Markov Model for Health State\nEstimation\nHierarchical HMM (HHMM) is designed to handle complex\nsystems. We implemented a two-level HHMM with top-level\nstates that represent health-states with sub-states representing\nthe non-stationarity within the hole. HHMM gives us the\nopportunity to model all health-states by using a single overall\nmodel (it is enough to train one HHMM instead of employing\ncompetitive learning for a pool of HMMs). In addition, the\ntop-level state transition probabilities make the modeling of\ntransitions between health-states possible, leading to RUL\nestimation, something not directly possible when employing a\nHMM pool. Health-states in HMM pool are represented by\ndistinct HMMs, in which, transition from a health-state to\nanother (a HMM to another one) cannot be modeled.\nIn the designing phase of DBN, we set a two-level\nhierarchical structure: one level for health-states, other for\nnon-stationarity within the hole. The conditional probabilities\nare defined as in Eqns. (5)-(10). Note that the health-state of\nthe drill-bit should go forward (i.e., deteriorate) not backwards\n(e.g., transition from \u2018torn out\u2019 state to \u2018brand new\u2019 state is not\nallowed). Thus, we implemented a \u2018left-to-right\u2019 HHMM,\nwhich allows only forward state transitions in order to\nrepresent health-state progression, with appropriate initial\ntransition probabilities.\nThe mean and variance parameters ( 2, ,,i j i j\uf06d \uf073 ) of the\nobservation distribution given the hidden states (see Eqn.\n(10)), representing thrust-force and torque signals, are learnt\nduring training by passing training data as in Fig. 10.a.\nNumber of mean and variance parameters to be learned for the\ndifferent observation distributions is the product of number of\nhealth-states and number of sub-states under each health-state.\nFor example, if 4 health-states are identified with 5 sub-states\nunder each health-state, there will be 20 mean vectors and 20\ncovariance matrices. Fig. 14 illustrates the projection of mean\nand covariance matrices of observation state given 4 health-\nstates and 5 sub-states within the HHMM. Each graph in the\nfigure (total 4) represents a health-state. Five mean and\ncontour plots are displayed in each graph representing the sub-\nstates under given health-state. As can be seen from the figure,\nnot unlike the HMM pool case, the data moves from interior to\nthe exterior as drill-bit is used and sub-states of HHMM can\neffectively represent this movement.\n13\nIn addition to mean and variance parameters of observed\nvariables, the conditional probabilities (Eqns. (5)-(9)) are also\nlearnt during training. Then, the top state probabilities given\nthe data sequence of holes are obtained by inference as in Fig.\n10.b. Most likely top state sequence is calculated as the health-\nstates given the observation sequence (2nd basic problem\nmentioned in HMM section III.A.1). Fig. 15 displays the\nlikelihood values for HHMMs with four states at the top-level\nfor a drill-bit. As can be seen from the figures, using the one\ntrained HHMM, the health-states can be identified as \u2018brand\nnew\u2019, \u2018used\u2019, and \u2018excessively used\u2019 and \u2018close to failure\u2019\nstates. The probabilities are more distinguishable compared to\nregular HMMs with competitive learning displayed in Fig. 13.\n-2 -1 0 1 2\n-2\n-1\n0\n1\n2\nTorque\nTh\nru\nst\nHealth State 1\n-2 -1 0 1 2\n-2\n-1\n0\n1\n2\nTorque\nTh\nru\nst\nHealth State 2\n-2 -1 0 1 2\n-2\n-1\n0\n1\n2\nTorque\nTh\nru\nst\nHealth State 3\n-2 -1 0 1 2\n-2\n-1\n0\n1\n2\nTorque\nTh\nru\nst\nHealth State 4\ndata represented by health state\ndata not represented by health state\nmean given hidden states\ncovariance given hidden states\nFigure 15: Mean and co-variance structure of sub-states of HHMM\nsuperimposed on data from drill-bit #1.\n0 2 4 6 8 10 12 14 16 18\n0\n0.5\n1\nLife of Drill bit (Holes)\nLi\nke\nlih\noo\nd\nExample of Health State Likelihoods\nClose to Failure State\nUsed State\nBrand New State\nLike New State\nFigure 16: Log-likelihood trajectories of top-level (health) states #4 of\nHHMM for data from drill-bit #12.\nThe health-state of a drill-bit for a given hole data is\nidentified as the top state of the HHMM with highest\nlikelihood value. Table 2 gives the health-state estimation of\nall 12 drill-bits displayed in rows and various numbers of\nholes shown in columns for HHMM with five health-states.\nNumber in each cell represents the top level state value\n(health-state) in the corresponding drill-bit and hole. Health-\nstate #4 is represented only in three drill-bits. Thus, health-\nstates #4 and #5 can be combined into one health-state. As can\nbe seen from the table, health-state estimation results seem\nbetter with HHMM when compared to the case of employing a\ncommittee of HMMs, since almost all drill-bits visit all the\nhealth-states. There are once again no reverse jumps.\nTable 2: Health-state estimation\/labeling results for data from all holes\nof all drill-bits using a HHMM\n(1: brand new, 2: like new, 3: moderately used,\n4: excessively used, 5: close to failure)\nHoles 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\nDr\nill-\nbi\nts\n1 1 1 1 1 1 1 1 1 1 1 2 2 2 3 3 3 3 3 3 3 4 5\n2 1 1 1 1 1 1 1 1 2 2 2 2 2 2 3 3 3 5\n3 1 1 1 1 1 1 1 1 2 2 2 2 3 3 5\n4 1 1 1 1 2 3 5\n5 1 1 1 1 1 1 1 1 2 2 2 3 5\n6 1 1 1 1 1 2 3 5\n7 1 1 1 1 1 1 1 2 2 2 2 3 3 5\n8 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 4 5\n9 1 1 1 1 1 1 1 1 1 1 2 2 2 2 3 5\n10 1 1 1 1 1 1 1 2 3 5\n11 1 1 1 1 1 1 2 3 5\n12 1 1 1 1 1 2 2 2 2 2 2 2 3 3 3 4 5\nIn trying to optimize the structure of HHMM, the general\nprinciple of Occam\u2019s razor is once again relevant. We tried\ndifferent numbers of health-states from three through six.\nWhile we prefer high health-state resolution, as the number of\ntop-level states (health-states) increases, some of the states\nmight not represent distinct health-states. Therefore, we will\nchoose the highest number of top-level states (health-states),\neach of which will be represented consistently within the life\nof the different drill-bits used for training.\nThe second parameter to be optimized is the number of sub-\nstates, which identifies the number of states within a hole. In\nour application, experimental evaluation suggests that five\nsub-states are adequate to represent the data from a hole. This\nis optimized with trial-error starting with two and increased\none-by-one. The maximum number of states with non-\noverlapping mean and co-variance structure contours is\nconsidered as optimum.\nThe training computational time for HHMM varied between\n174 and 255 seconds depending on the number of states used\nin top and lower levels. In our experiments, this was\ncomparable to regular HMM committees, which takes around\n212 seconds. Careful comparison of HMMs trained using\ncompetitive learning for health-state estimation versus using a\nsingle hierarchical HMM revealed the following advantages\nfor using HHMM:\n1.) Better Classification: HHMM gives better health-state\nestimation than regular HMM (compare Tables 1 and\n2).\n2.) Training: No need for competitive learning. It is\nenough to train just one HHMM. Initialization of sub-\nstate HHMs is not required for HHMM.\n3.) Topological Ordering: It naturally forces topological\nstructure, which minimizes reverse jumps.\n4.) Transition Probability: HHMM automatically\ncalculates the transition probabilities between health-\nstates (i.e. top level states), which regular HMM pool\n14\nlacks. Transition probabilities between health-states are\nimportant for RUL calculation (i.e., prognostics).\nThe next sub-section discusses a case study of RUL\nestimation using HHMMs.\nC. RUL Estimation Using HHMM\nThe following section builds on the results previously\nreported in Section IV. Here we employ the proposed Monte-\nCarlo approach to estimate on-line the RUL of drill-bits used\non the CNC drilling machine. Here we define RUL as the\npotential number of holes to be successfully drilled before\nfailure. All results reported in this study are based on Monte-\nCarlo simulation sample size of 10,000 (adequate given the\nMTTF of a brand new drill-bit under the stated operating\nconditions is under 20 holes). During each of the 10,000\nsimulation runs, next health-state is estimated based on the\ntransition probabilities (say 0.5 to state #2, 0.3 to state #3, and\n0.2 to state #4) by generating a uniformly distributed random\nnumber between 0 and 1. The location of the random number\n(0-0.5, 0.5-0.8, and 0.8-1.0) identifies the next state (#2, #3,\nand #4). This process is repeated considering the calculated\nnext state as the current state until the failure-state is reached.\nThen, the number of transitions is counted as the RUL value,\nsince each transition represents a hole. This is repeated for all\nsamples yielding 10,000 RUL values.\nProbability of RUL being a value (say r ) is calculated as\nthe ratio of number of r \u2018s obtained in RUL calculation to the\nsimulation sample size (10,000 in our case).\nRUL probability distribution is calculated for each hole of\nall twelve drill-bits assuming the corresponding hole as the\nmost recent hole drilled. For illustration purposes, RUL\nprobability distributions of drill-bit #2, which failed in the 17th\nhole, for several holes (holes 1, 3, 6, 9, 12, 13, 14, 15, 16) are\ngiven in Fig. 17. The x-axis displays the overall life of the\ndrill-bit (in holes), which is the sum of the RUL estimate and\nthe total number of holes already drilled (i.e., expected total\nnumber of holes to be drilled), and the y-axis displays\nprobability. The arrow within each graph shows the actual life\nof the drill-bit, 17 for the given example (i.e., drill-bit #2). As\nillustrated in the graphs, the variance of RUL probability\ndistribution decreases and the accuracy of RUL estimation\nincreases as the drill-bit approaches failure. In all drill-bits, the\nproposed method successfully estimates the RUL as zero with\n100% accuracy in the hole just prior to failure.\nFigure 17: Estimated RUL probability mass function\nat different stages of the life of drill-bit #2.\n0 5 10 15\n0\n5\n10\n15\nDrill bit 3\nMean RUL\nReal RUL\nRUL Lower Limit with 80% confidence\nRUL Upper Limit with 80% confidence\n(a) Plot of estimated RUL and prediction limits (80% confidence)\nalong with true RUL for drill-bit #3\n0 5 10 15 20\n0\n10\n20 Drill bit 1\n0 5 10 15\n0\n10\nDrill bit 2\n0 5 10 15\n0\n10 Drill bit 3\n0 2 4 6 8\n0\n5\nDrill bit 4\n0 5 10\n0\n5\n10 Drill bit 5\n0 2 4 6 8\n0\n5\nDrill bit 6\n0 5 10 15\n0\n5\n10\n15\nDrill bit 7\n0 5 10 15 20 25\n0\n10\n20 Drill bit 8\n0 5 10 15\n0\n10\nDrill bit 9\n0 5 10\n0\n5\n10\nDrill bit 10\n0 5 10\n0\n5\n10\nDrill bit 11\n0 5 10 15\n0\n10\nDrill bit 12\nRUL Estimation\nReal RUL\n(b) Plots of estimated RUL along with true RUL for all 12 drill-bits\nFigure 18: Estimated and true RUL for drill-bits.\nFig. 18.a reports the estimated RUL and prediction limits\n(80% confidence) along with true RUL for drill-bit #3. Fig.\n18.b reports the estimated RUL and prediction limits (80%\nconfidence) along with true RUL for all 12 drill-bits (each sub\ngraph is for a drill-bit). The x-axis represents the time (hole) at\nwhich the RUL is estimated and the y-axis the estimated RUL.\nThe dashed linear line is the true RUL value. The solid line is\nthe mean of the estimated RUL values. As it is more effective\nto report the RUL as a range rather than a singular value, we\nalso report the estimated prediction limits calculated with 80%\nconfidence (outer dotted lines in Fig. 18.a). Prediction limit is\nthe minimum range of RUL values that compose the\nconfidence percentage of the whole sample.\nThe prediction limits are identified as shown in (12):\nMe\nan\nof\nEs\ntim\nat\ned\nRU\nL\nLife of drill-bit (holes)\nPr\nob\nab\nili\nty\nEstimated life of drill-bit (holes)\nTrue RUL (# of\nholes before\nfailure)\nProbability of\nRemaining # of\nholes\nMe\nan\nof\nEs\ntim\nat\ned\nRU\nL\nLife of drill-bit (holes)\n15\n\uf028 \uf029\n\uf028 \uf029# b r e\nP b r e cf\ns\n\uf0a3 \uf0a3\n\uf0a3 \uf0a3 \uf03d \uf0b3 (12)\nwhere, r denotes the estimated RUL value, e and b denote\nthe upper and lower prediction limits, respectively, cf the\ndesired confidence, s the sample size (i.e., the total number of\nRUL estimations), and #( )b r e\uf0a3 \uf0a3 the number of RUL\nvalues between e and .b The width of the prediction limits is\nsimply, .e b\uf03d \uf02d\uf06c Consider that there exist 10 RUL\nestimations (e.g., 7, 8, 6, 7, 7, 5, 6, 4, 6, 5) obtained from\nsimulation. For 80% confidence, the width of the prediction\nlimit is 7-5=2, since eight out of ten are within the prediction\nlimit (7 and 5). To avoid clutter, the prediction limits for the\n12 drill-bits are not displayed in Fig. 18.\nThus, the expected output from the proposed prognostic\nmodule includes prediction limits and confidence that leads to\nan expression such as \u201cRUL is between 4 and 6 holes with\n95% confidence\u201d. Given a certain confidence, the narrower\nthe prediction limits (smaller its width) the higher the\nprecision and the more useful the RUL estimation. Fig. 19\ndisplays the width of prediction limit results of the proposed\nprognostic module. As expected, at the very early stages of\ndrill-bit use, the precision is poor (i.e., width of the prediction\nlimits is high). As the drill-bit approaches failure, the precision\nimproves (i.e., width of the prediction limit decreases) and\nestimation becomes more valuable.\n0 5 10 15 20\n0\n20\n40\nDrill bit 1\n0 5 10 15\n0\n20\n40\nDrill bit 2\n0 5 10 15\n0\n20\n40\nDrill bit 3\n0 2 4 6\n0\n20\n40\nDrill bit 4\n0 5 10\n0\n20\n40\nDrill bit 5\n0 2 4 6 8\n0\n20\n40\nDrill bit 6\n0 5 10\n0\n20\n40\nDrill bit 7\n0 5 10 15 20\n0\n20\n40\nDrill bit 8\n0 5 10 15\n0\n20\n40\nDrill bit 9\n0 5 10\n0\n20\n40\nDrill bit 10\n0 2 4 6 8\n0\n20\n40\nDrill bit 11\n0 5 10 15\n0\n20\n40\nDrill bit 12\nPrediction Limit Length\nFigure 19: Width of RUL prediction limits for all twelve drill-bits\nbased on 95% confidence.\nIn the proposed prognostic method, it is assumed that the\nhealth-state transition can only occur between holes. In other\nwords, a drill-bit cannot be partially in one health-state and\njump to the other health-state during the same hole. This is a\nreasonable assumption and necessary for RUL calculation,\nwhich leads to integer RUL values. In calculating width of the\nprediction limits, the target confidence is used as the minimum\nacceptable confidence threshold. For example, if the\nconfidence values of RUL being between \u20184 and 6 holes\u2019 and\n\u20184 and 7 holes\u2019 are 93% and 97%, respectively, then the\nproposed method will choose the latter for calculating\nprecision if the required confidence threshold is 95%.\nWhile the above discussion pertains to the quality of RUL\n\u2018interval estimates\u2019 available from the Monte-Carlo procedure,\none could also assess the quality of the \u2018point estimate\u2019 (i.e.,\nthe mean of the RUL frequency distribution). Accuracy,\ndenoted acc , evaluates the quality of point estimate, and is\ncalculated as the ratio of number of RUL estimations that\nexactly match the true RUL to the sample size of RUL\nestimations, as indicated in (10):\n\uf028 \uf029\n\uf028 \uf029# r actr\nacc P r actr\ns\n\uf03d\n\uf03d \uf03d \uf03d (13)\nwhere actr denotes the true or actual RUL value, #( )r actr\uf03d\ndenotes the number of RUL values that match actr , and\n( )P r actr\uf03d the probability of r being equal to .actr\nConsider that there exist 10 RUL estimations (e.g., 7, 8, 6,\n7, 7, 5, 6, 4, 6, 5) obtained from simulation. If the real RUL\nvalue is 6, then the accuracy is 0.3, since there are 3 RUL\nestimates that match the true RUL over a set of 10 predictions.\nFig. 20 reports these accuracies for the drilling case study,\nwhich is basically the probability that the point estimate\nmatches the actual RUL. It is evident from the plots that the\nestimation accuracy continuously improves as drill-bits\napproach failure. The two minor exceptions pertain to drill-\nbits #4 and #10, which only lasted 7 and 9 holes, respectively.\nTheir short lives may be an indication of lack of consistent\ndegradation prior to failure. However, note that the RUL is\nestimated as 0 with 100% confidence just prior to failure in all\ncases.\n0 5 10 15 20\n0\n0.5\n1\nDrill bit 1\n0 5 10 15\n0\n0.5\n1\nDrill bit 2\n0 5 10 15\n0\n0.5\n1\nDrill bit 3\n0 2 4 6\n0\n0.5\n1\nDrill bit 4\n0 5 10\n0\n0.5\n1\nDrill bit 5\n0 2 4 6 8\n0\n0.5\n1\nDrill bit 6\n0 5 10\n0\n0.5\n1\nDrill bit 7\n0 5 10 15 20\n0\n0.5\n1\nDrill bit 8\n0 5 10 15\n0\n0.5\n1\nDrill bit 9\n0 5 10\n0\n0.5\n1\nDrill bit 10\n0 2 4 6 8\n0\n0.5\n1\nDrill bit 11\n0 5 10 15\n0\n0.5\n1\nDrill bit 12\nRUL Accuracy\nFigure 20: RUL estimation accuracy for all twelve drill-bits.\nIn order to compare the RUL estimation results with results\nfrom [12], circular cross validation procedure is employed.\nTwelve drill-bits are separated into training and testing data as\nillustrated in Table 3. Tables 4 and 5 display average R2 and\nmedian RMSE (Root Mean Square Error) of the presented\nmethod. Table 6 compares the results with results in [12].\nNote that columns in Tables 4 and 5 represent the drill-bit\nused as nth training or testing drill-bit, not necessarily the nth\ndrill-bit in Table 3.\nThe results from employing methods 2 and 3 presented in\n[12] are reported as mean R-square and median RMSE in\nTable 6. The result of method 1 in [12] is reported as a figure\nthat displays the transition from health-state good to medium\nand from medium to bad, not in the form of R-square and\nRMSE. Given that method 1 is seen to be inferior to methods\n2 and 3 of [12], we limit our comparisons to methods 2 and 3.\nTable 3: Circular cross-validation training and testing datasets\nDB # 1 2 3 4 5 6 7 8 9 10 11 12 # Trn\nDBs\n# Tst\nDBs\n#Trn\nHoles\n#Tst\nHoles# Holes 22 18 15 7 13 8 14 24 7 10 9 17\nSet 1 1 1 1 1 1 1 1 1 1 0 0 0 9 3 128 36\nSet 2 0 1 1 1 1 1 1 1 1 1 0 0 9 3 116 48\nSet 3 0 0 1 1 1 1 1 1 1 1 1 0 9 3 107 57\nSet 4 0 0 0 1 1 1 1 1 1 1 1 1 9 3 109 55\nRU\nL\nEs\ntim\nat\nio\nn\nAc\ncu\nra\ncy\nLife of drill-bit (holes)\nRU\nL\nPr\ned\nic\ntio\nn\nLi\nm\nit\nLe\nng\nth\nLife of drill-bit (holes)\n16\nSet 5 1 0 0 0 1 1 1 1 1 1 1 1 9 3 124 40\nSet 6 1 1 0 0 0 1 1 1 1 1 1 1 9 3 129 35\nSet 7 1 1 1 0 0 0 1 1 1 1 1 1 9 3 136 28\nSet 8 1 1 1 1 0 0 0 1 1 1 1 1 9 3 129 35\nSet 9 1 1 1 1 1 0 0 0 1 1 1 1 9 3 118 46\nSet 10 1 1 1 1 1 1 0 0 0 1 1 1 9 3 119 45\nSet 11 1 1 1 1 1 1 1 0 0 0 1 1 9 3 123 41\nSet 12 1 1 1 1 1 1 1 1 0 0 0 1 9 3 138 26\nTable 4: Average R2 for training and testing sets\nTraining Set Testing Set\nDB # 1 2 3 4 5 6 7 8 9 1 2 3\nSet 1 0.82 0.88 0.81 0.94 0.94 0.84 0.91 0.84 0.83 0.93 0.90 0.91\nSet 2 0.88 0.91 0.88 0.92 0.94 0.91 0.95 0.92 0.83 0.94 0.90 0.90\nSet 3 0.96 0.86 0.91 0.87 0.90 0.96 0.94 0.94 0.90 0.83 0.86 0.97\nSet 4 0.96 0.92 0.92 0.89 0.84 0.96 0.93 0.92 0.95 0.95 0.77 0.92\nSet 5 0.98 0.96 0.96 0.87 0.90 0.92 0.92 0.96 0.89 0.97 0.82 0.82\nSet 6 0.87 0.97 0.93 0.90 0.88 0.84 0.89 0.89 0.97 0.90 0.89 0.91\nSet 7 0.94 0.86 0.88 0.98 0.92 0.85 0.90 0.93 0.91 0.90 0.90 0.99\nSet 8 0.94 0.89 0.83 0.83 0.95 0.95 0.89 0.88 0.87 0.91 0.91 0.92\nSet 9 0.96 0.97 0.92 0.77 0.96 0.97 0.97 0.91 0.84 0.93 0.92 0.95\nSet 10 0.95 0.90 0.97 0.93 0.83 0.98 0.98 0.89 0.91 0.88 0.86 0.91\nSet 11 0.90 0.94 0.92 1.00 0.94 0.75 0.95 0.97 0.99 0.88 0.86 0.85\nSet 12 0.89 0.86 0.94 0.89 0.93 0.97 0.73 0.96 0.97 0.90 0.91 0.86\nTable 5: Median RMSE for training and testing sets\nTraining Set Testing Set\nDB # 1 2 3 4 5 6 7 8 9 1 2 3\nSet 1 4.17 3.33 1.75 6.38 2.21 6.15 0.79 4.85 6.71 4.48 4.65 2.80\nSet 2 2.64 4.28 3.17 1.72 6.79 2.09 6.41 1.29 5.61 6.75 4.37 4.95\nSet 3 4.05 3.29 4.81 3.2 2.05 5.11 0.81 4.62 1.59 6.79 5.50 3.18\nSet 4 2.79 3.53 3.30 5.36 3.48 2.45 4.97 1.37 4.06 1.70 6.32 4.98\nSet 5 6.25 4.04 4.84 3.19 4.71 3.33 2.16 6.00 1.10 5.46 0.86 6.12\nSet 6 5.63 6.94 4.70 5.70 2.93 4.34 3.01 1.10 6.69 2.05 5.81 1.09\nSet 7 2.93 4.62 8.63 5.82 6.83 2.09 4.17 2.32 1.30 8.08 3.36 7.86\nSet 8 6.18 1.25 5.79 7.00 4.60 5.86 2.48 4.37 3.41 1.24 6.89 2.38\nSet 9 1.83 6.10 1.85 5.55 6.75 4.49 4.98 2.65 4.49 3.00 1.30 6.76\nSet 10 5.05 1.42 4.36 1.55 6.81 5.05 3.06 3.59 2.73 5.92 3.15 2.52\nSet 11 2.28 4.95 1.63 4.32 1.72 7.18 5.20 2.57 3.59 2.89 5.80 3.43\nSet 12 3.35 1.93 5.68 0.70 4.99 1.79 5.78 5.76 4.01 3.53 2.96 5.04\nTable 6: Comparison of presented method with methods from [12]:\nTR: Training, TST: Testing\nR Square RMSE\naverage worst best median\nHMM\nin [12]\nMethod1 N\/A N\/A N\/A N\/A\nMethod2 0.44 0.24 0.73 6.28\nMethod3 0.53 0.3 0.79 5.74\nPresented\nmethod HHMM\nTR TST 0.73 0.996 TR TST0.91 0.90 3.78 4.03\nAs can be seen from the tables above, the proposed method\nhighly outperforms the HMM models presented in [12]. Even\nthe worst R-square result from the presented method is better\nthan the average R-Square result of methods from [12]. Note\nhowever that R-square value equal to 1 does not necessarily\nindicate 100% accuracy and 100% confidence (mentioned\nbefore in eq. 12 and 13), but just indicative of the linear\ncorrelation between two prediction data sets. This can be seen\nin Set 11 and training drill-bit 4 (11th row and 4th column in\nTable 4 and 5) as R-square value being close to 1 and median\nRMSE value being 4.3 holes.\nBesides diagnostic performance, RUL estimation accuracies\n(both point as well as interval estimates) should also influence\nthe optimization of the HHMM structure (i.e., number of\nhealth-states and sub-states) to be used in HHMM. For the\ncurrent case study, best results are obtained with four health-\nstates and four sub-states for each health-state. Overall, from\nthe results, we can conclude that the proposed HHMM\napproach seems rather effective in supporting diagnostics and\nprognostics needs of the drilling process.\nV. CONCLUSION AND FUTURE RESEARCH\nWe implemented regular and hierarchical HMMs as\ndynamic Bayesian networks (DBNs) for health-state and\nremaining-useful-life (RUL) estimation. Casting HMMs as\nDBNs offers compact representation as well as additional\nflexibility with respect to the structure of the model.\nCompetitive learning is employed for training pools of regular\nHMMs, each of which represents a distinct health-state.\nAlthough regular HMMs seem to give reasonable diagnostic\naccuracy, if diagnostic accuracy is defined simply as a\nfunction of number of reverse jumps among the health-states,\nthey pose implementation difficulties such as long training\ntimes and the inability to facilitate calculation of RUL. On the\nother hand, a single hierarchical HMM can naturally represent\nall health-states and offer several advantages over pools of\nstandard HMMs, such as better diagnostic accuracy and ease\nof implementation and training. Additionally, hierarchical\nHMM also directly captures health-state transition\nprobabilities, which are a prerequisite for health-state\nprognostics (i.e., RUL estimation). Also discussed was the\nmethod used for RUL estimation that employs Monte Carlo\nsimulation with HHMM. The proposed methods are\nimplemented for monitoring drill-bits on a CNC machine. The\nresults are very promising for development of effective\nmethods for diagnostics and prognostics (in particular RUL\nestimation), even in the absence of a history of labeled\nexamples or cases. The proposed method also significantly\noutperforms other HMM based methods from the literature.\nFuture research will attempt to explicitly introduce state-\nduration densities into the HHMM model to overcome the\nneed for the assumption of exponential duration density. In\naddition, HHMM structures that are more flexible will be\nattempted to facilitate the monitoring of systems that exhibit\nmultiple and sometimes interacting failure modes.\nACKNOWLEDGEMENTS\nThe authors thank the editors and referees for their careful\nreview and feedback, which substantially improved this paper.\nREFERENCES\n[1] F. Camci, Autonomous, Process Monitoring, Diagnostics, and\nPrognostics Using Support Vector Machines and Hidden Markov\nModels, PHD Dissertation, Wayne State University, 2005\n[2] Harbor Research Pervasive Internet Report, Approaching Zero\nDowntime: The Center for Intelligent Maintenance Systems, April\n2003\n[3] C. Kwan, X. Zhang, R. Xu, L. Haynes, A novel approach to fault\ndiagnostics and prognostics, Proceedings of ICRA '03: IEEE\nInternational Conference on Robotics and Automation. 1(3),\nSeptember 2003, 604-609\n[4] NIST-ATP CBM Workshop Report, NIST-ATP Workshop on\nCondition-Based Maintenance, Atlanta, GA, November 1998\n(http:\/\/www.atp.nist.gov\/www\/cbm\/cbm_wp1.htm)\n[5] K. Maynard, C. S. Byington, G. W. Nickerson, and M. V. Dyke,\nValidation of Helicopter Nominal and Faulted Conditions Using Fleet\nData sets, Proceedings of the International Conference on Condition\nMonitoring, UK, 1999 129 \u2013141\n17\n[6] S. J. Engel, B. J. Gilmartin, K. Bongort, A. Hess, Prognostics, The\nReal Issues Involved With Predicting Life Remaining, IEEE\nAerospace Conference Proceedings, 6, 2000, 457-469.\n[7] M.J. Roemer and G.J. Kacprzynski, Advanced diagnostics and\nprognostics for gas turbine risk assessment, Proceedings of the 2000\nIEEE Aerospace Conference, Big Sky, Montana, March 18-25, 2000.\n[8] I. Sanches, Noise-compensated hidden Markov models, IEEE\nTransactions on Speech and Audio Processing, 8(5), Sep 2000, 533-\n540\n[9] B.D. Womack, J.H.L. Hansen, N-channel hidden Markov models for\ncombined stressed speech classification and recognition, IEEE\nTransactions on Speech and Audio Processing, 7(6), Nov 1999, 668-\n677\n[10] C. Bunks, D. McCarthy, T. Al Ani, Condition-based maintenance of\nmachines using hidden Markov model, Mechanical Systems and\nSignal Processing, 14(4), July 2000, 597-612\n[11] R. Dickman, R. Vidigal, Quasi-stationary distributions for stochastic\nprocesses with an absorbing state, Journal of Physics A:\nMathematical and General, (35), 2002, 1147-1166\n[12] R. B. Chinnam, P. Baruah, Autonomous diagnostics and prognostics\nin machining processes through competitive learning-driven HMM-\nbased clustering, International Journal of Production Research, 1-20,\nin press, iFirst, doi: 10.1080\/00207540802232930\n[13] M. Dong, D. He, A segmental hidden semi-Markov model (HSMM)\n-based diagnostics and prognostics framework and methodology,\nMechanical Systems and Signal Processing, 21 (2007), 2248-2266\n[14] M. Dong, A novel approach to equipment health management based\non auto-regressive hidden semi-Markov model (AR-HSMM), Science\nIn China Series F-Information Sciences , 51(9) (2008), pp. 1291-\n1304\n[15] R. B. Chinnam, P. Baruah, Autonomous diagnostics and prognostics\nthrough competitive learning driven HMM-based clustering,\nProceedings of the International Joint Conference on Neural\nNetworks, July 2003, 2466- 2471\n[16] P. Baruah, P and R.B. Chinnam, R.B., HMMs for diagnostics and\nprognostics in machining processes, International Journal of\nProduction Research, 43(6), March 2005, 1275-1293\n[17] L. Fu, Ling S., Neural Network Based Online Detection of Drill\nBreakage in Micro Drilling Process, Proc. 9th International\nConference on Neural Information Processing (ICONIP\u201902), 2002\npp. 2054-2058\n[18] H. M. Ertunc, K. A. Loparo, E. Ozdemir, H. Ocak, Real Time\nMonitoring of Tool Wear Using Multiple Modeling Method, Proc.\nIEEE International Conference Electric Machines and Drives, 2001.\nIEMDC 2001. pp. 687- 691\n[19] R. J. Furness, T. Tsao, J. S. Rankin, M. J. Muth, and K. W. Manes\nTorque control for a form tool drilling operation, IEEE Transactions\non Control Systems and Technology, Vol. 7, 1999, pp. 22-30\n[20] K. Subramanian and N. H. Cook, Sensing of drill wear and prediction\nof drill life, ASME Journal of Engineering for Industry, Vol. 99, pp.\n295-301, 1977\n[21] S. Y. Hong, Knowledge-based diagnosis of drill conditions, Journal\nof Intelligent Manufacturig, vol. 4, pp. 233-241 1993\n[22] E. Jantunen, A summary of methods applied to tool condition\nmonitoring in drilling, International Journal of Machine Tools &\nManufacture, 42 (2002), pp. 997-1010\n[23] S. Haykin, Neural networks : A comprehensive foundation, 2nd Ed.,\nUpper Saddle River, New York: Prentice Hall, 1999.\n[24] R. Lawrence, A tutorial on Hidden Markov Models and Selected\nApplications in Speech Recognition, Proceedings of IEEE 77(2),\n1989, 257-286\n[25] J. Blimes., Data-driven extensions to HMM statistical dependencies.\nInternational Conference on Speech Language Processing, 1998.\n[26] J. Blimes., What HMMs can do, Technical Report: UWEETR-2002-\n03, 2002.\n[27] A.B. Poritz., Hidden Markov Models: A Guided Tour, ICASSP, 1988\n[28] S. Fine, Y. Singer, and N. Tishby, The hierarchical hidden Markov\nmodel: Analysis and applications, Machine Learning 32, 1998, 41-62.\n[29] Wikipedia Contributors, Hierarchical hidden Markov model,\nWikipedia, The Free Encyclopedia, 8 April 2008, 20:59 UTC,\n<http:\/\/en.wikipedia.org\/w\/index.php?title=Hierarchical_hidden_Mar\nkov_model&oldid=204300061> [accessed 8 April 2008]\n[30] P. Smyth, Belief networks, hidden Markov models, and Markov\nrandom fields: a unifying view, Pattern Recognition Letters 18(11-\n13), 1998, 1261-1268\n[31] S. Roweis & Z. Ghahramani, A Unifying Review of Linear Gaussian\nModels, Neural Computation 11(2), 1999, 305-345\n[32] T. Dean and K. Kanazawa, A model for reasoning about persistence\nand causation. Artificial Intelligence, 93(1\u20132), 1\u201327, 1989\n[33] K. Murphy, Dynamic Bayesian Network: Representation, Inference,\nand Learning, PhD. Dissertation, University of California, Berkeley,\n2002.\n[34] K. P. Murphy, The Bayes Net Toolbox for Matlab, Computing\nScience and Statistics, 33, 2001.\n[35] C. Begg, T. Merdes, C.S. Byington, and K.P. Maynard, Mechanical\nSystem Modeling for Failure Diagnostics and Prognosis,\nMaintainability and Reliability Conference (MARCON 99),\nGatlinburg, May 1999\n[36] G. J. Kacprzynski et al., Enhancement of Physics-of-Failure\nPrognostic Models with System Level Features, Proceedings of the\n2000 IEEE Aerospace Conference, Big Sky, MT, March 2002.\n[37] C. Joseph Lu and William Q. Meeker, Using Degradation Measures\nto Estimate a Time-to-Failure Distribution, Technometrics, 35(2),\n1993, pp. 161-174\n[38] J. Lawless and M. Crowder, Covariates and Random Effects in a\nGamma Process Model with Application to Degradation and Failure,\nJournal Lifetime Data Analysis, 10(3), (2004), pp. 213-227\n[39] C.S. Byington, M.J. Roemer, and T. Galie, Prognostic Enhancements\nto Diagnostic Systems for Improved Condition-Based Maintenance,\nProceedings of the 2000 IEEE Aerospace Conference, Big Sky, MT,\nMarch 2002.\n[40] C.S. Byington and A.K. Garga, Data Fusion for Developing\nPredictive Diagnostics for Electromechanical Systems, Handbook of\nMultisensor Data Fusion, D.L. Hall and J. Llinas eds., CRC Press,\nFL: Boca Raton, 2001.\n[41] J.P. Cusumano, D. Chelidze, and N.K. Hecht, Using phase space\nreconstruction of tract parameter drift in a nonlinear system,\nProceedings of the ASME 16th Biennial Conference on Mechanical\nVibrations and Noise, Symposium on Time-Varying Systems and\nStructures, September 14-17, 1997.\n[42] A. Ray, S. Tangirala, Stochastic Modeling of Fatigue Crack\nDynamics for Online Failure Prognostics, IEEE Transactions on\nControl Systems Technology, 4(4), July 1996 443-451.\n[43] Zhengguo Xu Yindong Ji Donghua Zhou, Real-time Reliability\nPrediction for a Dynamic System Based on the Hidden Degradation\nProcess Identification, IEEE Transactions on Reliability, 57(2), 2008,\npp. 230-242\n[44] L.R. Rabiner, A tutorial on hidden Markov models and selected\napplications in speech recognition, Proceedings of IEEE, 1989, 77,\n257-285.\n[45] D.J.C. MacKay, Information Theory, Inference and Learning\nAlgorithms, 2003 (Cambridge University Press).\nFatih Camci is an Assistant Professor in the Department of\nComputer Engineering at Fatih University, Istanbul\nTurkey. He worked as senior project engineer at Impact\nTechnologies in Rochester, NY for two years with\nexperience in the development of diagnostic\/prognostic\nstrategies for naval ship systems and military aircraft applications. Camci\nreceived his PhD in Industrial Engineering from Wayne State University. He\nreceived his B.S. degree in Computer Engineering from Istanbul University\n(Turkey) and M.S. degree in Computer Engineering from Fatih University\n(Turkey). He has been involved in development of optimum maintenance\nscheduling tools and diagnostics\/prognostic methods for naval ship systems.\nHis expertise includes computational intelligence methods such as neural\nnetworks, fuzzy logic, support vector machines, etc, and optimization methods\nsuch as linear, quadratic optimization, and genetic algorithms, etc.\nRatna Babu Chinnam is an Associate Professor in the\nDepartment of Industrial & Manufacturing Engineering at\nWayne State University (U.S.A.). He received his B.S.\ndegree in Mechanical Engineering from Mangalore\nUniversity (India) in 1988 and the M.S. and Ph.D. degrees\nin Industrial Engineering from Texas Tech University\n(U.S.A.) in 1990 and 1994, respectively. He is the author of\nover 75 technical publications in the areas of Intelligent Quality Engineering,\n18\nCondition-Based Maintenance Systems, Operations Management, and\nComputational Intelligence. He is currently the associate editor for the\nInternational Journal of Modeling and Simulation. He regularly serves on\nseveral international conference planning committees such as IJCNN, ANNIE,\nand IASTED\u2019s NIC. His past research is mostly funded by such agencies as\nthe National Science Foundation. He carried out extensive collaborative\nresearch with Ford Motor Company, DaimlerChrysler, Chrysler, General\nDynamics, and consulted for such companies as Sirius, Energy Conversion\nDevices, and Tecton. He is a member of Alpha Pi Mu, INFORMS, and the\nNorth American Manufacturing Research Institute.\n"}