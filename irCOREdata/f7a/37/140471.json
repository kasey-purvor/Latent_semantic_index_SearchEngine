{"doi":"10.1016\/j.procs.2010.04.208","coreId":"140471","oai":"oai:dspace.lib.cranfield.ac.uk:1826\/4356","identifiers":["oai:dspace.lib.cranfield.ac.uk:1826\/4356","10.1016\/j.procs.2010.04.208"],"title":"A sparse matrix approach to reverse mode automatic differentiation in Matlab","authors":["Forth, Shaun A.","Sharma, N K"],"enrichments":{"references":[{"id":37939154,"title":"A macro language for derivative de in ADiMat, in:","authors":[],"date":"2005","doi":"10.1007\/3-540-28438-9_16","raw":"C. H. Bischof, H. M. B ucker, A. Vehreschild, A macro language for derivative denition in ADiMat, in: H. M. B ucker, G. Corliss, P. Hovland, U. Naumann, B. Norris (Eds.), Automatic Dierentiation: Applications, Theory, and Implementations, Lecture Notes in Computational Science and Engineering, Springer, 2005, pp. 181{188. doi:10.1007\/3-540-28438-9 16.","cites":null},{"id":37939150,"title":"ADMAT: Automatic dierentiation in MATLAB using object oriented methods, in:","authors":[],"date":"1999","doi":null,"raw":"A. Verma, ADMAT: Automatic dierentiation in MATLAB using object oriented methods, in: M. E. Henderson, C. R. Anderson, S. L. Lyons (Eds.), Object Oriented Methods for Interoperable Scientic and Engineering Computing: Proceedings of the 1998 SIAM Workshop, SIAM, Philadelphia, 1999, pp. 174{183.","cites":null},{"id":37939151,"title":"An ecient overloaded implementation of forward mode automatic dierentiation in MATLAB,","authors":[],"date":"2006","doi":"10.1145\/1141885.1141888","raw":"S. A. Forth, An ecient overloaded implementation of forward mode automatic dierentiation in MATLAB, ACM T. Math. Software. 32 (2) (2006) 195{222. doi:10.1145\/1141885.1141888.","cites":null},{"id":37939152,"title":"Associates, LLC, ADMAT: Automatic Dierentiation Toolbox for use with MATLAB.","authors":[],"date":"2008","doi":null,"raw":"Cayuga Research Associates, LLC, ADMAT: Automatic Dierentiation Toolbox for use with MATLAB. Version 2.0. (2008). URL http:\/\/www.math.uwaterloo.ca\/CandO Dept\/securedDownloadsWhitelist\/Manual.pdf","cites":null},{"id":37939155,"title":"Code optimization techniques in source transformations for interpreted languages, in:","authors":[],"date":"2008","doi":"10.1007\/978-3-540-68942-3_20","raw":"H. M. B ucker, M. Petera, A. Vehreschild, Code optimization techniques in source transformations for interpreted languages, in: C. H. Bischof, H. M. B ucker, P. D. Hovland, U. Naumann, J. Utke (Eds.), Advances in Automatic Dierentiation, Springer, 2008, pp. 223{233. doi:10.1007\/978-3-540-68942-3 20.","cites":null},{"id":37939153,"title":"Combining source transformation and operator overloading techniques to compute derivatives for MATLAB programs, in:","authors":[],"date":"2002","doi":"10.1109\/scam.2002.1134106","raw":"C. H. Bischof, H. M. B ucker, B. Lang, A. Rasch, A. Vehreschild, Combining source transformation and operator overloading techniques to compute derivatives for MATLAB programs, in: Proceedings of the Second IEEE International Workshop on Source Code Analysis and Manipulation (SCAM 2002), IEEE Computer Society, Los Alamitos, CA, USA, 2002, pp. 65{72. doi:10.1109\/SCAM.2002.1134106.","cites":null},{"id":37939149,"title":"Evaluating Derivatives:","authors":[],"date":"2008","doi":"10.1137\/1.9780898717761","raw":"A. Griewank, A. Walther, Evaluating Derivatives: Principles and Techniques of Algorithmic Dierentiation, 2nd Edition, SIAM, Philadelphia, PA, 2008.","cites":null},{"id":37939160,"title":"Issues in parallel automatic dierentiation, in:","authors":[],"date":"1991","doi":null,"raw":"C. H. Bischof, Issues in parallel automatic dierentiation, in: A. Griewank, G. F. Corliss (Eds.), Automatic Dierentiation of Algorithms: Theory, Implementation, and Application, SIAM, Philadelphia, PA, 1991, pp. 100{113.","cites":null},{"id":37939159,"title":"Jacobian code generated by source transformation and vertex elimination can be as ecient as hand-coding,","authors":[],"date":"2004","doi":"10.1145\/1024074.1024076","raw":"S. A. Forth, M. Tadjouddine, J. D. Pryce, J. K. Reid, Jacobian code generated by source transformation and vertex elimination can be as ecient as hand-coding, ACM T. Math. Software. 30 (3) (2004) 266{299. doi:10.1145\/1024074.1024076.","cites":null},{"id":37939158,"title":"Optimal accumulation of Jacobian matrices by elimination methods on the dual computational graph,","authors":[],"date":"2004","doi":"10.1007\/s10107-003-0456-9","raw":"U. Naumann, Optimal accumulation of Jacobian matrices by elimination methods on the dual computational graph, Math. Program., Ser. A 99 (3) (2004) 399{421. doi:10.1007\/s10107-003-0456-9.","cites":null},{"id":37939161,"title":"Sharing storage using dirty vectors, in:","authors":[],"date":"1996","doi":null,"raw":"B. Christianson, L. C. W. Dixon, S. Brown, Sharing storage using dirty vectors, in: M. Berz, C. Bischof, G. Corliss, A. Griewank (Eds.), Computational Dierentiation: Techniques, Applications, and Tools, SIAM, Philadelphia, PA, 1996, pp. 107{115. 9[14] B. M. Averick, R. G. Carter, J. J. Mor e, G.-L. Xue, The MINPACK-2 test problem collection, Preprint MCS{P153{0692, MCS ANL, Argonne, IL (1992).","cites":null},{"id":37939156,"title":"Source transformation for MATLAB automatic dierentiation, in:","authors":[],"date":"2006","doi":"10.1007\/11758549_77","raw":"R. V. Kharche, S. A. Forth, Source transformation for MATLAB automatic dierentiation, in: V. N. Alexandrov, G. D. van Albada, P. M. A. Sloot, J. Dongarra (Eds.), Computational Science { ICCS 2006, Vol. 3994 of Lect. Notes Comput. Sc., Springer, Heidelberg, 2006, pp. 558{565. doi:10.1007\/11758549 77.","cites":null},{"id":37939157,"title":"Sparse matrices in Matlab - design and implementation,","authors":[],"date":"1992","doi":"10.1137\/0613024","raw":"J. Gilbert, C. Moler, R. Schreiber, Sparse matrices in Matlab - design and implementation, SIAM J. Matrix Anal. Appl. 13 (1) (1992) 333{356.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-04-12","abstract":"We review the extended Jacobian approach to automatic di erentiation of a user-\nsupplied function and highlight the Schur complement form's forward and reverse\nvariants. We detail a Matlab operator overloaded approach to construct the\nextended Jacobian that enables the function Jacobian to be computed using\nMatlab's sparse matrix operations. Memory and runtime costs are reduced using a\nvariant of the hoisting technique of Bischof (Issues in Parallel Automatic Di\nerentiation, 1991). On ve of the six mesh-based gradient test problems from The\nMINPACK-2 Test Problem Collection (Averick et al, 1992) the reverse variant of\nour extended Jacobian technique with hoisting outperforms the sparse storage\nforward mode of the MAD package (Forth, ACM T. Math. Software. 32, 2006). For\nincreasing problems size the ratio of gradient to function cpu time is seen to\nbe bounded, if not decreasing, in line with Griewank and Walther's (Evaluating\nDerivatives, SIAM, 2008) cheap gradient principle","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/140471.pdf","fullTextIdentifier":"http:\/\/www.sc.rwth-aachen.de\/Events\/APGCSatICCS2010\/","pdfHashValue":"968db79a1a1808f75b088a2cd730096adc7a0986","publisher":null,"rawRecordXml":"<record><header><identifier>\noai:dspace.lib.cranfield.ac.uk:1826\/4356<\/identifier><datestamp>2017-11-20T11:33:48Z<\/datestamp><setSpec>hdl_1826_13<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>A sparse matrix approach to reverse mode automatic differentiation in Matlab<\/dc:title><dc:creator>Forth, Shaun A.<\/dc:creator><dc:creator>Sharma, N K<\/dc:creator><dc:description>We review the extended Jacobian approach to automatic di erentiation of a user-\nsupplied function and highlight the Schur complement form's forward and reverse\nvariants. We detail a Matlab operator overloaded approach to construct the\nextended Jacobian that enables the function Jacobian to be computed using\nMatlab's sparse matrix operations. Memory and runtime costs are reduced using a\nvariant of the hoisting technique of Bischof (Issues in Parallel Automatic Di\nerentiation, 1991). On ve of the six mesh-based gradient test problems from The\nMINPACK-2 Test Problem Collection (Averick et al, 1992) the reverse variant of\nour extended Jacobian technique with hoisting outperforms the sparse storage\nforward mode of the MAD package (Forth, ACM T. Math. Software. 32, 2006). For\nincreasing problems size the ratio of gradient to function cpu time is seen to\nbe bounded, if not decreasing, in line with Griewank and Walther's (Evaluating\nDerivatives, SIAM, 2008) cheap gradient principle.<\/dc:description><dc:date>2012-05-16T23:01:34Z<\/dc:date><dc:date>2012-05-16T23:01:34Z<\/dc:date><dc:date>2010-04-12<\/dc:date><dc:type>Article<\/dc:type><dc:identifier>10th International Conference on Computational Science, University of Amsterdam,\nThe Netherlands, http:\/\/www.iccs-meeting.org\/iccs2010\/. The paper has been\nsubmitted to the conference's workshop on Automated Program Generation for\nComputational Science http:\/\/www.sc.rwth-aachen.de\/Events\/APGCSatICCS2010\/<\/dc:identifier><dc:identifier>1877-0509<\/dc:identifier><dc:identifier>http:\/\/dx.doi.org\/10.1016\/j.procs.2010.04.208<\/dc:identifier><dc:identifier>http:\/\/dspace.lib.cranfield.ac.uk\/handle\/1826\/4356<\/dc:identifier><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["issn:1877-0509","1877-0509"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2010,"topics":[],"subject":["Article"],"fullText":"A sparse matrix approach to reverse mode automatic\ndifferentiation in Matlab\nShaun A. Forth\u2217and Naveen Kr. Sharma\u2020\u2021\nInternational Conference on Computational Science, ICCS 2010,\nUniversity of Amsterdam, The Netherlands\nMay 31 - June 2, 2010\nAbstract\nWe review the extended Jacobian approach to automatic differentiation of a user-supplied\nfunction and highlight the Schur complement form\u2019s forward and reverse variants. We detail\na Matlab operator overloaded approach to construct the extended Jacobian that enables\nthe function Jacobian to be computed using Matlab\u2019s sparse matrix operations. Memory and\nruntime costs are reduced using a variant of the hoisting technique of Bischof (Issues in Parallel\nAutomatic Differentiation, 1991). On five of the six mesh-based gradient test problems from\nThe MINPACK-2 Test Problem Collection (Averick et al, 1992) the reverse variant of our\nextended Jacobian technique with hoisting outperforms the sparse storage forward mode of\nthe MAD package (Forth, ACM T. Math. Software. 32, 2006). For increasing problems size\nthe ratio of gradient to function cpu time is seen to be bounded, if not decreasing, in line with\nGriewank and Walther\u2019s (Evaluating Derivatives, SIAM, 2008) cheap gradient principle.\n1 Introduction\nAutomated program generation techniques have been used to augment numerical simulation pro-\ngrams in order to compute derivatives (or sensitivities) of desired simulation outputs with respect\nto nominated inputs since the 1960\u2019s. Such techniques go by the collective name Automatic,\nor Algorithmic, Differentiation (AD) [1]. Advances in AD theory, techniques, tools and wide-\nranging applications may be found in the many references available on the international website\nwww.autodiff.org.\nHistorically, development has focussed on AD tools for programs written in Fortran and\nC\/C++ as these languages have dominated large simulations for technical computing. AD of\nMatlab was pioneered by Verma [2] who used the, then new, object-oriented features of Matlab\nin the overloaded AD package ADMAT 1.0 facilitating calculation of first and second derivatives\nand determination of Jacobian and Hessian sparsity patterns.\nOur experience is that ADMAT\u2019s derivative computation run times are significantly higher\nthan expected from AD complexity analysis leading us to develop the MAD package [3] (the\nrecent ADMAT 2.0 may have addressed these issues [4]). Presently MAD facilitates first derivative\ncomputations via its fmad class, an operator-overloaded implementation of forward mode AD. The\nfmad class\u2019s efficiency may be attributed to its use of the optimised derivvec class for storing and\ncombining directional derivatives. The derivvec class permits multiple directional derivatives to\nbe stored as full or sparse matrices: sparse storage gave good performance on a range of problems.\n\u2217Applied Mathematics and Scientific Computing, Department of Engineering Systems and Management, Cran-\nfield University, Shrivenham, Swindon, SN6 8LA, UK\n\u2020Department of Computer Science and Engineering, Indian Institute of Technology, Kharagpur 721302, West\nBengal, India\n\u2021Naveen Kr. Sharma gratefully acknowledges the support of a Summer Internship from the Department of\nEngineering Systems and Management, Cranfield University.\n1\nBischof et al [5] investigated forward mode Matlab AD by source transformation combined\nwith storing and combining direction derivatives via an overloaded library. This hybrid approach\ngave a significant performance improvement over ADMAT. Source transformation permits the\nuse of compile-time performance optimisations [6, 7]: forward substitution was found particularly\neffective. Kharche and Forth [8] investigated specialising their source transformation inlining of\nfunctions of MAD\u2019s fmad and derivvec classes in the cases of scalar and inactive variables. This\nwas particularly beneficial for small problem sizes for which overloading\u2019s run time requirements\nare dominated by the large relative cost of function call overheads and branching (required for\ncode relevant to scalar and inactive variables) compared to arithmetic operations.\nOur thesis is that automated program generation techniques, and specifically AD, should take\nadvantage of the most efficient features of a target language. For example, MAD\u2019s derivvec class\u2019s\nexploitation [3] of the optimised sparse matrix features of Matlab [9]. As we recap in Sec. 2, it\nis well known that forward and reverse mode AD may be interpreted in terms of forward and\nback substitution on the sparse, so-called, extended Jacobian system [1, Chap. 9]. In this article\nwe investigate whether we might use Matlab\u2019s sparse matrix features to effect reverse mode AD\nwithout recourse to the usual tape-based mechanisms [1, Chap 6.1]. Our implementation, including\noptimised variants, is described in Sec. 3 and the performance testing of Sec. 4 demonstrates our\napproach\u2019s potential benefits. Conclusions and further work are presented in Sec. 5.\n2 Matrix Interpretation of automatic differentiation\nFollowing Griewank and Walther [1], we consider a function \ud439 : \ud43c\ud445\ud45b 7\u2192 \ud43c\ud445\ud45a of the form,\n\ud466 = \ud439 (\ud465), (1)\nin which \ud465 \u2208 \ud43c\ud445\ud45b and \ud466 \u2208 \ud43c\ud445\ud45a are the vectors of independent, and dependent, variables respectively.\nOur aim is to calculate the function Jacobian \ud439 \u2032(\ud465) \u2208 \ud43c\ud445\ud45a \u00d7 \ud43c\ud445\ud45b for any given \ud465 from the source\ncode of a computer program that calculates \ud439 (x): this is AD\u2019s primary task. We consider the\nevaluation of \ud439 (\ud465) as a three-part evaluation procedure of the form,\n\ud463\ud456 = \ud465\ud456, \ud456 = 1, . . . , \ud45b, (2)\n\ud463\ud456 = \ud711\ud456(\ud463\ud457)\ud457\u227a\ud456, \ud456 = \ud45b + 1, . . . , \ud45b + \ud45d, (3)\n\ud463\ud456 = \ud463\ud457\u227a\ud456, \ud456 = \ud45b + \ud45d + 1, . . . , \ud45b + \ud45d + \ud45a. (4)\nIn (2) we copy the independent variables \ud465 to internal variables \ud4631, . . . , \ud463\ud45b. In (3) each \ud463\ud456, \ud456 =\n\ud45b+1, . . . , \ud45b+\ud45d is obtained as a result of \ud45d successive elementary operations or elementary functions\n(e.g., additions, multiplications, square roots, cosines, etc.), acting on a small number of already\ncalculated \ud463\ud457 . Finally in (4) the appropriate internal variables \ud463\ud457 are copied to the dependent\nvariables in such a way that \ud466\ud456 = \ud463\ud45b+\ud45d+\ud456, \ud456 = 1, . . . ,\ud45a, to complete the function evaluation.\nWe define the gradient operator \u2207 =\n(\n\u2202\n\u2202\ud4651\n, . . . , \u2202\u2202\ud465\ud45b\n)\n, differentiate (2)-(4) and arrange the\nresulting equations for the \u2207\ud463\ud456 as a linear system,\u23a1\u23a3 \u2212\ud43c\ud45b 0 0\ud435 \ud43f\u2212 \ud43c\ud45d 0\n\ud445 \ud447 \u2212\ud43c\ud45a\n\u23a4\u23a6\u23a1\u23a3 \u2207\ud4631,...,\ud45b\u2207\ud463\ud45b+1,...,\ud45b+\ud45d\n\u2207\ud463\ud45b+\ud45d+1,...,\ud45b+\ud45d+\ud45a\n\u23a4\u23a6 =\n\u23a1\u23a3 \u2212\ud43c\ud45b0\n0\n\u23a4\u23a6 . (5)\nIn (5): \ud43c\ud45b is the \ud45b\u00d7 \ud45b identity matrix (\ud43c\ud45d and \ud43c\ud45a are defined similarly); \u2207\ud4631,...,\ud45b is the matrix,\n\u2207\ud4631,...,\ud45b =\n\u23a1\u23a2\u23a3 \u2207\ud4631...\n\u2207\ud463\ud45b\n\u23a4\u23a5\u23a6 ,\nwith \u2207\ud463\ud45b+1,...,\ud45b+\ud45d and \u2207\ud463\ud45b+\ud45d+1,...,\ud45b+\ud45d+\ud45a defined similarly; from (3) the \ud45d \u00d7 \ud45b \ud435 and \ud45d \u00d7 \ud45d \ud43f\nare both sparse and contain partial derivatives of elementary operations and assignments; from\n2\n(4) the \ud45a \u00d7 \ud45b \ud445 and \ud45a \u00d7 \ud45d \ud447 are such that [\ud445 \ud447 ] contains exactly one unit entry per row. The\n(\ud45b + \ud45d + \ud45a) \u00d7 (\ud45b + \ud45d + \ud45a) coefficient matrix in (5) is known as the Extended Jacobian denoted\n\ud436 \u2212 \ud43c\ud45b+\ud45d+\ud45a and has sub-diagonal entries \ud450\ud456\ud457 .\nBy forward substitution on (5) we see that the system Jacobian \ud43d = \u2207\ud4661,...,\ud45a = \u2207\ud463\ud45b+\ud45d+1,...,\ud45b+\ud45d+\ud45a\nis given by,\n\ud43d = \ud445 + \ud447 (\ud43f\u2212 \ud43c\ud45d)\u22121\ud435, (6)\nthe Schur complement of \ud43f\u2212 \ud43c\ud45d. Using (6), \ud43d may be evaluated in two ways [1, p.188]:\n1. Forward variant: derivatives \u2207\ud463\ud45b+1,...,\ud45b+\ud45d and Jacobian \ud43d are determined by,\n(\ud43f\u2212 \ud43c\ud45d)\u2207\ud463\ud45b+1,...,\ud45b+\ud45d = \ud435, (7)\n\ud43d = \ud445 + \ud447\u2207\ud463\ud45b+1,...,\ud45b+\ud45d. (8)\ni.e., forward substitution on the lower triangular system (7) followed by a matrix multipli-\ncation and addition (8).\n2. Reverse variant: the \ud45d\u00d7\ud45a adjoint matrix \ud44d\u00af and Jacobian \ud43d are determined by,\n(\ud43c\ud45d \u2212 \ud43f\ud447 )\ud44d\u00af = \ud447\ud447 , (9)\n\ud43d = \ud445 + \ud44d\u00af\ud447\ud435. (10)\ni.e., back-substitution of the upper triangular system (9) followed by matrix multiplication\nand addition (10).\nThe arithmetic cost of both variants is dominated by the solution of the linear systems (7) and (9)\nwith common (though transposed in (9)) sparse coefficient matrix. These systems have \ud45b and\n\ud45a right-hand sides respectively giving the rule of thumb that the forward variant is likely to be\npreferred for \ud45b < \ud45a and reverse for \ud45a > \ud45b (see [1, p.189] for a counter example). Since matrices\n\ud435, \ud445 and \ud447 in (7) to (10) are also sparse, further reductions in arithmetic cost might be obtained\nby storing and manipulating the \ud463 or \ud44d\u00af as sparse matrices [1, Chap. 7].\nOther approaches to reducing arithmetic cost are based on performing row operations to reduce\nthe number of entries in (5) or, indeed, to entirely eliminate some rows [1, Chaps. 9-10]. Such\napproaches have been generalised by Naumann [10] and may be very efficient if performed at\ncompilation time by a source transformation AD tool [11]. In Sec. 3.3 we will adapt Bischof\u2019s\nhoisting technique [12] to reduced the size and number of entries of the extended Jacobian: reducing\nmemory and runtime costs. Unlike Bischof, ours is a runtime approach more akin to Christianson\net al\u2019s [13] dirty vectors.\n3 Three Implementations\nIn Secs. 3.2-3.4 we describe our three closely related overloaded classes designed to generate the\nextended Jacobian\u2019s entries as the user\u2019s function is executed. First, however, we introduce the\nMADExtJacStore class, objects of which are used by all three overloaded classes to store the\nextended Jacobian entries.\n3.1 Extended Jacobian Storage\nA MADExtJacStore object has components to store: the number of independent variables, the\nnumber of rows of the Extended Jacobian for which entries have been determined, the number of\nentries determined, and a three column matrix to store the \ud456, \ud457 and coefficient \ud450\ud456\ud457 for each entry.\nThe number of rows of the matrix component is doubled whenever its current size is exhausted.\nThe class inherits the Matlab handle class attribute rendering all assignments of such objects to be\npointer assignments allowing us to have a single object of this class accessible to multiple objects\nof our overloaded classes.\n3\n3.2 A First Implementation - ExtJacMAD\nFigure 1: Example code.\nfunction y = F(x)\ny = 2 .* x(2:3) .* x(1);\nWe describe our classes of this and the next three sections\nwith reference to a vector of independent inputs \ud465 =\n[0.5 1 \u22122.5]\ud447 and the example code of Figure 1 which\ncorresponds to the function (\ud4661, \ud4662) = (2\ud4652\ud4651, 2\ud4653\ud4651).\nExtJacMAD class objects have three properties. The\nfirst is the value property which stores the numeric value\nof the object and, since Matlab is an array-based lan-\nguage, may be an arbitrary size array. The second is the\nrow index property which is an array of the same size as, and holds the extended Jacobian row\nindex \ud456 for each corresponding element of, the object\u2019s value property. Finally, the JacStore\ncomponent is a handle to a MADExtJacStore object used to store the extended Jacobian entries.\nThe statement\nx = ExtJacMAD([0.5; 1; -2.5])\ncreates an object of ExtJacMAD class using the ExtJacMAD constructor function to set the three\nproperties of x. Firstly, x.value is assigned the column vector [0.5; 1; -2.5]. Secondly,\nx.row index is set to the column vector [1; 2; 3] indicating that the elements of the value\ncomponent array correspond to the first three rows of the extended Jacobian. Thirdly, x.JacStore\nis set to point to a new object of MADExtJacStore class with its array of entries zeroed and both\nthe number of independent variables and the numbers of rows set to three.\nApplying the function of Fig. 1, y = F(x), initiates four overloaded operations:\n1. Our class\u2019s overloaded subscript reference function subsref forms a temporary object, call\nit tmp1, with tmp1 = x(2:3). This function simply applies subscripting to the value and\nrow index properties of x such that tmp1.value = x.value(2:3) and tmp1.row index\n= x.row index(2:3). JacStore is pointed to the same MADExtJacStore object as for x.\nThis approach, involving just two subscripting operations and a pointer copy, is intrinsically\nmore efficient than that for MAD\u2019s forward mode [3] which involves further overloading on\nderivvec objects and multiple operations required to subscript on their derivative storage\narrays.\n2. Now 2 .* x(2:3) is effected as 2 .* tmp1 with the result stored in temporary object tmp2.\nThe associated overloaded times operation performs tmp2.value = 2 .* tmp1.value =\n[2 -5] and copies the JacStore component of tmp1. However, the derivatives of tmp2.value\u2019s\nelements are twice those of\ntmp1.value so two new extended Jacobian entries must be made for new rows i = [4 5],\ncolumns j = [2 3] (i.e., the row indices of tmp1), and local derivatives c = 2. If j or c is\nscalar our software automatically and efficiently replicates them by one-to-many assignments\nto be the same size as \ud456: thus c = [2 2]. The new row indices are assigned tmp2.row index\n= [4 5].\n3. As in step 1, tmp3 = x(1) effects tmp3.value = x.value(1) = 0.5, tmp3.row index =\nx.row index(1) = 1 and tmp3.JacStore = x.JacStore.\n4. Finally, y is formed by the overloaded times operation so that: y.value = tmp2.value .*\ntmp3.value = [1 -2.5]; y.JacStore=tmp2.JacStore; and another four entries in the ex-\ntended Jacobian are created. Entries for the tmp2 argument with i = [6 7], j = tmp2.row index\n= [4 5], c = tmp3.value = 0.5 (expanded to [0.5 0.5]) are first stored. These are fol-\nlowed by those for tmp3 with i = [6 7], j = tmp2.row index = [4 5], and c = tmp2.value\n= [2 -5].\nAs a result of these steps the indices i = [4 5 6 7 6 7], j = [2 3 4 5 1 1] and coefficients c\n= [2 2 0.5 0.5 2 -5] of the extended Jacobian\u2019s off-diagonal entries have been determined.\nWe may now obtain the Jacobian of y with respect to x,\n4\nJ = getJacobian(y)\nreturning,\nJ =\n2 1 0\n-5 0 1\nas expected. The ExtJacMAD class\u2019s getJacobian function first adds \ud45a rows to the extended\nJacobian corresponding to a copy of the elements of y to ensure the extended Jacobian is of the\nform (5) with the last \ud45a rows linearly independent. It then forms the sparse sub-matrices \ud435, \ud43f,\n\ud445 and \ud447 before evaluating the Jacobian using one of four subtly different Matlab statements,\nJ = R - T *( (L - eye(p)) \\ full(B)) (11)\nJ = R - T * ( (L - speye(p)) \\ B) (12)\nJ = R - (full(T) \/ (L - eye(p))) * B (13)\nJ = R - (T \/ (L - speye(p))) * B (14)\nEquation (11) uses full storage for the intermediate derivatives \u2207\ud463\ud45b+1,...,\ud45b+\ud45d of the forward sub-\nstitution (7) and Jacobian \ud43d of (8). Approach (12) performs the same operations (7)-(8) using\nsparse storage. Equations (13) and (14) correspond to full and sparse storage variants respectively\nof the back-substitution approach of (9)-(10).\n3.3 Hoisting - ExtJacMAD H\nAs an alternative to the computational graph approach used by Bischof et al [12], hoisting can\nbe interpreted as Gaussian elimination using pivot rows with a single off-diagonal entry in the\nextended Jacobian. Consider the extended Jacobian arising from the example code of Fig. 1 using\nthe approach of Sec. 3.2. As shown in (15), the two sub-diagonal entries of the \ud43f block arising\nfrom the use of the two elements of the vector variable tmp1 may be eliminated by row operations\nfor the cost of two multiplications while generating two entries by fill-in (though in general fill-in\nneed not occur).\n\ud436\u2212\ud43c =\n\u239b\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d\n\u22121\n\u22121\n\u22121\n2 \u22121\n2 \u22121\n2 .5 \u22121\n\u22125 .5 \u22121\n1 \u22121\n1 \u22121\n\u239e\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0\nrow 6\u2192row 6+0.5\u00d7row 4\nrow 7\u2192row 7+0.5\u00d7row 5\u2212\u2192\n\u239b\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d\n\u22121\n\u22121\n\u22121\n2 \u22121\n2 \u22121\n2 1 \u22121\n\u22125 1 \u22121\n1 \u22121\n1 \u22121\n\u239e\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0\n(15)\nIn the extended Jacobian, rows 4 and 5 may now be removed from blocks \ud43f and \ud435, and columns 4\nand 5 may be removed from blocks \ud43f and \ud447 . This also eliminates two rows from the intermediate\nmatrices \u2207\ud463\ud45b+1,...,\ud45b+\ud45d or \ud44d\u00af leaving the extended Jacobian as,\n\ud436 \u2212 \ud43c =\n\u239b\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239c\u239d\n\u22121\n\u22121\n\u22121\n2 1 \u22121\n\u22125 1 \u22121\n1 \u22121\n1 \u22121\n\u239e\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u239f\u23a0\n(16)\n5\nWe may now extract \ud435, \ud43f, \ud445 and \ud447 from (16) and calculate the Jacobian \ud43d . Hoisting is an\nexample of a safe pre-elimination which never increases the number of arithmetic operations [1, p.\n212] but can drastically reduce both these and memory costs. In Matlab hoisting may be applied\nto element-wise operations or functions with a single array argument (e.g., -x, sin(x), sqrt(x))\nand element-wise binary operations or functions with one inactive argument (e.g., 2 + x, A .*\nx with A inactive). Hoisting is not applicable to matrix operations or functions (e.g, linear solve\nX \\ Y or determinant det(X)).\nWe effect hoisting by a run-time mechanism distinguishing our work from Bischof et al [12].\nWe use a similar technique to that of Christianson et al [13] who used it to reduce forward or back\nsubstitution costs on the full extended Jacobian: we reduce the size of the extended Jacobian.\nOur hoisted class ExtJacMAD H has an additional property to that of class ExtJacMAD, an\naccumulated extended Jacobian entry array Cij. When we initialise our ExtJacMAD H object,\nx = ExtJacMAD_H([0.5; 1; -2.5])\nwe assign x.Cij = 1 indicating that the derivatives of the elements of x are a multiple of one times\nthose associated with x.row index, i.e., rows 1 to 3. Step 1 of the overloaded operations of Sec. 3.2\nis as before but with the additional copy tmp1.Cij = x.Cij. Step 2 differs more substantially\nwith no additions to the extended Jacobian, we merely copy tmp2.row_index = tmp1.row_index\nand set tmp2.Cij = 2*tmp1.Cij. Step 3 is similar to the revised step 1. Step 4 is modified to\naccount for the objects\u2019 accumulated extended Jacobian entry, so when dealing with the entries\nassociated with tmp2 we have c = temp2.Cij .* tmp3.value = 1 (expanded to [1 1]), and\nwhen dealing with tmp3 we have c = tmp3.Cij .* tmp2.value = 1 .* [2 -5] = [2 -5]. The\nassembled extended Jacobian then directly takes the form (16) and we see that the effects of\nhoisting have been mimicked at runtime. Note that the Cij component is maintained as a scalar\nwhenever possible. Array values of Cij would be created if our example function\u2019s coding were\ny = sin(x(2:3)) .* x(1) as then tmp2.Cij = cos(tmp1) = [0.5403 -0.8011] though this\nwould not prevent hoisting.\n3.4 Using Matlab\u2019s New Objected Oriented Programming Style\nMatlab release R2008a introduced substantially new object oriented programming styles and ca-\npabilities compared to those used by both our implementations of Secs. 3.2 and 3.3 and previous\nMatlab AD packages [2, 3]. Instead of the old style\u2019s definition of all an object\u2019s methods within\nseparate source files in a common folder, in the new style all properties and methods are defined\nin a single source file.\n4 Performance Testing\nAll tests involved calculating the gradient of a function from the MINPACK-2 Test Problem\ncollection [14] with the original Fortran functions recoded into Matlab by replacing loops with\narray operations and array functions. We performed all tests for a range of problem sizes \ud45b: for\neach \ud45b five different sets of independent variables \ud465 were used. For each derivative technique and\neach problem size the set of five derivative calculations were repeated sufficiently often that the\ncumulative cpu time exceeded 5 seconds. If a single set of five calculations exceeded 5 seconds\ncpu time then that technique was not used for larger \ud45b. All tests were performed using Matlab\nR2009b on a Windows XP SP3 PC with 2GB RAM. We first consider detailed results from one\nproblem.\n4.1 MINPACK-2 Optimal Design of Composites (ODC) test problem\nTable 1 presents the run time ratio of gradient cpu time to function cpu time for the Optimal\nDesign of Composites (ODC) problem for varying number of independent variables \ud45b and using\nthe extended Jacobian techniques of Sec. 3. We also give run time ratios for a hand-coded adjoint,\n6\none-sided finite differences (FD) and sparse forward mode AD using MAD\u2019s fmad class. For these,\nand all other results presented, AD generated derivatives agreed with the hand-coded technique\nto within round-off: errors for FD were in line with the expected truncation error. Within our\ntables a dash indicates that memory or cpu time limits were exceeded.\nTable 1: Gradient evaluation cpu time ratio cpu(\u2207\ud453)\/cpu(\ud453) for the MINPACK-2 Optimal Design\nof Composites (ODC) test problem.\ncpu(\u2207\ud453)\/cpu(\ud453) for problem size \ud45b\nGrad. Tech 25 100 2500 10000 40000\nhand-coded 1.8 1.9 2.0 2.0 1.7\nFD 26.2 102.8 2684.0 11147.2 -\nsparse forward AD 66.0 55.8 134.7 - -\nExtended Jacobian: Sec. 3.2\nforward full 58.6 79.9 - - -\nforward sparse 61.3 56.9 62.0 64.7 53.7\nreverse full 57.9 51.4 42.1 42.0 35.4\nreverse sparse 57.2 57.4 51.2 55.1 48.0\nExtended Jacobian + Hoisting: Sec. 3.3\nforward full 46.3 51.4 - - -\nforward sparse 48.3 42.7 34.0 33.9 28.6\nreverse full 47.0 41.0 24.3 23.1 19.9\nreverse sparse 44.9 39.9 26.3 26.4 23.3\nExtended Jacobian + Hoisting + New Object Orientation: Sec. 3.4\nforward full 99.4 95.2 - - -\nforward sparse 99.4 83.1 38.4 35.3 28.6\nreverse full 100.8 88.6 31.0 26.4 21.0\nreverse sparse 98.0 82.1 33.7 29.0 23.8\nThe hand-coded results show what might be achievable for a source transformation AD tool in\nMatlab. The FD cpu time ratio is in line with theory (\u2248 \ud45b + 1) but FD exceeded our maximum\npermitted run time for large \ud45b. Sparse forward mode AD outperformed FD with increasing \ud45b but\nexceeded our PC\u2019s 2 GB RAM for larger \ud45b.\nThe extended Jacobian approaches of Sec. 3.2, particularly the reverse variant with full storage,\nare seen to be competitive with, or outperform, sparse forward AD with the exception of the\nforward variant with full storage. Since \ud45a \u226a \ud45b we expect the reverse variants to outperform\nforward and as \ud45a = 1 there is no point employing sparse storage. Employing the hoisting technique\nof Sec. 3.3 was always beneficial and for larger problem sizes halved the run time. This is because\nhoisting reduces the number of entries in the Extended Jacobian by approximately 55% for all\nproblem sizes tested.\nEmploying Matlab\u2019s new object oriented features, described in Sec. 3.4, had a strongly detri-\nmental effect doubling required cpu times compared to using the old features for small to moderate\n\ud45b. The two sets of run times converge for large \ud45b because the underlying Matlab built-in functions\nand operations are identical for both. Matlab\u2019s run time overheads must be significantly higher\nfor the new approach and so dominate for small \ud45b.\n4.2 MINPACK-2 mesh-based minimisation test problem\nTable 2 presents selected results for the remaining mesh-based MINPACK-2 minimisation prob-\nlems. We only present the most efficient reverse full variants of the Extended Jacobian approach.\nPerformance of the FD, hand-coded and new Object Oriented Extended Jacobian approaches was\nin line with that for the ODC problem of Sec. 4.1.\nFrom Table 2, only for the GL1 problem did sparse forward mode AD outperform the Extended\nJacobian approach. For all other problems the Extended Jacobian approach with hoisting gave\n7\nTable 2: Gradient evaluation cpu time ratio cpu(\u2207\ud453)\/cpu(\ud453) for the MINPACK-2 mesh-based\nminimisation test problems. Ext. Jac. indicates use of the reverse full variant of the Extended\nJacobian approach.\ncpu(\u2207\ud453)\/cpu(\ud453) for problem size \ud45b\nProblem Grad. Tech 25 100 2500 10000 40000 160000\nEPT sparse fwd. AD 102.1 99.6 341.3 - - -\nExt. Jac. 106.4 108.6 120.2 120.8 111.8 60.6\nExt. Jac. + Hoisting 93.5 98.3 91.2 91.9 86.0 46.1\nGL1 sparse fwd. AD 138.0 93.1 13.4 7.5 6.6 7.3\nExt. Jac. 160.6 104.7 34.3 26.9 27.5 25.6\nExt. Jac. + Hoisting 115.5 85.6 23.2 18.0 17.9 16.6\nMSA sparse fwd. AD 85.1 69.2 158.7 - - -\nExt. Jac. 82.4 71.3 54.0 60.3 66.0 -\nExt. Jac. + Hoisting 68.8 57.7 31.5 33.9 39.6 27.9\nPJB sparse fwd. AD 74.7 79.4 - - - -\nExt. Jac. 61.5 69.7 131.8 71.6 68.5 -\nExt. Jac. + Hoisting 57.4 60.8 99.8 51.8 48.3 -\ncpu(\u2207\ud453)\/cpu(\ud453) for problem size \ud45b\nProblem Grad. Tech 100 400 10000 40000 160000 640000\nGL2 sparse fwd. AD 86.9 86.7 142.8 292.8 - -\nExt. Jac. 76.8 78.6 88.1 66.2 82.8 -\nExt. Jac. + Hoisting 58.5 59.1 53.3 39.4 50.1 -\nequivalent, or substantially faster, performance and used less memory allowing larger problem\nsizes to be addressed. Hoisting reduced the number of extended Jacobian entries by between 34%\n(EPT problem) to 49% (MSA problem) leading to significant performance benefits. In all cases,\nexcept perhaps the GL2 problem, the Extended Jacobian with Hoisting approach gave run time\nratios decreasing with \ud45b for large \ud45b in line with the cheap gradient principle [1, p. 88].\n5 Conclusions and further work\nOur extended Jacobian approach of Sec. 3 allowed us to use operator overloading to build a\nfunction\u2019s extended Jacobian before employing Matlab\u2019s sparse matrix operations to calculate\nthe Jacobian itself. Bischof\u2019s hoisting technique [12] was adapted for run time use to reduce the\nsize of the extended Jacobian. The performance testing of Sec. 4 shows that the reverse variant\nof our approach with full storage of adjoints and hoisting was substantially more efficient and\nable to cope with larger problem sizes than MAD\u2019s forward mode [3] for five of six gradient test\nproblems from the MINPACK-2 collection [14]. Performance is an order of magnitude worse than\nfor a hand-coded adjoint due to the additional costs of overloaded function calls and of branching\nbetween multiple control flow paths in the overloaded functions. Additionally, the tailoring of the\nhand-coded adjoint\u2019s back-substitution to the sparsity of a particular source function\u2019s extended\nJacobian will likely outperform our use of a general sparse solve.\nFuture work will address Jacobian problems. Equations (11)-(14) may be employed directly\nfor this together with compression [1, Chap. 8]. Compression requires Jacobian-matrix, \ud43d\ud446 =\n\ud445\ud446 + \ud447 (\ud43f\u2212 \ud43c\ud45d)\u22121(\ud435\ud446), or matrix-Jacobian, \ud44a\ud43d = \ud44a\ud445 + (\ud44a\ud447 )(\ud43f\u2212 \ud43c\ud45d)\u22121\ud435, products where \ud446\nand \ud44a are so-called seed matrices. Hessians can be computed using the fmad class to differentiate\nthe extended Jacobian computations: a forward-over-reverse strategy [1, Chap. 5].\nThe extended Jacobian approach is not suited to all problems. For example, the product of\ntwo \ud441 \u00d7 \ud441 matrices would create some 2\ud4413 extended Jacobian entries. Verma [2] noted that\nby working at the matrix, and not the element level, just the 2\ud4412 elements of the two matrices\nare needed to enable reverse mode. An under-development tape-based AD implementation will\n8\nshortly be compared with this article\u2019s extended Jacobian approach.\nReferences\n[1] A. Griewank, A. Walther, Evaluating Derivatives: Principles and Techniques of Algorithmic\nDifferentiation, 2nd Edition, SIAM, Philadelphia, PA, 2008.\n[2] A. Verma, ADMAT: Automatic differentiation in MATLAB using object oriented methods,\nin: M. E. Henderson, C. R. Anderson, S. L. Lyons (Eds.), Object Oriented Methods for In-\nteroperable Scientific and Engineering Computing: Proceedings of the 1998 SIAM Workshop,\nSIAM, Philadelphia, 1999, pp. 174\u2013183.\n[3] S. A. Forth, An efficient overloaded implementation of forward mode automatic differentiation\nin MATLAB, ACM T. Math. Software. 32 (2) (2006) 195\u2013222. doi:10.1145\/1141885.1141888.\n[4] Cayuga Research Associates, LLC, ADMAT: Automatic Differentiation Toolbox for use with\nMATLAB. Version 2.0. (2008).\nURL http:\/\/www.math.uwaterloo.ca\/CandO Dept\/securedDownloadsWhitelist\/Manual.pdf\n[5] C. H. Bischof, H. M. Bu\u00a8cker, B. Lang, A. Rasch, A. Vehreschild, Combining source transfor-\nmation and operator overloading techniques to compute derivatives for MATLAB programs,\nin: Proceedings of the Second IEEE International Workshop on Source Code Analysis and\nManipulation (SCAM 2002), IEEE Computer Society, Los Alamitos, CA, USA, 2002, pp.\n65\u201372. doi:10.1109\/SCAM.2002.1134106.\n[6] C. H. Bischof, H. M. Bu\u00a8cker, A. Vehreschild, A macro language for derivative definition in\nADiMat, in: H. M. Bu\u00a8cker, G. Corliss, P. Hovland, U. Naumann, B. Norris (Eds.), Automatic\nDifferentiation: Applications, Theory, and Implementations, Lecture Notes in Computational\nScience and Engineering, Springer, 2005, pp. 181\u2013188. doi:10.1007\/3-540-28438-9 16.\n[7] H. M. Bu\u00a8cker, M. Petera, A. Vehreschild, Code optimization techniques in source transfor-\nmations for interpreted languages, in: C. H. Bischof, H. M. Bu\u00a8cker, P. D. Hovland, U. Nau-\nmann, J. Utke (Eds.), Advances in Automatic Differentiation, Springer, 2008, pp. 223\u2013233.\ndoi:10.1007\/978-3-540-68942-3 20.\n[8] R. V. Kharche, S. A. Forth, Source transformation for MATLAB automatic differentiation,\nin: V. N. Alexandrov, G. D. van Albada, P. M. A. Sloot, J. Dongarra (Eds.), Computational\nScience \u2013 ICCS 2006, Vol. 3994 of Lect. Notes Comput. Sc., Springer, Heidelberg, 2006, pp.\n558\u2013565. doi:10.1007\/11758549 77.\n[9] J. Gilbert, C. Moler, R. Schreiber, Sparse matrices in Matlab - design and implementation,\nSIAM J. Matrix Anal. Appl. 13 (1) (1992) 333\u2013356.\n[10] U. Naumann, Optimal accumulation of Jacobian matrices by elimination methods on the dual\ncomputational graph, Math. Program., Ser. A 99 (3) (2004) 399\u2013421. doi:10.1007\/s10107-\n003-0456-9.\n[11] S. A. Forth, M. Tadjouddine, J. D. Pryce, J. K. Reid, Jacobian code generated by source\ntransformation and vertex elimination can be as efficient as hand-coding, ACM T. Math.\nSoftware. 30 (3) (2004) 266\u2013299. doi:10.1145\/1024074.1024076.\n[12] C. H. Bischof, Issues in parallel automatic differentiation, in: A. Griewank, G. F. Corliss\n(Eds.), Automatic Differentiation of Algorithms: Theory, Implementation, and Application,\nSIAM, Philadelphia, PA, 1991, pp. 100\u2013113.\n[13] B. Christianson, L. C. W. Dixon, S. Brown, Sharing storage using dirty vectors, in: M. Berz,\nC. Bischof, G. Corliss, A. Griewank (Eds.), Computational Differentiation: Techniques, Ap-\nplications, and Tools, SIAM, Philadelphia, PA, 1996, pp. 107\u2013115.\n9\n[14] B. M. Averick, R. G. Carter, J. J. More\u00b4, G.-L. Xue, The MINPACK-2 test problem collection,\nPreprint MCS\u2013P153\u20130692, MCS ANL, Argonne, IL (1992).\n10\n"}