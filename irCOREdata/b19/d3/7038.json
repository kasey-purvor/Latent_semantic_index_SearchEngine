{"doi":"10.1080\/0968776940020208","coreId":"7038","oai":"oai:generic.eprints.org:163\/core5","identifiers":["oai:generic.eprints.org:163\/core5","10.1080\/0968776940020208"],"title":"Reviews","authors":["Jacobs, Gabriel","Pennington, Gus","King, Terry","Gillie, Tony","Banerji, Ashok","Tan, Check","Barker, Philip","Collis, Betty","Richards, Stephen","Clive,","Washington, Neena","Warren, Lorraine"],"enrichments":{"references":[{"id":193690,"title":"The Windows Interface: An Application Design Guide,","authors":[],"date":"1992","doi":null,"raw":"Microsoft (1992), The Windows Interface: An Application Design Guide, Microsoft Press.","cites":null},{"id":193691,"title":"Guidelines for Designing User Interface Software, The Mitre Corporation, Report ESD-TR-86-278. Copies of this monograph are available free of charge from","authors":[],"date":"1986","doi":null,"raw":"Smith, S.L. and Mosier, J.N. (1986), Guidelines for Designing User Interface Software, The Mitre Corporation, Report ESD-TR-86-278. Copies of this monograph are available free of charge from Cambertown Ltd, Unit 8, Goldthorpe Industrial Estate, Goldthorpe, Rotheram, South Yorkshire S63 9BL. Quote reference 50-OL121 when ordering.","cites":null},{"id":193692,"title":"Designing Usable Electronic Text - Ergonomlc Aspects of Human Information Usage","authors":[],"date":"1993","doi":null,"raw":"Ashok Banerji and Check Meng Tan, University of Teesside 80ALTJ VOLUME 2 NUMBER 2 Designing Usable Electronic Text - Ergonomlc Aspects of Human Information Usage by Andrew Dillon, London, Taylor and Francis, 1993. ISBN: 0-7484-01130-X. Increasingly, text is being viewed not in printed form but on a computer screen. This book is therefore a timely one, as it concentrates on the ergonomic and human-factors issues underlying the design of electronic text. It has ten chapters, a bibliography, a single appendix and a subject index. Following a short introductory scene-setting chapter, the author moves straight into a discussion of usability and the potential utility of ergonomics and human factors within this area. This is followed by a review of the experimental literature on reading from paper and from screens. Two basic strands are considered: outcome measures (speed, accuracy, fatigue, comprehension and preference) and process measures (eye movements, manipulation and navigational issues). The review is organized into three basic parts, covering reported differences between paper and screen, analysis of differences in terms of physical, perceptual and cognitive processes, and issues relating to text and task variables. In chapter 4, the author examines the value of existing human-factors literature to electronic document designers. The problems of applying this knowledge to the design and evaluation of an electronic-text system are then illustrated by means of a case study (an interactive document retrieval system based on a CD-ROM called ADONIS). Chapter 5 delves into the problems of classifying texts by means of repertory grid analysis using the FOCUS program, and the implications of the results for electronic text design are then discussed. Chapter 6 explores in more depth readers' interactions with two particular types of text - academic journals and software manuals. The methodology used in the investigation is based on the WWH (Why, What and How) approach to document usage). The chapter concludes with a discussion of the design implications of the research findings. The material presented in chapter 7 is concerned with the literature dealing with readers' impressions of structure and shape in information space (based on schemata theory), and how this might relate to navigational problems within electronic information. Two simple experiments designed to investigate document structure are then briefly described. In chapter 8, the author proposes a framework which represents the ergonomic factors involved in using a text, and suggests the variables to consider when designing an electronic document. The framework consists of four interactive elements which reflect the issues dominating a reader's attention at various stages in a reading process. The components of the framework include the task model, the information model, a set of manipulation skills and facilities, and a serial reading processor. The validity and utility of the proposed framework are explored in chapter 9. This describes two experiments involving readers' use of an ordinary text (on paper) and electronic texts (both linear text and hypertexts) for information retrieval. Various tools were employed to produce the electronic texts (HyperCard, TIES, a word processor and GUIDE). The experiments were conducted in a usability laboratory, and verbal protocol techniques were used to analyse readers' behaviour. In the final chapter of the book the author summarizes and discusses what has previously been said, and then turns his attention to potential applications of the framework -particularly for the development of systems based on the use of electronic text. I found this book quite useful. It draws together much of the documented scientific knowledge on reading processes, and reviews its worth in the context of comparing paper-based and screen-based visual interaction with text. Although there are many more avenues yet to explore, the ideas contained here provide some useful starting points on which to base future research.","cites":null},{"id":193693,"title":"Hyperprogramming: Building Interactive Programs with HyperCard","authors":[],"date":"1992","doi":null,"raw":"Hyperprogramming: Building Interactive Programs with HyperCard by G. Coulouris and H. Thimbleby, Woklngham, Addison-Wesley, 1992. ISBN: 0-201-56886-1. This book is aimed at those wishing to develop HyperCard applications and who already have some programming experience (for example, in a conventional third-generation language such as Pascal or BASIC). The authors suggest that their work is suitable for a number of target reader groups: novice programmers, expert programmers, students, advanced students, and HyperCard developers, and individual chapters are identified in the Preface as being of particular relevance to each target reader group. A floppy disk containing HyperCard stacks accompanies the book, and complements it by providing the HyperTalk code discussed within it. In this way, readers with a Macintosh and HyperCard are able to run the programs and, more importantly, cut and paste both objects and code for use in their own programs (known as stacks in HyperCard). There are a number of books about HyperCard, but this one is quite novel in its approach. It goes straight into HyperTalk (without lengthy description of the HyperCard interface) and combines this with the pedagogic strategy of teaching by example. Throughout the book, example code is used to introduce new HyperCard and HyperTalk topics to readers. A key benefit of this topic-based approach is that it presents material in problem-oriented ways. Readers are thus able to discover hints and tips while learning about programming in HyperTalk. They are also presented with solutions to some of the practical problems that might be encountered when they come to develop their own HyperCard stacks. There are fifteen chapters, all oriented towards practical use. The first provides an immediate introduction to programming in HyperTalk. This is followed by two chapters on the basics of working with HyperCard and HyperCard objects. The fourth chapter then describes the basic commands and control structures used in HyperTalk programming. Chapters 5 to 11 adopt the topic-basedReviews approach mentioned earlier, with each chapter presenting a new theme. They take the reader through a wide variety of practical programming issues while presenting new syntax, structures, and tools. Programming topics covered include graphics, timing, animation, menus and searching. Chapters 12 to 14 deal with the concept of hypertext. A number of issues are addressed: design, authoring, reading (browsing), and so on. The final chapter presents ideas and strategies for undertaking a variety of projects in HyperCard; the aim is to provide some indication of the flexibility and scope of environments like it. A wide range of problems are discussed, including the development of books, games and a simple spreadsheet. In addition, techniques for protecting software from tampering and virus attack are outlined. The two appendices are useful. The first provides lists of information about HyperTalk (properties, commands, messages, constants, operators and functions). The second gives a set of comparisons between HyperTalk and Pascal, designed to allow readers with existing Pascal experience to transfer rapidly between the two environments. This second appendix is very well done: the structures used in Pascal are presented with the HyperTalk nearest equivalent presented alongside. This is an excellent book for anyone wishing to program HyperCard applications, particularly if they already have some programming experience. It also has some potential for people who wish to program in Toolbook on the PC, an environment similar to HyperCard. Stephen Richards, University of Newcastle-uponTyne STELLA II version 3 (Simulation and Modelling for System Dynamics), Apple Macintosh version. When STELLA was introduced several years ago, it provided the first opportunity for relatively unskilled workers to solve complex problems which normally would have required skills in algebra and calculus. STELLA was designed to model the behaviour of dynamic systems: that is, those in which the values of various components of a system (the system variables) vary in a complex manner with time. For example, the variations in populations in an ecosystem, or drug levels after taking a medicine, vary in a time-dependent manner which can be calculated from a physical model of the system. Normally, the study of dynamic systems requires the use of calculus to set up and solve differential equations; these describe the interrelations between the quantities of interest Setting up a dynamic model involves two steps; firstly, the identification of the interrelationships between the variables, and secondly, the writing of differential equations which embody these relationships in an algebraic manner. Finally, the solution to the differential equations reveals the timedependent behaviour of the system variables. Prior to the introduction of STELLA, there were a number of rather unfriendly programs available which assisted users in setting up the differential equations, and solved them numerically. Solving the equations has always been considered the 'hard' part of the problem. Unfortunately, if you do not have the skill or training to set up the model or equations, then that is the hard part, and finding the solution is irrelevant. STELLA allowed the user to define the problem simply by drawing boxes, which represented the time-dependent quantities in the system, and joining them by flows, which represented the inter-relationships between the quantities. The basic form of the system was defined by the structure of the diagram, and only very minor mathematics was required to define the flows. STELLA then transformed the model into a set of differential equations, and solved them, thus showing how the model system changed with time. The first version of STELLA was simple and in some ways did not conform to the usual Macintosh interface; however, its idiosyncrasies were easily learnt, and it was widely used. In this form, it was capable of studying most of the problems which its users posed; as long as a simple box and flow model could be used to represent the problem, STELLA could provide an indication of the likely behaviour. Despite this, there were a number of deficiencies: some fairly simple problems could not be broken down into a box-and-flow form, and there were limits to the presentation of the output, which was either a copyable string of numbers or a PICTable graph. STELLA II began to address these problems, and STELLA II 3.0 adds yet more functionality. The new modelling tools added to STELLA II allowed processes to be used which could only poorly be represented by boxes. These include the conveyor and the oven. A conveyor is a structure into which you can feed material at one end and it will appear at the other end after 84ALTJ VOLUME 2 NUMBER 2 a delay. This is a much more realistic model of many processes. Take, for example, a model we recently used to study the flow of material through the gastrointestinal tract. Originally, we used a sequence of boxes to represent the stomach, small intestine, and large intestine. The problem with this model was that as soon as the stomach had emptied some material into the next box (the small intestine), it could be further emptied into the large intestine. Of course, this is not a physically realistic model since the small intestine is a long tube and, as we know, it takes time for material to get to the far end. A conveyor is a much more realistic structure to model this behaviour. STELLA II also allowed oneway or two-way flows, and most usefully introduced a non-negativity constraint. In the earlier versions, \u2022accidental negativity of a variable had to be checked for by the model builder, which was most inconvenient. These additions, in our opinion, covered most of the requirements of the average user. STELLA II 3.0 adds to both the computational and presentation areas. Most of the changes have to do with presentation of the results. There are now two versions, STELLA II 3.0 ordinary, and STELLA II Authoring. This is in line with many programs which now provide a full-featured version for developers, and a simplified version for end-users of the models. We were only able to test the authoring version, so cannot comment on the lesser facilities of the ordinary version. However, the claimed features of the authoring version over the other version include the ability to create stand-alone models fmicroworlds' - ugh!) which can be locked so that students can explore their operation but not mess about with their structure. The authoring version also has a mapping mode to aid the construction of complex models, and an overview mode to display the broad structure of the model. This level allows the importation of graphics and Quicktime animation (which we did not test). In this aspect, STELLA 3 (let us call it that) becomes more of a presentation package than a laboratory program, which will be useful for people who need to use simulations as a teaching or negotiation tool. The ability to create stand-alone models is most useful for teaching purposes (students may not need the full ability to create models). There are rather fewer computational additions to STELLA 3. The built-in function list is slightly longer with 11 new items. Publish and subscribe are supported, which may allow some interesting connections to other calculation packages such as Excel or Mathematica. A valuable feature is the ability to plot several runs of a model on the same graph to allow comparison of changing a particular parameter. In previous versions of STELLA, different graphs had to be drawn and each plot locked before a parameter could be changed and the model re-run to produce the new graph. This made comparison very hard and necessitated printing each individual graph. One of the most serious difficulties with STELLA and STELLA 2 was that they were available only for the Macintosh. 'Is there a PC version?' was probably the most commonly asked question we encountered in our years of showing the program to our colleagues. STELLA 3 promises to provide model transferability between Mac and PC; however a PC version was not available to us so we could not try it. The review version of STELLA 3 was provided on a single disc and occupied 1.3 megabytes of space. It loaded without difficulty onto a Macintosh 2VI and a 2CI, both running system 7 with the usual heterogenous array of extensions, and claimed 1.5 megabytes of memory, although this could be reduced to 1 megabyte. We did not perform any timing comparisons with earlier versions, but the simple models we created ran smoothly and quickly, giving the same numerical results as those from STELLA 2. Models created in STELLA 2 opened without difficulty in STELLA 3. Despite the major additions, it must be said that the original version of STELLA has a simplicity which allows new users to grasp its functions rapidly, and will do virtually all that is asked of it. STELLA 3 is easy to learn if you have used earlier versions, but its many features can confuse novices. It is a common situation with all forms of software; for example, all we usually need is old MacWrite and Multiplan, but now we have Word and Excel, with massive functionality that is rarely accessed. Who will buy STELLA 3? We would expect that people needing to present their models to audiences will find the additions invaluable, and particularly teachers associated with computerassisted learning in higher education. PC-based departments will benefit from the availability of the package on their machines. Research users of modelling software will find new features in STELLA 3, but most of the time they will not need the glitz. Clive and Neena Washington, University of Nottingham 85Reviews Note STELLA II version 3 is available from Cognitus Systems, 1 Park View, Harrogate, Yorks, HG1 5LY (tel: 0423 562622). It is available only to bona-fide educational users; others must buy the equivalent commercial package. Prices are: Authoring \u00a3399, Standard \u00a3249; upgrades and LAN packs are also available.","cites":null},{"id":193694,"title":"The Open Learning Handbook - Promoting Quality In Designing and Delivering Flexible Learning","authors":[],"date":"1993","doi":null,"raw":"The Open Learning Handbook - Promoting Quality In Designing and Delivering Flexible Learning by Phil Race, 2nd edition, London, Kogan Page, 1993. ISBN: 0-89397-392-0. The transformation of higher education into a system adapted to servicing the needs of much larger numbers has created a demand for flexibility. Distance and open-learning methods free students from constraints of time and place, and can allow more individualized feedback than traditional teaching programmes. However, there is also an emphasis on auditing for quality - efficiency must be complemented by a concern for standards and excellence. This book is intended both for those already involved in the delivery of open learning, and those moving into the field for the first time. There are two main aims: firstly, to provide guidelines on good practice for those producing open-learning materials; secondly, to advise tutors and mentors on the effective support of open learning. The book is a readable, attractive reference text, packed with very useful checklists. The author is clearly drawing widely on his own experience, and although not an open-learning package itself, the book provides a feel for the subject by incorporating many of the features of open learning: each chapter begins with an abstract and a list of learning objectives, and some contain self-assessment questions. Chapter 1 discusses the meaning of the term open learning, setting it in the context of the whole learning process. Chapters 2 to 7 cover the production of open-learning materials, from the initial design stage through to marking student assignments. Chapters 8 and 9 describe respectively the role of tutors and mentors. Chapter 10 focuses on the integration of open-learning materials into traditional courses. The book concludes with an annotated bibliography and an index. It should be noted that the production, choice and use of non-print media is not covered. I would single out Chapters 2,3,4 and 6, on the design and production of open learning materials, as being of particular merit. In Chapter 2 ('Designing for open learning'), the author takes a realistic approach, opening with a discussion about the relative merits of adopting or adapting existing materials, or starting from scratch. A full description of the difference between open-learning materials and traditional textbooks is given. The importance of piloting materials is highlighted, alongside suggestions for obtaining meaningful feedback from the piloting phase. A Quality Checklist is also provided. Chapter 3 concentrates on the importance of showing learners where they are heading. Syllabus content is now widely expressed (GNVQ\/NVQ, for example) in terms of intended learning outcomes: competences, performance criteria and range statements. The section discussing these terms is therefore particularly relevant. Chapter 4 focuses on the purpose and design of self-assessment questions, and the provision of poor, as well as good examples, is most helpful. Chapter 6, on the design of tutor-marked assignments, benefits from covering not only expected topics, such as the development of marking schemes, but goes on to discuss how the delivery of constructive, supportive feedback might be achieved effectively in open-learning situations. Chapter 5 moves on to the importance of adopting informal user-friendly language when producing open-learning materials, and the readability of this book as a whole demonstrates the efficacy of this approach. However, emphasizing readability above all else can have disadvantages: using an example from this book itself, improving flow by deliberately omitting literature references in the text could hinder those readers wishing to research specific topics in more detail (despite the inclusion of an annotated bibliography). In the same vein, the lack of section numbers gives an informal appearance, but is an irritation when making reference to the book. There is also the danger that 'friendly' language can so easily become patronizing chattiness; thankfully, the author is too experienced to fall into this trap very often. Despite my minor criticisms, I would have no hesitation in recommending this handbook as a useful resource for those interested in the development of open learning materials, and those involved in tutoring open learners. Lorraine Warren, Hull 86","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"1994","abstract":"Europe In the Round CD\u2010ROM, Guildford, Vocational Technologies, 1994","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/7038.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/163\/1\/ALT_J_Vol2_No2_1994_Reviews.pdf","pdfHashValue":"508d99c5131cb3a8df3ec925f9df2b260d07010b","publisher":"Universit of Wales Press","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:163<\/identifier><datestamp>\n      2011-04-04T09:26:23Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/163\/<\/dc:relation><dc:title>\n        Reviews<\/dc:title><dc:creator>\n        Jacobs, Gabriel<\/dc:creator><dc:creator>\n        Pennington, Gus<\/dc:creator><dc:creator>\n        King, Terry<\/dc:creator><dc:creator>\n        Gillie, Tony<\/dc:creator><dc:creator>\n        Banerji, Ashok<\/dc:creator><dc:creator>\n        Tan, Check<\/dc:creator><dc:creator>\n        Barker, Philip<\/dc:creator><dc:creator>\n        Collis, Betty<\/dc:creator><dc:creator>\n        Richards, Stephen<\/dc:creator><dc:creator>\n        Clive, <\/dc:creator><dc:creator>\n        Washington, Neena<\/dc:creator><dc:creator>\n        Warren, Lorraine<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        Europe In the Round CD\u2010ROM, Guildford, Vocational Technologies, 1994.<\/dc:description><dc:publisher>\n        Universit of Wales Press<\/dc:publisher><dc:date>\n        1994<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/163\/1\/ALT_J_Vol2_No2_1994_Reviews.pdf<\/dc:identifier><dc:identifier>\n          Jacobs, Gabriel and Pennington, Gus and King, Terry and Gillie, Tony and Banerji, Ashok and Tan, Check and Barker, Philip and Collis, Betty and Richards, Stephen and Clive,  and Washington, Neena and Warren, Lorraine  (1994) Reviews.  Association for Learning Technology Journal, 2 (2).  pp. 75-86.  ISSN 0968-7769     <\/dc:identifier><dc:relation>\n        10.1080\/0968776940020208<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/163\/","10.1080\/0968776940020208"],"year":1994,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"Reviews\nEurope In the Round CD-ROM,\nGuildford, Vocational Technologies,\n1994.\nEurope in the Rotmdis a hypertextuall\nhypergraphical database of facts about Europe,\nbiased towards education and training, but\ntaking in its stride many peripheral areas\nrelevant to anyone considering living and\/or\nworking in, or even visiting, a member state of\nthe EU - population, economy, institutions, geo-\ngraphy, climate and so forth - all making up\nthousands of interconnected screens. These\nscreens contain maps with a selection of zooms\nand hotspots, all kinds of facts and figures about\navailable educational and vocational courses\n(presented both textually and, where\nappropriate, as graphs, bar charts and such like),\ndetails of life in each EU member state, details of\ncities and what they have to offer, and a lot\nmore. There are also extra facilities, such as\nbeing able to set currency exchange rates or to\ncreate customized notebooks.\nThe PC version received for review (on CD-\nROM - it is also available on floppy disks) has\nbeen written in Toolbook. The Macintosh\nversion (the original) is based on HyperCard.\nBoth were first published in 1991, and the design\nand structure of the software have changed little\nsince then, but this latest issue contains more\ninformation, and is as up to date as software of\nits kind can be. Improvements and additions\ninclude a new section on regional statistics, new\nsub-sections on work and finance for students, a\nnew section on EFTA, information on the\nEuropean job vacancy system, enhanced road\nmaps, distances between cities, a powerful index,\ngenerally improved links between sections, and\nmore use of colour.\nThree years (1991 to 1994) is a long spell in the\nworld of educational technology, yet the user-\ninterface of Europe in the Round looks as fresh\nas it ever did. And perhaps not surprisingly,\nsince many of the innovations it brought with it\nhave become well-tried features of multimedia\nCAL and reference software. The interface\nincorporates universal icon meaning and\nconsistent positioning on the screen, never\nshowing inactive icons, navigational clues\nprovided by different transitions between\nscreens, and the like, all of which shows how\nfar-seeing the members of the design team were.\nThe package lacks sound, though one cannot\nbut agree with what Jacquetta Megarry (its\nproducer) wrote not long after its initial\npublication (Megarry, 1991), that adding sound\nwould in turn have added little or nothing in\nterms of usefulness, while eating up computer\nmemory and human and financial resources.\nOne could also say that adding sound would be\nas risky today as it was in 1991: every extra\nmultimedia element brings with it an increased\nchance of instability.\nI mention this because my practice these days\nwhen reviewing any multimedia package is to\ngive it a tough test: as well as running it on a\nstandard 'clean' machine (where there is rarely\nany problem, and indeed none with Europe in\nthe Round), I run it too on a PC which conforms\nto MPC standards but which is also packed\nwith all manner of cards and device drivers. I do\nthis because not everyone has a perfectly clean\nmachine. If a piece of software passes this test, I\ncan be fairly certain of its robustness. Few\nprograms do, nor did Europe in the Round. I\nhad severe installation problems, including\nsome non-sensical messages (such as inadequate\ndisk space when I had over 100 MB free), and\nwhen I eventually managed to install the\nsoftware by ignoring the instructions in the\nmanual, it crashed or hung on me with irritating\nfrequency. The other side of this coin is, of\ncourse, that the test was particularly acid.\nFurthermore, at the end of the manual is a\ntroubleshooting section. This does not tell you\nhow to get out of difficulties, but rather how to\ncontact the publishers (far better than the usual\nadvice about checking whether or not you have\nplugged in the computer). In other words, the\npublishers are happy to help with problems of\ninstallation or running. I should emphasize this\npraiseworthy attitude rather than the problems\nI encountered which were, after all, the result of\ndifficult conditions. So I do.\nIn any case, this is my sole major complaint.\nOther complaints (such as the occasional use of\nthe word card for screen, a long-standing\nHyperCard hangover, or the fact that the\nprogram assumes a VGA display and therefore\ndoes not use the full screen in SVGA mode on a\nlarge monitor) detract little from the overall\nimpressiveness of the package. For it really is a\n75\nReviews\nveritable goldmine of information presented in a\ndigestible form. Indeed, why it has not yet been\ntaken up by large numbers of public\ninformation services throughout Europe is a\nmystery to me. It is certainly the best-presented\ninformation on opportunities for work and\nstudy in the EU, and its built-in facility for\ncreating slide-show demonstrations seems\ntailor-made for public access: it should be\navailable in the entrance halls of every\neducational establishment with European links.\nIt is, very clearly, a tool which can be profitably\nused by students, whether in HE or not,\nwhatever they are studying, and whether or not\nthey intend to study or work in Europe. It also\nrepresents a fascinating case study in good\npractical CAL design.\nGabriel Jacobs, University of Wales, Swansea\nReferences\nMegarry, J. (1991), 'Europe in the Round:\ntheory into practice', Interactive Multimedia, 2,\n3.\nMultilingual Multimedia - Bridging the\nLanguage Barrier with Intelligent\nSystems edited by Masoud Yazdanl,\nOxford, Intellect Books, 1993. ISBN: 1-\n871516-30-7.\nThis book is edited by Masoud Yazdani, who in\naddition to having written the preface, has a\nchapter to himself, and is joint author of no less\nthan four of the remaining eight chapters.\nYazdani is also the founder of Intellect, the\nbook's publisher. Is this work therefore some\nkind of ego trip? Well, no more than a single-\nauthor book is, and on the whole the work is\ncertainly worth reading.\nThe main theme is trans-language\ncommunication with the help of software tools\nsuch as grammar checkers and icon-based (non-\nlinguistic) systems. Along the way, however,\nother themes pop up, some of which will be\nrelevant to all readers of ALT-J.\nThe book opens with a descriptive piece about a\nprototype system for practising foreign\nlanguages in a restaurant, based around\ncartoons, sampled sound and hyperlinks. I have\nseen this program in action on a Macintosh. It\nis impressive when demonstrated but, as the\nauthors of the chapter admit, its most serious\nlimitation is that it does not allow for any\nalteration of the material.\nThe second chapter is concerned with a\nmultilingual approach to databases intended to\nbe accessed by speakers of different languages.\nIt mostly says little other than comparatively\nobvious things: such databases ought to have\nmultilingual interfaces, language-specific\nbrowsing facilities, non-linguistic systems of\ncommunication (such as icons, or video\nsequences without the use of language) where\nappropriate, and so on. However, the chapter\ndoes pick up when it treats cultural differences.\nChapter 3 is the first of two dealing with\ncommunication by icons and pictures. The idea\nof using graphics alone, to communicate ideas is\nstudied in a fair amount of depth, and while the\ndesign of such graphics is fraught with all\nmanner of difficulties (neatly outlined here), it is\nclear that there is considerable scope for using\npictures in a multilingual environment provided\none is careful about which pictures to use.\nBy the time we reach Chapter 5, on generalizing\nlanguage tutoring systems, we have left the soft\nstuff and have entered the realm of clause\nanalysis, syntactic representation and lexical\nprocessing. We are also introduced to\nLINGER, an expert-system grammar-checking\nprogram for would-be programmers with little\nprogramming experience.\nLINGER also forms the basis of Chapter 6,\nwhile Chapter 7 treats its successor known as\neL. The problems involved in checking\ngrammar are formidable, and the two chapters\n(especially Chapter 7) are at least useful in\nshowing just how difficult it is to get a computer\nto 'understand' natural language.\nChapter 8 continues the theme of checking\nnatural-language syntax by looking at some\nproblems and possible solutions in this area\nwhen error-checking software is used with\nlearners of English as a foreign language. There\nis a product review, together with a long\ncomparison between the seven programs\nconsidered. The unsurprising conclusion is that\nall the programs fail, in different ways, in terms\nof the (rightly) exacting criteria used for\nevaluation.\nThe final short chapter is a critical assessment\nof Artificial Intelligence as used in language\nteaching, again comparing some available\nproducts.\nFor me, the downside of this book is that it\nappears to have been put together somewhat\nartificially. The unifying motif in the mind of\nthe editor has clearly been that of addressing the\nproblem of multilingual electronic\n76\nALTJ VOLUME 2 NUMBER 2\ncommunication, but the reader has to work\nhard to see some of the direct connections\nbetween chapters. Furthermore, there is an\nappreciable disparity between material of\ninterest to researchers but probably beyond\nbeginners, and more basic material for relative\nnovices.\nNevertheless, the book will be of interest both\nto those concerned with computer-assisted\nlanguage learning and those concerned with\nmaking courseware in general available to\nspeakers of different languages. The chapters on\niconic communication will also interest\ncourseware developers in all disciplines.\nGabriel Jacobs, University College of Swansea\nUsing Records of Achievement In\nHigher Education edited by A. Assiter\nand E. Shaw, London, Kogan Page,\n1993. ISBN: 0-7494-1111-2.\nThis is the latest in the Using... series of books,\nand it will be of particular interest to all in\nhigher education who have responsibility for\ncurriculum design and delivery. The material\npresented is drawn from the Using Records of\nAchievement (ROA) conference organized by\nthe HEC in 1992, and like that event the main\nthrust of contributions is very much directed\ntowards educational practice, the enhancement\nof student learning and informed professional\naction.\nThe origins of ROA and the use of associated\nportfolios are rooted in two perceived\ndeficiencies of the higher-education system.\nFirst, an acknowledgement of the limitations of\nconventional examinations in doing justice to\nthe full range of learning achieved by students;\nand secondly a growing realization of the\neducational value to students of involving them\nmore fully in the reviewing, monitoring and\nrecording of their own learning. This latter\nstance has, of course, been at the centre of\nEnterprise and Capability programmes,\nalthough its importance is also to be found in a\nmuch older tradition which values autonomy in\nlearning as one of the distinguishing features of\nuniversity education.\nThe publication is timely, and presents a nicely\nbalanced blend of case studies, contextual issues\nand practical guidelines presented in an easy-to-\nread format. Both new and experienced staff\nwill find much to interest them here, and the use\nof bullet-point listings works well for readers\nwho want to engage with the text quickly. Some\n22 short chapters are presented under a number\nof organizing terms:\n\u2022 the use of ROAs and profiles for access,\nadmissions and accreditation of prior\nlearning;\n\u2022 the development of skills, both personal\nand professional;\n\u2022 the empowerment of learners through\ninsight into their own motivations, needs\nand actions;\n\u2022 the facilitation and assessment of work-\nbased learning.\nAny reader pondering these issues will be\ninterested in the cases and descriptions of\npractice from fellow-travellers who have already\ngrappled with them and who have begun to\nformulate responses. It is particularly welcome\nto see the odd contribution from employers\ninterleaved with those of academic staff, and\nsalutary to note that others from very different\ncontexts may have much of value to share with\nus. To this reader, the material certainly\nachieves its stated intention of 'providing a\nglimpse of what others are doing and what they\nhave learned by doing it', and herein lies its\nparamount strength.\nAn important sub-theme running through the\nvariety of perspectives is the deployment of\nlearning contracts and their importance as a\nmechanism for mediating the needs and\ninterests of students, tutors and employers.\nFirst-hand accounts of the management of these\nprocesses provide valuable insights into the\npractical difficulties surrounding attempts to\nintroduce innovation into higher-education\ninstitutions.\nIf there is a deficiency in the book - and this is a\nsmall one - it is that the students' voice and\nperspective is not placed alongside those of the\nother key participants. This is an irony, given\nthe commitment in many of the sections to a\nstudent-centred approach and learner\nempowerment. Despite that reservation,\nhowever, this volume makes a significant\ncontribution to what is an evolving area of work\nin many institutions, and as such is\nrecommended as an excellent starting point for\nthose considering similar initiatives.\nGus Pennington, University of Teesside\n77\nReviews\nHandbook of Educational Technology\nby Henry Ellington, Fred Percival and\nPhil Race, 3rd edition, London, Kogan\nPage, 1993. ISBN: 0-7494-0849-9.\nThis publication gives a taste of a wide field,\nand is supported by an extensive glossary of\nterms used in educational technology, and a\nbibliography of recent articles and books.\nThe authors go to some lengths to explain that\nby 'educational technology', they are\nemphasizing more the sense of 'the technology\nof education' than 'technology in education'. In\nthis respect, they deal extensively with the way\nin which effective education may be crafted or\nengineered rather than considering the\ntechnological devices which might be employed.\nTo this end, the book has chapters on basic\neducational strategies, educational objectives,\ninstruction and learning techniques, assessment\nand evaluation, and includes a chapter on\nresource centres. Each chapter is well structured\nwith a basic but detailed outline of the topics\nunder consideration, with their advantages and\ndisadvantages.\nIn the first chapter, the authors introduce a\nsimple model of learning comprising the four\nstages of 'wanting, doing, feedback and\ndigesting'. This is an interesting addition to the\ninformative but rather dry chapter. The authors\ncontinue to work with this model throughout\nthe book, often referring to it to support a\nparticular instructional technique or\neducational approach. This helps the book to be\nmore cohesive and rounded than it would\notherwise have been, and will be very helpful to\na reader with little or no exposure to theories of\nlearning. The authors also posit a simple idea\nthat there is a continuing trend from mass\neducation, through individualized education, to\ngroup learning, something which is again used\nthroughout to link otherwise diverse sections.\nHowever, the reader whose interest is in\ntechnological devices which can facilitate\neducational techniques, and moreover is\nlooking for basic detail on the specific\neducational applicability of hypertext or\nhypermedia, groupware or cognitive strategies,\nor is keen to delve into the wider social or\npolitical aspects of the introduction of\neducational technology, will be disappointed.\nAnd the disappointment may be particularly\nacute in the last chapter where four out of seven\nof expected future trends in education have\nrecently been under considerable attack by\nGovernment ministers (two others are\nsupported, but possibly for reasons of cost-\nsaving or social engineering, rather than in the\ninterests of effective education).\nChapter 10 deserves a detailed look in the light\nof my overall criticism of a lack of being up to\ndate: it offers a quite limited review of\ncomputers in education. I have not read the\nearlier editions of this handbook, but the\nchapter has a rather dated look-feel, with\nillustrations of a monstrously huge mainframe\nand of 'typical' textual computer interfaces\ncapable of displaying only very basic graphics.\nAlthough the end of the chapter seems to have\nbeen updated to include some sections on email,\nhypermedia, CD-I and Photo-CD, these do not\nseem to hang together very well, and the chapter\nas a whole still seems out of date.\nUnfortunately, in such a fast-moving field, the\nsuccessive updating of a handbook first\npublished in 1984 leaves an overall effect rather\nlike that of an archaeological dig, where each\nera overlays another. A handbook on\neducational technology that has more on\nfilmstrip projectors than hypermedia must be a\ncandidate for a complete re-write.\nThis is a suggested handbook for trainee\nteachers and other students of education or\ncomputers in education, and as a basic\nintroduction I suppose it works reasonably well.\nSupplemented as it is with a good bibliography,\nit will provide a fair starting point for any\nstudent, or indeed lecturer, with no previous\nexposure to educational theory. However, any\nnovice in the field of educational technology\nwould have to be encouraged to move on to\nmore comprehensive texts as quickly as possible\nto avoid the over-simplified Weltanschauung\npromoted by this publication.\nTerry King, University of Portsmouth\nThe Principles of Screen Design for\nComputer Based Learning Materials\nby Alan Clarke, 2nd edition, Moorfoot,\nSheffield, Learning Methods Branch,\nEmployment Department, 1992.\nThis book aims to inform authors of computer-\nbased learning materials of findings in the\nresearch literature relevant to screen design. In\nreality, it draws on work that relates to\ninformation presentation in general, whatever\nthe medium. There are chapters devoted to\nelectronic text, colour, and graphics, with a\ncollection of guidelines at the end of each\nchapter (the guidelines are also summarized at\n78\nALTJ VOLUME 2 NUMBER 2\nthe end of the book). Unlike many other sets of\nguidelines, this book does actually go into some\ndetail of the research it draws on, rather than\nsimply providing references. However, although\nthis format allows the reader to make an\nassessment of how generalizable the guidelines\nmay be, it does make it harder to find relevant\nadvice, a problem compounded by the lack of\nan index.\nApart from minor niggles with the number of\ntypographical errors, ungrammatical sentences,\nand the use of sexist language, I have other\ndeeper concerns with this book. My first is that\nit contains no discussion of the design process\nitself. I found no mention, for example, of\ninvolving users in developing material, no\nmention of evaluation, and no mention of\niterative design. Although none of these things\ncan guarantee producing good displays, they\nwill certainly be at least as effective as guidelines\nin helping to avoid unusable ones.\nA second concern is that the book contains an\nuncomfortable amount of what might be termed\nnaive psychology. For example, the assertion\nthat 'long term memory seems to operate in a\nsimilar way [to short term memory]' (it doesn't),\nand the suggestion that Miller's classic 'magic\nnumber seven' paper on the limits of short-term\nmemory informs the decision of how many\ncolours to use in any one display (it doesn't).\nThis simplistic style of presentation does little to\npoint to the importance of considering\npsychological factors when designing screen\ndisplays and learning materials (see, for\nexample, Helander 1988).\nA third concern is that the book is not\ncomprehensive. Seen simply as a collection of\nguidelines, there are surprisingly few of them\n(less than 80 in all). There are also surprising\nomissions from the list of references. For\nexample, there is no mention of the classic work\nedited by Smith and Mosier (1986) who list over\n900 guidelines (along with supporting\nreferences) for designing user-interface software,\nof which almost 300 are concerned with data\ndisplay alone. Finally, there is little discussion\nof the way particular authoring tools impose\ntheir own style on the finished product, or of the\nrecommendations made in the interface style\nguides produced by Apple and Microsoft (for\nexample, Microsoft 1992).\nInterestingly, Clarke is careful to distinguish\nbetween guidelines ('hypotheses which [are]\nsupported by evidence and dependable within\nlimits') and principles ('guidelines which-[are]\ndependable in a wide range of situations,\nlocations and with different groups of learners').\nHe concedes that much of the material\npresented in the book should be considered as\nhypotheses and guidelines, rather than\nprinciples, which makes one wonder about the\ntitle of the book. However, if you are looking\nfor a few simple guidelines for a computer-\nbased learning project, it may be a useful\nstarting point, even if, in my opinion, Smith &\nMosier or Helander would almost certainly be\nbetter.\nOverall, though, I have to confess to a certain\nscepticism that mechanically applying\nguidelines, whatever their source, will help\nanyone to design effective displays or learning\nmaterials. Involving users at an early stage,\nconducting proper evaluations, and using a\nstrategy of iterative design are far more likely to\nproduce successful results.\nTony Gillie, University of Reading\nReferences\nHelander, M. (ed) (1988), Handbook of Human-\nComputer Interaction, Amsterdam, North\nHolland.\nMicrosoft (1992), The Windows Interface: An\nApplication Design Guide, Microsoft Press.\nSmith, S.L. and Mosier, J.N. (1986), Guidelines\nfor Designing User Interface Software, The\nMitre Corporation, Report ESD-TR-86-278.\nCopies of this monograph are available free of\ncharge from Cambertown Ltd, Unit 8,\nGoldthorpe Industrial Estate, Goldthorpe,\nRotheram, South Yorkshire S63 9BL. Quote\nreference 50-OL121 when ordering.\nMultimedia Mania by D. Paulissen and\nH. Frater, Michigan, USA, Abacus\nSoftware and Dusseldorf, Germany,\nData Becker, 1993. ISBN 1-55755-166-\n9.\nIt is well known that information is more\neffectively communicated, and messages more\npersuasively delivered, through the use of an\nappropriate mixture of text, sound and images.\nWhat is new about current approaches to\nmultimedia is the technological opportunity\nthat now exists for delivering such information\nvia computers, but of course handling the\nvarious types of data involved is not easy.\nIndeed, in order to complete a multimedia\n79\nReviews\nproject successfully, specialized hardware and\nsoftware are usually needed, as well as relevant\nexperience. Unfortunately, few (if any) of the\ncurrently available books on multimedia cover\nall the necessary techniques in a way that is\nsuitable for newcomers to the field. Multimedia\nMania is a two-part polymedia publication that\nattempts to fill the gap. It consists of thirteen\nchapters, three appendices and a subject index\n(all printed on paper) and a CD-ROM\ncontaining a large number of demonstration\nand shareware programs.\nThe first chapter briefly describes various\nmultimedia applications, providing a useful\nintroduction for newcomers. The second\nchapter describes the multimedia PC (MPC)\nstandard and the various technologies (CD-\nROM, audio, video, image acquisition, etc.)\nnormally associated with multimedia.\nChapters 3 and 4 will be of interest to those\ninvolved in acquiring and installing a new MPC\nsystem. Configuring a multimedia PC is often a\ntricky job - particularly, for sound recording and\nplayback. And in this context, the tips provided\nhere will be found very helpful, especially with\nrespect to the installation of sound cards.\nFollowing on from this, the fifth chapter (on\nsound recording) describes the processes involved\nin making digital recordings using a PC. The\naccompanying CD-ROM is used to expose\nreaders to a sample music database developed\nusing Microsoft Excel as a retrieval engine. It\nalso takes readers through a hands-on session\nwith the EZSound FX sound-editing utility.\nCapturing and processing images are covered in\nchapter 6. Here, the Hijaak screen capture\nprogram (provided on the accompanying CD-\nROM) is described in some detail - anyone who\nintends to prepare a multimedia presentation\nwill find this software useful. Scanners and\nimage processing are the other two important\ntopics covered in the chapter, which also\nprovides up-to-date information on Kodak's\nPhoto CD.\nChapter 7 provides very good coverage of\nanimation and its associated techniques. The\nexamples presented include animation with\nToolbook and Autodesk Animator. Examples\nof 3-D animation are also there, and again, the\ndemonstrations provided on the CD-ROM will\nbe of help in learning about this topic. For those\ninterested in MIDI, chapter 8 will be of\nsubstantial practical value: it contains a wealth\nof tips and guidance on musical applications of\nthe MPC.\nSounds and pictures as essential components of\nmultimedia presentations are the subject of\nchapter 9, the style and content of which make it\nof significant value for those starting off in multi-\nmedia. The wide range of demonstrations (based\non MMPLAY, TEMPRA SHOW, Authorware\nStar, Excel Slide Show, and PictureBook\nProfessional) provide excellent hands-on experi-\nence. Authorware Star (which looks very similar\nto Authorware Professional) provides a useful\nway of sampling some of the features of a more\nsophisticated authoring system. The use of\nAuthorware Star is further discussed in chapter\n10 on authoring and hypertext, where methods\nfor creating hypertext systems using Microsoft's\nViewer are also described, as is the use of the\nMultimedia Development Kit (MDK).\nDigital video is covered in chapter 11 which is in\nsome ways disappointing: although a number of\ndifferent video cards are now available\ncommercially, the authors of this book choose\nto discuss only the MicroEye video output card,\nwhich obviously this limits the scope of the\ntreatment of video hardware. The software-\nbased approach for handling video is illustrated,\nhowever, using Microsoft's Video for Windows\n- a far more widespread product - which is\ndiscussed in some detail. And it is good to see\nthat the authors have not omitted the important\ntopic of virtual reality, treated in Chapter 12.\nThe last chapter, on Visual Basic, will be of par-\nticular interest to programmers who want to\nwrite code for multimedia applications. The\nexamples on the CD-ROM will help them\nexplore and experiment with the MCI (Media\nControl Interface) commands within Windows\n3.1.\nThe three appendices provide a glossary, an\noverview of various types of multimedia\nproducts, and a summary of the material held\non CD-ROM. The second of these gives a wide\nrange of information sources that multimedia\ndevelopers will be glad to have: it includes\ndetails of animation software, authoring\nsystems, image editors, MPC upgrade kits, PC\nsound cards, sound editors, video editing\nsystems, virtual-reality equipment, and suppliers\nof CD-ROM magazines.\nThis book will undoubtedly be a useful\ninformation resource for anybody wishing to\nenter the field of multimedia PC computing,\nand the tips it offers, as well as the CD-ROM\nbundled with it, make it a good buy.\nAshok Banerji and Check Meng Tan, University\nof Teesside\n80\nALTJ VOLUME 2 NUMBER 2\nDesigning Usable Electronic Text\n- Ergonomlc Aspects of Human\nInformation Usage by Andrew Dillon,\nLondon, Taylor and Francis, 1993.\nISBN: 0-7484-01130-X.\nIncreasingly, text is being viewed not in printed\nform but on a computer screen. This book is\ntherefore a timely one, as it concentrates on the\nergonomic and human-factors issues underlying\nthe design of electronic text.\nIt has ten chapters, a bibliography, a single\nappendix and a subject index. Following a short\nintroductory scene-setting chapter, the author\nmoves straight into a discussion of usability and\nthe potential utility of ergonomics and human\nfactors within this area. This is followed by a\nreview of the experimental literature on reading\nfrom paper and from screens. Two basic strands\nare considered: outcome measures (speed,\naccuracy, fatigue, comprehension and\npreference) and process measures (eye\nmovements, manipulation and navigational\nissues). The review is organized into three basic\nparts, covering reported differences between\npaper and screen, analysis of differences in\nterms of physical, perceptual and cognitive\nprocesses, and issues relating to text and task\nvariables.\nIn chapter 4, the author examines the value of\nexisting human-factors literature to electronic\ndocument designers. The problems of applying\nthis knowledge to the design and evaluation of\nan electronic-text system are then illustrated by\nmeans of a case study (an interactive document\nretrieval system based on a CD-ROM called\nADONIS).\nChapter 5 delves into the problems of\nclassifying texts by means of repertory grid\nanalysis using the FOCUS program, and the\nimplications of the results for electronic text\ndesign are then discussed. Chapter 6 explores in\nmore depth readers' interactions with two\nparticular types of text - academic journals and\nsoftware manuals. The methodology used in the\ninvestigation is based on the WWH (Why, What\nand How) approach to document usage). The\nchapter concludes with a discussion of the\ndesign implications of the research findings.\nThe material presented in chapter 7 is concerned\nwith the literature dealing with readers'\nimpressions of structure and shape in\ninformation space (based on schemata theory),\nand how this might relate to navigational\nproblems within electronic information. Two\nsimple experiments designed to investigate\ndocument structure are then briefly described.\nIn chapter 8, the author proposes a framework\nwhich represents the ergonomic factors involved\nin using a text, and suggests the variables to\nconsider when designing an electronic\ndocument. The framework consists of four\ninteractive elements which reflect the issues\ndominating a reader's attention at various\nstages in a reading process. The components of\nthe framework include the task model, the\ninformation model, a set of manipulation skills\nand facilities, and a serial reading processor.\nThe validity and utility of the proposed\nframework are explored in chapter 9. This\ndescribes two experiments involving readers' use\nof an ordinary text (on paper) and electronic\ntexts (both linear text and hypertexts) for\ninformation retrieval. Various tools were\nemployed to produce the electronic texts\n(HyperCard, TIES, a word processor and\nGUIDE). The experiments were conducted in a\nusability laboratory, and verbal protocol\ntechniques were used to analyse readers'\nbehaviour.\nIn the final chapter of the book the author\nsummarizes and discusses what has previously\nbeen said, and then turns his attention to\npotential applications of the framework -\nparticularly for the development of systems\nbased on the use of electronic text.\nI found this book quite useful. It draws together\nmuch of the documented scientific knowledge\non reading processes, and reviews its worth in\nthe context of comparing paper-based and\nscreen-based visual interaction with text.\nAlthough there are many more avenues yet to\nexplore, the ideas contained here provide some\nuseful starting points on which to base future\nresearch.\nPhilip Barker, University of Teesside\nCAL Into the Mainstream: Computer-\nAssisted Learning (selected\ncontributions from the CAL 93\nSymposium, 5-8 April 1993, University\nof York), edited by M.R. Kibby and J.\nHartley, Oxford, Pergamon Press,\n1994. ISBN: 0-08-041945-3.\nIt is somewhat difficult to choose the most\nuseful way to summarize a book of selected\ncontributions from a conference. The reader\n81\nReviews\ndoes not know if the contributions selected (in\nthis case, 21) are meant to represent as faithfully\nas possible the range of interest areas and\ncontributions presented at the conference, or if\nthey are chosen on a basis of attempted balance\namong institutions or regions represented\namong the participants, or some other criterion\nsuch as a mix of 'old' and 'new' contributors.\nPerhaps the standard is purely scientific: the\neditors' opinions of the most valuable entries.\nBut because of this uncertainty, the reader\ncannot fairly approach the book as an inventory\nof British activities with computers in\neducation, nor as conference proceedings. Thus,\nhow to proceed? By treating the book as a\nbook, and thus expecting that the title is an\norganizer for the content? This approach does\nnot help very much either, in that the reader\nentering the book through interest in its title\ngets very little sense of how much computer-\nassisted learning has gone 'into the mainstream*\nof UK education after reading the entries. This\nsaid, perhaps the only way to address the book\nis as a hard-covered journal, with a collection of\n21 articles offered without any overt editorial\nsynthesis.\nThat rather long-winded introduction is by way\nof explaining my choice of procedure for this\nreview. I will give a general comment about the\ntopics and settings addressed by the articles,\nthen note some that I have found to be\nparticularly useful.\nThus the overview of contents: 17 of the 21\narticles are from UK author teams, one a team\nof mixed UK and non-UK authors, and three\nfrom other countries (Canada, Germany and\nFinland). As to content and setting, my analysis\nis that:\n\u2022 three deal with software development\nmethodology;\n\u2022 ten could be described as 'specific-case'\nstudies situated at the university level,\n\u2022 seven of which I would most generally\ncategorize as case studies of particular\nprototype packages, including\ndescriptions of the theory of their designs\nand development processes and\nevaluations based on the\nauthor\/designers' experiences with the in-\nhouse use of the package,\n\u2022 one of which is a more cognitively-\noriented study of an experiment involving\nan existing package (a concept mapping\ntool),\n\u2022 and two of which relate, in different ways,\nto distributed learning (OU students'\nexperiences with home computer use in a\nparticular OU course, and experiences at\nLancaster with CMC);\n\u2022 four relate to experiences with children as\nthey use a particular software\nenvironment;\n\u2022 one, from Finland, studies a group of\nlower-secondary students and a\nLEGOLogo experience;\n\u2022 three, including one from Canada, look at\ngroups of primary students as they use\neither modelling software, or a prototype\ndeveloped by the authors, or a CD-ROM\n'talking storybook';\n\u2022 two describe case-study type observations\nabout groups of students in initial teacher\neducation at two institutions (articles 17\nand 19 - these articles were so similar in\nstyle and substance that I looked back\ntwice to see if I was not reading the same\narticle a second time); and\n\u2022 two describe, from large-scale and\nlongitudinal perspectives, implementation\nexperiences about computers in\neducation.\nThe latter two in the categorization I offer\nabove are, in my opinion, the two articles which\nmost substantially relate to the title of this\nbook. They are the two I solidly recommend to\nanyone interested in what happens when large-\nscale initiatives try very hard to stimulate\ncomputer use in schools. They are also two of\nthe three articles from the collection I have\nalready cited in my own writings, and will\ncontinue to use as valuable references, not only\nbecause they are based on broadscale samples of\nexperience, but also because of the value of the\ninsights they offer.\nThe first of these two, whose title does not do\nmuch to suggest its contents, is by Alan Brown\n('Processes to support the use of information\ntechnology to enhance learning') and is based\non an evaluation of the UK Information\nTechnology Teacher Training Development\nProgramme (1988-1992). It presents many rich\nand useful insights from seven projects focusing\non the management of IT use by teachers, and\nways teachers use IT to enhance learning in the\nclassroom. The second, by John Gardner et al\n('Learning with portable computers'), is a\nfascinating study of a broadscale attempt,\n82\nALTJ VOLUME 2 NUMBER 2\ninvolving nine schools and 235 pupils, to give\nstudents unlimited access to personal computers\n(at home and at school) over an entire school\nyear, then to investigate the impact of this\naccess on their performance in core school\nsubjects. The article, whose findings deserve\ncareful study by anyone interested in 'the\nmainstream', again points out the critical\ninfluence of the teacher in what happens with\ncomputers and learning.\nThe third article I found particularly useful is by\nMike Aston and Bob Dolden ('Logiciel sans\nfrontieres'), again a title which will not help the\nreader to identify its contents if seeing only a list\nof titles. This article is of less general interest\nthan the ones by Brown and Gardner et al, but\nof particular value to those involved in software\ndevelopment from a broad perspective. The\narticle summarizes some of the experiences from\na number of European-level projects involving\ncross-national educational software portability.\nAs for the rest of the articles, well, I certainly\ncan say they are well-written, well-edited, and a\nnumber of them I have noted for myself because\nof their relation to my various research\ninterests. But the reader will most benefit from\nthem by seeing them in wider contexts. Studies\nof the generic type: 'What a certain group of our\nstudents did with our prototype and what we\nhave learned from the experience', for example,\nare most valuable when one can place such\nobservations into the context of other studies\ninvolving similar cases in order to see where\ntrends and insights transcend the particular case\nunder description. For the reader interested in\nthe various types of software or IT-use\nsituations described in the case-study-like\narticles in this book, there will certainly be good\nreferences to add to his or her collection. For\nexample, because I am currently very interested\nin CMC, I quickly went to the article by\nSteeples et al on the topic, and am pleased to\nadd it to my CMC Insights and Experiences file.\nOne accepts a journal as a collection of articles,\nand the articles included here are quite\ninteresting. Nonetheless, one would like a book,\ndestined to live for years on library shelves, to\ngo one step beyond the collection level to bring\nsome sort of coherence in the articles for the\nreader who in the future seeks out the book\nbecause of its title. One hopes that those readers\nwill find the articles by Brown and by Gardner\nand his colleagues, if they want a reflective\nsummary of a wide set of experiences from the\nfield, from mainstream practice.\nBetty Collis, University of Twente, Netherlands\n83\nNOTE\nThe content of this book also forms Volume 22\n(Issues 1 and 2) of the journal Computers and\nEducation.\nHyperprogramming: Building Inter-\nactive Programs with HyperCard by\nG. Coulouris and H. Thimbleby,\nWoklngham, Addison-Wesley, 1992.\nISBN: 0-201-56886-1.\nThis book is aimed at those wishing to develop\nHyperCard applications and who already have\nsome programming experience (for example, in\na conventional third-generation language such\nas Pascal or BASIC). The authors suggest that\ntheir work is suitable for a number of target\nreader groups: novice programmers, expert\nprogrammers, students, advanced students, and\nHyperCard developers, and individual chapters\nare identified in the Preface as being of\nparticular relevance to each target reader group.\nA floppy disk containing HyperCard stacks\naccompanies the book, and complements it by\nproviding the HyperTalk code discussed within\nit. In this way, readers with a Macintosh and\nHyperCard are able to run the programs and,\nmore importantly, cut and paste both objects\nand code for use in their own programs (known\nas stacks in HyperCard).\nThere are a number of books about HyperCard,\nbut this one is quite novel in its approach. It\ngoes straight into HyperTalk (without lengthy\ndescription of the HyperCard interface) and\ncombines this with the pedagogic strategy of\nteaching by example. Throughout the book,\nexample code is used to introduce new\nHyperCard and HyperTalk topics to readers. A\nkey benefit of this topic-based approach is that\nit presents material in problem-oriented ways.\nReaders are thus able to discover hints and tips\nwhile learning about programming in Hyper-\nTalk. They are also presented with solutions to\nsome of the practical problems that might be\nencountered when they come to develop their\nown HyperCard stacks.\nThere are fifteen chapters, all oriented towards\npractical use. The first provides an immediate\nintroduction to programming in HyperTalk.\nThis is followed by two chapters on the basics of\nworking with HyperCard and HyperCard\nobjects. The fourth chapter then describes the\nbasic commands and control structures used in\nHyperTalk programming.\nChapters 5 to 11 adopt the topic-based\nReviews\napproach mentioned earlier, with each chapter\npresenting a new theme. They take the reader\nthrough a wide variety of practical\nprogramming issues while presenting new\nsyntax, structures, and tools. Programming\ntopics covered include graphics, timing,\nanimation, menus and searching.\nChapters 12 to 14 deal with the concept of hyper-\ntext. A number of issues are addressed: design,\nauthoring, reading (browsing), and so on.\nThe final chapter presents ideas and strategies\nfor undertaking a variety of projects in\nHyperCard; the aim is to provide some\nindication of the flexibility and scope of\nenvironments like it. A wide range of problems\nare discussed, including the development of\nbooks, games and a simple spreadsheet. In\naddition, techniques for protecting software\nfrom tampering and virus attack are outlined.\nThe two appendices are useful. The first provides\nlists of information about HyperTalk (properties,\ncommands, messages, constants, operators and\nfunctions). The second gives a set of comparisons\nbetween HyperTalk and Pascal, designed to allow\nreaders with existing Pascal experience to transfer\nrapidly between the two environments. This\nsecond appendix is very well done: the structures\nused in Pascal are presented with the HyperTalk\nnearest equivalent presented alongside.\nThis is an excellent book for anyone wishing to\nprogram HyperCard applications, particularly\nif they already have some programming\nexperience. It also has some potential for people\nwho wish to program in Toolbook on the PC,\nan environment similar to HyperCard.\nStephen Richards, University of Newcastle-upon-\nTyne\nSTELLA II version 3 (Simulation and\nModelling for System Dynamics),\nApple Macintosh version.\nWhen STELLA was introduced several years\nago, it provided the first opportunity for rela-\ntively unskilled workers to solve complex\nproblems which normally would have required\nskills in algebra and calculus. STELLA was\ndesigned to model the behaviour of dynamic\nsystems: that is, those in which the values of\nvarious components of a system (the system vari-\nables) vary in a complex manner with time. For\nexample, the variations in populations in an\necosystem, or drug levels after taking a medicine,\nvary in a time-dependent manner which can be\ncalculated from a physical model of the system.\nNormally, the study of dynamic systems\nrequires the use of calculus to set up and solve\ndifferential equations; these describe the inter-\nrelations between the quantities of interest\nSetting up a dynamic model involves two steps;\nfirstly, the identification of the inter-\nrelationships between the variables, and\nsecondly, the writing of differential equations\nwhich embody these relationships in an\nalgebraic manner. Finally, the solution to the\ndifferential equations reveals the time-\ndependent behaviour of the system variables.\nPrior to the introduction of STELLA, there\nwere a number of rather unfriendly programs\navailable which assisted users in setting up the\ndifferential equations, and solved them\nnumerically. Solving the equations has always\nbeen considered the 'hard' part of the problem.\nUnfortunately, if you do not have the skill or\ntraining to set up the model or equations, then\nthat is the hard part, and finding the solution is\nirrelevant. STELLA allowed the user to define\nthe problem simply by drawing boxes, which\nrepresented the time-dependent quantities in the\nsystem, and joining them by flows, which\nrepresented the inter-relationships between the\nquantities. The basic form of the system was\ndefined by the structure of the diagram, and\nonly very minor mathematics was required to\ndefine the flows. STELLA then transformed the\nmodel into a set of differential equations, and\nsolved them, thus showing how the model\nsystem changed with time.\nThe first version of STELLA was simple and in\nsome ways did not conform to the usual\nMacintosh interface; however, its idiosyncrasies\nwere easily learnt, and it was widely used. In\nthis form, it was capable of studying most of the\nproblems which its users posed; as long as a\nsimple box and flow model could be used to\nrepresent the problem, STELLA could provide\nan indication of the likely behaviour. Despite\nthis, there were a number of deficiencies: some\nfairly simple problems could not be broken\ndown into a box-and-flow form, and there were\nlimits to the presentation of the output, which\nwas either a copyable string of numbers or a\nPICTable graph. STELLA II began to address\nthese problems, and STELLA II 3.0 adds yet\nmore functionality.\nThe new modelling tools added to STELLA II\nallowed processes to be used which could only\npoorly be represented by boxes. These include\nthe conveyor and the oven. A conveyor is a\nstructure into which you can feed material at\none end and it will appear at the other end after\n84\nALTJ VOLUME 2 NUMBER 2\na delay. This is a much more realistic model of\nmany processes. Take, for example, a model we\nrecently used to study the flow of material\nthrough the gastrointestinal tract. Originally,\nwe used a sequence of boxes to represent the\nstomach, small intestine, and large intestine. The\nproblem with this model was that as soon as the\nstomach had emptied some material into the\nnext box (the small intestine), it could be further\nemptied into the large intestine. Of course, this is\nnot a physically realistic model since the small\nintestine is a long tube and, as we know, it takes\ntime for material to get to the far end. A con-\nveyor is a much more realistic structure to model\nthis behaviour. STELLA II also allowed one-\nway or two-way flows, and most usefully\nintroduced a non-negativity constraint. In the\nearlier versions, \u2022accidental negativity of a vari-\nable had to be checked for by the model builder,\nwhich was most inconvenient. These additions,\nin our opinion, covered most of the require-\nments of the average user.\nSTELLA II 3.0 adds to both the computational\nand presentation areas. Most of the changes\nhave to do with presentation of the results.\nThere are now two versions, STELLA II 3.0\nordinary, and STELLA II Authoring. This is in\nline with many programs which now provide a\nfull-featured version for developers, and a\nsimplified version for end-users of the models.\nWe were only able to test the authoring version,\nso cannot comment on the lesser facilities of the\nordinary version. However, the claimed features\nof the authoring version over the other version\ninclude the ability to create stand-alone models\nfmicroworlds' - ugh!) which can be locked so\nthat students can explore their operation but\nnot mess about with their structure. The\nauthoring version also has a mapping mode to\naid the construction of complex models, and an\noverview mode to display the broad structure of\nthe model. This level allows the importation of\ngraphics and Quicktime animation (which we\ndid not test). In this aspect, STELLA 3 (let us\ncall it that) becomes more of a presentation\npackage than a laboratory program, which will\nbe useful for people who need to use simulations\nas a teaching or negotiation tool. The ability to\ncreate stand-alone models is most useful for\nteaching purposes (students may not need the\nfull ability to create models).\nThere are rather fewer computational additions\nto STELLA 3. The built-in function list is\nslightly longer with 11 new items. Publish and\nsubscribe are supported, which may allow some\ninteresting connections to other calculation\npackages such as Excel or Mathematica. A\nvaluable feature is the ability to plot several\nruns of a model on the same graph to allow\ncomparison of changing a particular parameter.\nIn previous versions of STELLA, different\ngraphs had to be drawn and each plot locked\nbefore a parameter could be changed and the\nmodel re-run to produce the new graph. This\nmade comparison very hard and necessitated\nprinting each individual graph.\nOne of the most serious difficulties with\nSTELLA and STELLA 2 was that they were\navailable only for the Macintosh. 'Is there a PC\nversion?' was probably the most commonly\nasked question we encountered in our years of\nshowing the program to our colleagues.\nSTELLA 3 promises to provide model\ntransferability between Mac and PC; however a\nPC version was not available to us so we could\nnot try it.\nThe review version of STELLA 3 was provided\non a single disc and occupied 1.3 megabytes of\nspace. It loaded without difficulty onto a\nMacintosh 2VI and a 2CI, both running system\n7 with the usual heterogenous array of exten-\nsions, and claimed 1.5 megabytes of memory,\nalthough this could be reduced to 1 megabyte.\nWe did not perform any timing comparisons\nwith earlier versions, but the simple models we\ncreated ran smoothly and quickly, giving the\nsame numerical results as those from STELLA\n2. Models created in STELLA 2 opened without\ndifficulty in STELLA 3.\nDespite the major additions, it must be said that\nthe original version of STELLA has a simplicity\nwhich allows new users to grasp its functions\nrapidly, and will do virtually all that is asked of\nit. STELLA 3 is easy to learn if you have used\nearlier versions, but its many features can\nconfuse novices. It is a common situation with\nall forms of software; for example, all we\nusually need is old MacWrite and Multiplan,\nbut now we have Word and Excel, with massive\nfunctionality that is rarely accessed.\nWho will buy STELLA 3? We would expect\nthat people needing to present their models to\naudiences will find the additions invaluable, and\nparticularly teachers associated with computer-\nassisted learning in higher education. PC-based\ndepartments will benefit from the availability of\nthe package on their machines. Research users\nof modelling software will find new features in\nSTELLA 3, but most of the time they will not\nneed the glitz.\nClive and Neena Washington, University of\nNottingham\n85\nReviews\nNote\nSTELLA II version 3 is available from Cognitus\nSystems, 1 Park View, Harrogate, Yorks, HG1\n5LY (tel: 0423 562622). It is available only to\nbona-fide educational users; others must buy\nthe equivalent commercial package. Prices are:\nAuthoring \u00a3399, Standard \u00a3249; upgrades and\nLAN packs are also available.\nThe Open Learning Handbook\n- Promoting Quality In Designing and\nDelivering Flexible Learning by Phil\nRace, 2nd edition, London, Kogan\nPage, 1993. ISBN: 0-89397-392-0.\nThe transformation of higher education into a\nsystem adapted to servicing the needs of much\nlarger numbers has created a demand for flexibil-\nity. Distance and open-learning methods free\nstudents from constraints of time and place, and\ncan allow more individualized feedback than tra-\nditional teaching programmes. However, there is\nalso an emphasis on auditing for quality - effi-\nciency must be complemented by a concern for\nstandards and excellence. This book is intended\nboth for those already involved in the delivery of\nopen learning, and those moving into the field\nfor the first time. There are two main aims:\nfirstly, to provide guidelines on good practice for\nthose producing open-learning materials; sec-\nondly, to advise tutors and mentors on the\neffective support of open learning.\nThe book is a readable, attractive reference text,\npacked with very useful checklists. The author is\nclearly drawing widely on his own experience,\nand although not an open-learning package\nitself, the book provides a feel for the subject by\nincorporating many of the features of open\nlearning: each chapter begins with an abstract\nand a list of learning objectives, and some\ncontain self-assessment questions. Chapter 1\ndiscusses the meaning of the term open learning,\nsetting it in the context of the whole learning\nprocess. Chapters 2 to 7 cover the production of\nopen-learning materials, from the initial design\nstage through to marking student assignments.\nChapters 8 and 9 describe respectively the role\nof tutors and mentors. Chapter 10 focuses on\nthe integration of open-learning materials into\ntraditional courses. The book concludes with an\nannotated bibliography and an index. It should\nbe noted that the production, choice and use of\nnon-print media is not covered.\nI would single out Chapters 2 , 3 , 4 and 6, on the\ndesign and production of open learning\nmaterials, as being of particular merit. In\nChapter 2 ('Designing for open learning'), the\nauthor takes a realistic approach, opening with\na discussion about the relative merits of\nadopting or adapting existing materials, or\nstarting from scratch. A full description of the\ndifference between open-learning materials and\ntraditional textbooks is given. The importance\nof piloting materials is highlighted, alongside\nsuggestions for obtaining meaningful feedback\nfrom the piloting phase. A Quality Checklist is\nalso provided. Chapter 3 concentrates on the\nimportance of showing learners where they are\nheading. Syllabus content is now widely\nexpressed (GNVQ\/NVQ, for example) in terms\nof intended learning outcomes: competences,\nperformance criteria and range statements. The\nsection discussing these terms is therefore\nparticularly relevant. Chapter 4 focuses on the\npurpose and design of self-assessment questions,\nand the provision of poor, as well as good\nexamples, is most helpful. Chapter 6, on the\ndesign of tutor-marked assignments, benefits\nfrom covering not only expected topics, such as\nthe development of marking schemes, but goes\non to discuss how the delivery of constructive,\nsupportive feedback might be achieved\neffectively in open-learning situations.\nChapter 5 moves on to the importance of\nadopting informal user-friendly language when\nproducing open-learning materials, and the\nreadability of this book as a whole demonstrates\nthe efficacy of this approach. However,\nemphasizing readability above all else can have\ndisadvantages: using an example from this book\nitself, improving flow by deliberately omitting\nliterature references in the text could hinder\nthose readers wishing to research specific topics\nin more detail (despite the inclusion of an\nannotated bibliography). In the same vein, the\nlack of section numbers gives an informal\nappearance, but is an irritation when making\nreference to the book. There is also the danger\nthat 'friendly' language can so easily become\npatronizing chattiness; thankfully, the author is\ntoo experienced to fall into this trap very often.\nDespite my minor criticisms, I would have no\nhesitation in recommending this handbook as a\nuseful resource for those interested in the\ndevelopment of open learning materials, and\nthose involved in tutoring open learners.\nLorraine Warren, Hull\n86\n"}