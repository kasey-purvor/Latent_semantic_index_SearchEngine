{"doi":"10.1214\/10-STS327","coreId":"69424","oai":"oai:eprints.lancs.ac.uk:26838","identifiers":["oai:eprints.lancs.ac.uk:26838","10.1214\/10-STS327"],"title":"The random walk Metropolis : linking theory and practice through a case study.","authors":["Sherlock, Chris","Fearnhead, Paul","Roberts, Gareth"],"enrichments":{"references":[{"id":997668,"title":"An adaptive metropolis algorithm.","authors":[],"date":"2001","doi":null,"raw":"Haario, H., Saksman, E. and Tamminen, J. (2001). An adaptive metropolis algorithm. Bernoulli 7(2), 223\u2013242.","cites":null},{"id":996665,"title":"An exact Gibbs sampler for the Markov modulated Poisson processes.","authors":[],"date":"2006","doi":null,"raw":"Fearnhead, P. and Sherlock, C. (2006). An exact Gibbs sampler for the Markov modulated Poisson processes. J. R. Stat. Soc. Ser. B Stat. Methodol. 68(5), 767\u2013784.","cites":null},{"id":996435,"title":"An introduction to MCMC. In: Spatial Statistics and Computational Methods","authors":[],"date":"2003","doi":null,"raw":"Dellaportas, P. and Roberts, G. O. (2003). An introduction to MCMC. In: Spatial Statistics and Computational Methods (ed. J. Moller), number 173 in Lecture Notes in Statistics, Springer, Berlin, 1\u201341.","cites":null},{"id":995924,"title":"Analysis of photon count data from single-molecule \ufb02uorescence experiments.","authors":[],"date":"2003","doi":null,"raw":"Burzykowski, T., Szubiakowski, J. and Ryden, T. (2003). Analysis of photon count data from single-molecule \ufb02uorescence experiments. Chemical Physics 288, 291\u2013307.","cites":null},{"id":998053,"title":"Bayesian analysis of single-molecule experimental data.","authors":[],"date":"2005","doi":null,"raw":"Kou, S. C., Xie, X. S. and Liu, J. S. (2005). Bayesian analysis of single-molecule experimental data. Appl. Statist. 54, 1\u201328.","cites":null},{"id":996172,"title":"Bayesian methods for data analysis .","authors":[],"date":"2009","doi":null,"raw":null,"cites":null},{"id":16699119,"title":"Bayesian methods for data analysis.","authors":[],"date":"2009","doi":null,"raw":"Carlin, B. P. and Louis, T. A. (2009). Bayesian methods for data analysis. Texts in Statistical Science Series, CRC Press, Boca Raton, FL, 3rd edition.","cites":null},{"id":999953,"title":"Coupling and ergodicity of adaptive Markov chain Monte Carlo algorithms.","authors":[],"date":"2007","doi":null,"raw":"Roberts, G. O. and Rosenthal, J. S. (2007). Coupling and ergodicity of adaptive Markov chain Monte Carlo algorithms. J. Appl. Probab. 44(2), 458\u2013475.","cites":null},{"id":998351,"title":"Equations of state calculations by fast computing machine.","authors":[],"date":null,"doi":null,"raw":"Equations of state calculations by fast computing machine. J. Chem. Phys. 21, 1087\u20131091.","cites":null},{"id":999358,"title":"Geometric ergodicity and hybrid Markov chains.","authors":[],"date":"1997","doi":null,"raw":"Roberts, G. O. and Rosenthal, J. S. (1997). Geometric ergodicity and hybrid Markov chains. Electron. Comm. Probab. 2, no. 2, 13\u201325 (electronic).","cites":null},{"id":1000676,"title":"In discussion of \u2019Bayesian analysis of single-molecule experimental data\u2019.","authors":[],"date":"2005","doi":null,"raw":"Sherlock, C. (2005). In discussion of \u2019Bayesian analysis of single-molecule experimental data\u2019. Journal of the Royal Statistical Society, Series C 54, 500.","cites":null},{"id":999097,"title":"Linking theory and practice of MCMC. In: Highly structured stochastic systems ,","authors":[],"date":"2003","doi":null,"raw":null,"cites":null},{"id":16699152,"title":"Linking theory and practice of MCMC. In: Highly structured stochastic systems,","authors":[],"date":"2003","doi":null,"raw":"Roberts, G. O. (2003). Linking theory and practice of MCMC. In: Highly structured stochastic systems, volume 27 of Oxford Statist. Sci. Ser., Oxford Univ. Press, Oxford, 145\u2013178, with part A by Christian P. Robert and part B by Arnoldo Frigessi.","cites":null},{"id":997325,"title":"Markov Chain Monte Carlo in practice.","authors":[],"date":"1996","doi":null,"raw":"Gilks, W. R., Richardson, S. and Spiegelhalter, D. J. (1996). Markov Chain Monte Carlo in practice. Chapman and Hall, London, UK.","cites":null},{"id":996700,"title":"Markov chain Monte Carlo. Texts","authors":[],"date":"2006","doi":null,"raw":"Gamerman, D. and Lopes, H. F. (2006). Markov chain Monte Carlo. Texts in Statistical Science Series, Chapman & Hall\/CRC, Boca Raton, FL, 2nd edition, stochastic simulation for Bayesian inference.","cites":null},{"id":998598,"title":"Markov chains and stochastic stability . Communications and Control Engineering Series, Springer-Verlag London Ltd.,","authors":[],"date":"1993","doi":null,"raw":null,"cites":null},{"id":16699146,"title":"Markov chains and stochastic stability.","authors":[],"date":"1993","doi":null,"raw":"Meyn, S. P. and Tweedie, R. L. (1993). Markov chains and stochastic stability. Communications and Control Engineering Series, Springer-Verlag London Ltd., London.","cites":null},{"id":1000939,"title":"Methodology for inference on the Markov modulated Poisson process and theory for optimal scaling of the random walk Metropolis.","authors":[],"date":"2006","doi":null,"raw":"Sherlock, C. (2006). Methodology for inference on the Markov modulated Poisson process and theory for optimal scaling of the random walk Metropolis. Ph.D. thesis, Lancaster University, available from http:\/\/eprints.lancs.ac.uk\/850\/.","cites":null},{"id":16699526,"title":"Monte Carlo methods in statistical mechanics: foundations and new algorithms. In: Functional integration (Carg` ese,","authors":[],"date":"1997","doi":null,"raw":"Sokal, A. (1997). Monte Carlo methods in statistical mechanics: foundations and new algorithms. In: Functional integration (Carg` ese, 1996), volume 361 of NATO Adv. Sci. Inst. Ser. B Phys., Plenum, New York, 131\u2013192. A Convergence rates, eigenfunctions, and intuition To avoid technical details we present theory in a simpli\ufb01ed framework where the MCMC kernels have discrete spectra, and consider only distributions for which the L2 norm resulting from the inner product (14) exists. We \ufb01rst motivate (14). Proposition 1 Let P(x,dx\u2032) be a reversible kernel with stationary distribution \u03c0(\u00b7), eigenfunctions ei(\u00b7), and associated eigenvalues \u03b2i. All of the \u03b2i are real, and with the inner 43CRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism product de\ufb01ned in (14), < ei(\u00b7),ej(\u00b7) >= \u03b4ij. Proof: De\ufb01ne S(x,dx \u2032) := \u03c0(x) \u03c0(x\u2032) 1\/2 P(x,dx \u2032). Since P is reversible, \u03c0(x)P(x,dx \u2032) = \u03c0(x \u2032)P(x \u2032,dx). Divide both sides by (\u03c0(x)\u03c0(x\u2032))1\/2 to see that S(x,dx\u2032) = S(x\u2032,dx). Thus S is symmetric and consequently has real eigenvalues \u03b2i and associated eigenfunctions e\u2217 i(\u00b7) with dx e\u2217 i(x) e\u2217 j(x) = \u03b4ij. Now for any i, \u03b2i e \u2217 i(x \u2032) = dx e \u2217 i(x) S(x,dx \u2032) = dx e \u2217 i(x) \u03c0(x)1\/2 \u03c0(x\u2032)1\/2 dP(x,x \u2032). Thus ei := \u03c01\/2 e\u2217 i is an eigenfunction of P with eigenvalue \u03b2i. Further \u03b4ij = dx e \u2217 i(x) e \u2217 j(x) = dx ei(x) ej(x) \u03c0 =< ei,ej > . We next motivate the idea of geometric ergodicity and show that a geometric rate of convergence is given by the second largest eigenvalue, provided its value is strictly less than one. We employ the shorthand notation for measure \u03c1 and kernel P, \u03c1P := dx \u03c1(x)P(x,\u00b7). Proposition 2 Let P be a reversible kernel with stationary distribution \u03c0, eigenvalues \u03b2i with 1 = \u03b21 \u2265 \u03b22 \u2265 \u03b23,.... For initial density \u03c1, ||\u03c1P \u2212 \u03c0||2 \u2264 \u03b22 ||\u03c1 \u2212 \u03c0||2 . Proof: Write \u03c1(\u00b7) = \u221e i=1 ai ei(\u00b7) and note that, since e1 = \u03c0, a1 =< \u03c1,e1 >= 1. Thus ||\u03c1 \u2212 \u03c0||2 = \u221e ai ei = \u221e a i 1\/2 . 44CRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism But \u03c1 P = \u221e 1 ai \u03b2iei and so ||\u03c1P \u2212 \u03c0||2 = \u221e ai \u03b2i ei \u2264 \u03b22 \u221e ai ei = \u03b22 \u221e a i 1\/2","cites":null},{"id":1001237,"title":"Monte Carlo methods in statistical mechanics: foundations and new algorithms. In: Functional integration (Carge`se,","authors":[],"date":"1997","doi":null,"raw":null,"cites":null},{"id":995347,"title":"On the containment condition for adaptive Markov chain Monte Carlo algorithms.","authors":[],"date":"2009","doi":null,"raw":"Bai, Y., Roberts, G. O. and Rosenthal, J. S. (2009). On the containment condition for adaptive Markov chain Monte Carlo algorithms. Submitted Preprint.","cites":null},{"id":995718,"title":"Optimal acceptance rates for Metropolis algorithms: moving beyond 0.234. Stochastic Process.","authors":[],"date":"2008","doi":null,"raw":"B\u00b4 edard, M. (2008). Optimal acceptance rates for Metropolis algorithms: moving beyond 0.234. Stochastic Process. Appl. 118(12), 2198\u20132222.","cites":null},{"id":998822,"title":"Optimal scaling for partially updating MCMC algorithm.","authors":[],"date":"2006","doi":null,"raw":"Neal, P. and Roberts, G. (2006). Optimal scaling for partially updating MCMC algorithm. Ann. Appl. Probab. 16, 475\u2013515.","cites":null},{"id":999666,"title":"Optimal scaling for various Metropolis-Hastings algorithms.","authors":[],"date":"2001","doi":null,"raw":"Roberts, G. O. and Rosenthal, J. S. (2001). Optimal scaling for various Metropolis-Hastings algorithms. Statistical Science 16, 351\u2013367.","cites":null},{"id":1001159,"title":"Optimal scaling of the random walk Metropolis on elliptically symmetric unimodal targets.","authors":[],"date":"2009","doi":"10.3150\/08-BEJ176","raw":"Sherlock, C. and Roberts, G. O. (2009). Optimal scaling of the random walk Metropolis on elliptically symmetric unimodal targets. Bernoulli To appear.","cites":null},{"id":997799,"title":"Polynomial convergence rates of Markov chains.","authors":[],"date":"2002","doi":null,"raw":"Jarner, S. F. and Roberts, G. O. (2002). Polynomial convergence rates of Markov chains. Ann. Appl. Probab. 12(1), 224\u2013247.","cites":null},{"id":996960,"title":"Practical Markov chain Monte Carlo.","authors":[],"date":"1992","doi":null,"raw":"41CRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism Geyer, C. J. (1992). Practical Markov chain Monte Carlo. Statistical Science 7, 473\u2013483.","cites":null},{"id":1000412,"title":"The Markov modulated Poisson process and Markov Poisson cascade with applications to web tra\ufb03c modelling.","authors":[],"date":"2003","doi":null,"raw":"Scott, S. L. and Smyth, P. (2003). The Markov modulated Poisson process and Markov Poisson cascade with applications to web tra\ufb03c modelling. Bayesian Statistics 7, 1\u201310.","cites":null},{"id":1000269,"title":"Weak convergence and optimal scaling of random walk Metropolis algorithms.","authors":[],"date":"1997","doi":"10.1214\/aoap\/1034625254","raw":"Roberts, G. O., Gelman, A. and Gilks, W. R. (1997). Weak convergence and optimal scaling of random walk Metropolis algorithms. The Annals of Applied Probability 7, 110\u2013120.","cites":null},{"id":995577,"title":"Weak convergence of Metropolis algorithms for non-i.i.d. target distributions.","authors":[],"date":"2007","doi":null,"raw":"B\u00b4 edard, M. (2007). Weak convergence of Metropolis algorithms for non-i.i.d. target distributions. Ann. Appl. Probab. 17(4), 1222\u20131244.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-05","abstract":"The random walk Metropolis (RWM) is one of the most common Markov Chain Monte Carlo algorithms in practical use today. Its theoretical properties have been extensively explored for certain classes of target, and a number of results with important practical implications have been derived. This article draws together a selection of new and existing key results and concepts and describes their implications. The impact of each new idea on algorithm efficiency is demonstrated for the practical example of the Markov modulated Poisson process (MMPP). A reparameterisation of the MMPP which leads to a highly efficient RWM within Gibbs algorithm in certain circumstances is also developed","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/69424.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/26838\/1\/SherlockFearnheadRoberts.pdf","pdfHashValue":"5990edf89ccac5e1ad23e1ba639134f41b9bb097","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:26838<\/identifier><datestamp>\n      2018-01-24T02:49:14Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        The random walk Metropolis : linking theory and practice through a case study.<\/dc:title><dc:creator>\n        Sherlock, Chris<\/dc:creator><dc:creator>\n        Fearnhead, Paul<\/dc:creator><dc:creator>\n        Roberts, Gareth<\/dc:creator><dc:subject>\n        QA Mathematics<\/dc:subject><dc:description>\n        The random walk Metropolis (RWM) is one of the most common Markov Chain Monte Carlo algorithms in practical use today. Its theoretical properties have been extensively explored for certain classes of target, and a number of results with important practical implications have been derived. This article draws together a selection of new and existing key results and concepts and describes their implications. The impact of each new idea on algorithm efficiency is demonstrated for the practical example of the Markov modulated Poisson process (MMPP). A reparameterisation of the MMPP which leads to a highly efficient RWM within Gibbs algorithm in certain circumstances is also developed.<\/dc:description><dc:date>\n        2010-05<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        NonPeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/26838\/1\/SherlockFearnheadRoberts.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1214\/10-STS327<\/dc:relation><dc:identifier>\n        Sherlock, Chris and Fearnhead, Paul and Roberts, Gareth (2010) The random walk Metropolis : linking theory and practice through a case study. Statistical Science, 25 (2). pp. 172-190. ISSN 0883-4237<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/26838\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1214\/10-STS327","http:\/\/eprints.lancs.ac.uk\/26838\/"],"year":2010,"topics":["QA Mathematics"],"subject":["Journal Article","NonPeerReviewed"],"fullText":"CRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nThe random walk Metropolis: linking theory and\npractice through a case study.\nChris Sherlock1,3, Paul Fearnhead1, and Gareth O. Roberts2\n1. Department of Mathematics and Statistics, Lancaster University, Lancaster, LA1 4YF,\nUK\n2. Department of Statistics, University of Warwick, Coventry, CV4 7AL, UK.\n3. Correspondence should be addressed to Chris Sherlock.\n(e-mail: c.sherlock@lancs.ac.uk).\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nSummary: The random walk Metropolis (RWM) is one of the most common Markov Chain\nMonte Carlo algorithms in practical use today. Its theoretical properties have been exten-\nsively explored for certain classes of target, and a number of results with important practical\nimplications have been derived. This article draws together a selection of new and existing\nkey results and concepts and describes their implications. The impact of each new idea on\nalgorithm efficiency is demonstrated for the practical example of the Markov modulated Pois-\nson process (MMPP). A reparameterisation of the MMPP which leads to a highly efficient\nRWM within Gibbs algorithm in certain circumstances is also developed.\nKeywords: random walk Metropolis, Metropolis-Hastings, MCMC, adaptive MCMC, MMPP\n1\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\n1 Introduction\nMarkov chain Monte Carlo (MCMC) algorithms provide a framework for sampling from\na target random variable with a potentially complicated probability distribution \u03c0(\u00b7) by\ngenerating a Markov chain X(1),X(2), . . . with stationary distribution \u03c0(\u00b7). The single most\nwidely used sub-class of MCMC algorithms is based around the random walk Metropolis\n(RWM).\nTheoretical properties of RWM algorithms for certain special classes of target have been in-\nvestigated extensively. Reviews of RWM theory have, for example, dealt with optimal scaling\nand posterior shape (Roberts and Rosenthal, 2001), and convergence (Roberts, 2003). This\narticle does not set out to be a comprehensive review of all theoretical results pertinent to\nthe RWM. Instead the article reviews and develops specific aspects of the theory of RWM\nefficiency in order to tackle an important and difficult problem: inference for the Markov\nmodulated Poisson process (MMPP). It includes sections on RWM within Gibbs, hybrid\nalgorithms, and adaptive MCMC, as well as optimal scaling, optimal shaping, and conver-\ngence. A strong emphasis is placed on developing an intuitive understanding of the processes\nbehind the theoretical results, and then on using these ideas to improve the implementation.\nAll of the RWM algorithms described in this article are tested against data sets arising from\nMMPPs. Realised changes in efficiency are then compared with theoretical predictions.\nObserved event times of an MMPP arise from a Poisson process whose intensity varies\nwith the state of an unobserved continuous time Markov chain. The MMPP has been\nused to model a wide variety of clustered point processes, for example requests for web\npages from users of the World Wide Web (Scott and Smyth, 2003), arrivals of photons from\nsingle molecule fluorescence experiments (Burzykowski et al., 2003; Kou et al., 2005), and\noccurences of a rare DNA motif along a genome (Fearnhead and Sherlock, 2006).\nIn common with mixture models and other hidden Markov models, inference for the MMPP\n2\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nis greatly complicated by a lack of knowledge of the hidden data. The likelihood function\noften possesses many minor modes since the data might be approximately described by a\nhidden process with fewer states. For this same reason the likelihood often does not appoach\nzero as certain combinations of parameters approach zero and\/or infinity and so improper\npriors lead to improper posteriors (e.g. Sherlock, 2005). Further, as with many hidden\ndata models the likelihood is invariant under permutation of the states, and this \u201clabelling\u201d\nproblem leads to posteriors with several equal modes.\nThis article focusses on generic concepts and techniques for improving the efficiency of RWM\nalgorithms whatever the statistical model. The MMPP provides a non-trivial testing ground\nfor them. All of the RWM algorithms described in this article are tested against two sim-\nulated MMPP data sets with very different characteristics. This allows us to demonstrate\nthe influence on performance of posterior attributes such as shape and orientation near the\nmode and lightness or heaviness of tails.\nSection 2 introduces RWM algorithms and then describes theoretical and practical measures\nof algorithm efficiency in terms of both convergence and mixing. Next the two main theo-\nretical approaches to determining efficiency are decribed, and the section ends with a brief\noverview of the MMPP and a description of the data analysed in this article. Section 3 in-\ntroduces a series of concepts which allow potential improvements in the efficiency of a RWM\nalgorithm. The intuition behind each concept is described, followed by theoretical justifi-\ncation and then details of one or more RWM algorithms motivated by the theory. Actual\nresults are described and compared with theoretical predictions in Section 4, and the article\nis summarised in Section 5.\n3\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\n2 Background\nIn this section we introduce the background material on which the remainder of this article\ndraws. We describe the random walk Metropolis algorithm and a variation, the random\nwalk Metropolis-within-Gibbs. Both practical issues and theoretical approaches to algorithm\nefficiency are then discussed. We conclude with an introduction to the Markov modulated\nPoisson process and to the data sets used later in the article.\n2.1 Random walk Metropolis algorithms\nThe random walk Metropolis (RWM) updating scheme was first applied in Metropolis\net al. (1953) and proceeds as follows. Given a current value of the d-dimensional Markov\nchain, X, a new value X\u2217 is obtained by proposing a jump Y\u2217 := X\u2217 \u2212 X from the pre-\nspecified Lebesgue density\nr\u02dc (y\u2217;\u03bb) :=\n1\n\u03bbd\nr\n(\ny\u2217\n\u03bb\n)\n, (1)\nwith r(y) = r(\u2212y) for all y. Here \u03bb > 0 governs the overall size of the proposed jump and\n(see Section 3.1) plays a crucial role in determining the efficiency of any algorithm. The\nproposal is then accepted or rejected according to acceptance probability\n\u03b1(x,y\u2217) = min\n(\n1,\n\u03c0(x+ y\u2217)\n\u03c0(x)\n)\n. (2)\nIf the proposed value is accepted it becomes the next current value (X\u2032 \u2190 X+Y\u2217), otherwise\nthe current value is left unchanged (X\u2032 \u2190 X).\nThe acceptance probability (2) is chosen so that the chain is reversible at equilibrium with\nstationary distribution \u03c0(\u00b7). In this article the transition kernel, that is the combined process\nof proposal and acceptance\/rejection that leads from one element of the chain (x) to the\nnext, is denoted P (x, \u00b7).\n4\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nAn intuitive interpretation of the above formula is that \u201cuphill\u201d proposals (proposals which\ntake the chain closer to a local mode) are always accepted, whereas \u201cdownhill\u201d proposals\nare accepted with probability exactly equal to the relative \u201cheights\u201d of the posterior at the\nproposed and current values. It is precisely this rejection of some \u201cdownhill\u201d proposals which\nacts to keep the Markov chain in the main posterior mass most of the time.\nWe now describe a generalisation of the RWM which acts on a target whose components\nhave been split into k sub-blocks. In general we write X = (X1, . . . ,Xk), where Xi is the\nith sub-block of components of the current element of the chain. Starting from value X, a\nsingle iteration of this algorithm cycles through all of the sub-blocks updating each in turn.\nIt will therefore be convenient to define the shorthand\nx\n(B)\ni := x\n\u2032\n1, . . . ,x\n\u2032\ni\u22121,xi,xi+1, . . . ,xk\nx\n(B)\u2217\ni := x\n\u2032\n1, . . . ,x\n\u2032\ni\u22121,xi + y\n\u2217\ni ,xi+1, . . . ,xk ,\nwhere x\u2032j is the updated value of the j\nth sub-block. For the ith sub-block a jump Y \u2217i is proposed\nfrom symmetric density r\u02dci(y;\u03bbi) and accepted or rejected according to acceptance probability\n\u03c0\n(\nx\n(B)\u2217\ni\n)\n\/\u03c0\n(\nx\n(B)\ni\n)\n. Since this algorithm is in fact a generalisation of both the RWM and of\nthe Gibbs sampler (for a description of the Gibbs sampler see for example Gamerman and\nLopes, 2006) we follow for example Neal and Roberts (2006) and call this the random walk\nMetropolis-within-Gibbs or RWM-within-Gibbs. The most commonly used random walk\nMetropolis within Gibbs algorithm, and also the simplest, is that employed in this article:\nhere all blocks have dimension 1 so that each component of the parameter vector is updated\nin turn.\nEven though each stage of the RWM-within-Gibbs is reversible, the algorithm as a whole is\nnot. Reversible variations include the random scan RWM-within-Gibbs, wherein at each\niteration a single component is chosen at random and updated conditional on all the other\ncomponents.\n5\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nConvergence of the Markov chain to its stationary distribution can be guaranteed for all of\nthe above algorithms under quite general circumstances (e.g. Gilks et al., 1996).\n2.2 Algorithm efficiency\nAdjacent elements of an MCMC Markov chain are correlated and the sequence of marginal\ndistributions converges to \u03c0(\u00b7). Two main (and related) issues arise with regard to the\nefficiency of MCMC algorithms: convergence and mixing.\n2.2.1 Convergence\nIn this article we will be concerned with practical determination of a point at which a chain\nhas converged. The method we employ is simple heuristic examination of the trace plots for\nthe different components of the chain. Note that since the state space is multi-dimensional it\nis not sufficient to simply examine a single component. Alternative techniques are discussed\nin Chapter 7 of Gilks et al. (1996).\nTheoretical criteria for ensuring convergence (ergodicity) of MCMC Markov chains are ex-\namined in detail in Chapters 3 and 4 of Gilks et al. (1996) and references therein, and will\nnot be discussed here. We do however wish to highlight the concepts of geometric and poly-\nnomial ergodicity. A Markov chain is geometrically ergodic with stationary distribution\n\u03c0(\u00b7) if\n||P n(x, \u00b7)\u2212 \u03c0(\u00b7)||1 \u2264 M(x) rn (3)\nfor some positive r < 1 andM(\u00b7). Here ||F (\u00b7)\u2212G(\u00b7)||1 denotes the total variational distance\nbetween measures F (\u00b7) and G(\u00b7) (see for example Meyn and Tweedie, 1993). Efficiency of a\ngeometrically ergodic algorithm is measured by the geometric rate of convergence, r, which\nover a large number of iterations is well approximated by the second largest eigenvalue of\n6\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nthe transition kernel (the largest eigenvalue being 1, and corresponding to the stationary\ndistribution \u03c0(\u00b7)). Geometric ergodicity is usually a purely qualitative property since in\ngeneral the constants M(x) and r are not known. Crucially for practical MCMC however\nany geometrically ergodic reversible Markov chain satisfies a central limit theorem for all\nfunctions with finite second moment with respect to \u03c0(\u00b7). Thus there is a \u03c32f <\u221e such that\nn1\/2\n(\nf\u02c6n \u2212 E\u03c0 [f(X)]\n)\n\u21d2 N(0, \u03c32f) (4)\nwhere \u21d2 denotes convergence in distribution. The central limit theorem (4) guarantees not\nonly convergence of the Monte Carlo estimate (5) but also supplies its standard error, which\ndecreases as n\u22121\/2.\nWhen the second largest eigenvalue is also 1 a Markov chain is termed polynomially er-\ngodic if\n||P n(x, \u00b7)\u2212 \u03c0(\u00b7)||1 \u2264M(x) n\u2212r\nClearly polynomial ergodicity is a weaker condition than geometric ergodicity. Central limit\ntheorems for polynomially ergodic MCMC are much more delicate; see Jarner and Roberts\n(2002) for details.\nIn this article a chain is referred to as having \u201creached stationarity\u201d or \u201cconverged\u201d when\nthe distribution from which an element is sampled is as close to the stationary distribution\nas to make no practical difference to any Monte-Carlo estimates.\nAn estimate of the expectation of a given function f(X), which is more accurate than a\nnaive Monte Carlo average over all the elements of the chain, is likely to be obtained by\ndiscarding the portion of the chain X0, . . . ,Xm up until the point at which it was deemed to\nhave reached stationarity; iterations 1, . . .m are commonly termed \u201cburn in\u201d. Using only the\nremaining elements Xm+1, . . . ,Xm+n (with m+n = N) our Monte Carlo estimator becomes\nf\u02c6n :=\n1\nn\nm+n\u2211\nm+1\nf(Xi) (5)\n7\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nConvergence and burn in are not discussed any further here, and for the rest of this section\nthe chain is assumed to have started at stationarity and continued for n further iterations.\n2.2.2 Practical measures of mixing efficiency\nFor a stationary chain, X0 is sampled from \u03c0(\u00b7), and so for all k > 0 and i \u2265 0\nCov [f(Xk), f(Xk+i)] = Cov [f(X0), f(Xi)]\nThis is the autocorrelation at lag i. Therefore at stationarity, from the definition in (4),\n\u03c32f := lim\nn\u2192\u221e\nnVar\n[\nf\u02c6n\n]\n= Var [f(X0)] + 2\n\u221e\u2211\ni=1\nCov [f(X0), f(Xi)]\nprovided the sum exists (e.g. Geyer, 1992). If elements of the stationary chain were inde-\npendent then \u03c32f would simply be Var [f(X0)] and so a measure of the inefficiency of the\nMonte-Carlo estimate f\u02c6n relative to the perfect i.i.d. sample is\n\u03c32f\nVar [f(X0)]\n= 1 + 2\n\u221e\u2211\ni=1\nCorr [f(X0), f(Xi)] (6)\nThis is the integrated autocorrelation time (ACT) and represents the effective number of\ndependent samples that is equivalent to a single independent sample. Alternatively n\u2217 =\nn\/ACT may be regarded as the effective equivalent sample size if the elements of the chain\nhad been independent.\nTo estimate the ACT in practice one might examine the chain from the point at which it is\ndeemed to have converged and estimate the lag-i autocorrelation Corr [f(X0), f(Xi)] by\n\u03b3\u02c6i =\n1\nn\u2212 i\nn\u2212i\u2211\nj=1\n(\nf(Xj)\u2212 f\u02c6n\n)(\nf(Xj+i)\u2212 f\u02c6n\n)\n(7)\nNaively, substituting these into (6) gives an estimate of the ACT. However contributions\nfrom all terms with very low theoretical autocorrelation in a real run are effectively random\nnoise, and the sum of such terms can dominate the deterministic effect in which we are\n8\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\ninterested (e.g. Geyer, 1992). For this article we employ the simple solution suggested in\nCarlin and Louis (2009): the sum (6) is truncated from the first lag, l, for which the estimated\nautocorrelation drops below 0.05 . This gives the (slightly biassed) estimator\nACTest := 1 + 2\nl\u22121\u2211\ni=1\n\u03b3\u02c6i. (8)\nGiven the potential for relatively large variance in estimates of integrated ACT howsoever\nthey might be obtained (e.g. Sokal, 1997), this simple estimator should be adequate for\ncomparing the relative efficiencies of the different algorithms in this article. Geyer (1992)\nprovides a number of more complex window estimators and provides references for regularity\nconditions under which they are consistent.\nA given run will have a different ACT associated with each parameter. An alternative\nefficiency measure, which is aggregated over all parameters is provided by the Mean Square\nEuclidean Jump Distance (MSEJD)\nS2Euc :=\n1\nn\u2212 1\nn\u22121\u2211\ni=1\n\u2223\u2223\u2223\u2223x(i+1) \u2212 x(i)\u2223\u2223\u2223\u22232\n2\n.\nThe expectation of this quantity at stationarity is referred to as the Expected Square Eu-\nclidean Jump Distance (ESEJD). Consider a single component of the target with variance\n\u03c32i := Var [Xi] = Var [X\n\u2032\ni], and note that E [X\n\u2032\ni \u2212Xi] = 0, so\nE\n[\n(X \u2032i \u2212Xi)2\n]\n= Var [X \u2032i \u2212Xi] = 2\u03c32i (1\u2212 Corr [Xi, X \u2032i])\nThus when the chain is stationary and the posterior variance is finite, maximising the ESEJD\nis equivalent to minimising a weighted sum of the lag-1 autocorrelations.\nIf the target has finite second moments and is roughly elliptical in shape with (known)\ncovariance matrix \u03a3 then an alternative measure of efficiency is the Mean Square Jump\nDistance (MSJD)\nS2d :=\n1\nn\u2212 1\nn\u22121\u2211\ni=1\n(\nx(i+1) \u2212 x(i))t\u03a3\u22121 (x(i+1) \u2212 x(i)),\n9\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\n0 200 400 600 800 1000\n\u2212\n3\n\u2212\n1\n1\n2\n3\n(a) Chain: lambda=0.24\nindex in chain\n0 10 20 30 40\n0.\n0\n0.\n4\n0.\n8\nLag\nAC\nF\n(b) ACF: lambda=0.24\n0 200 400 600 800 1000\n\u2212\n3\n\u2212\n1\n1\n2\n3\n(c) Chain: lambda=2.4\nindex in chain\n0 10 20 30 40\n0.\n0\n0.\n4\n0.\n8\nLag\nAC\nF\n(d) ACF: lambda=2.4\n0 200 400 600 800 1000\n\u2212\n3\n\u2212\n1\n1\n2\n3\n(e) Chain: lambda=24\nindex in chain\n0 10 20 30 40\n0.\n0\n0.\n4\n0.\n8\nLag\nAC\nF\n(f) ACF: lambda=24\nx\nx\nx\nFigure 1: Trace plots ((a), (c), and (e)) and corresponding autocorrelation plots ((b), (d), and\n(f)), for exploration of a standard Gaussian initialised from x = 0 and using the random walk\nMetropolis algorithm with Gaussian proposal. Proposal scale parameters for the three scenarios\nare respectively (a) & (b) 0.24, (c) & (d) 2.4, and (e) & (f) 24.\nwhich is proportional to the unweighted sum of the lag-1 autocorrelations over the principal\ncomponents of the ellipse. The theoretical expectation of the MSJD at stationarity is known\nas the expected squared jump distance (ESJD).\nFigure 1 shows trace plots for three different Markov chains. Estimates of the autocorrelation\nfrom lag-0 to lag-40 for each Markov chain appear alongside the corresponding traceplot.\nThe simple window estimator for integrated ACT provides estimates of respectively 39.7,\n5.5, and 35.3. The MSEJDs are respectively 0.027, 0.349, and 0.063, and are equal to the\nMSJDs since the stationary distribution has a variance of 1.\n10\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\n2.2.3 Assessing accuracy\nAn MCMC algorithm might efficiently explore an unimportant part of the parameter space\nand never find the main posterior mass. ACT\u2019s will be low therefore, but the resulting\nposterior estimate will be wildly innaccurate. In most practical examples it is not possible\nto determine the accuracy of the posterior estimate, though consistency between several\nindependent runs or between different portions of the same run can be tested.\nFor the purposes of this article it was important to have a relatively accurate estimate of\nthe posterior, not determined by a RWM algorithm. Fearnhead and Sherlock (2006) detail\na Gibbs sampler for the MMPP; this Gibbs sampler was run for 100 000 iterations on each\nof the data sets analysed in this article. A \u201cburn-in\u201d of 1000 iterations was allowed for, and\na posterior estimate from the last 99 000 iterations was used as a reference for comparison\nwith posterior estimates from RWM runs of 10 000 iterations (after burn in).\n2.2.4 Theoretical approaches for algorithm efficiency\nTo date, theoretical results on the efficiency of RWM algorithms have been obtained through\ntwo very different approaches. We wish to quote, explain, and apply theory from both and\nso we give a heuristic description of each and define associated notation. Both approaches\nlink some measure of efficiency to the expected acceptance rate - the expected proportion of\nproposals accepted at stationarity.\nThe first approach was pioneered in Roberts et al. (1997) for targets with independent\nidentically distributed components and then generalised in Roberts and Rosenthal (2001) to\ntargets of the form\n\u03c0(x) =\nd\u220f\n1\nCi f(Cixi).\nThe inverse scale parameters, Ci, are assumed to be drawn from some distribution with a\n11\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\ngiven (finite) mean and variance. A single component of the d dimensional chain (without\nloss of generality the first) is then examined; at iteration i of the algorithm it is denoted\nX\n(d)\n1,i . A scaleless, speeded up, continuous time process which mimics the first component of\nthe chain is defined as\nW\n(d)\nt := C1X\n(d)\n1,[td],\nwhere [u] denotes the nearest integer less than or equal to u. Finally, proposed jumps are\nassumed to be Gaussian\nY(d) \u223c N (0, \u03bb2dI) .\nSubject to conditions on the first two deriviatives of f(\u00b7), Roberts and Rosenthal (2001)\nshow that if E [Ci] = 1 and E [C\n2\ni ] = b, and provided \u03bbd = \u00b5\/d\n1\/2 for some fixed \u00b5 (the\nscale parameter but \u201crescaled\u201d according to dimension) then as d\u2192 \u221e, W (d)t approaches\na Langevin diffusion process with speed\nh(\u00b5) =\nC21\u00b5\n2\nb\n\u03b1d where \u03b1d := 2\u03a6\n(\n\u22121\n2\n\u00b5I1\/2\n)\n. (9)\nHere \u03a6(x) is the cumulative distribution function of a standard Gaussian, I := E\n[\n((log f)\u2032)2\n]\nis a measure of the roughness of the target, and \u03b1d corresponds to the acceptance rate.\nBe\u00b4dard (2007) proves a similar result for a triangular sequence of inverse scale parameters\nci,d, which are assumed to be known. A necessary and sufficient condition equivalent to (11)\nbelow is attached to this result. In effect this requires the scale over which the smallest\ncomponent varies to be \u201cnot too much smaller\u201d than the scales of the other components.\nThe second technique (e.g. Sherlock and Roberts, 2009) uses expected square jump distance\n(ESJD) as a measure of efficiency. Exact analytical forms for ESJD (denoted S2d) and\nexpected acceptance rate are derived for any unimodal elliptically symmetric target and any\nproposal density. Many standard sequences of d-dimensional targets (d = 1, 2, . . . ), such as\nthe Gaussian, satisfy the condition that as d\u2192\u221e the probability mass becomes concentrated\nin a spherical shell which itself becomes infinitesimally thin relative to its radius. Thus the\n12\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nrandom walk on a rescaling of the target is, in the limit, effectively confined to the surface of\nthis shell. Sherlock and Roberts (2009) show that if the sequence of targets satisfies such a\n\u201cshell\u201d condition, and a slightly stronger condition is satisfied by the sequence of proposals\nthen as d\u2192\u221e\nd\nk\n(d)\nx\n2S\n2\nd(\u00b5)\u2192 \u00b52 \u03b1d with \u03b1d(\u00b5) := 2\u03a6\n(\n\u22121\n2\n\u00b5\n)\n. (10)\nHere \u03b1d is the limiting expected acceptance rate, \u00b5 := d\n1\/2\u03bbdk\n(d)\ny \/k\n(d)\nx , and k\n(d)\nx and k\n(d)\ny are\nthe rescalings appropriate for the target and proposal sequences so that the spherical shells\nto which the mass converges both have radius 1. For target and proposal distributions with\nindependent components, such as are used in the diffusion results, k\n(d)\nx = k\n(d)\ny = d1\/2, and\nhence (consistently) \u00b5 = d1\/2\u03bbd.\nA further condition is required on the triangular sequence of inverse scale parameters of the\naxes of the elliptical target\nmaxi c\n2\ni,d\u2211d\ni=1 c\n2\ni,d\n\u2192 0 as d\u2192\u221e (11)\nTheoretical results from the two techniques are remarkably similar and as will be seen, lead\nto identical strategies for optimising algorithm efficiency. It is worth noting however that\nresults from the first approach apply only to targets with independent components and\nresults from the second only to targets which are unimodal and elliptically symmetric. That\nthey lead to identical strategies indicates a certain potential robustness of these strategies\nto the shape of the target. This potential, as we shall see, is born out in practice.\n2.3 The Markov Modulated Poisson Process\nLet Xt be a continuous time Markov chain on discrete state space {1, . . . , d} and let \u03c8 :=\n[\u03c81, . . . , \u03c8d] be a d-dimensional vector of (non-negative) intensities. The linked but stochas-\ntically independent Poisson process Yt whose intensity is \u03c8Xt is a Markov modulated Poisson\n13\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\n0 2 4 6 8 10\n0 2 4 6 8 10\n2\n2\n1\n1\nst\na\nte\nst\na\nte\ntime\ntime\nFigure 2: Two 2-state continuous time Markov chains simulated from generator Q with q12 =\nq21 = 1; the rug plots show events from an MMPP simulated from these chains, with intensity\nvectors \u03c8 = (10, 30) (upper graph) and \u03c8 = (10, 17) (lower graph).\nprocess - it is a Poisson process whose intensity is modulated by a continuous time Markov\nchain.\nThe idea is best illustrated through two examples, which also serve to introduce the notation\nand data sets that will be used throughout this article. Consider a two-dimensional Markov\nchain Xt with generator Q with q12 = q21 = 1. Figure 2 shows realisations from two such\nchains over a period of 10 seconds. Now consider a Poisson process Yt which has intensity 10\nwhen Xt is in state 1 and intensity 30 when Xt is in state 2. This is an MMPP with event\nintensity vector \u03c8 = [10, 30]t. A realisation (obtained via the realisation of Xt) is shown as\na rug plot underneath the chain in the upper graph. The lower graph shows a realisation\nfrom an MMPP with event intensities [10, 17]t.\nIt can be shown (e.g. Fearnhead and Sherlock, 2006) that the likelihood for data from an\nMMPP which starts from a distribution \u03bd over its states is\nL(Q,\u03a8, t) = \u03bd \u2032e(Q\u2212\u03a8)t1\u03a8 . . . e(Q\u2212\u03a8)tn\u03a8e(Q\u2212\u03a8)tn+11. (12)\nHere \u03a8 := diag(\u03c8), 1 is a vector of 1\u2019s, n is the number of observed events, t1 is the time\n14\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nfrom the start of the observation window until the first event, tn+1 is the time from the last\nevent until the end of observation window, and ti (2 \u2264 i \u2264 n) is the time between the i\u2212 1th\nand ith events. In the absence of further information, the initial distribution \u03bd is often taken\nto be the stationary distribution of the underlying Markov chain.\nThe likelihood of an MMPP is invariant to a relabelling of the states. Hence if the prior is\nsimilarly invariant then so too is the posterior: if the posterior for a two dimensional MMPP\nhas a mode at (\u03c81, \u03c82, q12, q21) then it has an identical mode at (\u03c82, \u03c81, q21, q12). In this\narticle our overriding interest is in the efficiency of the MCMC algorithms rather than the\nexact meaning of the parameters and so we choose the simplest solution to this identifiablity\nproblem: the state with the lower Poisson intensity \u03c8 is always referred to as State 1.\n2.3.1 MMPP data in this article\nThe two data sets of event times used in this article arose from two independent MMPP\u2019s\nsimulated over an observation window of 100 seconds. Both underlying Markov chains have\nq12 = q21 = 1; data set D1 has event intensity vector \u03c8 = [10, 30] whereas data set D2 has\n\u03c8 = [10, 17].\nAs might be expected the overall intensity of events in D2 is lower than in D1. Moreover\nbecause the difference in intensity between the states is so much larger in D1 than in D2\nit is also easier with D1 than D2 to distinguish the state of the underlying Markov chain,\nand thus the values of the Markov and Poisson parameters. Further, in the limit of the\nunderlying chain being known precisely, for example as \u03c82 \u2192\u221e with \u03c81 finite, and provided\nthe priors are independent, the posteriors for the Poisson intensity parameters \u03c81 and \u03c82 are\ncompletely independent of each other and of the Markov parameters q12 and q21. Dependence\nbetween the Markov parameters is also small, being O(1\/T ) (e.g. Fearnhead and Sherlock,\n2006).\n15\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nIn Section 4, differences between D1 and D2 will be related directly to observed differences\nin efficiency of the various RWM algorithms between the two data sets.\n3 Implementations of the RWM: theory and practice\nThis section describes several theoretical results for the RWM or for MCMC in general.\nIntuitive explanation of the principle behind each result is emphasised and the manner in\nwhich it informs the RWM implementation is made clear. Each algorithm was run three\ntimes on each of the two data sets.\n3.1 Optimal scaling of the RWM\nIntuition: Consider the behaviour of the RWM as a function of the overall scale parameter\nof the proposed jump, \u03bb, in (1). If most proposed jumps are small compared with some\nmeasure of the scale of variability of the target distribution then, although these jumps will\noften be accepted, the chain will move slowly and exploration of the target distribution\nwill be relatively inefficient. If the jumps proposed are relatively large compared with the\ntarget distribution\u2019s scale, then many will not be accepted, the chain will rarely move and\nwill again explore the target distribution inefficiently. This suggests that given a particular\ntarget and form for the jump proposal distribution, there may exist a finite scale parameter\nfor the proposal with which the algorithm will explore the target as efficiently as possible.\nThese ideas are clearly demonstrated in Figure 1 which shows traceplots for a one dimen-\nsional Gaussian target explored using a Gaussian proposal with scale parameter an order of\nmagnitude smaller (a) and larger (c) than is optimal, and (b) with a close to optimal scale\nparameter.\n16\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nTheory: Equation (9) gives algorithm efficiency for a target with independent and identical\n(up to a scaling) components as a function of the \u201crescaled\u201d scale parameter \u00b5 = d1\/2\u03bbd\nof a Gaussian proposal. Equation (10) gives algorithm efficiency for a unimodal elliptically\nsymmetric target explored by a spherically symmetric proposal with \u00b5 = d1\/2\u03bbdk\n(d)\ny \/k\n(d)\nx . Ef-\nficiencies are therefore optimal at \u00b5 \u2248 2.38\/I1\/2 and \u00b5 \u2248 2.38 respectively. These correspond\nto actual scale parameters of respectively\n\u03bbd =\n2.38\nI1\/2d1\/2\nand \u03bbd =\n2.38 k\n(d)\nx\nd1\/2k\n(d)\ny\n.\nThe equivalence between these two expressions for Gaussian data explored with a Gaussian\ntarget is clear from Section 2.2.4. However the equations offer little direct help in choosing\na scale parameter for a target is neither elliptical, nor possesses components which are i.i.d.\nup to a scale parameter. Substitution of each expression into the corresponding acceptance\nrate equation, however, leads to the same optimal acceptance rate, \u03b1\u02c6 \u2248 0.234. This justifies\nthe relatively well known adage that for random walk algorithms with a large number of\nparameters, the scale parameter of the proposal should be chosen so that the acceptance rate\nis approximately 0.234. On a graph of asymptotic efficiency against acceptance rate (e.g.\nRoberts and Rosenthal, 2001), the curvature near the mode is slight, especially to its right,\nso that an acceptance rate of anywhere between 0.2 and 0.3 should lead to an algorithm of\nclose to optimally efficiency.\nIn practice updates are performed on a finite number of parameters; for example a two di-\nmensional MMPP has four parameters (\u03c81, \u03c82, q12, q21). A block update involves all of these,\nwhilst each update of a simple Metropolis within Gibbs step involves just one parameter. In\nfinite dimensions the optimal acceptance rate can in fact take any value between 0 and 1.\nSherlock and Roberts (2009) provide analytical formulae for calculating the ESJD and the\nexpected acceptance rate for any proposal and any elliptically symmetric unimodal target. In\none dimension, for example, the optimal acceptance rate for a Gaussian target explored by a\nGaussian proposal is 0.44, whilst the optimum for a double exponential target (\u03c0(x) \u221d e\u2212|x|)\nexplored with a double exponential proposal is exactly \u03b1\u02c6 = 1\/3. Sherlock (2006) considers\n17\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nseveral simple examples of spherically symmetric proposal and target across a range of di-\nmensions and finds that in all cases curvature at the optimal acceptance rate is small, so\nthat a range of acceptance rates is nearly optimal. Further, the optimal acceptance rate is\nitself between 0.2 and 0.3 for d \u2265 6 in all the cases considered.\nSherlock and Roberts (2009) also weaken the \u201cshell\u201d condition of Section 2.2.4 and consider\nsequences of spherically symmetric targets for which the (rescaled) radius converges to some\nrandom variable R rather than a point mass at 1. It is shown that, provided the sequence\nof proposals still satisfies the shell condition, the limiting optimal acceptance rate is strictly\nless than 0.234. Acceptance rate tuning should thus be seen as only a guide, though a guide\nwhich has been found to be robust in practice.\nAlgorithm 1 (Blk): The first algorithm (Blk) used to explore data sets D1 and D2 is a\nfour dimensional block updating RWM with proposal Y \u223c N(0, \u03bb2I) and \u03bb tuned so that\nthe acceptance rate is approximately 0.3.\n3.2 Optimal scaling of the RWM within Gibbs\nIntuition: Consider first a target either spherically symmetric, or with i.i.d. components,\nand let the overall scale of variability of the target be \u03b7. For full block proposals the optimal\nscale parameter should beO\n(\n\u03b7\/d1\/2\n)\nso that the square of the magnitude of the total proposal\nis O(\u03b72). If a Metropolis within Gibbs update is to be used with k sub-blocks and d\u2217 = d\/k\nof the components updated at each stage then the optimal scale parameter should be larger,\nO\n(\n\u03b7\/d\n1\/2\n\u2217\n)\n. However only one of the k stages of the RWM within Gibbs algorithm updates\nany given component whereas with k repeats of a block RWM that component is updated k\ntimes. Considering the squared jump distances it is easy to see that, given the additivity of\nsquare jump distances, the larger size of the RWM within Gibbs updates is exactly canceled\nby their lower frequency, and so (in the limit) there is no difference in efficiency when\n18\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\ncompared with a block update. The same intuition applies when comparing a random scan\nMetropolis within Gibbs scheme with a single block update.\nNow consider a target for which different components vary on different scales. If sub-blocks\nare chosen so as to group together components with similar scales then a Metropolis within\nGibbs scheme can apply suitable scale paramaters to each block whereas a single block\nupdate must choose one scale parameter that is adequate for all components. In this scenario,\nMetropolis within Gibbs updates should therefore be more efficient.\nTheory: Neal and Roberts (2006) consider a random scan RWM within Gibbs algorithm\non a target distribution with i.i.d. components and using i.i.d. Gaussian proposals all\nhaving the same scale parameter \u03bbd = \u00b5\/d\n1\/2. At each iteration a subset (of size dcd) of the\ncomponents is chosen at random and updated as a single block. It is shown (again subject\nto differentiability conditions on f(\u00b7)) that the processW (d)t := X(d)1,[td] approaches a Langevin\ndiffusion with speed\nhc(\u00b5) = 2c\u00b5\n2\u03a6\n(\n\u22121\n2\n\u00b5(cI)1\/2\n)\n.\nThe optimal scaling is therefore larger than for a standard block update (by a factor of c\u22121\/2)\nbut the optimal speed and the optimal acceptance rate (0.234) are identical to those found\nby Roberts et al. (1997).\nSherlock (2006) considers sequential Metropolis within Gibbs updates on a unimodal ellip-\ntically symmetric target, using spherical proposal distributions but allowing different scale\nparameters for the proposals in each sub-block. The k sub-blocks are assumed to correspond\nto disjoint subsets of the principal axes of the ellipse and updates for each are assumed\nto be optimally tuned. Efficiency is considered in terms of ESEJD and is again found to\nbe optimal (as d \u2192 \u221e) when the acceptance rate for each sub-block is 0.234. For equal\nsized sub-blocks, the relative efficiency of the Metropolis within Gibbs scheme compared to\n19\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nk optimally scaled single block updates is shown to be\nr =\n1\nk\n\u2211\nc2i(\n1\nk\n\u2211\n1\nc2i\n)\u22121 , (13)\nwhere c2i is the mean of the squares of the inverse scale parameters for the i\nth block. Since r\nis the ratio of an arithmetic mean to a harmonic mean, it is greater than or equal to one and\nthus the Metropolis within Gibbs step is always at least as efficient as the block Metropolis.\nHowever the more similar the blocks, the less the potential gain in efficiency.\nIn practice, parameter blocks do not generally correspond to disjoint subsets of the principal\naxes of the posterior or, in terms of single parameter updates, the parameters are not gen-\nerally orthogonal. Equation 13 therefore corresponds a limiting maximum efficiency gain,\nobtainable only when the parameter sub-blocks are orthogonal.\nAlgorithm 2 (MwG): Our second algorithm (MwG) is a sequential Metropolis within\nGibbs algorithm with proposed jumps Yi \u223c N(0, \u03bb2i ). Each scale parameter is tuned\nseperately to give an acceptance rate of between 0.4 and 0.45 (approximately the optimum\nfor a one-dimensional Gaussian target and proposal).\n3.3 Tailoring the shape of a block proposal\nIntuition: First consider a general target with roughly elliptical contours and covariance\nmatrix \u03a3, such as that shown in Figure 3. For simplicity we visualise a two parameter\nposterior but the following argument clearly generalises to any number of dimensions. It\nseems intuitively sensible that a \u201ctailored\u201d block proposal distribution with the same shape\nand orientation as the target will tend to produce larger jumps along the target\u2019s major\naxis and smaller jumps along its minor axis and should therefore allow for more efficient\nexploration of the target.\n20\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nx\ny\n\u22126 \u22124 \u22122 0 2 4 6\n\u2212\n6\n\u2212\n4\n\u2212\n2\n0\n2\n4\n6\nFigure 3: Contour plot for a two dimensional Gaussian density with \u03c321 = \u03c3\n2\n2 = 1 and correlation\n\u03c1 = 0.95.\nTheory: Sherlock (2006) considers exploration of a unimodal elliptically symmetric target\nwith either a spherically symmetric proposal or a tailored elliptically symmetric proposal in\nthe limit as d \u2192 \u221e. Subject to condition (11) (and a \u201cshell\u201d-like condition similar to that\nmentioned in Section 2.2.4), it is shown that with each proposal shape it is in fact possible to\nachieve the same optimal expected square jump distance. However if a spherically symmetric\nproposal is used on an elliptical target, some components are explored better than others and\nin some sense the overall efficiency is reduced. This becomes clear on considering the ratio\nr, of the expected squared Euclidean jump distance for an optimal spherically symmetric\nproposal to that of an optimal tailored proposal. Sherlock (2006) shows that for a sequence\nof targets, where the target with dimension d has elliptical axes with inverse scale parameters\ncd,1, . . . , cd,d, the limiting ratio is\nr =\nlimd\u2192\u221e\n(\n1\nd\n\u2211d\ni=1 c\n\u22122\nd,i\n)\u22121\nlimd\u2192\u221e\n1\nd\n\u2211d\ni=1 c\n2\nd,i\n.\nThe numerator is the limiting harmonic mean of the squared inverse scale parameters, which\nis less than or equal to their arithmetic mean (the denominator), with equality if and only if\n(for a given d) all the cd,i are equal. Roberts and Rosenthal (2001) examine similar relative\n21\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nefficiencies but for targets and proposals with independent components with inverse scale\nparameters C sampled from some distribution. In this case the derived measure of relative\nefficiency is the relative speeds of the diffusion limits for the first component of the target\nr\u2217 =\nE [C]2\nE [C2]\n.\nThis is again less than or equal to one, with equality when all the scale parameters are equal.\nHence efficiency is indeed directly related to the relative compatibility between target and\nproposal shapes.\nFurthermore Be\u00b4dard (2008) shows that if a proposal has i.i.d. components yet the target\n(assumed to have independent components) is wildly asymmetric, as measured by (11), then\nthe limiting optimal acceptance rate can be anywhere between 0 and 1. However even at\nthis optimum, some components will be explored infinitely more slowly than others.\nIn practice the shape \u03a3 of the posterior is not known and must be estimated, for example by\nnumerically finding the posterior mode and the Hessian matrix H at the mode, and setting\n\u03a3 = H\u22121. We employ a simple alternative which uses an earlier MCMC run.\nAlgorithm 3 (BlkShp): Our third algorithm first uses an optimally scaled block RWM\nalgorithm (Algorithm 1), which is run for long enough to obtain a \u201creasonable\u201d estimate of\nthe covariance from the posterior sample. A fresh run is then started and tuned to give an\nacceptance rate of about 0.3 but using proposals\nY \u223c N(0, \u03bb2\u03a3\u02c6).\nFor each data set, so that our implementation would reflect likely statistical practice, each\nof the three replicates of this algorithm estimated the \u03a3 matrix from iterations 1000-2000\nof the corresponding replicate of Algorithm 1 (i.e. using 1000 iterations after \u201cburn in\u201d). In\nall therefore, six different variance matrices were used.\n22\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\n3.4 Improving tail exploration\nIntuition: A posterior with relatively heavy polynomial tails such as the one-dimensional\nCauchy distribution has considerable mass some distance from the origin. Proposal scalings\nwhich efficiently explore the body of the posterior are thus too small to explore much of the\ntail mass in a \u201creasonable\u201d number of iterations. Further, polynomial tails become flatter\nwith distance from the origin so that for unit vector u, \u03c0(x+ \u03bbu)\/\u03c0(x)\u2192 1 as ||x||2 \u2192\u221e.\nHence the acceptance rate for a random walk algorithm approaches 1 in the tails, whatever\nthe direction of the proposed jump. The algorithm therefore loses almost all sense of the\ndirection to the posterior mass.\nTheory: Roberts (2003) brings together literature relating the tails of the posterior and the\nproposal to the ergodicity of the Markov chain and hence its convergence properties. Three\nimportant cases are noted\n1. If \u03c0(x) \u221d e\u2212s||x||2, at least outside some compact set, then the random walk algorithm\nis geometrically ergodic.\n2. If the tails of the proposal are bounded by some multiple of ||x||\u2212(r+d)2 and if \u03c0(x) \u221d\n||x||\u2212(r+d)2 , at least outside some compact set, then the algorithm is polynomially er-\ngodic with rate r\/2.\n3. If \u03c0(x) \u221d ||x||\u2212(r+d)2 , at least for large enough x, and the proposal has tails q(x) \u221d\n||x||\u2212(d+\u03b7)2 (0 < \u03b7 < 2) then the algorithm is polynomially ergodic with rate r\/\u03b7.\nThus posterior distributions with exponential or lighter tails lead to a geometrically er-\ngodic Markov chain, whereas polynomially tailed posteriors can lead to polynomially ergodic\nchains, and even this is only guaranteed if the tails of the proposal are at least as heavy as\nthe tails of the posterior. However by using a proposal with tails so heavy that it has infinite\nvariance, the polynomial convergence rate can be made as large as is desired.\n23\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nAlgorithm 4 (BlkShpCau): Our fourth algorithm is identical to BlkShp but samples\nthe proposed jump from the heavy tailed multivariate Cauchy. Proposals are generated by\nsimulating V \u223c N(0, \u03a3\u02c6) and Z \u223c N(0, 1) and setting Y\u2217 = V\/Z. No acceptance rate\ncriteria exist for proposals with infinite variance and so the optimal scaling parameter for this\nalgorithm was found (for each dataset and \u03a3\u02c6) by repeating several small runs with different\nscale parameters and noting which produced the best ACT\u2019s for each data set.\nAlgorithm 5 (BlkShpMul): The fifth algorithm relies on the fact that taking logarithms\nof parameters shifts mass from the tails to the centre of the distribution. It uses a random\nwalk on the posterior of \u03b8\u02dc := (log\u03c81, log\u03c82, log q12, log q21). Shape matrices \u03a3\u02c6 were estimated\nas for Algorithm 3, but using the logarithms of the posterior output from Algorithm 1. In\nthe original parameter space this algorithm is equivalent to a proposal with components\nX\u2217i = Xi e\nY \u2217i and so has been called the multiplicative random walk (see for example\nDellaportas and Roberts, 2003). In the original parameter space the acceptance probability\nis\n\u03b1(x,x\u2217) = min\n(\n1,\n\u220fd\n1 x\n\u2217\ni\u220fd\n1 xi\n\u03c0(x\u2217)\n\u03c0(x)\n)\n.\nSince the algorithm is simply an additive random walk on the log parameter space, the usual\nacceptance rate optimality criteria apply.\nA logarithmic transformation is clearly only appropriate for positive parameters and can\nin fact lead to a heavy left hand tail if a parameter (in the original space) has too much\nmass close to zero. The transformation \u03b8\u02dci = sign(\u03b8i) log(1 + |\u03b8i|) circumvents both of these\nproblems.\n3.5 Combining algorithms\nIntuition: Different MCMC algorithms may have different strengths and weaknesses. For\nexample algorithm A(1) may efficiently explore the tails of a distribution whereas algorithm\n24\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nA(2) might efficiently explore the body. In such circumstances a hybrid algorithm which\nalternates iterations from A(1) and A(2) should combine the strengths of both, with efficiency\nin a given portion of the posterior no worse than half that of the more efficient algorithm. A\nsimilar argument applies when two algorithms are each efficient at exploring a different type\nof posterior (e.g. relatively heavy tailed and relatively light tailed). In this case alternating\niterations from the algorithms produces a hybrid algorithm which is robust to the type of\nposterior.\nTheory: Consider the inner product\n< \u03bd1, \u03bd2 >:=\n\u222b\ndx\n\u03bd1(x)\u03bd2(x)\n\u03c0(x)\n, (14)\nand the associated L2 norm, ||\u03bd||2 :=< \u03bd, \u03bd >1\/2. To avoid technical detail, we restrict\nattention to distributions \u03bd(\u00b7) which are absolutely continuous with respect to \u03c0(\u00b7) and for\nwhich the L2 norm with respect to (14) exists: E\u03c0\n[|d\u03bd\/d\u03c0|2] < \u221e. We also assume that\neach transition kernel (A,A(1), A(2), and A\u2217) has a discrete spectrum; a more general theory\nexists and can be found used in the context of MCMC in Roberts and Rosenthal (1997), for\ninstance.\nWithin the simplified framework described above, it is shown in Appendix A that from\ninitial distribution \u03bd, for any reversible MCMC kernel A with stationary distribution \u03c0(\u00b7)\nand second largest eigenvalue \u03b22,\n\u2223\u2223\u2223\u2223\u03bdAk \u2212 \u03c0\u2223\u2223\u2223\u2223\n2\n\u2264 \u03b2k2 ||\u03bd \u2212 \u03c0||2 .\nSince\n\u2223\u2223\u2223\u2223\u03bdAk \u2212 \u03c0\u2223\u2223\u2223\u2223\n1\n\u2264 \u2223\u2223\u2223\u2223\u03bdAk \u2212 \u03c0\u2223\u2223\u2223\u2223\n2\nthis demonstrates geometric ergodicity as defined in Sec-\ntion 2.2.1.\nNext consider two MCMC algorithms A(1) and A(2) with stationary distribution \u03c0(\u00b7) and\nsecond largest eigenvalues \u03b2\n(1)\n2 and \u03b2\n(2)\n2 . Let A\n\u2217 be a combination algorithm which alternates\niterations from A(1) and A(2). Of course A\u2217 is not, in general, reversible; nonetheless it can\n25\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nalso be shown (see Appendix A) that\u2223\u2223\u2223\u2223\u2223\u2223\u03bd (A\u2217)k \u2212 \u03c0\u2223\u2223\u2223\u2223\u2223\u2223\n2\n\u2264\n(\n\u03b2\n(1)\n2 \u03b2\n(2)\n2\n)k\n||\u03bd \u2212 \u03c0||2 .\nThus the bound on geometric convergence rate for A\u2217 is at worst the geometric mean of the\nbounds on the convergence rates of its two component algorithms. The result generalises to\nthe sequential combination of any n algorithms.\nInstead of alternating A(1) and A(2), at each iteration one of the two algorithms could be\nchosen at random with probabilities 1\u2212 \u03b4 and \u03b4. Combining the triangle inequality with the\nfirst result in this section, for this mixture kernel A\u2217\u2217\n||\u03bdA\u2217\u2217 \u2212 \u03c0||2 =\n\u2223\u2223\u2223\u2223(1\u2212 \u03b4) (\u03bdA(1))\u2212 \u03c0)+ \u03b4 (\u03bdA(1))\u2212 \u03c0)\u2223\u2223\u2223\u2223\n2\n\u2264 ((1\u2212\u03b4)\u03b21+\u03b4\u03b22) ||\u03bd \u2212 \u03c0||2 . (15)\nThe geometric convergence rate for this (reversible) kernel, A\u2217\u2217 is clearly at most (1\u2212\u03b4)\u03b21+\n\u03b4\u03b22. Practical implementation of such a mixture kernel is illustrated in the next section in\nthe context of adaptive MCMC.\n3.6 Adaptive MCMC\nIntuition: Algorithm 3 used the output from a previous MCMC run to estimate the\nshape Matrix \u03a3. An overall scaling parameter was then varied to give an acceptance rate of\naround 0.3. With adaptive MCMC a single chain is run, and this chain gradually alters its\nown proposal distribution (e.g. changing \u03a3), by learning about the posterior from its own\noutput. This simple idea has a major potential pitfall, however.\nIf the algorithm is started away from the main posterior mass, for example in a tail or a\nminor mode, then it initially learns about that region. It therefore alters the proposal so that\nit efficiently explores this region of minor importance. Worse, in so altering the proposal the\nalgorithm may become even less efficient at finding the main posterior mass, remain in an\nunimportant region for longer and become even more influenced by that unimportant region.\n26\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nThe acceptance rate for each proposal is chosen so that its stationary distribution is \u03c0(\u00b7).\nHowever since the transition kernel is continually changing, potentially with the positive\nfeeback mechanism of the previous paragraph, this is not sufficient to guarantee that the\noverall stationary distribution of the chain is \u03c0(\u00b7). Roberts and Rosenthal (2007) give a very\nsimple adaptive MCMC scheme on a discrete state space for which the resulting stationary\ndistribution is not the intended target.\nA simple solution to this stationarity problem is so called finite adaptation wherein the\nalgorithm is only allowed to evolve for the first n0 iterations, after which time the transition\nkernel is fixed. Such a scheme is equivalent to running a shorter \u201ctuning\u201d chain and then\na longer subsequent chain (e.g. Algorithm 3). If the tuning portion of the chain has only\nexplored a minor mode or a tail this still leads to an inefficient algorithm. We would prefer\nto allow the chain to eventually correct for any errors made at early iterations and yet still\nlead to the intended stationary distribution. It seems sensible that this might be achieved\nprovided changes to the kernel become smaller and smaller as the algorithm proceeds and\nprovided the above-mentioned positive feedback mechanism can never pervert the entire\nalgorithm.\nTheory: At the nth iteration let \u0393n represent the choice of transition kernel; for the\nRWM it might represent the current shape matrix \u03a3 and the overall scaling \u03bb. Denote\nthe corresponding transition kernel P\u0393n(x, \u00b7). Roberts and Rosenthal (2007) derive a set\nof two conditions which together guarantee convergence to the stationary distribution. A\nkey concept is that of diminishing adaptation, wherein changes to the kernel must become\nvanishingly small as n\u2192\u221e\nsup\nx\n\u2223\u2223\u2223\u2223P\u0393n+1(x, \u00b7)\u2212 P\u0393n(x, \u00b7)\u2223\u2223\u2223\u22231 p\u2212\u2192 0 as n\u2192\u221e.\nA second containment condition considers the \u01eb-convergence time under repeated application\nof a fixed kernel, \u03b3, and starting point x,\nM\u01eb(x, \u03b3) := inf\nn\n{\nn \u2265 1 : \u2223\u2223\u2223\u2223P n\u03b3 (x, \u00b7)\u2212 \u03c0(\u00b7)\u2223\u2223\u2223\u22231 \u2264 \u01eb\n}\n,\n27\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nand requires that for all \u03b4 > 0 there is an N such that for all n\nP (M\u01eb(Xn,\u0393n) \u2264 N | X0 = x0,\u03930 = \u03b30) \u2265 1\u2212 \u03b4.\nThe containment condition is, in general, difficult to check in practice; some criteria are\nprovided in Bai et al. (2009).\nAdaptive MCMC is a highly active research area at present, and so when considering specific\nschemes, we confine ourselves to adaptations relating to posterior shape and scaling. Roberts\nand Rosenthal (2009) describe an adaptive RWM algorithm for which the proposal at the\nnth iteration is sampled from a mixture of adaptive and non-adaptive distributions\nY \u223c\n\uf8f1\uf8f2\n\uf8f3 N\n(\n0, 1\nd\n2.382\u03a3n\n)\nw.p. 1\u2212 \u03b4\nN\n(\n0, 1\n100d\nI\n)\nw.p. \u03b4.\nHere \u03b4 = 0.05 and \u03a3n is the variance matrix calculated from the previous n \u2212 1 iterations\nof the scheme. Changes to the variance matrix are O(1\/n) at the nth iteration and so the\nalgorithm satisfies the diminishing adaptation condition. Haario et al. (2001) show that\na similar adaptive scheme with Y \u223c N (0, 1\nd\n2.382\u03a3n + \u01eb\n2I\n)\n(for fixed \u01eb > 0) is ergodic\nprovided both the target density and its support are bounded.\nChoice of the overall scaling factor 2.382\/d follows directly from the optimal scaling limit\nresults reviewed in Section 3.1, with I = 1 or k\n(d)\nx = k\n(d)\ny . In general therefore a different\nscaling might be appropriate.\nAlgorithm 6 (BlkAdpMul): Our adaptive MCMC algorithm is a block multiplicative\nrandom walk which samples jump proposals on the log-posterior from the mixture\nY \u223c\n\uf8f1\uf8f2\n\uf8f3 N (0, m\n2\u03a3n) w.p. 1\u2212 \u03b4\nN\n(\n0, 1\nd\n\u03bb20I\n)\nw.p. \u03b4.\nHere \u03b4 = 0.05, d = 4, and \u03a3n is estimated from the logarithms of the posterior sample to\ndate. A few minutes were spent tuning the block multiplicative random walk with proposal\n28\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nvariance 1\n4\n\u03bb20I to give at least a reasonable value for \u03bb0 (acceptance rate \u2248 0.3), although\nthis is not stricly necessary.\nThe overall scaling factor for the adaptive part of the kernel was allowed to vary according\nto the following scheme.\n1. An initial scaling was set to m0 = 2.38\/d\n1\/2 and an adaptation quantity \u2206 = m0\/100\nwas defined.\n2. Proposals from the adaptive part of the mixture were only allowed once there had been\nat least 10 proposed jumps accepted.\n3. If iteration i was from the adaptive part of the kernel then m was altered:\n\u2022 If the proposal was rejected then m\u2190 m\u2212\u2206\/i1\/2.\n\u2022 If the proposal was accepted then m\u2190 m+ 2.3 \u2206\/i1\/2.\nStep 2 ensures a sufficient number of different parameter values to calculate a sensible co-\nvariance matrix (note that with three or fewer acceptances, rows of the covariance matrix\nare not even linearly independent). Step 3 leads to an equilibrium acceptance rate of 1\/3.3.\nChanges to m are scaled by i1\/2 since they must be large enough to adapt to changes in\nthe covariance matrix yet small enough that an equilibrium value is established relatively\nquickly. As with the variance matrix, such a value would then only change noticeably if\nthere were consistent evidence that it should.\n3.7 Utilising problem specific knowledge\nIntuition: All of the above techniques apply to RWM algorithms on any posterior. However\nalgorithms are always applied to specific data sets with specific forms for the likelihood and\n29\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nprior. Combining problem specific knowledge with techniques such as optimal scaling and\nshape adjustmet can often markedly improve efficiency. In the case of the MMPP we define a\nreparameterisation based on the intuition that for an MMPP with \u03c81 \u2248 \u03c82 the data contain\na great deal of information about the average intensity but relatively little information about\nthe difference between the intensities. With this reparameterisation the posterior for data\nset D2 may then be very efficiently sampled using a Metropolis within Gibbs algorithm.\nTheory: For a 2 dimensional MMPP define an overall transition intensity, stationary\ndistribution, mean intensity at stationarity, and a measure of the difference between the two\nevent intensities as follows\nq := q12 + q21 , \u03bd :=\n1\nq\n[q21, q12]\nt , \u03c8 := \u03bdt\u03c8 and \u03b4 :=\n(\u03c82 \u2212 \u03c81)\n\u03c8\n. (16)\nLet tobs be the total observation time. If the Poisson event intensities are similar, \u03b4 is small,\nand Taylor expansion of the log-likelihood in \u03b4 (see Appendix B) gives\nl(\u03c8, q, \u03b4, \u03bd1) = n log\u03c8 \u2212 \u03c8tobs + 2\u03b42\u03bd1\u03bd2f(\u03c8t, qt) + \u03b43\u03bd1\u03bd2(\u03bd2 \u2212 \u03bd1)g(\u03c8t, qt) +O(\u03b44) (17)\nfor some f(\u00b7, \u00b7) and g(\u00b7, \u00b7). Consider a reparameterisation from (\u03c81, \u03c82, q12, q21) to (\u03c8, q, \u03b1, \u03b2)\nwith\n\u03b1 := 2\u03b4 (\u03bd1\u03bd2)\n1\/2 and \u03b2 := \u03b4(\u03bd2 \u2212 \u03bd1). (18)\nParameters \u03c8; q and \u03b1; and \u03b2 (in this order) capture decreasing amounts of variation in\nthe log-likelihood and so, conversely, it might be anticipated that there be corresponding\ndecreasing amounts of information about these parameters contained in the likelihood. Hence\nvery different scalings might be required for each.\nAlgorithm 7 (MwGRep): A Metropolis within Gibbs update scheme was applied to the\nreparameterisation (\u03c8, q, \u03b1, \u03b2). A multiplicative random walk was used for each of the first\n3 parameters (since they are positive) and an additive update was used for \u03b2. Scalings for\neach of the four parameters were chosen to give acceptance rates of between 0.4 and 0.45.\n30\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nAlgorithm 8 (MwGRepCau): Our final algorithm is identical to MwGRep except that\nadditive updates for \u03b2 are proposed from a Cauchy distribution. The Cauchy scaling was\noptimised to give the best ACT over the first 1000 iterations.\n4 Results\nEach RWM variation was tested against data sets D1 and D2 as described in Section 2.3.1.\nFor each data set, each algorithm was started from the known \u201ctrue\u201d parameter values and\nwas run 3 times with 3 different random seeds (referred to as Replicates 1-3). All algorithms\nwere run for 11000 iterations; a burn in of 1000 iterations was sufficient in all cases.\nPriors were independent and exponential with means the known \u201ctrue\u201d parameter values.\nThe likelihood of an MMPP with maximum and minimum Poisson intensities \u03c8max and\n\u03c8min and with n events observed over a time window of length tobs, is bounded above by\n\u03c8nmaxe\n\u2212\u03c8mintobs. In this article only MMPP parameters and their logarithms are considered\nfor estimation. Since exponential priors are employed the parameters and their logarithms\ntherefore have finite variance, and geometric ergodicity is guaranteed.\nThe accuracy of posterior simulations is assessed via QQ plot comparison with the output\nfrom a very long run of a Gibbs sampler (see Section 2.2.3). QQ plots for all replicates\nwere almost entirely within their 95% confidence bounds. Figure 4 shows such plots for\nAlgorithms 1-3 on data set D2 (Replicate 1). In general these three combinations produced\nthe least accurate performance yet even in these cases there is little reason to doubt that\nACT\u2019s represent each algorithm\u2019s exploration of the true posterior rather than a tail or minor\nmode. The first replicate of Algorithm 4 on D2 also showed an imperfect fit in the tails.\nThis, and the replicate\u2019s uncharacteristically high ACT\u2019s arise directly from an excursion\nlasting for about 500 iterations, in which the Markov chain became stuck in a minor mode\n31\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nwith \u03c81 \u2248 7, \u03c82 \u2248 14, q12 \u2248 3, q21 \u2248 0.3.\nThe integrated ACT was estimated for each parameter and each replicate using the final 10\n000 iterations from that replicate. Calculation of the likelihood is by far the most compu-\ntationally intensive operation and is performed four times for each Metropolis within Gibbs\niteration (once for each parameter) and only once for each block update. To give a truer\nindication of overall efficiency the ACTs for each Metropolis within Gibbs replicate have\ntherefore been multiplied by four. Table 1 shows the mean adjusted ACT for each algo-\nrithm, parameter, and data set. for each set of three replicates most of the ACTs lay within\n20% of their mean, and for the exceptions (Blk and BlkShpCau for data sets D1 and D2, and\nBlkShp and BlkShpMul for data set D2) full sets of ACTs are given in Table 2 in Appendix\nC.\nIn general all algorithms performed better on D1 than on D2 because, as discussed in Section\n2.3.1 data set D1 contains more information on the parameters than D2; it therefore has\nlighter tails and is more easily explored by the chain.\nAs might be expected, the simple block additive algorithm using Gaussian proposals with\nvariance matrix proportional to the identity matrix (Blk) performs relatively poorly on both\ndata sets. In absolute terms there is much less uncertaintly about the transition intensities\nq12 and q21 (both are close to 1) than in the Poisson intensities \u03c81 (10) and \u03c82 (17 for D1\nand 30 for D2) since the variance of the output from a Poisson process is proportional to\nits value. The optimal single scale parameter necessarily tunes to the smallest variance and\nhence explores q12 and q21 much more efficiently than \u03c81 and \u03c82.\nOverall performance improves enormously once block proposals are from a Gaussian with\napproximately the correct shape (BlkShp). The efficiency of the Metropolis within Gibbs\nalgorithm with additive Gaussian updates (MwG) lies somewhere between the efficiencies of\nBlk and BlkShp but the improvement over Blk is larger for data set D1 than for data set\n32\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\n4 6 8 12\n4\n6\n8\n10\n12\n14 18 22\n14\n16\n18\n20\n22\n\u22122 0 1\n\u2212\n2\n\u2212\n1\n0\n1\n\u22123 \u22121 0 1\n\u2212\n3\n\u2212\n2\n\u2212\n1\n0\n1\n4 6 8 12\n4\n6\n8\n10\n12\n14 18 22\n14\n16\n18\n20\n22\n\u22122 0 1\n\u2212\n2\n\u2212\n1\n0\n1\n\u22123 \u22121 1\n\u2212\n3\n\u2212\n1\n0\n1\n4 6 8 12\n4\n6\n8\n10\n12\n14 18 22\n14\n16\n18\n20\n22\n\u22122 0 1\n\u2212\n2\n\u2212\n1\n0\n1\n\u22123 \u22121 0 1\n\u2212\n3\n\u2212\n2\n\u2212\n1\n0\n1\n\u03c81\n\u03c81\n\u03c81\n\u03c82\n\u03c82\n\u03c82\nlog(q12)\nlog(q12)\nlog(q12)\nlog(q21)\nlog(q21)\nlog(q21)\nB\nlk\nB\nlk\nB\nlk\nB\nlk\nM\nw\nG\nM\nw\nG\nM\nw\nG\nM\nw\nG\nB\nlk\nS\nh\np\nB\nlk\nS\nh\np\nB\nlk\nS\nh\np\nB\nlk\nS\nh\np\nGibbsGibbsGibbsGibbs\nGibbsGibbsGibbsGibbs\nGibbsGibbsGibbsGibbs\nFigure 4: QQ plots for algorithms Blk, MwG, and BlkShp on D2 (Replicate 1). Dashed lines are\napproximate 95% confidence limits obtained by repeated sampling from iterations 1000 to 100 000\nof a Gibbs sampler run; sample sizes were 10 000\/ACT, which is the effective sample size of the\ndata being compared to the Gibbs run.\n33\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nD1 D2\nAlgorithm \u03c81 \u03c82 log (q12) log (q21) \u03c81 \u03c82 log (q12) log (q21)\nBlk 66 126 15 19 176 175 80 70\nMwG* 22 22 33 33 103 90 114 99\nBlkShp 13 18 13 15 46 25 37 36\nBlkShpCau 19 32 25 24 63 50 56 38\nBlkShpMul 13 17 13 15 33 26 22 16\nBlkAdpMul 12 12 14 14 20 20 17 23\nMwGRep* 13 14 32 44 20 23 23 21\nMwGRepCau* 14 15 37 42 24 233 25 23\nTable 1: Mean estimated integrated autocorrelation time for the four parameters over three\nindependent replicates for data sets D1 and D2. *Estimates for MwG replicates have been\nmultiplied by 4 to provide figures comparable with full block updates in terms of CPU time.\nD2. As discussed in Section 2.3.1 the parameters in D1 are more nearly independent than\nthe parameters in D2. Thus for data set D1 the principal axes of an elliptical approximation\nto the posterior are more nearly parallel to the cartesian axes. Metropolis-within-Gibbs\nupdates are (by definition) parallel to each of the cartesian axes and so can make large\nupdates almost directly along the major axis of the ellipse for data set D1.\nFor the heavy tailed posterior of data set D2 we would expect block updates resulting from\na Cauchy proposal (BlkShpCau) to be more efficient than those from a Gaussian proposal.\nHowever for both data sets Cauchy proposals are slightly less efficient than Gaussian propos-\nals. It is likely that the heaviness of the Cauchy tails leads to more proposals with at least\none negative parameter, such proposals being automatically rejected. Moreover \u03a3\u02c6 represents\nthe main posterior mass, yet some large Cauchy jump proposals from this mass will be in\nthe posterior tail. It may be that \u03a3\u02c6 does not accurately represent the shape of the posterior\ntails.\n34\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nMultiplicative updates (BlkShpMul) make little difference for D1, but for the relatively heavy\ntailed D2 there is a definite improvement over BlkShpAdd. The adaptive multiplicative\nalgorithm (BlkAdpMul) is slightly more efficient still, since the estimated variance matrix\nand the overall scaling are refined thoughout the run.\nAs was noted earlier in this section, due to our choice of exponential priors the quantities\nestimated in this article have exponential or lighter posterior tails and so all the non-adaptive\nalgorithms in this article are geometrically ergodic. The theory in Section 3.4 suggests ways\nto improve tail exploration for polynomially ergodic algorithms and so, strictly speaking,\nneed not apply here. However the exponential decay only becomes dominant some distance\nfrom the posterior mass, especially for data set D2. Polynomially increasing terms in the\nlikelihood ensure that initial decay is slower than exponential, and that the multiplicative\nrandom walk is therefore more efficient than the additive random walk.\nThe adaptive overall scaling m showed variability of O(0.1) over the first 1000 iterations\nafter which time it quickly settled down to 1.2 for all three replicates on D1 and to 1.1 for all\nthree replicates on D2. Both of these values are very close to the scaling of 1.19 that would\nbe used for a four dimensional update in the scheme of Roberts and Rosenthal (2009). The\nalgorithm similarly learnt very quickly about the variance matrix \u03a3, with individual terms\nsettling down after less than 2000 iterations, and with exploration close to optimal after less\nthan 500 iterations. This can be seen clearly in Figure 5 which shows trace plots for the first\n2000 iterations of the first replicate of BlkAdpMul on D2.\nThe adaptive algorithm uses its own history to learn about d(d+1)\/2 covariance terms and a\nbest overall scaling. One would therefore expect that the larger the number of parameters, d,\nthe more iterations are required for the scheme to learn about all of the adaptive terms and\nhence reach a close to optimal efficiency. To test this a data set (D3) was simulated from a\nthree-dimensional MMPP with \u03c8 = [10, 17, 30]t and q12 = q13 = q21 = q23 = q31 = q32 = 0.5.\nThe following adaptive algorithm was then run three times, each for 20 000 iterations.\n35\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\n0 500 1000 1500 2000\n6\n8\n10\n12\nIndex\n0 500 1000 1500 2000\n14\n16\n18\n20\n22\nIndex\n0 500 1000 1500 2000\n\u2212\n2\n\u2212\n1\n0\n1\nIndex\n0 500 1000 1500 2000\n\u2212\n2.\n0\n\u2212\n1.\n0\n0.\n0\n1.\n0\nIndex\n\u03c8\n1\n\u03c8\n2\nlo\ng\n(q\n1\n2\n)\nlo\ng\n(q\n2\n1\n)\nFigure 5: Trace plots for the first 2000 iterations of BlkAdpMul on data set D2 (Replicate 1).\nAlgorithm 6b (BlkAdpMul(b)): This adaptive algorithm is identical to BlkAdpMul\n(with d = 9) except that no adaptive proposals were used until at least 100 non-adaptive\nproposals had been accepted, and that if an adaptive proposal was accepted then the overall\nscaling was updated with m \u2190 m + 3 \u2206\/i1\/2 so that the equilibrium acceptance rate was\napproximately 0.25.\nFigure 6 shows the evolution of four of the forty six adaptive parameters (Replicate 1). All\nparameters seem close to their optimal values after 10 000 iterations, although covariance\nparameters appear to be still slowly evolving even after 20 000 iterations. In contrast,\nexploration of the posterior is close to its final optimum after only 1500 iteration as can be\nseen in trace plots of the first 4000 iterations of the same replicate (Figure 7). This behaviour\nwas repeated across the other two replicates, indicating that, as with the two-dimensional\nadaptive and non-adaptive runs, even a very rough approximation to the variance matrix\nimproves efficiency considerably. Over the full 20 000 iterations, all three replicates showed a\ndefinite multimodality with \u03bb2 often close to either \u03bb1 or \u03bb3, indicating that the data might\n36\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\n0 5000 10000 15000 20000\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\n1.\n2\nIndex\n0 5000 10000 15000 20000\n0.\n00\n0\n0.\n00\n2\n0.\n00\n4\n0.\n00\n6\nIndex\n0 5000 10000 15000 20000\n0.\n0\n1.\n0\n2.\n0\n3.\n0\nIndex\n0 5000 10000 15000 20000\n\u2212\n0.\n04\n\u2212\n0.\n02\n0.\n00\nIndex\nm\nV\na\nr\n[l\no\ng\n\u03c8\n1\n]\nV\na\nr\n[l\no\ng\nq\n1\n2\n]\nC\no\nv\n[\u03c8\n1\n,\nq\n1\n2\n]\nFigure 6: Plots of the adaptive scaling parameter m and three estimated covariance parameters\nVar [\u03c81], Var [q12], and Cov [\u03c81, q12] for BlkAdpMul(b) on data set D3 (Replicate 1).\nreasonably be explained by a two dimensional MMPP. In all three replicates the optimal\nscaling settled between 0.25 and 0.3, noticeably lower than Roberts and Rosenthal (2009)\nvalue of 2.38\/\n\u221a\n9. With reference to Section 3.1 this is almost certainly due to the roughness\ninherent in a multimodal posterior.\nThe reparameterisation of Section 3.7 was designed for data sets similar to D2, and on this\ndata set the resulting Metropolis within Gibbs algorithm (MwGRep) is at least as efficient\nas the adaptive multiplicative random walk. On data set D1 however exploration of q12 and\nq21 is arguably less efficient than for the Metropolis within Gibbs algorithm with the original\nparameter set. The lack of improvement when using a Cauchy proposal for \u03b2 (MwGRepCau)\nsuggests that this inefficiency is not due to poor exploration of the potentially heavy tailed\n\u03b2. Further investigation in the (\u03c8, q, \u03b1, \u03b2) parameter space showed that for data set D1\nonly q was explored efficiently; the posteriors of \u03c8 and \u03b2 were strongly positively correlated\n(\u03c1 \u2248 0.8), and both \u03c8 and \u03b2 were strongly negatively correlated with \u03b1 (\u03c1 \u2248 \u22120.65).\n37\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\n0 1000 2000 3000 4000\n10\n12\n14\nIndex\nla\nm\n1\n0 1000 2000 3000 4000\n15\n20\n25\n30\nIndex\nla\nm\n2\n0 1000 2000 3000 4000\n28\n32\n36\n40\nIndex\nla\nm\n3\n0 1000 2000 3000 4000\n\u2212\n2.\n0\n\u2212\n1.\n0\n0.\n0\nIndex\n0 1000 2000 3000 4000\n\u2212\n2.\n5\n\u2212\n1.\n5\n\u2212\n0.\n5\nIndex\n0 1000 2000 3000 4000\n\u2212\n1.\n0\n0.\n0\n0.\n5\nIndex\n0 1000 2000 3000 4000\n\u2212\n3.\n0\n\u2212\n1.\n5\n0.\n0\nIndex\n0 1000 2000 3000 4000\n\u2212\n1.\n5\n\u2212\n0.\n5\n0.\n5\nIndex\n0 1000 2000 3000 4000\n\u2212\n1.\n5\n\u2212\n0.\n5\n0.\n5\nIndex\nlo\ng\n(q\n1\n2\n)\nlo\ng\n(q\n1\n3\n)\nlo\ng\n(q\n2\n1\n)\nlo\ng\n(q\n2\n3\n)\nlo\ng\n(q\n3\n1\n)\nlo\ng\n(q\n3\n2\n)\nFigure 7: Trace plots for the first 4000 iterations of the first replicate of BlkAdpMul(b) on data\nset D3.\n38\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nPosterior correlations were small |\u03c1| < 0.3 for all parameters with data set D2 and for all\ncorrelations involving q for data set D1.\nThe optimal scaling for the one-dimensional additive Cauchy proposal in MwGRepCau was\napproximately two thirds of the optimal scaling for the one-dimensional additive Gaussian\nproposal in MwGRep. In four dimensions the ratio was approximately one half. These ratios\nallow the Cauchy proposals to produce similar numbers of small to medium sized jumps to\nthe Gaussian proposals.\n5 Discussion\nWe have described the theory and intuition behind a number of techniques for improving the\nefficiency of random walk Metropoplis algorithms and tested these on two data sets generated\nfrom Markov modulated Poisson processes (MMPPs). Some implementations were uniformly\nsuccessful at improving efficiency, whilst for other\u2019s success depended on the shape and\/or\ntails of the posterior. All of the underlying concepts discussed here are quite general and\neasily applied to statistical models other than the MMPP.\nSimple acceptance rate tuning to obtain the optimal overall variance term for a symmetric\nGaussian proposal can increase efficiency by many orders of magnitude. However with our\ndata sets, even after such tuning, the RWM algorithm was very inefficient. The effectiveness\nof the sampling increased enormously once the shape of the posterior was taken into account\nby proposing from a Gaussian with variance proportional to an estimate of the posterior\nvariance. For Algorithms 3, 4 and 5 the posterior variance was estimated though a short\n\u201ctraining run\u201d - the first 1000 iterations after burn-in of Algorithm 1.\nAs expected, use of the \u201cmultiplicative random walk\u201d (Algorithm 5), a random walk on\nthe posterior of the logarithm of the parameters, improved efficiency most noticeably on\n39\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nthe posterior with the heavier tails. However, contrary to expectation, even on the heavier\ntailed posterior an additive Cauchy proposal (Algorithm 4) was, if anything, less efficient\nthan a Gaussian. Tuning of Cauchy proposals was also more time-consuming since simple\nacceptance rate criteria could not be used.\nAlgorithm 6 combined the successesful strategies of optimal scaling, shape tuning, and trans-\nforming the data, to create a multiplicative random walk which learned the most efficient\nshape and scale parameters from its own history as it progressed. This adaptive scheme was\neasy to implement and was arguably the most efficient algorithm for each of the data sets.\nA slight variant of this algorithm was used to explore the posterior of a three-dimensional\nMMPP and showed that in higher dimensions, such algorithms do take longer to discover\nclose to optimal values for the adaptive parameters. These runs also confirmed the finding\nfor the two dimensional MMPP that RWM efficiency improves enormously with knowledge\nof the posterior variance, even if this knowledge is only approximate. For a multimodal\nposterior such as that found for the three-dimensional MMPP it might be argued that a dif-\nferent variance matrix should be used for each mode. Such \u201cregionally adaptive\u201d algorithms\npresent additional problems, such as the definition of the different regions, and are discussed\nfurther in Roberts and Rosenthal (2009).\nMetropolis within Gibbs updates performed better when the parameters were close to or-\nthogonal, at which point they were almost as efficient as an equivalent block updated with\ntuned shape matrix. The best Metropolis within Gibbs scheme for data set D2 arose from a\nnew reparameterisation devised specifically for the two dimensional MMPP with parameter\northogonality in mind. On D2 this performed nearly as well as the best scheme, the adaptive\nmultiplicative random walk.\nThe adaptive schemes discussed here provide a significant step towards a goal of completely\nautomated algorithms. However, as already discussed, for d model-parameters, a posterior\nvariance matrix has O(d2) components. Hence the length of any \u201ctraining run\u201d or of the\n40\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nadaptive \u201clearning period\u201d increases quickly with dimension. For high dimension it is there-\nfore especially important to utilise to the full any problem specific knowledge that is available\nso as to provide as efficient a starting algorithm as possible.\nReferences\nBai, Y., Roberts, G. O. and Rosenthal, J. S. (2009). On the containment condition for\nadaptive Markov chain Monte Carlo algorithms. Submitted Preprint.\nBe\u00b4dard, M. (2007). Weak convergence of Metropolis algorithms for non-i.i.d. target distri-\nbutions. Ann. Appl. Probab. 17(4), 1222\u20131244.\nBe\u00b4dard, M. (2008). Optimal acceptance rates for Metropolis algorithms: moving beyond\n0.234. Stochastic Process. Appl. 118(12), 2198\u20132222.\nBurzykowski, T., Szubiakowski, J. and Ryden, T. (2003). Analysis of photon count data\nfrom single-molecule fluorescence experiments. Chemical Physics 288, 291\u2013307.\nCarlin, B. P. and Louis, T. A. (2009). Bayesian methods for data analysis . Texts in Statistical\nScience Series, CRC Press, Boca Raton, FL, 3rd edition.\nDellaportas, P. and Roberts, G. O. (2003). An introduction to MCMC. In: Spatial Statistics\nand Computational Methods (ed. J. Moller), number 173 in Lecture Notes in Statistics,\nSpringer, Berlin, 1\u201341.\nFearnhead, P. and Sherlock, C. (2006). An exact Gibbs sampler for the Markov modulated\nPoisson processes. J. R. Stat. Soc. Ser. B Stat. Methodol. 68(5), 767\u2013784.\nGamerman, D. and Lopes, H. F. (2006). Markov chain Monte Carlo. Texts in Statistical\nScience Series, Chapman & Hall\/CRC, Boca Raton, FL, 2nd edition, stochastic simulation\nfor Bayesian inference.\n41\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nGeyer, C. J. (1992). Practical Markov chain Monte Carlo. Statistical Science 7, 473\u2013483.\nGilks, W. R., Richardson, S. and Spiegelhalter, D. J. (1996). Markov Chain Monte Carlo in\npractice. Chapman and Hall, London, UK.\nHaario, H., Saksman, E. and Tamminen, J. (2001). An adaptive metropolis algorithm.\nBernoulli 7(2), 223\u2013242.\nJarner, S. F. and Roberts, G. O. (2002). Polynomial convergence rates of Markov chains.\nAnn. Appl. Probab. 12(1), 224\u2013247.\nKou, S. C., Xie, X. S. and Liu, J. S. (2005). Bayesian analysis of single-molecule experimental\ndata. Appl. Statist. 54, 1\u201328.\nMetropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H. and Teller, E. (1953).\nEquations of state calculations by fast computing machine. J. Chem. Phys. 21, 1087\u20131091.\nMeyn, S. P. and Tweedie, R. L. (1993). Markov chains and stochastic stability . Communi-\ncations and Control Engineering Series, Springer-Verlag London Ltd., London.\nNeal, P. and Roberts, G. (2006). Optimal scaling for partially updating MCMC algorithm.\nAnn. Appl. Probab. 16, 475\u2013515.\nRoberts, G. O. (2003). Linking theory and practice of MCMC. In: Highly structured stochas-\ntic systems , volume 27 of Oxford Statist. Sci. Ser., Oxford Univ. Press, Oxford, 145\u2013178,\nwith part A by Christian P. Robert and part B by Arnoldo Frigessi.\nRoberts, G. O. and Rosenthal, J. S. (1997). Geometric ergodicity and hybrid Markov chains.\nElectron. Comm. Probab. 2, no. 2, 13\u201325 (electronic).\nRoberts, G. O. and Rosenthal, J. S. (2001). Optimal scaling for various Metropolis-Hastings\nalgorithms. Statistical Science 16, 351\u2013367.\nRoberts, G. O. and Rosenthal, J. S. (2007). Coupling and ergodicity of adaptive Markov\nchain Monte Carlo algorithms. J. Appl. Probab. 44(2), 458\u2013475.\n42\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nRoberts, G. O. and Rosenthal, J. S. (2009). Examples of adaptive MCMC. J. Comp. Graph.\nStat. To appear.\nRoberts, G. O., Gelman, A. and Gilks, W. R. (1997). Weak convergence and optimal scaling\nof random walk Metropolis algorithms. The Annals of Applied Probability 7, 110\u2013120.\nScott, S. L. and Smyth, P. (2003). The Markov modulated Poisson process and Markov\nPoisson cascade with applications to web traffic modelling. Bayesian Statistics 7, 1\u201310.\nSherlock, C. (2005). In discussion of \u2019Bayesian analysis of single-molecule experimental data\u2019.\nJournal of the Royal Statistical Society, Series C 54, 500.\nSherlock, C. (2006). Methodology for inference on the Markov modulated Poisson process\nand theory for optimal scaling of the random walk Metropolis. Ph.D. thesis, Lancaster\nUniversity, available from http:\/\/eprints.lancs.ac.uk\/850\/.\nSherlock, C. and Roberts, G. O. (2009). Optimal scaling of the random walk Metropolis on\nelliptically symmetric unimodal targets. Bernoulli To appear.\nSokal, A. (1997). Monte Carlo methods in statistical mechanics: foundations and new algo-\nrithms. In: Functional integration (Carge`se, 1996), volume 361 of NATO Adv. Sci. Inst.\nSer. B Phys., Plenum, New York, 131\u2013192.\nA Convergence rates, eigenfunctions, and intuition\nTo avoid technical details we present theory in a simplified framework where the MCMC\nkernels have discrete spectra, and consider only distributions for which the L2 norm resulting\nfrom the inner product (14) exists. We first motivate (14).\nProposition 1 Let P (x, dx\u2032) be a reversible kernel with stationary distribution \u03c0(\u00b7), eigen-\nfunctions ei(\u00b7), and associated eigenvalues \u03b2i. All of the \u03b2i are real, and with the inner\n43\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nproduct defined in (14), < ei(\u00b7), ej(\u00b7) >= \u03b4ij.\nProof: Define\nS(x, dx\u2032) :=\n(\n\u03c0(x)\n\u03c0(x\u2032)\n)1\/2\nP (x, dx\u2032).\nSince P is reversible,\n\u03c0(x)P (x, dx\u2032) = \u03c0(x\u2032)P (x\u2032, dx).\nDivide both sides by (\u03c0(x)\u03c0(x\u2032))1\/2 to see that S(x, dx\u2032) = S(x\u2032, dx). Thus S is sym-\nmetric and consequently has real eigenvalues \u03b2i and associated eigenfunctions e\n\u2217\ni (\u00b7) with\u222b\ndx e\u2217i (x) e\n\u2217\nj (x) = \u03b4ij. Now for any i,\n\u03b2i e\n\u2217\ni (x\n\u2032) =\n\u222b\ndx e\u2217i (x) S(x, dx\n\u2032) =\n\u222b\ndx e\u2217i (x)\n\u03c0(x)1\/2\n\u03c0(x\u2032)1\/2\ndP (x,x\u2032).\nThus ei := \u03c0\n1\/2 e\u2217i is an eigenfunction of P with eigenvalue \u03b2i. Further\n\u03b4ij =\n\u222b\ndx e\u2217i (x) e\n\u2217\nj (x) =\n\u222b\ndx\nei(x) ej(x)\n\u03c0\n=< ei, ej > .\nWe next motivate the idea of geometric ergodicity and show that a geometric rate of conver-\ngence is given by the second largest eigenvalue, provided its value is strictly less than one.\nWe employ the shorthand notation for measure \u03c1 and kernel P , \u03c1P :=\n\u222b\ndx \u03c1(x)P (x, \u00b7).\nProposition 2 Let P be a reversible kernel with stationary distribution \u03c0, eigenvalues \u03b2i\nwith 1 = \u03b21 \u2265 \u03b22 \u2265 \u03b23, . . . . For initial density \u03c1,\n||\u03c1P \u2212 \u03c0||2 \u2264 \u03b22 ||\u03c1\u2212 \u03c0||2 .\nProof: Write\n\u03c1(\u00b7) =\n\u221e\u2211\ni=1\nai ei(\u00b7)\nand note that, since e1 = \u03c0, a1 =< \u03c1, e1 >= 1. Thus\n||\u03c1\u2212 \u03c0||2 =\n\u2223\u2223\u2223\u2223\u2223\n\u2223\u2223\u2223\u2223\u2223\n\u221e\u2211\n2\nai ei\n\u2223\u2223\u2223\u2223\u2223\n\u2223\u2223\u2223\u2223\u2223\n2\n=\n(\n\u221e\u2211\n2\na2i\n)1\/2\n.\n44\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nBut \u03c1 P =\n\u2211\u221e\n1 ai \u03b2iei and so\n||\u03c1P \u2212 \u03c0||2 =\n\u2223\u2223\u2223\u2223\u2223\n\u2223\u2223\u2223\u2223\u2223\n\u221e\u2211\n2\nai \u03b2i ei\n\u2223\u2223\u2223\u2223\u2223\n\u2223\u2223\u2223\u2223\u2223\n2\n\u2264 \u03b22\n\u2223\u2223\u2223\u2223\u2223\n\u2223\u2223\u2223\u2223\u2223\n\u221e\u2211\n2\nai ei\n\u2223\u2223\u2223\u2223\u2223\n\u2223\u2223\u2223\u2223\u2223\n2\n= \u03b22\n(\n\u221e\u2211\n2\na2i\n)1\/2\n.\nNote that\n\u2223\u2223\u2223\u2223\u03c1P k \u2212 \u03c0\u2223\u2223\u2223\u2223\n2\n=\n\u2223\u2223\u2223\u2223(\u03c1P k\u22121)P \u2212 \u03c0\u2223\u2223\u2223\u2223\n2\n\u2264 \u03b22\n\u2223\u2223\u2223\u2223\u03c1P k\u22121 \u2212 \u03c0\u2223\u2223\u2223\u2223\n2\n. Iterating this procedure,\nwe find that\n\u2223\u2223\u2223\u2223\u03c1P k \u2212 \u03c0\u2223\u2223\u2223\u2223\n2\n\u2264 \u03b2k2 ||\u03c1\u2212 \u03c0||2.\nWe finally consider two reversible kernels with the same stationary distribution, and apply\nthem sequentially.\nProposition 3 Let reversible kernels A(1) and A(2) both have stationary distribution \u03c0(\u00b7),\nand denote their second largest eigenvalues as \u03b2\n(1)\n2 and \u03b2\n(2)\n2 respectively. Let A\n\u2217 be a combi-\nnation algorithm which alternates iterations from A(1) and A(2). Then\n||\u03bdA\u2217 \u2212 \u03c0||2 \u2264 \u03b2(1)2 \u03b2(2)2 ||\u03bd \u2212 \u03c0||2 .\nProof: First decompose the eigenfunctions of A(1) in terms of the eigenfunctions of A(2):\ne\n(1)\ni =\n\u221e\u2211\nj=1\ncije\n(2)\nj ,\nwhere cij =< e\n(1)\ni , e\n(2)\nj >. Denote the remaining eigenvalues of A\n(1) and A(2) by \u03b2\n(1)\ni and \u03b2\n(2)\ni\nand expand \u03c1 in terms of the eigenfunctions of A(1) to obtain\n\u03c1A\u2217 =\n\u221e\u2211\ni=1\nai\u03b2\n(1)\ni e\n(1)\ni A\n(2) = \u03c0 +\n\u221e\u2211\ni=2\nai\u03b2\n(1)\ni e\n(1)\ni A\n(2) = \u03c0 +\n\u221e\u2211\ni=2\nai\u03b2\n(1)\ni\n\u221e\u2211\nj=2\ncij\u03b2\n(2)\nj e\n(2)\nj .\nTherefore\n||\u03c1A\u2217 \u2212 \u03c0||2 =\n\u2223\u2223\u2223\u2223\u2223\n\u2223\u2223\u2223\u2223\u2223\n\u221e\u2211\ni=2\nai\u03b2\n(1)\ni\n\u221e\u2211\nj=2\ncij\u03b2\n(2)\nj e\n(2)\nj\n\u2223\u2223\u2223\u2223\u2223\n\u2223\u2223\u2223\u2223\u2223\n2\n\u2264 \u03b2(1)2 \u03b2(2)2\n\u2223\u2223\u2223\u2223\u2223\n\u2223\u2223\u2223\u2223\u2223\n\u221e\u2211\ni=2\nai\n\u221e\u2211\nj=2\ncije\n(2)\nj\n\u2223\u2223\u2223\u2223\u2223\n\u2223\u2223\u2223\u2223\u2223\n2\n= \u03b2\n(1)\n2 \u03b2\n(2)\n2 ||\u03bd \u2212 \u03c0||2 .\nRepeated application of this result leads to:\n\u2223\u2223\u2223\u2223\u03bdA\u2217k \u2212 \u03c0\u2223\u2223\u2223\u2223\n2\n\u2264\n(\n\u03b2\n(1)\n2 \u03b2\n(2)\n2\n)k\n||\u03bd \u2212 \u03c0||2.\n45\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nB Reparameterisation for the 2 dimensional MMPP\nA Taylor expansion of the log-likelihood of a two-dimensional MMPP with \u03c81 \u2248 \u03c82 was\ngiven in Section 3.7. The derivation is sketched in this appendix and further details of the\n(\u03c8, q, \u03b1, \u03b2) reparameterisation are provided. For a fuller derivation the reader is referred to\nSherlock (2006).\nFor a two dimensional MMPP with stationary distribution [\u03bd1, \u03bd2]\nt, first reparameterise to\n(\u03c8,\u03a8\u2217, q,Q\u2217) with\n\u03c8 = \u03bdt\u03c8 , \u03a8 = \u03c8(I+\u03a8\u2217) , q = q12 + q21 , Q\n\u2217 = \u22121\nq\nQ =\n\uf8ee\n\uf8f0 \u03bd2 \u2212\u03bd2\n\u2212\u03bd1 \u03bd1\n\uf8f9\n\uf8fb .\nWith this reparameterisation\ne(Q\u2212\u03a8)ti = e\u2212\u03c8tie\u2212(Q\n\u2217qti+\u03a8\n\u2217\n\u03c8ti)\nand therefore\nL(Q,\u03a8, t) = \u03c8\nn\ne\u2212\u03c8tobs\u03bdte\u2212(Q\n\u2217qt1+\u03a8\n\u2217\n\u03c8t1)(I+\u03a8\u2217) . . .\n. . . e\u2212(Q\n\u2217qtn+\u03a8\n\u2217\n\u03c8tn)(I+\u03a8\u2217)e\u2212(Q\n\u2217qtn+1+\u03a8\n\u2217\n\u03c8tn+1)1.\nBut\ne\u2212(Q\n\u2217qti+\u03a8\n\u2217\n\u03c8ti) = I\u2212 (Q\u2217qti +\u03a8\u2217\u03c8ti) + 1\n2\n(Q\u2217qti +\u03a8\n\u2217\u03c8ti)\n2 + . . . .\nExpand the likelihood in terms of \u03a8\u2217 and for notational simplicity, temporarily ignore the\nfactor \u03c8\nn\ne\u2212\u03c8tobs and products of powers of \u03c8ti and qti. Since Q\n\u2217n = Q\u2217, terms in \u03a8\u2217, (\u03a8\u2217)2,\nand (\u03a8\u2217)3 are then multiples respectively of\n\u03bdtQ\u2217a1\u039b\u2217Q\u2217a21 , \u03bdtQ\u2217b1\u039b\u2217Q\u2217b2\u039b\u2217Q\u2217b31 , and \u03bdtQ\u2217c1\u039b\u2217Q\u2217c2\u039b\u2217Q\u2217c2\u039b\u2217Q\u2217c41\nwith a1, a2, b1, b2, b3, c1, c2, c3, c4 all either 0 or 1. From their definitions\n\u03bdtQ = Q1 = \u03bdt\u039b\u22171 = 0\n46\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nand so to third order the only non vanishing terms are quadratic terms with b1 = b3 = 0\nand cubic terms with c1 = c4 = 0. Further \u039b\n\u22171 = \u03b4[\u2212\u03bd2, \u03bd1]t is a right eigenvector of Q\u2217\nand \u03bdt\u039b\u2217 = \u03b4[\u03bd1, \u03bd2] is a left eigenvector of Q\n\u2217, both with eigenvalues 1. Hence in the\nabove products the remaining powers of Q\u2217 have no effect: both quadratic terms evaluate\nto \u03b42\u03bd1\u03bd2, and all cubic terms evaluate to \u03b4\n3\u03bd1\u03bd2(\u03bd2\u2212\u03bd1). To cubic terms in \u03b4, the likelihood\nis therefore\nL(\u03c8, q, \u03b4, \u03bd1) \u2248 \u03c8ne\u2212\u03c8tobs\n(\n1 + 2\u03b42\u03bd1\u03bd2f(\u03c8t, qt) + \u03b4\n3\u03bd1\u03bd2(\u03bd2 \u2212 \u03bd1)g(\u03c8t, qt)\n)\nwhere f(\u00b7, \u00b7) and g(\u00b7, \u00b7) are the sums of the many product terms in the expansion of the\nlikelihood involving respectively two and three occurences of \u039b\u2217. Equation (17) follows\ndirectly after a final Taylor expansion.\nViewed in terms of the original parameters, the transformation given in Section 3.7 is\n\u03c8 :=\nq21\u03bb1 + q12\u03bb2\nq12 + q21\n, q := q12+q21 , \u03b1 := 2\n(\u03bb2 \u2212 \u03bb1)(q12q21)1\/2\nq21\u03bb1 + q12\u03bb2\nand \u03b2 :=\n(\u03bb2 \u2212 \u03bb1)(q12 \u2212 q21)\nq21\u03bb1 + q12\u03bb2\n.\nIts Jacobian is\n\u2202(\u03c8, q, \u03b1, \u03b2)\n\u2202(\u03bb1, \u03bb2, q12, q21)\n=\n|\u03bb2 \u2212 \u03bb1|(q12 + q21)2\n(q21\u03bb1 + q12\u03bb2)2(q12q21)1\/2\n.\nC Runs with highly variable ACTs\nThree replicates were performed for each data set and algorithm, and ACTs are summarised\nby their mean in Table 1. However for certain algorithms and data sets the ACTs varied\nconsiderably; full sets of ACTs for these replicates are given in Table 2.\n47\nCRiSM Paper No. 09-16, www.warwick.ac.uk\/go\/crism\nAlgorithm \u03c81 \u03c82 log (q12) log (q21)\nBlk (D1) 59,64,75 120,155,104 12,15,17 19,21,17\nBlkShpCau (D1) 28,16,12 36,29,31 20,20,35 26,23,24\nBlk (D2) 121,259,146 107,262,157 41,139,61 51,110,48\nBlkShp (D2) 54,51,34 23,24,29 40,45,27 50,35,23\nBlkShpCau (D2) 92,51,46 48,57,46 94,42,31 35,41,39\nBlkShpMul (D2) 53,24,23 22,33,25 20,23,24 17,18,13\nTable 2: Estimated integrated autocorrelation time for the four parameters, on three inde-\npendent replicates for Blk and BlkShpCau on data set D1 and Blk, BlkShp, BlkShpCau and\nBlkShpMul on data set D2.\n48\n"}