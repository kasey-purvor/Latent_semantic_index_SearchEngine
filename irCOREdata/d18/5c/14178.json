{"doi":"10.1080\/0968776042000259537","coreId":"14178","oai":"oai:generic.eprints.org:607\/core5","identifiers":["oai:generic.eprints.org:607\/core5","10.1080\/0968776042000259537"],"title":"Putting interoperability to the test: building a large reusable assessment item bank","authors":["Sclater, Niall","MacDonald, Mary"],"enrichments":{"references":[{"id":196607,"title":"Question and test interoperability specification v1.2, IMS Global Learning Consortium. Available online at: www.imsglobal.org","authors":[],"date":"2002","doi":null,"raw":"IMS (2002) Question and test interoperability specification v1.2, IMS Global Learning Consortium. Available online at: www.imsglobal.org McCallon, E. L. & Schumacker, R. E. (2002) Developing and maintaining an item bank, ELM Metrics Inc. Available online at: www.elmmetrics.com McAlpine, M. (2002) Design requirements of a databank, CAA centre. Available online at: www.caacentre.ac.uk Sclater, N (2003) TOIA-COLA Metadata application profile v1.2. Available online at: www.cetis.ac.uk\/profiles\/uklomcore\/ SQA (2003) Guidelines on online assessment for further education. Available online at: www.sqa.org.uk Ward, A. W. & Murray-Ward, M. (1994) Guidelines for the development of item banks, Educational measurement: issues and practice (National Council on Measurement in Education).","cites":null},{"id":196606,"title":"Special Interest Group. Available online at: assessment.cetis.ac.uk COLEG. Available online at: www.coleg.org.uk e3an project. Available online at:","authors":[],"date":"2002","doi":null,"raw":"CETIS Assessment Special Interest Group. Available online at: assessment.cetis.ac.uk COLEG. Available online at: www.coleg.org.uk e3an project. Available online at: www.e3an.ac.uk HELM project. Available online at: www.lboro.ac.uk\/research\/helm\/ IEEE (2002) Standard for Learning Object Metadata, IEEE 1484, 12.1-2002.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2004","abstract":"The COLA project has been developing a large bank of assessment items for units across the Scottish further education curriculum since May 2003. These will be made available to learners mainly via colleges virtual learning environments (VLEs). Many people have been involved in the development of the COLA assessment item bank to ensure a high level of technical and pedagogical quality. Processes have included deciding on appropriate item types and subject areas, training authors, peer-reviewing and quality assuring the items and assessments, and ensuring they are tagged with appropriate metadata. One of the biggest challenges has been to ensure that the assessments are deliverable across the four main virtual learning environments in use in Scottish colleges-and also through a stand-alone assessment system. COLA is significant because no other large project appears to have successfully developed standards-compliant assessment content for delivery across multiple VLEs. This paper discusses how COLA has dealt with the organizational, pedagogical and technical issues which arise when commissioning items from many authors for delivery across an educational sector","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/14178.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/607\/1\/ALT_J_Vol12_No3_2004_Putting%20interoperability%20to%20th.pdf","pdfHashValue":"44dfaad40c385cb2f798249cf857ff892a83b0b5","publisher":"Taylor and Francis Ltd","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:607<\/identifier><datestamp>\n      2011-04-04T09:05:52Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/607\/<\/dc:relation><dc:title>\n        Putting interoperability to the test: building a large reusable assessment item bank<\/dc:title><dc:creator>\n        Sclater, Niall<\/dc:creator><dc:creator>\n        MacDonald, Mary<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        The COLA project has been developing a large bank of assessment items for units across the Scottish further education curriculum since May 2003. These will be made available to learners mainly via colleges virtual learning environments (VLEs). Many people have been involved in the development of the COLA assessment item bank to ensure a high level of technical and pedagogical quality. Processes have included deciding on appropriate item types and subject areas, training authors, peer-reviewing and quality assuring the items and assessments, and ensuring they are tagged with appropriate metadata. One of the biggest challenges has been to ensure that the assessments are deliverable across the four main virtual learning environments in use in Scottish colleges-and also through a stand-alone assessment system. COLA is significant because no other large project appears to have successfully developed standards-compliant assessment content for delivery across multiple VLEs. This paper discusses how COLA has dealt with the organizational, pedagogical and technical issues which arise when commissioning items from many authors for delivery across an educational sector.<\/dc:description><dc:publisher>\n        Taylor and Francis Ltd<\/dc:publisher><dc:date>\n        2004<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/607\/1\/ALT_J_Vol12_No3_2004_Putting%20interoperability%20to%20th.pdf<\/dc:identifier><dc:identifier>\n          Sclater, Niall and MacDonald, Mary  (2004) Putting interoperability to the test: building a large reusable assessment item bank.  Association for Learning Technology Journal, 12 (3).  pp. 205-213.  ISSN 0968-7769 (print)\/1741-1629 (online)     <\/dc:identifier><dc:relation>\n        10.1080\/0968776042000259537<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/607\/","10.1080\/0968776042000259537"],"year":2004,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"ALT-J, Research in Learning Technology\nVol. 12, No. 3, September 2004\nISSN 0968\u20137769 (print)\/ISSN 1741\u20131629 (online)\/04\/030205\u201309\n\u00a9 2004 Association for Learning Technology\nDOI: 10.1080\/0968776042000259537\nPutting interoperability to the test: \nbuilding a large reusable assessment \nitem bank\nNiall Sclatera* & Mary MacDonaldb\naUniversity of Strathclyde, UK;  bColleges Open Learning Exchange Group (COLEG)\nTaylor and Francis LtdCALT120302.sgm10.1080\/ 968776042000259537ALT-J, Research in Learning Technology0968 7769 (pri t)\/1741-1629 (onli e)Original Article2 04ssoci tion for Learning Techno ogy23 000Sept mber 20 4NiallSc aternial .s l t @strath.ac.uk\nThe COLA project has been developing a large bank of assessment items for units across the\nScottish further education curriculum since May 2003. These will be made available to learners\nmainly via colleges\u2019 virtual learning environments (VLEs). Many people have been involved in the\ndevelopment of the COLA assessment item bank to ensure a high level of technical and pedagogical\nquality. Processes have included deciding on appropriate item types and subject areas, training\nauthors, peer-reviewing and quality assuring the items and assessments, and ensuring they are\ntagged with appropriate metadata. One of the biggest challenges has been to ensure that the\nassessments are deliverable across the four main virtual learning environments in use in Scottish\ncolleges\u2014and also through a stand-alone assessment system. COLA is significant because no other\nlarge project appears to have successfully developed standards-compliant assessment content for\ndelivery across multiple VLEs. This paper discusses how COLA has dealt with the organizational,\npedagogical and technical issues which arise when commissioning items from many authors for\ndelivery across an educational sector.\nIntroduction\nVarious people have attempted to provide a definition of an item bank. These range\nfrom a simple: \ncollection of text items that may be easily accessed for use in preparing exams (Ward &\nMurray-Ward, 1994)\nto the more detailed but less generic: \ncollection of test items that can be readily accessed for use in preparing examinations \u2026\nnormally computerized for ease of item storage and to facilitate the generation of new tests.\nEach item \u2026 is coded according to competency area and instructional objective, as well as\n*Corresponding author: Learning Services, 155 George Street, University of Strathclyde, Glasgow,\nUK. Email: niall.sclater@strath.ac.uk\n206 N. Sclater & M. MacDonald\nempirically derived data such as measures of item difficulty and discrimination. (McCallon\n& Schumacker, 2002)\nSome of these definitions incorporate the concept of different but equivalent assess-\nments being produced dynamically and automatically for each learner from a bank of\nitems. Others imply that the database will be used to store data about the usage of the\nitems by learners. All the definitions suggest that items should be classified by\ndescriptive data (metadata) of some sort to enable them to be located. The decisions\ntaken over what type of metadata to use as well as the content and type of items will\ndifferentiate one item bank from another.\nWhile item banks have been around for many years and in a range of contexts,\nvarious factors are now coming together which suggest that their use is set to increase\nconsiderably. Firstly the software is now available and the hardware ubiquitous\nenough to deliver assessments to learners either through virtual learning environments\nor bespoke online assessment systems. Secondly there is an internationally-recognized\nformat for the transfer of items between these systems (IMS, 2002). This format can\nalso be used to store items in a database separately from any proprietary assessment\ndelivery system. Thirdly there are now pressing economic and political imperatives\nfor the development of national and international item banks.\nDeveloping items and assessments across a subject area or sector can bring\neconomies of scale in the development process and a considerable reduction in\nduplication of effort in different colleges and universities. The quality of items which\nare peer reviewed and validated centrally is likely to be higher than those developed\non an ad-hoc basis in an individual institution. An increased adherence to technical\nstandards should mean that the lifespan of items is prolonged and that items are\nmore likely to be deliverable through a variety of assessment systems and virtual\nlearning environments.\nThere are already some successful examples of item banks under development.\nThese tend to be either: (1) assessment-specific, e.g. the English as a Foreign Language\nitem bank for the University of Cambridge Local Examination Syndicate; or (2)\nsubject-specific, e.g. the Electronics and Electrical Engineering Assessment Network\n(e3an), Helping Engineers Learn Mathematics (HELM) and various initiatives taken\nby the Learning and Teaching Support Networks which are developing items in\neconomics and computing.\nHowever, a third type of item bank is now emerging: sectoral. The COLA\n(COLEG OnLine Assessment) project is developing a large bank of items across the\nentire Scottish further education (FE) curriculum. A simple definition of an item\nbank which incorporates these three types might be: a collection of items for a\nparticular assessment, subject or educational sector, classified by metadata which\nfacilitates searching or automated test creation.\nManagement of the COLA project\nThe COLA project was established with funding from the Scottish Further Education\nFunding Council (SFEFC) which identified online assessment as a strategic priority.\nA large reusable assessment item bank 207\nMost Scottish colleges had already deployed virtual learning environments (VLEs),\nproviding new opportunities for online learning. Feedback from the sector showed\nthat the lack of a national database of assessment instruments was proving a barrier\nto widespread use of the VLEs for assessment purposes. In addition the Scottish\nQualifications Agency (SQA) had recently produced a set of guidelines for the use of\nonline assessment (SQA, 2003) and was developing a strategy in this area. The\nFunding Council believed that the use of online assessment could reduce the burden\non academic staff and encourage more of them to engage with information and\ncommunications technology for learning and teaching.\nCOLA\u2019s aim was to develop a bank of high quality assessment instruments capa-\nble of being delivered through the four main VLEs in use in Scottish colleges in a\nwide range of courses at all levels within further education (FE). The project is\nmanaged by the Colleges Open Learning Exchange Group (COLEG)\u2014a partner-\nship of 42 Scottish colleges which undertakes collaborative projects to develop,\nexchange and promote open, flexible and online learning materials. COLEG\nmanages each project while college staff write, produce and peer review the materi-\nals and quality assurance staff check them through a rigorous quality assurance\nprocess before dissemination to the sector. COLEG has used the same approach for\nthe COLA project.\nA steering group was formed to oversee the project, which includes representation\nfrom the various agencies, senior FE managers and FE practitioners with expertise\nand experience in online assessment, VLEs, interoperability issues and staff\ndevelopment. The project team includes a project manager, an administrator, a tech-\nnical consultant with expertise in online assessment and interoperability, a technical\nadvisor experienced in online assessment and a staff developer. A technical advisory\ngroup was also appointed from college staff with substantial experience in on-line\nassessment and expertise in VLEs and interoperability issues. This group has strong\nlinks with the CETIS Assessment Special Interest Group and IMS, the international\nbody responsible for assessment interoperability specifications.\nSelecting areas for assessment\nThe prime aim of COLA is to provide a bank of assessment instruments to encourage\nmore widespread use of VLEs by college staff across the curriculum. Awareness of the\nproject was raised through local subject networks and staff were encouraged to put\nforward their suggestions for areas of the curriculum that would be appropriate for\nonline assessment. As a starting point for selecting areas of the curriculum, staff were\nasked to focus on learning outcomes within SQA units that would be appropriate for\nobjective testing. In practice subject specialists created assessments to meet the\nformative assessment requirements of complete outcomes, parts of outcomes\n(performance criteria) or a combination of topics (performance criteria) from several\noutcomes.\nIt was recommended that an assessment should contain a maximum of twenty\nitems in total. There was a general view among academic staff that twenty multiple\n208 N. Sclater & M. MacDonald\nchoice items would normally cover the formative assessment requirements of one\nSQA outcome.\nChoosing item types\nThe project has concentrated on developing pedagogically sound objective tests,\nusing a limited number of item types. After consulting the e3an project team on the\nitem types they had selected for their item bank in engineering, the types chosen were\ntrue\/false, multiple choice, multiple response, fill in the blank and matching. There\nwere a number of reasons for selecting a limited range of item types. The wide range\nand spread of assessments would be limited by specifying a small number of item\ntypes, allowing the assessment of a variety of skills and cognitive levels. A focused\nprogramme of staff development could be provided for writers. The assessments had\nto work in a range of VLEs and it was expected that the VLEs would accept these item\ntypes if they were marked up using the IMS Question and Test Interoperability v1.2\n(QTI) specification (IMS, 2002).\nDevelopment of the templates\nTo simplify the process of item creation, standard Word templates were developed\nfor the college writers. This approach had already been used successfully by the e3an\nproject and it was expected that staff familiar with using Word would be able to input\ncontent to the templates easily. A template was created for each item type. Item\ntemplates allow authors to specify the stem of an item, the options and the correct\nanswer, to incorporate graphics in the stem and the options and to provide feedback\nfor each option. They also included a section for additional information such as the\nexpected time to be taken, a description of the item, keywords and the subject topic.\nIn addition an assessment template was developed to contain metadata about the\nassessment itself and to specify which items were contained in the assessment.\nMetadata\nAs the COLA item bank grows it will become increasingly important to provide an\nadequate means of identifying items and assessments. The provision of appropriate\nand accurate metadata makes this possible. There is now an international standard\nfor learning object metadata (LOM) published by the IEEE (IEEE, 2003) which was\nchosen as the format in which to store COLA metadata. If a COLA item or assess-\nment is uploaded to a VLE or content repository, the metadata should be instantly\nrecognized and allow users to search for the material on the metadata fields.\nIEEE LOM had never before been used to classify items and assessments.\nHowever, a group of UK experts has got together to produce an application profile (a\nkind of subset specifying mandatory and optional elements) of the LOM for use\nwithin UK further and higher education. This is known as the UK LOM Core. It\nseemed appropriate to utilize this application profile for COLA in order to maximize\nA large reusable assessment item bank 209\nthe chances of its metadata being understood by other systems. COLA worked with\nexperts in metadata and assessment to produce further application profiles of the UK\nLOM Core for items and assessments. Work done for the COLA project on metadata\nand content packaging has fed directly into v2.0 of the IMS Question and Test\nInteroperability specification.\nThe COLA templates allow authors to enter items and assessments and also to\ncomplete most of the metadata used to classify them. A template conversion tool\nwhich was built for the project ensures that metadata fields are transferred accurately\nand consistently from the templates to the LOM format, while automatically\ncompleting some of the more esoteric fields which authors might have found difficult\nto understand. This is a much better solution than giving authors access to a tool\nwhich requires them to understand the LOM format itself. It ensures that metadata\nacross the entire collection of COLA assessments and items has high levels of quality\nand consistency without creating an excessive burden on authors.\nEach item is classified by the Scottish Credit and Qualifications Framework\n(SCQF) level, a number from 1 to 8. Assessment-level classification metadata is\ndefined in a similar way to that of items. In addition to the level there are entries for\nthe SQA Outcome Number, the Performance Criteria, the Unit Number and the\nUnit Title.\nIdentifying, training and supporting assessment writers\nCOLEG used its standard approach to recruit writers, working through its network\nof contacts in the colleges to disseminate information about the project to staff and to\ninvite them to commit to the project. The project was launched with an awareness-\nraising event for college staff\u2014curricular, technical and management\u2014to explain the\naims of the project, timescales and funding arrangements and to listen to their views\non implementation.\nFollowing the event, colleges were asked to confirm the services that they could\nprovide to the project. Standard levels of payment were set for writing and inputting\nof the assessments into the templates, for peer reviewing, for quality assurance and\nfor project management. Writers confirmed on a proforma the curriculum areas\/\ntopics and peer reviewers of their assessments. At the same time technical staff with\nrelevant experience of the different VLEs were invited to join the technical advisory\ngroup for the project and to advise the steering group on technical issues.\nThereafter a series of two workshops was organized for writers and peer reviewers.\nThe workshops provided information about the project and clarified its focus on\nobjective tests. The various item and assessment templates and the item types chosen\nwere explained. A set of guidelines was created, including a writer\u2019s\/peer reviewer\u2019s\nquality checklist for each item type and for the assessment information and a guide to\ncompletion of the templates. Evaluation forms showed that the workshops were well-\nreceived by participants. Technical and pedagogical quality of the items is likely to be\nhigher than if they not been carried out. Certainly there would have been confusion\nabout the use of the templates. The writers also confirmed that the workshops helped\n210 N. Sclater & M. MacDonald\ntheir understanding of the pedagogy of objective tests not just their understanding of\nthe templates.\nA timescale of six weeks between May and June 2003 was set between the first\nwriters\u2019 workshop and the deadline for submission of the assessments by the writers.\nFollowing the workshops, one-to-one guidance on pedagogy related issues was\navailable. Email and telephone support was also available for both pedagogical and\ntechnical issues. A further series of one day workshops was held for a second phase of\ndevelopment work between July and September 2003. In total 66 writers delivered\n165 assessments (approximately 3000 items) in the first two phases of the project.\nOnly three writers withdrew from the project.\nQuality assurance\nCOLEG implemented its standard quality assurance procedures in the project, includ-\ning checking the quality of the items (from the subject specialist\u2019s and the learner\u2019s\nperspective), checking the quality of the production (grammar, typos) and checking\nthe technical aspects (e.g. completion of template fields, use of standard file names).\nIn checking the quality of the items from the learner\u2019s perspective the quality\nassurance staff identified several key issues: \n\u25cf what is to be assessed?\n\u25cf why has a particular item type been selected?\n\u25cf is the item or instruction (stem) clear?\n\u25cf is contextualization necessary and appropriate?\nOf the items created, 75% were considered to be of good quality. After further devel-\nopment work, it has been possible, with the exception of five assessments, to validate\nall the assessments in the first development phase. Where there were questions over\nthe quality of the assessments, the robustness of the peer review process was ques-\ntioned, particularly where the wording of an item was inappropriate or the item type\nused was not suitable.\nIt was felt that the quality of the feedback to the learner was particularly important\nfor assessments which would primarily be used formatively. However, the online\ncontext and the VLE technology sometimes limited the feedback that could be\nprovided. For some item types it was stated in the guidance that only standard (No,\nthis is incorrect or Yes, this is correct) feedback could be provided because it would\nbe impossible to predict the learner\u2019s responses to the items. In practice some writers\nproved to be extremely creative with the additional general feedback that they provided.\nIt was not possible to clarify some of the technical issues related to the templates at\nthe time of the workshops. In addition further issues were identified at the later stage\nof testing of the exemplar assessments. In both cases these were addressed at the\nquality assurance stage.\nIn the main it was felt that the writers had made a reasonable attempt to complete\nthe fields in the templates. The general view was that it was important for the writers\nto gain skills in data input and that this would give them a better understanding of\nA large reusable assessment item bank 211\nhow the VLEs would handle the assessments. It was also established that it would be\npossible to standardize more of the content in the template such as feedback. This\nwould reduce the potential for error.\nVersion control and file management has been an important issue during the\nquality assurance process. A file management system has been developed for the\nproject that classifies the assessments into three categories: \n\u25cf initial version: received from the writer following peer review;\n\u25cf part-validated version: quality assured but checking or amendment required by the\nwriter;\n\u25cf validated version: approved by the quality assurance staff.\nA spreadsheet has been developed to record details of writer, peer reviewer, subject\narea and level, quality assurance process and administrative details. Overall this\nsystem has worked well, though management and maintenance of the files has been\ntime-consuming and requires a great deal of care and attention to detail. In a small\nnumber of cases writers changed items that had been validated and these needed to\nbe rechecked. Wherever practical, writers have been asked to notify the quality\nassurance staff of the amendments that they want to make rather than changing the\ntemplates themselves.\nTransfer to QTI and VLE formats\nOne of the primary aims of COLA was to encourage colleges to use their VLEs by\nproviding online assessments which could be run from the VLEs. In order to do this,\nthe assessments had to be in a format which the VLEs would understand. The only\ninternational specification (not yet a standard) for the exchange of items and\nassessments is the IMS Question and Test Interoperability v1.2 (QTI) specification.\nMany vendors pay lip service to their products\u2019 compliance with this specification but\ndo not properly implement it. The four VLEs in use in Scottish colleges all claim\nsome level of compliance with the specification. COLA took the decision to store all\ncontent in this platform independent format which is undoubtedly increasing in\nuptake Worldwide.\nIt was necessary to develop a program to convert the items and assessments from\nthe Word templates to the QTI format. This task was carried out by the JISC-funded\nTechnologies for Online Interoperable Assessment (TOIA) project which had the\nnecessary expertise in QTI in collaboration with an expert group representing the four\nmain VLEs. There were many complications due to the different ways in which the\nVLEs interpreted the QTI specification and their limited implementations of some of\nthe item types. Using a third party product called Respondus which accepts QTI it is\nnow possible to transfer COLA content into WebCT and Blackboard. Teknical now\naccepts COLA content directly and it is still hoped that a solution can be found to\nput COLA items into Granada Learnwise.\nHaving produced the items and assessments to the correct standard, they can also\nbe uploaded and stored in some of the emerging learning content repositories with\n212 N. Sclater & M. MacDonald\nease, allowing items to be searched for on their metadata. In order for teachers to be\nable to search for items with ease the conversion tool creates two indexes which can\nbe read using Microsoft Excel\u2014one for items, the other for assessments. On each line\nof the spreadsheet is one item\/assessment and all the metadata associated with that\nitem such as author name and SQA Unit Number.\nDistribution\nMuch discussion took place in the technical advisory group meetings as to how to\ndistribute the items and assessments. While the distribution of CD-ROMs would\nhave provided a further opportunity to disseminate the project to colleges, the\ntechnology is a backward one and it was considered to be simpler for colleges to\ndownload the latest versions of the item bank and install them in their VLEs directly\nfrom a central website. The COLEG named contact in each college would be\nauthenticated to do so.\nSeparate indexes of all items and assessments will be provided on the website, both\nsearchable on any item of the metadata. Staff will then be able to download the items\nand assessments required in IMS QTI format so that they can import them into their\nVLE.\nConclusions\nThe templates were developed in Word for ease of use and overall writers have coped\nreasonably well with them. The development process has however highlighted a\nnumber of issues. There are limitations in the type of data input that the template will\nallow. It would be possible to standardize the feedback in some cases, thus reducing\nthe potential for error. The filing protocol is cumbersome and a more simple\nreferencing arrangement should be devised. A web-based development system, while\nrequiring authors to be online, would remove the problems encountered with authors\nmisnaming and misplacing the various item and assessment templates and graphic\nfiles.\nThe workshops were well received by writers and the same format will be used in\nthe future. The guidance and checklists will be reviewed and improved in light of\nfeedback during the quality assurance process. This process has clarified where writ-\ners are likely to make errors. Also, it will be possible to demonstrate real examples of\nthe different item types and the creativity of writers using the items generated in the\nfirst phase of the project.\nThe quality assurance process itself has worked well and reduced the burden on\nwriters. The process has been resource intensive however and there has been a limited\npool of staff to undertake the work. The process has also highlighted the importance\nof recruiting experienced quality assurance staff. Care and attention to detail is\ncrucial. Improvements to the templates and to guidance and staff development for\nwriters and peer reviewers should reduce the quality assurance work required in the\nfuture.\nA large reusable assessment item bank 213\nThe file management systems and version control again have worked well but are\nresource intensive. Changes to the file protocol for the template should reduce the\nadministrative burden.\nThe development of a conversion tool which produces items which will render\ncorrectly in a range of VLEs has been a difficult process because the VLEs do not\ninterpret the QTI specification in the same way. In addition there has been no\ncommonly agreed format for item metadata. However the TOIA-COLA Assessment\nMetadata Application profile provided a mechanism for storing the metadata\nrequired by the COLA project in a standards-compliant format readable by a number\nof existing repositories.\nFor the development of future assessments, COLA is considering a web-based\nsystem which would remove some of the logistical problems currently being faced\nsuch as authors having to give items and assessments the correct name and having to\nzip up assessments and email them for validation and central collation. It would help\nconsiderably with version control of the templates, the conversion tool and the\ncontent itself and remove the dependence of the conversion tool on a particular\nversion of Word.\nA web-based system could act as a repository for the assessments themselves and a\nplace where college staff could try out the assessments before loading them into a\nVLE. It would allow instant viewing of the items and assessments by the authors\nthemselves and provide much better control over the various administrative\nprocesses. It could also allow automated notification to validators by email when an\nassessment is ready to be validated.\nThe next stage of the project will be to evaluate how easy colleges find it to import\nthe COLA items to their VLEs and to analyse the uptake of the materials by staff and\nstudents nationally.\nReferences\nCETIS Assessment Special Interest Group. Available online at: assessment.cetis.ac.uk\nCOLEG. Available online at: www.coleg.org.uk\ne3an project. Available online at: www.e3an.ac.uk\nHELM project. Available online at: www.lboro.ac.uk\/research\/helm\/\nIEEE (2002) Standard for Learning Object Metadata, IEEE 1484, 12.1-2002.\nIMS (2002) Question and test interoperability specification v1.2, IMS Global Learning\nConsortium. Available online at: www.imsglobal.org\nMcCallon, E. L. & Schumacker, R. E. (2002) Developing and maintaining an item bank, ELM\nMetrics Inc. Available online at: www.elmmetrics.com\nMcAlpine, M. (2002) Design requirements of a databank, CAA centre. Available online at:\nwww.caacentre.ac.uk\nSclater, N (2003) TOIA-COLA Metadata application profile v1.2. Available online at:\nwww.cetis.ac.uk\/profiles\/uklomcore\/\nSQA (2003) Guidelines on online assessment for further education. Available online at:\nwww.sqa.org.uk\nWard, A. W. & Murray-Ward, M. (1994) Guidelines for the development of item banks, Educational\nmeasurement: issues and practice (National Council on Measurement in Education).\n"}