{"doi":"10.1109\/PROMISE.2007.11","coreId":"102520","oai":"oai:epubs.surrey.ac.uk:1982","identifiers":["oai:epubs.surrey.ac.uk:1982","10.1109\/PROMISE.2007.11"],"title":"Project Data Incorporating Qualitative Facts for Improved Software Defect Prediction","authors":["Fenton, Norman","Neil, Martin","Marsh, William","Hearty, Peter","Radlinski, Lukasz","Krause, Paul"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2007-05-20","abstract":"<p>To make accurate predictions of attributes like defects found in complex software projects we need a rich set of process factors. We have developed a causal model that includes such process factors, both quantitative and qualitative. The factors in the model were identified as part of a major collaborative project. A challenge for such a model is getting the data needed to validate it. We present a dataset, elicited from 31 completed software projects in the consumer electronics industry, which we used for validation. The data were gathered using a questionnaire distributed to managers of recent projects. The dataset will be of interest to other researchers evaluating models with similar aims. We make both the dataset and causal model available for research use.<\/p","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:epubs.surrey.ac.uk:1982<\/identifier><datestamp>\n      2017-10-31T14:03:54Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      74797065733D636F6E666572656E63655F6974656D<\/setSpec><setSpec>\n      6469766973696F6E733D656E67616E64706879736963616C736369656E636573:436F6D707574696E67<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/epubs.surrey.ac.uk\/1982\/<\/dc:relation><dc:title>\n        Project Data Incorporating Qualitative Facts for Improved Software Defect Prediction<\/dc:title><dc:creator>\n        Fenton, Norman<\/dc:creator><dc:creator>\n        Neil, Martin<\/dc:creator><dc:creator>\n        Marsh, William<\/dc:creator><dc:creator>\n        Hearty, Peter<\/dc:creator><dc:creator>\n        Radlinski, Lukasz<\/dc:creator><dc:creator>\n        Krause, Paul<\/dc:creator><dc:description>\n        <p>To make accurate predictions of attributes like defects found in complex software projects we need a rich set of process factors. We have developed a causal model that includes such process factors, both quantitative and qualitative. The factors in the model were identified as part of a major collaborative project. A challenge for such a model is getting the data needed to validate it. We present a dataset, elicited from 31 completed software projects in the consumer electronics industry, which we used for validation. The data were gathered using a questionnaire distributed to managers of recent projects. The dataset will be of interest to other researchers evaluating models with similar aims. We make both the dataset and causal model available for research use.<\/p><\/dc:description><dc:date>\n        2007-05-20<\/dc:date><dc:type>\n        Conference or Workshop Item<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/epubs.surrey.ac.uk\/1982\/1\/fulltext.pdf<\/dc:identifier><dc:identifier>\n          Fenton, Norman, Neil, Martin, Marsh, William, Hearty, Peter, Radlinski, Lukasz and Krause, Paul  (2007) Project Data Incorporating Qualitative Facts for Improved Software Defect Prediction  In: Third International Workshop on Predictor Models in Software Engineering (PROMISE'07: ICSE Workshops 2007).     <\/dc:identifier><dc:relation>\n        10.1109\/PROMISE.2007.11<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/epubs.surrey.ac.uk\/1982\/","10.1109\/PROMISE.2007.11"],"year":2007,"topics":[],"subject":["Conference or Workshop Item","PeerReviewed"],"fullText":"Project Data Incorporating Qualitative Factors for  \nImproved Software Defect Prediction \n \n \nNorman Fenton, Martin Neil, William Marsh, Peter Hearty and \n\u0141ukasz Radli\n\u0000\nski\n\u2021\n \nDepartment of Computer Science \nQueen Mary, University of London \nMile End Road, London \nand \n\u2021\nInstitute of Information Technology in Management \nUniversity of Szczecin, Poland \nnorman, martin, william, hearty, lukrad \n@dcs.qmul.ac.uk \nPaul Krause \nDepartment of Computing \nUniversity of Surrey  \nGuildford, Surrey, UK \np.krause@surrey.ac.uk \n \n \nAbstract \n \nTo make accurate predictions of attributes like \ndefects found in complex software projects we need a \nrich set of process factors. We have developed a causal \nmodel that includes such process factors, both \nquantitative and qualitative. The factors in the model \nwere identified as part of a major collaborative \nproject. A challenge for such a model is getting the \ndata needed to validate it. We present a dataset, \nelicited from 31 completed software projects in the \nconsumer electronics industry, which we used for \nvalidation. The data were gathered using a \nquestionnaire distributed to managers of recent \nprojects. The dataset will be of interest to other \nresearchers evaluating models with similar aims. We \nmake both the dataset and causal model available for \nresearch use. \n \n \n1. Introduction \n \nThe proper goal of research in software metrics [7, \n11, 12] is to help project managers make decisions \nunder uncertainty. In particular, we wish to be able to \nestimate the cost of developing software, and to predict \nthe quality likely to be achieved by a given \ndevelopment effort. The MODIST (\u2018Models of \nUncertainty and Risk for Distributed Software \nDevelopment\u2019) Project [14], which was part-funded by \nthe EC, was concerned with improved predictions of \nquality in large distributed software projects. The \nproject partners were Agena, Israel Aircraft Industries, \nQinetiQ and Philips. As part of this project a group of \nexperienced project managers identified a set of factors \ninfluencing cost and quality outcomes, which we \nformed into a causal model. This model is summarized \nin Section 2.  \nThe objective of this paper is to describe how we \nvalidated this model and to make the data available to \nother researchers. We needed data that was not \navailable in any publicly accessible form (even though \nsimilar factors are used in models supporting software \nmanagers, most notably COCOMO-II [2] for software \ncost estimation). For example, whereas the ISBSG \ndataset [9] (which is accessible at low cost and contains \napproximately 3500 projects) helped us to quantify \nsome of the relationships in the model, it does not help \nin validation because of the absence of the qualitative, \ncausal factors.  \nTo get the necessary data, senior project managers \nin one organization provided the information for 31 \nnew projects. We had to provide refined and more \ndetailed descriptions and measurement schemes for \nmost of the factors in the model. This process is \ndescribed in Section 3. The resulting quantitative data \nis presented in Section 4, with the qualitative data in \nSection 5. Section 6 describes some issues arising from \ndata collection, while in Section 7 we summarise the \nmodel validation results.  The results show that the \ncausal model, built using expert judgement and \nhistorical data, was able to make accurate predictions \nfor the new projects. \n \nThird International Workshop on Predictor Models in Software Engineering (PROMISE'07)\n0-7695-2954-2\/07 $20.00  \u00a9 2007\n2. Background: the causal model \n \nThis section is a summary overview of the causal \nmodel whose factors were elicited from experienced \nproject managers in the MODIST project [14]. The \nmodel, which is a Bayesian Network, is presented in \nschematic form in Figure 1, where each rectangle \nrepresents a subnetwork. A detailed description of both \nthe model itself and the Bayesian Network formalism is \nnot necessary in this paper since our focus is on the \ndata. However, for those interested the necessary \ndetails can be found in [4, 5, 6, 8, 10, 15]. The model \nitself can be downloaded from [13] and viewed and \nexecuted using Bayesian net software which can be \ndownloaded for free from [1]. \n \nSpecification and \ndocumentation\nScale of new\nfunctionality\nimplemented\nCommon\ninfluences\nDesign and \ndevelopment\nTesting and \nrework\nDefect insertion\nand discovery\nExisting code\nbase\n \nFigure 1. Schematic view of defect prediction \nmodel \n \nExamples of two of the subnetworks, shown as \nrectangles in Figure 1, are provided in Figure 2 and \nFigure 3. Each subnetwork is a part of the Bayesian \nNetwork, with nodes representing probabilistic \nvariables and arcs representing causal relationships \nbetween variables. It is important to note that the model \nnot only reflects relationships between variables which \ncould be reflected in regression-type models, but also \ndirect cause-effect relationships. For example, a more \nrigorous testing process leads to an increased \nprobability of finding and fixing a defect and thus to a \nreduced number of defects left in the software after \ntesting.  As an extreme case, the model includes the \nknowledge that no defects will be found if no testing is \ndone. \n \nFigure 2. Defect insertion and discovery subnet  \n \n \n \n \nFigure 3. Testing and rework subnet \n \nThird International Workshop on Predictor Models in Software Engineering (PROMISE'07)\n0-7695-2954-2\/07 $20.00  \u00a9 2007\nFigure 2 shows the core of the Bayesian Network: \nthe nodes shared between this subnetwork and the \nothers are shown with dashed edges (unshaded).  These \nshared nodes are unobservable quantities, such as the \nprobability of finding a defect present in the software \nduring testing.  These values are predicted from \nqualitative process factors. Figure 3 shows one \nexample of the subnetworks containing these \nqualitative process factors, with two of the shared \nnodes shown on the right hand side of the figure.   \nWe developed this causal model (see [8] for further \ndetails) based on a combination of the following \nsources:  \n\u2022 empirical data from the literature; \n\u2022 empirical data from the project partners; \n\u2022 subjective judgment of project managers and \nother experts in the collaborative project, \nwhere no relevant data were available. \nThe causal model was not developed from the data \nreported in this paper, which was instead gathered after \nthe model had been developed to validate the model.   \n \n3. Qualitative factors \n \nAs the first stage of the development of the causal \nmodel outlined in Section 2, partners in the MODIST \nProject [14] identified qualitative factors that they \nbelieved had a significant influence on the outcome of \na software project. Once the model had been built the \nsecond stage was to gather a dataset to validate the \nmodel; to do this effectively a more detailed \ndescription of each factor was needed. In Section 3.1 \nwe describe the set of factors, together with the first \nlevel of detailed description. Section 3.2 gives an \nextract of the subsequent questionnaire given to project \nmanagers to gather data from completed projects.  \nSection 3.3 discusses some issues arising from this \nmethod of measuring the qualitative project factors. \nAlthough it was intended that the validation dataset \nwould cover all the data used in the Bayesian Network, \nthis was not achieved.  Some questions were not \nanswered by some project managers.  A small number \nof variable were omitted altogether.  This arose were \nthe data recording practices of the projects did not \nmatch the assumptions of the questionnaires: in \nparticular rework was not distinguished from testing, \nand the concept of the \u2018effort\u2019 spent on a project phase, \nrelative to what would be expected on average, was not \nused by project managers.  Fortunately, a Bayesian \nNetwork handles missing data (see Section 6.3). \n \n3.1.  Factor descriptions \n \nThe factors are conveniently grouped under five \ntopics: specification and documentation process (Table \n1), new functionality (Table 2), design and \ndevelopment process (Table 3), testing and rework \n(Table 4) and finally project management (Table 5). \nHowever, this grouping is not part of the dataset; the \nfactors should all be considered to be project attributes. \nEach factor is named and described by a question to \nbe answered. The descriptive questions were \nspecifically tailored for the organisation providing the \nproject data. \n \nTable 1 Specification and documentation process \n Factor Name Descriptive Question \nS1 \nRelevant \nExperience of \nSpec & Doc \nStaff \nHow would you rate the \nexperience and skill set of your \nteam members for executing this \nproject during the requirements \nand specifications phase? \nS2 \nQuality of \nDocumentation \ninspected \nHow would you rate the quality of \nthe requirements given by the \nclient or other groups? \nS3 \nRegularity of \nSpec & Doc \nReviews \nHave all the Requirements, \nDesign Documents and Test \nSpecifications been reviewed in \nthe project? \nS4 \nStandard \nProcedures \nFollowed \nIn your opinion, how effective \nwas the review procedure? \nS5 \nQuality of \nDocumentation \ninspected \nWhat was the review \neffectiveness in the project for the \nrequirements phase? \nS6 \nSpec Defects \nDiscovered in \nReview \nIn your opinion, is the defect \ndensity of spec reviews on the \nhigh side? \nS7 \nRequirements \nStability \nHow stable were the requirements \nin your project? \n \nTable 2. New functionality \n Factor Name Descriptive Question \nF1 \nComplexity of \nnew \nfunctionality \nWhat was the complexity of the \nnew development or new features \nthat happened in your project? \nF2 \nScale of New \nfunctionality \nimplemented \nHow large was the extent of \nworking on new functionality \nrather than just enhancing the \nolder functionalities in your \nproject? \nThird International Workshop on Predictor Models in Software Engineering (PROMISE'07)\n0-7695-2954-2\/07 $20.00  \u00a9 2007\n Factor Name Descriptive Question \nF3 \nTotal no. of \nInputs and \nOutputs \nFor your product domain, would \nyou rate the total no of \noutputs\/inputs (newly developed \/ \nenhanced) as high? \n \nTable 3. Design and development process \n Factor Name Descriptive Question \nD1 \nRelevant \nDevelopment \nStaff \nExperience \nHow would you rate the \nexperience and skill set of your \nteam members for executing this \nproject during the design and \ndevelopment phase? \nD2 \nProgrammer \ncapability \nOn an average, how would you \nassess the Quality of code \nproduced by the team members? \nD3 \nDefined \nprocesses \nfollowed \nWhat was the review effectiveness \nin the project for the Design and \nDevelopment phase? \nD4 \nDevelopment \nStaff \nmotivation \nWhat is your opinion about the \nmotivation levels of your team \nmembers? \n \nTable 4. Testing and rework \n Factor Name Descriptive Question \nT1 \nTesting \nProcess Well \nDefined \nHow effective was the testing \nprocess adopted by your project? \nT2 \nStaff \nExperience \u2013 \nUnit Test \nWhat was the level of software test \ncompetence of those performing \nthe unit test? \nT3 \nStaff \nExperience \u2013 \nIndependent \nTest \nHow would you rate the \nexperience and skill set of the \nindependent test engineers \n(Integration, functional or sub-\nsystem testing, Alpha, Beta)? \nT4 \nQuality of \nDocumented \nTest Cases \nWhat was the extent of the defects \nthat were found using formal \ntesting against the intuitive \/ \nrandom testing? \n \nTable 5. Project Management \n Factor Name Descriptive Question \nP1 \nDev. Staff \nTraining \nQuality \nWhat is the coverage of the \nidentified project \/ process \nrelated trainings as well as \ntrainings identified as per the \nroles, by the team members?  \n Factor Name Descriptive Question \nP2 \nConfiguration \nManagement \nHow effective is the project\u2019s \ndocument management and \nconfiguration management? \nP3 \nProject \nPlanning \nHas the project planning been \ndone adequately? \nP4 \nScale of \nDistributed \nCommunication \nHow many sites\/groups were \ninvolved in the project.  \nP5 \nStakeholder \ninvolvement  \nTo what extent were the key \nproject stakeholders involved? \nP6 \nCustomer \ninvolvement \nHow good was customer \ninteraction in the project? \nP7 \nVendor \nManagement \nHow would you rate the Vendor \/ \nSub-contractor Management (if \napplicable)? \nP8 \nInternal \ncommunication \n\/ interaction \nHow would you the rate the \nquality of internal interactions \/ \ncommunication within the team? \nP9 \nProcess \nMaturity \nWhat\u2019s your opinion about \nprocess maturity in the project? \n \n3.2. Questionnaire Design \n \nQualitative data are expressed on a 5-point ordinal \nscale. The ordinal values used are: Very High, High, \nMedium, Low, Very Low. The data values were \ngathered using a questionnaire, which was completed \nby the project manager, project quality manger or other \nsenior project staff. Each questionnaire item consists \nof: \n\u2022 More detailed questions \n\u2022 An interpretation of the ordinal scale. \nFor example, for factor S1 \u2018Relevant Experience of \nSpec & Doc Staff\u2019, the additional questions are: \n1. Did the Requirements team have adequate \nexperience in analysing and generating \nrequirements? \n2. Did the Requirements team have adequate \ndomain expertise? \nand the ordinal scale points are: \nVery High: Software engineers with greater than \nthree year\u2019s experience in requirements \nmanagement, and with extensive domain \nknowledge. \nHigh: Software engineers with greater than three \nyear\u2019s experience in requirements management, \nbut with limited domain knowledge. \nThird International Workshop on Predictor Models in Software Engineering (PROMISE'07)\n0-7695-2954-2\/07 $20.00  \u00a9 2007\nMedium: Software engineers having between one \nand three year\u2019s experience in requirements \nmanagement. \nLow: Software engineers having between one and \nthree year\u2019s experience, but with no experience in \nrequirements management. \nVery Low: Software engineers with less than one \nyear\u2019s experience, and with no previous domain \nexperience. \n \nIn some cases the questionnaire used a set of criteria \nand a score. An example is the factor S4 \u2018Standard \n(Review) Procedures Followed\u2019. The detailed \nquestions, giving the criteria, are:  \n1. In case of changes after baselining, have the \nmajor changes been re-reviewed? \n2. Are there any re-review triggers\/criteria defined? \n3. Have some domain specific standards been \nadhered to (like design rules, re-engineering \nguidelines, architectural guidelines, etc)? \n4. Was the requirements document checked for \nreview worthiness or pre-review checklist filled \nbefore the review? \n5. Have the reviews been planned upfront? \n6. Have the reviewers been assigned upfront? \n7. Were the reviews role-based? \n8. Were the reviewers identified appropriate and \nexperienced enough for reviewing? \n9. Was there adequate preparation time available \nfor the reviewers? \n10. Were there overview sessions for all complex \nwork products? \nThe scale point is then derived as follows: \nVery High: All the 10 sub questions answered yes \nHigh: 7-9 of the sub questions answered yes \nMedium: 5-6 of the sub questions answered yes \nLow: 4 of the sub questions answered yes \nVery Low: less than 4 of the sub questions \nanswered yes. \n \n3.3. Measurement Issues \n \nThe factors used in the model were originally \nidentified by a group of project manager from different \npartners in the MODIST project.  Although from \ndifferent organisations, it was possible for the project \nmanagers to agree on the importance of factors such as \n\u2018Requirements Stability\u2019. A further issue is whether it is \npossible to measure such values consistently between \norganisations.  \nAs shown by the example in Section 3.2, we \ndesigned the questionnaire to used objective criteria, \nsuch as the number of years of experience, whether \npossible.  However, we do not claim external validity \nof these measurements, since this is not needed for our \napproach, as we explain below. \nOne way experts were used in building the model \nwas to estimate the \u2018strength\u2019 of the effect of each \nqualitative factor in the causal model.  This information \nis represented in the conditional probability table for \neach node in the Bayesian Network.  As a result of this \nprocess, the model is applicable within the organisation \nwhere the experts have gained their experience. Since \nthe model itself is not universal, there is no need for the \nmeasurements to be so. It does not follow that the \nscope of the validation (see Section 7) becomes trivial, \neven tautological, as a result.  Instead, the validation \nshows that a model constructed using expert judgement \nand historical data, within one organisation, can be \nused within the same organisation to predict accurately \nthe outcome of new projects.  On the other hand, the \nvalidation described here does not consider issues such \nas the external validity of the causal structure of our \nmodel (see Section 6.4).  \n \n4. Quantitative data \n \nThe projects developed software for consumer \nelectronics products. Each project developed or \nenhanced some functionality provided by a product. \nThe developed software was not stand-alone but was \nintegrated with other software subsystems in the \nproduct. \nA waterfall lifecycle was followed. The software \nengineering part of the lifecycle covered a specification \nreview, design, a design review and development up to \nunit testing. The software was then passed to \nindependent test in several phases, from software \nintegration testing to overall system (i.e. product) \ntesting. \n \nTable 6. Size, effort and defects \n \nProject Hours KLoC Language Defects \n1 7109 6.0 C 148 \n2 1308 0.9 C 31 \n3 18170 53.9 C 209 \n4 7006 - C 228 \n5 9434 14.0 C 373 \n6 9441 14.0 C 167 \n7 13888 21.0 C 204 \n8 8822 5.8 C 53 \nThird International Workshop on Predictor Models in Software Engineering (PROMISE'07)\n0-7695-2954-2\/07 $20.00  \u00a9 2007\nProject Hours KLoC Language Defects \n9 2192 2.5 \nVC++,MF\nC 17 \n10 4410 4.8 C 29 \n11 14196 4.4 C 71 \n12 13388 19.0 C 90 \n13 25450 49.1 C 129 \n14 33472 58.3 C 672 \n15 34893 154.0 C 1768 \n16 7121 26.7 C 109 \n17 13680 33.0 C 688 \n18 32366 155.2 C 1906 \n19 12388 87.0 C 476 \n20 52660 50.0 C 928 \n21 18748 22.0 C 196 \n22 28206 44.0 C 184 \n23 53995 61.0 C 680 \n24 24895 99.0 C 1597 \n25 6906 23.0 C 546 \n26 1642 - C 261 \n27 14602 52.0 C 412 \n28 8581 36.0 C 881 \n29 3764 11.0 C 91 \n30 1976 1.0 C 5 \n31 15691 33.0 C 653 \n \nMost of the software development was at one site, \nbut the overall development was distributed over \ndifferent locations in a global organisation. Both the \nsoftware specification and the independent testing were \ntypically at a different location to the software \ndevelopment.  \nThe data values are shown in Table 6: \n\u2022 Software size: the size, in KLoC of the developed \ncode and the development language (Figure 4 \nshows the distribution of code size in the dataset). \nNote that for two projects, this data was not \navailable: the Bayesian Network can still be used \nand it will assume the projects to be \u2018average\u2019 but \nof uncertain size. \n\u2022 Effort: development effort measured in person \nhours for the software development, from \nspecification review to unit test \n0\n1\n2\n3\n4\n5\n6\n7\n8\n<5 5-10 10-25 25-50 50-75 75-100 100-\n200\n>200\nCode Size (KLoC)\nN\nu\nm\nb\ne\nr \no\nf \nP\nro\nje\nc\nts\n \nFigure 4. Code size distribution \n\u2022 mDefects: functional defects discovered during all \nthe independent testing phases, following the \nsoftware development. \n \nIn some projects existing software was reused as \npart of the development. The impact of this on the \ndataset is considered in Section 6. \nThis new dataset could, of course, be used to build \ntraditional statistical\/regression based models, as \nindicated, for example, by Figure 5. This could be the \nbasis for a simple regression model relating KLoC to \ndefects; indeed the correlation coefficient here is quite \nhigh (0.78). However, this does not correspond to the \nway that we used this data, which was to validate a \nmodel created before the data was gathered.  Therefore, \nwe do not pursue this comparison. \n0\n500\n1000\n1500\n2000\n2500\n0 50 100 150 200\nCode Size  (KLoC)\nD\ne\nfe\nc\nts\n F\no\nu\nn\nd\n \nFigure 5. Code size versus defects \n \n5. Qualitative data \n \nThe data values are shown in tables 7 to 10. Missing \ndata values are marked with \u2018-\u2019 (see Section 6.3). The \nletters VL, L, M, H, VH correspond to the ordinal scale \ndescribed in Section 3.2 (very low to very high). \n \nThird International Workshop on Predictor Models in Software Engineering (PROMISE'07)\n0-7695-2954-2\/07 $20.00  \u00a9 2007\nTable 7. Specification and documentation process \ndata \n \nProject S1 S2 S3 S4 S5 S6 S7 \n1 H M VH H M H L \n2 H H VH H M H H \n3 H H VH H H VH H \n4 L L M L L L L \n5 H M H M H - M \n6 VH M VH M H - H \n7 L M VH H H L M \n8 M M H M H L H \n9 H VH VH H VH M VH \n10 H H H M H M H \n11 H M H M H H H \n12 H M H M M M L \n13 VH M M L M H L \n14 H H H H H H H \n15 H H H H H VH VL \n16 H H H H H H M \n17 VH H M L H H M \n18 M H H H H VH VL \n19 H M H H H H M \n20 L L M VL L M VL \n21 H H H M L M M \n22 L L M M M M L \n23 M H VH H L M M \n24 M M M H M H L \n25 M H - H M M M \n26 M M H M H H H \n27 H M VH M M VH M \n28 H L VH M M M L \n29 H M VH H M M VH \n30 H H VH H H M VH \n31 - H H M M H M \n \nTable 8. Data for new functionality, design and \ndevelopment process \n \nProject F1 F2 F3 D1 D2 D3 D4 \n1 M L M L H H H \n2 L VL M L H H H \n3 H H VH H VH H VH \n4 M L M L M L M \n5 H H VH L M H H \n6 M M VH M H M M \n7 L VL M M VH H H \n8 M L M H H M M \nProject F1 F2 F3 D1 D2 D3 D4 \n9 L L M H VH VH H \n10 M L M H H H H \n11 H H H H H H H \n12 H H H VH M M H \n13 H H H H H H H \n14 VH H H H H H H \n15 H H M H H H H \n16 L VL M H H H H \n17 L VL M M M H H \n18 VH VH H M H H H \n19 H H H H H H H \n20 VH H VH VL VL L H \n21 L M VH H H H H \n22 M M VH H M L H \n23 H VH VH L H H H \n24 M M H M H H M \n25 H VL H M H M H \n26 M H M L M M M \n27 H VH VH M L M H \n28 VH VH VH M L H H \n29 M M H VH VH H H \n30 L L M H H H H \n31 M M H H H H H \n \nTable 9. Testing and rework data \n \nProject T1 T2 T3 T4 \n1 M H L H \n2 H H L H \n3 H H H H \n4 VL VL VL L \n5 M M L M \n6 H - M M \n7 H M M H \n8 H M M M \n9 H VH VH H \n10 H M M M \n11 H H M M \n12 H H M M \n13 M M L M \n14 H H H H \n15 M H M M \n16 M H M M \n17 M L L H \n18 H H M M \n19 H M M H \n20 VL VL VH H \nThird International Workshop on Predictor Models in Software Engineering (PROMISE'07)\n0-7695-2954-2\/07 $20.00  \u00a9 2007\nProject T1 T2 T3 T4 \n21 H H H H \n22 H M M H \n23 H H H H \n24 H M M M \n25 VL M H L \n26 M L H M \n27 M M M M \n28 M M M M \n29 H VH VH H \n30 H H H H \n31 M H M M \n \nTable 10. Project management data \n \nProject P1 P2 P3 P4 P5 P6 P7 P8 P9 \n1 VH H H L H M - VH H \n2 VH H H L H M - VH H \n3 H VH H - VH VH - VH VH \n4 L M VL L M M M H M \n5 H H H M M H L VH M \n6 H H H M M VH L VH H \n7 H H VH VL VH VH - H VH \n8 M H H VL H H - H H \n9 VH VH VH L VH VH - VH VH \n10 H H H VL H H - M H \n11 H H H VL H H - M M \n12 H H H L H H - M H \n13 M H H VL H M H M M \n14 H H H - H H - H H \n15 VH M H M VH VH - VH H \n16 VH M H M VH VH - VH H \n17 M M M M M H - H M \n18 VH M H H VH VH - VH H \n19 M H H L H H - H H \n20 H M L H H M - H H \n21 H H H H H H - H H \n22 H H M H H H - H H \n23 H H H H H M - H H \n24 H H M L M H - VH H \n25 M M M M M M M H H \n26 L M M L H H L H M \n27 H M L L M H H H M \n28 H M L L M M - H M \n29 M H H L VH VH - H H \n30 M H H L VH VH - H H \n31 H H H H VH VH - VH VH \n \n6. Issues arising from the data collection  \n \nThe complexity of software projects make gathering \ndata challenging. The most important challenges we \nfaced and lessons we learned during this work are \ndescribed below. Some of these challenges are not fully \nresolved by the data included in this dataset; how these \nissues were addressed in our models is described \nelsewhere [8]. \n \n6.1. Software size: intrinsic complexity \n \nBecause of the need to have a size based measure \nbased only on the amount of functionality to be \nimplemented, we had hoped to use function points as \nthe key metric for this purpose, as recommended from \nthe MODIST work. Unfortunately, function points \nwere not used by the software development \norganisation providing this data. It is well known [7] \nthat KLoC measures program length but the length of \nthe program is only one aspect of the size of the \ndevelopment task \u2013 which we term the \u2018intrinsic \ncomplexity\u2019. The factors F1-F3 were included in the \ndata gathered to give a better estimate of the intrinsic \ncomplexity than code size alone. Unfortunately, \nintrinsic complexity is not an observable quantity, so \nfinding sufficient factors to estimate the size of the \ndevelopment task remains a challenge. \n \n6.2. Code reuse \n \nIt is very common for software development to be \ncarried out as part of a product line development, \nnaturally giving rise to software reuse. This \ncomplicates the measurement of software size \u2013 the \nlines of developed software differs from the length of \nthe developed program \u2013 and also impacts the \nprediction of defects, since the quality of the reused \nsoftware is variable. \n \n6.3. Missing Data Values \n \nGiven the complexity of a dataset that attempts to \ncover relevant software cost and quality drivers, it is \ninevitable that some data values will be missing. It is \nessential that software prediction methods are able to \ncope with missing data values.  \nThe Bayesian net model used in this study is one \nsuch method that handles missing data, since the model \nincludes prior probability distributions for all the \nproject data. \n \nThird International Workshop on Predictor Models in Software Engineering (PROMISE'07)\n0-7695-2954-2\/07 $20.00  \u00a9 2007\n6.4. Generality of the data \n \nAn objective of the partners in the MODIST project \nwas to identify only factors (and the means of \nmeasuring them) that were generally relevant to \ncomplex software projects. Achieving this objective \nwould enable different organizations to make use of the \ncausal model. We recognize that the more detailed \ndescriptions and questionnaires refer to process-\nspecific information. The objective of generality would \nstill be partly achieved if other organizations (using \ndifferent processes) used the same factors, but adapted \nthe questionnaire as a means of measuring them. \n \n7. Model validation \n \nWe validated the causal model using the presented \nproject dataset. We did this by entering, for each \nproject, data excluding the defect data and ran the \nBayesian net model. This produces a (predicted) \nprobability distribution for number of defects found in \nindependent testing. Using the median values of these \ndistributions enables us to calculate the accuracy of the \npredictions. As presented in Figure 6, we achieved a \nvery high accuracy with an R2 value of 0.9311. \n \n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\n2000\n0 500 1000 1500 2000 2500\nActual\nP\nre\nd\nic\nte\nd\n \nFigure 6. Predicted and actual values \n \nThe outstanding result of the model validation gives \ngreat confidence in the value of the causal model, but \nof course further validation using additional datasets \nwould provide even greater confidence in the integrity \nand robustness of the model.  Moreover, the validation \nwe have described does not cover all the ways that the \nBayesian net model can be used, since it is possible to \nenter data at any of the variables and obtain the \nprobability distributions at any of the unobserved \nvariables. \n \n8. Conclusion \n \nWe have presented a comprehensive dataset for 31 \nsoftware development projects. This dataset \nincorporates the set of quantitative and qualitative \nfactors that were previously built into a causal model of \nthe software process. The factors (which had been \nidentified by a consortium of software project experts) \ninclude values for code size, effort and defects, \ntogether with qualitative data values judged by project \nmanagers (or other project staff) using a questionnaire. \nWe have used these data to evaluate the causal model \nand the results are extremely promising. Specifically \nthe model predicts, with remarkable accuracy, the \nnumber of software defects that will be found in \nindependent testing.  \nAlthough it is beyond the scope of the paper to \ndiscuss the causal model in detail, it should be noted \nthat good predictions of the defects can be achieved by \nentering relatively few of the project factors (size plus \n2 or 3 others). Hence this kind of model can be used \nfor effective decision-support and trade-off analysis \nduring early development phases.  \nBy presenting the raw data in this paper, we hope to \nenable other researchers to evaluate similar models and \ndecision-support techniques for software managers (the \ndataset can of course also be used for evaluating more \ntraditional types of software prediction models). We \nalso hope that similar datasets will become more \nwidely available in future.  \nTo ensure full visibility and repeatability, we also \nprovide an electronic version of the causal model for \nresearchers [13]. The model can be viewed and \nexecuted by downloading the free trial version of the \nBayesian network software [1]. \n \n9. Acknowledgments \n \nThis report is based in part on work undertaken on \nthe following funded research projects: MODIST (EC \nFramework 5 Project IST-2000-28749), SCULLY \n(EPSRC Project GR\/N00258), SIMP (EPSRC Systems \nIntegration Initiative Programme Project GR\/N39234), \nand eXdecide (EPSRC project EP\/C005406) [3]. We \nalso acknowledge the contributions of individuals from \nAgena, Philips, Israel Aircraft Industries, QinetiQ and \nBAE Systems.  \n \nThird International Workshop on Predictor Models in Software Engineering (PROMISE'07)\n0-7695-2954-2\/07 $20.00  \u00a9 2007\n10. References \n \n[1] AgenaRisk Bayesian Network Software Tool, \nwww.agenarisk.com, 2007. \n[2] B. Boehm, B. Clark, E. Horowitz, C. Westland, R. \nMadachy, and R. Selby, \u201cCost models for future \nsoftware life cycle process: COCOMO 2.0\u201d, Annals of \nSoftware Engineering, 1995. \n[3] eXdecide, \u201cQuantified Risk Assessment and \nDecision Support for Agile Software Projects\u201d, EPSRC \nproject EP\/C005406\/1, \nwww.dcs.qmul.ac.uk\/~norman\/radarweb\/core_pages\/pr\nojects.html . \n[4] N.E. Fenton, P. Krause, and M. Neil, \n\u201cProbabilistic Modelling for Software Quality \nControl\u201d, Journal of Applied Non-Classical Logics \n12(2), 2002, pp. 173-188. \n[5] N.E. Fenton, P. Krause, and M. Neil, \u201cSoftware \nMeasurement: Uncertainty and Causal Modelling\u201d, \nIEEE Software 10(4), 2002, pp. 116-122. \n[6] N.E. Fenton, W. Marsh, M. Neil, P. Cates, S. \nForey, and M Tailor, \u201cMaking Resource Decisions for \nSoftware Projects\u201d, Proceedings of 26th International \nConference on Software Engineering (ICSE 2004), \n(Edinburgh, United Kingdom, May 2004) IEEE \nComputer Society 2004, ISBN 0-7695-2163-0, pp. \n397-406. \n[7] N.E. Fenton, and S.L. Pfleeger, Software Metrics: \nA Rigorous and Practical Approach (2nd Edition), \nPWS, ISBN: 0534-95429-1, 1998. \n[8] N.E. Fenton, M. Neil, W. Marsh, P. Hearty, D. \nMarquez, P. Krause, and R. Mishra, \u201cPredicting \nSoftware Defects in Varying Development Lifecycles \nusing Bayesian Nets\u201d, Information and Software \nTechnology, Vol. 49 Issue 1, January 2007. \n[9] ISBSG International Software Benchmarking \nStandards Group, www.isbsg.org. \n[10] F.V. Jensen, An Introduction to Bayesian \nNetworks, UCL Press, 1996. \n[11] C. Jones, Programmer Productivity, McGraw \nHill, 1986. \n[12] C. Jones, \u201cSoftware sizing\u201d, IEE Review 45(4), \n1999, pp.165-167. \n[13] MODIST BN Model, 2007, \nhttp:\/\/www.dcs.qmul.ac.uk\/~norman\/Models\/BN_Mod\nel_PROMISE.html. \n[14] MODIST, \u201cModels of Uncertainty and Risk for \nDistributed Software Development\u201d, EC Information \nSociety Technologies Project IST-2000-28749, \nwww.modist.org. \n[15] M. Neil, P. Krause, and N.E. Fenton, \u201cSoftware \nQuality Prediction Using Bayesian Networks\u201d, in \nSoftware Engineering with Computational Intelligence, \n(Ed T.M. Khoshgoftaar), Kluwer, ISBN 1-4020-7427-\n1, Chapter 6, 2003. \nThird International Workshop on Predictor Models in Software Engineering (PROMISE'07)\n0-7695-2954-2\/07 $20.00  \u00a9 2007\n"}