{"doi":"10.1111\/j.1365-246X.2010.04796.x","coreId":"53803","oai":"oai:nora.nerc.ac.uk:12869","identifiers":["oai:nora.nerc.ac.uk:12869","10.1111\/j.1365-246X.2010.04796.x"],"title":"Parallel computation of optimized arrays for 2-D electrical\\ud\nimaging surveys","authors":["Loke, M.H.","Wilkinson, P.B.","Chambers, J.E."],"enrichments":{"references":[{"id":874447,"title":"2-D resistivity surveying for hydrocarbons \u2013 A primer, CSEG Recorder,","authors":[],"date":"2005","doi":null,"raw":null,"cites":null},{"id":877304,"title":"2D resistivity surveying for environmental and engineering applications,","authors":[],"date":"1996","doi":null,"raw":null,"cites":null},{"id":882472,"title":"32\/64-bit 80x86 assembly language architecture,","authors":[],"date":"2005","doi":null,"raw":null,"cites":null},{"id":874926,"title":"3D resistivity in the Athabasca basin with the pole-pole array,","authors":[],"date":"2006","doi":null,"raw":null,"cites":null},{"id":877805,"title":"A 3D resistivity investigation of a contaminated site at Lernacken in Sweden,","authors":[],"date":"2002","doi":null,"raw":null,"cites":null},{"id":879872,"title":"A comparison of automatic techniques for estimating the regularization parameter in non-linear inverse problems,","authors":[],"date":"2004","doi":null,"raw":null,"cites":null},{"id":883379,"title":"A comparison of Gauss-Newton and quasi-Newton methods in resistivity imaging inversion,","authors":[],"date":"2002","doi":null,"raw":null,"cites":null},{"id":883822,"title":"A comparison of smooth and blocky inversion methods in 2D electrical imaging surveys,","authors":[],"date":"2003","doi":null,"raw":null,"cites":null},{"id":878278,"title":"A numerical comparison of 2D resistivity imaging with 10 electrode arrays,","authors":[],"date":"2004","doi":null,"raw":null,"cites":null},{"id":873967,"title":"A survey of current trends in near-surface electrical and electromagnetic methods,","authors":[],"date":"2006","doi":null,"raw":null,"cites":null},{"id":875803,"title":"A tri-potential method of resistivity prospecting,","authors":[],"date":"1956","doi":null,"raw":null,"cites":null},{"id":885127,"title":"Accelerating a three-dimensional finite-difference wave propagation code using GPU graphics cards.","authors":[],"date":"2009","doi":null,"raw":null,"cites":null},{"id":888797,"title":"An introduction to this special section: High performance computing.","authors":[],"date":"2010","doi":null,"raw":null,"cites":null},{"id":879442,"title":"Applied geophysical inversion,","authors":[],"date":"1994","doi":null,"raw":null,"cites":null},{"id":889771,"title":"Aquifer characterization in the Blue Ridge physiographic province using resistivity profiling and borehole geophysics : Geologic analysis,","authors":[],"date":"2000","doi":null,"raw":null,"cites":null},{"id":882385,"title":"Capacitive Resistivity Imaging with Towed Arrays,","authors":[],"date":"2007","doi":null,"raw":null,"cites":null},{"id":890586,"title":"Comparison of the spatial resolution of standard and optimised Electrical Resistivity Tomography arrays,","authors":[],"date":"2006","doi":"10.1016\/j.jappgeo.2009.08.001","raw":null,"cites":null},{"id":889299,"title":"Electrical resistivity imaging for detecting soil cracking at the centimetric Scale.","authors":[],"date":"2003","doi":null,"raw":null,"cites":null},{"id":876256,"title":"Electrical resistivity tomography applied to geologic, hydrogeologic, and engineering investigations at a former waste-disposal site,","authors":[],"date":"2006","doi":null,"raw":null,"cites":null},{"id":887355,"title":"Examples of resistivity imaging using ME-100 resistivity field acquisition system,","authors":[],"date":"1996","doi":null,"raw":null,"cites":null},{"id":890270,"title":"Experimental design: Electrical resistivity data sets that provide optimum subsurface information,","authors":[],"date":"2004","doi":"10.1190\/1.1649381","raw":null,"cites":null},{"id":884299,"title":"Fast computation of optimized electrode arrays for 2D resistivity surveys, Computers and Geosciences,","authors":[],"date":"2010","doi":null,"raw":null,"cites":null},{"id":884643,"title":"Geophysical data analysis:","authors":[],"date":"1989","doi":null,"raw":null,"cites":null},{"id":891032,"title":"Improved strategies for the automatic selection of optimized sets of electrical resistivity tomography measurement configurations,","authors":[],"date":"2006","doi":"10.1111\/j.1365-246X.2006.03196.x","raw":null,"cites":null},{"id":881989,"title":"Interpretation of inaccurate, insufficient and inconsistent data,","authors":[],"date":"1972","doi":null,"raw":null,"cites":null},{"id":882890,"title":"Least-squares deconvolution of apparent resistivity pseudosections,","authors":[],"date":"1995","doi":null,"raw":null,"cites":null},{"id":882040,"title":"Leveraging graphics processing units (GPUs) for real-time seismic interpretation, The Leading Edge,","authors":[],"date":"2010","doi":null,"raw":null,"cites":null},{"id":886025,"title":"Massively parallel forward modeling of scalar and tensor gravimetry data,","authors":[],"date":"2010","doi":null,"raw":null,"cites":null},{"id":879229,"title":"Numerical linear algebra for high-performance computers, Society for Industrial and Applied Mathematics,","authors":[],"date":"1998","doi":null,"raw":null,"cites":null},{"id":888296,"title":"Numerical Recipes in C, 2nd edn,","authors":[],"date":"1992","doi":null,"raw":null,"cites":null},{"id":878747,"title":"Occam's inversion to generate smooth, twodimensional models form magnetotelluric data,","authors":[],"date":"1990","doi":null,"raw":null,"cites":null},{"id":876655,"title":"Parallel Programming in OpenMP,","authors":[],"date":"2001","doi":null,"raw":null,"cites":null},{"id":891477,"title":"Properties and effects of measurement errors on 2D resistivity imaging surveying,","authors":[],"date":"2003","doi":"10.3997\/1873-0604.2003001","raw":null,"cites":null},{"id":885570,"title":"Resolution analysis of geophysical images: Comparison between point spread function and region of data influence measures,","authors":[],"date":"2007","doi":null,"raw":null,"cites":null},{"id":881115,"title":"Resolution, stability and efficiency of resistivity tomography estimated from a generalized inverse approach,","authors":[],"date":"2003","doi":null,"raw":null,"cites":null},{"id":877147,"title":"Selecting the right hardware for reverse time migration, The Leading Edge,","authors":[],"date":"2010","doi":null,"raw":null,"cites":null},{"id":886463,"title":"The point-spread function measure of resolution for the 3-D electrical resistivity experiment.","authors":[],"date":"2009","doi":null,"raw":null,"cites":null},{"id":881566,"title":"The Software Optimization Cookbook : High-performance recipes for the Intel architecture, Intel Press,","authors":[],"date":"2002","doi":null,"raw":null,"cites":null},{"id":875373,"title":"Trends for high-performance scientific computing,","authors":[],"date":"2010","doi":null,"raw":null,"cites":null},{"id":876888,"title":"Using OpenMP,","authors":[],"date":"2008","doi":null,"raw":null,"cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-12","abstract":"Modern automatic multi-electrode survey instruments have made it possible to use non-traditional arrays to maximize the subsurface resolution from electrical imaging surveys. Previous studies have shown that one of the best methods for generating optimized arrays is to select the set of array configurations that maximizes the model resolution for a homogeneous earth model. The Sherman\u2013Morrison Rank-1 update is used to calculate the change in the model resolution when a new array is added to a selected set of array configurations. This method had the disadvantage that it required several hours of computer time even for short 2-D survey lines. The algorithm was modified to calculate the change in the model resolution rather than the entire resolution matrix. This reduces the computer time and memory required as well as the computational round-off errors. The matrix\u2013vector multiplications for a single add-on array were replaced with matrix\u2013matrix multiplications for 28 add-on arrays to further reduce the computer time. The temporary variables were stored in the double-precision Single Instruction Multiple Data (SIMD) registers within the CPU to minimize computer memory access. A further reduction in the computer time is achieved by using the computer graphics card Graphics Processor Unit (GPU) as a highly parallel mathematical coprocessor. This makes it possible to carry out the calculations for 512 add-on arrays in parallel using the GPU. The changes reduce the computer time by more than two orders of magnitude. The algorithm used to generate an optimized data set adds a specified number of new array configurations after each iteration to the existing set. The resolution of the optimized data set can be increased by adding a smaller number of new array configurations after each iteration. Although this increases the computer time required to generate an optimized data set with the same number of data points, the new fast numerical routines has made this practical on commonly available microcomputers.\\ud\n\\u","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/53803.pdf","fullTextIdentifier":"http:\/\/nora.nerc.ac.uk\/12869\/1\/ParallelOpt.pdf","pdfHashValue":"a14cd9a793e50deaf88ca7847edcd6b39f08cf16","publisher":"Wiley","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:nora.nerc.ac.uk:12869<\/identifier><datestamp>\n      2012-11-22T12:17:06Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D5338<\/setSpec><setSpec>\n      7375626A656374733D533234<\/setSpec><setSpec>\n      7375626A656374733D533130<\/setSpec><setSpec>\n      7375626A656374733D533231<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/nora.nerc.ac.uk\/id\/eprint\/12869\/<\/dc:relation><dc:title>\n        Parallel computation of optimized arrays for 2-D electrical\\ud\nimaging surveys<\/dc:title><dc:creator>\n        Loke, M.H.<\/dc:creator><dc:creator>\n        Wilkinson, P.B.<\/dc:creator><dc:creator>\n        Chambers, J.E.<\/dc:creator><dc:subject>\n        Computer Science<\/dc:subject><dc:subject>\n        Physics<\/dc:subject><dc:subject>\n        Earth Sciences<\/dc:subject><dc:subject>\n        Mathematics<\/dc:subject><dc:description>\n        Modern automatic multi-electrode survey instruments have made it possible to use non-traditional arrays to maximize the subsurface resolution from electrical imaging surveys. Previous studies have shown that one of the best methods for generating optimized arrays is to select the set of array configurations that maximizes the model resolution for a homogeneous earth model. The Sherman\u2013Morrison Rank-1 update is used to calculate the change in the model resolution when a new array is added to a selected set of array configurations. This method had the disadvantage that it required several hours of computer time even for short 2-D survey lines. The algorithm was modified to calculate the change in the model resolution rather than the entire resolution matrix. This reduces the computer time and memory required as well as the computational round-off errors. The matrix\u2013vector multiplications for a single add-on array were replaced with matrix\u2013matrix multiplications for 28 add-on arrays to further reduce the computer time. The temporary variables were stored in the double-precision Single Instruction Multiple Data (SIMD) registers within the CPU to minimize computer memory access. A further reduction in the computer time is achieved by using the computer graphics card Graphics Processor Unit (GPU) as a highly parallel mathematical coprocessor. This makes it possible to carry out the calculations for 512 add-on arrays in parallel using the GPU. The changes reduce the computer time by more than two orders of magnitude. The algorithm used to generate an optimized data set adds a specified number of new array configurations after each iteration to the existing set. The resolution of the optimized data set can be increased by adding a smaller number of new array configurations after each iteration. Although this increases the computer time required to generate an optimized data set with the same number of data points, the new fast numerical routines has made this practical on commonly available microcomputers.\\ud\n\\ud\n<\/dc:description><dc:publisher>\n        Wiley<\/dc:publisher><dc:date>\n        2010-12<\/dc:date><dc:type>\n        Publication - Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/nora.nerc.ac.uk\/id\/eprint\/12869\/1\/ParallelOpt.pdf<\/dc:identifier><dc:identifier>\n         \n\n  Loke, M.H.; Wilkinson, P.B.; Chambers, J.E..  2010  Parallel computation of optimized arrays for 2-D electrical imaging surveys.   Geophysical Journal International, 183 (3). 1302-1315.  https:\/\/doi.org\/10.1111\/j.1365-246X.2010.04796.x <https:\/\/doi.org\/10.1111\/j.1365-246X.2010.04796.x>     \n <\/dc:identifier><dc:relation>\n        http:\/\/onlinelibrary.wiley.com\/doi\/10.1111\/gji.2010.183.issue-3\/issuetoc<\/dc:relation><dc:relation>\n        10.1111\/j.1365-246X.2010.04796.x<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/nora.nerc.ac.uk\/id\/eprint\/12869\/","http:\/\/onlinelibrary.wiley.com\/doi\/10.1111\/gji.2010.183.issue-3\/issuetoc","10.1111\/j.1365-246X.2010.04796.x"],"year":2010,"topics":["Computer Science","Physics","Earth Sciences","Mathematics"],"subject":["Publication - Article","PeerReviewed"],"fullText":"Parallel computation of optimized arrays for 2-D electrical imaging surveys \nM. H. Loke1, P. B. Wilkinson2 & J. E. Chambers 2 \n1 Geotomo Software, 115 Cangkat Minden Jalan 5, Minden Heights, 11700 Gelugor, Penang, \nMalaysia. \n2 British Geological Survey, Natural Environment Research Council, Kingsley Dunham \nCentre, Keyworth, Nottingham, NG12 5GG, UK \n \nAccepted 2010 August 30. Received 2010 August 9; in original form 2010 January 25 \n \nAbbreviated title : Parallel computation of optimized arrays \n \nCorresponding author : M.H.Loke, 115 Cangkat Minden Jalan 5, Minden Heights, 11700 \nGelugor, Penang, Malaysia. Email : drmhloke@yahoo.com, Tel : +60 4 6574525, Fax : +60 4 \n6588437. \n \n \n \n \nPublished in Geophysical Journal International (Royal Astronomical Society \/ Blackwell \nPublishing). The definitive version is available at www.blackwell-synergy.com \n \nGeophysical Journal International (2010) 183, 1302-1315 \nhttp:\/\/onlinelibrary.wiley.com\/doi\/10.1111\/j.1365-246X.2010.04796.x\/pdf \n \n \nSummary \n Modern automatic multi-electrode survey instruments have made it possible to use \nnon-traditional arrays to maximize the subsurface resolution from electrical imaging surveys. \nPrevious studies have shown that one of the best methods for generating optimized arrays is \nto select the set of array configurations that maximizes the model resolution for a \nhomogeneous earth model. The Sherman-Morrison Rank-1 update is used to calculate the \nchange in the model resolution when a new array is added to a selected set of array \nconfigurations. This method had the disadvantage that it required several hours of computer \ntime even for short 2-D survey lines. The algorithm was modified to calculate the change in \nthe model resolution rather than the entire resolution matrix. This reduces the computer time \nand memory required as well as the computational round-off errors. The matrix-vector \nmultiplications for a single add-on array were replaced with matrix-matrix multiplications for \n28 add-on arrays to further reduce the computer time. The temporary variables were stored in \nthe double-precision SIMD registers within the CPU to minimize computer memory access. \nA further reduction in the computer time is achieved by using the computer graphics card \nGPU as a highly parallel mathematical coprocessor. This makes it possible to carry out the \ncalculations for 512 add-on arrays in parallel using the GPU. The changes reduce the \ncomputer time by more than two orders of magnitude. The algorithm used to generate an \noptimized data set adds a specified number of new array configurations after each iteration to \nthe existing set. The resolution of the optimized data set can be increased by adding a smaller \nnumber of new array configurations after each iteration. Although this increases the computer \ntime required to generate an optimized data set with the same number of data points, the new \nfast numerical routines has made this practical on commonly available microcomputers. \n \nKeywords : Parallel, computation, optimized, arrays, electrical, imaging \nINTRODUCTION \n In the past decade there have been many significant developments in the resistivity \nexploration method such that it is now one of the standard techniques used in engineering, \nenvironmental and mining surveys. Two-dimensional resistivity surveys are widely carried \nout, and even three-dimensional surveys are becoming more common in areas of very \ncomplex geology (Dahlin 1996; Auken et al. 2006; Chambers et al. 2006). The field \napplications range from agriculture (Samou\u00eblian et al. 2003), groundwater exploration \n(Seaton & Burbey 2000), engineering site investigation (Kuras et al. 2007), environmental \nassessment (Dahlin et al. 2002), mineral exploration (Bingham et al. 2006) to even \nhydrocarbon mapping (Bauman 2005). \nThe development of automatic multi-electrode survey instruments has made such \nsurveys fast and economical. It has also enabled the user to select the optimum array for the \nsurvey problem. Most surveys still use conventional arrays such as the Wenner, \nSchlumberger, pole-pole, pole-dipole, dipole-dipole and gradient (Dahlin & Zhou 2004). \nRecently there have been significant developments in algorithms to automatically select \narrays to maximize the resolution of the subsurface inversion model (Stummer et al. 2004). \nThe 'Compare R' method by Wilkinson et al. (2006b) that directly calculates the model \nresolution has proved to be the best of these methods (Loke et al. 2010). However the \n'Compare R' method had the disadvantage of requiring much more computer time compared \nto other faster but less accurate methods that use approximations of the model resolution. \nThus the main focus of this paper is on numerical and computational techniques to reduce the \ntime required by this method using commonly available personal computer systems as many \nelectrical imaging surveys are carried out by small geophysical companies.  \nThere is continuous demand for high-performance computing in the geophysical \nindustry (Sava 2010) as more sophisticated survey and data interpretation techniques are \ndeveloped to provide increasingly realistic models of the subsurface.  More recent trends \nplace less emphasis on increasing CPU speed that has reached a plateau of about 3 to 4 GHz \nfor common microprocessors.  There is now more emphasis on highly parallel computational \nmodels and more efficient memory to CPU data transfers (Camp and Thierry 2010) and the \nuse of non-conventional techniques such as GPU programming (Kadlec & Dorn 2010; \nMoorkamp et al. 2010) to achieve a greater level of performance.  There have been several \nrecent papers on high-performance computing techniques for seismic data processing which \nis the largest geophysical user of computer resources (Mich\u00e9a & Komatitsch 2009; Clapp & \nFu 2010). We discuss the use of similar techniques in the context of resistivity survey design \nin this paper. While some of the discussion is specific to the computer architectures used, the \ngeneral principles can be applied to other systems and potential field problems that use \nsimilar numerical matrix algorithms.  \nThis paper describes the numerical and computational techniques devised to reduce \nthe execution time of the optimization algorithm. Then a study is made of the optimum \nbalance between computer time and model resolution in calculating the optimized arrays. \nThis is followed by tests of the optimized arrays using a synthetic model. Finally, we \ncompare the use of the simple damped (Levenberg-Marquardt) and smoothness-constrained \nversions of the least-squares equation for generating the optimized data sets. \n \nTHEORY \n(a) Model resolution and point spread functions  \n The smoothness-constrained least-squares optimization method is frequently used for \n2-D inversion of resistivity data (Loke et al. 2003). The subsurface model usually consists of \na large number of rectangular cells. The linearized least-squares equation that gives the \nrelationship between the model parameters and the measured data is given below. \n( ) 1iTiT rCdJ\u2206rCJJ \u2212\u2212=+ \u03bb\u03bb ,      (1) \nThe Jacobian matrix J contains the sensitivities of the measurements with respect to the \nmodel parameters, C contains the roughness filter constraint, \u03bb is the damping factor and d is \nthe data misfit vector. ri-1 is the model parameter vector (the logarithm of the model \nresistivity values) for the previous iteration, while \u2206ri is the change in the model parameters. \nIt can be shown that the model resolution matrix R (Menke 1989) is given by \n( ) JJCJJR TT 1\u2212+= \u03bb .       (2) \nThe main diagonal elements of R that give an estimate of the model cells resolution have \nvalues of between 1.0 and 0.0. The sum of the elements in each row of the R matrix is equal \nto 1.0 (Jackson 1972). A model cell has perfect resolution if the resolution value is 1.0 and all \nother row elements of the R matrix are 0.0. In practice, the resolution values are less than 1.0 \nand decreases exponentially with depth (Loke et al. 2010).  The \u2018Compare R\u2019 method by \nWilkinson et al. (2006b) attempts to determine the set of array configurations that will \nmaximize the average resolution value for a homogeneous earth model.  \nSome authors (Friedel 2003; Miller & Routh 2007; Oldenborger & Routh 2009) have \nproposed the use of the point spread function as another measure of the resolution capability \nof the data. The point spread function for a model cell consists of the corresponding column \nof the resolution matrix. A spread criterion, that is a weighted sum of the elements of the \npoint spread function, is frequently used as it summarizes the information into a single \nnumber. The spread criterion (Miller & Routh 2007) value for the ith model cell, S(i), is \ngiven by the following equation. \n[ ] 2\/1\n1\n2\n1\n2\n),(\n),(\n)(\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fa\n\uf8fb\n\uf8f9\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8ef\n\uf8f0\n\uf8ee\n+\n\u2206\u2212\n=\n\u2211\n\u2211\n=\n=\n=\n=\nmj\nj\nj\nmj\nj\njijij\njiR\njiRW\niS\n\u03b4\u03b1\n\u03b4\n       (3) \nwhere ijij dW += 1  \n 1=\u2206 ij  for i=j, and 0=\u2206 ij  for i\u2260j.  \ndij is the normalized distance (distance divided by the unit electrode spacing) between \nthe centers of ith and jth model cells, \u03b1 is a small value (0.0001) and m is the number of \nmodel cells. \u03b4j is the (normalized) area of the jth model cell. We also use the average spread \ncriterion value, SCR, in this paper that is calculated using the following equation. \n\u2211\n=\n=\n=\nmj\nj\nCR iS\nm\nS\n1\n)(1         (4) \nIn a later section of this paper, plots of the spread criterion values are displayed together with \nthe model resolution values. \nIf the data errors are known the least-squares equation can be modified by a data \nweighting matrix (Menke 1989; Farquharson & Oldenburg 1998). Similarly, a weighted form \nof the roughness filter and Jacobian matrix is sometimes used to impose an L1-norm \nconstraint on the model roughness and data misfit (Farquharson & Oldenburg 1998) using the \niteratively reweighted least-squares method. However, for this paper, we choose the simpler \nform in equations (1) and (2) so that the results can be directly compared with previous work \nby Stummer et al. (2004) and Wilkinson et al. (2006b). For the same reason, we also use the \nsensitivity values for a homogeneous half-space (Loke & Barker 1995) in calculating the \nresolution and spread values. \n(b) Array optimization algorithm \nFor a system with N electrodes, there are N(N-1)(N-2)(N-3)\/8 independent four-\nelectrode configurations. To reduce the number of possible arrays, arrays of the Gamma type \nconfiguration (Carpenter & Habberjam 1956) with crossed current and potential electrodes as \nwell as those large geometric factors are excluded (Stummer et al. 2004). After excluding \nthese less stable configurations, a local optimization procedure is used to select a subset of \nthe comprehensive set of all the viable configurations that will maximize the model \nresolution. A small base data set consisting of the dipole-dipole configurations with the \u2018a' \ndipole length of 1 unit electrode spacing and \u2018n' dipole separation factor of 1 to 6 is initially \nselected. The change in the model resolution matrix R for each new array when added to the \nbase set is then calculated. The configurations that result in the largest increase in the model \nresolution, and have a suitable degree of orthogonality to the existing configurations, are then \nadded to the base data set (Wilkinson et al. 2006b). We also include the modification by Loke \net al. (2010) whereby for arrays that are not symmetrical about the center of the survey line, \nthe corresponding array configuration on the other half of the survey line is also included in \nthe optimized data set. This ensures that the distribution of data points (and thus the resulting \nmodel resolution section) is symmetrical. In each iteration the number of new configurations \nadded is normally set at about 9% of the present number in the base set. The model resolution \nfor the new base data set (after adding the new configurations) is recalculated using equation \n(2). This is repeated until the desired number of optimized array configurations is selected. \nFurther details on the optimization procedure are given in Wilkinson et al. (2006b) and Loke \net al. (2010). For the following discussion, we rewrite equation (2) into the following form. \nABR = , where JJA T= and  ( ) 1\u2212+= CJJB T \u03bb    (5) \n(c)  The original 'Compare R' method \n Wilkinson et al. (2006b) used the Sherman-Morrison Rank-1 update (Golub & van \nLoan 1989) to calculate the main diagonal elements of the model resolution matrix of the \nbase set plus the test configuration. The following set of updating formulae is used to \ncalculate the new resolution matrix Rb+1 when a new array is added to the base set  \nT\nb1b ggAA +=+ ,   \u00b5+\n\u2212=+ 1\nT\nb1b\nzz\nBB , 1b1b1b ABR +++ =   (6) \ng is the sensitivity vector for the new array, z=Bbg and \u00b5 = g.z. While this method proved to \nproduce the arrays with the highest model resolution, it was also the slowest taking about 6 \nhours on a 3 GHz PC to determine the optimum arrays for a 2-D survey line with 30 \nelectrodes (Wilkinson et al. 2006b). In order to use this method for longer 2-D survey lines, it \nis necessary to greatly improve its computational efficiency. The number of numerical \noperations required in equation (6) to calculate the diagonal elements of the updated model \nresolution matrix 1bR + for a single add-on array is proportional to m\n2\n where m is the number \nof model cells. For the survey line with 30 electrodes, the number of arrays in the \ncomprehensive data set is 51283. This increases to 166944 (40 electrodes), 411453 (50 \nelectrodes) and 854224 (60 electrodes). It is the huge number of possible add-on arrays in the \ncomprehensive data set that causes the long computer time needed. \nThis updating algorithm has three main steps; (i) calculate the updated elements for \nthe A matrix that is stored in a temporary matrix Ab+1, (ii) a similar calculation for the B \nmatrix that is stored in a temporary matrix Bb+1, (iii) finally multiply the two temporary \nmatrices. In the implementation used by Wilkinson et al. (2006b), updating the A matrix took \nabout 32% of the total time for one iteration. Updating the B matrix (including calculating z \nvector) took about 40%, the matrix multiplication (for the diagonal entries only) took about \n27% and the remaining 1% was used for miscellaneous operations such as finding the add-on \narrays that gave the largest increase in the model resolution. \nThe CPU in modern computer systems can operate at a much higher speed compared \nto the main memory. As an example, in the computer system used in this work, the CPU \noperates at a frequency of 2.66 GHz while the main memory (RAM) runs at 533 MHz. Thus \nthe time taken to transfer data between the main memory and the CPU can be much longer \ncompared to the time taken for the numerical operations within the CPU. The calculations \nwere carried out on a 2.66 GHz Intel i7 Quad-Core system (with 12 GB RAM) with a Nvidia \nGTX 285 graphics card (with 1 GB RAM). In this test, we use the same damped least-squares \nformulation (C=I) and damping factor (\u03bb=0.000025) as that used by Wilkinson et al. \n(2006b). \n(d) The matrix-vector method \nThe first step to improving the program efficiency is to reduce the traffic between the \nmain memory and the CPU. In order to achieve this, Loke et al. (2010) expanded equation (6) \nfor Rb+1 into the following form. \n,bb1b RRR \u2206+=+ where  \nT\nT\nT\nbb\nT\nb gg\nzz\nggBAzzR\n\u00b5\u00b5 +\n\u2212+\n+\n\u2212=\u2206\n11\n  (7) \nIn this paper, we further simplify the above equation by making using of the fact that z=Bbg \nand \u00b5 = g.z as follows. \n( )TbTTTTbTTTbTb gAzzgzgzzgAzzgzgzAzzR +\u2212\n+\n=\n+\n+\u2212+\n+\n\u2212=\n+\n\u2212+\n+\n\u2212=\u2206\n\u00b5\u00b5\u00b5\u00b5\n\u00b5\n\u00b5 11111\n \nThis equation can be further reduced to \n( )TTb ygzR \u2212\n+\n=\u2206\n\u00b51\n       (8) \nwhere y=Abz (note that A and B are symmetric matrices). Equation (8) only involves vector-\nvector multiplication and subtraction. The bulk of the numerical calculations are in the matrix \nvector products Bbg and Abz required to generate the z and y vectors. The simplification of \nequation (7) to (8) further reduces the calculation time by about 5%. \nIn equation (8) the change in the resolution matrix \u2206Rb is calculated rather then the \nentire updated resolution matrix Rb+1. The main diagonal elements of \u2206Rb are calculated one \nby one using equation (8), thus avoiding the use of the temporary matrices Ab+1 and Bb+1. \nThis reduces the computer time and memory required. The number of times the main \nmemory is accessed in equation (8) is drastically reduced. It is only necessary to access the \nmatrices Ab and Bb (with m2 elements each) once to calculate the vectors z and y (each with \nonly m elements) for a single add-on test configuration. The computer time can be further \nreduced by making use of the parallel processing capabilities of modern CPUs. \nThe computer code was optimized so that all the calculations needed to update the \nmodel resolution values can be carried out \u2018in situ\u2019 within the CPU floating point registers to \nreduce the traffic between the CPU and main memory to a minimum. The time critical parts \nof code were written in assembly language (Leiterman 2005) so that the use of the available \nfloating point CPU registers could be directly hand optimized. The SIMD (Single Instruction \nMultiple Data) registers in the Intel CPU are used for the floating point calculations (Gerber \n2002). Each SIMD register can store two double precision values. The SIMD instructions that \ncan carry out two double-precision operations with a single instruction are used.  This allows \nthe calculations of the updated resolution matrix values for two new test configurations to be \ncarried out simultaneously.  The calculations for several pairs of the test configurations are \nthen also carried out in parallel by using the multiple cores in modern CPUs (Chandra et al. \n2001; Chapman et al. 2008). The Intel i7 (Nahelem) processor has four physical cores, but it \nhas a hyper-threading capability where each physical core can be used as two logical \nprocessors (Gerber 2002). The hyper-threading function can reduce the calculation time by \nup to 30%.  \nIn this study, the Intel i7 CPU was programmed as an eight cores processor. Together \nwith the use of the SIMD registers, this means that the resolution matrices for 16 test \nconfigurations are calculated in parallel. Thus it is only necessary to transfer the elements of \nthe Ab and Bb matrices once from the computer memory to the CPU for 16 test \nconfigurations. On the 2.66 GHz Intel i7 system, the computational time required for a survey \nline with 30 electrodes was reduced to about 200 seconds (Table 1). \nTable 1 here. \n(e) The matrix-matrix method \n The bulk of the numerical calculations involve matrix-vector multiplications of the \nform z=Bbg and y=Abz. A single matrix-matrix multiplication is more efficient than a series \nof equivalent matrix-vector multiplications (Dongarra et al. 1998). The next step is to \ncalculate the change in the resolution matrix elements for a large number of add-on arrays at \nthe same time using the following equations.  \n GBZ b=  and ZAY b= , where [ ]kgggG ....21=  and [ ]k....zzzZ 21=   (9) \nThe columns of the matrix G  consist of the sensitivity vectors gi for k different test array \nconfigurations. The optimum value for k was found to be 28 for 64-bit Intel CPUs with 16 \nSIMD registers (Leiterman 2005).  In the matrix-vector method described previously, a single \nSIMD register (which can store two double precision values) was used to for the calculation \nof two z vectors with a single transfer of the elements of the Bb matrix from the memory to \nthe CPU.  The matrix-matrix method essentially carries out this optimization further by using \n14 SIMD registers for the calculation of 28 z vectors. Using the eight (virtual) cores of the i7 \nprocessor, the calculations for 224 z vectors can be carried out for a single transfer of the \nelements of the Bb matrix from the memory to the CPU. A similar optimization is made for \nthe calculation of the Y matrix. It is only necessary to transfer each element of the A (or B) \nmatrix from the computer memory to the CPU once where it can be used 224 times for the \nsame calculations involving different add-on array configurations. Once a data value is \ntransferred from the memory to the CPU, it is stored in a high speed internal data cache that \ncan be accessed by the multiple CPU cores. This reduces the time taken by the memory \ntransfer for these matrices by a factor of about 200 times. The matrix-matrix version of the \nupdating formula reduces the calculation time for the 30 electrodes example by more than \nhalf to 87 seconds (Table 1). An examination of the times taken by the different program \nroutines show that 76% of time is used in calculating the matrix-matrix multiplications in \nequation (9) for the 30 electrodes test (and 92% for the 50 electrodes test). Thus the next step \nis to reduce the time taken by the matrix-matrix multiplications. \n(f) The GPU matrix-matrix method \nThe Intel CPU used has 4 physical cores but the Graphics Processor Unit (GPU) in the \nNvidia GTX 285 graphics card has several hundred parallel computational units (Owens et al. \n2007; Nvidia 2008). The GPU is limited to simpler numerical operations than the CPU but it \nis well suited for matrix-matrix calculations. The calculations for 512 configurations can be \ncarried out simultaneously (i.e. k in equation (9) is now 512) using the GPU. This reduces the \ncalculation time for the 30 electrodes example to 50 seconds (Table 1). An examination of the \ntimes taken by the different subroutines in the program reveal that the major part is taken in \ntransferring the data between the main computer (CPU) memory and the graphics card (GPU) \nmemory. It takes about 60% of the overall time taken by the program, whereas the numerical \ncalculations within the GPU for equation (9) take only about 0.2% of the overall time. The \nmain bottleneck for the GPU program version is now the transfer rate of the data between the \nCPU memory and the GPU memory. The PCI-E 2.0 (Peripheral Component Interconnect \nExpress) graphics card bus in the computer system used has a transfer rate of 500 MB\/s. \nWhen the program was tested on an older computer system with a PCI-E 1.0 bus (that has a \ntransfer rate of 250 MB\/s), the time taken for the data transfer was almost doubled. This \nconfirms that the transfer rate of data between the CPU memory and the GPU memory is the \nmain limiting factor. It is only necessary to transfer the elements of the A and B matrices \nfrom the CPU memory to the GPU memory once in each iteration. However, the sensitivity \nvectors for different sets of the array configurations in the comprehensive data set have to be \ntransferred repeatedly from the CPU memory to the GPU memory. Similarly, the results of \nthe matrix-matrix products Z and Y are transferred from the GPU memory to the CPU \nmemory for each set of 512 array configurations. \n(g) The single-precision GPU matrix-matrix method \nThe following function FCR is used to the rank the improvement in the model \nresolution due to an add-on array. \n\u2211\u2211\u2211\n=\n=\n=\n=\n=\n=\n+ \u2206+=\n\uf8fa\n\uf8fb\n\uf8f9\n\uf8ef\n\uf8f0\n\uf8ee \u2206\n+==\nmj\nj b\nb\nmj\nj b\nb\nmj\nj b\nb\nCR jjR\njjR\nmjjR\njjR\nmjjR\njjR\nm\nF\n111\n1\n),(\n),(11),(\n),(\n11),(\n),(1\n  (10) \nThe change in the resolution \u2206Rb(j,j) can be several orders of magnitude smaller than \nresolution value Rb(j,j). Thus equation (8) is less sensitive to round-off errors compared to the \ndirect use of equation (6).  The calculations for \u2206Rb are next carried out in single-precision in \nthe GPU to further reduce the computer time. Table 1 gives the computer time and average \nrelative model resolution ratios achieved by the different versions of the 'Compare R' method. \nThe average relative model resolution is given by ( ) ( )iiRiiR\nm\nS c\nmi\ni\nbr ,\/,\n1\n1\n\u2211\n=\n=\n=  where Rb(i,i) \nand Rc(i,i) are the base and comprehensive data set model resolutions of the ith cell for a \nmodel with m cells. The GPU single-precision version is about twice as fast as the double-\nprecision version while differences in the average relative model resolutions are less than 1%. \nTo put the numerical improvements made in perspective, the double-precision and single-\nprecision GPU versions respectively take 50 and 29 seconds compared to 6 hours (21600 \nseconds) in the original version by Wilkinson et al. (2006b) for the 30 electrodes example. \nThe computer time has been reduced by more than two orders of magnitude. \nFigure 1a shows the change in the average relative model resolution with iteration \nnumber for survey lines with 30 to 60 electrodes. In general, there is an initial rapid increase \nin the model resolution followed by a slower increase. Figure 1b shows a plot of the average \nrelative model resolution values against the ratio of the number of arrays in the optimized \ndata set to the total number in the comprehensive data set. The optimized data sets for \ndifferent survey lines have similar average relative resolution values when the ratio exceeds \n1.5%.  Table 1 also gives the total number of array configurations generated after 40 \niterations. For a survey line with 30 electrodes, there are 4618 optimized arrays \nconfigurations. This is much higher than that normally collected in most field surveys where \nabout 200 to 500 data points are usually collected using conventional arrays (Wilkinson et al. \n2006a; Loke et al. 2010). Thus we next examine the use of the array optimization method for \ngenerating smaller optimized data sets. In the following tests, the double-precision GPU \nversion of the routines is used. \nFigure 1 here. \nRESULTS \n(a) Optimizing the number of add-on arrays \n In this section, we attempt to find the maximum average model resolution that can be \nachieved for a given number of array configurations by modifying one of the parameters in \nthe local optimization method used to generate the optimized array data sets. The Sherman-\nMorrison update calculates the change in the model resolution for a single add-on array. As \nthe change in the model resolution depends on the existing base data set (through the A and B \nmatrices that in turn depend the J Jacobian matrix), this method is only guaranteed to \ncorrectly identify the add-on array that gives the largest change in the resolution values. Thus, \nin theory, the optimal method to generate the data set is to add only the array (or symmetrical \npair of arrays) that gives the largest increase in the model resolution values to the base data \nset after each iteration. However, this approach is very expensive in terms of computer time, \nparticularly for the longer survey lines. Stummer et al. (2004) and Wilkinson et al. (2006a; \n2006b) increased the size of the optimized data set by 9% after each iteration.  \nThe tremendous reduction in the computer time achieved by the techniques described \nin the previous section now makes it possible to use a smaller step size when augmenting the \noptimized data set after each iteration. The calculations are made for a survey with 30 \nelectrodes. Figure 2a shows the change in the average relative model resolution when the size \nof the base data set is increased by 3, 4.5, 6 and 9 % respectively after each iteration for up to \nabout 800 data points (since our interest is in generating small optimized data sets). The \ncurves start to converge when the number of data points exceeds 700. However for less than \n600 data points there are significant differences in the model resolution achieved when \ndifferent step sizes are used. The data set generated with 3% step size has a significantly \nhigher resolution value compared to that generated with the 9% step size. The resolution \ncurves for the 4.5 and 6 % step sizes lie in between the two curves. \nFigure 2 here. \nWe also present results using a new \u2018single step\u2019 algorithm where only the array that \ngives the largest increase in the average relative model resolution is added to the base set \nafter each iteration. In most cases when the array is not symmetrical about the center of the \nsurvey line, the corresponding array on the other half of the survey line is also added. In \ntheory, both arrays give the same increase in the model resolution values when the \ndistribution of the data points in the starting base set is symmetrical. Thus for most iterations, \ntwo arrays are added to the base set in each iteration in the \u2018single step\u2019 method. The \u2018single \nstep\u2019 method produce array sets that have the highest average relative model resolution \nvalues (Figure 2a). It represents an upper bound for the model resolution (for the same \nnumber of data points) that can be achieved by the array optimization algorithm. \nFigure 2b shows the change in the average spread criterion value, SCR, with the \nnumber of data points. There is an initial rapid decrease in the average spread value followed \nby a slower decline after about 400 data points. The curves generated with the smaller step \nsizes have significantly lower average spread values for less than 500 data points after which \nthe curves tend to converge. In general the average spread curves tend to mirror the behaviour \nof the average resolution curves with higher resolution values corresponding to lower spread \nvalues. \nTable 2 lists the number of iterations and computer time required needed to generate \nan optimized data set with 400 data points using the different step sizes. The average relative \nmodel resolution and spread criterion values are also given. The left side of Figure 3 shows \nthe relative model resolution sections that give more information about the performance of \nthe algorithm with different step sizes. The largest difference is in the lower part of the \nsections. The initial base set consisting of the 147 dipole-dipole arrays with a dipole length of \n1 meter and the \u2018n\u2019 factor of 1 to 6 has low relative model resolution values of less than 0.5 \nbelow the top 3 meters (Figure 3a). The section for the optimized data set generated with a \nstep size of 9% exhibits low resolution values below the depth of 6.4 meters, particularly \naround the 15 meters mark near the center (Figure 3b). This low resolution patch is \nsignificantly reduced when the step size is reduced to 6%. Reducing the step size to 4.5% and \nto 3% further increases the resolution values in the lower part of the model sections. Almost \nthe entire section has relative resolution values of above 0.7 when the 3% or \u2018single step\u2019 \nsizes are used. Although the time taken to generate 400 data points with the 3% step about 3 \ntimes that required with a 9% step size, the model resolution is significantly higher. The \nnumber of iterations (and computer time) required by the \u2018single step\u2019 method is more than 4 \ntimes that required by a step size of 3%.  \nTable 2 here. \nFigure 3 here. \nThe right side of Figure 3 shows the spread criterion sections for the initial base set \nand the optimized data sets. The initial base set (Figure 3b) has large spread values below the \nfirst few meters reaching up to 108.3 at the bottom left and right corners of the section. The \noptimized data set with a 9% step size has much lower spread values with a maximum value \nof about 9.2. The spread values near the bottom of the model section are progressively \nreduced as the step size is reduced. The maximum spread value is reduced to 8.2 with a 3% \nstep size, and to 8.0 with a \u2018single step\u2019 size. There is a close correspondence between \nincreasing model resolution and decreasing spread values. \nSimilar tests were carried out for surveys lines with up to 60 electrodes. As an \nexample, Figure 4a shows the change in the average relative model resolution with number of \ndata points using the different step sizes for a survey line with 50 electrodes. There is a \nsignificant increase in the resolution when the step size is reduced from 9% to 6% \nparticularly for less than 2000 data points. The gap between the two curves become smaller \nas the data set size increases from 2000 to 2500 data points. There are smaller, but still \nsignificant, increases in the model resolution when the step size is reduced to 4.5% and 3%. \nThe \u2018single\u2019 step method achieves significantly higher model resolution values compared to \nthe 3% step size up to about 1500 data points. The average spread value curves (Figure 4b) \nshow a similar pattern with the \u2018single step\u2019 method having the lowest spread values and the \n9% step size having the highest spread values with all the curves converging above 2000 data \npoints. \nFigure 5 shows the relative resolution and spread criterion value sections for the initial \nbase set (with 267 data points) and the optimized data sets (with 1000 data points) generated \nusing the different step sizes. The initial base set with 267 dipole-dipole array data points has \nlow relative model resolution values of less than 0.5 below the top 3 meters (Figure 5a). For \nthe optimized data sets with 1000 data points, the relative model resolution sections show that \nmost of the region below a depth of about 6 meters has values of less than 0.6 when a step \nsize of 9% is used (Figure 5b), but has values of above 0.65 with a step size of 3% (Figure \n5e). The average relative resolution value is increased to 0.768 when a \u2018single step\u2019 size is \nused, compared to 0.662 and 0.733 with step sizes of 9% and 4.5%. The spread criterion \nsections show a gradual reduction in the spread values, particularly in the lower portion of the \nmodel sections, as the step size is reduced (right column of Figures 5).  Again, the greatest \nbenefit of using a smaller step size is an improvement in the resolution of the lower part of \nthe model section. The price of the higher resolution is much higher computer times required \nto generate the optimized data sets with the smaller step sizes (Table 3), particularly for the \n\u2018single step\u2019 method.  The \u2018single\u2019 step size method took 3.8 hours to generate the optimized \ndata set with 1000 data points, compared to 17 and 26 minutes using step sizes of 4.5% and \n3%. The use of a step size of between 3% and 4.5% probably represents the best compromise \nbetween maximizing the model resolution and reducing the calculation time for such long \nsurvey lines. \nTable 3 here. \nFigure 4 here. \nFigure 5 here. \n(b) Synthetic model inversion test \nFigure 6a shows a test model with 4 rectangular blocks at different depths in a \nbackground medium of 10 \u2126.m below a 2-D survey line with 35 electrodes 1 meter apart. \nThree of the blocks have 100 \u2126.m resistivity. One block has a gradational boundary rising \nfrom 20 to 100 \u2126.m to simulate a smooth edge. The horizontal distance between the edges of \nthe third and fourth deepest blocks is less than the depth to the deepest block. This makes it a \nmore challenging test in separating the two deepest blocks compared to earlier test models \nused by Wilkinson et al. (2006a; 2006b) and Loke et al. (2010). Figures 6b and 6c show the \napparent resistivity pseudosections for the Wenner-Schlumberger (Pazdirek & Blaha 1996) \nand dipole-dipole arrays. For both arrays, we use all the possible measurements subject to the \nrestriction that the geometric factor does not exceed that that for a dipole-dipole array with 'a' \nequal to 1 m and 'n' equal to 6 (i.e. a geometric factor of 1056 m). This results in data sets \nwith 599 and 530 data points respectively for the Wenner-Schlumberger and dipole-dipole \narrays. The apparent resistivity pseudosection for the optmized array data set (using a 4.5% \nstep size) with 599 data points is shown in Figure 6d for comparison. The contour pattern in \nthe pseudosection has a more jagged appearance compared to the conventional arrays. This is \nbecause the data consists of a mixture of arrays of the Alpha and Beta types (Carpenter & \nHabberjam 1956) that have different responses to the subsurface resistivity. This is illustrated \nby the differences in the pseudosections of the Wenner-Schlumberger (an Alpha type) and \ndipole-dipole (a Beta type) arrays.  \nFigure 6 here. \nThe smoothness-constrained Gauss-Newton least-squares optimization method used \nfor the inversion of the data sets is described in Loke and Dahlin (2002) and Loke et al. \n(2003). We use the \u2018discrepancy principle\u2019 technique (Farquharson and Oldenburg 2004) in \nselecting the damping factor. A relatively large damping factor value is initially used (usually \nabout 0.10 to 0.30) that is reduced by half after each iteration until the desired data misfit \nvalue is obtained. We choose a target data misfit of 0.5% for the data sets without noise that \nis similar to the accuracy of the finite-difference forward modelling used. We show the \ninversion model for the first iteration where the data misfit falls below 0.5% for the noise-free \ntest data sets. The final damping factor used at this iteration is usually between 0.005 and \n0.010. The smoothness-constrained inversion method using an L1-norm inversion was used \n(Loke et al. 2003). The same inversion settings were used for the different data sets. \nThe two upper blocks in the resulting inversion model for the Wenner-Schlumberger \ndata set after 6 iterations are fairly well resolved while the third deepest block is barely \nresolved (Figure 6e).  For the dipole-dipole data set, the third deepest block is fairly well \nresolved in the inversion model (Figure 6f). The deepest block shows up as an area of higher \nresistivity but it is not well resolved. The anomaly corresponding to this block is just barely \nseparated from the third deepest block. \n The next test is with the optimized data sets (with 599 data points) using step sizes of \n9% and 4.5%. The previous section showed that using a smaller step size can significantly \nimprove the resolution. In the first optimized data set model (with a step size of 9%) the third \ndeepest block is well resolved with a maximum resistivity of about 31 \u2126.m, and the deepest \nblock is now visible (Figure 6g) and clearly separated from the third block. The deepest block \nis even more clearly resolved (with a maximum resistivity of 17 \u2126.m) in the inversion model \nfor the optimized data set using a step size of 4.5% (Figure 6h). This agrees with the earlier \nobservation that the data set using a smaller step size has significantly better resolution in the \nlower part of the model section. The optimized data sets using step sizes of 9% and 4.5% \nhave average model resolution ratios of 0.770 and 0.804 respectively. \n Figure 7 shows the results of tests when noise is added to the data sets. Zhou & \nDahlin (2003) demonstrated that the error in resistivity field measurements varies inversely \nwith the measured potential value. Gaussian random noise (Press et al. 1992) is added to the \npotential values (for a current of 1 Ampere) for the different array configurations to simulate \nsuch potential dependent noise. The potential values are then converted to apparent resistivity \nvalues by multiplying with the geometric factor. The amplitude of the potential noise is \nchosen so that the readings with the lowest potential (and also the largest geometric factor) \nhave a noise level of 10 percent. The average percentage apparent resistivity noise depends \non the geometric factors of the array configurations in the data set used. As an example, \nFigure 7a shows the dipole-dipole array data set with overlapping data levels with the \npotential dependent noise added. This should be compared to Figure 6c that shows the same \npseudosection without noise. The largest differences between the two pseudosections are in \nthe lower sections that correspond to measurements with the larger geometric factors. The \ninversion model for the Wenner-Schlumberger array (Figure 7b) shows small changes \ncompared to the model without the noise (Figure 6e). This is because the array is relatively \ninsensitive to noise. The average geometric factor for this data set is 123 m. In comparison, \nthe dipole-dipole array data set that has an average geometric factor of 322 m is more \nsensitive to noise. This is also reflected in the data misfits of 0.9% for the Wenner-\nSchlumberger model and 1.9% for the dipole-dipole array model. The lower part of the \ndipole-dipole array model where the deepest block is located has significant distortions \n(Figure 7c). The high resistivity zone corresponding to the deepest block is barely visible, \nunlike the model obtained for the same data set without noise (Figure 6f). The optimized data \nset with a 4.5% step size has a higher data misfit of 3.1% due to the higher average geometric \nfactor of 569 m. The deepest block is still resolved (Figure 7d) although there is a slight shift \nin its position to the left. \n Figures 7e and 7f show the inversion models for the dipole-dipole array and optimized \ndata sets using the L-curve method (Farquharson and Oldenburg 2004) to automatically select \nthe damping factor. While there are slight differences in the models obtained using the \n\u2018discrepancy principle\u2019 method (Figures 7c and 7d), the models (Figures 7e and 7f) clearly \nshow that the deepest block is much better resolved by the optimized data set although the L-\ncurve method selected a lower damping factor value (0.008) for the dipole-dipole array data \nset compared to the optimized data set (0.013). The lower damping value selected by the L-\ncurve method is probably due to the lower average noise level in the dipole-dipole data set \n(due to its lower average geometric factor compared to the optimized data set). The damping \nfactor for the optimized data set selected by the L-curve method is slightly higher (0.013 \ncompared to 0.010) than that used by the model in Figure 7d. This results in slightly lower \nresistivity values for the deepest block (comparing Figures 7f and 7d) but the model also has \nfewer artefacts (such as the low resistivity area under the second upper block) due to the \nnoise. \nFigure 7 here. \n(c) Array optimization using the smoothness constraint \n In calculating the model resolution values, and consequently the optimized arrays, we \nhave used the simple damped (Levenberg-Marquardt) least-squares method by setting the C \nmatrix in equation (1) to be equal to the identity matrix I. This choice was made so that the \nresults can be directly compared with earlier work by Wilkinson et al. (2006a; 2006b). \nHowever most 2-D resistivity inversion work use a smoothness-constrained least-squares \nmethod (deGroot-Hedlin & Constable 1990; Loke at al. 2003) where a roughness filter is \nused to minimize changes in the resistivity between adjacent model cells. The C matrix is \nthen given by the following equation. \n X\nT\nXZ\nT\nZ ddddC +=          (11) \nThe roughness matrices Xd and Zd  differences the model parameters between adjacent \nlateral and vertical model cells. The structures of these matrices are described in the paper by \ndeGroot-Hedlin & Constable (1990).  \n Figure 8a shows the relative model resolution section for the small optimized data set \nwith 599 data points with a step size of 4.5% used in the previous section that was generated \nusing the damped least-squares equation. For comparison, Figure 8c shows a similar relative \nmodel resolution section obtained using the smoothness-constrained least-squares equation. \nNote the high relative model resolution values of over 0.95 are more highly concentrated near \nthe surface for the damped least-squares section compared to the smoothness-constrained \nleast-squares section.  The smoothness-constrained section has a more uniform distribution of \nthe high relative resolution values (with significantly higher values in the bottom half) \ncompared to the damped least-squares section. A second test was carried out using a larger \noptimized data set with 2401 data points that is about four times the size of the small \noptimized data set. The relative model resolution section for the smoothness-constrained \nmethod (Figure 8d) has slightly higher values in the lower part of the model section \ncompared to that obtained with the damped least-squares method (Figure 8b).  The \ncorresponding spread criterion sections are shown on the right side of Figure 8. There is a \ndecrease in the average spread value when the number of data points is increased for both the \ndamped constraint (Figures 8e and 8f) and smoothness constraint (Figures 8g and 8h). The \nsmoothness constraint sections have slightly higher average resolution and lower spread \nvalues than the damped constraint sections probably due to differences in the C matrix.  \nFigure 8 here. \nThe inversion models for the small optimized data sets generated by both methods are very \nsimilar (Figures 9a and 9c) with no significant differences. The deepest block in the model \nobtained from the inversion of the large optimized data set has a maximum value of 20 \u2126.m \n(Figure 9b). This is slightly higher than the maximum value of 17 \u2126.m in the inversion model \nof the small optimized data set (Figure 9a). The inversion model for the large optimized data \nset generated using the smoothness-constrained least-squares method achieves a higher value \nof 21 \u2126.m at the location of the deepest block. The base of the block is also slightly better \nresolved (Figure 9d) compared to the model for large optimized data set with the damped \nconstraint (Figure 9b) This could be due to the slightly higher relative model resolution \nvalues in the lower part of the smoothness-constrained model resolution section. Note that the \nthird deepest block is significantly better resolved by both large optimized data sets (where it \nreaches a maximum value of about 46 \u2126.m) compared to the small optimized data sets \n(maximum value of 36 \u2126.m).  \nFigure 9 here. \n \nCONCLUSION \n Optimized arrays generated by maximizing the model resolution have significantly \nbetter resolution and greater depth of investigation than conventional arrays for 2-D \nresistivity surveys. The computer time required to generate the optimized arrays is greatly \nreduced by using numerical algorithms that can make the best use of the CPUs and GPUs in \nmodern personal computer systems. The computer time was reduced through three steps. \nFirstly, the equation used to calculate the change in the model resolution for a single add-on \narray was simplified so that the final stage only involves vector-vector operations. Secondly, \nthe computer program was optimized at the CPU level by reducing the time taken to transfer \ndata between the main computer memory and the CPU registers through the use of matrix-\nmatrix multiplication algorithms, by storing the temporary variables in the CPU floating point \nregisters, and by using the parallel processing capabilities of modern CPUs.  A final reduction \nin the computer time is achieved by using the graphics card GPU as a highly parallel \nmathematical coprocessor.  \nThe resolution for small data sets can be significantly improved by using a smaller \nstep size for adding new configurations in the iterative algorithm used to generate the \noptimized data sets. For small optimized data sets the algorithm is largely insensitive to the \ntype of model constraint used in the optimisation; the simple damped and smoothness-\nconstrained least-squares methods generally gave similar results in terms of the quality of the \ninversion models obtained. For larger optimized sets (where the number of data points is \nseveral times larger than used in conventional arrays), using the smoothness constraint can \ngive slightly better resolution at depth. \n While the discussion in this paper is focused on the array optimization problem, the \ntechniques developed in this research can also be used to reduce the computer time required \nfor other aspects of electrical and electromagnetic data interpretation (such as solving the \nleast-squares equation, calculation of the model resolution matrix and calculating the forward \nmodel response using finite-element and finite-different techniques) that use similar matrix \nand vector operations. \n Further research is being carried out to reduce the time required by the matrix-matrix \nGPU routines to transfer data between the CPU memory and the GPU memory. We are also \ntesting a modified \"Compare R\" algorithm that minimizes the spread function instead of \nmaximizing the model resolution. Research is also being undertaken on array optimization \nfor 3-D surveys that has been made possible with the fast algorithms described in this paper. \nEllis & Oldenburg (1994) showed that the smoothness-constrained least-squares inversion \nmethod can be modified to take into account a-priori information so as to improve its \nresolution in selected regions. We are currently investigating using similar modifications to \nthe least-squares equation to improve the resolution of the optimized data sets. Other aspects \nof the array optimization problem such as the effects of different model discretizations, the \ndata noise distribution and L1-norm constraints are also being studied.  \nACKNOWLEDGEMENTS \nThis paper is published with permission of the Executive Director of the British Geological \nSurvey (NERC).  We would also like to thank the Associate Editor and two anonymous \nreviewers for their comments that have helped to improve the paper. \n \nREFERENCES \nAuken, E., Pellerin, L., Christensen, N.B. & S\u00f8rensen, K., 2006. A survey of current trends in \nnear-surface electrical and electromagnetic methods, Geophysics, 71,  G249-G260.  \nBauman, P., 2005. 2-D resistivity surveying for hydrocarbons \u2013 A primer, CSEG Recorder, \nApril 2005, 25-33. \nBingham, D., Nimeck, G., Wood, G. & Mathieson, T., 2006. 3D resistivity in the Athabasca \nbasin with the pole-pole array, In 1 day workshop - Geophysical methods and \ntechniques applied to uranium exploration proceedings. SEG Annual General \nMeeting 2006, New Orleans. \nCamp, W. & Thierry, P., 2010. Trends for high-performance scientific computing, The \nLeading Edge, 29 (1), 44-47. \nCarpenter, E.W. & Habberjam, G.M., 1956. A tri-potential method of resistivity prospecting, \nGeophysics, 11, 455-469. \nChambers , J.E., Kuras, O., Meldrum, P.I., Ogilvy, R.O. & Hollands, J., 2006. Electrical \nresistivity tomography applied to geologic, hydrogeologic, and engineering \ninvestigations at a former waste-disposal site, Geophysics, 71, B231-B239.  \nChandra, R., Dagum, L., Kohr, D., Maydan, D., McDonald, J. & Menon, R., 2001. Parallel \nProgramming in OpenMP, Academic Press, San Diego, California. \nChapman, B., Jost, G. & van der Pas, R., 2008. Using OpenMP, The MIT Press, Cambridge, \nMassachusetts. \nClapp, R. & Fu, H., 2010. Selecting the right hardware for reverse time migration, The \nLeading Edge, 29 (1), 48-59. \nDahlin, T., 1996. 2D resistivity surveying for environmental and engineering applications, \nFirst Break, 14, 275-284. \nDahlin T., Bernstone C. & Loke M.H. 2002. A 3D resistivity investigation of a contaminated \nsite at Lernacken in Sweden, Geophysics, 60, 1682-1690. \nDahlin, T. & Zhou, B., 2004. A numerical comparison of 2D resistivity imaging with 10 \nelectrode arrays, Geophysical Prospecting, 52, 379-398. \ndeGroot-Hedlin, C. & Constable, S., 1990. Occam's inversion to generate smooth, two-\ndimensional models form magnetotelluric data, Geophysics, 55, 1613-1624. \nDongarra, J.J., Duff, I.S., Sorensen, D.C. & van der Vorst, H.A., 1998. Numerical linear \nalgebra for high-performance computers, Society for Industrial and Applied \nMathematics, Philadelphia. \nEllis, R.G. & Oldenburg, D.W., 1994. Applied geophysical inversion, Geophysical Journal \nInternational, 116, 5-11. \nFarquharson, C.G. & Oldenburg, D.W., 2004. A comparison of automatic techniques for \nestimating the regularization parameter in non-linear inverse problems, Geophysical \nJournal International, 156, 411-425. \nFriedel, S., 2003. Resolution, stability and efficiency of resistivity tomography estimated \nfrom a generalized inverse approach, Geophysical  Journal International, 153, 305\u2013\n316. \nGerber, R., 2002. The Software Optimization Cookbook : High-performance recipes for the \nIntel architecture, Intel Press,  Hillsboro, Oregon  \nGolub, G.H. & van Loan, C.F., 1989. Matrix Computations, 2nd edn, The John Hopkins \nUniversity Press, Baltimore and London. \nJackson, D.D., 1972. Interpretation of inaccurate, insufficient and inconsistent data, \nGeophysical Journal of the Royal Astronomical Society, 28, 97\u2013109. \nKadlec, B. & Dorn, G., 2010. Leveraging graphics processing units (GPUs) for real-time \nseismic interpretation, The Leading Edge, 29 (1), 60-67. \nKuras, O., Meldrum, P.I., Beamish, D., Ogilvy, R.D. & Lala, D., 2007. Capacitive Resistivity \nImaging with Towed Arrays, Journal of Environmental and Engineering Geophysics, \n12 (3), 267-279. \nLeiterman, J.C., 2005. 32\/64-bit 80x86 assembly language architecture, Wordware \nPublishing, Inc. \nLoke M.H. & Barker, R.D., 1995. Least-squares deconvolution of apparent resistivity \npseudosections, Geophysics, 60, 1682-1690. \nLoke, M.H. & Dahlin, T., 2002. A comparison of Gauss-Newton and quasi-Newton methods \nin resistivity imaging inversion, Journal of Applied Geophysics, 49, 149-162. \nLoke M.H., Acworth I. & Dahlin T., 2003. A comparison of smooth and blocky inversion \nmethods in 2D electrical imaging surveys, Exploration Geophysics, 34, 182-187. \nLoke, M.H., Wilkinson, P. & Chambers, J., 2010, Fast computation of optimized electrode \narrays for 2D resistivity surveys, Computers and Geosciences, in press. \nMenke, W., 1989. Geophysical data analysis: Discrete inverse theory, Rev. ed, Academic \nPress, Inc. \nMich\u00e9a, D. & Komatitsch, D., 2009. Accelerating a three-dimensional finite-difference wave \npropagation code using GPU graphics cards. Geophysical Journal International, 182, \n389-402. \nMiller, C.R. & Routh, P.S., 2007. Resolution analysis of geophysical images: Comparison \nbetween point spread function and region of data influence measures, Geophysical \nProspecting, 55, 835\u2013852. \nMoorkamp, M., Jegen, M., Roberts, A. & Hobbs, R. 2010. Massively parallel forward \nmodeling of scalar and tensor gravimetry data, Computers & Geosciences, 36, 680-\n686 \nNvidia, 2008. Nvidia CUDA programming guide version 2.1, Nvidia Corporation. \nOldenborger, G.A. & Routh, P.S., 2009. The point-spread function measure of resolution for \nthe 3-D electrical resistivity experiment. Geophysical Journal International, 176, \n405\u2013414. \nOwens, J.D., Luebke, D., Govindaraju, N., Harris, M., Kr\u00fcger, J., Lefohn, A.. & Purcell, T.J., \n2007. A Survey of General-Purpose Computation on Graphics Hardware, Computer \nGraphics Forum, 26, 80-113. \nPazdirek, O. & Blaha, V., 1996. Examples of resistivity imaging using ME-100 resistivity \nfield acquisition system, In EAGE 58th Conference and Technical Exhibition \nExtended Abstracts, Amsterdam. \nPress, W.H. , Teukolsky, S.A., Vetterling, W.T. & Flannery, B.P, 1992. Numerical Recipes in \nC, 2nd edn, Cambridge University Press, Cambridge, UK. \nSava, P., 2010. An introduction to this special section: High performance computing. The \nLeading Edge, 29 (1), 42-43. \nSamou\u00eblian, A., Cousin, I., Richard, G., Tabbagh, A. & Bruand, A, 2003. Electrical \nresistivity imaging for detecting soil cracking at the centimetric Scale. Soil Science \nSociety of America Journal, 67, 1319-1326. \nSeaton, W.J. & Burbey, T.J., 2000. Aquifer characterization in the Blue Ridge physiographic \nprovince using resistivity profiling and borehole geophysics : Geologic analysis, \nJournal of Environmental & Engineering Geophysics, 5 (3), 45-58. \nStummer, P., Maurer, H. & Green, A., 2004. Experimental design: Electrical resistivity data \nsets that provide optimum subsurface information, Geophysics, 69, 120-129. \nWilkinson, P.B., Kuras, O., Meldrum, P.I., Chambers, J.E. & Ogilvy, R.D., 2006a. \nComparison of the spatial resolution of standard and optimised Electrical Resistivity \nTomography arrays, In EAGE 12th Annual Near Surface Geophysics Conference \nExtended Abstracts, Helsinki, Finland. \nWilkinson, P.B., Meldrum, P.I., Chambers, J.E., Kuras, O. & Ogilvy, R.D., 2006b. Improved \nstrategies for the automatic selection of optimized sets of electrical resistivity \ntomography measurement configurations, Geophysical Journal International, 167, \n1119-1126. \nZhou, B. & Dahlin, T, 2003. Properties and effects of measurement errors on 2D resistivity \nimaging surveying, Near Surface Geophysics, 1, 105-117. \n \n \n \nTable 1. The times in seconds for 40 iterations (and average relative resolution ratio \nachieved) for the different versions of the 'Compare R' array optimization method. The \nnumber of data points in the optimized (base) data set generated after 40 iterations is shown, \ntogether with the total number of data points in the comprehensive data set. The ratio of the \nnumber of data points in the optimized base set with the total number in the comprehensive \ndata set is given in percent. \nNumber of \nelectrodes \nNumber of data points Matrix-\nVector  \nDouble-\nprecision \nMatrix-\nMatrix  \nDouble-\nprecision \nGPU \nMatrix  \nDouble-\nprecision \nGPU \nMatrix  \nSingle-\nprecision \n Base All Ratio \n(%) \nTime taken in seconds. \n(Average relative model resolution) \n30 4618 51283 9.00 202 \n(0.958) \n87 \n(0.958) \n50  \n(0.958) \n29 \n(0.955) \n40 6503 166944 3.90 1245 \n(0.921) \n717 \n(0.921) \n324 \n(0.921) \n148 \n(0.915) \n50 8386 411453 2.04 6118 \n(0.886) \n3908 \n(0.886) \n1405 \n(0.886) \n577 \n(0.880) \n60 10272 854224 1.20 19967 \n(0.858) \n13944 \n(0.858) \n4421 \n(0.858) \n1760 \n(0.850) \n \nTable 2. Results obtained for a 30 electrodes survey line with different step sizes for an \noptimized data set with 400 data points. In the single step method, only the array (or \nsymmetrical pair of arrays) that gives the largest increase in average model resolution value is \nadded to the base set. \nStep size \n(%) \nNumber of iterations Time taken \n(s) \nAverage relative \nmodel resolution \nAverage spread \ncriterion value \nSingle 131 176 0.833 2.945 \n3 34 41 0.824 2.972 \n4.5 23 28 0.804 3.037 \n6 18 22 0.794 3.066 \n9 12 14 0.779 3.122 \n \nTable 3. Results obtained for a 50 electrodes survey line with different step sizes for an \noptimized data set with 1000 data points. \nStep size \n(%) \nNumber of iterations Time taken \n(s) \nAverage relative \nmodel resolution \nAverage spread \ncriterion value \nSingle 372 13552 0.768 3.723 \n3 45 1568 0.751 3.801 \n4.5 30 1046 0.733 3.860 \n6 23 803 0.719 3.937 \n9 16 557 0.662 4.070 \n \n Figure 1. Change of the average relative model resolution with (a) iteration number and (b) \nratio of number of arrays in optimized data set to comprehensive data set for survey lines \nwith 30 to 60 electrodes. \n Figure 2. (a) Change of the average relative model resolution with number of data points in \nthe optimized data set for a survey line with 30 electrodes using different step sizes.  (b) \nSimilar plots showing change of the average spread criterion value with the number of data \npoints. \n Figure 3. Relative model resolution sections for a survey line with 30 electrodes for (a) initial \nbase set and optimized data sets with 400 points produced using (b) 9%, (c) 6%, (d) 4.5%, (e) \n3% and (f) single step sizes. The right column shows corresponding spread criterion value \nsections. \n Figure 4. (a) Change of average relative model resolution with number of data points for a \nsurvey line with 50 electrodes using different step sizes. (b) Similar plots showing change of \nthe average spread criterion value with the number of data points. \n Figure 5. Relative model resolution sections for a survey line with 50 electrodes for (a) initial \nbase set (267 data points) and optimized data sets with 1000 points produced using (b) 9%, \n(c) 6%, (d) 4.5%, (e) 3% and (f) single step sizes. The right column shows corresponding \nspread criterion value sections. \n  \nFigure 6. (a) 2D test model. Pseudosections for (b) Wenner-Schlumberger, (c) dipole-dipole \nand (d) optimized (using a 4.5% step size) arrays. Inversion models for the (e) Wenner-\nSchlumberger array,  (f) dipole-dipole array, (g) optimized array data set using a 9% step \nsize, (h) optimized array data set using a 4.5% step size. \n Figure 7. (a) Dipole-dipole array pseudosection with noise added. Inversion model for (b) \nWenner-Schlumberger array, (c) dipole-dipole array and (d) optimized array data set using a \n4.5% step size. Inversion models using the L-curve method for (e) dipole-dipole array and (f) \noptimized data sets.  \n  \nFigure 8. Relative model resolution sections for optimized array set for a survey line with 35 \nelectrodes using the damped least-squares method for (a) small (599 data points) and (b) large \n(2401 data points) data sets. Similar relative model resolution sections generated using the \nsmoothness-constrained least-squares equation method for (c) small and (d) large data sets. \nRight column (e-h) shows corresponding spread criterion value sections for the different data \nsets. \n Figure 9. Inversion models for the optimized array data sets generated using a 4.5% step size. \nResults for (a) small (599 data points) and (b) large (2401 data points) optimized data sets \ngenerated using the damped least-squares method. Similar models for (c) small and (d) large \noptimized data sets generated using the smoothness-constrained least-squares method. \n \n"}