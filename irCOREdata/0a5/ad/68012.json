{"doi":"10.1109\/FUZZY.2010.5584130","coreId":"68012","oai":"oai:eprints.lancs.ac.uk:33928","identifiers":["oai:eprints.lancs.ac.uk:33928","10.1109\/FUZZY.2010.5584130"],"title":"Forecasting Time-Series for NN GC1 using Evolving Takagi-Sugeno (eTS) Fuzzy Systems with On-line Inputs Selection","authors":["Andreu, Javier","Angelov, Plamen"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2010","abstract":"In this paper we present results and algorithm used to predict 14 days horizon from a number of time series provided by the NN GC1 concerning transportation datasets [1]. Our approach is based on applying the well known Evolving Takagi-Sugeno (eTS) Fuzzy Systems [2-6] to self-learn from the time series. ETS are characterized by the fact that they self-learn and evolve the fuzzy rule-based system which, in fact, represents their structure from the data stream on-line and in real-time mode. That means we used all the data samples from the time series only once, at any instant in time we only used one single input vector (which consist of few data samples as described below) and we do not iterate or memorize the whole sequence. It should be emphasized that this is a huge practical advantage which, unfortunately cannot be compared directly to the other competitors in NN GC1 if only precision\/error is taken as a criteria. It is also worth to require time for calculations and memory usage as well as iterations and computational complexity to be provided and compared to build a fuller picture of the advantages the proposed technique offers. Nevertheless, we offer a computationally light and easy to use approach which in addition does not require any user-or problem-specific thresholds or parameters to be specified. Additionally, this approach is flexible in terms not only of its structure (fuzzy rule based and automatic self-development), but also in terms of automatic input selection as will be described below","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":"IEEE","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:33928<\/identifier><datestamp>\n      2018-01-24T02:05:29Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D626F6F6B5F73656374696F6E<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Forecasting Time-Series for NN GC1 using Evolving Takagi-Sugeno (eTS) Fuzzy Systems with On-line Inputs Selection<\/dc:title><dc:creator>\n        Andreu, Javier<\/dc:creator><dc:creator>\n        Angelov, Plamen<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        In this paper we present results and algorithm used to predict 14 days horizon from a number of time series provided by the NN GC1 concerning transportation datasets [1]. Our approach is based on applying the well known Evolving Takagi-Sugeno (eTS) Fuzzy Systems [2-6] to self-learn from the time series. ETS are characterized by the fact that they self-learn and evolve the fuzzy rule-based system which, in fact, represents their structure from the data stream on-line and in real-time mode. That means we used all the data samples from the time series only once, at any instant in time we only used one single input vector (which consist of few data samples as described below) and we do not iterate or memorize the whole sequence. It should be emphasized that this is a huge practical advantage which, unfortunately cannot be compared directly to the other competitors in NN GC1 if only precision\/error is taken as a criteria. It is also worth to require time for calculations and memory usage as well as iterations and computational complexity to be provided and compared to build a fuller picture of the advantages the proposed technique offers. Nevertheless, we offer a computationally light and easy to use approach which in addition does not require any user-or problem-specific thresholds or parameters to be specified. Additionally, this approach is flexible in terms not only of its structure (fuzzy rule based and automatic self-development), but also in terms of automatic input selection as will be described below.<\/dc:description><dc:publisher>\n        IEEE<\/dc:publisher><dc:date>\n        2010<\/dc:date><dc:type>\n        Contribution in Book\/Report\/Proceedings<\/dc:type><dc:type>\n        NonPeerReviewed<\/dc:type><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/FUZZY.2010.5584130<\/dc:relation><dc:identifier>\n        Andreu, Javier and Angelov, Plamen (2010) Forecasting Time-Series for NN GC1 using Evolving Takagi-Sugeno (eTS) Fuzzy Systems with On-line Inputs Selection. In: 2010 IEEE INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS (FUZZ-IEEE 2010). IEEE, New York, pp. 1479-1483. ISBN 978-1-4244-6920-8<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/33928\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":null,"relations":["http:\/\/dx.doi.org\/10.1109\/FUZZY.2010.5584130","http:\/\/eprints.lancs.ac.uk\/33928\/"],"year":2010,"topics":["QA75 Electronic computers. Computer science"],"subject":["Contribution in Book\/Report\/Proceedings","NonPeerReviewed"],"fullText":"  \n \n  \nAbstract\u2014 In this paper we present results and algorithm \nused to predict 14 days horizon from a number of time series \nprovided by the \u001b\u001b GC1 concerning transportation datasets \n[1].  Our approach is based on applying the well known \nEvolving Takagi-Sugeno (eTS) Fuzzy Systems [2-6] to self-learn \nfrom the time series.  ETS are characterized by the fact that \nthey self-learn and evolve the fuzzy rule-based system which, in \nfact, represents their structure from the data stream on-line \nand in real-time mode. That means we used all the data \nsamples from the time series only once, at any instant in time \nwe only used one single input vector (which consist of few data \nsamples as described below) and we do not iterate or memorize \nthe whole sequence. It should be emphasized that this is a huge \npractical advantage which, unfortunately cannot be compared \ndirectly to the other competitors in \u001b\u001b GC1 if only \nprecision\/error is taken as a criteria. It is also worth to require \ntime for calculations and memory usage as well as iterations \nand computational complexity to be provided and compared to \nbuild a fuller picture of the advantages the proposed technique \noffers. \u001bevertheless, we offer a computationally light and easy \nto use approach which in addition does not require any user- or \nproblem-specific thresholds or parameters to be specified. \nAdditionally, this approach is flexible in terms not only of its \nstructure (fuzzy rule based and automatic self-development), \nbut also in terms of automatic input selection as will be \ndescribed below.  \nI. INTRODUCTION \n \nThe time series are modeled by a simple regression model \nof the form: \n \n\u0001\u0002\u0003 = \u0005(\u0001\u0003\u0007\b, \u0001\u0003\u0007\n, \u2026 , \u0001\u0003\u0007\f)                                   (1) \n \nwhere \u0001\u0002\u0003 denotes the estimation of the value of the time \nseries at the k-th  time instant; \u0001\u0003\u0007\b denotes the value of the \ntime series estimation of the value of the time series at the \n(k-i)-th time instant; d is obviously the depth of the \nregression. \nThe non-linear, non-stationary function, f(x) is approximated \nby a fuzzy rule-based system of Takagi-Sugeno type [7]: \n \n\u000e\u0005 (\u0001\b\u0010\u0011 \u0001\b\u2217)\u0013\u0014\u0015 \u2026 (\u0001\f\u0010\u0011 \u0001\f\u2217 )\u0016\u0017\u0018\u0014 \u0001\u0002\u0003 =  \u0019\u001a + \u0019\b\u0001\b +  \u2026 + \u0019\f\u0001\f  (2) \n \nwhere  are prototypes\/focal points around which one \ncan form linguistic terms and a0,a1,..,ad are \nconsequence parameters.  \nIt is well known that fuzzy systems proven to be universal \n \nBoth  authors  are  with  the  Intelligent  Systems  Research  Laboratory,  \nDepartment of Communication Systems, InfoLab21, Lancaster University,  \nLancaster,  LA1  4WA,  UK;  e-mail: j.andreu@lancaster.ac.uk  \napproximations [8] and this approach seems reasonable. The \nmain problem with such an approach is how to design the \nmodel structure (how many fuzzy rules to select, where to \ncenter them, how to determine the parameters etc.) [9]. Data-\ndriven approaches to designing fuzzy rule-based systems \nwere first introduced in mid-90s of the last century [10]. \nThey did not answer the key problem, however, how to \ndetermine the structure of the model. Moreover, if the data \nstream is non-stationary this structure may need \nupdate\/evolving itself (as well as the model parameters). \nThis problem was addressed for the first time in the concept \nof evolving fuzzy [2,9] and neural network systems [11]. We \nadopt in this paper the first and one of the most popular \napproaches for evolving fuzzy rule-based systems, eTS [2-6] \nin its latest from which also includes on-line input variables \nselection. For the times series that we have in NN GC1 that \nmeans modifying the expression (1) into: \n \n\u0001\u0002\u0003 = \u0005(\u0001\u0003\u0007\u001c\b, \u0001\u0003\u0007\u001c\n, \u2026 , \u0001\u0003\u0007\u001c\u001d)         (1b) \n \nWhere the set I={i1,i2,\u2026,ir} and r<<d is determined \nautomatically by the algorithm on-line (taming one sample \nat a time with no iterations and memorizing the data stream). \nRespectively, equation (2) also changes in that it includes \na smaller number of premise fuzzy sets, r instead of d. \nIn the specific problem we had at hand in NN GC1 it turns \nout that r=4 and i={7,14,21,28} which means nothing else, \nbut a strong weekly pattern. It was even more impressive \nthat this weekly dependence was discovered by eTS \nalgorithm automatically and without off-line procedures or \nmemorizing the data streams. We also compared this result \nto an off-line test based on correlation analysis of input-\noutput pairs (Figure 1) and discovered that both approaches \nprovide the same result which will be detailed in the main \npaper. \n In the proposed approach the model structure evolves \nautomatically in an on-line manner through a recursive \nanalysis of each (new at a time of analysis) data sample as \ncompared to an accumulated history of the data stream (time \nseries) which is compressed in the existing rules as well as in \na small number of additional recursively updated parameters \nindicating the data density evolution.  \nIt should be noted that the proposed methodology has \nalready been successfully applied to a wide range of \napplications such as predictions, classification and control \nproblems. Further advances in the methodology in this \ncentury gives the capacity to reduce the computational cost \nForecasting Time-Series for \u001b\u001b GC1 using Evolving Takagi-Sugeno \n(eTS) Fuzzy Systems with On-line Inputs Selection  \nJavier Andreu Student Member, IEEE, Plamen Angelov Senior Member, IEEE \n*\nix\nWCCI 2010 IEEE World Congress on Computational Intelligence \nJuly, 18-23, 2010 - CCIB, Barcelona, Spain FUZZ-IEEE\n978-1-4244-8126-2\/10\/$26.00 c\u00a92010 IEEE 1479\n  \n \nof the algorithm, making feasible to embedded this \ntechniques in low computational devices such as sensors or \nmobile phones.  In addition new on-line capacities made the \nsystem much more attractive for real applications including \nareas such as robotics, advanced industrial process etc. \n[12,13].  \nThe key difference of eTS in comparison with other \napproaches is that it does not require a prefixed model \nstructure (number of fuzzy rules, number of inputs) but is \nfully data-driven method suitable to on-line and real-time \napplications. The term \u201cevolving\u201d is adopted due to the \ngradual evolution of the fuzzy rule-based model structure in \nterms of its components and fuzzy rules. This evolving \nbehavior is achieved by means of an on-line incremental \nclustering (partitioning of the data space) of the time series \nin terms of input-output data space (please, see equation (1). \nIn the case of time series, the inputs are the values of the \ntime series different number of steps back. Therefore, \nstarting from a guess of the value d (depth of the history) we \nare able to down-select a small subset, I of relevant inputs \n(steps back) based on their contribution to the prediction. \nThis makes the methodology efficient and suitable for online \nand real time applications. \nII. ON-LINE LEARNING FUZZY APPROACH BASED ON DATA \nDENSITY  \nThe key point of these approaches is the attempt to \nestimate data recursively which is more computationally \nefficient. By this aim we are able to get new on-line \ncapacities which are suitable for real time applications.  \nDirectly from the data streams and capturing data density \nvariations we can modify the shape and structure of the \nmodel. Another approaches deal with the same attempt, for \nexample this is the case of kernels in image processing \nwhich estimation is off-line [14], Parzen Windows [15] in \nstatistical learning. A well know approach is also the so \ncalled Mountain functions [16] that make use of the so \ncalled potential [17]. All of this last tree methods data \ndensity estimation is based on Gaussian distribution.  We \nproposed to use a Cauchy function over the sum of distances \nbetween several data points. As the Cauchy function is in \nfact a first order approximation of the Gaussian, both of \nthem have a series of common properties:  \n\u2022 Both are monotonic. \n\u2022 Its maximum is always 1. \n\u2022 When the argument tends to infinity, both \nasymptotically tends to Zero. \nHere is the formula of the first order approximation of the \nGaussian [14]. \n To form new clusters in the system the value called \npotential (data density) P has been used as criteria to form \nnew clusters in the evolving fuzzy clustering approach, \neClustering [14]: \n \n\u001e\u001f\u0001( )! = \b\n\b\" #$%# \u2211\n'(($)%(())'*\n*+*\n$%#),#\n            (3)\n             \nWhere k is the current time instant;  \u03c3 denotes the spread \nor zone of influence of the cluster. \nPoints with high potentials are selected to be candidates to \nform part of the model (focal points or fuzzy rules). \nEstimation of the data density is not an easy task because we \nwork out this density we need to work out the distance \nbetween every single point and other data samples. This fact \ncan be an issue for an on-line mode approach which does not \nkeep in memory in the totality of data samples. Therefore it \nis required to perform a recursive calculation which was \nproposed in: \n \n\u001e\u001f\u0001( )! = \u0003\u0007\b(\u0003\u0007\b)(-(\u0003)\"\b)\".(\u0003)\u0007\n\/(\u0003)                  (4) \n  \nValues a(k) and c(k) can be only calculated from the \ncurrent frame: \n \n\u0019( ) =  \u2211 \u00010\n( ); 2( ) =  \u2211 \u00010( )30( )4\"506\b4\"506\b            (5) \n \nDimensionalities are represented by n (input) and m \n(outputs) and dj (k) is calculated recursively as follows: \n The value b(k) is also defined by a recursive expression \nand accumulated during processing of the following frames \nindividually: \n \n7( ) =  7( \u2212 1) +  \u0019( \u2212 1); 7(1) =  0          (6) \n30( ) =  30( \u2212 1) +  \u00010( \u2212 1); 30( ) = 0         (7) \n    \n The area of influence or spread of the clusters \u03c3 is \nupdating on-line in a data-driven fashion by means of \nlearning the data distribution and variance [16]:  \n \n;\u001c0\n ( ) =  <;\u001c0\n ( \u2212 1) + (1 \u2212 <) \b=)(\u0003) \u2211 (\u0001>( ) \u2212 \u0001>( ))\n\n=)(\u0003)>6\b   (8) \n \nTaking as a initial value \u03c3j (1) = 0.5, \u03b1 denote the learning \nstep (recommended value 0.5); the number of data samples \nis represented by #i(k) associated with the number of \nclusters which belongs to, i\nth\n. \nIn the course of the process of the model, we only keep in \nthe memory the values of the focal points and their potential, \nall the other values are discarded from the memory. As a \nresult the potential is a representation of the data density \nregarding all of data samples. Therefore it is required to \nupdate each time step (a new data sample being read).  \nPotential of focal points is update even with the new data \nsample that will appear after a sample is taking as a \nprototype to form a cluster. The following formula is applied \nfor updating: \n \n\u001e\u001f\u0001\u001c\u2217( )! = \u0003\u0007\b\u0003\u0007\b\"(\u0003\u0007\n)? #@(()\u2217($%#))\u0007\bA\"BC)D\u2217 \u0007C)DB\n*        (9)              \n \nIn previous formulas (4)-(7) data density is estimated and the \nspread is adapted by (8). Thereupon we can form a fuzzy \nrule base in accordance with these basic principles: \n1480\n  \n \n1) A data with a high potential is suitable to be a focal \npoint; \n2) A data sample coordinates are placed in area where no \nprevious fuzzy rules are covering that space. \n3) Overlap and information redundancy in forming new \nrules must be avoided. \nWe can represent the first principle, 1) with the following \nexpression [18,19]: \n \n\u001e\u001f\u0001( )! >  \nF\nG\u0019\u0001 \n\u0010 = 1\n\u001e(\u0001\u2217( ))                                              (10)     \n                                                                                                                          \nZ\n*\n denotes a data sample that has been selected to be a \nfocal point (fuzzy rules); R represent the number of fuzzy \nrules till the current moment k (before condition (10) has \nbeen checked). \n  Regarding the second principle, this is represented by the \nnext expression [20]: \n \n\u001e\u001f\u0001( )! >  \nF\nG\u0010H\n\u0010 = 1\n\u001e(\u0001\u2217( ))                                 (11)   \n  \nTo accomplish the third principle, 3) we can step by step \nshrink the cluster radio by the next condition B:  \n \n\u2203\u0010, \u0010 =  J1, FK; L\u001c0\u001f\u0001( )! >  M\u0007\b;  \u2200O; O =  J1, HK         (12) \n \nIn this formula x = [x1,x2,\u2026,xn]\nT\n represent the input \nvector values (in the case of classification we would be \ntalking about features); uij represent a Gaussian type \nmembership function of j\nth\n fuzzy set of the i\nth\n fuzzy rule: \n \nL\u001c0(\u0001\u0003) =  M\n%((D($)% ()D\u2217 )*\n*+D*                                              (13) \n \nEspecially we have to take in consideration the previous \npoint for the focal points of rules that might be formed based \non the first principle, 1) following expression (10) which \nmight lies too close to each other. On the 3 principle, we \nhave to say that it makes easy the formation of fuzzy rules, \nin consideration with another approaches such as VQ [21], \nART [22] etc\u2026 all of them usually require later so called \n\u2018prunning\u2019 [23]. \n    The fundaments of the expression (11) are based on the so \ncalled \u2018one-sigma\u2019 condition, | xj(k) \u2013 x\n*\nij| > \u03c3j known from \nthe machine learning literature [15]. Thus, the expression \n(13) is valid when in the rule base there is implicit a fuzzy \nrule. The named this i, such that the input vector of the \ncurrent data sample, xk is multi-dimensional, represented by \nj by at least e\n-1 \n\u2248 0.36. \n On the basis of these principles, we can write a pseudo \ncode of the algorithm to learn on-line from the antecedents \nof the fuzzy system. \nAccordingly of this data density-based on-line clustering \nwe are able to compose antecedent part of the fuzzy rules \ndirectly generated from the data stream: \n \nF\u001c \u000e\u0005(\u0001\u001c \u0010\u0011 \u0001\u001c\b\u2217 )\u0013\u0014\u0015 (\u0001\n\u0010\u0011 \u0001\u001c\n\u2217 ) \u2026 (\u00014\u0010\u0011\u0001\u001c4\u2217 ); \u0010 = J1, FK   (14)   \n    \n \n        \n The antecedent part may be used in several applications and \nways: \n1) Stored and Analyzed by an operator; \n2) Combined with consequent part identification. \na. It can be used for prediction at each time. \nb. It can be used also for classification. \n3) Clustering the data and various applications, for \nexample in robotics. \nIII. EVOLVING FUZZY SYSTEM WITH ON-LINE INPUT \nSELECTION \nWe considered in the previous section and as far as we \nhave in knowledge, all previous research assumes that the \ndimensionality of the input (features) vector is pre-defined in \neach study or problem (Figure 2). \nWe put forward an approach that on the basis of Takagi-\nSugeno (TS) type fuzzy systems is able to select more \nmeaningful input features. The system automatically remove \nfrom the system those inputs does not contribute positively \nto the output. Takagi-Sugeno outputs are locally linear, so \nthe analysis of sensitivity is reduced to just analyze the \nconsequent parameters [5,6]. \n \n\u0016\u0017\u0018\u0014\u001fP\u001c = Q\u001c\u001a + \u2211 \u0001\u001c0Q\u001c0406\b !; \u0010 = J1, FK      \nF\u001c \u000e\u0005(\u0001\u001c \u0010\u0011 \u0001\u001c\b\u2217 ) \u0013\u0014\u0015 (\u0001\n\u0010\u0011 \u0001\u001c\n\u2217 ) \u2026 (\u00014\u0010\u0011 \u0001\u001c4\u2217 )              (15) \n \nThe overall output of the TS fuzzy system is worked out \nas the weighted average of the outputs of the local linear \nmodel [22]: \n \nP =  \u2211 R\u001cP\u001cS\u001c6\b                       (16) \nRead data sample x(k)  \nIF (k = 1) THE# \n       \/\/initialization stage\/\/ \n       \/\/initialize the variables of recursive calculation\/\/ \n       dj(k) =0;j=[1,n];b(k)=0 \n      \/\/the input part of the first data sample is the focal of the first       \ncluster (rule)\/\/ \n      X*1(1) \u0001 x(1); P(x*1(k)) \u00011; R \u00011 \n\/\/form the antecedent part of the first fuzzy rule\/\/ \n     Rule1 IF(x11 is x\n*\n11) A#D\u2026A#D (X1n is X\n*\n1n) \nELSE  \n     Recursively calculate potential of the current data sample, P(z(k)); \nUpdate the spread of the clusters (membership functions of the  \nrespective fuzzy sets) ;  \nRecursively update the potentials of the existing clusters; \nCheck \n    Condition A; \n    Condition B where, \n    A) the point is with high potential and covers new area of data \nspace; \n    B) the point overlap with the previously formed fuzzy rules; \nIF (A) THE# (x(k) is new focal point) \nX*R+1(k) \u0001 x(k) P(x\n*\nR+1(k)) \u0001P(x(k)); R \u0001R+1 \nAssign the new point to the nearest cluster. \nRepeat until end of data stream \nAlgorithm- Recursive data space partitioning based on the data density \n \n1481\n  \nWhere  R\u001c = \u220f \u00b5VW(X)\nYW,#\n\u2211 \u220f Z[D(C)\\D,#][,#\n   is the firing strength of the \ni\nth\n fuzzy rule. \n    Each one of the inputs features can be evaluated regarding \nimportance by ratio of the sum of consequent parameters for \nthe specific j\nth \n input feature in regard to \n(features) [23]: \n \n^\u001c0( ) =  _)D\u0006\u0003\r\u2211 _)`\u0006\u0003\r\\`,#  ; \u0010 \u0004  J1, FK; O \u0004 J1,\n \nWhere \u0016\u001c0\u0006 \r \u0004 \u2211 aQ\u001c0\u0006b\ra\n\u0003\n>6\b   represents\nsum of the parameters values of the i\nth\n rule.\n Inputs, outputs and internal variables of eTS can be \nnormalized online [7] then we may compare between each \nother. This comparison value gives us the weight value for \neach one of the input, then on we use this value to remove \ninput features j* which does not contribute or produce \nunnecessary noise to the overall output. The structure of the \nnext time instant can be determined by [5,6]\n \n\u2203O\u2217a^\u001c0\u2217( ) c  de \u2211 \u0016\u001c\u001d\u0006 \r\u001d6\b       \n\u0010 \u0004 J1, FK; O \u0004 J1, HK                      \n \nWhere \u03b51 is a pre-fixed value represent the tolerable \nminimum positive weigh of an input (feature). Suggested \nvalue is 3 to 5%.  \n \nFig. 1. Vertical axis, correlation values; horizontal axis\nbackward. \n \nFig 2. Vertical axis, number of input selected automatically (online input \nselection); horizontal axis, time instance. \n \nall n inputs \nHK                     (17) \n the accumulated \n \n: \n       (18) \n          \n \n, number of steps \n \nWe formulate the condition \nwhich the weight of a certain input (feature)\nsum or the maximum of the accumulated sum of parameters. \nIf the value achieved is less than \u03b5 this input feature is being \ninsignificant or even hindering the well performance of the \nsystem. Manifestly, the removal of this input is not going to \naffect the output. The two conditions \nthe number of inputs (features), \nbecome too large, at the same time,\nfeatures; thus, the sum gives a better representation that the \nmaximum (an averaging effect).\nThis technique is one of the\nEvolving Fuzzy system and should not be underestimated. In \nfact, in a real environment or issue the output selection or \nfeature extraction is a very critical point. The success of the \nreliability of the system is dependent upon\nof inputs. Input selection is \napproaches such as PCA [15\napproaches however require \nmodel structure. \nIV. CONCLUSION AND DISCUSSION\nA new approach to autonomous g\nsystem from data streams is explained in the paper. We \nupon the recently introduced evolving fuzzy Takagi\nto make predictions over NN GC1 time series which has \nbeen submitted to the #eura\nCompetition for #eural #etworks\npredictions are presented below \n \nFig. 3.  Prediction (Red) of 14 days ahead in time series number 1 from \nDataset E (Daily data). \n \nFig. 4.  Prediction (Red) of 14 days ahead in time series number 3 from \nDataset E (Daily data). \n \n \nin terms of the proportion \n from the total \ndiffer, because when \nn (the total sum) may \n for a small number of \n \n strongest points of the \n a good selection \noftentimes addressed by \n], GP [24], etc. All of these \na batch set of data and a fixed \n \neneration of fuzzy \nbuild \n-Sugeno \nl Time Series Forecasting \n. Representative figures of \nFigure 3 and Figure 4. \n \n \n1482\n  \n \nThis combination of techniques allow an extensible, \nflexible and open structure of fuzzy rule base and fuzzy sets \nwhich gives a result and more digested and understandable \nby humans than common neural networks. On the basis of \nthe flexibility and online structure simplification, a new \napproach of automatic input selection over time series is \npresented and explained. \nREFERENCES \n[1] Time Series Forecasting Grand Competition for Computational \nIntelligence. 2010 NNG1 Available: http:\/\/www.neural-forecasting-\ncompetition.com \n[2] P. Angelov, \u201cEvolving Rule-based Models: A Tool for Intelligent \nAdaptation\u201d, Proc. 9th IFSA World Congress, Vancouver, BC, \nCanada, 25-28, pp.1062-1067, July 2001. \n[3] P. Angelov and D. Filev, \u201cAn Approach to On-line Identification of \nTakagi-Sugeno Fuzzy Models\u201d,  IEEE Trans. on  System, Man, and \nCybernetics, part B - Cybernetics, vol.34, No. 1, pp. 484-498, 2004. \n[4] P. Angelov, \u201cAn Approach for Fuzzy Rule-base Adaptation using On-\nline Clustering\u201d, International Journal of Approximate Reasoning, \nvol. 35 , No 3, pp. 275-289, March 2004. \n[5] P. Angelov, X. Zhou, On Line Learning Fuzzy Rule-based System \nStructure from Data Streams, 2008 IEEE Intern. Conf. on Fuzzy \nSystems, IEEE World Congress on Computational Intelligence, Hong \nKong, June 1-6, 2008, pp.915-922, ISBN 978-1-4244-1821-3\/08. \n[6] P. Angelov, X. Zhou, Evolving Fuzzy Systems from Data Streams in \nReal-Time, 2006 Intern. Symp. on Evolving Fuzzy Systems, 7-9 Sept., \n2006, Ambleside, UK, IEEE Press, pp.29-35, ISBN 0-7803-9719-3. \n[7] T. Takagi and M. Sugeno, \u201cFuzzy identification of systems and its \napplication to modelling and control,\u201d IEEE Transactions on Systems, \nMan, and Cybernetics, vol. 15, No. 1, pp. 116-132, 1985. \n[8] L.-X. Wang, \u201cFuzzy Systems are Universal Approximators\u201d, Proc. of \nthe International Conference on Fuzzy Systems, San Diego, CA, USA, \n1992, pp.1163-1170. \n[9] P.P. Angelov, Evolving Rule-based Models: A Tool for Design of \nFlexible Adaptive Systems, Springer-Verlag, Heidelberg, New York, \n2002, ISBN 3-7908-1457-1. \n[10] R. Babuska, Data-driven Fuzzy Modeling: Transparency and \nComplexity Issues, Proc. of the Euro Symposium on Intelligent \nTechnologies ESIT'99, AB-01, Crete, Greece, June 1999 \nhttp:\/\/lcewww.et.tudelft.nl\/~babuska\/bib\/index.html. \n[11] N. Kasabov, Q. Song \u201cDENFIS: Dynamic Evolving Neural-Fuzzy \nInference System and Its Application for Time-Series Prediction,\u201d \nIEEE Trans. on Fuzzy Systems, Vol.10 (2), 2002, pp. 144-154. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n[12] X.  Zhou,  P.  Angelov,  \u201cAutonomous  Visual  Self-localization  in  \nCompletely    Unknown    Environment    using    Evolving    Fuzzy \nRule-based   Classifier\u201d,  Proc.  IEEE  Intern.  Conf.  on  \nComputational Intelligence Applications for Defense and Security, \nHonolulu, USA, 1-5 April 2007, pp. 131-138.  \n[13] J.  J.  Macias-Hernandez,  P.  Angelov,  X.  Zhou,  Soft  Sensor  for  \nPredicting Crude Oil Distillation Side Streams using Takagi-Sugeno  \nEvolving Fuzzy Models, Proc. 2007 IEEE Intern. Conf. on Systems,  \nMan,  and  Cybernetics,  7-10  Oct.,  2007,  Montreal,  Canada,  ISBN  \n1-4244-0991-8\/07, pp.3305-3310.   \n[14] A.   Elgammal,   R.   Suraiswami,   D.   Harwood,   and   L.   Davis,  \n\u201cBackground and Foreground modeling using non-parametric Kernel  \nDensity Estimation for visual surveillance KDE\u201d, Proc. 2002 IEEE  \nVol. 90 (7), pp. 1151 \u2013 1163.   \n[15] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical  \nLearning:  Data  Mining,  Inference  and  Prediction.  Heidelberg,  \nGermany: Springer Verlag, 2001. \n[16] R.  R.  Yager,  D.  P.  Filev,  \"Learning  of  fuzzy  rules  by  mountain  \nclustering, Proc. SPIE Conf. on Appl. of Fuzzy Logic  Technology,  \nBoston, MA, USA, pp. 246-254, 1993. \n[17] S. L. Chiu, \"Fuzzy model identification based on cluster estimation\",  \nJ. of Intel. and Fuzzy Syst. vol. 2, pp. 267-278, 1994.  \n[18] P.  Angelov,  \u201cAn  Approach  for  Fuzzy  Rule-base  Adaptation  using \nOn-line    Clustering\u201d,    International    Journal    of    Approximate \nReasoning, Vol. 35 , No 3, March 2004, pp. 275-289. \n[19] P.  Angelov,  D.  Filev,  \u201cAn  Approach  to  On-line  Identification  of  \nTakagi-Sugeno Fuzzy Models\u201d, IEEE Transactions on System, Man,  \nand   Cybernetics,   part   B   -   Cybernetics,   vol.34,   No1,   2004, \npp.484-498. ISSN 1094-6977.   \n[20] P.  Angelov,  \u201cEvolving  Rule-based  Models:  A  Tool  for  Design  of \nFlexible Adaptive Systems\u201d. Heidelberg, Germany: Springer, 2002. \n[21] R.  Gray, \u201cVector  quantization,\u201d  IEEE  ASSP  Magazine,  pp.  4\u201329, \n1984. \n[22] G.  A.  Carpenter  and  S.  Grossberg,  \u201cAdaptive  Resonance  Theory  \n(ART),\u201d in The Handbook of Brain Theory and Neural Networks, M.  \nA. Arbib, Ed. Cambridge, MA: MIT Press, 1995, pp. 79\u201382. \n[23] E.  Lughofer,  E.  Huellermeier,  and  E.  Klement,  \u201cImproving  the  \ninterpretability    of    data-driven    evolving    fuzzy    systems,\u201d    in  \nProceedings of EUSFLAT 2005, Barcelona, Spain, 2005, pp. 28\u201333. \n[24] G.  Smits,  A.  Kordon,  E.  Jordaan  .  C.  Vladislavleva,  and  M.  \nKotanchek, Variable Selection in Industrial Data Sets Using Pareto  \nGenetic Programming,  In: Yu T., R. Riolo, B. Worzel (Eds): Genetic  \nProgramming Theory & Practice III. Springer, NY, pp. 79-92, 2006. \n \n \n \n \n \n \n \n \n \n \n1483\n"}