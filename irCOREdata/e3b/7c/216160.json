{"doi":"10.1111\/j.1467-9868.2008.00692.x","coreId":"216160","oai":"oai:eprints.lse.ac.uk:30462","identifiers":["oai:eprints.lse.ac.uk:30462","10.1111\/j.1467-9868.2008.00692.x"],"title":"Finding an unknown number of multivariate outliers","authors":["Riani, Marco","Atkinson, Anthony C.","Cerioli, Andrea"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2009","abstract":"We use the forward search to provide robust Mahalanobis distances to detect the presence of outliers in a sample of multivariate normal data. Theoretical results on order statistics and on estimation in truncated samples provide the distribution of our test statistic. We also introduce several new robust distances with associated distributional results. Comparisons of our procedure with tests using other robust Mahalanobis distances show the good size and high power of our procedure. We also provide a unification of results on correction factors for estimation from truncated samples","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/216160.pdf","fullTextIdentifier":"http:\/\/eprints.lse.ac.uk\/30462\/1\/Finding_an_unknown_number_of_multivariate_outliers%28lsero%29.pdf","pdfHashValue":"a9b74bc63908bcf7b1b71556ad3def00337ca907","publisher":"Wiley on behalf of the Royal Statistical Society","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lse.ac.uk:30462<\/identifier><datestamp>\n      2017-10-13T15:55:05Z<\/datestamp><setSpec>\n      74797065733D4445505453:4C53452D5354<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lse.ac.uk\/30462\/<\/dc:relation><dc:title>\n        Finding an unknown number of multivariate outliers<\/dc:title><dc:creator>\n        Riani, Marco<\/dc:creator><dc:creator>\n        Atkinson, Anthony C.<\/dc:creator><dc:creator>\n        Cerioli, Andrea<\/dc:creator><dc:subject>\n        HA Statistics<\/dc:subject><dc:description>\n        We use the forward search to provide robust Mahalanobis distances to detect the presence of outliers in a sample of multivariate normal data. Theoretical results on order statistics and on estimation in truncated samples provide the distribution of our test statistic. We also introduce several new robust distances with associated distributional results. Comparisons of our procedure with tests using other robust Mahalanobis distances show the good size and high power of our procedure. We also provide a unification of results on correction factors for estimation from truncated samples.<\/dc:description><dc:publisher>\n        Wiley on behalf of the Royal Statistical Society<\/dc:publisher><dc:date>\n        2009<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lse.ac.uk\/30462\/1\/Finding_an_unknown_number_of_multivariate_outliers%28lsero%29.pdf<\/dc:identifier><dc:identifier>\n          Riani, Marco and Atkinson, Anthony C. and Cerioli, Andrea  (2009) Finding an unknown number of multivariate outliers.  Journal of the Royal Statistical Society. Series B: Statistical Methodology, 71 (2).  pp. 447-466.  ISSN 1369-7412     <\/dc:identifier><dc:relation>\n        http:\/\/onlinelibrary.wiley.com\/journal\/10.1111\/(ISSN)1467-9868<\/dc:relation><dc:relation>\n        10.1111\/j.1467-9868.2008.00692.x<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lse.ac.uk\/30462\/","http:\/\/onlinelibrary.wiley.com\/journal\/10.1111\/(ISSN)1467-9868","10.1111\/j.1467-9868.2008.00692.x"],"year":2009,"topics":["HA Statistics"],"subject":["Article","PeerReviewed"],"fullText":"  \nMarco Riani, Anthony C. Atkinson and Andrea Cerioli \nFinding an unknown number of multivariate \noutliers \nArticle (Accepted version) \n(Unrefereed) \n \n \nOriginal citation: \nRiani, Marco and Atkinson, Anthony C. and Cerioli, Andrea (2009) Finding an unknown number \nof multivariate outliers. Journal of the Royal Statistical Society: series B (statistical methodology), \n71 (2). pp. 447-466. ISSN 1369-7412 \nDOI: 10.1111\/j.1467-9868.2008.00692.x \n \n\u00a9 2009 Royal Statistical Society \n \nThis version available at: http:\/\/eprints.lse.ac.uk\/30462\/ \nAvailable in LSE Research Online: December 2011 \n \nLSE has developed LSE Research Online so that users may access research output of the \nSchool. Copyright \u00a9 and Moral Rights for the papers on this site are retained by the individual \nauthors and\/or other copyright owners. Users may download and\/or print one copy of any \narticle(s) in LSE Research Online to facilitate their private study or for non-commercial research. \nYou may not engage in further distribution of the material or use it for any profit-making activities \nor any commercial gain. You may freely distribute the URL (http:\/\/eprints.lse.ac.uk) of the LSE \nResearch Online website.  \n \nThis document is the author\u2019s final accepted version of the journal article. There may be \ndifferences between this version and the published version.  You are advised to consult the \npublisher\u2019s version if you wish to cite from it. \n \n \n \nResults in Finding an Unknown Number of\nMultivariate Outliers in Large Data Sets\nMarco Riani1\u2217, Anthony C. Atkinson2\u2020 and Andrea Cerioli1\u2021\n1Dipartimento di Economia, Universita` di Parma, Italy\n2The London School of Economics, London WC2A 2AE, UK\nApril 25, 2007\nAbstract\nWe use the forward search to provide parameter estimates for Mahalanobis dis-\ntances used to detect the presence of outliers in a sample of multivariate normal data.\nTheoretical results on order statistics and on estimation in truncated samples provide\nthe distribution of our test statistic. Comparisons of our procedure with tests using\nother robust Mahalanobis distances show the good size and high power of our proce-\ndure. We also provide a unification of results on correction factors for estimation from\ntruncated samples.\nKeywords: forward search; graphics; logistic plots; Mahalanobis distance; order statis-\ntics; power comparisons; truncated distributions; very robust methods\n1 Introduction\nThe normal distribution, perhaps following data transformation, has a central place in the\nanalysis of multivariate data. Mahalanobis distances provide the standard test for outliers\nin such data. However, the performance of the test depends crucially on the subset of\nobservations used to estimate the parameters of the distribution.\nIt is well known that the estimates of the mean and covariance matrix using all the\ndata are extremely sensitive to the presence of outliers. In this paper we use the forward\nsearch to provide an adaptively selected sequence of subsets of the data from which the\nparameters are estimated. We compare the resulting Mahalanobis distances as an outlier\ntest with a variety of robust procedures, all of which can be described as using estimates\nbased on one or two subsets. We show that our procedure has superior power as well as\ngood size and so is to be recommended.\nMahalanobis distances and the forward search are introduced in \u00a72. In \u00a73 we exhibit\nbootstrap envelopes for the distribution of distances in the forward search. Theoretical\nresults on the distribution are in \u00a74. In particular, \u00a74.1 uses results on order statistics to find\n\u2217e-mail: mriani@unipr.it\n\u2020e-mail: a.c.atkinson@lse.ac.uk\n\u2021e-mail: andrea.cerioli@unipr.it\n1\nthe distribution of ordered Mahalanobis distances. In \u00a74.2 we use a result of Tallis (1963)\nto adjust for the bias caused by estimation of the covariance from a subset of observations.\nWe use Mahalanobis distances to develop a test for the presence of one or more outliers\nin a sample. Our procedure for this form of outlier detection is described in \u00a75 with two\nexamples in the following section. Several established robust procedures for the detection\nof individual outlying observations, such as those of Davies and Gather (1993), Rousseeuw\nand Van Driessen (1999) and Hardin and Rocke (2005) are introduced in \u00a77. Some of these\nmethods use reweighted estimates and so are based on two subsamples of the data. To\nadapt these tests to determining whether there are any outliers in a sample, we introduce\nin \u00a78 a Bonferroni correction to allow for simultaneity. This allows us to develop two new\nversions of reweighted Mahalanobis distances. The comparisons of size in \u00a79.1 show that\nour procedure has better size than many competitors. In \u00a79.2 we use logistic plots of power\nto provide simple comparisons of tests with markedly different sizes. The results show the\nsuperior performance of our procedure.\nExamples of analyses of individual sets of data are in \u00a710. The first appendix discusses\nthe importance of careful numerical procedures in the calculation of extreme values of\norder statistics and the second draws a connection between the results of Tallis and the\ndistribution of observations in a truncated univariate normal distribution.\nOur procedure provides the most powerful test for outliers amongst those in our com-\nparisons. It can be further enhanced by use of the rich variety of information that arises\nfrom monitoring the forward search.\n2 Distances\nThe main tools that we use are the values of various Mahalanobis distances. The squared\ndistances for the sample are defined as\nd2i = {yi \u2212 \u00b5\u02c6}T \u03a3\u02c6\u22121{yi \u2212 \u00b5\u02c6}, (1)\nwhere \u00b5\u02c6 and \u03a3\u02c6 are the unbiased moment estimators of the mean and covariance matrix of\nthe n observations and yi is v \u00d7 1.\nIn the methods compared in this paper the parameters \u00b5 and \u03a3 are estimated from\na subset of m observations, yielding estimates \u00b5\u02c6(m) with \u00b5\u02c6(m)j = y\u00afj and \u03a3\u02c6(m) with\n\u03a3\u02c6(m)jk = (yj \u2212 y\u00afj)T (yk \u2212 y\u00afk)\/(m \u2212 1). Note that here yj and yk are m \u00d7 1. From this\nsubset we obtain n squared Mahalanobis distances\nd2i (m) = {yi \u2212 \u00b5\u02c6(m)}T \u03a3\u02c6\u22121(m){yi \u2212 \u00b5\u02c6(m)}, i = 1, . . . , n. (2)\nThe single subsets used for each MCD-based method are defined in \u00a77. In the forward\nsearch we use many subsets for outlier detection, rather than one. The difference is between\nviewing a movie and a single snapshot.\nIn the forward search we start with a subset of m0 observations which grows in size\nduring the search. When a subset S\u2217(m) of m observations is used in fitting we order the\nsquared distances and take the observations corresponding to the m + 1 smallest as the\nnew subset S\u2217(m + 1). Usually this process augments the subset by one observation, but\nsometimes two or more observations enter as one or more leave. To start the procedure we\nfind a starting subset S\u2217(m0) that is not outlying in any two-dimensional projection of the\ndata (Atkinson et al. 2004, \u00a72.13).\n2\nIn our examples we look at forward plots of quantities derived from the distances di(m)\nin which the parameters are estimated from the observations in S\u2217(m). These distances for\ni \/\u2208 S\u2217(m) tend to decrease as n increases. If interest is in the latter part of the search we\nmay use scaled distances\nd sci (m) = di(m)\u00d7\n(\n|\u03a3\u02c6(m)|\/|\u03a3\u02c6(n)|\n)1\/2v\n, (3)\nwhere \u03a3\u02c6(n) is the estimate of \u03a3 at the end of the search.\nTo detect outliers all methods compare selected Mahalanobis distances with a threshold.\nWe examine the minimum Mahalanobis distance amongst observations not in the subset\ndmin(m) = min di(m) i \/\u2208 S\u2217(m), (4)\nor its scaled version d scmin(m). If this ordered observation [m+1] is an outlier relative to the\nother m observations, this distance will be large compared to the maximum Mahalanobis\ndistance of observations in the subset. Because we potentially make many comparisons,\none for each value of m, the form of our threshold needs to allow for simultaneity, so\nthat we have a test with size \u03b1 for the presence of at least one outlier. Adjustment for\nsimultaneity in the other procedures is discussed in \u00a77.\n3 The Structure of Forward Plots and the Importance of\nEnvelopes: Swiss Banknotes\nOur purpose is to provide methods for relatively large data sets. Here we present a brief\nanalysis of a smaller example, which illustrates the use of forward plots with thresholds that\nare pointwise envelopes. In this example the bootstrap envelopes are found by simulating\nthe search 10,000 times. For larger examples we use the theoretical results of \u00a74\nFlury and Riedwyl (1988, pp. 4\u20138) introduce 200 six-dimensional observations on\nSwiss banknotes. Of these, units 101 to 200 are believed to be forgeries. The left-hand\npanel of Figure 1 shows a forward plot of the (unscaled) minimum Mahalanobis distances\nfor the forgeries. There is a large peak at m = 85, indicating that there are at least 15\noutliers. The peak occurs because the outliers form a loose cluster. Once one of these\nobservations has been included in S\u2217(m), the parameter estimates are slightly changed,\nmaking less remote the next outlier in the cluster. At the end of the search the distances\nincrease again when the remaining observations not in S\u2217(m) are somewhat remote from\nthe cluster of outliers. Large distances at the end of the search are, as shown in Figure 5,\ntypical of data with unclustered outliers.\nAn important feature of Figure 1 is that the plot goes outside the upper envelope when\nm is slightly less than 85. This is because, if we have a sample of 85 observations from\nthe normal distribution, the last few distances will be relatively large and the envelope will\ncurve upwards as it does in the plots for m a little less than 100. If we remove the 15\nobservations that form the outlying group and superimpose the new envelope for n = 85,\nwe can see whether all outliers have been identified.\nThe curve for scaled distances in the right-hand panel of the figure lies below the en-\nvelopes in the earlier part of the search because scaling is by the estimate \u03a3\u02c6(n) from the\nend of the search, which is inflated by the presence of the outliers. Hence the distances\n3\nSubset size m\nM\ni n\ni m\nu m\n M\na h\na l\na n\no b\ni s  \nd i\ns t\na n\nc e\n20 40 60 80 100\n3\n4\n5\n6\n7\nSubset size m\nM\ni n\ni m\nu m\n s\nc a\nl e\nd  \nM\na h\na l\na n\no b\ni s  \nd i\ns t\na n\nc e\n20 40 60 80 100\n2\n3\n4\n5\nFigure 1: Swiss banknotes, forgeries (n = 100): forward plot of minimum Mahalanobis dis-\ntance with superimposed 1, 5, 95, and 99% bootstrap envelopes using 10000 simulations.\nLeft panel unscaled distances, right panel scaled distances. There is a clear indication of\nthe presence of outliers which starts around m = 84.\nare shrunken. This plot shows that scaled distances may yield a procedure with low power\nif several outliers are present. However, we avoid extensive simulations by first finding\ntheoretical envelopes for scaled distances and then converting them to those that are un-\nscaled. Ease of computation is of particular importance if we have to superimpose a series\nof envelopes for different subsample sizes.\n4 Envelopes from Order Statistics\n4.1 Scaled Distances\nWe now use order statistics to find good, fast approximations to our bootstrap envelopes.\nFor the moment we take \u00b5 and \u03a3 as known, so our results apply to both scaled and unscaled\ndistances. The test statistic (4) is the m+1st ordered value of the n Mahalanobis distances.\nWe can therefore use distributional results to obtain approximate envelopes for our plots.\nSince these envelopes do not require simulation in their calculation, we can use them for\nmuch more extreme points of the distribution than would be possible for bootstrap intervals\nwithout massive simulations.\nLet Y[m+1] be the (m + 1)st order statistic from a sample of size n from a univariate\ndistribution with c.d.f. G(y). Then the c.d.f of Y[m+1] is given exactly by\nP{Y[m+1] \u2264 y} =\nn\u2211\nj=m+1\n(\nn\nj\n)\n{G(y)}j{1\u2212G(y)}n\u2212j.\n(5)\nSee, for example, Lehmann (1991, p. 353). Further, it is well known that we can apply\nproperties of the beta distribution to the RHS of (5) to obtain\nP{Y[m+1] \u2264 y} = IG(y)(m+ 1, n\u2212m), (6)\nwhere\nIp(A,B) =\n\u222b p\n0\n1\n\u03b1(A,B)\nuA\u22121(1\u2212 u)B\u22121du\n4\nis the incomplete beta integral. From the relationship between the F and the beta distribu-\ntion it is possible to rewrite equation (6) as\nP{Y[m+1] \u2264 y} = P\n{\nF2(n\u2212m),2(m+1) >\n1\u2212G(y)\nG(y)\nm+ 1\nn\u2212m\n}\n(7)\nwhere F2(n\u2212m),2(m+1) is the F distribution with 2(n\u2212m) and 2(m+1) degrees of freedom\n(Guenther 1977). Thus, the required quantile of order \u03b3 of the distribution of Y[m+1] say\nym+1,n;\u03b3 can be obtained as\nym+1,n;\u03b3 = G\n\u22121\n(\nm+ 1\nm+ 1 + (n\u2212m)x2(n\u2212m),2(m+1);1\u2212\u03b3\n)\n(8)\nwhere x2(n\u2212m),2(m+1);1\u2212\u03b3 is the quantile of order 1\u2212 \u03b3 of the F distribution with 2(n\u2212m)\nand 2(m+1) degrees of freedom. The argument of G\u22121(.) in (8) becomes extremely close\nto one at the end of the search, that is as m \u2192 n, particularly for large n and extreme \u03b3.\nConsequently, care needs to be taken to ensure that the numerical calculation of this inverse\ndistribution is sufficiently accurate. Details of one case are in \u00a710.3\nWe now consider the choice of G(x). If we knew both \u00b5 and \u03a3, G(x) would be \u03c72v.\nWhen both \u00b5 and \u03a3 are estimated using maximum likelihood on the whole sample, the\nsquared distances have a scaled beta distribution. But, in our case, we estimate from a sub-\nsample of m observations that do not include the observation being tested. Atkinson, Riani,\nand Cerioli (2004, p. 43-4) derive distributional results for such deletion Mahalanobis dis-\ntances. In the present case we estimate \u03a3 on m\u2212 1 degrees of freedom. If the estimate of\n\u03a3 were unbiased the null distribution of this squared distance would be\nd2(i) \u223c\nn\n(n\u2212 1)\nv(m\u2212 1)\n(m\u2212 v) Fv,m\u2212v. (9)\nThe superiority of the F -approximation is shown in Figure 2 for the case n = 100 and\nv = 6, values for which asymptotic arguments are unlikely to hold. The left-hand panel of\nthe figure shows that the \u03c72 approximation is poor, the envelopes being systematically too\nlow.\nUnfortunately, the estimate of \u03a3 that we use is biased since it is calculated from the\nm observations in the subset that have been chosen as having the m smallest distances.\nHowever, in the calculation of the scaled distances (3) we approximately correct for this\neffect by multiplication by a ratio derived from estimates of \u03a3. So the envelopes for the\nscaled Mahalanobis distances are given by\nVm,\u03b3 =\n\u221a\nn\n(n\u2212 1)\n\u221a\nv(m\u2212 1)\n(m\u2212 v)\n\u221a\nym+1,n;\u03b3. (10)\n4.2 Approximations for Unscaled Distances\nUnscaled distances cannot take advantage of the beneficial cancellation of bias provided by\nthe ratio |\u03a3\u02c6(m)|\/|\u03a3\u02c6(n)| in (3). However, an approximate correction factor for the envelopes\nof unscaled squared Mahalanobis distances (2) can be obtained from results on elliptical\ntruncation in the multivariate normal distribution. Suppose that yi \u223c N(\u00b5,\u03a3) is restricted\nto lie in the subspace\n0 \u2264 (yi \u2212 \u00b5)T\u03a3\u22121(yi \u2212 \u00b5) \u2264 b(m), (11)\n5\nSubset size m\nC h\ni  s\nq u\na r\ne d\n a\np p\nr o\nx i m\na t\ni o\nn\n60 70 80 90 100\n1\n2\n3\n4\n5\n6\nSubset size m\nF  \na p\np r\no x\ni m\na t\ni o\nn\n60 70 80 90 100\n1\n2\n3\n4\n5\n6\nFigure 2: Comparison of 1%, 50% and 99% asymptotic envelopes for scaled distances:\nn = 100, v = 6. Left-hand panel: \u03c72; right-hand panel: scaled F distribution. Continuous\nlines, envelopes found by simulation\nwhere b(m) is an arbitrary positive constant. Then it follows from the results of Tallis\n(1963) that\nE(yi) = \u00b5 and var(yi) = k(m)\u03a3,\nwhere\nk(m) =\nP{\u03c72v+2 < b(m)}\nP{\u03c72v < b(m)}\n.\nOur estimate of \u03a3 at step m is calculated from the m observations yi that have been\nchosen as having the m smallest (squared) Mahalanobis distances. If we ignore the sam-\npling variability in this truncation we can take b(m) as the limiting value of the m-th order\nstatistic in a sample of n squared Mahalanobis distances. Hence cFS(m) = k(m)\u22121 is the\ninflation factor for \u03a3\u02c6(m) to achieve consistency at the normal model. In large samples\ncFS(m) =\nm\/n\nP (\u03c72v+2 < X\n2\nv,m\/n)\n, (12)\nwhere X2v,m\/n is the m\/n quantile of \u03c7\n2\nv. Our envelopes for unscaled distances are then\nobtained by scaling up the values of the order statistics\nV \u2217m,\u03b3 = cFS(m)Vm,\u03b3.\nThe bound\n\u221a\nb(m) in (11), viewed as a function of m, is sometimes called a radius for\ntrimming size (n \u2212m)\/n. Garc\u0131\u00b4a-Escudero and Gordaliza (2005) studied the asymptotic\nbehaviour of its empirical version when \u00b5 and \u03a3 are replaced by consistent robust estima-\ntors, such as the MCD-based estimators of \u00a77.2. There we take m = h, where h, defined\nin (14), is a carefully selected half of the data. Then cFS(m) is equal to the consistency\nfactor (16) derived for the MCD scatter estimator by Butler, Davies, and Jhun (1993) and\nCroux and Haesbroeck (1999). A corollary of the results of Tallis, relating the truncated\nunivariate normal distribution and \u03c723 is given in Appendix 2.\n6\n4.3 Asymptotic Results for Very Large Samples\nFor very large n we use the asymptotic normality of order statistics to provide a satisfactory\napproximation to (5), once more for known \u00b5 and \u03a3. The asymptotic expectation of Y[m+1]\nis (Cox and Hinkley 1974, p.470) approximately\n\u03bem+1,n = G\n\u22121{(m+ 1\u2212 3\/8)\/(n+ 1\/4)}.\nIf we let p\u03be = (m + 1 \u2212 3\/8)\/(n + 1\/4) and \u03bem+1,n = G\u22121(p\u03be), the variance of \u03bem+1,n\n(Stuart and Ord 1987, p.331) is\n\u03c32\u03be = p\u03be(1\u2212 p\u03be)\/{nG2(\u03bem+1,n)}.\nThus, replacing G with the scaled F distribution (9) yields the asymptotic 100\u03b1% point of\nthe distribution of the scaled squared distance as\n\u03bem+1,n + \u03c3\u03be\u03a6\n\u22121(\u03b1), (13)\nwhere \u03a6(z) is the c.d.f. of the standard normal distribution.\nFor scaled distances (13) replaces (10). To obtain approximations for the unscaled\ndistance we again need to apply the results of \u00a74.2.\n4.4 A Comparison of Some Bootstrap and Order-Statistic Based En-\nvelopes\nWe now present plots illustrating the quality of our order-statistic approximations to the\nenvelopes.\nThe right-hand panels of Figure 3 show bootstrap envelopes (solid line) and the order-\nstatistic approximation of \u00a74.1 for scaled distances when n = 200 and v = 5 and 10.\nAgreement with the results of 10,000 simulations is very good virtually throughout the\nwhole range of m. The plots in the left-hand panels of the figure are for unscaled distances\nusing the results of \u00a74.2. Although the approximation is not perfect, as we shall see the\nbounds are adequate for outlier detection where we look at the upper boundaries typically\nin the last one third of the search.\nFigure 4 is a similar plot for n = 600. Here the approximations for the unscaled\ndistances are improved compared with those in Figure 3: the effect of increased v is reduced\nand the agreement in the upper envelope extends at least to n\/2.\n5 The Forward Search for Outlier Detection\n5.1 Motivation\nIf there are a few large outliers they will enter at the end of the search, and their detection\nis not a problem. As an instance Figure 5 shows an example with two appreciable outliers\nwith mean shift 2.1 in a sample of 200 six-dimensional observations. At the end of the\nsearch there are two observations lying clearly outside the 99.9% envelope. Here the for-\nward search has no difficulty in detecting these anomalous observations. The same is true\nfor many other outlier detection procedures.\n7\nSubset size m\n80 100 120 140 160 180 200\n2\n3\n4\n5\n6\nSubset size m\nM\na h\na l\na n\no b\ni s  \nd i\ns t\na n\nc e\ns\n80 100 120 140 160 180 200\n2\n3\n4\n5\n6\nSubset size m\n80 100 120 140 160 180 200\n3 .\n5\n4 .\n0\n4 .\n5\n5 .\n0\n5 .\n5\n6 .\n0\n6 .\n5\nSubset size m\n80 100 120 140 160 180 200\n3\n4\n5\n6\nFigure 3: Agreement between bootstrap envelopes (solid line) and the order-statistic ap-\nproximation of \u00a74 when n = 200. Left-hand panels: unscaled distances, right-hand panels:\nscaled distances. Top panels: v = 5, bottom panels v = 10.\nSubset size m\n100 200 300 400 500 600\n2\n3\n4\n5\n6\nSubset size m\n100 200 300 400 500 600\n2\n3\n4\n5\n6\nSubset size m\n100 200 300 400 500 600\n3 .\n5\n4 .\n0\n4 .\n5\n5 .\n0\n5 .\n5\n6 .\n0\n6 .\n5\nSubset size m\n100 200 300 400 500 600\n2\n3\n4\n5\n6\nFigure 4: Agreement between bootstrap envelopes (solid line) and the order-statistic ap-\nproximation of \u00a74 when n = 600. Left-hand panels: unscaled distances, right-hand panels:\nscaled distances. Top panels: v = 5, bottom panels v = 10.\n8\nSubset size m\nM\na h\na l\na n\no b\ni s  \nd i\ns t\na n\nc e\ns\n140 160 180 200\n3\n4\n5\n6\nFigure 5: Easily detected outliers. There are two contaminated units in this sample with\nn = 200 and v = 6 that are clearly revealed at the end of the search; 1%, 50%, 99% and\n99.9% envelopes\nEven relatively small clusters of outliers can however be more difficult to identify. Fig-\nure 1 of the Swiss banknote data shows that the forward plot of distances twice goes outside\nthe 99%envelope and twice returns within it. These events indicate two instances of mask-\ning. At the end of the search the Mahalanobis distance dmin for m = n\u22121 is 5.691 and lies\njust below the 99% envelope from bootstrap simulations for which the value is 5.834. Only\nif two observations are deleted do these few outliers become apparent: when m = n \u2212 3\nthe 99% bootstrap value of dmin is 4.62; the observed value is 4.77, which lies well outside\nthe envelope. However, there is also a cluster of outliers and the search has a central peak,\naround m = 85 in Figure 1, before a series of lower values of the distance. In more extreme\ncases with a cluster of outliers masking may cause the plot to return inside the envelopes\nat the end of the search. An example is in our second set of simulated data in Figure 10.\nMethods of using the forward search for the formal detection of outliers have to be sensitive\nto these two patterns - a few \u201cobvious\u201d outliers at the end and a peak earlier in the search\ncaused by a cluster of outliers.\n5.2 Procedure\nTo use the envelopes in the forward search for outlier detection we propose a two stage\nprocess. In the first stage we run a search on the data, monitoring the bounds for all n\nobservations until we obtain a \u201csignal\u201d indicating that observation m\u2020, and therefore suc-\nceeding observations, may be outliers, because it lies beyond our threshold. In the second\npart we superimpose envelopes for values of n from this point until the first time we intro-\n9\nduce an observation we recognise as an outlier.\nThe thresholds need to be chosen to avoid the problem of simultaneity. We require a\nprocedure that combines high power with a size of \u03b1 for declaring the sample to contain at\nleast one outlier. In our exposition and examples we take \u03b1 = 1%.\nWe can expect the occasional observation to fall outside the bounds during the search\neven if there are no outliers. If we ignore the correlation in adjacent distances induced by\nthe ordering imposed by the search, each observation can be taken to have a probability \u03b3 =\n1\u2212\u03b1 of falling above the \u03b1 point of the pointwise envelope. If \u03b3 is small, say 1%, and n =\n1, 000 the number of observations outside the envelope will have approximately a Poisson\ndistribution with mean 10. The probability that no observations fall above the envelope\nwill then be e\u221210, a very small number. We need to be able to distinguish these random\noccurrences during the search from the important peaks illustrated in the two figures.\nThe envelopes shown in Figures 3 and 4 consist roughly of two parts; a flat \u201ccentral\u201d\npart and a steeply curving \u201cfinal\u201d part. Our procedure FS1 for the detection of a \u201csignal\u201d\ntakes account of these two parts and is as follows:\n\u2022 In the central part of the search we require 3 consecutive values of dmin(m) above the\n99.99% envelope or 1 above 99.999%;\n\u2022 In the final part of the search we need two consecutive values of dmin(m) above 99.9%\nand 1 above 99%;\n\u2022 dmin(n\u2212 2) > 99.9% envelope;\n\u2022 dmin(n\u2212 1) > 99% envelope.\nThe final part of the search is defined as:\nm \u2265 n\u2212 [13 (n\/200)0.5] ,\nwhere here [] stands for rounded integer. For n = 200 the value is slightly greater than 6%\nof the observations.\nThe purpose of, in particular, the first point is to distinguish real peaks from random\nfluctuations. Once a signal takes place (at m = m\u2020) we start superimposing 99% envelopes\ntaking n = m\u2020 \u2212 1,m\u2020,m\u2020 + 1, . . . until the final, penultimate or antepenultimate value\nare above the 99% threshold or, alternatively, we have a value of dmin(m) for any m > m\u2020\nwhich is greater than the 99.9% threshold.\nSome slight variations of the former procedure are possible. Here are two. If we failed\nto detect any outliers in FS1 but had an incontrovertible signal:\n\u2022 FS2. Three consecutive values of dmin(m) above the 99.999% threshold, or\n\u2022 FS3. Ten values of dmin(m) above the 99.999% threshold,\nwe then decide that outliers are present.\nSome features of this procedure may seem arbitrary. However, as we see in \u00a77, there\nare likewise arbitrary decisions in the MCD based procedures in the definition of the subset\nof m observations that are used in the final calculation of Mahalanobis distances and in the\nreference distributions used for testing these distances.\n10\ny1\n\u22123 \u22122 \u22121 0 1 2 3 \u22124 \u22122 0 2 4\n\u2212\n2\n0\n2\n\u2212\n3\n\u2212\n1\n0\n1\n2\n3\ny2\ny3\n\u2212\n2\n0\n2\n4\n\u2212\n4\n\u2212\n2\n0\n2\n4\ny4\n\u22122 0 2 \u22122 0 2 4 \u22123 \u22122 \u22121 0 1 2 3\n\u2212\n3\n\u2212\n1\n0\n1\n2\n3\ny5\nFigure 6: Slight contamination. Scatterplot matrix of data yielding Figure 7: n = 500,\nv = 5; 5% of the units are contaminated. Level shift = 1.4 for each dimension. Original\nunits, \u2022; contaminated units +\n6 Two Examples\n6.1 Slight Contamination\nThe purpose of this example is to show in practice how the procedure works in the presence\nof slight contamination and a small number of contaminated units.\nThere are 500 observations and v = 5. There is a shift contamination of 1.4 in all\ndimensions applied to 5% of the units, those numbered 1\u201325. The scatterplot matrix of the\ndata is in Figure 6, with the forward plot of minimum Mahalanobis distances in Figure 7.\nThis plot shows that there is a series of large values around m = 480, even though the value\nat the end of the search is below the 99% envelope. There is thus visual evidence of the\npresence of around 20 masked outliers.\nMore formally, we now apply our rule FS1 and find that a signal occurs when m =\n479 because, for this value we have two consecutive values of dmin(m) above the 99.9%\nthreshold and, in addition, one other value above 99%. In particular the threshold levels\nare:\ndmin(479) > 99.9%, dmin(480) > 99.9% with dmin(478) > 99%.\nWe receive the signal at m = 479 because this is the first point at which we have an\nobservation above the 99.9% threshold.\nWe now proceed to the second part of our outlier detection process and superimpose\nenvelopes for a series of increasing sample sizes until we identify the outliers signalled\n11\nSubset size m\nM\na h\na l\na n\no b\ni s  \nd i\ns t\na n\nc e\ns\n300 350 400 450 500\n3 .\n0\n3 .\n5\n4 .\n0\n4 .\n5\n5 .\n0\n5 .\n5\n6 .\n0\nFigure 7: Slight contamination. Forward plot of minimum Mahalanobis distances for the\ndata of Figure 6. The first 3 bands are 1%, 50% and 99%. The highest 2 are 99.9%\nand 99.999%. The extreme envelope has been superimposed just to show that this sample\ncontains a strong evidence of not being homogeneous.\nin the first stage of the process. In this example we start with n = 479. Figure 8 shows\nthe envelopes and forward plot of minimum Mahalanobis distances for several values of n.\nWhen n = 482 the curve lies well within the envelopes. Around n = 490 the observed\ncurve starts to become closer to the 99% envelope. When n = 494 some values are close\nto the 99.9% envelope. The first time the observed values go out of the 99.9% envelope is\nwhen n = 495.\nThe procedure of resuperimposing envelopes stops when n = 495, the first time in\nwhich we have a value of dmin(m) for m \u2264 m\u2020 greater than the 99.9% threshold. The\ngroup can therefore be considered as homogeneous up to when we include 494 units. In\nthese data the shifted observations are units 1 - 25. The last six units included in the search\nplotted in Figure 7 are numbered 2, 343, 6, 16, 23, 1, so that five out of these six are indeed\ncontaminated units.\n6.2 Appreciable Contamination\nWe now consider an example with the same structure but with an appreciable number of\ncontaminated units, although the mean shift itself is not large. We take n = 200 and v = 5,\nwith 30% contamination from a mean shift of 1.2 in each dimension. The original obser-\nvations again have a standard independent multivariate normal distribution. The scatterplot\nmatrix of the data is in Figure 9 with the forward plot of minimum Mahalanobis distances\nin Figure 10.\nWith 30% contamination and 200 observations there are 60 outliers. Figure 10 shows\na peak around m = 130 followed by a trough a little after m = 140, which therefore\ncome roughly where we would expect. The peak is a little early because of the overlapping\nnature of the two groups. The trough is caused by the effect of the inclusion of outliers on\nthe estimate of \u03a3 which becomes too large, giving small Mahalanobis distances.\n12\n300 350 400 450\n3 .\n0\n4 .\n0\n5 .\n0\n6 .\n0\nm=482\n300 350 400 450\n3 .\n0\n4 .\n0\n5 .\n0\n6 .\n0\nm=491\n300 350 400 450 500\n3 .\n0\n4 .\n0\n5 .\n0\n6 .\n0\nm=494\n300 350 400 450 500\n3 .\n0\n4 .\n0\n5 .\n0\n6 .\n0\nm=495\nFigure 8: Slightly contaminated data. When n = 482 the curve lies well within the en-\nvelopes. Around n = 490 the observed curve starts getting closer to the 99% envelope and\nwhen n = 494 some values are close to the 99.9% envelope. The first time the curve goes\nabove the 99.9% envelope is step n = 495.\n13\ny1\n\u22122 0 2 \u22122 \u22121 0 1 2 3\n\u2212\n2\n0\n1\n2\n3\n\u2212\n2\n0\n2\ny2\ny3\n\u2212\n2\n0\n2\n4\n\u2212\n2\n\u2212\n1\n0\n1\n2\n3\ny4\n\u22122 \u22121 0 1 2 3 \u22122 0 2 4 \u22122 \u22121 0 1 2 3\n\u2212\n2\n0\n1\n2\n3\ny5\nFigure 9: Scatterplot matrix of appreciably contaminated data with n = 200 and v = 5\nyielding Figure 10: 30% of the units are contaminated. Level shift =1.2 for each dimension.\nOriginal units, \u2022; contaminated units +\nSubset size m\nM\na h\na l\na n\no b\ni s  \nd i\ns t\na n\nc e\ns\n100 120 140 160 180 200\n3\n4\n5\n6\n7\n99.99% and 99.999% bands\nFigure 10: Appreciably contaminated data; minimum Mahalanobis distance with superim-\nposed 1%, 50%, 99%, 99.99% and 99.999% confidence bands.\n14\nSubset size m\nM\ni n\n.  M\na h\na l\na n\no b\ni s  \nd i\ns t\na n\nc e\n500 1000 1500\n4\n5\n6\n7\nFigure 11: 2,000 normal observations, v = 10: forward plot of 90% and 99% envelopes of\nminimum Mahalanobis distances with superimposed Bonferroni bounds including Hadi\u2019s\ncorrection\nIf we apply our rules to this plot we find that, fromm = 122 tom = 133 the consecutive\nvalues of dmin(m) are greater than the 99.99% envelope while from m = 123 to m = 132\nthey are all greater than the 99.999% envelope. FS2 is therefore satisfied and we do not\nneed to confirm the outliers by successively superimposing bounds. The figure shows how\nmasking will cause the failure of procedures that look at only the largest values of the\ndistances, or that try to detect outliers by backwards deletion. The structure of a peak,\nfollowed by a dip, in the plot of Figure 10 is further evidence of the presence of a cluster\nof outliers that can only be obtained from the forward search. However we do not here\nmake use of this as a procedure for detecting outliers, concentrating instead solely on upper\nexceedances of the bounds.\n7 Other Outlier Detection Procedures\n7.1 Bonferroni Bounds\nThe statistic (2) provides the basis for our test of the outlyingness of observation [m + 1].\nHadi (1994) uses a Bonferroni bound to allow for the ordering of the distances during his\nforward search and compares a slightly scaled version of (2) with the percentage points of\n\u03c72v,(\u03b1\/n), the scaling being to allow for the estimation of \u03a3.\nSince the test is for an outlier in a sample of size m + 1, it seems appropriate to use\nthe Bonferroni bound \u03c72v,{\u03b1\/(m+1)} rather than \u03c7\n2\nv,(\u03b1\/n). Figure 11 shows the resulting 95\nand 99% bounds superimposed on a forward plot of bootstrap envelopes for n = 2000 and\nv = 10. These bounds were calculated using the empirical scaling in \u00a72 of Hadi (1994).\nThey are unrelated to the true distribution, except for the last step of the search; due to the\nlow correlation of the distances the bound is almost exact when m = n\u2212 1. Earlier in the\nsearch the bounds are far too large, because \u03a3\u02c6(m), despite Hadi\u2019s rescaling, is treated as\nan estimate from a full sample, rather than from the truncated sample that arises from the\nordering of the distances.\nWisnowski et al. (2001, p. 360) report that the related procedure of Hadi and Simonoff\n15\n(1993) for regression has a low detection rate for moderate and small outliers and an ab-\nnormally low false alarm rate. Similar properties for multivariate data can be inferred from\nFigure 11.\n7.2 Distances for Outlier Detection\nIn this section we describe a number of variants of the Mahalanobis distance that have been\nrecommended for outlier detection. These vary in the subset or subsets of observations\nused for parameter estimation. When robust estimates are used, there are several possible\nadjustments to obtain consistent estimators of \u03a3. There is also a choice of reference dis-\ntribution against which to assess the observed distances. We leave until \u00a78 the adjustments\nmade for simultaneous inference which introduce further subsets of the data to be used for\nestimation.\n\u2022 MD and MDK.\nThe Mahalanobis distance (1), with parameters estimated from all the data was long\nsuggested as an outlier test, for example by Wilks (1963). As is well known, it is ex-\nceptionally sensitive to masking. However, we include it in some of our comparisons to\nillustrate just how sensitive it is.\nIf the values of the parameters \u00b5 and \u03a3 were known, the distribution of the distance\nwould be \u03c72v. As an outlier test we call this MDK with MD the test based on the same\ndistances but referred to the correct scaled Beta distribution. Section 2.6 of Atkinson et al.\n(2004) gives this distribution; \u00a72.16 gives references to the repeated rediscovery of related\ndistributional results.\nRobust distances. The customary way to detect multivariate outliers is to compute\nrobust estimates of \u00b5 and \u03a3 based on one or two carefully chosen subsets of the data\n(Rousseeuw and van Zomeren 1990). Mahalanobis distances from this robust fit are then\ncompared with the \u03b1% cut-off value of the reference distribution, with \u03b1 usually between\n0.01 and 0.05, and unit i is nominated as an outlier if its distance exceeds the threshold. The\ndistribution of squared Mahalanobis distances depends on the robust estimators at hand,\nbut it has been proven that asymptotically it is either exactly or proportional to \u03c72v in many\nsituations; see e.g. Davies (1992), Butler, Davies, and Jhun (1993), Lopuhaa\u00a8 (1999) and\nMaronna, Martin, and Yohai (2006). We list four robust distances, versions of which are\nused in our comparisons.\n\u2022 MCD.\nWe consider the minimum covariance determinant (MCD) estimator described in Rousseeuw\nand Leroy (1987, p. 262) and some of its variants. In the MCD approach, the estimators of\n\u03a3 and \u00b5, say \u00b5\u02c6MCD and \u03a3\u02c6MCD, are defined to be the mean and the covariance matrix of the\nsubset of\nh = bn+ v + 1\n2\nc (14)\nobservations for which the determinant of the covariance matrix is minimal, where b\u00b7c\ndenotes the integer part. The resulting breakdown value is then\nb(n\u2212 v + 1)\/2c\nn\n. (15)\nThe MCD is used because it has rate of convergence n\u22121\/2, unlike the minimum volume\nellipsoid estimator (Davies 1992) for which convergence is at rate n\u22121\/3.\n16\nRousseeuw and Van Driessen (1999) developed a fast algorithm for computing \u00b5\u02c6MCD\nand \u03a3\u02c6MCD, which has been implemented in different languages, including R, S-Plus,\nFortran and Matlab. Software availability and faster rate of convergence with respect\nto other high breakdown estimators have made the MCD approach a popular choice in\napplied robust statistics.\nA crucial issue with the MCD scatter estimator \u03a3\u02c6MCD is that it tends to underestimate\n\u03a3 even in large samples. With breakdown value (15), the appropriate large-sample correc-\ntion factor for \u03a3\u02c6MCD was derived by Butler, Davies, and Jhun (1993) and by Croux and\nHaesbroeck (1999) as\ncMCD(h, n, v) =\nh\/n\nP (\u03c72v+2 < X\n2\nv,h\/n)\n. (16)\nHowever, although consistent at the normal model, the estimator\ncMCD(h, n, v)\u03a3\u02c6MCD\nis still biased for small sample sizes. Pison, Van Aelst, and Willems (2002) showed by\nMonte-Carlo simulation the importance of applying a small sample correction factor to\ncMCD(h, n, v)\u03a3\u02c6MCD. Let sMCD(h, n, v) be this factor for a specific choice of n and v and\nbreakdown value (15). The resulting robust Mahalanobis distances are then\nd(MCD)i =\n\u221a\nkMCD(yi \u2212 \u00b5\u02c6MCD)T \u03a3\u02c6\u22121MCD(yi \u2212 \u00b5\u02c6MCD) i = 1, . . . , n, (17)\nwhere kMCD = {cMCD(h, n, v)sMCD(h, n, v)}\u22121.\n\u2022 HR.\nThe exact finite-sample distribution of the robust Mahalanobis distances (17) is un-\nknown, but Hardin and Rocke (2005) proposed a scaled F approximation which, in small\nand moderate samples, outperforms the asymptotic \u03c72v approximation of MCD.\n\u2022 RMCD-C.\nTo increase efficiency, a reweighted version of the MCD estimators is often used in\npractice. These reweighted estimators, \u00b5\u02c6RMCD and \u03a3\u02c6RMCD, are computed by giving weight\n0 to observations for which d(MCD)i exceeds a cutoff value. Thus a first subset of h ob-\nservations is used to select a second subset from which the parameters are estimated. The\ndefault choice (Rousseeuw and Leroy 1987, Rousseeuw and Van Driessen 1999) for this\ncutoff value is \u221a\nX2v,0.025. (18)\nBoth the consistency (Croux and Haesbroeck 2000) and the small sample (Pison, Van Aelst,\nand Willems 2002) correction factors cRMCD(h, n, v) and sRMCD(h, n, v) can be applied to\n\u03a3\u02c6RMCD, when the robust Mahalanobis distances become\nd(RMCD\u2212C)i =\n\u221a\nkRMCD\u2212C(yi \u2212 \u00b5\u02c6RMCD)T \u03a3\u02c6\u22121RMCD(yi \u2212 \u00b5\u02c6RMCD) i = 1, . . . , n, (19)\nwhere kRMCD\u2212C = {cRMCD(h, n, v)sRMCD(h, n, v)}\u22121.\nRMCD.\nThe original MCD literature (Rousseeuw and Leroy 1987, Rousseeuw and Van Driessen\n1999) did not suggest use of the consistency correction factor cRMCD(h, n, v). The ro-\nbust Mahalanobis distances arising from this basic reweighted MCD estimator, d(RMCD)i,\nare then computed as in equation (19), but with kRMCD = sRMCD(h, n, v)\u22121 replacing\nkRMCD\u2212C.\n17\n8 Simultaneity and Bonferronisation\nThe published literature describing the properties of robust Mahalanobis distances for mul-\ntivariate outlier detection is mainly concerned with rejection of the single null hypothesis\nH0 : yi \u223c N(\u00b5,\u03a3) (20)\nat level \u03b1. On the contrary, in our procedure of \u00a74 the test statistic (4) is the m + 1st\nordered value of the n Mahalanobis distances. Therefore, its distribution involves the joint\ndistribution of all the n Mahalanobis distances d2i (m), so that the null hypothesis of interest\nbecomes the intersection hypothesis\nH0 : {y1 \u223c N(\u00b5,\u03a3)} \u2229 {y2 \u223c N(\u00b5,\u03a3)} \u2229 . . . \u2229 {yn \u223c N(\u00b5,\u03a3)} (21)\nthat there are no outliers in the data. The Forward Search \u03b1 is the size of the test of (21),\ni.e. the probability that at least one of the individual hypotheses (20) is rejected for some\nm when (21) is true. In our approach, we are willing to tolerate a wrong conclusion in\n(100\u03b1)% of data sets without outliers, while under (20) one should be prepared to declare\n(100\u03b1)% of observations as outliers in any application.\nWe let \u03b1 have the same interpretation in MCD procedures by comparing all the indi-\nvidual statistics d(MCD)i, d(RMCD)i and d(RMCD\u2212C)i, i = 1, . . . , n, with the \u03b1\u2217 = \u03b1\/n cutoff\nvalue of their reference distributions. A Bonferroni approach is appropriate in this con-\ntext because extreme observations are approximately independent of the MCD estimators\n\u00b5\u02c6MCD and \u03a3\u02c6MCD, as shown by Hardin and Rocke (2005). Hence the intersection between\nmultiple tests of (20), sharing the same MCD estimates, should be negligible, at least when\nH0 is rejected. Gather, Pawlitschko, and Pigeot (1997) discuss properties of multiple tests\nand provide further references.\nThis Bonferroni procedure applies to the level at which we say that at least one outlier\nis present. We can, in addition, apply the Bonferroni argument to selection of observations\nto be used in parameter estimation for the reweighted distances. We suggest two such\nmodifications.\n\u2022 RMCD-B.\nWe set \u03b1 = 0.01 in all our simulations. The default cutoff value for excluding obser-\nvations in the computation of reweighted MCD estimators is given by (18). However, this\ncutoff is inappropriate when testing the intersection hypothesis (21), as individual outlier\ntests are now performed with size \u03b1\u2217 = 0.01\/n. We accordingly calculate a modified ver-\nsion of the reweighted estimators, say \u00b5\u02c6RMCD\u2212B and \u03a3\u02c6RMCD\u2212B, where observations are\ngiven weight 0 if d(MCD)i exceeds \u221a\nX2v,\u03b1\u2217 . (22)\nSubstituting these modified estimators into (19), we obtain the Bonferroni-adjusted reweighted\ndistances\nd(RMCD\u2212B)i =\n\u221a\nkRMCD(yi \u2212 \u00b5\u02c6RMCD\u2212B)\u2032\u03a3\u02c6\u22121RMCD\u2212B(yi \u2212 \u00b5\u02c6RMCD\u2212B) i = 1, . . . , n,\n(23)\n\u2022 RMCD-D.\nAn alternative Bonferroni-adjusted reweighted-MCD distance is obtained by substitut-\ning kRMCD\u2212C for kRMCD in equation (23), thus including the consistency factor as we did\nin the definition of RMCD-C.\n18\nThe correction factors in these Bonferroni-adjusted versions of RMCD include the\nsmall sample correction sRMCD(h, n, v) which was derived without allowance for simul-\ntaneous inference. The appropriate small-sample factor for RMCD-B and RMCD-D is not\navailable in the MCD literature.\nA summary of the Mahalanobis distance outlier tests considered in our simulations is\ngiven in Table 1.\nTable 1: Mahalanobis distance outlier tests to be compared with the Forward Search\nAcronym Description\nMDK Squared non-robust distances d2i\nasymptotic \u03c72v distribution\nMD Squared non-robust distances d2i\nExact scaled Beta distribution\nMCD Squared MCD distances d2(MCD)i\nasymptotic \u03c72v distribution\nRMCD Squared reweighted-MCD distances d2(RMCD)i\nasymptotic \u03c72v distribution\nRMCD-C Squared reweighted-MCD distances\nwith consistency correction d2(RMCD\u2212C)i\nasymptotic \u03c72v distribution\nRMCD-B Squared Bonferroni-adjusted reweighted-MCD distances d2(RMCD\u2212B)i\nasymptotic \u03c72v distribution\nRMCD-D Squared Bonferroni-adjusted reweighted-MCD distances\nwith consistency correction d2(RMCD\u2212D)i\nasymptotic \u03c72v distribution\nHR Squared MCD distances d2(MCD)i\nscaled F distribution of Hardin and Rocke (2005)\n9 Size and Power\n9.1 Size\nTo compare the performance of the various outlier tests we need them to have at least\napproximately the same size. To establish the size we performed each nominal 1% test on\n10,000 sets of simulated multivariate normal data for four values of n from 100 to 1,000\nand with dimension v = 5 and 10. The result was considered significant if at least one\noutlier was detected.\nWe summarise our findings in Table 2. For the first eight tests, based on various Ma-\nhalanobis distances, we use the Bonferroni correction to obtain a test with nominal size of\n1%. The first entry in the table is for the standard Mahalanobis distance with reference\n19\nTable 2: Size of the nominal 1% test based on 10,000 simulations (v = 5 first entry and\nv = 10 second entry in each cell): classical Mahalanobis distances, the six MCD-based\nprocedures of Table 1 and our three proposals\nn = 100 n = 200 n = 500 n = 1000\nMDK 0.28% 0.42% 0.70% 0.79%\n0.06% 0.44% 0.52% 0.89%\nMD 1.12% 0.97% 0.97% 0.89%\n1.04% 1.21% 0.99% 1.19%\nMCD 62.43% 32.91% 8.81% 3.71%\n88.59% 49.21% 11.76% 4.72%\nRMCD 30.04% 10.95% 3.78% 3.02%\n61.78% 16.37% 5.15% 3.64%\nRMCD-C 10.13% 3.39% 1.70% 1.16%\n32.25% 6.04% 2.15% 1.77%\nRMCD-B 4.94% 1.94% 1.16% 1.03%\n12.45% 3.33% 1.61% 1.40%\nRMCD-D 3.41% 1.64% 1.09% 1.01%\n8.11% 2.90% 1.51% 1.36%\nHR 2.41% 2.53% 1.17% 0.97%\n5.28% 2.34% 1.09% 1.17%\nFS1 1.02% 1.14% 1.13% 1.15%\n1.16% 1.26% 1.15% 1.19%\nFS2 1.03% 1.15% 1.14% 1.15%\n1.53% 1.27% 1.17% 1.20%\nFS3 1.04% 1.16% 1.15% 1.16%\n1.54% 1.31% 1.18% 1.20%\n20\nvalues from asymptotic \u03c72 distribution that ignores the effect of estimating the parameters.\nThe results are surprisingly bad: for n = 100 and v = 10 the size is 0.06% rather than 1%.\nEven when n = 1, 000, a value by which asymptotics are usually expected to be a good\nguide, the size is only 0.79% when v = 5. There is a sharp contrast with the results using\nthe correct Beta distribution, when the sizes correctly fluctuate between 0.89 and 1.21%.\nThese results provide a measure of the fluctuation to be found in our simulation results.\nThey also confirm that our Bonferroni correction does indeed provide a test with power\nclose to 1%. Despite the correct size of the test, our simulations in \u00a79.2 quantify what is\nwell known in general, that the standard Mahalanobis distance can have very low power\nwhen used as an outlier test.\nThe next two sets of results are for the MCD and the RMCD. These results, especially\nfor n = 100 are exceptionally bad, with sizes of up to 89%, clearly rendering the test\nunusable for \u2018small\u2019 samples of 100. As n increases, the asymptotically based correction\nfactor improves the size. But even when n = 1, 000, the sizes are between 3 and 5%. In\nview of this performance, we do not need to consider these tests any further.\nThe following four tests are versions of the MCD but with better size that improves as\nwe go down the table. For RMCD-C, that is reweighted MCD with a consistency correction\nin the reweighting, the size is around 10% when n = 100 and v = 5. When v = 10 it rises\nto over 32%. For this and the other three reweighted MCD rules the size decreases with n,\nbeing close to the hoped-for value when n = 500. In RMCD-B we extend RMCD-C by\nincluding Bonferroni reweighting to obtain sizes around 5% when n = 100 and v = 5; for\nv = 10 the value is 12.5%. The version of RMCD-B with consistency correction, which\nwe call RMCD-D, has sizes of 3.4% and 8.1% when n = 100, with all sizes less than those\nfor RMCD-B. The sizes for HR when n = 100 are also too large, although throughout the\ntable this test has values amongst the best for all values of n. The three versions of the\nforward search have satisfactory sizes for all values of n in the range studied, although the\nvalues are slightly above 1%.\nAs a result of this preliminary exploration we decided to focus our investigation on\nthe properties of four outlier detection procedures: MD, RMCD-B, HR and FS3. Other\nprocedure are sometimes included if some particular property is thereby revealed.\n9.2 Power\nTo begin our comparisons of power, Table 3 shows the results of 10,000 simulations of\nsamples with n = 200, v = 5 and with 5% of shifted observations, for a shift in all\ndimensions from 1 to 2.4; the first line of the table, in which the shift is zero, serves as a\nreminder of the size. The results are the percentage of samples in which at least one outlier\nwas detected. In this table we have included all three of the Forward Search rules of \u00a75.2.\nFor this example there is nothing to choose between these three and, in the rest of the paper,\nwe only give results for FS3.\nThe general conclusion from this table is that the FS rules behave slightly better than\nRMCD-B, which has a larger size, but lower power for level shifts above 1.6. The HR rule\nbehaves less well than these, with MD by far the worst, despite its excellent size.\nTable 4 repeats the results of Table 3 but with 30% of the observations shifted. The\nbroad conclusions from the two tables are similar, but more extreme for the more contam-\ninated data. The best rule is FS3. Unlike HR, RMCD-B loses appreciable power in this\nmore heavily contaminated setting. Most sensationally of all, masking is so strong that MD\n21\nShift in location\nL o\ng i\nt  o\nf  p\no w\ne r\n0.0 0.5 1.0 1.5 2.0\n\u2212\n4\n\u2212\n2\n0\n2\n4\n6\nFS3\nHR\nRMCD\u2212B\nMD\nn=200, v=5, cont.=5%\nShift in location\nL o\ng i\nt  o\nf  p\no w\ne r\n0.0 0.5 1.0 1.5 2.0 2.5\n\u2212\n4\n\u2212\n2\n0\n2\n4\n6\n8\nFS3\nHR\nRMCD\u2212B\nMD\nn=200, v=5, cont.=30%\nFigure 12: Power comparisons n = 200, v = 5. Logit of power: upper panel 5% contami-\nnation, lower panel 30% contamination. The lower horizontal line corresponds to a power\nof 1%, the nominal size of the tests\nindicates that less than 1% of the samples contain outliers.\nThese comparisons are made simpler by the plots of Figure 12. It is customary to plot\nthe power directly, on a scale going from 0 to 100%. However, such plots are not usually\ninformative, since virtually all procedures start with a size near zero and finish up with a\npower near one. The eye is drawn to the less informative region of powers around 50%.\nAccordingly, we instead plot the logit of the power. That is, if the power of the procedure\nis p, we plot y = log p\/(1\u2212 p), an unbounded function of p. An additional advantage of\nsuch plots is that we are able to make useful comparisons of tests with different actual sizes\nalthough the nominal sizes may be the same.\nThe upper panel of Figure 12, for 5% contamination, shows that initially FS3 has a\nmore nearly correct size than the robust procedures RMCD-B and HR and that, as the shift\nin means increases, FS3 gradually becomes the most powerful procedure. The conclusions\n22\nTable 3: Power comparisons - %: n = 200, v = 5; 5% shifted observations\nShift FS1 FS2 FS3 HR RMCD-B MD\n0 1.14 1.15 1.16 2.53 1.94 0.97\n1 3.00 3.04 3.05 3.41 4.36 2.2\n1.2 5.68 5.69 5.71 6.41 8.47 3.82\n1.4 11.43 11.46 11.47 11.54 14.60 5.74\n1.6 26.61 26.64 26.65 20.95 26.43 8.00\n1.8 53.39 53.41 53.42 34.15 45.16 11.42\n2 80.42 80.43 80.44 49.38 66.32 14.94\n2.2 95.87 95.88 95.89 65.90 83.03 18.33\n2.4 99.64 99.65 99.66 79.12 93.73 22.89\nTable 4: Power comparisons - %: n = 200, v = 5; 30% shifted observations\nShift FS3 HR RMCD-B MD\n0 1.16 2.53 1.94 0.97\n1 0.92 1.90 1.29 0.73\n1.2 0.89 2.28 1.68 0.65\n1.4 1.03 3.09 1.47 0.76\n1.6 4.47 7.85 1.37 0.63\n1.8 24.48 19.49 2.10 0.67\n2 66.39 37.95 3.64 0.73\n2.2 94.27 58.66 8.36 0.78\n2.4 99.55 77.73 19.31 0.57\n2.6 99.97 89.35 36.55 0.73\nfrom the lower panel for 30% contamination are similar. For large displacements not only\nis FS3 again the most powerful procedure, but it is comparatively more powerful than the\nother procedures as the shift increases. Robust tests of the correct size could be found\nby subtracting a constant from the logits to bring the curves down to the 1% line, when\nthe curve for FS3 would lie together with or above those for the other curves, showing\nthe superiority of the forward search procedure for these configurations. Such a proce-\ndure however does not provide operational tests as a simulation is needed to establish the\nrequired adjustment to the logits.\nTwo minor points are also of interest. One is that the lower panel reveals the complete\nmasking associated with the non-robust MD. The other is that, for 30% contamination,\nall procedures require an appreciable shift before the high proportion of outliers can be\ndetected.\nTables 5 and 6 present the results for v = 10, with the powers plotted in Figure 13.\nTable 2 shows that the sizes of HR and RMCD-B are too large. This shows in the plot by\nthe lines for these two procedures being the highest for small contamination. However, as\nthe mean shift increases the power curve for FS3 rises more rapidly revealing it again as\n23\nthe most powerful procedure.\nShift in location\nL o\ng i\nt  o\nf  p\no w\ne r\n0.0 0.5 1.0 1.5 2.0\n\u2212\n4\n\u2212\n2\n0\n2\n4\n6\nFS3\nHR\nRMCD\u2212B\nMD\nn=200, v=10, cont.=5%\nShift in location\nL o\ng i\nt  o\nf  p\no w\ne r\n0.0 0.5 1.0 1.5 2.0\n\u2212\n4\n\u2212\n2\n0\n2\n4\nFS3\nHR\nRMCD\u2212B\nMD\nn=200, v=10, cont.=30%\nFigure 13: Power comparisons n = 200, v = 10. Logit of power: upper panel 5% contam-\nination, lower panel 30% contamination. The lower horizontal line corresponds to a power\nof 1%, the nominal size of the tests\nAs a final comparison we look at some results for much larger samples, n = 1000,\nwith v = 5 and 5% contamination. The results are in Table 7. The first four comparisons\nare those of the procedures FS3, HR, RMCD-B and MD that we have already compared\nfor n = 200. The results are plotted in the upper panel of Figure 14. Now, as we know\nfrom Table 2, all procedures have virtually the correct size, so the plots of power start close\ntogether. As we have seen before, FS3 has the highest power for larger shifts in mean.\nAlso included in Table 7 are the results for three further versions of reweighted MCD\ndistances. These power curves are plotted, together with that for FS3 in the lower panel\nof Figure 14. The plot for RMCD, the reweighted estimator without the consistency factor\nand lacking the Bonferroni adjustment in the reweighting, lies above the very similar curve\nfor RMCD-C, the version of RMCD in which the consistency correction was included.\n24\nShift in location\nL o\ng i\nt  o\nf  p\no w\ne r\n0.0 0.5 1.0 1.5\n\u2212\n4\n\u2212\n2\n0\n2\n4\n6\nFS3\nHR\nRMCD\u2212B\nMD\nn=1000, v=5, cont.=5%\nShift in location\nL o\ng i\nt  o\nf  p\no w\ne r\n0.0 0.5 1.0 1.5\n\u2212\n4\n\u2212\n2\n0\n2\n4\n6\nFS3\nRMCD\nRMCD\u2212C\nRMCD\u2212D\nn=1000, v=5, cont.=5%\nFigure 14: Power comparisons n = 1, 000, v = 10 with 5% contamination. Logit of power:\nupper panel FS3, HR, RMCD-B and the nonrobust MD. Lower panel FS3 and three further\nreweighted versions of MCD. The lower horizontal line corresponds to a power of 1%, the\nnominal size of the tests\nBoth have lower power than FS3 for large mean shifts. The final version of these robust\ndistances, RMCD-D, is the version of RMCD in which we use a Bonferroni adjustment\nin the reweighting of RMCD with additional consistency correction. This has the poorest\nperformance of all, apart from the non-robust MD. The conclusion is that, once adjustment\nis made for size, RMCD has much the same properties as HR and RMCD-C. Here HR is\nbetter than RMCD-B, as it is for the datasets with n = 200 and 30% contamination. There\nis little to choose between these two for n = 200 and 5% contamination when adjustment\nis made for size. In all comparisons FS3 has the highest power, combined with good size.\n25\n10 Examples\nOur results show the good size and superior power of our forward search procedures. In\nthis section we conclude by revisiting our three examples and suggest why our procedure\nhas greater power than that of MCD derived approaches.\n10.1 Slight Contamination\nIn this example there were 500 observations, v = 5 and the first 25 units were contaminated.\nThe forward plot of minimum Mahalanobis distances in Figure 7 exhibited a series of large\nvalues aroundm = 480, but with the value at the end of the search below the 99% envelope.\nSuch masked behaviour can be expected to cause difficulties for methods that test only the\nlargest value of a robust Mahalanobis distance based on an arbitrary or non-adaptive subset\nof the data. We now look at a variety of distances for each unit.\nIndex number\nM\nC D\n0 100 200 300 400 500\n0\n5\n1 0\n1 5\n2 0\n2 5\n3 0\nIndex number\nR\nM\nC D\n0 100 200 300 400 500\n0\n5\n1 0\n1 5\n2 0\n2 5\n3 0\nIndex number\nR\nM\nC D\n\u2212 B\n0 100 200 300 400 500\n0\n5\n1 0\n1 5\n2 0\n2 5\n3 0\nIndex number\nH\nR\n0 100 200 300 400 500\n0\n2\n4\n6\n8\nFigure 15: Slightly contaminated data: output from MCD, RMCD, RMCD-B and HR.\nDistances against unit number, the original units are represented by black dots, the 25\ncontaminated units by the symbol +. Upper line, 1% Bonferroni limit; lower line, 1% limit\nof individual distribution\nThe top left-hand panel of Figure 15 shows a plot of MCD distances against observa-\ntion number, with the first 25 units, shown by crosses, being those that are contaminated.\nThe lower horizontal line on the plot is the 1% point of the nominal distribution of the\nindividual statistics. As the figure shows, for this and all other distances, there are several\nuncontaminated units above this threshold as well as, in this case, half of the contaminated\nunits. A Bonferroni limit is clearly needed. However, in this and all other panels, imposi-\ntion of the Bonferroni limit, the upper horizontal bound, fails to reveal any outliers. This\nmuch structure of the four panels is common. However, the figure does show that RMCD\ncomes nearest to revealing the presence of outliers, with seven of the first 25 units forming\n26\nthe largest distances. Although the Bonferroni bound is so large that these distances are not\nsignificant, the bound is not, in general, conservative. For instance, the size of RMCD with\nn = 500 and v = 5 is almost 4%.\nTraditional MD\nM\nC D\n M\nD\n0 5 10 15 20\n0\n5\n1 0\n1 5\n2 0\nTraditional MD\nM\nC D\n M\nD\n0 5 10 15\n0\n1 0\n2 0\n3 0\nFigure 16: Robust Mahalanobis distances MCD against non-robust Mahalanobis distances\nMD. The original units are represented by black dots, the contaminated units by the sym-\nbol +. Left-hand panel, slightly contaminated data, right-hand panel, appreciably contam-\ninated data\nIt is frequently suggested, for example by Rousseeuw and van Zomeren (1990), that a\nuseful diagnostic for the presence of outliers is to compare robust and non-robust analyses.\nAccordingly, in the left hand-panel of Figure 16 we show a scatter plot of distances from the\nMCD against the full-sample Mahalanobis distances. This plot is unrevealing. Although\nthere is a preponderance of contaminated units in the upper-right hand corner of the plot,\nthe two sets of distances form a wedge pattern, with no obvious differences between the\ntwo. The structure is basically linear, with scatter increasing with magnitude. There is no\ndiagnosis of the presence of outliers.\n10.2 Appreciable Contamination\nWe now repeat this analysis but for the appreciably contaminated data of \u00a76.2; there are 200\nobservations with v = 5, but there is 30% contamination which is in units 1-60 caused by a\nmean shift of 1.2 in each dimension. The forward plot of minimum Mahalanobis distances\nwas given in Figure 10. This again has a peak well before the end, with \u2018good\u2019 behaviour\nafter m = 145.\nThe panels of Figure 17 repeat those of Figure 15, showing plots of four robust distances\nagainst observation number, now with the first 60 units, shown by crosses, being those that\nare contaminated. The lower horizontal line on the plot is the 1% point of the nominal\ndistribution of the individual statistics.\nThese plots make rather different points from those of Figure 15. Again, there would\nbe a large number of outliers if the limit for the individual statistics were used and virtually\nnone if the Bonferroni limit is employed. In fact, only the MCD would lead to detection of\nan \u2018outlier\u2019. However, as the upper-left panel of the figure shows, this observation is not an\noutlier. More surprisingly, none of the four procedures shows a contaminated observation\nas having the largest distance. With this relatively small sample size, the procedures are\ncompletely failing to detect the 30% of contamination in the data. This might seem puzzling\ngiven the large size of the procedures revealed in Table 2. However, the latter part of the\nforward plot of Figure 10 shows that if the parameters are estimated from a subset including\n27\nIndex number\nM\nC D\n0 50 100 150 200\n0\n1 0\n2 0\n3 0\nIndex number\nR\nM\nC D\n0 50 100 150 200\n0\n5\n1 0\n1 5\n2 0\n2 5\n3 0\nIndex number\nR\nM\nC D\n\u2212 B\n0 50 100 150 200\n0\n5\n1 0\n1 5\n2 0\n2 5\n3 0\nIndex number\nH\nR\n0 50 100 150 200\n0\n2\n4\n6\n8\nFigure 17: Appreciably contaminated data: output from MCD, RMCD, MCD-B and HR.\nDistances against unit number, the original units are represented by black dots, the 60\ncontaminated units by the symbol +. Upper line, 1% Bonferroni limit; lower line, 1% limit\nof individual distribution\ncontaminated observations, the resulting over-estimation of \u03a3 leads to small distances and\na failure to detect outliers.\nFinally, in the right hand-panel of Figure 16 we show the scatter plot of distances from\nthe MCD against the standard non-robust Mahalanobis distances. The structure is similar\nto that for the lightly contaminated data in the left-hand panel, but certainly no more infor-\nmative about the existence of the 60 outliers. In fact, the plot might even be thought to be\nmisleading; the majority of observations with larger distances lying away from the centre\nof the wedge shape are uncontaminated units.\n10.3 Swiss Banknotes\nApplication of FS1 to the forward plot of distances in Figure 1 yields a value of 84 for m\u2020.\nFigure 18 shows the successive superimposition of envelopes from this value. There is no\nevidence of any outliers when n = 84 and 85, but when n = 86 we obtain clear evidence\nof a single outlier with observation [86] well outside the 99% envelope. When n = 87 we\nhave even stronger evidence of the presence of outliers. As a result we conclude that there\nare 15 outlying observations in the data on forged banknotes.\nFor these data the four robust methods we have compared on other sets of data also all\nreveal the presence of outliers. As the index plots of Mahalanobis distances in Figure 19\nshow, the 1% Bonferroni level for MCD, RMCD and RMCD-B all reveal the 15 outliers\nwithout any false positives. However, the sizes of these procedures when n = 100 are\ntotally unacceptable, namely 62%, 30% and 5%. Only HR is too conservative, indicating\njust 5 outliers, from a test with size 2.4%. Since HR is a rescaled version of MCD, the\n28\nSubset size m\nM\na h\na l\na n\no b\ni s  \nd i\ns t\na n\nc e\ns\n20 30 40 50 60 70 80\n3\n4\n5\n6\n7\nTry n=84\nSubset size m\nM\na h\na l\na n\no b\ni s  \nd i\ns t\na n\nc e\ns\n20 30 40 50 60 70 80\n3\n4\n5\n6\n7\nTry n=85\nSubset size m\nM\na h\na l\na n\no b\ni s  \nd i\ns t\na n\nc e\ns\n20 30 40 50 60 70 80\n3\n4\n5\n6\n7\nTry n=86\nSubset size m\nM\na h\na l\na n\no b\ni s  \nd i\ns t\na n\nc e\ns\n20 30 40 50 60 70 80\n3\n4\n5\n6\n7\nTry n=87\nFigure 18: Swiss Banknotes: forward plot of minimum Mahalanobis distance. When n =\n84 and 85, the observed curve lies within the 99% envelope, but there is clear evidence of\nan outlier when n = 86. The evidence becomes even stronger when another observation is\nincluded.\n29\nIndex number\nM\nC D\n0 20 40 60 80 100\n0\n2 0\n4 0\n6 0\n8 0\n1 0\n0\n1 2\n0\nIndex number\nR\nM\nC D\n0 20 40 60 80 100\n0\n2 0\n4 0\n6 0\n8 0\n1 0\n0\n1 2\n0\nIndex number\nR\nM\nC D\n\u2212 B\n0 20 40 60 80 100\n0\n2 0\n4 0\n6 0\n8 0\n1 0\n0\n1 2\n0\nIndex number\nH\nR\n0 20 40 60 80 100\n0\n5\n1 0\n1 5\nFigure 19: Swiss banknote data: output from MCD, RMCD, MCD-B and HR. Distances\nagainst unit number; the 15 outlying units are represented by the symbol +. Upper line,\n1% Bonferroni limit; lower line, 1% limit of individual distribution\nfigure confirms that the 15 outliers do indeed have the largest distances for HR. The plot of\nrobust against non-robust distances in Figure 20 also revels the 15 outliers, which fall into\na separate group from the other 85 observations.\nThese plots serve to make the point that our comparisons have been solely of whether\nthe methods identify at least one outlier in a sample. The comparison of methods for the\nnumber of outliers can be problematic. Consider our canonical example of a multivariate\nnormal sample, some observations of which have a mean shift. If the shift is sufficiently\nlarge, the outliers will be evident and most methods will detect them. However, if as in\nour paper, the shift is slight, the two groups will overlap and the number of \u2018outliers\u2019 will\nnot be as great a the number of shifted observations. Comparisons of methods then require\na two-way table of counts for each procedure in which the factors are whether or not the\nobservation was shifted and whether it was identified as outlying.\nAppendix 1: Numerical\nIn \u00a74.1 we mentioned that care is needed in evaluating the integral in (8) for large n as\nm \u2192 n. For example, when n = 1, 000 and v = 10, in the final step of the search we\nhave m = n\u2212 1 = 999, x2,2000;0.01 = 0.01005 and F (y2000,2000;0.99) = 0.9999899497. This\nimplies that we have to find the quantile of an F distribution with 10 and 989 degrees of\nfreedom associated with probability 0.9999899497; in Fortran the IMSL function DFIN\ngave a value of 4.1985, the same value as the S-Plus function qf. Using this number we\nobtain a value of 6.512259 in equation (10). After dividing by the consistency factor we\nobtain a final value of 6.520. Note that the Bonferroni value is 6.426 and the coefficient\n30\nTraditional MD\nM\nC D\n M\nD\n0 5 10 15 20 25\n0\n2 0\n4 0\n6 0\n8 0\n1 0\n0\n1 2\n0\nFigure 20: Swiss banknote data: robust Mahalanobis distances MCD against non-robust\nMahalanobis distances MD. The 15 outlying units are represented by the symbol +\nobtained by Hadi using simulations is 6.511. From 30,000 simulations using Gauss the\nvalue we obtained was 6.521, very close to our final value coming from the theoretical\narguments leading to (10).\nAppendix 2: The \u03c723 c.d.f. as a Function of the Standard\nNormal Distribution\nThe application of standard results from probability theory shows that the variance of the\ntruncated normal distribution containing the central m\/n portion of the full distribution is\n\u03c32T (m) = 1\u2212\n2n\nm\n\u03a6\u22121\n(\nn+m\n2n\n)\n\u03c6\n{\n\u03a6\u22121\n(\nn+m\n2n\n)}\n,\nwhere \u03c6(.) and \u03a6(.) are respectively the standard normal density and c.d.f. See, for exam-\nple, Johnson, Kotz, and Balakrishnan (1994, pp. 156-162). On the other hand the results\nfrom elliptical truncation due to Tallis (1963) that we used in \u00a74.2 show that this variance\ncan be written as\n\u03c32T (m) =\nn\nm\nF\u03c723\n{\nF\u22121\n\u03c721\n(m\nn\n)}\nAfter some algebra it appears that\nF\u22121\n\u03c721\n(m\nn\n)\n=\n{\n\u03a6\u22121\n(\nm+ n\n2n\n)}2\n,\nwhen, rearranging terms, we easily obtain that\nF\u03c723(x\n2) =\nm\nn\n\u2212 2x\u03c6(x)\nwhere x = \u03a6\u22121{(m+ n)\/(2n)}. This result links the c.d.f of the \u03c723 in an unexpected way\nto the density and c.d.f. of the standard normal distribution.\n31\nReferences\nAtkinson, A. C., M. Riani, and A. Cerioli (2004). Exploring Multivariate Data with the\nForward Search. New York: Springer\u2013Verlag.\nButler, R. W., P. L. Davies, and M. Jhun (1993). Asymptotics for the minimum covari-\nance determinant estimator. The Annals of Statistics 21, 1385\u20131400.\nCox, D. R. and D. V. Hinkley (1974). Theoretical Statistics. London: Chapman and Hall.\nCroux, H. and G. Haesbroeck (1999). Influence function and efficiency of the minimum\ncovariance determinant scatter matrix estimator. Journal of Multivariate Analysis 71,\n161\u2013190.\nCroux, H. and G. Haesbroeck (2000). Principal component analysis based on robust es-\ntimators of the covariance or correlation matrix: influence functions and efficiencies.\nBiometrika 87, 603\u2013618.\nDavies, L. (1992). The asymptotics of Rousseeuw\u2019s minimum volume ellipsoid estima-\ntor. The Annals of Statistics 20, 1828\u20131843.\nDavies, L. and U. Gather (1993). The identification of multiple outliers (with discus-\nsion). Journal of the American Statistical Association 88, 782\u2013801.\nFlury, B. and H. Riedwyl (1988). Multivariate Statistics: A Practical Approach. London:\nChapman and Hall.\nGarc\u0131\u00b4a-Escudero, L. A. and A. Gordaliza (2005). Generalized radius processes for ellip-\ntically contoured distributions. Journal of the American Statistical Association 100,\n1036\u20131045.\nGather, U., J. Pawlitschko, and I. Pigeot (1997). A note on invariance of multiple tests.\nStatistica Neerlandica 51, 366\u2013372.\nGuenther, W. C. (1977). An easy method for obtaining percentage points of order statis-\ntics. Technometrics 19, 319\u2013321.\nHadi, A. S. (1994). A modification of a method for the detection of outliers in multivari-\nate samples. Journal of the Royal Statistical Society, Series B 56, 393\u2013396.\nHadi, A. S. and J. S. Simonoff (1993). Procedures for the identification of multiple\noutliers in linear models. Journal of the American Statistical Association 88, 1264\u2013\n1272.\nHardin, J. and D. M. Rocke (2005). The distribution of robust distances. Journal of\nComputational and Graphical Statistics 14, 910\u2013927.\nJohnson, N. L., S. Kotz, and N. Balakrishnan (1994). Continuous Univariate Distribu-\ntions - 1, 2nd Edition. New York: Wiley.\nLehmann, E. (1991). Point Estimation, 2nd edition. New York: Wiley.\nLopuhaa\u00a8, H. P. (1999). Asymptotics of reweighted estimators of multivariate location\nand scatter. The Annals of Statistics 27, 1638\u20131665.\nMaronna, R. A., R. D. Martin, and V. J. Yohai (2006). Robust Statistics: Theory and\nMethods. Chichester: Wiley.\n32\nPison, G., S. Van Aelst, and G. Willems (2002). Small sample corrections for LTS and\nMCD. Metrika 55, 111\u2013123.\nRousseeuw, P. J. and A. M. Leroy (1987). Robust Regression and Outlier Detection.\nNew York: Wiley.\nRousseeuw, P. J. and K. Van Driessen (1999). A fast algorithm for the minimum covari-\nance determinant estimator. Technometrics 41, 212\u2013223.\nRousseeuw, P. J. and B. C. van Zomeren (1990). Unmasking multivariate outliers and\nleverage points. Journal of the American Statistical Association 85, 633\u20139.\nStuart, A. and K. J. Ord (1987). Kendall\u2019s Advanced Theory of Statistics, Vol.1, 5th\nEdition. London: Griffin.\nTallis, G. M. (1963). Elliptical and radial truncation in normal samples. Annals of Math-\nematical Statistics 34, 940\u2013944.\nWilks, S. S. (1963). Multivariate statistical outliers. Sankhya A 25, 407\u2013426.\nWisnowski, J. W., D. C. Montgomery, and J. R. Simpson (2001). A comparative analy-\nsis of multiple outlier detection procedures in the linear regression model. Compu-\ntational Statistics and Data Analysis 36, 351\u2013382.\n33\nTable 5: Power comparisons - %: n = 200, v = 10; 5% shifted observations\nShift FS3 HR RMCD-B MD\n0 1.31 2.34 3.33 1.21\n1 4.79 6.59 10.34 2.78\n1.2 12.75 14.93 19.93 4.31\n1.4 36.96 31.01 40.61 5.96\n1.6 73.98 52.83 68.78 8.04\n1.8 95.93 75.47 90.47 10.31\n2 99.70 89.96 98.13 12.23\nTable 6: Power comparisons - %: n = 200, v = 10; 30% shifted observations\nShift FS3 HR RMCD-B MD\n0 1.31 2.34 3.33 0.0121\n1 1.04 2.42 2.98 0.086\n1.2 1.50 2.69 3.00 0.085\n1.4 10.79 7.30 3.64 0.091\n1.6 47.91 24.85 5.51 0.083\n1.8 79.79 53.85 15.75 0.085\n2 91.21 78.93 40.00 0.082\n2.2 95.65 91.18 71.79 0.084\n2.4 98.02 96.52 92.00 0.072\n2.6 99.17 98.13 97.67 0.073\nTable 7: Power comparisons for seven rules on large samples - %: n = 1000, v = 5; 5%\nshifted observations\nShift FS3 HR RMCD-B MD RMCD RMCD-C RMCD-D\n0 1.16 1.15 1.11 0.99 2.80 1.39 1.08\n1 6.02 5.98 3.45 2.88 11.63 5.79 3.33\n1.2 23.00 16.27 5.49 4.44 27.47 13.69 5.26\n1.4 52.40 39.47 10.07 7.04 59.75 35.60 9.40\n1.6 94.00 71.11 18.58 10.97 91.11 71.96 16.77\n1.8 99.90 92.26 35.39 14.20 99.65 95.90 30.31\n34\n"}