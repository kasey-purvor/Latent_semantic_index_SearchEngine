{"doi":"10.1080\/0968776030110304","coreId":"14203","oai":"oai:generic.eprints.org:412\/core5","identifiers":["oai:generic.eprints.org:412\/core5","10.1080\/0968776030110304"],"title":"Analysing tutor feedback to students: First steps towards constructing an electronic monitoring system","authors":["Whitelock, Denise","Watt, Stuart","Raw, Yvonne","Moreale, Emanuela"],"enrichments":{"references":[{"id":197279,"title":"A set of categories for the analysis of small group interaction',","authors":[],"date":"1950","doi":"10.2307\/2086790","raw":"Bales, R. F. (1950), 'A set of categories for the analysis of small group interaction', American Sociological Review, 15, 257-63.","cites":null},{"id":197280,"title":"Analyzing Teacher Behaviour,","authors":[],"date":"1970","doi":null,"raw":"Flanders, N. (1970), Analyzing Teacher Behaviour, Reading, MA: Addison-Wesley.","cites":null},{"id":197278,"title":"Communication in a web-based conferencing system: the quality of computer-mediated interactions',","authors":[],"date":"2003","doi":"10.1111\/1467-8535.00302","raw":"Angeli, C., Valanides, N. and Bonk, C. J. (2003), 'Communication in a web-based conferencing system: the quality of computer-mediated interactions', British Journal of Educational Technology, 34, 1, 31-43.","cites":null},{"id":197282,"title":"The debate on automated essay grading, IEEE intelligent systems', September-October, http:\/\/www.knowledge-technologies.com\/presskit\/KAT_IEEEdebate.pdf","authors":[],"date":"2000","doi":null,"raw":"Hearst, M. (2000), 'The debate on automated essay grading, IEEE intelligent systems', September-October, http:\/\/www.knowledge-technologies.com\/presskit\/KAT_IEEEdebate.pdf Landauer, T. K., Foltz, P. W. and Laham, D. (1997), Introduction to Latent Semantic Analysis, Discourse Processes, 25, 259-84. http:\/\/Isa.colorado.edu\/papers\/dpl.LSAintro.pdf Moreale, E., Whitelock, D., Raw, Y. and Watt, S. (2002), 'What measures do we need to build an electronic monitoring tool for post graduate tutor marked assignments?' Sixth International Computer Assisted Assessment Conference, 9-10 July, Loughborough, pp. 253-67.","cites":null},{"id":197281,"title":"The Digital University: Building a Learning Community,","authors":[],"date":"2002","doi":"10.1007\/978-1-4471-0167-3","raw":"Hazemi, R. and Hailes, S. (eds) (2002), The Digital University: Building a Learning Community, London: Springer.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2003","abstract":"Virtual Learning Environments provide the possibility of offering additional support to tutors, monitors and students in writing and grading essays and reports. They enable monitors to focus on the assignments that need most attention. This paper reports the findings from phase one of a feasibility study to assist the monitoring of student essays. It analyses tutor comments from electronically marked assignments and investigates how they match the mark awarded to each essay by the tutor. This involved carrying out a category analysis of the tutors\u2019 feedback to the students using Bales's \u2018interactional categories\u2019 as a theoretical basis. The advantage of this category system is that it distinguishes between task\u2010orientated contributions, and the \u2018socio\u2010emotive\u2019 element used by tutors to maintain student motivation. This reveals both how the tutor makes recommendations to improve the assignment content, and how they provide emotional support to students. Bales's analysis was presented to a group of tutors who felt an electronic feedback system based on this model would help them to get the right balance of responses to their students. These findings provide a modest start to designing a model of feedback for tutors of distance education students. Future work will entail refining these categories and testing this model with a larger sample from a different subject domain","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/14203.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/412\/1\/ALT_J_Vol11_No3_2003_Analysing%20tutor%20feedback%20to%20st.pdf","pdfHashValue":"03210dae5476662e60381bfc258c03f7b631521d","publisher":"University of Wales Press","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:412<\/identifier><datestamp>\n      2011-04-04T09:09:40Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/412\/<\/dc:relation><dc:title>\n        Analysing tutor feedback to students: First steps towards constructing an electronic monitoring system<\/dc:title><dc:creator>\n        Whitelock, Denise<\/dc:creator><dc:creator>\n        Watt, Stuart<\/dc:creator><dc:creator>\n        Raw, Yvonne<\/dc:creator><dc:creator>\n        Moreale, Emanuela<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        Virtual Learning Environments provide the possibility of offering additional support to tutors, monitors and students in writing and grading essays and reports. They enable monitors to focus on the assignments that need most attention. This paper reports the findings from phase one of a feasibility study to assist the monitoring of student essays. It analyses tutor comments from electronically marked assignments and investigates how they match the mark awarded to each essay by the tutor. This involved carrying out a category analysis of the tutors\u2019 feedback to the students using Bales's \u2018interactional categories\u2019 as a theoretical basis. The advantage of this category system is that it distinguishes between task\u2010orientated contributions, and the \u2018socio\u2010emotive\u2019 element used by tutors to maintain student motivation. This reveals both how the tutor makes recommendations to improve the assignment content, and how they provide emotional support to students. Bales's analysis was presented to a group of tutors who felt an electronic feedback system based on this model would help them to get the right balance of responses to their students. These findings provide a modest start to designing a model of feedback for tutors of distance education students. Future work will entail refining these categories and testing this model with a larger sample from a different subject domain.<\/dc:description><dc:publisher>\n        University of Wales Press<\/dc:publisher><dc:date>\n        2003<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/412\/1\/ALT_J_Vol11_No3_2003_Analysing%20tutor%20feedback%20to%20st.pdf<\/dc:identifier><dc:identifier>\n          Whitelock, Denise and Watt, Stuart and Raw, Yvonne and Moreale, Emanuela  (2003) Analysing tutor feedback to students: First steps towards constructing an electronic monitoring system.  Association for Learning Technology Journal, 11 (3).  pp. 31-42.  ISSN 0968-7769     <\/dc:identifier><dc:relation>\n        10.1080\/0968776030110304<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/412\/","10.1080\/0968776030110304"],"year":2003,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"Analysing tutor feedback to students: first steps\ntowards constructing an electronic monitoring\nsystem\nDenise Whitelock,* Stuart Watt,** Yvonne Raw* and Emanuela Moreale*\n*The Open University, **Robert Gordon University\nemail: dm.whitelock@open.Qc.uk\nVirtual Learning Environments provide the possibility of offering additional support to\ntutors, monitors and students in writing and grading essays and reports. They enable\nmonitors to focus on the assignments that need most attention. This paper reports the\nfindings from phase one of a feasibility study to assist the monitoring of student essays.\nIt analyses tutor comments from electronically marked assignments and investigates how\nthey match the mark awarded to each essay by the tutor. This involved carrying out a\ncategory analysis of the tutors' feedback to the students using Bales's 'interactional\ncategories' as a theoretical basis. The advantage of this category system is that it\ndistinguishes between task-orientated contributions, and the 'socio-emotive' element used\nby tutors to maintain student motivation. This reveals both how the tutor makes\nrecommendations to improve the assignment content, and how they provide emotional\nsupport to students. Bales's analysis was presented to a group of tutors who felt an\nelectronic feedback system based on this model would help them to get the right balance\nof responses to their students. These findings provide a modest start to designing a model\nof feedback for tutors of distance education students. Future work will entail refining\nthese categories and testing this model with a larger sample from a different subject\ndomain.\nIntroduction\nThe digital university is becoming more fact than fiction with the adoption of Virtual\nLearning Environments (VLEs), with perceived pedagogical and administrative advantages\n(Hazemi and Hailes, 2002). Students are encouraged to submit their course work\nelectronically with tutors commenting electronically on the scripts. This enables the\nfeedback process to be speeded up. This was found to be the case with the Open\nUniversity's bespoke electronic Tutor Marked Assignment (TMA) system. VLEs offer\n31\nDenise Whitebck et al Analysing tutor feedback to students: constructing an electronic monitoring system\nadditional support to tutors, monitors and students for writing and grading essays and\nreports. This enables monitors to focus on the assignments that most need their attention\nby providing tutors with high-quality feedback.\nThere are a number of options that can be taken to monitor the marking of students'\nelectronic assignments. One is to develop an automatic essay grading system to analyse the\ntext of a TMA and award an appropriate mark. This mark can then be checked against the\none given by the tutor. Such an automated system enables monitors to focus on\nassignments that most needed their attention. Tutors can then be given high-quality\nfeedback to assist them in developing their marking skills.\nThe basis for this approach comes from the development of automatic essay-grading\nsystems in the United States since the mid-1960s. Although these offered fairly good\ncorrelations with human graders, they relied on superficial features which are easy to\nextract such as length, average word length, use of punctuation and use of certain key\nwords. This is still largely the case, although the features used are often far more\ncomputationally intensive than they used to be (see Hearst, 2000).\nTwo essay grading systems are worth describing in particular, e-rater (ETS) and the\nIntelligent Essay Assessor (IEA). E-rater (ETS) developed about 100 different linguistic\nfeatures, and then used regression analysis to develop a scoring model that compared well\nwith human graders. ETS developed this into a system that scored essays with a 90 per cent\ncorrelation with expert assessors. The Intelligent Essay Assessor uses latent semantic\nanalysis (LSA), which is a statistical technique, designed to estimate how similar the\ncontent of one body of text is to another, at a semantic level rather than at the word level.\nIn estimating this similarity, it corresponds remarkably well to human scorers. Using LSA\nfor essay grading involves indexing pre-graded essays, with associated feedback, so that\nnew essays can be graded by finding the best matches among the pre-graded ones. The\nstrength of IEA is that it can be used to give constructive feedback. Finally, it is considered\npsychologically sound as it is a theory of language and matching several significant\npsycholinguistic effects, for example, Landauer, Foltz and Laham (1997).\nThe FRAMES project set out to explore how techniques such as these could be used to\nassist monitoring as well as student and tutor support, in the essay assessment process.\nInitially, we began with a set of readability and text scoring measures that were claimed\nwould account for the grade awarded to an undergraduate essay (Burstein, Marcu,\nAndreyev and Chodorov, 2001). These non-content metrics were used to build a predictive\nmodel from a training set of scripts using LSA. This model allows a previously unseen\nscript to be analysed and awarded a grade. We have found that the measures suggested by\nthe literature were not sufficient to construct an adequate model for master's level essays\nand that other indicators of student understanding needed to be included in order to\nproduce a workable model (see Moreale, Whitelock, Raw and Watt, 2002).\nA second option to monitor the marking of students' assignments is to focus on tutors'\ncomments that have been inserted into the students' essays. The comments found on the\nTMAs form a rich data set from which to extract some generic findings with respect to\ncomments and the mark awarded. This raised the question of how these trends could be\nidentified and translated into rules for an electronic monitoring system. One approach is to\nconstruct an analytical framework of the types of interactions that occur between tutor\n32\nALT-] Volume 11 Number 3\nand student in the tutor comments; then to count these interactions and see if a trend\nemerges according to the mark that was awarded to the assignments in question. The\nsimplicity of such an analysis is deceptive since the categories under investigation must be\nwell operationalized and the classification schema must be consistently adhered to as\nopposed to being dependent on the views of the individual observer. It would, therefore, be\nbetter to use a tried and tested system of interactional categories and see if it would fit our\ncontext rather than to construct one from scratch.\nOne system we considered was Flanders's (1970) set of interactional categories. These were\ndesigned to record what goes on in classrooms. Flanders's scheme uses ten separate\ncategories to record teacher and pupil interactions. Three of the categories that refer to\npupil talk are irrelevant to the TMA context. In our case the pupil 'talk' is the essay itself.\nThe other Flanders's categories are concerned with teacher talk in response to the student-\ninitiated responses, such as accepts ideas or feelings and praises, while the remaining\ncategories are concerned with teacher-initiated events which are directed at controlling\nclassroom behaviour. These include 'criticizes behaviour' which is expected to change\nthrough the teacher interaction. 'Gives direction or orders' is another controlling category.\nThese latter categories did not fit into the distance tutoring model for this master's level\ncourse and so this system was rejected.\nAll distance students require feedback from their tutor not only about the subject matter\nbut also an acknowledgement of their effort and progress. Hence an explicit level of socio-\nemotive support as well as direct instruction is necessary and is emphasized in the training\nof OU tutors. In fact the types of interactions that go on in face-to-face situations are\nencouraged in the remarks of both the online and paper-based tutor. A set of categories\ndevised by Bales (1950) appeared to be more suitable since it distinguishes between task-\norientated contributions and 'socio-emotive' interjections. However, it was devised to\nanalyse face-to-face interactions and not text dialogues. We also considered a system\ndeveloped by Angeli, Valanides and Bonk (2003), who distilled a set of eleven categories to\nhelp tutors provide online assistance but again the socio-emotive role is not explicit. The\nanalysis reported here adopted Bales's framework and set out to classify the tutor\ncomments typed on to the essay. This is the particular feedback addressing issues as they\nappear in the students' written text. The study aimed to:\n\u2022 investigate whether a Bales interactional analysis of the tutor comments could provide\nan adequate model of the tutors' written feedback on the student assignments;\n\u2022 identify trends in these interactions that accompany the gTade awarded to the\nassignment;\n\u2022 translate these trends into a set of heuristics which will form the basis of an automated\nassessment tool which will be used by the examinations office to select TMAs for\nmonitoring purposes.\nProcedure\nThe electronically marked TMAs chosen for analysis were taken from the MA module in\nOpen and Distance Learning entitled 'Foundations of Open and Distance Education'. The\ntotal number was 194 selected from 42 students. The students were required to submit five\n33\n- Denise Whitelock et al Analysing tutor feedback to students: constructing an electronic monitoring system\nTMAs before submitting their final dissertation; however, the fifth TMA involved\ndeveloping a proposal for a subsequent dissertation, and was therefore omitted from this\nstudy. Some students did not submit all their TMAs. The marks from the four TMAs\ncontributed to 50 per cent of the student's final grade. The students were seeking\nsubstantial feedback and guidance from their tutors' comments in their assignments in\norder to improve their marks during the course and this type of feedback was considered\nto be the most personally helpful to them throughout the duration of the course.\nThe syllabus for this module covered the following topics:\n\u2022 the theory and practice of open and distance learning;\n\u2022 terms and rationales in open and distance education;\n\u2022 becoming a critically reflective practitioner;\n\u2022 theories of open and distance learning;\n\u2022 characteristics and needs of learners;\n\u2022 interaction in open and distance learning.\nThe TMAs were designed to examine student understanding of all the topics in the course\nand required them to submit a well argued and informed account of current theories and\nresearch into open and distance learning.\nStudents\nThis cohort of students consisted of international educational professionals, from Greece,\nSwitzerland, Japan and the United States. There were no face-to-face tutorials; tutoring\ntook place online. A small number had already obtained Ph.D.s and were currently\nworking in universities but wanted to understand more about distance and online learning\nas they were about to embark on devising such courses themselves. Others had a software-\ndesign background. All were committed, conscientious participants, although they had not\nstudied for a number of years and were unused to being cast in a student role.\nTutors\nThe three tutors for this presentation of the MA module also wrote the course materials\nand were experienced researchers in the field. They had tutored at least three other OU\ncourses and were adept users of the electronic TMA system. Tutors' comments should\ntherefore not only illustrate a sound knowledge of the domain, but also provide exemplars\nof positive constructive advice to students on how to improve their TMA score - a facet of\ntutoring that can be detected by Bales's categories. The first tutor marked seventy scripts,\nthe second sixty-three scripts, and third sixty-one scripts. The small difference in number is\ndue to initial difference in tutorial group numbers and some students not submitting all\ntheir TMAs. It is also worth noting that the number of comments made by the tutors\nincreased in the later assignments, as shown in Figure 1.\nUsing Bales's interaction analysis\nBales' twelve interactional categories are shown in Table 1. They were designed to record\nwhat was being achieved during group interaction sessions. The strength of this system\naccording to Sapsford (1999) is that it is a subtle, rich and sophisticated measuring\n34\nALT-} Volume 11 Number 3\nFigure I: Changes in mean\ncomments through the\nassignments\nMean comments for each assignment\n45\n40\n35\no\n\u00b0\n2 3\nAssignment (TMA) number\ninstrument that was designed to distinguish between task-orientated and socio-emotive\ncontributions in a group session. Open University training for tutors stresses the\nimportance of praise and constructive guidance and these categories had the potential to\ncapture this type of tutor feedback.\nCategories Specific examples\nPositive reactions\nAI I. Shows solidarity\nA2 2. Shows tension release\nA3 3. Shows agreement\nAttempted answers\njokes, gives help, rewards others\nLaughs, shows satisfaction\nUnderstands, concurs, complies, passively accepts\nBI 4. Gives suggestion\nB2 5. Gives opinion\nB3 6. Gives information\nQuestions\nDirects, proposes, controls\nEvaluates, analyses, expresses feelings or wishes\nOrients, repeats, clarifies, confirms\nCI 7. Asks for information\nC2 8. Asks for opinion\nC3 9. Asks for suggestion\nNegative reactions\nRequests orientation, repetition, confirmation, clarification\nRequests evaluation, analysis, expression of .feeling or wishes\nRequests directions, proposals\nDI 10. Shows disagreement\nD2 I I . Shows tension\nD3 12. Shows antagonism\nPassively rejects, resorts to formality, withholds help\nAsks for help, withdraws\nDeflates others, defends or asserts self\nTable I: Bales's interaction categories\n35\nDen\/se Wh\/'tetock et at Analysing tutor feedback to students: constructing an electronic monitoring system\nCategories Specific examples\nPositive reactions\nAI I. Shows solidarity\nA2 2. Shows tension release\nA3 3. Shows agreement\nAttempted answers\nJokes: Rewards e.g. 'excellent', 'good point', 'well done' etc.\n'Yes now this is good\n'Agrees: 'yes', 'I agree', 'I can accept...' 'That's right1 etc.\nBI 4. Gives suggestion\nB2 5. Gives opinion\nB3 6. Gives information\nQuestions\nDirects: 'notice that...', 'please explain further', 'could expand this...',\n'discuss further', 'enlarge upon' etc.\nEvaluates: 'I'm wondering...', 'my reaction is ...', 'I assume that you\nare ...', 'I think', 'in my opinion', etc.\nOrients: e.g. 'As you know quite a few authors are ...', 'Your examples\nshow the potential for...', 'in fact much new technology...', etc.\nCI 7. Asks for information\nC2 8. Asks for opinion\nC3 9. Asks for suggestion\nNegative reactions\nQuestions:'?' 'Is this specific to ...?' 'Why is this different.?' 'What\nwould this be?' etc.\ne.g. 'so you're saying...?', 'is the content and pedagogy here likely to\nbe...?'\nWould the research design look like this or...?\nDI 10. Shows disagreement\nD2 I I . Shows tension\nD3 12. Shows antagonism\nDisagreement 'No', 'I don't think I agree', 'I'm afraid that isn't the\npoint'... etc'\nand again this does not follow'\nNo examples found\nTable 2: Examples of incidences ofBales's interaction process\nEach tutor comment was coded with respect to Bales's categories (see Table 1). The code was\nmarked up on the assignment next to the comment and later entered onto an electronic\nspreadsheet. Two researchers undertook this task and the inter-rater reliability factor was\n0.89. All the comments on the TMA were coded with the Bales system, although one\ncategory, D3 ('shows hostility') was redundant, which is encouraging as it indicated that there\nwas no evidence of tutors displaying antagonism towards the students. It was also helpful to\ndiscover that no extra categories needed to be added to account for all the tutor comments.\nThe categories which contained minimal comments with a mean value of less than two\nwere A2 ('Shows tension release'), C3 ('Asks for suggestion') and D2 ('Shows tension'). The\nlow values for A2 and D2 categories complement each other revealing a very low incidence\nof tension. C3 is also low because this is not a record of a face-to-face interaction and so\nthe tutor rarely asks for a further suggestion. Bales's interactional categories appear to\nprovide an appropriate analytical framework for these tutor comments despite being\ndesigned to monitor face-to-face interaction. This issue will be discussed further following\n36\nALT-} Volume 11 Number 3\nthe more detailed analysis of the comments illustrated in Section 4 below. Instances of\nBales's interactions can be seen in Table 2.\nResults\nThe main objective of this phase of the analysis was to identify a set of trends in the tutor\ninteractions that matched the grade awarded. In order to account for the variation in\nstudent background and the even bigger difference in tutoring style (one tutor wrote a\nthird more comments than the other two), the mean number of comments per category\nwas calculated for each level of pass awarded. These pass levels were given to students in\ntheir assignment guide and were as follows:\nPass 1 = 85-100\nPass 2 = 70-84\nPass 3 = 55-69\nPass 4 = 40-54\nBare Fail = 30-39\nFail (with the option of resitting) = 15-29\nFail outright = 0-14\nThe practicalities of the analysis then meant that the number of incidences of each of\nBales's categories for each standard of pass was counted as can be seen in Figure 3. The\ncategories were then conflated so that A category comments (positive reactions),\nB category comments (direct teaching comments), C category comments (questions), and\nD category comments (negative reactions) were grouped and counted for each standard of\npass (see Figure 2). This was done to see whether there was a notable differential in the\nnumber of incidences of any specific categories within each standard of pass.\nFigure 2: Graph to\nshow conflated Bales'\ncategories against\nproportion of\nincidences\nConflated Bal**' categoric* against p\u00abrc\u00abntiig* of\nIncidences\na Pass 2\n\u2022 PasaS\ns Pass 4\n\u2022 Overal\nA B C\nCcirilatod M e * Inttraettonal\n37\nDen\/se WWtetock et al Analysing tutor feedback to students: constructing an electronic monitoring system\nThe general picture that emerges when the categories were conflated is that over half of the\ntutor comments give suggestions, directions and opinions about the work (category 'B').\nThe rest of the comments are spread between the following two groups - questions\n(categories 'C') and providing socio-emotive support (category A). The former category\nwas used by tutors to illustrate things that were incorrect but in a non-confrontational\nmanner. The comments for categories 'B' and 'C point out difficulties and problems with\nthe assignment and offer constructive help to sort these out. The remaining comments (one\nsixteenth) demonstrate direct tutor disagreement with the student.\nA more generic picture emerges which offers a basic teaching model of the tutors'\ncomments but more importantly the data illustrate how this model is dynamic and shifts to\nmatch the competency of the student. This issue is explored by examining the variations\nthat are found within the pass levels. For example, with the higher passes, group B, that is,\ntutor direction, forms the bulk of the tutor comments. These students receive more praise\nand are questioned less about their presentation. They are not asked to reflect upon so\nmany problems with the text as they are clearly not there. The converse is true for the lower\npasses where category 'B' (such as direct teaching comments) still form the bulk of the\ntutor responses but there is more questioning and less praise.\nThese findings suggest that trends exist between the types of tutor comments per pass level,\nbut can these be translated into a set of heuristics for our monitoring system? In order to\naddress this question the full set of comments was scrutinized with respect to pass level as\nillustrated by Figure 2.\nA pass at Level 1 reveals that the tutor shows broad agreement with the student, using such\nphrases as 'I agree' and 'I can accept', together with 'that's right' type comments. The tutor\noffers more opinions about details in the assignment opening up more of a dialogue with the\nstudent. There is less direction given with a smaller amount of questioning the student for\ninformation. The same general pattern emerges with a Level 2 pass but there are now equal\namounts of 'Bl' and 'B2' comments. This means the tutor is not emphasizing opinion\ngeneration as much as giving direct suggestions for improvement with these students. There is\nalso more use of questioning to draw attention to problems in the assignment. Both for a\nLevel 3 and 4 pass the pattern changes. The tutor is giving far more direction ('Bl') and\nasking more questions that highlight inconsistencies and draw attention to problems in the\ntext. There are more disagreement comments too, although these are small in number. These\ntrends indicate that students who exhibit well integrated arguments obtain a higher grade.\nCritical argument is acknowledged by tutors explicitly with comments such as 'I agree', etc.\nThe teaching model lent itself to the formation of a number of metrics. These were:\n\u2022 more of the comments will be in the 'B' category (that is, directive interactions such as\n'gives suggestion', 'gives opinion', or 'gives information') compared with the other\ncategories (Wilcoxon signed ranks test, B compared with A, z= -8.34; j?<0.001; B\ncompared with C, z= -9.36, \/?<0.001; B compared with D, z= -10.37,^<0.001);\n\u2022 the number of times comments from the 'D' category occur (such as 'shows\ndisagreement', 'shows tension', or 'shows antagonism') is always less than the number\nof incidences in the other categories (Wilcoxon signed ranks test, D compared with A,\nz= -8.66, \/?<0001; D compared with B, z=-10.37, \/xO.QOl; D compared with C,\nz=-9.03,\/?<0.001);\n38\nAa-] Volume 11 Number 3\nCategories\nPositive reactions\nAI\nConflated\nI. Shows solidarity\nA2 2. Shows tension release\nA3 3. Shows agreement\nB Attempted answers\nAn overall significant positive correlation with score (r=0.196, p<0.05)\nSignificant positive correlation with both score (r=Q. 179, p<0.05) and\na similar trend at pass level, overall and for two tutors\nNo correlations or trends identified.\nTrend to a positive correlation with score; two tutors showed\nsignificant correlations with both score and pass level\nConflated\nBI 4. Gives suggestion\nB2 5. Gives opinion\nB3 6. Gives information\nC Questions\nA strong overall significant negative correlation with both score\n(r=-0.236, p<0.005) and pass level (rs=~0.266, p~O.OOI)\nSignificant negative correlation with both score (rs=-0.427, p<0.001)\nand pass level (r =-0.415, p<0.001) identified both overall and for each\ntutor\nOne tutor showed a positive correlation with both score and pass\nlevel, and a second showed a similar trend.\nNo correlations or trends identified.\nConflated\nCI 7. Asks for information\nC2 8. Asks for opinion\nC3 9. Asks for suggestion\nD Negative reactions\nA strong overall significant negative correlation with both score\n(rs=-0.3l4, p<0.00l) and pass level (r=-0.317, p<O.OOI).\nSignificant negative correlation with both score (r=-0.333, p<0.001)\nand pass level (rs=-0.327, p<0.001), overall and for two tutors.\nNo correlations or trends identified.\nNo correlations or trends identified.\nDl\nConflated\n10. Shows disagreement\nD2 I I . Shows tension\nD3 12. Shows antagonism\nA trend towards negative correlation with both score and pass level\nSignificant negative correlation with both score (r=-0.178, p<0.05)\nand pass level (rs=-0.l70, p<0.05) overall, not generally significant for\nindividual tutors.\nNo correlations or trends identified.\nNo examples found\nTable 3: Main correlations for Bales's categories, individual and conflated\n39\nDenise WWtefock et al Analysing tutor feedback to students: constructing an electronic monitoring system\nBates' ettegoilM against pvreMitag* of Incklanc**\nProportion of to\u00ab hcMmcM M\n10 20 30\nB1\nimiiMiiif\"\"1\"\"\"\"\"\"\"\"\"\"\"\"\" mi|iiiiiii|ninriiiiiiiiiiuii jut , i\nI I\n\u2022 Pans\nmPeoB.4\n\u2022 Overall\nKEV\nA \" PosHvswcto-WTtoOorialsupport\nB = DtoctteecNtxi\nC \" Quaelona\nD= Hagaly* aodo-wnoHcoal\nDO\nfigure 3: Graph to show Bales's categories against proportion of incidences\n\u2022 there is an inverse trend in the number of incidences of categories 'A' and 'C\ncorresponding with the standard of pass. For example, a 'Pass 1' will receive more\ncomments within category 'A' than a 'Pass 4' script, and a 'Pass 4' script will receive\nmore comments within category ' C than a 'Pass 1' script. Category 'B' is also\nnegatively linked to the strength of the assessment. In effect, positive reactions are\ngiven to stronger scripts, where attempted answers and questioning are used to address\nweakness.\nA full set of correlations were used to analyse the relationships between grades and\ncomments, summarized in Table 3 (Spearman's rank correlation coefficient, N=145).\nOverall, conflated category 'A' showed a statistically significant positive correlation with\nscore, and conflated categories 'B' and ' C showed significant negative correlations with\nscore.\n40\nALT-] Volume 11 Number 3\nPerhaps most interestingly, while all kinds of comments in category 'A' are positively linked\nwith pass level, in category 'B' the picture is more complex, within its overall negative link\nwith pass level. Bl 'Gives suggestion' is a strongly significant negative correlation, with\nmore suggestions being given to weaker assignments. However, B2 'Gives opinion' shows a\nweaker positive correlation: for one tutor it is strongly significant (^=0.525, \/><0.001), a\nsecond tutor shows a similar trend and the third tutor no effect at all. So, while there seem\nto be important individual differences in teaching style, opinions do seem to be used more\nin response to stronger assignments. This behaviour both needs and deserves further\nexploration.\nGiven the significance of these effects regarding the balance of comments in these\ncategories, it seems probable that there could be a sound basis for generating a rule-based\nsystem to analyse these comments. However, before this can be achieved we need to see if\nthe Bales categories can account for comments in other subject domains such as science,\ntechnology and business studies. These analyses are currently in progress.\nConclusions\nThe Bales category system accounted for all the tutor comments found on the assignments.\nThis suggests that it is a useful model for analysing tutor comments but needs testing in\nother subject domains. It provided a general tutoring model which showed dynamic\nvariation with level of pass rate. To summarize, a pattern emerges that could form the basis\nfor some expectations about how assignments should be marked. For example, for the best\nstudents obtaining the top grades there would be more praise given. Less direct teaching\ncomments would be needed but there still should be some questioning. This would then\nstimulate the student to reflect upon their answers and to improve in subsequent\nassignments. There would be few negative comments found. The balance of comments\nshould change as the mark awarded decreases. The students with the lowest marks need\nmore direct teaching and so the number of 'B', that is, teaching comments, should increase.\nHowever, some praise should be given where it is due so as to encourage and motivate the\nstudent to complete their studies.\nThe advantage of Bales's system is that it distinguishes between task-orientated\ncontributions, and the 'socio-emotive' element used by tutors to maintain student\nmotivation. Our analysis has detected that the tutor not only used questions to stimulate\nfurther reflection but also employs this category to point out constructively where there are\nproblems with an essay.\nThe findings to date have been presented at tutor workshops and tutors have been\nenthusiastic about developing an electronic system based on this model because it makes\nexplicit the training they have already received from the University. They felt feedback\nfrom such a system would help them to achieve the right balance in their responses to\nstudents. One tutor remarked that he now realized he did not give enough explicit praise to\nhis best students as he believed a good mark said it all!\nThe findings presented to date provide a modest start to the design of a model of feedback\nfor tutors of distance-education students. However, to achieve the primary goal of\nproviding automated support for monitoring, and supporting both students and tutors will\nrequire further research in two areas. First, the Bales approach needs to be validated in\n41\nDenise Whitetock et al Analysing tutor feedback to students: constructing an electronic monitoring-system\nother disciplines, work that is currently in progress. Second, we need to build a semantic\nrule-based system to allot tutor comments to the correct categories, and validate it against\na substantial corpus of analysed assignments. However, the work has other potential: for\nexample, it could help to analyse tutor comments in computer-based conferences and\ncould provide a form of quality assurance for these teaching approaches. Properly\nvalidated, the resulting system will have the potential to make a significant difference to\nquality assurance and learning support in virtual learning environments.\nReferences\nAngeli, C., Valanides, N. and Bonk, C. J. (2003), 'Communication in a web-based\nconferencing system: the quality of computer-mediated interactions', British Journal of\nEducational Technology, 34, 1, 31-43.\nBales, R. F. (1950), 'A set of categories for the analysis of small group interaction',\nAmerican Sociological Review, 15, 257-63.\nBurstein, J., Marcu, D., Andreyev, S. and Chodorow, M. (2001), Towards Automatic\nClassification of Discourse Elements in Essays, Annual Meeting of the Association for\nComputational Linguistics,Toulouse, France.\nFlanders, N. (1970), Analyzing Teacher Behaviour, Reading, MA: Addison-Wesley.\nHazemi, R. and Hailes, S. (eds) (2002), The Digital University: Building a Learning\nCommunity, London: Springer.\nHearst, M. (2000), 'The debate on automated essay grading, IEEE intelligent systems',\nSeptember-October, http:\/\/www.knowledge-technologies.com\/presskit\/KAT_IEEEdebate.pdf\nLandauer, T. K., Foltz, P. W. and Laham, D. (1997), Introduction to Latent Semantic\nAnalysis, Discourse Processes, 25, 259-84. http:\/\/Isa.colorado.edu\/papers\/dpl.LSAintro.pdf\nMoreale, E., Whitelock, D., Raw, Y. and Watt, S. (2002), 'What measures do we need to\nbuild an electronic monitoring tool for post graduate tutor marked assignments?' Sixth\nInternational Computer Assisted Assessment Conference, 9-10 July, Loughborough, pp.\n253-67.\nSapsford, R. J. (1999), Survey Research, London: Sage.\n42\n"}