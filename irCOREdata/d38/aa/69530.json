{"doi":"10.1007\/s11222-009-9163-6","coreId":"69530","oai":"oai:eprints.lancs.ac.uk:26279","identifiers":["oai:eprints.lancs.ac.uk:26279","10.1007\/s11222-009-9163-6"],"title":"Efficient Bayesian analysis of multiple changepoint models with dependence across segments.","authors":["Fearnhead, Paul","Liu, Zhen"],"enrichments":{"references":[{"id":1015347,"title":"An application of MCMC methods for the multiple change-points problem.","authors":[],"date":"2001","doi":null,"raw":null,"cites":null},{"id":1012367,"title":"Automatic Bayesian curve fitting.","authors":[],"date":"1998","doi":null,"raw":null,"cites":null},{"id":1015043,"title":"Bayesian analysis of isochores.","authors":[],"date":"2008","doi":null,"raw":null,"cites":null},{"id":1016683,"title":"Bayesian curve fitting using MCMC with applications to signal segmentation.","authors":[],"date":"2002","doi":null,"raw":null,"cites":null},{"id":1012633,"title":"Bayesian curve-fitting with free-knot splines.","authors":[],"date":"2001","doi":null,"raw":null,"cites":null},{"id":1017785,"title":"Bayesian forecasting and dynamic models .","authors":[],"date":"1989","doi":"10.2307\/1269581","raw":null,"cites":null},{"id":1015623,"title":"Bayesian inference on biopolymer models.","authors":[],"date":"1999","doi":null,"raw":null,"cites":null},{"id":1017244,"title":"Bayesian retrospective multiple-changepoint identification.","authors":[],"date":"1994","doi":null,"raw":null,"cites":null},{"id":1011798,"title":"Calculating posterior distributions and modal estimates in Markov mixture models.","authors":[],"date":"1996","doi":null,"raw":null,"cites":null},{"id":1014130,"title":"Computational methods for complex stochastic systems: A review of some alternatives to MCMC.","authors":[],"date":"2008","doi":null,"raw":null,"cites":null},{"id":1017502,"title":"Detection and estimation for abruptly changing systems.","authors":[],"date":"1982","doi":"10.1016\/0005-1098(82)90012-7","raw":null,"cites":null},{"id":1012084,"title":"Estimation and comparison of multiple change-point models.","authors":[],"date":"1998","doi":null,"raw":null,"cites":null},{"id":1018077,"title":"Estimation of a noisy discrete-time step function: Bayes and empirical Bayes approaches.","authors":[],"date":"1984","doi":"10.1214\/aos\/1176346802","raw":null,"cites":null},{"id":1013788,"title":"Exact and efficient inference for multiple changepoint problems.","authors":[],"date":"2006","doi":null,"raw":null,"cites":null},{"id":1013487,"title":"Exact Bayesian curve fitting and signal segmentation.","authors":[],"date":"2005","doi":null,"raw":null,"cites":null},{"id":1013205,"title":"Ideal spatial adaptation by wavelet shrinkage.","authors":[],"date":"1994","doi":null,"raw":null,"cites":null},{"id":1012952,"title":"Joint segmentation of wind speed and direction using a hierarchical model.","authors":[],"date":"2007","doi":null,"raw":null,"cites":null},{"id":1016436,"title":"Numerical Bayesion Methods Applied to Signal Processing .","authors":[],"date":"1996","doi":null,"raw":null,"cites":null},{"id":1014442,"title":"Online inference for hidden Markov models.","authors":[],"date":"2003","doi":null,"raw":null,"cites":null},{"id":1014753,"title":"Online inference for multiple changepoint problems.","authors":[],"date":"2007","doi":null,"raw":null,"cites":null},{"id":1011203,"title":"Posterior probability intervals for wavelet thresholding.","authors":[],"date":"2002","doi":null,"raw":null,"cites":null},{"id":1011412,"title":"Product partition models for change point problems.","authors":[],"date":"1992","doi":null,"raw":null,"cites":null},{"id":1010939,"title":"Real nonparametric regression using complex wavelets.","authors":[],"date":"2004","doi":null,"raw":null,"cites":null},{"id":1016931,"title":"Recursion-based multiple changepoint detection in multiple linear regression and application to river streamflows.","authors":[],"date":"2007","doi":null,"raw":null,"cites":null},{"id":1015900,"title":"Rejection control and sequential importance sampling.","authors":[],"date":"1998","doi":null,"raw":null,"cites":null},{"id":1016188,"title":"The fine-scale structure of recombination rate variation in the human genome.","authors":[],"date":"2004","doi":null,"raw":null,"cites":null},{"id":1011715,"title":"The interacting multiple model algorithm for systems with Markovian switching coefficients.","authors":[],"date":"1988","doi":null,"raw":null,"cites":null},{"id":1010919,"title":"Wavelet thresholding via a Bayesian approach.","authors":[],"date":"1998","doi":null,"raw":null,"cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2011-04","abstract":"We consider Bayesian analysis of a class of multiple changepoint models. While there are a variety of efficient ways to analyse these models if the parameters associated with each segment are independent, there are few general approaches for models where the parameters are dependent. Under the assumption that the dependence is Markov, we propose an efficient online algorithm for sampling from an approximation to the posterior distribution of the number and position of the changepoints. In a simulation study, we show that the approximation introduced is negligible. We illustrate the power of our approach through fitting piecewise polynomial models to data, under a model which allows for either continuity or discontinuity of the underlying curve at each changepoint. This method is competitive with, or out-performs, other methods for inferring curves from noisy data; and uniquely it allows for inference of the locations of discontinuities in the underlying curve","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/69530.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/26279\/1\/curve_paper4.pdf","pdfHashValue":"4dbceaabe2982d88c004b47ec629c9ead4cf8953","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:26279<\/identifier><datestamp>\n      2018-01-24T02:47:53Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Efficient Bayesian analysis of multiple changepoint models with dependence across segments.<\/dc:title><dc:creator>\n        Fearnhead, Paul<\/dc:creator><dc:creator>\n        Liu, Zhen<\/dc:creator><dc:subject>\n        QA Mathematics<\/dc:subject><dc:description>\n        We consider Bayesian analysis of a class of multiple changepoint models. While there are a variety of efficient ways to analyse these models if the parameters associated with each segment are independent, there are few general approaches for models where the parameters are dependent. Under the assumption that the dependence is Markov, we propose an efficient online algorithm for sampling from an approximation to the posterior distribution of the number and position of the changepoints. In a simulation study, we show that the approximation introduced is negligible. We illustrate the power of our approach through fitting piecewise polynomial models to data, under a model which allows for either continuity or discontinuity of the underlying curve at each changepoint. This method is competitive with, or out-performs, other methods for inferring curves from noisy data; and uniquely it allows for inference of the locations of discontinuities in the underlying curve.<\/dc:description><dc:date>\n        2011-04<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/26279\/1\/curve_paper4.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1007\/s11222-009-9163-6<\/dc:relation><dc:identifier>\n        Fearnhead, Paul and Liu, Zhen (2011) Efficient Bayesian analysis of multiple changepoint models with dependence across segments. Statistics and Computing, 21 (2). pp. 217-229. ISSN 0960-3174<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/26279\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1007\/s11222-009-9163-6","http:\/\/eprints.lancs.ac.uk\/26279\/"],"year":2011,"topics":["QA Mathematics"],"subject":["Journal Article","PeerReviewed"],"fullText":"Efficient Bayesian Analysis of Multiple Changepoint Models with Dependence\nacross Segments\nPaul Fearnhead and Zhen Liu\nDepartment of Mathematics and Statistics, Lancaster University\nSummary: We consider Bayesian analysis of a class of multiple changepoint models.\nWhile there are a variety of efficient ways to analyse these models if the parameters associ-\nated with each segment are independent, there are few general approaches for models where\nthe parameters are dependent. Under the assumption that the dependence is Markov, we\npropose an efficient online algorithm for sampling from an approximation to the posterior\ndistribution of the number and position of the changepoints. In a simulation study, we show\nthat the approximation introduced is negligible. We illustrate the power of our approach\nthrough fitting piecewise polynomial models to data, under a model which allows for either\ncontinuity or discontinuity of the underlying curve at each changepoint. This method is\ncompetitive with, or out-performs, other methods for inferring curves from noisy data; and\nuniquely it allows for inference of the locations of discontinuities in the underlying curve.\n1 Introduction\nChangepoint models are commonly used for time-series, to allow for abrupt changes in the\nunderlying model or structure for the data. Some example applications areas include ge-\nnetics (Liu and Lawrence, 1999; McVean et al., 2004), environmental time-series (Dobigeon\nand Toumeret, 2007; Seidou and Ouarda, 2007), and signal processing (Punskaya et al.,\n2002), amongst many others.\nWe consider Bayesian inference for changepoint models. Existing methods for such in-\nference are either based on MCMC approaches (e.g. Stephens, 1994; Chib, 1996, 1998;\nLavielle and Lebarbier, 2001), or methods for direct simulation from the posterior (see e.g.\nYao, 1984; Barry and Hartigan, 1992; Liu and Lawrence, 1999; Fearnhead, 2008). The\nmethods for direct simulation have the advantage over MCMC of producing iid draws from\nthe posterior, and they can also be implemented efficiently so that their computational\ncost is linear in the number of observations (Fearnhead and Liu, 2007). However they are\nlimited in terms of the class of models that can be considered. If we call the period of time\nbetween two successive changepoints a segment, then direct simulation methods require\nthe parameters associated with each segment to be independent of each other, and that\nthe marginal likelihood for the data within each segment can be calculated analytically (or\nnumerically, see Fearnhead, 2006).\n1\nOne implementation of the direct simulation methods is based on solving filtering recursions\n(Fearnhead and Liu, 2007). We process the observations one at a time, and when processing\nthe observation at a time t say, we calculate the posterior distribution of the time of the\nmost recent changepoint prior to t. Here, we extend this direct simulation methods to\nmodels where there is dependence across segments. We assume that the dependence is\nMarkov, so that parameters in the current segment depend only on the parameters in\nthe previous segment. The assumption of dependence across segments greatly increases\nthe complexity of calculating the posterior distribution, and to develop a computationally\nefficient algorithm we introduce a simple approximation. At time t we approximate the\ndistribution of the parameters associated with a new segment, conditional on a changepoint\nat t. While this conditional distribution is a mixture distribution, with the number of terms\nin the mixture increasing exponentially with t, we approximate the mixture by a single\ndistribution. This approximation leads to an efficient algorithm, but one that produces iid\nsamples from an approximation to the posterior distribution of interest.\nWe demonstrate our new method on the problem of fitting piece-wise polynomial models.\nHere dependence across segments arises due to assumptions of continuity of the underlying\ncurve. Existing methods for this problem include the MCMC methods of Denison et al.\n(1998) and DiMatteo et al. (2001), who also sample from an approximation to the posterior\nof interest, from approximating the marginal likelihood associated with each segment. Our\nmodel extends existing models that are considered, by allowing for the possibility of either\ncontinuity or discontinuity of the curve at each changepoint. Our approach also allows for\nonline analysis of time-series.\nThe outline of the paper is as follows. Firstly we introduce the class of changepoint models\nwe consider. Then in Section 3 we develop out algorithm for Bayesian inference for these\nmodels. Section 4 then analyses the resulting algorithm for the specific application of fitting\npiece-wise polynomial models. We first show that the approximation introduces negligible\nerror when analysing simulated data from the true model. We also compare the resulting\nmethod with both wavelet-based methods and the MCMC method of Denison et al. (1998),\nand look at the power of the method for detecting discontinuities in the underlying signal.\nSection 5 applies our method to analysing well-log data. Here the focus of inference is\nin detecting changepoints where the underlying signal is discontinuous. Finally the paper\nends with a discussion.\n2 Changepoint model\nWe consider the following hierarchical model for observations y1:n = (y1, . . . , yn). Firstly\nwe introduce a model for the number, l, and position, 0 < \u03c41 < \u00b7 \u00b7 \u00b7 < \u03c4l < n, of the\n2\nchangepoints. This is based on a distribution for the distance between two successive\nchangepoints\np(\u03c4k \u2212 \u03c4k\u22121 = d) = g(d), (1)\nfor some discrete distribution g(\u00b7) on the positive integers. We define \u03c40 = 0 and \u03c4l+1 = n,\nand we let G(s) =\n\u2211s\nd=1 g(d) be the corresponding cumulative distribution function. We\nassume independence of the distance between different pairs of successive changepoints, so\nthat the joint probability of l specific changepoints is\nPr(\u03c41, . . . , \u03c4l) =\n(\nl\u220f\nk=1\ng(\u03c4k \u2212 \u03c4k\u22121)\n)\n(1\u2212G(n\u2212 \u03c4l)).\nThe changepoints split the data into l + 1 segments, with the kth segment containing\nobservations y\u03c4k+1:\u03c4k+1 , for k = 0, . . . , l. For segment k we associate a model Mk and a\nvector of parameters \u03b8k. The model is drawn from a finite set of possible models, M and\nwe assume that there is independence of the choice of model across different segments.\n(Extension to the case where the model of a segment depends on the model of the previous\nsegment is possible, see Fearnhead and Vasileiou, 2008).\nFor k \u2265 1 we allow the distribution of \u03b8k to depend on the position of segment k\u2212 1, \u03c4k\u22121\nand \u03c4k, and its parameter \u03b8k\u22121. Thus we have that the conditional probability of the model\nand parameters for the segment can be factorised as\nPr(Mk = m)pm(\u03b8k|\u03b8k\u22121\u03c4k, \u03c4k\u22121).\nFor the first segment we assume a prior for \u03b80. Note that this framework includes mod-\nels where there are common parameters across segments. In this case some components\nof \u03b8k are equal to the equivalent components of \u03b8k\u22121 and the conditional probability\npm(\u03b8k|\u03b8k\u22121\u03c4k, \u03c4k\u22121) in only non-zero for parameter combinations that obey this constraint.\nGiven a segment defined by changepoints at positions s and t, and with model m and\nparameter \u03b8 we have a likelihood model\npm(ys+1:t|\u03b8). (2)\nWe assume that conditional on the changepoints, segment models and parameters, the\nobservations within each segment are independent of each other.\nFinally we assume that there exists a family of conjugate priors for \u03b8, pm(\u03b8|\u03b6). Thus for\nall m, \u03b6 and yt and s, t, we can calculate\nPs(t,m, \u03b6) =\n\u222b\npm(yt|\u03b8, s)pm(\u03b8|\u03b6)d\u03b8, (3)\n3\nwhere\npm(yt|\u03b8, s) =\npm(ys+1:t|\u03b8)\npm(ys+1:t\u22121|\u03b8)\nis the probability density of yt given a segment that started with observation ys+1. Fur-\nthermore, there exists a \u03b6 \u2032 such that\npm(\u03b8|\u03b6\n\u2032) \u221d pm(yt|\u03b8, s)pm(\u03b8|\u03b6), (4)\nwhere the constant of proportionality is defined so that the right-hand side integrates to 1\n(with respect to \u03b8). We denote the value of \u03b6 \u2032 defined by (4) by an update function us:\n\u03b6 \u2032 = us(t,m, \u03b6). (5)\nWe now give an example of such a changepoint model, which will be used throughout the\npaper to demonstrate and make concrete the ideas we present.\nExample: Piecewise Polynomial Regression\nWe consider filtering a piecewise polynomial regression model to bi-variate data (xi, yi) for\ni = 1, . . . , n, with the data ordered so that x1 < x2 < \u00b7 \u00b7 \u00b7 < xn. For concreteness we will\nfocus on piecewise quadratic models, but the extension to polynomials of different order is\nstraightforward.\nIf the observations ys+1:t are in the kth segment, we specify the model of (2) by:\nys+1:t = Hk\u03b2k + \u03b5k, (6)\nwhere the design matrix Hk is of form\nHk =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ed\n1 0 0\n1 xs+2 \u2212 xs+1 (xs+2 \u2212 xs+1)\n2\n...\n...\n...\n1 xt \u2212 xs+1 (xt \u2212 xs+1)\n2\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f8 ,\n\u03b5k is a vector of noises that are independently drawn from a N(0, \u03c3\n2) distribution, and\n\u03b2k = (\u03b2k,0, \u03b2k,1, \u03b2k,2) is a vector-valued regression parameter.\nFor simplicity, we model the distance between successive changepoints as geometric with\nmean 1\/p, so g(d) = p(1\u2212 p)d\u22121. For each segment except the first we allow for one of two\nmodels: M = 1 refers to the underlying curve being discontinuous at the changepoint that\nstarts the segment, and M = 2 refers to the curve being continuous at this changepoint.\nOur prior is that the model of each segment is equally likely to be either possibility. Note\n4\nthat if M = 2 then \u03b2k,0 will be determined by the length and parameters of the previous\nsegment.\nWe assume that \u03c32 is common to all the segments. However, to be consistent with the\nabove framework, we introduce \u03c32k to denote its value in the kth segment. Thus we have\nthat \u03b8k = (\u03c3k, \u03b2k) and \u03b8k depends on \u03b8k\u22121 as \u03c3k = \u03c3k\u22121, and if M = 2 through the\ndependence of \u03b2k,0 on \u03b2k\u22121.\nWe use the following standard conjugate priors for the variance \u03c32k and the regression\nparameter \u03b2k for both M = 1, 2:\n\u03c32k \u223c IG(\u03bd\/2, \u03b3\/2),\n\u03b2k|\u03c3\n2\nk \u223c N(\u00b5, \u03c3\n2\nkD), (7)\nwhere IG denotes the inverse Gamma distribution and N denotes the Gaussian distribution.\nWith the notation above, we have \u03b6 = (\u03bd, \u03b3, \u00b5,D). For the first segment, for whichM0 = 1,\nwe have prior parameter \u03b60,1 = (\u03bd0, \u03b30,0,D0), with D0 = diag(\u03b40, \u03b41, \u03b42). For a future\nsegment k with Mk = 1, we have the distribution for \u03b2k given by (7) with \u00b5 = (0, 0, 0)\nand D = D0. For a segment k with Mk = 2, and the previous segment starting with\nobservation xr+1 and ends with observation xs, the distribution for \u03b2k is given by (7) with\n\u00b5 = (\u03b2k\u22121,0 +\u2206\u03b2k\u22121,1 +\u2206\n2\u03b2k\u22121,2, 0, 0), where \u2206 = (xs+1 \u2212 xr+1), and D = diag(0, \u03b41, \u03b42).\nThis prior distribution ensures continuity of the underlying curve.\nWe can calculate Ps(t,m, \u03b6) and us(t,m, \u03b6) (see Equations 3 and 5) using standard updates\nfor dynamic linear models (West and Harrison, 1989); details are given in the Appendix.\nGiven the changepoint positions and segment models, we have a linear model for our data,\nand due to the choice of priors we can simulate directly from the posterior distribution\nof the parameters. The difficulty with Bayesian inference for this model is due to the\nintractability of the posterior distribution for changepoint positions and segment models.\n3 Approximate Inference\nWe now describe our method for drawing, approximately, from the posterior distribution\nof the number and position of changements, and model and parameter values for each\nsegment. The approach is based on recursive filtering and smoothing algorithms, which we\nwill describe in turn. Throughout our description we will introduce a (potentially artificial)\ntime, with observation yt arriving at time t. For ease of presentation it will be useful to\nrefer to the model and parameter values associated with the segment to which yt belongs.\nHence, for the rest of the paper we will slightly change notation, with \u03b8t and Mt refering\nto the parameter and model value at this time t. That is we will subscript by time rather\n5\nthan by segment. We also introduce a new variable, Ct, which will denote the position of\nthe most recent changepoint prior to time t.\n3.1 Filtering Algorithm\nTo simplify the following exposition we will first derive the filtering algorithm for the case\nof a geometric segment length, g(d) = p(1\u2212 p)d\u22121.\nFirst note that (Ct,Mt, \u03b8t) are a Markov process; and in particular the marginal dynamics\nfor Ct,Mt are given by\np(Ct+1 = j,Mt+1 = m|Ct = i,Mt = m\n\u2032) =\n\uf8f1\uf8f2\n\uf8f3\n1\u2212 p if j = i and m = m\u2032,\npPr(M = m) if j = t, m \u2208M,\n0 otherwise.\nThe top probability refers to there not being a changepoint between yt and yt+1, and the\nmiddle probability refers to the event that there is.\nNow we wish to recursively approximate\np(Ct,Mt, \u03b8t|y1:t) = p(Ct,Mt|y1:t)p(\u03b8t|y1:t, Ct,Mt).\nThe first term on the right-hand side is a discrete distribution, and we approximate p(Ct =\ns,Mt = m|y1:t) \u2248 w\n(s,m)\nt . Whereas for given Ct = s and Mt = m we will approximate\np(\u03b8t|y1:t, Ct,Mt) by pm(\u03b8t|\u03b6\n(s,m)\nt ), for some \u03b6\n(s,m)\nt . Our approximation is specified by the set\nof probabilities w\n(s,m)\nt and parameters \u03b6\n(s,m)\nt for s = 0, . . . , t \u2212 1 and m \u2208 M the set of\npossible models.\nWe initiate our algorithm using the model prior, with w\n(0,m)\n0 = Pr(M = m) for m \u2208 M,\nand prior for the parameters \u03b6\n(0,m)\n0 = \u03b60,m. For t = 1, . . . , n we have the following set of\nrecursions. Firstly for s \u2208 {0, . . . , t \u2212 1}, Ct+1 = s means that there is no changepoint at\ntime t. Thus we have Mt+1 = Mt and \u03b8t+1 = \u03b8t, and\np(Ct+1 = s,Mt+1 = m, \u03b8t+1 = \u03b8|y1:t+1) =\nKp(Ct = s,Mt = m, \u03b8t = \u03b8|y1:t) Pr(Ct+1 = s|Ct = s)pm(yt+1|\u03b8),\nfor some normalising constant K. Now we substite our approximation, p(Ct = s,Mt =\nm, \u03b8t+1|y1:t) \u2248 w\n(s,m)\nt p(\u03b8|\u03b6\n(s,m)\nt ). Integrating with respect to \u03b8 gives Pr(Ct = s,Mt =\nm|y1:t+1), and thus\nws,mt+1 = Kw\ns,m\nt (1\u2212 p)Ps\n(\nt+ 1,m, \u03b6\n(s,m)\nt\n)\n.\n6\nWhile, using the updates for the conjugate distribution for \u03b8 we get\np(\u03b8t+1|y1:t+1, Ct+1 = s,Mt+1 = m) \u221d pm(\u03b8t+1|\u03b6\n(s,m)\nt )pm(yt+1|\u03b8t+1) = pm(\u03b8t+1|\u03b6\n(s,m)\nt+1 ),\nfor \u03b6\n(s,m)\nt+1 = us(t+ 1,m, \u03b6\n(s,m)\nt ).\nNow consider Ct+1 = t. This corresponds to a changepoint at time t, and Ct can take any\nvalue in {0, . . . , t\u2212 1}. We derive an approximate recursion by considering\np(Ct+1 = t,Mt+1 = m, \u03b8t+1|y1:t) =\nt\u22121\u2211\ns=0\n\u2211\nm\u2032\u2208M\np(Ct = s,Mt = m\n\u2032, \u03b8t|y1:t) Pr(Ct+1 = t|Ct = s) Pr(M = m)p(\u03b8t+1|\u03b8t, t, s),\nwhere p(\u03b8t+1|\u03b8t, t, s) denotes the conditional distribution of \u03b8t+1 given \u03b8t and that the\nprevious segment contained observations ys+1:t. Now, substituing our approximations to\np(Ct = s,Mt = m\n\u2032, \u03b8t|y1:t) we have\np(\u03b8t+1|y1:t, Ct+1 = t,Mt+1 = m) \u221d\nt\u22121\u2211\ns=0\n\u2211\nm\u2032\u2208M\npw\n(s,m\u2032)\nt pm(\u03b8t|\u03b6\n(s,m\u2032)\nt )p(\u03b8t+1|\u03b8t, t, s). (8)\nOur approach is to approximate this by pm(\u03b8t+1|\u03b6\n(t,m)\nt ) for some suitable choice of \u03b6\n(t,m)\nt .\nThus as\np(Ct+1 = t,Mt+1 = m, \u03b8t+1|y1:t+1) = Kp(Ct+1 = t,Mt+1 = m, \u03b8t+1|y1:t)pm(yt+1|\u03b8t+1)\n= Kp(Ct+1 = t,Mt+1 = m|y1:t)p(\u03b8t+1|y1:t, Ct+1 = t,Mt+1 = m)pm(yt+1|\u03b8t+1),\nwe get the approximate recursion\nw\n(t,m)\nt+1 = K Pr(M = m)Pt(t+ 1,m, \u03b6\n(t,m)\nt )\nt\u22121\u2211\ns=0\n\u2211\nm\u2032\u2208M\nw\n(s,m\u2032)\nt p,\nand \u03b6\n(t,m)\nt+1 = ut(t+ 1,m, \u03b6\n(t,m)\nt ).\nNote that the only approximation in our filtering recursions is in the approximation of\n(8). There are various ways of choosing \u03b6\n(t,m)\nt for this approximation, and in practice we\nuse a simple method of moments approach (see below). Note that this approximation is\nrequired to avoid the exponentially increasing computational cost of the exact filtering\nrecursions. Similar approximations have been used in the Generalised Pseudo-Bayes al-\ngorithm (Tugnait, 1982), or the Interacting Multiple Model filter (Blom and Bar-Shalom,\n1988).\n7\nAlgorithm 1 Filtering Algorithm\nInitiate Set w\n(0,m)\n1 = Pr(M = m)Ps(1,m, \u03b6\n(0,m)\n0 ) and \u03b6\n(0,m)\n1 = us(1,m, \u03b6\n(0,m)\n0 ) for m \u2208 M.\nNormalise weights, w\n(0,m)\n1 and let t = 1.\nWhile t < n (i) For s = 0, . . . , t\u2212 1 and m \u2208M, set\nw\n(s,m)\nt+1 =\n1\u2212G(t+ 1\u2212 s)\n1\u2212G(t\u2212 s)\nw\n(s,m)\nt Ps\n(\nt+ 1,m, \u03b6\n(s,m)\nt\n)\n,\nand \u03b6\n(s,m)\nt+1 = us(t+ 1,m, \u03b6\n(s,m)\nt .\n(ii) For m \u2208M, calculate \u03b6\n(t,m)\nt to produce the approximation to (8).\n(iii) For m \u2208M, set\nw\n(t,m)\nt = Pr(M = m)Pt\n(\nt+ 1,m, \u03b6\n(t,m)\nt\n) t\u22121\u2211\ns=0\n\u2211\nm\u2032\u2208M\nw\n(t,m\u2032)\nt\n(\nG(t+ 1\u2212 s)\u2212G(t\u2212 s)\n1\u2212G(t\u2212 s)\n)\n,\nand \u03b6\n(t,m)\nt+1 = ut(t+ 1,m, \u03b6\n(t,m)\nt ).\n(iv) Normalise weights, w\n(s,m)\nt+1 .\n8\nThe full filtering algorithm, allowing for a general distribution of segment lengths and prior\ndistribution for models is described in Algorithm 1.\nExample Revisited\nWe now give details of step (ii) of the algorithm for the piecewise polynomial regression\nmodel. Remember \u03b6t = (\u03bdt, \u03b3t, \u00b5t,Dt). For a new segment with Mt+1 = 1 we have \u00b5t = 0\nand Dt = D0. We choose \u03bdt and \u03b3t to match moments of the predictive distribution of\n\u03c3\u22122t+1.\nAssume \u03bd\n(s,m\u2032)\nt and \u03b3\n(s,m\u2032)\nt are the first two components of \u03b6\n(s,m\u2032)\nt . Then we solve\nE(\u03c3\u22122t+1) =\nt\u22121\u2211\ns=0\n2\u2211\nm\u2032=1\nw\n(s,m\u2032)\nt\n\u03bd\n(s,m\u2032)\nt\n\u03b3\n(s,m)\nt\n=\n\u03bd\n(t,1)\nt\n\u03b3\n(t,1)\nt\n.\nand\nE(\u03c3\u22124t+1) =\nt\u22121\u2211\ns=0\n2\u2211\nm\u2032=1\nw\n(s,m\u2032)\nt\n\u03bd\n(s,m\u2032)\nt (2 + \u03bd\n(s,m\u2032)\nt )\n(\u03b3\n(s,m)\nt )\n2\n=\n\u03bd\n(t,1)\nt (2 + \u03bd\n(t,1)\nt )\n(\u03b3\n(t,1)\nt )\n2\n.\nfor \u03bd\n(t,1)\nt and \u03b3\n(t,1)\nt .\nFor a new segment with Mt+1 = 2, we have identical calculations for \u03bd\n(t,2)\nt and \u03b3\n(t,2)\nt .\nHowever, in this case we have \u00b5t+1 = (\u03b7, 0, 0) and Dt+1 = Diag(\u03c4, \u03b41, \u03b42) for some \u03b7 and \u03c4\nto be calculated. Again we choose values based on matching moments, this time of \u03b2t+1,0.\nLet \u2206s = (xt+1 \u2212 xs+1), and as = (1,\u2206s,\u2206\n2\ns)\nT , then\nE(\u03b2t+1,0) =\nt\u22121\u2211\ns=0\n2\u2211\nm\u2032=1\nw\n(s,m\u2032)\nt \u00b5\ns,m\u2032\nt as = \u03b7,\nand\nE(\u03b22t+1,0) =\nt\u22121\u2211\ns=0\n2\u2211\nm\u2032=1\nw\n(s,m\u2032)\nt\n[\naTsD\n(s,m\u2032\nt as + (\u00b5\ns,m\u2032\nt as)\n2\n]\n= \u03b72 + \u03c4.\n3.2 Smoothing\nOnce we have calculated the filtering distributions for all t, we can simulate, backwards in\ntime, the number and position of changepoints, the segment models and parameters, given\nthe full data y1:n.\n9\nFirstly, we can simulate (Cn,Mn, \u03b8n) from (our approximation to) p(Cn,Mn, \u03b8n|y1:n). These\nwill give us the start of the final segment, together with its model and parameter values.\nAssume we simulate Cn = t, then we will next simulate (Ct,Mt, \u03b8t) from\np(Ct,Mt, \u03b8t|y1:n, Ct+1 = t, Ct+2:n,Mt+1:n, \u03b8t+1:n).\nThis will give us the start of the penultimate segment, its model and parameter values. We\ncan then repeat this backwards in time until we simulate the first segment for our data.\nTo perform the simulation we use the fact that\np(Ct,Mt, \u03b8t|y1:n, Ct+1:n,Mt+1:n, \u03b8t+1:n) = p(Ct,Mt, \u03b8t|y1:t, Ct+1,Mt+1, \u03b8t+1),\nby the conditional independence structure of the model. Thus we have\np(Ct = s,Mt = m, \u03b8t|y1:n, Ct+1 = t, Ct+2:n,Mt+1 = m\n\u2032,Mt+2:n, \u03b8t+1:n)\n= p(Ct = s,Mt = m, \u03b8t|y1:t, Ct+1 = t,Mt+1 = m\n\u2032, \u03b8t+1)\n\u221d p(Ct = s,Mt = m, \u03b8t|y1:t)p(Ct+1 = t,Mt=1 = m\n\u2032, \u03b8t+1|Ct = s,Mt = m, \u03b8t, y1:t)\n\u221d p(Ct = s,Mt = m, \u03b8t|y1:t) Pr(Ct+1 = t|Ct = s)p(\u03b8t+1|Ct+1 = t,Mt+1 = m\n\u2032, Ct = s,Mt = m, \u03b8t),\nwhere in the final step we have used that the model of a new segment is independent of\nthe model of the preceeding segment.\nTo simplify notation, let Ft = {yt+1:n, Ct+1:n,Mt+1:n, \u03b8t+1:n} denote the future of the process\nafter time t. Now substituting p(Ct = s,Mt = m, \u03b8t|y1:t) = w\n(s,m)\nt p(\u03b8t|\u03b6\n(s,m)\nt ) we get\nPr(Ct = s,Mt = m|y1:t,Ft) \u221d w\n(s,m)\nt Pr(Ct+1 = t|Ct = s) (9)\n\u222b\np(\u03b8t|\u03b6\n(s,m)\nt )p(\u03b8t+1|Ct+1 = t,Mt+1 = m\n\u2032, Ct = s,Mt = m, \u03b8t)d\u03b8t, (10)\nand\np(\u03b8t|Ct = s,Mt = m, y1:t,Ft) \u221d p(\u03b8t|\u03b6\n(s,m)\nt )p(\u03b8t+1|Ct+1 = t,Mt+1 = m\n\u2032, Ct = s,Mt = m, \u03b8t).\n(11)\nWe need to be able calculate (or approximate) the integral in (10) and simulate from (11)\nto perform the smoothing. The full smoothing algorithm is given by Algorithm 2. The\nsmoothing algorithm simulates the number and position of the changepoints, and the seg-\nment models and parameters. Often more accurate results can be obtained by throwing\naway the simulated parameter values, and re-simulating these from their conditional distri-\nbution given the changepoints and segment models (assuming this distribution is tractable).\nSuch an approach is possible for our piecewise polynomial regression example, and is what\nwe used in the simulation studies later.\n10\nAlgorithm 2 Smoothing Algorithm\nInitiate 1. Simulate (Cn,Mn) from the discrete distribution that gives probability\nw\n(s,m)\nn to the value (s,m). Assuming (Cn,Mn) = (s,m), then simulate \u03b8n from\np(\u03b8n|\u03b6\n(s,m)\nn ).\n2. Set t = s,m\u2032 = m and \u03b8 = \u03b8n.\nWhile t > 0 1. For s = 0, . . . , t\u2212 1 and m \u2208M calculate\nw\u02dc(s,m) = w\n(s,m)\nt Pr(Ct+1 = t|Ct = s)\n\u00d7\n\u222b\np(\u03b8t|\u03b6\n(s,m)\nt )p(\u03b8t+1 = \u03b8|Ct+1 = t,Mt+1 = m\n\u2032, Ct = s,Mt = m, \u03b8t)d\u03b8t\n2. Simulate (Ct,Mt) from the discrete distribution that gives probability propor-\ntional to w\u02dc(s,m) to the value (s,m).\n3. Assume (Ct,Mt) = (s,m). Simulate \u03b8t from the distribution proportional to\np(\u03b8t|\u03b6\n(s,m)\nt )p(\u03b8t+1 = \u03b8|Ct+1 = t,Mt+1 = m\n\u2032, Ct = s,Mt = m, \u03b8t).\n4. Set t = s, m\u2032 = m and \u03b8 = \u03b8t+1.\n11\nWe now give details of the calculations involved in the smoothing algorithm for our example.\nExample Revisited\nFor our example \u03b8t = (\u03c3t, \u03b2t). Consider a changepoint at t, and Ct = s. Define h =\n(1,\u2206,\u22062) and \u2206 = (xt+1 \u2212 xs+1).\nFirstly consider calculating an integral of the form\u222b\np(\u03b8t|\u03b6)p(\u03b8t+1|Ct+1 = t,Mt+1 = m\n\u2032, Ct = s,Mt = m, \u03b8t)d\u03b8t,\nwhere \u03b6 = (\u03bd, \u03b3, \u00b5,D), for step 1 of Algorithm 2. For m\u2032 = 1 this becomes\nIG(\u03c3t+1; \u03bd\/2, \u03b3\/2)N(\u03b2t+1;0, \u03c3\n2\nt+1D0),\nwhere IG(x; a, b) denotes the probability density function (pdf) of an inverse-gamma dis-\ntribution with parameter a and b, evaluated at x; and N(x; \u03b7,\u03a3) denotes the pdf of a\nmultivariate normal distribution with mean \u00b5 and variance \u03a3, evaluated at x. The first\nterm comes from the fact that \u03c3t+1 = \u03c3t, and second due to the independence of \u03b2t+1 and\n\u03b2t. For m\n\u2032 = 2, the integral becomes\nIG(\u03c3t+1; \u03bd\/2, \u03b3\/2)N(\u03b2t+1; \u03b7, \u03c3\n2\nt+1\u03a3),\nwhere \u03b7 = (h\u00b5T , 0, 0), and \u03a3 = Diag(hTDh, \u03b41, \u03b42). Here we the term has changed as now\n\u03b2t+1,0 = h\u03b2t due to continuity.\nNow consider simulating \u03b8t from a density proportional to\np(\u03b8t|\u03b6)p(\u03b8t+1|Ct+1 = t,Mt+1 = m\n\u2032, Ct = s,Mt = m, \u03b8t),\nin step 3 of Algorithm 2. For m\u2032 = 1 we set \u03c3t = \u03c3t+1, and simulate \u03b2t from a multivariate\nnormal distribution with mean \u00b5 and variance \u03c32t+1D. For m\n\u2032 = 2 we again set \u03c3t = \u03c3t+1,\nbut now simulate \u03b2t from a multivariate normal distribution with mean \u00b5 and variance\n\u03c32t+1D conditional on h\u03b2\nT\nt = \u03b2t+1,0. Standard results (see e.g. Rue and Held, 2005), gives\nthat we simulate \u03b2t from a multivariate normal with mean\n\u00b5\u2212DhT (hDhT )\u22121(h\u00b5T \u2212 \u03b2t+1,0),\nand variance\n\u03c32t+1\n(\nD\u2212DhT (hDhT )\u22121hTD\n)\n.\n12\n3.3 Resampling\nSimulating from the posterior distribution of the number and position of changepoints, and\nthe segment models and parameters, using the filtering and smoothing algorithms has a\ncomplexity which is quadratic in n. This is due to the number of support points of (Ct,Mt)\nincreasing linearly with t.\nAt the expense of further approximation, we can develop an algorithm whose total com-\nputational cost is linear in n via using particle-filter resampling algorithms (e.g. Liu et al.,\n1998; Fearnhead and Clifford, 2003) to approximate the distributions of (Ct,Mt) by dis-\ncrete distributions with fewer support points. (The resampling procedures ensure that the\nnumber of support points in the resulting approximation is bounded by a constant for all t.)\nThis was investigated in Fearnhead and Liu (2007), who propose two optimal resampling\nalgorithms for changepoint models, and show that substantial computational savings can\nbe obtained with negligible approximation error.\n4 Simulation Study\nWe now evaluate out method through a simulation study using the piecewise quadratic\nmodel introduced within our example. We first look at the accuracy of our filtering and\nsmoothing method for simulating from the posterior distribution, and then compare the\naccuracy of our method to other approaches for curve-fitting. Finally we look at the\naccuracy of our method at inferring discontinuities in the underlying curve.\nIn implementing our method we used the filter and smoothing algorithms with the stratified\nrejection control resampling method of Fearnhead and Liu (2007). The threshold param-\neter within the resampling algorithm was set to 10\u22126 (see Fearnhead and Liu, 2007, for\ndetails). We used the filter and smoothing algorithms to simulate the number and position\nof changepoints, the value of the observation variance and the model for each segment.\nConditioned on these, we then simulated the \u03b2 values associated with each segment from\ntheir conditional distribution.\nThe filtering and smoothing algorithms were implemented within C++ and R. The compu-\ntational cost of the algorithms is roughly linear in the number of observations, and to run\nthem on a data set with 4000 data points took of the order of 10 seconds on a desktop PC.\n13\n4.1 Accuracy of the Simulation Method\nTo test the accuracy of the filtering and smoothing algorithms at drawing samples from\nthe true posterior distribution, we ran a simulation study where we simulated data under\nthe exact model that we used for analysis. We then calculated the posterior quantiles of\nthe true value for \u03c3 and the true value of the underlying curve at each time point. The\nrationale is that if we could draw from the true posterior, then these posterior quantiles\nshould be uniformly distributed on [0, 1]. Any inaccuracies in our simulation method\nwill be demonstrated through deviations of the posterior quantiles from such a uniform\ndistribution.\nWe simulated data for the piecewise-quadratic model with D0 = Diag(1, 10\n2, 402), p = 4\/n\nand \u03c32 = 1. We analysed the data under the model with the same value for D0 and p,\nbut with an improper prior for \u03c32 (equivalent to \u03bd = \u03b3 = 0). To detect any affect that\nthe amount of data had on the performance of our method we simulated 100 data sets for\neach of n = 256, 512 and 1024. In each case we used equally spaced xt points in [0, 1].\nPlots of the posterior quantiles are shown in Figure 1. In both cases they are close to that\nexpected if they were drawn from the true posterior distribution. The extra smoothness\nin the plot of posterior quantiles of the underlying curve is due to the larger number of\nquantiles obtained in this case, 100n as we obtain one quantile for each data point. For\nthe posterior quantiles of \u03c3 we are able to construct confidence intervals, as the posterior\nquantiles are independent. We notice that the observed quantiles generally lie within the\nplotted 90% confidence interval. Taken together, these results suggest that negligible error\nis being introduced by the approximations in our method for simulating from the posterior\ndistribution.\n4.2 Comparison for curve-fitting\nWe now look at the accuracy of our piecewise quadratic regression model, together with\nthe new simulation method, for curve-fitting. Firstly, in order to implement our method we\nneed to choose the prior parameter values. As above we will use the default uninformative\nprior for \u03c3. The values of D0 and p we used for inference were chosen in an iterative proce-\ndure (as suggested in Fearnhead, 2005). We did a preliminary analysis of the data (using\ndefault choices for D0 and p), and then estimated D0 and p from the posterior distribu-\ntion of the \u03b2s and the number of changepoints. If necessary this could be repeated, with\nsimulation from the posterior given the latest estimates for D0 and p, and new estimates\nof D0 and p obtained. In practice we did not find it necessary to repeat the procedure.\n14\n0.0 0.2 0.4 0.6 0.8 1.0\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\n(a)\nUniform Quantiles\nPo\nst\ner\nio\nr Q\nua\nnt\nile\ns\n0.0 0.2 0.4 0.6 0.8 1.0\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\n(b)\nUniform Quantiles\nPo\nst\ner\nio\nr Q\nua\nnt\nile\ns\nFigure 1: Posterior quantile plots of (a) the underlying curve and (b) \u03c32 for different values\nof n: 256 (red, dashed line), 512 (green dotted line) and 1024 (blue dot-dashed line). For\n(b) we give 90% confidence intervals obtained through simulation (black dashed line).\n15\nMSE Coverage\nn New BAYES.THR cthresh New wave.band\n256 0.056 0.215 0.15 0.87 0.79\n512 0.027 0.138 0.093 0.87 0.79\n1024 0.014 0.087 0.056 0.89 0.79\nTable 1: Mean square error (MSE) and coverage of putative 90% confidence\/credible in-\ntervals for our new method, and wavelet based methods.\nWe first quantify the accuracy of our method for analysing the same simulated data sets\nthat were used in Section 4.1. For a given data set let zt denote the value of the underlying\ncurve at time t (so observations are yt = zt + \u03c3\u01ebt where \u01ebt is a standard normal random\nvariable). Denote by z\u02c6t an estimate of zt, then we estimate the accuracy of an estimate of\nthe curve z1:n by the average mean square error\nMSE =\n1\nn\nn\u2211\nt=1\n(zt \u2212 z\u02c6t)\n2.\nFor our method we use the posterior mean as our estimate of zt. We also look at the mean\npoint-wise coverage of 90% credible (or confidence) intervals for zt.\nFor comparison we estimate the underlying curve using wavelets. We implement two\nwavelet methods, that of Abramovich et al. (1998) implemented using the BAYES.THR func-\ntion in R, and one using complex wavelest (Barber and Nason, 2004) implemented using\nthe cthresh function in R. We also constructed wavelet-based confidence intervals (Barber\net al., 2002) using the wave.band function in R. (See http:\/\/www.stats.bris.ac.uk\/\u223cwavethresh\/\nfor details of these functions; we used default settings for the R functions in all cases.)\nResults for the simulated data described in Section 4.1 are given in Table 1. We notice that\nthe MSE for estimates of the underlying curve is substantially smaller for our new approach\nthan for either wavelet method. Of the two wavelet methods, the one using complex\nwavelets gives superior performance. The MSE of our new method halves each time n is\ndoubled, whereas the MSE of the wavelet methods decreases by a smaller proportion each\ntime. Finally, the coverage of our 90% credible intervals are close to 90% in each case. The\nfact that the coverage of the intervals is less than their putative size is likely to be down\nto errors in estimating the hyperparameters.\nThe above results are not surprising, in the sense that the data was simulated under the\nmodel assumed by our new method. To test robustness of this method to data being\nsimulated from an alternative model, we repeated our simulation study but with data\nsimulated under a piecewise cubic model. For this model we set D0 = diag(1, 10\n2, 402, d2),\n16\nMSE Coverage\nd New BAYES.THR cthresh New wave.band\n100 0.06 0.34 0.16 0.86 0.80\n200 0.07 0.69 0.17 0.86 0.82\n400 0.11 2.45 0.18 0.84 0.86\nTable 2: Mean square error (MSE) and coverage of putative 90% confidence\/credible in-\ntervals for our new method, and wavelet based methods. Data simulated under a piecewise\ncubic model, with d affecting the size of the cubic co-efficients. All data sets were simulated\nwith n = 100.\nand considered the effect that d had. Note that the expected value of the modulus of the\ncubic co-efficient is d(2\/\u03c0)1\/2. For simplicity we fixed n = 256 for all simulations that we\ncarried out.\nResults are given in Table 2, again based on 100 simulated data sets for each set of pa-\nrameters. As expected, as d increases, which corresponds to an increasingly non-quadratic\ncomponents of the underlying curve, the performance of the new method deteriorates. This\nis both in terms of the coverage properties of the credible intervals, and the mean square\nerror of estimates of the underlying curve. However for all values of d we considered, the\nnew method still substantially out-performs both wavelet methods in terms of estimating\nthe underlying curve.\nAs a final comparison, we applied our new method to various test data sets from the\nliterature, and compare our method with the published results of Denison et al. (1998)\n(henceforth DMS). The test data sets used are shown in Figure 2, and consist of the\nHeavisine, Blocks, Bumps and Doppler signals of Donoho and Johnstone (1994); and the\nsmooth function (a) and (b) from Denison et al. (1998) (denoted DMS A amd DMS B).\nThe method of Denison et al. (1998) uses a reversible jump MCMC to fit a piecewise cubic\nfunction, under continuity and differentiability constraints. The MCMC algorithm samples\nfrom an approximation to the posterior, based on approximating the marginal likelihood\nfor each segment. The MCMC procedure takes up to about an order of magnitude longer\nto analyse the data than our approach.\nWe compare methods based on MSE as before. Results are given in Table 3. Our method\ndoes considerably better at estimating the curves which contain discontinuities, as our\nmodel allows for discontinuities in the underlying curve. While we do similarly or better\non DMS A and DMS B, our method is substantially worse for the Bumps and Doppler data\nsets. This is due to errors in estimating the peaks in the Bumps data set, and the initial part\nof the curve in the Doppler data set. In both cases these are where the underlying curve\n17\n0.0 0.2 0.4 0.6 0.8 1.0\n\u2212\n15\n\u2212\n5\n0\n5\n10\nHeavisine\n0.0 0.2 0.4 0.6 0.8 1.0\n0\n10\n20\n30\n40\n50\nBumps\n0.0 0.2 0.4 0.6 0.8 1.0\n\u2212\n10\n0\n5\n15\n25\nBlocks\n0.0 0.2 0.4 0.6 0.8 1.0\n\u2212\n10\n\u2212\n5\n0\n5\n10\nDoppler\n0.0 0.2 0.4 0.6 0.8 1.0\n\u2212\n2\n\u2212\n1\n0\n1\n2\nDMS (A)\n0.0 0.2 0.4 0.6 0.8 1.0\n\u2212\n1.\n0\n0.\n0\n1.\n0\n2.\n0\nDMS (B)\nFigure 2: Simulated data sets used for comparison with method of Denison et al. (1998)\n18\nn \u03c3 SNR DMS NEW\nHeavisine 2048 1.0 7 0.033 0.022\nBlocks 2048 1.0 7 0.170 0.016\nBumps 2048 1.0 7 0.167 0.318\nDoppler 2048 1.0 7 0.135 0.198\nDMS A 200 0.4 3 0.010 0.010\nDMS B 200 0.3 3 0.009 0.006\nTable 3: MSE results for 7 test data sets (see Figure 2). For each data set we give the\nnumber of data points, n, the observation error, \u03c3, and the signal-to-noise ratio. MSE\nresults for DMS are taken from Denison et al. (1998).\nchanges most rapidly. One explanation for this is that using only quadratic polynomials,\nrather than cubic, makes it harder for our model to fit these parts of the curve.\n4.3 Power at detecting discontinuities\nFinally we look at the power of our method for detecting discontinuities in the underlying\ncurve. Note that it is only our method that can potentially distinguish between change-\npoints at which the underlying curve may be either continuous or discontinuous. We focus\non this feature of our method due to the application of the method we consider in Section\n5.\nWe used as a basis the continuous curve in DMS B (see Figure 2). We then introduced\na discontinuity into the curve. If we denote the underlying DMS B curve by f(x) for\nx \u2208 [0, 1], then we introduce a changepoint of size c at point xc to produce the curve:\nf(x; c, xc) =\n{\nf(x)\u2212 c\u03c3 for x < xc,\nf(x) for x \u2265 xc,\nwhere \u03c32 is the variance of the observations. We then simulated data centered on this\ncurve, and look at the posterior probability of a discontinuous changepoint at between\n[xc \u2212 0.01, xc + 0.01]. We repeated this for different values of c, xc and sample size n.\nResults are given in Figure 3. As expected the posterior probability of a changepoint\nincreases with both n and c, and to a lesser extent by the position of the changepoint. The\nlowest posterior probability of a changepoint occurs when xc = 0.45, which is the point at\nwhich the gradient of the signal is greatest, and this makes jumps in the signal harder to\ninfer. In general an average posterior probability of a changepoint of greater than 0.5 can\noccurs with c > 3 when n is 200 or more; and when c > 2 and n = 800.\n19\n0 1 2 3 4 5\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\n(a)\nc\nPr\nob\nab\nilit\ny\n0 1 2 3 4 5\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\n(b)\nc\nPr\nob\nab\nilit\ny\n0 1 2 3 4 5\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\n(c)\nc\nPr\nob\nab\nilit\ny\n0 1 2 3 4 5\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\n(d)\nc\nPr\nob\nab\nilit\ny\nFigure 3: The posterior probability of a changepoint within [xc \u2212 0.01, xc + 0.01] for DMS\nA, for different changepoint positions xc, size of changepoint c and number of observations\nn. Figure (a) is xc = 0.3, (b) is xc = 0.45, (c) is xc = 0.6 and (d) is xc = 0.7. For each plot\nthe lines correspond to different values of n: n = 100 (black full line); n = 200 (red dashed\nline); n = 400 (green dotted line); and n = 800 (blue dot-dashed line).\n20\n5 Well-log Data\nWe now apply our method to analyse the well-log data of O\u00b4 Ruanaidh and Fitzgerald\n(1996). The data is shown in Figure 4, and consists of a time-series of measurements of\nrock as a probe is lowered through a bore-hole in the earth\u2019s surface. We have scaled time\nso that time-series is over the interval [0, 1]. The underlying signal has a number of abrupt\nchanges, due to the changes in rock strata. It is of interest to locate these abrupt changes\nin the signal. See O\u00b4 Ruanaidh and Fitzgerald (1996) and Fearnhead and Clifford (2003)\nfor further discussion of this data set, and the practical importance of detecting changes\nin rock strata. Furthermore Fearnhead and Clifford (2003) discuss the need for online\nmethods for analysing data of this type.\nBoth O\u00b4 Ruanaidh and Fitzgerald (1996) and Fearnhead and Clifford (2003) fit a piece-\nwise constant signal to the data and assume observation error is independent over time.\nHowever, Fearnhead (2006) suggests that such a model is inappropriate as it ignores local\nvariation within segments, and fitting such a model results in the detection of too many\nchangepoints. Thus here we will consider analysing the data under our model. The idea\nis that our model is flexible to allow for variation within rock strata through changepoints\nat which the underlying signal is continuous. Changes in rock strata will correspond to\nchangepoints at which the underlying signal is discontinuous. Our interest is thus in de-\ntecting the position of these discontinuous changepoints.\nAs in O\u00b4 Ruanaidh and Fitzgerald (1996) we first remove outliers from the data, and then\nanalyse the data in batch. We consider two analyses, one allowing for the possibility of\nchangepoints at which the underlying signal is either continuous of discontinuous; and the\nother which only allows changepoints where the underlying signal is discontinuous. The\nlatter mimics the models of O\u00b4 Ruanaidh and Fitzgerald (1996) and Fearnhead and Clifford\n(2003). We call these models, model A and model B respectively.\nResults are given in Figure 4. For each model we plot the posterior probability of a\ndiscontinuity of the signal in an interval [t\u2212 0.001, t + 0.001] for different values of t. For\nsimplicity we infer a discontinuity whenever this probability is greater than 0.5, and plot\nthe inferred changepoints for the two models. Model B appears to overfit discontinuities\nin the data (posterior mean number of discontinuities, 30, is nearly twice that for model\nA), and using our simple procedure for highlighting changepoints, infers an extra three\ndiscontinuities in the data \u2013 which by eye look spurious.\n21\n0.0 0.2 0.4 0.6 0.8 1.0\n11\n00\n00\n13\n00\n00\nTime\nN\nuc\nle\nar\n R\nes\npo\nns\ne\n0.0 0.2 0.4 0.6 0.8 1.0\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\nTime\nPr\nob\nab\nilit\ny\n0.0 0.2 0.4 0.6 0.8 1.0\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\nTime\nPr\nob\nab\nilit\ny\nFigure 4: (Top) Raw well-log data, with inferred discontinuities (red dashed vertical lines:\nboth models; blue dot-dashed line: model B only). (Middle and Bottom) Posterior proba-\nbility of discontinuity at time [t\u2212 0.001, t+ 0.001] for model A and B respectively.\n22\n6 Discussion\nWe have presented a novel and computationally efficient procedure for Bayesian inference\nfor changepoint models, where there is Markov dependence in the segment parameters. The\nmethod is approximate, in that it is based on an approximation to the filtering distribution\nof parameters associated with a new segment. When used with the resampling idea of\nSection 3.3 the resulting algorithm has computational and storage costs that are linear in\nthe number of observations. The simulation results in Section 4.1, showed that, for the\nexamples we considered, the error introduced by our approximations were negligible.\nWe demonstrated the potential of this new procedure through the fitting of piece-wise\nquadratic functions. The model we fit allowed for both the possibility of continuity of\ndiscontinuity at changepoints. Our simulation studies showed that this model is more\naccurate at fitting curves that contain discontinuities than the related method of Denison\net al. (1998), and can also perform better at estimating the underlying curve than wavelet\nprocedures. Further advantages of our approach is that it can allow for online inference,\nand also can allow for inference about discontinuities in the underlying signal.\nAppendix\nHere we give details of Ps(t,m, \u03b6) and us(t,m, \u03b6) for the piecewise polynomial regression.\nNow denote h = (1,\u2206,\u22062) where \u2206 = (xt \u2212 xs+1) (suppressing the dependence on s and\nt). Then given the most recent changepoint is at time s, the mean of the observation at\ntime t is h\u03b2Tt .\nRemember \u03b6 = (\u03bd, \u03b3, \u00b5,D), and define \u03b6 \u2032 = (\u03bd \u2032, \u03b3\u2032, \u00b5\u2032,D\u2032). Define e = yt \u2212 h\u00b5\nT , Q =\nhDhT + 1, and A = DhT\/Q. Then if \u03b6 \u2032 = us(t,m, \u03b6), we get\n\u03bd \u2032 = \u03bd + 1,\n\u03b3\u2032 = \u03b3 + e2\/Q,\n\u00b5\u2032 = \u00b5+Ae,\nD\u2032 = D\u2212ATAQ.\nFurthermore, let Td(x; a,R) denote the density of a student\u2019s t random variable d degrees\nof freedonm, and with mean a and scale parameter R. Then we have\nPs(t,m, \u03b6) = T\u03bd(yt;h\u00b5\nT , \u03b3Q\/\u03bd).\nAcknowledgements This work was funded by EPSRC grant GR\/T19698. We would like\nto thank Idris Eckley for helpful discussions.\n23\nReferences\nAbramovich, F., Sapatinas, T. and Silverman, B. W. (1998). Wavelet thresholding via a\nBayesian approach. Journal of the Royal Statistical Society, Series B 60, 725\u2013749.\nBarber, S. and Nason, G. P. (2004). Real nonparametric regression using complex wavelets.\nJournal of the Royal Statistical Society, Series B 66, 927\u2013939.\nBarber, S., Nason, G. P. and Silverman, B. W. (2002). Posterior probability intervals for\nwavelet thresholding. Journal of the Royal Statistical Society, Series B 64, 189\u2013205.\nBarry, D. and Hartigan, J. A. (1992). Product partition models for change point problems.\nThe Annals of Statistics 20, 260\u2013279.\nBlom, H. A. P. and Bar-Shalom, Y. (1988). The interacting multiple model algorithm for\nsystems with Markovian switching coefficients. IEEE Transactions on Automatic Control\n33, 780\u2013783.\nChib, S. (1996). Calculating posterior distributions and modal estimates in Markov mixture\nmodels. Journal of Econometrics 75, 79\u201398.\nChib, S. (1998). Estimation and comparison of multiple change-point models. Journal of\nEconometrics 86, 221\u2013241.\nDenison, D. G. T., Mallick, B. K. and Smith, A. F. M. (1998). Automatic Bayesian curve\nfitting. Journal of the Royal Statistical Society, series B 60, 333\u2013350.\nDiMatteo, I., Genovese, C. R. and Kass, R. E. (2001). Bayesian curve-fitting with free-knot\nsplines. Biometrika 88, 1055\u20131071.\nDobigeon, N. and Toumeret, J. Y. (2007). Joint segmentation of wind speed and direction\nusing a hierarchical model. Computational Statistics and Data Analysis 51, 5603\u20135621.\nDonoho, D. L. and Johnstone, I. M. (1994). Ideal spatial adaptation by wavelet shrinkage.\nBiometrika 81, 425\u2013455.\nFearnhead, P. (2005). Exact Bayesian curve fitting and signal segmentation. IEEE Trans-\nactions on Signal Processing 53, 2160\u20132166.\nFearnhead, P. (2006). Exact and efficient inference for multiple changepoint problems.\nStatistics and Computing 16, 203\u2013213.\nFearnhead, P. (2008). Computational methods for complex stochastic systems: A review\nof some alternatives to MCMC. Statistics and Computing 18, 151\u2013171.\n24\nFearnhead, P. and Clifford, P. (2003). Online inference for hidden Markov models. Journal\nof the Royal Statistical Society, Series B 65, 887\u2013899.\nFearnhead, P. and Liu, Z. (2007). Online inference for multiple changepoint problems.\nJournal of the Royal Statistical Society Series B 69, 589\u2013605.\nFearnhead, P. and Vasileiou, D. (2008). Bayesian analysis of isochores. To\nappear in Journal of the American Statistical Association Available from\nwww.maths.lancs.ac.uk\/\u223cfearnhea\/publications.\nLavielle, M. and Lebarbier, E. (2001). An application of MCMC methods for the multiple\nchange-points problem. Signal Processing 81, 39\u201353.\nLiu, J. S. and Lawrence, C. E. (1999). Bayesian inference on biopolymer models. Bioinfor-\nmatics 15, 38\u201352.\nLiu, J. S., Chen, R. and Wong, W. H. (1998). Rejection control and sequential importance\nsampling. Journal of the American Statistical Society 93, 1022\u20131031.\nMcVean, G. A. T., Myers, S. R., Hunt, S., Deloukas, P., Bentley, D. R. and Donnelly, P.\n(2004). The fine-scale structure of recombination rate variation in the human genome.\nScience 304, 581\u2013584.\nO\u00b4 Ruanaidh, J. J. K. and Fitzgerald, W. J. (1996). Numerical Bayesion Methods Applied\nto Signal Processing . New York: Springer.\nPunskaya, E., Andrieu, C., Doucet, A. and Fitzgerald, W. J. (2002). Bayesian curve fitting\nusing MCMC with applications to signal segmentation. IEEE Transactions on Signal\nProcessing 50, 747\u2013758.\nRue, H. and Held, L. (2005). Gaussian Markov Random Fields: Theory and Applications .\nCRC Press\/Chapman and Hall.\nSeidou, O. and Ouarda, T. B. M. J. (2007). Recursion-based multiple changepoint detec-\ntion in multiple linear regression and application to river streamflows. Water Resources\nResearch 43, W07404.\nStephens, D. A. (1994). Bayesian retrospective multiple-changepoint identification. Applied\nStatistics 43, 159\u2013178.\nTugnait, J. K. (1982). Detection and estimation for abruptly changing systems. Automatica\n18, 607\u2013615.\nWest, M. and Harrison, J. (1989). Bayesian forecasting and dynamic models . Springer-\nVerlag, New York.\n25\nYao, Y. (1984). Estimation of a noisy discrete-time step function: Bayes and empirical\nBayes approaches. The Annals of Statistics 12, 1434\u20131447.\n26\n"}