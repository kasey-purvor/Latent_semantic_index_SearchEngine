{"doi":"10.1007\/978-3-642-01044-6_64","coreId":"15306","oai":"oai:dro.dur.ac.uk.OAI2:7802","identifiers":["oai:dro.dur.ac.uk.OAI2:7802","10.1007\/978-3-642-01044-6_64"],"title":"Data compression and regression based on local principal curves.","authors":["Einbeck, J.","Evers, L.","Hinchliff, K."],"enrichments":{"references":[{"id":778484,"title":"Another look at principal curves and surfaces.","authors":[],"date":"2001","doi":null,"raw":null,"cites":null},{"id":778481,"title":"Determination of stellar parameters with GAIA.","authors":[],"date":"2002","doi":null,"raw":null,"cites":null},{"id":779307,"title":"Elastic maps and nets for approximating principal manifolds and their application to microarray data visualization.","authors":[],"date":"2008","doi":null,"raw":null,"cites":null},{"id":778483,"title":"Exploring multivariate data structures with local principal curves.","authors":[],"date":"2005","doi":null,"raw":null,"cites":null},{"id":778478,"title":"Ice flow identification in satellite images using mathematical morphology and clustering about principal curves.","authors":[],"date":"1992","doi":null,"raw":null,"cites":null},{"id":778487,"title":"Learning and design of principal curves.","authors":[],"date":null,"doi":null,"raw":null,"cites":null},{"id":778482,"title":"Local principal curves.","authors":[],"date":"2005","doi":null,"raw":null,"cites":null},{"id":778479,"title":"Path estimation from GPS tracks. Geocomputation","authors":[],"date":"2007","doi":null,"raw":null,"cites":null},{"id":779306,"title":"Piecewise linear skeletonization using principal curves.","authors":[],"date":"2002","doi":null,"raw":null,"cites":null},{"id":778480,"title":"Principal curves for nonlinear feature extraction and classification.","authors":[],"date":"1998","doi":null,"raw":null,"cites":null},{"id":778485,"title":"Principal curves.","authors":[],"date":"1989","doi":null,"raw":null,"cites":null},{"id":778486,"title":"The Elements of Statistical Learning.","authors":[],"date":"2001","doi":null,"raw":null,"cites":null}],"documentType":{"type":1}},"contributors":["Fink, A.","Lausen, B.","Seidel, W.","Ultsch, A."],"datePublished":"2010-01-01","abstract":"Frequently the predictor space of a multivariate regression problem of the type y = m(x_1, \u2026, x_p ) + \u03b5 is intrinsically one-dimensional, or at least of far lower dimension than p. Usual modeling attempts such as the additive model y = m_1(x_1) + \u2026 + m_p (x_p ) + \u03b5, which try to reduce the complexity of the regression problem by making additional structural assumptions, are then inefficient as they ignore the inherent structure of the predictor space and involve complicated model and variable selection stages. In a fundamentally different approach, one may consider first approximating the predictor space by a (usually nonlinear) curve passing through it, and then regressing the response only against the one-dimensional projections onto this curve. This entails the reduction from a p- to a one-dimensional regression problem.\\ud\nAs a tool for the compression of the predictor space we apply local principal curves. Taking things on from the results presented in Einbeck et al. (Classification \u2013 The Ubiquitous Challenge. Springer, Heidelberg, 2005, pp. 256\u2013263), we show how local principal curves can be parametrized and how the projections are obtained. The regression step can then be carried out using any nonparametric smoother. We illustrate the technique using data from the physical sciences.\\u","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/15306.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/7802\/1\/7802.pdf","pdfHashValue":"7a3f793ee186301f92907a488dc645fa07f45ec9","publisher":"Springer","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:7802<\/identifier><datestamp>\n      2011-01-31T10:09:42Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Data compression and regression based on local principal curves.<\/dc:title><dc:creator>\n        Einbeck, J.<\/dc:creator><dc:creator>\n        Evers, L.<\/dc:creator><dc:creator>\n        Hinchliff, K.<\/dc:creator><dc:description>\n        Frequently the predictor space of a multivariate regression problem of the type y = m(x_1, \u2026, x_p ) + \u03b5 is intrinsically one-dimensional, or at least of far lower dimension than p. Usual modeling attempts such as the additive model y = m_1(x_1) + \u2026 + m_p (x_p ) + \u03b5, which try to reduce the complexity of the regression problem by making additional structural assumptions, are then inefficient as they ignore the inherent structure of the predictor space and involve complicated model and variable selection stages. In a fundamentally different approach, one may consider first approximating the predictor space by a (usually nonlinear) curve passing through it, and then regressing the response only against the one-dimensional projections onto this curve. This entails the reduction from a p- to a one-dimensional regression problem.\\ud\nAs a tool for the compression of the predictor space we apply local principal curves. Taking things on from the results presented in Einbeck et al. (Classification \u2013 The Ubiquitous Challenge. Springer, Heidelberg, 2005, pp. 256\u2013263), we show how local principal curves can be parametrized and how the projections are obtained. The regression step can then be carried out using any nonparametric smoother. We illustrate the technique using data from the physical sciences.\\ud\n<\/dc:description><dc:subject>\n        Dimension reduction<\/dc:subject><dc:subject>\n         Principal component regression<\/dc:subject><dc:subject>\n         Principal curves<\/dc:subject><dc:subject>\n          Smoothing<\/dc:subject><dc:publisher>\n        Springer<\/dc:publisher><dc:source>\n        Fink, A. & Lausen, B. & Seidel, W. & Ultsch, A. (Eds.). (2010). Advances in data analysis, data handling and business intelligence. Berlin: Springer, pp. 701-712, Studies in classification, data analysis, and knowledge organization<\/dc:source><dc:contributor>\n        Fink, A.<\/dc:contributor><dc:contributor>\n        Lausen, B.<\/dc:contributor><dc:contributor>\n        Seidel, W.<\/dc:contributor><dc:contributor>\n        Ultsch, A.<\/dc:contributor><dc:date>\n        2010-01-01<\/dc:date><dc:type>\n        Book chapter<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:7802<\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/7802\/<\/dc:identifier><dc:identifier>\n        http:\/\/dx.doi.org\/10.1007\/978-3-642-01044-6_64<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/7802\/1\/7802.pdf<\/dc:identifier><dc:rights>\n        The original publication is available at www.springerlink.com\\ud\n<\/dc:rights><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2010,"topics":["Dimension reduction","Principal component regression","Principal curves","Smoothing"],"subject":["Book chapter","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n31 January 2011\nVersion of attached file:\nAccepted Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nEinbeck, J. and Evers, L. and Hinchliff, K. (2010) \u2019Data compression and regression based on local principal\ncurves.\u2019, in Advances in data analysis, data handling and business intelligence. Berlin: Springer, pp. 701-712.\nStudies in classification, data analysis, and knowledge organization.\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1007\/978-3-642-01044-664\nPublisher\u2019s copyright statement:\nThe original publication is available at www.springerlink.com\nAdditional information:\nProceedings of the 32nd Annual Conference of the Gesellschaft fr Klassifikation e.V., Joint Conference with the British\nClassification Society (BCS) and the Dutch\/Flemish Classification Society (VOC), Helmut-Schmidt-University,\nHamburg, July 1618, 2008.\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\nData compression and regression based on\nlocal principal curves\nJochen Einbeck1, Ludger Evers2 and Kirsty Hinchliff1\n1 Department of Mathematical Sciences, Durham University, Durham, UK.\njochen.einbeck@durham.ac.uk\n2 Department of Statistics, University of Glasgow, Glasgow, UK.\nludger@stats.gla.ac.uk\nSummary. Frequently the predictor space of a multivariate regression problem\nof the type y = m(x1, . . . , xp) + \u000f is intrinsically one-dimensional, or at least of\nfar lower dimension than p. Usual modeling attempts such as the additive model\ny = m1(x1)+ . . .+mp(xp)+ \u000f, which try to reduce the complexity of the regression\nproblem by making additional structural assumptions, are then inefficient as they\nignore the inherent structure of the predictor space and involve complicated model\nand variable selection stages. In a fundamentally different approach, one may con-\nsider first approximating the predictor space by a (usually nonlinear) curve passing\nthrough it, and then regressing the response only against the one-dimensional pro-\njections onto this curve. This entails the reduction from a p\u2212 to a one-dimensional\nregression problem.\nAs a tool for the compression of the predictor space we apply local principal\ncurves. Taking things on from the results presented in [6], we show how local princi-\npal curves can be parametrized and how the projections are obtained. The regression\nstep can then be carried out using any nonparametric smoother. We illustrate the\ntechnique using data from the physical sciences.\nKey words: Dimension reduction, smoothing, principal curves, principal\ncomponent regression.\n1 Introduction\nPrincipal curves are \u201csmooth one-dimensional curves passing through the mid-\ndle of a p\u2212dimensional data set, providing a nonlinear summary of the data\u201d\n[8]. Since Hastie & Stuetzle\u2019s pioneering work, principal curves have been\nfurther investigated, applied, and developed by quite a few researchers, and\ntoday exist at least half a dozen of algorithms for estimating them. These\ndiffer essentially in (i) what is understood of the \u201cmiddle\u201d of the data cloud;\n(ii) the algorithmic family (\u201ctop-down\u201d or \u201cbottom-up\u201d); (iii) the criterion\nused for minimizing the error (if used at all).\n2 Jochen Einbeck, Ludger Evers and Kirsty Hinchliff\nAmong the various principal curve concepts proposed are bias-corrected\nversions of the HS algorithm [1, 3], the polygonal line algorithm [10], the\n\u201cprincipal curves of orientated points\u201d (PCOPs, [7]), and the \u201clocal principal\ncurves\u201d (LPCs, [5]). PCOPs and LPCs are bottom-up algorithms, i.e., they\nproceed through the data cloud step by step and do not minimize a global er-\nror criterion. All other existing methods correspond to top-down algorithms,\nmeaning that they start with some initial line which is then iteratively dwelled\nout until it fits satisfactorily through the data cloud and some global error\ncriterion is minimized. Apart from the LPCs, which aim to approximate the\ndensity ridge, all concepts assume the existence of some theoretical \u201ctrue\u201d\nprincipal curve. Implementations of all algorithms mentioned above are pub-\nlicly available and have been applied to a wide range of problems, including\nthe recognition of hand-written characters [11], the reconstruction of river\noutlines or coastlines [5, 6], and path estimation from GPS tracks [2].\nSurprisingly, the existing literature seems to be happy with knowing that\nprincipal curves can be estimated and that the resulting curve can be visual-\nized, but has not proceeded with exploiting its benefits once it is there (with\nthe notable exception of [3], who make use of HS principal curves for further\npairwise compression of principal component scores). The value of their para-\nmetric counterpart, principal components, also brings to bear only when they\nare used for data compression or regression (e.g. [9], p. 66).\nIn Section 2, we consider a simple example taken from traffic engineering,\nillustrating how principal curves may be used for data compression and de-\ncompression. To motivate the necessity and value of nonparametric dimension\nreduction techniques, we proceed in Section 3 to a more complex applica-\ntion involving high-dimensional data from the future Galactic survey mission\nGAIA, and show how principal curves can be used for dimension reduction in\nmultiple regression problems. In both cases, the technique used is that of local\nprincipal curves. We finish with a brief outlook on the extension to principal\nmanifolds in Section 4.\n2 Data compression with local principal curves\n2.1 Local principal curves\nAssume we are given a data set X1, . . . , Xn, with Xi \u2208 R\np, the intrinsic\nstructure of which is to be described. Local principal curves [5, 6] are based\non the idea that, at each point x \u2208 Rp along a principal curve, the localized\nfirst principal component line forms approximately a tangent to the curve.\nThey can be seen as a simple and fast approximation to the mathematically\nand computationally more demanding PCOPs [7]. Beginning at some starting\npoint x = x0, LPCs work successively through the data cloud, alternating\nbetween the following two steps:\nData compression with principal curves 3\n(i) Calculate a localized center of mass \u00b5x =\n\u2211n\ni=1 wiXi, where\nwi = KH(Xi \u2212 x)Xi\/\n\u2211n\ni=1 KH(Xi \u2212 x).\n(ii) Compute the 1st local eigenvector \u03b3x of \u03a3x = (\u03c3xjk)(1\u2264j,k\u2264p), where \u03c3\nx\njk =\u2211n\ni=1 wi(Xij \u2212 \u00b5\nx\nj )(Xik \u2212 \u00b5\nx\nk) and \u00b5\nx\nj denotes the j\u2212th component of \u00b5\nx.\nUsing a predetermined step size t0, step from \u00b5\nx to x := \u00b5x + t0\u03b3\nx.\nThe sequence of the local centers of mass \u00b5x makes up the local principal\ncurve. Here, KH(\u00b7) = |H |\n\u22121\/2K(H\u22121\/2\u00b7), with a multivariate kernel K and a\npositive definite bandwidth matrix H = diag(h21, . . . , h\n2\np). Extensions to dis-\nconnected and branched curves were considered in [5] and [6], respectively,\nand are easily implemented by using suitable multiple starting points. Cross-\nings can be handled conveniently using an angle penalization [5]. As in each\niteration only points in the local neighborhood are considered, the algorithm\nis quite flexible, and, at the same time, robust to outlying data patterns.\n2.2 Simple example: Speed-flow data\nFig. 1 displays data recorded on the Californian freeway FR57-N on 9th of July\n2007. Each dot corresponds to the average of speed and flow values aggregated\nover 5-minute intervals. A LPC is fitted, using parameters h1 = h2 = t0 = 4,\nand a starting point selected at random from the original data. The resulting\npoints \u00b5x are symbolized by black squares in Fig. 1.\nHow does one go about connecting the points? For descriptive purposes a\nlinear interpolation is sufficient, as it was handled in the original references\n[5, 6]. However, if the curve is to be used for further processing, it would need\nto be fully parametrized. One way of achieving this is to use a cubic spline\n(a piecewise polynomial function constructed from third order polynomials),\nyielding a continuous and differentiable smooth curve, as outlined below.\n2.3 Parametrizations and Projections\nFor a fitted LPC consisting of L local centers of mass \u00b5x` \u2261 \u00b5` = (\u00b5`1, . . . , \u00b5\n`\np)\nT ,\n` = 1, . . . , L, we seek a parametrization t such that the curve can be written\nas a function\nf : R \u2212\u2192 Rp, t 7\u2192 (f1(t), . . . , fp(t))\nT\n,\nattaining the L points \u00b5` as outputs for certain parameter values t. Firstly,\none end point is chosen to be the origin corresponding to t = 0. This is an\narbitrary choice and we use the convention that t increases in the direction of\n\u03b3x0 . Technically, the curve is parametrized in three steps:\n(i) Compute a discrete, preliminary parametrization (s`)(1\u2264`\u2264L), with the\nsame origin as t, by adding up Euclidean distances between subsequent\n\u00b5`, ` = 1, . . . , L.\n(ii) For each j = 1, . . . , p, lay a cubic spline through the set of points\n(s`, \u00b5\n`\nj)1\u2264`\u2264L, yielding graphs (s, \u00b5j(s)). Putting them together, one ob-\ntains a continuous and differentiable spline function (\u00b51, . . . , \u00b5p)\nT (s).\n4 Jochen Einbeck, Ludger Evers and Kirsty Hinchliff\n+\n+\n+\n+\n+\n+\n++\n+ +\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+ +\n+\n+\n+ +\n++\n++\n+++\n++\n+ + ++\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+ + ++\n+\n+\n+\n+\n+\n+\n+\n+\n+ +\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+ +\n++\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+++\n+ +\n++\n+\n+ +\n+\n+\n++ ++ +\n+\n++ +\n+\n++++\n+\n+\n+\n+\n+\n+\n+\n+ +\n+\n+\n+\n++\n++++ +\n++\n+\n+\n+ +\n++\n+\n++\n+\n++\n+ +\n+ +\n+\n+\n+\n++ +\n+\n++ +\n+\n+\n+\n++ +\n++ ++\n+\n+ +\n+\n+\n+\n++ +\n+\n+\n+\n++\n+ ++\n++ +\n+\n+\n+ +\n++\n+ +\n+\n+\n+\n+\n+\n+\n++\n++\n++\n+\n+\n++\n+\n++\n+\n+ +\n++ +\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+ +\n++\n+\n+\n+\n+\n+\n+\n+ +\n+\n+\n+\n+\n++\n+\n+\n++\n+\n+ +\n+\n+\n++ +\n+++ +\n+ ++\n+\n+\n++\n+\n+\n+\n++\n+\n+\n+ +\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+ + ++\n+\n+\n+\n+ +\n+\n++ +\n+\n+\n+\n+\n+\n++ +\n++\n+\n+ ++ +\n+\n++\n+\n+ ++\n++\n0 20 40 60\n10\n20\n30\n40\n50\n60\n70\nflow (veh\/2.5min)\nsp\nee\nd \n(m\nph\n)\nFig. 1. Speed-flow data (+) and principal curve (solid curve) with local centers of\nmass (filled squares).\n(iii)Recalculate the parameter through the arc length of this spline function:\nt =\n\u222b s\n0\n\u221a\n(\u00b5\u20321(u))\n2 + . . .+ (\u00b5\u2032p(u))\n2 du.\nIt should be noted that no smoothing is involved in (ii) \u2014 this is a purely\nmechanical step interpolating the \u00b5` through a string of cubic polynomials.\nOnce that this parametrization is established, each data point Xi, i =\n1, . . . , n, can be projected on the point of the curve nearest to it (in terms\nof Euclidean distances), yielding the projection index ti. Data can be de-\ncompressed by evaluating the principal curve f , represented through the\np\u2212dimensional spline function, at ti.\nAn illustration is given in Fig. 2. Note that, though the parametrization is\nunit-speed (i.e., distances in parameter space correspond to distances in data\nspace along the principal curve), the projections are not topology-preserving:\ndata points which are neighboring in data space are not necessarily neighbor-\ning in parameter space. This is a general property of data compression through\nprincipal curves, which distinguishes such methods from topology-preserving,\nbut less interpretable mappings [12].\n3 Regression with principal curves\n3.1 GAIA data\nGAIA is an astrophysics mission of the European Space Agency (ESA). A\nsatellite is to be launched in 2011 which will undertake a detailed survey of\nData compression with principal curves 5\n+\n+\n+\n+\n+\n+\n++\n+ +\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+ +\n+\n+\n+ +\n++\n++\n+++\n++\n+ + ++\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+ + ++\n+\n+\n+\n+\n+\n+\n+\n+\n+ +\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+ +\n++\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+++\n+ +\n++\n+\n+ +\n+\n+\n++ ++ +\n+\n++ +\n+\n++++\n+\n+\n+\n+\n+\n+\n+\n+ +\n+\n+\n+\n++\n++++ +\n++\n+\n+\n+ +\n++\n+\n++\n+\n++\n+ +\n+ +\n+\n+\n+\n++ +\n+\n++ +\n+\n+\n+\n++ +\n++ ++\n+\n+ +\n+\n+\n+\n++ +\n+\n+\n+\n++\n+ ++\n++ +\n+\n+\n+ +\n++\n+ +\n+\n+\n+\n+\n+\n+\n++\n++\n++\n+\n+\n++\n+\n++\n+\n+ +\n++ +\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+ +\n++\n+\n+\n+\n+\n+\n+\n+ +\n+\n+\n+\n+\n++\n+\n+\n++\n+\n+ +\n+\n+\n++ +\n+++ +\n+ ++\n+\n+\n++\n+\n+\n+\n++\n+\n+\n+ +\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+ + ++\n+\n+\n+\n+ +\n+\n++ +\n+\n+\n+\n+\n+\n++ +\n++\n+\n+ ++ +\n+\n++\n+\n+ ++\n++\n0 20 40 60\n10\n20\n30\n40\n50\n60\n70\nflow (veh\/2.5min)\nsp\nee\nd \n(m\nph\n)\nFig. 2. Speed-flow data with principal curve (solid) and projections (dashed lines).\nover 109 stars in our Galaxy and extragalactic objects. The aims of the mission\nare, among others, to classify objects into stars, galaxies, quasars, etc., and to\ndetermine astrophysical parameters (\u201cAPs\u201d: temperature, metallicity, gravity)\nfrom spectroscopic data (photon counts at certain wavelengths) [4]. Yet, one\nhas to work with simulated data generated through complex computer models.\nFig. 3 gives an example for a set of n = 8286 sixteen-dimensional photon\ncounts simulated from APs through computer models.\nspec1\n0.78\n0.790.780.79\n0.76\n0.77\n0.760.77\nspec20.005\n0.0100.005.010\n0.000\n0.0050.000.005\nspec30.010\n0.0150.0200.010. 15.020\n0.0000.005\n0. 10\n0.000. 05.010\nspec40.015\n0.0200.0250.015.020.025\n0.0050.010\n0.015\n0.005.010.015\nspec50.04\n0.06.040.06\n0.00\n0.020.00.02\nspec60.06\n0.080.100.06.08.10\n0.000.02\n0.04\n0.00.02.04\nspec70.020.03\n0.040.02.03.04\n0.000.01\n0.02\n0.00.01.02\nspec80.010\n0.015.0100. 15\n0.000\n0.0050.0000. 05\nspec90.010\n0.015\n0.020.0100. 150.020\n0.000\n0.005\n0. 10\n0.0000. 050.010\nspec100.015\n0.0200.025. 15.020.025\n0.0000.005\n0.0\n0.000.005. 1\nspec11\n0.0060.0080.0060.008\n0.0020.0040.0020. 04\nspec12\n0.03\n0.040.030.04\n0.01\n0.02\n0.010.02\nspec130.015\n0.0200.0250.015.020.025\n0.0050.0 0\n0.015\n0.005.010.015\nspec140.015\n0.0200.0250.015.020.025\n0.0050.0 0\n0.015\n0.005.01.015\nspec150.03\n0.040.05.03.04.05\n0.000.01\n0.020.00.01.02\nspec160.04\n0.060.080.040.060.08\n0.000.02\n0.04\n0.000.020.04\nFig. 3. GAIA data. Pairwise plots of 16-dim. photon counts.\n6 Jochen Einbeck, Ludger Evers and Kirsty Hinchliff\nNote that, for the actual estimation problem, the photon counts form the\npredictor space and the APs form the response space, this is opposite to the\ndirection of simulation. As a consequence, the regression problem may be\ndegenerate (i.e. one set of photon counts may be associated to two different\nAPs). In the following, we will focus on the temperature, which features the\nleast amount of degeneracy. We use a sample of size n\u2032 = 1000 from the\noriginal data for all following calculations. Fitting a multiple linear regression\nmodel for temperature against the sixteen individual photon counts leads to\na residual standard error of 1978 on 983 degrees of freedom, with t\u2212values\nfor all variables around 0.65 and corresponding p\u2212values around 0.51. We\nconclude that this does not constitute a useful model for the data.\nComp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9 Comp.10\ngaia.pc\nVa\nria\nnc\nes\n0.\n00\n00\n0.\n00\n02\n0.\n00\n04\n0.\n00\n06\n0.\n00\n08\n0.\n00\n10\n0.\n00\n12\n0.\n00\n14\nFig. 4. Scree plot for GAIA data.\n3.2 Principal component regression\nThe usual remedies in this case are model\/variable selection procedures or di-\nmension reduction techniques. The second one is obviously the most promising\nhere. A common starting point for the application of the latter is the scree plot\n(Fig. 4), indicating that at most three components (these explain 98.9% of the\ntotal variance) appear to be sufficient to capture the information provided by\nthese data. The usual way to continue is then to regress y = temperature\nagainst the scores associated with the largest three principal components, i.e.\ny = \u03b20 + \u03b21score1 + \u03b22score2 + \u03b23score3 + \u000f (1)\nFitting this trivariate linear regression problem leads to a residual standard\nerror of 2060 on 996 degrees of freedom, with p\u2212values < 2e\u2212 16 for all four\nData compression with principal curves 7\nregression parameters. The residual standard error of this model is naturally\nlarger than the previous one, being just an approximation of the full linear\nmodel based on 98.9% of the available information. Nevertheless, this model\nis the more appropriate one. It remains the question whether the first three\nPC scores still feature some inner structure which we could exploit.\n3.3 Dimension reduction with local principal curves\nTo investigate this, we produce a three-dimensional scatterplot of the PC\nscores, and shade lower temperatures with darker grey tones (Fig. 5 left).\nClearly there is some curvilinear inner structure, which is informative for the\ntarget variable, temperature. Hence, the following is to do:\n(1) Fit a principal curve through the 3-dim. data cloud of PC scores.\n(2) Parametrize the principal curve and project all data points onto it.\n(3) Fit temperature (or other APs) against the (1-dimensional) projections.\nFor task (1), a LPC is straightforwardly fitted3 (Fig. 5 left). Alternatively,\nany other principal curve algorithm which provides access to the parametriza-\ntion and allows for continuous projections could be used. This would include\nthe HS algorithm, as far as it copes with the complexity of the data in itself.\nAlgorithms based on piecewise line segments as in [10] are rather problematic\nfor this purpose as projections tend to be clustered around the knots, unless\nthe procedure outlined in Subsection 2.3 is additionally applied to them.\nWe perform task (2) as described in Subsection 2.3 and plot temperature\nagainst the projection indices. In (3), we are left with a simple one-dimensional\nnonparametric regression problem yi = m(ti)+\u03b5i. We used penalized smooth-\ning splines to fit this model but any nonparametric smoother could be used.\nThe smooth fit is shown in Fig. 5 (right).\n3.4 Direct local principal curve regression\nOne may be wondering if there is a shortcut to this. Instead of the 2-stage\nstrategy \u201cPC+LPC\u201d used so far, one could consider to fit the local principal\ncurve directly through the n\u2032\u00d716 dimensional photon counts, as shown in Fig.\n6. Comparing this result cursorily with Fig. 3, it appears that the data are\nreasonably represented (For a more quantitative evaluation of the accuracy of\na principal curve, a coverage measure is available [5], and for the assessment\nof its precision bootstrap methods may be applied [2]). Indeed, the one-stage\nstrategy is feasible in principle, and the results for both strategies are quite\nsimilar. However, as data gets sparse in high dimensions, the LPC may miss\nremote parts of the predictor space (the previously mentioned robustness may\nbackfire here), which then get inadequately projected. The consequence of\n3 using the default settings of R function lpc for the parameters; these are: hj =\n1\/10 \u00d7 {range of variable j}, and t0 = (1\/d)\nP\nj\nhj\n8 Jochen Einbeck, Ludger Evers and Kirsty Hinchliff\n\u22120.08 \u22120.06 \u22120.04 \u22120.02  0.00  0.02  0.04\u2212\n0.0\n2\n\u2212\n0.0\n1\n \n0.0\n0\n \n0.0\n1\n \n0.0\n2\n \n0.0\n3\n \n0.0\n4\n\u22120.10\n\u22120.05\n 0.00\n 0.05\n 0.10\nComp.2\nCo\nmp\n.1\nCo\nmp\n.3\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n++\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n++\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n++\n++\n++\n++\n+\n++\n+ ++\n+\n++\n+\n++\n+\n+\n+\n+\n+\n++\n+\n++\n+\n++\n+\n+\n+\n++\n++\n++\n++++\n++++ ++\n++\n+\n+\n+ ++\n+\n++\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++++\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+++\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n++\n++\n+\n+++\n+\n++\n++\n+\n+++++\n+\n+\n+++++\n++++++\n+\n+\n++\n+\n+\n+++++++\n+++\n+\n+++++++\n+\n+\n+\n+\n+++\n+\n+++++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n0.00 0.05 0.10 0.15 0.20\n10\n00\n0\n20\n00\n0\n30\n00\n0\n40\n00\n0\n50\n00\n0\nProjection indices (t)\nTe\nmp\nera\ntur\ne\n+ Temperature\nSmoothing Spline\nFig. 5. Left: Scatterplot of first three principal component scores with local prin-\ncipal curve (\u2014). The less intense the grey tone, the larger is the temperature; right:\nTemperatures fitted versus projection indices.\nthis is an increased sensitivity of the 16-dimensional LPC to the choice of the\nstarting point compared to the 3-dimensional one. When approximating data\nthrough PCA in a first step, data are far less sparse in the second. Principal\ncomponents cannot miss isolated data points as PC lines can be thought of\nas being infinitely long.\nspec10.775\n0.7800.7850.7750.7800.785\n0.7600.765\n0.770\n0.7600.7650.770\nspec20.005\n0.0100.0050.010\n0.000\n0.0050.0000.005\nspec30.010\n0.0150.0200.010. 15.020\n0.0000.005\n0. 10\n0.000. 05.010\nspec40.015\n0.0200.0250.015.020. 25\n0.0050.010\n0.015\n0.005.010. 15\nspec50.04\n0.060.040.06\n0.00\n0.020.00.02\nspec60.040.06\n0.080.04.06.08\n0.000.02\n0.04\n0.00.02.04\nspec70.02\n0.030.040.02.03.04\n0.000.01\n0.02\n0.00.01.02\nspec80.010\n0.015.0100.015\n0.000\n0.0050.000.005\nspec90.010\n0.015.0100. 15\n0.000\n0.0050.000. 05\nspec100.015\n0.0200. 15.020\n0.0050.00.005. 1\nspec11\n0.006\n0.0080.006.008\n0.002\n0.004\n0.002. 04\nspec12\n0.03\n0.040.030.04\n0.01\n0.00.010.02\nspec13\n0.015\n0.0200.0150.020\n0.005\n0.010\n0.0050. 10\nspec140.010\n0.015.0100.015\n0.005\n0. 10\n0.005.010\nspec150.020.03\n0.040.02.03.04\n0.000.01\n0.02\n0.00.01.02\nspec160.04\n0.06.040.06\n0.00\n0.020.000.02\nFig. 6. Pairwise plot of LPC fitted through 16-dim. photon counts.\nData compression with principal curves 9\n3.5 Prediction and Comparison\nFor a new observation xnew (i.e., here, a new set of spectra), prediction pro-\nceeds as follows: (i) Project xnew onto the LPC (either in one or two steps),\ngiving tnew. (ii) Compute y\u02c6new = m\u02c6(tnew) from the fitted nonparametric\nsmoother (hereafter: NS).\nTable 1 shows prediction errors for each 200 observations sampled from the\ntraining data set and the remaining n \u2212 n\u2032 = 7286 data points, respectively.\nBeside the methods discussed so far, we include an additive model using PC\nscores (a model just as in (1), but with all linear terms replaced by smooth\nfunctions; hereafter: AM).\nTable 1. Prediction errors (\/103) in comparison. \u03b5\u02c6i is the difference between\ntrue and predicted temperature (LM= Linear Model, PC=Principal components,\nAM=Additive model, NS=Nonparametric Smoother)\nLM PC+LM PC+AM PC+LPC+NS LPC+ NS\nTraining average (\u03b5\u02c62i ) 4\u2019119 4\u2019395 1\u2019318 2\u2019633 2\u2019215\ndata median (\u03b5\u02c62i ) 1\u2019035 1\u2019300 123 51 66\nTest average (\u03b5\u02c62i ) 6\u2019393 6\u2019743 2\u2019054 5\u2019695 4\u2019667\ndata median (\u03b5\u02c62i ) 723 808 147 45 46\nAs expected, and mentioned earlier, PC+LM is slightly worse than LM,\nand obviously PC+AM is better than PC+LM. The three nonparametric\napproaches clearly beat the parametric ones. The best median of squared\nresiduals is taken by PC+LPC+NS, which is of a similar magnitude as that\nfor LPC+NS and PC+AM. The mean of the squared residuals falls behind\nfor the LPC-based methods compared to PC+AM. This can be explained as\npoints close to the \u201cend\u201d of the data cloud are all projected onto the endpoint\nof the LPC, which leads to a degeneracy at either t = 0 or t = tmax (or\nboth). This is visible in Fig\u2019s 2 and 5 (right). So, though the LPC-based\nmethods work very well for the large bulk of the data, they do not handle the\nfew points close to the endpoints of the principal curve very well. Artificially\nextrapolating the fitted LPC beyond its natural endpoints may help to solve\nthis problem.\n4 Outlook\nLocal principal curves are well suited to compress complex high-dimensional\ndata structures, as long as the intrinsic dimensionality of the data cloud is\nclose to one. When the intrinsic dimensionality is two or larger, the extension\nto local principal manifolds should be considered. In particular, the GAIA\ndata may be better approximated by a two-dimensional principal surface.\nThis would be particular helpful for the prediction of other APs as gravity or\n10 Jochen Einbeck, Ludger Evers and Kirsty Hinchliff\nmetallicity, information on which tends to be orthogonal to the principal curve\napproximating the predictor space. The work on extending LPC methodol-\nogy to higher-dimensional structures is currently ongoing, based on the idea\nof replacing the building block \u201clocalized principal component\u201d by suitably\norientated triangles or tetrahedrons.\nAcknowledgements\nWe would like to thank Coryn-Bailer-Jones, leader of the group \u201cAstrophysical\nparameters\u201d based at MPIA Heidelberg, for providing the simulated GAIA\ndata and explaining the background of the GAIA mission. The collaboration\nbetween the first two authors was supported by LMS Grant Ref 4709. The\nthird author was funded by an EPSRC Vacation Bursary.\nReferences\n1. J. D. Banfield and A. E. Raftery. Ice flow identification in satellite images using\nmathematical morphology and clustering about principal curves. Journal of the\nAmerican Statistical Association 87:7\u201316, 1992.\n2. C. Brunsdon. Path estimation from GPS tracks. Geocomputation 2007, NUI\nMaynooth, Ireland.\n3. K. Chang and J. Ghosh. Principal curves for nonlinear feature extraction and\nclassification. SPIE Applications of Artificial Neural Networks in Image Process-\ning III, 3307:120\u2013129, 1998.\n4. C.A.L. Bailer-Jones. Determination of stellar parameters with GAIA. Astro-\nphysics and Space Science, 280:21\u201329, 2002.\n5. J. Einbeck, G. Tutz, and L. Evers. Local principal curves. Statistics and Com-\nputing, 15:301\u2013313, 2005.\n6. J. Einbeck, G. Tutz, and L. Evers. Exploring multivariate data structures with\nlocal principal curves. In Weihs, C. and Gaul, W., editors, Classification - The\nUbiquitous Challenge, pages 256\u2013263. Springer, Heidelberg, 2005.\n7. P. Delicado. Another look at principal curves and surfaces. Journal of Multivari-\nate Analysis, 77:84\u2013116, 2001.\n8. T. Hastie and W. Stuetzle. Principal curves. Journal of the American Statistical\nAssociation, 84:502\u2013516, 1989.\n9. T. Hastie, R. Tibshirani and J. Friedman. The Elements of Statistical Learning.\nSpringer, New York, 2001.\n10. B. Ke\u00b4gl, A. Krzyz\u02d9ak, T. Linder and K. Zeger. Learning and design of principal\ncurves. IEEE Trans. on Pattern Analysis and Machine Intelligence, 22:281\u2013297,\n2000.\n11. B. Ke\u00b4gl and A. Krzyz\u02d9ak. Piecewise linear skeletonization using principal curves.\nIEEE Trans. on Pattern Analysis and Machine Intelligence, 24:59\u201374, 2002.\n12. M. Pen\u02dca, W. Barbakh and Colin Fyfe. Elastic maps and nets for approximat-\ning principal manifolds and their application to microarray data visualization.\nIn A.N. Gorban et al., editors, Principal Manifolds for Data Visualization and\nDimension Reduction, pages 131\u2013150. Springer, Berlin, 2008.\n"}