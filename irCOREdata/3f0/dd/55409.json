{"doi":"10.1109\/TRO.2008.2004642","coreId":"55409","oai":"oai:eprints.lincoln.ac.uk:2094","identifiers":["oai:eprints.lincoln.ac.uk:2094","10.1109\/TRO.2008.2004642"],"title":"A minimalistic approach to appearance-based visual SLAM","authors":["Andreasson, Henrik","Duckett, Tom","Lilienthal, Achim"],"enrichments":{"references":[{"id":18435241,"title":"6dof entropy minimization slam,\u201d in","authors":[],"date":"2006","doi":"10.1109\/robot.2006.1641928","raw":"J. Sez and F. Escolano, \u201c6dof entropy minimization slam,\u201d in Proc. IEEE Int. Conf. Robotics and Automation (ICRA), 2006, pp. 1548\u20131555.","cites":null},{"id":18435237,"title":"A framework for vision based bearing only 3D SLAM,\u201d in","authors":[],"date":"2006","doi":"10.1109\/robot.2006.1641990","raw":"P. Jensfelt, D. Kragic, J. Folkesson, and M. Bj\u00a8 orkman, \u201cA framework for vision based bearing only 3D SLAM,\u201d in Proc. IEEE Int. Conf. Robotics and Automation (ICRA), 2006, pp. 1944\u20131950.","cites":null},{"id":18435230,"title":"A multilevel relaxation algorithm for simultaneous localisation and mapping,\u201d","authors":[],"date":"2005","doi":"10.1109\/tro.2004.839220","raw":"U. Frese, P. Larsson, and T. Duckett, \u201cA multilevel relaxation algorithm for simultaneous localisation and mapping,\u201d IEEE Transactions on Robotics, vol. 21, no. 2, pp. 196\u2013207, April 2005.","cites":null},{"id":18435249,"title":"Dynamic map building and localization for autonomous vehicles,\u201d","authors":[],"date":"1995","doi":null,"raw":"J. Uhlmann, \u201cDynamic map building and localization for autonomous vehicles,\u201d Ph.D. dissertation, University of Oxford, 1995.","cites":null},{"id":18435246,"title":"Fast iterative optimization of pose graphs with poor initial estimates,\u201d","authors":[],"date":"2006","doi":"10.1109\/robot.2006.1642040","raw":"E. Olson, J. Leonard, and S. Teller, \u201cFast iterative optimization of pose graphs with poor initial estimates,\u201d 2006, pp. 2262\u20132269.","cites":null},{"id":18435254,"title":"GPU-based video feature tracking and matching,\u201d in Workshop on Edge Computing Using New Commodity Architectures (EDGE 2006), Chapel Hill,","authors":[],"date":"2006","doi":null,"raw":"S. N. Sinha, J.-M. Frahm, M. Pollefeys, and Y. Genc, \u201cGPU-based video feature tracking and matching,\u201d in Workshop on Edge Computing Using New Commodity Architectures (EDGE 2006), Chapel Hill, 2006. Henrik Andreasson is a Ph.D. student at Centre for Applied Autonomous Sensor System, \u00a8 Orebro University, Sweden. He received his Master degree in Mechatronics from Royal Institute of Technology, Sweden, in 2001. His research interests include mobile robotics, computer vision, and machine learning. Tom Duckett is a Reader at the Department of Computing and Informatics, University of Lincoln, where he is also Director of the Centre for Vision and Robotics Research. He was formerly a docent (Associate Professor) at \u00a8 Orebro University, where he was also leader of the Learning Systems Laboratory within the Centre for Applied Autonomous Sensor Systems. He obtained his Ph.D. from Manchester University, M.Sc. with distinction from Heriot-Watt University and B.Sc. (Hons.) from Warwick University, and has also studied at Karlsruhe and Bremen Universities. His research interests include mobile robotics, navigation, machine learning, AI, computer vision, and sensor fusion for perception-based control of autonomous systems. Achim Lilienthal is a docent (associate professor) at the AASS Research Center, \u00a8 Orebro, Sweden where he is leading the Learning Systems Lab. He obtained his Ph.D. in computer science from T\u00a8 ubingen University, Germany and his M.Sc. and B.Sc. in Physics from the University of Konstanz, Germany. The Ph.D. thesis addresses gas distribution mapping and gas source localisation with a mobile robot. The M.Sc. thesis is concerned with an investigation of the structure of (C60)+ n clusters using gas phase ion chromatography. His main research interests are mobile robot olfaction, robot vision, robotic map learning and safe navigation systems.","cites":null},{"id":18435247,"title":"Learning probabilistic motion models for mobile robots,\u201d in","authors":[],"date":"2004","doi":"10.1145\/1015330.1015413","raw":"A. I. Eliazar and R. Parr, \u201cLearning probabilistic motion models for mobile robots,\u201d in Proc. of the twenty-\ufb01rst Int. Conf. on Machine learning (ICML), 2004, p. 32.","cites":null},{"id":18435253,"title":"Least-squares \ufb01tting of two 3-d point sets,\u201d","authors":[],"date":"1987","doi":"10.1109\/tpami.1987.4767965","raw":"K. S. Arun, T. S. Huang, and S. D. Blostein, \u201cLeast-squares \ufb01tting of two 3-d point sets,\u201d IEEE Trans. Pattern Anal. Mach. Intell., vol. 9, no. 5, pp. 698\u2013700, 1987.","cites":null},{"id":18435229,"title":"Localization for mobile robots using panoramic vision, local features and particle \ufb01lter,\u201d in","authors":[],"date":"2005","doi":"10.1109\/robot.2005.1570627","raw":"H. Andreasson, A. Treptow, and T. Duckett, \u201cLocalization for mobile robots using panoramic vision, local features and particle \ufb01lter,\u201d in Proc. IEEE Int. Conf. Robotics and Automation (ICRA), 2005, pp. 3348\u20133353.","cites":null},{"id":18435243,"title":"Loop closure detection in SLAM by combining visual and spatial appearance,\u201d Robotics and Autonomous System,","authors":[],"date":"2006","doi":"10.1016\/j.robot.2006.04.016","raw":"K. L. Ho and P. Newman, \u201cLoop closure detection in SLAM by combining visual and spatial appearance,\u201d Robotics and Autonomous System, vol. 54, no. 9, pp. 740\u2013749, September 2006.","cites":null},{"id":18435232,"title":"Mini-SLAM: Minimalistic visual SLAM in large-scale environments based on a new interpretation of image similarity,\u201d","authors":[],"date":"2007","doi":"10.1109\/robot.2007.364108","raw":"H. Andreasson, T. Duckett, and A. Lilienthal, \u201cMini-SLAM: Minimalistic visual SLAM in large-scale environments based on a new interpretation of image similarity,\u201d in IEEE International Conference on Robotics and Automation (ICRA 2007), Rome, Italy, 2007.","cites":null},{"id":18435235,"title":"Mobile robot localization and mapping with uncertainty using scale-invariant visual landmarks,\u201d","authors":[],"date":"2002","doi":"10.1177\/027836402128964611","raw":"S. Se, D. Lowe, and J. Little, \u201cMobile robot localization and mapping with uncertainty using scale-invariant visual landmarks,\u201d International Journal of Robotics Research, vol. 21, no. 8, pp. 735\u2013758, 2002.","cites":null},{"id":18435252,"title":"Multi-robot simultaneous localization and mapping using particle \ufb01lters,\u201d","authors":[],"date":"2005","doi":"10.1177\/0278364906072250","raw":"A. Howard, \u201cMulti-robot simultaneous localization and mapping using particle \ufb01lters,\u201d in Proceedings of Robotics: Science and Systems, Cambridge, USA, June 2005.","cites":null},{"id":18435234,"title":"Object recognition from local scale-invariant features,\u201d in","authors":[],"date":"1999","doi":"10.1109\/iccv.1999.790410","raw":"D. Lowe, \u201cObject recognition from local scale-invariant features,\u201d in Proc. Int. Conf. Computer Vision ICCV, Corfu, 1999, pp. 1150\u20131157.","cites":null},{"id":18435245,"title":"Online constraint network optimization for ef\ufb01cient maximum likelihood map learning,\u201d","authors":[],"date":"2008","doi":"10.1109\/robot.2008.4543481","raw":"G. Grisetti, D. Lordi Rizzini, C. Stachniss, E. Olson, and W. Burgard, \u201cOnline constraint network optimization for ef\ufb01cient maximum likelihood map learning,\u201d Pasadena, CA, USA, 2008.","cites":null},{"id":18435236,"title":"Online visual motion estimation using FastSLAM with SIFT features,\u201d","authors":[],"date":"2005","doi":"10.1109\/iros.2005.1545444","raw":"T. Barfoot, \u201cOnline visual motion estimation using FastSLAM with SIFT features,\u201d in Proc. of the IEEE\/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), 2005, pp. 579\u2013585.","cites":null},{"id":18435244,"title":"Outdoor SLAM using visual appearance and laser ranging,\u201d in","authors":[],"date":"2006","doi":"10.1109\/robot.2006.1641869","raw":"P. M. Newman, D. M. Cole, and K. L. Ho, \u201cOutdoor SLAM using visual appearance and laser ranging,\u201d in Proc. IEEE Int. Conf. Robotics and Automation (ICRA), 2006, pp. 1180\u20131187.","cites":null},{"id":18435242,"title":"Real-time simultaneous localisation and mapping with a single camera,\u201d","authors":[],"date":"2003","doi":"10.1109\/iccv.2003.1238654","raw":"A. Davison, \u201cReal-time simultaneous localisation and mapping with a single camera,\u201d in Proceedings of the IEEE International Conference on Computer Vision (ICCV\u201903), 2003, pp. 1403\u20131410.","cites":null},{"id":18435248,"title":"Rover localization in natural environments by indexing panoramic images,\u201d","authors":[],"date":"2002","doi":"10.1109\/robot.2002.1014733","raw":"J. Gonzalez-Barbosa and S. Lacroix, \u201cRover localization in natural environments by indexing panoramic images,\u201d in Proceedings of the International Conference on Robotics and Automation (ICRA). IEEE, 2002, pp. 1365\u20131370.","cites":null},{"id":18435238,"title":"The vSLAM algorithm for robust localization and mapping,\u201d in","authors":[],"date":"2005","doi":"10.1109\/robot.2005.1570091","raw":"N. Karlsson, E. D. Bernardo, J. Ostrowski, L. Goncalves, P. Pirjanian, and M. E. Munich, \u201cThe vSLAM algorithm for robust localization and mapping,\u201d in Proc. IEEE Int. Conf. Robotics and Automation (ICRA), 2005, pp. 24\u201329.","cites":null},{"id":18435251,"title":"Using covariance intersection for slam,\u201d","authors":[],"date":"2007","doi":"10.1016\/j.robot.2006.06.011","raw":"S. J. Julier and J. K. Uhlmann, \u201cUsing covariance intersection for slam,\u201d Robotics and Autonomous Systems, vol. 55, no. 1, pp. 3\u201320, 2007.","cites":null},{"id":18435240,"title":"\u03c3SLAM: Stereo vision SLAM using the Rao-Blackwellised particle \ufb01lter and a novel mixture proposal distribution,\u201d in","authors":[],"date":"2006","doi":"10.1109\/robot.2006.1641930","raw":"P. Elinas, R. Sim, and J. Little, \u201c\u03c3SLAM: Stereo vision SLAM using the Rao-Blackwellised particle \ufb01lter and a novel mixture proposal distribution,\u201d in Proc. IEEE Int. Conf. Robotics and Automation (ICRA), 2006, pp. 1564\u20131570.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2008-10","abstract":"This paper presents a vision-based approach to SLAM in indoor \/ outdoor environments with minimalistic sensing and computational requirements. The approach is based on a graph representation of robot poses, using a relaxation algorithm to obtain a globally consistent map. Each link corresponds to a\\ud\nrelative measurement of the spatial relation between the two nodes it connects. The links describe the likelihood distribution of the relative pose as a Gaussian distribution. To estimate the covariance matrix for links obtained from an omni-directional vision sensor, a novel method is introduced based on the relative similarity of neighbouring images. This new method does not require determining distances to image features using multiple\\ud\nview geometry, for example. Combined indoor and outdoor experiments demonstrate that the approach can handle qualitatively different environments (without modification of the parameters), that it can cope with violations of the \u201cflat floor assumption\u201d to some degree, and that it scales well with increasing size of the environment, producing topologically correct and geometrically accurate maps at low computational cost. Further experiments demonstrate that the approach is also suitable for combining multiple overlapping maps, e.g. for solving the multi-robot SLAM problem with unknown initial poses","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/55409.pdf","fullTextIdentifier":"http:\/\/eprints.lincoln.ac.uk\/2094\/1\/Andreasson_etal_2008-IEEE_TRO.pdf","pdfHashValue":"09c6f2ad76e429116dd474e81e788f893f5fdb72","publisher":"IEEE","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lincoln.ac.uk:2094<\/identifier><datestamp>\n      2013-11-18T12:28:21Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47373030<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47373430<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lincoln.ac.uk\/2094\/<\/dc:relation><dc:title>\n        A minimalistic approach to appearance-based visual SLAM<\/dc:title><dc:creator>\n        Andreasson, Henrik<\/dc:creator><dc:creator>\n        Duckett, Tom<\/dc:creator><dc:creator>\n        Lilienthal, Achim<\/dc:creator><dc:subject>\n        G700 Artificial Intelligence<\/dc:subject><dc:subject>\n        G740 Computer Vision<\/dc:subject><dc:description>\n        This paper presents a vision-based approach to SLAM in indoor \/ outdoor environments with minimalistic sensing and computational requirements. The approach is based on a graph representation of robot poses, using a relaxation algorithm to obtain a globally consistent map. Each link corresponds to a\\ud\nrelative measurement of the spatial relation between the two nodes it connects. The links describe the likelihood distribution of the relative pose as a Gaussian distribution. To estimate the covariance matrix for links obtained from an omni-directional vision sensor, a novel method is introduced based on the relative similarity of neighbouring images. This new method does not require determining distances to image features using multiple\\ud\nview geometry, for example. Combined indoor and outdoor experiments demonstrate that the approach can handle qualitatively different environments (without modification of the parameters), that it can cope with violations of the \u201cflat floor assumption\u201d to some degree, and that it scales well with increasing size of the environment, producing topologically correct and geometrically accurate maps at low computational cost. Further experiments demonstrate that the approach is also suitable for combining multiple overlapping maps, e.g. for solving the multi-robot SLAM problem with unknown initial poses.<\/dc:description><dc:publisher>\n        IEEE<\/dc:publisher><dc:date>\n        2008-10<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lincoln.ac.uk\/2094\/1\/Andreasson_etal_2008-IEEE_TRO.pdf<\/dc:identifier><dc:identifier>\n          Andreasson, Henrik and Duckett, Tom and Lilienthal, Achim  (2008) A minimalistic approach to appearance-based visual SLAM.  IEEE Transactions on Robotics, 24  (5).   pp. 991-1001.  ISSN 1552-3098  <\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/TRO.2008.2004642<\/dc:relation><dc:relation>\n        10.1109\/TRO.2008.2004642<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lincoln.ac.uk\/2094\/","http:\/\/dx.doi.org\/10.1109\/TRO.2008.2004642","10.1109\/TRO.2008.2004642"],"year":2008,"topics":["G700 Artificial Intelligence","G740 Computer Vision"],"subject":["Article","PeerReviewed"],"fullText":"IEEE TRANSACTION ON ROBOTICS, SPECIAL ISSUE ON VISUAL SLAM 1\nA Minimalistic Approach to Appearance\nbased Visual SLAM\nHenrik Andreasson, Tom Duckett, and Achim J. Lilienthal\nAbstract\u2014This paper presents a vision-based approach to\nSLAM in indoor \/ outdoor environments with minimalistic sens-\ning and computational requirements. The approach is based on a\ngraph representation of robot poses, using a relaxation algorithm\nto obtain a globally consistent map. Each link corresponds to a\nrelative measurement of the spatial relation between the two\nnodes it connects. The links describe the likelihood distribution\nof the relative pose as a Gaussian distribution. To estimate the\ncovariance matrix for links obtained from an omni-directional\nvision sensor, a novel method is introduced based on the relative\nsimilarity of neighbouring images. This new method does not\nrequire determining distances to image features using multiple\nview geometry, for example. Combined indoor and outdoor exper-\niments demonstrate that the approach can handle qualitatively\ndifferent environments (without modification of the parameters),\nthat it can cope with violations of the \u201cflat floor assumption\u201d to\nsome degree, and that it scales well with increasing size of the\nenvironment, producing topologically correct and geometrically\naccurate maps at low computational cost. Further experiments\ndemonstrate that the approach is also suitable for combining\nmultiple overlapping maps, e.g. for solving the multi-robot SLAM\nproblem with unknown initial poses.\nIndex Terms\u2014SLAM, Omnidirectional Vision\nI. INTRODUCTION\nThis paper presents a new vision-based approach to the\nproblem of simultaneous localization and mapping (SLAM).\nEspecially compared to SLAM approaches using a 2-d laser\nscanner, the rich information provided by a vision-based\napproach about a substantial part of the environment allows for\ndealing with high levels of occlusion [1] and enables solutions\nthat do not rely strictly on a flat floor assumption. Cameras\ncan also offer a longer range and are therefore advantageous\nin environments that contain large open spaces.\nThe proposed method is called \u201cMini-SLAM\u201d since it is\nminimalistic in several ways. On the hardware side, it relies\nsolely on odometry and an omni-directional camera as the\nexternal source of information. This allows for less expensive\nsystems compared to methods that use 2-d or 3-d laser\nscanners. Please note that the robot used for the experiments\nwas also equipped with a 2-d laser scanner. This laser scanner,\nhowever, was not used in the SLAM algorithm but only to\nvisualize the consistency of the created maps.\nApart from the frugal hardware requirements, the method is\nalso minimalistic in its computational demands. Map estima-\ntion is performed on-line by a linear time SLAM algorithm\non an efficient graph representation. The main difference\nH. Andreasson and A. J. Lilienthal are with the Department\nof Technology, \u00a8Orebro University, Sweden e-mail: {henrik.andreasson,\nachim.lilienthal}@tech.oru.se.\nT. Duckett is with the Department of Computer Science, University of\nLincoln, UK e-mail: tduckett@lincoln.ac.uk\nto other vision-based SLAM approaches is that there is no\nestimate of the positions of a set of landmarks involved,\nenabling the algorithm to scale up better with the size of\nthe environment. Instead, a measure of image similarity is\nused to estimate the relative pose between corresponding\nimages (\u201cvisual relations\u201d) and the uncertainty of this estimate.\nGiven these \u201cvisual relations\u201d and relative pose estimates\nbetween consecutive images obtained from the odometry of\nthe robot (\u201codometry relations\u201d), the Multilevel Relaxation\nalgorithm [2] is then used to determine the maximum likeli-\nhood estimate of all image poses. The relations are expressed\nas a relative pose estimate and the corresponding covariance.\nA key insight is that the estimate of the relative pose in\nthe \u201cvisual relations\u201d does not need to be very accurate as\nlong as the corresponding covariance is modeled appropriately.\nThis is because the relative pose is only used as an initial\nestimate that the Multilevel Relaxation algorithm can adjust\naccording to the covariance of the relation. Therefore, even\nwith fairly imprecise initial estimates of the relative poses\nit is possible to build geometrically accurate maps using the\ngeometric information in the covariance of the relative pose\nestimates. Mini-SLAM was found to produce consistent maps\nin various environments, including, for example, a data set of\nan environment containing indoor and outdoor passages (path\nlength of 1.4 km) and an indoor data set covering five floor\nlevels of a department building.\nFurther to our previously published work [3], we extended\nthe Mini-SLAM approach to the multi-robot SLAM problem,\ndemonstrating its ability to combine multiple overlapping\nmaps with unknown initial poses. We also provide an evalua-\ntion of the robustness of the suggested approach with respect\nto poor odometry or a less reliable measure of visual similarity.\nA. Related Work\nUsing a camera as the external source of information in\nSLAM has received increasing attention during the past years.\nMany approaches extract landmarks using local features in\nthe images and track the positions of these landmarks. As\nthe feature descriptor, Lowe\u2019s scale invariant feature transform\n(SIFT) [4] has been used widely [5], [6]. An initial estimate of\nthe relative pose change is often obtained from odometry [6],\n[7], [8], or where multiple cameras are available as in [9],\n[10], multiple view geometry can be applied to obtain depth\nestimates of the extracted features. To update and maintain\nvisual landmarks, Extended Kalman Filters (EKF) [7], [11]\nand Rao-Blackwellised Particle Filters (RBPF) [6], [9] have\nbeen used. In the visual SLAM method proposed in [11]\nparticle filters were utilised to obtain the depth of landmarks\nIEEE TRANSACTION ON ROBOTICS, SPECIAL ISSUE ON VISUAL SLAM 2\nwhile the landmark positions were updated with an EKF. Initial\nlandmark positions had to be provided by the user. A similar\napproach described in [8] applies a converse methodology. The\nlandmark positions were estimated with a Kalman filter (KF)\nand a particle filter was used to estimate the path.\nDue to their suitability for addressing the correspondence\nproblem, vision-based systems have been applied as an addi-\ntion to laser scanning based SLAM approaches for detecting\nloop closure. The principle has been applied to SLAM systems\nbased on a 2D laser scanner [12] and a 3D laser scanner [13].\nIn the approach proposed in this paper, the SLAM optimiza-\ntion problem is solved at the graph-level with the Multilevel\nRelaxation (MLR) method of Frese and Duckett [2]. This\nmethod could be replaced by alternative graph based SLAM\nmethods, for example, the online method proposed by Grisetti\net al. [14] based on the stochastic gradient descent method\nproposed by Olson et al. [15].\nThe rest of this paper is structured as follows. Section II de-\nscribes the proposed SLAM approach. Then the experimental\nset-up is detailed and the results are presented in Section III.\nThe paper ends with conclusions and suggestions for future\nwork (Section IV).\nII. MINI-SLAM\nA. Multi-Level Relaxation\nThe SLAM optimization problem is solved at the graph-\nlevel with the Multilevel Relaxation (MLR) method of Frese\nand Duckett [2]. A map is represented as a set of nodes\nconnected in a graph structure. An example is shown in Fig. 1.\nEach node corresponds to the robot pose at a particular time\nand each link to a relative measurement of the spatial relation\nbetween the two nodes it connects. A node is created for each\nomni-image in this work and the terms node and frame are\nused interchangeably in this paper.\nThe MLR algorithm can be briefly explained as follows.\nThe input is a set R of m = |R| relations on n planar\nframes (i.e., a two-dimensional representation is used). Each\nrelation r \u2208 R describes the likelihood distribution of the\npose of frame a relative to frame b. Relations are modeled\nas a Gaussian distribution with mean \u00b5r and covariance Cr.\nThe output of the MLR algorithm is the maximum likelihood\nestimation vector x\u02c6 for the poses of all the frames. Thus, a\nglobally consistent set of Cartesian coordinates is obtained\nfor the nodes of the graph based on local (relative) and\ninconsistent (noisy) measurements, by maximizing the total\nlikelihood of all measurements.\nB. Odometry Relations\nThe Mini-SLAM approach is based on two principles. First,\nthat odometry is sufficiently accurate if the distance traveled is\nshort. Second, that by using visual matching, correspondence\nbetween robot poses can be detected reliably even though\nthe search region around the current pose estimate is large.\nAccordingly, two different types of relations are created in\nthe MLR graph, based on odometry ro and relations based on\nvisual similarity rv .\nFig. 1. The graph representation used. The figure shows frames (nodes) and\nrelations (edges), both the odometry ro and the visual relations rv . Visual\nrelations are indicated with dotted lines. Each frame a contains a reference to\na set of features Fa extracted from an omni-directional image Ia, an odometry\npose xoa, a covariance estimate of the odometry pose Cxoa , the estimated pose\nx\u02c6a and an estimate of its covariance Cx\u02c6a . Fig. 2 shows images corresponding\nto the region represented by the graph in this figure.\nOdometry relations ro are created between successive\nframes. The relative pose \u00b5ro is obtained directly from the\nodometry readings and the covariance Cro is estimated using\nthe motion model suggested in [16] as\nCro =\n\uf8ee\n\uf8f0 d\n2\u03b42Xd + t\n2\u03b42Xt 0 0\n0 d2\u03b42Yd + t\n2\u03b42Yt 0\n0 0 d2\u03b42\u03b8d + t\n2\u03b42\u03b8t\n\uf8f9\n\uf8fb\n(1)\nwhere d and t are the total distance traveled and total angle\nrotated between two successive frames. The \u03b4X parameters\nrelate to the forward motion, the \u03b4Y parameters to the side\nmotion and the \u03b4\u03b8 parameters to the rotation of the robot. The\nsix \u03b4-parameters adjust the influence of the distance d and\nrotation t in the calculation of the covariance matrix. They\nwere tuned manually once and then kept constant throughout\nthe experiments.\nC. Visual Similarity Relations\n1) Similarity Measure: Given two images Ia and Ib, fea-\ntures are first extracted using the SIFT algorithm [4]. This\nresults in two sets of features Fa and Fb for frame a and\nb. Each feature F = [x, y],H comprises the pixel position\n[x, y] and a histogram H containing the SIFT descriptor. The\nsimilarity measure Sa,b is based on the number of features that\nmatch between Fa and Fb.\nThe feature matching algorithm calculates the Euclidean\ndistance between each feature in image Ia and all the features\nin image Ib. A potential match is found if the smallest distance\nis smaller than 60% of the second smallest distance. This\ncriterion was found empirically and was also used in [17]. It\nguarantees that interest point matches are substantially better\nthan all other possible matches. We also do not allow features\nto be matched against more than one other feature. If a feature\nhas more than one candidate match, the match which has the\nlowest Euclidean distance among the candidates is selected.\nExamples of matched features are shown in Fig. 2.\nThe matching step results in a set of feature pairs Pa,b with\na total number Ma,b of matched pairs. Since the number of\nIEEE TRANSACTION ON ROBOTICS, SPECIAL ISSUE ON VISUAL SLAM 3\nDistance traveled (m)\nSi\nm\nila\nrit\ny \nm\nea\nsu\nre\n \u2212\n S\n 0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n\u22124 \u22123 \u22122 \u22121  0  1  2  3  4\nDistance traveled (m)\nSi\nm\nila\nrit\ny \nm\nea\nsu\nre\n \u2212\n S\n 0\n 0.05\n 0.1\n 0.15\n 0.2\n 0.25\n 0.3\n 0.35\n 0.4\n\u22124 \u22123 \u22122 \u22121  0  1  2  3  4\nFig. 2. Examples of loop closure detection outdoors (top) and indoors (bottom). In the outdoor example the distance to the extracted features is larger than in\nthe indoor example. Left: feature matches at the peak of the similarity value, S678,758 = 0.728 (top) and S7,360 = 0.322 (bottom). Middle: feature matches\ntwo steps (equivalent to \u223c3 meters distance) away, S680,758 = 0.286 (top) and S9,360 = 0.076 (bottom). The pose standard deviation \u03c3xrv = \u03c3yrv was\nestimated as 2.06 m (top) and 1.09 m (bottom), respectively, and the mean d\u00b5 as 0.199 m (top) and -0.534 m (bottom). Right: evolution of the similarity\nmeasure S against the distance travelled (obtained from odometry) together with the fitted Gaussian.\nfeatures varies heavily depending on the image content, the\nnumber of matches is normalized to Sa,b \u2208 [0, 1] as\nSa,b =\nMa,b\n1\n2 (nFa + nFb)\n(2)\nwhere nFa and nFb are the number of features in Fa and Fb\nrespectively. A high similarity measure indicates a perceptually\nsimilar position.\n2) Estimation of the Relative Rotation and Variance: The\nrelative rotation between two panoramic images Ia and Ib\ncan be estimated directly from the horizontal displacement of\nthe matched feature pairs Pa,b. If the flat floor assumption is\nviolated this will be only an approximation. Here, the relative\nrotations \u03b8p for all matched pairs p \u2208 Pa,b are sorted into\na 10 bin histogram and the relative rotation estimate \u00b5rv\u03b8 is\ndetermined as the maximum of a parabola fitted to the largest\nbin and its left and right neighbour, see Fig. 3.\nTo evaluate the accuracy of relative rotation estimates \u03b8p,\nwe collected panoramic images in an indoor laboratory envi-\nronment and computed the relative orientation with respect\n 0\n 2\n 4\n 6\n 8\n 10\n 12\n 14\n 16\n\u22123 \u22122 \u22121  0  1  2  3\nN\num\nbe\nr o\nf m\nat\nch\nes\nRelative orientation (rad)\nRelative rotation histogram\nbins\nselected angle\nfitted polynom\nFig. 3. Relative orientation histogram from two omnidirectional images taken\n2 meters apart. The dotted line marks the relative orientation estimate \u00b5rv\n\u03b8\n.\nto a reference image I0. Panoramic images were recorded\nat a translational distance of 0.5, 1.0 and 2.0 meters to the\nreference image I0. The ground truth rotation was obtained by\nmanually measuring the displacement of corresponding pixels\nin areas along the displacement of the camera. The results in\nTable I demonstrate the good accuracy obtained. Even at a\ndisplacement of 2 meters the mean error is only 7.15 degrees.\nTABLE I\nERRORS OF RELATIVE ROTATION \u03b8 ESTIMATE IN RADIANS.\ntransl (m) error\u03b8 \u03c3error\u03b8\n0.5 0.100 0.0630\n1.0 0.104 0.0500\n2.0 0.125 0.0903\nThe rotation variance \u03c32\u03b8rv is estimated by the sum of\nsquared differences between the estimate of the relative ro-\ntation \u00b5rv\u03b8 and the relative rotation of the matched pairs Pa,b.\n\u03c32\u03b8rv =\n1\nMa,b \u2212 1\n\u2211\np\u2208Pa,b\n(\u00b5rv\u03b8 \u2212 \u03b8p)\n2 (3)\nTo increase the robustness towards outliers, a 10% Winsorized\nmean is applied. For the evalutated data this had only a minor\neffect on the results compared to using an un-truncated mean.\n3) Estimation of Relative Position and Covariance: The\nMini-SLAM approach does not attempt to determine the\nposition of the detected features. Therefore, the relative po-\nsition between two frames a and b cannot be determined\nvery accurately. Instead we use only image similarity of the\nsurrounding images to estimate [\u00b5rvx , \u00b5rvy ] as described below.\nIt would be possible to estimate the relative position using\nmultiple view geometry but this would introduce additional\ncomplexity that we want to avoid.\nIEEE TRANSACTION ON ROBOTICS, SPECIAL ISSUE ON VISUAL SLAM 4\nFig. 4. Left: The physical distance to the features will influence the number\nof features that can be identified from different poses of the robot. The filled\nsquares represent features that could be matched in all three robot poses while\nthe unfilled squares represent the features for which correspondences could\nnot be found from all poses. The left wall in the figure is closer to the robot.\nThus, due to the faster change in appearance, the number of features of the\nleft wall, which can be matched over successive images, tends to be less\ncompared to the number of matched features of the right wall. Right: Outdoor\nrobot used in this paper, equipped with a Canon EOS 350D camera and a\npanoramic lens from 0-360.com, which were used to collect the data, a DGPS\nunit to determine ground truth positions, and an LMS SICK scanner used for\nvisualization and for obtaining ground truth.\nInstead, geometric information is obtained from an estimate\nof the covariance of the relative position between a current\nframe b and a previously recorded frame a. This covariance\nestimate is computed using only the similarity measures S of\nframe b with a and the neighbouring frames of a.\nThe number of matched features between successive frames\nwill vary depending on the physical distance to the features,\nsee Figs. 2 and 4. Consider, for example, a robot located in\nan empty car park where the physical distance to the features\nis large and therefore the appearance of the environment does\nnot change quickly if the robot is moved a certain distance.\nIf, on the other hand, the robot is located in a narrow corridor\nwhere the physical distance to the extracted features is small,\nthe number of feature matches in successive frames tends to\nbe smaller if the robot was moved the same distance.\nThe covariance of the robot pose estimate [x,y]\nCrv =\n[\n\u03c32xrv \u03c3xrv\u03c3yrv\n\u03c3xrv\u03c3yrv \u03c3\n2\nyrv\n]\n(4)\nis computed based on how the similarity measure varies over\nthe set N(a), which contains frame a and its neighbouring\nframes. The analyzed sequence of similarity measures is\nindicated in the zoomed in visualization of a similarity matrix\nshown in Fig. 5. In order to avoid issues estimating the\ncovariance orthogonal to the path of the robot if the robot was\ndriven along a straight path, the covariance matrix is simplified\nby setting \u03c32xrv = \u03c32yrv and \u03c3xrv\u03c3yrv = 0. The remaining\ncovariance parameter is estimated by fitting a 1D Gaussian\nto the similarity measures SN(a),b and the distance travelled\nas obtained from odometry, see Fig. 6. Two parameters are\ndetermined from the nonlinear least squares fitting process:\nmean (d\u00b5) and variance (\u03c32[x,y]rv ). The initial estimate of the\nrelative position [\u00b5rvx , \u00b5rvy ] of a visual relation is calculated as\n\u00b5rvx = cos(\u00b5\nrv\n\u03b8 )d\u00b5 (5)\n\u00b5rvy = sin(\u00b5\nrv\n\u03b8 )d\u00b5, (6)\nFig. 5. Left: Full similarity matrix for the lab data set. Brighter entries\nindicate a higher similarity measure S. Right: Zoomed in image. The left area\n(enclosed in a blue frame) corresponds to a sequence of similarity measures\nthat gives a larger position covariance than the right sequence (red frame).\nwhere d\u00b5 is the calculated mean of the fitted Gaussian and \u00b5\u03b8\nthe estimated relative orientation (Sec. II-C2).\nIn the experimental evaluation, the Gaussian was estimated\nusing 5 consecutive frames. To evaluate whether the evolution\nof the similarity measure in the vicinity of a visual relation\ncan be reasonably approximated by a Gaussian, the mean error\nbetween the 5 similarity measures and the fitted Gaussian\nwas calculated for the outdoor\/indoor data set (the data set\nis described in Sec. III-A). The results in Table II indicate\nthat the Gaussian represents the evolution of the similarity in\na reasonable way. Please note that frame b is recorded at a\nlater time than frame a meaning that the covariance estimate\nCa,brv can be calculated directly without any time lag.\n4) Selecting Frames to Match: In order to speed up the\nalgorithm and make it more robust to perceptual aliasing (the\nproblem that different regions have similar appearance), only\nthose frames are selected for matching that are likely to be\na\u22122\na\u22121\na+1 a+2\nb\na\nSa\u22122,b\nSa\u22121,b\nSa,b Sa+1,b\nSa+2,b\n\u00b5d\nS\nda+1,a+2d da\u22121,a a,a+1da\u22122,a\u22121\nFig. 6. Gaussian fitted to the distance travelled d (as obtained from\nodometry) and the similarity measures between frame b and the frames of the\nneighbourhood N(a) = {a\u2212 2, a\u2212 1, a, a + 1, a + 2}. From the similarity\nmeasures, both a relative pose estimate \u00b5rv and a covariance estimate Crv\nare calculated between node a and node b. The orientation and orientation\nvariance are not visualized in this figure.\nIEEE TRANSACTION ON ROBOTICS, SPECIAL ISSUE ON VISUAL SLAM 5\nTABLE II\nSTATISTICS OF THE ERROR \u01eb BETWEEN THE GAUSSIAN FIT AND THE\nSIMILARITY MEASURES Sa\u22122,b, ..., Sa+2,b FOR EACH NODE FOR WHICH\nTHE FIT WAS PERFORMED IN THE OUTDOOR\/INDOOR DATA SET.\nnode pair \u01eb \u03c3\u01eb\n< a\u2212 2, b > 0.031 0.0441\n< a\u2212 1, b > 0.029 0.0348\n< a, b > 0.033 0.0601\n< a + 1, b > 0.026 0.0317\n< a + 2, b > 0.028 0.0388\n 0\n 200\n 400\n 600\n 800\n 1000\n 0  100  200  300  400  500  600  700  800  900 1000\n# \nof\n si\nm\nila\nriy\n c\nal\ncu\nla\ntio\nns\nFrame index\n# of similarity calculations at each frame\nFig. 7. Number of similarity calculations performed at each frame in the\noutdoor\/indoor data set. The first frames were compared around frame 240,\nsince up to then none of the previous frames were within the search area\naround the current pose estimate defined by the estimated pose covariance.\nThe diagonal line indicates the linear increase for the case that the frames to\nmatch are not pre-selected.\nlocated close to each other.\nConsider the current frame b and a previously recorded\nframe a. If the similarity measure was to be calculated between\nb and all previously added frames, the number of frames to be\ncompared would increase linearly, see Fig. 7. Instead, frames\nare only compared if the current frame b is within a search\narea around the pose estimate of frame a. The size of this\nsearch area is computed from the estimated pose covariance.\nFrom the MLR algorithm (see Section II-A) we obtain\nthe maximum likelihood estimate x\u02c6b for frame b. There is,\nhowever, no estimate of the corresponding covariance Cx\u02c6 that\ncould be used to distinguish whether frame a is likely to be\nclose enough to frame b so that it can be considered a candidate\nfor a match, i.e. a frame for which the similarity measure\nSa,b should be calculated. So far, we have defined two types\nof covariances: the odometry covariance Cro and the visual\nrelation covariance Crv . To obtain an overall estimate of the\nrelative covariance between frame a and b we first consider\nthe covariances of the odometry relations ro between a and b\nand compute relative covariance Cxo\na,b\nas\nCxo\na,b\n=\n\u2211\nj\u2208(a,b\u22121)\nRjCroj R\nT\nj . (7)\nRj is a rotation matrix, which is defined as\nRj =\n\uf8eb\n\uf8ed cos(x\u02c6\n\u03b8\nj+1 \u2212 x\u02c6\n\u03b8\nj ) \u2212sin(x\u02c6\n\u03b8\nj+1 \u2212 x\u02c6\n\u03b8\nj ) 0\nsin(x\u02c6\u03b8j+1 \u2212 x\u02c6\n\u03b8\nj ) cos(x\u02c6\n\u03b8\nj+1 \u2212 x\u02c6\n\u03b8\nj ) 0\n0 0 1\n\uf8f6\n\uf8f8 , (8)\nwhere x\u02c6\u03b8j is the orientation estimated for frame j.\nAs long as no visual relation rv has been added, either\nbetween a and b or any of the frames between a and b,\nthe relative covariance Cx\u02c6a,b can be determined directly from\nthe odometry covariance Cxoa and Cxob as described above.\nHowever, when a visual relation ra,bv between a and b is\nadded, the covariance of the estimate Cx\u02c6b decreases. Using\nthe covariance intersection method [18], the covariance for\nframe b is therefore updated as\nCx\u02c6b = Cx\u02c6b \u2295 (Cx\u02c6a + Cra,bv ), (9)\nwhere \u2295 is the covariance intersection operator. The co-\nvariance intersection method weighs the influence of both\ncovariances Ca and Cb as\nCA \u2295 CB = [\u03c9C\n\u22121\nA + (1\u2212 \u03c9)C\n\u22121\nB ]\n\u22121. (10)\nThe parameter \u03c9 \u2208 [0, 1] is chosen so that the determinant of\nthe resulting covariance is minimized [19].\nThe new covariance estimate is also used to update the\nframes between a and b by adding the odometry covariances\nCxo\na..b\nin opposite order (i.e. simulate that the robot is moving\nbackwards from frame b to a). The new covariance estimate\nfor frame j \u2208 (a, b) is calculated as\nCx\u02c6j = Cx\u02c6j \u2295 (Cx\u02c6b + Cxob,j ). (11)\n5) Visual Relation Filtering: To avoid adding visual re-\nlations with low similarity, visual similarity relations ra,bv\nbetween frame a and frame b are only added if the similarity\nmeasure exceeds a threshold tvs: Sa,b > tvs. In addition,\nsimilarity relations are only added if the similarity value Sa,b\nhas its peak at frame a (compared to the neighbouring frames\nN(a)). There is no limitation on the number of visual relations\nthat can be added for each frame.\nD. Fusing Multiple Data Sets\nFusion of multiple data sets recorded at different times is\nrelated to the problem of multi-robot mapping where each of\nthe data sets is collected concurrently with a different robot.\nThe motivation for multi-robot mapping is not only to reduce\nthe time required to explore an environment but also to merge\nthe different sensor readings in order to obtain a more accurate\nmap. The problem addressed here is equivalent to \u201cmulti-robot\nSLAM with unknown initial poses\u201d [20] because the relative\nposes between the data sets are not given. The exploration\nproblem is not considered in this paper.\nOnly a minor modification of the standard method described\nabove is necessary to address the problem of fusing multiple\ndata sets. The absence of relative pose estimates between\nthe data sets is compensated for by not limiting the search\nregion for which similarity measures S are computed. This is\nimplemented by incrementally adding data sets and setting the\nrelative pose between consecutively added data sets initially\nto (0,0,0) with an infinite pose covariance. Such odometry\nrelations between data sets appear as long, diagonal lines in\nFig. 16 representing the transition between lab to studarea\nand studarea to lab\u2212 studarea.\nIEEE TRANSACTION ON ROBOTICS, SPECIAL ISSUE ON VISUAL SLAM 6\nIII. EXPERIMENTAL RESULTS\nIn this section, we present results from five different data\nsets with varying properties. An overview of all data sets is\npresented in Table III. All data sets were collected with our\nmobile robot Tjorven, see Fig. 4. The platform uses \u201cskid-\nsteering\u201d, which is prone to bad odometry. In the different\ndata sets different wheel types (indoor \/ outdoor) were used.\nThe robot\u2019s odometry was calibrated (for each wheel type) by\nfirst driving forward 5 meters to obtain a distance per encoder\ntick value, and second by completing one full revolution to\ndetermine the number of differential encoder ticks per angular\nrotation. Finally the drift parameter was adjusted so that the\nrobot would drive forward in a straight line, i.e. to compensate\nfor the slightly different size of the wheel pairs.\nThe omni-directional images were first converted to\npanoramic images with a resolution of 1000 x 289. When\nextracting SIFT features the initial doubling of the images was\nnot performed, i.e. SIFT features from the first octave were\nignored, simply to lower the amount of extracted features.\nThe results are presented both visually with maps obtained\nby superimposing laser range data using the poses estimated\nwith Mini-SLAM and quantitatively by the mean squared\nerror (MSE) from ground truth data. Since the corresponding\npose pairs < x\u02c6i, xGTi > between the estimated pose x\u02c6i\nand the corresponding ground truth pose xGTi are known,\nthe optimal rigid transformation between pose estimates and\nground truth data can be determined directly. We applied the\nmethod suggested by Arun et al. [21].\nTo investigate the influence of the threshold tvs, described\nin Section II-C5, the MSE was calculated for all data sets for\nwhich ground truth data were available. The result in Fig. 8\nshows that the value of the threshold tvs can be selected so that\nit is nearly optimal for all data sets and that there is a region\nin which minor changes of the tvs do not strongly influence\nthe accuracy of the map. Throughout the remainder of this\nsection a constant threshold tvs = 0.2 is used .\nIn order to give a better idea of the function of the Mini-\nSLAM algorithm, the number of visual relations per node\ndepending on the threshold tvs is shown in Fig. 9. The\noverview of all data sets presented in Table III also contains the\nnumber of similarity calculations performed and the evaluation\nrun time on a Pentium 4 (2GHz) processor with 512 MB of\nRAM memory. This time does not include the time required\nfor the similarity computation. Each similarity calculation\n(including relative rotation and variance estimation) took 0.30\nTABLE III\nFOR EACH DATA SET: NUMBER OF NODES #x\u02c6, VISUAL RELATIONS #rv ,\nPERFORMED SIMILARITY CALCULATIONS #S , AVERAGE NUMBER OF\nEXTRACTED VISUAL FEATURES \u00b5F PER NODE WITH VARIANCE \u03c3F ,\nEVALUATION RUN TIME T (EXCLUDING THE SIMILARITY COMPUTATION).\n#x\u02c6 #rv #S \u00b5F \u03c3F T (s)\noutdoor \/ indoor 945 113 24784 497.5 170.0 66.4\nmultiple floor levels 409 198 13764 337.9 146.7 21.0\nlab 86 60 443 571.5 39.6 3.6\nstudarea 134 31 827 426.6 51.1 9.4\nlab\u2212 studarea 86 10 101 459.8 125.8 3.8\nvs\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 0  0.2  0.4  0.6  0.8  1\nM\nSE\n S\nLA\nM\n \/ \nM\nSE\n o\ndo\nm\net\nry\nThreshold \u2212 t\noutdoor \/ indoor\nlab\nstudarea\nlab\u2212studarea\nFig. 8. The influence of the threshold parameter tvs on the relative MSE.\nseconds using a data set with an average of 522.3 features\nwith standard deviation of 21.4. Please note, however, that the\nimplementation used for feature matching in this paper was\nnot optimised for computational efficiency.\nA. Outdoor \/ indoor data set\nA large set of 945 omni-directional images was collected\nover a total distance of 1.4 kilometers with height differences\nof up to 3 meters. The robot was driven manually and the data\nwere collected in both indoor and outdoor areas over a period\nof 2 days (due to the limited capacity of the camera battery).\n1) Comparison to ground truth obtained from DGPS: To\nevaluate the accuracy of the created map, the robot position\nwas measured with differential GPS (DGPS) while collecting\nthe omni-directional images. Thus, for every SLAM pose esti-\nmate there is a corresponding DGPS position < x\u02c6i, xDGPSi >.\nDGPS gives a smaller position error than GPS. However,\nsince only the signal noise is corrected, the problem with\nmultipath reflections still remains. DGPS is also only avail-\nable if the radio link between the robot and the station-\nary GPS is functional. Thus, only a subset of pose pairs\n< x\u02c6i, x\nDGPS\ni >i=1..N can be used for ground truth evaluation.\nDGPS measurements were considered only when at least five\nsatellites were visible and the radio link to the stationary GPS\nwas functional. The valid DGPS readings are indicated as\nlight (blue) dots in Fig. 10. The total number of pairs used\nto calculate the MSE for the whole map was 377 compared to\nthe total number of frames of 945. To measure the difference\nbetween the poses estimated with Mini-SLAM x\u02c6 and the\nDGPS positions xDGPS (using UTM WGS84, which provides\na metric coordinate system), the two data sets have to be\naligned. Since the correspondence of the filtered pose pairs\nvs\n 0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7\n# \nvi\nsu\nal\n re\nla\ntio\nns\n \/ \n# \nno\nde\ns\nThreshold \u2212 t\noutdoor \/ indoor\nlab\nstudarea\nlab\u2212studarea\nFig. 9. The amount of visual nodes added to the graph depending on the\nthreshold tvs.\nIEEE TRANSACTION ON ROBOTICS, SPECIAL ISSUE ON VISUAL SLAM 7\nFig. 10. DGPS data xDGPS with aligned SLAM estimates x\u02c6 displayed on\nan aerial image of the area. The darker (red) squares show the Mini-SLAM\npose estimates and the lighter (blue) squares show the DGPS poses for which\nthe number of satellites was considered acceptable. The deviation seen at the\nbottom (the car park) is mainly caused by the fact that the car park is elevated\ncompared to the rest of the environment.\n 0\n 50\n 100\n 150\n 200\n 0  100  200  300  400  500  600  700  800  900 1000\nM\nSE\n p\nos\ne \ndi\nsta\nnc\ne \ner\nro\nr\nFrame index\nMSE of the SLAM pose estimates and odometry to DGPS\nSLAM\nodometry\nFig. 11. Evolution of the MSE between the ground truth position obtained\nfrom DGPS readings xDGPS and the Mini-SLAM estimate of the robot\npose x\u02c6 as frames are added to the map. Drops in the MSE indicate that\nthe consistency of the map has been increased. The final MSE of the raw\nodometry was 377.5 m2.\nis known, < x\u02c6i, xDGPSi >, an optimal rigid alignment can be\ndetermined directly with the method by Arun et al. [21] as\ndescribed above.\nThe mean square error (MSE) between xDGPS and x\u02c6 for the\ndata set shown in Fig. 10 is 4.89 meters. To see how it evolves\nover time when creating the map, the MSE was calculated\nfrom the new estimates x\u02c6 after each new frame was added.\nThe result is shown in Fig. 11 and compared to the MSE\nobtained using only odometry to estimate the robot\u2019s position.\nPlease note that the MSE was evaluated for each frame added.\nTherefore, when DGPS data are not available, the odometry\nMSE xo will stay constant for these frames. This can be seen,\nfor example, for the frames 250\u2212440 in Fig. 11. For the same\nframes, the MSE of the SLAM estimate x\u02c6 is not constant since\nnew estimates are computed for each frame added and loop\nclosing also occurs indoors or generally when no DGPS is\navailable. The first visual relation rv was added around frame\n260. Until then, the error of the Mini-SLAM estimate x\u02c6 and\nthe odometry MSE xo were the same.\nB. Multiple floor levels\nThis data set was collected inside a department building\nat \u00a8Orebro University. It includes all (five) floor levels and\nconnections between the floor levels by three elevators. The\ndata contain loops in 2-d coordinates and also involving\ndifferent floor levels. This data set consistst of 419 panoramic\nimages and covers a path with a length of 618 meters. The\ngeometrical layout differs for the different floors, see Fig. 13.\nNo information about the floor level is used as an input to the\nsystem, hence the robot pose is still described using (x, y, \u03b8).\n1) Visualized results: There are no ground truth data avail-\nable for this data set. It is possible, however, to get a visual\nimpression of the accuracy of the results from Fig. 12. The\nfigure shows occupancy grid maps obtained from laser scanner\nreadings and raw odometry poses (left), or the Mini-SLAM\npose estimates (right), respectively. All floors are drawn on\ntop of each other without any alignment. To further illustrate\nthe Mini-SLAM results, an occupancy map was also created\nseparately for each floor from the laser scanner readings and\nMini-SLAM pose estimates, see Fig. 13. Here, each pose was\nassigned to the corresponding floor level manually.\nThis experiment mainly illustrates the robustness of data\nassociation that is achieved using omni-directional vision data.\nThe similarity matrix and a similarity access matrix for the\n\u201cMultiple floor levels\u201d data set are shown in Fig. 14.\nC. Partly overlapping data\nThis data set consists of three separate indoor sets: lab\n(lab), student area (studarea) and a combination of both\n(lab\u2212studarea), see Fig. 15. Similar to the data set described\nin Sec. III-B, omni-directional images, 2D laser range data\nand odometry were recorded. Ground truth poses xGT were\ndetermined using the laser scanner and odometry together with\nthe MLR approach as in [2].\n1) Visualized results: Fig. 16 shows the final graph (left), a\nplot of laser scanner readings merged using poses from odom-\netry (middle) and poses obtained with Mini-SLAM (right).\nFig. 17 shows the similarity matrix and the similarity access\nmatrix for the lab\u2212 studarea data set.\n2) Comparison to ground truth obtained from laser based\nSLAM: As described in Sec. II-D, fusion of multiple maps is\nmotivated both by its need in multi-robot mapping and by the\nincreased accuracy of the resulting maps. Instead of simply\nFig. 12. Occupancy grid map of all five floors drawn on top of each other.\nLeft: Gridmap created using pose information from raw odometry. Right:\nUsing the estimated robot poses from Mini-SLAM.\nIEEE TRANSACTION ON ROBOTICS, SPECIAL ISSUE ON VISUAL SLAM 8\nFig. 13. Occupancy maps for floor levels 1-5, computed using laser scanner data at each estimated pose. The assignment of initial poses to floor levels was\ndone manually and is only used to visualize these maps.\nFig. 16. Left: A part of the final MLR graph containing the three different data sets. Middle: Laser range scanning based map using the raw odometry.\nRight: Laser range scanning based map using the Mini-SLAM poses.\nFig. 14. Left: Pose similarity matrix for the \u201cMultiple floor levels\u201d data set.\nRight: Similarity access matrix showing which similarity measures were used\nin the Mini-SLAM computation. Brighter pixels were used more often.\nadding the different maps onto each other, the fused maps\nalso use additional information from the overlapping parts to\nimprove the accuracy of the sub-maps. This is illustrated in Ta-\nble IV which shows the MSE (again obtained by determining\nthe rigid alignment between x\u02c6 and xGT ) before and after the\nfusion was performed. While the data sets lab and studarea\nshows a negligible change in accuracy, lab\u2212studarea clearly\ndemonstrate a large improvement.\nFig. 15. Sub-maps for the partly overlapping data. Left: lab. Middle:\nstudarea. Right: lab\u2212 studarea, overlapping both lab and studarea.\n3) Robustness evaluation: The suggested method relies on\nincremental pose estimates (odometry) and a visual similarity\nTABLE IV\nMSE RESULTS BEFORE AND AFTER MERGING OF THE DATA SETS AND\nUSING ODOMETRY ONLY.\nlab studarea lab\u2212 studarea\nbefore fusion 0.002 0.029 0.036\nafter fusion 0.002 0.029 0.013\nraw odometry 0.065 0.481 1.296\nIEEE TRANSACTION ON ROBOTICS, SPECIAL ISSUE ON VISUAL SLAM 9\nFig. 17. Left: Pose similarity matrix for the lab\u2212studarea data set. Right:\nSimilarity access matrix showing which similarity measures are used in the\nproposed method. Brighter pixels were used more often.\nmeasure S. The robustness of the method is evaluated by\ncorrupting these two inputs and evaluating the performance.\nFor this evaluation, the studarea data set is used and the tests\nwere repeated 10 times.\nIn the first test, the similarity measures S were corrupted\nby adding a random value drawn from a Gaussian distribution\nN (0, \u03c3) with varying standard deviation \u03c3, see Table V. The\namount of added noise has to be compared to the range of\n[0, 1] in which the similarity measure S lies, see Eq. 2.\nThe robustness evaluation with respect to the similarity\nmeasure S shows that the system can handle additional noise\nto some extent, but incorrect visual relations will affect the\naccuracy of the final map. This illustrates that the proposed\nmethod, as many others, would have difficulties in perceptually\nsimilar locations in case the uncertainty of the pose estimates\nCx\u02c6 is high.\nIn the second test, the odometry values were corrupted by\nadding additional noise to the incremental distance d and\nthe orientation \u03b8. The corrupted incremental distance d\u2032 is\ncalculated as\nd\u2032 = d + 0.1dN (0, \u03c3) + 0.2\u03b8N (0, \u03c3), (12)\nand the orientation \u03b8\u2032 as\n\u03b8\u2032 = \u03b8 + 0.2dN (0, \u03c3) + \u03b8N (0, \u03c3). (13)\nSince the odometry pose estimates are computed incrementally\nthe whole later trajectory is affected when adding noise at a\nparticular time step.\nThe results of the robustness evaluation with the corrupted\nodometry are shown in Fig. 18 together with the MSE of\nthe corrupted odometry. These results show that the system is\nrobust to substantial odometry errors. A failure case is shown\nin Fig. 19.\nTABLE V\nMSE RESULTS (mean AND stddev) AFTER ADDING A RANDOM VARIABLE\nDRAWN FROM N(0, \u03c3) TO EACH SIMILARITY MEASURE Sa,b .\n\u03c3 mean stddev\n0.02 0.03 0.004\n0.05 0.03 0.011\n0.10 0.11 0.074\n0.20 0.94 0.992\n0.40 1.35 1.304\n0.80 1.49 1.240\nIV. CONCLUSIONS AND FUTURE WORK\nMini-SLAM combines the principle of using similarity of\npanoramic images to close loops at the topological level with\na graph relaxation method to obtain a metrically accurate\nmap representation and with a novel method to determine the\ncovariance for visual relations based on visual similarity of\nneighbouring poses. The proposed method uses visual similar-\nity to compensate for the lack of range information about local\nimage features, avoiding computationally expensive and less\ngeneral methods such as tracking of individual image features.\nExperimentally, the method scales well to the investigated\nenvironments. The experimental results are presented by visual\nmeans (as occupancy maps rendered from laser scans and\nposes determined by the Mini-SLAM algorithm) and by com-\nparison with ground truth (obtained from DGPS outdoors or\nlaser-based SLAM indoors). The results demonstrate that the\nMini-SLAM method is able to produce topologically correct\nand geometrically accurate maps at low computational cost.\nA simple extension of the method was used to fuse multiple\ndata sets so as to obtain improved accuracy. The method has\nalso been used without any modifications to successfully map\na building consisting of 5 floor levels.\nMini-SLAM generates a 2-d map based on 2-d input from\nodometry. It is worth noting that the \u201coutdoor \/ indoor\u201d data set\nincludes variations of up to 3 meters in height. This indicates\nthat the Mini-SLAM can cope with violations of the flat\nfloor assumption to a certain extent. We expect a graceful\ndegradation in map accuracy as the roughness of the terrain\nincreases. The representation should still be useful for self-\nlocalization using 2-d odometry and image similarity, e.g.,\nusing the global localization method in [1], which in addition\ncould be used to improve the robustness towards perceptual\naliasing when fusing multiple data sets. In extreme cases, of\ncourse, it is possible that the method would create inconsistent\nmaps, and a 3-d representation should be considered.\nThe bottleneck of the current implementation in terms\nof computation time is the calculation of image similarity,\nwhich involves the comparison of many local features. The\nsuggested approach, however, is not limited to the particular\nmeasure of image similarity used in this work. There are\n\u03c3\nraw odometry\nSLAM const cov. odom.\nSLAM inc cov. odom.\n\u221240\n\u221220\n 0\n 20\n 40\n 60\n 80\n 100\n 0  0.5  1  1.5  2  2.5\nM\nSE\n (m\n2 )\nAdded noise \u2212\nFig. 18. MSE results (mean and stddev) for x (odometry) and x\u02c6 (estimated\nposes) after corrupting the odometry by adding random values drawn from\nN(0, \u03c3). The plot also shows the MSE when the odometry covariance is\nincreased with the added noise.\nIEEE TRANSACTION ON ROBOTICS, SPECIAL ISSUE ON VISUAL SLAM 10\nwalls\ncorresponding\nFig. 19. A failure case where the corrupted odometry error became too large\nresulting in a corrupted map. Left: SLAM map. Right: raw odometry.\nmany possibilities to increase the computation speed either by\nusing alternative similarity measures that are faster to compute\nwhile still being distinctive enough, or by optimizing the\nimplementation, for example, by executing image comparisons\non a graphics processing unit (GPU) [22].\nFurther plans for future work include an investigation of\nthe possibility of using a standard camera instead of an\nomni-directional camera, and incorporation of vision-based\nodometry to realise a completely vision-based system.\nREFERENCES\n[1] H. Andreasson, A. Treptow, and T. Duckett, \u201cLocalization for mobile\nrobots using panoramic vision, local features and particle filter,\u201d in Proc.\nIEEE Int. Conf. Robotics and Automation (ICRA), 2005, pp. 3348\u20133353.\n[2] U. Frese, P. Larsson, and T. Duckett, \u201cA multilevel relaxation algorithm\nfor simultaneous localisation and mapping,\u201d IEEE Transactions on\nRobotics, vol. 21, no. 2, pp. 196\u2013207, April 2005.\n[3] H. Andreasson, T. Duckett, and A. Lilienthal, \u201cMini-SLAM: Mini-\nmalistic visual SLAM in large-scale environments based on a new\ninterpretation of image similarity,\u201d in IEEE International Conference\non Robotics and Automation (ICRA 2007), Rome, Italy, 2007.\n[4] D. Lowe, \u201cObject recognition from local scale-invariant features,\u201d in\nProc. Int. Conf. Computer Vision ICCV, Corfu, 1999, pp. 1150\u20131157.\n[5] S. Se, D. Lowe, and J. Little, \u201cMobile robot localization and mapping\nwith uncertainty using scale-invariant visual landmarks,\u201d International\nJournal of Robotics Research, vol. 21, no. 8, pp. 735\u2013758, 2002.\n[6] T. Barfoot, \u201cOnline visual motion estimation using FastSLAM with SIFT\nfeatures,\u201d in Proc. of the IEEE\/RSJ Int. Conf. on Intelligent Robots and\nSystems (IROS), 2005, pp. 579\u2013585.\n[7] P. Jensfelt, D. Kragic, J. Folkesson, and M. Bjo\u00a8rkman, \u201cA framework for\nvision based bearing only 3D SLAM,\u201d in Proc. IEEE Int. Conf. Robotics\nand Automation (ICRA), 2006, pp. 1944\u20131950.\n[8] N. Karlsson, E. D. Bernardo, J. Ostrowski, L. Goncalves, P. Pirjanian,\nand M. E. Munich, \u201cThe vSLAM algorithm for robust localization and\nmapping,\u201d in Proc. IEEE Int. Conf. Robotics and Automation (ICRA),\n2005, pp. 24\u201329.\n[9] P. Elinas, R. Sim, and J. Little, \u201c\u03c3SLAM: Stereo vision SLAM using\nthe Rao-Blackwellised particle filter and a novel mixture proposal\ndistribution,\u201d in Proc. IEEE Int. Conf. Robotics and Automation (ICRA),\n2006, pp. 1564\u20131570.\n[10] J. Sez and F. Escolano, \u201c6dof entropy minimization slam,\u201d in Proc. IEEE\nInt. Conf. Robotics and Automation (ICRA), 2006, pp. 1548\u20131555.\n[11] A. Davison, \u201cReal-time simultaneous localisation and mapping with a\nsingle camera,\u201d in Proceedings of the IEEE International Conference on\nComputer Vision (ICCV\u201903), 2003, pp. 1403\u20131410.\n[12] K. L. Ho and P. Newman, \u201cLoop closure detection in SLAM by\ncombining visual and spatial appearance,\u201d Robotics and Autonomous\nSystem, vol. 54, no. 9, pp. 740\u2013749, September 2006.\n[13] P. M. Newman, D. M. Cole, and K. L. Ho, \u201cOutdoor SLAM using visual\nappearance and laser ranging,\u201d in Proc. IEEE Int. Conf. Robotics and\nAutomation (ICRA), 2006, pp. 1180\u20131187.\n[14] G. Grisetti, D. Lordi Rizzini, C. Stachniss, E. Olson, and W. Burgard,\n\u201cOnline constraint network optimization for efficient maximum likeli-\nhood map learning,\u201d Pasadena, CA, USA, 2008.\n[15] E. Olson, J. Leonard, and S. Teller, \u201cFast iterative optimization of pose\ngraphs with poor initial estimates,\u201d 2006, pp. 2262\u20132269.\n[16] A. I. Eliazar and R. Parr, \u201cLearning probabilistic motion models for\nmobile robots,\u201d in Proc. of the twenty-first Int. Conf. on Machine\nlearning (ICML), 2004, p. 32.\n[17] J. Gonzalez-Barbosa and S. Lacroix, \u201cRover localization in natural\nenvironments by indexing panoramic images,\u201d in Proceedings of the\nInternational Conference on Robotics and Automation (ICRA). IEEE,\n2002, pp. 1365\u20131370.\n[18] J. Uhlmann, \u201cDynamic map building and localization for autonomous\nvehicles,\u201d Ph.D. dissertation, University of Oxford, 1995.\n[19] S. J. Julier and J. K. Uhlmann, \u201cUsing covariance intersection for slam,\u201d\nRobotics and Autonomous Systems, vol. 55, no. 1, pp. 3\u201320, 2007.\n[20] A. Howard, \u201cMulti-robot simultaneous localization and mapping using\nparticle filters,\u201d in Proceedings of Robotics: Science and Systems,\nCambridge, USA, June 2005.\n[21] K. S. Arun, T. S. Huang, and S. D. Blostein, \u201cLeast-squares fitting of\ntwo 3-d point sets,\u201d IEEE Trans. Pattern Anal. Mach. Intell., vol. 9,\nno. 5, pp. 698\u2013700, 1987.\n[22] S. N. Sinha, J.-M. Frahm, M. Pollefeys, and Y. Genc, \u201cGPU-based video\nfeature tracking and matching,\u201d in Workshop on Edge Computing Using\nNew Commodity Architectures (EDGE 2006), Chapel Hill, 2006.\nHenrik Andreasson is a Ph.D. student at Centre\nfor Applied Autonomous Sensor System, \u00a8Orebro\nUniversity, Sweden. He received his Master degree\nin Mechatronics from Royal Institute of Technology,\nSweden, in 2001. His research interests include mo-\nbile robotics, computer vision, and machine learn-\ning.\nTom Duckett is a Reader at the Department of\nComputing and Informatics, University of Lincoln,\nwhere he is also Director of the Centre for Vision\nand Robotics Research. He was formerly a docent\n(Associate Professor) at \u00a8Orebro University, where he\nwas also leader of the Learning Systems Laboratory\nwithin the Centre for Applied Autonomous Sensor\nSystems. He obtained his Ph.D. from Manchester\nUniversity, M.Sc. with distinction from Heriot-Watt\nUniversity and B.Sc. (Hons.) from Warwick Univer-\nsity, and has also studied at Karlsruhe and Bremen\nUniversities. His research interests include mobile robotics, navigation, ma-\nchine learning, AI, computer vision, and sensor fusion for perception-based\ncontrol of autonomous systems.\nAchim Lilienthal is a docent (associate professor) at\nthe AASS Research Center, \u00a8Orebro, Sweden where\nhe is leading the Learning Systems Lab. He ob-\ntained his Ph.D. in computer science from Tu\u00a8bingen\nUniversity, Germany and his M.Sc. and B.Sc. in\nPhysics from the University of Konstanz, Germany.\nThe Ph.D. thesis addresses gas distribution mapping\nand gas source localisation with a mobile robot. The\nM.Sc. thesis is concerned with an investigation of\nthe structure of (C60)+n clusters using gas phase\nion chromatography. His main research interests are\nmobile robot olfaction, robot vision, robotic map learning and safe navigation\nsystems.\n"}