{"doi":"10.1007\/s12369-010-0047-x","coreId":"54479","oai":"oai:eprints.lincoln.ac.uk:2566","identifiers":["oai:eprints.lincoln.ac.uk:2566","10.1007\/s12369-010-0047-x"],"title":"A bank of unscented Kalman filters for multimodal human perception with mobile service robots","authors":["Bellotto, Nicola","Hu, Huosheng"],"enrichments":{"references":[{"id":971733,"title":"A generative framework for realtime object detection and classi\ufb01cation. Computer Vision and Image Understanding,","authors":[],"date":"2005","doi":"10.1016\/j.cviu.2004.07.014","raw":"Fasel, I., Fortenberry, B., and Movellan, J. (2005). A generative framework for realtime object detection and classi\ufb01cation. Computer Vision and Image Understanding, 98:182\u2013210.","cites":null},{"id":972239,"title":"A survey of socially interactive robots. Robotics and Autonomous Systems,","authors":[],"date":"2003","doi":"10.1016\/s0921-8890(02)00372-x","raw":"Fong, T., Nourbakhsh, I., and Dautenhahn, K. (2003). A survey of socially interactive robots. Robotics and Autonomous Systems, 42(3-4):143\u2013166.","cites":null},{"id":973770,"title":"An ef\ufb01cient face normalization algorithm based on eyes detection.","authors":[],"date":"2006","doi":"10.1109\/iros.2006.281791","raw":"Li, G., Cai, X., Li, X., and Liu, Y. (2006). An ef\ufb01cient face normalization algorithm based on eyes detection. In Proc. of IEEE\/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), pages 3843\u20133848, Beijing, China.","cites":null},{"id":973252,"title":"An introduction to biometric recognition.","authors":[],"date":"2004","doi":"10.1109\/tcsvt.2003.818349","raw":"Jain, A. K., Ross, A., and Prabhakar, S. (2004). An introduction to biometric recognition. IEEE Trans. on Circuits and Systems for Video Technology, 14(1):4\u201320.","cites":null},{"id":971255,"title":"Automatic extraction and description of human gait models for recognition purposes.","authors":[],"date":"2003","doi":"10.1016\/s1077-3142(03)00008-0","raw":"Cunado, D., Nixon, M. S., and Carter, J. N. (2003). Automatic extraction and description of human gait models for recognition purposes. Comput. Vis. Image Underst., 90(1):1\u201341.","cites":null},{"id":18442533,"title":"Computationally Ef\ufb01-cient Solutions for Tracking People with a Mobile Robot: an Experimental Evaluation of Bayesian Filters. Autonomous Robots,","authors":[],"date":"2010","doi":"10.1007\/s10514-009-9167-2","raw":"Bellotto, N. and Hu, H. (2010). Computationally Ef\ufb01-cient Solutions for Tracking People with a Mobile Robot: an Experimental Evaluation of Bayesian Filters. Autonomous Robots, 28(4):425\u2013438.","cites":null},{"id":968564,"title":"Computationally Efficient Solutions for Tracking People with a Mobile Robot: an Experimental Evaluation of Bayesian Filters. Autonomous Robots,","authors":[],"date":"2010","doi":"10.1007\/s10514-009-9167-2","raw":null,"cites":null},{"id":975368,"title":"Data fusion for visual tracking with particles.","authors":[],"date":"2004","doi":"10.1109\/jproc.2003.823147","raw":"P\u00b4 erez, P., Vermaak, J., and Blake, A. (2004). Data fusion for visual tracking with particles. Proc. of IEEE, 92(3):495\u2013513.","cites":null},{"id":974029,"title":"Detecting and tracking moving objects from a mobile platform using a laser range scanner.","authors":[],"date":"2001","doi":"10.1109\/iros.2001.977171","raw":"Lindstr\u00a8 om, M. and Eklundh, J.-O. (2001). Detecting and tracking moving objects from a mobile platform using a laser range scanner. In Proc. of IEEE\/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), volume 3, pages 1364\u20131369, Maui, HI, USA.","cites":null},{"id":972779,"title":"Ef\ufb01cient particle \ufb01lters for joint tracking and classi\ufb01cation.","authors":[],"date":"2002","doi":"10.1117\/12.478524","raw":"Gordon, N. J., Maskell, S., and Kirubarajan, T. (2002). Ef\ufb01cient particle \ufb01lters for joint tracking and classi\ufb01cation. In Proc. of Signal and Data Processing of Small Targets (SPIE), pages 439\u2013449, FL, USA.","cites":null},{"id":976597,"title":"Eigenfaces for recognition.","authors":[],"date":"1991","doi":"10.1109\/cvpr.1991.139758","raw":"Turk, M. and Pentland, A. (1991). Eigenfaces for recognition. Journal of Cognitive Neuroscience, 3(1):72\u201386.","cites":null},{"id":968070,"title":"Estimation with Applications to Tracking and Navigation.","authors":[],"date":"2001","doi":"10.1002\/0471221279","raw":"Bar-Shalom, Y., Li, X. R., and Kirubarajan, T. (2001). Estimation with Applications to Tracking and Navigation. Wiley.","cites":null},{"id":975909,"title":"Face and facial feature detection evaluation.","authors":[],"date":"2008","doi":null,"raw":"Santana, M. C., Suarez, O. D., Canalis, L. A., and Navarro, J. L. (2008). Face and facial feature detection evaluation. In Proc. of the 3rd Int. Conf. on Computer Vision Theory and Applications (VISAPP), pages 167\u2013172.","cites":null},{"id":977076,"title":"Face recognition: A literature survey.","authors":[],"date":"2003","doi":"10.1145\/954339.954342","raw":"Zhao, W., Chellappa, R., Phillips, P. J., and Rosenfeld, A. (2003). Face recognition: A literature survey. ACM Comput. Surv., 35(4):399\u2013458.","cites":null},{"id":972992,"title":"Facial recognition in video.","authors":[],"date":"2003","doi":"10.1007\/3-540-44887-x_60","raw":"Gorodnichy, D. (2003). Facial recognition in video. In Proc. of Int. Conf. on Audio- and Video-Based Biometric Person Authentication (AVBPA), pages 505\u2013514, Guildford, United Kingdom.","cites":null},{"id":970339,"title":"Fast face detection for mobile robots by integrating laser range data with vision.","authors":[],"date":"2003","doi":null,"raw":"Blanco, J., Burgard, W., Sanz, R., and Fern\u00b4 anez, J. (2003). Fast face detection for mobile robots by integrating laser range data with vision. In Proc. of the Int. Conf. on Advanced Robotics (ICAR), volume 2, pages 953\u2013958, Coimbra, Portugal.","cites":null},{"id":976146,"title":"Fast, reliable, adaptive, bimodal people tracking for indoor environments.","authors":[],"date":"2004","doi":"10.1109\/iros.2004.1389583","raw":"Scheutz, M., McRaven, J., and Cserey, G. (2004). Fast, reliable, adaptive, bimodal people tracking for indoor environments. In Proc. of IEEE\/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), volume 2, pages 1347\u20131352, Sendai, Japan.","cites":null},{"id":974889,"title":"Full-body person recognition system.","authors":[],"date":"2003","doi":"10.1016\/s0031-3203(03)00061-x","raw":"Nakajima, C., Pontil, M., Heisele, B., and Poggio, T. (2003). Full-body person recognition system. Pattern Recognition, 36(9):1997\u20132006.","cites":null},{"id":971505,"title":"Getting to Know Each Other - Arti\ufb01cial Social Intelligence for Autonomous Robots. Robotics and Autonomous Systems,","authors":[],"date":"1995","doi":"10.1016\/0921-8890(95)00054-2","raw":"Dautenhahn, K. (1995). Getting to Know Each Other - Arti\ufb01cial Social Intelligence for Autonomous Robots. Robotics and Autonomous Systems, 16:333\u2013356.","cites":null},{"id":977383,"title":"He was a General Co-Chair of","authors":[],"date":"2007","doi":"10.1109\/ISWPC.2007.342555","raw":"He was a General Co-Chair of IEEE International ConferenceonMechatronicsandAutomation,Harbin,China, 2007, Publication Chair of IEEE International Conference on Networking, Sensing and Control, London, 2007; CoChair of Special & Organised Sessions of IEEE InternationalConferenceonRoboticsandBiomimetics,Sanya,China, 2007;ChairforSpecialandOrganisedSessions,IEEE\/ASME International Conference on Advanced Intelligent Mechatronics, Xi\u2019an, China, 2008, etc. Prof. Hu is currently Editorin-chieffortheInternationalJournalofAutomationandComputing. He is a reviewer for a number of international journals such as IEEE Transactions on Robotics, SMC-Part B, AutomaticControl,NeuralNetworksandInternationalJournal of Robotics Research. Since 2000 he has been a Visiting Professor at 6 universities in China - namely Central South University, Shanghai University, Wuhan University of Science and Engineering, Kunming University of Science and Technology, Chongqing University of Post & Telecommunication, and Northeast Normal University. Prof. Hu is a Chartered Engineer, a senior member of IEEE and ACM, and a member of IET and IAS.","cites":null},{"id":974243,"title":"iBotGuard: an internet-based intelligent robot security system using invariant face recognition against intruder.","authors":[],"date":"2005","doi":"10.1109\/tsmcc.2004.840051","raw":"Liu, J. N. K., Wang, M., and Feng, B. (2005). iBotGuard: an internet-based intelligent robot security system using invariant face recognition against intruder. IEEE Trans. on Systems, Man, and Cybernetics (Part C), 35(1):97\u2013105.","cites":null},{"id":967573,"title":"Jijo-2: An of\ufb01ce robot that communicates and learns.","authors":[],"date":"2001","doi":"10.1109\/mis.2001.956081","raw":"Asoh, H., Vlassis, N., Motomura, Y., Asano, F., Hara, I., Hayamizu, S., Ito, K., Kurita, T., Matsui, T., Bunschoten, R., and Kr\u00a8 ose, B. (2001). Jijo-2: An of\ufb01ce robot that communicates and learns. IEEE Intelligent Systems, 16(5):46\u2013","cites":null},{"id":974675,"title":"Joint Target Tracking and Identi\ufb01cation - Part II: Shape video computing.","authors":[],"date":"2005","doi":"10.1109\/icif.2005.1591864","raw":"Minvielle, P., Marrs, A., Maskell, S., and Doucet, A. (2005). Joint Target Tracking and Identi\ufb01cation - Part II: Shape video computing. In Proc. of the 8th Int. Conf. on Information Fusion, Philadelphia, PA, USA.","cites":null},{"id":976874,"title":"Keeping track of humans: Have I seen this person before?","authors":[],"date":"2005","doi":"10.1109\/robot.2005.1570420","raw":"Zajdel, W., Zivkovic, Z., and Kr\u02d8 ose, B. J. A. (2005). Keeping track of humans: Have I seen this person before? In Proc. of IEEE Int. Conf. on Robotics and Automation (ICRA), pages 2093\u20132098, Barcelona, Spain.","cites":null},{"id":969005,"title":"Learning motion patterns of people for compliant robot motion. The Int.","authors":[],"date":"2005","doi":"10.1177\/0278364904048962","raw":"Bennewitz, M., Burgard, W., Cielniak, G., and Thrun, S. (2005). Learning motion patterns of people for compliant robot motion. The Int. Journal of Robotics Research, 24(1):31\u201348.","cites":null},{"id":969268,"title":"Learning motion patterns of persons for mobile service robots.","authors":[],"date":"2002","doi":"10.1109\/robot.2002.1014268","raw":"Bennewitz, M., Burgard, W., and Thrun, S. (2002). Learning motion patterns of persons for mobile service robots. In Proc. of IEEE Int. Conf. on Robotics and Automation (ICRA), pages 3601\u20133606, Washington, DC, USA.","cites":null},{"id":972526,"title":"Multi-modal anchoring for human-robot-interaction. Robotics and Autonomous Systems,","authors":[],"date":"2003","doi":"10.1016\/s0921-8890(02)00355-x","raw":"Fritsch, J., Kleinehagenbrock, M., Lang, S., Pl\u00a8 otz, T., Fink, G. A., and Sagerer, G. (2003). Multi-modal anchoring for human-robot-interaction. Robotics and Autonomous Systems, 43(2-3):133\u2013147.","cites":null},{"id":968298,"title":"Multisensor data fusion for joint people tracking and identi\ufb01cation with a service robot.","authors":[],"date":"2007","doi":"10.1109\/robio.2007.4522385","raw":"Bellotto, N. and Hu, H. (2007a). Multisensor data fusion for joint people tracking and identi\ufb01cation with a service robot. In Proc. of IEEE Int. Conf. on Robotics and Biomimetics (ROBIO), pages 1494\u20131499, Sanya, China.","cites":null},{"id":968789,"title":"Multisensor-based human detection and tracking for mobile service robots.","authors":[],"date":"2009","doi":"10.1109\/tsmcb.2008.2004050","raw":"Bellotto, N. and Hu, H. (2009). Multisensor-based human detection and tracking for mobile service robots. IEEE Trans. on Systems, Man, and Cybernetics \u2013 Part B, 39(1):167\u2013181.","cites":null},{"id":967799,"title":"MultitargetMultisensor Tracking: Principles","authors":[],"date":"1995","doi":null,"raw":"Bar-Shalom, Y. and Li, X. R. (1995). MultitargetMultisensor Tracking: Principles and Techniques. Y. BarShalom.","cites":null},{"id":975673,"title":"On target classi\ufb01cation using kinematic data.","authors":[],"date":"2004","doi":"10.1016\/j.inffus.2003.08.002","raw":"Ristic, B., Gordon, N., and Bessell, A. (2004). On target classi\ufb01cation using kinematic data. Information Fusion, 5(1):15\u201321.","cites":null},{"id":970587,"title":"Panoramic vision and laser range \ufb01nder fusion for multiple person tracking.","authors":[],"date":"2006","doi":"10.1109\/iros.2006.282149","raw":"Chakravarty, P. and Jarvis, R. (2006). Panoramic vision and laser range \ufb01nder fusion for multiple person tracking. In Proc. of IEEE\/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), pages 2949\u20132954, Beijing, China.","cites":null},{"id":970987,"title":"People recognition by mobile robots.","authors":[],"date":"2004","doi":"10.1109\/iros.2005.1545530","raw":"Cielniak, G. and Duckett, T. (2004). People recognition by mobile robots. In Proc. of AILS 2nd Joint SAIS\/SSLS15 Workshop, Lund, Sweden.","cites":null},{"id":976350,"title":"People Tracking with Mobile Robots Using Sample-based Joint Probabilistic Data Association Filters.","authors":[],"date":"2003","doi":"10.1177\/0278364903022002002","raw":"Schulz, D., Burgard, W., Fox, D., and Cremers, A. B. (2003). People Tracking with Mobile Robots Using Sample-based Joint Probabilistic Data Association Filters. Int. Journal of Robotics Research, 22(2):99\u2013116.","cites":null},{"id":970749,"title":"Person identi\ufb01cation by mobile robots in indoor environments.","authors":[],"date":"2003","doi":"10.1109\/rose.2003.1218704","raw":"Cielniak, G. and Duckett, T. (2003). Person identi\ufb01cation by mobile robots in indoor environments. In Proc. of the IEEE Int. Workshop on Robotic Sensing (ROSE), \u00a8 Orebro, Sweden.","cites":null},{"id":977151,"title":"Probabilistic human recognition from video.","authors":[],"date":"2002","doi":"10.1016\/S1077-3142(03)00080-8","raw":"Zhou, S. and Chellappa, R. (2002). Probabilistic human recognition from video. In Proc. of the 7th European Conference on Computer Vision (ECCV), pages 681\u2013697, London, UK. Springer-Verlag.16 Nicola Bellotto is a Lecturer in the School of Computer Science at the University of Lincoln and a member of the Lincoln Robotics Group. He holds a PhD in Computer Science from the University of Essex and a Laurea in Electronic Engineering from the University of Padua. Before joining the University of Lincoln, he was a research assistant in Cognitive Computer Vision at the University of Oxford. His research interests range from mobile robotics to cognitive perception,includingsensorfusion,Bayesianestimation,robot vision and localization. He gained also several years of professional experience in the industry as software developer and embedded system programmer. Huosheng Hu is a Professor in the School of Computer Science and Electronic Engineering, University of Essex, UK, leading the Human Centred Robotics Group. His research interests include autonomous mobile robots, humanrobot interaction, evolutionary robotics, multi-robot collaboration, embedded systems, pervasive computing, sensor integration, intelligent control and networked robotics. He has published over 250 papers in journals, books and conferences, and received a number of best paper awards. He has been a founding member of Networked Robots within the IEEE Robotics and Automation Technical Committee since","cites":null},{"id":971045,"title":"Realtime tracking of non-rigid objects using mean shift.","authors":[],"date":"2000","doi":"10.1109\/cvpr.2000.854761","raw":"Comaniciu, D., Ramesh, V., and Meer, P. (2000). Realtime tracking of non-rigid objects using mean shift. In Proc. of IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), volume 2, pages 142\u2013149, SC, USA.","cites":null},{"id":976669,"title":"Robust real-time face detection.","authors":[],"date":"2004","doi":"10.1023\/B:VISI.0000013087.49260.fb","raw":"Viola, P. and Jones, M. J. (2004). Robust real-time face detection. Int. Journal of Computer Vision, 57(2):137\u2013","cites":null},{"id":971985,"title":"Robust real-time pursuit of persons with a mobile robot using multisensor fusion.","authors":[],"date":"2000","doi":null,"raw":"Feyrer, S. and Zell, A. (2000). Robust real-time pursuit of persons with a mobile robot using multisensor fusion. In Proc. of the 6th Int. Conf. on Intelligent Autonomous Systems (IAS), pages 710\u2013715, Venice, Italy.","cites":null},{"id":974499,"title":"Sensor fusion using a probabilistic aggregation scheme for people detection and tracking.","authors":[],"date":"2005","doi":"10.1016\/j.robot.2006.04.012","raw":"Martin, C., Schaffernicht, E., Scheidig, A., and Gross, H.-M. (2005). Sensor fusion using a probabilistic aggregation scheme for people detection and tracking. In Proc. of the 2nd European Conference on Mobile Robots (ECMR), pages 176\u2013181, Ancona, Italy.","cites":null},{"id":969750,"title":"The CSU Face Identi\ufb01cation Evaluation System User\u2019s Guide: Version 5.0.","authors":[],"date":"2003","doi":"10.1007\/3-540-36592-3_29","raw":"Beveridge, R., Bolme, D., Teixeira, M., and Draper, B. (2003). The CSU Face Identi\ufb01cation Evaluation System User\u2019s Guide: Version 5.0. Computer Science Department, Colorado State University.","cites":null},{"id":975120,"title":"The mobot museum robot installations: a \ufb01ve year experiment.","authors":[],"date":"2003","doi":"10.1109\/iros.2003.1249720","raw":"Nourbakhsh, I., Kunz, C., and Willeke, T. (2003). The mobot museum robot installations: a \ufb01ve year experiment. In Proc. of IEEE\/RSJ Int. Conf. on Intelligent Robots and Systems (IROS), pages 3636\u20133641.","cites":null},{"id":970024,"title":"Tracking people from a mobile platform.","authors":[],"date":"2001","doi":"10.1007\/3-540-36268-1_20","raw":"Beymer, D. and Konolige, K. (2001). Tracking people from a mobile platform. In IJCAI Workshop on Reasoning with Uncertainty in Robotics, Seattle, WA, USA.","cites":null},{"id":973501,"title":"Unscented \ufb01ltering and nonlinear estimation.","authors":[],"date":"2004","doi":"10.1109\/jproc.2003.823141","raw":"Julier, S. J. and Uhlmann, J. K. (2004). Unscented \ufb01ltering and nonlinear estimation. Proc. of the IEEE, 92(3):401\u2013422.","cites":null},{"id":967322,"title":"Using boosted features for the detection of people in 2d range data. In","authors":[],"date":"2007","doi":"10.1109\/robot.2007.363998","raw":"Arras, K. O., Mozos, O. M., and Burgard, W. (2007). Using boosted features for the detection of people in 2d range data. In Proc. of IEEE Int. Conf. on Robotics and Automation (ICRA), pages 3402\u20133407, Rome, Italy.","cites":null},{"id":969534,"title":"Utilizing learned motion patterns to robustly track persons.","authors":[],"date":"2003","doi":null,"raw":"Bennewitz, M., Cielniak, G., and Burgard, W. (2003). Utilizing learned motion patterns to robustly track persons. In Proc. of IEEE Int. W. on VS-PETS, pages 102\u2013 109, France.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-06","abstract":"A new generation of mobile service robots could be ready soon to operate in human environments if they can robustly estimate position and identity of surrounding people. Researchers in this field face a number of challenging problems, among which sensor uncertainties and real-time constraints.\\ud\nIn this paper, we propose a novel and efficient solution for simultaneous tracking and recognition of people within the observation range of a mobile robot. Multisensor techniques for legs and face detection are fused in a robust probabilistic framework to height, clothes and face recognition algorithms. The system is based on an efficient bank of Unscented Kalman Filters that keeps a multi-hypothesis estimate of the person being tracked, including the case where the latter is unknown to the robot.\\ud\nSeveral experiments with real mobile robots are presented to validate the proposed approach. They show that our solutions can improve the robot's perception and recognition of humans, providing a useful contribution for the future application of service robotics","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/54479.pdf","fullTextIdentifier":"http:\/\/eprints.lincoln.ac.uk\/2566\/1\/Bellotto2010a.pdf","pdfHashValue":"f7e91b5f6d68a5adbf034181bd59483c46dbcce2","publisher":"Springer","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lincoln.ac.uk:2566<\/identifier><datestamp>\n      2013-03-13T08:38:30Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F48:6A6163735F48363730<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lincoln.ac.uk\/2566\/<\/dc:relation><dc:title>\n        A bank of unscented Kalman filters for multimodal human perception with mobile service robots<\/dc:title><dc:creator>\n        Bellotto, Nicola<\/dc:creator><dc:creator>\n        Hu, Huosheng<\/dc:creator><dc:subject>\n        H670 Robotics and Cybernetics<\/dc:subject><dc:description>\n        A new generation of mobile service robots could be ready soon to operate in human environments if they can robustly estimate position and identity of surrounding people. Researchers in this field face a number of challenging problems, among which sensor uncertainties and real-time constraints.\\ud\nIn this paper, we propose a novel and efficient solution for simultaneous tracking and recognition of people within the observation range of a mobile robot. Multisensor techniques for legs and face detection are fused in a robust probabilistic framework to height, clothes and face recognition algorithms. The system is based on an efficient bank of Unscented Kalman Filters that keeps a multi-hypothesis estimate of the person being tracked, including the case where the latter is unknown to the robot.\\ud\nSeveral experiments with real mobile robots are presented to validate the proposed approach. They show that our solutions can improve the robot's perception and recognition of humans, providing a useful contribution for the future application of service robotics.<\/dc:description><dc:publisher>\n        Springer<\/dc:publisher><dc:date>\n        2010-06<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lincoln.ac.uk\/2566\/1\/Bellotto2010a.pdf<\/dc:identifier><dc:identifier>\n          Bellotto, Nicola and Hu, Huosheng  (2010) A bank of unscented Kalman filters for multimodal human perception with mobile service robots.  International Journal of Social Robotics, 2  (2).   pp. 121-136.  ISSN 1875-4791  <\/dc:identifier><dc:relation>\n        http:\/\/www.springerlink.com\/content\/e7604747x468135u\/<\/dc:relation><dc:relation>\n        10.1007\/s12369-010-0047-x<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lincoln.ac.uk\/2566\/","http:\/\/www.springerlink.com\/content\/e7604747x468135u\/","10.1007\/s12369-010-0047-x"],"year":2010,"topics":["H670 Robotics and Cybernetics"],"subject":["Article","PeerReviewed"],"fullText":"Noname manuscript No.\n(will be inserted by the editor)\nA Bank of Unscented Kalman Filters for Multimodal Human Perception\nwith Mobile Service Robots\nNicola Bellotto \u00b7 Huosheng Hu\nReceived: date \/ Accepted: date\nAbstract A new generation of mobile service robots could\nbe ready soon to operate in human environments if they\ncan robustly estimate position and identity of surrounding\npeople. Researchers in this field face a number of challeng-\ning problems, among which sensor uncertainties and real-\ntime constraints. In this paper, we propose a novel and ef-\nficient solution for simultaneous tracking and recognition\nof people within the observation range of a mobile robot.\nMultisensor techniques for legs and face detection are fused\nin a robust probabilistic framework to height, clothes and\nface recognition algorithms. The system is based on an effi-\ncient bank of Unscented Kalman Filters that keeps a multi-\nhypothesis estimate of the person being tracked, including\nthe case where the latter is unknown to the robot. Several ex-\nperiments with real mobile robots are presented to validate\nthe proposed approach. They show that our solutions can\nimprove the robot\u2019s perception and recognition of humans,\nproviding a useful contribution for the future application of\nservice robotics.\nKeywords Robot Perception \u00b7 Human Tracking and\nRecognition \u00b7 Bayesian Estimation \u00b7 Service Robotics\n1 Introduction\nPeople tracking algorithms try to estimate the position of\nhumans in the environment from noisy measurements. How-\nN. Bellotto\nSchool of Computer Science\nUniversity of Lincoln\nLincoln LN6 7TS, United Kingdom\nE-mail: nbellotto@lincoln.ac.uk\nH. Hu\nSchool of Computer Science and Electronic Engineering\nUniversity of Essex\nColchester CO4 3SQ, United Kingdom\never, service robots must also distinguish and recognize dif-\nferent persons, which are otherwise treated as simple mov-\ning objects. Without recognition, robots would not be able\nto deal with the actual user needs and they could not deliver\nhigh-quality services.\nMobile service robots acting in human environments and\nprovided with interaction skills (e.g. tour-guide or security\nrobots) are more effective if they can distinguish between\nnew and former users, or between public visitors and staff\nmembers. Besides these applications, the capability to iden-\ntify people gives robots a certain grade of \u201csocial intelli-\ngence\u201d [19] as they can better adapt to individual human\nbehaviours.\nMost of existing robotic systems provided with vision-\nbased human recognition operate in two separate steps: first,\na frame is selected where the subjects satisfies some criteria,\nlike pose, size or number of visible features; then, some stan-\ndard recognition algorithm is applied versus a fixed database\nof known people [2, 15, 30]. Unfortunately, this approach ig-\nnores important clues like time and spatial evolution of the\nsubject to be identified. This information can be provided by\nhuman trackers and used to improve the robot\u2019s recognition\nsystem, while the latter can increase the robustness of the\ntracking process itself.\nIn this paper we propose a novel solution for simultane-\nous tracking and identification of humans with mobile ser-\nvice robots, which integrates several detection and recogni-\ntion algorithms in a robust probabilistic framework, mak-\ning use of different data sources and a bank of Bayesian fil-\nters. A new histogram-based recognition algorithm for hu-\nman clothes is presented, which takes into account the un-\ncertainty of the human position to select the image region\nwhere the histogram match is best. The success rate of hu-\nman identification is increased with a simple face recogni-\ntion algorithm, which makes use of an improved method for\nfast face alignment and scaling. Finally, these recognition\n2algorithms are fused with human height and 2D spatial in-\nformation, thanks to a modularized architecture that keeps\nmulti-hypothesis estimates of the subject being tracked.\nThe remainder of the paper is organized as follows. Sec-\ntion 2 presents and overview of related research work. An\noverview of the multisensor human detection is introduced\nin Section 3, while the algorithms for vision-based recog-\nnition are illustrated in Section 4. The following Section 5\nintroduces a bank of Bayesian filters and describes the archi-\ntecture implemented for simultaneous people tracking and\nrecognition. Several experimental results with mobile robots\nare presented and discussed in Section 6. Finally, Section\n7 concludes the paper with a summary of the progresses\nachieved and future research directions.\n2 Related Work\nIn the context of social robotics [19, 22], human detection,\ntracking and recognition are essential prerequisites for suc-\ncessful applications. Before starting close interactions, in-\ndeed, mobile social robot have to detect people and approach\nthem. This is also necessary for robots to attract human at-\ntention and explicitly ask for help in case they need assis-\ntance [34].\nIn literature, different solutions for tracking humans with\nmobile robots are reported. Many applications use robots\u2019\non-board cameras to detect people, often searching for faces\n[30] or other body parts [12, 42]. Several approaches instead\nmake use of range sensors, considering people as moving\nentities [29, 9].\nHuman recognition is another broad research field that\nincludes detection and interpretation of biometric features\n[26]. Most solutions are vision-based and use algorithms\nfor face recognition [40, 43, 44], or in some case gait and\nfull body analysis [18, 33]. However, just a few recogni-\ntion systems are actually implemented on real mobile robots,\nas their perception capabilities are limited by sensor uncer-\ntainty, motion and changes in the environment [2, 8, 15].\nHumans have articulated body postures and motion be-\nhaviours, difficult to be modelled and observed from a robot\nplatform. In order to reduce uncertainty and improve redun-\ndancy, two or more sensors can be integrated for human de-\ntection and recognition. Heuristic approaches are in some\ncases used to estimate the position of a person with mobile\nrobots equipped with laser and camera [38]. These sensors\ncan be combined to reduce the searching area during human\ndetection, using the laser to find the direction of possible\ntargets and applying face detection only to small portions of\nthe current frame [13]. Thermal and CCD cameras are used\nin [16] for human recognition, segmenting people\u2019s contours\non the thermal image and applying a Bayesian classifier to\nthe relative region on the colour image.\nlaser\nrobotic head\ncamera\ntouch\u2212screen\n\u223c\n0\n.4\nm\n\u223c\n1\n.7\nm\nFig. 1 The Scitos G5 robot with laser, camera and interaction devices.\nProbabilistic methods for robot sensor fusion have also\nbeen proposed. The system implemented in [21] and [7]\nadopts Kalman filters to track people using lasers and cam-\neras mounted on mobile robots. In [39], the authors illus-\ntrate a robot equipped with two laser range sensors that can\ntrack several people using a combination of particle filters\nand probabilistic data association. Another solution based\non particle filter is proposed in [14], which integrates laser\ndata and visual information from a panoramic camera. A co-\nvariance intersection method, using sonar, laser and visual\ndata, is implemented in [31] for tracking multiple people.\nThe last two implementations, however, are evaluated only\nwith static robot platforms.\n3 Multisensor Human Detection\nIn this work we consider mobile robots equipped with SICK\nlaser range sensors and colour cameras. The two robotic\nplatforms used in our research are a Scitos G5, shown in\nFig. 1, and a Pioneer 2 DX with on-board PCs and similar\nsensor configurations, illustrated in Fig. 2.\n3.1 Legs Detection\nThe laser sensor of the robot, mounted a few decimeters\nfrom the floor, can be used to detect human legs in a range\nof several meters. Most of the existing legs detection algo-\nrithms are based on the search of local minima [10, 39], mo-\ntion detection [14, 29] or machine learning techniques [1].\nThe legs detection algorithm implemented in this work\nis based on the recognition of typical legs patterns extracted\nfrom a single laser scan. These patterns correspond to three\npossible postures: legs apart, forward straddle and two legs\n3\u223c\n1.\n6m\n\u223c\n0.\n3m\nFig. 2 The Pioneer 2 DX robot with laser and camera, detecting legs\nand face respectively.\ntogether (or single leg). An example of legs detection is il-\nlustrated in Fig. 3. Briefly, the algorithm initially filters laser\ndata in order to smooth the readings, then detects all the\nedges lying on the directions of the laser scans. Groups of\nadjacent edges, possibly corresponding to legs, are identi-\nfied according to simple geometric relations and spatial con-\nstraints. Fig. 4 shows a scan of the three different legs pos-\ntures with the angle of the laser beam in the abscissa and the\nmeasured range in the ordinate. Direction and distance of\neach legs pattern is computed from the midpoint (red cross)\nbetween the extremes (black circles) of the outer vertical\nedges. The method is quite robust even in case of cluttered\nenvironments and, besides being computationally efficient,\nit is not influenced by the robot\u2019s motion. Further details and\ncomparisons to other techniques can be found in [7].\n3.2 Face Detection\nThe camera on the robot can be used to detect faces and\nrecognize people. Some of the most popular techniques to\nperform real-time face detection are based on the color seg-\nmentation of skin regions [23], but these are usually prone to\nerrors due to light variations, shadows and skin tones. Like\nin our previous work [7], the face detection algorithm im-\nplemented in the current system is based on the solution of\nViola & Jones [41], which offers a good balance between\ndetection performance and computational efficiency. Fig. 3\nshows an example of face detection with the robot\u2019s camera.\n4 Vision-based Recognition\nThe algorithms described next are used for clothes and faces\nrecognition. Although the former alone cannot provide a\nCa\nm\ner\na \n(ve\nrti\nca\nl) \npla\nne\nLa\nse\nr (\nho\nriz\non\ntal\n) p\nlan\ne\nY\nX\nZ\nX\nFig. 3 Face and legs detection. On the bottom, from left to right, three\ndifferent legs postures can be noted from the laser scan: legs apart,\nforward straddle and two legs together.\nFig. 4 Legs patterns and relative midpoints for measuring their direc-\ntion and distance.\nbiometric measure for robust human identification, it can\ngreatly improve the system performance when combined with\nheight and face recognition, as later shown in our experi-\nments.\n4.1 Clothes Recognition\nClothes recognition is performed using an improved ver-\nsion of the color histogram comparison described in [5].\nSince the main task of the robot is to have close interac-\ntions with humans, it is generally not possible to consider\nthe histogram of the whole body, so the region of interest\n(ROI) is limited to the human torso, which is the only part\nalways visible from the camera when the robot is at a mini-\nmum distance from the person (at least 2m in our case).\nAn efficient measure to compare color histograms is the\none adopted for the mean-shift tracking algorithm [17], which\nis based on the sample estimate of the Bhattacharyya co-\n4efficient. Given a discrete normalized density of reference\nq= {qu}u=1...m (i.e. an m-bin histogram) and the one to\nbe compared p(R) = {pu(R)}u=1...m, where R is the ROI,\nthe sample estimate of the Bhattacharyya coefficient is the\nfollowing:\n\u03c1 [p(R),q] =\nm\u2211\nu=1\n\u221a\npu(R) qu (1)\nUsing (1), the distance between the two distributions is de-\nfined as follows:\nd (R) \u2261 d [p(R),q] =\n\u221a\n1\u2212 \u03c1 [p(R),q] (2)\nSince based on discrete densities, this distance is scale in-\nvariant and is normalized between 0 and 1. From empirical\ntests on a number of subjects, it showed also to be very dis-\ncriminative, yet quite robust to different human poses.\nSome of the histogram-based techniques for human recog-\nnition rely on a precise calibration between camera and laser\nrange finder to select the body region on the current frame\n[8]. In case the laser is not available, motion detection tech-\nniques are used to highlight the ROI [42]. It is very difficult\nhowever to select precisely the correct body region on the\ncurrent frame, in particular when the robot and the person\nare moving. If this selection is not accurate, the histogram\nconsidered might be completely wrong. A fixed scale factor\nto increase the size of the ROI does not solve the problem, as\nthe measure could be seriously influenced by other objects\non the background. Differently from other works [8, 15, 33],\nthe algorithm described next explicitly considers the uncer-\ntainty from tracking, and therefore does not need an accurate\ncalibration between camera and laser. The selection proce-\ndure is also independent from lighting conditions and related\nproblems that usually affect video segmentation techniques.\nThe distance between color histograms is calculated us-\ning a selection procedure of the ROI that takes into account\nthe uncertainty of the current human estimate. The consid-\nered body proportions are those proposed in [15], and il-\nlustrated in Fig. 5(a), where the torso is 2\/6 of the total\nheight. Given the current 3D estimate [xk, yk, zk]T of the\nface position, the centre of the torso m = [xk, yk, \u03b7 zk]T is\ninitially determined, where \u03b7 = 8\/11 is a constant value cal-\nculated considering the abovementioned human proportions.\nThe point m is projected onto the image plane, obtaining the\nrelative pixel (um, vm). This is the centre of the initial torso\nregion Rt, the size of which is also set according to the given\nbody proportions (i.e. blue rectangle in Fig. 5(a)). Note that\nclothes recognition can be applied even when the person is\nnot facing the camera because Rt is proportional to the esti-\nmated height, which is kept in the state as long as the subject\nis being tracked.\nGiven the vector of standard deviations \u03c3 = [\u03c3x, \u03c3y, \u03c3z]T\nfrom the covariance matrix of the current human estimate\n(a) Body proportions. (b) Procedure for histogram region selection.\nFig. 5 Selection of the region for clothes recognition.\nand a scale factor s, the point (m + s\u03c3) is also projected on\nthe relative pixel (u\u03c3, v\u03c3). The differences \u2206u = |u\u03c3\u2212um|\nand \u2206v = |v\u03c3 \u2212 vm| are the quantities used to extend the\ninitial ROI and get the new R\u03c3 , as illustrated in Fig. 5(b). A\nscale factor s = 2 guarantees a region sufficiently large to\ninclude the targets torso in most of the situations.\nIn a way similar to standard template matching, the his-\ntogram of reference q is compared to the histograms of all\nthe sub-regions R, with the same size of Rt, inside the con-\nsidered region R\u03c3 . The histogram of reference is provided\nby a fixed database of known subjects. This is manually ini-\ntialized before operation, although in the future it would be\ndesirable to include an automatic initialization and update\nof the database. In order to limit the influence of light varia-\ntions, histograms are calculated in the HSV color space from\nthe Hue and Saturation components.\nThe sub-region R\u2217 for which the distance d\u2217 \u2261 d(R\u2217)\nis minimum is where the histogram of reference matches\nbest. The centre (u\u2217, v\u2217) of R\u2217 can be used to calculate the\ndirection of the human target with respect to the camera. The\nwhole procedure is briefly described in Algorithm 1.\n4.2 Face Recognition\nDuring the last decades, many solutions have been proposed\nfor the challenging task of face recognition [43]. Most of\nthe initial work concentrates on recognition from still im-\nages, but the recent availability of fast algorithms for real-\ntime face detection made possible the recognition on video\nsequences. However, a number of problems, like head pose,\nlighting condition and low resolution cameras, makes face\n5Algorithm 1 Histogram-based Detection\nInput: estimated xk, yk and zk, with relative \u03c3x, \u03c3y and \u03c3z\nOutput: position (u\u2217, v\u2217) and distance d\u2217\n1: m\u21d0 [xk, yk, \u03b7 zk]T . initialize ROI\n2: \u03c3 \u21d0 [\u03c3x, \u03c3y , \u03c3z ]T\n3: project m and (m + s\u03c3) on image plane to obtain, respectively,\n(um, vm) and (u\u03c3 , v\u03c3)\n4: \u2206u\u21d0 |u\u03c3 \u2212 um|\n5: \u2206v \u21d0 |v\u03c3 \u2212 vm|\n6: select initial ROI Rt from the torso, centered in m, as shown in\nFig. 5(a)\n7: calculate new ROI R\u03c3 increasing Rt by 2\u2206u and 2\u2206v, as shown\nin Fig. 5(b)\n8: d\u2217 \u21d0\u221e . initialize histogram match\n9: (u\u2217, v\u2217) \u21d0 (um, vm)\n10: get histogram of reference q\n11: for all R \u2208 R\u03c3 with R centred in (uR, vR) and having the same\nsize of Rt do\n12: if d(R) < d\u2217 then . new best match found\n13: (u\u2217, v\u2217) \u21d0 (uR, vR) . memorize match position\n14: d\u2217 \u21d0 d(R) . memorize histogram distance\n15: end if\n16: end for\nrecognition in video more difficult. On the other hand, videos\ncontain important time and spatial information, not available\notherwise from still images [44].\nIn general, the whole process of face recognition con-\nsists of three main steps: detect a face from the current frame,\nprocess the relative image region and finally apply some\nrecognition algorithm. Processing the considered image re-\ngion is one of the most crucial part of every identification\nsystem. To align a face horizontally, a common technique is\nto locate the eyes\u2019 position and rotate the face image, so that\ntheir final inclination is null. The distance between the eyes\nis used to resize the face to a pre-determined value. The pro-\ncessed face can be compared to a reference template, that in\nour system is the canonical face representation proposed in\n[25] and shown in Fig. 6.\nA fast algorithm for eye detection has been proposed in\n[28] and is based on the extraction of histogram minima\nwithin sub-regions containing the eyes. The method relies\non the assumption that the iris\u2019 color is darker than the sur-\nrounding region, which is not always true. The method il-\nlustrated in [20] makes use of a more robust probabilistic\napproach that takes into account the uncertainty of face de-\ntection. Unfortunately the implementation of the latter could\nnot work in real-time on the available robots.\nThe solution developed for our system is a fast, color-\nindependent procedure based on the same algorithm used for\nface detection [41, 37]. Using two classifiers, one trained for\nright eyes and another for left eyes, two independent local\nsearches are performed on specific sub-regions of the face\nbounding box. With reference to Fig. 6, the regions scanned\nFig. 6 Canonical model for face processing and recognition.\nare the 2w\u00d72w top-left and top-right areas. If more than one\neye is found within the considered region (false positives),\nthe detection closest to its centre is chosen.\nIn order to align it horizontally, the face is rotated of\nan angle \u03b1RL calculated from the positions (uR, vR) and\n(uL, vL) of the right and left eye respectively. From the model\nin Fig. 6, where the distance dRL = 2w between eyes is half\nthe size of the face, the scaling factor to obtain a face of size\nW \u00d7 W is s = W\/(2 dRL). Rotation and scaling of the\nface, centred on the right eye, are performed with an affine\ntransformation as follows:\na = s cos (\u03b1RL)\nb = s sin (\u03b1RL) (3)\n[\nu\u2032\nv\u2032\n]\n=\n[\na b (1\u2212 a) uR \u2212 b vR\n\u2212b a b uR + (1\u2212 a) vR\n]\uf8ee\n\uf8f0uv\n1\n\uf8f9\n\uf8fb\nwhere (u, v) is a pixel of the source image and (u\u2032, v\u2032) of the\ndestination. Note that, in order to avoid possible outliers of\nthe rotated face image, the affine transformation is actually\napplied to a sub-region slightly bigger than the original face\nbounding box, so a correction term for the offset is added\nto the coordinates (uR, vR) of the right eye. Eventually, the\nresulting face will be W \u00d7W pixels, with the right and left\neyes centred in (w,w) and (3w,w) respectively.\nThe last step includes cropping the face\u2019s area with an\nelliptical mask. This reduces the influence of hair and back-\nground pixels on the four corners of the rectangular region.\nThen, considering only the area within the ellipse, the face\nis equalized and normalized so that the distribution of the\npixels intensity has zero mean and standard deviation one.\nAn example of face processing, from detection to normal-\nization, is shown in Fig. 7.\nFinally, the popular Eigenface recognition algorithm [40]\nis applied to the normalized face versus a database of known\nfaces. The difference between current and reference face is\na quantity \u03be, between \u22121 and 0, given by the standard Ma-\nhalanobis cosine [11].\n6Fig. 7 Image processing before recognition. The detected face is\nshown on the left, the position of the eyes in the middle and the final\nresult on the right.\n5 Simultaneous Human Tracking and Recognition\nA part for military applications, where most of the results\nare available only in simulation, little work has been done\nso far for joint tracking and classification of objects and hu-\nmans [32, 44]. Current solutions are generally based on the\nuse of single sensor data and are often constrained by com-\nputation resources, which make them unfeasible for robot\napplications. The proposed approach, instead, is an effective\nsolution that uses sensor fusion to track and recognize hu-\nmans simultaneously and in real-time.\n5.1 Bank of Filters\nThe estimate of a particular target at time tk can be ex-\npressed by the joint state {xk, ci}, where xk \u2208 Rn is a\nvector containing information like position and velocity of\nthe target, and ci (with i = 1, . . . , N ) is a time-invariant at-\ntribute, or target class. Given the sequence of measurements\nZk = {z1, . . . , zk}, the prior distribution of the ith joint\nstate can be written as follows [24]:\np({xk, ci}|Zk\u22121)\n=\n\u222b\nRn\np(xk|{xk\u22121, ci}) p({xk\u22121, ci}|Zk\u22121) dxk\u22121\n(4)\nApplying Bayes\u2019 rule, the (normalized) posterior is calcu-\nlated using the following equation:\np({xk, ci}|Zk) \u221d p(zk|{xk, ci}) p({xk, ci}|Zk\u22121) (5)\nEquations (4) and (5) form a recursive estimation for track-\ning the ith target.\nIt is also possible to write a recursive update of the class\nprobability with the following expression:\np(ci|Zk) \u221d \u03bb\ni\nk p(ci|Zk\u22121) (6)\nwhere \u03bbik = p(zk|Zk\u22121, ci) is the likelihood function of\nclass i.\nThe optimal solution for joint target tracking and classi-\nfication is a bank of class-matched filters that, in absence of\nspecific feature observations, is characterized by different\nprediction models [24, 36]. Any combination of Bayesian\nFilter 1\nClass\u2212matched\nFilter 2\nClass\u2212matched\nFilter N\nClass\u2212matched\np(c1|Zk)\nzk\np(cN |Zk)\np(c2|Zk)\nC\nl\na\ns\ns\nP\nr\no\nb\na\nb\ni\nl\ni\nt\ny\nC\na\nl\nc\nu\nl\na\nt\ni\no\nn\np({xk, c1}|Zk)\n\u03bb2k\n\u03bbNk\n\u03bb1k\np({xk, c2}|Zk)\np({xk, cN}|Zk)\nFig. 8 Schematic representation of a generic bank of filters.\nestimators can be used: for example, a BoF can be built us-\ning both Kalman and particle filters, as long as each of them\nprovides a class likelihood. The advantage is that the de-\nsigner can choose the most appropriate filters for a partic-\nular class of targets, depending on state evolution (i.e. pre-\ndiction model) and sensor used (i.e. observation model). At\nevery time step k, each filter outputs the likelihood \u03bbik of\nthe relative class ci which is used to update recursively the\nclass probabilities with (6). A typical bank of filters (BoF)\nis schematically represented in Fig. 8.\nGiven a BoF with standard Bayesian estimators, the only\nunknown quantities are the class likelihoods \u03bb1k, . . . , \u03bbNk ,\nwhich must be provided by the filters at each time step. For\nKalman filters, this quantity corresponds to the mode likeli-\nhood function [4, 36], which under linear-Gaussian assump-\ntions is a zero-centred Gaussian function:\n\u03bbik = N\n(\n\u03bdk;0,S\ni\nk\n) (7)\nwhere \u03bdk is the innovation term of the Kalman filter, i.e. the\ndifference between real and expected observation, and Sik is\nthe relative covariance. The same expression is also used as\nan approximation when the linear-Gaussian assumptions do\nnot hold.\n5.2 System Architecture\nThe joint state in our system contains the vector xk, which\nconsists of position (xk, yk), height zk, orientation \u03c6k and\nvelocity vk, plus the attribute ci representing the identity of\nthe human target (label).\nPrevious target classification solutions based on BoF use\na different prediction model for each estimator, chosen to\nbest fit the relative motion behaviour. In the current system,\nhuman recognition is performed both at prediction and ob-\nservation level. Every estimator of our BoF differs from each\nother on the zk component of the prediction model, so to re-\nflect the expected height of a known person. Each filter is\n7Filter 1\nFilter 0\nFilter N\nDetector\nRecogn. 1\nRecogn. N\nSENSOR\nDB\n\u03bbNk\nx\nN\nk\n\u03bb1k\n\u03bb0k\nx\n1\nk\nx\n0\nk\np\n(c\ni|\nZ\nk\n)\n\u221d\n\u03bb\ni k\np\n(c\ni|\nZ\nk\n\u2212\n1\n)\nx\nk\n=\n\u2211 N i\n=\n0\np\n(c\ni|\nZ\nk\n)\nx\ni k\nFig. 9 Bank of filters for joint people tracking and identification.\nthen updated with target-specific measures of the person\u2019s\nidentity given by face and clothes recognition.\nThe implementation adopts a modular approach where\ndetectors, used to measure the human position (i.e. legs de-\ntector and face detector), are integrated with recognizers,\nwhich measure the similarity between current human fea-\ntures and information stored in a database (i.e. clothes rec-\nognizer and face recognizer). The system is schematically\nillustrated in Fig. 9, where xik is the state estimated by the\nith filter, and \u03bbik is the relative class likelihood. In case the\nidentity information is unavailable, our system includes an\nadditional estimator, called zero-filter, that outputs the like-\nlihood \u03bb0k of a person to be unknown. This is discussed later\nin Section 5.3.5.\nRaw sensor data from laser and camera are processed\nby the detectors. The extracted information, which is de-\nscribed by opportune observation models, is sent to all the\nfilters. The current implementation includes a laser-based\nlegs detector and a vision-based face detector, but the sys-\ntem could be easily extended to include other sensors (e.g.\nsonar, microphone, etc.) and detection algorithms (e.g. mo-\ntion, sound, etc.).\nSensor data are also used by recognizers, each one specif-\nically designed to identify a particular person. These rec-\nognizers, described too by observation models, can access\na database of known subjects and provide the filters with\nidentity information. Unlike detectors however, each recog-\nnizer can only serve one estimator, as shown in Fig. 9. In\nour system, the database contains height, color histogram\n(of the torso) and face of each subject. The first one is used\nin the prediction model of each filter, the other two during\nthe update step of the estimation. Other recognizers could be\nadded in case more sensors and recognition algorithms were\navailable (e.g. voice, gait, etc.).\nAt every time step k, all the filters are updated with the\ncurrent observations, and the identity probabilities are com-\nputed with (6). The actual output of the BoF is a mixture\nof probability densities, not necessarily Gaussian, the mean\nand covariance of which are calculated as follows [4]:\nxk =\nN\u2211\ni=0\np(ci|Zk) x\ni\nk (8)\nPk =\nN\u2211\ni=0\np(ci|Zk)\n[\nPik +\n(\nxik \u2212 xk\n) (\nxik \u2212 xk\n)T ] (9)\nEquations (8) and (9) are the current state of the human tar-\nget and its covariance. In case of multiple people, xk and Pk\nare estimated for each person being tracked and also used to\nassign new observations to the proper targets using Nearest\nNeighbour data association [3, 7].\nSince the number of estimators and recognizers needed\nis equal to the number of subjects in the database, the max-\nimum size of a BoF depends on the available computing re-\nsources. Using the Unscented Kalman Filter (UKF), which\nin [6] showed to provide fast and accurate estimations for\npeople tracking, our system can run in real-time on a PIII\n800 MHz (on-board PC of the Pioneer robot), tracking and\nidentifying several people at the same time. The proposed\narchitecture could accommodate other Bayesian estimators,\nincluding particle filters, and deal with a large database of\nknown people, provided sufficient computing resources are\navailable.\n5.3 Implementation\nThe choice of the best Bayesian estimator for the BoF is\nfundamental. The standard Kalman filter [3] provides an ef-\nficient way to integrate different sensor data and, in case of\nlinear systems with Gaussian noise, it is known to be op-\ntimal. An Extended Kalman Filter (EKF) can be used to\nprovide approximate solutions in case of non-linearities, al-\nthough most of the recent approaches for tracking people\nare based on particle filters [14, 39] because they are not\nconstrained by any linear or Gaussian assumption. Unfor-\ntunately, in terms of computational cost, particle filters can\nbe very demanding and pose serious constraints in case of\nBoFs or multiple people tracking.\n5.3.1 Unscented Kalman Filter\nThe estimator adopted for our system is the UKF [27]. In-\nstead of the first-order linearization used by the EKF, the\nUKF captures mean and covariance of the probability distri-\nbutions with carefully chosen weighted points. Differently\nfrom particle filters, these points are not randomly sampled\nand their weights do not have to sum up to one. Also, the\nnumber of points used by the UKF is small enough (twice\nthe size of the state vector, plus one) to make this estima-\ntor particularly suitable for real-time applications of mobile\nrobots with limited hardware resources.\n8yes\nyes\nno\npredict\nlegs\ndetected?\nface\ndetected?\nupdate\nupdate\nxk\u22121\nno\nupdate\nupdate\nyes\nyes\nxk\nface\nprocessed?\nno\nclothes\nprocessed?\nlegs\ndetection\nface\nface\nclothes\ndetection\nrecognition\nrecognition\nno\nFig. 10 Sequential update of the UKF.\nIn case of asynchronous and uncorrelated measurements,\nthe UKF can be updated sequentially using only the obser-\nvations that are currently available. If all of them are ready at\nthe same time, a sequential update of the filter, starting from\nthe most to the least accurate sensor, gives a better estimate\nfor non-linear systems and is computationally more efficient\n[3]. A diagram showing the sequential estimation process of\na single UKF is shown in Fig. 10.\nThe UKF\u2019s state vector is xk = [xk, yk, zk, \u03c6k, vk]T , al-\nready defined in Section 5.2. The observation space is con-\nstituted by bearing bk and range rk from legs detection, bear-\ning \u03b1k and elevation \u03b2k from face detection, bearing \u03d5k and\nhistogram distance dk from clothes recognition, and differ-\nence \u03bek from face recognition. The models described next\nconsider human motion relative to the local coordinate frame\nof the robot, the position of which is given by odometry.\nNote that in order to estimate the absolute human position,\nthe robot should be provided with an accurate localization\nsystem. However, since in this case only the robot\u2019s frame\nof reference is considered, tracking is not affected by the cu-\nmulative error of odometry. Furthermore, the odometry error\nbetween two consecutive estimations is usually very small\nand can be safely included in the noises of the observation\nmodels [7].\n5.3.2 Prediction Model\nThe model used to described the motion of a walking per-\nson is a variant of the standard constant-velocity model and\nconsists of the following equations [7]:\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nxk = xk\u22121 + xk\u22121 \u2206tk cos \u03c6k\u22121\nyk = yk\u22121 + vk\u22121 \u2206tk sin \u03c6k\u22121\nzk = z(ci) + n\nz\nk\u22121\n\u03c6k = \u03c6k\u22121 + n\n\u03c6\nk\u22121\nvk = |vk\u22121|+ n\nv\nk\u22121\n(10)\nwhere \u2206tk = tk \u2212 tk\u22121 is the time interval between two\nconsecutive predictions. Supposing a person can only walk\nforward, the velocity vk is assumed to be always positive.\nThe noises nzk\u22121, n\n\u03c6\nk\u22121 and nvk\u22121 are all zero-mean Gaus-\nsians.\nAs shown later in the experiments, height information\ncan improve the recognition rate. In order to recognize peo-\nple from their height, the prediction models of the estimators\nfor the BoF differ from each other on the zk component. The\npredicted zk is a constant z(ci) available from the database\n(plus a noise term), which is the known height of the ith\nsubject. Except in case the heights of two or more people\nare the same, this equation makes every prediction model\nunique, in a way conceptually similar to target classification\nwith different motion models.\nNote that height zk does not evolve over time and there-\nfore could be simply modeled in the likelihood function for\nrecognition purposes. However, in the current implementa-\ntion zk is part of the state vector for consistency with our\nprevious work [7] and, most of all, with the zero-filter ex-\nplained in Section 5.3.5, where the height does actually evolve\nover time. This choice simplifies also the software imple-\nmentation of the ROI selection for clothes recognition.\n5.3.3 Observation Models of the Detectors\nThe measurements provided by our laser-based legs detec-\ntion are bearing bk and range rk. The legs observation model\ncan be therefore written as follows:\uf8f1\uf8f4\uf8f2\n\uf8f4\uf8f3\nbk = tan\n\u22121\n[\nyk \u2212 l\ny\nk\nxk \u2212 lxk\n]\n\u2212 l\u03c6k + n\nb\nk\nrk =\n\u221a\n(xk \u2212 lxk)\n2\n+ (yk \u2212 l\ny\nk)\n2\n+ nrk\n(11)\nwhere the noises nbk and nrk are zero-mean Gaussians. The\nquantities lxk , l\ny\nk and l\n\u03c6\nk are correction terms taking into ac-\ncount the current position and orientation of the robot from\nodometry, as well as the displacement of the laser device\nwith respect to its frame of reference [7].\n9The face observation model takes into account the an-\ngles \u03c8k and \u03b8k of the camera\u2019s pan and tilt respectively. The\nrelative equations can be written as follows:\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n\u03b1k = tan\n\u22121\n[\nyk \u2212 c\ny\nk\nxk \u2212 cxk\n]\n\u2212 c\u03c6k \u2212 \u03c8k + n\n\u03b1\nk\n\u03b2k = \u2212tan\n\u22121\n\uf8ee\n\uf8f0 zk \u2212 czk\u221a\n(xk\u2212cxk)\n2\n+(yk\u2212c\ny\nk)\n2\n\uf8f9\n\uf8fb\u2212\u03b8k+n\u03b2k\n(12)\nEven in this case, the noises n\u03b1k and n\n\u03b2\nk are zero-mean Gaus-\nsians, while cxk , c\ny\nk, c\nz\nk and c\n\u03c6\nk are correction terms depending\non the robot and camera\u2019s position. Further details on the ob-\nservation models (11) and (12) can be found in [7].\n5.3.4 Observation Models of the Recognizers\nTo integrate the class likelihood (7) relative to face and clothes\nrecognition, the histogram distance dk and the Eigenface\ndifference \u03bek are included as noisy constants in the follow-\ning observation models, thus providing additional informa-\ntion to update the identity probability. This solution could\nbe improved implementing likelihood functions where his-\ntogram and face recognition errors are modeled from the\ncurrent state vector [35, 44].\nBesides the value of the histogram distance, the proce-\ndure for clothes recognition provides the person\u2019s direction\nwith respect to the camera. The relative observation model\nincludes therefore the bearing \u03d5k of the torso centre and the\ndistance dk of its color histogram, modeled as follows:\uf8f1\uf8f2\n\uf8f3\n\u03d5k = tan\n\u22121\n(\nyk \u2212 c\ny\nk\nxk \u2212 cxk\n)\n\u2212 c\u03c6k + n\n\u03d5\nk\ndk = d(ci) + n\nd\nk\n(13)\nwhere d(ci) is the histogram distance for the ith subject in\nthe database, null in case of perfect match ( d(ci) = 0 ), and\nthe noises n\u03d5k and ndk are zero-mean Gaussians with param-\neters empirically determined. The quantities cxk , c\ny\nk and c\n\u03c6\nk\nare the same correction terms used in (12).\nNote that in (13) the elevation angle has not been in-\ncluded, although it could be calculated from the best match\nposition (u\u2217, v\u2217) of the histogram detection. The reason is\nthat, when close to a person, our robots can observe only\nthe top part of the torso, from the chest up to the head. In\nthis case, the elevation measure would inevitably fall on a\nlocation higher than the actual centre of the torso. The prob-\nlem would influence the zk estimate (and hence the height\nrecognition), so we prefer to rely exclusively on the eleva-\ntion provided by face detection.\nThe best performance of the Eigenface algorithm can\nonly be achieved under controlled conditions. Unfortunately,\nin real applications faces have various postures and expres-\nsions that make their recognition very difficult. Therefore, in\nthe current implementation the main purpose of face recog-\nnition is to enhance the identification performance, without\nactually reducing the uncertainty of single UKFs. Note how-\never that face recognition does influence the output of the\nBoF, since the final estimate in (8) and (9) is weighted by\nidentity probabilities.\nThe face recognition measure from Eigenface is mod-\neled as follows:\n\u03bek = \u03be(ci) + n\n\u03be\nk (14)\nwhere \u03be(ci) is the value obtained for the ith face in the\ndatabase in case of perfect match ( \u03be(ci) = \u22121 ). For sim-\nplicity, the noise n\u03bek is assumed to be normally distributed,\nwith standard deviation determined by a number of tests\nwith different faces.\nBecause both the recognition algorithms depends on face\ndetection, the noises ndk and n\n\u03be\nk are actually correlated. How-\never, this is true only for the initialization stages, while the\nsubsequent refinements (best histogram match and face nor-\nmalization) are performed independently. Therefore, in our\nopinion, the assumption of uncorrelated measures is a justi-\nfied simplification. This accommodates also the use of Kalman\nestimators and proved to work reasonably well for our appli-\ncation.\n5.3.5 Zero-filter\nThe main purpose of our system is to track and recognize a\ncertain number of known people. However, there are situa-\ntions in which the robot has to deal with uncertain or missing\nhuman data, in particular:\n\u2013 the database has no information about the person cur-\nrently being tracked;\n\u2013 the information about this person is incomplete or incor-\nrect;\n\u2013 the person cannot be recognized because outside the cam-\nera\u2019s field of view.\nWithout considering the probability of a subject to be un-\nknown, the BoF would assign wrong identity probabilities\nto the N available subjects. Our system includes therefore\nan additional estimator, the zero-filter, which has the func-\ntion to track and identify unrecognized subjects.\nObviously, the (nearly) constant zk component in (10)\ndoes not apply to the zero-filter, but uses instead the follow-\ning height prediction:\nzk = zk\u22121 + n\nz\nk\u22121 (15)\nLike all the other estimators, the zero-filter is corrected\nby anonymous legs and face detections. However, since it\ndoes not hold any histogram information, clothes observa-\ntions are substituted by \u201cvirtual\u201d measurements including\nthe predicted bearing \u03d5\u02c6k and a constant histogram distance\n10\nOFFICE 2\nLABORATORY\nLIFT\nROBOT ARENA\nOFFICE 1\nCO\nRR\nID\nO\nR\n\u223c\n2\n2\nm\nFig. 11 Floor plan of the test environment.\nd\u02c6 = 2 \u03c3d (i.e. twice the standard deviation of the noise ndk).\nThis sets an adaptive threshold on the clothes observation,\nassuring that only good histogram detections influence the\nidentity probability. A similar approach is used for face recog-\nnition, the measure of which is set to \u03be\u02c6 = 2 \u03c3\u03be (i.e. twice the\nstandard deviation of the noise n\u03bek) for the relative update of\nthe zero-filter.\n6 Experimental Results\nThe system has been implemented in C++ and runs in real-\ntime on the embedded PCs of two mobile robots, a Pioneer\n2 DX and a Scitos G5, both equipped with a camera on the\ntop and a laser sensor on the bottom. The experimental sce-\nnario includes a typical office environment with cluttered\nrooms and a narrow corridor, which are illustrated in Fig. 11.\nThe data used for the current evaluation have been collected\nwith our robots following and approaching one or more per-\nsons in different rooms. The first part of the experiments\npresent tracking and recognition results using only height\nand clothes observations. The integration of face recogni-\ntion is evaluated in the last part.\nSince we are particularly interested in recognizing hu-\nmans who are willing to interact with the robot, most of the\ncases illustrated next refer to people facing its camera. Note\nhowever that a variety of situations has been covered during\nthe experiments, with people often moving randomly in the\nenvironment. In general, the face had to be visible only a few\ninstants for the BoF\u2019s recognition to converge successfully.\nOnce recognized, people were correctly identified as long as\nthey were tracked, even when outside the camera\u2019s field of\nview.\n1.68m 1.60m 1.77m 1.53m 1.68m\nFig. 12 Some examples of clothes and heights from the database of\nknown subjects.\n6.1 Simultaneous People Tracking and Recognition\nThe situation described next illustrates a typical case of si-\nmultaneous people tracking and recognition, executed in real-\ntime with the Scitos robot. Besides legs and face detectors,\nheight and clothes recognizers were used for human identi-\nfication. The database has been created manually including\nhistograms and height information relative to 13 different\nsubjects, so the BoF consists of 14 UKFs (one is the zero-\nfilter). Some of the clothes worn by people during the ex-\nperiment are shown in Fig. 12, together with their relative\nheight information. Note that some of the subjects had sim-\nilar clothes and heights, which made the recognition partic-\nularly challenging.\nThe snapshots in Fig. 13 illustrate a few moments of the\nexperiment, with the robot approaching subjects A and B.\nEach figures includes, from the robot\u2019s point of view, face\nand legs detection on the top, together with clothes recog-\nnition and position estimates on the bottom. In this particu-\nlar case, the robot was programmed to move autonomously\nand perform simple interactions. If there were no people, the\nrobot simply wandered in the environment avoiding obsta-\ncles. If one or more persons were detected, the robot tracked\nand approached them; once in proximity, it stopped to en-\ngage them in audio-visual interactions.\nThe graphs in Fig. 14 show the temporal evolution of\nthe identity probabilities for subjects A and B. From Fig.\n14(a), it can be noted that subject B was correctly recog-\nnized, but not immediately. In this case, the person was de-\ntected and tracked with the laser for a few seconds, before\nbeing actually observed by the camera. The identity prob-\nability of the zero-filter was therefore the highest one un-\ntil t ' 32 s, when eventually the probability of subject B\ntook over. The graph in Fig. 14(b) shows instead that sub-\nject A, visible by the camera since his first detection, was\npromptly recognized by the robot at time t ' 63 s. The\nidentity probability of subject A approached immediately 1,\nwhile the unknown identity of the zero-filter dropped to a\nminimum threshold value close to zero ( pmin = e\u2212100 ). A\nvideo of the experiment is available at the following address:\nhttp:\/\/robots.lincoln.ac.uk\/users\/nbellotto\/videos\/soro.mpg .\n11\n(a) Subjects B at time t = 33.4 s.\n(b) Subject A at time t = 65.3 s.\nFig. 13 Simultaneous people tracking and identification. A, B, and C\nare the identities of the subjects being tracked by the robot R.\n6.2 Evaluation of Height and Clothes Recognition\nThe success of height and clothes recognition depends on\nthe quality of the tracking estimate. Several recognition ex-\nperiments have been therefore conducted with people and\nrobot moving across different rooms. The results in Fig. 15\nshow the recognition performance for approximately 10 min-\nutes of recorded data, during which 13 different people have\nbeen followed and approached by the robot in 30 different\noccasions. The chart indicates the correct and wrong recog-\nnition rates, computed by the number of successful and error\ncases out of the total number of observed people. A sub-\nject was considered recognized when the relative identity\nprobability reached at least 0.9. The case where the person\nwas identified as unknown is also reported. The recognition\nperformances have been compared first using height, then\nclothes and finally the combination of both. From the re-\nsults, it can be seen that their integration led to a more re-\n24 25 26 27 28 29 30 31 32 33 34\n0\n0.2\n0.4\n0.6\n0.8\n1\npr\nob\nab\nili\nty\n \n \ntime [s]\nB\n?\n(a) Identity of subject B in Fig. 13(a).\n62 63 64 65 66 67 68 69 70 71 72\n0\n0.2\n0.4\n0.6\n0.8\n1\ntime [s]\npr\nob\nab\nili\nty\n \n \nA\n?\n(b) Identity of subject A in Fig. 13(b).\nFig. 14 Identity probabilities. The thick line is the probability of the\nsubject\u2019s identity, the dashed line is the probability of being unknown\nfrom the zero-filter. The identity probabilities from the other filters are\nalmost always null and omitted for clarity.\nFig. 15 Recognition performance using height and clothes informa-\ntion.\nliable estimation of the human identity, with a significant\nimprovement on the number of successful recognitions and,\nat the same time, a substantial reduction of error cases.\nNote in particular the increment of approximately 15%\nin the recognition rate when both clothes and height were\nused, compared to the results of clothes only. Although height\n12\n(a) Tracking and recognition of subject A and E in Office 1.\n(b) Recognition error in the robot arena.\nFig. 16 Recognition failure due to different lighting conditions.\nwas not informative per se, the relative bars in Fig. 15 show\nthere was a majority of indecision cases (unknown person)\nwhere the zero-filter prevailed. Many of these cases went\neventually in favor of the correct recognition once \u201cboosted\u201d\nby clothes recognition.\nMost of the recognition errors were due to considerable\nlight variations during the experiments, like the situation il-\nlustrated in Fig. 16. A couple of persons, subject A and E,\nwere initially tracked and correctly recognized starting in-\nside Office 1, as shown in Fig. 16(a). Unfortunately, a few\nminutes later in the Robot Arena, the system failed to rec-\nognize the same people. This was because the particular\nhalogen lamps of the Robot Arena modified significantly\nthe color histograms of the clothes, making their recogni-\ntion very unreliable. As shown in Fig. 16(b), subject A was\nidentified as a completely different person (i.e. subject H).\nThanks to the zero-filter, instead, subject E was identified as\nunknown, an error that can be considered more acceptable\nthan the previous one. This problem suggests however that\nfurther improvements are needed to make clothes recogni-\ntion more robust to lighting conditions.\n6.3 Identification of Unknown People\nThe main task of the zero-filter is to track and identify an\nunknown person when there is not sufficient information\nto recognize him\/her. Without this additional estimator, the\nsubject would be otherwise confused with the most similar\nperson in the robot\u2019s database. An example of recognition\nwith and without zero-filter is shown in Fig. 17. The graph\nreports the identity probability estimated for a subject B,\nduring the experiment in Section 6.1, removing the relative\nheight and clothes information from the database. The first\n30 32 34 36 38 40 42 44 46 48 50\n0\n0.2\n0.4\n0.6\n0.8\n1\npr\nob\nab\nili\nty\n \n \ntime [s]\nA\nC\nF\n?\n(a) Identity with zero-filter.\n30 32 34 36 38 40 42 44 46 48 50\n0\n0.2\n0.4\n0.6\n0.8\n1\npr\nob\nab\nili\nty\n \n \ntime [s]\nA\nC\nF\n(b) Identity without zero-filter.\nFig. 17 Identity probability of an unknown person (subject B) with\nand without zero-filter.\ngraph refers to the case with zero-filter and shows that the\nBoF can identify correctly subject B as unknown. The sec-\nond graph shows instead that without zero-filter the maxi-\nmum identity probability switches erroneously between three\ndifferent subjects (A, C and F).\nThe convergence speed of the probability could be \u201ctuned\u201d\nincreasing or decreasing the noises of the observation mod-\nels. However, the use of the zero-filter permits to have the\ndesirable property of fast, yet correct convergence. This is\na necessary condition for real-world robot applications, in\nparticular when a few seconds delay can completely spoil\nhuman-robot interactions.\nThe effectiveness of the zero-filter was also evaluated\nusing the same data recorded for the experiments in Section\n6.2. The same trial has been repeated several times, every\ntime removing one of the 13 people from the database, and\nchecking if the missing subject was actually identified as\nunknown. The error in this case was less than 7%, that is,\nthe robot confused the unknown subjects with someone else\nin the database only in 2 occasions out of 30.\n6.4 Tracking Errors\nRecognition can improve human tracking reducing the un-\ncertainty of the estimation and recovering from occasional\ndata association errors. In this experiment, a quantitative\nevaluation of the tracking robustness has been conducted\ncomparing the number of errors occurred with and with-\nout human recognition, using respectively BoFs and sim-\n13\nFig. 18 Comparison of tracking errors with normal UKF (no recogni-\ntion) and BoF (with recognition).\nple UKFs (one for each target). The parameter used is the\ntotal number or tracking errors, each one corresponding to\none of the following cases: a) the track hypothesis deviates\nfrom the correct trajectory of the human target and is even-\ntually deleted by the system; b) the track \u201cjumps\u201d to an ad-\njacent object due to a false positive in the human detection;\nc) the track shifts to another person (close to the original\none) because of data association errors. We considered ap-\nproximately 20 minutes of data recorded with our robots on\na number of trials. The test scenario included 13 people in 6\ndifferent locations, illustrated in Fig. 11, with various light-\ning conditions and levels of clutter.\nThe chart in Fig. 18 shows that, compared to the stan-\ndard UKF, the number of tracking failures decreased by 30%\nwith our new solution. We consider also the situation where\ntwo track hypotheses switched because of data association\nerrors, but the BoF promptly recovered their correct iden-\ntity labels. Without counting these cases, automatically cor-\nrected by the system, the number of actual failures drops\neven further, as shown by the last column of the chart.\n6.5 Integration of Face Recognition\nIn this section we analyze the identification performance us-\ning also face recognition. The experiments carried out are\nsimilar to the previous ones, but in this case only the Sci-\ntos robot was used, due to the computational burden of face\nrecognition. The results described next refer to approximately\n5 minutes of data, simultaneously tracking and recognizing\nup to 8 different people. Some of the faces contained in the\nrobot\u2019s database, scaled to 24 \u00d7 24 pixels and normalized,\nare shown in Fig. 19.\nFig. 19 Some of the faces in the database of known people.\n(a) Misrecognition of the left person, whose correct identity is J.\n(b) Person on the top-left not yet identified (correct identity is G).\nFig. 20 Some examples of tracking and recognition with BoFs.\nDespite face recognition, in a few cases the robot failed\nto identify some people wearing similar clothes. During the\nsituation depicted in Fig. 20, for example, two persons were\nsometimes misrecognized because they had similar (brown)\njackets. This happened in particular when the face of the\nsubject was not clearly visible by the robot.\nNevertheless, including face recognition, the performance\nof the system was generally better than the previous case\n(only height and clothes), especially on the number of suc-\ncessful identifications. The chart in Fig. 21 shows indeed\nthat the amount of successful identifications increased com-\nbining the three modalities (height, clothes and face), while\nthe number of errors remained unchanged. The performance\nof the system using only height and face recognition, in-\nstead, was not very reliable because the poor performance\nof Eigenface on low-resolution images was often worsened\nby occlusions and particular head postures.\n7 Conclusions and Future Work\nIn this paper, an original solution for multimodal human per-\nception with mobile robots has been presented using multi-\nsensor detection and data fusion. A robust histogram com-\nparison for clothes recognition has been illustrated, which\ntakes into account the uncertainty of the target estimate to\nmaximizes the histogram match and determine the position\n14\nFig. 21 Recognition performance using face, clothes and height.\nof the human torso. A fast technique to process and normal-\nize images for face recognition has also been proposed.\nThe major contribution of this work lies in the new ar-\nchitecture for simultaneous human tracking and recognition,\nwhich is based on a bank of UKFs to combine different al-\ngorithms and sensing modalities, and includes a dedicated\nfilter for the identification of unknown persons. Experiments\nin real scenarios prove the effectiveness of our solutions and\nshow that the robot perception of humans can be improved\nfusing tracking to height, clothes and face recognition.\nThe proposed system could be ameliorated in a number\nof ways, in particular with a robust face recognition algo-\nrithm and a solution to deal with the scalability issue. Our\nfuture research will focus also on the integration of sen-\nsor data using higher representation levels, providing service\nrobots with semantic information about human appearance\nand behaviours. The objective is to make these robots capa-\nble of \u201cunderstanding\u201d what (and who) they are perceiving\nby means of real-time AI techniques.\nReferences\n1. Arras, K. O., Mozos, O. M., and Burgard, W. (2007). Us-\ning boosted features for the detection of people in 2d range\ndata. In Proc. of IEEE Int. Conf. on Robotics and Automa-\ntion (ICRA), pages 3402\u20133407, Rome, Italy.\n2. Asoh, H., Vlassis, N., Motomura, Y., Asano, F., Hara, I.,\nHayamizu, S., Ito, K., Kurita, T., Matsui, T., Bunschoten,\nR., and Kro\u00a8se, B. (2001). Jijo-2: An office robot that com-\nmunicates and learns. IEEE Intelligent Systems, 16(5):46\u2013\n55.\n3. Bar-Shalom, Y. and Li, X. R. (1995). Multitarget-\nMultisensor Tracking: Principles and Techniques. Y. Bar-\nShalom.\n4. Bar-Shalom, Y., Li, X. R., and Kirubarajan, T. (2001).\nEstimation with Applications to Tracking and Navigation.\nWiley.\n5. Bellotto, N. and Hu, H. (2007a). Multisensor data fu-\nsion for joint people tracking and identification with a ser-\nvice robot. In Proc. of IEEE Int. Conf. on Robotics and\nBiomimetics (ROBIO), pages 1494\u20131499, Sanya, China.\n6. Bellotto, N. and Hu, H. (2010). Computationally Effi-\ncient Solutions for Tracking People with a Mobile Robot:\nan Experimental Evaluation of Bayesian Filters. Au-\ntonomous Robots, 28(4):425\u2013438.\n7. Bellotto, N. and Hu, H. (2009). Multisensor-based hu-\nman detection and tracking for mobile service robots.\nIEEE Trans. on Systems, Man, and Cybernetics \u2013 Part B,\n39(1):167\u2013181.\n8. Bennewitz, M., Burgard, W., Cielniak, G., and Thrun, S.\n(2005). Learning motion patterns of people for compli-\nant robot motion. The Int. Journal of Robotics Research,\n24(1):31\u201348.\n9. Bennewitz, M., Burgard, W., and Thrun, S. (2002).\nLearning motion patterns of persons for mobile service\nrobots. In Proc. of IEEE Int. Conf. on Robotics and\nAutomation (ICRA), pages 3601\u20133606, Washington, DC,\nUSA.\n10. Bennewitz, M., Cielniak, G., and Burgard, W. (2003).\nUtilizing learned motion patterns to robustly track per-\nsons. In Proc. of IEEE Int. W. on VS-PETS, pages 102\u2013\n109, France.\n11. Beveridge, R., Bolme, D., Teixeira, M., and Draper, B.\n(2003). The CSU Face Identification Evaluation System\nUser\u2019s Guide: Version 5.0. Computer Science Depart-\nment, Colorado State University.\n12. Beymer, D. and Konolige, K. (2001). Tracking people\nfrom a mobile platform. In IJCAI Workshop on Reasoning\nwith Uncertainty in Robotics, Seattle, WA, USA.\n13. Blanco, J., Burgard, W., Sanz, R., and Ferna\u00b4nez, J.\n(2003). Fast face detection for mobile robots by integrat-\ning laser range data with vision. In Proc. of the Int. Conf.\non Advanced Robotics (ICAR), volume 2, pages 953\u2013958,\nCoimbra, Portugal.\n14. Chakravarty, P. and Jarvis, R. (2006). Panoramic vision\nand laser range finder fusion for multiple person tracking.\nIn Proc. of IEEE\/RSJ Int. Conf. on Intelligent Robots and\nSystems (IROS), pages 2949\u20132954, Beijing, China.\n15. Cielniak, G. and Duckett, T. (2003). Person identifica-\ntion by mobile robots in indoor environments. In Proc.\nof the IEEE Int. Workshop on Robotic Sensing (ROSE),\n\u00a8Orebro, Sweden.\n16. Cielniak, G. and Duckett, T. (2004). People recognition\nby mobile robots. In Proc. of AILS 2nd Joint SAIS\/SSLS\n15\nWorkshop, Lund, Sweden.\n17. Comaniciu, D., Ramesh, V., and Meer, P. (2000). Real-\ntime tracking of non-rigid objects using mean shift. In\nProc. of IEEE Conf. on Computer Vision and Pattern\nRecognition (CVPR), volume 2, pages 142\u2013149, SC, USA.\n18. Cunado, D., Nixon, M. S., and Carter, J. N. (2003). Au-\ntomatic extraction and description of human gait models\nfor recognition purposes. Comput. Vis. Image Underst.,\n90(1):1\u201341.\n19. Dautenhahn, K. (1995). Getting to Know Each Other\n- Artificial Social Intelligence for Autonomous Robots.\nRobotics and Autonomous Systems, 16:333\u2013356.\n20. Fasel, I., Fortenberry, B., and Movellan, J. (2005). A\ngenerative framework for realtime object detection and\nclassification. Computer Vision and Image Understand-\ning, 98:182\u2013210.\n21. Feyrer, S. and Zell, A. (2000). Robust real-time pursuit\nof persons with a mobile robot using multisensor fusion.\nIn Proc. of the 6th Int. Conf. on Intelligent Autonomous\nSystems (IAS), pages 710\u2013715, Venice, Italy.\n22. Fong, T., Nourbakhsh, I., and Dautenhahn, K. (2003).\nA survey of socially interactive robots. Robotics and Au-\ntonomous Systems, 42(3-4):143\u2013166.\n23. Fritsch, J., Kleinehagenbrock, M., Lang, S., Plo\u00a8tz, T.,\nFink, G. A., and Sagerer, G. (2003). Multi-modal an-\nchoring for human-robot-interaction. Robotics and Au-\ntonomous Systems, 43(2-3):133\u2013147.\n24. Gordon, N. J., Maskell, S., and Kirubarajan, T. (2002).\nEfficient particle filters for joint tracking and classifica-\ntion. In Proc. of Signal and Data Processing of Small Tar-\ngets (SPIE), pages 439\u2013449, FL, USA.\n25. Gorodnichy, D. (2003). Facial recognition in video. In\nProc. of Int. Conf. on Audio- and Video-Based Biometric\nPerson Authentication (AVBPA), pages 505\u2013514, Guild-\nford, United Kingdom.\n26. Jain, A. K., Ross, A., and Prabhakar, S. (2004). An in-\ntroduction to biometric recognition. IEEE Trans. on Cir-\ncuits and Systems for Video Technology, 14(1):4\u201320.\n27. Julier, S. J. and Uhlmann, J. K. (2004). Unscented\nfiltering and nonlinear estimation. Proc. of the IEEE,\n92(3):401\u2013422.\n28. Li, G., Cai, X., Li, X., and Liu, Y. (2006). An efficient\nface normalization algorithm based on eyes detection. In\nProc. of IEEE\/RSJ Int. Conf. on Intelligent Robots and\nSystems (IROS), pages 3843\u20133848, Beijing, China.\n29. Lindstro\u00a8m, M. and Eklundh, J.-O. (2001). Detecting\nand tracking moving objects from a mobile platform using\na laser range scanner. In Proc. of IEEE\/RSJ Int. Conf. on\nIntelligent Robots and Systems (IROS), volume 3, pages\n1364\u20131369, Maui, HI, USA.\n30. Liu, J. N. K., Wang, M., and Feng, B. (2005). iBot-\nGuard: an internet-based intelligent robot security sys-\ntem using invariant face recognition against intruder.\nIEEE Trans. on Systems, Man, and Cybernetics (Part C),\n35(1):97\u2013105.\n31. Martin, C., Schaffernicht, E., Scheidig, A., and Gross,\nH.-M. (2005). Sensor fusion using a probabilistic aggrega-\ntion scheme for people detection and tracking. In Proc. of\nthe 2nd European Conference on Mobile Robots (ECMR),\npages 176\u2013181, Ancona, Italy.\n32. Minvielle, P., Marrs, A., Maskell, S., and Doucet, A.\n(2005). Joint Target Tracking and Identification - Part II:\nShape video computing. In Proc. of the 8th Int. Conf. on\nInformation Fusion, Philadelphia, PA, USA.\n33. Nakajima, C., Pontil, M., Heisele, B., and Poggio, T.\n(2003). Full-body person recognition system. Pattern\nRecognition, 36(9):1997\u20132006.\n34. Nourbakhsh, I., Kunz, C., and Willeke, T. (2003). The\nmobot museum robot installations: a five year experiment.\nIn Proc. of IEEE\/RSJ Int. Conf. on Intelligent Robots and\nSystems (IROS), pages 3636\u20133641.\n35. Pe\u00b4rez, P., Vermaak, J., and Blake, A. (2004). Data fu-\nsion for visual tracking with particles. Proc. of IEEE,\n92(3):495\u2013513.\n36. Ristic, B., Gordon, N., and Bessell, A. (2004). On target\nclassification using kinematic data. Information Fusion,\n5(1):15\u201321.\n37. Santana, M. C., Suarez, O. D., Canalis, L. A., and\nNavarro, J. L. (2008). Face and facial feature detection\nevaluation. In Proc. of the 3rd Int. Conf. on Computer Vi-\nsion Theory and Applications (VISAPP), pages 167\u2013172.\n38. Scheutz, M., McRaven, J., and Cserey, G. (2004). Fast,\nreliable, adaptive, bimodal people tracking for indoor en-\nvironments. In Proc. of IEEE\/RSJ Int. Conf. on Intelligent\nRobots and Systems (IROS), volume 2, pages 1347\u20131352,\nSendai, Japan.\n39. Schulz, D., Burgard, W., Fox, D., and Cremers, A. B.\n(2003). People Tracking with Mobile Robots Using\nSample-based Joint Probabilistic Data Association Filters.\nInt. Journal of Robotics Research, 22(2):99\u2013116.\n40. Turk, M. and Pentland, A. (1991). Eigenfaces for recog-\nnition. Journal of Cognitive Neuroscience, 3(1):72\u201386.\n41. Viola, P. and Jones, M. J. (2004). Robust real-time face\ndetection. Int. Journal of Computer Vision, 57(2):137\u2013\n154.\n42. Zajdel, W., Zivkovic, Z., and Kro\u02d8se, B. J. A. (2005).\nKeeping track of humans: Have I seen this person before?\nIn Proc. of IEEE Int. Conf. on Robotics and Automation\n(ICRA), pages 2093\u20132098, Barcelona, Spain.\n43. Zhao, W., Chellappa, R., Phillips, P. J., and Rosenfeld,\nA. (2003). Face recognition: A literature survey. ACM\nComput. Surv., 35(4):399\u2013458.\n44. Zhou, S. and Chellappa, R. (2002). Probabilistic hu-\nman recognition from video. In Proc. of the 7th European\nConference on Computer Vision (ECCV), pages 681\u2013697,\nLondon, UK. Springer-Verlag.\n16\nNicola Bellotto is a Lecturer in the School of Computer\nScience at the University of Lincoln and a member of the\nLincoln Robotics Group. He holds a PhD in Computer Sci-\nence from the University of Essex and a Laurea in Electronic\nEngineering from the University of Padua. Before joining\nthe University of Lincoln, he was a research assistant in\nCognitive Computer Vision at the University of Oxford. His\nresearch interests range from mobile robotics to cognitive\nperception, including sensor fusion, Bayesian estimation, robot\nvision and localization. He gained also several years of pro-\nfessional experience in the industry as software developer\nand embedded system programmer.\nHuosheng Hu is a Professor in the School of Computer\nScience and Electronic Engineering, University of Essex,\nUK, leading the Human Centred Robotics Group. His re-\nsearch interests include autonomous mobile robots, human-\nrobot interaction, evolutionary robotics, multi-robot collab-\noration, embedded systems, pervasive computing, sensor in-\ntegration, intelligent control and networked robotics. He has\npublished over 250 papers in journals, books and confer-\nences, and received a number of best paper awards. He has\nbeen a founding member of Networked Robots within the\nIEEE Robotics and Automation Technical Committee since\n2001. He was a General Co-Chair of IEEE International\nConference on Mechatronics and Automation, Harbin, China,\n2007, Publication Chair of IEEE International Conference\non Networking, Sensing and Control, London, 2007; Co-\nChair of Special & Organised Sessions of IEEE Interna-\ntional Conference on Robotics and Biomimetics, Sanya, China,\n2007; Chair for Special and Organised Sessions, IEEE\/ASME\nInternational Conference on Advanced Intelligent Mecha-\ntronics, Xi\u2019an, China, 2008, etc. Prof. Hu is currently Editor-\nin-chief for the International Journal of Automation and Com-\nputing. He is a reviewer for a number of international jour-\nnals such as IEEE Transactions on Robotics, SMC-Part B,\nAutomatic Control, Neural Networks and International Jour-\nnal of Robotics Research. Since 2000 he has been a Visiting\nProfessor at 6 universities in China - namely Central South\nUniversity, Shanghai University, Wuhan University of Sci-\nence and Engineering, Kunming University of Science and\nTechnology, Chongqing University of Post & Telecommuni-\ncation, and Northeast Normal University. Prof. Hu is a Char-\ntered Engineer, a senior member of IEEE and ACM, and a\nmember of IET and IAS.\n"}