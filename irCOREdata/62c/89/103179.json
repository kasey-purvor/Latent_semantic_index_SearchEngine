{"doi":"10.1117\/12.883799","coreId":"103179","oai":"oai:epubs.surrey.ac.uk:3038","identifiers":["oai:epubs.surrey.ac.uk:3038","10.1117\/12.883799"],"title":"Small-scale Anomaly Detection in Panoramic Imaging using Neural Models of Low-level Vision","authors":["Casey, MC","Hickman, DL","Pavlou, A","Sadler, JRE"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2011-04-25","abstract":"Our understanding of sensory processing in animals has reached the stage where we can exploit neurobiological principles in commercial systems. In human vision, one brain structure that offers insight into how we might detect anomalies in real-time imaging is the superior colliculus (SC). The SC is a small structure that rapidly orients our eyes to a movement, sound or touch that it detects, even when the stimulus may be on a small-scale; think of a camouflaged movement or the rustle of leaves. This automatic orientation allows us to prioritize the use of our eyes to raise awareness of a potential threat, such as a predator approaching stealthily. In this paper we describe the application of a neural network model of the SC to the detection of anomalies in panoramic imaging. The neural approach consists of a mosaic of topographic maps that are each trained using competitive Hebbian learning to rapidly detect image features of a pre-defined shape and scale. What makes this approach interesting is the ability of the competition between neurons to automatically filter noise, yet with the capability of generalizing the desired shape and scale. We will present the results of this technique applied to the real-time detection of obscured targets in visible-band panoramic CCTV images. Using background subtraction to highlight potential movement, the technique is able to correctly identify targets which span as little as 3 pixels wide while filtering small-scale noise","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:epubs.surrey.ac.uk:3038<\/identifier><datestamp>\n      2017-10-31T14:07:38Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      74797065733D636F6E666572656E63655F6974656D<\/setSpec><setSpec>\n      6469766973696F6E733D656E67616E64706879736963616C736369656E636573:436F6D707574696E67<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/epubs.surrey.ac.uk\/3038\/<\/dc:relation><dc:title>\n        Small-scale Anomaly Detection in Panoramic Imaging using Neural Models of Low-level Vision<\/dc:title><dc:creator>\n        Casey, MC<\/dc:creator><dc:creator>\n        Hickman, DL<\/dc:creator><dc:creator>\n        Pavlou, A<\/dc:creator><dc:creator>\n        Sadler, JRE<\/dc:creator><dc:description>\n        Our understanding of sensory processing in animals has reached the stage where we can exploit neurobiological principles in commercial systems. In human vision, one brain structure that offers insight into how we might detect anomalies in real-time imaging is the superior colliculus (SC). The SC is a small structure that rapidly orients our eyes to a movement, sound or touch that it detects, even when the stimulus may be on a small-scale; think of a camouflaged movement or the rustle of leaves. This automatic orientation allows us to prioritize the use of our eyes to raise awareness of a potential threat, such as a predator approaching stealthily. In this paper we describe the application of a neural network model of the SC to the detection of anomalies in panoramic imaging. The neural approach consists of a mosaic of topographic maps that are each trained using competitive Hebbian learning to rapidly detect image features of a pre-defined shape and scale. What makes this approach interesting is the ability of the competition between neurons to automatically filter noise, yet with the capability of generalizing the desired shape and scale. We will present the results of this technique applied to the real-time detection of obscured targets in visible-band panoramic CCTV images. Using background subtraction to highlight potential movement, the technique is able to correctly identify targets which span as little as 3 pixels wide while filtering small-scale noise.<\/dc:description><dc:date>\n        2011-04-25<\/dc:date><dc:type>\n        Conference or Workshop Item<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        attached<\/dc:rights><dc:identifier>\n        http:\/\/epubs.surrey.ac.uk\/3038\/2\/2011_casey_hickman_pavlou_sadler_anomaly_detection.pdf<\/dc:identifier><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/epubs.surrey.ac.uk\/3038\/4\/licence.txt<\/dc:identifier><dc:identifier>\n          Casey, MC, Hickman, DL, Pavlou, A and Sadler, JRE  (2011) Small-scale Anomaly Detection in Panoramic Imaging using Neural Models of Low-level Vision  In: SDisplay Technologies and Applications for Defense, Security, and Avionics V; and Enhanced and Synthetic Vision 2011, 2011-04-25 - 2011-04-29, Florida.     <\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1117\/12.883799<\/dc:relation><dc:relation>\n        10.1117\/12.883799<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/epubs.surrey.ac.uk\/3038\/","http:\/\/dx.doi.org\/10.1117\/12.883799","10.1117\/12.883799"],"year":2011,"topics":[],"subject":["Conference or Workshop Item","PeerReviewed"],"fullText":"Small-scale Anomaly Detection in Panoramic Imaging using Neural \nModels of Low-level Vision \n \nMatthew C. Casey*a, Duncan L. Hickmanb, Athanasios Pavloua, James R. E. Sadlerb \naDepartment of Computing, University of Surrey, Guildford, Surrey, GU2 7XH, UK; \nbWaterfall Solutions Ltd., 1 & 2 Saxton, Parklands, Guildford, Surrey GU2 9JX, UK \nABSTRACT   \nOur understanding of sensory processing in animals has reached the stage where we can exploit neurobiological \nprinciples in commercial systems.  In human vision, one brain structure that offers insight into how we might detect \nanomalies in real-time imaging is the superior colliculus (SC).  The SC is a small structure that rapidly orients our eyes \nto a movement, sound or touch that it detects, even when the stimulus may be on a small-scale; think of a camouflaged \nmovement or the rustle of leaves.  This automatic orientation allows us to prioritize the use of our eyes to raise \nawareness of a potential threat, such as a predator approaching stealthily.  In this paper we describe the application of a \nneural network model of the SC to the detection of anomalies in panoramic imaging.  The neural approach consists of a \nmosaic of topographic maps that are each trained using competitive Hebbian learning to rapidly detect image features of \na pre-defined shape and scale.  What makes this approach interesting is the ability of the competition between neurons to \nautomatically filter noise, yet with the capability of generalizing the desired shape and scale.  We will present the results \nof this technique applied to the real-time detection of obscured targets in visible-band panoramic CCTV images.  Using \nbackground subtraction to highlight potential movement, the technique is able to correctly identify targets which span as \nlittle as 3 pixels wide while filtering small-scale noise.  \nKeywords: Panoramic imaging, neural networks, anomaly detection, low-level vision \n1 INTRODUCTION  \nOur understanding of sensory processing in animals has reached the stage where we can exploit neurobiological \nprinciples in commercial systems.  In human vision, one brain structure that offers insight into real-time imaging is the \nsuperior colliculus (SC)1,2.  The SC is an evolutionary stable structure found in the midbrain of vertebrates, which is \nresponsible for shifting gaze to focus the eye\u2019s fovea towards stimuli of interest2.  This focusing of resources, leading \nultimately to conscious perception, is a key survival mechanism.  Imagine you are being stalked by a predator that is \nadept at camouflage and stealthy movement.  The predator\u2019s approach will be masked by environmental clutter, such as \nfoliage, while their footfalls will be quiet.  The SC has developed to prioritize the detection of slight movement and quiet \nsounds emanating from the same location so that we automatically look towards these locations, and hence react to \npotential threats1.  This specialization extends to detecting potential food sources as well, such as in frogs where the \noptic tectum (the equivalent of the SC in non-mammalian vertebrates) is sensitive to the movement of convex-shaped \ndark objects, which correspond to the location of flies3. \nAs a specialized survival mechanism, the processing of the SC demonstrates interesting computational principles.  First, \nthe SC is closely connected to sensory input.  Retinal input to the SC predominantly comes directly via the koniocellular \npathway, which rapidly responds to motion and luminance4, and hence fast connectivity and crude processing with a \nsmall lag time is key to indicating interesting visual activity.  Second, the SC combines visual, auditory and touch \ninformation in order to localize2.  This multisensory integration demonstrates the importance of fusing multiple sources \nof information from one event to provoke a response, even if the relative intensities of the individual visual, auditory or \ntouch stimuli are low5.  Third, the SC is closely connected to motor outputs so that it is able to rapidly react.  For \nhumans, this output controls the shift of our gaze towards the activity of interest via direct connection with the brain \nstem6.  The output also feeds structures such as the amygdala, which primes our body to react whenever fearful stimuli \n \n* m.casey@surrey.ac.uk; phone +44 1483 689635; fax +44 1483 686051; www.surrey.ac.uk\/computing \n \n \n \n \nare detected, such as running away from \u2018snakes in the grass\u20197.  Computationally, the SC therefore forms an influential \nlocalization system which works on crude sensory input to provoke a rapid reaction. \nGiven its computational credentials, the SC is therefore an attractive structure to model in order to evaluate whether the \nsame computational principles can be applied to commercial systems.  In the first instance this means understanding how \ncrude visual stimuli can be processed rapidly to detect areas of interest ignoring noise, with the longer term view of \ncombining vision with other modalities and more complex, but rapid processing of threats.  In this paper we concentrate \non the visual properties of the SC, and we therefore describe the application of a neural network model8 of the superficial \nlayers of the SC to the detection of anomalies in imaging.  Our application domain is that of panoramic CCTV images, \nwhich can be monitored by operators to detect threats from a wide area.  With wide area input, operators may focus only \non the central area of the image.  In contrast, the SC shows how anomalies in peripheral vision can be detected, and \nhence we apply this to assist detection rates.  Section 2 of this paper briefly describes the neural network model.  Section \n3 describes the application domain of panoramic imaging.  In section 4 we present the results of our approach applied to \nthe real-time detection of obscured targets in panoramic CCTV images.  Our conclusion and discussion is provided in \nSection 5.   \n2 MODEL SPECIFICATION \nThe SC is divided into superficial and deep layers1.  The superficial layer of the SC processes visual input which comes \ndirectly from the retina6, predominantly via the koniocellular pathway4.  The deep layers of the SC combine visual \nsensitivity with auditory and somatosensory input to perform multisensory localization2,5.  Each layer within the SC is \nformed from topographic maps of the visual, auditory or somatosensory space.  These layers are all aligned into an eye-\ncentered representation so that different sensory modalities can be combined9,10. \nFocusing just on the visual localization that the SC performs in its superficial layer, this provides motion11-13 and \ncontrast14,15 sensitivity on a binocular representation of visual space.  For example, a study on the frog optic tectum by \nLettvin et al3 demonstrated that the frog\u2019s equivalent to the SC was formed from four topographic layers.  The first layer \nwas sensitive to spatial contrast at sharp edges.  The second layer was sensitive to convex-shaped dark objects, which \nperhaps coincides with the broad shape of the frog\u2019s preferred diet of flies.  The third layer responds to temporal contrast \nof moving edges.  The fourth layer responds to a sudden reduction in illumination, as might occur if a predator\u2019s shadow \nmoves towards the frog. \nThese examples show how the superficial layer of the SC has developed to process simple spatial contrast, temporal \ncontrast and illumination information in order to localize threats or sources of food across the entire visual field.  Of \ncourse, because the processing is simple, this provides only a rapid response which can then be moderated by later, more \ncomplex processing, and it is therefore subject to false alarms.  However, what is important computationally is that the \nprocessing, albeit crude, is sufficiently specialized to filter out noise and to concentrate on important stimuli.  This \napproach of having specialist layers of processing is used throughout the visual system.  Of particular relevance is the \namygdala7, which is the structure responsible for responding to crude, fearful stimuli so that the body is prepared to react \nonce the stimuli has been consciously perceived and processed.  Here, the amygdala goes one stage further by allowing \nthe types of stimuli being detected to be learnt16, such as through classical conditioning17.  The question we address in \nthis paper is whether the principles of detecting simple stimuli to rapidly react, such as observed in the SC or other \nstructures like the amygdala, can be applied to imaging systems to improve anomaly detection?  To understand this, we \nfocus on how the SC can be modeled computationally before applying this to imaging. \n2.1 Modeling the Superior Colliculus \nFocusing just on the visual processing conducted in the superficial layers of the SC, we can see from the work of Lettvin \net al3 that this processing is achieved by using layers of topographic maps, where each layer is specialized to one type of \nvisual feature.  To achieve this, the retina provides spatial, temporal and illumination information.  The core of the \nprocessing conducted in the SC is therefore achieved with specialized topographic maps.  Each map responds only to a \nspecific type of input, such as contrast at a particular scale, and can automatically localize to the area which has the most \nsalient (or strongest) input. \nOur previous work on modeling the SC provides us with a topographic map algorithm that can achieve this \nlocalization8,18.  The map consists of a two-dimensional array of rate-coded neurons, where each neuron corresponds to a \n \n \n \n \nlocation in the input space.  The output y from the neuron at location (i, j) in the map given an m-dimensional input x is \ncalculated as8: \n  (1) \n\uf0e5\n\uf03d\n\uf03d\nm\nk\nkijkij twxu\n1\n)(\n \n\uf0ef\uf0ee\n\uf0ef\uf0ed\n\uf0ec\n\uf02d\n\uf03c\uf02d\uf03d\n)(\n)( if)(\nwinij\nwinijij\nij yuf\nthccuf\ny  (2) \n  (3) \n\uf0ef\uf0ee\n\uf0ef\uf0ed\n\uf0ec\n\uf0a3\n\uf03c\uf03c\n\uf0b3\n\uf03d\n00\n10\n11\nu\nuu\nu\nyij\nwhere wkij(t) is the weight at time step t during training which moderates the input k to neuron (i, j).  The key element of \nthis calculation is that some neurons are considered to be the winners in responding to the input (equation 2), and hence \nall other neurons outside the defined neighborhood of the winners, represented by the function h(t), have their output \nsuppressed.  The location of each neuron in the map is defined by the 2-dimensional co-ordinate vector cij, where the \nneuron with the maximum activation is located at cwin.  This improves upon the scheme defined by Armony et al19 for a \nsingle row of neurons to ensure that the map localizes to the strongest part of the input.  It is therefore important that the \nweights are defined appropriately in order for the map to localize to specific patterns of input, such as the signal with the \ngreatest spatial contrast, or particular type of motion \nIn order to set the weights for each neuron, the map undergoes a period of training using a competitive learning \nalgorithm in which target patterns are presented to the network, the output from each neuron is calculated by selecting a \nneighborhood of winners, and then each neuron\u2019s weight within the neighborhood is adjusted so that the neuron \nresponds more strongly to a similar input in future time steps.  During training the neighborhood and the amount of \nchange to each weight is reduced to produce a stable map.  Details of this training algorithm are defined by Pavlou & \nCasey8.  This training regimen ensures that the map is able to localize to the required patterns, whatever they may be. \n2.2 Anomaly Detection \nThis simple topographic map therefore provides neurons that can respond strongly to particular patterns at different \nlocations within the input.  For example, if the input to the map is an image, where the image has been filtered to \nrepresent a specific feature through intensity values, then the map can respond to the location and strength of each \nfeature.  Here the location is represented by the winning neurons which have the highest output y.  These neurons have \nthe highest output because their weights are selective for patterns occurring at specific locations within the input. Within \nthe context of anomaly detection, this means that a suitably trained map can take as input an image which represents, \nsay, movement recorded between successive video frames, with the map then localizing a particular shape or type of \nmovement.  The important aspect to this is the ability of the map to generalize so that neurons provide a graded response \nto partial shapes.  In this way, simple video processing techniques can be coupled with a trained neural topographic map \nin order to detect anomalies which may be obscured. \nIn the examples we explore in this paper, we train maps to respond to different crude shapes.  When presented with \nfiltered images the map can then detect the location in the input which has a signal that most represents the trained \nshape.  For example, if trained on a general shape of a walking person (section 4.1), then the map can detect the location \nwithin the image of the signal that most closely matches to the walking shape.  However, in order to achieve this we \nneed to provide appropriate filters.  Looking back at the role of the SC, one feature that is particularly important is \nmotion.  Through temporal information supplied by the retina, the SC can detect high contrast motion11-13, and this is \nwhere our anomaly detection is focused.  To simplify our experiments, we use a single filtered input to the map which \nhighlights motion (or more accurately change). \nWe achieve motion detection through the use of background subtraction on grayscale images (Figure 1).  Background \nsubtraction relies upon the definition of an appropriate background (assuming the camera is static), which can result in \nissues with noise and problems with variation in lighting.  Noise may occur, for example, with normal motion in the \nenvironment, such as from moving leaves.  While variations on background subtraction have been described20 we use \nsimple thresholding and then rely upon the ability of the map to filter noise in the image when detecting shapes.  One of \nthe computational principles important in modeling the SC is its ability to rapidly detect areas of interest.  Additional \n \n \n \n \nfiltering techniques may be applied to provide better motion detection, however these each add a computational \noverhead.  Instead we follow the principle of performing only a small number of simple filtering stages in order to \ndetermine if the map can operate correctly only on crude input.  The stages involved in our anomaly detection are shown \nin Figure 1. \nare input to \nthe neural map which then h) localizes the area with the closest matching input to its training patterns. \n of increasing the situational awareness of the human observer whilst minimizing the footprint of the \nstem used has previously \n22\na) Background color image \ne) Background minus sample \ng) Thresholded \n \nFigure 1. Anomaly detection using a topographic map that is motivated by visual processing in the superior \ncolliculus.  Images show the complete processing on a sample image used during the experiments (section 4.2). a) \nselected background image which is b) converted to grayscale, c) sample image which is converted to d) \ngrayscale.  The background and sample are then e) subtracted and the resulting image inverted to show change as \nf) a high value between 0 and 1.  The resulting input is then thresholded so that only values above 0.5 \n3 PANORAMIC IMAGING \nHuman binocular vision has a wide field of view in excess of 100\u00b0 in the horizontal direction.  Although human eyes can \nonly foveate on a small section of this image, typically around 1% to 2%, the system has evolved to detect motion or \nother stimuli in the periphery of the field of view using structures such as the SC.  The SC then provides input to later \nprocessing stages, such as the amygdala21 and successive areas of the visual cortex.  This natural processing chain is now \nbeing exploited in a variety of applications that centre on the use of panoramic imaging, or wide area surveillance, to \nprovide a method\nimaging system. \nThe use of a panoramic imaging system for the work detailed in this paper enables the neurobiological response to be \ntested using imagery that closely matches the human visual field. The panoramic imaging sy\ndemonstrated the benefits of wide area situational awareness in a number of user lead trials . \nWithin the context of anomaly detection, it is necessary to detect the small changes in the scene\u2019s content. These small \nchanges, or anomalies, can originate from a bag being placed in a secluded corner or people loitering and behaving \nb) Background grayscale image \nc) Sample color image \nd) Sample grayscale image \nh) Map output \nf) Inverted \n \n \n \n \nagainst the general pattern of life. A number of detection and tracking methodologies can either be run concurrently or \ncombined to give a single integrated solution. This has been done automatically within the context of processing the \nreceived imagery in order to form the panoramic image. The panoramic image itself is formed from any number of \npartially overlapping images. Typically we have constructed panoramic images from two or three camera inputs up to \n360o full panoramic images from twelve cameras. Detection algorithms are then applied to the resulting imagery \nenabling objects to be tracked across a full field of view. This provides the user with a significant enhancement in \nmerging of the \nures can be linked to provide a heightened situational awareness. \nanoramic images formed from three visible-band \ncameras arranged to give a 100\u00b0 horizontal field of view (Figure 2). \n from three visible band camera images.  Note the use of \nn distortion, however there are more complicated compound \ne co-aligned enabling objects of interest to be tracked across the \nhting.  There was no significant \nvariation in lighting within each scene, given the use of simple background subtraction. \nsituational awareness whilst minimizing the additional workload of a user. \nOne of the hardest problems to overcome is the issue of obscuration of the target. This is of particular interest for the \nprotection of installations and wider security and surveillance applications. In many real life cases there is often \nadditional clutter, or cover, provided in the scene which can result in reducing the probability of detection. The use of a \ndetection algorithm that can identify small amounts of motion in the scene, and through the intelligent \nresulting detections, these disparate feat\n3.1 Generating Panoramic Images \nThe panoramic images used for the experiments were generated using proprietary image warping and panorama \nformation algorithms22. The resulting sequences were based upon p\n \nFigure 2. Example office area panoramic image combined\na cylindrical warp in order to correctly align the images. \nThe process of generating a panoramic image involves a number of steps that correct for distortions in the imagery. As \nwith any imaging system, there are a number of distortions that arise from the optical assemblies used.  At their simplest \nthese can be approximated by a simple barrel or pincushio\ndistortions that are characteristic of some lens assemblies.  \nThe process of imaging a wide area using a number of sensors with smaller fields of view also leads to the necessity to \napply a cylindrical warp. This corrects discrepancies in the overlapping regions between neighboring sensors. The result \nis a seamless image where corresponding features ar\ncommon field of view between neighboring sensors.  \nThe imagery used for the experiments was captured in a built-up office area (Figure 2) and an industrial area (Figure 7).  \nBoth areas offered plenty of natural cover, provided by buildings and parked cars, for the target to hide behind or move \namongst. A number of color sequences were recorded with the target popping their head up from behind a car or around \nthe side of a building. This generally equated to a few pixels in the 100\u00b0 horizontal field of view.  The office area was \ncaptured during day time and the industrial area at dusk using low-light level street lig\n4 EXPERIMENTS AND EVALUATION \nIn the examples we explore in this paper, we train maps to respond to different human-type shapes at a specific small-\nscale.  The first experiment demonstrates how a walking person can be detected when they are in full view of the camera \n \n \n \n \nor partially obscured by a tree.  To achieve this, we trained a map for 400 epochs on 46 images of 64 by 32 pixels each \nof which had a representation of a human walking pose at different locations within the image (Figure 3).  The walking \npose was generated from a 3-dimensional model, but was static across the training images. \n \nages with the item of interest depicted by \n on 32 images of 64 by 32 pixels each of \nwhich had the Gaussian blob at different locations within the image (Figure 4). \n images with the item of interest depicted by a high (white) intensity value and the background as zero \nThe Gaussian blob was defined to have a radius \u03c3=3, such that the input x at pixel (i,j) is defined as: \n \nFigure 3. Training images used for the first experiment, a) template of human walking, b) three examples from the \nresulting 46 training images.  Note that the training data are grayscale im\na high (white) intensity value and the background as zero (black). \nFor the subsequent experiments we used a generic Gaussian blob shape to capture any change of a particular scale.  In \nparticular this allows us to detect a person even at a scale of 3 pixels wide but filters out small scale noise.  This was then \napplied to various scenarios, including a person popping their head out from behind a wall, to a person crouching behind \na car or walking behind a tree.  To achieve this, we trained a map for 400 epochs\n \nFigure 4. Training images used for detecting obscured small scale motion.  Note that the training data are \ngrayscale\n(black). \n\uf0f7\uf0f7\uf0f8\n\uf0f6\n\uf0e7\uf0e7\uf0e8\n\uf0e6 \uf02d\uf02b\uf02d\uf02d\n\uf03d 2\n22\n2\n)()(\n\uf073\uf06c\ndjci\nij ex  (4) \nzontally \nbined to provide a single localization output. \nly obscured.  The scene was taken during the rain and \ntherefore included additional environmental noise (Figure 5). \nwhere the center of the blob is at (c,d) and the amplitude \u03bb=1. \nThe generated panoramic images were 1344 pixel wide by 580 pixels high.  Each topographic map was trained on \nimages 64 by 32 pixel images.  Therefore, in order to apply the map to the panoramic images, each image used a mosaic \nof 21 by 18 topographic maps.  Each map consisted of 2 by 64 neurons so that a neuron represented 1 pixel hori\nand 16 pixels vertically.  The results from each map were then com\n4.1 Pilot Experiment: Detecting People using Shape Profiles \nThe first experiment was used to test the approach on a single (non-panoramic) image in order to evaluate whether the \nneural algorithm could be applied to visible band images to detect crude shapes.  To evaluate the approach, we recorded \nscenes which included people walking across the foreground and background of the image (hence different scales), \nwhich had both natural features, such as trees, and man-made features, such as buildings and walls.  From the candidate \nscenes recorded, we chose 114 frames from one sequence of 744 frames (recorded at 25 frames per second).  Each of the \n114 frames included a person depicted at a scale that the map should be able to detect, as well as people at larger scales.  \nThe person was either shown clearly, partially or was total\n1 16 32 \n1 24 46 \na) 3-dimensional template b) Sequence of training images used \n \n \n \n \n \nPeople at \ndifferent scales \nPerson walking at \ndesired scale \nEnvironmental \nnoise: rain and \nmovement of \nleaves \n \nFigure 5. Example image from the pilot experiment.  This was tested against maps trained on a walking person \nsilhouette of approximately the same scale as the person highlighted in the red circle. \nSince the image was smaller than the panoramic images tested in later experiments (768 by 576 pixels), a mosaic of 12 \nby 18 duplicate topographic maps were used.  Instead of using background subtraction, in this pilot we used the \ngrayscale pixel intensity as the input to the map.  Since the objects of interest were darker than the background (black \nclothes against a light colored wall), the image was inverted such that dark pixel values were given a high input to the \nmap.  The source map was trained on the patterns as shown in Figure 3 so that the person in the selected frames should \nbe detected.  This experiment was therefore designed only to determine if the map could detect crude shapes of the \nrelevant size that were obscured to various degrees and with environmental noise. \nThe mosaic of maps correctly localized the person when they were in clear view in 20 out of 85 frames (24%), and when \nthey were obscured by trees or other artifacts in 2 out of 20 frames (10%), giving a total detection rate of 21%.  When \nthe person was completely obscured (9 frames), and throughout all 114 frames tested, there were no false alarms.  This \ntherefore demonstrated that the map could detect the required shapes, albeit with at a low rate. \nThere were three clear limitations with this approach.  First, input to the map relied upon pixel intensity which was \ninverted to provide detection of dark shapes.  The map could therefore not detect people walking, for example, in light \ncolored clothing.  Second, the pattern on which the map was trained has a specific silhouette where the person is \ndepicted with their legs apart (Figure 3).  For a significant portion of the time when a person is walking, their legs are \ntogether.  The pattern that the map was trained upon was therefore too specific and detection only occurred when the \npattern matched closely.  Third, because we use a mosaic of maps, when a relevant shape crosses a map boundary, it is \nnot detected because each map only receives input for part of the shape.  We address the first two of these issues in \nsubsequent experiments. \n4.2 Evaluating Detection on Panoramic Images \nThe limitations of the pilot experiment demonstrated the need to make both the input and the pattern being detected more \ngeneric.  For example, by directly using pixel values we are constraining the detection to intensity, while using a very \nspecific pattern to train the map means that there is less generalization to similar patterns.  We overcome this by looking \nfor changes in the input using background subtraction, and use a more generic training pattern, recognizing that may \nsignificantly increase the number of false alarms. \nBackground subtraction is used to record change between an initial, background image and the current frame.  This \nprovides a high-valued input for any pixel values that differ to the background (Figure 1).  Instead of using a walking \npose to train the map, we use a Gaussian blob with radius of 3 pixels and therefore the map will detect the most \nsignificant change that is at least 3 pixels or larger, but this will filter out multiple responses for co-located change.  For \nexample, only the most significant change resulting from a person moving will be detected, not the movement of the \nwhole person. \nThis setup was applied to panoramic images captured during daylight, an example of which is shown in Figure 2.  Three \nvideo sequences were recorded and frames selected from these to test detection under different scenarios.  Details of \neach testing sequence and the detection results are given in Table 1 (sequences 1 to 4). \nSequences 1 to 3 were designed to test the ability of the map to detect targets that were obscured or in clear view.  They \ndid not contain any significant distracters and hence had a low false alarm rate.  The total detection rate for sequence 1 \nwas 60% (100% when the target was in clear view, 56% when obscured) with 0 false alarms.  For sequence 2 this was \n \n \n \n \n63% with 0 false alarms, and sequence 3 achieved 84% detection but with 3 false alarms.  An example output from \nsequence 3 is shown in Figure 6.  None of the target patterns crossed a map boundary.  False alarms were generated by \nchanges in the reflection of sunlight in the office windows.  Therefore, by having the input based upon change and by \nmaking the training pattern more generic, a greater detection rate was achieved. \nTable 1.  Panoramic testing sequences selected to test the model.  Sequences 1 to 4 were of the office area (Figure \n2), sequence 5 was of the industrial area at dusk (Figure 7).  Both consisted of a mixture of buildings, pavements, \nparked cars, trees and other foliage. \nDetection Sequence Description Frames\nObscured \n(total) \nIn clear \nview \n(total) \nFalse \nAlarms\n1 Detection of obscured person walking \nSingle movement from a person walking behind a \ntree and parked motorbike.  In full view the person \nwas 75 by 25 pixels.  No other significant movement.  \nSome environmental noise (leaves and reflections). \n70 35 (63) 7 (7) 0 \n2 Detection of person crouching behind a car \nSingle movement of person who starts crouched \nbehind a car, stands up to be half visible, moves \nalong the car then crouches down.  Visible area of \nperson varies from 7 to 28 pixels.  No other \nsignificant movement.  Some environmental noise \n(leaves and reflections). \n56 35 (56) 0 (0) 0 \n3 Detection of person\u2019s head popping out from behind \na wall \nSingle movement from a person standing out of view \nbehind a corner who then pops their head out.  \nVisible portion of the head is 3 pixels wide.  No other \nsignificant movement.  Some environmental noise \n(leaves and reflections). \n32 21 (25) 0 (0) 3 \n4 Detection of person\u2019s head popping out from behind \na wall with other movement \nMovement from a person standing out of view \nbehind a corner who then pops their head out.  Other \nsignificant movement from a car passing out of frame \nand a person walking through the center of the image \nwith size 55 by 20 pixels. \n61 48 (48) 0 (0) 28 \n5 Detection of obscured person walking and running \nVarious types of movement from a person starting \nbehind a parked car (head only visible as 1 pixel) \nwalking towards the camera, passing behind signs, \nhiding in a bush before running towards and then \naway from the camera.  In full view the person was \n68 by 34 pixels.  Other significant movement from a \nfluttering flag.  Environmental noise from moving \nclouds as well as noise from the low light conditions. \n821 Person: \n66 (243) \nPerson: \n155 (379) \nFlag: \n375 (821) \n859 \n \n \n \n \nSequence 4 was designed to test detection in a more realistic scenario where there was other movement and \nenvironmental noise.  This repeated the same scenario as sequence 3 but had another person walking through the center \nof the image and a car just leaving the left side of the frame.  Sunlight also varied in a number of reflections from parked \ncars and windows.  The total detection rate was 100% for the target but with 28 false alarms, assuming we treat other \nlarge-scale movement as a false alarm.  Here, the car was detected in 2 out of the 4 frames it was present, leaving the \nremaining 26 false alarms occurring because of reflections and background noise.  The second person walking through \nthe scene was not detected because they did not move far enough from their initial (background) position. \n \nFigure 6. Example output from the model for sequence 3.  The top frame shows the input panoramic image with \nthe target highlighted by the red box and inset.  The target is a person\u2019s head popping out from behind a wall with \na width of 3 pixels.  The bottom frame shows the model output superimposed over the input. \nThese sequences therefore demonstrated that the technique could be applied to panoramic images to detect small-scale \nmovement.  However, by using simple background subtraction and a generic training pattern we have increased the false \nalarm rate.  The advantage of the technique is that it can detect particular shapes rapidly and, despite the false alarms, it \ncan filter out some small-scale noise. \nHaving tested the capability of the technique, we then went on to evaluate it on a longer video sequence which included \na target that we wish to detect that was either hidden, obscured or in full view, as well as other targets and environmental \nnoise (sequence 5).  This scenario was also captured at dusk so that there was a mixture of street lighting and changing \npatterns of light from the sun (Figure 7). \n \nFigure 7. Example industrial area panoramic image combined from three visible band camera images. \n \n \n \n \nIn the 821 frames, 622 showed the target person moving towards and then away from the camera.  The visible portion of \nthe target started off at 1 pixel.  Clear throughout the sequence was a flag fluttering in the wind which should be detected \nwhen it moves from its initial position.  There was also environmental movement, such as clouds, which deviated \nincrementally from the background image to have the biggest change by the last frame. \nThe total detection rate of the person was 36% (27% obscured and 41% in clear view).  This is low in comparison with \nthe previous sequences, but demonstrates the difficulty of detecting small-scale movement in the order of a few pixels.  \nThe technique detected the person when their visible portion was as little as 3 pixels, right up until the maximum extent \nat 68 pixels.  This is most evident when the person was hiding behind a bush and moving around so that their head \npopped out for only one or two frames at a time.  However, the person was not detected at the boundary between maps.  \nThe detection rate of the flag was 46%, although this does not take into account when the flag was close to its initial, \nbackground position (no wind) where only slight movement would not be detected because it was less than 3 pixels.  The \nfalse alarm rate was significant at 859 for the whole sequence, with multiple alarms being raised in many images \n(maximum 4 in any given frame).  These were mostly caused by the small movement of clouds in the scene which \nincreased from the initial background over time, and which was particular prominent towards the end of the sequence in \nthe top right hand area of the image where the sun was strongest. \n5 CONCLUSION \nIn this paper we have evaluated the use of a biologically inspired model of the SC for anomaly detection in panoramic \nCCTV imaging.  We have demonstrated how a rate-coded neural network model of the visual layers of the SC can be \nused to rapidly localize small-scale movement in an image based upon a pre-defined shape.  This is different to other \ntypes of image detection algorithm in that the technique goes beyond the use of convolutions by using competition \nbetween neurons in a topographic map to provide the most salient localization.  We have evaluated this against the use of \na specific shape (a walking person) and a generic shape (a Gaussian blob).  For either, small-scale change in an image \ncan be detected in as little as 3 pixels allowing detection of partially obscured movement.  However, by using a specific \nshape we restrict detection and ignore other anomalies which do not match the target.  When using a generic shape the \nfalse alarm rate increases considerably but this allows us to detect any small-scale change.  The output from the maps \nmay therefore benefit from further processing, and could perhaps be used as an early warning input to more sophisticated \nanalysis and tracking algorithms which could not detect small-scale anomalies. \nThere are three limitations with this work, notably focused on the balance between detection rates and false alarms.  \nFirst, to detect anomalies, the topographic map relies on input that has been sufficiently processed to provide contrast \n(spatial or temporal).  Limited pre-processing is rapid but also contributes to the false alarm rate because of na\u00efve \ntechniques such as background subtraction.  To reduce the reliance on pre-processing, the topographic maps may be \ntrained to automatically detect, for example, temporal contrast (movement) by introducing lateral inhibition between \nneurons across frames.  This would also improve the detection rate while keeping the ability to pre-define shapes.  \nSecond, we have kept the map sizes small and used a mosaic of maps for detection.  The reason for this is the lengthy \nprocess of training which takes longer for larger map sizes.  A simple one-shot training scheme can be used to overcome \nthis and hence allow us to scale up the maps to avoid having map boundaries.  This will increase the detection rate by \nremoving map boundaries.  Third, when using a generic training pattern the number of false alarms is increased.  These \ncan be decreased by either using improved pre-processing techniques, rather than simple background subtraction, or by \ntraining the weights within the map to detect movement through lateral inhibition.  Such an approach will remove the \nneed for pre-processing, as described above.  Alternatively, the output from additional layers of maps trained on \ndifferent scale shapes could be compared to highlight the most salient signal at a range of scales. \nA further opportunity for the work is to explore the combination of different modality inputs, such as combining images \nwith sound.  The SC integrates visual, auditory and somatosensory information for localization.  Initial work combining \nimages and sound has already been attempted23 and this may prove beneficial to the detection of multimodal anomalies. \nACKNOWLEDGEMENTS \nThis work was sponsored by the EPSRC through the University of Surrey Knowledge Transfer Account project \n(EP\/H500189\/1). \n \n \n \n \nREFERENCES \n \n[1] King, A. J., \"The Superior Colliculus,\" Current Biology, 14(9) R335-R338 (2004). \n[2] Stein, B. E. and Meredith, M. A., [The Merging of the Senses], A Bradford Book, MIT Press, Cambridge, MA., \n(1993). \n[3] Lettvin, J. Y., Maturana, H. R., Mcculloch, W. S. and Pitts, W. H., \"What the Frog's Eye Tells the Frog's Brain,\" \nProceedings of the IRE, 47(11) 1940-1951 (1959). \n[4] Isbell, L. A., \"Snakes as Agents of Evolutionary Change in Primate Brains,\" Journal of Human Evolution, 51(1) 1-\n35 (2006). \n[5] Stein, B. E. and Stanford, T. R., \"Multisensory Integration: Current Issues from the Perspective of the Single \nNeuron,\" Nature Reviews Neuroscience, 9(4) 255-266 (2008). \n[6] May, P. J., [The Mammalian Superior Colliculus: Laminar Structure and Connections], Buttner-Ennever, J.A. (Ed), \nProgress in Brain Research: Neuroanatomy of the Oculomotor System, Volume 151, Elsevier, 321-378 (2006). \n[7] \u00d6hman, A., Flykt, A. and Esteves, F., \"Emotion Drives Attention: Detecting the Snake in the Grass,\" Journal of \nExperimental Psychology: General, 130(3) 466-478 (2001). \n[8] Pavlou, A. and Casey, M. C., \"Simulating the Effects of Cortical Feedback in the Superior Colliculus with \nTopographic Maps,\" Proceedings of the International Joint Conference on Neural Networks (IJCNN) 2010, (2010). \n[9] Zella, J. C., Brugge, J. F. and Schnupp, J. W. H., \"Passive Eye Displacement Alters Auditory Spatial Receptive \nFields of Cat Superior Colliculus Neurons,\" Nature Neuroscience, 4(12) 1167-1169 (2001). \n[10] King, A. J., Schnupp, W. H. and Thompson, I. D., \"Signals from the Superficial Layers of the Superior Colliculus \nEnable the Development of the Auditory Space Map in the Deeper Layers,\" Journal of Neuroscience, 18 9394-9408 \n(1998). \n[11] Sterling, P. and Wickelgren, B. G., \"Visual Receptive Fields in the Superior Colliculus of the Cat,\" Journal of \nNeurophysiology, 32(1) 1-15 (1969). \n[12] Rauschecker, J. P. and Harris, L. R., \"Auditory and Visual Neurons in the Cat's Superior Colliculus Selective for the \nDirection of Apparent Motion Stimuli,\" Brain Research, 490(1) 56-63 (1989). \n[13] Wallace, M. T., McHaffie, J. G. and Stein, B. E., \"Visual Response Properties and Visuotopic Representation in the \nNewborn Monkey Superior Colliculus,\" Journal of Neurophysiology, 78(5) 2732-2741 (1997). \n[14] Pr\u00e9vost, F., Lepore, F. and Guillemot, J. P., \"Spatio-temporal Receptive Field Properties of Cells in the Rat Superior \nColliculus,\" Brain Research, 1142 80-91 (2007). \n[15] Schneider, K. A. and Kastner, S., \"Visual Responses of the Human Superior Colliculus: A High-Resolution \nFunctional Magnetic Resonance Imaging Study,\" Journal of Neurophysiology, 94(4) 2491-2503 (2005). \n[16] Morris, J. S., \u00d6hman, A. and Dolan, R. J., \"Conscious and Unconscious Emotional Learning in the Human \nAmygdala,\" Nature, 393(6684) 467-470 (1998). \n[17] Pavlov, I. P., [Conditioned Reflexes: An Investigation of the Physiological Activity of the Cerebral Cortex], Oxford \nUniversity Press, London, (1927). \n[18] Pavlou, A. and Casey, M. C., \"A Computational Platform for Visual Fear Conditioning,\" Proceedings of the \nInternational Joint Conference on Neural Networks (IJCNN) 2009, (2009). \n[19] Armony, J. L., Servan-Schreiber, D., Cohen, J. D. and LeDoux, J. E., \"Computational Modeling of Emotion: \nExplorations Through the Anatomy and Physiology of Fear Conditioning,\" Trends in Cognitive Sciences, 1(1) 28-34 \n(1997). \n[20] Piccardi, M., \"Background Subtraction Techniques: A Review,\" Proceedings of the 2004 IEEE International \nConference on Systems, Man and Cybernetics, 4 3099-3104 (2004). \n[21] Shi, C. and Davis, M., \"Visual Pathways Involved in Fear Conditioning Measured With Fear-Potentiated Startle: \nBehavioral and Anatomic Studies,\" The Journal of Neuroscience, 21(24) 9844-9855 (2001). \n[22] Sadler, J. R. E., Davis, J. and Hickman, D. L., \"A Compact Wide-area Surveillance System for Defence and \nSecurity Applications,\" Proceedings of SPIE Defense, Security, and Sensing Conference 2011 on Enhanced and \nSynthetic Vision, 8042B (2011). \n[23] Casey, M. C., Pavlou, A. and Timotheou, A., \"Mind the (Computational) Gap,\" Proceedings of the UK Workshop \non Computational Intelligence (UKCI 2010), (2010). \n \n \n"}