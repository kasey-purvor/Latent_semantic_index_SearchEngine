{"doi":"10.1109\/DEST.2007.372039","coreId":"102519","oai":"oai:epubs.surrey.ac.uk:1981","identifiers":["oai:epubs.surrey.ac.uk:1981","10.1109\/DEST.2007.372039"],"title":"Protein Attributes Microtuning System (PAMS): an effective tool to increase protein structure prediction by data purification","authors":["Zhang, Fan","Povey, David","Krause, Paul J."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2007-02-21","abstract":"<p>Given the expense of more direct determinations, using machine-learning schemes to predict a protein secondary structure from the sequence alone remains an important methodology. To achieve significant improvements in prediction accuracy, the authors have developed an automated tool to prepare very large biological datasets, to be used by the learning network. By focusing on improvements in data quality and validation, our experiments yielded a highest prediction accuracy of protein secondary structure of 90.97%. An important additional aspect of this achievement is that the predictions are based on a template-free statistical modeling mechanism. The performance of each different classifier is also evaluated and discussed. In this paper a protein set of 232 protein chains are proposed to be used in the prediction. Our goal is to make the tools discussed available as services in part of a digital ecosystem that supports knowledge sharing amongst the protein structure prediction community.<\/p","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:epubs.surrey.ac.uk:1981<\/identifier><datestamp>\n      2017-10-31T14:03:54Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      74797065733D636F6E666572656E63655F6974656D<\/setSpec><setSpec>\n      6469766973696F6E733D656E67616E64706879736963616C736369656E636573:436F6D707574696E67<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/epubs.surrey.ac.uk\/1981\/<\/dc:relation><dc:title>\n        Protein Attributes Microtuning System (PAMS): an effective tool to increase protein structure prediction by data purification<\/dc:title><dc:creator>\n        Zhang, Fan<\/dc:creator><dc:creator>\n        Povey, David<\/dc:creator><dc:creator>\n        Krause, Paul J.<\/dc:creator><dc:description>\n        <p>Given the expense of more direct determinations, using machine-learning schemes to predict a protein secondary structure from the sequence alone remains an important methodology. To achieve significant improvements in prediction accuracy, the authors have developed an automated tool to prepare very large biological datasets, to be used by the learning network. By focusing on improvements in data quality and validation, our experiments yielded a highest prediction accuracy of protein secondary structure of 90.97%. An important additional aspect of this achievement is that the predictions are based on a template-free statistical modeling mechanism. The performance of each different classifier is also evaluated and discussed. In this paper a protein set of 232 protein chains are proposed to be used in the prediction. Our goal is to make the tools discussed available as services in part of a digital ecosystem that supports knowledge sharing amongst the protein structure prediction community.<\/p><\/dc:description><dc:date>\n        2007-02-21<\/dc:date><dc:type>\n        Conference or Workshop Item<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/epubs.surrey.ac.uk\/1981\/1\/fulltext.pdf<\/dc:identifier><dc:identifier>\n          Zhang, Fan, Povey, David and Krause, Paul J.  (2007) Protein Attributes Microtuning System (PAMS): an effective tool to increase protein structure prediction by data purification  In: 2007 Inaugural IEEE-IES Digital EcoSystems and Technologies Conference.     <\/dc:identifier><dc:relation>\n        10.1109\/DEST.2007.372039<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/epubs.surrey.ac.uk\/1981\/","10.1109\/DEST.2007.372039"],"year":2007,"topics":[],"subject":["Conference or Workshop Item","PeerReviewed"],"fullText":"2007 Inaugural IEEE International Conference on Digital Ecosystems and Technologies (IEEE DEST 2007)\n1\nProtein Attributes Microtuning System (PAMS):\nan effective tool to increase protein structure\nprediction by data purification\nFan Zhang4, David Povey**, Paul Krausett\nUniversity ofSurrey, Guildford, GU2 7XH, UK.\nAbstract- Given the expense of more direct determinations,\nusing machine-learning schemes to predict a protein secondary\nstructure from the sequence alone remains an important\nmethodology. To achieve significant improvements in prediction\naccuracy, the authors have developed an automated tool to\nprepare very large biological datasets, to be used by the learning\nnetwork. By focusing on improvements in data quality and\nvalidation, our experiments yielded a highest prediction accuracy\nof protein secondary structure of 90.97%. An important\nadditional aspect of this achievement is that the predictions are\nbased on a template-free statistical modeling mechanism. The\nperformance of each different classifier is also evaluated and\ndiscussed.\nIn this paper a protein set of 232 protein chains are proposed to be\nused in the prediction. Our goal is to make the tools discussed\navailable as services in part of a digital ecosystem that supports\nknowledge sharing amongst the protein structure prediction\ncommunity.\nIndex Terms-Automata, Biomedical Computing, Proteins,\nPrediction Methods, Data Management.\nI. BACKGROUND INTRODUCTION\nT ROTEINS carry out vital biological functionalities for each\nl living creature. To correctly enable this functionality, a\nprotein must fold into a unique three-dimensional shape in a\nmoist environment. Errors in protein folding usually cause\nfailures in performance ofbiological functions, and these errors\nare implicated in the early stages of deadly diseases such as\namyloidoses [1], Alzheimer's disease [2], prion diseases [3],\nand most cancers [4]. Clearly, further understanding of protein\nstructural characteristics, will help the pharmaceutical industry\nto may develop medicines to cure or even prevent these\ndiseases.\nThere are various ways to determine protein structure. X-Ray\ncrystallography [5] and Nuclear Magnetic Resonance (NMR)\nspectroscopy [6] are the most frequently used techniques to\nempirically determine the atomic model of the protein, and are\nwidely used in academic institutes and the pharmaceutical\nindustry. However, these techniques are both time-consuming,\n\u00a7 Email:fzhang 0surrey.ac.uk, collaborated project between Department of\nComputing and Division ofChemistry in University of Surrey.\nEmail:d.povey osurrey.ac.uk, Professor in Computer Aid Chemistry,\nDivision of Chemistry, University of Surrey.\n\" Email:p.krause 0surrey.ac.uk, Professor in Software Engineering,\nDepartment of Computing, University of Surrey.\nand expensive. Furthermore, specialised knowledge is required\nto apply these techniques, making them even more difficult to\nbe used by other scientists. An alternative methodology is to\npredict protein structure based on the application of machine\nlearning schemes to existing biological data. These techniques\nare gaining significant attention because of their potential low\ncost and ease of application.\nIn the early 1960s Christian Anfinsen and his colleagues\ndiscovered that the ribonuclease protein denatured while the\nchemical environment was changed by either heat, or adding\ncertain chemicals. The denatured ribonuclease folded back to\nits natural three dimensional shapes after the temperature was\nlowered or the certain chemicals were removed [7]. His\nconcern of connection between the amino acid sequence and\nthe biological active conformation laid out the foundation of in\nsilico prediction of protein structure from the study of\ncharacteristics of sequence.\nMachine learning techniques for studying and predicting\nprotein structure have been under development for the past\nthree decades. However these procedures suffered from the\nlack of the computational capacity and the limited availability\nof protein structure data. Along with the development of\ncomputer hardware and the growth of online protein data\nresources, the future of in silico prediction of the protein\nstructure seems to be more and more promising.\nThere are several categories of prediction methodologies,\nwhich vary from each other by processed data. The homology\nmodelling methods produce accurate predictions on proteins\nthat share more than 7000 sequence identity, but for sequences\nthat share less than 25-30% sequence similarity, the homology\nmodels may fail [8]. The latter are frequently used to provide\nstarting structures for molecular replacement in X-Ray\ncrystallography [9]. Protein threading techniques [10] search\nthrough currently known structures and identify the one which\nis most likely to be appropriate for the protein sequence undr\ninvestigation. Unlike the homology modelling tools the\nthreading methods deal with the proteins that share no obvious\nsequence identities, but have approximately similar folds. A\nrecent example of protein threading application is called\n\"Wurst\" [11]. Statistical template-free methods predict the\nsecondary structure subtype from the amino acid residue\ncharacteristics alone. There is no obvious template available in\nthese sequences. In our research, the predictions are based on a\ntemplate-free statistical modelling mechanism.\n1-4244-0470-3\/07\/$20.00 \u00a92007 IEEE\n565\n2007 Inaugural IEEE International Conference on Digital Ecosystems and Technologies (IEEE DEST 2007)\nThe problem of protein structure prediction has been tackled\nfor the past four decades, starting with methods such as the\nsingle residue statistical approach [25], often categorized as the\nfirst generation methodology. The first generation method\nproduced approximately 5000 prediction result and later on,\nalong with the advancement of machine learning techniques\nand the further understanding of feature descriptors describing\nthe characteristics of amino acid residues, the predictive\naccuracy reached nearly 80%. By using the PHD [27] method\nan average of 76% of amino acid residues are correctly\npredicted, while the JPRED [28, 29] yielded the best prediction\naccuracy of 76.4%. The PSIPRED [30] achieved average\naccuracy between 76.5% and 78.3%. The BAYESPROT [26]\nreported a highest prediction accuracy of 76.8%.\nThe mission in this paper is to explore an alternative way to\nimprove protein secondary structure prediction accuracy by\npurifying the applied biological data resources, instead of\nproducing a novel machine-learning scheme. In this paper the\nauthors present an effective tool - PAMS - to process the\nvarious available biological data resources, and then proceed to\nthe prediction of the secondary structure. The tool has the\ncapability to run through vast amount of experiments based on\nsimple instructions. Currently five million learning and\nprediction tasks have been carried out in a distributed\ncomputational platform. The human intervention in this process\nis significantly low.\nIn this research, PAMS produced a highest secondary\nstructure prediction accuracy of 90.9700. Apart from this\nsignificant achievement, we report a number of additional\nresults of interest. Firstly, an FD232** dataset has been collated,\nwhich we have found to be a good candidate protein sample set\nfor protein secondary structure prediction. Next, a wide range\nof amino acid feature descriptors are assessed and ranked\naccording to their performances. Furthermore, we found that\nafter the window size reaches 21, which is believed to be an\noptimal window size, the protein structure prediction\naccuracies do not vary radically if the window size is increased\nfurther. Finally, a list ofmachine learning classifiers is assessed\naccording to their prediction accuracy and efficiency. Here we\nfound that instance based classifiers [31] were capable of\nproducing good prediction accuracies, but were not efficient. In\ncontrast, the Tree-Augmented naive Bayesian network\nclassifier classifiers\/Forest-Augmented naive Bayesian\nnetwork classifier (FAN\/TAN) [21], were capable ofpredicting\nprotein secondary structure with an accuracy of81% - 84%, but\nfinish the task in a reasonably short time ( 3 seconds - 1 minute,\ndepend on the size of the dataset). Apart from these solid\nresults, we also raise a number of research questions at the end\nof the paper.\nThe tools discussed in this paper are being progressively\nmade available as web-services. A sub-goal ofour research is to\nsupport the creation of a digital-ecosystem for the Protein\nStructure Prediction community. Once our tools are available\n$$ A dataset generated in this research, contains 232 protein chains. Send an\nemail to the author to ask for a copy.\n2\nas services, they may be composed with other services in the\nnetwork to provide a configurable and extensible framework\nfor the research community.\nII. SYSTEM ANALYSIS AND DESIGN OF THE PAMS\nPAMS is a standalone tool for preparing training and testing\nsets for machine learning classifiers. It is coded in Java,\ndeployed and tested on a Red Hat Linux system. The Red Hat\noperation system is a reliable and well supported developing\nplatform, which is widely used in the bioinformatics research\narena. PAMS accesses a MySQL database [12], to retrieve data\nresources and deposit experiment results.\n......................... ......... ....\nStrage Retreve data\nAindDex| H And map into tH trbutes\n-----Windo -Calulatr\nSt_ET+P3_FAi3 1 hRchanism\nS6 CED data\n0Generated l, Buffered\ndatailes6 Writ r\n+'JVEKA'\nTHE PAMS\nIl-1: The PAMS infrastructure\nAs described in 11-1, the software contains four major\nmodules in the infrastructure. The DB Handler retrieves data\nfrom the database, and then transmits them to be mapped in a\nwindow.\n, \/\/i \\ ,\n\/ \\\n08 \/ f N %\nMF. L- 3 A Xa S J\n11 7--Linearwih v a x weigh\nwindow~~~bufe cotan N amn acds th setqI\nthe2linear weightvalefriteamionoacidinxponentionwight:\no~~~~\/\nvariatilon\nThere are two weight variation models implemented in this\npaper, as described in figure II-2. The straight line represents\nthe linear weight variation model, and the curve represents the\nexponential weight variation model. It is assumed that the\nwindow buffer contains N amino acids, the set\n{Wo4Ij,...WN l} indicates weight values assigned to each\namino acid according to its position. The function to calculate\nthe linear weight value for the amino acid in position i is:\n1 (N-1)\/2\n2Ax(N - 1) \/2 -i X((N-1)12 -WO)\nN-1\nAnd the function to calculate the exponential weight value\nfor the amino acid in position i is:\n1-4244-0470-3\/07\/$20.00 \u00a92007 IEEE\n566\n2007 Inaugural IEEE International Conference on Digital Ecosystems and Technologies (IEEE DEST 2007)\n( -N-1 )*Logwo\nN-1\ne 2\nThe choice of linear or exponential weight variation models\nis currently a matter of judgment. The weight assigned to an\namino acid represents the degree of influences ofthat particular\namino acid on the central amino acid. Further work is needed\non defining objective criteria for identifying the optimal\nfunction to be used to represent the degree of influences to the\ncentral amino acid, but this is outside the scope ofthis paper.\nThe database of the PAMS infrastructure contains several\navailable biological data resources. The Protein Data Bank\n(PDB) [13] contains 39853 records of protein structure, with\n33718 of them structured by X-Ray crystallography, 5914 of\nthe samples' structure determined by the NMR spectroscopy.\nEach record entry contains a list of three-dimensional\ncoordinates of each atom of that particular protein, therefore\npresents the shape of the protein.\nThe PDB SELECT25 [14,15] contains 3080 protein chains,\nwith 459963 amino acid residues, which are selected from the\ncurrent Protein Data Bank to be the representative data set.\nEach pair of chains ofthis dataset share less than 25% sequence\nidentity, hence the data set is considered as a collection of\nprotein entries without strong homological correlations among\nthem.\nThe DSSP [16] database is a database of secondary structure\nassignments for protein entries of the PDB. The database\ncontains the sequence, the corresponding secondary structure\nofproteins, and further information about the particular protein.\nThe sequences and secondary structures of the protein samples\nused in this paper were extracted from DSSP according to the\nprotein identities, which are defined in PDB SELECT25. The\nextracted information contains eight secondary structure\nsubtypes, e.g. the set of {H, E, B, G, I, T, S, -}. To simplify the\nprediction problem, a \"H, G, and I to H; E to E, the rest to C\"\neight-to-three state reduction method is applied to assign each\namino acid residue's secondary structure type to be one of the\nset of {C, H, E}.\nThe AAlndex [17, 18, 19] database published by the\nJapanese Genome project contains 516 characteristic\ndescriptors of each amino acid residue, which are collected\nfrom the literature. There are physical, chemical attributes of\neach amino acid within the database, and statistical analyses of\neach amino acid residues' structural propensities. The entries\nfrom the AAlndex database were used to represent the features\nof each amino acid residue. Ten sets of residue characteristics\nwere removed due to their containing undetermined values.\nThis use of available resources is an advantage of this\nresearch, saving much time and labour.\nThe Weka [20] machine learning toolbox is used to perform\nthe learning and prediction tasks for the study. Weka supports a\nset of machine learning classifiers, including decision tree\nbased classifiers [22, 32], instance based classifiers [31], neural\nnetwork classifiers, and Bayesian network classifiers [21].\n3\nThe training and testing data sets have to be prepared in the\nARFF [20] file format, in order to be processed by the Weka\nclassifiers toolbox. Another advantage for us is that Weka is\ncoded in Java, and so easily integrated into the PAMS software\nframework. In addition, Weka contains tools for data\npre-processing, regression, clustering, and visualization [20].\nThe measurement of the Weka is the cross-validation\nmeasurement, which is a standard method to estimate\nclassification accuracy over unseen data. For N-fold cross\nvalidation, the data are split into N subsets. One subset is then\nused as a testing set and the rest are combined together as the\ntraining set. This classification is performed N times, and the\naccuracies averaged. By default, all measurements used in this\nresearch are 10-fold cross validations.\nThe datasets processed in this research are discretized before\nbeing sent into the classifiers. The process of discretization\nconverts the numerical attributes into nominal (categorical)\nattributes, and decreases the computational complexity [24].\nIII. EXPERIMENTS PERFORMED BY THE PAMS\nA series of experiments are performed to explore the\ncorrelations between the amino acid residues of a particular\nprotein and their corresponding secondary structure types. A\nnumber of additional experiments are performed to gain further\nunderstanding of the factors that influence protein structure\nprediction accuracies.\nA. Selection ofProtein Samples\nTo perform a prediction, a list ofknown protein entries has to\nbe provided. The prediction of the protein structure is then\nbased on the knowledge that the learning network discovered\nfrom the protein samples with known structure. Apart from the\nhomological modelling, where selected proteins share\nhomological characteristics, in this study the 3080 protein\nsamples share no obvious homological commons. However the\nuse of 3080 protein entries is not effective due to the learning\nprocess needing to go through a large amount of residues. To\nselect a smaller and more representative protein sample set,\neach protein entry of the 3080 is tested with all the 506\nattributes, which are combined together, and the performance\nofthat particular protein is recorded. In the following plot III-1,\nthe X axis indicates the length ofthe particular protein sample,\nand the Y axis represents the self-test prediction accuracy ofthe\ndesignated protein sample:\nWeka project is especially good for bioinformatics study [23].\n1-4244-0470-3\/07\/$20.00 \u00a92007 IEEE\n567\n2007 Inaugural IEEE International Conference on Digital Ecosystems and Technologies (IEEE DEST 2007)\nlearning schemes with various window length and weight\nvariation models is shown in Table 111- 1:\n111-1: 3080 protein entries plotted as accuracy and peptide\nchain length.\nThe selection of the protein entries provides an alternative\nway to determine the quality of these proteins, which are\nparticipants in the process of protein secondary structure\nprediction. 232 peptide chains (including 11760 amino acid\nresidues) are selected from the above diagram. The selection is\nbased on considerations of both high self-test accuracy, and\nrepresentativeness to various lengths ofthe peptide chain.\nB. Selection of Window Mechanism\nWindowing mechanisms have been applied since 1980s [33,\n34, 35]. The window moderates neighbouring amino acids'\ninfluences on the central amino acid's secondary structure. The\ncritical problem is to determine an optimal window mechanism\nby selecting the length of the window and weight variation\nmodels. There simplest weight variation models are the linear\nand exponential weight variation models, as presented in the\ndiagram 11-1. We started our study with these.\nweithW fJer i\na-t-\n\/ . ~~~~~~~~~~~~~~~~JRP-ra-8 X --- E~~~~~~~~~~~~MT-eP''E\" ,A~~~~~~~~~~~~~~~~~~~~~-- -- -- -- - - - - ----\n1--Classifiers,then preicthionfaccuraciesdroppsiedan wifebigt,\na if tie hm iodfehlcre.f\npath wi ndowrc la s fength t each er21 couldr ignex ored. tFro thegh\ndiagriatinm,onealy allmthe classifies repote hbighesthaccuacites\nwhntewindow length isces setm to 2 1. h rdcinacrce\nBn oth os othe enila ndi i near,whe variation modhc rel aftre\ntested in this experiment. The overall performance of various\nNaive I I RBF\nBayes FAN\/TAN AODE Network IB 1 IBk KStar\nLinear 74.62% 81.14% 81.88% 80.70% 90.50% 90.67% 90.97%\nExp 75.45% 81.74% 82.36% 80.88% 87.67% 88.30% 88.51%\nRandom Decision\nID3 SMO Forest Table NNGE PART LBR\nLinear 80.20% 82.67% 85.60% 81.40% 83.94% 82.10% 82.65%\nExp 82.83% 82.36% 84.32%0 81.73%0 81.89%0 80.46%0 81.95%\nTable I1l-1: The performance table of machine learning\nschemes with linear and exponential weight variation\nmodels.\nThe instances based classifiers report the highest prediction\naccuracy when a linear weight variation model is applied.\nHowever, the classification may take days to finish. Using the\nFAN\/TAN classifiers is the most efficient, taking less than\nthree seconds in a standard Linux operation system. The\nprediction accuracy is approximately 81-82%.\nThese results imply that instances based classifiers should be\nused when the prediction accuracy is paramount. FAN\/TAN\ncan be used if preliminary results are required in a short time,\ne.g in a situation where a vast amount ofexperiments wait to be\ncarried out.\nC. Ranking the Amino Acid Feature Descriptors\nThere are 506 feature descriptors that could be used in the\nprocess of protein secondary structure prediction. Obviously\nusing all the feature descriptors in the prediction is not\neconomic, leading to time-consuming tasks. We thus need to\noptimise the selection of feature descriptors.\nThere are various ways to rank the feature descriptors. In this\nstudy each individual feature descriptor is applied to all of the\n3080 protein samples, and ranked by the self-test results. The\ntop ten ranked feature descriptors are listed in the following\ntable, by applying the instance based classifiers:\nFeature Descriptor ID Self-Testing Accuracy\nPTIO830101 55.40%\nMUNV940102 55.32%\nROBB760103 55.22%\nAURR980113 55.16%\nSUEM840101 55.00%\nKANM800103 54.93%\nAURR980109 54.90%\nQIAN880107 54.86%\nMUNV940101 54.83%\nRACS820108 54.81%\nTable 111-2: Top ten ranked feature descriptors.\nAlthough the process of prediction is time consuming, these\n10 feature descriptors are still tested by KStar, IBk, Decision\nTree, Bayesian network classifiers. The produced ranks are\nsimilar to the table listed above.\nThe top 168 ranked feature descriptors by applying the TAN\nclassifier is clustered and illustrated in the following table:\nAccuracy range Numbers of Attributes\n>= 55.0% 5\n>=54.0%&& <=55.0% 28\n>=53.0% && <= 54.0% 36\n1-4244-0470-3\/07\/$20.00 \u00a92007 IEEE\n568\n4\n2007 Inaugural IEEE International Conference on Digital Ecosystems and Technologies (IEEE DEST 2007)\n=52.0% && <= 53.0% I 311-.-0 &- 32\n>=5 1.0% && <= 52.0% 32\nTable 111-3: Top 168 ranked feature descriptors\nD. Selection ofthe Number ofApplied Feature Descriptors.\nThis experiment explores how the numbers of applied\nfeature descriptors influence the prediction accuracy. We are\nnot aware ofany existing results on the correlation between the\nnumbers of feature descriptors applied and the prediction\nresults. We can gain clear computational benefits if decreasing\nthe number of feature descriptors does not pull down the\nprediction accuracy drastically.\nIn the following diagram, the experiments are tested on the\n232 dataset, with the TAN classifier applied. Through the\ndiagram, the prediction accuracies do not vary radically while\nthe numbers of applied feature descriptors are changed. The\nquestions now is, which specific feature combinations should\nbe used?\n5\nstructure, and could easily be spotted by the classifiers used in\nthis research. Another interesting result is that the feature\ndescriptors \"BLAM930101\" and \"QIAN880132\" both feature\nin one third of the results. This phenomenon may mean that the\ncharacteristics ofthese two feature descriptors also have greater\ninfluences on the secondary structure of the amino acid than\nother feature descriptors - suggesting they may dominate any\npredictions in which they are involved.\nHilI i\n111-4: Feature Descriptor Appearances among\nCombinations of Good Quality\nSimilarly, the appearances of the feature descriptors among\nthe combinations with poor predictive quality are presented in\nthe following plot:\n111-3: accuracy ana numbers or appilea reature\ndescriptors.\nE. Selection ofCombinations ofFeature Descriptors.\nSelection of an optimal combination of feature descriptors is\nvital for accurate prediction of protein secondary structure. A\ntotal of 506 feature descriptors could be used, giving a vast\nnumber of potential combinations. We used a cluster of over\n100 processors to provide sufficient computational capacity for\nthese experiments.\nFirstly, combinations oftwo random feature descriptors from\nthe 506 sets are tested. Then random selections ofcombinations\nof any three feature descriptors from the top 168 feature\ndescriptors are tested. In general over five million tasks have\nbeen carried out.\nAmong these, a total of 139 combinations of feature\ndescriptors are categorized to be good quality combinations\nwith corresponding prediction accuracies of greater than 84%.\nThe diagram 111-4 shows the appearance of several particular\nfeature descriptors among these combinations. Several\ninteresting results are pertinent. Notice that feature descriptor\n\"BLAM930101\" appears in almost all of the good quality\npredictions. This potentially means that the physical\/chemical\ncharacteristics that the feature descriptor \"BLAM930101\"\nrepresents, may have a direct impact on the protein secondary\ntu III\n'.tuu{ _IraiwC7iD mnSD\n111-5: Feature Descriptor Appearances among\nCombinations of Bad Quality\nIV. CONCLUSION\nIn general, the PAMS tool is an effective tool for providing\neasy access to generate the desired datasets as and when a user\nwants. The above experiments significantly extend\nunderstanding ofthe machine learning schemes that participate\nin protein structure prediction activities. The instance-based\nclassifiers provide highest accuracy, but TAN\/FAN classifiers\nare significantly more efficient and hence suitable for use in\nexperiments containing vast amount of tasks. 232 protein\nsamples are selected to be both representative to the PDB\ndatabank and \"good quality\" in predicting protein secondary\nstructure in a template-free modelling manor. The window size\n1-4244-0470-3\/07\/$20.00 \u00a92007 IEEE\n569\n2, i\nm. . . l\n2007 Inaugural IEEE International Conference on Digital Ecosystems and Technologies (IEEE DEST 2007)\nof 21 is thought to be the optimal window size. The\nsuperiorities of the weight variation models are connected to\nboth the implemented classifiers and applied data sets. Finally,\nthe PAMS provides the basis for a unified tool for learning and\nprediction.\nV. FUTURE WORK OF THE PAMS\nIn the further study, a deeper study about the weight\nvariation model for the window mechanism is planned, to gain\nfurther understanding of how the adjacent amino acid residues\naffect the central amino acid's secondary structure type. More\ncombinations ofthe amino acid feature descriptors will also be\nexamined. Two additional development tasks are in hand for\nPAMS. Firstly, an automatic update module is required to\nretrieve online biological data resources. Secondly, the PAMS\nmodule could be migrated to web interface, enabling people\naround the world to configure it and generate the desired\ndataset, thus providing a foundation for the digital-ecosystem\ndiscussed in the introduction.\nACKNOWLEDGMENT\nWe would like to thank Dr Lee Gillam and Mr. Gary Dear in\nDepartment ofComputing in University of Surrey, support with\nthe computer grid; Dr Brendan Howlin and Dr Ian Hamerton in\nDivision of Chemistry of University of Surrey, for helpful\ndiscussion, Mr. Chris Bradshaw in Division ofChemistry for IT\nsupport.\nREFERENCES\n[1] E. M. Brunt and D. G. Tiniakos, \"Metabolic storage diseases:\namyloidosis,\" Clin. Liver Dis., vol. 8, no. 4, pp. 915-30, x, Nov.2004.\n[2] R. Taylor, \"Evolutions: Brain imaging,\" The Journal of NIH Research,\nMay 1990, Vol. 2, p. 103.\n[3] R. T. Johnson, \"Prion diseases,\" Lancet Neurol., vol. 4, no. 10, pp.\n635-642, Oct.2005.\n[4] R.W. Moss, Galen on Cancer - How Ancient Physicians Viewed\nMalignant Disease, 1989.\n[5] J. Drenth, Principles of Protein X-Ray Crystallography. Springer-Verlag\nInc. NY: 1999, ISBN 0-387-98587-5.\n[6] J.W. Akitt; B.E. Mann, NMR and Chemistry. Stanley Thornes.\nCheltenham, UK, 2000.\n[7] C. B. Anfinsen, \"Principles that govern the folding of protein chains,\"\nScience, vol. 181, no. 96, pp. 223-230, July1973.\n[8] B. Rost, \"Review: protein secondary structure prediction continues to\nrise,\" J. Struct. Biol., vol. 134, no. 2-3, pp. 204-218, May2001.\n[9] B. Rost and S. O'Donoghue, \"Sisyphus and prediction of protein\nstructure,\" Comput. Appl. Biosci., vol. 13, no. 4, pp. 345-356, Aug.1997.\n[10] Bowie, J. U.; Luthy, R.; and Eisenberg, D. 1991. \"A method to identify\nprotein sequences that fold into a known three-dimensional structure.\"\nScience 253:164 170.\n[11] Torda AE, Procter JB & Huber T (2004) \"Wurst: a protein threading\nserver with a structural scoring function, sequence profiles and optimized\nsubstitution matrices.\" Nucleic Acids Res 32, W532--W535.\n[12] MySQL Co. Ltd. http:\/\/www.mysql.com [Access Date: 07\/10\/2006]\n[13] H.M. Berman, J. Westbrook, Z. Feng, G. Gilliland, T.N. Bhat, H.\nWeissig, I.N. Shindyalov, P.E. Bourne: The Protein Data Bank. Nucleic\nAcids Research, 28 pp. 235-242 (2000).\n[14] U.Hobohm, M.Scharf, R.Schneider, C.Sander: \"Selection of a\nrepresentative set of structures from the Brookhaven Protein Data Bank\",\nProtein Science 1 (1992),409-417.\n[15] U.Hobohm, C.Sander: \"Enlarged representative set of protein structures\",\nProtein Science 3 (1994) 522.\n6\n[16] W. Kabsch and C. Sander, \"Dictionary of protein secondary structure:\npattern recognition of hydrogen-bonded and geometrical features,\"\nBiopolymers, vol. 22, no. 12, pp. 2577-2637, Dec.1983.\n[17] S. Kawashima, H. Ogata, and M. Kanehisa, \"AAindex: Amino Acid\nIndex Database,\" Nucleic Acids Res., vol. 27, no. 1, pp. 368-369,\nJan. 1999.\n[18] K. Tomii and M. Kanehisa, \"Analysis ofamino acid indices and mutation\nmatrices for sequence comparison and structure prediction of proteins,\"\nProtein Eng, vol. 9, no. 1, pp. 27-36, Jan.1996.\n[19] K. Nakai, A. Kidera, and M. Kanehisa, \"Cluster analysis of amino acid\nindices for prediction of protein structure and function,\" Protein Eng, vol.\n2, no. 2, pp. 93-100, July1988.\n[20] Ian H. Witten and Eibe Frank (2005) \"Data Mining: Practical machine\nlearning tools and techniques\", 2nd Edition, Morgan Kaufmann, San\nFrancisco, 2005.\n[21] R. Bouckaert, \"Bayesian Network Classifiers in Weka\", Technical\nReport, Department ofComputer Science, Waikato University, Hamilton,\nNZ 2005.\n[22] C.S. Lin, T. Smith, \"A tree-based algorithm for predicate-argument\nrecognition.\" Association for Computing Machinery New Zealand\nBulletin, 2(1): online journal, January 2006.\n[23] E. Frank, M. Hall, L. Trigg, G. Holmes, and I. H. Witten, \"Data mining in\nbioinformatics using Weka,\" Bioinformatics., vol. 20, no. 15, pp.\n2479-2481, Oct.2004.\n[24] U.M. Fayyad, and K.B. Irani. Multi-Interval Discretization of\nContinuous-Valued Attributes for Classification Learning. Proc. of 13th\nInt. Joint Conf. on Al, 1993.\n[25] P. Y. Chou and G. D. Fasman, \"Prediction of protein conformation,\"\nBiochemistry, vol. 13, no. 2, pp. 222-245, Jan.1974.\n[26] A. Chinnasamy, W. K. Sung, and A. Mittal, \"Protein structure and fold\nprediction using Tree-Augmented naive Bayesian classifier,\" J.\nBioinform. Comput. Biol., vol. 3, no. 4, pp. 803-819, Aug.2005.\n[27] B. Rost and J. Liu, \"The PredictProtein server,\" Nucleic Acids Res., vol.\n31, no. 13, pp. 3300-3304, July2003.\n[28] J. A. Cuffand G. J. Barton, \"Application of multiple sequence alignment\nprofiles to improve protein secondary structure prediction,\" Proteins, vol.\n40, no. 3, pp. 502-511, Aug.2000.\n[29] J. A. Cuff, M. E. Clamp, A. S. Siddiqui, M. Finlay, and G. J. Barton,\n\"JPred: a consensus secondary structure prediction server,\"\nBioinformatics., vol. 14, no. 10, pp. 892-893, 1998.\n[30] D. T. Jones and J. J. Ward, \"Prediction of disordered regions in proteins\nfrom position specific score matrices,\" Proteins, vol. 53 Suppl 6, pp.\n573-578, 2003.\n[31] Cleary J.G. and Trigg L.E. (1995) \"K*: An Instance-Based Learner Using\nan Entropic Distance Measure,\" Proc Machine Learning Conference,\nTahoe City, CA, USA, pp. 108-114.\n[32] J.R. Quinlan, \"C4.5: Programs for machine learning,\" San Mateo, CA:\nMorgann Kaufmann.\n[33] D. Przybylski and B. Rost, \"Alignments grow, secondary structure\nprediction improves,\" Proteins, vol. 46, no. 2, pp. 197-205, Feb.2002.\n[34] N. Qian and T. J. Sejnowski, \"Predicting the secondary structure of\nglobular proteins using neural network models,\" J. Mol. Biol., vol. 202,\nno. 4, pp. 865-884, Aug. 1988.\n[35] B. Rost, C. Sander, and R. Schneider, \"Redefining the goals of protein\nsecondary structure prediction,\" J. Mol. Biol., vol. 235, no. 1, pp. 13-26,\nJan. 1994.\n1-4244-0470-3\/07\/$20.00 \u00a92007 IEEE\n570\n"}