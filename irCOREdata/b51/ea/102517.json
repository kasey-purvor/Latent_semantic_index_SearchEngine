{"doi":"10.1109\/MIS.2006.4","coreId":"102517","oai":"oai:epubs.surrey.ac.uk:1979","identifiers":["oai:epubs.surrey.ac.uk:1979","10.1109\/MIS.2006.4"],"title":"CinemaScreen recommender agent: combining collaborative and content-based filtering","authors":["Salter, James","Antonopoulos, Nick"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2006-02-06","abstract":"<p>A film recommender agent expands and fine-tunes collaborative-filtering results according to filtered content elements - namely, actors, directors, and genres. This approach supports recommendations for newly released, previously unrated titles. Directing users to relevant content is increasingly important in today's society with its ever-growing information mass. To this end, recommender systems have become a significant component of e-commerce systems and an interesting application domain for intelligent agent technology.<\/p","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:epubs.surrey.ac.uk:1979<\/identifier><datestamp>\n      2017-10-31T14:03:53Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><setSpec>\n      6469766973696F6E733D656E67616E64706879736963616C736369656E636573:436F6D707574696E67<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/epubs.surrey.ac.uk\/1979\/<\/dc:relation><dc:title>\n        CinemaScreen recommender agent: combining collaborative and content-based filtering<\/dc:title><dc:creator>\n        Salter, James<\/dc:creator><dc:creator>\n        Antonopoulos, Nick<\/dc:creator><dc:description>\n        <p>A film recommender agent expands and fine-tunes collaborative-filtering results according to filtered content elements - namely, actors, directors, and genres. This approach supports recommendations for newly released, previously unrated titles. Directing users to relevant content is increasingly important in today's society with its ever-growing information mass. To this end, recommender systems have become a significant component of e-commerce systems and an interesting application domain for intelligent agent technology.<\/p><\/dc:description><dc:date>\n        2006-02-06<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/epubs.surrey.ac.uk\/1979\/1\/fulltext.pdf<\/dc:identifier><dc:identifier>\n          Salter, James and Antonopoulos, Nick  (2006) CinemaScreen recommender agent: combining collaborative and content-based filtering   IEEE Intelligent Systems, 21 (1).  pp. 35-41.      <\/dc:identifier><dc:relation>\n        10.1109\/MIS.2006.4<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/epubs.surrey.ac.uk\/1979\/","10.1109\/MIS.2006.4"],"year":2006,"topics":[],"subject":["Article","PeerReviewed"],"fullText":"JANUARY\/FEBRUARY 2006 1541-1672\/06\/$20.00 \u00a9 2006 IEEE 35\nPublished by the IEEE Computer Society\nCinemaScreen\nRecommender Agent:\nCombining\nCollaborative and\nContent-Based Filtering\nJames Salter and Nick Antonopoulos, University of Surrey\nD irecting users to relevant content is increasingly important in today\u2019s society withits ever-growing information mass. To this end, recommender systems have\nbecome a significant component of e-commerce systems and an interesting application\ndomain for intelligent agent technology. \nTraditionally, recommender systems employ col-\nlaborative filtering\u2014recommending movies, for\nexample, by matching a user to other users with sim-\nilar tastes and suggesting movies these others have\nenjoyed. For example, if Bob and Wendy liked the\nsame movies as you in the past and they both rated\nStar Wars highly, you might like it, too. However,\nrecommender systems that employ purely collabo-\nrative filtering can\u2019t recommend an item until sev-\neral users have rated it. \nWe\u2019ve developed a film recommender agent\n(available at www.filmrecommendations.co.uk) that\nextends predictions based on collaborative filtering\ninto the content space\u2014specifically, to actors, direc-\ntors, and film genres. This content-based filtering\nlets us include new movies in our recommendations.\nExperimental results show that our approach also\nimproves on the accuracy of predictions based solely\non content.\nRecommendation methods\nRecommender systems try to simulate the knowl-\nedge shopkeepers might develop about their cus-\ntomers\u2019 preferences over time.\nIn collaborative filtering, a recommender agent\nmatches a user to other users who\u2019ve expressed sim-\nilar preferences in the past. In content-based filtering,\nthe agent matches items users have previously rated\nhighly to other similar items, presuming that people\nwill like items similar to those they\u2019ve selected pre-\nviously. Similarity is based on content characteris-\ntics\u2014in movies, for example, on actors, directors,\nand genres.\nIf used in isolation, both techniques exhibit cer-\ntain weaknesses. Collaborative-only solutions suf-\nfer from a cold-start problem: the system can\u2019t pro-\nduce recommendations until a large number of users\nhave rated items in its databases. Systems relying\nexclusively on content-based filtering recommend\nonly items closely related to those the user has pre-\nviously rated. Such systems never reveal novel items\nthat users might enjoy outside their usual set of\nchoices. For example, if a user only rates war movies\nstarring a small set of actors, it\u2019s likely that the vast\nmajority of content-based system recommendations\nwill also be war movies starring those actors.\nPrevious work to combine the positive aspects of\nboth techniques has relied on one of two methods\n(see the \u201cRelated Work in Recommender Systems\u201d\nsidebar). One method generates two recommenda-\ntions sets\u2014one from each technique\u2014and then com-\nbines the results. The second method, collaboration-\nvia-content, expands each user\u2019s item ratings into\nratings for the item\u2019s content elements and then\nmatches to other users through a collaborative-fil-\ntering algorithm.\nLike collaborative filtering, collaboration-via-con-\ntent can only generate recommendations for items\nthat other users have already rated. Its content-based\ntechniques generate a set of intermediate scores\u2014\nfor example, a score for each actor, director, and film\ngenre. It then uses these intermediate scores, rather\nA film recommender\nagent expands and\nfine-tunes\ncollaborative-filtering\nresults according to\nfiltered content\nelements\u2014namely,\nactors, directors, and\ngenres. This approach\nsupports\nrecommendations for\nnewly released,\npreviously unrated\ntitles.\nA I ,  A g e n t s ,  a n d  t h e  W e b\nthan film ratings, in collaborative filtering to\nfind users with similar scores.\nThe CinemaScreen Recommender Agent\nuses a technique that reverses collaboration\nvia content. It executes content-based filter-\ning on a results set generated through col-\nlaborative filtering. By reversing the strat-\negy of filtering content first, the Cine-\nmaScreen Recommender Agent results set\ncan include films that haven\u2019t yet received\nany user ratings but do, for example, star\nactors that have also appeared in films from\nthe collaborative-filtering results.\nThis approach is crucial for the Cinema-\nScreen site function that lets users select\ntheir local cinemas and receive recommen-\ndations from the list of films currently show-\ning. The vast majority of these films are new\nor recent releases, so little or no rating data is\navailable.\nCinemaScreen recommender\nsystem\nWe built our system on the preexisting\nCinemaScreen Web site. The heart of the\nsystem is the recommender system (figure\n1), comprising the recommender agent and\nits associated databases. Users interact with\nthe system through a Web browser interface.\nThe system stores data in several Web\nserver databases. As figure 1 shows, these\ninclude information about users and the rat-\nings they\u2019ve given to films they\u2019ve seen, as\nwell as film information and showtime data-\nbases, which the CinemaScreen Web site\ngenerates for use by the site as a whole. A\npartly automated and partly manual process\ncollects and verifies data for the film infor-\nmation database from multiple sources such\nas news articles, archive collections, and\ndedicated content providers.\nFigure 2 presents a flow diagram for our\nrecommendation process.\nCollaborative filtering first\nThe top row in figure 2 describes the col-\nlaborative-filtering process. It involves first\nfinding a subset of users with film tastes sim-\nilar to the current user. Comparing the cur-\nrent user\u2019s rating history with the history of\nevery other user, the system finds the current\nuser\u2019s potential peers\u2014that is, other users\nwho have rated films the current user has\nrated. The system produces a list of films\nwith both rating sets and calculates a corre-\nlation measure between them (Pearson\u2019s\nproduct-moment correlation coefficient, r).1\nWe test the Pearson\u2019s r value, using a stan-\ndard significance test for the purpose. If the\nvalue is statistically significant, the agent\nadds the user to the current user\u2019s peer list. \nThis discriminating stage distinguishes our\nsystem from several others. Other approaches\ngenerate recommendations based on all users\u2019\nA I ,  A g e n t s ,  a n d  t h e  W e b\n36 www.computer.org\/intelligent IEEE INTELLIGENT SYSTEMS\nCollaborative filtering\nContent-based filtering\nCalculate weighted\naverage for each film\nin list\nFor each actor, add\ntheir average score to\ncurrent score for each\nfilm they appeared in\nCalculate average\nscore (predicted\nrating) for each film\nEnd\nGenerate list of films\nrated by at least one\nstatistically significant\npeer\nCalculate average\nscore of each actor,\ndirector, and genre\nFor each genre, add\naverage score to\ncurrent score for each\nfilm in that genre\nCalculate rating\ncorrelation between\ncurrent user and each\nother user\nFor each director, add\ntheir average score to\ncurrent score for each\nfilm they directed\nStart\nand genre it relates to\nFor each film in list,\nadd weighted average\nto current score for\neach actor, director,\nFigure 2. CinemaScreen recommendation process. The collaborative-filtering process\nfeeds a weighted list of films for the current user into the content-based filtering\nprocess. The weights reflect ratings by at least one statistically significant peer of the\ncurrent user.\nCinema\nWeb sites\nWeb\nbrowser Internet\nWeb robot\nCinemaScreen  \nWeb site\nHTML\nPages\nCinema\nshowtimes\nXML\nRecommender\nagent\nUser\nFilm info\ndatabase\nUser\nratings\nUser info\ndatabase\nShowtimes\ndatabase\nFigure 1. Components of the CinemaScreen recommender system.\nratings, even if the correlation with the cur-\nrent user\u2019s ratings is statistically insignificant.\nAlthough this approach might generate a\nlarger set of films for making recommenda-\ntions, it would likely also reduce the predic-\ntion accuracy.\nTo make its predictions, our collaborative-\nfiltering process uses the peer ratings and\ngives a weighted average to each film accord-\ning to the strength of each peer\u2019s correlation\nwith the current user. Conveniently, the value\nof Pearson\u2019s r for each user in the first stage\nis in the range \u00021 to 1, where 0 indicates no\ncorrelation, 1 indicates a perfect positive cor-\nrelation, and \u00021 indicates a perfect negative\ncorrelation. The agent can use peers with sig-\nnificant positive correlations to generate pre-\ndicted ratings.\nThe weighted mean equates to the pre-\ndicted rating for the film, and we calculate it\nas follows:\nwhere wf is the weighted mean for film f, P\nis the set of significant peers of the current\nuser, vp,f is the rating given by peer p to film\nf, rp is the correlation coefficient calculated\nfor peer p, and n is the current user\u2019s number\nof significant peers.\nOnce all calculations are complete, the\nagent stores the list of films and predicted rat-\nings. The system also stores the number of\nsignificant peers who rated the film because\nit gives an indication of the potential recom-\nmendation\u2019s strength. The system can there-\nfore use this number as a secondary sorting\nfield when it produces recommendation lists.\nThe system then feeds the predicted ratings\ninto the content-based filtering algorithms.\nContent-based filtering on \ncollaborative results\nWe designed the content-based filtering\nprocess (bottom two rows in figure 2) to use\ninformation about each film with a content-\nbased rating as input to the process of find-\ning links to other similar films.\nThere are several ways to find links. For\nexample, you could build film sets starring a\nparticular actor and associate a predicted rat-\ning with each set. However, we used a sim-\nple scoring mechanism. For every rated film\ninput to the process, the agent queries the film\ninformation database for relevant information\n, pp f\np P\nf\nv r\nw\nn\n\u0003\n\u0004\n\u0005\n\u0006\n \nJANUARY\/FEBRUARY 2006 www.computer.org\/intelligent 37\nGroupLens was one of the early recommender system imple-\nmentations.1 Unlike our hybrid technique, it used a collaborative-\nonly approach to generate recommendations. The GroupLens\nresearch group has since created MovieLens, a film recommender\nsystem. Again, it uses only collaborative filtering.\nAnother system, Ringo,2 uses collaborative filtering to recom-\nmend music to users, but it requires a training set of values to\ngenerate an initial user profile. Ringo therefore takes consider-\nable processing time before making recommendations available\nto users. The correlation method we use is faster,3 and it does-\nn\u2019t require training, which means it gives near-instantaneous\nrecommendations to users if any are available.\nIn their PTV system, Barry Smyth and Paul Cotter4 have used\nboth collaborative and content-based filtering to independently\ngenerate two recommendation sets, which are subsequently\ncombined. This approach differs from our agent, which seeks\nto combine the result sets at a much earlier stage. We believe\nour approach is better here, because determining how many\nitems from each recommendation set to include in the final\nrecommendation list is difficult, as is determining the order for\nlisting them when they are merged. Other examples of meth-\nods similar to PTV include Profbuilder,5 which asks users to\nmanually select between collaborative and content-based fil-\ntering results sets. It doesn\u2019t try to automatically combine the\ntwo methods.\nThe Fab system6 combines collaborative and content-based\nfiltering in its recommendations by measuring similarity be-\ntween users after first computing a profile for each user. This\nprocess reverses ours by running content-based filtering on the\nresults of collaborative filtering. Restaurant recommenders\nhave used this collaboration-via-content approach.7\nIn the context of a Chinese bookstore, Zan Huang and col-\nleagues used recommendations to test a graph-based method\nof combining collaborative and content-based filtering in a\ndigital library user service.8 However, their collaborative filter-\ning based user similarity on demographic information only,\nrather than the more usual technique of matching users based\non similar ratings.\nAndrew Schein and colleagues proposed an approach to the\ncold-start problem that used Bayesian techniques.9 However,\nresults show a na\u00efve Bayes recommender outperforms their\naspect model approach in predicting users\u2019 ratings for items of\nknown interest to them. The researchers didn\u2019t supply any cov-\nerage measures of the cold-start recommendations their system\ngenerated.\nReferences\n1. P. Resnick et al., \u201cGroupLens: An Open Architecture for Collabora-\ntive Filtering of Netnews,\u201d Proc. Conf. Computer Supported Coop-\nerative Work (CSCW 94), ACM Press, 1994, pp. 175\u2013186.\n2. U. Shardanand and P. Maes, \u201cSocial Information Filtering: Algorithms\nfor Automating \u2018Word of Mouth,\u2019\u201d Proc. Human Factors in Com-\nputing Systems Conf. (CHI 95), ACM Press\/Addison-Wesley, 1995,\npp. 210\u2013217.\n3. J.S. Breese, D. Heckerman, and C. Kadie, \u201cEmpirical Analysis of Predic-\ntive Algorithms for Collaborative Filtering,\u201d Proc. 14th Conf. Uncer-\ntainty in Artificial Intelligence, Morgan Kaufman, 1998, pp. 43\u201352.\n4. B. Smith and P. Cotter, \u201cPersonalised Electronic Program Guides for\nDigital TV,\u201d AI Magazine, Summer 2001, pp. 89\u201398.\n5. A.M.A. Wasfi, \u201cCollecting User Access Patterns for Building User\nProfiles and Collaborative Filtering,\u201d Proc. 1999 Int\u2019l Conf. Intelli-\ngent User Interfaces, ACM Press, 1999, pp. 57\u201364.\n6. M. Balabanovic\u00b4 and Y. Shoham, \u201cFab: Content-Based, Collaborative\nRecommendation,\u201d Comm. ACM, vol. 40, no. 3, Mar. 1997, pp. 66\u201372.\n7. M.J. Pazzani, \u201cA Framework for Collaborative, Content-Based and\nDemographic Filtering,\u201d Artificial Intelligence Rev., Dec. 1999, pp.\n393\u2013408.\n8. Z. Huang et al., \u201cA Graph-Based Recommender System for Digital\nLibrary,\u201d Proc. ACM\/IEEE Joint Conf. Digital Libraries (JCDL 2002),\nACM Press, 2002, pp. 65\u201373.\n9. A.I. Schein et al., \u201cMethods and Metrics for Cold-Start Recommen-\ndations,\u201d Proc. 25th Ann. Int\u2019l ACM SIGIR Conf. Research and\nDevelopment in Information Retrieval (SIGIR 02), ACM Press, 2002,\npp. 253\u2013260.\nRelated Work in Recommender Systems\n(actors, directors, and genres in this imple-\nmentation). It then adds the film\u2019s rating\n(either predicted or user-given) to the score\nfor each film element. For example, if the pre-\ndicted rating generated by collaborative fil-\ntering for a film were 5, each actor who\nstarred in the film would have 5 added to his\nor her score. Similarly, the agent would add 5\nto the director\u2019s score and the score for each\ngenre associated with the film.\nOnce it completes this process for all rat-\nings, the agent calculates the average score\nfor each actor, director, and genre. This score\nindicates how much the user likes or dislikes\neach element.\nThe agent can then compute the predicted\nrating for each film. For each element that\nreceived an average score, the agent queries\nthe film information database regarding the\nfilm. In a process similar to that for finding\nlinks, the element\u2019s average score is added\nto the film\u2019s score. System administrators\nwho are configuring the recommender sys-\ntem can also assign weights to the ele-\nments\u2014for example, giving more weight to\nthe actors than to a movie\u2019s genre.\nThe agent can then compute the predicted\nrating by dividing the film\u2019s total score by the\nnumber of elements used to calculate it. The\nagent can augment the list of films and pre-\ndicted ratings with any predictions that\nresulted from the initial collaborative-filter-\ning process but didn\u2019t appear in the final pre-\ndiction set (because of incomplete film infor-\nmation in the database). The agent also records\nthe number of elements for each film as an\nindicator of the prediction\u2019s strength, again so\nit can use the information as a secondary sort\nfield when it creates recommendation lists.\nDisplaying recommendations\nWe designed our system to be flexible,\nallowing system administrators to turn off\none of the filtering algorithms. If the admin-\nistrator turns off collaborative filtering (or if\ncollaborative filtering generated no recom-\nmendations), the content-based filtering\nmodule would have no input, so the actual\nratings given to films are also used for con-\ntent-based filtering. Where both a predicted\nrating from collaborative filtering and an\nactual user rating are available for a film, the\nagent uses the actual rating because it more\naccurately indicates the user\u2019s feelings about\nthe film.\nGenerating recommendations uses a lot of\nserver resources, so recalculating them more\noften than necessary is undesirable. Instead,\nafter generating a recommendation set, the sys-\ntem caches it for the remainder of the session,\nunless it encounters recalculation trigger\nevents, such as the user explicitly asking for a\nrecalculation or a counter reaching a trigger\nvalue for the number of new items rated since\nthe recommendation set was last generated.\nFigure 3 shows the system\u2019s Web-based\ninterface for displaying recommendations.\nRecommendation lists are paged, and but-\ntons at the bottom of the list facilitate access\nto next and previous pages.\nUsers can rate any of the recommended\nfilms they\u2019ve already seen by selecting a\nradio button to the right of the film title. The\nsystem saves these new ratings whenever a\nbutton is pressed. Users can force the system\nto produce recommendations at any stage by\nclicking the Save and Recommend button.\nBy submitting ratings, users are providing\nimplicit feedback about the quality of the rec-\nommendations produced.\nUsers can specify the film types they wish\nto rate to receive better recommendations by\nusing the \u201cLet Me Rate\u201d drop-down menu at\nthe top of the page. Items in the list include\nfilms from a particular decade, films within\na particular genre, and a complete listing of\nfilms the user has not yet rated.\nCinema-based recommendations\nThe system includes a Web robot to\nretrieve showtimes from cinema Web sites so\nusers can optionally restrict recommendations\nto films showing at cinemas near them.\nThe robot crawls highly targeted Web\nsites, as we require it to retrieve only explicit\ninformation (cinema name and location, film\ntitles, and dates and times of showings) from\nspecified cinema sites. A series of detailed\ntemplates define the crawl, giving the robot\nexact directions about what parts of each\nWeb page to extract. These templates give us\nconfidence in the retrieved data\u2019s quality (for\nexample, the name of the town where the cin-\nema is playing really is the name of the town\nand not part of a film title).\nIn the UK, system users select their local\ncinemas through an extension of the Web\ninterface. After selecting their postal code\narea (defined as the first one or two letters at\nthe beginning of their postcode) and the date\nthey wish to visit the cinema, the system pre-\nsents a list of available cinemas. Users can\ndeselect any cinemas they don\u2019t wish to visit.\nThe system generates a list of films show-\ning at the selected cinemas and extracts\nmatching films from the full list of generated\nrecommendations. It displays the list in an\nenhanced version of the standard results\ninterface, as shown in figure 4. For each film\nin the list, the display shows the cinema\nwhere it\u2019s playing. The list includes films the\nuser has previously rated if they meet certain\nrating strength conditions.\nSystem performance tests\nTo test the system\u2019s performance, we used\na data set consisting of 100,000 film ratings\nby 943 users. The University of Minnesota\u2019s\nGroupLens Research Group makes this data\nset publicly available for use in recommender\nsystem testing. The group collected the data\nbetween September 1997 and April 1998 on\nA I ,  A g e n t s ,  a n d  t h e  W e b\n38 www.computer.org\/intelligent IEEE INTELLIGENT SYSTEMS\nFigure 3. Recommendation display.\ntheir MovieLens Web site. The data set con-\ntained ratings given on an integer scale\nbetween one star (poor) and five stars (excel-\nlent). Recently, a new version of the Movie-\nLens data set consisting of one million rat-\nings by 6,000 users was released; we will test\nour system with it in the near future.\nOur tests involved automatically generat-\ning recommendations for a random sample of\n200 users. We removed a number of ratings\nfrom the data set for each user and attempted\nto generate predicted ratings. We wanted to\ncompare our technique with four others:\n\u2022 collaborative filtering used in isolation,\n\u2022 content-based filtering used in isolation,\n\u2022 collaboration-via-content filtering, and\n\u2022 running the results of content-based fil-\ntering through collaborative filtering.\nNumerous metrics are available for evalu-\nating recommender systems.2 Accuracy met-\nrics evaluate how well a system can predict\na rating for a specified item, which is a key\nmeasure of a recommender system\u2019s success.\nWe chose to use precision and recall in our\ntesting because they\u2019re popular, well-estab-\nlished metrics from the information retrieval\ncommunity. Precision measures the proba-\nbility that the system\u2019s selected films will be\nrelevant to the user, while recall measures the\nprobability that the system will select the\nentire set of relevant films.\nWe don\u2019t believe recall to be as important\nas precision for our recommender. Because a\nvisit to the cinema involves a financial out-\nlay, users should prefer a recommendation for\na film they are sure to enjoy (precision) over\na recommendation for all the films they might\nenjoy (recall). Erroneous recommendations\nfrom low system precision could decrease\nconfidence in the system to the point where\nusers might not trust any advice it gives.\nNevertheless, accuracy alone isn\u2019t suffi-\ncient proof of a recommender\u2019s usefulness.\nFor example, a recommender might be\nhighly accurate but produce rating predic-\ntions for only a small number of items.\nTherefore, we also use coverage metrics to\nindicate the number of films our system can\nproduce predictions about. We used three\ntypes of coverage in our testing:\n\u2022 Standard coverage measures the average\nnumber of films for which the system can\nproduce predictions for each user. We cal-\nculate this as a percentage of the total films\n(6,459) in the database.\n\u2022 Catalog coverage shows the percentage of\nfilms in the database for which the system\never generates predictions. We use it as a\nmeasure of recommendation diversity. \n\u2022 Prediction coverage shows how many of\nthe films removed from the user\u2019s rating\nset in our experiment to test for unrated\n(new) films could be regenerated by each\nrecommendation technique. By listing the\npredictions generated for films the user\nrated above each star threshold, we show\nhow many predictions the technique gen-\nerates for films the user would actually be\ninterested in watching. We express pre-\ndiction coverage as a percentage of the\ntotal number of ratings removed from the\ntest data set (50 for each user).\nFigure 5 plots each technique\u2019s precision\nand recall at various thresholds\u2014that is, the\npoint on a five-star rating system that marks\nthe border between a \u201cgood\u201d and \u201cbad\u201d film.\nObserving the overall precision, our technique\nis slightly\u2014but not significantly\u2014better than\nothers, excepting content-based filtering. How-\never, we can\u2019t consider content-based filtering\nto have the best overall performance, because\nit\u2019s recall is the worst at thresholds below four\nstars and misses approximately a third of rec-\nommendations at the one-star threshold. \nIf the user\u2019s threshold is set at 3.1, our\nagent\u2019s precision and recall are equivalent to\nany other technique, whereas content-based\nfiltering could recall less than 60 percent of\nmovies. A 3.1 threshold is significant, as it sep-\narates between one, two, and three stars (\u201cnot\nrelevant\u201d) and four and five stars (\u201crelevant\u201d).\nWe could also choose 3.5 as a sensible thresh-\nold; at this point, our technique shows equiv-\nalent precision and recall of around 10 percent\nless than the best-performing technique\u2014col-\nlaboration via content.\nThe recall graph shows our recall dropping\nat a higher rate than other methods beyond\nJANUARY\/FEBRUARY 2006 www.computer.org\/intelligent 39\n0\n1.0\n0.8\n0.6\n0.4\n0.2\n0\n1 2 3\nRating threshold\nCollaboration then content-based\nFiltering technique:\nContent-based then collaboration\n4 5\nPr\nec\nis\nio\nn\n0\n1.0\n0.8\n0.6\n0.4\n0.2\n0\n1 2 3\nRating threshold\n4 5\nRe\nca\nll\n(b)(a)\nContent-based only\nCollaboration via content\nCollaboration only\nFigure 5. Comparison of five recommender techniques for (a) precision and (b) recall. \nFigure 4. Interface to recommendations for films playing at local cinemas.\nthree stars. Having high precision but lower\nrecall shows that our agent can select four-\nand five-star films as well as other techniques\nbut has less capability in distinguishing\nbetween these films. Our agent made conser-\nvative recommendations, clustering films that\nusers in the test data set had rated as four or\nfive stars closer to three stars. The content-\nbased filtering algorithm causes this averag-\ning effect because it treats actors, directors,\nand genres independently. For example, a\nuser might give five-star ratings to three\ncomedies starring Robin Williams, but dis-\nlike two dramas he appeared in, giving them\nboth one star. The overall effect would be to\nlower the predicted rating for a fourth Robin\nWilliams comedy, even though the user\nappears to always enjoy his comedies.\nTable 1 summarizes coverage measures\nfor each recommendation technique. Rec-\nommender algorithms involving collabora-\ntive filtering in their final stage exhibit stan-\ndard coverage approximately six times lower\nthan our technique. This is because collabo-\nrative filtering can only make predictions for\nfilms that at least a few users have rated. Cat-\nalog coverage measurements support this,\nwith the last three techniques in the table also\nexhibiting catalog coverage over only a sixth\nof the database\u2019s films. Catalog coverage is\na useful guide to the novelty of recommen-\ndations; a system with low catalog coverage\nmight only be recommending blockbuster\nfilms that all users are likely to enjoy rather\nthan selecting films the user might not be\naware of.\nCoverage is particularly important for our\nagent, which users employ to recommend\nmovies from a small list of those showing at\nthe local cinema. If coverage was low, the\nsystem wouldn\u2019t be able to make a predic-\ntion. With the exception of content-based fil-\ntering, each technique had roughly compa-\nrable prediction coverage. Content-based\nfiltering\u2019s prediction coverage is low because\nnot all films in the database have sufficient\ninformation about directors, genres, and\nactors for the algorithm to deduce relation-\nships among them.\nBecause we designed our system to let\nusers view recommendations of movies at\ntheir local cinema, both coverage and pre-\ndiction accuracy for new movies is important.\nIt\u2019s highly probable that no users will have\nrated new movies at the time the system needs\nto generate predictions for them. To test this,\nwe modified our initial experiment. First, we\nremoved all ratings for a set of films, thus\nsimulating new movies that users hadn\u2019t yet\nrated. Then we used the remaining rating\ninformation for each user and attempted to\nregenerate predictions for this film set.\nThe prediction coverage is zero for the last\nthree techniques in table 2 because none of\nthem can make a prediction about a given\nmovie unless some users have rated it. Our sys-\ntem\u2019s prediction coverage is high because the\ncontent-based algorithm has enough informa-\ntion about the films to deduce relationships.\nFigure 6 shows the precision and recall\nresults from the revised experiment for our\nsystem and content-based filtering\u2014the only\nA I ,  A g e n t s ,  a n d  t h e  W e b\n40 www.computer.org\/intelligent IEEE INTELLIGENT SYSTEMS\nTable 2. Coverage of predictions using different recommendation techniques for new movies.\nStandard coverage Catalog coverage  Prediction coverage (percent, for n stars and above)\nFiltering technique (percent) (percent) 1 2 3 4 5\nCollaborative then content-based 64.4 66.4 98.8 98.8 98.8 98.7 98.9\nContent-based only 52.6 62.4 76.7 76.3 75.4 74.7 76.2\nCollaborative only 12.0 12.7 0 0 0 0 0\nContent-based then collaborative 11.8 12.7 0 0 0 0 0\nCollaboration via content 12.4 12.7 0 0 0 0 0\n0\n1.0\n0.8\n0.6\n0.4\n0.2\n0\n1 2 3\nRating threshold\nCollaboration then content-based\nFiltering technique:\n4 5\nPr\nec\nis\nio\nn\n0\n1.0\n0.8\n0.6\n0.4\n0.2\n0\n1 2 3\nRating threshold\n4 5\nRe\nca\nll\n(b)(a)\nContent-based only\nFigure 6. Comparison of two techniques for recommending new movies: (a) precision\nand (b) recall.\nTable 1. Coverage of predictions using different recommendation techniques.\nStandard coverage Catalog coverage  Prediction coverage (percent, for n stars and above)\nFiltering technique (percent) (percent) 1 2 3 4 5\nCollaborative then content-based 67.5 71.8 97.6 97.7 98.1 98.3 99.1\nContent-based only 40.9 67.2 65.2 65.7 66.5 68.3 70.4\nCollaborative only 11.9 14.2 96.5 96.6 97.2 97.3 98.0\nContent-based then collaborative 11.8 15.5 96.3 96.5 97.0 97.1 97.4\nCollaboration via content 13.3 14.2 97.9 98.0 98.3 98.3 98.5\ntwo techniques that produced predictions for\nthe simulated new movies.\nOur agent\u2019s precision is significantly higher\nthan content-based filtering for thresholds\nabove three stars, with recall becoming worse\nafter three and a half stars. However, the pre-\ncision isn\u2019t lower than in the first experiment,\nwhich means we can be confident in our sys-\ntem\u2019s ability to generate predictions for new\nmovies of a similar quality as for other movies\nin the database. Again, with a user\u2019s threshold\nset at 3.1, our technique classifies films as well\nas content-based filtering. At a threshold of\n3.5, our precision and recall are identical to\nthose of content-based filtering. When these\nscores are coupled with increased coverage,\nour technique is clearly better than others for\nrecommending new movies.\nThe second experiment confirms that our\ntechnique suffers only in the capability of dis-\ntinguishing between four- and five-star films\nto gain increases coverage and precision. Some\nprecision improvements are marginal but oth-\ners are significant under certain conditions.\nOverall, our testing results show that our\nagent can make more recommendations than\ncollaborative-filtering techniques and better\nrecommendations than content-based filter-\ning in terms of precision.\nAll of our measurements assume that users\nrate movies randomly rather than rating a\nmovie immediately after seeing it. If this is not\nthe case, the value of our enlarged standard\nand catalog coverage is decreased, but the\nvalue of our prediction coverage and recall\nwill increase. Recall will tend toward a mea-\nsure of the total number of relevant films that\nthe system would recommend and the user\nwould like. If the system\u2019s true recall were 100\npercent, further increasing coverage would\nhave no impact on the number of relevant rec-\nommendations the system could produce.\nImproving recommendations\nWe believe we can further improve our sys-\ntem\u2019s prediction accuracy by improving the\ncontent-based filtering algorithm. The algo-\nrithm appears to give an averaging effect, clus-\ntering predicted ratings around the midpoint.\nWe will investigate methods to reduce this\neffect by creating more complex groupings of\nactors, directors, and genres, rather than treat-\ning each as an independent entity. However,\ngenerating groupings of every combination of\nelements for each movie would have a devas-\ntating effect on the agent\u2019s time complexity.\nThe diversity of recommendations would also\nsuffer, as only films sharing several common\nelements would be considered similar. Finally,\nthis would eliminate the possibility of recom-\nmending a new film in, for example, the user\u2019s\nfavorite genre if it doesn\u2019t have any directors\nor actors in common with previously rated\nmovies. Therefore, we must investigate alter-\nnative methods that complement rather than\ninterfere with the existing behavior.\nThe current agent is an incomplete model of\nthe film recommendation process. It imple-\nments only some of the most obvious aspects.\nFor example, the agent doesn\u2019t consider\nchanges in user tastes and attitudes over time.\nAnother nontrivial parameter is a model of the\nway an actor or director\u2019s career evolves. For\nexample, a user might dislike certain actors in\ntheir early films but rate them higher as their\nskills developed in later films. The agent does-\nn\u2019t recognize these patterns; it creates an over-\nall average score for each actor.\nThe agent has no mechanism for deter-\nmining which actors played the main roles\nin a film and which made shorter appear-\nances. For example, the system establishes a\nlist of the user\u2019s preferred actors at the start\nof the session. The agent could recommend\na new film based on its inclusion of one or\nmore of these preferred actors. However,\nthese actors might only appear in short\ncameo roles and in reality have little bearing\non the user\u2019s enjoyment of the film.\nWe will use the results and experi-ence from this system\u2019s develop-\nment to further investigate the integration of\ncontent-based and collaborative-filtering\ntechniques. We plan to explore two avenues\nof further research.\nFirst, we want to enable the assignment of\ndifferent weightings to each filtering tech-\nnique\u2019s results according to certain parame-\nters. We also want to have the option of\nreversing the order of the filtering techniques\nused. For example, if the users\u2019 similarity is\nbased on a small number of ratings, we\u2019d like\nto assign higher weights to content-based fil-\ntering recommendations because the collab-\norative-filtering results include a high level\nof uncertainty. We want to test machine learn-\ning methods for calculating these weightings.\nSuch methods might also be appropriate for\ndynamically altering the component weight-\nings within the content-based filtering algo-\nrithm. For example, a highly rated director\nmight have more influence on a film recom-\nmendation than a highly rated actor.\nSecond, we want to apply and evaluate our\nhybrid recommendation method to other\ndomains and emerging technologies. Grid\ncomputing and peer-to-peer networking rely\non users being able to find resources. Rec-\nommender technology could help users find\npotential matches, including those in hidden\nresources available on the network. \nMobile computing is another application\narea. Current 3G mobile phones provide a\nviable platform for delivering rich content to\nmobile devices. However, users must pay for\nthe bandwidth they consume and the limited\nscreen size makes it cumbersome to select\nitems from large lists. Recommender tech-\nnology could help narrow the choices users\nmust make and enable the selection of appro-\npriate high-quality content. \nReferences\n1. J. Crawshaw and J. Chambers, A Concise\nCourse in A-Level Statistics, 3rd ed., Stanley\nThornes, 1994, pp.658\u2013664.\n2. J.H. Herlocker et al., \u201cEvaluation Collabora-\ntive Filtering Recommender Systems,\u201d ACM\nTrans. Information Systems, vol. 22, no. 1,\n2004, pp.5\u201353.\nJANUARY\/FEBRUARY 2006 www.computer.org\/intelligent 41\nT h e  A u t h o r s\nJames Salter is a PhD\nstudent in the Depart-\nment of Computing at\nthe University of Sur-\nrey. His research inter-\nests include recom-\nmender systems and\nresource discovery in\npeer-to-peer networks.\nHe has a BSc in computing and information\ntechnology from the University of Surrey. Con-\ntact him at the Dept. of Computing, Univ. of\nSurrey, Guildford, Surrey, GU2 7XH, UK;\nj.salter@surrey.ac.uk.\nNick Antonopoulos\nis currently lecturer and\ndirector of MSc Pro-\ngrammes in the Depart-\nment of Computing at\nthe University of Sur-\nrey. His research inter-\nests include emerging\ntechnologies, such as\nWeb services, peer-to-peer networks, software\nagent architectures, security, communication,\nand mobility management. He received his PhD\nin agent-based Internet computing from the Uni-\nversity of Surrey. Contact him at the Dept. of\nComputing, Univ. of Surrey, Guildford, Surrey,\nGU2 7XH, UK; n.antonopoulos@surrey.ac.uk.\n"}