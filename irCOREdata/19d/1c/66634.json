{"doi":"10.1117\/12.525853","coreId":"66634","oai":"oai:dro.dur.ac.uk.OAI2:652","identifiers":["oai:dro.dur.ac.uk.OAI2:652","10.1117\/12.525853"],"title":"Mapping perceived depth to regions of interest in stereoscopic images.","authors":["Holliman,  N. S."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":["Woods, Andrew J.","Merritt, John O.","Benton, Stephen A.","Bolas, Mark T."],"datePublished":"2004-05-21","abstract":"The usable perceived depth range of a stereoscopic 3D display is limited by human factors considerations to a\\ud\ndefined range around the screen plane. There is therefore a need in stereoscopic image creation to map depth\\ud\nfrom the scene to a target display without exceeding these limits. Recent image capture methods provide precise\\ud\ncontrol over this depth mapping but map a single range of scene depth as a whole and are unable to give\\ud\npreferential stereoscopic representation to a particular region of interest in the scene.\\ud\nA new approach to stereoscopic image creation is described that allows a defined region of interest in scene\\ud\ndepth to have an improved perceived depth representation compared to other regions of the scene. For example\\ud\nin a game this may be the region of depth around a game character, or in a scientific visualization the region\\ud\naround a particular feature of interest.\\ud\nTo realise this approach we present a novel algorithm for stereoscopic image capture and describe an implementation\\ud\nfor the widely used ray-tracing package POV-Ray. Results demonstrate how this approach provides\\ud\ncontent creators with improved control over perceived depth representation in stereoscopic images","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/66634.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/652\/1\/652.pdf","pdfHashValue":"749202b1f884f6464acca3beaae8d8a04192208a","publisher":"SPIE","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:652<\/identifier><datestamp>\n      2017-03-10T15:36:04Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Mapping perceived depth to regions of interest in stereoscopic images.<\/dc:title><dc:creator>\n        Holliman,  N. S.<\/dc:creator><dc:description>\n        The usable perceived depth range of a stereoscopic 3D display is limited by human factors considerations to a\\ud\ndefined range around the screen plane. There is therefore a need in stereoscopic image creation to map depth\\ud\nfrom the scene to a target display without exceeding these limits. Recent image capture methods provide precise\\ud\ncontrol over this depth mapping but map a single range of scene depth as a whole and are unable to give\\ud\npreferential stereoscopic representation to a particular region of interest in the scene.\\ud\nA new approach to stereoscopic image creation is described that allows a defined region of interest in scene\\ud\ndepth to have an improved perceived depth representation compared to other regions of the scene. For example\\ud\nin a game this may be the region of depth around a game character, or in a scientific visualization the region\\ud\naround a particular feature of interest.\\ud\nTo realise this approach we present a novel algorithm for stereoscopic image capture and describe an implementation\\ud\nfor the widely used ray-tracing package POV-Ray. Results demonstrate how this approach provides\\ud\ncontent creators with improved control over perceived depth representation in stereoscopic images.<\/dc:description><dc:subject>\n        Graphics systems<\/dc:subject><dc:subject>\n         Human factors<\/dc:subject><dc:subject>\n         Rendering<\/dc:subject><dc:subject>\n         Virtual reality<\/dc:subject><dc:subject>\n         Stereoscopic<\/dc:subject><dc:subject>\n         3D display.<\/dc:subject><dc:publisher>\n        SPIE<\/dc:publisher><dc:source>\n        Woods, Andrew J. & Merritt, John O. & Benton, Stephen A. & Bolas, Mark T. (Eds.). (2004). Stereoscopic displays and virtual reality systems XI. Bellingham, WA: SPIE, pp. 117-128, Proceedings of SPIE(5291)<\/dc:source><dc:contributor>\n        Woods, Andrew J.<\/dc:contributor><dc:contributor>\n        Merritt, John O.<\/dc:contributor><dc:contributor>\n        Benton, Stephen A.<\/dc:contributor><dc:contributor>\n        Bolas, Mark T.<\/dc:contributor><dc:date>\n        2004-05-21<\/dc:date><dc:type>\n        Book chapter<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:652<\/dc:identifier><dc:identifier>\n        issn:0277-786X<\/dc:identifier><dc:identifier>\n        doi:10.1117\/12.525853<\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/652\/<\/dc:identifier><dc:identifier>\n        https:\/\/doi.org\/10.1117\/12.525853<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/652\/1\/652.pdf<\/dc:identifier><dc:rights>\n        N. S. Holliman, \u201cMapping perceived depth to regions of interest in stereoscopic images,\u201d Proceedings of SPIE : Stereoscopic displays and virtual reality systems XI, A. J. Woods, J. O. Merritt, Stephen, A. Benton, Mark T. Bolas, Editors, 5291, 117-128 (2004).\\ud\n\\ud\nCopyright 2004 Society of Photo-Optical Instrumentation Engineers. One print or electronic copy may be made for personal use only. Systematic reproduction and distribution, duplication of any material in this paper for a fee or for commercial purposes, or modification\\ud\nof the content of the paper are prohibited.<\/dc:rights><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["issn:0277-786X","0277-786x"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2004,"topics":["Graphics systems","Human factors","Rendering","Virtual reality","Stereoscopic","3D display."],"subject":["Book chapter","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n21 August 2009\nVersion of attached file:\nPublished Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nHolliman, N. S. (2004) \u2019Mapping perceived depth to regions of interest in stereoscopic images.\u2019, in Proceedings\nof SPIE : Stereoscopic displays and virtual reality systems XI. Bellingham WA: SPIE, pp. 117-128.\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1117\/12.525853\nPublisher\u2019s copyright statement:\nN. S. Holliman, Mapping perceived depth to regions of interest in stereoscopic images, Proceedings of SPIE :\nStereoscopic displays and virtual reality systems XI, A. J. Woods, J. O. Merritt, Stephen, A. Benton, Mark T. Bolas,\nEditors, 5291, 117-128 (2004). Copyright 2004 Society of Photo-Optical Instrumentation Engineers. One print or\nelectronic copy may be made for personal use only. Systematic reproduction and distribution, duplication of any\nmaterial in this paper for a fee or for commercial purposes, or modification of the content of the paper are prohibited.\nAdditional information:\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\nMapping Perceived Depth to Regions of Interest in\nStereoscopic Images\nNick Holliman\nDepartment of Computer Science, University of Durham, Durham, United Kingdom\nABSTRACT\nThe usable perceived depth range of a stereoscopic 3D display is limited by human factors considerations to a\ndefined range around the screen plane. There is therefore a need in stereoscopic image creation to map depth\nfrom the scene to a target display without exceeding these limits. Recent image capture methods provide precise\ncontrol over this depth mapping but map a single range of scene depth as a whole and are unable to give\npreferential stereoscopic representation to a particular region of interest in the scene.\nA new approach to stereoscopic image creation is described that allows a defined region of interest in scene\ndepth to have an improved perceived depth representation compared to other regions of the scene. For example\nin a game this may be the region of depth around a game character, or in a scientific visualization the region\naround a particular feature of interest.\nTo realise this approach we present a novel algorithm for stereoscopic image capture and describe an imple-\nmentation for the widely used ray-tracing package POV-Ray. Results demonstrate how this approach provides\ncontent creators with improved control over perceived depth representation in stereoscopic images.\nKeywords: Graphics Systems, Human Factors, Rendering, Virtual Reality, Stereoscopic, 3D display\n1. INTRODUCTION\nThe properties of stereoscopic displays frequently present a challenge for content creators with many scenes\ncontaining a larger depth range than a 3D display can present to the viewer. For example, trials using a high\nquality desktop auto-stereoscopic 3D display by Sharp Corporation1 demonstrated that the range of available\nperceived depth range could be as little as [\u221250mm,+60mm] in-front and behind the display surface. These\nlimits are generally understood to result from a combination of factors including the display\u2019s level of inter-\nchannel crosstalk,2 the quality of the stereoscopic images3 and the ability of the viewer to fuse screen disparity\ninto a single image.\nMany methods have been proposed to control how scene depth is captured in a stereoscopic image. The\nproblem can be considered a mapping of scene depth onto the available perceived depth range on the target\nstereoscopic display as shown in figure 1(a). Recent solutions are able to guarantee this mapping has certain\nproperties, for example that the perceived depth will not exceed pre-defined ranges and that the perceived depth\nremains constant when there is head movement. This has resulted in benefits for content creators who can now\ndefine the scene depth range they want to capture while being sure that this will be reproduced as expected on\nthe target display. Experience generating images using one of these methods1 was positive, it allowed novice\n3D display users to have repeatable control over the depth mapping from scene to displayed image. As a result\ncontent production using computer graphics and digital photography no longer required repeated trial and error\nto produce an image with defined stereoscopic depth properties.\nHowever, current solutions to stereoscopic image capture have drawbacks due to the fact that they map a\nsingle range of scene depth onto the target display depth range as a whole. If the scene depth range is larger than\nthe available display depth range this will result in compression of perceived depth, as in figure 1(a). This has\nthe consquence that a region of interest in scene depth cannot be guaranteed a good representation in perceived\ndepth in the displayed image.\nFurther author information: (Send correspondence to Nick Holliman.)\nEmail: n.s.holliman@durham.ac.uk, Telephone: +44 191 334 1761, Web: http:\/\/www.durham.ac.uk\/n.s.holliman\/\nScene depth range\nPerceived depth range\nPhysical display\nVirtual display\n(a)\nScene depth range\nPerceived depth range\nRegion of interest\n(b)\nNR FR\nFigure 1. Stereoscopic image capture maps a scene depth range to a display perceived depth range: (a) Existing methods\nmap a defined range of scene depth to the usable display perceived depth as a whole. (b) The new algorithm maps depth\ndifferently in three regions, near region(NR), region of interest (ROI) and far region(FR) allowing the available stereoscopic\ndepth range to be split differently between the three regions, for example, as shown the ROI may be given the best stereo\ndepth representation.\nOur new approach to depth mapping allows the content creator to define different mappings from real scene\ndepth to perceived display depth for three different ranges; a region of interest (ROI) in the scene and a near\nregion (NR) and far region (FR) in the scene, as shown in figure 1(b). All three regions can be mapped to a\ndifferent proportion of the available perceived depth range on the target 3D display. The result is that the image\ncreator can choose to give better stereoscopic representation to the ROI compared to the near and far regions.\nWe aim to retain the scene outside the region of interest, rather than clip it or disguise it by blurring, because\nin many applications it is important to see context around the region of interest. For example in a game, or\nother interactive application, it can be important to see into the distance to check what is coming, but it is not\nimportant to have this represented using high resolution perceived stereo depth, particularly when other depth\ncues are available.4 Applications that will benefit from the new algorithm are primarily those that are computer\ngraphics based and include computer games, medical, scientific and information visualisation.\n2. BACKGROUND\nThe benefits provided by stereoscopic displays are well known3, 5\u20137 and include; depth perception relative to a\ndisplay surface, perception of structure in visually complex scenes, improved perception of surface curvature,\nimproved motion judgement and improved perception of surface material type. These benefits have been to\nshown to be helpful in providing users a better understanding or appreciation of visual information in a range\nof studies in different application domains.4, 8\u201310\nAn important step in obtaining these benefits is to use high quality 3D displays, with good basic 2D image\nquality (brightness, contrast, high resolution, full colour, moving images) as well as good 3D image quality (image\nchannels well aligned spatially, in brightness and contrast and with low inter-channel crosstalk).3, 8 Such displays\nare becoming available for desktop and laptop PC systems from companies including Dimension Technologies,\nStereographics, SeeReal and Sharp Corporation.3\nFor the user the quality of the stereoscopic image pair is at least as important as the quality of the display it is\npresented on. The image pair needs to be captured with aligned cameras (or rectified to appear as if it has been)\nand to have minimal differences in luminance, contrast, focus and magnification. Synthetic computer graphics\nsystems are better than optical photographic systems in this respect as the imaging process is not affected by\nphysical alignment differences between the two capture channels. For all stereoscopic image capture methods a\nkey requirement is to control the amount of perceived depth seen when the image is displayed on the target 3D\ndisplay as this directly affects the viewer\u2019s ability to perceive a comfortable fused 3D image.\n2.1. The Geometry of Perceived Depth\nDepth perception in planar stereoscopic images has been widely studied and the geometry of stereoscopic depth\nperception is well known.3, 11\u201313 The image disparity captured when a stereo image pair is created becomes\nphysical screen disparity when the stereo pair is displayed on an electronic 3D display. The screen disparity is\ndetected by the retina and interpreted by the brain as a perceived depth in-front or behind the screen plane, as\nshown in figure 2.\nL\nR\nL\nR\nd\nd\np\np\nz\ne\ne\np, perceived depth\nz, viewing distance\ne, eye separation\nd, screen disparity\n(1) gpd = z \/ ( (e\/d) \u2013 1 )\n(2) gpd = z \/ ( (e\/d) + 1 )\n3D display plane\nFigure 2. Geometric perceived depth for positive, (1), and negative, (2), screen disparity.\nWhile a viewer\u2019s actual perception of depth resulting from a given screen disparity is important we adopt\nthe common approximation of geometric perceived depth gpd.1, 3, 12, 13 This is calculated, as shown in figure 2,\nfrom the value of screen disparity the viewer perceives and it is currently thought to provide a good working\napproximation to the viewer\u2019s actual perceived depth.\nAs noted earlier the range of gpd should be limited if the viewer is to comfortably fuse a stereo image pair.1\nIn this paper we adopt a target gpd range around the display plane of [\u221255mm,+55mm] as a conservative, but\nuseful, range for users with normal stereoscopic vision, viewing a desktop stereoscopic display.\n2.2. Current Approaches to Stereoscopic Image Generation\nThe generation of high quality stereoscopic images has received substantial interest and recent methods have\nachieved new quality levels. Recent reviews of the literature can be found in several publications1, 3, 14, 15 and\nhere we briefly review two approaches, analysing one in more detail.\nWartell14, 15 studied the distortions in gpd due to different assumptions about the link between eye separation\nand camera placement when generating images for head tracked stereoscopic displays. Methods to remove part\nor all of these distortions by pre-distorting the scene using a defined transformation matrix were presented.\nThese methods worked well but the best was complex and only easily applied to the generation of computer\ngraphics images. In addition these methods considered the scene as a whole and were unable to give preferential\ntreatment to a particular region of interest in the scene.\nJones et al1 presented a method capable of controlling gpd applicable to content creation using computer\ngraphics or digital cameras. The algorithm maps scene depth to perceived display depth as illustrated in fig-\nure 1(a). Near and far limits of a single range of scene depth (top) are identified and the algorithm maps these\nto be within the defined gpd range of the target 3D display (bottom). The problem of choosing a stereo camera\nseparation is removed from the user as it is automatically calculated, along with other parameters, to achieve\nthe required mapping between scene depth and perceived display depth.\n400 450 500 550 600 650 700 750\nsd\n520\n540\n560\n580\n600\n620\ngpd\na bc\nFigure 3. The result of applying the Jones1 single range algorithm. In the orthoscopic case (a) the gpd directly reproduces\nscene depth sd. In case (b) scene depth is compressed into the available gpd range and in case (c) scene depth is expanded\ninto the available gpd range.\nThe result of applying the Jones algorithm to map different scene depth ranges to the same target gpd is\nillustrated in figure 3. This uses target display parameters similar to the Sharp RD3D display and assumes a\nnominal viewing distance of 570mm16 and a target gpd of [-55,+55] around the screen plane. In figure 3(a) the\nscene depth matches the gpd target and there is a one-to-one correspondence between the two. We can expect\nthe viewer will see minimal difference between perceived depth in the original scene and on the display as long as\nthe gpd target is kept within Panum\u2019s fusional area for the target display. In figure 3(b) the scene depth range\ncaptured is greater than the gpd target and there is predicted a compression of the scene depth; here the viewer\nis likely to perceive discrete objects to be closer together on the display than in the original scene. The opposite\nis predicted in figure 3(c) where the scene depth is smaller than the gpd target and there is an expansion of\nscene depth; here the viewer is likely to perceive discrete objects further apart on the display than in the original\nscene. Previous studies and our experience suggest these effects are apparent and we note in almost all cases\nwhen creating 3D content the scene depth is either compressed or expanded when viewed on a 3D display.\nConsidering the case in figure 3(b) where scene depth is compressed; an original range of [370, 770] is com-\npressed to a gpd of just [515, 625]. In this case a particular region of interest in the scene is at risk of receiving\nrelatively little stereo representation in the displayed 3D image. For example, a region of interest in the scene\nof [515, 625] will be represented to the viewer by a gpd range of [569, 597] a depth compression by a factor of\napproximately fourteen.\nIn summary the guarantee by the Jones1 algorithm not to exceed pre-defined ranges of gpd ensures that the\nviewer never sees excessive depth on the display treating the scene as a whole. If there is a specific region of\ninterest in a scene then it cannot be given better stereoscopic representation than other regions of the scene. In\naddition if the total scene depth range is varying such as in an animation or real time scene navigation then the\nregion of interest will not have a constant gpd representation.\n3. A THREE-REGION DEPTH MAPPING ALGORITHM\nWe present a new algorithm that uses a three-region depth mapping to overcome the limitations of previous\nsingle region depth mapping algorithms and as a result provide new control over the way scene depth is mapped\nto gpd.\nWe consider two distinct geometries; the geometry defining the relationship between the viewer and the\ndisplay and the geometry of the scene and camera. The three-region algorithm maps the defined regions, NR,\nROI and FR in scene depth onto corresponding defined ranges of gpd. This mapping is required to meet the\nconstraint that points on the region boundaries are projected to coincident positions, and hence depth, in the\nimage pair whichever region they are considered to belong to. The algorithm implements perspective projection\nas a piecewise continuous function of scene depth and uses a different perspective projection (different stereo\ncameras) to capture each of the three regions.\n3.1. Display Geometry\nThe geometry of display viewing is illustrated in figure 4. The viewer\u2019s half eye separation is given by e, the screen\nplane is at distance z from the viewer and the half screen width is w. The total working geometric perceived\nTotal gpd range\nROI\ne\nw\nFRNR\nF\nf\nz\nn\nN\nviewer eye positions\nperceived points at region limits.\ndisparity of points at region limits.\ndisplay width limits.\ndN\ndn\ndf\ndF\nPhysical display plane\nFigure 4. The display viewing geometry showing total gpd range and the split into near, ROI and far regions.\ndepth range is between the planes at distances N and F from the viewer. The total gpd range is divided into\na near range, NR [N,n], a region of interest range, ROI [n, f ], and a far range, FR [f, F ], by planes defined at\ndistances n and f from the viewer.\nThe half screen disparities of points lying on the display viewing centre line for the planes at distances N ,\nn, f , F are given by dN , dn, df and dF respectively. Note, in each case just the half disparity for one view is\nshown with the matching half from the other view omitted for clarity.\n3.2. Scene Geometry\nThe geometry of the scene and camera is illustrated in figure 5. We assume that the image creator has positioned\na single camera that frames the required view of the scene. The total depth range in the scene we are asked to\ncapture is [N \u2032, F \u2032] and this is divided into the near [N \u2032, n\u2032], ROI [n\u2032, f \u2032] and far [f \u2032, F \u2032] regions by the planes at\ndistances n\u2032 and f \u2032 from the viewer. These regions will be mapped to the defined ranges of gpd on the target\ndisplay.\nIn single region methods a single stereo camera separation a\u2032 is calculated to position the camera to take the\nleft and right images. In the three-region approach we need to find three camera separations one each for the\nNR, ROI and FR regions. The calculations to determine these are described in the following three sections.\n3.3. Region of Interest Mapping\nThe ROI mapping uses essentially the same approach as the Jones algorithm,1, 17 although it differs in some of\nthe measurements used for clarity.\nThe aim is to map the region of depth in the scene defined as the ROI onto the matching region of gpd\nidentified for the target display. As in the original method the display is represented in the scene by a virtual\ndisplay plane. This allows us to consider the mapping of scene depth onto disparities in the virtual display plane\nseparately from the physical disparities on the target display.\nConsidering the geometry of the display space in figure 4 we can derive by similar triangles that the following\nrelationships hold:\ndn =\ne(z \u2212 n)\nn\n(1)\na\u2019\nw\u2019\nF\u2019\nf\u2019\nz\u2019\nn\u2019\nN\u2019\ncamera positions\ndisparity of points at region limits.\ndisplay width.\nd\u2019n\nmonoscopic camera position\nd\u2019f\nt\u2019m\nROI\u2019NR\u2019 FR\u2019\nt\u2019\nVirtual display plane.\na\u2019\nFigure 5. The scene geometry showing defined near, ROI and far regions of scene depth and the monoscopic camera\nposition.\ndf =\ne(f \u2212 z)\nf\n(2)\nConsidering the geometry in the scene in figure 5 we can derive by similar triangles that the following\nrelationships hold:\nd\u2032n =\na\u2032(z\u2032 \u2212 n\u2032)\nn\u2032\n(3)\nd\u2032f =\na\u2032(f \u2032 \u2212 z\u2032)\nf \u2032\n(4)\nThe link between these quantities is that the ratio of the disparities remains the same between the display\nand the virtual display in the scene.1 Intuitively this is the case because the link between the scene and the\ndisplay is the captured image which is the same except for a scale factor between the virtual display in scene\nspace and the physical display. The ratio is:\nr =\ndn\ndf\n=\nd\u2032n\nd\u2032f\n(5)\nAs we are given e, n, f and z we can simply calculate the value of r. However, we do not know the distance\nto the virtual screen plane z\u2032 and of particular interest the half camera separation a\u2032. We can derive expressions\nfor these by substituting (3) and (4) into (5) as below:\nr =\nf \u2032(z\u2032 \u2212 n\u2032)\nn\u2032(f \u2032 \u2212 z\u2032) (6)\nwhich we can rearrange to give an expression for z\u2032:\nz\u2032 =\nf \u2032n\u2032 + f \u2032n\u2032r\nf \u2032 + n\u2032r\n(7)\nWe can now find the virtual screen width w\u2032, using t\u2032m the monoscopic camera field of view.\nw\u2032 = z\u2032tan\n(\nt\u2032m\n2\n)\n(8)\nFrom which we get the scaling, s, for disparities from the target display to the virtual screen.\ns =\nw\u2032\nw\n(9)\nAs we are given the target disparity ranges on the physical screen we can now calculate the disparity ranges\non the virtual screen. Then by rearranging (3) and substituting d\u2032n = sdn we can find a\n\u2032:\na\u2032 =\nsdnn\n\u2032\nz\u2032 \u2212 n\u2032 (10)\nWe now have almost all the information needed to calculate the left and right camera positions and generate the\nleft and right partial images for the ROI region with the exception of the new field of view:\nt\u2032 = 2arctan\n(\nw\u2032 + a\u2032\nz\u2032\n)\n(11)\nThis is the field of view for a camera with a symmetric frustum and hence we must clip a proportion of pixels\nfrom the left and right edges of the left and right partial images respectively. This ensures that points projecting\nzero disparity onto the virtual screen plane will also have zero disparity when displayed on the physical screen\nplane. The proportion of pixels to crop is given by:\nc\u2032 =\na\u2032\nw\u2032 + a\u2032\n(12)\n3.4. Near Region Mapping\nFor the near region (NR) we map the scene depth in [N \u2032, n\u2032] to the gpd [N,n] using the same image plane used\nfor the ROI mapping, as shown in figure 6.\nWe need to ensure that points on the plane at n\u2032 map to the same position in the final image whether they\nare mapped by the ROI step or the NR step. We can consider this to be a constraint that the field width of the\nROI camera and the NR camera be the same in the plane at distance n\u2032 from the camera location. This will\nresult in a piecewise continuous representation of stereoscopic depth which meets at region boundaries but may\nnot be smoothly continuous.\nFor the NR mapping we calculate a new half camera separation a\u2032\u2032, a symmetric field of view and the\nassociated image cropping. Additionally we need to calculate an offset adjustment o\u2032\u2032 to shift the NR disparity\nrange to be continuous with the disparity range for the ROI region.\nWe first consider the disparity on the virtual screen plane of a point on the planes at the near region limits\nof N \u2032 and n\u2032 when projected from a camera at our new near region camera position a\u2032\u2032.\nThe virtual screen disparity of a point on the far limit of the near region at n\u2032 is given by:\nd\u2032\u2032n =\na\u2032\u2032(z\u2032 \u2212 n\u2032)\nn\u2032\n(13)\nThe virtual screen disparity of a point on the near limit of the near region at N \u2032 is given by:\nd\u2032\u2032N =\na\u2032\u2032(z\u2032 \u2212N \u2032)\nN \u2032\n(14)\na\u2019\nw\u2019\u2019\nF\u2019\nf\u2019\nz\u2019\nn\u2019\nN\u2019\nROI camera positions\ndisparity of points at region limits\nfield widths\nd\u2019n\nmonoscopic camera position ROI\u2019NR\u2019 FR\u2019\nNR camera positions\na\u2019\u2019\nd\u2019\u2019N\nd\u2019\u2019n\no\u2019\u2019\no\u2019\u2019\nFigure 6. The scene geometry showing the variables related to the near region image generation.\nWe note that d\u2032\u2032N \u2212d\u2032\u2032n = s(dN \u2212dn) since we define [dN , dn] to be the target disparity range for the NR depth\nand hence, using equations(13) and (14), we find a\u2032\u2032 to be:\na\u2032\u2032 =\ns(dN \u2212 dn)(\nz\u2032\u2212N \u2032\nN \u2032\n)\u2212 ( z\u2032\u2212n\u2032n\u2032 ) (15)\nWe now find the offset correction to the disparity on the virtual screen so that the near region disparity is\ncontinuous with the ROI region disparity in the virtual screen plane:\no\u2032\u2032 = d\u2032n \u2212 d\u2032\u2032n (16)\nwhere d\u2032\u2032n is given by (13). Equation (16) may be derived by inspection or by considering application of the\nintercept theorem18 to the relevant geometry.\nThe field of view for the NR camera can now be calculated if we know the half field width w\u2032\u2032 in the virtual\nscreen plane which can be found as below:\nw\u2032\u2032 = w\u2032 \u2212 o\u2032\u2032 + a\u2032\u2032, o\u2032\u2032 < a\u2032\u2032 w\u2032\u2032 = w\u2032 \u2212 a\u2032\u2032 + o\u2032\u2032, o\u2032\u2032 >= a\u2032\u2032 (17)\nThe symmetric field of view for the left and right NR cameras is then:\nt\u2032\u2032 = 2atan\n(\nw\u2032\u2032\nz\u2032\n)\n(18)\nThere is a need to crop a proportion of pixels, c\u2032\u2032, from the resulting images where\nc\u2032\u2032 =\na\u2032\u2032 \u2212 o\u2032\u2032\nw\u2032\u2032\n, o\u2032\u2032 < a\u2032\u2032 c\u2032\u2032 =\no\u2032\u2032 \u2212 a\u2032\u2032\nw\u2032\u2032\n, o\u2032\u2032 >= a\u2032\u2032 (19)\nif o\u2032\u2032 < a\u2032\u2032 then we crop pixels from the left of the left image and the right of the right image while if o\u2032\u2032 >= a\u2032\u2032\nthen we crop pixels from the right of the left image and left of the right image.\nWe now have the new camera parameters and image adjustments we need to render the NR partial images\nfor the left and right views. While the projection of the NR and ROI regions will differ in order to map depth\ndifferently from scene depth to available gpd for each region we have ensured the depth effect will be piecewise\ncontinuous at the region boundary.\n3.5. Far Region Mapping\nFor the far region (FR) we need to map the scene depth in [f \u2032, F \u2032] to the gpd range [f, F ] rendering onto the\nsame image plane used for the ROI mapping, as shown in figure 7. The method is directly analogous to the NR\nalgorithm described above and is derived in a similar manner.\na\u2019\nF\u2019\nf\u2019\nz\u2019\nn\u2019\nN\u2019\nROI camera positions\ndisparity of points at region limits\nfield width limits\nmonoscopic camera position\nd\u2019f\nROI\u2019NR\u2019 FR\u2019\nNR camera positions\na\u2019\u2019\u2019\nd\u2019\u2019\u2019Fd\u2019\u2019\u2019f\nw\u2019\u2019\u2019\no\u2019\u2019\u2019\nFigure 7. The scene geometry showing the parameters related to far region image generation.\nAs for the NR mapping we need to ensure that points on the plane at f \u2032 map to the same position in the\nfinal image whether they are mapped by the ROI step or the FR step. We can consider this as a constraint that\nthe FR and ROI cameras have the same field width in the plane f \u2032.\nWe need to calculate a new camera separation a\u2032\u2032\u2032 that will map [f \u2032, F \u2032] to [f, F ], calculate the symmetric\nfield of view, associated cropping and finally calculate the offset needed to adjust the disparity range of the far\nregion to be piecewise continuous with that of the far limit of the ROI disparity range.\nWe first determine the disparity on the virtual screen plane of a point on each of the planes at the far region\nlimits of f \u2032 and F \u2032 when projected from a camera at our new far region camera position a\u2032\u2032\u2032 as below.\nThe virtual screen disparity of a point on the near limit of the far region at f \u2032 is given by:\nd\u2032\u2032\u2032f =\na\u2032\u2032\u2032(f \u2032 \u2212 z\u2032)\nf \u2032\n(20)\nThe virtual screen disparity of a point on the far limit of the far region on the plane F \u2032 is given by:\nd\u2032\u2032\u2032F =\na\u2032\u2032\u2032(F \u2032 \u2212 z\u2032)\nF \u2032\n(21)\nWe note that d\u2032\u2032\u2032F \u2212 d\u2032\u2032\u2032f = s(dF \u2212 df ) by definition, since we are given [df , dF ] as the target disparity range.\nHence from (20) and (21) we find a\u2032\u2032\u2032 to be:\na\u2032\u2032\u2032 =\ns(dF \u2212 df )(\nF \u2032\u2212z\u2032\nF \u2032\n)\u2212 ( f \u2032\u2212z\u2032f \u2032 ) (22)\nWe need to calculate the correction to the disparity on the virtual screen so that the far region is continuous\nwith the ROI:\no\u2032\u2032\u2032 = d\u2032f \u2212 d\u2032\u2032\u2032f (23)\nWhere d\u2032\u2032\u2032f is given by (20). Equation (23) may be derived by inspection or by application of the intercept\ntheorem18 to the relevant geometry.\nThe field of view for the FR camera location can be calculated if we know the field width w\u2032\u2032\u2032\nw\u2032\u2032\u2032 = w\u2032 + o\u2032\u2032\u2032 + a\u2032\u2032\u2032 (24)\nThe symmetric field of view for the FR camera is then\nt\u2032\u2032\u2032 = 2atan\n(\nw\u2032\u2032\u2032\nz\u2032\n)\n(25)\nThere is then the need to adjust the zero disparity plane by cropping a proportion of pixels from the left of\nthe left image and the right of the right image as below:\nc\u2032\u2032\u2032 =\na\u2032\u2032\u2032 + o\u2032\u2032\u2032\nw\u2032\u2032\u2032\n(26)\nWe now have the new camera parameters and image adjustments we need to render the FR partial images\nfor the left and right views. While the projection of the FR and ROI regions will differ in order to map depth\ndifferently from scene to display in each region we have ensured the depth effect will be piecewise continuous at\nthe region boundary.\n4. IMPLEMENTATION\nThe three-region mapping algorithm described above has been implemented using the POV-Ray ray tracing\nsystem to generate images of test scenes and application examples.\nTo use the POV-ray scripts the user defines a monoscopic camera position and field of view to frame the view\nthey wish to generate. They provide details of the target display including the proportion of the total available\ngpd to be used for the FR, ROI and NR regions. In addition the total scene depth and the proportions to be\nmapped to the FR, ROI and NR regions are specified.\nGiven the above information a script is run to analyse the depth range in the scene and calculate the\nstereoscopic camera parameters for the three regions. Note, that when rendering the NR, ROI and FR regions\nwe only use the scene geometry within each region so the script also calculates appropriate clipping volumes.\nThe resulting scripts are then run to generate three partial images for both the left and the right views.\nFinally the partial images are merged in depth order from far to near resulting in the stereo image pair which\ncan then be displayed on the target 3D display.\n5. RESULTS\nThe POV-ray results for a simple test scene are shown in figure 8. This shows the three regions rendered as\npartial images and the stereo pair resulting from merging the partial images in depth order.\nA comparison of the results of the new three-region mapping algorithm with an existing single region algorithm\nis shown in figure 9. The upper graph in figure 9 shows how the single range mapping compresses the region\nof interest along with the rest of the scene as a whole into the available gpd. The lower graph shows how the\nthree-region algorithm is able to distribute available gpd preferentially to the region of interest, in this case the\nROI is given a one-to-one representation at the expense of further compression of gpd in the near and far regions.\nAs well as preferentially allocating available perceived depth to the region of interest the new algorithm has\nthe benefit of being able to fix the gpd of the ROI even if the total depth range in the scene is changing. This\nseems likely to be of significant benefit when, for example, moving a game character around a scene with varying\ndepth range. The depth representation of the game character can now be fixed where with the previous single\nrange mapping the character\u2019s depth representation will vary depending on the total scene depth visible from\nthe current viewpoint.\nFigure 8. The partial images from left to right for the FR, ROI and NR regions are combined in depth order to form\nthe stereo pair for this view shown at the far left. The original images are available on the author\u2019s web site.\n450 500 550 600 650 700\nsd\n520\n540\n560\n580\n600\n620\ngpd\n450 500 550 600 650 700\nsd\n520\n540\n560\n580\n600\n620\ngpd\nFigure 9. The result of applying an existing single-region mapping (upper) and the new three-region algorithm (lower).\nThe scene depth, sd, along the abscissa is plotted against the geometric perceived depth, gpd, along the ordinate. The\nvertical dashed lines indicate the boundary of the region of interest.\n6. CONCLUSIONS\nThis paper has presented a new approach to capturing scene depth in stereoscopic images solving problems that\nexisting methods have representing depth in regions of interest. The new approach allows content creators to\ndistribute available perceived depth preferentially to regions of interest and keep this mapping constant even if\ntotal scene depth is changing. We presented a new three-region algorithm to achieve this and have demonstrated\nresults from analysis of the algorithm and a multi-pass rendering implementation using the POV-ray ray tracing\nsystem.\nThis approach will be important in animated content such as computer games where a game character moves\naround a scene and the total depth in view of the cameras is changing. It also has benefits for applications where\ndepth judgement is critical, such as scientific and medical imaging, where an identified region of interest can be\n\u201dlocked-down\u201d to maintain a defined perceived depth representation.\nFor future work the current approach can be implemented easily for real time graphics in the standard\ngraphics pipeline. We are also extending the algorithm from three to multiple-region mapping and in the limit\ncontinuously variable mapping of scene depth to display depth. Finally we are interested to investigate how to\ncombine the new algorithm with other methods for controlling perceived depth, for example blurring disparities\noutside the region of interest.\nACKNOWLEDGMENTS\nThanks are due to the Department of Computer Science at the University of Durham for purchasing the initial\n3D display hardware for this work.\nREFERENCES\n1. G. Jones, D. Lee, N. Holliman, and D. Ezra, \u201cControlling perceived depth in stereoscopic images,\u201d in\nStereoscopic Displays and Virtual Reality Systems VIII, Proceedings of SPIE 4297A, 2001.\n2. Y. Yeh and L. Silverstein, \u201cLimits of fusion and depth judgements in stereoscopic color displays,\u201d Human\nFactors 1(32), 1990.\n3. N. Holliman, \u201c3D Display Systems.\u201d to appear; Handbook of Opto-electronics, IOP Press, Spring 2004,\nISBN 0-7503-0646-7.\n4. C. Ware, Information visualization, perception for design, Morgan Kaufmann, 1999. ISBN 0-1-55860-511-8.\n5. L. Lipton, \u201cStereographics Developers Handbook.\u201d Stereographics Corporation, 1997.\nhttp:\/\/www.stereographics.com\/support\/downloads support\/handbook.pdf.\n6. D. Diner and D. Fender, Human engineering in stereoscopic viewing devices, Plenum Press, 1993. ISBN\n0-306-44667-7.\n7. D. McAllister, ed., Stereo computer graphics and other true 3D technologies, Princeton University Press,\n1993. ISBN 0-691-08741-5.\n8. R. Sand and A. Chiari, eds., Stereoscopic Television: Standards, Technology and Signal Processing, European\nCommision, Directorate General XIII-B, Brussels, 1998.\n9. C. Ware and G. Franck, \u201cEvaluating stereo and motion cues for visualizing information nets in three\ndimensions,\u201d Tech. Rep. TR 94-082, University of New Brunswick, 1984.\n10. R. Hubbold and D. Hancock, \u201cStereo display of nested 3d volume data using automatic tunnelling,\u201d in\nStereoscopic Displays and Virtual Reality Systems VI, Proceedings of SPIE 3639, 1999.\n11. H. Helmholtz, Treatise on physiological optics, Thoemmes Press, 1867. 1924 edition, reprinted 2000.\n12. A. Woods, T. Docherty, and R. Koch, \u201cImage distortions in stereoscopic video systems,\u201d Proceedings of\nSPIE 1915, 1993.\n13. L. Hodges and E. Davies, \u201cGeometric considerations for stereoscopic virtual environments,\u201d Presence 2(1),\n1993.\n14. Z. Wartell, Stereoscopic Head-Tracked Displays: Analysis and Development of Display Algorithms. PhD\nthesis, Georgia Institute of Technology, 2001.\n15. Z. Wartell, L. Hodges, and W. Ribarsky, \u201cAn analytic comparison of alpha-false eye separation, image\nscaling and image shifting in stereoscopic displays,\u201d IEEE Transactions on Visualization and Computer\nGraphics 8(2), pp. 129\u2013143, 2002.\n16. PC-RD3D Series Operation Manual, Sharp Corporation, Japan, 2003.\n17. G.R.Jones, D.Lee, and N.S.Holliman, \u201cMethod of producing a stereoscopic image.\u201d EP 1089573, April 2001.\n18. J. Harris and H. Stocker, Handbook of mathematics and computational science, Springer-Verlag, 1998. ISBN\n0-387-94746-9.\n"}