{"doi":"10.1109\/TCSVT.2010.2051282","coreId":"68319","oai":"oai:eprints.lancs.ac.uk:32893","identifiers":["oai:eprints.lancs.ac.uk:32893","10.1109\/TCSVT.2010.2051282"],"title":"Video foreground detection based on symmetric alpha-stable mixture models.","authors":["Bhaskar, H.","Mihaylova, Lyudmila","Achim, A."],"enrichments":{"references":[{"id":880031,"title":"A hierarchical approach to robust background subtraction using color and gradient information.","authors":[],"date":"2002","doi":null,"raw":"O. Javed, K. Sha\ufb01que, and M. Shah. A hierarchical approach to robust background subtraction using color and gradient information. In Proc. MOTION\u201902, pages 22\u201327, 2002.","cites":null},{"id":881720,"title":"A MRF-based real-time approach for subway monitoring.","authors":[],"date":"2001","doi":null,"raw":"N.Paragios and V.Ramesh. A MRF-based real-time approach for subway monitoring. In Proc. CVPR, pages 1034\u20131040, 2001.","cites":null},{"id":878715,"title":"Automatic target detection based on background modeling using adaptive cluster density estimation.","authors":[],"date":"2007","doi":null,"raw":"H. Bhaskar, L. Mihaylova, and S. Maskell. Automatic target detection based on background modeling using adaptive cluster density estimation. In LNCS, pages 130\u2013134, 2007.","cites":null},{"id":881475,"title":"Background modeling and subtraction using a local-linear-dependence-based Cauchy statistical model.","authors":[],"date":"2003","doi":null,"raw":"Y. Ming, J. Jiang, and J. Ming. Background modeling and subtraction using a local-linear-dependence-based Cauchy statistical model. In Proc. 7th International Conf. on Digital Image Computing: Techniques and Applications, pages 469\u2013478, 2003.","cites":null},{"id":883091,"title":"Background subtraction for freely moving cameras.","authors":[],"date":"2009","doi":null,"raw":"Y. Sheikh, O. Javed, and T. Kanade. Background subtraction for freely moving cameras. In Proc. of IEEE ICCV. 2009.","cites":null},{"id":878980,"title":"Detecting moving objects, ghosts and shadows in video streams.","authors":[],"date":"2005","doi":null,"raw":"R. Cucchiara, C. Grana, M. Piccardi, and A. Prati. Detecting moving objects, ghosts and shadows in video streams. IEEE Trans. PAMI, 25(10):1337\u20131342, 2005.","cites":null},{"id":884198,"title":"Ef\ufb01cient adaptive density estimation per image pixel for the task of background subtraction.","authors":[],"date":"2006","doi":"10.1016\/j.patrec.2005.11.005","raw":"Z. Zivkovic and F. van der Heijden. Ef\ufb01cient adaptive density estimation per image pixel for the task of background subtraction. Pattern Recognition Letters, 27(7):773\u2013780, 2006.","cites":null},{"id":883880,"title":"Improved adaptive Gausian mixture model for background subtraction.","authors":[],"date":"2004","doi":null,"raw":"Z. Zivkovic. Improved adaptive Gausian mixture model for background subtraction. In Proc. ICPR, volume 28 - 31, 2004.","cites":null},{"id":882668,"title":"Modelling with mixture of symmetric stable distributions using Gibbs sampling.","authors":[],"date":"2010","doi":null,"raw":"D. Salas-Gonzaleza, E. E. Kuruoglu, and D. P. Ruiz. Modelling with mixture of symmetric stable distributions using Gibbs sampling. Signal Processing, 90(3):774\u2013783, 2010.","cites":null},{"id":881929,"title":"Multi-resolution learning vector quantisation based automatic colour clustering.","authors":[],"date":"2008","doi":null,"raw":"A. Payne, H. Bhaskar, and L. Mihaylova. Multi-resolution learning vector quantisation based automatic colour clustering. In Proc. 11th Conf. on Inform. Fusion, pages 1934\u20131939, 2008.","cites":null},{"id":879500,"title":"Non-parametric model for background subtraction.","authors":[],"date":"2000","doi":null,"raw":"A. Elgammal, D. Harwood, and L. Davis. Non-parametric model for background subtraction. In Proc. 6th European Conf. on Computer Vision, pages II: 751\u2013767, June\/July 2000.","cites":null},{"id":881225,"title":"Parameter estimation and blind channel identi\ufb01cation in impulsive signal environment.","authors":[],"date":"1995","doi":null,"raw":"X. Ma and C. Nikias. Parameter estimation and blind channel identi\ufb01cation in impulsive signal environment. IEEE Tran. Sign. Proc., 43(12):2884\u20132897, Dec. 1995.","cites":null},{"id":878463,"title":"PETS: Performance evaluation of tracking and surveillance, available at http:\/\/www.cvg.rdg.ac.uk\/slides\/pets.html,","authors":[],"date":"2006","doi":null,"raw":"PETS: Performance evaluation of tracking and surveillance, available at http:\/\/www.cvg.rdg.ac.uk\/slides\/pets.html, 2006.","cites":null},{"id":882906,"title":"Signal processing with fractional lower order moments: Stable processes and their applications.","authors":[],"date":"1993","doi":null,"raw":"M. Shao and C. Nikias. Signal processing with fractional lower order moments: Stable processes and their applications. Proceedings of the IEEE, 81(7):986 \u2013 1010, July 1993.","cites":null},{"id":882425,"title":"Statistical background modeling for non-stationary camera.","authors":[],"date":"2003","doi":null,"raw":"Y. Ren, C.-S. Chua, and Y.-K. Ho. Statistical background modeling for non-stationary camera. Pattern Recogition Letters, 24(1-3):183\u2013196, 2003.","cites":null},{"id":880003,"title":"Statistical background subtraction for a mobile observer.","authors":[],"date":"2003","doi":null,"raw":"E. Hayman and J.-O. Eklundh. Statistical background subtraction for a mobile observer. In Proceedings of the 9th International Conf. on Computer Vision, pages 67\u201374, 2003.","cites":null},{"id":880586,"title":"Statistical modeling of complex backgrounds for foreground object detection.","authors":[],"date":"2004","doi":null,"raw":"L. Li, W. Huang, I. Yu-Hua Gu, and Q. Tian. Statistical modeling of complex backgrounds for foreground object detection. IEEE Trans. on Image Processing, 13(11):1459\u20131472, 2004.","cites":null},{"id":880290,"title":"The Eden project multi-sensor data set.","authors":[],"date":"2006","doi":null,"raw":"J.J. Lewis, S.G. Nikolov, A. Loza, E. Fernandez-Canga, N. Cvejic, L. Li, A. Cardinali, C.N. Canagarajah, D.R. Bull, T. Riley, D. Hickman, and M.I. Smith. The Eden project multi-sensor data set. Technical report, Univ. of Bristol, UK, 2006.","cites":null},{"id":879240,"title":"The relationship between precision-recall and ROC curves.","authors":[],"date":"2006","doi":null,"raw":"J. Davis and M. Goadrich. The relationship between precision-recall and ROC curves. In Proc. ICML, pages 233\u2013240, 2006.","cites":null},{"id":883366,"title":"Topology free hidden Markov models: application to background modeling.","authors":[],"date":"2001","doi":null,"raw":"B. Stenger, V. Ramesh, N. Paragios, F. Coetzec, and J.M. Buhmann. Topology free hidden Markov models: application to background modeling. In Proc. of the International Conf. on Computer Vision, pages 294\u2013301, 2001.","cites":null},{"id":882193,"title":"Tracking people by learning their appearance.","authors":[],"date":"2007","doi":null,"raw":"D. Ramanan, D. Forsyth, and A. Zisserman. Tracking people by learning their appearance. IEEE Tr. PAMI, 29(1):65\u201381, 2007.","cites":null},{"id":879737,"title":"Valmet: A new validation tool for assessing and improving 3D object segmentation.","authors":[],"date":"2001","doi":null,"raw":"G. Gerig, M. Jomier, and M. Chakos. Valmet: A new validation tool for assessing and improving 3D object segmentation. In Proceedings of MICCAI, LNCS 2208, pages 516\u2013523, 2001.","cites":null},{"id":883640,"title":"Wall\ufb02ower: Principles and practice of background maintenance.","authors":[],"date":"1999","doi":"10.1109\/ICCV.1999.791228","raw":"K. Toyama, J. Krumm, B. Brumitt, and B. Meyers. Wall\ufb02ower: Principles and practice of background maintenance. In Proc. Intl. Conf. Comp. Vision, pages 255\u2013261, 1999.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-08","abstract":"Background subtraction (BS) is an efficient technique for detecting moving objects in video sequences. A simple BS process involves building a model of the background and extracting regions of the foreground (moving objects) with the assumptions that the camera remains stationary and there exist no movements in the background. These assumptions restrict the applicability of BS methods to real-time object detection in video. In this paper, we propose an extended cluster BS technique with a mixture of symmetric alpha stable (SS) distributions. An on-line self-adaptive mechanism is presented that allows automated estimation of the model parameters using the log moment method. Results over real video sequences from indoor and outdoor environments, with data from static and moving video cameras are presented. The SS mixture model is shown to improve the detection performance compared with a cluster BS method using a Gaussian mixture model and the method of Li et al. [11]","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/68319.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/32893\/1\/SAS_BS_in_press.pdf","pdfHashValue":"630f7518e169db6134db47daf456756c7dc28168","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:32893<\/identifier><datestamp>\n      2018-01-24T03:00:19Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Video foreground detection based on symmetric alpha-stable mixture models.<\/dc:title><dc:creator>\n        Bhaskar, H.<\/dc:creator><dc:creator>\n        Mihaylova, Lyudmila<\/dc:creator><dc:creator>\n        Achim, A.<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        Background subtraction (BS) is an efficient technique for detecting moving objects in video sequences. A simple BS process involves building a model of the background and extracting regions of the foreground (moving objects) with the assumptions that the camera remains stationary and there exist no movements in the background. These assumptions restrict the applicability of BS methods to real-time object detection in video. In this paper, we propose an extended cluster BS technique with a mixture of symmetric alpha stable (SS) distributions. An on-line self-adaptive mechanism is presented that allows automated estimation of the model parameters using the log moment method. Results over real video sequences from indoor and outdoor environments, with data from static and moving video cameras are presented. The SS mixture model is shown to improve the detection performance compared with a cluster BS method using a Gaussian mixture model and the method of Li et al. [11].<\/dc:description><dc:date>\n        2010-08<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/32893\/1\/SAS_BS_in_press.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/TCSVT.2010.2051282<\/dc:relation><dc:identifier>\n        Bhaskar, H. and Mihaylova, Lyudmila and Achim, A. (2010) Video foreground detection based on symmetric alpha-stable mixture models. IEEE Transactions on Circuits and Systems for Video Technology, 20 (8). pp. 1133-1138. ISSN 1051-8215<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/32893\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1109\/TCSVT.2010.2051282","http:\/\/eprints.lancs.ac.uk\/32893\/"],"year":2010,"topics":["QA75 Electronic computers. Computer science"],"subject":["Journal Article","PeerReviewed"],"fullText":"IEEE TRANSACTIONS ON CIRCUITS, SYSTEMS AND VIDEO TECHNOLOGY, BRIEF PAPER, MARCH 2010 1\nVideo Foreground Detection Based on Symmetric\nAlpha-Stable Mixture Models\nHarish Bhaskar, Lyudmila Mihaylova, Senior Member, IEEE, and Alin Achim, Senior Member, IEEE\nAbstract\u2014Background subtraction (BS) is an efficient tech-\nnique for detecting moving objects in video sequences. A simple\nBS process involves building a model of the background and\nextracting regions of the foreground (moving objects) with the\nassumptions that the camera remains stationary and there exist\nno movements in the background. These assumptions restrict\nthe applicability of BS methods to real-time object detection in\nvideo. In this paper, we propose an extended cluster BS technique\nwith a mixture of symmetric alpha stable (S\u03b1S) distributions.\nAn on-line self-adaptive mechanism is presented that allows\nautomated estimation of the model parameters using the log\nmoment method. Results over real video sequences from indoor\nand outdoor environments, with data from static and moving\nvideo cameras are presented. The S\u03b1S mixture model is shown\nto improve the detection performance compared with a cluster\nBS method using a Gaussian mixture model and the method of\nLi et al. [11].\nIndex Terms\u2014automatic object detection, background subtrac-\ntion, segmentation, alpha stable distribution\nI. INTRODUCTION\nMoving object detection in video sequences represents a\ncritical component of many modern video processing systems.\nThe standard approach to object detection is Background\nSubtraction (BS), that attempts to build a representation of\nthe background and detect moving objects by comparing each\nnew frame with this representation [4]. A number of different\nBS techniques have been proposed in the literature and some\nof the popular methods include mixture of Gaussians [24],\nkernel density estimation [6], colour and gradient cues [9],\nhigh level region analysis [22], hidden Markov models [21],\nand Markov random fields [14]. Basic BS techniques detect\nforeground objects as the difference between two consecutive\nvideo frames, operate at pixel level and are applicable to\nstatic backgrounds [4]. Although the generic BS method is\nsimple to understand and implement, the disadvantages of the\nframe difference BS is that it does not provide a mechanism\nfor choosing the parameters, such as the detection threshold,\nand it is unable to cope with multi-modal distributions. One\nof the important techniques able to cope with multi-modal\nHarish Bhaskar is with Khalifa University of Science Technology and\nResearch, UAE, email: harish.bhaskar@kustar.ac.ae. Lyudmila Mihaylova is\nwith Lancaster University, UK, Email: mila.mihaylova@lancaster.ac.uk. Alin\nAchim is with the University of Bristol, UK, Email: alin.achim@bristol.ac.uk.\nWe acknowledge the support of the UK MOD Data and Infor-\nmation Fusion DTC (Tracking Cluster project DIFDTC\/CSIPC1\/02)\nand EU COST action TU0702. Corresponding author: L. Mihaylova,\nmila.mihaylova@lancaster.ac.uk. Manuscript received May 2008, revised\nNovember, 2009 and March 2010. Copyright (c) 2010 IEEE. Personal use\nof this material is permitted. However, permission to use this material for\nany other purposes must be obtained from the IEEE by sending an email to\npubs-permissions@ieee.org.\nbackground distributions and to update the detection threshold\nmakes use of Gaussian mixture models (GMMs). The model\nproposed in [24] describes each pixel as a mixture of Gaus-\nsians and an on-line update of this model. The larger Gaussian\ncomponents correspond to the background and this is used to\ngenerate the background model. An algorithm for background\nmodeling and BS based on Cauchy statistical distribution [13]\nis shown to be robust and adaptive to dynamic changes of the\nbackground scene and more cost effective than the GMM as\nit does not involve any exponential operation.\nIn [11] the foreground objects are detected in complex\nenvironments. The background appearance is characterised by\nprincipal features (spectral, spatial and temporal) and their\nstatistics, at each pixel. However, the learning method in [11]\nrequires \u2018training\u2019 since it relies on look up tables for the\nfeatures and adapts them to the changes of environment. The\nCBS-S\u03b1S technique that we propose does not need such\nlook up tables for the image features and is a cluster-based\ntechnique, which makes it different from [11]. According to\nour knowledge only one recent work [18] considers mixtures\nof S\u03b1S distributions for off-line data analysis and does not\nseem suitable for real-time object detection.\nIn this paper, we propose a novel cluster BS (CBS) tech-\nnique based on S\u03b1S distributions, which technique we call\nCBS-S\u03b1S. The main contributions of the paper are threefold.\nFirstly, the BS process is performed at cluster level as opposed\nto pixel level methods that are commonly used [6], [24],\n[4]. The CBS-S\u03b1S method reduces significantly the clutter\nnoise that arises due to slight variations in the pixel intensities\nwithin regions belonging to the same object. Secondly, due to\ntheir heavy tails, S\u03b1S distributions can help handling dynamic\nchanges in a scene, and hence they model moving backgrounds\nand moving camera in a better way than the GMM. Results of\nmodeling the background of a moving image sequence can\nbe best obtained while operating with estimated values of\nthe characteristic exponent parameter of the S\u03b1S distribution,\nrather than with fixed values corresponding to the Gaussian\nor Cauchy case. By estimating the parameters of the \u03b1 stable\ndistribution, the probability density function (PDF) of clusters\nof pixels can be faithfully represented and a reliable model\nof the background can be obtained. Thirdly, we show that a\nmixture of S\u03b1S distributions can represent well the multi-\nmodality and guarantees reliable object detection. A wide\nrange of tests is performed on indoor and outdoor environment,\non data from a static and moving cameras.\nThe remaining part of the paper is organised as follows.\nSection II presents the proposed CBS-S\u03b1S technique. A\ncomparison of the CBS-S\u03b1S with a CBS-GMM BS and\nIEEE TRANSACTIONS ON CIRCUITS, SYSTEMS AND VIDEO TECHNOLOGY, BRIEF PAPER, MARCH 2010 2\nthe background appearance method of Li et al. [11] on real\nvideo sequences is presented in Section III. Conclusions are\nsummarised in Section IV.\nII. THE PROPOSED CBS-S\u03b1S TECHNIQUE\nIn contrast with conventional BS techniques such as [6],\n[24], [4] that operate at pixel level, the developed BS technique\nmakes use of the advantages of the cluster image representa-\ntion. Here, an image frame at time instant k is subdivided\ninto constituent clusters cik, (0 \u2264 i \u2264 q), where q is the\ntotal number of clusters present in the image. Automatic\nclustering is performed with vector quantisation and using\ncolour features [15].\nThe problem of CBS involves a decision on whether a\ncluster cik belongs to the background (bG) or foreground (fG)\nobject from the ratio of PDFs:\np(bG|cik)\np(fG|cik)\n=\np(cik|bG)p(bG)\np(cik|fG)p(fG)\n, (1)\nwhere, the vector cik = (ci1,k, . . . , ci`,k) characterises the i-th\ncluster (0\u2264 i\u2264 q) at time instant k, containing ` number of\npixels such that [Im]k =\n[\nc1k, . . . , c\nq\nk\n]\nis the whole image;\np(bG|cik) is the PDF of the bG, evaluated using a certain\nfeature (e.g. colour or edges) of the cluster cik; p(fG|cik) is\nthe PDF of the fG of the same cluster cik; p(cik|bG) refers to\nthe PDF of the cluster feature given a model for the bG and\np(cik|fG) is the appearance model of the fG object. In our\ncluster BS technique the decision that a cluster belongs to a\nbG is made if:\np(cik|bG) > threshold\n(\n=\np(cik|fG)p(fG)\np(bG)\n)\n. (2)\nThe appearance of the fG, characterised by the PDF\np(cik|fG), is assumed uniform. The bG model represented as\np(cik|bG) is estimated from a training set <k =\n{\ncik, ..., c\ni\nk\u2212T\n}\nwhich is a rolling collection of images over a specific update\ntime T . The time T is crucial since its update determines the\nmodel ability to adapt to illumination changes and to handle\nappearances and disappearances of objects in a scene. If the\nframe rate is known, the time period T can be adapted, e.g.,\nas a ratio T = N\nfps\nbetween the number N of frames obtained\nthrough the online process and the frame rate, fps, frames per\nsecond. Since the threshold is a scalar, the decision in (2) is\nmade from the average of the distributions of all pixels within\nthe cluster cik.\nA. Alpha Stable Distributions\nThe appeal of S\u03b1S distributions as a statistical model for\nsignals derives from some important theoretical and empirical\nreasons [19]. Generally, there is no closed-form expression for\nthe PDF of S\u03b1S distributions. A convenient way of defining\nthem is by their characteristic function \u03d5(c) = exp(j\u03b4c \u2212\n\u03b3|c|\u03b1), where \u03b1 is the characteristic exponent parameter, with\nvalues 0 <\u03b1\u2264 2 that controls the heaviness of the tails of the\ndensity function, \u03b4 is the location parameter (\u2212\u221e< \u03b4 <\u221e)\nthat corresponds to the mean for 1 < \u03b1 \u2264 2, and to the median\nfor 0 < \u03b1 \u2264 1 and \u03b3 is the dispersion parameter (\u03b3 > 0),\nwhich determines the spread of the density around the location\nparameter. A S\u03b1S distribution, characterised by the above\nthree parameters is denoted by S(\u03b1, \u03b3, \u03b4). In fact, no closed-\nform expressions for the general symmetric \u03b1 stable PDF exist,\nexcept for the Gaussian and Cauchy members. Specifically, the\ncase \u03b1 = 2 corresponds to the Gaussian distribution and the\nPDF has the form\nf\u03b1=2(\u03b3, \u03b4; c) =\n1\u221a\n4pi\u03b3\nexp\n{\n\u2212 (c\u2212 \u03b4)\n2\n4\u03b3\n}\n. (3)\nThe case \u03b1 = 1 corresponds to the Cauchy distribution, for\nwhich the PDF is given by\nf\u03b1=1(\u03b3, \u03b4; c) =\n\u03b3\npi[\u03b32 + (c\u2212 \u03b4)2] . (4)\nB. S\u03b1S Mixture Models\nThe PDFs p\u02dc(cik|<k, bG + fG) of the fG and bG can be\ncalculated as a mixture\np\u02dc(cik|<k, bG+ fG) =\nM\u2211\nm=1\npim,k\u03d5(c\ni\nk; \u03b1\u02dck, \u03b3\u02dcm,k), (5)\nof an M component S\u03b1S PDFs \u03d5(cik; \u03b1\u02dck, \u03b3\u02dcm,k) (with pa-\nrameters \u03b1\u02dck, the characteristic exponent and \u03b3\u02dcm,k, dispersion\nparameter). The weighting coefficients pim,k are calculated as\nshown in the next subsection.\nC. Iterative Log-Moment Estimation\nThe most important parameters of a S\u03b1S distribution\nare the characteristic exponent \u03b1 and dispersion parame-\nter \u03b3. The location parameter \u03b4 is often assumed to be\nzero, i.e., the measurements are normalised with respect to\nthe origin. Several methods for estimating these parameters\nhave been introduced in [12]. In our CBS-S\u03b1S technique\nthe parameters of the S\u03b1S distribution are evaluated based\non the log-moment estimation method. The update of the\nparameter estimates \u03b1\u02dc(1,k), \u03b1\u02dc(2,k), . . . , \u03b1\u02dc(M,k) and \u03b3\u02dc(1,k),\n\u03b3\u02dc(2,k), . . . , \u03b3\u02dc(M,k) at time instant k is performed, respec-\ntively, from the estimates \u03b1\u02dc(1,k\u22121), \u03b1\u02dc(2,k\u22121), . . . , \u03b1\u02dc(M,k\u22121)\nand \u03b3\u02dc(1,k\u22121), \u03b3\u02dc(2,k\u22121), . . . , \u03b3\u02dc(M,k\u22121) at a previous time k\u22121.\nAccording to the log moment estimation technique, if <k\nis a real S\u03b1S random variable, then its p-th order moment\nsatisfies the relation E(|<k|p) = E(eplog|<k |) = E(epV ),\nwhere \u22121 < p < \u03b1 and V = log|<k| corresponds to a\nlog |S\u03b1S| process with \u00b5k and \u03c3k representing the mean and\nvariance of the <k samples. The estimates of the mean and\nvariances, \u00b5\u02dc1,k, ..., \u00b5\u02dcM,k and \u03c3\u02dc21,k, ..., \u03c3\u02dc2M,k, respectively of\nthe <k samples can be represented as in [23] with\nw\u02dcm,k+1 = w\u02dcm,k +\n1\nTk\n(om,k \u2212 w\u02dcm,k), (6)\n\u00b5\u02dcm,k+1 = \u00b5\u02dcm,k + om,k\n(\n1\nTkw\u02dcm,k\n)\n\u03b4m,k, (7)\n\u03c3\u02dc2m,k+1=\u03c3\u02dc\n2\nm,k+om,k\n(\n1\nTkw\u02dcm,k\n)\n(\u03b4\u2032m,k\u03b4m,k \u2212 \u03c32m,k), (8)\nwhere \u03b4m,k = cik \u2212 \u00b5\u02dcm,k, \u2032 denotes the transpose operation,\nom,k refers to the ownership of the new cluster and defines\nthe closeness of this cluster to a particular S\u03b1S component,\nIEEE TRANSACTIONS ON CIRCUITS, SYSTEMS AND VIDEO TECHNOLOGY, BRIEF PAPER, MARCH 2010 3\nand m = 1, . . . ,M . The dispersion parameter \u03b3m,k at any\ntime instant k can be updated iteratively using the following\nequations [12]\nlog\u03b3m,k\n\u03b1m,k\n= k\u22121\nk\n(\nCe(1\u2212\u03b1m,k\u22121)+log \u03b3m,k\u22121\n\u03b1m,k\u22121\n)\n+\n\u00b5m,k\nk\n+ Ce\n(\n1\u2212 1\n\u03b1m,k\n)\n(9)\nand similarly the characteristic exponent \u03b1k at time instant k\ncan be updated as follows [12]\npi2\n6\n(\n1\n\u03b1\n2\nm,k\n+ 12\n)\n= k\u22121\nk2\n(\nCe(1\u2212\u03b1m,k\u22121)+log \u03b3m,k\u22121\n\u03b1m,k\u22121\n\u2212\u00b5m,k\n)2\n+pi\n2(k\u22121)\n6k\n(\n1\n\u03b1\n2\nm,k\u22121\n+ 12\n)\n+ 1\nk\n\u03c3m,k, (10)\nwhere Ce = 0.57721566... is the Euler constant. The accuracy\nof this parameter estimation technique increases with the\nincrease in sample size. The ownership of any new cluster\nis set to 1 for \u201cclose\u201d components and the others are set to\nzero. A cluster is close to a component iff the Mahalanobis\ndistance between the component and the cluster centre is, e.g.,\nless than 3. If there are no \u201cclose\u201d components, a component\nis generated with w\u02dcm+1,k = 1\/Tk, with an initial mean \u00b5\u02dc0\nand variance \u03c3\u02dc20 . The model presents clustering of compo-\nnents and the background is approximated with the B largest\ncomponents, p\u02dc(cik|<k, bG) \u223c\n\u2211B\nm=1 w\u02dcm,k\u03d5(\u03b1m,k,\u03b3m,k),\nB = argminb(\n\u2211b\nm=1 w\u02dcm,k > (1\u2212 cf)), where b is a variable\ndefining the number of clusters considered and cf is the\nproportion of the data that belong to fG objects without\ninfluencing the background model. The proportion between\nthe pixels from the fG and the pixels from the bG is assumed\nconstant in most models [24]. This assumption is not valid\nwhen videos of objects are captured from a close proximity.\nIn such circumstances, the proportion of pixels belonging to\nthe objects of interest, i.e., the fG pixels, are much higher\nthan the bG pixels. The ratio defining the percentage of fG\nand bG pixels can be updated from the training set as follows:\ncf =\np\u02dc(cik|<k, fG)\np\u02dc(cik|<k, bG)\n. (11)\nIII. RESULTS AND ANALYSIS\nThe performance of the proposed CBS-S\u03b1S technique has\nbeen validated over a real video sequences CAVIAR [1],\nPETS [2], data taken with static and moving video cam-\neras, from outdoor and indoor scenarios [16]. The CBS-S\u03b1S\ntechnique is compared with the CBS-GMM [3] and with the\ntechnique for complex bG and fG object detection proposed in\n[11]. In our experiments (except where is it explicitly stated),\nwe have chosen the number of mixture components for the\nCBS-S\u03b1S and CBS-GMM to be 3.\nA. Static Camera Video Sequence\nWe first compare the performance of the proposed CBS-S\u03b1S\ntechnique with the CBS-GMM [3] performance using real\nvideo sequences taken with a stationary camera. In general,\nboth the CBS-GMM and the CBS-S\u03b1S models perform well.\nIt can be observed from the results on Figure 1 that the\n(Original Sequence)\n(a: Results from the CBS-GMM)\n(b: Results from the CBS-S\u03b1S)\nFig. 1. Results from: (a) CBS-GMM [3], (b) proposed CBS-S\u03b1S model on\nsequence 1 (static camera), DIF DTC testing data [10].\nproposed mixture of S\u03b1S distributions is able to avoid over-\nsegmentation, to cope well with the multi-modality and hence\nto represent in a better way the object of interest than the\nGMM. In the next subsection, results on video data captured\nwith a moving camera and movements in the bG are presented.\nB. Moving Camera Video Sequence\nDeep sea water video sequences are particularly challenging\ndue to the video camera movement. The object of interest is\n(Original Sequence)\n(a: Results from the CBS-GMM)\n(b: Results from the CBS-S\u03b1S)\nFig. 2. Results from the (a) CBS-GMM [3] and (b) proposed CBS-S\u03b1S\nmodel on sequence 2 (moving camera sequence)\na deep sea diver who is also in motion. There are sources of\nmulti-modality, moving bG and the camera is moving. From\nFig. 2 a) and b) is evident that the S\u03b1S CBS reduces signif-\nicantly the level of clutter. We performed additional tests on\nanother video sequence, corresponding to an outdoor scenario\n(taken from [16]). The results from Fig. 3 show that the\nIEEE TRANSACTIONS ON CIRCUITS, SYSTEMS AND VIDEO TECHNOLOGY, BRIEF PAPER, MARCH 2010 4\n(Original Sequence)\n(a: Results from the CBS-GMM)\n(b: Results from the CBS-S\u03b1S)\nFig. 3. Results from: (a) CBS-GMM [3], (b) proposed CBS-S\u03b1S model on\nsequence taken with moving camera (public data [16]).\nproposed CBS-S\u03b1S technique reduces the clutter noise and\nto cope with camera displacements and small movements in\nthe bG. The S\u03b1S densities with heavy tails can accommodate\nthe variations in the bG and possesses higher robustness to\nillumination changes than the CBS-GMM.\nC. Quantitative Analysis\nRecall and precision quantify how well an algorithm\nmatches the ground truth. Recall [5] is calculated as the ratio\nof the number of fG pixels correctly identified to the number\nof fG pixels in the ground truth and precision is computed as\na ratio of the number of fG pixels correctly identified to the\nnumber of fG pixels detected. In Figure 4 it can be observed\n0 0.2 0.4 0.6 0.8 1\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\nRecall\nP\nre\nci\nsi\non\nRecall\u2212Precision Cure for Sequence 1\n \n \nCBS\u2212SAS\nCBS\u2212GMM\n0 0.2 0.4 0.6 0.8 1\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\nRecall\nP\nre\nci\nsi\non\nRecall \u2212 Precision Curves on Baseball Sequence\n \n \nCBS\u2212GMM\nCBS\u2212SAS\nFig. 4. Recall-Precision curves of the CBS-GMM [3] and proposed CBS-\nS\u03b1S model, for sequences 2 and 3.\nthat both techniques show a steady decrease in precision with\nincrease in recall. However, the proposed CBS-S\u03b1S algorithm\n(blue line) displays higher levels of precision for the same\nvalues of the recall than the CBS-GMM (red dashed line). The\nhigher the rate of precision implies a greater rate of correct\nclassification of fG pixels [5].\nThe performance of the CBS-S\u03b1S technique was further\ntested over a large sequences from CAVIAR [1] and PETS [2]\ndatasets. In addition to the recall and precision measures,\nthe ratio S(A,B) = {A \u2229 B}\/{A \u222a B} [7] between the\nintersection of the ground truth and segmented regions over\nthe union of the ground truth and segmented regions is used\nto evaluate the performance of the CBS-S\u03b1S, CBS-GMM\nalgorithms and the algorithm of Li et al. [11]. The accuracy\nof the segmentation process increases with increasing the\nvalues of S. If S > 0.5 the performance of segmentation\nis generally considered good and nearly perfect for values of\nS > 0.8. The average values of precision, recall and S-ratio\nare shown in Table I calculated on CAVIAR data [1], from\nindoor environment and with the camera above the level of\nthe walking person. According to the these results the CBS-\nTable 1: Precision, recall and S-ratio with CAVIAR data [1]\nS\u03b1S technique achieves the highest values for the precision,\nrecall, and S-ratio followed by the algorithm of Li et al. [11]\nand then by the CBS-GMM algorithm. In order to explain\nthis performance of the CBS-S\u03b1S technique, we consider\nthe estimates of \u03b1 (the characteristic exponent) over different\nframes of a video sequence (plotted in blue) taken with a\nstatic camera and a video sequence (plotted in red) taken with\na moving camera from the PETS 2001 dataset. Figure 6 (a)\nshows that the value of \u03b1 for the static camera is around 1.5\nto 2. For moving camera the value ranges between 1 and 2\n(nearly 1 means that the distribution is nearly Cauchy).\nThe better performance of the CBS-S\u03b1S with respect to the\nCBS-GMM technique can be understood when we consider the\ngrey level intensity representing the bG subject to illumination\nchanges over a short period of time. For the PETS sequence\nthe histograms are given in Figure 7. The heavy tailed multi-\nmodal histogram cannot be represented well by the standard\nGMM as seen from Figure 7. Sample distributions of the\nGMM are compared with the S\u03b1S model of the intensity.\nClearly, the mixture of S\u03b1S distributions (dashed dot lines) is\nmodels variations in intensity in a better way than the standard\nGMM. Similar experiments with slight variations of the bG\ndue to camera movements have established the superiority of\nthe CBS-S\u03b1S model.\nIV. CONCLUSIONS AND FUTURE WORK\nA novel approach for automatic object detection based on\ncluster BS with S\u03b1S distribution was introduced. The heavy\ntail S\u03b1S distributions allow to cope with slight movements in\nthe background, camera shakes and clutter noise. An adaptive\nframework for parameter estimation is proposed that allows the\nmodel to adapt to environmental changes. A comparison of the\nmodel to its counterpart CBS-GMM model [3] is presented.\nExperimental results show that the CBS-S\u03b1S algorithm has\nefficient performance measured by precision, recall and S-\nratios and outperforms both the CBS algorithms with a GMM\nIEEE TRANSACTIONS ON CIRCUITS, SYSTEMS AND VIDEO TECHNOLOGY, BRIEF PAPER, MARCH 2010 5\n(a) PETS sequence with changeable lighting conditions: original video (left,\nframe 10200), CBC-S\u03b1S (middle), CBS-GMM (right)\n(b) PETS sequence with changeable lighting conditions: original video (left,\nframe 11044), CBC-S\u03b1S (middle), CBS-GMM (right)\n(c) PETS sequence from a moving camera (top left, frame 21398),\nCBC-S\u03b1S (middle), CBS-GMM (right)\nFig. 5. Results on PETS sequences [2]\n0 10 20 30 40 50 60 70\n1\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\n1.7\n1.8\n1.9\n2\nFrame Number\nV\nal\nue\n o\nf a\nlp\nha\nVariation of Alpha for a pixel from Static\/Dynamic Video Sequence\n \n \nAlpha in Static\nAlpha in Dynamic\nFig. 6. (a) Estimated \u03b1: in video from a static camera (\u03b1 in static) and from\na moving camera (\u03b1 in dynamic)\n0 50 100 150 200 250\n0\n0.005\n0.01\n0.015\n0.02\n0.025\n0.03\nGrey Level Intensity\nDe\nns\nity\nIntensity Histogram\nIntensity data\nCBS\u2212GMM\nCBS\u2212SAS\nFig. 7. Intensity histogram of a background pixel: CBS-S\u03b1S and CBS-GMM\nand the algorithm of Li et al. [11]. The model has relatively\nlow memory requirements and can process at the rate of 15-20\nfps on a Intel Duo Core processor machine. Our future work\nwill be focussed on BS for videos from moving cameras, with\nscenarios similar to the considered in [20], [17], [8].\nREFERENCES\n[1] CAVIAR, http:\/\/homepages.inf.ed.ac.uk\/rbf\/caviardata1\/, 2003.\n[2] PETS: Performance evaluation of tracking and surveillance, available at\nhttp:\/\/www.cvg.rdg.ac.uk\/slides\/pets.html, 2006.\n[3] H. Bhaskar, L. Mihaylova, and S. Maskell. Automatic target detection\nbased on background modeling using adaptive cluster density estimation.\nIn LNCS, pages 130\u2013134, 2007.\n[4] R. Cucchiara, C. Grana, M. Piccardi, and A. Prati. Detecting moving\nobjects, ghosts and shadows in video streams. IEEE Trans. PAMI,\n25(10):1337\u20131342, 2005.\n[5] J. Davis and M. Goadrich. The relationship between precision-recall\nand ROC curves. In Proc. ICML, pages 233\u2013240, 2006.\n[6] A. Elgammal, D. Harwood, and L. Davis. Non-parametric model for\nbackground subtraction. In Proc. 6th European Conf. on Computer\nVision, pages II: 751\u2013767, June\/July 2000.\n[7] G. Gerig, M. Jomier, and M. Chakos. Valmet: A new validation tool\nfor assessing and improving 3D object segmentation. In Proceedings of\nMICCAI, LNCS 2208, pages 516\u2013523, 2001.\n[8] E. Hayman and J.-O. Eklundh. Statistical background subtraction for\na mobile observer. In Proceedings of the 9th International Conf. on\nComputer Vision, pages 67\u201374, 2003.\n[9] O. Javed, K. Shafique, and M. Shah. A hierarchical approach to robust\nbackground subtraction using color and gradient information. In Proc.\nMOTION\u201902, pages 22\u201327, 2002.\n[10] J.J. Lewis, S.G. Nikolov, A. Loza, E. Fernandez-Canga, N. Cvejic, L. Li,\nA. Cardinali, C.N. Canagarajah, D.R. Bull, T. Riley, D. Hickman, and\nM.I. Smith. The Eden project multi-sensor data set. Technical report,\nUniv. of Bristol, UK, 2006.\n[11] L. Li, W. Huang, I. Yu-Hua Gu, and Q. Tian. Statistical modeling of\ncomplex backgrounds for foreground object detection. IEEE Trans. on\nImage Processing, 13(11):1459\u20131472, 2004.\n[12] X. Ma and C. Nikias. Parameter estimation and blind channel iden-\ntification in impulsive signal environment. IEEE Tran. Sign. Proc.,\n43(12):2884\u20132897, Dec. 1995.\n[13] Y. Ming, J. Jiang, and J. Ming. Background modeling and subtraction\nusing a local-linear-dependence-based Cauchy statistical model. In Proc.\n7th International Conf. on Digital Image Computing: Techniques and\nApplications, pages 469\u2013478, 2003.\n[14] N.Paragios and V.Ramesh. A MRF-based real-time approach for subway\nmonitoring. In Proc. CVPR, pages 1034\u20131040, 2001.\n[15] A. Payne, H. Bhaskar, and L. Mihaylova. Multi-resolution learning\nvector quantisation based automatic colour clustering. In Proc. 11th\nConf. on Inform. Fusion, pages 1934\u20131939, 2008.\n[16] D. Ramanan, D. Forsyth, and A. Zisserman. Tracking people by learning\ntheir appearance. IEEE Tr. PAMI, 29(1):65\u201381, 2007.\n[17] Y. Ren, C.-S. Chua, and Y.-K. Ho. Statistical background modeling\nfor non-stationary camera. Pattern Recogition Letters, 24(1-3):183\u2013196,\n2003.\n[18] D. Salas-Gonzaleza, E. E. Kuruoglu, and D. P. Ruiz. Modelling with\nmixture of symmetric stable distributions using Gibbs sampling. Signal\nProcessing, 90(3):774\u2013783, 2010.\n[19] M. Shao and C. Nikias. Signal processing with fractional lower order\nmoments: Stable processes and their applications. Proceedings of the\nIEEE, 81(7):986 \u2013 1010, July 1993.\n[20] Y. Sheikh, O. Javed, and T. Kanade. Background subtraction for freely\nmoving cameras. In Proc. of IEEE ICCV. 2009.\n[21] B. Stenger, V. Ramesh, N. Paragios, F. Coetzec, and J.M. Buhmann.\nTopology free hidden Markov models: application to background mod-\neling. In Proc. of the International Conf. on Computer Vision, pages\n294\u2013301, 2001.\n[22] K. Toyama, J. Krumm, B. Brumitt, and B. Meyers. Wallflower:\nPrinciples and practice of background maintenance. In Proc. Intl. Conf.\nComp. Vision, pages 255\u2013261, 1999.\n[23] Z. Zivkovic. Improved adaptive Gausian mixture model for background\nsubtraction. In Proc. ICPR, volume 28 - 31, 2004.\n[24] Z. Zivkovic and F. van der Heijden. Efficient adaptive density estimation\nper image pixel for the task of background subtraction. Pattern\nRecognition Letters, 27(7):773\u2013780, 2006.\n"}