{"doi":"10.1016\/j.cviu.2010.03.017","coreId":"54959","oai":"oai:eprints.lincoln.ac.uk:2315","identifiers":["oai:eprints.lincoln.ac.uk:2315","10.1016\/j.cviu.2010.03.017"],"title":"A modified model for the Lobula Giant Movement Detector and its FPGA implementation","authors":["Meng, Hongying","Appiah, Kofi","Yue, Shigang","Hunter, Andrew","Hobden, Mervyn","Priestley, Nigel","Hobden, Peter","Pettit, Cy"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-11","abstract":"The Lobula Giant Movement Detector (LGMD) is a wide-field visual neuron located in the Lobula layer of the Locust nervous system. The LGMD increases its firing rate in response to both the velocity of an approaching object and the proximity of this object. It has been found that it can respond to looming stimuli very quickly and trigger avoidance reactions. It has been successfully applied in\\ud\nvisual collision avoidance systems for vehicles and robots. This paper introduces a modified neural model for LGMD that provides additional depth direction information for the movement. The proposed model retains the simplicity of the previous model by adding only a few new cells. It has been\\ud\nsimplified and implemented on a Field Programmable Gate Array (FPGA), taking advantage of the inherent parallelism exhibited by the LGMD, and tested on real-time video streams. Experimental results demonstrate the effectiveness as a fast motion detector","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/54959.pdf","fullTextIdentifier":"http:\/\/eprints.lincoln.ac.uk\/2315\/1\/Meng2010CVIUAModifiedModelForTheLobulaGiantMovementDetectorAndItsFPGAImplementation.pdf","pdfHashValue":"3cfa3bb5f5f6ccf97252d4e09369c57e495a786a","publisher":"Elsevier","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lincoln.ac.uk:2315<\/identifier><datestamp>\n      2013-12-04T16:02:41Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47343030<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47373330<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47373430<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lincoln.ac.uk\/2315\/<\/dc:relation><dc:title>\n        A modified model for the Lobula Giant Movement Detector and its FPGA implementation<\/dc:title><dc:creator>\n        Meng, Hongying<\/dc:creator><dc:creator>\n        Appiah, Kofi<\/dc:creator><dc:creator>\n        Yue, Shigang<\/dc:creator><dc:creator>\n        Hunter, Andrew<\/dc:creator><dc:creator>\n        Hobden, Mervyn<\/dc:creator><dc:creator>\n        Priestley, Nigel<\/dc:creator><dc:creator>\n        Hobden, Peter<\/dc:creator><dc:creator>\n        Pettit, Cy<\/dc:creator><dc:subject>\n        G400 Computer Science<\/dc:subject><dc:subject>\n        G730 Neural Computing<\/dc:subject><dc:subject>\n        G740 Computer Vision<\/dc:subject><dc:description>\n        The Lobula Giant Movement Detector (LGMD) is a wide-field visual neuron located in the Lobula layer of the Locust nervous system. The LGMD increases its firing rate in response to both the velocity of an approaching object and the proximity of this object. It has been found that it can respond to looming stimuli very quickly and trigger avoidance reactions. It has been successfully applied in\\ud\nvisual collision avoidance systems for vehicles and robots. This paper introduces a modified neural model for LGMD that provides additional depth direction information for the movement. The proposed model retains the simplicity of the previous model by adding only a few new cells. It has been\\ud\nsimplified and implemented on a Field Programmable Gate Array (FPGA), taking advantage of the inherent parallelism exhibited by the LGMD, and tested on real-time video streams. Experimental results demonstrate the effectiveness as a fast motion detector.<\/dc:description><dc:publisher>\n        Elsevier<\/dc:publisher><dc:date>\n        2010-11<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lincoln.ac.uk\/2315\/1\/Meng2010CVIUAModifiedModelForTheLobulaGiantMovementDetectorAndItsFPGAImplementation.pdf<\/dc:identifier><dc:identifier>\n          Meng, Hongying and Appiah, Kofi and Yue, Shigang and Hunter, Andrew and Hobden, Mervyn and Priestley, Nigel and Hobden, Peter and Pettit, Cy  (2010) A modified model for the Lobula Giant Movement Detector and its FPGA implementation.  Computer vision and image understanding, 114  (11).   pp. 1238-1247.  ISSN 1077-3142  <\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1016\/j.cviu.2010.03.017<\/dc:relation><dc:relation>\n        10.1016\/j.cviu.2010.03.017<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lincoln.ac.uk\/2315\/","http:\/\/dx.doi.org\/10.1016\/j.cviu.2010.03.017","10.1016\/j.cviu.2010.03.017"],"year":2010,"topics":["G400 Computer Science","G730 Neural Computing","G740 Computer Vision"],"subject":["Article","PeerReviewed"],"fullText":"A Modified Model for the Lobula Giant\nMovement Detector and Its FPGA\nImplementation\nHongying Meng a Kofi Appiah a Shigang Yue a Andrew Hunter a\nMervyn Hobden b Nigel Priestley b Peter Hobden b Cy Pettit b\naSchool of Computer Science, University of Lincoln, UK\nbE2V Technologies PLC, Lincoln, UK.\nAbstract\nBio-inspired vision sensors are particularly appropriate candidates for navigation of\nvehicles or mobile robots due to their computational simplicity, allowing compact\nhardware implementations with low power dissipation. The Lobula Giant Movement\nDetector (LGMD) is a wide-field visual neuron located in the Lobula layer of the\nLocust nervous system. The LGMD increases its firing rate in response to both\nthe velocity of an approaching object and the proximity of this object. It has been\nfound that it can respond to looming stimuli very quickly and trigger avoidance\nreactions. It has been successfully applied in visual collision avoidance systems for\nvehicles and robots. This paper introduces a modified neural model for LGMD that\nprovides additional depth direction information for the movement. The proposed\nmodel retains the simplicity of the previous model by adding only a few new cells. It\nhas been simplified and implemented on a Field Programmable Gate Array (FPGA),\ntaking advantage of the inherent parallelism exhibited by the LGMD, and tested\non real-time video streams. Experimental results demonstrate the effectiveness as a\nfast motion detector.\nKey words: Neural networks, Bio-inspired vision chip, Embedded vision, Visual\nmotion, FPGA\n1 Introduction1\nFor animals, such as insects, the ability to detect approaching objects is impor-2\ntant, serving both to prevent collision as the animal moves and also to avoid3\ncapture by predators [1,2]. Evolved over millions of years, the visual collision4\navoidance systems in insects are both efficient and reliable. The neural cir-5\ncuits processing visual information in insects are relatively simple compared6\nPreprint submitted to Elsevier 7 January 2010\nto those in the human brain and provide an appropriate model for the op-7\ntical collision avoidance sensors that are needed to equip mobile intelligent8\nmachines [3].9\nThe Lobula Giant Movement Detector (LGMD) is a wide-field visual neu-10\nron located in the Lobula layer of the Locust nervous system. The LGMD11\nincreases its firing rate in response to both the velocity of the approaching12\nobject and its proximity. It responds to looming stimuli very quickly and can13\ntrigger avoidance reactions when a rapidly approaching object is detected.14\nIt is tightly tuned to respond to objects approaching on a direct collision15\ncourse [4], but produces little or no response to receding objects [5]. This16\nmakes the LGMD an ideal model to develop specialized sensors for automatic17\ncollision avoidance [6,7].18\nA functional neural network based on the LGMD\u2019s input circuitry was de-19\nveloped by Rind and Bramwell [8]. This neural network showed the same20\nselectivity as the LGMD neuron for approaching rather than receding objects21\nand responded best to objects approaching on collision rather than near-miss22\ntrajectories. The expanding edges of colliding objects and the use of lateral23\ninhibition were the key features of the model. This neural network has also24\nbeen used to mediate collision avoidance in a real-world environment by in-25\ncorporating it into the control structure of a miniature mobile robot [9,10].26\nInspired by the presence of direction selective neurons in the locust [11,12],27\na new specialized translation-sensitive neural network (TSNN) has been pro-28\nposed in [13,14]. The TSNN neuron has some common layers with the LGMD29\nmodel, allowing efficiency savings in the neural computation. The TSNN fuses30\nextracted visual motion cues from several whole-field direction selective neural31\nnetworks, and is only sensitive to translational movements.32\nTSNN can detect the direction of translation movements very well, but it33\nis not sensitive to movement in depth; LGMD [8,15] detects the direction34\nof movement in depth by both lateral inhibition and feed forward inhibition,35\nwhere feed forward inhibition plays a critical role in inhibiting LGMD spikes to36\nreceding objects. This use of feed forward inhibition can make the system over-37\nsensitive to background movements, thus decreasing the overall sensitivity of38\nLGMD. In this paper we propose a modified model for LGMD with several39\nextra cells to capture the directional information for depth movements quickly,40\nwhile the feed forward inhibition cell is only responsible for whole field image41\nmovements. The new model is efficiently implemented on FPGA. We have42\npreviously presented preliminary details of the new model [16], but without43\nthe full discussion or the FPGA implementation presented here.44\nThe rest of this paper is organized as follows: In section 2, we give an overview45\nof related work. In section 3, we address the modified LGMD model and its46\n2\nsoftware simulation. In section 4, we discuss the FPGA design and present ex-47\nperimental results from the hardware implementation; in section 5 we present48\nconclusions.49\n2 Related work50\nMotion sensors are presently employed in a wide variety of applications includ-51\ning surveillance, aerospace and automotive safety control systems and navi-52\ngational systems. Motion sensors are primarily based on ultrasound, passive53\ninfrared (PIR) and radar detectors. Ultrasonic motion sensors are commonly54\nused for automatic door openers and security alarms. PIR sensors are perhaps55\nthe most frequently used home security sensor. Radar sensors use microwave56\nsignals and detect intrusion by comparing a transmitted signal with a received57\necho signal and detect a Doppler shifted echo.58\nRecent years, vision sensors [17] are becoming increasingly cheap and reliable,59\nand may potentially be used for a number of tasks, including collision avoid-60\nance, navigation and object recognition. This makes it desirable to develop61\nefficient collision avoidance algorithms using visual sensors. However, collision62\navoidance is computationally demanding, and requires a very quick response63\nfrom the sensor [18\u201320].64\nMotion patterns in 2D video imagery contain distance information about ob-65\njects in a 3D environment [21]. An object on a collision course with the sensor66\nsystem displays movement in depth. There is a substantial body of literature67\non detection of depth from vision, primarily using stereo vision [22\u201324], al-68\nthough there is also some interesting work using monocular vision [25\u201327]. A69\nlooming object (one moving towards the sensor) appears to expand, which sug-70\ngests using optic flow algorithms and looking for a divergent flow pattern. A71\nnumber of authors have suggested using optic flow to compute obstacle time-72\nto-collision from a moving robot [28\u201330,26,31]. However, optic flow algorithms73\nare computationally expensive, and the difficulty in estimating accurate op-74\ntic flow from real world data [32] make these insufficiently robust for general75\napplications. Alternatively some collision avoidance systems are based on the76\nfusion of vision and radar sensors [33], exploiting the advantages of each.77\nBio-inspired vision algorithms are a particularly good candidate for collision78\navoidance systems as they use simple, easily parallelized algorithms. Galbraith79\net al [34] proposed a population coded algorithm, built on established models80\nof motion processing in the primate visual system, to estimate the time-to-81\ncollision with improved performance over the optic flow based method. How-82\never, it remains computationally expensive.83\n3\nThere have been a number of attempts to design a bio-inspired neural chip84\nbased on the LGMD neural network for motion detection. This bio-inspired85\nneural model features a particularly simple and highly parallelizable architec-86\nture, which may consequently be efficiently implemented on hardware, leading87\nto low cost and low power dissipation. It provides a much quicker response88\nthat the normal monocular or stereo visual sensors.89\nLaviana et al [35] proposed a vision chip architecture based on the LGMD90\nmodel described in [36] \u2013 a simplification of the model proposed in [8,1].91\nThe system includes an FPGA, a block of 100 \u00d7 150 6-bit retinotopic units,92\na controller, a 16Kbits SRAM memory block, I\/O registers and some other93\nperipherals needed for addressing, timing control, digital-to analog converters94\nand temperature monitoring. The FPGA chip uses 0.35\u00b5m 2P-2M technology.95\nOkuno and Yagi [37,38] implemented an LGMD model based on [8], for a96\nreal-time collision avoidance vision sensor. The system consists of an analog97\nVLSI silicon retina and a digital FPGA circuit. The system responds selec-98\ntively to colliding objects even in complicated real-world situations. These two99\nimplementations both use FPGA, but have some important limitations: first,100\nthey are based on the original LGMD model, which lacks movement direction101\ninformation; second, both have built-in restrictions due to their tight integra-102\ntion with the non-FPGA parts of the system (e.g. the retinotopic units), and103\ntherefore are not general purpose FPGA implementations.104\nIn this paper, in order to reduce the false alarm caused by receding objects in105\nthe LGMD model, we modify the model to distinguish approaching movement106\nfrom receding movement. The modified model retains simplicity in the soft-107\nware and hardware implementation. Its resource usage is low enough to admit108\nintegration with other functions on the FPGA, and it can be transferred to109\nany FPGA development platform. This design can achieve a very high frame110\nrate and can be applied in real-time vehicular collision avoidance systems with111\na low false alarm rate.112\n3 Modified LGMD neural network model113\nThe LGMD based neural network proposed in this paper is based on previous114\nstudies described in [8,10,39,40]. The modified neural network is shown in115\nfigure 1. The LGMD neural network in [8\u201310] was composed of four groups116\nof cells - photoreceptor cells (P ); excitatory and inhibitory cells (E and I);117\nsumming cells (S); and two single cells for feed-forward inhibition (FFI) and118\nLGMD. The model in [40,15] has an extra set of grouping cells between the119\nsumming cells and LGMD. This allows clusters of excitation in the summary120\ncells to feed into the LGMD cell, which is useful for collision detection in121\ncomplex backgrounds.122\n4\n3.1 Neural network model123\nThe input to the P cells is the luminance change. Lateral inhibition is indicated124\nwith dotted lines and has a one frame delay. Excitation is indicated with black125\nlines and has no delay. The FFI also has a one frame delay. The input to126\nFFI is the luminance change from the photoreceptor cells. The problem of127\nparameter selection in this LGMD model has been tackled in [41].128\n \nFig. 1. A schematic illustration of the modified LGMD neural network model. There\nare four groups of cells and five single cells: photoreceptor cells (P); excitatory\nand inhibitory cells (E and I); summing cells (S); grouping cells (J and H); depth\nmovement direction cell (D); the LGMD cell and the feed forward inhibition cell\n(FFI).\nThe model in [40] works very well for collision detection in complex envi-129\nronments. However, it cannot distinguish the direction of moving objects in130\ndepth. For example, it will respond to both an approaching object and a re-131\nceding object with high excitation level, especially when an object is very132\nclose. To enhance the ability to recognize the direction of the moving object133\nin depth, we add a new neural layer with two grouping cells J and H, and a134\n5\nnew cell D to give in-depth direction information; see figure 1. Note that the135\nJ , H and D cells may not have exact counterparts in the locust visual system.136\nThe model is described in detail below.137\n3.1.1 P layer138\nThe first layer contains the photoreceptor P cells arranged in a retinotopic139\nmatrix; the input frame pixel luminance Lf is captured by each photoreceptor140\ncell. The cells calculate the luminance change, which forms the output of this141\nlayer, using the equation:142\nPf (x, y) =\nnp\u2211\ni\npiPf\u2212i(x, y) + (Lf (x, y)\u2212 Lf\u22121(x, y)) (1)143\nwhere Pf (x, y) is the change of luminance corresponding to pixel (x, y) at144\nframe f , x and y are the index into the matrix, Lf and Lf\u22121 are the luminance,145\nsubscript f denotes the current frame and f \u2212 1 denotes the previous frame,146\nnp defines the maximum number of frames (or time steps) the persistence of147\nthe luminance change can last, the persistence coefficient pi \u2208 (0, 1) and148\npi = (1 + e\n\u00b5i)\u22121 (2)149\nwhere \u00b5 \u2208 (\u2212\u221e,+\u221e) and i indicates the previous ith frame counted from150\nthe current frame f . The LGMD neural network detects potential collision by151\nresponding to expansion of the image edges, a strategy that does not rely on152\nobject appearance. If there is no difference between successive images, the P153\ncells are not excited.154\n3.1.2 I E layer155\nThe output of the P cells forms the inputs to two separate cell types in the156\nnext layer. The excitatory cells pass excitation directly to their retinotopic157\ncounterparts in the third layer, the S layer. The excitation E(x, y) in an E158\ncell has the same value as that in the corresponding P cell. The lateral in-159\nhibition cells pass inhibition, after 1 image frame delay, to their retinotopic160\ncounterpart\u2019s neighboring cells in the S layer. The inhibition strength of a cell161\nin this layer is given by:162\nIf (x, y) =\n\u2211\ni\n\u2211\nj Pf\u22121(x+ i, y + j)wI(i, j), (if i = j, j 6= 0) (3)163\nwhere If (x, y) is the inhibition corresponding to pixel (x, y) at current frame164\nf , wI(i, j) is the local inhibition weight. Note that i and j are not allowed165\n6\nto be equal to zero simultaneously. Consequently, inhibition spreads out to166\nneighboring cells in next layer rather than to the direct counterpart.167\nIn our experiments, on both software simulation and hardware implementa-168\ntion, the local inhibition weight wI(i, j) are set to 0.25 for the four nearest169\nneighbors and 0.125 for the four diagonal neighbors. These values are espe-170\ncially convenient for hardware implementation.171\nwI =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n0.125 0.25 0.125\n0.25 0.25\n0.125 0.25 0.125\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n(4)172\n3.1.3 S layer173\nThe excitatory flow from the E cells and inhibition from the I cells is summed174\nby the S cells using the following equation:175\nSf (x, y) = Ef (x, y)\u2212 If (x, y)WI (5)176\nwhere WI is the inhibition weight (usually less than 0.8; 0.35 was empirically177\nchosen in our experiments). Excitations that exceed a threshold value are able178\nto reach the summation cell LGMD:179\nS\u02dcf (x, y) =\n\uf8f1\uf8f4\uf8f2\n\uf8f4\uf8f3\nSf (x, y), if Sf (x, y) \u2265 Tr\n0, if Sf (x, y) < Tr\n(6)180\nwhere Tr is the threshold.181\n3.1.4 J H cells182\nThe J andH cells are the two new grouping cells for depth movement direction183\nrecognition. The J cell is exactly the same as the LGMD cell in the previous184\nLGMD model in terms of spatiotemporal structure and the value it holds: it185\nsums the S cell activations to give an overall network response. The H cell186\nshares the same structure as J cell, but with a temporal difference, having a187\none frame delay from J .188\nJf =\n\u2211\nx,y\nS\u02dcf (x, y) (7)189\nHf = Jf\u22121 (8)190\n7\nFrom equations 1,3,5 and 7 it can been seen that the value of the J cell is191\nparticularly sensitive to pixels where there is a luminance changes between192\nconsecutive frames.193\n3.1.5 D cell194\nThe D cell is used to calculate the difference between the differences of frame195\nf , f \u2212 1 and f \u2212 2. It can be represented in the equation 9.196\nDf = abs(Jf )\u2212 abs(Hf ) (9)197\n(a )\n(b )\n)\nL i -1 L i L i+1 A i A i+ 1\nFig. 2. An illustration of the difference between approaching (a) and receding (b)\ndepth movement. Li\u22121, Li and Li+1 are three consecutive three frames in the video\nclip. Ai and Ai+1 are the affected areas while doing the frame subtractions between\nthese frames. In the approaching case, the affected area gets larger; in the receding\ncase smaller.\nThe D cell estimates the direction of movement in depth very well. It exploits198\nthe property that a looming object gets larger, whereas a receding one gets199\nsmaller; see figure 2. Due to the aperture effect, a moving object may only200\ncause detectable changes around the edge (or internal contrast boundaries);201\nhowever, at constant speed the size of the area of change is still related to the202\ndirection of movement in depth. When an object is moving away, abs(Jf ) is203\nsmaller than abs(Hf ). When an object is approaching, abs(Jf ) is bigger than204\nabs(Hf ). The absolute value function on J and H cells is used to cancel the205\ndifferent effects on their values when the object is darker or brighter than the206\nbackground. In order to distinguish slow movements we add a threshold TD207\nfor Df . We then get a simple variable D\u02dc that has only three values: \u20180\u2019, \u20181\u2019208\nand \u2018\u22121\u2019, where \u20181\u2019 stands for approaching, \u2018\u22121\u2019 for receding and \u20180\u2019 for no209\nsignificant movement. The threshold TD depends mainly on the size of the210\nimage.211\nD\u02dcf =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n1, if Df \u2265 TD\n0, if \u2212TD < Df < TD\n\u22121, if Df \u2264 \u2212TD\n(10)212\n8\nWhen augmented with the above cells, the LGMDmodel recognizes directional213\ninformation for depth movements quickly. The feed forward inhibition cell, as214\ndetailed later, is able to concentrate on whole image movements to avoid215\nperturbation from background movements.216\n3.1.6 LGMD cell217\nThe membrane potential J is then transformed to a spiking output using a218\nsigmoid transformation,219\nLGMDf = (1 + e\n\u2212Jfn\n\u22121\ncell)\u22121 (11)220\nwhere ncell is the total number of the cells in S layer. Since Jf is greater than221\nor equal to zero (as equation 7 is a sum of absolute value), the sigmoid mem-222\nbrane potential LGMDf varies from 0.5 to 1. The collision alarm is decided223\nby the spiking of cell LGMD. If the membrane potential LGMDf exceeds the224\nthreshold Ts, a spike is produced. A certain number of successive spikes, de-225\nnoted by SLGMD, will trigger the collision alarm in the LGMD cell. Of course,226\nin the modified model, the collision alarm is only triggered under the condi-227\ntion that D\u02dc = 1 where the moving object is approaching. The spikes may be228\nsuppressed by the FFI cell when whole field movement occurs [39].229\n3.1.7 FFI cell230\nIf it is not suppressed during turning, the network may produce spikes and231\neven false collision alerts due to sudden changes in the scene. The feed forward232\ninhibition and lateral inhibition work together to cope with such whole field233\nmovement [39]. The FFI excitation at the current frame is gathered from the234\nphotoreceptor cells with one frame delay,235\nFf =\nna\u2211\nj\n\u03b1Ff\u2212jFf\u2212j +\nnr\u2211\nx=1\nnc\u2211\ny=1\nabs(Pf\u22121(x, y))n\n\u22121\ncell (12)236\nwhere \u03b1Ff\u2212j is the persistence coefficient for FFI and \u03b1\nF\nf\u2212j \u2208 (0, 1), na defines237\nhow many time steps the persistence can last.238\nOnce Ff exceeds its threshold TFFI , spikes in the LGMD are inhibited imme-239\ndiately. The threshold TFFI is also adaptable,240\nTFFI = TFO + \u03b1ffiTFFIf\u22121 (13)241\n9\nwhere TFO is the initial value of the TFFI , the adaptable threshold is decided242\nby the previous TFFI and \u03b1ffi is a coefficient. The parameters, including TFO,243\n\u03b1ffi, are tuned to the application, the value depending on the image size and244\nthe style of camera movement. In the case when the camera is nearly stable,245\nthe FFI cell is normally ignored as it rarely reacts.246\n3.2 Simulation results on the proposed model247\nTwo data sets were used to test the efficiency and stability of the proposed248\nLGMD model in software simulation. The first experiment is on a simu-249\nlated data set that demonstrates carefully-calibrated approaching and receding250\nmovements. The second data sets are two recorded video clips. The parame-251\nters were kept the same in all experiments; values are shown in table X. [YOU252\nBEST REPLACE THIS X!]. The simulation was performed using MATLAB.253\nBecause the camera was still in the following experiments, FFI cell was ig-254\nnored. Other parameters used in the all following experiments are listed in the255\ntable 1256\nTable 1\nSettings for the control parameters of the LGMD model where nr and nc are the\nnumbers of the pixels in the horizontal and vertical directions in the video frame.\nnp \u00b5 p1 WI Tr TD ncell\n1 1.95 0.125 0.25 3 0.25*nr \u2217 nc nr \u2217 nc\n3.2.1 Results on simulated data set257\n                  5                                 20                                 35                                 50                                 65              \n                 80                                 95                                 110                                 120                            125            \nFig. 3. Selected frames from the simulated sequence. The square object looms and\nrecedes twice, with the second sequence at twice the speed of the first.\nWe created a sequence containing 125 frames, resolution 150\u00d7100, of a square258\nblack object on a white background. The object alternatively approaches and259\nrecedes. Sample frames are shown in figure 3. Initially the square is stationary260\nwith size 3\u00d7 3. It looms from frame 5\u2212 41, then recedes from frame 41\u2212 79,261\n10\nFig. 4. Output of the new LGMD model on the simulated sequence shown in fig-\nure 3. The vertical axis shows the normalized membrane potentials of the LGMD\ncell; the markers denote the depth movement direction of the object: \u2018\u25b3\u2019 denotes\napproaching, \u2018\u25bd\u2019 receding and \u2019\u00a9\u2019 no significant movement.\nboth at one pixel per side per frame. It is stationary from frames 79\u2212 84, at262\nsize 3 \u00d7 3, then approaches from 84 \u2212 101 and recedes from 101 \u2212 120, this263\ntime at 2 pixels per edge per frame. It remains stationary again at size 3\u00d7 3264\nfor the remainder of the sequence.265\nFigure 4 shows the output of the LGMD model on the simulated sequence266\nshown in figure 3. The vertical axis is the normalized membrane potential of267\nthe LGMD cell; the marker represents the output of the depth direction cell.268\nThis result shows that this model works very well in the simulation dataset.269\n3.2.2 Results on real recorded data270\nWe recorded two short video clips (shown in figures 5 and 7 respectively) for271\nthe second experiment, using 320 \u00d7 240 gray scale images. In these videos272\n(5) a ball is shown, mainly receding to the chair and then bouncing back273\nto approach the camera. There are 18 and 21 frames in the first and second274\nsequences respectively. The first recording has a bigger, fast-moving ball while275\nthe second has a smaller, slower-moving ball.276\nFigure 6 and 8 show the output of the new model on the recorded sequences277\nshown in figure 5 and 7 respectively. In the first dataset, the ball is a bit278\nbrighter than background while in the second dataset,the ball is a bit darker279\nthan the background. Although the situations are different, the simulation280\n11\n      \n             !                            \"                            #                           $                            %                           &  \n      \n             '                            (                            )                           !*                          !!                          !\"  \n      \n             !#                          !$                          !%                         !&                          !'                         !(  \nFig. 5. The first recorded sequence. There are 18 frames featuring a ball receding\nfrom the camera and then bouncing back to the camera after it hits a chair.\n \n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nFig. 6. The output of the model on the sequence shown in figure 5. The vertical axis\nis the normalized membrane potentials of the LGMD cell. The markers denote the\ndepth movement direction; \u2018\u25b3\u2019 denote approaching objects; \u2018\u25bd\u2019 receding objects\nand \u2019\u00a9\u2019 no significant movement.\nresults are quite similar. We can clearly see that the new model works very281\nwell on both recorded data sets.282\n12\n       \n             !                            \"                            #                           $                            %                           &                            ' \n       \n             (                            )                           !*                          !!                          !\"                          !#                        !$ \n       \n            !%                          !&                          !'                          !(                         !)                         \"*                         \"! \nFig. 7. The second recorded sequence. There are 21 frames, featuring a ball receding\nfrom the camera and then bouncing back towards the camera after it hits the chair.\n \n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nFig. 8. The output of the model on the sequence shown in figure 7. The vertical axis\nis the normalized membrane potentials of the LGMD cell. The markers denote the\ndepth movement direction; \u2018\u25b3\u2019 denote approaching objects; \u2018\u25bd\u2019 receding objects\nand \u2019\u00a9\u2019 no significant movement.\n4 Hardware design and implementation283\nThe entire collision detection algorithm, based on the modified LGMD as284\npresented in section 3 has been implemented on a Field Programmable Gate285\nArray (FPGA). In contrast to the previously published mixed digital\/analogue286\nimplementation of the LGMD[35,37], this all-digital implementation has key287\nadvantages in easy integration with other digital algorithms on the FPGA.288\n13\nFPGA fabric\nRAM 1\nRAM 2\nInput from camera\nStored data\nInput from camera\nStored data\nInput video\nRGB camera data\nP-layer\nAlert\nLG\nM\nD\n \nim\npl\ne\nm\ne\nn\nta\ntio\nn\n24\n24\n24\n8\n8\n8\n8\nFig. 9. A high-level block diagram of the FPGA implementation of the modified\nLGMD model.\n4.1 Overall architecture and platform289\nThe high-level block diagram of the overall architecture of the system is shown290\non figure 9. The real-time video stream is input from a digital camera to the291\nFPGA chip, displayed on an monitor and the frames transferred to gray scale292\nimages stored in two external RAMs. The neural computing is carried out on293\nthe FPGA chip, the excitation S-layer is displayed on another monitor, and294\nan alert is also generated.295\nFigure 10 shows the system setup. It includes a Celoxica RC340 board, a dig-296\nital camera and two monitors. The LGMD and D cell outputs are displayed297\non the board\u2019s LCD, and the LEDs (flash lights) are activated on alert. The298\nCeloxica RC340 board is packaged with a Xilinx Virtex-4 XC4VLX160, em-299\nbedded Block RAM totaling 5,184 Kbits and four banks of ZBT RAM totaling300\n32MB, LCD, LEDs and multiple video input and output ports.301\n14\nFig. 10. The system setup includes a Celoxica RC340 board, a digital camera and\ntwo monitors. The modified LGMD model lights up the LEDs (flash lights) on the\nFPGA board based on the values of both LGMD and D cells. These values are also\nshown on the LCD of the FPGA board.\n4.2 FPGA design302\nThe FPGA design (see figure 11) has five blocks: the input, P-layer, S-layer, J303\ncell and D cell. The input and P-layer blocks run in parallel, while the S-layer304\ngets triggered when the entire frame has been processed.305\nThe input block reads real-time camera data in 24 bit RGB format and con-306\nverts it into 8-bit gray-scale intensity. The 8-bit intensity value is written into307\none of the available RAM blocks whiles the corresponding stored data is read308\nfrom the other RAM block, serving as the previous pixel value. The 10-bit x-309\nlocation and y-location address is also use to address the store data in RAM.310\nThe two block of RAM are used to buffer input data from the camera.311\nThe current pixel value (from the camera) and the previous pixel value (from312\nRAM) are used to estimate the luminance P-layer value for the corresponding313\npixel. This three stage pipeline is completed when an entire frame is captured.314\nThe excitatory S-layer is then triggered. This layer uses all eight neighboring315\npixels in the P-layer. The architecture implemented here is as shown in figure316\n12. Pixel data from the three rows involved in the computation are copied317\ninto a buffer one after the other. The S-layer for each pixel takes exactly three318\n15\nRGB2Gray\nRAM 2RAM 1\nP layer\nH cell J cell\nD cell LGMD cell\nS layer\nE layer\nAddress bus\nI layer\nAddress bus\nFig. 11. A high-level circuitry diagram of the various blocks on FPGA\nclock cycles, the same number of cycles required to fill the three buffers.319\nThe processing requires seven comparators arranged in a chain as shown in320\nfigure 12 and begins execution as soon as the buffer is full. From figure 12,321\nthe shaded pixels in the second row are the pixel whose corresponding S-layer322\nvalue will be generated after three clock cycles.323\nThe S-layer data is passed over to the J cell, which sums all the pixels values324\nfrom the S-layer. This block runs in parallel with the S-layer and uses a single325\naccumulator. The J cell in conjunction with the H cell is used to generate the326\nvalue for the D cell. The D cell uses the H cell, which is the delayed J cell327\nvalue, as shown in figure 11.328\nIn addition, we simulate equation 11, which determines the output of the329\nLGMD cell from the input, J , using a step function, thus avoiding the com-330\nputation of exponentials and division. We discretize the output, LGMD, into331\nthe set {0.50, 0.51, \u00b7 \u00b7 \u00b7 , 0.99, 1.00}. Since equation 11 is monotonically increas-332\ning in J , we can rearrange equation 11 to equation 14, to back-calculate the333\nminimum and maximum values of J that yield a specified value of LGMD334\n(e.g. we plug values of LGMD = 0.505 and LGMD = 0.515 into 14 to cal-335\n16\nP layer\nS layer\nFig. 12. A detailed translation of the p-layer into s-layer\nculate the minimum and maximum values of J that map to LGMD = 0.51).336\nThese range limits are checked in parallel to determine the value of LGMD337\nin a single clock cycle.338\nJf = \u2212 ln(LGMD\n\u22121\nf \u2212 1)\u00d7 ncell (14)339\nAll the layers in the modified LGMD have been implemented on the FPGA340\nfabric with the use of the Block RAM, making it possible to address each341\nlayer like a dual-port memory block. The hardware implementation currently342\nexcludes the FFI cell as shown in figure 1. However, this can be easily added343\nas it is not computationally complex. The hardware implementation rather344\nmakes use of a predefined threshold to estimate the excitation. The excitation345\nof the LGMD cell in figure 12 is very dependent on the value of the D cell;346\nthus if the object is stationary or receding, there is no alert generated at the347\nLGMD cell.348\nThe resources used by the FPGA implementation are listed in table 2. It was349\nimplemented on a Xilinx Virtex-4 XC4VLX160 chip, package FF1148 and350\nspeed grade -10. Memory and IO requirements are high, but computational351\nrequirements are minimal.352\n17\n0 2 4 6 8 10 12 14\nx 105\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nInput values of the J cell\nO\nut\npu\nt v\nal\nue\ns \nof\n \nth\ne \nLG\nM\nD \nce\nll\nLGMD output in software simulation\nLGMD output in FPGA implementation\nFig. 13. A step function is used in FPGA implementations for determining the\noutputs of LGMD cell (vertical axis) from the inputs of J cell (horizontal axis).\nOnly 51 values {0.50, 0.51, \u00b7 \u00b7 \u00b7 , 0.99, 1.00} were used for the outputs of LGMD cell\nin the FPGA implementation. Here, the image size is 600\u00d7 400.\nTable 2\nImplementation results for the modified LGMD, using Virtex-4 XC4VLX160, pack-\nage FF1148 and speed grade -10 .\nResource Total Used\nName Total Used Per.(%)\nFlip Flops 135,168 2,325 1\n4 input LUTs 135,168 3,001 2\nbonded IOBs 768 355 46\nOccupied Slices 67,584 3,206 4\nRAM16s 288 285 98\n4.3 Hardware testing results353\nThe hardware implementation has been tested with two frame sizes, 300\u00d7200354\nand 600 \u00d7 400. The maximum attainable clock frequency is 50MHz, with355\n40MHz being the highest stable frequency. The design takes a total of 3N +7356\ncycles to completely generate an LGMD output, where N is the number of357\npixels in the entire frame. For frame size 300 \u00d7 200 running at 40MHz, the358\nsystem processes approximately 222 frames per second; for frame size 600\u00d7400359\nthe value reduces to 55 frames per second. The low resource utilization of360\n18\n          001                         008                         015                         022                         029 \n          042                         049                         056                         063                         070 \n           084                         091                         098                         105                         112 \nFig. 14. Frame samples from a video clip of a looming and receding hand movement.\nThe frame numbers are shown under each frame. There are 115 frames, size 600\u00d7400,\nat frame rate 25 f.p.s.\nthe implementation makes it possible to run multiple LGMD at the same361\nfrequency.362\nThe high computational efficiency makes it possible for the modified LGMD363\nto be used in visual sensor systems with very high frame rate and\/or high364\nimage resolution.365\nThe reported clock frequency of 40MHz to 50MHz also includes the design for366\ncontrolling the external logic for the 2 VGAs, the camera input and the LEDs367\nfor alerts. The design and verification was accomplished using Handel-C high368\nlevel descriptive language. Compilation and simulation were achieved using369\nthe Agility DK design suite. Synthesis, the translation of abstract high-level370\ncode into a gate-level netlist, was accomplished using Xilinx ISE tools.371\nFigure 14 shows a video sequence used to test the hardware implementation.372\nThe object (hand) approaches and recedes three times. The video was recorded373\ninto the digital camera and the outputs of the LGMD and D cells were written374\ninto the external memory, and retrieved for plotting; see figure 15. We can see375\nclearly that the FPGA implementation worked very well in response to this376\nobject movement. In comparison with the software simulation results (see377\nfigure 6), the curve is not as smooth, due to the step function used in the378\ncomputation of the LGMD values. Nevertheless, this implementation fulfils379\nthe task of giving correct alarms.380\n19\n0 20 40 60 80 100 120\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nFig. 15. Experimental results read from external memory of the FPGA board, using\nthe video sequence in figure 14. The vertical axis is the normalized membrane po-\ntentials of the LGMD cell. The markers denote the depth movement direction; \u2018\u25b3\u2019\ndenote approaching objects; \u2018\u25bd\u2019 receding objects and \u2018\u00a9\u2019 no significant movement.\n5 Conclusion381\nIn this paper, we propose an LGMD model that provides additional informa-382\ntion on the depth direction of the movement. It requires little additional com-383\nputational cost compared to previous models, and can distinguish approaching384\nfrom receding objects very quickly.385\nThe new model has been implemented on the Xilinx FPGA chip, and the386\ngeneral purpose design is suitable for transfer to any other FPGA device.387\nThe design is compact, occupying limited hardware resources, and therefore388\nbe easily integrated with other computational components on a single chip.389\nIt has been successfully tested on real-time video clips; experimental results390\nshowed hardware performance is consistent with software simulation results.391\nThe high computational efficiency makes the modified LGMD suitable for392\nuse in visual sensor systems with very high frame rate and\/or high image393\nresolution, and the implementation on a general purpose hardware platform394\nmakes it suitable for application in various situations.395\nIn future research we will design a complete chip combining this LGMD model396\nwith the specialized translation-sensitive neural network. This will provide397\nboth translation and depth movement information, and will work as a general398\nmotion tracking sensor.399\n20\n6 Acknowledgments400\nThe authors would like to thank the UK Trade and Strategy Board for sup-401\nporting the project under the program grant TP\/2\/SC\/6\/I\/10444.402\nReferences403\n[1] F. C. Rind, P. J. Simmons, Seeing what is coming: building collision-sensitive404\nneurones., Trends in Neurosciences 22 (1999) 215\u2013220.405\n[2] R. D. Santer, P. J. Simmons, F. C. Rind, Gliding behaviour elicited by406\nlateral looming stimuli in flying locusts., Journal of comparative physiology.407\nA, Neuroethology, sensory, neural, and behavioral physiology 191 (1) (2005)408\n61\u201373.409\n[3] F. C. Rind, R. D. Santer, M. J. Blanchard, P. F. M. J. Verschure, Locust looming410\ndetectors for robot sensors, Springer, Berlin, 2003.411\n[4] S. Judge, F. C. Rind, The locust DCMD, a movement-detecting neurone tightly412\ntuned to collision trajectories, Journal of Experimental Biology 200 (16) (1997)413\n2209\u20132216.414\n[5] F. C. Rind, P. J. Simmons, Orthopteran DCMD neuron: a reevaluation of415\nresponses to moving objects. I. selective responses to approaching objects, J416\nNeurophysiol 68 (5) (1992) 1654\u20131666.417\n[6] S. B. i Badia, P. F. Verschure, A collision avoidance model based on the lobula418\ngiant movement detector neuron of the locust, in: proceedings of IJCNN, 2004,419\npp. 1757\u20131761.420\n[7] S. Bermudez i Badia, P. Pyk, P. F. Verschure, A fly-locust based neuronal421\ncontrol system applied to an unmanned aerial vehicle: the invertebrate neuronal422\nprinciples for course stabilization, altitude control and collision avoidance, The423\nInternational Journal of Robotics Research 26 (7) (2007) 759\u2013772.424\n[8] F. Rind, D. Bramwell, Neural network based on the input organization of an425\nidentified neuron signaling impending collision., Journal of Neurophysiology 75426\n(1996) 967\u2013985.427\n[9] M. Blanchard, P. F. M. J. Verschure, F. C. Rind, Using a mobile robot to study428\nlocust collision avoidance responses, Int. J. Neural Syst. 9 (5) (1999) 405\u2013410.429\n[10] M. Blanchard, F. Rind, P. Verschure, Collision avoidance using a model of the430\nlocust LGMD neuron., Robotics and Automonous Systems 30 (2000) 17\u201338.431\n[11] F. C. Rind, A directionally selective motion-detecting neurone in the brain432\nof the locust: physiological and morphological characterization, Journal of433\nExperimental Biology 149 (1990) 1\u201319.434\n21\n[12] F. C. Rind, Identification of directionally selective motion-detecting neurones in435\nthe locust lobula and their synaptic connections with an identified descending436\nneurone, Journal of Experimental Biology 149 (1990) 21\u201343.437\n[13] S. Yue, F. C. Rind, Visual motion pattern extraction and fusion for438\ncollision detection in complex dynamic scenes, Computer Vision and Image439\nUnderstanding 104 (1) (2006) 48 \u2013 60.440\n[14] S. Yue, F. C. Rind, A synthetic vision system using directionally selective441\nmotion detectors to recognize collision, Artificial Life 13 (2) (2007) 93\u2013122.442\n[15] S. Yue, F. C. Rind, Collision detection in complex dynamic scenes using443\nan LGMD-based visual neural network with feature enhancement, IEEE444\nTransactions on Neural Networks 17 (3) (2006) 705\u2013716.445\n[16] H. Meng, S. Yue, A. Hunter, K. Appiah, M. Hobden, N. Priestley, P. Hobden,446\nC. Pettit, A modified neural network model for lobula giant movement detector447\nwith additional depth movement feature, in: Proceedings of IJCNN, USA, 2009,448\npp. 2078\u20132083.449\n[17] C. Connolly, Collision avoidance technology: from parking sensors to unmanned450\naircraft, Sensor Review 27 (3) (2007) 182\u2013188.451\n[18] M. Bertozzi, A. Broggi, A. Fascioli, Vislab and the evolution of vision-based452\nUGVs, IEEE Computer 39 (12) (2006) 31\u201338.453\n[19] M. Bertozzi, L. Bombini, A. Broggi, P. Zani, P. Cerri, P. Grisleri, P. Medici,454\nGold: A framework for developing intelligent-vehicle vision applications, IEEE455\nIntelligent Systems 23 (1) (2008) 69\u201371.456\n[20] A. Hanazawa, T. Miki, K. Horio (Eds.), Brain-Inspired Information Technology,457\nVol. 266 of Studies in Computational Intelligence, Springer, 2010.458\n[21] H. Longuet Higgins, K. Prazdny, The interpretation of a moving retinal image,459\nProceedings of the Royal Society of London. Series B, Biological Sciences 208460\n(1980) 385\u2013397.461\n[22] G. T. Kenyon, Time-to-collision estimation from motion based on primate visual462\nprocessing, IEEE Trans. Pattern Anal. Mach. Intell. 27 (8) (2005) 1279\u20131291.463\n[23] S. Nedevschi, S. Bota, C. Tomiuc, Stereo-based pedestrian detection464\nfor collision-avoidance applications, IEEE Trans. Intelligent Transportation465\nSystems 10 (3) (2009) 380\u2013391.466\n[24] A. Barth, U. Franke, Estimating the driving state of oncoming vehicles from467\na moving platform using stereo vision, IEEE Trans. Intelligent Transportation468\nSystems 10 (4) (2009) 560\u2013571.469\n[25] A. Wedel, U. Franke, J. Klappstein, T. Brox, D. Cremers, Realtime470\ndepth estimation and obstacle detection from monocular video, in: DAGM-471\nSymposium, 2006, pp. 475\u2013484.472\n22\n[26] G. Sandini, M. Tistarelli, Active tracking strategy for monocular depth inference473\nover multiple frames, IEEE Trans. Pattern Anal. Mach. Intell. 12 (1) (1990) 13\u2013474\n27.475\n[27] F. Woelk, S. Gehrig, R. Koch, A monocular collision warning system, in:476\nProceedings of the 2nd Canadian Conference on Computer and Robot Vision,477\n2005, pp. 220\u2013227.478\n[28] D. Coombs, M. Herman, T. Hong, M. Nashman, Real-time obstacle avoidance479\nusing central flow divergence and peripheral flow, in: IEEE International480\nConference on Computer Vision, IEEE Computer Society, Los Alamitos, CA,481\nUSA, 1995, p. 276.482\n[29] C. Colombo, Time to collision from first-order spherical image motion, Robotics483\nand Autonomous Systems 31 (1-2) (2000) 5\u201315.484\n[30] F. Meyer, Time-to-collision from first-order models of the motion field, IEEE485\nTrans. Robotics and Automation 10 (1994) 792\u2013798.486\n[31] R. C. Nelson, J. Aloimonos, Obstacle avoidance using flow field divergence,487\nIEEE Trans. Pattern Anal. Mach. Intell. 11 (10) (1989) 1102\u20131106.488\n[32] J. L. Barron, D. J. Fleet, S. S. Beauchemin, T. A. Burkitt, Performance of489\noptical flow techniques, International Journal of Computer Vision 12 (1) (1994)490\n43\u201377.491\n[33] A. Polychronopoulos, M. Tsogas, A. Amditis, L. Andreone, Sensor fusion for492\npredicting vehicles\u2019 path for collision avoidance systems, IEEE Transactions on493\nIntelligent Transportation Systems 8 (3) (2007) 549\u2013562.494\n[34] N. Lazaros, G. C. Sirakoulis, A. Gasteratos, Review of stereo vision algorithms:495\nFrom software to hardware, International Journal of Optomechatronics 2 (4)496\n(2008) 435 \u2013 462.497\n[35] R. Laviana, L. Carranza, S. Vargas, G. Linan, E. Roca, A bioinspired498\nvision chip architecture for collision detection in automotive applications,499\nin: R. A. Carmona, G. Linan-Cembrano (Eds.), Society of Photo-Optical500\nInstrumentation Engineers (SPIE) Conference Series, Vol. 5839, 2005, pp. 13\u2013501\n24.502\n[36] J. Cuadri, G. Linan, R. Stafford, M. S. Keil, E. Roca, A bioinspired collision503\ndetection algorithm for VLSI implementation, Vol. 5839, SPIE, 2005, pp. 238\u2013504\n248.505\n[37] H. Okuno, T. Yagi, Real-time robot vision for collision avoidance inspired by506\nneuronal circuits of insects, in: IROS, 2007, pp. 1302\u20131307.507\n[38] H. Okuno, T. Yagi, A visually guided collision warning system with a508\nneuromorphic architecture, Neural Networks 21 (10) (2008) 1431\u20131438.509\n[39] R. D. Santer, R. Stafford, F. C. Rind, Retinally-generated saccadic suppression510\nof a locust looming detector neuron: Investigations using a robot locust., Journal511\nof the Royal Society: Interface 1 (2004) 61\u201377.512\n23\n[40] S. Yue, F. C. Rind, A collision detection system for a mobile robot inspired by513\nthe locust visual system, in: ICRA, 2005, pp. 3832\u20133837.514\n[41] S. Yue, F. C. Rind, M. S. Keil, J. Cuadri, R. Stafford, A bio-inspired visual515\ncollision detection mechanism for cars: Optimisation of a model of a locust516\nneuron to a novel environment, Neurocomputing 69 (13-15) (2006) 1591\u20131598.517\n24\n"}