{"doi":"10.1214\/07-AOS544","coreId":"96437","oai":"oai:eprints.lse.ac.uk:31548","identifiers":["oai:eprints.lse.ac.uk:31548","10.1214\/07-AOS544"],"title":"Profile-kernel likelihood inference with diverging number of parameters","authors":["Lam, Clifford","Fan, Jianqing"],"enrichments":{"references":[{"id":17288117,"title":"A note on the e\ufb03ciency of sandwich covariance matrix estimation.","authors":[],"date":"2001","doi":"10.1198\/016214501753382309","raw":"Kauermann, G. and Carroll, R.J. (2001). A note on the e\ufb03ciency of sandwich covariance matrix estimation. J. Amer. Statist. Assoc., 96, 1387\u20131396.","cites":null},{"id":17288131,"title":"An elementary estimator for the partially linear model.","authors":[],"date":"1997","doi":"10.1016\/s0165-1765(97)00218-8","raw":"Yatchew, A. (1997). An elementary estimator for the partially linear model. Economics Letters, 57, 135\u2013143.","cites":null},{"id":17288122,"title":"Asymptotic Behavior of Likelihood Methods for Exponential Families When the Number of Parameters Tends to In\ufb01nity.","authors":[],"date":"1988","doi":"10.1214\/aos\/1176350710","raw":"Portnoy, S. (1988). Asymptotic Behavior of Likelihood Methods for Exponential Families When the Number of Parameters Tends to In\ufb01nity. Ann. Statist., 16, 356\u2013366.","cites":null},{"id":17288126,"title":"Asymptotic Statistics.","authors":[],"date":"1998","doi":"10.1017\/cbo9780511802256","raw":"Van Der Vaart, A.W. (1998). Asymptotic Statistics. Cambridge Univ. Press.","cites":null},{"id":17288121,"title":"Asymptotics for least absolute deviation regression estimators.","authors":[],"date":"1991","doi":"10.1017\/s0266466600004394","raw":"Pollard, D. (1991). Asymptotics for least absolute deviation regression estimators. Econ. Theory, 7, 186\u2013199.","cites":null},{"id":17288114,"title":"Central Limit Theorems for C(S)-valued Random Variables.","authors":[],"date":"1975","doi":"10.1016\/0022-1236(75)90056-7","raw":"Jain, N. and Marcus, M. (1975). Central Limit Theorems for C(S)-valued Random Variables. J. Funct. Anal., 19, 216\u2013231.","cites":null},{"id":17288098,"title":"E\ufb03cient Estimation and Inferences for Varying-Coe\ufb03cient Models.","authors":[],"date":"2000","doi":"10.1080\/01621459.2000.10474280","raw":"Cai, Z., Fan, J. and Li, R. (2000). E\ufb03cient Estimation and Inferences for Varying-Coe\ufb03cient Models. J. Amer. Statist. Assoc., 95, 888\u2013902.","cites":null},{"id":17288130,"title":"E\ufb03cient estimation for semivarying-coe\ufb03cient models.","authors":[],"date":"2004","doi":"10.1093\/biomet\/91.3.661","raw":"Xia, Y., Zhang, W. and Tong, H. (2004). E\ufb03cient estimation for semivarying-coe\ufb03cient models. Biometrika, 91, 661\u2013681.","cites":null},{"id":17288097,"title":"E\ufb03cient Estimation of a Semiparametric Partially Linear Varying Coe\ufb03cient Model.","authors":[],"date":"2005","doi":"10.1214\/009053604000000931","raw":"Ahmad, I., Leelahanon, S. and Li, Q. (2005). E\ufb03cient Estimation of a Semiparametric Partially Linear Varying Coe\ufb03cient Model. Ann. Statist., 33, 258\u2013283. 45[2] Albright, S.C., Winston, W.L. and Zappe, C.J. (1999). Data Analysis and Decision Making with Microsoft Excel. Duxbury, Paci\ufb01c Grove, CA.","cites":null},{"id":17288109,"title":"Generalized Likelihood Ratio Statistics and Wilks Phenomenon.","authors":[],"date":"2001","doi":"10.1214\/aos\/996986505","raw":"Fan, J., Zhang, C. and Zhang, J. (2001). Generalized Likelihood Ratio Statistics and Wilks Phenomenon. Ann. Statist., 29, 153\u2013193.","cites":null},{"id":17288099,"title":"Generalized Partially Linear Single-Index Models.","authors":[],"date":"1997","doi":"10.1080\/01621459.1997.10474001","raw":"Carroll, R.J., Fan, J., Gijbels, I. and Wand, M.P. (1997). Generalized Partially Linear Single-Index Models. J. Amer. Statist. Assoc., 92, 477\u2013489.","cites":null},{"id":17288100,"title":"High-Dimensional Data Analysis: The Curses and","authors":[],"date":null,"doi":null,"raw":"Donoho, D.L., High-Dimensional Data Analysis: The Curses and Blessings of Dimensionality. Lecture on August 8, 2000, to the American Mathematical Society \u201dMath Challenges of the 21st Century\u201d.","cites":null},{"id":17288125,"title":"Kernel smoothing in partial linear models.","authors":[],"date":"1988","doi":null,"raw":"Speckman, P. (1988). Kernel smoothing in partial linear models. J. R. Statist. Soc. B, 50, 413\u2013436.","cites":null},{"id":17288132,"title":"Local Polynomial \ufb01tting in semivarying coe\ufb03cient model.","authors":[],"date":"2002","doi":"10.1006\/jmva.2001.2012","raw":"Zhang, W., Lee, S.Y., and Song, X.Y. (2002). Local Polynomial \ufb01tting in semivarying coe\ufb03cient model. J. Mult. Anal., 82, 166\u2013188.","cites":null},{"id":17288102,"title":"Local Polynomial Modelling and Its Applications.","authors":[],"date":"1996","doi":"10.1007\/978-1-4899-3150-4_5","raw":"Fan, J. and Gijbels, I. (1996). Local Polynomial Modelling and Its Applications. New York: Chapman and Hall.","cites":null},{"id":17288106,"title":"Nonconcave penalized likelihood with a diverging number of parameters.","authors":[],"date":"2004","doi":"10.1214\/009053604000000256","raw":"Fan, J. and Peng, H. (2004). Nonconcave penalized likelihood with a diverging number of parameters. Ann. Statist., 32, 928\u2013961.","cites":null},{"id":17288108,"title":"Normalization and analysis of cDNA microarrays using within-array replications applied to neuroblastoma cell response to a cytokine.","authors":[],"date":"2004","doi":"10.1073\/pnas.0307557100","raw":"Fan, J., Tam, P., Vande Woude, G. and Ren, Y. (2004) Normalization and analysis of cDNA microarrays using within-array replications applied to neuroblastoma cell response to a cytokine. Proc. Natl. Acad. Sci. USA, 101, 1135\u20131140.","cites":null},{"id":17288128,"title":"Partial spline models for semiparametric estimation of functions of several variables.","authors":[],"date":"1984","doi":null,"raw":"Wahba, G. (1984) Partial spline models for semiparametric estimation of functions of several variables. In Statistical Analysis of Time Series, Proceedings of the Japan U.S. Joint Seminar, Tokyo, 319\u2013329. Institute of Statistical Mathematics, Tokyo.","cites":null},{"id":17288110,"title":"Partially Linear Models.","authors":[],"date":"2000","doi":"10.1007\/978-3-642-57700-0_6","raw":"H\u00a8 ardle, W., Liang, H. and Gao, J.T. (2000). Partially Linear Models. Springer-Verlag, New York.","cites":null},{"id":17288129,"title":"Pro\ufb01le Likelihood and Conditionally Parametric Models.","authors":[],"date":"1992","doi":"10.1214\/aos\/1176348889","raw":"Wong, W.H. and Severini, T.A. (1992). Pro\ufb01le Likelihood and Conditionally Parametric Models. Ann. Statist., 20, 1768\u20131802.","cites":null},{"id":17288103,"title":"Pro\ufb01le Likelihood Inferences on Semiparametric Varying-Coe\ufb03cient Partially Linear Models.","authors":[],"date":"2005","doi":"10.3150\/bj\/1137421639","raw":"Fan, J. and Huang, T. (2005). Pro\ufb01le Likelihood Inferences on Semiparametric Varying-Coe\ufb03cient Partially Linear Models. Bernoulli., 11, 1031\u20131057.","cites":null},{"id":17288112,"title":"Pro\ufb01le-kernel versus back\ufb01tting in the partially linear models for longitudinal\/clustered data.","authors":[],"date":"2004","doi":"10.1093\/biomet\/91.2.251","raw":"Hu, Z., Wang, N. and Carroll, R.J. (2004). Pro\ufb01le-kernel versus back\ufb01tting in the partially linear models for longitudinal\/clustered data. Biometrika, 91, 251\u2013262.","cites":null},{"id":17288124,"title":"Quasi-likelihood Estimation in Semiparametric Models.","authors":[],"date":"1994","doi":"10.2307\/2290852","raw":"Severini, T.A. and Staniswalis, J.G. (1994). Quasi-likelihood Estimation in Semiparametric Models. J. Amer. Statist. Assoc., 89, 501\u2013511.","cites":null},{"id":17288113,"title":"Robust Statistics.","authors":[],"date":"1981","doi":"10.1002\/0471725250","raw":"Huber, P.J. (1981). Robust Statistics. New York: John Wiley & Sons. 46[18] Huber, P.J. (1973). Robust Regression: Asymptotics, Conjectures and Monte Carlo. Ann. Statist., 1, 799\u2013821.","cites":null},{"id":17288101,"title":"Semiparametric estimates of the relation between weather and electricity sales.","authors":[],"date":"1986","doi":"10.1080\/01621459.1986.10478274","raw":"Engle, R.F., Granger, C.W.J., Rice, J. and Weiss, A. (1986). Semiparametric estimates of the relation between weather and electricity sales. J. Amer. Statist. Assoc., 81, 310\u2013320.","cites":null},{"id":17288120,"title":"Semiparametric estimation in general repeated measures problems.","authors":[],"date":"2006","doi":"10.1111\/j.1467-9868.2005.00533.x","raw":"Lin, X. and Carroll, R.J. (2006). Semiparametric estimation in general repeated measures problems. J. R. Statist. Soc. B, 68, Part 1, 69\u201388.","cites":null},{"id":17288118,"title":"Semiparametric smooth coe\ufb03cient models.","authors":[],"date":"2002","doi":"10.1198\/073500102288618531","raw":"Li, Q., Huang, C.J., Li., D. and Fu, T.T. (2002). Semiparametric smooth coe\ufb03cient models. J. Bus. Econom. Statist., 20, 412\u2013422.","cites":null},{"id":17288105,"title":"Statistical Challenges with High Dimensionality: Feature Selection in Knowledge Discovery.","authors":[],"date":"2006","doi":"10.4171\/022-3\/31","raw":"Fan, J. and Li, R. (2006). Statistical Challenges with High Dimensionality: Feature Selection in Knowledge Discovery. Proceedings of the Madrid International Congress of Mathematicians 2006. To appear.","cites":null},{"id":17288104,"title":"Variable bandwidth and One-step Local M-Estimator.","authors":[],"date":"1999","doi":"10.1007\/bf02903849","raw":"Fan, J. and Jiang, J. (1999). Variable bandwidth and One-step Local M-Estimator. Science in China, 29, 1\u201315; (English series) 2000, 35, 65 \u2013 80.","cites":null},{"id":17288119,"title":"Variable Selection in Semiparametric Regression Modeling.","authors":[],"date":"2005","doi":"10.1214\/009053607000000604","raw":"Li, R. and Liang, H. (2005). Variable Selection in Semiparametric Regression Modeling. To appear.","cites":null},{"id":17288111,"title":"Varying-coe\ufb03cient models.","authors":[],"date":"1993","doi":null,"raw":"Hastie, T.J. and Tibshirani, R. (1993). Varying-coe\ufb03cient models. J. R. Statist. Soc. B, 55, 757\u2013796.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2008-10","abstract":"The generalized varying coefficient partially linear model with growing number of predictors arises in many contemporary scientific endeavor. In this paper we set foot on both theoretical and practical sides of profile likelihood estimation and inference. When the number of parameters grows with sample size, the existence and asymptotic normality of the profile likelihood estimator are established under some regularity conditions. Profile likelihood ratio inference for the growing number of\\ud\nparameters is proposed and Wilk\u2019s phenomenon is demonstrated. A new algorithm, called the accelerated profile-kernel algorithm, for computing profile-kernel estimator is proposed and investigated. Simulation studies show that the resulting estimates are as efficient as the fully iterative profile-kernel estimates. For moderate sample sizes, our proposed procedure saves much\\ud\ncomputational time over the fully iterative profile-kern one and gives stabler estimates. A set of\\ud\nreal data is analyzed using our proposed algorithm","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/96437.pdf","fullTextIdentifier":"http:\/\/eprints.lse.ac.uk\/31548\/1\/A_profile_kernel_estimation_with_diverging_number_of_linear_parameters_%28lsero%29.pdf","pdfHashValue":"4bfe20027f5301c319a91b50758dc114928c259b","publisher":"Institute of Mathematical Statistics","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lse.ac.uk:31548<\/identifier><datestamp>\n      2017-10-26T10:40:05Z<\/datestamp><setSpec>\n      74797065733D4445505453:4C53452D5354<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lse.ac.uk\/31548\/<\/dc:relation><dc:title>\n        Profile-kernel likelihood inference with diverging number of parameters<\/dc:title><dc:creator>\n        Lam, Clifford<\/dc:creator><dc:creator>\n        Fan, Jianqing<\/dc:creator><dc:subject>\n        HA Statistics<\/dc:subject><dc:description>\n        The generalized varying coefficient partially linear model with growing number of predictors arises in many contemporary scientific endeavor. In this paper we set foot on both theoretical and practical sides of profile likelihood estimation and inference. When the number of parameters grows with sample size, the existence and asymptotic normality of the profile likelihood estimator are established under some regularity conditions. Profile likelihood ratio inference for the growing number of\\ud\nparameters is proposed and Wilk\u2019s phenomenon is demonstrated. A new algorithm, called the accelerated profile-kernel algorithm, for computing profile-kernel estimator is proposed and investigated. Simulation studies show that the resulting estimates are as efficient as the fully iterative profile-kernel estimates. For moderate sample sizes, our proposed procedure saves much\\ud\ncomputational time over the fully iterative profile-kern one and gives stabler estimates. A set of\\ud\nreal data is analyzed using our proposed algorithm.<\/dc:description><dc:publisher>\n        Institute of Mathematical Statistics<\/dc:publisher><dc:date>\n        2008-10<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lse.ac.uk\/31548\/1\/A_profile_kernel_estimation_with_diverging_number_of_linear_parameters_%28lsero%29.pdf<\/dc:identifier><dc:identifier>\n          Lam, Clifford and Fan, Jianqing  (2008) Profile-kernel likelihood inference with diverging number of parameters.  Annals of Statistics, 36 (5).  pp. 2232-2260.  ISSN 0090-5364     <\/dc:identifier><dc:relation>\n        http:\/\/imstat.org\/aos\/<\/dc:relation><dc:relation>\n        10.1214\/07-AOS544<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lse.ac.uk\/31548\/","http:\/\/imstat.org\/aos\/","10.1214\/07-AOS544"],"year":2008,"topics":["HA Statistics"],"subject":["Article","PeerReviewed"],"fullText":"  \nClifford Lam   \nProfile-kernel likelihood inference with \ndiverging number of parameters \n \nArticle (Accepted version) \n(Refereed) \nOriginal citation: \nLam, Clifford and Fan, Jianqing (2008) Profile-kernel likelihood inference with diverging number \nof parameters. The annals of statistics, 36 (5). pp. 2232-2260. ISSN 0090-5364 \n \nDOI: 10.1214\/07-AOS544  \n \n\u00a9 2008 Institute of Mathematical Statistics\n \nThis version available at: http:\/\/eprints.lse.ac.uk\/31548\/\n \nAvailable in LSE Research Online: March 2011 \n \nLSE has developed LSE Research Online so that users may access research output of the \nSchool. Copyright \u00a9 and Moral Rights for the papers on this site are retained by the individual \nauthors and\/or other copyright owners. Users may download and\/or print one copy of any \narticle(s) in LSE Research Online to facilitate their private study or for non-commercial research. \nYou may not engage in further distribution of the material or use it for any profit-making activities \nor any commercial gain. You may freely distribute the URL (http:\/\/eprints.lse.ac.uk) of the LSE \nResearch Online website.  \n \nThis document is the author\u2019s final manuscript accepted version of the journal article, \nincorporating any revisions agreed during the peer review process.  Some differences between \nthis version and the published version may remain.  You are advised to consult the publisher\u2019s \nversion if you wish to cite from it. \nA Profile-Kernel Estimation with Diverging\nNumber of Linear Parameters\nBy Clifford Lam and Jiangqing Fan\nDepartment of Operations Research and Financial Engineering\nPrinceton University, Princeton, NJ, 08544\nMay 26, 2006\nAbstract\nA generalization to the varying coefficient model, the generalized varying coefficient par-\ntially linear model (GVCPLM) has gained significant attention because of its generality\nand incorporated predictive and explanatory power. Since modern statistical problems\nusually deal with data of vast dimensionality, a large model is usually unavoidable for\npredictive purpose. In this paper we set foot on both theoretical and practical sides of\nprofile likelihood estimation when the number of linear parameters in the model grows\nwith sample size. Existence of profile likelihood estimator and asymptotic normality for\nthe linear parameters are established under regularity conditions. Profile likelihood ratio\nstatistic for the linear parameters is discussed and Wilk\u2019s phenomenon demonstrated as\nproposed by Fan, Zhang and Zhang (2001). We propose a profile-kernel based algorithm\nfor evaluating the varying coefficients and the linear parameters. Simulation study shows\nthat the resulting estimates are as efficient as the fully iterative profile-kernel estimates.\nFor moderate sample size, our proposed procedure saves much computational time over\nthe fully iterative profile-kernel one and gives stabler estimates. A set of real data has\nbeen analyzed using the GVCPLM with our proposed algorithm.\n1 Introduction\nThe generalized varying-coefficient models, proposed by Hastie and Tibshirani (1993),\nhas attracted more attention over the last decade. It is a form of semiparametric regres-\nsion which extends the generalized linear model (e.g. McCullagh and Nelder (1989))\n1\nnaturally so that the linear parameters become nonparametric functions of a covariate\nU , e.g. time variable in a longitudinal data analysis. For instance, see Cai, Fan and\nLi (2000) for a detailed account on statistical inferences on such models and references\ntherein. A further generalization to the generalized varying coefficient model is to allow\nfor an additive parametric part, resulting in the generalized varying coefficient partially\nlinear model (GVCPLM). If Y is a response variable and (U,X,Z) is the associated\ncovariates, then by letting \u00b5(u,x, z) = E{Y |(U,X,Z) = (u,x, z)}, the GVCPLM takes\nthe form\n(1) g{\u00b5(u,x, z)} = xT\u03b1(u) + zT\u03b2,\nwhere g(\u00b7) is a known link function, \u03b2 an unknown regression coefficient and \u03b1(\u00b7) an un-\nknown regression function. One of the advantages over the varying coefficient model is\nthat GVCPLM allows for estimation of effects more efficiently when they are not really\nvarying with U , after adjustment of other genuine varying effects. It also allows for more\ninterpretable model, where primary interest is focused on the parametric component.\nThis model is relatively new in the literature. Instead, a special case called the partially\nlinear model (PLM) is studied more extensively, where the vector x is set to the scalar 1.\nSee, for example, Engle,et al. (1986), Wahba (1984) and Speckman (1988). Severini and\nWong (1992) established theories in generalized profile likelihood approach for efficient\nestimation of the parametric component without the need of undersmoothing, and Sev-\nerini and Staniswalis (1994) proposed an iterative procedure for this profile likelihood\nestimation. Carroll et al. (1997) studied the generalized partially linear single-index\nmodel. More references can be found in Ha\u00a8rdle, Liang and Gao (2000).\nThe goals of this paper are two-fold: to establish theories in statistical inferences\nwhen the dimension of the parametric component diverges with the sample size, and to\ncompute the estimates efficiently without sacrificing accuracy.\nFor the estimation aspect, Zhang, Lee and Song (2002), Li, Huang, Ki and Fu (2002)\nand Xia, Zhang and Tong (2004) considered the varying coefficient partially linear model\n(VCPLM, g being the identity link) and proposed different methods of estimation. Ah-\nmad, Leelahanon and Li (2005) considered a series approximation approach for estimat-\ning the nonparametric component in the VCPLM, while Fan and Huang (2005) proposed\na profile-kernel approach for the VCPLM which has closed form solutions. Li and Liang\n(2005) considered a backfitting-based procedure for estimating a GVCPLM (a general\nlink g).\nIn this paper we propose a profile-kernel procedure for the GVCPLM in (1) based\non Newton-Raphson iterations. Computational difficulties (e.g. Lin and Carroll (2006))\nof the profile-kernel approach is overcome by introducing modifications to updating of\n2\nthe parametric component. For moderate sample size the computational expenses are\nthen greatly reduced while nice properties of profile-kernel approach over backfitting\n(e.g. Hu et al. (2004)) are retained. This will be further demonstrated in section 4,\nwhere Poisson and Logistic GVCPLM are considered for simulations. We also introduce\na difference-based estimation for the parametric component of the GVCPLM, which\nserves well for an initial estimate of our proposed profile-kernel procedure. Such an idea\nfor estimation is used, for example, in Yatchew (1997) for the partial linear model.\nFor estimation with diverging number of parameters, early of such works include\nHuber (1973) (more of his work can be found in Huber (1981)) which gave related\ntheories on M-estimators, and Portnoy (1988) which analyzed a regular exponential\nfamily under the same setting. Fan and Peng (2004) analyzed a general parametric\nmodel using the penalized likelihood approach under such setting. Donoho (2000) gave\na full introduction on how high dimensional data affects the trend of data analysis, with\nexamples in various fields of applications. Fan and Li (2006) proposed the penalized\nlikelihood method to achieve both estimation and variable selection simultaneously in\nvarious fields involving high dimensional data analysis. We give two examples where a\nlarge number of parameters is to be estimated relative to the sample size.\nExample 1 (Framingham Heart Study (FHS)). In this classical study initiated in 1948,\nthe FHS follows a representative sample of 5,209 adults and their offspring aged 28-62\nyears in Framingham, Massachusetts. One goal of the study is to identify major risk\nfactors associated with heart disease, stroke and other diseases. The study lasted for\nmore than half a century, with original participants\u2019 adult children and their spouses\nalso participated in the study. There are around p = 100 variables for the study\nand so the number of parameters is large relative to the sample size. For more in-\nformation on this study, see the website of National Heart, Lung and Blood Institute\n(http:\/\/www.nhlbi.nih.gov\/about\/framingham).\nExample 2 (Computational Biology). DNA microarrays monitor the mRNA expres-\nsions of thousands of genes in many areas of biomedical research. The cDNA microarrays\nmeasures the abundance of mRNA expressions by mixing mRNAs of treatment and con-\ntrol cells or tissues. However, systematic biases due to experimental variations have to\nbe removed first before the expression data can be used for further analysis. Exam-\nple of such biases include efficiency of dye incorporation, intensity effect and print-tip\nblock effect, among others. The process of removing these experimental biases is called\nnormalization, and is critical to multiple array comparison.\nLet Yg be the log-ratio of the intensity of gene g of the treatment sample over that\nof the control sample. Denote Ag the average log-intensities of gene g at the treatment\n3\nand control samples, rg and cg the row and column of the block where the cDNA of gene\ng resides. Fan et al.(2004) proposed the following model to estimate the intensity and\nblock effect:\nYg = \u03b1g + \u03b2rg + \u03b3cg + f(Ag) + \u000fg, g = 1, \u00b7 \u00b7 \u00b7 , N\nwhere \u03b1g is the treatment effect of gene g, \u03b2rg and \u03b3rg are block effects decomposed into\nrow and column components, f(Ag) represents the intensity effect, and N is the total\nnumber of genes. Even with replications of genes, we can see that the above model has\nnumber of parameters p = O(N). However the number of significant genes is relatively\nsmall, so that \u03b1g has a sparse structure. The goal is to find genes g with \u03b1g statistically\nsignificantly different from 0.\nThe outline of the paper is as follows. In section 2 we briefly review the profile\nlikelihood estimation with local polynomial modelling, as well as presenting asymptotic\nresults in sections 2.1-2.3. Section 3 turns to the computational aspect, and sections\n3.1-3.4 discuss the elements of our proposed profile-kernel procedure, as well as how to\nchoose smoothing parameters. A simulation study is given in section 4, as well as an\nanalysis of a real data set using the proposed methodology. The proofs of our results is\ngiven in section 5, and technical details in the appendix.\n2 Properties of profile likelihood estimation\nLet (Yni;Xi,Zni, Ui)1\u2264i\u2264n be a random sample where Yni is a scalar response variable, Ui\nis a scalar variable, Xi \u2208 Rq and Zni \u2208 Rpn are vectors of explanatory variables. Note\nthat Yni and Zni depends on n, and pn \u2192\u221e as n\u2192\u221e.\nThe model we consider for the data is the generalized varying coefficient partially\nlinear model(GVCPLM), as in model (1), with \u03b2n and Zn having dimensions depending\non n now. The quasi-likelihood function for the response Y is\nQ(\u00b5, y) =\n\u222b y\n\u00b5\ns\u2212 y\nV (s)\nds,\nwhere V (\u00b7) is the variance function for Y . As in Severini and Wong (1992), we denote\nby \u03b1\u03b2n(u) the \u2018least favorable curve\u2019 of the nonparametric function \u03b1(u) when we fix\nthe linear parameter to be \u03b2n for estimation purpose. It can be defined such that\n(2)\n\u2202\n\u2202\u03b7\nE0\n{\nQ(g\u22121(\u03b7TX+ \u03b2n\nTZn), Yn)|U = u\n} |\u03b7=\u03b1\u03b2n (u) = 0,\n4\nwhere E0 means expectation is taken under the true parameters \u03b10(u) and \u03b2n0. Note\nthat \u03b1\u03b2n0(u) = \u03b10(u). The global likelihood function for the data is then\n(3) Qn(\u03b2n) =\nn\u2211\ni=1\nQ{g\u22121(\u03b1\u03b2n(Ui)TXi + \u03b2TnZni), Yni}.\nTo estimate the parameters in (3), we first treat \u03b2n as a constant. The model\nthen becomes purely nonparametric and estimation of \u03b1\u03b2n(Ui) is done through a local\npolynomial regression of order p for the jth component of \u03b1\u03b2n(Ui), which approximate\n\u03b1j(U) \u2248 \u03b1j(u) + \u2202\u03b1j(u)\n\u2202u\n(U \u2212 u) + \u00b7 \u00b7 \u00b7+ \u2202\np\u03b1j(u)\n\u2202up\n(U \u2212 u)p\/p!\n\u2261 a0j + a1j(U \u2212 u) + \u00b7 \u00b7 \u00b7+ apj(U \u2212 u)p\/p!\nfor U in a neighborhood of u. Denote ar = (ar1, \u00b7 \u00b7 \u00b7 , arq)T for r = 0, . . . , p , noting that\nthey depend on \u03b2n. We then maximize the local likelihood\n(4)\nn\u2211\ni=1\nQ{g\u22121(\np\u2211\nr=0\nar\nTXi(Ui \u2212 u)r\/r! + \u03b2TnZni), Yni}Kh(Ui \u2212 u)\nwith respect to a0, \u00b7 \u00b7 \u00b7 , ap. K(\u00b7) is a kernel function, and Kh(t) = K(t\/h)\/h is a re-\nscaling of K with bandwidth h. So we get estimate \u03b1\u02c6\u03b2n(Uj) = a\u02c60(Uj) for j = 1, . . . , n.\nPlugging our estimates into the global likelihood function (3), we have\n(5) Q\u02c6n(\u03b2n) :=\nn\u2211\ni=1\nQ{g\u22121(\u03b1\u02c6\u03b2n(Ui)TXi + \u03b2TnZni), Yni}.\nThis is now a pure parametric model with parameter \u03b2n. Maximizing Q\u02c6n(\u03b2n) with\nrespect to \u03b2n to get \u03b2\u02c6n, which amounts to solving \u2207Q\u02c6n(\u03b2n) = 0. With \u03b2\u02c6n, we estimate\nour varying coefficients as \u03b1\u02c6\u03b2\u02c6n(u).\nOne property of the quasi-likelihood is that the first and second order Bartlett\u2019s\nidentities hold. In particular, if we define the marginal global likelihood for \u03b2n as in (3),\nthen\n(6) E\u03b2n\n(\n\u2202Qn\n\u2202\u03b2n\n)\n= 0, nIn(\u03b2n) = E\u03b2n\n(\n\u2202Qn\n\u2202\u03b2n\n\u2202Qn\n\u2202\u03b2Tn\n)\n= \u2212E\u03b2n\n(\n\u22022Qn\n\u2202\u03b2n\u2202\u03b2\nT\nn\n)\n,\nwhere In(\u03b2n) is the marginal Fisher Information of a single observation for \u03b2n (See\nSeverini and Wong (1992) for more details).\n5\nNote that equation (2) is true for all \u03b2n, and so by differentiating w.r.t. \u03b2n we get\nthe following important formulas:\nE0(q1(mn(\u03b2n), Yn)X|U = u) = 0,\nE0(q2(mn(\u03b2n), Yn)X(Zn +\u03b1\n\u2032\n\u03b2n\n(U)X)T |U = u) = 0,\n(7)\nwhere \u03b1\u2032\u03b2n(u) =\n\u2202\u03b1\u03b2n (u)\n\u2202\u03b2n\nand ql(x, y) =\ndl\ndxl\nQ(g\u22121(x), y).\nIn the subsequent sections we need some regularity conditions, which are presented\nin section 5, for our results to hold.\n2.1 Asymptotic normality and consistency of \u03b2\u02c6n\nTheorem 1 (Existence of profile likelihood estimator). Assume that conditions (A)-\n(G) are satisfied. If p4n\/n \u2192 0 as n \u2192 \u221e and nh2p+2 = O(1) with nhp+2 \u2192 \u221e, then\nthere is a local maximizer \u03b2\u02c6n \u2208 \u2126n of Q\u02c6n(\u03b2n) such that\n\u2225\u2225\u2225\u03b2\u02c6n \u2212 \u03b2n0\u2225\u2225\u2225 = OP (\u221apn\/n).\nThis consistent rate is the same as the result of the M-estimator that was studied\nby Huber (1973), in which the number of parameters diverges. This rate of convergence\nis also obtained by Zhang, Lee and Song (2002) for pn a constant. They also assumed\nnh2p+2 = O(1).\nSince the usual optimal bandwidth for minimizing conditional MSE or weighted\nMISE is h = O(n\u22121\/(2p+3))(Fan and Gijbels (1996)), it does not satisfy the assump-\ntion nh2p+2 = O(1). However, note that under the optimal bandwidth, we have\u2225\u2225\u2225\u03b2\u02c6n \u2212 \u03b2n0\u2225\u2225\u2225 = OP (\u221apn\/n(2p+2)\/(2p+3)) (follow the same lines of proof in theorem 1\nto get this). This rate is worse than\n\u221a\nn\/pn, but with somewhat stronger assumption\np5n\/n\n(2p+1)\/(2p+3) = o(1), a form of\n\u221a\nn-consistency can be recovered as in theorem 2. In\nparticular, if supn pn <\u221e, this stronger assumption is automatically satisfied, showing\nthat\n\u221a\nn-consistency can be achieved under optimal bandwidth. This is in line with the\nresults, for instance, by Severini and Staniswalis(1994) or Carroll et al. (1997).\nTheorem 2 (Asymptotic normality). Under Conditions (A) - (G), if p5n\/n \u2192 0 as\nn\u2192\u221e, then the \u221an\/pn-consistent local maximizer \u03b2\u02c6n in theorem 1 satisfies\n\u221a\nnAnI\n1\/2\nn (\u03b2n0)(\u03b2\u02c6n \u2212 \u03b2n0) D\u2212\u2192 N(0, G),\nwhere An is an l \u00d7 pn matrix such that AnATn \u2192 G, and G is a l \u00d7 l nonnegative\nsymmetric matrix. Furthermore, if p5n\/n\n(2p+1)\/(2p+3) \u2192 0, then the local maximizer \u03b2\u02c6n\n6\nin theorem 1, estimated under the optimal bandwidth h = O(n\u22121\/(2p+3)), still satisfies\nthe above asymptotic normality.\nThis result shows that profile likelihood estimation produces semi-parametric efficient\nestimate of linear parameters when number of parameters diverges. To see this more\nexplicitly, let pn = r be a constant. Then taking An = Ir, we obtain\n\u221a\nn(\u03b2\u02c6n \u2212 \u03b2n0) D\u2212\u2192 N(0, I\u22121n (\u03b2n0)),\nwhich shows that the variance of \u03b2\u02c6n achieves the efficient lower bound (See for example\nCarroll et al. (1997)). This also agrees with the result by Fan and Huang(2005), who\nstudied the same type of model under the usual linear regression setting with pn a\nconstant. The result presented here can be considered a further generalization of theirs.\n2.2 Hypothesis testing\nAfter estimation of parameters, it is of interest to test the statistical significance of\ncertain variables in the parametric component. Consider the problem of testing linear\nhypotheses:\nH0 : An\u03b2n0 = 0 vs H1 : An\u03b2n0 6= 0,\nwhere An is a l \u00d7 pn matrix and AnATn = Il for a fixed l. Both the null and the alter-\nnative hypotheses are semi-parametric, with nuisance functions \u03b1(\u00b7). The generalized\nlikelihood ratio test (GLRT) has statistic of the form\nTn = 2\n{\nsup\n\u2126n\nQ\u02c6n(\u03b2n)\u2212 sup\n\u2126n;An\u03b2n=0\nQ\u02c6n(\u03b2n)\n}\n,\nwhere Q\u02c6n(\u03b2n) is as defined in (5). It turns out that, even when the number of parame-\nters diverges with sample size, Tn still follows a chi-square distribution asymptotically,\nwithout reference to any nuisance parameters. This reveals the Wilk\u2019s phenomenon,\nas termed in Fan et al (2001). Hence under a semi-parametric model with increasing\nnumber of parameters, traditional likelihood ratio theory continues to apply and testing\nof linear hypotheses becomes easy.\nTheorem 3 Assuming conditions (A) - (G), under H0, we have\nTn\nD\u2212\u2192 \u03c72l ,\nprovided that p5n\/n \u2192 0 when nh2p+2 = O(1), or p5n\/n(2p+1)\/(2p+3) \u2192 0 when h =\nO(n\u22121\/(2p+3)).\n7\n2.3 Consistency of the sandwich covariance formula\nThe estimated covariance matrix for \u03b2\u02c6n can be obtained by the sandwich formula\n\u03a3\u02c6n = {\u22072Q\u02c6n(\u03b2\u02c6n)}\u22121c\u0302ov{\u2207Q\u02c6n(\u03b2\u02c6n)}{\u22072Q\u02c6n(\u03b2\u02c6n)}\u22121,\nwhere the middle matrix has (j, k) entry given by\n(c\u0302ov{\u2207Q\u02c6n(\u03b2\u02c6n)})jk =\n{\nn\u2211\ni=1\n\u2202Q\u02c6ni(\u03b2\u02c6n)\n\u2202\u03b2nj\n\u2202Q\u02c6ni(\u03b2\u02c6n)\n\u2202\u03b2nk\n}\n\u2212\n{\n1\nn\nn\u2211\ni=1\n\u2202Q\u02c6ni(\u03b2\u02c6n)\n\u2202\u03b2nj\nn\u2211\ni=1\n\u2202Q\u02c6ni(\u03b2\u02c6n)\n\u2202\u03b2nk\n}\n.\nWith the notation \u03a3n = n\n\u22121I\u22121n (\u03b2n0), we have the following consistency result for the\nsandwich formula.\nTheorem 4 Assuming conditions (A) - (G). If p5n\/n \u2192 0 when nh2p+2 = O(1) and\nnh2 \u2192\u221e as n\u2192\u221e, we have\nAn\u03a3\u02c6nA\nT\nn \u2212 An\u03a3nATn P\u2212\u2192 0 as n\u2192\u221e\nfor any l\u00d7pn (l is a fixed integer) matrix An such that AnATn = G. The same conclusion\nholds if p5n\/n\n(2p+2)\/(2p+3) = o(1) when h = O(n\u2212(2p+3)).\nThis result provides a way for constructing confidence intervals for \u03b2n. However\nwe stress the independence of such estimate in testing hypothesis as in section 2.2.\nSimulation results show that this formula indeed provide good estimates of the variances\nfor \u03b2\u02c6n. For more details on sandwich covariance formula, see Kauermann and Carroll\n(2001).\nThe theorems presented so far have assumptions p4n\/n = o(1) or p\n5\nn\/n = o(1) which\nare somewhat strong. However we will use p3n\/n = O(1) in our simulation in section 4\nto demonstrate a wider applicability of our theories in models like the generalized linear\nmodels.\n3 Computation of the estimates\nA profile-kernel approach for estimating \u03b2n in (3) is to find \u03b2\u02c6n maximizing (5). Backfit-\nting algorithm, on the other hand, does not assume \u03b1\u02c6\u03b2n(u) in (5) to depend on \u03b2n, and\nthe maximization w.r.t. \u03b2n is thus much easier to carry out. The updated \u03b2n is then\nsubstituted into (4) to find \u03b1\u02c6(u) again, and the iterations repeated until convergence.\n8\nSee Lin and Carroll (2006), Hu et al (2004) for more descriptions of the two methods\nand some closed-form solutions proposed for the partially linear models.\nIn general, the profile-kernel estimation can be carried out through the use of the\nNewton-Raphson algorithm on updating both \u03b2n and \u03b1\u03b2n(u) alternately. We will de-\nscribe modifications and implementations of the following steps in subsequent sections:\nUnmodified profile-kernel updating procedure\nStep 0 (Initialization). Find \u03b2(0)n , an initial estimate for \u03b2n. Set k = 0.\nStep 1. Compute bi = Z\nT\nni\u03b2\n(k)\nn . Replaces Z\nT\nni\u03b2n in (3) by bi and the problem becomes\npurely nonparametric (generalized varying coefficient model). Efficient estimation\nfor \u03b1\u02c6\n\u03b2\n(k)\nn\n(u) is available, for instance, in Cai, Fan and Li(2000).\nStep 2. Replaces \u03b1\u03b2n(u) in (3) by \u03b1\u02c6\u03b2n(u) and the problem becomes purely parametric.\nPerform a Newton-Raphson iteration\n\u03b2(k+1)n = \u03b2\n(k)\nn \u2212 {\u22072Q\u02c6n(\u03b2(k)n )}\u22121\u2207Q\u02c6n(\u03b2(k)n ).\nHere Q\u02c6n(\u03b2n) is as defined in (5). Derivative is taken with respect to \u03b2n, noting\nthat \u03b1\u02c6\u03b2n(u) depends on \u03b2n as well. Set k to k + 1.\nStep 3. Iterate steps 1 and 2 until convergence.\nSection 3.1 gives a detail account of obtaining an initial estimate for \u03b2n.\nFor modifications, we introduce a quick implementation of step 2 in section 3.3,\nwhich not only helps save vast amount of computational time for moderate sample size,\nbut also is much stabler comparing with the full procedure.\nThe idea behind the foregoing algorithm is to estimate a least favorable curve \u03b1\u02c6\u03b2n(u)\nfor \u03b1\u03b2n(u) at \u03b2n = \u03b2\n(k)\nn in light of lemma 6, which then allow us to update \u03b2\n(k)\nn to \u03b2\n(k+1)\nn\nas in step 2. Step 1 involves nonparametric estimation and is discussed in section 3.2.\nIn step 3 we need to iterate steps 1 and 2 until convergence. In practice, as is\ndemonstrated in simulation study in section 4, only several iterations are needed for\npractical accuracy. We name the estimates by doing step 0 and step 1 the one-cycle\nestimates, and those obtained by iterating steps 2 and step 1 (m\u2212 1) more times as the\nm-cycles estimates.\n9\n3.1 Difference-based estimation for VCPLM\nThe idea of differencing to remove nonparametric part in a partially linear model (PLM)\nhas been applied, with different usages, in Yatchew (1997) and Fan and Huang (2005).\nWe generalize this idea and apply on the varying coefficient partially linear model (VC-\nPLM).\nConsider the VCPLM with the structure\n(8) Y = \u03b1(U)TX+ \u03b2n\nTZn + \u03b5,\nwhere Y is a response variable and (U,XT ,Zn\nT ) is the vector of associated covariates,\nwith X being a q dimensional and Zn being a pn dimensional vector. The error term \u03b5\nhas mean 0 and unknown variance \u03c32. This is a special case of the GVCPLM where in\nequation (3), g is the identity link and Q is the log-likelihood of normal density. However\nit is only used to motivate our procedure.\nLet {(Ui,XTi ,ZTni, Yi)}ni=1 be a random sample from (8) above, with the data ordered\naccording to the Ui\u2019s. Under mild conditions, the spacing Ui+1\u2212Ui is OP (1\/n), so that\n\u03b1(Ui+1)\u2212\u03b1(Ui) \u2248 \u03b30 + \u03b31(Ui+1 \u2212 Ui). Using model (8),\nq+1\u2211\nj=1\nwjYi+j\u22121 =\nq+1\u2211\nj=1\nwj\u03b1(Ui+j\u22121)TXi+j\u22121 + \u03b2\nT\nn\nq+1\u2211\nj=1\nwjZn(i+j\u22121) +\nq+1\u2211\nj=1\nwj\u03b5i+j\u22121.\nHere wj depends on i as well, but we drop this subscript for simplicity. If we define\nY \u2217i =\n\u2211q+1\nj=1 wjYi+j\u22121, Z\n\u2217\nni =\n\u2211q+1\nj=1 wjZn(i+j\u22121), \u03b5\n\u2217\ni =\n\u2211q+1\nj=1 wj\u03b5i+j\u22121 and impose the\nconstraint\n\u2211q+1\nj=1 wjXi+j\u22121 = 0, then we can re-write the above equation as\nY \u2217i \u2248 \u03b30T\nq+1\u2211\nr=2\nq+1\u2211\nj=r\nwjXi+j\u22121 + \u03b31T\nq+1\u2211\nr=2\nq+1\u2211\nj=r\nwjXi+j\u22121(Ui+r\u22121 \u2212 Ui+r\u22122) + \u03b2TnZ\u2217i + \u03b5\u2217i ,\nwhich is a linear model with parameter (\u03b30,\u03b31,\u03b2n). In our simulation study in section\n4, we choose i to be 1, 2, \u00b7 \u00b7 \u00b7 , n\u2212 q so that we have exactly (n\u2212 q) \u2018starred\u2019 data points\nand the \u03b5\u2217i \u2019s are dependent in general, but with known dependence structure. So we\ncan perform a weighted least square fit to the starred data to find (\u03b3\u02c60, \u03b3\u02c61, \u03b2\u02c6n). To solve\u2211q+1\nj=1 wjXi+j\u22121 = 0, we need to find the rank r of the matrix (Xi, \u00b7 \u00b7 \u00b7 ,Xi+q), and then\nfix q + 1 \u2212 r of the wj\u2019s so that the rest can be determined uniquely by just solving a\nsystem of linear equations.\nOne concern of the above approximation is the sparsity of the Ui\u2019s, especially in\nthe tail regions. Then OP (1\/n) spacing is not achievable in the tails. In this case\n10\nwe may want to remove these sparse data points first before aggregating with wj\u2019s\nto avoid deterioration of quality for the estimate \u03b2\u02c6n. In section 4, we take U to be\nuniformly distributed over (0, 1) so that sparsity problem can be avoided for the ease of\nour demonstration.\nTo use the differencing idea to obtain an initial estimate of \u03b2n for GVCPLM, we apply\ntransformation of the data. If g is the link function, we use g(Yi) as the transformed\ndata and proceed with the difference-based method as for the VCPLM. Note that for\nsome models like the logistic regression with logit link and Poisson log-linear model,\nadjustments needed to be made in transforming the data. We use g(y) = log\n(\ny+\u03b4\n1\u2212y+\u03b4\n)\nfor the logistic regression and g(y) = log(y + \u03b4) for the Poisson regression. Here \u03b4\nis treated as a smoothing parameter like h in estimating varying coefficients, and the\nchoice of which are discussed in section 3.4.\n3.2 One-step estimation for the nonparametric component\nGiven \u03b2n = \u03b2\n(k)\nn , model (3) becomes purely nonparametric and we estimate the varying\ncoefficients \u03b1\u03b2n(u) by using the one-step local MLE. The one-step estimates are as\nefficient as the fully iterative ones but save considerable computational time. For more\ntheoretical properties, see for example Cai, Fan and Li (2000). We briefly describe the\nmethod here.\nThe local likelihood is as defined in (4), denoted by l\u03b2n(\u03b3, u), where \u03b3 =\n(\na0\nT , \u00b7 \u00b7 \u00b7 , apT\n)T\n.\nGiven an initial estimator \u03b3\u02c60 = \u03b3\u02c60(u0) =\n(\na\u02c60(u0)\nT , \u00b7 \u00b7 \u00b7 , a\u02c6p(u0)T\n)T\n, one step of the\nNewton-Raphson algorithm produces the updated estimator\n\u03b3\u02c6OS = \u03b3\u02c60 \u2212 {\u22072l\u03b2n(\u03b3\u02c60, u0)}\u22121\u2207l\u03b2n(\u03b3\u02c60, u0),\nwhere derivatives are taken with respect to \u03b3. In univariate generalized linear models,\nthe least-squares estimate serves a natural candidate as an initial estimator. We adapt\na variation as described in Cai, Fan and Li (2000), where we first find a sub-grid points\nof all the Ui\u2019s and obtain local MLE \u03b3\u02c6 on the sub-grid points. Then use these estimates\nas initial values for carrying out the one-step local MLE procedure on the rest of the\nUi\u2019s.\nThe matrix \u22072l\u03b2n(\u03b3, u) can be nearly singular for certain Ui, due to possible data\nsparsity in certain local regions, or when bandwidth is too small. We adapt the ridge\nregression approach to overcome this problem. We omit the details here.\n11\n3.3 Fast updating of \u03b2(k)n\nThe profile-kernel approach essentially treats \u03b1\u02c6\u03b2n(u) from step 1 as a function of both\nu and \u03b2n (Lin and Carroll (2006)). Updating of \u03b2\n(k)\nn in step 2 needs the first and second\nderivatives of \u03b1\u02c6\u03b2n(u) with respect to \u03b2n, which can be computationally intensive to\ncalculate. More precisely, denote \u03b1\u02c6\u2032\u03b2n(u) =\n\u2202\u03b1\u02c6\u03b2n (u)\n\u2202\u03b2n\nwhich is a pn by q matrix, \u03b1\n(r)\n\u03b2n\n(u)\nthe rth component of \u03b1\u03b2n(u) and m\u02c6ni(\u03b2n) = \u03b1\u02c6\u03b2n(Ui)\nTXi+Z\nT\nni\u03b2n, we need to calculate\n\u2207Q\u02c6n(\u03b2n) =\nn\u2211\ni=1\nq1(m\u02c6ni(\u03b2n), Yni)(Zni + \u03b1\u02c6\n\u2032\n\u03b2n\n(Ui)Xi),\n\u22072Q\u02c6n(\u03b2n) =\nn\u2211\ni=1\nq2(m\u02c6ni(\u03b2n), Yni)(Zni + \u03b1\u02c6\n\u2032\n\u03b2n\n(Ui)Xi)(Zni + \u03b1\u02c6\n\u2032\n\u03b2n\n(Ui)Xi)\nT\n+\nn\u2211\ni=1\n{\nq1(m\u02c6ni(\u03b2n), Yni)\nq\u2211\nr=1\n\u22022\u03b1\u02c6\n(r)\n\u03b2n\n(Ui)\n\u2202\u03b2n\u2202\u03b2\nT\nn\nXir\n}\n.\n(9)\nThe following lemma shows how to construct a consistent estimator of \u03b1\u2032\u03b2n(u). The\nproof is in the Appendix.\nLemma 5 Under regularity conditions (A)-(G), provided\n\u221a\npn\n(\nh+ 1\u221a\nnh\n)\n= o(1), we\nhave for each \u03b2n \u2208 \u2126n,\n\u03b1\u02c6\u2032\u03b2n(u)\ndef\n= \u2212\n{\nn\u2211\ni=1\nq2(\u03b1\u02c6\u03b2n(u)\nTXi + Z\nT\nni\u03b2n, Yni)ZniX\nT\ni Kh(Ui \u2212 u)\n}\n\u00b7\n{\nn\u2211\ni=1\nq2(\u03b1\u02c6\u03b2n(u)\nTXi + Z\nT\nni\u03b2n, Yni)XiX\nT\ni Kh(Ui \u2212 u)\n}\u22121\nbeing a consistent estimator of \u03b1\u2032\u03b2n(u) which holds uniformly in u \u2208 \u2126.\nIn implementing step 2 of the profile-kernel procedure, the first and second derivatives\nof \u03b1\u02c6 w.r.t. \u03b2n are to be calculated at each Ui, which post a computational challenge\nto the profile-kernel procedure. On the other hand, the backfitting algorithm set\nall such derivatives to zero in equation (9), thus reducing vastly the amount of\ncomputations of each update. See Hu et al (2004) for a comparison of the two methods.\nWe propose a profile-kernel procedure which is \u2018in between\u2019 the full profile-kernel\nprocedure and backfitting, with two major modifications to the full profile-kernel one:\nModifications of step 2 in the proposed profile-kernel procedure\n(I) The second derivatives\n\u22022\u03b1\u02c6\n(r)\n\u03b2n\n(u)\n\u2202\u03b2n\u2202\u03b2\nT\nn\nare set to 0 in equation (9).\n12\n(II) The first derivatives \u03b1\u02c6\u2032\u03b2n(u) are calculated on a sub-grid points of the Ui\u2019s and\nthose on the rest of the Ui\u2019s are approximated by interpolation.\nSince the function q2(\u00b7, \u00b7) < 0 by regularity condition (D), we see that the modified\n\u22072Q\u02c6n(\u03b2n) in equation (9) is negative-definite. This ensures the Newton-Raphson update\nin step 2 of the profile-kernel procedure can be carried out without trouble.\nThe idea behind modification (I) is that, for a neighborhood around the true parame-\nter \u03b2n0 which is small enough, the least favorable curve \u03b1\u03b2n(u) should be approximately\nlinear in \u03b2n. In fact, to estimate such second derivatives, same amount of local data\naround u is needed which has served to estimate the first derivative \u03b1\u2032\u03b2n(u) already, so\nvariability of the resulting estimates of \u03b2n may increase by incorporating the second\nderivatives into the updating procedure.\nFor modification (II), the idea is that \u03b1\u2032\u03b2n(u) is approximately linear in a small neigh-\nborhood of u. The bandwidth h in estimating \u03b1\u03b2n(u) is a natural parameter to define\nwhat is a \u2018small\u2019 neighborhood around u. In this paper where a constant bandwidth h\nis used (see section 3.4), we calculate \u03b1\u2032\u03b2n(u) at the minimum and maximum values of\nUi\u2019s from the data (assuming sparsity of the tail regions is avoided, see section 3.1), as\nwell as calculating such on a grid of values of u with grid width approximately equals\nto h. Then \u03b1\u2032\u03b2n(Ui) for data point Ui is found by interpolating the nearest two points\non the grid. If variable type of bandwidth is used then the grid points can be defined\nalso according to how h varies.\nWith these modifications, the update of \u03b2(k)n is much faster than the original profile-\nkernel procedure.\n3.4 Choice of bandwidth\nAs usual the optimal bandwidth hopt for estimating\u03b1\u03b2n(u) given \u03b2n is of order n\n\u22121\/(2p+3),\nwhich can be seen immediately from equation (18). The equation also gives the order\nof the MSE to be n\u2212(2p+2)\/(2p+3) when such an optimal bandwidth is used. This optimal\nbandwidth order can be used without affecting the asymptotic properties of our estima-\ntor \u03b2\u02c6n, as shown in Theorems 1 and 2. We do not derive explicit expressions for the\ntheoretical optimal bandwidth and MSE here.\nAs mentioned at the end of section 3.1, we have an extra smoothing parameter \u03b4\nto be determined due to adjustments to transformation of the response Yni. This two\ndimensional smoothing parameter (\u03b4, h) can be found by doing a K-fold cross-validation.\nSince we have suggested a quick profile-kernel procedure and practical accuracy can be\nachieved in several iterations as demonstrated in section 4, for K not too large (e.g.\nK=5 or 10) the cross-validation procedure is not too computationally intensive.\n13\n4 Simulation Study\nIn this section we first demonstrate how our proposed iterative procedure saves compu-\ntational time as well as being stabler over the fully iterative procedure. Then using our\niterative procedure, we demonstrate the finite sample performance of our estimates and\naugment our theoretical results.\nTo evaluate the performance of estimator \u03b1\u02c6(\u00b7), we use the square-root of average\nerrors (RASE)\nRASE =\n{\nn\u22121grid\nngrid\u2211\nk=1\n\u2016\u03b1\u02c6(uk)\u2212\u03b1(uk)\u20162\n}1\/2\n,\nwhere {uk, k = 1, \u00b7 \u00b7 \u00b7 , ngrid} are the grid points at which the function \u03b1\u02c6(\u00b7) is evaluated.\nThe Epanechnikov kernel K(u) = 0.75(1\u2212 u2)+ and ngrid = 200 are used in our simula-\ntion. For assessing the performance of the estimator \u03b2\u02c6n, we use the generalized mean\nsquare error (GMSE)\nGMSE = (\u03b2\u02c6n \u2212 \u03b2n0)TEZ\u2217Z\u2217T (\u03b2\u02c6n \u2212 \u03b2n0),\nwhere Z\u2217 is a new realization of the random variable Z.\nSimulation 1. In this simulation, we consider a semi-varying Poisson regression model.\nThe response Y , given (U,X,Zn), has a Poisson distribution with mean function \u00b5(U,X,Zn)\nwhere\nlog(\u00b5(U,X,Zn)) = X\nT\u03b1(U) + ZTn\u03b2n.\nWe simulate 50 samples of sizes 200 and 400 with pn = b1.8n1\/3c from the above model,\nmeaning p200 = 10 and p400 = 13. For the covariates, we take U \u223c U(0, 1),X =\n(X1, X2)\nT with X1 \u2261 1 and X2 \u223c N(0, 1) such that (ZTn , X2)T is a (pn+1)\u2212dimensional\nnormal distribution with mean zero and covariance matrix (\u03c3ij), where \u03c3ij = 0.5\n|i\u2212j|.\nFor the parameters of the model, \u03b2n0 = (0.5, 0.3,\u22120.5, 1, 0.1,\u22120.25, 0, \u00b7 \u00b7 \u00b7 , 0)T which is\npn\u2212dimensional, \u03b1(u) = (\u03b11(u), \u03b12(u))T where\n\u03b11(u) = 4 + sin(2piu), and \u03b12(u) = 2u(1\u2212 u).\nUsing a 5-fold cross-validation (CV), we calculate 4-cycles estimates using our proposed\nprofile-kernel procedure in order to obtain the CV value. We finally chose \u03b4 = 0.1 and\nh = 0.1, 0.08 for n = 200, 400 respectively.\nThe median GMSE and respective computing times of \u03b2\u02c6n among the 4-cycles esti-\nmators of backfitting, the proposed and full profile-kernel procedures are summarized\nin table 1. The SDmad is a robust estimate of standard deviation and is defined by\n14\nTable 1: Simulation results of different fitting schemes for Poisson model\nMedian(SDmad) GMSE (multiplied by 10000)\nbackfitting profile-kernel, profile-kernel,\nn pn proposed full\n200 10 10.72(6.47) 5.45(2.71) 9.74(14.67)\n400 13 5.63(4.39) 2.78(1.19) 5.26(9.46)\nMedian(SDmad) of computing times in seconds\n200 10 0.6(0.0) 0.7(0.0) 77.2(0.2)\n400 13 0.8(0.0) 1.4(0.0) 463.2(0.9)\nRelative Median RASE (%)\n200 10 84.8 97.0 89.5\n400 13 85.6 98.6 88.2\ninterquartile range divided by 1.349. We see that the proposed profile-kernel procedure\nhas the smallest GMSE. The full profile-kernel procedure performs only slightly better\nthan backfitting, but with much greater variability in the GMSE. In terms of comput-\ning times, backfitting wins against our proposed procedure slightly, but at the price\nof doubling the GMSE on average. Hence the proposed profile-kernel procedure gains\nthe best trade-off between computational cost and accuracy. Comparing with the full\nprofile-kernel procedure, it saves a vast amount of computations as well on average, and\nthe savings grows as n increases. We also know (not shown in the table) that on average\nbackfitting needs more than 20 iterations to converge without improving the GMSE too\nmuch. For a logistic data simulation (not shown here), our proposed procedure is still\nbetter than backfitting in terms of accuracy, but not as large an improvement as in the\nPoisson case.\nThe relative median RASE in table 1 is defined as RASE0\/RASE1, where RASE0\nis the RASE calculated from the fit with true value of \u03b2n known in advance (oracle\nestimate), and RASE1 is the RASE calculated from different procedures. Clearly our\nproposed procedure is closest to the oracle estimate on average.\nSimulation 2. In this simulation 400 samples of sizes 200, 400, 800 and 1500 with\npn = b1.8n1\/3c are drawn from the Poisson model introduced in simulation 1. Estimators\n\u03b2\u02c6n and \u03b1\u02c6\u03b2\u02c6n(u) are obtained by the proposed profile-kernel procedure, but with variants:\nOS Our proposed profile-kernel procedure, iterated until convergence.\nFS Same, except that we don\u2019t use the One-step procedure as in Cai, Fan and Li (2000)\nto estimate the nonparametric component, but by iterating Newton-Raphson al-\ngorithm until convergence.\n15\nDBE The difference-based estimation, same as one-cycle estimate.\n4C The four-cycles estimate.\nWe compare median GMSE of the above procedures in table 2. The OS, 4C and\nFS procedures perform as good as each other, meaning that the one-step updating of\nnonparametric component works well and our proposed procedure converges early. In\nfact (not shown in the table) the two-cycles estimates improve the DBE dramatically\nalready.\nWe summarized the effect of bandwidth choice and practical accuracy of estimated\nparameters (two-cycles) in table 3. We denote hCV the choice of our bandwidth for\nthe nonparametric component. It is clear that the GMSE does not sensitively depends\non the bandwidth on average, as long as it is close to hCV. The right column of the\ntable shows the estimate for \u03b25. Being close to the true parameter value at different\nbandwidth choices with small variability (estimates of other \u03b2i\u2019s are performing well\nsimilarly, and are not shown), the two-cycles estimate works well.\nTo test the accuracy of the sandwich covariance formula, the standard deviations of\nthe estimated coefficients (two-cycles esimates) are computed among the 400 simulations\nat hCV. These can be regarded as the true standard errors (columns labeled SD in table\n4), and the 400 estimated standard errors are summarized by their median (columns\nSDm) and the associated SDmad (interquartile range divided by 1.349). Note that we\nhave multiplied all values by 1000 for compact presentation. Clearly the sandwich\nformula does a good job, and accuracy gets better as n increases.\nFinally we want to examine if the GLRT in section 2.2 performs well in testing a\nlinear hypothesis on \u03b2n. To this end, we consider the following null hypothesis:\nH0 : \u03b27 = \u03b28 = \u00b7 \u00b7 \u00b7 = \u03b2pn = 0,\nwhere we still have pn = b1.8n1\/3c. The alternative hypothesis is indexed by a parameter\nTable 2: Simulation results for variants of profile-kernel procedures\nRelative Median GMSE (%)\nPoisson Logistic\nn pn FS\/OS FS\/DBE FS\/4C FS\/OS FS\/DBE FS\/4C\n200 10 100.0 8.2 99.9 99.8 64.1 101.7\n400 13 100.2 6.0 100.2 99.9 52.7 104.7\n800 16 100.1 5.0 100.1 100.0 50.9 102.6\n1500 20 100.0 4.2 100.0 100.0 46.4 100.5\n16\nTable 3: Summary statistics of two-cycles estimate\nPoisson Logistic\nMedian(SDmad) \u03b2\u02c65 Median(SDmad) \u03b2\u02c65\nGMSE\u00d7105 mean(SD)\u00d7104 GMSE\u00d710 mean(SD)\nn pn hCV 1.5hCV 0.66hCV hCV 0.66hCV hCV hCV 1.5hCV\n200 10 5.9(3.0) 6.4(3.3) 993(112) 995(105) 8.2(4.4) 8.4(5.1) 1.78(.40) 1.59(.37)\n400 13 3.1(1.4) 3.0(1.4) 1004(67) 1001(65) 4.8(2.2) 5.4(2.5) 1.81(.26) 1.64(.27)\n800 16 1.7(0.7) 1.7(0.6) 999(47) 999(46) 2.7(1.0) 2.7(1.1) 1.94(.20) 1.85(.19)\n1500 20 1.1(0.3) 1.1(0.4) 1000(32) 1000(32) 1.8(0.7) 1.8(0.6) 1.97(.15) 1.91(.14)\n\u03b4 as follows:\nH1 : \u03b27 = \u03b28 = \u03b4, \u03b2j = 0 for j > 8.\nWhen \u03b4 = 0, the alternative collapses to the null hypothesis. The GLRT statistic is\ncomputed for each simulation using the two-cycles estimates. Corresponding to \u03b4 = 0,\nthe kernel density estimate of the finite sample null distribution of these statistics is\ncompared to the proposed asymptotic chi-squared density with d.f.= pn\u22126. Figure 1(a)\nshows the comparison when n = 400. The finite sample null density is seen to be close\nto the theoretical asymptotic chi-squared density.\nTo see the power of the test, we increases \u03b4 in the alternative H1 and calculate the\nGLRT statistic in each simulation based on two-cycles estimates again. Three power\nfunctions are calculated corresponding to three different significance levels: 0.1, 0.05\nand 0.01, using the theoretical chi-squared distribution to find the corresponding critical\nregion. The proportion of rejection among the 400 statistics is the simulated power. We\nsee from figure 1(b) that the upper two power curves are of slightly higher significance\nlevels (corresponds to \u03b4 = 0) than the theoretical significance levels 0.1 and 0.05. This\nsuggests slightly thicker tail regions in the null density as seen also in figure 1(a). The\npower curves increase rapidly with \u03b4, showing that the GLRT performs well.\nTable 4: Standard deviations and estimated standard errors\nPoisson, values\u00d71000 Logistic, values\u00d710\n\u03b2\u02c61 \u03b2\u02c63 \u03b2\u02c62 \u03b2\u02c64\nSDm SDm SDm SDm\nn pn SD (SDmad) SD (SDmad) SD (SDmad) SD (SDmad)\n200 10 9.1 8.5(1.3) 9.9 9.4(1.3) 3.6 2.9(.4) 3.2 2.8(.4)\n400 13 6.0 5.6(0.7) 6.5 6.1(0.7) 2.3 2.1(.2) 2.2 2.0(.2)\n800 16 3.7 3.8(0.3) 4.1 4.2(0.4) 1.7 1.6(.1) 1.5 1.5(.1)\n1500 20 2.8 2.7(0.2) 3.1 3.0(0.2) 1.2 1.2(.1) 1.1 1.1(.1)\n17\nFigure 1: Plots for simulation 2 and 3. (a) and (b) are plots for the Poisson GVCPLM\nwhile (c) and (d) are plots for the Logistic GVCPLM. In (a) and (c), dotted lines are\nthe estimated null densities and the solid lines are \u03c72\u2212densities with d.f.=pn \u2212 6. (7\nand 10 resp.) (b) and (d) are power functions of GLRT.\nSimulation 3. In this simulation, we consider a semi-varying Logistic regression model.\nThe response Y , given (U,X,Zn), has a Bernoulli distribution with success probability\np(U,X,Zn) where\np(U,X,Zn)) = exp{XT\u03b1(U) + ZTn\u03b2n}\/[1 + exp{XT\u03b1(U) + ZTn\u03b2n}].\nSame as simulation 1, we simulate 400 samples of sizes 200, 400, 800 and 1500 with\npn = b1.8n1\/3c from the above model. For the covariates, we take U \u223c U(0, 1),X =\n(X1, X2)\nT with X1 \u2261 1 and X2 \u223c N(0, 1), and Zn is a pn\u2212dimensional normal distri-\nbution with mean zero and covariance matrix (\u03c3ij), where \u03c3ij = 0.5\n|i\u2212j|. For the pa-\nrameters of the model, \u03b2n0 = (3, 1,\u22122, 0.5, 2,\u22122, 0, \u00b7 \u00b7 \u00b7 , 0)T which is pn\u2212dimensional,\n\u03b1(u) = (\u03b11(u), \u03b12(u))\nT where\n\u03b11(u) = 2(u\n3 + 2u2 \u2212 2u), and \u03b12(u) = 2 cos(2piu).\nBandwidth (\u03b4, h) is chosen by a 5-fold CV, where \u03b4 appears in the transformation\nof data y \u2192 log\n(\ny+\u03b4\n1\u2212y+\u03b4\n)\n. We finally chose \u03b4 = 0.005 and h = 0.45, 0.4, 0.25 and 0.18,\ncorresponding to n = 200, 400, 800 and 1500.\n18\nWe compare median GMSE of the above procedures on the right of the table 2.\nThe OS and FS procedures perform similar to each other, meaning that the one-step\nupdating of nonparametric component works fine. The FS\/DBE column shows that,\nunlike in the Poisson regression case, one update of the initial estimate \u03b2(0)n does not\ndecrease the GMSE by a very large proportion.\nSimilar to the Poisson case, the right side of table 3 shows that sensitivity of estimates\nto bandwidth choice is not high. We also see a good accuracy of the sandwich covariance\nformula from table 4.\nTo examine the performance of the GLRT for the Logistic GVCPLM we use the\nsame null and alternative hypotheses as defined in simulation 2. The estimated null\ndensity is close to the theoretical \u03c72 density in figure 1(c) and the GLRT works well as\nseen from figure 1(d).\nReal data example. We used Example 11.3 and the accompanying data set of Al-\nbright, Winston and Zappe (1999), where the Fifth National Bank of Springfield faced\na gender discrimination suit in which female received substantially smaller salaries than\nmale employees. (This example is based on a real case with data dated 1995. Only\nthe bank\u2019s name is changed.) Fan and Peng (2004) has done such a salary analysis\nusing an additive model with quadratic spline, and did not find a significant evidence of\ngender discrimination. We focus on another question: whether it was harder for female\nemployees to be promoted.\nThe data set consists of 208 employees which include the following variables:\n\u2022 EduLev: educational level, a categorical variable with categories 1 (finished school),\n2 (finished some college courses), 3 (obtained a bachelor\u2019s degree), 4 (took some\ngraduate courses), 5 (obtained a graduate degree).\n\u2022 JobGrade: a categorical variable indicating the current job level, the possible levels\nbeing 1\u20136 (6 highest).\n\u2022 YrHired: year that an employee was hired.\n\u2022 YrBorn: year that an employee was born.\n\u2022 Gender: a categorical variable with values \u2018Female\u2019 and \u2018Male\u2019.\n\u2022 YrsPrior: number of years of working experience at another bank prior to working\nat the Fifth National Bank.\n\u2022 PCJob: a dummy variable with value 1 if the employee\u2019s current job is computer\nrelated and value 0 otherwise.\n19\nTable 5: Fitted coefficients (sandwich SD) for model (10)\nResponse Female PCJob Edu1 Edu2 Edu3 Edu4\nHighGrade4 -1.66(.50) -0.11(.71) -4.32(.68) -4.12(.80) -2.33(.45) -2.44(.89)\nHighGrade5 -1.66(.50) -1.25(.50) -3.86(.52) -3.92(.59) -2.41(.59) -0.95(.98)\nWe use JobGrade as the response variable and Gender as one of the covariates.\nThe aim is to find if the Gender variable, after controlling for other factors such as\neducational level and years of prior experience, is significant in explaining JobGrade.\nWe want to fit as large a model as possible to reduce modelling bias, and our theories\nallow us to interpret the model as usual. To simplify analysis, we create a response\nvariable HighGrade4 which is 0 if JobGrade is less than 4 and 1 otherwise. We\ncan then fit a logistic regression or a logistic GVCPLM to the data and then carry out\na GLRT to test the gender effect. From figure 2(a), the correlation between Age and\nTotalYrsExp (the total years of relevant working experience, calculated from YrHired\nand YrsPrior) is high, we use the following logistic GVCPLM\nlog\n(\npH\n1\u2212 pH\n)\n= \u03b11(Age) + \u03b12(Age)TotalYrsExp\n+ \u03b21Female + \u03b22PCJob +\n4\u2211\ni=1\n\u03b22+iEdui\n(10)\nto reduce modelling bias, where pH is the probability of having a job grade 4 or above.\nInteraction terms such as that between Female and Edui are considered, but tested\nnon-significant with GLRT so that we do not include those terms in the model above.\n(Including interaction terms increases the number of linear parameters, but theorem 3\nstill applies.) We use a 20-fold CV and find hCV = 23.5, \u03b4CV = 0.1.\nTable 5 shows the results of the fit. (Two-cycles estimates using our proposed profile-\nkernel procedure.) It has a negative coefficient for Female and appears statistically\nsignificant since the estimated sandwich SD is small. Figure 2(b) shows the standardized\nresiduals (y \u2212 p\u02c6H)\/\n\u221a\np\u02c6H(1\u2212 p\u02c6H) against Age and the fit seems reasonable. (Other\ndiagnostic plots are not shown.) From figure 2(c), we see that as age increases one has a\nbetter chance of being in a higher job grade. Figure 2(d) shows that the marginal effect\nof working experience is large when age is around 30 or less, but start to fall as one gets\nolder.\nWe have done another fit using a binary variable HighGrade5 which is similar to\nHighGrade4 but is 0 only when job grade is less than 5. The coefficients are shown in\ntable 5 and the Female coefficient is very close to the first fit.\n20\nFigure 2: (a) and (b):TotalYrsExpand standardized residuals against Age. (c) and (d):\nVarying coefficients for the logistic GVCPLM for the data.\nFormally, we are testing\nH0 : \u03b21 = 0\u2190\u2192 H1 : \u03b21 < 0.\nTable 6 shows significant test results no matter we are using HighGrade4 or High-\nGrade5 as the response. Not shown in this paper, we have done the test again after\ndeleting 6 data points corresponding to 5 male executives and 1 female having many\nyears of working experience and high salaries. The test results are still similar. In fact\nfrom the raw data, female staffs are usually having a lower job grade than male with\nsimilar profile of educational level, working experience and age, even their salaries dif-\nference may not be apparent. The test results support that female staff of the Fifth\nNational Bank of Springfield is harder to be promoted to a higher job grade than male.\nTable 6: Generalized likelihood ratio test for \u03b21 = 0\nResponse \u03c72-statistic P-value\nHighGrade4 13.8095 0.0002\nHighGrade5 11.3544 0.0008\n21\n5 Technical Proofs.\nIn this section proofs of Theorems 1-4 will be given. We introduce some notations and\nregularity conditions for our results to hold. In the following and thereafter, the symbol\n\u2297 represents the Kronecker product between matrices, and \u03bbmin(A), \u03bbmax(A) denotes\nrespectively the minimum and maximum eigenvalues of the matrix A.\nDenote the true linear parameter by \u03b2n0, with parameter space \u2126n \u2282 Rpn . Let\n\u03c1l(t) = (dg\n\u22121(t)\/dt)l \/V (g\u22121(t), mni(\u03b2n) = \u03b1\u03b2n(Ui)\nTXi + \u03b2\nT\nnZni,\n\u00b5k =\n\u222b\nukK(u)du, Ap(X) = (\u00b5i+j)0\u2264i,j\u2264p \u2297XXT , \u03b1\u2032\u03b2n(u) =\n\u2202\u03b1\u03b2n (u)\n\u2202\u03b2n\n,\n\u03b1\n(r)\u2032\u2032\n\u03b2n\n(u) =\n\u22022\u03b1\n(r)\n\u03b2n\n(u)\n\u2202\u03b2n\u2202\u03b2\nT\nn\nand ql(x, y) =\ndl\ndxl\nQ(g\u22121(x), y) for l = 1, \u00b7 \u00b7 \u00b7 , 4.\nRegularity Conditions:\n(A) |(Zn)j| , \u2016X\u2016 , are OP (1) and\n\u2225\u2225\u2225\u2202\u03b1\u03b2n (u)\u2202\u03b2nj \u2225\u2225\u2225 ,\u2225\u2225\u2225\u22022\u03b1\u03b2n (u)\u2202\u03b2nj\u2202\u03b2nk \u2225\u2225\u2225 and \u2225\u2225\u2225 \u22023\u03b1\u03b2n (u)\u2202\u03b2nj\u2202\u03b2nk\u2202\u03b2nl\u2225\u2225\u2225 are finite,\nj, k, l = 1, \u00b7 \u00b7 \u00b7 , pn.\n(B) In(\u03b2n0) = E0\n[\u2207Qn1(\u03b2n0)\u2207TQn1(\u03b2n0)]\n= E0\n{\nq21(mn1(\u03b2n0), Yn1)(Zn1 +\u03b1\n\u2032\n\u03b2n0\n(U1)X1)(Zn1 +\u03b1\n\u2032\n\u03b2n0\n(U1)X1)\nT\n}\nsatisfies the condition\n0 < C1 < \u03bbmin {In(\u03b2n0)} \u2264 \u03bbmax {In(\u03b2n0)} < C2 <\u221e for all n.\n(C) E\u03b2n\n\u2223\u2223\u2223 \u2202l+jQni(\u03b2n)\u2202j\u03b1\u2202\u03b2nk1 \u00b7\u00b7\u00b7\u2202\u03b2nkl \u2223\u2223\u2223 \u2264 Cl < \u221e, E\u03b2n \u2223\u2223\u2223 \u2202l+jQni(\u03b2n)\u2202j\u03b1\u2202\u03b2nk1 \u00b7\u00b7\u00b7\u2202\u03b2nkl \u2223\u2223\u22232 \u2264 C\u02dcl < \u221e for some con-\nstants Cl, C\u02dcl and for all n, with l = 1, \u00b7 \u00b7 \u00b7 , 4 and j = 0, 1.\n(D) The function q2(x, y) < 0 for x \u2208 R and y in the range of the response variable,\nand E0 {q2(mn1(\u03b2n), Yn1)Ap(X1)|U = u} is invertible.\n(E) The functions V \u2032\u2032(\u00b7) and g\u2032\u2032\u2032(\u00b7) are continuous. The varying coefficient \u03b1\u03b2n(u) is\nthree times continuously differentiable in \u03b2n and u.\n(F) The random variable U has a compact support \u2126. The density function fU(u) of\nU has a continuous second derivative and is uniformly bounded away from zero.\n(G) The kernel K is a bounded symmetric density function with bounded support.\nNote the above conditions are assumed to hold uniformly in u \u2208 \u2126. Condition\n(D) ensures a unique solution in the local likelihood (4). Condition (B) and (C) are\n22\nuniformity conditions on higher-order moments of the likelihood functions. They are\nstronger than those of the usual asymptotic likelihood theory, but they facilitate techni-\ncal proofs. Condition (G) is imposed just for the simplicity of proofs. It can be relaxed\nat the expense of longer proofs.\nBefore proving Theorem 1, we need two important lemmas concerning order approx-\nimations to the varying coefficients. Let cn = (nh)\n\u22121\/2, \u03b1(p)u\u03b2n(u) =\n\u2202p\u03b1\u03b2n (u)\n\u2202up\n. Define the\nfollowing:\n\u03b1\u00afni(u) = X\nT\ni\n(\np\u2211\nk=0\n(Ui \u2212 u)k\nk!\n\u03b1\n(k)\nu\u03b2n\n(u)\n)\n+ \u03b2TnZni,\n\u03b2\u02c6\n\u2217\n= c\u22121n\n(\n(a\u02c60\u03b2n \u2212\u03b1\u03b2n(u))T , h(a\u02c61\u03b2n \u2212\u03b1\u2032u\u03b2n(u))T , \u00b7 \u00b7 \u00b7 ,\nhp\np!\n(a\u02c6p\u03b2n \u2212\u03b1(p)u\u03b2n(u))T\n)T\n,\nX\u2217i =\n(\n1,\nUi \u2212 u\nh\n, \u00b7 \u00b7 \u00b7 ,\n(\nUi \u2212 u\nh\n)p)T\n\u2297Xi.\nLemma 6 Under regularity conditions (A) - (G), for each \u03b2n \u2208 \u2126n, the following holds\nuniformly in u \u2208 \u2126: \u2225\u2225a\u02c60\u03b2n(u)\u2212\u03b1\u03b2n(u)\u2225\u2225 = OP (hp+1 + 1\u221a\nnh\n).\nLikewise, the norm of the kth derivative of the above with respect to any \u03b2nj\u2019s, k =\n1, \u00b7 \u00b7 \u00b7 , 4, all have the same order uniformly in u \u2208 \u2126.\nProof of lemma 6. Our first step is to show that, uniform in u \u2208 \u2126,\n\u03b2\u02c6\n\u2217\n= A\u02dc\u22121n Wn +OP (h\np+1 + cn log\n1\/2(1\/h)),\nwhere\nA\u02dcn = fU(u)E0\n{\n\u03c12(\u03b1\u03b2n(U)\nTX+ ZTn\u03b2n)Ap(X)|U = u\n}\n,\nWn = hcn\nn\u2211\ni=1\nq1(\u03b1\u00afni, Yni)X\n\u2217\niKh(Ui \u2212 u),\nAn = hc\n2\nn\nn\u2211\ni=1\nq2(\u03b1\u00afni, Yni)X\n\u2217\niX\n\u2217T\ni Kh(Ui \u2212 u).\nSince expression (4) is maximized at (a\u02c60\u03b2n , \u00b7 \u00b7 \u00b7 , a\u02c6p\u03b2n)T , \u03b2\u02c6\n\u2217\nmaximizes\n23\nln(\u03b2\n\u2217) = h\nn\u2211\ni=1\n{\nQ(g\u22121(cnX\u2217Ti \u03b2\n\u2217 + \u03b1\u00afni), Yni)\u2212Q(g\u22121(\u03b1\u00afni), Yni)\n}\n=WTn\u03b2\n\u2217 +\n1\n2\n\u03b2\u2217TAn\u03b2\n\u2217 +\nhc3n\n6\nn\u2211\ni=1\nq3(\u03b7i, yni)(X\n\u2217T\ni \u03b2\n\u2217)3Kh(Ui \u2212 u),\nwhere \u03b7i lies between \u03b1\u00afni and \u03b1\u00afni + cnX\n\u2217T\ni \u03b2\n\u2217. The concavity of ln(\u03b2\n\u2217) is ensured by\ncondition (D). Note that K(\u00b7) is bounded, so under condition (C) the third term on the\nright hand side is bounded by\nOP (nhc\n3\nnE|q3(\u03b71, Yn1)\u2016X1\u20163Kh(U1 \u2212 u)| = OP (cn) = oP (1).\nDirect calculation yields\nE0An = \u2212A\u02dcn + o(1),\nVar0((An)ij) = O((nh)\n\u22121),\nso that mean-variance decomposition yields\nAn = \u2212A\u02dcn + oP (1).\nHence we have\n(11) ln(\u03b2\n\u2217) =WTn\u03b2\n\u2217 \u2212 1\n2\n\u03b2\u2217T A\u02dcn\u03b2\n\u2217 + oP (1).\nNote that An is a sum of i.i.d. random variables of kernel form, by lemma (A.2),\n(12) An = \u2212A\u02dcn + oP (1) +OP\n{\nhp+1 + cnlog\n1\/2(1\/h)\n}\nuniformly in u \u2208 \u2126. Hence by the Convexity lemma (Pollard, 1991), equation (11) also\nholds uniformly in \u03b2\u2217 \u2208 C for any compact set C. Lemma A.1 then yields\n(13) sup\nu\u2208\u2126\n|\u03b2\u02c6\u2217 \u2212 A\u02dc\u22121n Wn| P\u2212\u2192 0.\nFurthermore, by the definition of \u03b2\u02c6\n\u2217\n,\n(14)\n\u2202\n\u2202\u03b2\u2217\nln(\u03b2\n\u2217)|\u03b2\u2217=\u03b2\u02c6\u2217 = hcn\nn\u2211\ni=1\nq1(\u03b1\u00afni + cnX\n\u2217T\ni \u03b2\u02c6\n\u2217\n, Yni)X\n\u2217\niKh(Ui \u2212 u) = 0.\n24\nExpanding q1(\u03b1\u00afni + cnX\n\u2217T\ni \u03b2\u02c6\n\u2217\n, \u00b7) at \u03b1\u00afni,\n(15) Wn +An\u03b2\u02c6\n\u2217\n+\nhc3n\n2\nn\u2211\ni=1\nq3(\u03b1\u00afni + \u03b6\u02c6i, Yni)X\n\u2217\ni (X\n\u2217T\ni \u03b2\u02c6\n\u2217\n)2Kh(Ui \u2212 u) = 0\nwhere \u03b6\u02c6i lies between 0 and cnX\n\u2217T\ni \u03b2\u02c6\n\u2217\n. Using condition (C), the last term has order\nOP (c\n3\nnhn\u2016\u03b2\u02c6\n\u2217\u20162) = OP (cn\u2016\u03b2\u02c6\u2217\u20162). By (13), we know that \u2016\u03b2\u02c6\u2217\u2016 \u2264 oP (1) + \u2016A\u02dc\u22121n Wn\u2016 \u2264\noP (1) +O(1) \u00b7 \u2016Wn\u2016. Note that by direct calculation,\nE0Wn =\n\u221a\nnhhp+1\n(p+ 1)!\n\u03b1\n(p+1)\nu\u03b2n\n(u)T\n\u00d7 E0\n{\n\u03c12(\u03b1\u03b2n(U)\nTX+ ZTn\u03b2n)X(\u00b5p+1, \u00b7 \u00b7 \u00b7 , \u00b52p+1)T \u2297X|U = u\n}\n+ o(c\u22121n h\np+1),\nVar0Wn = O(1),\n(16)\nand hence \u2016Wn\u2016 = OP (1 + c\u22121n hp+1) which implies OP (cn\u2016\u03b2\u02c6\n\u2217\u20162) = oP (1). With this,\ncombining (12) and (15), we obtain\nWn \u2212 A\u02dcn\u03b2\u02c6\u2217\n[\n1 +OP\n{\nhp+1 + cnlog\n1\/2(1\/h)\n}]\n+ oP (1) = 0.\nHence,\n(17) \u03b2\u02c6\n\u2217\n= A\u02dc\u22121n Wn +OP (h\np+1 + cn log\n1\/2(1\/h))\nholds uniformly for u \u2208 \u2126 by (13). As a direct consequence, by using (16),\n(18)\n\u2225\u2225a\u02c60\u03b2n(u)\u2212\u03b1\u03b2n(u)\u2225\u2225 = OP (hp+1 + 1\u221a\nnh\n)\nwhich holds uniformly for u \u2208 \u2126.\nDifferentiate both sides of (14) w.r.t. \u03b2nj,\n(19) hcn\nn\u2211\ni=1\nq2(\u03b1\u00afni + cnX\n\u2217T\ni \u03b2\u02c6\n\u2217\n, Yni)\n\uf8eb\uf8ed\u2202\u03b1\u00afni\n\u2202\u03b2nj\n+ cn\n(\n\u2202\u03b2\u02c6\n\u2217\n\u2202\u03b2nj\n)T\nX\u2217i\n\uf8f6\uf8f8X\u2217iKh(Ui \u2212 u) = 0,\nwhich holds for all u \u2208 \u2126. By Taylor\u2019s expansion and similar treatments to (15),\nW1n +W\n2\nn + (An +B\n1\nn +B\n2\nn)\n\u2202\u03b2\u02c6\n\u2217\n\u2202\u03b2nj\n+OP (cn\u2016\u03b2\u02c6\u2217\u20162),\n25\nwhere\nW1n = hcn\nn\u2211\ni=1\nq2(\u03b1\u00afni, Yni)\n\u2202\u03b1\u00afni\n\u2202\u03b2nj\nX\u2217iKh(Ui \u2212 u),\nW2n = hcn\nn\u2211\ni=1\nq3(\u03b1\u00afni, Yni)cnX\n\u2217T\ni \u03b2\u02c6\n\u2217\u2202\u03b1\u00afni\n\u2202\u03b2nj\nX\u2217iKh(Ui \u2212 u),\nB1n = hc\n2\nn\nn\u2211\ni=1\nq3(\u03b1\u00afni, Yni)cnX\n\u2217T\ni \u03b2\u02c6\n\u2217\nX\u2217iX\n\u2217T\ni Kh(Ui \u2212 u),\nB2n =\n1\n2\nhc2n\nn\u2211\ni=1\nq4(\u03b1\u00afni + \u03b6\u02c6i, Yni)(c\n2\nnX\n\u2217T\ni \u03b2\u02c6\n\u2217\n)2X\u2217iX\n\u2217T\ni Kh(Ui \u2212 u),\nwith \u03b6\u02c6i lies between 0 and cnX\n\u2217T\ni \u03b2\u02c6\n\u2217\n. The equation holds for all u \u2208 \u2126. Note that\nOP (cn\u2016\u03b2\u02c6\u2217\u20162) = oP (1) uniformly for u \u2208 \u2126 by (13). The order of W2n is smaller than\nthat of W1n, and the order of B\n1\nn and B\n2\nn are smaller than that of An. Hence\n\u2202\u03b2\u02c6\n\u2217\n\u2202\u03b2nj\n= A\u02dc\u22121n W\n1\nn + oP (1 + c\n\u22121\nn h\np+1)\nuniformly in u \u2208 \u2126, by noting that\nE0W\n1\nn =\n\u2202\n\u2202\u03b2nj\nE0Wn + o(c\n\u22121\nn h\np+1),\nVar0W\n1\nn = O(1).\nFrom this, for j = 1, \u00b7 \u00b7 \u00b7 , pn, we have\n(20)\n\u2225\u2225\u2225\u2225\u2202a\u02c60\u03b2n(u)\u2202\u03b2nj \u2212 \u2202\u03b1\u03b2n(u)\u2202\u03b2nj\n\u2225\u2225\u2225\u2225 = OP (hp+1 + 1\u221anh).\nuniformly in u \u2208 \u2126. Differentiating (14) again w.r.t. \u03b2nk and so on, and follow similar\narguments as above, we get results for higher order derivatives. \u0003\nLemma 7 Under regularity conditions (A) - (G), the following holds uniformly in u \u2208\n\u2126: \u2225\u2225a\u02c60\u03b2n(u)\u2225\u2225 = OP (1).\nLikewise, the norm of the kth derivative of the above with respect to any \u03b2nj\u2019s, k =\n1, \u00b7 \u00b7 \u00b7 , 4, all have order O(1) uniformly in u \u2208 \u2126.\nProof of lemma 7. It follows immediately from lemma 6 and condition (A). \u0003\nProof of Theorem 1. Let \u03b3n =\n\u221a\npn\/n. Our aim is to show that, for a given \u000f > 0,\n26\n(21) P\n{\nsup\n\u2016v\u2016=C\nQ\u02c6n(\u03b2n0 + \u03b3nv) < Q\u02c6n(\u03b2n0)\n}\n\u2265 1\u2212 \u000f,\nso that this implies with probability tending to 1 there is a local maximum \u03b2\u02c6n in the\nball {\u03b2n0 + \u03b3nv : \u2016v\u2016 \u2264 C} such that \u2016\u03b2\u02c6n \u2212 \u03b2n0\u2016 = OP (\u03b3n).\nBy Taylor\u2019s expansion,\nDn(v) := Q\u02c6n(\u03b2n0 + \u03b3nv)\u2212 Q\u02c6n(\u03b2n0)\n= \u2207T Q\u02c6n(\u03b2n0)v\u03b3n +\n1\n2\nvT\u22072Q\u02c6n(\u03b2n0)v\u03b32n +\n1\n6\n\u2207T (vT\u22072Q\u02c6n(\u03b2\u2217n)v)v\u03b33n\n:= I\u02c61 + I\u02c62 + I\u02c63,\nwhere \u03b2\u2217n lies between \u03b2n0 and \u03b2n0 + \u03b3nv, and \u2016v\u2016 = C with C a large constant.\nConsider\nI\u02c61 =\nn\u2211\ni=1\nq1(m\u02c6ni(\u03b2n0), Yni)(Zni + \u03b1\u02c6\n\u2032\n\u03b2n0\n(Ui)Xi)\nTv\u03b3n\n=\nn\u2211\ni=1\nq1(m\u02c6ni(\u03b2n0), Yni)(Zni +\u03b1\n\u2032\n\u03b2n0\n(Ui)Xi)\nTv\u03b3n\n+\nn\u2211\ni=1\nq1(m\u02c6ni(\u03b2n0), Yni)X\nT\ni (\u03b1\u02c6\n\u2032\n\u03b2n0\n(Ui)\u2212\u03b1\u2032\u03b2n0(Ui))Tv\u03b3n,\n:= D1 +D2\nwhere m\u02c6ni(\u03b2n) = \u03b1\u02c6\u03b2n(Ui)\nTXi+\u03b2\nT\nnZni. D2 has order smaller than D1 by condition (A)\nand lemma 6. Using Taylor\u2019s expansion,\nD1 = \u03b3nv\nT\nn\u2211\ni=1\n\u2202Qni(\u03b2n0)\n\u2202\u03b2n\n+\n\u221a\nnK1 + smaller order terms,\nwhere K1 is as defined in lemma 8 so that within the lemma\u2019s proof we have \u2016K1\u2016 =\noP (1). Using equation (6), we have by mean-variance decomposition\u2225\u2225\u2225\u2225\u2225\u03b3nvT\nn\u2211\ni=1\n\u2202Qni(\u03b2n0)\n\u2202\u03b2n\n\u2225\u2225\u2225\u2225\u2225 = OP (\u03b3n\u221anvT In(\u03b2n0)v) \u2264 OP (\u221anpn)\u03b3n\u2016v\u2016,\nwhere last inequality follows from Cauchy-Schwarz and condition (B).\nHence\n|I\u02c61| \u2264 OP (\u221anpn)\u03b3n\u2016v\u2016.\n27\nNext consider I\u02c62 = I2 + (I\u02c62 \u2212 I2), where\nI2 =\n1\n2\nvT\u22072Qn(\u03b2n0)v\u03b32n\n= \u2212n\n2\nvT In(\u03b2n0)v\u03b3\n2\nn +\nn\n2\nvT\n{\nn\u22121\u22072Qn(\u03b2n0) + In(\u03b2n0)\n}\nv\u03b32n\nlemma 16\n= \u2212n\n2\nvT In(\u03b2n0)v\u03b3\n2\nn + oP (1)n\u03b3\n2\nn\u2016v\u20162.\nWe want to show that I\u02c62 \u2212 I2 has order smaller than n2vT In(\u03b2n0)v\u03b32n.\nBy Taylor\u2019s expansion,\nI\u02c62 \u2212 I2 = 1\n2\nvT\n{\n\u22072Q\u02c6n(\u03b2n0)\u2212\u22072Qn(\u03b2n0)\n}\nv\u03b32n\n=\n1\n2\nvT\u22072\n{\nn\u2211\ni=1\nq1(m\u02dcni(\u03b2n0), Yni)X\nT\ni (\u03b1\u02c6\u03b2n0(Ui)\u2212\u03b1\u03b2n0(Ui))\n}\nv\u03b32n\n=\n1\n2\nvTBnv\u03b3\n2\nn + smaller order terms\nwhere m\u02dcni(\u03b2n) = \u03b1\u02dc\u03b2n0(Ui)\nTXi+Z\nT\nni\u03b2n with \u03b1\u02dc\u03b2n(Ui) lies between \u03b1\u02c6\u03b2n(Ui) and \u03b1\u03b2n(Ui).\nDenote \u03b1\u03b2n(Ui) = \u03b1\u03b2n and so on. We have used condition (C) together with lemma 6\nand 7 to arrive at the last equality, where\nBn =\nn\u2211\ni=1\n{q3(mni(\u03b2n0), Yni)(Zni +\u03b1\u2032\u03b2n0Xi)(Zni +\u03b1\u2032\u03b2n0Xi)T (\u03b1\u02c6\u03b2n0 \u2212\u03b1\u03b2n0)TXi\n+ q2(mni(\u03b2n0), Yni)\nq\u2211\nr=1\nXir\u03b1\n(r)\u2032\u2032\n\u03b2n0\nXTi (\u03b1\u02c6\u03b2n0 \u2212\u03b1\u03b2n)\n+ q2(mni(\u03b2n0), Yni)(Zni +\u03b1\n\u2032\n\u03b2n0\nXi)X\nT\ni (\u03b1\u02c6\n\u2032\n\u03b2n0\n\u2212\u03b1\u2032\u03b2n0)T\n+ q2(mni(\u03b2n0), Yni)(\u03b1\u02c6\n\u2032\n\u03b2n0\n\u2212\u03b1\u2032\u03b2n0)Xi(Zni +\u03b1\u2032\u03b2n0Xi)T\n+ q1(mni(\u03b2n0), Yni)\nq\u2211\nr=1\nXir(\u03b1\u02c6\n(r)\u2032\u2032\n\u03b2n0\n\u2212\u03b1(r)\u2032\u2032\u03b2n0 )},\nwith \u03b1\u2032\u03b2n =\n\u2202\u03b1\u03b2n\n\u2202\u03b2n\nand \u03b1\n(r)\u2032\u2032\n\u03b2n\n=\n\u22022\u03b1\n(r)\n\u03b2n\n\u2202\u03b2n\u2202\u03b2\nT\nn\n, r = 1, \u00b7 \u00b7 \u00b7 , q. Using Cauchy-Schwarz inequality,\nconditions (A), (B), lemma 6 and 7,\n|vTBnv\u03b32n| \u2264 OP (pn(hp+1 +\n1\u221a\nnh\n)) \u00b7OP (n\u03b32n\u2016v\u20162)\n= oP (n\u03b3\n2\nn\u2016v\u20162).\nBy condition (B), we have\u2223\u2223\u2223\u2223n\u03b32n2 vT In(\u03b2n0)v\n\u2223\u2223\u2223\u2223 \u2265 O(n\u03b32n\u03bbmin(In(\u03b2n0))\u2016v\u20162)\n= O(n\u03b32n\u2016v\u20162).\n28\nFinally consider I\u02c63. Note that\nQ\u02c6n(\u03b2\n\u2217\nn) \u2264 Qn(\u03b2n0) + {\nn\u2211\ni=1\nq1(mni(\u03b2n0), Yni)X\nT\ni (\u03b1\u02c6\u03b2n0(Ui)\u2212\u03b1\u03b2n0(Ui))\n+\nn\u2211\ni=1\nq1(m\u02c6ni(\u03b2n0), Yni)(Zni + \u03b1\u02c6\n\u2032\n\u03b2n0\nXi)\u03b3nv}(1 + oP (1)),\nand by condition (C), lemma 6 and 7 again, we have\nI\u02c63 =\n1\n6\npn\u2211\ni,j,k=1\n\u22023Qn(\u03b2n0)\n\u2202\u03b2ni\u2202\u03b2nj\u2202\u03b2nk\nvivjvk\u03b3\n3\nn + smaller order terms.\nHence,\n|I\u02c63| \u2264 OP (np3\/2n \u03b33n\u2016v\u20163) \u2264 OP (np3\/2n \u03b33n\u2016v\u20163)\n= OP (\n\u221a\np4n\nn\n\u2016v\u2016)n\u03b32n\u2016v\u20162 = oP (1)n\u03b32n\u2016v\u20162.\nComparing, we find the order of \u2212n\u03b32n\n2\nvT In(\u03b2n0)v, which is negative, dominates all other\nterms by allowing \u2016v\u2016 = C to be large enough. This proves (21). \u0003\nBefore proving Theorem 2, we need another lemma.\nLemma 8 Under regularity conditions (A) - (G), if p3n\/n \u2192 0 with nhp+2 \u2192 \u221e and\nnh2p+3 = O(1), then for each \u03b2n \u2208 \u2126n,\n1\u221a\nn\n\u2016\u2207Q\u02c6n(\u03b2n)\u2212\u2207Qn(\u03b2n)\u2016 = oP (1).\nProof of lemma 8. Define\nK1 =\n1\u221a\nn\nn\u2211\ni=1\nq2(mni(\u03b2n), Yni)(Zni +\u03b1\n\u2032\n\u03b2n\n(Ui)Xi)(\u03b1\u02c6\u03b2n(Ui)\u2212\u03b1\u03b2n(Ui))TXi,\nK2 =\n1\u221a\nn\nn\u2211\ni=1\nq1(mni(\u03b2n), Yni)(\u03b1\u02c6\n\u2032\n\u03b2n\n(Ui)\u2212\u03b1\u2032\u03b2n(Ui))Xi,\nthen by Taylor\u2019s expansion, lemma 6 and condition (C),\n1\u221a\nn\n(\u2207Q\u02c6n(\u03b2n)\u2212\u2207Qn(\u03b2n)) = K1 +K2 + smaller order terms,\nwhere mni(\u03b2n) = \u03b1\u03b2n(Ui)\nTXi + Z\nT\nni\u03b2n. Define, for \u2126 as in condition (F),\nS =\n{\nf \u2208 C2(\u2126) : \u2016f\u2016\u221e \u2264 1\n}\n,\n29\nequipped with a metric\n\u03c1(f1, f2) = \u2016f1 \u2212 f2\u2016\u221e,\nwith \u2016f\u2016\u221e = supu\u2208\u2126 |f(u)|. We also let, for r = 1, \u00b7 \u00b7 \u00b7 , q and l = 1, \u00b7 \u00b7 \u00b7 , pn,\nArl(y, u,X,Zn) = q2(X\nT\u03b1\u03b2n(u) + Z\nT\nn\u03b2n, y)Xr\n(\nZnl +X\nT \u2202\u03b1\u03b2n(u)\n\u2202\u03b2nl\n)\n,\nBr(y, u,X,Zn) = q1(X\nT\u03b1\u03b2n(u) + Z\nT\nn\u03b2n, y)Xr.\nBy lemma 6, for any \u03b4 > 0 and as n\u2192\u221e, we have\nP0\n(\nn\u2212\u03b4\n(\nhp+1 +\n1\u221a\nnh\n)\u22121\n(\u03b1\u02c6\n(r)\n\u03b2n\n\u2212 \u03b1(r)\u03b2n)\ufe38 \ufe37\ufe37 \ufe38\n:=\u03bbr\n\u2208 S\n)\n\u2192 1,\nP0\n(\nn\u2212\u03b4\n(\nhp+1 +\n1\u221a\nnh\n)\u22121(\u2202\u03b1\u02c6(r)\u03b2n\n\u2202\u03b2nl\n\u2212 \u03b1\n(r)\n\u03b2n\n\u2202\u03b2nl\n)\n\ufe38 \ufe37\ufe37 \ufe38\n:=\u03b3rl\n\u2208 S\n)\n\u2192 1,\nwhere r = 1, \u00b7 \u00b7 \u00b7 , q and l = 1, \u00b7 \u00b7 \u00b7 , pn. Hence for sufficiently large n, we have \u03bbr, \u03b3rl \u2208 S.\nThe following three points allow us to utilize Jain and Marcus (1975) to prove our\nlemma.\nI. For any v \u2208 S, we will view the map v 7\u2192 Arl(y, u,X,Zn)v(u) as an element of\nC(S), the space of continuous functions on S equipped with the sup norm. For\nv1, v2 \u2208 S, we have\n|Arl(y, u,X,Zn)v1(u)\u2212 Arl(y, u,X,Zn)v2(u)| = |Arl(y, u,X,Zn)(v1 \u2212 v2)(u)|\n\u2264 |Arl(y, u,X,Zn)|\u2016v1 \u2212 v2\u2016.\nSimilar result holds for Br(y, u,X,Zn).\nII. By equation (7), we can easily see that\nE0(Arl(Y, U,X,Zn)) = 0\nfor each r = 1, \u00b7 \u00b7 \u00b7 , q and l = 1, \u00b7 \u00b7 \u00b7 , pn. Also we have\nE0(Arl(Y, U,X,Zn)\n2) <\u221e,\nby regularity conditions (A) and (C). Similar results hold for Br(Y, U,X,Zn).\nIII. Let H(\u00b7, S) denote the metric entropy of the set S w.r.t. the metric \u03c1. Then\nH(\u000f, S) \u2264 C0\u000f\u22121\nfor some constant C0. Hence\n\u222b 1\n0\nH(\u000f, S)d\u000f <\u221e.\n30\nConditions of Theorem 1 in Jain and Marcus(1975) can be derived from the three\nnotes above, so that we have\n1\u221a\nn\nn\u2211\ni=1\nArl(Yi, Ui,Xi,Zni)(\u00b7),\nwhere Arl(Yi, Ui,Xi,Zni)(\u00b7), i = 1, \u00b7 \u00b7 \u00b7 , n being i.i.d. replicates of Arl(Y, U,X,Zn)(\u00b7) in\nC(S), converges weakly to a Gaussian measure on C(S). Hence, since \u03bbr, \u03b3rl \u2208 S,\n1\u221a\nn\nn\u2211\ni=1\nArl(Yi, Ui,Xi,Zni)(\u03bbr) = OP (1),\nwhich implies that\n1\u221a\nn\nn\u2211\ni=1\nArl(Yi, Ui,Xi,Zni)(\u03b1\u02c6\n(r)\n\u03b2n\n\u2212 \u03b1(r)\u03b2n) = OP\n(\nn\u03b4\n(\nhp+1 +\n1\u221a\nnh\n))\n.\nSimilarly, apply Theorem 1 of Jain and Marcus(1975) again, we have\n1\u221a\nn\nn\u2211\ni=1\nBr(Yi, Ui,Xi,Zni)\n(\n\u2202\u03b1\u02c6\n(r)\n\u03b2n\n\u2202\u03b2nl\n\u2212 \u03b1\n(r)\n\u03b2n\n\u2202\u03b2nl\n)\n= OP\n(\nn\u03b4\n(\nhp+1 +\n1\u221a\nnh\n))\n.\nThen the column vector K1 which is pn\u2212dimensional, has the lth component equals\nq\u2211\nr=1\n{\n1\u221a\nn\nn\u2211\ni=1\nArl(Yi, Ui,Xi,Zni)(\u03b1\u02c6\n(r)\n\u03b2n\n\u2212 \u03b1(r)\u03b2n)\n}\n= OP\n(\nn\u03b4\n(\nhp+1 +\n1\u221a\nnh\n))\n,\nusing the result just proved. Hence we have shown\n\u2016K1\u2016 = OP\n(\u221a\npnn\n\u03b4\n(\nhp+1 +\n1\u221a\nnh\n))\n= oP (1),\nsince \u03b4 can be made arbitrarily small. Similarly, we have \u2016K2\u2016 = oP (1) as well. The\nconclusion of the lemma follows. \u0003\nProof of Theorem 2. We first assume nh2p+2 = O(1) and nhp+2 \u2192\u221e as in Theorem 1,\nso that \u2016\u03b2\u02c6n \u2212 \u03b2n0\u2016 = OP (\n\u221a\npn\/n). Since \u2207Q\u02c6n(\u03b2\u02c6n) = 0, by Taylor\u2019s expansion,\n\u2207Q\u02c6n(\u03b2n0) +\u22072Q\u02c6n(\u03b2n0)(\u03b2\u02c6n \u2212 \u03b2n0) +\n1\n2\n(\u03b2\u02c6n \u2212 \u03b2n0)T\u22072(\u2207Q\u02c6n(\u03b2\u2217n))(\u03b2\u02c6n \u2212 \u03b2n0) = 0,\nwhere \u03b2\u2217n lies between \u03b2n0 and \u03b2\u02c6n. This implies\n1\nn\n\u22072Q\u02c6n(\u03b2n0)(\u03b2\u02c6n \u2212 \u03b2n0) =\u2212\n1\nn\n(\u2207Q\u02c6n(\u03b2n0)\n+\n1\n2\n(\u03b2\u02c6n \u2212 \u03b2n0)T\u22072(\u2207Q\u02c6n(\u03b2\u2217n))(\u03b2\u02c6n \u2212 \u03b2n0)).\n(22)\n31\nDefine C = 1\n2\n(\u03b2\u02c6n \u2212 \u03b2n0)T\u22072(\u2207Q\u02c6n(\u03b2\u2217n))(\u03b2\u02c6n \u2212 \u03b2n0)). Using similar argument to\napproximating I\u02c63 in Theorem 1, using lemma 6 and lemma 7, and noting \u2016\u03b2\u2217n\u2212\u03b2n0\u2016 =\noP (1), we have\n\u2225\u2225\u2225\u22072 \u22022Q\u02c6n(\u03b2\u2217n)\u2202\u03b2nj \u2225\u2225\u22252 = OP (n2p2n). Hence\n\u2016n\u22121C\u20162 \u2264 1\n2n2\n\u2016\u03b2n \u2212 \u03b2n0\u20164\n\u2225\u2225\u2225\u2225\u2225\u22072\u22022Q\u02c6n(\u03b2\u2217n)\u2202\u03b2nj\n\u2225\u2225\u2225\u2225\u2225\n2\n\u2264 1\n2n2\nOP\n(\np2n\nn2\n) pn\u2211\nj=1\nOP (n\n2p2n)\n= OP\n(\np5n\nn2\n)\n= oP\n(\n1\nn\n)\n.\n(23)\nAt the same time, by lemma 16 and Cauchy-Schwarz inequality,\u2225\u2225\u2225\u2225 1n\u22072Q\u02c6n(\u03b2n0)(\u03b2\u02c6n \u2212 \u03b2n0) + In(\u03b2n0)(\u03b2\u02c6n \u2212 \u03b2n0)\n\u2225\u2225\u2225\u2225\n\u2264 oP\n(\n1\u221a\nnpn\n)\n+OP\n(\u221a\np3n\nn\n(\nhp+1 +\n1\u221a\nnh\n))\n\u2264 oP\n(\n1\u221a\nn\n)\n+OP\n(\n1\u221a\nn\n\u00b7\n(\u221a\np3n\nn\n+\n\u221a\np3n\nn(p+1)\/(p+2)\n))\n= oP\n(\n1\u221a\nn\n)\n,\n(24)\nwhere the second last line used nh2p+2 = O(1) and nhp+2 \u2192 \u221e, and the last line used\nassumption p5n\/n\u2192 0.\nCombining (22),(23) and (24), we have\nIn(\u03b2n0)(\u03b2\u02c6n \u2212 \u03b2n0) =\n1\nn\n\u2207Q\u02c6n(\u03b2n0) + oP\n(\n1\u221a\nn\n)\n=\n1\nn\n\u2207Qn(\u03b2n0) + oP\n(\n1\u221a\nn\n)\n,\n(25)\nwhere the last line follows from lemma 8. Consequently, using equation (25), we get\n\u221a\nnAnI\n1\/2\nn (\u03b2n0)(\u03b2\u02c6n \u2212 \u03b2n0)\n=\n1\u221a\nn\nAnI\n\u22121\/2\nn (\u03b2n0)\u2207Qn(\u03b2n0) + oP (AnI\u22121\/2n (\u03b2n0))\n=\n1\u221a\nn\nAnI\n\u22121\/2\nn (\u03b2n0)\u2207Qn(\u03b2n0) + oP (1),\n(26)\n32\nwhere the last equality holds since by condition of Theorem 2, \u2016AnI\u22121\/2n (\u03b2n0)\u2016 is of\norder O(1).\nLet Bni =\n1\u221a\nn\nAnI\n\u22121\/2\nn (\u03b2n0)\u2207Qni(\u03b2n0), where Qni(\u03b2n) = Q(g\u22121(mni(\u03b2n)), Yni), i =\n1, \u00b7 \u00b7 \u00b7 , n. Given \u000f > 0,\nn\u2211\ni=1\nE0\u2016Bni\u201621{\u2016Bni\u2016 > \u000f} = nE0\u2016Bn1\u201621{\u2016Bn1\u2016 > \u000f}\n\u2264 n\n\u221a\nE0\u2016Bn1\u20164 \u00b7 P(\u2016Bni\u2016 > \u000f).\nUsing Chebyshev\u2019s inequality,\nP(\u2016Bn1\u2016 > \u000f) \u2264 E0\u2016Bn1\u2016\n2\n\u000f2\n=\n1\nn\u000f2\nE\u2016AnI\u22121\/2n (\u03b2n0)\u2207Qn1(\u03b2n0)\u20162\n=\n1\nn\u000f2\ntr{I\u22121\/2n (\u03b2n0)ATnAnI\u22121\/2n (\u03b2n0)E0(\u2207Qn1(\u03b2n0)\u2207Qn1(\u03b2n0)T )}\n=\n1\nn\u000f2\ntr{I\u22121\/2n (\u03b2n0)ATnAnI1\/2n (\u03b2n0)}\n=\n1\nn\u000f2\ntr(G) = O\n(\n1\nn\n)\n,\n(27)\nwhere tr(A) is the trace of square matrix A. Similarly, we can show that\nE0\u2016Bn1\u20164 \u2264\n\u221a\nl\nn2\n\u03bb2max(AnA\nT\nn )\u03bb\n2\nmax(I\n\u22121\nn (\u03b2n0))\n\u221a\nE0\u2207Qn1(\u03b2n0)T\u2207Qn1(\u03b2n0)\n= O\n(\np2n\nn2\n)\n.\n(28)\nTherefore (27) and (28) together implies\nn\u2211\ni=1\nE0\u2016Bni\u201621{\u2016Bni\u2016 > \u000f} = O\n(\nn \u00b7 pn\nn\n\u00b7 1\u221a\nn\n)\n= O\n(\u221a\np2n\nn\n)\n= o(1).\nAlso,\nn\u2211\ni=1\nVar0(Bni) = nVar0(Bn1) = Var0(AnI\n\u22121\/2\nn (\u03b2n0)\u2207Qn1(\u03b2n0))\n= AnI\n\u22121\/2\nn (\u03b2n0)E0\u2207Qn1(\u03b2n0)\u2207Qn1(\u03b2n0)T I\u22121\/2n (\u03b2n0)ATn\n= AnA\nT\nn \u2192 G.\n33\nTherefore Bni satisfies the conditions of the Lindeberg-Feller central limit Theorem (see\nfor example, Van der Vaart(1998)). Consequently, asymptotic normality of\n\u2211n\ni=1Bni\nfollows. Using (26), it means\n\u221a\nnAnI\n1\/2\nn (\u03b2n0)(\u03b2\u02c6n \u2212 \u03b2n0) D\u2212\u2192 N(0, G).\nFor the optimal bandwidth h = O(n\u22121\/(2p+3)), we can follow same lines of proof in\nTheorem 1 to arrive at\n\u2225\u2225\u2225\u03b2\u02c6n \u2212 \u03b2n0\u2225\u2225\u2225 = OP (\u221apn\/n(2p+2)\/(2p+3)). Note that the proof of\nTheorem 2 is affected only in (23) and (24). With the condition p5n\/n\n(2p+1)\/(2p+3) \u2192 0,\n(23) becomes\n\u2016n\u22121C\u20162 \u2264 1\n2n2\nOP\n(\np2n\nn2\n\u00b7 n2\/(2p+3)\n) pn\u2211\nj=1\nOP (n\n2p2n)\n= OP\n(\np5n\nn2\n\u00b7 n2\/(2p+3)\n)\n= OP (p\n5\nn\/n\n(2p+1)\/(2p+3) \u00b7 1\nn\n)\n= oP\n(\n1\nn\n)\n.\nFor (24), since p5n\/n \u2192 0, p4n\/n(2p+2)\/(2p+3) \u2192 0 is automatically satisfied and so by\nlemma 16,\n\u2225\u2225\u2225\u2225 1n\u22072Q\u02c6n(\u03b2n0)(\u03b2\u02c6n \u2212 \u03b2n0) + In(\u03b2n0)(\u03b2\u02c6n \u2212 \u03b2n0)\n\u2225\u2225\u2225\u2225\n= oP\n(\nn1\/(4p+6)\npnn1\/(4p+6)\n\u00b7\n\u221a\npn\nn\n)\n+OP\n(\n2pnn\n\u2212(p+1)\/(2p+3) \u00b7\n\u221a\npn\nn\n\u00b7 n1\/(4p+6)\n)\n= oP\n(\n1\u221a\nnpn\n)\n+OP\n(\n1\u221a\nn\n\u00b7\n\u221a\np3n\nn(2p+1)\/(2p+3)\n)\n= oP\n(\n1\u221a\nn\n)\n.\nHence conclusion of Theorem 2 still follows. \u0003\nRefer back to section 2.2, let Bn be a (pn \u2212 l)\u00d7 pn matrix satisfying BnBTn = Ipn\u2212l\nand AnB\nT\nn = 0. Since An\u03b2n = 0 under H0, rows of An are perpendicular to \u03b2n and the\northogonal complement of rows of An is spanned by rows of Bn by AnB\nT\nn = 0. Hence\n\u03b2n = B\nT\nn\u03b3\nunder H0, where \u03b3 is an (pn \u2212 l) \u00d7 1 vector. Then under H0 the profile likelihood\nestimator is also the local maximizer \u03b3\u02c6n of the problem\nQ\u02c6n(B\nT\nn \u03b3\u02c6n) = max\n\u03b3n\nQn(B\nT\nn\u03b3n).\n34\nTo prove Theorem 3 we need the following lemmas, the proofs of which are given in the\nappendix.\nLemma 9 Assuming regularity conditions (A) - (G). Under the null hypothesis H0 as\nin Theorem 3, if nh2p+2 = O(1), then under p5n\/n = o(1),\nBTn (\u03b3\u02c6n \u2212 \u03b3n0) =\n1\nn\nBTn {BnIn(\u03b2n0)BTn }\u22121BTn\u2207Qn(\u03b2n0) + oP (n\u22121\/2).\nMoreover, if h = O(n\u22121\/(2p+3)), then under p5n\/n\n(2p+1)\/(2p+3) = o(1), the same con-\nclusion still holds.\nLemma 10 Under regularity conditions (A) - (G) and p5n\/n = o(1), we have\n1\nn\n\u2016\u22072Q\u02c6n(\u03b2\u02c6n)\u2212\u22072Q\u02c6n(\u03b2n0)\u2016 = oP\n(\n1\u221a\npn\n)\nif nh2p+2 = O(1). Moreover if h = O(n\u22121\/(2p+3)), then assuming further p5n\/n\n(2p+2)\/(2p+3) =\no(1), the same conclusion still holds.\nLemma 11 Assuming the conditions of Theorem 3, under the null hypothesis H0, we\nhave\nQ\u02c6n(\u03b2\u02c6n)\u2212 Q\u02c6n(BTn \u03b3\u02c6n) =\nn\n2\n(\u03b2\u02c6n \u2212BTn \u03b3\u02c6n)T In(\u03b2n0)(\u03b2\u02c6n \u2212BTn \u03b3\u02c6n) + oP (1).\nProof of Theorem 3. Adapting the notation in lemma 11, substituting equation (30)\ninto its conclusion we get\nQ\u02c6n(\u03b2\u02c6n)\u2212 Q\u02c6n(BTn \u03b3\u02c6n) =\nn\n2\n\u03a6Tn\u0398\n\u22121\/2\nn Sn\u0398\n\u22121\/2\nn \u03a6n + oP (1),\nwhere \u0398n = In(\u03b2n0), \u03a6n =\n1\nn\n\u2207Qn(\u03b2n0) and Sn = In \u2212 \u03981\/2n BTn (Bn\u0398nBTn )\u22121Bn\u03981\/2n .\nSince Sn is idempotent, it can be written as Sn = D\nT\nnDn where Dn is a l \u00d7 pn matrix\nsatisfying DnD\nT\nn = Il.\nBy the proof of Theorem 2, substituting An there with Dn, using equation (26), we\nhave already shown that\n\u221a\nnDn\u0398\n\u22121\/2\nn \u03a6n\nD\u2212\u2192 N(0, Il). Hence\n2{Q\u02c6n(\u03b2\u02c6n)\u2212 Q\u02c6n(\u03b2n0)} = n(Dn\u0398\u22121\/2n \u03a6n)T (Dn\u0398\u22121\/2n \u03a6n) D\u2212\u2192 \u03c72l . \u0003\nTo prove Theorem 4, we need two lemmas. The proofs are given in the appendix.\n35\nLemma 12 Assuming the conditions of Theorem 4, we have\nn\u22121\u2016\u22072Qn(\u03b2\u02c6n)\u2212\u22072Qn(\u03b2n0)\u2016 = oP (1).\nLemma 13 Assuming the conditions of Theorem 4, we have for each \u03b2n \u2208 \u2126n,\nn\u22121\u2016\u22072Q\u02c6n(\u03b2n)\u2212\u22072Qn(\u03b2n)\u2016 = oP (1).\nProof of Theorem 4. Let A\u02c6n = \u2212n\u22121\u22072Q\u02c6n(\u03b2\u02c6n), B\u02c6n = c\u0302ov{\u2207Q\u02c6n(\u03b2\u02c6n)} and C = In(\u03b2n0).\nWrite\nI1 = A\u02c6\u22121n (B\u02c6n \u2212 C)A\u02c6\u22121n , I2 = A\u02c6\u22121n (C \u2212 A\u02c6n)A\u02c6\u22121n , I3 = A\u02c6\u22121n (C \u2212 A\u02c6n)C\u22121,\nthen we can rewrite\n\u03a3\u02c6n \u2212 \u03a3n = I1 + I2 + I3.\nOur aim is to show that, for all i = 1, \u00b7 \u00b7 \u00b7 , pn,\n\u03bbi(\u03a3\u02c6n \u2212 \u03a3n) = oP (1),\nso that An(\u03a3\u02c6n \u2212\u03a3n)ATn P\u2212\u2192 0, where \u03bbi(A) is the ith eigenvalue of a symmetric matrix\nA. Using the inequalities\n\u03bbmin(I1) + \u03bbmin(I2) + \u03bbmin(I3) \u2264 \u03bbmin(I1 + I2 + I3)\n\u2264 \u03bbmax(I1 + I2 + I3) \u2264 \u03bbmax(I1) + \u03bbmax(I2) + \u03bbmax(I3),\nit suffices to show that \u03bbi(Ij) = oP (1) for j = 1, 2, 3. From the definition of I1, I2 and I3,\nit is clear that we only need to show \u03bbi(C \u2212 A\u02c6n) = oP (1) and \u03bbi(B\u02c6n \u2212 C) = oP (1). Let\nK1 = In(\u03b2n0) + n\n\u22121\u22072Qn(\u03b2n0),\nK2 = n\n\u22121(\u22072Qn(\u03b2\u02c6n)\u2212\u22072Qn(\u03b2n0)),\nK3 = n\n\u22121(\u22072Q\u02c6n(\u03b2\u02c6n)\u2212\u22072Qn(\u03b2\u02c6n)),\nthen\nC \u2212 A\u02c6n = K1 +K2 +K3.\nApplying lemma 16 on K1, lemma 12 on K2 and lemma 13 on K3, we have \u2016C \u2212 A\u02c6\u2016 =\noP (1), and so \u03bbi(C\u2212A\u02c6) = oP (1). Hence the only thing left to show is \u03bbi(B\u02c6n\u2212C) = oP (1).\nTo this end, consider the decomposition\n\u03bbi(B\u02c6n \u2212 C) = K4 +K5\n36\nwhere\nK4 =\n{\n1\nn\nn\u2211\ni=1\n\u2202Q\u02c6ni(\u03b2\u02c6n)\n\u2202\u03b2nj\n\u2202Q\u02c6ni(\u03b2\u02c6n)\n\u2202\u03b2nk\n}\n\u2212 In(\u03b2n0),\nK5 = \u2212\n{\n1\nn\nn\u2211\ni=1\n\u2202Q\u02c6ni(\u03b2\u02c6n)\n\u2202\u03b2nj\n}{\n1\nn\nn\u2211\ni=1\n\u2202Q\u02c6ni(\u03b2\u02c6n)\n\u2202\u03b2nk\n}\n.\nOur goal is to show that K4 and K5 are oP (1), which then implies \u03bbi(B\u02c6n \u2212 C) = oP (1).\nWe consider K4 first, which can be further decomposed such that\nK4 = K6 +K7,\nwhere\nK6 =\n{\n1\nn\nn\u2211\ni=1\n\u2202Q\u02c6ni(\u03b2\u02c6n)\n\u2202\u03b2nj\n\u2202Q\u02c6ni(\u03b2\u02c6n)\n\u2202\u03b2nk\n\u2212 1\nn\nn\u2211\ni=1\n\u2202Qni(\u03b2n0)\n\u2202\u03b2nj\n\u2202Qni(\u03b2n0)\n\u2202\u03b2nk\n}\n,\nK7 =\n{\n1\nn\nn\u2211\ni=1\n\u2202Qni(\u03b2n0)\n\u2202\u03b2nj\n\u2202Qni(\u03b2n0)\n\u2202\u03b2nk\n}\n\u2212 In(\u03b2n0).\nObserve that\nK6 =\n{\n1\nn\nn\u2211\ni=1\n\u2202Qni(\u03b2n0)\n\u2202\u03b2nj\n{\n\u2202Q\u02c6ni(\u03b2\u02c6n)\n\u2202\u03b2nk\n\u2212 \u2202Qni(\u03b2n0)\n\u2202\u03b2nk\n}\n+\n1\nn\nn\u2211\ni=1\n\u2202Qni(\u03b2n0)\n\u2202\u03b2nk\n{\n\u2202Q\u02c6ni(\u03b2\u02c6n)\n\u2202\u03b2nj\n\u2212 \u2202Qni(\u03b2n0)\n\u2202\u03b2nj\n}\n+\n1\nn\nn\u2211\ni=1\n{\n\u2202Q\u02c6ni(\u03b2\u02c6n)\n\u2202\u03b2nk\n\u2212 \u2202Qni(\u03b2n0)\n\u2202\u03b2nk\n}{\n\u2202Q\u02c6ni(\u03b2\u02c6n)\n\u2202\u03b2nj\n\u2212 \u2202Qni(\u03b2n0)\n\u2202\u03b2nj\n}}\n,\nand this suggests that an approximation of the order of \u2202\n\u2202\u03b2nk\n(Q\u02c6ni(\u03b2\u02c6n) \u2212 Qni(\u03b2n0)) for\neach k = 1, \u00b7 \u00b7 \u00b7 , pn and i = 1, \u00b7 \u00b7 \u00b7 , n is rewarding. Define\naik =\n\u2202\n\u2202\u03b2nk\n(Q\u02c6ni(\u03b2\u02c6n)\u2212Qni(\u03b2\u02c6n)),\nbik =\n\u2202\n\u2202\u03b2nk\n(Qni(\u03b2\u02c6n)\u2212Qni(\u03b2n0)),\nthen \u2202\n\u2202\u03b2nk\n(Q\u02c6ni(\u03b2\u02c6n)\u2212Qni(\u03b2n0)) = aik + bik. By Taylor\u2019s expansion,\naik =\n\u2202Q\u02c6ni(\u03b2\u02c6n)\n\u2202\u03b2nk\n\u2212 \u2202Qni(\u03b2\u02c6n)\n\u2202\u03b2nk\n=\n\u2202\n\u2202\u03b2nk\n(q1(m\u02dcni(\u03b2\u02c6n), Yni)(\u03b1\u02c6\u03b2\u02c6n(Ui)\u2212\u03b1\u03b2\u02c6n(Ui))\nTXi)\n= q2(m\u02dcni(\u03b2\u02c6n), Yni)\n(\nZnik +\n\u2202\u03b1\u02dc\u03b2\u02c6n(Ui)\n\u2202\u03b2nk\nT\nXi\n)\n(\u03b1\u02c6\u03b2\u02c6n(Ui)\u2212\u03b1\u03b2\u02c6n(Ui))\nTXi\n+ q1(m\u02dcni(\u03b2\u02c6n), Yni)\n(\n\u2202\u03b1\u02c6\u03b2\u02c6n(Ui)\n\u2202\u03b2nk\n\u2212 \u2202\u03b1\u03b2\u02c6n(Ui)\n\u2202\u03b2nk\n)T\nXi,\n37\nwhere m\u02dcni(\u03b2\u02c6n) = \u03b1\u02dc\u03b2\u02c6n(Ui)\nTXi + Z\nT\nni\u03b2\u02c6n and \u03b1\u02dc\u03b2\u02c6n(Ui) lies between \u03b1\u03b2\u02c6n(Ui) and \u03b1\u02c6\u03b2\u02c6n(Ui).\nUsing lemma 6, 7 and conditions (A) and (C), with argument similar to the proof of\nlemma 13, we then have\n|aik| \u2264 OP\n(\nhp+1 +\n1\u221a\nnh\n)\n.\nSimilarly, using Taylor\u2019s expansion and lemma 6, 7, regularity conditions (A) and\n(C),\nbik =\n\u2202Qni(\u03b2\u02c6n)\n\u2202\u03b2nk\n\u2212 \u2202Qni(\u03b2n0)\n\u2202\u03b2nk\n=\n{\nq2(mni(\u03b2n0), Yni)(Zni +\u03b1\n\u2032\n\u03b2n0\n(Ui)Xi)\nT (Znik +X\nT\ni\n\u2202\u03b1\u03b2n0(Ui)\n\u2202\u03b2nk\n)\n+ q1(mni(\u03b2n0), Yni)\n(\nXTi\n\u22022\u03b1\u03b2n0(Ui)\n\u2202\u03b2nk\u2202\u03b2\nT\nn\n)}\n(\u03b2\u02c6n \u2212 \u03b2n0) + smaller order terms,\nwhich implies that, by Cauchy-Schwarz inequality, together with Theorem 1 and regu-\nlarity conditions (A) and (C) again,\n|bik| \u2264 OP\n(\npn\u221a\nndh\n)\n, where dh =\n{\n1, if nh2p+2 = O(1).\n2p+2\n2p+3\n, if nh2p+3 = O(1).\nHence using the approximations of aik and bik above,\u2223\u2223\u2223\u2223 1n\nn\u2211\ni=1\n\u2202Qni(\u03b2n0)\n\u2202\u03b2nj\n{\n\u2202Q\u02c6ni(\u03b2\u02c6n)\n\u2202\u03b2nk\n\u2212 \u2202Qni(\u03b2n0)\n\u2202\u03b2nk\n}\u2223\u2223\u2223\u2223\n\u2264 1\nn\nn\u2211\ni=1\n\u2223\u2223\u2223\u2223q1(mni(\u03b2n0), Yni)(Znij +XTi \u2202\u03b1\u03b2n0(Ui)\u2202\u03b2nj\n)\u2223\u2223\u2223\u2223 \u00b7 |aik + bik|\n\u2264 sup\n1\u2264k\u2264pn,1\u2264i\u2264n\n|aik + bik| \u00b7\n{\nE0\n(\n|q1(mni(\u03b2n0), Yni)|\n\u2223\u2223\u2223\u2223Znij +XTi \u2202\u03b1\u03b2n0(Ui)\u2202\u03b2nj\n\u2223\u2223\u2223\u2223)+ oP (1)}\n\u2264 OP\n(\nhp+1 +\n1\u221a\nnh\n+\npn\u221a\nndh\n)\n,\nwhere the second last line follows from mean variance decomposition and conditions (A)\nand (C). This shows that\n\u2016K6\u2016 \u2264 OP\n(\npn\n(\nhp+1 +\n1\u221a\nnh\n)\n+\n\u221a\np4n\nndh\n)\n= oP (1)\nby the conditions of the Theorem.\n38\nFor K7, note that\nP\n{\u2225\u2225\u2225\u2225\u2225\n{\n1\nn\nn\u2211\ni=1\n\u2202Qni(\u03b2n0)\n\u2202\u03b2nj\n\u2202Qni(\u03b2n0)\n\u2202\u03b2nk\n}\n\u2212 In(\u03b2n0)\n\u2225\u2225\u2225\u2225\u2225 \u2265 \u000f\n}\n\u2264 1\nn2\u000f2\nE0\npn\u2211\nj,k=1\nn\u2211\ni=1\n{\n\u2202Qni(\u03b2n0)\n\u2202\u03b2nj\n\u2202Qni(\u03b2n0)\n\u2202\u03b2nk\n\u2212 E0\n(\n\u2202Qni(\u03b2n0)\n\u2202\u03b2nj\n\u2202Qni(\u03b2n0)\n\u2202\u03b2nk\n)}2\n= O\n(\nnp2n\nn2\u000f2\n)\n= O\n(\np2n\nn\n)\n= o(1),\nwhich implies that \u2016K7\u2016 = oP (1). Hence using K4 = K6 +K7,\n\u2016K4\u2016 \u2264 oP (1) +OP\n(\npn\n(\nhp+1 +\n1\u221a\nnh\n)\n+\n\u221a\np4n\nndh\n)\n= oP (1).\nFinally consider K5. Define Aj =\n1\nn\n\u2211n\ni=1(aij+bij) +\n1\nn\n\u2211n\ni=1\n\u2202Qni(\u03b2n0)\n\u2202\u03b2nj\n, where aij and\nbij are defined as before, we can then rewrite K5 = {AjAk}. Now\n|Aj| \u2264 sup\ni,j\n|aij + bij|+\n\u2223\u2223\u2223\u2223\u2223 1n\nn\u2211\ni=1\n\u2202Qni(\u03b2n0)\n\u2202\u03b2nj\n\u2223\u2223\u2223\u2223\u2223\n\u2264 OP\n(\nhp+1 +\n1\u221a\nnh\n+\n\u221a\np4n\nndh\n)\n+OP (n\n\u22121\/2),\nwhere the last line follows from the approximations for aij and bij, and mean-variance\ndecomposition of the term 1\nn\n\u2211n\ni=1\n\u2202Qni(\u03b2n0)\n\u2202\u03b2nj\n. Hence\n\u2016K5\u2016 \u2264 OP\n\uf8eb\uf8edpn(hp+1 + 1\u221a\nnh\n+\n\u221a\np4n\nndh\n)2\uf8f6\uf8f8 = oP (1),\nand the proof completes. \u0003\n6 Appendix\nLemma 14 (Lemma A.1) Let C and D be respectively compact sets in Rd and Rp\nand f(x,\u03b8) is a continuous function in \u03b8 \u2208 C and x \u2208 D. Assume that \u03b8\u02c6(x) \u2208 C\nis continuous in x \u2208 D, and is the unique maximizer of f(x,\u03b8). Let \u03b8\u02c6n(x) \u2208 C be a\nmaximizer of fn(x,\u03b8). If\nsup\n\u03b8\u2208C,x\u2208D\n|fn(x,\u03b8)\u2212 f(x,\u03b8)| \u2192 0, then sup\nx\u2208D\n|\u03b8\u02c6n(x)\u2212 \u03b8\u02c6(x)| \u2192 0, as n\u2192\u221e.\n39\nProof: This is Lemma A.1 of Carroll et al. (1997).\nLemma 15 (Lemma A.2) Let (X1, Y1), \u00b7 \u00b7 \u00b7 , (Xn, Yn) be i.i.d. random vectors, where\nthe Yi\u2019s are scalar random variables. Assume further that E|Y |r <\u221e and supx\n\u222b |y|rf(x, y)dy <\n\u221e where f denotes the joint density of (X, Y ). Let K be a bounded positive function\nwith a bounded support, satisfying a Lipschitz condition. Then,\nsup\nx\u2208D\n\u2223\u2223\u2223\u2223\u2223n\u22121\nn\u2211\ni=1\n{Kh(Xi \u2212 x)Yi \u2212 E[Kh(Xi \u2212 x)Yi]}\n\u2223\u2223\u2223\u2223\u2223 = OP\n(\u221a\nlog(1\/h)\nnh\n)\n,\nprovided that n2\u000f\u22121h\u2192\u221e for some \u000f < 1\u2212 r\u22121.\nProof: This is a direct result of Mack and Silverman (1982).\nLemma 16 Under conditions of Theorem 1, when nh2p+2 = O(1),\u2225\u2225\u2225\u22251n\u22072Qn(\u03b2n0) + In(\u03b2n0)\n\u2225\u2225\u2225\u2225 = oP ( 1pn\n)\n,\u2225\u2225\u2225\u22251n\u22072Q\u02c6n(\u03b2n0) + In(\u03b2n0)\n\u2225\u2225\u2225\u2225 = oP ( 1pn\n)\n+OP\n(\npn\n(\nhp+1 +\n1\u221a\nnh\n))\n.\nMoreover, if h = O(n\u22121\/(2p+3)), then assuming p4n\/n\n(2p+2)\/(2p+3) = o(1),\u2225\u2225\u2225\u22251n\u22072Qn(\u03b2n0) + In(\u03b2n0)\n\u2225\u2225\u2225\u2225 = oP ( 1pnn1\/(4p+6)\n)\n,\u2225\u2225\u2225\u22251n\u22072Q\u02c6n(\u03b2n0) + In(\u03b2n0)\n\u2225\u2225\u2225\u2225 = oP ( 1pnn1\/(4p+6)\n)\n+OP\n(\npn\n(\nhp+1 +\n1\u221a\nnh\n))\n.\nProof of lemma 16. First we assume p4n\/n \u2192 0 and nh2p+2 = O(1). Given \u000f > 0, by\nChebyshev\u2019s inequality,\nP\n(\npn\n\u2225\u2225\u2225\u22251n\u22072Qn(\u03b2n0) + In(\u03b2n0)\n\u2225\u2225\u2225\u2225 \u2265 \u000f)\n\u2264 p\n2\nn\nn2\u000f2\nE0\npn\u2211\ni,j=1\n{\n\u22022Qn(\u03b2n0)\n\u2202\u03b2ni\u2202\u03b2nj\n\u2212 E0\u2202\n2Qn(\u03b2n0)\n\u2202\u03b2ni\u2202\u03b2nj\n}2\n= O\n(\nnp4n\nn2\u000f2\n)\n= O\n(\np4n\nn\n)\n= o(1)\nwhich proves the first equation in the lemma. From this, triangle inequality immediately\ngives \u2225\u2225\u2225\u2225 1n\u22072Q\u02c6n(\u03b2n0) + In(\u03b2n0)\n\u2225\u2225\u2225\u2225 = oP ( 1pn\n)\n+ \u2016n\u22121\u22072(Q\u02c6n(\u03b2n0)\u2212Qn(\u03b2n0))\u2016.\n40\nNote that by Taylor\u2019s expansion,\n\u22072(Q\u02c6n(\u03b2n0)\u2212Qn(\u03b2n0)) =\nn\u2211\ni=1\n\u22072q1(m\u02dcni(\u03b2n0), Yni)XTi (\u03b1\u02c6\u03b2n0(Ui)\u2212\u03b1\u03b2n0(Ui)),\nwhere m\u02dcni(\u03b2n) = \u03b1\u02dc\u03b2n(Ui) + Z\nT\nni\u03b2n, with \u03b1\u02dc\u03b2n(Ui) lies between \u03b1\u03b2n(Ui) and \u03b1\u02c6\u03b2n(Ui).\nExpanding the above (details omitted), using lemma 2 and 3, Cauchy-Schwarz inequality\nand condition (C), we can obtain\n\u2016n\u22121\u22072(Q\u02c6n(\u03b2n0)\u2212Qn(\u03b2n0))\u2016 \u2264 OP\n(\npn\n(\nhp+1 +\n1\u221a\nnh\n))\n,\nand this yields the second equation in the lemma.\nNow assume h = O(n\u22121\/(2p+3)) and p4n\/n\n(2p+2)\/(2p+3). Given \u000f > 0,\nP\n(\npnn\n1\/(4p+6)\n\u2225\u2225\u2225\u22251n\u22072Qn(\u03b2n0) + In(\u03b2n0)\n\u2225\u2225\u2225\u2225 \u2265 \u000f)\n\u2264 p\n2\nnn\n1\/(2p+3)\nn2\u000f2\nE0\npn\u2211\ni,j=1\n{\n\u22022Qn(\u03b2n0)\n\u2202\u03b2ni\u2202\u03b2nj\n\u2212 E0\u2202\n2Qn(\u03b2n0)\n\u2202\u03b2ni\u2202\u03b2nj\n}2\n= O\n(\np4n\nn(2p+2)\/(2p+3)\n)\n= o(1)\nwhich proves the third equation. The fourth one follows from similar arguments as\nbefore. \u0003\nProof of lemma 16. In expression (4), we set p = 0, which effectively assumes \u03b1\u03b2n(Ui) \u2248\n\u03b1\u03b2n(u) for Ui in a neighborhood of u. Using the same notation as in the proof of lemma\n6, we have \u03b1\u00afni(u) = \u03b1\u03b2n(u)\nTXi + Z\nT\nni\u03b2n, \u03b2\u02c6\n\u2217\n= c\u22121n (a\u02c60\u03b2n(u) \u2212 \u03b1\u03b2n(u)) and X\u2217i = Xi.\nFollowing the proof of lemma 6, we arrive at equation (19), which in this case is reduced\nto\nn\u2211\ni=1\nq2(X\nT\ni a\u02c60\u03b2n(u) + Z\nT\nni\u03b2n, Yni)\n(\nZnij +\n(\n\u2202a\u02c60\u03b2n(u)\n\u2202\u03b2nj\n)T\nXi\n)\nXiKh(Ui \u2212 u) = 0.\nSolving for\n\u2202a\u02c60\u03b2n (u)\n\u2202\u03b2n\nfrom the above equation, which is true for j = 1, \u00b7 \u00b7 \u00b7 , pn, we get the\nsame expression as given in the lemma.\nHence it remains to show that\n\u2202a\u02c60\u03b2n (u)\n\u2202\u03b2n\nis a consistent estimator of \u03b1\u2032\u03b2n(u). However\nthis is done by the proof of lemma 6 already, where equation (20) becomes\u2225\u2225\u2225\u2225\u2202a\u02c60\u03b2n(u)\u2202\u03b2n \u2212 \u03b1\u02c6\u2032\u03b2n(u)\n\u2225\u2225\u2225\u2225 = OP (\u221apn(h+ 1\u221anh\n))\n= oP (1)\n41\nand the proof completes. \u0003\nProof of lemma 9. Since BnB\nT\nn = Ipn\u2212l, for each v \u2208 Rpn\u2212l, we have\n(29) \u2016BTnv\u2016 \u2264 \u2016v\u2016.\nFollowing the proof of Theorem 1, we still have \u2016BTn (\u03b3\u02c6n \u2212 \u03b3n)\u2016 = OP\n(\u221a\npn\nn\n)\nwhen\nnh2p+2 = O(1) (resp. \u2016BTn (\u03b3\u02c6n \u2212 \u03b3n)\u2016 = OP\n(\u221a\npn\nn\n\u00b7 n1\/(4p+6)) when h = O(n\u22121\/(2p+3))).\nHence under p5n\/n\u2192 0 (resp. p5n\/n(2p+1)\/(2p+3)), following the proof of Theorem 2,\nIn(\u03b2n0)B\nT\nn (\u03b3\u02c6n \u2212 \u03b3n0) =\n1\nn\n\u2207Q\u02c6n(\u03b2n0) + oP\n(\n1\u221a\nn\n)\nlemma8\u21d2 In(\u03b2n0)BTn (\u03b3\u02c6n \u2212 \u03b3n0) =\n1\nn\n\u2207Qn(\u03b2n0) + oP\n(\n1\u221a\nn\n)\nEqn.(29)\u21d2 BnIn(\u03b2n0)BTn (\u03b3\u02c6n \u2212 \u03b3n0) =\n1\nn\nBn\u2207Qn(\u03b2n0) + oP\n(\n1\u221a\nn\n)\n\u21d2 BTn (\u03b3\u02c6n \u2212 \u03b3n0) =\n1\nn\nBTn (BnIn(\u03b2n0)B\nT\nn )\n\u22121Bn\u2207Qn(\u03b2n0) + oP\n(\n1\u221a\nn\n)\n,\nwhere the last line is true since BnIn(\u03b2n0)B\nT\nn has eigenvalues uniformly bounded away\nfrom 0 and infinity, like In(\u03b2n0) does. \u0003\nProof of lemma 10. First we assume nh2p+2 = O(1). By Taylor\u2019s expansion and Cauchy-\nSchwarz inequality,\n1\nn2\n\u2016\u22072Q\u02c6n(\u03b2\u02c6n)\u2212\u22072Q\u02c6n(\u03b2n0)\u20162 \u2264\n1\nn2\n\u2225\u2225\u2225\u2207T (\u22072Q\u02c6n(\u03b2\u2217n))\u2225\u2225\u22252 \u00b7 \u2016\u03b2\u02c6n \u2212 \u03b2n0\u20162\n=\n1\nn2\nOP (n\n2p3n) \u00b7OP\n(pn\nn\n)\n= OP\n(\np4n\nn\n)\n= oP\n(\n1\npn\n)\n,\nwhere \u03b2\u2217n lies between \u03b2\u02c6n and \u03b2n0. The second line follows from the result of Theorem\n1 and the proof of order for |I\u02c63| in the Theorem.\nIf h = O(n\u22121\/(2p+3)), then\n1\nn2\n\u2016\u22072Q\u02c6n(\u03b2\u02c6n)\u2212\u22072Q\u02c6n(\u03b2n0)\u20162 \u2264\n1\nn2\n\u2225\u2225\u2225\u2207T (\u22072Q\u02c6n(\u03b2\u2217n))\u2225\u2225\u22252 \u00b7 \u2016\u03b2\u02c6n \u2212 \u03b2n0\u20162\n=\n1\nn2\nOP (n\n2p3n) \u00b7OP\n(pn\nn\n\u00b7 n1\/(2p+3)\n)\n= OP\n(\np4n\nn\n\u00b7 n1\/(2p+3)\n)\n= oP\n(\n1\npn\n)\n,\n42\nwhere the second line follows from the proof of Theorem 1 again. The last line holds\nsince we assumed p5n\/n\n(2p+2)\/(2p+3) \u2192 0. \u0003\nProof of lemma 11. By Taylor\u2019s expansion, expanding Q\u02c6(BTn \u03b3\u02c6n) at \u03b2\u02c6n,\nQ\u02c6n(\u03b2\u02c6n)\u2212 Q\u02c6n(BTn \u03b3\u02c6n) = \u2207T Q\u02c6n(\u03b2\u02c6n)(\u03b2\u02c6n \u2212BTn \u03b3\u02c6n)\n\u2212 1\n2\n(\u03b2\u02c6n \u2212BTn \u03b3\u02c6n)T\u22072Q\u02c6n(\u03b2\u02c6n)(\u03b2\u02c6n \u2212BTn \u03b3\u02c6n)\n+\n1\n6\n\u2207{(\u03b2\u02c6n \u2212BTn \u03b3\u02c6n)T\u22072Q\u02c6n(\u03b2\u2217n)(\u03b2\u02c6n \u2212BTn \u03b3\u02c6n)}(\u03b2\u02c6n \u2212BTn \u03b3\u02c6n)\n:= T1 + T2 + T3.\nNote T1 = 0 by definition of \u03b2\u02c6n. Denote \u0398n = In(\u03b2n0) and \u03a6n =\n1\nn\n\u2207Qn(\u03b2n0). Using\nequation (25) and noting that \u0398n has eigenvalues uniformly bounded away from 0 and\ninfinity (condition (B)), we have\n\u03b2\u02c6n \u2212 \u03b2n0 = \u0398\u22121n \u03a6n + oP\n(\n1\u221a\nn\n)\n.\nCombining this with lemma 9, under the null hypothesis H0,\n\u03b2\u02c6n \u2212BTn \u03b3\u02c6n = \u0398\u22121\/2n {In \u2212\u03981\/2n BTn (Bn\u0398nBTn )\u22121Bn\u03981\/2n }\u0398\u22121\/2n \u03a6n\n+ oP (n\n\u22121\/2).\n(30)\nBut Sn := In \u2212 \u03981\/2n BTn (Bn\u0398nBTn )\u22121Bn\u03981\/2n is a pn \u00d7 pn idempotent matrix with rank\npn \u2212 (pn \u2212 l) = l, it follows by s standard argument that\n\u2016\u03b2\u02c6n \u2212BTn \u03b3\u02c6n\u2016 = OP\n(\u221a\nl\nn\n)\n.\nHence using similar argument as in the approximation of order for |I\u02c63| in Theorem 1,\nwe have\n|T3| = OP (np3\/2n ) \u00b7 \u2016\u03b2\u02c6n \u2212BTn \u03b3\u02c6n\u20163\n= OP\n(\nnp3\/2n \u00b7\nl3\/2\nn3\/2\n)\n= OP\n(\np\n3\/2\nn l3\/2\u221a\nn\n)\n= oP (1).\nHence\nQ\u02c6n(\u03b2\u02c6n)\u2212 Q\u02c6(BTn \u03b3\u02c6n) = T2 + oP (1).\n43\nFinally by lemma 16 and 10, we have\n\u2225\u2225\u2225\u222512(\u03b2\u02c6n \u2212BTn \u03b3\u02c6n){\u22072Q\u02c6n(\u03b2\u02c6n) + nIn(\u03b2n0)}(\u03b2\u02c6n \u2212BTn \u03b3\u02c6n)\n\u2225\u2225\u2225\u2225\n\u2264 OP\n(\nl\nn\n)\n\u00b7 n\n{\noP\n(\n1\u221a\npn\n)\n+OP\n(\npn\n(\nhp+1 +\n1\u221a\nnh\n))}\n= oP\n(\nl\u221a\npn\n)\n+OP\n(\nlpn\n(\nhp+1 +\n1\u221a\nnh\n))\n= op(1),\nand the conclusion of the lemma follows. \u0003\nProof of lemma 12. Consider\nn\u22121\u2016\u22072Qn(\u03b2n)\u2212\u22072Qn(\u03b2n0)\u20162 =\n1\nn2\npn\u2211\ni,j=1\n(\n\u22022Qn(\u03b2\u02c6n)\n\u2202\u03b2ni\u2202\u03b2nj\n\u2212 \u2202\n2Qn(\u03b2n0)\n\u2202\u03b2ni\u2202\u03b2nj\n)2\n=\n1\nn2\npn\u2211\ni,j=1\n(\npn\u2211\nk=1\n\u22023Qn(\u03b2\n\u2217)\n\u2202\u03b2ni\u2202\u03b2nj\u2202\u03b2nk\n(\u03b2\u02c6nk \u2212 \u03b20k)\n)2\n\u2264 1\nn2\npn\u2211\ni,j=1\npn\u2211\nk=1\n(\n\u22023Qn(\u03b2\n\u2217)\n\u2202\u03b2ni\u2202\u03b2nj\u2202\u03b2nk\n)2\n\u2016\u03b2\u02c6nk \u2212 \u03b20k\u20162,\nwhere \u03b2\u2217 lies between \u03b2\u02c6n and \u03b2n0. Similar to approximating the order of I\u02c63 in the proof\nof Theorem 1, the last line of the above equation is less than or equal to\n(31)\n1\nn2\nOp(n\n2p3n)\u2016\u03b2\u02c6n \u2212 \u03b2n0\u20162.\nIf nh2p+2 = O(1), then by Theorem 1, we have \u2016\u03b2\u02c6n \u2212 \u03b2n0\u2016 = OP\n(\u221a\npn\nn\n)\n. Hence\n(31) =\n1\nn2\nOP (n\n2p3n)OP\n(pn\nn\n)\n= OP\n(\np4n\nn\n)\n= oP (1).\nIf h = O(n\u2212(2p+3), using similar arguments in the proof of Theorem 1 we have\n\u2016\u03b2\u02c6n \u2212 \u03b2n0\u2016 \u2264 OP\n(\u221a\npn\/n(2p+2)\/(2p+3)\n)\n. Hence\n(31) =\n1\nn2\nOP (n\n2p3n)OP\n(\npn\/n\n(2p+2)\/(2p+3)\n)\n= OP (p\n4\nn\/n\n(2p+2)\/(2p+3)) = oP (1). \u0003\n44\nProof of lemma 13. By Taylor\u2019s expansion,\n1\nn\n\u2202\n\u2202\u03b2nk\n(\u2207Q\u02c6n(\u03b2n)\u2212\u2207Qn(\u03b2n))\n=\n1\nn\nn\u2211\ni=1\nq3(m\u02dcni(\u03b2n), Yni)(Znik +\n(\n\u2202\u03b1\u02dc\u03b2n(Ui)\n\u2202\u03b2nk\n)T\nXi)(Zni + \u03b1\u02dc\n\u2032\n\u03b2n\n(Ui)Xi)\n\u00d7XTi (\u03b1\u02c6\u03b2n(Ui)\u2212\u03b1\u03b2n(Ui))\n+\n1\nn\nn\u2211\ni=1\nq2(m\u02dcni(\u03b2n), Yni)\n(\n\u2202\u03b1\u02dc\u2032\u03b2n(Ui)\n\u2202\u03b2nk\n)\nXiX\nT\ni (\u03b1\u02c6\u03b2n(Ui)\u2212\u03b1\u03b2n(Ui))\n+\n1\nn\nn\u2211\ni=1\nq2(m\u02dcni(\u03b2n), Yni)(Zni + \u03b1\u02dc\n\u2032\n\u03b2n\n(Ui)Xi)X\nT\ni\n(\n\u2202\u03b1\u02c6\u03b2n(Ui)\n\u2202\u03b2nk\n\u2212 \u2202\u03b1\u03b2n(Ui)\n\u2202\u03b2nk\n)\n+\n1\nn\nn\u2211\ni=1\nq2(m\u02dcni(\u03b2n), Yni)(Znik +\n(\n\u2202\u03b1\u02dc\u03b2n(Ui)\n\u2202\u03b2nk\n)T\nXi)(\u03b1\u02c6\n\u2032\n\u03b2n\n(Ui)\u2212 \u03b1\u02c6\u2032\u03b2n(Ui))Xi\n+\n1\nn\nn\u2211\ni=1\nq1(m\u02dcni(\u03b2n), Yni)\n(\n\u2202\u03b1\u02c6\u2032\u03b2n(Ui)\n\u2202\u03b2nk\n\u2212 \u03b1\n\u2032\n\u03b2n\n(Ui)\n\u2202\u03b2nk\n)\nXi,\nwhere m\u02dcni(\u03b2n) = \u03b1\u02dc\u03b2n(Ui)\nTXi+Z\nT\nni\u03b2n, with \u03b1\u02dc\u03b2n(Ui) lies between \u03b1\u02c6\u03b2n(Ui) and \u03b1\u03b2n(Ui).\nBy lemmas 6 and 7, the main order of the above sum comes from the non-tilde version\nof individual terms in the sum. Together with regularity conditions (A) and (C),\u2225\u2225\u2225\u22251n \u2202\u2202\u03b2nk (\u2207Q\u02c6n(\u03b2n)\u2212\u2207Qn(\u03b2n))\n\u2225\u2225\u2225\u2225\n\u2264 O(1) \u00b7\n(\nsup\ni\n\u2016\u03b1\u02c6\u03b2n(Ui)\u2212\u03b1\u03b2n(Ui)\u2016+ sup\ni\n\u2225\u2225\u2225\u2225\u2202\u03b1\u02c6\u03b2n(Ui)\u2202\u03b2nk \u2212 \u2202\u03b1\u03b2n(Ui)\u2202\u03b2nk\n\u2225\u2225\u2225\u2225\n+ sup\ni\n\u2016\u03b1\u02c6\u2032\u03b2n(Ui)\u2212 \u03b1\u02c6\u2032\u03b2n(Ui)\u2016+ sup\ni\n\u2225\u2225\u2225\u2225\u2202\u03b1\u02c6\u2032\u03b2n(Ui)\u2202\u03b2nk \u2212 \u03b1\n\u2032\n\u03b2n\n(Ui)\n\u2202\u03b2nk\n\u2225\u2225\u2225\u2225)\n\u2264 O(1)oP\n(\u221a\npn\n(\nhp+1 +\n1\u221a\nnh\n))\n,\nwhere the last line follows from lemma 6. Hence\nn\u22121\u2016\u22072Q\u02c6n(\u03b2n)\u2212\u22072Qn(\u03b2n)\u2016 \u2264 oP\n(\npn\n(\nhp+1 +\n1\u221a\nnh\n))\n= oP (1)\nwhich follows from conditions on h in the lemma. \u0003\nReferences\n[1] Ahmad, I., Leelahanon, S. and Li, Q. (2005). Efficient Estimation of a Semiparametric Partially\nLinear Varying Coefficient Model. Ann. Statist., 33, 258\u2013283.\n45\n[2] Albright, S.C., Winston, W.L. and Zappe, C.J. (1999). Data Analysis and Decision Making with\nMicrosoft Excel. Duxbury, Pacific Grove, CA.\n[3] Cai, Z., Fan, J. and Li, R. (2000). Efficient Estimation and Inferences for Varying-Coefficient\nModels. J. Amer. Statist. Assoc., 95, 888\u2013902.\n[4] Carroll, R.J., Fan, J., Gijbels, I. and Wand, M.P. (1997). Generalized Partially Linear Single-Index\nModels. J. Amer. Statist. Assoc., 92, 477\u2013489.\n[5] Donoho, D.L., High-Dimensional Data Analysis: The Curses and Blessings of Dimensionality.\nLecture on August 8, 2000, to the American Mathematical Society \u201dMath Challenges of the 21st\nCentury\u201d.\n[6] Engle, R.F., Granger, C.W.J., Rice, J. and Weiss, A. (1986). Semiparametric estimates of the\nrelation between weather and electricity sales. J. Amer. Statist. Assoc., 81, 310\u2013320.\n[7] Fan, J. and Gijbels, I. (1996). Local Polynomial Modelling and Its Applications. New York:\nChapman and Hall.\n[8] Fan, J. and Huang, T. (2005). Profile Likelihood Inferences on Semiparametric Varying-Coefficient\nPartially Linear Models. Bernoulli., 11, 1031\u20131057.\n[9] Fan, J. and Jiang, J. (1999). Variable bandwidth and One-step Local M-Estimator. Science in\nChina, 29, 1\u201315; (English series) 2000, 35, 65 \u2013 80.\n[10] Fan, J. and Li, R. (2006). Statistical Challenges with High Dimensionality: Feature Selection in\nKnowledge Discovery. Proceedings of the Madrid International Congress of Mathematicians 2006.\nTo appear.\n[11] Fan, J. and Peng, H. (2004). Nonconcave penalized likelihood with a diverging number of para-\nmeters. Ann. Statist., 32, 928\u2013961.\n[12] Fan, J., Tam, P., Vande Woude, G. and Ren, Y. (2004) Normalization and analysis of cDNA\nmicroarrays using within-array replications applied to neuroblastoma cell response to a cytokine.\nProc. Natl. Acad. Sci. USA, 101, 1135\u20131140.\n[13] Fan, J., Zhang, C. and Zhang, J. (2001). Generalized Likelihood Ratio Statistics and Wilks\nPhenomenon. Ann. Statist., 29, 153\u2013193.\n[14] Ha\u00a8rdle, W., Liang, H. and Gao, J.T. (2000). Partially Linear Models. Springer-Verlag, New York.\n[15] Hastie, T.J. and Tibshirani, R. (1993). Varying-coefficient models. J. R. Statist. Soc. B, 55,\n757\u2013796.\n[16] Hu, Z., Wang, N. and Carroll, R.J. (2004). Profile-kernel versus backfitting in the partially linear\nmodels for longitudinal\/clustered data. Biometrika, 91, 251\u2013262.\n[17] Huber, P.J. (1981). Robust Statistics. New York: John Wiley & Sons.\n46\n[18] Huber, P.J. (1973). Robust Regression: Asymptotics, Conjectures and Monte Carlo. Ann. Statist.,\n1, 799\u2013821.\n[19] Jain, N. and Marcus, M. (1975). Central Limit Theorems for C(S)-valued Random Variables. J.\nFunct. Anal., 19, 216\u2013231.\n[20] Kauermann, G. and Carroll, R.J. (2001). A note on the efficiency of sandwich covariance matrix\nestimation. J. Amer. Statist. Assoc., 96, 1387\u20131396.\n[21] Li, Q., Huang, C.J., Li., D. and Fu, T.T. (2002). Semiparametric smooth coefficient models. J.\nBus. Econom. Statist., 20, 412\u2013422.\n[22] Li, R. and Liang, H. (2005). Variable Selection in Semiparametric Regression Modeling. To\nappear.\n[23] Lin, X. and Carroll, R.J. (2006). Semiparametric estimation in general repeated measures prob-\nlems. J. R. Statist. Soc. B, 68, Part 1, 69\u201388.\n[24] McCullagh, P. and Nelder, J.A. (1989). Generalized Linear Models (2nd ed.), London: Chapman\nand Hall.\n[25] Pollard, D. (1991). Asymptotics for least absolute deviation regression estimators. Econ. Theory,\n7, 186\u2013199.\n[26] Portnoy, S. (1988). Asymptotic Behavior of Likelihood Methods for Exponential Families When\nthe Number of Parameters Tends to Infinity. Ann. Statist., 16, 356\u2013366.\n[27] Severini, T.A. and Staniswalis, J.G. (1994). Quasi-likelihood Estimation in Semiparametric Mod-\nels. J. Amer. Statist. Assoc., 89, 501\u2013511.\n[28] Speckman, P. (1988). Kernel smoothing in partial linear models. J. R. Statist. Soc. B, 50,\n413\u2013436.\n[29] Van Der Vaart, A.W. (1998). Asymptotic Statistics. Cambridge Univ. Press.\n[30] Wahba, G. (1984) Partial spline models for semiparametric estimation of functions of several\nvariables. In Statistical Analysis of Time Series, Proceedings of the Japan U.S. Joint Seminar,\nTokyo, 319\u2013329. Institute of Statistical Mathematics, Tokyo.\n[31] Wong, W.H. and Severini, T.A. (1992). Profile Likelihood and Conditionally Parametric Models.\nAnn. Statist., 20, 1768\u20131802.\n[32] Xia, Y., Zhang, W. and Tong, H. (2004). Efficient estimation for semivarying-coefficient models.\nBiometrika, 91, 661\u2013681.\n[33] Yatchew, A. (1997). An elementary estimator for the partially linear model. Economics Letters,\n57, 135\u2013143.\n[34] Zhang, W., Lee, S.Y., and Song, X.Y. (2002). Local Polynomial fitting in semivarying coefficient\nmodel. J. Mult. Anal., 82, 166\u2013188.\n47\n"}