{"doi":"10.1002\/pst.321","coreId":"71220","oai":"oai:eprints.lancs.ac.uk:4494","identifiers":["oai:eprints.lancs.ac.uk:4494","10.1002\/pst.321"],"title":"Confidence Intervals for Ratios of AUC's in the Case of Serial Sampling : A Comparison of Seven Methods.","authors":["Jaki, Thomas","Wolfsegger, Martin J.","Ploner, Meinhard"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2009-01","abstract":"Pharmacokinetic studies are commonly performed using the two-stage approach. The first stage involves estimation of pharmacokinetic parameters like the area under the concentration versus time curve (AUC) for each analysis subject separately and the second stage uses the individual parameter estimates for statistical inference. This two-stage approach is not applicable in sparse sampling situations where only one sample is available per analysis subject like in non-clinical in-vivo studies. In a serial sampling design only one sample is taken from each analysis subject. A simulation study was carried out to assess coverage, power and type I error of seven methods to construct two-sided 90% confidence intervals for ratios of two AUC's assessed in a serial sampling design, which can be used to assess bioequivalence in this parameter","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/71220.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/4494\/3\/AUCRatio.pdf","pdfHashValue":"80022ef03ff31b8c10cf33f51d67032533048e8d","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:4494<\/identifier><datestamp>\n      2018-01-24T03:13:00Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Confidence Intervals for Ratios of AUC's in the Case of Serial Sampling : A Comparison of Seven Methods.<\/dc:title><dc:creator>\n        Jaki, Thomas<\/dc:creator><dc:creator>\n        Wolfsegger, Martin J.<\/dc:creator><dc:creator>\n        Ploner, Meinhard<\/dc:creator><dc:subject>\n        QA Mathematics<\/dc:subject><dc:description>\n        Pharmacokinetic studies are commonly performed using the two-stage approach. The first stage involves estimation of pharmacokinetic parameters like the area under the concentration versus time curve (AUC) for each analysis subject separately and the second stage uses the individual parameter estimates for statistical inference. This two-stage approach is not applicable in sparse sampling situations where only one sample is available per analysis subject like in non-clinical in-vivo studies. In a serial sampling design only one sample is taken from each analysis subject. A simulation study was carried out to assess coverage, power and type I error of seven methods to construct two-sided 90% confidence intervals for ratios of two AUC's assessed in a serial sampling design, which can be used to assess bioequivalence in this parameter.<\/dc:description><dc:date>\n        2009-01<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/4494\/3\/AUCRatio.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1002\/pst.321<\/dc:relation><dc:identifier>\n        Jaki, Thomas and Wolfsegger, Martin J. and Ploner, Meinhard (2009) Confidence Intervals for Ratios of AUC's in the Case of Serial Sampling : A Comparison of Seven Methods. Pharmaceutical Statistics, 8 (1). pp. 12-24. ISSN 1539-1604<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/4494\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1002\/pst.321","http:\/\/eprints.lancs.ac.uk\/4494\/"],"year":2009,"topics":["QA Mathematics"],"subject":["Journal Article","PeerReviewed"],"fullText":"Confidence Intervals for Ratios of\nAUC\u2019s in the Case of Serial Sampling:\nA Comparison of Seven Methods\nThomas Jaki\nDepartment of Mathematics and Statistics,\nLancaster University, United Kingdom\nMartin J. Wolfsegger\nGlobal Biopharm Preclinical Research and Development,\nBaxter AG, Vienna, Austria\nMeinhard Ploner\nwww.data-ploner.com, Brunico, Italy\nTo whom correspondence should be addressed: E-mail: jaki.thomas@gmail.com\nThis is a preprint of an article accepted for publication in\nPharmaceutical Statistics c\u00a92008 John Wiley & Sons, Ltd.\nMarch 19, 2008\nSummary\nPharmacokinetic studies are commonly performed using the two-stage approach.\nThe first stage involves estimation of pharmacokinetic parameters like the area under\nthe concentration versus time curve (AUC) for each analysis subject separately and\nthe second stage uses the individual parameter estimates for statistical inference.\nThis two-stage approach is not applicable in sparse sampling situations where only\none sample is available per analysis subject like in non-clinical in-vivo studies. In\na serial sampling design only one sample is taken from each analysis subject. A\nsimulation study was carried out to assess coverage, power and type I error of seven\nmethods to construct two-sided 90% confidence intervals for ratios of two AUC\u2019s\nassessed in a serial sampling design, which can be used to assess bioequivalence in\nthis parameter.\nKeywords: AUC; bioequivalence; bootstrap; serial sampling design; serial sac-\nrifice design; sparse sampling\n1 Introduction\nA formula to calculate a 1\u2212\u03b1 confidence interval for the area under the concentration\nversus time curve (AUC) from 0 to the last observed time point assessed in a serial\nsampling design for normally distributed errors based on the linear trapezoidal rule\nis presented in [1]. Procedures based on the t-distribution are presented in [2] and in\n[3]. A simulation study for these approaches can be found in [4]. Wolfsegger & Jaki\nin [5] derive an estimator and a confidence interval for the AUC from 0 to infinity\n(AUC0\u2212\u221e) for a serial sampling design.\nHeinzl [6] presented a test for the null hypothesis of no difference between two\nAUC\u2019s from 0 to the last time point using the critical value from a t-distribution\nwhile Bailer & Ruberg [7] propose a permutation test for this null hypothesis using\na z-statistic as the test statistic to be resampled.\n1\nIn this note we will consider the ratio of two AUC\u2019s and the corresponding confi-\ndence intervals which are often assessed in bioequivalence studies using the confidence\ninterval inclusion approach. One of the main advantages of considering ratios instead\nof the differences is interpretability. While the same conclusions can be obtained from\nboth approaches it is often easier to discuss ratios. The problem of bioequivalency is\ndiscussed in great detail in the FDA guideline \u2018Statistical Approaches to Establishing\nBioequivalence\u2019 [8]. The guideline also addresses in-vitro and in-vivo studies for in-\nvestigational new drug applications in which sparse sampling may arise. Hu et al. [9]\npresent a modelling approach, by ways of a nonlinear model, to assess bioequivalence\nor PK similarity for parameters estimated in a serial sampling design. Wolfsegger\n[10] presented three methods for calculation of a 1\u2212\u03b1 confidence interval for the ratio\nof two AUC\u2019s from zero to the last time point without assuming a specific nonlinear\nmodel.\nWe will proceed by giving an overview of the ratio of AUC\u2019s in serial sampling\ndesigns and its estimation followed by a brief summary of seven methods to construct\nconfidence intervals considered in this simulation study. In Section 4 we will describe\nthe simulation study conducted in detail while Section 5 provides the simulation\nresults. We will conclude with an example and a brief discussion of the findings and\nfuture directions.\n2 Serial Sampling Design\nConsider a study with two treatment groups, k, in which measurements are taken\nat J time points, tj (1 \u2264 j \u2264 J), and at each time point blood is sampled from\nnj analysis subjects. It is assumed that the time points are the same for the two\ntreatment groups and that each subject is only sampled once across all timepoints.\nThis design leads to independent random variables, both per time point as well\nas between time points. Let Xijk be the measured drug concentration from the\n2\nith analysis subject at time tj receiving the treatment k. Let E[Xijk] = \u00b5jk and\nV [Xijk] = \u03c3\n2\njk be the population mean and population variance at time point tj. The\ngeneral heteroscedastic model then is defined as\nXijk = \u00b5jk + \u000fijk (1)\nwhere the errors, \u000fijk, are identical and independently distributed with continuous\ndistribution Gjk and the range of Xijk is effectively positive. The theoretical AUC\nfrom 0 to the last time point for a specific treatment to be estimated can be defined\nas\nAUCk =\nJ\u2211\nj=1\nwj\u00b5jk. (2)\nUsing the linear trapezoidal rule, the weights wj equal\nw1 =\n1\n2\n(t2 \u2212 t1)\nwj =\n1\n2\n(tj+1 \u2212 tj\u22121) (2 \u2264 j \u2264 J \u2212 1) (3)\nwJ =\n1\n2\n(tJ \u2212 tJ\u22121).\nThis AUC can be estimated by\nA\u0302UCk =\nJ\u2211\nj=1\nwjX\u00afjk (4)\nwith V [A\u0302UCk] =\n\u2211J\nj=1w\n2\nj\u03c3\n2\njkn\n\u22121\njk where X\u00afjk =\n1\nnjk\n\u2211njk\ni=1 Xijk represents the arith-\nmetic mean at time point tj in the k\nth treatment group.\nIn this note the parameter of interest therefore is defined as\n\u2206 =\nJ\u2211\nj=1\nwj\u00b5j1\nJ\u2211\nj=1\nwj\u00b5j2\n(5)\n3\nand will be estimated by\n\u2206\u0302 =\nJ\u2211\nj=1\nwjX\u00afj1\nJ\u2211\nj=1\nwjX\u00afj2\n. (6)\nWolfsegger [10] shows that\n\u221a\nn(\u2206\u02c6 \u2212\u2206) is asymptotically normal distributed. A\nsimilar approach can be used to estimate the ratio of AUCs from 0 to infinity by\nreplacing the presented formulas for the AUC with the respective formulas for the\nAUC from 0 to infinity in [5]. An approximate 1\u2212\u03b1 confidence interval for the ratio\ncan be obtained by application of Fieller\u2019s theorem [11] based on the asymptotic\nnormal distribution and corresponding standard errors.\n3 Methods Compared\nIn this section we will give a brief overview of the seven different methods to con-\nstruct (approximate) 1\u2212\u03b1 confidence intervals that are considered in the simulation\nstudy of Section 4. We will denote the parameter of interest by \u03b8 and a standard\nestimator for it (e. g. maximum likelihood estimator) by \u03b8\u02c6. Further, in accordance\nto Davison & Hinkley [12] and others, we will use the \u2018star\u2019 notation to indicate\nbootstrap based estimators. Therefore, \u03b8\u02c6\u2217(b) is the bth bootstrap replication of the\nestimator \u03b8\u02c6. The \u03b1-percentile of the set of bootstrap replicates {\u03b8\u02c6\u2217(b), b = 1, . . . , B}\nwill be denoted by \u03b8\u02c6\u2217\u03b1.\nThe simplest and probably most often used resampling method to construct con-\nfidence intervals among practitioners is the percentile method [13, pp. 170-177]. This\nmethod assumes that for an unknown monotone increasing transformation h(\u03b8), a\nstatement of the type\nh(\u03b8\u02c6)\u2212 h(\u03b8) \u223c N(0, \u03c32\nh(\u03b8\u02c6)\n)\n4\nholds. A simple approximate 1\u2212 \u03b1 confidence interval can be found as\n[\u03b8\u02c6\u2217\u03b1\n2\n; \u03b8\u02c6\u22171\u2212\u03b1\n2\n]. (7)\nAn improvement upon the percentile method was introduced by Efron [14] in the\nbias-corrected and accelerated percentile method, BCa. As before this method has\nthe assumption of the existence of an unknown monotone increasing transformation,\nh(\u03b8), for which the statement\nh(\u03b8\u02c6)\u2212 h(\u03b8) \u223c N(\u2212z0\u03c3h(\u03b8), \u03c32h(\u03b8))\nholds. z0 denotes a constant bias correction factor and the relationship \u03c3h(\u03b8) =\n1 \u2212 ah(\u03b8) holds for some a, the skewness correction factor (also referred to as the\nacceleration constant). An approximate 1 \u2212 \u03b1 confidence interval therefore can be\nestimated as [\n\u03b8\u02c6\u2217\u03a6(zcorr\u03b1\n2\n); \u03b8\u02c6\n\u2217\n\u03a6(zcorr\n1\u2212\u03b12\n)\n]\n(8)\nwith zcorr\u03b1 = z0 +\nz0+z\u03b1\n1\u2212a(z0+z\u03b1) , \u03a6(x) the CDF of the standard normal distribution\nand z\u03b1 the corresponding \u03b1-percentile. While it is not necessary for both methods\ndescribed above to know the transformation h(.), they will fail if the transformation\nto a normal distribution is not possible.\nA different type of bootstrap intervals can be constructed based on pivotal quan-\ntities. The hybrid method [15] is the most intuitive of these approaches. The idea\nis to estimate the distribution of the pivot \u03b8\u02c6 \u2212 \u03b8 by the bootstrap distribution con-\nstructed upon \u03b8\u02c6\u2217(b)\u2212 \u03b8\u02c6, the bootstrap equivalent of the pivot. The confidence limits\nthen can be found to be\n[2\u03b8\u02c6 \u2212 \u03b8\u02c6\u22171\u2212\u03b1\n2\n; 2\u03b8\u02c6 \u2212 \u03b8\u02c6\u2217\u03b1\n2\n]. (9)\nAlthough this method has a totally different justification than the percentile\nmethod, it is easy to see the connection between both methods. The hybrid method,\nhowever, in this note is particularly interesting since the next method, called the\n5\nratio method is based upon its idea.\nFor the ratio method introduced in [16], we consider a different pivot, \u03b8\u02c6\n\u03b8\n, whose\ndistribution is to be approximated by the bootstrap equivalent quantity, \u03b8\u02c6\n\u2217(b)\n\u03b8\u02c6\nwhich\nyields [ \u03b8\u02c62\n\u03b8\u02c6\u22171\u2212\u03b1\n2\n;\n\u03b8\u02c62\n\u03b8\u02c6\u2217\u03b1\n2\n]\n(10)\nas the confidence bounds. A very particular feature of this method is that it\nshould only be used if the values of the statistic are strictly positive, a feature clearly\nprominent in the estimation of the AUC and ratio of two AUC\u2019s. It does, however,\nalso imply that the propagated error, which arises by every further iteration, emerges\nin an approximately multiplicative way. In other words, the method will work poorly\nif the distribution to be estimated deviates strongly from the estimate. This, fur-\nthermore, means that the sample sizes required tend to be large.\nThe assumptions of the bootstrap-t-interval [17] are less restrictive than those\nof the hybrid method, allowing the pivot to be of a more general form \u03b8\u02c6\u2212\u03b8\n\u03c3\u02c6\nwhose\ndistribution is to be estimated by the distribution of t\u2217 = \u03b8\u02c6\n\u2217(b)\u2212\u03b8\u02c6\n\u03c3\u02c6\u2217 . The resulting\nconfidence interval then becomes\n[\u03b8\u02c6 \u2212 t\u22171\u2212\u03b1\n2\n\u03c3\u02c6; \u03b8\u02c6 \u2212 t\u2217\u03b1\n2\n\u03c3\u02c6]. (11)\nNotice that, in order to obtain good results for this type of interval, in addition\nto the bootstrap statistic t\u2217 a separate estimator of the standard deviation, \u03c3\u02c6\u2217 (usu-\nally the jackknife estimator for \u03c3) is needed. To obtain this estimator generally one\nadditional \u2018layer\u2019 of resampling is necessary, making this method often more compu-\ntational intensive than the intervals presented before. Furthermore, this additional\nbootstrap layer may lead to poor estimates in the case of small sample sizes per\ntime point which is frequently the case in non-clinical in-vivo studies. In this study,\ninstead of a resampled estimator for \u03c3, we used the asymptotic standard deviation\n6\nderived in [10] to avoid these problems.\nThe last two methods considered in the manuscript differ from the previous meth-\nods significantly as they are not resampling based methods. The asymptotic confi-\ndence interval simply uses normal theory to obtain a z-type interval as\n[\u03b8\u02c6 + z\u03b1\n2\n\u03c3\u02c6\u03b8\u02c6; \u03b8\u02c6 + z1\u2212\u03b12 \u03c3\u02c6\u03b8\u02c6].\nThis technique has the implicit assumption that some form of a limiting theory\nholds for the statistic such that \u03b8\u02c6\u2212\u03b8\n\u03c3\u02c6\u03b8\u02c6\n\u223c. N(0, 1). This approach will result in a sym-\nmetric confidence interval around the observed effect which is often inappropriate\nin the case of ratios since the parameter space for ratios ranges from 0 to infinity.\nThe Fieller-type procedure, which is described in great detail in [10] for the ratio\nof AUC\u2019s, on the other hand uses a t-distribution to model the distribution of the\ncorresponding pivot. The degrees of freedom are approximated using Satterthwaite\u2019s\nmethod [18], yielding a complicated looking, but computationally straightforward,\nconfidence interval.\n4 Simulations\nThe following one-compartmental model with first order absorption and elimination\nafter extravascular administration (e. g. oral, intramuscular, rectal, etc.) was used\nfor data generation\nXij = f (tj) + \u000fij =\nkaFD\nV (ka \u2212 \u03bb)\n(\ne\u2212\u03bbtj \u2212 e\u2212katj)+ \u000fij (12)\nwith the parameterization \u03bb = 0.0693, ka = 0.231, V = 10, F = 1 and the dose\nD = 500. To eliminate the bias created by the linear trapezoidal rule to approx-\nimate the integral, the true AUC for treatment k was defined as in Equation (2)\nusing \u00b5jk = f(tj) =\nkaFD\nV (ka\u2212\u03bb)\n(\ne\u2212\u03bbtj \u2212 e\u2212katj) specified at baseline and ten time points\n(1h, 2h, 3h, 4h, 6h, 8h, 12h, 18h, 24h and 36h) post study drug administration.\n7\nBoth, model and time points, are taken from Gibaldi & Perrier, page 440 [19]. For\neach subject only one observation is generated across all time points to reflect the\nsparse sampling situation of the serial sampling design. As a result all generated\nobservations are independent of each other. Consequently missing data effectively\nreduces the sample size at specific time points, but need no special consideration in\nthe estimation of the AUC\u2019s in serial sampling designs as the presented procedures\nallow for unequal sample sizes per time point.\nNormal, log-normal and double exponential distributed errors were used for vari-\nous combinations of sample size (N=3, 5 and 10) and time point variabilities. Table\n1 shows the three variability scenarios that were studied which are inspired by real\ndata. The coefficient of variations at later time points tend to increase which might\nbe due to the inaccuracy of an assay to determine the drug concentration in the\nblood when dealing with values close to the limit of detection. 10000 simulation runs\n(yielding an estimation error of 0.003 for nominal coverage of 90%) were carried out\nfor each parameter setting with preselected sample sizes. Within each simulation\nrun, N random samples were generated for each of the specified time points. 1000\nbootstrap replications were used for bootstrap based confidence intervals.\nTable 1: Coefficient of Variations (%) Used for Simulations\nTime point\nScenario Baseline 1 to 18 hours 24 hours 36 hours\n1 0% 20% 20% 20%\n2 0% 40% 40% 40%\n3 0% 40% 80% 80%\nEmpirical coverage estimates are reported for a nominal coverage probability of\n1 \u2212 \u03b1 = 0.90. Empirical lower and upper tail probabilities presented additionally\nwere defined as the probability that the true ratio is below\/above the calculated\ntwo-sided 90% confidence interval for the ratio. Empirical type I errors, \u03b3, in the\nsense of bioequivalency are reported under non-equivalence for a nominal coverage\n8\nprobability of 1\u2212 \u03b1 = 0.90.\nEmpirical power and type I error estimates are presented using the conventional\nlimits of bioequivalence for ratio of averages ranging from 0.8 to 1.25. Expected val-\nues at different time points used to study \u2206 = 0.799 and \u2206 = 0.90 were determined\nby varying parameters \u03bb, ka, V , D and F accordingly.\nAll simulations were performed with R version 2.4.1 [20]. Empirical coverage,\npower and type I error for the methods compared for a given scenario and sample\nsize per time point were calculated on basis of the same simulation runs.\n4.1 Generation of Error Terms\nOur starting point to model the uncertainty in the AUC will be normally distributed\nerrors, since some of the theoretical results are based upon this assumption [10].\nAdditionally, log-normal distributed errors will be evaluated as drug levels cannot be\nnegative, while the upper end is open which may lead to a non-symmetrical distri-\nbution. The last distribution considered, a double exponential distribution, is used\nto represent frequent extreme observations. For all distributions we use the param-\neterization in Casella & Berger, pages 623-625 [21].\nThe variation in the drug levels at a given time point has been fixed in terms of\nthe dimensionless coefficient of variation, cv = \u03c3\/\u00b5, where \u00b5 is the mean drug level\nat a given time point and \u03c3 the corresponding standard deviation. While the mean\nlevel, \u00b5, is given by the true AUC the challenge is to find the parameters of the un-\nderlying distribution that yield a pre-specified coefficient of variation. For the normal\ndistribution, where the parameters of the distribution correspond to the moments\nof interest, the solution is simple and given by data generated from a N (\u00b5, \u00b52c2v)\n-distribution.\nTo find the parameters for the other two distributions, however, is more difficult.\n9\nLets first consider the log-normal case for which we let \u03b1 and \u03b2 be the mean and\nstandard deviation of the underlying normal distribution, respectively. The mean of\nthe distribution then is exp (\u03b1 + \u03b22\/2) and the coefficient of variation can be found\nto be\n\u221a\nexp (\u03b22)\u2212 1. To determine the proper values for \u03b1 and \u03b2 for given values of\n\u00b5 and cv we need to solve the system of equations\n\u00b5 = exp\n(\n\u03b1 + \u03b22\/2\n)\n(13)\ncv =\n\u221a\nexp (\u03b22)\u2212 1,\nwhich yields \u03b1 = ln (\u00b5) \u2212 1\n2\nln (1 + c2v) and \u03b2\n2 = ln (1 + c2v). The data for this\nmodel therefore can be obtained by generating data from a log-normal distribution\nwith the parameters from above.\nUsing the equivalent approach for the double exponential distribution parame-\nterized by \u03b1 and \u03b2 gives the set of equations\n\u00b5 = \u03b1 (14)\ncv =\n1\n\u03b1\n\u221a\n2\u03b22,\nwhich are solved by \u03b1 = \u00b5 and \u03b22 = 1\n2\n\u00b52c2v. Thus, we can generate the heavy\ntailed distribution by using a double exponential distribution with the respective\nparameters.\nIn the rare case that we generated a negative value using the normal and double\nexponential distribution, we chose to simply replace the value by a newly generated\nnonnegative observation. While we are aware that this will change the true underly-\ning structure of the model we found that the difference in \u00b5 and cv was negligible as\nonly a few observations had to be replaced. Further, this approach was applied for\nboth samples for which the ratio of AUC\u2019s was calculated and additional simulation\nstudies indicated that this replacement method has asymptotically no effect on the\n10\ntrue ratios studied.\n4.2 Contaminated Data\nAside from the \u2018clean\u2019 data situations described before, we also study contaminated\ndata. The first set of contaminations is thought to describe either erroneous data due\nto measurement errors or extreme drug-levels for a few subjects at some time points.\nIn our model we use the same normal and log-normal distributions as described in\nSection 4.1 with cv = 0.2 but have 10% of the data generated come from the same\ndistribution with the mean shifted by 3 standard deviations. For the normal distri-\nbution this yields a true cv of 0.254 while the coefficient of variation becomes 0.305\nfor log-normal data.\nThe second type of contamination studied is a change in the error distribution over\ntime. This is motivated by potentially different decomposition of the drug among\nsubjects. We will model this by having the initial error distribution be normal and\nchange it to double exponential for the last three time points. The coefficient of\nvariation will be 0.2 at all time points. Note that this approach is different to just\nchanging cv for the last 3 time points since this still has the coefficient of variation\nfixed while increasing the number of extreme values.\n5 Results\nTables 2 - 5 show some of the results of the simulation study for the \u2018clean\u2019 data for\nthe scenarios in Table 1. The omitted results are available from the authors upon\nrequest. For all combinations of sample size, distribution and \u2206, the empirical over-\nall coverage and the empirical power\/type-I-error is presented. Additionally left and\nright tail coverages are included as they are important for one sided hypothesis such\nas tests for non-inferiority. The power in this context is defined as the probability\nthat the confidence interval estimated is within 0.8 and 1.25, the conventional range\nfor bioequivalency.\n11\nThe bootstrap confidence intervals with the exception of the bootstrap-t-interval\nshow a clear pattern in terms of coverage for all parameter settings considered. For\na sample size of 3 per time point these intervals undercover severely, yielding only\nan empirical coverage of approximately 0.8 while they approach nominal coverage as\nsample size increase. The bootstrap-t-interval as well as the asymptotic and Fieller\ntype interval are superior in coverage than the other procedures investigated for all\nsample sizes and scenarios studied. Empirical coverage of the asymptotic proce-\ndure is slightly below the nominal coverage level for a sample size of 3 whereas the\nbootstrap-t and Fieller approach yield nominal coverage. In addition, the asymptotic\ninterval indicates imbalance in tail probabilities. On average, the Fieller approach\nis marginally more conservative in terms of coverage than the bootstrap-t approach\nwith double exponential errors and a sample size of 3 per time point. However, differ-\nences in power and type-I-error between the Fieller interval and bootstrap-t-interval\nare small across all sample size and error distributions considered.\nAnother interesting point is that there appears to be no influence of the error\ndistribution on the coverage of the different types of confidence intervals while the\npower clearly depends on the distributional shape and the values of the coefficient\nof variation across time points.\nA surprising result can be found for the hybrid intervals for which the empirical\ncoverage of the tails differ strongly such that the lower bound covers much better\nand, in fact almost always, yields the desired coverage on the lower tail, while badly\nundercovering on the upper tail. Interestingly the same behavior can not be found\nfor the related ratio method. While the poor coverage suggests a poor approximation\nof the distribution of the pivot by the bootstrap distribution, the resulting intervals\nappear to be more symmetric in probability.\nIn the simulation for equivalent true AUC\u2019s (\u2206 = 1), presented in Tables 2 and\n12\n5, the power quickly reaches one for a coefficient of variation of 0.2 and 0.4 as sample\nsize increases. An interesting side note here is that the double exponential errors\nyield a higher power than the other two error distributions considered. This may be\ndue to the effectively smaller value of cv obtained by replacing negative values by a\nnew, positive, random error.\nFor the situation of different AUC\u2019s that are still considered to be bioequivalent\n(\u2206 = 0.9) hardly any difference in the performance of the intervals dependent on the\nerror distribution can be found once again. Only for the high variation scenario a\nslight difference in power between the symmetric and the non-symmetric error dis-\ntribution can be seen (Table 5). The magnitude of this difference is at most 7%. It\nis notable, however, that the power of all intervals is markedly lower than for \u2206 = 1\nacross all sample sizes with up to 40% reduced power for scenario 3.\nThe most encouraging results for all the methods considered can be found for\nAUC\u2019s that are not considered bioequivalent. Even when the true parameter is only\n0.001 outside of the range of bioequivalency, all the confidence intervals reflect this\nyielding only a type-I-error, \u03b3, that is the probability to decide on equivalency when\nyou should not, of approximately 5%. Even more astonishing is that the value of \u03b3\nis stable across all choices of distribution, sample size and variations.\n5.1 Contamination\nWe will now look at the performance of the intervals for contaminated data. In\nterms of coverage the results mimic the pattern discussed for the \u2018clean\u2019 data. The\nbootstrap-t-interval, asymptotic and Fieller interval are superior in coverage to the\nother procedures investigated across all sample size and error distributions consid-\nered.\nThe power of the contaminated errors show the expected pattern as the power\n13\nTable 2: Empirical Coverage and Power for \u2206 = 1 Using a Nominal Coverage of 90% for\nScenario 1\nError distribution\nNormal Log-normal Double exponential\nN Method Coverage Power Coverage Power Coverage Power\n3 Percentile 0.8063 (0.9048;0.9015) 0.9459 0.8013 (0.9012;0.9001) 0.9505 0.7986 (0.9004;0.8982) 0.9373\nHybrid 0.8072 (0.9240;0.8832) 0.9450 0.7948 (0.9165;0.8783) 0.9459 0.8109 (0.9259;0.8850) 0.9378\nRatio 0.8067 (0.9049;0.9018) 0.9490 0.7977 (0.8987;0.8990) 0.9492 0.8091 (0.9055;0.9036) 0.9436\nBCa 0.8046 (0.9032;0.9014) 0.9464 0.8001 (0.9003;0.8998) 0.9493 0.7985 (0.9000;0.8985) 0.9338\nBoot-t 0.9110 (0.9579;0.9531) 0.8438 0.9110 (0.9579;0.9531) 0.8388 0.9118 (0.9602;0.9516) 0.7951\nAsymptotic 0.8835 (0.9514;0.9321) 0.8985 0.8796 (0.9474;0.9322) 0.9002 0.8857 (0.9543;0.9314) 0.8847\nFieller 0.9072 (0.9527;0.9545) 0.8699 0.9078 (0.9541;0.9537) 0.8718 0.9314 (0.9684;0.9630) 0.8245\n5 Percentile 0.8513 (0.9259;0.9254) 0.9934 0.8483 (0.9247;0.9236) 0.9952 0.8415 (0.9219;0.9196) 0.9913\nHybrid 0.8526 (0.9397;0.9129) 0.9937 0.8443 (0.9374;0.9069) 0.9945 0.8524 (0.9428;0.9096) 0.9921\nRatio 0.8525 (0.9261;0.9264) 0.9935 0.8469 (0.9251;0.9218) 0.9944 0.8526 (0.9273;0.9253) 0.9926\nBCa 0.8522 (0.9252;0.9270) 0.9930 0.8469 (0.9229;0.9240) 0.9948 0.8388 (0.9193;0.9195) 0.9898\nBoot-t 0.9040 (0.9534;0.9506) 0.9871 0.8999 (0.9524;0.9475) 0.9878 0.8897 (0.9471;0.9426) 0.9746\nAsymptotic 0.8925 (0.9526;0.9399) 0.9892 0.8899 (0.9531;0.9368) 0.9910 0.8907 (0.9533;0.9374) 0.9860\nFieller 0.9038 (0.9510;0.9528) 0.9873 0.9039 (0.9535;0.9504) 0.9896 0.9025 (0.9523;0.9502) 0.9840\n10 Percentile 0.8790 (0.9426;0.9364) 1.0000 0.8698 (0.9364;0.9334) 1.0000 0.8765 (0.9415;0.9350) 0.9999\nHybrid 0.8763 (0.9520;0.9243) 1.0000 0.8687 (0.9448;0.9239) 1.0000 0.8803 (0.9536;0.9267) 0.9999\nRatio 0.8802 (0.9430;0.9372) 1.0000 0.8689 (0.9351;0.9338) 1.0000 0.8807 (0.9442;0.9365) 0.9999\nBCa 0.8768 (0.9404;0.9364) 1.0000 0.8683 (0.9325;0.9358) 1.0000 0.8723 (0.9379;0.9344) 0.9999\nBoot-t 0.9019 (0.9538;0.9481) 1.0000 0.8936 (0.9480;0.9456) 1.0000 0.8945 (0.9504;0.9441) 0.9999\nAsymptotic 0.8985 (0.9560;0.9425) 1.0000 0.8882 (0.9496;0.9386) 1.0000 0.8988 (0.9567;0.9421) 0.9999\nFieller 0.9023 (0.9529;0.9494) 1.0000 0.8949 (0.9475;0.9474) 1.0000 0.9053 (0.9547;0.9506) 0.9999\nValues in parentheses refer to left and right tail probabilities\nPower is based on conventional limits for bioequivalency\nN is sample size per time point\nincreases with sample size for \u2206 = 1 and \u2206 = 0.9 while the type-I-error is rather\nstable at 5% for non-equivalent AUC\u2019s. The biggest and most surprising difference\ncan be seen between the error distributions. The contaminated log-normal distribu-\ntion (Table 6) has markedly better power for the values of the parameter that are\nconsidered bioequivalent than the contaminated normal distribution (Table available\nupon request). In fact the power of the contaminated log-normal distribution is up\nto 25% higher than for normally distributed errors and is almost identical to the\npower for the \u2018clean\u2019 normal errors. This result suggests that the power of the inter-\nvals is not so much influenced by the skewness of the error distribution than by the\nfrequency of outliers. Keeping this in mind it is striking that the same feature was\nnot seen in the initial simulations using \u2018clean\u2019 errors. This further indicates that the\nextreme tail needs to be very heavy in order to yield a reduction in power as seen here.\nThe results for changing error distribution once again show the familiar patterns\nand the details therefore have been omitted, but are available upon request. The\ncoverage of the bootstrap based methods converges in sample size toward nominal\nlevel for all these methods besides the bootstrap-t-interval whos coverage is on target\n14\nTable 3: Empirical Coverage and Type I Error for \u2206 = 0.799 Using a Nominal Coverage\nof 90% for Scenario 1\nError distribution\nNormal Log-normal Double exponential\nN Method Coverage \u03b3 Coverage \u03b3 Coverage \u03b3\n3 Percentile 0.8046 (0.9028;0.9018) 0.0949 0.8047 (0.9020;0.9027) 0.0946 0.7984 (0.8985;0.8999) 0.0961\nHybrid 0.8082 (0.9235;0.8847) 0.1107 0.8036 (0.9194;0.8842) 0.1123 0.8090 (0.9222;0.8868) 0.1090\nRatio 0.8084 (0.9036;0.9048) 0.0921 0.8013 (0.9007;0.9006) 0.0965 0.8071 (0.9033;0.9038) 0.0931\nBCa 0.8046 (0.9015;0.9031) 0.0930 0.8055 (0.9012;0.9043) 0.0923 0.7922 (0.8948;0.8974) 0.0985\nBoot-t 0.9142 (0.9572;0.9570) 0.0414 0.9149 (0.9588;0.9561) 0.0422 0.9100 (0.9580;0.9520) 0.0453\nAsymptotic 0.8853 (0.9505;0.9348) 0.0636 0.8821 (0.9496;0.9325) 0.0645 0.8826 (0.9512;0.9314) 0.0655\nFieller 0.9258 (0.9627;0.9631) 0.0357 0.9187 (0.9591;0.9596) 0.0384 0.9047 (0.9533;0.9514) 0.0466\n5 Percentile 0.8462 (0.9246;0.9216) 0.0751 0.8451 (0.9246;0.9205) 0.0754 0.8429 (0.9197;0.9232) 0.0727\nHybrid 0.8481 (0.9407;0.9074) 0.0897 0.8439 (0.9378;0.9061) 0.0905 0.8512 (0.9392;0.9120) 0.0841\nRatio 0.8465 (0.9251;0.9214) 0.0761 0.8453 (0.9244;0.9209) 0.0761 0.8506 (0.9238;0.9268) 0.0703\nBCa 0.8449 (0.9232;0.9217) 0.0753 0.8436 (0.9221;0.9215) 0.0757 0.8384 (0.9161;0.9223) 0.0751\nBoot-t 0.8984 (0.9511;0.9473) 0.0503 0.8976 (0.9511;0.9465) 0.0506 0.8894 (0.9442;0.9452) 0.0524\nAsymptotic 0.8861 (0.9529;0.9332) 0.0635 0.8853 (0.9517;0.9336) 0.0654 0.8892 (0.9509;0.9383) 0.0584\nFieller 0.9005 (0.9518;0.9487) 0.0492 0.8992 (0.9520;0.9472) 0.0500 0.9055 (0.9518;0.9537) 0.0436\n10 Percentile 0.8760 (0.9396;0.9364) 0.0595 0.8763 (0.9401;0.9362) 0.0606 0.8762 (0.9371;0.9391) 0.0572\nHybrid 0.8751 (0.9488;0.9263) 0.0679 0.8749 (0.9489;0.9260) 0.0698 0.8815 (0.9501;0.9314) 0.0632\nRatio 0.8776 (0.9398;0.9378) 0.0581 0.8760 (0.9395;0.9365) 0.0587 0.8817 (0.9401;0.9416) 0.0552\nBCa 0.8757 (0.9379;0.9378) 0.0584 0.8740 (0.9375;0.9365) 0.0594 0.8731 (0.9347;0.9384) 0.0578\nBoot-t 0.8976 (0.9501;0.9475) 0.0483 0.8973 (0.9501;0.9472) 0.0484 0.8941 (0.9478;0.9463) 0.0511\nAsymptotic 0.8953 (0.9531;0.9422) 0.0534 0.8937 (0.9531;0.9406) 0.0556 0.8977 (0.9537;0.9440) 0.0526\nFieller 0.8994 (0.9506;0.9488) 0.0486 0.9007 (0.9516;0.9491) 0.0471 0.9014 (0.9509;0.9505) 0.0464\nValues in parentheses refer to left and right tail probabilities\nN is sample size per time point\nfor all sample sizes. Additionally we see again the increasing power in sample size for\nequivalent AUC\u2019s while the type-I-error remains stable at about 5% if the areas are\nin fact different. This suggests that the constructed intervals are rather insensitive\nto distributional assumptions.\n6 Example\nTo illustrate the seven methods the rats data [3] were used. We consider testing for\ndose proportionality using the plasma concentrations based on doses of 30 mg\/kg\nand 100 mg\/kg. The observed concentrations are scaled by the administered dose\nand the corresponding AUC\u2019s tested for equivalence. Figure 1 shows the two sets of\ndata while Table 7 displays the seven confidence intervals discussed based on 10000\nbootstrap resamples.\nThe graph of the data shows that plasma concentrations at a dose of 30 mg\/kg\nare slightly lower at one and two hours and shows much higher variability at 4 hours\nwhile otherwise matching the scaled results for a dose of 100 mg\/kg well. All seven\n15\nTable 4: Empirical Coverage for \u2206 = 0.9 Using a Nominal Coverage of 90% for Scenario 3\nError distribution\nNormal Log-normal Double exponential\nN Method Coverage Coverage Coverage\n3 Percentile 0.8100 (0.9052;0.9048) 0.7931 (0.8965;0.8966) 0.7998 (0.9002;0.8996)\nHybrid 0.8180 (0.9463;0.8717) 0.7894 (0.9356;0.8538) 0.8049 (0.9398;0.8651)\nRatio 0.8201 (0.9100;0.9101) 0.7835 (0.8911;0.8924) 0.8039 (0.9024;0.9015)\nBCa 0.8103 (0.9035;0.9068) 0.7922 (0.8939;0.8983) 0.7970 (0.8968;0.9002)\nBoot-t 0.9165 (0.9634;0.9531) 0.9091 (0.9617;0.9474) 0.9077 (0.9582;0.9495)\nAsymptotic 0.8921 (0.9646;0.9275) 0.8743 (0.9579;0.9164) 0.8834 (0.9591;0.9243)\nFieller 0.9220 (0.9634;0.9586) 0.9098 (0.9572;0.9526) 0.9240 (0.9644;0.9596)\n5 Percentile 0.8522 (0.9265;0.9257) 0.8454 (0.9238;0.9216) 0.8446 (0.9207;0.9239)\nHybrid 0.8587 (0.9615;0.8972) 0.8378 (0.9520;0.8858) 0.8497 (0.9522;0.8975)\nRatio 0.8588 (0.9301;0.9287) 0.8412 (0.9221;0.9191) 0.8498 (0.9225;0.9273)\nBCa 0.8519 (0.9241;0.9278) 0.8428 (0.9189;0.9239) 0.8403 (0.9151;0.9252)\nBoot-t 0.9059 (0.9556;0.9503) 0.8947 (0.9508;0.9439) 0.8951 (0.9480;0.9471)\nAsymptotic 0.8975 (0.9652;0.9323) 0.8825 (0.9569;0.9256) 0.8895 (0.9572;0.9323)\nFieller 0.9135 (0.9565;0.9570) 0.9129 (0.9582;0.9547) 0.9062 (0.9511;0.9551)\n10 Percentile 0.8846 (0.9435;0.9411) 0.8647 (0.9328;0.9319) 0.8740 (0.9359;0.9381)\nHybrid 0.8848 (0.9649;0.9199) 0.8646 (0.9555;0.9091) 0.8778 (0.9577;0.9201)\nRatio 0.8868 (0.9447;0.9421) 0.8635 (0.9324;0.9311) 0.8743 (0.9361;0.9382)\nBCa 0.8830 (0.9397;0.9433) 0.8611 (0.9274;0.9337) 0.8709 (0.9306;0.9403)\nBoot-t 0.9072 (0.9559;0.9513) 0.8878 (0.9454;0.9424) 0.8923 (0.9461;0.9462)\nAsymptotic 0.9055 (0.9641;0.9414) 0.8839 (0.9544;0.9295) 0.8950 (0.9561;0.9389)\nFieller 0.9068 (0.9552;0.9516) 0.8968 (0.9489;0.9479) 0.8993 (0.9486;0.9507)\nValues in parentheses refer to left and right tail probabilities\nN is sample size per time point\nlower bounds of the two-sided 90% confidence intervals fall below the conventional\nmargins of bioequivalence for ratios of averages ranging from 0.8 to 1.25, indicating\nthat dose proportionality can not be established. As expected the bootstrap-t and\nthe Fieller interval are widest, which is reflected in the higher coverage in the simu-\nlation studies above.\n7 Discussion\nIn this note the performance of seven different types of confidence intervals for the\nratio of two area under the concentration versus time curves in a serial sampling de-\nsign, where the otherwise comonly used two-stage approach is not applicable due to\nsparsity, are evaluated. For all combinations of variation, sample size and distribu-\ntion the asymptotic, Fieller and bootstrap-t-interval are clearly superior to the other\napproaches considered. Among these three intervals only small difference in coverage\nor power\/type-I-error can be seen which is particularly surprising for the asymptotic\ninterval for contaminated data as it implies that the asymptotic normality shown in\n[10] is reached quickly. This result strengthened the argument to use the asymptotic\n16\nTable 5: Power and Type I error for different \u2206 Using a Nominal Coverage of 90% for\nScenario 3\nError distribution\nNormal Log-normal Double exponential\n\u2206 = 1 \u2206 = 0.9 \u2206 = 0.799 \u2206 = 1 \u2206 = 0.9 \u2206 = 0.799 \u2206 = 1 \u2206 = 0.9 \u2206 = 0.799\nN Method Power Power \u03b3 Power Power \u03b3 Power Power \u03b3\n3 Percentile 0.2447 0.1823 0.0644 0.2249 0.1685 0.0680 0.3418 0.2554 0.0819\nHybrid 0.2164 0.1845 0.0753 0.1875 0.1700 0.0776 0.3153 0.2667 0.1010\nRatio 0.2489 0.1852 0.0632 0.2232 0.1662 0.0702 0.3441 0.2520 0.0803\nBCa 0.2465 0.1789 0.0643 0.2227 0.1656 0.0669 0.3376 0.2514 0.0818\nBoot-t 0.0506 0.0366 0.0118 0.0469 0.0308 0.0143 0.1051 0.0769 0.0212\nAsymptotic 0.0879 0.0665 0.0240 0.0771 0.0634 0.0268 0.1651 0.1333 0.0393\nFieller 0.0387 0.0298 0.0089 0.0492 0.0363 0.0159 0.0890 0.0645 0.0166\n5 Percentile 0.4805 0.3170 0.0696 0.4306 0.2832 0.0702 0.5714 0.3715 0.0730\nHybrid 0.4525 0.3589 0.0923 0.3903 0.3133 0.0952 0.5526 0.4157 0.0974\nRatio 0.4898 0.3184 0.0665 0.4284 0.2835 0.0729 0.5747 0.3693 0.0703\nBCa 0.4756 0.3106 0.0668 0.4261 0.2788 0.0679 0.5672 0.3618 0.0714\nBoot-t 0.3367 0.2138 0.0413 0.2590 0.1638 0.0407 0.4162 0.2644 0.0472\nAsymptotic 0.3603 0.2569 0.0559 0.3021 0.2135 0.0575 0.4634 0.3153 0.0626\nFieller 0.3119 0.1883 0.0341 0.2366 0.1453 0.0335 0.4231 0.2569 0.0414\n10 Percentile 0.8550 0.5196 0.0574 0.7809 0.4685 0.0657 0.8973 0.5656 0.0605\nHybrid 0.8428 0.5825 0.0767 0.7656 0.5307 0.0878 0.8917 0.6234 0.0764\nRatio 0.8562 0.5225 0.0561 0.7798 0.4667 0.0663 0.8994 0.5661 0.0596\nBCa 0.8497 0.5088 0.0551 0.7727 0.4578 0.0638 0.8945 0.5533 0.0580\nBoot-t 0.8239 0.4854 0.0470 0.7233 0.4182 0.0551 0.8687 0.5255 0.0512\nAsymptotic 0.8262 0.5226 0.0565 0.7431 0.4675 0.0686 0.8780 0.5647 0.0590\nFieller 0.8215 0.4823 0.0464 0.7349 0.4167 0.0501 0.8742 0.5219 0.0475\nPower is based on conventional limits for bioequivalency\nN is sample size per time point\nstandard deviation as an estimator for \u03c3 for the bootstrap-t and asymptotic intervals.\nWe recommend the use of the Fieller interval if the number of time points is moder-\nate to large since it does not rely on heavy computation like the bootstrap-t-interval\nand it does not give symmetric confidence intervals as the asymptotic approach.\nFor a small number of time points where computation time is not an issue, the\nbootstrap-t-interval should be used since it has fewer underlying assumptions and\nyields almost identical results. The Fieller, asymptotic and bootstrap-t-interval pre-\nsented in this paper will be implemented in the R-package PK [22] in the near future.\nIn vast contrast to prior expectation, the ratio method specifically designed for\nproblems of this type, showed poor coverage for small sample sizes and was ultimately\nindistinguishable from the related hybrid interval in terms of overall coverage. A big\ndifference between these two, however, was to be found when looking at the tail-\nbehavior. The ratio method lead to more balanced tail coverages while the hybrid\nmethod severely undercovered on the upper bound. This insight suggests that it\nmight be possible to refine the ratio method further by including a small sample bias\ncorrection.\n17\nTable 6: Empirical Coverage and Power for a contaminated log-normal distribution using\na Nominal Coverage of 90% for varying delta\nDelta\n1 0.9 0.799\nN Method Coverage Power Coverage Power Coverage \u03b3\n3 Percentile 0.7911 (0.8933;0.8978) 0.9516 0.7916 (0.8941;0.8975) 0.6753 0.7915 (0.8949;0.8966) 0.1028\nHybrid 0.7925 (0.9117;0.8808) 0.9487 0.7930 (0.9125;0.8805) 0.7152 0.7927 (0.9135;0.8792) 0.1202\nRatio 0.7888 (0.8935;0.8953) 0.9510 0.7888 (0.8940;0.8948) 0.6729 0.7890 (0.8954;0.8936) 0.1059\nBCa 0.7922 (0.8931;0.8991) 0.9518 0.7916 (0.8933;0.8983) 0.6715 0.7921 (0.8944;0.8977) 0.1021\nBoot-t 0.9066 (0.9524;0.9542) 0.8406 0.9063 (0.9525;0.9538) 0.5040 0.9060 (0.9528;0.9532) 0.0465\nAsymptotic 0.8725 (0.9442;0.9283) 0.8992 0.8724 (0.9446;0.9278) 0.5970 0.8720 (0.9448;0.9272) 0.0728\nFieller 0.8951 (0.9454;0.9497) 0.8842 0.8947 (0.9458;0.9489) 0.5290 0.8948 (0.9464;0.9484) 0.0513\n5 Percentile 0.8430 (0.9192;0.9238) 0.9945 0.8429 (0.9200;0.9229) 0.7935 0.8428 (0.9210;0.9218) 0.0779\nHybrid 0.8415 (0.9347;0.9068) 0.9938 0.8412 (0.9353;0.9059) 0.8217 0.8412 (0.9364;0.9048) 0.0950\nRatio 0.8415 (0.9192;0.9223) 0.9945 0.8415 (0.9201;0.9214) 0.7922 0.8412 (0.9208;0.9204) 0.0791\nBCa 0.8415 (0.9163;0.9252) 0.9940 0.8410 (0.9172;0.9238) 0.7892 0.8398 (0.9179;0.9219) 0.0777\nBoot-t 0.8955 (0.9485;0.9470) 0.9862 0.8954 (0.9489;0.9465) 0.7217 0.8957 (0.9497;0.9460) 0.0539\nAsymptotic 0.8851 (0.9481;0.9370) 0.9909 0.8852 (0.9486;0.9366) 0.7637 0.8854 (0.9495;0.9359) 0.0639\nFieller 0.8957 (0.9470;0.9487) 0.9890 0.8953 (0.9475;0.9478) 0.7280 0.8952 (0.9478;0.9474) 0.0525\n10 Percentile 0.8830 (0.9414;0.9416) 1.0000 0.8830 (0.9424;0.9406) 0.9558 0.8832 (0.9436;0.9396) 0.0599\nHybrid 0.8825 (0.9503;0.9322) 1.0000 0.8826 (0.9510;0.9316) 0.9653 0.8819 (0.9518;0.9301) 0.0693\nRatio 0.8814 (0.9409;0.9405) 1.0000 0.8817 (0.9419;0.9398) 0.9569 0.8817 (0.9430;0.9387) 0.0609\nBCa 0.8831 (0.9394;0.9437) 1.0000 0.8830 (0.9403;0.9427) 0.9527 0.8824 (0.9412;0.9412) 0.0584\nBoot-t 0.9046 (0.9530;0.9516) 1.0000 0.9043 (0.9535;0.9508) 0.9419 0.9041 (0.9542;0.9499) 0.0499\nAsymptotic 0.9011 (0.9558;0.9453) 1.0000 0.9009 (0.9568;0.9441) 0.9525 0.9011 (0.9576;0.9435) 0.0563\nFieller 0.9058 (0.9544;0.9514) 1.0000 0.9061 (0.9554;0.9507) 0.9440 0.9058 (0.9561;0.9497) 0.0500\nValues in parentheses refer to left and right tail probabilities\nPower is based on conventional limits for bioequivalency\nN is sample size per time point\nTable 7: Two-sided 90% confidence intervals for the ratio of AUCs of dose scaled plasma\nconcentrations of CPI 975 in rats\nMethod Lower limit Upper limit\nPercentile 0.7258 1.2081\nHybrid 0.6681 1.1504\nRatio 0.7285 1.2125\nBCa 0.7322 1.2215\nBoot-t 0.6741 1.2778\nAsymptotic 0.6652 1.2110\nFieller 0.6760 1.2839\nThe most encouraging finding in this note is the great distinction between bioe-\nquivalent and non-equivalent results for all intervals. While the intervals show good\npower to assess bioequivalence when it is present (\u2206 = 1 and \u2206 = 0.9), the type-I-\nerror drops to about 5% as soon as the true ratio is less than 0.8.\nA natural extension of the work presented here is the use of batch designs. In\nbatch designs each analysis subject is sampled at more than one time point but not\nat all time points. While a variety of authors such as Holder et al. [23] and Yeh\n18\nFigure 1: Dose scaled plasma concentrations of CPI 975 at different time points in\nrats.\nll\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\n5 10 15 20\n0\n20\n40\n60\n80\n10\n0\n12\n0\nTime points (hours)\nD\nos\ne\u2212\nsc\nal\ned\n p\nla\nsm\na \nco\nnc\nen\ntra\ntio\nn \nof\n C\nPI\n 9\n75\n (n\ng\/m\nL)\nl\nDose\n30 mg\/kg\n100 mg\/kg\n[24] have proposed estimators for the AUC for this sampling design, its theoretical\nproperties are not yet well established. Furthermore, tests for bioequivalence have\nto be developed and evaluated in this context.\nAnother point of interest regards observations that fall below the detection limit.\nCommon practice in this situation is to either set those values to half the detection\nlimit or zero. Setting all values below the detection limit to zero may be sufficient\nfor calculation of AUC\u2019s but is not an option for other PK parameters as for example\nwhen estimating terminal elimination rate where values of zero cannot be used. A\ndifferent approach to the ad-hoc methods mentioned before is to model non-detected\ndata as censored data. Lambert et al. [25] suggest a method in the context of envi-\nronmental data that should be explored further for medical data in general and the\nestimation of pharmacokinetic parameters in particular.\n19\nReferences\n1 Bailer JA. Testing for the equality of area under the curves when using destructive\nmeasurement techniques. Journal of Pharmacokinetics and Biopharmaceutics,\n16(3):303\u2013309, 1988.\n2 Tang-Liu DDS, Burke PJ. The effect of azone on ocular levobunolol absorption:\nCalculating the area under the curve and its standard error using tissue sampling\ncompartments. Pharmaceutical Research, 5(4):238\u2013241, 1988.\n3 Nedelman JR, Gibiansky E, Lau DTW. Applying Bailer\u2019s method for AUC\nconfidence intervals to sparse sampling. Pharmaceutical Research, 12(1):124\u2013\n128, 1995.\n4 Pai SM, Nedelman JR, Hajian G, Gibiansky E, Batra VK. Performance of Bailer\u2019s\nmethod for AUC confidence intervals from sparse non-normally distributed drug\nconcentrations in toxicokinetic studies. Pharmaceutical Research, 13(9):1280\u2013\n1282, 1996.\n5 Wolfsegger MJ, Jaki T. Estimation of AUC from 0 to infinity in serial sacrifice\ndesigns. Journal of Pharmacokinetics and Pharamcodynamics, 32(5-6):757\u2013766,\n2005.\n6 Heinzl H. A note on testing areas under the curve when using destructive measure-\nment techniques. Journal of Pharmacokinetics and Biopharmaceutics, 24(6):651\u2013\n655, 1996.\n7 Bailer JA, Ruberg SJ. Randomization tests for assessing the equality of area under\ncurves for studies using destructive sampling. Journal of Applied Toxicology,\n16(5):391\u2013395, 1996.\n8 Guidance for Industry. Statistical Approaches to Establishing Bioequivalence. U.S.\nDepartment of Health and Human Services Food and Drug Administration. Cen-\nter for Drug Evaluation and Research (CDER), 2001.\n20\nURL: http:\/\/www.fda.gov\/Cder\/guidance\/3616fnl.pdf; last visited at 2007-\n09-25.\n9 Hu C, Moore KHP, Kim YH, Sale ME. Statistical issues in a modeling approach\nto assessing bioequivalence or PK similarity with presence of sparsely sampled\nsubjects. Journal of Pharmacokinetics and Pharmacodynamics, 31(4):321\u2013339,\n2004.\n10 Wolfsegger MJ. Establishing bioequivalence in serial sacrifice designs. Journal of\nPharmacokinetics and Pharmacodynamics, 34(1):103\u2013113, 2007.\n11 Fieller EC. Some problems in interval estimation. Journal of the Royal Statistical\nSociety Series B, 16(2):175-185, 1954.\n12 Davison AC, Hinkley DV. Bootstrap Methods and their Applications. Cambridge\nSeries in Statistical and Probabilistic Mathematics. Cambridge University Press,\n1997.\n13 Efron B, Tibshirani RJ. An Introduction to the Bootstrap. Chapman and Hall,\nLondon, 1993.\n14 Efron B. Better bootstrap confidence intervals (with discussion). Journal of the\nAmerican Statistical Association, 82(397):171\u2013200, 1987.\n15 Hall P. The Bootstrap and Edgeworth Expansion. Springer-Verlag, New York,\n1992.\n16 Ploner M. Bootstrap Confidence Intervals, an overview and comparisons by means\nof selected applications. Ph. D. thesis, University of Vienna, 2002.\n17 Efron B. Nonparametric standard errors and confidence intervals (with discus-\nsion). The Canadian Journal of Statistics, 9(2):139\u2013172, 1981.\n18 Satterthwaite FE. An approximate distribution of estimates of variance compo-\nnents. Biometrics Bulletin, 2(6):110\u2013114, 1946.\n21\n19 Gibaldi M, Perrier D. Pharmacokinetics. Marcel Dekker, New York and Basel,\n1982.\n20 R Development Core Team. R: A Language and Environment for Statistical\nComputing. R Foundation for Statistical Computing, Vienna, Austria, 2006.\nISBN 3-900051-07-0.\n21 Casella G, Berger RL. Statistical Inference. Duxbury Press, 2nd edition edition,\n2002.\n22 Wolfsegger MJ, Jaki T. PK: Basic Pharmacokinetics, 2006. R package Version\n0.03.\n23 Holder DJ, Hsuan F, Dixit R, Soper K. A method for estimating and testing\narea under the curve in serial, batch and complete data designs. Journal of\nBiopharmaceutical Statistics, 9(3):451\u2013464, 1999.\n24 Yeh C. Estimation and significance tests of area under the curve derived from\nincomplete blood sampling. In Biopharmaceutical Section of the Proceedings of\nthe Annual Meeting of the American Statistical Association, 1999.\n25 Lambert D, Peterson B, Terpenning I. Nondetects, detection limits, and\nthe probability of detection. Journal of the American Statistical Association,\n86(414):266\u2013277, 1991.\n22\n"}