{"doi":"10.1016\/S0377-2217(03)00163-2","coreId":"66239","oai":"oai:dro.dur.ac.uk.OAI2:2300","identifiers":["oai:dro.dur.ac.uk.OAI2:2300","10.1016\/S0377-2217(03)00163-2"],"title":"Minimimally biased weight determination in personnel selection.","authors":["Jessop,  A."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2004-03","abstract":"The derivation of weights from preference statements is subject to difficulties, some of which are due to the unreliability of the judgement of the decision maker. To overcome this Jaynes\u2019 principle of maximum entropy has been invoked and may be applied either to weights or to the linear weighted scores of the candidates in a selection problem. When candidates are relatively few the two strategies give different styles of interaction. These are discussed and illustrated by application to a problem of personnel selection","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/66239.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/2300\/1\/2300.pdf","pdfHashValue":"dfee6e74d3ebb3fb5c6d72b72c3c065700db6552","publisher":"Elsevier","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:2300<\/identifier><datestamp>\n      2011-08-17T08:34:34Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Minimimally biased weight determination in personnel selection.<\/dc:title><dc:creator>\n        Jessop,  A.<\/dc:creator><dc:description>\n        The derivation of weights from preference statements is subject to difficulties, some of which are due to the unreliability of the judgement of the decision maker. To overcome this Jaynes\u2019 principle of maximum entropy has been invoked and may be applied either to weights or to the linear weighted scores of the candidates in a selection problem. When candidates are relatively few the two strategies give different styles of interaction. These are discussed and illustrated by application to a problem of personnel selection.<\/dc:description><dc:subject>\n        Multiple criteria analysis<\/dc:subject><dc:subject>\n         Human resources<\/dc:subject><dc:subject>\n         Entropy<\/dc:subject><dc:subject>\n         Personnel selection.<\/dc:subject><dc:publisher>\n        Elsevier<\/dc:publisher><dc:source>\n        European journal of operational research, 2004, Vol.153(2), pp.433-444 [Peer Reviewed Journal]<\/dc:source><dc:date>\n        2004-03<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:2300<\/dc:identifier><dc:identifier>\n        issn:0377-2217<\/dc:identifier><dc:identifier>\n        doi:10.1016\/S0377-2217(03)00163-2<\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/2300\/<\/dc:identifier><dc:identifier>\n        http:\/\/dx.doi.org\/10.1016\/S0377-2217(03)00163-2<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/2300\/1\/2300.pdf<\/dc:identifier><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["0377-2217","issn:0377-2217"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2004,"topics":["Multiple criteria analysis","Human resources","Entropy","Personnel selection."],"subject":["Article","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n29 August 2008\nVersion of attached file:\nAccepted Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nJessop, A. (2004) \u2019Minimimally biased weight determination in personnel selection.\u2019, European journal of\noperational research., 153 (2). pp. 433-444.\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1016\/S0377-2217(03)00163-2\nPublisher\u2019s copyright statement:\nAdditional information:\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\n Use policy \n \nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without \nprior permission or charge, for personal research or study, educational, or not-for-profit purposes \nprovided that : \n \n\u0083 a full bibliographic reference is made to the original source \n\u0083 a link is made to the metadata record in DRO \n\u0083 the full-text is not changed in any way \n \nThe full-text must not be sold in any format or medium without the formal permission of the copyright \nholders.  \n \nPlease consult the full DRO policy for further details. \n \nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom \nTel : +44 (0)191 334 2975 | Fax : +44 (0)191 334 2971 \nhttp:\/\/dro.dur.ac.uk \nDurham Research Online \n Deposited in DRO:\n29 August 2008\nVersion of attached file:\nAccepted\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nJessop, A. (2004) 'Minimimally biased weight determination in personnel selection.',\nEuropean journal of operational research., 153 (2), pp.\u0000433-444.\nFurther information on publisher\u0000s website:\nhttp:\/\/dx.doi.org\/10.1016\/S0377-2217(03)00163-2\n 1 \nMINIMALLY BIASED WEIGHT \nDETERMINATION IN PERSONNEL \nSELECTION \n \n \nAlan Jessop \n \nDurham Business School \nMill Hill Lane \nDurham \nDH1 3LB \n \ne-mail: a.t.jessop@durham.ac.uk \nphone: 0191 374 1233 \nfax:     0191 374 3748 \n \n \n \n 2 \nMINIMALLY BIASED WEIGHT DETERMINATION IN \nPERSONNEL SELECTION \n \n \n \n \nAbstract \n \nThe derivation of weights from preference statements is subject \nto difficulties, some of which are due to the unreliability of the \njudgement of the decision maker. To overcome this Jaynes\u2019 \nprinciple of maximum entropy has been invoked and may be \napplied either to weights or to the linear weighted scores of the \ncandidates in a selection problem. When candidates are \nrelatively few the two strategies give different styles of \ninteraction. These are discussed and illustrated by application to \na problem of personnel selection. \n \nKEY WORDS: multiple criteria analysis, human resources, entropy, \npersonnel selection \n \n \n \n 3 \n1. Introduction \n \nThe use of simple weighted sums as a method of aggregating \nincommensurate characteristics or alternatives is common (Stewart, \n1992) and has generated a vast literature. The model is \n \n yi  =  \u03a3 wjuj(xij)      (1) \n                      j \nwhere xij is the value of attribute j achieved by choosing alternative i; \nwj is a weight; uj(\u00b7)is a value function and \n \n \u03a3 wj  =  1       (2) \n               j    \nThe vector of scores, Y, provides a basis for ranking or selection.  \nIt is generally perceived that the largest technical difficulty is \nfinding values for the weights and many methods are available for \nderiving weights from preference judgements. The choice of method \nmay be more a matter availability or of personal preference on the part \nof the analyst than on any other consideration (Bottomley and Doyle, \n2001). These judgements are necessarily subjective,  and so fallible, as \na result of the cognitive limits of the respondent (Barron and Barrett, \n1996; Borcherding, Schmeer and Weber, 1995; Ranyard and Abdel-\nNabi, 1993; Larichev, 1992; von Winterfeldt and Edwards, 1986). \nInasmuch as these result in biases which, though psychological in \norigin and innocent of any prejudice, may be (mis)taken as evidence \nof behaviour which is socially unacceptable or even illegal then the \nderivation of weights which may be seen as unbiased becomes a \npressing task. Nowhere is this more true than in personnel selection. \nIn what follows two articulations of unbiased weight estimates are \ngiven; issues surrounding personnel selection are reviewed; an \nillustrative application of unbiased estimation to a personnel problem \nis discussed. \n \n \n2.  Avoiding bias \n \nEdwin Jaynes was concerned to provide estimates of probability \ndistributions which were unbiased in that they were as uniform (flat) \nas possible subject only to specified constraints. He proposed that \nmaximising the entropy of the distribution subject to these constraints \nprovided just such an estimate, the maximisation ensuring the flattest \npossible distribution (Jaynes, 1957). These flat distributions are said to \n 4 \nbe minimally discriminating; in Jaynes\u2019 case minimally discriminating \nbetween the relative likelihoods of  different events or states. The \nsame principle may be applied to the calculation of weight \ndistributions and a review is provided by Jessop (1999). This method \nis therefore proposed as a way of finding weights and so making \nselection decisions which may be said to be unbiased. \nThe entropy of an arbitrary vector, Z, is \n \nH(Z) = ln(\u2211zi) - \u2211ziln(zi) \/ \u2211zi     (3) \n                  i         i                i \n \nwhich becomes \n \nH(Z)  =  -\u03a3 zjln(zj)      (4) \n                              j    \n \nin those cases, such as probabilities and weights, in which the sum of \nthe vector is unity. \nThe most common application of this method in multicriteria \nmodelling is in the derivation of weights directly. Some preference \ninformation may be available, as ratios between weights or as the \nresolution of bicriterial problems, and these provide constraints given \nwhich, and with (2), H(W) is maximised. The full programme is: \n \nmax   H(W)  =  -\u03a3 wjln(wj)     (5) \n                                     j    \n \ns.t.  \u03a3 wj  =  1      (6) \n                    j    \n \n  wi  \u2265  \u03b5  ;    \u2200 i         (7) \n \n  wi  \u2265  awk            (8) \n   \n  wi  =  bwk                    (9) \n \n  \u03a3 wjuj(xij)  \u2265  c\u03a3 wjuj(xkj)                   (10) \n    j                      j \n   \n  \u03a3 wjuj(xij)  =  d\u03a3 wjuj(xkj)                   (11) \n    j                        j \n \n 5 \nConstraint (7), which is optional, ensures that all weights are non-zero. \nIn the calculations described below \u03b5 = 0.02. Constraints (8) and (9) \nencode the results of pairwise comparisons of weights and constraints \n(10) and (11) similarly encode the results of pairwise comparisons of \ncandidates. Parameters a to d are given by the decision maker in those \ncomparisons in expressing the strength of the preferences expressed. \nThere are as many of the constraints (7) to (11) as the decision maker \nwishes to provide. \nCall this method maxEntWeights. In the absence of preference \ninformation the result is a uniform weight distribution. \nCalculating weights in this way will in general give scores, Y, \ndifferent for each candidate and so will permit a decision to be made \nbetween them. But, as has been pointed out (Jessop, 1999), it seems \ncounterintuitive to say that minimally discriminating weights yet \npermit discrimination. The conflict arises as the result of a tension \nbetween the dual foci of weights and scores. If the purpose of the task \nis to find weights which reflect the underlying values of the decision \nmaker then using maxEntWeights may be justifiable. However, in \nmost cases, and certainly in personnel selection, this is not the \npurpose: what is required is the least biased selection from among \ncandidates. That being so it makes sense to be minimally \ndiscriminating between candidates rather than between weights and so \nto maximise H(Y). The programme is as given above but with H(Y) \nreplacing H(W). The objective function is, from (1) and (3), \n \nmax H(Y)  =   \n          ln(\u03a3\u03a3wjuj(xij)) - \u03a3[\u03a3wjuj(xij)ln(\u03a3wjuj(xij))] \/ \u03a3\u03a3wjuj(xij)      (12) \n               i  j                  i    j                           j                    i  j  \n \nwith the constraints (6) to (11) as before. Call this method \nmaxEntScores. \nIt is unlikely, and undesirable, that in practice a decision maker \nwould proceed without preference but, whatever preference \ninformation is given, it will remain the case that maxEntScores will \ngive a result that is minimally discriminating between candidates and \nmaxEntWeights will not and that in personnel selection this is a \ndesirable precaution against the effects of judgemental bias. \nThe results obtained by using maxEntScores will depend in an \ninteresting way on both the number of attributes and the number of \ncandidates. When the number of attributes is large compared to the \nnumber of candidates it will be possible to incorporate some \njudgmental constraints without being able to discriminate between \nalternatives. When the number of candidates is greater than the \n 6 \nnumber of attributes then, even with no judgmental information, a \npreference order will be induced among candidates. The effect is \nbasically due to different degrees of freedom, though the exact point \ncorresponding to the zero degrees of freedom situation may be \ndifficult to determine in advance if the judgmental constraints are \nexpressed as inequalities rather than equalities. Call these two types of \nproblem Short Lists and Long Lists, in recognition of the two stages of \nmost selection problems, not least in personnel selection. \n \n \n3. Personnel selection \n \nThe assessment and selection of personnel for promotion, \nreassignment or to fill new positions is a task much discussed and is \nthe subject of a large body of research. A good review is given by \n(Robertson and Smith, 2001) who describe the commonly used \nprocess: \n \n1. detailed analysis of the job leading to \n2. an indication of the psychological attributes required of a \nsuccessful candidate \n3. personnel selection methods aim to assess the extent to which \nthese attributes are possessed by candidates \n4. a validation process tracks the success of the selection \nprocess in identifying suitable candidates. \n \nThose factors assessed will typically be both job specific and \ngeneric in that they refer to the personality of the candidate, though \nRee, Earles and Teachout (1994) suggest that job specific evaluations \noffer little that could not otherwise be found by an assessment of \ngeneral intelligence. Personality is generally described by the \u201cBig \nFive\u201d personality factors of emotional stability, extraversion, \nopenness, agreeableness and conscientiousness (Salgado, 1997). \nBratton and Gold (1994) say that the objectives of a selection \nprocess are twofold: to assess the differences between candidates and \nto predict future performance. The extent to which a process is \nsuccessful may be measured by its reliability (the extent to which \nresults may be replicated by, for instance, different assessors) and \nvalidity (the degree to which it measures what it says it will). This \nlatter is difficult to assess due to the large samples required and the \nvarious other temporal changes that may affect employees. Table 1 \nshows the validity of some criteria. Table 2 shows an assessment of \nthe reliability of some methods, assessed as the improvement that they \n 7 \noffer over random selection of candidates. The use of different \nmethods in some European countries is shown in Table 3. Similar \nresults were given by Robertson and Makin (1986). \nThe difference between the popularity of methods used and their \nevidential value is striking and is the cause for dissatisfaction of \nwriters in this area (Guion, 1998b). Two useful trends should be \nnoted. First, the use of structured interviews rather than the more \nfreewheeling conversations which once were the more common (and \nstill may be) has increased the validity of the interview (Robertson and \nSmith, 2001; Cortina et al, 2000). However, the extent to which \ninterviews provide incremental information given that other \nassessments, notably of cognitive ability and conscientiousness, are \navailable is disputed: Cortina et al (2000) present evidence that highly \nstructured interviews can contribute \u201csubstantially to prediction\u201d of \nperformance in the job, while Barrick, Patton and Haugland (2000) \nthink that it does not, although it may be well suited to assess the fit of \na candidate within an organisation. Second, several techniques are \nused in assessment centres and assessments made which typically \nestimate performance against more criteria than the there are tests. \nHenderson, Anderson and Rick (1995) describe a centre of four \nexercises from which performance against fourteen criteria was \nassessed. The centre described by Blackham and Smith (1989) used \nsix tests or exercises to evaluate ten attributes. In both cases ratings \nwere given on a simple scale (five point in the latter) and aggregated \nusing equal weights. Lowry (1994) gives evidence that structured \ninterviews and assessment centres have similar validity. \n \n \n4.  Formal methods \n \nThe use of scoring or rating to describe assessments of performance \nagainst criteria is not limited to assessment centres. Scoring in \nstructured interviews is also common: 63% of the respondents in a \nstudy by Barclay (2001) used scoring for this purpose. The motivation \nfor explicit scoring systems, as for any structured method, is to reduce \nthe effects of interviewer subjectivity and bias (Cascio, 1991, ch 5; \nCampion, Palmer and Campion,1997; Guion, 1998a, ch 12). While the \narticulation of assessment judgement via scores is accepted the use of \na similarly explicit aggregation of  a number of scores to give an \noverall judgement of the merit of candidates seems to be resisted and \nso the benefits obtained from scoring are not fully utilised (Barclay, \n2001). This may in part be due to the perceptions of interviewers that \ninterviews, however strongly structured, do not have the scientific \n 8 \nbase of, say, psychometric tests and  so are not capable of giving \nnumerically codable results. Alternatively, it may be that interviewers \nand managers wish to maintain the flexibility in weighting the \nimportance of different components that comes with inexplicit \ncombination. The use of an aggregating model more complicated than \njust summing the individual ratings, as in the case of the assessment \ncentres discussed above, will not be helped by the generally low use of \nIT by personnel specialists (Huo and Kearns, 1992; Kinnie and \nArthurs, 1996). \nNonetheless, there have been some attempts to apply formal \nmethods to the aggregation problem. Fuzzy set theory has been \nproposed (Miller and Feinzig, 1993; Karsak, 2001; Capaldo and Zollo, \n2001)  as have the applications of linear weighted sums familiar in \nmulticriteria analyses (Bohanec, Urh and Rajkovi\u010d,1992; Gardiner and \nArmstrong-Wright, 2000; Spyridakos et al, 2001). Timmermans and \nVlek (1992; 1996) explore how the use of formal methods may help in \novercoming bias and information overload and conclude that using a \nweighted sum for aggregation is of use for some problems but that it is \nunnecessary for small problems and fails to be of help in large \ncomplex problems. Roth and Bobko (1997) review some of the issues \nsurrounding the use of multiattribute methods in human resource \nmanagement. Ganzach, Kluger and Klayman (2000) give a method of \ncombination based on regression analysis. \nThis last study compares a single unaided assessment of candidates \nand what is effectively a weighted sum and finds no great difference \nbetween the two, although the authors note that this finding is \nsomewhat atypical and cite other studies showing the superiority of \nmore formal disaggregated methods. But increased accuracy is not the \nonly objective. The elimination of bias is becoming increasingly \nimportant not only in the technical sense of reducing the impact of \ncognitive bias but in eliminating, and being seen to eliminate, gender \nbias, racial bias and other similar biases which are socially undesirable \nor illegal. Gardiner and Armstrong-Wright (2000)  show how a \nmulticriteria approach to such decisions can be of help by providing a \nprocedure which is fair, consistently applied, well documented, \ntransparent and so legally defensible. They also note that personnel \ndecisions are not often made by one individual and that the problem is \nessentially one of group decision support. In these potentially litigious \nsituations it seems reasonable that minimising discrimination between \ncandidates by using maxEntScores is most likely to guard against \naccusations of bias in favour of or against a particular candidate. \nIt is this property of being minimally discriminating between \ncandidates except through explicitly stated judgements which makes \n 9 \nmaxEntScores such an appropriate decision aid for personnel \nselection, for avoiding implicit, even unconscious, bias is exactly what \nmanagers faced with these decisions are required to do. It is perhaps \nmore common, if a little mistaken, to articulate this requirement by \nhaving equal or near equal weights: this is maxEntWeights.   \n \n \n5.  Illustration \n \nThe main issues to arise from this discussion are, first, the \nusefulness of a weighted average scoring model in general and, \nsecond, the relative usefulness of maxEntWeights  and maxEntScores \nas criteria in the determination of weights. While maxEntScores is the \npreferable criterion for avoiding bias in selection problems, the \npractical issue is whether the different style of interaction when using \nthis criterion would be practicable or whether users would find the fact \nthat discrimination between candidates did not immediately result \nfrom their first few judgemental inputs sufficiently counterintuitive to \nrender the method unacceptable. To examine these issues a simple \nexperiment was conducted. \nThe experimental group was sixteen staff and MBA students of  \nDurham Business School. The staff were academics and \nadministrators and the students were people who had held managerial \npositions before joining the MBA programme. Each was asked to \nselect which of five candidates, a \u2013 e, should be chosen for the post of \nPostgraduate Admissions Officer at the University, a post which had \nin fact recently been advertised and filled. They were provided with \nthe two page description of the job that had been sent to applicants. \nFrom the required skills described a list of seven criteria was \nconstructed: \n \n1. Written communication \n2. Oral communication \n3. Planning \n4. Organising ability \n5. Team player \n6. Works independently \n7. Decisiveness \n \nThe job description contained no indication of the relative \nimportance of the criteria. \nAdditionally each person was given the one page description of the \nexperiment shown in Figure 1 sufficiently in advance of the \n 10 \nexperiment itself that they could understand clearly what was to \nhappen and what were the characteristics of the job. \nThe performance of each candidate against the criteria were \ndescribed on a five point scale, with 5 denoting excellence: these are \nthe values uj(xij) in (1). These values were generated randomly, with \nadjustment to ensure that a large number of very poor scores of 1 was \navoided because the presence of too many such weak performances \nmight lead to an early elimination of candidates without the need to \nmake the tradeoff judgements required by the methods, \nmaxEntWeights  and maxEntScores, being tested. Two sets of \nhypothetical candidates, called the Red and Blue sets, were used and \nare shown in Table 4. As can be seen from the sums given in the last \nline of the table, candidates in the Red set were slightly the stronger \nand exhibited a little higher spread of abilities between candidates. \nThe size of the problem is seen as within the range discussed by \nTimmermans and Vlek (1992; 1996) within which multiattribute \ndecision aids ma be of use. \nThe experiment was to test the two forms of weight calculation and \nthe corresponding interactions. To do this each of the participants in \nthe experiment was asked to make two selections, one using \nmaxEntWeights  and one using maxEntScores. Since there was likely \nto be some, perhaps some considerable, learning carried over from the \nfirst selection task to the second, the order in which the two methods \nwere used was varied. Each selection also used a different candidate \ndata set. As a result there were four task pairs, each of which is coded \naccording to the first task, as shown in Table 5. \nEach participant was confronted with a computer screen showing \nthe display as described in Figure 1 but with weighted average scores \nat the foot of each column. Initially all weights were set to zero and so \nthese scores were also zero. It was explained that this was so because \nno initial judgemental information had yet been given and that scores \nwould appear as soon as the first pairwise statement had been made. \nAfter each pairwise statement the corresponding constraint was \nentered and the weights recomputed and calculated scores displayed. \nThis process continued until a recommendation could comfortably be \nmade. The time taken for the task was noted. Each participant gave a \nscore indicating how difficult they found the task with 1 representing \n\u201ceasy\u201d and 5 \u201chard\u201d. Comments both about the tasks and about the \napproach in general were also optionally provided. \nThe task was to select rather than to find weights per se. It has been \nnoted elsewhere, by Jessop (2002) for instance, that people \nnonetheless like to focus on weights as these are seen as important in \nterms of policy or as explicit representations of personal preferences. \n 11 \nIt was thought to be of some interest to examine this effect here and so \nto this end values of weights were at no point shown. \nNo restrictions were placed on how each person should interact \nwith the data so that statements about criteria or scores could be given \nin any order and either singly or in groups. It was also permissible to \neliminate candidates from further consideration at some stage in the \nprocess if, for instance, it was believed that performance against a \ncrucial criterion so poor as to disbar the candidate from the job. \nIn the instructions (Figure 1) two typical responses were given to \nindicate that preference statements about criteria or candidates or both \nwere permissible. Although the example given for criteria was phrased \nto elicit an equality constraint most responses were inequality \nstatements such as \u201cwritten communication is more important than \nplanning\u201d rather than the specification of a precise ratio between the \nweights. This was so for preference statements concerning both \ncriteria and candidates.  \n \n \n6. Results \n \nThe results are summarised in Table 6. Column a gives the \nidentifying number of the respondent; column b gives the task pairs; \ncolumns c to f give the times and difficulty scores for the first and \nsecond problems (P1 and P2 respectively). Columns g and h show the \njudgemental inputs provided, with those shown in parentheses for P2 \nindicating judgements carried over from P1 so that, for instance, in P1 \nrespondent 6 (hereafter written as {6}) provided 2 judgemental inputs \nregarding weights directly (e.g. \u201cI think criterion 4 is at least twice as \nimportant as criterion 2\u201d) and 1 regarding candidates (\u201cI believe that \ncandidate c is better than candidate e\u201d) all three of which were used as \nconstraints in the optimisation. In P2 the two judgements about \ncriteria were carried over from P1 and an additional judgement made \nabout criteria and one about candidates. It is, of course, impossible to \ncarry over a weight about candidates since the set of candidates \nchanges. In all cases except one {16} all judgemental constraints were \ncarried over from P1 to P2. \nTable 7 shows the mean characteristics for each pair group. With \nonly four responses in each group a detailed statistical analysis is \ninappropriate but some conclusions may, with circumspection, be \ndrawn.  \nThe number of extra judgements needed for P2 is small in all cases \nand will not be further considered here. \n 12 \nThe four experimental pairs have been ordered according to the \nnumber of judgements needed for P1. The two maxEntScores \nproblems head the list. This is as expected since this model requires \nmore constraints before non-uniform scores are found. In both \nmaxEntScores and maxEntWeights  problems the Blue set required \nless input than the Red for a satisfactory recommendation. As seen at \nthe bottom of Table 4 the Blue set are generally poorer performers and \nalso less differentiated so, again, the results are as expected. \nBoth time taken and the perceived difficulty of the P1 problems are \nroughly negatively correlated when considering means though the \npicture is less clear when all sixteen data are considered as, for \ninstance, in Figure 2. The same picture emerges, though less strongly, \nfor P2 problems. Inasmuch the idea of a negative correlation is \nentertained, however tentatively, it suggests that the extra number of \njudgements evidently required by the method of weight calculation, \nmoderated by the inherent characteristics of  the data, have the benefit \nof requiring less deliberation by the decision maker on other grounds. \nIn Table 7 the P2 problems are the complements of the P1 \nproblems (Table 5). In all four groups the mean times fall substantially \nfrom P1 to P2. The data are represented in Table 8. Here the P2 \nproblems are not the complements of P1 but are the same method\/data \npair in both cases. For example, BS problems are P1 in the group \nlabelled \u201cBS\u201d and P2 in the group labelled, complementarily, \u201cRW\u201d. \nIn Table 8 both have been entered on the \u201cBS\u201d line. The groups which \nrequire most and least input, BS and RW, may be thought to be the \nhardest and simplest problems respectively as judged by the \ninformation required for a decision to be made. Looking at these two \nin Table 8 it is seen that for the hard problem, BS, the solution time is \nnot much different whether it was P1 or followed, as P2, the simplest \nproblem, RW. In this second case, because of the simplicity of the first \nof the pair not much was learned that was useful in tackling the harder \nproblem which required greater informational input. Look now at the \nRW entries and exactly the opposite is seen: when RW is encountered \nas P2 after BW then the mean time is just 3.25 mins., a large reduction \nfrom 11.75mins. RS and BW problems have similar information \nrequirements and so this effect is not seen when comparing their \ntimes. The same observations hold when considering the difficulty \nscores but, since the measure here is a coarse five point scale, the \nmagnitude of the effects is smaller. \nIn all sixteen cases judgements about criteria preceded those about \ncandidates and were more numerous. Most participants began by \ncomparing the relative importance of the two most important criteria. \nSome then continued comparing the second and third most important \n 13 \nand so on. In large part these constraints were inequalities establishing \na rank ordering of criteria and candidates. \nAlthough weights were not displayed nobody asked what values \nthe weights had, being quite content to just input constraints. In just \ntwo cases when using  maxEntScores respondents remarked that the \nscores were unchanged after some information had been given. It was \nexplained that this was because not enough information had been \nprovided to permit distinctions to be made and this was accepted, \nwithout further question, as a satisfactory explanation: it was also true. \nAlmost all participants liked the method for its objectivity and as a \nway both of removing potential bias and as a means for ensuring that \nall performance data were examined. Most made the point that the \ngeneral method was good for filtering but that a final decision would \nhave to be based on, or confirmed by, a knowledge of the applicants \nand spoke warmly of the role of the interview particularly in \ndetermining whether a candidate would fit well into the organisation. \nOne participant {8} put it this way: \u201cthe scores measure who can do \nthe job best but an interview finds who is best for the job\u201d. Being told \nof the poor properties of unstructured interviews did not cause \nrevisions of these views. \nThree participants {2}, {3} and {15} felt that performance \nassessments of 1 were unacceptable and so ruled out those two \ncandidates with these low assessments, thereby reducing the selection \nproblem to a choice between three candidates. Others noted these low \nvalues but said that they would assume that appropriate staff \ndevelopment would improve performance  to an acceptable level and \nso selected a candidate with an assessment of 1. The majority of \nparticipants made no comment about this issue. One participant {7} \nwould have liked to see an unweighted sum for each candidate, as \ngiven in Table 4, as a guide to choice.  \n \n7.  Conclusion \nTwo articulations of unbiasdness in the context of multiattribute \ndecision making have been described and applied to an illustrative \nproblem of personnel selection. Both were shown to be feasible. The \npreferred method, maxEntScores, is shown to be practicable and that \nthe requirement of some small number of judgmental inputs before it \nis possible to begin to differentiate candidates caused no difficulty for \nusers. This initial response lag is the main difference in the styles of \ninteraction required by the two methods.  While this start up cost, as it \nmight be seen, may at first seem unusual it ought not to be surprising \nthat some extra effort is necessary in final selection in order that the \n 14 \nprocess may be seen as free from bias. In any case, when making \nselections in practice it would never be sufficient to provide just one \njudgmental input and then be able to feel comfortable with the result. \nIn this respect the initial inertia or stickiness of maxEntScores is much \ncloser to the experience of decision makers than the alternative \nconcentration on weights and the instant differentiation it allows. This \ncharacteristic of forcing more initial inputs appears to reduce overall \nsolution time and perceived difficulty.  \nThe experiment was artificial: a real selection process was not \nobserved. The participants did not have to live with the consequences \nof their decisions in the way that decision makers often do. Selecting \nparticipants familiar with interviewing or the University or both and \nusing a real job description (though not, for reasons of confidentiality \ndata on real applicants) added a degree of realism. Nonetheless, it was \na constructed experiment and so the results should be interpreted \ncircumspectly. Some results, incuriosity about the values of weights, \nfor instance, should be viewed in this light. \nThe experimental problem represents an extreme case in that the \nselections were made in ignorance of candidates and without \nparticipation in the assessment of performance against individual \ncriteria. This certainly would prevent accusations of bias but is an \nunlikely, and probably an undesirable, procedure: those making the \nfinal recommendations will usually have been part of all stages of the \nselection process. It is clear from the respondents that having some \nappreciation of the person behind the figures is important, as probably \nit ought to be, but the unjustified faith which some had in the utility of \ninterviews suggests that the need for a decision aid which guards \nagainst unjustified bias is as great as ever.  \n \n \n 15 \nReferences \n \nBarclay, J.M., 2001. Improving selection interviews with structure: \norganisations\u2019 use of \u201cbehavioural\u201d interviews, Personnel Review  30 \n81\u2013101. \nBarrick, M.R., Patton, G.K., Haugland, S.N., 2000. Accuracy of \ninterviewer judgements of job applicant personality traits, Personnel \nPsychology 53 925\u2013951.  \nBarron, F.H., Barrett, B.E., 1996. The efficacy of smarter \u2013 Simple \nMulti-Attribute Rating Technique Extended to Ranking, Acta \nPsychologica 93 23\u201336. \nBeardwell, I., Holden, L., 1994. Human Resource Management: A \nContemporary Perspective, Pitman, London. \nBlackham, R.B., Smith, D., 1989. Decision-making in a management \nassessment centre,  Journal of the Operational Research Society 40  \n953\u2013960. \nBohanec, M., Urh, B., Rajkovi\u010d, V., 1992. Evaluating options by \ncombined qualitative and quantitative methods, Acta Psychologica 80 \n67\u201389.  \nBorcherding, K., Schmeer, S., Weber, M., 1995. Biases in \nmultiattribute weight elicitation. In: Caverni J-P., Bar-Hillel M., \nBarron FH., Junngermann H. (Eds), Contributions to Decision Making \n\u2013 I, Elsevier, Amsterdam. \nBottomley, P.A., Doyle, J.R., 2001. A comparison of three weight \nelicitation methods: good, better and best, Omega 29 553\u2013560. \nBratton, J., Gold, J., 1994. Human Resource Management: Theory and \nPractice, Macmillan, Basingstoke. \nCampion, M.A., Palmer, D.K., Campion, J.E., 1997. A review of \nstructure in the selection interview, Personnel Psychology 50 655\u2013702. \nCapaldo, G., Zollo, G., 2001. Applying fuzzy logic to personnel \nassessment: a case study, Omega 29 585\u2013597. \nCascio, W.F., 1991. Applied Psychology in Personnel Management \n(3rd edn), Prentice-Hall, Englewood Cliffs NJ. \nCortina, J.M., Goldstein, N.B., Payne, S.C., Davison, H.K., Gilliland, \nS.W., 2000. The incremental validity of interview scores over and \nabove cognitive ability and conscientiousness scores, Personnel \nPsychology 53 325\u2013351.  \nDany. F., Torchy, V., 1994. Recruitment and selection in Europe: \npolicies practices and methods. In: Brewster C., Hegewisch A. (Eds), \nPolicy and Practice in European Human Resource Management, \nRoutledge, London. \nGanzach, Y., Kluger, A.N., Klayman, N., 2000. Making decisions \nfrom an interview: expert measurement and mechanical combination, \nPersonnel Psychology 53 1\u201320. \n 16 \nGardiner, A.R., Armstrong-Wright, D., 2000. Employee selection and \nanti-discrimination law: implications for multi-criteria group decision \nsupport, Journal of Multi-Criteria Decision Analysis 9 99\u2013109. \nGuion, R.M., 1998a. Assessment, Measurement and Prediction for \nPersonnel Decision, Lawrence Erlbaum, Mahwah NJ.  \nGuion, R.M., 1998b. Some virtues of dissatisfaction in the science and \npractice of personnel selection, Human Resource Management Review \n8 351\u2013365. \nHenderson, F., Anderson, N., Rick, S., 1995. Future competency \nprofiling, Personnel Review 24 19\u201331.  \nHuo, Y.P., Kearns, J., 1992. Optimizing the Job-person Match with \nComputerized Human Resource Information Systems, Personnel \nReview 21 3\u201318.  \nJaynes, E.T., 1957. Information theory and statistical mechanics, \nPhys. Review 106 620\u2013630 and 108 171\u2013190. \nJessop, A., 1999. Entropy in multiattribute problems, Journal of Multi-\nCriteria Decision Analysis 8 61\u201370. \nJessop, A., 2002. Prioritisation of an IT budget within a local \nauthority, Journal of the Operational Research Society 53 36\u201346. \nKarsak, E.E., 2001. Personnel selection using a fuzzy MCDM \napproach based on ideal and anti-ideal solutions. In: K\u00f6ksalan M., \nZionts S., (Eds), Multiple Criteria Decision Making in the New \nMillenium, Springer, Berlin. \nKinnie, N.J., Arthurs, A.J., 1996. Personnel specialists\u2019 advanced use \nof information technology: evidence and explanations, Personnel \nReview 25 3\u201319.  \nLarichev, O.I., 1992. Cognitive validity in design of decision-aiding \ntechniques, Journal of Multi-Criteria Decision Analysis 1 127\u2013138. \nLowry, P.E., 1994. The structured interview: an alternative to the \nassessment center, Public Personnel Management 23 201\u2013215. \nMiller, G.M., Feinzig, S.L., 1993. Fuzzy sets and personnel selection: \ndiscussion and an application, Journal of Occupational and \nOrganizational Psychology 66 163\u2013169.  \nRanyard, R., Abdel-Nabi, D., 1993. Mental accounting and the \nprocess of multiattribute choice, Acta Psychologica 84 161\u2013177. \nRee, M.J., Earles, J.A., Teachout, M.S., 1994. Predicting job \nperformance: not much more than g, Journal of Applied Psychology 79 \n518\u2013524. .  \nRobertson, I.T., Makin, P.J., 1986 .Management selection in Britain: a \nsurvey and critique, Journal of Occupational Psychology 59 45\u201347. \nRobertson, I.T., Smith, M., 2001. Personnel selection, Journal of \nOccupational and Organizational Psychology 74 441\u2013472 \n 17 \nRoth, P.L., Babko, P., 1997. A research agenda for multi-attribute \nutility analysis in human resource management, Human Resource \nManagement Review 7 341\u2013369. \nSalgado, J.F., 1997. The five factor model of personality and job \nperformance in the European Community, Journal of Applied \nPsychology 82 30\u201343. \nSpyridakos, A., Siskos, Y., Yannacopoulos, D., Skouris, A., 2001. \nMulticriteria job evaluation for large organisations, Eurpoean Journal \nof Operational Research. 130 375\u2013387.  \nStewart, T.J., 1992. A critical survey on the status of multiple criteria \ndecision making theory and practice, Omega 20 569\u2013586. \nTimmermans, D., Vlek, C., 1992. Multi-attribute decision support and \ncomplexity: an evaluation and process analysis of aided versus unaided \ndecision making, Acta Psychologica 80 49\u201365. \nTimmermans, D., Vlek, C., 1996. Effects on decision quality of \nsupporting multi-attribute evaluation in groups, Organizational \nBehavior and Human Decision Processes 68 158\u2013170.  \nvon Winterfeldt, D., Edwards, W., 1986. Decision Analysis and \nBehavioral Research, Cambridge University Press, Cambridge. \n \n \n 18 \n \n \n \ncriterion validity \ncognitive ability and integrity 0.65 \ncognitive ability and structured \ninterviews \n0.63 \ncognitive ability and worksample 0.60 \nwork sample tests 0.54 \ncognitive tests 0.51 \nstructured interviews 0.51 \njob knowledge tests 0.48 \nintegrity tests 0.41 \npersonality tests  0.40 \nassessment centres 0.37 \nbiodata 0.35 \nconscientiousness 0.31 \nreferences 0.26 \nyears of job experience 0.18 \nyears of education 0.10 \ninterests 0.10 \ngraphology 0.02 \nage -0.01 \n \nTable 1.  The validity of job performance criteria \n(Robertson and  Smith, 2001, Figure 1) \nNote: validity has a maximum of 1.0 \n \n \n \n 19 \n \n \n \nMethod % better than chance \nAssessment centres 17\u201318 \nWork sample \/ simulation 14\u201329 \nSupervisory \/ line management evaluation 18 \nMental ability 6\u201320 \nBiodata 6\u201314 \nReferences 3\u20137 \nInterviews 2\u20135 \nPersonality test and self-report questionnaires 2.5 \nGraghology 0 \n \nTable 2.  Selection method improvement on chance \n(Nelson and Wedderburn in Beardwell and Holden, 1994, p.251) \n \n \n 20 \n \n \n DW DK E F FIN IRL N NL P S T UK\nApplication form 96 48 87 95 82 91 59 94 83 na 95 97\nInterview panel 86 99 85 92 99 87 78 69 97 69 64 71\nBio data 20 92 12 26 48 7 56 20 62 69 39 8\nPsychometric testing 6 38 60 22 74 28 11 31 58 24 8 46\nGraphology 8 2 8 57 2 1 0 2 2 0 0 1\nReferences 66 79 54 73 63 91 92 47 55 96 69 92\nAptitude test 8 17 72 28 42 41 19 53 17 14 33 45\nAssessment centre 13 4 18 9 16 7 5 27 2 5 4 18\nGroup selection methods 4 8 22 10 8 8 1 2 18 3 23 13\nOther 3 2 4 3 2 6 5 6 0 5 6 4\n \nTable 3. Recruitment methods in 12 countries in Europe. \n(% of selections using each) (Dany and Torchy, 1994) \n \n 21 \n \n \n \n \n \n candidates (Red set) candidates (Blue set) \ncriterion  a b c d e a b c d e \n1  3 2 2 3 4 3 5 3 2 4 \n2  5 3 3 4 1 2 3 5 2 3 \n3  2 3 4 2 5 4 4 2 3 1 \n4  5 3 4 4 4 3 2 2 3 4 \n5  2 3 4 4 3 3 2 3 2 3 \n6  5 1 3 5 5 3 2 1 4 2 \n7  2 4 2 3 4 2 4 5 2 5 \nsum:  24 19 22 25 26 20 22 21 18 22 \n \n \nTable 4. Data sets used in experiment. \n \n \n 22 \n \n \n \ncode first problem, P1 second problem, P2 \nBS Blue set & maxEntScores Red set & maxEntWeights   \nRS Red set & maxEntScores Blue set & maxEntWeights   \nBW Blue set & maxEntWeights  Red set & maxEntScores \nRW Red set & maxEntWeights   Blue set & maxEntScores \n \nTable 5.  The four experimental pairs \n \n \n 23 \n \n \n \n \n \n  time (mins) difficulty inputs \nno.  pairs P1 P2 P1 P2 P1 P2 \na  b c d e f g h \n1  BS 11 1 2 1 W8 (W8) \n2   5 4 1 2 W7,S1 (W7),S1 \n3   6 4 4 3 W12 (W12) \n4   5 4 3 4 W6 (W6),S1 \nmean: 6.75 3.25 2.5 2.5 W8.25,S0.25 (W8.25),S0.25 \n5  BW 10 7 2 3 W5,S1 (W5),S1 \n6   8 4 4 3 W2,S1 (W2),W1,S1 \n7   12 3 3 2 W4,S2 (W4),S1 \n8   20 2 5 3 W11 (W11) \nmean: 12.5 4 3.5 2.75 W5.5,S0.75 (W5.5),W0.25,S0.75 \n9  RS 9 5 2 3 W5 (W5),S1 \n10   12 5 2 4 W6 (W6) \n11   6 3 2 4 W6 (W6) \n12   5 1 2 1 W13 (W12) \nmean: 8 3.5 2 3 W7.5 (W7.5),S0.25 \n13  RW 19 8 3 4 W3,S2 (W3),W1,S2 \n14   8 1 4 1 W5 (W5) \n15   12 9 4 4 W4 (W4),W4 \n16   8 7 4 5 W1,S1 S4 \nmean: 11.75 6.25 3.75 3.5 W4.25,S0.5 (W4),W0.5,S0.5 \n \n \n \nTable 6.  Results. \n \n \n 24 \n \n \n \n \n \n \npairs \nnumber of \njudgemental \nnumber of \nextra inputs\n \ntime \n \ndifficulty \n inputs for P1 for P2 P1 P2 P1 P2 \nBS 8.5 0.5 6.75 3.25 2.5 2.5 \nRS 7.5 0.25 8 3.5 2 3 \nBW 7.25 1 12.5 4 3.5 2.75 \nRW 4.75 1 11.75 6.25 3.75 3.5 \n \n \n \nTable 7.  Average results for different pairs \n \n 25 \n \n \n \npairs \nnumber of \njudgemental \n \ntime \n \ndifficulty \n inputs for P1 P1 P2 P1 P2 \nBS 8.5 6.75 6.25 2.5 3.5 \nRS 7.5 8 4 2 2.75 \nBW 7.25 12.5 3.5 3.5 3 \nRW 4.75 11.75 3.25 3.75 2.5 \n \n \nTable 8.  Averages showing time and difficulty when each pair \nappears as P1 and as P2 \n \n \n 26 \n \n  \nMAKING RECRUITMENT DECISIONS \n \nThe purpose of this experiment is to examine the ease of use and \nusefulness of a simple decision aid for personnel selection. \nFive hypothetical candidates have made it to the shortlist for the \njob of Postgraduate Admissions Officer for the University. The details \nof the post are attached. \nEach candidate has been assessed against seven criteria. The \nassessment is based on their application form, cv, interview, \nreferences, and performance at an assessment centre. Assessments \nhave been scored by a panel on a scale of 1 to 5, with 5 representing \nexcellence. \nYou will be presented with a table such as this \n \n candidate  \ncriterion a b c d e \n1. Written communication 2 5 4 5 4 \n2. Oral communication 3 1 4 2 2 \n3. Planning 5 3 3 4 1 \n4. Organising ability 5 3 4 2 1 \n5. Team player 4 4 2 4 2 \n6. Works independently 3 4 3 1 5 \n7. Decisiveness 4 2 4 4 2 \n \n \nThe task is to select the preferred candidate. \nAn overall score for each candidate will be given by the weighted \naverage of the scores in the table above. The candidate with the \nhighest score will be selected. \nYour input is to make statements about the relative importance of \nthe criteria and\/or the relative merits of the candidates. You may say \nsomething like \n \n\u201cI think that being a team player is at least twice as important \nas being good at oral communication\u201d \n \nor \n \n\u201cI think that candidate a is better than candidate d\u201d \n \nAfter each such input the score for each candidate is recalculated \nand displayed. You can continue with these judgements until you are \nsatisfied with the selection recommended. \nYou will be asked to make two such selections, with different \ncandidates in each case. \n \n \n \n \nFigure 1.  Instructions for experiment. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n0\n5\n10\n15\n20\n25\n0 2 4 6 8 10 12 14\nno. of judgements\ntim\ne \n(m\nin\ns) BS\nBW\nRS\nRW\n \nFigure 2.  Time taken and no. of judgementd needed for P1 problems \n \n \n \n \n \n \n 27 \n"}