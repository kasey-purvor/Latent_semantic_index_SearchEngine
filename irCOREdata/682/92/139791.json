{"doi":"10.1145\/1141885.1141888","coreId":"139791","oai":"oai:dspace.lib.cranfield.ac.uk:1826\/4158","identifiers":["oai:dspace.lib.cranfield.ac.uk:1826\/4158","10.1145\/1141885.1141888"],"title":"An Efficient overloaded implementation of forward mode automatic differentiation\nin MATLAB","authors":["Forth, Shaun A."],"enrichments":{"references":[{"id":37951594,"title":"Accepted Aug. 2005.E\ufb03cient Forward Mode","authors":[],"date":"2004","doi":null,"raw":"ACM Transactions on Mathematical Software, Vol. , No. , Accepted Aug. 2005.E\ufb03cient Forward Mode AD in MATLAB \u00b7 25 Forth, S. A. and Ketzscher, R. 2004. High-level interfaces for the MAD (Matlab Automatic Di\ufb00erentiation) package. In 4th European Congress on Computational Methods in Applied Sciences and Engineering (ECCOMAS), P. Neittaanm\u00a8 aki, T. Rossi, S. Korotov, E. O\u02dc nate, J. P\u00b4 eriaux, and D. Kn\u00a8 orzer, Eds. Vol. 2. University of Jyv\u00a8 askyl\u00a8 a, Department of Mathematical Information Technology, Finland. ISBN 951-39-1869-6.","cites":null},{"id":37951614,"title":"AD tools and prospects for optimal AD in CFD \ufb02ux Jacobian calculations. In Automatic Di\ufb00erentiation: From Simulation to","authors":[],"date":"2001","doi":"10.1007\/978-1-4613-0075-5_30","raw":"Tadjouddine, M., Forth, S. A., and Pryce, J. D. 2001. AD tools and prospects for optimal AD in CFD \ufb02ux Jacobian calculations. In Automatic Di\ufb00erentiation: From Simulation to ACM Transactions on Mathematical Software, Vol. , No. , Accepted Aug. 2005.26 \u00b7 Shaun A. Forth August 23, 2006 Optimization, G. Corliss, C. Faure, A. Griewank, L. Hasco\u00a8 et, and U. Naumann, Eds. Computer and Information Science. Springer, New York, Chapter 30, 247\u2013252.","cites":null},{"id":37951581,"title":"ADIC \u2014 An extensible automatic di\ufb00erentiation tool for ANSI-C.","authors":[],"date":"1997","doi":"10.1002\/(sici)1097-024x(199712)27:12<1427::aid-spe138>3.0.co;2-q","raw":"Bischof, C. H., Roh, L., and Mauer, A. 1997. ADIC \u2014 An extensible automatic di\ufb00erentiation tool for ANSI-C. Software \u2013 Practice and Experience 27, 12, 1427\u20131456. See www-fp.mcs.anl. gov\/adic\/.","cites":null},{"id":37951579,"title":"ADIFOR 2.0: Automatic di\ufb00erentiation of Fortran 77 programs.","authors":[],"date":"1996","doi":"10.1109\/99.537089","raw":"Bischof, C. H., Carle, A., Khademi, P., and Mauer, A. 1996. ADIFOR 2.0: Automatic di\ufb00erentiation of Fortran 77 programs. IEEE Computational Science & Engineering 3, 3, 18\u201332.","cites":null},{"id":37951586,"title":"ADMAT: An automatic di\ufb00erentiation toolbox for MATLAB.","authors":[],"date":"1998","doi":"10.1145\/347837.347879","raw":"Coleman, T. F. and Verma, A. 1998a. ADMAT: An automatic di\ufb00erentiation toolbox for MATLAB. Tech. rep., Computer Science Department, Cornell University.","cites":null},{"id":37951619,"title":"ADMAT: Automatic di\ufb00erentiation in MATLAB using object oriented methods.","authors":[],"date":"1998","doi":null,"raw":"Diploma Thesis, Institute for Scienti\ufb01c Computing, Aachen University, Germany. Verma, A. 1998a. ADMAT: Automatic di\ufb00erentiation in MATLAB using object oriented methods. In SIAM Interdisciplinary Workshop on Object Oriented Methods for Interoperability. SIAM, National Science Foundation, Yorktown Heights, New York, 174\u2013183.","cites":null},{"id":37951589,"title":"ADMIT-1: Automatic di\ufb00erentiation and MATLAB interface toolbox.","authors":[],"date":"2000","doi":"10.1145\/347837.347879","raw":"Coleman, T. F. and Verma, A. 2000. ADMIT-1: Automatic di\ufb00erentiation and MATLAB interface toolbox. ACM Trans. Math. Softw. 26, 1 (Mar.), 150\u2013175.","cites":null},{"id":37951607,"title":"ADO1, a Fortran 90 code for automatic di\ufb00erentiation.","authors":[],"date":"1998","doi":null,"raw":"Pryce, J. D. and Reid, J. K. 1998. ADO1, a Fortran 90 code for automatic di\ufb00erentiation.","cites":null},{"id":37951598,"title":"Algorithm 755: ADOL\u2013C, a package for the automatic di\ufb00erentiation of algorithms written in C\/C++.","authors":[],"date":"1996","doi":"10.1145\/229473.229474","raw":"Griewank, A., Juedes, D., and Utke, J. 1996. Algorithm 755: ADOL\u2013C, a package for the automatic di\ufb00erentiation of algorithms written in C\/C++. ACM Trans. Math. Softw. 22, 2, 131\u2013167.","cites":null},{"id":37951578,"title":"Automatic di\ufb00erentiation for MATLAB programs.","authors":[],"date":"2003","doi":"10.1002\/pamm.200310013","raw":"Bischof, C., Lang, B., and Vehreschild, A. 2003. Automatic di\ufb00erentiation for MATLAB programs. Proc. Appl. Math. Mech 2, 1, 50\u201353.","cites":null},{"id":37951608,"title":"Automatic di\ufb00erentiation in MATLAB.","authors":[],"date":"1992","doi":"10.1016\/0168-9274(92)90065-l","raw":"Rich, L. C. and Hill, D. R. 1992. Automatic di\ufb00erentiation in MATLAB. App. Num. Math. 9, 33\u201343.","cites":null},{"id":37951590,"title":"Automatic Di\ufb00erentiation: From Simulation to Optimization. Computer and Information Science.","authors":[],"date":"2001","doi":null,"raw":"Corliss, G., Faure, C., Griewank, A., Hasco\u00a8 et, L., and Naumann, U., Eds. 2001. Automatic Di\ufb00erentiation: From Simulation to Optimization. Computer and Information Science. Springer, New York.","cites":null},{"id":37951601,"title":"Chapter 19: The TOMLAB optimization environment.","authors":[],"date":"2004","doi":"10.1007\/978-1-4613-0215-5_19","raw":"Holmstr\u00a8 om, K. and Edvall, M. M. January 2004. Chapter 19: The TOMLAB optimization environment. In Modeling Languages in Mathematical Optimization, J. Kallrath, Ed. APPLIED OPTIMIZATION 88, ISBN 1-4020-7547-2. Kluwer Academic Publishers, Boston\/Dordrecht\/London.","cites":null},{"id":37951577,"title":"Combining source transformation and operator overloading techniques to compute derivatives for MATLAB programs.","authors":[],"date":"2002","doi":"10.1109\/scam.2002.1134106","raw":"Bischof, C., B\u00a8 ucker, H., Lang, B., Rasch, A., and Vehreschild, A. 2002. Combining source transformation and operator overloading techniques to compute derivatives for MATLAB programs. In Proceedings of the Second IEEE International Workshop on Source Code Analysis and Manipulation (SCAM 2002). IEEE Computer Society, p. 65\u201372.","cites":null},{"id":37951604,"title":"E\ufb03cient calculation of Jacobian matrices by optimized application of the chain rule to computational graphs.","authors":[],"date":"1999","doi":null,"raw":"Naumann, U. 1999. E\ufb03cient calculation of Jacobian matrices by optimized application of the chain rule to computational graphs. Ph.D. thesis, Technical University of Dresden.","cites":null},{"id":37951580,"title":"E\ufb03cient computations of gradients and Jacobians by dynamic exploitation of sparsity in automatic di\ufb00erentiation.","authors":[],"date":"1996","doi":"10.1080\/10556789608805642","raw":"Bischof, C. H., Khademi, P. M., Bouaricha, A., and Carle, A. 1996. E\ufb03cient computations of gradients and Jacobians by dynamic exploitation of sparsity in automatic di\ufb00erentiation. Optimization Methods and Software 7, 1\u201339.","cites":null},{"id":37951597,"title":"Evaluating Derivatives: Principles and Techniques of Algorithmic Di\ufb00erentiation.","authors":[],"date":"2000","doi":"10.1137\/1.9780898717761","raw":"Griewank, A. 2000. Evaluating Derivatives: Principles and Techniques of Algorithmic Di\ufb00erentiation. Number 19 in Frontiers in Appl. Math. SIAM, Philadelphia, Penn.","cites":null},{"id":37951609,"title":"Improved \ufb01tting of constrained multivariate regression models using automatic di\ufb00erentiation.","authors":[],"date":"2002","doi":"10.1007\/978-3-642-57489-4_57","raw":"Ringrose, T. J. and Forth, S. A. 2002. Improved \ufb01tting of constrained multivariate regression models using automatic di\ufb00erentiation. In COMPSTAT 2002: Proceedings in Computational Statistics, 15th Symposium, W. Hardle and B. Ronz, Eds. Physica-Verlag, Heidelberg, Berlin, Germany, 383\u2013388.","cites":null},{"id":37951595,"title":"Jacobian code generated by source transformation and vertex elimination can be as e\ufb03cient as hand-coding.","authors":[],"date":"2004","doi":"10.1145\/1024074.1024076","raw":"Forth, S. A., Tadjouddine, M., Pryce, J. D., and Reid, J. K. 2004. Jacobian code generated by source transformation and vertex elimination can be as e\ufb03cient as hand-coding. ACM Trans.","cites":null},{"id":37951606,"title":"Numerical Optimization. Springer series in operational research.","authors":[],"date":"1999","doi":"10.1007\/b98874","raw":"Nocedal, J. and Wright, S. J. 1999. Numerical Optimization. Springer series in operational research. Springer-Verlag, New York.","cites":null},{"id":37951582,"title":"On e\ufb03cient solutions to the continuous sensitivity equation using automatic di\ufb00erentiation.","authors":[],"date":"2000","doi":"10.1137\/s1064827599352136","raw":"Borggaard, J. and Verma, A. 2000. On e\ufb03cient solutions to the continuous sensitivity equation using automatic di\ufb00erentiation. SIAM J. Sci. Comput. 22, 1, 39\u201362.","cites":null},{"id":37951599,"title":"On the calculation of Jacobian matrices by the Markowitz rule.","authors":[],"date":"1991","doi":null,"raw":"Griewank, A. and Reese, S. 1991. On the calculation of Jacobian matrices by the Markowitz rule. In Automatic Di\ufb00erentiation of Algorithms: Theory, Implementation, and Application, A. Griewank and G. F. Corliss, Eds. SIAM, Philadelphia, Penn., 126\u2013135.","cites":null},{"id":37951605,"title":"Optimal accumulation of Jacobian matrices by elimination methods on the dual computational graph.","authors":[],"date":"2004","doi":"10.1007\/s10107-003-0456-9","raw":"Naumann, U. 2004. Optimal accumulation of Jacobian matrices by elimination methods on the dual computational graph. Mathematical Programming 99, 3 (April), 399\u2013421.","cites":null},{"id":37951617,"title":"Optimization Toolbox User\u2019s Guide, Version 2.","authors":[],"date":"2003","doi":"10.2172\/809999","raw":"The Mathworks Inc. Sept 2003. Optimization Toolbox User\u2019s Guide, Version 2. The Mathworks Inc., 3 Apple Hill Drive, Natick MA 01760-2098.","cites":null},{"id":37951600,"title":"Ordinary Di\ufb00erential Equations II, Sti\ufb00 and Di\ufb00erentialAlgebraic Problems.","authors":[],"date":"1991","doi":"10.1007\/978-3-662-09947-6_1","raw":"Hairer, E. and Wanner, G. 1991. Ordinary Di\ufb00erential Equations II, Sti\ufb00 and Di\ufb00erentialAlgebraic Problems. Springer-Verlag, Berlin.","cites":null},{"id":37951621,"title":"revised","authors":[],"date":"2004","doi":null,"raw":"Received May 2004; revised May 2005; accepted August 2005.","cites":null},{"id":37951618,"title":"Semantic augmentation of MATLAB programs to compute derivatives.","authors":[],"date":"2001","doi":null,"raw":"Vehreschild, A. 2001. Semantic augmentation of MATLAB programs to compute derivatives.","cites":null},{"id":37951611,"title":"Simplifying multivariate second order response surfaces by \ufb01tting constrained models using automatic di\ufb00erentiation. Technometrics Accepted.","authors":[],"date":"2004","doi":"10.1198\/004017005000000148","raw":"Ringrose, T. J. and Forth, S. A. 2004. Simplifying multivariate second order response surfaces by \ufb01tting constrained models using automatic di\ufb00erentiation. Technometrics Accepted.","cites":null},{"id":37951603,"title":"Source transformation for automatic di\ufb00erentiation in MATLAB.","authors":[],"date":"2004","doi":"10.1007\/11758549_77","raw":"Kharche, R. V. 2004. Source transformation for automatic di\ufb00erentiation in MATLAB. M.S. thesis, Cran\ufb01eld University (Shrivenham Campus), Applied Mathematics & Operational Research Group, Engineering Systems Department, RMCS Shrivenham, Swindon SN6 8LA, UK.","cites":null},{"id":37951596,"title":"Sparse matrices in MATLAB: Design and implementation.","authors":[],"date":"1992","doi":"10.1137\/0613024","raw":"Gilbert, J. R., Moler, C., and Schreiber, R. 1992. Sparse matrices in MATLAB: Design and implementation. SIAM Journal on Matrix Analysis and Applications 13, 1 (Jan.), 333\u2013356.","cites":null},{"id":37951584,"title":"Structure and e\ufb03cient Jacobian calculation.","authors":[],"date":"1996","doi":"10.1007\/978-1-4613-3335-7_3","raw":"Coleman, T. F. and Verma, A. 1996. Structure and e\ufb03cient Jacobian calculation. In Computational Di\ufb00erentiation: Techniques, Applications, and Tools, M. Berz, C. Bischof, G. Corliss, and A. Griewank, Eds. SIAM, Philadelphia, Penn., 149\u2013159.","cites":null},{"id":37951620,"title":"Structured automatic di\ufb00erentiation.","authors":[],"date":"1998","doi":"10.1007\/978-1-4612-1780-0_7","raw":"Verma, A. 1998b. Structured automatic di\ufb00erentiation. Ph.D. thesis, Cornell University Department of Computer Science, Ithaca, NY. ACKNOWLEDGMENTS The author would like to thank colleagues: Robert Ketzscher for valuable work performed optimizing Mad [Shampine et al. 2005]; and Venkat Sastry, John Pryce and Yi Cao for helpful suggestions regarding MATLAB performance optimization.","cites":null},{"id":37951588,"title":"The e\ufb03cient computation of sparse Jacobian matrices using automatic di\ufb00erentiation.","authors":[],"date":"1998","doi":"10.1137\/s1064827595295349","raw":"Coleman, T. F. and Verma, A. 1998b. The e\ufb03cient computation of sparse Jacobian matrices using automatic di\ufb00erentiation. SIAM J. Sci. Comput. 19, 4, 1210\u20131233.","cites":null},{"id":37951612,"title":"The MATLAB ODE suite.","authors":[],"date":"1997","doi":"10.1137\/s1064827594276424","raw":"Shampine, L. and Reichelt, M. 1997. The MATLAB ODE suite. SIAM J. Sci. Comput. 18, 1\u201322.","cites":null},{"id":37951615,"title":"The TAPENADE tutorial http:\/\/www-sop.inria.fr\/tropics\/tapenade\/ tutorial.html. Web Site.","authors":[],"date":"2003","doi":null,"raw":"Tapenade 2003. The TAPENADE tutorial http:\/\/www-sop.inria.fr\/tropics\/tapenade\/ tutorial.html. Web Site.","cites":null},{"id":37951583,"title":"The use of numerical optimisation to determine on-limit handling behaviour of race cars.","authors":[],"date":"2004","doi":null,"raw":"Bradshaw, D. 2004. The use of numerical optimisation to determine on-limit handling behaviour of race cars. Ph.D. thesis, School of Engineering, Department of Automotive, Mechanical and Structural Engineering, Cran\ufb01eld University, Bedfordshire, MK43 0AL, UK.","cites":null},{"id":37951591,"title":"Transformation of Algorithms in Fortran, Manual, Draft Version,","authors":[],"date":null,"doi":null,"raw":"FastOpt 2003. Transformation of Algorithms in Fortran, Manual, Draft Version, TAF Version 1.6. FastOpt. See http:\/\/www.FastOpt.com\/taf.","cites":null},{"id":37951592,"title":"User guide for MAD - a Matlab automatic di\ufb00erentiation toolbox.","authors":[],"date":"2001","doi":null,"raw":"Forth, S. A. 2001. User guide for MAD - a Matlab automatic di\ufb00erentiation toolbox. Applied Mathematics and Operational Research Report AMOR 2001\/5, Cran\ufb01eld University (RMCS Shrivenham), Swindon, SN6 8LA, UK. June.","cites":null},{"id":37951593,"title":"User Guide for MAD - MATLAB Automatic Di\ufb00erentiation Toolbox TOMLAB\/MAD, Version 1.1 The Forward Mode.","authors":[],"date":"2004","doi":null,"raw":"Forth, S. A. and Edvall, M. M. 2004. User Guide for MAD - MATLAB Automatic Di\ufb00erentiation Toolbox TOMLAB\/MAD, Version 1.1 The Forward Mode. TOMLAB Optimisation Inc., 855 Beech St 12, San Diego, CA 92101, USA. See http:\/\/tomlab.biz\/products\/mad.","cites":null},{"id":37951602,"title":"User\u2019s guide for TOMLAB 4.3.","authors":[],"date":"2004","doi":null,"raw":"Holmstr\u00a8 om, K., G\u00a8 oran, A. O., and Edvall, M. M. 2004. User\u2019s guide for TOMLAB 4.3. TOMLAB Optimisation Inc., 855 Beech St 12, San Diego, CA 92101, USA. See http:\/\/www. tomlab.biz.","cites":null},{"id":37951613,"title":"Using AD to solve BVPs in MATLAB.","authors":[],"date":"2005","doi":"10.1145\/1055531.1055535","raw":"Shampine, L. F., Ketzscher, R., and Forth, S. A. 2005. Using AD to solve BVPs in MATLAB. ACM Trans. Math. Softw. 31, 1 (Mar.), 79\u201394.","cites":null},{"id":37951616,"title":"Using Matlab, Version 6.","authors":[],"date":"2003","doi":"10.1002\/047172386x.ch1","raw":"The MathWorks Inc. 2003. Using Matlab, Version 6. The MathWorks Inc., 24 Prime Park Way, Natick, MA 01760-1500.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2006-06-01T00:00:00Z","abstract":"The Mad package described here facilitates the evaluation of first derivatives\nof multi-dimensional functions that are defined by computer codes written in\nMATLAB. The underlying algorithm is the well-known forward mode of automatic\ndifferentiation implemented via operator overloading on variables of the class\nfmad. The main distinguishing feature of this MATLAB implementation is the\nseparation of the linear combination of derivative vectors into a separate\nderivative vector class derivvec. This allows for the straightforward\nperformance optimisation of the overall package. Additionally by internally\nusing a matrix (two-dimensional) representation of arbitrary dimension\ndirectional derivatives we may utilise MATLAB\"s sparse matrix class to\npropagate sparse directional derivatives for MATLAB code which uses arbitrary\ndimension arrays. On several examples the package is shown to be more efficient\nthan Verma\"s ADMAT package.\u00c2\u00a9 ACM, 2006. This is the author's version of\nthe work. It is posted here by permission of ACM for your personal use. Not for\nredistribution. The definitive version was published in ACM Transactions on\nMathematical Software (TOMS) Volume 32 , Issue 2 (June 2006) 195 - 222, 2006\nISSN:0098-3500 http:\/\/doi.acm.org\/10.1145\/1141885.114188","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/139791.pdf","fullTextIdentifier":"http:\/\/dx.doi.org\/10.1145\/1141885.1141888","pdfHashValue":"4bf5f4b6299b3b2cbc7f6411ef0554f5e53eb0fb","publisher":"ACM Association for Computing Machinery","rawRecordXml":"<record><header><identifier>\noai:dspace.lib.cranfield.ac.uk:1826\/4158<\/identifier><datestamp>2011-03-10T10:54:06Z<\/datestamp><setSpec>hdl_1826_13<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>An Efficient overloaded implementation of forward mode automatic differentiation\nin MATLAB<\/dc:title><dc:creator>Forth, Shaun A.<\/dc:creator><dc:description>The Mad package described here facilitates the evaluation of first derivatives\nof multi-dimensional functions that are defined by computer codes written in\nMATLAB. The underlying algorithm is the well-known forward mode of automatic\ndifferentiation implemented via operator overloading on variables of the class\nfmad. The main distinguishing feature of this MATLAB implementation is the\nseparation of the linear combination of derivative vectors into a separate\nderivative vector class derivvec. This allows for the straightforward\nperformance optimisation of the overall package. Additionally by internally\nusing a matrix (two-dimensional) representation of arbitrary dimension\ndirectional derivatives we may utilise MATLAB\"s sparse matrix class to\npropagate sparse directional derivatives for MATLAB code which uses arbitrary\ndimension arrays. On several examples the package is shown to be more efficient\nthan Verma\"s ADMAT package.\u00c2\u00a9 ACM, 2006. This is the author's version of\nthe work. It is posted here by permission of ACM for your personal use. Not for\nredistribution. The definitive version was published in ACM Transactions on\nMathematical Software (TOMS) Volume 32 , Issue 2 (June 2006) 195 - 222, 2006\nISSN:0098-3500 http:\/\/doi.acm.org\/10.1145\/1141885.1141888<\/dc:description><dc:publisher>ACM Association for Computing Machinery<\/dc:publisher><dc:date>2010-12-17T13:38:57Z<\/dc:date><dc:date>2010-12-17T13:38:57Z<\/dc:date><dc:date>2006-06-01T00:00:00Z<\/dc:date><dc:type>Article<\/dc:type><dc:identifier>CM Transactions on Mathematical Software (TOMS) Volume 32 , Issue 2 (June 2006) 195 - 222, 2006<\/dc:identifier><dc:identifier>0098-3500<\/dc:identifier><dc:identifier>http:\/\/dx.doi.org\/10.1145\/1141885.1141888<\/dc:identifier><dc:identifier>http:\/\/dspace.lib.cranfield.ac.uk\/handle\/1826\/4158<\/dc:identifier><dc:language>en_UK<\/dc:language><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["issn:0098-3500","0098-3500"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2006,"topics":[],"subject":["Article"],"fullText":"An Efficient Overloaded Implementation of Forward\nMode Automatic Differentiation in MATLAB\nSHAUN A. FORTH\nCranfield University (Shrivenham Campus)\nThe Mad package described here facilitates the evaluation of first derivatives of multi-dimensional\nfunctions that are defined by computer codes written in MATLAB. The underlying algorithm is\nthe well-known forward mode of automatic differentiation implemented via operator overloading\non variables of the class fmad. The main distinguishing feature of this MATLAB implementation is\nthe separation of the linear combination of derivative vectors into a separate derivative vector class\nderivvec. This allows for the straightforward performance optimisation of the overall package.\nAdditionally by internally using a matrix (two-dimensional) representation of arbitrary dimension\ndirectional derivatives we may utilise MATLAB\u2019s sparse matrix class to propagate sparse direc-\ntional derivatives for MATLAB code which uses arbitrary dimension arrays. On several examples\nthe package is shown to be more efficient than Verma\u2019s ADMAT package.\nCategories and Subject Descriptors: D.1.5 [Programming Techniques]: Object Oriented Pro-\ngramming; G.1.4 [Numerical Analysis]: Quadrature and Numerical Differentiation\u2014Automatic\nDifferentiation; G.1.6 [Numerical Analysis]: Optimization\u2014Gradient Methods; G.1.7 [Numer-\nical Analysis]: Ordinary Differential Equations\u2014Stiff Equations; G.1.7 [Numerical Analysis]:\nRoots of Nonlinear Equations\u2014Systems of Equations; G.4. [Mathematical Software]: Effi-\nciency\nGeneral Terms: Performance\nAdditional Key Words and Phrases: MATLAB, efficient computation of Jacobians\n1. INTRODUCTION\nIn the standard reference for the subject, Griewank [2000] states that,\nAlgorithmic, or automatic differentiation (AD) is concerned with the\naccurate and efficient evaluation of derivatives for functions defined by\ncomputer programs.\nAD uses the systematic application of the chain rule of differentiation applied to\nthe floating point representation of a variable\u2019s value and its derivatives. Unlike\nthe finite-difference approximation, no discretisation or cancellation errors are in-\ncurred, and the resulting derivative values are accurate to within floating-point\nround-off. Since only floating point values are used (unlike differentiation within\nsymbolic algebra packages such as Mathematica or Maple) good efficiency may be\nobtained. Additionally, AD permits the use of control structures (loops, branches\nand sub-functions) common to modern computer languages but not easily amenable\nto symbolic differentiation.\nAuthor\u2019s address: Applied Mathematics and Operational Research Group, Engineering\nSystems, Cranfield University (Shrivenham Campus), SWINDON SN6 8LA, U.K., email:\nS.A.Forth@cranfield.ac.uk.\nc\u00a9ACM, (2006). This is the author\u2019s version of the work. It is posted here by permission of\nACM for your personal use. Not for redistribution. The definitive version was published in ACM\nTransactions on Mathematical Software Volume 32 No 2, p 195-222, June 2006.\nACM Transactions on Mathematical Software, Vol. , No. , Accepted Aug. 2005, Pages 1\u201326.\n2 \u00b7 Shaun A. Forth August 23, 2006\n1.1 Forward and Reverse Mode AD\nThere are two fundamental algorithms or modes of AD for calculating first deriva-\ntives \u2013 forward and reverse [Griewank 2000, Chap. 3].\nForward, or Tangent(-Linear), Mode AD involves enhancing the original\nfunction code so that a variable\u2019s directional derivatives are calculated along with its\nvalue. For example, if we set the values of scalar variables x1 and x2 and their scalar\nderivatives Dx1 and Dx2, then a variable defined by the statement y = x1*x2 must\nhave its derivatives Dy calculated via the product rule as Dy = x1*Dx2+Dx1*x2.\nClearly, if we initialise Dx1 = 1 and Dx2 = 0, for arbitrary, initialised values of x1\nand x2, we obtain the value of the directional derivative of y in the x1 coordinate\ndirection. If we define Dx1 and Dx2 to be vectors of length 2 with Dx1 = (1,0) and\nDx2 = (0,1), and interpret Dy = x1*Dx2+Dx1*x2 as a vector statement, the calcu-\nlated Dy is the gradient of y for the supplied values of x1 and x2. By systematically\nperforming such derivative operations for all the necessary floating point operations\nin the original function code, following the control flow (through branches, loops,\nsub-functions) as dictated by the values of variables, then gradients of all variables\nmay be calculated.\nReverse, or Adjoint, Mode AD is a two stage process. First the original func-\ntion code is run, perhaps augmented by statements to store data to enable the code\nto be run a second time in reverse, propagating the sensitivities of the function\u2019s\noutput to each calculated variable. Such sensitivities are termed adjoints.\nGriewank [2000] presents a computational complexity analysis for the run time,\ntime(Jf(x)), to calculate the Jacobian Jf(x) of a function f(x) \u2208 IRn \u2192 IRm\nby both forward and reverse mode AD. For the forward mode, time(Jf(x)) =\n\u03c9fwd \u00d7 time(f(x)), where time(f(x)) is the run-time for the original function.\nThe coefficient \u03c9fwd \u2208 [1 + n, 1 + 1.5n] is dependent on machine characteristics\n(e.g., relative time for memory access compared to floating point operation). In\na similar manner for reverse mode AD time(Jf(x)) = \u03c9rev \u00d7 time(f(x)) with\n\u03c9rev \u2208 [1+2m, 1.5+2.5m] again dependent on machine characteristics. The larger\ncoefficients in the bounds for \u03c9rev compared to \u03c9fwd reflect the extra memory\noperations in the reverse mode\u2019s reverse pass to recover values stored in its forward\npass. Such operations are not needed in the single pass forward mode.\nIf m \u001c n, i.e. the gradients of a small number of function outputs are required\nwith respect to a large number of function inputs, then reverse mode is to be\npreferred. Examples of such cases typically arise in large-scale optimisation.\n1.2 Implementation of AD\nAD is implemented in one of two ways: operator overloading or source transforma-\ntion [Griewank 2000, Chapter 5].\nThe operator overloading approach takes advantage of the facility to define\nnew classes (or types) within modern computer languages such as Fortran 95, C++\nor MATLAB. Objects of the new AD class are defined to have a component which\nstores their value and components to store derivative information. Arithmetic and\nintrinsic functions are extended to the AD class making use of operator and function\noverloading. In typed languages such as Fortran or C++, all that remains is for\nthe user to redefine the classes of all relevant objects within the function and all\nACM Transactions on Mathematical Software, Vol. , No. , Accepted Aug. 2005.\nEfficient Forward Mode AD in MATLAB \u00b7 3\nsub-functions to that of the AD class, initialise appropriate values and derivatives,\ninvoke the function, and then extract the values of the derivatives. Representative\nexamples of such implementations are the packages ADO1 [Pryce and Reid 1998]\nand ADOL-C [Griewank et al. 1996].\nThe alternative source transformation approach requires the development of\nsophisticated compiler-type software to read in a computer program, determine\nwhich statements require differentiation, and then write a new version of the original\nprogram augmented with statements to calculate derivatives. Such sophisticated\nAD source transformation packages exist for languages such as Fortran [Bischof\net al. 1996; FastOpt 2003; Tapenade 2003] and C [Bischof et al. 1997].\n1.3 AD in MATLAB\nRich and Hill [1992] provided a limited facility for MATLAB that enabled AD of\nsimple arithmetic expressions defined by a character string. Such strings, together\nwith necessary values of variables were passed to an external routine, written in\nturbo-C, for differentiation. However, the first significant work was that of Cole-\nman and Verma [1998b; 1998a],[Verma 1998b] who, in a monumental coding ef-\nfort, produced an operator-overloading AD package named ADMAT that provides\nfacilities for forward and reverse mode AD for both first and second derivatives\nand runtime Jacobian sparsity detection. These authors also interfaced ADMAT\nwith ADMIT [Coleman and Verma 2000], a package for efficient sparse Jacobian\ncalculation via various colouring algorithms. Recently the ADiMat hybrid source-\ntransformation\/operator overloading AD tool [Vehreschild 2001] has been devel-\noped, and comparisons [Bischof et al. 2003; Bischof et al. 2002] show its forward\nmode to be more efficient than that of ADMAT. It would appear that while AD-\nMAT\u2019s operation count is in agreement with AD theory (see for example the opera-\ntions counts in [Borggaard and Verma 2000]), its run time is not. We have encoun-\ntered similar experiences leading to development of the Mad AD package [Forth\n2001].\n1.4 The Mad AD Package\nOur aim in developing the Mad AD package is to implement in MATLAB the var-\nious AD algorithms in a careful, step-wise manner, taking care to ensure efficiency\nand ease of extension at each stage. The first AD algorithm to be implemented\nwas, as described in this paper, the standard forward mode for first derivatives.\nThis immediately provided a facility for calculating derivatives accurate to float-\ning point round-off. Such derivatives are required for Newton-based algorithms for\nboth nonlinear equation solution and optimization. In the absence of AD such\nderivatives are more conventionally approximated by finite differencing (FD). In\norder to replace FD as the default method for derivative evaluation, we must show\nthat AD has comparable efficiency for derivative evaluation and further that its\ninherent accuracy yields greater overall efficiency and robustness when used within\nderivative-exploiting algorithms.\nAn early decision, based on expedience, was to adopt an operator-overloading\nimplementation. Programming overloaded classes, operations and functions allows\nfor a rapid coverage of most commonly used MATLAB functionality. Indeed, the\nfirst version of the forward mode was written, debugged and facilitating improved\nACM Transactions on Mathematical Software, Vol. , No. , Accepted Aug. 2005.\n4 \u00b7 Shaun A. Forth August 23, 2006\nstatistical data fitting [Ringrose and Forth 2002] for less than a man-month\u2019s effort.\nThe alternative source-transformation approach would be far more time-consuming.\nWe describe our operator-overloaded implementation of forward-mode AD for\na single-directional derivative through the fmad class in Section 2. Our imple-\nmentation of the derivvec class for storing and manipulating multiple directional\nderivatives, described in Section 3, allows us to enhance the fmad class to propagate\nan arbitrary number of directional derivatives. This separation of the storage and\nmanipulation of directional derivatives to within the derivvec class allows us to\ngreatly optimize overall performance by optimizing performance of the small num-\nber of functions of the derivvec class. To ensure efficiency the functions of the\nderivvec class only use high-level array operations [The MathWorks Inc. 2003,\nSection 22]. Also, internal to the derivvec class we use a 2-dimensional array\n(i.e., a matrix) for storing directional derivatives associated with arrays of arbi-\ntrary dimensions. This allows us to use matrices of MATLAB\u2019s sparse class to\nstore sparse directional derivatives with commensurate reduction in run-time and\nmemory-requirements for many large calculations. Section 4 describes how to use\nthe fmad class for Jacobian evaluation using dense, sparse and compressed storage of\ndirectional derivatives. Several examples in Section 5 demonstrate the effectiveness\nof our approach. Section 6 concludes and gives plans for future work.\n2. FORWARD MODE AD FOR A SINGLE DIRECTIONAL DERIVATIVE\nIt is straightforward to implement forward mode AD for a single directional deriv-\native via operator overloading in MATLAB. In Section 2.1 we briefly outline how\nthis is achieved since in Section 3 we will re-use essentially the same MATLAB\nfunctions to handle arbitrary numbers of directional derivatives. Although the im-\nplementation of this section is extremely simple, it is of immediate practical use for\ndifferentiating functions of a single variable as seen in Section 2.2 or conceivably\nin the matrix-free iterative solution of large-scale nonlinear systems [Nocedal and\nWright 1999, p285].\n2.1 Implementation of the Forward Mode\nThe forward mode is implemented via a MATLAB class fmad. A MATLAB class\nconsists of a set of functions that create and manipulate objects of that class. Here\nthe manipulations that concern us are extending the arithmetic operations of MAT-\nLAB to those that calculate both an object\u2019s value and an associated directional\nderivative.\nUse of the fmad class to calculate directional derivatives of a user\u2019s MATLAB\nfunction or statements is straightforward. The user first initialises value and deriv\ncomponents of objects of fmad class corresponding to those variables we need deriv-\natives with respect to using the fmad constructor function of Section 2.1.2. Then the\nuser\u2019s MATLAB function or statements are executed to propagate both values and\ndirectional derivatives via overloaded arithmetic operations and intrinsic functions\nas described in Sections 2.1.3 and 2.1.4. Values and derivatives associated with\nfmad objects are obtained using the extraction functions described in Section 2.1.5.\n2.1.1 fmad Objects. fmad objects have two components, value and deriv. The\nvalue component stores an object\u2019s value as a class double or sparse array. The\nACM Transactions on Mathematical Software, Vol. , No. , Accepted Aug. 2005.\nEfficient Forward Mode AD in MATLAB \u00b7 5\nfunction xad=fmad(x,dx)\n% Set value component\nswitch class(x)\ncase {\u2019double\u2019,\u2019sparse\u2019}\nxad.value=x;\ncase \u2019fmad\u2019\nxad.value=x.value;\notherwise\nerror(\u2019FMAD: first argument must be class double, sparse or fmad\u2019)\nend\nsx=size(xad.value);\n% Set deriv component\nif nargin==2\nswitch class(dx)\ncase \u2019derivvec\u2019\nxad.deriv=reshape(dx,size(xad.value));\ncase {\u2019double\u2019,\u2019sparse\u2019}\nsd=size(dx);\nif prod(sx)==prod(sd)\nxad.deriv=reshape(dx,sx);\nelse\nxad.deriv=derivvec(dx,size(xad.value));\nend\notherwise\nerror([\u2019FMAD\/FMAD: argument dx of illegal class\u2019,class(dx)])\nend\nxad=class(xad,\u2019fmad\u2019);\nelse\nerror(\u2019FMAD: must supply 2 arguments\u2019)\nend\nFig. 1. Constructor function for the fmad class.\nderiv component stores directional derivatives associated with the object. This\ncomponent may be of class double, sparse or, in the case of multiple directional\nderivatives, class derivvec. Throughout this section we restrict our description to\na single directional derivative, and thus the value and deriv components are of the\nsame size1 and of class double or sparse. Variables of fmad class are created via\nthe fmad constructor function which necessarily resides in the @fmad sub-directory.\n2.1.2 The fmad Constructor Function. In Figure 1, we show the fmad construc-\ntor function with, as will be our practice in this paper, some comments removed for\nbrevity. This function takes two inputs, x and dx, and returns an object xad of fmad\nclass. Generally a user will supply an array x of values of class double or sparse,\nand an associated directional derivative dx also of class double or sparse. In such\ncases x is assigned to the value component xad.value of the function output xad.\nAssuming the second argument to fmad is also of class double or sparse then, if\nit has the same number of elements as the value, it is reshape\u2019d to the same size\nas the value before being assigned to the deriv component of the output xad.\n1Here we use the term size in the MATLAB sense that the two have the same number of dimensions\nand number of elements in each dimension\nACM Transactions on Mathematical Software, Vol. , No. , Accepted Aug. 2005.\n6 \u00b7 Shaun A. Forth August 23, 2006\nFig. 2. The times function of the fmad class for\nelement-wise multiplication.\nfunction z=times(x,y)\nif isa(x,\u2019fmad\u2019)&isa(y,\u2019fmad\u2019)\nz.value=x.value.*y.value;\nz.deriv=x.deriv.*y.value...\n+x.value.*y.deriv;\nelseif isa(x,\u2019fmad\u2019)\nz.value=x.value.*y;\nz.deriv=x.deriv.*y;\nelse\nz.value=x.*y.value;\nz.deriv=x.*y.deriv;\nend\nz=class(z,\u2019fmad\u2019);\nFig. 3. The sin function of the fmad class\nfunction y=sin(x)\nxval=x.value;\ny.value=sin(xval);\ny.deriv=cos(xval).*x.deriv;\ny=class(y,\u2019fmad\u2019);\nWe also see that if the first input is already of fmad class then the function\nmay be used to change just the derivative deriv component. Should the second\nargument dx correspond to multiple directional derivatives then this is handled via\nthe derivvec class, discussion of which is postponed until Section 3.\n2.1.3 Propagating Derivatives Via Operator and Function Overloading. Once\nfmad objects have been initialised then values and derivatives must be propagated\nby invoking overloaded versions of all intrinsic arithmetic operations and functions\nthat comprise a user\u2019s MATLAB code.\nIn Figure 2, we show the coding of the times function of the fmad class invoked\nto calculate the element-wise multiplication of two arrays z=x.*y when one or more\nof x and y are of fmad class. As the function\u2019s coding indicates, first the MATLAB\nintrinsic function isa is used to determine which of x and y are of fmad class. If\nboth are of fmad class, then the value component of each is used to determine the\nappropriate value component for the output z.value=x.value.*y.value. Then\nthe output\u2019s directional derivative component is determined by the product rule\nz.deriv=x.deriv.*y.value+x.value.*y.deriv using both the values (x.value,\ny.value) and derivatives (x.deriv, y.deriv) of the inputs x and y. If just one of\nx or y is of fmad class then the appropriate branches are correspondingly simpler.\nFinally, the output z is cast to be of fmad class using the class intrinsic function.\nOn grounds of efficiency, and following Verma [1998a], we have adopted this casting\napproach to ensure z is of fmad class, rather than use the class constructor fmad\nfunction as advocated by the MATLAB documentation [The MathWorks Inc. 2003,\nSection 21].\nIn a similar manner, and using undergraduate level calculus, we may overload\nother arithmetic operations: addition (+), subtraction (-), matrix multiplication\n(*), element-wise division (.\/). We may also overload intrinsic functions, such as\nsin, as shown in Figure 3.\nACM Transactions on Mathematical Software, Vol. , No. , Accepted Aug. 2005.\nEfficient Forward Mode AD in MATLAB \u00b7 7\nfunction x=subsref(y,s)\nx.value=subsref(y.value,s);\nx.deriv=subsref(y.deriv,s);\nx=class(x,\u2019fmad\u2019);\nFig. 4. The subsref function of the fmad class\n2.1.4 Dealing with Array Indexing. A major difference between operator over-\nloading in MATLAB and that in traditional programming languages such as Fortran\nis in the handling of arrays. In Fortran arrays are regarded as an ordered collec-\ntion of their scalar components. Consequently the referencing of components via\nsubscripts is built into the language and, even for derived types (c.f., objects in\nMATLAB), need not be coded. In MATLAB all objects are arrays, and program-\nmers must provide functions to deal with array subscripting.\nIn Figure 4, we show the subsref function invoked for array subscript ref-\nerencing of fmad objects. For example, if y is of fmad class then x=y(1:5,:)\ninvokes the function call subsref(y,s), where s is a MATLAB structure with\ncomponents s.type=\u2019()\u2019, and s.subs = {1:5,\u2019:\u2019}. The s.type indicates the\ntype of subscripting, with the string () indicating conventional array indexing\nas opposed to cell array subscripting or structure component referencing. The\ns.subs = {1:5,\u2019:\u2019} component gives the actual indexing to be applied to y. As\nis seen from Figure 4, the fmad version of subsref simply uses two calls to the\nintrinsic subsref to perform equivalent subscripting on the value and deriv com-\nponents of an fmad object.\nSubscript assignments of, for example a(1:5,:)=b, assign an array (here b)\nto a subscript defined sub-region of another array (here a(1:5,:)). For single-\ndirectional derivatives an fmad class subsasgn function may be written in a similar\nmanner to that for subsref.\nUsually when designing a new MATLAB class we would provide a function\ndouble to convert an object of fmad class to intrinsic class double. We have pro-\nvided such a function, but it always produces an error message and halts execution\nof a user\u2019s MATLAB function or script. We have taken this step since if a variable,\na say, is already of class double and we assign an fmad variable b to a component\nof it, for example a(1)=b, then MATLAB will use the fmad class\u2019s double function\nto convert b to class double before the assignment. This will prevent the correct\npropagation of derivative information and, unknown to the user, the derivatives\nproduced will be incorrect. By trapping this situation as a run-time error the user\nmay take the appropriate action (ensuring a is of class fmad) so that derivative\npropagation proceeds correctly.\nTo minimise the number of instances in a user\u2019s code for which we would assign\nan fmad object to a component of a non-fmad array we have adopted a technique of\nVerma [1998b]. In MATLAB arrays are frequently created using the ones or zeros\nintrinsics, for example to initialise an array a to be of the same size of an object b\nbut consist of all zero entries the statement a=zeros(size(b)) could be used. In\nMad we ensure that if b is an fmad object then: size(b) returns an fmad object;\nzeros(size(b)) returns an fmad object; consequently a is an fmad object, and we\nmay subscript assign other fmads to it, e.g., a(1)=b(1). The difference between\nVerma\u2019s implementation of this technique and ours is that whereas ADMAT uses\nan empty matrix for the derivatives of the object returned by the size function,\nACM Transactions on Mathematical Software, Vol. , No. , Accepted Aug. 2005.\n8 \u00b7 Shaun A. Forth August 23, 2006\nfmad stores the derivatives as zeros; this trivial memory overhead in our approach\nis more than compensated for by the reduction in complexity obtained by never\nhaving to test if a derivative component is empty.\n2.1.5 Extracting Values and Derivatives. Functions getvalue and getderivs\nare provided that extract the value and deriv components of an fmad variable.\n2.2 Use of the fmad Class\nTrivially the fmad class may be used to obtain the derivative of a function of a\nsingle variable. For example, consider the function y = x2 + x. If we require the\nderivative at x = 2 then we may simply type at the MATLAB prompt,\nx=fmad(2,1);\ny=x^2+x;\ndy=getderivs(y)\nto get the output\ndy =\n5\nIn Section 3 we consider extending the fmad class to calculate multiple directional\nderivatives with the aid of the derivvec class.\n3. DERIVATIVE VECTORS\nIn Section 2, we have seen how forward mode AD calculates objects\u2019 values and their\nderivatives as the source code is executed. We see that the derivatives are calculated\nas linear combinations of derivatives previously calculated. In order for AD to\ncalculate the derivative of one or more outputs with respect to one scalar input\nrequires a single derivative value to be calculated for each scalar value calculated in\nthe code. Usually, calculation of nderivs directional derivatives requires either the\npropagation of nderivs directional derivatives (or nderivs runs of the AD code\nfor a single directional derivative). Exceptions to this are when the calculation\npossesses some intrinsic sparsity which may be exploited [Griewank 2000, Chapters\n6,7] or [Coleman and Verma 1998b], in which case a number less than nderivs\nmight be used.\nIn order to calculate multiple directional derivatives simultaneously and effi-\nciently, we aimed to optimise the operations associated with linear combinations\nof derivative vectors. Additionally we wished to exploit MATLAB\u2019s sparse matrix\noperations [Gilbert et al. 1992] to reduce memory requirements and improve per-\nformance for large scale calculations that exhibit sparsity in their Jacobians. Many\nFortran AD tools, such as ADIFOR [Bischof et al. 1996] and TAF [FastOpt 2003],\nutilise Fortran\u2019s efficiency for array operations to calculate multiple derivatives.\nADIFOR may also use the SparseLinC library [Bischof et al. 1996] to propagate\nsparse derivatives. Our aim was to emulate these capabilities in MATLAB. To do\nthis the derivvec class was designed and implemented.\n3.1 Design of the derivvec Class\nThe derivvec class was designed after studying features of the intrinsic MATLAB\ndouble and sparse classes. Key features of these intrinsic classes are:\nACM Transactions on Mathematical Software, Vol. , No. , Accepted Aug. 2005.\nEfficient Forward Mode AD in MATLAB \u00b7 9\n\u2014In MATLAB an array is a fundamental object. Whole array operations are\noptimised, and access to individual elements is relatively slow.\n\u2014The lowest rank arrays in MATLAB are rank 2 (i.e. matrices), and array indexing\nis column major, i.e., an increment of one in the first index alone refers to the\nnext element as stored in memory (fast access times).\n\u2014MATLAB can handle arbitrary rank (\u2265 2) arrays of class double.\n\u2014MATLAB\u2019s sparse matrices are strictly matrices, not arrays, and hence of rank 2.\nConsider a D dimensional n1\u00d7 n2\u00d7 . . .\u00d7nD MATLAB array representing values,\nsay A(1:n1,1:n2,...,1:nD). A natural way to store the derivatives would be to\nappend an extra dimension to give an array DA(1:n1,1:n2,...,1:nD,nderivs).\nWe would choose to append (rather than prepend) the dimension since then the ith\ndirectional derivative DA (1:n1,1:n2,...,1:nD,i) is then readily available and\ncontiguous in memory. As we shall see in Sections 3.2\u20133.4, this property is crucial\nin allowing use of high-level, efficient MATLAB matrix operations. We shall refer to\nthis way of storing multiple directional derivatives as the external representation\nof the derivative vector, since we shall ensure that all functions external to the\nderivvec class may access the derivatives assuming this multi-dimensional array\nstructure.\nUnfortunately, with the simple approach sketched in the previous paragraph, we\ncannot seamlessly use MATLAB\u2019s sparse matrices to store derivatives for arbitrary\ndimension arrays. To circumvent this problem we chose to have an internal rep-\nresentation of the derivative vectors in terms of an unrolled matrix. We then\nexplicitly handle the interface of such objects and their interaction with arbitrary\nrank arrays. In order to do this we explicitly store the size and number of deriva-\ntives associated with a derivative object. Objects of the derivvec class therefore\nhave the following components:\nnderivs. number of derivatives,\nshape. row vector [n1,n2,...,nD] storing size of corresponding value array,\nderivs. reshaped matrix of derivatives =reshape(DA,[prod(shape) nderivs]).\nIn this way the derivatives derivs are stored as a two-dimensional full or sparse\nmatrix. The use of full or sparse storage is determined by the user when they provide\na full or sparse matrix as the second, derivative argument dx to the fmad constructor\nfunction of Figure 1. The derivative dx is passed to the derivvec constructor\nfunction to form the derivs component of the derivvec object. We now explain\nhow representative arithmetic operations may be performed on derivvec objects.\n3.2 Addition\nFigure 5 contains the coding of the plus function of the derivvec class invoked\nwhen two derivvec objects, a and b, are added together to form a third c. We\nsee that the first action of the plus function is a deep copy of the input argument\na to the output c, ensuring that c is of class derivvec. In MATLAB addition\nmust be between two arrays that either have an identical size, or else one of the\narrays must be a scalar. Consequently, the local variables ssa and ssb are set\nto be the product of the shape component of a and b, respectively. If they have\nACM Transactions on Mathematical Software, Vol. , No. , Accepted Aug. 2005.\n10 \u00b7 Shaun A. Forth August 23, 2006\nFig. 5. The plus\nfunction of the\nderivvec class.\nfunction c=plus(a,b)\nc=a; % deep copy of a\nssa=prod(a.shape);\nsb=b.shape;\nssb=prod(sb);\nif ssa==ssb % a,b No. elements equal so simply add\nc.derivs=a.derivs+b.derivs;\nelseif ssa==1 % a is scalar\nc.shape=sb; % adopt shape of b\n% replicate a.derivs to have ssb rows\nif issparse(a.derivs)\n% use sparsity to replicate a.derivs\n[i,j,val]=find(a.derivs); %row vectors i,j,val\nnentry=length(j);\nnd=a.nderivs;\ni=(1:ssb)\u2019; % create i for each row needed\ni=i(:,ones(1,nentry)); % replicate i for each entry\npad=ones(ssb,1); % need to replicate j,val ssb times\nj=j(pad,:);\nval=val(pad,:);\nc.derivs=sparse(i,j,val,ssb,nd)+b.derivs;\nelse\n% replicate a ssb times before adding\nc.derivs=a.derivs(ones(1,ssb),:)+b.derivs;\nend\nelseif ssb==1\n% omitted for brevity\n.\n.\n.\nend\nthe same value a.derivs and b.derivs may be safely added together. If one\nof ssa or ssb take the value one the corresponding fmad operation must be the\nlegitimate addition of a scalar to an array. Consider first ssa==1 corresponding to\na being the row vector of derivatives for a scalar fmad object. We must add this\nrow vector to each row of b.derivs. Such an addition could be performed by a\nloop but might compromise performance. Instead we replicate a.derivs ssb times\nforming a matrix of the same dimensions as b.derivs before adding. As indicated\nin the code of Figure 5, if a.derivs is a sparse matrix we extract the (i, j) indices\nand values of its entries, explicitly replicate them, and use them to form a sparse\nmatrix that is added to b.derivs. If a.derivs is full a simpler indexing operation\nis used to perform the replication. A similar sequence of operations is performed\nif b corresponds to a scalar (omitted from Figure 5). There are no loops of any\nkind in the plus function of Figure 5, and the presence of any derivatives stored\nas sparse matrices is fully exploited. These properties are crucial to consistently\nachieving good performance.\nThe use of array operations rather than loops might be regarded as unnecessary\nsince the release of MATLAB version 6.5 for which the just-in-time (JIT) accelerator\ncapabilities often enable fast performance of operations on arrays performed in\nloops. This capability was not available during much of code development and,\nACM Transactions on Mathematical Software, Vol. , No. , Accepted Aug. 2005.\nEfficient Forward Mode AD in MATLAB \u00b7 11\nfunction c=times(a,b)\nif isa(b,\u2019derivvec\u2019);\nc=b; % pick out b as derivatives\nmults=a;% pick out a as multipliers\nelse\nc=a; % pick out a as derivatives\nmults=b; % pick out b as multipliers\nend\nssd=prod(c.shape);\nsm=size(mults);\nssm=prod(sm);\nmults=mults(:);\n% make everything conformable\nif ssd==ssm\nderivs=c.derivs;\n% check for sparsity\nif issparse(derivs)\n% only need multipliers corresponding to\n% entries of c.derivs\n[i,j]=find(derivs);\nnd=c.nderivs;\nc.derivs=sparse(i,j,mults(i),ssd,nd).*derivs;\nelse\nc.derivs=mults(:,ones(1,c.nderivs)).*derivs;\nend\nelseif ssd==1\nc.shape=sm;\nc.derivs=mults*c.derivs;\nelseif ssm==1\nc.derivs=mults.*c.derivs;\nend\nFig. 6. The times function\nof the derivvec class\nsince many of our users employ earlier versions of MATLAB, we do not wish to\nrely on it. Note that ADMAT\u2019s use of cell arrays prevents JIT acceleration under\nMATLAB version 6.5.\n3.3 Element-Wise Multiplication\nAs a second example of the use of high-level MATLAB matrix operations within\nthe functions of the derivvec class consider the element-wise multiplication of a\nmatrix with a derivvec class object as coded in the times function of Figure 6.\nThis function is invoked from fmad functions with just one of the inputs a or b of\nderivvec class. First the function determines which argument is of derivvec class,\nmakes a deep copy of it for the function result c, unrolls the non-derivvec class\ninput, and stores it as the local column vector mults. Then local variables ssd\nand ssm are used to store the number of elements corresponding to the derivvec\ninput and the number of elements in the multiplier mults, respectively. If ssd is\nequal to ssm the multiplier is conformable with a single directional derivative and\nso must be replicated, taking account of sparsity, to be of nderivs columns before\nan element-wise multiplication calculates the c.derivs component. If ssd==1 the\nderivvec input is recognised as corresponding to a scalar, and the shape of the\noutput c is given by that of the multiplier. Now c.derivs is given as a matrix with\nith row given by the single row of the derivvec input multiplied by the ith element\nACM Transactions on Mathematical Software, Vol. , No. , Accepted Aug. 2005.\n12 \u00b7 Shaun A. Forth August 23, 2006\nfunction b=subsref(a,s)\nb=a; % deep copy of a\nsa=a.shape;\nssa=prod(sa);% get total number of elements for each directional derivative\nind=reshape((1:ssa),sa);% form an indexing array\nnewind=subsref(ind,s);% grab required indexes using subscripting\ns=size(newind); % use size of newind to get size of result\nb.shape=s;\nnewind=newind(:); % reshape newind to column vector\nb.derivs=a.derivs(newind,:); % use indices to grab derivatives\nFig. 7. The subsref function of the derivvec class\nof the unrolled mults. Such a matrix is easily obtained by the matrix-multiplication\nof mults and c.derivs. Finally, if the multiplier is scalar ssm==1 we multiply all\nrows of the input derivvec argument, currently stored as c.derivs, by the scalar.\n3.4 Subscript Referencing\nTo facilitate subscript referencing for multiple directional derivatives in the fmad\nclass we must supply a derivvec class subscript referencing function so that the\nfmad subsref function of Figure 4 operates correctly when y.deriv is of derivvec\nclass. At first sight, this is is somewhat problematic to code efficiently given the\ninternal, unrolled storage chosen for the derivatives. One approach would be to\nreshape the internal representation of the derivatives to match the external repre-\nsentation, append a \u2018:\u2019 to the array of indices (c.f. Section 2.1.4), and then perform\na subscript reference. This is undesirable since it precludes the use of sparse matrix\nstorage for the derivatives. Instead, as shown in Figure 7, we form an indexing array\ncorresponding to the elements of the external representation of DA and ignoring the\nderivatives. This array is then acted upon by the intrinsic subscript referencing\nfunction to return the required rows of the internal representation of the derivative\nvector, which may then be trivially accessed. The coding of Figure 7 illustrates\nthe elegance of this approach with the use of high-level operations again ensuring\nefficiency.\nA similar strategy is used to code the transpose function, invoked as A.\u2019, of the\nderivvec class. First an indexing array is formed, it is then transposed, and the\nresulting array used to index the rows of the internal representation of the input\narray of derivatives to form the rows of the output array of derivatives.\n3.5 Extracting Derivatives\nFunction getderivs of Section 2.1.5 can be used to extract derivatives from fmad\nobjects with derivatives stored internally as derivvec objects. The derivatives are\nreturned in the external representation of Section 3.1 with dimension one degree\nhigher than the value. Derivatives stored internally as (2-dimensional) sparse\nmatrices are first converted to full arrays since sparse arrays can only be 2-\ndimensional. Since this would prevent users accessing the sparse data structure\ncrucial for efficiency in large problems a function getinternalderivs is provided\nto return the matrix internal representation of an fmad object\u2019s derivatives.\nHaving described both Mad\u2019s fmad and derivvec classes we present a simple\nACM Transactions on Mathematical Software, Vol. , No. , Accepted Aug. 2005.\nEfficient Forward Mode AD in MATLAB \u00b7 13\nfunction J=FmadFullJac(t,y0,N)\n% uses full storage in fmad\ny=fmad(y0,eye(2*N));% initialise y\ndydt=f(t,y,N);% calculate F\nJ=getinternalderivs(dydt);% grab Jacobian\nFig. 8. Calculating the Jacobian of the\nBrusselator problem using full storage of\nderivatives.\nexample in Section 4 to demonstrate their use.\n4. USING THE FMAD CLASS\nIn this section we describe how to use the fmad class to calculate Jacobians taking\nas an example the Brusselator test problem. We defer detailed discussion of the\nrelative efficiencies of these techniques until Section 5.\nThe Brusselator ODE problem [Hairer and Wanner 1991] is supplied as a test case\n(file brussode.m) for the ode15s stiff differential equation solver in MATLAB [The\nMathWorks Inc. 2003, Section 14]. It corresponds to a method-of-lines discretisa-\ntion of a 2-species reaction-diffusion equation on a 1-D mesh of N points, giving\nan ODE in standard form y\u2032 = f(t,y) with n=length(y)= 2N . To facilitate the\nembedded quasi-Newton solver of the BDF-like ode15s, the sparse n\u00d7 n Jacobian\nJf = \u2202f\/\u2202y must be evaluated. The interface to the function is,\nfunction dydt = f(t,y,N)\nwhere t is the scalar time, y is the solution vector and N is the number of mesh\npoints.\nWe now describe how to calculate the Jacobian of the Brusselator function us-\ning the fmad class with derivatives stored and manipulated using full, sparse or\ncompressed matrix storage.\n4.1 Full Storage\nThe use of full (dense) storage is illustrated by the function FmadFullJac of Fig-\nure 8. For y0 supplied to FmadFullJac the vector y is initialised to be of fmad\nclass with value given by y0 and derivatives given by the 2N \u00d7 2N identity matrix\neye(2*N). Consequently element y(j) of y has directional derivative given by the\njth column of eye(2*N), that is the vector with entry one in position j and zeros\nelsewhere. Consequently the jth directional derivative corresponds to derivatives\nwith respect to y(j). The function f is then called with the fmad object y as an\nargument. The overloaded operators and functions of Sections 2 and 3 then com-\npute the function\u2019s value and derivatives. The derivatives are then extracted from\nthe function\u2019s return value dydt using the fmad function getinternalderivs and\nare returned in the full storage matrix J such that the ith row of J comprises the\nderivatives of f(i), and since the jth derivatives are with respect to y(j) we see\nthat\nJ(i,j) =\n\u2202fi\n\u2202yj\n.\n4.2 Sparse Storage\nThe use of sparse storage is illustrated by the function FmadSparseJac of Figure 9.\nThe only difference compared to the code of Figure 8 is that the derivatives are\nACM Transactions on Mathematical Software, Vol. , No. , Accepted Aug. 2005.\n14 \u00b7 Shaun A. Forth August 23, 2006\nFig. 9. Calculating the Jacobian of\nthe Brusselator problem using sparse\nstorage of derivatives.\nfunction J=FmadSparseJac(t,y0,N)\n% uses sparse storage in fmad\ny=fmad(y0,speye(2*N));% initialise y\ndydt=f(t,y,N);% calculate F\nJ=getinternalderivs(dydt);% grab Jacobian\nFig. 10. Determin-\ning sparsity pattern,\ncoloring, and seed\nmatrix for com-\npressed storage of\nderivatives.\nsparsity_pattern=jpattern(N); % sparsity pattern\ncolor_groups=MADcolor(sparsity_pattern); % coloring\nseed=MADgetseed(sparsity_pattern,color_groups); % seed\nFig. 11. Calculating\nthe Jacobian of the\nBrusselator problem\nusing compressed\nstorage of derivatives.\nfunction J=Fmadcmpjac(t,y0,N,... sparsity_pattern,color_groups,seed)\n% uses compressed fmad\ny=fmad(y0,seed);% initialise y\ndydt=f(t,y,N);% calculate F\nJcomp=getinternalderivs(dydt);% grab compressed Jacobian\nJ=MADgetcompressedJac(Jcomp,...\nsparsity_pattern,color_groups);\ninitialised using the MATLAB function speye(2N), which returns a sparse identity\nmatrix. In the ensuing overloaded fmad calculations all derivatives are stored and\nmanipulated as sparse matrices, and the extracted Jacobian J will be returned in\nsparse format.\n4.3 Compressed Storage\nHere a two stage process is used.\n(1) The first stage, shown in Figure 10, involves determining information on the\nJacobian\u2019s sparsity pattern and how to perform the compressed Jacobian cal-\nculation. Provided the Jacobian\u2019s sparsity pattern is fixed, or is safely over-\nestimated, then this stage need only be performed once, and the information\ndetermined reused for multiple Jacobian calculations. For given problem size\nN the function jpattern, supplied by MATLAB within the file brussode.m,\nreturns the Jacobian\u2019s sparsity pattern. The sparsity pattern is a matrix of\nthe same size as the Jacobian but with unit entry in position (i, j) if J(i,j) is\nnonzero and zero otherwise. Given the sparsity pattern, the function MADcolor\ndetermines a group, or color, for each element of the function\u2019s vector of inputs\ny such that those in the same group do not affect the same rows of the Jacobian.\nLike MATLAB\u2019s numjac function [Shampine and Reichelt 1997], MADcolor uses\nthe most effective of first-fit and first-fit after reverse column minimum degree\norderings. The function MADgetseed uses the coloring to construct a seed ma-\ntrix with seed(i,k) taking the value one if y(i) is in color group k. Note that\nsparsity pattern is an argument to MADgetseed solely as a device to supply\nthe Jacobian size.\n(2) The second stage of the calculation is shown in Figure 11. First the seed matrix\nis used to initialise the derivatives of y. Then, after propagating y through\nthe calculation of the function f, the compressed Jacobian Jcomp is extracted,\nand the MAD function MADgetcompressedJac is used, in conjunction with the\nACM Transactions on Mathematical Software, Vol. , No. , Accepted Aug. 2005.\nEfficient Forward Mode AD in MATLAB \u00b7 15\nsparsity pattern and coloring, to return the Jacobian in sparse format.\nRecently we have automated the use of the fmad class within ODE and optimi-\nsation solvers using high-level interface functions [Forth and Ketzscher 2004] and\nby directly coding fmad function calls into the solvers [Shampine et al. 2005; Forth\nand Edvall 2004].\nIn the following section we compare the performance of the fmad class with\nthe existing ADMAT automatic differentiation package [Verma 1998b] and finite-\ndifferencing.\n5. TEST CASES\nIn this section we present several test cases all performed using MATLAB version\n6.5 on a PC running Windows XP Professional on a 3.0 GHz Pentium IV processor\nwith 512 MB of RAM. Testing was also performed on a SUN Blade 1000 workstation\nrunning UNIX and a Pentium IV PC running Linux. Results on these two platforms\nare qualitatively similar to those presented here and, in particular, the rankings of\ndifferent differentiation techniques are the same. Application of Mad to several\nboundary value problems may be found in Shampine et al. [2005]. OurMad package\nis constantly being updated with new functionality; the version used here was that\nas of December 2004.\n5.1 Polynomial Data Fitting\nThis problem concerns the calculation of the coefficients of them-degree polynomial\np(x) = p1+ p2x+ p2x2+ . . .+ pmxm\u22121 that best fits the points (xi, di), i = 1, . . . , n\nin the least squares sense. This leads to the over-determined linear system Vp = d,\nwhere V is the well-known Vandermonde matrix,\nV =\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8f0\n1 x1 x21 . . . x\nm\u22121\n1\n1 x2 x22 . . . x\nm\u22121\n2\n...\n...\n...\n1 xn x2n . . . x\nm\u22121\nn\n\uf8f9\uf8fa\uf8fa\uf8fa\uf8fb .\nThe problem of calculating the derivatives of the m coefficients p with respect to\nthe n abscissas x has been considered previously [Bischof et al. 2002] via the hybrid\nsource-transformation\/overloaded MATLAB AD tool ADiMat. These authors pro-\nvided a short MATLAB function to calculate p and showed that ADiMat-generated\nderivative code executed with a similar efficiency to one-sided finite-differencing.\nThough competitive for N < 100, forward mode ADMAT was uncompetitive for\nlarger n.\nHere we effectively repeat the calculation of [Bischof et al. 2002] but using the\nfmad class and also MATLAB\u2019s numjac finite-difference Jacobian function. The\nnumjac function approximates the Jacobian via one-sided finite-differencing with\nautomatic adjustment of the step-size to ensure accuracy of the larger elements in\neach column of Jf [Shampine and Reichelt 1997]. In Table I we show the ratio of\nJacobian to function CPU times for this problem; function timings may be found\nin Table VII of Appendix A. For n \u2265 40 we see that fmad(full) with full storage for\nderivatives has comparable performance to numjac. Using sparse derivative storage\nwith fmad(sparse) improves performance further, outperforming numjac for n \u2265 40\nACM Transactions on Mathematical Software, Vol. , No. , Accepted Aug. 2005.\n16 \u00b7 Shaun A. Forth August 23, 2006\nTable I. Ratio CPU(Jf)\/CPU(f) of Jacobian to function CPU times for the Polyno-\nmial Data Fitting problem with m = 4. Jacobian and function calculations were timed\nover loops of 7680\/n and 25600 evaluations respectively, and this process was repeated\n10 times to give an average CPU time. Further information is given in Table VII of\nAppendix A.\nCPU(Jf)\/CPU(f) for problem size n\nMethod 10 20 40 80 160 320 640 1280\nnumjac 19.2 31.6 56.9 106.6 202.4 393.0 823.2 1528.8\nfmad(full) 42.9 40.8 46.9 75.0 167.0 403.1 802.0 1704.2\nfmad(sparse) 44.1 39.0 34.3 32.4 33.6 71.1 127.2 257.2\nADMAT(full) 44.1 60.4 97.8 175.3 888.9 7220.0 30399.3 128588.1\nADMAT(sparse) 47.6 63.0 94.3 150.9 265.9 623.4 922.6 1806.9\nTable II. Ratio CPU(Jf)\/CPU(f) of Jacobian to function CPU times for the\nBrusselator problem. Jacobian and function calculations were timed over loops\nof 1000 and 5000 evaluations respectively, and this process was repeated 10 times\nto give an average CPU time. Further information is given in Table VIII of\nAppendix A.\nCPU(Jf)\/CPU(f) for problem size n\nMethod 20 40 80 160 320 640 1280 2560\nnumjac(comp,vect) 5.4 6.1 6.7 8.0 8.2 8.4 9.7 9.7\nfmad(sparse) 71.8 72.7 67.1 60.2 51.2 41.7 43.1 35.8\nfmad(comp) 64.8 64.2 54.8 46.8 35.7 25.1 19.8 15.2\nADMAT(comp) 101.4 98.5 84.0 70.1 51.4 35.1 25.9 18.1\nand for n \u2265 160 being five times more efficient. Although the Jacobian \u2202p\/\u2202x is\nfull, sufficient intermediate values (in particular the Vandermonde matrix V itself)\nhave sparse derivatives that the use of sparsity is very beneficial. We see that use\nof ADMAT\u2019s forward mode with full storage is reasonably efficient for n = 10, but\nits apparent quadratic growth of CPU time with n [Bischof et al. 2002] makes it\nuncompetitive for all other n. Use of sparse derivatives in ADMAT is beneficial,\nbut at best has comparable efficiency to fmad\u2019s full storage.\nAs observed by Bischof et al. [2002], although m = 4 \u001c n for this prob-\nlem, ADMAT\u2019s reverse mode is less efficient than forward, e.g. for n = 40,\nCPU(Jf)\/CPU(f) = 14241.\n5.2 The Brusselator\nFor this test case, described in Section 4, the sparse n \u00d7 n Jacobian Jf = \u2202f\/\u2202y\nmust be evaluated. By default, MATLAB\u2019s stiff ODE solver ode15s uses the\nMATLAB-supplied numjac function. If the Jacobian\u2019s sparsity pattern is supplied,\nthen numjac uses Jacobian row compression techniques [Shampine and Reichelt\n1997], [Griewank 2000, Chap. 7]. For this example, Jacobian compression enables\nconstruction of the Jacobian from just four Jacobian-vector products which, by de-\nfault, are approximated by one-sided finite differencing. If the user indicates that\nthe function f(t,y) is vectorizable, which is the case here, then the four extra func-\ntion evaluations may be performed in a single call of the function f(t,Y), where Y\nis a matrix whose columns are the perturbed y\u2019s.\nIn Table II, we present the ratio of average Jacobian to average function CPU\ntimes CPU(Jf)\/CPU(f) for various sparsity-exploiting Jacobian calculation tech-\nniques. The technique numjac(comp,vect), uses sparse finite differencing via Jaco-\nACM Transactions on Mathematical Software, Vol. , No. , Accepted Aug. 2005.\nEfficient Forward Mode AD in MATLAB \u00b7 17\nTable III. ODE solution CPU time for the Brusselator problem. Each integra-\ntion was performed in a loop 10 times, and this process was repeated 10 times to\nget an average CPU time.\nCPU(ODE solve) for problem size n (s)\nMethod 20 40 80 160 320 640 1280 2560\nnumjac(comp,vect) 0.07 0.08 0.11 0.15 0.24 0.42 0.93 2.49\nfmad(sparse) 0.09 0.10 0.13 0.17 0.24 0.45 0.80 1.67\nfmad(comp) 0.10 0.11 0.13 0.17 0.25 0.46 0.80 1.70\nfmad(comp,recolor) 0.10 0.11 0.14 0.18 0.27 0.50 0.98 2.45\nADMAT(comp) 0.11 0.12 0.14 0.18 0.26 0.44 0.81 1.75\nADMAT(comp,recolor) 0.11 0.12 0.15 0.19 0.28 0.49 0.99 2.43\nbian compression and vectorisation. Rows labelled fmad(sparse) and fmad(comp)\ncorrespond to use of the fmad class with use of dynamic sparsity and Jacobian\ncompression, respectively. The row labelled ADMAT(comp) corresponds to use of\nforward mode ADMAT with compression. Use of dynamic sparsity with ADMAT\nfailed for this test case due to the use of two-dimensional array indexing in the\nvectorized coding of f .\nFrom Table II, we see that compressed finite differencing is the most efficient\nJacobian calculation technique, though for large values of n both compressed AD\ntechniques (fmad(comp) and ADMAT(comp)) are only about two to three times\nslower.\nIn Table III, we show the effect of Jacobian technique on CPU times taken to\nsolve the Brusselator ODE problem, as defined in [The MathWorks Inc. 2003, Sec-\ntion 14]. For all integrations, only two Jacobian evaluations are performed. For\nlarge n fmad(comp) and ADMAT(comp) outperform numjac(comp,vect), despite\nthe fact that Table II indicates that they should not. This is because at the start\nof integration ode15s always re-computes the coloring required to use compression\nwith numjac. If we force such a recomputation when using fmad(comp) and AD-\nMAT(comp) (rows fmad(comp,recolor) and ADMAT(comp,recolor) of Table III),\nthen we see the pre-eminence of numjac(comp,vect) over the AD compressed tech-\nniques restored. Note also that fmad(sparse) (which requires no coloring) is ap-\nproximately as fast or outperforms numjac(comp,vect) for this problem.\n5.3 Coleman & Verma\u2019s Arrowhead Function\nThis function [Griewank 2000, Section 7.4], with x\u2192 f(x) and x, f(x) \u2208 IRn,\nf1 = 2x21 +\n\u2211n\ni=1 x\n2\ni\nfi = x21 + x\n2\ni , i = 2, . . . , n\n}\n,\nhas sparse Jacobian with one full row, one full column and a full diagonal, e.g., for\nn = 7 the Jacobian has sparsity pattern,\nJf(x) =\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022\n\u2022 \u2022\n\u2022 \u2022\n\u2022 \u2022\n\u2022 \u2022\n\u2022 \u2022\n\u2022 \u2022\n\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n,\nACM Transactions on Mathematical Software, Vol. , No. , Accepted Aug. 2005.\n18 \u00b7 Shaun A. Forth August 23, 2006\nTable IV. Ratio CPU(Jf)\/CPU(f) of Jacobian to function CPU times for the\nArrowhead problem. Jacobian and function calculations were timed over loops of\n500 and 500, 000 evaluations respectively, and this process was repeated 10 times to\ngive an average CPU time. Further information is given in Table IX of Appendix A.\nCPU(Jf)\/CPU(f) for problem size n\nMethod 20 40 80 160 320 640 1280\nnumjac(vect) 20.8 35.0 124.6 507.1 3394.5 17898.0 113879.5\nfmad(sparse) 90.6 90.5 100.4 102.8 124.0 168.6 235.7\nADMAT(sparse) 192.4 326.1 619.9 1160.9 2427.7 5206.9 15443.1\nADMIT 285.2 274.5 280.7 264.5 272.3 288.2 313.0\nwhere the \u2022 symbols denote non-zero entries of the Jacobian. Such a sparsity\npattern prohibits row or column compression techniques. An effective strategy\nis bi-coloring [Coleman and Verma 1996], [Griewank 2000, Section 7.4]: finding\nand using combinations of Jacobian-vector products evaluated via forward mode\nand vector-Jacobian products via reverse mode to evaluate the Jacobian. For the\nArrowhead function two Jacobian-vector products can calculate the full column and\ndiagonal, and one vector-Jacobian product calculates the full row. The bi-coloring\ntechnique is available in MATLAB via the ADMIT Toolbox [Coleman and Verma\n2000].\nIn Table IV, we show the ratio of Jacobian to function CPU times for the Ar-\nrowhead function for various problem sizes n and several Jacobian evaluation tech-\nniques. We use Verma\u2019s original function coding for this problem with the exception\nof row numjac(vect), where we use a slightly modified version to allow for vectorized\nfinite differencing. Although most efficient for small n, numjac(vect)\u2019s inability to\nexploit sparsity makes it two orders of magnitude slower than fmad(sparse) and\nADMIT for the largest n. We see that fmad(sparse) clearly out-performs AD-\nMAT(sparse) and even the sophisticated ADMIT technique.\nThe results for this problem contradict the conventional wisdom [Griewank 2000,\np137] that an effective compression should outperform sparse forward mode due\nto the latter\u2019s overheads arising from manipulating sparse data structures [Gilbert\net al. 1992]. In Appendix B we show that fmad\u2019s sparse mode requires just 7n+ 2\nfloating point operations to propagate derivative information compared to the\n12n + 3 floating point operations required by ADMIT. So sparse forward mode\nmay use fewer flops than a bi-coloring approach (it always uses fewer flops than\ncompressed forward mode [Griewank and Reese 1991]). Also ADMIT itself has\nsubstantial overheads: the computation and necessary values of variables must be\ntaped [Griewank 2000, Chap. 3], the tape traversed in both forward and reverse\ndirections to propagate derivatives and adjoints, and the Jacobian must be assem-\nbled from the propagated derivatives and adjoints. For ADMIT there is also the\none-off effort of determining the sparsity pattern and a suitable coloring, although\nthis is not included in the timings of Table IV. We note that previous articles on\nbi-coloring [Coleman and Verma 1998b; 2000] have demonstrated that it is possible\nto get a good coloring for a given sparsity pattern but contain no timings of Ja-\ncobian calculation via ADMAT; the single timed example in Coleman and Verma\n[1998b] uses an ADOL-C [Griewank et al. 1996] tape for the Jacobian calculation.\nACM Transactions on Mathematical Software, Vol. , No. , Accepted Aug. 2005.\nEfficient Forward Mode AD in MATLAB \u00b7 19\nTable V. Ratio CPU(Jf)\/CPU(f) of Jacobian to function CPU times for Large-Scale Examples\nfrom MATLAB Optimization Toolbox. Jacobian and function calculations were timed over loops\nof 10 and 1000 evaluations respectively, and this process repeated 10 times to give average CPU\ntimes. Further information on these problems is given in Table X of Appendix A.\nCPU(Jf)\/CPU(f) for given technique\nProblem Hand-\ncoded\nsfd(nls) fmad-\n(sparse)\nfmad-\n(comp)\nADMAT-\n(sparse)\nADMAT-\n(comp)\nADMAT-\n(rev)\nnlsf1a 5.1 43.2 41.1 21.1 996.1 29.4 -\nbrownf 3.9 1123.2 12.2 - 281.7 - 84.5\nbrownfg 3.8 6.4 12.6 5.1 390.6 6.3 -\ntbroyf 4.7 898.5 18.9 - 397.5 - 121.9\ntbroyfg - 14.6 25.8 13.4 26891.9 19.5 -\n5.4 Large-Scale Examples from MATLAB Optimization Toolbox\nIn Table V, we give Jacobian to function CPU time ratios for test problems of\nthe MATLAB Optimization Toolbox [The Mathworks Inc. 2003]; we include two\nHessian calculations regarding them as the Jacobian of hand-coded gradients. For\nproblem brownfg (obtaining the Hessian of the gradient of the Brown problem), the\nMATLAB supplied hand-coded Hessian was incorrect. With the aid of MATLAB\u2019s\nSymbolic Toolbox, it took about half a day to correctly derive Hessian code (denoted\nHand-coded in Table V), while all the AD techniques were coded in just a few\nminutes. We made just two changes to the supplied MATLAB code to enable\nautomatic differentiation. For the Brown brownfg problem some two-dimensional\narray indexing x(i+1,1) was changed to one-dimensional x(i+1) to enable use of\nADMAT\u2019s sparse forward mode, and in tbroyfg initialisation of a vector z was\nchanged from,\nn=length(x); j=1:(n\/2); z=zeros(length(j),1);\nto\nn=length(x); j=1:(n\/2); z=zeros(n\/2,1);\nto force z to be of fmad class, c.f., Section 2.1.4.\nThe one technique used in Table V we have not yet met is sfd(nls), referring\nto MATLAB\u2019s Optimization Toolbox sparse finite-difference functions sfdnls for\nJacobians\/gradients (using function evaluations) and sfd for Hessians (using gra-\ndient evaluations). Both use compression if the Jacobian\/Hessian sparsity pattern\nis supplied but, unlike numjac, neither take advantage of vectorization.\nFrom Table V, we see that in all cases for which hand-coding is available it\nout-performs all other techniques. For the sparse Jacobian (nlsf1a) and Hessian\ncalculations (brownfg, tbroyfg) we see that fmad(comp),Mad\u2019s forward mode AD\nwith compression, out-performs both compressed finite-differencing (sfd(nls)) and\nADMAT\u2019s forward mode with compression (ADMAT(comp)). For the gradient cal-\nculations (brownf, tbroyf), then after hand-coding fmad(sparse) is most efficient\nand out-performs ADMAT\u2019s reverse mode. It is also 50 to 100 times faster than\nfinite-differencing since there is sparsity in the gradient calculation (the functions\nare partially value separable [Griewank 2000, p206]), but the function gradient itself\nis dense (preventing sfd(nls) using compression).\nTable VI shows the effect of varying the derivative calculation technique on the\nACM Transactions on Mathematical Software, Vol. , No. , Accepted Aug. 2005.\n20 \u00b7 Shaun A. Forth August 23, 2006\nTable VI. CPU time (CPU(calculated technique)) for optimization of the Large-Scale Examples\nfrom the MATLAB Optimization Toolbox. For each problem, its type, the MATLAB optimization\nfunction used and how derivatives are calculated is specified. Each optimization was timed over\na loop of 10 evaluations, and this process repeated 10 times giving an average CPU over 100\nevaluations.\nCPU(calculated technique)(s)\nProblem Hand-\ncoded\nsfd(nls) fmad-\n(sparse)\nfmad-\n(comp)\nnlsf1a Nonlinear solve (fsolve: calculated\nJacobian)\n0.46 0.79 0.58 0.52\nbrownf Minimisation (fminunc: calculated\ngradient, sfd(nls) Hessian)\n1.19 - 2.59 -\nbrownfg Minimisation (fminunc: hand-coded\ngradient, calculated Hessian)\n0.62 1.15 1.92 0.85\ntbroyf Constrained minimisation (fmincon:\ncalculated gradient, sfd(nls) Hessian)\n1.22 - 4.71 -\ntbroyfg Constrained minimisation (fmincon:\nhand-coded gradient, calculated\nHessian)\n- 1.30 1.35 1.07\nrun-times of various optimization problems. Hand-coding gives fastest overall opti-\nmization times. For the sparse Jacobian\/Hessian cases (nlsf1a, brownfg, tbroyfg)\nfmad\u2019s compressed forward mode (fmad(comp)) outperforms sparse-finite differenc-\ning (sfd(nls)). For the two gradient cases (brownf, tbroyf) fmad\u2019s sparse forward\nmode (fmad(sparse)) gives very acceptable performance, far better than would be\nexpected from finite-differencing (c.f., sfd(nls)in Table V).\n6. CONCLUSIONS\nSections 2 and 3 of this paper detail the implementation of forward mode AD for\nfirst derivatives in the Mad package. A major feature of our fmad class for first\nderivatives is that it uses object components of intrinsic classes double or sparse for\nsingle directional derivatives and our derivvec class for multiple directional deriv-\natives. Internal to the derivvec class multiple directional derivatives are stored\nas matrices (2-D arrays) using MATLAB\u2019s intrinsic full or sparse matrix classes.\nBy making careful use of MATLAB\u2019s high-level matrix operations we have heavily\noptimised functions of the derivvec class. As seen in Section 4, use of the fmad\nclass is straightforward.\nThe results of Section 5 demonstrate the effectiveness of our approach. For the\ndense Jacobian polynomial data fitting case of Section 5.1, although the finite-\ndifferencing of numjac is most efficient for small problem size n, fmad\u2019s sparse\nforward mode is five times faster than numjac for large n. Compressed vectorized\nfinite-differencing outperforms Mad in evaluating Jacobians for the Brusselator\nproblem of Section 5.2, although when solving the associated stiff ODE fmad\u2019s\nsparse mode outperforms numjac for large n since it does not need to calculate an\nexpensive coloring. For small n, numjac is the fastest technique for the arrowhead\nJacobian of Section 5.3, but fmad(sparse) is 450 times faster for n = 1280. For the\nthree sparse Jacobian\/Hessian problems from the MATLAB Optimization Toolbox\nof Section 5.4 fmad\u2019s compressed mode just outperforms the Toolbox\u2019s sparse-finite\ndifferencing functions sfd\/sfdnls. For the two gradient calculations fmad(sparse)\nis some 50 and 100 times faster than finite-differencing. When these techniques\nACM Transactions on Mathematical Software, Vol. , No. , Accepted Aug. 2005.\nEfficient Forward Mode AD in MATLAB \u00b7 21\nwere applied to the five optimization calculations of Table VI then in three cases\nan fmad technique was seen to outperform finite-differencing, and for the other two\nfinite-differencing was so expensive it was not even tried (c.f., Table V)!\nIn all the test problems considered one of either fmad(sparse) or fmad(comp)\noutperformed comparable, or even more sophisticated, techniques (reverse mode,\nbi-coloring) from the ADMAT\/ADMIT package. For large numbers of directional\nderivatives ADMAT\u2019s forward mode performs poorly compared to fmad because its\nderivative manipulation operations use loops over each directional derivative. This\nis not as efficient as the matrix operations of the derivvec class and gives fmad a\nclear superiority in efficiency. When only a small number of directional derivatives\nare required, for example when Jacobian compression is used, this superiority of\nfmad is reduced. Performance may have been further improved because: in fmad\nwe never test to see if an fmad object has an empty derivative component whereas\nADMAT does (see Section 2.1.4); also fmad\u2019s runtime switching between code for\none or multiple directional derivatives is performed by the MATLAB system based\non the class (double or derivvec) of the derivatives rather than ADMAT\u2019s use of\nbranching. Additionally ADMAT may only use MATLAB\u2019s sparse matrix class for\nstoring multiple directional derivatives of vector objects and even then ADMAT\u2019s\ninternal use of loops degrades performance. The derivvec class enables fmad to\nuse such sparse matrix storage for arbitrary dimension array objects with excellent\nefficiency. This is particularly apparent for partially separable functions such as\nbrownf and tbroyf of Table V, and even the Arrowhead problem of Table IV for\nwhich previous authors [Coleman and Verma 1998b; 2000] have suggested that more\nsophisticated algorithms such as bi-coloring are needed.\nThe capabilities of Mad are currently being extended in three areas.\nSource Transformation. For compiled languages source-transformation AD tools\ntend to produce differentiated code that out-performs its operator overloaded coun-\nterpart [Tadjouddine et al. 2001]. When differentiating MATLAB code contain-\ning array operations, as for the examples of Section 5, we have observed good\nperformance as the problem size increases. We are now developing a source-\ntransformation tool for MATLAB which, in preliminary tests, has demonstrated im-\nproved performance over the fmad class, particularly for smaller problems [Kharche\n2004]. Some work has already been done in this area [Vehreschild 2001; Bischof\net al. 2003; Bischof et al. 2002].\nReverse Mode. We are presently implementing the reverse mode of AD for first\nderivatives using a taping approach [Griewank 2000, Chapter 3]. Our implementa-\ntion reuses the derivvec class for storing and propagating multiple adjoints.\nAutomatic Sparsity Detection. Results from Sections 5.2 and 5.4 demonstrate\nthe effectiveness of Jacobian compression which requires a safe over-estimate of the\nJacobian sparsity pattern. We are working on this sparsity estimation problem and\naim to provide more robust and efficient capabilities than those of Verma [1998b].\nIn the longer term we hope to include elimination AD based approaches [Griewank\n2000, Sections 8.1-8.2], [Naumann 1999; 2004] intoMad, since for Jacobian calcula-\ntions it has theoretical efficiency advantages over conventional forward and reverse\nmodes and may be implemented in the source-transformation framework [Forth\net al. 2004].\nACM Transactions on Mathematical Software, Vol. , No. , Accepted Aug. 2005.\n22 \u00b7 Shaun A. Forth August 23, 2006\nTable VII. Function CPU times for the Polynomial Data Fitting Problem.\nProblem size n 10 20 40 80 160 320 640 1280\nFunction CPU (ms) 0.066 0.076 0.096 0.133 0.204 0.342 0.657 1.206\nTable VIII. Function CPU times for the Brusselator\nProblem size n 20 40 80 160 320 640 1280 2560\nFunction CPU (ms) 0.145 0.151 0.183 0.227 0.329 0.521 0.918 1.885\nTable IX. Function CPU times for the Arrowhead Problem\nProblem size n 20 40 80 160 320 640 1280\nFunction CPU (ms) 0.021 0.022 0.022 0.024 0.025 0.027 0.030\nFinally, Mad\u2019s performance and reliability is sufficient for it to be distributed\ncommercially (after a free evaluation period a small license fee is payable) [Forth\nand Edvall 2004] for both stand-alone use and in-conjunction with the TOMLAB\noptimisation package [Holmstro\u00a8m and Edvall 2004; Holmstro\u00a8m et al. 2004]. Several\nTOMLAB users, both academic and industrial, together with colleagues at Cran-\nfield University have used Mad to differentiate involved MATLAB functions for\napplications such as race car trajectory optimization [Bradshaw 2004] and nonlin-\near response surface fitting [Ringrose and Forth 2004; 2004]. With just one excep-\ntion, whenever fmad failed to compute correct derivatives and the user submitted a\nbug report we have either extended fmad to deal with the user\u2019s code or suggested\na simple change to the user\u2019s code (e.g., ensuring arrays are of fmad class before\nsubscript assignments as described in Section 2.1.4, or removing non-differentiable\nbranches such as if abs(x)<=1e-6; x=0; end) to enable differentiation. The ex-\nception was a case with one million independent variables and for which reverse\nmode would appear necessary.\nAPPENDIX\nA. FUNCTION CPU TIMES\nTables VII to IX give function CPU times for the test cases of Sections 5.1 to 5.3.\nAs well as function CPU times, Table X gives problem types, sizes and Jacobian\ninformation for the Optimization Toolbox problems of Section 5.4.\nB. COMPLEXITY ANALYSIS FOR THE ARROWHEAD EXAMPLE\nColeman and Verma\u2019s [1996] coding for the arrowhead example of Section 5.3 is\nshown in Figure 12. For the purposes of analysis we consider an evaluation pro-\ncedure [Griewank 2000, Chap. 2] of statements with scalar left hand-sides and\nright-hand sides that could be written as a single MATLAB statement. Such an\nevaluation procedure for the arrowhead problem is given in the first column of Ta-\nble XI. Following the notation of [Forth et al. 2004] we see we have n independent\nvariables vi, i = 1, . . . , n, m = n dependent varables vi, i = 2n + 4, . . . , 3n + 3 and\np = n + 3 intermediate variables vi, i = n + 1, . . . , 2n + 3. The second column\nof Table XI gives the local derivatives ci,j = \u2202vi\/\u2202vj for each statement. We see\nthat we have N|c|=1 = 2n + 2 trivial local derivatives with value |ci,j | = 1, and\nN|c|6=1 = 2n+ 1 nontrivial local derivatives with |ci,j | 6= 1, 0.\nForth et al. [2004] show that, provided local derivatives ci,j have already been\ndetermined, the cost in floating point operations of propagating q directional deriv-\nACM Transactions on Mathematical Software, Vol. , No. , Accepted Aug. 2005.\nEfficient Forward Mode AD in MATLAB \u00b7 23\nTable X. Problem type, size (m,n), maximum number of entries in Jacobian row\nn\u02c6, number of directional derivatives for compression p and function CPU times for\nlarge scale problems of the MATLAB Optimisation Toolbox of Table V\nProblem Problem Type (m,n) n\u02c6 p CPU(f)(ms)\nnlsf1a Function Jacobian (1000,1000) 3 3 0.35\nbrownf gradient from function (1,1000) 1000 1000 1.57\nbrownfg Hessian from gradient (1000,1000) 3 3 5.74\ntbroyf gradient from function (1,800) 800 800 1.16\ntbroyfg Hessian from gradient (800,800) 6 8 4.27\nfunction f=arrowhead(x,extra)\n% arrowhead function taken from Coleman & Verma ADMIT\nf=x.*x;\nf(1)=f(1)+x.\u2019*x;\nf=f+x(1)*x(1);\nFig. 12. Coding of the\nIRn \u2192 IRnarrowhead func-\ntion\nTable XI. Evaluation trace for the IRn \u2192 IRn arrowhead example\noperation local derivatives sparsity pattern \u03c7i\nvi= xi, i = 1, . . . , n \u03c7i = ei\nvi= vi\u2212n \u2217 vi\u2212n, i = n+1, . . . ,2n ci,i\u2212n= 2vi\u2212n \u03c7i= ei\u2212n\nv2n+1=\n\u2211n\nj=1\nvj \u2217 vj c2n+1,j= 2vj , j = 1, . . . , n \u03c72n+1= {1, 1, . . . , 1}\nv2n+2= vn+1 + v2n+1 c2n+2,j= 1, j = n+1, 2n+1 \u03c72n+2= {1, 1, . . . , 1}\nv2n+3= v1 \u2217 v1 c2n+3,1= 2v1 \u03c72n+3 = e1\nv2n+4= v2n+2 + v2n+3 c2n+4,j = 1, j = 2n+2, 2n+3 \u03c72n+4= {1, 1, . . . , 1}\nvi= vi\u2212n\u22123 + v2n+3, i = 2n+5, . . . , 3n+3 ci,j = 1, j = i\u2212n+3, 2n+3 \u03c7i = e1 + ei\u22122n\u22123\natives by forward mode AD is q(2N|c|6=1+N|c|=1\u2212p\u2212m) floating point operations\nand of propating q adjoints by reverse mode AD is q(2N|c|6=1 + N|c|=1 \u2212 p \u2212 n).\nSince ADMIT calculates the Jacobian of the arrowhead problem using two direc-\ntional derivatives and one adjoint the cost will be 12n+3 floating point operations\nabove those needed for the function and local derivatives.\nNow let us consider the case of forward propagation of directional derivatives\nstored as sparse vectors as used by fmad\u2019s sparse forward mode. The sparsity\npattern of each variable in the calculation is given in the third column of Table XI\nto assist in determining floating point operations counts. We start with derivatives\nfor the first n variables initialised to rows of the n\u00d7 n identity matrix,\n\u2207vi = ei, i = 1, . . . , n.\nNow we consider in turn each set of operations given in Table XI.\n\u2014For i = n + 1, . . . , 2n we have \u2207vi = ci,i\u2212n\u2207vi\u2212n for a cost of n multiplications\nsince the \u2207vi\u2212n correspond to derivatives of the independents, each of which has\nonly one entry.\n\u2014For i = 2n+ 1 we have\u2207v2n+1 =\n\u2211n\nj=1 c2n+1,j\u2207vj for a cost of nmultiplications,\nsince each \u2207vj corresponds to derivatives of the independents which have only\none nonzero entry, and n additions as each multiplied \u2207vj is added to a dense\nworking vector [Gilbert et al. 1992]. The resulting \u2207v2n+1 is full.\n\u2014For i = 2n+ 2 we have \u2207v2n+2 = \u2207vn+1 +\u2207v2n+1 which, since \u2207v2n+1 is full\nbut \u2207vn+1 has one entry, incurs (n+1) additions as they are accumulated in the\nworking vector. The resulting \u2207v2n+2 is full.\nACM Transactions on Mathematical Software, Vol. , No. , Accepted Aug. 2005.\n24 \u00b7 Shaun A. Forth August 23, 2006\n\u2014For i = 2n+ 3 \u2207v2n+3 = c2n+3,1\u2207v1 incurs just one multiplication since v1 has\njust one entry and one addition in the sparse accumulation.\n\u2014For i = 2n+ 4, \u2207v2n+4 = \u2207v2n+2 +\u2207v2n+3 incurs n+ 1 additions since \u2207v2n+2\nis full but \u2207v2n+3 has just one entry.\n\u2014For i = 2n+5, . . . , 3n+3, the (n\u2212 1) derivative combinations \u2207vi = \u2207vi\u2212n\u22123 +\n\u2207v2n+3 each incur two additions, giving a total of 2(n\u2212 1), since both \u2207vi\u2212n\u22123\nand \u2207v2n+3 have just one entry.\nThus sparse propagation of derivatives requires just 7n+2 floating point operations.\nREFERENCES\nBerz, M., Bischof, C., Corliss, G., and Griewank, A., Eds. 1996. Computational Differenti-\nation: Techniques, Applications, and Tools. SIAM, Philadelphia, Penn.\nBischof, C., Bu\u00a8cker, H., Lang, B., Rasch, A., and Vehreschild, A. 2002. Combining source\ntransformation and operator overloading techniques to compute derivatives for MATLAB pro-\ngrams. In Proceedings of the Second IEEE International Workshop on Source Code Analysis\nand Manipulation (SCAM 2002). IEEE Computer Society, p. 65\u201372.\nBischof, C., Lang, B., and Vehreschild, A. 2003. Automatic differentiation for MATLAB\nprograms. Proc. Appl. Math. Mech 2, 1, 50\u201353.\nBischof, C. H., Carle, A., Khademi, P., and Mauer, A. 1996. ADIFOR 2.0: Automatic\ndifferentiation of Fortran 77 programs. IEEE Computational Science & Engineering 3, 3,\n18\u201332.\nBischof, C. H., Khademi, P. M., Bouaricha, A., and Carle, A. 1996. Efficient computations\nof gradients and Jacobians by dynamic exploitation of sparsity in automatic differentiation.\nOptimization Methods and Software 7, 1\u201339.\nBischof, C. H., Roh, L., and Mauer, A. 1997. ADIC \u2014 An extensible automatic differentiation\ntool for ANSI-C. Software \u2013 Practice and Experience 27, 12, 1427\u20131456. See www-fp.mcs.anl.\ngov\/adic\/.\nBorggaard, J. and Verma, A. 2000. On efficient solutions to the continuous sensitivity equation\nusing automatic differentiation. SIAM J. Sci. Comput. 22, 1, 39\u201362.\nBradshaw, D. 2004. The use of numerical optimisation to determine on-limit handling behaviour\nof race cars. Ph.D. thesis, School of Engineering, Department of Automotive, Mechanical and\nStructural Engineering, Cranfield University, Bedfordshire, MK43 0AL, UK.\nColeman, T. F. and Verma, A. 1996. Structure and efficient Jacobian calculation. In Compu-\ntational Differentiation: Techniques, Applications, and Tools, M. Berz, C. Bischof, G. Corliss,\nand A. Griewank, Eds. SIAM, Philadelphia, Penn., 149\u2013159.\nColeman, T. F. and Verma, A. 1998a. ADMAT: An automatic differentiation toolbox for\nMATLAB. Tech. rep., Computer Science Department, Cornell University.\nColeman, T. F. and Verma, A. 1998b. The efficient computation of sparse Jacobian matrices\nusing automatic differentiation. SIAM J. Sci. Comput. 19, 4, 1210\u20131233.\nColeman, T. F. and Verma, A. 2000. ADMIT-1: Automatic differentiation and MATLAB\ninterface toolbox. ACM Trans. Math. Softw. 26, 1 (Mar.), 150\u2013175.\nCorliss, G., Faure, C., Griewank, A., Hascoe\u00a8t, L., and Naumann, U., Eds. 2001. Auto-\nmatic Differentiation: From Simulation to Optimization. Computer and Information Science.\nSpringer, New York.\nFastOpt 2003. Transformation of Algorithms in Fortran, Manual, Draft Version, TAF Version\n1.6. FastOpt. See http:\/\/www.FastOpt.com\/taf.\nForth, S. A. 2001. User guide for MAD - a Matlab automatic differentiation toolbox. Applied\nMathematics and Operational Research Report AMOR 2001\/5, Cranfield University (RMCS\nShrivenham), Swindon, SN6 8LA, UK. June.\nForth, S. A. and Edvall, M. M. 2004. User Guide for MAD - MATLAB Automatic Differ-\nentiation Toolbox TOMLAB\/MAD, Version 1.1 The Forward Mode. TOMLAB Optimisation\nInc., 855 Beech St 12, San Diego, CA 92101, USA. See http:\/\/tomlab.biz\/products\/mad.\nACM Transactions on Mathematical Software, Vol. , No. , Accepted Aug. 2005.\nEfficient Forward Mode AD in MATLAB \u00b7 25\nForth, S. A. and Ketzscher, R. 2004. High-level interfaces for the MAD (Matlab Automatic\nDifferentiation) package. In 4th European Congress on Computational Methods in Applied\nSciences and Engineering (ECCOMAS), P. Neittaanma\u00a8ki, T. Rossi, S. Korotov, E. On\u02dcate,\nJ. Pe\u00b4riaux, and D. Kno\u00a8rzer, Eds. Vol. 2. University of Jyva\u00a8skyla\u00a8, Department of Mathematical\nInformation Technology, Finland. ISBN 951-39-1869-6.\nForth, S. A., Tadjouddine, M., Pryce, J. D., and Reid, J. K. 2004. Jacobian code generated by\nsource transformation and vertex elimination can be as efficient as hand-coding. ACM Trans.\nMath. Softw. 30, 3 (Sep.), 266 \u2013 299. See http:\/\/doi.acm.org\/10.1145\/1024074.1024076.\nGilbert, J. R., Moler, C., and Schreiber, R. 1992. Sparse matrices in MATLAB: Design and\nimplementation. SIAM Journal on Matrix Analysis and Applications 13, 1 (Jan.), 333\u2013356.\nGriewank, A. 2000. Evaluating Derivatives: Principles and Techniques of Algorithmic Differ-\nentiation. Number 19 in Frontiers in Appl. Math. SIAM, Philadelphia, Penn.\nGriewank, A., Juedes, D., and Utke, J. 1996. Algorithm 755: ADOL\u2013C, a package for the\nautomatic differentiation of algorithms written in C\/C++. ACM Trans. Math. Softw. 22, 2,\n131\u2013167.\nGriewank, A. and Reese, S. 1991. On the calculation of Jacobian matrices by the Markowitz\nrule. In Automatic Differentiation of Algorithms: Theory, Implementation, and Application,\nA. Griewank and G. F. Corliss, Eds. SIAM, Philadelphia, Penn., 126\u2013135.\nHairer, E. and Wanner, G. 1991. Ordinary Differential Equations II, Stiff and Differential-\nAlgebraic Problems. Springer-Verlag, Berlin.\nHolmstro\u00a8m, K. and Edvall, M. M. January 2004. Chapter 19: The TOMLAB opti-\nmization environment. In Modeling Languages in Mathematical Optimization, J. Kall-\nrath, Ed. APPLIED OPTIMIZATION 88, ISBN 1-4020-7547-2. Kluwer Academic Publishers,\nBoston\/Dordrecht\/London.\nHolmstro\u00a8m, K., Go\u00a8ran, A. O., and Edvall, M. M. 2004. User\u2019s guide for TOMLAB 4.3.\nTOMLAB Optimisation Inc., 855 Beech St 12, San Diego, CA 92101, USA. See http:\/\/www.\ntomlab.biz.\nKharche, R. V. 2004. Source transformation for automatic differentiation in MATLAB. M.S. the-\nsis, Cranfield University (Shrivenham Campus), Applied Mathematics & Operational Research\nGroup, Engineering Systems Department, RMCS Shrivenham, Swindon SN6 8LA, UK.\nNaumann, U. 1999. Efficient calculation of Jacobian matrices by optimized application of the\nchain rule to computational graphs. Ph.D. thesis, Technical University of Dresden.\nNaumann, U. 2004. Optimal accumulation of Jacobian matrices by elimination methods on the\ndual computational graph. Mathematical Programming 99, 3 (April), 399\u2013421.\nNocedal, J. and Wright, S. J. 1999. Numerical Optimization. Springer series in operational\nresearch. Springer-Verlag, New York.\nPryce, J. D. and Reid, J. K. 1998. ADO1, a Fortran 90 code for automatic differentiation.\nTech. Rep. RAL-TR-1998-057, Rutherford Appleton Laboratory, Chilton, Didcot, Oxfordshire,\nOX11 OQX, England. See ftp:\/\/matisa.cc.rl.ac.uk\/pub\/reports\/prRAL98057.ps.gz.\nRich, L. C. and Hill, D. R. 1992. Automatic differentiation in MATLAB. App. Num. Math. 9,\n33\u201343.\nRingrose, T. J. and Forth, S. A. 2002. Improved fitting of constrained multivariate regression\nmodels using automatic differentiation. In COMPSTAT 2002: Proceedings in Computational\nStatistics, 15th Symposium, W. Hardle and B. Ronz, Eds. Physica-Verlag, Heidelberg, Berlin,\nGermany, 383\u2013388.\nRingrose, T. J. and Forth, S. A. 2004. Simplifying multivariate second order response surfaces\nby fitting constrained models using automatic differentiation. Technometrics Accepted.\nShampine, L. and Reichelt, M. 1997. The MATLAB ODE suite. SIAM J. Sci. Comput. 18,\n1\u201322.\nShampine, L. F., Ketzscher, R., and Forth, S. A. 2005. Using AD to solve BVPs in MATLAB.\nACM Trans. Math. Softw. 31, 1 (Mar.), 79\u201394.\nTadjouddine, M., Forth, S. A., and Pryce, J. D. 2001. AD tools and prospects for optimal\nAD in CFD flux Jacobian calculations. In Automatic Differentiation: From Simulation to\nACM Transactions on Mathematical Software, Vol. , No. , Accepted Aug. 2005.\n26 \u00b7 Shaun A. Forth August 23, 2006\nOptimization, G. Corliss, C. Faure, A. Griewank, L. Hascoe\u00a8t, and U. Naumann, Eds. Computer\nand Information Science. Springer, New York, Chapter 30, 247\u2013252.\nTapenade 2003. The TAPENADE tutorial http:\/\/www-sop.inria.fr\/tropics\/tapenade\/\ntutorial.html. Web Site.\nThe MathWorks Inc. 2003. Using Matlab, Version 6. The MathWorks Inc., 24 Prime Park Way,\nNatick, MA 01760-1500.\nThe Mathworks Inc. Sept 2003. Optimization Toolbox User\u2019s Guide, Version 2. The Mathworks\nInc., 3 Apple Hill Drive, Natick MA 01760-2098.\nVehreschild, A. 2001. Semantic augmentation of MATLAB programs to compute derivatives.\nDiploma Thesis, Institute for Scientific Computing, Aachen University, Germany.\nVerma, A. 1998a. ADMAT: Automatic differentiation in MATLAB using object oriented meth-\nods. In SIAM Interdisciplinary Workshop on Object Oriented Methods for Interoperability.\nSIAM, National Science Foundation, Yorktown Heights, New York, 174\u2013183.\nVerma, A. 1998b. Structured automatic differentiation. Ph.D. thesis, Cornell University Depart-\nment of Computer Science, Ithaca, NY.\nACKNOWLEDGMENTS\nThe author would like to thank colleagues: Robert Ketzscher for valuable work\nperformed optimizing Mad [Shampine et al. 2005]; and Venkat Sastry, John Pryce\nand Yi Cao for helpful suggestions regarding MATLAB performance optimization.\nReceived May 2004; revised May 2005; accepted August 2005.\nACM Transactions on Mathematical Software, Vol. , No. , Accepted Aug. 2005.\n"}