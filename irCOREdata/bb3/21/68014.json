{"doi":"10.1109\/FUZZY.2010.5584905","coreId":"68014","oai":"oai:eprints.lancs.ac.uk:33926","identifiers":["oai:eprints.lancs.ac.uk:33926","10.1109\/FUZZY.2010.5584905"],"title":"User modeling : through statistical analysis and an evolving classifier","authors":["Iglesias, Jose","Angelov, Plamen","Ledezma, Agapito","Sanchis, Araceli"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-07","abstract":"Knowledge about computer users is very beneficial for assisting them, predicting their future actions or detecting masqueraders. In this paper, an approach for creating and recognizing automatically the behavior profile of a computer user is combined with an evolving method to keep up to date the created profiles. The behavior of a computer is represented in this research as the sequence of commands s\/he types during a period of time. This sequence is treated using statistical methods in order to create the corresponding user profile. However, as a user profile is usually not fixed but rather it changes and evolves, we propose a user profile classifier based on Evolving Systems. This paper describes briefly the model creation method and the evolving classifier, which are compared with well established off-line and on-line classifiers. (c) IEEE Pres","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":"IEEE","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:33926<\/identifier><datestamp>\n      2018-01-24T02:05:27Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D626F6F6B5F73656374696F6E<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        User modeling : through statistical analysis and an evolving classifier<\/dc:title><dc:creator>\n        Iglesias, Jose<\/dc:creator><dc:creator>\n        Angelov, Plamen<\/dc:creator><dc:creator>\n        Ledezma, Agapito<\/dc:creator><dc:creator>\n        Sanchis, Araceli<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        Knowledge about computer users is very beneficial for assisting them, predicting their future actions or detecting masqueraders. In this paper, an approach for creating and recognizing automatically the behavior profile of a computer user is combined with an evolving method to keep up to date the created profiles. The behavior of a computer is represented in this research as the sequence of commands s\/he types during a period of time. This sequence is treated using statistical methods in order to create the corresponding user profile. However, as a user profile is usually not fixed but rather it changes and evolves, we propose a user profile classifier based on Evolving Systems. This paper describes briefly the model creation method and the evolving classifier, which are compared with well established off-line and on-line classifiers. (c) IEEE Press<\/dc:description><dc:publisher>\n        IEEE<\/dc:publisher><dc:date>\n        2010-07<\/dc:date><dc:type>\n        Contribution in Book\/Report\/Proceedings<\/dc:type><dc:type>\n        NonPeerReviewed<\/dc:type><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/FUZZY.2010.5584905<\/dc:relation><dc:identifier>\n        Iglesias, Jose and Angelov, Plamen and Ledezma, Agapito and Sanchis, Araceli (2010) User modeling : through statistical analysis and an evolving classifier. In: IEEE International Conference on Fuzzy Systems (FUZZ), 2010. IEEE, pp. 3226-3233. ISBN 978-1-4244-6919-2<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/33926\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":null,"relations":["http:\/\/dx.doi.org\/10.1109\/FUZZY.2010.5584905","http:\/\/eprints.lancs.ac.uk\/33926\/"],"year":2010,"topics":["QA75 Electronic computers. Computer science"],"subject":["Contribution in Book\/Report\/Proceedings","NonPeerReviewed"],"fullText":"User Modeling: Through Statistical Analysis and an Evolving\nClassifier.\nJose A. Iglesias, Plamen Angelov, Agapito Ledezma and Araceli Sanchis\nAbstract\u2014 Knowledge about computer users is very beneficial\nfor assisting them, predicting their future actions or detecting\nmasqueraders. In this paper, an approach for creating and\nrecognizing automatically the behavior profile of a computer\nuser is combined with an evolving method to keep up to date\nthe created profiles. The behavior of a computer is represented\nin this research as the sequence of commands s\/he types during a\nperiod of time. This sequence is treated using statistical methods\nin order to create the corresponding user profile. However, as a\nuser profile is usually not fixed but rather it changes and evolves,\nwe propose a user profile classifier based on Evolving Systems.\nThis paper describes briefly the model creation method and the\nevolving classifier, which are compared with well established\noff-line and on-line classifiers.\nI. INTRODUCTION\nRecognizing the behavior of users in real-time is a sig-\nnificant challenge in different tasks, such as to predict\ntheir behavior, to coordinate with them or to assist them.\nComputer user modeling is the process of learning about\nordinary computer users by observing the way the use the\ncomputer. The result of a user modeling in computer systems\nis usually stored in user profiles that contains information that\ncharacterizes the usage behavior of a computer user. This\ninformation may be used in many areas, such as interaction\nwith users appropriately [1] or computer-aided instruction [2].\nExperience has shown that users themselves do not know\nhow to articulate what they do, especially if they are very\nfamiliar with the tasks they perform. Computer users, like all\nof us, leave out activities that they do not even notice they are\ndoing. Thus, only by observing users we can model his\/her\nbehavior correctly [3]. However, the construction of effective\ncomputer user profiles is a difficult problem because of the\nfollowing aspects: human behavior is usually erratic, and\nsometimes humans behave differently because of a change\nin their goals. The latter problem makes necessary that the\nuser profiles we create evolve.\nMost existing techniques for computer user modeling as-\nsume the availability of carefully hand-crafted user profiles,\nwhich encode the a-priori known behavior repertoire of the\nobserved user. Unfortunately, techniques for automatically\nacquiring user profiles from observations are only beginning\nto emerge.\nA user model may take different forms depending on the\npurposes for which it is created. As Webb et al. propose [4],\nuser models may seek to describe:\nPlamen Angelov is with the Department of Communication Systems,\nInfoLab21, Lancaster University, UK. E-mail:p.angelov@lancaster.ac.uk\nJose A. Iglesias, Agapito Ledezma and Araceli Sanchis are with\nthe CAOS Group, Carlos III University of Madrid, Spain. E-\nmails:{jiglesia,ledezma,masm}@inf.uc3m.es.\n1) the cognitive processes that underlie the users actions;\n2) the difference between the users skills and expert skills;\n3) the users behavioral patterns or preferences; or\n4) the user characteristics.\nRecent research has predominantly pursued the third ap-\nproach, focusing on users behaviors as proposed by Webb\n[5], rather than on the cognitive processes that underlie that\nbehavior. This research is also focused on finding relevant\nusers behavioral patterns in order to classify a user behavior.\nWe will use the approach presented in [6] for automatically\ncreating the profile of a user based on the analysis of the\nsequence of commands s\/he typed.\nOnce many profiles have been created, in order to classify\na new user during run-time, most existing algorithms match\nthe observed behavior of a user against a profile-library,\nand matches are reported as hypotheses. However, as a user\nprofile is not necessarily fixed but rather it evolves\/changes,\nwe use in this research the evolving classification method\nproposed in [7]. This approach based on Evolving Systems [8]\nclassifies an observed user and the profiles created are always\nup to date. In this research, this method is exhaustively ana-\nlyzed and compared in detail with many other classification\nmethods. A novel study about the ability of this approach to\nadapt to new data is also presented.\nThis paper is organized as follows; Section 2 provides a\nbrief overview of the background and related work relevant\nto this research. How the computer user profiles are created\nfrom a sequence of commands is explained in section 3. In\nsection 4, it is detailed the evolving user classifier. Section\n5 describes the experimental setting and the experimental\nresults obtained. Finally, Section 6 contains future work and\nconcluding remarks.\nII. BACKGROUND AND RELATED WORK\nTo model, recognize, or classify the behavior of a com-\nputer user is very useful in many different computer areas:\nDiscovery of navigation patterns [9], Web recommender\nsystems [10] or Computer security [11]. For this reason,\nthe literature of agent modeling is truly vast. However, in\nthis research we focus on discovering computer user patterns\nfrom the sequence of commands a user types. The problem of\nbehavior classification is examined as a problem of learning\nto characterize the behavior of a user in terms of sequences\nof commands.\nAs in this research, there are other areas in which sequen-\ntial data are analyzed in order to solve a specific problem. In\ngeneral, the sequence learning problem can be categorized in\nWCCI 2010 IEEE World Congress on Computational Intelligence \nJuly, 18-23, 2010 - CCIB, Barcelona, Spain FUZZ-IEEE\n978-1-4244-8126-2\/10\/$26.00 c\u00a92010 IEEE 3226\nfour basic categories: sequence prediction, sequence genera-\ntion, sequence classification and sequential decision making.\nIn this paper, the sequence classification is the category\nanalyzed and developed.\nConsidering the sequence classification, the main reason\nto need to handle sequential data is because of the observed\ndata from some environments are inherently sequential. An\nexample of these data is the DNA sequence. Ma et al. [12]\npresent new techniques for bio-sequence classification. In a\nvery different problem (computer intrusion detection prob-\nlem), Coull et al. [13] propose an algorithm that uses pair-\nwise sequence alignment to characterize similarity between\nsequences of commands. The algorithm produces an effective\nmetric for distinguishing a legitimate user from a masquer-\nader. Schonlau et al. [14] investigate a number of statistical\napproaches for detecting masqueraders.\nA very important issue in sequence learning is tempo-\nral dependencies. The following aspect is essential in our\nresearch: A current situation or the action that an agent\nperforms usually depend on what has happened before. This\naspect is taken into account in our research and in models\nsuch as HMMs; however, there are some other models which\nhave problems dealing with such dependencies. For example,\nrecurrent neural network models or reinforcement learning\ncan not manage efficiently the long-range dependencies.\nIII. CREATING A COMPUTER USER PROFILE\nIn this research we consider that the commands typed\nby a user are usually influenced by past experiences. This\naspect motivates the idea of automated sequence learning\nfor behavior classification; if we do not know the features\nthat influence the behavior of an agent, we can consider a\nsequence of past actions to incorporate some of the historical\ncontext of the agent. Indeed, sequence learning is arguable\nthe most common form of human and animal learning.\nSequences are absolutely relevant in human skill learning and\nin high-level problem solving and reasoning [15]. Taking this\naspect into account in this paper, the computer user modeling\nis transformed into a sequence analysis problem where a\nsequence of commands represents a specific behavior. This\ntransformation can be done because it is clear that any\nbehavior has a sequential aspect, as actions are performed\nin a sequence.\nThe commands typed by a computer user are inherently\nsequential, and this sequentiality is considered in the mod-\neling process (when a user types a command, it usually\ndepends on the previous typed commands and it is related to\nthe following commands). According to this aspect, in order\nto get the most representative set of subsequences from a\nsequence, we propose the use of a trie data structure [16].\nThis trie data structure was also used in [17], [18] where a\nteam behavior was learned as well as in [19] to classify the\nbehavior patterns of a RoboCup soccer simulation team.\nThe construction of a user profile from a single sequence of\ncommands is done as it is explained in [6] for any sequence of\nevents. This process consists of three steps: 1. Segmentation\nof the sequence of commands, 2. Storage of the subsequences\nin a trie, and 3. Creation of the user profile. These steps are\ndetailed in the following 3 subsections.\nThese steps are detailed in the following 3 subsections. For\nthe sake of simplicity, let us consider the following sequence\nas an example: {ls \u2192 date \u2192 ls \u2192 date \u2192 cat}.\nA. Segmentation of the sequence of commands\nFirst, the sequence is segmented into subsequences of\nequal length from the first to the last element. Thus, the\nsequence A=A1A2...An (where n is the number of commands\nof the sequence) will be segmented in the subsequences de-\nscribed by Ai...Ai+length \u2200 i,i=[1,n-length+1], where length\nis the size of the subsequences created and this value deter-\nmines how many commands are considered as dependent. In\nthe remainder of the paper, we will use the term subsequence\nlength to denote the value of this length. In addition, in this\ncase, the value of this term represents how many commands\na user usually types consecutively as part of his\/her behavior\npattern.\nIn the proposed sample sequence ({ ls\u2192 date\u2192 ls\u2192 date\n\u2192 cat}), let 3 be the subsequence length, then we obtain:\n{ls \u2192 date \u2192 ls}, {date \u2192 ls \u2192 date}, {ls \u2192 date \u2192\ncat}\nB. Storage of the subsequences in a trie\nThe subsequences of commands are stored in a trie in\na way that all possible subsequences are accessible and\nexplicitly represented. A node of a trie represents a command,\nand its children represent the commands that follow it. Also,\neach node keeps track of the number of times a command\nhas been inserted into it. When a new subsequence is inserted\ninto a trie, the existing nodes are modified and\/or new nodes\nare created. Moreover, as the dependencies of the commands\nare relevant in the user profile, the subsequence suffixes\n(subsequences that extend to the end of the given sequence)\nare also inserted.\nConsidering the previous example, the first subsequence\n({ls \u2192 date \u2192 ls}) is added as the first branch of the empty\ntrie (Figure 1 a). Each node is labeled with the number 1\nwhich indicates that the command has been inserted in the\nnode once (in Figure 1, this number is enclosed in square\nbrackets). Then, the suffixes of the subsequence ({date\u2192 ls}\nand {ls}) are also inserted (Figure 1 b). Finally, after inserting\nthe three subsequences and its corresponding suffixes, the\ncompleted trie is obtained (Figure 1 c).\nC. Creation of the user profile\nOnce the trie is created, the subsequences that characterize\nthe user profile and its relevance are calculated by traversing\nthe trie. For this purpose, frequency-based methods are used.\nIn particular, to evaluate the relevance of a subsequence,\nits relative frequency or support [20] is calculated. In this\ncase, the support of a subsequence is defined as the ratio of\nthe number of times the subsequence has been inserted into\nthe trie to the total number of subsequences of equal size\ninserted.\n3227\nFig. 1. Steps of creating an example trie.\nFig. 2. Distribution of subsequences of commands - Example.\nThus, in this step, the trie can be transformed into a\nset of subsequences labeled by its support value. This set\nof subsequences is represented as a distribution of relevant\nsubsequences. In the previous example, the trie consists of\n9 nodes; therefore, the corresponding profile consists of 9\ndifferent subsequences which are labeled with its support.\nFigure 2 shows the distribution of these subsequences.\nOnce a user behavior profile has been created, it is classi-\nfied and used to update the Evolving Profile Library (EPLib),\nas explained in the next section.\nIV. EVOLVING COMPUTER USER CLASSIFIER\nA classifier is a mapping from the feature space to the\nclass label space. In the proposed evolving classifier, the\nfeature space is defined by distributions of subsequences\nof commands. On the other hand, the class label space is\nrepresented by the most representative distributions. Thus,\na distribution in the class label space represents a specific\nprofile which is one of the prototypes of the evolving library\nEPLib. These prototypes are not fixed and evolve taking\ninto account the new information collected on-line from the\ndata stream - this is what makes the classifier Evolving. The\nnumber of these prototypes is not pre-fixed but it depends on\nthe homogeneity of the observed sequences.\nThe following subsections describes how a user profile\nis represented by the proposed classifier, called EVABCD\n- Evolving Agent Behavior Classifier Based on Distributions\nof commands, and how this classifier is created in an evolving\nmanner.\nA. User behavior representation\nFirst, EVABCD converts the sequence of commands into\nthe corresponding distribution of subsequences on-line. In\norder to classify a user profile, these distributions must be\nrepresented in a data space. For this reason, each distribution\nis considered as a data vector that defines a point that can be\nrepresented in the data space.\nThe data space in which these points can be represented\nshould consist of n dimensions, where n is the number of\nthe different subsequences observed. It means that we should\nknow all the different subsequences of the environment a\npriori. However, this value is unknown and the creation of\nthis data space from the beginning is not efficient. For this\nreason, in EVABCD, the dimension of the data space is in-\ncrementally growing according to the different subsequences\nthat are represented in it.\nFig. 3. Distributions of subsequences of commands in an evolving system\napproach - Example\nFigure 3 explains graphically this idea. In this example,\nthe distribution of the first user consists of 5 subsequences of\ncommands (ls, ls-date, date, cat and date-cat), therefore we\nneed a 5 dimensional data space to represent this distribution\nbecause each different subsequence is represented by one\ndimension. If we consider the second user, we can see that\n3228\n3 of the 5 previous subsequences have not been typed by\nthis user (ls-date, date and date-cat), so these values are\nnot available. Also, there are 2 new subsequences (cp and\nls-cp) and the representation of these values in the same\ndata space needs to increase the dimensionality of the data\nspace from 5 to 7. To sum up, the dimensions of the\ndata space represent the different subsequences typed by the\nusers and they will increase according to the different new\nsubsequences obtained.\nB. Structure of the classifier EVABCD\nOnce the corresponding distribution has been created, it\nis processed by the classifier. This classifier does not need\nto be configured according to the environment where it is\nused because it can start \u2019from scratch\u2019. Also, the relevant\ninformation of the obtained samples is necessary to update\nthe library; but, as we will explain in the next subsection,\nthere is no need to store all the samples in it. The structure\nof this classifier includes:\n1) Classify the new sample in a user group represented\nby a prototype.\n2) Calculate the potential of the new data sample to be\na prototype.\n3) Update all the prototypes considering the new data\nsample. It is done because the density of the data\nspace surrounding certain data sample changes with the\ninsertion of each new data sample. Insert the new data\nsample as a new prototype if needed.\n4) Remove any prototype if needed.\nNext 4 subsections explain each step of this evolving\nclassification method.\n1) Classify the new sample: In order to classify a new\ndata sample, we compare it with all the prototypes stored in\nEPLib. This comparison is done using cosine distance and\nthe smallest distance determines the closest similarity. This\naspect is considered in equation (1).\nClass(xz) = Class(Prot\u2217);\nProt\u2217 =MINNumProti=1 (cosDist(xPrototypei , xz))\n(1)\nThe time-consumed for classifying a new sample depends\non the number of prototypes and its number of attributes.\nHowever, we can consider, in general terms, that both\nthe time-consumed and the computational complexity are\nreduced and acceptable for real-time applications (in order\nof milliseconds per data sample).\n2) Calculate the potential of a new data sample: As in\n[21], a prototype is a data sample (a computer user profile\nrepresented by a distribution of subsequences of commands)\nthat groups several samples which represent a certain class.\nThe classifier is initialized with the first data sample, which\nis stored in EPLib. Then, each data sample is classified to one\nof the prototypes (classes) defined in the classifier. Finally,\nbased on the potential of the new data sample to become a\nprototype [22], it could form a new prototype or replace an\nexisting one.\nThe potential (P) of the kth data sample (xk) is calculated\nby the equation (2) which represents a function of the\naccumulated distance between a sample and all the other k-\n1 samples in the data space [21]. The result of this function\nrepresents the density of the data that surrounds a certain data\nsample.\nP (xk) =\n1\n1 +\n\u2211k\u22121\ni=1 distance(xk,xi)\nk\u22121\n(2)\nwhere distance represents the distance between two sam-\nples in the data space.\nIn [23] the potential is calculated using the euclidean\ndistance and in [21] it is calculated using the cosine distance.\nCosine distance has the advantage that it tolerates different\nsamples to have different number of attributes (in this case, an\nattribute is the support value of a subsequence of commands).\nAlso, cosine distance tolerates that the value of several\nsubsequences in a sample can be null (null is different than\nzero). Therefore, EVABCD uses the cosine distance (cosDist)\nto measure the similarity between two samples; as it is\ndescribed in equation (3).\ncosDist(xk, xp) = 1\u2212\n\u2211n\nj=1 xkjxpj\u221a\u2211n\nj=1 x\n2\nkj\n\u2211n\nj=1 x\n2\npj\n(3)\nwhere xk and xp represent the two samples to measure its\ndistance and n represents the number of different attributes\nin both samples.\nNote that the expression in the equation (2) requires all\nthe accumulated data sample available to be calculated,\nwhich contradicts to the requirement for real-time and on-\nline application needed in the proposed approach. For this\nreason, in [21] it is developed a recursive expression cosine\ndistance. This formula is as follows:\nPk(zk) =\n1\n2\u2212 1k\u22121 1\u221a\u2211n\nj=1(z\nj\nk)\n2\nBk\n; k = 2, 3, ...;P1(z1) = 1\nwhere Bk =\nn\u2211\nj=1\nzjkb\nj\nk ; b\nj\nk = b\nj\n(k\u22121) +\n\u221a\n(zjk)2\u2211n\nl=1(z\nl\nk)2\nand bj1 =\n\u221a\n(zj1)2\u2211n\nl=1(z\nl\n1)2\n; j = [1, n+ 1]\n(4)\nUsing this expression, it is only necessary to calculate\n(n+1) values where n is the number of different subsequences\nobtained; this value is represented by b, where bjk, j = [1, n]\nrepresents the accumulated value for the kth data sample.\n3) Creating new prototypes: The proposed evolving user\nbehavior classifier, EVABCD, can start \u2019from scratch\u2019 (with-\nout prototypes in the library) in a similar manner as eClass\nevolving fuzzy rule-based classifier proposed in [23], used\nin [24] for robotics and further developed in [21]. The\npotential of each new data sample is calculated recursively\n3229\nand the potential of the other prototypes is updated. Then,\nthe potential of the new sample (zk) is compared with the\npotential of the existing prototypes. A new prototype is\ncreated if its value is higher than any other existing prototype,\nas shown in equation (5).\n\u2203i, i = [1, NumPrototypes] : P (zk) > P (Proti) (5)\nThus, if the new data sample is not relevant, the overall\nstructure of the classifier is not changed. Otherwise, if\nthe new data sample has high descriptive power and\ngeneralization potential, the classifier evolves by adding a\nnew prototype which represents a part of the observed data\nsamples.\n4) Removing existing prototypes: After adding a new\nprototype, we check whether any of the already existing pro-\ntotypes are described well by the newly added prototype [21].\nBy well we mean that the value of the membership function\nthat describes the closeness to the prototype is a Gaussian\nbell function chosen due to its generalization capabilities:\n\u2203i, i = [1, NumPrototypes] : \u00b5i(zk) > e\u22121 (6)\nFor this reason, we calculate the membership function\nbetween a data sample and a prototype which is defined as:\n\u00b5i(zk) = e\n\u2212 12 [\ncosDist(zk,Proti)\n\u03c3i\n]\n, i = [1, NumPrototypes]\n(7)\nwhere cosDist(zk, P roti) represents the cosine distance\nbetween a data sample (zk) and the ith prototype (Proti);\n\u03c3i represents the spread of the membership function, which\nalso symbolizes the radius of the zone of influence of the\nprototype. This spread is determined based on the scatter [25]\nof the data. The equation to get the spread of the kth data\nsample is defined as:\n\u03c3i(k) =\n\u221a\u221a\u221a\u221a1\nk\nk\u2211\nj=1\ncosDist(Proti, zk) ; \u03c3i(0) = 1 (8)\nwhere k is the number of data samples inserted in the data\nspace; cosDist(Proti, zk) is the cosine distance between the\nnew data sample (zk) and the ith prototype.\nHowever, to calculate the scatter without storing all the\nreceived samples, this value can be updated (as shown\nin [23]) recursively by:\n\u03c3i(k) =\n=\n\u221a\n[\u03c3i(k \u2212 1)]2 + [cosDist\n2(Proti, zk)\u2212 [\u03c3i(k \u2212 1)]2]\nk\n(9)\nV. EXPERIMENTAL SETUP AND RESULTS\nIn order to evaluate EVABCD in a command interface, we\nuse a data set with the UNIX commands typed by 168 real\nusers and labeled in 4 different groups. In this section we will\ndetail the experimental results obtained and the comparison\nwith other different classification methods.\nA. Data Set\nFor these experiments, we use the command-line data\ncollected by Greenberg [26] using UNIX csh command\ninterpreter. In this data, four target groups were identified,\nrepresenting a total of 168 male and female users with a\nwide cross-section of computer experience and needs. Salient\nfeatures of each group are described below, and the sample\nsizes (the number of people observed) are indicated in table I.\n\u2022 Novice Programmers: The users of this group had little\nor no previous exposure to programming, operating\nsystems, or UNIX-like command-based interfaces. These\nusers spent most of their time learning how to program\nand use the basic system facilities.\n\u2022 Experienced Programmers: These group members were\nsenior Computer Science undergraduates, expected to\nhave a fair knowledge of the UNIX environment. These\nusers used the system for coding, word processing, em-\nploying more advanced UNIX facilities to fulfill course\nrequirements, and social and exploratory purposes.\n\u2022 Computer Scientist: This group, graduates and re-\nsearchers from the Department of Computer Science,\nhad varying experience with UNIX, although all were\nexperts with computers. Tasks performed were less\npredictable and more varied than other groups, research\ninvestigations, social communication, word-processing,\nmaintaining databases, and so on.\n\u2022 Non-programmers: Word-processing and document\npreparation was the dominant activity of the members of\nthis group, made up of office staff and members of the\nFaculty of Environmental Design. Knowledge of UNIX\nwas the minimum necessary to get the job done.\nTABLE I\nSAMPLE GROUP SIZES AND COMMAND LINES RECORDED.\nGroup of users name Sample size Total number\nof command lines\nNovice Programmers 55 77423\nExperienced Programmers 36 74906\nComputer Scientists 52 125691\nNon-Programmers 25 25608\nTotal 168 303628\nB. Experimental Design\nAlthough the proposed classifier can be used in real-time,\nin order to measure its performance using the above data set,\nthe 10-fold cross-validation thecnique is used. all the users\n(training set) are divided into 10 disjoint subsets with equal\nsize. Each of the 10 subsets is left out in turn for evaluation.\nIt should be emphasized that EVABCD does not need to\nwork in this model. This is done mainly in order to have\ncomparable results with very different techniques.\nThe number of UNIX commands typed by a user and\nused for creating his\/her profile, is very relevant in the\nclassification process. When EVABCD is carried out in the\nfield, the behavior of a user is classified (and the evolving\nbehavior library updated) after s\/he types a limited number\n3230\nof commands. In order to show the relevance of this aspect\nusing the data set already described, we consider sequences\nof different number of UNIX commands for creating the\nuser profile: 100, 500 and 1000 commands per user. Also,\nif the number of users increases, the number of different\nsubsequences increases, too.\nIn the phase of user profile creation, the length of the\nsubsequences in which the original sequence is segmented\n(used for creating the trie) is a relevant parameter: using long\nsubsequences, the time consumed for creating the trie and\nthe number of relevant subsequences of the corresponding\ndistribution increase drastically. In the experiments presented\nin this paper, the subsequence length varies from 2 to 6.\nIn addition, we should consider that the number of subse-\nquences obtained using different sequences of data is often\nvery large. To get an idea of how this number increases,\ntable II shows the number of different subsequences obtained\nusing different number of commands for training (100, 500\nand 1000 commands per user) and subsequence lengths (3\nand 5).\nTABLE II\nTOTAL NUMBER OF DIFFERENT SUBSEQUENCES OBTAINED.\nNumber of Subsequence Number of\ncommands per user Length different subsequences\n100 3 11451\n5 34164\n500 3 39428\n5 134133\n1000 3 63375\n5 227715\nUsing EVABCD, the number of prototypes to create per\neach group is not fixed, it varies automatically depending on\nthe heterogeneity of the data. To get an idea about this aspect,\ntable III tabulates the number of different prototypes per\ngroup created in each of the 10 runs using 1000 commands\nper user as training dataset and a subsequence length of 3.\nTABLE III\nEVABCD: NUMBER OF PROTOTYPES CREATED PER GROUP USING\n10-FOLD CROSS-VALIDATION\nNumber of prototypes in each of the 10 runs\nGroup 1 2 3 4 5 6 7 8 9 10\nNovice Progr. 4 5 3 4 4 3 2 3 4 5\nExp. Progr. 1 1 1 1 2 3 2 1 1 2\nComp. Scientists 1 1 1 1 1 1 2 2 2 2\nNon-Progr. 1 1 1 1 1 1 1 1 1 1\nC. Results\nIn order to evaluate the performance of EVABCD, we\ncompare it with incremental and non-incremental classifiers:\nNaive Bayes [27], k-Nearest Neighbor [28] (incremental\nand non-incremental), and C5.0 (a commercial version of\nC4.5 [29], [30]).\nFor this purpose, these classifiers were trained using a\nfeature vector for each user (168 samples). This vector\nconsists of the support value of all the different subsequences\nobtained for all the users; thus, there are a lot of subsequences\nwhich do not have a value because the corresponding user\nhas not typed those commands. In this case, in order to be\nable to use this data for training the classifiers, we consider\nthe value 0 (although its real value is null).\nTable IV shows the percentage of users correctly classified\ninto its corresponding group using different number of com-\nmands for training (100, 500 and 1000 commands per user)\nand subsequences lengths for segmenting the initial sequence\n(from 2 to 6).\nAccording to this data, we can see that the percentages\nof users correctly classified by our approach are better than\nthe obtained by K-NN but worse than the obtained by Naive\nBayes. For small subsequences length (2 or 3) the difference\nbetween EVABCD and Naive Bayes is considerable; but\nthis difference decreases if this length is longer (5 or 6). In\ngeneral, these results show that the proposed classifier works\nwell in this kind of environments when the subsequence\nlength is around 5. However, EVABCD is suitable in the\nproposed environment because it does not need to store the\nentire data stream in the memory and disregards any sample\nafter being used. In addition, EVABCD is one-pass (each\nsample is proceeded once at the time of its arrival), while non-\nincremental classifiers are offline algorithms which require a\nbatch set of training data in the memory and make many\niterations. For this reason, EVABCD is computationally\nsimple and efficient as it is recursive and one pass. In fact,\nbecause the number of attributes is very large in a real\nenvironment, the proposed approach EVABCD is the best\nworking alternative.\nIn addition to the advantage that EVABCD is an online\nclassifier, we want to prove the ability of our approach\nto adapt quickly to new data (in this case, new users\nor a different behavior of a user). For this purpose, we\ndesign a new experimental scenario in which the number\nof commands used as training set for each user in a new\nclass is incremented to detect how the different classifiers\nrecognize the users of that new class. Thus, using the same\nexperimental setup explained above, firstly the users of the\nNovice Programmers group will be added incrementally in\nthe training data. In this case, we also use 10-fold cross-\nvalidation, hence the process finishes when the training data\ncontains the 90% of the users of that group. The sequence\nof commands used for creating the profile of a user contains\n1000 commands and it is created using a subsequence length\nof 5.\nThe first graph of the Figure 4 shows the results of\nthis experiment: x-axis represents the number of Novice\nProgrammers users that have been considered in the training\ndata, and y-axis represents the classification rate. In the graph\nwe can see how quickly EVABCD evolves and adapts to the\nnew class. Only with 3 users of the new class, our method is\nable to create a new prototype in which almost 90% of the\ntest users are classified correctly. However, the other non-\nincremental classifiers need a higher number of samples for\n3231\nTABLE IV\nRESULTS OF UNIX USER CLASSIFICATION - COMPARATIVE. #1: NUMBER OF COMMANDS IN A TRAINING SEQUENCE. #2: SUBSEQUENCE LENGTH.\nClassifier and classification rate (%)\nIncremental Classifier Non Incremental Classifier\nNaive k-NN\nEVABCD Bayes Incremental C5.0 Naive k-NN\n#1 #2 Incremental (k=1) Bayes (k=1)\n2 65,5 77,3 38,3 73,9 79,1 42,8\n3 64,9 76,1 36,5 69,6 79,7 39,8\n100 4 64,5 72 34,1 74,6 74,4 39,2\n5 67,9 73,2 32,3 68,6 75 33,3\n6 64,3 73,2 32,3 70,1 77,3 32,7\n2 58,3 77,9 33,9 73,9 82,1 41,9\n3 59,5 73,8 36,9 74,6 77,3 39,8\n500 4 59,2 70,8 39,2 73,9 76,7 38,9\n5 66,7 71,4 35,1 73,6 76,1 37,3\n6 70,8 72,6 35,7 75,6 75,5 36,9\n2 60,1 78,5 43,6 73,9 85,1 44,1\n3 65,5 77,9 44,0 74,6 81,5 47,3\n1.000 4 61,3 77,3 43,5 73,9 79,1 46,1\n5 70,2 76,7 42,5 73,6 78,5 44,0\n6 72,0 76,1 41,9 74,6 77,9 44,6\nFig. 4. Evolution of the classification rate during online learning with a subset of users data set. - Comparison with non-incremental classifiers.\n3232\nrecognizing the users of that new class. The increase in the\nclassification rate is not perfectly smooth because the new\ndata bring useful information but also noise. The other three\ngraphs of the Figure show similar performance for the other\n3 groups.\nVI. CONCLUSIONS\nIn this paper we describe a generic approach to model\nand classify automatically computer users from the sequence\nof commands s\/he types during a period of time. However,\nas a user profile is usually not fixed but rather it changes\nand evolves, we have proposed a user profile classifier able\nto keep up to date the created profiles based on Evolving\nSystems. This evolving classifier is one pass, non-iterative,\nrecursive and it has the potential to be used in an interactive\nmode; therefore, it is computationally very efficient and fast.\nThe test results with a data set of 168 real UNIX users\ndemonstrates that, using an appropriate subsequence length,\nEVABCD can perform almost as well as other well estab-\nlished off-line classifiers in terms of correct classification on\nvalidation data. However, the proposed classifier is able to\nadapt extremely quickly to new data.\nAlthough it is not addressed in this paper, the proposed\nmethod can be also used to monitor, analyze and detect\nabnormalities based on a time varying behavior of same\nusers and to detect masqueraders. It can also be applied\nto other type of users such as users of e-services, digital\ncommunications, etc.\nREFERENCES\n[1] F. Nasoz and C. L. Lisetti, \u201cAffective user modeling for adaptive\nintelligent user interfaces.\u201d in HCI (3), ser. Lecture Notes in Computer\nScience, J. A. Jacko, Ed., vol. 4552. Springer, 2007, pp. 421\u2013430.\n[2] L. Barrow, L. Markman, and C. E. Rouse, \u201cTechnology\u2019s edge: The\neducational benefits of computer-aided instruction,\u201d National Bureau\nof Economic Research, Working Paper 14240, August 2008.\n[3] J. T. Hackos and J. C. Redish, User and Task Analysis for Interface\nDesign. Wiley, 1998.\n[4] G. I. Webb, M. J. Pazzani, and D. Billsus, \u201cMachine learning\nfor user modeling,\u201d User Modeling and User-Adapted Interaction,\nvol. 11, no. 1, pp. 19\u201329, March 2001. [Online]. Available:\nhttp:\/\/dx.doi.org\/10.1023\/A:1011117102175\n[5] G. I. Webb, \u201cFeature based modelling: A methodology for producing\ncoherent, consistent, dynamically changing models of agents compe-\ntency,\u201d in Proceedings of the 1993 World Conference on Artificial\nIntelligence in Education (AI-ED\u201993), P. Brna, S. Ohlsson, and H. Pain,\nEds. Charlottesville, VA: AACE, 1993, pp. 497\u2013504.\n[6] J. A. Iglesias, A. Ledezma, and A. Sanchis, \u201cCreating user profiles\nfrom a command-line interface: A statistical approach,\u201d in UMAP\n2009: Proceedings of the 1st and 7th International Conference on\nUser Modeling, Adaptation, and Personalization, ser. LNCS, vol. 5535.\nSpringer, June 2009, pp. 90\u2013101.\n[7] J. A. Iglesias, P. Angelov, A. Ledezma, and A. Sanchis, \u201cModelling\nevolving user behaviours,\u201d in IEEE Workshop on Evolving and Self-\nDeveloping and Self-Developing Intelligent Systems, ESDIS 2009,\n2009, pp. 16\u201323.\n[8] P. Angelov, Rule-based Models: A Tool for Design of Flexible Adaptive\nSystems. Heidelberg, New York: Springer-Verlag, 2002.\n[9] M. Spiliopoulou and L. C. Faulstich, \u201cWum: A web utilization miner,\u201d\nin In Proceedings of EDBT Workshop WebDB98. Springer Verlag,\n1998, pp. 109\u2013115.\n[10] A. A. Macedo, K. N. Truong, J. A. Camacho-Guerrero, and\nM. da GraC\u00b8a Pimentel, \u201cAutomatically sharing web experiences\nthrough a hyperdocument recommender system,\u201d in HYPERTEXT\n2003. New York, NY, USA: ACM, 2003, pp. 48\u201356.\n[11] D. L. Pepyne, J. Hu, and W. Gong, \u201cUser profiling for computer\nsecurity,\u201d in Proc. American Control Conference, 2004, pp. 982\u2013987.\n[12] Q. Ma, J. T.-L. Wang, D. Shasha, and C. H. Wu, \u201cDna sequence\nclassification via an expectation maximization algorithm and neural\nnetworks: a case study.\u201d IEEE Transactions on Systems, Man, and\nCybernetics, Part C, vol. 31, no. 4, pp. 468\u2013475, 2001.\n[13] S. E. Coull, J. W. Branch, B. K. Szymanski, and E. Breimer, \u201cIntrusion\ndetection: A bioinformatics approach,\u201d in ACSAC, 2003, pp. 24\u201333.\n[14] M. Schonlau, W. DuMouchel, W. Ju, A. Karr, M. Theus, and Y. Vardi,\n\u201cComputer intrusion: Detecting masquerades,\u201d 2001, statistical Sci-\nence.\n[15] J. Anderson, Learning and Memory: An Integrated Approach. New\nYork: John Wiley and Sons., 1995.\n[16] E. Fredkin, \u201cTrie memory,\u201d Comm. A.C.M., vol. 3, no. 9, pp. 490\u2013499,\n1960.\n[17] J. A. Iglesias, A. Ledezma, and A. Sanchs, \u201cSequence classification\nusing statistical pattern recognition.\u201d in IDA, ser. LNCS, vol. 4723.\nSpringer, 2007, pp. 207\u2013218.\n[18] G. A. Kaminka, M. Fidanboylu, A. Chang, and M. M. Veloso, \u201cLearn-\ning the sequential coordinated behavior of teams from observations,\u201d\nin RoboCup, ser. Lecture Notes in Computer Science, vol. 2752.\nSpringer, 2002, pp. 111\u2013125.\n[19] J. A. Iglesias, A. Ledezma, and A. Sanchis, \u201cA comparing method of\ntwo team behaviours in the simulation coach competition,\u201d in MDAI,\nser. LNCS, vol. 3885. Springer, 2006, pp. 117\u2013128.\n[20] R. Agrawal and R. Srikant, \u201cMining sequential patterns,\u201d in Interna-\ntional Conference on Data Engineering, Taiwan, 1995, pp. 3\u201314.\n[21] P. Angelov and X. Zhou, \u201cEvolving fuzzy rule-based classifiers from\ndata streams,\u201d IEEE Transactions on Fuzzy Systems: Special issue on\nEvolving Fuzzy Systems, vol. 16, no. 6, pp. 1462\u20131475, 2008.\n[22] P. Angelov and D. Filev, \u201cAn approach to online identification of\ntakagi-sugeno fuzzy models,\u201d Systems, Man, and Cybernetics, Part B,\nIEEE Transactions on, vol. 34, no. 1, pp. 484\u2013498, Feb. 2004.\n[23] P. Angelov, X. Zhou, and F. Klawonn, \u201cEvolving fuzzy rule-based clas-\nsifiers,\u201d Computational Intelligence in Image and Signal Processing,\n2007. CIISP 2007. IEEE Symposium on, pp. 220\u2013225, April 2007.\n[24] X. Zhou and P. Angelov, \u201cAutonomous visual self-localization in\ncompletely unknown environment using evolving fuzzy rule-based\nclassifier,\u201d Computational Intelligence in Security and Defense Appli-\ncations, 2007. CISDA 2007. IEEE Symp., pp. 131\u2013138, April 2007.\n[25] P. Angelov and D. Filev, \u201cSimpl eTS: a simplified method for learning\nevolving Takagi-Sugeno fuzzy models,\u201d The IEEE International Con-\nference on Fuzzy Systems. FUZZ-IEEE 2005., vol. -, pp. 1068\u20131073,\nMay 2005.\n[26] S. Greenberg, \u201cUsing unix: Collected traces of 168 users,\u201d Master\u2019s\nthesis, Department of Computer Science, University of Calgary, Al-\nberta, Canada, 1988.\n[27] I. Rish, \u201cAn empirical study of the naive bayes classifier,\u201d in Pro-\nceedings of IJCAI-01 Workshop on Empirical Methods in Artificial\nIntelligence, 2001.\n[28] T. Cover and P. Hart, \u201cNearest neighbor pattern classification,\u201d IEEE\nTransactions on Information Theory, vol. 13, no. 1, pp. 21\u201327, 1967.\n[29] J. R. Quinlan, C4.5: programs for machine learning. San Francisco,\nCA, USA: Morgan Kaufmann Publishers Inc., 1993.\n[30] J. Quinlan, \u201cData mining tools see5 and c5.0,\u201d 2003 [online], available:\nhttp:\/\/www.rulequest.com\/see5-info.html.\n3233\n"}