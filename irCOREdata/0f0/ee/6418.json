{"doi":"10.1080\/0968776010090303","coreId":"6418","oai":"oai:generic.eprints.org:357\/core5","identifiers":["oai:generic.eprints.org:357\/core5","10.1080\/0968776010090303"],"title":"On\u2010line student feedback: A pilot study","authors":["Galbraith, Liz","Gee, Paul","Jennings, Fran","Riley, Ron"],"enrichments":{"references":[{"id":192215,"title":"(1991-6), a series of LSE internal studies of the student experience.","authors":[],"date":null,"doi":null,"raw":"Stockdale, J. (ed.) (1991-6), a series of LSE internal studies of the student experience.","cites":null},{"id":441255,"title":"A new survey of NEC learners',","authors":[],"date":"1992","doi":"10.1080\/0268051920070109","raw":"Webb, R. (1992), 'A new survey of NEC learners', Open Learning, 7 (1), 58-62.","cites":null},{"id":192211,"title":"A postal survey of OU students' reading skills',","authors":[],"date":"1997","doi":"10.1080\/0268051970120204","raw":"Macdonal-Ross, M. and Scott, B. (1997), 'A postal survey of OU students' reading skills', Open Learning, 12 (2), 29-40.","cites":null},{"id":192208,"title":"Alternatives to student ratings of college teaching',","authors":[],"date":"1980","doi":"10.2307\/1981172","raw":"Greenwood, G. E. and Ramagli, H. J. (1980), 'Alternatives to student ratings of college teaching', Journal of Higher Education, 51 (6), 673-84.","cites":null},{"id":192207,"title":"Interim report on the teaching quality assurance committee (TQAC) - pilot study in Lent Term","authors":[],"date":"1999","doi":null,"raw":"Galbraith, I, Gee, P. and Riley R. (1999), 'Interim report on the teaching quality assurance committee (TQAC) - pilot study in Lent Term 1999 of an on-line, Web-based, student 24ALT-J Volume 9 Number 3 course evaluation questionnaire', Internal Report to the Teaching Quality Assurance Committee in LSE.","cites":null},{"id":441140,"title":"Peering through a glass darkly: integrative evaluation of an on-line course',","authors":[],"date":"2000","doi":null,"raw":"Taylor, X, Woodman, M., Sumner, T. and Blake, C. T. (2000), 'Peering through a glass darkly: integrative evaluation of an on-line course', Education, Technology and Society, 3 (4), available online at http:llifets.ieee.org\/periodical\/vol_4_2000\/taylor.html.","cites":null},{"id":441254,"title":"Student evaluation of college teaching effectiveness: a brief review', Assessment and Evaluation","authors":[],"date":"1998","doi":"10.1080\/0260293980230207","raw":"Wachtel, H. K. (1998), 'Student evaluation of college teaching effectiveness: a brief review', Assessment and Evaluation in Higher Education, 23 (2), 191-211.","cites":null},{"id":192214,"title":"Student Feedback- Context Issues and Practice,","authors":[],"date":"1993","doi":null,"raw":"Partington, P. (ed.) (1993), Student Feedback- Context Issues and Practice, USDU.","cites":null},{"id":192210,"title":"Student rating of courses in HE: further challenges and opportunities',","authors":[],"date":"1998","doi":"10.1080\/0260293980230106","raw":"Kerridge, J. R. and Mathews, B. P. (1998), 'Student rating of courses in HE: further challenges and opportunities', Assessment and Evaluation in HE, 23 (1), 71-82.","cites":null},{"id":192213,"title":"Students' evaluation of university teaching: a multidimensional perspective',","authors":[],"date":"1992","doi":null,"raw":"Marsh, H. W. and Dunkin, M. J. (1992), 'Students' evaluation of university teaching: a multidimensional perspective', Higher Education: Handbook of Theory and Research, 8, New York: Agathon Press, 143-233.","cites":null},{"id":192212,"title":"Students' evaluation of university teaching',","authors":[],"date":"1987","doi":null,"raw":"Marsh, H. W. (1987), 'Students' evaluation of university teaching', International Journal of Educational Research, 11 (3), 255-78.","cites":null},{"id":192209,"title":"Variations in students' evaluations of teachers' lecturing in different courses on which they lecture: a study at the London School of Economics and Political Science',","authors":[],"date":"1997","doi":"10.1080\/03075079612331381358","raw":"Husbands, C. (1997), 'Variations in students' evaluations of teachers' lecturing in different courses on which they lecture: a study at the London School of Economics and Political Science', Higher Education, 33, 51-70.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2001","abstract":"This paper reports on the outcomes of two experimental trials of the use of on\u2010line questionnaires to assess student satisfaction with courses at the London School of Economics and Political Science. In the first year, eighteen course modules were selected from three departments, surveying a total of 1,100 student places. Students on ten of the courses were invited to complete the \u2018experimental\u2019 on\u2010line survey and the remainder were invited to complete the paper\u2010based questionnaires which have been in use for several years. In the second year, the scale of the experiment was increased, to include forty\u2010six courses across seven departments. Response rates were compared and possible barriers to completion of the on\u2010line questionnaire were considered Whilst electronic monitoring indicated that 95 per cent (first trial) and 80 per cent (second trial) of those contacted for the on\u2010line survey opened the introductory email, only 23 per cent (first trial) and 27 per cent (second trial) completed the on\u2010line survey, compared with a 60 per cent response rate on the paper\u2010based survey. The on\u2010line response is also slightly lower than that achieved by postal surveys of LSE students (30\u201350 per cent response rates). Whilst some technical difficulties could have acted as a barrier, motivation appeared to be the main barrier. Initial results from the second trial, which included two reminder emails and some small incentives, show that it is possible to increase the response rate, but this may still be unacceptably low for staff whose promotion prospects may be affected by results. A third trial has been proposed, looking at ways in which the process as a whole could be amended, to overcome the problem of \u2018survey fatigue\u2019 that the current system faces","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/6418.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/357\/1\/ALT_J_Vol9_No3_2001_On_line_student_feedback__A_pi.pdf","pdfHashValue":"dfa75c2ff76ebc3c1d5d55afcff9548da25cdcc6","publisher":"University of Wales Press","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:357<\/identifier><datestamp>\n      2011-04-04T09:13:31Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/357\/<\/dc:relation><dc:title>\n        On\u2010line student feedback: A pilot study<\/dc:title><dc:creator>\n        Galbraith, Liz<\/dc:creator><dc:creator>\n        Gee, Paul<\/dc:creator><dc:creator>\n        Jennings, Fran<\/dc:creator><dc:creator>\n        Riley, Ron<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        This paper reports on the outcomes of two experimental trials of the use of on\u2010line questionnaires to assess student satisfaction with courses at the London School of Economics and Political Science. In the first year, eighteen course modules were selected from three departments, surveying a total of 1,100 student places. Students on ten of the courses were invited to complete the \u2018experimental\u2019 on\u2010line survey and the remainder were invited to complete the paper\u2010based questionnaires which have been in use for several years. In the second year, the scale of the experiment was increased, to include forty\u2010six courses across seven departments. Response rates were compared and possible barriers to completion of the on\u2010line questionnaire were considered Whilst electronic monitoring indicated that 95 per cent (first trial) and 80 per cent (second trial) of those contacted for the on\u2010line survey opened the introductory email, only 23 per cent (first trial) and 27 per cent (second trial) completed the on\u2010line survey, compared with a 60 per cent response rate on the paper\u2010based survey. The on\u2010line response is also slightly lower than that achieved by postal surveys of LSE students (30\u201350 per cent response rates). Whilst some technical difficulties could have acted as a barrier, motivation appeared to be the main barrier. Initial results from the second trial, which included two reminder emails and some small incentives, show that it is possible to increase the response rate, but this may still be unacceptably low for staff whose promotion prospects may be affected by results. A third trial has been proposed, looking at ways in which the process as a whole could be amended, to overcome the problem of \u2018survey fatigue\u2019 that the current system faces.<\/dc:description><dc:publisher>\n        University of Wales Press<\/dc:publisher><dc:date>\n        2001<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/357\/1\/ALT_J_Vol9_No3_2001_On_line_student_feedback__A_pi.pdf<\/dc:identifier><dc:identifier>\n          Galbraith, Liz and Gee, Paul and Jennings, Fran and Riley, Ron  (2001) On\u2010line student feedback: A pilot study.  Association for Learning Technology Journal, 9 (3).  pp. 17-25.  ISSN 0968-7769     <\/dc:identifier><dc:relation>\n        10.1080\/0968776010090303<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/357\/","10.1080\/0968776010090303"],"year":2001,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"On-line student feedback: a pilot study\nLiz Barnettjane Galbraith, Paul Gee, Fran Jennings and Ron Riley\nLondon School of Economics and Political Science\nemail: l.barnett@lse.ac.uk\nThis paper reports on the outcomes of two experimental trials of the use of on-line\nquestionnaires to assess student satisfaction with courses at the London School of\nEconomics and Political Science. In the first year, eighteen course modules were selected\nfrom three departments, surveying a total of 1,100 student places. Students on ten of the\ncourses were invited to complete the 'experimental' on-line survey and the remainder were\ninvited to complete the paper-based questionnaires which have been in use for several\nyears. In the second year, the scale of the experiment was increased, to include forty-six\ncourses across seven departments. Response rates were compared and possible barriers to\ncompletion of the on-line questionnaire were considered Whilst electronic monitoring\nindicated that 95 per cent (first trial) and 80 per cent (second trial) of those contacted\nfor the on-line survey opened the introductory email, only 23 per cent (first trial) and 27\nper cent (second trial) completed the on-line survey, compared with a 60 per cent\nresponse rate on the paper-based survey. The on-line response is also slightly lower than\nthat achieved by postal surveys of LSE students (30-50 per cent response rates). Whilst\nsome technical difficulties could have acted as a barrier, motivation appeared to be the\nmain barrier. Initial results from the second trial, which included two reminder emails and\nsome small incentives, show that it is possible to increase the response rate, but this may\nstill be unacceptably low for staff whose promotion prospects may be affected by results.\nA third trial has been proposed, looking at ways in which the process as a whole could be\namended, to overcome the problem of 'survey fatigue' that the current system faces.\nIntroduction\nStudent feedback on courses has been a standard feature of university life for several years,\nwith a substantial research backing to it (see, for example, reviews by Marsh, 1987; Marsh\nand Dunkin, 1992). Most feedback systems endeavour to serve more than one purpose,\nwith student feedback forming part of the evidence used by:\n\u2022 individual teachers to improve teaching;\n17\nUz Barnett et 0\/ On-line student feedback a pilot study\n\u2022 heads of departments to monitor and guide individual teachers, especially part-time\nteachers;\n\u2022 course leaders for course development;\n\u2022 senior managers to make decisions on probation and promotion related to teaching\nperformance; consider changes to the wider learning environment; help with the\nmarketing of programmes;\n\u2022 external agencies to assess quality of provision.\nThere are a number of critiques questioning the validity of student response as a measure\nof educational quality. For example, Greenwood and Ramagli (1980) review a number of\nstudies showing low correlations between student ratings and final course outcomes, thus\nquestioning whether 'satisfied' students are necessarily 'effectively educated' students.\nKerridge and Mathews (1998) revisit this debate, questioning the notion of 'student as\nclient' versus 'student as customer', and again stressing that student satisfaction ratings do\nnot equate with student outcomes. However, within the HE sector, the increasing\nperception of 'students as customers' is significant (especially as personal financing of HE\nis increasing). As such, student satisfaction is considered important, and measurement of it\nwill no doubt continue to grow.\nMany institutions use a combination of approaches, including standardized questionnaires\n(often combining both closed and open-ended questions), student discussion sessions,\nfocus groups and meetings, and a range of other student-focused data collection methods\n(see Partington, 1993 for examples).\nThe emergence in the UK of the Quality Assurance Agency (QAA) subject review process\nhas encouraged many institutions to look at how systematic their feedback process is. QAA\nplaces importance on transparent and robust quality assurance systems, which provide\nevidence on course quality and are integrated into explicit quality feedback loops. In some\ninstitutions, central units take responsibility for the management of the student feedback\nprocess; see, for example, the work of the Centre for Research into Quality at the University\nof Central England (UCE (http:\/\/www.uce.aauk\/crq\/)). This can be a major task with\nsubstantial time and resource implications. In the case of the London School of Economics\nand Political Science, the Teaching Quality Assurance and Review Office (TQARO) has been\noperating a centralized student feedback system since 1991 (Husbands, 1997). Up to now the\nsystem has been paper-based, with data subsequently scanned into computer and processed\nelectronically. Faced with the substantial costs needed to update\/revise the existing processing\noperation, the Teaching Quality Assurance Committee (TQAC), responsible for the survey,\nagreed to experiment with on-line delivery. The purpose of the experiment was to establish\nwhether the technology can be used to maintain a robust quality assurance procedure which\nis less resource- and time-intensive and which can deliver results more quickly.\nResearch evidence on the use of on-line questionnaires is limited. A HEFCE-funded\ndevelopment project considering student feedback practices across a number of business\nschools in the UK makes reference to it, but does not cite any research evidence\n(http:\/\/www.lboro.ac.uk\/departments\/bs\/fdtl.html). A search of the past ten years of the\nOpen Learning journal included a number of reports of evaluations of distance learning\nprogrammes - but interestingly these did not include any examples of studies using on-line\n18\nALT-J Volume 9 Number 3\nsurveys. Taylor, Woodman, Sumner and Blake (2000) used both on-line and paper postal\nquestionnaires in evaluating an Open University course. They found that the respondents to\nthe two types of survey had similar profiles. They do not give response rates but report that,\nover successive on-line surveys, 'the response rate begins high then rapidly tails off'. There is\na general impression from those working in the field that getting students to complete on-\nline questionnaires is likely to result in a low response rate, with motivation, access and IT\nskill considered the three main barriers. This paper reports on a trial, at LSE, comparing\nstudent response to on-line and paper-based surveys, to assess variations in response rate\nand possible explanations for them.\nThe LSE student feedback system\nAt LSE, student feedback is collected twice each year, with a separate survey (focused on\npart-time teachers) and the standard survey (for lecture courses completed in the first term)\nundertaken in November\/December, and the main data collection (for all other lecture\ncourses) in February\/March. Students are asked to comment on each course they attend\n(generally four courses per student per year). The questionnaire is in two parts. Part A\nconsists of a number of closed questions\/rating scales. Questions address both broad issues\nof course management and resources, and detailed responses from the students on up to\nthree specified lecturers and three seminar\/class teachers per course. Part B is a series of\nopen-ended questions for free response which ask students to detail aspects of the course\nthey most\/least appreciate. A central unit, using scanners, data analysis and interpretation\nsoftware, processes the completed questionnaires. Part A data is processed into a set of\nnumerical results. Part B is checked, any abusive comments removed, and the remainder\npresented verbatim, under the question headings. Individualized reports are prepared for\neach member of staff and are delivered to them by hand. They are expected to follow up any\nissues arising and adjust their programmes as appropriate. Details about individuals are also\navailable to convenors (heads of departments) for management purposes, and play a part in\ndecision-making about staff at the end of their probationary period, and at promotion.\nTeachers are asked to arrange for the questionnaires to be circulated in class, and to\norganize for a student representative to collect the completed forms, and return them to the\ncentral office for processing. Student anonymity is guaranteed and staff receive reports\nonly on responses to items completed by five or more students.\nThe process - from sending questionnaires out to classes, to returning the results to\nindividual members of staff - takes around eight weeks. Two members of staff work full-\ntime on this task. Around 14,000 questionnaires are processed, with each student being\nexpected to fill out up to four questionnaires, one for each course attended.\nKey features of the paper-based system include:\n\u2022 a standardized approach, across the institution, achieving a consistently high response\nrate (of around 60 per cent);\n\u2022 contact with all students who turn up to classes when the questionnaire is being\nadministered (there are no attempts to verify class lists for this purpose - some students\nmay be absent);\n\u2022 time set aside in classes for completion of the questionnaire;\n19\nLiz Barnett et al On-line student feedback a pilot study\n\u2022 a high degree of anonymity for the students;\n\u2022 a process which is consistent over time, enabling individual members of staff to build\nup a picture of their teaching over several years;\n\u2022 a confidential system for staff, who are aware of who has access to their individual\ndata, and how that data may be used.\nThe disadvantages of the system are:\n\u2022 the high cost of operating the system, including the vast quantities of paper involved;\n\u2022 wasting class time on what is seen to be an essentially administrative task;\n\u2022 processing time, and consequent delay in feedback to staff;\n\u2022 the lack of 'ownership' by academic staff of the process, lessening its potential as a\ndevelopment tool.\nIn addition to these regular surveys of student opinion, the school undertakes more\ngeneral surveys of the student experience. The purpose and format of these surveys is quite\ndifferent from that of the annual course survey, but they provide a useful alternative for\ncomparison on response rates, as, like the on-line questionnaire, they are left open to the\nstudent to decide whether or not to respond - a very different scenario from the 'forced'\nsituation where students are expected to complete surveys in a classroom situation.\nIn looking at the development of an on-line alternative, the concern was to maintain the\npositive features of the existing system, whilst addressing some of the disadvantages.\nIssues to be addressed in developing an on-line system\nFrom the start, there was concern about the response rate - particularly given that results\non individual teachers are subsequently used as evidence to judge teaching quality at\nprobation\/promotion. Any reduction in overall response rate may be associated with a\nskewing of response (for example, a move away from a 'captive audience' may lead to\nresponse only from those who are dissatisfied, or possibly very enthusiastic, about a given\ncourse). In this context, worries were also expressed about differential response by course,\nbased on the assumption that students in some subject areas may be less technically literate\nthan others, and hence less likely to respond to an electronic questionnaire.\nThe other main area of concern was with data protection and anonymity. Would students\nfeel confident that their anonymity would be maintained, given that they are well aware\nthat email and Internet access can be monitored? On the other hand, might students\nattempt to 'sabotage' the process, for example, by multiple 'malicious' responses from a\nsingle person? On this point, staff would need assurances, given that the evidence collected\nis used in the context of their career progression.\nTrial One\nGiven some of the reservations expressed above, as well as lack of definitive evidence from\nelsewhere on the robustness of on-line course evaluation, a pilot study was set up, taking a\nlimited range of courses in a small selection of departments.\n20\nALT-J Volume 9 Number 3\nThree departments were selected (Government, Philosophy and Statistics) to give some\nindication as to whether different student groups have different IT skills\/preparedness to use\nan on-line format. Six modules were selected from each participating department and struc-\ntured randomization was used to allocate some to the control and some to the trial group.\nOver 1,000 potential respondents were included in the pilot, with.530 being sent the\nelectronic version (trial modules), and the rest being sent the traditional paper-based\nsystem (control modules). One difference with the on-line system was that circulation lists\nwere based on central records of course members, whilst the paper version went only to\nthose who happened to be in class on the appropriate day. These figures do not necessarily\ntally! Given that each student attends four or more courses each year, it was quite possible\nthat some students may have found themselves responding to one course using the paper-\nbased questionnaire, and another using the on-line format. The actual questions in the\npaper-based and on-line formats were the same, but the presentation was not. In the paper-\nbased version, students were presented with several questions per page. On-line, they were\nfaced with a single question per screen.\nStudents attending the control courses received the paper-based questionnaires following\nthe normal procedures. Class teachers (provided with an OHP describing the new system)\nalerted students attending the trial courses to the on-line questionnaire. Students were then\nindividually emailed with a message describing the on-line survey pilot, and giving them\ndirect access to the on-line site with a single mouse-click. They were sent a reminder three\nweeks later. Emails were monitored to see how many students opened the initial message,\nand how quickly it was accessed. Completed Web forms were also monitored to check that\neach student only responded once per course. A follow-up survey was later emailed to all\nstudents on the trial courses to gauge their reactions.\nTrial Two\nTrial Two used the experiences of Trial One to see if it would be possible to scale up the on-\nline survey without facing major technical difficulties. For Trial Two, 2,011 students from\ntwenty-four courses in seven departments received the on-line survey, along with 2,347\nstudents from twenty-two courses in the same selection of departments being included as a\n'control', receiving the paper-based survey. The additional departments were Law, Social\nPsychology, Information Systems and Economics. Trial Two continued to look at ways of\nincreasing response rate through improving the quality of presentation and ease of\ncompletion; increasing the number of 'reminder emails' sent to two (only sent to those who\nhad not replied), and testing out the use of small incentives. On the last point, we ran an\nautomated 'lottery', with students randomly allocated to different groups being told they\nmight win photocopying cards worth \u00a35 or \u00a310 if they returned the survey.\nKey questions addressed in evaluating the trials were:\n1. Do the response rates on-line compare favourably with the paper-based system? Is there\nany obvious difference in uptake by department\/student year group? Is there any evidence\nto suggest that incentives and reminders improve the response rate?\n2. How confident are students about the on-line version? Are concerns about confiden-\ntiality, anonymity and access to the data set addressed to a degree that ensures confidence?\n21\nUz Barrett et al On-line student feedback: a pilot study\n3. Has the system for monitoring responses (to ensure that only students registered for the\nmodule respond) worked, without allowing the identity of respondents to be revealed?\n4. Is it possible and useful to attempt to replicate the paper-based system on-line, or does the\nnew delivery method require fundamental changes to the structure\/form of the questionnaire?\n5. How costly of staff time and effort are the two systems?\nResults\nResponse rates\nIn Trial One, email monitoring indicated that 95 per cent of students contacted read the\ninitial message within two days of it being sent out. However, only 23 per cent went on to\ncomplete the questionnaire. In contrast about 60 per cent of paper questionnaires were\nreturned. There were no significant variations by discipline or by year of study.\nIn Trial Two, 80 per cent of the on-line group opened the initial email message (the\nremainder deleted without reading (7 per cent), or did not open the message). A total of 48\nper cent went on to open the survey applet, and 30 per cent completed and submitted the\nsurvey. The effects of the reminder were marked by peaking response rates in the two days\nfollowing the emails, but there was a fall off over time, and the second reminder 'peak' was\nsubstantially lower than the first. Given the study design, it was not possible to identify the\nrelative effects of improvements in the technical delivery of the questionnaire, improved\npublicity, the use of an extra reminder, and the introduction of incentives in raising the\nresponse rate from 23 per cent in the first trial to 30 per cent in the second.\nStudent and staff confidence\nIn Trial One, 45 students (8.5 per cent) who were sent the on-line survey replied to the\nfollow-up survey designed to elicit their reactions. Most of these (35) had actually\ncompleted the on-line survey. Respondents were generally appreciative of the experiment,\nbeing pleased that it saved class time, saved paper and was more private. There were\nconcerns about time taken to complete the on-line version. As each question is a separate\nscreen, it 'feels' much longer than the paper version. There were a few comments on\ntechnical difficulties, but this did not seem to be a major barrier to use. Fifteen claimed that\nthe briefing OHP had not been shown in class. Twenty-three students thought\narrangements for confidentiality were satisfactory but four still had some concerns.\nFor staff, the main concern lies with the poor response rate. Monitoring of response should\nbe able to weed out any people who try to abuse the system by replying several times on the\nsame course. There was no evidence of any attempts to do this on this occasion.\nAnother possible issue of concern for staff is that the low response rate using the on-line\nversion might lead to quite a different set of results as compared with the paper-based\nversion. For example, one might hypothesize that only the more motivated students would\nrespond to the on-line survey. These might include students who are very concerned and\/or\nstudents who are most enthusiastic about a given course, and this might lead to more extreme\nratings and greater deviation on scoring. At LSE, questionnaire scores play a part in\nprobation\/promotion decisions. Hence any systematic difference in scoring would have\nserious implications for staff confidence. To explore this issue, six questions were selected\nfrom the total questionnaire, and variations in response from the two groups compared.\n22\nALT-J Volume 9 Number 3\nQuestions on the library, overall course satisfaction, and the quality of lecturing showed no\nsystematic differences in response from the two groups. However, on class teaching there does\nappear to be a slight tendency for on-line respondents to give a less favourable response to\ntutors than those completing the paper questionnaire. Results here were consistent, but\nnumbers of respondents were too small to draw any firm conclusions. Anecdotal evidence\nfrom both staff and students suggests that in some cases students feel uncomfortable about\ncompleting questionnaires in the presence of class teachers; this might provide an\nexplanation for this effect, and is in line with other studies (see, for example, Wachtel, 1998:\n195, reference to Pulich who 'contends that even if the instructor leaves the room while the\nstudents are filling out the forms, some students may still be inhibited by the fact that the\ninstructor himself\/herself distributed them').\nReplicating the paper-based system\nIt was technically possible to replicate the paper-based questionnaire for the on-line trial.\nHowever, several respondents did find the format off-putting, as each question was on a\nseparate screen. The format was improved in Trial Two, but still maintained the match with\nthe paper-based version. With the agreement of the staff, it may be possible to move to a\ndifferent format for the on-line survey in the future, but this will require careful political\nnegotiation, given the sensitive nature of student evaluation of staff.\nThe move from paper to on-line presents a new set of options on customization, making it\neasier to design questionnaires more relevant to the needs of individual departments\nwithout at the same time seriously increasing costs of data processing and analysis.\nCosts\nThere are clearly costs in setting up any new system. However, the set-up costs for the on-\nline trials have been minor, compared with estimates for upgrading equipment\/software\nneeded for the paper-based system. There are some recurrent costs associated with the on-\nline survey, which do not apply to the paper-based system. These include sorting out class\nemail listings, and monitoring of response. However, current estimates suggest that this is a\nmuch smaller task than the major data input and data-cleaning requirements of the paper-\nbased system. With the latter, around 20 per cent of scanned data showed problems which\nrequired operator intervention.\nSpeed\nThe processing of the on-line version was very quick, thus enabling faster feedback to the\nteachers.\nDiscussion\nIn terms of cost, time, accuracy of data translation and speed, the on-line questionnaire is\nclearly better. Trial Two, involving many more students, demonstrated that technical issues\nin scaling up the process should not prove too problematic, although as yet, students\ncannot access the survey applet from outside the school network with ease (which may be\nanother barrier to response for some students).\nOn concerns about confidentiality of the data, and anonymity of respondents, it is\nimportant to be able to assure the integrity of the technical staff operating the system - but\nthis does not seem to be an issue of undue concern to students.\n23\nLiz Barnett et al On-line student feedback a pilot study\nThe key factor considered in the LSE on-line trial was response rate. On this point, the trial\nwas considered a failure. Anecdotal evidence from elsewhere suggests that motivation,\naccess and IT skill are the three main barriers to effective use of on-line feedback. Our\nresults tend to suggest that student IT literacy and access to computers do not appear to be\nmajor barriers, although issues of screen design and time taken to complete the\nquestionnaire will continue to be addressed in future trials.\nWhat of motivation? This perceived 'failure' requires further analysis. The high response\nrate of the paper-based version is in part due to it being delivered in a 'captive' situation in\nclass. What is more, it totally fails to gauge the opinions of students who, for whatever\nreason, are not in class on the relevant day. As such, it may be biased against those who\nhave ceased to attend because of problems with the quality and\/or content of the teaching.\nA more appropriate comparison would be to look at questionnaires sent out to students,\nrather than delivered through the 'captive' classroom setting. At LSE, a series of one-off\npostal surveys of the student experience achieved response rates of 30-48 per cent\n(Stockdale, 1991,1993,1993a, 1994,1996). The most recent postal survey of LSE students\nwas undertaken by MORI in spring 2000. This achieved a response rate of 33 per cent. At\nUCE, a centrally managed annual student satisfaction survey achieved a response rate of\njust over 50 per cent in 1992, but this declined to around 38 per cent in 1997. Various\nreports of postal surveys of distance learning students (for example, Macdonald-Ross and\nScott, 1997; Webb, 1992) indicate response rates of 24-33 per cent. Trial Two, with a\nresponse rate of 30 per cent, is still not up to the level achieved by most of these postal\nsurveys. However, one other feature of the on-line survey (and the chosen 'control') is the\nfrequency and volume of surveys of this nature that students are expected to complete.\nWork at UCE has raised concern about student 'questionnaire fatigue'. The UCE systems\nworks on a sampling basis, and students will only receive a maximum of one questionnaire\nper year (although one reason for the fatigue is the growing number of departments doing\ntheir own surveys). At LSE, each student faces around four surveys every year. At some\nstage, the issue of quality over quantity of response will need to be addressed. It may well\nbe that if the on-line survey is to be perceived as successful - in terms of improved response\nrate - then more consideration will need to be given to the content and detail in the survey,\nand to the purposes for which the data is used.\nFor now, we await a decision on whether to continue with the trials. If they do continue, it\nis likely that the next stage will again address the issue of 'scaling up' to ever greater\nnumbers of students, to check the impact that this may have on our computer system.\nFollowing this, we hope that it will be possible to revise the format, cutting down on the\nnumber of separate surveys students will be expected to complete (thus addressing the\nfatigue problem) and creating new survey designs more fitted to the on-line medium.\nUltimately, it will be important to get an appropriate balance between the demands of\nquality control and judgement centrally and the needs of staff for information and\nguidance on how to improve their teaching.\nReferences\nGalbraith, I , Gee, P. and Riley R. (1999), 'Interim report on the teaching quality assurance\ncommittee (TQAC) - pilot study in Lent Term 1999 of an on-line, Web-based, student\n24\nALT-J Volume 9 Number 3\ncourse evaluation questionnaire', Internal Report to the Teaching Quality Assurance\nCommittee in LSE.\nGreenwood, G. E. and Ramagli, H. J. (1980), 'Alternatives to student ratings of college\nteaching', Journal of Higher Education, 51 (6), 673-84.\nHusbands, C. (1997), 'Variations in students' evaluations of teachers' lecturing in different\ncourses on which they lecture: a study at the London School of Economics and Political\nScience', Higher Education, 33, 51-70.\nKerridge, J. R. and Mathews, B. P. (1998), 'Student rating of courses in HE: further\nchallenges and opportunities', Assessment and Evaluation in HE, 23 (1), 71-82.\nMacdonal-Ross, M. and Scott, B. (1997), 'A postal survey of OU students' reading skills',\nOpen Learning, 12 (2), 29-40.\nMarsh, H. W. (1987), 'Students' evaluation of university teaching', International Journal of\nEducational Research, 11 (3), 255-78.\nMarsh, H. W. and Dunkin, M. J. (1992), 'Students' evaluation of university teaching: a\nmultidimensional perspective', Higher Education: Handbook of Theory and Research, 8,\nNew York: Agathon Press, 143-233.\nPartington, P. (ed.) (1993), Student Feedback- Context Issues and Practice, USDU.\nStockdale, J. (ed.) (1991-6), a series of LSE internal studies of the student experience.\nTaylor, X, Woodman, M., Sumner, T. and Blake, C. T. (2000), 'Peering through a glass\ndarkly: integrative evaluation of an on-line course', Education, Technology and Society, 3\n(4), available online at http:llifets.ieee.org\/periodical\/vol_4_2000\/taylor.html.\nWachtel, H. K. (1998), 'Student evaluation of college teaching effectiveness: a brief\nreview', Assessment and Evaluation in Higher Education, 23 (2), 191-211.\nWebb, R. (1992), 'A new survey of NEC learners', Open Learning, 7 (1), 58-62.\n25\n"}