{"doi":"10.1016\/j.asoc.2010.07.003","coreId":"68016","oai":"oai:eprints.lancs.ac.uk:33824","identifiers":["oai:eprints.lancs.ac.uk:33824","10.1016\/j.asoc.2010.07.003"],"title":"Handling drifts and shifts in on-line data streams with evolving fuzzy systems.","authors":["Lughofer, Edwin","Angelov, Plamen"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2011-03","abstract":"In this paper, we present new approaches to handling drift and shift in on-line data streams with the help of evolving fuzzy systems (EFS), which are characterized by the fact that their structure (rule base and parameters) is not xed and not pre-determined, but is extracted from data streams on-line and in an incremental manner. When dealing with so-called drifts and shifts in data streams, one needs to take into account 1) automatic detection of drifts and shifts, and 2) automatic reaction to the drifts and shifts. This is important to avoid interruptions in the learning process and downtrends in predictive accuracy. To address the rst problem, we propose an approach based on the concept fuzzy rule age. The second problem is addressed by including gradual forgetting of 1.) antecedent parts and 2.) consequent parameters. The latter can be achieved by including a forgetting factor in the recursive local learning process of the parameters, whose value is automatically extracted based on the intensity of the shift\/drift. For addressing the former problem, we introduce two alternative methods: one is based on the evolving density-based clustering (eClustering) used to form the antecedents in the eTS approach; the other is based on the automatic adaptation of the learning rate of the evolving vector quantization (eVQ) method used to form the antecedent in the FLEXFIS approach. The paper concludes with an empirical evaluation of the impact of the proposed approaches in (on-line) real-world data sets in which drifts and shifts occur","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:33824<\/identifier><datestamp>\n      2018-01-24T03:01:52Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Handling drifts and shifts in on-line data streams with evolving fuzzy systems.<\/dc:title><dc:creator>\n        Lughofer, Edwin<\/dc:creator><dc:creator>\n        Angelov, Plamen<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        In this paper, we present new approaches to handling drift and shift in on-line data streams with the help of evolving fuzzy systems (EFS), which are characterized by the fact that their structure (rule base and parameters) is not xed and not pre-determined, but is extracted from data streams on-line and in an incremental manner. When dealing with so-called drifts and shifts in data streams, one needs to take into account 1) automatic detection of drifts and shifts, and 2) automatic reaction to the drifts and shifts. This is important to avoid interruptions in the learning process and downtrends in predictive accuracy. To address the rst problem, we propose an approach based on the concept fuzzy rule age. The second problem is addressed by including gradual forgetting of 1.) antecedent parts and 2.) consequent parameters. The latter can be achieved by including a forgetting factor in the recursive local learning process of the parameters, whose value is automatically extracted based on the intensity of the shift\/drift. For addressing the former problem, we introduce two alternative methods: one is based on the evolving density-based clustering (eClustering) used to form the antecedents in the eTS approach; the other is based on the automatic adaptation of the learning rate of the evolving vector quantization (eVQ) method used to form the antecedent in the FLEXFIS approach. The paper concludes with an empirical evaluation of the impact of the proposed approaches in (on-line) real-world data sets in which drifts and shifts occur.<\/dc:description><dc:date>\n        2011-03<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:relation>\n        http:\/\/dx.doi.org\/10.1016\/j.asoc.2010.07.003<\/dc:relation><dc:identifier>\n        Lughofer, Edwin and Angelov, Plamen (2011) Handling drifts and shifts in on-line data streams with evolving fuzzy systems. Applied Soft Computing, 11 (2). pp. 2057-2068. ISSN 1568-4946<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/33824\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":null,"relations":["http:\/\/dx.doi.org\/10.1016\/j.asoc.2010.07.003","http:\/\/eprints.lancs.ac.uk\/33824\/"],"year":2011,"topics":["QA75 Electronic computers. Computer science"],"subject":["Journal Article","PeerReviewed"],"fullText":"Handling Drifts and Shifts in On-Line Data\nStreams with Evolving Fuzzy Systems\nE. Lughofer a P. Angelov b\naDepartment of Knowledge-based Mathematical Systems, Johannes Kepler\nUniversity of Linz, A-4040 Linz, Austria\n(email: edwin.lughofer@jku.at)\nbIntelligent Systems Research Laboratory, Department of Communication Systems,\nInfoLab21, Lancaster University, LA1 4WA, United Kingdom\n(email: p.angelov@lancaster.ac.uk)\nAbstract\nIn this paper, we present new approaches to handling drift and shift in on-line data\nstreams with the help of evolving fuzzy systems (EFS), which are characterized\nby the fact that their structure (rule base and parameters) is not fixed and not\npre-determined, but is extracted from data streams on-line and in an incremental\nmanner. When dealing with so-called drifts and shifts in data streams, one needs to\ntake into account 1) automatic detection of drifts and shifts, and 2) automatic reac-\ntion to the drifts and shifts. This is important to avoid interruptions in the learning\nprocess and downtrends in predictive accuracy. To address the first problem, we\npropose an approach based on the concept fuzzy rule age. The second problem is\naddressed by including gradual forgetting of 1.) antecedent parts and 2.) consequent\nparameters. The latter can be achieved by including a forgetting factor in the recur-\nsive local learning process of the parameters, whose value is automatically extracted\nbased on the intensity of the shift\/drift. For addressing the former problem, we intro-\nduce two alternative methods: one is based on the evolving density-based clustering\n(eClustering) used to form the antecedents in the eTS approach; the other is based\non the automatic adaptation of the learning rate of the evolving vector quantization\n(eVQ) method used to form the antecedent in the FLEXFIS approach. The paper\nconcludes with an empirical evaluation of the impact of the proposed approaches in\n(on-line) real-world data sets in which drifts and shifts occur.\nKey words: drifts and shifts in data streams, evolving fuzzy systems, eTS,\nFLEXFIS, detection and reaction to drifts and shifts, age of a cluster\/fuzzy rule,\ngradual forgetting\nPreprint submitted to Elsevier Science 12 May 2010\n1 Introduction\n1.1 Motivation and State of the Art\nNowadays, data-driven fuzzy systems have become popular in many indus-\ntrial applications, as in contrast to expert-based fuzzy systems, they can be\ngenerated automatically from process data such as measurements, images, and\nsignal streams. Furthermore, they are proven universal approximators [46] (i.e.\nthey can model a given problem to any degree of accuracy \u2014 theoretically, at\nleast) that allow some insights in the form of linguistically [15] and visually\n[38] [16] interpretable rules.\nDuring the last decade, the field of \u2019evolving fuzzy systems\u2019 (EFS) has emerged\nas an important topic in fuzzy systems research 1 , as they are capable of in-\ncluding new information on demand and on-the-fly into models without neces-\nsarily using prior knowledge. EFS learn permanently from their environment,\nhence they may even be considered as a step towards true computational in-\ntelligence [6]. Furthermore, they are applicable in fast on-line identification\n[5] and modeling processes, and can be used with huge data bases which can-\nnot be loaded into the virtual memory all at once [31]. Often, off-line data\navailable in advance is simply not sufficient to build reliable models with high\npredictive quality; this may also necessitate the application of EFS as in [37].\nVarious EFS approaches have been established during the last years. One of\nthe most popular and pioneering approach among them is the eTS family,\nwhich comes with a regression [4] [5] and a classification variant [2] exploiting\nvarious fuzzy model architectures [8]. These were further advanced in [1] [9]\n[3]. Another approach inspired by the evolving vector quantization (eVQ) for\nevolution and adaptation of clusters [32] is the so-called FLEXFIS family [34],\nin particular FLEXFIS for regression [33] and for classification [35] (referred to\nFLEXFIS-Class). The range of other alternative approaches includes ePL [28]\n(evolving participatory learning, deduced from Yager\u2019s participatory learning\nparadigm [49]), SAFIS [22] (a sequential learning algorithm), and evolving\nfuzzy neural networks such as SOFNN [27] or GDFNN [48].\nAll these methods have in common that they are life-long learning approaches,\nwhich means that they incorporate all data samples into the fuzzy models with\nequal weights and in the same order as they are coming in during the on-line\nprocess. Hence, older and newer information are treated equally, and fuzzy\n1 Regular International Conferences take place, e.g. IEEE Symposia such as\nEFS\u201906, Ambleside, UK; GEFS\u201908, Witten-Bommerholz, Germany; ESDIS-2009,\nNashville, TN, USA, and numerous special sessions, tutorials and other activities\nthat are dedicated to this emerging topic\n2\nmodels assign equal importance to all the samples seen so far. On-line model\nidentification is advantageous, especially when convergence to an optimality\ncriterion or stable state of the model structure can be achieved [34]. However,\nthis only holds for data streams which are generated from the same under-\nlying data distribution and do not show any drift or shift behavior to other\nparts of the input\/output space [44]. Drift (respectively shift) indicates the\nnecessity of (gradual) out-dating of previously learned relationships (in terms\nof structure and parameters) during the incremental learning process as they\nare not valid any longer and should hence be eliminated from the model (for\ninstance, consider completely new types of images in a surface inspection sys-\ntem). An alternative to gradual out-dating is the concept of re-learning, which\ncan be either done based on all samples seen so far, providing lower weights\nfor older samples in the learning process, or based on the latest data blocks\nonly. The first variant slows down the learning process significantly over time,\nsuch that on-line real-time demands are hardly met. The second variant has\nthe problem that older data is usually completely forgotten when extracting\nthe models based on the new data blocks from scratch, causing a crisp switch\nbetween two models (from the old to the new). With gradual forgetting, a\nsmooth transition from an old model to a new one can be achieved instead\nof an abrupt switch. Drift handling (in connection with gradual forgetting)\nwas already applied in other machine learning techniques, e.g. in connection\nwith Support Vector Machines (SVMs) [25] [26], ensemble classifiers [39], and\ninstance-based (lazy) learning approaches [14] [18]. However, to the best of our\nknowledge, this concept has not yet been applied to fuzzy systems (neither\nthe concept of re-learning).\n1.2 Our Approach\nIn this paper, we propose approaches to handling drifts and shifts using the\nexample of EFS, incorporating eTS and FLEXFIS. However, parts of the con-\ncept, especially the detection of and reaction to drifts in the consequent parts\nof the rules, can be applied to a wider range of approaches. A more detailed\ndescription of drifts\/shifts and specific data examples is given in Section 2. To\ndevelop an automatic approach, we investigated the following two issues:\n(1) Detecting a drift\/shift (Section 3): This includes the tracking of changes in\nthe age of fuzzy rules during the on-line learning process. utility function\n[10] of fuzzy rules.\n(2) Reacting to a drift\/shift once it is detected (Section 4): 1.) in the an-\ntecedent parts of the rules, this is accomplished in eClustering (as applied\nin eTS) and in eVQ (as applied in FLEXFIS); and 2.) in the consequent\nparts of Takagi-Sugeno fuzzy systems (Section 4.2); this method is appli-\ncable to any approach exploiting TS fuzzy model architecture.\n3\nFig. 1. Work-flow of using evolving fuzzy systems in connection with drift\/shift\nhandling techniques as proposed in this paper and marked in bold font\nFigure 1 demonstrated an overview of the work-flow in an on-line modelling\nscenario, using evolving fuzzy systems connected with drift detection and re-\naction techniques as proposed in the subsequent section. The concepts marked\nwith text in bold font are novel ones treated in this paper.\nThe paper concludes with an empirical evaluation of the impact of the pro-\nposed approaches on the predictive accuracy of the evolving fuzzy models when\nused with (on-line) real-world data sets in which drifts and shifts occur (Sec-\ntion 5). This includes 1.) on-line data from rolling mills, where the resistance\nvalue was to be predicted with high accuracy for each measurement and where\ndifferent stitches may cause (slightly) different data distributions, indicating\ndrifts; 2.) on-line data from surface inspection systems for CD imprints (vari-\nous CD orders may indicate drift\/shift in the data) and egg production, and\n3.) and on-line data for modelling of the product composition in a distillation\ntower. Note that no benchmark data from the Internet or well-known data\nbases could be used for empirical evaluation, as these are usually all smooth,\ni.e. containing no drifts.\n2 Problem Statement\nIn machine learning literature, they distinguish between different types of\n\u2019concept change\u2019 of the underlying distribution of (on-line) data streams: a)\ndrifts, and b) shifts, see [44]. Drift refers to a gradual evolution of the concept\nover time. The concept drift concerns the way the data distribution slides\n4\nFig. 2. A drift in an evolving cluster (used for learning the antecedent parts of EFS):\nCircles represent the underlying data distribution before the drift, squares represent\nthe underlying data distribution after the drift; the big circles represent the cluster\ncenter of the original data (black), the center wrongly updated by a conventional\nlearning algorithm (middle-grey) and the correct center of the new data distribution\n(light-grey)\nsmoothly through the data\/feature space from one region to another. For\ninstance, one may consider a data cluster moving from one position to another.\nThis concept is closely related to the time-space representation of the data\nstreams. While the concept of (data) density is represented in the data space\ndomain, drift and shift are concepts in the joint data-time space domain. In\nmachine learning literature they also recognize so called \u2019virtual concept drift\u2019\nwhich represents a change in the distribution of the underlying data generating\nprocess instead of the change of the data concept itself [47]. Trends in time\nseries data denote a specific form of drifts, where for instance periodic patterns\n(such as sinusoidal-type wave-forms) are drifting (slightly) away from the x-\naxis. In the joint feature space, this may cause drifting regions or clusters\nas visualized in Figure 2 (an artificial example, in fact). There, the original\ndata distribution (in a 2-D data space) is marked by circular samples, which\nchanges over time into a data distribution marked by rectangular samples. If a\nconventional clustering process would be applied that weights all new incoming\nsamples equally, the cluster center would end up exactly in the middle of the\ncombined data cloud, averaging old and new data.\nTo address the drift problem (or trends) in data streams, a mechanism is to be\ntriggered once such a drift is detected. This may involve an incremental learn-\ning mechanism that assigns higher weights to new than to old data. This is\n5\nFig. 3. Example of a shift in the output variable; compare the dots (original data\ndistribution) with the crosses (data distribution after the shift) on the right-hand\nside of the graph\naccomplished by applying a gradual forgetting process in order to guarantee a\nsmooth transition from the old state (before the drift) to the new one (after the\ndrift) and is described in Sections 4.1.1 and 4.1.2. The shift concept refers to\nan abrupt change in the underlying concept to be learned. A shift in the input\nspace opens up a data cloud in an unexplored region and usually causes a new\nrule to be evolved by the evolving clustering algorithm. Such an occurrence\ncan be caused by seasonality in data-streams, i.e. changing patterns occurring\non different levels of one or more input features\/variables (for instance, CO2\nconcentrations are tendentially higher in spring than in autumn), usually ex-\ntending the input feature space (when seen as a joint high-dimensional space).\nIn our evolving fuzzy systems approaches, such occurrences are handled au-\ntomatically by extending the model to these new regions in the feature space\nthrough the evolution of a new (local) rule.\nIn Figure 3 a case of a specific shift, namely a shift in the output\/target\nvariable is shown, where an automatic evolution of new rules is not favorable:\nthe original trajectory of the 2-dimensional non-linear relationship (consisting\nof noisy samples) is indicated with dots, whereas the shift is represented by\nthe data samples marked as pluses forming a trajectory below the other one in\nthe middle and right part of the image. In case the consequent parameters are\nadapted in the usual way using weighted recursive least squares (as in [13]) and\nold data is not forgotten, then the approximation curve of the fuzzy model lies\nexactly between these two trajectories. The same is the case when evolving\nseparate rules for the shifted trajectory (causing two consequent functions,\n6\none lying over the other). Also, gradual forgetting is recommended here as a\nsmooth drift (where the surface slides softly from one position to another in\nvertical direction) may also occur instead of an abrupt shift. This implies the\nuse of an adaptive forgetting factor dependent on the intensity of the drift (see\nSection 4.2).\nIn this paper, we demonstrate novel approaches for autonomous drift and shift\ndetection and handling when using fuzzy rule-based systems of the Takagi-\nSugeno type to learn from on-line data streams in an evolving manner [5]. We\nfocus on EFS approaches exploiting the Takagi-Sugeno model architecture\n[43] but the drift detection and our technique for evolving the antecedent\nstructure of the fuzzy rule-based methods are applicable more generally to\nany type of fuzzy model framework. Therefore, without limiting the scope\nof the validity of the conclusions, we consider the following fuzzy rule-based\nmodel assuming that p input features, C fuzzy rules and Gaussian membership\nfunctions describe the fuzzy sets, which are connected by the product t-norm\nin the antecedent parts defined as:\nf\u02c6fuz(~x) = y\u02c6fuz =\nC\u2211\ni=1\nli(~x)\u03a8i(~x) (1)\nwith the normalized membership functions\n\u03a8i(~x) =\n\u00b5i(~x)\u2211C\nj=1 \u00b5j(~x)\n(2)\nand \u00b5i as rule fulfillment of the ith rule\n\u00b5i(~x) = exp\n\uf8eb\uf8ed\u22121\n2\np\u2211\nj=1\n(xj \u2212 cij)2\n\u03c32ij\n\uf8f6\uf8f8 (3)\nand consequent functions\nli(~x) = wi0 + wi1x1 + wi2x2 + ...+ wipxp (4)\nwhere cij is the center and \u03c3ij the width of the Gaussian function appearing\nas a fuzzy set in the j-th antecedent part of the i-th rule.\n3 Autonomous Detection of Drifts and Shifts in Data Streams Us-\ning EFS\nThis section describes the method for autonomous detection of shifts and\ndrifts in data streams based on the age [1] of the cluster\/fuzzy rule. This is an\nimportant step in the process of handling non-stationarity in data streams and\n7\nin building autonomous self-developing, self-learning, and evolving models and\nsystems. Such a technique has already been applied to evolving self-learning\nand self-developing classifiers [8]. Here, we present a more detailed analysis.\nDrifts occur when there is a significant difference between the distributions of\nthe old and new data. In [26], drift is detected off-line using an SVM method\n[45]. Since we use fuzzy rule-based systems and a neuro-fuzzy model framework\nwe need to develop an approach to 1.) autonomously detect drifts\/shifts in the\ndata concept on-line and 2.) to react on them in the system\/model structure\nevolution in a recursive manner.\n3.1 Detecting Drifts by the Age of Clusters (Rules)\nIn [1], the concept of cluster (respectively fuzzy rule) age was introduced, which\nis extended in this paper by including the \u03a8i,l as the membership degree of\nthe lth sample in the ith rule:\nagei = k \u2212\n\u2211ni\nl=1 Il\u03a8i,l\nni\n(5)\nwhere i is the rule index; ni denotes the support of rule i; Il denotes the time\ninstance at which the data sample was read; k is the current time instance.\nSince\n\u2211ni\nl=1 Il\u03a8i,l can also be accumulated recursively, the age can be calculated\neasily when necessary. The age of the cluster\/rule changes with each new data\nsample being read. If the sample does not fall into that cluster (does not\nsupport that fuzzy rule), resp. supports the ith fuzzy rule with a low or even 0\nvalue of \u03a8i,l, the age increases by the rate of one sample at a time. That means,\nif a cluster (fuzzy rule) is supported only slightly by any future samples after\nbeing initiated (say at time instance, ti), then its age at any time instance, k\nwill be approximately k \u2212 ti. However, if a new data sample (between ti and\nk) supports that fuzzy rule with a high value of \u03a8i,l, the age does not increase\nat the same rate, but at a smaller one; in other words, the cluster (fuzzy rule)\nis being refreshed.\nGenerally, the age of a cluster (fuzzy rule) has a range [0, k]. Figure 4 shows\na curve representing age evolution - a process called ageing. Rules that are\nolder are used and supported less by future data samples and become less\nimportant. The example depicted is based on a real application (propylene\nproduction data) and contains two drifts.\nThe cluster (fuzzy rule) age is important and closely linked to the data streams\n(which are sequences of data in time) and to the concept drift. We propose\nanalyzing the age of clusters (respectively, fuzzy rules) on-line by using both,\nthe gradient of the ageing curve and its second derivative, which indicate a\n8\nFig. 4. A typical example of the development of rule ages from a real production\nsystem in which shifts and drifts occur (as outlined by the markers in the plots)\nchange in the slope of the ageing curve. Indeed, the rate of ageing is equivalent\nto the gradient of the age variable. We can evaluate the average ageing rate\nby calculating the mean value of the age variables and can then compare the\ncurrent ageing rate to the mean ageing [10] to detect drift\/shift in the data\nstream autonomously and on-line. When the change in ageing, and hence also\nin the slope, is significant, then obviously the second derivative of the age\ncurve will be indicative of these inflection points. The right-hand side plot\nin Figure 5 shows an example in which the drift and activation phases are\nindicated in the right plot for a specific rule. In general, different rules may,\nof course, cause a different appearance of the corresponding age curves.\nPlease note that originally the concept of rule age was defined without includ-\ning the membership degree \u03a8, as used for deleting old rules from the rule base\n[1]. In case of smaller drifts as shown in Figure 2, the inclusion is necessary as\notherwise the rule (cluster) is still always attached with data samples from the\nnew distribution as still being the nearest cluster. Hence, no increase in the\nrule age curves would be visible. Instead, when using the membership degree,\nthe rule age (of the ith rule) will increase as the numerator in (5) decreases\ndue to the lower weights \u03a8i,l. In this sense, it is also possible to distinguish\nbetween drift and shift cases, as drifts will cause slight increases of the gradi-\nent in the rule age curves, and shifts more intense ones (as causing nearly 0\nvalues of \u03a8i,l). In case of shifts, usually new rule\/cluster centers are evolved\nnot disturbing already generated clusters and occupying the new situation\nor other clusters are attached (shift away from one cluster often triggers as\n9\nFig. 5. Left: Evolving process where no drift for a specific cluster\/rule occurs over\ntime (x-axis); right: evolving process where two drift phases and two confirmation\nphases occur for a specific rule.\nshift towards another cluster - as shown in Figure 4). In case of drifts the\nrule\/cluster centers are forced to stronger movements in order to help them\nout from the converged situation within the old data distribution and being\nable to follow the new one (recall Figure 2). In the case when there is a shift\nonly in the output variable, no new rule should be evolved, but the output\ntrajectory of the function shifted (recall Figure 3), which can be forced by\na forgetting approach in the rule consequent parameters (see Section 4.2).\nWe can detect shifts of the output variable by virtually excluding it in the\nclustering process and see whether the input features alone are triggering a\nshift: if this is not the case, but it happens when the output is included (as\nusually done in the incremental product space clustering methods in our EFS\napproaches, eClustering in eTS and eVQ in FLEXFIS), then obviously a shift\nin the output variable takes place.\nAn alternative approach for discriminating between drifts and shifts is simply\nby calculating the rule ages just over the number of samples belonging to\na certain rule (i.e. for which the rule was the nearest one) and not over all\nsamples seen so far. Then, an increase in the gradient of the age curves always\nindicates a drift; shift cases (in the output) have to be treated separately as\nmentioned above.\n4 Reactions by EFS to Drifts and Shifts in Data Streams\n4.1 Reaction to Drifts and Shifts in the Antecedents\nIn this section, we introduce methods for addressing both drifts and shifts in\ndata streams by adapting learning mechanisms for consequent parts and by\n10\nFig. 6. Shift in the time domain representation and two new rules evolved as a\nreaction to it.\nevolving antecedent parts. We demonstrate this using the example of the pop-\nular eTS method [5] and on FLEXFIS [33]. Whereas our method of reacting\nto drifts and shifts in the consequents can be applied to a broader range of\nEFS approaches (note that many of these approaches apply (weighted) re-\ncursive least squares (wRLS) learning methods), our approach to reacting to\nshift and drift in the antecedent parts is specifically tailored to the clustering\napproaches used - in our case these are eClustering [3] for eTS and eVQ [32]\nfor FLEXFIS.\n4.1.1 Reacting to Drifts and Shifts in Data Streams in eClustering\nIn eTS, shifts in data streams are detected quite naturally by recursive on-\nline calculation of the global data density\/potential\/mountain function [5] at\nthe current data sample, and comparing this to the data density at all focal\npoints (cluster centers) existing so far in the fuzzy rule base. Reaction to a\ndetected shift is either by a) forming a new rule around a new data sample,\nwhich becomes an attraction point for the global data distribution (also see\nFigure 6), or b) replacing a fuzzy rule which in itself consists of; i) forming a\nnew rule around the new point \u2014 as in the previous step, and ii) removing\nthe rule which has lower density and is close to this newly added one [2].\nWhen a shift in the data concept is detected, the respective fuzzy rules are\nbeing removed\/deleted or new ones are added. If locally optimal learning is\nbeing applied, then removing a cluster, and hence a locally valid Kalman\n11\nfilter\/RLS, does not affect the overall learning significantly (only through the\nfuzzy weight \u03a8 [5]). If globally optimal learning is being applied, removing a\ncluster (respectively fuzzy rules) affects n columns and n rows of the covariance\nmatrix directly and the remaining values of the covariance matrix indirectly\n[5]. A significant delay in reacting on drifts may arise as an evolution\/re-\nsetting of an old cluster center requires a higher density (number) of samples\nappearing in a new data cloud than in an already existing one. This means,\nthat in the case when the data distribution slides from an old (heavy) local\nregion to another, it may take quite a long time that a sample in the new\ndata distribution has a higher potential than the older cluster center. Hence,\nin the case of a detected drift, the potential of this cluster center where the\ndrift occurs is re-set to a small value, forcing an earlier evolution\/re-setting of\ncenters after the detection of the drift.\n4.1.2 Reacting to Drifts and Shifts in eVQ\nWhen shifts occur in the data stream, new clusters are usually evolved auto-\nmatically in eVQ, because shifts cause new data clouds in explored regions,\nat least in regions further away than a fraction of the space diagonal in the\ninput space (to which the vigilance parameter \u03c1 is usually set, i.e. 0.1 to 0.3\nof the space diagonal, see [32]).\nWe define the tracking concepts for the ith rule throughout this section, which\ncan be generalized to any rule in a straightforward manner. To react to drift\nin the antecedent parts of the rules for FLEXFIS, we propose re-adjusting\nthe parameter in the eVQ clustering algorithm [32] that steers the learning\ngain and hence the convergence of the centers and surfaces of the cluster over\ntime (with more samples loaded), namely \u03b7. This convergence is according to\na specific adjustment of the learning gain in the same way as also exploited\nin conventional VQ, which was proven to converge to the k-means solution\n[21] obtained by solving a least squares optimization problem between data\nsamples and nearest centers [20]; following this approach, we have for the ith\ncluster:\n\u03b7i =\n0.5\nni\n(6)\nwhere ni the number of samples forming the cluster (i.e. for which the cluster\nwas the nearest one) in the past; therefore, for different clusters we obtain\ndifferent (decreasing) learning gains. The is applied in the update of the pro-\ntotype ci of the ith cluster in the following way:\n~c\n(new)\ni = ~c\n(old)\ni + \u03b7i(~x\u2212 ~c(old)i ) (7)\nThus, the old center ~c\n(old)\ni is moved towards the current sample (~x) by a frac-\ntion of the distance between the current sample and its center coordinates,\nachieving a new center ~c\n(new)\ni . Considering equation (6), it becomes clear that\n12\nthe learning gain decreases with the number of samples forming the ith rule\n(ni), which is a favorable characteristic in order to converge to 1.) the center of\na local region (data cloud) [32] and 2.) to optimality in the least squares sense\nwhen updating consequent parameters in the Takagi-Sugeno fuzzy systems\n[31]. If the learning gain would not be decreased, fluctuation of the cluster\ncenters and therefore of the antecedent parts of the rules would ensue. Hence,\nthe whole EFS approach would become unstable\/lose stability. However, if\na drift occurs in a data stream, the effect described above is no longer de-\nsirable, because center(s) and width(s) of the cluster(s) should adapt to the\nnew data distribution (as shown in Figure 2). Unlike a shift, a drift usually\ndoes not trigger a new cluster, just indicates a stronger movement of already\nexisting clusters as expected. To re-activate the converged clusters, i.e. re-\nanimate them for stronger movements in a drifting case, we force a sudden\nincrease of the learning gain for the first sample in the drift phase, followed by\na gradual decrease for the next samples in order to balance in the new sample\ndistribution in the same manner as is done for original ones.\n4.1.2.1 Drift Tracking with Abrupt Increase of the Learning Gain\nFollowed by Gradual Decrease Here, we propose the following mecha-\nnisms for the learning gain \u03b7: first, we transform the forgetting factor \u03bb, which\nis used for gradual out-weighting in consequent learning (see (25)), and assign\na value in [0, 1] to the intensity of a drift. We map 0.9 (the minimal value for\n\u03bb) to 0.99, while 1 is mapped to 0. Hence, we obtain:\n\u03bb trans = \u22129.9\u03bb+ 9.9 (8)\nWhen a drift occurs, we then re-set the number of samples forming the ith\ncluster (ni) (used in the denominator of the calculation of \u03b7i) according to\nni = ni \u2212 ni \u2217 \u03bb trans (9)\nThis means that the stronger the drift is, the more ni is decreased and hence\nthe stronger the forgetting effect will be. Figure 7 shows how \u03b7i develops\n(lines) in usual (i.e., non-drift) case (for the first 100 samples). Then, a drifting\nscenario was triggered at three intensity levels, leading to three \u03bb values. The\ndevelopment curves of \u03b7i in these three cases are shown as a solid line, dashed\nline and dashed dotted line. After the drift indicator (at sample 100), the\nlearning gain is decreased in usual way allowing the center to converge to the\nnew data distribution (in the same manner as is the case in eVQ using (6) for\nconvergence reasons, see explanation above).\nThis resetting of ni to a significantly lower value also affects the update of the\nranges of influence of the clusters, as these are estimated using the recursive\n13\nFig. 7. Abrupt increase in the learning gain \u03b7 in case of a drift (after 100 samples)\nwhen using different values for the forgetting factor \u03bb (indicated by different line\nstyles).\nvariance formula dimension-wise (the variance of the data samples forming a\ncluster is calculated separately for each dimension):\n\u03c32i,j(new) =\nni \u2212 1\nni\n\u03c32i,j(old) + \u2206c\n2\ni,j +\n1\nni\n(ci,j \u2212 xj)2 \u2200j = 1, ..., p+ 1 (10)\nwith p as the number of inputs (note that we perform a product-space cluster-\ning, so inputs and output feature in a joint space), \u03c32i,j(old) the old variance\nof the ith cluster with respect to the jth dimension, ci,j the center of the ith\ncluster in the jth dimension, xj the jth dimension in the new data sample and\n\u2206ci,j the difference of the old cluster center to the new one (after updating\nwith (7)) with respect to dimension j. The ranges of influence are needed to\nobtain the widths \u03c3i,j of the Gaussian fuzzy sets as used in the Takagi-Sugeno\nfuzzy systems, see (3). Obviously, when ni is reset to a low value according to\n(9), the update of \u03c3i,j is revived because 1.) the influence of \u03c3\n2\ni,j(old) decreases\nsignificantly (the smaller ni is, the lower\nni\u22121\nni\ngets) and 2.) the deviation of\nthe new sample to the center (last term in formula) gets significantly higher\naccording to (7). This means that new samples in the new data distribution\nare represented in \u03c3i,j with a much stronger influence than the older ones in\nthe old data distribution, thus achieving a forgetting effect.\nFigure 8 depicts a two-dimensional clustering example with a data drift from\none local region (data cloud in the left image, samples indicated as dots) to\na neighboring one (additional cloud in the right image, samples indicated as\npluses). In the upper row the trajectories of cluster centers (big dark dots) and\n14\nFig. 8. Left: original data cloud; right: drifted data cloud indicated by crosses; the\nupper row shows the trajectory of cluster movements without abrupt decrease in\nthe learning gain (no forgetting), the lower row with abrupt decrease (forgetting)\nthe range of influence (ellipsoid in solid line) of the final cluster is shown for\nconventional adaptation of cluster centers without forgetting. The bottom two\nimages in Figure 8 illustrate the analogous case using forgetting as described\nabove. Note, in this case, the bigger jump from the converged cluster center (as\nshown left image) to the new data distribution is bigger, forcing a new compact\ncluster for the new data distribution to emerge (marked with crosses). Without\nforgetting, an incorrect big joint cluster is created, because all samples are\nweighted equally (see right image, upper row).\n4.2 Reaction to Drifts and Shifts in the Consequents\nFor the rule consequents, drifts and shifts can be handled in one sweep, since\nonly an approximate setting of the forgetting factor value is required as de-\nscribed below. Whenever a drift in the output variable occurs (see Figure 3)\nand is detected by the approach described in Section 3, it is necessary to ap-\nply a specific mechanism to the sample-wise incremental learning steps of the\n15\nFig. 9. Left: The adapted model (dotted line) and the new incoming samples (dark\ndots) when applying a conventional recursive weighted least squares approach as\ndefined in (11)-(13); right: the adapted model (dotted line) when including a forget-\nting factor of 0.95 and using (21)-(23); the approximation surface coincides exactly\nwith the trajectory of the data samples representing the new data distribution\nconsequent parameters in Takagi-Sugeno fuzzy systems as defined in (1).\nThe usual incremental learning approach exploits local learning of the conse-\nquent functions (which has some advantages over global learning, particularly\nproviding much more flexibility for adjoining and deleting rules on demand,\nsee [5] [10] [31] [8]) and defines a recursive fuzzily weighted update scheme, in\nwhich the membership degrees of the fuzzy rules serve as weights. For the ith\nrule, the update scheme is defined in the following way:\n~\u02c6wi(k + 1) = ~\u02c6wi(k) + \u03b3(k)(y(k + 1)\u2212 ~rT (k + 1) ~\u02c6wi(k)) (11)\n\u03b3(k) =\nPi(k)~r(k + 1)\n1\n\u03a8i(~x(k+1))\n+ ~rT (k + 1)Pi(k)~r(k + 1)\n(12)\nPi(k + 1) = (I \u2212 \u03b3(k)~rT (k + 1))Pi(k) (13)\nwith Pi(k) = (Ri(k)\nTQi(k)Ri(k))\n\u22121 the inverse Hessian matrix at the previous\ntime instance k, ~r(k+1) = [1 x1(k+1) x2(k+1) . . . xp(k+1)]\nT the regressor\nvalues of the k + 1th data sample, and \u03a8i as the fulfillment degree of the ith\nrule, serving as weight in the recursive least squares algorithm, y(k + 1) the\nmeasured value of the output (target) variable in the new time instance k+ 1\nand ~\u02c6wi(k) the consequent parameter vector of the ith rule as defined in (4)\nestimated by the first k samples.\nThis approach indeed includes different weights for different samples (see the\n\u03a8i in the denominator of (12)), but rather than being motivated by a changing\ncharacteristics of the sample distribution over time, this relates to the local\nposition of the samples with respect to the rules. This means that all newer\nsamples at the same local positions relative to the rules as the older samples\nare included with the same rule weights in the update process. For the drift\nproblem as shown in Figure 3, the wRLS estimation hence results in a fuzzy\n16\nmodel approximation curve as shown in the left plot of Figure 9 with dotted\nlines. Of course, the approximation lies precisely between the two trajectories,\nsince it seeks to minimize the quadratic errors (least squares) of all samples to\nthe curve. Hence, it is necessary to include a parameter in the update process,\nwhich forces older samples to be out-dated over time. Gradualism is important\nhere in order to guarantee a smooth forgetting and to prevent abrupt changes\nin the approximation surface. For this purpose, we re-define the least squares\noptimization function for the ith rule by\nJi =\nN\u2211\nk=1\n\u03bbN\u2212k\u03a8i(~x(k))e2i (k) \u2212\u2192 min\n~w\n(14)\nwith ei(k) = y(k)\u2212 y\u02c6(k) the error of the ith rule in sample k and \u03bb a forgetting\nfactor. Assuming that N is the number of samples loaded so far, this function\noutdates the sample processed i steps before by \u03bbN\u2212k. Common values of \u03bb\nlie between 0.9 and 1, where a value near 1 means slow forgetting and a value\nnear 0.9 means fast forgetting of previously loaded data samples. The exact\nchoice depends strongly on the behavior of the drift (see below). From (14),\nit is quite clear that the weighting matrix for rule i becomes\nQi =\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u03bbN\u22121\u03a8i(~x(1)) 0 ... 0\n0 \u03bbN\u22122\u03a8i(~x(2)) ... 0\n...\n...\n...\n...\n0 0 ... \u03bb0\u03a8i(~x(N))\n\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\nwith N the number of data samples and \u03a8i(~x(j) the membership degree of\nsample j to the ith rule. Let the weighted error vector for the ith rule at time\ninstance k be defined as\nek,i = [e(1)\n\u221a\n\u03bb\nk\u22121\u221a\n\u03a81,i . . . e(k)\n\u221a\n\u03a8k,i]\nT (15)\nwith \u03a8k,i = \u03a8i(~x(k)) and the objective function Ji for the ith rule at time\ninstance k defined as\nJk,i = e\nT\nk,iek,i (16)\nFor the (k + 1)-th data set, we define:\nJk+1,i =\nk+1\u2211\nj=1\n\u03bbk+1\u2212j\u03a8j,ie2i (j) (17)\nrespectively:\n17\nJk+1,i =\u03bb\nk\u2211\nj=1\n\u03bbk\u2212j\u03a8j,ie2i (j) + \u03a8k+1,ie\n2\ni (k + 1) = (18)\n\u03bbeTk,iek,i + e\n2\ni (k + 1)\nor\nJk+1,i =\n\uf8eb\uf8ec\uf8ed\n\u221a\n\u03bb\u03a8k,iek,i\n\u03a8k+1,iei(k + 1)\n\uf8f6\uf8f7\uf8f8\nT \uf8eb\uf8ec\uf8ed\n\u221a\n\u03bb\u03a8k,iek,i\n\u03a8k+1,iei(k + 1)\n\uf8f6\uf8f7\uf8f8 (19)\nRecall the conventional LS estimator at time instance k, which minimizes\nJk,i =\n\u2211k\nj=1 \u03a8j,ie\n2\ni (j), i.e. ~\u02c6wi(k) = (Ri(k)\nTQi(k)Ri(k))\n\u22121Ri(k)TQi(k)y(k) [36].\nThe modified wLS solution with forgetting at step k + 1 is:\n~\u02c6wi(k + 1) =\uf8eb\uf8ec\uf8ec\uf8ed\n\uf8eb\uf8ec\uf8ed\n\u221a\n\u03bb\n\u221a\nQi(k)Ri(k)\u221a\nQi(k + 1)~ri\nT (k + 1)\n\uf8f6\uf8f7\uf8f8\nT \uf8eb\uf8ec\uf8ed\n\u221a\n\u03bb\n\u221a\nQi(k)Ri(k)\u221a\n\u03a8i(k + 1)~ri\nT (k + 1)\n\uf8f6\uf8f7\uf8f8\n\uf8f6\uf8f7\uf8f7\uf8f8\n\u22121\n(20)\n\uf8eb\uf8ec\uf8ed\n\u221a\n\u03bbQi(k)Ri(k)\n\u03a8i(k + 1)~ri\nT (k + 1)\n\uf8f6\uf8f7\uf8f8\nT \uf8eb\uf8ec\uf8ed\n\u221a\n\u03bb~y(k)\ny(k + 1)\n\uf8f6\uf8f7\uf8f8\nFollowing the same recursive deduction scheme as in conventional wLS [29]\n(defining P (k+1) as the inverse matrix in (20), exploiting the fact that Pi(k+\n1) = (P\u22121i (k) + ~ri(k + 1)\u03a8i(k + 1)~ri\nT (k + 1))\u22121, and applying the matrix\ninversion theorem, also known as Sherman-Morrison formula [41]), we obtain\nthe following incremental update formulas:\n~\u02c6wi(k + 1) = ~\u02c6wi(k) + \u03b3(k)(y(k + 1)\u2212 ~rT (k + 1) ~\u02c6wi(k)) (21)\n\u03b3(k) =\nPi(k)~r(k + 1)\n\u03bb\n\u03a8i(~x(k+1))\n+ ~rT (k + 1)Pi(k)~r(k + 1)\n(22)\nPi(k + 1) = (I \u2212 \u03b3(k)~rT (k + 1))Pi(k) 1\n\u03bb\n(23)\nNow, the final question is how to set the parameter \u03bb in order to guarantee\nan appropriate drift tracking. Two possible solutions exist:\n(1) Define a fixed parameter value of \u03bb in [0,1] according to some prior knowl-\nedge about how strong the drifts might become.\n(2) Introduce a variable forgetting factor, which depends on the intensity\/speed\nof the drift.\n18\nFor the latter, methods exist that include the gradient LS error with respect\nto \u03bb to track the dynamic properties (see, e.g. [42]). In this paper, we propose\na strategy to deduce it directly from the age curves analysis (Section 3.1),\nbecause it provides the degree of change in the rules ages, which indicates the\nspeed of the drift (see Section 3). In Section 3, we mentioned that the age of\na rule always lies in [0, k]. Hence, we normalize the age of the ith rule to [0, 1]\nby\nagei norm =\nagei\nk\n(24)\nin order to achieve gradients of the normalized rule ages \u2206agei norm also\nlying in [0, 1]. Based on the figures in Section 3, we explained that a sudden\nincrease in the gradient of the rule age curves indicates a drift in the data\nstream. This means, whenever the change of the gradient is significant, wRLS\nwith forgetting should be triggered. We use the following estimation for \u03bb:\n\u03bb = 1\u2212 0.1\u22062agei norm (25)\nwith \u22062agei norm the second deviation of the age (curve) of the ith rule.\nThis guarantees a value of \u03bb between 0.9 (strong forgetting) and 1 (no forget-\nting), according to the degree of the gradient change (1 = maximal change,\n0 = no change) (note: to the best of our knowledge, a value smaller than\n0.9 produces instabilities in the models). The forgetting factor is then kept\nat this level for a while, as otherwise a single sample would cause a gradual\nforgetting. We reset \u03bb to 1, when a stable gradient phase is achieved (usually\nafter around 20 to 30 samples with moderate \u22062agei norm values). Resetting\n\u03bb to 1 is necessary to prevent forgetting from continuing in the new data dis-\ntribution. This causes the drift phase in the antecedents to stop. In the case\nof a shift, we always set \u03bb to the minimal value of 0.9. Another important\nissue is concerned with the consequences of over-detecting drifts or shifts. In\nthis case, older samples are forgotten and the significance of the parameters\nand the whole model decreases. However, the surface of the model stays at\nthe correct (old) position in the detected drift\/shift region, since the samples\ndo not move (significantly). In other regions the update will be quite small.\nHowever, it may become significant with more samples loaded, since all rules\nalways fire to a small degree (as Gaussian fuzzy sets possess infinite support).\nThis could cause an unlearning effect [30]. Hence, keeping the drift phase low\nat 20 to 30 samples is very important.\n5 Evaluation\nThis section deals with the evaluation of the impact of reacting to drifts and\nshifts in the case of data streams in which drifts and shifts actually occur. This\nwas done by implementing the approaches discussed throughout this paper in\n19\nthe two EFS approaches (eTS and FLEXFIS). One application example was\nconcerned with identifying a prediction model on-line for the resistance value\nof steel plate in a rolling mill. The second came from a surface inspection\nsystem supervising CD imprints and eggs with respect to faults. The third\napplication example came from the chemical industry, where eTS was applied\nfor modeling and predicting the chemical product composition by modelling\nthe product composition in a distillation tower.\n5.1 On-Line Prediction Models for Rolling Mills\nThe task was to identify a prediction model for the resistance value of a steel\nplate in a rolling mill. In a first step, we used some off-line (pre-collected)\nmeasurement data in order to obtain a rough idea about the quality the fuzzy\nmodel achieved. Then we refined the prediction model with newly recorded on-\nline data. The second step was possible, because first a prediction for the resis-\ntance was provided, influencing the whole process of the rolling mill, whereas\na few seconds later (after passing the steel plate), the real value of the re-\nsistance was measured, which was then incorporated into the model adapta-\ntion process. Thus, not the predicted (and maybe false) value was used for\nfurther adaptation, but the correct, measured value; hence, the method can\ntherefore be considered as a potential procedure for improving the models.\nThe initial situation was as follows: an analytical model in which some pa-\nrameters were estimated through linear regression and should be improved\nby the fuzzy model component FLEXFIS, which was applied purely on the\nbasis of measurement data. The analytical model is physically motivated by\nmaterial laws formulated by Spittel and Hensel [23]. Using a modified form\nthese laws, the resistance value can be estimated by an exponential function\nof the temperature, speed and thickness of the steel plate. We achieved a sig-\nnificant improvement in the predictive power of the analytical models using\nfuzzy models trained with FLEXFIS in off-line batch mode. The data set for\ntraining and cross-validation comprised 6000 samples, and 6600 test samples\nwere used to estimate the predictive power for new on-line samples. A further\nimprovement could be achieved by updating the fuzzy models during on-line\noperation mode. For details of the experimental setup and results see [33] (and\nalso Table 1, which summarizes all the results).\nSubsequently, we wanted to examine whether reacting to drifts by gradually\nforgetting older samples during the on-line process further improves the qual-\nity of the models. The application of a drift reaction method was justified by\nthe fact that the operation process of rolling mills is divided into different\n\u201dstitches\u201d. One stitch represents one closed cycle in the rolling process. In on-\nline mode, the measurements are taken continuously from stitch to stitch. For\nthe current stitch, the previous stitch should be of little or even no importance.\n20\nTable 1\nComparison of evolving fuzzy prediction models obtained by conventional FLEXFIS\nand when applying gradual forgetting as outlined in Sections 4.1 and 4.2\nMethod MAE Max MAE Too High \/\nMax MAE Too Low \/\n# MAEs > 20\nAnalytical 7.84 63.47 \/ 87.37 \/ 259\nStatic fuzzy models 6.76 45.05 \/ 81.65 \/ 176\nFLEXFIS 5.41 38.05 \/ 78.88 \/ 159\nFLEXFIS with grad. forgetting 4.65 31.99 \/ 74 \/ 68\nHowever, the measurements from the previous stitch are already included in\nthe fuzzy models as they are being updated by the current samples. This\nmeans that older samples from the previous stitch should be forgotten when\nincluding samples from the current stitch. Furthermore, no drift\/shift detec-\ntion is needed, as the drift\/shift is indicated by the beginning of a new stitch.\nThis start signal was transferred to the computer and was also included in the\nstored measurement data. As no drift detection was carried out, \u03bb was set to\n0.97, which is a good compromise between fast forgetting (=strong locality\nof models) and slow forgetting (=weak locality of models). The results are\nsummarized in Table 1. It shows that both the static and the evolving fuzzy\nmodel outperformed the analytical model in predictive accuracy. Three differ-\nent types of errors were reported: 1.) the mean absolute error over all won-line\nsamples (note that first a prediction was made and then the model updated\nwith the same samples and based on feedback), 2.) the number of mean ab-\nsolute error (MAE) greater than 20, 3.) the mean MAE over all samples for\nwhich the prediction was too low, and 4.) the mean MAE over all samples\nwhere the prediction was too high. The last value is the most important one\nas it implies that the steel plate may get damaged, whereas predicting too low\nvalues has no such detrimental effects. The essential row in Table 1 is the last\none, which shows the impact of gradual forgetting when triggered at each new\nstitch \u2014 compare to FLEXFIS without the forgetting procedure: the number\nof predictions for which the errors were too high could be reduced by more\nthan 50% and the maximal error was lowered to almost 31.99, which was quite\nbeneficial. Also, the advantage of evolving the fuzzy models during the on-line\nprocess (four more rules) is clearly evident in Table 1 (compare the second\nwith the third and the fourth row).\nAnother interesting aspect is that the error of individual measurements starts\nto drift over time when gradual forgetting is not applied. This is visualized in\nthe left image of Figure 10, which shows the individual errors over the 6600\non-line samples: note the drift of the main error area away from the zero line\n21\nat the end of the data stream. The right-hand side plot shows the individual\nerrors when applying gradual forgetting: the drift occurrence at the end of the\nplot could be eliminated.\nFig. 10. Left: The error curve for the 6600 on-line samples when no forgetting is\napplied: at the end the error starts drifting as the body area around zero increases\nto a value around 10 to 15; right: no drift in case of gradual forgetting as applied\nat the beginning of each stitch.\n5.2 Surface Inspection of CD Imprints and Eggs\nThe task was to automatically detect errors in CD imprints to sort out bad\nones during the production process. Errors include color drift during offset\nprint, a pinhole caused by a dirty sieve (color cannot pass through), occur-\nrence of colors on dirt, and palettes running out of ink. For this purpose, a\ncamera was installed directly above the conveyor belt over the trays which\nrecord the CDs. The images were then processed further using an on-line im-\nage classification framework [40] that was setup for an EU-Project (DynaVis\n\u2014 see www.dynavis.org), and classifies the samples into good and bad ones.\nA fault-free master was used to obtain a contrast image in which potential\nCD imprint faults could be identified. The most difficult challenge was to dis-\ntinguish between real errors and pseudo-errors in these imprints (a pseudo\nerror, for example, a small shift of the CD in the tray that causes an arc-type\nor circular deviation region in the contrast image). The contrast image was\nprocessed further by extracting a high-dimensional feature set characterizing\nthe deviation pixels in these images. Based on the feature set extracted from\na pre-labelled training data set, various machine learning methods (CART,\nC4.5, k-NN, SVMs, eVQ-Class, ...) were used to train classifiers, and also\nfuzzy classifiers with the help of FLEXFIS-Class, the classification variant of\nFLEXFIS (see [40] for a comparison of methods) were employed.\nThis had to be further refined with on-line data samples, as the initial setup\n22\nFig. 11. Drift analysis of the CD Imprint data labeled by Operator 2, applying\ntwo different speeds in the gradual forgetting process and starting the \u2019drift phase\ntracking\u2019 at different sample numbers (x-axis); the y-axis shows the accuracy for\nthe separate test set.\nwas quite time-consuming for the experts. Especially, the labeling process in-\nvolved great workload effort, as the contrast images had to be labeled, in\nsome cases even single objects in the images instead of complete images in\norder to guarantee high-performance classifiers [37]. This means that only a\nfew training samples were labeled, which may cause over-fitting of the ini-\ntial (off-line trained) classifiers, especially in high-dimensional cases. Hence,\nrefining with more data during on-line mode is mandatory. For a set of 512\non-line samples, the conventional update scheme in FLEXFIS-Class achieved\nclassification rates of 89.92%, increasing the classification rate by around 4%\ncompared to static (initially built and not further adapted) classifiers.\nSubsequently, we studied the impact of gradual forgetting in the antecedent\nand consequent part of the fuzzy models. It is important to note that FLEXFIS-\nClass evolves K Takagi-Sugeno fuzzy regression (sub-)models as defined in (1)\nfor K different classes. Hence, the approaches proposed in this paper can be\ndirectly applied to all sub-models. The the importance of examining gradual\nforgetting is underlined by the fact, that the CD imprint data was originally\nrecorded for different orders, where different orders may contain drifts or shifts\nin the image data distribution. We performed a drift analysis with different\nstarting points (beginning with gradual evolution at 0, 50, 100, 150, ..., 500\non-line samples), with drift phases encompassing 30 samples (as suggested in\nSection 4.2) and with two different speeds for gradual forgetting. The results\nare shown in Figure 11, where the slow gradual forgetting is denoted by the\ndotted-dashed line and the fast gradual forgetting by the solid line. The sample\nnumber in the second half of the training set at which the drift is initialized is\nrepresented by the x-axis, the achieved accuracies on separate test data when\n23\nFig. 12. Rule age curves for five rules evolved for the egg data set, containing 4238\nsamples and 17 input features, parts with significantly increasing gradients (drifts)\nmarked by ellipses\nincluding the gradual forgetting on the rest of the second half of the training\ndata is represented by the y-axis. In fact, the accuracies were boosted up by\nabout 3% (to 91.39% maximum) when initiating a drift phase at the correct\nsample number and choosing the appropriate speed in the gradual forgetting\nprocess. In our case, this means that a slow forgetting starting between sam-\nples 100 and 150 and fast forgetting starting at sample 350 is beneficial, as\nthis tracks two different types of drift. In fact, combining these two results in\na classification rate of over 92%, gaining another 1% accuracy.\nRegarding the surface inspection scenario for eggs on the conveyor belt, the\ntask was basically to distinguish between dirt and yolk occurrences, as both\nare causing significant entries in the deviation image (obtained by subtract-\ning newly recorded image with a master). For our fuzzy classifiers evolved\nby FLEXFIS-Class (without drift detection and reaction), it was possible to\nachieve a classification rate of about 83%, which could be further increased to\naround 90%, when applying rule ages for drift detection and gradual forget-\nting of antecedents as explained in Section 4.1.2 for reacting onto the drift.\nThe rule age curves of five rules evolved during the incremental learning phase\nare shown in Figure 12, the positions where drifts were detected indicated by\nellipses.\n24\nFig. 13. Comparison of the predicted and real product quality data. Solid line -\npredictions by eTS with shift and drift detection; circles - real data.\n5.3 Modeling the Product Composition in a Distillation Tower\nA case study based on real data (courtesy of Dr. Arthur Kordon, The Dow\nChemical Co.) from the chemical industry [7] was used to illustrate of the\ndetection of and reaction to drift and shift in eTS. The eTS training method\n[9] was applied to model and predict the chemical product composition in a\ndistillation tower. The traditional technique for estimating the various chemi-\ncal properties of the compositions is based on off-line manual and cost-related\nlaboratory analysis of the properties based on grab samples using gas chro-\nmatographs. The sampling period in the case of laboratory analysis is 8 hours\nand the accuracy is 2.2% measurement error ([7]). The data set included a\nchange in the operating regime of the process, which brings a challenge im-\nposed on the structure of the model (fuzzy rule based system). The data set\nalso incorporated a number of other challenges, such as noise in the data, and\na large number of initial variables. These problems cover a wide range of real\nissues in the industry. Process data retrieved from physical (\u2019hard\u2019) sensors\nwas used as inputs to the eTS, applying hourly averages for every eight hour\nperiod. The product composition (real output) was estimated by laboratory\nanalysis for comparison (it see line with circles in Figure 13). Process data\nrepresent hourly averages around the time when the sample due to laboratory\nanalysis has been taken. The total data samples includes 309 records.\nThe estimation of the product composition contained noise due to the nature of\n25\nTable 2\nAccuracies obtained by eTS when applying drift detection and reaction (third col-\numn) and when performing conventional eTS (second column)\nError measure Without drift reaction With drift reaction Best (in theory) value\nNDEI 0.44278 0.3559 0\nVAF, % 80.461 87.319 100\ncorrelation 0.89971 0.9357 1\nthe analysis. A significant operating condition change took place after sample\n127 (see Figure 13). If no detection of the shift in the data exists and no\nreaction to this shift in terms of changing structure of the underlying fuzzy\nrule-based model then the result is significantly poorer. Due to the detection\nand reaction to this shift in the data pattern (see Figures 4 and 6), the eTS\nrule-based system evolved its structure and achieved a better result (see Table\n2). Note that the non-dimensional error index (NDEI) is defined as the ratio\nof the root mean square error over the standard deviation of the target data\nand should ideally be 0, while the variance accounted for (VAF) is defined as\nthe ratio between the variance of the real data and the model output and is\ngiven out of a maximum of 100 (when the predictions coincide with the real\ndata).\n6 Conclusion\nIn this paper, we have proposed novel strategies and techniques for address-\ning concept drift and shift in on-line data streams. To this end, two EFS\napproaches (eTS and FLEXFIS) were exploited as on-line modeling method-\nologies. These were extended by mechanisms which are able to 1.) detect drifts\nand shifts with fuzzy rule ages and 2.) react to such occurrences appropriately.\nReacting applies i) to the rule antecedents by clustering the data space (forc-\ning more significant moves of clusters than in the usual, non-drift case); and\nii) to rule consequent parameters by including gradual forgetting in the re-\ncursive local learning approach. This can be applied to any EFS technique\nexploiting Takagi-Sugeno type fuzzy systems. Evaluation with real-world data\nsets showed that the novel techniques are able to improve the accuracy and\nstability of the fuzzy models, whenever drifts and shifts occur. Consequently,\nautomatic detection of and reaction to drifts and shifts may represent an\nimportant component in EFS approaches.\n26\nAcknowledgements\nThis work was partially supported by the Upper Austrian Technology and\nResearch Promotion and partially by The Royal Society, UK. This publication\nreflects only the authors\u2019 views.\nReferences\n[1] P. Angelov and D. Filev. Simpl eTS: A simplified method for learning evolving\nTakagi-Sugeno fuzzy models. In Proceedings of FUZZ-IEEE 2005, pages 1068\u2013\n1073, Reno, Nevada, U.S.A., 2005.\n[2] P. Angelov and X. Zhou. Evolving fuzzy-rule-based classifiers from data\nstreams. IEEE Transactions on Fuzzy Systems, special issue on Evolving Fuzzy\nSystems, 16(6):1462\u20131475, 2008.\n[3] P.P. Angelov. Evolving takagi-sugeno fuzzy systems from streaming data, eTS+.\nIn P. Angelov, D. Filev, and N. Kasabov, editors, Evolving Intelligent Systems:\nMethodology and Applications, pp. 21\u201350. John Wiley & Sons, IEEE Press Series\non Computational Intelligence, New York, 2010, ISBN 978-0-470-28719-4.\n[4] P.P. Angelov and R.A. Buswell. Evolving rule-based models: A tool for\nintelligent adaptation. In 9th IFSA World Congress, pages 1062\u20131067, 2001.\n[5] P.P. Angelov and D. Filev. An approach to online identification of Takagi-\nSugeno fuzzy models. IEEE Trans. on Systems, Man and Cybernetics, part B,\n34(1):484\u2013498, 2004.\n[6] P.P Angelov and N. Kasabov. Evolving computational intelligence systems. In\nProceedings of the 1st International Workshop on Genetic Fuzzy Systems, pages\n76\u201382, Granada, Spain, 2005.\n[7] P.P. Angelov and A. Kordon. Adaptive Inferential Sensors based on Evolving\nFuzzy Models: An Industrial Case Study. IEEE Transactions on Systems, Man\nand Cybernetics, part B: Cybernetics, 40(2): 529\u2013539, 2010.\n[8] P.P. Angelov, E. Lughofer, and X. Zhou. Evolving fuzzy classifiers using\ndifferent model architectures. Fuzzy Sets and Systems, 159(23):3160\u20133182, 2008.\n[9] P.P. Angelov and X.-W. Zhou. Evolving fuzzy systems from data streams in\nreal-time. In 2006 International Symposium on Evolving Fuzzy Systems, pages\n29\u201335, 2006.\n[10] P. Angelov and X.-W. Zhou. On Line Learning Fuzzy Rule-based System\nStructure from Data Streams. In 2008 IEEE International Conference on Fuzzy\nSystems within the IEEE World Congress on Computational Intelligence, Hong\nKong, June 1-6, 2008, pages 915\u2013922, 2008.\n27\n[11] P.P. Angelov Evolving Rule-based Models: A Tool for Flexible Systems Design,\nSpringer Physica Verlag, Heidelberg, Germany, February, 2002.\n[12] P. Angelov. Machine Learning (Collaborative Systems), patent\n(WO2008053161, priority date: 1 November 2006; international filing date 23\nOctober 2007; USA publication 11 February 2010, number 2010-0036780).\n[13] R. Babuska. Fuzzy Modeling for Control. Kluwer Academic Publishers, Boston,\n1998.\n[14] J. Beringer and E. Hu\u00a8llermeier. Efficient instance-based learning on data\nstreams. Intelligent Data Analysis, 11(6):627\u2013650, 2007.\n[15] J. Casillas and O. Cordon and F. Herrera and L. Magdalena. Interpretability\nIssues in Fuzzy Modeling. Springer Verlag, Berlin Heidelberg, 2003.\n[16] F. Hopner and F. Klawonn. Obtaining interpretable fuzzy models from fuzzy\nclustering and fuzzy regression. Proc. 4th Intern. Conf. on Knowledge-based\nIntell. Eng. Syst. (KES), Brighton, UK, pages pp.162\u2013165, 2000.\n[17] S. Chiu. Fuzzy model identification based on cluster estimation. Journal of\nIntelligent and Fuzzy Systems, 2(3):267\u2013278, 1994.\n[18] S. J. Delany, P. Cunningham, A. Tsymbal, and L. Coyle. A case-based technique\nfor tracking concept drift in spam filtering. Knowledge-Based Systems, 18(4\u2013\n5):187\u2013195, 2005.\n[19] R.O. Duda, P.E. Hart, and D.G. Stork. Pattern Classification - Second\nEdition. Wiley-Interscience, Southern Gate, Chichester, West Sussex PO 19\n8SQ, England, 2000.\n[20] G. Gan and C. Ma and J. Wu. Data Clustering: Theory, Algorithms, and\nApplications (Asa-Siam Series on Statistics and Applied Probability), Society\nfor Industrial & Applied Mathematics, U.S.A., 2007\n[21] R.M. Gray. Vector Quantization. IEEE ASSP Magazine, pp. 4\u201329, 1984.\n[22] N. Sundararajan H.-J. Rong, G.-B. Huang, and P. Saratchandran. Sequential\nadaptive fuzzy inference system (SAFIS) for nonlinear system identification and\nprediction. Fuzzy Sets and Systems, 157(9):1260\u20131275, 2006.\n[23] A. Hensel and\nT. Spittel. Kraft- und Arbeitsbedarf bildsamer Formgebungsverfahren. VEB\nDeutscher Verlag fr Grundstoffindustrie, 1978.\n[24] N. K. Kasabov and Q. Song. DENFIS: Dynamic evolving neural-fuzzy inference\nsystem and its application for time-series prediction. IEEE Trans. on Fuzzy\nSystems, 10(2):144\u2013154, 2002.\n[25] R. Klinkenberg. Learning drifting concepts: Example selection vs. example\nweighting. Intelligent Data Analysis, vol. 8 (3), pages 281\u2013300, 2004.\n28\n[26] R. Klinkenberg and T. Joachims. Detection concept drift with support vector\nmachines. In Proc. of the Seventh International Conference on Machine\nLearning (ICML), pages 487\u2013494. Morgan Kaufmann, 2000.\n[27] G. Leng, T.M McGinnity, and G. Prasad. An approach for on-line extraction\nof fuzzy rules using a self-organising fuzzy neural network. Fuzzy Sets and\nSystems, 150(2):211\u2013243, 2005.\n[28] E. Lima, M. Hell, R. Ballini, and F. Gomide. Evolving fuzzy modeling using\nparticipatory learning. In P. Angelov, D. Filev, and N. Kasabov, editors,\nEvolving Intelligent Systems: Methodology and Applications, pp. 67\u201386. John\nWiley & Sons, IEEE Press Series on Computational Intelligence, New York,\n2010, ISBN 978-0-470-28719-4.\n[29] L. Ljung. System Identification: Theory for the User. Prentice Hall PTR,\nPrentic Hall Inc., Upper Saddle River, New Jersey 07458, 1999.\n[30] E. Lughofer. Process safety enhancements for data-driven evolving fuzzy\nmodels. In Proceedings of 2nd Symposium on Evolving Fuzzy Systems, pages\n42\u201348, Lake District, UK, 2006.\n[31] E. Lughofer. Evolving Fuzzy Models \u2014 Incremental Learning, Interpretability\nand Stability Issues, Applications. VDM Verlag Dr. Mu\u00a8ller, Saarbru\u00a8cken, 2008.\n[32] E. Lughofer. Extensions of vector quantization for incremental clustering.\nPattern Recognition, 41(3):995\u20131011, 2008.\n[33] E. Lughofer. FLEXFIS: A robust incremental learning approach for evolving\nTS fuzzy models. IEEE Trans. on Fuzzy Systems (special issue on Evolving\nFuzzy Systems), 16(6):1393\u20131410, 2008.\n[34] E. Lughofer. Towards robust evolving fuzzy systems. In P. Angelov,\nD. Filev, and N. Kasabov, editors, Evolving Intelligent Systems: Methodology\nand Applications, pp. 87\u2013126. John Wiley & Sons, IEEE Press Series on\nComputational Intelligence, New York, 2010, ISBN 978-0-470-28719-4.\n[35] E. Lughofer, P. Angelov, and X. Zhou. Evolving single- and multi-model fuzzy\nclassifiers with FLEXFIS-Class. In Proceedings of FUZZ-IEEE 2007, pages\n363\u2013368, London, UK, 2007.\n[36] E. Lughofer and S. Kindermann. Improving the robustness of data-driven\nfuzzy systems with regularization. In Proc. of the IEEE World Congress on\nComputational Intelligence (WCCI) 2008, pages 703\u2013709, Hongkong, 2008.\n[37] E. Lughofer, J. E. Smith, M. A. Tahir, P. Caleb-Solly, C. Eitzinger, D. Sannen,\nand H. van Brussel. Human-Machine Interaction Issues in Quality Control\nBased on On-Line Image Classification. In IEEE Transactions on Systems,\nMan and Cybernetics, part A: Systems and Humans, vol. 39(5), pp. 960\u2013971,\n2009.\n[38] D. Nauck and R. Kruse. NEFCLASS-X \u2013 a Soft Computing Tool to Build\nReadable Fuzzy Classifiers. BT Technology Journal, vol. 16 (3): 180\u2013190, 1998\n29\n[39] S. Ramamurthy and R. Bhatnagar. Tracking recurrent concept drift in\nstreaming data using ensemble classifiers. In Proceedings of the Sixth\nInternational Conference on Machine Learning and Applications (ICMLA),\n2007, pages 404\u2013409, 2007.\n[40] D. Sannen, M. Nuttin, J.E. Smith, M.A. Tahir, E. Lughofer, and C. Eitzinger.\nAn interactive self-adaptive on-line image classification framework. In\nA. Gasteratos, M. Vincze, and J.K. Tsotsos, editors, Proceedings of ICVS 2008,\nvolume 5008 of LNCS, pages 173\u2013180. Springer, Santorini Island, Greece, 2008.\n[41] J. Sherman and W.J. Morrison. Adjustment of an inverse matrix corresponding\nto changes in the elements of a given column or a given row of the original\nmatrix. Annals of Mathematical Statistics, 20:621, 1949.\n[42] C.F. So, S.C. Ng, and S.H. Leung. Gradient based variable forgetting factor rls\nalgorithm. Signal Processing, 83(6):1163\u20131175, 2003.\n[43] T. Takagi and M. Sugeno. Fuzzy identification of systems and its applications\nto modeling and control. IEEE Trans. on Systems, Man and Cybernetics,\n15(1):116\u2013132, 1985.\n[44] A. Tsymbal. The problem of concept drift: definitions and related work.\nTechnical Report TCD-CS-2004-15, Department of Computer Science, Trinity\nCollege Dublin, Ireland, 2004.\n[45] V. Vapnik. Statistical Learning Theory. Wiley and Sons, New York, 1998.\n[46] L.X. Wang. Fuzzy systems are universal approximators. In Proc. 1st IEEE\nConf. Fuzzy Systems, pages 1163\u20131169, San Diego, CA, 1992.\n[47] G. Widmer and M. Kubat. Learning in the presence of concept drift and hidden\ncontexts. Machine Learning, 23(1):69\u2013101, 1996.\n[48] S. Wu, M.J. Er, and Y. Gao. A fast approach for automatic generation of fuzzy\nrules by generalized dynamic fuzzy neural networks. IEEE Trans. on Fuzzy\nSystems, 9(4):578\u2013594, 2001.\n[49] R. R. Yager. A model of participatory learning. IEEE Trans. on Systems, Man\nand Cybernetics, 20: 1229\u20131234, 1990.\n[50] R.R. Yager and D.P. Filev. Learning of fuzzy rules by mountain clustering. In\nProc. of SPIE Conf. on Application of Fuzzy Logic Technology, pages 246\u2013254.\nBoston, MA, U.S.A., 1993.\n30\n"}