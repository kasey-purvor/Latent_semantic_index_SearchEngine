{"doi":"10.1186\/1471-2105-9-183","coreId":"61958","oai":"oai:nora.nerc.ac.uk:5885","identifiers":["oai:nora.nerc.ac.uk:5885","10.1186\/1471-2105-9-183"],"title":"Data capture in bioinformatics: requirements and experiences with Pedro","authors":["Jameson, Daniel","Garwood, Kevin","Garwood, Chris","Booth, Tim","Alper, Pinar","Oliver, Steven G.","Paton, Norman W."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2008-04-10","abstract":"Background\\ud\nThe systematic capture of appropriately annotated experimental data is a prerequisite for most bioinformatics analyses. Data capture is required not only for submission of data to public repositories, but also to underpin integrated analysis, archiving, and sharing \u2013 both within laboratories and in collaborative projects. The widespread requirement to capture data means that data capture and annotation are taking place at many sites, but the small scale of the literature on tools, techniques and experiences suggests that there is work to be done to identify good practice and reduce duplication of effort.\\ud\n\\ud\nResults\\ud\nThis paper reports on experience gained in the deployment of the Pedro data capture tool in a range of representative bioinformatics applications. The paper makes explicit the requirements that have recurred when capturing data in different contexts, indicates how these requirements are addressed in Pedro, and describes case studies that illustrate where the requirements have arisen in practice.\\ud\n\\ud\nConclusion\\ud\nData capture is a fundamental activity for bioinformatics; all biological data resources build on some form of data capture activity, and many require a blend of import, analysis and annotation. Recurring requirements in data capture suggest that model-driven architectures can be used to construct data capture infrastructures that can be rapidly configured to meet the needs of individual use cases. We have described how one such model-driven infrastructure, namely Pedro, has been deployed in representative case studies, and discussed the extent to which the model-driven approach has been effective in practice","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/61958.pdf","fullTextIdentifier":"http:\/\/nora.nerc.ac.uk\/5885\/1\/1471-2105-9-183.pdf","pdfHashValue":"000554dffd6cef1b87ca17683aefe393ba953083","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:nora.nerc.ac.uk:5885<\/identifier><datestamp>\n      2017-01-18T12:52:23Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D5339<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/nora.nerc.ac.uk\/id\/eprint\/5885\/<\/dc:relation><dc:title>\n        Data capture in bioinformatics: requirements and experiences with Pedro<\/dc:title><dc:creator>\n        Jameson, Daniel<\/dc:creator><dc:creator>\n        Garwood, Kevin<\/dc:creator><dc:creator>\n        Garwood, Chris<\/dc:creator><dc:creator>\n        Booth, Tim<\/dc:creator><dc:creator>\n        Alper, Pinar<\/dc:creator><dc:creator>\n        Oliver, Steven G.<\/dc:creator><dc:creator>\n        Paton, Norman W.<\/dc:creator><dc:subject>\n        Data and Information<\/dc:subject><dc:description>\n        Background\\ud\nThe systematic capture of appropriately annotated experimental data is a prerequisite for most bioinformatics analyses. Data capture is required not only for submission of data to public repositories, but also to underpin integrated analysis, archiving, and sharing \u2013 both within laboratories and in collaborative projects. The widespread requirement to capture data means that data capture and annotation are taking place at many sites, but the small scale of the literature on tools, techniques and experiences suggests that there is work to be done to identify good practice and reduce duplication of effort.\\ud\n\\ud\nResults\\ud\nThis paper reports on experience gained in the deployment of the Pedro data capture tool in a range of representative bioinformatics applications. The paper makes explicit the requirements that have recurred when capturing data in different contexts, indicates how these requirements are addressed in Pedro, and describes case studies that illustrate where the requirements have arisen in practice.\\ud\n\\ud\nConclusion\\ud\nData capture is a fundamental activity for bioinformatics; all biological data resources build on some form of data capture activity, and many require a blend of import, analysis and annotation. Recurring requirements in data capture suggest that model-driven architectures can be used to construct data capture infrastructures that can be rapidly configured to meet the needs of individual use cases. We have described how one such model-driven infrastructure, namely Pedro, has been deployed in representative case studies, and discussed the extent to which the model-driven approach has been effective in practice.<\/dc:description><dc:date>\n        2008-04-10<\/dc:date><dc:type>\n        Publication - Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/nora.nerc.ac.uk\/id\/eprint\/5885\/1\/1471-2105-9-183.pdf<\/dc:identifier><dc:identifier>\n         \n\n  Jameson, Daniel; Garwood, Kevin; Garwood, Chris; Booth, Tim; Alper, Pinar; Oliver, Steven G.; Paton, Norman W..  2008  Data capture in bioinformatics: requirements and experiences with Pedro.   BMC Bioinformatics, 2008 (9), 183.   https:\/\/doi.org\/10.1186\/1471-2105-9-183 <https:\/\/doi.org\/10.1186\/1471-2105-9-183>     \n <\/dc:identifier><dc:relation>\n        http:\/\/www.pubmedcentral.nih.gov\/articlerender.fcgi?tool=pubmed&pubmedid=18402673<\/dc:relation><dc:relation>\n        doi:10.1186\/1471-2105-9-183<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/nora.nerc.ac.uk\/id\/eprint\/5885\/","http:\/\/www.pubmedcentral.nih.gov\/articlerender.fcgi?tool=pubmed&pubmedid=18402673","doi:10.1186\/1471-2105-9-183"],"year":2008,"topics":["Data and Information"],"subject":["Publication - Article","PeerReviewed"],"fullText":"BioMed CentralBMC Bioinformatics\nssOpen AcceSoftware\nData capture in bioinformatics: requirements and experiences with \nPedro\nDaniel Jameson1, Kevin Garwood2, Chris Garwood2, Tim Booth3, \nPinar Alper2, Stephen G Oliver4,5 and Norman W Paton*2\nAddress: 1School of Chemistry, Manchester Interdisciplinary Biocentre, The University of Manchester, 131 Princess Street, Manchester, M1 7DN, \nUK, 2School of Computer Science, The University of Manchester, Oxford Road, Manchester, M13 9PL, UK, 3NERC Centre for Ecology and \nHydrology, Mansfield Road, Oxford, OX1 3SR, UK, 4Faculty of Life Sciences, The University of Manchester, Michael Smith Building, Oxford Road, \nManchester, M13 9PT, UK and 5Department of Biochemistry, University of Cambridge, Sanger Building, 80 Tennis Court Road, Cambridge, CB2 \n1GA, UK\nEmail: Daniel Jameson - daniel.jameson@manchester.ac.uk; Kevin Garwood - Kevin.L.Garwood@manchester.ac.uk; \nChris Garwood - cgarwood@cs.man.ac.uk; Tim Booth - tbooth@ceh.ac.uk; Pinar Alper - pinar.alper@manchester.ac.uk; \nStephen G Oliver - steve.oliver@bioc.cam.ac.uk; Norman W Paton* - norman.paton@manchester.ac.uk\n* Corresponding author    \nAbstract\nBackground: The systematic capture of appropriately annotated experimental data is a\nprerequisite for most bioinformatics analyses. Data capture is required not only for submission of\ndata to public repositories, but also to underpin integrated analysis, archiving, and sharing \u2013 both\nwithin laboratories and in collaborative projects. The widespread requirement to capture data\nmeans that data capture and annotation are taking place at many sites, but the small scale of the\nliterature on tools, techniques and experiences suggests that there is work to be done to identify\ngood practice and reduce duplication of effort.\nResults: This paper reports on experience gained in the deployment of the Pedro data capture\ntool in a range of representative bioinformatics applications. The paper makes explicit the\nrequirements that have recurred when capturing data in different contexts, indicates how these\nrequirements are addressed in Pedro, and describes case studies that illustrate where the\nrequirements have arisen in practice.\nConclusion: Data capture is a fundamental activity for bioinformatics; all biological data resources\nbuild on some form of data capture activity, and many require a blend of import, analysis and\nannotation. Recurring requirements in data capture suggest that model-driven architectures can be\nused to construct data capture infrastructures that can be rapidly configured to meet the needs of\nindividual use cases. We have described how one such model-driven infrastructure, namely Pedro,\nhas been deployed in representative case studies, and discussed the extent to which the model-\ndriven approach has been effective in practice.\nPublished: 10 April 2008\nBMC Bioinformatics 2008, 9:183 doi:10.1186\/1471-2105-9-183\nReceived: 19 October 2007\nAccepted: 10 April 2008\nThis article is available from: http:\/\/www.biomedcentral.com\/1471-2105\/9\/183\n\u00a9 2008 Jameson et al; licensee BioMed Central Ltd. \nThis is an Open Access article distributed under the terms of the Creative Commons Attribution License (http:\/\/creativecommons.org\/licenses\/by\/2.0), \nwhich permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.Page 1 of 15\n(page number not for citation purposes)\nBMC Bioinformatics 2008, 9:183 http:\/\/www.biomedcentral.com\/1471-2105\/9\/183Background\nWhy Data Capture Matters\nAlthough many biologists are shielded from the intrica-\ncies of the field, the use of bioinformatics tools is a day-to-\nday reality for many. Modern high-throughput technolo-\ngies yield substantial volumes of complex data that need\nto be stored, indexed and analysed. Effective storage and\nannotation of such data is imperative as, for example, the\nburgeoning field of Systems Biology emphasises the need\nto cross-reference and analyse results from multiple exper-\niments and multiple sources.\nThe nature of biological research and the evolution of\ntechnology have resulted in myriad bespoke systems for\ndata capture, storage and retrieval as reflected in the\nincreasing number of databases documented in The\nMolecular Biology Database Collection [1] \u2013 115 in 2000,\n548 in 2004, and 968 in 2007. Yet this represents only a\nfraction of the data resources currently in use in the life\nsciences, since it only counts those that are publicly acces-\nsible. Internally, research groups and institutions rely on a\nwide range of systems and architectures, ranging from the\narchiving of lab books to well-designed experimental data\nrepositories. As a result, numerous sites undertake data\ncapture tasks so that experimental and derived data can be\nanalysed and archived. Despite this ubiquity, data capture\nrequirements, tools and techniques receive rather little\ndirect attention in the bioinformatics literature.\nTypes of Data\nBiology is a broad field and, as a result, there are a wide\nvariety of data types that may need to be captured and\nannotated. Nonetheless, biological data can be grouped\ninto three major categories:\n1) Primary data, which are generated directly by experi-\nments. These may be recorded electronically in output\nfiles from instrumentation, by hand in lab books or as\nimages. Examples include micrographs, fitness measure-\nments from different organisms, sequence data, spectros-\ncopy results, and microarray image files.\n2) Secondary data, which are derived from primary data,\nare the results of some form of analysis that has been per-\nformed on the primary data. These include things like pro-\ntein structures, phylogenetic trees, normalised gene\nexpression values and protein identifications. Sometimes\nsecondary data are generated from a single piece of pri-\nmary data, but often they are the results of analyses\ninvolving primary or secondary data from many sources.\n3) Metadata, which may relate to primary data or second-\nary data or both, provides detail and context to the data.\nExamples include experimental protocols, references to\nstandard experimental materials, experimenter details,\ntimes and dates, keywords and bibliographic references.\nAlthough data resources may combine these different\ntypes of data in different ways, the following scenarios are\ncommon:\n1) Primary data with associated metadata, e.g. GenBank\n[2].\n2) Secondary data with associated metadata, e.g. pFam\n[3].\n3) Primary data and directly derived secondary data with\nassociated metadata, e.g. SGD [4].\nRequirements of Data Capture Tools\nRequirements of data capture tools vary depending on the\ncontext within which the data is to be captured, the nature\nof the data, and the use to which it is to be put. However\nthe following requirements have been identified in several\ncontexts:\nR1. Import and export of data in appropriate formats.\nR2. Manipulation and update of existing records.\nR3. Checking of integrity of captured data.\nR4. Callable by, or able to call on, other applications.\nR5. Able to use controlled vocabularies for annotation.\nR6. Able to selectively re-use existing data in new entries.\nExisting Data Capture tools for Bioinformatics\nThis section reviews several representative data capture\nsolutions for bioinformatics. These have been selected as\nthey fulfil the following critera:\n1) They are publicly available.\n2) They are used for capturing rich descriptions.\n3) They represent a wide range of functionalities.\n4) They include offerings from the major international\nbiological data centres (EBI-EMBL, NCBI).\nThe extent to which the solutions examined fulfil the\nrequirements listed above is summarised in Table 1. We\nnote that not all requirements are relevant to all contexts,\nso a solution may be effective for a given application with-\nout supporting all the requirements.Page 2 of 15\n(page number not for citation purposes)\nBMC Bioinformatics 2008, 9:183 http:\/\/www.biomedcentral.com\/1471-2105\/9\/183Sequin and Webin\nSequin and Webin [2] support data capture and submis-\nsion for the NCBI's GenBank sequence database, and are\ncapable of marking up DNA, mRNA and protein\nsequences in a format suitable for submission to the Gen-\nBank, EMBL and DDBJ databases. The user is led through\na series of steps to construct the annotated file that will\nultimately be uploaded (Figure 1). The software also\nallows visualisation of sequence data before uploading. A\nSequin data capture toolFig re 1\nSequin data capture tool. The user is led through import of data and subsequent annotation stepwise. This results in a file \nsuitable for submission to GenBank, EMBL or DDBJ.\nTable 1: Requirements of data capture tools\nRequirement Pedro Sqeuin maxDLoad2 ArrayExpress MAGE-TAB PRIDE\nR1 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713\nR2 \u2713 \u2713 \u2713 \u2713\nR3 \u2713 \u2713 \u2713 \u2713 \u2713\nR4 \u2713\nR5 \u2713 \u2713 \u2713 \u2713 \u2713\nR6 \u2713 \u2713 \u2713 \u2713 \u2713Page 3 of 15\n(page number not for citation purposes)\nBMC Bioinformatics 2008, 9:183 http:\/\/www.biomedcentral.com\/1471-2105\/9\/183high level of context-sensitive help is available, and is dis-\nplayed alongside the data entry window.\nAlthough short sequences are reasonably straightforward\nto annotate by hand, larger sequences (for example com-\nplete genomes) require a different approach. To support\nthis, Sequin allows the bulk import of sequence features\nthrough the use of a tab-delimited file. While this does not\nafford the same level of validation as the software inter-\nface, it speeds the submission of entries that may contain\nthousands of individual annotations.\nmaxdLoad2\nmaxdLoad2 [5] is a data-loader\/annotator and repository\nfor microarray datasets that, once captured, can be manip-\nulated and analysed using maxdView (Figure 2).\nMicroarray datasets from experiments that are to be pub-\nlished often have to be annotated to the level of detail\nspecified by the MIAME [6] (Minimum Information to\nAnnotate a Microarray Experiment) standard, and there is\ngenerally a requirement that the annotated experiments\nare submitted to a public repository (e.g. GEO [7] or\nArrayExpress [8]) before publication. maxdLoad2 allows\nthe export of annotated data in MAGE-ML [9] format,\nwhich may be read into the ArrayExpress database.\nAlthough free text may be used to provide annotation up\nto MIAME standards, maxdLoad2 provides a logical route\nthrough each level of annotation, which ensures that the\nstandard is reached.\nMuch of the data entered (e.g. array types) can be re-used\nin future annotations, thereby reducing the amount of\nwork that needs to be undertaken when a subsequent\nexperiment is loaded. Although it takes time to annotate a\nsimple experiment using the point and click interface of\nmaxdLoad2, it is also possible to bulk upload annotations\nin a spreadsheet format and then refine the annotations\nwithin the software.\nArrayExpress\nArrayExpress is a public repository for microarray experi-\nmental data [8]. It provides both a web-based and a Java\napplication for the annotation and upload of experimen-\ntal data to the database. Again, these lead the user through\na series of steps, by the end of which they have annotated\ntheir data to the level required by MIAME.\nThe above solutions are representative of the diverse col-\nlection of bespoke data capture interfaces found in bioin-\nformatics. They are essentially \"Wizards\", leading the\nannotator through a pre-defined path at the end of which\nthey will have generated a dataset suitable for storage in\nthe desired repository. This achieves the aim of simplify-\ning the loading of data to a point where scientists can typ-\nically perform the task with minimal training, but can\nresult in certain annotation procedures becoming long-\nwinded and repetitive; hence the oft-provided facility to\nbulk upload data in a spreadsheet or tab-delimited for-\nmat.\nGeneric Infrastructures\nGeneric data capture infrastructures are desirable as they\ncan, at least in principle, be rapidly applied to new data\ncapture contexts. While a bespoke solution, given appro-\npriate resources, should be able to support the require-\nments of specific tasks and user communities directly, the\nability to instigate a data capture regime quickly when a\nRelationship between entities in the maxdLoad2 data modelFigure 2\nRelationship between entities in the maxdLoad2 data model.Page 4 of 15\n(page number not for citation purposes)\nBMC Bioinformatics 2008, 9:183 http:\/\/www.biomedcentral.com\/1471-2105\/9\/183need is identified can be essential to keeping the data cap-\nture infrastructure in step with experimental practice,\nwhile also reducing development costs.\nSpreadsheets are generic data capture and analysis tools.\nThey permit the entry and editing of tabular data, and can\noutput these data in a tab-delimited format that can be\nquickly parsed into databases. In addition, modern\nspreadsheets can be programmed to check that the data\nentered satisfies certain completeness and integrity con-\nstraints. Furthermore, the look and feel of the sheet may\nbe adapted to isolate the user from its inner workings; for\nexample, by only allowing data to be entered in specific\ncells. Spreadsheets have the added advantage that many\nbiologists use them on a regular basis to tabulate and ana-\nlyse data. With this in mind, there have been recent efforts\nto produce data-capture spreadsheets for specific pur-\nposes.\nPRIDE\nPRIDE is the PRoteomics IDEntifications database at the\nEBI [10]. Submission of data in an XML format conform-\ning to a model defined as an XML Schema is encouraged.\nA spreadsheet known as Proteome Harvest has been\ndeveloped that supports the generation of PRIDE-XML.\nThe spreadsheet works with Microsoft Excel, and makes\nextensive use of Microsoft's Visual Basic Scripting lan-\nguage to generate XML documents. The user is guided\nthrough a number of spreadsheet tabs where the requisite\ndata to build the XML document is entered. Additionally,\nwhen appropriate, values for fields can be extracted from\na remote, controlled vocabulary database to ensure that\nvalues are drawn from consistent terminologies. For\nexample, when detailing biological source samples, ontol-\nogy terms may be retrieved from NEWT (Taxonomy), the\nBRENDA Tissue Ontology and the Cell Type Ontology to\nprovide a complete and controlled description of that\nsample.\nThe Proteome Harvest Spreadsheet is essentially a generic\ntool (i.e. Excel) that has been configured to support a spe-\ncific task by software plug-ins that generate PRIDE-XML\nfrom a collection of custom data entry forms (Figure 3).\nProteome Harvest SpreadsheetFigure 3\nProteome Harvest Spreadsheet. Macro functions allow searching for ontology terms and provide validation for input data.Page 5 of 15\n(page number not for citation purposes)\nBMC Bioinformatics 2008, 9:183 http:\/\/www.biomedcentral.com\/1471-2105\/9\/183MAGE-TAB\nMAGE-TAB is a standard format for annotating microarray\ndatasets to the MIAME standard using spreadsheets or tab\ndelimited files [11]. It relies on four separate files to define\nan experiment:\n1) Investigation Description Format\nThis is a tab-delimited file that gives general information\nabout the experiment performed, including contact\ndetails, references and free-text descriptions of the proto-\ncols used.\n2) Array Design Format\nThis describes the array used and the sequences associated\nwith individual array locations. It may alternatively pro-\nvide an accession number linking it to a pre-defined array\nalready located in a public repository.\n3) Sample and Data Relationship Format\nThis tab-delimited file describes the relationships between\nthe samples, arrays, and data generated by the experiment.\n4) Raw and processed data files\nThese are typically native format data files generated by\nthe array imaging software. Alternatively there is a \"data\nmatrix\" tab-delimited format in the MAGE-TAB specifica-\ntion.\nThis approach is at the other end of the spectrum from\nProteome Harvest in the sense that it uses the spreadsheet\nsolely as format to be adhered to. The drawback with this\nmethod of data capture is the lack of validation in the pro-\ncedure \u2013 as the files are simply text and generated using\nwhich ever text editor the user desires, there can be no\nchecks for consistency in, for example, the Sample and\nData Relationship file. This makes it more likely that\nannotation errors will be introduced when capturing a\ncomplicated dataset.\nEffective data capture is a recurring need in the biological\nsciences. As a result, there are many bespoke solutions\nwith overlapping capabilities, and several activities are\nbuilding on spreadsheets as a configurable infrastructure.\nHowever, as yet, there is limited practical experience with\nconfigurable data capture infrastructures, and the devel-\nopment of bespoke infrastructures (such as Sequin and\nWebin for GenBank) continues to be a significant drain\non bioinformatics resources.\nResults\nModel-driven data capture in Pedro\nThe Pedro software [12] was originally developed to sup-\nport data capture in the PEDRo proteomics model [13], in\na context where it was anticipated that standard models\nand ontologies would evolve rapidly. Thus, rather than\ndeveloping a bespoke application that would need regular\nchanges to its code base, a model-driven approach was\nadopted in which the code base is independent of the\nstructure of the data to be captured.\nIn model-driven software development, aspects of the\nbehaviour of a system are configurable using models [14].\nIn Pedro, there are two models: (i) a domain model, repre-\nsented in XML Schema, which describes structure of the\ndata that is to be captured, and against which the resulting\nXML file must validate; and (ii) a configuration model,\nwhich characterises aspects of the behaviour of the appli-\ncation. In Pedro, the configuration model defines the con-\ntext sensitive help (relevant information is dynamically\ndisplayed when the mouse passes over specific form fields\nand labels), where ontologies are to be used for populat-\ning specific elements in the domain model, which plug-\nins are to be used and from where they can be obtained,\nand what validation is to take place in addition to that\nsupported directly by the domain model.\nPedro is part of a family of model-driven tools that\nincludes the Pierre system for accessing biological data-\nbases [15]; Pedro is used to edit the models that drive\nPierre applications. Where the developers of a model-\ndriven system anticipate differences between the require-\nments of different applications, and can capture those dif-\nferences using a model, the development of a new\napplication reduces to the provision of a new model.\nWhere a new application requires highly specialised\nbehaviour, such as the extraction of data from a proprie-\ntary file format, such behaviours may be supported by\nplug-in software components, for which extensibility\npoints are provided by the model-driven architecture.\nA data capture application in Pedro involves the following\ncomponents:\n1) A model of the data to be captured\nthe structure of the data is represented as an XML Schema\n[16].\n2) An optional collection of plug-in software components\nwhere specialised behaviour is required by the applica-\ntion, for example connecting to a remote controlled\nvocabulary server or reading from a proprietary file for-\nmat, software components can be provided that imple-\nment these capabilities. These reside in a library jar file\nwhich is specific to the domain model in use.\n3) A model of the behaviour of the system\nthe model of the behaviour of the system (the configura-\ntion model). This model resides in a configuration file\nwhich is itself created and revised using Pedro.Page 6 of 15\n(page number not for citation purposes)\nBMC Bioinformatics 2008, 9:183 http:\/\/www.biomedcentral.com\/1471-2105\/9\/1834) The Pedro software\nGiven 1) to 3), the Pedro software provides an interactive\nuser interface with which to capture data conforming to\nthe model in 1).\nBelow we describe in more detail how the model is used\nto generate forms and how the functionality of the tool\nmay be extended through the use of ontologies, plug-ins\nand validation services.\nForm Generation\nPedro captures hierarchical documents which consist of\nrecords containing fields which may contain further\nrecords. Forms are the fundamental method by which a\nuser interacts with Pedro to capture data. Pedro's forms\nare driven by user defined XML Schema Definitions\n(XSDs) which describe the records and fields that may\nappear in a document. Information about additional\nlabels, form buttons and which fields they are associated\nwith is read from the configuration file discussed above.\nPedro renders forms using information combined from\nthe schema and configuration file. Figure 4 shows a sec-\ntion of schema that may be used to describe part of a\nmicroscopy experiment and how the elements in the\nschema map directly to form fields rendered on screen.\nPossible form fields may be:\n\u2022 Edit Fields \u2013 these may be Text Fields, URI Fields and\nRadio Buttons (grouped and ungrouped). Attached to\nthese are any form comments, tool tips, help links defined\nin the configuration file.\n\u2022 List Fields \u2013 these are lists of records that are children of\nthe current record.\nData Import\/Export\nPedro is able to directly import data from files in its own\nnative format, from XML files that conform to the current\nschema and also from tab delimited text files. This latter\nfeature enables data that may have been captured or\nrecorded in a spreadsheet tabular format to be rapidly\nimported into records within an XML document. When\nperforming this operation, for any record in the docu-\nment, columns within the file may be mapped to specific\nfields whilst multiple lines relate to multiple records.\nBefore finalisation of a document it may be saved in\nPedro's native format for later editing; however when\nexporting to XML the document is tested for validity\nagainst the model schema and cannot be saved until it\nconforms.\nUser Plug-ins\nWithout any additional coding, Pedro is able to generate\nforms from a schema, capture data, and output that data\nPedro uses an XML Schema to generate forms for data captureFigure 4\nPedro uses an XML Schema to generate forms for data capture. Here the direct mapping between schema and form \nelements is illustrated. List boxes are displayed where an element may have multiple instances or simple text boxes where only \none instance is allowed. Compulsory fields are rendered in bold type.Page 7 of 15\n(page number not for citation purposes)\nBMC Bioinformatics 2008, 9:183 http:\/\/www.biomedcentral.com\/1471-2105\/9\/183as an XML file that conforms to the original schema defi-\nnition. This may be sufficient for some basic data capture\napplications, however to extend functionality plug-ins can\nbe implemented by the developers who are configuring\nPedro for use in a specific domain. The plug-ins may pro-\nvide, for example, database access, data analysis and data\nimport or export functions.\nThe plug-ins themselves are written in Java and imple-\nment the PedroPlugin interface. The records and fields\nassociated with a particular plug-in are defined by the\nConfiguration File. When the data entry tool is running,\nplug-ins may appear in different places. Document-level\nplug-ins appear in one or more of the menus in the menu\nbar. If, during document editing, plug-ins are associated\nwith the current record type, then a \"Plug-ins.\" button is\ndisplayed in the main form (Figure 5). If plug-ins are asso-\nciated with a field, the \"Plug-ins.\" button appears at the\nend of the form field. Plug-ins have access to all of docu-\nment data structures within Pedro, and can manipulate\nthem independently of the user interface. This can be par-\nticularly useful when parsing foreign file formats into\nPedro or enabling Pedro to interact directly with an exter-\nnal database. The use of plug-ins in a production environ-\nment is discussed in the Cell Imaging case study below.\nOntology Services\nAn ontology service allows end-users to mark up a form\nfield with terms that come from a controlled list of terms.\nAlthough its scope of effect is a single form text field, it\nmay use other information about the current form, the\nuser or document to help constrain the choices of terms it\nprovides to the viewer. From a developer's perspective the\nontology services comprise two parts, Ontology Source\nand Ontology Viewer. Terms are derived from the Ontol-\nogy Source and presented to the user by the Ontology\nViewer.\nThe Ontology Source for a particular field is defined\nwithin the configuration editor and manifests itself as a\nJava class that implements the OntologySource interface.\nA list of available OntologySource classes and interfaces is\nshown in Table 2, and by extending these or implement-\ning a new OntologySource classes it is possible to utilise\nontologies from a variety of sources (local or remote)\nunconstrained by format.\nUsing a Pedro plug-in appropriate to the current recordFigure 5\nUsing a Pedro plug-in appropriate to the current record. The top level record for the Cell Imaging annotation docu-\nment has three different plugins (Save or Load from database and Save directly to File) associated with it which may either read \nfrom or manipulate the complete document structure.Page 8 of 15\n(page number not for citation purposes)\nBMC Bioinformatics 2008, 9:183 http:\/\/www.biomedcentral.com\/1471-2105\/9\/183Ontology sources may also implement other interfaces,\nwhich allow Pedro to make a decision about which ontol-\nogy viewer to use. For example, an ontology source which\nimplements the TreeOntologySource interface will be able\nto provide Pedro with the information necessary to render\nthat ontology as a tree, and hence Pedro will do as such.\nAgain it is possible to implement new viewers by imple-\nmenting the OntologyViewer interface or extending the\nexisting viewers (Table 3). In Figure 6 we show the use of\na simple ontology to control the names of microscopes\navailable for use in the document being edited.\nValidation Services\nValidation is extremely important for ensuring data integ-\nrity, and Pedro supports validation services which can\naffect a field, record or entire document. Field and record\nvalidation services are triggered whenever the user\nattempts to commit changes to the current record. Many\nof the validation constraints are implicitly defined in the\nXML Schema (field types, number of instances); however\nthe facility exists to extend these with user defined valida-\ntion classes which may then be specified in the configura-\ntion file.\nPedro's document-level validation services are triggered\nwhen users try to export a data set to a final submission\nformat or when they use the \"Show Errors\" feature in the\nView menu.\nField-level validation services are intended to identify\nproblems in the value of an edit field or with the compo-\nsition of child records found in a list field. Record-level\nvalidation services are intended to identify field values\nwhich are legitimate when considered in isolation but are\nwrong when considered in combination with other field\nvalues. For example, within the microscopy experiment\nannotations, a form could have fields with values such as\n\"start_date = 10\/2\/07\" and \"end_date = 4\/2\/07\" which\nwould form an illegal combination of values. Document-\nlevel validation services are intended to identify errors\nthat appear in disparate parts of the same data set. A list of\navailable validation service interfaces is shown in Table 4.\nPedro is a generic but highly configurable data capture\ntool which has allowed it to be deployed in the variety of\nscenarios we describe below.\nCase Studies\nHere, we present three use-cases that have made use of\nPedro. These examples, from different projects and labo-\nratories, illustrate how data capture requirements encoun-\ntered in practice can be addressed using the model-driven\napproach.\nMetadata Capture for Cell Imaging\nThis project has investigated gene function through high-\nthroughput image analysis of living cells [17]. Using\nmicroscopy, fluorescently labelled proteins are tracked in\nreal time as they move within cells. The experiments con-\nducted allow a single microscope to observe a large\nnumber of fields in a single run. The archiving and index-\ning of these rich data sets requires a database to store the\nresults of microscope runs along with associated descrip-\ntive metadata. Capturing and consolidating these data\ninvolves parsing and integrating output from multiple\nsources to conform to a schema that makes extensive use\nof controlled vocabularies. To support this functionality,\na data capture infrastructure must support requirements\nR1, R2, R3, R5 and R6, as listed previously.\nThe principal components in the data capture and storage\ninfrastructure are illustrated in Figure 7. To encourage\nconsistent annotation of data, it is important to minimise\nboth the complexity and amount of manual annotation\nthat must be performed. In this example, much of the\ninformation needed to describe the experiments comes\nTable 2: Pedro Ontology Sources\nClass\/Interface Function\nOntologySource Implementing classes provide ontology terms to an OntologyViewer.\nTreeOntologySource Extends OntologySource but implementing classes provide ontology terms that can be \norganised as a tree structure.\nDictionaryDescriptionSupport Implementing classes provide a definition for OntologyTerms.\nImageDescriptionSupport Implementing classes can provide images based on an identifier.\nURLDescriptionSupport Implementing classes can provide a link to a webpage.\nSingleColumnTextSource, OntologySource An ontology source that reads terms from a text file that contains a single column of ontology \nterms.\nAbstractTreeOntologySource, TreeOntologySource A class to manage a tree of terms. This is extended in the classes below.\nTabIndentedTextSource Extends AbstractTreeOntologySource. This is an ontology source that reads terms from a tab-\nindented text file.\nXMLOntologySource Extends AbstractTreeOntologySource. This is an ontology source that reads its terms from a \nbespoke XML format.Page 9 of 15\n(page number not for citation purposes)\nBMC Bioinformatics 2008, 9:183 http:\/\/www.biomedcentral.com\/1471-2105\/9\/183from other software systems. Experimental metadata from\nthe microscope is initially stored in a proprietary format,\nwhich is parsed and mapped into the domain model by a\nPedro plug-in. The experimental metadata includes\ndetails about the microscopy equipment used, locations\nrecorded, the time points at which data were collected,\nlasers used and their settings. Images from the microscope\nare analysed by Cell-Tracker [18], which extracts features\nfrom the images; these features are integrated with the\nexperimental metadata using a second plug-in. Experi-\nmentalists use the interactive interface to provide addi-\ntional experimental metadata, for example on the sample\nstudied, and a third plug-in stores the data in the Tamino\nnative XML database [19].\nTable 5 provides a complete list of the plug-ins employed;\nthe experiment import and export plug-ins allow the\nTable 3: Pedro Ontology Viewers\nClass\/Interface Function\nOntologyViewer Implementing classes can render a set of ontology terms.\nDefaultOntologyViewer, OntologyViewer This is the default viewer used to render terms provided by ontology sources. Depending on the \ninterfaces (Table 2) provided it can render the ontology terms in a variety of different ways:\n\u2022 Simple list\n\u2022 Tree\n\u2022 With dictionary definitions panel\n\u2022 With image thumbnail panel\n\u2022 With webpage panel\nUtilisation of an ontologyFigure 6\nUtilisation of an ontology. Here a simple list type ontology is used to annotate details of Microscope Name in a record. \nIncreasing the number of available ontology terms in a simple list will result in it being rendered in multiple sublists.Page 10 of 15\n(page number not for citation purposes)\nBMC Bioinformatics 2008, 9:183 http:\/\/www.biomedcentral.com\/1471-2105\/9\/183direct submission of the experiment to the database, as\nwell as the retrieval and updating of records (Figure 5).\nPedro's use of an XML Schema as the domain model, cou-\npled with the Tamino XML DBMS [20], allows for valida-\ntion of the generated document before submission to the\ndatabase. The same schema is implemented in both loca-\ntions, and thus ensures that model constraints are\nenforced consistently in different parts of the architecture.\nFurther integrity of the data is ensured by using plug-ins\nto support experiment annotation using predefined XML\nentities stored on the database server. In this situation, an\nalternative methodology would be to employ Pedro's\nontology features as described below.\nSeveral features of the model-based approach have been\nimportant to this project. In particular, the model of the\ndomain has gone through several iterations, to reflect\nchanges in experimental practice and in the experimental-\nists' understanding of what it was appropriate to archive.\nThe fact that changes to the model can be reflected imme-\ndiately in the interface has supported an iterative develop-\nment process where end users are able to quickly feed\nback on changes to the model, and further modifications\nand updates can be rapidly distributed to the end users\nwithout any modifications to their installations. Further-\nmore, support for plug-ins has enabled the interactive\ndata entry features to be used in conjunction with several\nexisting software systems in a distributed environment.\nThe end user experience of Pedro has, by and large, been\npositive. An initial deployment with laboratory-based\nbiologists has seen its effective use for data capture with\nminimal training. As issues of clarity in the forms pre-\nsented have become apparent, these have been addressed\nthrough use of annotations displayed in the form text, and\nthe users are now annotating complete experiments to the\nstandards required for the database.\nUse of the Pedro ontology services in the EnvBase data \ncatalogue\nEnvBase is a high-level, project-centric catalogue of data-\nsets created and maintained by the NERC Environmental\nBioinformatics Centre (NEBC). Due to the varied nature\nof projects recording data in the system, and the rapidly\nevolving nature of research in environmental genomics, a\nflexible approach was required. It was decided to repre-\nsent catalogue entries as XML documents, and to employ\na range of free and open source software solutions \u2013 Perl,\nPostgreSQL, GenQuery and Apache \u2013 to implement the\ncatalogue. Pedro was chosen as an editor for the catalogue\nof XML documents as it was easy to configure, runs stan-\ndalone on platforms that support Java, while combining\nXML editing and validation with access to ontology serv-\nices. Overall, to support this application, a data capture\ninfrastructure must support requirements R1, R2, R3, R4\nand R5, as listed previously.\nThe use of the ontology services in EnvBase is mainly con-\nfined to word lists. Both single-column lists (for example,\nto select the funding programme under which the work\nOrganisation of the data capture and storage architecture for the cell imaging projectFigure 7\nOrganisation of the data capture and storage archi-\ntecture for the cell imaging project.\nXML\nDatabase\nPedro\nCell Tracker\nMicroscope\nOutput\nPierre Front\nEnd\nPlugins\nTable 4: Pedro Validation Service Interfaces\nInterface Context Role\nListFieldValidationService Field Implementing classes provide a validation service for list fields that is triggered whenever the user \nexplicitly activates \"Show Errors\" or when the current data file is exported to a final submission format.\nEditFieldValidationService Field Implementing classes provide a validation service for edit fields that is triggered whenever the user \nexplicitly activates \"Show Errors\" or when the current data file is exported to a final submission format.\nRecordModelValidationService Record Implementing classes provide a validation service that is activated whenever the user tries to commit \nchanges to the current record.\nDocumentValidationService Document Implementing classes provide a validation service that is triggered whenever the user explicitly activates \n\"Show Errors\" or when the current data file is exported to a final submission format.Page 11 of 15\n(page number not for citation purposes)\nBMC Bioinformatics 2008, 9:183 http:\/\/www.biomedcentral.com\/1471-2105\/9\/183took place) and tree lists (for example, to select the repos-\nitory within which a particular data holding resides) are\nused. An example of the latter is to select the type of data\nbeing catalogued; a hierarchy of data types was discussed\nand agreed within the data centre, and the master copy of\nthis file is stored in a CVS repository. A disadvantage of\nthis approach is that, because data may be annotated on\nmany different machines, changes to any ontology file\nmust be distributed to all these machines. An alternative\narrangement for networked machines would be to use a\nplug-in that loads and caches terms from a server, remov-\ning the need for manual updates.\nOne situation where the basic ontology services proved\ninsufficient was for browsing the NCBI taxonomy data-\nbase, which has the following features:\n\u2022 The taxonomy is large, with over 235,000 nodes.\n\u2022 An organism is only put into the taxonomy if it is repre-\nsented by some accession in the nucleotide sequence data-\nbase and, therefore, species are continually being added.\n\u2022 Each taxon has a canonical unique name within the\ndatabase. Many species have one or more pseudonyms,\nand these names may be used by the researchers or in the\nliterature, but we must record the official NCBI name.\nThe NCBI taxonomy is available as a flat file, suitable for\nloading into a relational database, or via a web services\ninterface. For the reasons given above, particularly in rela-\ntion to the size of the taxonomy, the standard ontology\nbrowsers within Pedro were insufficient. Thus a plug-in\nwas developed, known as TaxInspector, for browsing a\nlocally held copy of the database. TaxInspector retrieves\nand loads the flat files from NCBI into the local database,\nand then allows direct browsing over the taxonomy tree,\nsearching over taxon names, common names and pseudo-\nnyms, and a quick access list to some of the most common\nspecies. When a taxon is selected, details of the selection\nare shown, as well as the full path from the root of the tree\nto that node.\nTaxInspector can be run as a standalone Java application,\nbut the ontology interface provided by Pedro made it\nstraightforward to have the program run as a plug-in.\nWhen a user right-clicks on the \"species_name\" heading,\nthe TaxInspector GUI appears. Once the desired taxon has\nbeen located, the user clicks on the \"Use Term\" button to\ninsert the selection into Pedro. TaxInspector is shown in\nFigure 8.\nThe user group for Pedro in EnvBase consists of a small\ncentral team of curators and, periodically, members of the\nprojects whose results are recorded in the catalogue. The\nimpression is that, although there is an initial learning\ncurve, the tool has been well received, with little training\nrequired. However, users do need to go through an initial\nfamiliarisation process. Pedro has been designed princi-\npally for use with complex data sets; as a result, it contains\nquite rich functionality and can seem complicated com-\npared with typical online data entry forms, such as those\nthat capture conference registration information. For\nexample, Pedro allows data to be captured in different\norders and supports the saving of partial data sets, so val-\nidation is less straightforward than in linear data entry sys-\ntems, where all validation at one step is completed before\nmoving on to the next step. In Pedro, some validation\ninformation, for example on mandatory fields, is always\nvisible and users can perform global validation checks.\nHowever, different users may prefer different approaches\nto validation, and the presence of explicit functionality to\nrequest validation is an example of a feature that must be\nlearned, and for which an effective mode of operation\nmust be developed by users.\nPedro as a myGrid Workbench Plug-in\nThe myGrid project [21] is developing service-based mid-\ndleware and applications to support biological in silico\nexperiments. In silico experimentation requires the inte-\ngration of heterogeneous, disparate and autonomous bio-\ninformatics resources, i.e. data and analysis tools,\navailable on the web. The myGrid environment allows sci-\nentist to discover resources, orchestrate them into stored\nworkflows, enact these workflows, and manage and pub-\nlish the results.\nTable 5: Plug-ins for the Beacon Database\nPlug-in Function\nImport\/Export These plug-ins allow the import and export of \"Dye\", \"Plasmid\", \"Characteristic\", \"Person\" and \"Experiment\" records \nfrom the database.\nEquipment Import Plug-in Reads a microscope database file and populates the record based upon this.\nSpotted Import Plug-in Reads a spotter program file and associated microscope database file and populates a record based upon these.\nSpot Export Plug-in Allows the export of a single spot record to a file.\nTracker Import Plug-in Reads the XML output from Cell Tracker and adds this to a record.\nExcel Import Plug-in Reads the Excel output from Cell Tracker and adds this to a record.Page 12 of 15\n(page number not for citation purposes)\nBMC Bioinformatics 2008, 9:183 http:\/\/www.biomedcentral.com\/1471-2105\/9\/183In myGrid, Pedro has been used as part of resource discov-\nery [22]. The number of bioinformatics resources availa-\nble to the scientist is large (~3000) and increasing rapidly.\nThese resources, generally exposed via web services, nor-\nmally provide only limited descriptions of their function-\nality, their expected inputs and the outputs they produce.\nAs a result, finding suitable resources for inclusion in a\nworkflow poses a challenge. To address this, myGrid pro-\nvides an ontological model for representing the bioinfor-\nmatics domain and service properties, over which a\nregistry has been developed that allows storage and query-\ning of service descriptions. As part of this framework,\nPedro has been used as an annotation tool to help myGrid\nusers create service descriptions. Overall, to support this\napplication, a data capture infrastructure must support\nrequirements R1, R2, R3, R4 and R5, as listed previously.\nTo ease the overall service annotation process for users,\nPedro has been integrated into the myGrid workbench\nenvironment, rather than being used as a stand-alone\ntool. As part of the integration, the Pedro ontology serv-\nices have been extended to support RDFS (Resource\nDescription Framework Schema [23]) ontology sources.\nAs Pedro uses XML Schema to describe domain models,\nXML is the sole serialization format supplied with Pedro.\nAs a result, where the data are to be stored other than by\nusing file-based XML, a plug-in must be developed that\ninterfaces with the required data repository.\nThe model-driven approach and the generic nature of\nPedro were exploited at the exploratory stages in the\ndevelopment of the service discovery framework. During\nthis period, the discovery system was iteratively proto-\ntyped over a changing model, and was used to evaluate\nthe service description model. In this setting, as in the cell\nimaging example, iterative development was supported\nby automatic user interface generation.\nPedro has been used by two categories of users within\nmyGrid, namely the scientists performing the in silico\nTaxInspector being called from PedroFigure 8\nTaxInspector being called from Pedro.Page 13 of 15\n(page number not for citation purposes)\nBMC Bioinformatics 2008, 9:183 http:\/\/www.biomedcentral.com\/1471-2105\/9\/183experiments and the bioinformatics domain experts\/cura-\ntors that specialize in building ontologies and service\ndescriptions. Both groups of users, particularly the non-\nexperts, found Pedro's display of a service description\nintricate in certain aspects, e.g. the tree view directly sup-\nports the tree structure in the model, and doesn't provide\nmodel-driven grouping of related elements to provide\ncustom displays. This reflects a design feature in Pedro\nwhereby there are no display models as are found in some\nother model-driven user interface development environ-\nments (e.g. [24]).\nDiscussion\nWhile the specifics of data capture vary from task to task,\nthe underlying requirements outlined in the Introduction\nare common to most of the situations encountered when\nPedro has been applied in the life sciences. Pedro goes\nsome way to fulfilling all of these requirements:\nR1. Import and export of data in appropriate formats. Pedro\nimports and exports XML, includes support for tab-delim-\nited files, and plug-ins can be used to export to other for-\nmats.\nR2. Manipulation and update of existing records. Pedro sup-\nports both creation and modification of documents\ndescribed using the domain model.\nR3. Checking of integrity of captured data. The model-driven\napproach allows captured data to be validated against the\nrelevant XML schema, and plug-ins can be used to provide\nadditional validation. This requirement is also supported\nthrough the use of ontology services.\nR4. Callable by or able to call to other applications. Although\nPedro can be used without plug-ins, data capture rarely\ntakes place in isolation from other software tools, so the\nability to call out to plug-ins, and be called down from as\na plug-in, are supported, and used frequently.\nR5. Able to use controlled vocabularies for annotation. Support\nfor user-defined ontologies is a core feature, and plug-ins\ncan be used to provide access to specialised ontology serv-\ners or browsers.\nR6. Able to selectively re-use existing data in new entries.\nPedro's models may specify data to be included in each\nrecord, removing the need for this to be entered manually.\nPedro occupies a niche, in which it seeks to support a wide\nrange of data-capture tasks through a judicious mixture of\nconfigurable properties and plug-in services. This config-\nurability enables Pedro to be deployed in diverse data-\ncapture applications, while also providing sufficient func-\ntionality out-of-the-box to enable direct use in straightfor-\nward settings. As with other model-based systems, a\nbalance has to be struck between the level of configurabil-\nity provided and the complexity of the models required to\nprovide that flexibility. In this context, Pedro seeks to con-\nstrain the complexity of model development by commit-\nting to a single type of domain model (i.e. XML schema),\na single look and feel (i.e. a tree browser and form-based\nentry), and a single type of task (i.e. data capture). How-\never, within those constraints, Pedro provides over 80\nconfiguration options, thereby enabling significant\nenhancements to be made to default behaviours, and pro-\nviding incremental tailoring to reflect user needs.\nAvailability and requirements\nThe Pedro has been available from http:\/\/sourceforge.net\/\nprojects\/pedro since April 2004, since when it has been\ndownloaded over 2,700 times. Pedro is distributed under\nthe Academic Free License. The software has been tested\non Windows-based platforms and currently depends on\nJava 1.4.\nAuthors' contributions\nDJ contributed to the development of the cell imaging\napplication, wrote the first draft of the paper and made\nsubsequent revisions. KG developed the Pedro software.\nCG contributed to development by documentation and\ntesting. TB and PA provided text on their experiences using\nPedro in EnvBase and myGrid, respectively. SGO contrib-\nuted application experience throughout the development\nof Pedro. NWP supervised the development of Pedro, and,\ntogether with SGO, refined the text.\nAcknowledgements\nResearch staff were supported as follows: DJ was funded by a DTI Beacon \nGrant; KG was supported by BBSRC grants on PEDRo and FuGE; CG was \nsupported by the DTI Beacon Grant and the BBSRC grant on FuGE; PA was \nsupported by the EU through the OntoGrid project.\nReferences\n1. Galperin MY: The Molecular Biology Database Collection:\n2007 update.  Nucl Acids Res 2007, 35:D3-4.\n2. Benson DA, Karsch-Mizrachi I, Lipman DJ, Ostell J, Wheeler DL:\nGenBank.  Nucleic Acids Research 2007, 35:D21-D25.\n3. Finn RD, Mistry J, Schuster-Bockler B, Griffiths-Jones S, Hollich V,\nLassmann T, Moxon S, Marshall M, Khanna A, Durbin R, Eddy SR, Son-\nnhammer ELL, Bateman A: Pfam: clans, web tools and services.\nNucleic Acids Research 2006, 34:D247-D251.\n4. Cherry JM, Ball C, Weng S, Juvik G, Schmidt R, Adler C, Dunn B,\nDwight S, Riles L, Mortimer RK, Botstein D: Genetic and physical\nmaps of Saccharomyces cerevisiae.  Nature 1997,\n387(6632):67-73.\n5. Hancock D, Wilson M, Velarde G, Morrison N, Hayes A, Hulme H,\nWood AJ, Nashar K, Kell DB, Brass A: maxdLoad2 and maxd-\nBrowse: standards-compliant tools for microarray experi-\nmental annotation, data management and dissemination.\nBmc Bioinformatics 2005, 6:.\n6. Brazma A, Hingamp P, Quackenbush J, Sherlock G, Spellman P,\nStoeckert C, Aach J, Ansorge W, Ball CA, Causton HC, Gaasterland\nT, Glenisson P, Holstege FCP, Kim IF, Markowitz V, Matese JC, Par-\nkinson H, Robinson A, Sarkans U, Schulze-Kremer S, Stewart J, Taylor\nR, Vilo J, Vingron M: Minimum information about a microarrayPage 14 of 15\n(page number not for citation purposes)\nBMC Bioinformatics 2008, 9:183 http:\/\/www.biomedcentral.com\/1471-2105\/9\/183Publish with BioMed Central   and  every \nscientist can read your work free of charge\n\"BioMed Central will be the most significant development for \ndisseminating the results of biomedical research in our lifetime.\"\nSir Paul Nurse, Cancer Research UK\nYour research papers will be:\navailable free of charge to the entire biomedical community\npeer reviewed and published immediately upon acceptance\ncited in PubMed and archived on PubMed Central \nyours \u2014 you keep the copyright\nSubmit your manuscript here:\nhttp:\/\/www.biomedcentral.com\/info\/publishing_adv.asp\nBioMedcentral\nexperiment (MIAME) - toward standards for microarray\ndata.  Nature Genetics 2001, 29(4):365-371.\n7. Barrett T, Edgar R: Gene expression omnibus: Microarray data\nstorage, submission, retrieval, and analysis.  DNA Microarrays,\nPart B: Databases and Statistics 2006, 411:352-369.\n8. Parkinson H, Kapushesky M, Shojatalab M, Abeygunawardena N,\nCoulson R, Farne A, Holloway E, Kolesnykov N, Lilja P, Lukk M, Mani\nR, Rayner T, Sharma A, William E, Sarkans U, Brazma A: ArrayEx-\npress - a public database of microarray experiments and\ngene expression profiles.  Nucleic Acids Research 2007,\n35:D747-D750.\n9. Spellman P, Miller M, Stewart J, Troup C, Sarkans U, Chervitz S, Bern-\nhart D, Sherlock G, Ball C, Lepage M, Swiatek M, Marks WL, Gon-\ncalves J, Markel S, Iordan D, Shojatalab M, Pizarro A, White J, Hubley\nR, Deutsch E, Senger M, Aronow B, Robinson A, Bassett D, Stoeckert\nC, Brazma A: Design and implementation of microarray gene\nexpression markup language (MAGE-ML).  Genome Biology\n2002, 3(9):research0046 .\n10. Hermjakob H, Apweiler R: The Proteomics Identifications\nDatabase (PRIDE) and the ProteomExchange Consortium:\nmaking proteomics data accessible.  Expert Review of Proteomics\n2006, 3(1):1-3.\n11. Rayner TF, Rocca-Serra P, Spellman PT, Causton HC, Farne A, Hol-\nloway E, Irizarry RA, Liu JM, Maier DS, Miller M, Petersen K, Quack-\nenbush J, Sherlock G, Stoeckert CJ, White J, Whetzel PL, Wymore F,\nParkinson H, Sarkans U, Ball CA, Brazma A: A simple spreadsheet-\nbased, MIAME-supportive format for microarray data:\nMAGE-TAB.  Bmc Bioinformatics 2006, 7:.\n12. Garwood KL, Taylor CF, Runte KJ, Brass A, Oliver SG, Paton NW:\nPedro: a configurable data entry tool for XML.  Bioinformatics\n2004, 20(15):2463-2465.\n13. Taylor CF, Paton NW, Garwood KL, Kirby PD, Stead DA, Yin ZK,\nDeutsch EW, Selway L, Walker J, Riba-Garcia I, Mohammed S, Deery\nMJ, Howard JA, Dunkley T, Aebersold R, Kell DB, Lilley KS, Roep-\nstorff P, Yates JR, Brass A, Brown AJP, Cash P, Gaskell SJ, Hubbard SJ,\nOliver SG: A systematic approach to modeling, capturing, and\ndisseminating proteomics experimental data.  Nature Biotech-\nnology 2003, 21(3):247-254.\n14. Swertz MA, Jansen RC: Beyond standardization: dynamic soft-\nware infrastructures for systems biology.  Nat Rev Genet 2007,\n8(3):235-243.\n15. Garwood K, Garwood C, Hedeler C, Griffiths T, Swainston N, Oliver\nSG, Paton NW: Model-driven user interfaces for bioinformat-\nics data resources: regenerating the wheel as an alternative\nto reinventing it.  BMC Bioinformatics 2006, 7:.\n16. Fallside DC, Walmsley P: XML Schema Part 0: Primer Second\nEdition.   [http:\/\/www.w3.org\/TR\/xmlschema-0\/].\n17. Nelson DE, Ihekwaba AEC, Elliott M, Johnson JR, Gibney CA, Fore-\nman BE, Nelson G, See V, Horton CA, Spiller DG, Edwards SW,\nMcDowell HP, Unitt JF, Sullivan E, Grimley R, Benson N, Broomhead\nD, Kell DB, White MRH: Oscillations in NF-kappa B signaling\ncontrol the dynamics of gene expression.  Science 2004,\n306(5696):704-708.\n18. Shen H, Nelson G, Nelson DE, Kennedy S, Spiller DG, Griffiths T,\nPaton N, Oliver SG, White MRH, Kell DB: Automated tracking of\ngene expression in individual cells and cell compartments.\nJournal of the Royal Society Interface 2006, 3(11):787-794.\n19. Schoning H: Tamino - A database system combining text\nretrieval and XML.  Intelligent Search on Xml Data 2003,\n2818:77-89.\n20. A.G. S: Tamino - The XML Database.   [http:\/\/www.softwar\neag.com\/tamino].\n21. Stevens RD, Tipney HJ, Wroe CJ, Oinn TM, Senger M, Lord PW,\nGoble CA, Brass A, Tassabehji M: Exploring Williams-Beuren\nsyndrome using myGrid.  Bioinformatics 2004,\n20(suppl_1):i303-310.\n22. Lord PW, Alper P, Wroe CJ, Goble CA: Feta: A Light-Weight\nArchitecture for User Oriented Semantic Service Discovery:\nHeraklion, Crete, Greece.   Springer-Verlag ; 2005:17-31. \n23. W3C: RDF Vocabulary Description Language 1.0: RDF\nSchema.   [http:\/\/www.w3.org\/TR\/2004\/REC-rdf-schema-20040210\/\n].\n24. Barclay PJ, Griffiths T, McKirdy J, Kennedy J, Cooper R, Paton NW,\nGray P: Teallach - a flexible user-interface development envi-\nronment for object database applications.  Journal of Visual Lan-\nguages and Computing 2003, 14(1):47-77.Page 15 of 15\n(page number not for citation purposes)\n"}