{"doi":"10.1080\/09687760701673618","coreId":"14124","oai":"oai:generic.eprints.org:726\/core5","identifiers":["oai:generic.eprints.org:726\/core5","10.1080\/09687760701673618"],"title":"The Online Faculty Development and Assessment System","authors":["Villar, Luis","Alegre, Olga"],"enrichments":{"references":[{"id":195446,"title":"A cross-cultural study of classroom learning environments in Australia and Taiwan,","authors":[],"date":"2000","doi":"10.1080\/095006900289994","raw":"Aldridge, J. M. & Fraser, B. J. (2000) A cross-cultural study of classroom learning environments in Australia and Taiwan, Learning Environments Research, 3, 101\u2013134.","cites":null},{"id":447082,"title":"An investigation of broad and narrow personality traits in relation to general and specific life satisfaction of college students,","authors":[],"date":"2005","doi":"10.1007\/s11162-004-4140-6","raw":"Lounsbury, J. W., Saudargas, R. A., Gibson, L. W. & Leong, F. T. (2005) An investigation of broad and narrow personality traits in relation to general and specific life satisfaction of college students, Research in Higher Education, 46(6), 707\u2013729.","cites":null},{"id":195455,"title":"Assessing enhancement of learning, personal learning environment, and student efficacy: alternatives to traditional faculty evaluation in higher education,","authors":[],"date":"1997","doi":null,"raw":"Ellett, C. D., Loup, K. S., Culcross, R. R., McMullen, J. H. & Rugutt, J. K. (1997) Assessing enhancement of learning, personal learning environment, and student efficacy: alternatives to traditional faculty evaluation in higher education, Journal of Personnel Evaluation in Education, 11(2), 167\u2013192.","cites":null},{"id":447079,"title":"Classroom environment instruments: development, validity and applications,","authors":[],"date":"1998","doi":null,"raw":"Fraser, B. J. (1998) Classroom environment instruments: development, validity and applications, Learning Environments Research, 1, 7\u201333.","cites":null},{"id":195453,"title":"Classroom participation and discussion effectiveness: student-generated strategies,","authors":[],"date":"2004","doi":"10.1080\/0363452032000135805","raw":"Dallimore, E. J., Hertenstein, J. H. & Platt, M. B. (2004) Classroom participation and discussion effectiveness: student-generated strategies, Communication Education, 53(1), 103\u2013115.","cites":null},{"id":1042184,"title":"Course evaluation on the web: facilitating student and teacher reflection to improve learning, New Directions for Teaching and Learning,","authors":[],"date":"2003","doi":"10.1002\/tl.125","raw":"Tucker, B., Jones, S., Straker, L. & Cole, J. (2003) Course evaluation on the web: facilitating student and teacher reflection to improve learning, New Directions for Teaching and Learning, 96, 81\u201393.","cites":null},{"id":195450,"title":"Developing a taxonomy of faculty participation in asynchronous learning environments-an exploratory investigation,","authors":[],"date":"2003","doi":"10.1016\/s0360-1315(03)00033-2","raw":"Blignaut, S. & Trollip, S. R. (2003) Developing a taxonomy of faculty participation in asynchronous learning environments-an exploratory investigation, Computers and Education, 41, 149\u2013172.","cites":null},{"id":195447,"title":"Developing globally-competent university teachers,","authors":[],"date":"2000","doi":"10.1080\/13558000050138489","raw":"Badley, G. (2000) Developing globally-competent university teachers, Innovations in Education and Training International, 37(3), 244\u2013253.","cites":null},{"id":447080,"title":"Does the use of student feedback questionnaires improve the overall quality of teaching?,","authors":[],"date":"2002","doi":"10.1080\/0260293022000009294","raw":"Kember, D., Leung, D. Y. P. & Kwan, K. P. (2002) Does the use of student feedback questionnaires improve the overall quality of teaching?, Assessment and Evaluation in Higher Education, 27(5), 411\u2013425.","cites":null},{"id":447084,"title":"Exploring technology-mediated learning from a pedagogical perspective,","authors":[],"date":"2003","doi":"10.1076\/ilee.11.2.111.14136","raw":"Oliver, R. & Herrington, J. (2003) Exploring technology-mediated learning from a pedagogical perspective, Interactive Learning Environments, 11(2), 111\u2013126.","cites":null},{"id":1042188,"title":"Faculty study groups: solving \u2018good problems\u2019 through study, reflection, and collaboration,","authors":[],"date":"2000","doi":"10.1023\/b:ihie.0000047413.00693.8c","raw":"Wildman, T. M., Hable, M. P., Preston, M. M. & Magliaro, S. G. (2000) Faculty study groups: solving \u2018good problems\u2019 through study, reflection, and collaboration,  Innovative Higher Education, 24(4), 247\u2013263.","cites":null},{"id":1042180,"title":"Flying not flapping: a strategic framework for e-learning and pedagogical innovation in higher education institutions,","authors":[],"date":"2005","doi":"10.3402\/rlt.v13i3.11218","raw":"Salmon, G. (2005) Flying not flapping: a strategic framework for e-learning and pedagogical innovation in higher education institutions, ALT-J, Research in Learning Technology, 13(3), 201\u2013218.","cites":null},{"id":447078,"title":"Jumping the hurdles: challenges of staff development delivered in a blended learning environment,","authors":[],"date":"2004","doi":"10.1080\/1358165042000186253","raw":"Fitzgibbon, K. M. & Jones, N. (2004) Jumping the hurdles: challenges of staff development delivered in a blended learning environment, Journal of Educational Media, 29(1), 25\u201335.","cites":null},{"id":1042187,"title":"Learning environment perceptions of European university students,","authors":[],"date":"1999","doi":null,"raw":"Wierstra, R. F. A. (1999) Learning environment perceptions of European university students, Learning Environments Research, 2(1), 79\u201398.","cites":null},{"id":195451,"title":"Online collection of midterm student feedback,","authors":[],"date":"2003","doi":"10.1002\/tl.126","raw":"Bullock, C. D. (2003) Online collection of midterm student feedback, New Directions for Teaching and Learning, 96, 95\u2013101.","cites":null},{"id":195448,"title":"Online evaluations of teaching: an examination of current practice and considerations for the future, New Directions for Teaching and Learning,","authors":[],"date":"2003","doi":"10.1002\/tl.127","raw":"Ballantyne, C. (2003) Online evaluations of teaching: an examination of current practice and considerations for the future, New Directions for Teaching and Learning, 96, 103\u2013112.","cites":null},{"id":195452,"title":"Professional development for faculty. A conceptual framework of barriers and supports,","authors":[],"date":"1999","doi":null,"raw":"Caffarella, R. S. & Zinn, L. F. (1999) Professional development for faculty. A conceptual framework of barriers and supports, Innovative Higher Education, 23(4), 241\u2013254.","cites":null},{"id":1042186,"title":"Programa para la Mejora de la Docencia Universitaria (Madrid,","authors":[],"date":"2004","doi":"10.4272\/978-84-9745-081-2.ch1","raw":"Villar, L. M. (2004)  Programa para la Mejora de la Docencia Universitaria  (Madrid, Pearson\/ Prentice Hall).","cites":null},{"id":447085,"title":"Reconceptualizing the evaluation of teaching in higher education,","authors":[],"date":"1997","doi":null,"raw":"Pratt, D. D. (1997) Reconceptualizing the evaluation of teaching in higher education, Higher Education, 34(1), 23\u201344.","cites":null},{"id":1042185,"title":"Requirements for an assessment procedure for beginning teachers: implications from recent theories on teaching and assessment,","authors":[],"date":"2002","doi":"10.1111\/1467-9620.00162","raw":"Uhlenbeck, A. M., Verloop, N. & Beijaard, D. (2002) Requirements for an assessment procedure for beginning teachers: implications from recent theories on teaching and assessment, Teachers College Record, 104(2), 242\u2013272.","cites":null},{"id":195449,"title":"Students\u2019 perceptions of and satisfaction with group grades and the group experience in the college classroom, Assessment and Evaluation","authors":[],"date":"2003","doi":"10.1080\/0260293032000066191","raw":"Barfield, R. L. (2003) Students\u2019 perceptions of and satisfaction with group grades and the group  experience in the college classroom,  Assessment and Evaluation in Higher Education, 28(4), 49\u201364.","cites":null},{"id":1042183,"title":"The development and validation of a framework for teaching competencies in higher education,","authors":[],"date":"2004","doi":"10.1023\/b:high.0000034318.74275.e4","raw":"Tigelaar, D. E. H., Dolmans, D. H. J. M., Wolfhagen, I. H. A. P. & Van Der Vleuten, B. C. P. M. (2004) The development and validation of a framework for teaching competencies in higher education, Higher Education, 48, 253\u2013268.","cites":null},{"id":1042189,"title":"The impact of student perceptions and characteristics on teaching evaluations: a case study in finance education,","authors":[],"date":"2002","doi":"10.1080\/02602930120105054","raw":"Worthington, A. C. (2002) The impact of student perceptions and characteristics on teaching evaluations: a case study in finance education, Assessment and Evaluation in Higher Education, 27(1), 49\u201364.","cites":null},{"id":1042181,"title":"The use of self, peer and teacher assessment as a feedback fystem in a learning environment aimed at fostering skills of cooperation in an entrepreneurial context, Assessment and Evaluation","authors":[],"date":"2004","doi":"10.1080\/0260293042000188465","raw":"Schelfhout, W., Dochy, F. & Janssens, S. (2004) The use of self, peer and teacher assessment as a feedback fystem in a learning environment aimed at fostering skills of cooperation in an entrepreneurial context, Assessment and Evaluation in Higher Education, 29(2), 177\u2013201.230 L. M. Villar and O. M. Alegre Thomas, E. H. & Galambos, N. (2004) What satisfies students? Mining student-opinion data with regression and decision tree analysis, Research in Higher Education, 45(3), 251\u2013269.","cites":null},{"id":447083,"title":"Using a Web-based course-management system. An evaluation of management tasks and time implications for the instructor, Evaluation and Programme Planning,","authors":[],"date":"2003","doi":"10.1016\/s0149-7189(03)00005-3","raw":"Nijhuis, G. G. & Collis, B. (2003) Using a Web-based course-management system. An evaluation of management tasks and time implications for the instructor,  Evaluation and Programme Planning, 26, 193\u2013201.","cites":null},{"id":195454,"title":"Validation and use of an instrument to assess university-level psychosocial environment in Australian universities,","authors":[],"date":"2000","doi":"10.1080\/030987700112291","raw":"Dorman, J. P. (2000) Validation and use of an instrument to assess university-level psychosocial environment in Australian universities, Journal of Further and Higher Education, 24(1), 25\u201338.","cites":null},{"id":447081,"title":"What makes a student group successful? Student\u2013student and student\u2013teacher interaction in a problem-based learning environment,","authors":[],"date":"2003","doi":null,"raw":"Lindblom-Yl\u00e4nne, S., Pihlajam\u00e4ki, H. & Kotkas, T. (2003) What makes a student group successful? Student\u2013student and student\u2013teacher interaction in a problem-based learning environment, Learning Environments Research, 6(1), 59\u201376.","cites":null},{"id":1042182,"title":"What satisfies students? Mining student-opinion data with regression and decision tree analysis,","authors":[],"date":"2004","doi":"10.1023\/b:rihe.0000019589.79439.6e","raw":null,"cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2007-09","abstract":"This article evaluates the role of the Online Faculty Development and Assessment System (OFDAS), created at universities in the Canary Islands, Spain, in staff development. The evaluation indicates that the system helped staff in learning to teach curriculum and teaching capacities. The tasks, online resources and opportunities for discussions provided within the learning environment created for the system helped shape their attitudes towards learning curriculum and teaching capacities and enabled them to share their concerns about students\u2019 classroom learning environment assessment","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/14124.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/726\/1\/ALT_J%2DVol15_No3_2007_The_Online_Faculty_Development.pdf","pdfHashValue":"a4a49ec1d274132724a3abec6a89e694331b5247","publisher":"Taylor and Francis Ltd","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:726<\/identifier><datestamp>\n      2011-04-04T09:01:10Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/726\/<\/dc:relation><dc:title>\n        The Online Faculty Development and Assessment System<\/dc:title><dc:creator>\n        Villar, Luis<\/dc:creator><dc:creator>\n        Alegre, Olga<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        This article evaluates the role of the Online Faculty Development and Assessment System (OFDAS), created at universities in the Canary Islands, Spain, in staff development. The evaluation indicates that the system helped staff in learning to teach curriculum and teaching capacities. The tasks, online resources and opportunities for discussions provided within the learning environment created for the system helped shape their attitudes towards learning curriculum and teaching capacities and enabled them to share their concerns about students\u2019 classroom learning environment assessment.<\/dc:description><dc:publisher>\n        Taylor and Francis Ltd<\/dc:publisher><dc:date>\n        2007-09<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/726\/1\/ALT_J-Vol15_No3_2007_The_Online_Faculty_Development.pdf<\/dc:identifier><dc:identifier>\n          Villar, Luis and Alegre, Olga  (2007) The Online Faculty Development and Assessment System.  Association for Learning Technology Journal, 15 (3).  pp. 217-230.  ISSN 0968-7769 (print)\/1741-1629 (online)     <\/dc:identifier><dc:relation>\n        10.1080\/09687760701673618<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/726\/","10.1080\/09687760701673618"],"year":2007,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"ALT-J, Research in Learning Technology\nVol. 15, No. 3, September 2007, pp. 217\u2013230\nISSN 0968-7769 (print)\/ISSN 1741-1629 (online)\/07\/030217\u201314\n\u00a9 2007 Association for Learning Technology\nDOI: 10.1080\/09687760701673618\nThe Online Faculty Development and \nAssessment System\nLuis M. Villara* and Olga M. Alegreb\naUniversity of Seville, Spain;  bUniversity of La Laguna, Spain\nTaylor and Francis LtdCALT_A_267211.sgm10.1080\/09687760701673618ALT-J, Research in Learning Technology0968 7769 (pri t)\/1741-1629 (onli e)Original Article2 07 & Fran is53 000D c mber 2007Lu sVil armvi lar@u .es\nThis article evaluates the role of the Online Faculty Development and Assessment System (OFDAS),\ncreated at universities in the Canary Islands, Spain, in staff development. The evaluation indicates\nthat the system helped staff in learning to teach curriculum and teaching capacities. The tasks, online\nresources and opportunities for discussions provided within the learning environment created for\nthe system helped shape their attitudes towards learning curriculum and teaching capacities and\nenabled them to share their concerns about students\u2019 classroom learning environment assessment.\nIntroduction\nAs with other European higher education institutions, Spanish universities have expe-\nrienced an exponential increase in the number of staff professional development\ncourses offered online. At two public Spanish universities in the Canary Islands,\ndistance education initiatives and faculty development courses have been used\nrecently in targeted disciplinary subjects. The Online Faculty Development and\nAssessment System (OFDAS) is a voluntary education programme designed to\nenhance practitioners\u2019 pedagogical awareness and understanding. It includes topics\nsuch as teaching beliefs, educational quality and pedagogical excellence in the univer-\nsity profession (Caffarella & Zinn, 1999). It also includes topics such as planning,\norganizing, structuring, tracking, reporting and communicating assessments (Nijhuis\n& Collis, 2003). As discussed by Fitzgibbon and Jones (2004), the social dimensions\nand organizational factors of online faculty professional learning are also important\nand were taken into account when creating the system. The instructional design\napproach used in the OFDAS was guided by the principles of instructional systems\ndesign (Oliver & Herrington, 2003), considering both the technical and pedagogical\nfactors associated with the development of quality digital materials.\n*Corresponding author. Dpto. Did\u00e1ctica y Organizaci\u00f3n Educativa, Facultad de Ciencias de la\nEducaci\u00f3n, University of Seville, C\/Camilo Jos\u00e9 Cela s\/n, 41018 Sevilla, Spain. Email: mvillar@us.es\n218 L. M. Villar and O. M. Alegre\nThe two core elements of the OFDAS (teaching and assessment) were designed to\nhelp practitioners to rethink their pedagogical approach to teaching and supporting\nstudents. The teaching aspects included the design, facilitation and direction of an\nintegrated set of knowledge, beliefs, abilities and attitudes that are fundamental to\ngood teaching in a university context\u2014these are referred in this paper collectively as\ncurriculum and teaching capacities (CTC).\nAim and objectives of the research\nThe aim of our research was to develop and validate an online framework of CTC\ncalled the OFDAS as an intelligent tutoring system. We set out to examine three objec-\ntives: the learning experiences of faculty in the online system, evaluation of what the\nparticipants learnt about CTC, and students\u2019 evaluation of the classroom learning\nenvironment. The overall research question was \u2018how did the OFDAS elements affect\nstaffs\u2019 opinions and what was its impact on teaching attitudes and on the resultant\nlearning environment?\u2019 This question was examined via three subquestions (Figure 1). \n1. Was there a difference in the practitioners\u2019 opinion about the quality of the\nOFDAS?\n2. Did the practitioners learn how to teach CTC having completed the OFDAS?\n3. Was there a relationship between the teachers\u2019 attitudes and students\u2019 learning\nenvironment after completion of the OFDAS?\nFigure 1. The OFDAS model and sample variables\nThe OFDAS model\nThe OFDAS model consisted of the following elements: \n(a) A four-hour, face-to-face mentors and participants workshop.\n(b) A sequence of structured and comprehensive lessons. Learning activities were\ndesigned to engage and direct the participants through the process of CTC\nknowledge acquisition, ensuring that the development of the instructional CTC\ntransferred to practical classroom settings.\n(c) A communication support system to scaffold the teaching and learning process,\nas well as a mechanism for providing meaningful feedback and sharing of ideas\/\nproblems with colleagues.\n(d) A set of associated learning resources to complement the learning activities and\nto provide guidance.\n(e) A specific inventory of students\u2019 evaluation of the classroom-learning environment\ngenerated by the participants to provide them with feedback on matters relating\nto participant instructors\u2019 learning.\nSome of the key features of the multimedia platform (http:\/\/gid.us.es:8083) are\ndescribed here. Faculty worked through the site in the following ways: \n1. They used a CTC handbook (Villar, 2004), which was based on a review of\nreferences on college teaching and included 30 critical CTC related to class prep-\naration, classroom structure and organization.\nOnline Faculty Development and Assessment System 219\n2. They interpreted resource materials that were divided into 10 CTC lessons,\nreleased on a weekly basis. These focused on seven basic dimensions, similar to\nthe teaching competencies framework proposed by Tigelaar et al. (2004). The\nstructure of a CTC was described as having a four-phase cycle: purpose, uses,\neducational setting and case study. The resources formed a substantial body of\nmaterial, including 156 pdf and html documents (on relevant teaching and learn-\ning facts, concepts and theories), 114 web sites, 10 PowerPoint presentations,\nand over 500 glossary items on relevant educational concepts and references with\nembedded hyperlinked in the materials. Mentors also used voice-over Internet\nprotocols (see Service and Content Information in Figure 1).\n3. Participants were also expected to take part in two discussion topics in the asyn-\nchronous forums: \u2018European convergence issues\u2019 and \u2018Student mental effort to\ncope with the new European credit system\u2019. These themes were organized and\nreleased on a fortnightly basis, but remained accessible throughout the remainder\nof the course. The last forum included reflective questions. The forums were a\n\u0000\u0001\u0002\u0003\n\u0004\u0005\u0006\u0007\b\t\n\u000b\u0005\u0006\f\u0005\r\n\u0002\u0007\u000b\b\u0005\f\u0005\r\n\u000e\f\t\u000f\u0010\t\t\f\u0011\u0005\n\u0013\u0007\r\u0011\n\f\u000b\n\f\u0011\u0005\n\u0014\u0015\u000b\u0016\u0010\u000b\n\f\u0011\u0005\n\u0000\u0004\u0017\u0000\u0003\n\u0018\u0011\n\f\u0015\u000b\n\f\u0011\u0005\n\u0019\u0005\u0015\u0011\u0016\u0015\u0007\u001a\u0007\u0005\n\n\u001b\u000f\u000b\u001c\u001c\u0011\u0016\u0006\f\u0005\r\n\u0001\u0016\f\u001a\u000b\n\u0007\n\u0001\u0016\u000b\b\f\u001c\f\u000f\u000b\n\f\u0011\u0005\n\u0004\t\u0007 \u0011\u001c \b\u0007\t\u0011\u0010\b\u000f\u0007\t\n\u001d\u001e\u000e\u0000\u001b \u001d\u0010\n\u000f\u0011\u001a\u0007\n\u001d\u001e\u000e\u0000\u001b \u000e\u0007\u0016\f\u0015\u0007\b\u001f\n\u0003\u0010\u000b\u0016\f\n\u001f \t\u000f\u000b\u0016\u0007\n \u0007\u0016\u0007\u0015\u000b\u0005\u000f\u0007\n\u0004\t\u0007\u001c\u0010\u0016\u0005\u0007\t\t\n\u0000!!\b\u0011!\b\f\u000b\n\u0007\u0005\u0007\t\t\n\u0000\u0006\u000b!\n\u000b\n\f\u0011\u0005\n\u0017\f!\t\n\u001b\n\b\u0010\u000f\n\u0010\b\u0007\n\"\u0007\b\n\f\u0005\u0007\u0005\u000f\u0007\n \u0007\u000b\u0006\f\u0005\r\n\u0019\u001a!\u000b\u000f\n\n\u0017\f\u001a\u0007#\u0001\u0011\u0005\t\u0010\u001a\f\u0005\r\n\u0000\u000f\n\f\u0015\f\n\f\u0007\t\n$$% \u000b\t\t\f\r\u0005\u001a\u0007\u0005\n\t& !\b\u000b\u000f\n\f\u000f\u0007\n!\b\u0011'\u0016\u0007\u001a\t( \u000f\u000b\t\u0007 \t\n\u0010\u0006\f\u0007\t( \u0007\n\u000f)\n\u0018\u0007\u000b\t\u0010\b\u0007\u001a\u0007\u0005\n \n\u0007\t\n\t\n$% !\b\u000b\u000f\n\f\u000f\u0007 \u0007*\u000b\u001a\t\n\u001d\u001e\u000e\u0000\u001b \u000e\u0007\t\f\r\u0005\n\u0001\u0011\u0005\n\u0007\u0005\n \u0019\u0005\u001c\u0011\b\u001a\u000b\n\f\u0011\u0005\n\u0001\u0017\u0001$) +\u0005\u0011,\u0016\u0007\u0006\r\u0007 \u0011\u001c \t\n\u0010\u0006\u0007\u0005\n\n\u001a\u0011\n\f\u0015\u000b\n\f\u0011\u0005 \u000b\u0005\u0006 \u000b'\f\u0016\f\n\u001f \n\u0011 !\b\u0011\u001a\u0011\n\u0007\n\t\n\u0010\u0006\u0007\u0005\n\t- !\u0011\t\f\n\f\u0015\u0007 \u000b\n\n\f\n\u0010\u0006\u0007\t\n\u0001\u0017\u0001.) \u0000,\u000b\b\u0007\u0005\u0007\t\t \u0011\u001c \t\n\u0010\u0006\u0007\u0005\n\t-\n\u0006\f\u0015\u0007\b\t\f\n\u001f \f\u0005 \u000b\u0016\u0016 \f\n\t \u001c\u0011\b\u001a\t\n\u0001\u0017\u0001\/) \u0001\u000b!\u000b\u000f\f\n\u001f \n\u0011 \t\u0011\u0016\u0015\u0007 \t\n\u0010\u0006\u0007\u0005\n\t-\n!\b\u0011'\u0016\u0007\u001a\t\n\u0001\u0017\u00010) \u0001\u000b!\u000b\u000f\f\n\u001f \n\u0011 \u0006\u0007\u0015\u0007\u0016\u0011!\n\u001a\u0007\n\u000b\u000f\u0011\r\u0005\f\n\f\u0015\u0007 \t1\f\u0016\u0016\t \f\u0005 \n2\u0007 \n\b\u000b\f\u0005\u0007\u0007\n\u0001\u0017\u00013) \u0001\u000b!\u000b\u000f\f\n\u001f \n\u0011 !\b\u0011\u0015\f\u0006\u0007 \u0007\u001c\u001c\u0007\u000f\n\f\u0015\u0007\n\u000b\u0005\u0006 \u001c\b\u0007\u0007 \u000f\u0010\b\b\f\u000f\u0010\u0016\u0010\u001a \n\f\u001a\u0007\n\u0001\u0017\u00014) +\u0005\u0011,\u0016\u0007\u0006\r\u0007 \u0011\u001c \u000b\b\u0007\u000b '\u0007\f\u0005\r\n\t\u0010!\u0007\b\u0015\f\t\u0007\u0006 5\u0016\u0007\u000b\b\u0005\f\u0005\r \n\u000b\t1\t( \b\u0007\t\u0007\u000b\b\u000f2(\n\u000b\t\t\u0007\t\t\u001a\u0007\u0005\n( \u0007\n\u000f)6\n\u0001\u0017\u00017) \u0017\u0007\u000b\u000f2\f\u0005\r \u000b\u0005\u0006 \u0006\f\u0006\u000b\u000f\n\f\u000f \t1\f\u0016\u0016\t \u001c\u0011\b\n\u0016\u000b\b\r\u0007 \r\b\u0011\u0010!\t\n\u0001\u0017\u00018) +\u0005\u0011,\u0016\u0007\u0006\r\u0007 \u0011\u001c 9\u0010\u0007\t\n\f\u0011\u0005\f\u0005\r\n\t1\f\u0016\u0016\t\n\u0001\u0017\u0001:) +\u0005\u0011,\u0016\u0007\u0006\r\u0007 \u0011\u001c \u001c\u0011\b\u001a\u000b\n\f\u0015\u0007 \u000b\u0005\u0006\n\t\u0010\u001a\u001a\u000b\n\f\u0015\u0007 \u0007\u0015\u000b\u0016\u0010\u000b\n\f\u0011\u0005\n\u0001\u0017\u0001$%) \u0001\u000b!\u000b\u000f\f\n\u001f \n\u0011 \u000f\u0011\u0005\u0006\u0010\u000f\n \u0011,\u0005 \t\u0007\u0016\u001c#\n\u000b\t\t\u0007\t\t\u001a\u0007\u0005\n !\b\u0011\u000f\u0007\t\t\n\u001b\u0007\b\u0015\f\u000f\u0007\n \u0007\t!\u0011\u0005\t\f\u0015\u0007 5;\u0011\u0019\" !\b\u0011\n\u0011\u000f\u0011\u00166\n\u0014\u001c\u001c\u0007\u000f\n\f\u0015\u0007\u0016\u001f !\b\u0007\t\u0007\u0005\n\u0007\u0006\n\u0014\u000b\t\u001f#\n\u0011#\u0010\t\u0007 5\u001a\u0011\u0011\u0006\u0016\u00076\n\u0004!#\n\u0011#\u0006\u000b\n\u0007\n\u0001\u0016\u0007\u000b\b\u0016\u001f ,\b\f\n\n\u0007\u0005\n\u0017\u0010\n\u0011\b\f\u000b\u0016\n\u001e\u0011\b\u0010\u001a\t\n\u001b\n\u0010\u0006\u0007\u0005\n \u001c\u0007\u0007\u0006'\u000b\u000f1\n \u0003\n$\n \u0003\n.\n \u0003\n\/\nFigure 1. The OFDAS model and sample variables\n220 L. M. Villar and O. M. Alegre\ncore element of our design approach as we believed that participation in the\nforums was a crucial element as it fostered collaboration, social dialogue and\nnegotiation of meaning between the participants. Blignaut and Trollip (2003,\np. 152) note that \u2018Determining the elements of faculty participation and involve-\nment can lead to the development of improved skills, which in turn may lead to\nimproved learner satisfaction, instructor satisfaction, and the lowering of attrition\nrates\u2019.\n4. They used email for one-to-one interactions with mentors or other participants.\n5. They could browse the curriculum materials and resources containing URL links\nto related articles and institutions, notes and grades from any location, at a time\nand pace that suited them.\n6. They could download the presentations, key concept maps, study guides and\nresources onto their personal computer.\n7. They were able to submit their learning activity assignments via a Web-forms\ninterface or by email. These assignments were designed to be authentic activities\nthat had real-university relevance linked to classroom practice and that presented\ncomplex teaching\u2013learning tasks to be completed over a sustained period of time.\nThe assessed activities aimed to be realistic representations of the tasks in which\nthe authors wanted to demonstrate capacity; and therefore participants had\nsubstantial freedom in selecting activities relevant to them. This is in line with the\nliterature on the importance of authentic assessment (Uhlenbeck et al., 2002).\n8. Participants were expected to complete 10 online tests; the answers were\nrecorded in a database on the server. Each CTC test was programmed via\nrandom selection to be unique and provided instant feedback to the participants.\nParticipants evaluated the quality of instructional materials and of the training\nprocess as a formative evaluation for course revision, so that an authentic assess-\nment was included, which was seamlessly integrated into the learning activity\nassignments, and which provided a formative assessment of their understanding\nof basic concepts and theories, aiding them to gain a sense of progress.\n9. They were asked to provide evaluative feedback on the course and the multime-\ndia OFDAS environment by completely the Attitude Towards Course Learning\nQuestionnaire (ACLQ) (see Table 1).\n10. Finally, they also received feedback on the students\u2019 evaluation of their classroom\nlearning environment; the students completed the Assessment of University\nTeaching Activities Questionnaire (AUTAQ) system (see Table 2).\nOverview of the literature\nFaculty competence\nOur OFDAS framework emphasizes a student-centred approach to teaching and\nlearning (Villar, 2004), which we believe is innovative and helps in terms of \u2018embed-\nding\u2019 e-learning into everyday classroom practice. The OFDAS focus was on student\nlearning experiences and processes (e.g. major concepts, assumptions, processes of\nOnline Faculty Development and Assessment System 221\ninquiry and ways of student knowing) within the university social context (Badley,\n2000). Participants were expected to have a deep understanding of their subject disci-\npline field as well as the necessary pedagogical and didactic skills. Core elements in\nthe OFDAS programme were development of participants\u2019 competence in the design\nof curriculum and course material, along with development of their generic didactic\nand guidance skills (Tigelaar et al., 2004). In addition, participants were expected to\nbe applied researchers in charge of constructing and interpreting activities, having\nbeliefs and a voice of their own (Wildman et al., 2000).\nClassroom learning environments\nThe general literature regarding students\u2019 evaluation of classroom environments has\nincreased over the past decade (Aldridge & Fraser, 2000) and the types of learning\nenvironments has undergone remarkable \u2018diversification and internationalisation\u2019\n(Fraser, 1998, p. 7). Evidence (derived largely from on-demand university teaching\nquality assessment) had been accumulating regarding the potential of classroom\nlearning environment assessments to improve university teaching and learning as well\nas staff development (Dallimore et al., 2004). In addition, some results have showed\nthat students\u2019 \u2018sense of belonging\u2019 was an important predictor of satisfaction\nmeasures (Thomas & Galambos, 2004).\nThus, a list of assumptions underpinned the development of the system. First,\nstudent evaluations are influenced by the students\u2019 demographic characteristics\nand background factors (Worthington, 2002; Barfield, 2003). Second, students\u2019\nTable 1. Description of scales and a sample item for each scale of the ACLQ\nScale Description Sample item\nUnderstanding Extent to which faculty are able to \nreconceptualize, explain and use \nreceived information about teaching\nI take time to understand the aspects of \nmy teaching in which I am mistaken\nLearning Extent to which faculty acquire \nknowledge, skills, attitudes or values, \nthrough study, experience or teaching, \nwhich lead to behavioural changes that \nare persistent, measurable and \nspecified\nI discuss mistakes on authors\u2019 articles \nand books that I read about teaching\nDiscussion Extent to which faculty use a method of \ninteraction and position \nrepresentational argument regarding \nteaching\nI point out my colleagues\u2019 teaching \nweaknesses to help them clarify their \neducational rationale\nNegotiation Extent to which faculty agree on \ncourses of action to take in teaching\nI share odd opinions about teaching \nwith colleagues\nEvaluation Extent to which faculty determine the \nmerit, worth and significance of \nteaching\nI regard teaching as a problem situation \nbecause I carefully keep in mind results \nand evidences of my subject\n222 L. M. Villar and O. M. Alegre\ninterpersonal skills are also an important factor in enhancing their academic focus\nand their perceived satisfaction with the social environment of the class\n(Lindblom-Yl\u00e4nne et al., 2003). Third, we wanted to incorporate the students\u2019\nevaluations into the OFDAS model and as such the evaluations were used as feed-\nback to the course participants (Schelfhout et al., 2004). Fourth, we also believed\nthat the students\u2019 evaluations were valuable as they represented one means of\nassessing teaching quality (Wierstra, 1999). Fifth, students\u2019 evaluation was used as\na mechanism to improve the quality process (Villar & Alegre, 2004). Finally, we\nbuilt on previously research in the field, which provides a means of validating our\napproach (Dorman, 2000).\nMethod\nThe study focused on the experiences of staff and students at two campuses. Partici-\npants in the study included academics from 24 tenured and contracted instructors\nwho enrolled on the OFDAS. All participants were volunteers and met the following\nselection criteria: located on a university campus, associated with a particular\nTable 2. Description of scales and a sample item for each scale of the AUTAQ\nScale Description Sample item\nMotivation Extent to which university students are involved \nin an innovative activity\nI am motivated to work in \nclassroom learning activities (+)\nInvolvement Student perception that university teaching is \nstudent-centred and that he\/she has been offered \nthe opportunity to make decisions on his\/her \nlearning\nThese activities have changed \nmy views on the role of \nuniversity students (+)\nScaffolding Extent to which instructors demonstrate the steps \nor structure of a problem and provide keys and \nhelp for successfully completing the activities\nThese activities relate new \ninformation to what I have \npreviously learnt (+)\nClimate Extent to which conjecture, questioning and \ndiscussion in activities are fostered, and to which \nstudents socially interact with each other to give \nmeanings to and reach agreements on teaching \nactivities and viewpoints\nThese activities encourage \nuniversity students to ask \nquestions and discuss answers \ngiven in a book (+)\nClarification Extent to which university students are given \nexplanations, examples and multiple forms of \nunderstanding a problem or difficult material\nThe instructor does not clarify \ndifficult aspects of these \nactivities (\u2212)\nUse of \nresources\nExtent to which new technological tools and other \nacademic resources facilitate university students\u2019 \ngeneration of ideas and knowledge construction\nThese activities help to develop \nother study capacities in \nuniversity students (e.g. \nhandling of tools, document \nsearch, and library use) (+)\nNote: (+), items are scored one, two, three, four and five, respectively, for the responses \u2018almost never\u2019, \n\u2018seldom\u2019, \u2018sometimes\u2019, \u2018often\u2019 and \u2018almost always\u2019; (\u2212), items are scored five, four, three, two and one, \nrespectively, for the responses \u2018almost never\u2019, \u2018seldom\u2019, \u2018sometimes\u2019, \u2018often\u2019 and \u2018almost always\u2019.\nOnline Faculty Development and Assessment System 223\ndiscipline, and had professional merits. The faculty were full-time instructors at two\npublic Canary universities: 11 from the \u2018research-led old\u2019 University of La Laguna\n(www.ull.es) (46%) and 13 from the \u2018technological-led new\u2019 University of Las\nPalmas de Gran Canaria (www.ulpgc.es) (54%); 10 participants (42%) were men\nand 14 participants (58%) were women. With regard to teaching experience, 19\nparticipants (79%) were experts (with more than five years of teaching experience)\nand 14 had a Ph.D. (58%). Participants were from a range of different disciplines:\nsocial science (eight participants), experimental sciences (five participants), health-\ncare sciences (four participants), humanities (three participants) and technical\nsciences (four participants). The OFDAS programme took place during the spring\nquarter of the 2006 academic year and lasted 11 weeks.\nAs emphasized by other researchers, faculty opinion and student perceptions are\ntwo of the best means of evaluating the quality of CTC (Pratt, 1997). Three evaluation\ntools were used: \n\u25cf CTC quality scale. This scale was used to measure participants\u2019 ability to under-\nstand the CTC and the degree to which individuals or groups wished to practice\nand apply them (Cronbach\u2019s alpha = 0.944). The scale consisted of 10 items about\nthe CTC structure, conditions, technologies and teaching practices, and was\ndesigned to assess participants\u2019 perspectives on the extent to which their personal\nCTC learning was enhanced. A five-point scale was used: one = \u2018strongly agree\u2019 to\nfive = \u2018strongly disagree\u2019 in items one to seven; items 8\u201310 had tailored five-point\nscales. The 10 items covered aspects such as the relevance, usefulness and appro-\npriateness of the CTC, suggested adaptations, tips, structure, pertinence, reading,\nimpact and time-consuming (see Figure 1, OFDAS Delivery).\n\u25cf Attitude Towards Course Learning Questionnaire. The ACLQ measure was designed\nto be used as a generic tool for the participants using CTC learning activities\n(Cronbach\u2019s alpha = 0.950). The ACLQ consisted of 20 items using a five-point\nLikert-type rating scale (one = \u2018strongly agree\u2019 to five = \u2018strongly disagree\u2019) (see\nTable 1).\n\u25cf Assessment of University Teaching Activities Questionnaire. The AUTAQ was admin-\nistered to 78 students taking courses at the two universities. The sample was repre-\nsentative of gender, age, area of study, level of study and other academic and social\ncharacteristics. The questionnaire focused on the students\u2019 perceptions of the\nclassroom learning environment (Cronbach\u2019s alpha = 0.958). The questionnaire\nconsisted of six scales and 22 items scored on a five-point Likert-type rating (see\nTable 2).\nA variety of data analyses were completed on data gathered through three instru-\nments. These included descriptive statistical summaries, alpha reliabilities of the\nsubscales of the ACLQ and AUTAQ surveys, and t-tests to compare the means of\nparticipants. In addition, a 10-point scale was applied by the two mentors to all of the\nCTC participants\u2019 activities based on an interpretation of script expressions:\n\u2018maximum distinction (9\u201310)\u2019, \u2018important for its intensity (7\u20138), \u2018suitable (5\u20136)\u2019,\n\u2018minimum qualification (3\u20134)\u2019, and \u2018differed execution (0\u20132)\u2019. Content analysis was\n224 L. M. Villar and O. M. Alegre\nchosen as a methodology for analysing the participants\u2019 online learning activities,\nwhich involved comparing, contrasting and scoring them.\nThe OFDAS was designed to make teaching and learning more of a collegial activ-\nity. Decisions about what was desirable and feasible pedagogically and technologi-\ncally were made through telephone conversations and emails. The participants\ninvolved in the programme explained the purpose of the research to their students\nand assured the students that all data would be anonymized. Ten multiple-choice\nitems were used to assess the participants\u2019 knowledge and understanding of CTC.\nOnline data were collected during the course (i.e. the CTC quality scale, CTC\nlearning tests and the ACLQ); the AUTAQ was collected after participants had\ncompleted the OFDAS programme.\nResults\nThe first research question asked participants about their opinion of the quality of\nthe materials and the OFDAS online learning environment. Responses ranged\nfrom 3.08 (item eight, reading: \u2018I read Web sites and pdf documents which were\nlinked to the capacity\u2019) to 1.33 (item one, relevance: \u2018The capacity was relevant\nfor my teaching\u2019). Standard deviations varied from 1.52 (item eight, reading) to\n0.76 (item 1, relevance). All item mean scores exceeded the midpoint scale\n(3.00, normal), and item eight (reading) exceeded the midpoint scale (3.00,\nfrequently).\nA t-test was conducted on each item. In terms of gender differences, there were\nsignificant differences in five of the CTC quality items (usefulness, adaptation, tips,\nstructure and pertinence). In terms of the level of educational achievement, a\nsignificant difference was found in eight of the CTC quality items (relevance,\nusefulness, appropriateness, adaptation, tips, structure, pertinence and time-\nconsuming). In terms of the level of teaching expertise, differences were evident for\nfive of the CTC quality items (usefulness, appropriateness, adaptation, tips and\nstructure) (see Table 3).\nQuestion two focused on whether the participants felt that the OFDAS facili-\ntated their learning in this area. This question was divided into two subcatego-\nries: facilitating learning activities and assessing the cognitive domain of CTC\nlearning.\nDescriptive summaries provided information about the activities undertaken by the\nparticipants for each of the 10 CTC. The results showed that overall the participants\ncompleted 1587 learning activities. The coaching and scaffolding for the course was\ndone by the two OFDAS mentors, who diagnosed the strengths and weaknesses of\neach participant and tailored their support accordingly. Activity transcripts were\nscored, and the scores were then actively discussed to arrive at a final version in which\nmost scored learning activities had been brought into alignment. All CTC were\napproved by participants except for the last two: CTC9, \u2018Knowledge of formative and\nsummative evaluation\u2019; and CTC10, \u2018Capacity to conduct own self-assessment\nprocess\u2019) (see Figure 2).\nOnline Faculty Development and Assessment System 225\nFigure 2. Learning activity qualificationsTest means varied from a high score of 7.4 (CTC1, \u2018Knowledge of student\nmotivation and ability to promote students\u2019 positive attitudes\u2019) to a very high score of\n10 (CTC7, \u2018Teaching and didactic skills for large groups\u2019). All 10 test means\nexceeded score seven on the 10-point scale used.\nMeans and standard deviations on the 10 self-assessment test scores are shown\nin Figure 3, which indicates that results overall were positive. However, objective\ntesting of the CTC showed that performance was more effective in CTC7,\nTable 3. Significant t-test results for demographic and academic factor comparisons\nContrast Variable t value p value\nMale versus female Usefulness 2.496 0.021\nAdaptation 2.566 0.018\nTips 3.382 0.003\nStructure 0.453 0.041\nPertinence 2.452 0.023\nDoctor versus Bachelor Relevance \u22123.246 0.003\nUsefulness \u22122.572 0.021\nAppropriateness \u22122.383 0.031\nAdaptation \u22122.456 0.022\nTips \u22123.183 0.005\nStructure \u22122.713 0.016\nPertinence \u22122,432 0.030\nTime-consuming \u22122.499 0.022\nNovel versus expert Usefulness 2.800 0.015\nAppropriateness 2.947 00.013\nAdaptation 2.725 0.018\nTips 3.253 0.007\nStructure 2.590 0.037\nFigure 2. Learning activity qualifications\n226 L. M. Villar and O. M. Alegre\n\u2018Teaching and didactic skills for large groups\u2019, than in CTC1, \u2018Knowledge of\nstudent motivation and ability to promote students\u2019 positive attitudes\u2019. There were\nsignificant gender differences in the learning of CTC3, \u2018Capacity to solve students\u2019\nproblems\u2019 (t (15) = 2.520, p < 0.018). Also, significant differences were found\nbetween instructors with and without previous educational knowledge in: CTC1,\n\u2018Knowledge of student motivation and ability to promote students\u2019 positive\nattitudes\u2019 (t (15) = \u22123.119, p < 0.008); in CTC3, \u2018Capacity to solve students\u2019\nproblems\u2019 (t (15) = \u22122.477, p < 0.027); in CTC4, \u2018Capacity to develop metacog-\nnitive skills in the trainee\u2019 (t (15) = \u22122.385, p < 0.032); in CTC7, \u2018Teaching and\ndidactic skills for large groups\u2019 (t (15) = \u22122.449, p < 0.028); and in CTC8,\n\u2018Knowledge of questioning skills\u2019 (t (15) = \u22122.590, p < 0.022). Finally, with regard\nto teaching experience, significant differences in learning CTC3, \u2018Capacity to solve\nstudents\u2019 problems\u2019, were found between novel and expert participants (t (15) =\n2.800, p < 0.015).\nFigure 3. Self-assessment test scoresThe student sample consisted of 78 undergraduate students from a variety of\ndisciplines from the two Canary Island universities. The first index of validity was\nscale reliability. Estimates of the internal consistency of the actual and preferred\nforms of each AUTAQ scale were calculated using Cronbach\u2019s alpha coefficient.\nData were reported separately for the two forms using the individual as the unit of\nanalysis. The scale values obtained for the alpha coefficient ranged from 0.083 to\n0.830. These data together suggested that each AUTAQ scale had adequate internal\nconsistency, except for \u2018Use of resources\u2019. It appeared that the AUTAQ measured\ndistinct, although somewhat overlapping, aspects of classroom environment. The\nscale values for the ACLQ obtained for the alpha coefficient ranged from 0.739 to\n0.911. These data together suggested that each ACLQ scale had adequate internal\nconsistency. Inter-correlations showed the association between the students\u2019\n02 46 8 10 12\nCTC1\nCTC2\nCTC3\nCTC4\nCTC5\nCTC6\nCTC7\nCTC8\nCTC9\nCTC10\nC\nap\nac\nit\ny \nte\nst\ns\nS.D. 1,94 1,22 0,44 0,89 2,07 1,09 0 0,44 2,34 0,89\nMeans 7,4 8 9,8 8,4 8,4 7,8 10 9,8 8,4 9,6\nCTC1 CTC2 CTC3 CTC4 CTC5 CTC6 CTC7 CTC8 CTC9 CTC10\nFigure 3. Self-assessment test scores\nOnline Faculty Development and Assessment System 227\nperceptions (AUTAQ scales) and the participants\u2019 teaching attitudes (ACLQ\nscales). Only two correlations were statistically significant (p < 0.05), negative in\ndirection and somewhat moderate: r = \u22120.548, Clarification, actual\/learning; and\nr = \u22120.548, Clarification, actual\/evaluation.\nDiscussion\nOverall participants\u2019 evaluation of the programme was positive. The manner in which\nthe mentors managed the OFDAS had a direct impact on their appreciation of the\nprogramme and also on the CTC learning results, which mirrors findings from other\nWeb-supported courses (Nijhuis & Collis, 2003). As discussed in the findings section,\nsome differences in attitudes were evidence in terms of gender, level of qualification\nand teaching experience, which supports our first research question about the\nparticipants\u2019 opinions with respect to value and ease of use of CTC in the OFDAS.\nThe usefulness of learning activities can be inferred from the fact that the partici-\npants actively engaged with the programme, completing 1587 learning activities.\nCaffarella and Zinn (1999, p. 253) pose the question \u2018Do professional development\nactivities assist in a faculty members\u2019 professional success?\u2019 We argue that from our\nstudy the answer is yes; all but two of the CTC learning activities were rated positively\nby participants (Figure 2), and all 10 CTC self-assessment test means exceeded seven\non the 10-point scale used (Figure 3). Significant differences in CTC learning were\nfound between participants in three nominal variables: gender, prior educational\nknowledge and level of teaching experience.\nThe results stressed two somewhat different but conceptually related measures\nand brought about new perspectives on assessing learning environments in higher\neducation settings. The \u2018climate\u2019 scale, particularly, emphasized the importance of\nthe development of mature, interpersonal relationships and friendships, social\nbonds, and connections with other students, as a vector of behaviour of student\ndevelopment (Lounsbury et al., 2005). The results from the AUTAQ were given\nback to participants; Kember et al. (2002) adopted a similar approach with the\nStudent Feedback Questionnaire used in their study. Correlations between the two\nmeasures used suggested that the attitudes of faculty towards teaching were not\nparticularly related to the students\u2019 perception of the classroom environment, except\non the \u2018Clarification with learning\u2019 and \u2018Evaluation\u2019 scales. Contrary to the results of\nFraser\u2019s (1998) research, the findings in our study did not support the conclusion\nthat there was a close relationship between faculty teaching attitudes and student\nclassroom perceptions.\nOverall, the findings are encouraging; all CTC were perceived to be useful and\neasy to use, although at varying levels depending on specific factors such as how\ntime-consuming they were. As Fitzgibbon and Jones (2004) report, appropriate\ncoordination of such an online programme is crucial. With directed and purposeful\ndesign efforts, and by determining which activities are best suited for different\ndisciplines, participants completing the programme are likely to enhance their\nunderstanding of CTC and be able to apply them in their own practice.\n228 L. M. Villar and O. M. Alegre\nFinally, the variables explored in this study seemed to directly address ongoing\nconcerns about the need to improve online training in higher education as well as\nemphasizing newer ideas about important variables that might be measured as\nalternatives to the more traditional approaches in the evaluation of staff develop-\nment (Ellett et al., 1997). In comparison with other questionnaires (see, for\nexample, Tucker et al., 2003), the ACLQ and AUTAQ online systems facilitated\ntimely data collection, feedback and evaluation. In general, participants reported\nthat collecting feedback online with the AUTAQ system was convenient, as had\nalready been remarked by researchers regarding other online systems (Bullock,\n2003).\nOne of the limitations found in this study was that it only examined one online\nprogramme and its use in two public urban universities over an 11-week period.\nBecause the participants volunteered their classes for inclusion in the study, these\nwere not randomly selected. Therefore it is questionable to what extent the findings\ncan be generalized to other university contexts. Another limitation was that response\nrates for the student online evaluation were low, a common problem in studies of this\nkind (see, for example, Ballantyne, 2003). Finally, it is worth noting that although the\nAUTAQ consisted of two sections, only section two was used; Barfield (2003) did use\nboth in a related study.\nConclusion\nOliver and Herrington (2003) note the value of focusing on learning tasks and the 110\nlearning activities included in the system were chosen with care to get across the key\nconcepts of the course. Good resources, however, are not enough; given the huge\ntime pressures and work commitments of staff, an online staff development system\nalso needs elements of \u2018enticement\u2019 and inclusion of appropriate support (Salmon,\n2005). We contend that one of the most important issues associated with online\ndevelopment programmes is not so much the quality of the resources, but the ways in\nwhich they are used to enable instructors from different disciplines to enhance the\nstatus and level of their pedagogical practice.\nOur evaluation has demonstrated that the OFDAS programme was an effec-\ntive tool to improve reflective practice on teaching and learning. No correlation\nwas found between staffs\u2019 teaching attitudes and students\u2019 classroom learning\nenvironment at the end of the programme. Our main conclusions about the use\nand effectiveness of the system are that: it was evident that there was positive\nmentor\u2013instructor interpersonal relationships with respect to the development of\nthe instructors\u2019 pedagogical content knowledge (e.g. instructional approaches,\nstrategies and representations of subject-specific topics); there was insufficient\ntime provided for the online professional development given the time and\ncommitment required to engage with and apply CTC to their practice; and\nthere was some personal beliefs about the association between the CTC\nonline professional development and actual student evaluation of the learning\nenvironment.\nOnline Faculty Development and Assessment System 229\nReferences\nAldridge, J. M. & Fraser, B. J. (2000) A cross-cultural study of classroom learning environments in\nAustralia and Taiwan, Learning Environments Research, 3, 101\u2013134.\nBadley, G. (2000) Developing globally-competent university teachers, Innovations in Education and\nTraining International, 37(3), 244\u2013253.\nBallantyne, C. (2003) Online evaluations of teaching: an examination of current practice and\nconsiderations for the future, New Directions for Teaching and Learning, 96, 103\u2013112.\nBarfield, R. L. (2003) Students\u2019 perceptions of and satisfaction with group grades and the\ngroup experience in the college classroom, Assessment and Evaluation in Higher Education,\n28(4), 49\u201364.\nBlignaut, S. & Trollip, S. R. (2003) Developing a taxonomy of faculty participation in asynchronous\nlearning environments-an exploratory investigation, Computers and Education, 41, 149\u2013172.\nBullock, C. D. (2003) Online collection of midterm student feedback, New Directions for Teaching\nand Learning, 96, 95\u2013101.\nCaffarella, R. S. & Zinn, L. F. (1999) Professional development for faculty. A conceptual\nframework of barriers and supports, Innovative Higher Education, 23(4), 241\u2013254.\nDallimore, E. J., Hertenstein, J. H. & Platt, M. B. (2004) Classroom participation and discussion\neffectiveness: student-generated strategies, Communication Education, 53(1), 103\u2013115.\nDorman, J. P. (2000) Validation and use of an instrument to assess university-level psychoso-\ncial environment in Australian universities, Journal of Further and Higher Education, 24(1),\n25\u201338.\nEllett, C. D., Loup, K. S., Culcross, R. R., McMullen, J. H. & Rugutt, J. K. (1997) Assessing\nenhancement of learning, personal learning environment, and student efficacy: alternatives to\ntraditional faculty evaluation in higher education, Journal of Personnel Evaluation in Education,\n11(2), 167\u2013192.\nFitzgibbon, K. M. & Jones, N. (2004) Jumping the hurdles: challenges of staff development\ndelivered in a blended learning environment, Journal of Educational Media, 29(1), 25\u201335.\nFraser, B. J. (1998) Classroom environment instruments: development, validity and applications,\nLearning Environments Research, 1, 7\u201333.\nKember, D., Leung, D. Y. P. & Kwan, K. P. (2002) Does the use of student feedback question-\nnaires improve the overall quality of teaching?, Assessment and Evaluation in Higher Education,\n27(5), 411\u2013425.\nLindblom-Yl\u00e4nne, S., Pihlajam\u00e4ki, H. & Kotkas, T. (2003) What makes a student group\nsuccessful? Student\u2013student and student\u2013teacher interaction in a problem-based learning\nenvironment, Learning Environments Research, 6(1), 59\u201376.\nLounsbury, J. W., Saudargas, R. A., Gibson, L. W. & Leong, F. T. (2005) An investigation of\nbroad and narrow personality traits in relation to general and specific life satisfaction of\ncollege students, Research in Higher Education, 46(6), 707\u2013729.\nNijhuis, G. G. & Collis, B. (2003) Using a Web-based course-management system. An evaluation\nof management tasks and time implications for the instructor, Evaluation and Programme\nPlanning, 26, 193\u2013201.\nOliver, R. & Herrington, J. (2003) Exploring technology-mediated learning from a pedagogical\nperspective, Interactive Learning Environments, 11(2), 111\u2013126.\nPratt, D. D. (1997) Reconceptualizing the evaluation of teaching in higher education, Higher\nEducation, 34(1), 23\u201344.\nSalmon, G. (2005) Flying not flapping: a strategic framework for e-learning and pedagogical\ninnovation in higher education institutions, ALT-J, Research in Learning Technology, 13(3),\n201\u2013218.\nSchelfhout, W., Dochy, F. & Janssens, S. (2004) The use of self, peer and teacher assessment as a\nfeedback fystem in a learning environment aimed at fostering skills of cooperation in an\nentrepreneurial context, Assessment and Evaluation in Higher Education, 29(2), 177\u2013201.\n230 L. M. Villar and O. M. Alegre\nThomas, E. H. & Galambos, N. (2004) What satisfies students? Mining student-opinion data with\nregression and decision tree analysis, Research in Higher Education, 45(3), 251\u2013269.\nTigelaar, D. E. H., Dolmans, D. H. J. M., Wolfhagen, I. H. A. P. & Van Der Vleuten, B. C. P. M.\n(2004) The development and validation of a framework for teaching competencies in higher\neducation, Higher Education, 48, 253\u2013268.\nTucker, B., Jones, S., Straker, L. & Cole, J. (2003) Course evaluation on the web: facilitating\nstudent and teacher reflection to improve learning, New Directions for Teaching and Learning,\n96, 81\u201393.\nUhlenbeck, A. M., Verloop, N. & Beijaard, D. (2002) Requirements for an assessment procedure\nfor beginning teachers: implications from recent theories on teaching and assessment, Teachers\nCollege Record, 104(2), 242\u2013272.\nVillar, L. M. (2004) Programa para la Mejora de la Docencia Universitaria (Madrid, Pearson\/\nPrentice Hall).\nVillar, L. M. & Alegre, O. M. (2004) Manual para la Excelencia en la Ense\u00f1anza Superior (Madrid,\nMcGraw-Hill).\nWierstra, R. F. A. (1999) Learning environment perceptions of European university students,\nLearning Environments Research, 2(1), 79\u201398.\nWildman, T. M., Hable, M. P., Preston, M. M. & Magliaro, S. G. (2000) Faculty study groups:\nsolving \u2018good problems\u2019 through study, reflection, and collaboration, Innovative Higher\nEducation, 24(4), 247\u2013263.\nWorthington, A. C. (2002) The impact of student perceptions and characteristics on teaching\nevaluations: a case study in finance education, Assessment and Evaluation in Higher Education,\n27(1), 49\u201364.\n"}