{"doi":"10.1080\/0968776042000216192","coreId":"14185","oai":"oai:generic.eprints.org:597\/core5","identifiers":["oai:generic.eprints.org:597\/core5","10.1080\/0968776042000216192"],"title":"Evaluating a virtual learning environment in the context of its community of practice","authors":["Ellaway, Rachel","Dewhurst, David","McLeod, Hamish"],"enrichments":{"references":[{"id":197005,"title":"00) From change to renewal: educational technology foundations of electronic learning envi(Open University of the Netherlands). Available online at: http:\/\/eml.ou.nl\/introducs\/koper-inaugural-address.pdf enger E.","authors":[],"date":"1991","doi":null,"raw":null,"cites":null},{"id":197002,"title":"1) The role of virtual learning environments in UK medical education. JTAP Report 623. online at: http:\/\/www.ltss.bris.ac.uk\/jules\/jtap-623.pdf","authors":[],"date":"2003","doi":null,"raw":null,"cites":null},{"id":197001,"title":"A framework for the pedagogical evaluation of virtual learning ents.","authors":[],"date":"1999","doi":null,"raw":null,"cites":null},{"id":1879265,"title":"A framework for the pedagogical evaluation of virtual learning environments. JTAP Report 41. Available online at: http:\/\/www.jtap.ac.uk\/reports\/htm\/jtap041.html","authors":[],"date":"1999","doi":null,"raw":"Britain, S. & Liber, O. (1999) A framework for the pedagogical evaluation of virtual learning environments. JTAP Report 41. Available online at: http:\/\/www.jtap.ac.uk\/reports\/htm\/jtap041.html Bruce, B. C., Peyton, J. K. & Batson, T. W. (1993) Electronic quills: a situated evaluation of using computers for writing in classrooms. Available online at: http:\/\/alexia.lis.uiuc.edu\/\u223c chip\/pubs\/ Equills\/siteval\/index.shtml Chalk, P. D. (2001) Learning software engineering in a community of practice\u2014a case study. CAL2001 Conference, Warwick. Available online at: http:\/\/www.ics.ltsn.ac.uk\/pub\/conf2001\/papers\/ Chalk.htm Cook, J. (2001) The role of virtual learning environments in UK medical education. JTAP Report 623. Available online at: http:\/\/www.ltss.bris.ac.uk\/jules\/jtap-623.pdf Ellaway, R., Dewhurst, D. & Cumming, A. (2003) Managing and supporting medical education with a virtual learning environment\u2014the Edinburgh electronic medical curriculum, Medical Teacher, 25(4), 372\u2013380.","cites":null},{"id":1879271,"title":"From change to renewal: educational technology foundations of electronic learning environments (Open University of the Netherlands). Available online at: http:\/\/eml.ou.nl\/introduction\/docs\/koper-inaugural-address.pdf","authors":[],"date":"2000","doi":null,"raw":"Koper, R. (2000) From change to renewal: educational technology foundations of electronic learning environments (Open University of the Netherlands). Available online at: http:\/\/eml.ou.nl\/introduction\/docs\/koper-inaugural-address.pdf Lave, J. & Wenger E. (1991) Situated learning: legitimate peripheral participation (Cambridge, Cambridge University Press).","cites":null},{"id":1879261,"title":"Grounded theory as an approach to studying students\u2019 use of learning management systems,","authors":[],"date":"2002","doi":"10.3402\/rlt.v10i2.11402","raw":"Alsop, G. & Tompsett, C. (2002) Grounded theory as an approach to studying students\u2019 use of learning management systems, ALT-J, 10(2), 63\u201376.","cites":null},{"id":196999,"title":"Grounded theory as an approach to studying students\u2019 use of management systems,","authors":[],"date":"2002","doi":"10.1080\/0968776020100207","raw":null,"cites":null},{"id":197000,"title":"Implementing virtual learning environments: looking for holistic , Educational Technology and Society, 3(3). Available online at: http:\/\/ifets.ieee.org\/ l\/vol_3_2000\/barajas.html\/ nkins,","authors":[],"date":"2000","doi":null,"raw":null,"cites":null},{"id":1879263,"title":"Implementing virtual learning environments: looking for holistic approach, Educational Technology and Society, 3(3). Available online at: http:\/\/ifets.ieee.org\/ periodical\/vol_3_2000\/barajas.html\/","authors":[],"date":"2000","doi":null,"raw":"Barajas, M. & Owen, M. (2000) Implementing virtual learning environments: looking for holistic approach, Educational Technology and Society, 3(3). Available online at: http:\/\/ifets.ieee.org\/ periodical\/vol_3_2000\/barajas.html\/ Breen, R., Jenkins, A., Lindsay, R., & Smith, P. (1998) Insights through triangulation: combining research methods to enhance the evaluation of IT based learning methods, in: M. Oliver (Ed.) Innovation in the evaluation of learning technology (London, University of North London).","cites":null},{"id":197004,"title":"Management and implementation of virtual learning ents: a UCISA funded survey UK, UCISA. Available online at: http:\/\/www.ucisa.ac.uk\/ lig\/vle\/VLEsurvey.pdf ISA","authors":[],"date":"2001","doi":null,"raw":null,"cites":null},{"id":1879270,"title":"Management and implementation of virtual learning environments: a UCISA funded survey UK, UCISA. Available online at: http:\/\/www.ucisa.ac.uk\/ groups\/tlig\/vle\/VLEsurvey.pdf JISC and UCISA","authors":[],"date":"2001","doi":null,"raw":"Jenkins, M., Browne, T. & Armitage, S. (2001) Management and implementation of virtual learning environments: a UCISA funded survey UK, UCISA. Available online at: http:\/\/www.ucisa.ac.uk\/ groups\/tlig\/vle\/VLEsurvey.pdf JISC and UCISA (2003) Managed learning environment activity in further and higher education in the UK (prepared by SIRU (University of Brighton), Education for Change Ltd & The Research Partnership).","cites":null},{"id":1879272,"title":"Teaching at a distance: building a virtual learning environment (JTAP 033) (JTAP, UK). Available online at: http:\/\/www.jisc.ac.uk\/uploaded_documents\/ jtap-033.docEvaluating a virtual learning environment 145","authors":[],"date":"1999","doi":null,"raw":"Lee, M. & Thompson, R. (1999) Teaching at a distance: building a virtual learning environment (JTAP 033) (JTAP, UK). Available online at: http:\/\/www.jisc.ac.uk\/uploaded_documents\/ jtap-033.docEvaluating a virtual learning environment 145 Lisewski, B. & Joyce, P. (2003) Examining the five-stage e-moderating model: designed and emergent practice in the learning technology profession, ALT-J, 11(1), 55\u201366.","cites":null},{"id":449357,"title":"Teaching at a distance: building a virtual learning environment 33) (JTAP, UK). Available online at: http:\/\/www.jisc.ac.uk\/uploaded_documents\/ .doc Lisewski, B. & gent pra","authors":[],"date":"1999","doi":null,"raw":null,"cites":null},{"id":197003,"title":"The Internet: a philosophical enquiry","authors":[],"date":"1999","doi":null,"raw":"Graham, G. (1999) The Internet: a philosophical enquiry (London, Routledge).","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2004","abstract":"The evaluation of virtual learning environments (VLEs) and similar applications has, to date, largely consisted of checklists of system features, phenomenological studies or measures of specific forms of educational efficacy. Although these approaches offer some value, they are unable to capture the complex and holistic nature of a group of individuals using a common system to support the wide range of activities that make up a course or programme of study over time. This paper employs Wenger's theories of 'communities of practice' to provide a formal structure for looking at how a VLE supports a pre-existing course community. Wenger proposes a Learning Architecture Framework for a learning community of practice, which the authors have taken to provide an evaluation framework. This approach is complementary to both the holistic and complex natures of course environments, in that particular VLE affordances are less important than the activities of the course community in respect of the system. Thus, the VLE's efficacy in its context of use is the prime area of investigation rather than a reductionist analysis of its tools and components. An example of this approach in use is presented, evaluating the VLE that supports the undergraduate medical course at the University of Edinburgh. The paper provides a theoretical grounding, derives an evaluation instrument, analyses the efficacy and validity of the instrument in practice and draws conclusions as to how and where it may best be used","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/14185.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/597\/1\/ALT_J_Vol12_No2_2004_Evaluating%20a%20virtual%20learning%20.pdf","pdfHashValue":"0b8805fbd87d88dd260aaf78c2392be61c40a9b0","publisher":"Taylor and Francis Ltd","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:597<\/identifier><datestamp>\n      2011-04-04T09:06:32Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/597\/<\/dc:relation><dc:title>\n        Evaluating a virtual learning environment in the context of its community of practice<\/dc:title><dc:creator>\n        Ellaway, Rachel<\/dc:creator><dc:creator>\n        Dewhurst, David<\/dc:creator><dc:creator>\n        McLeod, Hamish<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        The evaluation of virtual learning environments (VLEs) and similar applications has, to date, largely consisted of checklists of system features, phenomenological studies or measures of specific forms of educational efficacy. Although these approaches offer some value, they are unable to capture the complex and holistic nature of a group of individuals using a common system to support the wide range of activities that make up a course or programme of study over time. This paper employs Wenger's theories of 'communities of practice' to provide a formal structure for looking at how a VLE supports a pre-existing course community. Wenger proposes a Learning Architecture Framework for a learning community of practice, which the authors have taken to provide an evaluation framework. This approach is complementary to both the holistic and complex natures of course environments, in that particular VLE affordances are less important than the activities of the course community in respect of the system. Thus, the VLE's efficacy in its context of use is the prime area of investigation rather than a reductionist analysis of its tools and components. An example of this approach in use is presented, evaluating the VLE that supports the undergraduate medical course at the University of Edinburgh. The paper provides a theoretical grounding, derives an evaluation instrument, analyses the efficacy and validity of the instrument in practice and draws conclusions as to how and where it may best be used.<\/dc:description><dc:publisher>\n        Taylor and Francis Ltd<\/dc:publisher><dc:date>\n        2004<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/597\/1\/ALT_J_Vol12_No2_2004_Evaluating%20a%20virtual%20learning%20.pdf<\/dc:identifier><dc:identifier>\n          Ellaway, Rachel and Dewhurst, David and McLeod, Hamish  (2004) Evaluating a virtual learning environment in the context of its community of practice.  Association for Learning Technology Journal, 12 (2).  pp. 125-145.  ISSN 0968-7769 (print)\/1741-1629 (online)     <\/dc:identifier><dc:relation>\n        10.1080\/0968776042000216192<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/597\/","10.1080\/0968776042000216192"],"year":2004,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":" ALT-J, Research in Learning Technology\nVol. 12, No. 2, June 2004\n           Evaluating a virtual learning \nenvironment in the context of its \ncommunity of practice\nRachel Ellaway*, David Dewhurst & Hamish McLeod\nUniversity of Edinburgh, UK\nTaylor and Francis LtdCALT12203.sgm10.1080\/0968776042000216192ALT-T Research in Learing Technology0968 7769 (p i t)\/1741-1629 (online)Original Article2 042 000 4R chelEllaw yLearning Tech ology SectionCol ege of Medicine and Veterinary MedicineUniversity of Edinburgh15 George SquareEdinburghEH8 9XDUKrachel.e away@ed.ac.uk\nThe evaluation of virtual learning environments (VLEs) and similar applications has, to date, largely\nconsisted of checklists of system features, phenomenological studies or measures of specific forms\nof educational efficacy. Although these approaches offer some value, they are unable to capture the\ncomplex and holistic nature of a group of individuals using a common system to support the wide\nrange of activities that make up a course or programme of study over time. This paper employs\nWenger\u2019s theories of \u2018communities of practice\u2019 to provide a formal structure for looking at how a\nVLE supports a pre-existing course community. Wenger proposes a Learning Architecture Frame-\nwork for a learning community of practice, which the authors have taken to provide an evaluation\nframework. This approach is complementary to both the holistic and complex natures of course\nenvironments, in that particular VLE affordances are less important than the activities of the course\ncommunity in respect of  the system. Thus, the VLE\u2019s efficacy in its context of use is the prime area\nof investigation rather than a reductionist analysis of its tools and components. An example of this\napproach in use is presented, evaluating the VLE that supports the undergraduate medical course\nat the University of Edinburgh. The paper provides a theoretical grounding, derives an evaluation\ninstrument, analyses the efficacy and validity of the instrument in practice and draws conclusions\nas to how and where it may best be used.\nIntroduction\nVirtual learning environments (VLEs), and systems like them, provide \u2018the \u2018online\u2019\ninteractions of various kinds which can take place between learners and tutors,\nincluding online learning\u2019 (JISC and UCISA, 2003). These systems all share a\ncommon thread; they can take on many roles and they can support a wide range of\n*Corresponding author. Learning Technology Section, College of Medicine and Veterinary Medi-\ncine, University of Edinburgh, 15 George Square, Edinburgh EH8 9XD, UK. Email: Rachel.Ella-\nway@ed.ac.ukISSN 0968\u20137769 (print)\/ISSN 1741\u20131629 (online)\/04\/020125\u201321\n\u00a9 2004 Association for Learning Technology\nDOI: 10.1080\/0968776042000216192\n 126\n \nR. Ellaway et al.\n           educational, administrative and logistical processes, each of which is able to interact\nand integrate with the others.\nBecause of this integration, because VLEs can be used in many different ways, and\nbecause much that was implicit in the traditional learning environment becomes\nexplicit in its online equivalent, the evaluation of VLEs has proved to be a particularly\ncomplex problem. Furthermore, because of the sheer scale, complexity and cost of\nVLEs, their adoption and use is increasingly undertaken at an institutional level and\nany subsequent evaluation, if it is not done at the level of the individual learner, is\nmost often also undertaken at this institutional level. Between the micro and macro\napproaches are levels that remain relatively disregarded, those of the programme of\nstudy or cognate discipline area, which can, in some cases, be usefully modelled as a\ndistinct \u2018community of practice\u2019.\nThis paper proposes a holistic approach to evaluating VLEs in the context of the\ncommunity of practice in which they are used, where such a community already\nexists. This is based around the \u2018learning architecture framework\u2019 (LAF) proposed by\nEtienne Wenger (1998). This approach is practitioner oriented and was developed in\nresponse to requirements for evaluating programme-wide VLEs in integrated subjects\nsuch as medicine and veterinary medicine.\nEvaluating VLEs\nAlthough there has been much published on evaluative work on VLEs, this has until\nrecently rarely gone further than analysing their various features and functions (see\nCHEST MLE\/VLE comparison grid;1 EDUTOOLS comparison grid;2 or Jenkins et\nal., 2001) or the phenomenology of VLEs in use (Lee & Thompson, 1999; Barajas &\nOwen, 2000; Richardson & Turner, 2000). Occasionally more sophisticated\napproaches to evaluating VLEs have emerged (Britain & Liber, 1999; Koper, 2000)\nwhich take a more grounded and pedagogically orientated approach, but which\ncontinue to orientate towards predictive and intrinsic properties of the VLE. In\npresuming that a VLE has intrinsic properties, that the context into which a VLE will\nbe deployed is neutral and that any given VLE will automatically deliver predictable\nbenefits (or otherwise) into that context, the predictive approach is significantly\nlimited in providing a useful perspective of a VLE in a grounded course context. It is\nimportant to note that most of these approaches have been directed towards a novi-\ntiate audience looking for the best evidence or advice available to help them select a\nsuitable system to meet their needs.\nA different approach to prospective and predictive models is the evaluation of a\n\u2018VLE-in-use\u2019, which investigates the unique properties and dynamics of a course-\nVLE instance. This is a variation on \u2018situated\u2019 or \u2018holistic\u2019 evaluation (Bruce et al.,\n1993). Such approaches are now beginning to appear in the literature, for instance\nusing grounded theory (Alsop & Tompsett, 2002).\nIn introducing a typology of approaches to evaluating learning technologies Oliver\n(1998) identifies \u2018holistic evaluations\u2019 as starting from the position that introducing\ntechnology to educational settings will tend to alter learning outcomes rather than just\n Evaluating a virtual learning environment\n \n127\n    the quantity or quality of what is learnt. The holistic approach seeks to encompass\nbroader aspects of learning technology such as its social or organizational dimensions\nas well the immediate focus of interest. Oliver describes this methodology as seeking: \nto identify the positive and negative aspects of technology use, as compared with tradi-\ntional teaching methods, to build categories from these data, and then to statistically anal-\nyse the response patterns in order to arrive at generalizable findings.\nA situated and\/or holistic \u2018VLE-in-use\u2019 approach is therefore likely to be a more\nappropriate methodology to adopt to unlock the nature and value of our VLEs in their\ncomplex and multifactorial relationships with their course contexts.\nAsking the right questions\nWhether used for distance learning or deployed in support of on-campus courses, and\nwhether a commercial \u2018off-the-shelf\u2019 or a \u2018home-grown\u2019 system is being used, VLEs\nshare two essential characteristics; multiple systems integration and a course or\nmodule focus.\nIt might be expected that any given VLE might be employed in different ways\nwithin different course contexts. In the same way, a particular course could be\nexpected to use different VLEs in different ways. Thus, the pairing of a course and a\nVLE should be understood as a unique instance, with a unique set of characteristics,\nrather than as two separate entities. The effectiveness and value of a VLE to a\ncourse is therefore not an inherent property of the VLE software but depends on its\nuse in facilitating and mediating the needs and activities of a particular course.\nExtending this argument further it might be concluded that all VLE functions exist\nin a \u2018blended\u2019 relationship with human activities, independent of whether they are\nthe primary delivery medium (e.g. distance courses) or one among many (e.g. on-\ncampus courses).\nThis is of course true of any technology, but it is of key importance when consid-\nering complex situations where the permutations of how different kinds of affor-\ndances can be taken up far exceed the prime designed affordances of any given\ntechnology. Added to this is a need to consider the adaptive nature of human prac-\ntices to the affordances of the available technologies. If a thing can be done, not only\nwill the direct affordances it offers tend to change our practices, but, in doing so, it\nwill most likely change what we want to do, and what value we attach to these differ-\nent activities and functions (Graham, 1999).\nThe defining aspect is therefore how such complex technologies are used in specific\ncircumstances. The question which should be asked about a VLE is not \u2018what can it\ndo?\u2019 but rather \u2018what is it doing?\u2019, thereby focusing on its function and role in the situ-\nated educational context.\nIt is often the case, however, even for the best managed courses, that over time their\nethos and procedures become blurred, sometimes to the point where there is no clear\nor common understanding of what the course is about and, more importantly, exactly\nhow it works. Furthermore the deployment of a VLE into any course context initiates\n 128\n \nR. Ellaway et al.\n         a complex set of exchanges, each component shaping the other in a cycle of mutual\nadaptation. \nthe individual organization is neither merely a passive receiver of predetermined techno-\nlogical artefacts nor an autonomous controller of technological change. Rather, in organiz-\ning the flows of knowledge and resources within and between groups, organizations shape\nthe technology process at the same time as it shapes them. (Scarborough & Corbett, 1992,\np. 10)\nThere may therefore be no single reliable or meaningful way to independently\nmeasure a course\u2019s dynamics or the relevant contextualized affordances of a VLE.\nHowever, given a common set of defining criteria or factors, a VLE may be evaluated\nwithin the specific course context in which it is used, avoiding the problems of inde-\npendent analysis and benefiting from the situatedness of such an approach. Because\nof the complex nature of this VLE problem space it is unlikely that any one technique\nwill be effective in evaluating a VLE fully. However, a triangulation approach (Breen\net al., 1998), employing a range of techniques and perspectives is likely to provide a\nbetter solution. The rest of this paper describes the creation and use of an evaluation\ninstrument based on Wenger\u2019s theories on \u2018communities of practice\u2019 that can provide\na strong contributing dimension to this kind of analysis.\nCommunities of practice and VLEs\nOriginally focusing on ideas of apprenticeship, the development of theories of\n\u2018communities of practice\u2019 coalesced around the concepts of legitimate peripheral\nparticipation in a community of practice identified by Lave and Wenger (1991).\nLegitimate peripheral participation in a community of practice is centred on the\nnotion that: \nlearners inevitably participate in communities of practitioners and that mastery of knowl-\nedge and skill requires newcomers to move toward full participation in the sociocultural\npractices of a community. (Lave & Wenger, 1991, p. 29)\nIn \u2018Communities of Practice\u2019, Wenger (1998) proceeded to argue that current ortho-\ndox approaches to learning that are based on concepts of individual learners learning\nin prescribed ways causally linked to teaching are redundant. Wenger proposed\ninstead a \u2018social theory of learning\u2019, which is based upon learning as individual\nengagement and participation in a community of practice.\nWenger\u2019s theories can be particularly relevant in modelling learning environments\nwhere they encompass a pre-existing learning community of students, teachers\/\ntutors, support staff and potentially many other roles and groups. Furthermore, any\nparticipant may adopt or change roles; students may be involved in teaching each\nother, teachers may become learners, support and administration responsibilities may\nfall to different participants at different times and so on. All of this activity is in turn\ninformed by the socio-cultural norms and values inherent in the practice and the\nrelated social contexts in which it is situated. If this is the case then such a course may\nbe modelled as a community of practice, and indeed, its component parts (such as\n Evaluating a virtual learning environment\n \n129\n                     modules of study or groupings such as \u2018students\u2019) may themselves constitute subsid-\niary communities of practice. The relevance of this model depends on the degree and\ncoherence of shared purpose, meaning, activity and identity across the course\ncommunity. For instance in a modular arts or science context the model may be\nexpected to be weak as students pursue individual patterns of cross-disciplinary study\nwhile in an integrated vocational context such as law or teaching the model would be\nexpected to be more relevant.\nThere are existing studies that have applied Wenger\u2019s theories in the context of\nVLEs (e.g. Rogers, 2000; Chalk, 2001). These have, however, tended to take\nWenger\u2019s general topics of \u2018mutual engagement\u2019, \u2018joint enterprise\u2019 and \u2018shared reper-\ntoire\u2019 as the basis for their work rather than anything more structured. Their focus has\nalso tended to fall uneasily between the learner and the environment without clearly\nidentifying one from the other.\nHowever, following his discussions of the general dynamics and characteristics of\ncommunities of practice, Wenger goes on to formalize these dynamics in his \u2018Learn-\ning Architecture Framework\u2019 (LAF) for a learning community of practice. This\nframework has the following defining characteristics or properties (Wenger, 1998, pp.\n237\u2013239):\nFacilities of engagement\n\u0002 Mutuality\u2014interactions, joint tasks, help, encounters across boundaries, degrees\nof belonging.\n\u0002 Competence\u2014opportunities to develop and test competences, devise solutions,\nmake decisions.\n\u0002 Continuity\u2014repositories, documentation, tracking, \u2018participative memory\u2019, story-\ntelling, \u2018paradigmatic trajectories\u2019.\nFacilities of imagination\n\u0002 Orientation\u2014location in space, time, meaning and power.\n\u0002 Reflection\u2014models and patterns, opportunities for engaging with other practices\nor break rhythm with the community mainstream.\n\u0002 Exploration\u2014trying things out, simulations, play.\nFacilities of alignment\n\u0002 Convergence\u2014common focus or cause, direction, vision, values, principles.\n\u0002 Coordination\u2014procedures, plans, schedules, deadlines, communication channels,\nboundary encounters and brokers.\n\u0002 Jurisdiction\u2014policies, contracts, rules, authority, arbitration, mediation.\nBy accepting these as key properties of a learning community of practice, this\nframework can be used as the basis of a more structured evaluation methodology,\n 130\n \nR. Ellaway et al.\n       evaluating a VLE in its context of use. It is proposed that by using this framework a\nVLE can be evaluated in terms of its success and value in supporting these nine prop-\nerties in the context of the community of practice that employs it.\nA note of caution should be added at this point. The approach advocated in this\npaper is a descriptive post-hoc evaluation model and is not intended as a template for\ndesigning a VLE. As Schwen and Hara observe: \nwhile Wenger\u2019s work is a provocative ideal to achieve and useful as a dialogue between the\ndesigners and client systems, it is not a recipe for construction of such phenomena\n(Schwen & Hara, 2003, p. 262)\nThis approach is also intended for use where a VLE is one medium for course delivery\namongst many. Some approaches that draw upon the principles of communities of\npractice are predicated on the community of practice being either fully or predomi-\nnantly online (Notess & Plaskoff, 2004). It is more usual in higher education for a\nVLE to provide scaffolding and support within a multi-modal environment (Ellaway\net al., 2003).\nMethod: developing a VLE evaluation tool based on the LAF\nThe first stage of the development process was, starting with Wenger\u2019s LAF, to move\nfrom the general component factors of the LAF to increasingly specific questions\naimed at evaluating VLEs. The first three steps of this process are shown in Table 1\nand were at this stage fully derived from Wenger.\nThe second stage was to extend Wenger\u2019s theories to develop a pool of VLE-\noriented questions based on the \u2018specific aspects\u2019 column of Table 1. This pool of\nquestions was piloted with a variety of members of the target learning community\nand, as a result, a number of questions were combined, rephrased or omitted. A\nparticular outcome of this piloting was the development of a three-stem structure for\neach question, based on general effectiveness, personal utility and personal value. The\nquestions were then rephrased as value statements and participant response option\nwas structured against Likert scales. Rather than creating new scales, the Likert scales\nwere selected from those available in the online evaluation system that was to be used\nin delivering the instrument. The instrument was piloted again and further refine-\nments and adjustments made.\nIn addition to questions derived from the LAF, a number of questions were\nincluded to verify responses against other data (such as server logs) and to act as\n\u2018consistency traps\u2019. When completed the evaluation instrument comprised of 60\nitems (see Table 2). The instrument was designed to be administered to all of the\nVLE\u2019s users, irrespective of their role in the learning community supported by the\nVLE; whether student, academic or support staff.\nThis is, as was noted earlier, an inherently \u2018situated\u2019 and faceted approach; the\nnine-point LAF has no inherent hierarchy or ranking of importance or relevance.\nThese can only be judged or evaluated in the context at hand. Indeed, any given\ncourse context may well contain distinct constituent communities of practice (such as\n Evaluating a virtual learning environment\n \n131\n       staff or students) that themselves hold contrasting and conflicting perspectives and\nvalue systems within the broader course community of practice.\nIn taking this approach the authors are making the following assumptions: \n\u0002 that Wenger\u2019s theories adequately model a community of practice;\n\u0002 that the subject area or discipline has a strong identity as a community of practice;\n\u0002 that the community of practice encompasses the whole course.\nThis approach does not seek to test theories of communities of practice. Rather it\nassumes a pre-existing course community of practice as a given reference point and\nthereby evaluates a VLE by its ability to support that course community of practice.\nTable 1. First steps in developing Wenger\u2019s Learning Architecture into an evaluation instrument \n(after Wenger, 1998, pp. 237\u2013239)\nGeneral factors General questions Specific aspects\nMutuality Does\/will the system support and facilitate \nthe required mutuality and how important \nis this?\nInteraction\nJoint tasks\nPeripherality\nCompetence Does\/will the system support and facilitate \nthe required competences and how \nimportant is this?\nInitiative and knowledgeability\nAccountability\nTools\nContinuity Does\/will the system support and facilitate \nthe required continuity and how important \nis this?\nReificative memory\nParticipative memory\nOrientation Does\/will the system support and facilitate \nthe required orientation and how \nimportant is this?\nLocation in space\nLocation in time\nLocation in meaning\nLocation in power\nReflection Does\/will the system support and facilitate \nthe required reflection and how important \nis this?\nModels and representations\nComparisons\nTime off\nExploration Does\/will the system support and facilitate \nthe required exploration and how \nimportant is this?\nScenarios\nSimulations\nPracticum\nConvergence Does\/will the system support and facilitate \nthe required convergence and how \nimportant is this?\nFocus, vision and values\nLeadership\nCoordination Does\/will the system support and facilitate \nthe required coordination and how \nimportant is this?\nStandards and methods\nCommunication\nBoundary facilities\nFeedback facilities\nJurisdiction Does\/will the system support and facilitate \nthe required jurisdiction and how \nimportant is this?\nPolicies, mediation, arbitration \nand authority\n 132\n \nR. Ellaway et al.\n \nTable 2. Learning Design Questionnaire\u2014generic format showing response type and mapping to the Learning \nArchitecture Framework\nNo. Root Type LAF Mapping\n1 How effective is the system\u2019s engagement with the course in \ngeneral?\nQA\n2 How useful is the system at engaging you with the course? QA General\n3 The system\u2019s support of my engagement with the course is \nimportant to me \u2026\nSA\n4 How effective is the system in general at supporting \ninteractions with students and staff on the course?\nQA\n5 How useful is the system in supporting your interactions with \nstudents and staff on the course?\nQA Mutuality, competence, \ncontinuity, coordination\n6 The system\u2019s support of my interactions with students and \nstaff is important to me \u2026\nSA\n7 How effective in general is the system at supporting \ncollaborative activities required by the course?\nQA\n8 How effective is the system at supporting the collaborative \nactivities you are involved in?\nQA Mutuality, competence, \ncontinuity, exploration\n9 The system\u2019s support of the courses\u2019 collaborative activities is \nimportant to me \u2026\nSA\n10 How effective is the system in general at providing the course \ninformation and help required for the course?\nQA\n11 How useful is the system at providing the course information \nand help that you require to participate fully in the course?\nQA\nCompetence, continuity, \norientation, convergence, \n12 The system\u2019s provision of course information and help is \nimportant to me \u2026\nSA\ncoordination\n13 How useful is the system in general at interacting with \nUniversity services and systems beyond the course?\nQA\n14 How useful is the system at supporting your interactions with \nUniversity services and systems beyond the course?\nQA Mutuality, reflection, \ncoordination\n15 The system\u2019s support of my interactions with University \nservices and systems beyond the course is important to me \u2026\nSA\n16 How effective is the system in general at supporting \nassessment in the course?\nQA\nCompetence, continuity, \n17 How useful is the system at supporting your assessment needs \nin the course?\nQA orientation, coordination, \njurisdiction\n18 The system\u2019s support of assessment is important to me \u2026 SA\n19 How effective is the system at providing the courses\u2019 \nguidelines, rules and regulations?\nQA\n20\n21\nHow useful is The system\u2019s provision of guidelines, rules and \nregulations you require?\nThe provision of guidelines, rules and regulations by the \nQA\nCompetence, continuity, \nconvergence, coordination, \njurisdiction\nsystem is important to me \u2026 SA\n22\n23\n24\nHow effective are the tools provided by the system?\nHow useful to you are the tools provided by The system?\nThe provision of tools by the system is important to me \u2026\nQA\nQA\nSA\nMutuality, competence, \ncontinuity\n Evaluating a virtual learning environment\n \n133\n \nTable 2.\n \nContinued\n \nNo. Root Type LAF Mapping\n25 How effectively in general does the system support \nprogression through the course?\nQA\n26 How useful is the system\u2019s support of your progression \nthrough the course?\nQA Continuity, orientation, \nconvergence\n27 The system\u2019s support of my progression through the course is \nimportant to me \u2026\nSA\n28 How effective in general is the system at supporting out of \nhours working?\nQA\n29 How useful is the system at supporting your need to work out \nof hours?\nQA Continuity, orientation, \nreflection\n30 The system\u2019s support of my work out of hours is important to \nme \u2026\nSA\n31 How effective is the system in general at supporting teaching \nand learning activities at different locations?\nQA\n32 How useful is the system at supporting your teaching and \nlearning activities at different locations?\nQA Continuity, orientation, \nreflection\n33 The system\u2019s support of my teaching and learning activities at \ndifferent locations is important to me \u2026\nSA\n34 How effective is the system at providing timetabling and \nscheduling information?\nQA\n35 How useful is the system at providing the timetabling and \nscheduling information you require?\nQA Continuity, orientation, \ncoordination\n36 The system\u2019s provision of timetabling and scheduling \ninformation is important to me \u2026\nSA\n37 How effective is the system in general at providing secondary \nlearning materials?\nQA\n38 How effective is the system at providing you with secondary \nlearning materials?\nQA Competence, orientation, \nexploration\n39 The system\u2019s provision of secondary learning materials is \nimportant to me \u2026\nSA\n40 How effective is the system at providing access to materials \nand resources that help with the reflective aspects of the \ncourse?\nQA\n41 How useful is the system at providing materials and resources \nthat help you with reflective aspects of the course?\nQA Competence, orientation, \nreflection, exploration\n42 The system\u2019s provision of access to materials and resources \nthat help with reflective aspects of the course is important to \nme \u2026\nSA\n43 To what degree does the system embody the focus, vision and \nvalues inherent in the course?\nQA\n44 How useful is the system\u2019s embodiment of the focus, vision \nand values inherent in the?\nQA Orientation, convergence, \ncoordination, jurisdiction\n45 The system\u2019s embodiment of the focus, vision and values \ninherent in the course is important to me \u2026\nSA\n46 How effective is the system at supporting the educational \npractices and methods of the course?\nQA\n47 How useful is the system at supporting the educational \npractices and methods of the course?\nQA Mutuality, competence, \nconvergence, coordination\n48 The system\u2019s support of the educational practices and \nmethods of the course is important to me \u2026\nSA\n 134\n \nR. Ellaway et al.\n           Using the LAF evaluation instrument\nThe LAF evaluation instrument was used to evaluate the \u2018Edinburgh Electronic\nMedical Curriculum\u2019 (EEMeC),3 a purpose-built VLE system supporting the under-\ngraduate medical course at the University of Edinburgh. It was considered that this\ncourse had a strong existing course community of practice: students followed a\ncommon and integrated programme of study that was not shared with any other\nstudents; the programme of study was intrinsically focused on inducting students into\nmedical practice; it was taught by practicing clinicians (often in real clinical contexts),\nit had a strong socializing agenda, and it had a continuous history going back over\nmore than two centuries..\nEEMeC was already well established across the course supporting approximately\n1,200 students and approximately 700 staff across all five years of the programme.\nThe development and characteristics of EEMeC have been described elsewhere\n(Warren et al., 2002; Ellaway et al., 2003).\nTable 2. Continued\nNo. Root Type LAF Mapping\n49 How effective is the system at supporting feedback and \nevaluation within the course?\nQA\n50 How useful to you is the system at supporting feedback and \nevaluation within the course?\nQA Mutuality, continuity, \ncoordination\n51 The system\u2019s support of feedback and evaluation within the \ncourse is important to me \u2026\nSA\n52 How effective is the system at tracking student and staff use \nof The system?\nQA\n53 How useful to you is the system\u2019s ability to track student and \nstaff use of The system?\nQA Competence, continuity, \njurisdiction\n54 The ability to track student and staff use of the system is \nimportant to me \u2026\nSA\n55 Overall I think the system is a very useful system in helping me \nengage with the course \u2026\nSA\n56 Overall I think the system is a very valuable system in helping \nme engage with the course \u2026\nSA\nGeneral\n57 Overall I think the system is a reliable system in helping me \nengage with the course \u2026\nSA\n58 How often do you use the system? ON\n59 How responsive to requests for help and\/or support is the \nsystem?\nQA\n60 Are there any aspects of the system that you think should be \nadded to, improved or changed to make the system more \nuseful to you?\nFT\nResponse types: QA = excellent to awful; SA = strongly agree to strongly disagree; ON = all the time to never; \nFT = free text; YN = yes\/no.\n Evaluating a virtual learning environment\n \n135\n         The survey instrument was deployed using EEMeC\u2019s own \u2018evaluation engine\u2019\nwhich allows staff to create, schedule, deliver, record and analyse questionnaires\nonline (Wylde et al., 2003). This is done by mapping different copies of the instru-\nment in the VLE database to the groups that would receive them. By setting a start\nand end time, when a user logged in to EEMeC with the period set, the system would\ncheck to see that an uncompleted questionnaire was set for the user\u2019s group and, if\nthis was the case, would present the questionnaire to them in a pop-up window.\nAlthough a log of who had completed questionnaires was kept, this was separated\nfrom the responses so that they were anonymized at the point of storage and only one\nresponse per individual was permitted.\nThe period set for delivery was from 10 April to 30 April 2003, a period that\nmapped on to different years based on their term or rotation schedule: \nYear 1\u2014on vacation to 15th April then starting term 3.\nYear 2\u2014on vacation to 15th April then starting term 3.\nYear 3\u2014new clinical rotation started 10th April.\nYear 4\u2014in middle of clinical rotation (24\/2 to 30\/5).\nYear 5\u2014in middle of clinical rotation (31\/3 to 23\/5).\nIt is important to note that this was the first time that staff had been surveyed in this\nfashion.\nThe response rates (shown in detail in Table 3) were high overall although the staff\nresponses were particularly low. The figure of 699 staff includes 50 or so guest logins\nand a large number of clinical and related staff who have a relatively peripheral\nengagement with the course. It is a peculiarity of medical education that a large\nnumber of clinical staff will be involved in teaching but only for a fraction of the work-\ning year. Thus, despite the high potential numbers of staff in the course, at any given\ntime only a relatively low number are actively engaged in teaching. This explains to a\nmajor degree the relatively poor responses in the staff cohort.\nThere is also a notable lower response rate in year 3 with only just over half of the\nyear responding relative to that in year 4. In the 2002\u20133 academic session, EEMeC\nwas progressively less important to the students\u2019 engagement with the course in later\nyears, as there were less tools and materials provided, and it was expected that there\nTable 3. Responses for all cohorts\nCourse role Population Returns % returns\nYear 1 students 236 207 87\nYear 2 students 214 186 87\nYear 3 students 258 142 55\nYear 4 students 221 192 87\nYear 5 students 176 120 68\nAll staff 699 45 6\n 136\n \nR. Ellaway et al.\n                              would be a gradual decrease in the response rates in later years. In this respect, the\nrelatively high response rate in year 4 is more atypical than the low response rate in\nyear 3. This may be interpreted as being due to year 3 students focusing on orienting\nthemselves within their new clinical attachments and not engaging with EEMeC to a\ngreat extent while year 4 students, already established in their attachments, were using\nEEMeC to research and submit coursework as well as to link back to their peers and\ntutors.\nIt is acknowledged that using EEMeC as the medium to evaluate itself could have\nintroduced bias to the returns, in that only EEMeC users can have responded. However,\nthe high response rates achieved, the near-mandatory requirement for students to\naccess EEMeC regularly and the high commitment to evaluative activity across the\ncourse in general are considered to have ameliorated the effects of any such bias.\nAnalysis and interpretation\nTriad analysis (effectiveness\/utility\/importance)\nQuestions 1\u201354 were delivered in triads framing the same question in terms of general\neffectiveness, personal utility and personal importance. A mean for each respondent\nfor each of the three stem variants was taken. This was then analysed for internal reli-\nability by calculating a Cronbach\u2019s alpha reliability coefficient, which showed a high\ndegree of internal reliability (\u03b1=0.8652). Non-parametric correlation analysis (Spear-\nman\u2019s \u03c1) was then carried out between each of the three pairings (respondent means)\nwith the following results: \n\u0002 there was a significant positive correlation between general effectiveness and\npersonal utility (\u03c1=0.914, n=821, p<0.0005). This indicates that there are fairly\nbalanced feelings as regards EEMeC; respondents did not rate EEMeC as partic-\nularly good or bad in general relative to EEMeC\u2019s usefulness to them. Analysis of\nthis correlation would be expected to indicate whether a VLE had a particular\nsubjective reputation-bias relative to its objective evaluation. In the case of\nEEMeC effectiveness and utility were essentially equivalent in the respondents\u2019\nminds.\n\u0002 there was a significant positive correlation between general effectiveness and\npersonal importance (\u03c1=0.429, n=818, p<0.0005) and a significant positive corre-\nlation between personal utility and personal importance (\u03c1=0.448, n=818,\np<0.0005). Respondents considered EEMeC\u2019s importance to them was less than\nits perceived general effectiveness or personal utility. These pairings are taken to\nindicate the degree to which the VLE is the medium for course business. High posi-\ntive correlations would indicate that the VLE was the principal medium for the\ncourse, no correlation that the VLE was no more or less important than other\nmedia for the course and high negative correlations that the VLE was of little or no\nrelevance to the course. The data reflects the situation that EEMeC is a significant\nmedium for course business but not the largest or most important one.\n Evaluating a virtual learning environment\n \n137\n   LAF validation\nSince the questions had originally been generated from the LAF, it was important to\nverify that the predicted mapping between questions and the LAF was statistically\nvalid. Factor analysis and inter-item reliability and correlation tests were performed.\nThe factor analysis identified fourteen underlying significant factors with eigenval-\nues higher than 1.0, the first 13 of which were interpretable with the dominant\nfactor being that of \u2018personal importance\u2019 (these are shown in Table 4). Although\nthe factor analysis was interesting there was no strong equivalence between it and\nTable 4. Factor analysis and interpretations (extraction method: principal component analysis, \nrotation method: varimax with Kaiser normalization)\nFactor Rotation Sums of Squared \nLoadings\nQuestions with eigenvalues \nover 0.3\nInterpreted factor \ndescription\nTotal % of \nVariance\nCumulativ\ne %\n1 9.296 15.756 15.756 3,6,9,12,15,18,21,24,27,30,33\n,36,39,42,45,48,51,55,56,57,5\n8\nPersonal \nimportance and \nrelevance\n2 3.875 6.567 22.323 1,2,7,8,25,26,46,47 General course \nparticipation\n3 3.781 6.409 28.732 13,14,15,34,35,37,38,43,44 External \nconnectivity\n4 3.589 6.083 34.816 17,22,23,28,34,35,41 Support of \nactivities\n5 3.380 5.729 40.544 17,31,32,40,41,42,43,44,45,4\n6\nEducational \nsupport\n6 3.109 5.270 45.814 26,28,29,31,32,59 Personal logistics\n7 2.204 3.735 49.549 49,50,52,59 Feedback\n8 2.187 3.707 53.256 19,20,25 Authority\n9 1.978 3.352 56.608 45,52,53,54 Tracking and \nprotection\n10 1.906 3.230 59.838 4,5 Communication\n11 1.887 3.199 63.037 16,17,52 Assessment\n12 1.848 3.132 66.169 2,10,11 Provision of \nInformation\n13 1.720 2.915 69.084 25,46,47,59 General support\n 138\n \nR. Ellaway et al.\n         the LAF map although there was some congruence. However, as the responses were\noverall very positive and therefore heavily negatively skewed, there was a low level of\nvariance and therefore a factor analysis would not be expected to be a particularly\nilluminative tool.\nAn inter-item test of reliability (Cronbach\u2019s alpha) was performed for each set of\nmapped responses to the LAF (as shown in Table 5). This was performed on the\npersonal utility component of each triad (the previous section displayed that there was\na very high correlation between effectiveness and utility and a reasonably high corre-\nlation between utility and importance). The results show a strong level of consistency\nacross the question groups and therefore an acceptable level of reliability for the ques-\ntion-LAF map (none of the reliability coefficients were <0.8).\nA non-parametric correlation analysis (Spearman\u2019s \u03c1) was also carried out for all\nitem pairs. There was no significant difference between the mean correlation for\nthe LAF mappings and the overall mean correlation. This indicates that, although\na reasonable level of reliability has been established, the overall mapping is not\nvery strong and further work needs to be done to refine this part of the instrument.\nOverall LAF analysis\nHaving established the question-LAF map, each factor was analysed for each respon-\ndent group. The results of this are shown in Figure 1. From this a ranking of factors\nwas plotted as shown in Figure 2.\nLearning architecture framework scores for EEMeCRanked learning architecture framework factors for EEMeC\nTable 5. Questionnaire-to-LAF map and inter-item reliability\nQuestion triad Mutuality Competence Continuity Orientation Reflection Exploration Convergence Coordination Jurisdiction\n4, 5, 6 x x x x\n7, 8, 9 x x x x\n10, 11, 12 x x x x x\n13, 14, 15 x x x\n16, 17, 18 x x x x\n19,20, 21 x x x x x\n22, 23, 24 x x x\n25, 26, 27 x x x\n28, 29, 30 x x x\n31, 32, 33 x x x\n34, 35, 36 x x x\n37, 38, 39 x x x\n40, 41, 42 x x x x\n43, 44, 45 x x x x x\n46, 47, 48 x x x x\n49, 50, 51 x x x\n52, 53,54 x x x\nCronbach\u2019s \ninter-item \nalpha \nreliability \ncoefficient\n0.886 0.917 0.926 0.920 0.838 0.818 0.876 0.914 0.823\n Evaluating a virtual learning environment\n \n139\nFigure 1. Learning architecture framework scores for EEMeC\nFigure 2. Ranked learning architecture framework factors for EEMeC\n 140\n \nR. Ellaway et al.\n             Individual question triad analysis\nEach of the question triads was then analysed across respondent groups and in\ncomparison to the triad average and the overall average. A graph for each triad\nprovides a useful illustration of the dynamics within the course community for each\nof the 18 issues addressed. A few examples are shown in Figures 3 and 4.\nInvolvement with the MBChB course\u2014triad average SD = 0.17, overall average SD = 0.30). Interpretation: EEMeC is considered to provide a highly effective and useful service to the MBChB community of practice in engaging it with the course. However, because the course is not predominantly delivered or mediated online, EEMeC is considered to be less important by the community than it is effective or useful. Although there is deviation between cohorts, this is relativelysmal  and i dicates a relative consensus respons . The staff cohort shows atypically higher importance rati g whil  year 4 shows an atypically low importance r ting. Conclusion: EEMeC is successfully mediating members of the MBC B community engagement with the course. There is a gradual decrease in scor  across the course with staff scoring s m where in the middle.Providi g timetabling and scheduli g inform tion qu stion average SD=0.43, overall av age SD=0.30. Possible int rpretat n: EEMeC\u2019s provision of timetabling and scheduli g information, altho gh scoring easona ly well, shows little consensus betwe n cohorts. Most rat d the importance of this factor higher th n EEMeC\u2019s effectiveness an  utili y in thi  area. P rsona  utility was generally rated h gher than general effec iveness. Conclusion: althoug  not entirely lacking,EEMeC\u2019s prov sion of timetabli g and sche ul ng information to th  MBChB ommunity could be sign ficantly b t . Thi  appe rs to be  pri r ty f r he community that is currently insufficien ly well supported.\nFree-text analysis\nQuestion 60 was a free-text response allowing any free-text comments to be added.\nAbout 30% of respondents took up this option. Their comments largely echoed the\nfindings from the rest of the questionnaire or made suggestions for specific changes\nor developments in the system. These were fed back to the course community as the\nbasis for discussion within EEMeC\u2019s steering and user groups. These comments will\nalso contribute to the overall triangulated VLE evaluation but fall outside the imme-\ndiate scope of this paper.\nFigure 3. Involvement with the MBChB course:triad average SD = 0.17, overall average SD =\n0.30). Interpretation: EEMeC is considered to provide a highly effective and useful service to the\nMBChB community of practice in engaging it with the course. However, because the course is not\npredominantly delivered or mediated online, EEMeC is considered to be less important by the com-\nmunity than it is effective or useful. Although there is deviation between cohorts, this is relatively\nsmall and indicates a relative consensus response. The staff cohort shows atypically higher impor-\ntance rating while year 4 shows an atypically low importance rating. Conclusion: EEMeC is suc-\ncessfully mediating members of the MBChB community engagement with the course. There is a\ngradual decrease in score across the course with staff scoring somewhere in the middle.\n Evaluating a virtual learning environment\n \n141\n    Longitudinal and parallel measures\nThe Edinburgh MBChB is heavily evaluated (like most other medical courses).\nEvaluation fatigue is a major concern and, as the instrument is relatively large, a\nrepeat survey will not take place until the autumn of 2004 (establishing an 18\nmonth cycle).\nThis evaluation has recently been used on a VLE supporting undergraduate\nmedical education in another university. A full data analysis is not yet available but\nprovisional analysis indicates a similar ranking of LAF factors to those shown in\nFigure 2.\nRespondent group bias\nNo accommodation has been made in the analysis for cohort bias as each academic\nyear in the course has the approximately the same make up regarding gender, ethnic-\nity etc and there is a significant mixing up between cohorts mid-course as about 40%\nof each year\u2019s students take a year out to do an intercalated honours course.\nFigure 4. Providing timetabling and scheduling information:question average SD=0.43, overall\naverage SD=0.30. Possible interpretation: EEMeC\u2019s provision of timetabling and scheduling infor-\nmation, although scoring reasonably well, shows little consensus between cohorts. Most rated the\nimportance of this factor higher than EEMeC\u2019s effectiveness and utility in this area. Personal utility was\ngenerally rated higher than general effectiveness. Conclusion: although not entirely lacking, EEMeC\u2019s\nprovision of timetabling and scheduling information to the MBChB community could be signifi-\ncantly better. This appears to be a priority for the community that currently considers itself to be\ninsufficiently well supported.\n 142\n \nR. Ellaway et al.\n   Delivery bias\nAs the VLE was the medium for delivery of the questionnaire, it is reasonable to antic-\nipate a degree of bias from the fact that EEMeC users were the only ones who could\nhave responded. The fact that use of EEMeC is mandatory for a number of course\nactivities and the high student response rates indicate a fairly comprehensive coverage\nof the overall population however.\nDiscussion\nThe development and use of an evaluation instrument (based on Wenger\u2019s learning\narchitecture framework) to investigate the utility of a VLE in the context of a specific\ncourse context has proved to be useful in capturing many of the dynamics of a VLE-\nin-use and thereby contributing to a broader holistic evaluation process. Validation of\nthe instrument has however pointed out limitations in the mapping between the\ninstrument and the LAF. Further work therefore needs to be done in refining this tool\nboth in terms of the mapping between questionnaire items and the LAF and in carry-\ning out longitudinal and parallel studies using this instrument.\nThe application of the evaluation instrument to an undergraduate medical VLE\n(EEMeC) indicates that there are aspects of the VLE that can be improved, particu-\nlarly in the areas of course coordination (e.g. timetables), jurisdiction (e.g. rules and\nauthority) and exploration (e.g. secondary learning materials) while other aspects are\nrelatively strong. Analysis of its effectiveness, utility and importance show that the\ncommunity using EEMeC has a reasonably realistic view of this VLE, and analysis of\nthe individual questions provides feedback on specific aspects of how the system\nrelates to its community of practice.\nIt is important to emphasise that this is a theory-based approach, which is predi-\ncated on a pre-existing course community of practice. In those situations where this\nis a valid assumption, for instance in subjects such as medicine, then it has immedi-\nate relevance and utility. For other situations, for instance in modular programmes\nof study, where communities of practice may not equate to a course (or even exist\ncoherently at all) then there may be less relevance in such a study, although a\nmodule may in some cases retain a degree of internal coherence as a community of\npractice.\nThis approach to evaluating VLEs does not provide information on what the VLE\ncan do, nor what its features are or even how it is used. These can be obtained from\nother sources and indeed these are usually already reasonably well known within the\nVLE\u2019s user community. What it does help to provide is a perspective of how success-\nfully the VLE is serving the communities of practice involved with the course in ques-\ntion, and thereby is able to provide pointers to areas in which it could be improved to\nthe benefit of that community.\nIt is important to restate that this approach is intended to contribute to a multidi-\nmensional \u2018triangulated\u2019 approach to VLE evaluation; other components may include\nlog file analysis, use case expressions and analysis of impact although elaboration of\nthese approaches are outwith the scope of this paper.\n Evaluating a virtual learning environment\n \n143\n    The proposal is that the LAF is not presented as a prescriptive framework around\nwhich a VLE should be designed and built; such prescriptive frameworks have been\nidentified as inimical to professional practice (Lisewski & Joyce, 2003). It is descrip-\ntive rather than prescriptive; the themes used and the insights gained are recom-\nmended as tools to inform the reflective practices of those responsible for the design\nand delivery of VLE systems within coherent community of practice contexts. As\nSchwen and Hara point out: \na rich descriptive theory is not a warrant or recipe for the construction of certain phenom-\nena, and a useful prescriptive theory may not provide a full understanding of the phenom-\nena but rather a perspective on the conditions or circumstances of its applied use. (Schwen\n& Hara, 2003, pp. 261\u2013262)\nEEMeC has been developed organically over a number of years in response to the\ncourse\u2019s needs and wishes. This process was not directly based on the LAF although\nit was developed with the intent to support the whole 5-year programme. The LAF\nevaluation is therefore not intended to be a formal validation of a particular VLE\ndesign but rather as a contribution to the ongoing and iterative development of a VLE\nin support of its course and course community. It is also intended as a technique for\npractitioner-researchers who need immediate and reflexive insights into VLE design,\ndeployment and support for specific educational contexts.\nConclusion\nAs Oliver (2000) observes: \u2018evaluation forms a unique meeting point between policy,\ntheory and practice, and as a consequence, it seems unlikely that its practice will ever\nbe uncontentious\u2019. However, the approach presented here has been grounded in\ntheory, is based around a holistic view of course-VLE instances and has provided\nsignificant utility to the authors in the evaluation of their own work. Oliver (2001)\nidentifies a potential weakness in this kind of approach when others seek to use it\u2014\n\u2018the purpose of theory may not be fixed but may depend on the way users appropriate\nit\u2019. Thus the level to which other users may find utility in this work may depend on\nthe degree of agreement and alignment in approach and philosophy with that of the\nauthors, and the contexts they are working in.\nIn situations where VLEs are used in programme settings where there is a strong\nand coherent community of practice then there is particular benefit in carrying out\nthis kind of evaluation. There is often a major investment in, and dependence on,\nVLEs in these contexts (Cook, 2001), yet little in the way of appropriate evaluation\ninstrumentation to supply those making this investment with feedback as to their effi-\ncacy and outcome.\nIt is hoped that the LAF framework can also provide a common language with\nwhich to compare different systems or ways of working with VLEs. Even if a VLE is\nonly supposed to provide one or two aspects of support to a course it is still valid to\napply this approach to investigate, for instance, the \u2018functionality creep\u2019 as the\ncommunity using it tends to adopt originally unintended affordances from the system.\nAll of these factors and issues will need further attention.\n 144\n \nR. Ellaway et al.The development of approaches to evaluating VLEs as described in this paper can\ncontribute insights on the multifactorial interactions between communities of prac-\ntice and technology-mediated extensions to their learning environments and make a\nstrong and valid contribution to broader holistic and triangulated evaluation\nprograms.\nNotes\n1. http:\/\/www.chest.ac.uk\/datasets\/vle\/checklist.html\n2. http:\/\/www.edutools.info\/course\/compare\/all.jsp\n3. EEMeC-online at www.eemec.med.ed.ac.uk\nReferences\nAlsop, G. & Tompsett, C. (2002) Grounded theory as an approach to studying students\u2019 use of\nlearning management systems, ALT-J, 10(2), 63\u201376.\nBarajas, M. & Owen, M. (2000) Implementing virtual learning environments: looking for holistic\napproach, Educational Technology and Society, 3(3). Available online at: http:\/\/ifets.ieee.org\/\nperiodical\/vol_3_2000\/barajas.html\/\nBreen, R., Jenkins, A., Lindsay, R., & Smith, P. (1998) Insights through triangulation: combining\nresearch methods to enhance the evaluation of IT based learning methods, in: M. Oliver (Ed.)\nInnovation in the evaluation of learning technology (London, University of North London).\nBritain, S. & Liber, O. (1999) A framework for the pedagogical evaluation of virtual learning\nenvironments. JTAP Report 41. Available online at: http:\/\/www.jtap.ac.uk\/reports\/htm\/jtap-\n041.html\nBruce, B. C., Peyton, J. K. & Batson, T. W. (1993) Electronic quills: a situated evaluation of using\ncomputers for writing in classrooms. Available online at: http:\/\/alexia.lis.uiuc.edu\/\u223cchip\/pubs\/\nEquills\/siteval\/index.shtml\nChalk, P. D. (2001) Learning software engineering in a community of practice\u2014a case study. CAL2001\nConference, Warwick. Available online at: http:\/\/www.ics.ltsn.ac.uk\/pub\/conf2001\/papers\/\nChalk.htm\nCook, J. (2001) The role of virtual learning environments in UK medical education. JTAP Report 623.\nAvailable online at: http:\/\/www.ltss.bris.ac.uk\/jules\/jtap-623.pdf\nEllaway, R., Dewhurst, D. & Cumming, A. (2003) Managing and supporting medical education\nwith a virtual learning environment\u2014the Edinburgh electronic medical curriculum, Medical\nTeacher, 25(4), 372\u2013380.\nGraham, G. (1999) The Internet: a philosophical enquiry (London, Routledge).\nJenkins, M., Browne, T. & Armitage, S. (2001) Management and implementation of virtual learning\nenvironments: a UCISA funded survey UK, UCISA. Available online at: http:\/\/www.ucisa.ac.uk\/\ngroups\/tlig\/vle\/VLEsurvey.pdf\nJISC and UCISA (2003) Managed learning environment activity in further and higher education in the\nUK (prepared by SIRU (University of Brighton), Education for Change Ltd & The Research\nPartnership).\nKoper, R. (2000) From change to renewal: educational technology foundations of electronic learning envi-\nronments (Open University of the Netherlands). Available online at: http:\/\/eml.ou.nl\/introduc-\ntion\/docs\/koper-inaugural-address.pdf\nLave, J. & Wenger E. (1991) Situated learning: legitimate peripheral participation (Cambridge,\nCambridge University Press).\nLee, M. & Thompson, R. (1999) Teaching at a distance: building a virtual learning environment\n(JTAP 033) (JTAP, UK). Available online at: http:\/\/www.jisc.ac.uk\/uploaded_documents\/\njtap-033.doc\nEvaluating a virtual learning environment 145Lisewski, B. & Joyce, P. (2003) Examining the five-stage e-moderating model: designed and emer-\ngent practice in the learning technology profession, ALT-J, 11(1), 55\u201366.\nNotess, M. & Plaskoff, J. (2004) Preliminary heuristics for the design and evaluation of online\ncommunities of practice systems, eLearn Magazine.\nOliver, M. (Ed.) (1998) Innovation in the evaluation of learning technology (London, University of\nNorth London).\nOliver, M. (2000) An introduction to the evaluation of learning technology, Educational Technology\nand Society, 3(4). Available online at: http:\/\/ifets.ieee.org\/periodical\/vol_4_2000\/intro.html\/\nOliver, M. (2001) What\u2019s the purpose of theory in learning technology? (ALT Special Interest Group\nfor Theory and Learning Technology Positional Paper). Available online at: http:\/\/homep-\nages.unl.ac.uk\/%7Ecookj\/alt_lt\/Oliver.htm\nRichardson, J. A. & Turner, A. (2000) A large-scale \u2018local\u2019 evaluation of students\u2019 learning experi-\nences using virtual learning environments, Educational Technology and Society, 3, 4. Available\nonline at: http:\/\/ifets.gmd.de\/periodical\/vol_4_2000\/richardson.html\nRogers, J. (2000) Communities of practice: a framework for fostering coherence in virtual learning\ncommunities, Educational Technology and Society, 3(3). Available online at: http:\/\/\nifets.ieee.org\/periodical\/vol_3_2000\/e01.html\/\nScarborough, H. & Corbett, J. M. (1992) Technology and organization: power, meaning and design\n(London, Routledge).\nSchwen, T. M. & Hara N. (2003) Community of practice: a metaphor for online design? The Infor-\nmation Society, 19, 257\u2013270.\nWarren, P., Ellaway, R. & Evans, P. (2002) Meet George \u2026 Using learning technology in a\n\u2018blended\u2019 approach to enhance the integration of knowledge and understanding across a 5-\nyear medical course, ASME 2002 Conference Proceedings, Norwich, UK.\nWenger, E. (1998) Communities of practice: learning, meaning and identity (Cambridge, Cambridge\nUniversity Press).\nWylde, K., Ellaway, R., Cumming, A. & Cameron, H. (2003) Electronic submission and delivery\nof student feedback, AMEE 2003 Conference Proceedings, Bern, Switzerland.\n"}