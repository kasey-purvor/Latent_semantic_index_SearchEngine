{"doi":"10.1109\/CIISP.2007.369172","coreId":"71451","oai":"oai:eprints.lancs.ac.uk:933","identifiers":["oai:eprints.lancs.ac.uk:933","10.1109\/CIISP.2007.369172"],"title":"Evolving fuzzy rule-based classifiers","authors":["Angelov, Plamen","Zhou, Xiaowei","Klawonn, F"],"enrichments":{"references":[],"documentType":{"type":null}},"contributors":[],"datePublished":"2007-04-02","abstract":"In this paper a new method for training single-model and multi-model fuzzy classifiers incrementally and adaptively is proposed, which is called FLEXFIS-Class. The evolving scheme for the single-model case exploits a conventional zero-order fuzzy classification model architecture with Gaussian fuzzy sets in the rules antecedents, crisp class labels in the rule consequents and rule weights standing for confidence values in the class labels. In the multi-model case FLEXFIS-Class exploits the idea of regression by an indicator matrix to evolve a Takagi-Sugeno fuzzy model for each separate class and combines the single models' predictions to a final classification statement. The paper includes a technique for increasing the prediction quality, whenever a drift in a data stream occurs. An empirical analysis will be given based on an online, adaptive image classification framework, where images showing production items should be classified into good or bad ones. This analysis will include the comparison of evolving single- and multi-model fuzzy classifiers with conventional batch modelling approaches with respect to achieved prediction accuracy on new online data. It will also be shown that multi-model architecture can outperform conventional single-model architecture (`classical' fuzzy classification models) for all data sets with respect to prediction accuracy. (c) IEEE Pres","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:933<\/identifier><datestamp>\n      2018-01-24T02:17:27Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D636F6E666572656E63655F6974656D<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Evolving fuzzy rule-based classifiers<\/dc:title><dc:creator>\n        Angelov, Plamen<\/dc:creator><dc:creator>\n        Zhou, Xiaowei<\/dc:creator><dc:creator>\n        Klawonn, F<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        In this paper a new method for training single-model and multi-model fuzzy classifiers incrementally and adaptively is proposed, which is called FLEXFIS-Class. The evolving scheme for the single-model case exploits a conventional zero-order fuzzy classification model architecture with Gaussian fuzzy sets in the rules antecedents, crisp class labels in the rule consequents and rule weights standing for confidence values in the class labels. In the multi-model case FLEXFIS-Class exploits the idea of regression by an indicator matrix to evolve a Takagi-Sugeno fuzzy model for each separate class and combines the single models' predictions to a final classification statement. The paper includes a technique for increasing the prediction quality, whenever a drift in a data stream occurs. An empirical analysis will be given based on an online, adaptive image classification framework, where images showing production items should be classified into good or bad ones. This analysis will include the comparison of evolving single- and multi-model fuzzy classifiers with conventional batch modelling approaches with respect to achieved prediction accuracy on new online data. It will also be shown that multi-model architecture can outperform conventional single-model architecture (`classical' fuzzy classification models) for all data sets with respect to prediction accuracy. (c) IEEE Press<\/dc:description><dc:date>\n        2007-04-02<\/dc:date><dc:type>\n        Contribution to Conference<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/CIISP.2007.369172<\/dc:relation><dc:identifier>\n        Angelov, Plamen and Zhou, Xiaowei and Klawonn, F (2007) Evolving fuzzy rule-based classifiers. In: Symposium, 2007-04-012007-04-04.<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/933\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1109\/CIISP.2007.369172","http:\/\/eprints.lancs.ac.uk\/933\/"],"year":2007,"topics":["QA75 Electronic computers. Computer science"],"subject":["Contribution to Conference","PeerReviewed"],"fullText":" \n \n \nEvolving Fuzzy Rule-based Classifiers \n \nPlamen Angelov \nDept of Communication Systems \nInfoLab21 \nLancaster University \nLancaster LA1 4WA, UK  \nXiaowei Zhou \nDept of Communication Systems \nInfoLab21 \nLancaster University \nLancaster LA1 4WA, UK \nFrank Klawonn \nDept of Computer Science \nUniversity of Applied Sciences \nBS\/WF, Salzdahlumer Str. 46\/48 \nD-38302 Wolfenbuettel, Germany \n \n \nAbstract\u2014 A novel approach to on-line classification based on \nfuzzy rules with an open\/evolving structure is introduced in this \npaper. This classifier can start \u2018from scratch\u2019, learning and \nadapting to the new data samples or from an initial rule-based \nclassifier that can be updated based on the new information \ncontained in the new samples. It is suitable for real-time \napplications such as classification streaming data, robotic \napplications, e.g. target and landmark recognition, real-time \nmachine health monitoring and prognostics, fault detection and \ndiagnostics etc. Each prototype is a data sample that represents \nthe focal point of a fuzzy rule per class and is selected based on \nthe data density by an incremental and evolving procedure. This \napproach is transparent, linguistically interpretable, and \napplicable to both fully unsupervised and partially supervised \nlearning. It has been validated by two well known benchmark \nproblems and by real-life data in a parallel paper. The \ncontributions of this paper are: i) introduction of the concept of \nevolving (open structure) classification (eClass) of streaming \ndata; ii) experiments with well known benchmark classification \nproblems (Iris and wine reproduction data sets). \nI. INTRODUCTION \nLASSIFICATION problems appear quite often in \nindustrial systems, robotics, defence and are the basic \ntool used for pattern recognition tasks in signal and \nimage processing, decision making, data mining, fault \ndetection, automatic object recognition etc [1,2]. The basic \nproblem in classification is to induce a classification function, \nor classifier, from a set of data samples. In many practical \nproblems nowadays the data are produced in large quantity \nand very fast [3]. Such high-volume, non-stationary data \nstreams bring new challenges to the well established learning \nmethods [4]. In particular storing the complete data is often \npractically impossible and as a result the data streams cannot \nbe analyzed in a batch mode. At the same time, most \nconventional learning methods, such as support vector \nmachines [5,6], which aims at designing a classifier with \nguaranteed boundaries of the error, discriminant analysis [1], \ndecision trees [7] or neural network classifiers [8], design the \nclassifier in batch mode, that is, by using the complete data \nand labels that has been observed. Thus, they allow for \nextracting knowledge from a snapshot of the data stream at a \ncertain point of time. Facing the challenge to cope with \nreal-time classification of streaming data there is a need to \ndevelop classifiers that extract tractable knowledge from the \ndata or digital signals in real-time.  \nMore recently, learning classifiers [9] have been developed \nin the framework of evolutionary\/genetic algorithms \n(EA\/GA) and fuzzy rule-based systems that adapt their \nrule-base with new samples arriving on-line. They are, \nhowever, driven by a \u2018directed\u2019 random search according to \nthe EA\/GA concepts. Incremental Bayesian classifiers has \nalso been reported which allow data samples to arrive one at a \ntime, but the classifier structure is assumed to be fixed [10]. \nAn evolving Bayesian classifier is under development and the \npreliminary results are reported in [11]. \nOn the other hand, fuzzy rule-based systems that are \nevolving in the sense that their structure is not fixed, but can \ngrow and shrink has been recently developed [12,29] and \napplied successfully to a number of identification [13], \ntime-series prediction [14], fault detection [15], and control \nproblems [16]. These systems are transparent and \ninterpretable. We use the term \u2018evolving\u2019 in a different \ncontext to the context when used in EA\/GA. According to the \nOxford Dictionary \u2018evolve\u2019 means \u2018unfold; develop; be \ndeveloped, naturally and gradually\u2019 [17, p.294]. One can \ncontrast this to the more general \u2018evolutionary\u2019 [17, p.294] \n\u2018development of more complicated forms of life (plants, \nanimals) from earlier and simpler forms\u2019, which is naturally \nrelated to the \u2018genetic\u2019 [17, p.358] \u2018branch of biology dealing \nwith the heredity, the ways in which characteristics are passed \non from parents to offspring.\u2019 We use further the term \n\u2018evolving\u2019 fuzzy classifier (eClass) in the sense of \u2018gradual \ndevelopment\u2019 of the classifier structure (fuzzy rule-base). \nThis new paradigm introduced for neural networks in [18,19], \nfor decision trees in [20], and for fuzzy rule-based systems in \n[21] can be regarded as a higher level adaptation. This \nemerging new paradigm mimics the evolution of individuals \nin nature during their life-cycle, specifically the autonomous \nmental development typical of humans: learning from \nexperience, inheritance, gradual change, knowledge \ngeneration from routine operations, and rules extraction from \nthe data. A trivial analogy is the way people learn during their \nlife \u2013 starting with an empty rule-base they learn new rules \nduring their life from experience and based on the data \nstreams that their preceptors generate to the brain. The \ndevelopment of the rule-base is gradual, but the rules are not \nfixed or pre-defined. We generate new rules when new facts \n(data samples) that can not be described by the existing rules \nand when they are descriptive enough, not to be \u2018one-off\u2019\u2019 \noutliers [22]. It is well known that fuzzy rule-based systems \nare universal function approximators [23]; they are suitable \nfor extracting interpretable knowledge, therefore, they are \nC \n220\nProceedings of the 2007 IEEE Symposium on Computational\nIntelligence in Image and Signal Processing (CIISP 2007)\n1-4244-0707-9\/07\/$25.00 \u00a92007 IEEE\n \n \n \nviewed as a promising framework for designing effective and \npowerful classifiers.  \nII. THE STRUCTURE OF THE PROPOSED CLASSIFIER \nThe rule base that describes the non-linear evolving \nclassifier eClass can be described as a set of fuzzy rules of the \nfollowing form: \n( )iil\ni\nnn\nii\nl\nfyTHEN\nxisxANDANDxisxIFR\n=\n)(...)(: **11   (1) \nwhere Tnxxxx ],...,,[ 21= is the vector of features; ilR  \ndenotes the ith fuzzy rule; i=[1, Nl]; l=[1,L]; Nl is the number \nof fuzzy rules per cluster; L is the number of classes (note that \nNL \u2264 ; \u2211\n=\n= L\nl\nlNN\n1\n, N is the overall number of fuzzy \nrules; that is there is at least one fuzzy rule per \nclass); ( )*ijj xisx denotes the jth fuzzy set of the ith fuzzy rule; \nj=[1,n]; \n*i\nx is the prototype (focal point) of the ith  rule \nantecedent; ],...,,[ 21\ni\nl\niii yyyy =  is the L-dimensional \nbinary output of the MIMO exTS [22] fuzzy system. \nNote that the type of the fuzzy rule depends on the type of \nthe consequent [22]: \na) first order Takagi-Sugeno, TS (the consequents are \nlinear classifiers): \n[ ]\nT\ni\nnl\ni\nn\ni\nn\ni\nl\nii\ni\nl\nii\nTi xf\n\u23a5\u23a5\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n=\n\u03b1\u03b1\u03b1\n\u03b1\u03b1\u03b1\n\u03b1\u03b1\u03b1\nK\nKKKK\nK\nK\n21\n11211\n00201\n,1\n (1a) \nb) zero order TS (the consequents are the class labels):  [ ]Ti liiif 00201 \u03b1\u03b1\u03b1=   (1b) \nThe output of the exTS, y is formed as [22]: \n\u2211\u2211\u220f\n\u220f\n=\n= =\n== N\ni\ni\nlN\nj\nn\nj\nj\ni\njl\nn\nj\nj\ni\njl\nl y\nx\nx\ny\n1\n1 1\n1\n)(\n)(\n\u00b5\n\u00b5\n  (1c) \nwhere ijl\u00b5 is the membership value of the jth feature, \nj=[1,n]; l=[1,L]. \nWe use the so called \u2018winner-takes-all\u2019 de-fuzzification to \ndetermine the correct class, which is the usual choice in \nclassification problems: \n( )lL\nl\nyClass\n1\nmaxarg\n=\n=              (2) \nThe membership functions that describe the closeness to \nthe prototype can be of any form, but usually the Gaussian \nbell function is preferred due to its generalization capabilities: \n2\n2\n1\n\u239f\u239f\u23a0\n\u239e\n\u239c\u239c\u239d\n\u239b\u2212\n= ijl\ni\njld\ni\njl e\n\u03c3\u00b5  i=[1,Nl];  j=[1,n]; l=[1,L]  (3) \nwhere ijld is the distance between a sample and the \nprototype (focal point) of the ith fuzzy rule; ijl\u03c3  is the spread \nof the membership function, which also represents the radius \nof the zone of influence of the fuzzy rule. \nNote that in order to simplify notations further the index l \nwill be omitted and a remark will be made that calculations \nare made per class.  \nThe spread of the membership \u03c3 is determined based on the \nscatter [26] of the data per cluster. In Figure 1 both, so called \n\u2018one sigma\u2019 and \u2018two sigma\u2019 zones, are given around each \nprototype (each fuzzy rule) for the IRIS data set using \nEuclidean distance. In this particular example there are two \nprototypes (respectively, two fuzzy rules) for one of the \nclasses and one rule for the other two classes. \nThe scatter resembles standard deviation and is given by \n[22]:  \n1)0(;)(\n)(\n1)(\n)(\n1\n2*\n,cos == \u2211\n=\nl\nkS\nj\nj\ni\nl\nl\nl\nxxd\nkS\nk \u03c3\u03c3   (4) \nwhere l=[1,L] is the number of clusters; )( *,cos ji xxd  \ndenotes the distance from cluster centre to new sample \nassigned into this cluster.  \n      The scatter can be updated recursively by: \n[ ] [ ] ( ) [ ][ ]2*,222 )1()(,)(1)1()( \u2212\u2212+\u2212= kkxxdkSkk lilll \u03c3\u03c3\u03c3  (5) \nWhen a new cluster\/rule is formed, Nl\u2190Nl+1, its initial \nlocal scatter [26] is approximated by the average of the local \nscatters for the existing fuzzy rules for that class [22]: \n\u2211\n=\n+ = ll\nN\ni\ni\nk\nl\nN\nN\nk\n1\n1 1)( \u03c3\u03c3   (6) \nThe structure of the proposed classifier is thus formed by \nsets of fuzzy rules of type (1) in such a way that there is at \nleast one fuzzy rule per class. The prototypes around which \nthe fuzzy rules are formed are samples selected from the \navailable data by unsupervised learning (eClustering). The \nlearning of the exTS fuzzy model is described in detail in \n[22].  \nIII. EVOLVING CLASSIFIERS FROM DATA STREAMS \nA. Potential of the prototypes and its recursive calculation \nThe eClass is initialized with the first data samples. A \nfuzzy rule is formed around each one of these samples per \nclass. Potential of the prototype is set to 1)( *111 =xP . \nConsequently, each data sample is first being classified to one \nof the classes defined in the classifier and then its suitability \nto become a prototype (to form another fuzzy rule) has been \nchecked. The decision whether a data sample is used to form a \nprototype or to replace an existing prototype is based on the \n221\nProceedings of the 2007 IEEE Symposium on Computational\nIntelligence in Image and Signal Processing (CIISP 2007)\n \n \n \ndata spatial density measure, called potential [13,24]. \nThe potential calculated for a data sample is a function of \nthe accumulated distance between this sample and all other \nsamples in the data space per class. Thus, it represents the \ndensity of the data that surrounds a certain data sample. \nOriginally [21,24] using Euclidean distance:  \n( )\n( )\n,..3,2;\n1)(\/)(1\n1)(\n1\n1)(\n1\n2\n=\n\u2212\u239f\u239f\u23a0\n\u239e\n\u239c\u239c\u239d\n\u239b \u2212+\n=\n\u2211\u2211\n=\n\u2212\n=\nk\nkSkxx\nkxP\nl\nn\nj\nkS\ni\nji\nkl\nl\n  (7) \nwhere Pk(xk) denotes the potential of the kth data sample,  \nxk. \nNote that formula (7) requires accumulating the \ninformation from history of all the data which obviously \ncontradicts the requirement for real-time and on-line \napplication. The derivation of the recursive version of the \npotential expression (7) is given in [13]. Starting from \nsubstituting (4) into (7) and performing certain manipulations \nthat are given in the Appendix we arrive at: \n( ) ( ) )1()(2)(1)()( 1)()( \u2212+\u2212+\u2212 \u2212= kkkkSk kSkxP l lk \u03b3\u03b2\u03b1  (8) \nwhere  ( )\u2211\n=\n= n\nj\nj kxk\n1\n2)()(\u03b1          (9a) \n( )\u2211 \u2211\u2212\n= =\n= 1)(\n1 1\n2)(\nkS\ni\nn\nj\nj\nk\nl\nix\u03b2          (9b) \n)()()(\n1\nkkxk j\nn\nj\nj \u0393=\u2211\n=\n\u03b3          (9c) \n\u2211\u2212\n=\n=\u0393\n1)(\n1\n)()(\nkS\ni\njj\nl\nixk            (9d) \nIn this equation, values )(k\u03b1 and )(k\u03b3  can easily be \ncalculated based on the availability of the current data point, \nx(k) only. The values )(k\u03b2  and )(kj\u0393  that require \naccumulation of past information can be easily stored in two \nvariables with small size (the scalar, )1( \u2212k\u03b2  and the \nn-dimensional vector-column ( )Tn kkkk )(),...,(),()( 21 \u0393\u0393\u0393=\u0393 ). \nThen one can calculate recursively )(k\u03b2  and )(kj\u0393  by [13]:  \n0)1(\n)1()1()(\n=\n\u2212+\u2212=\n\u03b2\n\u03b1\u03b2\u03b2 kkk\n  (10a) \n0)1(\n)1()1()(\n=\u0393\n\u2212+\u2212\u0393=\u0393\nj\njjj kzkk\n (10b)  \n     Each time a new data sample is read it affects the data \ndensity of the data space of the respective class, therefore the \npotentials of the previous centre needs to be updated. This \nupdate can also be done in a recursive way as detailed in [13], \nand no extra variable needs to be memorized, apart from the \ncurrent potential of the existing prototypes (focal points):  \n( ) ( )\n( ) ( )\u2211\n=\n\u2212\u2212\n\u2212\n\u2212++\u2212\n\u2212= n\nj\nj\nii\nk\ni\nkl\ni\nkli\nk\nkxkxkxPkxPkS\nkxPkSkxP\n1\n2**\n1\n*\n1\n*\n1*\n)()()()(2)(\n)(1)())((\n (11)  \nB. eClass Procedure \nThe proposed evolving fuzzy rule-based classifier can start \neither \u2018from scratch\u2019 (with an empty rule-base) or with some \npre-specified set of fuzzy rules in form of (1). Each new data \nsample that has been read can be used to upgrade or modify \nthe rule base if the label is also provided. We call this mode D \n(for Design) and we apply it in on-line mode (possibly in \nreal-time). If the label is not provided, the existing fuzzy rule \nbase will generate the predicted class, that is, it will work in \nmode C (for Classification). Note that eClass can work in any \ncombination of these two modes. For example, one can \nperform D for certain number of samples and afterwards \nperform C for another set of samples. This is close to the \noff-line classifiers design concept. One can also perform D \nand C for the same sample (performing C first and using the \nrule-base that existed before this sample was read and only \nafter the classification to perform D if the class label is \nprovided). This is close to the adaptive modelling and control \nconcept when the prediction and learning are combined in the \nsame time step. An important specific of eClass is that not \nonly the number of fuzzy rules but the number of classes, L \nmay also be evolving and does not need to be pre-fixed. This \nspecific is not used in this paper, because the two benchmark \nFig. 1 Classification for Iris problem: a snapshot illustrating four \nprototypes (two for the class Versicolor and one for each of the remaining \nclasses); \u03c3 are different for different clusters on different dimensions. \n222\nProceedings of the 2007 IEEE Symposium on Computational\nIntelligence in Image and Signal Processing (CIISP 2007)\n \n \n \n \nproblems that have been considered have a pre-specified and \nfixed number of features. This characteristic of eClass has \nbeen, however, used in a real-life example [27].  \nWhen eClass is in D mode from the second data sample \nonwards its potential, Pk(x(k)) is updated recursively by \n(8)-(9). Then the potential of each of the previously existing \nprototypes, ( ))(* kxP ik  is also updated using (11). Comparing \nthe potential of the new data sample with the potential of each \nof the existing prototypes the following three outcomes are \npossible: \na) ( ) ( ))(max)( *\n1\nkxPkxP ik\nR\nik\ni\n=>           (12) \nIf condition (12) occurs that means that we have a data \nsample that is strongly representative [21,22] and thus we add \na new prototype to the rule base. For each newly added \nprototype we form a new fuzzy rule based at that prototype as \na focal point. Additionally, we check whether any of the \nalready existing prototypes are described well by the newly \nadded fuzzy rule. By well we mean [22] that the value of the \nmembership function satisfies: ( ) ],1[;],1[,)(];,1[, 1 LlnjjekxNii jil ==\u2200>=\u2203 \u2212\u00b5  (13) \nFor the previously existing prototypes for which this is the \ncase, we remove them. \nAlternatively (if (12) does not hold), we do not change the \noverall structure of the classifier. \nThe procedure for the evolving fuzzy rule based classifier \neClass when it applies a joint classification and classifier \ndesign (C+D) can be summarised in the following \npseudo-code: \n \nBEGIN eClass (C + D) \nInitialize (get first data sample or \nstart from a pre-trained classifier); \nDO for the pair xk, k=2,3,\u2026 WHILE data \nstream ends \n Classify xk (assign it to a class, C\nl); \n Calculate Pk(xk)using (8)-(9); \n Update P(x*) using (11) \n Calculate the potential difference \nIF (12) THEN add a new fuzzy rule \naround xk; \nIF (11) THEN remove the prototype(s), \nxi* for which it is true;  \nGet the real class label, Cl  \nEND DO \n \n \nThe flow chart (Figure 2) describes the structure of eClass \nas a server thread in real-time application implementations. \n \nIV. EXPERIMENTAL RESULTS \nThe proposed classifier, eClass was tested on two well \nknown benchmark problems. It should be noted that despite \nthe clearly off-line nature of these benchmark problems they \nwere used to test the proposed classifier in order to have some \ncomparison. In a parallel paper [27], a real-life application for \nimage data on-line classification is presented. Results on \nthese two datasets are also compared with the results from the \napproach \u2018Incremental Principle Component Analysis\u2019 [28]. \nA. Iris Data set \nThe Iris data set is a widely used benchmark for \nclassification studies. The data set has three classes that \nrepresent three types of the Iris plants, namely Iris Setosa, Iris \nVerginica, and Iris Versicolor. There are four features of the \nplants that are available for all the samples in the data set. \nThey represent the sepal and petal lengths and widths in cm. \nThe data set consists of 150 samples that is, 50 for each \nspecies. In [25] 19 different classifiers were reviewed, and all \nof them give between three and 24 misclassifications. The \nresults when using eClass are tabulated in Table I. When \ncomparing the result given by eClass with these result, one \nshould take into account the fact that eClass starts in this \nexperiment \u2018from scratch\u2019 with an empty rule base and no \npre-training and evolves its structure from the data stream. \nIn our experiment, tests have been carried out in three \ndifferent ways. In the first test we classify each sample first \nwithout knowing its label. Afterwards, we get its correct class \nlabel and use the pair (sample and label) to evolve the \nrule-base. The joint classification and learning process (C + \nD) is continued during the whole process of 150 samples. The \nsamples are in the original order as presented in the UCI \nrepository. In the second test, 100 randomly ordered samples \nare used to evolve the classifier (D mode), and the rest 50 are \nused to test (C mode). In the last test, so called \u2018leave one out\u2019 \nstrategy is used that is eClass runs 150 times, each time \n \nFig. 2 Flow chart of Evolving Classification for real-time applications. \n223\nProceedings of the 2007 IEEE Symposium on Computational\nIntelligence in Image and Signal Processing (CIISP 2007)\n \n \n \nhaving 149 samples for classifier design (D) and the \nremaining single sample is used for classification.  \nB. Wine Reproduction Data \nWine Reproduction data set is another common benchmark \nproblem. The data set comes from the chemical analysis of \nwines grown in Italy derived from three different cultivars. \nThere are three classes, 178 samples with thirteen continuous \nnumerical features available in the data set.  \nIn a similar way the three experiments have been carried \nout with this data set. In the third experiment, 80 randomly \norganized samples are used as training data to evolve the rule \nbase, and the remaining 98 data samples are used to test the \nfixed classifier. All the results of testing eClass with Wine \ndata set are tabulated in Table II. \nBoth tests on Iris data and Wine data shows that the \nproposed classifier eClass has the advantage of that evolving \nits structure from scratch without losing much precision \n(classification rate). In the online evolving mode, the \nperformance slightly deteriorates, but is comparable to the \nresults of other off-line approaches reported in [25]. In its \noffline mode, eClass has results of the same level of precision \nas as the best of the classifiers reported in [25]. \n \nV. CONCLUSION \nA novel approach, eClass, to on-line classification is \nintroduced in this paper. It stems from subtractive and \nMountain clustering [24]. It works in online mode and can \nwork in real-time. It is non-iterative, and recursive \ncalculations are used which avoids memorizing the whole \nhistory of the data without loss of information about the data \ndensity. This enables low memory requirements and much \nless computation. The learning process can start \u2018from \nscratch\u2019 learning and adapting to the new data samples. It can \nalso start from some initial rule-based classifier that can be \nupdated based on the new information contained in the new \nsamples. Thus it is suitable for on-line and real-time \napplications such as classification of signals and streaming \ndata, robotic applications, e.g. target and landmark \nrecognition, real-time machine health monitoring and \nprognostics, fault detection and diagnostics etc. It can work \nfor classification or evolvable classifier design only or for \njoint classification and classifier design. A novel formula for \nrecursive calculation of the cosine distance is introduced in \nthe paper that is suitable for on-line and real-time \napplications. Experiments with well known benchmark \nclassification problems (Iris and wine data sets) have been \nperformed and the results are very promising. The results \nindicate that eClass can achieve high classification rate \n(comparable or better than the off-line pre-fixed classifiers) \nas reported in the literature for two benchmark problems. At \nthe same time it has a transparent form and can accommodate \nnew data samples integrating the new data and existing \nknowledge. \nREFERENCES \n[1] R. O. Duda, P. E. Hart, and D.G. Stork. \u201cPattern Classification\u201d - \nSecond Edition. Wiley-Interscience, Chichester, West Sussex, England, \n2000. \n[2] U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, From Data Mining to \nKnowledge Discovery: An Overview, Advances in Knowledge \nDiscovery and Data Mining, MIT Press, Cambridge, Massachusetts, \nUSA, 1996. \n[3] P. Domingos and G. Hulten, \u201c Catching Up with the Data: Research \nIssues in Mining Data Streams, Workshop on Research Issues in Data \nMining and Knowledge Discovery, Santa Barbara, CA, USA, 2001. \n[4] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical \nLearning: Data Mining, Inference and Prediction. Heidelberg, \nGermany: Springer Verlag, 2001. \n[5] V. N. Vapnik, The Statistical Learning Theory, Springer Verlag, Berlin, \nGermany, 1998. \n[6] V. Kecman. Learning and Soft Computing. MIT Press, Cambridge, \nMassachusetts, London, England, 2001. \n[7] J. R. Quinlan, \u201cSimplifying decision trees\u201d, International Journal of \nMan-Machine Studies, vol. 27, No3, pp.221\u2013234, 1987. \n[8] C. M. Bishop, Neural Networks for Pattern Recognition, Oxford \nUniversity Press, Oxford, UK, 1995. \n[9] M. Butz, Rule-based Evolutionary Online Learning Systems: A \nPrincipal Approach to LCS Analysis and Design, Physica Verlag, \nv.191, Berlin, Heidelberg, Germany, 2006, ISBN 3-540-25379-3. \n[10] K. M. A. Chai, H. T. Ng, and H. L. Chieu, \u201cBayesian Online Classifiers \nfor Text Classification and Filtering\u201d, Proc.of the SIGIR\u201902, August \n11-15, 2002, pp. 97-104, Tampere, Finland. \n[11] F. Klawonn and P. Angelov, Evolving Extended Naive Bayesian \nClassifier, 2006 IEEE Intern. Conf. on Data Mining, 18-22 December, \n2006, Hong Kong, to appear \n[12] P. Angelov, Evolving Rule-based Models: A Tool for Design of Flexible \nAdaptive Systems. Heidelberg, Germany: Springer Verlag, 2002. \n[13] P. Angelov and D. Filev, \"An approach to on-line identification of \nevolving Takagi-Sugeno models\", IEEE Transactions on Systems, Man \nand Cybernetics, part B, Cybernetics, vol.34, No1, pp. 484-498, 2004. \n[14] P. Angelov and R. Buswell, \u201cIdentification of Evolving Rule-based \nModels\u201d, IEEE Transactions on Fuzzy Systems, vol.10, No5, \npp.667-677, 2002. \n[15] P. Angelov, V. Giglio, C. Guardiola, E. Lughofer, and J. M. Lujan, \u201cAn \nApproach to Model-based Fault Detection in In- dustrial Measurement \nSystems with Application to Engine Test Benches\u201d, Measurement \nScience and Technology, vol.17, No7, 2006, pp.1809-1818. \n[16] P. Angelov, \u201cA Fuzzy Controller with Evolving Structure\u201d, \nInformation Sciences, ISSN 0020-0255, vol.161, 2004, pp.21-35. \nTABLE I \nRESULTS FOR IRIS PROBLEM \n Design Classify Mode Rules Rate \nI 150  150 (C&D)x150 4 98.0% \nII 100 50 100D+50C 4 96.0% \nIII 149 (x 150) \n1 \n(x 150) \n149D+1C \n(x150) 4 99.3% \niPCA \n[28] 178 178 - \n2 eigen \nvectors 93.3% \n \nTABLE II \nRESULTS FOR WINE PROBLEM \n Design Classify Mode Rules Rate \nI 178  178 (C&D)x178  7 96.1% \nII 80 98 80D+98C 7 95.9% \nIII 177 (x 150) \n1 \n(x 150) \n177D+1C \n(x150) 7 98.7% \niPCA \n[28] 178 178 - \n7 eigen \nvectors 87.6% \n \n224\nProceedings of the 2007 IEEE Symposium on Computational\nIntelligence in Image and Signal Processing (CIISP 2007)\n \n \n \n[17] A. S. Hornby, Oxford Advance Learner\u2019s Dictionary, Oxford \nUniversity Press, UK, 1974. \n[18] B. Fritzke, \u201cGrowing cell structures \u2013 a self-organizing network for \nunsupervised and supervised learning\u201d, Neural Networks, vol. 7, No 9, \npp.1441-1460, 1994. \n[19] N. Kasabov, \u201cEvolving fuzzy neural networks for on-line \nsupervised\/unsupervised, knowledge-based learning,\u201d IEEE \nTransactions on Systems, Man, and cybernetics, part B \u2013 Cybernetics, \nvol. 31, pp. 902-918, 2001. \n[20] R. Jin and G. Agrawal, \u201cEfficient Decision Tree Construction on \nStreaming Data\u201d, Proc. of ACM SIGKDD, 2003. \n[21] P. Angelov and R. Buswell, Evolving Rule-based Models: A Tool for \nIntelligent Adaptation, Proc. of the 9th IFSA World Congress, \nVancouver, BC, Canada, 25-28 July 2001, pp.1062-1067. \n[22] P. Angelov and X. Zhou, \u201cEvolving Fuzzy Systems from Data Streams \nin Real-Time\u201d, Proc. 2006 International Symposium on Evolving Fuzzy \nSystems, UK, IEEE Press, pp.29-35, September 2006, ISBN \n0-7803-9719-3. \n[23] L.-X. Wang, \u201cFuzzy Systems are Universal Approximators\u201d, Proc.  1st \nIEEE International Conference on Fuzzy Systems, FUZZ-IEEE, San \nDiego, CA, USA, pp.1163-1170, 1992. \n[24] R. R. Yager and D. P. Filev, \u201cLearning of Fuzzy Rules by Mountain \nClustering,\u201d Proc. of SPIE Conference on Application of Fuzzy Logic \nTechnology, Boston, MA, USA, pp.246-254,1993. \n[25] H. Ishibuchi and T. Nakashima, \u201cVoting in fuzzy rule-based systems \nfor pattern classification problems\u201d, Fuzzy Sets and Systems, vol. 103, \npp.223-238, 1999. \n[26] P. Angelov and D. Filev, \u201cSimpl_eTS: A Simplified Method for \nLearning Evolving Takagi-Sugeno Fuzzy Models\u201d, The 2005 IEEE \nInternational Conference on Fuzzy Systems FUZZ-IEEE, Reno, Las \nVegas, USA, 22-25 May 2005, pp.1068-1073, ISSN 0-7803-9158-6\/05. \n[27] X. Zhou and P. Angelov, \u201cAutonomous Self-localization in Completely \nUnknown Environment using Evolving Fuzzy Rule-based Classifier\u201d, \n1st IEEE Symposium on Computational Intelligence for Security and \nDefense Applications, CISDA 2007, April 1-5, 2007 Honolulu, HI, \nUSA, to appear. \n[28] S.Pang, S.Ozawa, and N.Kasabov, \u201cIncremental Linear Discriminant \nAnalysis for Classification of Data Streams\u201d, IEEE Transactions on \nSystems, Man, And Cybernetics \u2013Part B: Cybernetics, Vol35, No.5, \nOctober 2005. \n[29] E. Lughofer and E. Klement, \u201cFLEXFIS: A variant for incremental \nlearning of Takagi-Sugeno fuzzy systems,\u201d in Proceedings of \nFUZZ-IEEE 2005, Reno, Nevada, U.S.A., 2005. \n225\nProceedings of the 2007 IEEE Symposium on Computational\nIntelligence in Image and Signal Processing (CIISP 2007)\n"}