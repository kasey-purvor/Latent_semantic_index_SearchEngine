{"doi":"10.1007\/978-3-540-74853-3_29","coreId":"70134","oai":"oai:eprints.lancs.ac.uk:13040","identifiers":["oai:eprints.lancs.ac.uk:13040","10.1007\/978-3-540-74853-3_29"],"title":"Cooperative Augmentation of Smart Objects with Projector-Camera Systems","authors":["Molyneaux, David","Gellersen, Hans","Kortuem, Gerd","Schiele, Bernt"],"enrichments":{"references":[{"id":16329838,"title":"A Projector-Camera System with Real-Time Photometric Adaptation for Dynamic Environments,","authors":[],"date":"2005","doi":"10.1109\/cvpr.2005.41","raw":"A  Projector-Camera  System  with  Real-Time  Photometric  Adaptation  for  Dynamic Environments, K. Fujii, M.D. Grossberg and S.K. Nayar, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Vol.1, pp.814-821, Jun, 2005.","cites":null},{"id":16329832,"title":"A Wearable Mixed Reality with On-board Projector,","authors":[],"date":"2003","doi":"10.1109\/ismar.2003.1240740","raw":"A Wearable Mixed Reality with On-board Projector, T. Karitsuka, K. Sato, In Second IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR2003), 7-10 October 2003, Tokyo, Japan.","cites":null},{"id":16329830,"title":"Adaptive background mixture models for real-time tracking,","authors":[],"date":"1999","doi":"10.1109\/cvpr.1999.784637","raw":"Adaptive  background  mixture  models  for  real-time  tracking,  C.  Stauffer  and  W.  E.  L. Grimson, In Computer Vision Pattern Recognition, pages 246--252, Ft. Collins, CO, 1999.","cites":null},{"id":16329840,"title":"Affine invariant interest point detectors,","authors":[],"date":"2004","doi":"10.1023\/b:visi.0000027790.02288.f2","raw":"Scale and Affine invariant interest point detectors, K. Mikolajczyk and C. Schmid, In Proceedings of IJCV 60(1):63-86, 2004.","cites":null},{"id":16329819,"title":"An Architecture and Framework for Steerable Interface Systems, Levas,","authors":[],"date":"2003","doi":"10.1007\/978-3-540-39653-6_26","raw":"An Architecture and Framework for Steerable Interface Systems, Levas, A., Pinhanez, C., Pingali, G., Kjeldsen, R., Podlaseck, M., Sukaviriya, N., in Proceedings of UbiComp 2003.","cites":null},{"id":16329848,"title":"conditional density propagation for visual tracking, Michael Isard and Andrew Blake,","authors":[],"date":"1998","doi":"10.1007\/bfb0015549","raw":"CONDENSATION -- conditional density propagation for visual tracking, Michael Isard and Andrew Blake, In International Journal of Computer Vision, 29, 1, 5--28, 1998.","cites":null},{"id":16329825,"title":"Context Acquisition based on","authors":[],"date":"2002","doi":"10.1007\/3-540-45809-3_26","raw":"Context Acquisition based on Load Sensing, A. Schmidt, M. Strohbach, K. Van Laerhoven, A. Friday and H.-W. Gellersen, In Proceedings of Ubicomp 2002, G. Boriello and L.E. 18 Holmquist  (Eds).  Lecture  Notes  in  Computer  Science,  Vol  2498,  Springer  Verlag, Gothenburg, Sweden, September 2002, pp. 333 \u2013 351.","cites":null},{"id":16329823,"title":"Cooperative Artefacts: Assessing Real World Situations with Embedded","authors":[],"date":"2004","doi":"10.1007\/978-3-540-30119-6_15","raw":"Cooperative Artefacts: Assessing Real World Situations with Embedded Technology. M. Strohbach, H.-W. Gellersen, G. Kortuem and Christian Kray, In Proceedings of Ubicomp 2004, Nottingham, UK.","cites":null},{"id":16329828,"title":"Cooperative Artefacts: Assessing Real World Situations with Embedded Technology,","authors":[],"date":"2004","doi":"10.1007\/978-3-540-30119-6_15","raw":"Cooperative Artefacts: Assessing Real World Situations with Embedded Technology, M. Strohbach, H.-W. Gellersen, G. Kortuem and C. Kray, In Proceedings of: Ubicomp 2004.","cites":null},{"id":16329839,"title":"eSeal \u2013 a system for enhanced electronic assertion of authenticity and integrity.","authors":[],"date":"2004","doi":"10.1007\/978-3-540-24646-6_19","raw":"eSeal \u2013 a system for enhanced electronic assertion of authenticity and integrity. C. Decker, M. Beigle, A. Krohn, P. Robinson, U. Kubach, In Pervasive 2004 (2004) Vienna, Austria.","cites":null},{"id":16329817,"title":"Interacting with a Self-Describing World via Photosensing Wireless Tags","authors":[],"date":null,"doi":"10.1145\/1198555.1198717","raw":"RFIG Lamps: Interacting with a Self-Describing World via Photosensing Wireless Tags and Projectors, R. Raskar, P. Beardsley, J. van Baar, Y. Wang, P.Dietz, J. Lee, D. Leigh, T. Willwatcher, In Proceedings of SIGGRAPH 2004, Los Angeles, USA.","cites":null},{"id":16329806,"title":"Lamps: Painting on Movable Objects,","authors":[],"date":"2001","doi":"10.1109\/isar.2001.970539","raw":"Dynamic Shader Lamps: Painting on Movable Objects, D. Bandyopadhyay, R. Raskar, H. Fuchs, In Proc. IEEE and ACM Int. Symposium on Augmented Reality, New York, 2001.","cites":null},{"id":16329813,"title":"Moveable Interactive Projected Displays Using Projector Based Tracking,","authors":[],"date":"2005","doi":"10.1145\/1095034.1095045","raw":"Moveable Interactive Projected Displays Using Projector Based Tracking, J. C. Lee, S. E. Hudson, J. W. Summet, P. H. Dietz, Proceedings of the ACM Symposium on User Interface Software and Technology (UIST), pages 63-72, Seattle, WA. October 23-26, 2005.","cites":null},{"id":16329808,"title":"Projected Augmentation \u2013 Augmented Reality using Rotatable Video Projectors,","authors":[],"date":"2004","doi":"10.1109\/ismar.2004.47","raw":"Projected Augmentation \u2013 Augmented Reality using Rotatable Video Projectors, J. Ehnes, K. Hirota,  M.  Hirose,  Third  IEEE  and  ACM  International  Symposium  on  Mixed  and Augmented Reality (ISMAR'04), September-October, 2004 Arlington, VA, USA.","cites":null},{"id":16329811,"title":"Projecting Rectified Images In an Augmented Environment,","authors":[],"date":"2003","doi":null,"raw":"Projecting  Rectified  Images  In  an  Augmented  Environment,  S.  Borkowski,  O.  Riff,  J. Crowley, IEEE International Workshop on Projector-Camera Systems (PROCAMS-2003), Nice, France, October 12, 2003.","cites":null},{"id":16329850,"title":"Random sample consensus: a paradigm for model fittingwith applications to image analysis and automated","authors":[],"date":"1981","doi":"10.1145\/358669.358692","raw":"Random sample consensus: a paradigm for model fittingwith applications to image analysis and automated cartography, M.A. Fischler, and R.C. Bolles, Communications of the ACM 24, 6 (Jun. 1981), 381-395.","cites":null},{"id":16329852,"title":"Recognition without Correspondence using Multidimensional Receptive Field Histograms,","authors":[],"date":"2000","doi":"10.1109\/icpr.1996.546722","raw":"Recognition without Correspondence using Multidimensional Receptive Field Histograms, International Journal of Computer Vision, 36(1), pp.31-50, 2000.","cites":null},{"id":16329836,"title":"Searchlight: A Lightweight Search Function for Pervasive Environments,","authors":[],"date":"2004","doi":"10.1007\/978-3-540-24646-6_26","raw":"Searchlight: A Lightweight Search Function for Pervasive Environments, Andreas Butz, Michael  Schneider,  and  Mira  Spassova,  Pervasive  Computing,  Second  International Conference, PERVASIVE 2004, Vienna, Austria, April 21-23, 2004.","cites":null},{"id":16329842,"title":"SIFT: Distinctive image features from scale invariant keypoints, D. Lowe,","authors":[],"date":"2004","doi":"10.1023\/b:visi.0000029664.99615.94","raw":"SIFT: Distinctive image features from scale invariant keypoints, D. Lowe, In Proceedings of IJCV 60(2):91-110, 2004","cites":null},{"id":16329837,"title":"Spatial Augmented Reality Merging Real and Virtual","authors":[],"date":null,"doi":"10.1201\/b10624","raw":"Spatial Augmented Reality Merging Real and Virtual Worlds, O. Bimber and R. Raskar, A K Peters LTD (publisher), ISBN: 1-56881-230-2.","cites":null},{"id":16329834,"title":"The Everywhere Displays Projector: A Device to Create Ubiquitous Graphical Interfaces,","authors":[],"date":"2001","doi":"10.1007\/3-540-45427-6_27","raw":"The Everywhere Displays Projector: A Device to Create Ubiquitous Graphical Interfaces, C. Pinhanez, Proceedings of Ubiquitous Computing 2001 (Ubicomp'01), September 2001.","cites":null},{"id":16329821,"title":"The MediaCup: Awareness Technology embedded in an Everyday","authors":[],"date":"1999","doi":"10.1007\/3-540-48157-5_30","raw":"The MediaCup: Awareness Technology embedded in an Everyday Object, H. Gellersen, M. Beigl,  H.  Krull,  1st  International  Symposium  on  Handheld  and  Ubiquitous  Computing (HUC99), Karlsruhe, Germany, 1999. Lecture notes in computer science; Vol 1707,  H-W Gellersen ed, ISBN 3-540-66550-1; Springer, 1999, pp 308-310.","cites":null},{"id":16329844,"title":"The Particle Computer System, IPSN Track on Sensor Platform, Tools and Design Methods for Networked Embedded Systems (SPOTS), Christian Decker, Albert Krohn, Michael Beigl, Tobias Zimmer,","authors":[],"date":"2005","doi":null,"raw":"The Particle Computer System, IPSN Track on Sensor Platform, Tools and Design Methods for Networked Embedded Systems (SPOTS), Christian Decker, Albert Krohn, Michael Beigl, Tobias Zimmer, In Proceedings of the ACM\/IEEE 4th International Conference on Information Processing in Sensor Networks (IPSN05), pp443-448, Los Angeles, April 2005.","cites":null},{"id":16329810,"title":"Tracking and HMD Calibration for a video-based Augmented Reality Conferencing","authors":[],"date":"1999","doi":"10.1109\/iwar.1999.803809","raw":"Marker Tracking and HMD Calibration for a video-based Augmented Reality Conferencing System,  Kato,  H.,  Billinghurst,  M.  (1999),  In  Proceedings  of  the  2nd  International Workshop on Augmented Reality (IWAR 99). October 1999, San Francisco, USA.","cites":null},{"id":16329815,"title":"Tracking Locations of Moving Hand-held Displays Using Projected","authors":[],"date":"2005","doi":"10.1007\/11428572_3","raw":"Tracking Locations of Moving Hand-held Displays Using Projected Light, J. Summet and R. Sukthankar, In Proceedings of Pervasive 2005, Munich, Germany.","cites":null},{"id":16329826,"title":"Ubiquitous Interaction - Using Surfaces in Everyday Environments as Pointing Devices,","authors":[],"date":null,"doi":"10.1007\/3-540-36572-9_21","raw":"Ubiquitous Interaction - Using Surfaces in Everyday Environments as Pointing Devices, A. Schmidt, M. Strohbach, K. Van Laerhoven, and H.W. Gellersen, 7th ERCIM Workshop &quot;User Interfaces For All&quot;, 23 - 25 October, 2002.","cites":null},{"id":16329846,"title":"What\u2019s Real About Virtual Reality?,","authors":[],"date":"1999","doi":"10.1109\/vr.1999.756916","raw":"What\u2019s Real About Virtual Reality?, F.P. Brooks, Jr., IEEE Computer Graphics and Applications, 19, 6: 16-27, 1999.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2007-09","abstract":"In this paper we present a new approach for cooperation between mobile smart objects and projector-camera systems to enable augmentation of the surface of objects with interactive projected displays. We investigate how a smart object's capability for self description and sensing can be used in cooperation with the vision capability of projector-camera systems to help locate, track and display information onto object surfaces in an unconstrained environment. Finally, we develop a framework that can be applied to distributed projector-camera systems, cope with varying levels of description knowledge and different sensors embedded in an object","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/70134.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/13040\/1\/Ubicomp%2D2007%2DMolyneaux.pdf","pdfHashValue":"db424cf901ee98bc0270e311f7e391335840a77f","publisher":"Springer-Verlag Ltd.","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:13040<\/identifier><datestamp>\n      2018-01-24T02:08:48Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D626F6F6B5F73656374696F6E<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Cooperative Augmentation of Smart Objects with Projector-Camera Systems<\/dc:title><dc:creator>\n        Molyneaux, David<\/dc:creator><dc:creator>\n        Gellersen, Hans<\/dc:creator><dc:creator>\n        Kortuem, Gerd<\/dc:creator><dc:creator>\n        Schiele, Bernt<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        In this paper we present a new approach for cooperation between mobile smart objects and projector-camera systems to enable augmentation of the surface of objects with interactive projected displays. We investigate how a smart object's capability for self description and sensing can be used in cooperation with the vision capability of projector-camera systems to help locate, track and display information onto object surfaces in an unconstrained environment. Finally, we develop a framework that can be applied to distributed projector-camera systems, cope with varying levels of description knowledge and different sensors embedded in an object.<\/dc:description><dc:publisher>\n        Springer-Verlag Ltd.<\/dc:publisher><dc:date>\n        2007-09<\/dc:date><dc:type>\n        Contribution in Book\/Report\/Proceedings<\/dc:type><dc:type>\n        NonPeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/13040\/1\/Ubicomp%2D2007%2DMolyneaux.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1007\/978-3-540-74853-3_29<\/dc:relation><dc:identifier>\n        Molyneaux, David and Gellersen, Hans and Kortuem, Gerd and Schiele, Bernt (2007) Cooperative Augmentation of Smart Objects with Projector-Camera Systems. In: Proc. Ubicomp 2007: 9th International Conference on Ubiquitous Computing. Springer-Verlag Ltd., pp. 501-518.<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/13040\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1007\/978-3-540-74853-3_29","http:\/\/eprints.lancs.ac.uk\/13040\/"],"year":2007,"topics":["QA75 Electronic computers. Computer science"],"subject":["Contribution in Book\/Report\/Proceedings","NonPeerReviewed"],"fullText":"Cooperative Augmentation of Smart Objects with \nProjector-Camera Systems \nDavid Molyneaux1, Hans Gellersen1, Gerd Kortuem1 and Bernt Schiele2 \n1Computing Department, Lancaster University, England \n2Computer Science Department, Darmstadt University of Technology \n1{d.molyneaux, hwg, kortuem}@comp.lancs.ac.uk  2schiele@informatik.tu-darmstadt.de \nAbstract. In this paper we present a new approach for cooperation between \nmobile smart objects and projector-camera systems to enable augmentation of \nthe surface of objects with interactive projected displays. We investigate how a \nsmart object\u2019s capability for self description and sensing can be used in \ncooperation with the vision capability of projector-camera systems to help \nlocate, track and display information onto object surfaces in an unconstrained \nenvironment. Finally, we develop a framework that can be applied to \ndistributed projector-camera systems, cope with varying levels of description \nknowledge and different sensors embedded in an object. \n \nKeywords: Cooperative Augmentation, Smart Objects, Projector-Camera \nSystems \n1 Introduction \nThe interest in embedding sensing, communication and computation in everyday \nphysical artefacts is growing. Such smart objects are expected to bridge the gap \nbetween the physical and digital world, and become part of out lives in economically \nimportant areas such as retail, supply chain or asset management [29,30,31] and \nsafety critical situations in work places [10]. A challenge for the design of such smart \nobjects is to preserve their original appearance, purpose and function, thus exploiting \nnatural interaction and a user\u2019s familiarity with the object [12]. Consequently adding \noutput capability to objects is difficult, as embedding displays would fundamentally \nchange an objects appearance. Mobile objects are also typically constrained in terms \nof power, weight and space availability. However, the recent availability of small, \ncheap and bright video projectors makes them practical for augmenting objects with \nnon-invasive displays. By adding a camera and using computer vision techniques, a \nprojector system can also dynamically detect and track objects [2,4], correct for object \nsurface geometry [2,4,16,18], varying surface colour and texture [19] and allow the \nuser to interact directly with the projected image [1,30]. \nWe can imagine an unconstrained environment in the future containing many smart \nobjects. In this environment new objects can arrive, move around or be manipulated \nby users and leave. If we assume projector-camera systems are installed ubiquitously \nin this environment offering a display service, the smart objects can request use of the \n2       \nprojection capability to obtain a display on its surface and solve its output problem. \nTo realise this vision we have to address the two challenges of how the object can \nmake use of the projector-camera system capability to be a). Located and tracked, and \nb). Projected on so the display is undistorted and visible to the user.  \nIn this paper we investigate a new approach to these challenges by using \nspontaneous cooperation between the smart object and projector-camera system.  In \nparticular, we investigate how capabilities of the smart object (such as knowledge \nstorage and sensing) can assist projector-camera systems in the object detection, \ntracking and projection tasks.  \nIn cooperative augmentation there is a division of labour between the projector-\ncamera system and smart object as follows: \n! The objects themselves are self-describing. They carry information about \nthemselves (such as knowledge of their appearance) that is vital to the \ndetection process. We call this information the Object Model. \n! The projector-camera system provides a display service that can be used by \nany smart object in the vicinity. The projector-camera system display service \nis generic, as it holds no knowledge about any of the objects. Consequently, it \ncould be used by any type of smart object, for example, smart cups [9], smart \nchemical containers [10] or smart tables [11].   \n! The Object Model is transmitted to the projector-camera system whenever the \nobject enters proximity of the projector-camera system. \nThe projector-camera system uses the Object Model to dynamically tailor its \nservices to the object.  In contrast to traditional vision-based detection approaches \nwhere all object knowledge is held in the detection system, no user intervention is \nrequired to configure the detection and projection system for new objects. Objects \nbring all information with them so system configuration happens automatically in \nresponse to the Object Model. The object detection task is also made simpler and \nfaster as the projector-camera system need not maintain and search a large database of \nobject information. With the registration process the cooperative augmentation system \nalways knows which smart objects exist in the environment. \nThe cooperative augmentation approach is flexible as the dynamic configuration \nprocess caters for varying amounts of knowledge stored in the object. The projector-\ncamera system can also use its camera in a learning process to extract more \nappearance knowledge about the object over time and re-embed it within the object.  \nThe main contribution of the cooperative augmentation approach is a flexible \nframework to allow smart objects to spontaneously use projection capability in an \nenvironment for output.  Our approach can locate and track mobile objects in the \nenvironment, determine suitable areas for projection and finally align the projection \nwith the object\u2019s surfaces so it appears undistorted, as shown in Figure 1. \nIn section 2 we compare our approach to related work. Section 3 follows with an \nanalysis of the cooperative augmentation process in detail, with reference to a real \nworld example. Sections 4 and 5 explain the visual detection process and projection \nprocess in more detail. Section 6 validates our concept using an example \nimplementation of the cooperative augmentation concept. Finally, section 7 discusses \nthe concept evaluation and lessons learned. \n      3 \n \nSmart Object\nProjector Camera\nSystem\nSensors\nWireless Link\n \nFig. 1. Cooperative Augmentation of Smart Objects with Projector-Camera Systems \n2 Related Work \nThe question of how to augment mobile objects with projected displays was \ninvestigated by Bandyopadhyay et al. in [1]. Objects with planar surfaces were \nequipped with a magnetic and infra-red tracking system. Static projectors were used \nto augment the objects in real-time.  However, this work suffered from two key \nproblems of latency and limited working volume due to the tracking systems used. \nOur approach uses a projector-camera system with a vision-based object detection \nsystem. This allows augmentation of objects anywhere within the field of view of the \nsystem at camera frame-rates, without relying on separate tracking hardware. The use \nof a camera also allows direct interaction with the projection, for example, by visual \ndetection and tracking of the user\u2019s fingertips as described by Kjeldsen et al. in [30]. \nAlthough there is an enormous body of work on detection and location of mobile \nnon-smart objects using a camera, there is little work which uses the capabilities of \nsmart objects themselves. For example, vision-based detection and tracking \napproaches have been taken by Ehnes et al. in [2], using AR Toolkit fiducial markers \n[3] to track and project on mobile planar surfaces. Borkowski et al. also demonstrate a \nmobile projected interactive display screen object tracked by its black border in [4].  \nHowever, both these systems rely on modifying or engineering the external \nappearance of a non-smart object to enable detection. In contrast, our approach uses \nfeatures of the natural appearance of a smart object for detection. \nThe sensing capabilities of smart objects were used by Raskar et al. in [7] to detect \nthe location and orientation of static smart objects relative to a handheld projector. \nHere embedded light sensors detected the projection of gray codes (which encode a \nspatial location by changes in brightness over time) onto the object\u2019s surface to \ndirectly locate the object in the projector\u2019s frame of reference. Projection onto mobile \nplanar smart objects was addressed using the same techniques by Summet and \nSukthankar in [6] and Lee et al. in [5] where a 12Hz location update rate was \nachieved. For these techniques a minimum of one un-occluded light sensor is required \nto be in the view of the projector to enable detection. 3D location and orientation of \nan object can be calculated from a static projector location with three light sensors in \nview of the projector, however, 3D or self-occluding objects require many more light \n4       \nsensors to guarantee correct pose calculation. For example, cubical objects require at \nleast 3 sensors per face (18 total) to detect all poses. \nIn contrast, our cooperative augmentation approach does not require a minimum \nnumber of light sensors to operate. Instead, we use appearance knowledge stored in \nthe object to visually detect the object with algorithms that offer robustness to partial \nocclusion. Movement sensor information is used to further constrain the detection \ntask and distinguish between objects with similar appearances.  \nThere exist many implementations of projector-camera systems \u2013 for example, we \ncan decompose existing systems into three categories with respect to display mobility: \n1. Static projector-camera systems \n2. Steerable projection from static system with pan and tilt hardware \n3. Mobile, handheld and wearable projector-camera systems \nAll types of projector-camera system have been used for augmenting objects with \nprojection, however, static [1], mobile, handheld [7] or wearable [15] projector-\ncamera systems can only opportunistically detect and project on objects passing \nthrough the field of view of the projector and camera. \nIn contrast, projector-camera systems in the second category with computer \ncontrolled steerable mirrors or pan and tilt platforms [16][2][4][17] allow a much \nlarger system field of view and the ability to track objects moving in the environment.  \nLevas et al. first presented a framework for steerable projector-camera systems to \nproject onto objects and surfaces in their Everywhere Display framework [8]. \nHowever, although supporting a distributed architecture, this framework was limited \nto creating displays on static surfaces in locations pre-calibrated by the user.  Our \ncooperative augmentation approach enables spontaneous displays on the surfaces of \nmobile smart objects without user intervention or calibration.  \n3 Cooperative Augmentation \nThis section expands the concept behind cooperative augmentation by explaining the \nthree areas of cooperative augmentation:  \n1. The Object Model representation of the smart object. \n2. The projector-camera system.  \n3. The cooperative augmentation process. \n \n3.1 Object Model \nThe Object Model is a description of the object and its capabilities, allowing the \nprojection system to dynamically configure its detection and projection services for \neach object at runtime. We assume the Object Model knowledge is embedded within \nthe object during manufacture. \n \nThe model consists of five components: \n1. Unique Object Identifier \n      5 \nThis allows an object to be uniquely identified on the network as a source \nand recipient of event messages and data streams. \n2. Appearance Knowledge \nThis knowledge describes the appearance of the smart object. The \ndescription is specific information extracted by computational methods \nfrom camera images of the object. For example, colour histograms, an \nimage of the object itself, or locations of features detected on the object. \n3. 3D Model \nA 3D model of the object is required in VRML representation to allow the \nprojector-camera system to compute the object\u2019s pose.  \n4. Sensor Knowledge \nThe sensor model is a description of the data delivered by the object\u2019s \nsensors. The data type is classified into three groups with regard to the \noriginating sensor: movement sensor data, light sensor data and others.  \nThe data is further classified into streaming or event-based, depending on \nthe way sensor data is output from the smart object.  The model contains \nassociated sensor resolutions, and sensor range information to allow the \nprojector-camera system to interpret sensor events. \n5. Location and Orientation of the Object \nWhen an object enters an environment, it does not know its location and \norientation. The projector-camera system provides this information on \ndetection of the object to complete the Object Model. \n3.2 Projector-Camera Systems \nA projector-camera system consists of a co-located projector and camera.  We \nassume they are mounted so the respective projection and viewing frustums overlap, \nallowing objects detected by the camera system to be projected on by the projector.  \nIn this work we use an intelligent steerable projector-camera system, composed of \na computer-controlled pan and tilt platform on which the projector and camera are \nmounted.  This platform is ceiling mounted for a greater view of the environment and \ncan rotate the projector-camera system hardware in two dimensions \u2013 horizontal (pan) \nand vertical (tilt) about the centre of projection. \nThe projector-camera system has six main capabilities: \n1. To provide a service allowing smart objects to register for detection and \nprojection. \n2. To search an environment for smart objects by automatically rotating the \npan and tilt platform. \n3. To detect smart objects in the camera images and calculate their location \nand orientation based on the knowledge and sensing embedded in the \nobject, as explained in section 3.3. \n6       \n4. To track detected objects by automatically rotating the pan and tilt \nplatform to centre the detected object. \n5. To project an image onto an object in an area specified by the smart \nobject, or choose the area most visible to the projector.  This image is \ngeometry corrected so that the image appears to be attached to the \nobject\u2019s surface and is undistorted. \n6. To further correct an image before projection for variations in an object\u2019s \nsurface colour and texture so that the image appears more visible. \n3.3 Cooperative Augmentation Process \nTo illustrate the cooperative augmentation process in action, we can imagine a \ngoods warehouse scenario, in which objects are stored for distribution. In this \nscenario the objects are augmented with computing, giving them knowledge of their \ncontents and sensors allowing them to monitor themselves and the local environment \nto ensure integrity and to maintain the authenticity of the goods [20]. Such sensing \nallows them to detect rough handling based on sensed movement and automatically \nreport their position and status wirelessly for goods tracking and inventory purposes. \nWe can decompose the cooperative augmentation of an object such as a chemical \ncontainer into five steps: \n1. Registration \nAs the container enters the warehouse it detects the presence of a location and \nprojection service through a service discovery mechanism. The object sends a \nmessage to the projector-camera system requesting registration for the projection \nservice to display messages. On receipt of the registration request, the projector-\ncamera system requests the Object Model from the smart object. \n \nProjector\nB) Camera captures image\nC) System detects location\n     of Smart Object\nA) Send Object Model & Sensor Data\nD) Update Location\nCamera\n \nFig. 2.  Detection Sequence Diagram \n2. Detection \nFollowing registration, the object begins streaming sensor data to the projector-\ncamera system, as shown in Figure 2 (A). This data is used in combination with the \n      7 \nObject Model to constrain the visual detection process and generate location and \norientation hypotheses (B and C). When an object is located with sufficient accuracy, \nthe 3D location and orientation hypothesis is returned to the smart object (D). This \nprocess is explained in more detail in section 4. \n3. Projection \nWhen an object has knowledge of its location and orientation it can request a \nprojection onto its surfaces. For example, if it detects it has been dropped, it can \nrequest a message is projected onto it requesting employees visually inspect it for \ndamage. This projection request message contains both the content to project and \nlocation description of where on the object to project the content, as shown in Figure \n3 (A).  The projector-camera system automatically corrects the projection of the \nmessage for the object\u2019s geometry based on the 3D model stored in the Object Model \nand the calculated object location and orientation, so that it appears undistorted (B).  \nThe projection is also corrected for the surface colour of the object to make it more \nvisible to the user [19]. The projector system starts displaying the corrected content \non the objects surfaces immediately on receipt of the request, if the object is in view \nand the projector system is idle (C).  \nCamera\nB) Calculate Projection with\n     with Geometry and Colour\n     Correction\nA) Request Projection\nC) Projection\n \nFig. 3. Projection Sequence Diagram \n4. Manipulation of Smart Object \nA requested projection is active as long as the object is detected, including during \nmovement or manipulation of the object. Consequently, smart objects can give direct \nfeedback to the user in response to the manipulation or movement of the object by \nchanging their projection. For example, as the projector sends location information to \nthe object, if an employee places an object in the wrong storage area of the warehouse \nit could request a warning message is projected until moved to the correct location. \n \n5. Update Appearance \nIf an object does not enter the environment with much appearance knowledge (see \nTable 1 in section 4), additional knowledge about the appearance of its surfaces is \nextracted once the object has been detected and its pose calculated. As part of the \n8       \ncooperative process this new knowledge can be re-embedded into the Object Model \nfor faster and more robust detection on next entry to an augmented environment.  \n4 Visual Object Detection \nThe projector-camera system dynamically configures its visual object detection \nprocessing based on the type of appearance knowledge in the Object Model, and the \nsensors the object possesses. \nObjects in the real world have appearances that vary widely, for example, in \ncolour, texture, shape and the features that appear on their surfaces. Their appearance \ncan also be easily changed by influences in the surrounding environment such as \nlighting conditions (including changes in intensity, colour and direction of lighting) or \nscene changes (such as partial occlusion by other objects or background changes). An \nobject\u2019s appearance also changes with the relative location and orientation of the \nobject to the viewer. \nTo cope with these changes we use four different detection algorithms: \ni.) Colour Histograms \nSwain and Ballard [27] first proposed the use of colour histograms to describe an \nobject by its approximate colour distribution. Objects can be detected by matching a \ncolour histogram from a camera image region to a histogram from a training sample \nof the object using histogram intersection and statistical divergence measurements \nsuch as chi-square (!2). Colour histograms offer a simple and fast object recognition \nmethod which has been shown to be robust to many transformations of an objects \nappearance, such as orientation, scale, partial occlusion and even shape. However, \ncolour histograms are sensitive to changes in light intensity and colour.  \n \nii.) Multidimensional Receptive Field Histograms \nAs many objects cannot be described by colour alone (for example, black objects), \nthe histogram approach has been generalised by Schiele and Crowley [28] to \nmultidimensional receptive field histograms. The histograms encode a statistical \nrepresentation of the appearance of objects based on vectors of joint statistics of local \nneighbourhood operators such as image intensity gaussian derivatives (Dx,Dy) or \ngradient magnitude and the local response of the laplacian operator (Mag-Lap). \nExperimental results show the histograms are robust to partial occlusion of the object \nand are able to recognise multiple objects in cluttered scenes in real-time using the \nprobabilistic local-appearance hashing approach proposed by Schiele and Crowley.  \n \niii.) Shape Context \nShape detection compares the silhouette contours of an object to a pre-computed \ndatabase of object appearances with the object in different poses. The database of \nobject appearances can be calculated directly from the 3D model of the object stored \nin the Object Model by rendering the model in different poses and extracting the \nsilhouette contour using the Canny edge detection algorithm. We use the Shape \n      9 \nContext descriptor described by Belongie et al. in [29] to enable scale and rotation \ninvariant matching of the contours. \n \niv.) Local Features \nLocal feature based detection algorithms aim to uniquely describe (and therefore \ndetect) an object using just a few key points. To extract features, training images of an \nobject are searched for a set of interest points (such as corners, blobs or lines) that can \nbe repeatably detected under transformations of an objects appearance.  The local \nimage area immediately surrounding these interest points can then be used to \ncalculate a feature vector which we assume serves to uniquely describe and identify \nthat point. The feature descriptor can be a simple colour histogram of the local area, \nor as complex as the gaussian derivative histogram based SIFT algorithm, described \nby Lowe in [22].  Object detection now becomes a problem of matching a feature set \nbetween the training image and camera images. A comparison of different feature \ndetection and descriptor algorithms can be found in [21]. \n \n The different detection methods are shown in Table 1, corresponding to different \naspects of an objects possible appearance.  \n \nAppearance \nKnowledge Detection Method \nDiscriminative \nPower \nCost in \nTime \nColour Colour histogram comparison Low Medium \nTexture  Multidimensional Receptive Field Histograms Medium Medium \nShape Contour detection and Shape Context Medium Medium \nLocal Features Interest point detection and feature descriptor comparison High High \nTable 1.  Appearance knowledge levels and detection methods with associated processing cost \nThese methods form a flexible layered detection process that allows an object to \nenter the environment with different levels of appearance knowledge.  As we descend \nthe table, the power of the detection methods to discriminate between objects with \nsimilar appearances increases, however, at the cost of increased processing time.  We \nconsider higher discriminative methods to hold more knowledge about the object.  \nWhere an object holds more than one piece of appearance knowledge, one of two \nstrategies can be followed. The first is using the most discriminative (least abstract) \ninformation to increase the probability of an accurate detection. The second strategy \nis to fuse the results of multiple detection methods to make the detection more robust. \nHowever, detection method selection is always a trade-off, as both the use of multiple \nmethods and the more discriminative individual methods (such as local features) \nshare the cost of increased processing requirements. \nOur cooperative augmentation method can also serendipitously use any movement \nsensors the object possesses to constrain the detection process. Common sensors that \ncan be used for movement detection on objects are accelerometers, ball-switches and \nforce sensors which detect pick-up and put-down events. If an object is moving, we \nuse visual differences generated between the camera image and a gaussian-mixture \n10       \nmodel of the background [14] to provide a basic figure-ground segmentation for the \ndetection algorithms, increasing the probability of correct detection. \nMaintaining a background model also allows us to take the object\u2019s context into \naccount when performing the method selection step, for example, we can compare the \nobject\u2019s colour histogram to the global environment colour histogram and if they are \ntoo similar we would not use the colour method as the probability of detection is low. \nThe detection method selection step forms part of the visual detection pipeline \nshown in Figure 4. Here, following each camera frame acquisition the method \nselection step is performed based directly on the appearance knowledge embedded in \nthe object.  If the object is successfully detected a 2D location result is generated.  This \ncan take the form of correspondences between extracted image features and features in \nthe Object Model, or a 2D image region in which the object has been detected, \n \nImage Acquisition\nLocal Features\nPose Computation\nSmart \nObject\nAppearance\nKnowledge\n3D Model\nCamera\nShape Colour Movement\n2D Location\n3D Location and Orientation Hypothesis\nMethod Selection\nSensing\nTexture\n \nFig. 4. Detection method selection based on smart object knowledge \n \nFollowing 2D location of the object in the camera image, a pose computation step \nis performed. The object pose is calculated either directly from matched local feature \ncorrespondences or by fitting the 3D model to edges detected in the 2D image region \nfrom the detection step. RANSAC is used for robust model parameter estimation [26] \nand eliminates incorrectly matched correspondences. Typically the pose computation \nstep achieves a mean location error under 5mm in the X and Y axes, 2cm in distance \nto object and mean orientation error under 1 degree with an object at 3m distance. \nSensing can also be used in the pose computation step if a smart object contains 3D \naccelerometer sensors. Here the sensed gravity vector can be directly used to \nconstrain the number of 3D model poses that must be tested to match the edges \ndetected in the 2D image region from the detection step. \n5 Object Projection Processing \nWhen the smart object requests a projection its message includes both the content \nto project (which can be images, text or video or a URL where content can be found) \nand the location to project it. We can project onto any object surface visible to the \n      11 \nprojector.  The location description refers to the projection location abstractly or \nspecifically. Abstract locations refer to faces of the object\u2019s 3D model. For example, a \nprojection can be requested on the top or front face.  A more specific location can also \nbe specified as coordinates in the 3D model coordinate system, allowing exact \nplacement and sizing of the projection on an object. \nThere are cases where projection cannot begin immediately, such as where the \nsystem is busy, the object is occluded or the object is out of the field of view of the \nprojector.  Here the display requests are cached at the projector-camera system and \nthe projection commences when the object is in view and the projector is available. \nProjection requests are displayed sequentially and can be ended by the object \nrequesting a null content projection. Simultaneous projection onto multiple objects \ncan be accomplished if all are detected within the field of view of the projector-\ncamera system. \nA rectangular image projected on to a non-perpendicular or non-planar surface \nexhibits geometric distortion. We compensate for this distortion by warping our \nprojected image if we know both the surface geometry of the object and the \norientation angle of the surface with respect to the projector.  We obtain the \norientation of the object from the object detection step, and the surface shape from the \ngeometric 3D model contained within the Object Model.  The surface shape directly \nconfigures the projection geometric correction method [18], as shown in Table 2. \n \nObject Geometries Correction Method \nPlanar \nRectilinear Planar Homography \nCylindrical \nSpherical Quadric Image Transfer \nIrregular Discretised Warping \nTable 2.  Projection geometric correction methods based on object geometries [18]. \nThe projector-camera system uses a real-time colour correction algorithm \ndeveloped by Fujii et al. [19] to correct for the colour of the object\u2019s surface and make \nthe projection more visible. This entails an initial one-time projection of four colour \ncalibration image frames (red, green, blue and grey) to recover the reflectivity \nresponse of the surface followed by calculation of the adaptation algorithm for each \nframe to be projected. \n6 Concept Validation \nThis section uses the scenario outlined in section 3 to present a concrete detection and \nprojection process for two smart chemical containers in a warehouse.  \n \n6.1 Registration \nObjects enter proximity of the projector-camera system; detect the presence of a \nprojection service and register.  This process transfers Object Model knowledge from \n12       \nthe smart object to the projector-camera system. Here, an employee enters the \nenvironment with two smart chemical containers, as seen in Figure 5. \nThe projector-camera system registers the objects, and returns a confirmation \nmessage to the containers. On receipt of this message the containers begin sending \nsensor events to the projector-camera system. In this case, they are being carried by \nthe employee so embedded accelerometer sensors generate movement events. \n \n6.2 Detection  \nThe registering objects trigger the detection process in the projector-camera \nsystem. Here the challenge is to simultaneously detect mobile or static objects and \ndistinguish between objects with similar appearances. \n \n           \nFig. 5. Left: New objects arrives in environment, Centre: An employee walks with containers, \nRight: The employee places one object on the floor \nThe steerable projector now rotates from its current position to search the \nenvironment. As the objects have just entered, the system does not know their \nlocation. Consequently, the projector system uses a creeping line search pattern with a \nhorizontal major axis to thoroughly search the whole environment. \nThe projector uses the appearance knowledge embedded in the Object Model and \nthe sensor events to configure its detection process. In this case the containers store \nknowledge of a colour histogram, and sense they are moving. This knowledge triggers \nthe method selection step to choose colour and movement detection processes. The \nmovement process generates a motion mask which is used by the colour detection \nprocess to constrain its search for the object by masking the back-projection result of \nthe object\u2019s colour histogram. \nAs the two chemical containers look identical, two possible objects are identified \nin the image.  It is not currently possible for the camera to distinguish between the \nobjects. Consequently the steerable projector tracks the moving areas in the camera \nimage by centring their centre of gravity. \nBoth objects generate movement event messages while they are being carried by \nthe employee. However, when an employee places one of the containers on the floor \n(see Figure 5) the container\u2018s movement sensors stop sending movement events. The \n      13 \nprojector-camera system now only detects one moving area and the system can \ndifferentiate between the objects directly based on sensing. A 3D location and pose is \nnow calculated and sent wirelessly to the containers, completing the Object Model. \n \n6.3 Projection \nOnce an object\u2019s 3D location and orientation is calculated by the projector-camera \nsystem, objects can request projection of content on their surfaces. Here the challenge \nis to correct the projection for the orientation of the object, and variations in its \nsurface colour to ensure the most undistorted and visible projection. \n \n       \nFig. 6. Left: Warning message projection on two chemical containers,  \nRight: Scale and rotation invariant local features detected on chemical containers \nThe container detects it was put down in the wrong storage area based on the \nlocation it was sent and requests a warning message is projected (see figure 6). The \nprojector-camera system projects the warning message on the front surface of the \ncontainer objects so as to appear undistorted by drawing the text and images with the \ncalculated transformations applied.  \n \n6.4 Manipulating the Object \nWhen projecting onto objects, the object can respond to sensed manipulation or \nnetwork events by dynamically modifying the projected content. The challenge here \nis to keep the projection aligned with the object as it is manipulated or moved. \nThe employee sees the projected message and picks up the object. The detection \nprocess continues to track it and generate 3D location and pose information. \nConsequently, the message appears to remain fixed to its surface as long as the \nsurface is visible to the projector system. When the object is in the correct area it \nrequests the projection stops. The employee puts down the container when they see \nthe message disappear. The projector-camera system keeps tracking the objects. \n \n6.5 Knowledge Updating \nIf objects enter the environment with only partial knowledge of their appearance, \ntheir knowledge can be increased over time by performing extra detection processes \nand re-embedding the result into the Object Model.  The challenge is how to make the \nknowledge extraction accurate, given that the initial knowledge was incomplete. \nThe two containers entered the environment only with knowledge of their colour, \nso the projector-camera system extracts more appearance knowledge over time.  In \nthis case, the SIFT algorithm [22] is used to detect scale and rotation-invariant \n14       \nfeatures on the object just put down, as shown in Figure 8. The SIFT descriptors are \ncalculated on small image patches around the detected interest points. The resulting \n128 value feature vectors are mapped to locations on the object\u2019s 3D model using the \nknown 3D location and orientation of the container.  \nIf the object is manipulated so it is rotated from its original pose new features will \nbe detected as they come into view. The projector-camera system manages the Object \nModel local feature database to merge new features or update the database if the \nobject appearance is changed. The new local feature appearance knowledge is sent to \nthe smart containers to be embedded in the Object Model and used for faster, more \naccurate detection in future. \n \n6.6 Objects Departing the Environment \nWhen objects depart the proximity of the projector-camera system, their virtual \nobject representation is removed by the projector system and the projector is free to \ntrack other objects. Here, the employee moves to the exit with the container that was \nnever put down. This container continues to generate motion events. As there are no \nother moving objects or projections active, the projector system tracks the carried \nobject, as shown in Figure 9.  \n \n \nFig. 9. A container leaves the environment with the employee \nAs the employee exits through the door with the object, the system looses sight of \nthe object and it no longer responds to messages from the projector-camera system. \nThe system assumes it has departed the environment after a short time-out. \nThe projector-camera system then returns to the last-known position of the other \ncontainer objects. If no objects can be detected the projector system begins an \nexpanding square search pattern centred on their former locations. \n7 Discussion \nThis section discusses issues arising from the concept validation in terms of the \nfive cooperative augmentation steps presented in section 6. \n \n      15 \n7.1 Registration \nCurrently, smart object registration and communication is performed over a \nwireless network, implemented using Smart-Its sensor nodes [23]. The wireless \nnetwork bandwidth requirements for smart objects depend on where the sensor data is \nabstracted to events.  If a sensor node is not powerful enough to perform this \nprocessing then raw sensor data must be streamed to another device on the network. \nDue to the 13ms timeslots used for each node with the Smart-Its AwareCon protocol \n[23], only a maximum of 2 smart devices can stream sensor data simultaneously and \nremain synchronised with a 30Hz (33.3ms) camera refresh rate. \nThe use of active smart objects with sensing provides three benefits over passive \ntechnologies such as RFID: \n1. Active sensing (such as movement or light sensing) can constrain the detection \nprocess to make it more robust and differentiate between objects. \n2. Objects whose appearance or geometry changes can update the projector-camera \nsystem dynamically with new appearance knowledge. (For example, if a user \nopens a smart book the appearance is updated and tracking is un-interrupted). \n3. The object itself can be modelled as a state machine which requests projections \nbased on sensed changes in its environment, location or direct interaction with \nthe projection. (For example, a message about how to assemble two smart objects \ncan be projected only when they are moved together into the same location). \n \n7.2 Detection \nIt has been reported by Brooks in [24] that users of projector based interactive \nsystems routinely accept total system latencies of 150ms. There are three main \nsources of latency in the detection and projection framework \u2013 camera frame \nacquisition, image processing for object detection and projection. For a camera \nrunning at 30Hz the frame acquisition takes up to 33.3ms, while for a 60Hz projector \na frame is projected every 16.7ms.  Maximum latency before image processing is \n50ms; consequently, the object detection step should be performed below 100ms. \nThe use of complex or multiple computer vision methods in the object detection \nstep is CPU intensive. For example, a CPU optimised version of the SIFT local \nfeature algorithm takes approximately 333ms to detect a single object in a 640x480 \npixel image [22]. Our approach is to make use of the ability of the Graphics \nProcessing Unit (GPU) on the graphics card to process pixels in parallel, allowing our \nsystem to achieve detection and augmentation of objects in near real-time. \n \n7.3 Projection \nAs we do not change the appearance of smart objects, their surfaces can present a \nchallenge to projection. Generally, a smooth, diffuse, light coloured object is ideal for \nprojection; however, few objects exhibit these characteristics. Certain combinations \nof projected content and object surface colour can make the projection almost \ninvisible to the human eye. For example, when projecting a yellow font on a deep red \nbackground. Conversely, with a smooth, diffuse, light coloured object, projection \nillumination on the object can significantly alter its appearance, causing the object \ndetection step to fail. \n16       \nConsequently, the use of colour correction techniques in the projection step was \nchosen, as it goes part way to solving these competing problems.  Colour correction \nalgorithms can change the projected image to correct for non-uniform and non-white \nsurface colours. An image of the object without projection can also be calculated as \npart of this process and used for object detection.  \nDespite the large body of work on photometric correction, the algorithm by Fujii et \nal. [19] was chosen for this step as it is the only algorithm demonstrated to perform in \nreal-time. However, this correction does have the cost of a one camera frame delay to \nallow the camera image to be used in the algorithm. The algorithm also cannot \ncompletely correct very saturated surfaces, as the dynamic range of typical projectors \nis not sufficient to invert the natural surface colour. \n \n7.4 Manipulation of Objects \nThe maximum speed a smart object can move is limited by the camera frame rate \nand object detection step processing time. For an average camera acquisition and \nprocessing step of 133.3ms and a typical human walking speed of 5kph a handheld \nobject could move 18cm. As the lack of projection would be very obvious to a user \nduring a move of this distance, we can de-couple the projection from the detection \nstep. By using a Condensation algorithm particle filter [25] to predict the 3D location \nand orientation of the smart objects between detections we can exploit the faster \nframe rate of the projector.  The benefit of using a particle filter over a Kalman filter \nis that it allows us to model multiple alternative hypotheses; it can integrate detection \nresults from multiple distributed cameras and better suits the non-linear movement \ntypically seen in handheld objects.   \n \n7.5 Knowledge Updating \nWhen the projector-camera system updates or merges new knowledge about an \nobject, constraints on smart object sensor node memory limit the amount of \nknowledge that can be stored in a smart object. For example, the particle Smart-Its \nsensor node [23] currently only has 512KB of flash memory which can be used for \nObject Model storage. Our solution for larger models is to only store a URL link to \nthe actual Object Model in the smart object (which assumes a network connection). \n8 Conclusion \nIn this paper we have presented the concept of cooperative augmentation and \nvalidated our approach with an implementation using a warehouse scenario. We \ndiscussed issues arising from the implementation and the lessons learned. \nOur contribution is a new approach to augmenting smart objects with a display \ncapability without changing their natural appearance, by using projector-camera \nsystems. Our approach can locate and track mobile objects in the environment, align \nthe projection with the object\u2019s surfaces and correct for surface colour so the display \nappears undistorted and visible to a user. \n      17 \nThe main challenges in our approach are real-time visual detection of smart \nobjects, keeping the projection synchronised when the object is moved or manipulated \nand correcting the projection for non-ideal surface colours and textures.  \nMore research is required in how different levels of knowledge change the \ndetection performance, what impact sensing has on the robustness of detection and \nwhich computer vision algorithms are best suited to detecting the objects.  Open \nquestions remain in the area concerning location of projections on an object. \nSpecifically, how can we determine the best strategy to ensure the most visible, \nreadable and useable projection location on an object\u2019s surfaces for the user? Also, if \nan object is in view of multiple distributed projector-camera systems, what is the best \nstrategy to decide which system should project onto each object surface? \nAcknowledgements \nThis research is supported by the EPSRC, the Ministry of Economic Affairs of the \nNetherlands through the BSIK project Smart Surroundings under contract no. 03060 \nand by Lancaster University through the e-Campus grant. \n9 References \n1. Dynamic Shader Lamps: Painting on Movable Objects, D. Bandyopadhyay, R. Raskar, H. \nFuchs, In Proc. IEEE and ACM Int. Symposium on Augmented Reality, New York, 2001. \n2. Projected Augmentation \u2013 Augmented Reality using Rotatable Video Projectors, J. Ehnes, K. \nHirota, M. Hirose, Third IEEE and ACM International Symposium on Mixed and \nAugmented Reality (ISMAR'04), September-October, 2004 Arlington, VA, USA. \n3. Marker Tracking and HMD Calibration for a video-based Augmented Reality Conferencing \nSystem, Kato, H., Billinghurst, M. (1999), In Proceedings of the 2nd International \nWorkshop on Augmented Reality (IWAR 99). October 1999, San Francisco, USA. \n4. Projecting Rectified Images In an Augmented Environment, S. Borkowski, O. Riff, J. \nCrowley, IEEE International Workshop on Projector-Camera Systems (PROCAMS-2003), \nNice, France, October 12, 2003. \n5. Moveable Interactive Projected Displays Using Projector Based Tracking, J. C. Lee, S. E. \nHudson, J. W. Summet, P. H. Dietz, Proceedings of the ACM Symposium on User Interface \nSoftware and Technology (UIST), pages 63-72, Seattle, WA. October 23-26, 2005. \n6. Tracking Locations of Moving Hand-held Displays Using Projected Light, J. Summet and R. \nSukthankar, In Proceedings of Pervasive 2005, Munich, Germany. \n7. RFIG Lamps: Interacting with a Self-Describing World via Photosensing Wireless Tags and \nProjectors, R. Raskar, P. Beardsley, J. van Baar, Y. Wang, P.Dietz, J. Lee, D. Leigh, T. \nWillwatcher, In Proceedings of SIGGRAPH 2004, Los Angeles, USA. \n8. An Architecture and Framework for Steerable Interface Systems, Levas, A., Pinhanez, C., \nPingali, G., Kjeldsen, R., Podlaseck, M., Sukaviriya, N., in Proceedings of UbiComp 2003. \n9. The MediaCup: Awareness Technology embedded in an Everyday Object, H. Gellersen, M. \nBeigl, H. Krull, 1st International Symposium on Handheld and Ubiquitous Computing \n(HUC99), Karlsruhe, Germany, 1999. Lecture notes in computer science; Vol 1707,  H-W \nGellersen ed, ISBN 3-540-66550-1; Springer, 1999, pp 308-310. \n10. Cooperative Artefacts: Assessing Real World Situations with Embedded Technology. M. \nStrohbach, H.-W. Gellersen, G. Kortuem and Christian Kray, In Proceedings of Ubicomp \n2004, Nottingham, UK. \n11. Context Acquisition based on Load Sensing, A. Schmidt, M. Strohbach, K. Van Laerhoven, \nA. Friday and H.-W. Gellersen, In Proceedings of Ubicomp 2002, G. Boriello and L.E. \n18       \nHolmquist (Eds). Lecture Notes in Computer Science, Vol 2498, Springer Verlag, \nGothenburg, Sweden, September 2002, pp. 333 \u2013 351. \n12. Ubiquitous Interaction - Using Surfaces in Everyday Environments as Pointing Devices, A. \nSchmidt, M. Strohbach, K. Van Laerhoven, and H.W. Gellersen, 7th ERCIM Workshop \n\"User Interfaces For All\", 23 - 25 October, 2002. \n13. Cooperative Artefacts: Assessing Real World Situations with Embedded Technology, M. \nStrohbach, H.-W. Gellersen, G. Kortuem and C. Kray, In Proceedings of: Ubicomp 2004. \n14. Adaptive background mixture models for real-time tracking, C. Stauffer and W. E. L. \nGrimson, In Computer Vision Pattern Recognition, pages 246--252, Ft. Collins, CO, 1999. \n15. A Wearable Mixed Reality with On-board Projector, T. Karitsuka, K. Sato, In Second IEEE \nand ACM International Symposium on Mixed and Augmented Reality (ISMAR2003), 7-10 \nOctober 2003, Tokyo, Japan. \n16. The Everywhere Displays Projector: A Device to Create Ubiquitous Graphical Interfaces, \nC. Pinhanez, Proceedings of Ubiquitous Computing 2001 (Ubicomp'01), September 2001. \n17. Searchlight: A Lightweight Search Function for Pervasive Environments, Andreas Butz, \nMichael Schneider, and Mira Spassova, Pervasive Computing, Second International \nConference, PERVASIVE 2004, Vienna, Austria, April 21-23, 2004. \n18. Spatial Augmented Reality Merging Real and Virtual Worlds, O. Bimber and R. Raskar, A \nK Peters LTD (publisher), ISBN: 1-56881-230-2. \n19. A Projector-Camera System with Real-Time Photometric Adaptation for Dynamic \nEnvironments, K. Fujii, M.D. Grossberg and S.K. Nayar, IEEE Conference on Computer \nVision and Pattern Recognition (CVPR), Vol.1, pp.814-821, Jun, 2005. \n20.eSeal \u2013 a system for enhanced electronic assertion of authenticity and integrity. C. Decker, \nM. Beigle, A. Krohn, P. Robinson, U. Kubach, In Pervasive 2004 (2004) Vienna, Austria. \n21. Scale and Affine invariant interest point detectors, K. Mikolajczyk and C. Schmid, In \nProceedings of IJCV 60(1):63-86, 2004. \n22. SIFT: Distinctive image features from scale invariant keypoints, D. Lowe, In Proceedings \nof IJCV 60(2):91-110, 2004 \n23. The Particle Computer System, IPSN Track on Sensor Platform, Tools and Design Methods \nfor Networked Embedded Systems (SPOTS), Christian Decker, Albert Krohn, Michael \nBeigl, Tobias Zimmer, In Proceedings of the ACM\/IEEE 4th International Conference on \nInformation Processing in Sensor Networks (IPSN05), pp443-448, Los Angeles, April 2005. \n24. What\u2019s Real About Virtual Reality?, F.P. Brooks, Jr., IEEE Computer Graphics and \nApplications, 19, 6: 16-27, 1999. \n25. CONDENSATION -- conditional density propagation for visual tracking, Michael Isard \nand Andrew Blake, In International Journal of Computer Vision, 29, 1, 5--28, 1998. \n26. Random sample consensus: a paradigm for model fittingwith applications to image analysis \nand automated cartography, M.A. Fischler, and R.C. Bolles, Communications of the ACM \n24, 6 (Jun. 1981), 381-395. \n27. Color indexing. M. J. Swain, and D. H. Ballard, International Journal of Computer Vision, \n7(1). 1991. \n28.  Recognition without Correspondence using Multidimensional Receptive Field Histograms, \nInternational Journal of Computer Vision, 36(1), pp.31-50, 2000. \n29. Matching Shapes, S. Belongie, J. Malik, J. Puzicha, In International Conference on \nComputer Vision (ICCV'01), 2001. \n30. Interacting with Steerable Projected Displays, R. Kjeldsen, C. Pinhanez, G. Pingali, J. \nHartman, T. Levas, M. Podlaseck, Proc. of the 5th International Conference on Automatic \nFace and Gesture Recognition (FG'02), Washington (DC), May 20-21 2002 \n"}