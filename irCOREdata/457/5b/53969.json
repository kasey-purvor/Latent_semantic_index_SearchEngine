{"doi":"10.1109\/CVPRW.2010.5543760","coreId":"53969","oai":"oai:eprints.lincoln.ac.uk:2797","identifiers":["oai:eprints.lincoln.ac.uk:2797","10.1109\/CVPRW.2010.5543760"],"title":"Binary histogram based split\/merge object detection using FPGAs","authors":["Appiah, Kofi","Meng, Hongying","Hunter, Andrew","Dickinson, Patrick"],"enrichments":{"references":[{"id":869068,"title":"A fast model-free morphology-based object tracking algorithm.","authors":[],"date":"2002","doi":"10.5244\/c.16.75","raw":"J. Owens, A. Hunter, and E. Fletcher. A fast model-free morphology-based object tracking algorithm. In Proceedings of the British Machine Vision Conference. British Machine Vision Association, 2002.","cites":null},{"id":866812,"title":"Adaptive bayesian recognition in tracking rigid objects.","authors":[],"date":"2000","doi":"10.1109\/cvpr.2000.854942","raw":"Y. Boykov and D. P. Huttenlocher. Adaptive bayesian recognition in tracking rigid objects. Computer Vision and Pattern Recognition, IEEE Computer Society Conference on, 2:2697, 2000.","cites":null},{"id":867853,"title":"Adaptive tracking of nonrigid objects based on color histograms and automatic parameter selection.","authors":[],"date":"2005","doi":"10.1109\/acvmot.2005.19","raw":"A. Jacquot, P. Sturm, and O. Ruch. Adaptive tracking of nonrigid objects based on color histograms and automatic parameter selection. In Proceedings of the IEEE Workshop on Motion and Video Computing, pages 103\u2013109. IEEE Computer Society, 2005.","cites":null},{"id":868579,"title":"An fpga implementation of hierarchical motion estimation for embedded object tracking.","authors":[],"date":"2006","doi":"10.1109\/isspit.2006.270805","raw":"M. McErlean. An fpga implementation of hierarchical motion estimation for embedded object tracking. International Symposium on Signal Processing and Information Technology, 0:242\u2013247, 2006.","cites":null},{"id":866599,"title":"Autonomous real-time surveillance system with distributed ip cameras.","authors":[],"date":"2009","doi":"10.1109\/icdsc.2009.5289387","raw":null,"cites":null},{"id":18448482,"title":"Autonomous real-time surveillance system with distributed ipcameras. InThirdACM\/IEEEInternationalConferenceon Distributed Smart Cameras.","authors":[],"date":"2009","doi":"10.1109\/icdsc.2009.5289387","raw":"K. Appiah, A. Hunter, J. Owens, P. Aiken, and K. Lewis. Autonomous real-time surveillance system with distributed ipcameras. InThirdACM\/IEEEInternationalConferenceon Distributed Smart Cameras. IEEE Computer Society, 2009.","cites":null},{"id":868360,"title":"Colour-based object tracking in surveillance application.","authors":[],"date":"2009","doi":null,"raw":"T. S. Ling, L. K. Meng, L. M. Kuan, Z. Kadim, and A. A. B. Al-Deen. Colour-based object tracking in surveillance application. Proceedings of the International MultiConference of Engineers and Computer Scientists 2009 Vol I, 1, 2009.","cites":null},{"id":868804,"title":"Design and implementation of an object tracker on a recon\ufb01gurable system on chip.","authors":[],"date":"2006","doi":"10.1109\/rsp.2006.13","raw":"F. Muhlbauer and C. Bobda. Design and implementation of an object tracker on a recon\ufb01gurable system on chip. IEEE International Workshop on Rapid System Prototyping, 2006.","cites":null},{"id":867049,"title":"Estimation of object motion parameters from noisy images.","authors":[],"date":"1986","doi":"10.1109\/tpami.1986.4767755","raw":"T. J. Broida and R. Chellappa. Estimation of object motion parameters from noisy images. IEEE Trans. Pattern Anal. Mach. Intell., 8(1):90\u201399, 1986.","cites":null},{"id":870240,"title":"Fpga accelerating algorithms of active shape model in people tracking applications.","authors":[],"date":"2007","doi":"10.1109\/dsd.2007.4341504","raw":"J. Xu, Y. Dou, J. Li, X. Zhou, and Q. Dou. Fpga accelerating algorithms of active shape model in people tracking applications. Digital Systems Design, Euromicro Symposium on, 0:432\u2013435, 2007.","cites":null},{"id":867523,"title":"Fpga-based real-time visual tracking system using adaptive color histograms.","authors":[],"date":"2007","doi":"10.1109\/robio.2007.4522155","raw":"J. U. Cho, S. H. Jin, X. D. Pham, D. Kim, and J. W. Jeon. Fpga-based real-time visual tracking system using adaptive color histograms. Proc. of the IEEE International Conference on Robotics and Biomimetics, pages 172\u2013177, 2007.","cites":null},{"id":869827,"title":"Likelihood-based object detection and object tracking using color histograms and em.","authors":[],"date":"2002","doi":"10.1109\/icip.2002.1038092","raw":"P. Withagen and F. G. K. Schutte. Likelihood-based object detection and object tracking using color histograms and em. Proceedings of International Conference on Image Processing., 1:589\u2013592, 2002.","cites":null},{"id":867610,"title":"Mean-shift blob tracking through scale space.","authors":[],"date":"2003","doi":"10.1109\/cvpr.2003.1211475","raw":"R. T. Collins. Mean-shift blob tracking through scale space. Computer Vision and Pattern Recognition, IEEE Computer Society Conference on, 2:234, 2003.","cites":null},{"id":866760,"title":"Multi-class object tracking algorithm that handles fragmentation and grouping.","authors":[],"date":"2007","doi":"10.1109\/cvpr.2007.383175","raw":"B. Bose, X. Wang, and E. Grimson. Multi-class object tracking algorithm that handles fragmentation and grouping. Computer Vision and Pattern Recognition, IEEE Computer Society Conference on, 0:1\u20138, 2007.","cites":null},{"id":869343,"title":"Multi-object tracking using color, texture and motion.","authors":[],"date":"2007","doi":"10.1109\/cvpr.2007.383506","raw":"V. Takala and M. Pietikainen. Multi-object tracking using color, texture and motion. IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2007.","cites":null},{"id":870490,"title":"Object tracking: A survey.","authors":[],"date":"2006","doi":"10.1145\/1177352.1177355","raw":"A. Yilmaz, O. Javed, and M. Shah. Object tracking: A survey. ACM Computing Surveys, 38(4):13, 2006.","cites":null},{"id":870032,"title":"P\ufb01nder: Real-time tracking of the human body.","authors":[],"date":"1997","doi":"10.1109\/34.598236","raw":"C. R. Wren, A. Azarbayejani, T. Darrell, and A. P. Pentland. P\ufb01nder: Real-time tracking of the human body. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(7):780\u2013785, 1997.","cites":null},{"id":868123,"title":"Real-time object tracking and classi\ufb01cation using a static camera.","authors":[],"date":null,"doi":null,"raw":"S. Johnsen and A. Tews. Real-time object tracking and classi\ufb01cation using a static camera. Proceedings of IEEE International Conference on Robotics and Automation, workshop on People Detection and Tracking.","cites":null},{"id":867273,"title":"Self Organizing Feature Map for Color Quantization on FPGA.","authors":[],"date":"2006","doi":"10.1007\/0-387-28487-7_8","raw":"C. Chang, M. Shibu, and R. Xiao. Self Organizing Feature Map for Color Quantization on FPGA. Springer, 2006.","cites":null},{"id":869570,"title":"Study on a real-time image object tracking system.","authors":[],"date":"2008","doi":"10.1109\/ISCSCT.2008.193","raw":"Q. Wang and Z. Gao. Study on a real-time image object tracking system. In Proceedings of the 2008 International Symposium on Computer Science and Computational Technology, pages 788\u2013791. IEEE Computer Society, 2008.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-06-13","abstract":"Tracking of objects using colour histograms has proven successful in various visual surveillance systems. Such systems rely heavily on similarity matrices to compare the appearance of targets in successive frames. The computational cost of the similarity matrix is increased if proximate objects merge into a single object or a single object fragments into two or more parts. This paper presents a method of reducing this computational cost with the use of a reconfigurable computing architecture. Colour histogram data of moving targets are used to generate binary signatures for the detection of merged or fragmented objects. The main contribution in this paper is how binary histogram data is generated and used to detect split\/merge object with the use of logical operations native to the hardware architecture used for its implementation. The results show a 10 fold improvement in processing speed over the microprocessor based implementation, and that it is also capable of detecting split\/merge objects efficiently","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/53969.pdf","fullTextIdentifier":"http:\/\/eprints.lincoln.ac.uk\/2797\/1\/ECVW_11.pdf","pdfHashValue":"4ce147da0e265eb2c7bf532fedbe476c71058dfb","publisher":"IEEE Computer Society","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lincoln.ac.uk:2797<\/identifier><datestamp>\n      2013-07-08T16:24:03Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47373430<\/setSpec><setSpec>\n      74797065733D636F6E666572656E63655F6974656D<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lincoln.ac.uk\/2797\/<\/dc:relation><dc:title>\n        Binary histogram based split\/merge object detection using FPGAs<\/dc:title><dc:creator>\n        Appiah, Kofi<\/dc:creator><dc:creator>\n        Meng, Hongying<\/dc:creator><dc:creator>\n        Hunter, Andrew<\/dc:creator><dc:creator>\n        Dickinson, Patrick<\/dc:creator><dc:subject>\n        G740 Computer Vision<\/dc:subject><dc:description>\n        Tracking of objects using colour histograms has proven successful in various visual surveillance systems. Such systems rely heavily on similarity matrices to compare the appearance of targets in successive frames. The computational cost of the similarity matrix is increased if proximate objects merge into a single object or a single object fragments into two or more parts. This paper presents a method of reducing this computational cost with the use of a reconfigurable computing architecture. Colour histogram data of moving targets are used to generate binary signatures for the detection of merged or fragmented objects. The main contribution in this paper is how binary histogram data is generated and used to detect split\/merge object with the use of logical operations native to the hardware architecture used for its implementation. The results show a 10 fold improvement in processing speed over the microprocessor based implementation, and that it is also capable of detecting split\/merge objects efficiently.<\/dc:description><dc:publisher>\n        IEEE Computer Society<\/dc:publisher><dc:date>\n        2010-06-13<\/dc:date><dc:type>\n        Conference or Workshop contribution<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lincoln.ac.uk\/2797\/1\/ECVW_11.pdf<\/dc:identifier><dc:identifier>\n          Appiah, Kofi and Meng, Hongying and Hunter, Andrew and Dickinson, Patrick  (2010) Binary histogram based split\/merge object detection using FPGAs.  In: 6th Workshop on Embedded Computer Vision, 13 - 18 June 2010, San Francisco, CA. USA.  <\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/CVPRW.2010.5543760<\/dc:relation><dc:relation>\n        10.1109\/CVPRW.2010.5543760<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lincoln.ac.uk\/2797\/","http:\/\/dx.doi.org\/10.1109\/CVPRW.2010.5543760","10.1109\/CVPRW.2010.5543760"],"year":2010,"topics":["G740 Computer Vision"],"subject":["Conference or Workshop contribution","PeerReviewed"],"fullText":"Binary Histogram based Split\/Merge Object Detection using FPGAs\nKofi Appiah, Hongying Meng, Andrew Hunter, Patrick Dickinson\nLincoln School of Computer Science, University of Lincoln\nBrayford Campus, Lincoln\n{kappiah, hmeng, ahunter, pdickinson}@lincoln.ac.uk\nAbstract\nTracking of objects using colour histograms has proven\nsuccessful in various visual surveillance systems. Such sys-\ntems rely heavily on similarity matrices to compare the ap-\npearance of targets in successive frames. The computa-\ntional cost of the similarity matrix is increased if proximate\nobjects merge into a single object or a single object frag-\nments into two or more parts. This paper presents a method\nof reducing this computational cost with the use of a recon-\nfigurable computing architecture. Colour histogram data of\nmoving targets are used to generate binary signatures for\nthe detection of merged or fragmented objects. The main\ncontribution in this paper is how binary histogram data is\ngenerated and used to detect split\/merge object with the use\nof logical operations native to the hardware architecture\nused for its implementation. The results show a 10 fold\nimprovement in processing speed over the microprocessor\nbased implementation, and that it is also capable of detect-\ning split\/merge objects efficiently.\n1. Introduction\nVisual surveillance systems usually have the ability to\ndetect moving objects, track them in the field of view and\nperform high level behavioural analysis [1]. When the cam-\nera is stationary, a natural approach to detect moving objects\nis to use background differencing, in which a reference im-\nage is maintained and constantly updated to reflect changes\nin the scene. Images acquired from the camera are com-\npared with the reference image to extract any dissimilarity,\nwhich is further processed into objects.\nMoving objects detected are then tracked from frame to\nframe by establishing correspondence between persistent\nblobs in the scene. Tracking is a very important intermedi-\nate processing step in visual surveillance, yet it is suscepti-\nble to segmentation errors. Most of the problems arise when\ntwo moving objects gets close to each other, when an object\nbecome partially occluded, when there is a new object in the\nscene and when an existing object leaves the scene. Yilmaz\net al. [20] categorises tracking algorithms into three major\ngroups: point-based [4], kernel-based and silhouette-based\n[8]. Wren et al. [18] presents a silhouette-based approach,\nwhich uses object colour and location statistics to generate\nthe trajectory of an object in every frame. Boykov et al. [3]\npresents a point-based approach using a Kalman filter. The\nsystem tracks rigid objects based on an adaptive recogni-\ntion technique that incorporates dependencies between ob-\nject features. In [7], Collins presents a kernel-based track-\ning system using the mean-shift algorithm. The problem as-\nsociated with the choice of correct scale for tracking a blob\nusing the mean-shift algorithm has also been addressed us-\ning difference of Gaussian.\nAn ideal visual tracking system should be temporally\nand spatially complete [2]. Thus, there should be no sin-\ngle frame in which an object in the field of view would not\nbe accounted for \u2013 temporal completeness; and each object\nin the field of view should correspond to a single connected\nsilhouette (blob) in the image space that completely covers\nonly the object\u2019s pixels \u2013 spatial completeness. In an en-\nvironment with a variable number of objects from different\nclasses, it is not always possible to achieve both temporal\nand spatial completeness during tracking.\nOwens et al. [14] presents a model-free tracking al-\ngorithm which is capable of handling object grouping and\nfragmentations. The system uses area and histogram of the\nobject pixels to match objects to silhouette from frame to\nframe. A cost minimization function is employed to merge\nfragmented silhouettes. Similarly, a silhouette regression\nline histogram is used to partition merged silhouettes. The\npaper reported 84.9% accuracy in tracking multiple objects\nover a period of three days.\nThe main challenge with object tracking is also ad-\ndressed by Bose et al. [2] using an inference graph. Using\nthe inference graph and a generic object model, tracked ob-\nject blobs are labelled as either whole object, fragments of\nobjects, or groups of interacting objects. The model uses the\naverage difference in speed between any pair of elementary\ntargets. The algorithm takes approximately 40s in tracking\nmultiple objects recorded over a period of 30s.\n978-1-4244-7028-0\/10\/$26.00 \u00a92010 IEEE\nTakala et al. [15] presents a colour, texture and motion\nbased tracking systems capable of running in real-time. The\nsystem used RGB colour histogram and correlogram to de-\nscribe the object\u2019s colour properties, and local binary pat-\nterns for texture and geometric location to handle merging\nand splitting of objects. The system has reported under per-\nformance when object are emerging in the camera scene. It\nis capable of tracking multiple objects in diverse conditions\nwhile achieving speeds of 10 \u2212 15 frames per second on a\n3GHz computer.\nLing et al. [11], emphasized the ability of using colour\nfeatures to achieve robust object tracking. The system ex-\ntracts colour information cluster-by-cluster to compare the\nobject\u2019s similarity across the image sequences. Clusters are\nderived for each moving object based on the number of ar-\neas with similar RGB colour properties. Weighted cluster-\nbased matching is then used to compare cluster colour in-\nformation between the current and previous frame. Results\nshow that the cluster colour information can be used in de-\ntecting merged objects as well as fragmented objects.\nWithagen et al. [17], also presents a colour his-\ntogram based object detection and tracking system using\na likelihood-based framework to classify object and back-\nground pixels. The likelihood framework starts with match-\ning of histogram data to find the best location of the object-\ncore followed by the classification of pixels into objects.\nJohnsen et al. [10] presents and object tracking and classifi-\ncation systems capable of tracking multiple objects in real-\ntime. To successfully classify an object as a person, group\nof people, or a vehicle, the assumption that objects merge\nto form a single object of size similar to the sum of the two\nobjects is made. The systems works in real time with re-\nported processing speed of about 50 frame per second for a\n768\u00d7 576 size colour image.\nThe need to handle fragmentation and grouping of mov-\ning object silhouettes in a robust visual tracking system has\nbeen mentioned above. Typically, a similarity matrix is use\nto establish a frame-to-frame correspondence between ob-\njects and silhouettes. The computational requirement in-\ncreases significantly if the number of objects is varying and\nunknown. This paper presents an object tracking compo-\nnent, implemented on a Field Programmable Gate Array\n(FPGA) to enhance the real-time multiple object tracking\nin a visual surveillance system.\nThe paper is organised as follows, section 2 presents re-\nlated FPGA object tracking systems. Section 3 gives a gen-\neral description of the entire system and where the module\npresented here fits into a complete tracking system. Sec-\ntion 4 gives the theory alongside the rules applied to resolve\nfragmented and grouped silhouettes using binary histogram\ndata. This is followed by the FPGA architectural detials of\nthe design in section 5. Sections 6 and 7 present some ex-\nperimental results and conclusion respectively.\n2. Related Work\nObject detection and tracking algorithms need to run in\nreal-time, if they are to be used for the purposes of secu-\nrity surveillance. Most robust tracking systems are imple-\nmented purely in software, exhibiting a high level of ro-\nbustness with little or no real-time processing capabilities.\nMuhlbauer et al. [13] presents the design and implemen-\ntation of a feature tracking systems on reconfigurable hard-\nware. The design is a modular implementation of a feature\ntracker on a Xilinx Spartan3 FPGA, capable of processing\nVGA size image at 162fps. McErlean [12], presents a mo-\ntion detection and object tracking system targeting an Altera\nStratix FPGA. The design is based on an algorithm devel-\noped specifically for hardware implementation, capable of\ndetecting and tracking moving objects. The design of the\nsystem allows for a number of Block Based Phase Corre-\nlation pipelines to be instantiated in parallel. The use of\nFPGA BlockRAM as pipeline buffers in the system imple-\nmentation has contributed to the increased performance.\nXu et al. [19] presents an FPGA implementation of the\nActive Shape Model algorithm for recognition of non-rigid\nobjects. The design takes advantage of the parallel nature\nof the algorithm by implementing a deep pipeline structure\nfor its acceleration. The design has been accomplished on\na single Altera Stratix FPGA with a speed up of about 15\ntimes that of a software implementation running on an Intel\nPentium 4 processor.\nCho et al. [6], also presents a real-time visual track-\ning system fully implemented on FPGA using colour his-\ntograms. The proposed visual tracking circuit searches all\nregions of the image to perform a matching operation in\norder to estimate the position of a moving object. The suc-\ncessful implementation of the tracking circuitry on FPGA\nhas a speedup of approximately 9 times compared to a soft-\nware implementation on an Intel Pentium 4 processor.\nWang et al. [16] presents a real-time object tracking\nsystems implemented on TMS320DM643 DSP and a Spar-\ntan3E FPGA. The design takes advantage of the flexibility\nof DSP and the logic rich properties of the FPGA architec-\nture, and can be used for tracking short-range air-defence\nanti-missile weapon.\nPoint-based tracking methods like Kalman filtering, par-\nticle filtering and condensation algorithms have success-\nfully been used for tracking multiple objects [6, 20]. How-\never, they fail to track successfully when used to track fast\nmoving objects. Silhouette-based approaches, which re-\nlies on colour histograms can be used to track fast moving\nobjects because of their robustness to rotation, partial oc-\nclusion and non rigidity of the targets [9]. Similarly, the\nsilhouette-based approach can fail due to changes in illumi-\nnation in camera parameters or in object motion [6].\nTo demonstrate the potential of colour information in\nhandling split\/merge silhouettes on a hardware architecture\nfor the purposes of security surveillance, binary signatures\nextracted from colour histogram of moving objects are used\nto establish when two objects group into a single silhouette,\nand when a single object fragments into two silhouettes.\n3. System Overview\nA system capable of detecting when two tracked objects\nmerge into a single object, or when a single tracked object\nfragment into parts, has been developed and tested. Vari-\nous issues may contribute to these two problems (split and\nmerge) in tracking. When a tracked object is partially oc-\ncluded by a stationary object like a tree or a fence, the object\nsilhouette may fragment into parts. Similarly, when part of a\nmoving or tracked object has very similar colour and texture\nproperties to the background, pixels of the tracked object\ncan be misclassified, which can also cause fragmentation.\nNot only can fragmentation cause problems during track-\ning, but also in the grouping of objects. Spatially proximate\nobjects can cause tracked objects to merge temporarily into\na single silhouette [2].\nMost silhouette-based tracking algorithms like the one\npresented by Owens et al. [14] are purely measurements\nbased object-to-silhouette matching, which attempt to track\nobjects in that form they are initially detected. Therefore,\nthe system always attempts to track distinct objects even if\ntheir silhouettes merge with that of other objects. A best-\nmatch process (similarity matrix), a computationally inten-\nsive task, is used to match silhouettes conservatively to ex-\nisting objects from frame-to-frame. The task of matching\nsilhouettes to objects becomes more computationally de-\nmanding when the number of objects varies from the num-\nber of silhouettes in the current frame.\nThis is usually the case when spatially proximate ob-\njects merge or objects leave the field of camera view \u2013 thus\nthe total number of objects will exceed the total number of\nsilhouettes. Similarly, when a partially occluded object is\nfragmented into parts or when a new object enters the field\nof camera view, the total number of objects will exceed the\ntotal number of silhouettes. Our goal is to use a massively\nparallel architecture, like FPGA, to speedup the best-match\nprocess in a silhouette-based tracking algorithm. The role\nof our system is to determine when a new object has en-\ntered the scene as opposed to a single object fragmentation.\nAlso the system should determine when an object leaves the\nscene as opposed to grouping by proximity.\nTo achieve this challenging task on FPGA architecture,\ntaking advantage of data parallelism and deep pipelining,\nbinary signatures extracted from colour histograms of mov-\ning objects have been used. The FPGA is used as a copro-\ncessor to facilitate the tracking process, but only when the\ntotal number of objects (from a previously processed frame)\nvaries from the total number of silhouettes in the current\nprocessed frame. Figure 1 is a flow diagram of the entire\nImage \nsegmentation\nObjects\n(Obj.)\nSilhouettes \n(Sil.)\nSil. > Obj.\nConnected \nComponents\nBest match equal\nFPGA split \ndetector\nFPGA merge \ndetector\nYES\nNO\nObj update\nFigure 1. A flow diagram of the overall system, showing when\nFPGA is used to detect split\/merge in object tracking.\nsystem, showing the split\/merge components implemented\non FPGA. A history of all objects in the camera view is\nmaintained. For each frame the number of silhouettes ex-\ntracted after the connected component analysis is compared\nwith the number of known objects. If the number is un-\nequal, the binary signatures (from the histogram data) of\nthe silhouettes, as well as the objects are fed into the appro-\npriate block on the FPGA for further analysis. The output\nof the FPGA describes when:\n1. Two objects have merged into a single silhouette,\n2. A single object has split into two silhouette,\n3. A new object has entered the scene,\n4. An object has left the scene.\n4. Split\/Merge with Binary Histogram\nThe splitting and merging of the objects during tracking\nis always a challenging problem. The reason is that most\nobject detection algorithms can only detect a blob (silhou-\nette) as one object, and can only provide the feature for this\nobject. When two blobs merge into one blob it is difficult\nto make a decision based on the features. This problem\nbecomes more challenging, especially in embedded object\ntracking systems, as the algorithms should be able to run in\nreal-time.\nIn our system we use colour histogram features. We de-\nfine a binary signature from the colour histogram of each\nblob that is not only robust and consistent across frames,\nbut also easy to store and implement in the FPGA hardware.\nIn the following, we can prove that the Hamming distances\nbetween the binary signatures of merged blob and original\nblobs are significantly different, and can be used to detect\nsplitting and merging of the blobs.\nLemma 1 Assume x \u2208 {0, 1, ..., 2K \u2212 1} and y \u2208\n{0, 1, ..., 2K \u2212 1} (K is a positive integer) are two i.i.d.\nrandom variables with discrete uniform distribution, then\ntheir sum z = x + y has a discrete probability distribution\nwith z \u2208 {0, 1, ..., 4K \u2212 2}. Define x\u02dc is a binary variable\nbased on x and its mean x\u00af value,\nx\u02dc =\n{\n1 x \u2265 x\u00af\n0 x < x\u00af (1)\nThen the probability that z\u02dc and x\u02dc are not equal is 1\/4.\nP {z\u02dc \u0004= x\u02dc} = 1\/4 (2)\nBecause the distribution of x, y are known, and z = x+y\nis a simple variable, this lemma can be proved by counting\nthe mismatch of the x\u02dc and z\u02dc where z\u00af = x\u00af + y\u00af. Of course,\nthe following is also true:\nP {z\u02dc \u0004= y\u02dc} = 1\/4 (3)\nIf we have two i.i.d. sequences X = {x1, x2, ..., xN}\nand Y = {y1, y2, ..., yN} in which xi \u2208 {0, 1, ..., 2K \u2212 1}\nand yi \u2208 {0, 1, ..., 2K \u2212 1} have same discrete uni-\nform distributions, the sum of the two sequences Z =\n{z1, z2, ..., zN} where zi = xi+ yi will have the associated\nresult, because every component has the result of the above\nlemma. As the Hamming distance of two binary sequences\nis the number of mismatches, we have the following theo-\nrem.\nTheorem 1 In the above condition, the mean values of the\nHamming distance Hmd between the binary sequences Z\u02dc,\nX\u02dc and Y\u02dc have the following results.\nmean(Hmd(Z\u02dc, X\u02dc)) = N\/4\nmean(Hmd(Z\u02dc, Y\u02dc )) = N\/4\n(4)\nIn our tracking problem, we can treat two colour his-\ntograms of two original blobs as X and Y . Then the merg-\ning blob can be considered as Z = X + Y at the frame\nwhere merging and splitting occurs. We can find that both\nthe Hamming distance of the binary sequences Z\u02dc, X\u02dc are\nvery big. For example, when N = 768, the mean hamming\ndistance will be 192. So we can define a threshold on the\nHamming distance to detect splitting and merging.\nFor a sparsely populated scene, one can track the centres\nof the blobs, frame-by-frame. Thus, within a local area only\none merging or splitting event will normally occur. In this\ncase, our method as presented is very efficient and accurate\nin detecting splitting and merging.\n1\n0\n1 1 1 0 1 0 1 0 1 1 0 0 0 0 1 0\nMean \nvalue\nFigure 2. A sample 16 bin histogram, showing how the binary fea-\nture vector can be extracted. The bins in yellow (grey) are the bins\ngreater or equal to the threshold value \u03b8 (mean of all bins) and the\nblue (black) bins are bins less than the threshold value. The yellow\nbins give a binary output of 1 and the blue a binary output of 0.\n4.1. Feature Vector Construction\nUsing colour histograms to model scenes can be very ex-\npensive if the full 24-bit representation of the RGB pixels is\nused. Various encoding schemes can be used to reduce the\nnumber of bins required to represent the 24 bit colour in-\nformation at the cost of reduced performance. The colour\nhistogram itself can be used as a feature vector. When there\nare more bins in the histogram, the feature vector gives a\nmore detailed descriptive representation with reduced pro-\ncessing time. Given the silhouette of a moving object after\napplying image segmentation (background differencing), a\n768 bin histogram is generated; 256 bins for each of the\nRGB colour components. To convert this into a binary sig-\nnature, the average \u03b8 for the 768 bins is computed as shown\nin equation 5 and any histogram bin with a value greater\nthan or equal to \u03b8 is represented as binary 1 and binary 0\notherwise as shown in figure 2.\n\u03b8 =\n\u2211768\ni=1 bini\n768\n(5)\nA binary feature vector (binary signature), x =\n{x1, x2, . . . , xn} for n = 768 is generated as in equation\n6.\nxi =\n{\n1 if bini >= \u03b8\n0 otherwise (6)\n4.2. Properties of Feature Vector\nIf the lightning conditions remain fairly constant, then\nthe normalized colour histogram of a moving object also re-\nmains constant, irrespective of the position of the object to\nthe camera. Similarly, the normalized histogram of a slow\nmoving object appearing in the field of view of a camera\nwill also remain constant. From equations 1-4, if two ob-\njects merge into a single silhouette and the overlapping re-\ngion is fairly small as compared to the two constituent sil-\nhouettes, the histogram of the merged silhouette is approxi-\nmately equal to the sum of the two individual silhouettes.\nAlso, if an object is fragmented into two parts, then the\nhistogram of the joint silhouette is approximately equal to\n01\n1100111100000111\n0000001111000110\n1100111111000111\n0\nFigure 3. Two image sequences used to demonstrate the behaviour\nof binary vectors of tracked as object when they merge. The top\nimage has two objects with different binary vectors, the bottom\nshows the two object merged with a binary vector similar to an \u2295\nof the binary vectors of the two constituent objects.\nthe histogram of the sum of the fragmented parts. Binary\nhistogram feature (binary vector) of a merged silhouette is\napproximately equal to the logical OR (\u2295) of the constituent\nsilhouettes. Thus, an \u2295 of binary vectors of two fragments\nof a silhouette will be similar to the binary vector of the\njoint silhouette. Using binary signatures, a natural approach\nto measure similarity is the Hamming distance measure.\nGiven two sets of binary sequences\nOf\u22121 = (o1,o2, . . . ,on) and Sf = (s1, s2, . . . , sm),\nwhere Sf is the set of binary vectors collected from frame\nf , representing the m distinct silhouettes and Of\u22121 is the\nupdated history of objects collected over f \u2212 1 frames\nfor n objects. Each element of the set is a binary vector\nwith 768 bits, representing the 768 colour histogram bins,\ni.e o1 = (o1[0], o1[1], . . . , o1[767]). Figure 3 shows two\nimage sequences with the corresponding binary vectors\nof the tracked objects. In the top frame, two objects have\nsuccessfully been segmented as they are far apart. In the\nbottom frame, the two objects overlap and hence segmented\nas single object (a merge). The binary vector extracted\nfrom the bottom frame is approximately the \u2295 of the two\nbinary vector from the top image.\nWhen two objects enter the camera view next to each\nother (overlap) the tracking system will attempt to track\nthem as they appear in the first frame. It is also possible\nfor the object to move apart in the scene; creating an ex-\ntra object for the tracker. The binary vector of the initially\ntracked object is expected to be approximately equal to the\n\u2295 of the two objects. Section 6, shows the result of this im-\nplementation on real image sequences. The best-matching\nprocess is significantly reduced if split\/merge is detected in\na pre-processing stage.\nSize of binary vectors 768 bits\nMaximum Silhouette in a frame 5\nMaximum stored Objects 5\nThreshold value 90\nTable 1. Specification of the FPGA based split\/merge implementa-\ntion. The design has been tested with a maximum of four objects\nin the camera view.\n5. FPGA Implementation\nThe most critical aspect of any hardware design is the se-\nlection and design of the architecture that provides the most\nefficient and effective implementation [5]. Base on binary\nvectors of moving objects, we implement an efficient FPGA\nbased architecture for the fragmented objects, grouped ob-\njects, new objects and objects leaving the camera view.\nThe specifications of the circuit implemented on FPGA is\ngiven in table 1 with its corresponding block diagram in fig-\nure 4. The circuitry is made up of five basic blocks: the\ncomparison, logical OR, Hamming distance computation,\nMinimum Hamming distance computation and threshold-\ning blocks. The thresholding block is triggered when the\nminimum Hamming distance is computed. Details of the\nfive basic blocks are presented in the following sections.\n5.1. The Comparison block\nThis is used to check the number of input objects and\nsilhouettes to determine which block to trigger. A split\nmight have occurred if the number of silhouettes exceeds\nthe number of tracked objects. Similarly, a merge might\nhave occurred if the number of objects exceeds the num-\nber of silhouettes. The design is limited to a maximum of\nfour objects and five silhouettes for the split block, and a\nmaximum of four silhouettes and five objects for the merge\nblock. This can be scaled up to any number, depending on\nthe available FPGA resource and most importantly if the\nscene is not crowded.\n5.2. Logical OR block\nThis block is used to compute a logical OR between all\npairs of silhouettes for the split detector (and all pairs of ob-\njects for the merge detector). For a maximum of five silhou-\nettes, there are 40 possible pairs. The logical OR between\nall the 40 pairs are computed in parallel and takes exactly\n768 clock cycles. The number of cycles can further be re-\nduced if the storage structure is changed.\nObj0\nObj1\nObj0\nObj2\nObj3\nObj4\nHD S0 HD S1 HD S2 HD S3\nHD S0 HD S1 HD S2 HD S3\nHD S0 HD S1 HD S2 HD S3\nMerge detection\nSil0\nSil1\nSil0\nSil2\nSil3\nSil4\nHD O0 HD O1 HD O2 HD O3\nSplit detection\nHD O0 HD O1 HD O2 HD O3\nHD O0 HD O1 HD O2 HD O3\nSilhouettes >ObjectsNO YES\nMinimum Hamming Distance Computation block\nFigure 4. A block diagram of the FPGA design. Depending on the\ntotal number of silhouettes and objects, the split or merge block\nis used. For the split block, thus when the number of silhouettes\nis greater than the number of objects, a logical OR of all pairs of\nbinary vectors id first computed. This is followed by the compu-\ntation of the Hamming distance between every OR results and all\nbinary vectors of the objects. The minimum Hamming distance is\ncomputed to establish the pair of split silhouettes. If the minimum\nHamming distance is greater than the threshold value, a new ob-\njet is considered to have entered the scene. Similarly, the merge\nblock is used to determine a possible merge or an object leaving\nthe scene.\n5.3. Hamming distance computation\nThis block is used to compute the Hamming distance be-\ntween each logical OR output and all binary vectors of the\nobjects (for the split detection) and all binary vectors of the\nsilhouettes (for the merge detection). The Hamming dis-\ntance between a logical OR output xi and an object oj , as\nshown in equation 7 is a bitwise operation, and hence takes\nas many clock cycles as there are bits in the binary vec-\ntor. Since the Hamming distance for all the 40 logical OR\noutputs are computed in parallel, it takes exactly 768 clock\ncycles to compute the Hamming distance for all the objects\nand silhouettes.\nHij =\n768\u2211\nk=1\nHijk (7)\nwhere k iterates through the bits in the binary vector and\ni \u2208 (1 \u00b7 \u00b7 \u00b7 40) addresses the logical OR output.\n5.4. Minimum Hamming distance\nThis block is used to identify the minimum value of all\nthe Hamming distances computed in the previous section.\nThe hardware design, as shown in figure 5, uses a series of\nHD1\nHD2\nHD3\nHD4\nHD39\nHD40\nS1\nS2\nD\nC ENB\nMultiplexer\nS1\nS2\nD\nC ENB\nMultiplexer\nS1\nS2\nD\nC ENB\nMultiplexer\nHD37\nHD38\nS1\nS2\nD\nC ENB\nMultiplexer\nS1\nS2\nD\nC ENB\nMultiplexer\nS1\nS2\nD\nC ENB\nMultiplexer\nS1\nS2\nD\nC ENB\nMultiplexer\nMinimum \nHamming \ndistance and  \naddress of \ncorresponding \npair\n7 clock cycles\nFigure 5. Structure of the minimum Hamming distance computa-\ntion block. This block uses seven cycles to compute the minimum\nHamming distance. The first cycle requires twenty comparators,\nwhich is halved every cycle to one in the seventh cycle.\ncomparators to select the minimum of a pair of two input\nHamming distances. For an implementation with 40 values,\nthe design takes exactly seven clock cycles to compute the\nminimum Hamming distance.\n5.5. Thresholding block\nThis block compares the output of the minimum Ham-\nming distance block to make a decision. The output of this\nblock is one of four values:\n\u2022 0 (split) \u2013 when the minimum Hamming distance from\nthe split block is less than the threshold.\n\u2022 1 (new object) \u2013 when the minimum Hamming dis-\ntance from the split block is greater than the threshold.\n\u2022 2 (merge) \u2013 when the minimum Hamming distance\nfrom the merge block is less than the threshold.\n\u2022 3 (object left) \u2013 when the minimum Hamming distance\nfrom the merge block is greater than the threshold.\nWhen there is a split or merge, the split pair or merged sil-\nhouette is also reported.\nThe split\/merge architecture discussed here has been im-\nplemented on a Xilinx Virtex-4 FPGA chip (XC4VLX160)\nwith approximately 152,064 logic cells with embedded\nRAM totalling 5,184 Kbits. The design and verification\nwas accomplished using the Handel-C high level descrip-\ntive language. Compilation and simulation were achieved\nusing the Mentor Graphics DK design suite. Synthesis \u2013\nthe translation of abstract high-level code into a gate-level\nnet-list \u2013 was accomplished using Xilinx ISE tools.\nThe entire design has been clocked at 40MHz, making it\npossible to process up to 25,000 binary vectors of size 768\nbits in a second. The clock frequency of 40MHz also in-\ncludes the design for controlling the external logic for the\ndisplay and the input. This is the actual hardware test and\nthe most stable clock frequency. The frequency could be\nmuch higher without the requirement to interface these de-\nvices. Table 2 gives the details of the resource utilization of\nthe FPGA implementation.\nResource Total Used\nName Total Used Per.(%)\nFlip Flops 135,168 5,095 4\n4 input LUTs 135,168 18,387 13\nbonded IOBs 768 147 19\nOccupied Slices 67,584 13,468 17\nRAM16s 288 38 13\nTable 2. Implementation results for the split\/merge, using Virtex-4\nXC4VLX160, package FF1148 and speed grade -10.\n6. Experimental Results\nWe evaluate the performance of the binary vector based\nsplit\/merge algorithm with five different scenes, each col-\nlected over a period of 60 minutes. The total number of\nobjects in any of the scenes is less than five. One of the five\nimage sequences has been taken from an indoor environ-\nment with the other four from outdoor. The total number of\nframes used in our experiments is approximately 180, 000\nwith 78 objects, mostly persons. The tracking algorithm\npresented in [14] is used to extract binary histogram of mov-\ning blobs. Blobs with less than 768 pixels are removed as\nnoise.\nThe main aim is to establish the characteristics of the\nbinary histogram features of moving objects in the scene.\nWe want to establish the similarity between the binary vec-\ntors for a single non-ridged object over a period of time,\nfor its entire existence in the camera scene. Again, we will\nlike to compute the joint binary vector of objects to esti-\nmate their similarity, should they merge. Figure 6, shows\ntwo images each from all the five test sequences. The bi-\nnary vectors of objects are compared from frame-to-frame\nto determine the threshold value for the Hamming distance.\nTypical, the Hamming distance between the binary vectors\nof the same object across frames should be approximately\n  \n \n  \n \n  \n \n  \n \n \nFigure 6. Two frames each taken from all the five scenes used in\nthe experiments.\nequal to zero. Due, to the non-ridged nature of the objects\nbeing tracked, the colour histogram may vary from frame-\nto-frame and hence the binary vectors.\nTable 3, shows the Hamming distance between the two\nobjects from the top row of figure 6. The Hamming dis-\ntance is computed for each tracked object between consec-\nutive frames, from frame 25 to 29. The Hamming distance\nbetween the joint binary vector of the merged objects, in\nframe 30 and the binary vectors for the two individual ob-\njects taken from frame 29 is 56. Similarity between objects\nand silhouettes is established with the minimum Harming\ndistance. A threshold value of 90 has been chosen empiri-\ncally after test with all the five image sequences. It has also\nHamming Distance\nFrame # Obj.0 Obj.1 Obj.0 vrs Obj.1\n25 83 87 121\n26 87 84 127\n27 75 79 118\n28 72 72 114\n29 67 70 109\nTable 3. Table showing the Hamming distance between two ob-\njects from frame-to-frame to establish when there is a split\/merge,\nusing the binary vectors generated from their colour histograms.\nbeen established from the experiments that, Hamming dis-\ntance between different objects exceed the threshold from\nframe-to-frame.\n7. Conclusion\nWe have demonstrated a vision based framework for de-\ntecting grouped and object fragmentation using binary vec-\ntors generated from colour histograms. The use of binary\nvectors makes it possible to efficiently implement the de-\nsign on FPGA to run in real-time. We have also demon-\nstrated how the execution time for the best-match process\nused by most silhouette-base tracking algorithms can be\nreduced with the detection of split\/merge objects prior to\nit. Work is ongoing on the design of an efficient similar-\nity measure for use in the best-match process, to enable a\ncomplete tracking system fully implementable on FPGA.\nAcknowledgements\nThis work was supported by TSB under the BRAINS\nProject. Special thanks goes to the project team; Prof.\nDerek Sheldon, Nigel Priestley, Mervyn Hobden, Peter\nHobden and Cy Pettit.\nReferences\n[1] K. Appiah, A. Hunter, J. Owens, P. Aiken, and K. Lewis.\nAutonomous real-time surveillance system with distributed\nip cameras. In Third ACM\/IEEE International Conference on\nDistributed Smart Cameras. IEEE Computer Society, 2009.\n[2] B. Bose, X. Wang, and E. Grimson. Multi-class object\ntracking algorithm that handles fragmentation and grouping.\nComputer Vision and Pattern Recognition, IEEE Computer\nSociety Conference on, 0:1\u20138, 2007.\n[3] Y. Boykov and D. P. Huttenlocher. Adaptive bayesian recog-\nnition in tracking rigid objects. Computer Vision and Pat-\ntern Recognition, IEEE Computer Society Conference on,\n2:2697, 2000.\n[4] T. J. Broida and R. Chellappa. Estimation of object motion\nparameters from noisy images. IEEE Trans. Pattern Anal.\nMach. Intell., 8(1):90\u201399, 1986.\n[5] C. Chang, M. Shibu, and R. Xiao. Self Organizing Feature\nMap for Color Quantization on FPGA. Springer, 2006.\n[6] J. U. Cho, S. H. Jin, X. D. Pham, D. Kim, and J. W. Jeon.\nFpga-based real-time visual tracking system using adaptive\ncolor histograms. Proc. of the IEEE International Confer-\nence on Robotics and Biomimetics, pages 172\u2013177, 2007.\n[7] R. T. Collins. Mean-shift blob tracking through scale space.\nComputer Vision and Pattern Recognition, IEEE Computer\nSociety Conference on, 2:234, 2003.\n[8] M. Isard and A. Blake. International Journal of Computer\nVision, 29:5\u201328, 1998.\n[9] A. Jacquot, P. Sturm, and O. Ruch. Adaptive tracking of non-\nrigid objects based on color histograms and automatic pa-\nrameter selection. In Proceedings of the IEEE Workshop on\nMotion and Video Computing, pages 103\u2013109. IEEE Com-\nputer Society, 2005.\n[10] S. Johnsen and A. Tews. Real-time object tracking and clas-\nsification using a static camera. Proceedings of IEEE Inter-\nnational Conference on Robotics and Automation, workshop\non People Detection and Tracking.\n[11] T. S. Ling, L. K. Meng, L. M. Kuan, Z. Kadim, and A. A. B.\nAl-Deen. Colour-based object tracking in surveillance appli-\ncation. Proceedings of the International MultiConference of\nEngineers and Computer Scientists 2009 Vol I, 1, 2009.\n[12] M. McErlean. An fpga implementation of hierarchical mo-\ntion estimation for embedded object tracking. International\nSymposium on Signal Processing and Information Technol-\nogy, 0:242\u2013247, 2006.\n[13] F. Muhlbauer and C. Bobda. Design and implementation of\nan object tracker on a reconfigurable system on chip. IEEE\nInternational Workshop on Rapid System Prototyping, 2006.\n[14] J. Owens, A. Hunter, and E. Fletcher. A fast model-free\nmorphology-based object tracking algorithm. In Proceed-\nings of the British Machine Vision Conference. British Ma-\nchine Vision Association, 2002.\n[15] V. Takala and M. Pietikainen. Multi-object tracking using\ncolor, texture and motion. IEEE Computer Society Confer-\nence on Computer Vision and Pattern Recognition, 2007.\n[16] Q. Wang and Z. Gao. Study on a real-time image object\ntracking system. In Proceedings of the 2008 International\nSymposium on Computer Science and Computational Tech-\nnology, pages 788\u2013791. IEEE Computer Society, 2008.\n[17] P. Withagen and F. G. K. Schutte. Likelihood-based object\ndetection and object tracking using color histograms and em.\nProceedings of International Conference on Image Process-\ning., 1:589\u2013592, 2002.\n[18] C. R. Wren, A. Azarbayejani, T. Darrell, and A. P. Pent-\nland. Pfinder: Real-time tracking of the human body. IEEE\nTransactions on Pattern Analysis and Machine Intelligence,\n19(7):780\u2013785, 1997.\n[19] J. Xu, Y. Dou, J. Li, X. Zhou, and Q. Dou. Fpga accelerating\nalgorithms of active shape model in people tracking appli-\ncations. Digital Systems Design, Euromicro Symposium on,\n0:432\u2013435, 2007.\n[20] A. Yilmaz, O. Javed, and M. Shah. Object tracking: A sur-\nvey. ACM Computing Surveys, 38(4):13, 2006.\n"}