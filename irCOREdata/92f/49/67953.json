{"doi":"10.1109\/TSMCB.2010.2098866","coreId":"67953","oai":"oai:eprints.lancs.ac.uk:34372","identifiers":["oai:eprints.lancs.ac.uk:34372","10.1109\/TSMCB.2010.2098866"],"title":"Fuzzily Connected Multimodel Systems Evolving Autonomously From Data Streams","authors":["Angelov, Plamen"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2011-08","abstract":"A general framework and a holistic concept are proposed in this paper that combine computationally light machine learning from streaming data with the online identification and adaptation of dynamic systems in regard to their structure and parameters. According to this concept, the system is assumed to be decomposable into a set of fuzzily connected simple local models. The main thrust of this paper is in the development of an original approach for the self-design, self-monitoring, self-management, and self-learning of such systems in a dynamic manner from data streams which automatically detect and react to the shift in the data distribution by evolving the system structure. Novelties of this contribution lie in the following: 1) the computationally simple approach (simpl_e_Clustering-simplified evolving Clustering) to data space partitioning by recursive evolving clustering based on the relative position of the new data sample to the mean of the overall data, 2) the learning technique for online structure evolution as a reaction to the shift in the data distribution, 3) the method for online system structure simplification based on utility and inputs\/feature selection, and 4) the novel graphical illustration of the spatiotemporal evolution of the data stream. The application domain for this computationally efficient technique ranges from clustering, modeling, prognostics, classification, and time-series prediction to pattern recognition, image segmentation, vector quantization, etc., to more general problems in various application areas, e. g., intelligent sensors, mobile robotics, advanced manufacturing processes, etc","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:34372<\/identifier><datestamp>\n      2018-01-24T03:03:19Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Fuzzily Connected Multimodel Systems Evolving Autonomously From Data Streams<\/dc:title><dc:creator>\n        Angelov, Plamen<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        A general framework and a holistic concept are proposed in this paper that combine computationally light machine learning from streaming data with the online identification and adaptation of dynamic systems in regard to their structure and parameters. According to this concept, the system is assumed to be decomposable into a set of fuzzily connected simple local models. The main thrust of this paper is in the development of an original approach for the self-design, self-monitoring, self-management, and self-learning of such systems in a dynamic manner from data streams which automatically detect and react to the shift in the data distribution by evolving the system structure. Novelties of this contribution lie in the following: 1) the computationally simple approach (simpl_e_Clustering-simplified evolving Clustering) to data space partitioning by recursive evolving clustering based on the relative position of the new data sample to the mean of the overall data, 2) the learning technique for online structure evolution as a reaction to the shift in the data distribution, 3) the method for online system structure simplification based on utility and inputs\/feature selection, and 4) the novel graphical illustration of the spatiotemporal evolution of the data stream. The application domain for this computationally efficient technique ranges from clustering, modeling, prognostics, classification, and time-series prediction to pattern recognition, image segmentation, vector quantization, etc., to more general problems in various application areas, e. g., intelligent sensors, mobile robotics, advanced manufacturing processes, etc.<\/dc:description><dc:date>\n        2011-08<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/TSMCB.2010.2098866<\/dc:relation><dc:identifier>\n        Angelov, Plamen (2011) Fuzzily Connected Multimodel Systems Evolving Autonomously From Data Streams. IEEE Transactions on Systems, Man and Cybernetics, Part B: Cybernetics, 41 (4). pp. 898-910. ISSN 1083-4419<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/34372\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":null,"relations":["http:\/\/dx.doi.org\/10.1109\/TSMCB.2010.2098866","http:\/\/eprints.lancs.ac.uk\/34372\/"],"year":2011,"topics":["QA75 Electronic computers. Computer science"],"subject":["Journal Article","PeerReviewed"],"fullText":"IE\nEE\nPr\noo\nf\nIEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART B: CYBERNETICS 1\nFuzzily Connected Multimodel Systems Evolving\nAutonomously From Data Streams\n1\n2\nPlamen Angelov, Senior Member, IEEE3\nAbstract\u2014A general framework and a holistic concept are pro-4\nposed in this paper that combine computationally light machine5\nlearning from streaming data with the online identification and6\nadaptation of dynamic systems in regard to their structure and7\nparameters. According to this concept, the system is assumed to be8\ndecomposable into a set of fuzzily connected simple local models.9\nThe main thrust of this paper is in the development of an originalAQ1 10\napproach for the self-design, self-monitoring, self-management,11\nand self-learning of such systems in a dynamic manner from data12\nstreams which automatically detect and react to the shift in the13\ndata distribution by evolving the system structure. Novelties of14\nthis contribution lie in the following: 1) the computationally simple15\napproach (simpl_e_Clustering\u2014simplified evolving Clustering) to16\ndata space partitioning by recursive evolving clustering based on17\nits relative position to the mean of the overall data, 2) the learningAQ2 18\ntechnique for online structure evolution as a reaction to the shift19\nin the data distribution, 3) the method for online system structure20\nsimplification based on utility and inputs\/feature selection, and21\n4) the novel graphical illustration of the spatiotemporal evolu-22\ntion of the data stream. The application domain for this com-23\nputationally efficient technique ranges from clustering, modeling,24\nprognostics, classification, and time-series prediction to pattern25\nrecognition, image segmentation, vector quantization, etc., to more26\ngeneral problems in various application areas, e.g., intelligent27\nsensors, mobile robotics, advanced manufacturing processes, etc.28\nIndex Terms\u2014Evolving fuzzy systems, fuzzily weighted recur-29\nsive least-squares estimation, fuzzy rule-based systems.30\nI. INTRODUCTION31\nS EVERAL important problems from control theory and ma-32 chine learning such as system modeling and identification33\n[9], clustering and classification [20], time series prediction [2],34\nand controller design [22] can be generalized into a common35\nframework that can be represented as a nonlinear mapping of36\nsome inputs onto some outputs. For example, in classification,37\nthe inputs are the features, while the outputs are the class38\nlabels; in clustering, the outputs are not existing; in control, the39\ninputs are usually the error and the derivative (or integral) of40\nthe error, while the outputs are the control actions; in system41\nmodeling, the inputs represent the independent variables, while42\nthe outputs\u2014the dependant ones; in time series predictions, the43\ninputs are past and present values, while the outputs are the44\npredicted future values of the time series. An obvious approach45\nManuscript received January 25, 2010; revised July 13, 2010 and October 12,\n2010; accepted December 5, 2010. This paper was recommended by Associate\nEditor F. Karray.\nThe author is with InfoLab21, School of Computing and Com-\nmunications, Lancaster University, Lancaster, LA1 4WA, U.K. (e-mail:\np.angelov@lancaster.ac.uk).\nColor versions of one or more of the figures in this paper are available online\nat http:\/\/ieeexplore.ieee.org.\nDigital Object Identifier 10.1109\/TSMCB.2010.2098866\n(that was dominant until 1970s) is to use some existing prior 46\nknowledge about the structure of the mapping (usually in the 47\nform of first principle models) and then to fine tune the param- 48\neters. The alternatives to the first principle models include the 49\nfollowing: 1) so-called black-box approach to which we can cat- 50\negorize state-space and polynomial models [9], more recently, 51\nneural networks (NN) [15], etc., and 2) expert and fuzzy rule- 52\nbased (FRB) models [22]. The former lack transparency and 53\nis often dubbed \u201cnumber fitting.\u201d The latter were tedious and 54\ndifficult to design until 1990s and were closely related to expert 55\nknowledge. During 1990s, it was proven that FRB are dual AQ356\nto certain types of NN. Thus, the term \u201cneuro-fuzzy systems\u201d 57\n(NFS) was introduced, and data-driven learning techniques 58\nwere developed [16] which decoupled the FRB systems from 59\nthe expert knowledge. This can be seen as an extension of 60\nthe well-known concepts of adaptive (usually linear) systems, 61\nindependent multiple model systems [24], Gaussian mixture 62\nmodels [2], generalized regression models [11], etc. 63\nThe importance of data-driven FRB systems becomes vital 64\nnowadays when we are surrounded by huge amounts of stream- 65\ning data flows leading to so-called \u201cdigital obesity\u201d [25] (every 66\nyear, more than 1 EB (= 1018 bytes) of data are generated AQ467\nworldwide, most of it in digital form). The availability of 68\nconveniently using efficient online real-time algorithms for 69\nextracting knowledge in a human-intelligible form from data 70\nstreams [13] is a pressing demand. It is important to emphasize AQ571\nthat the data-driven design methods do not exclude the expert 72\nknowledge which can be used (if available) at the initialization 73\nsteps or at a higher supervisory level. These approaches are 74\nimportant because they make the automation of the process of 75\ncomplex nonlinear systems design possible as well as extracting 76\nautomatically in real-time knowledge and providing it to human 77\nusers or decision makers which is vital for numerous practical 78\napplications [3], [17], [25]. 79\nSystem design problem can be divided into the following: 80\n1) system structure identification and 2) parameter learning [9]. 81\nWhile the latter problem is well studied [9], [16], the former 82\none is still an open problem. Traditionally, it was assumed that 83\nthe structure is selected and fixed by an expert based on some 84\nprior knowledge or experience or derived offline [9], [16], [18], 85\n[22]. During the last decade, a number of approaches emerged 86\naddressing the problem of online identification of FRB system 87\nassuming an evolving structure, such as self-organized fuzzy 88\nneural network (SOFNN) [4], SAFIS [5], ePL [8], eTS [12],\nAQ6\nAQ7\nAQ8\nAQ9\n89\ndynamic evolving neural-fuzzy inference system (DENFIS) 90\n[14], SELM [29], optimally pruned extreme learning machine AQ1091\n(OP-ELM) [30], enabling new function mode [31], evolving AQ11AQ1292\nNN [10], etc. Most of these require user- or problem-specific 93\n1083-4419\/$26.00 \u00a9 2010 IEEE\nIE\nEE\nPr\noo\nf\n2 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART B: CYBERNETICS\nthresholds that influence the resulting structure complexity94\nwhich leads to a large number of rules being generated; to95\nremedy this, they apply afterward \u201cpruning\u201d techniques [4],96\n[5], [14] that require further threshold parameters and may lead97\nto instability and represents an ad hoc technique. In eTS [12]98\nand DENFIS [14], the spreads of the membership functions99\nbelonging to various inputs have the same value. Improved ver-100\nsions were also published, such as exTS [19] which introduceAQ13 101\nlearning of the spread (cluster radii)\u201e but a major shortcoming102\nof these approaches is that they assume a fixed number of103\ninputs\/features determined in advance.104\nIn this paper, we propose an entirely data-driven method105\ncalled simplified evolving Takagi\u2013Sugeno simpl_eTS+ NFS106\nthat allows truly flexible and evolving structure systems to107\nbe designed autonomously online including selecting the in-108\nput variables, adapting the cluster radii, and using a compu-109\ntationally simplified evolving online clustering method. The110\nbasic principle is that of a gradual evolution of the fuzzily111\ncoupled multimodel structure in terms of local subsystems112\nas well as in terms of input variables. Contrast this to the113\nabrupt \u201cpruning\u201d and lack of flexibility in terms of input114\nvariable selection and cluster radii\/membership spread of all115\nother currently existing approaches. The structure is determined116\nby a new computationally simple evolving clustering method117\n(simpl_e_Clustering) in the joint input\u2013output data space118\n(see Section III). The quality of the local models and the119\nrespective automatically generated FRB can be monitored on-120\nline by qualitative measures, such as utility (Section IV). The121\ninputs\/features are gradually selected from an initial pool based122\non their recursively accumulated sensitivity using an original123\nmethod (Section V). The proposed approach is verified on a124\nnumber of challenging synthetic and real industrial examples125\n(Section VI).126\nII. PROBLEM STATEMENT127\nThe proposed simpl_eTS+ NFS can be seen as a gener-128\nalized framework that can model and describe nonlinear and129\nnonstationary processes, as shown in Fig. 1.130\nIt differs from all current NFS such as adaptive network-131\nbased fuzzy inference systems (ANFIS) [16], DENFIS [14],132\neTS [12], SAFIS [5], SOFNN [4], FLEXFIS [17], ePL [8],AQ14 133\nevolving NN [10], etc., by the fact that they have a prefixed134\nnumber of inputs (see Condition E, Fig. 2).AQ15 135\nIt is also unique in the way the system structure is generated136\n(evolves) from the streaming database on the data density137\nincrement (see next section and Fig. 3) that reflects shifts in the138\ndata stream (see Fig. 5). It also differs by the ability to gradually139\nsimplify the system structure based on the rules\u2019 utility.140\nSimpl_eTS+ can be interpreted as an evolving set of141\nlinguistic fuzzy rules of the following form:142\nRi : IF\n(\nx1 is x\ni\u2217\n1\n)\nAND . . . AND\n(\nxn is x\ni\u2217\nn\n)\nTHEN\n(\nLocalModel (yi)\n) (1)\nwhere Ri denotes the ith fuzzy rule, i = [1, N ], N is the num-143\nber of fuzzy rules, x = [x1, x2, . . . , xn]T is the input vector,144\nFig. 1. Schematic representation of the simpl_eTS+ as an NFS. Layer 1\nrepresents the membership functions grouped by fuzzy rules, layer 2\u2014the\naggregation operators, layer 3\u2014the normalization, layer 4 weighs the local\nmodels, and layer 5 sums the partial outputs.\nFig. 2. Rule-based evolution in the simpl_eTS+. System structure is not\nfixed but can rather gradually evolve in terms of fuzzy rules (Condition A for\ngrowth and Conditions B and C for reduction) as well as in terms of input\nvariables and respective fuzzy membership functions and dimensionality of the\nconsequences that are associated with them (Condition D).\n(xj is xi\u2217j ) denotes the jth fuzzy sets of the ith fuzzy rule, 145\nj = [1, n]; note that n also evolves, xi\u2217 is the focal point of the 146\nith rule antecedent, LM(yi) is the Local Model of the ith fuzzy 147\nrule, which is expressed by the (generally, m-dimensional) 148\noutput variables yi = [yi1, yi2, . . . , yim]. 149\nThe data are standardized online using recursively updated 150\nmean and standard deviation [19] 151\nxst =\nx\u2212 x\n\u03c3x\nx(k) =\nk \u2212 1\nk\nx(k \u2212 1) + 1\nk\nx(k)\nx(1) =x(1)\n\u03c32x(k) =\nk \u2212 1\nk\n\u03c32x(k \u2212 1) +\n1\nk \u2212 1 (x(k)\u2212 x(k))\n2 \u03c32x(1)\n= 0nx1\nIE\nEE\nPr\noo\nf\nANGELOV: FUZZILY CONNECTED MULTIMODEL SYSTEMS EVOLVING AUTONOMOUSLY FROM DATA STREAMS 3\nFig. 3. (a) Two-dimensional illustration of the principle of forming new clus-\nters and fuzzy rules based on density increment \u03b4 [see (8) and the Appendix]\nrelative to the global mean (denoted by a diamond). New clusters\/rules of one\nof the following types are formed by new data samples. Type 1 (inside the\ninner circle)\u2014they provide better generalization and summarization. Type 2\n(outside the outer circle)\u2014they provide better coverage of the data space. The\ndata between the two circles are interpreted by the existing rules. (b) For\ncomparison, the same data from Fig. 3(a) are used to demonstrate the zone\nof influence (using ellipsoids at a fixed membership degree value). It is obvious\nthat they ensure a good coverage.\nyst =\ny \u2212 y\n\u03c3y\ny(k) =\nk \u2212 1\nk\ny(k \u2212 1) + 1\nk\ny(k)\ny(1) = y(1)\n\u03c32y(k) =\nk \u2212 1\nk\n\u03c32y(k \u2212 1) +\n1\nk \u2212 1 (y(k)\u2212 y(k))\n2\n\u03c32y(1) = 0mx1 (2)\nwhere xst denotes the standardized input data, yst\u2014152\nstandardized output data, x is the mean of the input data, y\u2014the153\nmean of the output data, \u03c3x denotes the standard deviation154\n(variance) of the input data, and \u03c3y\u2014the variance of the155\noutput data.156\nFor a specific type of the local subsystems, one can transform157\nthe simpl_eTS+ into one of the following types:158\n1) first order TS-type [1] FRB system when the local sub-159\nsystems are linear160\nyi = xTe \u03c0\ni xTe = [1, x\nT ] \u03c0i =\n\u2223\u2223\u2223\u2223\u2223\u2223\nai01 . . . a\ni\n0m\n. . . . . . . . .\nain1 .. a\ni\nnm\n\u2223\u2223\u2223\u2223\u2223\u2223 (3)\n2) zero-order TS-type FRB system that can also be consid- 161\nered as a simplified Mamdani (sM) type when the local AQ16162\nsubsystems are singletons (crisp scalar values) 163\nyi = Ai (4)\nwhere Ai = [ai01 ai02 ai0m]T denotes the local subsys- 164\ntem parameters. 165\nThe overall output of the simpl_eTS+ system y is formed 166\nas a collection of loosely\/fuzzily combined multiple local sim- 167\npler subsystems yi. The degree of activation of each local 168\nsubsystem is proportional to the level of its contribution to the 169\noverall output [22] 170\ny =\nN\u2211\ni=1\n\u03bbiyi; \u03bbi = \u03c4 i\n\/\nN\u2211\nj=1\n\u03c4 j (5)\nwhere yi represents the output of the ith local subsystems, \u03bbi 171\nis the normalized activation level of the ith rule, and \u03c4 i is the 172\nfiring level of the ith rule. 173\nThe firing level can be defined as a Cartesian product 174\n(t-norm) of respective degrees of membership of the antecedent 175\nfuzzy sets (activation of neurons) for this rule [22] 176\n\u03c4 i =\nn\nT\nj=1\n\u03bcij(xj) (6)\nwhere \u03bcij is the membership value of the jth input xj , j = [1, n] 177\nin the ith fuzzy rule i = [1, N ]. 178\nThe membership function can be of any known form. The 179\nGaussian and bell functions are preferable due to their general- 180\nization capabilities (resembling normal distribution and cover- 181\ning the whole domain of variables) [22] 182\n\u03bcij = e\n\u2212\n\u2016x\u2212xi\u2217\u20162\nj\n2(rij)\n2\n(7)\nwhere, for (rij)2, i = [1, N ] j = [1, n] is the spread of the 183\nmembership function, which represents the spread of the zone 184\nof influence of the cluster\/rule\/neuron projected on the jth axis. AQ17185\nIn the following section, we will describe the 186\nsimpl_e_Clustering method used in simpl_eTS+ 187\nto automatically extract the system structure from data streams 188\nby \u201con fly\u201d clustering, detecting novel data patterns and 189\nevolving the shape of the clusters. 190\nIII. SIMPLIFIED EVOLVING CLUSTERING METHOD 191\n(simpl_e_Clustering) 192\nThe basic principles of the newly proposed 193\nsimpl_e_Clustering approach are to ensure the following: 194\nP1) good generalization and summarization of data by clus- 195\ntering around the focal points located in areas of high 196\ndensity D; 197\nP2) good coverage of the entire data space by expanding the 198\narea of interpolation. 199\nIE\nEE\nPr\noo\nf\n4 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART B: CYBERNETICS\nThe latter condition is evaluated in respect to the relative200\nposition of the new data point, existing focal points, and the201\nglobal mean of all standardized data [23]. Additionally, the202\nlevel of overlap of the fuzzy rules is being controlled removing203\npreviously existing rules that overlap significantly with the204\nnewly formed rules. Finally, rules\/neurons\/clusters are being205\nremoved if they are not used actively.206\nMachine learning literature introduced the paradigm of con-207\ncept change [6] of the underlying distribution of data streams208\ncalled drift (gradual evolution of the concept over time) or209\nshift (a more abrupt and sudden change of the concept over210\ntime). The concept drift is represented as a smooth sliding of211\nthe data distribution in the data\/feature space from one region212\nto another. The concept shift is directly related to the concept213\nof evolving systems\u2014forming new clusters\/neurons\/rules or214\nremoving existing ones that are not used actively.AQ18 215\nWhile the data density is represented in the data space216\ndomain, both the drift and shift must be analyzed in the joint217\ndata\u2013time space domain. In the newly proposed simpl_eTS+218\nmethod, shifts in data streams are detected naturally by the219\ndensity increment \u03b4(k) [see Fig. 3(a)].220\nReaction to a detected shift is by either the following:221\n1) forming a new rule around a new data sample which222\nbecomes a focal point for global data distribution;AQ19 223\n2) replacement of an existing fuzzy rule.224\nReplacement of a rule itself consists of the following:225\na) forming a new rule around the new point\u2014same as in the226\nprevious step;227\nb) removal of the rule which has lower density and is close228\nto this newly added one.229\nIn summary, we evolve the structure of the simpl_eTS+230\naccording to the following conditions:231\nCondition A) a data sample that covers a new area of the data232\nspace represented by the density increment relative to the233\nglobal mean [see Fig. 3(a)];234\nCondition B) avoid overlap and information redundancy (see235\nFig. 4).236\nCondition A is defined (see Lemma 1 in the Appendix for237\ndetails) as238\nIF (\u03b4(k) = N) THEN (x(k) \u2192 new centre) (8)\nwhere \u03b4(k) denotes the total density increment239\n\u03b4(k) =\n\u2223\u2223\u2223\u2223\u2223\u2223\nN\u2211\ni=1\nsign\nn+m\u2211\nj=1\n\u03b3i\u2217j (k)\n\u2223\u2223\u2223\u2223\u2223\u2223\n\u03b3i\u2217j (k) =\n(\nzi\u2217j (k)\n)2 \u2212 z2j (k) + v2 (zj(k)\u2212 zi\u2217j (k)) zi\u2217j (k)\nis the partial (per cluster and per input) density incre-240\nment and z = [xT ; y]T denotes the input\u2013output vector (data241\nsample).242\nIt can be proven that the global mean tends asymptotically243\ntoward zero when the number of standardized data points tends244\nto infinity (Fig. 5)\u2014see Lemma 2 in the Appendix for the proof245\nlim\nk\u2192\u221e\nzst = 0. (9)\nFig. 4. Condition B: If the overlap between the new and existing membership\nfunctions is very high, then the previous (old) rule is removed and replaced by\nthe new one. AQ20\nFig. 5. Convergence of the global mean of the standardized data to the origin\n(Lemma 3)\u2014data space representation for two inputs.\nCorollary 1: When standardized data are being used, then 246\nlimk\u2192\u221e \u03b3i\u2217j (k) = (z\ni\u2217\nj (k))\n2 \u2212 z2j (k). 247\nAs a result of Condition A, two types of new focal points of 248\nclusters\/rules will be formed [see Fig. 3(a)]: 1) type 1\u2014based 249\non principle P1, for which\n\u2211N\ni=1 sign\n\u2211n+m\nj=1 \u03b3\ni\u2217\nj (k) = N 250\nand 2) type 2\u2014based on principle P2, for which 251\u2211N\ni=1 sign\n\u2211n+m\nj=1 \u03b3\ni\u2217(k) = \u2212N . 252\nNew rules of type 1 represent focal points with higher density 253\nthan any previous focal point [see the inner circle in Fig. 3(a)]. 254\nNew rules\/clusters of type 2 are defined by focal points that 255\ncover new areas of data space that cannot be interpolated well 256\nusing previously existing rules only\u2014outside the outer circle in 257\nFig. 3(a). 258\nCorollary 2: When standardized data are used, Condi- 259\ntion A reduces to\n\u2211n+m\nj=1 (z\ni\u2217\nj (k))\n2 >\n\u2211n+m\nj=1 z\n2\nj (k) \u2200i, i = 260\n[1, N ] for type 1 and\n\u2211n+m\nj=1 (z\ni\u2217\nj (k))\n2 <\n\u2211n+m\nj=1 z\n2\nj (k) \u2200i, 261\ni = [1, N ] for type 2. This can be conveniently represented 262\ngraphically for a 2-D case as shown in Fig. 3(a). In Fig. 3(b), 263\nfor the sake of comparison, we illustrate exactly the same 264\ndata as in Fig. 3(a) but instead of the circles indicating the 265\nareas where new rules of type 1 and 2 will be formed, 266\nwe indicate the zones of influence of the existing clusters 267\nby ellipsoids (one can also notice the variable radii of the 268\nclusters). AQ21269\nIE\nEE\nPr\noo\nf\nANGELOV: FUZZILY CONNECTED MULTIMODEL SYSTEMS EVOLVING AUTONOMOUSLY FROM DATA STREAMS 5\nTABLE I\nCOMPUTATIONAL COMPLEXITY\nIt is important to note that Condition A only requires the270\nfollowing to be known:271\n1) current data point zk;272\n2) previous focal points zi\u2217(k);273\n3) the global mean z.274\nIt codifies, however, the exact information about the density at275\nthe current point, although all previous points are not memo-276\nrized. All other existing approaches for evolving NFS do not277\nuse the density information of all previous points except eTS278\n[12], [19], [28] which is computationally less efficient than the279\nproposed approach, as can be seen in Table I. The compu-280\ntational complexity of the proposed simpl_e_Clustering281\nmethod in the structure stage is of order O(1) because it only282\nrequires an online update of a single value\u2014the global mean283\nz. In terms of memory, its complexity is of order O(N) since284\nit requires memorizing previous N focal points plus the global285\nmean. For comparison, eClustering (the approach used in eTS286\nto generate the system structure) has computational complexity287\nof order O(N) that includes the updating of potentials of the288\nexisting N rules and calculating the potential of the new point.289\nIts memory requirements are of order O(N + n) for centers290\nand accumulated variables needed to recursively calculate the291\npotential [12], [19], [28]. Note that k \u0006 N , k \u0006 n, while N292\nand n are comparable.293\nAs opposed to the clustering approach used in eTS [12], [19],294\n[28], there is no need to calculate the data density\/potential,295\nneither the density at the focal points nor to keep them in the296\nmemory and to update them. At the same time, differently from297\nall other published approaches, known to the author, the density298\ninformation from all previous data samples is being used,299\nrecursively accumulated and compared to all previous focal300\npoints, as detailed in the Appendix. To update the global mean301\nper input [see equation (2)] is much easier and computationally302\ncheaper than to calculate and update densities\/potentials.303\nMoreover, it can also be proven that the global mean is the304\npoint with the highest density\/potential\u2014see Lemma 3. Thus,305\nprinciple P1 alone (selecting data points with highest den-306\nsity\/potential as candidates to form new centers) is not enough307\nto ensure coverage of the data space. This was emphasized on308\nan empirical basis in [21].309\nCondition B can be formulated as \u201cthe new point that is310\naccepted to be a new focal point based on Condition A. It should311\nnot have a high degree of membership to any of the already312\nexisting rules (should not be very close to any of the previously313\nexisting focal points, as shown in Fig. 4)AQ22 314\n\u03bcN+1j\n(\nxi\u2217j\n)\n> \u03b50 \u2200j; j=[1, n] i\u2217=\nN\nargmin\ni=1\n\u2225\u2225xj(k)\u2212xij\u2225\u22252\n(10)\nwhere \u03b50 is a level that represents a high degree of member- 315\nship (closeness to the center) and a suggested value \u03b50 = 0.9 316\nof 90%. AQ23317\nThis condition leads to a simpler structure compared to other 318\nmethods which often require the so-called \u201cpruning,\u201d which 319\nwill be discussed later. AQ24320\nIV. SELF-MONITORING THE QUALITY OF CLUSTERS IN 321\nsimpl_e_Clustering 322\nSimpl_eClusteringis a method that self-monitors and 323\nself-regulates the quality of the generated clusters. In 324\nsimpl_eClustering, only cluster centers and the mean 325\nvalue are kept in the memory that are N + 1 values of 326\ndimension (n + m), while all other data points are dis- 327\ncarded. The question arises \u201chow well do these centers 328\nrepresent the data that were discarded from the memory?\u201d 329\nSimpl_eClustering addresses this generic issue of evolv- 330\ning online system design that is usually ignored by other 331\nexisting methods by monitoring and reacting online to certain 332\nproperties of the clusters (respectively, rules) that are formed. 333\nThe value of the spread (zone of influence) of the clusters 334\nr can be updated per input j online by learning the data 335\ndistribution and variance [24] 336(\nrij(k)\n)2= \u03b2 (rij(k\u22121))2+(1\u2212\u03b2) (\u03c3ij(k))2 , rij(1)=0.5\n(11)\nwhere \u03b2 is the learning step (recommended value is 0.5); 337\n\u03c3ij(k) is the scatter\/spread, which is determined by (\u03c3ij(k))2 = 338\n(1\/Si(k))\n\u2211Si(k)\nl=1 \u2016zi\u2217 \u2212 zl\u20162; \u03c3ij(1) = 1; Si is the support of 339\nthe ith cluster; and i = [1, N ], which is updated by 340\nIF\n(\nl =\nN\nargmin\n\u2225\u2225z(k)\u2212 zi\u2217\u2225\u2225\ni=1\n)\nTHEN\n(\nSl(k + 1) = Sl(k) + 1\n)\n.\nDue to the incremental nature of the approach, the relevance 341\nof a cluster (respectively, fuzzy rule) may change. One of the 342\nmeasures of the relevance of the rule is the utilityU which 343\naccumulates the weight of the rule contributions to the overall 344\noutput during the life of the rule (from the moment when this 345\nrule was generated until the current time instant) [23] 346\nU l(k) =\n1\nk \u2212 tl(k)\ntl(k)\u2211\nl=1\n\u03bbl. (12)\nIt is a measure of importance of the respective fuzzy rule 347\ncompared to the other rules [comparison is hidden in the relative 348\nnature of \u03bb, [see (5)]. 349\nA condition to remove rules that have low utility can be 350\nformulated as: 351\nCondition C: 352\nIF\n(\nU l(k) < \u03b51\n)\nTHEN (\u03bbl \u2190 0) (13)\nwhere \u03b51 denotes tolerance (recommended values 3%\u201310%). 353\nIn simpl_eTS+, the cluster\/rule utility is used for real-time 354\nmanagement of the structure of the system as summarized in the 355\nfollowing algorithm: 356\nIE\nEE\nPr\noo\nf\n6 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART B: CYBERNETICS\nAs a result of this procedure, the antecedent part of the 357\nsimpl_eTS+ system is generated from the data stream as 358\nRi IF\n(\nx1 is xi\u22171\n)\nAND\n(\nx2 is xi\u22172\n)\n. . .\n(\nxn is xi\u2217n\n)\n, i=[1, N ].\n(14)\nIt can be directly used for clustering data and various applica- 359\ntions, e.g., in robotics [27] and user behavior modeling [26]. 360\nIt can also be used as a basis of FRB or NFS for: 361\n1) online prediction; 362\n2) classification; 363\n3) control. 364\nV. SELF-LEARNING simpl_eTS+ 365\nA. Learning Consequents 366\nOnce the antecedent part of the fuzzy model is determined 367\nand fixed, parameters of the consequent part \u03c0i can be learned 368\nusing a recursive least square (RLS) estimation [9]. The real- 369\ntime algorithm performs both tasks (clustering and parameter 370\nestimation) at the same time instant (per data point). The 371\nantecedent part of the rules (layers 1\u20133 of the simpl_eTS+, 372\nFig. 1) can be determined in a fully unsupervised manner, while 373\nthe consequent part (layers 4\u20135) requires a supervised feedback. 374\nThe supervision is by error feedback which guarantees opti- 375\nmality (subject to fixed rule-base structure) of the consequent 376\nparameters. The overall output of the simpl_eTS+ system 377\ngiven by (1)\u2013(7) can be rewritten in a vector form as follows: 378\ny = \u03c8T\u03b8 (15)\nwhere \u03b8 = [(\u03c01)T , (\u03c02)T , . . . , (\u03c0N )T ]T is a vector formed by 379\nthe local model parameters, \u03c8 = [\u03bb1xTe , . . . , \u03bbNxTe ]T is a vec- 380\ntor of the inputs that are weighted by the normalized activation 381\nlevels of the rules \u03bbi, i = [1, N ] for the first-order TS model 382\n(3), and \u03c8 = [\u03bb1, \u03bb2, . . . , \u03bbN ]T for the sM (4). 383\nFor a given data point x(k), the (globally) optimal in LS 384\nsense solution \u03b8\u02c6k that minimizes the following cost function: 385\n(Y \u2212\u03a8T\u03b8)T (Y \u2212\u03a8T\u03b8) \u2192 min (16)\ncan be found by applying the weighted RLS (wRLS) with 386\nforgetting [9], [12] 387\n\u03b8\u02c6(k) = \u02c6\u03b8(k\u22121)+C(k)\u03c8(k)\n(\ny(k)\u2212\u03c8T(k) \u02c6\u03b8(k\u22121)\n)\n(17)\n\u03b3C(k) =C(k \u2212 1)\u2212 C(k \u2212 1)\u03c8(k)\u03c8\nT(k)C(k \u2212 1)\n\u03b3 + \u03c8T(k)C(k \u2212 1)\u03c8(k) (18)\ninitialized by \u03b8\u02c6(1) = 0; C(1) = \u03a9I , where k = 2, 3, . . .; C is 388\nan Nn\u00d7Nn covariance matrix; \u03a9 is a large positive number; 389\nI is the identity matrix; \u03b3 denotes the gradual forgetting factor 390\n\u03b3 (0.9 < \u03b3 \u2264 1); Y , \u03a8, and \u03b8 are the diagonal matrices with 391\ny(k), \u03c8(k), and \u03b8(k) in their main diagonal, respectively. 392\nWe call this locally optimal fuzzily weighted RLS learning 393\nwith forgetting (wRLSf) algorithm. In this expression, the RLS 394\nis fuzzily weighted through the activation levels which are not 395\nthe same as the conventional weighted RLS [9] that is directly 396\napplicable under the assumption that the model in (1)\u2013(4) 397\nIE\nEE\nPr\noo\nf\nANGELOV: FUZZILY CONNECTED MULTIMODEL SYSTEMS EVOLVING AUTONOMOUSLY FROM DATA STREAMS 7\nhas a fixed structure. Under this assumption, the optimizationAQ25 398\nproblem (16) is linear in parameters. The concept of evolving399\nsystems, however, assumes a gradually evolving model struc-400\nture. As a result, the activation level of the fuzzy rules \u03bbi401\nchanges. These changes (although infrequent and gradual in the402\nsense that only one out of N rules is affected and only when a403\nnew rule\/cluster is created) have retrospective effect (they affect404\npreviously calculated activation levels \u03bbij , where i = [1, N ] and405\nj = [1, k \u2212 1]).406\nLocally optimal in LS sense solution of the online parameter407\nidentification task is as follows [12]:AQ26 408\n\u02c6\u03c0i(k) = \u02c6\u03c0i(k \u2212 1) + ci(k)xe(k)\u03bbi (x(k))\n\u00d7\n(\nyl(k)\u2212 xTe (k) \u02c6\u03c0i(k \u2212 1)\n)\n(19)\n\u03b3ci(k) = ci(k \u2212 1)\u2212 \u03bb\ni (x(k)) ci(k\u22121)xe(k)xTe (k)ci(k\u22121)\n\u03b3 + \u03bbi (x(k))xTe (k)ci(k \u2212 1)xe(k)\n.\n(20)\nInitialized by \u02c6\u03c0i(1) = 0; ci(1) = \u03a9I; l = 1, 2, . . . ,m; k =409\n2, 3, . . . will minimize the following cost function:410\nN\u2211\ni=1\n(Y \u2212XT\u03c0i)T\u039bi(Y \u2212XT\u03c0i) (21)\nwhere \u039b and X are the diagonal matrices with \u03bb and xe,411\nrespectively, in their main diagonals.412\nThe local wRLSf is significantly less affected by this dis-413\nturbance of the theoretical optimality of the RLS condition as414\ncompared to the global wRLSf. In addition, it is significantly415\nless computationally complex.416\nB. Online Input Selection417\nSelecting the most informative inputs\/features is a critical418\ntask that is usually associated with preprocessing stages [2]419\nand is addressed by approaches such as principal component420\nanalysis [2], GP [7], etc. These approaches, however, require aAQ27AQ28 421\nbatch set of data and a fixed model structure.422\nIn the previous section, and, to the best of our knowledge,423\nin all previous research in online system identification, indeed,424\nit was assumed that the dimensionality of the input\/features425\nvector n is predefined for each problem at hand. Here, we426\npresent an approach which breaks this assumption by gradually427\nremoving inputs\/features that do not contribute to the output(s)428\nbased on online estimation of the sensitivity of the output(s)429\nto the inputs. Because in TS fuzzy systems [1], the output is430\nlocally linear; the sensitivity analysis is reduced to the analysis431\nof the consequent parameters. The importance of each input\/AQ29 432\nfeature can be evaluated by the ratio of the accumulated sum of433\nthe consequent parameters for the specific jth input\/feature in434\nrespect to all n inputs\/features [23]435\n\u03c9ij(k) = Tij(k)\n\/ n\u2211\nr=1\nTir(k) i = [1, N ] j = [1, n]\n(22)\nFig. 6. Importance of the input variables estimated online (Condition D) based\non real industrial data as described in case study B.\nwhere Tij(k) =\n\u2211k\nl=1 |aij(l)| denotes the accumulated sum of 436\nparameter values of the ith rule. 437\nSince the inputs, outputs, and the internal variables of the 438\nsimpl_eTS+ system are standardized, they are comparable 439\nbetween each other. The value of the weight can be used for 440\nthe gradual removal of inputs\/features that contribute little to 441\nthe overall output. Then, the inputs\/features j\u2217 that do not 442\ncontribute significantly to the output can be removed at the next 443\ntime instant, simplifying from the system structure 444\nCondition D: 445\n\u2203j\u2217| \u03c9ij\u2217(k) < \u03b52 nmax\nr=1\n\u03c9ir(k), i = [1, N ] (23)\nwhere \u03b52 denotes the tolerable minimum weight of an 446\ninput\/feature\u2014suggested value is from 3% to 5%. 447\nThis approach provides a tool for monitoring and analyzing 448\nthe contribution of each input variable\/feature online. Removal 449\nof features\/inputs, however, should be used with care because, 450\nas for any online and incremental approach, simpl_eTS+ is 451\norder dependant, and therefore, removal is recommended on the 452\nbasis of longer period of monitoring\/observation. 453\nCondition D is in terms of the proportion which the weight of 454\na certain input\/feature represents from the maximum of the ac- 455\ncumulated sum of parameters (see Fig. 6 for an example). If this 456\nproportion is negligible (less than \u03b52), then this input\/feature 457\ncan be removed without significantly affecting the output. This 458\ntechnique has high practical importance because, very often, in 459\na real environment, there are many measurable variables that 460\ninfluence the output. 461\nThis approach is prototype-based in that some of the data 462\npoints are used as prototypes (focal points). A number of pre- 463\nvious algorithms that concern (neuro-) fuzzy system learning 464\nuse mean-based clustering [4], [5] (the centers are located at 465\nthe mean, which, in general, do not coincide with any data 466\npoint). The clustering approaches used in [4], [5], [8], [14], 467\n[15], etc., are threshold-based, and the result highly depends 468\non the selection of appropriate threshold(s). Therefore, these 469\napproaches form a large number of clusters that later has to be 470\n\u201cpruned\u201d [5]. 471\nCombining simpl_e_Clustering for antecedents\u2019 struc- 472\nture identification (Section III) with the wRLSf learning algo- 473\nrithm (Section V-A) and online input selection (Section V-B), 474\nIE\nEE\nPr\noo\nf\n8 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART B: CYBERNETICS\nthe following simple procedure for evolving simpl_eTS+475\nfrom data streams can be formulated:476\nVI. EXPERIMENTAL RESULTS AND ANALYSIS477\nA number of experiments were carried out on data streams478\nfrom a synthetic and two real industrial processes. The synthetic479\ndata set is a widely used benchmark for predictive models and is480\na very challenging chaotic time series and was used in order to481\ncompare the results of using simpl_eTS+ with the results of482\nother published approaches on the same data set. Both industrial483\ndata streams have noise and are provided to simpl_eTS+ and484\nother algorithms without any preprocessing. Both data streams485\nare collected from a large number of measurements (candidate486\ninput variables\u2014180 and 23, respectively), and these put the487\nalgorithms to a test that is close to a real world situation. The488\nexperiments aim to demonstrate that simpl_eTS+ is able489\nto autonomously self-develop, self-monitor, and self-improve,490\nand in this sense, the experiments are very challenging. In491\naddition, the second industrial data stream contains a sudden492\nchange around sample 1300 which was caused by replacing493\nthe catalyzer in the process run by The Dow Chemical Com-494\npany. This real life change is an excellent challenge which495\nonce addressed successfully demonstrates the ability to self-496\nadapt model structure according to dynamically changing data497\nstreams.498\nEach data stream was tested with several settings of the499\nsimpl_eTS+ in order to analyze the effects and perfor-500\nmance of the improvements introduced in this paper; the501\nfirst setting of simpl_eTS+ included the removal of rules502\n(Conditions A\u2013C); the second setting was with simpl_eTS+503\nincluding online input selection (using Conditions A, B, D); and504\nthe last setting is of the full simpl_eTS+ (all Conditions A\u2013D505\nbeing active).506\nA. Synthetic Data Sets507\nFirst, we compare the performance of simpl_eTS+ with508\nother published approaches on the widely used time series509\ngenerated from the Mackey\u2013Glass differential delay equation510\n\u2202\u02d9z\n\u2202t\n=\n0.2z(t\u2212 T )\n1 + z10(t\u2212 T ) \u2212 0.1z(t).\nThe following experiment was conducted: 3000 data points,511\nfor t = [201, 3200], are extracted from the time series and512\nTABLE II\nRESULTS FOR MACKEY\u2013GLASS TEST\nused as training data and 500 data points, for t = [5001, 5500], 513\nare used as validation data. The learning was stopped during 514\nthe validation to allow comparability with other published 515\napproaches. The aim is to predict the value z(t + 85) based 516\non four previous values, namely, z(t), z(t\u2212 6), z(t\u2212 12), and 517\nz(t\u2212 18). The precision is measured using nondimensional AQ30518\nerror index (NDEI) defined as the ratio of the root mean square 519\nerror over the standard deviation of the target data. 520\nThe model complexity is measured by the number of fuzzy 521\nrules generated. The results of the comparison of the proposed 522\napproach with several published results (see Table II) demon- 523\nstrate the superiority of simpl_eTS+ that offers the simplest 524\nstructure (number of fuzzy rules and number of inputs) with 525\nvery high precision. 526\nThe best result in terms of lower error NDEI = 0.316 is 527\nregistered when Conditions A, B, and D are active but not 528\nCondition C. This means that rules with low utility are not 529\nbeing removed from the rule base (in this case, the rule base 530\nhas 20 rules and all four inputs). Activating Condition C AQ31531\n(in addition to A and B) leads to removing two not much used 532\n(with low utility) rules. Adding to that, Condition D allows 533\nfurther removal of one input\/feature that contributes least to 534\nthe overall prediction. The structure of the system gets much 535\nsimpler (15 rules and 3 inputs only) with some deterioration in 536\nterms of the error (NDEI = 0.374\/5) which still compares fa- 537\nvorably with the alternative approaches taking into account that 538\nthey provide similar error level but use much more complicated 539\nrule bases. 540\nB. NOxNOx Emission Case Study 541\nThis data set was collected from car engines (courtesy of 542\nDr. E. Lughofer, Johannes Kepler University Linz, Linz, AQ32543\nAustria) to estimate the NOx content in the emissions that they 544\nproduce based on the variables that are easy to measure, such 545\nas pressure in the cylinders, engine torque, rotation speed, etc. 546\n[17]. In total, as much as 180 input variables are considered 547\nas potential inputs (these also include the physical variables 548\ndescribed previously taken at different time instants, i.e., with 549\ndifferent time delays). In [17], we used offline input variable 550\nselection method and common knowledge to determine the best 551\nfive inputs. Instead, in this paper, we start processing all the data 552\ninputs available and select the best subset automatically online. 553\nWhen we apply Condition D, the end result is a fuzzy model 554\nwith 101 inputs and 36 fuzzy rules; when we add all conditions 555\nIE\nEE\nPr\noo\nf\nANGELOV: FUZZILY CONNECTED MULTIMODEL SYSTEMS EVOLVING AUTONOMOUSLY FROM DATA STREAMS 9\nTABLE III\nRESULTS FOR NOx CAR EMISSION ANALYSIS\nTABLE IV\nPREDICTING PROPYLENE CONTENT OF DISTILLATION\n(A\u2013D), the model evolves to 13 fuzzy rules with seven inputs556\n(fuzzy sets) (see the results in Table III).557\nMoreover, the prediction error is lower with the model that558\nhas inputs selected automatically. In this experiment, again, a559\nmuch simpler structure rule base is achieved when all condi-560\ntions (A\u2013D) are applied, but if the error is the only criteria561\n(having a larger and much more complex structure), then con-562\nditions A, B, and D are most effective.563\nC. Propylene Case Study564\nThe propylene data set is collected from a chemical distilla-565\ntion process run at The Dow Chemical Company, U.S. (courtesy566\nof Dr. A. Kordon, [3]). The data set consists of 3000 readings567\nfrom 23 \u201chard\u201d sensors. These are used to model the propylene568\ncontent in the product output from the distillation. Some of the569\ninputs proved to be irrelevant to the model and, thus, bring570\nnoise. Therefore, the input selection is very relevant for this571\nparticular problem.572\nFrom Table IV, it is seen that using simpl_eTS+, a com-573\npact fuzzy model of seven fuzzy rules and two inputs (fuzzy574\nsets per rule) can be evolved online which also provides the575\nbest precision (see also Fig. 7).576\nThis demonstrates that highly compact, transparent, and577\ninterpretable models can be designed from data streams online578\nusing simpl_eTS+.AQ33 579\nFinal Rule base for propylene:580\nR1: IF (x1is 24.6) AND (x2is 26.3)581\nTHEN(y = \u22120.039 + x1 \u2212 0.324x2)582\nR2: IF (x1is 39.0) AND (x2is 43.5)583\nTHEN(y = \u22120.615 + 4.77x1 \u2212 0.340x2)584\nFig. 7. Predicting propylene content in the product using simpl_eTS+.\nThe horizontal axis represents the data samples taken every 15 min; the\nvertical axis represents the normalized values of the output y. A significant\ntechnological change takes place around sample 1300.\nR3: IF (x1is 46.2) AND (x2is 49.5) 585\nTHEN(y = \u22120.679 + 1.090x1 + 0.450x2) 586\nR4: IF (x1is 45.9) AND (x2is 49.9) 587\nTHEN(y = \u22121.340 + 5.570x1 \u2212 3.320x2) 588\nR5: IF (x1is 36.2) AND (x2is 43.5) 589\nTHEN(y = \u22120.002 + 0.320x1 \u2212 0.065x2) 590\nR6IF (x1is 31.6) AND (x2is 38.7) 591\nTHEN(y = \u22120.007 + 0.366x1 \u2212 0.129x2) 592\nR7IF (x1 is 40.6) AND (x2 is 39.5) 593\nTHEN (y = \u22120.527 + 0.406x1 \u2212 0.345x2Z) 594\nVII. CONCLUSION 595\nThis paper has introduced a novel computationally and 596\nconceptually simple scheme for a joint structure and pa- 597\nrameter identification of evolving NFS of a generic type 598\n(Fig. 1). The proposed new simpl_eTS+ approach also 599\nevolves the inputs of the multiple inputs and multiple out- 600\nput structure, removing rules\/ neurons based on their utility. AQ34601\nThe simpl_e_Clustering approach that is used for system 602\nstructure identification and evolution does not use directly the 603\ndata density\/potential (as it is in the eTS approach) and, thus, 604\nreduces the computational efforts by an order of magnitude 605\n(Table I). It differs from all other existing techniques of this 606\ntype by taking into account the accumulated proximity (den- 607\nsity) information without memorizing it directly. The proposed 608\nmethod allows complete autonomous knowledge extraction 609\nfrom streaming data (including model input selection online). 610\nThe proposed approach takes into account the shift in the data 611\nstream by evolving the structure of the system. Compared to the 612\nother well-known concepts of adaptive systems, the proposed 613\none allows a much higher level of flexibility and achieves better 614\nresults on real-life data. The root of its better efficiency is in 615\nthe simpler model structure achieved based on data density 616\ndetermined recursively (recursive noniterative manner of cal- 617\nculations is the key why the approach is much faster); the better 618\nprecision is achieved due to flexibility (model structure evolves 619\nand follows as a reaction to the dynamically changing data 620\npattern as opposed to averaging all data or using less efficient 621\nclustering techniques or assumptions). AQ35622\nIE\nEE\nPr\noo\nf\n10 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART B: CYBERNETICS\nThe main contributions of this paper are:623\n1) the novel and computationally simple approach to data624\nspace partitioning by recursive evolving clustering based625\non the relative position to the mean of the overall data626\nsimpl_e_Clustering;627\n2) the learning technique for online structure evolution as a628\nreaction to the shifts in the data distribution;629\n3) the method for online input\/feature selection630\n(Condition D);631\n4) the method for system structure simplification based on632\nutility (Condition C);633\n5) the novel graphical illustration of the spatial-temporal634\nevolution of the data stream (Fig. 3).635\nThe application domain for this computationally efficient636\ntechnique ranges from simple clustering-based techniques for637\npattern recognition, image segmentation, vector quantization,638\netc., to more general modeling, prognostics, classification, and639\ntime-series prediction problems in various application areas,640\ne.g., intelligent sensors, mobile robotics, advanced manufactur-641\ning processes, sensor networks, etc. The proposed approach can642\nbe used as a basis to build adaptive (with evolving structure)643\nself-calibrating inferential (soft) sensors for chemical and oil644\nindustries [3] and can also be implemented as an embedded645\nsystem.646\nAPPENDIX647\nA. Lemma 1648\nIn eClustering [19], the main condition to form new clusters649\nand respectively new fuzzy rules was described as650\nCondition A_old:651\nIF\n(\nDk(zk)>\nN\nmax\ni=1\nDk(zi\u2217)\n)\nOR\n(\nDk(zk)<\nN\nmin\ni=1\nDk(zi\u2217)\n)\n.\n(A1)\nStarting from Condition A_old, it can be proven that a signifi-652\ncantly simpler Condition A can be derived that is based on the653\ndensity increment \u03b4(k), i.e., (8) is identical to Condition A_old654\n(used in eTS [12], [19], [28]), but it does not require the density655\nto be calculated and updated for every new data point.656\nProof [23]: One can express the partial density increment657\nas a difference between the densities of the following:658\n1) the new data point zk calculated at the time instant k,659\nDk(zk);660\n2) the density of the cluster\/rule center calculated at the 661\nsame time instant Dk(zi\u2217) 662\n\u0394i\u2217k = Dk(zk)\u2212Dk(zi\u2217). (A2)\nThe meaning of the partial density increment is \u201chow much 663\nthe density will change if we measure it in the new data point 664\ninstead of the previous i\u2217th cluster center (focal point of a 665\nrule).\u201d We can express this quantity from the definition of the 666\ndensity as a Cauchy function [12] 667\nD(z) =\n1\n1 + 1k\u22121\nk\u2211\ni=1\n\u2016z \u2212 zi\u20162\n(A3)\nwhich leads to (A4), shown at the bottom of the page, 668\nwhere Li = {1, 2, . . . , i \u2217 \u22121, i \u2217+1, . . . , k \u2212 1} denotes a set 669\nof (k \u2212 2) indices (all indices from 1 to (k \u2212 1) excluding the 670\nindex i\u2217. 671\nReorganizing (A4) gives 672\n\u0394i\u2217k =\n(k \u2212 2)\nn+m\u2211\nj=1\n{((\nzi\u2217j\n)2\u2212z2jk)+2 (zjk\u2212zi\u2217j ) \u2211\nl\u2208Li\nzjl\n}\n(k \u2212 1) denom1 \u2217 denom2\n(A5)\nwhere 673\ndenom1 =1 +\n\u2211\nl\u2208Li\nn+m\u2211\nj=1\n{\nz2jk \u2212 2zjkzjl + z2jl\n}\nk \u2212 1\n+\nn+m\u2211\nj=1\n{\nz2jk \u2212 2zjkzi\u2217j +\n(\nzi\u2217j\n)2}\nk \u2212 1\ndenom2 =1 +\n\u2211\nl\u2208Li\nn+m\u2211\nj=1\n{(\nzi\u2217j\n)2 \u2212 2zi\u2217j zji + z2jl}\nk \u2212 1\n+\nn+m\u2211\nj=1\n{\nz2jk \u2212 2zjkzi\u2217j +\n(\nzi\u2217j\n)2}\nk \u2212 1 .\nEquation (A5) can be further simplified as follows: 674\n\u0394i\u2217k =\n(k \u2212 2)\n{\nn+m\u2211\nj=1\n{(\nzi\u2217j\n)2 \u2212 z2jk + 2 (zjk \u2212 zi\u2217j ) zi\u2217j }\n}\n(k \u2212 1) denom1 \u2217 denom2\n(A6)\n\u0394i\u2217k =\n1\n1 + 1k\u22121\n{\u2211\nl\u2208Li\nn+m\u2211\nj=1\n(\nz2jk \u2212 2zjkzjl + z2jl\n)\n+\nn+m\u2211\nj=1\n(\nz2jk \u2212 2zjkzi\u2217j +\n(\nzi\u2217j\n)2)}\n\u2212 1\n1 + 1k\u22121\n{\u2211\nl\u2208Li\nn+m\u2211\nj=1\n{(\nzi\u2217j\n)2 \u2212 2zi\u2217j zjl + z2jl}+ n+m\u2211\nj=1\n{\nz2jk \u2212 2zjkzi\u2217j +\n(\nzi\u2217j\n)2}} (A4)\nIE\nEE\nPr\noo\nf\nANGELOV: FUZZILY CONNECTED MULTIMODEL SYSTEMS EVOLVING AUTONOMOUSLY FROM DATA STREAMS 11\nwhere zi\u2217j = (1\/(k \u2212 2))\n\u2211\nl\u2208Li z\nl\nj is the mean of all points675\nexcept the i\u2217th cluster\/rule center and the last (k \u2212 1)th point.676\nSince both denominators (denom1 and denom2) express677\nsums of distances, they are positive by definition. Similarly,678\n(k \u2212 2)\/(k \u2212 1) is also positive. Therefore, the sign of \u03b3i\u2217k679\ndetermines the sign of the density increment \u0394i\u2217k , where680\n\u03b3i\u2217jk =\n(\nzi\u2217j\n)2 \u2212 z2jk + 2 (zjk \u2212 zi\u2217j ) zi\u2217j . (A7)\nIf the sign of the density increment\u0394i\u2217k is the same for each681\nrule, then the new data point brings a spatial innovation. OneAQ36 682\ncan judge this by observing the sign of the \u2211n+mj=1 \u03b3i\u2217jk. If it is683\nthe same for all previously existing (N) rules, then the density684\nincrement takes place in respect to each previously existing685\nrule. The overall sign of the density increment in respect to all686\npreviously existing N rules can be calculated by687\n\u03b4k =\n\u2223\u2223\u2223\u2223\u2223\u2223\nN\u2211\ni=1\nsign\nn+m\u2211\nj=1\n\u03b3i\u2217jk\n\u2223\u2223\u2223\u2223\u2223\u2223 . (A8)\nThe mean zi\u2217j used in (A6) can be calculated by subtracting688\nthe value of the i\u2217th center from the mean of all points up to689\n(k \u2212 1)th690\nzi\u2217j =\n1\n(k \u2212 2)\n\u2211\nl\u2208Li\nzjl =\n1\n(k \u2212 2)\n(\nk\u22121\u2211\ni=1\nzjl \u2212 zi\u2217j\n)\n=\n(k \u2212 1)\n(k \u2212 2)zj(k\u22121) \u2212\n1\n(k \u2212 2)z\ni\u2217\nj (A9)\nwhere zj(k\u22121) = (1\/(k \u2212 1))\n\u2211k\u22121\nl=1 zjl is the mean of all points691\nup to the (k \u2212 1)th.692\nIt can easily be calculated recursively693\nzj(k\u22121) =\n(k \u2212 2)\n(k \u2212 1)zj(k\u22122) +\n1\n(k \u2212 1)zj(k\u22121).\nIf the new data point does not bring density increment to all of694\nthe existing clusters\/rules695\n0 < \u03b4k < N (A10)\nthen it can be interpolated by the existing rules represented by696\nthe existing centers [see the area between the two concentric697\ncircles in Fig. 3(a)]. Thus, in this case, we do not change the698\ncluster structure.699\nIf it does bring a density increment to all of the previously700\nexisting rules, however701\n\u03b4k = N (A11)\na new cluster\/rule is formed around this point. \u0002702\nB. Lemma 2703\nThe density\/potential D measured at the global mean z is the704\nmaximal density possible705\nD(z) =\nk\nmax\ni=1\nD(zi) (A12)\nwhere z denotes the global mean defined as z = (1\/k)\n\u2211k\ni=1 zi. 706\nProof: First, we can rewrite (A12) as D(z) > D(zi);\u2200i = 707\n1, 2, . . . , k. 708\nThen, we can assume that there exists a point which violates 709\nthis condition \u2203j|D(zj) > D(z), and finally, we can prove that 710\nthis assumption is wrong (that it cannot hold). For any point 711\nother than the global mean z\u2217, we have 712\nD(z\u2217) =\n1\n1 + 1k\u22121\nk\u2211\ni=1\n\u2016z\u2217 \u2212 zi\u20162\n. (A13)\nNow, if we assume that D(z\u2217) > D(z), we only need to prove 713\nthat this is impossible 714\n1\n1+ 1k\u22121\nk\u2211\ni=1\n\u2016z\u2217\u2212zi\u20162\n>\n1\n1 + 1k\u22121\nk\u2211\ni=1\n\u2016z \u2212 zi\u20162\n(A14)\nk\u2211\ni=1\nn+m\u2211\nj=1\n(\nz\u2217j \u2212 zij\n)2\n<\nk\u2211\ni=1\nn+m\u2211\nj=1\n(zj \u2212 zij)2 (A15)\nk\nn+m\u2211\nj=1\n(\nz\u2217j\n)2\u22122n+m\u2211\nj=1\nz\u2217j\nk\u2211\ni=1\nzij +\nn+m\u2211\nj=1\nk\u2211\ni=1\n(zij)2\n<k\nn+m\u2211\nj=1\n(zj)2\u22122\nn+m\u2211\nj=1\nzj\nk\u2211\ni=1\nzij\n+\nn+m\u2211\nj=1\nk\u2211\ni=1\n(zij)2 (A16)\nk\nn+m\u2211\nj=1\n(\nz\u2217j\n)2\u22122n+m\u2211\nj=1\nz\u2217j\nk\u2211\ni=1\nzij <k\nn+m\u2211\nj=1\n(zj)2 \u2212 2\nn+m\u2211\nj=1\nzj\nk\u2211\ni=1\nzij .\n(A17)\nNow, from the definition of the global mean, we can write 715\u2211k\ni=1 zi = kz or respectively\n\u2211k\ni=1 zij = kzj . Taking this into 716\naccount, we can rewrite (A17) into 717\nk\nn+m\u2211\nj=1\n(\nz\u2217j\n)2 \u2212 2k n+m\u2211\nj=1\nz\u2217jzj < k\nn+m\u2211\nj=1\n(zj)2 \u2212 2k\nn+m\u2211\nj=1\nzjzj .\n(A18)\nBy reorganizing, we got 718\nn+m\u2211\nj=1\n(\nz\u2217j\n)2 \u2212 2 n+m\u2211\nj=1\nzjz\n\u2217\nj \u2212\nn+m\u2211\nj=1\n(zj)2 + 2\nn+m\u2211\nj=1\nzjzj < 0.\n(A19)\nSimplifying the last two terms, we got 719\nn+m\u2211\nj=1\n(\nz\u2217j\n)2 \u2212 2 n+m\u2211\nj=1\nzjz\n\u2217\nj +\nn+m\u2211\nj=1\n(zj)2 < 0 (A20)\nIE\nEE\nPr\noo\nf\n12 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART B: CYBERNETICS\nwhich can be, finally, rewritten as720\nn+m\u2211\nj=1\n(\nz\u2217j \u2212 zj\n)2\n< 0. (A21)\nObviously, this can never be satisfied, and our initial assumption721\nwas wrong. Thus, D(z) > D(zi);\u2200i = 1, 2, . . . , k \u0002722\nC. Lemma 3723\nThe mean of the standardized data tends asymptotically to724\nzero with the number of data tending to infinity (see Fig. 9)AQ37 725\nlim\nk\u2192\u221e\nzst \u2192 0. (A22)\nProof: The mean of the standardized data is derived by726\napplying both the mean and standardization operations. Let usAQ38 727\nstart with the offline case728\nzst =\n1\nk\nk\u2211\ni=1\nzi \u2212 z\n\u03c3\n=\n1\nk\nk\u2211\ni=1\nzi\n\u03c3\n\u2212 1\nk\nk\u2211\ni=1\nz\n\u03c3\n=\n1\n\u03c3\n(z \u2212 z) = 0.\n(A23)\nIn the online case, we can prove that when k \u2192\u221e, the same729\ncondition (A22) holds but in asymptotical sense730\nzk+1 =\n1\nk + 1\nk+1\u2211\ni=1\nzi =\nk\nk + 1\nzk +\nzk+1\nk + 1\n. (A24)\nHowever,731\nlim\nk\u2192\u221e\nk\nk + 1\n=1 (A25a)\nlim\nk\u2192\u221e\nxk+1\nk + 1\n=0 for bounded zk+1. (A25b)\nCombining (A24) and (A25a) and (A25b), we have732\nlim\nk\u2192\u221e\nzk+1 = zk (A26)\nand finally, this leads to limk\u2192\u221e zst = 0 (see Fig. 5). \u0002733\nACKNOWLEDGMENT734\nThe author would like to thank Dr. N. Pal for the comments735\nhe made to the manuscript from 2009 to the beginning of 2010.AQ39 736\nREFERENCES737\n[1] T. Takagi and M. Sugeno, \u201cFuzzy identification of systems and its ap-738\nplication to modeling and control,\u201d IEEE Trans. Syst., Man, Cybern.,739\nvol. SMC-15, no. 1, pp. 116\u2013132, Feb. 1985.740\n[2] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statis-741\ntical Learning: Data Mining, Inference and Prediction. Heidelberg,742\nGermany: Springer-Verlag, 2001.743\n[3] P. Angelov and A. Kordon, \u201cAdaptive inferential sensors based on evolv-744\ning fuzzy models: An industrial case study,\u201d IEEE Trans. Syst., Man,745\nCybern. B, Cybern., vol. 40, no. 2, pp. 529\u2013539, Apr. 2010.746\n[4] G. Leng, T. M. McGuinnity, and G. Prasad, \u201cAn approach for on-747\nline extraction of fuzzy rules using a self-organising neural network,\u201d748\nFuzzy Sets Syst., vol. 150, no. 2, pp. 211\u2013243, Mar. 2005.749\n[5] N. Sundararajan, H.-J. Rong, G.-B. Huang, and P. Saratchandran, \u201cSe- 750\nquential adaptive fuzzy inference systems for non-linear systems identifi- 751\ncation and prediction,\u201d Fuzzy Sets Syst., vol. 157, no. 9, pp. 1260\u20131275, 752\nMay 2006. 753\n[6] G. Widmer and M. Kubat, \u201cLearning in the presence of concept drift and 754\nhidden contexts,\u201d Mach. Learn., vol. 23, no. 1, pp. 69\u2013101, Apr. 1996. 755\n[7] J. Koza, Genetic Programming: On the Programming of Computers by 756\nMeans of Natural Selection. Cambridge, MA: MIT Press, 1992. 757\n[8] E. Lima, M. Hell, R. Ballini, and F. Gomide, \u201cEvolving fuzzy mod- 758\neling using participatory learning,\u201d in Evolving Intelligent Systems, 759\nP. Angelov, D. Filev, and N. Kasabov, Eds. Hoboken, NJ: Wiley, 2010, 760\npp. 67\u201386. 761\n[9] L. Ljung, System Identification: Theory for the User. Englewood Cliffs, 762\nNJ: Prentice-Hall, 1999. 763\n[10] J. J. Rubio, D. M. Vazquez, and J. Pacheco, \u201cBack propagation to train 764\nan evolving radial basis function neural network,\u201d Evolving Syst., vol. 1, 765\nno. 3, pp. 173\u2013180, Oct. 2010. 766\n[11] D. Specht, \u201cA general regression neural network,\u201d IEEE Trans. Neural 767\nNetw., vol. 2, no. 6, pp. 568\u2013576, Nov. 1991. 768\n[12] P. Angelov and D. Filev, \u201cAn approach to on-line identification of evolving 769\nTakagi-Sugeno models,\u201d IEEE Trans. Syst., Man, Cybern. B, Cybern., 770\nvol. 34, no. 1, pp. 484\u2013498, 2004. AQ40771\n[13] U. M. Fayyad, G. Piatetsky-Shapiro, and P. Smyth, From Data Mining to 772\nKnowledge Discovery: An Overview, Advances in Knowledge Discovery 773\nand Data Mining. Cambridge, MA: MIT Press, 1996. 774\n[14] N. Kasabov and Q. Song, \u201cDENFIS: Dynamic evolving neural-fuzzy in- 775\nference system and its application for time-series prediction,\u201d IEEE Trans. 776\nFuzzy Syst., vol. 10, no. 2, pp. 144\u2013154, Apr. 2002. 777\n[15] M. Norgaard, O. Ravn, N. Poulsen, and L. Hansen, Eds., Neural Networks 778\nfor Modelling and Control of Dynamic Systems. New York: Springer- 779\nVerlag, 2000. 780\n[16] J. S. R. Jang, \u201cANFIS: Adaptive network-based fuzzy inference sys- 781\ntems,\u201d IEEE Trans. Syst., Man, Cybern., vol. 23, no. 3, pp. 665\u2013685, 782\nMay\/Jun. 1993. 783\n[17] P. Angelov and E. Lughofer, \u201cA comparative study of two approaches 784\nfor data-driven design of evolving fuzzy systems: eTS and FLEXFIS,\u201d 785\nInt. J. Gen. Syst., vol. 37, no. 1, pp. 45\u201367, 2008. 786\n[18] S. L. Chiu, \u201cFuzzy model identification based on cluster estimation,\u201d 787\nJ. Intell. Fuzzy Syst., vol. 2, no. 3, pp. 267\u2013278, Sep. 1994. 788\n[19] P. Angelov and X. Zhou, \u201cOn line learning fuzzy rule-based system struc- 789\nture from data streams,\u201d in Proc. IEEE Int. Conf. Fuzzy Syst., Hong Kong, 790\nJun. 1\u20136, 2008, pp. 915\u2013922. 791\n[20] P. Angelov and X. Zhou, \u201cEvolving fuzzy-rule-based classifiers from 792\ndata streams,\u201d IEEE Trans. Fuzzy Syst., vol. 16, no. 6, pp. 1462\u20131475, 793\nDec. 2008. 794\n[21] P. Angelov, J. Victor, A. Dourado, and D. Filev, \u201cOn-line evolution 795\nof Takagi-Sugeno fuzzy models,\u201d in Proc. 2nd IFAC Workshop Adv. 796\nFuzzy\/Neural Control, Oulu, Finland, Sep. 16\u201317, 2004, pp. 67\u201372. 797\n[22] R. Yager and D. Filev, Essentials of Fuzzy Modeling and Control. 798\nHoboken, NJ: Wiley, 1994. 799\n[23] P. Angelov, Machine learning (collaborative systems), Patent 800\nWO2008053161, priority date: Nov. 1, 2006; international filing 801\ndate: Oct. 23, 2007. AQ41802\n[24] E. Mazor, A. Averbuch, Y. Bar-Shalom, and J. Dayan, \u201cIndependent mul- 803\ntiple model methods in target tracking: A survey,\u201d IEEE Trans. Aerosp. 804\nElectron. Syst., vol. 34, no. 1, pp. 103\u2013123, Jan. 1998. 805\n[25] T. Martin, \u201cFuzzy sets in the fight against digital obesity,\u201d Fuzzy Sets Syst., 806\nvol. 156, no. 3, pp. 411\u2013417, Dec. 2005. 807\n[26] J. A. Iglesias, P. Angelov, A. Ledezma, and A. Sanchis, \u201cEvolving classifi- 808\ncation of agents\u2019 behaviours: A general approach,\u201d Evolving Syst., vol. 1, 809\nno. 3, pp. 161\u2013171, Oct. 2010. 810\n[27] P. Angelov and X.-W. Zhou, \u201cEvolving fuzzy classifier for real-time 811\nnovelty detection and landmark recognition by a mobile robot,\u201d in Mobile 812\nRobots: The Evolutionary Approach, N. Nedjah, L. dos Santos Coelho, 813\nand L. de Macedo Mourelle, Eds. Heidelberg, Germany: Springer- 814\nVerlag, Mar. 2007, pp. 95\u2013124. 815\n[28] P. Angelov and D. Filev, \u201cFlexible models with evolving structure,\u201d Int. J. 816\nIntell. Syst., vol. 19, no. 4, pp. 327\u2013340, Apr. 2004. 817\n[29] D. Wang, X.-J. Zeng, and J. A. Keane, \u201cA structure evolving learning 818\nmethod for fuzzy systems,\u201d Evolving Syst., vol. 1, no. 2, pp. 83\u201395, 819\nSep. 2010. 820\n[30] F. M. Pouzols and A. Lendasse, \u201cEvolving fuzzy optimally pruned ex- 821\ntreme learning machine for regression problems,\u201d Evolving Syst., vol. 1, 822\nno. 1, pp. 43\u201358, Aug. 2010. 823\n[31] H. Soleimani-B., C. Lucas, and B. N. Araabi, \u201cRecursive Gath\u2013Geva 824\nclustering as a basis for evolving neuro-fuzzy modelling,\u201d Evolving Syst., 825\nvol. 1, no. 1, pp. 59\u201371, Aug. 2010. 826\nIE\nEE\nPr\noo\nf\nANGELOV: FUZZILY CONNECTED MULTIMODEL SYSTEMS EVOLVING AUTONOMOUSLY FROM DATA STREAMS 13\nPlamen Angelov (SM\u2019XX) received the M.Eng.AQ42 827\ndegree in electronics and automation from Sofia828\nTechnical University, Sofia, Bulgaria, in 1989, and829\nthe Ph.D. degree from the Bulgaria Academy of830\nSciences, Sofia, in 1993.831\nHe spent over ten years as a Research Fellow832\nworking on computational intelligence and control833\nsystems. During 1995\u20131996, he was with Hans-834\nKnoell Institute, Jena, Germany. In 1997, he was835\na Visiting Researcher at the Catholic University,836\nLeuvain-la-neuve, Belgium. In 1998, he became a837\nResearch Fellow at Loughborough University, Loughborough, U.K. In 2003,838\nhe joined Lancaster University, Lancaster, U.K., as a Lecturer, where he is839\ncurrently a Senior Lecturer (equivalent to Associate Professor). He was a Vis-840\niting Professor at the Ostfalia University of Applied Sciences, Braunschweig,AQ43 841\nGermany, in 2007, and the University Carlos III, Madrid, Spain, in 2010. He842\nwas also a Visiting Fellow at the University of Campinas, Campinas, Brazil,843\nin 2005, and the Johannes Kepler University of Linz, Linz, Austria, in 2006.AQ44 844\nHe has authored or coauthored over 150 peer-reviewed publications, including845\nthe monograph Evolving Rule Based Models: A Tool for Design of Flexible846\nAdaptive Systems (Springer-Verlag, 2002), a number of books, book chapters,847\nover 40 peer-reviewed journal papers, including eight IEEE TRANSACTIONS848\npapers, and is a holder of a patent in machine learning in 2006. He has849\na wide portfolio of externally funded research including sponsors such as850\nU.K. Research Councils, European Commission, U.K. Ministry of Defence,AQ45 851\nUSA-based American Society of Heating, Refrigeration and Air ConditioningAQ46 852\nEngineers, industries (BAE Systems, 4S Information Systems), U.K. Royal853\nSociety, etc. He is the Editor-in-Chief of the Springer journal Evolving Systems.854\nDr. Angelov is the Chair of the Standards Committee, Computational Intel-855\nligence Society, IEEE, and the founding Chair of the Task Force on Adaptive856\nand Evolving Fuzzy Systems.857\nIE\nEE\nPr\noo\nf\nAUTHOR QUERIES\nAUTHOR PLEASE ANSWER ALL QUERIES\n*Note that your paper will incur overlength page charges of $175 per page. The page limit for regular papers\nis 12 pages, and the page limit for correspondence papers is 6 pages.\nAQ1 = The word \u201ctrust\u201d was changed to \u201cthrust.\u201d Please check if appropriate and correct if necessary.\nAQ2 = The sentence was reworded for clarity. Please check if the original thought was retained and correct\nif necessary.\nAQ3 = The sentence was reworded for clarity. Please check if the original thought was retained and correct\nif necessary.\nAQ4 = \u201cExabyte\u201d was changed to \u201cEB.\u201d Please check if appropriate and correct if necessary.\nAQ5 = The sentence was reworded for clarity. Please check if the original thought was retained and correct\nif necessary.\nAQ6 = The acronym \u201cSOFNN\u201d was defined as \u201cself-organized fuzzy neural network.\u201d Please check if\nappropriate and correct if necessary.\nAQ7 = Please provide the expanded form of the acronym \u201cSAFIS.\u201d\nAQ8 = Please provide the expanded form of the acronym \u201cePL.\u201d\nAQ9 = Please provide the expanded form of the acronym \u201ceTS.\u201d\nAQ10 = Please provide the expanded form of the acronym \u201cSELM.\u201d\nAQ11 = The acronym \u201cOP-ELM\u201d was defined as \u201coptimally pruned extreme learning machine.\u201d Please\ncheck if appropriate and correct if necessary.\nAQ12 = The acronym \u201cENFM\u201d was defined as \u201cenabling new function mode.\u201d Please check if appropriate\nand correct if necessary.\nAQ13 = Please provide the expanded form of the acronym \u201cexTS.\u201d\nAQ14 = Please provide the expanded form of the acronym \u201cFLEXFIS.\u201d\nAQ15 = There is no Condition E in Fig. 2. Please check.\nAQ16 = \u201cSimplified Mamdani\u201d was abbreviated as \u201csM.\u201d Please check if appropriate and correct if\nnecessary.\nAQ17 = The sentence was reworded for clarity. Please check if the original thought was retained and correct\nif necessary.\nAQ18 = The word \u201cones\u201d was inserted here for clarity. Please check if appropriate and correct if necessary.\nAQ19 = The word \u201cor\u201d was deleted in this sentence per IEEE instruction. Please check if appropriate and\ncorrect if necessary.\nAQ20 = The word \u201cbeing\u201d was deleted in this sentence. Please check if appropriate and correct if necessary.\nAQ21 = The sentence was reworded for clarity. Please check if the original thought was retained and correct\nif necessary.\nAQ22 = The word \u201cit\u201d is referred in this sentence as Condition B. Please check if appropriate and correct if\nnecessary.\nAQ23 = The sentence was reworded for clarity. Please check if the original thought was retained and correct\nif necessary.\nAQ24 = The sentence was reworded for clarity. Please check if the original thought was retained and correct\nif necessary.\nAQ25 = The sentence was reworded for clarity. Please check if the original thought was retained and correct\nif necessary.\nAQ26 = The sentence was reworded for clarity. Please check if the original thought was retained and correct\nif necessary.\nIE\nEE\nPr\noo\nf\nANGELOV: FUZZILY CONNECTED MULTIMODEL SYSTEMS EVOLVING AUTONOMOUSLY FROM DATA STREAMS 15\nAQ27 = The acronym \u201cPCA\u201d was defined as \u201cprincipal component analysis.\u201d Please check if appropriate\nand correct if necessary.\nAQ28 = Please provide the expanded form of the acronym \u201cGP.\u201d\nAQ29 = The sentence was reworded for clarity. Please check if the original thought was retained and correct\nif necessary.\nAQ30 = The sentence was reworded for clarity. Please check if the original thought was retained and correct\nif necessary.\nAQ31 = The word \u201cif\u201d is removed in this sentence for clarity. Please check if appropriate and correct if\nnecessary.\nAQ32 = The \u201cUniversity of Linz Austria\u201d was changed to \u201cJohannes Kepler University Linz.\u201d Please check\nif appropriate and correct if necessary.\nAQ33 = The following data were captured as algorithm. Please check if appropriate and correct if necessary.\nAQ34 = The acronym \u201cMIMO\u201d was defined as \u201cmultiple inputs and multiple outputs.\u201d Please check if\nappropriate and correct if necessary.\nAQ35 = The sentence was reworded for clarity. Please check if the original thought was retained and correct\nif necessary.\nAQ36 = The word \u201cthan\u201d was changed to \u201cthen.\u201d Please check if appropriate and correct if necessary.\nAQ37 = Fig. 9 was not found in the manuscript. Please check.\nAQ38 = The phrase \u201cthe mean and standardization operations\u201d was removed from the parentheses for clarity.\nPlease check if appropriate and correct if necessary.\nAQ39 = The sentence was reworded for clarity. Please check if the original thought was retained and correct\nif necessary.\nAQ40 = Please provide month of publication in Ref. [12].\nAQ41 = Please provide complete date the patent was issued in Ref. [23].\nAQ42 = Please provide the IEEE membership history of Plamen Angelov.\nAQ43 = Braunschweig was captured as the city campus of the University of Applied Sciences. Please check\nif appropriate and correct if necessary.\nAQ44 = The University of Linz was changed to Johannes Kepler University of Linz. Please check if\nappropriate and correct if necessary.\nAQ45 = The acronym \u201cEC\u201d was defined as \u201cEuropean Commission.\u201d Please check if appropriate and correct\nif necessary.\nAQ46 = The acronym \u201cASHRAE\u201d was defined as \u201cAmerican Society of Heating, Refrigerating and Air\nConditioning Engineers.\u201d Please check if appropriate and correct if necessary.\nEND OF ALL QUERIES\nIE\nEE\nPr\noo\nf\nIEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART B: CYBERNETICS 1\nFuzzily Connected Multimodel Systems Evolving\nAutonomously From Data Streams\n1\n2\nPlamen Angelov, Senior Member, IEEE3\nAbstract\u2014A general framework and a holistic concept are pro-4\nposed in this paper that combine computationally light machine5\nlearning from streaming data with the online identification and6\nadaptation of dynamic systems in regard to their structure and7\nparameters. According to this concept, the system is assumed to be8\ndecomposable into a set of fuzzily connected simple local models.9\nThe main thrust of this paper is in the development of an originalAQ1 10\napproach for the self-design, self-monitoring, self-management,11\nand self-learning of such systems in a dynamic manner from data12\nstreams which automatically detect and react to the shift in the13\ndata distribution by evolving the system structure. Novelties of14\nthis contribution lie in the following: 1) the computationally simple15\napproach (simpl_e_Clustering\u2014simplified evolving Clustering) to16\ndata space partitioning by recursive evolving clustering based on17\nits relative position to the mean of the overall data, 2) the learningAQ2 18\ntechnique for online structure evolution as a reaction to the shift19\nin the data distribution, 3) the method for online system structure20\nsimplification based on utility and inputs\/feature selection, and21\n4) the novel graphical illustration of the spatiotemporal evolu-22\ntion of the data stream. The application domain for this com-23\nputationally efficient technique ranges from clustering, modeling,24\nprognostics, classification, and time-series prediction to pattern25\nrecognition, image segmentation, vector quantization, etc., to more26\ngeneral problems in various application areas, e.g., intelligent27\nsensors, mobile robotics, advanced manufacturing processes, etc.28\nIndex Terms\u2014Evolving fuzzy systems, fuzzily weighted recur-29\nsive least-squares estimation, fuzzy rule-based systems.30\nI. INTRODUCTION31\nS EVERAL important problems from control theory and ma-32 chine learning such as system modeling and identification33\n[9], clustering and classification [20], time series prediction [2],34\nand controller design [22] can be generalized into a common35\nframework that can be represented as a nonlinear mapping of36\nsome inputs onto some outputs. For example, in classification,37\nthe inputs are the features, while the outputs are the class38\nlabels; in clustering, the outputs are not existing; in control, the39\ninputs are usually the error and the derivative (or integral) of40\nthe error, while the outputs are the control actions; in system41\nmodeling, the inputs represent the independent variables, while42\nthe outputs\u2014the dependant ones; in time series predictions, the43\ninputs are past and present values, while the outputs are the44\npredicted future values of the time series. An obvious approach45\nManuscript received January 25, 2010; revised July 13, 2010 and October 12,\n2010; accepted December 5, 2010. This paper was recommended by Associate\nEditor F. Karray.\nThe author is with InfoLab21, School of Computing and Com-\nmunications, Lancaster University, Lancaster, LA1 4WA, U.K. (e-mail:\np.angelov@lancaster.ac.uk).\nColor versions of one or more of the figures in this paper are available online\nat http:\/\/ieeexplore.ieee.org.\nDigital Object Identifier 10.1109\/TSMCB.2010.2098866\n(that was dominant until 1970s) is to use some existing prior 46\nknowledge about the structure of the mapping (usually in the 47\nform of first principle models) and then to fine tune the param- 48\neters. The alternatives to the first principle models include the 49\nfollowing: 1) so-called black-box approach to which we can cat- 50\negorize state-space and polynomial models [9], more recently, 51\nneural networks (NN) [15], etc., and 2) expert and fuzzy rule- 52\nbased (FRB) models [22]. The former lack transparency and 53\nis often dubbed \u201cnumber fitting.\u201d The latter were tedious and 54\ndifficult to design until 1990s and were closely related to expert 55\nknowledge. During 1990s, it was proven that FRB are dual AQ356\nto certain types of NN. Thus, the term \u201cneuro-fuzzy systems\u201d 57\n(NFS) was introduced, and data-driven learning techniques 58\nwere developed [16] which decoupled the FRB systems from 59\nthe expert knowledge. This can be seen as an extension of 60\nthe well-known concepts of adaptive (usually linear) systems, 61\nindependent multiple model systems [24], Gaussian mixture 62\nmodels [2], generalized regression models [11], etc. 63\nThe importance of data-driven FRB systems becomes vital 64\nnowadays when we are surrounded by huge amounts of stream- 65\ning data flows leading to so-called \u201cdigital obesity\u201d [25] (every 66\nyear, more than 1 EB (= 1018 bytes) of data are generated AQ467\nworldwide, most of it in digital form). The availability of 68\nconveniently using efficient online real-time algorithms for 69\nextracting knowledge in a human-intelligible form from data 70\nstreams [13] is a pressing demand. It is important to emphasize AQ571\nthat the data-driven design methods do not exclude the expert 72\nknowledge which can be used (if available) at the initialization 73\nsteps or at a higher supervisory level. These approaches are 74\nimportant because they make the automation of the process of 75\ncomplex nonlinear systems design possible as well as extracting 76\nautomatically in real-time knowledge and providing it to human 77\nusers or decision makers which is vital for numerous practical 78\napplications [3], [17], [25]. 79\nSystem design problem can be divided into the following: 80\n1) system structure identification and 2) parameter learning [9]. 81\nWhile the latter problem is well studied [9], [16], the former 82\none is still an open problem. Traditionally, it was assumed that 83\nthe structure is selected and fixed by an expert based on some 84\nprior knowledge or experience or derived offline [9], [16], [18], 85\n[22]. During the last decade, a number of approaches emerged 86\naddressing the problem of online identification of FRB system 87\nassuming an evolving structure, such as self-organized fuzzy 88\nneural network (SOFNN) [4], SAFIS [5], ePL [8], eTS [12],\nAQ6\nAQ7\nAQ8\nAQ9\n89\ndynamic evolving neural-fuzzy inference system (DENFIS) 90\n[14], SELM [29], optimally pruned extreme learning machine AQ1091\n(OP-ELM) [30], enabling new function mode [31], evolving AQ11AQ1292\nNN [10], etc. Most of these require user- or problem-specific 93\n1083-4419\/$26.00 \u00a9 2010 IEEE\nIE\nEE\nPr\noo\nf\n2 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART B: CYBERNETICS\nthresholds that influence the resulting structure complexity94\nwhich leads to a large number of rules being generated; to95\nremedy this, they apply afterward \u201cpruning\u201d techniques [4],96\n[5], [14] that require further threshold parameters and may lead97\nto instability and represents an ad hoc technique. In eTS [12]98\nand DENFIS [14], the spreads of the membership functions99\nbelonging to various inputs have the same value. Improved ver-100\nsions were also published, such as exTS [19] which introduceAQ13 101\nlearning of the spread (cluster radii)\u201e but a major shortcoming102\nof these approaches is that they assume a fixed number of103\ninputs\/features determined in advance.104\nIn this paper, we propose an entirely data-driven method105\ncalled simplified evolving Takagi\u2013Sugeno simpl_eTS+ NFS106\nthat allows truly flexible and evolving structure systems to107\nbe designed autonomously online including selecting the in-108\nput variables, adapting the cluster radii, and using a compu-109\ntationally simplified evolving online clustering method. The110\nbasic principle is that of a gradual evolution of the fuzzily111\ncoupled multimodel structure in terms of local subsystems112\nas well as in terms of input variables. Contrast this to the113\nabrupt \u201cpruning\u201d and lack of flexibility in terms of input114\nvariable selection and cluster radii\/membership spread of all115\nother currently existing approaches. The structure is determined116\nby a new computationally simple evolving clustering method117\n(simpl_e_Clustering) in the joint input\u2013output data space118\n(see Section III). The quality of the local models and the119\nrespective automatically generated FRB can be monitored on-120\nline by qualitative measures, such as utility (Section IV). The121\ninputs\/features are gradually selected from an initial pool based122\non their recursively accumulated sensitivity using an original123\nmethod (Section V). The proposed approach is verified on a124\nnumber of challenging synthetic and real industrial examples125\n(Section VI).126\nII. PROBLEM STATEMENT127\nThe proposed simpl_eTS+ NFS can be seen as a gener-128\nalized framework that can model and describe nonlinear and129\nnonstationary processes, as shown in Fig. 1.130\nIt differs from all current NFS such as adaptive network-131\nbased fuzzy inference systems (ANFIS) [16], DENFIS [14],132\neTS [12], SAFIS [5], SOFNN [4], FLEXFIS [17], ePL [8],AQ14 133\nevolving NN [10], etc., by the fact that they have a prefixed134\nnumber of inputs (see Condition E, Fig. 2).AQ15 135\nIt is also unique in the way the system structure is generated136\n(evolves) from the streaming database on the data density137\nincrement (see next section and Fig. 3) that reflects shifts in the138\ndata stream (see Fig. 5). It also differs by the ability to gradually139\nsimplify the system structure based on the rules\u2019 utility.140\nSimpl_eTS+ can be interpreted as an evolving set of141\nlinguistic fuzzy rules of the following form:142\nRi : IF\n(\nx1 is x\ni\u2217\n1\n)\nAND . . . AND\n(\nxn is x\ni\u2217\nn\n)\nTHEN\n(\nLocalModel (yi)\n) (1)\nwhere Ri denotes the ith fuzzy rule, i = [1, N ], N is the num-143\nber of fuzzy rules, x = [x1, x2, . . . , xn]T is the input vector,144\nFig. 1. Schematic representation of the simpl_eTS+ as an NFS. Layer 1\nrepresents the membership functions grouped by fuzzy rules, layer 2\u2014the\naggregation operators, layer 3\u2014the normalization, layer 4 weighs the local\nmodels, and layer 5 sums the partial outputs.\nFig. 2. Rule-based evolution in the simpl_eTS+. System structure is not\nfixed but can rather gradually evolve in terms of fuzzy rules (Condition A for\ngrowth and Conditions B and C for reduction) as well as in terms of input\nvariables and respective fuzzy membership functions and dimensionality of the\nconsequences that are associated with them (Condition D).\n(xj is xi\u2217j ) denotes the jth fuzzy sets of the ith fuzzy rule, 145\nj = [1, n]; note that n also evolves, xi\u2217 is the focal point of the 146\nith rule antecedent, LM(yi) is the Local Model of the ith fuzzy 147\nrule, which is expressed by the (generally, m-dimensional) 148\noutput variables yi = [yi1, yi2, . . . , yim]. 149\nThe data are standardized online using recursively updated 150\nmean and standard deviation [19] 151\nxst =\nx\u2212 x\n\u03c3x\nx(k) =\nk \u2212 1\nk\nx(k \u2212 1) + 1\nk\nx(k)\nx(1) =x(1)\n\u03c32x(k) =\nk \u2212 1\nk\n\u03c32x(k \u2212 1) +\n1\nk \u2212 1 (x(k)\u2212 x(k))\n2 \u03c32x(1)\n= 0nx1\nIE\nEE\nPr\noo\nf\nANGELOV: FUZZILY CONNECTED MULTIMODEL SYSTEMS EVOLVING AUTONOMOUSLY FROM DATA STREAMS 3\nFig. 3. (a) Two-dimensional illustration of the principle of forming new clus-\nters and fuzzy rules based on density increment \u03b4 [see (8) and the Appendix]\nrelative to the global mean (denoted by a diamond). New clusters\/rules of one\nof the following types are formed by new data samples. Type 1 (inside the\ninner circle)\u2014they provide better generalization and summarization. Type 2\n(outside the outer circle)\u2014they provide better coverage of the data space. The\ndata between the two circles are interpreted by the existing rules. (b) For\ncomparison, the same data from Fig. 3(a) are used to demonstrate the zone\nof influence (using ellipsoids at a fixed membership degree value). It is obvious\nthat they ensure a good coverage.\nyst =\ny \u2212 y\n\u03c3y\ny(k) =\nk \u2212 1\nk\ny(k \u2212 1) + 1\nk\ny(k)\ny(1) = y(1)\n\u03c32y(k) =\nk \u2212 1\nk\n\u03c32y(k \u2212 1) +\n1\nk \u2212 1 (y(k)\u2212 y(k))\n2\n\u03c32y(1) = 0mx1 (2)\nwhere xst denotes the standardized input data, yst\u2014152\nstandardized output data, x is the mean of the input data, y\u2014the153\nmean of the output data, \u03c3x denotes the standard deviation154\n(variance) of the input data, and \u03c3y\u2014the variance of the155\noutput data.156\nFor a specific type of the local subsystems, one can transform157\nthe simpl_eTS+ into one of the following types:158\n1) first order TS-type [1] FRB system when the local sub-159\nsystems are linear160\nyi = xTe \u03c0\ni xTe = [1, x\nT ] \u03c0i =\n\u2223\u2223\u2223\u2223\u2223\u2223\nai01 . . . a\ni\n0m\n. . . . . . . . .\nain1 .. a\ni\nnm\n\u2223\u2223\u2223\u2223\u2223\u2223 (3)\n2) zero-order TS-type FRB system that can also be consid- 161\nered as a simplified Mamdani (sM) type when the local AQ16162\nsubsystems are singletons (crisp scalar values) 163\nyi = Ai (4)\nwhere Ai = [ai01 ai02 ai0m]T denotes the local subsys- 164\ntem parameters. 165\nThe overall output of the simpl_eTS+ system y is formed 166\nas a collection of loosely\/fuzzily combined multiple local sim- 167\npler subsystems yi. The degree of activation of each local 168\nsubsystem is proportional to the level of its contribution to the 169\noverall output [22] 170\ny =\nN\u2211\ni=1\n\u03bbiyi; \u03bbi = \u03c4 i\n\/\nN\u2211\nj=1\n\u03c4 j (5)\nwhere yi represents the output of the ith local subsystems, \u03bbi 171\nis the normalized activation level of the ith rule, and \u03c4 i is the 172\nfiring level of the ith rule. 173\nThe firing level can be defined as a Cartesian product 174\n(t-norm) of respective degrees of membership of the antecedent 175\nfuzzy sets (activation of neurons) for this rule [22] 176\n\u03c4 i =\nn\nT\nj=1\n\u03bcij(xj) (6)\nwhere \u03bcij is the membership value of the jth input xj , j = [1, n] 177\nin the ith fuzzy rule i = [1, N ]. 178\nThe membership function can be of any known form. The 179\nGaussian and bell functions are preferable due to their general- 180\nization capabilities (resembling normal distribution and cover- 181\ning the whole domain of variables) [22] 182\n\u03bcij = e\n\u2212\n\u2016x\u2212xi\u2217\u20162\nj\n2(rij)\n2\n(7)\nwhere, for (rij)2, i = [1, N ] j = [1, n] is the spread of the 183\nmembership function, which represents the spread of the zone 184\nof influence of the cluster\/rule\/neuron projected on the jth axis. AQ17185\nIn the following section, we will describe the 186\nsimpl_e_Clustering method used in simpl_eTS+ 187\nto automatically extract the system structure from data streams 188\nby \u201con fly\u201d clustering, detecting novel data patterns and 189\nevolving the shape of the clusters. 190\nIII. SIMPLIFIED EVOLVING CLUSTERING METHOD 191\n(simpl_e_Clustering) 192\nThe basic principles of the newly proposed 193\nsimpl_e_Clustering approach are to ensure the following: 194\nP1) good generalization and summarization of data by clus- 195\ntering around the focal points located in areas of high 196\ndensity D; 197\nP2) good coverage of the entire data space by expanding the 198\narea of interpolation. 199\nIE\nEE\nPr\noo\nf\n4 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART B: CYBERNETICS\nThe latter condition is evaluated in respect to the relative200\nposition of the new data point, existing focal points, and the201\nglobal mean of all standardized data [23]. Additionally, the202\nlevel of overlap of the fuzzy rules is being controlled removing203\npreviously existing rules that overlap significantly with the204\nnewly formed rules. Finally, rules\/neurons\/clusters are being205\nremoved if they are not used actively.206\nMachine learning literature introduced the paradigm of con-207\ncept change [6] of the underlying distribution of data streams208\ncalled drift (gradual evolution of the concept over time) or209\nshift (a more abrupt and sudden change of the concept over210\ntime). The concept drift is represented as a smooth sliding of211\nthe data distribution in the data\/feature space from one region212\nto another. The concept shift is directly related to the concept213\nof evolving systems\u2014forming new clusters\/neurons\/rules or214\nremoving existing ones that are not used actively.AQ18 215\nWhile the data density is represented in the data space216\ndomain, both the drift and shift must be analyzed in the joint217\ndata\u2013time space domain. In the newly proposed simpl_eTS+218\nmethod, shifts in data streams are detected naturally by the219\ndensity increment \u03b4(k) [see Fig. 3(a)].220\nReaction to a detected shift is by either the following:221\n1) forming a new rule around a new data sample which222\nbecomes a focal point for global data distribution;AQ19 223\n2) replacement of an existing fuzzy rule.224\nReplacement of a rule itself consists of the following:225\na) forming a new rule around the new point\u2014same as in the226\nprevious step;227\nb) removal of the rule which has lower density and is close228\nto this newly added one.229\nIn summary, we evolve the structure of the simpl_eTS+230\naccording to the following conditions:231\nCondition A) a data sample that covers a new area of the data232\nspace represented by the density increment relative to the233\nglobal mean [see Fig. 3(a)];234\nCondition B) avoid overlap and information redundancy (see235\nFig. 4).236\nCondition A is defined (see Lemma 1 in the Appendix for237\ndetails) as238\nIF (\u03b4(k) = N) THEN (x(k) \u2192 new centre) (8)\nwhere \u03b4(k) denotes the total density increment239\n\u03b4(k) =\n\u2223\u2223\u2223\u2223\u2223\u2223\nN\u2211\ni=1\nsign\nn+m\u2211\nj=1\n\u03b3i\u2217j (k)\n\u2223\u2223\u2223\u2223\u2223\u2223\n\u03b3i\u2217j (k) =\n(\nzi\u2217j (k)\n)2 \u2212 z2j (k) + v2 (zj(k)\u2212 zi\u2217j (k)) zi\u2217j (k)\nis the partial (per cluster and per input) density incre-240\nment and z = [xT ; y]T denotes the input\u2013output vector (data241\nsample).242\nIt can be proven that the global mean tends asymptotically243\ntoward zero when the number of standardized data points tends244\nto infinity (Fig. 5)\u2014see Lemma 2 in the Appendix for the proof245\nlim\nk\u2192\u221e\nzst = 0. (9)\nFig. 4. Condition B: If the overlap between the new and existing membership\nfunctions is very high, then the previous (old) rule is removed and replaced by\nthe new one. AQ20\nFig. 5. Convergence of the global mean of the standardized data to the origin\n(Lemma 3)\u2014data space representation for two inputs.\nCorollary 1: When standardized data are being used, then 246\nlimk\u2192\u221e \u03b3i\u2217j (k) = (z\ni\u2217\nj (k))\n2 \u2212 z2j (k). 247\nAs a result of Condition A, two types of new focal points of 248\nclusters\/rules will be formed [see Fig. 3(a)]: 1) type 1\u2014based 249\non principle P1, for which\n\u2211N\ni=1 sign\n\u2211n+m\nj=1 \u03b3\ni\u2217\nj (k) = N 250\nand 2) type 2\u2014based on principle P2, for which 251\u2211N\ni=1 sign\n\u2211n+m\nj=1 \u03b3\ni\u2217(k) = \u2212N . 252\nNew rules of type 1 represent focal points with higher density 253\nthan any previous focal point [see the inner circle in Fig. 3(a)]. 254\nNew rules\/clusters of type 2 are defined by focal points that 255\ncover new areas of data space that cannot be interpolated well 256\nusing previously existing rules only\u2014outside the outer circle in 257\nFig. 3(a). 258\nCorollary 2: When standardized data are used, Condi- 259\ntion A reduces to\n\u2211n+m\nj=1 (z\ni\u2217\nj (k))\n2 >\n\u2211n+m\nj=1 z\n2\nj (k) \u2200i, i = 260\n[1, N ] for type 1 and\n\u2211n+m\nj=1 (z\ni\u2217\nj (k))\n2 <\n\u2211n+m\nj=1 z\n2\nj (k) \u2200i, 261\ni = [1, N ] for type 2. This can be conveniently represented 262\ngraphically for a 2-D case as shown in Fig. 3(a). In Fig. 3(b), 263\nfor the sake of comparison, we illustrate exactly the same 264\ndata as in Fig. 3(a) but instead of the circles indicating the 265\nareas where new rules of type 1 and 2 will be formed, 266\nwe indicate the zones of influence of the existing clusters 267\nby ellipsoids (one can also notice the variable radii of the 268\nclusters). AQ21269\nIE\nEE\nPr\noo\nf\nANGELOV: FUZZILY CONNECTED MULTIMODEL SYSTEMS EVOLVING AUTONOMOUSLY FROM DATA STREAMS 5\nTABLE I\nCOMPUTATIONAL COMPLEXITY\nIt is important to note that Condition A only requires the270\nfollowing to be known:271\n1) current data point zk;272\n2) previous focal points zi\u2217(k);273\n3) the global mean z.274\nIt codifies, however, the exact information about the density at275\nthe current point, although all previous points are not memo-276\nrized. All other existing approaches for evolving NFS do not277\nuse the density information of all previous points except eTS278\n[12], [19], [28] which is computationally less efficient than the279\nproposed approach, as can be seen in Table I. The compu-280\ntational complexity of the proposed simpl_e_Clustering281\nmethod in the structure stage is of order O(1) because it only282\nrequires an online update of a single value\u2014the global mean283\nz. In terms of memory, its complexity is of order O(N) since284\nit requires memorizing previous N focal points plus the global285\nmean. For comparison, eClustering (the approach used in eTS286\nto generate the system structure) has computational complexity287\nof order O(N) that includes the updating of potentials of the288\nexisting N rules and calculating the potential of the new point.289\nIts memory requirements are of order O(N + n) for centers290\nand accumulated variables needed to recursively calculate the291\npotential [12], [19], [28]. Note that k \u0006 N , k \u0006 n, while N292\nand n are comparable.293\nAs opposed to the clustering approach used in eTS [12], [19],294\n[28], there is no need to calculate the data density\/potential,295\nneither the density at the focal points nor to keep them in the296\nmemory and to update them. At the same time, differently from297\nall other published approaches, known to the author, the density298\ninformation from all previous data samples is being used,299\nrecursively accumulated and compared to all previous focal300\npoints, as detailed in the Appendix. To update the global mean301\nper input [see equation (2)] is much easier and computationally302\ncheaper than to calculate and update densities\/potentials.303\nMoreover, it can also be proven that the global mean is the304\npoint with the highest density\/potential\u2014see Lemma 3. Thus,305\nprinciple P1 alone (selecting data points with highest den-306\nsity\/potential as candidates to form new centers) is not enough307\nto ensure coverage of the data space. This was emphasized on308\nan empirical basis in [21].309\nCondition B can be formulated as \u201cthe new point that is310\naccepted to be a new focal point based on Condition A. It should311\nnot have a high degree of membership to any of the already312\nexisting rules (should not be very close to any of the previously313\nexisting focal points, as shown in Fig. 4)AQ22 314\n\u03bcN+1j\n(\nxi\u2217j\n)\n> \u03b50 \u2200j; j=[1, n] i\u2217=\nN\nargmin\ni=1\n\u2225\u2225xj(k)\u2212xij\u2225\u22252\n(10)\nwhere \u03b50 is a level that represents a high degree of member- 315\nship (closeness to the center) and a suggested value \u03b50 = 0.9 316\nof 90%. AQ23317\nThis condition leads to a simpler structure compared to other 318\nmethods which often require the so-called \u201cpruning,\u201d which 319\nwill be discussed later. AQ24320\nIV. SELF-MONITORING THE QUALITY OF CLUSTERS IN 321\nsimpl_e_Clustering 322\nSimpl_eClusteringis a method that self-monitors and 323\nself-regulates the quality of the generated clusters. In 324\nsimpl_eClustering, only cluster centers and the mean 325\nvalue are kept in the memory that are N + 1 values of 326\ndimension (n + m), while all other data points are dis- 327\ncarded. The question arises \u201chow well do these centers 328\nrepresent the data that were discarded from the memory?\u201d 329\nSimpl_eClustering addresses this generic issue of evolv- 330\ning online system design that is usually ignored by other 331\nexisting methods by monitoring and reacting online to certain 332\nproperties of the clusters (respectively, rules) that are formed. 333\nThe value of the spread (zone of influence) of the clusters 334\nr can be updated per input j online by learning the data 335\ndistribution and variance [24] 336(\nrij(k)\n)2= \u03b2 (rij(k\u22121))2+(1\u2212\u03b2) (\u03c3ij(k))2 , rij(1)=0.5\n(11)\nwhere \u03b2 is the learning step (recommended value is 0.5); 337\n\u03c3ij(k) is the scatter\/spread, which is determined by (\u03c3ij(k))2 = 338\n(1\/Si(k))\n\u2211Si(k)\nl=1 \u2016zi\u2217 \u2212 zl\u20162; \u03c3ij(1) = 1; Si is the support of 339\nthe ith cluster; and i = [1, N ], which is updated by 340\nIF\n(\nl =\nN\nargmin\n\u2225\u2225z(k)\u2212 zi\u2217\u2225\u2225\ni=1\n)\nTHEN\n(\nSl(k + 1) = Sl(k) + 1\n)\n.\nDue to the incremental nature of the approach, the relevance 341\nof a cluster (respectively, fuzzy rule) may change. One of the 342\nmeasures of the relevance of the rule is the utilityU which 343\naccumulates the weight of the rule contributions to the overall 344\noutput during the life of the rule (from the moment when this 345\nrule was generated until the current time instant) [23] 346\nU l(k) =\n1\nk \u2212 tl(k)\ntl(k)\u2211\nl=1\n\u03bbl. (12)\nIt is a measure of importance of the respective fuzzy rule 347\ncompared to the other rules [comparison is hidden in the relative 348\nnature of \u03bb, [see (5)]. 349\nA condition to remove rules that have low utility can be 350\nformulated as: 351\nCondition C: 352\nIF\n(\nU l(k) < \u03b51\n)\nTHEN (\u03bbl \u2190 0) (13)\nwhere \u03b51 denotes tolerance (recommended values 3%\u201310%). 353\nIn simpl_eTS+, the cluster\/rule utility is used for real-time 354\nmanagement of the structure of the system as summarized in the 355\nfollowing algorithm: 356\nIE\nEE\nPr\noo\nf\n6 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART B: CYBERNETICS\nAs a result of this procedure, the antecedent part of the 357\nsimpl_eTS+ system is generated from the data stream as 358\nRi IF\n(\nx1 is xi\u22171\n)\nAND\n(\nx2 is xi\u22172\n)\n. . .\n(\nxn is xi\u2217n\n)\n, i=[1, N ].\n(14)\nIt can be directly used for clustering data and various applica- 359\ntions, e.g., in robotics [27] and user behavior modeling [26]. 360\nIt can also be used as a basis of FRB or NFS for: 361\n1) online prediction; 362\n2) classification; 363\n3) control. 364\nV. SELF-LEARNING simpl_eTS+ 365\nA. Learning Consequents 366\nOnce the antecedent part of the fuzzy model is determined 367\nand fixed, parameters of the consequent part \u03c0i can be learned 368\nusing a recursive least square (RLS) estimation [9]. The real- 369\ntime algorithm performs both tasks (clustering and parameter 370\nestimation) at the same time instant (per data point). The 371\nantecedent part of the rules (layers 1\u20133 of the simpl_eTS+, 372\nFig. 1) can be determined in a fully unsupervised manner, while 373\nthe consequent part (layers 4\u20135) requires a supervised feedback. 374\nThe supervision is by error feedback which guarantees opti- 375\nmality (subject to fixed rule-base structure) of the consequent 376\nparameters. The overall output of the simpl_eTS+ system 377\ngiven by (1)\u2013(7) can be rewritten in a vector form as follows: 378\ny = \u03c8T\u03b8 (15)\nwhere \u03b8 = [(\u03c01)T , (\u03c02)T , . . . , (\u03c0N )T ]T is a vector formed by 379\nthe local model parameters, \u03c8 = [\u03bb1xTe , . . . , \u03bbNxTe ]T is a vec- 380\ntor of the inputs that are weighted by the normalized activation 381\nlevels of the rules \u03bbi, i = [1, N ] for the first-order TS model 382\n(3), and \u03c8 = [\u03bb1, \u03bb2, . . . , \u03bbN ]T for the sM (4). 383\nFor a given data point x(k), the (globally) optimal in LS 384\nsense solution \u03b8\u02c6k that minimizes the following cost function: 385\n(Y \u2212\u03a8T\u03b8)T (Y \u2212\u03a8T\u03b8) \u2192 min (16)\ncan be found by applying the weighted RLS (wRLS) with 386\nforgetting [9], [12] 387\n\u03b8\u02c6(k) = \u02c6\u03b8(k\u22121)+C(k)\u03c8(k)\n(\ny(k)\u2212\u03c8T(k) \u02c6\u03b8(k\u22121)\n)\n(17)\n\u03b3C(k) =C(k \u2212 1)\u2212 C(k \u2212 1)\u03c8(k)\u03c8\nT(k)C(k \u2212 1)\n\u03b3 + \u03c8T(k)C(k \u2212 1)\u03c8(k) (18)\ninitialized by \u03b8\u02c6(1) = 0; C(1) = \u03a9I , where k = 2, 3, . . .; C is 388\nan Nn\u00d7Nn covariance matrix; \u03a9 is a large positive number; 389\nI is the identity matrix; \u03b3 denotes the gradual forgetting factor 390\n\u03b3 (0.9 < \u03b3 \u2264 1); Y , \u03a8, and \u03b8 are the diagonal matrices with 391\ny(k), \u03c8(k), and \u03b8(k) in their main diagonal, respectively. 392\nWe call this locally optimal fuzzily weighted RLS learning 393\nwith forgetting (wRLSf) algorithm. In this expression, the RLS 394\nis fuzzily weighted through the activation levels which are not 395\nthe same as the conventional weighted RLS [9] that is directly 396\napplicable under the assumption that the model in (1)\u2013(4) 397\nIE\nEE\nPr\noo\nf\nANGELOV: FUZZILY CONNECTED MULTIMODEL SYSTEMS EVOLVING AUTONOMOUSLY FROM DATA STREAMS 7\nhas a fixed structure. Under this assumption, the optimizationAQ25 398\nproblem (16) is linear in parameters. The concept of evolving399\nsystems, however, assumes a gradually evolving model struc-400\nture. As a result, the activation level of the fuzzy rules \u03bbi401\nchanges. These changes (although infrequent and gradual in the402\nsense that only one out of N rules is affected and only when a403\nnew rule\/cluster is created) have retrospective effect (they affect404\npreviously calculated activation levels \u03bbij , where i = [1, N ] and405\nj = [1, k \u2212 1]).406\nLocally optimal in LS sense solution of the online parameter407\nidentification task is as follows [12]:AQ26 408\n\u02c6\u03c0i(k) = \u02c6\u03c0i(k \u2212 1) + ci(k)xe(k)\u03bbi (x(k))\n\u00d7\n(\nyl(k)\u2212 xTe (k) \u02c6\u03c0i(k \u2212 1)\n)\n(19)\n\u03b3ci(k) = ci(k \u2212 1)\u2212 \u03bb\ni (x(k)) ci(k\u22121)xe(k)xTe (k)ci(k\u22121)\n\u03b3 + \u03bbi (x(k))xTe (k)ci(k \u2212 1)xe(k)\n.\n(20)\nInitialized by \u02c6\u03c0i(1) = 0; ci(1) = \u03a9I; l = 1, 2, . . . ,m; k =409\n2, 3, . . . will minimize the following cost function:410\nN\u2211\ni=1\n(Y \u2212XT\u03c0i)T\u039bi(Y \u2212XT\u03c0i) (21)\nwhere \u039b and X are the diagonal matrices with \u03bb and xe,411\nrespectively, in their main diagonals.412\nThe local wRLSf is significantly less affected by this dis-413\nturbance of the theoretical optimality of the RLS condition as414\ncompared to the global wRLSf. In addition, it is significantly415\nless computationally complex.416\nB. Online Input Selection417\nSelecting the most informative inputs\/features is a critical418\ntask that is usually associated with preprocessing stages [2]419\nand is addressed by approaches such as principal component420\nanalysis [2], GP [7], etc. These approaches, however, require aAQ27AQ28 421\nbatch set of data and a fixed model structure.422\nIn the previous section, and, to the best of our knowledge,423\nin all previous research in online system identification, indeed,424\nit was assumed that the dimensionality of the input\/features425\nvector n is predefined for each problem at hand. Here, we426\npresent an approach which breaks this assumption by gradually427\nremoving inputs\/features that do not contribute to the output(s)428\nbased on online estimation of the sensitivity of the output(s)429\nto the inputs. Because in TS fuzzy systems [1], the output is430\nlocally linear; the sensitivity analysis is reduced to the analysis431\nof the consequent parameters. The importance of each input\/AQ29 432\nfeature can be evaluated by the ratio of the accumulated sum of433\nthe consequent parameters for the specific jth input\/feature in434\nrespect to all n inputs\/features [23]435\n\u03c9ij(k) = Tij(k)\n\/ n\u2211\nr=1\nTir(k) i = [1, N ] j = [1, n]\n(22)\nFig. 6. Importance of the input variables estimated online (Condition D) based\non real industrial data as described in case study B.\nwhere Tij(k) =\n\u2211k\nl=1 |aij(l)| denotes the accumulated sum of 436\nparameter values of the ith rule. 437\nSince the inputs, outputs, and the internal variables of the 438\nsimpl_eTS+ system are standardized, they are comparable 439\nbetween each other. The value of the weight can be used for 440\nthe gradual removal of inputs\/features that contribute little to 441\nthe overall output. Then, the inputs\/features j\u2217 that do not 442\ncontribute significantly to the output can be removed at the next 443\ntime instant, simplifying from the system structure 444\nCondition D: 445\n\u2203j\u2217| \u03c9ij\u2217(k) < \u03b52 nmax\nr=1\n\u03c9ir(k), i = [1, N ] (23)\nwhere \u03b52 denotes the tolerable minimum weight of an 446\ninput\/feature\u2014suggested value is from 3% to 5%. 447\nThis approach provides a tool for monitoring and analyzing 448\nthe contribution of each input variable\/feature online. Removal 449\nof features\/inputs, however, should be used with care because, 450\nas for any online and incremental approach, simpl_eTS+ is 451\norder dependant, and therefore, removal is recommended on the 452\nbasis of longer period of monitoring\/observation. 453\nCondition D is in terms of the proportion which the weight of 454\na certain input\/feature represents from the maximum of the ac- 455\ncumulated sum of parameters (see Fig. 6 for an example). If this 456\nproportion is negligible (less than \u03b52), then this input\/feature 457\ncan be removed without significantly affecting the output. This 458\ntechnique has high practical importance because, very often, in 459\na real environment, there are many measurable variables that 460\ninfluence the output. 461\nThis approach is prototype-based in that some of the data 462\npoints are used as prototypes (focal points). A number of pre- 463\nvious algorithms that concern (neuro-) fuzzy system learning 464\nuse mean-based clustering [4], [5] (the centers are located at 465\nthe mean, which, in general, do not coincide with any data 466\npoint). The clustering approaches used in [4], [5], [8], [14], 467\n[15], etc., are threshold-based, and the result highly depends 468\non the selection of appropriate threshold(s). Therefore, these 469\napproaches form a large number of clusters that later has to be 470\n\u201cpruned\u201d [5]. 471\nCombining simpl_e_Clustering for antecedents\u2019 struc- 472\nture identification (Section III) with the wRLSf learning algo- 473\nrithm (Section V-A) and online input selection (Section V-B), 474\nIE\nEE\nPr\noo\nf\n8 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART B: CYBERNETICS\nthe following simple procedure for evolving simpl_eTS+475\nfrom data streams can be formulated:476\nVI. EXPERIMENTAL RESULTS AND ANALYSIS477\nA number of experiments were carried out on data streams478\nfrom a synthetic and two real industrial processes. The synthetic479\ndata set is a widely used benchmark for predictive models and is480\na very challenging chaotic time series and was used in order to481\ncompare the results of using simpl_eTS+ with the results of482\nother published approaches on the same data set. Both industrial483\ndata streams have noise and are provided to simpl_eTS+ and484\nother algorithms without any preprocessing. Both data streams485\nare collected from a large number of measurements (candidate486\ninput variables\u2014180 and 23, respectively), and these put the487\nalgorithms to a test that is close to a real world situation. The488\nexperiments aim to demonstrate that simpl_eTS+ is able489\nto autonomously self-develop, self-monitor, and self-improve,490\nand in this sense, the experiments are very challenging. In491\naddition, the second industrial data stream contains a sudden492\nchange around sample 1300 which was caused by replacing493\nthe catalyzer in the process run by The Dow Chemical Com-494\npany. This real life change is an excellent challenge which495\nonce addressed successfully demonstrates the ability to self-496\nadapt model structure according to dynamically changing data497\nstreams.498\nEach data stream was tested with several settings of the499\nsimpl_eTS+ in order to analyze the effects and perfor-500\nmance of the improvements introduced in this paper; the501\nfirst setting of simpl_eTS+ included the removal of rules502\n(Conditions A\u2013C); the second setting was with simpl_eTS+503\nincluding online input selection (using Conditions A, B, D); and504\nthe last setting is of the full simpl_eTS+ (all Conditions A\u2013D505\nbeing active).506\nA. Synthetic Data Sets507\nFirst, we compare the performance of simpl_eTS+ with508\nother published approaches on the widely used time series509\ngenerated from the Mackey\u2013Glass differential delay equation510\n\u2202\u02d9z\n\u2202t\n=\n0.2z(t\u2212 T )\n1 + z10(t\u2212 T ) \u2212 0.1z(t).\nThe following experiment was conducted: 3000 data points,511\nfor t = [201, 3200], are extracted from the time series and512\nTABLE II\nRESULTS FOR MACKEY\u2013GLASS TEST\nused as training data and 500 data points, for t = [5001, 5500], 513\nare used as validation data. The learning was stopped during 514\nthe validation to allow comparability with other published 515\napproaches. The aim is to predict the value z(t + 85) based 516\non four previous values, namely, z(t), z(t\u2212 6), z(t\u2212 12), and 517\nz(t\u2212 18). The precision is measured using nondimensional AQ30518\nerror index (NDEI) defined as the ratio of the root mean square 519\nerror over the standard deviation of the target data. 520\nThe model complexity is measured by the number of fuzzy 521\nrules generated. The results of the comparison of the proposed 522\napproach with several published results (see Table II) demon- 523\nstrate the superiority of simpl_eTS+ that offers the simplest 524\nstructure (number of fuzzy rules and number of inputs) with 525\nvery high precision. 526\nThe best result in terms of lower error NDEI = 0.316 is 527\nregistered when Conditions A, B, and D are active but not 528\nCondition C. This means that rules with low utility are not 529\nbeing removed from the rule base (in this case, the rule base 530\nhas 20 rules and all four inputs). Activating Condition C AQ31531\n(in addition to A and B) leads to removing two not much used 532\n(with low utility) rules. Adding to that, Condition D allows 533\nfurther removal of one input\/feature that contributes least to 534\nthe overall prediction. The structure of the system gets much 535\nsimpler (15 rules and 3 inputs only) with some deterioration in 536\nterms of the error (NDEI = 0.374\/5) which still compares fa- 537\nvorably with the alternative approaches taking into account that 538\nthey provide similar error level but use much more complicated 539\nrule bases. 540\nB. NOxNOx Emission Case Study 541\nThis data set was collected from car engines (courtesy of 542\nDr. E. Lughofer, Johannes Kepler University Linz, Linz, AQ32543\nAustria) to estimate the NOx content in the emissions that they 544\nproduce based on the variables that are easy to measure, such 545\nas pressure in the cylinders, engine torque, rotation speed, etc. 546\n[17]. In total, as much as 180 input variables are considered 547\nas potential inputs (these also include the physical variables 548\ndescribed previously taken at different time instants, i.e., with 549\ndifferent time delays). In [17], we used offline input variable 550\nselection method and common knowledge to determine the best 551\nfive inputs. Instead, in this paper, we start processing all the data 552\ninputs available and select the best subset automatically online. 553\nWhen we apply Condition D, the end result is a fuzzy model 554\nwith 101 inputs and 36 fuzzy rules; when we add all conditions 555\nIE\nEE\nPr\noo\nf\nANGELOV: FUZZILY CONNECTED MULTIMODEL SYSTEMS EVOLVING AUTONOMOUSLY FROM DATA STREAMS 9\nTABLE III\nRESULTS FOR NOx CAR EMISSION ANALYSIS\nTABLE IV\nPREDICTING PROPYLENE CONTENT OF DISTILLATION\n(A\u2013D), the model evolves to 13 fuzzy rules with seven inputs556\n(fuzzy sets) (see the results in Table III).557\nMoreover, the prediction error is lower with the model that558\nhas inputs selected automatically. In this experiment, again, a559\nmuch simpler structure rule base is achieved when all condi-560\ntions (A\u2013D) are applied, but if the error is the only criteria561\n(having a larger and much more complex structure), then con-562\nditions A, B, and D are most effective.563\nC. Propylene Case Study564\nThe propylene data set is collected from a chemical distilla-565\ntion process run at The Dow Chemical Company, U.S. (courtesy566\nof Dr. A. Kordon, [3]). The data set consists of 3000 readings567\nfrom 23 \u201chard\u201d sensors. These are used to model the propylene568\ncontent in the product output from the distillation. Some of the569\ninputs proved to be irrelevant to the model and, thus, bring570\nnoise. Therefore, the input selection is very relevant for this571\nparticular problem.572\nFrom Table IV, it is seen that using simpl_eTS+, a com-573\npact fuzzy model of seven fuzzy rules and two inputs (fuzzy574\nsets per rule) can be evolved online which also provides the575\nbest precision (see also Fig. 7).576\nThis demonstrates that highly compact, transparent, and577\ninterpretable models can be designed from data streams online578\nusing simpl_eTS+.AQ33 579\nFinal Rule base for propylene:580\nR1: IF (x1is 24.6) AND (x2is 26.3)581\nTHEN(y = \u22120.039 + x1 \u2212 0.324x2)582\nR2: IF (x1is 39.0) AND (x2is 43.5)583\nTHEN(y = \u22120.615 + 4.77x1 \u2212 0.340x2)584\nFig. 7. Predicting propylene content in the product using simpl_eTS+.\nThe horizontal axis represents the data samples taken every 15 min; the\nvertical axis represents the normalized values of the output y. A significant\ntechnological change takes place around sample 1300.\nR3: IF (x1is 46.2) AND (x2is 49.5) 585\nTHEN(y = \u22120.679 + 1.090x1 + 0.450x2) 586\nR4: IF (x1is 45.9) AND (x2is 49.9) 587\nTHEN(y = \u22121.340 + 5.570x1 \u2212 3.320x2) 588\nR5: IF (x1is 36.2) AND (x2is 43.5) 589\nTHEN(y = \u22120.002 + 0.320x1 \u2212 0.065x2) 590\nR6IF (x1is 31.6) AND (x2is 38.7) 591\nTHEN(y = \u22120.007 + 0.366x1 \u2212 0.129x2) 592\nR7IF (x1 is 40.6) AND (x2 is 39.5) 593\nTHEN (y = \u22120.527 + 0.406x1 \u2212 0.345x2Z) 594\nVII. CONCLUSION 595\nThis paper has introduced a novel computationally and 596\nconceptually simple scheme for a joint structure and pa- 597\nrameter identification of evolving NFS of a generic type 598\n(Fig. 1). The proposed new simpl_eTS+ approach also 599\nevolves the inputs of the multiple inputs and multiple out- 600\nput structure, removing rules\/ neurons based on their utility. AQ34601\nThe simpl_e_Clustering approach that is used for system 602\nstructure identification and evolution does not use directly the 603\ndata density\/potential (as it is in the eTS approach) and, thus, 604\nreduces the computational efforts by an order of magnitude 605\n(Table I). It differs from all other existing techniques of this 606\ntype by taking into account the accumulated proximity (den- 607\nsity) information without memorizing it directly. The proposed 608\nmethod allows complete autonomous knowledge extraction 609\nfrom streaming data (including model input selection online). 610\nThe proposed approach takes into account the shift in the data 611\nstream by evolving the structure of the system. Compared to the 612\nother well-known concepts of adaptive systems, the proposed 613\none allows a much higher level of flexibility and achieves better 614\nresults on real-life data. The root of its better efficiency is in 615\nthe simpler model structure achieved based on data density 616\ndetermined recursively (recursive noniterative manner of cal- 617\nculations is the key why the approach is much faster); the better 618\nprecision is achieved due to flexibility (model structure evolves 619\nand follows as a reaction to the dynamically changing data 620\npattern as opposed to averaging all data or using less efficient 621\nclustering techniques or assumptions). AQ35622\nIE\nEE\nPr\noo\nf\n10 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART B: CYBERNETICS\nThe main contributions of this paper are:623\n1) the novel and computationally simple approach to data624\nspace partitioning by recursive evolving clustering based625\non the relative position to the mean of the overall data626\nsimpl_e_Clustering;627\n2) the learning technique for online structure evolution as a628\nreaction to the shifts in the data distribution;629\n3) the method for online input\/feature selection630\n(Condition D);631\n4) the method for system structure simplification based on632\nutility (Condition C);633\n5) the novel graphical illustration of the spatial-temporal634\nevolution of the data stream (Fig. 3).635\nThe application domain for this computationally efficient636\ntechnique ranges from simple clustering-based techniques for637\npattern recognition, image segmentation, vector quantization,638\netc., to more general modeling, prognostics, classification, and639\ntime-series prediction problems in various application areas,640\ne.g., intelligent sensors, mobile robotics, advanced manufactur-641\ning processes, sensor networks, etc. The proposed approach can642\nbe used as a basis to build adaptive (with evolving structure)643\nself-calibrating inferential (soft) sensors for chemical and oil644\nindustries [3] and can also be implemented as an embedded645\nsystem.646\nAPPENDIX647\nA. Lemma 1648\nIn eClustering [19], the main condition to form new clusters649\nand respectively new fuzzy rules was described as650\nCondition A_old:651\nIF\n(\nDk(zk)>\nN\nmax\ni=1\nDk(zi\u2217)\n)\nOR\n(\nDk(zk)<\nN\nmin\ni=1\nDk(zi\u2217)\n)\n.\n(A1)\nStarting from Condition A_old, it can be proven that a signifi-652\ncantly simpler Condition A can be derived that is based on the653\ndensity increment \u03b4(k), i.e., (8) is identical to Condition A_old654\n(used in eTS [12], [19], [28]), but it does not require the density655\nto be calculated and updated for every new data point.656\nProof [23]: One can express the partial density increment657\nas a difference between the densities of the following:658\n1) the new data point zk calculated at the time instant k,659\nDk(zk);660\n2) the density of the cluster\/rule center calculated at the 661\nsame time instant Dk(zi\u2217) 662\n\u0394i\u2217k = Dk(zk)\u2212Dk(zi\u2217). (A2)\nThe meaning of the partial density increment is \u201chow much 663\nthe density will change if we measure it in the new data point 664\ninstead of the previous i\u2217th cluster center (focal point of a 665\nrule).\u201d We can express this quantity from the definition of the 666\ndensity as a Cauchy function [12] 667\nD(z) =\n1\n1 + 1k\u22121\nk\u2211\ni=1\n\u2016z \u2212 zi\u20162\n(A3)\nwhich leads to (A4), shown at the bottom of the page, 668\nwhere Li = {1, 2, . . . , i \u2217 \u22121, i \u2217+1, . . . , k \u2212 1} denotes a set 669\nof (k \u2212 2) indices (all indices from 1 to (k \u2212 1) excluding the 670\nindex i\u2217. 671\nReorganizing (A4) gives 672\n\u0394i\u2217k =\n(k \u2212 2)\nn+m\u2211\nj=1\n{((\nzi\u2217j\n)2\u2212z2jk)+2 (zjk\u2212zi\u2217j ) \u2211\nl\u2208Li\nzjl\n}\n(k \u2212 1) denom1 \u2217 denom2\n(A5)\nwhere 673\ndenom1 =1 +\n\u2211\nl\u2208Li\nn+m\u2211\nj=1\n{\nz2jk \u2212 2zjkzjl + z2jl\n}\nk \u2212 1\n+\nn+m\u2211\nj=1\n{\nz2jk \u2212 2zjkzi\u2217j +\n(\nzi\u2217j\n)2}\nk \u2212 1\ndenom2 =1 +\n\u2211\nl\u2208Li\nn+m\u2211\nj=1\n{(\nzi\u2217j\n)2 \u2212 2zi\u2217j zji + z2jl}\nk \u2212 1\n+\nn+m\u2211\nj=1\n{\nz2jk \u2212 2zjkzi\u2217j +\n(\nzi\u2217j\n)2}\nk \u2212 1 .\nEquation (A5) can be further simplified as follows: 674\n\u0394i\u2217k =\n(k \u2212 2)\n{\nn+m\u2211\nj=1\n{(\nzi\u2217j\n)2 \u2212 z2jk + 2 (zjk \u2212 zi\u2217j ) zi\u2217j }\n}\n(k \u2212 1) denom1 \u2217 denom2\n(A6)\n\u0394i\u2217k =\n1\n1 + 1k\u22121\n{\u2211\nl\u2208Li\nn+m\u2211\nj=1\n(\nz2jk \u2212 2zjkzjl + z2jl\n)\n+\nn+m\u2211\nj=1\n(\nz2jk \u2212 2zjkzi\u2217j +\n(\nzi\u2217j\n)2)}\n\u2212 1\n1 + 1k\u22121\n{\u2211\nl\u2208Li\nn+m\u2211\nj=1\n{(\nzi\u2217j\n)2 \u2212 2zi\u2217j zjl + z2jl}+ n+m\u2211\nj=1\n{\nz2jk \u2212 2zjkzi\u2217j +\n(\nzi\u2217j\n)2}} (A4)\nIE\nEE\nPr\noo\nf\nANGELOV: FUZZILY CONNECTED MULTIMODEL SYSTEMS EVOLVING AUTONOMOUSLY FROM DATA STREAMS 11\nwhere zi\u2217j = (1\/(k \u2212 2))\n\u2211\nl\u2208Li z\nl\nj is the mean of all points675\nexcept the i\u2217th cluster\/rule center and the last (k \u2212 1)th point.676\nSince both denominators (denom1 and denom2) express677\nsums of distances, they are positive by definition. Similarly,678\n(k \u2212 2)\/(k \u2212 1) is also positive. Therefore, the sign of \u03b3i\u2217k679\ndetermines the sign of the density increment \u0394i\u2217k , where680\n\u03b3i\u2217jk =\n(\nzi\u2217j\n)2 \u2212 z2jk + 2 (zjk \u2212 zi\u2217j ) zi\u2217j . (A7)\nIf the sign of the density increment\u0394i\u2217k is the same for each681\nrule, then the new data point brings a spatial innovation. OneAQ36 682\ncan judge this by observing the sign of the \u2211n+mj=1 \u03b3i\u2217jk. If it is683\nthe same for all previously existing (N) rules, then the density684\nincrement takes place in respect to each previously existing685\nrule. The overall sign of the density increment in respect to all686\npreviously existing N rules can be calculated by687\n\u03b4k =\n\u2223\u2223\u2223\u2223\u2223\u2223\nN\u2211\ni=1\nsign\nn+m\u2211\nj=1\n\u03b3i\u2217jk\n\u2223\u2223\u2223\u2223\u2223\u2223 . (A8)\nThe mean zi\u2217j used in (A6) can be calculated by subtracting688\nthe value of the i\u2217th center from the mean of all points up to689\n(k \u2212 1)th690\nzi\u2217j =\n1\n(k \u2212 2)\n\u2211\nl\u2208Li\nzjl =\n1\n(k \u2212 2)\n(\nk\u22121\u2211\ni=1\nzjl \u2212 zi\u2217j\n)\n=\n(k \u2212 1)\n(k \u2212 2)zj(k\u22121) \u2212\n1\n(k \u2212 2)z\ni\u2217\nj (A9)\nwhere zj(k\u22121) = (1\/(k \u2212 1))\n\u2211k\u22121\nl=1 zjl is the mean of all points691\nup to the (k \u2212 1)th.692\nIt can easily be calculated recursively693\nzj(k\u22121) =\n(k \u2212 2)\n(k \u2212 1)zj(k\u22122) +\n1\n(k \u2212 1)zj(k\u22121).\nIf the new data point does not bring density increment to all of694\nthe existing clusters\/rules695\n0 < \u03b4k < N (A10)\nthen it can be interpolated by the existing rules represented by696\nthe existing centers [see the area between the two concentric697\ncircles in Fig. 3(a)]. Thus, in this case, we do not change the698\ncluster structure.699\nIf it does bring a density increment to all of the previously700\nexisting rules, however701\n\u03b4k = N (A11)\na new cluster\/rule is formed around this point. \u0002702\nB. Lemma 2703\nThe density\/potential D measured at the global mean z is the704\nmaximal density possible705\nD(z) =\nk\nmax\ni=1\nD(zi) (A12)\nwhere z denotes the global mean defined as z = (1\/k)\n\u2211k\ni=1 zi. 706\nProof: First, we can rewrite (A12) as D(z) > D(zi);\u2200i = 707\n1, 2, . . . , k. 708\nThen, we can assume that there exists a point which violates 709\nthis condition \u2203j|D(zj) > D(z), and finally, we can prove that 710\nthis assumption is wrong (that it cannot hold). For any point 711\nother than the global mean z\u2217, we have 712\nD(z\u2217) =\n1\n1 + 1k\u22121\nk\u2211\ni=1\n\u2016z\u2217 \u2212 zi\u20162\n. (A13)\nNow, if we assume that D(z\u2217) > D(z), we only need to prove 713\nthat this is impossible 714\n1\n1+ 1k\u22121\nk\u2211\ni=1\n\u2016z\u2217\u2212zi\u20162\n>\n1\n1 + 1k\u22121\nk\u2211\ni=1\n\u2016z \u2212 zi\u20162\n(A14)\nk\u2211\ni=1\nn+m\u2211\nj=1\n(\nz\u2217j \u2212 zij\n)2\n<\nk\u2211\ni=1\nn+m\u2211\nj=1\n(zj \u2212 zij)2 (A15)\nk\nn+m\u2211\nj=1\n(\nz\u2217j\n)2\u22122n+m\u2211\nj=1\nz\u2217j\nk\u2211\ni=1\nzij +\nn+m\u2211\nj=1\nk\u2211\ni=1\n(zij)2\n<k\nn+m\u2211\nj=1\n(zj)2\u22122\nn+m\u2211\nj=1\nzj\nk\u2211\ni=1\nzij\n+\nn+m\u2211\nj=1\nk\u2211\ni=1\n(zij)2 (A16)\nk\nn+m\u2211\nj=1\n(\nz\u2217j\n)2\u22122n+m\u2211\nj=1\nz\u2217j\nk\u2211\ni=1\nzij <k\nn+m\u2211\nj=1\n(zj)2 \u2212 2\nn+m\u2211\nj=1\nzj\nk\u2211\ni=1\nzij .\n(A17)\nNow, from the definition of the global mean, we can write 715\u2211k\ni=1 zi = kz or respectively\n\u2211k\ni=1 zij = kzj . Taking this into 716\naccount, we can rewrite (A17) into 717\nk\nn+m\u2211\nj=1\n(\nz\u2217j\n)2 \u2212 2k n+m\u2211\nj=1\nz\u2217jzj < k\nn+m\u2211\nj=1\n(zj)2 \u2212 2k\nn+m\u2211\nj=1\nzjzj .\n(A18)\nBy reorganizing, we got 718\nn+m\u2211\nj=1\n(\nz\u2217j\n)2 \u2212 2 n+m\u2211\nj=1\nzjz\n\u2217\nj \u2212\nn+m\u2211\nj=1\n(zj)2 + 2\nn+m\u2211\nj=1\nzjzj < 0.\n(A19)\nSimplifying the last two terms, we got 719\nn+m\u2211\nj=1\n(\nz\u2217j\n)2 \u2212 2 n+m\u2211\nj=1\nzjz\n\u2217\nj +\nn+m\u2211\nj=1\n(zj)2 < 0 (A20)\nIE\nEE\nPr\noo\nf\n12 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART B: CYBERNETICS\nwhich can be, finally, rewritten as720\nn+m\u2211\nj=1\n(\nz\u2217j \u2212 zj\n)2\n< 0. (A21)\nObviously, this can never be satisfied, and our initial assumption721\nwas wrong. Thus, D(z) > D(zi);\u2200i = 1, 2, . . . , k \u0002722\nC. Lemma 3723\nThe mean of the standardized data tends asymptotically to724\nzero with the number of data tending to infinity (see Fig. 9)AQ37 725\nlim\nk\u2192\u221e\nzst \u2192 0. (A22)\nProof: The mean of the standardized data is derived by726\napplying both the mean and standardization operations. Let usAQ38 727\nstart with the offline case728\nzst =\n1\nk\nk\u2211\ni=1\nzi \u2212 z\n\u03c3\n=\n1\nk\nk\u2211\ni=1\nzi\n\u03c3\n\u2212 1\nk\nk\u2211\ni=1\nz\n\u03c3\n=\n1\n\u03c3\n(z \u2212 z) = 0.\n(A23)\nIn the online case, we can prove that when k \u2192\u221e, the same729\ncondition (A22) holds but in asymptotical sense730\nzk+1 =\n1\nk + 1\nk+1\u2211\ni=1\nzi =\nk\nk + 1\nzk +\nzk+1\nk + 1\n. (A24)\nHowever,731\nlim\nk\u2192\u221e\nk\nk + 1\n=1 (A25a)\nlim\nk\u2192\u221e\nxk+1\nk + 1\n=0 for bounded zk+1. (A25b)\nCombining (A24) and (A25a) and (A25b), we have732\nlim\nk\u2192\u221e\nzk+1 = zk (A26)\nand finally, this leads to limk\u2192\u221e zst = 0 (see Fig. 5). \u0002733\nACKNOWLEDGMENT734\nThe author would like to thank Dr. N. Pal for the comments735\nhe made to the manuscript from 2009 to the beginning of 2010.AQ39 736\nREFERENCES737\n[1] T. Takagi and M. Sugeno, \u201cFuzzy identification of systems and its ap-738\nplication to modeling and control,\u201d IEEE Trans. Syst., Man, Cybern.,739\nvol. SMC-15, no. 1, pp. 116\u2013132, Feb. 1985.740\n[2] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statis-741\ntical Learning: Data Mining, Inference and Prediction. Heidelberg,742\nGermany: Springer-Verlag, 2001.743\n[3] P. Angelov and A. Kordon, \u201cAdaptive inferential sensors based on evolv-744\ning fuzzy models: An industrial case study,\u201d IEEE Trans. Syst., Man,745\nCybern. B, Cybern., vol. 40, no. 2, pp. 529\u2013539, Apr. 2010.746\n[4] G. Leng, T. M. McGuinnity, and G. Prasad, \u201cAn approach for on-747\nline extraction of fuzzy rules using a self-organising neural network,\u201d748\nFuzzy Sets Syst., vol. 150, no. 2, pp. 211\u2013243, Mar. 2005.749\n[5] N. Sundararajan, H.-J. Rong, G.-B. Huang, and P. Saratchandran, \u201cSe- 750\nquential adaptive fuzzy inference systems for non-linear systems identifi- 751\ncation and prediction,\u201d Fuzzy Sets Syst., vol. 157, no. 9, pp. 1260\u20131275, 752\nMay 2006. 753\n[6] G. Widmer and M. Kubat, \u201cLearning in the presence of concept drift and 754\nhidden contexts,\u201d Mach. Learn., vol. 23, no. 1, pp. 69\u2013101, Apr. 1996. 755\n[7] J. Koza, Genetic Programming: On the Programming of Computers by 756\nMeans of Natural Selection. Cambridge, MA: MIT Press, 1992. 757\n[8] E. Lima, M. Hell, R. Ballini, and F. Gomide, \u201cEvolving fuzzy mod- 758\neling using participatory learning,\u201d in Evolving Intelligent Systems, 759\nP. Angelov, D. Filev, and N. Kasabov, Eds. Hoboken, NJ: Wiley, 2010, 760\npp. 67\u201386. 761\n[9] L. Ljung, System Identification: Theory for the User. Englewood Cliffs, 762\nNJ: Prentice-Hall, 1999. 763\n[10] J. J. Rubio, D. M. Vazquez, and J. Pacheco, \u201cBack propagation to train 764\nan evolving radial basis function neural network,\u201d Evolving Syst., vol. 1, 765\nno. 3, pp. 173\u2013180, Oct. 2010. 766\n[11] D. Specht, \u201cA general regression neural network,\u201d IEEE Trans. Neural 767\nNetw., vol. 2, no. 6, pp. 568\u2013576, Nov. 1991. 768\n[12] P. Angelov and D. Filev, \u201cAn approach to on-line identification of evolving 769\nTakagi-Sugeno models,\u201d IEEE Trans. Syst., Man, Cybern. B, Cybern., 770\nvol. 34, no. 1, pp. 484\u2013498, 2004. AQ40771\n[13] U. M. Fayyad, G. Piatetsky-Shapiro, and P. Smyth, From Data Mining to 772\nKnowledge Discovery: An Overview, Advances in Knowledge Discovery 773\nand Data Mining. Cambridge, MA: MIT Press, 1996. 774\n[14] N. Kasabov and Q. Song, \u201cDENFIS: Dynamic evolving neural-fuzzy in- 775\nference system and its application for time-series prediction,\u201d IEEE Trans. 776\nFuzzy Syst., vol. 10, no. 2, pp. 144\u2013154, Apr. 2002. 777\n[15] M. Norgaard, O. Ravn, N. Poulsen, and L. Hansen, Eds., Neural Networks 778\nfor Modelling and Control of Dynamic Systems. New York: Springer- 779\nVerlag, 2000. 780\n[16] J. S. R. Jang, \u201cANFIS: Adaptive network-based fuzzy inference sys- 781\ntems,\u201d IEEE Trans. Syst., Man, Cybern., vol. 23, no. 3, pp. 665\u2013685, 782\nMay\/Jun. 1993. 783\n[17] P. Angelov and E. Lughofer, \u201cA comparative study of two approaches 784\nfor data-driven design of evolving fuzzy systems: eTS and FLEXFIS,\u201d 785\nInt. J. Gen. Syst., vol. 37, no. 1, pp. 45\u201367, 2008. 786\n[18] S. L. Chiu, \u201cFuzzy model identification based on cluster estimation,\u201d 787\nJ. Intell. Fuzzy Syst., vol. 2, no. 3, pp. 267\u2013278, Sep. 1994. 788\n[19] P. Angelov and X. Zhou, \u201cOn line learning fuzzy rule-based system struc- 789\nture from data streams,\u201d in Proc. IEEE Int. Conf. Fuzzy Syst., Hong Kong, 790\nJun. 1\u20136, 2008, pp. 915\u2013922. 791\n[20] P. Angelov and X. Zhou, \u201cEvolving fuzzy-rule-based classifiers from 792\ndata streams,\u201d IEEE Trans. Fuzzy Syst., vol. 16, no. 6, pp. 1462\u20131475, 793\nDec. 2008. 794\n[21] P. Angelov, J. Victor, A. Dourado, and D. Filev, \u201cOn-line evolution 795\nof Takagi-Sugeno fuzzy models,\u201d in Proc. 2nd IFAC Workshop Adv. 796\nFuzzy\/Neural Control, Oulu, Finland, Sep. 16\u201317, 2004, pp. 67\u201372. 797\n[22] R. Yager and D. Filev, Essentials of Fuzzy Modeling and Control. 798\nHoboken, NJ: Wiley, 1994. 799\n[23] P. Angelov, Machine learning (collaborative systems), Patent 800\nWO2008053161, priority date: Nov. 1, 2006; international filing 801\ndate: Oct. 23, 2007. AQ41802\n[24] E. Mazor, A. Averbuch, Y. Bar-Shalom, and J. Dayan, \u201cIndependent mul- 803\ntiple model methods in target tracking: A survey,\u201d IEEE Trans. Aerosp. 804\nElectron. Syst., vol. 34, no. 1, pp. 103\u2013123, Jan. 1998. 805\n[25] T. Martin, \u201cFuzzy sets in the fight against digital obesity,\u201d Fuzzy Sets Syst., 806\nvol. 156, no. 3, pp. 411\u2013417, Dec. 2005. 807\n[26] J. A. Iglesias, P. Angelov, A. Ledezma, and A. Sanchis, \u201cEvolving classifi- 808\ncation of agents\u2019 behaviours: A general approach,\u201d Evolving Syst., vol. 1, 809\nno. 3, pp. 161\u2013171, Oct. 2010. 810\n[27] P. Angelov and X.-W. Zhou, \u201cEvolving fuzzy classifier for real-time 811\nnovelty detection and landmark recognition by a mobile robot,\u201d in Mobile 812\nRobots: The Evolutionary Approach, N. Nedjah, L. dos Santos Coelho, 813\nand L. de Macedo Mourelle, Eds. Heidelberg, Germany: Springer- 814\nVerlag, Mar. 2007, pp. 95\u2013124. 815\n[28] P. Angelov and D. Filev, \u201cFlexible models with evolving structure,\u201d Int. J. 816\nIntell. Syst., vol. 19, no. 4, pp. 327\u2013340, Apr. 2004. 817\n[29] D. Wang, X.-J. Zeng, and J. A. Keane, \u201cA structure evolving learning 818\nmethod for fuzzy systems,\u201d Evolving Syst., vol. 1, no. 2, pp. 83\u201395, 819\nSep. 2010. 820\n[30] F. M. Pouzols and A. Lendasse, \u201cEvolving fuzzy optimally pruned ex- 821\ntreme learning machine for regression problems,\u201d Evolving Syst., vol. 1, 822\nno. 1, pp. 43\u201358, Aug. 2010. 823\n[31] H. Soleimani-B., C. Lucas, and B. N. Araabi, \u201cRecursive Gath\u2013Geva 824\nclustering as a basis for evolving neuro-fuzzy modelling,\u201d Evolving Syst., 825\nvol. 1, no. 1, pp. 59\u201371, Aug. 2010. 826\nIE\nEE\nPr\noo\nf\nANGELOV: FUZZILY CONNECTED MULTIMODEL SYSTEMS EVOLVING AUTONOMOUSLY FROM DATA STREAMS 13\nPlamen Angelov (SM\u2019XX) received the M.Eng.AQ42 827\ndegree in electronics and automation from Sofia828\nTechnical University, Sofia, Bulgaria, in 1989, and829\nthe Ph.D. degree from the Bulgaria Academy of830\nSciences, Sofia, in 1993.831\nHe spent over ten years as a Research Fellow832\nworking on computational intelligence and control833\nsystems. During 1995\u20131996, he was with Hans-834\nKnoell Institute, Jena, Germany. In 1997, he was835\na Visiting Researcher at the Catholic University,836\nLeuvain-la-neuve, Belgium. In 1998, he became a837\nResearch Fellow at Loughborough University, Loughborough, U.K. In 2003,838\nhe joined Lancaster University, Lancaster, U.K., as a Lecturer, where he is839\ncurrently a Senior Lecturer (equivalent to Associate Professor). He was a Vis-840\niting Professor at the Ostfalia University of Applied Sciences, Braunschweig,AQ43 841\nGermany, in 2007, and the University Carlos III, Madrid, Spain, in 2010. He842\nwas also a Visiting Fellow at the University of Campinas, Campinas, Brazil,843\nin 2005, and the Johannes Kepler University of Linz, Linz, Austria, in 2006.AQ44 844\nHe has authored or coauthored over 150 peer-reviewed publications, including845\nthe monograph Evolving Rule Based Models: A Tool for Design of Flexible846\nAdaptive Systems (Springer-Verlag, 2002), a number of books, book chapters,847\nover 40 peer-reviewed journal papers, including eight IEEE TRANSACTIONS848\npapers, and is a holder of a patent in machine learning in 2006. He has849\na wide portfolio of externally funded research including sponsors such as850\nU.K. Research Councils, European Commission, U.K. Ministry of Defence,AQ45 851\nUSA-based American Society of Heating, Refrigeration and Air ConditioningAQ46 852\nEngineers, industries (BAE Systems, 4S Information Systems), U.K. Royal853\nSociety, etc. He is the Editor-in-Chief of the Springer journal Evolving Systems.854\nDr. Angelov is the Chair of the Standards Committee, Computational Intel-855\nligence Society, IEEE, and the founding Chair of the Task Force on Adaptive856\nand Evolving Fuzzy Systems.857\nIE\nEE\nPr\noo\nf\nAUTHOR QUERIES\nAUTHOR PLEASE ANSWER ALL QUERIES\n*Note that your paper will incur overlength page charges of $175 per page. The page limit for regular papers\nis 12 pages, and the page limit for correspondence papers is 6 pages.\nAQ1 = The word \u201ctrust\u201d was changed to \u201cthrust.\u201d Please check if appropriate and correct if necessary.\nAQ2 = The sentence was reworded for clarity. Please check if the original thought was retained and correct\nif necessary.\nAQ3 = The sentence was reworded for clarity. Please check if the original thought was retained and correct\nif necessary.\nAQ4 = \u201cExabyte\u201d was changed to \u201cEB.\u201d Please check if appropriate and correct if necessary.\nAQ5 = The sentence was reworded for clarity. Please check if the original thought was retained and correct\nif necessary.\nAQ6 = The acronym \u201cSOFNN\u201d was defined as \u201cself-organized fuzzy neural network.\u201d Please check if\nappropriate and correct if necessary.\nAQ7 = Please provide the expanded form of the acronym \u201cSAFIS.\u201d\nAQ8 = Please provide the expanded form of the acronym \u201cePL.\u201d\nAQ9 = Please provide the expanded form of the acronym \u201ceTS.\u201d\nAQ10 = Please provide the expanded form of the acronym \u201cSELM.\u201d\nAQ11 = The acronym \u201cOP-ELM\u201d was defined as \u201coptimally pruned extreme learning machine.\u201d Please\ncheck if appropriate and correct if necessary.\nAQ12 = The acronym \u201cENFM\u201d was defined as \u201cenabling new function mode.\u201d Please check if appropriate\nand correct if necessary.\nAQ13 = Please provide the expanded form of the acronym \u201cexTS.\u201d\nAQ14 = Please provide the expanded form of the acronym \u201cFLEXFIS.\u201d\nAQ15 = There is no Condition E in Fig. 2. Please check.\nAQ16 = \u201cSimplified Mamdani\u201d was abbreviated as \u201csM.\u201d Please check if appropriate and correct if\nnecessary.\nAQ17 = The sentence was reworded for clarity. Please check if the original thought was retained and correct\nif necessary.\nAQ18 = The word \u201cones\u201d was inserted here for clarity. Please check if appropriate and correct if necessary.\nAQ19 = The word \u201cor\u201d was deleted in this sentence per IEEE instruction. Please check if appropriate and\ncorrect if necessary.\nAQ20 = The word \u201cbeing\u201d was deleted in this sentence. Please check if appropriate and correct if necessary.\nAQ21 = The sentence was reworded for clarity. Please check if the original thought was retained and correct\nif necessary.\nAQ22 = The word \u201cit\u201d is referred in this sentence as Condition B. Please check if appropriate and correct if\nnecessary.\nAQ23 = The sentence was reworded for clarity. Please check if the original thought was retained and correct\nif necessary.\nAQ24 = The sentence was reworded for clarity. Please check if the original thought was retained and correct\nif necessary.\nAQ25 = The sentence was reworded for clarity. Please check if the original thought was retained and correct\nif necessary.\nAQ26 = The sentence was reworded for clarity. Please check if the original thought was retained and correct\nif necessary.\nIE\nEE\nPr\noo\nf\nANGELOV: FUZZILY CONNECTED MULTIMODEL SYSTEMS EVOLVING AUTONOMOUSLY FROM DATA STREAMS 15\nAQ27 = The acronym \u201cPCA\u201d was defined as \u201cprincipal component analysis.\u201d Please check if appropriate\nand correct if necessary.\nAQ28 = Please provide the expanded form of the acronym \u201cGP.\u201d\nAQ29 = The sentence was reworded for clarity. Please check if the original thought was retained and correct\nif necessary.\nAQ30 = The sentence was reworded for clarity. Please check if the original thought was retained and correct\nif necessary.\nAQ31 = The word \u201cif\u201d is removed in this sentence for clarity. Please check if appropriate and correct if\nnecessary.\nAQ32 = The \u201cUniversity of Linz Austria\u201d was changed to \u201cJohannes Kepler University Linz.\u201d Please check\nif appropriate and correct if necessary.\nAQ33 = The following data were captured as algorithm. Please check if appropriate and correct if necessary.\nAQ34 = The acronym \u201cMIMO\u201d was defined as \u201cmultiple inputs and multiple outputs.\u201d Please check if\nappropriate and correct if necessary.\nAQ35 = The sentence was reworded for clarity. Please check if the original thought was retained and correct\nif necessary.\nAQ36 = The word \u201cthan\u201d was changed to \u201cthen.\u201d Please check if appropriate and correct if necessary.\nAQ37 = Fig. 9 was not found in the manuscript. Please check.\nAQ38 = The phrase \u201cthe mean and standardization operations\u201d was removed from the parentheses for clarity.\nPlease check if appropriate and correct if necessary.\nAQ39 = The sentence was reworded for clarity. Please check if the original thought was retained and correct\nif necessary.\nAQ40 = Please provide month of publication in Ref. [12].\nAQ41 = Please provide complete date the patent was issued in Ref. [23].\nAQ42 = Please provide the IEEE membership history of Plamen Angelov.\nAQ43 = Braunschweig was captured as the city campus of the University of Applied Sciences. Please check\nif appropriate and correct if necessary.\nAQ44 = The University of Linz was changed to Johannes Kepler University of Linz. Please check if\nappropriate and correct if necessary.\nAQ45 = The acronym \u201cEC\u201d was defined as \u201cEuropean Commission.\u201d Please check if appropriate and correct\nif necessary.\nAQ46 = The acronym \u201cASHRAE\u201d was defined as \u201cAmerican Society of Heating, Refrigerating and Air\nConditioning Engineers.\u201d Please check if appropriate and correct if necessary.\nEND OF ALL QUERIES\n"}