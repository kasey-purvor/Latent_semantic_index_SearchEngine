{"doi":"10.1117\/12.430855","coreId":"66639","oai":"oai:dro.dur.ac.uk.OAI2:647","identifiers":["oai:dro.dur.ac.uk.OAI2:647","10.1117\/12.430855"],"title":"Controlling perceived depth in stereoscopic images.","authors":["Jones,  G. R.","Lee,  D.","Holliman,  N. S.","Ezra,  D."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":["Woods, Andrew J.","Bolas, Mark T.","Merrit, John O.","Benton, Stephen A."],"datePublished":"2001-06-22","abstract":"Stereoscopic images are hard to get right, and comfortable images are often only produced after repeated trial and error. The\\ud\nmain difficulty is controlling the stereoscopic camera parameters so that the viewer does not experience eye strain or double\\ud\nimages from excessive perceived depth. Additionally, for head tracked displays, the perceived objects can distort as the viewer\\ud\nmoves to look around the displayed scene. We describe a novel method for calculating stereoscopic camera parameters with\\ud\nthe following contributions:\\ud\n(1) Provides the user intuitive controls related to easily measured physical values. (2) For head tracked displays; necessarily\\ud\nensures that there is no depth distortion as the viewer moves. (3) Clearly separates the image capture camera\/scene space from\\ud\nthe image viewing viewer\/display space. (4) Provides a transformation between these two spaces allowing precise control of\\ud\nthe mapping of scene depth to perceived display depth.\\ud\nThe new method is implemented as an API extension for use with OpenGL, a plug-in for 3D Studio Max and a control\\ud\nsystem for a stereoscopic digital camera. The result is stereoscopic images generated correctly at the first attempt, with precisely\\ud\ncontrolled perceived depth. A new analysis of the distortions introduced by different camera parameters was undertaken","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/66639.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/647\/1\/647.pdf","pdfHashValue":"151b8564c4d0a84cd98336c067aafb301c83519e","publisher":"SPIE","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:647<\/identifier><datestamp>\n      2017-03-10T15:38:01Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Controlling perceived depth in stereoscopic images.<\/dc:title><dc:creator>\n        Jones,  G. R.<\/dc:creator><dc:creator>\n        Lee,  D.<\/dc:creator><dc:creator>\n        Holliman,  N. S.<\/dc:creator><dc:creator>\n        Ezra,  D.<\/dc:creator><dc:description>\n        Stereoscopic images are hard to get right, and comfortable images are often only produced after repeated trial and error. The\\ud\nmain difficulty is controlling the stereoscopic camera parameters so that the viewer does not experience eye strain or double\\ud\nimages from excessive perceived depth. Additionally, for head tracked displays, the perceived objects can distort as the viewer\\ud\nmoves to look around the displayed scene. We describe a novel method for calculating stereoscopic camera parameters with\\ud\nthe following contributions:\\ud\n(1) Provides the user intuitive controls related to easily measured physical values. (2) For head tracked displays; necessarily\\ud\nensures that there is no depth distortion as the viewer moves. (3) Clearly separates the image capture camera\/scene space from\\ud\nthe image viewing viewer\/display space. (4) Provides a transformation between these two spaces allowing precise control of\\ud\nthe mapping of scene depth to perceived display depth.\\ud\nThe new method is implemented as an API extension for use with OpenGL, a plug-in for 3D Studio Max and a control\\ud\nsystem for a stereoscopic digital camera. The result is stereoscopic images generated correctly at the first attempt, with precisely\\ud\ncontrolled perceived depth. A new analysis of the distortions introduced by different camera parameters was undertaken.<\/dc:description><dc:subject>\n        Graphics systems<\/dc:subject><dc:subject>\n         Human factors<\/dc:subject><dc:subject>\n         Rendering<\/dc:subject><dc:subject>\n         Virtual reality<\/dc:subject><dc:subject>\n         Stereoscopic<\/dc:subject><dc:subject>\n         3D display.<\/dc:subject><dc:publisher>\n        SPIE<\/dc:publisher><dc:source>\n        Woods, Andrew J. & Bolas, Mark T. & Merrit, John O. & Benton, Stephen A. (Eds.). (2001). Stereoscopic displays and applications VIII. Bellingham, WA: SPIE, pp. 42-53, Proceedings of SPIE(4297)<\/dc:source><dc:contributor>\n        Woods, Andrew J.<\/dc:contributor><dc:contributor>\n        Bolas, Mark T.<\/dc:contributor><dc:contributor>\n        Merrit, John O.<\/dc:contributor><dc:contributor>\n        Benton, Stephen A.<\/dc:contributor><dc:date>\n        2001-06-22<\/dc:date><dc:type>\n        Book chapter<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:647<\/dc:identifier><dc:identifier>\n        issn:0277-786X<\/dc:identifier><dc:identifier>\n        doi:10.1117\/12.430855<\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/647\/<\/dc:identifier><dc:identifier>\n        https:\/\/doi.org\/10.1117\/12.430855<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/647\/1\/647.pdf<\/dc:identifier><dc:rights>\n        G. R. Jones, D. Lee, N. S. Holliman, D. Ezra, \u201c'Controlling perceived depth in stereoscopic images\u201d, Proceedings of SPIE : Stereoscopic displays and applications XII : 2001, A. J. Woods, Mark T. Bolas, J. O. Merritt, Stephen A. Benton, Editors, 4297, 42-53, (2001).\\ud\nCopyright 2001 Society of Photo-Optical Instrumentation Engineers. One print or electronic copy may be made for personal use only. Systematic reproduction and distribution, duplication of any material in this paper for a fee or for commercial purposes, or modification\\ud\nof the content of the paper are prohibited.<\/dc:rights><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["issn:0277-786X","0277-786x"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2001,"topics":["Graphics systems","Human factors","Rendering","Virtual reality","Stereoscopic","3D display."],"subject":["Book chapter","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n21 August 2009\nVersion of attached file:\nPublished Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nJones, G. R. and Lee, D. and Holliman, N. S. and Ezra, D. (2001) \u2019Controlling perceived depth in stereoscopic\nimages.\u2019, in Proceedings of SPIE : Stereoscopic displays and applications XII : 2001. Bellingham WA: SPIE,\npp. 42-53.\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1117\/12.430855\nPublisher\u2019s copyright statement:\nG. R. Jones, D. Lee, N. S. Holliman, D. Ezra, \u2019Controlling perceived depth in stereoscopic images, Proceedings of SPIE\n: Stereoscopic displays and applications XII : 2001, A. J. Woods, Mark T. Bolas, J. O. Merritt, Stephen A. Benton,\nEditors, 4297, 42-53, (2001). Copyright 2001 Society of Photo-Optical Instrumentation Engineers. One print or\nelectronic copy may be made for personal use only. Systematic reproduction and distribution, duplication of any\nmaterial in this paper for a fee or for commercial purposes, or modification of the content of the paper are prohibited.\nAdditional information:\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\nControlling Perceived Depth in Stereoscopic Images\nGraham Jones, Delman Lee, Nicolas Holliman*, David Ezra,\nSharp Laboratories of Europe Ltd., Edmund Halley Road,\nOxford Science Park, Oxford, OX4 4GB, UK\n*Presently at Codemasters, Warwickshire, UK.\nABSTRACT\nStereoscopic images are hard to get right, and comfortable images are often only produced after repeated trial and error. The\nmain difficulty is controlling the stereoscopic camera parameters so that the viewer does not experience eye strain or double\nimages from excessive perceived depth. Additionally, for head tracked displays, the perceived objects can distort as the viewer\nmoves to look around the displayed scene. We describe a novel method for calculating stereoscopic camera parameters with\nthe following contributions:\n(1) Provides the user intuitive controls related to easily measured physical values. (2) For head tracked displays; necessarily\nensures that there is no depth distortion as the viewer moves. (3) Clearly separates the image capture camera\/scene space from\nthe image viewing viewer\/display space. (4) Provides a transformation between these two spaces allowing precise control of\nthe mapping of scene depth to perceived display depth.\nThe new method is implemented as an API extension for use with OpenGL, a plug-in for 3D Studio Max and a control\nsystem for a stereoscopic digital camera. The result is stereoscopic images generated correctly at the first attempt, with precisely\ncontrolled perceived depth. A new analysis of the distortions introduced by different camera parameters was undertaken.\nKeywords: Graphics Systems, Human Factors, Rendering, Virtual Reality, stereoscopic, 3D display\n1. INTRODUCTION\nStereoscopic displays in a variety of designs are becoming common. LCD shutter glasses are popular for CRT displays1\u20133,\nas are polarizing glasses for projection displays and new technologies including auto-stereoscopic (no glasses) flat panel LCD\ndisplays4\u20139. Several of these systems are also capable of user head tracking to enable wide viewing freedom and update of the\nstereoscopic image to simulate look-around.\nThe benefits that stereoscopic displays can provide are widely understood3,10\u201312 and include: depth perception relative\nto the display surface; spatial localization, allowing concentration on different depth planes; perception of structure in visu-\nally complex scenes; improved perception of surface curvature; improved motion judgement; improved perception of surface\nmaterial type. These benefits give stereoscopic displays improved representation capabilities that allow the user a better under-\nstanding or appreciation of the visual information presented13\u201315.\nStereoscopic display systems need to maintain high image quality if they are to provide a convincing and comfortable\nviewing experience in widespread use. In part the solution is to build a high quality display which has good inherent 2D image\nquality (bright, high resolution, full color, moving images) and has very low crosstalk between the the two viewing channels to\nreduce ghosting14. In addition, the image generation process must be carefully controlled so that the stereoscopic image data\npresented on the display does not contain misalignments or unnecessary distortions.\nThe key variable that must be determined for any stereoscopic image creation is the camera separation, as this directly\naffects the amount of depth a viewer perceives in the final image. A number of approaches have been tried, including exact\nmodeling of the user\u2019s eye separation. However, depending on the scene content, this may capture very large or small image\ndisparities (the difference between corresponding scene points in the left and right image, which the brain interprets as depth)\nthat can produce too much or too little perceived depth on the target display. Using exact eye spacing is only reasonable for the\northoscopic case, where the object size and depth matches the target display size and comfortable depth range. In practice this\nis rarely the case and the camera separation is often determined by trial and error, which is tedious and can easily result in an\nimage suited only to the creator\u2019s binocular vision.\nCorrespondence: E-mail: 3D@sharp.co.uk; WWW: http:\/\/www.sle.sharp.co.uk\/3D\nCondition: Textured box (look down)\n2\n8\n3\n8\n7\n1\n5\n6\n4\n3\n1\n2\n4\n5\n7\n6\n-2 -1.5 -1 -0.5 0 0.5 1\nsu\nbje\nct\nDistance in virtual space (m)\nin\nout\nin -1.237 -0.527 -0.14 -0.523 -19.5 -10.42 -8.24 -0.996\nout 0.207 0.313 0.143 0.193 0.257 0.317 0.53 0.453\n1 2 3 4 5 6 7 8\ntending towards infinity\nDistance \n(m)\ntowards observerscreen plane\nsubject\nFigure 1. Results of the human factors study of subjects looking\nat a simple scene, viewing distance 700mm from a 13.8\u201d display.\nThe range of depths tolerated before loss of stereo fusion can be\nquite large.\nCondition: Mallet Box (look down)\n1\n2\n3\n4\n7\n8\n1\n2\n3\n4\n5\n7\n8\n5\n66\n-2 -1.5 -1 -0.5 0 0.5 1\nSu\nbje\nct\nDistance in virtual space (m)\nin\nout\nin -0.15 -0.099 -0.069 -0.097 -0.18 -0.926 -0.117 -0.062\nout 0.112 0.19 0.113 0.11 0.106 0.103 0.093 0.117\n1 2 3 4 5 6 7 8\nDistance \n(m)\nsubject\nFigure 2. Results of the human factors study of subjects looking\nat a Mallet unit. The depths tolerated before stereo fusion is lost\nare much smaller than before.\n1.1. Human factors analysis of stereoscopic displays\nThe issue of viewer comfort for stereoscopic displays is studied in detail in relatively few human factors publications16\u201319.\nHowever, a similar conclusion is drawn by all the studies: the amount of disparity in stereoscopic images should be limited\nto be within a defined comfortable range. The main reason given for this is that the human visual system normally operates\nsuch that the convergence of the eyes and the accommodation (focus) are linked. For all stereoscopic displays this relationship\nis thought to be stressed by requiring the viewer\u2019s eyes to converge perceived depth a long way off the display plane but still\nbeing required to focus on the display plane. Limiting disparity ensures that the viewer\u2019s perceived depth is controlled and the\nconvergence\/accommodation link is not stressed.\nThe suggested limits on the amount of disparity in a stereoscopic image14,19, in the region of 24min arc, do not allow a large\namount of depth to be reproduced. For typical desktop displays with a viewing distance in the region of 700mm this puts the\ncomfortable perceived depth range as little as 50mm in front and 60mm behind the display surface. One immediate implication\nis that most objects and scenes will have depth compression when they are shown on a stereoscopic display.\nRecently some human factors research was carried out using a 13.8\u201d LCD autostereoscopic display from Sharp Laboratories\nof Europe20. The fusional thresholds of several subjects were measured, both into and out of the screen, for different types of\nstimuli and different task requirements. (Viewing distance for all tests was 700mm).\nThe results showed several points of interest:\n\u2022 When the subject is free to alter the depth in a simple scene at will, the limit behind the display at which fusion was lost\ncould be up to 20m, and was typically greater than 2m. The depth limit in front of the display was generally between\n300mm and 500mm with the maximum reported being 540mm.\n\u2022 With the same simple scene, but when the subject had to look away from the display before carrying out the next step in\ndepth the limits were much closer. Some of the subjects could still achieve distances greater than 2m behind the display,\nbut several found the limit between 500mm and 1m. The limits to the front were between 200mm to 400mm with one\nsubject still able to reach 520mm (See figure 1).\n\u2022 When the subjects were looking at a much more sensitive test, a modified Mallet unit, and free to move it to the limits,\nthe maximum depth behind the screen was between 80mm and 150mm. The range in front of the screen was typically\nbetween 100mm and 300mm, with one subject reaching 500mm.\n\u2022 When the subject has to look away from Mallet unit all but one reached a maximum depth between 60mm and 150mm\nbehind the display and 110mm in front of the display (See figure 2).\nFigure 3. Parallel cameras are used to avoid vertical disparity.\nThe viewing frustum or image must be cropped to ensure that\nthe intended object appears at the plane on the display. A is\nthe camera separation. W \u2032 is the effective width of the virtual\ndisplay.\nFigure 4. Perceived depth, P , is easy to measure and understand\nwhen composing images. E is the separation of the viewers eyes. a\nand b are the angles between the eyes and, respectively, the display (a\ndistance Z from the viewer) and a point at perceived depth P .\nThis shows that there is a real need to consider the limit of depth which is comfortably fusible by any individual observer.\nThe Mallet unit is a very sensitive test for loss of fusion, so the limits set could be relaxed from around 60mm behind and 50mm\nin front, perhaps as far as to 500mm behind and 200mm in front, but most realistically to an amount inbetween the two limits.\nOur goal is to enable an image creator to compose stereoscopic images in very much the same way as they compose\nmonoscopic images by setting a camera position, direction and field of view. Our new method then automatically determines\ncamera separation, accounting for the human factors results on viewing comfort. This is achieved by mapping a defined depth\nrange in the scene to a defined perceived depth range on the target display. If the perceived depth range is chosen in accordance\nwith human factors results, the stereoscopic image should be comfortable for all users with normal binocular vision. By\nchoosing to use physical depths as control parameters, we believe that novice and experienced users will have a more intuitive\ncontrol of stereoscopic image generation and will be better able to understand the effects of different image composition choices.\n2. BACKGROUND\nIt is now becoming widely accepted17,21\u201327 that a stereoscopic camera system with parallel axes is necessary to avoid the\nvertical image disparity generated by systems that verge the camera axes. For a parallel camera system, points at infinity have\nzero disparity and are perceived by the viewer in the plane of the target display. To ensure that corresponding points in the left\nand right images, at other distances from the viewer, are perceived in the screen plane, the images must be adjusted during or\nafter capture. This is accomplished either by using an asymmetric viewing frustum, as is possible in OpenGL28, or by cropping\nthe sides of the images after capture as illustrated in figure 3. With a physical camera the image sensors may be offset to achieve\nthe same effect.\nHuman factors studies suggest that a certain angular disparity (b\u2212 a in figure 4) should be maintained in order to generate\ncomfortable images. As can be seen in figure 5 this approach3,18 does not maintain constant perceived depth over a range of\ntypical viewing distances for a desktop stereoscopic display, where P = ZE\/dF\u22121 .\nIn our new method we control perceived depth and accept, like Lipton3, that for many scenes depth compression is in-\nevitable, as shown in figure 6. From our experience to date too much perceived depth will immediately cause viewers discom-\nfort and therefore we aim to control this precisely. For head tracked displays where the viewer moves and the displayed image\nis updated we must also ensure that the depth compression effect remains constant so that as the viewer moves to look around\nan object there is no depth distortion.\n3. PREVIOUS WORK\nSeveral approaches have been described for setting the camera separation for stereoscopic image capture, based on a knowledge\nof the viewing arrangement, the scene and the camera.\nLipton29 published a set of tables for capturing stereoscopic images with several different film formats, using converging\ncameras. For a range of camera separations and convergence distances the maximum and minimum suitable depths were\n020\n40\n60\n80\n100\n120\n140\n160\n500 550 600 650 700 750 800 850 900\nZ, viewing distance (mm)\nP,\n p\ner\nce\niv\ned\n d\nep\nth\n (m\nm\n)\nConstant angular disparity, (b-a) = 35\u2019\nConstant perceived depth, P = 60mm\nFigure 5. It is often suggested that a constant angular disparity\nshould be maintained but as can be seen this does not maintain\nconstant perceived depth over typical viewing ranges for desktop\nstereoscopic displays.\nFigure 6. In most imaging situations the scene depth (bottom) is\nmapped to a different perceived depth range on the display (top).\nThis results in depth compression or expansion.\ntabulated. This approach requires re-calculation of the tables for each new filming format. He acknowledges that it would be\npossible to capture images with parallel cameras, except that distant objects would always appear around the screen plane. The\ntables assumed the maximum allowable disparity was either equal to the interocular separation of the viewer, or when there was\n1\u25e6 divergence of the viewers eyes.\nKitrosser24 developed the Polaroid Interocular Calculator in 1952. Given the image capture parameters, including the\ndistances to the closest and furthest points on the subject, the required camera separation is obtained. This device made an\nassumption about the desired image parallax(disparity), in that experience of professional stereographers suggested that a ratio\nof 1:24 of parallax to image width was desirable. This rule of thumb is not strongly supported by recent human factors work,\nand the calculator is also unable to take into account varying interocular separation.\nWare30 conducted experiments with a group of people, where the subjects were allowed to alter the camera separation used\nto generate images as the depth in the images was altered. This information was then used in a two stage method to select\nappropriate scaling followed by an appropriate camera separation for any given scene. The camera separation used is entirely\ndependent on the selections of the subjects of the original experiments and cannot account for different displays, or viewers\nwith different characteristics (perhaps a child, with relatively small eye separation, viewing the display).\nWartell27 analyses the distortions perceived when stereoscopic images are viewed from different directions on a head\ntracked display. This analysis leads to a transformation matrix that is used to correct for the distortion. An equation to calculate\ncamera separation is derived which brings points infinitely far from the viewer to the furthest fusible distance of a display. The\nmaximum depth plane, and hence the furthest fusible distance, is taken to be a point giving a screen disparity of the modeled eye\nseparation (smaller than the true viewer eye separation) when viewed by the user. This cannot precisely control the perceived\ndepth on a target display.\nWartell further asserts that even in a perfect head tracked system, the user will perceive dynamic distortion while moving,\nhence the need for a transformation matrix to pre-distort the geometry. However the pre-distortion matrix presented does\nnot maintain the depth compression of an object when the viewer moves in a direction perpendicular to the display. As we\nshall show later, this perpendicular aspect of depth distortion can also be removed, so that the perceived depth compression\nof an object remains constant. We achieve this important goal by separating consideration of the viewer\/display space and\ncamera\/scene space (for CG and photographic cases). Previous systems, such as Wartell, have assumed that the camera\/scene\nspace is first scaled to the viewer\/display space and the cameras are then placed at the viewer\u2019s eye positions, often with slightly\nreduced separation to reduce the disparity and avoid discomfort. In such systems camera movement is tied directly to the viewer\nmovement. It is this direct link which results in the unwanted depth distortion and requires the transformation matrix to correct\nfor the resulting shear distortion.\n4. WHEN THE VIEWER IS STATIC\nWe present an approach to calculating the camera separation which clearly defines the relationships between the viewer, the\nscene, and the camera placement. The method enables the precise control of these parameters to account for human factors\na) Viewer\/Display space b) Camera\/Scene space\nFigure 7. Notations a) N and F are the furthest distances each side of the display at which objects should appear to the viewer. W is the\nwidth of the display. dN and dF are the disparities, on the display, of objects appearing at the limits N and F . b) Z\u2032 is the distance of the\ncameras from the \u2018virtual\u2019 display in the scene, N \u2032 and F \u2032 are distances from the cameras to the closest and furthest visible points in the\nscene. d\u2032N and d\u2032F are the disparities, at a distance Z\u2032 from the cameras, of objects N \u2032 and F \u2032 units away from the cameras.\nresults and is flexible enough that new results can be utilised when they are available. The different requirements for com-\nfort while viewing different stereoscopic displays are accommodated without changing the method, only the initial display\nparameter values.\nThe first situation considered is when the viewer is not tracked. The viewer is defined to be at a certain distance from the\ndisplay, horizontally and vertically centered so that the viewing frusta for the left and right eyes, and therefore the left and right\ncameras, are symmetrical. This is most applicable to still image generation because for many displays there is an optimal or\nnormal viewing distance, even when the viewer has freedom to move. In some cases there is a fixed viewing distance at which\nthe viewer must be in order to see a stereoscopic image.\n4.1. Maths\nThe arrangement of the display and viewer are given, and the parameters are shown in figure 7a (Z, N , F , W and E). In\naddition some details of the scene are given (figure 7b): the desired position of the camera in the scene, the distance from the\ncamera to the closest and furthest points in the scene (N \u2032, F \u2032). The field of view \u03b8, or focal length f , have been chosen (if focal\nlength is used the film width, Wf , must also be known). Together the display set-up and the distances from the camera allow\nus to calculate the other scene parameters, Z \u2032, W \u2032 and A, the camera separation. Z \u2032 is the distance to Zero-Disparity-Plane\n(ZDP); points at this distance will appear to be on the display plane when the images are viewed, so this is the effective position\nof the display in scene space, called the \u2018virtual display\u2019.\nThe first stage is to obtain expressions for the screen disparities dN and dF :\ndN =\nNE\nZ \u2212N and dF=\nFE\nZ + F\n(1)\nand the world disparities, d\u2032N and d\u2032F :\nd\u2032N =\nA(Z \u2032 \u2212N \u2032)\nN \u2032\nand d\u2032F=\nA(F \u2032 \u2212 Z \u2032)\nF \u2032\n(2)\nEquation 2 relies on the two unknowns, A and Z \u2032.\nThe disparities on the display and in the world will not be equal, but should be in same proportion so that they are correct\nwhen the final image is displayed:\ndN\ndF\n= R =\nd\u2032N\nd\u2032F\n=\n(Z \u2032 \u2212N \u2032)F \u2032\n(F \u2032 \u2212 Z \u2032)N \u2032 (3)\nThis removes A allowing Z \u2032 to be calculated\nZ \u2032 =\nR+ 1\n1\nN \u2032 +\nR\nF \u2032\n(4)\nFigure 8. The OpenGL asymmetric viewing frustum is defined by the four parameters shown, l, r, t and b, and the near and far clipping\nplanes (not shown).\nThe mapping of depth from the scene to the display has now been specified. Once the correct camera separation is calculated\nobjects between N \u2032 and Z \u2032 from the cameras will appear up to N units in front of the display, and objects between Z \u2032 and F \u2032\nfrom the cameras will appear up to F units behind the display.\nThe FOV or focal length are given and Z \u2032 has been calculated, so the field width at Z \u2032 may be obtained:\nW \u2032 = 2Z \u2032 tan\n(\n\u03b8\n2\n)\n=\nZ \u2032Wf\nf\n(5)\nThis gives a scaling from the display to the virtual display:\nS =W \u2032\/W (6)\nFrom here A is calculated, noting that d\u2032N = SdN , from\nA =\nd\u2032NN\n\u2032\nZ \u2032 \u2212N \u2032 =\nSdNN\n\u2032\nZ \u2032 \u2212N \u2032 (7)\nWhen using the OpenGL viewing frustum all the required details are now available. The frustum is specified as follows, noting\nthat all results must be multiplied by near clip plane distanceZ\u2032 . For the left camera the frustum is:\nl = \u2212\n(\nW \u2032\n2\n\u2212 A\n2\n)\nr =\nW \u2032\n2\n+\nA\n2\nt =\nH \u2032\n2\nb = \u2212H\n\u2032\n2\n(8)\nand similarly the right camera, assuming the viewer is positioned in front of the center of the display, as shown in figure 8. If\nthe viewer is not there then tracking must be incorporated, see section 5.\nThe equations derived so far are intended to maintain the same field width at Z \u2032 once the images are captured. When using\nphotographic cameras a symmetric perspective frustum is used, so the images require cropping to simulate the asymmetric\nfrustum of OpenGL. In order to maintain the same field width the field of view must be increased to take into account the\ncropped portion of the image: a new field of view, focal length, and a cropping fraction are calculated:\n\u03b8\u2032 = 2arctan\n(\nW \u2032 +A\n2Z \u2032\n)\nf \u2032 =\nWf\n2 tan\n(\n\u03b8\u2032\n2\n) = WfZ \u2032\nW \u2032 +A\ncrop = A\nW \u2032 +A\n(9)\nwhere crop is the proportion of the image to crop from the left hand side of the left image and the right hand side of the right\nimage. If using real cameras distances should be measured to the nodal point of the lens, not to the film plane. Otherwise an\nadjustment is needed to account for this difference \u2014 especially noticeable when objects being photographed are close to the\ncamera.\nWhen using real cameras it may not be possible to adjust the focal length by the relatively small amount required by\nequation 9. In this case the camera separation is calculated slightly differently. The field width captured now is effectively:\n(W \u2032 +A) = 2Z \u2032 tan\n\u03b8\n2\n(10)\nwhich alters the scale factor to:\nS =\nW \u2032 +A\nW + AS\n(11)\nrearranging equation 7 gives\nA\nS\n=\ndNN\n\u2032\nZ \u2032 \u2212N \u2032 (12)\nsubstituting this result into equation 11 and then into 7 gives the following equation for A:\nA =\n2Z \u2032 tan \u03b82dNN\n\u2032\nW (Z \u2032 \u2212N \u2032) + dNN \u2032 (13)\nEquation 9 is then evaluated to determine amount to crop from the image edges in order to adjust the ZDP.\nThe case described above applies when N \u2032 and F \u2032 are given, however it is possible to choose any two of N \u2032, Z \u2032 or F \u2032. If\nZ \u2032 and one of the other two are given, simply re-arrange equation 4 to solve for the unknown.\n5. WHEN THE VIEWER IS TRACKED\nWhen tracking it is desirable that the scene perceived by the user does not distort. The mapping of scene depth to display\nperceived depth must remain unchanged as the viewer moves, i.e. any depth compression should remain constant under viewer\nmotion. There are two elements to the motion of the viewer which might affect the perceived image:\n\u2022 As the viewer moves in a direction parallel to the screen plane there should be no shear distortion of the objects viewed.\n\u2022 As the viewer moves in a direction perpendicular to the display, the depth distribution about the screen plane should not\nalter, while the maximum and minimum depth limits remain the same.\nWhen tracking, the distinction between the viewer\/display and camera\/scene spaces is especially important. To avoid extra\ndistortion the cameras must be free to be placed in the appropriate position and not tied directly to the viewer position. As is\nshown below the relationship between cameras and viewer is not linear and is not the same for the perpendicular component as\nfor the parallel component.\n5.1. Maths\nWhen tracking, the method generally used is to move the cameras the same distance as the eyes have moved (after the camera\nspace is scaled to viewer space)27. This results in a shear distortion of the objects perceived by the viewer. This effect is shown\nin figure 9a, where the viewer has moved from position 1 to position 2. In this example the right eye in position 2 is in the\nsame position as the left eye in position 1, and the mid-point between the cameras is moved by the same amount. The light\ngrey lines show the ray from a camera to a point on an object giving homologous points in the image. The dark lines show the\nrays from the eyes, through the homologous points, to the point perceived by the user. Moving the cameras in this way causes\nthe perceived point to drift sideways. Since points at different depths drift by different amounts, this appears to the viewer as a\nshear distortion.\nIn figure 9b the cameras are moved in proportion with the movement of the eyes. In this example the eyes move by one\ninterocular separation so the cameras are moved by their interaxial separation. Now the perceived point remains at exactly the\nsame location, however far to the side the viewer moves. For a tracked offset from the screen center of (X,Y ) (ignoring the Z\ncomponent for the moment), the camera motion must be (X AE , Y AE ) parallel to the display.\na) b)\nFigure 9. Two methods for controlling the cameras when the tracked position of the viewer has moved parallel to the display. Position 1 is\nthe initial reference position, position 2 is the new position. a) Tying the camera positions to the viewer position causes perceived points to\ndrift. b) Allowing independent motion of the viewer and cameras avoids any distortion of the stereoscopic image.\nWe propose a new method as follows. First evaluate equations 1 to 6 using a reference position to give the values needed.\nThe reference defines the depth compression which is to be maintained as the viewer moves (figure 10). From this stage N \u2032ref ,\nZ \u2032ref and F \u2032ref are known for the reference position, giving the relative position on the display and the near and far objects\n(N\u2217 = Z \u2032ref \u2212N \u2032ref and F \u2217 = F \u2032ref \u2212 Z \u2032ref ). It is this relationship which must be maintained while the viewer moves.\nNow recalculate R = dNdF using the tracked Z position to obtain dN and dF . Equation 3 is now rearranged in terms of the\nknown values, N\u2217 and F \u2217, and Z \u2032 which refers to the tracked position:\nd\u2032N\nd\u2032F\n= R =\nN\u2217(F \u2217 \u2212 Z \u2032)\nF \u2217(Z \u2032 \u2212N\u2217) (14)\nwhich finally yields an equation for Z \u2032:\nZ \u2032 =\n(R+ 1)N\u2217F \u2217\nRF \u2217 \u2212N\u2217 (15)\nZ \u2032ref and Z \u2032 have been calculated, so the difference is applied to N \u2032ref and F \u2032ref to give the distances to the nearest and\nfurthest objects from the position of the cameras after adjustment for tracking. The camera separation is calculated from\nequation 7, using the updated N \u2032, Z \u2032, but using the same scale factor, S, as for the reference, since we want the same scene\narea at the display plane to be captured in order to avoid altering the displayed object width. The camera center is moved by\n(Z \u2032ref \u2212 Z \u2032) perpendicular to the display, as well as the parallel motion described above, before offsetting by \u00b1A2 for the left\nand right views.\nThe final stage, for OpenGL style graphics, is to compute the correct asymmetric camera frustum (all results must be\nmultiplied by near clip plane distanceZ\u2032 when used with OpenGL). For the left camera the frustum is:\nl = \u2212\n(\nW \u2032\n2\n\u2212 A\n2\n+X\nA\nE\n)\nr =\nW \u2032\n2\n+\nA\n2\n\u2212XA\nE\nt =\nH \u2032\n2\n\u2212 Y A\nE\nb = \u2212\n(\nH \u2032\n2\n+ Y\nA\nE\n) (16)\nand similarly for the right camera. If a new field of view for symmetric perspective cameras is required, equation 9 is evaluated.\n6. ANALYSIS AND RESULTS\n6.1. The Stereo Projective Distortion\nIn this section we derive the projective transformation which maps the camera\/scene space to the viewer\/display space, for\na certain configuration of cameras and eyes. The projective transformation helps us to visualize depth compression and any\nstereoscopic distortion, and to verify our method of eliminating the unwanted distortion when the viewer moves from the\nreference position.\nFigure 10. Tracking reference positions. The depth compression\nseen by the viewer is defined by the camera parameters when the\nviewer is at the reference position. The cameras are moved relative\nto their reference such that the depth compression and perceived\ndepth are constant and there is no distortion as the viewer moves.\nFigure 11. Coordinate system of the projective transform.\nGiven a certain display, the parameters of the cameras, and the positions of the eyes, a point in the scene is transformed to\na perceived point in the display. The scene and cameras are first scaled by a factor of 1\/S (equation 6), thus allowing us to\noverlay the scene and cameras in the display world as shown in figure 11. Consider a scaled scene point H in figure 11. The\ntransformation which takes the point H to the perceived point K is derived by finding the corresponding points I and J on the\ndisplay, and then intersecting the lines \u001aLI and \u001aRJ to give the perceived point K.\nThe origin of our coordinates is the center of the display (denoted by \u201cOrigin\u201d in figure 11). The point J is given by\nJ = QH where:\nQ =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\n1 0 cx+A\/2\u2212cz 0\n0 1 cy\u2212cz 0\n0 0 0 0\n0 0 S\u2212cz 0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb (17)\nA similar expression exists for the point I . Deriving the intersection of \u001aLI and \u001aRJ gives the following projective transformation\nT =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\n1 0 Aex\u2212EcxEcz 0\n0 1 Aey\u2212EcyEcz 0\n0 0 AezEcz 0\n0 0 A\u2212SEEcz 1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb (18)\nFor example, K and H in equation 18 are related by K = TH . An OpenGL software tool has been implemented to visualize\nthe effects of the transformation in equation 18.\nThe analysis leading to equation 18 assumes that the screen curvature is negligible. See31 for details of how to correct\nfor screen curvature. Our analysis allows general placement of the eyes and cameras. It is different from that of27, where the\ncameras and eyes are constrained to be on the same baseline.\nPositional Root Mean Square Error To quantify the difference in the perceived object when the viewer moves away from\nthe nominal position, we use the following measure. The nominal configuration of cameras and eyes defines a reference\ntransformation Tref according to equation 18. The current configuration of cameras and eyes defines a transformation Ttrack.\nFor a set of 5\u00d75\u00d75 points in the scene, we map them to the display world according to Tref and Ttrack, and measure the root\nmean square error in position between the two sets of mapped points.\n6.2. Results\nFigures 12 to 15 show the results of our new camera parameters calculation method. The images are snapshots from the software\ndescribed in 6.1. The display and viewer (dark spheres) are on the left of each pair, the cameras (grey pyramids) and object\nbeing viewed are on the right. The scaling is arranged such that the virtual display in the scene, shown on the right, is the same\nsize as the display shown on the left.\nIn all cases the dotted lines represent the position of the display. On the right the dark lines represent the limits of depth\nin the scene. On the left the dark lines show where those depth limits have been mapped to, relative to the display, these are\nusually mostly obscured by the light grey lines which show the perceived depth limits specified for the display.\nFigure 12a shows the results of our new method for calculating camera parameters. Cameras are placed such that the object\non the right of figure 12a appears not more than 60mm in front of the display and not more than 60mm behind the display\nwhen viewed by someone with 60mm eye separation. The viewing distance is 700mm from the display. For convenience of\ndemonstrating the results and comparison with other methods, the object has been placed so that the ZDP is 700mm from the\ncameras. The closest part of the object is 562mm from the cameras, the furthest part is at a distance of 884mm. The camera\nseparation, A, is calculated to be 27.81mm. On the display the desired perceived depth limits match the actual depth range of\nthe object, which is the desired result.\nIn figure 12b the maximum distance allowed in front of the display has been changed to 120mm. The camera separation is\nnow calculated to be 37.75mm. In figure 12c the maximum distance allowed behind the display has been changed to 100mm.\nAgain, the camera separation is now calculated to be 28.89mm. In each case the distance to the ZDP changes, therefore the part\nof the object appearing at the plane of the display changes.\nFigure 13 shows what happens when the viewer is tracked and moves horizontally. Figure 13a is the central position, with\nno distortion except the depth compression to the chosen display parameters. In figure 13b the cameras are moved the same\ndistance as the viewer moves (125mm in this case). A distortion of the stereoscopic image is observed, the RMS error of the\nimage is 12.4mm. Alternatively, when the cameras are placed according to our method, a smaller movement of 47.51mm, no\ndistortion is observed.\nFigure 14 shows what happens when the viewer moves towards the display. In figures 14a and b the viewer has moved\n450mm towards the display. If, as in figure 14a, the cameras are moved the same distance as the viewer, a distortion of the\nstereoscopic image depth is observed. The purple lines showing the desired perceived depth do not match the actual depth limits\nof the display. The shape of the box bounding the object is not identical to the desired result of figure 13a. The RMS error of\nthe image is 26.2mm. Alternatively, when the cameras are placed according to our method, in this case a smaller movement\nof 317.88mm, no distortion is observed. The object is perceived exactly as it was in the initial\/reference position. The camera\nseparation is increased to 37.92mm, in addition to the change in camera position.\nFigure 15 shows a more complex example. The object is a different distance from the cameras. The depth allowed on the\ndisplay places the perceived object completely behind the display, and the viewer has moved to the side and in towards the\ndisplay from the reference position. In this case the depth compression is different from the previous examples, but the object\nis perceived exactly within the defined range. There is also no distortion of the object even though the viewer has moved away\nfrom the reference position.\n7. CONCLUSION\nWe have demonstrated a method for stereoscopic camera arrangement which allows a specified scene depth range to be mapped\nto a specified perceived depth range when viewed on a stereoscopic display. The method has been used for generating still\nimages rendered with software such as 3D Studio Max, photography using digital cameras and real time computer graphics as\nmay be produced using OpenGL.\nIn addition, when the position of the viewer is tracked relative to the display, the method presented avoids any changes in\nthe perceived object. There is no change in the depth compression, and there is no shear distortion due to the motion of the\nviewer. No additional matrix transformations are required to achieve this result.\nOur new method allows users to fully understand and control the nature of depth compression in their stereoscopic images.\nWe believe this is an important tool for all stereoscopic display users and essential in certain applications. In medical imaging\nit is necessary to know that the perceived data is being viewed without compression or distortion. For CAD\/CAM applications,\ne.g. surface design, subjective judgements on shape need to be made with knowledge that the perceived surface is an accurate\nrepresentation of the final product.\n8. FUTURE\nWe believe that future work in this area is important in order to enable more widespread use of stereoscopic displays. In\nparticular further human factors work on understanding and defining comfortable perceived depth ranges is required.\nThere are several implications of this work, including the need to capture images for a specific display configuration. For\nstill images this implies a need to adjust the camera separation after capture depending on the display characteristics. This\nwill require high fidelity image based rendering to generate the images as seen from the correct camera positions. This is an\nexcellent challenge for future work in image based rendering, particularly as binocular vision can easily identify errors in depth\ninterpolation.\nACKNOWLEDGMENTS\nThe authors gratefully acknowledge their colleagues at Sharp Labs of Europe for many discussions concerning this work,\nand supporting the display hardware; John Wann and Anna Plooy at the Action Research Laboratory, in the Department of\nPsychology at Reading University and finally we thank Sharp Corporate R&D Group for their continuing support of our work.\nREFERENCES\n1. ELSA AG, 3D Revelator, Quick Start Guide, Aachen, Germany, 1999.\n2. M. Agrawala, A. C. Beers, B. Fro\u00a8hlich, P. Hanrahan, I. McDowall, and M. Bolas, \u201cThe Two-User Responsive Workbench:: Support for\nCollaboration Through Independent Views of a Shared Space,\u201d in Computer Graphics Proceedings, Annual Conference Series, 1997\n(ACM SIGGRAPH 97 Proceedings), pp. 327 \u2013 332, 1997.\n3. L. Lipton, Stereographics, Developers Handbook, Stereographics Corporation, 1997.\n4. J. Eichenlaub, \u201cA Lightweight, Compact 2D\/3D Autostereoscopic LCD Backlight for Games, Monitor and Notebook Applications,\u201d in\nProceedings of the SPIE, vol. 3295, pp. 180 \u2013 185, Jan. 1998.\n5. H. Morishima, H. Nose, N. Taniguchi, K. Inogucchi, and S. Matsumura, \u201cRear Cross Lenticular 3D Display Without Eyeglasses,\u201d in\nProceedings of the SPIE, vol. 3295, pp. 193 \u2013 222, Jan. 1998.\n6. Sanyo, SANYO Announces Development of Non-glasses 15-inch XGA 3D Display, Sanyo Electric Corporation Limited,\nhttp:\/\/www.sanyo.com\/koho\/hypertext4-eng\/9707news-e\/07243dd.htm, 1997.\n7. A. Schwerdtner and H. Heidrich, \u201cDresden 3D display (D4D),\u201d in Proceedings of the SPIE, vol. 3295, pp. 203 \u2013 210, Jan. 1998.\n8. D. Trayner and E. Orr, \u201cAutostereoscopic Display using Holographic Optical Elements,\u201d in Proceedings of the SPIE, vol. 2653, pp. 65\n\u2013 74, Jan. 1996.\n9. G. J. Woodgate, D. Ezra, J. Harrold, N. S. Holliman, G. R. Jones, and R. R. Moseley, \u201cAutostereoscopic 3D Display Systems with\nObserver Tracking,\u201d Signal Processing: Image Communication 14, pp. 131 \u2013 145, 1998.\n10. D. B. Diner and D. H. Fender, eds., Human engineering in stereoscopic viewing devices, Plenum Press, ISBN 0-306-44667-7, 1993.\n11. I. P. Howard and B. J. Rogers, Binocular Vision and Stereopsis, Oxford University Press, ISBN 0-19-508476-4, 1995.\n12. D. McAllister, ed., Stereo computer graphics and other true 3D technologies, Princeton University Press, ISBN 0-691-08741-5, 1993.\n13. T. Mitsuhashi, \u201cSubjective Image Position in Stereoscopic TV Systems - Considerations on Comfortable Stereoscopic Images,\u201d in\nProceedings of the SPIE, vol. 2179, pp. 259 \u2013 266, Mar. 1994.\n14. R. Sand and A. Chiari, eds., Stereoscopic Television: Standards, Technology and Signal Processing, European Commission, Directorate\nGeneral XIII-B, Brussels, 1998.\n15. C. Ware and G. Franck, \u201cEvaluating Stereo and Motion Cues for Visualizing Information Nets in Three Dimensions,\u201d tech. rep., Uni-\nversity of New Brunswick, Technical Document #TR94-082, 1994.\n16. N. Hiruma and T. Fukuda, \u201cAccomodation Response to Binocular Stereoscopic TV Images and their Viewing Conditions,\u201d SMPTE\nJournal , pp. 1137 \u2013 1144, Dec. 1993.\n17. A. Woods, T. Docherty, and R. Koch, \u201cImage Distortions in Stereoscopic Video Systems,\u201d in Proceedings of the SPIE, vol. 1915, pp. 36\n\u2013 48, 1993.\n18. M. Wopking, \u201cViewing Comfort with Stereoscopic Pictures: An Experimental Study on the Subjective Effects of Disparity Magnitude\nand Depth of Focus,\u201d Journal of the SID 3(3), pp. 101 \u2013 103, 1995.\n19. Y.-Y. Yeh and L. D. Silverstien, \u201cLimits of Fusion and Depth Judgement in Stereoscopic Color Displays,\u201d Human Factors 32(1), pp. 45\n\u2013 60, 1990.\n20. Action Research Laboratory, Sharp 3D Screen, Evaluation Report: Stages 1-3, Private communication to Sharp Labs., 2000.\n21. J. Baker, \u201cGenerating Images for a Time-Multiplexed, Stereoscopic Computer Graphics System,\u201d in True 3D Imaging Techniques and\nDisplay Technologies, Proc. SPIE, vol. 761, pp. 44\u201352, 1987.\n22. V. Grinberg, G. Podnar, and M. Seigel, \u201cGeometry of Binocular Imaging,\u201d in Proceedings of the SPIE, vol. 2177, pp. 56 \u2013 65, Feb.\n1994.\n23. L. F. Hodges and E. T. Davis, \u201cGeometric Considerations for Stereoscopic Virtual Environments,\u201d Presence 2, pp. 34 \u2013 43, Winter 1993.\n24. S. Kitrosser, \u201cPhotography in the Service of Stereoscopy,\u201d Journal of Imaging Science and Technology 42, pp. 295 \u2013 300, July\/August\n1998.\n25. W. Robinett and R. Holloway, \u201cThe Visual Display Transformation for Virtual Reality,\u201d Presence 4, pp. 1 \u2013 23, Winter 1995.\n26. D. A. Southard, \u201cViewing Model for Virtual Environment Displays,\u201d Journal of Electronic Imaging 4, pp. 413 \u2013 420, Oct. 1995.\n27. Z. Wartell, L. F. Hodges, and W. Ribarsky, \u201cBalancing Fusion, Image Depth and Distortion in Stereoscopic Head-Tracked Displays,\u201d in\nComputer Graphics Proceedings, Annual Conference Series, 1999 (ACM SIGGRAPH 99 Proceedings), pp. 351 \u2013 358, 1999.\n28. R. Akka, \u201cConverting Existing Applications to Support High Quality Stereoscopy,\u201d in Proceedings of the SPIE, vol. 3639, pp. 290 \u2013\n299, Jan. 1999.\n29. L. Lipton, Foundations of the Stereoscopic Cinema: A Study in Depth, Van Nostrand Reinhold Company, 1982.\n30. C. Ware, C. Gobrecht, and M. Paton, \u201cAlgorithm for Dynamic Disparity Adjustment,\u201d in Proceedings of the SPIE \u2013 The International\nSociety for Optical Engineering, Stereoscopic Displays and Virtual Reality Systems II, vol. 2409, pp. 150 \u2013 156, Feb. 1995.\n31. M. Deering, \u201cHigh Resolution Virtual Reality,\u201d in Computer Graphics Proceedings, Annual Conference Series, 1992 (ACM SIGGRAPH\n92 Proceedings), vol. 26, pp. 195 \u2013 201, 1992.\na) b) c)\nFigure 12. a) Using the method described, cameras are arranged so that the object on the right is viewed in the depth range shown on display\non the left. b) The distance allowed in front of the display is changed. c) The distance allowed behind the display is changed.\na) b) c)\nFigure 13. a) Reference image, the same as figure 12a. b) The viewer moves horizontally, tying the camera positions to the viewer position\ninduces a distortion. c) Using the method in this paper, the cameras move in such a way that there is no distortion of the stereoscopic image.\na) b)\nFigure 14. The viewer moves towards the display. a) Camera position tied to the viewer\nposition induces a distortion. b) Our method moves the cameras to ensure there is no distor-\ntion.\nFigure 15. The viewer, object and dis-\nplay perceived depth have all changed.\n"}