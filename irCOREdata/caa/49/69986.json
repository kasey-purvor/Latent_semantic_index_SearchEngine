{"doi":"10.1109\/TFUZZ.2008.925904","coreId":"69986","oai":"oai:eprints.lancs.ac.uk:19213","identifiers":["oai:eprints.lancs.ac.uk:19213","10.1109\/TFUZZ.2008.925904"],"title":"Evolving Fuzzy Rule-based Classifiers from Data Streams","authors":["Angelov, Plamen","Zhou, Xiaowei"],"enrichments":{"references":[{"id":16320198,"title":"A neuro-fuzzy method to learn fuzzy classi\ufb01cation rules from data,\u201d","authors":[],"date":"1997","doi":"10.1016\/s0165-0114(97)00009-2","raw":"D. Nauck and R. Kruse, \u201cA neuro-fuzzy method to learn fuzzy classi\ufb01cation rules from data,\u201d Fuzzy Sets Syst., vol. 89, pp. 277\u2013288, 1997.","cites":null},{"id":16320248,"title":"Adaptive Control.","authors":[],"date":"1989","doi":"10.1007\/bfb0042931","raw":"K. J. Astroem and B. Wittenmark, Adaptive Control. Reading, MA: Addison-Wesley, 1989.","cites":null},{"id":16320246,"title":"An approach for fuzzy rule-base adaptation using on-line clustering,\u201d","authors":[],"date":"2004","doi":"10.1016\/j.ijar.2003.08.006","raw":"P. Angelov, \u201cAn approach for fuzzy rule-base adaptation using on-line clustering,\u201d Int. J. Approx. Reason., vol. 35, no. 3, pp. 275\u2013289, Mar. 2004.","cites":null},{"id":16320262,"title":"An approach to autonomous self-localization of a mobile robot in completely unknown environment using evolving fuzzy rule-based classi\ufb01er,\u201d in","authors":[],"date":null,"doi":"10.1109\/cisda.2007.368145","raw":"X. Zhou and P. Angelov, \u201cAn approach to autonomous self-localization of a mobile robot in completely unknown environment using evolving fuzzy rule-based classi\ufb01er,\u201d in Proc. 2007 IEEE Int. Symp. Comput.Intell. Appl. Defense Security Symp. Series Comput. Intell. (SSCI 2007), Honolulu, HI, Apr. 1\u20135, pp. 131\u2013138 (ISBN 1-4244-0698-6).","cites":null},{"id":16320266,"title":"Applying the wrapper approach for auto discovery of under-sampling and over-sampling percentages on skewed datasets,","authors":[],"date":"2004","doi":null,"raw":"A. D. Joshi. (2004). Applying the wrapper approach for auto discovery of under-sampling and over-sampling percentages on skewed datasets, M.Sc. Thesis, Univ. South Florida, Tampa [Online]. pp. 1\u201377. Available: http:\/\/etd.fcla.edu\/SF\/SFE0000491\/Thesis-AjayJoshi.pdf","cites":null},{"id":16320201,"title":"Bayesian online classi\ufb01ers for text classi\ufb01cation and \ufb01ltering,\u201d in","authors":[],"date":null,"doi":"10.1145\/564376.564395","raw":"K. M. A. Chai, H. T. Ng, and H. L. Chieu, \u201cBayesian online classi\ufb01ers for text classi\ufb01cation and \ufb01ltering,\u201d in Proc. SIGIR 2002, Tampere, Finland, Aug. 11\u201315, pp. 97\u2013104.","cites":null},{"id":16320200,"title":"Classi\ufb01cation and Modeling With Linguistic Granules: Advanced Information Processing.","authors":[],"date":"2004","doi":"10.1007\/3-540-26875-8_1","raw":"H. Ishibuchi, T. Nakashima, and M. Nii, Classi\ufb01cation and Modeling With Linguistic Granules: Advanced Information Processing. Berlin, Germany: Springer-Verlag, 2004.","cites":null},{"id":16320203,"title":"Classi\ufb01cation and Regression Trees.","authors":[],"date":"1993","doi":"10.2307\/2530946","raw":"L. Breiman, J. Friedman, C. Stone, and R. Olshen, Classi\ufb01cation and Regression Trees. Boca Raton, FL: Chapman & Hall, 1993.","cites":null},{"id":16320215,"title":"Complete expression tree for evolving fuzzy classi\ufb01er systems with genetic algorithms,\u201d in","authors":[],"date":"2002","doi":"10.1109\/nafips.2002.1018105","raw":"J. Gomez, F. Gonzalez, D. Dasgupta, and O. Nasaroui, \u201cComplete expression tree for evolving fuzzy classi\ufb01er systems with genetic algorithms,\u201d in Proc. North Amer. Fuzzy Inf. Process. Soc. Conf. Fuzzy Logic Internet (NAFIPS-FLINT), 2002, pp. 469\u2013474.","cites":null},{"id":16320209,"title":"COR: A methodology to improve ad hoc data-driven linguistic rule learning methods by inducing cooperation among rules,\u201d","authors":[],"date":"2002","doi":"10.1109\/tsmcb.2002.1018771","raw":"J. Casillas, O. Cordon, and F. Herrera, \u201cCOR: A methodology to improve ad hoc data-driven linguistic rule learning methods by inducing cooperation among rules,\u201d IEEE Trans. Syst., Man, Cybern. B, Cybern., vol. 32, no. 4, pp. 526\u2013537, Aug. 2002.","cites":null},{"id":16320227,"title":"DENFIS: Dynamic evolving neural-fuzzy inferencesystemanditsapplicationfortime-seriesprediction,\u201d IEEETrans.","authors":[],"date":"2002","doi":"10.1109\/91.995117","raw":"N. Kasabov and Q. Song, \u201cDENFIS: Dynamic evolving neural-fuzzy inferencesystemanditsapplicationfortime-seriesprediction,\u201d IEEETrans. Fuzzy Syst., vol. 10, no. 2, pp. 144\u2013154, Apr. 2002.","cites":null},{"id":16320241,"title":"Detection concept drift with support vector machines,\u201d in","authors":[],"date":"2000","doi":null,"raw":"R. Klinkenberg and T. Joachims, \u201cDetection concept drift with support vector machines,\u201d in Proc. 7th Int. Conf. Mach. Learning (ICML), 2000, pp. 487\u2013494.","cites":null},{"id":16320202,"title":"Ef\ufb01cient decision tree construction on streaming data,\u201d in","authors":[],"date":"2003","doi":"10.1145\/956750.956821","raw":"R. Jin and G. Agrawal, \u201cEf\ufb01cient decision tree construction on streaming data,\u201d in Proc. ACM SIGKDD, 2003, pp. 571\u2013576.","cites":null},{"id":16320254,"title":"Evolving fuzzy systems from data streams in real-time,\u201d in","authors":[],"date":null,"doi":"10.1109\/isefs.2006.251157","raw":"P. Angelov and X. Zhou, \u201cEvolving fuzzy systems from data streams in real-time,\u201d in Proc. 2006 Int. Symp. Evol. Fuzzy Syst., Sep. 7\u20139, pp. 29\u2013 35.","cites":null},{"id":16320216,"title":"Evolving Rule-Based Models: A Tool for Design of Flexible Adaptive Systems.","authors":[],"date":"2002","doi":"10.1007\/978-3-7908-1794-2_3","raw":"P. Angelov, Evolving Rule-Based Models: A Tool for Design of Flexible Adaptive Systems. Berlin, Germany: Springer-Verlag, 2002.","cites":null},{"id":16320208,"title":"From approximative to descriptive fuzzy classi\ufb01ers,\u201d","authors":[],"date":"2002","doi":"10.1109\/tfuzz.2002.800687","raw":"J. G. Marin-Blazquez and Q. Shen, \u201cFrom approximative to descriptive fuzzy classi\ufb01ers,\u201d IEEE Trans. Fuzzy Syst., vol. 10, no. 4, pp. 484\u2013497, Aug. 2002.","cites":null},{"id":16320212,"title":"From Data Mining to Knowledge Discovery: An Overview,","authors":[],"date":"1996","doi":"10.1145\/240455.240464","raw":"U. Fayyad, G. Piatetsky-Shapiro, and P. Smyth. From Data Mining to Knowledge Discovery: An Overview, Advances in Knowledge Discovery and Data Mining. Cambridge, MA: MIT Press, 1996.","cites":null},{"id":16320235,"title":"Fuzzy ARTMAP: A neural network architecture for incremental supervised learning of analog multidimensional maps,\u201d","authors":[],"date":"1992","doi":"10.1109\/72.159059","raw":"G. A. Carpenter, S. Grossberg, N. Markuzon, J. H. Reynolds, and D. B. Rosen, \u201cFuzzy ARTMAP: A neural network architecture for incremental supervised learning of analog multidimensional maps,\u201d IEEE Trans. Neural Netw., vol. 3, no. 5, pp. 698\u2013712, Sep. 1992.","cites":null},{"id":16320194,"title":"Fuzzy Classi\ufb01ers.","authors":[],"date":"2000","doi":"10.1007\/978-3-7908-1850-5_5","raw":"L. Kuncheva, Fuzzy Classi\ufb01ers. Heidelberg, Germany: Physica-Verlag, 2000.","cites":null},{"id":16320210,"title":"Fuzzy identi\ufb01cation of systems and its application to modeling and control,\u201d","authors":[],"date":"1985","doi":"10.1016\/b978-1-4832-1450-4.50045-6","raw":"T. Takagi and M. Sugeno, \u201cFuzzy identi\ufb01cation of systems and its application to modeling and control,\u201d IEEE Trans. Syst., Man, Cybern., vol. SMC-15, no. 1, pp. 116\u2013132, Jan. 1985.","cites":null},{"id":16320252,"title":"Fuzzy model identi\ufb01cation based on cluster estimation,\u201d","authors":[],"date":"1994","doi":"10.1109\/fuzzy.1994.343644","raw":"S. L. Chiu, \u201cFuzzy model identi\ufb01cation based on cluster estimation,\u201d J. Intell. Fuzzy Syst., vol. 2, pp. 267\u2013278, 1994.","cites":null},{"id":16320268,"title":"Fuzzy sets as the basis for a theory of possibility,\u201d","authors":[],"date":"1978","doi":"10.1016\/0165-0114(78)90029-5","raw":"L. Zadeh, \u201cFuzzy sets as the basis for a theory of possibility,\u201d Fuzzy Sets Syst., vol. 1, pp. 3\u201328, 1978. Plamen P. Angelov (M\u201999\u2013SM\u201904) received the M.Eng. degree in electronics and automation from So\ufb01a Technical University, So\ufb01a, Bulgaria, in 1989 and the Ph.D. degree in optimal control from Bulgaria Academy of Sciences, So\ufb01a, Bulgaria, in 1993. He spent over ten years as a Research Fellow working on computational intelligence and control. During 1995\u20131996, he was at Hans-Knoell Institute, Jena,Germany.In1997,hewasaVisitingResearcher at the Catholic University, Leuvain-la-neuve, Belgium. In 2007, he was a Visiting Professor at the University of WolfenbuettelBraunschweig,Germany.HeiscurrentlyaSeniorLecturer(AssociateProfessor) at Lancaster University, Lancaster, U.K. He has authored or coauthored over 100 peer-reviewed publications, including the book Evolving Rule Based Models: A Tool for Design of Flexible Adaptive Systems (Springer-Verlag, 2002), and over 30 journal papers, and is a holder of a patent (2006). He is a Member of the Editorial Boards of three international scienti\ufb01c journals. He also holds a portfolio of research projects including a participation in a \u00a332M program ASTRAEA. He has received funding from the industry, European Union (EU), U.S., U.K. Research Council, etc. His current research interests include adaptiveand evolving(self-organizing) fuzzysystems as aframeworkof anevolving computational intelligence. He is also engaged in online and real-time identi\ufb01cation and design of such systems with evolving (self-developing) structure, intelligent data processing, particularly in evolving fuzzy-rule-based models, self-organizing and autonomous systems, and intelligent (inferential) sensors. Dr. Angelov is a member of the Technical Committee (TC) on Fuzzy Systems, Vice Chair of the TC on Standards, Computational Intelligence Society, Chair of the Task Force on Adaptive Fuzzy Systems, and member of the Autonomous Systems Working Group of the North-West Science Council of U.K. He serves regularly on the technical\/program committees of leading international conferences on different aspects of computational intelligence. Xiaowei Zhou (S\u201906) received the Bachelor\u2019s degree in computing science from Nanjing University of Science and Technology, Nanjing, China, in 2003, and the Master\u2019s degree (with distinction) in IT and data communications in 2004 from the Department of Communication Systems, Lancaster University, Lancaster, U.K., where he is currently working toward the Ph.D. degree at Infolab 21. He has authored or coauthored over a dozen of peer-reviewedpapersonevolvingintelligentsystems. His current research interests include evolving intelligent systems, robotics, image processing, and their applications under the framework of computational intelligence and machine learning. Mr. Zhou was a recipient of the IEEE Student Travel Grant in 2006 to attend the World Congress on Computational Intelligence and a number of other smaller travel grants. He is the main author of the fuzzy car algorithm, which was ranked third in the competition during the IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2007), London, U.K., July. Authorized licensed use limited to: Lancaster University Library. Downloaded on June 22, 2009 at 08:51 from IEEE Xplore.  Restrictions apply.","cites":null},{"id":16320258,"title":"Fuzzy Sets, Uncertainty and Information.","authors":[],"date":"1988","doi":"10.1137\/1030119","raw":"G. Klir and T. Folger, Fuzzy Sets, Uncertainty and Information. Englewood Cliffs, NJ: Prentice-Hall, 1988.","cites":null},{"id":16320206,"title":"Fuzzy systems are universal approximators,\u201d in","authors":[],"date":null,"doi":"10.1109\/fuzzy.1992.258721","raw":"L.-X. Wang, \u201cFuzzy systems are universal approximators,\u201d in Proc. 1st IEEE Conf. Fuzzy Syst. (FUZZ-IEEE), San Diego, CA, Mar. 8\u201312, 1992, pp. 1163\u20131170.","cites":null},{"id":16320213,"title":"Genetic Algorithms in Search, Optimization and Machine Learning.","authors":[],"date":"1989","doi":"10.5860\/choice.27-0936","raw":"D. E. Goldberg, Genetic Algorithms in Search, Optimization and Machine Learning. Reading, MA: Addison-Wesley, 1989.","cites":null},{"id":16320196,"title":"How good are fuzzy if\u2013then classi\ufb01ers?\u201d","authors":[],"date":"2000","doi":"10.1109\/3477.865167","raw":"L. I. Kuncheva, \u201cHow good are fuzzy if\u2013then classi\ufb01ers?\u201d IEEE Trans. Syst., Man, Cybern. B, Cybern., vol. 30, no. 4, pp. 501\u2013509, Aug. 2000.","cites":null},{"id":16320260,"title":"Improved use of continuous attributes in C4.5,\u201d","authors":[],"date":"1996","doi":null,"raw":"J. R. Quinlan, \u201cImproved use of continuous attributes in C4.5,\u201d J. AI Res., vol. 4, pp. 77\u201390, 1996.","cites":null},{"id":16320229,"title":"Incremental rule learning based on example nearness from numerical data,\u201d in Proc.","authors":[],"date":"2005","doi":"10.1145\/1066677.1066808","raw":"F. Ferrer-Troyano, J. S. Aguilar-Ruiz, and J. C. Riquelme, \u201cIncremental rule learning based on example nearness from numerical data,\u201d in Proc. 2005 ACM Symp. Appl. Comput., 2008, pp. 568\u2013572.","cites":null},{"id":16320239,"title":"Issues in data stream management,\u201d in","authors":[],"date":"2003","doi":"10.1145\/776985.776986","raw":"L. Golab and M. T. Ozsu, \u201cIssues in data stream management,\u201d in Proc. ACM SIGMOD Conf., Jun. 2003, vol. 32, no. 2, pp. 5\u201314.","cites":null},{"id":16320199,"title":"J.C.Bezdek,J.Keller,R.Krishnapuram,andN.R.Pal,FuzzyModelsand Algorithms for Pattern Recognition and Image Processing.","authors":[],"date":"1999","doi":"10.1007\/b106267","raw":"J.C.Bezdek,J.Keller,R.Krishnapuram,andN.R.Pal,FuzzyModelsand Algorithms for Pattern Recognition and Image Processing. Norwell, MA: Kluwer, 1999.","cites":null},{"id":16320242,"title":"Learning fuzzy classi\ufb01cation rules from data,\u201d","authors":[],"date":"2003","doi":"10.1007\/978-3-7908-1829-1_13","raw":"J. Roubos, M. Setnes, and J. Abonyi, \u201cLearning fuzzy classi\ufb01cation rules from data,\u201d Inf. Sci., vol. 150, pp. 77\u201393, 2003.","cites":null},{"id":16320240,"title":"Learning in the presence of concept drift and hidden contexts,\u201d","authors":[],"date":"1996","doi":"10.1007\/bf00116900","raw":"G. Widmer and M. Kubat, \u201cLearning in the presence of concept drift and hidden contexts,\u201d Mach. Learning, vol. 23, no. 1, pp. 69\u2013101, 1996.","cites":null},{"id":16320250,"title":"Learning of fuzzy rules by mountain clustering,\u201d in","authors":[],"date":"1993","doi":"10.1117\/12.165030","raw":"R. R. Yager and D. P. Filev, \u201cLearning of fuzzy rules by mountain clustering,\u201d in Proc. SPIE Conf. Appl. Fuzzy Logic Technol., Boston, MA, 1993, pp. 246\u2013254.","cites":null},{"id":16320243,"title":"Mathematical analysis of fuzzy classi\ufb01ers,\u201d","authors":[],"date":"1997","doi":"10.1007\/bfb0052854","raw":"F. Klawonn and P. E. Klement, \u201cMathematical analysis of fuzzy classi\ufb01ers,\u201d Lect. Notes Comput. Sci., vol. 1280, pp. 359\u2013370, 1997.","cites":null},{"id":16320237,"title":"Mining emerging patterns and classi\ufb01cation in data streams,\u201d in","authors":[],"date":"2005","doi":"10.1109\/wi.2005.96","raw":"H. Alhammady and K. Ramamohanarao, \u201cMining emerging patterns and classi\ufb01cation in data streams,\u201d in Proc. 2005 IEEE\/WIC\/ACM Int. Conf. Web Intell. (WI 2005), Sep., pp. 272\u2013275.","cites":null},{"id":16320204,"title":"Neural Networks for Pattern Recognition.","authors":[],"date":"1995","doi":"10.1145\/294828.1067910","raw":"C. M. Bishop, Neural Networks for Pattern Recognition. Oxford, U.K.: Oxford Univ. Press, 1995.","cites":null},{"id":16320207,"title":"Obtaining interpretable fuzzy models from fuzzy clustering and fuzzy regression,\u201d in","authors":[],"date":"2000","doi":"10.1109\/kes.2000.885783","raw":"F. Hopner and F. Klawonn, \u201cObtaining interpretable fuzzy models from fuzzy clustering and fuzzy regression,\u201d in Proc. 4th Int. Conf. Knowl.-Based Intell. Eng. Syst. (KES), Brighton, U.K., 2000, pp. 162\u2013 165.","cites":null},{"id":16320256,"title":"On-line identi\ufb01cation of MIMO evolving Takagi\u2013Sugeno fuzzy models,\u201d in","authors":[],"date":null,"doi":"10.1109\/fuzzy.2004.1375687","raw":"P. Angelov, C. Xydeas, and D. Filev, \u201cOn-line identi\ufb01cation of MIMO evolving Takagi\u2013Sugeno fuzzy models,\u201d in Proc. Int. Joint Conf. Neural Netw. Int. Conf. Fuzzy Syst. (IJCNN-FUZZ-IEEE), Budapest, Hungary, Jul. 25\u201329, 2004, pp. 55\u201360.","cites":null},{"id":16320214,"title":"Oxford Advance Learner\u2019s Dictionary.","authors":[],"date":"1974","doi":"10.2307\/3586015","raw":"A. S. Hornby, Oxford Advance Learner\u2019s Dictionary. London, U.K.: Oxford Univ. Press, 1974.","cites":null},{"id":16320222,"title":"P.Angelov,V.Giglio,C.Guardiola,E.Lughofer,andJ.M.Lujan,\u201cAnapproach to model-based fault detection in industrial measurement systems with application to engine test benches,\u201d","authors":[],"date":"2006","doi":"10.1088\/0957-0233\/17\/7\/020","raw":"P.Angelov,V.Giglio,C.Guardiola,E.Lughofer,andJ.M.Lujan,\u201cAnapproach to model-based fault detection in industrial measurement systems with application to engine test benches,\u201d Meas. Sci. Technol., vol. 17, no. 7, pp. 1809\u20131818, 2006.","cites":null},{"id":16320219,"title":"P.AngelovandD.Filev,\u201cAnapproachtoon-lineidenti\ufb01cationofevolving Takagi\u2013Sugeno models,\u201d","authors":[],"date":"2004","doi":null,"raw":"P.AngelovandD.Filev,\u201cAnapproachtoon-lineidenti\ufb01cationofevolving Takagi\u2013Sugeno models,\u201d IEEE Trans. Syst., Man, Cybern. B, Cybern., vol. 34, no. 1, pp. 484\u2013498, Feb. 2004.","cites":null},{"id":16320211,"title":"P.DomingosandG.Hulten,\u201cCatchingupwiththedata:Researchissuesin mining data streams,\u201d presented at the Workshop Res. Issues Data Mining Knowl. Discovery,","authors":[],"date":"2001","doi":null,"raw":"P.DomingosandG.Hulten,\u201cCatchingupwiththedata:Researchissuesin mining data streams,\u201d presented at the Workshop Res. Issues Data Mining Knowl. Discovery, Santa Barbara, CA, 2001.","cites":null},{"id":16320224,"title":"Predicting quality of the crude oil distillation using evolving Takagi\u2013Sugeno fuzzy models,\u201d in","authors":[],"date":"2009","doi":"10.1109\/icsmc.2007.4413939","raw":"J. Macias, P. Angelov, and X. Zhou, \u201cPredicting quality of the crude oil distillation using evolving Takagi\u2013Sugeno fuzzy models,\u201d in Proc. 2006 Int. Symp. Evol. Fuzzy Syst., pp. 201\u2013207. Authorized licensed use limited to: Lancaster University Library. Downloaded on June 22, 2009 at 08:51 from IEEE Xplore.  Restrictions apply.ANGELOV AND ZHOU: EVOLVING FRB CLASSIFIERS FROM DATA STREAMS 1475","cites":null},{"id":16320232,"title":"S.Pang,S.Ozawa,andN.Kasabov,\u201cIncrementallineardiscriminantanalysis for classi\ufb01cation of data streams,\u201d","authors":[],"date":"2005","doi":null,"raw":"S.Pang,S.Ozawa,andN.Kasabov,\u201cIncrementallineardiscriminantanalysis for classi\ufb01cation of data streams,\u201d IEEE Trans. Syst., Man Cybern. B, Cybern., vol. 35, no. 5, pp. 905\u2013914, Oct. 2005.","cites":null},{"id":16320192,"title":"Selecting fuzzy if\u2013then rules for classi\ufb01cation problems using genetic algorithms,\u201d","authors":[],"date":"1995","doi":"10.1109\/91.413232","raw":"H. Ishibuchi, K. Nozaki, N. Yamamoto, and H. Tanaka, \u201cSelecting fuzzy if\u2013then rules for classi\ufb01cation problems using genetic algorithms,\u201d IEEE Trans. Fuzzy Syst., vol. 3, no. 3, pp. 260\u2013270, Aug. 1995.","cites":null},{"id":16320244,"title":"Simpl_eTS: A simpli\ufb01ed method for learning evolving Takagi\u2013Sugeno fuzzy models,\u201d in","authors":[],"date":null,"doi":"10.1109\/fuzzy.2005.1452543","raw":"P. Angelov and D. Filev, \u201cSimpl_eTS: A simpli\ufb01ed method for learning evolving Takagi\u2013Sugeno fuzzy models,\u201d in Proc. 2005 IEEE Int. Conf. Fuzzy Syst. (FUZZ-IEEE), Reno, NV, May 22\u201325, pp. 1068\u20131073.","cites":null},{"id":16320264,"title":"The MP13 approach to the KDD\u201999 classi\ufb01er learning contest,\u201d","authors":[],"date":"1999","doi":"10.1145\/846183.846202","raw":"V. Mikheev, A. Vopilov, and I. Shabalin, \u201cThe MP13 approach to the KDD\u201999 classi\ufb01er learning contest,\u201d ACM SIGKDD Explorations Newslett., vol. 1, no. 2, pp. 76\u201377, 1999.","cites":null},{"id":16320205,"title":"The Statistical Learning Theory.","authors":[],"date":"1998","doi":"10.1007\/978-1-4757-3264-1","raw":"V. N. Vapnik, The Statistical Learning Theory. New York: SpringerVerlag, 1998.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2008-12-22","abstract":"A new approach to the online classification of streaming data is introduced in this paper. It is based on a self-developing (evolving) fuzzy-rule-based (FRB) classifier system of Takagi-Sugeno ( eTS) type. The proposed approach, called eClass (evolving class ifier), includes different architectures and online learning methods. The family of alternative architectures includes: 1) eClass0, with the classifier consequents representing class label and 2) the newly proposed method for regression over the features using a first-order eTS fuzzy classifier, eClass1. An important property of eClass is that it can start learning ldquofrom scratch.rdquo Not only do the fuzzy rules not need to be prespecified, but neither do the number of classes for eClass (the number may grow, with new class labels being added by the online learning process). In the event that an initial FRB exists, eClass can evolve\/develop it further based on the newly arrived data. The proposed approach addresses the practical problems of the classification of streaming data (video, speech, sensory data generated from robotic, advanced industrial applications, financial and retail chain transactions, intruder detection, etc.). It has been successfully tested on a number of benchmark problems as well as on data from an intrusion detection data stream to produce a comparison with the established approaches. The results demonstrate that a flexible (with evolving structure) FRB classifier can be generated online from streaming data achieving high classification rates and using limited computational resources","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/69986.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/19213\/1\/getPDF6.pdf","pdfHashValue":"c79066f28b12d5554af0ffed21de9b8787e52ece","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:19213<\/identifier><datestamp>\n      2018-01-24T02:31:02Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Evolving Fuzzy Rule-based Classifiers from Data Streams<\/dc:title><dc:creator>\n        Angelov, Plamen<\/dc:creator><dc:creator>\n        Zhou, Xiaowei<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        A new approach to the online classification of streaming data is introduced in this paper. It is based on a self-developing (evolving) fuzzy-rule-based (FRB) classifier system of Takagi-Sugeno ( eTS) type. The proposed approach, called eClass (evolving class ifier), includes different architectures and online learning methods. The family of alternative architectures includes: 1) eClass0, with the classifier consequents representing class label and 2) the newly proposed method for regression over the features using a first-order eTS fuzzy classifier, eClass1. An important property of eClass is that it can start learning ldquofrom scratch.rdquo Not only do the fuzzy rules not need to be prespecified, but neither do the number of classes for eClass (the number may grow, with new class labels being added by the online learning process). In the event that an initial FRB exists, eClass can evolve\/develop it further based on the newly arrived data. The proposed approach addresses the practical problems of the classification of streaming data (video, speech, sensory data generated from robotic, advanced industrial applications, financial and retail chain transactions, intruder detection, etc.). It has been successfully tested on a number of benchmark problems as well as on data from an intrusion detection data stream to produce a comparison with the established approaches. The results demonstrate that a flexible (with evolving structure) FRB classifier can be generated online from streaming data achieving high classification rates and using limited computational resources.<\/dc:description><dc:date>\n        2008-12-22<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/TFUZZ.2008.925904<\/dc:relation><dc:identifier>\n        Angelov, Plamen and Zhou, Xiaowei (2008) Evolving Fuzzy Rule-based Classifiers from Data Streams. IEEE Transactions on Fuzzy Systems, 16 (6). pp. 1462-1475. ISSN 1063-6706<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/19213\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1109\/TFUZZ.2008.925904","http:\/\/eprints.lancs.ac.uk\/19213\/"],"year":2008,"topics":["QA75 Electronic computers. Computer science"],"subject":["Journal Article","PeerReviewed"],"fullText":"1462 IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. 16, NO. 6, DECEMBER 2008\nEvolving Fuzzy-Rule-Based Classifiers\nFrom Data Streams\nPlamen P. Angelov, Senior Member, IEEE, and Xiaowei Zhou, Student Member, IEEE\nAbstract\u2014A new approach to the online classification of stream-\ning data is introduced in this paper. It is based on a self-\ndeveloping (evolving) fuzzy-rule-based (FRB) classifier system of\nTakagi\u2013Sugeno (eTS) type. The proposed approach, called eClass\n(evolving classifier), includes different architectures and online\nlearning methods. The family of alternative architectures includes:\n1) eClass0, with the classifier consequents representing class label\nand 2) the newly proposed method for regression over the fea-\ntures using a first-order eTS fuzzy classifier, eClass1. An important\nproperty of eClass is that it can start learning \u201cfrom scratch.\u201d Not\nonly do the fuzzy rules not need to be prespecified, but neither\ndo the number of classes for eClass (the number may grow, with\nnew class labels being added by the online learning process). In\nthe event that an initial FRB exists, eClass can evolve\/develop it\nfurther based on the newly arrived data. The proposed approach\naddresses the practical problems of the classification of streaming\ndata (video, speech, sensory data generated from robotic, advanced\nindustrial applications, financial and retail chain transactions, in-\ntruder detection, etc.). It has been successfully tested on a number\nof benchmark problems as well as on data from an intrusion de-\ntection data stream to produce a comparison with the established\napproaches. The results demonstrate that a flexible (with evolving\nstructure) FRB classifier can be generated online from streaming\ndata achieving high classification rates and using limited compu-\ntational resources.\nIndex Terms\u2014Evolving fuzzy systems, fuzzy-rule-based (FRB)\nclassifiers, recursive least squares (RLS), Takagi\u2013Sugeno fuzzy\nmodels.\nI. INTRODUCTION\nA. Background and the State of the Art\nFUZZY systems have been successfully applied to differentclassification tasks [1]\u2013[7] including, but not limited to,\ndecision making, fault detection, pattern recognition, and im-\nage processing. Fuzzy-rule-based (FRB) systems have become\none of the alternative frameworks for classifier design, together\nwith the more established Bayesian classifiers [8]\u2013[10], decision\ntrees [11], [12], and more recently, neural network (NN)-based\nclassifiers [13] and support vector machines [14]. Since the clas-\nsifier task is to map the set of features of sample data onto a\nset of class labels, fuzzy systems are particularly suitable as\nproven universal approximators [15]. Additionally, when com-\npared with the NN-based classifiers, they have the advantage of\nManuscript received February 2, 2007; revised June 26, 2007, October 15,\n2007, and December 11, 2007; accepted December 19, 2007. First published\nMay 23, 2008; current version published December 19, 2008.\nThe authors are with the Intelligent Systems Research Laboratory, Depart-\nment of Communication Systems, Infolab21, Lancaster University, Lancaster\nLA1 4YW, U.K. (e-mail: p.angelov@lancaster.ac.uk).\nColor versions of one or more of the figures in this paper are available online\nat http:\/\/ieeexplore.ieee.org.\nDigital Object Identifier 10.1109\/TFUZZ.2008.925904\ngreater transparency and interpretability of results [16]. There-\nfore, recently, the so-called neurofuzzy hybrids have become\npopular [5]. Originally, FRB classifiers (similar to FRB systems)\nwere designed based on linguistic and expert knowledge. Since\nthe 1990s, however, the so-called data-driven approaches have\nbecome dominant in the fuzzy systems design area [5], [28].\nNote that data-driven methods can preserve and even improve\nthe interpretability of the fuzzy system [7], [16]\u2013[18].\nIn this paper, we use FRB classifiers as a promising linguistic\nframework that offers high performance (classification rate) and\ncomputational efficiency. Another important reason for using\nthis type of classifier is that it is suitable for the realization of\nincremental online and evolving schemes.\nB. Need for Evolving Online Classifiers\nWe are witnessing an information revolution; currently, large\nquantities of information are produced at a fast rate by the\nsensors in advanced industrial processes, autonomous systems,\nspace, and aircraft, by the users of the Internet, the finance in-\ndustry, consumer markets, etc. The challenges that are faced\nby information processing, and classification in particular, are\nrelated to: 1) the need to cope with huge amounts of data and\n2) process streaming data online and in real time [20], [21].\nStoring the complete dataset and analyzing the data streams in\nan offline (batch) mode is often impossible or impractical, and\ndata streams are very often nonstationary. At the same time,\nmost of the conventional classifiers are designed to operate in\nbatch mode and do not change their structure online (do not cap-\nture new patterns that may be present in the data stream once\nthe classifier is built). An example from the network intrusion\ndetection area is the arrival of new, previously unseen threats\n(hackers are very inventive and they use newer ways to disrupt\nthe normal operation of servers and users). Offline pretrained\nclassifiers may be good for certain scenarios, but they need to\nbe redesigned or retrained for new circumstances. The conven-\ntional classifiers applied to data streams extract a \u201csnapshot\u201d\nof the data stream and require all the previous data, which im-\nplies higher memory and computational requirements. This is\nin addition to the classical pair of requirements, namely: 1) high\nclassification rate and 2) simple and interpretable\/transparent\nclassifier structure [16], [17]. In contrast, the so-called incre-\nmental (or online) classifiers work on a per-sample basis and\nonly require the features of that sample plus a small amount of\naggregated information (a rule base, a small number of variables\nneeded for recursive calculations); but they do not require all\nthe history of the data stream (all previously seen data sam-\nples). Sometimes they are also called one-pass (each sample is\nprocessed only once at a time and is then discarded from the\n1063-6706\/$25.00 \u00a9 2008 IEEE\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on June 22, 2009 at 08:51 from IEEE Xplore.  Restrictions apply.\nANGELOV AND ZHOU: EVOLVING FRB CLASSIFIERS FROM DATA STREAMS 1463\nmemory). To address the challenge to classify streaming data\nonline and in real time with a classifier that adapts\/evolves its\nstructure, we introduce the eClass family.\nC. Evolving Versus Genetic\/Evolutionary Classifiers\nGenetic and evolutionary algorithms [22], [23] are computa-\ntional techniques for a \u201cdirected\u201d random search for a solution\nto a loosely formulated optimization problem, which borrows\nfrom nature the concept of genetic or population evolution based\non computational mechanisms that mimic such paradigms as\nmutation, chromosomes crossover, reproduction, and selection.\nThe definition that the Oxford Dictionary [24, p. 358] gives to\n\u201cgenetic\u201d is a \u201cbranch of biology dealing with the heredity, the\nways, in which characteristics are passed on from parents to\noffspring.\u201d Similarly, \u201cevolutionary\u201d [24, p. 294] is the \u201cdevel-\nopment of more complicated forms of life (plants, animals)\nfrom earlier and simpler forms.\u201d Genetic\/evolutionary algo-\nrithms have been applied successfully as learning techniques\nfor the offline design of FRB classifiers [1], [2] and decision\ntrees [25].\nMeanwhile, FRB systems that are evolving (\u201cgradually de-\nveloping\u201d) have recently been introduced [26] and successfully\napplied to a range of problems, such as online system identifica-\ntion [27], time series prediction [28], fault detection [29], intelli-\ngent sensors [30], and control. The definition of evolving in [24,\np. 294] is \u201cunfolding; developing; being developed, naturally\nand gradually.\u201d While genetic\/evolutionary is related to the pop-\nulation of individuals and parents-to-offspring heredity, evolving\nis applicable to individuals\u2019 self-development (known in humans\nas autonomous mental development). \u201cEvolving\u201d relates more\nto learning from experience, gradual change, knowledge gen-\neration from routine operation, and rules extraction from the\ndata. Such capabilities are highly demanded by autonomous,\nrobotic, and advanced industrial systems, among others. While\na genetic\/evolutionary FRB system generates new rules as a\ncrossover or mutation of previous rules driven by a \u201cdirected\u201d\nrandom process [1], [2], an evolving FRB system learns new\nrules from new data gradually preserving\/inheriting the rules\nlearned already. This is very similar to the way in which indi-\nvidual people (especially children) learn. In a similar way to a\nhuman, an evolving fuzzy system can be initiated by an initial\nrule base or can start learning \u201cfrom scratch.\u201d This paradigm has\nbeen applied to NNs [31] and fuzzy systems [26]\u2013[30]. In this\npaper, we propose evolving FRB classifiers (eClass), which are\ncharacterized by the \u201cgradual development\u201d of their structure\nfrom the streaming data.\nD. Evolving Versus Incremental\nIn the data mining literature, there are a number of approaches\nto address the problems of data streams [10]\u2013[12], [32], [33],\n[35]\u2013[39]. Incremental classifiers are one of them. They have\nbeen implemented in different frameworks: decision trees [32],\nNN-based [e.g., adaptive resonance theory (ART) [34], incre-\nmental learning vector quantizers (iLVQs)], probabilistic [e.g.,\nincremental versions of Bayesian classifiers [10], linear dis-\ncriminant analysis (iLDA) [33], principal component analysis\n(iPCA) [33]. Note, however, that the classifier structure in these\ncases is assumed to be fixed. In terms of data streams, such clas-\nsifiers cannot address the problem of the so-called concept drift\nand shift. By drift in the machine learning literature, they refer\nto a modification of the concept over time [38], [39]. This relates\nto a rather smooth transition of the data distribution from one\nlocal region of the feature space (described in a fuzzy classifier\nby a fuzzy rule) to another. Very often, however, there are con-\ncept shifts [38], [39]. This usually has some practical meaning,\nsuch as the sudden appearance of a fault or an abrupt change of\na regime of operation [29]. To capture such sudden and abrupt\nchanges, one would require not only tuning parameters of the\nclassifier, but a change in its structure as well. To the best of our\nknowledge, the FRB classifier, as proposed in this contribution,\nis the first one to have an evolving structure that can be seen\nas a technique to address the concept drift to new areas of the\ndata space and concept shift. NNs with evolving structure have\nalso been proposed recently [31], but so far they have only been\napplied to time series prediction and not to classification.\nE. Structure of the Paper\nThe remainder of the paper is organized as follows. Section II\ndescribes the architecture of eClass0, which is an FRB clas-\nsifier with class labels as outputs. Section III details a more\neffective scheme, eClass1, which performs regression over the\nfeatures. Section IV describes the experimental results based\non a number of widely used benchmark datasets organized as\npseudostreams, as well as using intruder detection data [40].\nFinally, Section V presents our conclusions. The derivation of\nthe recursive cosine-distance-based potential and pseudocode\nof the proposed algorithms are detailed in the Appendix.\nII. ECLASS0\nA. Architecture of eClass0\nA classifier is a mapping from the feature space to the class\nlabel space. An FRB classifier describes, with its antecedents\npart, the fuzzy partitioning of the feature space x \u2208 Rn, and\nwith the consequent part, the class label Li, i = [1,K]. The\nstructure of eClass0 follows this typical construct of an FRB\nclassifier [1]\u2013[4]:\nRi : IF\n(\nx1 is around xi\u22171\n)\nAND\n(\nx2 is around xi\u22172\n)\nAND \u00b7 \u00b7 \u00b7 AND (xn is around xi\u2217n ) THEN (Li)\n(1)\nwhere x = [x1 , x2 , . . . , xn ]T is the vector of features, Ri de-\nnotes the ith fuzzy rule, i = [1, N], N is the number of fuzzy\nrules,\n(\nxj is around xi\u2217j\n)\ndenotes the jth fuzzy set of the ith\nfuzzy rule, j = [1, n], xi\u2217 is the focal point of the ith rule\nantecedent (note that this is a prototype\u2014a real, existing data\nsample and not the mean), and Li is the label of the class of the\nith prototype (focal point).\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on June 22, 2009 at 08:51 from IEEE Xplore.  Restrictions apply.\n1464 IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. 16, NO. 6, DECEMBER 2008\nThe inference in eClass0 is produced using the well-known\n\u201cwinner takes all\u201d rule [1]\u2013[4], [42]:\nL = Li\u2217, i\u2217 =\nN\nargmax\ni=1\n(\n\u03c4 i\n) (2)\nwhere \u03c4 i denotes the firing level of (degree of confidence in)\nthe ith fuzzy rule, which is determined as a product (a t-norm\nthat represents the logical AND operator [2], [18]) of the mem-\nbership values \u00b5ij of the jth feature, j = [1, n], and the fuzzy set(\nxj is around xi\u2217j\n)\n:\n\u03c4 i =\nn\u220f\nj=1\n\u00b5ij (xj ), i = [1, N ]. (3)\nThe membership functions that describe the degree of asso-\nciation with a specific prototype are of Gaussian form (charac-\nterized by good generalization capabilities and coverage of the\nwhole feature space):\n\u00b5ij = exp\n\uf8eb\n\uf8ed\u22121\n2\n(\ndij\nrij\n)2\uf8f6\uf8f8 , i = [1, N ], j = [1, n]\n(4)\nwhere dij is the distance between a sample and the prototype\n(focal point) of the ith fuzzy rule and rij is the spread of the\nmembership function, which also represents the radius of the\nzone of influence of the fuzzy rule.\nNote that this representation resembles the normal distribu-\ntion [8], [9], and the spread of the membership function can also\nbe represented by the standard deviation. The spread rij is deter-\nmined based on the scatter \u03c3ij [43] of the data per cluster\/rule\n(i = [1, N]) and per feature (j = [1, n])\n\u03c3ijk =\n\u221a\u221a\u221a\u221a 1\nSik\nS i\nk\u2211\nl=1\nd2(xlj,x\ni\u2217\nj ), \u03c3\ni\nj1 = 1 (5)\nwhere Sik denotes the support of the ith cluster\/rule at the kth\ntime instant (when k data samples are read).\nNote that in (4) and (5), the projection of the distance dij on\nthe axis formed by the jth feature is used. This leads to hyper-\nellipsoidal clusters with different spreads for different features\n(see also Fig. 1) and facilitates the interpretation. Support is the\nnumber of points that are in the zone of influence of a clus-\nter [43]. It is initialized with 1 when a prototype is generated\nand incrementally increases by 1 afterward for each sample that\nis closer to that prototype than to any other:\nSik+1 = S\ni\nk + 1, i =\nN\nargmin\ni=1\n\u2225\u2225xk \u2212 xi\u2217\u2225\u2225 , k = 2, 3, . . . .\n(6)\nWhen a new cluster\/rule is formed, N \u2190 N + 1, its initial\nscatter is approximated by the average of the scatter for the\nexisting fuzzy rules for that class [43], and its support is initially\nset to 1:\n\u03c3N +1k =\n1\nN\nN\u2211\ni=1\n\u03c3ik , S\nN +1\nk \u2190 1, k = 2, 3, . . . . (7)\nFig. 1. Prototype-based classification versus mean-based classification.\nHere, we also introduce support per class Qlk , which repre-\nsents the number of data samples that are associated with the\nclusters that form the antecedent parts of all fuzzy rules that\nhave the same consequents Nl :\nQlk =\nN l\u2211\nj=1\nSjk . (6a)\nThe distance d in (5) can have a Euclidean [26]\u2013[28] or a\ncosine form. The cosine distance can cope with problems, such\nas a different number of features and zero values, and is defined\nas [9]\ndcos (xk , xl) = 1\u2212\n\u2211n\nj=1 xkjxlj\u221a\u2211n\nj=1 x\n2\nkj\n\u2211n\nj=1 x\n2\nlj\n(8)\nwhere xk and xl are two n-dimensional vectors.\nB. Learning eClass0\nTypically, FRB classifiers are trained offline using evolution-\nary algorithms [1]\u2013[4], [41], [42] or gradient-based schemes,\nsuch as backpropagation when combined with NN [5]. The\neClass family, however, is designed for online applications with\nan evolving (self-developing) FRB structure. To achieve this,\nthe antecedents of the FRB are formed from the data stream\naround highly descriptive focal points (prototypes) in the input\u2013\noutput space z =\n[\nxT , L\n]T per class. In this paper, we modify\nthe recently introduced recursive density-based fuzzy partition-\ning mechanism [44] that is very robust to outliers and noise\nso that it works per class and uses cosine distance. Also, we\napply this online evolving (open structure) partitioning to a\nclassification problem while it is originally introduced [27],\n[44] as a first stage of a predictive models design. This data\npartitioning mechanism identifies the most representative pro-\ntotypes and builds fuzzy rules around them. In this way, it leads\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on June 22, 2009 at 08:51 from IEEE Xplore.  Restrictions apply.\nANGELOV AND ZHOU: EVOLVING FRB CLASSIFIERS FROM DATA STREAMS 1465\nto the formation of information granules, described linguisti-\ncally by fuzzy sets. Thus, it serves the transformation of data\ninto primitive forms of knowledge. This online algorithm works\nsimilarly to adaptive control and estimation [45]\u2014in the pe-\nriod between two samples, two phases are performed: 1) class\nprediction (classification) and 2) classifier update or evolution.\nDuring the first phase, the class label is not known and is being\npredicted; during the second phase, however, it is known and is\nused as supervisory information to update the classifier (includ-\ning its structure evolution as well as its parameters update). The\nsecond phase may be active for certain data samples only.\nThe main difference between eClass0 and a conventional\nFRB classifier is: 1) the open structure of the rule base (it self-\ndevelops online starting \u201cfrom scratch,\u201d while in a conventional\nFRB classifier, it is determined offline and then fixed) and 2) the\nonline learning mechanism that takes into account this flexible\nrule-base structure. Note that the overall FRB is composed of\nK subrule bases so that, in each subrule base, the consequents\nof all rules are the same, but the number of rules N should\nbe no less than the number of classes (N \u2265 K). That is, every\nnew data sample with a class label that has not been previ-\nously seen becomes automatically a prototype. Since there is\na prototype replacement and removal mechanism, this is usu-\nally temporary (this prototype is often later replaced by more\ndescriptive prototypes). In this way, the classifier learns \u201cfrom\nscratch\u201d and the number of classes does not need to be known in\nadvance.\nThe focal points of the fuzzy rules are generated as a result\nof a projection of Gaussian membership functions centered at\nprototypes onto the axes representing different features. This\nprocess is widely used in data-driven approaches [41] for fuzzy\nsystems design and is facilitated by the use of standard Euclidean\nor cosine distance measures. The difference from most of the\nother classification approaches is that fuzzy sets in eClass are\nformed around existing data samples (prototypes), not around\nmeans (Fig. 1). Note that the latter can often be infeasible, as\nthe means are usually nonexisting abstract points.\nThe basic notion of the partitioning algorithm is that of the\npotential, which is defined as a Cauchy function of the sum\nof distances between a certain data sample and all other data\nsamples in the feature data space [44]. The interpretation of the\npotential is as a measure of the data density\nPk (zk ) =\n1\n1 +\n(\u2211k\u22121\ni=1 d(zk , zi)\n)\/\n(k \u2212 1)\n(9)\nEquation (9) defines the so-called global potential [43]. In\nthis paper, we define local (per class) potential as a measure of\nthe data density for that specific class, l:\nP lk (zk ) =\n1\n1 +\n(\u2211Ql\nk\n\u22121\ni=1 d(zk , zi)\n)\/\n(Qlk \u2212 1)\n. (10)\nPartitioning using the potential is based on the following\nprinciple: \u201cThe point with the highest potential is chosen to be\nthe focal point of the antecedent of a fuzzy rule\u201d [46], [47]. In this\nway, fuzzy rules with high descriptive power and generalization\ncapabilities are generated. This is illustrated in Fig. 2.\nFig. 2. Potential of data samples (Pima dataset, two of the features, k = 7).\nAlthough we used Euclidean distance in [26], in the present\npaper, we develop the recursive formulas for calculating the\npotential when cosine distance is used. The recursive calcula-\ntion that is introduced (the detailed derivation is given in the\nAppendix) needs the current data point zjk and (n + 1) memo-\nrized quantities only (\u03b2k and \u03c7jk , j = [1, n])\nP lk (zk ) =\n1\n2\u2212\n[\n1\/\n\u221a\u2211n+1\nj=1 (z\ni\nk )2\n]\u2211n+1\nj=1 z\ni\nk b\ni\nk\u22121\n,\nk = 2, 3, . . . , P1 (z1) = 1 (11)\nwhere\nbik = b\ni\nk\u22121 +\n\u221a\u221a\u221a\u221a\u221a\n(\nzik\n)2\n\u2211n+1\nj=1\n(\nzjk\n)2 , bi1 =\n\u221a\u221a\u221a\u221a\u221a\n(\nzi1\n)2\n\u2211n+1\nj=1\n(\nzj1\n)2 ,\ni = [1, n + 1].\nEach time a new data sample is read, it affects the data den-\nsity; therefore, the potentials of the existing centers need to be\nupdated. This update can also be done in a recursive way (the\nderivation is also given in the Appendix):\nP lk\n(\nzi\u2217l\n)\n=\n(\nQlk \u2212 1\n)\nP lk\u22121\n(\nzi\u2217l\n)\nQlk\u22121+\n[(\nQlk\u22122\n)( 1\nP l\nk \u22121 (z i \u2217l )\n\u2212 1\n)\n+dcos\n(\nzi\u2217l , zk\n)] ,\nk = 2, 3, . . . . (12)\nOnce the potential of the new coming data sample is cal-\nculated recursively using (11) and the potential of each of the\npreviously existing prototypes is recursively updated using (12),\nthey are compared. If the new data sample has a higher potential\nthan any of the previously existing prototypes of that class, L,\nthen it is a good candidate to become a focal point of a new rule\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on June 22, 2009 at 08:51 from IEEE Xplore.  Restrictions apply.\n1466 IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. 16, NO. 6, DECEMBER 2008\nin this subrule base because it has high descriptive power and\ngeneralization potential:\nPLk (zk ) > P\nL\nk\n(\nzi\u2217L\n) \u2200i\u2217 \u2208 NL. (13)\nForming a new fuzzy rule around the newly added prototype\nleads to a gradual increase of the size of the subrule base, which\nis why the approach is called \u201cevolving\u201d:\nz(N\nL +1)\u2217 \u2190 zk . (14)\nThe potential of the newly generated rule is set to 1 tem-\nporarily (it will be updated to take into account the influence\nof each new data sample on the generalization potential of this\nparticular focal point by (12) with each new data sample being\nread):\nPLk\n(\nz(N\nL +1)\u2217\n)\n\u2190 1. (15)\nTo increase the interpretability and update the rule base, one\nneeds to remove previously existing rules that become ambigu-\nous after the insertion of a new rule. Therefore, each time a new\nfuzzy rule is added, it is also checked whether any of the already\nexisting prototypes in this subrule base is described by this rule\nto a degree higher than e\u22121 (this is an analogy to the so-called\none-sigma condition [9]):\n\u2203i, i = [1, NL ], \u00b5ji (zN\nL +1) > e\u22121 \u2200j, j = [1, n].\n(16)\nIf any of the previously existing prototypes satisfies this con-\ndition, the rules that correspond to them are removed from this\nsubrule base (in fact, replaced by the newly formed rule).\nThe spreads of the membership functions of the subrule base\nof the respective class are also recursively updated based on the\ndata distribution [48]:\nrik = \u03c1r\ni\nk\u22121 + (1\u2212 \u03c1)\u03c3ik\u22121 , i = [1, NL ] (17)\nwhere \u03c1 is a learning parameter; it determines how quickly the\nspread of the membership functions will converge to the local\nscatter of that cluster. The default value of 0.5 further simplifies\nthe expression into\nrik =\nrik\u22121 + \u03c3\ni\nk\u22121\n2\n, i = [1, NL ]. (18)\nThe procedure of eClass0 is summarized in the pseudocode\ngiven in the Appendix.\nIII. ECLASS1\nA. Architecture of eClass1\nThe architecture of eClass1 differs significantly from the ar-\nchitecture of eClass0 and the typical FRB [1]\u2013[5], [41], [42] in\nthat it regresses over the feature vector using first-order multiple-\ninput\u2013multiple-output evolving Takagi\u2013Sugeno (MIMO-eTS)\nmodels [49]. Note that the classification surface in a data stream\nis dynamically changing (see Fig. 3), and eClass1 aims to evolve\nits rule base so that it reacts to this by dynamically adapting pa-\nrameters of the classifier (spreads, consequent parameters) as\nwell as the focal points and the size of the rule base. eClass1 is\nFig. 3. Evolution of classification boundary in eClass1-MISO (pima dataset,\ntwo of the features) (a) after k = 50 data samples are read, and (b) after k = 80\ndata samples are read.\ndefined by a set of rules of the following type:\nRi : IF\n(\nx1 is around xi\u22171\n)\nAND \u00b7 \u00b7 \u00b7 AND (xn is around xi\u2217n )\nTHEN\n(\nyi = xT \u0398\n) (19)\nwhere x = [1, x1 , x2 , . . . , xn ]T denotes the (n + 1)\u00d7 1 ex-\ntended vector of features and yi is the output.\nThe outputs of particular rules can be normalized so that they\nsum up to 1 by\nyi =\nyi\u2211N\ni=1 y\ni\n. (20)\nOne can easily check that\n\u2211N\ni=1 y\ni = 1.\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on June 22, 2009 at 08:51 from IEEE Xplore.  Restrictions apply.\nANGELOV AND ZHOU: EVOLVING FRB CLASSIFIERS FROM DATA STREAMS 1467\nThe normalized outputs per rule can be interpreted [56] as\nthe possibility of a data sample belonging to a certain class\nif we use target labels for the classes: 0 for nonmembership\nand 1 for membership. In that respect, this approach has some\nsimilarity to the so-called indicator matrix [8] used in offline\ncrisp classifiers. Note that a conventional FRB classifier has a\nclass label as outputs of each rule. In the case that MISO eTS is\nused (for two-class problems), outputs per fuzzy rule are scalars\nand the approach resembles a fuzzily weighted locally valid\nLDA. In this case, parameters of the local linear models \u0398 are\nvector column \u0398 =\n[\n\u03b8i01 \u03b8\ni\n11 \u00b7 \u00b7 \u00b7 \u03b8in1\n]T\n. In the case that\nMIMO eTS is used, they can form an (n + 1)\u00d7K matrix\n\u0398 =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\n\u03b8i01 \u03b8\ni\n02 \u00b7 \u00b7 \u00b7 \u03b8i0K\n\u03b8i11 \u03b8\ni\n12 \u00b7 \u00b7 \u00b7 \u03b8i1K\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\n\u03b8in1 \u03b8\ni\nn2 \u00b7 \u00b7 \u00b7 \u03b8inK\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb .\nIn this case, the outputs of the fuzzy rules form a K-\ndimensional row vector\u2014one normalized output for each class,\nyi = [yi1 , y\ni\n2 , . . . , y\ni\nK ].\nThe overall output in eClass1 is formed as a weighted sum\nof the normalized outputs of each rule, also called the \u201ccenter-\nof-gravity\u201d [50] principle, which is typical for regression, time\nseries prediction, and modeling, but not for classification:\ny =\nN\u2211\ni=1\n\u03c4 i\u2211N\nj=1 \u03c4j\nyi. (21)\nThe output is then used to discriminate between the classes.\nIn a two-class problem using MISO eTS, one can have targets of\n0 for class 1 and 1 for class 2, or vice versa. The discrimination\nin this case will be at a threshold 0.5 with outputs above 0.5\nclassified as class 2 and outputs below 0.5 classified as class 1,\nor vice versa. In problems with more than two classes, one can\napply MIMO eTS, where each of the K outputs corresponds to\nthe possibilities of the data sample to belong to a certain class\n(as discussed before). Note that one can use MIMO eTS for a\ntwo-class problem, too. Then the target outputs will be 2-D, e.g.,\ny = [1 0] for class 1 and y = [0 1] for class 2 or vice versa. The\nlabel for eClass1-MISO (which is used in two-class problems)\nis determined by\nIF (y > 0.5) THEN (Class1) ELSE (Class2) .\n(22)\nFig. 3 illustrates the classification of a benchmark (Pima [51])\ndataset into two classes (signs of diabetes or no signs of diabetes)\nafter 50 and after 80 data samples. The two fuzzy rules formed\nare represented by their focal points in a 2-D illustration for two\nof the nine features. The nonlinear classification surface (the\nsolid curve in Fig. 3) is derived for this two-class problem as\ny = 0.5. This figure illustrates the dynamics when comparing\nthe two plots (after 50 samples are read, and after 80 samples),\nwhich illustrates the need for online and evolving classification\nschemes.\nIn eClass1-MIMO, the label is determined by the highest\nvalue of the discriminator yl :\nL = Li\u2217, i\u2217 =\nK\nargmax\nl=1\nyl . (23)\nNote that (23) is not the typical \u201cwinner takes all\u201d prin-\nciple in terms of the firing strength of the rules as applied\nin classification problems. In this sense, the proposed ap-\nproach resembles more the LDA [9] and support vector ma-\nchine [14], rather than typical fuzzy classifiers [1]\u2013[5], [7]. Also\nnote that a two-class problem can be solved by eClass1-MISO\nand by eClass1-MIMO, but while the number of parameters\nof the antecedent part in both architectures is the same, the\nnumber of consequent parameters in eClass1-MISO is halved\n(n + 1 since \u0398 = [\u03b8i01 \u03b8i11 \u00b7 \u00b7 \u00b7 \u03b8in1 ]T ) compared to eClass1-\nMIMO, i.e., 2\u00d7 (n + 1), since\n\u0398 =\n[\n\u03b8i01 \u03b8\ni\n11 \u00b7 \u00b7 \u00b7 \u03b8in1\n\u03b8i02 \u03b8\ni\n12 \u00b7 \u00b7 \u00b7 \u03b8in2\n]T\n.\nIn eClass1, the quality of the fuzzy rules is constantly mon-\nitored by calculating their \u201cage.\u201d The age of a fuzzy rule [43]\nis especially important for data streams. It gives accumulated\ninformation about the timing of when a certain sample was\nassigned to a cluster\/respective, rule. It is well known that in-\ncremental approaches are order-dependent. With a rule\u2019s age,\none can make use of this specific feature of the data streams. It\nis proposed that age is calculated as\nAi = k \u2212\n\u2211S i\nk\nl=1 kl\nSik\n(24)\nwhere kl is the time index when the data sample was read.\nNote that the factor\n\u2211S i\nk\nl=1 kl\/S\ni\nk can easily be calculated re-\ncursively, and thus, A itself can be calculated recursively. When\na new data sample creates a new rule, its age is initiated by\nthe index of that sample (the time instant, if in a time-related\nstream). Each time a new data sample is simply assigned to an\nexisting rule, the age of that rule gets smaller [see (24)]. If no\nsample is assigned to a rule, it gets older by 1. Note that the\nrange of A is [0; k]. The age of fuzzy rules (and aging rate\u2014a\nderivative of age in terms of the sampling period k) proved to be\nvery useful in the online analysis of the concept drift in the data\nstream. While the classifiers of the eClass family aim to react to\nthe concept shift by evolving their structure online, generating\nnew rules and removing outdated rules, the online analysis of\nthe (first and second) gradient of the age of the rules can identify\nthe concept drift. In Fig. 4, we illustrate this for the example of\nan ionosphere dataset from the University of California, Irvine\n(UCI), repository [51].\nThe first image [Fig. 4(a)] illustrates the case where the rules\nare aging with a normal rate (the gradient of the age is smaller\nbut close to 1, and more importantly, it is nearly constant).\nThe second image [Fig. 4(b)] illustrates a case where there are\nperiods (around samples 155 and 213) where the data cause a\nstrong update of a specific rule (no. 3). In this case, the gradient\nof the age decreases and even becomes negative, and then, its\nage increases again (the rule is not updated so often or not at\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on June 22, 2009 at 08:51 from IEEE Xplore.  Restrictions apply.\n1468 IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. 16, NO. 6, DECEMBER 2008\nFig. 4. Age evolution (ionosphere dataset). (a) Rule 2, no drift. (b) Rule 3,\ndrift detected.\nall). The explanation is that when a concept drift occurs (as in\nthe second image), the frequency of usage of some rules (as for\nrule 3) will decrease because a transition from one operating\nstate, which affects the data density in one local region (around\nthe focal point of rule 3), to another one, which affects the data\ndensity in another local region, takes place.\nThe following simple rule for updating of the rule base is\nintroduced by removing older rules (rules whose age is above\nthe mean age for that rule by more than the \u201cone-sigma\u201d value,\ncalculated recursively up to that moment):\nIF (Ai > Ai + std(Ai)) THEN (\u03bbi \u2190 0;N \u2190 (N\u22121))\n(25)\nwhere Ai denotes the mean age and std(Ai) denotes the standard\ndeviation of the age of the ith rule.\nB. Learning eClass1\nLearning for eClass1 is based on the approach for online\nlearning TS models with evolving structure introduced in [27]\nand extended for the MIMO case in [49]. Learning is based on\nthe decomposition of the identification problem into: 1) FRB\nstructure design and 2) parameters identification. Both of these\nsubproblems can be performed in online mode during a single\ntime step (per sample).\nThe first subproblem, structure identification, is addressed\nusing scatter-based fuzzy partitioning as described for eClass0.\nThe main difference is that, in eClass1, the potential is cal-\nculated globally and not per class. The reason is that the aim\nof partitioning in eClass0 is different from that in eClass1. In\neClass0, it is performed with the aim of identifying representa-\ntive prototypes, which have high within-class density. Note that\nthis is within-class, but not necessarily within-cluster (per rule)\ndensity and is expressed mathematically by the local potential\n[see (9)]. In eClass1, however, clustering serves quite a different\nrole\u2014it is one of the two phases of the TS model identification\nand is combined with the consequents parameter identification\nusing a version of the recursive least squares (RLS) technique. A\nsimilar approach is used for offline TS fuzzy systems designed\nin [47] and for evolving online TS (eTS) in [27]. In this paper,\nwe use the online learning of eTS to regress on the features in\norder to estimate the possibility of a data sample belonging to\na certain class. Since, in eClass1, we have one FRB that is not\ndivided per class, the global potential is used to identify the\nprototypes that are representative for any class. It is recursively\ncalculated by (11) but applied globally, not per class. The equa-\ntion for the update of the potential of all previous prototypes\neach time a new data sample is read is given by:\nPk\n(\nzi\u2217\n)\n=\n(k \u2212 1)Pk\u22121\n(\nzi\u2217\n)\nk \u2212 1 + [(k \u2212 2) [{1\/Pk\u22121(zi\u2217)} \u2212 1] + dcos (zi\u2217, zk )] ,\nk = 2, 3, . . . . (12a)\nThen, the potentials of the new data sample (11) and the\nupdated potential of all of the previous focal points are compared\nby applying (13). The algorithm then follows the logic of the\nalgorithm described in Section II-B with the only differences\nbeing that the potential is global instead of local and N l is\nreplaced by N in all equations.\nOnce the antecedents are formed, the consequents parameters\nestimation (the second subproblem of the design of eClass1) is\naddressed as a fuzzily weighted RLS estimation problem per\nrule [27]:\n\u0398ik = \u0398\ni\nk\u22121 + C\ni\nk\u03bb\nixk (yk \u2212 xTk \u0398ik\u22121), \u0398i1 = 0 (26)\nCik = C\ni\nk\u22121 \u2212\n\u03bbiCik\u22121xkx\nT\nk C\ni\nk\u22121\n1 + \u03bbixTk C\ni\nk\u22121xk\n, Ci1 = \u2126I, k = 2, 3, . . .\n(27)\nwhere C \u2208 RN (n+L)xN (n+L) denotes the covariance matrix, \u2126\nis a large positive number, and I is the identity matrix.\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on June 22, 2009 at 08:51 from IEEE Xplore.  Restrictions apply.\nANGELOV AND ZHOU: EVOLVING FRB CLASSIFIERS FROM DATA STREAMS 1469\neClass1 is optimally (in LS sense) [27], [48] tuned in terms\nof consequents parameters \u0398. In terms of its antecedents and\nrule-base structure, it is based on the robust online partition-\ning approach [44]. Condition (13) is very hard to satisfy, and\ntherefore, usually a small number of rules are formed; addition-\nally, condition (16) further automatically removes rules with\nambiguous meaning. The objective of the optimal estimation\n[seen from the factor in brackets in (26)] is to generate values\nas close as possible to 1 for the samples of certain classes and\nvalues as close to 0 for all other classes by regressing on the\nfeature space [see (19)]. In this way, eClass1 is unique as an\nevolving robust nonlinear FRB classifier suitable for the online\napplications required in data streams. The algorithm procedure\nis given in the Appendix.\nThe main novelties of eClass1 can be summarized as follows.\n1) It is evolving (the rule base is not pretrained and fixed;\nlearning can start \u201cfrom scratch\u201d with the very first data\nsample).\n2) It can have a MIMO structure and thus build a separate\nregression model for each class (output). Note that if a\nsample with a previously unseen class label is met, the\nMIMO structure expands naturally by initializing learning\nof the new (L + 1)th class from this point onward in the\nsame way as for the remaining L classes.\n3) It can attain high classification rates compared favorably\nto well-known offline and incremental classifiers (as seen\nin the next section).\n4) It is one-pass, recursive, and therefore, has extremely low\nmemory requirements.\n5) It is useful for online analysis and monitoring of the con-\ncept drift using fuzzy rules aging (see Fig. 4).\nThe effect of gradual evolution in learning is illustrated in\nFig. 5, where the same validation data have been used after\ndifferent periods of incremental and evolving learning for\nthe example of intrusion detection data. It can be seen that\nthe classification rate improves when more data are used for\nevolving the classifier online. The increase in the classification\nrate is not perfectly smooth [as can be seen in the zoomed area in\nFig. 5(b)], as the new data samples bring noise as well as useful\ninformation. Also, as new classes are met for the very first time,\nthe classification rate drops for a short period. In eClass1, the\nnew data samples are used not only to form new prototypes,\nbut also to update the consequents parameters by (26) and (27),\nin order to adapt to the dynamically changing nonlinear classi-\nfication surface (Fig. 3). It is obvious, however, that an overall\nstabilization of the classification rate to values well over 90%\nbased on only four fuzzy rules [the time stamp of which is shown\nin Fig. 5(a)] occurs after a very small number of samples\u2014just\nover 100 out of a total dataset of 5 000 000 samples. Similar\nbehavior has been observed for the other experimental cases.\nIV. EXPERIMENTAL RESULTS\nThe classifiers from the eClass family were tested on a num-\nber of widely used benchmark problems from the UCI machine\nlearning repository [51] and on a large data stream of intrusion\ndetection data [40]. Note that the datasets from the UCI reposi-\nFig. 5. Evolution of the classification rate during online learning of eClass1\nwith a subset of intruder detection dataset. (a) Subset of 4000 data samples from\nthe four subclasses; the square dots indicate the time instant of the generation\nof a new fuzzy rule (rule base evolution). (b) Zoom-in of the previous plot for\n1500 data.\ntory, despite their relatively small size and often lacking time tag,\ncan be considered as pseudodata streams. For example, if credit\ncard data are collected for a long period of time, an interest-\ning data stream would be generated similar to the \u201ccredit card\u201d\ndataset from [51]. The main reason these benchmark datasets\nwere used in this paper is that a number of solutions exist in the\nliterature, mostly based on offline classification approaches [35],\nand also on incremental versions [32]. The main aim of compar-\ning eClass to existing classifiers is to illustrate that it can not only\ngive similar or even better classification rates if working offline,\nbut can also operate online on a per-sample basis, which is a\ncomputationally more efficient and flexible option. The robust-\nness of eClass was also tested using noisy training data. Offline\nexperiments were performed using tenfold cross-validation and\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on June 22, 2009 at 08:51 from IEEE Xplore.  Restrictions apply.\n1470 IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. 16, NO. 6, DECEMBER 2008\nstatistical averaging to compare the classification rate. Well-\nestablished tree-based classifiers C4.5 [52] and kNN [9] were\nused in the tests in the following two modes: 1) tenfold cross-\nvalidation and 2) an incremental version using a sliding window\nas described in [37]. eClass was run on the intrusion detection\ndata online recursively. In what follows, each of the experimen-\ntal datasets used will be briefly described (more details can be\nfound in [40] and [51]) and the results tabulated and analyzed.\nA. Benchmark Problems From UCI Repository\n1) Sonar Data: This dataset contains patterns obtained from\nthe reflected sonar signals from a metal cylinder at various angles\nand under various conditions. Based on the readings of these\nsignals, it is required to classify the object as a rock or a mine\n(metal cylinder). Thus, there are two classes. The inputs consist\nof the reflected wave energy at 60 frequencies from different\nangles. This problem is a very realistic potential application\nof eClass. Similar experiments were performed at Lancaster\nUniversity\u2019s Intelligent Systems Research Laboratory using the\nPioneer 3DX mobile robot reported in [53].\n2) Credit Card Dataset: This is a set of records about the\ninformation related to credit card application processing. The\nclassifier should assist the decision to accept the application for\na new credit card or not. This is a very realistic data stream\nscenario if we assume that a huge amount of data is required\nto be processed in a short time with new data possibly having\na changing pattern. In the UCI repository, a \u201csnapshot\u201d of such\na scenario is presented, but one can use this to make compar-\nisons with offline classifiers, keeping in mind that eClass is\ndesigned to work in the more realistic scenario where the data\nare presented in real time and in significantly greater quantity.\nIn this classification problem, there are 15 features describing\nthe applicants, such as their annual income, age, marital status,\netc. The number of classes is two: unsuccessful and successful\napplications.\n3) Ten-Digit Dataset: This dataset contains the handwriting\ninformation from a pressure-sensitive tablet PC. The aim is to\nclassify the ten digits from 0 to 9. In this way, the number\nof classes is ten. Sixteen features represent the readings from\ndifferent areas of the tablet.\n4) Ionosphere Dataset: This set contains 34 attributes of\nthe pulse return from a radar that scans the Earth\u2019s ionosphere\n[51]. Based on these features, the classifier is required to judge\nwhether the radar being tested is working or not. The features\nare obtained by analyzing the complex electromagnetic signal.\n5) Pima Dataset: This is a database collected from the pop-\nulation of Phoenix, USA, which contains information about\npatients who show signs of diabetes [51]. The aim is to aid\nthe diagnostic process by classifying the patients into \u201cpossi-\nble\u201d and \u201cno\u201d diabetes classes using eight different features of\nthe individual, such as blood pressure, number of times preg-\nnant, age, etc. A real-life scenario could include a much larger\ndatabase of a similar type, which is completed every day, and\nthis would constitute a data stream. One can apply eClass to\nsuch a realistic scenario and the classifier will evolve, adapting\nto the new data. In this experiment, the main aim was to compare\nthe performance to established classifiers (kNN and C4.5).\n6) Wine Dataset: This dataset contains the results of a chem-\nical analysis of wines grown in the same region of Italy but\nderived from three different cultivars. The analysis determined\nthe quantities of 13 constituents found in each of the three types\nof wines [51].\nFor each of the aforementioned six datasets, a tenfold cross-\nvalidation was applied using eClass, kNN, and C4.5. This was\nrepeated ten times with randomized sample order each time,\nand the results were averaged to eliminate the effect of order-\ndependency. As a result, 100 runs were performed for each\napproach. The results are listed in Table I. Note that eClass\nis one-pass (processes each sample just once at the time of its\narrival and disregards it afterward), while both kNN and C4.5 use\nall training data (90% of the whole dataset) in the memory and\nmake many iterations. The validation is done in all approaches\non the remaining unused 10% of the data. The time required to\nprocess one sample is also presented in microseconds. Note that\nfor the kNN and C4.5, this is derived as the time required to train\nthe classifier divided by the number of training samples, while\nfor eClass, this includes both training and prediction steps since\neClass does not require pretraining and starts \u201cfrom scratch.\u201d\nThe overall training time for kNN and C4.5 is a multiplication\nof the number of training samples by the time shown in Table I.\nThe result demonstrates that eClass (especially eClass1) can\ncompete well with established offline approaches even though\nthe learning is performed in a single pass and a significantly\nsmaller memory is used. Additionally, the structure of eClass is\nopen, which allows capturing new data patterns.\nIn another experiment, in order to examine the robustness\nof eClass, we added random normally distributed 5% noise to\nthe training datasets within the same experimental setting. The\nresults are tabulated in Table II. They are very close to the results\nachieved without noise, which demonstrates the robustness of\neClass that is due to the very conservative partitioning, which\nnaturally avoids outliers (see Fig. 2 where the red point does\nnot have a chance to become a prototype because there will be\nother points with higher potential).\nAdditionally, the proposed classifiers have been compared\nto the incremental versions of the well-established classifiers\nC4.5 and kNN, as described in [35]. The original datasets were\ndivided into tenfolds, and each time, two consecutive folds have\nbeen used for training with the prediction performed on the next\nfold. This incremental, but not sample-by-sample, approach\nleads to sliding window versions of C4.5 and kNN. Note that\neClass in its original form works on a per-sample basis and does\nnot need a sliding window. The results are tabulated in Table III.\nThe results clearly demonstrate the superiority of eClass over\nthe incremental versions of kNN and C4.5.\nB. Intrusion Detection Data Stream\nThe second experiment was carried out on a real data stream\ncalled \u201cintrusion detection dataset,\u201d which has been used in\nthe Knowledge Discovery and Data Mining (KDD) 1999 Cup\ncompetition [40]. In this scenario, eClass works as an evolving\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on June 22, 2009 at 08:51 from IEEE Xplore.  Restrictions apply.\nANGELOV AND ZHOU: EVOLVING FRB CLASSIFIERS FROM DATA STREAMS 1471\nTABLE I\nRESULTS OF THE TENFOLD CROSS-VALIDATION OFFLINE EXPERIMENT\nTABLE II\neClass WITH NOISY DATA (TEST FOR ROBUSTNESS)\nintrusion detector to classify between a normal connection to\na server and different types of attacks (intrusion connections).\nThe input data flow contains the details of the connections,\nsuch as protocol type, connection duration, login type, etc. In\ntotal, 40 features have been used [40]. Each connection can be\ncategorized into five main classes (one normal class and four\nmain intrusion classes: probe, DOS, U2R, R2L) [40]. There are\n22 different types of attacks that were grouped into the four main\ntypes listed before (probe, DOS, U2R, R2L). The experimental\nsetting is the same as the one used in the KDD 1999 Cup [40],\ntaking 10% of the whole real raw data stream (over 5 000 000\ndata samples) for training and 311 029 data samples for testing.\nThis was done to make a comparison with the results published\nfor this data stream, including the results from the competition.\nThe results of the comparison of eClass with the winner\nof the competition and with the established approaches (kNN\nand C4.5) are tabulated in Table IV. Note that all the partic-\nipants used offline approaches; some used expert knowledge\n[54], while eClass starts with no predefined rules and is fully\nautomatic. The overall cost of the result (taking into account the\nimportance and weight of each misclassification) calculated in\nthe same manner as in [54] is 0.2480 for eClass1 and 0.3020 for\neClass0. Both results are better than the average result achieved\nby all the 24 participants in the competition (a cost of 0.3114).\nThe result achieved by eClass1 is practically the same as the one\nachieved by C4.5 and marginally worse than the result achieved\nby the winner [37]. Both C4.5 and the approach used by the\nwinner (using bagging and boosting plus intensive preprocess-\ning) are offline and require a batch set of training data. Note\nthat eClass1 achieved a better result on the rarest type of attack\n(R2L) than the winner of the competition. This is due to the\nability of eClass1 to adapt quickly to new data patterns.\nThe final test was performed on a small subset of the intrusion\ndata (containing 4000 data samples randomly selected from the\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on June 22, 2009 at 08:51 from IEEE Xplore.  Restrictions apply.\n1472 IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. 16, NO. 6, DECEMBER 2008\nTABLE III\nONLINE EXPERIMENT WITH UCI-BASED DATASETS\nTABLE IV\nCOMPARISON OF THE RESULTS FOR THE INTRUDER DETECTION PROBLEM\nfour subclasses met in the dataset) in order to test the ability of\neClass1 to respond quickly to new patterns by evolving the FRB\nwhile preserving the stability of the learning. eClass1 starts to\nevolve from an empty rule base. The evolution of the classifica-\ntion rate is illustrated in Fig. 5. It has been achieved by testing the\nevolved classifier at different stages of learning. One can see the\nsteady increase in the classification rate with the increase of the\nnumber of data samples met. The figure also demonstrates that\neClass1 is able to evolve five fuzzy rules automatically, reaching\nclassification rates well over 90% after less than 500 samples.\nWith a closer look [Fig. 5(b)], one can see that the curve is not\nideally smooth\u2014the drops in classification rate occur when a\nnew type of attack is met. This ability to rapidly evolve and\nadapt is critical to detecting new types of attacks that may be\nlaunched in a real-life scenario. Therefore, we assess eClass\nas a better classifier overall, combining the ability to work on\na per-sample basis, to quickly achieve high classification rates\nand to start \u201cfrom scratch,\u201d while at the same time achieving\nresults comparable to the well-known offline classifiers.\nV. CONCLUSION\nThis paper introduced a family of FRB classifiers with an\nevolving structure (fuzzy rules) called eClass that includes\neClass0 and eClass1 (eClass1 can have single or multiple out-\nputs). Both classifiers are based on eTS-type fuzzy systems.\neClass0 uses class labels as outputs. The proposed classifiers can\nstart learning \u201cfrom scratch.\u201d They can also be used to evolve\n(adapt and further develop the FRB structure) an initial classifier\niniClass, if such a classifier is available in an interactive scheme.\nStarting \u201cfrom scratch\u201d (which is equivalent to an empty\niniClass FRB) is a feature that is unique for eClass classifiers.\nThe architecture of eClass1 differs significantly\u2014it is based on\nregression over the features producing an estimate of the possi-\nbility that a data sample may belong to a certain class. It attains\nhigher classification rates, which compare favorably with re-\nsults from well-established offline and incremental classifiers.\nLearning for the eClass family is based on a computationally\nefficient, recursive procedure, and it is therefore applicable for\nonline real-time applications. The proposed approach addresses\nthe problem of classification of streaming data (video, speech,\nfinancial data, sensory inputs in robotic and advanced indus-\ntrial and Internet applications, etc.). It has been successfully\ntested on a number of benchmark problems simulating pseu-\ndodata streams to produce a comparison with the established\napproaches. It has also been successfully tested on an intrusion\ndetection data stream where it has demonstrated its advantages.\nThe results demonstrate that flexible classifiers can be generated\nonline from streaming data, achieving high classification rates.\nThe proposed method has the potential to be used in interactive\nmode or as a part of a system of collaborative classifiers.\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on June 22, 2009 at 08:51 from IEEE Xplore.  Restrictions apply.\nANGELOV AND ZHOU: EVOLVING FRB CLASSIFIERS FROM DATA STREAMS 1473\nAPPENDIX\nA. Recursive Expression of the Potential Using Cosine Distance\nCombining (8) and (9) we get\nPk (zk ) =\n1\n1 + 1k\u22121\n\u2211k\u22121\ni=1\n\uf8ee\n\uf8f01\u2212 \u2211nj = 1 z jk z ji\u221a\u2211n\nj = 1 (z\nj\nk )\n2 \u2211n\nj = 1 (z\nj\ni )\n2\n\uf8f9\n\uf8fb\n.\n(A1)\nReorganizing this, we get\nPk (zk ) =\n1\n2\u2212 1k\u22121 1\u221a\u2211n\nj = 1 (z\nj\nk )\n2\n\u2211k\u22121\ni=1\n\u2211n\ni = 1\nz j\nk\nz j\ni\u221a\u2211n\nj = 1 (z\nj\ni )\n2\n. (A2)\nLet us denote by Ak the following expression:\nBk =\nk\u22121\u2211\ni=1\n\u2211n\ni=1 z\nj\nk z\nj\ni\u221a\u2211n\ni=1\n(\nzji\n)2 . (A3)\nBk can be further simplified into\nBk =\nk\u22121\u2211\ni=1\n\u2211n\nj=1 z\nj\nk z\nj\ni\u221a\u2211n\nj=1\n(\nzji\n)2\n=\nk\u22121\u2211\ni=1\n1\u221a\u2211n\nj=1\n(\nzji\n)2\nn\u2211\nj=1\nzjk\n\u221a(\nzji\n)2 (A4)\nand finally into\nBk =\nn\u2211\nj=1\nzjk\nk\u22121\u2211\ni=1\n\u221a\u221a\u221a\u221a (zji )2\u2211n\nl=1\n(\nzli\n)2 . (A5)\nLet us further denote by bjk\nbjk =\nk\u22121\u2211\ni=1\n\u221a\u221a\u221a\u221a (zji )2\u2211n\nl=1\n(\nzli\n)2 . (A6)\nThen, we have\nBk =\nn\u2211\nj=1\nzjk b\nj\nk , b\nj\nk = b\nj\nk\u22121 +\n\u221a\u221a\u221a\u221a (zjk)2\u2211n\nl=1\n(\nzlk\n)2 ,\nbj1 =\n\u221a\u221a\u221a\u221a (zj1)2\u2211n\nl=1\n(\nzl1\n)2 . (A7)\nFrom (A2), (A3), and (A7), we finally get (11).\nUpdate of the Potential of the Focal Points (Prototypes): By\ndefinition (9), we have the global potential for each center as\nPk (z\u2217) =\n1\n1 +\n(\u2211k\ni=1 dcos(z\u2217, zi)\n)\/\n(k \u2212 1)\n(A8)\nor if calculated based on (k \u2212 1) data samples\nPk\u22121(z\u2217) =\nk \u2212 2\nk \u2212 2 +\u2211k\u22121i=1 dcos(zi, z\u2217) . (A8a)\nReorganizing (A8) and (A8a), we get\nPk (z\u2217)\n=\n1\n1 +\n(\u2211k\u22121\ni=1 dcos(zi, z\u2217) + dcos(zk , z\u2217)\n)\/\n(k \u2212 1)\n(A9)\n\u00d7\nk\u22121\u2211\ni\u22121\ndcos(zi, z\u2217) = (k \u2212 2)\n(\n1\nPk\u22121(z\u2217)\n\u2212 1\n)\n. (A10)\nFrom (A9) and (A10), we finally get (12a).\nIn a similar way, if we start from the definition of local po-\ntential (10) and apply the same logic, we get (A9a) and (A10a),\nwhich when combined, give (12):\nPk (z\u2217)\n=\n1\n1 +\n(\u2211Ql\nk\n\u22121\ni=1 dcos(zi, z\u2217) + dcos(zk , z\u2217)\n)\n\/(Qlk \u2212 1)\n(A9a)\n\u00d7\nQl\nk\n\u22121\u2211\ni\u22121\ndcos(zi, z\u2217)=\n(\nQlk\u22122\n)( 1\nPk\u22121(z\u2217)\n\u22121\n)\n. (A10a)\nB. Pseudocode of the eClass0 Algorithm\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on June 22, 2009 at 08:51 from IEEE Xplore.  Restrictions apply.\n1474 IEEE TRANSACTIONS ON FUZZY SYSTEMS, VOL. 16, NO. 6, DECEMBER 2008\nC. Pseudocode of the Algorithm eClass1\nACKNOWLEDGMENT\nThe authors would like to thank Dr. E. Lughofer, J. Ke-\npler University, Linz, Austria, and Dr. D. Filev, Ford Motor\nCompany, for their comments and useful discussions, and Mr.\nM. Everett for proofreading the manuscript.\nREFERENCES\n[1] H. Ishibuchi, K. Nozaki, N. Yamamoto, and H. Tanaka, \u201cSelecting fuzzy\nif\u2013then rules for classification problems using genetic algorithms,\u201d IEEE\nTrans. Fuzzy Syst., vol. 3, no. 3, pp. 260\u2013270, Aug. 1995.\n[2] O. Cordon, F. Gomide, F. Herrera, F. Hoffmann, and L. Magdalena, \u201cTen\nyears of genetic fuzzy systems: Current framework and new trends,\u201d Fuzzy\nSets Syst., vol. 141, no. 1, pp. 5\u201331, 2004.\n[3] L. Kuncheva, Fuzzy Classifiers. Heidelberg, Germany: Physica-Verlag,\n2000.\n[4] L. I. Kuncheva, \u201cHow good are fuzzy if\u2013then classifiers?\u201d IEEE Trans.\nSyst., Man, Cybern. B, Cybern., vol. 30, no. 4, pp. 501\u2013509, Aug.\n2000.\n[5] D. Nauck and R. Kruse, \u201cA neuro-fuzzy method to learn fuzzy classifica-\ntion rules from data,\u201d Fuzzy Sets Syst., vol. 89, pp. 277\u2013288, 1997.\n[6] J. C. Bezdek, J. Keller, R. Krishnapuram, and N. R. Pal, Fuzzy Models and\nAlgorithms for Pattern Recognition and Image Processing. Norwell,\nMA: Kluwer, 1999.\n[7] H. Ishibuchi, T. Nakashima, and M. Nii, Classification and Modeling\nWith Linguistic Granules: Advanced Information Processing. Berlin,\nGermany: Springer-Verlag, 2004.\n[8] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statisti-\ncal Learning: Data Mining, Inference and Prediction. Heidelberg,\nGermany: Springer-Verlag, 2001.\n[9] R. O. Duda, P. E. Hart, and D. G. Stork, Pattern Classification, 2nd ed.\nChichester, U.K.: Wiley\u2013Interscience, 2000.\n[10] K. M. A. Chai, H. T. Ng, and H. L. Chieu, \u201cBayesian online classifiers for\ntext classification and filtering,\u201d in Proc. SIGIR 2002, Tampere, Finland,\nAug. 11\u201315, pp. 97\u2013104.\n[11] R. Jin and G. Agrawal, \u201cEfficient decision tree construction on streaming\ndata,\u201d in Proc. ACM SIGKDD, 2003, pp. 571\u2013576.\n[12] L. Breiman, J. Friedman, C. Stone, and R. Olshen, Classification and\nRegression Trees. Boca Raton, FL: Chapman & Hall, 1993.\n[13] C. M. Bishop, Neural Networks for Pattern Recognition. Oxford, U.K.:\nOxford Univ. Press, 1995.\n[14] V. N. Vapnik, The Statistical Learning Theory. New York: Springer-\nVerlag, 1998.\n[15] L.-X. Wang, \u201cFuzzy systems are universal approximators,\u201d in Proc. 1st\nIEEE Conf. Fuzzy Syst. (FUZZ-IEEE), San Diego, CA, Mar. 8\u201312, 1992,\npp. 1163\u20131170.\n[16] F. Hopner and F. Klawonn, \u201cObtaining interpretable fuzzy models\nfrom fuzzy clustering and fuzzy regression,\u201d in Proc. 4th Int. Conf.\nKnowl.-Based Intell. Eng. Syst. (KES), Brighton, U.K., 2000, pp. 162\u2013\n165.\n[17] J. G. Marin-Blazquez and Q. Shen, \u201cFrom approximative to descriptive\nfuzzy classifiers,\u201d IEEE Trans. Fuzzy Syst., vol. 10, no. 4, pp. 484\u2013497,\nAug. 2002.\n[18] J. Casillas, O. Cordon, and F. Herrera, \u201cCOR: A methodology to improve\nad hoc data-driven linguistic rule learning methods by inducing coopera-\ntion among rules,\u201d IEEE Trans. Syst., Man, Cybern. B, Cybern., vol. 32,\nno. 4, pp. 526\u2013537, Aug. 2002.\n[19] T. Takagi and M. Sugeno, \u201cFuzzy identification of systems and its ap-\nplication to modeling and control,\u201d IEEE Trans. Syst., Man, Cybern.,\nvol. SMC-15, no. 1, pp. 116\u2013132, Jan. 1985.\n[20] P. Domingos and G. Hulten, \u201cCatching up with the data: Research issues in\nmining data streams,\u201d presented at the Workshop Res. Issues Data Mining\nKnowl. Discovery, Santa Barbara, CA, 2001.\n[21] U. Fayyad, G. Piatetsky-Shapiro, and P. Smyth. From Data Mining to\nKnowledge Discovery: An Overview, Advances in Knowledge Discovery\nand Data Mining. Cambridge, MA: MIT Press, 1996.\n[22] J. H. Holland, Adaptation in Natural and Artificial Systems. Ann Arbor,\nMI: Univ. Michigan Press, 1975.\n[23] D. E. Goldberg, Genetic Algorithms in Search, Optimization and Machine\nLearning. Reading, MA: Addison-Wesley, 1989.\n[24] A. S. Hornby, Oxford Advance Learner\u2019s Dictionary. London, U.K.:\nOxford Univ. Press, 1974.\n[25] J. Gomez, F. Gonzalez, D. Dasgupta, and O. Nasaroui, \u201cComplete expres-\nsion tree for evolving fuzzy classifier systems with genetic algorithms,\u201d\nin Proc. North Amer. Fuzzy Inf. Process. Soc. Conf. Fuzzy Logic Internet\n(NAFIPS-FLINT), 2002, pp. 469\u2013474.\n[26] P. Angelov, Evolving Rule-Based Models: A Tool for Design of Flexible\nAdaptive Systems. Berlin, Germany: Springer-Verlag, 2002.\n[27] P. Angelov and D. Filev, \u201cAn approach to on-line identification of evolving\nTakagi\u2013Sugeno models,\u201d IEEE Trans. Syst., Man, Cybern. B, Cybern.,\nvol. 34, no. 1, pp. 484\u2013498, Feb. 2004.\n[28] P. Angelov and R. Buswell, \u201cIdentification of evolving rule-based models,\u201d\nIEEE Trans. Fuzzy Syst., vol. 10, no. 5, pp. 667\u2013677, Oct. 2002.\n[29] P. Angelov, V. Giglio, C. Guardiola, E. Lughofer, and J. M. Lujan, \u201cAn ap-\nproach to model-based fault detection in industrial measurement systems\nwith application to engine test benches,\u201d Meas. Sci. Technol., vol. 17,\nno. 7, pp. 1809\u20131818, 2006.\n[30] J. Macias, P. Angelov, and X. Zhou, \u201cPredicting quality of the crude oil\ndistillation using evolving Takagi\u2013Sugeno fuzzy models,\u201d in Proc. 2006\nInt. Symp. Evol. Fuzzy Syst., pp. 201\u2013207.\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on June 22, 2009 at 08:51 from IEEE Xplore.  Restrictions apply.\nANGELOV AND ZHOU: EVOLVING FRB CLASSIFIERS FROM DATA STREAMS 1475\n[31] N. Kasabov and Q. Song, \u201cDENFIS: Dynamic evolving neural-fuzzy in-\nference system and its application for time-series prediction,\u201d IEEE Trans.\nFuzzy Syst., vol. 10, no. 2, pp. 144\u2013154, Apr. 2002.\n[32] F. Ferrer-Troyano, J. S. Aguilar-Ruiz, and J. C. Riquelme, \u201cIncremental\nrule learning based on example nearness from numerical data,\u201d in Proc.\n2005 ACM Symp. Appl. Comput., 2008, pp. 568\u2013572.\n[33] S. Pang, S. Ozawa, and N. Kasabov, \u201cIncremental linear discriminant anal-\nysis for classification of data streams,\u201d IEEE Trans. Syst., Man Cybern.\nB, Cybern., vol. 35, no. 5, pp. 905\u2013914, Oct. 2005.\n[34] G. A. Carpenter, S. Grossberg, N. Markuzon, J. H. Reynolds, and D. B.\nRosen, \u201cFuzzy ARTMAP: A neural network architecture for incremen-\ntal supervised learning of analog multidimensional maps,\u201d IEEE Trans.\nNeural Netw., vol. 3, no. 5, pp. 698\u2013712, Sep. 1992.\n[35] H. Alhammady and K. Ramamohanarao, \u201cMining emerging patterns and\nclassification in data streams,\u201d in Proc. 2005 IEEE\/WIC\/ACM Int. Conf.\nWeb Intell. (WI 2005), Sep., pp. 272\u2013275.\n[36] L. Golab and M. T. Ozsu, \u201cIssues in data stream management,\u201d in Proc.\nACM SIGMOD Conf., Jun. 2003, vol. 32, no. 2, pp. 5\u201314.\n[37] B. Pfahringer, \u201cWining the KDD99 Classification Cup: Bagged boosting,\u201d\nin Proc. ACM SIGKDD, Jan. 2000, vol. 1, no. 2, pp. 65\u201366.\n[38] G. Widmer and M. Kubat, \u201cLearning in the presence of concept drift\nand hidden contexts,\u201d Mach. Learning, vol. 23, no. 1, pp. 69\u2013101,\n1996.\n[39] R. Klinkenberg and T. Joachims, \u201cDetection concept drift with support\nvector machines,\u201d in Proc. 7th Int. Conf. Mach. Learning (ICML), 2000,\npp. 487\u2013494.\n[40] C. Elkan. (2007, Jan. 27). Results of the KDD\u201999 Knowledge\nDiscovery Contest [Online]. Available: http:\/\/www-cse.ucsd.edu\/users\/\nelkan\/clresults.html\n[41] J. Roubos, M. Setnes, and J. Abonyi, \u201cLearning fuzzy classification rules\nfrom data,\u201d Inf. Sci., vol. 150, pp. 77\u201393, 2003.\n[42] F. Klawonn and P. E. Klement, \u201cMathematical analysis of fuzzy classi-\nfiers,\u201d Lect. Notes Comput. Sci., vol. 1280, pp. 359\u2013370, 1997.\n[43] P. Angelov and D. Filev, \u201cSimpl_eTS: A simplified method for learning\nevolving Takagi\u2013Sugeno fuzzy models,\u201d in Proc. 2005 IEEE Int. Conf.\nFuzzy Syst. (FUZZ-IEEE), Reno, NV, May 22\u201325, pp. 1068\u20131073.\n[44] P. Angelov, \u201cAn approach for fuzzy rule-base adaptation using on-line\nclustering,\u201d Int. J. Approx. Reason., vol. 35, no. 3, pp. 275\u2013289, Mar.\n2004.\n[45] K. J. Astroem and B. Wittenmark, Adaptive Control. Reading, MA:\nAddison-Wesley, 1989.\n[46] R. R. Yager and D. P. Filev, \u201cLearning of fuzzy rules by mountain cluster-\ning,\u201d in Proc. SPIE Conf. Appl. Fuzzy Logic Technol., Boston, MA, 1993,\npp. 246\u2013254.\n[47] S. L. Chiu, \u201cFuzzy model identification based on cluster estimation,\u201d J.\nIntell. Fuzzy Syst., vol. 2, pp. 267\u2013278, 1994.\n[48] P. Angelov and X. Zhou, \u201cEvolving fuzzy systems from data streams in\nreal-time,\u201d in Proc. 2006 Int. Symp. Evol. Fuzzy Syst., Sep. 7\u20139, pp. 29\u2013\n35.\n[49] P. Angelov, C. Xydeas, and D. Filev, \u201cOn-line identification of MIMO\nevolving Takagi\u2013Sugeno fuzzy models,\u201d in Proc. Int. Joint Conf. Neural\nNetw. Int. Conf. Fuzzy Syst. (IJCNN-FUZZ-IEEE), Budapest, Hungary,\nJul. 25\u201329, 2004, pp. 55\u201360.\n[50] G. Klir and T. Folger, Fuzzy Sets, Uncertainty and Information. Engle-\nwood Cliffs, NJ: Prentice-Hall, 1988.\n[51] A. Asuncion and D. J. Newman. (2007). UCI Machine Learn-\ning Repository [Online]. School of Information and Computer Sci-\nence, University of California, Irvine. Available: http:\/\/www.ics.uci.edu\/\n\u223cmlearn\/MLRepository.html\n[52] J. R. Quinlan, \u201cImproved use of continuous attributes in C4.5,\u201d J. AI Res.,\nvol. 4, pp. 77\u201390, 1996.\n[53] X. Zhou and P. Angelov, \u201cAn approach to autonomous self-localization of\na mobile robot in completely unknown environment using evolving fuzzy\nrule-based classifier,\u201d in Proc. 2007 IEEE Int. Symp. Comput. Intell. Appl.\nDefense Security Symp. Series Comput. Intell. (SSCI 2007), Honolulu, HI,\nApr. 1\u20135, pp. 131\u2013138 (ISBN 1-4244-0698-6).\n[54] V. Mikheev, A. Vopilov, and I. Shabalin, \u201cThe MP13 approach to\nthe KDD\u201999 classifier learning contest,\u201d ACM SIGKDD Explorations\nNewslett., vol. 1, no. 2, pp. 76\u201377, 1999.\n[55] A. D. Joshi. (2004). Applying the wrapper approach for auto discovery\nof under-sampling and over-sampling percentages on skewed datasets,\nM.Sc. Thesis, Univ. South Florida, Tampa [Online]. pp. 1\u201377. Available:\nhttp:\/\/etd.fcla.edu\/SF\/SFE0000491\/Thesis-AjayJoshi.pdf\n[56] L. Zadeh, \u201cFuzzy sets as the basis for a theory of possibility,\u201d Fuzzy Sets\nSyst., vol. 1, pp. 3\u201328, 1978.\nPlamen P. Angelov (M\u201999\u2013SM\u201904) received the\nM.Eng. degree in electronics and automation from\nSofia Technical University, Sofia, Bulgaria, in 1989\nand the Ph.D. degree in optimal control from\nBulgaria Academy of Sciences, Sofia, Bulgaria, in\n1993.\nHe spent over ten years as a Research Fellow\nworking on computational intelligence and control.\nDuring 1995\u20131996, he was at Hans-Knoell Institute,\nJena, Germany. In 1997, he was a Visiting Researcher\nat the Catholic University, Leuvain-la-neuve, Bel-\ngium. In 2007, he was a Visiting Professor at the University of Wolfenbuettel-\nBraunschweig, Germany. He is currently a Senior Lecturer (Associate Professor)\nat Lancaster University, Lancaster, U.K. He has authored or coauthored over\n100 peer-reviewed publications, including the book Evolving Rule Based Mod-\nels: A Tool for Design of Flexible Adaptive Systems (Springer-Verlag, 2002),\nand over 30 journal papers, and is a holder of a patent (2006). He is a Member\nof the Editorial Boards of three international scientific journals. He also holds\na portfolio of research projects including a participation in a \u00a332M program\nASTRAEA. He has received funding from the industry, European Union (EU),\nU.S., U.K. Research Council, etc. His current research interests include adap-\ntive and evolving (self-organizing) fuzzy systems as a framework of an evolving\ncomputational intelligence. He is also engaged in online and real-time identi-\nfication and design of such systems with evolving (self-developing) structure,\nintelligent data processing, particularly in evolving fuzzy-rule-based models,\nself-organizing and autonomous systems, and intelligent (inferential) sensors.\nDr. Angelov is a member of the Technical Committee (TC) on Fuzzy Sys-\ntems, Vice Chair of the TC on Standards, Computational Intelligence Society,\nChair of the Task Force on Adaptive Fuzzy Systems, and member of the Au-\ntonomous Systems Working Group of the North-West Science Council of U.K.\nHe serves regularly on the technical\/program committees of leading interna-\ntional conferences on different aspects of computational intelligence.\nXiaowei Zhou (S\u201906) received the Bachelor\u2019s de-\ngree in computing science from Nanjing University\nof Science and Technology, Nanjing, China, in 2003,\nand the Master\u2019s degree (with distinction) in IT and\ndata communications in 2004 from the Department\nof Communication Systems, Lancaster University,\nLancaster, U.K., where he is currently working to-\nward the Ph.D. degree at Infolab 21.\nHe has authored or coauthored over a dozen of\npeer-reviewed papers on evolving intelligent systems.\nHis current research interests include evolving intel-\nligent systems, robotics, image processing, and their applications under the\nframework of computational intelligence and machine learning.\nMr. Zhou was a recipient of the IEEE Student Travel Grant in 2006 to at-\ntend the World Congress on Computational Intelligence and a number of other\nsmaller travel grants. He is the main author of the fuzzy car algorithm, which\nwas ranked third in the competition during the IEEE International Conference\non Fuzzy Systems (FUZZ-IEEE 2007), London, U.K., July.\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on June 22, 2009 at 08:51 from IEEE Xplore.  Restrictions apply.\n"}