{"doi":"10.1109\/TNN.2010.2098481","coreId":"67955","oai":"oai:eprints.lancs.ac.uk:34354","identifiers":["oai:eprints.lancs.ac.uk:34354","10.1109\/TNN.2010.2098481"],"title":"An uniformly stable backpropagation algorithm to train a feedforward neural network","authors":["Rubio, Jos\u00e9 de Jes\u00fas","Angelov, Plamen","Garc\u00eda, Enrique"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2011-03","abstract":"The neural networks are applied to many online elds, but the stability of the neural networks is not always assured and it could damage the instruments causing accidents. There is some research related with the stability of the continuous time neural networks, but there are some systems that are better described in discrete time, for example the population systems, the annual expenses in an industry, the interest earned by the loan of a bank, or the prediction of the distribution of loads received each hour in a warehouse, that is way it is important to consider the stability of the discrete time neural networks. The major contributions of this paper are as follows: 1) a theorem to assure the uniform stability of the general discrete time systems is proposed, 2) it is proven that the backpropagation algorithm with a new time varying rate is uniformly stable for online identication, the identication error converges to a small zone bounded by a uncertainty, 3) it is proven that the weights error is bounded by the initial weights error, i.e. the overtting is not presented in the proposed algorithm, 4) the backpropagation is applied to predict the distribution of loads that a transelevator receive from a trailer and place in the deposits each hour in a warehouse, the deposits in the warehouse can be reserved in advance using the prediction results, 5) the backpropagation algorithm is compared with the recursive least square algorithm and with the Sugeno fuzzy inference system in the problem of the prediction of the distribution of loads in a warehouse, giving that the rst and the second are stable and the third is unstable, and 6) the backpropagation algorithm is compared with the recursive least square algorithm and with the Kalman lter algorithm in an academic example. (c) IEEE Pres","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/67955.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/34354\/1\/10IEEETNN_(2).pdf","pdfHashValue":"2845b4f62c6d2ae9cc821ba0faf6cc2475de7a08","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:34354<\/identifier><datestamp>\n      2018-01-24T03:03:18Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        An uniformly stable backpropagation algorithm to train a feedforward neural network<\/dc:title><dc:creator>\n        Rubio, Jos\u00e9 de Jes\u00fas<\/dc:creator><dc:creator>\n        Angelov, Plamen<\/dc:creator><dc:creator>\n        Garc\u00eda, Enrique<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        The neural networks are applied to many online elds, but the stability of the neural networks is not always assured and it could damage the instruments causing accidents. There is some research related with the stability of the continuous time neural networks, but there are some systems that are better described in discrete time, for example the population systems, the annual expenses in an industry, the interest earned by the loan of a bank, or the prediction of the distribution of loads received each hour in a warehouse, that is way it is important to consider the stability of the discrete time neural networks. The major contributions of this paper are as follows: 1) a theorem to assure the uniform stability of the general discrete time systems is proposed, 2) it is proven that the backpropagation algorithm with a new time varying rate is uniformly stable for online identication, the identication error converges to a small zone bounded by a uncertainty, 3) it is proven that the weights error is bounded by the initial weights error, i.e. the overtting is not presented in the proposed algorithm, 4) the backpropagation is applied to predict the distribution of loads that a transelevator receive from a trailer and place in the deposits each hour in a warehouse, the deposits in the warehouse can be reserved in advance using the prediction results, 5) the backpropagation algorithm is compared with the recursive least square algorithm and with the Sugeno fuzzy inference system in the problem of the prediction of the distribution of loads in a warehouse, giving that the rst and the second are stable and the third is unstable, and 6) the backpropagation algorithm is compared with the recursive least square algorithm and with the Kalman lter algorithm in an academic example. (c) IEEE Press<\/dc:description><dc:date>\n        2011-03<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/34354\/1\/10IEEETNN_(2).pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/TNN.2010.2098481<\/dc:relation><dc:identifier>\n        Rubio, Jos\u00e9 de Jes\u00fas and Angelov, Plamen and Garc\u00eda, Enrique (2011) An uniformly stable backpropagation algorithm to train a feedforward neural network. IEEE Transactions on Neural Networks, 22 (3). pp. 356-366. ISSN 1045-9227<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/34354\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":null,"relations":["http:\/\/dx.doi.org\/10.1109\/TNN.2010.2098481","http:\/\/eprints.lancs.ac.uk\/34354\/"],"year":2011,"topics":["QA75 Electronic computers. Computer science"],"subject":["Journal Article","PeerReviewed"],"fullText":"JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, XX 1\nAn uniformly stable backpropagation algorithm to\ntrain a feedforward neural network\nJos\u00e9 de Jes\u00fas Rubio, Angelov Plamen, Enrique Garc\u00eda\nAbstract\u0097The neural networks are applied to many online\n\u0002elds, but the stability of the neural networks is not always\nassured and it could damage the instruments causing accidents.\nThere is some research related with the stability of the continuous\ntime neural networks, but there are some systems that are better\ndescribed in discrete time, for example the population systems,\nthe annual expenses in an industry, the interest earned by the\nloan of a bank, or the prediction of the distribution of loads\nreceived each hour in a warehouse, that is way it is important to\nconsider the stability of the discrete time neural networks. The\nmajor contributions of this paper are as follows: 1) a theorem to\nassure the uniform stability of the general discrete time systems\nis proposed, 2) it is proven that the backpropagation algorithm\nwith a new time varying rate is uniformly stable for online\nidenti\u0002cation, the identi\u0002cation error converges to a small zone\nbounded by a uncertainty, 3) it is proven that the weights error\nis bounded by the initial weights error, i.e. the over\u0002tting is not\npresented in the proposed algorithm, 4) the backpropagation is\napplied to predict the distribution of loads that a transelevator\nreceive from a trailer and place in the deposits each hour in\na warehouse, the deposits in the warehouse can be reserved\nin advance using the prediction results, 5) the backpropagation\nalgorithm is compared with the recursive least square algorithm\nand with the Sugeno fuzzy inference system in the problem of the\nprediction of the distribution of loads in a warehouse, giving that\nthe \u0002rst and the second are stable and the third is unstable, and\n6) the backpropagation algorithm is compared with the recursive\nleast square algorithm and with the Kalman \u0002lter algorithm in\nan academic example.\nNeural networks, stability, prediction, identi\u0002cation, ware-\nhouse.\nI. INTRODUCTION\nThe online neural networks can be used in many \u0002elds, in-\ncluding nonlinear adaptive control, fault detection, diagnostics,\nperformance analysis of dynamic systems, pattern and image\nrecognition, time-series, identi\u0002cation of nonlinear systems,\nintelligent agents, modeling, robotic, and mechatronic systems.\nThe stability problem of the neural networks is important for\nthe aforementioned online \u0002elds and the stability of the neural\nnetworks is not always assured.\nThere are some researchers who have worked with the\nstability of continuous time neural networks as are [19], [25],\n[28], [31], [32], [33], [36], [40], [41].\nIn [19], they study the approximation and the learning\nproperties of one class of recurrent networks, known as high-\nRubio and Garcia are with the Instituto Polit\u00e9cnico Nacional - ESIME\nAzcapotzalco, Secci\u00f3n de Estudios de Posgrado e Investigaci\u00f3n, Av. de las\nGranjas no. 682, Col. Santa Catarina, Del. Azcapotzalco, M\u00e9xico D.F., 02550,\nM\u00e9xico, (phone:(+52)55-57296000. Ext.64497; email: jrubioa@ipn.mx)\nAngelov is with the Intelligent Systems Research Laboratory, Department\nof Communication Systems, InfoLab21, South Drive, Lancaster University,\nLancaster, LA1 4WA, UK;\norder neural networks, and they apply these architectures to\nthe identi\u0002cation of dynamic systems. In [25], the stability\nconditions of online identi\u0002cation are derived by Lyapunov\u0096\nKrasovskii approach, which are described by linear matrix\ninequality. In [28], they present the suf\u0002cient conditions for\nthe global asymptotic stability for a kind of recurrent neural\nnetwork. In [31], they consider the robust stability of neural\nnetworks with multiple delays. The paper of [32] is concerned\nwith the global robust exponential stability of a class of inter-\nval Cohen\u0096Grossberg neural networks with both multiple time-\nvarying delays and continuously distributed delays. In [33], the\nstatic neural network model and a local \u0002eld neural network\nmodel are theoretically compared in terms of their trajectory\ntransformation property, equilibrium correspondence property,\nnontrivial attractive manifold property, global convergence as\nwell as stability in many different senses. In [36], dynamic\nmultilayer neural networks are used for nonlinear system on-\nline identi\u0002cation and the passivity approach is applied to\naccess several stability properties of the neuro-identi\u0002er. In\n[40], the passivity-based approach is used to derive stability\nconditions for dynamic neural networks with different time\nscales. In [41], the Lyapunov function approach is used to\nrigorously analyze the convergence of weights, with the use\nof the backpropagation algorithm, toward minima of the error\nfunction. All the works are interesting, but all consider the\ncontinuous time neural networks and there are some systems\nthat are better described in discrete time, for example the\npopulation systems of some kind of animals [27], or the annual\nexpenses in an industry [5], or the interest earned by the loan\nof a bank [5], or the prediction of the distribution of loads\nreceived each hour in a warehouse, that is way it is important\nto consider the stability of the discrete time neural networks.\nThere are some researchers who have worked with the\nstability of discrete time neural networks as are [24], [29],\n[35], [37], [38], [39].\nIn [24], a double dead-zone is used to assure the stability\nof the identi\u0002cation error in the gradient descent algorithm.\nIn [29], they derive a condition for robust local stability of\nthe multilayer recurrent neural networks. In [35], an input\nto state stability approach is used to create robust training\nalgorithms for discrete time neural networks. The paper of\n[37] suggests new learning laws for Mamdani and Takagi\u0096\nSugeno\u0096Kang type fuzzy neural networks based on input-to-\nstate stability approach. In [38], the input-to-state stability\napproach is applied to access robust training algorithms of\ndiscrete-time recurrent neural networks. In [39], they modify\nthe backpropagation approach and they employ a time varying\nrate that is determined from the input output data and the\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, XX 2\nmodel structure and stable learning algorithms for the premise\nand the consequence parts of the fuzzy rules are proposed.\nAll the works propose new neural network algorithms as\n[29] or they modify the general backpropagation employing\na time varying rate to prove the input-to-state stability as\n[24], [35], [37], [38], [39], in this paper it is proven that the\nbackpropagation algorithm with a new time varying rate is\nuniformly stable.\nOn the other hand, there is some research related with the\nwarehouses as is: [6], [8], [2], [22], [34].\nThe authors in [6] propose a method for selecting and\nmaterializing views, which selects and horizontally fragments\na view, recomputes the size of the stored partitioned view\nwhile deciding further views to select. In [8], they consider a\nmatrix-based discrete event control approach for a warehouse,\nthe control system is organized in two modules: a dynamic\nmodel and a controller. In [2], they focus on the technical\nchallenges of designing and implementing an effective data\nwarehouse for health care information. In [22], they propose,\nas an extension to the data warehouse model, a knowledge\nwarehouse architecture that will not only facilitate the captur-\ning and coding of knowledge but also enhance the retrieval\nand sharing of knowledge across the organization. In [34]\nthey propose a new constrained evolutionary algorithm for\nthe maintenance-cost view-selection problem. All the works\nare interesting, but none uses the neural networks for the\nprediction of the distribution of loads in a warehouse, in [22],\nthey only mention that it could be made.\nIn this paper, it is proposed a theorem to assure the uniform\nstability of the discrete time systems, it is proven that the\nbackpropagation algorithm with a new time varying rate is\nuniformly stable for online identi\u0002cation, the identi\u0002cation\nerror converges to a small zone bounded by the uncertainty,\nand the weights error is bounded by the initial weights error;\nthe backpropagation is applied to predict the distribution of\nloads that a transelevator receive from a trailer and place in\nthe deposits each hour in a warehouse, the deposits in the\nwarehouse can be reserved in advance using the prediction\nresults, the backpropagation algorithm is compared with the\nrecursive least square algorithm and with the Sugeno fuzzy\ninference system in the problem of the prediction of the dis-\ntribution of loads inside a warehouse, and the backpropagation\nalgorithm is compared with the recursive least square and with\nthe Kalman \u0002lter in an academic example.\nThis paper is organized as follows. In section II, the theorem\nthat proves the uniformly stability of the discrete time systems\nis presented. In section III, the general backpropagation to train\na feedforward neural network with a hidden layer is presented.\nIn section IV, the uniformly stability of the backpropagation\nalgorithm is proven. In section V, the application of the pro-\nposed algorithm is described. In section VI, a brief description\nof the warehouse is presented. In section VII, the backpropa-\ngation algorithm is compared with the recursive least square\nalgorithm, with the Sugeno fuzzy inference system, and with\nthe Kalman \u0002lter algorithm in the problem of the prediction\nof the distribution of loads in a warehouse and in an academic\nexample. Finally, in section VIII, the results and the possible\nfuture research are explained.\nII. PRELIMINARIES\nLet us consider the following discrete-time nonlinear sys-\ntem:\nxk+1 = f [xk; uk] (1)\nWhere uk 2 <m is the input vector, xk 2 <n is the state\nvector, uk and xk are known. f is an unknown nonlinear\nsmooth function f 2 C1.\nDe\u0002nition 1: The system(1) is said to be uniformly stable\nif 8\u000f > 0 , 9 \u000e = \u000e(\u000f) such that:\nkxk1k < \u000e ) kxkk < \u000f, 8k > k1 (2)\nIf the system has \u000e = \u000e(\u000f; k); then the system (1) only is\nstable.\nNow, a basic stability theorem for discrete-time nonlinear\nsystems is given, it is an analogous version of the continuous-\ntime version given by [3] and of the delayed-continuous-time\nversion given by [24].\nTheorem 1: Let Lk(x(k)) be a Lyapunov function of the\ndiscrete-time nonlinear system (1), if it satis\u0002es:\n\r1 (kxkk) \u0014 Lk(xk) \u0014 \r2 (kxkk)\n\u0001Lk(xk) \u0014 \u0000\r3 (kxkk) + \r3 (\u000e) (3)\nWhere \u000e is a positive constant, \r1 (\u0001) and \r2 (\u0001) are K1\nfunctions, and \r3 (\u0001) is a K function, then the system (1) is\nuniformly stable.\nProof: First, let us de\u0002ne \ri : [0;1) ! [0;1) ;\nkx(k)k ! y = \ri (kxkk) ; i = 1; 2: So 8y 2 [0;1),\n9 kx(k)k 2 [0;1) such that y = \ri (kxkk) ; 9\r\u00001i such\nthat \r\u00001i (\ri (kxkk)) = kxkk. Two cases are discussed: 1) if\nkxkk \u0015 \u000e, then using the second equation of (3) \u0001Lk(xk) \u0014 0\nand the system is uniformly stable, 2) if kxkk < \u000e, the de\u0002n-\nition of uniform stability (2) is used to prove that the system\nis uniformly stable. Let us de\u0002ne \u000e as \u000e =\n\u0000\n\r\u000012 \u000e \r1\n\u0001\n(\u000f) =\n\r\u000012 [\r1(\u000f)] : For contradiction, let us suppose that 9 k3 > k1\nsuch that:\nkxk3k > \u000f (4)\nThen, 9 k2\u000f [k1; k3) such that:\nkxk2k = \u000e (5)\nAnd kxkk \u0015 \u000e, 8k 2 [k2; k3], then \r3 (kxkk) \u0015 \r3 (\u000e), 8k 2\n[k2; k3], it gives:\n\u0000\r3 (kxkk) \u0014 \u0000\r3 (\u000e) , 8k 2 [k2; k3] (6)\nBecause \r3 is non-decreasing, using k3 = k in the \u0002rst\ninequality of (3) \r1 (kxk3k) \u0014 Lk(xk3) = Lk(xk2) +\n\u0001k3k2Lk(xk). Using the \u0002rst and the second inequality of (3),\ngives \r1 (kxk3k) \u0014 \r2 (kxk2k) + [\u0000\r3 (kxkk) + \r3 (\u000e)]jk3k2 .\nUsing (5) and (6) gives \r1 (kxk3k) \u0014 \r2 (\u000e) ; or kxk3k \u0014\u0000\n\r\u000011 \u000e \r2\n\u0001\n(\u000e): From the de\u0002nition of \u000e gives:\nkxk3k \u0014 \u000f (7)\nWhere (7) contradicts (4), thus (2) is satis\u0002ed and the system\nis uniformly stable. If \u000e > \u000f, then 9 k3 > k1 such that (4) is\nsatis\u0002ed, and from this inequality it gives a contradiction, that\nis:\n\u000e \u0014 \u000f, 8k > k1 (8)\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, XX 3\nFig. 1. Architecture of the neural network\nFinally, from (2) and the de\u0002nition of 0 \u0014 kxkk < \u000f gives:\n\u000f > 0 (9)\n(8) and (9) proves that \u000f is well de\u0002ned.\nIII. THE BACKPROPAGATION ALGORITHM TO TRAIN A\nNEURAL NETWORK\nLet us consider the following unknown discrete-time non-\nlinear system:\ny(k) = f [Xk] (10)\nWhere Xk = [x1(k) : : : ; xi(k); : : : ; xN (k)]T =\n[y(k \u0000 1); : : : ; y(k \u0000 n); u (k \u0000 1) ; : : : ; u (k \u0000m)]T 2 <N\u00021\n(N = n +m) is the input vector, u(k \u0000 1) 2 < is the input\nof the plant, y(k) 2 < is the output of the plant, and f is\nan unknown nonlinear function, f 2 C1. The output of the\nneural network with one hidden layer can be expressed as\n[10], [11], [14]:\nby(k) = Vk\bk = MX\nj=1\nVjk\u001ejk\n\bk = [\u001e1k; : : : ; \u001ejk; : : : ; \u001eMk]\nT\n\u001ejk = tanh(\nNX\ni=1\nWijkxi(k))\n(11)\nWhere i = 1; : : : ; N , j = 1; : : : ;M , Xk 2 <N\u00021 is the input\nvector given by (10), by(k) 2 < is the output of the neural\nnetwork, Vk 2 <1\u0002M andWk 2 <M\u0002N are the weights of the\noutput and the hidden layer of the neural network, respectively,\nWijk 2 <, xi(k) 2 <, \bk 2 <M\u00021, \u001ejk 2 <, Vjk 2 <, the\nFigure 1 shows the feedforward neural network.\nIV. STABILITY OF THE BACKPROPAGATION ALGORITHM\nThe stability of the parameter learning is needed, because\nthis algorithm works online. First, the model is linearized, and\nlater, the stability of the proposed algorithm is analyzed.\nAccording to the Stone Weierstrass theorem [4], the un-\nknown nonlinear function f of (10) is approximated as:\ny(k) = V\u0003\b\u0003k+ 2f=\nMX\nj=1\nVj\u0003\u001e\u0003jk+ 2f\n\b\u0003k = [\u001e\u00031k; : : : ; \u001e\u0003jk; : : : ; \u001e\u0003Mk]\nT\n\u001e\u0003jk = tanh(\nNX\ni=1\nWij\u0003xi(k))\n(12)\nWhere \b\u0003k 2 <M\u00021, 2f= y(k)\u0000V\u0003\b\u0003k 2 < is the modeling\nerror, \u001e\u0003jk 2 <, Vj\u0003 2 <, Wij\u0003 2 <, Vj\u0003 2 < and Wij\u0003 2 <\nare the optimal parameters that can minimize the modeling\nerror 2f [19].\nFirst, the network model is linearized, it is used to de\u0002ne the\nparameters-updating and to prove the stability of the proposed\nalgorithm.\nIn the case of two independent variables, a function has a\nTaylor series as follows:\nf(!1; !2) = f(!10 ; !20) + (!1 \u0000 !10) @f(!1;!2)@!1\n+(!2 \u0000 !20) @f(!1;!2)@!2 +Rf\n(13)\nWhere Rf 2 < is the remainder of the Taylor series. If we\nlet !1, and !2 correspond to Wijk 2 <, and Vjk 2 <, !10\nand !20 correspond to Wij\u0003 2 < and Vj\u0003 2 <, let us de\u0002nefWijk =Wijk\u0000Wij\u0003 2 < and eVjk = Vjk\u0000Vj\u0003 2 <, then the\nTaylor series is applied to linearize (11) as follows [24], [25],\n[26]:\nVk\bk = V\u0003\b\u0003k +\nMX\nj=1\nNX\ni=1\nfWijk @Vk\bk\n@Wijk\n+\nMX\nj=1\neVjk @Vk\bk\n@Vjk\n+Rf\n(14)\nWhere @Vk\bk@Wijk 2 < and\n@Vk\bk\n@Vjk\n2 <, please note that Vk\bk =\nMX\nj=1\nVjk\u001ejk and V\u0003\b\u0003k =\nMX\nj=1\nVj\u0003\u001e\u0003jk. As all the parameters\nare scalars, the Taylor series is well applied. Considering (11)\nand using the chain rule [15], [24], [25], [26], [30] gives:\n@Vk\bk\n@Wjk\n= Vjk\n@\bk\n@Wjk\n= Vjk\n@ tanh(\nNX\ni=1\nWijkxi(k))\n@Wijk\n= Vjksech2(\nNX\ni=1\nWijkxi(k))xi(k) = \u001bijk\n(15)\nWhere \u001bijk = Vjksech2(\nNX\ni=1\nWijkxi(k))xi(k) 2 < because\nVjk 2 <, sech2(\nNX\ni=1\nWijkxi(k)) 2 < and xi(k) 2 <.\n@Vk\bk\n@Vjk\n=\n@\nMX\nj=1\nVjk\u001ejk\n@Vjk\n= \u001ejk (16)\nWhere \u001ejk = tanh(\nNX\ni=1\nWijkxi(k)) 2 <. Substituting @Vk\bk@Wijk\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, XX 4\nof (15), and @Vk\bk@Vjk of (16) into (14) gives:\nVk\bk = V\u0003\b\u0003k +\nMX\nj=1\nNX\ni=1\nfWijk\u001bijk\n+\nMX\nj=1\neVjk\u001ejk +Rf (17)\nLet us de\u0002ne the identi\u0002cation error e(k) 2 < as follows:\ne(k) = by(k)\u0000 y(k) (18)\nWhere y(k) and by(k) are de\u0002ned in (10) and (11), respectively.\nSubstituting (11), (12), and (18) into (17) gives:\ne (k) =\nMX\nj=1\neVjk\u001ejk + MX\nj=1\nNX\ni=1\nfWijk\u001bijk + \u0016(k) (19)\nWhere \u0016(k) = Rf\u0000 2f .\nFrom (19), it is obtained that:\nMX\nj=1\neVjk\u001ejk + MX\nj=1\nNX\ni=1\nfWijk\u001bijk = e (k)\u0000 \u0016(k) (20)\nThe proposed backpropagation algorithm uses a new time\nvarying rate as follows:\nVjk+1 = Vjk \u0000 \u000bk\u001ejke(k)\nWijk+1 =Wijk \u0000 \u000bk\u001bijke(k) (21)\nWhere the new time varying rate \u000bk is:\n\u000bk =\n\u000b0\n2\n0@ 1\n2 +\nMX\nj=1\n\u001e2jk +\nMX\nj=1\nNX\ni=1\n\u001b2ijk\n1A\nWhere i = 1; : : : ; N , j = 1; : : : ;M , \u001bijk =\nVjksech2(\nNX\ni=1\nWijkxi(k))xi(k) 2 < is de\u0002ned in (15), \u001ejk =\ntanh(\nNX\ni=1\nWijkxi(k)) 2 < is de\u0002ned in (11) and used in (16),\ne(k) is de\u0002ned in (18), 0 < \u000b0 \u0014 1 2 <, so 0 < \u000bk 2 <, it is\nassumed that the uncertainty is bounded [13], [18], [19], [20],\n[21], [24], [25], [26], [37] where \u0016 is the upper bound of the\nuncertainty \u0016(k), j\u0016(k)j < \u0016.\nRemark 1: Please note that e(k) = by(k) \u0000 y(k) =\nMX\nj=1\nVjk\u001ejk \u0000 y(k) used in (21) is well de\u0002ned because Vjk,\n\u001ejk, and y(k) are known.\nThe following theorem gives the stability of the proposed\nbackpropagation algorithm.\nTheorem 2: The backpropagation algorithm (11), (18), and\n(21) applied for the identi\u0002cation of the nonlinear system\n(10) is uniformly stable and the upper bound of the average\nidenti\u0002cation error e2p(k) satis\u0002es:\nlim sup\nT!1\n1\nT\nTX\nk=2\ne2p(k) \u0014 \u000b0\u00162 (22)\nWhere e2p(k) =\n\u000bk\n2 e\n2(k\u00001), 0 < \u000b0 \u0014 1 2 < and 0 < \u000bk 2 <\nare de\u0002ned in (21), e(k) is de\u0002ned in (18), \u0016 is the upper\nbound of the uncertainty \u0016(k), j\u0016(k)j < \u0016.\nProof: Let us de\u0002ne the following Lyapunov function:\nLk =\n1\n2\n\u000bke\n2(k \u0000 1) +\nMX\nj=1\neV 2jk + MX\nj=1\nNX\ni=1\nfW 2ijk (23)\nWhere eVjk and fWijk are de\u0002ned in (13). Then \u0001Lk is:\n\u0001Lk =\n1\n2\u000bke\n2(k) +\nMX\nj=1\neV 2jk+1 + MX\nj=1\nNX\ni=1\nfW 2ijk+1\n\u0000 12\u000bke2(k \u0000 1)\u0000\nMX\nj=1\neV 2jk \u0000 MX\nj=1\nNX\ni=1\nfW 2ijk (24)\nNow, the weights error can be rewritten as follows:\nMX\nj=1\neV 2jk+1 = MX\nj=1\neV 2jk\n\u00002\u000bke(k)\nMX\nj=1\neVjk\u001ejk + \u000b2ke2(k) MX\nj=1\n\u001e2jk\nMX\nj=1\nNX\ni=1\nfW 2ijk+1 = MX\nj=1\nNX\ni=1\nfW 2ijk\n\u00002\u000bke(k)\nMX\nj=1\nNX\ni=1\nfWijk\u001bijk + \u000b2ke2(k) MX\nj=1\nNX\ni=1\n\u001b2ijk\n(25)\nIt will be proven that (25) is true. Let us consider the eVjk\ncase, substituting eVjk+1 of (21) into MX\nj=1\neV 2jk+1 gives:\nMX\nj=1\neV 2jk+1 = MX\nj=1\nheVjk \u0000 \u000bk\u001ejke(k)i2\n=\nMX\nj=1\neV 2jk \u0000 2\u000bke(k) MX\nj=1\neVjk\u001ejk + \u000b2ke2(k) MX\nj=1\n\u001e2jk\nThen for eVjk of (25) is true. Let us consider the fWijk case,\nsubstituting fWijk+1 of (21) into MX\nj=1\nNX\ni=1\nfW 2ijk+1 gives:\nMX\nj=1\nNX\ni=1\nfW 2ijk+1 = MX\nj=1\nNX\ni=1\nhfWijk \u0000 \u000bk\u001bijke(k)i2 = MX\nj=1\nNX\ni=1\nfW 2ijk\n\u00002\u000bke(k)\nMX\nj=1\nNX\ni=1\nfWijk\u001bijk + \u000b2ke2(k) MX\nj=1\nNX\ni=1\n\u001b2ijk\nThen for fWijk of (25) is true. Thus (25) is true. Substituting\n(25) into (24) gives:\n\u0001Lk = \u00002\u000bke(k)\nMX\nj=1\neVjk\u001ejk + \u000b2ke2(k) MX\nj=1\n\u001e2jk\n\u00002\u000bke(k)\nMX\nj=1\nNX\ni=1\nfWijk\u001bijk + \u000b2ke2(k) MX\nj=1\nNX\ni=1\n\u001b2ijk\n+ 12\u000bke\n2(k)\u0000 12\u000bke2(k \u0000 1)\n(26)\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, XX 5\n(26) can be rewritten as:\n\u0001Lk = \u00002\u000bke(k)\n24 MX\nj=1\neVjk\u001ejk + MX\nj=1\nNX\ni=1\nfWijk\u001bijk\n35\n+\u000b2ke\n2(k)\n24 MX\nj=1\n\u001e2jk +\nMX\nj=1\nNX\ni=1\n\u001b2ijk\n35\n+ 12\u000bke\n2(k)\u0000 12\u000bke2(k \u0000 1)\n(27)\nSubstituting (20) in the \u0002rst term of the equation (27) gives:\n\u0001Lk = \u00002\u000bke(k) [e (k)\u0000 \u0016(k)]\n+\u000b2ke\n2(k)\n24 MX\nj=1\n\u001e2jk +\nMX\nj=1\nNX\ni=1\n\u001b2ijk\n35\n+ 12\u000bke\n2(k)\u0000 12\u000bke2(k \u0000 1)\n\u0001Lk = \u00002\u000bke2(k) + 2\u000bke(k)\u0016(k)\n+\u000b2ke\n2(k)\n24 MX\nj=1\n\u001e2jk +\nMX\nj=1\nNX\ni=1\n\u001b2ijk\n35\n+ 12\u000bke\n2(k)\u0000 12\u000bke2(k \u0000 1)\n(28)\nSubstituting \u000bk of (21) into the term\n\u000b2ke\n2(k)\n24 MX\nj=1\n\u001e2jk +\nMX\nj=1\nNX\ni=1\n\u001b2ijk\n35 and considering \u000b0 \u0014 1\ngives:\n\u000b2ke\n2(k)\n24 MX\nj=1\n\u001e2jk +\nMX\nj=1\nNX\ni=1\n\u001b2ijk\n35\n=\n\u000b0\n2664\nMX\nj=1\n\u001e2jk+\nMX\nj=1\nNX\ni=1\n\u001b2ijk\n3775\n2\n0BB@ 12+\nMX\nj=1\n\u001e2jk+\nMX\nj=1\nNX\ni=1\n\u001b2ijk\n1CCA\n\u000bke\n2(k)\n\u0014 \u000b02 \u000bke2(k) \u0014 12\u000bke2(k)\n(29)\nConsidering that 2\u000bke(k)\u0016(k) \u0014 \u000bke2(k) + \u000bk\u00162(k), and\nconsidering (29) in (28) gives:\n\u0001Lk \u0014 \u00002\u000bke2(k) + \u000bke2(k) + \u000bk\u00162(k)\n+ 12\u000bke\n2(k) + 12\u000bke\n2(k)\u0000 12\u000bke2(k \u0000 1)\n\u0001Lk \u0014 \u00001\n2\n\u000bke\n2(k \u0000 1) + \u000bk\u00162(k) (30)\nFrom (21), \u000bk = \u000b0\n2\n0BB@ 12+\nMX\nj=1\n\u001e2jk+\nMX\nj=1\nNX\ni=1\n\u001b2ijk\n1CCA\n\u0014 \u000b0, and\nconsidering that j\u0016(k)j \u0014 \u0016 in (30) gives:\n\u0001Lk \u0014 \u00001\n2\n\u000bke\n2(k \u0000 1) + \u000b0\u00162 (31)\nIt is known that there exits K1 functions \r1 (\u0001) and \r2 (\u0001)\nsuch that:\n\r1\n\u0010\n\u000bke\n2(k \u0000 1), eV 2jk, fW 2ijk\u0011 \u0014 Lk\nLk \u0014 \r2\n\u0010\n\u000bke\n2(k \u0000 1), eV 2jk, fW 2ijk\u0011 (32)\nUsing the fact that (32) satis\u0002es the \u0002rst condition of (3),\n(31) satis\u0002es the second condition of (3), thus using the\ntheorem of the preliminaries, it is known that the identi\u0002cation\nerror of the neural network applied for the identi\u0002cation of a\nnonlinear system is uniformly stable. So Lk is bounded, i.e.\nthe identi\u0002cation error is bounded. Using (31) and using e2p(k)\nde\u0002ned in (22) gives:\n\u0001Lk \u0014 \u0000e2p(k) + \u000b0\u00162 (33)\nSummarizing (33) from 2 to T , gives:\nTX\nk=2\n\u0000\ne2p(k)\u0000 \u000b0\u00162\n\u0001 \u0014 L1 \u0000 LT (34)\nSince LT > 0 is bounded:\n1\nT\nTX\nk=2\ne2p(k) \u0014 \u000b0\u00162 + 1T L1\nlim sup\nT!1\n1\nT\nTX\nk=2\ne2p(k) \u0014 \u000b0\u00162\n(35)\n(22) is established.\nRemark 2: There are two conditions to apply this algorithm\nfor nonlinear systems, the \u0002rst one is that the nonlinear system\nmay have the form described by equation (10), the second one\nis that the uncertainty \u0016(k) may be bounded.\nRemark 3: The value of the parameter \u0016 is unimportant,\nbecause this parameter is not used in the algorithm. The bound\nof \u0016(k) is needed to guarantee the stability of the algorithm,\nbut it is not used in the backpropagation algorithm (11), (18),\n(21).\nRemark 4: The fact that \u0016(k) is bounded has been used for\nother authors in some earlier studies as are [18], [19], [20] and\n[21] in continuous time systems and [13], [24], [25], [26], and\n[37] in discrete time systems.\nThe following theorem proves that the weights of the\nproposed backpropagation algorithm are bounded.\nTheorem 3: When the average error e2p(k) is bigger than the\nuncertainty \u000b0\u00162, the weights error is bounded by the initial\nweights error as follows:\ne2p(k) \u0015 \u000b0\u00162\n=)\nMX\nj=1\neV 2jk+1 + MX\nj=1\nNX\ni=1\nfW 2ijk+1 \u0014 MX\nj=1\neV 2j1 + MX\nj=1\nNX\ni=1\nfW 2ij1\n(36)\nWhere i = 1; : : : ; N , j = 1; : : : ;M , eVjk and fWijk is de\u0002ned\nin (13), eVj1 and fWij1 is the initial weights error, e2p(k) =\n\u000bk\n2 e\n2(k), Vjk+1, Wijk+1, 0 < \u000b0 \u0014 1 2 <, and 0 < \u000bk 2 <\nare de\u0002ned in (21), e(k) is de\u0002ned in (18), \u0016 is the upper\nbound of the uncertainty \u0016(k), j\u0016(k)j < \u0016.\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, XX 6\nProof: From (25), the weights are written as follows:\nMX\nj=1\neV 2jk+1 = MX\nj=1\neV 2jk\n\u00002\u000bke(k)\nMX\nj=1\neVjk\u001ejk + \u000b2ke2(k) MX\nj=1\n\u001e2jk\nMX\nj=1\nNX\ni=1\nfW 2ijk+1 = MX\nj=1\nNX\ni=1\nfW 2ijk\n\u00002\u000bke(k)\nMX\nj=1\nNX\ni=1\nfWijk\u001bijk + \u000b2ke2(k) MX\nj=1\nNX\ni=1\n\u001b2ijk\n(37)\nAdding\nMX\nj=1\neV 2jk+1 with MX\nj=1\nNX\ni=1\nfW 2ijk+1 of (37) gives:\nMX\nj=1\neV 2jk+1 + MX\nj=1\nNX\ni=1\nfW 2ijk+1 = MX\nj=1\neV 2jk + MX\nj=1\nNX\ni=1\nfW 2ijk\n\u00002\u000bke(k)\nMX\nj=1\neVjk\u001ejk + \u000b2ke2(k) MX\nj=1\n\u001e2jk\n\u00002\u000bke(k)\nMX\nj=1\nNX\ni=1\nfWijk\u001bijk + \u000b2ke2(k) MX\nj=1\nNX\ni=1\n\u001b2ijk\n(38)\n(38) can be rewritten as:\nMX\nj=1\neV 2jk+1 + MX\nj=1\nNX\ni=1\nfW 2ijk+1 = MX\nj=1\neV 2jk + MX\nj=1\nNX\ni=1\nfW 2ijk\n\u00002\u000bke(k)\n24 MX\nj=1\neVjk\u001ejk + MX\nj=1\nNX\ni=1\nfWijk\u001bijk\n35\n+\u000b2ke\n2(k)\n24 MX\nj=1\n\u001e2jk +\nMX\nj=1\nNX\ni=1\n\u001b2ijk\n35\n(39)\nSubstituting (20) in the second term of the equation (39) gives:\nMX\nj=1\neV 2jk+1 + MX\nj=1\nNX\ni=1\nfW 2ijk+1 = MX\nj=1\neV 2jk + MX\nj=1\nNX\ni=1\nfW 2ijk\n\u00002\u000bke(k) [e (k)\u0000 \u0016(k)]\n+\u000b2ke\n2(k)\n24 MX\nj=1\n\u001e2jk +\nMX\nj=1\nNX\ni=1\n\u001b2ijk\n35\nMX\nj=1\neV 2jk+1 + MX\nj=1\nNX\ni=1\nfW 2ijk+1 = MX\nj=1\neV 2jk + MX\nj=1\nNX\ni=1\nfW 2ijk\n\u00002\u000bke2(k) + 2\u000bke(k)\u0016(k)\n+\u000b2ke\n2(k)\n24 MX\nj=1\n\u001e2jk +\nMX\nj=1\nNX\ni=1\n\u001b2ijk\n35\n(40)\nSubstituting \u000bk of (21) into the last term of (40) and consid-\nering \u000b0 \u0014 1 gives:\n\u000b2ke\n2(k)\n24 MX\nj=1\n\u001e2jk +\nMX\nj=1\nNX\ni=1\n\u001b2ijk\n35\n=\n\u000b0\n2664\nMX\nj=1\n\u001e2jk+\nMX\nj=1\nNX\ni=1\n\u001b2ijk\n3775\n2\n0BB@ 12+\nMX\nj=1\n\u001e2jk+\nMX\nj=1\nNX\ni=1\n\u001b2ijk\n1CCA\n\u000bke\n2(k)\n\u0014 \u000b02 \u000bke2(k) \u0014 12\u000bke2(k)\n(41)\nConsidering that 2\u000bke(k)\u0016(k) \u0014 \u000bke2(k) + \u000bk\u00162(k), and\nconsidering (41) in (40) gives:\nMX\nj=1\neV 2jk+1 + MX\nj=1\nNX\ni=1\nfW 2ijk+1 \u0014 MX\nj=1\neV 2jk + MX\nj=1\nNX\ni=1\nfW 2ijk\n\u00002\u000bke2(k) + \u000bke2(k) + \u000bk\u00162(k) + 12\u000bke2(k)\nMX\nj=1\neV 2jk+1 + MX\nj=1\nNX\ni=1\nfW 2ijk+1\n\u0014\nMX\nj=1\neV 2jk + MX\nj=1\nNX\ni=1\nfW 2ijk \u0000 12\u000bke2(k) + \u000bk\u00162(k)\n(42)\nFrom (21), \u000bk = \u000b0\n2\n0BB@ 12+\nMX\nj=1\n\u001e2jk+\nMX\nj=1\nNX\ni=1\n\u001b2ijk\n1CCA\n\u0014 \u000b0 and\nconsidering that j\u0016(k)j \u0014 \u0016 in (42) gives:\nMX\nj=1\neV 2jk+1 + MX\nj=1\nNX\ni=1\nfW 2ijk+1\n\u0014\nMX\nj=1\neV 2jk + MX\nj=1\nNX\ni=1\nfW 2ijk \u0000 12\u000bke2(k) + \u000b0\u00162\n(43)\nConsidering e2p(k) =\n\u000bk\n2 e\n2(k) gives:\ne2p(k) \u0015 \u000b0\u00162\n)\nMX\nj=1\neV 2jk+1 + MX\nj=1\nNX\ni=1\nfW 2ijk+1 \u0014 MX\nj=1\neV 2jk + MX\nj=1\nNX\ni=1\nfW 2ijk\nConsidering that e2p(k) \u0015 \u000b0\u00162 for k 2 [1, k] is true, so:\nMX\nj=1\neV 2jk+1 + MX\nj=1\nNX\ni=1\nfW 2ijk+1 \u0014 MX\nj=1\neV 2jk + MX\nj=1\nNX\ni=1\nfW 2ijk\n\u0014 : : : \u0014\nMX\nj=1\neV 2j1 + MX\nj=1\nNX\ni=1\nfW 2ij1\nThus (36) is true.\nRemark 5: From the Theorem 2 the average identi\u0002cation\nerror e2p(k) of the backpropagation algorithm is bounded and\nfrom the Theorem 3 the weights error eV 2jk+1 and fW 2ijk+1\nis bounded, i.e. the proposed backpropagation algorithm to\ntrain a feedforward neural network is uniformly stable in\nthe presence of unknown and bounded uncertainties and the\nover\u0002tting mentioned in [14] and [35] is not presented. In\naddition, the identi\u0002cation error converges to a small zone\nbounded by the uncertainty \u0016.\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, XX 7\nFig. 2. The authomatic warehouse\nV. THE PROPOSED ALGORITHM\nThe proposed algorithm is as follows:\n1) Obtain the output of the nonlinear system y(k) with equa-\ntion (10), note that nonlinear system may have the structure\nwith equation (10), the parameter N is selected according to\nthis nonlinear system.\n2) Select the following parameters: V1 and W1 are selected\nas random number between 0 and 1. M is selected as an\nentire number and \u000b0 is selected with positive values smaller\nor equal to 1; obtain the output of the neural network by(1)\nwith equation (11).\n3) For each iteration k, obtain the output of the neural\nnetwork by(k) with equation (11), obtain the identi\u0002cation error\ne(k) with equation (18), and update the parameters Vjk+1 and\nWijk+1 with equation (21).\n4) Note that the behavior of the algorithm could be improved\nby changing the values of M or \u000b0.\nRemark 6: The proposed neural network has one hidden\nlayer. Some earlier results as [4], [19], and [30] mention that\nthere is a result where the feedforward neural network with one\nhidden layer is enough to approximate any nonlinear system.\nVI. THE WAREHOUSE\nAn automatic warehouse has elements used to make easy\nthe work of moving loads from one place to another one in an\nautomatic way. The loads are some objects inside of boxes that\nare saved in the warehouse until they are sent to the costumers.\nThe deposits are the place where the loads are placed. The\nFigure 2 shows the automatic warehouse in gray color, the\ndeposits in black color and the loads in brown color.\nA transelevator moves inside of the warehouse. This transe-\nlevator can be used to move some load from one place to\nanother one in the warehouse, for example, from the \u0003oor to\nthe deposit, from the deposit to the \u0003oor, from one deposit to\nanother one, or from a trailer to the deposits. The Figure 3\nshows a transelevator inside of the warehouse in yellow color\nand the Figure 4 shows the same transelevator moving a load.\nThe Figure 5 shows the trailer with the loads that are saved\nin the warehouse. The transelevator takes the loads from the\ntrailer and place them in the deposits.\nFig. 3. The transelevator inside of the warehouse\nFig. 4. The transelevator with a load\nFig. 5. The trailer with loads for the warehouse\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, XX 8\nIn this paper, the main prediction problem in the warehouse\nis the distribution of the loads that the transelevator receive\nfrom the trailer and place in the deposits each hour inside the\nwarehouse, the deposits in the warehouse can be reserved in\nadvance using the prediction results.\nVII. SIMULATIONS\nIn this section, two examples are considered. In the \u0002rst\nexample, the backpropagation algorithm is applied for the\nprediction of the distribution of loads inside a warehouse, the\nproposed algorithm is compared with the recursive least square\nalgorithm given by [9] and used by [1] and [17] and with the\nSugeno fuzzy inference system given by [14] and [30]. In\nthe second example, the backpropagation algorithm is applied\nin an academic problem, the proposed algorithm is compared\nwith the recursive least square algorithm given by [9] and used\nby [1] and [17] and with the Kalman \u0002lter algorithm given by\n[9] and [10] and used by [25].\nThe root mean square error (RMSE) [16] is used and it is\ngiven as follows:\nRMSE =\n \n1\nN\nNX\nk=1\ne2(k)\n! 1\n2\n(44)\nExample 1: In this example, the backpropagation is applied\nfor the prediction of the distribution of loads that the transel-\nevator receive from the trailer and place in the deposits each\nhour in the warehouse, there are 3 kind of loads received by\nthe transelevator inside the warehouse, the 3 kind of loads are\ndenoted as A, B and C, the 3 kind of loads are received in\nthe warehouse each hour, the number of loads of the kind A\nreceived each hour in the warehouse can vary from 4 to 5,\nthe number of loads of the kind B received each hour in the\nwarehouse can vary from 3 to 4 and the number of loads of the\nkind C received each hour in the warehouse can vary from 1 to\n3. The data of 1800 hours are used for the training and the data\nof the least 200 hours are used for the testing, the prediction\nis obtained with 200 hours in advance. 3 neural networks are\nused for the training and the same neural networks are used\nfor the testing, B(k) and C(k) are the inputs and A(k+200)\nis the output for the training of the \u0002rst neural network, A(k)\nand C(k) are the inputs and B(k+ 200) is the output for the\ntraining of the second neural network, A(k) and B(k) are the\ninputs and C(k + 200) is the output for the training of the\nthird neural network. Similar inputs are used for the testing\nof the three neural networks, and the outputs are not used for\nthe testing. In this prediction example, the backpropagation\nalgorithm is given as (11), (18), and (21) changing y(k)\nby y(k + 200) and changing e(k) by e(k + 200) [9]. The\nparameters of the backpropagation algorithm are N = 2,\nM = 5, Vj1 and Wij1 are random number between 0 and 1,\nand \u000b0 = 1. The backpropagation algorithm is compared with\nthe recursive least square algorithm given by [9] and used by\n[1] and [17] with parameters P1 = cI 2 <2\u00022, where c = 1,\nV1 are random number between 0 and 1 and is compared with\nthe Sugeno fuzzy inference system given by [14] and [30]\nwith parameters M = 2, m1, \u001b1 and v1 are random number\nbetween 0 and 1. The training results are shown in the Figure\nFig. 6. Training results for example 1\nFig. 7. Testing results for example 1\n6 and the testing results are shown in the Figure 7, the Table\n1 shows the training and the testing RMSE results using (44).\nThe Figure 8 shows that in this example not all the algorithms\nare stable because the Sugeno fuzzy inference system is not\nstable and it is reported in the Table 1.\nTable 1: Results for Example 1\nMethods Training RMSE Testing RMSE\nRecursive Least Square 0:0717 0:0121\nBackpropagation 0:0321 3:2561\u0002 10\u00005\nSugeno Fuzzy Inference NAN \u0000\nFrom the Table 1, it can be seen that the backpropagation\nalgorithm achieves better accuracy when compared with the re-\ncursive least square because the training RMSE and the testing\nRMSE are smaller for the backpropagation algorithm. From\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, XX 9\nFig. 8. Training for the Sugeno fuzzy inference system\nFig. 9. Average identi\u0002cation error for example 1\nthe Figures 6 and 7, it can be seen that the backpropagation\nimproves the recursive least square because the signal of the\n\u0002rst one follows better the signal of the plant than the signal of\nthe second one. From the Figure 8, the Sugeno fuzzy inference\nsystem is unstable for this prediction example, that is way it\nis important to guarantee the stability of the algorithms. Thus\nthe backpropagation is good for the prediction problems.\nThe Figure 9 shows the average of the identi\u0002cation error\nfor the modi\u0002ed backpropagation algorithm. From this Figure,\nit can be observed that the average of the identi\u0002cation error\nlim sup\nT!1\n1\nT\nTX\nk=2\ne2p(k) decrease and it will converge to a value\nsmaller than the upper bound of the uncertainty \u000b0\u00162, as stated\nin the Theorem 2.\nThe simulation of the weights error for the Theorem 3\nFig. 10. Training results for example 2\ncannot be obtained because the optimal weights which can\nminimize the modeling error are unknown [19].\nExample 2: Let us consider the nonlinear system given in\nan earlier study [30]:\ny(k) = 0:3y(k \u0000 1) + 0:6y(k \u0000 2) + f(u(k \u0000 1)) (45)\nWith f(u(k \u0000 1)) = 0:6 sin(\u0019u(k \u0000 1)) + 0:3 sin(3\u0019u(k \u0000\n1))+0:1 sin(5\u0019u(k\u00001)), the input is u(k\u00001) = sin(8\u0019(k\u0000\n1)=200). In this example, the backpropagation algorithm given\nas (11), (18), and (21) is used for the identi\u0002cation of the\nnonlinear plant (45). The parameters of the backpropagation\nalgorithm are N = 2, M = 3, Vj1 and Wij1 are random\nnumber between 0 and 1, and \u000b0 = 0:25. The backpropagation\nalgorithm is compared with the recursive least square algo-\nrithm given by [9] and used by [1] and [17] with parameters\nP1 = cI 2 <2\u00022, where c = 1, V1 are random number\nbetween 0 and 1, and is compared with the Kalman \u0002lter\nalgorithm given by [9] and [10] and used by [25] with\nparameters P1 = cI 2 <2\u00022, where c = 1, R1 = 0:1, R2 = 1,\nV1 are random number between 0 and 1. The training results\nare shown in the Figure 10 and the testing results are shown\nin the Figure 11, using (44) the Table 2 shows the training\nand the testing RMSE results.\nTable 2: Results for Example 2\nMethods Training RMSE Testing RMSE\nRecursive Least Square 0:0714 0:0183\nKalman Filter 0:0520 0:0283\nBackpropagation 0:0413 0:0132\nFrom the Table 2, it can be seen that the backpropagation\nalgorithm achieves better accuracy when compared with the\nrecursive least square and the Kalman \u0002lter because the\ntraining RMSE and the testing RMSE are smaller for the\nbackpropagation algorithm. From the Figures 10 and 11, it\ncan be seen that the backpropagation improves the recursive\nleast square and the Kalman \u0002lter because the signal of the\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, XX 10\nFig. 11. Testing results for example 2\nFig. 12. Average identi\u0002cation error for example 2\n\u0002rst follows better the signal of the plant than the signal of\nthe second and the third. Thus, the backpropagation is good\nfor the identi\u0002cation problems.\nThe Figure 12 shows the average of the identi\u0002cation error\nfor the modi\u0002ed backpropagation algorithm. From this Figure,\nit can be observed that the average of the identi\u0002cation error\nlim sup\nT!1\n1\nT\nTX\nk=2\ne2p(k) decrease and it will converge to a value\nsmaller than the upper bound of the uncertainty \u000b0\u00162, as stated\nin the Theorem 2.\nThe simulation of the weights error for the Theorem 3\ncannot be obtained because the optimal weights which can\nminimize the modeling error are unknown [19].\nVIII. CONCLUSION\nIn this paper, it was proposed a theorem to assure the\nuniform stability of discrete time systems, it was proven that\nthe backpropagation algorithm with a new time varying rate\nis uniformly stable for online identi\u0002cation, the identi\u0002cation\nerror converges to a small zone bounded by the uncertainty,\nand the weights error are bounded by their initial weights\nerror. The backpropagation algorithm was compared with the\nrecursive least square algorithm and with the Sugeno fuzzy\ninference system in the problem of the prediction of the\ndistribution of loads each hour inside a warehouse and the\nbackpropagation algorithm was compared with the recursive\nleast square and with the Kalman \u0002lter in an academic\nexample. From the Tables 1 and 2, it can be seen that\nthe backpropagation algorithm achieved better accuracy when\ncompared with the recursive least square algorithm and with\nthe Kalman \u0002lter algorithm, in addition, the Sugeno fuzzy\ninference system was unstable. From the Figures 6, 7, 10,\nand 11, it can be seen that the backpropagation algorithm\nimproves the recursive least square algorithm and with the\nKalman \u0002lter algorithm, from the Figure 8, it can be seen the\nSugeno fuzzy inference system is unstable in this example.\nFrom the simulation results, the backpropagation is good for\nthe prediction and the identi\u0002cation problems. As a future\nwork, an stable algorithm for the radial basis function will be\nproposed, a new algorithm for the feedforward neural network\nthat guarantees asymptotic stability will be proposed, a method\nto \u0002nd the number of neurons in the hidden layer online will\nbe proposed, and the proposed algorithms will be applied for\nother real problems.\nIX. ACKNOWLEDGEMENTS\nThe authors thank the Secretaria de Investigaci\u00f3n y Pos-\ngrado, the Comisi\u00f3n de Operaci\u00f3n y Fomento de Actividades\nAcad\u00e9micas del IPN, and the Consejo Nacional de Ciencia y\nTecnologia for their help in this research.\nREFERENCES\n[1] P. P. Angelov, D. P. Filev, A approach to Online Identi\u0002cation of\nTakagi-Sugeno Fuzzy Models, IEEE Transactions on Systems, Man and\nCybernetics, Vol.32, No.1, 484-498, 2004.\n[2] D. J. Berndt, A. R. Hevner, J. Studnicki, The Catch data warehouse:\nsupport for community health care decision-making, Decision Support\nSystems, Vol. 35, 367-384, 2003\n[3] C. I. Byrnes, A. Isidori and J. C. Willems, Passivity, feedback equiva-\nlence, and the global estabilization of minimum phase nonlinear systems,\nIEEE Transactions on Automatic Control, Vol.36, pp. 1228-1240, 1991.\n[4] G. Cybenco, Approximation by superposition of sigmoidal activation\nfunction, Math. Control Sig. Syst., 2, 303-314, 1989.\n[5] Eronini I, Umez Eronini E., System dinamics and control, ISBN 970-\n686-041-X, 1998.\n[6] C.I. Ezeife, Selecting and materializing horizontally partitioned ware-\nhouse views, Data & Knowledge Engineering, Vol. 36, 185-210, 2001.\n[7] B. Egardt, Stability of Adaptive Controlers, Springer-Verlag Berlin,\nLecture Notes in Control and Information Sciences, Vol.20, 1979.\n[8] V. Giordano, J. Bing, D. Naso, F. Lewis, Integrated Supervisory and\nOperational Control of a Warehouse With a Matrix-Based Approach,\nIEEE Transactions on Automation Science and Engineering, Vol. 5, No.\n1, 53-70, 2008\n[9] G. C. Goodwin and K. S. Sin, Adaptive \u0002ltering prediction and control,\nPrentice Hall, Englewood Clif\u0002s, NJ07632, 1984.\n[10] S. Haykin, Neural Networks- A Comprehensive Foundation, Macmillan\nCollege Publ. Co., New York, 1994.\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, XX 11\n[11] J. R. Hilera, V. J. Martines, Redes Neuronales Arti\u0002ciales, Fundamentos,\nModelos y Aplicaciones, Addison Wesley Iberoamericana, U.S.A., 1995.\n[12] P. A. Ioannou and J. Sun, Robust Adaptive Control, Prentice-Hall, Inc,\nUpper Saddle River: NJ, 1996.\n[13] S. Jagannathan, Control of a Class of Nonlinear Discrete-Time Systems\nUsing Multilayer Neural Networks, IEEE Transactions on Neural Net-\nworks, Vol. 12, No. 5, 1113-1120, 2001.\n[14] J. S. R. Jang, C. T. Sun, Neuro-Fuzzy and soft computing, Prentice Hall,\nNJ, 1996.\n[15] C.F. Juang and C.T. Lin, An Online Self Constructing Neural Fuzzy\nInference Network and Its Applications, IEEE Transactions on Fuzzy\nSystems, Vol.6, No.1, 12-32, 1998.\n[16] N. K. Kasabov, Evolving Fuzzy Neural Networks for Super-\nvised\/Unsupervised Online Knowledge-Based Learning, IEEE Transac-\ntions on Systems, Man and Cybernetics, Vol.31, No.6, 902-918, 2001.\n[17] N. K. Kasabov, Q. Song, DENFIS: Dynamic Evolving Neural-Fuzzy\nInference System and Its Application for Time-Series Prediction, IEEE\nTransactions on Fuzzy Systems, Vol.10, No.2, 144-154, 2002.\n[18] Y. H. Kim, F. L. Lewis, Optimal Design of CMAC Neural-Network\nController for Robot Manipulators, IEEE Transactions on Systems, Man\nand Cybernetics-Part C: Applications and Reviews, Vol.30, No.1, pp.\n22-30, 2000.\n[19] E.B. Kosmatopoulos, M.M. Polycarpou, M.A. Christodoulou, P. A.\nIoannou, High-order neural network structures for identi\u0002cation of\ndynamic systems, IEEE Transactions on Neural Networks, Vol.6, No.2,\n422-431, 1995.\n[20] C. Kwan, F. L. Lewis, D. M. Dawson, Robust Neural Network Control\nof Rigid-Link Electrically Driven Robots, IEEE Transactions on Neural\nNetworks, Vol.9, No.8, 591-588, 1998.\n[21] G. Loreto, R. Garrido, Stable Neurovisual Servoing for Robot Manipu-\nlators, IEEE Transactions on Neural Networks, Vol.17, No.4, 953-965,\n2006.\n[22] H. R. Nemati, D. M. Steiger, L. S. Iyer, R. T. Herschel, Knowledge\nwarehouse: an architectural integration of knowledge management,\ndecision support, artif icial intelligence and data warehousing, Decision\nSupport Systems, Vol. 33, 143-161, 2002.\n[23] G. V. Puskorius and L. A. Feldkamp, Neurocontrol of Nonlinear\nDynamical Systems with Kalman Filter Trained Recurrent Networks,\nIEEE Transactions on Neural Networks, Vol.5, No.2, 279-297, March\n1994.\n[24] J. J. Rubio, W. Yu, Stability Analysis of Nonlinear System Identi\u0002cation\nvia Delayed Neural Networks, IEEE Transactions on Circuits and\nSystems-II, Vol. 54, No. 2, 161-165, 2007.\n[25] J. J. Rubio and W. Yu, Nonlinear system identi\u0002cation with recurrent\nneural networks and dead-zone Kalman \u0002lter algorithm, Neurocomput-\ning, Vol.70, 2460-2466, 2007.\n[26] J. J. Rubio, SOFMLS, Online Self-Organizing Fuzzy Modi\u0002ed Least-\nSquares Network, IEEE Trancactions on Fuzzy Systems, Vol. 17, No. 6,\n1296-1309, 2009.\n[27] J. J. Rubio, Stability Analysis for an Online Evolving Neuro-Fuzzy\nRecurrent Network, Evolving Intelligent Systems: Methodology and\nApplications, John Wiley and Sons - IEEE Press, ISBN: 978-0-470-\n28719-4, 173-199, 2010.\n[28] J. A. K. Suykens, J. Vandewalle, B. D. Moor, Lu\u00b4re systems with\nmultilayer perceptron and recurrent neural networks: absolute stability\nand dissipativity, IEEE Trancactions on Automatic Control, Vol. 44, No.\n4, 770-774, 1999.\n[29] J. A. K. Suykens, B. D. Moor, J. Vandewalle, Robust local stability\nof multilayer recurrent neural networks, IEEE Trancactions on Neural\nNetworks, Vol. 11, No. 1, 222-229, 2000.\n[30] L. X. Wang, A Course in Fuzzy Systems and Control, Prentice Hall,\nEnglewood Cliffs, NJ, 1997.\n[31] Z. Wang, H. Zhang, W. Yu, Robust exponential stability analysis of\nneural networks with multiple time delays, Neurocomputing, Vol. 70,\n2534-2543, 2007.\n[32] Z. Wang, H. Zhang, W. Yu, Robust Stability of Cohen-Grosberg Neural\nNetworks via State Transmission Matrix, IEEE Transactions on Neural\nNetworks, Vol. 20, No. 1, 169-174, 2009.\n[33] Z. B. Xu, H. Qiao, J. Peng, B. Zhang, A comparative study of two\nmodeling approaches in nerual networks, Neural Networks, Vol. 17, 73-\n85, 2004.\n[34] J. Xu, X. Yao, C.-H. Choi, G. Gou, Materialized View Selection as\nConstrained Evolutionary Optimization, IEEE Transactions on Systems,\nMan and Cybernetics-Part C: Applications and Reviews, Vol. 33, No.\n4, ,458-467, 2003\n[35] W. Yu, X. Li, Discrete-time neuro-identi\u0002cation without robust iden-\nti\u0002cation, IEE Proccedings-Control Theory and Applications, Vol.150,\nNo.3, 311-316, 2003.\n[36] W. Yu, Passivity Analysis for Dynamic Multilayer Neuro Identi\u0002er,\nIEEE Transactions on Circuits and Systems I: Fundamental Theory an\nApplications, Vol. 50, No. 1, 2003.\n[37] W. Yu and X. Li, Fuzzy Identi\u0002cation Using Fuzzy Neural Networks\nwith Stable Learning Algorithms, IEEE Transactions on Fuzzy Systems,\nVol.12, No.3, 411-420, 2004.\n[38] W. Yu, Nonlinear system identi\u0002cation, using discrete-time recurrent\nneural networkswith stable learning algorithms, Information Sciences,\nVol. 158, 131-147, 2004.\n[39] W. Yu, M. A. Moreno, F. Ortiz, System identi\u0002cation using hierarchi-\ncal fuzzy neural networks with stable learning algorithm, Journal of\nIntelligent & Fuzzy Systems, Vol. 18, 171-183, 2007.\n[40] W. Yu, X. Li, Passivity Analysis of Dynamic Neural Networks, with\nDifferent Time-scales, Neural Processing Letters, Vol. 25, 143-155,\n2007.\n[41] X. Yu, M. Onder, O. Kaynak, A general Backpropagation Algorithm for\nFeedforward Neural Networks Learning, IEEE Transactions on Neural\nNetworks, Vol. 13, No. 1, 2002.\nJos\u00e9 de Jes\u00fas Rubio was born in M\u00e9xico City in 1979.\nHe received the B.S. degree from the Instituto Polit\u00e9cnico\nNacional in M\u00e9xico in 2001. He received the MS in automatic\ncontrol form the CINVESTAV IPN in M\u00e9xico in 2004, and\nthe Ph.D. in automatic control from the CINVESTAV IPN\nin M\u00e9xico in 2007. He was a full time professor in the Au-\ntonomous Metropolitan University - Mexico City from 2006\nto 2008. Since 2008, he is a full time professor of the Secci\u00f3n\nde Estudios de Posgrado e Investigaci\u00f3n - Instituto Polit\u00e9cnico\nNacional \u0096 ESIME Azcapotzalco. He has published 18 papers\nin international journals, 8 chapters in international books, and\nhe has presented 26 papers in International Conferences. He\nis a member of the IEEE AFS Adaptive Fuzzy Systems. He is\npart of the editorial board of the journal Evolving Systems. His\nresearch interests are primarily focused on neural networks,\nstability, evolving intelligent systems, nonlinear and adaptive\ncontrol systems, neural-fuzzy systems, mechatronic, robotic,\ndelayed systems and modeling.\n"}