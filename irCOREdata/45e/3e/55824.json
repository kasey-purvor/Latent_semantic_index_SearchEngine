{"doi":"10.1016\/j.imavis.2008.12.001","coreId":"55824","oai":"oai:eprints.lincoln.ac.uk:1752","identifiers":["oai:eprints.lincoln.ac.uk:1752","10.1016\/j.imavis.2008.12.001"],"title":"A spatially distributed model for foreground segmentation","authors":["Dickinson, Patrick","Hunter, Andrew","Appiah, Kofi"],"enrichments":{"references":[{"id":18438854,"title":"A Bayesian computer vision system for modeling human interactions.","authors":[],"date":"2000","doi":"10.1007\/3-540-49256-9_16","raw":"N. Oliver, B. Rosario, and A. Pentland. A Bayesian computer vision system for modeling human interactions. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8):831\u2013843, 2000.","cites":null},{"id":18438881,"title":"A dynamic conditional random \ufb01eld model for foreground and shadow segmentation.","authors":[],"date":"2006","doi":"10.1109\/tpami.2006.25","raw":"Y. Wang, K. Loe, and J. Wu. A dynamic conditional random \ufb01eld model for foreground and shadow segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(2):279\u2013289, 2006.","cites":null},{"id":18438880,"title":"A dynamic hidden Markov random \ufb01eld model for foreground and shadow segmentation.","authors":[],"date":"2005","doi":"10.1109\/acvmot.2005.3","raw":"Y. Wang, K. Loe, T. Tan, and J. Wu. A dynamic hidden Markov random \ufb01eld model for foreground and shadow segmentation. In Proc. of IEEE Workshop on Motion and Video Computing, pages 474\u2013480, Breckenridge, CO, USA, 2005.","cites":null},{"id":18438856,"title":"A MRF-based approach for realtime subway monitoring.","authors":[],"date":"2001","doi":"10.1109\/cvpr.2001.990644","raw":"N. Paragios and V. Ramesh. A MRF-based approach for realtime subway monitoring. In Proc. of IEEE Conference on Computer Vision and Pattern Recognition, volume 1, pages 1034\u20131040, Hawaii, USA, 2001.","cites":null},{"id":18438848,"title":"A pixel-wise object tracking algorithm with target and background sample.","authors":[],"date":"2006","doi":"10.1109\/icpr.2006.152","raw":"C. Hua, H. Wu, Q. Chen, and T. Wada. A pixel-wise object tracking algorithm with target and background sample. In Proc. of International Conference on Pattern Recognition, volume 1, pages 739\u2013742, Hong Kong, China, 2006.","cites":null},{"id":18438877,"title":"Adaptive background mixture models for real-time tracking.","authors":[],"date":"1999","doi":"10.1109\/cvpr.1999.784637","raw":"C. Stau\ufb00er and W.Grimson. Adaptive background mixture models for real-time tracking. In Proc. of IEEE Conference on Computer Vision and Pattern Recognition, volume 2, pages 246\u2013252, Fort Collins, CO, USA, 1999.","cites":null},{"id":18438849,"title":"An improved adaptive background mixture model for real-time tracking with shadow detection. In In","authors":[],"date":"2001","doi":"10.1007\/978-1-4615-0913-4_11","raw":"P. KaewTraKulPong and R. Bowden. An improved adaptive background mixture model for real-time tracking with shadow detection. In In Proc. European Workshop on Advanced Video Based Surveillance Systems, Kingston, UK, September 2001.","cites":null},{"id":18438843,"title":"Background and foreground modeling using nonparametric kernel density estimation for visual surveillance.","authors":[],"date":"2002","doi":"10.1109\/jproc.2002.801448","raw":"A. Elgammal, R. Duraiswami, D. Harwood, and L. Davis. Background and foreground modeling using nonparametric kernel density estimation for visual surveillance. Proceedings of the IEEE, 90(7):1151\u20131163, July 2002.","cites":null},{"id":18438853,"title":"Background modeling and subtraction of dynamic scenes.","authors":[],"date":"2003","doi":"10.1109\/iccv.2003.1238641","raw":"A. Monnet, A. Mittal, N. Paragios, and V. Ramesh. Background modeling and subtraction of dynamic scenes. In Proc. of IEEE International Conference on Computer Vision, volume 2, pages 1305\u20131312, Nice, France, 2003.","cites":null},{"id":18438869,"title":"Background subtraction based on cooccurrence of image variations.","authors":[],"date":"2003","doi":"10.1109\/cvpr.2003.1211453","raw":"M. Seki, T. Wada, H. Fujiwara, and K. Sumi. Background subtraction based on cooccurrence of image variations. In Proc. of IEEE Conference on Computer Vision and Pattern Recognition, volume 2, pages 65\u201372, Madison, WI, USA, 2003.","cites":null},{"id":18438851,"title":"Background subtraction using Markov thresholds.","authors":[],"date":"2005","doi":"10.1109\/acvmot.2005.33","raw":"J. Migdal and W. Grimson. Background subtraction using Markov thresholds. In Proc. of IEEE Workshop on Applications of Computer Vision \/ IEEE Workshop on Motion and Video Computing, volume 2, pages 58\u201365, Breckenridge, CO, USA, January 2005.","cites":null},{"id":18438871,"title":"Bayesian modeling of dynamic scenes for object detection.","authors":[],"date":"2005","doi":"10.1109\/cvpr.2005.86","raw":"Y. Sheikh and M. Shah. Bayesian modeling of dynamic scenes for object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(11):1778\u20131792, 2005.","cites":null},{"id":18438860,"title":"Color model selection and adaptation in dynamic scenes.","authors":[],"date":"1998","doi":"10.1007\/bfb0055684","raw":"Y. Raja, S. J. Mckenna, and S. Gong. Color model selection and adaptation in dynamic scenes. In Proc. of European Conference on Computer Vision, volume 1, pages 460\u2013474, Freiberg, Germany, 1998.","cites":null},{"id":18438873,"title":"Dynamic control of adaptive mixture-of-Gaussians background model.","authors":[],"date":"2006","doi":"10.1109\/avss.2006.44","raw":"A. Shimada, D. Arita, and R Taniguchi. Dynamic control of adaptive mixture-of-Gaussians background model. In Proc. of IEEE International Conference on Video and Signal Based Surveillance, Sydney, Australia, 2006.","cites":null},{"id":18438865,"title":"Evaluation of global image thresholding for change detection.","authors":[],"date":"2003","doi":"10.1016\/s0167-8655(03)00060-6","raw":"P. Rosin and E. Ioannidis. Evaluation of global image thresholding for change detection. Pattern Recognition Letters, 24(14):2345\u20132356, 2003.","cites":null},{"id":18438841,"title":"Flexible background mixture models for foreground segmentation.","authors":[],"date":"2006","doi":"10.1016\/j.imavis.2006.01.018","raw":"J. Cheng, J. Yang, Y. Zhou, and Y. Cui. Flexible background mixture models for foreground segmentation. Image and Vision Computing, 24(5):473\u2013482, 2006.","cites":null},{"id":18438846,"title":"Foreground segmentation using adaptive mixture models in color and depth.","authors":[],"date":"2001","doi":"10.1109\/event.2001.938860","raw":"M. Harville, G. Gordon, and J. Wood\ufb01ll. Foreground segmentation using adaptive mixture models in color and depth. In Proc. of IEEE Workshop on Detection and Recognition of Events in Video, pages 3\u201311, Vancouver, Canada, July 2001.","cites":null},{"id":18438862,"title":"Motion detection with nonstationary background.","authors":[],"date":"2003","doi":"10.1007\/s00138-002-0091-0","raw":"Y. Ren, C. Chua, and Y. Ho. Motion detection with nonstationary background. Machine Vision and Applications, 13(5):332\u2013343, 2003.","cites":null},{"id":18438847,"title":"Motion-based object detection and tracking in color image sequences.","authors":[],"date":"2000","doi":null,"raw":"B. Heisele. Motion-based object detection and tracking in color image sequences. In Proc. of Asian Conference on Computer Vision, pages 1028\u20131033, Taipei, Taiwan, 2000.","cites":null},{"id":18438875,"title":"Moving object segmentation by background subtraction and temporal analysis.","authors":[],"date":"2006","doi":"10.1016\/j.imavis.2006.01.001","raw":"P. Spagnolo, T. D\u2019Orazio, M. Leo, and A. Distante. Moving object segmentation by background subtraction and temporal analysis. Image and Vision Computing, 24(5):411\u2013423, 2006.","cites":null},{"id":18438882,"title":"P\ufb01nder: Real-time tracking of the human body.","authors":[],"date":"1997","doi":"10.1109\/34.598236","raw":"C. Wren, A. Azarbayejani, T. Darrell, and A. Pentland. P\ufb01nder: Real-time tracking of the human body. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(7):780\u2013785, 1997.","cites":null},{"id":18438844,"title":"Probabilistic spacetime video modeling via piecewise GMM.","authors":[],"date":"2004","doi":"10.1109\/tpami.2004.1262334","raw":"H. Greenspan, J. Goldberger, and A. Mayer. Probabilistic spacetime video modeling via piecewise GMM. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(3):384\u2013396, 2004.","cites":null},{"id":18438878,"title":"Robust and e\ufb03cient foreground analysis for real-time video surveillance.","authors":[],"date":"2005","doi":"10.1109\/cvpr.2005.304","raw":"Y. Tian, M. Lu, and A. Hampapur. Robust and e\ufb03cient foreground analysis for real-time video surveillance. In Proc. of IEEE Conference on Computer Vision and Pattern Recognition, volume 1, pages 1182\u20131187, San Diego, CA, USA, 2005.","cites":null},{"id":18438867,"title":"Smooth foreground-background segmentation for video processing.","authors":[],"date":"2006","doi":"10.1007\/11612704_58","raw":"K. Schindler and H. Wang. Smooth foreground-background segmentation for video processing. In Proc. of Asian Conference 12on Computer Vision, volume 2, pages 581\u2013590, Hyderabad, India, 2006.","cites":null},{"id":18438858,"title":"Tracking by cluster analysis of image di\ufb00erences.","authors":[],"date":"2000","doi":"10.1016\/s0921-8890(02)00203-8","raw":"A. Pece. Tracking by cluster analysis of image di\ufb00erences. In Proc. of International Symposium on Intelligent Robotic Systems, pages 295\u2013303, Reading, UK, 2000.","cites":null},{"id":18438850,"title":"Tracking groups of people.","authors":[],"date":"2000","doi":"10.1006\/cviu.2000.0870","raw":"S. McKenna, S. Jabri, Z. Duric, A. Rosenfeld, and H. Wechsler. Tracking groups of people. Computer Vision and Image Understanding, 80(1):42\u201356, 2000.","cites":null},{"id":18438845,"title":"W4: Who? when? where? what? A real time system for detecting and tracking people.","authors":[],"date":"1998","doi":"10.1109\/cvpr.1998.698720","raw":"I. Haritaoglu, D. Harwood, and L. Davis. W4: Who? when? where? what? A real time system for detecting and tracking people. In Proc. of International Conference on Automatic Face and Gesture Recognition, pages 222\u2013227, Nara, Japan, 1998.","cites":null},{"id":18438879,"title":"Wall\ufb02ower: Principles and practice of background maintenance.","authors":[],"date":"1999","doi":"10.1109\/iccv.1999.791228","raw":"K. Toyama, J. Krumm, B. Brumitt, and B. Meyers. Wall\ufb02ower: Principles and practice of background maintenance. In Proc. of International Conference on Computer Vision, volume 1, pages 255\u2013261, Corfu, Greece, 1999.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2009-08-03","abstract":"Foreground segmentation is a fundamental first processing stage for vision systems which monitor real-world activity. In this paper we consider the problem of achieving robust segmentation in scenes where the appearance of the background varies unpredictably over time. Variations may be caused by processes such as moving water, or foliage moved by wind, and typically degrade the performance of standard per-pixel background models.\\ud\nOur proposed approach addresses this problem by modeling homogeneous regions of scene pixels as an adaptive mixture of Gaussians in color and space. Model components are used to represent both the scene background and moving foreground objects. Newly observed pixel values are probabilistically classified, such that the spatial variance of the model components supports correct classification even when the background appearance is significantly distorted. We evaluate our method over several challenging video sequences, and compare our results with both per-pixel and Markov Random Field based models. Our results show the effectiveness of our approach in reducing incorrect classifications","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/55824.pdf","fullTextIdentifier":"http:\/\/eprints.lincoln.ac.uk\/1752\/1\/iavc-pld-06.pdf","pdfHashValue":"fc173b0da53fcbc9b3b1768fc2bbd05898a15c68","publisher":"Elsevier","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lincoln.ac.uk:1752<\/identifier><datestamp>\n      2013-12-04T15:49:02Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47373430<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lincoln.ac.uk\/1752\/<\/dc:relation><dc:title>\n        A spatially distributed model for foreground segmentation<\/dc:title><dc:creator>\n        Dickinson, Patrick<\/dc:creator><dc:creator>\n        Hunter, Andrew<\/dc:creator><dc:creator>\n        Appiah, Kofi<\/dc:creator><dc:subject>\n        G740 Computer Vision<\/dc:subject><dc:description>\n        Foreground segmentation is a fundamental first processing stage for vision systems which monitor real-world activity. In this paper we consider the problem of achieving robust segmentation in scenes where the appearance of the background varies unpredictably over time. Variations may be caused by processes such as moving water, or foliage moved by wind, and typically degrade the performance of standard per-pixel background models.\\ud\nOur proposed approach addresses this problem by modeling homogeneous regions of scene pixels as an adaptive mixture of Gaussians in color and space. Model components are used to represent both the scene background and moving foreground objects. Newly observed pixel values are probabilistically classified, such that the spatial variance of the model components supports correct classification even when the background appearance is significantly distorted. We evaluate our method over several challenging video sequences, and compare our results with both per-pixel and Markov Random Field based models. Our results show the effectiveness of our approach in reducing incorrect classifications.<\/dc:description><dc:publisher>\n        Elsevier<\/dc:publisher><dc:date>\n        2009-08-03<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lincoln.ac.uk\/1752\/1\/iavc-pld-06.pdf<\/dc:identifier><dc:identifier>\n          Dickinson, Patrick and Hunter, Andrew and Appiah, Kofi  (2009) A spatially distributed model for foreground segmentation.  Image and vision computing, 27  (9).   pp. 1326-1335.  ISSN 0262-8856  <\/dc:identifier><dc:relation>\n        http:\/\/www.sciencedirect.com\/science?_ob=GatewayURL&_origin=IRSSCONTENT&_method=citationSearch&_piikey=S0262885608002540&_version=1&md5=74976148d8b797ad625ae2957c2ac048<\/dc:relation><dc:relation>\n        10.1016\/j.imavis.2008.12.001<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lincoln.ac.uk\/1752\/","http:\/\/www.sciencedirect.com\/science?_ob=GatewayURL&_origin=IRSSCONTENT&_method=citationSearch&_piikey=S0262885608002540&_version=1&md5=74976148d8b797ad625ae2957c2ac048","10.1016\/j.imavis.2008.12.001"],"year":2009,"topics":["G740 Computer Vision"],"subject":["Article","PeerReviewed"],"fullText":"A Spatially Distributed Model for Foreground Segmentation\nPatrick Dickinson, Andrew Hunter, Kofi Appiah\nCenter for Visual Surveillance and Machine Perception, University of Lincoln, Lincoln, UK\nAbstract\nForeground segmentation is a fundamental first processing stage for vision systems which monitor real-world activity. In this paper\nwe consider the problem of achieving robust segmentation in scenes where the appearance of the background varies unpredictably\nover time. Variations may be caused by processes such as moving water, or foliage moved by wind, and typically degrade the\nperformance of standard per-pixel background models.\nOur proposed approach addresses this problem by modeling homogeneous regions of scene pixels as an adaptive mixture of\nGaussians in color and space. Model components are used to represent both the scene background and moving foreground objects.\nNewly observed pixel values are probabilistically classified, such that the spatial variance of the model components supports correct\nclassification even when the background appearance is significantly distorted. We evaluate our method over several challenging\nvideo sequences, and compare our results with both per-pixel and Markov Random Field based models. Our results show the\neffectiveness of our approach in reducing incorrect classifications.\nKey words: Foreground segmentation, Background model, Spatial coherence, Mixture of Gaussians\n1. Introduction\nThe purpose of an automated visual surveillance sys-\ntem is to extract meaningful information from an image\nsequence. A series of \u201cbottom-up\u201d processing steps is typ-\nically applied to each new image frame, each eliciting a\nmore refined and descriptive representation of the observed\nscene.\nThe lowest level of processing is applied to the entire\nset of image pixels. The aim is to identify regions of inter-\nest (usually moving objects) for further processing. With-\nout making prior assumptions about appearance, an effec-\ntive approach is to first build a model of the empty scene,\nor background. New foreground objects may then be seg-\nmented by comparison: \u201cbackground subtraction\u201d removes\nthose pixels which closely match the background model,\nleaving a residual subset of pixels corresponding to fore-\nground objects. Typically, further stages of processing clus-\nter the foreground pixels into object representations, track\nobjects from frame-to-frame, and infer relevant behavioral\ncharacteristics. The process of background subtraction is\ncritical to the system performance, as segmentation errors\nreduce the effectiveness of subsequent processing. However,\nthis remains a challenging task outside of laboratory con-\nditions.\nUntil recently, most background subtraction schemes\nhave employed a per-pixel background model. Previous\nobservations are used to construct a background represen-\ntation for each pixel location: this may include intensity,\ncolor, and possibly other features. The background sub-\ntraction process then independently classifies each pixel:\nthe new observed value is compared with its model, and la-\nbeled as either foreground or background. There are many\nexamples of per-pixel models. Haritaoglu\u2019s \u201cW4\u201d [4] sys-\ntem models each pixel\u2019s background as a mean value, with\nlower and upper ranges of tolerance. A statistical model\nis used by Wren\u2019s \u201cPfinder\u201d system [28], which maintains\nadaptive Gaussian distributions. The model parameters\nare estimated from previous observed values, and new ob-\nservations with low probability are labeled as foreground.\nThe Wallflower system [25] uses a Weiner filter to pre-\ndict pixel background values from a linear combination of\nrecent observations.\nWe can expect that the appearance of the scene back-\nground will change over time; for example, due to gradual\nchanges in lighting, or small movements by background ob-\njects. To account for this, systems such as those described\nabove employ adaptive models. However, some changed\npixels will be misclassified during the adaptation process.\nMoreover, simple adaptation is not enough to capture more\ncomplex background processes. Dynamic backgrounds,\nwhich exhibit repeating spatio-temporal variations, are\nPreprint submitted to Elsevier 21 August 2008\ncommon: for example, outdoor scenes often include foliage\nwhich may move in the wind.\nIn such cases, background values observed at a single\npixel may be generated by more than one process. Per-\npixel models have therefore been developed which repre-\nsent multi-modal behavior. For example, Elgammal [2] uses\na non-parametric kernel density estimate to model back-\nground distributions. The most widely adopted model is\nthat proposed by Stauffer and Grimson [23].\nStauffer uses an adaptive mixture of Gaussians (MoG)\nto model observations of each pixel\u2019s process in RGB color\nspace. Thus, at time t, the probability of observing a new\ncolor value xi,t at pixel i is given by:\np\n(\nx(i,t)|\u0398(i,t)\n)\n=\n\u2211K\nk=1\n\u03c9k(i,t)\u03b7\n(\n\u00b5k(i,t),\u03a3\nk\n(i,t)\n)\n(1)\nWhere \u0398(i,t) =\n{\n\u03b81(i,t) . . . \u03b8\nK\n(i,t)\n}\nare the mixture model\nparameters estimated at time t, K is the (fixed) number\nof components, \u03b7 is the multivariate Gaussian probability\ndistribution function, and\n\u2211K\nk=1\n\u03c9k(i,t) = 1 (2)\nTypically, between 3 and 5 components are used, and\nhighly weighted components are taken to be generated by\nbackground processes. Each new pixel value is matched\nagainst existing components. If it is matched to a back-\nground component, the pixel is labeled as background; or\nforeground otherwise. The model is then updated to incor-\nporate the new observation.\nStauffer\u2019s multi-modal scheme allows a time-varying\nbackground to be modeled on a per-pixel basis, provided\nthat the model is suitably parameterized, and that each\npixel\u2019s background modes are frequently presented. This\nmodel has become a de facto standard in automated\nsurveillance, and much research has been directed at refin-\ning it. For example, KaewTraKulPong [8] proposes a differ-\nent model update procedure, and a normalized color space;\nHarville [5] adds depth information from a stereo camera,\nand sets a separate learning rate for each pixel; Shimada\n[21] and Cheng [1] have both recently investigated optimal\nmodel orders; and Tian [24] has developed modifications\nto deal with illumination changes, and shadowing.\nDespite its popularity, there are a number of well doc-\numented limitations to the per-pixel MoG model. Varia-\ntions which are sporadic, or where one mode dominates,\nare still not well represented. Unfortunately such variations\nare common where the underlying process is erratic, for ex-\nample, moving foliage. Similarly, as Tian [24] notes, fore-\nground objects are absorbed at different rates at different\npixels, causing object fragmentation. Fragmentation prob-\nlems also arise where foreground objects overlap spatially\nwith background objects of similar color.\nThese types of errors are systemic under the assump-\ntion of an independent pixel model. Scene images are gen-\nerated by a set of discrete objects (both background and\nforeground) such that pixel values generated by the same\nobject exhibit a strong spatial, chromatic, and temporal\ncoherence. Such relationships are not represented by a per-\npixel model, but can be used to address the above classifica-\ntion problems, and to produce a more robust segmentation\nin general.\nThe work we present here is directed at this goal. We be-\ngin by reviewing some existing approaches to this problem,\nand proceed to describe our algorithm in detail. We con-\nclude with an experimental comparison of our algorithm\nwith Stauffer\u2019s original, and also with a recentMarkov Ran-\ndom Field (MRF) based model, in sequences with challeng-\ning spatio-temporal backgrounds variations.\n2. Previous Work\nInterest has grown recently in background models which\nrepresent spatial relationships between pixels, and in this\nsection we review some existing work. In particular we ex-\namine methods which, like ours, explicitly model spatial\ndistributions in the background. We also pay particular at-\ntention to Random Field based methods, which we use for\ncomparison in our evaluation.\nAlthough per-pixel models do not directly express spa-\ntial relationships between pixels, some authors have modi-\nfied the classification process to account for perturbations\nin scene structure. In this type of approach, classification\nincorporates not only the background model of the pixel,\nbut also the models of pixels in its local neighborhood. Each\npixel model is still independent, and so no account is made\nof the overall coherency of segmentation; however, misclas-\nsifications may be locally eliminated.\nAt the simplest level, Elgammal [2] adds a neighborhood\ncomparison to his non-parametric model. Pixels which do\nnot match their own background model are compared to\nthe models in a small neighborhood. A similar, but more\ncomprehensive, approach is taken by Ren\u2019s \u201cSpatially Dis-\ntributed Gaussians\u201d (SDG) [16] to account for global back-\nground transformations caused by a moving camera. This\nsystem first registers global image features to estimate a\nglobal translation between the background model and ob-\nserved image. This estimate is used as the starting point for\na search for matches between an observed pixel value and a\nneighborhood of background models. A MoG background\nis used, and a background label is generated for any match\nin the search area.\nSpagnolo [22] takes a slightly different approach, incorpo-\nrating neighboring values directly into the matching value\nof each pixel. A radiometric similarity measure is used\nto compare an observed pixel value with its background\nvalue, and incorporates values from the local neighbour-\nhood. Classification is performed by thresholding the simi-\nlarity. Pixel-wise temporal difference is first generated from\nthe two most recent frames, and used as a mask for the\nbackground subtraction.\nToyama [25] also uses pixel neighborhood color values to\nidentify pixels incorrectly labeled as background. Strongly\n2\nsupported regions of foreground pixels are used to build\ncolor histograms, and then to seed areas for connected com-\nponents expansion against their color model. Although this\napproach still employs a per-pixel model, there some con-\nsideration of how the structure of a segmented object can\nbe used to support pixel classification.\n3. Sub-space Background Representations\nEigenbackgrounds [12] represent the background as a set\nof dominant eigenvectors, extracted from a set of training\nimages. This sub-space identifies regions of the image which\nare invariant, and foreground pixels are identified by com-\nparing a new image with its projection through the sub-\nspace back to image space. Monnet [11] develops this by em-\nploying an on-line auto-regressive model to predict changes\nin background structure. This method captures repeating\nvariations in the spatial structure of the scene background,\nbut is inflexible as only variations presented during train-\ning are represented.\nAn interesting and unusual approach is taken by Seki\n[19], who uses co-occurrence of adjacent spatial features\nto model the spatio-temporal structure of a time-varying\nbackground. A series of background training frames are di-\nvided into blocks of pixels, and eigen-decomposition [12] is\nused to represent each block in an appropriate sub-space.\nThe training data is used to learn model temporal correla-\ntions between adjacent blocks, such that an observed value\nfor a block may be used to predict the appearance of its\nneighbors. Sets of blocks which are not well correlated in\nan input image are considered to have low background like-\nlihood. This offers an improvement over eigenbackgrounds,\nas the correlations are localized. However, as these spatial\nrelationships are learned the model still lacks the flexibility\nto deal with unpredictable variations.\n3.1. Random Fields\nThemethods discussed above are somewhat limited: they\nare either modifications of pixel-based methods, or learn\npatterns of invariance to detect unusual observations. A\nmore flexible and useful model is one which can express\ngeneral spatial properties of a scene\u2019s structure. For this\nreason, Markov Random Fields (MRFs), and (more gener-\nally) Conditional Random Fields (CRFs), have recently re-\nceived some recent attention as a foreground segmentation\nmethod.\nMRFs are probabilistic graphical models in which each\nnode represents a random variable, and edges between\nnodes represent dependencies. In the case of foreground\nsegmentation schemes, nodes represent pixel labelings\n{foreground, background}, and each node is connected\nto its spatial 4 or 8 pixel neighborhood. The edge depen-\ndencies express the Markovian nature of the local node\ndependency: given its neighbors, each node is condition-\nally independent of the rest of the field. Given an observed\nimage I, and suitable observation likelihood function, the\naim is to estimate the labeling L which maximizes the\nposterior (MAP estimate):\np (L|I) = p (I|L) p (L) (3)\nThis is made tractable by the Hammersley-Clifford theo-\nremwhich by re-expresses the dependencies as a set of node-\nclique potentials. Quantifying these potentials defines the\nprior probability for a given global field labeling. Assuming\nan independent observation likelihood for each pixel, the\nprobability of a single node label change can be estimated\nby simply summing energy terms. This forms the basis of\nstandard MAP-MRF labeling techniques. The advantage\nof these techniques is that clique potentials which impose\nsuitable spatial dependencies can be easily expressed at\npixel level. An appropriate observation likelihood can also\nbe defined for each pixel, and the MAP estimation results\nin a segmentation which is globally optimal.\nParagios [13] proposed aMRF segmentation scheme for a\nsubway monitoring system which used normalized color as\nan observation likelihood. Pixel-wise gradient observations\nwere also used to relax spatial continuity constraints across\ndiscontinuities. The iterated conditional modes algorithm\nis used to estimate the optimal labeling. More recently,\nSchindler [18] uses a per-pixel MoG model to develop the\nobservation likelihood, and resolves the field using a graph\ncutting algorithm. Sheikh [20] also uses a graph cutting\nalgorithm in conjunction with a kernel density estimate to\nbuild background and foreground observation likelihoods.\nBoth thesemethods uses 4-neighborhood cliques to simplify\nthe graph.\nWang has sought to incorporate temporal as well as spa-\ntial constraints by incorporating a Hidden Markov Model\n(HMM) into a MRF framework [26] and by using a CRF\n[27]. In both cases, a third label and corresponding obser-\nvation likelihood is added to represent areas of shadow.\nThe MRF scheme proposed by Migdal and Grimson [10]\ndevelops directly from Stauffer\u2019s per-pixel MoGmodel. The\nMoG model is used to initialize the field for each frame,\nand the dominant background distribution is used as a\nbackground observation likelihood. Foreground likelihood\nis modeled as a uniform distribution in RGB color space.\nThis model also includes temporal dependencies by linking\neach pixel to its previous labeling. We have chosen this al-\ngorithm as a benchmark for our work, and so we describe\nsome implementation details here.\nThe Hammersley-Clifford theorem formulates the prob-\nability of a field labeling L \u2208 \u03a6, where \u03a6 is the set of all\npossible labellings, as a Gibbs distribution:\np (L) =\ne\u2212U(L)\/T\nZ\n(4)\nWhere Z is a normalisation constant, T is a temperature\nterm used in the annealing process, and (L) is an energy\nfunction such that:\n3\nU (L) =\n\u2211\nc\u2208C\nV (L) (5)\nMigdal uses Gibbs sampling to estimate the MAP field\nlabeling, proposing a linear annealing schedule for T over a\nfixed number of field iterations. The clique potentials V (L)\ndefine the spatial and temporal constraints, and are applied\nto pair-wise cliques in a pixel\u2019s 8-neighborhood. Values for\nthe pair-wise clique potentials are not specified, but we have\nexperimented with a range of values in our evaluation.\n3.2. Spatially Distributed Model Components\nA scene object is likely to generate pixels which are spa-\ntially coherent, and share similar color attributes. Conse-\nquently, a number of authors [14,6,7,3] have proposed back-\nground (and foreground) models in which clusters of ho-\nmogeneous pixels are represented parametrically in an ex-\ntended feature space. A typical feature spaces includes spa-\ntial and color information, such that a single pixel is rep-\nresented by a 5-dimensional vector xt = [x, y,R,G,B]\nT\n.\nClassifying an individual pixel involves assigning it to the\nmodel component most likely to have generated it.\nPixels are not then explicitly labeled as foreground or\nbackground: a model component may represent either part\nof the foreground or the background, so labeling is implied\nby association. Model adaptation may be implemented by\nre-estimating the components from their assigned pixels.\nIn a probabilistic framework this approach amounts to\nexpressing learned spatial relationships in the observation\nlikelihood, rather than, in the case of random fields, apply-\ning spatial dependencies as a prior. Like per-pixel models,\nspatial distributions may be learned and updated as the\nscene evolves, but considerably fewer components are re-\nquired.\nThis type of representation naturally captures small\nchanges in background structure. Small movements of\nbackground objects generate new background pixels which\nstill have a high likelihood under the background model.\nIn addition, there are a number of other advantages over\nthe more typical processing architecture. Pixel clustering\nis more usually executed after classification, as a second\nforeground processing stage [9], [28]. Using clusters di-\nrectly for segmentation integrates the two processes such\nthat model adaptation automatically effects frame-to-\nframe object correspondence, and changes in scene struc-\nture are fed back to the next image classification step. It\nalso allows foreground and background to be defined at\nthe object level rather than pixel level: attributes such as\nsize, or movement, can be used to specify which pixels are\nforeground and which are background.\nA partial implementation of this type of model is pre-\nsented by Raja\u2019s object tracker [15] which builds an off-line\nMoG model of color distributions for a known object and\nscene background. A spatial component is not included in\nthe likelihood function, but implemented more simply as\nan axis-aligned bounding box approximating the extents of\nthe object. Pixels inside the bounding box are classified as\nforeground or background according to their likelihood of\ntheir observed color value.\nA more principled approach is taken by Pece [14], using\npixel intensity rather than color. In this system, the spa-\ntial foreground components are represented by a Gaussian\ndistribution, and the intensity as a uniform distribution.\nThe background distribution is uniform in space, and ex-\nponential in pixel intensity. For each new image, each pixel\nis assigned to the most likely component, and the compo-\nnents are updated using Expectation Maximisation (EM).\nForeground clusters are added and removed to adapt the\nmodel as objects appear and disappear from the scene.\nHeisele [6] uses an iterative K-Means algorithm to cluster\npixels in 5-Dimensional space. The model is adapted appro-\npriately, and clusters with similar trajectories are grouped\nto form object hypotheses. Recently, Huac [7] has built on\nthis work by explicitly classifying clusters as background\nor foreground. Spatial ambiguities are resolved by defining\nan elliptical search area for each cluster derived from the\nspatial covariance of its assigned pixels.\nThe distribution model used by Greenspan\u2019s video in-\ndexing system [3] bears some similarity to ours. Spatial and\ncolor cluster coordinates are modeled separately as inde-\npendent Gaussian distributions, and time is added to give a\n6-dimensional feature space. A video sequence is split into\nsub-sequences, and each is segmented separately. This is a\ntwo stage process: first, an appropriate model order is esti-\nmated, and then it is used to segment each image.\nThe modeling process using EM, and the Minimum De-\nscription Length principle to estimate the optimal model\norder. This involves building a series of models for each sub-\nsequence, and then selecting the most appropriate: conse-\nquently there is a considerable processing overhead. The\nsub-sequences are then aligned by building correspondences\nbetween components in successive sub-sequences, which al-\nlows objects to be tracked across the entire sequence.\nUnlike our system, Greenspan\u2019s is unsuitable for on-line\nprocessing. Firstly, it is necessary to capture and sub-divide\nthe sequence before segmentation can be applied. Secondly,\nmodel estimation requires that many models are built and\ncompared for each sub-sequence, incurring a high process-\ning overhead. Greenspan\u2019s system also exhibits model dis-\ncontinuity at the transition from one sub-sequence to the\nnext: a separate correspondence scheme is needed to track\nobjects across transitions.\n4. Our Approach\nIn our systemwemodel homogeneous regions of the scene\nusing an adaptive mixture of Gaussians in 5-dimensional\nfeature space. Each pixel observation is represented by a\ncorresponding feature vector xt = [x, y, Y, U, V ]\nT\nwhere\ncolor is encoded using the YUV format. The probability\ndistribution function for each model component is given by:\n4\np(xt|\u03b8(j,t)) = \u03c9(j,t)\ne\u2212\n1\n2 (xt\u2212\u00b5(j,t))(\u03a3(j,t))\n\u22121(xt\u2212\u00b5(j,t))\u221a\n(2pi)d|\u03a3(j,t)|\n(6)\nWhere the parameters \u03b8(j,t) =\n{\n\u03c9(j,t), \u00b5(j,t),\u03a3(j,t)\n}\nare\nthe component weight, mean, and covariance matrix of the\njth component at time t, and the dimensionality, d, is 5. For\nk components, the general mixture model conditions given\nby equations (1) and (2) also hold in their appropriate form.\nGiven a new observed image, and a set of model parame-\nters, an observed pixel value may be classified by assigning\nit to the component with the maximum posterior probabil-\nity, Cmap. Using log likelihoods:\nCmap = argmaxj\n{\nlog(p(xt|\u03b8(j,t))\n}\n(7)\nWe have simplified the model slightly by assuming that\nthe spatial and color distributions are independent and un-\ncorrelated. The distribution function in equation (7) is re-\nexpressed as the product of a 2-dimensional spatial Gaus-\nsian and a 3-dimensional color Gaussian, with parameter\nsets \u03b8s(j,t) and \u03b8\nc\n(j,t). Each pixel value is then expressed by\ncorresponding spatial vectors xst = [x, y]\nT , and color vector\nxct = [Y,U, V ]\nT . Hence, equation (7) becomes:\nCmap = argmaxj\n{\nlog(p(xst |\u03b8\ns\n(j,t)) + log(p(x\nc\nt |\u03b8\nc\n(j,t))\n}\n(8)\n4.1. Implementation Overview\nWe use model components to represent both background\nand foreground regions of the scene, under the premise that\nsuch a region is generated by a single corresponding pro-\ncess, such as part of an object. Background components are\ninitialized from the first image of a sequence, in which it is\nassumed no foreground objects appear. Foreground com-\nponents are introduced as required, in response to the ap-\npearance of pixel values which are not well represented by\nthe background. Each component is explicitly labeled as\nLc \u2208 {foreground, background}, and pixels are implicitly\nlabeled according to the component to which were assigned\nusing equation (8).\nThe current assignments are stored in an image \u201csup-\nport map\u201d. Figure 1 depicts a model instantiation corre-\nsponding to a frame from one of our test sequences. In this\nvisualization the components are represented by rendering\ntheir mean color value at each pixel where they are spa-\ntially dominant.\nAll model components are updated by the statistics of\ntheir assigned pixels. Background components are updated\nmore slowly than foreground components, reflecting the ex-\npectation that foreground will exhibit more dynamic be-\nhavior. The initialisation, assignments, and update proce-\ndures are described in more detail in the remainder of this\nsection.\n4.2. Building the Background Model\nThe initial set of background components are con-\nstructed from the first frame of the sequence. We have\nalready described how Greenspan\u2019s system [3] uses EM\nto build a maximum likelihood parameter set for a simi-\nlar Gaussian mixture. This technique is effective and well\nprincipled, but there are some problems using it for on-\nline processing. Firstly, a large number of iterations are\nrequired, making it computationally expensive. Secondly,\nwe wish to adapt the model dynamically, in response to\nchanges in scene structure. EM is proven to converge\nfor a fixed data set, however, our data set changes with\neach new input image: thus, for example, we may need to\nre-estimate the model order when new objects enter the\nscene. Greenspan deals with this by dividing the video into\nclosed sections, and building a separate model for each.\nHowever, this is not suitable for on-line processing.\nThe technique of splitting and merging components has\nbeen used by Raja [15] and by Pece [14] as a technique\nfor dynamically adapting model order. We use an iterative\nsplitting and merging technique to build an initial set of\nbackground components. We find that this method is com-\nputationally manageable, and, by minimizing the variance\nof components, generates an appropriate representation of\nthe major regions of the scene. We use the following proce-\ndure to build the components:\n1 The model is initialized with a single component, and\neach pixel\u2019s support map entry is set.\n2 It is iteratively split until a suitable number of new\ncomponents have been generated.\n3 Pairs of similar components are then merged.\n4 Any components which are spatially disconnected\n(possibly representing more than one object or pro-\ncess) are split.\n4.3. Splitting a Background Component\nA single iteration of the splitting procedure described in\nstep 2 divides an existing component into two new ones.\nGiven the background image, and current set of compo-\nnents, we first find the component with the highest spatial\nvariance.\nWe calculate the principle eigenvalue, \u03bbsj and correspond-\ning eigenvector, \u039bsj for each component\u2019s spatial covariance\nmatrix. The component Cssp = argmaxj\n{\n\u03bbsj\n}\nis selected.\nLet Isp be the pixels currently assigned to this compo-\nnent. If its eigenvalue \u03bbssp > T\ns\nsp, where T\ns\nsp is a predefined\nthreshold, then it is split. We create a new component and\nre-assign to it those pixels x \u2208 Isp which satisfy:\n(xs \u2212 \u00b5ssp) \u00b7 \u039b\ns\nsp > 0 (9)\nThis amounts to placing a separating plane through the\nspatial mean, perpendicular to \u039bssp. The parameters of both\ncomponents are then re-estimated from the statistics of\ntheir respective assigned pixels:\n5\nFig. 1. Background and Foreground Models. Left to right : Original image, background model, foreground model, Foreground pixel set. In\nthis visualization the model components are rendered with their mean color value, covering the area in which they are spatially dominant.\n\u03c9j =\nnj\nN\n(10)\n\u00b5j =\n1\nnj\n\u2211\nx\u2208Isp\nx (11)\nzj = \u00b5j\nT \u00b5j (12)\n\u03a3j =\n\u2211\nx\u2208Isp\nxTx\nnj\n\u2212 zj (13)\nWhere nj is the number of pixels assigned to the com-\nponent, and N is the total number of pixels in the image,\nand the value of \u00b5j used in equation (12) is the new value\ncalculated using equation (11). We then apply the same se-\nlection and splitting procedure in color space, using a cor-\nresponding threshold T csp, to split the component with the\nhighest color variance.\nWe repeat this process, alternating between highest spa-\ntial and color variances, until reaching a maximum number\nof components, or until the largest found eigenvalues fall\nbelow their thresholds. The procedure is initialized with\nthe single component built in step 1. The parameters of\nthis component are estimated from the entire set of image\npixels, using equations (10) to (13) with nj = N .\n4.4. Completing the Initial Background Model\nWe can now merge any similar components. If Msj(x\ns)\nis the spatial Mahalanobis distance of xs from \u00b5sj , and\nMcj(x\nc) is the color Mahalanobis distance of xc from \u00b5cj ,\nthen a pair of components is considered suitable for merg-\ning if the following holds:\nMs1(\u00b5\ns\n2) < T\ns\nmg \u2227 M\ns\n2(\u00b5\ns\n1) < T\ns\nmg \u2227\nMc1(\u00b5\nc\n2) < T\nc\nmg \u2227 M\nc\n2(\u00b5\nc\n1) < T\nc\nmg (14)\nWhere T smg and T\nc\nmg are predefined thresholds. We con-\nsider each pair of components, and merge the qualifying\npair with the lowest value ofmax(Mc1(\u00b5\nc\n2), M\nc\n2(\u00b5\nc\n1)). This\nprocedure is repeated until no qualifying pairs remain.\nWe next seek to identify components which represent\nspatially disconnected regions, and split them to represent\nthose regions separately. The purpose of this step is to\nidentify single components which represent more than one\nbackground process, and separate them. We order the com-\nponents in descending value of \u03bbsi , and step through the\nlist. For each, we use a connected components algorithm\nto determine if it represents two or more disconnected re-\ngions of the support map: if so, we split the largest region\naway from the rest as a new component. For reasons of ef-\nficiency we implement this at a reduced resolution. We re-\npeat this until no disconnected components are found, or\nfor a maximum number of iterations. Finally, when this\nprocess is complete, components which have a zero or very\nsmall weight are culled from the model.\n4.5. Assigning Image Pixels to Model Components\nWhen a new image frame is captured, each pixel is as-\nsigned to its most likely model component, using equation\n(8). The pixel\u2019s support map entry is updated to record the\nassignment. The spatial variance \u03a3si for large background\ncomponents is typically high. This frequently results in pix-\nels being assigned to regions from which they are signifi-\ncantly disconnected, and in particular, hampers detection\nof new foreground regions. To resolve this we apply the ad-\nditional restriction that a pixel may only be assigned to a\nbackground component if its spatial likelihood exceeds a\npredefined threshold T slik:\nlog(p(xs|\u03b8sj )) > T\ns\nlik (15)\nA minimum probability threshold Tmap is used to detect\nnew objects and processes in the scene. The pixel is labeled\n\u201cunassigned\u201d in the support map if:\nlog(p(xt|\u03b8Cmap)) < Tmap (16)\nWe implement Tmap by introducing a uniformly dis-\ntributed component into the model. The density of this\ncomponent is given by the extents of the feature space.\nFor a frame size of 720 \u00d7 576 and YUV components in the\nrange [0, 1]:\np(xt) =\n1\n720\u00d7 576\n(17)\nThis component has a fixed weight Wu, and pixels for\nwhich this is the most likely component are set as unas-\nsigned.\n6\nWe also make an important performance optimization to\nthe assignment process. If a pixel is currently assigned to an\nexisting component, its color value remains relatively un-\nchanged, and its probability given the same assignment is\ngreater than Tmap then we leave its assignment unchanged.\nThis significantly reduces processing time, as we do not\nneed to calculate the likelihood of each model component.\nA pixel value is defined as unchanged if each element of its\nYUV color value is within a threshold deviation from the\nvalue first used to assign it to the component. The result\nof this optimization is a significant increase in execution\nspeed (approximately 5\u00d7), and an increase in the perceived\nstability of the algorithm. We have experimented with ap-\nplying this optimization to all assigned pixels, and also to\nonly those pixels assigned to background components, with\nsimilar results.\n4.6. Introducing Foreground Components\nAll of the initial model components are labeled as back-\nground. Foreground components are introduced when re-\ngions of pixels appear which have a low probability under\nthe mixture model. Such regions are taken to be generated\nby new foreground objects entering the scene, and appear\nin the support map as regions in which a high density of\npixels have been labeled as unassigned by equation (16).\nThe support map is divided into a grid such that each\ncell has a resolution 16 \u00d7 16 pixels. The number of unas-\nsigned pixels is counted for each location. Locations ex-\nceeding a threshold density are considered to correspond to\nnew foreground regions. A single foreground component is\nbuilt from the statistics of all the unassigned pixels in these\nlocations. The parameters of the component are estimated\nusing equations of the form (10) to (13) to build the spatial\nand color distributions. This new component is then recur-\nsively split using the same procedure used to split back-\nground components. All new components introduced into\nthe model in this way are initially labeled as foreground.\n4.7. Updating the Model\nAfter pixel assignment, the parameters of the existing\nbackground and foreground components are re-estimated.\nFor foreground components, equations of the form (10) to\n(13) are used to calculate the spatial and color parameters\nfrom their assigned pixels.\nFor background components we adapt the parameters\nmore slowly. For each component j we start by calculating\na set of parameter values \u03b8(j,sm) from the new support map,\nin the same way as for foreground components. Given the\nprevious parameters \u03b8(j,t\u22121), we calculate the new set \u03b8(j,t)\nusing an adaptive learning rate:\n\u03b8(j,t) = \u03b1j\u03b8(j,sm) + (1\u2212 \u03b1j)\u03b8(j,t\u22121) (18)\nWhere \u03b1j is a vector of learning rates, one for each model\nparameter, modified by a variable factor \u03b1cj such that:\n\u03b1j = \u03b1\nc\nj\n[\n\u03b1s\u00b5, \u03b1\ns\n\u03a3, \u03b1\nc\n\u00b5, \u03b1\nc\n\u03a3\n]\n(19)\nWhere \u03b1s\u00b5, \u03b1\ns\n\u03a3, \u03b1\nc\n\u00b5, \u03b1\nc\n\u03a3 are constants used to update the\nspatial mean and covariance, and color mean and covari-\nance, respectively, and:\n\u03b1cj =\n\u03c9(j,sm)\n\u03c9(j,t\u22121)\n, \u03b1cj \u2208 [0, 1] (20)\nWhere \u03c9(j,sm) and \u03c9(j,t\u22121) are the weights from \u03b8(j,sm)\nand \u03b8(j,t\u22121) respectively. Using \u03b1\nc\nj to factor the adaptation\nin this way ensures that if a background component is oc-\ncluded it does not adapt too quickly to represent only the\nvisible part. It also helps to prevent the background from\nover adapting to misclassified foreground pixels. It is nec-\nessary to renormalize the component weights at this point,\nto enforce the condition in equation (2).\nRegardless of whether any new foreground components\nhave been added this frame, we test all foreground com-\nponents for possible merging. First, we restrict the spatial\nand color variances of each component to pre-defined max-\nimum values. This helps prevent over adaptation to mis-\nclassified background pixels. We then merge similar com-\nponents using the same pair-wise method as was used for\nthe background model. We also examine foreground com-\nponents for fragmentation, using a similar process to that\nused for detection and splitting of disconnected background\ncomponents. This helps to maintain a one-to-one correla-\ntion between components and object processes. Finally, we\nconclude frame processing by culling any foreground com-\nponents which have a zero or very low weight.\n4.8. Reclassifying Components\nAll components introduced after model initialisation are\nclassified as foreground. However, occasionally, a new fore-\nground component will be introduced which does not cor-\nrespond to foreground component, but to a change in the\nbackground process (for example, an illumination change).\nIn such cases, the component classification is erroneous,\nand needs to be corrected.\nWe expect that foreground components will represent\nobjects that are moving through the scene when they are\nfirst detected. Background components represent processes\nwhich may show some movement around an a mean posi-\ntion, but are relatively static.\nWe use this feature to detect inappropriately classified\nforeground components. In order to retain its classification,\nwe impose the condition that a foreground component must\nexhibit a significant spatial translation immediately after\ninstantiation. We implement this using two thresholds T sfg\nand T tfg such that if the the following condition is not satis-\nfied, a foreground component is reclassified as background:\n|\u00b5(j,t=0) \u2212 \u00b5(j,t=T t\nfg\n)| \u2265 T\ns\nfg (21)\nWhere t = 0 corresponds to the image frame at which the\ncomponent was created. The values used for T sfg and T\nt\nfg\n7\nare contextualized and reflect our expectations about fore-\nground object behavior. Thus, unlike per-pixel methods,\nwe can define the difference between foreground and back-\nground as an object-level attribute. We have experimented\nwith various values of T sfg and T\nt\nfg in our experiments.\n4.9. Frame to Frame Object Correspondence\nMany systems (for example, [9]) implement frame-to-\nframe foreground object correspondence as a separate\nhigher level process. In our system, correspondence is\nintegrated with the model update process.\nAssuming that foreground object movements from one\nframe to the next are relatively small, pixel values gener-\nated by a moving foreground object at time t will generally\nbe assigned to the corresponding component \u03b8(j,t\u22121). Al-\nthough spatial translation of the object decreases the com-\nponent likelihood, the color likelihood will remain high.\nThus object pixels are repeatedly re-assigned to the cor-\nresponding component(s), and the component parameters\nare re-estimated, such that the spatial mean of the compo-\nnent tracks the moving object.\nLarge object translations, or nearby similarly colored\nbackground components may cause pixels to be incorrectly\nassigned to a different component. In this case the existing\ncorresponding foreground component will be extinguished,\nand a new component re-introduced automatically: the seg-\nmentation process is still effective, but correspondence is\nlost.\n5. Experiments\nWe have performed a series of experiments to compare\nthe segmentation quality of our model with Stauffer\u2019s per-\npixel algorithm [23], and with Migdal\u2019s MRF based scheme\n[10].We are particularly interested in the ability of our algo-\nrithm to extract foreground objects where the scene back-\nground exhibits unpredictable changes in spatial structure;\nhowever, we are also interested in general performance. We\nhave therefore conducted evaluations against two separate\ndata sets.\nThe first set comprises eleven video sequences filmed\nmainly in indoor environments in which the background is\nstatic: the only variations arise from slight changes in light-\ning conditions. Most sequences comprise one or two human\ntargets performing routine actions such as walking and sit-\nting down.\nThe second set comprises ten sequences which are more\nchallenging. These have been filmed in outdoor sequences\nin which there is significant, and sometimes large, move-\nment in the background. Some backgrounds comprise back-\nground foliage which are moved by wind. Others contain\nmoving water. In most sequences the foreground target is\nhuman, though we have also included a sequence in which\nthe target is a car, and in another, a moving bird. Some\nexample frames from sequences in the second data set are\nshown in the left hand column of figure 2.\nAll sequences were filmed using a standard consumer DV\ncamcorder producing a PAL format video stream (720 \u00d7\n576 pixel frame size, at 25 Hz, interlaced). The captured\nsequences were re-sampled to a frequency of 10Hz (by omit-\nting frames), and a simple de-interlacing algorithm was ap-\nplied. The duration of the processed sequences ranged be-\ntween 50 and 500 frames. For each of the algorithms we\nused parameter ranges which ensured that the background\nmodel was learned robustly well within the minimum se-\nquence duration (see section 5.1).\n5.1. Quantifying Performance\nIn order to quantify the performance of the three algo-\nrithms we constructed a set of \u201cground truth\u201d frames for\neach of the sequences. From each we arbitrarily selected a\nsample of sixteen frames: to avoid bias, the frames were se-\nlected without prior inspection. We also avoided selecting\nframes from the beginning each sequence, so that the al-\ngorithms were able to properly initialise their background\nmodels. These frames were then copied, and the copies\nmanually annotated by marking the foreground pixels as\npure red (RGB 255,0,0) in an image editing program. These\nannotated frames are considered to represent a \u201ccorrect\u201d\nground truth segmentation of the corresponding frames.\nFor each of the three algorithms we performed the fol-\nlowing automated procedure. The algorithm was run over\neach sequence, generating a set of foreground pixels for each\nframe. For our algorithm, this corresponds to the set of pix-\nels assigned to foreground components. For frames which\nhave a ground truth, the algorithm output was compared to\nthe manual segmentation. A pixel classified as foreground\nby both the algorithm and the annotation is denoted \u201ctrue\npositive\u201d (TP) foreground. If it classified as foreground by\nonly the algorithm, it is considered \u201cfalse positive\u201d (FP).\nFinally, if it classified as foreground by only the annota-\ntion then it is considered \u201cfalse negative\u201d (FN). The total\nnumber of TP, FP, and FN pixels is summed for for each\nsequence, resulting totals for each algorithm against each\nsequence. We avoided choosing ground truth frames near\nthe beginning of each sequence, so that the algorithms were\nable to initialize properly before we examined their output.\nWe use the TP, FP, and FN values for each sequence\n(and summed for all sequences in each data set) to con-\nstruct two different comparison metrics. A range of metrics\nis presented by Rosin [17], including Jaccard coefficient Jc,\nwhere:\nJc =\nTP\n(TP + FP + FN)\n(22)\nThis metric was also used by Migdal [10], and so we use\nit to represent our results. For our second metric we use the\ntotal error, Etot, used by Toyama [25], where:\nEtot = FP + FN (23)\n8\nParameter Value\nMax. Background Components 1000\nMax. Foreground Components 300\nBackground split (spatial) T ssp 800\nBackground split (color) T csp 50\nBackground merge (spatial) T smg 2\nBackground merge (color) T cmg 1\nBackground update rate (Color) \u03b1c\u00b5 0.05\nBackground update rate (Spatial) \u03b1s\u00b5 0.0 (no update)\nUniform Component Weight Wu 0.1\nForeground reclassification time T t\nfg\n2.0s\nForeground reclassification distance T s\nfg\n48 pixels\nTable 1\nMost Effective Parameter Values for our Algorithm (Data Set 2)\nParameter Value\nNumber of components per pixel 3\nLearning rate 0.02\nMatch threshold 3.0\nTable 2\nMost Effective Parameter Values for Stauffer\u2019s Algorithm (Data Set\n2)\nParameter Value\nLearning rate (component weight) 0.005\nLearning rate (component parameters) 0.05\nStart temperature 1.0\nEnd temperature 0.2\nTable 3\nMost Effective Parameter Values for Migdal\u2019s Algorithm (Data Set\n2)\nWe repeated our experiments with a range of parameters\nfor each algorithm. In the case of Migdal\u2019s algorithm we\nexperimented with both the proposed linear cooling sched-\nule, and an exponential schedule more commonly used to\nestimate MAP-MRF field labelings. For our algorithm, the\nparameters which effect performance were found to be the\nnumber of components used for the model, the model up-\ndate rate, and the parameters used to reclassify foreground\ncomponents as background. A summary of the best param-\neter values found for the second data set is shown in table\n1. We compared results using this parameter set against\nthe best parameters found for both Stauffer\u2019s and Migdal\u2019s\nalgorithms. The best parameters for these are shown in ta-\nbles 2 and 3 respectively.\n5.2. Results\nResults for the first data set, with static backgrounds, are\nshown in table 4. These table shows Jaccard coefficients and\ntotal errors for each sequence, and totals for the whole set.\nBoth metrics indicate that the MRF segmentation gives a\nJaccard Coefficient Total Errors (\u00d7103)\nSequence Stauffer Migdal Ours Stauffer Migdal Ours\n1 0.54 0.83 0.68 244 76 196\n2 0.63 0.84 0.72 333 109 174\n3 0.48 0.84 0.73 354 79 147\n4 0.19 0.31 0.32 374 577 386\n5 0.54 0.62 0.52 237 202 164\n6 0.53 0.78 0.65 98 38 52\n7 0.39 0.48 0.45 194 182 150\n8 0.23 0.63 0.56 890 221 313\n9 0.65 0.70 0.60 434 350 264\n10 0.68 0.84 0.79 162 59 122\n11 0.36 0.52 0.44 194 165 187\ntotal 0.47 0.66 0.59 3515 2060 2154\nTable 4\nJaccard Coefficients and Total errors (\u00d7103) for Scenes with Static\nBackgrounds (Data Set 1)\nbetter overall performance than our algorithm, with both\ngiving much better results than Stauffer\u2019s model. Migdal\u2019s\nscheme has the highest Jaccard coefficient for ten of the\neleven sequences, and the lowest error rate in seven cases.\nOur algorithm has the highest Jaccard coefficient for one\nsequence, and the lowest error rate in three sequences.\nAlthough Migdal\u2019s algorithm gives a better performance\non the first data set, results for the second data set, with\ndynamic backgrounds, are very different. These are shown\nin table 5. On this data set, our algorithm outperforms the\nMRF and per-pixel models by a considerable margin, with\nthe highest Jaccard coefficient in nine out of ten cases, and\na lower error rate in all cases. To visualize, Figure 2 shows\nexample image frames and segmentations from six of the\nten sequences. In each case, the original frame is shown\nin the left hand column, the per-pixel segmentation in the\nnext column, the MRF segmentation in the next, and the\nresult for our algorithm in the right hand column. In all\ncases, the background exhibits significant spatio-temporal\nvariation during the sequence.\nComparing the results for the two data sets, we see that\nvariations in the background, caused by moving foliage or\nwater, drastically reduce the effectiveness of both the MRF\nand per-pixel models whilst our algorithm remains robust.\nThe total Jaccard coefficient for the MRF scheme reduces\nfrom 0.66 for the first data set to only 0.15 for the second.\nFor Stauffer\u2019s algorithm, it is reduced from 0.44 to 0.07. In\nboth cases, the segmentations for the second data set are\npoor. For our algorithm, performance drops only a little,\nfrom 0.59 to 0.52. Based on these results we assert that our\nalgorithm gives a better segmentation than either Stauffer\u2019s\nor Migdal\u2019s in scenes with significant levels of background\nmovement.\n9\nFig. 2. Segmented foreground pixels. Left to right : Original image, Stauffer, Migdal, Our algorithm\n10\nJaccard Coefficient Total Errors (\u00d7103)\nSequence Stauffer Migdal Ours Stauffer Migdal Ours\n1 0.10 0.15 0.58 1322 1100 109\n2 0.04 0.04 0.52 1642 1529 77\n3 0.14 0.52 0.34 546 131 127\n4 0.01 0.23 0.33 964 567 14\n5 0.07 0.21 0.58 886 250 40\n6 0.05 0.08 0.61 1146 797 46\n7 0.14 0.25 0.58 1237 671 181\n8 0.09 0.28 0.45 1000 348 118\n9 0.02 0.01 0.46 981 382 187\n10 0.09 0.21 0.53 933 328 95\ntotal 0.07 0.15 0.52 10657 5594 827\nTable 5\nJaccard Coefficients and Total errors (\u00d7103) for Scenes with Dynamic\nBackgrounds (Data Set 2)\n5.3. Significance of the Results\nIn this section we verify that the differences in perfor-\nmance apparent in the previous section are statistically sig-\nnificant.\nWe first consider the results of our algorithm against\nStauffer\u2019s on the second data set. Our hypothesis is that our\nalgorithm gives a better segmentation (referenced against\nthe ground truth) than Stauffer\u2019s, on sequences with a high\ndegree of background movement. Our null hypothesis, H10 ,\nis that there is at least a 0.5 probability that Stauffer\u2019s algo-\nrithm will return a higher Jaccard coefficient than ours on\nan image sequence. This hypothesis asserts that, using the\nJaccard coefficient as a performance metric, Stauffer\u2019s al-\ngorithm will out-perform ours on average. We consider the\nresult on each sequence as a Bernoulli trial with p = 0.5\nand use the binomial distribution to determine an appro-\npriate p-value. In this case our algorithm returns a higher\nJaccard coefficient for all ten sequences, resulting in a p-\nvalue of 0.510 \u2248 0.001. We can therefore reject H10 at the\nstandard 5% significance level. Similarly, we reject the null\nhypothesis regarding total errors, H20 , that there is at least\na 0.5 probability that Stauffer\u2019s algorithm will return a lower\ntotal error than ours, at the same significance level.\nApplying the same procedure to the results against\nMigdal\u2019s algorithm, we construct two similar null hypothe-\nses H30 and H\n4\n0 . Our algorithm returns a higher Jaccard\ncoefficient than Migdal\u2019s in all but one case. For H30 this\nequates to a p-value of 11 \u00d7 0.510 \u2248 0.011. Again, this is\nwell within than standard 5% significance level, and we can\nreject H30 . Our algorithm returns a lower total error than\nMigdal\u2019s in all cases, and so, as with H20 , we can reject H\n4\n0\nat the 5% significance level.\nWe proceed to consider the significance of our results\nagainst data set 1, shown in table 4. In this dataset, our al-\ngorithm gave a higher Jaccard coefficient that Stauffer\u2019s in\nnine out of 11 sequences, and a lower error rate in ten cases.\nAlgorithm Approx. frame processing time (s)\nStauffer 0.1\nMigdal 5.0\nOur algorithm 2.0 - 4.0\nTable 6\nApproximate image frame processing times, in seconds\nFollowing the same procedure as for data set 2, we propose\nthe same null hypotheses for data set 1: H50 and H\n6\n0 which\ncorrespond to H10 and H\n2\n0 respectively. Corresponding p-\nvalues are 67 \u00d7 0.511 \u2248 0.033 and 12 \u00d7 0.511 \u2248 0.006, and\nwe can reject both at the 5% significance level.\nWe use a different treatment for our results against\nMigdal\u2019s algorithm on data set 1, since Migdal\u2019s algo-\nrithm returns a better Jaccard coefficient in ten cases,\nand lower error rate in 7 cases. We therefore propose al-\nternative null hypotheses to validate the superiority of\nMigdal\u2019s algorithm on sequences with static backgrounds,\ndenoted H70 and H\n8\n0 respectively: there is at least a 0.5\nprobability that our algorithm will return a higher Jaccard\ncoefficient than Migdal\u2019s; there is at least a 0.5 probabil-\nity that our algorithm will return a lower total error than\nMigdal\u2019s. Corresponding p-values are 12 \u00d7 0.511 \u2248 0.001\nand 552 \u00d7 0.511 \u2248 0.27. We therefore reject H70 at the 5%\nsignificance level, and conclude that Migdal\u2019s algorithm\nreturns a better Jaccard coefficient for scenes with static\nbackgrounds. However, we are unable to reject H80 , so our\ncomparison of error rates is inconclusive in this case.\nWe have thus shown that our results are statistically sig-\nnificant except in one case: the comparison of total error\nrates between our algorithm and Migdal\u2019s on data set 1,\nsupporting our assertions in the previous section.\n5.4. Frame Processing Time\nAll three algorithms were coded in unoptimized C++\nand our experiments were run on a 2.8 GHz Pentium 4\nbased PC. As previously mentioned, the processed frame\nsize is 720\u00d7 576. The execution times per frame are shown\nin table 6. From this table it can be seen that Stauffer\u2019s al-\ngorithm is considerably faster than ours or Migdal\u2019s, trad-\ning segmentation accuracy for speed. Migdal\u2019s algorithm is\nthe slowest, whereas execution time for our algorithm varies\ndepending on the number of components and the relative\nchange in scene structure between frames.\nThe results reported in table 6 suggest that, whilst our al-\ngorithm is significantly faster than Migdal\u2019s MRF scheme,\nneither scheme is yet capable of real-time performance. In\ncomparison with Stauffer\u2019s algorithm it is clear that there\nis a significant computational cost to enforcing spatial co-\nherency, using either method. However, we believe that\nthere is considerable scope for improving the frame execu-\ntion times that we have reported: firstly through optimiza-\ntion of our C++ implementation, and secondly by the use\nof specialized hardware.\n11\n6. Conclusions\nThe processes observed in real-world scenes are com-\nplex and chaotic, and often make accurate background sub-\ntraction a challenging task. We have considered the prob-\nlem of robust segmentation in scenes with dynamic back-\ngrounds: where objects in the background are subject to\nspatial variations. Per-pixel background models are unable\nto effectively represent such variations, leading to frequent\nmis-classifications. Random field models are able to impose\nglobal spatial and temporal dependencies on field label-\nings, but do not explicitly model the spatial structure of\nthe scene.\nWe have proposed a scheme in which homogeneous re-\ngions of the scene are modeled by an adaptive mixture of\nGaussians in color and space. Components of themodel rep-\nresent clusters of pixels generated by discreet processes or\nobjects in both the background and foreground.We use this\nmodel to probabilistically classify new pixel observations,\nand remove misclassifications caused by spatio-temporal\nbackground variations.\nWe conducted a series of experiments to investigate the\neffectiveness of our model, and compare its performance\nto per-pixel and MRF-based algorithms. We tested the al-\ngorithms on two data sets, the first comprising sequences\nwith static backgrounds, and the second comprising scenes\nin which the background exhibited significant structural\nvariations.\nOur results show that whilst all three models are able\nto produce effective segmentations when the background is\nstatic, the output of the per-pixel and MRF based models\nis severely degraded by background variations. Our model\nis robust, however, and the segmentation quality was only\nmarginally reduced. Having demonstrated the statistical\nsignificance of our results, there is strong evidence that ex-\nplicitly modeling spatial features of the background results\nin a more robust segmentation.\nThere is a trade off between accuracy and processing\ntime. A more complex model requires more processing, and\nboth our system and the MRF-based system we tested were\nan order of magnitude slower than the per-pixel model. This\nis an issue for real-time monitoring systems; however, we\nbelieve that there is scope for speed optimizations, which\nwould make our system a viable proposition for real-time\nimplementation.\n7. Acknowledgments\nThe work presented in this article was supported by an\nEPSRC CASE studentship (reference GP-P04329-01), in\nconjunction with Nectar Electronics Ltd. UK. The authors\nwould like to thank Ray Broadbridge of Nectar Electronics\nfor contributions in supporting this work.\nReferences\n[1] J. Cheng, J. Yang, Y. Zhou, and Y. Cui. Flexible background\nmixture models for foreground segmentation. Image and Vision\nComputing, 24(5):473\u2013482, 2006.\n[2] A. Elgammal, R. Duraiswami, D. Harwood, and L. Davis.\nBackground and foreground modeling using nonparametric\nkernel density estimation for visual surveillance. Proceedings of\nthe IEEE, 90(7):1151\u20131163, July 2002.\n[3] H. Greenspan, J. Goldberger, and A. Mayer. Probabilistic\nspacetime video modeling via piecewise GMM. IEEE\nTransactions on Pattern Analysis and Machine Intelligence,\n26(3):384\u2013396, 2004.\n[4] I. Haritaoglu, D. Harwood, and L. Davis. W4: Who? when?\nwhere? what? A real time system for detecting and tracking\npeople. In Proc. of International Conference on Automatic Face\nand Gesture Recognition, pages 222\u2013227, Nara, Japan, 1998.\n[5] M. Harville, G. Gordon, and J. Woodfill. Foreground\nsegmentation using adaptive mixture models in color and depth.\nIn Proc. of IEEE Workshop on Detection and Recognition of\nEvents in Video, pages 3\u201311, Vancouver, Canada, July 2001.\n[6] B. Heisele. Motion-based object detection and tracking in color\nimage sequences. In Proc. of Asian Conference on Computer\nVision, pages 1028\u20131033, Taipei, Taiwan, 2000.\n[7] C. Hua, H. Wu, Q. Chen, and T. Wada. A pixel-wise object\ntracking algorithm with target and background sample. In Proc.\nof International Conference on Pattern Recognition, volume 1,\npages 739\u2013742, Hong Kong, China, 2006.\n[8] P. KaewTraKulPong and R. Bowden. An improved adaptive\nbackground mixture model for real-time tracking with shadow\ndetection. In In Proc. European Workshop on Advanced Video\nBased Surveillance Systems, Kingston, UK, September 2001.\n[9] S. McKenna, S. Jabri, Z. Duric, A. Rosenfeld, and H. Wechsler.\nTracking groups of people. Computer Vision and Image\nUnderstanding, 80(1):42\u201356, 2000.\n[10] J. Migdal and W. Grimson. Background subtraction using\nMarkov thresholds. In Proc. of IEEE Workshop on Applications\nof Computer Vision \/ IEEE Workshop on Motion and Video\nComputing, volume 2, pages 58\u201365, Breckenridge, CO, USA,\nJanuary 2005.\n[11] A. Monnet, A. Mittal, N. Paragios, and V. Ramesh. Background\nmodeling and subtraction of dynamic scenes. In Proc. of IEEE\nInternational Conference on Computer Vision, volume 2, pages\n1305\u20131312, Nice, France, 2003.\n[12] N. Oliver, B. Rosario, and A. Pentland. A Bayesian\ncomputer vision system for modeling human interactions. IEEE\nTransactions on Pattern Analysis and Machine Intelligence,\n22(8):831\u2013843, 2000.\n[13] N. Paragios and V. Ramesh. A MRF-based approach for real-\ntime subway monitoring. In Proc. of IEEE Conference on\nComputer Vision and Pattern Recognition, volume 1, pages\n1034\u20131040, Hawaii, USA, 2001.\n[14] A. Pece. Tracking by cluster analysis of image differences.\nIn Proc. of International Symposium on Intelligent Robotic\nSystems, pages 295\u2013303, Reading, UK, 2000.\n[15] Y. Raja, S. J. Mckenna, and S. Gong. Color model selection\nand adaptation in dynamic scenes. In Proc. of European\nConference on Computer Vision, volume 1, pages 460\u2013474,\nFreiberg, Germany, 1998.\n[16] Y. Ren, C. Chua, and Y. Ho. Motion detection with\nnonstationary background. Machine Vision and Applications,\n13(5):332\u2013343, 2003.\n[17] P. Rosin and E. Ioannidis. Evaluation of global image\nthresholding for change detection. Pattern Recognition Letters,\n24(14):2345\u20132356, 2003.\n[18] K. Schindler and H. Wang. Smooth foreground-background\nsegmentation for video processing. In Proc. of Asian Conference\n12\non Computer Vision, volume 2, pages 581\u2013590, Hyderabad,\nIndia, 2006.\n[19] M. Seki, T. Wada, H. Fujiwara, and K. Sumi. Background\nsubtraction based on cooccurrence of image variations. In\nProc. of IEEE Conference on Computer Vision and Pattern\nRecognition, volume 2, pages 65\u201372, Madison, WI, USA, 2003.\n[20] Y. Sheikh and M. Shah. Bayesian modeling of dynamic scenes\nfor object detection. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 27(11):1778\u20131792, 2005.\n[21] A. Shimada, D. Arita, and R Taniguchi. Dynamic control\nof adaptive mixture-of-Gaussians background model. In Proc.\nof IEEE International Conference on Video and Signal Based\nSurveillance, Sydney, Australia, 2006.\n[22] P. Spagnolo, T. D\u2019Orazio, M. Leo, and A. Distante. Moving\nobject segmentation by background subtraction and temporal\nanalysis. Image and Vision Computing, 24(5):411\u2013423, 2006.\n[23] C. Stauffer and W.Grimson. Adaptive background mixture\nmodels for real-time tracking. In Proc. of IEEE Conference\non Computer Vision and Pattern Recognition, volume 2, pages\n246\u2013252, Fort Collins, CO, USA, 1999.\n[24] Y. Tian, M. Lu, and A. Hampapur. Robust and efficient\nforeground analysis for real-time video surveillance. In Proc. of\nIEEE Conference on Computer Vision and Pattern Recognition,\nvolume 1, pages 1182\u20131187, San Diego, CA, USA, 2005.\n[25] K. Toyama, J. Krumm, B. Brumitt, and B. Meyers. Wallflower:\nPrinciples and practice of background maintenance. In Proc. of\nInternational Conference on Computer Vision, volume 1, pages\n255\u2013261, Corfu, Greece, 1999.\n[26] Y. Wang, K. Loe, T. Tan, and J. Wu. A dynamic hidden Markov\nrandom field model for foreground and shadow segmentation.\nIn Proc. of IEEE Workshop on Motion and Video Computing,\npages 474\u2013480, Breckenridge, CO, USA, 2005.\n[27] Y. Wang, K. Loe, and J. Wu. A dynamic conditional random\nfield model for foreground and shadow segmentation. IEEE\nTransactions on Pattern Analysis and Machine Intelligence,\n28(2):279\u2013289, 2006.\n[28] C. Wren, A. Azarbayejani, T. Darrell, and A. Pentland. Pfinder:\nReal-time tracking of the human body. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 19(7):780\u2013785, 1997.\n13\n"}