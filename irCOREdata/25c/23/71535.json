{"doi":"10.1111\/j.1467-9868.2007.00601.x","coreId":"71535","oai":"oai:eprints.lancs.ac.uk:745","identifiers":["oai:eprints.lancs.ac.uk:745","10.1111\/j.1467-9868.2007.00601.x"],"title":"Online Inference for Multiple Changepoint Problems.","authors":["Fearnhead, P","Liu, Z"],"enrichments":{"references":[{"id":16345191,"title":"A Bayesian changepoint analysis of electromyographic data: detecting muscle activation patterns and associated applications.","authors":[],"date":"2003","doi":"10.1093\/biostatistics\/4.1.143","raw":"Johnson, T. D., Elasho\ufb00, R. M., and Harkema, S. J. (2003). A Bayesian changepoint analysis of electromyographic data: detecting muscle activation patterns and associated applications. Biostatistics, 4:143\u2013164.","cites":null},{"id":16345174,"title":"A sequential particle \ufb01lter method for static models.","authors":[],"date":"2002","doi":"10.1093\/biomet\/89.3.539","raw":"Chopin, N. (2002). A sequential particle \ufb01lter method for static models. Biometrika, 89:539\u2013551.","cites":null},{"id":16345195,"title":"A theoretical framework for sequential importance sampling with resampling.","authors":[],"date":"2001","doi":"10.1007\/978-1-4757-3437-9_11","raw":"Liu, J. S., Chen, R., and Logvinenko, T. (2001). A theoretical framework for sequential importance sampling with resampling. In Doucet, A., de Freitas, N., and gordon, N., editors, Sequential Monte Carlo Methods in Practice, pages 225\u2013246. Springer\u2013Verlag; New York.","cites":null},{"id":16345171,"title":"An improved particle \ufb01lter for non-linear problems.","authors":[],"date":"1999","doi":"10.1049\/ip-rsn:19990255","raw":"Carpenter, J., Cli\ufb00ord, P., and Fearnhead, P. (1999). An improved particle \ufb01lter for non-linear problems. IEE proceedings-Radar, Sonar and Navigation, 146:2\u20137.","cites":null},{"id":16345205,"title":"Bayesian curve \ufb01tting using MCMC with applications to signal segmentation.","authors":[],"date":"2002","doi":"10.1109\/78.984776","raw":"Punskaya, E., Andrieu, C., Doucet, A., and Fitzgerald, W. J. (2002). Bayesian curve \ufb01tting using MCMC with applications to signal segmentation. IEEE Transactions on Signal Processing, 50:747\u2013758.","cites":null},{"id":16345199,"title":"Bayesian inference on biopolymer models.","authors":[],"date":"1999","doi":"10.1093\/bioinformatics\/15.1.38","raw":"Liu, J. S. and Lawrence, C. E. (1999). Bayesian inference on biopolymer models. Bioinformatics, 15:38\u201352.","cites":null},{"id":16345209,"title":"Bayesian retrospective multiple-changepoint identi\ufb01cation. Applied Statistics,","authors":[],"date":"1994","doi":"10.2307\/2986119","raw":"Stephens, D. A. (1994). Bayesian retrospective multiple-changepoint identi\ufb01cation. Applied Statistics, 43:159\u2013178.24","cites":null},{"id":16345189,"title":"Covariation in frequencies of substitution,23 deletion, transposition, and recombination during eutherian evolution.","authors":[],"date":"2003","doi":"10.1101\/gr.844103","raw":"Hardison, R. C., Roskin, K. M., Yanf, S., Diekhans, M., Kent, W. J., Weber, R., Elnitski, L., and Li et al., J. (2003). Covariation in frequencies of substitution,23 deletion, transposition, and recombination during eutherian evolution. Genome Research, 13:13\u201326.","cites":null},{"id":16345207,"title":"Detection of onset of neuronal activity by allowing for heterogeneity in the change points.","authors":[],"date":"2002","doi":"10.1016\/s0165-0270(02)00275-3","raw":"Ritov, Y., Raz, A., and Bergman, H. (2002). Detection of onset of neuronal activity by allowing for heterogeneity in the change points. Journal of Neuroscience Methods, 122:25\u201342.","cites":null},{"id":16345201,"title":"Detection of undocumented changepoints: A revision of the two-phase regression model.","authors":[],"date":"2002","doi":"10.1175\/1520-0442(2002)015<2547:doucar>2.0.co;2","raw":"Lund, R. and Reeves, J. (2002). Detection of undocumented changepoints: A revision of the two-phase regression model. Journal of Climate, 15:2547\u20132554.","cites":null},{"id":16345176,"title":"Dynamic detection of change points in long time series.","authors":[],"date":"2006","doi":"10.1007\/s10463-006-0053-9","raw":"Chopin, N. (2006). Dynamic detection of change points in long time series. Annals of the Institute of Statistical Mathematics, page to appear.","cites":null},{"id":16345173,"title":"Estimation and comparison of multiple change-point models.","authors":[],"date":"1998","doi":"10.1016\/s0304-4076(97)00115-2","raw":"Chib, S. (1998). Estimation and comparison of multiple change-point models. Journal of Econometrics, 86:221\u2013241.","cites":null},{"id":16345183,"title":"Exact and e\ufb03cient inference for multiple changepoint problems.","authors":[],"date":"2006","doi":"10.1007\/s11222-006-8450-8","raw":"Fearnhead, P. (2006). Exact and e\ufb03cient inference for multiple changepoint problems. Statistics and Computing, 16:203\u2013213.","cites":null},{"id":16345181,"title":"Exact Bayesian curve \ufb01tting and signal segmentation.","authors":[],"date":"2005","doi":"10.1109\/tsp.2005.847844","raw":"Fearnhead, P. (2005). Exact Bayesian curve \ufb01tting and signal segmentation. IEEE Transactions on Signal Processing, 53:2160\u20132166.","cites":null},{"id":16345179,"title":"Ideal spatial adaptation by wavelet shrinkage.","authors":[],"date":"1994","doi":"10.1093\/biomet\/81.3.425","raw":"Donoho, D. L. and Johnstone, I. M. (1994). Ideal spatial adaptation by wavelet shrinkage. Biometrika, 81:425\u2013455.","cites":null},{"id":16345169,"title":"Isochores and evolutionary genomics of vertebrates.","authors":[],"date":"2000","doi":"10.1016\/s0378-1119(99)00485-0","raw":"Bernardi, G. (2000). Isochores and evolutionary genomics of vertebrates. Gene, 241:3\u201317.","cites":null},{"id":16345203,"title":"IsoFinder: computational prediction of isochores in genome sequences. Nuceleic Acids Research, 32:W287\u2013W292. Web Server Issue.","authors":[],"date":"2004","doi":"10.1093\/nar\/gkh399","raw":"Oliver, J. L., Carpena, P., Hackenberg, M., and Bernaola-Galvan, P. (2004). IsoFinder: computational prediction of isochores in genome sequences. Nuceleic Acids Research, 32:W287\u2013W292. Web Server Issue.","cites":null},{"id":16345172,"title":"Mixture Kalman \ufb01lters.","authors":[],"date":"2000","doi":"10.1111\/1467-9868.00246","raw":"Chen, R. and Liu, J. (2000). Mixture Kalman \ufb01lters. Journal of the Royal Statistical Society, Series B, 62:493\u2013508.","cites":null},{"id":16345170,"title":"Multiple changepoint \ufb01tting via quasilikelihood, with application to DNA sequence segmentation.","authors":[],"date":"2000","doi":"10.1093\/biomet\/87.2.301","raw":"Braun, J. V., Braun, R. K., and Muller, H. G. (2000). Multiple changepoint \ufb01tting via quasilikelihood, with application to DNA sequence segmentation. Biometrika, 87:301\u2013314.22 Braun, J. V. and Muller, H. G. (1998). Statistical methods for DNA sequence segmentation. Statistical Science, 13:142\u2013162.","cites":null},{"id":16345185,"title":"Online inference for hidden Markov models.","authors":[],"date":"2003","doi":"10.1111\/1467-9868.00421","raw":"Fearnhead, P. and Cli\ufb00ord, P. (2003). Online inference for hidden Markov models. Journal of the Royal Statistical Society, Series B, 65:887\u2013899.","cites":null},{"id":16345168,"title":"Product partition models for change point problems. The Annals of Statistics,","authors":[],"date":"1992","doi":"10.1214\/aos\/1176348521","raw":"Barry, D. and Hartigan, J. A. (1992). Product partition models for change point problems. The Annals of Statistics, 20:260\u2013279.","cites":null},{"id":16345197,"title":"Rejection control and sequential importance sampling.","authors":[],"date":"1998","doi":"10.2307\/2669846","raw":"Liu, J. S., Chen, R., and Wong, W. H. (1998). Rejection control and sequential importance sampling. Journal of the American Statistical Society, 93:1022\u20131031.","cites":null},{"id":16345187,"title":"Reversible jump Markov chain Monte Carlo computation and Bayesian model determination.","authors":[],"date":"1995","doi":"10.2307\/2337340","raw":"Green, P. (1995). Reversible jump Markov chain Monte Carlo computation and Bayesian model determination. Biometrika, 82:711\u2013732.","cites":null},{"id":16345193,"title":"Sequential Monte Carlo methods for dynamic systems.","authors":[],"date":"1998","doi":"10.2307\/2669847","raw":"Liu, J. S. and Chen, R. (1998). Sequential Monte Carlo methods for dynamic systems. Journal of the American Statistical Association., 93:1032\u20131044.","cites":null},{"id":16345177,"title":"Sequential Monte Carlo samplers.","authors":[],"date":"2006","doi":"10.1111\/j.1467-9868.2006.00553.x","raw":"Del Moral, P., Doucet, A., and Jasra, A. (2006). Sequential Monte Carlo samplers. Journal of the Royal Statistical Society, Series B, 68:411\u2013436.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2007","abstract":"We propose an on-line algorithm for exact filtering of multiple changepoint problems. This algorithm enables simulation from the true joint posterior distribution of the number and position of the changepoints for a class of changepoint models. The computational cost of this exact algorithm is quadratic in the number of observations. We further show how resampling ideas from particle filters can be used to reduce the computational cost to linear in the number of observations, at the expense of introducing small errors; and propose two new, optimum resampling algorithms for this problem. One, a version of rejection control, allows the particle filter to automatically choose the number of particles required at each time-step. The new resampling algorithms substantially out-perform standard resampling algorithms on examples we consider; and we demonstrate how the resulting particle filter is practicable for segmentation of human GC content","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/71535.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/745\/1\/online_chpt4.pdf","pdfHashValue":"688581bc42b30458f8ac951ea31d6fe5e4df164a","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:745<\/identifier><datestamp>\n      2018-01-24T03:16:42Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Online Inference for Multiple Changepoint Problems.<\/dc:title><dc:creator>\n        Fearnhead, P<\/dc:creator><dc:creator>\n        Liu, Z<\/dc:creator><dc:description>\n        We propose an on-line algorithm for exact filtering of multiple changepoint problems. This algorithm enables simulation from the true joint posterior distribution of the number and position of the changepoints for a class of changepoint models. The computational cost of this exact algorithm is quadratic in the number of observations. We further show how resampling ideas from particle filters can be used to reduce the computational cost to linear in the number of observations, at the expense of introducing small errors; and propose two new, optimum resampling algorithms for this problem. One, a version of rejection control, allows the particle filter to automatically choose the number of particles required at each time-step. The new resampling algorithms substantially out-perform standard resampling algorithms on examples we consider; and we demonstrate how the resulting particle filter is practicable for segmentation of human GC content.<\/dc:description><dc:date>\n        2007<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/745\/1\/online_chpt4.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1111\/j.1467-9868.2007.00601.x<\/dc:relation><dc:identifier>\n        Fearnhead, P and Liu, Z (2007) Online Inference for Multiple Changepoint Problems. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 69 (4). pp. 589-605. ISSN 1369-7412<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/745\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1111\/j.1467-9868.2007.00601.x","http:\/\/eprints.lancs.ac.uk\/745\/"],"year":2007,"topics":[],"subject":["Journal Article","PeerReviewed"],"fullText":"On-line Inference for Multiple Change Points\nProblems\nPaul Fearnhead and Zhen Liu\nDepartment of Mathematics and Statistics\nFylde College,Lancaster University,UK\n1\n2Abstract\nWe propose an on-line algorithm for exact filtering of multiple changepoint prob-\nlems. This algorithm enables simulation from the true joint posterior distribution\nof the number and position of the changepoints for a class of changepoint mod-\nels. The computational cost of this exact algorithm is quadratic in the number of\nobservations. We further show how resampling ideas from particle filters can be\nused to reduce the computational cost to linear in the number of observations, at\nthe expense of introducing small errors; and propose two new, optimum resampling\nalgorithms for this problem. One, a version of rejection control, allows the particle\nfilter to automatically choose the number of particles required at each time-step.\nThe new resampling algorithms substantially out-perform standard resampling al-\ngorithms on examples we consider; and we demonstrate how the resulting particle\nfilter is practicable for segmentation of human GC content.\nKEY WORDS AND PHRASES : Direct simulation; Isochores; Rejection con-\ntrol; Sequential Monte Carlo; Stratified sampling; Particle Filtering.\n1 Introduction\nChangepoint models are commonly used to model heterogeneity of data (usually\nover time). Given a set of observations collected over time, these models intro-\nduce a (potentially random) number of changepoints which split the data into a\nset of disjoint segments. It is then assumed that the data arise from a single\nmodel within each segment, but with different models across the segments. Ex-\namples of changepoint problems include Poisson processes with changing intensity\n(Ritov et al., 2002), changing linear regressions (Lund and Reeves, 2002), Gaussian\nmodels with changing variance (Johnson et al., 2003) and Markov models with time-\nvarying transition matrices (Braun and Muller, 1998). These models have been ap-\nplied to problems in a range of areas, including engineering, physical and biological\nsciences and finance.\nWe consider Bayesian inference for changepoint models where the number of change-\npoints is unknown. The most common approach to such inference for these mod-\n3els is the use of Markov chain Monte Carlo (MCMC; see for example Chib, 1998;\nStephens, 1994), and reversible jump MCMC (Green, 1995). However, for many\nchangepoint models (for example the models in Green, 1995; Punskaya et al., 2002)\nit is possible to simulate independent realisations directly from the posterior dis-\ntribution. This idea was used for DNA segmentation by Liu and Lawrence (1999),\nand has been proposed more generally by Fearnhead (2006) and Fearnhead (2005).\nThe idea for direct simulation are based on exact methods for calculating posterior\nmeans (Barry and Hartigan, 1992). The advantages of direct simulation methods\nover MCMC and reversible jump MCMC are that (i) there is no need to diagnose\nwhether the MCMC algorithm has converged; and (ii) as the draws from the pos-\nterior distribution are independent it is straightforward to quantify uncertainty in\nestimates of features of the posterior distributions based on them. For examples of\nthe potential difficulties with MCMC caused by (i), compare the inferences obtained\nfor the Coal-mining disaster data analysed in Green (1995) with those based on the\ndirect simulation method of Fearnhead (2006).\nIn this paper we extend the direct simulation algorithms to on-line problems; where\nthe data is obtained incrementally over time, and new inferences are required each\ntime an observation is made. The use of on-line algorithms has also been suggested\nfor static problems (e.g. Chopin, 2002; Del Moral et al., 2006).\nThe computational cost of our exact on-line algorithm increases linearly over time,\nhowever the on-line version of direct simulation is similar to particle filter algorithms,\nand we consider using resampling algorithms taken from particle filters to reduce the\ncomputational cost of our direct simulation algorithm (at the expense of introducing\nerror). We propose two simple extensions of existing resampling algorithms that\nare particularly well-suited to changepoint models. One is a stratified extension of\nthe rejection-control approach of Liu et al. (1998), which can limit the maximum\namount of error introduced by each resampling step. In simulation studies we find\nthis stratified version can reduce the error of the resulting algorithm by about one\nthird as compared to the non-stratified version.\nThe resulting online algorithm can be viewed as a Rao-Blackwellised version of the\nParticle Filter algorithm of Chopin (2006): we have integrated out the parameters\n4associated with each segment. Note that this is an extremely efficient version of Rao-\nBlackwellisation. For example, consider analysing n data points. Our algorithm with\nn particles will give exact inference and thus will always outperform the algorithm\nof Chopin (2006) regardless of the number of particles used in that particle filter.\nNote however, that the filter of Chopin (2006) can be used more widely, as it does\nnot require that the parameters within each segment can be integrated out.\nThe outline of the paper is as follows. We introduce the class of changepoint models\nwe consider in Section 2. In Section 3 we introduce the online direct simulation\nalgorithm, and approximate versions of it that utilise various resampling ideas. We\ntest these approximate algorithms, and compare different resampling algorithms, on\nsimulated data in Section 4 before applying our algorithm to the analysis of the C+G\nstructure of human DNA data (Section 5). The paper concludes with a discussion.\n2 Models and Notations\nAssume we have data y1:n = (y1, y2, . . . , yn). We consider changepoint models for\nthe data with the following conditional independence property: given the position of\na changepoint, the data before that changepoint is independent of the data after the\nchangepoint. These models can be described in terms of the following hierarchical\nstructure.\nFirstly we model the changepoint positions via a Markov process. This Markov\nprocess is determined by a set of transition probabilities,\nPr(next changepoint at t|changepoint at s). (1)\nFor this paper we make the simplification that these transition probabilities depend\nonly on the distance between the two changepoints. Extending our work to the\ngeneral case, where the distribution of the length of a segment could depend on the\ntime at which it starts, is straightforward . Specifically we let g(\u00b7) be the probability\nmass function for the distance between two successive changepoints (equivalently the\nlength of segments); so that (1) is g(t\u2212 s). We further let G(l) =\n\u2211l\ni=1 g(i) be the\ndistribution function of this distance, and assume that g(\u00b7) is the probability mass\nfunction for the position of the first changepoint.\n5Note that any such model implies a prior distribution on the number of changepoints.\nFor example if a geometric distribution is used for g(\u00b7), then our model implies that\nthere is a fixed probability of a changepoint at any time-point, independent of other\nchangepoints. Hence this model implies a binomial distribution for the number of\nchangepoints.\nNow we condition on m changepoints at times \u03c41, \u03c42, . . . , \u03c4m. We let \u03c40 = 0 and\n\u03c4m+1 = n, so our changepoints define m + 1 segments, with segment i consisting\nof observations y\u03c4i+1:\u03c4i+1 for i = 0, . . . ,m. We allow a set of p\u00af possible models\nfor the data from each segment, labeled {1, 2, . . . , p\u00af}, and assume an arbitrary prior\ndistribution for models, common across segments, with the model in a given segment\nbeing independent of the models in all other segments.\nFor a segment consisting of observations ys+1:t and model q we will have a set of\nunknown parameters, \u03b2 say. We have a prior distribution, \u03c0(\u03b2) for \u03b2 (which may\ndepend on q), but assume that the parameters for this segment are independent of\nthe parameters in other segments. Finally we define\nP (s, t, q) =\n\u222b\nPr(ys+1:t|\u03b2,model q)\u03c0(\u03b2)d\u03b2, (2)\nand assume that these probabilities can be calculated for all s < t and q. This\nrequires either conjugate priors for \u03b2, or the use of numerical integration. Numerical\nintegration is often computationally feasible in practice if the dimension of the part\nof \u03b2 that cannot be integrated analytically is low (see Fearnhead, 2006, for an\nexample).\nFor concreteness we describe a specific example of this model which we will use\nin Section 4 (see also Punskaya et al., 2002; Fearnhead, 2005). Here we model the\ndata as piecewise linear regressions. So within a segment we have a linear regression\nmodel of unknown order. For a given model order q, we have\nys+1:t = H\u03b2 + \u01eb (3)\nwhere H is a (t \u2212 s) \u00d7 q matrix of basis functions, \u03b2 is a vector of q regression\nparameters and \u01eb is a vector of iid Gaussian noise with mean 0 and variance \u03c32.\nWe assume conjugate priors. The variance of the Gaussian noise has an inverse\ngamma distribution with parameters \u03bd\/2 and \u03b3\/2, and the components of the re-\n6gression vector have independent Gaussian priors. The prior for the jth component\nis Gaussian with mean 0 and variance \u03c32\u03b42j .\nThe likelihood function of ys+1:t conditional on a model q is obtained by integrating\nout the regression parameters \u03b2 and variance \u03c32 :\nP (s, t, q) = \u03c0\u2212(t\u2212s)\/2\n(\n|M|\n|D|\n) 1\n2 (\u03b3)\u03bd\/2\n(||ys+1:t||\n2\nP\n+ \u03b3)(t\u2212s+\u03bd)\/2\n\u0393((t\u2212 s+ \u03bd)\/2)\n\u0393(\u03bd\/2)\n, (4)\nwhereM = (HTH+D\u22121)\u22121, P = (I\u2212HMHT ), ||y||2\nP\n= yTPy,D = diag(\u03b421, . . . , \u03b4\n2\nq )\nand I is a (t\u2212 s)\u00d7 (t\u2212 s) identity matrix.\n3 On-line Inference\nWe consider on-line inference for the multiple changepoint model of Section 2. We\nassume that observations accrue over time, so that yt is the observation at time t. At\neach time-step, our aim is to calculate the posterior distributions of interest based\non all the observations to date. To do this efficiently requires updating our posterior\ndistributions at the previous time-step to take account of the new observation. Note\nthat on-line algorithms can be used to analyse batch data by introducing an artificial\ntime for each observation.\nWe focus on on-line inference of the position of the changepoints. Under the mod-\neling assumptions of Section 2, inference for the parameters conditional on knowing\nthe number and position of the changepoints is straightforward. We first describe an\nexact on-line algorithm, which is an on-line version of the direct simulation method\nof Fearnhead (2005). The computational cost of this exact algorithm increases over\ntime, so we then present an approximate on-line algorithm, which uses resampling\nideas from particle filters, and which has constant computational cost over time.\n3.1 Exact On-line Inference\nWe introduce a state at time t, Ct, which is defined to be the time of the most\nrecent change-point prior to t (with Ct = 0 if there have been no change-points\nbefore time t). Initially we focus on calculating the posterior distribution for Ct\n7given the observation y1:t. We then describe how, if these distributions are stored\nfor all t, it is straightforward to simulate from the joint posterior distribution of the\nposition of all changepoints prior to the current time.\nFiltering Recursions\nThe state Ct can take values in 0, 1, . . . , t \u2212 1, and C1, C2, . . . , Ct, . . . is a Markov\nchain. Conditional on Ct = j, either Ct+1 = j, which corresponds to no changepoint\nat time t, or Ct+1 = t, if there is a changepoint at time t. The transition probabilities\nfor this Markov chain can thus be calculated as:\nPr(Ct+1 = j|Ct = i) =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f3\n1\u2212G(t\u2212i)\n1\u2212G(t\u2212i\u22121)\nif j = i,\nG(t\u2212i)\u2212G(t\u2212i\u22121)\n1\u2212G(t\u2212i\u22121)\nif j = t,\n0 otherwise,\n(5)\nwhere G(\u00b7) is the distribution function of distance between two successive change\npoints.\nNow, standard filtering recursions give\nPr(Ct+1 = j|y1:t+1) \u221d Pr(yt+1|Ct+1 = j,y1:t) Pr(Ct+1 = j|y1:t),\nand\nPr(Ct+1 = j|y1:t) =\nt\u22121\u2211\ni=0\nPr(Ct+1 = j|Ct = i) Pr(Ct = i|y1:t).\nThus, if we let w\n(j)\nt+1 = Pr(yt+1|Ct+1 = j,y1:t), and substitute in the transition\nprobabilities (5), we obtain\nPr(Ct+1 = j|y1:t+1) \u221d\n\uf8f1\uf8f2\n\uf8f3 w\n(j)\nt+1\n1\u2212G(t\u2212j)\n1\u2212G(t\u2212j\u22121)\nPr(Ct = j|y1:t) if j < t,\nw\n(t)\nt+1\n\u2211t\u22121\ni=0\n(\nG(t\u2212i)\u2212G(t\u2212i\u22121)\n1\u2212G(t\u2212i\u22121)\nPr(Ct = i|y1:t)\n)\nif j = t.\nIf we define P (s, t, q) as in (2) then we get for j < t\nw\n(j)\nt+1 =\n\u2211p\u00af\nq=1 P (j, t+ 1, q)p(q)\u2211p\u00af\nq=1 P (j, t, q)p(q)\n, (6)\nwhile if j = t then the weight is\n\u2211p\u00af\nq=1 P (t, t+ 1, q)p(q).\nIn most situations, such as for the linear regression models described in Section 2, the\nincremental weights w\n(j)\nt+1 can be calculated efficiently, as each P (j, t, q) depends on a\n8set of summary statistics of the observations yj+1:t, which can be updated recursively.\nEfficient calculation of the incremental weights w\n(j)\nt+1 is possible by storing these\nsummary statistics for each set of values for the time of the last changepoint, j, and\nthe model for the current segment, q. These can be updated to give the summary\nstatistics required for each P (j, t+1, q). Thus the computational cost of calculating\neach w\n(j)\nt+1 is fixed, and does not increase with t\u2212 j.\nSimulating Changepoints\nIf we store the filtering densities Pr(Ct|y1:t) for all t = 1, . . . , n, then it straightfor-\nward to simulate from the joint posterior distribution of the position of all change-\npoints prior to time n, using the idea of Chopin (2006). To simulate one realisation\nfrom this joint density:\n(1) Set t0 = n, and k = 0\n(2) Simulate tk+1 from the filtering density Pr(Ctk |y1:tk), and set k = k + 1.\n(3) If tk > 0 return to (2); otherwise output the set of simulated changepoints,\ntk\u22121, tk\u22122, . . . , t1.\nA simple extension of this algorithm which enables a large sample of realisations of\nsets of changepoints to be simulated efficiently is described in Fearnhead (2006).\nMAP estimation\nWe can obtain an on-line Viterbi algorithm for calculating the maximum a posteriori\n(MAP) estimate of the positions of the changepoints and the model orders for each\nsegment as follows. We defineMj to be the event that given a changepoint at time j,\nthe MAP choice of changepoints and model occurs prior to time j. For t = 1, . . . , n,\nj = 0, . . . , t\u2212 1 and q = 1, . . . , p\u00af,\nPt(j, q) = Pr(Ct = j,model q,Mj,y1:t), and\nPMAPt = Pr(Changepoint at t,Mt,y1:t).\nWe obtain the following equations\nPt(j, q) = (1\u2212G(t\u2212 j \u2212 1))P (j, t, q)p(q)P\nMAP\nj , and\n9PMAPt = max\nj,q\n{Pt(j, q)g(t\u2212 j)\/(1\u2212G(t\u2212 j \u2212 1)}. (7)\nAt time t, the MAP estimates of Ct and the current model order are given respec-\ntively by the values of j and q which maximise Pt(j, q). Given a MAP estimate\nof Ct, c\u02c6t we can the calculate the MAP estimates of the changepoint prior to c\u02c6t\nand the model order of that segment by the values of j and q that maximised the\nright-hand side of (7). This procedure can be repeated to find the MAP estimates\nof all changepoint positions and model orders.\n3.2 Approximate Inference\nThe computational and memory costs of the recursions for exact inference presented\nin Section 3.1 both increase with time. The computational cost of both the filtering\nrecursion and MAP recursion at time t is proportional to t, the number of possible\nvalues of Ct. While the memory cost of storing all filtering densities up to time t,\nnecessary to simulate from the joint posterior of all changepoints prior to t, increases\nquadratically with t. For large data sets, these computational and memory costs\nmay become prohibitive.\nA similar problem of increasing computational cost occurs in the analysis of some\nhidden Markov models \u2013 though generally computational cost increases exponen-\ntially with time (Chen and Liu, 2000). Particle filters have been successfully ap-\nplied to these problems (Fearnhead and Clifford, 2003) by using a resampling step\nto limit the computational cost at each time-step. Here we show how similar re-\nsampling ideas can be applied to the online inference of the changepoint models\nwe are considering. We present a variation on the optimal resampling method of\nFearnhead and Clifford (2003) which is specifically designed for changepoint models,\nand show theoretically why this is an optimal resampling algorithm in this case. We\nalso present an extension of the rejection control approach of Liu et al. (1998) which\nis suitable for the analysis of batch data, and for which it is possible to control the\namount of error introduced at each resampling step.\nControlling Computational Cost\nOut first approach is to control the average (and maximum) computational cost for\n10\nanalysing each new observation. At time t our exact algorithm stores the complete\nposterior distribution of the time of the last changepoint Pr(Ct = ct|y1:t), for ct =\n0, 1, . . . , t \u2212 1. We can approximate this by a discrete distribution with fewer, N ,\nsupport points. This approximate distribution can be described by the set of support\npoints, c(1), . . . , c(N), henceforth called particles, and the probability mass associated\nwith each of these particles, w(1), . . . , w(N), which we call weights. (The particles\nand their weights will depend on t; we have suppressed this dependence to simplify\nnotation.)\nWe impose a maximum number of particles to be stored at any one time, N , such\nthat whenever we have N particles we immediately perform resampling to reduce\nthe number of particles to M < N . The average computational cost per iteration\nwill thus be proportional to (M +N + 1)\/2, and the maximum computational cost\nper iteration proportional to N .\nAssume that at the current time point we have N particles; and wish to reduce these\nto M particles. We propose the following stratified version of the optimal resam-\npling algorithm of Fearnhead and Clifford (2003), which we call Stratified Optimal\nResampling (SOR).\nInitialisation Assume we currently have a set of ordered particles c(1) < c(2) <\n\u00b7 \u00b7 \u00b7 < c(N), with associated weights w(1), . . . , w(N), which sum to unity.\n(SOR1) Calculate \u03b1 the unique solution to\n\u2211N\ni=1min{1, w\n(i)\/\u03b1} = M ;\n(SOR2) For i = 1, . . . , N if w(i) \u2265 \u03b1 then keep particle c(i) with weight w(i).\nAssume that A particles are kept.\n(SOR3) Use the stratified resampling algorithm of Carpenter et al. (1999) to re-\nsample M \u2212 A times from the ordered set of the remaining N \u2212 A particles\n(without shuffling). Each resampled particle is assigned a weight \u03b1.\nThe stratified resampling algorithm used in step SOR3 is given in Appendix A.\nThe use of stratified resampling in step SOR3 means that at most one copy of\neach particle is kept, as the expected number of times a particle with weight w\nis resampled is w\/\u03b1 < 1 (note there is no advantage in having multiple copies\n11\nof particles, see Fearnhead and Clifford, 2003). The only difference between this\nSOR algorithm and the original algorithm of Fearnhead and Clifford (2003) is that\nparticles are ordered before resampling in step SOR3.\nAs shown in Fearnhead and Clifford (2003), if we denote by W (i) the (random)\nweight of a particle after resampling (so W (i) = w(i), \u03b1, or 0 depending on whether\nthe respective particle is kept, resampled or not resampled), then SOR is optimal\nover all resampling algorithms that satisfy E(W (i)) = w(i) in terms of minimising\nthe mean square error: E(\n\u2211N\ni=1(W\n(i) \u2212 w(i))2). By ordering the particles in step\nSOR3 we obtain the further property:\nTheorem 3.1 Consider a set of N particles, c(1) < c(2) < . . . < c(N) with weights\nw(1), . . . , w(N). Let W (i) be the (random) weight of particle c(i) after resampling.\nDefine the maximum Kolmogorov Smirnov Distance for a resampling algorithm as\nmKSD = max\n{\nmax\ni\n\u2223\u2223\u2223\u2223\u2223\ni\u2211\nj=1\n(\nw(i) \u2212W (i)\n)\u2223\u2223\u2223\u2223\u2223\n}\n(8)\nwhere the first maximisation is over realisations of W (1), . . . ,W (N) with positive\nprobability. Then the SOR algorithm above satisfies mKSD \u2264 \u03b1 (where \u03b1 is defined\nas in SOR1). Furthermore (i) for a resampling algorithm to have mKSD \u2264 \u03b1 then all\nparticles with w(i) > \u03b1 must be propagated without resampling;and (ii) the mKSD for\nthe SOR algorithm above is less than or equal to the mKSD of the optimal resampling\nalgorithm of Fearnhead and Clifford (2003), and the rejection control algorithm of\nLiu et al. (1998).\nProof: See Appendix B. \u0003\nKolmogorov Smirnov distance is a natural metric for the distributions of 1-dimensional\nrandom variables. By using (8) as a measure of error of a resampling algorithm, we\nare considering the bound on Kolmogorov Smirnov distance that a resampling algo-\nrithm can introduce. The theorem gives a simple interpretation of the \u03b1 calculated\nin step SOR1; in terms of an upper bound on the Kolmogorov Smirnov distance\nbetween the original and resampled weights.\nWe define a resampling algorithm to be unbiased if E(W (i)) = w(i) for all i. (This is\nrelated to the properly weighted condition of Liu et al., 2001). The optimal resam-\n12\npling algorithm of Fearnhead and Clifford (2003) and rejection control are currently\nthe only other unbiased resampling algorithm which satisfy the condition (i) of The-\norem 1. (Note that rejection control will not produce a fixed number of particles\nafter resampling; though implementing rejection control with a threshold of \u03b1 will\nproduce on average N resampling particles, and further that while E(W (i)) = w(i),\nthe resampled weights do not necessarily sum to 1.) So results (i) and (ii) of Theo-\nrem 1 show that our SOR algorithm is optimal over all existing unbiased resampling\nalgorithms in terms of minimising mKSD.\nWe have presented the SOR algorithm in terms of the general case of resampling\nM particles from N current particles. For on-line inference, where there is a fixed\namount of time to analyse each observation, it is natural to choose N to be the\nlargest number of particles that enable an observation to be analysed in less than\nthis amount of time; and to set M = N \u2212 1. In this case there is no difference\nbetween SOR and the existing optimal resampling algorithm. In practice, it may\nbe better to choose N \u2212M > 1 (see Section 4) as this enables the particles to be\nremoved to be jointly chosen in a stratified manner.\nControlling Resampling Error\nAn alternative to basing resampling on the average and maximum number of parti-\ncles to be kept at each time step, is to choose the amount of resampling to control\nthe size of error that is introduced at each time step. Such an approach is most\nsuitable for using online algorithms to analyse batch data. For real-time data, the\nfrequency of observations will place an upper bound on the CPU time, and hence the\nnumber of particles, that can be used to process each observation. By controlling\nthe resampling error, rather than the number of particles, we cannot ensure that\nthe number of particles always stays below this error.\nThe idea behind controlling the resampling error is given by the interpretation of \u03b1\nfor SOR that comes from Theorem 1. The value of \u03b1 defines the maximum error\n(as defined by Kolmogorov Smirnov distance) that is introduced by the resampling\nerror. So rather than specifying the number of resampled particles which in turn\ndefines \u03b1, and hence the amount of error we introduce, we can instead specify \u03b1\nwhich will then define the number of resampled particles.\n13\nOur method for controlling the resampling error is to use a stratified version of\nrejection control (Liu et al., 1998), rather than adapt the SOR algorithm. For a\nprespecified value of \u03b1, our stratified rejection control (SRC) algorithm is:\nInitialisation Assume we currently have a set of ordered particles c(1) < c(2) <\n\u00b7 \u00b7 \u00b7 < c(N), with associated weights w(1), . . . , w(N), which sum to unity.\n(SRC1) For i = 1, . . . , N if w(i) \u2265 \u03b1 then keep particle c(i) with weight w(i).\nAssume that A particles are kept.\n(SRC2) Use the stratified resampling algorithm of Carpenter et al. (1999) to re-\nsample from the ordered set of the remaining N \u2212 A particles (without shuf-\nfling). The expected number of times particle c(i) is resampled is w(i)\/\u03b1. Each\nresampled particle is assigned a weight \u03b1.\nAgain the use of stratified resampling in (SRC2) means that at most one copy of\neach particle is kept. Note that the sum of the particles\u2019 weights after resampling\nwill not necessarily sum to 1 (though they lie between 1\u2212\u03b1 and 1+\u03b1), and should\nbe normalised to produce a probability distribution.\nThe difference between SRC and rejection control (Liu et al., 1998) is that particles\nare ordered and stratified resampling is used in step SRC2, as opposed to indepen-\ndent resampling of each particles. The use of stratified resampling means that the\nmaximum error of the unnormalised weights introduced by SRC, as measured by\nKolmogorov Smirnov distance, is \u03b1 (this can be proved in an identical manner to\nTheorem 1). Furthermore, the error of the normalised weights can be shown to be\nbounded above by \u03b1\/(1\u2212 \u03b1) = \u03b1+ o(\u03b1) (see Appendix C).\n4 Numerical Examples\nWe tested our algorithm on three simulated examples: the Blocks and Heavisine\nexamples from Donoho and Johnstone (1994) and a piecewise auto-regressive model.\nEach of the three data-sets are analysed under a piecewise regression model. The\ndesign matrices for the piecewise AR model and the piecewise polynomial regression\n14\nmodel (for the Blocks and Heavisine data) are\nHs:t =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nys\u22121 ys\u22122 ys\u22123\nys ys\u22121 ys\u22122\n...\n...\n...\nyt yt\u22121 yt\u22122\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n, and Hs:t =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n1 xs x\n2\ns\n1 xs+1 x\n2\ns+1\n...\n...\n...\n1 xt x\n2\nt\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n.\nrespectively, where xs = s\/n and n is the number of data points. In each case we\nperform model choice within each segment, choosing between model orders 1, 2 and\n3. We allow for different variances of the measurement error within each segment,\nalthough for the Blocks and Heavisine examples we simulated data with a common\nerror variance across segments. Further details of the model, and calculations re-\nquired for calculating the P (s, t, q)s is given in Section 2 (See also Punskaya et al.,\n2002; Fearnhead, 2005).\nOur focus here is on the performance of different possible resampling algorithms.\nThe Blocks data set (see Figure 1) is a particularly simple data set to analyse, and\nall reasonable resampling algorithms will give almost identical results. We show the\nresults here to demonstrate how the SRC algorithm naturally adapts the number of\nparticles that are kept. The Blocks data set has a number of obvious changepoints,\nand when each of these are encountered the number of particles that are needed to\nbe kept is reduced to close to 1.\nFor the Heavisine example (see Figure 1) we compared the accuracy of various\nresampling algorithms: stratified rejection control (SRC), rejection control (RC),\nstratified optimal resampling (SOR), and optimal resampling (OR). We considered\ntwo values of \u03b1 for SRC and RC; and for a meaningful comparison, fixed the mean\nnumber of particles in OR and SOR to the mean number of particles kept by SRC\nfor each of these two values. If we set the number of resampled particles (M) to be\none less than the number of particles prior to resampling (N), then OR and SOR\nare identical. We tested both N = M + 1 and N = M + 5.\nOur comparison is based on the Kolmogorov Smirnov distance (KSD) between the\ntrue filtering distribution of the most recent changepoint, p(Ct|y1:t) (calculated using\nthe online algorithm with no resampling), and its approximation based on the various\nresampling algorithms, for each t. Results are given in Table 1. The results show\n15\n0 100 200 300 400 500\n\u2212\n10\n0\n5\n10\n20\nBlocks Data\nTime\ny\n0 100 200 300 400 500\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\nTime\nPo\nst\ner\nio\nrs\n o\nf c\nha\nng\nep\noi\nnt\ns\n0 100 200 300 400 500\n0\n20\n40\n60\n80\n10\n0\nTime\nN\num\nbe\nr o\nf P\nar\ntic\nle\ns\n0.0 0.2 0.4 0.6 0.8 1.0\n\u2212\n15\n\u2212\n5\n0\n5\n10\nHeavisine Data\nx\ny\n0.0 0.2 0.4 0.6 0.8 1.0\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\nx\nPo\nst\ner\nio\nrs\n o\nf c\nha\nng\nep\noi\nnt\ns\n0 50 100 150 200\n0\n20\n40\n60\nTime\nN\num\nbe\nr o\nf P\nar\ntic\nle\ns\nFigure 1: The Blocks data set (left-hand column) and Heavisine data set (right-hand\ncolumn) together with results of analysis by the SRC algorithm with \u03b1 = 10\u22126:\ndata and inferred signal (top); marginal probability of changepoints (middle); and\nnumbers of particles kept (bottom).\nthat the mean KSD error is reduced by one third by using SRC rather than RC.\nBoth of these methods perform better than the resampling algorithms that use a\nfixed number of particles (for the same average number of particles); showing the\nadvantage of allowing the number of particles used to adapt to the filtering density\nbeing approximated. Of the two algorithms considered which use a fixed number\nof particles, we see an improvement of using SOR where we remove 5 particles at\neach resampling step over OR (or equivalently SOR) where 1 particle is removed\nat each resampling step. By removing many particles in one step, SOR is able to\njointly choose the particles to remove in a stratified way so as to reduce the error\nintroduced. (Note OR where we remove 5 particles at each resampling step has\nworse results than the OR results shown in Table 1.)\nNote that while all resampling algorithms introduce small errors at each resampling\nstep, it is possible for these errors to accumulate. The reason for this, appears to\nbe that the evidence for a changepoint at a given time t can change substantially as\nmore data is collected. If the evidence is small (and hence the filtering probability\nof a changepoint at t is less than \u03b1) at a resampling step, this can lead to the\n16\nAutoregressive Data\nTime\ny\n0 50 100 150 200\n\u2212\n5\n0\n5\n0 50 100 150 200\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\nStratified Rejection Control\nTime\nPo\nst\ner\nio\nrs\n o\nf c\nha\nng\ne \npo\nin\nts\n0 50 100 150 200\n0\n20\n40\n60\n80\n12\n0\nNumber of Particles\nTime\nN\num\nbe\nr o\nf P\nar\ntic\nle\ns\n0 50 100 150 200\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\nParticle Filters of Chopin(2006)\nTime\nPo\nst\ner\nio\nrs\n o\nf c\nha\nng\ne \npo\nin\nts\nFigure 2: Results of analysing the AR dataset using SRC with \u03b1 = 10\u22126, and\nthe particle filter of Chopin (2006) with 50,000 particles: data (top left), marginal\nprobabilities of changepoint for SRC (top right) and particle filter of Chopin (2006)\n(bottom right), and number of particles kept using SRC (bottom left). The true\nAR model to the four segments have model orders 1, 1, 2, and 3 respectively. The\ncorresponding parameters are \u03b2 = 0.4; \u03b2 = \u22120.6; \u03b2 = (\u22121.3,\u22120.36, 0.25) and\n\u03b2 = (\u22121.1,\u22120.24) with error variances 1.22, 0.72, 1.32 and 0.92 respectively.\ncorresponding particle being removed. Such a particle can not be \u201cresurrected\u201d as\nfuture observations are made, even if they carry strong evidence for a changepoint\nat t. However stratified resampling should ensure that a particle corresponding to\na changepoint close to t is kept, and thus the error in estimating the position of the\nchangepoints will still be small.\nWe repeated this analysis for a piecewise AR model. The results of the SRC analysis\nwith \u03b1 = 1\u00d7 10\u22126 given in Figure 2, and results of the accuracy of each resampling\nmethod given in Table 1. We observe similar results to the Heavisine example in\nterms of the relative performance of the resampling algorithms. In this case SRC\nagain outperforms RC by about a third. The difference in performance between\n17\nSRC RC SOR OR\nHeavisine 1.3\u00d710\u22122 2.0\u00d710\u22122 4.2\u00d710\u22122 6.4\u00d710\u22122\nAR 1.3\u00d710\u22126 2.2\u00d710\u22126 2.2\u00d710\u22124 3.5\u00d710\u22124\nTable 1: Mean Kolmogorov Smirnov Distance in P (Ct|y1:t) averaged over t for the\nHeavisine and AR models and the four resampling algorithms. Stratified Rejection\nControl (SRC) and Rejection Control (RC) were implemented with \u03b1 = 10\u22126; these\nalgorithms used an average number of 43 and 70 particles for the Heavisine and\nAR models respectively. Optimal Resampling (OR) was implemented with N =\nM + 1 = 49 and N = M + 1 = 90; Stratified Optimal Resampling (SOR) used\nN = M + 5 = 51 and N = M + 5 = 92 (chosen so that the average number\nof particles is the same for all algorithms for each data set). Results based on 50\nreplications of each algorithm for one version of each data set. The true distribution,\nP (Ct|y1:t), was calculated using the exact online algorithm.\nSRC and RC as compared to SOR and OR is quite substantial in this case, because\ntowards the end of the time series it is forced to use too few particles to adequately\napproximate the filtering densities. This again demonstrates the potential gains to\nbe obtained by allowing the number of particles used to change over time and to\nadapt to the filtering distribution that is being approximated.\nWe also ran the particle filter of Chopin (2006). This filter does not integrate out the\nparameters associated with each segment, so each particle consists of a time for the\nlast changepoint together with a value of the parameters for the current segment.\nThe filter uses MCMC to update the parameters of a subset of particles at each\niteration. We ran the filter with 50,000 particles, using a Gibbs sampler update on\nthe parameters of 1\/3 of the particles at each iteration. This took over an order of\nmagnitude longer to run than the SRC algorithm, and even is substantially more\ntime-consuming to implement than the exact online algorithm.\nThe results for the estimate of the marginal probabilities of the changepoints is\nshown in Figure 2. The filter of Chopin (2006) suffers from a loss of diversity in the\nparticles \u2013 with many positions being assigned zero probability of being a change-\n18\npoint, when in fact there is a non-negligible probability as can be seen from the\noutput of the SRC filter. To give a quantitative comparison of the two methods we\ncalculated the mean absolute error between the estimates of the marginal probabil-\nities of the changepoints shown in Figure 2 with those based on the exact particle\nfilter algorithm. These were 0.010 and 0.002 for the filter of Chopin (2006) and the\nSRC filter respectively.\n5 DNA Segmentation\nIn recent years there has been an explosion in the amount of data describing the\ngenetic make-up of different organisms; for example the complete DNA sequence of\none human genome is now known as a result of the Human Genome project. There is\ninterest in learning about the genomic features of different organisms, and learning\nhow these features may have evolved and how they correlate with each other.\nWe consider the problem of understanding the structure of C+G content within\nthe genome. A common model for the C+G content of the human genome is that\nthere are large, of the order of 300 kilobases (kb), regions of roughly homogeneous\nC+G content, called Isochores (see Bernardi, 2000, for background). Furthermore\nC+G content is known to correlate with various features of the genome, such as\nhigh recombination rates and gene density (Hardison et al., 2003).\nCurrently, the most common method for segmenting an organism\u2019s genome into re-\ngions of different C+G content is implemented in the computer program IsoFinder\n(Oliver et al., 2004). This is based on a recursive segmentation procedure, which\ninitially classifies a large genomic region as consisting of a single Isochore (region of\ncommon C+G content). It then considers in turn each possible position for adding\na changepoint, and splitting the data into two Isochores. For each possible position,\na t-statistic is calculated for testing whether the mean C+G content is different in\nthe two putative Isochores. For each changepoint, a p-value is calculated for its\nvalue of the t-statistic using a bootstrap procedure, and if the smallest p-value is\nless than some predefined threshold, then the corresponding changepoint is added.\nThis procedure is repeated, with at each step each current Isochore being tested for\n19\nwhether it can be split into two Isochores. See Oliver et al. (2004) for more details.\nWe consider a Bayesian approach to segmenting a genomic region into Isochores.\nThe potential advantages of a Bayesian approach include (i) quantifying and aver-\naging over the uncertainty in the number and positions of the Isochores; (ii) jointly\nestimating all Isochore positions (which Braun et al., 2000, show to be more accu-\nrate than segmentation procedures); and (iii) the large amount of data available for\neach organism makes it straightforward to construct sensible prior distributions.\nOne of the computational challenges of such an analysis is the large amount of\ndata that needs to be analysed (for example human chromosomes consist of around\n100 million bases). We simplify this burden by first summarising our data by the\nnumber of DNA sites which are C or G within consecutive windows (each window\nbeing of the order of a few kb in width), an approach which also has the advantage\nof averaging our the very local high variation in C+G content caused for example by\nCpG islands and Alu elements. We then hope that our online changepoint algorithm\nwill be able to efficiently analyse the resulting data, and one of the main aims of\nthe study we present here is to test whether such an approach is computationally\npracticable for analysing the large amount of genomic data currently available.\nThe model we use is based on the following simple model for the data y1:n, which is\nsimilar to the implicit model assumed by IsoFinder. A data set is shown in Figure\n3. The tth data point, yt, represents the number of DNA bases which are either C\nor G within the tth window. If this window lies within the ith Isochore then we\nassume\nyt = \u00b5i + \u03c3i\u01ebt,\nwhere \u00b5i is the mean C+G content of each window within the ith Isochore, \u03c3\n2\ni is\nthe error variance within the ith Isochore, and \u01ebt is some independent error. We\nassume that \u01ebt has a Gaussian distribution and we assume standard conjugate priors\n(see Section 2) for the \u00b5is and \u03c3is, with the prior parameters chosen from an initial\nanalysis of C+G data with a moving median filter. For each model we assumed a\ngeometric distribution for the length of each Isochore.\nResults of our analysis using SRC with \u03b1 = 10\u22126 are shown in Figure 3. Our main\nfocus is on the computational practicability of a Bayesian analysis of such data, and\n20\n0 10000 20000 30000\n0.\n3\n0.\n4\n0.\n5\n0.\n6\nPositions(kb)\nPe\nrc\nen\nta\nge\n o\nf C\n+G\nFigure 3: Analysis of 35Mb of data from human chromosome 1. The red line is the\nposterior mean GC content.\nour method took 6 seconds on a desktop PC to analyse this data set.\nThis application does not need to be analysed by an online algorithm, such as the\none we used. However Fearnhead (2006) showed that the version of our algorithm\nwithout resampling can be more efficient for analysing changepoint models than\nsome commonly used MCMC algorithms. Furthermore, by using resampling we\nhave been able to vastly reduce the computational and storage cost of analysing\nthe data. For example our implementation with resampling uses an average of 117\nparticles at each time step; whereas without resampling the algorithm would require\nan average of over 3,500 particles for each time-step.\n6 Discussions\nWe have considered a class of changepoint models, which have a specific conditional\nindependence structure (see Section 2), and shown how the direct simulation algo-\nrithm of Fearnhead (2005) can be implemented online. Such an algorithm can be\nviewed as an exact particle filter, and resampling ideas taken from particle filters\ncan be used to reduce the computational complexity of the direct simulation algo-\nrithm (at the cost of introducing error). We have presented two simple extensions\n21\nof existing resampling algorithms, which are particularly well suited to changepoint\nproblems (or any problems where the underlying state of interest is 1-dimensional).\nIn simulation studies, our new resampling algorithms decreased the error of the\nresulting particle filter by up to one third, compared to particle filters using the\nexisting resampling approaches. We have shown that the new resampling algorithms\nsatisfy a minimax optimality criteria on the error, as measured by Kolmogorov\nSmirnov distance, introduce by resampling. Furthermore this result gives a natural\ninterpretation of the threshold that needs to be specified in the stratified rejection\ncontrol algorithm which will aid its implementation in practice.\nThere is great flexibility with implementing resampling algorithms within particle\nfilters which we have not explored. For example Liu and Chen (1998) discuss the\nfrequency with which resampling should occur, and Liu et al. (1998) suggest us-\ning rejection control only when the variance of the particle filter weights exceeds\nsome threshold. Whilst we have not fully investigated these issues, the results from\nSection 4 suggests that the advantages of using stratification within optimal resam-\npling or rejection control will increase as the frequency of resampling decreases (or\nequivalently the amount of particles resampled increases at each resampling step).\nAcknowledgements This work is supported by EPSRC grant C531558. We ded-\nicate this paper to the memory of Nick Smith who helped with the application to\ndetecting Isochores.\nReferences\nBarry, D. and Hartigan, J. A. (1992). Product partition models for change point\nproblems. The Annals of Statistics, 20:260\u2013279.\nBernardi, G. (2000). Isochores and evolutionary genomics of vertebrates. Gene,\n241:3\u201317.\nBraun, J. V., Braun, R. K., and Muller, H. G. (2000). Multiple changepoint fitting\nvia quasilikelihood, with application to DNA sequence segmentation. Biometrika,\n87:301\u2013314.\n22\nBraun, J. V. and Muller, H. G. (1998). Statistical methods for DNA sequence\nsegmentation. Statistical Science, 13:142\u2013162.\nCarpenter, J., Clifford, P., and Fearnhead, P. (1999). An improved particle filter for\nnon-linear problems. IEE proceedings-Radar, Sonar and Navigation, 146:2\u20137.\nChen, R. and Liu, J. (2000). Mixture Kalman filters. Journal of the Royal Statistical\nSociety, Series B, 62:493\u2013508.\nChib, S. (1998). Estimation and comparison of multiple change-point models. Jour-\nnal of Econometrics, 86:221\u2013241.\nChopin, N. (2002). A sequential particle filter method for static models. Biometrika,\n89:539\u2013551.\nChopin, N. (2006). Dynamic detection of change points in long time series. Annals\nof the Institute of Statistical Mathematics, page to appear.\nDel Moral, P., Doucet, A., and Jasra, A. (2006). Sequential Monte Carlo samplers.\nJournal of the Royal Statistical Society, Series B, 68:411\u2013436.\nDonoho, D. L. and Johnstone, I. M. (1994). Ideal spatial adaptation by wavelet\nshrinkage. Biometrika, 81:425\u2013455.\nFearnhead, P. (2005). Exact Bayesian curve fitting and signal segmentation. IEEE\nTransactions on Signal Processing, 53:2160\u20132166.\nFearnhead, P. (2006). Exact and efficient inference for multiple changepoint prob-\nlems. Statistics and Computing, 16:203\u2013213.\nFearnhead, P. and Clifford, P. (2003). Online inference for hidden Markov models.\nJournal of the Royal Statistical Society, Series B, 65:887\u2013899.\nGreen, P. (1995). Reversible jump Markov chain Monte Carlo computation and\nBayesian model determination. Biometrika, 82:711\u2013732.\nHardison, R. C., Roskin, K. M., Yanf, S., Diekhans, M., Kent, W. J., Weber, R.,\nElnitski, L., and Li et al., J. (2003). Covariation in frequencies of substitution,\n23\ndeletion, transposition, and recombination during eutherian evolution. Genome\nResearch, 13:13\u201326.\nJohnson, T. D., Elashoff, R. M., and Harkema, S. J. (2003). A Bayesian change-\npoint analysis of electromyographic data: detecting muscle activation patterns\nand associated applications. Biostatistics, 4:143\u2013164.\nLiu, J. S. and Chen, R. (1998). Sequential Monte Carlo methods for dynamic\nsystems. Journal of the American Statistical Association., 93:1032\u20131044.\nLiu, J. S., Chen, R., and Logvinenko, T. (2001). A theoretical framework for se-\nquential importance sampling with resampling. In Doucet, A., de Freitas, N., and\ngordon, N., editors, Sequential Monte Carlo Methods in Practice, pages 225\u2013246.\nSpringer\u2013Verlag; New York.\nLiu, J. S., Chen, R., and Wong, W. H. (1998). Rejection control and sequential\nimportance sampling. Journal of the American Statistical Society, 93:1022\u20131031.\nLiu, J. S. and Lawrence, C. E. (1999). Bayesian inference on biopolymer models.\nBioinformatics, 15:38\u201352.\nLund, R. and Reeves, J. (2002). Detection of undocumented changepoints: A revi-\nsion of the two-phase regression model. Journal of Climate, 15:2547\u20132554.\nOliver, J. L., Carpena, P., Hackenberg, M., and Bernaola-Galvan, P. (2004).\nIsoFinder: computational prediction of isochores in genome sequences. Nuceleic\nAcids Research, 32:W287\u2013W292. Web Server Issue.\nPunskaya, E., Andrieu, C., Doucet, A., and Fitzgerald, W. J. (2002). Bayesian curve\nfitting using MCMC with applications to signal segmentation. IEEE Transactions\non Signal Processing, 50:747\u2013758.\nRitov, Y., Raz, A., and Bergman, H. (2002). Detection of onset of neuronal activ-\nity by allowing for heterogeneity in the change points. Journal of Neuroscience\nMethods, 122:25\u201342.\nStephens, D. A. (1994). Bayesian retrospective multiple-changepoint identification.\nApplied Statistics, 43:159\u2013178.\n24\nAppendix A: Stratified Resampling Algorithm\nWe describe the stratified resampling algorithm of Carpenter et al. (1999) in terms\nof the SOR and SRC algorithms. Assume we currently have a set of N ordered\nparticles c(1) < c(2) < \u00b7 \u00b7 \u00b7 < c(N), with associated weights w(1), . . . , w(N), which sum\nto unity. For the SOR algorithm define \u03b1 as in step (SOR1); and for SRC we assume\nthat the value of \u03b1 is given. Resampling of M particles proceeds as follows:\n(A) Simulate u a realisation of a uniform random variable on [0, \u03b1]. Set i = 1.\n(B1) If w(i) \u2265 \u03b1 then propagate particle c(i) with weight w(i); else let u = u\u2212 w(i);\nif u \u2264 0 then resample particle c(i) and assign a weight \u03b1, and set u = u+ \u03b1.\n(C) Let i = i+ 1; if i \u2264 N then return to (B).\nAppendix B: Proof of Theorem 1\nTheorem 1 considers the error of a resampling algorithm as measured by:\nmKSD = max\n{\nmax\ni\n\u2223\u2223\u2223\u2223\u2223\ni\u2211\nj=1\nw(i) \u2212W (i)\n\u2223\u2223\u2223\u2223\u2223\n}\nFor SOR, if w(i) \u2265 \u03b1 then W (i) = w(i) with probability 1. As such we can consider\nthe mKSD solely for the subset of particles which have w(i) < \u03b1. Assume we have\nN \u2032 such particles, and relabel these particles c(1) < c(2) < . . . < c(N\n\u2032).\nThe only randomness in the SOR algorithm is the simulation of u in step (A) of the\nalgorithm detailed in Appendix A. Now for a given value of u\ni\u2211\nj=1\nW (i) = \u03b1\n[(\ni\u2211\nj=1\nw(i) + \u03b1\u2212 u\n)\n\/\u03b1\n]\n, (9)\nwhere [x] is the integer part of x. Thus for all u and i\u2223\u2223\u2223\u2223\u2223\ni\u2211\nj=1\nw(i) \u2212W (i)\n\u2223\u2223\u2223\u2223\u2223 \u2264 \u03b1,\nso mKSD \u2264 \u03b1.\nFor result (i) it suffices to note that if the probability of resampling particle c(i) is\nstrictly less than 1; then mKSD \u2265 w(i).\n25\nFor result (ii) it is sufficient to note that both the optimal resampling algorithm\nof Fearnhead and Clifford (2003) (where particles are shuffled prior to stratified\nresampling) and rejection control (where each particle with weight less then \u03b1 is\nresampled independently of all others) give positive probability to all realisation of\nweights W (1),W (2), . . . ,W (N) that our SOR algorithm does. It trivially follows that\nthe mKSD for these algorithms will be greater than that of our SOR algorithm.\nAppendix C: Error bound for SRC\nConsider N particles, ordered so that c(1) < c(2) < . . . < c(N). We denote the\nweight of these particles prior to resampling by w(i), the unnormalised weights after\nresampling by W (i), and the normalised weights after resampling by W\u00af (i). We let\nu denote the realisation of the Uniform [0, \u03b1] random variable used in the stratified\nresampling algorithm. Finally we let\n\u01eb(i) =\ni\u2211\nj=1\n(\nw(j) \u2212W (j)\n)\n.\nThe sum of the resampling weights depends on the number of particles resampled\nin stage SRC2. There exists a constant, \u03b2, satisfying 0 \u2264 \u03b2 < \u03b1 such that\nN\u2211\ni=1\nW (i) =\n\uf8f1\uf8f2\n\uf8f3 1 + \u03b1\u2212 \u03b2 u \u2264 \u03b2,1\u2212 \u03b2. u > \u03b2\nFix u and \u03b2. From (9) it can be shown that u\u2212 \u03b1 \u2264 \u01eb(i) \u2264 u for all i. We consider\nin turn the situation u \u2264 \u03b2 and u > \u03b2, corresponding to the two possible values of\nthe sums of the unnormalised weights after resampling.\nFirstly, assume u \u2264 \u03b2. Then we have\u2223\u2223\u2223\u2223\u2223\ni\u2211\nj=1\n(\nw(j) \u2212 W\u00af (j)\n)\u2223\u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223\u2223\ni\u2211\nj=1\n(\nw(j) \u2212W (j)\/(1\u2212 \u03b2 + \u03b1)\n)\u2223\u2223\u2223\u2223\u2223\n=\n1\n1 + \u03b1\u2212 \u03b2\n\u2223\u2223\u2223\u2223\u2223\u01eb(i) + (\u03b1\u2212 \u03b2)\ni\u2211\nj=1\nw(j)\n\u2223\u2223\u2223\u2223\u2223\n\u2264\n1\n1 + \u03b1\u2212 \u03b2\nmax\n{\nu+ (\u03b1\u2212 \u03b2)\ni\u2211\nj=1\nw(j), \u03b1\u2212 u\u2212 (\u03b1\u2212 \u03b2)\ni\u2211\nj=1\nw(j)\n}\n,\nwhere the two terms we are maximising over correspond to the largest positive and\nnegative values of \u01eb(i). Now, as u \u2264 \u03b2 and 0 <\n\u2211i\nj=1w\n(j) < 1, both these terms are\nbounded above by \u03b1. Thus we have mKSD < \u03b1 in this case.\n26\nNow if u > \u03b2, by a similar argument we obtain\u2223\u2223\u2223\u2223\u2223\ni\u2211\nj=1\n(\nw(j) \u2212 W\u00af (j)\n)\u2223\u2223\u2223\u2223\u2223 \u2264 11\u2212 \u03b2 max\n{\nu\u2212 \u03b2\ni\u2211\nj=1\nw(j), \u03b1\u2212 u+ \u03b2\ni\u2211\nj=1\nw(j)\n}\n\u2264\n\u03b1\n(1\u2212 \u03b2)\n.\nThe last inequality uses the fact that u \u2264 \u03b2 and 0 <\n\u2211i\nj=1w\n(j) < 1. Finally as\n\u03b2 < \u03b1 we can obtain that mKSD < \u03b1\/(1\u2212 \u03b1).\n"}