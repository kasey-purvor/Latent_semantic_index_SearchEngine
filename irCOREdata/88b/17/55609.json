{"doi":"10.1007\/s005210070023","coreId":"55609","oai":"oai:eprints.lincoln.ac.uk:1913","identifiers":["oai:eprints.lincoln.ac.uk:1913","10.1007\/s005210070023"],"title":"Feature selection using genetic algorithms and probabilistic neural networks","authors":["Hunter, Andrew"],"enrichments":{"references":[{"id":18437151,"title":"A Generalized Regression Neural Network.","authors":[],"date":"1991","doi":null,"raw":"Speckt, D.F. A Generalized Regression Neural Network.  IEEE Transactions on Neural Networks 2 (6), 568\u2014576, 1991.","cites":null},{"id":18437156,"title":"Analysis of hidden units in a layered network trained to classify sonar targets.","authors":[],"date":"1988","doi":"10.1016\/0893-6080(88)90023-8","raw":"Gorman, R.P. and Sejnowski, T.J. Analysis of hidden units in a layered network trained to classify sonar targets. Neural Networks 1 (1), 75\u201489, 1988.","cites":null},{"id":18437149,"title":"Classification of radar returns from the ionosphere using neural networks.","authors":[],"date":"1989","doi":null,"raw":"Sigillito, V. G., Wing, S. P., Hutton, L. V. and Baker, K. B. Classification of radar returns from the ionosphere using neural networks.  Johns Hopkins APL Technical Digest, 10, 262\u2014266, 1989.","cites":null},{"id":18437153,"title":"Crossing over Genetic Algorithms:","authors":[],"date":"1998","doi":null,"raw":". Hunter, A. Crossing over Genetic Algorithms: the SUGAL Generalised GA, Heuristics 4 (2), 179\u2014192, 1998.","cites":null},{"id":18437148,"title":"Feature Selection: Evaluation, Application and Small Sample Performance","authors":[],"date":"1997","doi":"10.1109\/34.574797","raw":"Jain, A. and Zongker, D. Feature Selection: Evaluation, Application and Small Sample Performance IEEE Trans. Pattern Analysis and Machine Intelligence, 19 (2), 1997.","cites":null},{"id":18437157,"title":"Feature Subset Selection Using a Genetic Algorithm.","authors":[],"date":"1998","doi":"10.1007\/978-1-4615-5725-8_8","raw":"Yang, J. and Honavar, V. Feature Subset Selection Using a Genetic Algorithm. IEEE Int. Systems and their Applications 13 (2), 44\u201449, 1998. FORWARD GA (1) GA (2)","cites":null},{"id":18437147,"title":"Genetic Algorithms.","authors":[],"date":"1989","doi":"10.1109\/robot.1989.100000","raw":". Goldberg, D. E. Genetic Algorithms. Reading, MA: Addison Wesley, 1989.","cites":null},{"id":18437150,"title":"Probabilistic Neural Networks.","authors":[],"date":"1990","doi":"10.1016\/0893-6080(90)90049-q","raw":"Speckt, D.F. Probabilistic Neural Networks. Neural Networks 3 (1), 109--118, 1990.","cites":null},{"id":18437154,"title":"Simultaneous Feature Extraction and Selection Using a Masking Genetic Algorithm.","authors":[],"date":"1997","doi":null,"raw":"Raymer, M.L., Punch, W.F., Goodman, E.D., Sanschagrin, P.C. and Kuhn, L.A. Proc. Simultaneous Feature Extraction and Selection Using a Masking Genetic Algorithm.  7 th Int. Conf. on Genetic Algorithms, 561\u2014567, Morgan Kaufmann, San Francisco, June 1997.","cites":null},{"id":18437146,"title":"UCI repository of machine learning databases. http:\/\/www.ics.uci.edu\/~mlearn\/MLRepository.html. I rvine, CA:","authors":[],"date":null,"doi":null,"raw":"Blake, C., Keogh, E. and Merz, C.J. UCI repository of machine learning databases. http:\/\/www.ics.uci.edu\/~mlearn\/MLRepository.html. I rvine, CA: University of California, Dept. Information and Computer Science.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2000-07-07","abstract":"Selection of input variables is a key stage in building\\ud\npredictive models, and an important form of data mining. As exhaustive evaluation of potential input sets using full non-linear models is impractical, it is necessary to use simple fast-evaluating models and heuristic selection strategies. This paper discusses a fast, efficient, and powerful nonlinear input selection procedure using a combination of Probabilistic Neural Networks and repeated\\ud\nbitwise gradient descent. The algorithm is compared\\ud\nwith forward elimination, backward elimination and genetic algorithms using a selection of real-world data sets. The algorithm has comparative performance and greatly reduced execution time with respect to these alternative approaches. It is demonstrated empirically that reliable results cannot be gained using any of these approaches without the use of resampling","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/55609.pdf","fullTextIdentifier":"http:\/\/eprints.lincoln.ac.uk\/1913\/1\/FeatureSelectionGAPNN.pdf","pdfHashValue":"262245bc28508201fe608d827042eab1c093cd74","publisher":"Springer","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lincoln.ac.uk:1913<\/identifier><datestamp>\n      2013-03-13T08:33:00Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47373030<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47373630<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47373330<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lincoln.ac.uk\/1913\/<\/dc:relation><dc:title>\n        Feature selection using genetic algorithms and probabilistic neural networks<\/dc:title><dc:creator>\n        Hunter, Andrew<\/dc:creator><dc:subject>\n        G700 Artificial Intelligence<\/dc:subject><dc:subject>\n        G760 Machine Learning<\/dc:subject><dc:subject>\n        G730 Neural Computing<\/dc:subject><dc:description>\n        Selection of input variables is a key stage in building\\ud\npredictive models, and an important form of data mining. As exhaustive evaluation of potential input sets using full non-linear models is impractical, it is necessary to use simple fast-evaluating models and heuristic selection strategies. This paper discusses a fast, efficient, and powerful nonlinear input selection procedure using a combination of Probabilistic Neural Networks and repeated\\ud\nbitwise gradient descent. The algorithm is compared\\ud\nwith forward elimination, backward elimination and genetic algorithms using a selection of real-world data sets. The algorithm has comparative performance and greatly reduced execution time with respect to these alternative approaches. It is demonstrated empirically that reliable results cannot be gained using any of these approaches without the use of resampling.<\/dc:description><dc:publisher>\n        Springer<\/dc:publisher><dc:date>\n        2000-07-07<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lincoln.ac.uk\/1913\/1\/FeatureSelectionGAPNN.pdf<\/dc:identifier><dc:identifier>\n          Hunter, Andrew  (2000) Feature selection using genetic algorithms and probabilistic neural networks.  Neural Computing & Applications, 9  (2).   pp. 124-132.  ISSN 0941-0643  <\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1007\/s005210070023<\/dc:relation><dc:relation>\n        10.1007\/s005210070023<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lincoln.ac.uk\/1913\/","http:\/\/dx.doi.org\/10.1007\/s005210070023","10.1007\/s005210070023"],"year":2000,"topics":["G700 Artificial Intelligence","G760 Machine Learning","G730 Neural Computing"],"subject":["Article","PeerReviewed"],"fullText":"Abstract \nSelection of input variables is a key stage in build-\ning predictive models, and an important form of \ndata mining. As exhaustive evaluation of potential \ninput sets using full non-linear models is impracti-\ncal, it is necessary to use simple fast-evaluating \nmodels and heuristic selection strategies. This pa-\nper discusses a fast, efficient, and powerful non-\nlinear input selection procedure using a combina-\ntion of Probabilistic Neural Networks and repeated \nbitwise gradient descent. The algorithm is com-\npared with forward elimination, backward elimina-\ntion and genetic algorithms using a selection of \nreal-world data sets. The algorithm has comparative \nperformance and greatly reduced execution time \nwith respect to these alternative approaches. It is \ndemonstrated empirically that reliable results can-\nnot be gained using any of these approaches with-\nout the use of resampling. \n1 Introduction \nIn many machine learning domains, the objective is to infer a \nmodel that allows one or more output (dependent) variables \nto be predicted given the values of input variables (inde-\npendent variables, or features \u2013 we will use the terms input \nvariable and feature interchangeably throughout this paper). \nA wide variety of modeling techniques can be used to form \nthe prediction, including conventional statistical models \nsuch as linear (least squares) models and logistic regression, \nclustering algorithms, neural networks, fuzzy logic and \nneuro-fuzzy techniques, and decision trees. Irrespective of \nthe modeling technique, a key issue is to determine which of \nthe available input variables should be used in modeling - \nthis is known as feature selection. Typically, the model must \nbe inferred from a set of historical data, D, that includes a \nnumber, N, of cases (or vectors), cj, each containing values \nfor an output variable, o j, together with the associated vector \nof V input variables, xj, and the feature subset must be se-\nlected on the basis of this same data set. \nFeature selection is non-trivial for a number of reasons. First, \nvariables are seldom entirely independent \u2013 there may be \nredundancy (where two or more variables are correlated so \nthat it is not necessary to include all of them in modeling \u2013 \nthe most extreme example occurs when a variable is simply \nreplicated), and interdependence (where two or more vari-\nables between them convey important information that is \nobscure if any of them is included on its own \u2013 the well-\nknown two-spirals problems demonstrates the case of two \ninterdependent variables). Second, it may actually be benefi-\ncial to discard variables that have some low level of genuine \ninformation, as the \u201ccurse of dimensionality\u201d implies that \nsmaller models generalize better, and we often encounter the \nproblem where the number of cases available is small with \nrespect to the number of variables. \nAs a consequence of these problems, the only way to select \nthe optimal feature subset with certainly is to evaluate all \npossible combinations, of which there are 2V for a V variable \nproblem. This means building 2V models, and if the modeling \nprocess itself is subject to experimental variability (for exa m-\nple, knowing the input variables to a neural network, we \nmust still determine the number of hidden units, and train \nmany times to avoid local minima) then each of the 2V \nevaluations of variable subsets may itself be extremely com-\nputationally expensive. Even if exhaustive evaluation is pos-\nsible, the variable subset selected may be dependent on the \ntraining data used, which is itself a sample from an unknown \ndistribution, and therefore the results are unreliable. \nIn reality, exhaustive evaluation is not practical for more than \na few input variables. It is common practice to apply heuris-\ntic algorithms based on a smaller number of evaluations, \nsuch as forward stepwise and backward stepwise selection. \nWe may also reduce the computational burden by perform-\ning feature selection using some quick to evaluate model; for \nexample, by using a linear model for feature selection even if \nthe model that will ultimately be deployed is non-linear [Jain \nand Zongker, 1997]. \nThis paper discusses the application of a feature selection \nalgorithm using a combination of repeated bitwise gradient \ndescent and Probabilistic Neural Networks [Speckt, 1990]. \nThe algorithm is compared with forward stepwise, backward \nstepwise, and genetic algorithms. The algorithms are evalu-\nated using a selection of real-world data sets drawn from the \nUCI machine learning repository [Blake et. al., 1998]. The \nnew algorithm is shown to be effective in selecting feature \nFeature Selection using Genetic Algorithms and Probabilistic Neural Networks \nContent Areas: neural networks, genetic algorithms, data mining \nTracking Number: A694 \n \n \nsubsets, and extremely efficient. In addition, it can be used \nto differentiate between important and ambiguous variables. \n2. Evaluation of Feature Subsets  \nThe feature selection task can be conveniently represented \nas a binary string search problem. A feature selection algo-\nrithm searches for a binary string, S, with the number of bits \nequal to the number of candidate input features, V; si=0 in-\ndicates that a feature should not be used; si=1 indicates that \nit should be used. Such a string is sometimes referred to as a \nmask. Any given mask can be evaluated by building a model \nusing the indicated combination of inputs, and assessing its \nperformance. Feature selection algorithms therefore have \ntwo key parts: a search algorithm that generates candidate \nmask strings, and an evaluation algorithm that assigns a \nperformance rating to the strings (the performance may be \nused by the search algorithm to guide the generation of new \ncandidate masks for evaluation). \nThis section describes the evaluation algorithm used in this \npaper; the next section describes the new search algorithm \nand the benchmark search algorithms. \nProbabilistic neural networks (PNNs) are simple non-linear \nmodeling techniques that have modest computational re-\nquirements for a reasonably small data set. Probabilistic neu-\nral networks are used for classification problems [Speckt, \n1990], where the objective is to assign cases to one of a \nnumber of discrete classes. The output of the model is an \nestimate of the class membership probabilities. This paper \nconcentrates on the application of feature selection in classi-\nfication problems; however, the techniques describe extend \ntrivially to the case of regression problems (where the output \nis a continuous variable) using Generalized Regression Neu-\nral Networks [Speckt, 1991], a closely related technique with \nsimilar performance characteristics. \nProbabilistic Neural Networks estimate the probability den-\nsity functions (p.d.f.s) using the training data set in a very \ndirect fashion, and then assign class membership probabili-\nties to new cases by using the p.d.f. estimates. The class \np.d.f.s are estimated by adding together kernel functions \n(typically Gaussians) located at each case in the training set. \nIntuitively, the presence of a case in the training set can be \ntaken as evidence of some probability density at that point, \nand of (somewhat lower) probability density at nearby \npoints. Where there is a cluster of training cases belonging \nto the same class, the probability estimate for that class will \nbe high as a number of overlapping kernel functions are \nadded together. \nThe PNN estimates the probability that a new case, x, belong \nto class i as: \n \n \nwhere xij is the jth training case belonging to class i, k i is the \nnumber of training cases in class i, and ?  is the smoothing \nfactor, which is determined experimentally. \nThe PNN is constructed as a neural network using three lay-\ners: input layer, pattern units, and summation\/output units. \nThe input layer distributes inputs to the next layer. The pat-\ntern units each contain a weight vector that is a copy of a \ncase from the training set. These units calculate the squared \nEuclidean distance of the pattern unit vector from the input, \ndivide this by 2? 2, and then calculate the exponential of the \nnegative of this. They thus form a Gaussian function cen-\ntered at the training case. The third layer contains one unit \nfor each class, and each of these units is connected only to \npattern units of that class. The weights on these connec-\ntions are all one. At the output layer, the activations are \nnormalized to sum to one; thus, the constant in the formula \nabove can be ignored. Modifications may also be made to \naccount for known disparities between prior class distribu-\ntions and the distribution in the training set, and to incorpo-\nrate a loss matrix if the cost of misclassification varies from \nclass to class. \nThe only variable that needs to be optimized in a PNN is the \nsmoothing factor. This is easily done experimentally. The \ndata set, D, is divided into two subsets: a training set, T, and \na test set, X (typically with equal numbers of randomly se-\nlected cases). A line search algorithm is used to select the \nsmoothing factor, by building a PNN for each smoothing \nfactor, and assessing its performance using a composite \nerror function applied to the test cases. The error function \nmight be the correct classification rate, the sum of the test \ncase cross-entropies, or the sum-squared error function. \nThis paper uses the latter, as it is commonly applied in the \nneural network community: \n \nPNNs are not too sensitive to the precise choice of smooth-\ning factor, and it is sufficient to optimize the parameter once \nbefore commencing the feature selection process; the com-\nputational burden is therefore negligible. \nTo evaluate a feature subset, we use the same error function, \nin this case with a fixed smoothing factor, but with the input \nvariables varying. \nFrom the point of view of feature selection, the PNN has \nsignificant advantages over other forms of neural network, \nand even over linear modeling. First, there is no \u201ctraining \nalgorithm\u201d to speak of. A PNN is \u201ctrained\u201d by recording the \ntraining cases in the hidden layer, and setting the connec-\ntions to the output layer to indicate the class. It is not even \nnecessary to do this - execution of a PNN can be simulated \ndirectly within the data set, so that there is no training phase \n?\n? ?\n?\n?\n?\n?\n?\n?\n? ??\n??\nik\nj\nij\nT\nij\ni\nvvi\nxxxx\nk\nxf\n1\n22\/ 2\n)()(\nexp\n1\n2\n1\n)(\n???\n? ? ??\nX O\nii oxfE\n2))((?\nat all. This contrasts with neural networks such as standard \nmultilayer perceptrons, which require an extensive period of \ntraining (Speckt [1990] quotes a 200,000 times speedup com-\npared with back propagation on one particular problem). \nSecond, once the smoothing factor has been fixed there are \nno variable training parameters, so that repeated execution is \nnot necessary. Third, the PNN is non-linear and is capable of \nmodeling arbitrarily complex problems. It is therefore a more \nintuitively appealing option than the commonly-selected \nalternative, linear modeling, if the problem domain is known \nor suspected to be non-linear. \nThe computational cost of evaluating a feature set using a \nPNN is, however, quite high. The execution time of the net-\nwork is proportional to NTV, where NT is the number of train-\ning cases. To evaluate a feature set, the PNN is executed on \neach of the test cases, so the total evaluation time is propor-\ntional to NTNXV, where NX is the number of verification \ncases. As NT and NX sum to N, the total number of cases \navailable (typically half are used in training and half in verifi-\ncation), the execution time is proportional to N2V. This cost \nbecomes quite significant if the number of cases is large, and \nis certainly much greater than the execution time for a linear \nor multilayer perceptron model. Nonetheless, given that \ntraining is the dominant element in the use of most neural \nnetworks, the disadvantage in execution time is greatly out-\nweighed by the advantage in training time. \nOne approach to reducing computational cost is to sub-\nsample cases during evaluation. Execution speed may be \nincreased by selecting a sub-sample of the cases, dividing \nthis into a training and test set, and evaluating on that. As \nthe computational cost is proportion to N2, halving the sam-\nple size quarters the evaluation time, so the effect can be \nvery significant. This is a valuable alternative if the data set \ncontains many cases. Of course, reducing the number of \ncases used to form the model will also make the selection \nprocedure more prone to random errors. However, the data \nsets we have used in this paper have relatively few cases for \nthe number of variables, and we have not employed sub-\nsampling. \n3. Selection algorithms  \nThe PNN can be used, as described in the previous section, \nto evaluate an input variable mask. This evaluation capabil-\nity can be plugged into any algorithm that searches for bi-\nnary strings. This paper compares a very simple approach \u2013 \nbitwise gradient descent from a random starting point - with \nthree popular approaches to feature selection: forward step-\nwise, backward stepwise, and the genetic algorithm. \nIn forward stepwise selection, a feature subset is iteratively \nbuilt up [Jain and Zongker, 1997]. On the first iteration, N \nmodels are tested, each of which uses a single input variable \n(corresponding to masking strings consisting of all zeros \nwith a one in a single position). The mask with the lowest \nerror is selected, indicating that the variable that gives the \nbest performance on its own should be selected first. On \neach subsequent iteration, each of the unused variables is \nadded to the model in turn, and the variable that most im-\nproves the model is selected. The algorithm terminates when \nadding an extra variable results in no improvement in per-\nformance (it is also possible to consider versions where the \nalgorithm terminates if the improvement falls below some \nthreshold, indicating an acceptable complexity\/performance \ntrade-off). \nIn backward stepwise selection, the algorithm starts by \nbuilding a model that includes all available input variables. \nOn each iteration, the algorithm locates the variable that, if \nremoved, most improves the performance (or causes least \ndeterioration). The algorithm terminates when removing a \nvariable results in no deterioration in performance (it is also \npossible to consider versions where the algorithm terminates \nif the deterioration rises above some threshold). \nA problem with forward selection is that it may fail to include \nvariables that are interdependent, as it adds variables one at \na time. However, it may locate small, effective, subsets quite \nrapidly, as the early evaluations, involving relatively few \nvariables, are fast. In contrast, in backwards selection inter-\ndependencies are well-handled, but early evaluations are \nrelatively expensive. In either case, the maximum number of \npossible evaluations is V(V-1)\/2, which is potentially quite \nsubstantial. \nThe genetic algorithm [Goldberg, 1989] is a well-known ap-\nproach for selecting binary strings, and a number of authors \nhave suggested its use for feature selection [Yang and Ho-\nnavar, 1998; Raymer et.al., 1996].  An important aspect of the \ngenetic algorithm is that it is explicitly designed to exploit \nepistasis (that is, interdependencies between bits in the \nstring), and thus should be well-suited for this problem do-\nmain. However, Genetic Algorithms typically require a large \nnumber of evaluations to reach a minimum (a population of \n100 strings, evaluated over 100 generations, for a total 10,000 \nevaluations is commonplace). This implies that the Genetic \nAlgorithm is only likely to require less evaluations than for-\nward or backward stepwise algorithms if the number of vari-\nables is very large (100 or more). We also note that 10,000 \nevaluations is sufficient to exhaustively evaluate all possible \ncombinations of up to 13 variables. However, the genetic \nalgorithm might achieve better results than forward or back-\nward selection for feature sets of between 14 and 20 vari-\nables. \nThis paper introduces a simple and effective approach for \nfeature selection called bitwise gradient descent. The algo-\nrithm starts with a randomly initialized string. It then \u201cflips\u201d \neach bit in the string in turn, retaining the changed bit only if \nthe change causes a reduction in error. The total number of \nevaluations is only V - substantially less than the other algo-\nrithms described above. If the variables are not interdepend-\nent, then this approach will yield an optimum solution. Even \nif they are some modest interdependencies involving a cou-\nple of variables, repeated application is likely to discover \nthese. In addition, repeated application of the algorithm \ngives valuable information about the importance of the indi-\nvidual variables, as will be described below. \n4. Resampling  \nA key issue in all the feature selection algorithms is the divi-\nsion of the available data into the training and test subsets. \nSome variables may be of great individual importance, and \nwill be selected by any of the algorithms describe above. \nHowever, if variables are of marginal importance, or are mu-\ntually redundant, then their pre sence in the selected subset \nmay be strongly influenced by the division between the \ntraining and test subsets. This problem is clearly present in \nall the data sets used in these experiments - repeating any of \nthe feature selection algorithms with different training\/test \nsubset selections invariably produces different results. \nA practical solution to this problem is to repeat the fe ature \nselection process a number of times, counting the number of \noccasions on which each variable is selected, and to regard \nthe frequency distribution of the variables as the output of \nthe feature selection procedure. This is more informative and \nmore useful than a straightforward single evaluation, the \nresults of which are extremely suspect. \nIt is the combination with resampling that makes bitwise gra-\ndient descent particularly effective. The algorithm is efficient \nenough to be repeated a moderately large number of times, \nand resampling helps to avoid problems where particular \ninterdependencies between variables are entirely missed. \n5. Experiments  \nFour data sets were selected from the UCI machine learning \nrepository [Blake et. al., 1998]. These are all real-world prob-\nlem domains, and all have a large number of input variables \nand a relatively small number of cases. The data sets were \nnot artificially chosen to demonstrate particular issues such \nas interdependency between fe atures, reflect a range of vari-\nable types, and include missing value1. The data sets are \nbriefly described below: \nAnneal. 798 cases, 37 variables (9 numeric, 29 nominal), 6 \noutput classes, no missing values. \nHorse colic. 368 cases, 27 variables (3 numeric, 24 nominal), \n3 output classes, 30% missing values. Prediction of survival \nof horses with colic. \nIonosphere. 351 cases, 34 variables (all continuous, 2 output \nclasses, no missing values. Distinguish radar measurements \nwhich show structure. [Sigillito et. al., 1989] \nSonar. 208 cases, 60 variables, 2 output classes. Distinguish \nrocks from mines on sea bed. [Gorman and Sejnowski, 1988] \nAll data sets are randomly divided into training and test \nsubsets of equal size. Nominal variables we re encoded using \nstandard binary encoding (two -state) or one-of-N encoding \n(3 or more state) techniques. Numeric variables were normal-\nized into the range [0,1] using the minimax procedure. \n                                                \n1 Missing values are substituted using the mean training value \nof numeric variables, and the training set distributions for \nnominal variables. \n The bitwise gradient descent algorithm was repeated 20 \ntimes for each data set, with a random starting string and \nrandom division into training and test cases on each execu-\ntion; the variable selection frequencies are discussed below. \nThe forward and backward stepwise selection algorithms \nwere each repeated 5 times (the reduced number of experi-\nments is due to the greatly increased execution time) with \nrandom division into training and test cases on each execu-\ntion. \nAs the genetic algorithm is itself a stochastic population-\nbased algorithm, it was speculated that a similar frequency-\nbased approach could be applied to the final population of \nthe algorithm. The algorithm was repeated twice for each of \nthe Ionosphere and Horse Colic data sets (as a consequence \nof the extreme execution time, only a limited number of ex-\nperiments were conducted), and the results compared for \nconsistency. There are a large number of control parameters \nthat can be altered in a genetic algorithm. For the purposes \nof these experiments, the following factors were selected: a \nstandard genetic algorithm with elitism, mutation rate aver-\nage 1.0 per string, crossover one-point, rate 0.3, selection by \nexpected value roulette method, fitness linearly norma lized \nfor constant bias (fittest member of each population 5 times \nfitter than least fit member). These correspond to the SUGAL \nsettings [Hunter, 1998] replacement uniform, replace-\nment_condition unconditional, replacement_rate 1.0, elit-\nism on, mutation invert, mutation_rate 1.0, muta-\ntion_rate_type per_chromosome, crossover onepoint, \ncrossover_rate 0.3, selection integral_roulette, normalisa-\ntion reverse_scale, bias 5.0 . These settings were selected \n\u201cby eye\u201d on the basis of some initial experiments. \nOnce completed, the individual runs of each algorithm were \nassembled into frequency tables, giving the percentage of \nruns of each algorithm that each feature was selected. These \nfrequency tables were examined in graphical format; see fig-\nure 1. Due to space considerations in this paper, only ex-\ntracts from some of the frequency distributions have been \nincluded in detail. Table 1 summarizes the overall perform-\nance, showing the average percentage disparities between \nthe feature selection frequencies of the forward and back-\nward stepwise algorithms, and of the forward and bitwise \nalgorithms. If the results of the algorithms were entirely unre-\nlated, we would expect figures of approximately 50%. The \nfigures are substantially lower than this.  \n \n Ionos H. Colic Sonar Anneal \nFwd-Bwd 25.3 23.0 25.7 2.6 \nFwd-Bit 22.9 27.6 37.3 12.1 \nBwd-Bit 17.9 23.1 20.9 11.1 \nTable 1: Average disparities in frequencies per bit \n5.1. Comparative performance  \nFigure 1 shows the frequency of feature selection by forward \nstepwise, backward stepwise and bitwise gradient descent \nfor the first ten features in the Ionosphere data set. \n \nThis is typical of most of the data sets \u2013 the algorithms give \nconsistent results on most of the features. The bitwise gra-\ndient descent algorithm consistently selects features that are \nselected by both of the other algorithms, and consistently \nrejects features that they would also reject. Where there is a \nnoticeable difference between the bitwise gradient descent \nfrequencies and the stepwise algorithms, there also tend to \nbe differences between the stepwise algorithms. On the An-\nneal data set, the agreement is striking, with the algorithms \nselecting the same feature subset with close to 100% consis-\ntency. Here, the bitwise gradient descent algorithm does \nhave significantly different frequencies to the other algo-\nrithms, as marked by the disparity in table 1. However, on \ncloser analysis this disparity is due to the bitwise algorithm \nselecting frequencies in the range below 20% or above 80%, \nas opposed to consistent 0% and 100% from the other algo-\nrithms. In practice, this would not affect the feature set se-\nlected. The Horse Colic and So nar data sets are more chal-\nlenging \u2013 there the choice of variables seems to be, to a sig-\nnificant extent, arbitrary. This is indicated by a tendency for \nthe selection frequencies to be less markedly extreme (often \nin the range 30-70%) and, unsurprisingly, this ambiguity is \nreflected in disparities between the frequencies recorded by \nthe three methods; see figure 2. \nIt is difficult to find any evidence that the bitwise gradient \ndescent algorithm fails to find interdependent features in any \nof the data sets. We would expect, in that case, to find fea-\ntures that are selected with close to zero frequency by for-\nward and bitwise selection and with high frequency by \nbackward selection. This may reflect the fact that complete \ninterdependence is, in reality, very rarely encountered, so \nthat the putative advantage of backward selection in being \nable to handle interdependence is largely theoretical. \n5.2. Sensitivity to data set division \nA far more pertinent issue is the sensitivity of all the algo-\nrithms to the division of the data set into training and test \ncases. For example, in the Sonar data set forward selection \nchooses anywhere from 48 to 56 variables, in five tests, and \nbetween 31 and 54 variables in backward selection! In the \nIonosphere data, forward selection yields between 13 and 32 \nvariables, and backward selection from 25 to 33. This implies \nthat to apply any of the algorithms without resampling is \nextremely deceptive. \nHowever, if the feature selection algorithm is run repeatedly \nwith resampling, and a frequency table assembled, the infor-\nmation yielded is extremely useful. It is possible to identify \ndefinitely useful or useless variables, and to distinguish \nthese from the ambiguous variables. If a very large number \nof the variables are ambiguous, then we may conclude that \nthere is a high level of correlation between variables, and \nperhaps look to perform some feature extraction, such as \nprincipal component analysis, before continuing with the \nnext stage of the analysis.  \n5.3. The genetic algorithm \nThe genetic algorithm is computationally extremely demand-\ning compared with the other techniques. A naive way to use \nthe genetic algorithm in feature selection is to run it for the \nrequisite number of generations, then to select the best \nmember of the final population as the result of the algorithm. \nThe entire algorithm can then be run a number of times, just \nas with the other algorithms, to check for consistency. \nHowever, this approach ignores the information available in \nthe final population, which contains a large number of can-\ndidate solutions. One might expect that the genetic algorithm \nwould heavily select bits for features that are definitely use-\nful or useless, while bits that are ambiguous would be sub-\nject to contrary selective pressures and thus remain more \ndiverse. To test this theory, the genetic algorithm was run on \ntwo of the data sets, and the frequency distributions of the \nbits in the final population compared between these two \nruns. \nFORWARD\nBACKWARD\nBITWISE\nFigure 2: Feature selection frequency, Horse Colic\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nFORWARD\nBACKWARD\nBITWISE\nFigure 1: Feature selection frequency, Ionosphere\n0\n20\n40\n60\n80\n100\nFigure 3 shows the results on the Ionosphere data set, which \nare clearly disappointing (the forward selection frequencies \nfrom figure 1 are repeated, for ease of comparison). It is clear \nthat the two runs of the genetic algorithm produce quite \nradically different frequency distributions for each bit, which \ncontrasts unfavorably with the consistent results produced \nby the other three algorithms. This may be due to \u201cparasit i-\ncal\u201d effects, where bits with a low fitness contribution are \npropagated because they are located on strings which have \nhigh fitness because of more influential bit settings, or it may \nbe a result of \u201cgenetic drift\u201d (the tendency of unused bits to \ndrift towards the extremes over time). \nWhatever the reason, the genetic algorithm is clearly less \nuseful as a frequency-based feature selection algorithm, al-\nthough it should be emphasized that if treated in the conven-\ntional fashion (the best string being selected as the single \noutput of the algorithm) the results are comp arable with the \nother algorithms \u2013 although with significantly greater execu-\ntion time. \n6. Conclusion \nThis paper has presented a new, efficient, feature sele ction \nalgorithm based on repeated bitwise gradient descent com-\nbined with Probabilistic Neural Networks. The algorithm has \nacceptable computational requirements (taking only a few \nminutes for twenty runs on all the data sets used in this pa-\nper, on a Pentium 266; this compares with up to an hour for \neach run of the stepwise algorithms, and up to ten hours for \na single run of the genetic algorithm). Comparison with stan-\ndard forward and backward stepwise algorithms shows that \nthe new algorithm yields equally good results, at far greater \nspeeds. \nAn analysis of resampling effects shows that it is critical to \nperform feature selection a number of times, and to base fea-\nture selection on frequencies of feature selection rather than \na single run. The proposed algorithm is well-suited for this \ntask. \nReferences \n[Blake et. al., 1998] Blake, C., Keogh, E. and Merz, C.J. UCI \nrepository of machine learning databases. \nhttp:\/\/www.ics.uci.edu\/~mlearn\/MLRepository.html. Irvine, \nCA: University of California, Dept. Information and Com-\nputer Science. \n \n[Goldberg, 1989]. Goldberg, D. E. Genetic Algorithms. Read-\ning, MA: Addison Wesley, 1989. \n \n[Jain and Zongker, 1997] Jain, A. and Zongker, D. Feature \nSelection: Evaluation, Application and Small Sample Per-\nformance IEEE Trans. Pattern Analysis and Machine Intel-\nligence, 19 (2), 1997. \n \n[Sigillito et. al., 1989] Sigillito, V. G., Wing, S. P., Hutton, L. V. \nand Baker, K. B. Classification of radar returns from the \nionosphere using neural networks. Johns Hopkins APL \nTechnical Digest, 10, 262\u2014266, 1989. \n \n[Speckt, 1990] Speckt, D.F. Probabilistic Neural Networks. \nNeural Networks 3 (1), 109--118, 1990. \n \n[Speckt, 1991] Speckt, D.F. A Generalized Regression Neural \nNetwork. IEEE Transactions on Neural Networks 2 (6), \n568\u2014576, 1991. \n \n[Hunter, 1998]. Hunter, A. Crossing over Genetic Algorithms: \nthe SUGAL Generalised GA, Heuristics 4 (2), 179\u2014192, 1998. \n \n[Raymer et.al., 1997] Raymer, M.L., Punch, W.F., Goodman, \nE.D., Sanschagrin, P.C. and Kuhn, L.A. Proc. Simultaneous \nFeature Extraction and Selection Using a Masking Genetic \nAlgorithm. 7th Int. Conf. on Genetic Algorithms, 561\u2014567, \nMorgan Kaufmann, San Francisco, June 1997. \n \n[Gorman and Sejnowski, 1988] Gorman, R.P. and Sejnowski, \nT.J. Analysis of hidden units in a layered network trained to \nclassify sonar targets. Neural Networks 1 (1), 75\u201489, 1988. \n \n[Yang and Honavar, 1998] Yang, J. and Honavar, V. Feature \nSubset Selection Using a Genetic Algorithm. IEEE Int. Sys-\ntems and their Applications 13 (2), 44\u201449, 1998. \nFORWARD\nGA (1)\nGA (2)\nFigure 3: Genetic Algorithm, Ionosphere\n0\n20\n40\n60\n80\n100\n"}