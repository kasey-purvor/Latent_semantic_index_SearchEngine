{"doi":"10.1016\/j.compchemeng.2009.01.014","coreId":"140455","oai":"oai:dspace.lib.cranfield.ac.uk:1826\/6047","identifiers":["oai:dspace.lib.cranfield.ac.uk:1826\/6047","10.1016\/j.compchemeng.2009.01.014"],"title":"Bidirectional branch and bound for controlled variable selection. Part II: exact\nlocal method for self-optimizing control","authors":["Kariwala, Vinay","Cao, Yi"],"enrichments":{"references":[{"id":37938648,"title":"A branch and bound algorithm for feature subset selection.","authors":[],"date":"1977","doi":"10.1109\/tc.1977.1674939","raw":"P. Narendra and K. Fukunaga. A branch and bound algorithm for feature subset selection. IEEE Trans. Comput., C-26:917{922, 1977.","cites":null},{"id":37938654,"title":"A more ecient branch and bound algorithm for feature selection. Pattern Recognition,","authors":[],"date":"1993","doi":"10.1016\/0031-3203(93)90054-z","raw":"B. Yu and B. Yuan. A more ecient branch and bound algorithm for feature selection. Pattern Recognition, 26:883{889, 1993.","cites":null},{"id":37938653,"title":"A review of methods for input\/output selection.","authors":[],"date":"2001","doi":"10.1016\/s0005-1098(00)00181-3","raw":"M. Van de Wal and B. de Jager. A review of methods for input\/output selection. Automatica, 37(4): 487{510, 2001.","cites":null},{"id":37938637,"title":"An improved branch and bound algorithm for feature selection.","authors":[],"date":"2003","doi":"10.1016\/s0167-8655(03)00020-5","raw":"X.-W. Chen. An improved branch and bound algorithm for feature selection. Pattern Recognition Letters, 24:1925{1933, 2003.","cites":null},{"id":37938634,"title":"Bidirectional branch and bound for controlled variable selection: Part I. Principles and minimum singular value criterion.","authors":[],"date":"2008","doi":"10.1016\/j.compchemeng.2007.11.011","raw":"Y. Cao and V. Kariwala. Bidirectional branch and bound for controlled variable selection: Part I. Principles and minimum singular value criterion. Comput. Chem. Engng., 32(10):2306{2319, 2008.","cites":null},{"id":37938649,"title":"Dynamics and control of distillation columns - A tutorial introduction.","authors":[],"date":"1997","doi":"10.4173\/mic.1997.3.1","raw":"S. Skogestad. Dynamics and control of distillation columns - A tutorial introduction. Trans. IChemE Part A, 75:539{562, 1997.","cites":null},{"id":37938652,"title":"Fast branch and bound algorithm in feature selection.","authors":[],"date":"2000","doi":"10.1109\/tpami.2004.28","raw":"P. Somol, P. Pudil, F. Ferri, and J. Kittler. Fast branch and bound algorithm in feature selection. In B. Sanchez, J. Pineda, J. Wolfmann, Z. Bellahsense, and F. Ferri, editors, Proceedings of World Multiconference on Systemics, Cybernetics and Informatics, volume VII, pages 1646{651, Orlando, Florida, USA, 2000.","cites":null},{"id":37938636,"title":"Globally optimal control structure selection using branch and bound method.","authors":[],"date":"1998","doi":"10.1016\/s1570-7946(03)80441-8","raw":"Y. Cao, D. Rossiter, and D. H. Owens. Globally optimal control structure selection using branch and bound method. In Proc. 5th International Symposium on DYCOPS, pages 183{188, Corfu, Greece, 1998.","cites":null},{"id":37938635,"title":"Improved branch and bound method for control structure screening.","authors":[],"date":"2005","doi":"10.1016\/j.ces.2004.10.025","raw":"Y. Cao and P. Saha. Improved branch and bound method for control structure screening. Chem. Engg. Sci., 60(6):1555{1564, 2005.","cites":null},{"id":37938645,"title":"Local self-optimizing control with average loss minimization.","authors":[],"date":"2008","doi":"10.1021\/ie070897+","raw":"V. Kariwala, Y. Cao, and S. Janardhanan. Local self-optimizing control with average loss minimization. Ind. Eng. Chem. Res., 47(4):1150{1158, 2008.","cites":null},{"id":37938651,"title":"Multivariable Feedback Control: Analysis and Design.","authors":[],"date":"1996","doi":"10.1002\/(sici)1099-1115(199803)12:2<223::aid-acs470>3.0.co;2-f","raw":"S. Skogestad and I. Postlethwaite. Multivariable Feedback Control: Analysis and Design. John Wiley & sons, Chichester, UK, 1st edition, 1996.","cites":null},{"id":37938632,"title":"Null space method for selecting optimal measurement combinations as controlled variables.","authors":[],"date":"2007","doi":"10.1021\/ie060285+","raw":"V. Alstad and S. Skogestad. Null space method for selecting optimal measurement combinations as controlled variables. Ind. Eng. Chem. Res., 46(3):846{853, 2007.","cites":null},{"id":37938642,"title":"Optimal measurement combination for local self-optimizing control.","authors":[],"date":"2007","doi":"10.1021\/ie0610187","raw":"V. Kariwala. Optimal measurement combination for local self-optimizing control. Ind. Eng. Chem. Res., 46(11):3629{3634, 2007.","cites":null},{"id":37938633,"title":"Optimal measurement combinations as controlled variables.","authors":[],"date":"2009","doi":"10.1016\/j.jprocont.2008.01.002","raw":"V. Alstad, S. Skogestad, and E. S. Hori. Optimal measurement combinations as controlled variables. J. Proc. Control, 19(1):138{148, 2009.","cites":null},{"id":37938638,"title":"Optimal selection of controlled variables.","authors":[],"date":"2003","doi":"10.1021\/ie020833t","raw":"I. J. Halvorsen, S. Skogestad, J. C. Morud, and V. Alstad. Optimal selection of controlled variables. Ind. Eng. Chem. Res., 42(14):3273{3284, 2003.","cites":null},{"id":37938650,"title":"Plantwide control: The search for the self-optimizing control structure.","authors":[],"date":"2000","doi":"10.1016\/s0959-1524(00)00023-8","raw":"S. Skogestad. Plantwide control: The search for the self-optimizing control structure. J. Proc. Control, 10(5):487{507, 2000.","cites":null},{"id":37938640,"title":"Selection of controlled variables: Maximum gain rule and combination of measurements.","authors":[],"date":"2008","doi":"10.1021\/ie0711978","raw":"E. S. Hori and S. Skogestad. Selection of controlled variables: Maximum gain rule and combination of measurements. Ind. Eng. Chem. Res., 47(23):9465{9471, 2008. 29[11] R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, Cambridge, UK, 1985.","cites":null},{"id":37938631,"title":"Studies on Selection of Controlled Variables.","authors":[],"date":"2005","doi":null,"raw":"V. Alstad. Studies on Selection of Controlled Variables. PhD thesis, Norwegian University of Science and Technology, Trondheim, Norway, 2005. Available at http:\/\/www.nt.ntnu.no\/users\/skoge\/ publications\/thesis\/2005_alstad\/.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2009-08-12T00:00:00Z","abstract":"The selection of controlled variables (CVs) from available measurements through\nenumeration of all possible alternatives is computationally forbidding for\nlarge-dimensional problems. In Part I of this work [Cao, Y., & Kariwala, V.\n(2008). Bidirectional branch and bound for controlled variable selection: Part\nI. Principles and minimum singular value criterion. Comput. Chem. Eng., 32\n(10),2306-2319], we proposed a bidirectional branch and bound (BAB) approach for\nsubset selection problems and demonstrated its efficiency using the minimum\nsingular value criterion. In this paper, the BAB approach is extended for CV\nselection using the exact local method for self-optimizing control. By\nredefining the loss expression, we show that the CV selection criterion for\nexact local method is bidirectionally monotonic. A number of novel determinant\nbased criteria are proposed for fast pruning and branching purposes resulting in\na computationally inexpensive BAB approach. We also establish a link between the\nproblems of selecting a subset and combinations of measurements as CVs and\npresent a partially bidirectional BAB method for selection of measurements,\nwhose combinations can be used as CVs. Numerical tests using randomly generated\nmatrices and binary distillation column case study demonstrate the computational\nefficiency of the proposed methods. (C) 2009 Elsevier Ltd. All rights reserved","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/140455.pdf","fullTextIdentifier":"http:\/\/dx.doi.org\/10.1016\/j.compchemeng.2009.01.014","pdfHashValue":"836879937701e1be6a27222f2a6f808515d6cca9","publisher":"Elsevier Science B.V., Amsterdam.","rawRecordXml":"<record><header><identifier>\noai:dspace.lib.cranfield.ac.uk:1826\/6047<\/identifier><datestamp>2016-07-12T10:56:25Z<\/datestamp><setSpec>hdl_1826_19<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>Bidirectional branch and bound for controlled variable selection. Part II: exact\nlocal method for self-optimizing control<\/dc:title><dc:creator>Kariwala, Vinay<\/dc:creator><dc:creator>Cao, Yi<\/dc:creator><dc:subject>Branch and bound Control structure design Controlled variables Combinatorial optimization Self-optimizing control optimal measurement combinations algorithm<\/dc:subject><dc:description>The selection of controlled variables (CVs) from available measurements through\nenumeration of all possible alternatives is computationally forbidding for\nlarge-dimensional problems. In Part I of this work [Cao, Y., & Kariwala, V.\n(2008). Bidirectional branch and bound for controlled variable selection: Part\nI. Principles and minimum singular value criterion. Comput. Chem. Eng., 32\n(10),2306-2319], we proposed a bidirectional branch and bound (BAB) approach for\nsubset selection problems and demonstrated its efficiency using the minimum\nsingular value criterion. In this paper, the BAB approach is extended for CV\nselection using the exact local method for self-optimizing control. By\nredefining the loss expression, we show that the CV selection criterion for\nexact local method is bidirectionally monotonic. A number of novel determinant\nbased criteria are proposed for fast pruning and branching purposes resulting in\na computationally inexpensive BAB approach. We also establish a link between the\nproblems of selecting a subset and combinations of measurements as CVs and\npresent a partially bidirectional BAB method for selection of measurements,\nwhose combinations can be used as CVs. Numerical tests using randomly generated\nmatrices and binary distillation column case study demonstrate the computational\nefficiency of the proposed methods. (C) 2009 Elsevier Ltd. All rights reserved.<\/dc:description><dc:publisher>Elsevier Science B.V., Amsterdam.<\/dc:publisher><dc:date>2011-09-08T09:31:23Z<\/dc:date><dc:date>2011-09-08T09:31:23Z<\/dc:date><dc:date>2009-08-12T00:00:00Z<\/dc:date><dc:type>Article<\/dc:type><dc:identifier>Vinay Kariwala and Yi Cao, Bidirectional branch and bound for controlled variable selection. Part II: Exact local method for self-optimizing control, Computers & Chemical Engineering, Volume 33, Issue 8, 12 August 2009, Pages 1402-1412.<\/dc:identifier><dc:identifier>0098-1354<\/dc:identifier><dc:identifier>http:\/\/dx.doi.org\/10.1016\/j.compchemeng.2009.01.014<\/dc:identifier><dc:identifier>http:\/\/dspace.lib.cranfield.ac.uk\/handle\/1826\/6047<\/dc:identifier><dc:language>en_UK<\/dc:language><dc:rights>This is the author\u2019s version of a work that was accepted for publication in Computers & Chemical Engineering . Changes resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms may not be reflected in this document. Changes may have been made to this work since it was submitted for publication. A definitive version was subsequently published in Computers & Chemical Engineering , VOL 33, ISSUE 8, (2009) DOI:10.1016\/j.compchemeng.2009.01.014<\/dc:rights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["issn:0098-1354","0098-1354"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2009,"topics":["Branch and bound Control structure design Controlled variables Combinatorial optimization Self-optimizing control optimal measurement combinations algorithm"],"subject":["Article"],"fullText":"Bidirectional Branch and Bound for Controlled Variable Selection\nPart II. Exact Local Method for Self-optimizing Control\nVinay Kariwala\u2020 and Yi Cao\u2021\u2217\n\u2020 Division of Chemical & Biomolecular Engineering,\nNanyang Technological University, Singapore 637459\n\u2021School of Engineering, Cranfield University, Cranfield, Bedford MK43 0AL, UK\nThis version: January 28, 2009\nAbstract\nThe selection of controlled variables (CVs) from available measurements through enumeration of\nall possible alternatives is computationally forbidding for large-dimensional problems. In Part I of\nthis work [5], we proposed a bidirectional branch and bound (BAB) approach for subset selection\nproblems and demonstrated its efficiency using the minimum singular value criterion. In this paper, the\nBAB approach is extended for CV selection using the exact local method for self-optimizing control.\nBy redefining the loss expression, we show that the CV selection criterion for exact local method is\nbidirectionally monotonic. A number of novel determinant based criteria are proposed for fast pruning\nand branching purposes resulting in a computationally inexpensive BAB approach. We also establish a\nlink between the problems of selecting a subset and combinations of measurements as CVs and present\na partially bidirectional BAB method for selection of measurements, whose combinations can be used\nas CVs. Numerical tests using randomly generated matrices and binary distillation column case study\ndemonstrate the computational efficiency of the proposed methods.\nKeywords: Branch and bound, Control structure design, Controlled variables, Combinatorial opti-\nmization, Self-optimizing control.\n\u2217Corresponding Author: Tel: +44-1234-750111; Fax: +44-1234-754685; E-mail:y.cao@cranfield.ac.uk\n1\nNomenclature\n1p,q p\u00d7 q matrix of ones\na column vector (lower case bold face letter)\nA matrix (upper case bold face letter)\nB best available lower bound on selection criterion\nC candidate set of a node\nCnm binomial coefficient of m choose n\nF fixed set of a node\nG\u02dc Defined as G\u02dc = GyJ\u22121uu ; see equation (22)\nG\u02dcX sub-matrix of G\u02dc consisting of rows with indices in set X\nH measurement selection or combination matrix\nIp p\u00d7 p Identity matrix\nJ objective functional related to steady-state economics of process\nL1 local loss when individual measurements are used as CVs\nL2 local loss when measurement combinations are used as CVs\nL2(X) lower bound on L2 for all n-element supersets of X\nM(Xp) Defined as M(Xp) = R\u2212T G\u02dcXpG\u02dcTXpR\n\u22121; see Equation 31\nn number of measurements to be selected, whose combinations are used\nas CVs\nnu number of degrees of freedom or inputs\nny number of available measurements\nN(Xp) Defined as N(Xp) = G\u02dcTXp(YXpY\nT\nXp\n)\u22121G\u02dcXp ; see Equation 32\nR Cholesky factor\nS union of the sets F and C, i.e. S = F \u222a C\nS a two-tuple, S = (F,C) represents a node in the search tree\nT selection criterion (to be minimized)\nTn(X) lower bound on T for all n-element subsets or supersets of X\nXt subscript t represents the size of the set X\nXi superscript i represents the index of the sub or super set obtained\nfrom X\nY Defined as Y =\n[\n(Gy J\u22121uu Jud \u2212Gyd)Wd We\n]\n; see equation (21)\n\u03b1 downwards pruning index\n\u03b2 upwards pruning index\n2\n\u03bbi i\nth largest eigenvalue of a square matrix\n\u03bb\u00af maximum eigenvalue of a square matrix\n\u03bb least non-zero eigenvalue of a square matrix\n\u03c3\u00af maximum singular value of a matrix\n1 Introduction\nA plant usually has many measurements available for monitoring and control purposes. Self-optimizing\ncontrol involves selection of a subset or combinations of available measurements as controlled variables\n(CVs) such that when the selected CVs are maintained at constant setpoints using feedback controllers,\nthe overall plant operation is nearly optimal even in the presence of various disturbances [16]. Thus, the\nconcept of self-optimizing control provides a simple operational strategy, where the loss incurred due to the\nuse of suboptimal feedback based strategy in comparison with the use of an online optimizer is minimal.\nThe loss incurred by the feedback based strategy depends on the selected CVs. For appropriate selection\nof CVs using the concept of self-optimizing control, various criteria have been proposed including the\nminimum singular value (MSV) rule [17] and exact local methods with worst-case [9, 12] and average\nloss minimization [3, 13]. Like other control structure selection problems, CV selection is a combinatorial\noptimization problem; see e.g. [19]. To find the optimal CVs, the selection criteria need to be evaluated\nfor each possible alternative resulting in huge computational requirements, especially when the number of\navailable measurements and the number of CVs to be selected are large. For such large scale problems,\nsome heuristic rules may have to be applied to reduce the size of the search space. With the use of heuristic\nrules, however, the global optimality of selected CVs cannot be guaranteed.\nThe combinatorial difficulty associated with the CV selection problem was recently addressed in Part I\nof this work [5], where a novel bidirectional branch and bound (BAB) approach was proposed and its\nefficiency for CV selection was demonstrated using the MSV rule. The MSV rule, however, is approximate\nand can sometimes lead to non-optimal set of CVs [10]. In general, it is more appropriate to select CVs\nusing the exact local methods [3, 9, 13]. The objective of this paper is to extend the BAB approach for\nCV selection using the exact local method with worst-case loss minimization.\nThe selection of CVs from available measurements can be seen as a subset selection problem, where the\nnumber of CVs to be selected is the same as the number of available inputs or degrees of freedom. For\nsuch problems, a bidirectional BAB method gains its efficiency by pruning both supersets and subsets\n3\n(measurement sets with the number of elements greater than and smaller than the number of inputs,\nrespectively), which cannot lead to the optimal solution. A difficulty in the use of BAB method for the\nexact local method is that the loss expression for this method is restrictively defined for square systems,\ni.e. where the number of selected measurements is equal to the number of inputs. On the other hand, a\nBAB method requires evaluation of the selection criterion, when the number of selected variables differs\nfrom the target subset size. In this paper, we re-define the loss expression for exact local method such\nthat it holds for non-square configurations. We subsequently show that the re-defined loss expression is\nbidirectionally monotonic and thus is amenable to the use of bidirectional BAB approach.\nIn comparison with the traditional unidirectional BAB approaches, the use of bidirectional pruning, i.e.\nsimultaneous pruning of both supersets and subsets, provides significant improvement in computational\nefficiency. The evaluation of (re-defined) loss expression for exact local method, however, is computa-\ntionally expensive. We note that a BAB method spends most of its time in evaluation of non-optimal\nnodes. Therefore, we develop several efficient determinant based conditions to replace the computation-\nally demanding calculation of exact local loss so as to quickly decide upon whether expansion of a node\ncan lead to the optimal solution. With these improvements, the proposed BAB method achieves similar\ncomputational efficiencies as the BAB approach for CV selection using MSV rule [5].\nA related problem involves selection of combinations of available measurements as CVs, which provides\nlower losses than the use of a subset of available measurements as CVs [2, 9, 12, 13]. Halvorsen et al. [9]\nproposed the use of nonlinear optimization based approach to design the combination matrix, which is\ncomputationally expensive and may converge to local optima. Alstad and Skogestad [2] proposed the use\nof computationally more efficient null space method to find measurement combinations, but this method\nignores implementation error and thus can only provide a suboptimal solution. Recently, explicit solutions\nto the problem of finding locally optimal measurement combinations have been proposed [3, 12, 13], which\nsignificantly simplify the design procedure. It is noted in [1, 10, 12, 13] that the use of combinations of\na few measurements as CVs often provides similar loss as compared to the case where combinations of\nall available measurements are used. Though the former approach results in control structures with lower\ncomplexity, it gives rise to another combinatorial optimization problem involving the identification of the\nset of measurements, whose combinations can be used as CVs.\nIn this paper, we extend the BAB method to find a subset of available measurements, whose combinations\ncan be used CVs. Unlike the selection of a subset of measurements as CVs, however, the selection\ncriterion for this problem is only downwards monotonic (gradually decreasing subset size). We show that\nthe advantages of bidirectional BAB method can still be realized to some extent, as a lower bound of the\n4\nselection criterion satisfies upwards monotonicity, when the number of selected measurements is greater\nthan a certain number. We propose partially bidirectional BAB method for this problem and demonstrate\nthe efficiency using the case study of a binary distillation column [15]. In addition to the extension\nof the bidirectional BAB for CV selection using exact local method, a contribution of this work is the\ndemonstration of the fact that BAB methods can still be used when the selection criterion does not satisfy\nmonotonicity requirements.\nThe rest of the paper is organized as follows: Section 2 provides a tutorial overview of unidirectional\nand bidirectional BAB methods for subset selection problems. The problems of selecting a subset or\ncombinations of available measurements as CVs using the concept of self-optimizing control are formulated\nin Section 3. For these problems, efficient bidirectional BAB algorithms are developed in Section 4. The\ndeveloped algorithms are tested with several numerical examples in Section 5 and the work is concluded\nin Section 6.\n2 Branch and bound methods for subset selection\nThis section gives a brief overview of the principles of unidirectional (upward or downward) and bidirec-\ntional BAB approaches for subset selection problems; see [5] for further details. The bidirectional BAB\napproach is adapted for CV selection using the exact local method for self-optimizing control later in the\npaper.\n2.1 Subset selection problem\nAssume that Xm is an m-element set of all the available elements. The subset selection problem involves\nfinding an n-element subset Xn \u2282 Xm such that the selection criterion T is minimized among all possible\nXn \u2282 Xm, i.e.\nT (Xoptn ) = minT (Xn) \u2200Xn \u2282 Xm (1)\nwhich is a combinatorial optimization problem. For small m and n, the globally optimal subset Xoptn\ncan be obtained through an exhaustive search. For large m and n, however, the number of available\nalternatives Cnm = m!\/(m \u2212 n)!n! can be too large to carry out a brute-force search. BAB is one of the\nefficient approaches, which are able to find the globally optimal subset without exhaustive evaluation.\n5\n2.2 Branch and bound approaches\nThe first BAB approach for subset selection problems was proposed by Narendra and Fukunaga [14], which\nwas further improved in [8, 18, 20]. The BAB method used in this paper differs from these approaches,\nas it uses the concepts of fixed and candidate sets introduced in [6, 7] to facilitate the implementation of\nbidirectional pruning and branching. The basic principle, however, remains the same, as discussed next.\nPrinciple. The basic principle of BAB approach is to divide the original selection problem into smaller\nsub-problems (branching). Then, if an estimated lower bound of T of a sub-problem is larger than an upper\nbound of T (Xoptn ), then the sub-problem under consideration cannot lead to the optimal solution and hence\ncan be discarded without further evaluation (pruning). If a sub-problem cannot be discarded, it is further\ndivided into smaller sub-problems. This procedure is repeated until there are no more sub-problems left\nto solve.\nFixed and candidate sets. To standardize notation, consider a sub-problem S = (Ff , Cc) with an\nf -element fixed set Ff and a c-element candidate set Cc, where f \u2264 n and n \u2264 f + c \u2264 m. Here, the\nelements of Ff are included in all n-element subsets that can be obtained by solving S, while elements of\nCc can be freely chosen to append Ff . Then, a subset Xn belonging to S must satisfy the following two\nrelationships:\nupwards relationship: Ff \u2282 Xn (2)\ndownwards relationship: Xn \u2282 (Ff \u222a Cc) (3)\nFurthermore, S = (Ff , Cc) can be divided into 2c subproblems either by moving xj \u2208 Cc to Ff or by\ndiscarding xk \u2208 Cc, where j, k = 1, 2, \u00b7 \u00b7 \u00b7 , c. Each of the sub-problems Si = (F ifi , Cici), i = 1, 2, \u00b7 \u00b7 \u00b7 , 2c,\nsatisfy\nupwards fixed-set relationship: Ff \u2286 F ifi (4)\ndownwards candidate-set relationship: Cc \u2287 Cici (5)\ndownwards union relationship: (Ff \u222a Cc) \u2287 (F ifi \u222a Cici) (6)\n6\nBidirectional pruning using monotonicity. Let T (S) be a lower bound of T over all n-element\nsubsets that can be reached from S, i.e.\nT (S) \u2264 min\nXn\u2283Ff\nXn\u2282(Ff\u222aCc)\nT (Xn) (7)\nFurther, let B be an upper bound of T (Xoptn ), i.e. B \u2265 T (Xoptn ). Then, S can be discarded, if T (S) > B.\nThe computation of T (S) can be considerably simplified, if the selection criterion is monotonic. Here, the\nselection criterion T is said to be upwards monotonic, when\nT (Xs) \u2265 T (Xt) ifXs \u2283 Xt; t < s < n (8)\nSimilarly, T is said to be downwards monotonic, when\nT (Xs) \u2265 T (Xt) ifXs \u2282 Xt; t > s > n (9)\nFor upwards monotonic T , the lower bound of T on S = (Ff , Cc) can be estimated as\nT (S) = T (Ff ) (10)\nIn this case, an upward pruning operation to discard S can be conducted if T (Ff ) > B. Similarly, for\ndownwards monotonic T , the lower bound of T on S = (Ff , Cc) can be estimated as\nT (S) = T (Ff \u222a Cc) (11)\nA downward pruning operation to discard S can be carried out if T (Ff \u222a Cc) > B. Furthermore, it is\nshown in [5] that if T satisfies both upward (for subset size less than n) and downward (for subset size\nlarger than n) monotonicity, then pruning can be carried out bidirectionally so that efficiency can be\nsignificantly improved.\nIn bidirectional BAB approach [5], pruning is carried out on the 2c sub-problems of S, instead of on S\ndirectly. Assume that T (Ff ) < B and T (Ff \u222a Cc) < B. For xi \u2208 Cc, upward pruning is conducted by\ndiscarding xi from Cc, if T (Ff \u222a xi) > B. Similarly, if T (Ff \u222a (Cc\\xi)) > B, then downward pruning is\nperformed by moving xi from the candidate set to the fixed set. Finally, if both conditions are satisfied,\nthen bidirectional pruning discards all the 2c sub-problems and thus entire S. Here, an advantage of\nperforming pruning on sub-problems is that the bounds T (Ff \u222a xi) and T (Ff \u222a (Cc\\xi)) can be computed\nfrom T (Ff ) and T (Ff \u222a Cc), respectively, for all xi \u2208 Cc together resulting in computational efficiency.\n7\n2.3 Bidirectional branching\nA BAB approach also gains its efficiency by an effective branching rule, i.e. the way in which a problem is\ndivided into several subproblems. The aim of an effective branching rule is to facilitate pruning of as many\nnon-optimal subproblems as possible. Based on the bidirectional BAB principle, a bidirectional branching\nrule has been proposed in [5]. In this approach, when branching is required for S = (Ff , Cc), instead of\nbranching all 2c subproblems of S, only two branches are produced based on a decision element, xk \u2208 Cc;\nsee Figure 3 for an example. Here, an upward branch corresponds to moving xk from the candidate set\nCc to the fixed set Ff and a downward branch corresponds to discarding xk from the candidate set Cc.\nBetween these two branches, the branch with fewer n-element subsets (terminal nodes) is evaluated first\nso that the branch with more alternatives might be discarded at a later stage.\nFor a given problem S = (Ff , Cc), the upward and downward branches have Cn\u2212f\u22121c\u22121 and Cn\u2212fc\u22121 terminal\nnodes, respectively. Thus, upward-first branching is conducted if Cn\u2212f\u22121c\u22121 \u2264 Cn\u2212fc\u22121 or 2(n \u2212 f) \u2264 c \u2212 1\nand downward-first branching otherwise. The decision element itself is chosen on a best-first basis. More\nspecifically, for upwards-first branching, the decision element xk is chosen to provide the minimum T (Ff \u222a\nxk) (best upward branch evaluated first) or the maximum T (Ff \u222a (Cc\\xk)) (worst downward branch kept\nfor future pruning) among all xk \u2208 Cc, whilst for downwards-first branching, xk is selected to give the\nmaximum T (Ff \u222a xk) (best downward branch evaluated first) or the minimum T (Ff \u222a (Cc\\xk)) (worst\nupward branch kept for future pruning) among all xk \u2208 Cc.\n3 Exact Local Method for Self-optimizing Control\nIn this section, we introduce the exact local method for self-optimizing control. We also represent the\nproblems of selecting measurements, which can be either directly used or combined as CVs, as subset\nselection problems.\n3.1 Self-optimizing Control\nThe economically optimal operation of a process requires the use of an online optimizer to update the\noperating point according to the changes in disturbances d \u2208 Rnd . A simpler strategy is to update the\ndegrees of freedom or inputs u \u2208 Rnu indirectly using feedback controllers such that some CVs are held\nconstant. The use of this simpler strategy is clearly sub-optimal. Self-optimizing control is said to occur,\n8\nwhen an acceptable loss is achieved by the feedback based operational strategy without the need to re-\noptimize when disturbances occur [16]. Based on this concept, the appropriate CVs can be selected by\ncomparing the losses for different alternatives.\n3.2 Local method\nCV selection based on the general non-linear formulation of self-optimizing control can be time-consuming\nand local methods are often used for pre-screening alternatives. To present the local methods, let the\neconomics of the plant be characterized by the scalar objective function J(u,d) and uopt(d\u2217) be the\noptimal value of inputs minimizing J for the nominal disturbance d\u2217. Around the nominally optimal\noperating point (uopt(d\u2217),d\u2217), let the linearized model of the process be\ny = Gy u+GydWd d+We e (12)\nwhere y \u2208 Rny denotes the process measurements and e \u2208 Rny denotes the implementation error, which\nresults due to measurement and control error. Here, the diagonal matrices Wd and We contain the mag-\nnitudes of expected disturbances and implementation errors associated with the individual measurements,\nrespectively. The CVs c \u2208 Rnu are given as\nc = Hy = Gu+GdWd d+HWe e (13)\nwhere\nG = HGy and Gd = HG\ny\nd (14)\nIt is assumed that G \u2208 Rnu\u00d7nu is invertible. This assumption is necessary for integral control.\nAssume that the feedback controller maintains c at c\u2217 and let uopt(d) denote the optimal value of u for\nany allowable disturbance. Then for given d and e, the loss incurred due to controlling CVs at constant\nset-point is defined as\nL(H,d, e) = J(u,d, e)|c=c\u2217 \u2212 J(uopt(d),d) (15)\nWhen d and e are constrained to satisfy\u2225\u2225\u2225[ dT eT ]\u2225\u2225\u2225T\n2\n\u2264 1 (16)\n9\nHalvorsen et al. [9] have shown that the worst-case loss over the set (16) is given as\nL1(H) =\n1\n2\n\u03c3\u00af2 ([Md Me ]) (17)\nwhere\nMd = J1\/2uu\n(\nJ\u22121uuJud \u2212G\u22121Gd\n)\nWd (18)\nMe = J1\/2uu G\n\u22121HWe (19)\nHere, Juu and Jud represent \u2202\n2J\n\u2202u2\nand \u2202\n2J\n\u2202u \u2202d , evaluated at the nominally optimal operating point, respectively.\n3.3 Selection of controlled variables\nIndividual measurements. The loss in (17) depends on H and CVs are selected by minimizing loss\nwith respect to H. When individual measurements are selected as CVs, the elements of H are restricted\nto be 0 or 1 and\nHHT = I (20)\nIn words, selection of a subset of available measurements as CVs involves selecting nu among ny measure-\nments, where the number of available alternatives is Cnuny . We note that, however, the expression for L1 in\n(17) requires inversion of G and thus only holds, when G is a square matrix. On the other hand, BAB\nmethods require evaluation of loss, when the number of selected measurements differs from nu. Motivated\nby this drawback, we present an alternate representation of L1 in the following discussion. For notational\nsimplicity, we define\nY =\n[\n(Gy J\u22121uu Jud \u2212Gyd)Wd We\n]\n(21)\nG\u02dc = GyJ\u22121\/2uu (22)\nNow, L1 can be represented as\nL1(H) =\n1\n2\n\u03c3\u00af2\n(\n(HG\u02dc)\u22121HY\n)\n(23)\n=\n1\n2\n\u03bb\u00af\n(\n(HG\u02dc)\u22121HYYTHT (HG\u02dc)\u2212T\n)\n(24)\n=\n1\n2\n\u03bb\u22121\n(\n(HG\u02dc)T (HYYTHT )\u22121HG\u02dc\n)\n(25)\nwhere \u03bb(\u00b7) denotes the least non-zero eigenvalue. We note that in practice, every measurement has non-\nzero implementation error associated with it. Thus, based on (21), [We]ii 6= 0 and Y has full row rank.\n10\nThese observations imply that the inverse of HYYTHT is well defined for all practical problems and the\nexpression for L1 in (25) holds for any number of measurements.\nTo represent L1 in (25) using index notation, let Xp be an p-element index set, p \u2264 ny, consisting of\nthe indices of selected measurements and, G\u02dcXp and YXp consist of rows of G\u02dc and Y with indices in Xp,\nrespectively. Then,\nL1(Xp) =\n1\n2\n\u03bb\u22121\n(\nG\u02dcTXp(YXpY\nT\nXp)\n\u22121G\u02dcXp\n)\n(26)\n=\n1\n2\n\u03bb\u22121\n(\nR\u2212T G\u02dcXpG\u02dc\nT\nXpR\n\u22121\n)\n(27)\nwhere RTR = YXpYTXp (Cholesky factorization). As the expressions for L1 in (17) and (27) are same for\np = nu, the optimal set of CVs can be found by minimizing L1 in (27).\nMeasurement combinations. When individual measurements are used as CVs, the information con-\ntained in only nu out of ny measurements is used for updating the inputs. Clearly, better self-optimizing\nproperties or lower loss can be obtained by using the information contained in other measurements as well.\nThis can be achieved by using combinations of all the available measurements as CVs. For example, for\nthe binary distillation column case study discussed in Section 5.2, the lowest achievable loss is 0.2809 with\nthe use of individual measurements as CVs. The loss for this process, however, decreases approximately 5\ntimes to 0.0517, when combinations of all available measurements as CVs.\nWhen measurement combinations are used CVs, the integer constraint on H \u2208 Rnu\u00d7ny is relaxed, but the\ncondition rank(H) = nu is still imposed to ensure invertibility of HGy. The minimal worst-case loss over\nthe set (16) using measurements combinations as CVs is given as [12, 13]\nL2 = min\nH\nL1 =\n1\n2\n\u03bb\u22121nu\n(\nG\u02dcT (YYT )\u22121 G\u02dc\n)\n(28)\nEquation (28) can be used to calculate the minimum loss provided by the optimal combination of a given set\nof measurements. It is noted in [1, 10, 12, 13], however, that use of all measurements is often unnecessary\nand equivalent losses can be obtained by combining only a few of the available measurements. Then, the\ncombinatorial optimization problem involves finding the set of n among ny measurements (nu \u2264 n \u2264 ny)\nthat can provide minimal loss, where n is specified.\nIn index notation, for a given p-element index set Xp, L2 is denoted as\nL2(Xp) =\n1\n2\n\u03bb\u22121\n(\nG\u02dcTXp (YXpY\nT\nXp)\n\u22121 G\u02dcXp\n)\n(29)\n=\n1\n2\n\u03bb\u22121\n(\nR\u2212T G\u02dcXpG\u02dc\nT\nXpR\n\u22121\n)\n(30)\n11\nwhere nu \u2264 p \u2264 ny and RTR = YXpYTXp (Cholesky factorization). Now, the set of best n measurements,\nwhose combination can be used as CVs, can be selected by minimizing L2 in (30). Note that L2(Xp) =\nL1(Xp) for nu \u2264 p \u2264 ny. This observation noted in this work is useful for development of efficient BAB\napproaches, as discussed in the next section.\n4 Bidirectional controlled variable selection\nAs shown in Section 3, the selection of CVs using exact local method can be seen as subset selection\nproblems. In this section, we present BAB methods for solving these problems efficiently. For simplicity\nof notation, we define the p\u00d7 p matrix M(Xp) as\nM(Xp) = R\u2212T G\u02dcXpG\u02dc\nT\nXpR\n\u22121 (31)\nwhere R is the Cholesky factor of YXpYTXp . Moreover, we denote the nu \u00d7 nu matrix N(Xp) as\nN(Xp) = G\u02dcTXp(YXpY\nT\nXp)\n\u22121G\u02dcXp (32)\nNote that \u03bb(M(Xp)) = \u03bb(N(Xp)).\n4.1 Monotonicity\nIn this section, we show that the loss expressions in (27) and (30) or their lower bounds satisfy the mono-\ntonicity requirement and thus are amenable to the application of BAB approach discussed in Section 2.\nIndividual measurements. To prove monotonicity for L1 in (27), we use the following property of\nmatrices:\nLemma 1 Let the matrix A\u02c6 be defined as\nA\u02c6 =\n\uf8ee\uf8f0 A b\nbT a\n\uf8f9\uf8fb (33)\nwhere A \u2208 Rp\u00d7p is a Hermitian matrix, b \u2208 Rp\u00d71 and a \u2208 R. Let the eigenvalues of A and A\u02c6 be arranged\nin descending order. Then [11, Th. 4.3.8]\n\u03bbp+1(A\u02c6) \u2264 \u03bbp(A) \u2264 \u03bbp(A\u02c6) \u2264 \u03bbp\u22121(A) \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bb1(A) \u2264 \u03bb1(A\u02c6) (34)\n12\nProposition 1 Consider a node S = (Ff , Cc) and index i \u2208 Cc. For L1 defined in (27),\nL1(Ff ) \u2264 L1(Ff \u222a i); f < nu (35)\nL1((Ff \u222a Cc) \\ i) \u2265 L1(Ff \u222a Cc); f + c > nu (36)\nProof : Let G\u02dcFf\u222ai =\n[\nG\u02dcTFf G\u02dc\nT\ni\n]T\nand YFf\u222ai =\n[\nYTFf Y\nT\ni\n]T\n. Further, let RTR = YFfY\nT\nFf\nand R\u02dcT R\u02dc =\nYFf\u222aiY\nT\nFf\u222ai (Cholesky factorization). Then, it follows that R and M(Ff ) are principal submatrices of R\u02dc\nand M(Ff \u222a i), respectively, obtained by deleting the last row and column of the corresponding matrices.\nUsing (34), we have\n\u03bb\u22121f (M(Ff )) \u2264 \u03bb\u22121f+1(M(Ff \u222a i)); f < nu (37)\nwhich implies (35).\nTo prove (36), let RTR = Y(Ff\u222aCc)\\iY\nT\n(Ff\u222aCc)\\i and R\u02dc\nT R\u02dc = YFf\u222aCcY\nT\nFf\u222aCc (Cholesky factorization). As\nbefore, it can be shown that R and M((Ff \u222a Cc) \\ i) are principal submatrices of R\u02dc and M(Ff \u222a Cc),\nrespectively. Based on (34), we have\n\u03bb\u22121nu (M((Ff \u222a Cc) \\ i)) \u2265 \u03bb\u22121nu (M(Ff \u222a Cc)); f + c > nu (38)\nNow the result follows by noting that \u03bbj(M((Ff \u222a Cc) \\ i)) = \u03bbj(M(Ff \u222a Cc)) = 0 for j > nu and thus\n\u03bbnu(M((Ff \u222a Cc) \\ i)) and \u03bbnu(M(Ff \u222a Cc)) represent the least non-zero eigenvalues of M((Ff \u222a Cc) \\ i)\nand M(Ff \u222a Cc), respectively.\nBased on Proposition 1, it follows that L1(Ff ) and L1(Ff \u222a Cc) represent lower bounds on the loss seen\nusing any nu measurements as CVs, which can be obtained by appending measurement indices to Ff or\nremoving measurement indices from Ff \u222aCc, respectively. Let B represent the best available upper bound\non L1(X\nopt\nnu ). Then repeated application of (35) implies that, if L1(Ff ) > B, the optimal solution cannot\nbe a superset of Ff and hence all supersets of Ff need not be evaluated. Similarly, if L1(Ff \u222a Cc) > B,\nrepeated application of (36) implies that the optimal solution cannot be a subset of Ff \u222aCc and hence all\nsubsets of Ff \u222aCc need not be evaluated. Thus, upwards and downwards pruning can be conduced using\n(35) and (36), respectively, and the optimal solution can be found without complete enumeration.\nMeasurements combinations. We note that the expression for L2 in (30) is the same as the expression\nfor L1 in (27). Thus, based on Proposition 1\nL2((Ff \u222a Cc) \\ i) \u2265 L2(Ff \u222a Cc); f + c > n (39)\n13\nFor selecting a subset of measurements, whose linear combinations can be used as CVs, the result in (39)\ncan be used for downwards pruning. Equation (36), however, also implies that when nu \u2264 f < n, L2(Ff )\ndecreases as the size of the fixed set increases. Thus, unlike L1, L2 does not posses upwards monotonicity.\nIn the following proposition, we present a lower bound on L2, which shows upwards monotonicity, whenever\nn\u2212 nu < f < n.\nProposition 2 For the node S = (Ff , Cc), let\nL2(Ff ) = 0.5\u03bb\n\u22121\nf+nu\u2212n (M(Ff )) ; f > n\u2212 nu (40)\nThen, L2(Ff ) represents a lower bound on the loss corresponding to combinations of any n measurements\nobtained by appending indices to Ff , i.e.\nL2(Ff ) \u2264 min\nXn\u2283Ff\nXn\u2282(Ff\u222aCc)\nL2(Xn) (41)\nwhere L2 is defined in (30). Furthermore, L2(Ff ) satisfies upwards monotonicity, i.e. for any i \u2208 Cc\nL2(Ff ) \u2264 L2(Ff \u222a i); f < n (42)\nProof : Consider the index setXn \u2282 (Ff\u222aCc). For j \u2208 Xn with j \/\u2208 Ff , similar to the proof of Proposition 1,\nit can be shown that M(Xn \\ j) is a principal submatrix of M(Xn). Based on the interlacing property of\neigenvalues in (34), we have that\n\u03bb\u22121nu\u22121(M(Xn \\ j)) \u2264 \u03bb\u22121nu (M(Xn)) (43)\nThrough repeated application of (34), for i \u2208 Xn, i \/\u2208 Ff and i 6= j\n\u03bb\u22121nu\u2212(n\u2212f)(M(Ff )) \u2264 \u03bb\n\u22121\nnu\u2212(n\u2212f\u22121)(M(Ff \u222a i)) \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bb\n\u22121\nnu\u22121(M(Xn \\ j)) \u2264 \u03bb\u22121nu (M(Xn)) (44)\nwhich implies (41) and (42).\nProposition 2 implies that the lower bound of L2 defined in (40) posses upwards monotonicity and thus\ncan be used for upwards pruning. In this case, upwards pruning can only be applied when the size of\nfixed set of the node under consideration is greater than n\u2212nu. Thus, the BAB algorithm based on L2 in\n(40) is referred to as partial bidirectional BAB (PB3) algorithm. Development of fully bidirectional BAB\nalgorithm for selection of measurements, which can be combined to yield CVs, is an open problem.\n14\nFigure 1: Monotonicity of local loss functions for CV selection; for subset size i, the loss is calculated for\nthe measurement set {1, 2, \u00b7 \u00b7 \u00b7 , i}\nExample 1. To illustrate the findings of this section, we use a simple toy example, where\nGy =\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n15 4 \u22124\n10 \u22121 6\n3 7 6\n\u22128 \u221218 10\n\u22125 12 9\n9 \u22121 12\n\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n, Gyd =\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u22122 \u22124\n\u22127 3\n\u22126 8\n\u22123 \u221210\n12 \u22121\n\u221215 1\n\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n,\nJuu = I3, Jud = 13,2 (matrix of ones), Wd = I2 and Wn = I6. Figure 1 shows the variation of local loss\nfor different subset sizes. For selection of individual measurements as CVs, the local loss L1 (solid line in\nFigure 1) monotonically increases with subset size i, when i \u2264 3 (nu) and monotonically decreases with\nsubset size i, when i \u2265 3, hence showing bidirectional monotonicity. The case with i > 3 corresponds to the\nuse of combinations of selected measurements as CVs and thus a lower loss is expected than seen using 3\nindividual measurements as CVs. The local loss decreases, even when the subset size i < 3. This happens\nas the control of every CV can be seen as an equality constraint for the optimization problem describing\nthe optimal operation of the process. Thus, the increase in the number of such equality constraints results\nin increased loss because it leads to more sub-optimal variation of the inputs as compared to the truly\noptimal operation, which has no equality constraints.\n15\nFor selection of measurements, whose combinations can be used as CVs, the local loss L2 only satisfies\ndownwards monotonicity. For example, when n = 4, L2 = L1 increases when the subset size is decreased\nfrom 5 to 4. In comparison the lower bound on local loss L2 (dashed line in Figure 1) satisfies upwards\nmonotonicity and can be used for application of bidirectional BAB method. Note that, however, the lower\nbound L2 can be used only when the subset size is greater than n \u2212 nu, i.e. 4 \u2212 3 = 1 for n = 4 and\n5\u2212 3 = 2 for n = 5.\n4.2 Fast pruning algorithms\nAs the criterion for selection of CVs using the exact local method satisfy bidirectional monotonicity, the\nnon-optimal nodes can be pruned quickly. Thus, the optimal solution can be found with evaluation of\nfewer nodes, but the solution time can still be large, as direct evaluation of L1 in (27) and L2 in (40) is\ncomputationally expensive. We note that for pruning purposes, it suffices to know whether a sub- or super-\nnode obtained from the node under consideration can provide a better bound than the best available bound\nB. With this observation, we present computationally efficient determinant-based pruning algorithms such\nthat the evaluation of L1 and L2 is avoided at non-terminal nodes.\nIndividual measurements. We first present fast pruning algorithms for selection of a subset of available\nmeasurements as CVs through minimization of L1 in (26). The case, where combinations of available\nmeasurements are used as CVs through minimization of L2 in (30) is dealt with later in this section.\nProposition 3 (Upwards pruning for L1) Consider a node S = (Ff , Cc) and index i \u2208 Cc. For a\ngiven positive scalar B, if L1(Ff ) < B,\n\u03b2i = dTi di \u2212 dTi DT (M(Ff )\u2212 (0.5\/B)If )\u22121Ddi < (0.5\/B)\u21d4 L1(Ff \u222a i) > B (45)\nwhere D = R\u2212T G\u02dcFf with R being the Cholesky factor of YFfY\nT\nFf\n, and dTi = (G\u02dci \u2212 pTi D)\/\u03b4i with\npi = R\u2212TYFfY\nT\ni and \u03b4i =\n\u221a\nYiYTi \u2212 pTi pi.\nProof : Let Q be the Cholesky factor of YFf\u222aiY\nT\nFf\u222ai, i.e. Q\nTQ = YFf\u222aiY\nT\nFf\u222ai. Through simple algebraic\nmanipulations, it can be shown that\nQ =\n\uf8ee\uf8f0R pi\n0 \u03b4i\n\uf8f9\uf8fb ; Q\u2212T G\u02dcFf\u222ai =\n\uf8ee\uf8f0D\ndTi\n\uf8f9\uf8fb (46)\n16\nSince, L1(Ff ) < B, \u03bb(M(Ff )) > 0.5\/B, which implies that det(M(Ff )\u2212 (0.5\/B)If ) > 0 [5]. Using Schur\ncomplement Lemma [11],\ndet(M(Ff \u222a i)\u2212 (0.5\/B)If+1) = (\u03b2i \u2212 0.5\/B) det(M(Ff )\u2212 (0.5\/B)If ) (47)\nThus, \u03b2i < 0.5\/B \u21d4 det(M(Ff \u222a i)\u2212 (0.5\/B)If ) < 0, as det(M(Ff )\u2212 (0.5\/B)If ) > 0. Now,\ndet(M(Ff \u222a i)\u2212 (0.5\/B)If ) = (\u03bb(M(Ff \u222a i))\u2212 0.5\/B)\nf\u220f\ni=1\n(\u03bbi(M(Ff \u222a i))\u2212 0.5\/B) (48)\nSince \u03bb(M(Ff )) > 0.5\/B, (\u03bbk(M(Ff \u222a i)) \u2212 0.5\/B) > 0 for k = 1, 2, \u00b7 \u00b7 \u00b7 f due to interlacing property of\neigenvalues. Finally, we have \u03b2i < 0.5\/B \u21d4 \u03bb(M(Ff \u222a i)) < 0.5\/B \u21d4 L1(Ff \u222a i) > B.\nFor a node S = (Ff , Cc), if the condition (45) is satisfied, then candidate i can be discarded or the super-\nnode Si = (Ff \u222a i, Cc \\ i) can be pruned. Therefore, this condition is referred to as upwards pruning\ncondition. Furthermore, the main computation load in condition (45) is the Cholesky factorization and\nthe matrix inversion, which needs to be calculated only once for all i \u2208 C. Hence, this test is more efficient\nthan direct calculation of L1.\nProposition 4 (Downward pruning for L1) For a node S = (Ff , Cc), let Ss = Ff\u222aCc, where s = f+c.\nFor a given positive scalar B, if L1(Ss) < B,\n\u03b1i = 1\u2212 xTi (N(Ss)\u2212 (0.5\/B)Inu)\u22121xi\/\u03b7i < 0\u21d4 L1(Ss \\ i) > B (49)\nwhere xi = G\u02dcTSs\\i(YSs\\iY\nT\nSs\\i)\n\u22121YSs\\iY\nT\ni \u2212 G\u02dcTi and \u03b7i = Yi(I\u2212YTSs\\i(YSs\\iYTSs\\i)\u22121YSs\\i)YTi .\nProof : For simplicity of notation, define Q = YSs\\iY\nT\nSs\\i. Then,\n(YSsY\nT\nSs)\n\u22121 =\n\uf8ee\uf8f0 Q YSs\\iYTi\nYiYTSs\\i YiY\nT\ni\n\uf8f9\uf8fb\u22121 (50)\n=\n\uf8ee\uf8f0Q\u22121 +Q\u22121YSs\\iYTi YiYTSs\\iQ\u22121\/\u03b7i \u2212Q\u22121YSs\\iYTi \/\u03b7i\n\u2212YiYTSs\\iQ\u22121\/\u03b7i 1\/\u03b7i\n\uf8f9\uf8fb (51)\n=\n\uf8ee\uf8f0Q\u22121 0\n0 0\n\uf8f9\uf8fb+ 1\/\u03b7i\n\uf8ee\uf8f0Q\u22121YSs\\iYTi\n\u22121\n\uf8f9\uf8fb[YiYTSs\\iQ\u22121 \u22121] (52)\nwhere (51) is obtained using the matrix inversion formula for partitioned matrices [11]. Since G\u02dcTSs =[\nG\u02dcTSs\\i G\u02dc\nT\ni\n]\n, we have\nN(Ss) = G\u02dcTSs(YSsY\nT\nSs)\n\u22121G\u02dcSs = G\u02dc\nT\nSs\\iQ\n\u22121G\u02dcSs\\i + xix\nT\ni \/\u03b7i = N(Ss \\ i) + xixTi \/\u03b7i (53)\n17\nwhich implies that\ndet(N(Ss \\ i)\u2212 (0.5\/B)Inu) = det(N(Ss)\u2212 (0.5\/B)Inu \u2212 xixTi \/\u03b7i) (54)\n= det(N(Ss)\u2212 (0.5\/B)Inu)\u03b1i (55)\nAs L1(Ss) < B, \u03bb(N(Ss)) > 0.5\/B, which implies that det(N(Ss)\u2212 (0.5\/B)Inu) > 0 [5]. Thus \u03b1i < 0 \u21d4\ndet(N(Ss \\ i) \u2212 (0.5\/B)Inu) < 0. Using similar arguments about the interlacing property of eigenvalues,\nas used in the proof of Proposition 3, we have that \u03b1i < 0\u21d4 \u03bb(N(Ss \\ i) < 0.5\/B \u21d4 L1(Ss \\ i) > B.\nFor a node S = (Ff , Cc), if the condition (49) is satisfied, candidate i can be fixed or the sub-node\nSi = (Ff , Cc \\ i) can be pruned. Therefore, this condition is referred to as downwards pruning condition.\nTo evaluate the computational efficiency of the condition (49), let the index set Ss = Ff \u222aCc be permuted\nsuch that the index i is the last element of Ss. Now, based on (51), we note that 1\/\u03b7i is the (nu, nu)th\nelement of (YSsYTSs)\n\u22121 and xTi \/\u03b7i is the last row of the matrix \u2212(YSsYTSs)\u22121G\u02dcSs . Therefore, the use of\ncondition (49) requires inversion of two matrices, (YSsYTSs)\n\u22121 and (N(Ss) \u2212 (0.5\/B)Inu)\u22121, which need\nto be calculated only once for all i \u2208 Cc. Hence, this test is more efficient than direct calculation of L1.\nAs L1 in (26) satisfies bidirectional monotonicity, both upwards and downwards pruning conditions in\nPropositions 3 and 4, respectively, can be applied simultaneously reducing the solution time enormously.\nMeasurement combinations. As before, the downwards pruning condition presented in Proposition 4\ncan also be applied for selection of CVs as combinations of available measurements. In the next proposition,\nwe present algorithms for fast upwards pruning for measurement selection through minimization of L2.\nProposition 5 (Partially upwards pruning rules for L2) Consider a node S = (Ff , Cc) and index\ni \u2208 Cc. For a positive scalar B and f > n \u2212 nu, if L2(Ff ) < B or \u03bbf+nu\u2212n(M(Ff )) > 0.5\/B and\n\u03bbf+nu\u2212n+1(M(Ff )) < 0.5\/B,\n\u03b2i < (0.5\/B)\u21d4 L2(Ff \u222a i) > B (56)\nwhere \u03b2i is defined in (45).\nProof : Based on (47), we have\nf+1\u220f\nj=1\n\u03bbj(M(Ff \u222a i)\u2212 (0.5\/B)) = (\u03b2i \u2212 0.5\/B)\nf\u220f\nj=1\n\u03bbj(M(Ff )\u2212 (0.5\/B)) (57)\n18\nAs M(Ff ) is a principal submatrix of M(Ff \u222a i), the interlacing property of eigenvalues implies that\n\u03bbf+nu\u2212n+1(M(Ff )) \u2264 \u03bbf+nu\u2212n+1(M(Ff \u222a i)) \u2264 \u03bbf+nu\u2212n(M(Ff )) \u2264 \u03bbf+nu\u2212n(M(Ff \u222a i)) (58)\nSince \u03bbf+nu\u2212n(M(Ff )) > 0.5\/B, (58) implies that \u03bbj(M(Ff )) > 0.5\/B and \u03bbj(M(Ff \u222a i)) > 0.5\/B for\nj = 1, 2, \u00b7 \u00b7 \u00b7 , f+nu\u2212n. Similarly, since \u03bbf+nu\u2212n+1(M(Ff )) < 0.5\/B, (58) implies that \u03bbk(M(Ff )) < 0.5\/B\nand \u03bbk+1(M(Ff\u222ai)) < 0.5\/B for k = f+nu\u2212n+1, \u00b7 \u00b7 \u00b7 , f . Since equal number of eigenvalues of M(Ff ) and\nM(Ff\u222ai) are greater than and less than 0.5\/B, based on (57), the signs of (\u03bbf+nu\u2212n+1(M(Ff\u222ai))\u22120.5\/B)\nand (\u03b2i \u2212 0.5\/B) are the same and the result follows.\nThe reader should note the similarities between Propositions 3 and 5, when n = nu. Note that Proposition 5\nrequires checking whether \u03bbf+nu\u2212n+1(M(Ff )) < 0.5\/B. When this condition is not satisfied, due to the\ninterlacing property of eigenvalues, \u03bbf+nu\u2212n+1(M(Ff \u222a i)) > 0.5\/B or L2(Ff \u222a i) < B for all i \u2208 Cc. Thus,\nany super-node of the node under consideration cannot be pruned.\n4.3 Fast branching algorithms\nThe availability of fast pruning algorithms avoids the calculation of loss at non-terminal nodes for pruning\npurposes. The efficiency of the bidirectional BAB method can be further improved using bidirectional\nbranching. As mentioned in Section 2.3, bidirectional branching involves selecting a decision element so\nthat the upward and downward branches can be formulated. Here, the decision element itself is chosen on\na best-first basis, i.e. the element that leads to the lowest loss among the members of the candidate set\nis taken as the decision element. Thus, the loss still needs to be calculated at non-terminal nodes for the\nselection of the decision element. In this section, we establish relationships between the pruning indices (\u03b1\nand \u03b2) calculated for different nodes and the expected loss upon expansion of these nodes. These results\ncan be used to avoid the loss computation at non-terminal nodes entirely, hence greatly enhancing the\ncomputational efficiency.\nProposition 6 (Loss bounds for fast branching) For a node S = (Ff , Cc), let Ss = Ff \u222a Cc, where\ns = f + c. For a given positive scalar B and i \u2208 Cc\nL1(Ff \u222a i) \u2265 0.5\/\u03b2i (59)\nL\u221211 (Ss \\ i)\u2212 1\/B\nL\u221211 (Ss)\u2212 1\/B\n\u2264 \u03b1i (60)\nwhere \u03b2i and \u03b1i are given by (45) and (49), respectively.\n19\nProof : To show that (59) holds, based on (47), we note that\u220ff+1\ni=1 (\u03bbi(M(Ff \u222a i))\u2212 0.5\/B)\u220ff\ni=1 (\u03bbi(M(Ff ))\u2212 0.5\/B)\n= \u03b2i \u2212 0.5\/B (61)\nf\u220f\ni=1\n\u03bbi(M(Ff \u222a i))\u2212 0.5\/B\n\u03bbi(M(Ff ))\u2212 0.5\/B (\u03bb(M(Ff \u222a i))\u2212 0.5\/B) = \u03b2i \u2212 0.5\/B (62)\nSince M(Ff ) is a principal submatrix of M(Ff \u222a i), the interlacing property of eigenvalue implies that\n\u03bbi(M(Ff \u222a i)) \u2265 \u03bbi(M(Ff )), i = 1, 2, \u00b7 \u00b7 \u00b7 , f . Thus,\nf\u220f\ni=1\n\u03bbi(M(Ff \u222a i))\u2212 0.5\/B\n\u03bbi(M(Ff ))\u2212 0.5\/B \u2265 1 (63)\nand we have \u03bb(M(Ff \u222a i)) \u2264 \u03b2i, which implies (59). That (60) holds can be shown to be true similarly\nusing (55), where\nnu\u22121\u220f\ni=1\n\u03bbi(N(Ss \\ i))\u2212 0.5\/B\n\u03bbi(N(Ss))\u2212 0.5\/B\n\u03bb(N(Ss \\ i))\u2212 0.5\/B\n\u03bb(N(Ss))\u2212 0.5\/B = \u03b1i (64)\nDue to the interlacing properties of eigenvalues,\nnu\u22121\u220f\ni=1\n\u03bbi(N(Ss \\ i))\u2212 0.5\/B\n\u03bbi(N(Ss))\u2212 0.5\/B \u2264 1 (65)\nThus, we have\n\u03bb(N(Ss \\ i))\u2212 0.5\/B\n\u03bb(N(Ss))\u2212 0.5\/B \u2265 \u03b1i (66)\nwhich implies (60).\nAccording to Proposition 6, both \u03b1 and \u03b2 can be used to select the decision element for bidirectional\nbranching. More specifically, consider a selection problem S(Ff , Cc). Based on the discussion in Section 2.3,\nupward branch is evaluated first, if 2(nu \u2212 f) \u2264 c\u2212 1, and downward branch otherwise. For upward-first\nbranching, the decision element is determined as the element with largest \u03b2i or smallest \u03b1i among all\ni \u2208 Cc. Similarly, for downward-first branching, the decision element is selected as the element with\nsmallest \u03b2i or largest \u03b1i among all i \u2208 Cc.\nWe point out that the selection of decision element based on the loss relationships in (59) and (60) does\nnot necessary result in a sub or super-node with smallest loss among the different alternatives. Although\nthe sub-optimal choice of the decision element does not affect the optimality of the solution, it may lead\nto evaluation of more nodes for finding the optimal solution. Bidirectional branching based on \u03b1 and \u03b2\nis still useful, as the computational load for calculating exact local loss at every node far outweighs the\ncomputational cost for evaluating a few additional nodes. A flowchart for recursive implementation of the\n20\nS(Xf, Xc)\nvalidity\ncheck\nupwards\npruning\ndownwards\npruning\npruned?\nsingle\nselection?\nupdate bound\nReturn to\nrecursive call\nbidirectional\nbranching\nReturn to\nrecursive call\nNo\nYes\nYes\nNo\nNo\nYes\nRecursive Call\nto Itself\nSecond branch\nFirst branch\nFigure 2: Flow chart of bidirectional branch and bound algorithm\nbidirectional BAB (B3) algorithm, based on the principles of bidirectional pruning and branching using\nthe determinant based criteria developed in this paper, is shown in Figure 2.\nFor selection of n measurements, whose combinations can be used as CVs, the downward pruning index\n\u03b1 can be used for selecting the decision element as before. It is, however, difficult to establish a relation-\nship between the upwards pruning index \u03b2 and expected loss L2, when the node under consideration is\nexpanded. For this reason, only the downward pruning index \u03b1 is used to select the decision element for\nboth upward-first and downward-first branching in the partially bidirectional BAB (PB3) algorithm. The\nPB3 algorithm can also be implemented using the flowchart shown in Figure 2, except that the upwards\npruning condition only needs to be checked when f > n\u2212 nu.\nExample 1 continued. To illustrate the application of B3 algorithm, Example 1 is revisited. The\nobjective is to select 3 out of 6 measurements, which can be used as CVs. The bidirectional solution tree for\nthis example is shown in Figure 3. The algorithm is initialized with F = \u2205, C = {1, 2, 3, 4, 5, 6} and B =\u221e.\nAs the current bound is infinite, no pruning is possible. For branching, only the upwards pruning indices\nare calculated as \u03b2(0) =\n[\n0.3948 0.2178 0.1712 2.3689 0.8170 0.1424\n]\n. Since 2(nu\u2212f) > (c\u22121) (i.e.\n21\n6 > 5), downward-first branching is desired. Hence, the decision element is chosen as the smallest element\nof \u03b2(0), i.e. measurement 6. Two sub-problems are generated by removing element 6 from the candidate\nset (sub-problem S1) and by moving element 6 from candidate to fixed set (sub-problem S2). As S1 is the\nupward branch, it is evaluated first.\nFigure 3: Bidirectional solution tree for the toy example\nAs the bound is still infinite, no pruning is possible and the calculation of downwards pruning index is\nnot required. Moreover, \u03b2(1) = \u03b2(0) (unchanged), as the fixed set F or the bound B has not changed as\ncompared to the previous iteration. For sub-problem S1, c = 5 and f = 0. Since, 2(nu \u2212 f) > c \u2212 1 (i.e.\n6 > 4), downward-first branching is conducted. Among the first 5 elements of \u03b2(1) (members of candidate\nset C), element 3 has the smallest value and is thus taken as the decision element for branching purposes.\nAgain, two sub-problems are generated by removing element 3 from the candidate set (sub-problem S3)\nand by moving element 3 from candidate to fixed set (sub-problem S4), where sub-problem S3 is evaluated\nfirst.\nFor sub-problem S3, c = 4, i.e. only one element needs to be discarded. In this case, the use of downward\npruning index is better than the use of upward pruning index for selecting the decision element. There-\nfore, it is calculated as \u03b1(3) =\n[\n0.0084 0.0086 \u2217 0.0018 0.0030 \u2217\n]\n. A terminal node is obtained by\nremoving element 2, which has the highest value of \u03b1(3). The corresponding loss for this terminal node\nis L1 = 3.9537 and the bound B is updated to be 3.9537. The other sub-problem S5 is obtained by\nmoving element 2 from the candidate to the fixed set. As the bound is updated, \u03b2 and \u03b1 are calculated as\n\u03b2(5) =\n[\n1.9711 \u2217 \u2217 191.6600 1.1601 \u2217\n]\nand \u03b1(5) =\n[\n0.0076 \u2217 \u2217 0.0014 0.0020 \u2217\n]\n. Since every\nelement of \u03b2(5) is greater than 0.5\/B and every element of \u03b1(5) is greater than zero, pruning is not possible.\nA terminal node is obtained by removing the element with highest value of \u03b1(5), i.e. element 1. The loss\nfor this terminal node with elements {2, 4, 5} is L1 = 2.7704. As the loss for this node is less than best\n22\navailable bound, B is updated to 2.7704. This gives 0.5\/B = 0.1805, which is larger than both \u03b203 = 0.1712\nand \u03b206 = 0.1424. Therefore, both elements 3 and 6 should be removed, i.e. both sub-problems S2, which\nhas F = {6}, and S4, which has F = {3}, can be pruned without further evaluation.\nAs there are no problems left for evaluation, the algorithm terminates. The optimal subset is {2, 4, 5},\nwhich provides a loss of 2.7704. The B3 algorithm finds the optimal solution by evaluating 6 nodes, where\nas complete enumeration requires evaluation of 20 nodes.\n5 Numerical examples\nTo examine the efficiency of the proposed BAB algorithms, numerical tests are conducted using randomly\ngenerated matrices and a binary distillation column case study. Programs used for loss minimization are\nlisted in Table 1 [4]. All tests are conducted on a Windows XP SP2 notebook with an Intelr CoreTM Duo\nProcessor T2500 (2.0 GHz, 2MB L2 Cache, 667 MHz FSB) using MATLABr R2008a.\nTable 1: Branch and bound programs for loss minimization\nprogram description\nUP upwards pruning using determinant condition (45)\nDOWN downwards pruning using determinant condition (49)\nB3 bidirectional branch and bound algorithm by combining (45) and (49)\nPB3 partially bidirectional branch and bound algorithm by combining (49) and (56)\n5.1 Random tests\nFour sets of random tests are conducted to evaluate the efficiency of the B3 algorithm for selection of a\nsubset of available measurements as CVs through minimization of L1. For each test, six random matrices\nare generated: three full matrices, Gy \u2208 Rny\u00d7nu , Gyd \u2208 Rny\u00d7nd and Jud \u2208 Rnu\u00d7nd , and three diagonal\nmatrices, We \u2208 Rny\u00d7ny , Wd \u2208 Rnd\u00d7nd and Juu \u2208 Rnu\u00d7nu . The elements of these matrices are normally\ndistributed with zero mean and unit variance. For all tests, we use nd = 5, while nu and ny are varied.\nThe first and second tests are designed to select nu = 5 and nu = ny \u2212 5 out of ny measurements,\nrespectively. Each selection problem is tested for 100 sets of randomly generated matrices and the average\ncomputation time and average number of nodes evaluated are summarized in Figure 4. It is seen that\nupwards pruning based algorithm (UP) is more suitable for problems involving selection of a few variables\n23\n10\u22122\n100\n102\ncp\nu \ntim\ne,\n s\n(a)\n10 100 200 300 400 500\n100\n104\n108\n1012\nny\ne\nva\nlu\nat\nio\nns\n(b)\n10\u22122\n100\n102\ncp\nu \ntim\ne,\n s\n(c)\n10 100 200 300 400 500\n100\n104\n108\n1012\nny\ne\nva\nlu\nat\nio\nns\n(d)\n \n \nDOWN UP B3 BRUTE\nFigure 4: Random test 1: selection of 5 out of ny measurements, (a) computation time against ny and (b)\nnumber of nodes evaluated against ny; Random test 2: selection of ny \u2212 5 out of ny measurements, (c)\ncomputation time against ny and (d) number of nodes evaluated against ny.\n24\nfrom a large candidate set, whilst downwards pruning based algorithm (DOWN) is more efficient for\nproblems, where a few among many candidate variables need to be discarded to find the optimal solution.\nThe solution times for UP and DOWN algorithms increase only modestly with problem size, when nu <<\nny and nu \u2248 ny, respectively. The solution times for the B3 algorithm is similar to the better of UP and\nDOWN algorithms, however, its efficiency is insensitive to the kind of selection problem.\n10\u22122\n100\n102\ncp\nu \ntim\ne,\n s\n(a)\n5 10 15 20\n100\n104\n108\n1012\nn\nu\ne\nva\nlu\nat\nio\nns\n(b)\n10\u22122\n100\n102\ncp\nu \ntim\ne,\n s\n(c)\n1 10 20 30 40\n100\n104\n108\n1012\nn\nu\ne\nva\nlu\nat\nio\nns\n(d)\n \n \nDOWN UP B3 BRUTE\nFigure 5: Random test 3: selection of nu out of ny = 2nu measurements, (a) computation time against nu\nand (b) number of nodes evaluated against nu; Random test 4: selection of nu out of 40 measurements,\n(c) computation time against nu and (d) number of nodes evaluated against nu.\nThe third test consists of selecting nu out of ny = 2nu measurements with nu increasing from 5 to 20, while\nthe fourth test involves selecting nu out of ny = 40 variables with nu ranging from 1 to 39. For each nu,\n100 sets of random matrices are generated and the average computation time and average number of nodes\nevaluated are summarized in Figure 5. While the UP and DOWN problems show reasonable performance\nfor small nu, their performances degrade rapidly for the fourth test, when nu approaches ny\/2. Within 300\nseconds, both UP and DOWN algorithms can only handle problems with nu < 18. For all cases, however,\n25\nthe B3 algorithm exhibits superior efficiency by combining upward and downward pruning and is able to\nsolve problems up to nu = 20 within 100 seconds.\nIn summary, for selection of individual measurements as CVs by minimizing L1, all the developed algo-\nrithms (UP, DOWN and B3) show much superior performance than the currently used brute force method.\nIn comparison with the UP and DOWN algorithms, the B3 algorithm shows superior performance and\nsimilar efficiency for different problem dimensions including problems with nu << ny, nu \u2248 ny and\nnu \u2248 ny\/2.\n5.2 Distillation column case study\nTo demonstrate the efficiency of the developed PB3 algorithm, we consider self-optimizing control of a\nbinary distillation column [15]. The objective is to minimize the deviation of the distillate and bottoms\ncomposition from their nominal steady-state values in presence of disturbances in feed flow rate, feed\ncomposition and vapor fraction of feed. Two degrees of freedom (reflux and vapor boilup rates) are available\nand thus two CVs are required for implementation of self-optimizing control strategy. It is considered that\nthe temperatures on 41 trays are measured with an accuracy of \u00b10.5o C. The combinatorial optimization\nproblem involves selection of n out of 41 candidate measurements, whose combinations can be used as\nCVs. The reader is referred to [10] for further details of this case study.\nThe PB3 algorithm is used to select the 10 best measurement combinations for every n, where n ranges\nfrom 2 to 41. The trade-off between the losses corresponding to the 10 best selections and n is shown\nin Figure 6(a). It can be seen that when combinations of more than 14 measurements are used as CVs,\nthe loss is less than 0.075, which is close to the minimum loss (0.0517) seen using combinations of all\n41 measurements. Furthermore, the reduction in loss is negligible, when combinations of more than 25\nmeasurements are used. Figure 6(a) also shows that the 10 best selections have similar self-optimizing\ncapabilities particularly when combinations of more than 5 measurements are used. Then, the designer\ncan choose the subset of measurements among these 10 best alternatives based on some other important\ncriteria, such as dynamic controllability.\nFigure 6(b) and (c) show the computation time and number of node evaluations for PB3 and DOWN\nalgorithms. To facilitate the comparison further, the ratios of number of node evaluations and computation\ntimes are also shown in Figure 6(d). The PB3 algorithm is able to reduce the number of node evaluations\nand hence computation time up to a factor of 20 for selection problems involving selection of a few\nmeasurements from a large candidate set. It is expected that a fully upwards pruning rule would improve\n26\n5 10 15 20 25 30 35 40\n0\n0.1\n0.2\n0.3\nLo\nss\n(a)\n5 10 15 20 25 30 35 40\n0\n100\n200\n300\nCP\nU \ntim\ne,\n s\n(b)\n \n \nPB3\nDOWN\n5 10 15 20 25 30 35 40\n100\n105\n1010\nN\num\nbe\nr o\nf e\nva\nlu\nat\nio\nns\n(c)\n \n \nPB3\nDOWN\nBRUTE\n5 10 15 20 25 30 35 40\n5\n10\n15\n20\n25\nNumber of measurments seleted\nD\nO\nW\nN \n: P\nB3\n(d)\n \n \ntime ratio\nevaluation number ratio\nFigure 6: (a) Losses of 10-best measurement combinations against the number of measurements, (b)\nComparison of computation time between PB3 and DOWN algorithms, (c) Comparison of number of node\nevaluations between PB3 and DOWN algorithms, and (d) Ratios of computation time and number of node\nevaluations required by PB3 and DOWN algorithms\n27\nthe efficiency even further, but the derivation of such a rule is currently an open problem.\nOverall, both algorithms are very efficient and are able to reduce the number of node evaluations by\n5 to 6 orders of magnitude, as compared to the brute force search method. For example, to select 20\nmeasurements from 41 candidates, evaluation of a single alternative requires about 0.15 ms on the specified\nnotebook computer. Thus, a brute force search methods would take more than one year to evaluate all\npossible alternatives. However, the proposed PB3 and DOWN algorithms are able to solve this problem\nwithin 4 and 12 seconds, respectively. Therefore, the generation of the trade-off curve shown in Figure 6(a)\nwould be practically impossible without the algorithms developed in the work.\n6 Conclusions\nIn this paper, the concept of bidirectional branch and bound (BAB) proposed in Part I of this work [5] has\nbeen further developed for selection of controlled variables (CVs) using the exact local method for self-\noptimizing control. The numerical tests using randomly generated matrices and binary distillation column\ncase study show that the number of evaluations for proposed algorithms is 5 to 6 orders of magnitude\nlower than the current practice of CV selection using brute force search.\nThe computationally efficiency of the algorithms developed in this paper based on bidirectional pruning\nand branching principles using novel determinant based criteria is similar to the BAB approach for CV\nselection based on minimum singular value (MSV) rule [5]. Despite the availability of the exact local\ncriterion, one of the apparent reasons for continued use of the approximate MSV rule is its computational\nefficiency. This work makes CV selection using the exact local criterion computationally tractable so that\nit can be adopted as a standard tool for selection of CVs based on the concept of self-optimizing control.\nWhile the algorithm for selection of individual measurements as CVs is fully bidirectional, the algorithm for\nselection of subset of measurements, whose combinations can be used as CVs, is only partially bidirectional.\nIt is expected that the development of a fully bidirectional BAB algorithm for the latter problem would\nimprove the computational efficiency further. This challenging problem is currently open and is an issue\nfor future research. Furthermore, an extension of the bidirectional BAB algorithm to select CVs based on\nthe minimization of local average loss for self-optimizing control [13] is currently under consideration.\n28\nAcknowledgements\nThe first author gratefully acknowledges the financial support from Office of Finance, Nanyang Techno-\nlogical University, Singapore through grant no. R42\/06.\nReferences\n[1] V. Alstad. Studies on Selection of Controlled Variables. PhD thesis, Norwegian University of Science\nand Technology, Trondheim, Norway, 2005. Available at http:\/\/www.nt.ntnu.no\/users\/skoge\/\npublications\/thesis\/2005_alstad\/.\n[2] V. Alstad and S. Skogestad. Null space method for selecting optimal measurement combinations as\ncontrolled variables. Ind. Eng. Chem. Res., 46(3):846\u2013853, 2007.\n[3] V. Alstad, S. Skogestad, and E. S. Hori. Optimal measurement combinations as controlled variables.\nJ. Proc. Control, 19(1):138\u2013148, 2009.\n[4] Y. Cao and V. Kariwala. B3WC. MATLAB File Exchange, January 2009. Available at http:\n\/\/www.mathworks.com\/matlabcentral\/fileexchange\/22632.\n[5] Y. Cao and V. Kariwala. Bidirectional branch and bound for controlled variable selection: Part I.\nPrinciples and minimum singular value criterion. Comput. Chem. Engng., 32(10):2306\u20132319, 2008.\n[6] Y. Cao and P. Saha. Improved branch and bound method for control structure screening. Chem.\nEngg. Sci., 60(6):1555\u20131564, 2005.\n[7] Y. Cao, D. Rossiter, and D. H. Owens. Globally optimal control structure selection using branch and\nbound method. In Proc. 5th International Symposium on DYCOPS, pages 183\u2013188, Corfu, Greece,\n1998.\n[8] X.-W. Chen. An improved branch and bound algorithm for feature selection. Pattern Recognition\nLetters, 24:1925\u20131933, 2003.\n[9] I. J. Halvorsen, S. Skogestad, J. C. Morud, and V. Alstad. Optimal selection of controlled variables.\nInd. Eng. Chem. Res., 42(14):3273\u20133284, 2003.\n[10] E. S. Hori and S. Skogestad. Selection of controlled variables: Maximum gain rule and combination\nof measurements. Ind. Eng. Chem. Res., 47(23):9465\u20139471, 2008.\n29\n[11] R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, Cambridge, UK, 1985.\n[12] V. Kariwala. Optimal measurement combination for local self-optimizing control. Ind. Eng. Chem.\nRes., 46(11):3629\u20133634, 2007.\n[13] V. Kariwala, Y. Cao, and S. Janardhanan. Local self-optimizing control with average loss minimiza-\ntion. Ind. Eng. Chem. Res., 47(4):1150\u20131158, 2008.\n[14] P. Narendra and K. Fukunaga. A branch and bound algorithm for feature subset selection. IEEE\nTrans. Comput., C-26:917\u2013922, 1977.\n[15] S. Skogestad. Dynamics and control of distillation columns - A tutorial introduction. Trans. IChemE\nPart A, 75:539\u2013562, 1997.\n[16] S. Skogestad. Plantwide control: The search for the self-optimizing control structure. J. Proc. Control,\n10(5):487\u2013507, 2000.\n[17] S. Skogestad and I. Postlethwaite. Multivariable Feedback Control: Analysis and Design. John Wiley\n& sons, Chichester, UK, 1st edition, 1996.\n[18] P. Somol, P. Pudil, F. Ferri, and J. Kittler. Fast branch and bound algorithm in feature selection.\nIn B. Sanchez, J. Pineda, J. Wolfmann, Z. Bellahsense, and F. Ferri, editors, Proceedings of World\nMulticonference on Systemics, Cybernetics and Informatics, volume VII, pages 1646\u2013651, Orlando,\nFlorida, USA, 2000.\n[19] M. Van de Wal and B. de Jager. A review of methods for input\/output selection. Automatica, 37(4):\n487\u2013510, 2001.\n[20] B. Yu and B. Yuan. A more efficient branch and bound algorithm for feature selection. Pattern\nRecognition, 26:883\u2013889, 1993.\n30\n"}