{"doi":"10.1109\/TNS.2004.828875","coreId":"69521","oai":"oai:eprints.lancs.ac.uk:26343","identifiers":["oai:eprints.lancs.ac.uk:26343","10.1109\/TNS.2004.828875"],"title":"An overview of the ATLAS High Level Trigger Dataflow and Supervision.","authors":["ATLAS, TDAQ authorlist","Smizanska, Maria"],"enrichments":{"references":[{"id":1009480,"title":"An overview of algorithms for the ATLAS high-level trigger\u201d (in Available Online: http:\/\/doc.cern.ch\/\/archive\/electronic\/ cern\/others\/atlnot\/Note\/conf\/conf-2003-003.pdf),","authors":[],"date":"2003","doi":null,"raw":"S. Armstrong, \u201cAn overview of algorithms for the ATLAS high-level trigger\u201d (in Available Online: http:\/\/doc.cern.ch\/\/archive\/electronic\/ cern\/others\/atlnot\/Note\/conf\/conf-2003-003.pdf), IEEE Trans. Nucl. Sci., pp. 367\u2013374, June 2003.","cites":null},{"id":1008809,"title":"ATLAS high-level trigger group author list for real time","authors":[],"date":"2003","doi":null,"raw":"ATLAS high-level trigger group author list for real time 2003 (2003, May). [Online]. Available: http:\/\/atlas.web.cern.ch\/Atlas\/ GROUPS\/DAQTRIG\/HLT\/AUTHORLISTS\/rt2003.pdf","cites":null},{"id":1009135,"title":"Event Filter Event Handler Design,","authors":[],"date":"2002","doi":null,"raw":"ATLAS TDAQ\/DCS Event Filter Event Handler Design, C. Bee et al.. (2002, July). [Online]. Available: https:\/\/edms.cern.ch\/document\/ 367089\/1","cites":null},{"id":1010382,"title":"Event Filter Test Results for the EF Supervision,","authors":[],"date":"2003","doi":null,"raw":"ATLAS TDAQ Event Filter Test Results for the EF Supervision, S. Wheeler, F. Touchard, Z. Qian, C. Meessen, and A. Negri. (2003, Mar.).","cites":null},{"id":1009564,"title":"GAUDI\u2014A software architecture and framework for building HEP data processing applications. presented at","authors":[],"date":null,"doi":"10.1016\/S0010-4655(01)00254-5","raw":"G. Barrand et al.. GAUDI\u2014A software architecture and framework for building HEP data processing applications. presented at Proc. CHEP 2000 Computing in High Energy and Nuclear Physics. [Online]. Available: http:\/\/lhcb-comp.web.cern.ch\/lhcb-comp\/General\/Publications\/longpap-a152.pdf","cites":null},{"id":1010673,"title":"High-Level Trigger Data Acquisition and Controls Tech. Design Rep.,\u201d ATLAS HLT\/DAQ\/DCS group,","authors":[],"date":"2003","doi":null,"raw":"\u201cThe ATLAS High-Level Trigger Data Acquisition and Controls Tech. Design Rep.,\u201d ATLAS HLT\/DAQ\/DCS group, Geneva, Switzerland, ATLAS TDR-016, June 2003. CERN. Authorized licensed use limited to: Lancaster University Library. Downloaded on May 1, 2009 at 10:28 from IEEE Xplore.  Restrictions apply.","cites":null},{"id":1010084,"title":"Online software for the ATLAS test beam data acquisition system\u201d (in Available Online: http:\/\/doc.cern.ch\/\/archive\/electronic\/cern\/others\/atlnot\/Note\/daq\/daq-2003-044.pdf),","authors":[],"date":"2003","doi":null,"raw":"I. Alexandrov et al., \u201cOnline software for the ATLAS test beam data acquisition system\u201d (in Available Online: http:\/\/doc.cern.ch\/\/archive\/electronic\/cern\/others\/atlnot\/Note\/daq\/daq-2003-044.pdf), IEEE Trans. Nucl. Sci., pp. 578\u2013584, June 2003.","cites":null},{"id":1009775,"title":"Site","authors":[],"date":"2001","doi":"10.1023\/A:1015133818512","raw":"Athena Web Site (2001, Jan.). [Online]. Available: http:\/\/atlas.web. cern.ch\/Atlas\/GROUPS\/SOFTWARE\/OO\/architecture\/General\/index. html","cites":null},{"id":1009066,"title":"The base-line dataflow system of the ATLAS trigger and DAQ\u201d (in Available Online: http:\/\/doc.cern.ch\/\/archive\/electronic\/ cern\/others\/atlnot\/Communication\/daq\/com-daq-2003-037.pdf),","authors":[],"date":"2003","doi":null,"raw":"H. P. Beck et al., \u201cThe base-line dataflow system of the ATLAS trigger and DAQ\u201d (in Available Online: http:\/\/doc.cern.ch\/\/archive\/electronic\/ cern\/others\/atlnot\/Communication\/daq\/com-daq-2003-037.pdf), IEEE Trans. Nucl. Sci., pp. 470\u2013475, June 2003.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2004-06","abstract":"The ATLAS high-level trigger (HLT) system provides software-based event selection after the initial LVL1 hardware trigger. It is composed of two stages, the LVL2 trigger and the event filter (EF). The LVL2 trigger performs event selection with optimized algorithms using selected data guided by Region of Interest pointers provided by the LVL1 trigger. Those events selected by LVL2 are built into complete events, which are passed to the EF for a further stage of event selection and classification using off-line algorithms. Events surviving the EF selection are passed for off-line storage. The two stages of HLT are implemented on processor farms. The concept of distributing the selection process between LVL2 and EF is a key element in the architecture, which allows it to be flexible to changes (luminosity, detector knowledge, background conditions, etc.) Although there are some differences in the requirements between these subsystems there are many commonalities. An overview of the dataflow (event selection) and supervision (control, configuration, monitoring) activities in the HLT is given, highlighting where commonalities between the two subsystems can be exploited and indicating where requirements dictate that implementations differ. An HLT prototype system has been built at CERN. Functional testing is being carried out in order to validate the HLT architecture","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/69521.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/26343\/1\/getPDF.pdf","pdfHashValue":"9158f3b16b940ce4ae356b6a9362885a2c0b1939","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:26343<\/identifier><datestamp>\n      2018-01-24T02:48:05Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5143<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        An overview of the ATLAS High Level Trigger Dataflow and Supervision.<\/dc:title><dc:creator>\n        ATLAS, TDAQ authorlist<\/dc:creator><dc:creator>\n        Smizanska, Maria<\/dc:creator><dc:subject>\n        QC Physics<\/dc:subject><dc:description>\n        The ATLAS high-level trigger (HLT) system provides software-based event selection after the initial LVL1 hardware trigger. It is composed of two stages, the LVL2 trigger and the event filter (EF). The LVL2 trigger performs event selection with optimized algorithms using selected data guided by Region of Interest pointers provided by the LVL1 trigger. Those events selected by LVL2 are built into complete events, which are passed to the EF for a further stage of event selection and classification using off-line algorithms. Events surviving the EF selection are passed for off-line storage. The two stages of HLT are implemented on processor farms. The concept of distributing the selection process between LVL2 and EF is a key element in the architecture, which allows it to be flexible to changes (luminosity, detector knowledge, background conditions, etc.) Although there are some differences in the requirements between these subsystems there are many commonalities. An overview of the dataflow (event selection) and supervision (control, configuration, monitoring) activities in the HLT is given, highlighting where commonalities between the two subsystems can be exploited and indicating where requirements dictate that implementations differ. An HLT prototype system has been built at CERN. Functional testing is being carried out in order to validate the HLT architecture.<\/dc:description><dc:date>\n        2004-06<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/26343\/1\/getPDF.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/TNS.2004.828875<\/dc:relation><dc:identifier>\n        ATLAS, TDAQ authorlist and Smizanska, Maria (2004) An overview of the ATLAS High Level Trigger Dataflow and Supervision. IEEE Transactions on Nuclear Science, 51 (3 Part). pp. 361-366. ISSN 0018-9499<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/26343\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1109\/TNS.2004.828875","http:\/\/eprints.lancs.ac.uk\/26343\/"],"year":2004,"topics":["QC Physics"],"subject":["Journal Article","PeerReviewed"],"fullText":"IEEE TRANSACTIONS ON NUCLEAR SCIENCE, VOL. 51, NO. 3, JUNE 2004 361\nAn Overview of the ATLAS High-Level\nTrigger Dataflow and Supervision\nJ. T. Baines, C. P. Bee, A. Bogaerts, M. Bosman, D. Botterill, B. Caron, A. dos Anjos, F. Etienne, S. Gonz\u00e1lez,\nK. Karr, W. Li, C. Meessen, G. Merino, A. Negri, J. L. Pinfold, P. Pinto, Z. Qian, F. Touchard, P. Werner, S. Wheeler,\nF. J. Wickens, W. Wiedenmann, and G. Zobernig\nAbstract\u2014The ATLAS high-level trigger (HLT) system provides\nsoftware-based event selection after the initial LVL1 hardware\ntrigger. It is composed of two stages, the LVL2 trigger and the\nevent filter (EF). The LVL2 trigger performs event selection with\noptimized algorithms using selected data guided by Region of In-\nterest pointers provided by the LVL1 trigger. Those events selected\nby LVL2 are built into complete events, which are passed to the\nEF for a further stage of event selection and classification using\noff-line algorithms. Events surviving the EF selection are passed\nfor off-line storage. The two stages of HLT are implemented on\nprocessor farms. The concept of distributing the selection process\nbetween LVL2 and EF is a key element in the architecture, which\nallows it to be flexible to changes (luminosity, detector knowledge,\nbackground conditions, etc.) Although there are some differences\nin the requirements between these subsystems there are many\ncommonalities. An overview of the dataflow (event selection) and\nsupervision (control, configuration, monitoring) activities in the\nHLT is given, highlighting where commonalities between the two\nsubsystems can be exploited and indicating where requirements\ndictate that implementations differ. An HLT prototype system\nhas been built at CERN. Functional testing is being carried out in\norder to validate the HLT architecture.\nIndex Terms\u2014ATLAS, event selection, high level trigger, trigger\ncontrol and supervision.\nNOTE: This paper was presented by Sarah Wheeler on behalf\nof the ATLAS High-Level Trigger Group [1].\nI. INTRODUCTION\nATLAS is a general-purpose high-energy physics exper-iment for recording proton-proton collisions, currently\nunder construction at the Large Hadron Collider (LHC) at\nCERN. ATLAS has been designed to study the largest possible\nrange of physics at the LHC including searches for as yet\nunobserved phenomena such as the Higgs boson and super-\nsymmetry.\nManuscript received June 2, 2003; revised November 26, 2003.\nJ. T. Baines, D. Botterill, W. Li, and F. J. Wickens are with Rutherford Ap-\npleton Laboratory, Chilton, Oxon OX11 0QX, U.K.\nC. P. Bee, F. Etienne, C. Meessen, Z. Qian, and F. Touchard are with CPPM,\n13288 Marseille, France.\nA. Bogaerts, A. dos Anjos, P. Pinto, and P. Werner are with CERN, CH-1211\nGeneva 23, Switzerland.\nM. Bosman, K. Karr, and G. Merino are with Barcelona IFAE, 08194 Bel-\nlaterra, Spain.\nB. Caron, J. L. Pinfold, and S. Wheeler are with the University of Alberta,\nEdmondton, AL T6G 2N5, Canada (e-mail: sarah.wheeler@cern.ch).\nS. Gonz\u00e1lez, W. Wiedenmann, and G. Zobernig are with the University of\nWisconsin, Madison, WI 53706 USA.\nA. Negri is with the Universit\u00e0 di Pavia e I.N.F.N., 27100 Pavia, Italy.\nDigital Object Identifier 10.1109\/TNS.2004.828875\nThe ATLAS Trigger and Data Acquisition (TDAQ) system\nwill have to deal with extremely high data rates, due both to\nthe high bunch crossing frequency at the LHC (40 MHz) and\nthe large amount of data produced by the ATLAS detector it-\nself (1\u20132 Mbytes per event). The task of the TDAQ system is to\nselect from this unprecedented amount of data, the most inter-\nesting events and save them for later analysis at a rate of about\n200 per second. ATLAS relies on a three-level trigger system\nto perform the selection: a very fast, coarse granularity, hard-\nware-based LVL1 trigger which reduces the event rate to 75 kHz\nfollowed by two software-based triggers, the LVL2 trigger and\nthe Event Filter (EF) which perform increasingly fine-grained\nselection of events at lower rates. The LVL2 trigger, working on\na subset of full granularity detector data reduces the rate to about\n2 kHz and finally the EF using full event data reduces the rate\nto about 200 Hz after which the selected events are written to\nmass storage. The LVL2 and EF comprise the ATLAS high-level\ntrigger (HLT) system.\nII. OVERVIEW OF THE HLT\nThe HLT is a software trigger implemented as software\napplications running on large processor farms consisting of\ncommodity components connected via high-speed Ethernet\nnetworks. For the sake of simplicity and flexibility the LVL2\ntrigger and event filter farms have been implemented separately.\nThe farms themselves are split into subfarms for reasons of\npracticality and organization.\nThe role of the HLT is to reduce the LVL1 trigger rate to a\nrate compatible with writing the events to mass storage. Note\nthat this is no longer a technical limit. It is rather constrained by\nthe cost of storage devices and on the computing power avail-\nable to analyze the data off-line. Input to the HLT consists of\nevents that have passed the LVL1 trigger. The LVL1 trigger is\na coarse-grained hardware trigger which uses information from\nthe calorimeters and muon detectors only. In addition to pro-\nviding the trigger result, the LVL1 trigger identifies the geo-\ngraphical locations in the detector of candidate muons, elec-\ntrons, photons, jets and hadrons. These are known as regions of\ninterest (RoIs). The RoIs are classified into two types. Primary\nRoIs that caused the LVL1 accept, and secondary RoIs, which\nwere not used in the LVL1 accept. Both types of RoI are used to\nseed the LVL2 selection. The concept of seeded reconstruction\nis fundamental to the HLT.\nIn parallel with forwarding accepted LVL1 events to the HLT,\nthe corresponding event fragments are sent from the detector\n0018-9499\/04$20.00 \u00a9 2004 IEEE\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on May 1, 2009 at 10:28 from IEEE Xplore.  Restrictions apply.\n362 IEEE TRANSACTIONS ON NUCLEAR SCIENCE, VOL. 51, NO. 3, JUNE 2004\nFig. 1. Functional decomposition of the ATLAS HLT system. The boxes represent the different functions and the name of the software application(s) implementing\neach function are shown in brackets. The arrows represent the exchange of messages and event data between the various functional components.\nfront-end electronics to the readout systems (ROS) which con-\ntain readout buffers.\nThe various actions which are then executed by the HLT are\nsummarized in the following sections and in Fig. 1. The figure\nis a logical representation of the HLT, where the boxes repre-\nsent the various functions and the name(s) in brackets are those\nof the software application(s) which implement each function.\nIt should be noted that each function is implemented as many\ninstances of the associated software applications.\nA. LVL2 Supervisor\nThe LVL1 trigger sends the RoI information (LVL1 Result)\nto one of several LVL2 Event Supervisor applications which\nimplement the first function of the LVL2 trigger and therefore\nof the HLT. The LVL2 Event Supervisor allocates the event to\none of many LVL2 Processing Unit applications (L2PUs) and\nforward the LVL1 result to the selected L2PU.\nB. LVL2 Processing Unit\nHighly optimized algorithms run in the L2PU applications\nand perform the selection, which is broken down into a series\nof sequential processing steps. Typically, the first step involves\nconfirming the LVL1 trigger RoIs. Full-granularity event data\n(Event Fragments) from the calorimeters and muon detectors\nare requested from the relevant ROS applications and analyzed.\nIn subsequent steps, data are requested from other detectors,\ne.g., tracking detectors, corresponding to the RoI and analyzed.\nAt each step, if the result is not consistent with any of the pos-\nsible physics candidates, the event is rejected and no further pro-\ncessing occurs. Only the events surviving all steps are accepted\nby the LVL2 trigger. For these, the detailed LVL2 Result is sent\nto the pROS. For all events the LVL2 Decision is sent to the\nLVL2 Event Supervisor.\nThe LVL2 selection uses full-granularity event data, in-\ncluding information from the inner tracking chambers not\navailable to the LVL1 trigger. The RoI mechanism and sequen-\ntial processing allows this to be done transferring only a few\npercent of the entire event data to the LVL2 trigger, thereby\nconsiderably reducing the network bandwidth required (to\nGbyte\/s).\nC. Event Building\nThe LVL2 Event Supervisor passes the LVL2 decisions to\nthe event builder and the events accepted by the LVL2 trigger\nare built. Event fragments in the ROSs are collected under the\nguidance of Data Flow Manager (DFM) applications and as-\nsembled into complete event records by SubFarm Input (SFI)\napplications.\nFor the event building, the pROS is treated as just another\nROS so the LVL2 Result is included in the event to be built.\nThe bandwidth required for the event building is Gbyte\/s.\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on May 1, 2009 at 10:28 from IEEE Xplore.  Restrictions apply.\nBAINES et al.: ATLAS HLT DATAFLOW AND SUPERVISION 363\nD. Event Handler\nThe complete events are sent from the event builder to Event\nFilter Dataflow applications (EFDs) running in the event han-\ndler (see Fig. 1). The role of the EFD is to distribute the events to\nso-called Processing Task applications (PTs) which run off-line\nfiltering algorithms, adapted for the EF, to perform the selection\nprocess using the entire event data. It is foreseen that the LVL2\nresult, passed via the pROS should be used by the algorithms to\nseed the selection in order to reduce the time taken to obtain a\nresult.\nE. DataBaseLoader\nEvents selected by the event handler are sent to permanent\nstorage, via the DataBaseLoader implemented by SubFarm\nOutput (SFO) applications.\nIII. FUNCTIONAL COMPONENTS OF THE HLT\nThere are many similarities between the two stages of the\nHLT. The boundary between LVL2 and EF is intentionally flex-\nible in order that the HLT can be configured to take into account\ndifferent running environments. The aim of this section is to dis-\ncuss each of these aspects in turn, describing where similarities\nexist between LVL2 and the EF and how these may be exploited\nand why in other cases, due to differing requirements, imple-\nmentations differ.\nThe HLT consists of three main functional parts:\n1) the dataflow code, i.e., the code responsible for transfer-\nring the event data and trigger decisions to and from the\nselection algorithms;\n2) the event selection software (algorithms) and the inter-\nfaces required for obtaining event data;\n3) the supervision system, the software responsible for\npreparing the HLT for datataking activities.\nA. Dataflow\nThe dataflow system is responsible for moving event data and\ntrigger decisions to and from the selection algorithms. Imple-\nmentations differ between LVL2 and the EF.\n1) LVL2 Dataflow: In the LVL2 trigger, dataflow activities\nare performed by the LVL2 Event Supervisor and the L2PU ap-\nplications. These are two of the applications implemented using\nthe dataflow framework [2]. Dataflow is a system of the TDAQ\nresponsible for the movement of event data from the ROS to\nmass storage via the LVL2 trigger and the EF. The LVL2 Event\nSupervisor applications send LVL1 results to the L2PU applica-\ntions. In order to achieve load balancing on the L2PUs, it is fore-\nseen that LVL1 results are assigned to L2PUs on a round-robin\nor a least-queued basis. The LVL1 result contains only the LVL1\nevent ID, trigger type and a list of primary and secondary RoIs.\nIt does not contain any event data. It is the responsibility of the\nL2PU applications to access over the network the event data re-\nquired from the ROSs. Full event building at the LVL2 input\nrate would require more than 100 Gbyte\/s bandwidth, however\nby limiting the data requests to the RoIs and only fetching data\nas algorithms need them the bandwidth reduces to a much more\nmanageable level ( Gbyte\/s). The data for each request are\nassembled and passed to the requesting algorithm via the inter-\nface to the LVL2 algorithms inside the L2PU.\n2) EF Dataflow: The EF is located after event building,\nwhere complete events are already assembled and available,\ntherefore allowing dataflow and selection tasks to be cleanly\nseparated. The dataflow is implemented by the EFD application\n[3], which receives complete events from the SFI applications\nand delivers them to the PTs, where event selection takes place.\nIt subsequently collects the analyzed events and delivers them\nto mass storage via the SFO applications. The EF is the first part\nof the TDAQ system in which complete events are available. It\nis therefore a convenient place to perform physics monitoring,\ncalibration and alignment procedures. Therefore, in addition\nto making events available to the PTs running the selection\nalgorithms, the EFD must sort events, using information in the\nevent headers to different types of Processing Tasks according\nto the event type. For example, it should send calibration events\nto the appropriate detector calibration task.\nThe EFD functionality is divided into specific internal tasks\nthat can be dynamically interconnected to form an EF dataflow\nnetwork within the EFD application. The tasks can be purely\ninternal (sorting to different data streams, internal monitoring\nof dataflow) or provide the interface to external components\n(SFI, SFO, PTs). Within the Event Handler the dataflow is based\non reference passing, using a pointer to the event (stored in a\nmemory-mapped file) between the different tasks, thus avoiding\ncopying the events.\nB. Event Selection Software (ESS)\nThe tasks of the ESS are \u201cevent selection\u201d and \u201cevent clas-\nsification.\u201d The HLT algorithms construct objects representing\nphysics candidates such as electrons, jets, and muons. An event\nis selected if the reconstructed object is consistent with at least\none physics signature. In both the LVL2 and the EF, events are\nrejected if they do not pass any of the selection criteria.\nA common ESS architecture has been developed for use\nacross HLT. This greatly simplifies migration of algorithms be-\ntween the various environments. For instance, although LVL2\nalgorithms will not be used in the off-line context to analyze\ndata they can be developed and tested in the more user-friendly\noff-line environment. Furthermore, it becomes trivial to move\nLVL2 algorithms to the EF. For instance, LVL2 algorithms\nmight be moved to the EF to crosscheck the efficiency of the\nalgorithms, or to take advantage of more accurate calibration\ninformation. Due to the short decision time of the LVL2 trigger,\nL2PUs will only be able to access calibration databases at\nthe start of each datataking run. In the EF it will be possible\nto access calibration databases on an event-by-event basis. It\nshould also be remembered that flexibility in the configuration\nof the HLT will be vital for tuning the trigger to select events\ncorresponding to novel, unpredicted physics.\nSince constraints differ for the LVL2 trigger and EF, dif-\nferent implementations exist for the algorithms and dataflow\ninterfaces. These are described in more detail in the following\nsections.\n1) LVL2 Trigger Selection Algorithms: The ESS in the\nLVL2 trigger is implemented by specially written algorithms\nrunning in the L2PUs, further details may be found in [4]. In the\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on May 1, 2009 at 10:28 from IEEE Xplore.  Restrictions apply.\n364 IEEE TRANSACTIONS ON NUCLEAR SCIENCE, VOL. 51, NO. 3, JUNE 2004\nFig. 2. The plot shows the time required to launch varying numbers of EF PTs on a 212 node testbed using the Axis implementation of Web services. The number\nof PTs launched per processing node was varied from 2 to 80. Good linearity is observed and the performance is satisfactory considering that no optimization in\nthe use of the services was sought and no asynchronous operations were used due to limitations in the current implementation.\nLVL2 trigger, the average trigger processing time must be short\n( 10 ms) because of the high rate to be handled. Therefore,\nalgorithms must be highly optimized. Furthermore, no disk\naccess is allowed while datataking is in progress requiring that\nall necessary database information is available in memory.\nAlgorithms running in the LVL2 environment must also sat-\nisfy stringent thread-safety rules. In the L2PU, the selection\nalgorithms run inside one of several \u201cworkerthreads,\u201d where\neach workerthread is responsible for processing one event. This\nmultithreaded approach has been chosen to avoid stalling the\nCPU when waiting for requested RoI data to arrive from the\nROSs. It also allows efficient use of multi-CPU processors.\n2) EF Selection Algorithms: In the EF the requirement on\nthe trigger decision time is less severe ( 1 s). This allows ac-\ncess to disk-based databases during datataking activities, per-\nmitting the use of up-to-date calibration constants in the event\nreconstruction and selection procedure. Second, the algorithms\nrun in single-threaded processing tasks. Therefore, algorithms\nneed not be thread-safe. Studies are underway [4] in the EF\nto assess the suitability of all the off-line algorithms for use in\nthe on-line environment and changes implemented where nec-\nessary. For example, although the trigger decision time is less\ncritical in the EF, it may still not be entirely practical to run the\nalgorithm with all the precision that it would have in an off-line\nenvironment because it takes too long. It may be \u201cdetuned\u201d in\norder to achieve a decision more quickly.\n3) Dataflow Interfaces: Due to the different environments\nin which the ESS runs, the interface used by the ESS to col-\nlect event data is different in LVL2 and EF as well as in the\noff-line situation. In the case of LVL2, a subset of detector data,\nin on-line format, corresponding to LVL1 RoIs is fetched across\nthe network from the ROSs. In the EF, full-event data, in on-line\nformat, is accessed from the EFD via a memory-mapped file. In\nthe off-line environment, the event data will be received from\ndatabases, in off-line format. However, a key design decision\nhas been that the data should be delivered to the algorithms\nin exactly the same way in all environments. The algorithms\nthemselves are not aware of where they are running. This has\nbeen achieved by using the off-line Gaudi [5] and ATHENA [6]\nframeworks to implement the LVL2 and EF interfaces. The in-\nterfaces also provide data conversion services to transform the\nincoming event data from the on-line format into the off-line\nformat expected by the algorithms.\nC. Supervision\nThe supervision system [7] is responsible for all aspects of\nsoftware task management and control in the HLT. Mandates\nof the supervision system include: configuration of the HLT\nsoftware processes, synchronizing the HLT processes with\ndatataking activities in the rest of the experiment, monitoring\nthe status of HLT processes, e.g., checking that they are running\nand restarting crashed processes. Both the LVL2 trigger and the\nEF are implemented as hundreds of software processes running\non large processor farms, split for reasons of practicality into a\nnumber of subfarms. In view of this, the supervision require-\nments for the two systems are very similar and an integrated\nHLT supervision system has been developed. It has been\nimplemented using services provided by the Online Software\n(OnlineSW) [8]. The Online Software is a system of the TDAQ\nproject. It encompasses the software to configure, control, and\nmonitor the TDAQ. It does not contain any elements that are\ndetector specific and has been successfully adapted for use in\nthe HLT trigger farms.\nThe OnlineSW configuration database is used to describe\nthe HLT in terms of the software processes and hardware (pro-\ncessing nodes) of which it is comprised. The HLT supervision\nand control system uses information stored in the OnlineSW\nconfiguration database to determine which processes need to\nbe started on which hardware and subsequently monitored and\ncontrolled. The smallest set of HLT elements, which can be con-\nfigured and controlled independently from the rest of the TDAQ\nsystem is the subfarm. This allows subfarms to be dynamically\nincluded\/excluded from partitions during datataking. Synchro-\nnization with the rest of the TDAQ system is achieved using\nthe OnlineSW run control system. Each subfarm has a local run\ncontroller, which will interface to the Online Software run con-\ntrol via a farm controller. The controller collaborates with a sub-\nfarm supervisor, which provides process management and mon-\nitoring facilities within the subfarm. The controller and super-\nvisor cooperate to maintain the subfarm in the best achievable\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on May 1, 2009 at 10:28 from IEEE Xplore.  Restrictions apply.\nBAINES et al.: ATLAS HLT DATAFLOW AND SUPERVISION 365\nstate by keeping each other informed about changes in supervi-\nsion or run-control state and by taking appropriate actions, e.g.,\nrestarting crashed processes. Where possible, errors should be\nhandled internally within the HLT processes. Only when they\ncannot be handled internally should errors be sent to the super-\nvision and control system for further consideration.\nScalability tests carried out at CERN [9] using a prototype su-\npervision system implemented with the OnlineSW showed that\nexecution times taken for starting and stopping HLT trigger pro-\ncesses and for performing run control transitions to prepare for\ndatataking do not depend strongly on the number of controlled\nnodes. The times are shorter than 3 s for configurations of a size\nvarying between 50 and 230 nodes and running up to approxi-\nmately 1000 HLT software processes.\nThe Axis implementation of Web services [10] is currently\nbeing investigated for possible use in future implementations of\nthe supervision systems. Scalability tests (see Fig. 2) have been\nencouraging [9]. The HiWesD Histogram Web Service Demon-\nstrator (based on Axis) has already been used successfully in\nboth the LVL2 and EF context.\nIV. HLT INTEGRATED TESTBED\nAn integrated HLT prototype system has been built at CERN.\nThe system is a functional integration of previously existing\nLVL2 and EF prototypes. A schematic diagram of the testbed\nis shown in Fig. 3. The integrated prototype consists of the fol-\nlowing components.\n1) One ROS preloaded with event data from a file. The data\nin the file correspond to LVL1 trigger accepted\nevents containing electron and jet RoIs.\n2) One pROS to pass the LVL2 trigger result to the EF.\n3) A LVL2 system consisting of a LVL2 Event Supervisor\n(L2SV on the diagram) and two L2PUs. The L2PUs run\nthe TCALO algorithm for e\/gamma identification.\n4) An Event Builder consisting of a DFM and SFI.\n5) An EF system consisting of three EFDs and six PTs run-\nning the TCAL algorithm in the ATHENA environment.\n6) One SFO application to which the EFD sends the event\nonce the PTs have completed the selection.\n7) OnlineSW supervision infrastructure to provide synchro-\nnization and process control.\nThe computing infrastructure from previous LVL2 and EF\nstandalone tests has been reused. The ROS, LVL2, and Event\nBuilder (including the SFI and SFO applications) run on a\nsystem in one building at CERN, using PCs and a local switch.\nThe EF components, consisting of the EFD and the ATHENA\nPTs run on a system located in another building, using PCs and\na local switch.\nThe main emphasis of the prototype is on functionality. De-\ntailed performance measurements have been made in separate\nLVL2 and EF slice tests [11] and the execution time of algo-\nrithms can be measured in off-line mode. However, some of the\nmeasurements made for the standalone systems will be repeated\nin the integrated testbed to verify its correct functioning.\nThe LVL2 and EF control and supervision infrastructures\nhave been successfully integrated to create a common control\nFig. 3. Schematic diagram showing the configuration of the HLT integrated\ntestbed. The top box represents the previously existing LVL2\/Event Builder\ntestbed and the bottom box the previously existing Event Filter testbed. The\ntwo testbeds are located in different buildings at CERN. For the integrated HLT\nprototype they were linked via the CERN network. The small boxes represent\nprocessing nodes and the ellipses, the applications running on those processing\nnodes. For the sake of simplicity the supervision is represented on the diagram\nby a single \u201ccontrol\u201d application for LVL2 and the EF. In reality it is a number\nof interacting applications.\nstructure. This supervision system has been used to start all the\nHLT processes and bring them to the \u201crunning\u201d state. Events\nhave been observed to flow through the full system. Tests\nare currently underway to repeat key measurements made on\nthe standalone testbeds, in particular, the throughput in the\nEF will be measured as this depends critically on the correct\nfunctioning of the link between the LVL2 and EF parts of the\nsystem.\nV. CONCLUSION\nA final design now exists for the HLT system and is pre-\nsented in the ATLAS TDAQ Technical Design Report (TDR)\n[11]. LVL2 and EF prototypes based on the design have already\nbeen implemented and the required performance demonstrated\nin the critical area of data access and the principal aspects of\nsystem scalability. Integration of the LVL2 and EF prototypes\ninto a coherent HLT prototype is currently underway and tests\nwill be performed to test functionality and validate the design.\nResults are included in the ATLAS TDAQ TDR.\nACKNOWLEDGMENT\nThe authors would like to thank the many people in the\nATLAS collaboration who made this work possible, particu-\nlarly in the other parts of ATLAS Trigger\/DAQ.\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on May 1, 2009 at 10:28 from IEEE Xplore.  Restrictions apply.\n366 IEEE TRANSACTIONS ON NUCLEAR SCIENCE, VOL. 51, NO. 3, JUNE 2004\nREFERENCES\n[1] ATLAS high-level trigger group author list for real time 2003\n(2003, May). [Online]. Available: http:\/\/atlas.web.cern.ch\/Atlas\/\nGROUPS\/DAQTRIG\/HLT\/AUTHORLISTS\/rt2003.pdf\n[2] H. P. Beck et al., \u201cThe base-line dataflow system of the ATLAS trigger\nand DAQ\u201d (in Available Online: http:\/\/doc.cern.ch\/\/archive\/electronic\/\ncern\/others\/atlnot\/Communication\/daq\/com-daq-2003-037.pdf), IEEE\nTrans. Nucl. Sci., pp. 470\u2013475, June 2003.\n[3] ATLAS TDAQ\/DCS Event Filter Event Handler Design, C. Bee et\nal.. (2002, July). [Online]. Available: https:\/\/edms.cern.ch\/document\/\n367 089\/1\n[4] S. Armstrong, \u201cAn overview of algorithms for the ATLAS high-level\ntrigger\u201d (in Available Online: http:\/\/doc.cern.ch\/\/archive\/electronic\/\ncern\/others\/atlnot\/Note\/conf\/conf-2003-003.pdf), IEEE Trans. Nucl.\nSci., pp. 367\u2013374, June 2003.\n[5] G. Barrand et al.. GAUDI\u2014A software architecture and framework\nfor building HEP data processing applications. presented at Proc.\nCHEP 2000 Computing in High Energy and Nuclear Physics. [Online].\nAvailable: http:\/\/lhcb-comp.web.cern.ch\/lhcb-comp\/General\/Publica-\ntions\/longpap-a152.pdf\n[6] Athena Web Site (2001, Jan.). [Online]. Available: http:\/\/atlas.web.\ncern.ch\/Atlas\/GROUPS\/SOFTWARE\/OO\/architecture\/General\/index.\nhtml\n[7] ATLAS TDAQ HLT-Online Software Interface, D. Burckhart-Chromek\net al.. (2002, Nov.). [Online]. Available: https:\/\/edms.cern.ch\/docu-\nment\/363 702\/1\n[8] I. Alexandrov et al., \u201cOnline software for the ATLAS test beam data ac-\nquisition system\u201d (in Available Online: http:\/\/doc.cern.ch\/\/archive\/elec-\ntronic\/cern\/others\/atlnot\/Note\/daq\/daq-2003-044.pdf), IEEE Trans.\nNucl. Sci., pp. 578\u2013584, June 2003.\n[9] ATLAS TDAQ Event Filter Test Results for the EF Supervision, S.\nWheeler, F. Touchard, Z. Qian, C. Meessen, and A. Negri. (2003, Mar.).\n[Online]. Available: https:\/\/edms.cern.ch\/document\/374 118\/1\n[10] Web Services\u2014Axis (2000\u20132003). [Online]. Available: http:\/\/ws.\napache.org\/axis\/\n[11] \u201cThe ATLAS High-Level Trigger Data Acquisition and Controls Tech.\nDesign Rep.,\u201d ATLAS HLT\/DAQ\/DCS group, Geneva, Switzerland,\nATLAS TDR-016, June 2003. CERN.\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on May 1, 2009 at 10:28 from IEEE Xplore.  Restrictions apply.\n"}