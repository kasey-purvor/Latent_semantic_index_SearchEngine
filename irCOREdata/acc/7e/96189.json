{"doi":"10.1016\/j.jeconom.2009.10.029","coreId":"96189","oai":"oai:eprints.lse.ac.uk:28868","identifiers":["oai:eprints.lse.ac.uk:28868","10.1016\/j.jeconom.2009.10.029"],"title":"Nonparametric transfer function models","authors":["Liu, Jun M.","Chen, Rong","Yao, Qiwei"],"enrichments":{"references":[{"id":17248118,"title":"A method for the estimation and identi\ufb01cation of transfer function models.","authors":[],"date":"1989","doi":null,"raw":"Poskitt, D.S., 1989, A method for the estimation and identi\ufb01cation of transfer function models. Journal of the Royal Statistical Society B51, 29-46.","cites":null},{"id":17248104,"title":"A review of nonparametric time series analysis.","authors":[],"date":"1997","doi":null,"raw":"H\u00a8 adle, W., H. L\u00a8 utkepohl, and R. Chen, 1997, A review of nonparametric time series analysis. International Statistical Review 65, 49-72.","cites":null},{"id":17248103,"title":"Adaptive varying-coe\ufb03cient linear models.","authors":[],"date":"2003","doi":"10.1111\/1467-9868.00372","raw":"Fan, J., Q. Yao and Z. Cai, 2003, Adaptive varying-coe\ufb03cient linear models. Journal of the Royal Statistical Society, Series B 65, 57\u201380.","cites":null},{"id":17248127,"title":"Additive extensions to generalized estimation equation methods.","authors":[],"date":"1996","doi":null,"raw":"Wild, C.J. and T.W. Yee, 1996, Additive extensions to generalized estimation equation methods. Journal of the Royal Statistical Society: Series B 58, 711-725.","cites":null},{"id":17248122,"title":"Additive nonparametric regression with autocorrelated errors.","authors":[],"date":"1998","doi":"10.1111\/1467-9868.00127","raw":"Smith, M., C.M. Wong, and R. Kohn, 1998, Additive nonparametric regression with autocorrelated errors. Journal of the Royal Statistical Society 60, 311-331.","cites":null},{"id":17248130,"title":"An adaptive estimation of dimension reduction space (with discussion).","authors":[],"date":"2002","doi":"10.1111\/1467-9868.03411","raw":"Xia, Y., H. Tong, W.K. Li and L. Zhu, 2002, An adaptive estimation of dimension reduction space (with discussion). Journal of the Royal Statistical Society, Series B 64, 363-410.","cites":null},{"id":17248128,"title":"Asymptotic con\ufb01dence regions for kernel smoothing of a varying coe\ufb03cient model with longitudinal data.","authors":[],"date":"1998","doi":"10.2307\/2670054","raw":"Wu, C.O., C.T. Chiang, and D.R. Hoover, 1998, Asymptotic con\ufb01dence regions for kernel smoothing of a varying coe\ufb03cient model with longitudinal data. Journal of the American Statistical Association 93, 1388-1402.","cites":null},{"id":17248115,"title":"Bayesain estimation of Box-Jenkins transfer function-noise models.","authors":[],"date":"1973","doi":null,"raw":"Newbold, P., 1973, Bayesain estimation of Box-Jenkins transfer function-noise models. Journal of the Royal Statistical Society 35, 323-336.","cites":null},{"id":17248109,"title":"Characterizing selection bias using experimental data.","authors":[],"date":"1998","doi":"10.3386\/w6699","raw":"Heckman, J., H. Ichimura, J. Smith, and P. Todd, 1998, Characterizing selection bias using experimental data. Econometrica 66, 1017\u20131098.","cites":null},{"id":17248116,"title":"E\ufb03ciency of weighted average derivative estimators and index models.","authors":[],"date":"1993","doi":"10.2307\/2951498","raw":"Newey, W.K., and T.M. Stoker, 1993, E\ufb03ciency of weighted average derivative estimators and index models. Econometrica 61, 1199-1223.","cites":null},{"id":17248098,"title":"Functional-coe\ufb03cient autoregressive models.","authors":[],"date":"1993","doi":"10.2307\/2290725","raw":"Chen, R. and R.S. Tsay, 1993a, Functional-coe\ufb03cient autoregressive models. Journal of the American Statistical Association 88, 298-308.","cites":null},{"id":17248093,"title":"Functional-coe\ufb03cient regression models for nonlinear time series.","authors":[],"date":"2000","doi":"10.1080\/01621459.2000.10474284","raw":"Cai, Z., J. Fan and Q. Yao, 2000, Functional-coe\ufb03cient regression models for nonlinear time series. Journal of the American Statistical Association 95, 941\u2013956.","cites":null},{"id":17248094,"title":"Generalized partially linear single-index models.","authors":[],"date":"1997","doi":"10.1080\/01621459.1997.10474001","raw":"25Carroll, R.J., J. Fan, I. Gijbels and M.P. Wand, 1997, Generalized partially linear single-index models. Journal of the American Statistical Association 92, 477\u2013489.","cites":null},{"id":17248112,"title":"Identi\ufb01cation of multiple-input transfer function models.","authors":[],"date":"1982","doi":"10.1080\/03610928208828236","raw":"Liu, L.M. and D.M. Hanssens, 1982, Identi\ufb01cation of multiple-input transfer function models. Communications in Statistics A11, 297-314.","cites":null},{"id":17248090,"title":"Identi\ufb01cation of nonlinear time series: First order characterization and order estimation.","authors":[],"date":"1990","doi":"10.2307\/2337091","raw":"Auestad, B. and D. Tj\u00a8 ostheim, 1990, Identi\ufb01cation of nonlinear time series: First order characterization and order estimation. Biometrika 77, 669-687.","cites":null},{"id":17248132,"title":"Limiting behavior of U-statistics for a stationary absolutely regular process. Zeitschrift fur Wahrscheinlichkeitstheorie verw.","authors":[],"date":"1976","doi":"10.1007\/bf00532676","raw":"Yoshihara, K., 1976, Limiting behavior of U-statistics for a stationary absolutely regular process. Zeitschrift fur Wahrscheinlichkeitstheorie verw. Gebiete, 35, 237-252.","cites":null},{"id":17248101,"title":"Local Polynomial Modeling and Its Applications.","authors":[],"date":"1996","doi":null,"raw":"Fan, J. and I. Gilbels, 1996, Local Polynomial Modeling and Its Applications. Chapman and Hall, Su\ufb00olk.","cites":null},{"id":17248131,"title":"Model e\ufb03cient local polynomial estimation in nonparametric regression with autocorrelated errors.","authors":[],"date":"2003","doi":"10.1198\/016214503000000936","raw":"Xiao, Z., O.B. Linton, R.J. Carroll, and E. Mammen, 2003, Model e\ufb03cient local polynomial estimation in nonparametric regression with autocorrelated errors. Journal of the American Statistical Association 98, 980-992.","cites":null},{"id":17248126,"title":"Model identi\ufb01cation in dynamic regression (distributed lag) models.","authors":[],"date":"1985","doi":"10.1080\/07350015.1985.10509454","raw":"27Tsay, R.S., 1985, Model identi\ufb01cation in dynamic regression (distributed lag) models. Journal of Business Economic Statistics 3, 228-237.","cites":null},{"id":17248124,"title":"Modeling multiple time series with applications.","authors":[],"date":"1981","doi":"10.1080\/01621459.1981.10477728","raw":"Tiao, G.C. and G.E.P. Box, 1981, Modeling multiple time series with applications. Journal of the American Statistical Association 76, 802-816.","cites":null},{"id":17248107,"title":"Modeling nonlinear vibrations using an amplitude-dependent autoregressive time series model.","authors":[],"date":"1981","doi":"10.1093\/biomet\/68.1.189","raw":"Haggan V. and T. Ozaki, 1981, Modeling nonlinear vibrations using an amplitude-dependent autoregressive time series model. Biometrika 68, 189196.","cites":null},{"id":17248123,"title":"More e\ufb03cient estimation in nonparametric regression with nonparametric autocorrelated errors.","authors":[],"date":"2006","doi":"10.1017\/s026646660606004x","raw":"Su, L. and A. Ullah, 2006, More e\ufb03cient estimation in nonparametric regression with nonparametric autocorrelated errors. Econometric Theory 22, 98-126.","cites":null},{"id":17248113,"title":"Multivariate local polynomial regression for time series: Uniform consistency and rates.","authors":[],"date":"1996","doi":"10.1111\/j.1467-9892.1996.tb00294.x","raw":"Masry, E., 1996a, Multivariate local polynomial regression for time series: Uniform consistency and rates. Journal of Time Series Analysis 17, 571-599.","cites":null},{"id":17248114,"title":"Multivariate regression estimation: Local polynomial \ufb01tting for time series.","authors":[],"date":"1996","doi":"10.1016\/s0304-4149(96)00095-6","raw":"Masry, E., 1996b, Multivariate regression estimation: Local polynomial \ufb01tting for time series. Stochastic Processes and Their Applications 65, 81-101.","cites":null},{"id":17248099,"title":"Nonlinear additive ARX models,","authors":[],"date":"1993","doi":"10.1080\/01621459.1993.10476363","raw":"Chen, R. and R.S. Tsay, 1993b, Nonlinear additive ARX models, Journal of the American Statistical Association 88, 955-967.","cites":null},{"id":17248125,"title":"Nonlinear time series: a selective review.","authors":[],"date":"1994","doi":"10.2307\/2291002","raw":"Tj\u00f8stheim, D., 1994, Nonlinear time series: a selective review. Scandinavian Journal of Statistics 21, 97-130.","cites":null},{"id":17248102,"title":"Nonlinear Time Series: Nonparametric and Parametric Methods.","authors":[],"date":"2003","doi":"10.1007\/b97702","raw":"Fan, J. and Q. Yao, 2003, Nonlinear Time Series: Nonparametric and Parametric Methods. Springer, New York.","cites":null},{"id":17248097,"title":"Nonlinear transfer functions.","authors":[],"date":"1996","doi":"10.1080\/10485259608832671","raw":"Chen, R. and R.S. Tsay, 1996, Nonlinear transfer functions. Journal of Nonparametric Statistics 66, 193-204.","cites":null},{"id":17248119,"title":"Nonparametric estimators for time series.","authors":[],"date":"1983","doi":"10.1111\/j.1467-9892.1983.tb00368.x","raw":"Robinson, P.M., 1983, Nonparametric estimators for time series. Journal of Time Series Analysis 4, 185-207.","cites":null},{"id":17248120,"title":"Nonparametric function estimation of the relationship between two repeatedly measured variables.","authors":[],"date":"2000","doi":null,"raw":"Ruckstuhl, A., A.H. Welsh, and R.J. Carroll, 2000, Nonparametric function estimation of the relationship between two repeatedly measured variables. Statistica Sinica 10, 51-71.","cites":null},{"id":17248111,"title":"Nonparametric Transfer Function Models.","authors":[],"date":"2005","doi":null,"raw":"26Liu, J.M., R. Chen and Q. Yao, 2005, Nonparametric Transfer Function Models. Technical report, Georgia Southern University.","cites":null},{"id":17248129,"title":"On single-index coe\ufb03cient regression models.","authors":[],"date":"1999","doi":"10.2307\/2669941","raw":"Xia, Y. and W.K. Li, 1999, On single-index coe\ufb03cient regression models. Journal of the American Statistical Association 94, 1275-1285.","cites":null},{"id":17248105,"title":"Optimal smoothing in single-index models.","authors":[],"date":"1993","doi":"10.1214\/aos\/1176349020","raw":"H\u00a8 ardle, W., P. Hall, and H. Ichimura, 1993, Optimal smoothing in single-index models. The Annals of Statistics 21, 157\u2013178.","cites":null},{"id":17248106,"title":"Partially Linear Models.","authors":[],"date":"2000","doi":"10.1007\/978-3-642-57700-0_6","raw":"H\u00a8 ardle, W., H. Liang, and J. Gao, 2000, Partially Linear Models. Physica-Verlag, Heidelberg.","cites":null},{"id":17248121,"title":"Quasi-likelihood estimation in semiparametric models.","authors":[],"date":"1994","doi":"10.2307\/2290852","raw":"Severini, T.A. and J.G. Staniswalis, 1994, Quasi-likelihood estimation in semiparametric models. Journal of the American Statistical Association 89, 501-511.","cites":null},{"id":17248110,"title":"Semiparametric least-squares (SLS) and weighted SLS estimation of singleindex models.","authors":[],"date":"1993","doi":"10.1016\/0304-4076(93)90114-k","raw":"Ichiruma, H., 1993, Semiparametric least-squares (SLS) and weighted SLS estimation of singleindex models. Journal of Econometrics 58, 71\u2013120.","cites":null},{"id":17248133,"title":"Semiparametric models for longitudinal data with application to CD4 cell number in HIV seroconverters.","authors":[],"date":"1994","doi":"10.2307\/2532783","raw":"Zeger, S.L. and P.J. Diggle, 1994, Semiparametric models for longitudinal data with application to CD4 cell number in HIV seroconverters. Biometrics 50, 789-699.","cites":null},{"id":17248100,"title":"Smoothing noisy data with spline functions.","authors":[],"date":"1979","doi":"10.1007\/bf01404567","raw":"Craven, P. and G.Wahba, 1979, Smoothing noisy data with spline functions. Numerical Mathematics 31, 377-403.","cites":null},{"id":17248108,"title":"Some automated methods of smoothing time-dependent data.","authors":[],"date":"1996","doi":"10.1080\/10485259608832667","raw":"Hart, J.D., 1996, Some automated methods of smoothing time-dependent data. Journal of Nonparametric Statistics 6 115-142, 1996.","cites":null},{"id":17248117,"title":"Statistical identi\ufb01cation of storage models with application to stochastic hydrology.","authors":[],"date":"1985","doi":"10.1111\/j.1752-1688.1985.tb05381.x","raw":"Ozaki, T., 1985, Statistical identi\ufb01cation of storage models with application to stochastic hydrology. Water Resources Bulletin 21, 663-675.","cites":null},{"id":17248092,"title":"Time Series: Theory and Methods.","authors":[],"date":"1987","doi":"10.1007\/978-1-4899-0004-3","raw":"Brockwell, P.J. and R.A. Davis, 1987, Time Series: Theory and Methods. Springer-Verlag, New York.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-07","abstract":"In this paper a class of nonparametric transfer function models is proposed to model nonlinear relationships between 'input' and 'output' time series. The transfer function is smooth with unknown functional forms, and the noise is assumed to be a stationary autoregressive-moving average (ARMA) process. The nonparametric transfer function is estimated jointly with the ARMA parameters. By modelling the correlation in the noise, the transfer function can be estimated more efficiently. The parsimonious ARMA structure improves the estimation efficiency in finite samples. The asymptotic properties of the estimators are investigated. The finite-sample properties are illustrated through simulations and one empirical example","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/96189.pdf","fullTextIdentifier":"http:\/\/eprints.lse.ac.uk\/28868\/1\/Nonparametric_transfer_function_models_%28LSERO_version%29.pdf","pdfHashValue":"d09244fcfe7f657121c425c1580e016ad1615119","publisher":"Elsevier","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lse.ac.uk:28868<\/identifier><datestamp>\n      2017-05-04T09:35:28Z<\/datestamp><setSpec>\n      74797065733D434F4C4C53:4C53455F435F454F<\/setSpec><setSpec>\n      74797065733D4445505453:4C53452D5354<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lse.ac.uk\/28868\/<\/dc:relation><dc:title>\n        Nonparametric transfer function models<\/dc:title><dc:creator>\n        Liu, Jun M.<\/dc:creator><dc:creator>\n        Chen, Rong<\/dc:creator><dc:creator>\n        Yao, Qiwei<\/dc:creator><dc:subject>\n        HB Economic Theory<\/dc:subject><dc:subject>\n        QA Mathematics<\/dc:subject><dc:description>\n        In this paper a class of nonparametric transfer function models is proposed to model nonlinear relationships between 'input' and 'output' time series. The transfer function is smooth with unknown functional forms, and the noise is assumed to be a stationary autoregressive-moving average (ARMA) process. The nonparametric transfer function is estimated jointly with the ARMA parameters. By modelling the correlation in the noise, the transfer function can be estimated more efficiently. The parsimonious ARMA structure improves the estimation efficiency in finite samples. The asymptotic properties of the estimators are investigated. The finite-sample properties are illustrated through simulations and one empirical example.<\/dc:description><dc:publisher>\n        Elsevier<\/dc:publisher><dc:date>\n        2010-07<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lse.ac.uk\/28868\/1\/Nonparametric_transfer_function_models_%28LSERO_version%29.pdf<\/dc:identifier><dc:identifier>\n          Liu, Jun M. and Chen, Rong and Yao, Qiwei  (2010) Nonparametric transfer function models.  Journal of Econometrics, 157 (1).  pp. 151-164.  ISSN 0304-4076     <\/dc:identifier><dc:relation>\n        http:\/\/www.elsevier.com\/locate\/jeconom<\/dc:relation><dc:relation>\n        10.1016\/j.jeconom.2009.10.029<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lse.ac.uk\/28868\/","http:\/\/www.elsevier.com\/locate\/jeconom","10.1016\/j.jeconom.2009.10.029"],"year":2010,"topics":["HB Economic Theory","QA Mathematics"],"subject":["Article","PeerReviewed"],"fullText":"  \nJun M. Liu, Rong Chen and Qiwei Yao\nNonparametric transfer function models \n \nArticle (Accepted version) \n(Refereed) \n \nOriginal citation: \nLiu, Jun M. and Chen, Rong and Yao, Qiwei (2010) Nonparametric transfer function models. \nJournal of econometrics, 157 (1). pp. 151-164. ISSN 0304-4076  \n \nDOI: 10.1016\/j.jeconom.2009.10.029  \n \n\u00a9 2010 Elsevier B.V. \n \nThis version available at: http:\/\/eprints.lse.ac.uk\/28868\/ \nAvailable in LSE Research Online: August 2010 \n \nLSE has developed LSE Research Online so that users may access research output of the \nSchool. Copyright \u00a9 and Moral Rights for the papers on this site are retained by the individual \nauthors and\/or other copyright owners. Users may download and\/or print one copy of any \narticle(s) in LSE Research Online to facilitate their private study or for non-commercial research. \nYou may not engage in further distribution of the material or use it for any profit-making activities \nor any commercial gain. You may freely distribute the URL (http:\/\/eprints.lse.ac.uk) of the LSE \nResearch Online website.  \n \nThis document is the author\u2019s final manuscript accepted version of the journal article, \nincorporating any revisions agreed during the peer review process.  Some differences between \nthis version and the published version may remain.  You are advised to consult the publisher\u2019s \nversion if you wish to cite from it. \nNonparametric Transfer Function Models\nJun M. Liu1, Rong Chen2 and Qiwei Yao3\n1Georgia Southern University,\n2,3Peking University\n2University of Illinois at Chicago,\n3London School of Economics 1\nAbstract\nIn this paper a class of nonparametric transfer function models is proposed to model nonlinear\nrelationships between \u2018input\u2019 and \u2018output\u2019 time series. In this approach, the functional form of the\ntransfer function is assumed to be unknown but smooth, and the noise is assumed to be stationary\nwith a parametric autoregressive-moving average (ARMA) form. A new method is developed to\njointly estimate the transfer function nonparametrically and the ARMA parameters parametri-\ncally. By modeling the transfer function nonparametrically, the model is flexible and can be used\nto model nonlinear relationship of unknown functional forms; by modeling the noise explicitly as a\nparsimonious ARMA model, the correlation in the data is removed so the transfer function can be\nestimated more efficiently. Additionally, the estimated ARMA parameters can be used to improve\nthe forecasting performance. Estimation procedures are introduced and the asymptotic proper-\nties of the estimators are investigated. The finite-sample properties of the estimators are studied\nthrough simulations and one real example.\nJEL Classification: C14, C22\nKeywords: Nonparametric smoothing, Time series, Transfer Function\n1Jun M. Liu is Assistant Professor of Quantitative Methods, Department of Finance & Quantitative Analysis,\nGeorgia Southern University. Rong Chen is Professor of Statistics, Department of Business Statistics and Economet-\nrics, Peking University and Department of Information & Decision Sciences, University of Illinois at Chicago. Qiwei\nYao is Professor of Statistics, London School of Economics and Department of Business Statistics and Econometrics,\nPeking University. Corresponding author: Rong Chen, 601 South Morgan Street (M\/C 294), Chicago, IL 60607,\nUSA. Tel: (312)996-2323, Fax: (312)413-0385, Email: rongchen@uic.edu.\n1\n1 Introduction\nLinear transfer function models (Box and Jenkins, 1976) have been extensively used to model the\nrelationship between one \u2018output\u2019 time series and several \u2018input\u2019 time series. With one input series,\nit assumes the form Yt = \u03b1(B)\u03b2(B)\u22121Xt+ et, where Yt is the observed output series of interest, Xt\nis an observed input time series, et follows an ARMA process, and \u03b1(B) and \u03b2(B) are polynomials\nof the backshift operator B defined as BiXt \u2261 Xt\u2212i. Linear transfer function models have been well\nstudied and proven successful in many fields (e.g., Newbold, 1973; Tiao and Box, 1981; Tsay, 1985;\nPoskitt, 1989; Liu and Hanssens, 1982). However, its linear nature limits its applicability because\nmany nonlinear features encountered in practice cannot be well approximated by linear models. To\nmodel nonlinear relationships between time series, Chen and Tsay (1996) proposed the nonlinear\ntransfer function model of the form Yt = f(Xt\u2212d, \u00b7 \u00b7 \u00b7 , Xt\u2212d\u2212p;\u03b8) + \u03b5t, where f(\u00b7) is a parametric\nfunction assuming the Volterra series representation, \u03b5t is stationary and modeled by an ARMA\nmodel.\nThere are infinitely many candidate nonlinear functions beyond the linear domain. Therefore,\nit is usually difficult to justify the explicit parametric functional forms a priori for nonlinear mod-\nels. Following the \u201cletting the data speak for themselves\u201d principle, nonparametric smoothing\nmethods provide a more flexible alternative to model nonlinear time series (e.g., Robinson, 1983;\nAuestad and Tj\u00f8stheim, 1990; Lewis and Stevens, 1991; Masry, 1996a,b; Fan and Gilbels, 1996;\nSmith, Wong, and Kohn, 1998). To overcome the \u2018curse of dimensionality\u2019, various specially struc-\ntured nonparametric models have been proposed, including the functional-coefficient autoregressive\n(FAR) model (Chen and Tsay, 1993a; Cai, Fan and Yao, 2000), the nonlinear additive autoregres-\nsive model (Chen and Tsay, 1993b), the adaptive functional-coefficient model (Ichimura, 1993; Xia\nand Li, 1999; Fan, Yao and Cai, 2003), the single index model (e.g., Ha\u00a8rdle, Hall, and Ichimura,\n1993; Carroll, Fan, Gijbels, and Wand, 1997; Newey and Stoker, 1993; Heckman, Ichimura, Smith,\nand Todd, 1998; Xia, Tong, Li, and Zhu, 2002) and the partially linear models (Ha\u00a8rdle, Liang and\nGao, 2000). There is vast literature about nonlinear and nonparametric time series analysis. Some\nreviews can be found in Tj\u00f8stheim (1994), Ha\u00a8rdle, Lu\u00a8tkepohl and Chen (1997) and Fan and Yao\n(2003).\nIn this paper a class of nonparametric transfer function models is proposed. Consider the model\nYt = f(Xt) + et, (1)\nwhere f(\u00b7) is an unknown and smooth function, {Xt} and {et} are strictly stationary processes.\nThe transfer function f(\u00b7) is modeled via nonparametric smoothing and the innovation process {et}\nis assumed to follow a stationary and invertible ARMA(p, q) process, i.e., \u03c6(B)et = \u03b8(B)\u03b5t, where\n\u03c6(B) = 1\u2212\u2211pi=1 \u03c6iBi, \u03b8(B) = 1\u2212\u2211qj=1 \u03b8jBj , \u03c6 = (\u03c61, \u03c62, \u00b7 \u00b7 \u00b7 , \u03c6p)\u03c4 and \u03b8 = (\u03b81, \u03b82, \u00b7 \u00b7 \u00b7 , \u03b8q)\u03c4 are\nunknown parameters and {\u03b5t} is a sequence of independent (0, \u03c32) random variables. An iterative\n2\nprocedure is used to estimate both the transfer function and the ARMA parameters. Because of\nits close connections to the Box-Jenkins transfer function model and nonparametric smoothing, the\nproposed method is named nonparametric transfer function model. {Xt} and {\u03b5t} are assumed to\nbe independent, which implies the independence between {Yt} and {et}.\nBy modeling the transfer function f(\u00b7) nonparametrically, the model is flexible therefore can\nbe used to model nonlinear relationship of unknown functional forms. By modeling {et} as an\nARMA(p, q) process, the autocorrelation in the data is removed so f(\u00b7) can be estimated more\nefficiently. Additionally, the explicit correlation structure can be used to improve the forecasting\nperformance.\nThe problem of estimating f(\u00b7) in (1) can be viewed as a regression with correlated noise\nproblem. Under certain mixing conditions, the windowing-and-whitening effect (Hart, 1996) makes\nthe local smoothing method valid even when the correlation is ignored (Zeger and Diggle, 1994;\nWild and Yee, 1996; Wu, Chiang and Hoover, 1998; Ruchstuhl, Welsh and Caroll, 2000). To take\nadvantage of the correlation in the data, Severini and Staniswalis (1994) proposed to estimate the\ncovariance matrix and incorporate the estimated covariance structure in the kernel weights.\nRecently Xiao, Linton, Carroll and Mammen (2003) and Su and Ullah (2006) considered a\nproblem similar to the one considered in this paper. These studies are closely related, but major\ndifference exists, especially in the handling of the noise {et}. In Xiao et al. (2003) the noise series\n{et} is assumed to be a general linear process and is approximated by a truncated AR process; in\nSu and Ullah (2006) {et} is modeled as a finite-order nonparametric AR process. In this paper\n{et} is modeled explicitly as an ARMA(p, q) process. This parsimonious representation allows us\nto improve the efficiency of estimation in finite samples. It has special advantages over Xiao et\nal. (2003) when the innovation process cannot be approximated with small-order AR models (e.g.,\nseasonal ARMA models or ARMA models with roots close to one in the MA part). Comparing\nto the approach of Su and Ullah (2006), an explicit parametric form of the noise process allows\nfaster convergence in the estimation of the innovation structure, hence the ability of generating\nmore accurate predictions using the model.\nThis paper is organized as follows. In section 2, the estimation procedure and the asymptotic\nproperties of the proposed estimator when et follows an AR(p) process are presented. In section\n3 the results for the AR(p) case are extended to the general case when et follows an ARMA(p, q)\nprocess. Although AR(p) case is a special case of ARMA(p, q), different algorithms are used and\ndifferent approaches are needed to prove the theorems. The pure AR structure provides a better\nalgorithm and simpler proof of the asymptotic results. The performance of the proposed estimators\nare studied through simulation and compared with those of Xiao et al. (2003) and Su and Ullah\n(2006), the results are presented in section 4. The proposed procedures are applied on one real-life\napplication and the results are presented in section 5. Section 6 contains summary and discussion.\n3\nThe technical proofs are given in Appendix A. In the proof one important result of Yoshihara (1976)\nis used and an account of this result is given in Appendix B.\n2 Estimation procedure in the pure AR case\n2.1 The algorithm\nWhen {et} is a stationary AR(p) process, model (1) can be written as\nYt = f(Xt) + et, \u03c6(B)et = \u03b5t.\nWith observations {(Xt, Yt)}nt=1, first a preliminary estimator for f(\u00b7) is obtained by local linear\nregression, ignoring the correlation in {et}. Namely, f\u02dc(x) = a\u02dc0, where (a\u02dc0, a\u02dc1) minimizes\nn\u2211\nt=1\n{Yt \u2212 a0 \u2212 a1(Xt \u2212 x)}2Kb(Xt \u2212 x), (2)\nwhere Kb(\u00b7) = b\u22121K(\u00b7\/b), K(\u00b7) is a kernel function in R, and b > 0 is a bandwidth. By simple\nalgebra,\nf\u02dc(x)\u2212 f(x) = 1\nnb\nn\u2211\nt=1\nWn\n(Xt \u2212 x\nb\n, x\n)\n{Yt \u2212 f(x)\u2212 f\u02d9(x)(Xt \u2212 x)}, (3)\nwhere\nWn(t, x) = (1, 0)Sn(x)\u22121\n\uf8eb\uf8ed 1\nt\n\uf8f6\uf8f8K(t). (4)\nIn the above expression, Sn(x) is a 2\u00d7 2 matrix with si+j\u22122(x) as its (i, j)-th element, and\nsk(x) =\n1\nn\nn\u2211\nt=1\n(Xt \u2212 x\nb\n)k\nKb(Xt \u2212 x). (5)\nUnder normal assumption, the maximum likelihood estimation for f(\u00b7) and \u03c6 boils down to the\nfollowing optimization problem:\ninf\nf,\u03c6\nn\u2211\nt=1\n{Yt \u2212 f(Xt)\u2212\np\u2211\ni=1\n\u03c6i(Yt\u2212i \u2212 f(Xt\u2212i))}2, (6)\nwhere the infimum is taken over all smooth function f and \u03c6 \u2208 Rp satisfies the stationary condition.\nLet e\u02dct = Yt \u2212 f\u02dc(Xt) be the initial estimate of the innovation series et. Define\nX1 =\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\ne\u02dcp e\u02dcp\u22121 \u00b7 \u00b7 \u00b7 e\u02dc1\ne\u02dcp+1 e\u02dcp \u00b7 \u00b7 \u00b7 e\u02dc2\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\ne\u02dcn\u22121 e\u02dcn\u22122 \u00b7 \u00b7 \u00b7 e\u02dcn\u2212p\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 , Y1 =\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\ne\u02dcp+1\ne\u02dcp+2\n\u00b7 \u00b7 \u00b7\ne\u02dcn\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 ,\nand W=diag\n{\u220fp\ni=0w(Xt\u2212i)\n}\n, where w(\u00b7) is a weight function controlling the boundary effect in\nnonparametric estimation. An iterative estimation procedure is defined as follows:\n4\n1. Specify an initial value \u03c6 = \u03c6\u02dc defined as\n\u03c6\u02dc = (X\u03c41WX1)\n\u22121X\u03c41WY1. (7)\n2. For given \u03c6, let f\u02c7j \u2261 f\u02c7(Xj) = a\u03020, where (a\u03020, a\u03021) minimizes\nn\u2211\nt=1\n{\nYt \u2212 a0 \u2212 a1(Xt \u2212Xj)\u2212\np\u2211\ni=1\n\u03c6i\n[\nYt\u2212i \u2212 f\u02dc(Xt\u2212i)\n]}2\nKh(Xt \u2212Xj)\np\u220f\ni=1\nw(Xt\u2212i), (8)\nwhere Kh(\u00b7) = h\u22121K(\u00b7\/h), and h > 0 is a bandwidth. Obviously a\u03021 is an estimator for\nf\u02d9j \u2261 f\u02c7(Xj).\n3. Obtain \u03c6\u02c7 by minimizing\nn\u2211\nj=1\nn\u2211\nt=1\n{\nYt\u2212 f\u02c7j \u2212 \u02c7\u02d9f j(Xt\u2212Xj)\u2212\np\u2211\ni=1\n\u03c6i\n[\nYt\u2212i\u2212 f\u02dc(Xt\u2212i)\n]}2\nKh(Xt\u2212Xj)w(Xj)\np\u220f\ni=1\nw(Xt\u2212i). (9)\n4. Repeat Steps 2 and 3 above until convergence. The terminal values are defined as estimators\nf\u0302(Xj) = f\u02c7j and \u03c6\u0302 = \u03c6\u02c7.\nRemark 1: Note that in (8) and (9), the values of f\u02dc(Xt\u2212i) are fixed at the initial estimate\nthroughout the iterations. This setting guarantees that the sum of squares is non-increasing in\nevery iteration, hence guarantees the convergence. In practice, replacing f\u02dc with the newly esti-\nmated function values may improve the results, though convergence is no longer guaranteed, and\nasymptotically it is not necessary.\nRemark 2: In practice, only those f\u0302(Xj) with w(Xj) > 0 will be calculated in order to eliminate\nthe boundary bias in nonparametric estimation. One may let w(\u00b7) be an indicator function on, for\nexample, the 80% inner sample range of Xt.\nRemark 3: There are two bandwidths b and h in the estimation procedure. The asymptotic\nresults below show that the bandwidth h in the iteration step should be of the standard order of\nn\u22121\/5. However, the bandwidth at the preliminary step (2) should be of smaller order b = o(h) but\nnb4 \u2192\u221e (Condition A4 in Appendix A). This requirement controls the bias in the preliminary step\nof the estimation. In practice, standard bandwidth selection in the iteration steps can be utilized.\nExperiments show that the final results are usually not very sensitive to the choice of bandwidth\nb. A fraction of the usual optimal bandwidth often works well.\nRemark 4: In this paper {et} and {Xt} are assumed to be independent. For otherwise, the least\nsquares-based estimators, such as local polynomial estimators, may not be consistent. Unfortu-\nnately this assumption essentially forbids the use of lagged Y s as explanatory variables. When\nlagged Y s are needed on the right-hand side of the model, alternative approaches are needed. For\nexample, one may consider including enough lags of Y on the RHS of the model so that the inno-\nvation process becomes nearly uncorrelated and standard smoothing methods can be applied. Xiao\net al. (2003) made a similar observation, here we share their view.\n5\n2.2 Asymptotic results\nLet\nX2 =\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nep ep\u22121 \u00b7 \u00b7 \u00b7 e1\nep+1 ep \u00b7 \u00b7 \u00b7 e2\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\nen\u22121 en\u22122 \u00b7 \u00b7 \u00b7 en\u2212p\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 , Y2 =\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nep+1\nep+2\n\u00b7 \u00b7 \u00b7\nen\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 .\nDefine the \u201cidealized\u201d estimator\n\u03c6\u0302Ideal = (X\n\u03c4\n2WX2)\n\u22121X\u03c42WY2,\nwhere W is the boundary weight matrix defined in section 2.1. This would be the \u2018idealized\u2019 least\nsquare estimator of the AR coefficients if {et} is actually observable. It has been shown (e.g.,\nBrockwell and Davis, 1987) that\n\u221a\nn(\u03c6\u0302Ideal \u2212 \u03c6) D\u2212\u2192 N\n(\n0,\nE(\u03a0pi=0w(Xt\u2212i))\n2\n[E(\u03a0pi=0w(Xt\u2212i))]2\n\u03c32V(\u03c6)\u22121\n)\n,\nwhere V(\u03c6) is a p \u00d7 p matrix and its (i, j)-th element is Cov(ei, ej). The following theorem links\nour estimator to \u03c6\u0302Ideal.\nTheorem 1 Under the conditions (A1)-(A6) in Appendix A, and that \u03c6 satisfies the stationarity\ncondition, then as n\u2192\u221e, \u221a\nn(\u03c6\u02dc\u2212 \u03c6\u0302Ideal) = op(1),\nwhere \u03c6\u02dc is the preliminary estimator defined in (7).\nAs a result of Theorem 1, \u03c6\u02dc shares the same asymptotic distribution of \u03c6\u0302Ideal, i.e.,\n\u221a\nn\n(\n\u03c6\u02dc\u2212 \u03c6\n)\nD\u2212\u2192 N\n(\n0,\nE\n(\n\u03a0pi=0w(Xt\u2212i)\n)2\n[\nE\n(\n\u03a0pi=0w(Xt\u2212i)\n)]2\u03c32V(\u03c6)\u22121). (10)\nAs for the nonparametric function f , note that the local linear estimator defined by (8) may be\nexpressed, for a generic x, as follows:\nf\u0302(x)\u2212 f(x) = 1\nnh\nn\u2211\nt=1\nW \u2217n\n(Xt \u2212 x\nh\n, x,Xt\u22121, \u00b7 \u00b7 \u00b7 , Xt\u2212p\n){\nY\u02dct \u2212 f(x)\u2212 f\u02d9(x)(Xt \u2212 x)\n}\n, (11)\nwhere Y\u02dct = Yt \u2212\u2211pi=1 \u03c6\u02dci{Yt\u2212i \u2212 f\u02dc(Xt\u2212i)}, and\nW \u2217n(t, x, y1, y2, \u00b7 \u00b7 \u00b7 , yp) = (1, 0)S\u2217n(x)\u22121(1, t)\u03c4K(t)\u03a0pi=1w(yi),\nand S\u2217n(x) is defined in the same manner as Sn(x) in (5) with Kb(Xt \u2212 x) replaced by Kh(Xt \u2212\nx)\n\u220fp\ni=1w(Xt\u2212i) (See also (3)). Theorem 2 below indicates that the above estimator is asymptot-\nically efficient in the sense that the estimator admits the same (the first order) asymptotic distri-\nbution as if {Yt} would be defined by a simpler model with i.i.d. noise, namely Yt = f(Xt) + \u03b5t.\n6\nTheorem 2 Under the conditions (A1) to (A6) in Appendix A, for any point x in the support of\nXt, as n\u2192\u221e, \u221a\nnh\n{\nf\u0302(x)\u2212 f(x)\u2212 h\n2\u00b52\n2\nf\u00a8(x)\n}\nD\u2212\u2192 N\n(\n0, \u03c3(x)2\n)\n,\nwhere\n\u03c3(x)2 =\n\u03c32\n\u222b\nK(u)2du\ng1(x)\nE\n{[\nW (Xt\u22121)W (Xt\u22122) \u00b7 \u00b7 \u00b7W (Xt\u2212p)\n]2|Xt = x}{\nE\n[\nW (Xt\u22121)W (Xt\u22122) \u00b7 \u00b7 \u00b7W (Xt\u2212p)|Xt = x\n]}2 , (12)\nand g1(x) is the marginal density of Xt.\nThis theorem shows that the nonparametric transfer function estimator f\u0302(\u00b7) is indeed more efficient\nthan the conventional local polynomial estimator f\u02dc(\u00b7). If f\u02dc(\u00b7) is used, the resulting asymptotic\nvariance would have the same form as (12), but the white noise variance \u03c32 in (12) would be\nreplaced by the variance of et, which is strictly greater than \u03c32 for a nontrivial AR(p) model. On\nthe other hand, the asymptotic bias is not affected by the correlation structure. As a result, f\u0302 is\nmore efficient than the conventional estimator f\u02dc in the sense of mean square error. It can also be\nseen that the gain in efficiency of f\u0302(\u00b7) over f\u02dc(\u00b7) will be greater if the correlation is stronger.\n3 Estimation procedure in the ARMA(p, q) case\nHere we consider the general case when {et} follows an ARMA(p, q) process. The estimation shares\nthe similar \u201cpre-whitening\u201d idea with the AR(p) case and the asymptotic results are also similar.\nHowever the estimation procedures are more complicated in details and different techniques are\nrequired to establish the asymptotic results.\n3.1 The algorithm\nModeling {et} as a stationary, invertible ARMA(p, q) process, model (1) becomes\nYt = f(Xt) + et, et = \u03c6\u22121(B)\u03b8(B)\u03b5t.\n{et} is assumed to be stationary and invertible, so {et} admits the linear process representations\net = \u2212\u2211\u221ei=1 piiet\u2212i+\u03b5t and et =\u2211\u221ei=0 \u03c8i\u03b5t\u2212i, pii and \u03c8i are absolutely summable, i.e.,\u2211\u221ei=0 |pii| <\u221e\nand\n\u2211\u221e\ni=0 |\u03c8i| < \u221e (Box and Jenkins, 1976). Denote \u03b2 = (\u03c61, \u03c62, \u00b7 \u00b7 \u00b7 , \u03c6p, \u03b81, \u03b82, \u00b7 \u00b7 \u00b7 , \u03b8q)\u03c4 . f(\u00b7) and\n\u03b2 are estimated by solving the following nonlinear optimization problem\ninf\nf,\u03b2\nn\u2211\nt=1\n{\nYt \u2212 f(Xt) +\n[\u03c6(B)\n\u03b8(B)\n\u2212 1\n][\nYt \u2212 f(Xt)\n]}2\n, (13)\nwhere the infimum is taken over all smooth function f and all \u03b2 \u2208 Rp+q satisfying the stationary\nand invertible conditions. To initiate the iteration, an initial estimate f\u02dc(\u00b7) is obtained by local\n7\nlinear regression, ignoring the serial correlation in {et} (see also (2)). The iterative procedure is\ndescribed as follows:\n1. Obtain an initial estimate \u03b2\u02dc = (\u03c6\u02dc, \u03b8\u02dc) by minimizing\nn\u2211\nt=1\n{\u03c6(B)\n\u03b8(B)\n[\nYt \u2212 f\u02dc(Xt)\n]}2\n(14)\nwith respect to \u03c6 and \u03b8.\n2. Given \u03b2, let f\u02c7j \u2261 f\u02c7(Xj) = a\u02c60, where (a\u02c60, a\u02c61) minimizes\nn\u2211\nt=1\n{\nYt \u2212 a0 \u2212 a1(Xt \u2212Xj) +\n[\u03c6(B)\n\u03b8(B)\n\u2212 1\n][\nYt \u2212 f\u02dc(Xt)\n]}2\nKh(Xt \u2212Xj),\nwhere Kh(\u00b7) = 1\/hK(\u00b7\/h), h is a bandwidth and h is of larger order than b.\n3. Define \u03b2\u02c7 to minimize\nn\u2211\nj=1\nn\u2211\nt=1\n{\nYt \u2212 f\u02c7j \u2212 \u02c7\u02d9f j(Xt \u2212Xj) +\n[\u03c6(B)\n\u03b8(B)\n\u2212 1\n][\nYt \u2212 f\u02dc(Xt)\n]}2\nKh(Xt \u2212Xj). (15)\n4. Repeat steps 2 and 3 until {f\u02c7j} and \u03b2\u02c7 change only by a small amount in two successive\niterations. The terminal values of f\u0302(Xj) = f\u02c7j and \u03b2\u0302 = \u03b2\u02c7 are the estimators of f(\u00b7) and \u03b2,\nrespectively.\nSeveral algorithms can be used to solve the nonlinear optimization problems presented in equations\n(13) to (15). In this study, a nonlinear estimation method based on the Gauss-Newton algorithm\nis used. In this method, steps 1 and 3 can be iterated to improve the finite sample performance.\nThe details of this method can be found in Appendix A.\n3.2 Asymptotic results\nSimilar to the AR(p) case, the \u201cidealized\u201d estimator of \u03b2 is defined as the solution of \u03b2\u0302Ideal =\ninf\u03b2\n{\n\u03c6(B)\u03b8(B)\u22121et\n}2\n, assuming {et} observable. As a standard estimator of an ARMA model, it\nhas been shown that (e.g., Brockwell and Davis, 1987)\n\u221a\nn(\u03b2\u0302Ideal \u2212 \u03b2) D\u2212\u2192 N\n(\n0, \u03c32V(\u03b2)\u22121\n)\n,\nwhere\nV(\u03b2) = E\n(\nU1U\u03c41 U1V\n\u03c4\n1\nV1U\u03c41 V1V\n\u03c4\n1\n)\n, (16)\nUt = (Ut, Ut\u22121, \u00b7 \u00b7 \u00b7 , Ut+1\u2212p)\u03c4 , Vt = (Vt, Vt\u22121, \u00b7 \u00b7 \u00b7 , Vt+1\u2212q)\u03c4 . {Ut} is an AR(p) process defined by\n\u03c6(B)Ut = at and {Vt} is an AR(q) process defined by \u03b8(B)Vt = bt, at and bt are white noise\nprocesses. Obviously, when the model does not contain the AR component (pure MA(q) model),\nV(\u03b2) = E (V1V\u03c41) . Using this result, the following asymptotic results for the ARMA(p, q) case can\nbe established.\n8\nTheorem 3 Under the conditions (A1) to (A5) and (A6\u2217) in Appendix A, and that \u03c6 satisfies the\nstationarity condition and \u03b8 satisfies the invertibility condition, then as n\u2192\u221e,\n\u221a\nn(\u03b2\u02dc \u2212 \u03b2\u0302Ideal) = op(1).\nAs a result of Theorem 3, \u03b2\u02dc shares the same asymptotic distribution of \u03b2\u0302Ideal, i.e.,\n\u221a\nn\n(\n\u03b2\u02dc \u2212 \u03b2\n)\nD\u2212\u2192 N\n(\n0, \u03c32V(\u03b2)\u22121\n)\n,\nwhere V(\u03b2) is defined in (16).\nTheorem 4 Under the conditions (A1) to (A5) and (A6\u2217) in Appendix A and that {et} is a\nstationary, invertible ARMA(p, q) process, then for any point x in the support of Xt, as n\u2192\u221e,\n\u221a\nnh\n{\nf\u0302(x)\u2212 f(x)\u2212 h\n2\u00b52\n2\nf\u00a8(x)\n}\nD\u2212\u2192 N\n(\n0, \u03c3(x)2\n)\n,\nwhere\n\u03c3(x)2 =\n\u03c32\n\u222b\nK(u)2du\ng1(x)\n,\nand g1(x) is the marginal density function of Xt.\nTheorems 3 and 4 show that similar results as those in the AR(p) case continue to hold in the\nARMA(p, q) case, despite the more complicated correlation structure. Results similar to Theorems\n2 and 4 are established by Xiao et al. (2003, Theorem 2) and Su and Ullah (2006, Theorem 3.1)\nunder different assumptions on et.\n4 Numerical properties\nTo study the finite-sample properties of the proposed estimator, simulation studies are conducted\nusing model (1), where\nf(Xt) = sin(4Xt) + cos(2Xt),\nand Xt is generated from an AR(1) model Xt = 0.3Xt\u22121 + at, at \u223c i.i.d. N(0, 0.32). For {et}, an\nARMA(1,1) model (et = \u03c6et\u22121 + \u03b5t \u2212 \u03b8\u03b5t\u22121) and two simple seasonal models (et = \u03c64et\u22124 + \u03b5t and\net = \u03b5t \u2212 \u03b84\u03b5t\u22124, denoted as AR(1)4 and MA(1)4, respectively) are considered. In these models,\n\u03b5t \u223c N(0, 0.52).\nThree sample sizes (100, 200 and 400) are considered and 200 replications are used in each case.\nThe standard normal density function is used as the kernel function. Different bandwidths b and h\nare experimented. Due to the fact that the results are not very sensitive to the bandwidths, only\nthe case of h = 1.06sXn\u22121\/5 and b = 1.06sXn\u22121\/4 is reported here.\n9\nFor comparison, under the same setting specified above, simulations are run using the proposed\nnonparametric transfer function approach, the AR approximation approach of Xiao et al. (2003),\nthe nonparametric AR approximation approach of Su and Ullah (2006), and the \u201cconventional\u201d\nlocal linear estimator, in which {et} is assumed to be white noise. In the sequel, the approaches\nwill be abbreviated as NPTF, XLCM, SU and WHITE, respectively.\nThe mean squared errors (MSE\u2261 1n\n\u2211n\nt=1{f\u0302(Xt)\u2212 f(Xt)}2) of all four estimators are averaged\nover the replications. As a measure of relative efficiency, the relative MSEs of NPTF, XLCM,\nand SU are calculated by dividing their average MSEs by that of WHITE. The relative MSEs are\nreported in Tables 1 and 2 under the corresponding procedure names. The means and standard\ndeviations of \u03c6\u0302 and \u03b8\u0302 from NPTF are also reported, as well as the average mean squared error of\nWHITE (AMSE), which is the common denominator of the relative MSEs. A histogram of \u03c6\u0302 and\na plot of a typical simulation are given in Figure 1.\n\u22120.6 \u22120.4 \u22120.2 0 0.2\n0\n10\n20\n30\n40\n50\n60\n70\n80\nPhi Hat\n\u22121 \u22120.5 0 0.5 1\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\nmean and estimated mean\nFigure 1: \u03c6 = \u22120.2, n=200. Left panel: histogram of \u03c6\u02c6, right panel: true (solid line) and estimated (dashed\nline) transfer function in a typical simulation.\nThe following phenomena are also observed in Xiao et al. (2003) and Su and Ullah (2006)\nso they are only briefly mentioned here. (1) The NPTF estimator f\u0302(\u00b7) is more efficient than the\nconventional local linear regression estimator, the stronger the autocorrelation, the larger the gain\nin efficiency of f\u0302(\u00b7). (2) the performance of the estimators improves with the increase of sample\nsize. (3) The MA estimates may have large bias and larger sample sizes are needed to improve\nthe performance. In this study we model et explicitly as an ARMA(p, q) process. As illustrated in\nFigure 1, the sampling distributions of \u03c6\u02c6 and \u03b8\u02c6 are close to their asymptotic normal distributions.\nFor a comparison between NPTF, XLCM and SU, the simulation shows that generally they are\nall more efficient than the conventional estimator. When {et} follows an ARMA model with small\n10\nTable 1: Simulation results: AR(1) and MA(1) models\n\u03c6 \u03b8 n mean(\u03c6\u02c6), s\u03c6\u02c6 mean(\u03b8\u02c6), s\u03b8\u02c6 AMSE NPTF XLCM SU\n100 -.786, .070 .033 .466 .484 .553\n-.8 200 -.802, .049 .023 .405 .414 .464\n400 -.799, .034 .015 .395 .400 .482\n100 -.507, .100 .019 .792 .810 .895\n-.5 200 -.508, .065 .011 .801 .809 .900\n400 -.501, .047 .006 .756 .767 .836\n100 -.216, .105 .018 .992 1.01 1.12\n-.2 200 -.210, .082 .010 .966 .970 1.04\n400 -.200, .054 .006 .981 .982 1.05\n100 .196, .107 .020 1.01 1.07 1.10\n.2 200 .198, .078 .012 1.05 1.06 1.12\n400 .198, .054 .007 1.01 1.01 1.06\n100 .483, .096 .031 .912 .926 .943\n.5 200 .493, .066 .019 .904 .910 .944\n400 .494, .048 .010 .898 .902 .921\n100 .774, .076 .092 .835 .837 .845\n.8 200 .792, .049 .053 .758 .761 .776\n400 .799, .032 .030 .738 .740 .745\n100 -.712, .091 .120 .818 .883 .916\n-.8 200 -.742, .057 .069 .753 .816 .859\n400 -.765, .035 .038 .746 .797 .847\n100 -.492, .099 .092 .884 .921 .961\n-.5 200 -.497, .069 .052 .849 .885 .933\n400 -.496, .048 .029 .833 .872 .927\n100 -.184, .115 .064 .990 1.02 1.05\n-.2 200 -.198, .075 .039 .953 .954 1.03\n400 -.200, .052 .023 .950 .949 1.03\n100 .219, .123 .058 .955 .965 1.06\n.2 200 .209, .078 .034 .936 .950 1.03\n400 .204, .054 .021 .919 .926 1.02\n100 .516, .099 .059 .791 .824 .987\n.5 200 .497, .073 .037 .757 .773 .866\n400 .501, .048 .023 .742 .759 .858\n100 .725, .094 .053 .691 .737 .843\n.8 200 .745, .061 .047 .665 .696 .773\n400 .757, .040 .029 .643 .682 .740\n11\nTable 2: Simulation results: ARMA(1,1), AR(1)4 and MA(1)4 models\n\u03c6 \u03b8 n mean(\u03c6\u02c6), s\u03c6\u02c6 mean(\u03b8\u02c6), s\u03b8\u02c6 AMSE NPTF XLCM SU\n100 .217, .151 -.645, .127 .039 .836 .869 .944\n.2 -.8 200 .211, .093 -.685, .076 .026 .802 .873 .985\n400 .209, .065 -.739, .055 .013 .737 .815 .909\n100 .512, .123 -.537, .147 .076 .737 .780 .839\n.5 -.8 200 .518, .083 -.592, .104 .044 .703 .748 .784\n400 .522, .056 -.639, .075 .025 .669 .728 .771\n100 .819, .079 -.286, .163 .259 .761 .819 .857\n.8 -.8 200 .816, .053 -.397, .142 .161 .692 .748 .761\n400 .812, .039 -.482, .083 .083 .666 .710 .726\n100 .207, .183 -.457, .168 .029 .882 .930 1.04\n.2 -.5 200 .210, .127 -.469, .111 .016 .865 .907 1.03\n400 .207, .086 -.485, .080 .011 .859 .886 .975\n100 .516, .141 -.381, .147 .059 .771 .823 .885\n.5 -.5 200 .507, .086 -.422, .088 .034 .759 .781 .841\n400 .503, .056 -.447, .069 .019 .758 .783 .802\n100 .806, .094 -.235, .148 .210 .784 .813 .830\n.8 -.5 200 .811, .051 -.305, .103 .113 .714 .753 .783\n400 .812, .038 -.359, .079 .060 .663 .703 .730\n\u03c64 \u03b84 n mean(\u03c6\u02c64), s\u03c6\u02c64 mean(\u03b8\u02c64), s\u03b8\u02c64 AMSE NPTF XLCM SU\n100 -.764, .072 .039 .471 .927 1.03\n-.8 200 -.783, .048 .025 .434 .895 .978\n400 -.791, .034 .015 .421 .896 .962\n100 -.484, .094 .020 .874 1.01 1.13\n-.5 200 -.488, .065 .013 .836 .996 1.10\n400 -.495, .048 .008 .815 .989 1.07\n100 .495, .113 .018 .959 1.05 1.23\n.5 200 .493, .064 .011 .875 1.02 1.19\n400 .495, .050 .007 .866 1.02 1.18\n100 .698, .098 .023 .872 1.03 1.17\n.8 200 .721, .061 .013 .842 1.01 1.20\n400 .741, .045 .008 .809 1.00 1.16\n12\n|\u03b8| (including pure AR models), NPTF and XLCM have similar efficiency, however when |\u03b8| is\nlarge, the NPTF estimator is more efficient. For the seasonal models, NPTF has similar gain in\nefficiency as in the non-seasonal models, while in many cases XLCM and SU fail to approximate et\nappropriately and the estimate is no longer efficient (Table 2). In the simulation higher-order AR\napproximations are also used in XLCM, but the performance does not always improve, partially\ndue to the additional error introduced in estimating more parameters. Since the finding is similar,\nthe detailed results are omitted. In the simulation, SU is not as efficient as NPTF and XLCM,\nmainly because here et is generated from ARMA models of finite order. In a separate study, et is\ngenerated from nonlinear finite order AR processes and SU is found to be more efficient.\n5 Example: river flow and rainfall\nIn this section the proposed nonparametric transfer function approach is used to analyze the effect\nof daily rain fall on river flow of Kanna river (Japan) in year 1956. The effect of rainfall on river\nflow is usually highly nonlinear, mainly because the soil moisture varies from rainy period to dry\nperiod. This dataset was analyzed by Ozaki (1985) and later used by Chen and Tsay (1996) as an\nexample of the nonlinear transfer function (NLTF) model. For details of the data, see Chen and\nTsay (1996).\nThe proposed nonparametric transfer function model is used to analyze this dataset and the\nperformance is compared with those of the NLTF model and the linear transfer function model\n(LTF). The sample autocorrelation function (ACF) of Yt indicates non-stationarity. After taking\nfirst order difference of Yt, the resulting series appears to be stationary. Let Zt = Yt \u2212 Yt\u22121 and\nconsider the following model\nZt = f(Xt, Xt\u22121, Xt\u22122) + et. (17)\nNote here a low-dimensional smoothing model is used instead of an univariate smoothing model.\nFollowing the proposed estimation procedures, f(\u00b7) is first estimated assuming {et} i.i.d., then the\nresulting preliminary estimate f\u02dc(\u00b7) is removed from Zt and a model is identified for {et} based on\nthe sample autocorrelation function of the partial residuals (Figure 2). The resulting model is an\nAR model with lagged variables at lags 4, 5, 6 and 14.\nThe bandwidth is selected via the generalized cross validation (GCV) criteria (Craven and\nWahba, 1979).\nh = argmin\nh\n(Y \u2212 f\u0302)\u03c4 (Y \u2212 f\u0302)\nn[1\u2212 tr(Sh)\/n]2 ,\nwhere Sh is the smoother matrix associated with h such that f\u0302 = ShY, and Y is the vector of\nobservations. In order to compare with the parametric models, the equivalent number of parameters\ndefined as tr(Sh) is also calculated. The resulting bandwidth is 5 and the equivalent number of\n13\n0 10 20 30 40 50 60\nLag\n-\n0.10\n-\n0.05\n0.00\n0.05\n0.10\n0.15\nACF\n Sample ACF of the Preliminary Residuals\nFigure 2: Sample ACF plot of the partial residuals after removing f\u02dc(\u00b7)\nparameters is 33.46. The estimated AR parameters are \u03c6\u03024 = .0912, \u03c6\u03025 = .1264, \u03c6\u03026 = .1593 and\n\u03c6\u030214 = .0704. Figure 3 is the ACF plot of the final residuals.\n0 10 20 30 40 50 60\nLag\n-\n0.15\n-\n0.10\n-\n0.05\n-\n0.00\n0.05\n0.10\nACF\n Sample ACF of the Final Residuals\nFigure 3: Sample ACF plot of the final residuals\nTo study the forecasting performance of the NPTF model, the following rolling forecasting\nscheme is employed: for each t = 180, 181, \u00b7 \u00b7 \u00b7 , 365, data available at t are used to build the\nmodel and make one-step ahead prediction. For convenience, actual values of Xt+1 are used in the\nprediction. For each t, the forecasting error Yt+1\u2212Y\u0302t(1) is calculate. Finally, the squared forecasting\nerrors are averaged over t. The square-root of this average is referred to as \u201cpost-sample forecasting\nRMSE\u201d.\nTable 3 shows a comparison between the NPTF model with a parametric nonlinear transfer\nfunction model (NLTF) and a linear transfer function model (LTF) fitted by Chen and Tsay\n(1996). Residual variances and RMSEs from rolling forecasts are obtained using the model settings\ndetailed in Chen and Tsay (1996).\nThe above results show that the NPTF has smaller residual variance, but large equivalent\nnumber of parameters. This may indicate overfitting. However, the better forecasting performance\nof the NPTF model justifies its use of more parameters.\nThe one-step ahead forecast errors of the NPTF model and the NLTF model are plotted against\nthe forecasting origins in Figure 4. The performance of the LTF model is not as good as the NLTF\nand NPTF models, so its errors are not plotted in this figure for clearer presentation. From this\n14\nTable 3: Within- and Post-Sample Comparisons\nNPTF NLTF LTF\n(Equivalent) Number of Parameters 33.46 12 10\nResidual variance 4.58 6.23 20.81\nForecasting RMSE 8.80 12.56 13.93\n0 20 40 60 80 100 120 140 160 180 200\n\u221280\n\u221260\n\u221240\n\u221220\n0\n20\n40\n60\n80\n100\nOne\u2212Step Forecast Error\nFigure 4: The one-step ahead forecast errors of the NPTF model (solid line) and the NLTF model\n(dashed line)\nfigure it is clear that the NPTF model outperforms the NLTF model most of the time. On average,\nthe NPTF model performs better than the NLTF and LTF models in that it produces not only\nsmaller within-sample RMSE but also smaller post-sample RMSE. This example shows the potential\nof the nonparametric transfer function model in modeling nonlinear time series.\n6 Summaries and discussions\nIn this paper a new method is proposed to model nonlinear relationships between an input and\nan output time series. The transfer function f(\u00b7) is modeled by nonparametric smoothing and the\ninnovation process {et} is modeled as a stationary ARMA(p, q) process. The nonparametric feature\nof this model allows us to model highly nonlinear relationships of unknown functional forms, while\nmodeling {et} as an ARMA model improves not only the efficiency in estimating f(\u00b7) but also the\nforecasting performance. The simulations and empirical study show good potential of this model\n15\nin analyzing nonlinear time series.\nThere are some issues in the nonparametric transfer function model that deserve further study.\nFor example, in this study the transfer function is univariate. It is easy, though tedious, to generalize\nthe results to multi-dimensional cases, under the general model Yt = f(X1t, \u00b7 \u00b7 \u00b7 , Xpt)+et. However,\nsuch a direct generalization is often not practical in practice due to the aforementioned \u201ccurse of\ndimensionality\u201d. To solve this problem, more restrictive models, such as the additive model, must\nbe considered. Research addressing this topic is ongoing.\nAcknowledgement\nRong Chen\u2019s research is partially supported by NSF grant DMS-0244541 and NIH grant R01\nGM068958. Qiwei Yao\u2019s research is partially supported by EPSRC grants GR\/R97436 and EP\/C549058.\nWe would like to thank the editor and two anonymous referees for their valuable comments and\nsuggestions which led to a substantial improvement of the paper.\nAppendix A \u2013 Technical Proofs\nIn the proofs that follow, C > 0 denotes a generic constant that may vary from line to line. Let\ng1(\u00b7) be the density function of Xt and gi(xt1, \u00b7 \u00b7 \u00b7 , xti) be the i-dimensional joint density function of\n{Xt1, \u00b7 \u00b7 \u00b7 , Xti}. The following assumptions are needed, of which (A1) to (A5) are needed for both\nthe pure AR(p) and the ARMA(p, q) cases, (A6) is needed for the pure AR(p) case and (A6*) is\nneeded for the ARMA(p, q) case.\n(A1) {Xt} is \u03b2-mixing in the sense that\n\u03b2(k) = E{ sup\nB\u2208F\u221e\nk\n|P (B)\u2212 P (B|X0, X\u22121, \u00b7 \u00b7 \u00b7)|} \u2192 0\nas k \u2192 \u221e, where F ji is the \u03c3-algebra generated by {Xi, \u00b7 \u00b7 \u00b7 , Xj} for i \u2264 j. In addition,\u2211\nk\u22651 k\u03b2(k)\u03b4\/(2+\u03b4) <\u221e for some \u03b4 \u2208 (0, 8).\n(A2) The kernel function is symmetric, compactly supported and Lipschitz continuous.\n(A3) f(\u00b7) has continuous second derivative f\u00a8(\u00b7) and g1(\u00b7) is bounded away from zero.\n(A4) As n\u2192\u221e, h = O(n\u22121\/5), b = o(n\u22121\/5), and nb4 \u2192\u221e.\n(A5) {Xt} and {\u03b5t} are two independent processes.\n(A6) The weight function w(\u00b7) is continuous on its compact support contained in {g1(x) > 0}.\n16\n(A6*) Xt has bounded support [a, b]. The density functions g1(\u00b7), g2(\u00b7, \u00b7), g4(\u00b7, \u00b7, \u00b7, \u00b7) and\ng6(\u00b7, \u00b7, \u00b7, \u00b7, \u00b7, \u00b7) are continuous and have continuous first two derivatives.\nThe following lemma is needed to prove the theorems:\nLemma 1 As n\u2192\u221e, it holds uniformly for x in any compact subset of {g1(x) > 0} that\nf\u02dc(x)\u2212 f(x) = 1\nnbg1(x)\nn\u2211\nt=1\nK\n(Xt \u2212 x\nb\n)\net +\nb2\n2\n\u00b52f\u00a8(x) +Op\n[\nRn(x)\n{\n(\nlogn\nnb\n)1\/4 + b\n}]\n,\nwhere \u00b52 =\n\u222b\nu2K(u)du, and\nRn(x) =\n1\nnbg1(x)\n{\u2223\u2223\u2223 n\u2211\nt=1\nK\n(Xt \u2212 x\nb\n)\net\n\u2223\u2223\u2223+ \u2223\u2223\u2223 n\u2211\nt=1\n(Xt \u2212 x\nb\n)\nK\n(Xt \u2212 x\nb\n)\net\n\u2223\u2223\u2223}+O(b2).\nProof of Lemma 1\nIt follows from Theorem 5.3 of Fan and Yao (2003) that\nsk(x) = g1(x)\u00b5k +Op\n{( logn\nnb\n)1\/2\n+ b2\n}\nuniformly for x \u2208 A, where sk(x) is defined in (5), \u00b5k =\n\u222b\nukK(u)du, and A is any compact set\ncontained in {g1(x) > 0}. Hence it holds uniformly for x \u2208 A that\nSn(x) = S(x) +Op\n{( logn\nnb\n)1\/2\n+ b2\n}\n,\nwhere S(x) = g1(x)diag(1, \u00b52). Write Y \u2217t = Yt\u2212f(x)\u2212 f\u02d9(x)(Xt\u2212x). It is easy to see from (4) that\u2223\u2223\u2223 n\u2211\nt=1\n{\nWn\n(Xt \u2212 x\nb\n, x\n)\n\u2212 g1(x)\u22121K\n(Xt \u2212 x\nb\n)}\nY \u2217t\n\u2223\u2223\u2223\n=\n\u2223\u2223\u2223(1, 0){Sn(x)\u22121 \u2212 S(x)\u22121} n\u2211\nt=1\n(\n1,\nXt \u2212 x\nb\n)\u03c4\nK\n(Xt \u2212 x\nb\n)\nY \u2217t\n\u2223\u2223\u2223\n\u2264 [(1, 0){Sn(x)\u22121 \u2212 S(x)\u22121}2(1, 0)\u03c4 ]1\/2\n{\u2223\u2223\u2223 n\u2211\nt=1\nK\n(Xt \u2212 x\nb\n)\nY \u2217t\n\u2223\u2223\u22232 + \u2223\u2223\u2223 n\u2211\nt=1\nXt \u2212 x\nb\nK\n(Xt \u2212 x\nb\n)\nY \u2217t\n\u2223\u2223\u22232}1\/2\n\u2264 [(1, 0){Sn(x)\u22121 \u2212 S(x)\u22121}2(1, 0)\u03c4 ]1\/2\n{\u2223\u2223\u2223 n\u2211\nt=1\nK\n(Xt \u2212 x\nb\n)\nY \u2217t\n\u2223\u2223\u2223+ \u2223\u2223\u2223 n\u2211\nt=1\nXt \u2212 x\nb\nK\n(Xt \u2212 x\nb\n)\nY \u2217t\n\u2223\u2223\u2223}\n\u2264 Op\n[{( logn\nnb\n)1\/2\n+ b2\n}1\/2]{\u2223\u2223\u2223 n\u2211\nt=1\nK\n(Xt \u2212 x\nb\n)\net\n\u2223\u2223\u2223+ \u2223\u2223\u2223 n\u2211\nt=1\nXt \u2212 x\nb\nK\n(Xt \u2212 x\nb\n)\net\n\u2223\u2223\u2223+O(nb3)}.\nThe last inequality follows from the fact that Yt = f(Xt) + et, K(\u00b7) has a compact support. Now\nthe lemma follows from (3) and a simple Taylor expansion. The proof is completed.\n17\nProof of Theorem 1\nSince {et} is a stationary Gaussian AR(p) process, it is also \u03b2-mixing with exponentially decaying\nmixing coefficients. Put wt = w(Xt), let A = X\u03c41WX1 and B = X\n\u03c4\n1WY1, where X1, Y1 and W\nare defined in section 2.1. From (7) we have \u03c6\u02dc = A\u22121B, the (r, s)-th element of A is\nArs =\nn\u2211\nt=1\n[\nYt\u2212r \u2212 f\u02dc(Xt\u2212r)\n][\nYt\u2212s \u2212 f\u02dc(Xt\u2212s)\n] p\u220f\nk=0\nwt\u2212k\n=\nn\u2211\nt=1\n[\net\u2212r + f(Xt\u2212r)\u2212 f\u02dc(Xt\u2212r)\n][\net\u2212s + f(Xt\u2212s)\u2212 f\u02dc(Xt\u2212s)\n] p\u220f\nk=0\nwt\u2212k\n=\nn\u2211\nt=1\net\u2212ret\u2212s\np\u220f\nk=0\nwt\u2212k +Ars1 +Ars2 +Ars3,\nwhere\nArs1 =\nn\u2211\nt=1\n{f(Xt\u2212r)\u2212 f\u02dc(Xt\u2212r)}{f(Xt\u2212s)\u2212 f\u02dc(Xt\u2212s)}\np\u220f\nk=0\nwt\u2212k,\nArs2 =\nn\u2211\nt=1\net\u2212r{f(Xt\u2212s)\u2212 f\u02dc(Xt\u2212s)}\np\u220f\nk=0\nwt\u2212k, Ars3 =\nn\u2211\nt=1\net\u2212s{f(Xt\u2212r)\u2212 f\u02dc(Xt\u2212r)}\np\u220f\nk=0\nwt\u2212k.\nThe r-th element of B is\nBr =\nn\u2211\nt=1\n[\nYt \u2212 f\u02dc(Xt)\n][\nYt\u2212r \u2212 f\u02dc(Xt\u2212r)\n] p\u220f\nk=0\nwt\u2212k\n=\nn\u2211\nt=1\n[\net + f(Xt)\u2212 f\u02dc(Xt)\n][\net\u2212r + f(Xt\u2212r)\u2212 f\u02dc(Xt\u2212r)\n] p\u220f\nk=0\nwt\u2212k\n=\nn\u2211\nt=1\netet\u2212r\np\u220f\nk=0\nwt\u2212k +Br1 +Br2 +Br3,\nwhere\nBr1 =\nn\u2211\nt=1\n{f(Xt)\u2212 f\u02dc(Xt)}{f(Xt\u2212r)\u2212 f\u02dc(Xt\u2212r)}\np\u220f\nk=0\nwt\u2212k,\nBr2 =\nn\u2211\nt=1\net{f(Xt\u2212r)\u2212 f\u02dc(Xt\u2212r)}\np\u220f\nk=0\nwt\u2212k, Br3 =\nn\u2211\nt=1\net\u2212r{f(Xt)\u2212 f\u02dc(Xt)}\np\u220f\nk=0\nwt\u2212k.\nThe Theorem follows immediately from the two statements below:\n(i) Br1 +Br2 +Br3 = op(\n\u221a\nn), and\n(ii) Ars1 +Ars2 +Ars3 = op(\n\u221a\nn).\nfor all r, s = 1, 2, \u00b7 \u00b7 \u00b7 , p.\nHere only (i) is established. The proof for (ii) is similar and simpler. By Lemma 1, we may\nwrite\nBr1 = {Br11 +Br12 +Br13 +Op(nb4)}{1 + op(1)}, (18)\n18\nwhere\nBr11 =\n1\nn2b2\n\u2211\ni,j,k\nK\n(Xi \u2212Xk\nb\n)\nK\n(Xj \u2212Xk\u2212r\nb\n) eiej\ng1(Xk)g1(Xk\u2212r)\np\u220f\nl=0\nwk\u2212l \u2261 1\nn2b2\n\u2211\ni,j,k\n\u03b6(\u03bei, \u03bej , \u03bek),\nBr12 =\nb\u00b52\n2n\n\u2211\ni,k\neif\u00a8(Xk\u2212r)\ng1(Xk)\nK\n(Xi \u2212Xk\nb\n) p\u220f\nl=0\nwk\u2212l, Br13 =\nb\u00b52\n2n\n\u2211\ni,k\neif\u00a8(Xk)\ng1(Xk\u2212r)\nK\n(Xi \u2212Xk\u2212r\nb\n) p\u220f\nl=0\nwk\u2212l,\nwhere \u03bei = (Xi, Xi\u22121, \u00b7 \u00b7 \u00b7 , Xi\u2212p, ei)\u03c4 . Br11 is split into two sums Br111 and Br112 consisting of,\nrespectively, the terms with different i, j, k and the terms with at least two of i, j, k the same. To\nperform the Hoeffding decomposition on the U -statistic Br111, put\n\u03ba(\u03bei, \u03bej , \u03bek) = \u03b6(\u03bei, \u03bej , \u03bek) + \u03b6(\u03bei, \u03bek, \u03bej) + \u03b6(\u03bej , \u03bei, \u03bek)\n+ \u03b6(\u03bej , \u03bek, \u03bei) + \u03b6(\u03bek, \u03bei, \u03bej) + \u03b6(\u03bek, \u03bej , \u03bei).\nDefine\n\u03b8(P ) =\n\u222b \u222b \u222b\n\u03ba(\u03bei, \u03bej , \u03bek) dP (\u03bei) dP (\u03bej) dP (\u03bek);\n\u03ba\u02dc1(\u03bei) =\n\u222b \u222b\n\u03ba(\u03bei, \u03bej , \u03bek) dP (\u03bej) dP (\u03bek);\n\u03ba\u02dc2(\u03bei, \u03bej) =\n\u222b\n\u03ba(\u03bei, \u03bej , \u03bek) dP (\u03bek);\n\u03ba\u02dc3(\u03bei, \u03bej , \u03bek) = \u03ba(\u03bei, \u03bej , \u03bek),\nThen \u03ba(\u03bei, \u03bej , \u03bek) satisfies the following:(\nn\n3\n)\u22121 \u2211\n1\u2264i<j<k\u2264n\n\u03ba(\u03bei, \u03bej , \u03bek) =\n3\u2211\nc=0\n(\n3\nc\n)\nU (c)n ,\nwhere\nU (0)n = \u03b8(P ),\nU (1)n =\n1\nn\nn\u2211\ni=1\n\u03ba\u02dc1(\u03bei)\u2212 \u03b8(P ),\nU (2)n =\n2\nn(n\u2212 1)\n\u2211\n1\u2264i<j\u2264n\n\u03ba\u02dc2(\u03bei, \u03bej)\u2212\n2\nn\nn\u2211\ni=1\n\u03ba\u02dc1(\u03bei) + \u03b8(P ),\nU (3)n =\n6\nn(n\u2212 1)(n\u2212 2)\n\u2211\n1\u2264i<j<k\u2264n\n\u03ba\u02dc3(\u03bei, \u03bej , \u03bek)\u2212\n6\nn(n\u2212 1)\n\u2211\n1\u2264i<j\u2264n\n\u03ba\u02dc2(\u03bei, \u03bej) +\n3\nn\nn\u2211\ni=1\n\u03ba\u02dc1(\u03bei)\u2212 \u03b8(P ).\nWe can show the following:\n\u03ba\u02dc1(\u03bei) = 0,\n\u03ba\u02dc2(\u03bei, \u03bej) = b\n2 eiejwiwjR(Xi, Xj)\ng1(Xi)g1(Xj)\n{g2(Xi, Xj) + g2(Xj , Xi)}{1 +O(b)},\n19\nwhere R(xi, xj) =E(w(Xk\u22121) \u00b7 \u00b7 \u00b7w(Xk\u2212i+1)w(Xk\u2212i\u22121) \u00b7 \u00b7 \u00b7w(Xk\u2212p)|Xk = xi, Xk\u2212i = xj). Thus\nU (1)n = \u2212\u03b8(P ),\nU (2)n =\n2\nn(n\u2212 1)\n\u2211\n1\u2264i<j\u2264n\n\u03ba\u02dc2(\u03bei, \u03bej) + \u03b8(P ),\nU (3)n =\n6\nn(n\u2212 1)(n\u2212 2)\n\u2211\n1\u2264i<j<k\u2264n\n\u03ba(\u03bei, \u03bej , \u03bek)\u2212\n6\nn(n\u2212 1)\n\u2211\n1\u2264i<j\u2264n\n\u03ba\u02dc2(\u03bei, \u03bej)\u2212 \u03b8(P )\n=\n6\nn(n\u2212 1)(n\u2212 2)\n\u2211\n1\u2264i<j<k\u2264n\n[\u03ba(\u03bei, \u03bej , \u03bek)\u2212 \u03ba\u02dc2(\u03bei, \u03bej)\u2212 \u03ba\u02dc2(\u03bei, \u03bek)\u2212 \u03ba\u02dc2(\u03bej , \u03bek)]\u2212 \u03b8(P )\n\u2261 6\nn(n\u2212 1)(n\u2212 2)\n\u2211\n1\u2264i<j<k\u2264n\n\u03ba3(\u03bei, \u03bej , \u03bek)\u2212 \u03b8(P ).\nCombining the above results, we have\nBr111 =\n1\nn2b2\n\u2211\n1\u2264i<j<k\u2264n\n\u03ba3(\u03bei, \u03bej , \u03bek) +\nn\u2212 2\nn2\n\u2211\n1\u2264i<j\u2264n\n\u03ba\u02dc2(\u03bei, \u03bej)\/b\n2.\nIt follows from Lemma 2 of Yoshihara (1976) (Appendix B) that for any \u00b2 > 0,\nP\n{ 1\nn2b2\n\u2223\u2223\u2223 \u2211\n1\u2264i<j<k\u2264n\n\u03ba3(\u03bei, \u03bej , \u03bek)\n\u2223\u2223\u2223 > \u00b2\u221an} \u2264 n\u00b2\u22122\nb4\nE\n\u2223\u2223\u2223 1\nn3\n\u2211\n1\u2264i<j<k\u2264n\n\u03ba3(\u03bei, \u03bej , \u03bek)\n\u2223\u2223\u22232\n= O(n\u22121b\u22124)\u2192 0,\nand\nP\n{ 1\nn\n\u2223\u2223\u2223 \u2211\n1\u2264i<j\u2264n\n\u03ba\u02dc2(\u03bei, \u03bej)\/b\n2\n\u2223\u2223\u2223 > \u00b2\u221an} \u2264 n\u00b2\u22122E\u2223\u2223\u2223 1\nn2\n\u2211\n1\u2264i<j\u2264n\n\u03ba\u02dc2(\u03bei, \u03bej)\/b\n2\n\u2223\u2223\u22232 = O(n\u22121).\nThus Br111 = op(\n\u221a\nn). Similar (but simpler) arguments may show that Br112 = op(\n\u221a\nn) (therefore\nBr11 = op(\n\u221a\nn)), Br12 = op(\n\u221a\nn) and Br13 = op(\n\u221a\nn). Note that Assumption A4 implies\n\u221a\nnb4 \u2192 0.\nNow argument (i) holds due to (18). The proof is completed.\nProof of Theorem 2\nDefine\nY\u02dct = Yt \u2212\np\u2211\ni=1\n\u03c6\u02dci\n[\nYt\u2212i \u2212 f\u02dc(Xt\u2212i)\n]\n= Yt \u2212\np\u2211\ni=1\n\u03c6i\n[\nYt\u2212i \u2212 f\u02dc(Xt\u2212i)\n]\n+\np\u2211\ni=1\n(\u03c6i \u2212 \u03c6\u02dci)\n[\nYt\u2212i \u2212 f\u02dc(Xt\u2212i)\n]\n= f(Xt) +\np\u2211\ni=1\n\u03c6iet\u2212i + \u03b5t \u2212\np\u2211\ni=1\n\u03c6i\n[\nf(Xt\u2212i)\u2212 f\u02dc(Xt\u2212i) + et\u2212i\n]\n+\np\u2211\ni=1\n(\u03c6i \u2212 \u03c6\u02dci)\n[\nf(Xt\u2212i)\u2212 f\u02dc(Xt\u2212i) + et\u2212i\n]\n.\n20\nBy Theorem 1, \u03c6\u02dc = \u03c6+Op(n\u22121\/2), the convergence rate is faster than that for the nonparametric\nestimator f\u0302(x). Therefore we may treat \u03c6\u02dc = \u03c6 in the proof, so Y\u02dct = \u03b5t+f(Xt)+\n\u2211p\ni=1 \u03c6i{f\u02dc(Xt\u2212i)\u2212\nf(Xt\u2212i)}. By Theorem 5.3 of Fan and Yao (2003),\ns\u2217k(x) = p1(x)\u00b5k +Op\n{\n(\nlogn\nnh\n)1\/2 + h)\n}\n,\nwhere p1(x) = g1(x)E{w(Xt\u22121)w(Xt\u22122) \u00b7 \u00b7 \u00b7w(Xt\u2212p)|Xt = x}. From Lemma 1 and (11), it holds\nthat\nf\u0302(x)\u2212 f(x) = 1\nnhp1(x)\nn\u2211\nt=1\nK\n(Xt \u2212 x\nh\n) p\u220f\nl=1\nw(Xt\u2212l)\n{\n\u03b5t + f(Xt)\n+\np\u2211\nk=1\n\u03c6k[f\u02dc(Xt\u2212k)\u2212 f(Xt\u2212k)]\u2212 f(x)\u2212 f\u02d9(x)(Xt \u2212 x)\n}\n=\n1\nnhp1(x)\nn\u2211\nt=1\nK\n(Xt \u2212 x\nh\n) p\u220f\nl=1\nw(Xt\u2212l)\n{\n\u03b5t + f(Xt)\u2212 f(x)\u2212 f\u02d9(x)(Xt \u2212 x)\n}\n+\nb2\u00b52\n2nhp1(x)\np\u2211\nk=1\n\u03c6k\nn\u2211\nt=1\nK\n(Xt \u2212 x\nh\n) p\u220f\nl=1\nw(Xt\u2212l)f\u00a8(Xt\u2212k)\n+\n1\nn2hbp1(x)\np\u2211\nk=1\n\u03c6k\nn\u2211\ni,j=1\nK\n(Xi \u2212 x\nh\n) p\u220f\nl=1\nw(Xt\u2212l)K\n(Xj \u2212Xi\u2212k\nb\n) ej\ng1(Xi\u2212k)\n.(19)\nBy an ergodic theorem, the second term on the RHS of the above expression is of the order\nOp(b2) = op(h2). To show that the third term on the RHS is of the desired order, we prove it for\nsome particular k, say k = 1, the same argument holds for all k = 1, 2, \u00b7 \u00b7 \u00b7 , p. Put\n\u03b6(\u03bei, \u03bej) = K\n(Xi \u2212 x\nh\n) p\u220f\nl=1\nw(Xi\u2212l)K\n(Xj \u2212Xi\u22121\nb\n) ej\ng1(Xi\u22121)\n,\nwhere \u03bei = (Xi, Xi\u22121, \u00b7 \u00b7 \u00b7 , Xi\u2212p, ei). Denote the third term on the RHS of (19) as J .\nJ =\n\u03c61\nn2bhp1(x)\nn\u2211\ni,j=1\n\u03b6(\u03bei, \u03bej) =\n\u03c61\nn2bhp1(x)\n\u2211\n1\u2264i<j\u2264n\n[\n\u03b6(\u03bei, \u03bej) + \u03b6(\u03bej , \u03bei)\n]\n\u2261 \u03c61\nn2bhp1(x)\n\u2211\n1\u2264i<j\u2264n\n\u03ba(\u03bei, \u03bej).\nThen it holds that\nJ =\n\u03c61\nn2hbp1(x)\n\u2211\n1\u2264i<j\u2264n\n{\u03ba(\u03bei, \u03bej)\u2212 \u03ba1(\u03bei)\u2212 \u03ba1(\u03bej)}+\n\u03c61(n\u2212 1)\nn2p1(x)\nn\u2211\ni=1\n\u03ba1(\u03bei)\/(hb), (20)\nwhere\n\u03ba1(\u03bei) \u2261\n\u222b\n\u03ba(\u03bei, \u03bej)dP (\u03bej) = hb eiw(Xi)p2(x,Xi)\/g1(Xi){1 +O(h)},\nwhere p2(x,Xi) =E{w(Xj\u22122) \u00b7 \u00b7 \u00b7w(Xj\u2212p)|Xj = x,Xj\u22121 = Xi}g2(x,Xi). Denote the two terms on\nthe RHS of (20) by J1 and J2, respectively. By a CLT for mixing processes (e.g., Theorem 2.21(i)\n21\nof Fan and Yao 2003), J2 = Op(n\u22121\/2) = op{(nh)\u22121\/2}. By Lemma 2 in Appendix 2 below,\nP{\n\u221a\nnh|J1| > \u00b2} \u2264 \u03c6\n2\n1\u00b2\n\u22122nh\nh2b2p1(x)2\nE\n\u2223\u2223\u2223 1\nn2\n\u2211\n1\u2264i<j\u2264n\n{\u03ba(\u03bei, \u03bej)\u2212 \u03ba1(\u03bei)\u2212 \u03ba1(\u03bej)}\n\u2223\u2223\u22232\n= O{(nb2h)\u22121} \u2192 0.\nHence J1 = op{(nh)\u22121\/2}. Note h2 = O{(nh)\u22121\/2} under Assumption A4. Now it follows from (19)\nthat\nf\u0302(x)\u2212 f(x) = 1\nnhp1(x)\nn\u2211\nt=1\nK\n(Xt \u2212 x\nh\n) p\u220f\nl=1\nw(Xt\u2212l){\u03b5t + f(Xt)\u2212 f(x)\u2212 f\u02d9(x)(Xt \u2212 x)}+ op\n{ 1\n(nh)\n1\n2\n}\n=\n1\nnhp1(x)\nn\u2211\nt=1\nK\n(Xt \u2212 x\nh\n) p\u220f\nl=1\nw(Xt\u2212l)\u03b5t +\nh2\n2\n\u00b52f\u00a8(x) + op\n{ 1\n(nh)\n1\n2\n}\n.\nNow the theorem follows from, for example, Theorem 2.21(i) of Fan and Yao (2003). The proof is\ncompleted.\nProof of Theorem 3\nSeveral algorithms are available to solve the nonlinear optimization problem needed for estimating\nthe ARMA case. Here a nonlinear estimator based on the Gauss-Newton method is adopted.\nSpecifically, given initial estimate \u03b20 = (\u03c601, \u00b7 \u00b7 \u00b7 , \u03c60p, \u03b801, \u00b7 \u00b7 \u00b7 , \u03b80q)\u03c4 , we adopt the following notations\n\u03c60(B)\u03b80(B)\u22121 =\n\u221e\u2211\ni=0\npi0iB\ni, \u03b80(B)\u22121 =\n\u221e\u2211\ni=0\n\u03be0iB\ni, \u03c60(B)\u03b80(B)\u22122 =\n\u221e\u2211\ni=0\n\u03b70iB\ni,\nand we use the approximations\n\u03c60(B)\u03b80(B)\u22121et =\nt\u22121\u2211\ni=0\npi0i et\u2212i, \u03b80(B)\n\u22121et =\nt\u22121\u2211\ni=0\n\u03be0i et\u2212i, \u03c60(B)\u03b80(B)\n\u22122 =\nt\u22121\u2211\ni=0\n\u03b70i et\u2212i. (21)\nBy a linear Taylor expansion at \u03b20, we have\n\u03b5t \u2248 \u03c60(B)\n\u03b80(B)\net \u2212\np\u2211\ni=1\n1\n\u03b80(B)\net\u2212i\u2206\u03c6i +\nq\u2211\nj=1\n\u03c60(B)\n\u03b820(B)\net\u2212j\u2206\u03b8j ,\nwhere \u2206\u03c6i = \u03c6i \u2212 \u03c60i and \u2206\u03b8j = \u03b8j \u2212 \u03b80j . By the approximations in (21), we have the following\nregression equation\nt\u22121\u2211\ni=0\npi0i et\u2212i =\np\u2211\nj=1\nt\u2212j\u22121\u2211\ni=0\n\u03be0i et\u2212j\u2212i\u2206\u03c6i \u2212\nq\u2211\nj=1\nt\u2212j\u22121\u2211\ni=0\n\u03b70i et\u2212j\u2212i\u2206\u03b8i + \u03b5t.\nLet m = max(p, q) + 1, \u2206\u03b2 can be estimated by minimizing\nn\u2211\nt=m\n{ t\u22121\u2211\ni=0\npi0i et\u2212i \u2212\np\u2211\nj=1\nt\u2212j\u22121\u2211\ni=0\n\u03be0i et\u2212j\u2212i\u2206\u03c6i +\nq\u2211\nj=1\nt\u2212j\u22121\u2211\ni=0\n\u03b70i et\u2212j\u2212i\u2206\u03b8i\n}2\n22\nwith respect to \u2206\u03c6 and \u2206\u03b8, \u03b2\u0302 = \u03b20 + \u2206\u0302\u03b2 serves as the estimate of \u03b2. Therefore we minimize\nn\u2211\nj=1\nn\u2211\nt=m\n{\nYt\u2212a0\u2212a1(Xt\u2212Xj)+\nt\u22121\u2211\nl=1\npi0l e\u02dct\u2212l\u2212\np\u2211\ni=1\nt\u2212i\u22121\u2211\nl=0\n\u03be0l e\u02dct\u2212i\u2212l\u2206\u03c6i+\nq\u2211\ni=1\nt\u2212i\u22121\u2211\nl=0\n\u03b70l e\u02dct\u2212i\u2212l\u2206\u03b8i\n}2\nKh(Xt\u2212Xj)\nto estimate f(\u00b7) and \u03b2. Re-express the above in matrix notation, for initial estimate \u03b20, let\nD\u03c4t =\n(\u2202\u03b5t(\u03b20)\n\u2202\u03c61\n,\n\u2202\u03b5t(\u03b20)\n\u2202\u03c62\n, \u00b7 \u00b7 \u00b7 , \u2202\u03b5t(\u03b20)\n\u2202\u03c6p\n,\n\u2202\u03b5t(\u03b20)\n\u2202\u03b81\n,\n\u2202\u03b5t(\u03b20)\n\u2202\u03b82\n, \u00b7 \u00b7 \u00b7 , \u2202\u03b5t(\u03b20)\n\u2202\u03b8q\n)\n,\nwhere \u2202\u03b5t(\u03b20)\/\u2202\u03b2i, i = 1, \u00b7 \u00b7 \u00b7 , p+ q means \u2202\u03b5t\/\u2202\u03b2i evaluated at \u03b20. By a Taylor expansion,\n\u03b5t \u2248 \u03b5t(\u03b20) +D\u03c4t (\u03b2 \u2212 \u03b20) = \u03b5t(\u03b20) +D\u03c4t\u2206\u03b2,\nwhere \u03b5t(\u03b20) = \u03b80(B)\u22121\u03c60(B)et. Re-arranging terms, we have \u03b5t(\u03b20) = \u2212D\u03c4t\u2206\u03b2+ \u03b5t. An estimate\nof \u2206\u03b2 can be obtained by minimizing the sum of squares\n\u2211n\nt=1{\u03b5t(\u03b20) +D\u03c4t\u2206\u03b2}2. Define\nD = \u2212\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u2202\u03b5m(\u03b20)\n\u2202\u03c61\n\u2202\u03b5m(\u03b20)\n\u2202\u03c62\n\u00b7 \u00b7 \u00b7 \u2202\u03b5m(\u03b20)\u2202\u03c6p\n\u2202\u03b5m(\u03b20)\n\u2202\u03b81\n\u2202\u03b5m(\u03b20)\n\u2202\u03b82\n\u00b7 \u00b7 \u00b7 \u2202\u03b5m(\u03b20)\u2202\u03b8q\n\u2202\u03b5m+1(\u03b20)\n\u2202\u03c61\n\u2202\u03b5m+1(\u03b20)\n\u2202\u03c62\n\u00b7 \u00b7 \u00b7 \u2202\u03b5m+1(\u03b20)\u2202\u03c6p\n\u2202\u03b5m+1(\u03b20)\n\u2202\u03b81\n\u2202\u03b5m+1(\u03b20)\n\u2202\u03b82\n\u00b7 \u00b7 \u00b7 \u2202\u03b5m+1(\u03b20)\u2202\u03b8q\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\n\u2202\u03b5n(\u03b20)\n\u2202\u03c61\n\u2202\u03b5n(\u03b20)\n\u2202\u03c62\n\u00b7 \u00b7 \u00b7 \u2202\u03b5n(\u03b20)\u2202\u03c6p\n\u2202\u03b5n(\u03b20)\n\u2202\u03b81\n\u2202\u03b5n(\u03b20)\n\u2202\u03b82\n\u00b7 \u00b7 \u00b7 \u2202\u03b5n(\u03b20)\u2202\u03b8q\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n=\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nem\u22121\n\u03b80(B)\nem\u22122\n\u03b80(B)\n\u00b7 \u00b7 \u00b7 em\u2212p\u03b80(B) \u2212\n\u03c60(B)em\u22121\n\u03b820(B)\n\u2212\u03c60(B)em\u22122\n\u03b820(B)\n\u00b7 \u00b7 \u00b7 \u2212\u03c60(B)em\u2212q\n\u03b820(B)\nem\n\u03b80(B)\nem\u22121\n\u03b80(B)\n\u00b7 \u00b7 \u00b7 em\u2212p+1\u03b80(B) \u2212\n\u03c60(B)em\n\u03b820(B)\n\u2212\u03c60(B)em\u22121\n\u03b820(B)\n\u00b7 \u00b7 \u00b7 \u2212\u03c60(B)em\u2212q+1\n\u03b820(B)\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\nen\u22121\n\u03b80(B)\nen\u22122\n\u03b80(B)\n\u00b7 \u00b7 \u00b7 en\u2212p\u03b80(B) \u2212\n\u03c60(B)en\u22121\n\u03b820(B)\n\u2212\u03c60(B)en\u22122\n\u03b820(B)\n\u00b7 \u00b7 \u00b7 \u2212\u03c60(B)en\u2212q\n\u03b820(B)\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 .\nLet\nu =\n(\u03c60(B)\n\u03b80(B)\nem,\n\u03c60(B)\n\u03b80(B)\nem+1, \u00b7 \u00b7 \u00b7 , \u03c60(B)\n\u03b80(B)\nen\n)\u03c4\n.\nBy the same approximations in (21), we have the \u201cregressor\u201d matrix\nD =\n\uf8eb\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u2211m\u22122\ni=0 \u03be\n0\ni em\u22121\u2212i \u00b7 \u00b7 \u00b7\n\u2211m\u2212p\u22121\ni=0 \u03be\n0\ni em\u2212p\u2212i \u2212\n\u2211m\u22122\ni=0 \u03b7\n0\ni em\u22122\u2212i \u00b7 \u00b7 \u00b7 \u2212\n\u2211m\u2212q\u22121\ni=0 \u03b7\n0\ni em\u2212q\u2212i\u2211m\u22121\ni=0 \u03be\n0\ni em\u2212i \u00b7 \u00b7 \u00b7\n\u2211m\u2212p\ni=0 \u03be\n0\ni em\u2212p+1\u2212i \u2212\n\u2211m\u22121\ni=0 \u03b7\n0\ni em\u22121\u2212i \u00b7 \u00b7 \u00b7 \u2212\n\u2211m\u2212q\ni=0 \u03b7\n0\ni em\u2212q+1\u2212i\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\u2211n\u22122\ni=0 \u03be\n0\ni en\u22121\u2212i \u00b7 \u00b7 \u00b7\n\u2211n\u2212p\u22121\ni=0 \u03be\n0\ni en\u2212p\u2212i \u2212\n\u2211n\u22122\ni=0 \u03b7\n0\ni en\u22122\u2212i \u00b7 \u00b7 \u00b7 \u2212\n\u2211n\u2212q\u22121\ni=0 \u03b7\n0\ni en\u2212q\u2212i\n\uf8f6\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8 ,\nand\nu =\n(m\u22121\u2211\ni=0\npi0i em\u2212i,\nm\u2211\ni=0\npi0i em+1\u2212i, \u00b7 \u00b7 \u00b7 ,\nn\u22121\u2211\ni=0\npi0i en\u2212i\n)\u03c4\n.\nThe estimate of \u03b2 can be obtained by \u03b20 + \u2206\u0302\u03b2Ideal, where \u2206\u0302\u03b2Ideal is the \u201cidealized\u201d estimator of\n\u2206\u03b2 obtained from \u201cobservations\u201d {et}:\n\u2206\u0302\u03b2Ideal = (D\n\u03c4D)\u22121D\u03c4u.\n23\nThe estimate of \u03b2 based on the initial estimate of the innovation process e\u02dct = Yt \u2212 f\u02dc(Xt), denoted\nby \u03b2\u02dc, is obtained similarly as \u03b2\u02dc = \u03b20 + \u2206\u02dc\u03b2, where \u2206\u02dc\u03b2 = (D\u03c41D1)\n\u22121D\u03c41u1, D1 and u1 are defined\nsimilarly as D and u, with et replaced by e\u02dct.\nThe proof of the theorem is complete by showing\n(i) D\u03c41D1 = D\n\u03c4D+ op(\n\u221a\nn), and\n(ii) D\u03c41u1 = D\n\u03c4u+ op(\n\u221a\nn).\nHowever, to save the space we have to omit the quite lengthy proof here. For detailed proof, please\nsee a technical report by Liu, Chen and Yao (2005).\nProof of Theorem 4\nDefine\nY\u02dct = Yt +\nt\u22121\u2211\ni=1\np\u02dcii[Yt\u2212i \u2212 f\u02dc(Xt\u2212i)]\n= f(Xt)\u2212\n\u221e\u2211\ni=1\npiiet\u2212i + \u03b5t +\nt\u22121\u2211\ni=1\npii[Yt\u2212i \u2212 f\u02dc(Xt\u2212i)] +\nt\u22121\u2211\ni=1\n(p\u02dcii \u2212 pii)[Yt\u2212i \u2212 f\u02dc(Xt\u2212i)]\n= f(Xt) + \u03b5t \u2212\n\u221e\u2211\ni=1\npiiet\u2212i +\nt\u22121\u2211\ni=1\npii[f(Xt\u2212i)\u2212 f\u02dc(Xt\u2212i) + et\u2212i]\n+\nt\u22121\u2211\ni=1\n(p\u02dcii \u2212 pii)[f(Xt\u2212i)\u2212 f\u02dc(Xt\u2212i) + et\u2212i]\n= f(Xt) + \u03b5t \u2212\n\u221e\u2211\ni=t\npiiet\u2212i +\nt\u22121\u2211\ni=1\npii[f(Xt\u2212i)\u2212 f\u02dc(Xt\u2212i)] +\nt\u22121\u2211\ni=1\n(p\u02dcii \u2212 pii)[f(Xt\u2212i)\u2212 f\u02dc(Xt\u2212i) + et\u2212i]\nBy Theorem 5.3 of Fan and Yao (2003), we have\nf\u0302(x)\u2212 f(x)\n=\n1\nnhg1(x)\nn\u2211\nt=1\nK(\nXt \u2212 x\nh\n)\n{\nf(Xt) + \u03b5t \u2212 f(x)\u2212 f\u02d9(x)(Xt \u2212 x) +\nt\u22121\u2211\ni=1\npii[f(Xt\u2212i)\u2212 f\u02dc(Xt\u2212i)]\n\u2212\n\u221e\u2211\ni=t\npiiet\u2212i +\nt\u22121\u2211\ni=1\n(p\u02dcii \u2212 pii)[f(Xt\u2212i)\u2212 f\u02dc(Xt\u2212i) + et\u2212i]\n}\n=\n1\nnhg1(x)\nn\u2211\nt=1\nK(\nXt \u2212 x\nh\n)\n{\nf(Xt)\u2212 f(x)\u2212 f\u02d9(x)(Xt \u2212 x) + \u03b5t\n}\n+\n1\nnhg1(x)\nn\u2211\nt=2\nK(\nXt \u2212 x\nh\n)\nt\u22121\u2211\ni=1\npii[f(Xt\u2212i)\u2212 f\u02dc(Xt\u2212i)]\u2212 1\nnhg1(x)\nn\u2211\nt=2\n\u221e\u2211\ni=t\nK(\nXt \u2212 x\nh\n)piiet\u2212i\n+\n1\nnhg1(x)\nn\u2211\nt=2\nt\u22121\u2211\ni=1\n(p\u02dcii \u2212 pii)K(Xt \u2212 x\nh\n)[f(Xt\u2212i)\u2212 f\u02dc(Xt\u2212i) + et\u2212i]\n\u2261 S1 + S2 + S3 + S4\n24\nBy a Taylor expansion and Lemma 1, we can show that the remainder term in S1 related to Rn(\u00b7)\nis ignorable and we only need to consider the leading term of S1:\n1\nnhg1(x)\nn\u2211\nt=1\nK(\nXt \u2212 x\nh\n)\u03b5t +\nh2\n2\n\u00b52f\u00a8(x).\nBy Theorem 2.21 of Fan and Yao (2003), the proof is complete by showing S2+S3+S4 is of order\nop{(nh)\u22121\/2}. Again, the proof of this theorem is quite lengthy, hence omitted here. For detailed\nproof, please refer to Liu, Chen and Yao (2005).\nAppendix B \u2013 A note on Lemma 2 of Yoshihara (1976)\nYoshihara (1976) is influential as it establishes asymptotic properties of U -statistics for strictly\nstationary and \u03b2-mixing processes. Its lemma 2, which estimates the orders for the second moments\nof residual terms in the Hoeffding decomposition, appears to have an error in presentation, since \u03b3\nin (2.12) of Yoshihara (1976) may be arbitrarily large by choosing \u03b4\u2032 > 0 arbitrarily small. (Note\nthat we may let \u03b4\u2032 > 0 arbitrarily small for, for example, independent processes.) We state below a\nrectified version of the lemma, which can be derived in the same manner as the proof in the original\npaper. All the notation and citation below are referred to Yoshihara (1976).\nLemma 2 (Yoshihara 1976) . If there is a positive number \u03b4 such that for r = 2 + \u03b4 (2.3) and\n(2.4) hold, and\n\u2211\nn\u22651 n\u03b2(n)\u03b4\/(2+\u03b4) <\u221e, then we have\nE(U (c)n )\n2 = O(n\u22122), 2 \u2264 c \u2264 m.\nNote that we impose a stronger condition on the mixing coefficients \u03b2(n), and the rate O(n\u22122) is\noptimal.\nReferences\nAuestad, B. and D. Tjo\u00a8stheim, 1990, Identification of nonlinear time series: First order charac-\nterization and order estimation. Biometrika 77, 669-687.\nBox, G.E.P. and G.M. Jenkins, 1976, Time Series Analysis: Forecasting and Control. Holden-Day,\nSan Francisco, 1st ed.\nBrockwell, P.J. and R.A. Davis, 1987, Time Series: Theory and Methods. Springer-Verlag, New\nYork.\nCai, Z., J. Fan and Q. Yao, 2000, Functional-coefficient regression models for nonlinear time series.\nJournal of the American Statistical Association 95, 941\u2013956.\n25\nCarroll, R.J., J. Fan, I. Gijbels and M.P. Wand, 1997, Generalized partially linear single-index\nmodels. Journal of the American Statistical Association 92, 477\u2013489.\nChen, R. and R.S. Tsay, 1996, Nonlinear transfer functions. Journal of Nonparametric Statistics\n66, 193-204.\nChen, R. and R.S. Tsay, 1993a, Functional-coefficient autoregressive models. Journal of the Amer-\nican Statistical Association 88, 298-308.\nChen, R. and R.S. Tsay, 1993b, Nonlinear additive ARX models, Journal of the American Statis-\ntical Association 88, 955-967.\nCraven, P. and G.Wahba, 1979, Smoothing noisy data with spline functions. Numerical Mathe-\nmatics 31, 377-403.\nFan, J. and I. Gilbels, 1996, Local Polynomial Modeling and Its Applications. Chapman and Hall,\nSuffolk.\nFan, J. and Q. Yao, 2003, Nonlinear Time Series: Nonparametric and Parametric Methods.\nSpringer, New York.\nFan, J., Q. Yao and Z. Cai, 2003, Adaptive varying-coefficient linear models. Journal of the Royal\nStatistical Society, Series B 65, 57\u201380.\nHa\u00a8dle, W., H. Lu\u00a8tkepohl, and R. Chen, 1997, A review of nonparametric time series analysis.\nInternational Statistical Review 65, 49-72.\nHa\u00a8rdle, W., P. Hall, and H. Ichimura, 1993, Optimal smoothing in single-index models. The\nAnnals of Statistics 21, 157\u2013178.\nHa\u00a8rdle, W., H. Liang, and J. Gao, 2000, Partially Linear Models. Physica-Verlag, Heidelberg.\nHaggan V. and T. Ozaki, 1981, Modeling nonlinear vibrations using an amplitude-dependent\nautoregressive time series model. Biometrika 68, 189196.\nHart, J.D., 1996, Some automated methods of smoothing time-dependent data. Journal of Non-\nparametric Statistics 6 115-142, 1996.\nHeckman, J., H. Ichimura, J. Smith, and P. Todd, 1998, Characterizing selection bias using\nexperimental data. Econometrica 66, 1017\u20131098.\nIchiruma, H., 1993, Semiparametric least-squares (SLS) and weighted SLS estimation of single-\nindex models. Journal of Econometrics 58, 71\u2013120.\n26\nLiu, J.M., R. Chen and Q. Yao, 2005, Nonparametric Transfer Function Models. Technical report,\nGeorgia Southern University.\nLiu, L.M. and D.M. Hanssens, 1982, Identification of multiple-input transfer function models.\nCommunications in Statistics A11, 297-314.\nMasry, E., 1996a, Multivariate local polynomial regression for time series: Uniform consistency\nand rates. Journal of Time Series Analysis 17, 571-599.\nMasry, E., 1996b, Multivariate regression estimation: Local polynomial fitting for time series.\nStochastic Processes and Their Applications 65, 81-101.\nNewbold, P., 1973, Bayesain estimation of Box-Jenkins transfer function-noise models. Journal of\nthe Royal Statistical Society 35, 323-336.\nNewey, W.K., and T.M. Stoker, 1993, Efficiency of weighted average derivative estimators and\nindex models. Econometrica 61, 1199-1223.\nOzaki, T., 1985, Statistical identification of storage models with application to stochastic hydrol-\nogy. Water Resources Bulletin 21, 663-675.\nPoskitt, D.S., 1989, A method for the estimation and identification of transfer function models.\nJournal of the Royal Statistical Society B51, 29-46.\nRobinson, P.M., 1983, Nonparametric estimators for time series. Journal of Time Series Analysis\n4, 185-207.\nRuckstuhl, A., A.H. Welsh, and R.J. Carroll, 2000, Nonparametric function estimation of the\nrelationship between two repeatedly measured variables. Statistica Sinica 10, 51-71.\nSeverini, T.A. and J.G. Staniswalis, 1994, Quasi-likelihood estimation in semiparametric models.\nJournal of the American Statistical Association 89, 501-511.\nSmith, M., C.M. Wong, and R. Kohn, 1998, Additive nonparametric regression with autocorrelated\nerrors. Journal of the Royal Statistical Society 60, 311-331.\nSu, L. and A. Ullah, 2006, More efficient estimation in nonparametric regression with nonpara-\nmetric autocorrelated errors. Econometric Theory 22, 98-126.\nTiao, G.C. and G.E.P. Box, 1981, Modeling multiple time series with applications. Journal of the\nAmerican Statistical Association 76, 802-816.\nTj\u00f8stheim, D., 1994, Nonlinear time series: a selective review. Scandinavian Journal of Statistics\n21, 97-130.\n27\nTsay, R.S., 1985, Model identification in dynamic regression (distributed lag) models. Journal of\nBusiness Economic Statistics 3, 228-237.\nWild, C.J. and T.W. Yee, 1996, Additive extensions to generalized estimation equation methods.\nJournal of the Royal Statistical Society: Series B 58, 711-725.\nWu, C.O., C.T. Chiang, and D.R. Hoover, 1998, Asymptotic confidence regions for kernel smooth-\ning of a varying coefficient model with longitudinal data. Journal of the American Statistical\nAssociation 93, 1388-1402.\nXia, Y. and W.K. Li, 1999, On single-index coefficient regression models. Journal of the American\nStatistical Association 94, 1275-1285.\nXia, Y., H. Tong, W.K. Li and L. Zhu, 2002, An adaptive estimation of dimension reduction space\n(with discussion). Journal of the Royal Statistical Society, Series B 64, 363-410.\nXiao, Z., O.B. Linton, R.J. Carroll, and E. Mammen, 2003, Model efficient local polynomial\nestimation in nonparametric regression with autocorrelated errors. Journal of the American\nStatistical Association 98, 980-992.\nYoshihara, K., 1976, Limiting behavior of U-statistics for a stationary absolutely regular process.\nZeitschrift fur Wahrscheinlichkeitstheorie verw. Gebiete, 35, 237-252.\nZeger, S.L. and P.J. Diggle, 1994, Semiparametric models for longitudinal data with application\nto CD4 cell number in HIV seroconverters. Biometrics 50, 789-699.\n28\n"}