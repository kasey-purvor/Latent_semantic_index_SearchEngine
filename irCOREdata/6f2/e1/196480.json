{"doi":"10.1007\/s10664-010-9130-z","coreId":"196480","oai":"oai:lra.le.ac.uk:2381\/8663","identifiers":["oai:lra.le.ac.uk:2381\/8663","10.1007\/s10664-010-9130-z"],"title":"Measuring fidelity to extreme programming: a psychometric approach","authors":["Michaelides, George","Thomson, Chris","Wood, Stephen"],"enrichments":{"references":[{"id":44774527,"title":"An ethnographic study of XP practice.","authors":[],"date":"2004","doi":"10.1023\/b:emse.0000039884.79385.54","raw":"Sharp H, Robinson H (2004) An ethnographic study of XP practice. Empirical Software Engineering 9: 353\u2013375.","cites":null},{"id":44774529,"title":"Empirical study on extreme programming. Unpublished PhD thesis,","authors":[],"date":"2005","doi":null,"raw":"Syed-Abdullah S (2005) Empirical study on extreme programming. Unpublished PhD thesis, University of Sheffield.","cites":null},{"id":44774530,"title":"Experiences in learning XP practices: A qualitative study.","authors":[],"date":"2003","doi":"10.1007\/3-540-44870-5_17","raw":"Tessem BR (2003) Experiences in learning XP practices: A qualitative study. Extreme Programming and Agile Processes in Software Engineering, Lecture Notes in Computer Science, 2675\/2003:1012, Springer Berlin \/ Heidelberg.","cites":null},{"id":44774534,"title":"Extreme programming evaluation framework for object-oriented languages \u2013 version 1.3.","authors":[],"date":null,"doi":null,"raw":"Williams L, Krebs W, Layman L (2004b). Extreme programming evaluation framework for object-oriented languages \u2013 version 1.3. Technical report, NCSU.","cites":null},{"id":44774533,"title":"Extreme programming evaluation framework for object-oriented languages \u2013 version 1.4.","authors":[],"date":"2004","doi":null,"raw":"Williams L, Krebs W, Layman L (2004a) Extreme programming evaluation framework for object-oriented languages \u2013 version 1.4. Technical report, NCSU.","cites":null},{"id":44774528,"title":"Extreme programming refactored: The case against XP. APress,","authors":[],"date":"2003","doi":"10.1007\/978-1-4302-0810-5","raw":"Stephens M, Rosenberg D (2003) Extreme programming refactored: The case against XP. APress, Berkeley.","cites":null},{"id":44774535,"title":"Personality characteristics in an XP team: A repertory grid study.","authors":[],"date":"2005","doi":"10.1145\/1083106.1083123","raw":"Young M, Edwards H, McDonald S, Thompson B (2005) Personality characteristics in an XP team: A repertory grid study. HSSE '05: Proceedings of the 2005 workshop on Human and social factors of software engineering. St. Louis MO, ACM. a. Comparison between 1D linear and 1D quadratic b. Comparison between 4D linear and 4D quadratic c. Comparison between 1D quadratic and 4D quadratic","cites":null},{"id":44774531,"title":"The Sheffield software engineering observatory archive: Six years of empirical data collected from 73 complete projects.","authors":[],"date":"2009","doi":null,"raw":"Thomson C, Holcombe M (2009) The Sheffield software engineering observatory archive: Six years of empirical data collected from 73 complete projects. CS-09-01. Department of Computer Science, University of Sheffield.","cites":null},{"id":44774526,"title":"The social side of technical practices. Extreme Programming and Agile","authors":[],"date":"2005","doi":"10.1007\/11499053_12","raw":"Robinson H, Sharp H (2005b) The social side of technical practices. Extreme Programming and Agile Processes in Software Engineering, Lecture Notes in Computer Science, 3556\/2005: 100-108, Springer Berlin \/ Heidelberg.Sfetsos P, Angelis L, Stalmenos I (2006) Investigating the extreme programming system: An empirical study. Empirical Software Engineering, 11: 269\u2013301.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-12","abstract":"This is the author's final draft of the paper published as Empirical Software Engineering, 2010, 15 (6), pp. 599-617.  The original publication is available at www.springerlink.com.  Doi: 10.1007\/s10664-010-9130-zThis study assesses the Shodan survey as an instrument for measuring an individual\u2019s or a team\u2019s adherence to the extreme programming (XP) methodology. Specifically, we hypothesize that the adherence to the XP methodology is not a uni-dimensional construct as presented by the Shodan survey but a multidimensional one reflecting dimensions that are theoretically grounded in the XP literature. Using data from software engineers in the University of Sheffield\u2019s Software Engineering Observatory, two different models were thus tested and compared using confirmatory factor analysis: a uni-dimensional model and a four-dimensional model. We also present an exploratory analysis of how these four dimensions affect students\u2019 grades. The results indicate that the four-dimensional model fits the data better than the uni-dimensional one. Nevertheless, the analysis also uncovered flaws with the Shodan survey in terms of the reliability of the different dimensions. The exploratory analysis revealed that some of the XP dimensions had linear or curvilinear relationship with grades. Through validating the four-dimensional model of the Shodan survey this study highlights how psychometric techniques can be used to develop software engineering metrics of fidelity to agile or other software engineering methods","downloadUrl":"http:\/\/hdl.handle.net\/2381\/8663","fullTextIdentifier":"https:\/\/lra.le.ac.uk\/bitstream\/2381\/8663\/3\/Fidelity%20to%20Extreme%20Programming%20-%20Manuscript_revision%20150410.pdf","pdfHashValue":"3f80a86bf0c334c8d13afed40722e34a1617261d","publisher":"Springer Verlag","rawRecordXml":"<record><header><identifier>\n        \n            \n                oai:lra.le.ac.uk:2381\/8663<\/identifier><datestamp>\n                2015-12-08T14:36:06Z<\/datestamp><setSpec>\n                com_2381_322<\/setSpec><setSpec>\n                com_2381_9551<\/setSpec><setSpec>\n                col_2381_512<\/setSpec>\n            <\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:doc=\"http:\/\/www.lyncode.com\/xoai\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n            \nMeasuring fidelity to extreme programming: a psychometric approach<\/dc:title><dc:creator>\nMichaelides, George<\/dc:creator><dc:creator>\nThomson, Chris<\/dc:creator><dc:creator>\nWood, Stephen<\/dc:creator><dc:description>\nThis is the author's final draft of the paper published as Empirical Software Engineering, 2010, 15 (6), pp. 599-617.  The original publication is available at www.springerlink.com.  Doi: 10.1007\/s10664-010-9130-z<\/dc:description><dc:description>\nThis study assesses the Shodan survey as an instrument for measuring an individual\u2019s or a team\u2019s adherence to the extreme programming (XP) methodology. Specifically, we hypothesize that the adherence to the XP methodology is not a uni-dimensional construct as presented by the Shodan survey but a multidimensional one reflecting dimensions that are theoretically grounded in the XP literature. Using data from software engineers in the University of Sheffield\u2019s Software Engineering Observatory, two different models were thus tested and compared using confirmatory factor analysis: a uni-dimensional model and a four-dimensional model. We also present an exploratory analysis of how these four dimensions affect students\u2019 grades. The results indicate that the four-dimensional model fits the data better than the uni-dimensional one. Nevertheless, the analysis also uncovered flaws with the Shodan survey in terms of the reliability of the different dimensions. The exploratory analysis revealed that some of the XP dimensions had linear or curvilinear relationship with grades. Through validating the four-dimensional model of the Shodan survey this study highlights how psychometric techniques can be used to develop software engineering metrics of fidelity to agile or other software engineering methods.<\/dc:description><dc:date>\n2010-10-25T15:02:48Z<\/dc:date><dc:date>\n2010-10-25T15:02:48Z<\/dc:date><dc:date>\n2010-12<\/dc:date><dc:type>\nArticle<\/dc:type><dc:identifier>\nEmpirical Software Engineering, 2010, 15 (6), pp. 599-617.<\/dc:identifier><dc:identifier>\n1382-3256<\/dc:identifier><dc:identifier>\nhttp:\/\/link.springer.com\/article\/10.1007%2Fs10664-010-9130-z<\/dc:identifier><dc:identifier>\nhttp:\/\/hdl.handle.net\/2381\/8663<\/dc:identifier><dc:identifier>\n10.1007\/s10664-010-9130-z<\/dc:identifier><dc:language>\nen<\/dc:language><dc:publisher>\nSpringer Verlag<\/dc:publisher>\n<\/oai_dc:dc>\n<\/metadata>\n        <\/record>","journals":[{"title":null,"identifiers":["issn:1382-3256","1382-3256"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2010,"topics":[],"subject":["Article"],"fullText":" 1 \nMeasuring Fidelity to Extreme Programming: A Psychometric Approach \n \nAbstract \n \nThis study assesses the Shodan survey as an instrument for measuring an individual\u2019s or a \nteam\u2019s adherence to the extreme programming (XP) methodology. Specifically, we \nhypothesize that the adherence to the XP methodology is not a uni-dimensional construct as \npresented by the Shodan survey but a multidimensional one reflecting dimensions that are \ntheoretically grounded in the XP literature. Using data from software engineers in the \nUniversity of Sheffield\u2019s Software Engineering Observatory, two different models were thus \ntested and compared using confirmatory factor analysis: a uni-dimensional model and a four-\ndimensional model. We also present an exploratory analysis of how these four dimensions \naffect students\u2019 grades. The results indicate that the four-dimensional model fits the data \nbetter than the uni-dimensional one. Nevertheless, the analysis also uncovered flaws with the \nShodan survey in terms of the reliability of the different dimensions. The exploratory analysis \nrevealed that some of the XP dimensions had linear or curvilinear relationship with grades. \nThrough validating the four-dimensional model of the Shodan survey this study highlights \nhow psychometric techniques can be used to develop software engineering metrics of fidelity \nto agile or other software engineering methods.   \n \n \nKeywords: psychometrics, confirmatory factor analysis, Extreme Programming (XP), fidelity, \nadherence, Shodan survey  \n \n \n \n1. Introduction \nWhen undertaking studies of software engineering methods it is important to assess the \nfidelity to which the teams follow the method. Often developers do not apply the method \ncorrectly or consistently over a long period of time and this may have detrimental effects on \nperformance. On the other hand, excessively enforcing adoption can result in frustration \namong developers. These issues are more pronounced in development approaches that span \nthe main software development lifecycle, often called methodologies. To aid our \nunderstanding of how teams adopt particular methods and whether the level of adoption \naffects the outcomes of the team we need to develop instruments for quantifying fidelity to \nsoftware engineering methodologies. Such instruments would allow (a) organizations to be \nable to assess their own teams, and (b) researchers in empirical software engineering to \nmeasure and statistically control adoption of the methodology in consistent and comparative \nways. Without these instruments we are unable to make hard conclusions about the relative \nmerits of a method; poor performing teams could simply be those that did not follow it \ncomprehensively, and equally the best performing teams could be the ones that constructively \nadapt the methodology to meet the circumstances or discard elements of it that they find do \nnot work. This paper will focus on the development of an instrument for measuring the \n 2 \nadherence to one methodology, Extreme Programming (XP). More specifically, we build on \nthe adherence metrics section of the XP evaluation framework of Williams et al. (2004a, \n2004b), the most comprehensive method available. The basis of these metrics is the Shodan \nquestionnaire (see Krebs 2002), and we will employ a psychometric approach to evaluate \nthem and develop a four-dimensional rather than uni-dimensional measurement model.  \n1.1. Approaches to XP adherence \nMany studies of XP seem to focus only on specific aspects of it that are directly related to the \nresearch question at hand. We found 27 papers that contained a study of XP. Of these, three \ndid not discuss the way in which XP was applied other than by referencing the standard \nmethod (Mannaro et al. 2004; Noll and Atkinson 2003; Young et al. 2005). Three further \npapers used a qualitative method that briefly described the application (Jokela and \nAbrahamsson 2004; Merisalo-Rantanen et al. 2005; Moser et al. 2007), ten  papers discussed \nsome but not all of the practices used with little or no information on fidelity (Bahli and Zeid \n2005; Koskela and Abrahamsson 2004; Mackenzie and Monk 2004; Martin et al. 2004a, \n2004b; M\u00fcller and Tichy 2001; Newkirk and Martin 2000; Robinson and Sharp 2004, 2005b; \nTessem 2003) and three had significant information about fidelity (Cao et al. 2004; Chong \n2005; Fruhling et al. 2005; Robinson and Sharp 2005a; Sharp and Robinson 2004). In terms \nof quantitative studies, two papers used metrics to assess seven of the XP practices \n(Abrahamsson 2003; Abrahamsson and Koskela 2004).  \nTwo papers assessed fidelity of the complete set of XP practices. Gittins and Hope \n(2001) used a qualitative approach to explore each of the practices of XP systematically. In \ntheir approach, the authors described how each of the practices had been adopted by the \nteams. Sfetsos et al. (2006) used a combination of quantitative and qualitative methods. They \nused a semi-structured interview and questionnaire to systematically collect data from several \ncompanies. The questionnaire has a single question for each practice that had a three point \nscale to indicate that it was partially, fully or not used. The interviews were used to enhance \nthis data to explain why it was adopted. In both studies the results were presented in terms of \nhow each of the practices had been adopted with no attempt to describe the overall fidelity to \nXP. \nFinally, Williams et al. (2004a, 2004b) developed the XP evaluation framework. \nSome examples of the application of this framework include case studies by Layman et al. \n(2004) and Layman (2004). In this framework a questionnaire instrument, often referred to as \nthe Shodan survey, was developed to form quantitative assessments of adherence to XP. This \nis the instrument that we will be examining in this paper.  \n1.2. The Shodan survey \nThe XP evaluation framework presented by Williams et al. (2004a, 2004b; see also Krebs \n2002) contains a number of components: context factors, adherence metrics and outcome \n 3 \nmeasures. Of these, the adherence metrics are used to measure fidelity to XP, through \nobjective measures, interview techniques and the Shodan survey. In these metrics, a set of \nweights is proposed to combine the scores from the questions to give a single value that \ncorresponds to fidelity. These weights were obtained though analysis of the relatedness of the \npractices as presented by Beck and Andres (2004).  \nQuestions from the Shodan survey are grouped in six categories: foundations, \ncustomer planning, teaming, craftsmanship, introspection and perspectives. For the purposes \nof this paper we only focus on the first four. The first category, foundations, concentrates on \ntesting and pair programming. Customer planning concerns the planning game, customer \naccess, short releases and stand-up meetings, whereas the teaming category addresses issues \nrelated to collective code ownership, coding standards and continuous integration. The fourth \ncategory, craftsmanship, is concerned with sustainable pace, simple design and the use of \nmetaphor. Introspection is not discussed in this paper because it is based on the assumption \nthat teams have worked together on more than one project, which the teams in our study had \nnot. While it is often the case that the team composition changes from project to project, this \nis not an inherent characteristic of agile methodologies. The Shodan survey\u2019s final category, \nperspectives, assesses which practices participants felt were the most threatening or \npromising. Again, this is not central to an evaluation of XP.  \nThere are a number of advantages to the Shodan survey and to using a questionnaire-\nbased methodology in general to measure adherence to XP:  \n(a) It provides overall adherence scores instead of relying on individual practices. \n(b) It is quantitative and thus can be used in both quantitative and qualitative \nstudies (whilst qualitative approaches are not useful to quantitative studies). \n(c) It is easily applicable in organizations. \n(d) Results from different organizations or research contexts should be \ncomparable.  \nNonetheless, there are a number of issues that need to be addressed if this \nmethodology is to be developed into a robust measurement instrument. First, while Williams \net al. (2004a; 2004b) group their questionnaire items into theoretical categories, there is an \nunderlying assumption in the way they apply the method that adherence to XP can be \nmeasured as one dimension. It is important to evaluate whether the uni-dimensional model is \nadequate for capturing adherence or whether the categories used in the survey questions could \nreflect different dimensions of adherence to XP. Second, the instrument has not been \nvalidated. Thus as it stands we do not know whether the instrument measures what its \narchitects claim it measures. Finally, Williams et al. apply weights to the various survey items \naccording to the importance of the various XP practices, but there is no evidence that these \nweights correctly capture the relative contribution of each to the overall approach. It is \n 4 \npossible that weighting and aggregating the results of the survey questions in this way creates \nan invalid measure or underutilizes important information.  \nGiven it may well be, then, that a theoretically driven four-dimensional model could \nmore accurately measure adherence to XP, we now present a test of this using psychometric \nmethods. Having identified dimensions of adherence we then validate and assess whether \nindividual items need to be weighted differently to the original Shodan model.  \n1.3. Alternative psychometric approach \nPsychometrics refers to the study of psychological measurement and it is concerned with the \ndevelopment and analysis of psychological tests, questionnaires and related instruments, in \norder to develop valid and reliable measures. One of the most frequently used techniques for \ntesting measurement models, which will be employed in this paper, is latent or factor analysis. \nFactor analysis techniques are typically used to evaluate whether or not a common factor or \nlatent variable underlies a set of survey items or observed variables (Bartholomew and Knott \n1999). Specifically, these techniques aim to describe variability in the observed variables in \nterms of a smaller set of latent factors. Thus it is possible to assess whether one dimension can \nreflect the variability in the Shodan items or whether a multidimensional measure would be \nmore appropriate.  \nBesides the need for data reduction and the evaluation of competing measurement \nmodels, factor analysis techniques can provide evidence for construct validity. The underlying \nlogic behind using factor analysis to assess construct validity is that the latent construct is the \nreason that individuals respond to a set of items in specific ways. If the measurement model is \nvalid then adherence is an underlying property of the software engineering process, which \nwill result in individuals responding to the survey items in specific patterns. Through those \npatterns we can evaluate whether or not our latent construct or constructs are valid. In the case \nof the Shodan survey there are two main measurement models for which the construct validity \nshould be evaluated: the uni-dimensional instrument and the four-dimensional instrument \nbased on the four core categories in the survey.  \nFurthermore, the loadings obtained from a factor analysis signify the contribution of \neach of the items to the corresponding dimension or dimensions. In effect the factor loadings \nor standardized coefficients for the items are weights for calculating the latent factors. When \nthe aim of the analysis is to maximize the variance explained by the underlying factors, \nexploratory factor analysis is better suited for obtaining the factor loadings. However, since a \nprior theoretical model already exists, as derived from the categorization of the questionnaire \nitems, it is likely to be more useful to test that model rather than create another classification \nor set of dimensions. In addition, through confirmatory factor analysis each of the items loads \nonly to one factor, whilst in exploratory factor analysis each item has typically a high loading \nin one of the identified dimensions and weaker loadings in others. Thus, for the purposes of \n 5 \nidentifying the appropriate loadings for measuring fidelity to XP, a confirmatory rather than \nan exploratory approach is preferable. \nIn our study we will first test the uni-dimensional and four-dimensional models using \nconfirmatory factor analyses, then derive different weights for each of the questions from that \nanalysis, and finally compare the two models. These three steps are framed in the following \nhypothesis: \nH1: A four-dimensional measurement model will have better fit than a uni-\ndimensional model for measuring adherence to the XP methodology. \nIn addition to evaluating a measurement model using these techniques this study will \nalso present an exploratory analysis of the four dimensions in terms of how they relate to \nstudents\u2019 performance as assessed through their grades. The obvious assumption of the utility \nof an instrument that measures adherence to XP is that teams following XP practices most \nclosely will be better performers than those who adopt the same practices to a lesser extent \n(Stephens and Rosenberg 2003). For instance, case studies of software development showed \nthat (a) XP teams deliver above average quality and productivity and (b) that adoption of XP \nresults in improvements in quality and productivity when compared to performance of the \nsame team prior to adoption (Layman et al. 2004, 2006). Previous results have also shown \nthat student teams using XP deliver marginally higher quality results than those using a plan-\ndriven approach (Macias 2004) and that there is a positive relationship between quality and \nthe number of practices used (Syed-Abdullah 2005).  \nTaking this evidence from past research suggests that valid measures should be able \nto explain more variability in students\u2019 grades than less valid measures. This leads to the \nsecond hypothesis of this paper: \nH2: The four-dimensional measurement model will explain more variability in \nstudents\u2019 grades than the uni-dimensional model.  \n2. Methods \nThe Shodan questionnaire was administered to software engineering students working in \nteams that were following the XP methodology (Thomson and Holcombe 2009). \n2.1. Measures \nThe questionnaire administered consisted of 13 questions on the topics that are presented in \nTable 1 below.  \n[Insert Table 1 about here]  \nIn the original Shodan survey there were two additional questions that were omitted \nhere. The first was about the planning game and was part of the customer planning category. \nThe second was about the use of a metaphor and was part of the craftsmanship category. Both \nquestions required a binary answer rather than a continuous scale. There were a large number \n 6 \nof missing cases for these two variables and since they were considered to overlap with other \nquestions, we excluded them from our analysis. An 11 point scale from 0\u2013100% was used for \nthe included questions. \nFor exploring the second hypothesis, the dependent variable was the grades awarded \nto each of the students at the end of the year for their software engineering project. These \nwere independently awarded to students and thus overall team performance did not determine \nthe individual grades. The grade was based on the average of two independent assessments by \ntwo different assessors using the same guidelines and criteria, and thus the results reflect the \nstudents\u2019 experience, knowledge and expertise in a consistent way. If the two assessments \ndiffered by more than five points out of 100, the assessors would discuss the discrepancy in \norder to arrive at a consensus. Although these grades are not necessarily the best way to \nassess individual performance in software engineering we would expect them to correlate \nhighly with other software engineering metrics.  \n2.2. Sample \nParticipants consisted of first year, second year and Master\u2019s students, who had varying \nexperience and expertise with the XP methodology and programming in general. The first \nyear students were working on projects assigned by the University, while the second year and \nMaster\u2019s students were working on real software engineering projects that were assigned from \nclients to be used in their respective business.  \nSample variability in terms of students\u2019 experience is useful when testing \nmeasurement models because it should permit greater generalizability of the findings and \nensures a good spread in the data. Indeed, for the purposes of this study having greater \nvariability in terms of experience in applying XP is a virtue rather than a limitation of the \nsample.  \nThe questionnaire was given to 289 students, and completed by 187. Of those, only \n138 completed all of the questions, which formed the sample used for the confirmatory factor \nanalysis. To assess whether the missing values were systematic an ANOVA test was \nperformed on the 13 items of the 187 returned questionnaires (138 students who had no \nmissing cases, and the remaining 49 who had some missing values) and on students\u2019 \nperformance. There were no significant differences for any of the 13 items tested or the \nstudents\u2019 performance scores. The missing values were therefore not systematic and thus \ncases with missing values were removed from the analysis.  \nFor the exploratory analysis of students\u2019 grades, the Shodan scores were estimated for \nthe 187 students and the missing cases were excluded pair-wise. A total of eight students did \nnot answer all the questions that comprised one of the four latent factors and they were \nremoved from the analysis. Out of the remaining sample there were 29 cases with missing \nperformance data reducing the sample to a total of 150 valid cases.  \n 7 \nTo examine whether the sample size would be adequate for the predictive validity \ntests we estimated the effect size that would be required for the various models in order to \nachieve a power of 0.8 and a significance level of 0.05. The tests were performed using \nCohen\u2019s (1988) tests. For the uni-dimensional model the required effect size was f2 =.053, \nwhich suggests that variance explained by the model should be more than 5% (R\n2\n=0.05). For \nthe four-dimensional model the effect size was estimated at f\n2\n =.082, thus requiring at least \n7.6% (R\n2\n=0.076) of the variance to be explained in order to achieve the required power. \nHence, the sample can be considered adequate for both analyses.  \n2.3. Procedure \nQuestionnaires were administered to all first year, second year and Master\u2019s students at the \nDepartment of Computer Science at the University of Sheffield. This was done for two \nconsecutive years for the first and second year students, and three consecutive years for the \nMaster\u2019s students. All questionnaires were administered at the end of the students\u2019 projects to \nensure that the students\u2019 responses reflected the adopted XP practices throughout the project. \nStudents were advised that their participation in the study was voluntary and they could \nwithdraw from the study at any point. The performance data were collected after the students \nwere awarded their grades at the end of the academic year. To ensure anonymity, the data \nwere collated using an ID number assigned to each participant.  \n3. Results \n3.1. Confirmatory factor analysis \nThe uni-dimensional and four-dimensional models were tested through a confirmatory factor \nanalysis approach using the Mplus v4.21 package. Maximum likelihood estimation was used \nto analyse the covariances for the 13 questionnaire items. The covariance matrix is shown in \nTable 2 below. \n[Insert Table 2 about here] \nFor the uni-dimensional model, the unstandardized coefficient estimates were all \nsignificant except for two of the items (items 8 and 13), which were not statistically \nsignificant (p<.05). The model coefficients can be found in Table 3 below. The Cronbach\u2019s \nalpha reliability coefficient (Cronbach 1951; Nunnaly and Bernstein 1994) was satisfactory \nfor the uni-dimension, with \u03b1=0.83. A comparison between the unstandardized coefficients \n(factor loadings) and the weights of the original survey (see Table 1) highlights that the \noriginal weights did not present the best solution in terms of the covariance of the items.  \n[Insert Table 3 about here] \nThe coefficients obtained indicate that the first three questions appear to dominate the \nanalysis with the rest of the items having small coefficients. Nevertheless, the fact that the \nfirst three questions are primarily testing questions does not imply that testing was \noveremphasized in the courses from which we drew the sample. A strong relationship \n 8 \nbetween the testing questions should still manifest regardless of how high or low teams \nscored. Figure 1 below shows the uni-dimensional model and the standardized coefficients. \n[Insert Figure 1 about here] \nOverall fit of the models were assessed using the \uf0632 test, the Comparative Fit Index \n(CFI), the Tucker\u2013Lewis Fit index (TFI), the Root Mean Square Error of Approximation \n(RMSEA) and the Standardized Root Mean Square residual (SRMS) indices. The \uf0632 test \nevaluates whether the predicted and observed covariance matrices are different. A good fit is \ninferred when the two matrices are not different; a significant difference implies a poor fit. \nHowever \uf0632 tests should be interpreted with caution. The \uf0632 estimates can increase when there \nis a large sample size or high correlations among the variables resulting in significant test \neven when there is a good model fit. Using the CFI and TFI measures, a good model is \nindicated by higher values. Above 0.90 is generally considered an indication of an acceptable \nmodel and above 0.95 indicates an excellent model. For RMSEA, a good fit is indicated by a \nsmall value. In general, models with less than 0.10 are considered acceptable, whilst models \nwith less than 0.05 are considered to have a very good fit. The SRMS index is the \nstandardized difference between the observed and predicted covariance. A zero value would \nindicate a perfect fit, whilst the higher the value the poorer the fit of the model. A value less \nthan 0.08 is considered a good fit. In all tests the uni-dimensional model shows a weak fit. \nTable 4 below shows various fit statistics for the uni-dimensional and four-dimensional \nmodels. \n[Insert Table 4 about here] \nFor the four-dimensional model all of the unstandardized coefficient estimates were \nstatistically significant (see Table 5). Overall, the coefficients indicate a considerable \nimprovement over those for the single-factor model. In terms of reliability, Cronbach\u2019s alphas \nfor the hypothesized model were \u03b1=0.83 for foundation, \u03b1=0.48 for customer planning, \n\u03b1=0.64 for teaming, and \u03b1=0.41 for craftsmanship. Although the foundations and teaming \nfactors achieved a high reliability coefficient, customer planning and the craftsmanship are \nquite low.  \n[Insert Table 5 about here] \nThe factor loadings (unstandardized coefficients) are again different from the original \nShodan weights (see Table 1) but are also different from those obtained from the analysis of \nthe uni-dimensional model. Nevertheless, the weights for the first five questions are high and \nsimilar in both the uni- and four-dimensional models. This suggests that the latent factor in \nthe first model reflected more of the foundations of XP but failed to adequately capture the \nother three categories. Figure 2 shows the four-dimensional model with the standardized \ncoefficients. \n 9 \n[Insert Figure 2 about here] \nFor assessing the overall goodness of fit for the model, the \uf0632 test is significant, \nshowing that the observed covariance matrices are significantly different from the predicted \ncovariance matrices. This indicates a not so good fit for the model. As explained above, \nthough, the \uf0632 test can often be unreliable and the overall fit should be evaluated using a \nnumber of indices. Using CFI, TFI, RMSEA, and SRMS (see Table 4), the model shows a \nvery good fit. \nFinally, a comparison between the uni-dimensional and the four-dimensional models \nwas performed using the chi-square differences between the two models. The chi-square \ndifferences between two nested models follows a chi-square distribution with degrees of \nfreedom equal to the difference of the degrees of freedom of the two models. If the two \nmodels are significantly different, the model with the smaller chi-square is significantly better \nthan the first. The comparison indicated that the four-dimensional model was significantly \nbetter than the uni-dimensional model (\uf044\uf0632 =73.2, \uf044df =6, p<.001). Comparisons of all other \nfit indices corroborate this result. These results provide strong support for the first hypothesis \n(H1) indicating that the four-dimensional model can better explain the variability in the \nShodan questionnaire items, whilst at the same time providing evidence about its construct \nvalidity.  \n3.2. Exploratory analysis of adherence and students\u2019 grades \nLinear regression models were used to assess the exploratory analysis for the two models \n(H2). The models were tested with R 2.8.0 (R Development Core Team 2008). In all models \nthe dependent variable was students\u2019 performance, as assessed by their lecturers on a 0\u2013100 \nscale.  \nFor the uni-dimensional model the independent variable was the Shodan adherence \nscore, calculated using its factor loadings (unstandardized coefficients). This model failed to \nachieve any significant results \u2013 F(1,148)=0.001, p>.05 \u2013 and accounted for only 0.001% of \nthe variance. For the four-dimensional model, the independent variables were the four latent \nvariables identified. These were estimated by weighting each of the items using the \nunstandardized coefficients obtained from the measurement model above. \nContrary to the uni-dimensional model, the four-dimensional model was very \nsignificant \u2013 F(4,145)=11.87, p<.001 \u2013 and explained 24.7% of the variance. Out of the four \npredictors, foundations, teaming and craftsmanship were significant and only customer \nplanning was not. However, the variable foundations had a negative rather than a positive \neffect on performance. Further analysis through scatterplots of the data revealed that there \ncould be curvilinear effects. These were also tested using second-order polynomial linear \nregression. For reasons of consistency and fairness we tested for the quadratic effects in both \nthe uni-dimensional and four-dimensional models. In order to reduce the possibility for \n 10 \ncollinearity between linear and quadratic predictors, the latter were centred on their means \nbefore they were raised to the second power. The results from all models are presented in \nTable 6 below.  \n[Insert Table 6 about here] \nFor the uni-dimensional quadratic model the results indicate that there is a modest \neffect (R\n2\n=.07), which is significant (F(2,147)=5.53, p<.01). Nevertheless, an ANOVA \ncomparison between the linear and quadratic models reveals that their difference is \nstatistically significant (\uf044F=11.06, \uf044R2=.07, p<.05). The t-tests for the predictor indicate that \nonly the quadratic term and not the linear was significant, which had a negative \uf062 coefficient. \nThe negative coefficient here indicates an inverted U-shaped relationship with the dependent \nvariable. \nFor the four-dimensional model the results were further improved after addition of \nquadratic predictors for foundations and craftsmanship. With the exception of customer \nplanning, all linear and curvilinear terms added to the regression equation were significant. \nOverall the model explained 30.9% of the variance in performance and was statistically \nsignificant (F(6,143)=10.66, p<.001). The model also provided significant improvement over \nthe linear model (F=6.46, \uf044R2=.062, p<.01). For the two predictors that had significant \nquadratic effects, foundations had a negative coefficient whilst craftsmanship\u2019s was positive, \nindicating an inverted U type of relationship for the former and a U-shaped relationship for \nthe latter (see Figure 3).  \n[Insert Figure 3 about here] \nA comparison between the quadratic uni-dimensional and the quadratic four-\ndimensional models indicated that the latter was significantly better (F=12.38, p<.001) \nexplaining 23.9% more variance than the quadratic linear model. Overall, the results indicate \nthat the four-dimensional measurement model can explain more variance in students\u2019 grades \nand thus provide strong evidence for the second hypothesis of this paper.  \n4. Discussion \nThis study examined the measurement model of the Shodan survey for quantifying the degree \nof adherence to the XP methodology. First, a uni-dimensional model does not explain the \nvariability in the Shodan items and thus aggregating them in one variable does not constitute a \nvalid measure of adherence to XP. Second, the four-dimensional model provides a far more \naccurate way of measuring adherence to the XP methodology. Third, the loadings used in the \noriginal Shodan survey are very different from those obtained empirically in our analysis.  \nAlthough the analysis showed better support for the four-dimensional model, the \nresults should be approached with caution. The Cronbach\u2019s alpha scores were not that strong \nfor three of the four factors, with the exception of the foundations factor. This indicates that \n 11 \nthere is relatively low internal consistency in the items making up these three factors. Thus, \nusing the Shodan survey at its current state could be problematic: on the one hand a four-\nfactor solution is more appropriate, but on the other hand the items have weak internal \nconsistency and thus do not adequately describe three of the four factors.  \nThe results from the exploratory analysis showed that the four-dimensional \nmeasurement model could explain more variation in students\u2019 grades than the uni-\ndimensional model. It is interesting that quadratic effects exist for both the original Shodan as \nwell as for the measurement model identified here. For the original uni-dimensional model, \nthe results suggest that overall there is a small curvilinear effect of adherence to the XP \nmethodology on grades. Specifically, it was found that there is a positive effect up to medium \nlevels of adherence and a negative effect for individuals scoring higher on the adherence \nscale. The nature of the relationships in the four-dimensional model indicated two curvilinear \neffects and one linear effect. The linear effect was between teaming and grades whilst the \ncurvilinear were between (a) foundations and (b) craftsmanship and grades. For the \nfoundations the relationship was similar to the one identified for the original Shodan scale. In \ncontrast, the effect of craftsmanship was in the opposite direction, indicating that applying \ncraftsmanship to a small extent can have a detrimental effect and only when the practices are \nmore closely adopted do they tend to have a positive effect on grades. Customer planning did \nnot appear to have any effect on grades.  \nTaken together these results show that the XP methodology is in fact \nmultidimensional and thus it should be treated as such both theoretically and empirically in \nterms of measurement. For researchers, the psychometric analysis of the Shodan survey \nprovides a step forward in creating instruments that can accurately capture the degree to \nwhich XP has been adopted. This paves the way for potential research into explaining \nindividual or team performance as well as product quality. Furthermore, we believe that \npsychometric techniques are invaluable for studies that are developing instruments for \nquantifying the degree of adherence of software engineering methods.  \nFor practitioners using Shodan to assess the adoption of XP by individuals or teams in \ntheir organization, using a four-dimensional instrument provides greater flexibility and better \nresults in terms of the adoption. Such metrics as these are invaluable for identifying potential \nproblems in the development process as well as any effects that their adoption may have on \nthe quality of the product.  \n4.1. Future research \nThere are a number of ways that this study could be taken forward. First, a bigger \nsample from industry would help to establish how general the results are. A sample including \nsome teams that have been working together for a long period of time and on more than one \nproject would also allow the inclusion of questions on introspection (the fifth possible \n 12 \ndimension) that were excluded here. Second, the XP evaluation framework (Williams et al. \n2004a) relies on data collection from other techniques (e.g. interviews) and thus we cannot \nsee the complete picture with the questions used here. Effective communication, for example, \nwhich is an important element of the XP methodology but not a practice, is not measured by \nthe questionnaire. Future research could refine the questionnaire to include such elements. In \naddition, the issue of the small reliability coefficients can be addressed through the inclusion \nof more items per latent factor or a different conceptualization of the dimensions of adherence \nto XP. Ideally, future research should focus on a measurement model and questionnaire items \nspecifically developed with the assumptions of measurement underlying factor analysis and \nrelated techniques. \nThird, whether these measures truly operationalize the theorized dimensions should \nbe given consideration. Following the different factor loadings it can be argued that there is \nincongruence between the labels of the hypothesized dimensions and the questionnaire items. \nFoundations, for instance, was reflected in the first three questions which refer to testing. \nSimilarly, customer planning seems to have more to do with planning in general rather than \nthe customer, and the items of the third factor, teaming, appear to relate more to team code \nmanagement. Finally, craftsmanship was primarily reflected in the question about sustainable \npace, and simple design had only a small loading. Future research should consider whether \nrenaming these factors would give a better perspective of the factors representing adoption of \nXP practices or whether the original dimensions should be operationalized with different \nitems that more accurately capture their essence.  \nFourth, future research should expand to other methodologies by identifying possible \ndimensions that span methodologies, ideally with the aim to create a generic way of assessing \nfidelity that can be applied to all methods. This will enable more refinement in the \ncomparisons of approaches, but also allows for the fact that practices that are not highlighted \nas important in some methodologies may still be used when following others. For instance, \nalthough teamwork and communication are core elements of agile methods, they are not \nunimportant in traditional waterfall methods. Equally, XP teams could adopt practices from \nother methods that aid their performance. Using the techniques applied here would make \npossible the development of a generic method using the best elements and practices from \nvarious different methodologies.  \nFinally, there is a need to evaluate the predictive validity of this instrument (or an \nupdated version of it with more reliable items), particularly using proper software engineering \nperformance metrics or expert evaluations of fidelity to XP. In addition, although we tend to \nthink about adherence at the team level, we were only able to assess relationships between \nagile methods and individual grades. Future research should thus address this issue through \ncollecting data from a bigger population. Subject to within-group reliability (James et al. \n 13 \n1984), analysis could be done by either aggregating individual scores to the team level or by \nusing multilevel confirmatory factor analysis (Muth\u00e9n 1994). From this perspective, data \nfrom diverse teams from different organizations would be invaluable in developing a model \nthat would be applicable to XP teams operating within different organizational contexts.  \n4.2. Conclusions \nIt is apparent from the results of this study that the four-dimensional measurement model of \nthe Shodan survey is a better conceptualization of adherence to XP. However, it is \nrecommended that more research is needed both in process conformance as well as in terms \nof adherence questionnaires. The psychometric approach adopted here can potentially be used \nto devise more complex, accurate and interesting measures. Such measures can enable \nquantitative research with more statistical rather than experimental control over the adopted \nprocesses. The results also uncovered nonlinear relationships between adoption of XP and \nperformance that have not been conceptualized or theorized before.  \n \n \n \nAcknowledgements \nWe will leave out for now, as may aid identification of authors or their affiliation. \n \n 14 \n \nReferences \nAbrahamsson P (2003) Extreme programming: First results from a controlled case study. \nEuromicro Conference 2003. Proceedings. 29\nth\n: 259\u2013266. \nAbrahamsson P, Koskela J (2004) Extreme programming: A survey of empirical data from a \ncontrolled case study. Proceedings 2004 International Symposium on Empirical Software \nEngineering: 73\u201382. \nBahli B, Zeid ESA (2005) The role of knowledge creation in adopting extreme programming \nmodel: An empirical study. ITI 3rd International Conference on Information and \nCommunications Technology 2005: Enabling Technologies for the New Knowledge \nSociety. \nBartholomew DJ, Knott M (1999) Latent variable models and factor analysis. Oxford \nUniversity Press, New York. \nBeck K, Andres C (2004) Extreme programming explained: Embrace change (2nd Edn). \nAddison-Wesley Professional, Boston. \nCao L, Mohan K, Xu P, Ramesh B (2004) How extreme does extreme programming have to \nbe? Adapting XP practices to large-scale projects. Proceedings of the 37th Annual Hawaii \nInternational Conference on System Sciences 2004. \nChong J (2005) Social behaviors on XP and non-XP teams: A comparative study. In Proc. \nAgile United Conference. \nCohen J (1988) Statistical power analysis for the behavioral sciences. Lawrence Erlbaum \nAssociates, New York. \nCronbach LJ (1951). Coefficient alpha and the internal structure of tests. Psychometrika, 16: \n297\u2013334. \nFruhling A, Tyser L, De Vreede GJ (2005) Experiences with extreme programming in \ntelehealth: Developing and implementing a biosecurity health care application. HICSS '05. \nProceedings of the 38th Annual Hawaii International Conference on System Sciences. \nGittings R, Hope S (2001) A study of human solutions in extreme programming. Proc. 13\nth\n \nWorkshop of the Psychology of Programming Group: 41\u201351. \nJames LR, Demaree RG, Wolf G (1984) Estimating within-group interrater reliability with \nand without response bias. Journal of Applied Psychology, 69: 85\u201398. \nJokela T, Abrahamsson P (2004) Usability assessment of an extreme programming project: \nClose co-operation with the customer does not equal to good usability. Product Focused \nSoftware Process Improvement: 397\u2013407. \nKoskela J, Abrahamsson P (2004) On-site customer in an XP project: Empirical results from a \ncase study. Software Process Improvement, 3281: 1\u201311. \n 15 \nKrebs W (2002) Turning the knobs: A coaching pattern for XP through agile metrics. \nPresented at Extreme Programming\/Agile Universe, Chicago IL: 60\u201369. \nLayman L (2004) Empirical investigation of the impact of extreme programming practices on \nsoftware projects. OOPSLA '04: Companion to the 19th annual ACM SIGPLAN \nconference on Object-oriented programming systems, languages, and applications. \nVancouver, Canada, ACM. \nLayman L, Williams L, Cunningham L (2004) Exploring extreme programming in context: an \nindustrial case study. Agile Development Conference 2004. \nLayman L, Williams L, Cunningham L (2006) Motivations and measurements in an agile case \nstudy. Journal of Systems Architecture 52(11): 654\u2013667. \nMacias F (2004) Empirical assessment of extreme programming. Unpublished PhD thesis, \nUniversity Of Sheffield.  \nMackenzie A, Monk S (2004) From cards to code: How extreme programming re-embodies \nprogramming as a collective practice. Computer Supported Cooperative Work (CSCW), \n13: 91\u2013117. \nMannaro K, Melis M, Marchesi M (2004) Empirical analysis on the satisfaction of it \nemployees comparing XP practices with other software development methodologies. \nExtreme Programming and Agile Processes in Software Engineering, Lecture Notes in \nComputer Science, 3092\/2004: 166-174, Springer Berlin \/ Heidelberg. \nMartin A, Biddle R, Noble J (2004a) When XP met outsourcing. Extreme Programming and \nAgile Processes in Software Engineering, Lecture Notes in Computer Science, 3092\/2004: \n51-59, Springer Berlin \/ Heidelberg. \nMartin A, Biddle R, Noble J (2004b) The XP customer role in practice: three studies. Agile \nDevelopment Conference 2004. \nMerisalo-Rantanen H, Tuure T, Matti R (2005) Is extreme programming just old wine in new \nbottles: A comparison of two cases. Journal of Database Management, 16: 41\u201361. \nMoser R, Scotto M, Sillitti A, Succi G (2007) Does XP deliver quality and maintainable \ncode? Agile Processes in Software Engineering and Extreme Programming, Lecture Notes \nin Computer Science, 4536\/2007: 105-114, Springer Berlin \/ Heidelberg. \nMuller M, Tichy W (2001) Case study: Extreme programming in a university environment. \nICSE '01: Proceedings of the 23\nrd\n International Conference on Software Engineering. \nToronto, Canada, IEEE Computer Society. \nMuth\u00e9n BO (1994) Multilevel covariance structure analysis. Sociological Methods & \nResearch, 22: 376\u2013398.  \nNewkirk J, Martin R (2000) Extreme programming in practice. OOPSLA '00: Addendum to \nthe 2000 proceedings of the conference on Object-oriented programming, systems, \nlanguages, and applications (Addendum). Minneapolis, ACM. \n 16 \nNoll J, Atkinson D (2003) Comparing extreme programming to traditional development for \nstudent projects: A case study. Extreme Programming and Agile Processes in Software \nEngineering, Lecture Notes in Computer Science, 2675\/2003: 1013, Springer Berlin \/ \nHeidelberg. \nNunnaly JC, Bernstein IH (1994) Psychometric theory (3rd edn). McGraw-Hill, New York.  \nR Development Core Team (2008) R: A language and environment for statistical computing. \nSeries in Psychology. R Foundation for Statistical Computing, Vienna, Austria.  \nRobinson H, Sharp H (2004) The characteristics of XP teams. Extreme Programming and \nAgile Processes in Software Engineering, Lecture Notes in Computer Science, 3092\/2004: \n139-147, Springer Berlin \/ Heidelberg. \nRobinson H, Sharp H (2005a) Organisational culture and XP: Three case studies. Proceedings \nAgile Conference 2005. \nRobinson H, Sharp H (2005b) The social side of technical practices. Extreme Programming \nand Agile Processes in Software Engineering, Lecture Notes in Computer Science, \n3556\/2005: 100-108, Springer Berlin \/ Heidelberg.Sfetsos P, Angelis L, Stalmenos I \n(2006) Investigating the extreme programming system: An empirical study. Empirical \nSoftware Engineering, 11: 269\u2013301. \nSharp H, Robinson H (2004) An ethnographic study of XP practice. Empirical Software \nEngineering 9: 353\u2013375. \nStephens M, Rosenberg D (2003) Extreme programming refactored: The case against XP. \nAPress, Berkeley. \nSyed-Abdullah S (2005) Empirical study on extreme programming. Unpublished PhD thesis, \nUniversity of Sheffield. \nTessem BR (2003) Experiences in learning XP practices: A qualitative study. Extreme \nProgramming and Agile Processes in Software Engineering, Lecture Notes in Computer \nScience, 2675\/2003:1012, Springer Berlin \/ Heidelberg. \nThomson C, Holcombe M (2009) The Sheffield software engineering observatory archive: \nSix years of empirical data collected from 73 complete projects. CS-09-01. Department of \nComputer Science, University of Sheffield. \nWilliams L, Krebs W, Layman L (2004a) Extreme programming evaluation framework for \nobject-oriented languages \u2013 version 1.4. Technical report, NCSU. \nWilliams L, Krebs W, Layman L (2004b). Extreme programming evaluation framework for \nobject-oriented languages \u2013 version 1.3. Technical report, NCSU. \nYoung M, Edwards H, McDonald S, Thompson B (2005) Personality characteristics in an XP \nteam: A repertory grid study. HSSE '05: Proceedings of the 2005 workshop on Human and \nsocial factors of software engineering. St. Louis MO, ACM. \n \n 17 \n \nTable 1: Questionnaire topics \n \nQuestions \nOriginal \nWeights  \nItem 1 Automated Unit Tests .40 \nItem 2 Customer Acceptance Tests .20 \nItem 3 Test-First Design .20 \nItem 4  Pair Programming .80 \nItem 5 Refactoring .70 \nItem 6 Release\/Planning Game .32 \nItem 7  Short Releases .40 \nItem 8 Stand-Up Meeting .05 \nItem 9 Continuous Integration .60 \nItem 10 Coding Standards .30 \nItem 11 Collective Code Ownership .50 \nItem 12 Sustainable Pace .30 \nItem 13 Simple Design .55 \n \n \n \n 18 \n \nTable 2: Descriptive statistics, Correlation and Covariance matrix for the 13 items \nItem  Question Mean St. D 1 2 3 4 5 6 7 8 9 10 11 12 13 \n1 Automated Unit \nTests 3.28 2.81 7.92 6.04 5.86 3.16 2.53 3.30 3.63 0.04 3.09 1.90 1.72 2.86 0.44 \n2 Customer \nAcceptance \nTests 3.32 3.33 0.64*** 11.09 5.67 3.64 2.62 3.88 4.02 0.04 1.52 1.58 0.49 2.40 0.86 \n3 Test-First Design 2.86 2.76 0.76*** 0.62*** 7.61 3.55 2.52 3.44 4.01 1.39 2.93 1.15 1.70 2.58 0.20 \n4 Pair \nProgramming 5.64 2.61 0.43*** 0.42*** 0.49*** 6.82 1.92 2.70 3.13 0.90 2.70 0.94 2.02 2.15 -0.20 \n5 Refactoring 3.98 2.26 0.40*** 0.35*** 0.41*** 0.33*** 5.09 2.62 2.44 0.81 2.20 1.42 0.56 1.49 0.22 \n6 Release\/Planning \nGame 4.09 2.88 0.41*** 0.40*** 0.43*** 0.36*** 0.40*** 8.28 3.11 2.31 3.05 2.40 0.88 3.10 1.03 \n7 Short Releases 3.79 3.02 0.43*** 0.40*** 0.48*** 0.40*** 0.36*** 0.36*** 9.12 1.02 3.34 1.21 1.79 2.56 0.51 \n8 Stand-Up \nMeeting 3.68 3.12 0.00 0.00 0.16 0.11 0.12 0.26** 0.11 9.71 1.65 0.97 1.08 1.47 1.35 \n9 Continuous \nIntegration 5.04 2.94 0.37*** 0.16 0.36*** 0.35*** 0.33*** 0.36*** 0.38*** 0.18* 8.66 4.05 3.35 2.56 0.91 \n10 Coding \nStandards 5.99 3.02 0.22** 0.16 0.14 0.12 0.21* 0.28** 0.13 0.10 0.46*** 9.09 2.02 2.93 1.60 \n11 Collective Code \nOwnership 5.28 2.73 0.22** 0.05 0.23** 0.28** 0.09 0.11 0.22* 0.13 0.42*** 0.25** 7.44 2.54 0.80 \n12 Sustainable Pace 5.31 2.36 0.43*** 0.31*** 0.40*** 0.35*** 0.28** 0.46*** 0.36*** 0.20* 0.37*** 0.41*** 0.39*** 5.59 1.38 \n13 Simple Design 6.16 2.21 0.07 0.12 0.03 -0.04 0.04 0.16 0.08 0.20* 0.14 0.24** 0.13 0.26** 4.89 \n\uf0b7 * <p.05, ** p<.01, *** p<.001  \n\uf0b7 The upper right triangle are the covariances, the lower left triangle (in italics) are the correlations \n \n19 \nTable 3: Model coefficients for the uni-dimensional model \n Unstandardized \nCoefficients \nSE Z \nStandardized \nCoefficients \nAutomated Unit Tests 1 --- --- 0.823 \nCustomer Acceptance Tests 0.997 0.115 8.657* 0.693 \nTest-First Design 0.992 0.091 10.940* 0.833 \nPair Programming 0.665 0.093 7.117* 0.59 \nRefactoring 0.509 0.082 6.181* 0.522 \nRelease\/Planning Game 0.720 0.103 6.963* 0.579 \nShort Releases 0.766 0.108 7.080* 0.587 \nStand-Up Meeting 0.220 0.121 1.827 0.164 \nContinuous Integration 0.640 0.108 5.932* 0.503 \nCoding Standards 0.407 0.115 3.546* 0.312 \nCollective Code Ownership 0.374 0.104 3.608* 0.318 \nSustainable Pace 0.575 0.085 6.738* 0.563 \nSimple Design 0.128 0.086 1.492 0.134 \n* P <.001 \n20 \n \nTable 4: Fit assessment for models of adherence to XP \nStatistic \nUni-dimensional \nModel \nFour-dimensional \nModel \n\uf0632 171.181** 97.984* \n\uf0632 df 65 59 \n\uf0632 baseline 618.600** 618.600** \n\uf0632 baseline df 78 78 \nCFI .804 .928 \nTLI .764 .905 \nRMSEA .109** .069 \nRMSEA CF .089\u2013.129 .044\u2013.093 \nSRMS .091 .065 \n* p<.01, ** p<.001 \n21 \n \nTable 5: Model coefficients for the four-dimensional model \n Unstandardized \nCoefficients \nSE Z \nStandardized \nCoefficients \nFoundations     \nAutomated Unit Tests 1 0 -- 0.850 \nCustomer Acceptance Tests 1.011 0.107 9.465** 0.726 \nTest-First Design 1.001 0.084 11.937** 0.869 \nPair Programming 0.620 0.089 6.929** 0.568 \nRefactoring 0.470 0.079 5.950** 0.499 \nCustomer Planning     \nRelease\/Planning Game 1 0 -- 0.633 \nShort Releases 0.998 0.172 5.813** 0.602 \nStand-Up Meeting 0.384 0.162 2.374* 0.224 \nTeaming     \nContinuous Integration 1 0 -- 0.765 \nCoding Standards 0.777 0.147 5.300* 0.580 \nCollective Code Ownership 0.639 0.129 4.939** 0.527 \nCraftsmanship     \nSustainable Pace 1 0 0 0.895 \nSimple Design 0.308 0.129 2.391* 0.295 \n* p<.05, ** p<.001 \n22 \n \nTable 6: Regression models for predictive validity \nWeighted \nmodels \nPredictors \uf062 \nStd. \nErro\nr \nt F (df1,df2) R\n2\n \uf044F \uf044R2 \nUni-\nDimensional \nLinear \nIntercept 63.37 1.83 34.52*** 0.001(1,148) .000   \nAdherence 1.96 1.17 1.68     \n        \nUni-\nDimensional \nQuadratic \nIntercept 64.67 1.82 35.56*** 5.53(2,147)** .07 11.06\na\n* * .07 \nAdherence 0.28 0.68 0.41     \nAdherence^2 -1.73 0.52 -3.33**     \n         \nFour- \nDimensional \nLinear \nIntercept 55.72 2.08 26.79*** 11.87(4,145)*** .247   \nFoundations -2.00 .44 -4.51***     \nCustomer Planning .25 .48 .51     \n Teaming 1.83 .41 4.44***     \n Craftsmanship 1.27 .61 2.07*     \n         \nFour- \nDimensional \nQuadratic \nIntercept 53.16 2.35 22.62*** 10.66(6,143)*** .309 6.46\nb\n** .062 \nFoundations -1.76 .43 -4.06***     \nFoundations^2 -.61 .20 -3.02**   12.38\nc\n*** .239 \n Customer Planning .25 .46 .53     \n Teaming 1.95 .41 4.81***     \n Craftsmanship 1.79 .61 2.94**     \n Craftsmanship^2 .91 .34 2.65**     \n* P<.05, ** p<.01, ***p<.001 \na. Comparison between 1D linear and 1D quadratic \nb. Comparison between 4D linear and 4D quadratic \nc. Comparison between 1D quadratic and 4D quadratic \n23 \n \n \nFigure 1: Uni-dimensional model of adherence to XP  \n \n \n \n \n \n24 \n \nFigure 2: Hypothesized model (4 dimensions) of adherence to XP \n \n \n \n \n \n25 \nFigure 3: Plots of regression terms. Plotted data are the partial residuals and dotted lines \nare the pointwise standard errors \n \n \n \n"}