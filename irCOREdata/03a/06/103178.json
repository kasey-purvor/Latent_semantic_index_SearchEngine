{"doi":"10.1109\/UKCI.2010.5625604","coreId":"103178","oai":"oai:epubs.surrey.ac.uk:3037","identifiers":["oai:epubs.surrey.ac.uk:3037","10.1109\/UKCI.2010.5625604"],"title":"Mind the (Computational) Gap","authors":["Casey, MC","Pavlou, A","Timotheou, A"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2010","abstract":"Despite many advances in both computational intelligence and computational neuroscience, it is clear that we have yet to achieve the full potential of nature inspired solutions from studying the human brain. Models of brain function have reached the stage where large-scale models of the brain have become possible, yet these tantalising computational structures cannot yet be applied to real-world problems because they lack the ability to be connected to real-world inputs or outputs. This paper introduces the notion of creating a computational hub that has the potential to link real sensory stimuli to higher cortical models. This is achieved through modelling subcortical structures, such as the superior colliculus, which have desirable computational principles, including rapid, multisensory and discriminative processing. We demonstrate some of these subcortical principles in a system that performs real-time speaker localisation using live video and audio, showing how such models may help us bridge the computational gap","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:epubs.surrey.ac.uk:3037<\/identifier><datestamp>\n      2017-10-31T14:07:37Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      74797065733D636F6E666572656E63655F6974656D<\/setSpec><setSpec>\n      6469766973696F6E733D656E67616E64706879736963616C736369656E636573:436F6D707574696E67<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/epubs.surrey.ac.uk\/3037\/<\/dc:relation><dc:title>\n        Mind the (Computational) Gap<\/dc:title><dc:creator>\n        Casey, MC<\/dc:creator><dc:creator>\n        Pavlou, A<\/dc:creator><dc:creator>\n        Timotheou, A<\/dc:creator><dc:description>\n        Despite many advances in both computational intelligence and computational neuroscience, it is clear that we have yet to achieve the full potential of nature inspired solutions from studying the human brain. Models of brain function have reached the stage where large-scale models of the brain have become possible, yet these tantalising computational structures cannot yet be applied to real-world problems because they lack the ability to be connected to real-world inputs or outputs. This paper introduces the notion of creating a computational hub that has the potential to link real sensory stimuli to higher cortical models. This is achieved through modelling subcortical structures, such as the superior colliculus, which have desirable computational principles, including rapid, multisensory and discriminative processing. We demonstrate some of these subcortical principles in a system that performs real-time speaker localisation using live video and audio, showing how such models may help us bridge the computational gap.<\/dc:description><dc:date>\n        2010<\/dc:date><dc:type>\n        Conference or Workshop Item<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        attached<\/dc:rights><dc:identifier>\n        http:\/\/epubs.surrey.ac.uk\/3037\/2\/Paper.pdf<\/dc:identifier><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/epubs.surrey.ac.uk\/3037\/4\/licence.txt<\/dc:identifier><dc:identifier>\n          Casey, MC, Pavlou, A and Timotheou, A  (2010) Mind the (Computational) Gap  In: UK Workshop on Computational Intelligence (UKCI 2010), 2010-09-08 - 2010-09-10, University of Essex.     <\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/UKCI.2010.5625604<\/dc:relation><dc:relation>\n        10.1109\/UKCI.2010.5625604<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/epubs.surrey.ac.uk\/3037\/","http:\/\/dx.doi.org\/10.1109\/UKCI.2010.5625604","10.1109\/UKCI.2010.5625604"],"year":2010,"topics":[],"subject":["Conference or Workshop Item","PeerReviewed"],"fullText":"Mind the (Computational) Gap\nMatthew Casey, Athanasios Pavlou, and Anthony Timotheou\nAbstract\u2014Despite many advances in both computational\nintelligence and computational neuroscience, it is clear that we\nhave yet to achieve the full potential of nature inspired solutions\nfrom studying the human brain. Models of brain function have\nreached the stage where large-scale models of the brain have\nbecome possible, yet these tantalising computational structures\ncannot yet be applied to real-world problems because they lack\nthe ability to be connected to real-world inputs or outputs.\nThis paper introduces the notion of creating a computational\nhub that has the potential to link real sensory stimuli to\nhigher cortical models. This is achieved through modelling\nsubcortical structures, such as the superior colliculus, which\nhave desirable computational principles, including rapid, mul-\ntisensory and discriminative processing. We demonstrate some\nof these subcortical principles in a system that performs real-\ntime speaker localisation using live video and audio, showing\nhow such models may help us bridge the computational gap.\nI. INTRODUCTION\nDespite many advances in both computational intelligence\n(CI) and computational neuroscience (CN), it is clear that\nwe have yet to achieve the full potential of nature inspired\nsolutions from studying the human brain. As a consequence,\na broader reference point for intelligent behaviour which\nencompasses artificial agents is now being advocated [1].\nHowever, before we abandon human intelligence as our ref-\nerence, have we fully exploited the available neuroscience?\nTo understand this question, let us remind ourselves of\nthe approach adopted by Turing in proposing how to build\nan intelligent machine [2]. Turing focused on building an\nartificial cortex, which he described as an \u201cunorganised\nmachine\u201d [2, p6]; a machine which starts in a random state\nand becomes organised through learning. In essence, Turing\nidentified the computational principle of learning in the cor-\ntex as a key component of developing intelligent behaviour.\nExtracting such principles has become the foundation of CI\nand the development of biologically-inspired and plausible\nalgorithms has reached the stage where large-scale models\nof the brain have become possible [3], [4], bringing us closer\nto Turing\u2019s original ideal. However, while such large-scale\nmodels are impressive CN tools, they are not connected to\nreal-world inputs or outputs, yet this is an essential step\nif we wish to exploit such models and develop \u201cintelligent\nmachines\u201d.\nAlthough Turing and contemporary followers of his ideas\nwere less concerned with \u201ccircuits required for quite definite\npurposes\u201d, such as controlling respiration or eye move-\nments [2, p12], focusing on the cortex appears too narrow.\nFor example, the nervous system of a human is far more\nMatthew Casey, Athanasios Pavlou and Anthony Timotheou are with the\nDepartment of Computing, University of Surrey, Guildford, Surrey, GU2\n7XH, UK (phone: +44 (0)1483 689635; fax: +44 (0)1483 686051; email:\n{m.casey, athanasios.pavlou, at00087}@surrey.ac.uk).\ncomplex than just the cortex and even non-nervous system\nbiological processes appear to exhibit cognitive behaviour,\nsuch as the immune system [5]. One potential solution is\nto focus on other areas of the nervous system that exhibit\ninteresting computational properties and which are more\nclosely associated with sensory inputs and motor control.\nSuch areas can be found in subcortical sensory processing. To\nfocus on one example from Turing, saccadic eye movements\nare controlled by at least 10 cortical and subcortical regions\nof the brain, with the superior colliculus (SC) in the midbrain\nplaying a pivotal role [6]. Although performing a \u2018definite\npurpose\u2019, the SC is known to develop becoming mature\nin mid gestation [7] and may adapt [8], hence it shows\nsome interesting computational principles. It is also directly\nconnected to the optic tract and projects to the brain stem,\nand it is therefore many steps closer to real-world inputs\nthan the cortex. With other similar examples, can focusing\non subcortical areas help us bridge the gap?\nIn this paper, we attempt to answer this question by\ndemonstrating how we can extract computational principles\nfrom sensory subcortical structures and demonstrate their\npractical application on a simple real-time speaker locali-\nsation task (a more intelligent version of eye saccades). This\nallows us to show how a CN model can be applied to a real-\nworld CI problem. Our aim is to demonstrate that there is\nstill potential left in modelling the nervous system (but not\njust the cortex) to inspire adaptive, real-time, neuro-inspired\nsolutions to problems, and that this potential lies in exploiting\noften overlooked structures that directly process inputs and\noutputs, as well as mediate with the (still important) cortex.\nIn section II we present our candidate set of principles. In\nsection III we describe the nature-inspired method we will\nuse in the demonstration of these computational principles.\nWe demonstrate the results from this model and evaluate its\neffectiveness in section IV, while we discuss the implications\nof this approach in section V.\nII. EXTRACTING COMPUTATIONAL PRINCIPLES\nFollowing the example set by Turing on eye movements,\nwe focus in this paper on subcortical visual processing. This\nfocus is also overtly motivated by our own previous work on\ndeveloping CN models of relevant structures, including the\nSC [9] and amygdala [10], which provide a foundation upon\nwhich we can extract computational principles.\nSome of the key functional areas in the visual pathway\nare shown in Figure 1, together with their afferent cortical\nareas. In cognitive psychology, human vision is often referred\nto in terms of the retino-geniculate-cortical pathway [11],\nplacing emphasis on the retina, lateral geniculate nucleus\n(LGN) and the visual cortex. In general terms the LGN,\nsituated in the thalamus, is perceived as routing retinal\nstimuli to appropriate areas of the visual cortex. While large-\nscale models, such as Izhikevich and Edelman\u2019s [4] use a\nmodel of the visual thalamocortical circuit in their 1,000,000\nneuron simulation, take care over how neurons and their\nconnections are modelled, their view of the LGN and other\nsubcortical structures appears too simplistic. For example,\nthe LGN receives input from the retina and forms a layered\ntopographic map of the whole visual space, which develops\nbefore the animal can open it eyes and hence before it can\nmatch binocular images [12]. Another area in the visual\npathway which is topographic is the SC. This also receives\ndirect input from the retina, and consists of a series of aligned\ntopographic maps of the visual, auditory and somatosensory\nspace, combining these into a multisensory representation to\ninitiative eye movement [13].\nFig. 1. Selected subcortical visual pathways leading to the visual cor-\ntex [14], [15]. Retinal neurons connect to the superior colliculus (SC), and\nthe lateral posterior nucleus (LP) and lateral geniculate nucleus (LGN) in\nthe thalamus. LP connects to the basolateral amygdala complex (BLA),\nwhich in turn connects to the central amygdaloid nucleus (Ce). The Ce and\nSC provide motor output via the pontine reticular nucleus (PnC). Cortical\nareas include the primary and secondary visual cortices (V1, V2), temporal\ncortical area (TE2) and the perirhinal cortex (PR).\nThe lateral posterior nucleus (LP) and pulvinar, which are\nlinked to the LGN, also perform more complex functions\nthan they are often credited with, such as being orientation\nand motion selective [16], properties often associated with\nthe visual cortex. If we travel further up the visual pathways\nto the limbic system, then the amygdala also performs\ncomplex sensory processing to discriminate. For example,\nthe amygdala has been implicated in the crude processing of\nvisual emotional stimuli [17]. This crude discrimination is\nalso adaptive as demonstrated through conditioning [18].\nThese subcortical structures therefore demonstrate key\nproperties we normally associate with many different areas\nof the brain, and particularly those in the cortex. However,\nthey also have other key properties associated with their\nlower level of operation. First, subcortical sensory processing\noccurs rapidly. For example, the SC is implicated in \u201cexpress\nsaccades\u201d which can occur in as little as 80ms [6]. Second,\nthis rapidity may occur because they are more directly\nconnected to sensory inputs and motor outputs (this is true\nnot just for vision, but also for olfactory connections to\nthe amygdala [19]). Third, while some structures operate on\nsingle sensory modalities, such as the LGN, LP, pulvinar for\nvision [16] or the inferior colliculus (IC) for audition [20],\nkey structures are multisensory [21]. Fourth, subcortical\nstructures appear essential not just for sensory to cortex rout-\ning [22], but also for cortical-to-cortical communication [12],\nwhile they also receive cortical feedback to moderate their\noperation [23].\nAlthough we have biased our argument on just a few\nof the many subcortical structures, the properties that are\nexhibited by just these are sufficiently interesting to warrant\nexploitation. To summarise these principles, our selected sub-\ncortical structures develop representations through processes\nsuch as self-organisation, adapt such as with conditioning\nand cortical feedback, discriminate, albeit on crude stimuli,\nare rapid in operation by being closely connected to sensory\ninputs and motor outputs, are often multisensory and are\nessential for cortical operation and communication.\nIII. FUNCTIONAL IMPLEMENTATION\nTo implement the subcortical computational principles we\nhave identified, we take as inspiration system level models\nof the visual and audio pathways [18], [24], and in particular\nthose modelling the SC [9], [10]. This approach allows us\nto modify existing CN models for practical use as we can\nselect the key aspects we wish to embody through inputs,\narchitecture and parameters. A systems level model simplifies\nthe extensive processing undertaken by these structures, yet\nallows us to focus on key functionality and properties, par-\nticularly development, discrimination, rapidity resulting from\nsimple, parallel processing of unisensory stimuli, combining\nthese into a multisensory representation.\nOur architecture is a variation of a model of the SC [9],\nwhich uses Hebbian learning for the development of a\nlayered, modularised model based on principles defined\nby [18]. It comprises three modules each of size 2 by 128\nneurons: two modules process the visual and auditory inputs\nseparately, the output of which is then combined in a third\nintegration map. The visual input size is 32 by 128 pixels,\nwhile the auditory input is 5 by 100 pixels wide (we use\nthe term pixels here, even though we are dealing with sound\nlocations). To train the model we use virtual inputs, prior\nto testing it on live video and audio. The activation y of a\nneuron at location (i, j) in a map given an m dimensional\ninput x is given by:\nuij =\nm\u2211\nk=1\nxkwkij(t)\nyij =\n{\nf(uij) if \u2016cij \u2212 cwin\u2016 < h(t)\nf(uij \u2212 ywin) otherwise\nf(u) =\n\uf8f1\uf8f2\uf8f3 1 u \u2265 1u 0 < u < 1\n0 u \u2264 0\nThe weight from an input k to a neuron (i, j) on the\nmap at time step t \u2265 0 is represented by wkij(t). The\nmodel implements circular winner areas that introduce lateral\ninhibition in order to topographically organise during the\ntraining process, similar to Kohonen\u2019s SOM [25]. At each\ntime step the neuron which produces the maximum activation\nywin = maxijf(uij) on the map is labelled as the winner.\nAny neuron, cij , is considered to fall within the winning\nneuron\u2019s, cwin, area if its location (i, j) lies within the current\nradius h(t). Training occurs by epochs, each of which is a\nrandom presentation of the full training set. At each epoch\nthe winning neuron\u2019s area is reduced following a Gaussian\nneighbourhood:\nh(t) = rmin + (rmax \u2212 rmin)e\u2212(\n(t\/te)\n2\n2r2s\n)\nHere te equals the number of training samples, rmin, rmax\nrepresenting the minimum and maximum radii for the neigh-\nbourhood and rs representing the bandwidth. This results in\na gradual tuning of neurons\u2019 weights to respond to inputs\ndepending on their location in the input space. The maximum\nradius rmax is always the larger dimension of a map (rmax =\n128) and the minimum radius was set to rmin = 1. The\nbandwidth was set to rs = 100. Each weight is updated\nseparately and we normalize weights to avoid exponential\ngrowth:\nw\u2032kij(t+ 1) = wkij(t) + \u000f(t)xkyij\nwkij(t+ 1) =\nw\u2032kij(t+ 1)\u2211m\nl=1 w\n\u2032\nlij(t+ 1)\nThe learning rate \u000f(t) is constant for each map (\u000f(t) = 0.001\nfor the visual map, 0.01 for the auditory map, and 0.0001\nfor the integration map). Each of the modules is trained for\n400 epochs. The parameters used were derived from an in\ndepth analysis described in [9].\nIV. SPEAKER LOCALISATION\nThe aim of our experiments is to demonstrate that the\nmodel can develop topographic maps with integrative capa-\nbilities that result in real-time detection of coincident visual\nand auditory stimuli. The context of the experiments is\nspeaker localisation where higher responses are to be induced\nupon coincident detection of a human head and sound. Here\nwe are not aiming for state-of-the-art speaker localisation,\nwhich can be achieved reliably using specific voice and\nhead tracking algorithms [26], rather we are attempting to\ndemonstrate that generic, adaptive solutions can be applied\nto such specific problems. A stepwise evaluation approach is\nfollowed and we start by examining the model responses\nto virtual stimuli to determine how well the model has\ndeveloped. Next we evaluate the model against real-time\nvisual and auditory inputs from a camera and a microphone.\nVideo and audio data for the experiments were captured\nusing a Java implementation of the model1. This was run\non a Dell Latitude D630 laptop with an Intel Core 2 Duo\nT7250 (2.00GHz) processor, 3.5GB of RAM, a Logitech\nLive! Cam voice USB camera and two Logitech USB desktop\n1A Java demonstration of the system will be made available at http:\/\/\nwww2.surrey.ac.uk\/computing\/people\/matthew_casey\/.\nmicrophones, running Windows XP Professional SP3, Live!\nCam driver version 1.1.2.410, JRE 1.6.0 17, JMF 2.1.1e.\nThe models were trained and evaluated using Matlab ver-\nsion 7.8.0.347. The camera was positioned 0.7m above the\nplane of the microphones and 0.54m centrally behind. The\nmicrophones were separated by 1.34m. The heads of the\nsubjects were 1.66m from the camera. To produce a loud and\nsustained sound, a sound source positioned 0.42m from the\nmicrophones. This was a pure tone produced from a laptop\nthat could be moved independently of the head positions to\nallow testing of non-coincident audio and visual stimuli.\na) Visual Map Input b) Auditory Map Input\nFig. 2. a) Generic face used to train the visual map and b) the Gaussian\nactivation pattern used to represent a localized sound used to train the\nauditory map.\nFor the model\u2019s training we use virtual visual and auditory\ninputs (Figure 2). Such stimuli ensure consistency amongst\ntraining examples and act as generic templates of the real\nstimuli. In particular, for the training of the visual map we\nuse a generic non-textured head produced by the FaceGen\napplication [27]. The head is grey scaled, normalised between\n0 to 1 and rescaled to 32 by 32 pixels. This size allows com-\nputational efficiency while at the same time maintaining the\ncharacteristics of the original virtual head. For the creation\nof the visual map training data, we place the virtual head\nat consecutive locations within a 32 by 128 visual space.\nThis results in 96 visual training examples. The hypothetical\nvisual space and examples represent a stripe of a visual scene\nwithin which a head is moving across from one end to the\nother.\nThe training data for the auditory map are Gaussian\nactivation patterns that represent the location (and not the\ntype) of a sound within the defined auditory space. We define\nthe location of a sound as a 5 by 5 grid with the Gaussian\npattern centred at (c, d):\nxij = \u03bbe\n\u2212( (i\u2212c)2+(j\u2212d)22\u03c32 )\nwhere \u03bb = 1 for the amplitude and \u03c3 = 4 for the\nbandwidth. Since we have 96 visual examples we need the\nsame number of auditory inputs so that we can achieve a\none to one correspondence (a head and a sound at the same\nlocation). Therefore, we place 96 consecutive examples of\nsound locations in a similar manner to the visual inputs\nresulting in the 5 by 100 auditory space.\nA. Model Responses to Virtual Visual and Auditory Stimuli\nThe aim of this experiment is to demonstrate the key\nproperties of the model. Each of the unisensory maps is\ntrained using the 96 virtual samples described above. After\nthe maps have finished training, their outputs were presented\nwith coincident audio-visual samples in order to train the in-\ntegration map. Figure 3 illustrates that the trained unisensory\nmaps have learned to develop receptive fields that respond\npreferentially thus demonstrating discriminative properties to\ncertain inputs at particular locations.\na) Visual Map Weights b) Auditory Map Weights\nFig. 3. Weight values of neuron (1, 96) on the visual map and neuron\n(1, 96) on the auditory map. The neuron weights resemble the virtual head\nand auditory location inputs that they are trained on.\nTo evaluate the model development of topographic maps\nwe tested it with a visual input that consisted of two virtual\nheads at either end of the input (combining example 1 and\n96), while the auditory input is a moving virtual sound from\nleft to right within the auditory space (all 96 locations).\nSince the location of the sound changes, there will be only\ntwo auditory locations which are fully coincident with visual\n(head) locations. By observing the visual map activation for\nexample 8 and 88 (Figure 4a, b) it is clear that the location\nof the heads activate the edges of the map since they are on\nthe edges of the visual space. However, the sound location\nin trial example 8 is towards the centre of the auditory space\nwhilst example 88 towards its end. This results in different\nareas of the auditory map being activated (Figure 4a, b)\nwhich in turn affects the activation strength of the integration\nmap. Figure 4c shows that the integration map is highly\nresponsive on its edges and this is attributed to the auditory\nstimulus being coincident with the visual stimuli towards the\nstart and end of its movement within the auditory space.\nThis demonstrates multisensory enhancement on coincident,\nmultimodal stimuli, which is a well-studied property of the\nSC [14].\nB. Model Responses to Real Visual and Virtual Auditory\nStimuli\nHaving established the functionality of the model on\nartificial data we now evaluate it on video images and virtual\nmoving sound. The image examples here depict two heads\nnear the edges of the visual space. The images themselves\nhave been extracted from a video sequence of two non-\nmoving people. Therefore the only difference to the previous\ntrial is the substitution of the virtual with real faces. As seen\na) Responses for Example 48\nb) Responses for Example 88\nc) Maximum Integrated Activations\nFig. 4. Model responses to virtual stimuli: a) and b) show the the visual\nand auditory inputs, visual and auditory map activations, and integration map\nactivation for example inputs 48 and 88; c) shows maximum activations of\nthe integration map for all 96 inputs.\nin Figure 5a the visual map shows that it is still activated\nnear its edges. Crucially, we also observe that the profile\nof activations in Figure 5b is similar to that of Figure 4c.\nThis means that the model can successfully generalise its\nbehaviour when given real visual data.\nC. Model Responses to Real Visual and Auditory Stimuli\nFor our final experiment we present the model with both\nreal visual and audio data and demonstrate its real-time\ndeployment. For this trial we have used a single non-moving\nperson on the right side of the visual space that activates\na) Responses for Example 8\nb) Maximum Integrated Activations\nFig. 5. Model responses to real non moving head stimuli and virtual moving\nsound: a) shows the the visual and auditory inputs, visual and auditory map\nactivations, and integration map activation for example input 8; b) shows\nmaximum activations of the integration map for all 96 inputs.\na sound in front of them for a duration of 10 seconds. The\nsound is a pure tone (16 bit, 800Hz). This is used, rather than\ntalking, to provide a consistent and sufficiently loud sound\nfor the microphones to capture. Although an appropriate\nlocalisation is given in response to talking, because of the\nlow volume and reverberation in the room, the location varies\nwith every frame unless noise is minimised. The pure tone\nwas therefore used to provide consistency and to allow the\nsound to vary to different locations to that of the heads. The\n325 examples used in this trial correspond to 13 seconds at\n25 frames per second audio and video, with the sound played\napproximately after 2 seconds from the trial initialisation.\nSound localisation in mammals is achieved using both\nILD and ITD, with the IC being the pivotal subcortical\nstructure having a topographic representation of sound. For\neye movement via the SC, ILD is used predominantly for\nazimuth localisation [20], which is our chosen task. We\ntherefore use a simplified version of an ILD algorithm [28]\nwhich allows us to localise using two microphones. Here,\nthe sound intensity for each microphone is calculated from\na sample buffer of sound pressures. The intensity from each\nmicrophone is then used to describe the centre and radius of\na circle which represents the possible locations of the sound\ngiven the relative position of the two microphones. With only\ntwo microphones it is not possible to localise any further\nthan this circle without assumptions about the possible sound\nsource locations. Here, we assume that the sound location\nfalls at the point on the circle which is closest to the centre\npoint between the two microphones.\nFigure 6a shows the results for example input 100. We\nobserved that all activation values above 0.22 correspond\nto coincident detections. On the other hand, non-coincident\nexamples (such as example 300 illustrated in Figure 6b) have\na lower activation in the integration map. We can therefore\nuse this value to discriminate speakers as it corresponds to\nan activation of coincident head and sound localisation.\na) Responses for Example 100\nb) Responses for Example 300\nc) Maximum Integrated Activations\nFig. 6. Model responses to real visual and audio stimuli: a) and b) show\nthe the visual and auditory inputs, visual and auditory map activations,\nand integration map activation for example inputs 100 and 300; c) shows\nmaximum activations of the integration map for all 96 inputs.\nV. CONCLUSIONS\nIn this paper we have demonstrated how CN models\nof subcortical processing may be used to inspire CI solu-\ntions. We focused on key subcortical structures because they\ndirectly process sensory input and produce motor output,\noffering us insight into how more complex cortical models\nmight be connected to the real-world. Although we focused\non only a small number of subcortical structures, including\nthe SC and amygdala, and ignored others, such as the\nhippocampus, we identified seven computational principles\nthat we wish to emulate: development, adaptation, discrim-\nination, rapid operation, sensory input and motor output\nconnectivity, multisensory integration and communication. At\nleast the first three are shared with cortical structures, yet\nin terms of real-world operation, rapid processing of crude\nmultisensory stimuli appears key to our aim of demonstrating\nthe potential for inspiring novel and relevant solutions to\nreal-world problems. However, we recognise that this intu-\nitive \u201creasoning by metaphor\u201d [29] approach of extracting\nprinciples is not ideal, and we should be following a more\nanalytic approach to understand the computational properties\nof these structures. Nonetheless, this is enough to initiate\nbridging the computational gap of higher cortical simulations\nand real sensory input.\nTo demonstrate the potential of subcortical approaches to\nCI, we selected the real-time task of speaker localisation.\nHere, we adapted a system level model of the SC to\nlocalise heads and sound sources. Through evaluating this\non both virtual and real data, we demonstrated that the\nmodel develops topographic representations, discriminating\nand combining multisensory stimuli, all in real-time. Al-\nthough this demonstrator is not as capable as state-of-the-art\nimplementations of speaker localisation, it does demonstrate\nhow CN models may be applied to CI problems.\nACKNOWLEDGEMENTS\nThe authors would like to thank Andre Gru\u00a8ning and Jon\nTimmis for listening to the argument presented in this paper.\nREFERENCES\n[1] N. Cristianini, \u201cAre we there yet?\u201d Neural Networks, vol. 23, no. 4,\npp. 466\u2013470, 2010.\n[2] A. M. Turing, \u201cIntelligent machinery,\u201d National Physical Laboratory,\nTech. Rep., 1948. [Online]. Available: http:\/\/www.alanturing.net\/\nturing archive\/archive\/l\/l32\/L32-001.html\n[3] H. Markram, \u201cThe blue brain project,\u201d Nature Reviews Neuroscience,\nvol. 7, no. 2, pp. 153\u2013160, 2006.\n[4] E. Izhikevich and G. M. Edelman, \u201cLarge-scale model of mammalian\nthalamocortical systems,\u201d Proceedings of the National Academy of\nSciences of the USA, vol. 105, no. 9, pp. 3593\u20133598, 2008.\n[5] I. R. Cohen and D. Harel, \u201cTwo views of a biology-computer science\nalliance,\u201d in Proceedings of the 2009 Workshop on Complex Systems\nModelling and Simulation (CoSMoS 2009), S. Stepney, P. Welch, P. S.\nAndrews, and J. Timmis, Eds. Luniver Press, 2009, pp. 1\u20138.\n[6] A. K. Moschovakis, C. A. Scudder, and S. M. Highstein, \u201cThe\nmicroscopic anatomy and physiology of the mammalian saccadic\nsystem,\u201d Progress in Neurobiology, vol. 50, no. 2-3, pp. 133\u2013254,\n1996.\n[7] J. Qu, X. Zhou, H. Zhu, G. Cheng, W. S. Ashwell, and F. Lu,\n\u201cDevelopment of the human superior colliculus and the retinocollicular\nprojection,\u201d Experimental Eye Research, vol. 82, no. 2, pp. 300\u2013310,\n2006.\n[8] A. G. Constantin, H. Wang, and J. D. Crawford, \u201cRole of superior\ncolliculus in adaptive eye-head coordination during gaze shifts,\u201d J\nNeurophysiol, vol. 92, no. 4, pp. 2168\u20132184, 2004.\n[9] A. Pavlou and M. C. Casey, \u201cSimulating the effects of cortical feed-\nback in the superior colliculus with topographic maps,\u201d in Proceedings\nof the International Joint Conference on Neural Networks (IJCNN)\n2010. IEEE, 2010.\n[10] \u2014\u2014, \u201cIdentifying emotions using topographic conditioning maps,\u201d in\nAdvances in Neuro-Information Processing: Proceedings of the 15th\nInternational Conference on Neuro-Information Processing, Lecture\nNotes in Computer Science 5506, M. Koeppen, N. Kasabov, and\nG. Coghill, Eds. Springer-Verlag, 2009, pp. 40\u201347.\n[11] T. Pasternak, J. W. Bisley, and D. J. Calkins, \u201cVisual processing\nin the primate brain,\u201d in Biological Psychology, ser. Handbook of\nPsychology, M. Gallagher and R. J. Nelson, Eds. John Wiley and\nSons, Inc., 2003, vol. 3, chapter 6, pp. 139\u2013185.\n[12] S. M. Sherman and R. W. Guillery, \u201cThe visual relays in the thalamus,\u201d\nin The Visual Neurosciences, L. M. Chalupa and J. S. Werner, Eds. A\nBradford Book, The MIT Press, 2004, vol. 1, chapter 35, pp. 565\u2013591.\n[13] A. J. King, \u201cThe superior colliculus,\u201d Current Biology, vol. 14, no. 9,\npp. R335\u2013R338, 2004.\n[14] B. E. Stein, R. F. Spencer, and S. B. Edwards, \u201cEfferent projections\nof the neonatal cat superior colliculus: Facial and cerebellum-related\nbrainstem structures,\u201d The Journal of Comparative Neurology, vol.\n230, no. 1, pp. 47\u201354, 1984.\n[15] C. Shi and M. Davis, \u201cVisual pathways involved in fear conditioning\nmeasured with fear-potentiated startle: Behavioral and anatomic stud-\nies,\u201d The Journal of Neuroscience, vol. 21, no. 24, pp. 9844\u20139855,\n2001.\n[16] C. Casanova, \u201cThe visual functions of the pulvinar,\u201d in The Visual\nNeurosciences, L. M. Chalupa and J. S. Werner, Eds. A Bradford\nBook, The MIT Press, 2004, vol. 1, chapter 36, pp. 592\u2013608.\n[17] L. Pessoa, \u201cTo what extent are emotional visual stimuli processed\nwithout attention and awareness?\u201d Current Opinion in Neurobiology,\nvol. 15, pp. 188\u2013196, 2005.\n[18] J. L. Armony, D. Servan-Schreiber, L. M. Romanski, J. D. Cohen, and\nJ. E. LeDoux, \u201cStimulus generalization of fear responses: Effects of\nauditory cortex lesions in a computational model and in rats,\u201d Cerebral\nCortex, vol. 7, no. 2, pp. 157\u2013165, 1997.\n[19] J. A. Gottfried, \u201cSmell: Central nervous processing,\u201d in Taste and\nSmell: An Update (Advances in Oto-Rhino-Laryngology), T. Hummel\nand A. Welge-Lssen, Eds. S. Karger AG, 2006, pp. 44\u201369.\n[20] B. Delgutte, P. X. Joris, R. Y. Litovsky, and T. C. Yin, \u201cReceptive fields\nand binaural interactions for virtual-space stimuli in the cat inferior\ncolliculus,\u201d J Neurophysiol, vol. 81, no. 6, pp. 2833\u20132851, 1999.\n[21] B. E. Stein and M. A. Meredith, The Merging of the Senses. Cam-\nbridge, MA.: A Bradford Book, MIT Press, 1993.\n[22] D. G. Amaral, H. Behniea, and J. L. Kelly, \u201cTopographic organization\nof projections from the amygdala to the visual cortex in the macaque\nmonkey,\u201d Neuroscience, vol. 118, no. 4, pp. 1099\u20131120, 2003.\n[23] J. C. Alvarado, T. R. Stanford, B. A. Rowland, J. W. Vaughan,\nand B. E. Stein, \u201cMultisensory integration in the superior colliculus\nrequires synergy among corticocollicular inputs,\u201d The Journal of\nNeuroscience, vol. 29, no. 20, pp. 6580\u20136592, 2009.\n[24] R. Miikkulainen, J. A. Bednar, Y. Choe, and J. Sirosh, Computational\nMaps in the Visual Cortex. New York: Springer Science+Business\nMedia, 2005.\n[25] T. Kohonen, \u201cSelf-organized formation of topologically correct feature\nmaps,\u201d Biological Cybernetics, vol. 43, pp. 59\u201369, 1982.\n[26] K. Hyun-Don, C. Jong-Suk, and K. Munsang, \u201cSpeaker localization\namong multi-faces in noisy environment by audio-visual integration,\u201d\n2006, pp. 1305\u20131310.\n[27] Singular Inversions Inc. (2010, Jun) Facegen - 3d human faces.\n[Online]. Available: http:\/\/www.facegen.com\/\n[28] S. T. Birchfield and R. Gangishetty, \u201cAcoustic localization by interaural\nlevel difference,\u201d in Proceedings of the IEEE International Conference\non Acoustics, Speech, and Signal Processing (ICASSP 2005), vol. 4,\n2005, pp. iv\/1109\u2013iv\/1112.\n[29] S. Stepney, R. E. Smith, J. Timmis, A. M. Tyrrell, M. J. Neal,\nand A. N. W. Hone, \u201cConceptual frameworks for artificial immune\nsystems,\u201d International Journal of Unconventional Computing, vol. 1,\nno. 3, pp. 315\u2013338, 2005.\n"}