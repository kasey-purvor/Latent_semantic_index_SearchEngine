{"doi":"10.1109\/TKDE.2011.17","coreId":"67956","oai":"oai:eprints.lancs.ac.uk:34353","identifiers":["oai:eprints.lancs.ac.uk:34353","10.1109\/TKDE.2011.17"],"title":"Creating evolving user behavior profiles automatically","authors":["Iglesias, Jose","Angelov, Plamen","Ledezma, Agapito","Sanchis, Aracheli"],"enrichments":{"references":[{"id":833234,"title":"A case study of incremental concept induction,\u201d in AAAI,","authors":[],"date":"1986","doi":null,"raw":null,"cites":null},{"id":832147,"title":"A comparative study of data mining algorithms for network intrusion","authors":[],"date":"2008","doi":null,"raw":null,"cites":null},{"id":832383,"title":"A comparative study of selected classifiers with classification accuracy in user profiling,\u201d","authors":[],"date":"2009","doi":null,"raw":null,"cites":null},{"id":837577,"title":"A comparing method of two team behaviours in the simulation coach competition,\u201d","authors":[],"date":"2006","doi":null,"raw":null,"cites":null},{"id":840730,"title":"A decision-theoretic generalization of on-line learning and an application to boosting,\u201d","authors":[],"date":"1997","doi":null,"raw":null,"cites":null},{"id":836013,"title":"An approach to incremental SVM learning algorithm,\u201d","authors":[],"date":"2000","doi":null,"raw":null,"cites":null},{"id":838100,"title":"An approach to online identification of takagi-sugeno fuzzy models,\u201d","authors":[],"date":"2004","doi":null,"raw":null,"cites":null},{"id":833724,"title":"Art2-a: An adaptive resonance algorithm for rapid category learning and recognition,\u201d","authors":[],"date":"1991","doi":null,"raw":null,"cites":null},{"id":830006,"title":"Automatically sharing web experiences through a hyperdocument recommender system,\u201d","authors":[],"date":"2003","doi":null,"raw":null,"cites":null},{"id":838636,"title":"Autonomous visual self-localization in completely unknown environment using evolving fuzzy rule-based classifier,\u201d","authors":[],"date":"2007","doi":null,"raw":null,"cites":null},{"id":835505,"title":"Bayesian online classifiers for text classification and filtering,\u201d","authors":[],"date":"2002","doi":null,"raw":null,"cites":null},{"id":839923,"title":"C4.5: programs for machine learning.","authors":[],"date":"1993","doi":null,"raw":null,"cites":null},{"id":841043,"title":"Comparison of adaboost and support vector machines for detecting alzheimer\u2019s disease through automated hippocampal segmentation,\u201d","authors":[],"date":"2010","doi":null,"raw":null,"cites":null},{"id":829584,"title":"Creating user profiles from a command-line interface: A statistical approach,\u201d","authors":[],"date":"2009","doi":null,"raw":null,"cites":null},{"id":839676,"title":"Data mining tools see5 and C5.0,\u201d","authors":[],"date":"2003","doi":null,"raw":null,"cites":null},{"id":832964,"title":"Data streams classification by incremental rule learning with parameterized generalization,\u201d","authors":[],"date":"2006","doi":null,"raw":null,"cites":null},{"id":835065,"title":"DVQ: Dynamic vector quantization - an incremental LVQ,\u201d","authors":[],"date":"1991","doi":null,"raw":null,"cites":null},{"id":832691,"title":"Efficient incremental induction of decision trees,\u201d","authors":[],"date":"1996","doi":null,"raw":null,"cites":null},{"id":840267,"title":"Estimating continuous distributions in bayesian classifiers,\u201d in","authors":[],"date":"1995","doi":null,"raw":null,"cites":null},{"id":834238,"title":"Evolving fuzzy neural networks for supervised\/unsupervised online knowledge-based learning,\u201d","authors":[],"date":"2001","doi":null,"raw":null,"cites":null},{"id":831975,"title":"Evolving fuzzy rule-based classifiers from data streams,\u201d","authors":[],"date":"2008","doi":null,"raw":null,"cites":null},{"id":838358,"title":"Evolving fuzzy rule-based classifiers,\u201d","authors":[],"date":"2007","doi":null,"raw":null,"cites":null},{"id":834488,"title":"Evolving improved incremental learning schemes for neural network systems,\u201d","authors":[],"date":"2005","doi":null,"raw":null,"cites":null},{"id":839168,"title":"Flexible models with evolving structure,\u201d","authors":[],"date":"2004","doi":null,"raw":null,"cites":null},{"id":833973,"title":"Fuzzy artmap: A neural network architecture for incremental supervised learning of analog multidimensional maps,\u201d Neural Networks,","authors":[],"date":"2002","doi":null,"raw":null,"cites":null},{"id":833473,"title":"Id5: An incremental id3,\u201d","authors":[],"date":"1988","doi":null,"raw":null,"cites":null},{"id":835742,"title":"Incremental and decremental support vector machine learning,\u201d","authors":[],"date":"2000","doi":null,"raw":null,"cites":null},{"id":835345,"title":"Incremental bayesian classification for multivariate normal distribution data,\u201d","authors":[],"date":"2008","doi":null,"raw":null,"cites":null},{"id":833538,"title":"Incremental induction of decision trees,\u201d","authors":[],"date":"1989","doi":null,"raw":null,"cites":null},{"id":831672,"title":"Intrusion detection: A bioinformatics approach,\u201d","authors":[],"date":"2003","doi":null,"raw":null,"cites":null},{"id":832670,"title":"Learn++: an incremental learning algorithm for supervised neural networks,\u201d","authors":[],"date":"2001","doi":null,"raw":null,"cites":null},{"id":830801,"title":"Learning and Memory: An Integrated Approach.","authors":[],"date":"1995","doi":null,"raw":null,"cites":null},{"id":836297,"title":"Learning in the presence of concept drift and hidden contexts,\u201d","authors":[],"date":"1996","doi":null,"raw":null,"cites":null},{"id":837349,"title":"Learning the sequential coordinated behavior of teams from observations,\u201d","authors":[],"date":"2002","doi":null,"raw":null,"cites":null},{"id":834807,"title":"Lvq pak: A program package for the correct application of learning vector quantization algorithms,\u201d","authors":[],"date":"1992","doi":null,"raw":null,"cites":null},{"id":841135,"title":"Machines using sequential minimal optimization,\u201d","authors":[],"date":"1998","doi":null,"raw":null,"cites":null},{"id":829761,"title":"Masquerade detection using truncated command lines,\u201d","authors":[],"date":"2002","doi":null,"raw":null,"cites":null},{"id":837882,"title":"Mining sequential patterns,\u201d","authors":[],"date":"1995","doi":null,"raw":null,"cites":null},{"id":840542,"title":"Nearest neighbor pattern classification,\u201d","authors":[],"date":"1967","doi":null,"raw":null,"cites":null},{"id":836568,"title":"On behavior classification in adversarial environments,\u201d","authors":[],"date":"2000","doi":null,"raw":null,"cites":null},{"id":840194,"title":"Pattern Classification and Scene Analysis.","authors":[],"date":"1973","doi":null,"raw":null,"cites":null},{"id":831152,"title":"Removing biases in unsupervised learning of sequential patterns,\u201d","authors":[],"date":"2007","doi":null,"raw":null,"cites":null},{"id":841389,"title":"Self-Organizing Maps.","authors":[],"date":"2001","doi":null,"raw":null,"cites":null},{"id":837089,"title":"Sequence classification using statistical pattern recognition.\u201d","authors":[],"date":"2007","doi":null,"raw":null,"cites":null},{"id":838884,"title":"Simpl ets: a simplified method for learning evolving takagi-sugeno fuzzy models,\u201d","authors":[],"date":"2005","doi":null,"raw":null,"cites":null},{"id":831403,"title":"Temporal sequence learning and data reduction for anomaly detection,\u201d","authors":[],"date":"1998","doi":null,"raw":null,"cites":null},{"id":836826,"title":"Trie memory,\u201d","authors":[],"date":"1960","doi":null,"raw":null,"cites":null},{"id":830259,"title":"User profiling for computer security,\u201d","authors":[],"date":"2004","doi":null,"raw":null,"cites":null},{"id":830535,"title":"User profiling for web page filtering,\u201d","authors":[],"date":"2005","doi":null,"raw":null,"cites":null},{"id":829334,"title":"User profiling in personal information agents: a survey,\u201d","authors":[],"date":"2005","doi":null,"raw":null,"cites":null},{"id":839433,"title":"Using unix: Collected traces of 168 users,\u201d","authors":[],"date":"1988","doi":null,"raw":null,"cites":null}],"documentType":{"type":null}},"contributors":[],"datePublished":"2012-05-01","abstract":"Knowledge about computer users is very beneficial for assisting them, predicting their future actions or detecting masqueraders. In this paper, a new approach for creating and recognizing automatically the behavior profile of a computer user is presented. In this case, a computer user behavior is represented as the sequence of the commands (s)he types during her\/his work. This sequence is transformed into a distribution of relevant subsequences of commands in order to find out a profile that defines its behavior. Also, because a user profile is not necessarily fixed but rather it evolves\/changes, we propose an evolving method to keep up to date the created profiles using an Evolving Systems approach. In this paper we combine the evolving classifier with a trie-based user profiling to obtain a powerful self-learning on-line scheme. We also develop further the recursive formula of the potential of a data point to become a cluster center using cosine distance, which is provided in the Appendix. The novel approach proposed in this paper can be applicable to any problem of dynamic\/evolving user behavior modeling where it can be represented as a sequence of actions or events. It has been evaluated on several real data streams","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:34353<\/identifier><datestamp>\n      2018-01-24T03:03:17Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Creating evolving user behavior profiles automatically<\/dc:title><dc:creator>\n        Iglesias, Jose<\/dc:creator><dc:creator>\n        Angelov, Plamen<\/dc:creator><dc:creator>\n        Ledezma, Agapito<\/dc:creator><dc:creator>\n        Sanchis, Aracheli<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        Knowledge about computer users is very beneficial for assisting them, predicting their future actions or detecting masqueraders. In this paper, a new approach for creating and recognizing automatically the behavior profile of a computer user is presented. In this case, a computer user behavior is represented as the sequence of the commands (s)he types during her\/his work. This sequence is transformed into a distribution of relevant subsequences of commands in order to find out a profile that defines its behavior. Also, because a user profile is not necessarily fixed but rather it evolves\/changes, we propose an evolving method to keep up to date the created profiles using an Evolving Systems approach. In this paper we combine the evolving classifier with a trie-based user profiling to obtain a powerful self-learning on-line scheme. We also develop further the recursive formula of the potential of a data point to become a cluster center using cosine distance, which is provided in the Appendix. The novel approach proposed in this paper can be applicable to any problem of dynamic\/evolving user behavior modeling where it can be represented as a sequence of actions or events. It has been evaluated on several real data streams.<\/dc:description><dc:date>\n        2012-05-01<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/TKDE.2011.17<\/dc:relation><dc:identifier>\n        Iglesias, Jose and Angelov, Plamen and Ledezma, Agapito and Sanchis, Aracheli (2012) Creating evolving user behavior profiles automatically. IEEE Transactions on Knowledge and Data Engineering, 24 (5). pp. 854-867. ISSN 1041-4347<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/34353\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1109\/TKDE.2011.17","http:\/\/eprints.lancs.ac.uk\/34353\/"],"year":2012,"topics":["QA75 Electronic computers. Computer science"],"subject":["Journal Article","PeerReviewed"],"fullText":"IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING , VOL., NO., JUNE 2010 1\nCreating Evolving User Behavior Profiles\nAutomatically\nJose Antonio Iglesias, Plamen Angelov, Agapito Ledezma and Araceli Sanchis\nAbstract\u2014Knowledge about computer users is very beneficial for assisting them, predicting their future actions or detecting\nmasqueraders. In this paper, a new approach for creating and recognizing automatically the behavior profile of a computer user is\npresented. In this case, a computer user behavior is represented as the sequence of the commands (s)he types during her\/his work.\nThis sequence is transformed into a distribution of relevant subsequences of commands in order to find out a profile that defines its\nbehavior. Also, because a user profile is not necessarily fixed but rather it evolves\/changes, we propose an evolving method to keep up\nto date the created profiles using an Evolving Systems approach.\nIn this paper we combine the evolving classifier with a trie-based user profiling to obtain a powerful self-learning on-line scheme. We\nalso develop further the recursive formula of the potential of a data point to become a cluster center using cosine distance, which\nis provided in the Appendix. The novel approach proposed in this paper can be applicable to any problem of dynamic\/evolving user\nbehavior modeling where it can be represented as a sequence of actions or events. It has been evaluated on several real data streams.\nIndex Terms\u2014Evolving fuzzy systems, fuzzy-rule-based (FRB) classifiers, user modeling.\nF\n1 INTRODUCTION\nR ECOGNIZING the behavior of others in real-time is asignificant aspect of different human tasks in many\ndifferent environments. When this process is carried out by\nsoftware agents or robots, it is known as user modeling. The\nrecognition of users can be very beneficial for assisting them\nor predicting their future actions. Most existing techniques\nfor user recognition assume the availability of hand-crafted\nuser profiles, which encode the a-priori known behavioral\nrepertoire of the observed user. However, the construction of\neffective user profiles is a difficult problem for different rea-\nsons: human behavior is often erratic, and sometimes humans\nbehave differently because of a change in their goals. This\nlast problem makes necessary that the user profiles we create\nevolve.\nThere exists several definitions for user profile [1]. It can be\ndefined as the description of the user interests, characteristics,\nbehaviors and preferences. User profiling is the practice of\ngathering, organizing and interpreting the user profile infor-\nmation. In recent years, significant work has been carried out\nfor profiling users; but most of the user profiles do not change\naccording to the environment and new goals of the user. An\nexample of how to create these static profiles is proposed in\na previous work [2].\n\u2022 Plamen Angelov is with the Department of Communication Systems,\nInfoLab21, Lancaster University, UK. E-mail:p.angelov@lancaster.ac.uk.\n\u2022 Jose A. Iglesias, Agapito Ledezma and Araceli Sanchis are with\nthe CAOS Group, Carlos III University of Madrid, Spain. E-\nmails:{jiglesia,ledezma,masm}@inf.uc3m.es.\n\u2022 This work is partially supported by the Spanish Government under project\nTRA2007-67374-C02-02.\nIn this paper, we propose an adaptive approach for\ncreating behavior profiles and recognizing computer users.\nWe call this approach EVABCD (Evolving Agent behavior\nClassification based on Distributions of relevant events) and\nit is based on representing the observed behavior of an agent\n(computer user) as an adaptive distribution of her\/his relevant\natomic behaviors (events). Once the model has been created,\nEVABCD presents an evolving method for updating and\nevolving the user profiles and classifying an observed user.\nThe approach we present is generalizable to all kinds of user\nbehaviors represented by a sequence of events.\nThe UNIX operating system environment is used in this\nresearch for explaining and evaluating EVABCD. A user\nbehavior is represented in this case by a sequence of UNIX\ncommands typed by a computer user in a command-line\ninterface. Previous research studies in this environment [3],\n[4] focus on detecting masquerades (individuals who imper-\nsonate other users on computer networks and systems) from\nsequences of UNIX commands. However, EVABCD creates\nevolving user profiles and classifies new users into one of the\npreviously created profiles. Thus, the goal of EVABCD in the\nUNIX environment can be divided into two phases:\n1) Creating and updating user profiles from the commands\nthe users typed in a UNIX shell.\n2) Classifying a new sequence of commands into the prede-\nfined profiles.\nBecause we use an evolving classifier, it is constantly\nlearning and adapting the existing classifier structure to ac-\ncommodate the newly observed emerging behaviors. Once a\nuser is classified, relevant actions can be done, however this\ntask is not addressed in this paper.\nThe creation of a user profile from a sequence of UNIX\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING , VOL., NO., JUNE 2010 2\ncommands should consider the consecutive order of the\ncommands typed by the user and the influence of his\/her\npast experiences. This aspect motivates the idea of automated\nsequence learning for computer user behavior classification;\nif we do not know the features that influence the behavior\nof a user, we can consider a sequence of past actions\nto incorporate some of the historical context of the user.\nHowever, it is difficult, or in general, impossible, to build\na classifier that will have a full description of all possible\nbehaviors of the user, because these behaviors evolve with\ntime, they are not static and new patterns may emerge as well\nas an old habit may be forgotten or stopped to be used. The\ndescriptions of a particular behavior itself may also evolve,\nso we assume that each behavior is described by one or more\nfuzzy rules. A conventional system do not capture the new\npatterns (behaviors) that could appear in the data stream once\nthe classifier is built. In addition, the information produced\nby a user is often quite large. Therefore, we need to cope\nwith large amounts of data and process this information in\nreal time, because storing the complete data set and analyzing\nit in an off-line (batch) mode, would be impractical. In\norder to take into account these aspects, we use an evolving\nfuzzy-rule-based system that allows for the user behaviors to\nbe dynamic, to evolve.\nThis paper is organized as follows: Section 2 provides an\noverview of the background and related work. The overall\nstructure of our approach, EVABCD, is explained in section\n3. Section 4 describes the construction of the user behavior\nprofile. The evolving UNIX user classifier is detailed in\nSection 5. Section 6 describes the experimental setting and\nthe experimental results obtained. Finally, Section 7 contains\nfuture work and concluding remarks.\n2 BACKGROUND AND RELATED WORK\nDifferent techniques have been used to find out relevant\ninformation related to the human behavior in many different\nareas. The literature in this field is vast; Macedo et al. [5]\npropose a system (WebMemex) that provides recommended\ninformation based on the captured history of navigation from\na list of known users. Pepyne et al. [6] describe a method using\nqueuing theory and logistic regression modeling methods for\nprofiling computer users based on simple temporal aspects of\ntheir behavior. In this case, the goal is to create profiles for\nvery specialized groups of users, who would be expected to use\ntheir computers in a very similar way. Gody and Amandi [7]\npresent a technique to generate readable user profiles that\naccurately capture interests by observing their behavior on the\nWeb.\nThere is a lot of work focusing on user profiling in a\nspecific environment, but it is not clear that they can be\ntransferred to other environments. However, the approach\nwe propose in this paper can be used in any domain in\nwhich a user behavior can be represented as a sequence\nof actions or events. Because sequences play a crucial role\nin human skill learning and reasoning [8], the problem\nof user profile classification is examined as a problem of\nsequence classification. According to this aspect, Horman\nand Kaminka [9] present a learner with unlabeled sequential\ndata that discovers meaningful patterns of sequential behavior\nfrom example streams. Popular approaches to such learning\ninclude statistical analysis and frequency based methods.\nLane and Brodley [10] present an approach based on the\nbasis of instance-based learning (IBL) techniques, and several\ntechniques for reducing data storage requirements of the user\nprofile.\nAlthough the proposed approach can be applied to any\nbehavior represented by a sequence of events, we focus on\nthis research in a command-line interface environment. Related\nto this environment, Schonlau et al. [3] investigate a number\nof statistical approaches for detecting masqueraders. Coull\net al. [11] propose an effective algorithm that uses pair-\nwise sequence alignment to characterize similarity between\nsequences of commands. Recently, Angelov and Zhou propose\nin [12] to use evolving fuzzy classifiers for this detection task.\nIn [13], Panda et al. compared the performance of different\nclassification algorithms - Naive Bayesian (NB), C4.5 and\nIterative Dichotomizer 3 (ID3) - for network intrusion\ndetection. According to the authors, ID3 and C4.5 are\nrobust in detecting new intrusions, but NB performs better\nthan all in terms of classification accuracy. Cufoglu et\nal. [14] evaluated the classification accuracy of NB, IB1,\nSimpleCART, NBTree, ID3, J48 and SMO algorithms with\nlarge user profile data. According to the simulation results,\nNBTree classifier performs best on user related information.\nIt should be emphasized that all of the above approaches\nignore the fact that user behaviors can change and evolve.\nHowever, this aspect needs to be taken into account in the\nproposed approach. In addition, owing to the characteristics\nof the proposed environment, we need to extract some sort\nof knowledge from a continuous stream of data. Thus, it\nis necessary that the approach deals with the problem of\nclassification of streaming data. Incremental algorithms build\nand refine the model at different points in time, in contrast\nto the traditional algorithms which train the model in a batch\nmanner. It is more efficient to revise an existing hypothesis\nthan it is to generate hypothesis each time a new instance\nis observed. Therefore, one possible solution to the proposed\nscenario is to use an incremental classifiers.\nAn incremental learning algorithm can be defined as one\nthat meets the following criteria [15]:\n1) It should be able to learn additional information from new\ndata.\n2) It should not require access to the original data, used to\ntrain the existing classifier.\n3) It should preserve previously acquired knowledge.\n4) It should be able to accommodate new classes that may\nbe introduced with new data.\nSeveral incremental classifiers have been implemented using\ndifferent frameworks:\n\u2022 Decision trees. The problem of processing streaming\ndata in on-line has motivated the development of many\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING , VOL., NO., JUNE 2010 3\nalgorithms which were designed to learn decision trees in-\ncrementally [16], [17]. Some examples of the algorithms\nwhich construct incremental decision trees are: ID4 [18],\nID5 [19] and ID5R [20].\n\u2022 Artificial Neural Networks (ANN). Adaptive Resonance\nTheory (ART) networks [21] are unsupervised ANNs\nproposed by Carpenter that dynamically determine the\nnumber of clusters based on a vigilance parameter [22]. In\naddition, Kasabov proposed another incremental learning\nneural network architecture, called Evolving Fuzzy Neu-\nral Network (EFuNN) [23]. This architecture does not\nrequire access to previously seen data and can accommo-\ndate new classes. A new approach to incremental learning\nusing evolving neural networks is proposed by Seipone\net al [24]. This approach uses an evolutionary algorithm\nto evolve some MLP parameters. This process aims at\ndetermining the parameters to produce networks with\nbetter incremental abilities. However, it performs this\nparameter optimization task off-line, based on separate\ntesting and validating data sets.\n\u2022 Prototype-based supervised algorithms. Learning Vector\nQuantization (LVQ) is one of the well-known nearest pro-\ntotype learning algorithms [25]. LVQ can be considered\nto be a supervised clustering algorithm, in which each\nweight vector can be interpreted as a cluster center. Using\nthis algorithm, the number of reference vectors has to be\nset by the user. For this reason, Poirier et al. proposed\na method to generate new prototypes dynamically. This\nincremental LVQ is known as DVQ (Dynamic Vector\nQuantization) [26]. However, this method lacks the gen-\neralization capability, resulting in the generation of many\nprototype neurons for applications with noisy data.\n\u2022 Bayesian. Bayesian classifier is an effective methodology\nfor solving classification problems when all features are\nconsidered simultaneously. However, when the features\nare added one by one in Bayesian classifier in batch\nmode in forward selection method huge computation is\ninvolved. For this reason, Agrawal et al. [27] proposed\nan incremental Bayesian classifier for multivariate nor-\nmal distribution datasets. Several incremental versions of\nBayesian classifiers are proposed in [28]. They work well\nwith data which have normal distributions or close to\nnormal ones.\n\u2022 SVM. A Support Vector Machine (SVM) performs\nclassification by constructing a N-dimensional hyperplane\nthat optimally separates the data into two categories.\nTraining an SVM \u201cincrementally\u201d on new data by\ndiscarding all previous data except their support vectors,\ngives only approximate results. Cauwenberghs et al. [29]\nconsider incremental learning as an exact on-line method\nto construct the solution recursively, one point at a time.\nIn addition, Xiao et al. [30] propose an incremental\nalgorithm which utilizes the properties of SV set, and\naccumulates the distribution knowledge of the sample\nspace through some adjustable parameters. However,\nthis provides an approximation of the optimal separation\nonly and works well for normally distributed data.\nHowever, as this research focus on a command-line\ninterface environment, it is necessary the approach to be able\nto process streaming data in real-time and also cope with huge\namounts of data efficiently. Several incremental classifiers do\nnot considered this last aspect. In addition, the structure of the\nincremental classifiers is usually not transparent\/interpretable\nand is assumed to be fixed, and they can not address the\nproblem of so called concept drift and shift [31]. By drift\nthey refer to a modification of the concept over time, and shift\nusually refers to sudden and abrupt changes in the streaming\ndata. To capture these changes, it is necessary not only to\ntune parameters of the classifiers, but also to change their\nstructure. A simple incremental algorithm does not evolve\nthe structure of the classifier. The interpretation of the results\nis also an important characteristic in a classifier, and several\nincremental classifiers (such as ANN or SVM) are not good\nin terms of interpretation of the results. They are very much\na \u201cblack box\u201d type of approach which performs \u201cnumber\nfitting\u201d. Finally, the computational efficiency of the proposed\nclassifier is very important, and some incremental algorithms\n(such SVM) have to solve quadratic optimization problems\nmany times.\nTaking all these aspects into account, we propose in this\npaper an evolving fuzzy-rule-base system which satisfies all\nof the criteria of the incremental classifiers. However, the\napproach has also important advantages which make it very\nuseful in real environments:\n1) It can cope with huge amounts of data very efficiently\nand its complexity is linear;\n2) Its evolving structure can capture sudden and abrupt\nchanges in the stream of data efficiently;\n3) Its structure meaning is very clear, as it is a rule-based\nclassifier;\n4) It is non-iterative and single pass; therefore, it is compu-\ntationally very efficient and fast;\n5) Its structure is simple and interpretable (human-\nintelligible).\nThus, we present an approach for creating and recognizing\nautomatically the behavior profile of a computer user with the\ntime evolving structure in a very efficient way. To the best of\nour knowledge, this is the first publication where user behavior\nis considered, treated and modeled as a dynamic and evolving\nphenomenon. This is the most important contribution of this\npaper.\n3 THE PROPOSED APPROACH\nThis section introduces the proposed approach for automatic\nclustering, classifier design and classification of the behavior\nprofiles of users. The novel evolving user behavior classifier is\nbased on Evolving Fuzzy Systems and it takes into account the\nfact that the behavior of any user is not fixed, but is rather\nchanging. Although the proposed approach can be applied\nto any behavior represented by a sequence of events, we\ndetail it using a command-line interface (UNIX commands)\nenvironment.\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING , VOL., NO., JUNE 2010 4\nIn order to classify an observed behavior, our approach, as\nmany other agent modeling methods [32], creates a library\nwhich contains the different expected behaviors. However, in\nour case this library is not a pre-fixed one, but is evolving,\nlearning from the observations of the users real behaviors\nand, moreover, it starts to be filled in \u2019from scratch\u2019 by\nassigning temporarily to the library the first observed user\nas a prototype. The library, called Evolving-Profile-Library\n(EPLib), is continuously changing, evolving influenced by the\nchanging user behaviors observed in the environment.\nThus, the proposed approach includes at each step the\nfollowing two main actions:\n1) Creating and Evolving the classifier: This action in-\nvolves in itself two sub-actions:\na) Creating the user behavior profiles. This sub-action\nanalyzes the sequences of commands typed by different\nUNIX users on-line (data stream), and creates the cor-\nresponding profiles. This process is detailed in Section\n4.\nb) Evolving the Classifier. This sub-action includes on-\nline learning and update of the classifier, including the\npotential of each behavior to be a prototype, stored in\nthe EPLib. The whole process is explained in Section\n5.\n2) User Classification: The user profiles created in the\nprevious action are associated with one of the prototypes\nfrom the EPLib, and they are classified into one of the\nclasses formed by the prototypes. This action is also\ndetailed in Section 5.\n4 CONSTRUCTION OF THE USER BEHAVIOR\nPROFILE\nIn order to construct a user behavior profile in on-line mode\nfrom a data stream, we need to extract an ordered sequence\nof recognized events; in this case, UNIX commands. These\ncommands are inherently sequential, and this sequentiality is\nconsidered in the modeling process. According to this aspect\nand based on the work done in [2], in order to get the\nmost representative set of subsequences from a sequence, we\npropose the use of a trie data structure [33]. This structure was\nalso used in [34] to classify different sequences and in [35],\n[36] to classify the behavior patterns of a RoboCup soccer\nsimulation team.\nThe construction of a user profile from a single sequence\nof commands is done by a three step process:\nA. Segmentation of the sequence of commands.\nB. Storage of the subsequences in a trie.\nC. Creation of the user profile.\nThese steps are detailed in the following three subsections.\nFor the sake of simplicity, let us consider the following\nsequence of commands as an example: {ls \u2192 date \u2192 ls \u2192\ndate \u2192 cat}.\n4.1 A. Segmentation of the sequence of commands\nFirst, the sequence is segmented into subsequences of equal\nlength from the first to the last element. Thus, the sequence\nA=A1A2...An (where n is the number of commands of the\nsequence) will be segmented in the subsequences described by\nAi...Ai+length \u2200 i,i=[1,n-length+1], where length is the size of\nthe subsequences created. In the remainder of the paper, we\nwill use the term subsequence length to denote the value of\nthis length. This value determines how many commands are\nconsidered as dependent.\nIn the proposed sample sequence ({ ls\u2192 date\u2192 ls\u2192 date\n\u2192 cat}), let 3 be the subsequence length, then we obtain:\n{ls\u2192 date\u2192 ls}, {date\u2192 ls\u2192 date}, {ls\u2192 date\u2192 cat}\n4.2 B. Storage of the subsequences in a trie\nThe subsequences of commands are stored in a trie data-\nstructure. When a new model needs to be constructed, we\ncreate an empty trie, and insert each sub-sequence of events\ninto it, such that all possible subsequences are accessible\nand explicitly represented. Every trie-node represents an event\nappearing at the end of a sub-sequence, and the nodes chil-\ndren represent the events that have appeared following this\nevent. Also, each node keeps track of the number of times a\ncommand has been recorded into it. When a new subsequence\nis inserted into a trie, the existing nodes are modified and\/or\nnew nodes are created. As the dependencies of the commands\nare relevant in the user profile, the subsequence suffixes\n(subsequences that extend to the end of the given sequence)\nare also inserted.\nConsidering the previous example, the first subsequence ({ls\n\u2192 date \u2192 ls}) is added as the first branch of the empty trie\n(Figure 1 a). Each node is labeled with the number 1 which\nindicates that the command has been inserted in the node once\n(in Figure 1, this number is enclosed in square brackets). Then,\nthe suffixes of the subsequence ({date \u2192 ls} and {ls}) are\nalso inserted (Figure 1 b). Finally, after inserting the three\nsubsequences and its corresponding suffixes, the completed\ntrie is obtained (Figure 1 c).\n4.3 C. Creation of the user profile\nOnce the trie is created, the subsequences that characterize\nthe user profile and its relevance are calculated by traversing\nthe trie. For this purpose, frequency-based methods are\nused. In particular, in EVABCD, to evaluate the relevance\nof a subsequence, its relative frequency or support [37] is\ncalculated. In this case, the support of a subsequence is defined\nas the ratio of the number of times the subsequence has been\ninserted into the trie and the total number of subsequences of\nequal size inserted.\nIn this step, the trie can be transformed into a set of\nsubsequences labeled by its support value. In EVABCD this\nset of subsequences is represented as a distribution of relevant\nsubsequences. Thus, we assume that user profiles are n-\ndimensional matrices, where each dimension of the matrix will\nrepresent a particular subsequence of commands.\nIn the previous example, the trie consists of nine nodes;\ntherefore, the corresponding profile consists of nine different\nsubsequences which are labeled with its support. Figure 2\nshows the distribution of these subsequences.\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING , VOL., NO., JUNE 2010 5\nFig. 1. Steps of creating an example trie.\nFig. 2. Distribution of subsequences of commands - Example.\nOnce a user behavior profile has been created, it is classified\nand used to update the Evolving-Profile-Library (EPLib), as\nexplained in the next section.\n5 EVOLVING UNIX USER CLASSIFIER\nA classifier is a mapping from the feature space to the class\nlabel space. In the proposed classifier, the feature space is\ndefined by distributions of subsequences of events. On the\nother hand, the class label space is represented by the most\nrepresentative distributions. Thus, a distribution in the class\nlabel space represents a specific behavior which is one of the\nprototypes of the EPLib. The prototypes are not fixed and\nevolve taking into account the new information collected on-\nline from the data stream - this is what makes the classifier\nEvolving. The number of these prototypes is not pre-fixed but\nit depends on the homogeneity of the observed behaviors.\nThe following subsections describes how a user behavior is\nrepresented by the proposed classifier, and how this classifier\nis created in an evolving manner.\n5.1 User behavior representation\nEVABCD receives observations in real-time from the envi-\nronment to analyze. In our case, these observations are UNIX\ncommands and they are converted into the corresponding\ndistribution of subsequences on-line. In order to classify a\nUNIX user behavior, these distributions must be represented\nin a data space. For this reason, each distribution is considered\nas a data vector that defines a point that can be represented in\nthe data space.\nThe data space in which we can represent these points\nshould consist of n dimensions, where n is the number of\nthe different subsequences that could be observed. It means\nthat we should know all the different subsequences of the\nenvironment a priori. However, this value is unknown and\nthe creation of this data space from the beginning is not\nefficient. For this reason, in EVABCD, the dimension of the\ndata space also evolves, it is incrementally growing according\nto the different subsequences that are represented in it.\nFigure 3 explains graphically this novel idea. In this exam-\nple, the distribution of the first user consists of 5 subsequences\nof commands (ls, ls-date, date, cat and date-cat), therefore we\nneed a 5 dimensional data space to represent this distribution\nbecause each different subsequence is represented by one\ndimension. If we consider the second user, we can see that\n3 of the 5 previous subsequences have not been typed by\nthis user (ls-date, date and date-cat), so these values are not\navailable. Also, the values of the 2 new subsequences (cp and\nls-cp) need to be represented in the same data space, thus its\nis necessary to increase the dimensionality of the data space\nfrom 5 to 7. To sum up, the dimensions of the data space\nrepresent the different subsequences typed by the users and\nthey will increase according to the different new subsequences\nobtained.\nFig. 3. Distributions of subsequences of events in an\nevolving system approach - Example\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING , VOL., NO., JUNE 2010 6\n5.2 Structure of the EvABCD\nOnce the corresponding distribution has been created from the\nstream, it is processed by the classifier. The structure of this\nclassifier includes:\n1) Classify the new sample in a class represented by a\nprototype. Section 5.6.\n2) Calculate the potential of the new data sample to be a\nprototype. Section 5.3.\n3) Update all the prototypes considering the new data\nsample. It is done because the density of the data space\nsurrounding certain data sample changes with the in-\nsertion of each new data sample. Insert the new data\nsample as a new prototype if needed. Section 5.4\n4) Remove any prototype if needed. Section 5.5\nTherefore, as we can see, the classifier does not need\nto be configured according to the environment where it is\nused because it can start \u2019from scratch\u2019. Also, the relevant\ninformation of the obtained samples is necessary to update\nthe library; but, as we will explain in the next subsection,\nthere is no need to store all the samples in it.\n5.3 Calculating the potential of a data sample\nAs in [12], a prototype is a data sample (a behavior repre-\nsented by a distribution of subsequences of commands) that\nrepresents several samples which represent a certain class.\nThe classifier is initialized with the first data sample, which\nis stored in EPLib. Then, each data sample is classified to one\nof the prototypes (classes) defined in the classifier. Finally,\nbased on the potential of the new data sample to become a\nprototype [38], it could form a new prototype or replace an\nexisting one.\nThe potential (P) of the kth data sample (xk) is calculated\nby the equation (1) which represents a function of the accumu-\nlated distance between a sample and all the other k-1 samples\nin the data space [12]. The result of this function represents\nthe density of the data that surrounds a certain data sample.\nP (xk) =\n1\n1 +\n\u2211k\u22121\ni=1 distance\n2(xk,xi)\nk\u22121\n(1)\nwhere distance represents the distance between two samples\nin the data space.\nIn [39] the potential is calculated using the euclidean\ndistance and in [12] it is calculated using the cosine distance.\nCosine distance has the advantage that it tolerates different\nsamples to have different number of attributes; in this case, an\nattribute is the support value of a subsequence of commands.\nAlso, cosine distance tolerates that the value of several subse-\nquences in a sample can be null (null is different than zero).\nTherefore, EVABCD uses the cosine distance (cosDist) to\nmeasure the similarity between two samples; as it is described\nin equation (2).\ncosDist(xk, xp) = 1\u2212\n\u2211n\nj=1 xkjxpj\n\u221a\u2211n\nj=1 x\n2\nkj\n\u2211n\nj=1 x\n2\npj\n(2)\nwhere xk and xp represent the two samples to measure its\ndistance and n represents the number of different attributes in\nboth samples.\n5.3.1 Calculating the Potential Recursively\nNote that the expression in the equation (1) requires all the\naccumulated data sample available to be calculated, which\ncontradicts to the requirement for real-time and on-line ap-\nplication needed in the proposed approach. For this reason,\nwe need to develop a recursive expression of the potential, in\nwhich it is not needed to store the history of all the data.\nIn equation (3), the potential is calculated in the input-output\njoint data space, where z=[x, Label]; therefore, the kth data\nsample (xk) with its corresponding label is represented as zk.\nEach sample is represented by a set of values; the value of the\nith attribute (element) of the zk sample is represented as zik.\nAs it is detailed in the Appendix, this derivation is proposed\nas a novel contribution and the result is as follows:\nPk(zk) =\n1\n2 + [ 1h(k\u22121) [[\u22122BK ] + [\n1\nhDk]]]\nk = 2, 3...;P1(z1) = 1\nwhere :\nBk =\nn\u2211\nj=1\nzjkb\nj\nk ; b\nj\nk = b\nj\n(k\u22121) +\n\u221a\n(zjk)\n2\n\u2211n\nl=1(z\nl\nk)\n2\nbj1 =\n\u221a\n(zj1)2\u2211n\nl=1(z\nl\n1)2\n; j = [1, n+ 1]\nDk =\nn\u2211\nj=1\nzjk\nn\u2211\np=1\nzpkd\njp\nk ; d\njp\nk = d\njp\n(k\u22121) +\nzjkz\np\nk\u2211n\nl=1(z\nl\nk)\n2\ndj11 =\nzj1z\n1\n1\u2211n\nl=1(z\nl\n1)2\n; j = [1, n+ 1]\n(3)\nwhere dijk represents this accumulated value for the k\nth\ndata sample considering the multiplication of the ith and jth\nattributes of the data sample. Thus, to get recursively the\nvalue of the potential of a sample using the equation (1), it is\nnecessary to calculate kxk different accumulated values which\nstore the result of the multiplication of an attribute value of\nthe data sample by all the other attribute values of the data\nsample.\nHowever, since the number of needed accumulated values\nis very large, we simplify this expression in order to calculate\nit faster and with less memory.\n5.3.2 Simplifying the Potential expression\nIn our particular application, the data are represented by a\nset of support values and are thus positive. To simplify the\nrecursive calculation of the expression (1), we can use simply\nthe distance instead of square of the distance. For this reason,\nwe use in this case the equation (4) instead of (1).\nPk(zk) =\n1\n1 +\n\u2211k\u22121\ni=1 cosDist(xk,xi)\nk\u22121\n(4)\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING , VOL., NO., JUNE 2010 7\nUsing the equation (4), we develop a recursive expression\nsimilar to the recursive expressions developed in [38], [39]\nusing euclidean distance and in [12] using cosine distance.\nThis formula is as follows:\nPk(zk) =\n1\n2\u2212 1k\u22121\n1\u221a\u2211n\nj=1(z\nj\nk)\n2\nBk\n; k = 2, 3, ...;P1(z1) = 1\nwhere :\nBk =\nn\u2211\nj=1\nzjkb\nj\nk ; b\nj\nk = b\nj\n(k\u22121) +\n\u221a\n(zjk)\n2\n\u2211n\nl=1(z\nl\nk)\n2\nbj1 =\n\u221a\n(zj1)2\u2211n\nl=1(z\nl\n1)2\n; j = [1, n+ 1]\n(5)\nUsing this expression, it is only necessary to calculate (n+1)\nvalues where n is the number of different obtained subse-\nquences; this value is represented by b, where bjk, j = [1, n]\nrepresents the accumulated value for the kth data sample.\n5.4 Creating new prototypes\nThe proposed evolving user behavior classifier, EVABCD,\ncan start \u2019from scratch\u2019 (without prototypes in the library)\nin a similar manner as eClass evolving fuzzy rule-based\nclassifier proposed in [39], used in [40] for robotics and further\ndeveloped in [12]. The potential of each new data sample is\ncalculated recursively and the potential of the other prototypes\nis updated. Then, the potential of the new sample (zk) is\ncompared with the potential of the existing prototypes. A new\nprototype is created if its value is higher than any other existing\nprototype, as shown in equation (6).\n\u2203i, i = [1, NumPrototypes] : P (zk) > P (Proti) (6)\nThus, if the new data sample is not relevant, the overall\nstructure of the classifier is not changed. Otherwise, if the\nnew data sample has high descriptive power and generalization\npotential, the classifier evolves by adding a new prototype\nwhich represents a part of the observed data samples.\n5.5 Removing existing prototypes\nAfter adding a new prototype, we check whether any of\nthe already existing prototypes are described well by the\nnewly added prototype [12]. By well we mean that the value\nof the membership function that describes the closeness to\nthe prototype is a Gaussian bell function chosen due to its\ngeneralization capabilities:\n\u2203i, i = [1, NumPrototypes] : \u00b5i(zk) > e\n\u22121 (7)\nFor this reason, we calculate the membership function\nbetween a data sample and a prototype which is defined as:\n\u00b5i(zk) = e\n\u2212 12 [\ncosDist(zk,Proti)\n\u03c3i\n] , i = [1, NumPrototypes]\n(8)\nwhere cosDist(zk, P roti) represents the cosine distance\nbetween a data sample (zk) and the ith prototype (Proti); \u03c3i\nrepresents the spread of the membership function, which also\nsymbolizes the radius of the zone of influence of the prototype.\nThis spread is determined based on the scatter [41], [42] of\nthe data. The equation to get the spread of the kth data sample\nis defined as:\n\u03c3i(k) =\n\u221a\n\u221a\n\u221a\n\u221a 1\nk\nk\u2211\nj=1\ncosDist(Proti, zk) ; \u03c3i(0) = 1 (9)\nwhere k is the number of data samples inserted in the data\nspace; cosDist(Proti, zk) is the cosine distance between the\nnew data sample (zk) and the ith prototype.\nHowever, to calculate the scatter without storing all the\nreceived samples, this value can be updated recursively (as\nshown in [39]) by:\n\u03c3i(k) =\n=\n\u221a\n[\u03c3i(k \u2212 1)]2 +\n1\nk\n[cosDist2(Proti, zk)\u2212 [\u03c3i(k \u2212 1)]2]\n(10)\n5.6 Classification Method\nIn order to classify a new data sample, we compare it with\nall the prototypes stored in EPLib. This comparison is done\nusing cosine distance and the smallest distance determines the\nclosest similarity. This comparison is shown in equation (11).\nClass(xz) = Class(Prot\n\u2217);\nProt\u2217 =MINNumProti=1 (cosDist(xPrototypei , xz))\n(11)\nThe time-consumed for classifying a new sample depends\non the number of prototypes and its number of attributes.\nHowever, we can consider, in general terms, that both the time-\nconsumed and the computational complexity are reduced and\nacceptable for real-time applications (in order of milliseconds\nper data sample).\n5.7 Supervised and unsupervised learning\nThe proposed classifier has been explained taking into account\nthat the observed data samples do not have labels; thus, using\nunsupervised learning. In this case, the classes are created\nbased on the existing prototypes and, thus, any prototype\nrepresents a different class (label). Such a technique is used\nfor example in eClass0 [12], [39] which is a clustering-based\nclassification.\nHowever, the observed data samples can have a label\nassigned to them a priori. In this case, using EVABCD, a\nspecific class is represented by several prototypes, because\nthe previous process is done for each different class. Thus,\neach class has a number of prototypes that depends on how\nheterogeneous are the samples of the same class. In this case,\na new data sample is classified in the class which belongs\nthe closest prototype. This technique is used for example in\neClass1 [12], [39].\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING , VOL., NO., JUNE 2010 8\n6 EXPERIMENTAL SETUP AND RESULTS\nIn order to evaluate EVABCD in the UNIX environment,\nwe use a data set with the UNIX commands typed by 168\nreal users and labeled in four different groups. Therefore, in\nthese experiments, we use supervised learning. The explained\nprocess is applied for each of the four group (classes) and one\nor more prototypes will be created for each group. EVABCD\nis applied in these experiments considering the data set as\npseudo-on-line streams. However, only using data sets in an\noff-line mode, the proposed classifier can be compared with\nother incremental and non-incremental classifiers.\n6.1 Data Set\nIn these experiments, we use the command-line data collected\nby Greenberg [43] using UNIX csh command interpreter. This\ndata are identified in four target groups which represent a total\nof 168 male and female users with a wide cross-section of\ncomputer experience and needs. Salient features of each group\nare described below:\n\u2022 Novice Programmers: The users of this group had little or\nno previous exposure to programming, operating systems,\nor UNIX-like command-based interfaces. These users\nspent most of their time learning how to program and\nuse the basic system facilities.\n\u2022 Experienced Programmers: These group members were\nsenior Computer Science undergraduates, expected to\nhave a fair knowledge of the UNIX environment. These\nusers used the system for coding, word processing, em-\nploying more advanced UNIX facilities to fulfill course\nrequirements, and social and exploratory purposes.\n\u2022 Computer Scientist: This group, graduates and re-\nsearchers from the Department of Computer Science,\nhad varying experience with UNIX, although all were\nexperts with computers. Tasks performed were less pre-\ndictable and more varied than other groups, research\ninvestigations, social communication, word-processing,\nmaintaining databases, and so on.\n\u2022 Non-programmers: Word-processing and document\npreparation was the dominant activity of the members of\nthis group, made up of office staff and members of the\nFaculty of Environmental Design. Knowledge of UNIX\nwas the minimum necessary to get the job done.\nThe sample sizes (the number of people observed) and the\ntotal number of command lines are indicated in Table 1.\nTABLE 1\nSample group sizes and command lines recorded.\nGroup of users name Sample size Total number\nof command lines\nNovice Programmers 55 77.423\nExper. Programmers 36 74.906\nComputer Scientists 52 125.691\nNon-Programmers 25 25.608\nTotal 168 303.628\n6.2 Comparison with other classifiers\nIn order to evaluate the performance of EVABCD, it is\ncompared with several different types of classifiers. This ex-\nperiment focuses on using the well established batch classifiers\ndescribed as follows:\n\u2022 The C5.0 algorithm [44] is a commercial version of the\nC4.5 [45] decision-tree based classifier.\n\u2022 The Naive Bayes classifier (NB) is a simple probabilistic\nclassifier based on applying Bayes theorem with strong\n(naive) independence assumptions [46]. In this case, the\nnumeric estimator precision values are chosen based on\nthe analysis of the training data. For this reason, the\nclassifier is no-incremental [47].\n\u2022 The K Nearest Neighbor classifier (kNN) is an instance-\nbased learning technique for classifying objects based\non closest training examples in the feature space [48].\nThe parameter k affects the performance of the kNN\nclassifier significantly. In these experiments, the value\nwith which better results are obtained is k=1; thus a\nsample is assigned to the class of its nearest neighbor.\nHowever, using this value, the corresponding classifier\nmay not be robust enough to the noise in the data sample.\nThis classifier also does not have a clear structure.\n\u2022 The AdaBoost (adaptive boosting) classifier [49] com-\nbines multiple weak learners in order to form an efficient\nand strong classifier. Each weak classifier uses a different\ndistribution of training samples. Combining weak clas-\nsifiers take advantage of the so-called instability of the\nweak classifier. This instability causes the classifier to\nconstruct sufficiently different decision surfaces for minor\nmodifications in their training datasets. In these experi-\nments, we use a decision stump as a weak learner. A\ndecision stump is advantageous over other weak learners\nbecause it can be calculated very quickly. In addition,\nthere is a one-to-one correspondence between a weak\nlearner and a feature when a decision stump is used [50].\n\u2022 The Support Vector Machine classifier (SVM) relies\non the statistical learning theory. In this case, the algo-\nrithm SMO (Sequential Minimal Optimization) is used\nfor training support vector machines by breaking a large\nquadratic programming (QP) optimization problem into a\nseries of smallest possible QP problems [51]. In addition,\nin order to determine the best kernel type for this task,\nwe have performed several experiments using polynomial\nkernel function with various degrees and radial basis\nfunction. In this case, the polynomial kernel function\nresults performed best in all experiments. Therefore, we\nuse polynomial kernel function of degree 1 for proposed\nexperiments.\n\u2022 The Learning Vector Quantization classifier (LVQ) is a\nsupervised version of vector quantization, which defines\nclass boundaries based on prototypes, a nearest-neighbor\nrule and a winner-takes-all paradigm. The performance\nof LVQ depends on the algorithm implementation. In\nthese experiments, the enhanced version of LVQ1, the\nOLVQ1 implementation is used [52].\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING , VOL., NO., JUNE 2010 9\nIn addition, we compare EVABCD with two incremental\nclassifiers which share a common theoretical basis with batch\nclassifiers. However, they can be incrementally updated as new\ntraining instances arrive; they do not have to process all the\ndata in one batch. Therefore, most of the incremental learning\nalgorithms can process data files that are too large to fit in\nmemory. The incremental classifiers used in these experiments\nare detailed as follows:\n\u2022 Incremental Naive Bayes. This classifier uses a default\nprecision of 0.1 for numeric attributes when it is created\nwith zero training instances.\n\u2022 Incremental kNN. The kNN classifier adopts an\ninstance-based approach whose conceptual simplicity\nmakes its adaptation to an incremental classifier\nstraightforward. There is an algorithm called incremental\nkNN as the number of nearest neighbors k needs not\nbe known in advance and it is calculated incrementally.\nHowever, in this case, the difference between Incremental\nkNN and non-incremental kNN is the way all the data\nare processed. Unlike other incremental algorithms,\nkNN stores entire dataset internally. In this case, the\nparameter k with which better results are obtained is k=1.\nIn these experiments, the classifiers were trained using a\nfeature vector for each of the 168 users. This vector consists\nof the support value of all the different subsequences of com-\nmands obtained for all the users; thus, there are subsequences\nwhich do not have a value because the corresponding user has\nnot typed those commands. In this case, in order to be able\nto use this data for training the classifiers, we consider this\nvalue as 0 (although its real value is null). In EVABCD we\nuse cosine distance which makes possible to take these values\nas \u2019null\u2019.\n6.3 Experimental Design\nIn order to measure the performance of the proposed classifier\nusing the above data set, the well-established technique of 10-\nfold cross-validation is chosen. Thus, all the users (training\nset) are divided into 10 disjoint subsets with equal size. Each\nof the 10 subsets is left out in turn for evaluation. It should\nbe emphasized that EVABCD does not need to work in this\nmode; this is done mainly in order to have comparable results\nwith the established off-line techniques.\nThe number of UNIX commands typed by a user and used\nfor creating his\/her profile is very relevant in the classification\nprocess. When EVABCD is carried out in the field, the behav-\nior of a user is classified (and the evolving behavior library\nupdated) after s\/he types a limited number of commands.\nIn order to show the relevance of this value using the data\nset already described, we consider different number of UNIX\ncommands for creating the classifier: 100, 500 and 1.000\ncommands per user.\nIn the phase of behavior model creation, the length of the\nsubsequences in which the original sequence is segmented\n(used for creating the trie) is an important parameter: using\nlong subsequences, the time consumed for creating the trie\nand the number of relevant subsequences of the corresponding\ndistribution increase drastically. In the experiments presented\nin this paper, the subsequence length varies from 2 to 6.\nBefore showing the results, we should consider that the\nnumber of subsequences obtained using different streams of\ndata is often very large. To get an idea of how this number\nincreases, Table 2 shows the number of different subsequences\nobtained using different number of commands for training\n(100, 500 and 1.000 commands per user) and subsequence\nlengths (3 and 5).\nTABLE 2\nTotal number of different subsequences obtained.\nNumber of Subsequence Number of\ncommands per user Length different subsequences\n100 3 11.451\n5 34.164\n500 3 39.428\n5 134.133\n1.000 3 63.375\n5 227.715\nUsing EVABCD, the number of prototypes per class is not\nfixed, it varies automatically depending on the heterogeneity\nof the data. To get an idea about it, Table 3 tabulates the\nnumber of prototypes created per group in each of the 10\nruns using 1.000 commands per user as training dataset and a\nsubsequence length of 3.\nTABLE 3\nEvABCD: Number of prototypes created per group using\n10-fold cross-validation\nNumber of prototypes in each of the 10 runs\nGroup 1 2 3 4 5 6 7 8 9 10\nNovice Programmers 4 5 3 4 4 3 2 3 4 5\nExp. Programmers 1 1 1 1 2 3 2 1 1 2\nComputer Scientists 1 1 1 1 1 1 2 2 2 2\nNon-Programmers 1 1 1 1 1 1 1 1 1 1\n6.4 Results\nResults are listed in Table 4. Each major row corresponds to\na training-set size (100, 500 and 1.000 commands) and each\nsuch row is further sub-divided into experiments with different\nsubsequence lengths for segmenting the initial sequence (from\n2 to 6 commands). The columns show the average classifica-\ntion rate using the proposed approach (EvABCD) and the other\n(incremental and non-incremental) classification algorithms.\nAccording to this data, SVM and Non-Incremental Naive\nBayes (NB) perform slightly better than the Incremental NB\nand C5.0 classifiers in terms of accuracy. The percentages of\nusers correctly classified by EVABCD are higher than the\nresults obtained by LVQ and lower than the percentages ob-\ntained by NB, C5.0, and SVM. Both kNN classifiers perform\nmuch worse than the others. Note that the changes in the\nsubsequence length do not modify the classification results\nobtained by AdaBoost. This is due to the fact that the classifier\ncreates the same classification rule (weak hypotheses) although\nthe subsequence length varies.\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING , VOL., NO., JUNE 2010 10\nTABLE 4\nClassification rate (%) of different classifiers in the UNIX users environments using different size of training data set\nand different subsequence lengths\nClassifier and Classification Rate (%)\nIncremental Classifiers Non-Incremental Classifiers\nNumber of Incremental K-NN\ncommands for subsequences EVABCD Naive Incremental Naive K-NN C5.0 AdaBoost SVM LVQ\ntraining (per user) length Bayes (k=1) Bayes (k=1)\n2 65,5 77,3 38,3 79,1 42,8 73,9 59,5 82,1 68,4\n3 64,9 76,1 36,5 79,7 39,8 69,6 59,5 82,1 65,5\n100 4 64,5 72,0 34,1 74,4 39,2 74,6 59,5 77,4 64,3\n5 67,9 73,2 32,3 75,0 33,3 68,6 59,5 72,6 65,5\n6 64,3 73,2 32,3 77,3 32,7 70,1 59,5 69,6 63,1\n2 58,3 77,9 33,9 82,1 41,9 73,9 60,2 83,3 69,6\n3 59,5 73,8 36,9 77,3 39,8 74,6 60,2 82,7 71,4\n500 4 59,2 70,8 39,2 76,7 38,9 73,9 60,2 79,2 69,0\n5 66,7 71,4 35,1 76,1 37,3 73,6 60,2 76,2 66,1\n6 70,8 72,6 35,7 75,5 36,9 75,6 60,2 75,0 65,5\n2 60,1 78,5 43,6 85,1 44,1 73,9 61,3 83,9 73,2\n3 65,5 77,9 44,0 81,5 47,3 74,6 61,3 81,5 75,0\n1.000 4 61,3 77,3 43,5 79,1 46,1 73,9 61,3 80,4 71,4\n5 70,2 76,7 42,5 78,5 44,0 73,6 61,3 79,2 69,7\n6 72,0 76,1 41,9 77,9 44,6 74,6 61,3 77,4 68,5\nIn general, the difference between EVABCD and the NB\nand SVM algorithms is considerable for small subsequence\nlengths (2 or 3 commands); but this difference decreases when\nthis length is longer (5 or 6 commands). These results show\nthat using an appropriate subsequence length, the proposed\nclassifier can compete well with off-line approaches.\nNevertheless, the proposed environment needs a classifier\nable to process streaming data in on-line and in real-time.\nOnly the incremental classifiers satisfy this requirement; but\nunlike EVABCD, they assume a fixed structure. In spite of\nthis, taking into account the incremental classifiers, it can\nbe noticed that the difference in the results is not very\nsignificant; besides EVABCD can cope with huge amounts\nof data efficiently because its structure is open and the rule-\nbase evolves in terms of creating new prototypes as they\noccur automatically. In addition, the learning in EVABCD is\nperformed in single pass (non-iteratively) and a significantly\nsmaller memory is used. Spending too much time for training\nis clearly not adequate for this purpose.\nIn short, EVABCD needs an appropriate subsequence\nlength to get a classification rate similar to that obtained by\nother classifiers which use different techniques. However,\nEVABCD does not need to store the entire data stream in\nthe memory and disregards any sample after being used.\nEVABCD is one-pass (each sample is proceeded once at the\ntime of its arrival), while other off-line algorithms require\na batch set of training data in the memory and make many\niterations. Thus, EVABCD is computationally more simple\nand efficient as it is recursive and one pass. Unlike other\nincremental classifiers, EVABCD does not assume a prefixed\nstructure and it changes according to the changes in the data\npattern (shift in the data stream). In addition, as EVABCD\nuses a recursive expression for calculating the potential of\na sample, it is also computationally very efficient. In fact,\nsince the number of attributes is very large in the proposed\nenvironment and it changes frequently, EVABCD is the\nmost suitable alternative. Last, but not least, the EVABCD\nstructure is simple and interpretable.\n6.5 Scenario: A new class appears\nIn addition to the advantage that EVABCD is an on-line\nclassifier, we want to prove the ability of our approach to adapt\nto new data; in this case, new users or a different behavior\nof a user. This aspect is especially relevant in the proposed\nenvironment since a user profile changes constantly. For this\npurpose, we design a new experimental scenario in which the\nnumber of users of a class is incrementing in the training data\nin order to detect how the different classifiers recognize the\nusers of a new class.\nIn this case, EVABCD is compared with three non-\nincremental classifiers and 10-fold cross-validation is used.\nThis experiment consists of four parts:\n1) One of the 4 classes is chosen to detect how EVABCD\ncan adapt to it. The chosen class is named as new class.\n2) Initially, the training data set is composed by a sample\nof the new class and all the samples of the other three\nclasses. The test data set is composed of several samples\nof the four classes.\n3) EVABCD is applied and the classification rate of the\nsamples of the new class is calculated.\n4) The number of samples of the new class in the training\ndata set is increasing one by one and step 3 is repeated.\nThus, the classification rate of the new class samples is\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING , VOL., NO., JUNE 2010 11\ncalculated in each increase.\nFigure 4 shows the four graphic results of this experiment\nconsidering in each graph one of the four classes as the new\nclass:\n\u2022 x-axis represents the number of users of the new class\nthat contains the training data set.\n\u2022 y-axis represents the percentage of users of the new class\ncorrectly classified.\nIn the different graphs, we can see how quickly EVABCD\nevolves and adapts to the new class. If we consider the class\nNovice Programmers, it is remarkable that after analyzing 3\nusers of this class, the proposed classifier is able to create\na new prototype in which almost 90% of the test users are\ncorrectly classified. However, the other classifiers need a larger\nnumber of samples for recognizing the users of this new class.\nSimilar performance has been observed for the other 3 classes.\nSpecially, C5.0 needs several samples for creating a suitable\ndecision-tree. As we can see in the graph which represents the\nclass Computer scientist as the new class, the percentages of\nusers correctly in the 1-NN classifier is always 0 because all\nthe users of this class are classified in the Novice programmers\nclass. The increase in the classification rate is not perfectly\nsmooth because the new data bring useful information but also\nnoise.\nTaking into account these results, we would like to remark\nthat the proposed approach is able to adapt to a new user\nbehavior extremely quick. In a changing and real-time\nenvironment, as the proposed in this paper, this property is\nessential.\n7 CONCLUSIONS\nIn this paper we propose a generic approach, EVABCD,\nto model and classify user behaviors from a sequence of\nevents. The underlying assumption in this approach is that\nthe data collected from the corresponding environment can\nbe transformed into a sequence of events. This sequence is\nsegmented and stored in a trie and the relevant sub-sequences\nare evaluated by using a frequency-based method. Then, a\ndistribution of relevant sub-sequences is created. However, as\na user behavior is not fixed but rather it changes and evolves,\nthe proposed classifier is able to keep up to date the created\nprofiles using an Evolving Systems approach. EVABCD is\none pass, non-iterative, recursive and it has the potential to be\nused in an interactive mode; therefore, it is computationally\nvery efficient and fast. In addition, its structure is simple and\ninterpretable.\nThe proposed evolving classifier is evaluated in an\nenvironment in which each user behavior is represented\nas a sequence of UNIX commands. Although EVABCD\nhas been developed to be used on-line, the experiments\nhave been performed using a batch data set in order to\ncompare the performance to established (incremental and\nnon-incremental) classifiers. The test results with a data\nset of 168 real UNIX users demonstrates that, using an\nappropriate subsequence length, EVABCD can perform\nalmost as well as other well established off-line classifiers in\nterms of correct classification on validation data. However,\ntaking into account that EVABCD is able to adapt extremely\nquickly to new data, and that this classifier can cope with\nhuge amounts of data in a real environment which changes\nrapidly, the proposed approach is the most suitable alternative.\nAlthough, it is not addressed in this paper, EVABCD can\nalso be used to monitor, analyze and detect abnormalities\nbased on a time varying behavior of same users and to detect\nmasqueraders. It can also be applied to other type of users\nsuch as users of e-services, digital communications, etc.\nAPPENDIX A\nDERIVATION OF THE COSINE DISTANCE PO-\nTENTIAL RECURSIVELY\nIn this appendix the expression of the potential is transformed\ninto a recursive expression in which it is calculated using\nonly the current data sample (zk). For this novel derivation\nwe combine the expression of the potential for a sample data\n(equation (1)) represented by a vector of elements and the\ndistance cosine expression (equation (2)).\nPk(zk) =\n1\n1 + [ 1k\u22121\n\u2211k\u22121\ni=1 [1\u2212\n\u2211n\nj=1 z\nj\nkz\nj\ni\u221a\u2211n\nj=1(z\nj\nk)\n2\n\u2211n\nj=1(z\nj\ni )\n2\n]2]\n(12)\nwhere zk denotes the kth sample inserted in the data space.\nEach sample is represented by a set of values; the value of\nthe ith attribute (element) of the zk sample is represented as zik.\nIn order to explain the derivation of the expression step by\nstep; firstly, we consider the denominator of the equation (12)\nwhich is named as den.P (zk).\nden.Pk(zk) =\n= 1 + [\n1\nk \u2212 1\nk\u22121\u2211\ni=1\n[1\u2212\n\u2211n\nj=1 z\nj\nkz\nj\ni\n\u221a\u2211n\nj=1(z\nj\nk)\n2\n\u2211n\nj=1(z\nj\ni )\n2\n]2]\nden.Pk(zk) = 1 + [\n1\nk \u2212 1\nk\u22121\u2211\ni=1\n[1\u2212\nfi\nh gi\n]2]\nwhere :\nfi =\nn\u2211\nj=1\nzjkz\nj\ni , h =\n\u221a\n\u221a\n\u221a\n\u221a\nn\u2211\nj=1\n(zjk)\n2 and gi =\n\u221a\n\u221a\n\u221a\n\u221a\nn\u2211\nj=1\n(zji )\n2\n(13)\nWe can observe that the variables fi and gi depend on\nthe sum of all the data samples (all these data samples are\nrepresented by i); but the variable h represents the sum of the\nattribute values of the sample. Therefore, den.Pk(zk) can be\nsimplified further into:\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING , VOL., NO., JUNE 2010 12\nFig. 4. Evolution of the classification rate during on-line learning with a subset of UNIX users data set.\nden.Pk(zk) = 1 + [\n1\n(k \u2212 1)\n+\nk\u22121\u2211\ni=1\n[1\u2212 [\n\u22122fi\nhgi\n] + [\nfi\nhgi\n]2]]\nden.Pk(zk) = 1 + [\n1\n(k \u2212 1)\n[(k \u2212 1) +\nk\u22121\u2211\ni=1\n[\n\u22122fi\nhgi\n+\nf2i\nh2g2i\n]]]\n(14)\nAnd finally, into:\nden.Pk(zk) = 2 + [\n1\nh(k \u2212 1)\n[[\u22122\nk\u22121\u2211\ni=1\nfi\ngi\n] + [\n1\nh\nk\u22121\u2211\ni=1\n(\nfi\ngi\n)2]]]\n(15)\nIn order to obtain an expression for the potential from (15),\nwe rename it as follows:\nden.Pk(zk) = 2 + [\n1\nh(k \u2212 1)\n[[\u22122BK ] + [\n1\nh\nDk]]]\nwhere : Bk =\nk\u22121\u2211\ni=1\nfi\ngi\nand Dk =\nk\u22121\u2211\ni=1\n(\nfi\ngi\n)2\n(16)\nIf we analyze each variable (Bk and Dk) separately\n(considering the renaming done in (13)):\nFirstly, we consider Bk\nBk =\nk\u22121\u2211\ni=1\n\u2211n\nj=1 z\nj\nkz\nj\ni\n\u221a\u2211n\nj=1(z\nj\ni )\n2\n=\nn\u2211\nj=1\nzjk\nk\u22121\u2211\ni=1\n\u221a\n(zji )\n2\n\u2211n\nl=1(z\nl\ni)\n2\n(17)\nIf we define each attribute of the sample Bk by:\nbjk =\nk\u22121\u2211\ni=1\n\u221a\n(zji )\n2\n\u2211n\nl=1(z\nl\ni)\n2\n(18)\nThus, the value of Bk can be calculated as a recursive\nexpression:\nBk =\nn\u2211\nj=1\nzjkb\nj\nk ; b\nj\nk = b\nj\n(k\u22121) +\n\u221a\n(zjk)\n2\n\u2211n\nl=1(z\nl\nk)\n2\nbj1 =\n\u221a\n(zj1)2\u2211n\nl=1(z\nl\n1)2\n(19)\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING , VOL., NO., JUNE 2010 13\nSecondly, considering Dk with the renaming done in (13),\nwe get:\nDk =\nk\u22121\u2211\ni=1\n(\n\u2211n\nj=1 z\nj\nkz\nj\ni\n\u221a\u2211n\nj=1(z\nj\ni )\n2\n)2 =\nk\u22121\u2211\ni=1\n1\n\u2211n\nl=1(z\nl\ni)\n2\n[\nn\u2211\nj=1\nzjkz\nj\ni ]\n2\nDk =\nn\u2211\nj=1\nzjk\nn\u2211\np=1\nzpk\nk\u22121\u2211\ni=1\nzijk z\nip\nk\n1\n\u2211n\nl=1(z\nl\ni)\n2\n(20)\nIf we define djpk as each attribute of the sample Dk, we get:\ndjpk =\nk\u22121\u2211\ni=1\nzijk z\nip\nk\n1\n\u2211n\nl=1(z\nl\ni)\n2\n(21)\nTherefore:\nDk =\nn\u2211\nj=1\nzjk\nn\u2211\np=1\nzpkd\njp\nk ; d\njp\nk = d\njp\n(k\u22121) +\nzjkz\np\nk\u2211n\nl=1(z\nl\nk)\n2\n;\nd1j1 =\nzj1z\n1\n1\u2211n\nl=1(z\nl\n1)2\n; j = [1, n+ 1]\n(22)\nFinally:\nPk(zk) =\n1\n2 + [ 1h(k\u22121) [[\u22122BK ] + [\n1\nhDk]]]\nk = 2, 3...;P1(z1) = 1\n(23)\nwhere Bk is obtained as in (19), and Dk is described in (22).\nNote that to get recursively the value of Bk, it is necessary\nto calculate n accumulated values (in this case, n is the\nnumber of the different subsequences obtained). However, to\nget recursively the value of Dk we need to calculate nxn\ndifferent accumulated values which store the result of multiply\na value by all the other different values (these values are\nrepresented as dijk ).\nREFERENCES\n[1] D. Godoy and A. Amandi, \u201cUser profiling in personal information\nagents: a survey,\u201d Knowledge Engineering Review, vol. 20, no. 4, pp.\n329\u2013361, 2005.\n[2] J. A. Iglesias, A. Ledezma, and A. Sanchis, \u201cCreating user profiles\nfrom a command-line interface: A statistical approach,\u201d in Proceedings\nof the International Conference on User Modeling, Adaptation, and\nPersonalization (UMAP), 2009, pp. 90\u2013101.\n[3] M. Schonlau, W. Dumouchel, W. H. Ju, A. F. Karr, and Theus,\n\u201cComputer Intrusion: Detecting Masquerades,\u201d in Statistical Science,\nvol. 16, 2001, pp. 58\u201374.\n[4] R. A. Maxion and T. N. Townsend, \u201cMasquerade detection using trun-\ncated command lines,\u201d in Proceedings of the International Conference\non Dependable Systems and Networks (DSN), 2002, pp. 219\u2013228.\n[5] A. Alaniz-Macedo, K. N. Truong, J. A. Camacho-Guerrero, and\nM. Graca-Pimentel, \u201cAutomatically sharing web experiences through a\nhyperdocument recommender system,\u201d in HYPERTEXT 2003. New\nYork, NY, USA: ACM, 2003, pp. 48\u201356.\n[6] D. L. Pepyne, J. Hu, and W. Gong, \u201cUser profiling for computer\nsecurity,\u201d in Proceedings of the American Control Conference, 2004,\npp. 982\u2013987.\n[7] D. Godoy and A. Amandi, \u201cUser profiling for web page filtering,\u201d IEEE\nInternet Computing, vol. 9, no. 4, pp. 56\u201364, 2005.\n[8] J. Anderson, Learning and Memory: An Integrated Approach. New\nYork: John Wiley and Sons., 1995.\n[9] Y. Horman and G. A. Kaminka, \u201cRemoving biases in unsupervised\nlearning of sequential patterns,\u201d Intelligent Data Analysis, vol. 11, no. 5,\npp. 457\u2013480, 2007.\n[10] T. Lane and C. E. Brodley, \u201cTemporal sequence learning and data\nreduction for anomaly detection,\u201d in Proceedings of the ACM conference\non Computer and communications security (CCS). New York, NY,\nUSA: ACM, 1998, pp. 150\u2013158.\n[11] S. E. Coull, J. W. Branch, B. K. Szymanski, and E. Breimer, \u201cIntrusion\ndetection: A bioinformatics approach,\u201d in Proceedings of the Annual\nComputer Security Applications Conference (ACSAC), 2003, pp. 24\u201333.\n[12] P. Angelov and X. Zhou, \u201cEvolving fuzzy rule-based classifiers from\ndata streams,\u201d IEEE Transactions on Fuzzy Systems: Special issue on\nEvolving Fuzzy Systems, vol. 16, no. 6, pp. 1462\u20131475, 2008.\n[13] M. Panda and M. R. Patra, \u201cA comparative study of data mining\nalgorithms for network intrusion detection,\u201d International Conference\non Emerging Trends in Engineering & Technology, vol. 0, pp. 504\u2013507,\n2008.\n[14] A. Cufoglu, M. Lohi, and K. Madani, \u201cA comparative study of selected\nclassifiers with classification accuracy in user profiling,\u201d in Proceedings\nof the WRI World Congress on Computer Science and Information\nEngineering (CSIE). IEEE Computer Society, 2009, pp. 708\u2013712.\n[15] R. Polikar, L. Upda, S. S. Upda, and V. Honavar, \u201cLearn++: an\nincremental learning algorithm for supervised neural networks,\u201d IEEE\nTransactions on Systems, Man and Cybernetics, Part C (Applications\nand Reviews), vol. 31, no. 4, pp. 497\u2013508, 2001. [Online]. Available:\nhttp:\/\/dx.doi.org\/10.1109\/5326.983933\n[16] D. Kalles and T. Morris, \u201cEfficient incremental induction of decision\ntrees,\u201d Machine Learning, vol. 24, no. 3, pp. 231\u2013242, 1996.\n[17] F. J. Ferrer-Troyano, J. S. Aguilar-Ruiz, and J. C. R. Santos, \u201cData\nstreams classification by incremental rule learning with parameterized\ngeneralization,\u201d in Proceedings of ACM Symposium on Applied Com-\nputing (SAC), 2006, pp. 657\u2013661.\n[18] J. C. Schlimmer and D. H. Fisher, \u201cA case study of incremental concept\ninduction,\u201d in AAAI, 1986, pp. 496\u2013501.\n[19] P. E. Utgoff, \u201cId5: An incremental id3,\u201d in Machine Learning, 1988, pp.\n107\u2013120.\n[20] P. E. Utgof, \u201cIncremental induction of decision trees,\u201d Machine Learn-\ning, vol. 4, no. 2, pp. 161\u2013186, 1989.\n[21] G. A. Carpenter, S. Grossberg, and D. B. Rosen, \u201cArt2-a: An adaptive\nresonance algorithm for rapid category learning and recognition,\u201d Neural\nNetworks, vol. 4, pp. 493\u2013504, 1991.\n[22] G. A. Carpenter, S. Grossberg, N. Markuzon, J. H. Reynolds, and D. B.\nRosen, \u201cFuzzy artmap: A neural network architecture for incremental\nsupervised learning of analog multidimensional maps,\u201d Neural Networks,\nIEEE Transactions on, vol. 3, no. 5, pp. 698\u2013713, 2002.\n[23] N. Kasabov, \u201cEvolving fuzzy neural networks for super-\nvised\/unsupervised online knowledge-based learning,\u201d IEEE\nTransactions on Systems, Man and Cybernetics - Part B: Cybernetics,\nvol. 31, no. 6, pp. 902\u2013918, 2001.\n[24] T. Seipone and J. A. Bullinaria, \u201cEvolving improved incremental learn-\ning schemes for neural network systems,\u201d in Congress on Evolutionary\nComputation, 2005, pp. 2002\u20132009.\n[25] T. Kohonen, J. Kangas, J. Laaksonen, and K. Torkkola, \u201cLvq pak: A\nprogram package for the correct application of learning vector quan-\ntization algorithms,\u201d in International Conference on Neural Networks.\nIEEE, 1992, pp. 725\u2013730.\n[26] F. Poirier and A. Ferrieux, \u201cDVQ: Dynamic vector quantization - an\nincremental LVQ,\u201d in International Conference on Artificial Neural\nNetworks, 1991, pp. 1333\u20131336.\n[27] R. K. Agrawal and R. Bala, \u201cIncremental bayesian classification\nfor multivariate normal distribution data,\u201d Pattern Recognition\nLetters, vol. 29, no. 13, pp. 1873\u20131876, 2008. [Online]. Available:\nhttp:\/\/dx.doi.org\/10.1016\/j.patrec.2008.06.010\n[28] K. M. A. Chai, H. L. Chieu, and H. T. Ng, \u201cBayesian online classifiers\nfor text classification and filtering,\u201d in Proceedings of the International\nConference on Research and Development in Information Retrieval\n(SIGIR), 2002, pp. 97\u2013104.\n[29] G. Cauwenberghs and T. Poggio, \u201cIncremental and decremental support\nvector machine learning,\u201d in NIPS, 2000, pp. 409\u2013415. [Online].\nAvailable: http:\/\/citeseer.ist.psu.edu\/cauwenberghs00incremental.html\n[30] R. Xiao, J. Wang, and F. Zhang, \u201cAn approach to incremental SVM\nlearning algorithm,\u201d IEEE International Conference on Tools with Arti-\nficial Intelligence, vol. 0, pp. 268\u2013278, 2000.\n[31] G. Widmer and M. Kubat, \u201cLearning in the presence of concept drift\nand hidden contexts,\u201d in Machine Learning, 1996, pp. 69\u2013101.\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING , VOL., NO., JUNE 2010 14\n[32] P. Riley and M. M. Veloso, \u201cOn behavior classification in adversarial\nenvironments,\u201d in Proceedings of Distributed Autonomous Robotic Sys-\ntems (DARS), 2000, pp. 371\u2013380.\n[33] E. Fredkin, \u201cTrie memory,\u201d Comm. A.C.M., vol. 3, no. 9, pp. 490\u2013499,\n1960.\n[34] J. A. Iglesias, A. Ledezma, and A. Sanchis, \u201cSequence classification\nusing statistical pattern recognition.\u201d in Proceedings of Intelligent Data\nAnalysis (IDA), ser. LNCS, vol. 4723. Springer, 2007, pp. 207\u2013218.\n[35] G. A. Kaminka, M. Fidanboylu, A. Chang, and M. M. Veloso, \u201cLearning\nthe sequential coordinated behavior of teams from observations,\u201d in\nRoboCup, ser. LNCS, vol. 2752. Springer, 2002, pp. 111\u2013125.\n[36] J. A. Iglesias, A. Ledezma, and A. Sanchis, \u201cA comparing method of two\nteam behaviours in the simulation coach competition,\u201d in Proceedings of\nModeling Decisions for Artificial Intelligence (MDAI), ser. LNCS, vol.\n3885. Springer, 2006, pp. 117\u2013128.\n[37] R. Agrawal and R. Srikant, \u201cMining sequential patterns,\u201d in International\nConference on Data Engineering, Taipei, Taiwan, 1995, pp. 3\u201314.\n[38] P. Angelov and D. Filev, \u201cAn approach to online identification of\ntakagi-sugeno fuzzy models,\u201d IEEE Transactions on Systems, Man, and\nCybernetics, Part B, vol. 34, no. 1, pp. 484\u2013498, 2004.\n[39] P. Angelov, X. Zhou, and F. Klawonn, \u201cEvolving fuzzy rule-based\nclassifiers,\u201d Computational Intelligence in Image and Signal Processing,\n2007. CIISP 2007. IEEE Symposium on, pp. 220\u2013225, 2007.\n[40] X. Zhou and P. Angelov, \u201cAutonomous visual self-localization in com-\npletely unknown environment using evolving fuzzy rule-based classi-\nfier,\u201d Computational Intelligence in Security and Defense Applications,\n(CISDA), pp. 131\u2013138, 2007.\n[41] P. Angelov and D. Filev, \u201cSimpl ets: a simplified method for learning\nevolving takagi-sugeno fuzzy models,\u201d The IEEE International Confer-\nence on Fuzzy Systems (IEEE-FUZZ)., pp. 1068\u20131073, 2005.\n[42] P. Angelov and D. Filiv, \u201cFlexible models with evolving structure,\u201d\nInternational Journal of Intelligent Systems, vol. 19, no. 4, pp. 327\u2013\n340, 2004.\n[43] S. Greenberg, \u201cUsing unix: Collected traces of 168 users,\u201d Master\u2019s\nthesis, Department of Computer Science, University of Calgary, Alberta,\nCanada, 1988.\n[44] J. Quinlan, \u201cData mining tools see5 and C5.0,\u201d 2003 [online], available:\nhttp:\/\/www.rulequest.com\/see5-info.html.\n[45] J. R. Quinlan, C4.5: programs for machine learning. San Francisco,\nCA, USA: Morgan Kaufmann Publishers Inc., 1993.\n[46] R. O. Duda and P. E. Hart, Pattern Classification and Scene Analysis.\nJohn Wiley & Sons Inc, 1973.\n[47] G. John and P. Langley, \u201cEstimating continuous distributions in bayesian\nclassifiers,\u201d in In Proceedings of the Conference on Uncertainty in\nArtificial Intelligence, 1995, pp. 338\u2013345.\n[48] T. Cover and P. Hart, \u201cNearest neighbor pattern classification,\u201d IEEE\nTransactions on Information Theory, vol. 13, no. 1, pp. 21\u201327, 1967.\n[49] Y. Freund and R. E. Schapire, \u201cA decision-theoretic generalization of\non-line learning and an application to boosting,\u201d Journal of Computer\nand System Sciences, vol. 55, no. 1, pp. 119\u2013139, 1997.\n[50] J. H. Morra, Z. Tu, L. G. Apostolova, A. Green, A. W. Toga, and P. M.\nThompson, \u201cComparison of adaboost and support vector machines for\ndetecting alzheimer\u2019s disease through automated hippocampal segmenta-\ntion,\u201d IEEE Transactions on Medical Imaging, vol. 29, no. 1, pp. 30\u201343,\n2010.\n[51] J. Platt, \u201cMachines using sequential minimal optimization,\u201d in Advances\nin Kernel Methods - Support Vector Learning, B. Schoelkopf, C. Burges,\nand A. Smola, Eds. MIT Press, 1998.\n[52] T. Kohonen, M. R. Schroeder, and T. S. Huang, Eds., Self-Organizing\nMaps. Secaucus, NJ, USA: Springer-Verlag New York, Inc., 2001.\nJose Antonio Iglesias is a Teaching Assistant of computer science\nand member of the CAOS research group at Carlos III University of\nMadrid (UC3M), Spain. He obtained his Ph.D. in Computer Science\nfrom UC3M in 2010. He has a B.S. in Computer Science (2002) from\nValladolid University. He has published over 25 journal and conference\npapers and he takes part in several national research projects. He is\nmember of the Fuzzy Systems Technical Committee (IEEE\/CIS) and\ncommittee member of several international conferences. His research\ninterests include agent modeling, plan recognition, sequence learning,\nmachine learning and evolving fuzzy systems.\nPlamen Angelov received the M.Eng. degree in electronics and au-\ntomation from Sofia Technical University, Sofia, Bulgaria, in 1989 and\nthe Ph.D. degree in optimal control from Bulgaria Academy of Sciences,\nSofia, Bulgaria, in 1993. He spent over ten years as a Research Fellow\nworking on computational intelligence and control. During 1995-1996,\nhe was at Hans-Knoell Institute, Jena, Germany. In 1997, he was a Vis-\niting Researcher at the Catholic University, Leuvain-la-neuve, Belgium.\nIn 2007, he was a Visiting Professor at the University of Wolfenbuettel-\nBraunschweig, Germany. He is currently a Senior Lecturer (Associate\nProfessor) at Lancaster University, Lancaster, U.K. He has authored\nor coauthored over 140 peer-reviewed publications, including the book\nEvolving Rule Based Models: A Tool for Design of Flexible Adaptive\nSystems (Springer-Verlag, 2002), and over 40 journal papers, and is\na holder of a patent (2006). He is the Editor-in-Chief of the Springer\nJournal Evolving Systems. He is the Chair of Standards Committee,\nComputational Intelligence Society, IEEE. His current research interests\ninclude adaptive and evolving (self-organizing) fuzzy systems as a\nframework of an evolving computational intelligence.\nAgapito Ledezma is an Associate Professor of computer science and\nmember of the CAOS research group at Carlos III University of Madrid\n(UC3M). He obtained his Ph.D. in computer science from UC3M in\n2004. He has a B. S. in computer science (1997) from Universidad\nLatinoamericana de Ciencia y Tecnologia (ULACIT) of Panama. He\nhas contributed to several national research projects on data mining\nand image processing. His research interests span data mining, agent\nmodeling, ensemble of classifiers and cognitive robotics. He has written\nmore than 40 technical papers for computer science journals and\nconferences and he is a committee member of several international\nconferences.\nAraceli Sanchis has been a University Associate Professor of computer\nscience at Carlos III University of Madrid (UC3M) since 1999. She\nreceived her Ph.D. in physical chemistry from Complutense University\nof Madrid in 1994 and in computer science from Politecnic University\nof Madrid in 1998. She has a B. Sc. in chemistry (1991) from the\nComplutense University of Madrid. She had been Vice-Dean of the\nComputer Science degree at UC3M and, currently, she is head of\nthe CAOS group (Grupo de Control, Aprendizaje y Optimizacin de\nSistemas) based on machine learning and optimization. She has led\nseveral research projects founded by the Spanish Ministry of Education\nand Science and also by the European Union. She has published over\n60 journal and conference papers mainly in the field of machine learning.\nHer topics of interests are multi-agent systems, agent modeling, data\nmining and cognitive robotics.\n"}