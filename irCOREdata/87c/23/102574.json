{"doi":"10.1109\/ICIP.2007.4379329","coreId":"102574","oai":"oai:epubs.surrey.ac.uk:2059","identifiers":["oai:epubs.surrey.ac.uk:2059","10.1109\/ICIP.2007.4379329"],"title":"An ES based effecient motion estimation technique for 3D integral video compression","authors":["Adedoyin, S","Fernando, WAC","Aggoun, A","Weerakkody, WARJ"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2007-11-12","abstract":"In this paper we propose a novel approach to use both motion and disparity information to compress 3D integral video sequences. The integral video sequence is decomposed into 8 viewpoint video sequences and a block search is performed to jointly exploit the motion and disparity redundancies to maximize the compression. An Evolutionary Strategy (ES) based search algorithm is used to reduce the complexity. Experimental results show that an ES based strategy can reduce the motion estimation complexity by 95%","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":"IEEE","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:epubs.surrey.ac.uk:2059<\/identifier><datestamp>\n      2017-01-03T19:10:19Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      74797065733D636F6E666572656E63655F6974656D<\/setSpec><setSpec>\n      6469766973696F6E733D656E67616E64706879736963616C736369656E636573:656C656374726F6E6963656E67696E656572696E67:63637372<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/epubs.surrey.ac.uk\/2059\/<\/dc:relation><dc:title>\n        An ES based effecient motion estimation technique for 3D integral video compression<\/dc:title><dc:creator>\n        Adedoyin, S<\/dc:creator><dc:creator>\n        Fernando, WAC<\/dc:creator><dc:creator>\n        Aggoun, A<\/dc:creator><dc:creator>\n        Weerakkody, WARJ<\/dc:creator><dc:description>\n        In this paper we propose a novel approach to use both motion and disparity information to compress 3D integral video sequences. The integral video sequence is decomposed into 8 viewpoint video sequences and a block search is performed to jointly exploit the motion and disparity redundancies to maximize the compression. An Evolutionary Strategy (ES) based search algorithm is used to reduce the complexity. Experimental results show that an ES based strategy can reduce the motion estimation complexity by 95%.<\/dc:description><dc:publisher>\n        IEEE<\/dc:publisher><dc:date>\n        2007-11-12<\/dc:date><dc:type>\n        Conference or Workshop Item<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/epubs.surrey.ac.uk\/2059\/1\/SRF002176.pdf<\/dc:identifier><dc:identifier>\n          Adedoyin, S, Fernando, WAC, Aggoun, A and Weerakkody, WARJ  (2007) An ES based effecient motion estimation technique for 3D integral video compression  In: IEEE International Conference on Image Processing, 2007. ICIP 2007, 2007-09-16-2007-10-19, San Antonio, USA.     <\/dc:identifier><dc:relation>\n        http:\/\/ieeexplore.ieee.org\/xpls\/abs_all.jsp?arnumber=4379329<\/dc:relation><dc:relation>\n        10.1109\/ICIP.2007.4379329<\/dc:relation><dc:language>\n        EN<\/dc:language><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/epubs.surrey.ac.uk\/2059\/","http:\/\/ieeexplore.ieee.org\/xpls\/abs_all.jsp?arnumber=4379329","10.1109\/ICIP.2007.4379329"],"year":2007,"topics":[],"subject":["Conference or Workshop Item","PeerReviewed"],"fullText":"AN ES BASED EFFECIENT MOTION ESTIMATION TECHNIQUE FOR 3D INTEGRAL \nVIDEO COMPRESSION \nS. Adedoyin1, W.A.C. Fernando1, A.Aggoun2, W.A.R.J.Weerakkody1 \n1Centre for Communications Systems Research, University of Surrey, Guildford, GU2 7XH, UK \n2School of Engineering and Design, Brunel University, Uxbridge, UB8 3PH, United Kingdom \n sadedoyin@hotmail.com; w.fernando@surrey.ac.uk; amar.aggoun@brunel.ac.uk; rajithawee@yahoo.com \n \nABSTRACT  \n \n In this paper we propose a novel approach to use both \nmotion and disparity information to compress 3D integral \nvideo sequences. The integral video sequence is decomposed \ninto 8 viewpoint video sequences and a block search is \nperformed to jointly exploit the motion and disparity \nredundancies to maximize the compression. An Evolutionary \nStrategy (ES) based search algorithm is used to reduce the \ncomplexity. Experimental results show that an ES based \nstrategy can reduce the motion estimation complexity by \n95%. \n \nIndex terms - 3D video, Evolutionary Strategy, 3D integral \nimaging \n \n1. INTRODUCTION \n \nEmulating human 3D vision, which relies on processing \n\u2018left eye\u2019 and \u2018right eye\u2019 images, has been a dream for many \nyears. Current stereo-spectacles, cameras, and displays have \nrelied on creating two images focusing on two perspective \nviewpoints (the parallax effect) leading to an impression of \ndepth. Such systems require users to simultaneously focus \non the screen plane whilst converging their eyes to a \ndifferent point in space, which causes eyestrain and fatigue, \nand therefore have not found wide applications. \nFurthermore multiview stereoscopic systems tend to \nsuffer from the effect of 'card boarding' and \u2018flipping\u2019. So \nideally we require true autostereoscopic 3D visualisation \nsystems, exhibiting full parallax and continuous view points \nwhich allow accommodation and convergence to function in \nunison. Holographic techniques demonstrating this feature, \nare being researched by various groups to produce full \ncolour realistic spatial images. However, such systems are \ninherently slow and costly, their disadvantages including: \nthe need for coherent radiation, poor colour rendering, \nspecialised environmental conditions, and the huge data \ncontent needed for processing. \nIntegral imaging is a technique that is capable of \ncreating and encoding a true volume spatial optical model of \nthe object scene in the form of a planar intensity distribution \nby using unique optical components [1]. It is akin to \nholography in that 3D information recorded on a 2D \nmedium can be replayed as a full 3D optical model, \nhowever, in contrast to holography, coherent light sources \nare not required.  This conveniently allows more \nconventional live capture and display procedures to be \nadopted.  A 3D integral image is represented entirely by a \nplanar intensity distribution, which may be recorded on to a \nphotographic film for later electronic scanning and \nprocessing or directly recorded as an intensity distribution \nusing a CCD with a standard camera lens.  \nTV based on 3D integral imaging video technology, that \nrequires only one camera, will be attractive to service \nproviders because it will seamlessly provide the added value \nof 3D realism. 3D integral imaging encoded video can be \ndesigned to be scalable with 2D video and can be encoded \nefficiently so as to economically provide attractive high \nvalue services over high value systems. Thus, the \ndevelopment of a 3D integral imaging TV system will also \ndemonstrate how added-value broadband services of this \ntype can be delivered, providing benefit to designers of \nthese type of services in the future. However, lots of work \nneeds to be done in this area especially in the compression \nof 3D integral imaging video. In this paper we propose an \nES based motion estimation algorithm for 3D integral \nimaging video.  \nThe rest of the paper is organized as follows. Section 2 \ncomprises of the proposed 3D video encoder with ES-Based \nME, section 3 contains the results and section 4 concludes \nthis paper. \n \n2. PROPOSED 3D VIDEO ENCODER WITH  \nES-BASED ME \n \nEvolutionary computation (EC) theories were \ndeveloped originally from observing natural evolution of life \nform. Because of this, the terminology surrounding the field \nof EC is full of analogies with natural evolutionary process. \nIt was particularly from Darwin\u2019s theories [2] that the best \ntechniques regarding the optimization, modelling and the \ncontrol of unknown processes were developed. EC has long \nbeen exploited in the video coding field. A very well known \nform of EC called Genetic Algorithm (GA) was used to \nperform image registration as part of a larger Digital \nSubtraction Angiography (DAS) system [3][4]. \nSubsequently, GA search algorithm has been applied for \nIII - 3931-4244-1437-7\/07\/$20.00 \u00a92007 IEEE ICIP 2007\nAuthorized licensed use limited to: University of Surrey. Downloaded on May 12,2010 at 09:36:09 UTC from IEEE Xplore.  Restrictions apply. \nmotion estimation [5][6][7][8]. Hardware implementation of \nFour-Step genetic search algorithm was proposed in [5].  \nSimilarly, the Evolutionary Strategies (ES) were \ndeveloped to solve technical optimization problems in video \ncoding field. Thus, the motion and disparity estimation has \nbeen carried out using a (1+\u021c) rudimentary ES for \nstereoscopic video sequences, which includes calculation of \nP- and B-frames, weighted prediction, joint motion disparity \nestimation [10][11]. In this paper, we apply ES to estimate \nthe motion and inter-view disparity vectors in lenticular \nvideo coding. \n \n2.1 Coding structure \n \nIntegral imaging involves many microlenses in one \nrecording. Consequently a number of perspective 2-D \nimages, as many as there are lenses in the lens array, are \nobtained in a single capture process. These images are called \n\u201celemental images\u201d according to the convention in the \nliterature. In this paper, the images are recorded using a \nlenticular lens sheet (1D cylindrical microlens array).  This \nresults in vertically running bands to present in the planar \nintensity distribution captured by the 3D integral imaging \ncamera [1]. \nEach viewpoint video sequence represents a unique \nrecording direction of the object scene. Hence the 3D \nintegral video sequence can be separated into its respective \ndistinctive viewpoint videos. Figure 1 illustrates the \nviewpoint extraction for a lenticular video of 4 distinctive \nviewpoints. Columns of pixels formed by each micro lens \nrepresenting the similar view points are placed near to each \nother to form the viewpoint images. These viewpoint video \nsequences are used in the motion compensation.  \n \n \nFigure 1 Viewpoint extraction  \n \nThe best motion vectors for each viewpoint can be \nfound by applying a conventional block-matching algorithm. \nHowever such a technique would be too complex and time \nconsuming. Since viewpoint images are captured by slightly \ndifferent viewing angles, there is a great deal of \nredundancies. Therefore, it is advantageous to find a proper \nset of motion vectors only for a single viewpoint and utilize \nthis correlation to minimize the overall coding complexity. \nCompression efficiency can be maximized if disparity \ncorrelations amongst the viewpoints are also considered. \nExploitation of such additional redundancies, however, \nincreases the computational complexity further. Following \nthis argument, we propose to motion compensate one of the \nmiddlemost viewpoints and the rest of the view points are \nmotion and disparity compensate jointly considering the \nmotion compensated viewpoint as the base viewpoint. \nProposed structure is illustrated in figure 2. Note that the \nfigure represents only the first five view points. After coding \nthe base viewpoint (i.e. Viewpoint 5) with respect to the \nbase viewpoint of the reference frame, viewpoint 3 is coded \ntaking the corresponding viewpoint from the reference \nframe and the reconstructed version of the base viewpoint of \nthe current frame as references. Subsequently, the viewpoint \n4 is coded taking reconstructed versions of the viewpoint 3 \nand 5 of the current frame and the viewpoint 4 of the \nreference frame as references.  This process is repeated for \nthe other viewpoints in the sequence. \n \n \n \n \n \n2.2 Proposed evolutionary strategy  \n \nES typically uses deterministic selection in which the \nworst solutions are purged from the population based \ndirectly on their fitness function value. The (\u03bc+\u03bb) \nEvolutionary Strategy demonstrated in Figure 3 is used in \nthis work with an increasing level of imitation of biological \nevolution [9], where \u03bc means the total number of parents in \nReference \nFrame \nCurrent \nFrame \nViewpoint 1 \nViewpoint 2 \nViewpoint 3 \nViewpoint 4 \nViewpoint 5  \nBase viewpoint \nMotion Prediction \nDisparity Prediction \n\u2026. \u2026. \nFigure 2 Proposed motion and disparity estimation \ntechnique \nIII - 394\nAuthorized licensed use limited to: University of Surrey. Downloaded on May 12,2010 at 09:36:09 UTC from IEEE Xplore.  Restrictions apply. \nprevious population, and \u03bb stands for the number of \noffspring generated from mutated parents. \n \n \nFigure 3 (\u03bc+\u03bb) Evolutionary Strategy-based motion estimation \nalgorithm \n2.3 Chromosome representation \n \nEach chromosome represents three elements of a \nmotion vector, i.e. the data for coordinates x and y and the \nreference frame (in case of a motion compensation) or \nviewpoint (in case of a disparity compensation). Each \nelement is described by 2 genes: object and strategy \nparameters, as shown in the Figure 4.  \nObject parameters define the actual coordinate in the \nimage. The value of object parameter is determined from the \nsearch window size. For example, if the search window is \nwithin the range [-16, 16], then the value of object \nparameter can take any integer number inside of this range. \nThe search window size depends on the maximum motion \nvector size. Strategy parameter determines wherever a local \nor global search will be carried out. The smaller the value of \nthe strategy parameter, the more localized the search process \nbecomes. The negative value defines the decrement of \nmutated gene and positive values respectively determine the \nincrement of the mutated gene. The strategy parameter \ndepends on the window size and can take any value up to its \nmaximum. In order to implement the local search, we \nchoose to set the strategy parameter to values -1 or 1. \n \n \nFigure 4 Chromosome representation \n2.3.1 Fitness Function \nThe quality of the chromosome is defined by fitness \nfunction. Fitness function is calculated based on the Sum of \nAbsolute Difference (SAD). Each chromosome in newly \ngenerated population is evaluated using fitness function. \n \n2.3.2 Evolutionary strategy operators \nIn order to reduce the number of generations required to \nobtain the satisfactory solution, the initial population is \ngenerated from predefined and randomly generated \nchromosomes. The pre-defined chromosomes are \ndetermined based on the knowledge from the previously \ncoded blocks from both adjacent viewpoints and viewpoints \nfrom the previously encoded frame. Selection takes place \nonly amongst the offspring\u2019s (mutated values) and parents. \nThe size of population in the next generation is fixed. The \nnew population is generated from the best chromosomes \nfrom the previous population that combines both parents and \noffspring as shown in Figure 3. \nMutation rate defines the percentage of genes to be \nmutated in a newly generated population. Mutation rate used \nin the experiments was set to 8.5%. In general the value of \nthe strategy parameter is generated randomly from the local \nsearch increment window specified in advance. In our case, \nthe strategy parameter value can vary within the range [-1, \n1]. The new value of the object parameter (if this gene has \nbeen chosen to be mutated) is defined as following: \n spop\nnew\nop xxx +=                                 (1) \nwhere \nnew\nopx\n is the new value of mutated object gene for x \ncoordinate, opx  and spx  are the values of object and strategy \nparameters for x coordinate respectively. Similarly the \nparameters for y coordinate and the reference are calculated. \n \n3. RESULTS \n \nProposed joint motion and disparity estimation \ntechnique is implemented in a 3D-DCT integral image \ncodec based on the architecture described in [1] for \nperformance evaluation. An adaptive arithmetic coder is \nused for the entropy coding and the quantizer step size \nranged from 10 \u2013 50. For experimental purposes a \npopulation size of 30 was used for ES. The number of \ngeneration and the mutation rate are set to 10 and 8.5% \nrespectively based on preliminary experimental results. Peak \nSignal-to-Noise Ratio (PSNR) is used to measure the \nobjective quality. Figure 5-7 show the objective quality \ncomparison of the proposed ES based joint motion and \ndisparity estimation technique (denoted as ES-JM&D) for \nthe Room integral video test sequence of image size \n512\u00d7512 against three reference cases namely: \n(i) Motion only full search (FS-MOTION) \u2013 motion \ncompensated prediction is used and the motion vectors are \ncalculated using the full search algorithm. \n(ii) Motion only ES search (ES-MOTION) \u2013 as above \nexcept full search is replaced with ES search. \n(iii) Joint motion and disparity full search (FSJM&D) \u2013 joint \nmotion and disparity compensated prediction is used with ES \nsearch. Figure 5 and Figure 6 depict the relative performance \nof the above algorithms for selected viewpoints and Figure 7 \ndoes for the entire frame. Results show that FS-JM&D has \noutperformed FS-MOTION by over 1 dB. This is clear \nevidence that the use of disparity redundancies together with \nthe motion can greatly improve the compression efficiency. \nCombining ES with JM&D reduces coding complexity by \napproximately 95% this can be seen in Table 1. This is also \nObject \nparameter \nStrategy \nparameter \nObject \nparameter \nStrategy \nparameter \nObject \nparameter \nStrategy \nparameter \nx y Reference \nIII - 395\nAuthorized licensed use limited to: University of Surrey. Downloaded on May 12,2010 at 09:36:09 UTC from IEEE Xplore.  Restrictions apply. \nnoted in comparing ES with a motion only full search, ES \ngives an 84% decrease in coding complexity. Combining ES \nwith JM&D or with a motion only search results in a small \nreduction in image quality, as seen in Figures 5-7.  \n \n4. CONCLUSION \nThis paper proposes a novel technique to exploit motion \nand disparity redundancies in 3D integral video sequence and \nlow complexity optimization technique. Experimental results \nshow that ES based joint motion and disparity estimation \ntechnique achieve over 1 dB objective quality gain while \nmaintaining up to 94% computational cost saving. \n \n \n \nTABLE 1 \n \n \n5. REFERENCES \n \n[1] A Aggoun: \u2018A 3D DCT Compression Algorithm For \nOmnidirectional Integral Images\u2019 Proceedings of IEEE \nICASSP 2006, Volume 2, 14-19 May 2006 Page(s):II-517 - \nII-520. \n[2] Charles Darwin, \u201cThe Origin of Species: By Means of \nNatural Selection or the Preservation of Favoured Races in \nthe Struggle for Life (Bantam Classic),\u201d Bantam Classics, \nReprint. 1999 \n[3] Fitzpatrick, J.M., Grefenstette, J.J. and Van-Gucht, D. \n\u201cImage registration by genetic search,\u201d Proceedings of \nSoutheastcon 84, Louisville, KY, 460-464, Apr 1984. \n[4] Grefenstette, J.J. and Fitzpatrick, J.M. \u201cGenetic search with \napproximate function evaluations,\u201d Proc. Intl. Conf. on \nGenetic Algorithms and their Applications, Pittsburgh, PA, \n112-120, Jul. 1985. \n[5] Man F. So & Angus Wu, \u201cHardware Implementation of \nFour-Step Genetic Search Algorithm\u201d, IEEE Signal \nProcessing Society 1999 Workshop on Multimedia Signal \nProcessing, Copenhagen, Denmark, September 1999. \n[6] Xu Yuelei, Bi Duyan and Mao Baixin, \u201c A Genetic Search \nAlgorithm For Motion Estimation\u201d, Proceedings of 5th \nInternational Conference on Signal Processing \nProceedings, Beijing, China, 2000. \n[7] Guanghua Qiu, Chaohuan Hou, \u201cA New Fast Algorithm \nfor the Estimation of Block Motion Vectors\u201d, Proceedings \nof 3rd International Conference on Signal Processing, \nBeijing, China, 1996. \n[8] Shen Li, Wei-pu Xu, Hui Wang, Nan-ning Zheng, \u201cA \nNovel Fast Motion Estimation Method Based on Genetic \nAlgorithm\u201d, Proceedings of International Conference on \nImage Processing, Kobe, Japan, 1999. \n[9] Ingo Rechenberg, Evolutionsstrategie '94. Stuttgart: \nFrommann-Holzboog 1994. \n[10] K. Ponudurai, W.A.C. Fernando and K.K. Loo, \u201cJoint \nMotion and Disparity Estimation in Stereo Video \nSequences Using Evolutionary Strategy,\u201d Proceedings of \n1st Regional Conference on ICT and E-Paradigms, \nColombo, Sri Lanka, 2004. \n[11] K. Ponudurai, W.A.C. Fernando and K.K. Loo, \u201cJoint \nMotion and Disparity Estimation in Stereo Video \nSequences Using Evolutionary Strategy,\u201d Proceedings of \nThe Seventh International Symposium on Wireless \nPersonal Multimedia Communications, Abano Terme, \nItaly, September 2004. \n \nNumber of search points  \nFull search ES search \nComplexity \nMotion 1024 132 87.11% \nDisparity 3072 183 94.04% \nViewpoint 8\n25\n27\n29\n31\n33\n35\n37\n39\n41\n0.5 1 1.5 2 2.5 3\nBitrate (bpp)\nPS\nNR\n \n(dB\n)\nFS-MOTION\nFS-JM&D\nES-MOTION\nES-JM&D\n \nFigure 6 Objective quality comparisons for the viewpoint 8 \nViewpoint 2\n25\n27\n29\n31\n33\n35\n37\n39\n41\n0.5 1 1.5 2 2.5 3\nBitrate (bpp)\nPS\nNR\n \n(dB\n)\nFS-MOTION\nFS-JM&D\nES-MOTION\nES-JM&D\n \nFigure 5 Objective quality comparisons for the viewpoint 2 \nIntegral\n25\n27\n29\n31\n33\n35\n37\n39\n41\n0.5 1 1.5 2 2.5 3\nBitrate (bpp)\nPS\nNR\n \n(dB\n)\nFS-MOTION\nFS-JM&D\nES-MOTION\nES-JM&D\n \nFigure 7 Objective quality comparison for the complete \nintegral video \nIII - 396\nAuthorized licensed use limited to: University of Surrey. Downloaded on May 12,2010 at 09:36:09 UTC from IEEE Xplore.  Restrictions apply. \n"}