{"doi":"10.1080\/0968776970050203","coreId":"14318","oai":"oai:generic.eprints.org:239\/core5","identifiers":["oai:generic.eprints.org:239\/core5","10.1080\/0968776970050203"],"title":"Quality criteria for multimedia","authors":["Rushby, Nick"],"enrichments":{"references":[{"id":1882125,"title":"29Nick Rushby Quality criteria for multimedia","authors":[],"date":"1975","doi":null,"raw":"29Nick Rushby Quality criteria for multimedia Mager, R. F. (1975), Preparing Instructional Objectives, Belmont CA: Fearon Publishers.","cites":null},{"id":200517,"title":"Evaluating interactive multimedia courseware - a methodology',","authors":[],"date":"1993","doi":"10.1016\/0360-1315(93)90034-g","raw":"Barker, P. and King, T. (1993), 'Evaluating interactive multimedia courseware - a methodology', Computers and Education, 24 (4), 307-319.","cites":null},{"id":200521,"title":"Exploring open and distance learning,","authors":[],"date":"1992","doi":"10.1080\/0268051980130204","raw":"Rowntree, D. (1992), Exploring open and distance learning, London: Kogan Page.","cites":null},{"id":200520,"title":"Producing Instructional Systems,","authors":[],"date":"1984","doi":null,"raw":"Romiszowski, A. J. (1984), Producing Instructional Systems, London: Kogan Page, 215ff.","cites":null},{"id":200519,"title":"Quality criteria for multimedia","authors":[],"date":"1975","doi":"10.1080\/0968776970050203","raw":null,"cites":null},{"id":200518,"title":"Report of the Task Force Educational Software and Multimedia (Working document of the Commission Services),","authors":[],"date":"1996","doi":null,"raw":"European Commission (1996), Report of the Task Force Educational Software and Multimedia (Working document of the Commission Services), SEC (96), 1426, Brussels.","cites":null},{"id":200522,"title":"The Performance Improvement Process:","authors":[],"date":"1994","doi":null,"raw":"Savage, C. (1994), The Performance Improvement Process: PIP, Bromley: Sundridge Park Management Centre.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"1997","abstract":"The meaning of the term quality as used by multimedia workers in the field has become devalued. Almost every package is promoted by its developers as being of the \u2018highest quality\u2019. This paper draws on practical experience from a number of major projects to argue, from a quality\u2010assurance position, that multimedia materials should meet pre\u2010defined criteria relating to their objectives, content and incidence of errors","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/14318.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/239\/1\/ALT_J_Vol5_No2_1997_Quality%20criteria%20for%20multimedi.pdf","pdfHashValue":"806937b02291c77bd5b2bd3b9d1949180a3c61f2","publisher":"Universit of Wales Press","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:239<\/identifier><datestamp>\n      2011-04-04T09:21:39Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/239\/<\/dc:relation><dc:title>\n        Quality criteria for multimedia<\/dc:title><dc:creator>\n        Rushby, Nick<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        The meaning of the term quality as used by multimedia workers in the field has become devalued. Almost every package is promoted by its developers as being of the \u2018highest quality\u2019. This paper draws on practical experience from a number of major projects to argue, from a quality\u2010assurance position, that multimedia materials should meet pre\u2010defined criteria relating to their objectives, content and incidence of errors.<\/dc:description><dc:publisher>\n        Universit of Wales Press<\/dc:publisher><dc:date>\n        1997<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/239\/1\/ALT_J_Vol5_No2_1997_Quality%20criteria%20for%20multimedi.pdf<\/dc:identifier><dc:identifier>\n          Rushby, Nick  (1997) Quality criteria for multimedia.  Association for Learning Technology Journal, 5 (2).  pp. 18-30.  ISSN 0968-7769     <\/dc:identifier><dc:relation>\n        10.1080\/0968776970050203<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/239\/","10.1080\/0968776970050203"],"year":1997,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"Quality criteria for multimedia\nNick Rushby\nPA Consulting Group, 123 Buckingham Palace Road, London SW1W 9SR\nThe meaning of the term quality as used by multimedia workers in the field has become devalued.\nAlmost every package is promoted by its developers as being of the 'highest quality'. This paper draws\non practical experience from a number of major projects to argue, from a quality-assurance position,\nthat multimedia materials should meet pre-defined criteria relating to their objectives, content and\nincidence of errors.\nWhat do we mean by quality?\nAlmost every piece of multimedia software is extolled by those who developed it and those\nwho publish it as being of the 'highest quality'. In practice, most multimedia packages\ncontain content and software errors, and do not realize their full potential in terms of\ncommunicating a message that endures. This does not mean to say that the users are\ndisappointed with their multimedia. They may not realize that the communication could\nbe more effective, they may not notice some of the errors, or they may have been led to\nbelieve that this is the natural order of things. The accepted meaning of the term quality as\nused by workers 'in the field' has become devalued. Discussions with typical purchasers\nand users over recent months indicate that they are still remarkably tolerant of low quality.\nIn this respect, multimedia lags behind much of the IT industry, and it is unlikely that the\ntolerance will last much longer. As users expectations and demands increase, those\nmultimedia developers who have quality processes in place and are able to assure the\nquality of their products will have a clear business advantage.\nThe European Commission has identified among its priorities for educational multimedia,\nthe need for quality control - methods and procedures evaluating the technical quality of\neducational software and the infrastructure for checking and certifying quality procedures\n(European Commission, 1996).\nThis paper draws on the practical experience of managing and commissioning a large\n18\nALT-J Volume 5 Number 2\nnumber of complex multimedia projects for major clients. It argues that we should set our\nexpectations, both as developers and as users, far higher than they are at present, and work\ntowards creating multimedia software that does what the developers and publishers claim it\nwill do. In this context, assuring the quality of multimedia means providing an assurance\nthat the product meets specific criteria, typically in respect of its effectiveness and\nrobustness. Unless we set out those criteria in advance, it is difficult to manage a project so\nas to achieve them, or even to recognize whether they have been achieved at all. It is easier\nto improve the quality of any product if we know what criteria we are achieving now.\nFigure l:The project manager's dilemma\nNot all multimedia needs to be of the highest quality. Excellence may be sacrificed\ndeliberately to reduce costs or time, to meet constraints imposed on the project. Balancing\ntime, cost and quality is the dilemma that faces every project manager.\nHowever, within those constraints it behoves us to deliver the most quality that we can and,\nas users, to be less tolerant of multimedia that falls short of our expectations. In the body\nof this paper I look at the impact of rapid applications development, the quality of\ncommunication, and the quality of the product, discussing what criteria are appropriate\nand giving some thought to how they might be achieved.\nThe Impact of Rapid Applications Development\nThere are some very good reasons why a Rapid Applications Development (RAD)\napproach to multimedia, introducing a highly parallel, iterative model as shown in Figure\n3, is used in preference to a more traditional waterfall process shown in Figure 2. A major\nbenefit is the early and continual involvement of end users, which maximizes their\ncommitment to the completed package. However, giving users a real say in the evolution of\nthe package through its prototypes, and empowering them to suggest significant changes to\nstructure, content and functionality, results in significant rework.\nA significant reason why projects overrun is not because of poor productivity but the\namount of work that has to be re-done late in the project timeframe. An important\nconclusion to draw is that projects will more easily hit their targeted end-dates if rework is\nidentified early in the project. Traditional software development approaches do not\npromote the early discovery of rework - typically major amounts of rework are identified\nonly during testing after all the software has been 'completed'.\nChanges suggested by users are identified earlier through the use of iterative development\ntechniques. Each iteration delivers a working component of the final system and ensures\n19\nNick Rushby Quality criteria for muttimedia\nAcceptance\nSpecification\nSys design Integration test\nDetailed design\nProgramming I Figure 2: The traditional\n\"waterfall' development\nInitial development Update\nRequirements analysis & Requirements analysis &\nPROGRAMMING PROGRAMMING\nUnit test\nIntegration test Integration test\nSystem\nAcceptance Acceptance\nTime\nKno\nRev\nRmwrt '\nDiscovery \" ^ t\nUndiK\nRevroi\nMl\nork\nWork To\nBe Done\nY\nQuaMy\novered 1 9\nWork Really\nDone\nFigure 4:The rework cyde\nFigure 3:The Rapid\nApplications\nDevelopment (RAD)\n20\nAir-J Volume 5 Number 2\nthat effective feedback of a functional and technical nature is obtained. Care must be taken\nto ensure that the changes to the successive prototypes are managed carefully, and that\nquality is not damaged by a torrent of amendments.\nIn traditional developments, much of the documentation produced is done so to facilitate\ncommunication between the team members or between the team and outside agencies. In\nRAD, documentation is primarily produced for the business and operational users of the\nsystem. Some key baseline documents are produced within the team but these are brief,\nfactual and instructive rather than huge manuals or specifications. As we shall see later, the\nabsence of a detailed specification is a potential problem for the testing process.\nA key aspect is that quality assurance and testing become continual processes throughout\nthe lifetime of a package. Testing can start as soon as the first prototype has taken shape,\nand the testing procedures then evolve in step with the package.itself.\nQuality of communication\nThe evaluation of multimedia packages - in particular of multimedia training packages -\nhas been extensively discussed elsewhere. Comprehensive and authoritative accounts are\ngiven by Romiszowski (1984), Rowntree (1992) and Barker and King (1993). In practice,\nevaluation is honoured more in the breach than in its observance. Evaluation - literally,\nestablishing the value - comes at the end of a project, after the excitement of developing\nthe package and implementing it in the field, after the plaudits, and after the adrenaline\nhas ceased to flow. The team is tired; the project is often over budget; there is the tiresome\nprocess of responding to the errors which are starting to be discovered by real users;\neveryone is looking forward to the next exciting project; and there is little enthusiasm for\nestablishing the value of something that has been completed. Why spend more time and\nresources on proving what is already known? The package is clearly successful and is\ndelivering real value.\nIn educational environments enquiry into outcomes, and into how and why they came\nabout, is a legitimate occupation, justified by its contribution to our understanding of\ncommunication and cognition, which can be funded and which can yield papers published\nfor the benefit and enlightenment of others. The quality of the evaluations is variable, but\nthe best add value to the multimedia industry as well as valuing its products. It is less\ncommon to find examples of evaluation in the business world that go beyond superficial\nenquiries of users to find our whether they enjoyed the multimedia experience and\nsubjective measures of whether it was helpful to them. Perversely, summative (post-hoc)\nevaluation is more helpful to those developing the package, since it helps them improve the\nquality and effectiveness of their future products, rather than assuring the quality of work\nin progress now. If the timescale of the project permits, it may be possible to include some\nformative evaluation. This aims to establish the value of the development processes and\ntheir outcomes at intermediate points throughout the project, the results being used to\ninform the later stages, so making it possible to improve quality as the work progresses.\nA more relevant activity is validation - establishing whether the multimedia package has\nmet its goals in bringing about the correct changes in knowledge, behaviour and attitude in\nthe intended users. If it does not, then it is unfit for the purpose for which it was\ncommissioned and produced. This is a key quality issue.\n21\nNick Rushby Quality criteria for multimedia\nDesired changes should be set out in advance as part of the quality criteria for the package.\nThey may look similar to the learning objectives advocated by Mager (1975): 'After\nworking through the package the learner will be able to . . . \" The problem with such\nobjectives is that they assume a perfect user who is not able to do whatever it is before\nworking through the package and, if the package meets its quality criteria, is able to do it\nafterwards. Unfortunately, real users are less than perfect, and there are some who will\nnever be prompted by even the most effective package to acquire new knowledge or skills,\nto change their behaviour or their attitude. We need to express the criteria in statistical\nterms so as to aggregate the changes in a number of users. As a consequence of using the\npackage:\n\u2022 '90 per cent of trainees will be able to demonstrate their competence in . . . ' ;\n\u2022 'the average time spent retrieving marketing information will fall by 20 per cent';\n\u2022 'customer complaints will fall by at least 45 per cent'.\nIn the long term, success against these criteria can be measured across the whole of the\nuser population by determining the situation now (for example, how many trainees can\ndemonstrate these competences? how long does it take to retrieve marketing information?\nhow many customer complaints do we receive?), and then repeating the measurements\nwhen the package is established to determine the change. However, it would be helpful to\nhave an earlier assessment of the changes that the package will bring about, to inform the\nquality assurance process and acceptance. To do this we need:\n\u2022 a representative sample of users - a sufficient number to ensure that any changes we\nmeasure are not due to random effects;\n\u2022 a means of establishing their knowledge, skills and behaviour before using the package;\n\u2022 a means of establishing their knowledge, skills and behaviour after using the package;\n\u2022 sufficient time to carry out the validation.\nNote that a single sample of users will suffice for validation: there is no need for a control\ngroup since the aim is to find out whether the desired changes come about, not whether the\nprocess is better, faster, cheaper, etc. than an alternative way of doing things.\nCare is needed to devise instruments that are themselves reliable and valid. For example, if\nthe purpose of the package is to train users - to help them acquire competences which they\ndo not already have - then some form of competence-based assessment is indicated, rather\nthan written questions that may only test their knowledge. The competences assessed\nshould be those addressed by the package. The Performance Improvement Process\ndeveloped at PA Management Centre at Sundridge Park (Savage, 1994) asks users - in this\ncase delegates on management development programmes - and their line managers to rate\ntheir perception of a user's competences on a 4- or 6-point scale before and after the\nprogramme, and again three to six months after the end of the programme. The responses\nare scored using an optical mark reader and the improvements in perceived competence for\nindividuals and the group as a whole are computed automatically. It should be noted that\nthis technique is based on the perceptions that the individual and his\/her line manager have\nabout specific competences rather than a more objective assessment carried out by a trained\n22\nALT-] Volume 5 Number 2\nassessor in the workplace; and also that the binary 'does\/does not demonstrate\ncompetence' is replaced by a 6-point scale. Finally, this approach will not, of course,\nilluminate unexpected outcomes of learning through multimedia. Although no rigorous\nresearch has been carried out to validate the Performance Improvement Process,\nexperience has shown good agreement between the individuals' perceptions and those of\ntheir line managers, indicating an acceptable level of reliability for the instrument. Self-\nassessment by questionnaire reduces the time and resources required to a practicable level.\nThere seems no reason why, with carefully worded questions, this technique should not\nalso be applied to the evaluation of other forms of multimedia communication, providing\na systematic means of establishing the quality of the package against pre-determined\ncriteria.\nThe validation exercise should also look at the usability of the package in question. The\ndevelopment team may have made certain assumptions about the users' familiarity with\nkeyboards, or their levels of knowledge before using the package. If these assumptions are\nnot justified, the users may be unable to operate the package correctly or may be\nbewildered by the content. It follows that the effectiveness of the package will be\nsignificantly compromised.\nThe RAD process lends itself to the incorporation of validation exercises at each iteration.\nIf the validation process starts early in the project, misconceptions and mis-directions are\ndetected quickly and the results can be used to improve the effectiveness of later work. A\nmini-evaluation may be used to address part of the whole package, or the exercise may take\nthe form of a workshop where a few key users gather together to work through a prototype\nand discuss their experiences with members of the development team. The aim is to\nprovide continual assurance that the package will achieve its objectives.\nQuality of the software\nWhat are the quality criteria for the software itself? First, it is reasonable to ask that the\ncontent be accurate and up to date. Some content is relatively static; other content may\ndate rapidly so that currency can be maintained only by frequent new releases of the\nmultimedia package. As we shall see, frequent updates bring their own inimitable problems.\nSecondly, it is a basic requirement - often sadly neglected - that there should be no\ntypographical errors. Thirdly, we need to think about the number of software errors that an\naverage user should have to endure while using the package. Let us consider two examples.\n1. It may reasonable that the average user of a multimedia reference package should\nencounter no more than one error during a year of use. If this average user works with the\npackage for one hour each day, this amounts to .220 hours in that year. If one error in a\nyear is acceptable, then for safety, the mean time between software errors or failures\n(MTBF) should be no less than 500 hours. An MTBF of 220 hours would result in an\naverage of one error per year with some users experiencing more. To reach the criterion of\nnot more than one error per year we should therefore err on the side of caution.\n2. Students or trainees working through a multimedia induction programme lasting four\nhours should have only a 2 per cent probability of encountering an error. This equates to a\nminimum of 200 hours MTBF.\n23\nNick Rushiy Quality criteria for multimedia\nIn both these examples, we can define an error as an event which causes the multimedia\npackage to stop running, or exhibit some behaviour which the user realises is not normal.\nIt is arguable that these are unreasonably stringent criteria. Those of us who use IT continually\nin our daily lives have grown accustomed to much higher error rates. I was saddened to hear one\ncolleague suggest that he would find one error per hour acceptable - although on further\ninterrogation he admitted that this was based on his expectations rather than on his aspirations.\nA selection of published software reviews indicated that reviewers are quite tolerant of errors.\nGiven that they typically only use the review copy for a few hours and encounter one or more\nerrors during that time, we may infer that a MTBF of 10 hours is not uncommon.\nI would argue that these more stringent criteria are not unreasonable but are comparable with\nthe errors rates of other job aids. A desktop computer with an MTBF of 500 hours would\nbreak down on average every three months, and would be deemed unreliable. The operating\nsystem or network probably fails more frequently. The combination fails more often still,\nbecause the error rates are additive. Error-prone software just makes the problem worse.\nThe key is to set appropriate quality criteria and engineer the multimedia package to meet\nthem.\nAccurate and up-to-date content\nResponsibility for providing the correct content (text, graphics, sound, video, etc.) and\nensuring that it is up to date must be clearly allocated. In a large project it can be difficult\nto keep track of all the content and it may be helpful to set up a database to hold\ninformation on each item of content and its status. Typically, the life of each item will\nfollow the cycle shown in Figure 5.\nIdentification\nReview and\namendment\nRelease\n\\\nCommissioning\n\\\nDelivery\n1\nCheck accuracy\nand currency\nInclusion in\nthe package\n\u2022 Revision\nfigure 5:The content cycle\nA database can simplify the process of ensuring that all the content is tracked and of\nproducing status reports on specific items. Time invested in maintaining the database and\nplotting the progress of each item as it moves through the cycle will be more than repaid\nlater in the project, with greater certainty on the status of each item and less rework to\nreplace outdated content.\nBecause of the difficulty in plotting a simple route through a complex multimedia package\nthat takes in every individual screen, it may be easier for the person (or people) responsible\n24\nALT-J Volume 5 Number 2\nfor content to work from hard copies of the screens (generated from the authoring tool)\nrather than from the package itself. The hard copy is likely to be in black and white rather\nthan in colour (for reasons of cost) and it may be necessary to carry out a final check with\nthe actual package to ensure that the colours are correct.\nTypographical accuracy and consistency\nWe require a rigorous process to ensure that there are no typographical errors or\ninconsistencies in the content. Receiving and processing most of the text electronically will\ntend to reduce the number of residual typographical errors (in contrast, continual retyping\ntends to increase the number of errors), but there should be a systematic proof-reading\nstage. This is best carried out by a professional proof-reader who has had no previous\ninvolvement with the project and is not compromised by what he or she expects to see. A\ngood proof-reader will also check for consistency of style and layout, and can also be used\nto comment on grammar, readability, obscure terminology and the inappropriate use of\nacronyms or jargon. The proof-reader may find it easier to work from hard copy rather\nthan follow a long and systematic route through the package itself.\nSoftware accuracy\nThe software that drives a multimedia package is complex and intangible. Its complexity\nmakes it prone to errors, and it is difficult to demonstrate the absence of errors in\nsomething that cannot easily be inspected.\nThe traditional testing process involves someone working through the package, usually at\nthe Beta testing stage when the package is essentially complete, and recording any errors\nencountered. As the number of remaining errors decreases, it takes longer and longer to\nfind each one. Since the psychological reward for testing is finding errors (and thus proving\nyour superiority over the person who wrote the program), motivation decreases, and the\ntesting process ends when the tester decides that the reward does not justify further efforts.\nThis, of course, does not mean that there are no errors left to be discovered.\nIf the software is rigorously structured, it is possible to carry out all-path validation. This is a\nprocess whereby, with a small number of carefully devised tests, every path in the program is\nexercised and can be validated as correct. It can therefore be demonstrated there are zero\nerrors. However, the hyperlinking that is at the heart of most interesting multimedia\nprograms, and that and adds significantly to its value, compromises that structure. There is\nan unpractically large number of paths to be validated: time and resources do not permit\ntotal validation and we must resort to exhaustive testing that still leaves a finite possibility of\nan undiscovered error. The challenge now is to devise and manage the testing process to\nachieve the criteria for MTBF within an affordable budget and realistic timescale.\nThe test schedule\nIt is reasonable to suppose that the pattern of usage for most multimedia packages follows\nthe Pareto Principle: 90 per cent of the users will only use 10 per cent of the total package.\nClearly that 10 per cent requires careful testing, but so does the remaining 90 per cent.\nTotal coverage of every single path through the package may be impractical for the reasons\ndiscussed earlier; we thus have to devise a systematic schedule to ensure that:\n25\nNick Rushby Quality criteria for multimedia\nevery piece of content is visited and checked;\nall the main paths are followed and exhaustively checked;\nthere is reasonable justification to assume that any paths which are not to be\nexhaustively checked do really work correctly (for example, in the figure below, if the\nsoftware is rigorously structured, and the functionality and content of section A has\nbeen exhaustively tested through path a, and the hyperlink path P which goes to A and\nthen returns has also been checked, then we might reasonably assume that A will work\ncorrectly through path a without testing the whole of A again);\nFigure 6: Component validation\n\u2022 careful attention is paid to areas which are likely to be error-prone (for example, areas\nthat are more than typically complex);\n\u2022 particular attention is paid to areas where errors have been discovered in the past - we\nacknowledge that a small number of errors will escape detection and will manifest\nthemselves only once the package is released and used in the field, and when they are\nfound and corrected the package must be re-tested with additional tests that exercise\nthe area that caused the error; those new tests must be retained in the evolving schedule.\n\u2022 the sequence of steps in the test is documented and followed so that the test is always\nrepeatable, which makes it possible to isolate the circumstances leading up to the error\nand collect forensic evidence that will identify the cause. Non-repeatable or random\nerrors provide an intellectual challenge to the programmer but are not conducive to\nquality multimedia packages.\nThere are four steps in developing the software test schedule, as follows.\n1. Identify the key scenarios - the main operations the user will carry out.\n2. Develop and document a series of steps that will exercise the package through every part\nof that scenario.\n3. Add perverse actions. Naive users do not always do what you expect them to, through\ninnocence and inexperience and sometimes through frustration. The schedule should\ninclude some totally unexpected and perverse steps (for example, resting a book on top of\nthe mouse so that the mouse input buffer suffers terminal overload).\n4. Keep the test schedule up to date. The schedule, which may run to many thousands of\nsteps, is an evolving document. Each time the structure or functionality of the multimedia\npackage changes, the schedule must be amended to include the new and changed scenarios.\nIf an error is discovered and corrected, the schedule should be amended to ensure that this\nproblem area is included for further tests.\nThe test schedule should be developed by someone who is not a member of the\n26\nALT-] Volume 5 Number 2\nprogramming team but who is familiar with the structure of the package and has a good\noversight of its intended content and functionality. Separation from the programming\nteam reduces the possibility that the tests will be compromised by assumptions as to what\nthe package does.\nThe absence of an initial formal specification in an RAD environment creates problems for\nthe test designer because there is no single definitive document describing the functionality\nand content against which to test the package. If the test designer is working only from a\nprototype without an overall understanding of the package itself, there is a risk that the\ntests will be defined by inaccurate code: there is a tendency to assume that 'this is what the\npackage does and is therefore what the package should do'.\nThe schedule will be a multi-page document setting out a series of steps that instruct the\ntester what to do and what should happen, with columns to record the success of failure of\neach step and a description of any unexpected result. An example can be seen in Figure 7.\nFigure 7: Section from a\ntest schedule\nPackage name Test no: date:\nstep\n1.\n2.\n3.\naction expected result * o r x\ntester:\nactual result\netc.\nThe final schedule will probably be highly detailed to the point of being obsessive, but the\ndevil of testing is in the detail. For example, an important (but often overlooked) area of\ntesting is to verify that all the hot buttons surrounding icons and hot text are the correct\nshape and size and are in the correct position. Such attention to detail may not endear the\ntest schedule (or its author) to the programming team!\nThere is a balance to be struck between the programming team and the QA (Quality\nAssessment) team. A certain amount of intellectual rivalry sharpens the wits so that the\nprogramming team takes greater care with its code and the QA team becomes more\npenetrating in its tests. But the ultimate aim is to produce a quality package - not to score\npoints against the opposition, and the creative tension must be carefully managed. The\ncontinual iterative process inherent in RAD enables the first test schedules to be devised\nand run very early in the development process. Feedback on errors can start with the first\nprototype so that the programming team can embark on continual quality improvement.\nIn a traditional waterfall model, the detection and reporting of errors is held back until the\nfinal stages of the project when, as we have seen, it is likely to be too late for any real\nimprovement in quality.\nThe evolving test schedule can be used not only as an assurance of the quality of each\nprototype but also as a development tool for the programming team to support its own\ninternal testing. And this provides an opportunity for the programming team to suggest\nimprovements to the schedule.\n27\nNick Rushby Quality criteria for multimedia\nRegression testing\nAs each prototype is completed it is tested against the evolving test schedule. Almost\ninvariably, there will be discrepancies. These are of various kinds:\n\u2022 the test schedule may be wrong - the QA team has made incorrect assumptions or\nstraightforward mistakes (it happens);\n\u2022 the package does not behave as expected and the resolution of the discrepancy is to\nchange the declared functionality of the package (for example, the backtracking\nthrough a lengthy hypertext sequence returns to an intermediate point instead of the\nlast screen);\n\u2022 the package does not behave as expected and clearly needs to be changed; this will\ninclude all occurrences of abnormal terminations and situations where the user is\nbewildered by the outcome.\nBecause the test sequence is documented step by step, the programming team should be\nable to repeat the sequence leading up to the error and obtain the forensic evidence it needs\nto correct it:\nHaving corrected a batch of errors, the test schedule must then be run again in its entirety -\nand again, and again until all the discrepancies have been resolved. It is quite possible that,\nin correcting one identified error, another has been introduced (during the development of\nIBM's MVS, it was estimated that 20 per cent of the bugs in the software were caused by\ncorrections that failed).\nError management\nThe traditional testing process yields a stream of errors which eventually decreases until\nthe package is deemed to be 'working'. In contrast, systematic testing against a carefully\nconstructed schedule results in a large number of discrepancies each time the tests are run,\nuntil the final tests when no errors are left. The approach is used precisely because its\ngreater effectiveness finds more errors. But the greater rate of error-detection brings its\nown problems in managing the corrections, integrating the revisions, and ensuring that re-\ntesting is managed efficiently and effectively. There need to be processes to allocate known\nerrors to the appropriate members of the development team, and to co-ordinate the\nchanges they make.\nComputer-assisted multimedia testing\nThe test schedule for a complex piece of multimedia software may run to several hundreds\nof pages and many thousands of steps. At each step, the tester must check carefully that\nthe package does what is intended, and that what is presented on the screen is correct. One\npass through the full test can take several days of meticulous work.\nAutomated tools are available to assist in the testing of software (see, for example Graham\net al, 1995). The computer-assisted software testing (CAST) process typically involves:\n\u2022 identifying the user-scenarios;\n\u2022 documenting the paths through these scenarios;\n28\nALT-J Volume 5 Number 2\n\u2022 'teaching' the CAST package to work through the paths and automatically generate a\nscript that will enable the CAST package to replicate the test automatically;\n\u2022 editing the script to deal with situations not encountered in the teaching phase;\n\u2022 running an automated test in whole or in part each time the software is changed, to\nensure that it meets the quality criteria.\nMost CAST packages have been developed for use in a development process that starts\nwith an agreed software specification, and with software that runs in a true Windows\nenvironment where the attributes of each window are unambiguous and known. They can\nbring improved productivity to the testing of software written using, for example, Visual\nBasic or Visual C. Unfortunately, the RAD process tends to eliminate the need for a\nformal specification, replacing it with a series of iterative prototypes. There is no longer a\ndefinitive list of each window and its attributes that can be used by a CAST package. The\nstarting point for the test schedule is an early (perhaps the first) prototype. This presents a\nmajor problem for current CAST packages because there is little or nothing for them to\nexamine. The automated test becomes a trivial pass through the multimedia package,\neither failing everything or failing nothing.\nMany tools also offer facilities for testing software destined for client-server environments,\nchecking the impact of increasing numbers of users, the complexity of interactions, etc. In\nmost cases these are not relevant for the kinds of multimedia were are considering here.\nPA's experience indicates that, at present, computer-assisted software testing has little to\noffer for packages that are written using tools such as Authorware and ToolBook rather\nthan Visual Basic.\nConclusion\nMultimedia presents a unique set of challenges for developers who aim for zero errors in\ntheir work and who are prepared to meet pre-defined criteria for the quality of the\npackages they produce. However, it is no longer acceptable to tolerate errors and lack of\neffectiveness. Market forces will favour those who can demonstrate quality in their\nproducts: those who cannot are likely to fade quietly into obscurity.\nNote\nAn earlier version of the paper was presented at the PEG Conference in Sozopol, Bulgaria,\nin May 1997.\nReferences\nBarker, P. and King, T. (1993), 'Evaluating interactive multimedia courseware - a\nmethodology', Computers and Education, 24 (4), 307-319.\nEuropean Commission (1996), Report of the Task Force Educational Software and\nMultimedia (Working document of the Commission Services), SEC (96), 1426, Brussels.\nGraham, D., Herzlich, P., and Morelli, C. (1995), CAST Report, London: Cambridge\nMarket Intelligence.\n29\nNick Rushby Quality criteria for multimedia\nMager, R. F. (1975), Preparing Instructional Objectives, Belmont CA: Fearon Publishers.\nRomiszowski, A. J. (1984), Producing Instructional Systems, London: Kogan Page, 215ff.\nRowntree, D. (1992), Exploring open and distance learning, London: Kogan Page.\nSavage, C. (1994), The Performance Improvement Process: PIP, Bromley: Sundridge Park\nManagement Centre.\n30\n"}