{"doi":"10.1016\/j.chemolab.2010.05.014","coreId":"140441","oai":"oai:dspace.lib.cranfield.ac.uk:1826\/4743","identifiers":["oai:dspace.lib.cranfield.ac.uk:1826\/4743","10.1016\/j.chemolab.2010.05.014"],"title":"State-space independent component analysis for nonlinear dynamic process\nmonitoring","authors":["Odiowei, P. P.","Cao, Yi"],"enrichments":{"references":[{"id":37940038,"title":"A plant-wide industrial process control problem&quot;.","authors":[],"date":"1993","doi":"10.1016\/0098-1354(93)80018-i","raw":"Downs, J. J., and Vogel, E., 1993. \\A plant-wide industrial process control problem&quot;. Computers and Chemical Engineering, 17, pp. 245{255.","cites":null},{"id":37940031,"title":"Applied Smoothing Techniques for Data Analysis, The Kernel Approach with S-Plu Illustrations.","authors":[],"date":"1997","doi":"10.1007\/s001800000033","raw":"Bowman, A. W., and Azzalini, A., 1997. Applied Smoothing Techniques for Data Analysis, The Kernel Approach with S-Plu Illustrations. Clarendon Press, Oxford.","cites":null},{"id":37939957,"title":"Chemical process monitoring and fault diagnosis based on independent component analysis&quot;.","authors":[],"date":"2004","doi":"10.1109\/wcica.2004.1340933","raw":"Chen, G., Liang, J., and Qian, J., 2004. \\Chemical process monitoring and fault diagnosis based on independent component analysis&quot;. In 5th World Congress on Intelligent Control and Automation, p. 1646.","cites":null},{"id":37940041,"title":"Comparison of multivariate statistical process monitoring methods with applications to the eastman challenge problem&quot;.","authors":[],"date":"2002","doi":"10.1016\/s0098-1354(01)00738-4","raw":"Kano, M., Koji, N., Shinji, H., Ioro, H., Hiromo, O., Ramon, S., and Bhavik, R. B., 2002. \\Comparison of multivariate statistical process monitoring methods with applications to the eastman challenge problem&quot;. Computers and Chemical Engineering, 26, pp. 161{174.","cites":null},{"id":37940029,"title":"Fast and robust  algorithms for independent component analysis&quot;.","authors":[],"date":"1999","doi":null,"raw":"Hyv arinen, A., 1999. \\Fast and robust xed-point algorithms for independent component analysis&quot;. IEEE Transactions on Neural Networks, 3, pp. 626{634. 22ACCEPTED MANUSCRIPT ACCEPTED MANUSCRIPT","cites":null},{"id":37939960,"title":"Fault detection and diagnosis based on modi independent component analysis.&quot;.","authors":[],"date":"2006","doi":"10.1002\/aic.10978","raw":"Lee, J., Qin, S. J., and Lee, I., 2006. \\Fault detection and diagnosis based on modied independent component analysis.&quot;. AIChE Journal, 52(10), pp. 3501{ 3514.","cites":null},{"id":37940021,"title":"Fault Detection and Diagnosis in Industrial Systems.","authors":[],"date":"2001","doi":"10.1007\/978-1-4471-0347-9","raw":"Chiang, L. H., Russell, E. L., and Braatz, R. D., 2001. Fault Detection and Diagnosis in Industrial Systems. Springer, London.","cites":null},{"id":37939962,"title":"Historical data analysis based on plots of independent and parallel coordinates and statistical control limits.&quot;.","authors":[],"date":"2006","doi":"10.1016\/j.jprocont.2005.05.005","raw":"Albazzaz, H., and Wang, X. Z., 2006. \\Historical data analysis based on plots of independent and parallel coordinates and statistical control limits.&quot;. Journal of Process Control, 16, pp. 103{114. 21ACCEPTED MANUSCRIPT ACCEPTED MANUSCRIPT","cites":null},{"id":37940034,"title":"Kernel density estimation for an anomaly based intrusion detection system&quot;.","authors":[],"date":"2006","doi":null,"raw":"Xiaoping, S., and Sonali, A., 2006. \\Kernel density estimation for an anomaly based intrusion detection system&quot;. In Proceedings of the 2006 World Congress in Computer Science, Computer Engineering and Applied Computing, p. 161.","cites":null},{"id":37940018,"title":"Monitoring of multivariable dynamic processes and sensor auditing&quot;.","authors":[],"date":"1998","doi":"10.1016\/s0959-1524(98)00006-7","raw":"Negiz, A., and Cinar, A., 1998. \\Monitoring of multivariable dynamic processes and sensor auditing&quot;. Journal of Process Control, 8(56), pp. 357{380.","cites":null},{"id":37939933,"title":"Multidimensional visualization of principal component scores for process historical data analysis&quot;.","authors":[],"date":"2004","doi":"10.1021\/ie030816j","raw":"Wang, X. Z., Medasani, S., Marhoon, F., and Albazzaz, H., 2004. \\Multidimensional visualization of principal component scores for process historical data analysis&quot;. Industrial Engineering Chemical Research, 43, pp. 7036{7048.","cites":null},{"id":37940032,"title":"Non-parametric con bounds for process performance monitoring charts&quot;.","authors":[],"date":"1996","doi":"10.1016\/0959-1524(96)00010-8","raw":"Martin, E. B., and Morris, A. J., 1996. \\Non-parametric condence bounds for process performance monitoring charts&quot;. Journal of Process Control, 6(6), pp. 349{358.","cites":null},{"id":37940028,"title":"Nonlinear dynamic process monitoring using canonical variate analysis and kernel density estimations&quot;.","authors":[],"date":"2009","doi":"10.1016\/s1570-7946(09)70650-9","raw":"Odiowei, P., and Cao, Y., 2009. \\Nonlinear dynamic process monitoring using canonical variate analysis and kernel density estimations&quot;. IEEE Transactions on Industrial Informatics, 6(1), pp. 36{45.","cites":null},{"id":37940026,"title":"Pls, balanced and canonical variate realization techniques for identifying varma models in state space&quot;.","authors":[],"date":"1997","doi":"10.1016\/s0169-7439(97)00035-x","raw":"Negiz, A., and Cinar, A., 1997. \\Pls, balanced and canonical variate realization techniques for identifying varma models in state space&quot;. Chemometrics and Intelligent Laboratory Systems, 38, pp. 209{221.","cites":null},{"id":37940019,"title":"Statistical modelling of dynamic multivariate process using canonical variate analysis&quot;.","authors":[],"date":"2006","doi":"10.1109\/icinfa.2006.374115","raw":"Juan, L., and Fei, L., 2006. \\Statistical modelling of dynamic multivariate process using canonical variate analysis&quot;. In Proceedings IEEE International on Information and Automation, 2006. ICIA 2006, p. 218.","cites":null},{"id":37939958,"title":"Statistical monitoring of dynamic independent component analysis&quot;.","authors":[],"date":"2004","doi":"10.1016\/j.ces.2004.04.031","raw":"Lee, J., Yoo, C., and Lee, I., 2004. \\Statistical monitoring of dynamic independent component analysis&quot;. Chemical Engineering Sciences, 59, pp. 2995{3006.","cites":null},{"id":37940022,"title":"Statistical performance monitoring of dynamic multivariate processes using state space modelling&quot;.","authors":[],"date":"2002","doi":"10.1016\/s0098-1354(02)00012-1","raw":"Simouglou, A., Martin, E. B., and Morris, A. J., 2002. \\Statistical performance monitoring of dynamic multivariate processes using state space modelling&quot;. Computers and Chemical Engineering, 26, pp. 909{920.","cites":null},{"id":37939935,"title":"Statistical process control charts for batch operations based on independent component analysis&quot;.","authors":[],"date":"2004","doi":"10.1021\/ie049582+","raw":"Abazzaz, H., and Wang, X. Z., 2004. \\Statistical process control charts for batch operations based on independent component analysis&quot;. Industrial & Engineering Chemistry Research, 43(21), pp. 6731{6741.","cites":null},{"id":37939932,"title":"Statistical process control of multivariate processes&quot;.","authors":[],"date":"1995","doi":"10.1016\/0967-0661(95)00014-l","raw":"MacGregor, J. F., and Kourti, T., 1995. \\Statistical process control of multivariate processes&quot;. Control Engineering Practice, 3(3), pp. 403{414.","cites":null},{"id":37939937,"title":"Statistical process monitoring with independent component analysis&quot;.","authors":[],"date":"2004","doi":"10.1016\/j.jprocont.2003.09.004","raw":"Lee, J., Yoo, C., and Lee, I., 2004. \\Statistical process monitoring with independent component analysis&quot;. Journal of Process Control, 14, pp. 467{485.","cites":null},{"id":37939964,"title":"Statistical-based monitoring of multivariate non-gaussian systems analysis.&quot;.","authors":[],"date":"2008","doi":"10.1002\/aic.11526","raw":"Liu, X., Xie, L., Kruger, U., Littler, T., and Wang, S., 2008. \\Statistical-based monitoring of multivariate non-gaussian systems analysis.&quot;. AIChE Journal, 54(9), pp. 2379{2391.","cites":null},{"id":37940030,"title":"Synthesis of t2 and q statistics for process monitoring&quot;.","authors":[],"date":"2004","doi":"10.1016\/j.conengprac.2003.08.004","raw":"Chen, Q., Kruger, U., Meronk, M., and Leung, A. Y. T., 2004. \\Synthesis of t2 and q statistics for process monitoring&quot;. Control Engineering Practice, 12, pp. 745{755.","cites":null},{"id":37940024,"title":"System identi reduced order  and modelling via canonical correlation analysis&quot;.","authors":[],"date":"1983","doi":null,"raw":"Larimore, W. E., 1983. \\System identication reduced order ltering and modelling via canonical correlation analysis&quot;. In Proceedings of the American Control Conference, p. 445.","cites":null},{"id":37940036,"title":"Tennessee eastman challenge archive.","authors":[],"date":"2001","doi":"10.1016\/0959-1524(96)00031-5","raw":"Ricker, N. L., 2001. Tennessee eastman challenge archive.","cites":null},{"id":37939965,"title":"The application of independent component analysis in process monitoring.&quot;.","authors":[],"date":"2006","doi":"10.1109\/icicic.2006.164","raw":"Hongguang, L., and Hui, G., 2006. \\The application of independent component analysis in process monitoring.&quot;. In Proceedings of the 1st International Conference on Innovative Computing, Information and Control (ICICIC06), p. 97.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-08-15T00:00:00Z","abstract":"The cost effective benefits of process monitoring will never be over emphasised.\nAmongst monitoring techniques, the Independent Component Analysis (ICA) is an\nefficient tool to reveal hidden factors from process measurements, which follow\nnon-Gaussian distributions. Conventionally, most ICA algorithms adopt the\nPrincipal Component Analysis (PCA) as a pre-processing tool for dimension\nreduction and de-correlation before extracting the independent components (ICs).\nHowever, due to the static nature of the PCA, such algorithms are not suitable\nfor dynamic process monitoring. The dynamic extension of the ICA (DICA), similar\nto the dynamic PCA, is able to deal with dynamic processes, however\nunsatisfactorily. On the other hand, the Canonical Variate Analysis(CVA) is an\nideal tool for dynamic process monitoring, however is not sufficient for\nnonlinear systems where most measurements follow non-Gaussian distributions. To\nimprove the performance of nonlinear dynamic process monitoring, a state space\nbased ICA (SSICA) approach is proposed in this work. Unlike the conventional\nICA, the proposed algorithm employs the CVA as a dimension reduction tool to\nconstruct a state space, from where statistically independent components are\nextracted for process monitoring. The proposed SSICA is applied to the Tennessee\nEastman Process Plant as a case study. It shows that the new SSICA provides\nbetter monitoring performance and detect some faults earlier than other\napproaches, such as the DICA and the CVA. (C) 2010 Elsevier B.V. All rights\nreserved","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/140441.pdf","fullTextIdentifier":"http:\/\/dx.doi.org\/10.1016\/j.chemolab.2010.05.014","pdfHashValue":"f2282e311e35c7419b5eec73d62e847399d3af9e","publisher":"Elsevier Science B.V., Amsterdam.","rawRecordXml":"<record><header><identifier>\noai:dspace.lib.cranfield.ac.uk:1826\/4743<\/identifier><datestamp>2012-03-09T09:23:41Z<\/datestamp><setSpec>hdl_1826_19<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>State-space independent component analysis for nonlinear dynamic process\nmonitoring<\/dc:title><dc:creator>Odiowei, P. P.<\/dc:creator><dc:creator>Cao, Yi<\/dc:creator><dc:subject>Dynamic system Nonlinearity Principal component analysis Canonical variate analysis Independent component analysis Probability density function Kernel density estimation Process monitoring historical data-analysis multivariate processes<\/dc:subject><dc:description>The cost effective benefits of process monitoring will never be over emphasised.\nAmongst monitoring techniques, the Independent Component Analysis (ICA) is an\nefficient tool to reveal hidden factors from process measurements, which follow\nnon-Gaussian distributions. Conventionally, most ICA algorithms adopt the\nPrincipal Component Analysis (PCA) as a pre-processing tool for dimension\nreduction and de-correlation before extracting the independent components (ICs).\nHowever, due to the static nature of the PCA, such algorithms are not suitable\nfor dynamic process monitoring. The dynamic extension of the ICA (DICA), similar\nto the dynamic PCA, is able to deal with dynamic processes, however\nunsatisfactorily. On the other hand, the Canonical Variate Analysis(CVA) is an\nideal tool for dynamic process monitoring, however is not sufficient for\nnonlinear systems where most measurements follow non-Gaussian distributions. To\nimprove the performance of nonlinear dynamic process monitoring, a state space\nbased ICA (SSICA) approach is proposed in this work. Unlike the conventional\nICA, the proposed algorithm employs the CVA as a dimension reduction tool to\nconstruct a state space, from where statistically independent components are\nextracted for process monitoring. The proposed SSICA is applied to the Tennessee\nEastman Process Plant as a case study. It shows that the new SSICA provides\nbetter monitoring performance and detect some faults earlier than other\napproaches, such as the DICA and the CVA. (C) 2010 Elsevier B.V. All rights\nreserved.<\/dc:description><dc:publisher>Elsevier Science B.V., Amsterdam.<\/dc:publisher><dc:date>2011-09-08T09:13:15Z<\/dc:date><dc:date>2011-09-08T09:13:15Z<\/dc:date><dc:date>2010-08-15T00:00:00Z<\/dc:date><dc:type>Article<\/dc:type><dc:identifier>0169-7439<\/dc:identifier><dc:identifier>http:\/\/dx.doi.org\/10.1016\/j.chemolab.2010.05.014<\/dc:identifier><dc:identifier>http:\/\/dspace.lib.cranfield.ac.uk\/handle\/1826\/4743<\/dc:identifier><dc:language>en_UK<\/dc:language><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["issn:0169-7439","0169-7439"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2010,"topics":["Dynamic system Nonlinearity Principal component analysis Canonical variate analysis Independent component analysis Probability density function Kernel density estimation Process monitoring historical data-analysis multivariate processes"],"subject":["Article"],"fullText":"\u0000\u0002\u0002\u0003\u0004\u0005\u0003\u0006 \u0007\b\t\n\u000b\u0002\f\r\u0004\u0005\nState-space independent component analysis for nonlinear dynamic process\nmonitoring\nP.P. Odiowei, Y. Cao\nPII: S0169-7439(10)00090-0\nDOI: doi: 10.1016\/j.chemolab.2010.05.014\nReference: CHEMOM 2233\nTo appear in: Chemometrics and Intelligent Laboratory Systems\nReceived date: 25 December 2009\nRevised date: 20 May 2010\nAccepted date: 21 May 2010\nPlease cite this article as: P.P. Odiowei, Y. Cao, State-space independent component\nanalysis for nonlinear dynamic process monitoring, Chemometrics and Intelligent Labora-\ntory Systems (2010), doi: 10.1016\/j.chemolab.2010.05.014\nThis is a PDF file of an unedited manuscript that has been accepted for publication.\nAs a service to our customers we are providing this early version of the manuscript.\nThe manuscript will undergo copyediting, typesetting, and review of the resulting proof\nbefore it is published in its final form. Please note that during the production process\nerrors may be discovered which could affect the content, and all legal disclaimers that\napply to the journal pertain.\nAC\nCE\nPT\nED\n M\nAN\nUS\nCR\nIP\nT\nACCEPTED MANUSCRIPT\nState-Space Independent Component Analysis for\nNonlinear Dynamic Process Monitoring\nP.P. Odiowei and Y. Cao\u2217\nSchool of Engineering, Cranfield University, Bedford, MK43 0AL, UK\nThis version: May 20, 2010\nAbstract\nThe cost effective benefits of process monitoring will never be over emphasised.\nAmongst monitoring techniques, the Independent Component Analysis (ICA) is an\nefficient tool to reveal hidden factors from process measurements, which follow non-\nGaussian distributions. Conventionally, most ICA algorithms adopt the Principal\nComponent Analysis (PCA) as a pre-processing tool for dimension reduction and\nde-correlation before extracting the independent components (ICs). However, due\nto the static nature of the PCA, such algorithms are not suitable for dynamic pro-\ncess monitoring. The dynamic extension of the ICA (DICA), similar to the dynamic\nPCA, is able to deal with dynamic processes, however unsatisfactorily. On the other\nhand, the Canonical Variate Analysis (CVA) is an ideal tool for dynamic process\nmonitoring, however is not sufficient for nonlinear systems where most measurements\nfollow non-Gaussian distributions. To improve the performance of nonlinear dynamic\nprocess monitoring, a state space based ICA (SSICA) approach is proposed in this\nwork. Unlike the conventional ICA, the proposed algorithm employs the CVA as a\ndimension reduction tool to construct a state space, from where statistically inde-\n\u2217To whom all correspondence should be addressed, E-Mail: y.cao@cranfield.ac.uk\n1\nAC\nCE\nPT\nED\n M\nAN\nUS\nCR\nIP\nT\nACCEPTED MANUSCRIPT\npendent components are extracted for process monitoring. The proposed SSICA is\napplied to the Tennessee Eastman Process Plant as a case study. It shows that the\nnew SSICA provides better monitoring performance and detect some faults earlier\nthan other approaches, such as the DICA and the CVA.\nKeywords: Dynamic system, Nonlinearity, Principal component analysis, Canoni-\ncal variate analysis, Independent component analysis, Probability density function,\nKernel density estimation, Process monitoring.\n1 Introduction\nProcess monitoring techniques are employed to detect abnormal deviations of process\noperation conditions and diagnose the causes for these deviations to maintain high\nquality products and process safety. The Principal Component Analysis (PCA) is a\ndimension reduction technique that summarises the variation in a set of correlated\nvariables to a set of de-correlated principal components (PCs), each of which is a linear\ncombination of the original variables. The PCA exploits the correlation amongst a\nlarge number of measured variables and is popular for its simplicity. The PCA by\nreducing the dimension of the process variables is able to eliminate noise and retain\nonly important process information. MacGregor and Kourti [1] established a PCA\nmodel from the training data and judged the behaviour of online processes against the\nPCA model to detect deviations from the normal operating process. Wang et al. [2]\nalso presented a PCA approach based on visualization using parallel co-ordinates\nwith the Manresa Waste Water Treatment Plant as a case study.\nThe widely applied PCA is a static approach that assumes that the observations are\nin a steady-state, which is time independent. Furthermore, the PCA is normally\nassociated with the Hotelling\u2019s T 2 statistic, which, in order to determine the upper\ncontrol limit, assumes that the principal components derived by the PCA follow a\nGaussian distribution. However, the assumption of time-independence may not be\nvalid for processes subject to dynamic disturbances. The assumption of normality\nmay also be invalid for most chemical processes where strong nonlinearity makes\n2\nAC\nCE\nPT\nED\n M\nAN\nUS\nCR\nIP\nT\nACCEPTED MANUSCRIPT\nvariables driven by noise and disturbances non-Gaussian. In these situations, the\nPCA is not an appropriate tool for process monitoring.\nA solution to address the limitation of Gaussian assumption associated with the PCA\nis the Independent Component Analysis (ICA) [3\u20135]. The ICA technique recovers a\nfew statistically independent source signals from collected process measurements by\nassuming that these independent signals are non-Gaussian. A set of variables are\nsaid to be statistically independent from each other when the value of one vari-\nable cannot be predicted by giving the value of another variable. Unfortunately in\nprocess systems, such independent sources are in general not directly measurable,\nbut mixing each other in measurement variations. To identify the unmeasured ICs\nfrom measurements, the ICA algorithm involves a pre-processing stage known as the\nwhitening stage to eliminate the cross correlation between the process variables before\nextracting the independent components [3\u201310]. Although the ICA is sometimes con-\nsidered as an extension of the PCA [3], the objectives of the ICA are clearly different\nfrom those of the PCA. The ICA decomposes process measurements into statisti-\ncally (high-order statistics) ICs of a lower dimension whereas the PCA decomposes\nthe process measurements into a set of de-correlated (second order statistics) PCs\nof a lower dimension. Therefore, the ICA is able to extract more useful information\nthan the PCA [4, 5] and as a result performs better than the PCA based monitoring\ntechniques.\nProcess monitoring based on the ICA approach aims to extract the essential ICs, that\ndrive a process, for detecting underlying faults. Lee et al. [4] in their ICA approach\nemployed the Euclidean norm to determine the number of ICs to retain in the ICA\nmodel. The ICs in the model space were described as the dominant ICs while the\nignored ICs were described as the excluded ICs. Three monitoring metrics; the I2d for\nthe dominant ICs, the I2e for the excluded ICs and the Q-metric for the residual space\nwere determined and then kernel density estimations (KDE) employed to derive the\ncontrol limit for all three statistics.\nAlbazzaz and Wang [8] in another way, employed all the extracted ICs for process\nmonitoring because it was argued that no single IC was more important than another.\nIn addition, a box-cox transformation was applied to change the non-Gaussian coor-\n3\nAC\nCE\nPT\nED\n M\nAN\nUS\nCR\nIP\nT\nACCEPTED MANUSCRIPT\ndinates of the ICs to a Gaussian distribution in order to justify the use of the control\nlimits estimated based on the Gaussian assumption. Their technique was applied to\nthe Manresa Waste Water treatment plant in Spain to demonstrate its efficiency.\nConventionally, most published ICA studies have utilised the PCA for whitening and\ndimension reduction in the pre-processing stage, allowing the ICs to be interpreted by\nthe simple geometry of the PCA [7]. However, the connection of the ICA with PCA\nmakes such ICA approaches not appropriate for dynamic process monitoring due to\nthe static nature of the PCA. For most industrial processes, variables driven by noise\nand disturbances are strongly auto-correlated and time-varying making the PCA and\nthe ICA inappropriate for monitoring of such dynamic processes. This means that a\ndynamic process would require a monitoring technique that will take the serial corre-\nlations of the process data into account in order to achieve efficient dynamic process\nmonitoring. For this reason, Lee et al. [6] extended the ICA methods and proposed\nthe dynamic ICA (DICA) approach to improve the monitoring performance. In the\nso called DICA approach, a dynamic extension of the PCA (DPCA) is applied to\nan augmented data set in the pre-processing stage, where each observation vector is\naugmented with the previous observations and stacked together to account for the\nauto-correlations, then the ICs are extracted from the decomposed principal compo-\nnents (PCs). However, the DICA, like the DPCA, is not the best approach to capture\nthe dynamic behaviour from process measurements [11]. As a result, the statistical\nadvantage of the ICA is not fully exploited by the DICA and the performance of the\nDICA in dynamic process monitoring is still not satisfactory.\nNevertheless, the state-space models like the Canonical Variate Analysis (CVA) on\nthe other hand are reported to be efficient tools for dynamic process monitoring [11\u2013\n17]. The CVA is a dimension reduction technique that is based on state variables\nand is well suited for auto-correlated and cross-correlated process measurements. This\nmakes the CVA based approaches a better choice than the PCA based approaches\nfor dynamic process monitoring. However, on the other hand, the states obtained\nfrom the standard CVA, like the PCs are only de-correlated, but not statistically\nindependent, hence are not efficient enough for nonlinear process monitoring.\nTo derive an efficient tool for nonlinear dynamic process monitoring, in this paper,\n4\nAC\nCE\nPT\nED\n M\nAN\nUS\nCR\nIP\nT\nACCEPTED MANUSCRIPT\nthe CVA, rather than the PCA, is proposed as the pre-processing tool to associate\nwith the ICA resulting in a novel State Space ICA (SSICA) approach. In the ap-\nproach, the CVA is adopted as a dimension reduction tool to construct a state space\nand perform the dynamic whitening in the pre-processing stage. Then, the ICA is\napplied to the constructed state space in order to identify the statistically indepen-\ndent components. The SSICA approach is developed for nonlinear dynamic process\nmonitoring and applied to the Tennessee Eastman Process Plant as a case study.\nIt demonstrates that generally, the proposed SSICA is able to improve the process\nmonitoring performance over the existing DICA technique, which is reported to be an\nimprovement of the traditional ICA [6]. Also, the overall performance of the SSICA\nis better than the CVA, which was reported to be an efficient dynamic monitoring\ntool [11\u201317]. The performance improvements of the SSICA over the DICA and the\nCVA include increases in detection reliability and decreases in both detection delay\nand false alarms.\nThis paper is organised as follows: Section 2 describes the SSICA technique in details,\nwhich include the CVA, SSICA and KDE algorithms. The SSICA is then applied to\nthe Tennessee Eastman Process Plant in section 3. Finally, this work is concluded in\nsection 4.\n2 State Space Independent Component Analysis\nThe development of process monitoring techniques is geared towards applying these\ntechniques to industrial processes in order to improve process performance monitor-\ning. It is well known that most real time processes to which the monitoring techniques\nare applied are dynamic and nonlinear. The SSICA approach is developed to deal\nwith such processes. Firstly, from a general nonlinear dynamic system, a linearized\nstate space model can be constructed from normal operation data through the CVA.\nThe non-Gaussian collective modelling errors are then extracted as statistically in-\ndependent components through the SSICA. The obtained state space independent\ncomponents together with the residuals are used for process monitoring by compar-\ning the upper control limits estimated through the KDE approach. These algorithms\n5\nAC\nCE\nPT\nED\n M\nAN\nUS\nCR\nIP\nT\nACCEPTED MANUSCRIPT\nare described in details as follows.\n2.1 Canonical Variate Analysis\nConsider a nonlinear dynamic plant represented as:\nxk+1 = f(xk) + wk\nyk = g(xk) + vk (1)\nwhere xk \u2208 Rn and yk \u2208 Rm are state and measurement vectors respectively, f(\u00b7)\nand g(\u00b7) are unknown nonlinear functions, whereas wk and vk are plant disturbances\nand measurement noise vectors respectively. Clearly, it is difficult to monitor such\nunknown nonlinear dynamic systems directly. Fortunately, under normal operation\nconditions, the plant in Equation (1) can be approximated by a linear stochastic state\nspace model as:\nxk+1 = Axk + \u03b5k\nyk = Cxk + \u03b7k (2)\nwhere A and C are unknown state and output matrices respectively, whereas \u03b5k and\n\u03b7k are collective modelling errors partially due to the underlying nonlinearity of the\nplant, which has not been included in the linear model, and partially associated with\nprocess disturbance and measurement noise, wk and vk respectively. Note, as a result\nof the nonlinearity of the physical plant represented in Equation (1), the collective\nmodelling errors, \u03b5k and \u03b7k in Equation (2) generally will be non-Gaussian although\nwk and vk might be normally distributed.\nTo monitor the linear dynamic process represented in (2) without knowing matrices\nA and C, the CVA is employed to extract the state variables xk from process mea-\nsurements, yk. The CVA is based on the so called subspace identification, where the\nprocess measurements are stacked to form the past and future spaces through the\npast, yp,k and future, yf,k observations defined as follows.\n6\nAC\nCE\nPT\nED\n M\nAN\nUS\nCR\nIP\nT\nACCEPTED MANUSCRIPT\nyp,k =\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\nyk\u22121\nyk\u22122\n...\nyk\u2212q\n\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb \u2208 R\nmq, y\u02dcp,k = yp,k \u2212 y\u00afp,k (3)\nyf,k =\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\nyk\nyk+1\n...\nyk+q\u22121\n\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb \u2208 R\nmq, y\u02dcf,k = yf,k \u2212 y\u00aff,k (4)\nwhere the first subscripts of yp,k and yf,k indicate the past (p) and the future (f) ob-\nservations respectively, whilst the second subscripts stand for the reference sampling\npoint, where the past and future observations are defined. The sample means of the\npast and future observations are represented as y\u00afp,k and y\u00aff,k respectively, whilst y\u02dcp,k\nand y\u02dcf,k are the pre-processed past and future observations with zero means.\nThe past and future truncated Hankel matrices Yp and Yf are then defined in Equa-\ntion (5) and Equation (6) respectively.\nYp =\n[\ny\u02dcp,(q+1) y\u02dcp,(q+2) \u00b7 \u00b7 \u00b7 y\u02dcp,(q+M)\n]\n\u2208 Rmq\u00d7M (5)\nYf =\n[\ny\u02dcf,(q+1) y\u02dcf,(q+2) \u00b7 \u00b7 \u00b7 y\u02dcf,(q+M)\n]\n\u2208 Rmq\u00d7M (6)\nFrom the Hankel matrices defined above, the covariance of the past, future and cross-\ncovariance matrices are estimated as follows:\n\u03a3pp = E(y\u02dcpky\u02dc\nT\npk) = YpY\nT\np (M \u2212 1)\u22121 (7)\n\u03a3ff = E(y\u02dcfky\u02dc\nT\nfk) = YfY\nT\nf (M \u2212 1)\u22121 (8)\n\u03a3fp = E(y\u02dcfky\u02dc\nT\npk) = YfY\nT\np (M \u2212 1)\u22121 (9)\nThe goal of the CVA is to find the best linear combinations, aT y\u02dcfk and b\nT y\u02dcpk of the\nfuture and past observations so that the correlation between these combinations is\nmaximised. The correlation can be represented as:\n\u03c1fp(a,b) =\naT\u03a3fpb\n(aT\u03a3ffa)1\/2(bT\u03a3ppb)1\/2\n(10)\n7\nAC\nCE\nPT\nED\n M\nAN\nUS\nCR\nIP\nT\nACCEPTED MANUSCRIPT\nLet u = \u03a3\n1\/2\nff a and v = \u03a3\n1\/2\npp b. The optimization problem can be casted as:\nmaxu,v u\nT (\u03a3\n\u22121\/2\nff \u03a3fp\u03a3\n\u22121\/2\npp )v\ns.t. uTu = 1 (11)\nvTv = 1\nThe solution of this problem can be obtained through the singular value decomposi-\ntion (SVD) on the scaled Hankel matrix, H as indicated in Equation (12).\nH = \u03a3\n\u22121\/2\nff \u03a3fp\u03a3\n\u22121\/2\npp = U\u03a3V\nT (12)\nwhere U =\n[\nu1 u2 \u00b7 \u00b7 \u00b7 umq\n]\n\u2208 Rmq\u00d7mq, V =\n[\nv1 v2 \u00b7 \u00b7 \u00b7 vmq\n]\n\u2208 Rmq\u00d7mq\n\u03a3 =\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u03c31 0 \u00b7 \u00b7 \u00b7 0\n0 \u03c32 \u00b7 \u00b7 \u00b7 0\n...\n. . .\n...\n0 0 \u00b7 \u00b7 \u00b7 \u03c3mq\n\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb \u2208 R\nmq\u00d7mq\nFrom Equation (12) above, the canonical variate, zk \u2208 Rmq based on the past mea-\nsurements can be derived as in Equation (13).\nzk =\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\nb1\nT\nb2\nT\n...\nbTmq\n\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb y\u02dcp,k =\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\nv1\nT\nv2\nT\n...\nvTmq\n\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\u03a3\n\u22121\/2\npp y\u02dcp,k = V\nT\u03a3\u22121\/2pp y\u02dcp,k = Jy\u02dcp,k (13)\nwhere J = VT\u03a3pp\n\u22121\/2 \u2208 Rmq\u00d7mq is the transformation matrix, which transforms the\nmq-dimensional past measurements to the mq-dimensional canonical variate space.\nThe canonical variate estimated in (13) can be separated into the state and residual\nspaces based on the order of the system, n. According to the magnitude of the singular\nvalues, the first n dominant singular values are determined and the corresponding n\nelements of the canonical variate retained as the state variables where n < mq.\nIn addition, the remaining (mq \u2212 n) elements of the canonical variate are said to\nbe in the residual space. Equation (14) below shows the entire canonical variate\n8\nAC\nCE\nPT\nED\n M\nAN\nUS\nCR\nIP\nT\nACCEPTED MANUSCRIPT\nspace (zk \u2208 Rmq) is spanned by the state variables (xk \u2208 Rn) and the residuals\n(dk \u2208 Rmq\u2212n), both of which are subsets of the canonical variate, zk.\nzk =\n[\nxk\nT dk\nT\n]T\n(14)\nThe previous work [17] showed that the state variables, xk obtained through CVA\nprovides a tool better than directly using the past or future observations to monitor\nthe dynamic systems in (1). However, as shown in (2), the states are combinations\nof statistically independent non-Gaussian sources. To make process monitoring more\nefficient, identifying these sources from the states is desired. The corresponding\nalgorithm is to be developed in the next section.\n2.2 State Space Independent Component Analysis\nAccording to (2), xk can be expressed as a linear combination of the initial state, x0\nand the collective modelling errors, \u03b5j, for j = 0, 1, . . . , k \u2212 1.\nxk = A\nkx0 +\nk\u22121\u2211\nj=0\nAj\u03b5k\u22121\u2212j (15)\nEquation (15) indicates that if x0 and \u03b5j, j = 0, . . . , k \u2212 1 are mixtures of m(\u2264 n)\nunknown independent components, sj \u2208 Rm, for j = 0, \u00b7 \u00b7 \u00b7 , k \u2212 1, then the states,\nxk, for k = 1, . . .M are also linear combinations of these unknown independent\ncomponents. More specifically, the relationship can be expressed as follows.\nX = BxSx (16)\nwhere X =\n[\nx1 \u00b7 \u00b7 \u00b7 xM\n]\n\u2208 Rn\u00d7M is the state matrix, Bx =\n[\nb1 \u00b7 \u00b7 \u00b7 bm\n]\n\u2208 Rn\u00d7m\nis an unknown mixing matrix, and Sx =\n[\nsx,0 . . . sx,M\u22121\n]\n\u2208 Rm\u00d7M is unknown\nindependent component matrix. The SSICA aims to estimate both mixing matrix, Bx\nand independent component matrix, Sx, from the state matrix, X obtained through\nthe CVA as described above.\nThe problem can be solved through an existing ICA algorithm, such as the Fas-\ntICA [18] to find a de-mixing matrix, W such that the rows of the estimated inde-\npendent component matrix,\nS\u02c6x = WxX (17)\n9\nAC\nCE\nPT\nED\n M\nAN\nUS\nCR\nIP\nT\nACCEPTED MANUSCRIPT\nare as independent of each other as possible. Based on the \u201cnon-Gaussian repre-\nsents independence\u201d principle [18], the de-mixing matrix as well as the independent\ncomponent matrix are obtained through iterative optimizations to maximize certain\nnon-Gaussian criteria.\nThe ICA can be applied to the residual space spanned by dk. The independent\ncomponent matrix in the residual space is obtained by applying the ICA algorithm\nto the residual matrix, D as follows.\nS\u02c6d = WdD (18)\nwhere D =\n[\nd1 \u00b7 \u00b7 \u00b7 dM\n]\n\u2208 R(mq\u2212n)\u00d7M .\nThe ICA based process monitoring is frequently associated with the Mahalanobis\ndistance I2, also known as the D-statistic [4, 6, 10]. The I2 metric is the sum of the\nsquared independent components extracted from the ICA algorithm.\nI2x,k = s\u02c6\nT\nx,ks\u02c6x,k (19)\nI2d,k = s\u02c6\nT\nd,ks\u02c6d,k (20)\nwhere s\u02c6x,k and s\u02c6d,k are the k-th columns of S\u02c6 and S\u02c6d, respectively. The M I\n2\nx,k and I\n2\nd,k\nvalues for k = 1, . . . ,M are then used to derived the upper control limits, I2x,UCL(\u03b1)\nand I2d,UCL(\u03b1) using the KDE algorithm described in the next section.\nFor online monitoring, the ICs of the state and residual spaces is calculated from the\nnew measurements, y\u02dcnewp,k using the transformation matrix, J =\n[\nJTx J\nT\nd\n]T\nand the\nde-mixing matrices, Wx and Wd respectively.\ns\u02c6newx,k = WxJxy\u02dc\nnew\np,k (21)\ns\u02c6newd,k = WdJdy\u02dc\nnew\np,k (22)\nThe corresponding I2 metrics for the new measurements are then obtained as follows.\nI2,newx,k = (s\u02c6\nnew\nx,k )\nT s\u02c6newx,k (23)\nI2,newd,k = (s\u02c6\nnew\nd,k )\nT s\u02c6newd,k (24)\nA fault condition is then detected if either I2 metric is larger than the corresponding\nUCL.\n10\nAC\nCE\nPT\nED\n M\nAN\nUS\nCR\nIP\nT\nACCEPTED MANUSCRIPT\n2.3 Control Limit Through Kernel Density Estimations\nThe ICs are not Gaussian. Therefore, the UCL for the I2 metric cannot be derived\nanalytically. The kernel density estimation (KDE) is a well established approach to\nestimate the PDF of random processes [19\u201321]. Hence, it is a natural selection using\nthe KDE to determine the UCL [17]. Considering both I2 metrics are positive, a\nKDE algorithm with lower bound support is adopted in this work to estimate the\nUCL.\nLet y > 0 be the random variable under consideration. Firstly, the bounded y is\nconverted into unbounded x by defining x = ln(y). Then, the density function p(x)\ncan be estimated by the normal KDE algorithm. Finally, the density function of y is\np(ln(y))\/y as derived in (25).\nP (y < b) = P (x < ln(b)) =\n\u222b ln(b)\n\u2212\u221e\np(x)dx =\n\u222b b\n0\np(ln(y))\n1\ny\ndy (25)\nTherefore, by knowing p(x), an appropriate control limit can be determined for a\nspecific confidence bound, \u03b1 using Equation (25). The estimation of the probability\ndensity function p\u02c6(x) at point x through the kernel function, K(\u00b7) is defined as follows\np\u02c6(x) =\n1\nMh\nM\u2211\nk=1\nK\n(\nx\u2212 xk\nh\n)\n. (26)\nwhere xk, k = 1, 2, \u00b7 \u00b7 \u00b7 ,M are samples of x and h is the bandwidth. The bandwidth\nselection in KDE is an important issue because selecting a bandwidth too small will\nresult in the density estimator being too rough, a phenomenon known as under-\nsmoothed while selecting a bandwidth too big will result in the density estimator\nbeing too flat. There is no single perfect way to determine the bandwidth. However,\na rough estimation of the optimal bandwidth hopt subject to minimising the approx-\nimation of the mean integrated square error can be derived in Equation (27), where\n\u03c3 is the standard deviation [22].\nhopt = 1.06\u03c3N\n\u22121\/5 (27)\nTo use both I2x and I\n2\nd metrics together, the joint distribution of these two metrics\n11\nAC\nCE\nPT\nED\n M\nAN\nUS\nCR\nIP\nT\nACCEPTED MANUSCRIPT\nhas to be considered. In general, the joint probability of two random variables, x and\ny is defined as follows.\nP (x < a, y < b) =\n\u222b a\n\u2212\u221e\n\u222b b\n\u2212\u221e\np(x, y)dxdy (28)\nHowever, for the SSICA and the DICA, I2x and I\n2\ny are independent. Hence,\nP (x < a, y < b) = P (x < a)P (y < b) (29)\nEquation (29) can also be approximately applied to T 2 and Q metrics for the CVA\nbecause x and d in (14) are uncorrelated [23]. This means the joint PDF estimation\ncan be simplified by two univariate PDF estimations.\nBy replacing xk in Equation (26) with I\n2\nx,k and I\n2\nd,k obtained in (19) and (20) respec-\ntively, the above KDE approach is able to estimate the underlying PDFs of the I2x\nand I2d metrics. The corresponding control limits, I\n2\nx,UCL(\u03b1) and I\n2\nd,UCL(\u03b1) can then\nbe obtained from the PDFs of the I2x and I\n2\nd metrics for a given confidence level, \u03b1\nby solving the following equations respectively.\n\u222b I2x,UCL(\u03b1)\n0\np(ln(I2x))\nI2x\ndI2x\n\u222b I2d,UCL(\u03b1)\n0\np(ln(I2d))\nI2d\ndI2d = \u03b1 (30)\u222b I2x,UCL(\u03b1)\n0\np(ln(I2x))\nI2x\ndI2x =\n\u221a\n\u03b1 (31)\u222b I2d,UCL(\u03b1)\n0\np(ln(I2d))\nI2d\ndI2d =\n\u221a\n\u03b1 (32)\nIn this work, a fault is then identified (Fk = 1) if either I\n2,new\nx,k > I\n2\nxUCL(\u03b1) or I\n2,new\nx,d >\nI2dUCL(\u03b1) conditions are satisfied, i.e.\nFk = (I\n2,new\nx,k > I\n2\nx,UCL(\u03b1))\u2295 (I2,newd,k > I2d,UCL(\u03b1)) (33)\nwhere \u2295 represents a logical \u201cOR\u201d operation.\n12\nAC\nCE\nPT\nED\n M\nAN\nUS\nCR\nIP\nT\nACCEPTED MANUSCRIPT\n3 APPLICATION - Tennessee Eastman Process\nPlant\nThe Tennessee Eastman Process (TEP) plant has 5 main units which are the reactor,\ncondenser, separator, stripper and compressor [13, 24, 25]. A graphical description\nof the TEP plant is presented in Figure 1.\nXC\nXF\nXE\nXF\nXH\nXE\nXD\nXG\n11C\n4\n5\n6\n8\n9\n12\nCondenser\nFI\nTI\nPI\nLI\nJI\nTI\nFI\nA\nN\nA\nL\nY\nZ\nE\nR\nXA\nXB\nXC\nXD\nXE\nXF\nCompressor\nStripper\nVap\/Liq\nseparator\n7\nA\nPCFI\n1\nCWS\n13\nCWR\nCWS\nPI\nTI\nFI\nSC\nTI\nCWR\nCond\nFI\nStm\nTI\n10\nFI\nLI\nPurge\nA\nN\nA\nL\nY\nZ\nE\nR\nLI\nReactor\nPI\nFI\nFI\nProduct\nXBA\nN\nA\nL\nY\nZ\nE\nR\nXA\nXD\nXH\nXG\nPCFI\n2\nD\n3\nE\nPCFI\nFigure 1 Graphical Description of the TEP Plant\nThe TEP process is a large dimensional, nonlinear process with unknown mathemat-\nical representation as the simulation is intentionally distributed as an undocumented\nFORTRAN program [24, 25]. The TEP data consists of two blocks; the training and\ntest data sets each of which has 22 continuous process measurements, 12 manipulated\nvariables and 19 composition measurements sampled with time delays. There are also\n21 scenarios corresponding to Faults 0 \u2212 20, with Fault 0 being the data simulated\nat normal operating condition (no fault) and Faults 1 - 20 corresponding to data sets\n13\nAC\nCE\nPT\nED\n M\nAN\nUS\nCR\nIP\nT\nACCEPTED MANUSCRIPT\nfrom the simulated fault processes, each with a specified fault as listed in Table 1.\nTable 1 Brief Description of TEP Plant Faults\nFault Description Type\n1 A\/C Feed Ratio, B Composition Constant (Stream 4) Step\n2 An increase in B while A\/C Feed ratio is constant (stream 4) Step\n3 D Feed Temperature (Stream 2) Step\n4 Reactor Cooling Water Inlet Temperature Step\n5 Condenser Cooling Water Inlet Temperature Step\n6 A loss in Feed A (stream 1) Step\n7 C Header Pressure Loss - Reader Availability (Stream 4) Step\n8 A,B,C Feed Composition (Stream 4) Random variation\n9 D Feed Temperature (Stream 2) Random variation\n10 C Feed Temperature (Stream 4) Random variation\n11 Reactor Cooling Water Inlet Temperature Random variation\n12 Condenser Cooling Water Inlet Temperature Random variation\n13 Reaction Kinetics Slow drift\n14 Reaction Cooling Water Valve Sticking\n15 Condenser Cooling Water Valve Sticking\n16 Unknown Unknown\n17 Unknown Unknown\n18 Unknown Unknown\n19 Unknown Unknown\n20 Unknown Unknown\nA total of 52 measurements are collected for each data set of length, N = 960\nrepresenting 48-hour operation with a sampling rate of 3 minutes. Among these mea-\nsurements, 19 analyzer measurements, 14 of which are sampled at 6 minute interval\nwhilst other 5 are sampled in every 15 minutes, have not been included in this study\ndue to the measurement time delay, while 11 manipulated variables are treated the\nsame as other measured variables because under feedback control, these variables are\nnot independent any more. The simulation time of each operation run in the test\ndata block is 48 hours and the various faults are introduced only after 8 hours. This\nmeans that for each of the faults, the process is in normal operation condition for the\n14\nAC\nCE\nPT\nED\n M\nAN\nUS\nCR\nIP\nT\nACCEPTED MANUSCRIPT\nfirst 8 simulation hours before the process becomes abnormal after the introduction\nof the fault. Furthermore, all twenty TEP faults have also been studied to investi-\ngate the effectiveness of the proposed SSICA technique. The results are based on a\n\u03b1 = 99% confidence level (\n\u221a\n\u03b1 = 0.995 for individual metrics).\nThe TEP plant is under closed-loop control, which by its nature, tries to overcome\nthe abnormal deviations that occur as a result of the introduction of the various faults\nto the simulated TEP process. Due to the closed loop nature of the plant, deviations\ncaused by some faults are relatively small such that these faults are difficult to be\ndetected by most monitoring approaches, such as principal component analysis and\npartial least squares [17]. Different from these conventional methods, the SSICA\nproposed in this work aims to address both dynamic and nonlinear issues effectively.\nThe monitoring performance in this study is assessed by the percentage reliability,\nwhich is defined as the percentage of the samples outside the control limits [26].\nHence, a monitoring technique is said to have a better performance over another if\nthe percentage reliability of this technique is numerically higher than the percentage\nreliability of another technique. Another criterion employed in this study to judge\nthe performance of the monitoring techniques is the detection delay, which is how\nlong it takes for a technique to identify a fault after the introduction of the fault.\nA monitoring technique is said to be better than another if it is able to detect a\nfault earlier than another technique. The false alarm rate, which is the percentage of\nsamplings classified as abnormal during the 8 hour normal operation period before\nintroducing a fault, is also considered for performance comparison.\nTo demonstrate the efficiency of the proposed SSICA, the monitoring performance\nof the proposed SSICA is compared with the monitoring performance of the DICA\ntechnique, an existing dynamic extension of the ICA. The SSICA is also compared\nwith the CVA to demonstrate the improvement by performing ICA on the state space\nobtained by the CVA. For the pre-processing CVA described above, the number of\nstate variables to retain in the dominant space is normally determined by the dom-\ninant singular values from the scaled Hankel matrix H in Equation (32). However,\napplying the CVA to the TEP case study showed that using the dominant singular\nvalues left an unrealistically large number of state variables in the dominant space.\n15\nAC\nCE\nPT\nED\n M\nAN\nUS\nCR\nIP\nT\nACCEPTED MANUSCRIPT\nHence a more realistic number of state variables 28 were retained in the dominant\nspace so that 28 state variables were retained from the pre-processing stage of CVA.\nTo make a fair comparison with the proposed SSICA, in the DICA, equal number\nof latent variables were retained in the dominant spaces while the rest of the latent\nvariables spanned the excluded spaces. The percentage reliability and the detection\ndelay of all twenty TEP faults for the proposed SSICA technique is compared with\nthat of the CVA and DICA techniques and presented in Table 2. The corresponding\nfalse alarm rate of all faults is 0.6849% for the DICA and 0% for both the CVA and\nthe SSICA.\nTable 2 Performance Comparison\nFault Reliability (%) Detection Delay (minute)\nSSICA CVA DICA SSICA CVA DICA\n1 99.75 99.75 99.75 9 9 9\n2 99.63 99.63 99.50 12 12 15\n3 73.03 70.04 19.48 15 21 21\n4 99.88 99.88 99.88 6 6 6\n5 99.88 99.88 99.88 6 6 6\n6 99.88 99.88 99.88 6 6 6\n7 99.88 99.88 99.88 6 6 6\n8 99.00 98.88 98.75 18 30 33\n9 91.64 90.01 46.82 18 39 48\n10 96.75 96.38 96.13 18 90 96\n11 99.38 99.38 99.38 18 18 18\n12 99.50 99.50 99.50 15 15 15\n13 96.25 96.13 96.13 18 96 96\n14 99.88 99.88 99.88 6 6 6\n15 99.63 99.63 99.50 12 12 15\n16 99.38 99.38 99.25 18 18 21\n17 98.38 98.25 98.13 18 45 48\n18 99.25 99.25 99.25 21 21 21\n19 99.88 99.88 99.88 6 6 6\n20 97.63 97.50 97.13 18 63 72\n16\nAC\nCE\nPT\nED\n M\nAN\nUS\nCR\nIP\nT\nACCEPTED MANUSCRIPT\nThe superiority of the SSICA over the CVA and DICA techniques is demonstrated\nin Table 2. For 10 of the 20 faults (2,3,8,9,10,13,15,16,17 and 20), the SSICA is able\nto improve the monitoring performance over the existing DICA technique in both\nreliability and detection delay, while for the rest 10 faults, the SSICA maintains the\nsame performance as the DICA does. This is achieved by the SSICA with a reduced\nfalse alarm rate for all faults. In the reliability, the improvement of the SSICA over\nthe DICA is significant (> 0.5%) for 4 of the faults (3,9,10 and 20). Particularly,\nfor faults 3 and 9, the improvement is extremely significant, over 40%. Meanwhile,\nthe SSICA is able to reduce the detection delay significantly (> 10 minutes) for 6\nfaults (8, 9, 10, 13, 17 and 20). The over one hour reduction in detection delay\nis achieved by using the SSICA on faults 10 and 13. In comparison between the\nSSICA and CVA, the performance of the SSICA is better than that of the CVA for\n7 faults (3, 8, 9 10, 13, 17 and 20) in both the reliability and the detection delay,\nwhilst these performance criteria of the remaining 13 faults are the same for both\nmethods. In the reliability, the improvement on 2 faults (3 and 9) are significant (over\n1%). Meanwhile, significant improvements in the detection delay (> 10 minutes) are\nobserved for 6 faults (8, 9, 10, 13, 17 and 20), for two of which (10 and 13), the\nimprovements are over one hour.\nTo appreciate the capability of the SSICA, fault detection by these three methods\nalong with fault propagation is further analysed for Faults 3 and 9. As shown in\nTable 1, both Faults 3 and 9 relate to the temperature of D feed (stream 2), one\nfor step change (Fault 3) and another for random variations (Fault 9). These faults\ndirectly result in small deviations in the reactor cooling water outlet temperature,\nwhich can be easily corrected by the closed-loop control system by manipulating the\ncooling water flow. Therefore, both faults are generally difficult to be detected by\nmost monitoring approaches.\nFigure 2 shows a comparison of the fault detection along with the propagation of\nFault 3 for the SSICA (a), CVA (b) and DICA (c) techniques, using Fk derived from\nEquation (33), whilst Figure 3 shows the fault detection along with the propagated\nFault 9 process for these three techniques.\n17\nAC\nCE\nPT\nED\n M\nAN\nUS\nCR\nIP\nT\nACCEPTED MANUSCRIPT\n8 16 24 32 40 48\n0\n0.5\n1\n(a) SSICA 73.0337 % Relability, 15 minute detection delay\nF k\n8 16 24 32 40 48\n0\n0.5\n1\n(b) CVA 70.0375 % Relability, 21 minute detection delay\nF k\n8 16 24 32 40 48\n0\n0.5\n1\n(c) DICA 19.4757 % Relability, 21 minute detection delay\nF k\ntime, hour\nFigure 2 Comparison of fault detection along with the propagation of Fault\n3\n18\nAC\nCE\nPT\nED\n M\nAN\nUS\nCR\nIP\nT\nACCEPTED MANUSCRIPT\n8 16 24 32 40 48\n0\n0.5\n1\n(a) SSICA 91.6355 % Relability, 18 minute detection delay\nF k\n8 16 24 32 40 48\n0\n0.5\n1\n(b) CVA 90.0125 % Relability, 39 minute detection delay\nF k\n8 16 24 32 40 48\n0\n0.5\n1\n(c) DICA 46.8165 % Relability, 48 minute detection delay\nF k\ntime, hour\nFigure 3 Comparison of fault detection along with the propagation of Fault\n9\nIt is for such faults as Faults 3 and 9 that the superiority of the proposed SSICA\ntechnique over the CVA and particularly the DICA techniques is most outstanding\nas illustrated in Figure 2 and Figure 3. The performance of the SSICA is better\nthan that of the CVA and DICA techniques for both Faults 3 and 9. Particularly,\nit is clear that for both faults the SSICA is able to show a significant improvement\nof fault detection over the DICA technique within a few hours of the early stage of\nfault propagation. The improvement in the early fault propagation stage is important\nsince it will give more time for operators to deal with the detected fault.\nAlthough the DICA, also referred to as the ICA with delays is reported to be a more\nefficient dynamic monitoring tool than the traditional ICA [6], the proposed SSICA\ntechnique is able to significantly improve the monitoring performance over the DICA\ntechnique for most of the faults considered in this work. This is because the pre-\nprocessing stage of the SSICA is based on the CVA, which is a more appropriate\n19\nAC\nCE\nPT\nED\n M\nAN\nUS\nCR\nIP\nT\nACCEPTED MANUSCRIPT\ndynamic monitoring tool than the DPCA on which the DICA technique is firstly based\non. Furthermore, the efficiency of the SSICA over the CVA is owed to the fact that the\nSSICA is more suited than the CVA to deal with non-Gaussian process measurement,\nseparating the original sources to a greater degree than the CVA technique. The\nresults illustrated above demonstrate that there were no faults for which either the\nCVA or DICA techniques outperformed the proposed SSICA technique.\nIt is worth to note that the CVA approach adopted in this work is able to cope with\ncertain level of nonlinearities due to the use of the KDE to determine the UCL [17].\nMoreover, the superiority of the CVA over the DICA indicates that the dynamic issue\nhas more impact on the fault detection performance than the nonlinearity for the TE\nprocess. This might be due to the feedback control, which widely propagates the\ntransient response caused by a fault, as well as restricts the variations caused by a\nfault to relatively small level. This restriction on variation causes some faults to be\ndifficult to detect without taking into account the correlations in time. Meanwhile,\nthe effect of nonlinearity on fault responses is also restricted so that the CVA with\nKDE approach is able to detect most faults adequately. This may also be the reason\nfor most faults the performance of the SSICA and the CVA is very close.\n4 Conclusion\nIn this study, an ICA model was developed based first on CVA in the pre-processing\nstage before applying the ICA algorithm and then control limits derived based on\nkernel density estimations with 99% joint confidence intervals. The proposed ap-\nproach is applied to the Tennessee Eastman Process. The monitoring performance\nof the proposed SSICA is assessed and compared with those of the CVA and DICA\ntechniques also considered in this study. The percentage reliability, detection delays\nas well as the false alarm rates were adopted to assess and compare the monitoring\nperformance of the proposed approach with those of the CVA and DICA techniques.\nThe percentage reliability of the SSICA was significantly higher than both of the\nDICA and the CVA for some of the faults although the significance of improvement\nover the CVA was not as high as that over the DICA. Moreover, the SSICA is also\n20\nAC\nCE\nPT\nED\n M\nAN\nUS\nCR\nIP\nT\nACCEPTED MANUSCRIPT\nable to dramatically reduce the detection delay over both the CVA and the DICA\nfor certain faults. In particular, the remarkable superiority of the SSICA is demon-\nstrated in the faults that are more difficult to detect, emphasizing the efficiency of\nthe proposed SSICA over the existing CVA and DICA techniques.\nReferences\n[1] MacGregor, J. F., and Kourti, T., 1995. \u201cStatistical process control of multi-\nvariate processes\u201d. Control Engineering Practice, 3(3), pp. 403\u2013414.\n[2] Wang, X. Z., Medasani, S., Marhoon, F., and Albazzaz, H., 2004. \u201cMultidi-\nmensional visualization of principal component scores for process historical data\nanalysis\u201d. Industrial Engineering Chemical Research, 43, pp. 7036\u20137048.\n[3] Abazzaz, H., and Wang, X. Z., 2004. \u201cStatistical process control charts for batch\noperations based on independent component analysis\u201d. Industrial & Engineering\nChemistry Research, 43(21), pp. 6731\u20136741.\n[4] Lee, J., Yoo, C., and Lee, I., 2004. \u201cStatistical process monitoring with inde-\npendent component analysis\u201d. Journal of Process Control, 14, pp. 467\u2013485.\n[5] Chen, G., Liang, J., and Qian, J., 2004. \u201cChemical process monitoring and fault\ndiagnosis based on independent component analysis\u201d. In 5th World Congress on\nIntelligent Control and Automation, p. 1646.\n[6] Lee, J., Yoo, C., and Lee, I., 2004. \u201cStatistical monitoring of dynamic indepen-\ndent component analysis\u201d. Chemical Engineering Sciences, 59, pp. 2995\u20133006.\n[7] Lee, J., Qin, S. J., and Lee, I., 2006. \u201cFault detection and diagnosis based on\nmodified independent component analysis.\u201d. AIChE Journal, 52(10), pp. 3501\u2013\n3514.\n[8] Albazzaz, H., and Wang, X. Z., 2006. \u201cHistorical data analysis based on plots\nof independent and parallel coordinates and statistical control limits.\u201d. Journal\nof Process Control, 16, pp. 103\u2013114.\n21\nAC\nCE\nPT\nED\n M\nAN\nUS\nCR\nIP\nT\nACCEPTED MANUSCRIPT\n[9] Liu, X., Xie, L., Kruger, U., Littler, T., and Wang, S., 2008. \u201cStatistical-based\nmonitoring of multivariate non-gaussian systems analysis.\u201d. AIChE Journal,\n54(9), pp. 2379\u20132391.\n[10] Hongguang, L., and Hui, G., 2006. \u201cThe application of independent component\nanalysis in process monitoring.\u201d. In Proceedings of the 1st International Con-\nference on Innovative Computing, Information and Control (ICICIC06), p. 97.\n[11] Negiz, A., and Cinar, A., 1998. \u201cMonitoring of multivariable dynamic processes\nand sensor auditing\u201d. Journal of Process Control, 8(56), pp. 357\u2013380.\n[12] Juan, L., and Fei, L., 2006. \u201cStatistical modelling of dynamic multivariate\nprocess using canonical variate analysis\u201d. In Proceedings IEEE International\non Information and Automation, 2006. ICIA 2006, p. 218.\n[13] Chiang, L. H., Russell, E. L., and Braatz, R. D., 2001. Fault Detection and\nDiagnosis in Industrial Systems. Springer, London.\n[14] Simouglou, A., Martin, E. B., and Morris, A. J., 2002. \u201cStatistical performance\nmonitoring of dynamic multivariate processes using state space modelling\u201d. Com-\nputers and Chemical Engineering, 26, pp. 909\u2013920.\n[15] Larimore, W. E., 1983. \u201cSystem identification reduced order filtering and mod-\nelling via canonical correlation analysis\u201d. In Proceedings of the American Control\nConference, p. 445.\n[16] Negiz, A., and Cinar, A., 1997. \u201cPls, balanced and canonical variate realization\ntechniques for identifying varma models in state space\u201d. Chemometrics and\nIntelligent Laboratory Systems, 38, pp. 209\u2013221.\n[17] Odiowei, P., and Cao, Y., 2009. \u201cNonlinear dynamic process monitoring using\ncanonical variate analysis and kernel density estimations\u201d. IEEE Transactions\non Industrial Informatics, 6(1), pp. 36\u201345.\n[18] Hyva\u00a8rinen, A., 1999. \u201cFast and robust fixed-point algorithms for independent\ncomponent analysis\u201d. IEEE Transactions on Neural Networks, 3, pp. 626\u2013634.\n22\nAC\nCE\nPT\nED\n M\nAN\nUS\nCR\nIP\nT\nACCEPTED MANUSCRIPT\n[19] Chen, Q., Kruger, U., Meronk, M., and Leung, A. Y. T., 2004. \u201cSynthesis of\nt2 and q statistics for process monitoring\u201d. Control Engineering Practice, 12,\npp. 745\u2013755.\n[20] Bowman, A. W., and Azzalini, A., 1997. Applied Smoothing Techniques for\nData Analysis, The Kernel Approach with S-Plu Illustrations. Clarendon Press,\nOxford.\n[21] Martin, E. B., and Morris, A. J., 1996. \u201cNon-parametric confidence bounds\nfor process performance monitoring charts\u201d. Journal of Process Control, 6(6),\npp. 349\u2013358.\n[22] Xiaoping, S., and Sonali, A., 2006. \u201cKernel density estimation for an anomaly\nbased intrusion detection system\u201d. In Proceedings of the 2006 World Congress\nin Computer Science, Computer Engineering and Applied Computing, p. 161.\n[23] Chen, Q., Krunger, U., Meronk, M., and Leung, A., 2004. \u201cSynthesis of t2\nand q statistics for process monitoring\u201d. Control Engineering Practice, 12,\npp. 745\u2013755.\n[24] Ricker, N. L., 2001. Tennessee eastman challenge archive.\n[25] Downs, J. J., and Vogel, E., 1993. \u201cA plant-wide industrial process control\nproblem\u201d. Computers and Chemical Engineering, 17, pp. 245\u2013255.\n[26] Kano, M., Koji, N., Shinji, H., Ioro, H., Hiromo, O., Ramon, S., and Bhavik,\nR. B., 2002. \u201cComparison of multivariate statistical process monitoring methods\nwith applications to the eastman challenge problem\u201d. Computers and Chemical\nEngineering, 26, pp. 161\u2013174.\n23\n"}