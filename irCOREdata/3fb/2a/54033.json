{"doi":"10.1016\/j.imavis.2008.06.002","coreId":"54033","oai":"oai:eprints.lincoln.ac.uk:2755","identifiers":["oai:eprints.lincoln.ac.uk:2755","10.1016\/j.imavis.2008.06.002"],"title":"Multiple object tracking using a neural cost function","authors":["Humphreys, James","Hunter, Andrew"],"enrichments":{"references":[{"id":885995,"title":"A fast model-free morphology-based object tracking algorithm.","authors":[],"date":"2002","doi":"10.5244\/c.16.75","raw":"J. Owens, A. Hunter, and E. Fletcher. A fast model-free morphology-based object tracking algorithm. In British Machine Vision Conference, volume 2, pages 767{776, 2002.","cites":null},{"id":886220,"title":"A hierarchical approach to robust background subtraction using color and gradient information.","authors":[],"date":"2002","doi":"10.1109\/motion.2002.1182209","raw":"O. Javed, S. Khurram, and M. Shah. A hierarchical approach to robust background subtraction using color and gradient information. In IEEE Workshop on Motion and Video Computing, Orlando, Dec 5-6, 2002.","cites":null},{"id":884598,"title":"A neural system for automated cctv surveillance.","authors":[],"date":"1994","doi":"10.1049\/ic:20030040","raw":"A. Hunter, J. Owens, and M. Carpenter. A neural system for automated cctv surveillance. In IEE Symposium on Intelligent Distributed Surveillance Systems, ed. S. Velastin, 26 Feb. 2003, IEE Savoy Place, London, IEE London, ISSN 0963-3308, 2003. 18[4] D. Koller, J. Weber, and J. Malik. Robust multiple car tracking with occlusion reasoning. In The third European conference on Computer vision (vol. 1), Stockholm, Sweden, pages 189{196, 1994.","cites":null},{"id":884125,"title":"An adaptive eigenshape model.","authors":[],"date":"1995","doi":"10.5244\/c.9.9","raw":"A. Baumberg and D. Hogg. An adaptive eigenshape model. In Proc of the 6th British Machine Vision Conference, Vol 1, pp 87-96, 1995.","cites":null},{"id":885497,"title":"An improved adaptive background mixture model for real-time tracking with shadow detection.","authors":[],"date":"2001","doi":"10.1007\/978-1-4615-0913-4_11","raw":"P. KaewTraKulPong and R. Bowden. An improved adaptive background mixture model for real-time tracking with shadow detection. In 2nd European Workshop on Advanced Video Based Surveillance Systems, AVBS01. Sept, 2001.","cites":null},{"id":888029,"title":"An integrated tra\u00b1c and pedestrian model-based vision system.","authors":[],"date":"1997","doi":null,"raw":null,"cites":null},{"id":885041,"title":"Application of the self-organising map to trajectory classi\u00afcation.","authors":[],"date":"2000","doi":"10.1109\/vs.2000.856860","raw":"J. Owens and A. Hunter. Application of the self-organising map to trajectory classi\u00afcation. In Proc. 3rd IEEE International Workshop on Visual Surveillance, pages 77{83. Dublin, 2000.","cites":null},{"id":887019,"title":"Learning the distribution of object trajectories for event recognition.","authors":[],"date":"1996","doi":"10.1016\/0262-8856(96)01101-8","raw":"N. Johnson and D.C. Hogg. Learning the distribution of object trajectories for event recognition. Image and Vision Computing, 14:609{615, 1996.","cites":null},{"id":886765,"title":"Novelty detection in video surveillance using hierarchical neural networks.","authors":[],"date":"2002","doi":"10.1007\/3-540-46084-5_202","raw":"J. Owens, A. Hunter, and E. Fletcher. Novelty detection in video surveillance using hierarchical neural networks. In Proc. International Conference on Arti\u00afcial Neural Networks (ICANN 2002), volume 2, pages 1249{1254. Madrid, 2002.","cites":null},{"id":885285,"title":"P\u00afnder: Real-time tracking of the human body.","authors":[],"date":"1997","doi":"10.1109\/34.598236","raw":"C. Wren, A. Azarbayejani, T. Darrell, and A. Pentland. P\u00afnder: Real-time tracking of the human body. In IEEE Transactions on Pattern Analysis and Machine Intelligence Vol.19, number 7, pages 780{785, 1997.","cites":null},{"id":885805,"title":"Pyramidal implementation of the lucas kanade feature tracker: Description of the algorithm.","authors":[],"date":"2000","doi":null,"raw":"J.-Y. Bouguet. Pyramidal implementation of the lucas kanade feature tracker: Description of the algorithm. Technical report, Intel Corporation Microprocessor Research Labs, 2000.","cites":null},{"id":884831,"title":"Robust multiple car tracking with occlusion reasoning.","authors":[],"date":"1994","doi":"10.1007\/3-540-57956-7_22","raw":null,"cites":null},{"id":887258,"title":"The automated tracking of vehicles and pedestrians in cctv for use in the detection of novel behaviour. Master's thesis,","authors":[],"date":"2004","doi":null,"raw":"J.A. Humphreys. The automated tracking of vehicles and pedestrians in cctv for use in the detection of novel behaviour. Master's thesis, University of Durham, Durham, United Kingdom, 2004.","cites":null},{"id":884355,"title":"Tracking and object classi\u00afcation for automated surveillance.","authors":[],"date":"2002","doi":"10.1007\/3-540-47979-1_23","raw":"O. Javed and M. Shah. Tracking and object classi\u00afcation for automated surveillance. In 7th European Conference on Computer Vision, Copenhagen, Denmark, May 28-31, 2002.","cites":null},{"id":887752,"title":"Visual surveillance using deformable models of vehicles. Robotics and Autonomous Systems,","authors":[],"date":"1997","doi":"10.1016\/S0921-8890(97)83348-9","raw":"J.M. Ferryman, A.D. Worral, G.D. Sullivan, and K.D. Baker. Visual surveillance using deformable models of vehicles. Robotics and Autonomous Systems, 19(3-4), 1997. 19[17] P. Remagnino, A. Baumberg, T. Grove, D. Hogg, T. Tan, A. Worral, and K. Baker. An integrated tra\u00b1c and pedestrian model-based vision system. In British Machine Vision Conference, volume 2, 1997. Statement regarding amendments The reviewer made two comments: That the model requires training using hand-marked data, and this is timeconsuming. As the reviewer notes, we have drawn attention to this, and consider it a subject for future work. It does not invalidate the scienti\u00afc contribution of the paper, and I think the reviewer has recognised this, and has not actually asked for any change to this part of the paper. A future paper will consider the impact of automatic data gathering for this purpose. Eqn. four looks like there should be some normalization. The reviewer is correct to recognise a problem. In fact the dH term is formed from a normalized histogram, which we had not described properly in the previous paragraph. We have therefore reworded the de\u00afnition of g to make this clear. With this de\u00afnition, the histogram cost is correctly scaled to be composed with the other costs by addition without need for further normalization.","cites":null},{"id":887517,"title":"w4:who? when? where? what? a real time system for detecting and tracking people.","authors":[],"date":"1998","doi":"10.1109\/cvpr.1998.698720","raw":"I. Haritaoglu, D. Harwood, and L.S. Davis. w4:who? when? where? what? a real time system for detecting and tracking people. In International Conference on Face and Gesture Recognition, Nara, Japan, April 14-16, 1998.","cites":null},{"id":886497,"title":"Wall\u00b0ower: Principles and practice of background maintenance.","authors":[],"date":"1999","doi":"10.1109\/iccv.1999.791228","raw":"K. Toyama, J. Krumm, B. Brumitt, and B. Meyers. Wall\u00b0ower: Principles and practice of background maintenance. In Seventh International Conference on Computer Vision, Kerkyra, Greece, pages 255{261, 1999.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2009-03","abstract":"This paper presents a new approach to the tracking of multiple objects in CCTV surveillance using a combination of simple neural cost functions based on Self-Organizing Maps, and a greedy assignment algorithm. Using a reference standard data set and an exhaustive search algorithm for benchmarking, we show that the cost function plays the most significant role in realizing high levels of performance. The neural cost function\u2019s context-sensitive treatment of appearance, change of appearance and trajectory yield better tracking than a simple, explicitly designed cost function. The algorithm matches 98.8% of objects to within 15 pixels","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/54033.pdf","fullTextIdentifier":"http:\/\/eprints.lincoln.ac.uk\/2755\/1\/Humphreys2009IVCMultipleObjectTrackingUsingANeuralCostFunction.pdf","pdfHashValue":"c0168cec414c2007e3551348ea537bba6cbdc9cf","publisher":"Elsevier","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lincoln.ac.uk:2755<\/identifier><datestamp>\n      2013-11-18T14:12:09Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47343030<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lincoln.ac.uk\/2755\/<\/dc:relation><dc:title>\n        Multiple object tracking using a neural cost function<\/dc:title><dc:creator>\n        Humphreys, James<\/dc:creator><dc:creator>\n        Hunter, Andrew<\/dc:creator><dc:subject>\n        G400 Computer Science<\/dc:subject><dc:description>\n        This paper presents a new approach to the tracking of multiple objects in CCTV surveillance using a combination of simple neural cost functions based on Self-Organizing Maps, and a greedy assignment algorithm. Using a reference standard data set and an exhaustive search algorithm for benchmarking, we show that the cost function plays the most significant role in realizing high levels of performance. The neural cost function\u2019s context-sensitive treatment of appearance, change of appearance and trajectory yield better tracking than a simple, explicitly designed cost function. The algorithm matches 98.8% of objects to within 15 pixels.<\/dc:description><dc:publisher>\n        Elsevier<\/dc:publisher><dc:date>\n        2009-03<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lincoln.ac.uk\/2755\/1\/Humphreys2009IVCMultipleObjectTrackingUsingANeuralCostFunction.pdf<\/dc:identifier><dc:identifier>\n          Humphreys, James and Hunter, Andrew  (2009) Multiple object tracking using a neural cost function.  Image and vision computing, 27  (4).   pp. 417-424.  ISSN 0262-8856  <\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1016\/j.imavis.2008.06.002<\/dc:relation><dc:relation>\n        10.1016\/j.imavis.2008.06.002<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lincoln.ac.uk\/2755\/","http:\/\/dx.doi.org\/10.1016\/j.imavis.2008.06.002","10.1016\/j.imavis.2008.06.002"],"year":2009,"topics":["G400 Computer Science"],"subject":["Article","PeerReviewed"],"fullText":"Multiple Object Tracking using a Neural Cost\nFunction\nJames Humphreys a Andrew Hunter b,\u2217\naJames Humphreys, Flat 7, Pembroke House, Station Road, Borehamwood, WD6\n1DB, UK\nbCentre for Visual Surveillance and Machine Perception, University of Lincoln,\nBrayford Pool, Lincoln, LN2 7TS, UK\nAbstract\nThis paper presents a new approach to the tracking of multiple objects in CCTV\nsurveillance using a combination of simple neural cost functions based on Self-\nOrganizing Maps, and a greedy assignment algorithm. Using a reference standard\ndata set and an exhaustive search algorithm for benchmarking, we show that the\ncost function plays the most significant role in realizing high levels of performance.\nThe neural cost function\u2019s context-sensitive treatment of appearance, change of ap-\npearance and trajectory yield better tracking than a simple, explicitly designed cost\nfunction. The algorithm matches 98.8% of objects to within 15 pixels.\nKey words: Surveillance, Tracking, Background differencing, Self-organizing\nmaps, Neural networks\nPACS:\nAutomated video surveillance systems monitor CCTV systems and detect\nanomalous or suspicious behavior [3] [6] [4] [7] [2], with a view to alerting\nhuman operators who can take appropriate actions. The most popular ap-\nproaches rely on motion detection algorithms to identify objects of interest.\nTwo main classes of motion detection algorithms are used: optic flow algo-\nrithms based on tracking movement of some salient points [8], and background\ndifferencing-based systems, which assume a static camera, maintain a model\nof the background of the scene, and detect foreground objects by calculating\nthe difference between the current scene and the background model [9] [6] [4].\nBackground differencing algorithms yield, at each frame, a binary image, the\nsilhouette map, with silhouettes (connected components) corresponding to\n\u2217 corresponding author, +44 1522 886456\nEmail address: ahunter@lincoln.ac.uk (Andrew Hunter ).\nPreprint submitted to Elsevier 11 May 2008\nmoving objects. A good overview of the manifold problems inherent in the\napproach is given in the papers by Javed [10] and Toyama et al [11]. Surveil-\nlance systems based on background differencing use the silhouette map to\ndetect objects of interest (typically people and\/or vehicles) which are then\ntracked; the trajectory of the objects may then be used to highlight objects\nof interest. A typical system thus has three stages: background differencing,\ntracking, and event detection. Events of interest may be defined either explic-\nitly (e.g. movement within a defined zone), or implicitly (e.g. anomalous or\nunusual object trajectories). The output of the tracking system is a number of\nobject identities and trajectories, which may be used for a variety of purposes,\nincluding anomalous behavior detection [5] [12] [13]. Here, we discuss only the\ntracking problem.\nThis paper is concerned with the tracking phase in a background-differencing\nbased system (further details are available in [14]). This phase must deal with\na variety of common problems, including: occlusions (which may lead to ob-\njects disappearing temporarily from view), mutual occlusions (where multiple\nmoving objects overlap \u2013 tracking objects in busy scenes is extremely chal-\nlenging), appearance and disappearance (as objects move in and out of the\nframe), and inherent failings of the background differencing approach. The lat-\nter include: false positive silhouettes induced by reflection artefacts, shadows,\ncamera jitter, and other problems; object fragmentation (where an object is\nbroken into several silhouettes); object merging (where two or more objects\ncreate a single silhouette).\nA number of approaches to tracking have been proposed in the context of\nbackground-differencing based systems. We are concerned with the approach\nwhere the algorithm assigns an object identity to newly detected silhouettes,\nand attempts to maintain this identity through successive frames, deleting\nobjects when they ultimately disappear [1] [15] [9] [3].\nThe tracking component essentially solves a correspondence problem (assign-\ning silhouettes to objects) \u2013 with the added complication that objects may be\ncreated and deleted. The difficulty of the problem relates to the number of\nobjects in the scene; background differencing based systems are able to deal\nonly with low to medium density traffic, with advances in tracking contribut-\ning to the ability to deal with higher densities. The correspondence problem\nis solved by exploiting consistencies in the objects between frames \u2013 typi-\ncally of motion, position and\/or appearance. Cost functions are often defined,\nimplicitly or explicitly, to characterize the appropriateness of assigning partic-\nular silhouettes to particular objects, combined with algorithms that try out\ndifferent assignment possibilities in the search for a low cost solution.\nSimple approaches to the correspondence include matching simple features of\nthe objects and silhouettes, such as bounding boxes, areas and appearance\n2\nhistograms [9]. A number of authors have investigated relatively sophisticated\nmethods based on matching shape models, including splines [4], eigenshapes [1]\nand active contours [15]. These matching algorithms are typically integrated\nwith simple search algorithms designed to make local searches for appropriate\nsilhouettes to match.\nIn this paper we cast this correspondence problem explicitly as one of defining\ntwo components: an appropriate cost function, together with a search algo-\nrithm to identify an acceptable (and preferably optimal) correspondence. We\nuse a detailed reference standard data set with correct correspondences de-\nfined, and an exhaustive search algorithm, as benchmarks. This allows us to\nexplicitly separate the issues of cost function and search algorithm develop-\nment. We demonstrate two effective cost functions: one using a simple, explicit\nappearance discrepancy measure; the other an implicit neural cost function us-\ning Self-Organizing Feature Maps. The latter demonstrates the improvements\nthat can be made by taken into account position in the scene, and contextual\ninformation such as typical behavior, in solving the correspondence problem.\nFor the search problem, we introduce a simple greedy algorithm with close to\noptimal performance. These experiments indicate that a relatively simple fea-\ntures are sufficient to allow robust tracking, provided that the cost function is\nsophisticated and makes full use of both appearance and motion invariances.\n1 The correspondence problem\nBackground differencing maintains a background image,Bt; as each new frame\nFt arrives, it updates the background image, and produces a foreground map\n(a binary image), St, by thresholding the difference image St = (Ft\u2212Bt) > \u03b8\n(and possible applying some morphological clean-up). Connected components\nin the foreground map are called silhouettes, Stj. To track objects, the al-\ngorithm maintains a relationship between N object records, Qi, and the M\nsilhouettes, Sj; this is conveniently described using a bipartite graph, which\nmay be represented by a binary M \u00d7 N match matrix (see figure 1). The\ncorrespondence problem requires, on each time-step, the generation of a new\nmatch matrix, based on the previous time-step and current silhouette image.\nOur approach is to define a search algorithm which produces candidate match\nmatrices. Potential algorithms range from the simple, fast greedy algorithm\nwhich produces a single (possibly sub-optimal) match matrix, to the slow, re-\nliable exhaustive algorithm which produces all possible match matrices within\ncertain constraints. In both cases, the search algorithm makes use of a cost\nfunction, which is designed to identify good object-silhouette matches by com-\nparing the current object position, motion and appearance with the previous\nframe.\n3\n S4 \nS0 \nQ3 \nQ2 \nQ1 \nQ0 S1 \nS2 \nS3 \nObjects Silhouettes \n10100Q3\n00100Q2\n00011Q1\n00011Q0\nS4S3S2S1S0\nFig. 1. A candidate match matrix illustrated as a bipartite graph\nA number of complications arise in establishing a proper correspondence. First,\nmultiple silhouettes may be assigned to a single object, which has broken up\nduring detection. Second, multiple objects may correspond to a single silhou-\nette, due to proximity. Third, there may be spurious silhouettes caused by\nnoise, shadows, reflections, and other artefacts. Fourth, parts of objects may\nsometimes not be represented in silhouettes, due to occlusions or low contrast\nwith the background. Fifth, silhouettes may sometimes include both parts of\nan object and spurious areas (most commonly, from shadows). Sixth, object\nappearance may change over time, for a variety of reasons, but most com-\nmonly changes of facing. Complex cases may involve several of these factors\nsimultaneously.\nThe cost functions require that a one to one correspondence between objects\nand silhouettes be established. To deal with the first and second issues, we\nallow silhouettes to be merged or partitioned (resulting in new silhouettes)\nin a conflict resolution step. We also allow silhouettes to be left unmatched,\ncorresponding to noise patches. This may create a greater or smaller number\nof silhouettes than the number of original connected components. We may\nalso create new objects to correspond to silhouettes that are not assigned to\nexisting objects. Each object is ultimately assigned a single silhouette (which\nmay or may not have been created by merging and\/or partitioning), and this\nsilhouette is considered to define the position and appearance of the object.\nThe overall structure is therefore of a search algorithm that generates one or\nmore candidate match matrices; a conflict resolution step that resolves such\nmatch matrices so that only one to one correspondences between objects and\nsilhouettes exists; and a cost function which evaluates the resulting correspon-\ndence to drive the search algorithm.\n2 An explicit cost function\nIn general, we expect that a moving object will retain approximately the same\nappearance from frame to frame. There may be some changes, including: scale\nchanges due to distance from the camera, lighting conditions in different parts\n4\nof the scene, facing of object with respect to the camera, articulation of the\nobject (e.g. human pose), and changes in occlusion. However, for a sufficiently\nrapid frame rate these changes are usually reasonably gradual. In addition,\nwe expect that the position of an object will not change very much in a sin-\ngle frame, and will do so in a predictable fashion. Many trackers implicitly\nor explicitly construct a cost function based on differences between expected\nand actual measurements of appearance and\/or position. For appearance cor-\nrespondence, we may extract a number of features, including: size (in pixels),\naspect ratio, bounding box, color or intensity histogram, etc. To ensure effi-\ncient merging, it is helpful if these features can be composed (i.e. given two\nsilhouettes, it is possible to directly calculate the features of the union of\nthe silhouettes from the features of the individual silhouettes); partitioning\ninvariably involves recourse back to the pixel values.\nLet Stj be the j\nth silhouette at frame t, and Qt\u22121i be the i\nth object record.\nLet f be a function mapping a silhouette or object record to a column vector\nof features (which may also involve sampling the pixels corresponding to the\nsilhouette). Let g(.) be a matching cost function.\nThe cost function g is easily defined for matching a particular object, Qt\u22121i ,\nwith a particular silhouette, Stj, based on disparity between the two; see equa-\ntion 1. A typical choice for g is a weighted sum-square of the disparities; see\nequation 2.\nEti,j = g(f(Q\nt\u22121\ni )\u2212 f(Stj)) (1)\ng(x) = x\u2032.x (2)\nA global cost function, for a particular configuration matching all objects and\nsilhouettes, is derived by summing the costs of individual matches. However,\nthe issue is complicated by the need to handle multiple matches, object cre-\nation and object deletion. We therefore introduce a conflict resolution step,\nwhich takes a given match matrixM, and produces a new match matrixC(M)\nwith a new set of silhouettes and objects such that each object is matched to\nexactly one silhouette, and each silhouette to exactly one object (a binary\nmonomial matrix). New objects are created and existing objects removed as\nnecessary during the conflict resolution stage. We may then define the global\ncost function as in equation 3, where \u03b4 is an object creation cost, u is the\nnumber of newly-created objects, \u03b3 is an object removal cost function, and c\nthe number of removed objects. These costs prevent excessive object removal\nand addition.\nEt =\n\u2211\ni,j\u2208M\nEti,j + c\u03b4 + u\u03b3 (3)\n5\nWe represent silhouettes and objects using a feature vector f = (x, y, a, h, w,g),\nwhere (x, y) is the centroid position of the pixels, a the area in pixels, h and\nw the height and width respectively of the bounding box, and g = (gk) is the\n16-bin normalized histogram of the pixel intensities (\n\u2211\nk gk = 1). Let fi and fj\nrepresent the feature vectors for the ith object and jth silhouette respectively.\nThen we define the matching cost using normalized disparities in the size and\nappearance histogram, following a cost function implicitly defined by Owens\n[9]; see equation 4.\nEi,j = pA+ pH + pW + dH =\nai \u2212 as\nai\n+\nhi \u2212 hs\nhi\n+\nwi \u2212 ws\nwi\n+ \u2016gi \u2212 gj\u2016(4)\n3 A Neural Approach to Cost Functions\nSelf-Organizing Feature Maps (SOMs), are frequently used to characterize\nnormal and abnormal features. For example, in surveillance they may be used\nto identify anomalous trajectories [5] [12] [13]. In such systems, a large number\nof trajectories are tracked, and the SOM is trained by examples to identify\nnormal and unusual activity. Such systems can pick up surprisingly subtle\nevents, and have the potential benefit of context-sensitivity (e.g. particular\nevents may be more common in particular parts of the scene).\nIn this section we propose the use of SOMs to provide the matching cost\nfunction, by learning based on hand-marked reference matches. SOMs are\ndesigned to produce a novelty signal, which is monotonically related to the\na posteriori probability of observing the given activity, assuming a \u201cnormal\u201d\nevent (i.e. one which is consistent with the events present in the training set).\nMatching cost functions require precisely such signals, with lower probability\nmatches being accorded higher costs. The use of a SOM allows us to capture\nsubtleties, and context-specific issues, that are very hard to build into a \u201chand-\ncoded\u201d cost function. The price paid for this improved performance is the need\nto hand-label a reference match set.\nThe cost function is provided by three SOMs, which characterise correct\nmatches from objects to silhouettes: the Motion SOM, Comparative SOM,\nand Appearance SOM. In operation, a proposed match of object Qi to silhou-\nette Sj is costed by assuming that the match is made, and then summing the\noutput of the SOMs (the novelty signal, the Euclidian distance from the input\nfeature vector to the prototype vector of the winning neuron). Each 40 \u00d7 40\nSOM is trained using a classic two-phase approach: 100 iterations with learn-\ning rate \u03b1 = 0.1\u2192 0.02 and neighbourhood w = 3\u2192 1, then 1000 iterations\nwith a = 0.1\u2192 0.02, w = 0.\nThe Motion SOM is similar to that described by Owens et. al. [5] to perform\n6\nmotion analysis. It is trained to detect whether the combination of current\nand recent positions is a usual combination. In its original role, it indicates\nwhether a detected object trajectory is unusual (and therefore worthy of op-\nerator attention) or not, and can be deployed to this intent in addition to\nbeing used as part of the matching cost function. The SOM has eight inputs:\n(x, y, dx, dy, w(x), w(y), w(dx), w(dy)) where (x, y) is the current position, the\nmotion vector (dx, dy) is given by (dx, dy) = (xt \u2212 xt\u22121, yt \u2212 yt\u22121), and w(.)\nis a time-smoothed average function given by equation 5, where n = 5 is the\nwindow size.\nwt(x) =\n1\nn\nxt +\nn\u2212 1\nn\nwt\u22121 (5)\nThis SOM learns to identify normal motion patterns, which are locale-specific.\nFor example, in an area where north-to-south motion is normal it will generate\na high cost for south-to-north motion. It is worth contrasting this approach\nwith the use of prediction-corrective tracking (e.g. Kalman filtering), which\nis typically not locale-specific, and therefore gives preference to conservation\nof movement [16] [17]. Consider a location with a sharply-turning path; the\nMotion SOM will learn to treat a rapid change of direction to follow the path\nas normal, while motion directly ahead and leaving the path may be unusual;\nin contrast, the Kalman filter is likely to predict that the next location will\nbe straight ahead and off the path. This location-sensitivity gives the Motion\nSOM advantages in resolving uncertainties in match-conflicts. The effect is to\nfavor matches which produce normal movement patterns; for example, if two\nobjects taking different paths come together and separate, this element of the\ncost function will tend to select the match which produces the most usual pair\nof resulting trajectories.\nThe Comparative SOM plays a similar role to the Owen\u2019s cost function pre-\nsented in the previous section. It has eight inputs: (x, y, dx, dy, pA, pH, pW, dH),\nwhere (x, y) is the centroid position, (dx, dy) the motion vector, and pA, pW ,\npH and dH are the four terms in the Owen\u2019s cost function; see equation 4.\nThe last four terms allow the Comparative SOM to assign costs to changes\nin the basic appearance of object. The first four terms provide the \u201ccontext,\u201d\nallowing the system to estimate the cost differently according to position and\nvelocity. For example, an object entering the edge of the screen tends to grow\nin size rapidly as it becomes visible. Similarly, pedestrians exiting their vehicles\nmay be partially occluded by their own or other cars; they then apparently\ngrow in size as they emerge from occlusion. The Comparative SOM learns\nto model these localized effects, becoming tolerant of various context-specific\nchanges in object appearance.\nThe Appearance SOM is the third element of the system. Its role is to assess\nwhether the appearance of the object is normal \u2013 again, in a location-specific\n7\nway. In contrast with the Comparative SOM, it assesses the absolute appear-\nance rather than change in appearance. The inputs are (x, y, a, ar, h, w), with\n(x, y) the centroid location, a the area, (ar, h, w) the aspect-ratio, height and\nwidth of the bounding box. Critically, the object size typically varies with\ny (since high-mounted cameras tilted downwards show more distant objects\nhigher on the y axis). More subtle location-specific variations may also be\ncaptured, including again differences due to partial occlusions and appear-\nance\/disappearance zones.\nArguably, the three SOMs could be combined into a single SOM with an input\nvector including the features of all three; however, our experiments indicate\ninferior performance, no doubt due to the higher dimensionality. We have also\nexperimented with the use of a single set of SOMs for all objects, versus two\nsets of SOMs \u2013 one for pedestrians, and one for vehicles. Again, although in\nprinciple a single SOM should suffice, we achieved higher performance with\nseparate versions for pedestrians and vehicles.\nIn order to apply separate SOMs for pedestrians and vehicles, it is necessary\nto classify objects. We experimented with two classifiers; a simple Bayesian\nclassifier based solely on object area, which correctly classified 95% of objects;\nand a Multilayer Perception with input vector (a, w, h, ar,max(s), y) (where\ns is the inter-frame centroid speed in pixels), and four hidden units, which\ncorrectly classified 99.2%. The rare object classification failures of the MLP\nare invariably due to occlusion events. Further details are available in reference\n[14].\n4 Conflict Resolution\nThe conflict resolution stage processes a match matrix with conflicts (multiple\nobjects matched to a single silhouette and\/or multiple silhouettes matched to\na single object), and removes these conflicts by merging and splitting silhou-\nettes. In merging, multiple silhouettes are combined into a single silhouette; in\nsplitting, a silhouette is divided into several sub-silhouettes (one per matching\nobject).\nIn merging, the new silhouette is effectively created from the statistics of the\nunion of the pixels in the merged silhouettes. However, the use of composable\nfeatures allows us to generate a new feature vector efficiently. Given the silhou-\nette feature vectors f1 = (x1, y1, a1, h1, w1,g1) and f2 = (x2, y2, a2, h2, w2,g2),\nand the bounding box definitions ((ti, bi, li, ri), hi = ti\u2212bi, wi = ri\u2212li, i \u2208 1, 2)\nthe merged vector is calculated as below.\nam = a1 + a2 (6)\n8\nxm =\nx1a1 + x2a2\nam\n(7)\nym =\ny1a1 + y2a2\nam\n(8)\ngim =\ngi1a1 + g\ni\n2a2\nam\n(9)\nlm = min(l1, l2); rm = max(r1, r2); tm = min(t1, t2); bm = max(b1, b2) (10)\nSplitting is more complex. Our approach is to split a silhouette that is matched\nto multiple objects along a single direction, in proportion to the size of the\nsource objects. Our first approach was to calculate the principal component\nof the pixel coordinates of the silhouette, and to split along a line orthogonal\nto this. This approach relies on the assumption that the principal axis is likely\nto run between objects, and that the level of mutual occlusion is not too high.\nThe first of these assumptions is routinely violated by pedestrians walking\nside by side, a problem which may be corrected by adjusting the covariance\nmatrix in the PCA calculations using the typical pedestrian aspect ratio (2.5);\nsee figure 2.\nFig. 2. The PCA-based partitioning of two merged pedestrians. The merged sil-\nhouette is taller than it is wide, causing partitioning to fail. Scaling corrects the\nproblem.\nA more computationally expensive approach to line partitioning is to search\nthrough a number of splits at different angles, choosing that with the lowest\ncost (we take 30 divisions at every 15 degrees); see figure 3. This algorithm has\nexcellent performance providing its inherent assumptions are not violated: the\nobjects have been tracked correctly, there is little mutual occlusion, division\nalong a line is possible, and the visible area has not changed significantly,\nand we use it throughout this work. Figure 3 also shows the cost versus angle\nfor this case, where the cost is estimated using the SOMs on the silhouettes\narising from each tested angle.\nThe conflict resolution algorithm applies merges and splitting efficiently as\nfollows. The (binary) candidate match matrix can also be represented using\na bipartite graph; see figure 1. The match matrix is augmented with two\n9\n\u22121.5 \u22121 \u22120.5 0 0.5 1 1.5\n0\n0.5\n1\n1.5\n2\nAngle\nCo\nst\nFig. 3. Angle-search method \u2013 (a) 30 different angles are assessed, and (b) the best\nangle is chosen\nadditional working states, as follows:\nMc(q, s) =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n0 No link between silhouette and object\n1 Link between object and silhouette that needs resolving\n2 Secure link between object and silhouette\n3 Silhouette-object match no longer possible\nAll matches are initialized to state 1. Splitting and merging operations both\nresult in extension of the match matrix, with new silhouettes and\/or objects\nadded, and some original silhouettes marked as no longer used (state 3). Ulti-\nmately, all matches resolve to state 2 (secure links). The algorithm has three\nsteps, with the first two repeated until there are no conflicts; the final step\nconsolidates results; see figure 4. In step one, fully connected subgraphs are\nresolved. Such a sub-graph contains silhouettes and objects that have been\nmutually assigned to each other and to no others. Trivial cases include one\nto one matches (which are accepted as is), a single object matching multiple\nsilhouettes (which are merged), and a single silhouette which matches multiple\nobjects (which are split). Many to many fully connected subgraphs typically\noccur when a group of objects are moving closely together and the joint silhou-\nette breaks up due to noise effects. Such a subgraph is resolved by merging\nall the silhouettes and then splitting using the algorithms discussed above.\nSubgraph identification is performed efficiently by sorting objects by valency\n(since members of a fully connected subgraph must have the same valency).\n \nNo \nYes Step 1: resolve fully \nconnected subgraphs \nStep 2: resolve non-fully \nconnected subgraphs \nInsecure (\u201c1\u201d ) \nmatches \nremain in M? \nStep 3: consolidate \nsecure matches \nFig. 4. An overview of the steps taken by the conflict resolution module\nStep two reduces the number of insecure matches, yielding further fully con-\nnected subgraphs that can be resolved by iteration through step one. This step\n10\ndeals with the unsatisfactory situation where we have non-fully connected sub-\ngraphs, where it is not so clear how to merge and\/or partition. One option is to\nenforce full connection within the subgraph and treat as above. However, this\nleads to a less powerful search algorithm than the method described below.\nInstead, we find the lowest valency silhouette, and make the match secure for\nthis silhouette, splitting it in the case that the valency is greater than one.\nThe areas of the matched object(s) are temporarily adjusted by subtraction\nof the silhouette area, so that any further splitting is appropriate. The most\ncommon case here is that a single silhouette (valency one) is firmly matched\nto an object, and iteration through step one leads to further merges and splits\nto resolve the remaining areas of that object.\nStep three resolves any remaining matches of multiple silhouettes to single\nobjects \u2013 these are firm matches arising when step two has confirmed a match\nand further iterations of step one have assigned other silhouettes to the same\nobject. This stage ensures that all remaining matches are one to one. With\na one to one match established, the cost function can be used to assess how\nacceptable the assignment is.\n5 The Search Algorithm\nThe object correspondence is solved by a search algorithm, which considers\nsome or all of the possible match matrices arising from the current set of\nsilhouettes and objects. Naively, The number of possible match matrices is\n2M\u2217N . We can reduce the size of the problem trivially by specifying a maximum\nmatch radius between object and silhouette centroids; this is used to initialize\na binary valid match matrix, with unit entries only for \u201cin-range\u201d object-\nsilhouette matches. The number of possible match matrices is then 2V , where\nV is the number of unit entries in the valid match matrix.\nA global search algorithm finds the lowest cost of all possible match matrices.\nThis is particularly useful during development, as it allows us to analyze the\nperformance of the cost function separately from the search algorithm. Any\ninadequate matches generated while using a global search algorithm are the\nfault of the cost function. When testing any other search algorithm, we can\nbenchmark its performance against the global search algorithm. The simplest\nglobal search is the exhaustive search algorithm, which considers all possible\nmatch matrices. However, this has high computational expenses, and so is not\nsuitable for real-time use.\nWe introduce a greedy algorithm that is designed to yield an acceptable (but\npossibly sub-optimal) match matrix rapidly, and is capable of real-time exe-\ncution. It has four stages:\n11\nIn stage one, each object in turn is matched to the single valid silhouette with\nthe lowest cost match. Objects are left unmatched only if there are no valid (in\nrange) silhouettes; some silhouettes may be matched to multiple objects. An\nanalysis of the reference standard shows that 98.7% of silhouettes are assigned\nby this phase to an object that is at least part of that silhouette.\nIn stage two, we try to verify if potential merges of objects are likely to be\nvalid. Such potential merges are represented by multiple objects assigned to\nthe same silhouette by stage one. We create a temporary \u201cmacro object\u201d\nby merging the object records using the technique described for silhouette\nmerging above, with the modification that the object positions are projected\nforward one time-step in space using the previous time-step velocity. The cost\nof matching the macro object to the silhouette is compared with the cost of\nmatching the single best-matching object to the silhouette; if the macro object\nmatches best, it is assumed that a \u201cmerge event\u201d has occurred.\nIf it is concluded that a merge event has not occurred, only the best-matching\ncorrespondence is maintained. The other objects are reassigned to their next-\nbest match. This may in turn cause further match conflicts, and the process\nis repeated until these have all been removed.\nIn stage three, unmatched silhouettes are considered. Each such silhouette is\nconsidered against each valid object; the silhouette is assigned to the object\nwhere this yields the lowest cost, provided the resulting cost is less than the\ncost of leaving the silhouette unassigned (bearing in mind that leaving the sil-\nhouette unassigned imposes an \u201cunmatched silhouette\u201d cost). The assignment\ncosts are calculated by using the conflict resolution and global cost function\nsteps.\nStep four removes poor object matches. It unmatches each object in turn from\nits silhouettes, and calculates if this lowers the global cost. Such matches may\narise, for example, when an object leaves the scene and the record is matched\nto a noise silhouette within the valid range.\nThis greedy algorithm is extremely simple, with complexity O(MN), and has\nperformance close to optimal, as discussed in the next section.\n6 Evaluation\nA key part of the work reported in this paper is the use of a reference stan-\ndard data set. In this data set, the correct object-silhouette correspondence is\nmanually identified. Given the complexity of the algorithm, generating such\na reference standard offers some challenges, which are discussed below. The\n12\nbenefits include the ability to develop the cost function and search algorithm\nindependently, and to have a powerful mechanism to evaluate both. This has\nallowed us to develop algorithms which are both simpler and more powerful\nthan our previously-published versions \u2013 including discarding features which\nintuitively seemed helpful, but in reality were shown to have no impact on\nperformance.\nIn the context of object tracking, a large number of video frames are required\nto provide a reasonably diverse set of activities. We used three sequences of 1.5\nhours each, sampled at 4 frames per second; there were approximately 15000\nframes with some activity. The scene chosen exhibits a mixture of pedestrian\nand car traffic, with low to medium levels of crowding (up to twenty or so\nvisible objects, but usually much lower).\nAn ideal reference standard for this problem would have each object labelled,\npixel by pixel, for every frame. This is clearly impracticable for such a large\nnumber of frames. Given that our aim is to investigate effective algorithms for\ngenerating match matrices, we instead use a reference standard which speci-\nfies the optimal match matrix. To generate the reference standard, a special\nversion of the tracking programme was developed with a simple greedy assign-\nment algorithm, and the ability to walk through the sequence frame by frame.\nThe user interface displays object identities and types against silhouettes, and\nallows these to be altered where the greedy algorithm makes mistakes.\nIn evaluation of a tracking algorithm against the reference standard, it is nec-\nessary to \u201creset\u201d the tracking algorithm objects to those found in the reference\nstandard on each frame \u2013 if the tracking algorithm is run continuously, object\nidentities may be permuted with respect to the reference standard, and so\nmatch statistics cannot be easily and automatically generated. A disadvan-\ntage of this approach is that our statistics do not address the capability of a\ntracking algorithm to recover from errors.\nIn both reference standard and tracker, each object from the previous frame\nmay either be matched (assigned to a silhouette), or unmatched. This yields\nfour possible results: if the object is matched in both, we can assess to what\nextent the match is consistent; if matched in neither, then the object has\nbeen correctly removed. If the reference standard has a match and the tracker\nnone, then the object is defined as lost. Hanging objects are created when\nthe reference standard removes an object, but the tracker retains it (e.g. by\nspuriously assigning it to a noise patch). In addition, errors sometimes cause\nthe tracker to create entirely new spurious extra objects (e.g. as a result of\ncamera jitter). When the object has been matched in both reference standard\nand tracker, we can assess the correctness of the match in a variety of ways.\nPerhaps the most useful is the distance between the reference and tracker\ncentroids, which we place into five pixel bins for simplicity of analysis. A\n13\nsecond measure is the number of \u201cflips\u201d - disparities in the match-matrix for\na given object (i.e. a count of the number of silhouettes missing or added).\nThis gives us a good picture of the goodness of fit at the match matrix level,\nalthough it does not distinguish between the effect of small and large silhouette\nerrors.\nThe use of the reference standard has several deficiencies. It accepts the back-\nground differencing results as fact, although this stage in the processing some-\ntimes produces serious errors. Figure 5 illustrates one extreme case, where a\npedestrian is exiting a car and his or her shadow has caused very poor seg-\nmentation. Any match matrix will give questionable results here. To avoid\ndistorting effects, we have excluded a small number of frames from the refer-\nence standard, where such issues are particularly severe.\nFig. 5. A poorly segmented pedestrian. Whichever match matrix is chosen for the\nreference standard, the result will always be unsatisfactory\nThe exhaustive algorithm, although very useful, is extremely slow for frames\nwith large numbers of objects. Consequently, we removed a small proportion\nof very busy frames (1.3%; the removal of the worst three alone reduced the\nprocessing time by a third).\nThe reference standard data set consists of three sequences of 1.5 hours each.\nThe first sequence was used to train the SOMs for the neural cost function.\nThe second sequence, the selection set, was used during the development of\nthe system to identify effective algorithms, SOM configuration, and control\nsettings. The third sequence, the test sequence, is used to generate unbiased\nperformance statistics, reported below. The test set contains 5785 frames.\nFigures 6 shows the performance of four system configurations (combining\ngreedy and exhaustive algorithms with Owen\u2019s and SOM cost functions) on\nthe test set.\nIt is apparent that the greatest factor in performance is the cost function,\nwith the SOM neural cost function out-performing the explicit Owens cost\nfunction. There is surprisingly little differentiation between the exhaustive\nand greedy algorithms when the neural cost function is used, indicating that\nthe greedy algorithm is more than adequate to the search task. Using the\nOwens cost function, the exhaustive algorithm matches 99.72% of objects to\nwithin 15 pixels, whereas the greedy algorithm manages only 98.83% (roughly\nthree times the error rate). As the cost function improves, the search algorithm\nbecomes less relevant \u2013 perhaps because the first stage match in the greedy\n14\n5 10 15 20 25 30 35 40 45\n75\n80\n85\n90\n95\n100\nDistance (pixels)\n%\n w\nith\nin\n d\nist\nan\nce\n \n \nOwens, Greedy\nOwens, Exhaust\nSOM, Greedy\nSOM, Exhaust\n0 1 2 3 4\n75\n80\n85\n90\n95\n100\nNumber of flips\n%\n w\nith\nin\n N\n fl\nip\ns\n \n \nOwens, Greedy\nOwens, Exhaust\nSOM, Greedy\nSOM, Exhaust\nFig. 6. Percentage of object matches within given distance and flips of the reference\nstandard, using four combinations of cost and search function\nalgorithm is more likely to be correct. The initial step of the neural cost\nfunction finds a silhouette which is part of the object 98.7% of the time,\ncompared with 96.6% for the Owen\u2019s cost function.\nThe number of lost, hanging and extra objects is shown in table 1. The picture\nis rather more ambiguous here, with the SOM and exhaustive approaches\nshowing overall advantages over the Owen\u2019s and greedy approaches, but with\nsome differences in performance. The exhaustive algorithms tend to do worse\non hanging objects, as they search more thoroughly for a matching silhouette\nwhen an object has gone out of sight, and are consequently more prone to\nfalsely assign the object to a noise silhouette. A substantial number of extra\nobjects are created in the test sequence \u2013 these are transients due to camera\njudder in high winds, and are an inevitable occurrence. However, they do not\ndamage the quality of the tracking of true objects, and are rapidly eliminated\nby further stages of processing [14].\nTo illustrate how and why the SOM-based cost function out-performs the\nOwen\u2019s cost function, we conducted a number of sensitivity analysis experi-\nments. The Motion SOM is designed to respond to unusual paths. Figure 7\nshows the effect of modifying two normal silhouettes by artificially displacing\ntheir positions a short distance at a variety of angles; object A was sampled\n15\nTable 1\nLost, hanging and extra objects\nAlgorithm Lost Hanging Extra\nOwens, Greedy 18 14 425\nOwens, Exhaust 11 17 416\nSOM, Greedy 12 2 411\nSOM, Exhaust 1 4 410\nat a position where movement up and down the screen are usual, whereas\nobject B was sampled at a position where only downward motion is normal.\nThe graph illustrates that the Motion SOM is extremely sensitive to context-\ndependent direction of motion.\n0 1.57 3.14 4.71 6.28\n0\n0.02\n0.04\n0.06\n0.08\n0.1\n0.12\n0.14\n0.16\nMotion angle (radians)\nCo\nst\n \n \nA\nB\nTrue angle\nFig. 7. Performance of the motion SOM, given a specific point and speed but dif-\nferent angles of motion.\nFigure 8 illustrates context-sensitive performance of the Comparative SOM.\nIn this experiment, object A is a car ready to park near the centre of the\nscene; object B is another car entering the camera view. The figure illustrates\nsensitivity to changes of the object area. For object A, the lowest cost occurs\nwhere the area does not change, as expected. However, for object B the Com-\nparative SOM assigns the lowest cost when the area increases at the normal\nrate for a car entering the scene in this location.\nFigure 9 shows the performance of the Appearance SOM, under manipulation\nof the aspect ratio. Pedestrian A is getting into a car parked amid a row of\ncars, and consequently is partially occluded, which is usual in this location;\npedestrian B is in an uncluttered part of the scene. The SOM demonstrates a\nclear locale-dependent preference for particular aspect ratios.\nIt is this ability of the SOM cost function to model context-dependent cost\nrelationships (including position and velocity dependent costs) which makes it\nso effective. Further sensitivity experiments reveal similar responses to other\ninput variables of the cost function. This is well-illustrated by the performance\n16\n\u22122 \u22121.6 \u22121.2 \u22120.8 \u22120.4 0 0.4 0.8 1.2 1.6 2\n0\n0.02\n0.04\n0.06\n0.08\n0.1\n0.12\n0.14\n0.16\n0.18\npArea\nCo\nst\n \n \nVehicle A\nVehicle B\nFig. 8. Performance of the comparative SOM, testing the effect of changing pArea\non the output cost. Vehicle A is in the centre of the scene, about to park. Vehicle\nB is just entering the scene.\n0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nAspect Ratio\nCo\nst\n \n \nPedestrian A\nPedestrian B\nFig. 9. Performance of the appearance SOM, testing the effect of changing aspect\nratio on the output cost. Pedestrian A has just exited his\/her vehicle. Pedestrian B\nis in an unobstructed area of the scene.\nFig. 10. The two pedestrians used to capture the data for figure 9\nof the neural cost function on objects touching the edge of the image, where\nobject appearance and disappearance is most common. Contrary to our initial\nexpectations, the greedy neural system matches 99.55% of such objects to\nwithin five pixels, as opposed to 99.1% of non-edge objects, thus indicating\nthat the context-sensitivity of the SOMs is able to handle such appearance\n17\nand disappearance zones perfectly adequately, without the need to explicitly\nmodel them.\n7 Conclusion\nWe have introduced a new algorithm for the tracking of multiple objects in\na background-differencing based system using object to silhouette matching.\nThe algorithm uses a neural cost function based on three SOMs that learn\nnormal patterns of motion, change and appearance of objects in the scene in a\ncontext-sensitive fashion. This cost function is combined with a simple, highly\neffective greedy algorithm to allow object identification consistent with normal\npatterns of behavior in the scene. We have used a reference standard data set,\nan explicitly designed cost function and an exhaustive search algorithm to\nbenchmark the new algorithm. This study shows that the context-sensitive\nperformance of the neural cost function is key in achieving good tracking\nperformance, allowing us to use a relatively simple approach to assigning the\nbest match. We have thus established that the use of SOMs in constructing\ncost functions for configuration problems is viable.\nThe most significant drawback of the approach is the need for a reference-\nstandard marked-up data set to train the SOMs. The mark-up is a time-\nconsuming procedure, and is clearly not viable for practical use. However,\nit should be possible to automatically produce a training reference set, by\ngathering data for a period of time, and inserting into the reference set only\ndata that can be unambiguously identified automatically \u2013 that is, where there\nare no other objects within the vicinity, and a \u201cclean track\u201d (with no break-up\nof the silhouette) is achieved throughout. This will be the subject of future\nwork.\nReferences\n[1] A. Baumberg and D. Hogg. An adaptive eigenshape model. In Proc of the 6th\nBritish Machine Vision Conference, Vol 1, pp 87-96, 1995.\n[2] O. Javed and M. Shah. Tracking and object classification for automated\nsurveillance. In 7th European Conference on Computer Vision, Copenhagen,\nDenmark, May 28-31, 2002.\n[3] A. Hunter, J. Owens, and M. Carpenter. A neural system for automated cctv\nsurveillance. In IEE Symposium on Intelligent Distributed Surveillance Systems,\ned. S. Velastin, 26 Feb. 2003, IEE Savoy Place, London, IEE London, ISSN\n0963-3308, 2003.\n18\n[4] D. Koller, J. Weber, and J. Malik. Robust multiple car tracking with occlusion\nreasoning. In The third European conference on Computer vision (vol. 1),\nStockholm, Sweden, pages 189\u2013196, 1994.\n[5] J. Owens and A. Hunter. Application of the self-organising map to\ntrajectory classification. In Proc. 3rd IEEE International Workshop on Visual\nSurveillance, pages 77\u201383. Dublin, 2000.\n[6] C. Wren, A. Azarbayejani, T. Darrell, and A. Pentland. Pfinder: Real-time\ntracking of the human body. In IEEE Transactions on Pattern Analysis and\nMachine Intelligence Vol.19, number 7, pages 780\u2013785, 1997.\n[7] P. KaewTraKulPong and R. Bowden. An improved adaptive background\nmixture model for real-time tracking with shadow detection. In 2nd European\nWorkshop on Advanced Video Based Surveillance Systems, AVBS01. Sept, 2001.\n[8] J.-Y. Bouguet. Pyramidal implementation of the lucas kanade feature\ntracker: Description of the algorithm. Technical report, Intel Corporation\nMicroprocessor Research Labs, 2000.\n[9] J. Owens, A. Hunter, and E. Fletcher. A fast model-free morphology-based\nobject tracking algorithm. In British Machine Vision Conference, volume 2,\npages 767\u2013776, 2002.\n[10] O. Javed, S. Khurram, and M. Shah. A hierarchical approach to robust\nbackground subtraction using color and gradient information. In IEEE\nWorkshop on Motion and Video Computing, Orlando, Dec 5-6, 2002.\n[11] K. Toyama, J. Krumm, B. Brumitt, and B. Meyers. Wallflower: Principles and\npractice of background maintenance. In Seventh International Conference on\nComputer Vision, Kerkyra, Greece, pages 255\u2013261, 1999.\n[12] J. Owens, A. Hunter, and E. Fletcher. Novelty detection in video surveillance\nusing hierarchical neural networks. In Proc. International Conference on\nArtificial Neural Networks (ICANN 2002), volume 2, pages 1249\u20131254. Madrid,\n2002.\n[13] N. Johnson and D.C. Hogg. Learning the distribution of object trajectories for\nevent recognition. Image and Vision Computing, 14:609\u2013615, 1996.\n[14] J.A. Humphreys. The automated tracking of vehicles and pedestrians in cctv for\nuse in the detection of novel behaviour. Master\u2019s thesis, University of Durham,\nDurham, United Kingdom, 2004.\n[15] I. Haritaoglu, D. Harwood, and L.S. Davis. w4:who? when? where? what? a\nreal time system for detecting and tracking people. In International Conference\non Face and Gesture Recognition, Nara, Japan, April 14-16, 1998.\n[16] J.M. Ferryman, A.D. Worral, G.D. Sullivan, and K.D. Baker. Visual surveillance\nusing deformable models of vehicles. Robotics and Autonomous Systems, 19(3-\n4), 1997.\n19\n[17] P. Remagnino, A. Baumberg, T. Grove, D. Hogg, T. Tan, A. Worral, and\nK. Baker. An integrated traffic and pedestrian model-based vision system.\nIn British Machine Vision Conference, volume 2, 1997.\nStatement regarding amendments\nThe reviewer made two comments:\nThat the model requires training using hand-marked data, and this is time-\nconsuming. As the reviewer notes, we have drawn attention to this, and con-\nsider it a subject for future work. It does not invalidate the scientific contri-\nbution of the paper, and I think the reviewer has recognised this, and has not\nactually asked for any change to this part of the paper. A future paper will\nconsider the impact of automatic data gathering for this purpose.\nEqn. four looks like there should be some normalization. The reviewer is cor-\nrect to recognise a problem. In fact the dH term is formed from a normalized\nhistogram, which we had not described properly in the previous paragraph.\nWe have therefore reworded the definition of g to make this clear. With this\ndefinition, the histogram cost is correctly scaled to be composed with the other\ncosts by addition without need for further normalization.\n20\n"}