{"doi":"10.1109\/ICIP.2008.4712305","coreId":"102812","oai":"oai:epubs.surrey.ac.uk:2354","identifiers":["oai:epubs.surrey.ac.uk:2354","10.1109\/ICIP.2008.4712305"],"title":"Flexible generation of video summaries from layered video bit-streams","authors":["Calic, J","Mrak, M","Kondoz, A"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2008-12-12","abstract":"The work presented in this paper introduces a method for efficient adaptability of video summaries to spatial requirements defined by display size, user\u2019s needs and channel limitations. By utilising compressed domain features and an efficient contour evolution algorithm, a scale space of temporal video descriptors is generated, enabling dynamic video summarisation in real-time. The summary is laid out utilising an unsupervised robust spectral clustering technique and a fast discrete optimisation algorithm. Results show excellent scalability of the video summarisation interface and highly improved efficiency of summary generation","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":"IEEE","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:epubs.surrey.ac.uk:2354<\/identifier><datestamp>\n      2017-01-04T11:00:24Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      74797065733D636F6E666572656E63655F6974656D<\/setSpec><setSpec>\n      6469766973696F6E733D656E67616E64706879736963616C736369656E636573:656C656374726F6E6963656E67696E656572696E67:63637372<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/epubs.surrey.ac.uk\/2354\/<\/dc:relation><dc:title>\n        Flexible generation of video summaries from layered video bit-streams<\/dc:title><dc:creator>\n        Calic, J<\/dc:creator><dc:creator>\n        Mrak, M<\/dc:creator><dc:creator>\n        Kondoz, A<\/dc:creator><dc:description>\n        The work presented in this paper introduces a method for efficient adaptability of video summaries to spatial requirements defined by display size, user\u2019s needs and channel limitations. By utilising compressed domain features and an efficient contour evolution algorithm, a scale space of temporal video descriptors is generated, enabling dynamic video summarisation in real-time. The summary is laid out utilising an unsupervised robust spectral clustering technique and a fast discrete optimisation algorithm. Results show excellent scalability of the video summarisation interface and highly improved efficiency of summary generation.<\/dc:description><dc:publisher>\n        IEEE<\/dc:publisher><dc:date>\n        2008-12-12<\/dc:date><dc:type>\n        Conference or Workshop Item<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/epubs.surrey.ac.uk\/2354\/1\/SRF002474.pdf<\/dc:identifier><dc:identifier>\n          Calic, J, Mrak, M and Kondoz, A  (2008) Flexible generation of video summaries from layered video bit-streams  In: 15th IEEE International Conference on Image Processing, 2008-10-12-2008-10-15, San Diego, CA, USA.     <\/dc:identifier><dc:relation>\n        http:\/\/ieeexplore.ieee.org\/xpls\/abs_all.jsp?arnumber=4712305&tag=1<\/dc:relation><dc:relation>\n        10.1109\/ICIP.2008.4712305<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/epubs.surrey.ac.uk\/2354\/","http:\/\/ieeexplore.ieee.org\/xpls\/abs_all.jsp?arnumber=4712305&tag=1","10.1109\/ICIP.2008.4712305"],"year":2008,"topics":[],"subject":["Conference or Workshop Item","PeerReviewed"],"fullText":"The work presented was developed within VISNET II, a European Network of \nExcellence, funded under the European Commission IST FP6 programme. \nFLEXIBLE GENERATION OF VIDEO SUMMARIES \nFROM LAYERED VIDEO BIT-STREAMS \n \nJanko \u0004ali\u0005, Marta Mrak and Ahmet Kondoz \n \nI-Lab, Centre for Communication System Research \nUniversity of Surrey, Guildford, United Kingdom \n{j.calic, m.mrak, a.kondoz}@surrey.ac.uk \n \nABSTRACT \n \nThe work presented in this paper introduces a method for \nefficient adaptability of video summaries to spatial require-\nments defined by display size, user's needs and channel \nlimitations. By utilising compressed domain features and an \nefficient contour evolution algorithm, a scale space of tem-\nporal video descriptors is generated, enabling dynamic video \nsummarisation in real-time. The summary is laid out utilis-\ning an unsupervised robust spectral clustering technique and \na fast discrete optimisation algorithm. Results show excel-\nlent scalability of the video summarisation interface and \nhighly improved efficiency of summary generation. \n \nIndex Terms\u2014 video analysis, video summarisation, \nlayered video coding \n \n1. INTRODUCTION \n \nFollowing the trend of convergence in multimedia technol-\nogy, the work presented in this paper introduces an efficient \nsystem for large-scale video summarisation that exploits \ncompressed-domain analysis of scalable video. Aimed at \nresponsive and intuitive browsing interfaces for large video \ndatabases, the system generates visual representation of \nvideo data in a form of a comic-like summary with low \nlatency, thus making a shift towards more user-centred \nsummarisation and browsing of large video collections by \naugmenting user's interaction with the content rather than \nlearning the way users create related semantics. \nThe presented algorithm follows the narrative structure \nof comics, linking the temporal flow of video sequence with \nthe spatial position of panels in a comic strip. This approach \ndifferentiates our work from the typical reverse storyboard-\ning [1] or video summarisation approaches. Although there \nhave been attempts to utilise the form of comics as a me-\ndium for visual summarisation of videos [2] [3], the high \ncomplexity of these algorithms hindered summarisation at \nthe larger scale or with low processing latency. In order to \novercome the demanding complexity constraints, the pro-\nposed system utilises video analysis algorithm that uses \ncompressed-domain hierarchical motion information, cou-\npled with a fast discrete optimisation algorithm for creation \nof the comic-like layout of extracted key-frames. Avoiding \nfull video decoding, the analysis algorithm decompresses \nonly the motion information from targeted temporal decom-\nposition level of the scalable video. The motion activity \nmetric is chosen for capturing intensity of action since it is \nhighly correlated to human perception [4]. Using a fast and \nrobust geometrical curve simplification algorithm, a set of \nmost representative key-frames is extracted as a visual \nsummary of the analysed video. In order to generate an in-\ntuitive and yet compact video browsing interface, our ap-\nproach introduces a novel solution based on dynamic pro-\ngramming (DP). In addition, the presented algorithm applies \na new approach to the estimation of key-frame sizes in the \nfinal layout by exploiting a spectral clustering methodology \ncoupled with a specific cost function that balances between \ngood content representability and discovery of unanticipated \ncontent. Furthermore, a robust unsupervised estimation of \nnumber of clusters is introduced. The evaluation results \ncompared to existing methods of video summarization \nshowed substantial improvements in terms of algorithm ef-\nficiency. \nIn order to facilitate compressed-domain analysis of lay-\nered bit-streams in an efficient manner, utilisation of scal-\nable video coding technology is presented in Section 2 fol-\nlowed by the description of an algorithm for fast selection of \nthe most representative key-frames. Section 3 introduces a \nnovel method for forming of video summaries that optimises \ngenerated visual representation to available spatial re-\nsources. The results of the algorithms presented are given in \nSection 4, followed by the final conclusions. \n \n2. KEY-FRAME SELECTION FROM LAYERED \nVIDEO BIT-STREAM \n \nIn the proposed framework the video analysis is performed \nover the activity measure, which is a frame-based descriptor. \nMotion activity descriptor is a standard tool for capturing \nintensity of action correlated to human perception [4] and in \nour work its extraction is performed from layered bit-\nstreams. \n2516978-1-4244-1764-3\/08\/$25.00 \u00a92008 IEEE ICIP 2008\nAuthorized licensed use limited to: University of Surrey. Downloaded on April 23,2010 at 15:53:02 UTC from IEEE Xplore.  Restrictions apply. \nLow-complexity computation of the activity measure is \nachieved using compressed domain data. The motion infor-\nmation available from the compressed video is obtained \nusing partial decoding only. Final selection of key-frames is \nbased on evaluation of motion activity metric, relatively to \nthe neighbouring frames compared to the overall video se-\nquence activity. \n \n2.1. Compressed domain video analysis \n \nFor each frame an activity metric is extracted from associ-\nated motion information used in video coding. Decoding of \nmotion information without complete video decoding is a \nlow-complexity process, which can be further simplified, if \nlayered coded video, such as scalable coded video [5] [6] \n[7], is used because of underlying layered structure. For bit-\nstreams that are encoded using motion compensated tempo-\nral filtering [5] the motion information can be obtained for \neach compensated frame from different levels of temporal \nfiltering. Lower temporal bit-stream layers consist of the \ndata related to more distant frames. While in the context of \ncompression the coding efficiency drops when the frames \nare more distant, those frames are still close enough for an-\nalysis. Therefore the analysis for key-frame selection can be \nperformed from lower bit-stream layers.  \nIn order to obtain the motion activity metric for the ob-\nserved frame, a variance of magnitude of motion vectors is \ncalculated for all temporal prediction modes. For bidirec-\ntional prediction the motion vector magnitudes are averaged \nand treated as the magnitudes for unidirectional prediction. \nThe overall activity measure, \u0006t for a frame at time position t \nis computed for all motion compensated frames at the low-\nest accessible bit-stream layer which is then used in key-\nframe selection algorithm described in the following section. \n \n2.2. Discrete contour evolution and key-frame extraction \n \nIn order to generate a scalable temporal descriptor that fa-\ncilitates dynamic extraction of key-frames from the se-\nquence, the activity metric needs to be simplified in a way \nthat spurious and small changes are discarded without any \ninfluence to the main features of the metrics curve. A \nmethod called Discrete Curve Evolution (DCE) [8] effi-\nciently achieves this requirement: it leads to the simplifica-\ntion of curve complexity with no peak rounding effects and \nno dislocation of relevant features. The curve evolution pro-\ncess is guided by a relevance measure K, which is stable \nwith respect to noisy deformations, and is given as: \n \n Ki = |(ai - ai - 1) \u00b7 (ti - ti - 1)| + |(ai + 1 - ai) \u00b7 (ti + 1 - ti)|, \n \nwhere ai are motion activity values at time indices ti at the \nparticular DCE simplification stage. Initial values of ai cor-\nrespond to \u0006i, i.e. to the motion activity metrics for each \nanalysed frame. The relevance measure Ki is proportional to \na change of area below the motion activity curve caused by \nthe removal of the point i on the curve. At each stage of \nsimplification the relevance measure Ki is iteratively up-\ndated. The optimal complexity level of the temporal descrip-\ntor is calculated as:  \n \nKopt = K  \u2013 log(\u0002) \u0003 \u0003K \n \nwhere K is the mean of all K at different simplification \nstages, \u0003K is its standard deviation, while the parameter \u0002 \ncontrols the sensitivity of the event detection and is driven \nby the application requirements. Key-frame positions are \ndetermined by the local minima in the temporal descriptor at \nthe scale where K equals Kopt. Being located at the local \ntroughs of motion activity, the key-frames will have maxi-\nmum probability of avoiding motion blur and other artefacts \ndue to object motion or camera work. In addition, the most \nrepresentative information will be conveyed by the key-\nframes in areas with no camera work, since the cameraman \ntends to focus on the main object of interest using a static \ncamera. In case the level of detail required cannot be \nachieved by using DCE simplification, i.e. initial cost of \nDCE simplification is too high, algorithm switches to a \nhigher layer of scalable video and generates more detailed \nmetric for a given section of video. \n \n3. GENERATION OF VIDEO SUMMARY \n \nHaving the user's experience at the centre of our browsing \ninterface design task, our main aim is to generate an intui-\ntive video summary by conveying the significance of a shot \nfrom analysed videos via the size of its key-frame represen-\ntation. In our case, the objective is to clearly present visual \ncontent that is dominant throughout the analysed section of \nthe video, as well as to highlight some unanticipated con-\ntent. \nFollowing the approach described above, where the size \nof a key-frame represents its summarisation significance, a \ncost function C(i) that represent the desired frame size in the \nfinal layout is generated, where C(i) \u0002 [0, 1] for i = 1,..., N,  \nand N is the number of extracted key-frames for a given \nsequence. In order to evaluate the cost function in a way that \nwill support the user\u2019s visual experience of the final layout, \nthe clustering based on perceptual similarity is used. More \nspecifically, the self-tuning K-way spectral clustering ap-\nproach [9] that utilises 18 \u0001 3 \u0001 3 HSV colour histogram is \napplied. This approach enables unsupervised analysis of \ninherent structure of the key-frame data and it copes well \nwith non-linearity of cluster shapes. \nIn order to estimate the number of clusters, an efficient \nand robust approach presented in [10] is used. It follows the \nidea that in an ideal case where the eigenvalues of the af-\nfinity matrix converge towards either 1 (high values) or 0 \n(low values), the size of high-value group gives a good es-\ntimate of number of clusters. In a real case, convergence to \nthose extreme values will deteriorate, but there will be two \nopposite tendencies and thus two groups in the eigenvalue \n2517\nAuthorized licensed use limited to: University of Surrey. Downloaded on April 23,2010 at 15:53:02 UTC from IEEE Xplore.  Restrictions apply. \nset. Thus, the K-means clustering method is applied on \nsorted eigenvalues, where the number of clusters is two and \ninitial locations of cluster centres are set to 1 for high-value \ncluster and 0 to low-value cluster. After clustering, the size \nof a high-value cluster gives a reliable estimate of number of \nclusters in analysed dataset. \nHaving grouped the key-frames into the defined number \nof clusters, the maximum cost function C(i) = hmax is as-\nsigned to the key-frame closest to the centre of the corres-\nponding cluster, thus representing the dominant content in \nthe selected section of video. Other key-frames get assigned \na cost as follows: \n \n( )\n( )\n2\nmax2\n1 exp\n2\ni\nd i\nC i h\u0002\n\u0003\n\u0005 \u0007\u0005 \u0007\n\t \n= \u0004 \u0001 \u0001 \u0004\t \n\t \n\t \n\u0004\u0006 \b\u0006 \b\n \n \nwhere d(i) is the i\nth\n frame's distance to the central frame and \n\u0003i is the variance of the cluster. The parameter \u0001 controls the \nbalance between the importance of the cluster centre and its \noutliers, and it is set empirically to 0.7. \nThe main task of the layout module is to generate a \nvisual summary that optimally follows the values of the cost \nfunction by using only frame sizes available in comic-like \npanel templates. Precision of this approximation depends \nupon the maximum height of a panel hmax that gives granu-\nlarity of the solution. For a given hmax, a set of panel tem-\nplates is generated, assigning a vector of frame sizes to each \ntemplate. The templates follow the narrative structure of a \ncomic book, while maintaining the original aspect ratio of \nimages forming the panel [11]. \nSince the aim here is to optimally utilise the available \nspace given the required sizes of images, this is a problem of \ndiscrete optimisation. However, unlike thoroughly explored \ndiscrete optimisation methods like stock cutting or bin pack-\ning [12], there is a non-linear transformation layer of panel \ntemplates between the error function and available re-\nsources. Therefore, a sub-optimal solution using dynamic \nprogramming is proposed. It follows a typical structure of \nthe DP algorithm by efficiently finding the solution to an \noptimisation problem in case the variables in the evaluation \nfunction are not interrelated simultaneously. Although there \nis an indirect dependency between non-adjacent panels due \nto the fact that the width of the last panel is directly depend-\nent upon the sum of widths of previously used panels, by \nintroducing specific corrections to the DP error function \n[10] the sub-optimal solution often achieves optimal results. \nThis correction assigns additional cost if the layout needs \nresizing in order to fit to the required width. \n \n4. EXPERIMENTAL RESULTS \n \nThe results presented in this section follow the steps of the \nproposed framework for video analysis. In the conducted \nexperiments the TRECVID 2006 evaluation content has \nbeen used. This content is provided by NIST as the bench-\nmarking material for evaluation of video retrieval systems. \nThe videos from provided set were transcoded to scalable \nvideo format in order to achieve flexible representation \nneeded for fast adaptation. For proposed video analysis the \nscalable representation enables easy access to different \nscales of motion information.  \nThe videos were encoded in 5 temporal layers using 4 \nlevels of temporal filtering. Therefore the motion informa-\ntion associated to the lowest temporal level corresponds to \nevery 16\nth\n (2\n4\n) frame. Motion activity metrics computed \nfrom this lowest layer are used to initialise DCE algorithm \nthat selects the key-frames. Low complexity of key-frame \nextraction has been obtained using entropy decoding of mo-\ntion information from the selected layers needed for compu-\ntation of initial activity metrics and DCE refinement.  \nThe results depicted in the Fig. 1 represent a scale space \nat four stages of the DCE simplification process, applied to \nthe motion activity metric \u00060 which corresponds to the video \nbit-stream layer with the lowest available frame-rate (top of \nFig. 1). The frame numbers in Fig. 1 are related to the origi-\nnal sequence while the number of actual samples of the mo-\ntion activity metric is 16 times lower. \nGradual removal of less important features of the metric \ncurve is performed using DCE, Fig. 1, where i in a\ni\n denotes \nthe DCE simplification stage. The bottom curve a\n3\n from Fig. \n1 is used in key-frame selection where the indices of minima \ndefine key-frames. The key-frames are then used for model-\nling of a summary. \n \n \nFig. 1. Scale space of the temporal descriptor generated by \nthe DCE algorithm for the sequence summaries in Fig. 2. \nThe final layouts obtained from layered bit-streams \nusing the proposed layout forming method are presented in \nFig. 2. The results are obtained from the same set of key-\nframes targeting different heights of panels. It can be seen \nthat the spatial compression of the layouts depend on selec-\nted height since larger hmax allows for more compact repre-\nsentations. Without loosing the notion of temporal structure \nas well as representing every detail of the content, the resul-\nting video summary achieves spatial compression ration of \n2 \/ 5 while producing visually pleasant experience for the \nuser. The layout algorithm complexity is evaluated by com-\nparing the processing speed with the methods that utilise \n2518\nAuthorized licensed use limited to: University of Surrey. Downloaded on April 23,2010 at 15:53:02 UTC from IEEE Xplore.  Restrictions apply. \ncomic-like narrative structure in video summarisation, \nwhich are presented in [2] and [3]. Firstly, both methods \nhave been proved unfeasible for summaries with more than \n100 key-frames. The layout processing times for of the al-\ngorithms presented in [2] TOR and in [3] TFS, compared to \nthe proposed method TDP are numerically given in Table 1. \nFrom the results shown it can be observed that the utilised \nmethod achieves linear complexity and thus proves to be \nsuitable for fast summarisation and responsive browsing \ninterfaces.  \nTable 1. Layout speeds for a given number of key-frames N. \nN 25 75 125 150 1000 2500 \nTOR 0.03 0.16 1.8 X X X \nTFS 0.03 0.57 200 X X X \nTDP 0.04 0.13 0.32 0.44 1.07 4.20 \n\u0001\n5. CONCLUSIONS \n \nThis paper introduces an efficient and scalable approach to \nlarge-scale video summarisation and browsing. By utilising \nlayered bit-stream structure and compressed domain motion \nfeatures, coupled by a fast and robust curve simplification \nalgorithm, the system efficiently extracts the most represen-\ntative set of key-frames. Exploiting the narrative structure of \ncomics and using its well-known intuitive rules, visual \nsummaries are generated in a user centred way. Given the \nspatial constraints of the display, a fast layout algorithm \ngenerates a comic-like summary of the video sequence using \na sub-optimal discrete optimisation method. Not only does \nthis approach improves the processing time of the summari-\nsation task, but it enables new functionalities of visualisa-\ntion for large-scale video archives, such as real-time interac-\ntion and relevance feedback. In addition, a set of high-level \nrules of comics\u2019 grammar can be exploited to improve rep-\nresentation of time to further improve compactness of gen-\nerated summary. \n6. REFERENCES \n \n[1] R. Dony, J. Mateer, J. Robinson, \"Techniques for automated \nreverse storyboarding,\" IEE Proc. Vision, Image and Signal \nProcessing, Vol. 152, No. 4, pp. 425 - 436, 2005. \n[2] S. Uchihashi, J. Foote, A. Girgensohn, J. Boreczky, \"Video \nmanga: generating semantically meaningful video sum-\nmaries,\" Proc. 7th ACM Int'l Conference on Multimedia, pp. \n383 - 392, 1999. \n[3] A. Girgensohn, \"A fast layout algorithm for visual video sum-\nmaries,\" Proc Int'l Conference on Multimedia and Expo, Vol. \n2, pp. 77 - 80, 2003. \n[4] Video Mining, A Rosenfeld, D. Doermann, D. DeMenthon \n(Editors), Kluwer Academic Publishers, 2003. \n[5] N. Adami, A. Signoroni, R. Leonardi, \"State-of-the-art and \ntrends in scalable video compression with wavelet-based ap-\nproaches,\" IEEE Trans. on Circ. and Sys. for Video Tech., \nVol. 17, Iss. 9, pp. 1238 - 1255, Sept. 2007. \n[6] M. Mrak, N. Sprljan, E. Izquierdo, \"Motion estimation in \ntemporal subbands for quality scalable motion coding,\" Elec-\ntronics Letters, No. 41, pp. 1050 \u2013 1051, 2005. \n[7] H. Schwarz, D. Marpe, T. Wiegand, \"Overview of the scalable \nvideo coding extension of the H.264\/AVC standard,\" IEEE \nTrans. on Circ. and Sys. for Video Tech., Vol. 17, Iss. 9, pp. \n1103 - 1120, Sept. 2007. \n[8] L. J. Latecki, R. Lakamper, \"Convexity rule for shape decom-\nposition based on discrete contour evolution,\" Computer Vi-\nsion and Image Understanding, Vol. 73, pp. 441\u2013454, 1999. \n[9] L. Zelnik-Manor, P. Perona, \u201cSelf-tuning spectral clustering\u201d, \nProc. Adv. Neural Inf. Process. Syst., 2004. \n[10] J. Calic, D. P. Gibson, and N. W. Campbell, \"Efficient layout \nof comic-like video summaries,\" IEEE Trans. on Circ. and \nSys. for Video Tech., Vol. 17, Iss. 7, pp. 931 - 936, July 2007.  \n[11] J. Calic and N. W. Campbell, \"Compact Visualisation of \nVideo Summaries,\" EURASIP Journal on Advances in Signal \nProcessing, vol. 2007, Article ID 19496, 2007. \n[12] A. Lodi, S. Martello, M. Monaci, \u201cTwo-dimensional packing \nproblems: A survey,\" European Journal of Operational Re-\nsearch, Vol. 141, Iss. 2, pp. 241 - 252, 2002. \n \n \n \n \n \nhmax = 1 \nhmax = 2 \nhmax = 3 \nhmax = 4 \na) Resulting layouts using panels with different heights \n \nb) Enlarged layout from hmax = 3 \nFig. 2. Resulting video summaries for different panel heights. \n2519\nAuthorized licensed use limited to: University of Surrey. Downloaded on April 23,2010 at 15:53:02 UTC from IEEE Xplore.  Restrictions apply. \n"}