{"doi":"10.1016\/j.jprocont.2010.07.007","coreId":"140494","oai":"oai:dspace.lib.cranfield.ac.uk:1826\/4783","identifiers":["oai:dspace.lib.cranfield.ac.uk:1826\/4783","10.1016\/j.jprocont.2010.07.007"],"title":"A branch and bound method for isolation of faulty variables through missing\nvariable analysis","authors":["Kariwala, Vinay","Odiowei, P. E.","Cao, Yi","Chen, T."],"enrichments":{"references":[{"id":37941351,"title":"A branch and bound algorithm for feature subset selection.","authors":[],"date":"1977","doi":"10.1109\/tc.1977.1674939","raw":"P. Narendra and K. Fukunaga. A branch and bound algorithm for feature subset selection. IEEE Trans. Comput., C-26:917{922, 1977.","cites":null},{"id":37941352,"title":"A more ecient branch and bound algorithm for feature selection. Pattern Recognition,","authors":[],"date":"1993","doi":"10.1016\/0031-3203(93)90054-z","raw":"B. Yu and B. Yuan. A more ecient branch and bound algorithm for feature selection. Pattern Recognition, 26:883{889, 1993.","cites":null},{"id":37941329,"title":"A review of process fault detection and diagnosis Part I: Quantitative model-based methods. Computers and chemical engineering,","authors":[],"date":"2003","doi":"10.1016\/s0098-1354(02)00161-8","raw":"V. Venkatasubramanian, R. Rengaswamy, K. Yin, and S.N. Kavuri. A review of process fault detection and diagnosis Part I: Quantitative model-based methods. Computers and chemical engineering, 27(3):293{311, 2003.","cites":null},{"id":37941332,"title":"A review of process fault detection and diagnosis Part III: Process history based methods.","authors":[],"date":"2003","doi":"10.1016\/s0098-1354(02)00161-8","raw":"V. Venkatasubramanian, R. Rengaswamy, S.N. Kavuri, and K. Yin. A review of process fault detection and diagnosis Part III: Process history based methods. Computers and Chemical Engineering, 27(3):327{346, 2003.","cites":null},{"id":37941354,"title":"An improved branch and bound algorithm for feature selection.","authors":[],"date":"2003","doi":"10.1016\/s0167-8655(03)00020-5","raw":"X.-W. Chen. An improved branch and bound algorithm for feature selection. Pattern Recognition Letters, 24:1925{1933, 2003.","cites":null},{"id":37941356,"title":"Bidirectional branch and bound for controlled variable selection: Part I. Principles and minimum singular value criterion.","authors":[],"date":"2008","doi":"10.1016\/j.compchemeng.2007.11.011","raw":"Y. Cao and V. Kariwala. Bidirectional branch and bound for controlled variable selection: Part I. Principles and minimum singular value criterion. Comput. Chem. Engng., 32(10):2306{2319, 2008.","cites":null},{"id":37941357,"title":"Bidirectional branch and bound for controlled variable selection: Part II. Exact local method for self-optimizing control.","authors":[],"date":"2009","doi":"10.1016\/j.compchemeng.2009.01.014","raw":"V. Kariwala and Y. Cao. Bidirectional branch and bound for controlled variable selection: Part II. Exact local method for self-optimizing control. Comput. Chem. Eng., 33:1402{1412, 2009.","cites":null},{"id":37941358,"title":"Bidirectional branch and bound for controlled variable selection: Part III. Local average loss minimization.","authors":[],"date":"2010","doi":"10.1109\/tii.2009.2037494","raw":"V. Kariwala and Y. Cao. Bidirectional branch and bound for controlled variable selection: Part III. Local average loss minimization. IEEE Transactions on Industrial Informatics, to appear, 2010.","cites":null},{"id":37941342,"title":"Contribution plots: a missing link in multivariate quality control.","authors":[],"date":"1998","doi":null,"raw":"P. Miller, R. E. Swanson, and C. F. Heckler. Contribution plots: a missing link in multivariate quality control. International Journal of Applied Mathematics and Computer Science, 8:775{ 792, 1998.","cites":null},{"id":37941353,"title":"Fast branch and bound algorithm in feature selection.","authors":[],"date":"2000","doi":"10.1109\/tpami.2004.28","raw":"P. Somol, P. Pudil, F. Ferri, and J. Kittler. Fast branch and bound algorithm in feature selection. In B. Sanchez, J. Pineda, J. Wolfmann, Z. Bellahsense, and F. Ferri, editors, Proceedings of World Multiconference on Systemics, Cybernetics and Informatics, volume VII, pages 1646{651, Orlando, Florida, USA, 2000.","cites":null},{"id":37941349,"title":"Fault detection based on a maximum-likelihood principal component analysis (PCA) mixture.","authors":[],"date":"2005","doi":"10.1021\/ie049081o","raw":"S. W. Choi, E. B. Martin, and A. J. Morris. Fault detection based on a maximum-likelihood principal component analysis (PCA) mixture. Industrial and Engineering Chemistry Research, 44:2316{2327, 2005. 23[21] J. J. Downs and E. F. Vogel. A plant-wide industrial process control problem. Comput. Chem. Eng., 17(3):245{255, 1993.","cites":null},{"id":37941355,"title":"Improved branch and bound method for control structure screening.","authors":[],"date":"2005","doi":"10.1016\/j.ces.2004.10.025","raw":"Y. Cao and P. Saha. Improved branch and bound method for control structure screening. Chem. Engg. Sci., 60(6):1555{1564, 2005.","cites":null},{"id":37941334,"title":"Improved principal component monitoring of large-scale processes.","authors":[],"date":"2004","doi":"10.1016\/j.jprocont.2004.02.002","raw":"U. Kruger, Y. Zhou, and G.W. Irwin. Improved principal component monitoring of large-scale processes. Journal of Process Control, 14(8):879{888, 2004.","cites":null},{"id":37941360,"title":"Matrix Analysis.","authors":[],"date":"1985","doi":"10.1017\/cbo9780511810817","raw":"R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, Cambridge, UK, 1985.","cites":null},{"id":37941359,"title":"Matrix Computations. The Johns Hopkins","authors":[],"date":"1993","doi":null,"raw":"G. H. Golub and C. F. van Loan. Matrix Computations. The Johns Hopkins University Press, Baltimore, MD, 3rd edition, 1993.","cites":null},{"id":37941341,"title":"Multimode process monitoring with Bayesian inference-based  Gaussian mixture models.","authors":[],"date":"2008","doi":"10.1002\/aic.11515","raw":"J. Yu and S. J. Qin. Multimode process monitoring with Bayesian inference-based nite Gaussian mixture models. AIChE Journal, 54:1811{1829, 2008.","cites":null},{"id":37941340,"title":"Multivariate statistical process control using mixture modelling.","authors":[],"date":"2005","doi":"10.1002\/cem.903","raw":"U. Thissen, H. Swierenga, A. P. de Weijer, R. Wehrens, W. J. Melssen, and L. M. C. Buydens. Multivariate statistical process control using mixture modelling. Journal of Chemometrics, 19: 23{31, 2005.","cites":null},{"id":37941336,"title":"Nonlinear dynamic process monitoring using canonical variate analysis and kernel density estimations.","authors":[],"date":"2010","doi":"10.1109\/tii.2009.2032654","raw":"P.P. Odiowei and Y. Cao. Nonlinear dynamic process monitoring using canonical variate analysis and kernel density estimations. IEEE Transactions on Industrial Informatics, 6(1):36{45, 2010.","cites":null},{"id":37941333,"title":"On-line dynamic process monitoring using wavelet-based generic dissimilarity measure.","authors":[],"date":"2005","doi":"10.1205\/cherd.04370","raw":"SI Alabi, AJ Morris, and EB Martin. On-line dynamic process monitoring using wavelet-based generic dissimilarity measure. Chemical Engineering Research and Design, 83(6):698{705, 2005.","cites":null},{"id":37941350,"title":"Principal Component Analysis.","authors":[],"date":"2002","doi":"10.1007\/0-387-22440-8_7","raw":"I. T. Jollie. Principal Component Analysis. Springer, 2nd edition, 2002.","cites":null},{"id":37941345,"title":"Probabilistic contribution analysis for statistical process monitoring: A missing variable approach.","authors":[],"date":"2009","doi":"10.1016\/j.conengprac.2008.09.005","raw":"T. Chen and Y. Sun. Probabilistic contribution analysis for statistical process monitoring: A missing variable approach. Control Engineering Practice, 17(4):469{477, 2009.","cites":null},{"id":37941346,"title":"Probabilistic principal component analysis.","authors":[],"date":"1999","doi":"10.1111\/1467-9868.00196","raw":"M. E. Tipping and C. M. Bishop. Probabilistic principal component analysis. Journal of the Royal Statistical Society B, 61:611{622, 1999.","cites":null},{"id":37941338,"title":"Probability density estimation via an in Gaussian mixture model: application to statistical process monitoring.","authors":[],"date":"2006","doi":"10.1111\/j.1467-9876.2006.00560.x","raw":"T. Chen, J. Morris, and E. Martin. Probability density estimation via an innite Gaussian mixture model: application to statistical process monitoring. Journal of the Royal Statistical Society C (Applied Statistics), 55:699{715, 2006.","cites":null},{"id":37941348,"title":"Process monitoring based on probabilistic PCA. Chemometrics and intelligent laboratory systems,","authors":[],"date":"2003","doi":"10.1016\/s0169-7439(03)00063-7","raw":"D. Kim and I.-B. Lee. Process monitoring based on probabilistic PCA. Chemometrics and intelligent laboratory systems, 67:109{123, 2003.","cites":null},{"id":37941339,"title":"Process monitoring using a Gaussian mixture model via principal component analysis and discriminant analysis.","authors":[],"date":"2004","doi":"10.1016\/j.compchemeng.2003.09.031","raw":"S. W. Choi, J. H. Park, and I.-B. Lee. Process monitoring using a Gaussian mixture model via principal component analysis and discriminant analysis. Computers and Chemical Engineering, 28:1377{1387, 2004.","cites":null},{"id":37941343,"title":"Reconstruction based fault identi using a combined index.","authors":[],"date":"2001","doi":"10.1021\/ie000141+","raw":"H.H. Yue and S.J. Qin. Reconstruction based fault identication using a combined index. Industrial and Engineering Chemistry Research, 40:4403{4414, 2001.","cites":null},{"id":37941337,"title":"State-space independent component analysis for nonlinear dynamic process monitoring. Chemometrics and Intelligent Laboratory Systems,","authors":[],"date":"2010","doi":"10.1016\/j.chemolab.2010.05.014","raw":"P.P. Odiowei and Y. Cao. State-space independent component analysis for nonlinear dynamic process monitoring. Chemometrics and Intelligent Laboratory Systems, In Press, Corrected Proof, 2010. ISSN 0169-7439. doi: DOI: 10.1016\/j.chemolab.2010.05.014. 22[9] M. N. Nounou, B. R. Bakshi, P. K. Goel, and X. Shen. Bayesian principal component analysis. Journal of Chemometrics, 16:576{595, 2002.","cites":null},{"id":37941335,"title":"Statistical monitoring of dynamic processes based on dynamic independent component analysis.","authors":[],"date":"2004","doi":"10.1016\/j.ces.2004.04.031","raw":"J.M. Lee, C.K. Yoo, and I.B. Lee. Statistical monitoring of dynamic processes based on dynamic independent component analysis. Chemical engineering science, 59(14):2995{3006, 2004.","cites":null},{"id":37941330,"title":"Statistical process monitoring: basics and beyond.","authors":[],"date":"2003","doi":"10.1002\/cem.800","raw":"S. Joe Qin. Statistical process monitoring: basics and beyond. Journal of Chemometrics, 17 (8-9):480{502, 2003.","cites":null},{"id":37941344,"title":"Synthesis of T2 and Q statistics for process monitoring. Control Engineering Practice,","authors":[],"date":"2004","doi":"10.1016\/j.conengprac.2003.08.004","raw":"Q. Chen, U. Kruger, M. Meronk, and A. Y. T. Leung. Synthesis of T2 and Q statistics for process monitoring. Control Engineering Practice, 12:745{755, 2004.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-12-31T00:00:00Z","abstract":"Fault detection and diagnosis is a critical approach to ensure safe and\nefficient operation of manufacturing and chemical processing plants. Although\nmultivariate statistical process monitoring has received considerable attention,\ninvestigation into the diagnosis of the source or cause of the detected process\nfault has been relatively limited. This is partially due to the difficulty in\nisolating multiple variables, which jointly contribute to the occurrence of\nfault, through conventional contribution analysis. In this work, a method based\non probabilistic principal component analysis is proposed for fault isolation.\nFurthermore, a branch and bound method is developed to handle the combinatorial\nnature of problem involving finding the contributing variables, which are most\nlikely to be responsible for the occurrence of fault. The efficiency of the\nmethod proposed is shown through benchmark examples, such as Tennessee Eastman\nprocess, and randomly generated cases","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/140494.pdf","fullTextIdentifier":"http:\/\/dx.doi.org\/10.1016\/j.jprocont.2010.07.007","pdfHashValue":"813c7bf901b5f864a99a5a4813a8901822324cf4","publisher":"Elsevier Science B.V., Amsterdam.","rawRecordXml":"<record><header><identifier>\noai:dspace.lib.cranfield.ac.uk:1826\/4783<\/identifier><datestamp>2012-03-09T09:24:06Z<\/datestamp><setSpec>hdl_1826_19<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>A branch and bound method for isolation of faulty variables through missing\nvariable analysis<\/dc:title><dc:creator>Kariwala, Vinay<\/dc:creator><dc:creator>Odiowei, P. E.<\/dc:creator><dc:creator>Cao, Yi<\/dc:creator><dc:creator>Chen, T.<\/dc:creator><dc:subject>Branch and bound Combinatorial optimization Global optimization Multivariate contribution analysis Multivariate statistical process monitoring Principal component analysis principal component analysis gaussian mixture model feature-selection part i algorithm pca<\/dc:subject><dc:description>Fault detection and diagnosis is a critical approach to ensure safe and\nefficient operation of manufacturing and chemical processing plants. Although\nmultivariate statistical process monitoring has received considerable attention,\ninvestigation into the diagnosis of the source or cause of the detected process\nfault has been relatively limited. This is partially due to the difficulty in\nisolating multiple variables, which jointly contribute to the occurrence of\nfault, through conventional contribution analysis. In this work, a method based\non probabilistic principal component analysis is proposed for fault isolation.\nFurthermore, a branch and bound method is developed to handle the combinatorial\nnature of problem involving finding the contributing variables, which are most\nlikely to be responsible for the occurrence of fault. The efficiency of the\nmethod proposed is shown through benchmark examples, such as Tennessee Eastman\nprocess, and randomly generated cases.<\/dc:description><dc:publisher>Elsevier Science B.V., Amsterdam.<\/dc:publisher><dc:date>2011-09-08T10:06:55Z<\/dc:date><dc:date>2011-09-08T10:06:55Z<\/dc:date><dc:date>2010-12-31T00:00:00Z<\/dc:date><dc:type>Article<\/dc:type><dc:identifier>0959-1524<\/dc:identifier><dc:identifier>http:\/\/dx.doi.org\/10.1016\/j.jprocont.2010.07.007<\/dc:identifier><dc:identifier>http:\/\/dspace.lib.cranfield.ac.uk\/handle\/1826\/4783<\/dc:identifier><dc:language>en_UK<\/dc:language><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["issn:0959-1524","0959-1524"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2010,"topics":["Branch and bound Combinatorial optimization Global optimization Multivariate contribution analysis Multivariate statistical process monitoring Principal component analysis principal component analysis gaussian mixture model feature-selection part i algorithm pca"],"subject":["Article"],"fullText":"A Branch and Bound Method for Isolation of Faulty Variables\nthrough Missing Variable Analysis\nVinay Kariwala\u2020, Pabara-Ebiere Odiowei\u2021, Yi Cao\u2021\u2217and Tao Chen\u2020\n\u2020 School of Chemical and Biomedical Engineering,\nNanyang Technological University, Singapore 637459, Singapore\n\u2021School of Engineering, Cranfield University, Cranfield, Bedford MK43 0AL, UK\nThis version: June 17, 2010\nAbstract\nFault detection and diagnosis is a critical approach to ensure safe and efficient operation of\nmanufacturing and chemical processing plants. Although multivariate statistical process moni-\ntoring has received considerable attention, investigation into the diagnosis of the source or cause\nof the detected process fault has been relatively limited. This is partially due to the difficulty in\nisolating multiple variables, which jointly contribute to the occurrence of fault, through conven-\ntional contribution analysis. In this work, a method based on probabilistic principal component\nanalysis is proposed for fault isolation. Furthermore, a branch and bound method is developed\nto handle the combinatorial nature of problem involving finding the contributing variables, which\nare most likely to be responsible for the occurrence of fault. The efficiency of the method pro-\nposed is shown through benchmark examples, such as Tennessee Eastman process, and randomly\ngenerated cases.\nKeywords: Branch and bound, combinatorial optimization, global optimization, multivariate\ncontribution analysis, multivariate statistical process monitoring, principal component analysis.\n\u2217Corresponding Author: Tel: +44-1234-750111; Fax: +44-1234-754685; E-mail:y.cao@cranfield.ac.uk\n1\n1 Introduction\nFault detection and diagnosis (FDD) is a critical approach to ensure safe and efficient operation\nof manufacturing and chemical processing plants. The available current FDD techniques can be\nclassified into the following three major categories: quantitative model-based, qualitative model-\nbased and process history based methods [1]. Among these techniques, multivariate statistical\nprocess monitoring (MSPM), which is based on historical process data, has received considerable\nattention in terms of both methodological research and industrial applications [2, 3]. The success\nof MSPM may be attributed to the fact that a large amount of historical process data is usually\navailable, and thus the developed model attains high accuracy in detecting any deviation from\nestablished normal operating conditions (NOC).\nIn traditional MSPM, the principal component analysis (PCA) or the partial least squares (PLS) are\napplied to model the data collected under NOC. Subsequently, monitoring statistics like Hotelling\u2019s\nT 2 and the squared prediction error (SPE) are used for fault detection. During the past few years, the\nfocus of the research in MSPM has been on developing sophisticated statistical models to obtain more\nrealistic representation of the process behaviour. Some notable progress has been made in dealing\nwith dynamic processes [4\u20138] and process with non-Gaussian distributed data [8\u201313]. However,\ninvestigation into the \u201cdownstream\u201d step of MSPM, that is, the diagnosis of the source or cause of the\ndetected fault, has been relatively limited. The primary tool used for fault diagnosis is contribution\nanalysis, which quantifies the contribution of individual variables to the T 2 and SPE [14]. Since both\nthe T 2 and SPE are monitored in conventional MSPM, contribution analysis is typically performed\nfor these two statistics. Thus, if the two contribution plots indicate different sets of responsible\nfaulty variables, it can be difficult to resolve these conflicts and the decision is usually subject to\nthe operators\u2019 experience. Furthermore, contribution analysis investigates individual variables one\nby one. Thus, it can be ineffective in isolating multiple variables which jointly contribute to the\noccurrence of fault and can lead to the identification of incorrect variables as being faulty due to the\ncorrelation among variables.\nMore recently, the need to use a unified monitoring statistic, as opposed to using the T 2 and SPE\nindividually, has been well recognized. One way is to combine the T 2 and SPE algorithmically [15,\n16]; the other approach is to rely on a fully probabilistic model to provide a single likelihood-based\nstatistic [10, 17]. A single monitoring statistic offers a clearer interpretation of the contribution plots.\nIn addition, these studies suggested an alternative approach to quantifying the contribution by using\n2\nthe idea of missing variables [15, 17]. The original idea is that each process variable is treated as\nif it were missing and the monitoring statistic is re-calculated. This procedure is carried out for all\nvariables and the variable giving the largest reduction of the monitoring statistic is considered to\ncontribute the most to the fault. The missing variable approach opens up the opportunity to analyze\nthe joint effect of a group of variables by treating them as missing. However, the major difficulty is\nthe vast number of possible variable combinations that are required to be evaluated. For example,\nif 5 out of total 50 variables are suspected to be the contributing source, then the total number\nof combinations is 2, 118, 760. Therefore, Yue and Qin [15] made a restrictive assumption that the\ncombinations of faulty variables are specified a priori according to known faulty modes, and thus\ncontribution analysis only needs to be conducted on these combinations.\nThis paper extends the missing-variable based contribution analysis to consider the joint effect of\nmultiple variables, namely multivariate contribution analysis. In this study, we choose the proba-\nbilistic PCA (PPCA) [18, 19] for modelling normal operating data and on-line process monitoring.\nThe probabilistic framework of PPCA provides a single monitoring statistic, which avoids possible\nconflict in fault isolation stage if multiple statistics need to be analyzed. If needed, the PPCA can\nbe extended to a mixture model to suit non-Gaussian distributed process data [20].\nBased on the PPCA under NOC, we derive a statistical criterion to quantify the contribution of\nmultiple missing variables. Using the criterion, when the occurrence of fault is identified through\non-line process monitoring, the fault diagnosis can then be conducted by solving a series of subset\nselection problems. The combinatorial difficulty of the subset selection problem is tackled by a nu-\nmerically efficient branch and bound (BAB) algorithm developed to isolate the set of faulty variables\nbased on the criterion derived. The efficiency of the proposed method is demonstrated using a linear\nbenchmark example [15], the Tennessee Eastman (TE) process [21] and randomly generated cases.\nIt is worth to point out that although the proposed algorithm is developed based on the PPCA, the\nmethodology is readily extendable to other MSPM methods.\nThe rest of this paper is organised as follows: after introducing the general principle of the PPCA,\nthe methodology of missing variable analysis for fault isolation is presented in Section 2. Section 3\nproposes the BAB algorithm for missing variable selection. The efficiency and effectiveness of the\ndeveloped algorithms are demonstrated through various numerical examples in Section 4. Finally,\nthe work is concluded in Section 5.\n3\n2 Probabilistic principal component analysis\nPrincipal component analysis (PCA) [22] is a general multivariate statistical projection technique\nfor dimension reduction. The central idea of PCA is to project the original r-dimensional data, x,\nonto a space where the variance is maximized:\nx = Wt+ x\u00af+ e (1)\nwhere W refers to the eigenvectors of the sample covariance matrix corresponding to the q (q \u2264 r)\nlargest eigenvalues, t is the q-dimensional scores, x\u00af is the mean of the data, and e is the noise term.\nRecently, Tipping and Bishop [18] proposed a probabilistic formulation of PCA (PPCA) from the\nperspective of a Gaussian latent variable model. Specifically the noise is assumed to be Gaussian:\ne \u223c G(0, \u03c32I), which implies\nx|t \u223c G(Wt+ x\u00af, \u03c32I) (2)\nFurthermore, by adopting a Gaussian distribution for the scores, t \u223c G(0, I), the marginal distribu-\ntion of the data is also Gaussian: x \u223c G(x\u00af,C), where the covariance matrix is C = WWT +\u03c32I. In\ncontrast to the sample covariance S, the PPCA model defines C in terms of the auxiliary parameters\nW and \u03c32. Note the number of free parameters in C is rq + 1 \u2212 q(q \u2212 1)\/2, which is smaller than\nthe r(r + 1)\/2 parameters in S if q < r [18]. Therefore the PPCA provides a way to constrain the\nmodel complexity via the selection of q. The model parameters, {x\u00af, W, \u03c32}, can be estimated using\nthe maximum likelihood algorithm; see [18] for details. Later, the PPCA was applied for process\nmonitoring with improved fault detection capability [19].\nThe probabilistic framework of PPCA provides a single statistic for fault detection, as opposed to\nthe T 2 and SPE in the traditional PCA. It was shown by Chen and Sun [17] that the data point x\nshould be considered as out-of-control when\nM2 = (x\u2212 x\u00af)TC\u22121(x\u2212 x\u00af) > \u03c72r(\u03b2) (3)\nwhere \u03c72r(\u03b2) is the \u03b2-fractile of the chi-square distribution with r degrees of freedom.\nFor the purpose of contribution analysis, Chen and Sun [17] suggested a missing variable approach\nin the PPCA framework. In particular, consider that the measurement vector x is partitioned into\nn-dimensional xo and d-dimensional xm as\nxT =\n[\nxTo x\nT\nm\n]\n(4)\n4\nwhere the subscripts o and m refer to observed and missing variables, respectively. Similarly, let the\nmean (x\u00af) and covariance matrix (C) be partitioned as\nx\u00af =\n\uf8ee\uf8f0 x\u00afo\nx\u00afm\n\uf8f9\uf8fb ; C =\n\uf8ee\uf8f0Coo Com\nCmo Cmm\n\uf8f9\uf8fb (5)\nThe conditional mean (x\u00afm|o) and covariance matrix (Cm|o) of xm given xo are\nx\u00afm|o = x\u00afm +CmoC\u22121oo (xo \u2212 x\u00afo) (6)\nCm|o = Cmm \u2212CmoC\u22121oo Com (7)\nThen, the conditional mean (x\u00af|o) and covariance matrix (C|o) of whole x given xo are\nx\u00af|o =\n\uf8ee\uf8f0 xo\nx\u00afm|o\n\uf8f9\uf8fb ; C|o =\n\uf8ee\uf8f00 0\n0 Cm|o\n\uf8f9\uf8fb (8)\nTherefore, the re-calculated monitoring statistic with missing values is given by\nE[M2] = tr(C\u22121((x\u00af|o \u2212 x\u00af)(x\u00af|o \u2212 x\u00af)T +C|o)) (9)\nwhere E[\u00b7] denotes the expectation operator.\nNow, the variable can be considered to be out-of-control if E[M2] in (9) exceeds \u03c72r(\u03b2). This criterion\nis applicable to any number of missing variables. In the original method of Chen and Sun [17], each\nindividual variable of x is regarded as missing, and the monitoring statistic in (9) is re-calculated.\nIf a variable contributes significantly to the data being detected as faulty, then the re-calculated\nstatistic will be dramatically reduced. Furthermore, if E[M2] is smaller than the confidence bound,\nthen we can say that by removing the corresponding variable and replacing it by its conditional\nmean, the process would return to the normal operating region. However, this approach is not\ncapable of studying the joint contribution of multiple variables.\nFrom the fault diagnosis point of view, the original source of a fault should correspond to a small\nset of measurements. Therefore, the objective becomes to select a minimum number of missing\nvariables, whose re-calculated monitoring statistic is below the confidence bound. The flow of the\nmethod is given by the following algorithm:\nAlgorithm 1 Initially set d = 1.\n5\n1. Select d missing variables such that the re-calculated monitoring statistic is minimized.\n2. If the minimum statistic is below the confidence bound, then the corresponding variables are\nisolated as the source of fault, and the algorithm can be terminated.\n3. Otherwise, set d = d+ 1, and return to Step 1.\nThe selection of d-dimensional xd from r-dimensional x in Step 1 of Algorithm 1 is a combinatorial\noptimization problem, which is NP-hard. One of the main contributions of this work is the devel-\nopment of a branch and bound (BAB) algorithm to solve this optimization problem efficiently, as\ndiscussed in the next section.\n3 Branch and bound method\n3.1 General principle\nLet Xr = {xi|i = 1, 2, \u00b7 \u00b7 \u00b7 , r}, be an r-element set. A subset selection problem with the selection\ncriterion \u03c6 involves finding the optimal solution, X\u2217n, such that\n\u03c6(X\u2217n) = min\nXn\u2282Xr\n\u03c6(Xn) (10)\nFor this problem, the number of alternatives is Cnr = r!(r\u2212n)!n! , which grows very quickly with r and\nn rendering exhaustive search unviable. A BAB approach can provide globally optimal solution\nfor the subset selection problem in (10) without exhaustive search. In this approach, the original\nproblem (node) is divided (branched) into several non-overlapping subproblems (sub-nodes). If any\nof the n-element solution of a sub-problem cannot lead to the optimal solution, the sub-problem is\nnot evaluated further (pruned), else it is branched again. The pruning of sub-problems allows the\nBAB approach to gain efficiency in comparison with exhaustive search.\nThe available BAB methods for subset selection can be classified as downwards [23\u201327] and up-\nwards [28\u201330] BAB methods. The distinguishing feature of these approaches is the search direction.\nWhile the subset size is gradually decreased from r to n in a downwards BAB approach, it is grad-\nually increased from 0 to n in the upwards BAB approach. The use of downwards (upwards) BAB\napproach is appropriate, when the lower bound on the selection criteria holds for subset sizes larger\n6\n(smaller) than the target subset size n [28]. As shown in Section 3.2, the lower bound on the moni-\ntoring statistic in (9) holds in the upwards direction and thus the upwards BAB approach is used in\nthis paper. In the subsequent discussion, we provide a brief overview of the upwards BAB approach;\nsee [28] for details.\nroot\n1 2 3 4 5\n665654654365432\nsearch\ndirection\nFigure 1: Solution tree for selecting 2 out of 6 elements\nIn an upwards BAB approach, each node has a fixed set Ff and a candidate set Cc, which have f\nand c elements, respectively. The relationship between the fixed and candidate sets of a node and\nits ith sub-node (branching rule) is given as follows:\nF if+1 = Ff \u222a xi; Cic\u2212i = Cc \\ {x1, \u00b7 \u00b7 \u00b7 , xi} (11)\nwhere F if+1 and C\ni\nc\u2212i denote the fixed and candidate sets of the i\nth sub-node and i = 1, 2, \u00b7 \u00b7 \u00b7 , f +\nc \u2212 n + 1. Based on (11), it can be noted that Ff is gradually expanded using the elements of Cc,\nuntil the dimension of Ff reaches the target subset size n. An example of the solution tree obtained\nby recursively applying the branching rule in (11) is shown in Figure 1. For the root node in this\nsolution tree, we have Ff = \u2205 and Cc = Xr. The label of the nodes denote the element being moved\nfrom Cc to Ff . The solution tree has Crn terminal nodes, which represent different n-element subsets\nof Xr.\nTo describe the pruning principle, let X denote the ensemble of all n-element subsets, which can be\nobtained by expanding Ff using (11), i.e.,\nX = {{Ff , Xn\u2212f}|Xn\u2212f \u2282 Cc} (12)\nand \u03c6(Ff ) be the lower bound on \u03c6 computed over all elements of X , i.e.\n\u03c6(Ff ) = min\nXn\u2208X\n\u03c6(Xn) (13)\nAssume that B is an upper bound of the globally optimal criterion, i.e. B \u2265 \u03c6(X\u2217n). Then,\n\u03c6(Xn) > \u03c6(X\u2217n) \u2200Xn \u2208 X , if \u03c6(Ff ) > B (14)\n7\nHence, any Xn \u2208 X cannot be optimal and can be pruned without further evaluation, if \u03c6(Ff ) > B.\nAlthough pruning of nodes using (14) results in an efficient BAB algorithm, further efficiency can\nbe gained by performing pruning on the sub-nodes directly. This happens as the lower bounds for\ndifferent sub-nodes are related and can be computed together from \u03c6(Ff ) resulting in computational\nefficiency. For xi \u2208 Cc, the ith sub-node can be pruned if\n\u03c6(Ff \u222a xi) > B (15)\nThe pruning of ith sub-node is conducted by discarding xi from Cc, such that xi is not included in\nthe fixed sets of any subsequent sub-nodes. If (15) is satisfied for multiple sub-nodes simultaneously,\nall these elements can be discarded from Cc together.\nFor a BAB method involving pruning of sub-nodes, branching needs to be carried on sub-node level\nas well, which requires choosing a decision element to branch upon. Here, the decision element is\nselected as the element with smallest \u03c6(Ff \u222axi) among all xi \u2208 Cc (best-first search). The branching\noperation can be conducted by moving xi from Cc to Ff such that xi is included in the fixed sets of\nany subsequent sub-nodes.\n3.2 Application to fault isolation problem\nTo apply the BAB approach for fault isolation purposes, we first express the monitoring statistic in\n(9) in a form that is more amenable to the application of BAB method. We have\nE[M2] = tr(C\u22121C|o) + (x\u00af|o \u2212 x\u00af)TC\u22121(x\u00af|o \u2212 x\u00af) (16)\nUsing (8), tr(C\u22121C|o) = tr([C\u22121]22C|o), where [C\u22121]22 denotes the (2,2) block of C\u22121 having same\ndimensions as C|o. Based on formula for inverse of partitioned matrices [31], [C\u22121]22 = (Cmm \u2212\nCmoC\u22121oo Com)\u22121 = C\n\u22121\nm|o. Therefore, tr(C\n\u22121C|o) = d, where d is the number of missing variables\ngiven in Algorithm 1.\nFor the second term of (16), note that we can write\nx\u00af|o \u2212 x\u00af =\n\uf8ee\uf8f0 xo \u2212 x\u00afo\nCmoC\u22121oo (xo \u2212 x\u00afo)\n\uf8f9\uf8fb =\n\uf8ee\uf8f0Coo\nCmo\n\uf8f9\uf8fbC\u22121oo (xo \u2212 x\u00afo)\n8\nand\n(x\u00af|o \u2212 x\u00af)TC\u22121(x\u00af|o \u2212 x\u00af) = (xo \u2212 x\u00afo)TC\u22121oo\n[\nCToo C\nT\nmo\n]\nC\u22121\n\uf8ee\uf8f0Coo\nCmo\n\uf8f9\uf8fbC\u22121oo (xo \u2212 x\u00afo)\nNote that\n[\nCToo C\nT\nmo\n]\ndenotes the first n = r\u2212 d rows of C, as Coo is symmetric and Com = CTmo.\nThus,\n(x\u00af|o \u2212 x\u00af)TC\u22121(x\u00af|o \u2212 x\u00af) = (xo \u2212 x\u00afo)TC\u22121oo\n[\nI 0\n]\uf8ee\uf8f0Coo\nCmo\n\uf8f9\uf8fbC\u22121oo (xo \u2212 x\u00afo)\n= (xo \u2212 x\u00afo)TC\u22121oo CooC\u22121oo (xo \u2212 x\u00afo)\n= (xo \u2212 x\u00afo)TC\u22121oo (xo \u2212 x\u00afo)\nFinally, the simplified criterion is given as\nE[M2] = (xo \u2212 x\u00afo)TC\u22121oo (xo \u2212 x\u00afo) + d (17)\nDuring the application of Algorithm 1, d = r \u2212 n is constant during every iteration. Thus, variable\nselection can be carried out by minimizing the first term in (17). Furthermore, x\u00afo can be subtracted\nin a preprocessing stage to simplify calculations. By defining y = x \u2212 x\u00af, the n observed variables\ncan be selected (equivalent to selecting d missing variables) by solving the following problem:\nmin\nXn\u2282Xr\n\u03c6(Xn) = yTXn(CXn,Xn)\n\u22121yXn (18)\nwhere Xr = {1, 2, \u00b7 \u00b7 \u00b7 , r}, yXn denotes the elements of y with indices in Xn and CXn,Xn represents\nthe principal submatrix of C with rows and columns indexed by Xn.\nThe use of BAB for solving the optimization problem in (18) requires a lower bound on the selection\ncriteria, calculated over all X in (12). This lower bound is derived in the next proposition.\nProposition 1 Consider a node with fixed set Ff and candidate set Cc. For X in (12),\n\u03c6(Ff ) \u2264 min\nXn\u2208X\n\u03c6(Xn) (19)\nProof : Let R and R\u02dc be the Cholesky factors of CXn,Xn for some Xn \u2208 X and CFf ,Ff , respectively,\ni.e. RTR = C and R\u02dcT R\u02dc = CFf ,Ff . As Ff \u2282 Xn, R\u02dc is a principal submatrix of R, which implies\nthat R\u02dc\u22121 is a principal submatrix of R\u22121. Define z = R\u2212TyXn . Then,\n\u03c6(Xn) = yTXnR\n\u22121R\u2212TyXn = z\nT z (20)\n9\nSimilarly,\n\u03c6(Ff ) = yTFf R\u02dc\n\u22121R\u02dc\u2212TyFf = z\u02dc\nT z\u02dc (21)\nwhere z\u02dc = R\u02dc\u2212TyFf . Since z\u02dc is a subset of z, \u03c6(Ff ) \u2264 \u03c6(Xn) and (19) follows.\nProposition 1 implies that the non-optimal nodes can be pruned based on \u03c6(Ff ). To gain further\nefficiency by pruning the sub-nodes directly, we relate the selection criteria of a node with its sub-\nnodes in the next proposition.\nProposition 2 Consider a node with fixed set Ff and candidate set Cc. For xi \u2208 Cc, i = 1, 2, \u00b7 \u00b7 \u00b7 , c,\n\u03c6(Ff \u222a i) = \u03c6(Ff ) + \u03b1i (22)\nwhere\n\u03b1i =\n(\nyi \u2212 yFfC\u22121Ff ,FfCFf ,i\n)2\n(Ci,i \u2212CTFf ,iC\u22121Ff ,FfCFf ,i)2\n(23)\nProof : Let Q be the Cholesky factor of C(Ff\u222ai),(Ff\u222ai), i.e. Q\nTQ = C(Ff\u222ai),(Ff\u222ai). Through simple\nalgebraic manipulations, it can be shown that\nQ =\n\uf8ee\uf8f0R pi\n0 \u03b4i\n\uf8f9\uf8fb (24)\nwhere R is the Cholesky factor of CFf ,Ff , pi = R\n\u2212TCFf ,i and \u03b4i =\n\u221a\nCi,i \u2212 pTi pi. Using the formula\nfor inversion of partitioned matrices [32], we get\nQ\u22121 =\n\uf8ee\uf8f0R\u22121 \u2212R\u22121pi\/\u03b4i\n0 1\/\u03b4i\n\uf8f9\uf8fb (25)\nSince, yFf\u222ai =\n[\nyFf yi\n]\n,\n\u03c6(Ff \u222a i) = yTFf\u222aiQ\u22121Q\u2212TyFf\u222ai (26)\n= yTFfR\n\u22121R\u2212TyFf +\n1\n\u03b42i\n(\nyi \u2212 yXsR\u22121pi\n)2 (27)\nNow, the result follows by noting that \u03c6(Ff ) = yTFfR\n\u22121R\u2212TyFf and substituting for pi and \u03b4i.\nThe evaluation of (22) requires inversion of only one matrix CFf ,Ff , which is the same for all i \u2208 Cc.\nThus, the use of (22) to obtain the selection criteria for all sub-nodes together is computationally\n10\nmore efficient than directly evaluating the selection criteria for every node. In summary, the following\nBAB algorithm can be used as Step 1 of Algorithm 1 for fault isolation purposes.\nAlgorithm 2 Initialize f = 0, Ff = \u2205, Cc = Xr, \u03c6(Ff ) = 0 and B = \u221e. Call the following\nrecursive algorithm:\n1. If \u03c6(Ff ) > B, prune the current node and return, else perform the following steps.\n2. Calculate \u03b1i in (23) \u2200i \u2208 Cc. Prune the subsets with \u03c6(Ff ) + \u03b1i > B.\n3. If f = n, go to next step. Otherwise, generate the c sub-nodes according to the branching rule\nin (11) and call the recursive algorithm in Step 1 for each sub-node. Return to the caller after\nthe execution of the loop finishes.\n4. Find Jmin = \u03c6(Ff ) + mini\u2208Cc \u03b1i. If Jmin < B, update B = Jmin. Return to the caller.\n4 Examples\nIn this section, the fault isolation approach (Algorithm 1) is applied to a linear benchmark example\nand the Tennessee Eastman benchmark problem. In addition, the computational efficiency of the\nproposed BAB algorithm (Algorithm 2) is further evaluated using randomly generated cases. All\ncomputations are carried out on a notebook with Intelr CoreTM Duo Processor T2400 (1.83 GHz,\n2MB RAM) using MATLABr 2007b.\n4.1 Linear benchmark example\nTo illustrate the principle of missing variable analysis for fault isolation and algorithms proposed\nin this work, the linear benchmark example presented in [15] is studied. The system has 5 sensor\nvariables and 2 internal states, represented as follows:\nx = Gt+ e (28)\n11\nwhere\nG =\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u22120.1670 \u22120.1352\n\u22120.5671 \u22120.3695\n\u22120.1608 \u22120.1019\n0.7574 \u22120.0563\n\u22120.2258 0.9119\n\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\nThe internal state, t is distributed normally with zero mean and unit variance. Under NOC, the\nsensor error, e is normally distributed with zero mean and 0.01 variance. Firstly, 1000 samples\nrepresenting NOC were produced through simulation. The covariance matrix, C of the PPCA\nmodel is as follows:\nC =\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n0.0604 0.1548 0.0435 \u22120.1247 \u22120.0983\n0.1548 0.4963 0.1369 \u22120.4270 \u22120.2400\n0.0435 0.1369 0.0491 \u22120.1225 \u22120.0634\n\u22120.1247 \u22120.4270 \u22120.1225 0.5997 \u22120.2020\n\u22120.0983 \u22120.2400 \u22120.0634 \u22120.2020 0.9262\n\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n(29)\nThe upper control limit of the M2 statistic for a 95% confidence level is \u03c725(0.95) = 11.07.\nThen, a single sensor fault with fault magnitude being 1.8 in the 4th sensor is considered. Af-\nter subtracting the mean calculated at NOC, the deviation of the faulty sensor vector is y =[\n\u22120.079 \u22120.59 \u22120.22 \u22121.78 \u22120.024\n]T\n. The M2 statistic for this fault is 244.43 > 11.07. Hence\nthis fault is easily detected by the PPCA. To isolate the fault, the conventional contribution plot\nis shown in Figure 2. Due to the significant correlation between the second and the fourth sensors,\n1 2 3 4 5\n0\n50\n100\n150\n200\nco\nn\ntr\nib\nut\nio\nn\nsensor\nFigure 2: PPCA contribution plot for the single sensor fault.\nas indicated by Yue and Qin [15], the fault in the fourth sensor causes large contributions from\n12\nboth second and fourth sensors. Hence, the conventional contribution analysis will not be able to\ncorrectly isolate this single sensor fault.\nTo apply the missing variable analysis, according to Algorithm 1, the values of E[M2] for 5 cases,\nwhere one of the variables are considered to be missing are shown in Table 1. Table 1 clearly indicates\nthat E[M2] is less than the upper control limit, only when x4 is considered to be missing. Thus, the\nfourth sensor is correctly identified as the source of the fault by the PPCA.\nTable 1: Missing variable analysis for the single sensor fault\nMissing Variable E[M2]\n{x4} 3.02\n{x5} 28.66\n{x2} 67.15\n{x1} 233.82\n{x3} 241.73\nNext, a multi-sensor fault with fault magnitude being 1.5 in both the third and fourth sensors is\nstudied. The fault deviation vector is y =\n[\n\u22120.079 \u22120.59 1.49 \u22121.48 \u22120.024\n]T\n. The corre-\nsponding monitoring criterion is E[M2] = 385.32 > 11.07, which successfully detects the fault. For\nfault isolation, the PPCA contribution plots of 5 sensors are shown in Figure 3(a). The contribution\nplot incorrectly indicates that sensors 2, 3 and 4 are faulty due to the strong correlation between\nsensors 2 and 4.\n1 2 3 4 5\n0\n50\n100\n150\n200\nco\nn\ntr\nib\nut\nio\nn\nsensor\n(a) Fault Magnitude = 1.5\n1 2 3 4 5\n0\n5\n10\n15\n20\n25\n30\n35\nco\nn\ntr\nib\nut\nio\nn\nsensor\n(b) Fault Magnitude = 0.5\nFigure 3: PPCA contribution plot for the multi-sensor fault.\n13\nTo apply Algorithm 1, first all cases with one of the variables missing are analysed. The minimum\nE[M2] for these cases is 145.38, which is larger than the upper control limit, indicating that the\nfault is not a single sensor fault. Further, all cases with two of the variables missing are evaluated\nand shown in Table 2. These results clearly and correctly indicate that sensors 3 and 4 are faulty.\nTable 2: Missing variable analysis for the multi-sensor fault\nFault magnitude = 1.5 Fault magnitude = 0.5\nMissing Variables E[M2] Missing Variables E[M2]\n{x3, x4} 3.67 {x3, x4} 3.67\n{x3, x5} 22.51 {x2, x3} 6.23\n{x2, x3} 25.35 {x3, x5} 8.68\n{x1, x2} 118.69 {x1, x2} 14.77\n{x2, x4} 138.46 {x2, x4} 19.39\n{x2, x5} 142.82 {x2, x5} 19.68\n{x1, x3} 178.17 {x1, x4} 41.25\n{x4, x5} 245.66 {x4, x5} 41.28\n{x1, x4} 246.28 {x1, x5} 43.22\n{x1, x5} 253.01 {x1, x3} 52.98\nFinally, to evaluate the effect of fault to noise ratio on the proposed algorithm, the case of multi-\nsensor fault is considered again with fault magnitude being 0.5. As the maximum value of noise for\ndifferent sensors is approximately 0.31, the fault magnitude is comparable to the noise level. In this\ncase, E[M2] is 75.74 > 11.07 and thus the fault is successfully detected by the PPCA. Similar to\nthe previous case, the contribution plots, shown in Figure 3(b), incorrectly identify faults in sensors\n2, 3 and 4. For all cases with one of the variables missing, the minimum E[M2] is 18.77 > 11.07.\nThe values of E[M2] for all cases with two of the variables missing are shown in Table 2. Although\nE[M2] is lower than the control limit for multiple cases, the lowest value of E[M2] is still seen, when\n3rd and 4th variables are considered to be missing.\nThis simple example illustrates how the missing variable analysis can be applied to isolate faulty\nvariables and its advantages over traditional contribution plot based approach. Due to the strong\ncorrelation between sensors 2 and 4, both single sensor and multi-sensor faults were not correctly\nisolated by the contribution analysis. Nevertheless, the missing variable analysis successfully isolated\nall the faults.\n14\n4.2 Tennessee Eastman process\nTo demonstrate the effectiveness and efficiency, Algorithm 1 for missing variable analysis is applied\nfor fault isolation of the Tennessee Eastman (TE) process [21]. This process has 5 main units, which\nare the reactor, condenser, separator, stripper and compressor. Streams of the plant consists of 8\ncomponents; A, B, C, D, E, F, G and H. Components A, B and C are gaseous reactants which are\nfed to the reactor to form products G and H. The flowsheet of the TE process is shown in Figure 4.\nFor fault isolation, the TE process is considered under closed-loop control as described by Downs and\nVogel [21]. Data for the 41 available measurements and 11 out of 12 manipulated variables (MVs)\nare collected for 21 operational modes, which correspond to the normal and 20 faulty operation\nmodes. These 52 measured variables are listed in Table 3. The root cause for five of these 20 faults\nare unknown according the original description of the plant [21]. Hence, it is not possible to validate\nthe isolation results for these faults by analysing the process flowsheet. Furthermore, there are four\nfaults, which are difficult to be detected by the PCA based approaches due to dynamic and nonlinear\nnature of these faults [7, 8]. Therefore, these 9 faults are excluded and the remaining 11 faults, listed\nin Table 4, are adopted for the case study. The identified variables for these eleven faults studied\nare listed in Table 5. Note that for some of the faults, Algorithm 1 identifies more than 1 set of\nvariables, among which the variable set with smallest values of E[M2] is shown in Table 5.\nAs indicated in Table 5, most of the faults result in only one and two responsible variables. An\nexception is Fault 7, for which the BAB method indicates that there are up to seven measured vari-\nables responsive to this fault. To select 7 out of 52 measured variables, there are C752 = 133, 784, 560\nalternatives. If one had to evaluate all alternatives to find the subset with minimum value of mon-\nitoring statistic, it would take more than a day to get the conclusion even if each evaluation takes\nonly one millisecond. However, it only takes about 3.37 seconds for the BAB algorithm to find the\ncontributing subset. This indicates that the proposed algorithm is suitable for on-line FDD to isolate\ncontributing variables of a fault under investigation in a short time so that more sophisticated root-\ncause analysis can be carried out based on the isolated contributing variables and the consequent\nloss due to the fault can be minimised.\nTo better appreciate the proposed fault diagnosis algorithm, the relationship of the minimum cri-\nterion value E[M2] against the number of variables to be observed n is shown in Figure 5. The\nhorizontal line in Figure 5 is the upper control limit calculated as \u03c72r(\u03b2) with r = 52 and \u03b2 = 0.99.\n15\nx45\nFault 7\nXC\nXE\nXF\nXE\nXF\nXD\nXG\nXH\n11C\nx4\nx6\nx22\nx16\n4\n5\n6\n8\n9\n12\nCondenser\nFI\nTI\nPI\nLI\nJI\nTI\nFI\nA\nN\nA\nL\nY\nZ\nE\nR\nXA\nXB\nXC\nXD\nXE\nXF\nCompressor\nStripper\nVap\/Liq\nseparator\n7\nA\nPCFI\n1\nCWS\n13\nCWR\nCWS\nPI\nTI\nFI\nSC\nTI\nCWR\nCond\nFI\nStm\nTI\n10\nFI\nLI\nPurge\nA\nN\nA\nL\nY\nZ\nE\nR\nLI\nReactor\nPI\nFI\nFI\nProduct\nXBA\nN\nA\nL\nY\nZ\nE\nR\nXA\nXD\nXH\nXG\nPCFI\n2\nD\n3\nE\nPCFI\nx51\nx9\nFigure 4: Flowsheet of the TEP Plant showing detected variables for Fault 7\n16\nTable 3: Measured variables\nID Description ID Description\nx1 A Feed (Stream 1) x27 Component E (Stream 6)\nx2 D Feed (Stream 2) x28 Component F (Stream 6)\nx3 E Feed (Stream 3) x29 Component A (Stream 9)\nx4 Total Feed (Stream 4) x30 Component B (Stream 9)\nx5 Recycle Flow (Stream 8) x31 Component C (Stream 9)\nx6 Reactor Feed Rate (Stream 6) x32 Component D (Stream 9)\nx7 Reactor Pressure x33 Component E (Stream 9)\nx8 Reactor Level x34 Component F (Stream 9)\nx9 Reactor Temperature x35 Component G (Stream 9)\nx10 Purge Rate (Stream 9) x36 Component H (Stream 9)\nx11 Product Separator Temperature x37 Component D (Stream 11)\nx12 Product Separator Level x38 Component E (Stream 11)\nx13 Product Separator Pressure x39 Component F (Stream 11)\nx14 Product Separator Underflow (Stream 10) x40 Component G (Stream 11)\nx15 Stripper Level x41 Component H (Stream 11)\nx16 Stripper Pressure x42 MV to D Feed Flow (Stream 2)\nx17 Stripper Underflow (Stream 11) x43 MV to E Feed Flow (Stream 3)\nx18 Stripper Temperature x44 MV to A Feed Flow (Stream 1)\nx19 Stripper Steam Flow x45 MV to Total Feed Flow (Stream 4)\nx20 Compressor Work x46 Compressor Recycle Valve\nx21 Reactor Cooling Water Outlet Temperature x47 Purge Valve (Stream 9)\nx22 Separator Cooling Water Outlet Temperature x48 MV to Separator Pot Liquid Flow (Stream 10)\nx23 Component A (Stream 6) x49 MV to Stripper Liquid Product Flow (Stream 11)\nx24 Component B (Stream 6) x50 Stripper Steam Valve\nx25 Component C (Stream 6) x51 MV to Reactor Cooling Water Flow\nx26 Component D (Stream 6) x52 MV to Condenser Cooling Water Flow\n17\nTable 4: Operational Faults\nFault ID Description\n1 Step in A\/C Feed Ratio, B Composition Constant (stream 4)\n2 Step in B composition while A\/C ratio is constant (stream 4)\n4 Step in Reactor Cooling Water Inlet Temperature\n5 Step in Condenser Cooling Water Inlet Temperature\n6 A Feed loss (step change in stream 1)\n7 C Header Pressure Loss - reduced availability (step change in stream 4)\n8 Random variation in A,B,C Feed Composition (stream 4)\n11 Random variation in Reactor Cooling Water Inlet Temperature\n12 Random variation in Condenser Cooling Water Inlet Temperature\n13 Slow drift in Reaction Kinetics\n14 Sticking Reaction Cooling Water Valve\nTable 5: Possible Fault responsive variables detected by the branch and bound algorithm\nFault IDs Variables eliminated\n1 {x16}\n2 {x21, x24, x30}\n4 {x9, x51}\n5 {x11, x22}\n6 {x1, x44}\n7 {x4, x6, x9, x16, x22, x45, x51}\n8 {x37}\n11 {x51}\n12 {x22}\n13 {x37}\n14 {x9, x51}\n18\nIt is shown that for n \u2264 45 (left to the dashed line), the minimum criterion value is less than the\nupper control limit, whilst for n \u2265 46, (right to the dotted line) the criterion is above the upper\ncontrol limit. Therefore, the maximum number of non-responsive variables is 45 and the number of\npossible responsive variables to Fault 7 is 52 \u2212 45 = 7. The actual deviations of these 7 variables\nare y4 = \u221214.5015, y6 = \u22123.8761, y9 = \u22123.1665, y16 = \u22124.2085, y22 = \u22123.4847, y45 = 4.4334,\nand y51 = \u22124.4255. The corresponding locations of these 7 variables and the fault are marked in\nFigure 4.\n10 20 30 40 50\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nm\nin\nim\num\n M\n2\nnumber of retained variables\nUCL\nFigure 5: The minimum M2 against number of variables (n) observed for Fault 7.\nA physical explanation of the propagation of fault to the identified variables is as follows: Fault 7\nshown in Figure 4 involves C Header Pressure Loss - reduced availability (step change in stream 4),\nresulting in a decrease in the Total Feed in stream 4 (x4) and a corresponding increase in the\nMV to Total Feed Flow in stream 4 (x45) to counter the effect of the fault occurrence through\nthe corresponding flow control loop of the plant. The decrease in the Total Feed in stream 4 (x4)\nresults in a corresponding decrease in the Recycle back to the reactor through stream 6, thereby\nreducing the Reactor Feed Rate in stream 6 (x6) as well as the Reactor Temperature (x9). This\ndecrease in the Reactor Temperature (x9) results in a decrease in the Reactor Cooling Water Flow\n(x51). Also, the decrease in the Total Feed in stream 4 (x4) into the Stripper results in a decrease\nin the Stripper Pressure (x16) and consequently, a decrease in the Separator Cooling Water Outlet\nTemperature (x22). From the above explanation, it can be concluded that the results obtained\nthrough the BAB algorithm is also supported by the analysis through a physical understanding of\nthe plant. Therefore, this proposed approach will be practically useful for fault isolation associating\nwith condition monitoring.\n19\nIt is worth to note that there are two pairs of faults, i.e. Faults 4 and 14, and Faults 8 and 13, which\nhave the same faulty variable set isolated by the missing variable analysis. That the same faulty\nvariable set is identified for Faults 4 and 14 is understandable as both of these faults are associated\nwith the reactor cooling water although one is a step change in the cooling water temperature whilst\nanother is due to a sticking cooling water control valve. The identification of same faulty variable set\nfor Faults 8 and 13, however, is difficult to explain without the knowledge of the reaction kinetics,\nwhich was hidden for the benchmark problem [21]. Note that Algorithm 2 identifies several single-\nvariables sets for these two faults in addition to the common set corresponding to the minimum\nE[M2]. For Fault 8, three single-variable sets, {x37}, {x6} and {x30} are identified, whilst for Fault\n13, 8 single-variable sets are detected with E[M2] < \u03c72r(\u03b2), which are {x37}, {x8}, {x30}, {x26},\n{x40}, {x23}, {x22} and {x39}. This extra information can provide useful insight for plant engineers\nto correctly diagnose the underlying causes of these two faults.\n4.3 Random tests\nTo further evaluate the computational efficiency of the BAB in Algorithm 2, random cases are\nconsidered. The test involves retaining n out of 40 variables. For each n, a vector y of dimension 40\nand a positive definite matrix C of dimension 40\u00d7 40 are randomly generated. To ensure positive-\ndefiniteness of C, a matrix C\u02dc is randomly generated, where C = C\u02dcC\u02dcT . The elements of y and\nC\u02dc are normally distributed with zero means and unit variance. For each problem, 1000 cases are\ntested and the average computation time and number of nodes evaluated are shown in Figure 6. For\ncomparison, the computational time of a brute force search is estimated by multiplying Cn40 with the\ntime required for evaluating the criterion given in (17) of n variables.\nFigure 6 indicates that the worst efficiency of the BAB algorithm occurs for n = 28 instead of n = 20\nas expected for the brute force search. This efficiency shift towards a higher number of variables\nis due to the upwards nature of the BAB algorithm. Further explanation of the efficiency shift for\ndifferent types of BAB algorithms can be found in Cao and Kariwala [28]. For most fault isolation\napplications, the number of missing variables to be detected is relatively small, i.e. the number of\nretained variables is relatively large. Thus, the efficiency shift indicated in Figure 6 is not favourable\nfor fault isolation applications. Nevertheless, even for the worst case of n = 28, the average number\nof nodes evaluated and average solution time required by the BAB approach are still 5 orders of\nmagnitude lower than that required by the brute-force search showing the advantages of the proposed\n20\n2 10 20 30 39\n10\u22123\n100\n104\n108\nCP\nU\n ti\nm\ne,\n s\n(a)\n \n \nBrute\u2212Force\nBAB\n2 10 20 30 39\n100\n104\n108\n1012\nn\nev\nal\nua\ntio\nns\n(b)\n \n \nBrute\u2212Force\nBAB\nFigure 6: Random tests for retaining n from 40 variables (a) computation time and; (b) number of\nnodes evaluated\napproach. This random test together with the TE case study indicates that the proposed the BAB\nalgorithm is capable of efficiently solving multivariate problems with up to 50 variables. In future, a\nbidirectional BAB algorithm [28\u201330] will be developed for handling fault isolation applications with\nmore variables.\n5 Conclusions\nIn contrast to considerable attention paid to multivariate statistical process monitoring (MSPM),\nresearch in fault diagnosis has been relatively limited due to the difficulty in dealing with the multi-\nvariate contribution analysis. It is the first time that a multivariate missing-variable approach and\nan efficient branch and bound algorithm are proposed to efficiently solve this problem. Although the\nfocus of this paper is on using the probabilistic principal component analysis (PPCA), the frame-\nwork developed in this work can be easily extended to adopt other criteria for fault diagnosis. The\nnumerical case studies using a linear example and the Tennessee Eastman process show that the\nproposed method is able to find the minimum set of variables, which are affected by the fault, in\na short time. This computational efficiency, which is examined through a random test, can give\noperators more time to identify and further deal with the fault in order to minimize the consequent\nloss due to the occurrence of the fault. However, the random tests also indicate that the largest size\n21\nof variable set solvable by the BAB algorithm is limited. Future work will focus on improving the\ncomputational efficiency of the BAB algorithm to make it applicable to dynamic data with a large\nnumber of variables.\n6 Acknowledgement\nThis work was supported by the Royal Society, UK through grant no. IV0871568.\nReferences\n[1] V. Venkatasubramanian, R. Rengaswamy, K. Yin, and S.N. Kavuri. A review of process fault\ndetection and diagnosis Part I: Quantitative model-based methods. Computers and chemical\nengineering, 27(3):293\u2013311, 2003.\n[2] S. Joe Qin. Statistical process monitoring: basics and beyond. Journal of Chemometrics, 17\n(8-9):480\u2013502, 2003.\n[3] V. Venkatasubramanian, R. Rengaswamy, S.N. Kavuri, and K. Yin. A review of process fault\ndetection and diagnosis Part III: Process history based methods. Computers and Chemical\nEngineering, 27(3):327\u2013346, 2003.\n[4] SI Alabi, AJ Morris, and EB Martin. On-line dynamic process monitoring using wavelet-based\ngeneric dissimilarity measure. Chemical Engineering Research and Design, 83(6):698\u2013705, 2005.\n[5] U. Kruger, Y. Zhou, and G.W. Irwin. Improved principal component monitoring of large-scale\nprocesses. Journal of Process Control, 14(8):879\u2013888, 2004.\n[6] J.M. Lee, C.K. Yoo, and I.B. Lee. Statistical monitoring of dynamic processes based on dynamic\nindependent component analysis. Chemical engineering science, 59(14):2995\u20133006, 2004.\n[7] P.P. Odiowei and Y. Cao. Nonlinear dynamic process monitoring using canonical variate analysis\nand kernel density estimations. IEEE Transactions on Industrial Informatics, 6(1):36\u201345, 2010.\n[8] P.P. Odiowei and Y. Cao. State-space independent component analysis for nonlinear dynamic\nprocess monitoring. Chemometrics and Intelligent Laboratory Systems, In Press, Corrected\nProof, 2010. ISSN 0169-7439. doi: DOI: 10.1016\/j.chemolab.2010.05.014.\n22\n[9] M. N. Nounou, B. R. Bakshi, P. K. Goel, and X. Shen. Bayesian principal component analysis.\nJournal of Chemometrics, 16:576\u2013595, 2002.\n[10] T. Chen, J. Morris, and E. Martin. Probability density estimation via an infinite Gaussian\nmixture model: application to statistical process monitoring. Journal of the Royal Statistical\nSociety C (Applied Statistics), 55:699\u2013715, 2006.\n[11] S. W. Choi, J. H. Park, and I.-B. Lee. Process monitoring using a Gaussian mixture model via\nprincipal component analysis and discriminant analysis. Computers and Chemical Engineering,\n28:1377\u20131387, 2004.\n[12] U. Thissen, H. Swierenga, A. P. de Weijer, R. Wehrens, W. J. Melssen, and L. M. C. Buydens.\nMultivariate statistical process control using mixture modelling. Journal of Chemometrics, 19:\n23\u201331, 2005.\n[13] J. Yu and S. J. Qin. Multimode process monitoring with Bayesian inference-based finite Gaus-\nsian mixture models. AIChE Journal, 54:1811\u20131829, 2008.\n[14] P. Miller, R. E. Swanson, and C. F. Heckler. Contribution plots: a missing link in multivariate\nquality control. International Journal of Applied Mathematics and Computer Science, 8:775\u2013\n792, 1998.\n[15] H.H. Yue and S.J. Qin. Reconstruction based fault identification using a combined index.\nIndustrial and Engineering Chemistry Research, 40:4403\u20134414, 2001.\n[16] Q. Chen, U. Kruger, M. Meronk, and A. Y. T. Leung. Synthesis of T 2 and Q statistics for\nprocess monitoring. Control Engineering Practice, 12:745\u2013755, 2004.\n[17] T. Chen and Y. Sun. Probabilistic contribution analysis for statistical process monitoring: A\nmissing variable approach. Control Engineering Practice, 17(4):469\u2013477, 2009.\n[18] M. E. Tipping and C. M. Bishop. Probabilistic principal component analysis. Journal of the\nRoyal Statistical Society B, 61:611\u2013622, 1999.\n[19] D. Kim and I.-B. Lee. Process monitoring based on probabilistic PCA. Chemometrics and\nintelligent laboratory systems, 67:109\u2013123, 2003.\n[20] S. W. Choi, E. B. Martin, and A. J. Morris. Fault detection based on a maximum-likelihood\nprincipal component analysis (PCA) mixture. Industrial and Engineering Chemistry Research,\n44:2316\u20132327, 2005.\n23\n[21] J. J. Downs and E. F. Vogel. A plant-wide industrial process control problem. Comput. Chem.\nEng., 17(3):245\u2013255, 1993.\n[22] I. T. Jolliffe. Principal Component Analysis. Springer, 2nd edition, 2002.\n[23] P. Narendra and K. Fukunaga. A branch and bound algorithm for feature subset selection.\nIEEE Trans. Comput., C-26:917\u2013922, 1977.\n[24] B. Yu and B. Yuan. A more efficient branch and bound algorithm for feature selection. Pattern\nRecognition, 26:883\u2013889, 1993.\n[25] P. Somol, P. Pudil, F. Ferri, and J. Kittler. Fast branch and bound algorithm in feature selection.\nIn B. Sanchez, J. Pineda, J. Wolfmann, Z. Bellahsense, and F. Ferri, editors, Proceedings of\nWorld Multiconference on Systemics, Cybernetics and Informatics, volume VII, pages 1646\u2013651,\nOrlando, Florida, USA, 2000.\n[26] X.-W. Chen. An improved branch and bound algorithm for feature selection. Pattern Recogni-\ntion Letters, 24:1925\u20131933, 2003.\n[27] Y. Cao and P. Saha. Improved branch and bound method for control structure screening. Chem.\nEngg. Sci., 60(6):1555\u20131564, 2005.\n[28] Y. Cao and V. Kariwala. Bidirectional branch and bound for controlled variable selection: Part\nI. Principles and minimum singular value criterion. Comput. Chem. Engng., 32(10):2306\u20132319,\n2008.\n[29] V. Kariwala and Y. Cao. Bidirectional branch and bound for controlled variable selection: Part\nII. Exact local method for self-optimizing control. Comput. Chem. Eng., 33:1402\u20131412, 2009.\n[30] V. Kariwala and Y. Cao. Bidirectional branch and bound for controlled variable selection: Part\nIII. Local average loss minimization. IEEE Transactions on Industrial Informatics, to appear,\n2010.\n[31] G. H. Golub and C. F. van Loan. Matrix Computations. The Johns Hopkins University Press,\nBaltimore, MD, 3rd edition, 1993.\n[32] R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, Cambridge, UK,\n1985.\n24\n"}