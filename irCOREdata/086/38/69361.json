{"doi":"10.1109\/IJCNN.2008.4633989","coreId":"69361","oai":"oai:eprints.lancs.ac.uk:27121","identifiers":["oai:eprints.lancs.ac.uk:27121","10.1109\/IJCNN.2008.4633989"],"title":"Autonomous novelty detection and object tracking in video streams using evolving clustering and Takagi-Sugeno type neuro-fuzzy system.","authors":["Angelov, Plamen","Ramezani, Ramin","Zhou, Xiao"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2008","abstract":"Autonomous systems for surveillance, security, patrol, search and rescue are the focal point of extensive research and interest from defense and the security related industry, traffic control and other institutions. A range of sensors can be used to detect and track objects, but optical cameras or camcorders are often considered due to their convenience and passive nature. Tracking based on color intensity information is often preferred than the motion cues due to being more robust. The technique presented in this paper can also be used in conjunction with infra-red cameras, 3D lasers which result in a grey scale image. Novelty detection and tracking are two of the key elements of such systems. Most of the currently reported techniques are characterized by high computational, memory storage costs and are not autonomous because they usually require a human operator in the loop. This paper presents new approaches to both the problem of novelty detection and object tracking in video streams. These approaches are rooted in the recursive techniques that are computationally efficient and therefore potentially applicable in real-time. A novel approach for recursive density estimation (RDE) using a Cauchy type of kernel (as opposed to the usually used Gaussian one) is proposed for visual novelty detection and the use of the recently introduced evolving Takagi-Sugeno (eTS) neuro-fuzzy system for tracking the object detected by the RDE approach is proposed as opposed to the usually used Kalman filter (KF). In fact, eTS can be seen as a fuzzily weighted mixture of KF. The proposed technique is significantly faster than the well known kernel density estimation (KDE) approach for background subtraction for novelty detection and is more precise than the usually used KF. Additionally the overall approach removes the need of manually selecting the object to be tracked which makes possible a fully autonomous system for novelty detection and tracking to be developed. (c) IEEE Pres","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":"IEEE","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:27121<\/identifier><datestamp>\n      2018-01-24T02:04:40Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D626F6F6B5F73656374696F6E<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Autonomous novelty detection and object tracking in video streams using evolving clustering and Takagi-Sugeno type neuro-fuzzy system.<\/dc:title><dc:creator>\n        Angelov, Plamen<\/dc:creator><dc:creator>\n        Ramezani, Ramin<\/dc:creator><dc:creator>\n        Zhou, Xiao<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        Autonomous systems for surveillance, security, patrol, search and rescue are the focal point of extensive research and interest from defense and the security related industry, traffic control and other institutions. A range of sensors can be used to detect and track objects, but optical cameras or camcorders are often considered due to their convenience and passive nature. Tracking based on color intensity information is often preferred than the motion cues due to being more robust. The technique presented in this paper can also be used in conjunction with infra-red cameras, 3D lasers which result in a grey scale image. Novelty detection and tracking are two of the key elements of such systems. Most of the currently reported techniques are characterized by high computational, memory storage costs and are not autonomous because they usually require a human operator in the loop. This paper presents new approaches to both the problem of novelty detection and object tracking in video streams. These approaches are rooted in the recursive techniques that are computationally efficient and therefore potentially applicable in real-time. A novel approach for recursive density estimation (RDE) using a Cauchy type of kernel (as opposed to the usually used Gaussian one) is proposed for visual novelty detection and the use of the recently introduced evolving Takagi-Sugeno (eTS) neuro-fuzzy system for tracking the object detected by the RDE approach is proposed as opposed to the usually used Kalman filter (KF). In fact, eTS can be seen as a fuzzily weighted mixture of KF. The proposed technique is significantly faster than the well known kernel density estimation (KDE) approach for background subtraction for novelty detection and is more precise than the usually used KF. Additionally the overall approach removes the need of manually selecting the object to be tracked which makes possible a fully autonomous system for novelty detection and tracking to be developed. (c) IEEE Press<\/dc:description><dc:publisher>\n        IEEE<\/dc:publisher><dc:date>\n        2008<\/dc:date><dc:type>\n        Contribution in Book\/Report\/Proceedings<\/dc:type><dc:type>\n        NonPeerReviewed<\/dc:type><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/IJCNN.2008.4633989<\/dc:relation><dc:identifier>\n        Angelov, Plamen and Ramezani, Ramin and Zhou, Xiao (2008) Autonomous novelty detection and object tracking in video streams using evolving clustering and Takagi-Sugeno type neuro-fuzzy system. In: IEEE International Joint Conference on Neural Networks, 2008. IJCNN 2008. (IEEE World Congress on Computational Intelligence). IEEE, pp. 1456-1463. ISBN 978-1-4244-1820-6<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/27121\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":null,"relations":["http:\/\/dx.doi.org\/10.1109\/IJCNN.2008.4633989","http:\/\/eprints.lancs.ac.uk\/27121\/"],"year":2008,"topics":["QA75 Electronic computers. Computer science"],"subject":["Contribution in Book\/Report\/Proceedings","NonPeerReviewed"],"fullText":"  \n \n  \nAbstract \u2014 Autonomous systems for surveillance, security, \npatrol, search and rescue are the focal point of extensive \nresearch and interest from defense and the security related \nindustry, traffic control and other institutions. A range of \nsensors can be used to detect and track objects, but optical \ncameras or camcorders are often considered due to their \nconvenience and passive nature. Tracking based on color \nintensity information is often preferred than the motion cues \ndue to being more robust. The technique presented in this \npaper can also be used in conjunction with infra-red cameras, \n3D lasers which result in a grey scale image. Novelty detection \nand tracking are two of the key elements of such systems. Most \nof the currently reported techniques are characterized by high \ncomputational, memory storage costs and are not autonomous \nbecause they usually require a human operator in the loop. \nThis paper presents new approaches to both the problem of \nnovelty detection and object tracking in video streams. These \napproaches are rooted in the recursive techniques that are \ncomputationally efficient and therefore potentially applicable \nin real-time. A novel approach for recursive density estimation \n(RDE) using a Cauchy type of kernel (as opposed to the \nusually used Gaussian one) is proposed for visual novelty \ndetection and the use of the recently introduced evolving \nTakagi-Sugeno (eTS) neuro-fuzzy system for tracking the \nobject detected by the RDE approach is proposed as opposed \nto the usually used Kalman filter (KF). In fact, eTS can be seen \nas a fuzzily weighted mixture of KF. The proposed technique \nis significantly faster than the well known kernel density \nestimation (KDE) approach for background subtraction for \nnovelty detection and is more precise than the usually used \nKF. Additionally the overall approach removes the need of \nmanually selecting the object to be tracked which makes \npossible a fully autonomous system for novelty detection and \ntracking to be developed.  \nI. INTRODUCTION \nhe role of security and surveillance systems has grown \nsignificantly recently driven by both an increased \ndemand (defense and security applications  related to \nthe more insecure situation in the World) and by the \nincrease of capabilities and often lower price of the \ntechnology that is producing video streams.  \nTraditional off-line methods require a large amount of \ncomputer storage for archiving video streams and hence are \nnot very efficient [1,2,32,33]. The most prominent \n \nAll authors are with the Intelligent Systems Research Lab, Digital Signal \nProcessing Group, Dept of Communication Systems, InfoLab21, \nLancaster University, Lancaster, LA1 4WA, UK; tel: +44(1524)510391; \ne-mail: p.angelov@lancaster.ac.uk \napproaches are based on so called background subtraction \nand background modeling [2-5,31]. Alternatively, video \nsequences can be processed in real time which leads to \nreducing the storage requirement, enhancement of the video \ntransmission (tackling the bandwidth problem), and \neliminating the possibility of potential mistakes by human \noperators. \nThe main challenge is to develop autonomous systems \nthat require little processing time and storage capacity and \nare suitable for on-line applications. Additionally, it is \nhighly desirable these systems to be free from task-specific \nthresholds and tuning, because autonomous systems assume \nabsence of a direct human involvement. In a security \nmonitoring scenario such ability will solve the problem of \nboredom and lack of concentration of the operators. \nThe two main elements of an autonomous security and \nsurveillance system are the automatic detection and tracking \nof objects of interest [6,7]. The automatic novelty detector \nshould get the video stream and should identify the pixels \nthat are different from the background and are thus \nsuspected to be a part of a physical object new to the scene. \nThis requires modeling the background [2-6,31] pixel by \npixel and comparing the current image to this model. This is \nthe principle behind the Kernel Density Estimation (KDE) \napproach which is using probabilistic model of the \nbackground and a threshold to detect foreground which is \nsuspected part of a new object [2,5,31]. The computational \ncomplexity of the KDE approach is high and therefore it is \nusually applied off-line over a window of certain length, N.  \nThe main objective of visual tracking algorithm is to \nperform fast and reliable matching of the target from frame \nto frame. A variety of tracking algorithms and techniques \nhas been developed and reported [8,9] for different \napplication domains. Approaches that are based on using \ncolor as a cue are preferable because of their computational \nefficiency and robustness in respect to the object geometry \nand occlusion [10,11].  \nA specialized system for tracking people, Pfinder [12] is \nbased on static statistical model of color variation and shape \nto obtain a 2D representation of the head and hands of \npeople being tracked. In [10] a color blob tracking \ntechnique is proposed based on color image segmentation \nusing principle component analysis, PCA and a parametric \nellipsoidal model of the color distribution. This method \nrequires a model, based on the color distribution and image \nAutonomous Novelty Detection and Object Tracking in Video \nStreams using Evolving Clustering and Takagi-Sugeno type \nNeuro-Fuzzy System \nPlamen Angelov, Senior Member IEEE, Ramin Ramezani, Xiaowei Zhou, Student Member IEEE \nT \n1457\n978-1-4244-1821-3\/08\/$25.00 c\u00a92008 IEEE\n  \n \nsegmentation and is applicable off-line only. An on-line \nmethod was reported in [11] based on Kalman filter [13]. In \nthis paper, we propose an on-line learning method based on \nthe recently introduced evolving Takagi-Sugeno (eTS) \nneuro-fuzzy systems [14,15] to predict and update the \nposition of the color blob in the next frame. The recursive \nnature of the eTS algorithm makes possible to design an \nevolving fuzzy rule-base in on-line mode, which adapts to \nthe variations of the data pattern. Hence, the system can \npredict the target position independent of the movement \npattern. In fact, the eTS is a fuzzy blend [15] of Kalman \nfilters. The prediction based on eTS is superior to the \nusually used Kalman filter as illustrated by experiments and \nthe system is capable of tracking objects in real-time. \nThe remainder of the paper is organized as follows. \nSection II describes the Novelty Detection part of the \nsystem. It starts with the review of the KDE algorithm and \ncovers the recursive version of the KDE approach proposed \nin this paper called RDE. Section III describes the object \ntracking approach based on eTS neuro-fuzzy system. \nSection IV represents the experimental set up and results, \nincluding a discussion. Section V concludes with a brief \noutline of the future directions. \nII. NOVELTY DETECTION IN VIDEO STREAMS THROUGH \nRECURSIVE BACKGROUND SUBTRACTION \nA. Background subtraction \nOne of the most popular approaches for visual novelty \ndetection is the so called background subtraction method \n[2,5]. It is based on the statistical representation of the \nbackground that is representative, robust to the noise and is \nsensitive to new objects [1,2]. Robustness is required to \ndistinguish the appearance of a new object on the scene from \nfluctuations in the statistical characteristics of the video \nstream due to wind, movement of tree leaves etc. According \nto this method, each pixel in a video frame is modeled as a \nrandom variable in a particular feature space (usually color) \nand its probability density function (pdf) is being estimated \n[1,2]. A more advanced approach is based on mixture of \nGaussians [16].  In this approach the underlying assumption \nof the form of the pdf is more realistic (being a multimodal \nrather than a simple Gaussian). This approach, however, \nrequires weights and modes of the mixture to be specified as \nwell as the thresholds which are problem- and \nuser-dependent and influence the result. \nB. Kernel Density Estimation \nKDE is one of the most common techniques for modeling \nthe background in video processing area [1,5] which usually \nassumes a Gaussian kernel to represent the pdf for each \npixel [1,6] to be in the background. The pdf is estimated \nnumerically from training data based on a window with a \nlength N (usually N>10 but not excessively large because \nthe background may not necessarily be static). Once the pdf \nof a pixel to be a part of the background is estimated it is \nsimply compared with a pre-defined threshold value [2]. If it \nis lower than this predefined threshold, it is assumed that \nthis pixel is part of the foreground. The threshold is usually \nselected subjectively or results from a number of off-line \nexperiments [2]. KDE is very accurate but very expensive \napproach in terms of the memory and computation time. A \nserious disadvantage is the requirement to pre-specify the \nthreshold. \nIf we denote by x(1), x(2), \u2026,x(t),\u2026,x(N) the color \nintensity values of N consecutive frames of a video stream \nthat have a specific position (i,j) in each frame (see Figure 1) \nthen the pdf of the current, tth pixel, x(t) to be a part of the \nbackground can be estimated based on the similarity of its \ncolor intensity value to all color intensity values of pixels in \nthe same position in a window of N frames by [2]:  \n( ) ( )\u00a6\u220f\n= =\n\u2212=\nN\ni\nr\nj\nixtxk\nN\ntxp\n1 1\n)()(1)( \u03c3      (1) \nwhere x(i) denotes the color intensity value of the pixel in \nith frame; k\u03c3 is the kernel function with bandwidth \u0131 [1]; r \ndenotes the color channel (R, G, and B or H, S, V [7] and is \nequal to 3 in most cases; \u03c3j is the bandwidth of the kernel \nfunction in that particular channel.  \n \n \n \nFigure 1. Window of N frames used in KDE approach. \n \nThe most commonly used kernel function is Gaussian [2] \nwhich leads to: \n( )\n( )\n\u00a6\u220f\n= =\n\u2212\n\u2212\n=\nN\ni\nr\nj\nixtx\nj\nj\njj\ne\nN\ntxp\n1 1\n)()(\n2\n1\n2\n2\n2\n2\n11)( \u03c3\n\u03c0\u03c3\n (2) \nWhich can be simplified as: \n( )\n( )\n\u00a6\n=\n\u2212\n\u2212\u00a6\n=\n=\nN\ni\nixtx\nj\nr\ni j\njj\ne\nN\ntxp\n1\n2\n)()(\n2\n1\n2\n2\n2\n11)( \u03c3\n\u03c0\u03c3\n    (3) \nIn practice, the pdf is estimated by calculating the kernel \nfunction in offline mode and using lookup tables [2]. \nDefining a proper threshold is a significant drawback and \nmay result in distortion and low performance of the whole \nsystem in different environments. Another major problem of \nthe KDE approach is to define a proper bandwidth for the \nkernel function. If a very narrow bandwidth is selected the \ndensity estimation will be over-sensitive; on the contrary, a \nwide bandwidth causes the density estimation to be \n1458 2008 International Joint Conference on Neural Networks (IJCNN 2008)\n  \n \nover-smoothed and detection may become difficult [1]. \nTheoretically, as the number of samples increases to \ninfinity, the role of the bandwidth fades, but using a window \nwith large size is contradicting the attempts to make the \napproach computationally tractable and fast. Therefore, \nusually a window of certain size (10 or more frames) is used.  \nC. The concept of the proposed approach \nThe main idea of the proposed recursive KDE (RDE) \napproach is to approximate the pdf of the color intensity by a \nCauchy type kernel instead of the Gaussian kernel that used \nin the original KDE approach [2,30]. As a consequence, one \ncan use recursive expressions to update the pdf estimation \non-line using the information from the pixel color intensity \nwhich is brought by the next image frame. This approach \nallows the image frames to be discarded once they have \nbeen processed and not to be kept in the memory. Instead, \ninformation concerning the color density per pixel is \naccumulated and is being kept in the memory. In this way, \none needs to keep in the memory a fixed amount of \ninformation namely 3xM (where M is the size of the frame in \npixels) or simply M (if the image is grey scale) irrespective \nof the size of the window used comparing to 3xNxM or NxM \nfor the original KDE approach. The coefficient 3 is due to \nthe dimension of the color channel (red, R; green, G; and \nblue, B; or alternatively one for each of hue, H; saturation, S; \nand brightness value, V) and one frame for grey images. In \nthis way, the proposed RDE approach requires N times less \nmemory (where N is usually >10) and the processing time is \nproportionally reduced.  \nThe main idea is to approximate the Gaussian function in \n(3) with Cauchy function and then to recursively calculate it. \nIndeed, the KDE can be seen as a special case of the more \ngeneric Parzen windows [20], generalized regression \nmodels [21], Mountain function [17] and potential [18,19] \napplied for image processing. The Cauchy function has the \nsame basic properties as the Gaussian [17], namely it is \nmonotonic; its maximum is unique and of value 1; and it \nasymptotically tends to zero when the argument tends to \nplus or minus infinity. Even more, Cauchy function can be \nseen as a first order approximation of the Gaussian. \nCauchy function-based potential represents an estimate \nof the density of a certain data sample (pixel color intensity \nvalue) based on the similarity to all previously seen data \nsamples or samples from a window with length, L (Figure \n1):  \n( )\u00a6\u00a6\n= =\n\u2212\n+\n=\nL\ni\nr\nj j\njj ixtx\ntxP\n1 1\n2\n2\n2\n)()(\n1\n1))((\n\u03c3\n   (4) \nD. Recursive Estimation of the Density  \nIn the original KDE approach [2] one needs to store the \nprevious N frames in a buffer and use the equation (4) which \nrequires a large processing time. Alternatively, one can \ncalculate (4) recursively as in [19] and process the video \nstream on a frame-by-frame basis (on-line) and discard the \nframes that have been processed already.  \n)(2)()1)()(1(\n1))((\ntctbtat\nt\ntxP\n\u2212++\u2212\n\u2212\n=               (5) \nValues a(t) and c(t) can be calculated from the current frame \nonly: \n\u00a6\n=\n=\nr\nj\nj txta\n1\n2 )()(              (6) \n\u00a6\n=\n=\nr\nj\njj tdtxtc\n1\n)()()(             (7) \nWhere )(td j is calculated recursively as shown below. \nThe value )(tb is also accumulated during the processing of \nthe frames one by one as given by the following recursive \nexpressions: \n \n)1()1()( \u2212+\u2212= tatbtb ;  0)1( =b         (8) \n \n)1()1()( \u2212+\u2212= txtdtd jjj ; 0)1( =jd       (9) \nThis will be equivalent to taking into account the visual \ninformation from all the previous frames. Note that the \nvalue of the spread of the Cauchy function \u03c3 can be updated \nby the data samples by learning the data variance [15]: \n \n5.0)1(;)()(1)1(\n2\n1)(\n1\n222\n=\u2212+\u2212= \u00a6\n=\nj\nt\ni\njitjj txtxt\ntt \u03c3\u03c3\u03c3 \u0003  (10) \nE. Novelty Detection using RDE approach \nThe core concept of the proposed recursive KDE \napproach is to estimate the data density of a specific pixel in \nthe current image frame based on the recursively \naccumulated information through (5)-(9) that represents the \ndensity in all previous N frames, where N can be as big as \nnecessary. Moreover, the proposed RDE approach does not \nuse a pre-fixed threshold which is often task-, or \nuser-specific. Instead, the comparison is with the point with \nthe minimal potential (data density) so far which is \nobviously adaptive and not user-specific:  \n( )( ) ( )foregroundistxTHENtPtxPIF )()()( < (11) \nwhere )(tP  is the minimum value of P  \nBy applying the condition (11) to each pixel of the current \nframe and using the values of )1( \u2212tb and )1( \u2212td j it is \npossible to identify the pixels that potentially form a novel \nobject. One way to determine the object for tracking \npurposes [7,10,12] is by using the spatial mean of all pixels \nthat has been classified as a foreground by (11) in a given \nimage frame. \n2008 International Joint Conference on Neural Networks (IJCNN 2008) 1459\n  \n \n{ }i\nF\nil\nhmeanh\n1=\n= ; { }i\nF\nil\nvmeanv\n1=\n=\n    (12) \nwhere h denotes the horizontal position of the pixel in the \nimage frame; v denotes the vertical position of the pixel in \nthe image frame; F denotes the number of pixels in a frame \nclassified as foreground (F<<M). \n The use of mean is prone to the influence of the noise. \nThe reasons for the noise might be phenomena like wind, \nchange of illumination, moves of the leaves of the trees, \nvibrations etc. This may lead to false positioning which \nmight be misleading for the tracking. While there are \neffective techniques to reduce the effect of the noise on \nillumination, this is not the case with the other effects listed \nabove. One approach to cope with this problem is to use \nagain the potential value (data density), but this time in \nspatial terms inside a frame (as in Figure 2); contrast this to \nthe application of the potential between frames as shown in \nFigure 1.  \nThe logic is inverted \u2013 the point with the maximum value \nof the potential can be used to represent the object of interest \nand it will guarantee a better lock on the target because the \npotential is a measure of data density (spatial density in a \nframe in terms of the position of the pixels) which ignores \nthe noise and outliers in a natural way [19].  \n{ })(maxarg];,[\n1\niPlvhT\nF\ni\nll\n=\n==        (13) \nwhere T denotes the target with its horizontal and vertical \ncomponents. \nThe spatial potential can be calculated recursively in a \nsimilar manner to (5)-(9): \n[ ]( ) ],1[;)(2)()1)()(1(\n1\n, Fjjjjj\nj\nvhP jj =\n\u2212++\u2212\n\u2212\n=\n\u03b3\u03b2\u03b1  (14) \n22)( jj vhj +=\u03b1              (15) \n)()()( 21 jvjhj jj \u03b4\u03b4\u03b3 +=         (16) \n0)1();1()1()( =\u2212+\u2212= \u03b2\u03b1\u03b2\u03b2 jjj      (17) \n0)1();1()1()( 111 =\u2212+\u2212= \u03b4\u03b4\u03b4 jhjj      (18) \n0)1();1()1()( 222 =\u2212+\u2212= \u03b4\u03b4\u03b4 jvjj      (19) \nAs a result the lock to the object that will be tracked is better \nas seen from Figure 2. \nAccording to (13) the pixel with the maximum value of \nthe spatial potential is the focal point of the object that is \nsurrounded by more pixels classified as foreground. \nTherefore, the proposed potential\/density - based way to \nlocate the novelty is more robust if compare to the original \nKDE approach [1].  \nWhen applying both stages of the proposed RDE \napproach expressed by (11) in terms of frames in a window \nof size N (13) in terms of pixel location in the current frame, \none arrives at a result that is faster than the original KDE \napproach by a factor of N (note that N may be as large as the \nspecific task requires) and as precise as KDE \u2013 see Figure 3. \n \n \nFigure 2 Pixels detected as novelty; upper scene uses the mean of all \nsuspected pixels to determine the target; the lower scene uses the \nmaximum of the potential to locate the target. Note the pixels that are \non the right hand side edge of the bottom scene are due to noise. \nThis approach can be extended for automatic object \ntracking when combined with an adaptive model of the \ntargets motion which is presented in the next section. It can \nalso be used for image segmentation [22], landmark \ndetection [23], and self \u2013 localization in robotics [24]. \nAnother practical application is to use such a system in \npatrol and security application in order to automatically \nreduce the huge amount of data not by compression, but by \nextraction of areas from a scene of observation that are \nsuspicious. In such a scenario, a surveillance and patrol \nsystem can automatically focus the attention of the \nsupervisory decision making module or the human operator \nthat usually form the higher level of such systems to the \nsuspicious areas only.   \nIII. REAL-TIME TRACKING USING ETS NEURO-FUZZY \nSYSTEM \nThe evolving Takagi-Sugeno (eTS) fuzzy model [14,15] \nrepresents a fuzzy mixture of Kalman filters (KF) that are \nlocally active. Moreover, the number of the local regions in \nwhich a separate KF operates, are not pre-determined, but \nthey appear or disappear (in other words the structure of the \neTS is evolving).  \n1460 2008 International Joint Conference on Neural Networks (IJCNN 2008)\n  \n \n \nFig. 3. Background Subtraction using the proposed RDE method, \nTop scene is the original frame; middle scene is the modeled one. \nBottom scene is the zoomed version of the middle plot. The red \nsquare denotes the focal point of the foreground. \nLearning eTS is based on: \na) Decomposing the data space (pixel locations in the \ncurrent and next\/predicted images) into local \nsub-areas by eClustering [19]; \nb) Applying fuzzily weighted recursive least squares \n(RLS) approach [15] to determine the parameters of \nthe consequent parts of the fuzzy rules that are being \nformed around centers of the clusters defined by \neClustering; \neTS is trained and used during the same time interval \n(before reading the next data sample) in a prediction-update \nmode in a similar manner to the on-line estimation [25] and \nadaptive control [26]. eTS is an on-line and evolving \nversion of the well known Takagi-Sugeno fuzzy model [29] \nthat can be represented as a neuro-fuzzy system [16] with a \nrule-base that expands or shrinks (evolves) according to the \ndata pattern in the joint input-output (current - next frame) \ndata space (Figure 4). \n \n \n \nFigure 4 Clusters in the joint input-output data space formed by \neClustering and used by eTS to form linear local models; top plot \u2013 \nafter 50 frames (t=50); bottom plot \u2013 after 100 frames (t=100). \n \nIn the tracking problem the aim is to predict the next \nposition of the color blob in the next, (t+1)th frame \nidentified by the RDE (11) followed by (13).  \n( ))()1(^ tTftT =+            (20) \nwhere \n^\n)1( +tT is the prediction of the position of the \ncolor blob in the (t+1)th frame. \neTS neuro-fuzzy system has another advantage \u2013 it can be \nrepresented by linguistically tractable fuzzy rules of the \nfollowing type: \n( ) ( )\n)21(\n)()()1(\n)()()1(\n)()(:\n210\n^\n210\n^\n**\n\u00b0\u00af\n\u00b0\u00ae\n\u00ad\n++=+\n++=+\nthbthbbtv\nthathaathTHEN\nvtocloseistvANDhtocloseisthIFRule\niiii\niiii\niii\n \nwhere C*=(h*,v*) is the centre of a cluster (focal point of \nthe fuzzy rule); i=[1,R]; R is the number of the fuzzy \nrules\/sub-regions of the data space that is formed by the \nvector T=(T(t),T(t+1)); a\n \n and b are the parameters of the \n(linear) consequents. \n2008 International Joint Conference on Neural Networks (IJCNN 2008) 1461\n  \n \nIt is interesting to note that local areas of the data space \nrepresent different parts of the image frame, e.g. \u2018upper \nleft\u2019, \u2018bottom right\u2019, etc. Moreover, in eTS learning the \nlocation of these sub-areas is changing with each new frame \nthat has been processed.  \nEquation (21) represents a fuzzy Takagi-Sugeno model \n[15] that is used for prediction of the position of the color \nblob one step ahead (in the next image frame). One can see \nthat eTS (same as TS) is linear in the consequents part and \nthus it renders the use of well established learning \napproaches such as RLS. In fact, this is partially correct, \nbecause the linearity is correct only locally and eTS (same \nas TS) is non-linear as a whole. Therefore a fuzzily \nweighted version of RLS (wRLS) introduced in [15] is \nnecessary to be applied. It can be applied locally (per fuzzy \nrule and per cluster) or globally. The local implementation \nhas number of advantages, including a better convergence \nproperties, better interpretability, smaller computational \ndemands etc. [15]. The local wRLS can be described as: \n))1()1()(()1(()1()1()1()(\n^^^\n\u2212\u2212\u2212\u2212\u2212\u2212+\u2212= tatxtytxtxtCtata iTiiii \u03bb  (22) \n)1()1()1())1((1\n)1()1()1()1())1(()1()(\n\u2212\u2212\u2212\u2212+\n\u2212\u2212\u2212\u2212\u2212\n\u2212\u2212=\ntxtCtxtx\ntCtxtxtCtx\ntCtC\ni\nT\ni\ni\nT\nii\nii \u03bb\n\u03bb\n   (23) \nwhere TtTtx )](;1[)( = denotes the extended input vector to \nthe eTS;  TtTy )]1(;1[ += denotes the extended output \nvector from the eTS; C denotes the co-variance matrix, \u03bb \ndenotes the normalized firing strength of the ith fuzzy rule \n(to be defined below); the initial conditions for the \nparameters are [26] 0)1( =ia ; ICi \u03a9=)1( with \u03a9 a \nlarge value and I identity matrix. \n  The left hand side part of the eTS model consists of \nlinguistically expressed fuzzy sets with membership \nfunctions of Gaussian type: \nih\nii hth\nih eh\n\u03c3\u03bc 2\n)( *\n)(\n\u2212\n\u2212\n=\n       (24) \nwhere )(hih\u03bc denotes the membership to the fuzzy set \n*)( ihtocloseisth .  \nSimilar membership functions are defined for the vertical \ncomponent, v for each fuzzy rule, i=[1,R]. Note that in eTS \nthe number of fuzzy rules is not pre-determined and fixed as \nin the original Takagi-Sugeno approach [29]. \nThe overall prediction of the next time instant position of \nthe color blob is produced using centre of gravity type \ndefuzzification: \n1\n\u02c6 \u02c6( 1) ( )\nR\ni i\ni\nT t T t\u03bb\n=\n+ = \u00a6        (25) \nwhere \n1\n( ) ( )\n( ) ( )\ni ih iv\nR\nih iv\ni\nh v\nh v\n\u03bc \u03bc\u03bb\n\u03bc \u03bc\n=\n=\n\u00a6  is the normalized firing \nlevel of the ith RLS estimator; \u02c6( 1)iT t + is the \nprediction by the ith fuzzy local sub-model; \u02c6( 1)T t + is \nthe overall prediction. \nIV. EXPERIMENTAL RESULTS AND DISCUSSION \nDifferent tracking sequences (both produced by us in \nlaboratory conditions and available in the public domain \n[31]) were used in order to evaluate the performance of the \nproposed approach. The illumination conditions were \nvarying throughout the sequence. The size of the video \nframes was 255x255 while video length was 120 seconds. \nNote that, in principle, there is no restriction on video \nlengths for the proposed real-time algorithm.  Initially the \ntarget was detected by applying the RDE approach as \ndescribed in Section II. After that, the tracking algorithm \ndescribed in the previous section was applied to the focal \npoint of the detected object identified by (13). \nThe performance of the system was verified at different \nsurrounding conditions and backgrounds. The performance \nwas good despite the noise caused by the rain in the images \n[30]. In contrast to traditional KDE, the proposed recursive \nmethod enables the system to perform without any threshold \nrequired for background modeling. Faster computation and \nlower memory storage requirement is another advantage of \nRDE algorithm. \nTable I Comparison of the performance of KDE and RDE \n \nNote that the time indicated in the Table is when \nimplementing the proposed RDE algorithm in Matlab on a \nPC. If use C language instead of Matlab this time can be \nsignificantly reduced. Also note that RDE  was realised on \nhardware (FPGA) [27] which paves the way to real-time \nimplementations.\u0003\nThe color model is useful to estimate the real position of \nthe object which is needed for supervised learning. At the kth \ntime instance the target position in the next frame is \npredicted using the eTS model. Based on that prediction, the \ncolor of the pixels that are estimated to be in the region of \nthe target are matched with the color model at time (k+1)th. \nThe performance of the eTS model is superior to that of the \nKalman filter in predicting the target position. The values of \nthe target position that were predicted by eTS provides a \n1462 2008 International Joint Conference on Neural Networks (IJCNN 2008)\n  \n \nsmaller mean square error (RMSE) in estimating the true \nlocation comparing to the KF by over 20% as seen from \nTable II and Figure 6. \n \n \n \nFigure 6 Tracking performance of KF vs. eTS (top plot \u2013 horizontal \ncomponent; bottom plot \u2013 vertical component). \nTable II Tracking precision using KF and eTS \nMethod RMSE  \nKF 44.2737 \neTS 35.7623 \nRMSE denotes the error in predicting the positions of the \ntarget comparing to the actual positions at any frame and is \nestimated by the following expression: \n[ ]\nT\nN\ni\niiii\nT\nN\ni\nT\nN\ni\nN\nvvhh\nN\ndist\nN\nrmse\n\u00a6\u00a6\u00a6\n===\n\u2212+\u2212\n===\n1\n22\n1\n2\n1\n2 )~()~(\u03b5\n   (26) \n \nWhere NT is the total number of image frames used in \ntracking; \u03b5  is the error  \n22 )~()~( iiii vvhh \u2212+\u2212=\u03b5  \nAs Figure 6 depicts, overall the predicted values by eTS \nare closer to the actual values comparing to the KF \npredictions, even though at some points especially when the \ntarget abruptly changes its direction the error might be \nhigher.   \nAdditional advantage of using fuzzy models is that the \nresult is transparent and interpretable. In the sequence \n\u2018Rain\u2019 [30] three fuzzy rules were generated by eTS which \ncan be described as: \n( ) ( )1 : ( ) 283 ( ) 9\n\u02c6 ( 1) -4.77 ( ) 0.068 ( ) (27)\n\u02c6 ( 1) 20.75 0.03 ( ) 0.84 ( )\ni\ni\nRule IF h t is close to AND v t is close to\nh t h t v t\nTHEN\nv t h t v t\n\u00ad\n+ = + +\u00b0\u00ae\u00b0 + = \u2212 +\u00af\n \n( ) ( )2 : ( ) 301 ( ) 135\n\u02c6 ( 1) 5.04 0.98( ) 0.01 ( )\n\u02c6 ( 1) 31.48 0.10 ( ) ( )\ni\ni\nRule IF h t is close to AND v t is close to\nh t t v t\nTHEN\nv t h t v t\n\u00ad\n+ = + +\u00b0\u00ae\u00b0 + = \u2212 +\u00af\n \n( ) ( )3 : ( ) 333 ( ) 354\n\u02c6 ( 1) 50.37 0.8 ( ) 0.05 ( )\n\u02c6 ( 1) 248.54 0.94 ( ) .1.18 ( )\ni\ni\nRule IF h t is close to AND v t is close to\nh t h t v t\nTHEN\nv t h t v t\n\u00ad\n+ = + +\u00b0\u00ae\u00b0 + = \u2212 +\u00af\n \nThat is, three separate fuzzily blended RLS estimators \nwere running for the lower left and for the upper right part of \nthe frame respectively. The use of the transparency of the \ntracking model will be further investigated in terms of \ncooperative systems tracking, behavior analysis and \nprediction in application to autonomous systems and \nrobotics.  \nV. CONCLUSION AND FUTURE DIRECTION \nIn this paper we introduce a new approach for autonomous \nnovelty detection in video streams based on recursive \ndensity estimation (RDE) which is in order of magnitude \nfaster than the well known kernel density estimator (KDE) \napproach [2] and as precise as KDE. Additionally it does not \nrequire a user- or problem- specific threshold to be \npre-defined. Moreover, we also introduce a more efficient \nmethod for tracking objects detected by RDE using \nevolving Takagi-Sugeno fuzzy models (eTS) which proved \nto provide smaller (root mean squares) error comparing to \nthe widely used Kalman filter on a range of tested video \nsequences (only one is shown in the paper due to the space \nlimitations). As a combination the proposed RDE approach \nfor autonomous novelty detection and lock to the target and \neTS approach for target tracking provides an efficient \n2008 International Joint Conference on Neural Networks (IJCNN 2008) 1463\n  \n \ntechnique for automation of the tasks typical for \nsurveillance, security, patrol, search and rescue. The \ncomputational simplicity of the proposed approach and \nrealizations of RDE on chip (FPGA) provide a solid basis \nfor real-time implementations. Practical experiments using \nmobile robots Pioneer 3DX were performed in the \nIntelligent Systems Laboratory of Lancaster University. \nREFERENCES \n[1] R. C. Gonzalez, R. E. Woods, \u201cDigital Image Processing\u201d, 2nd \nedition, Prentice Hall, 2002. ISBN:  0201180758. \n[2] A. Elgammal, R. Suraiswami, D. Harwood, and L. Davis, \n\u201cBackground and Foreground modeling using non-parametric Kernel \nDensity Estimation for visual surveillance KDE\u201d, Proc. 2002 IEEE \nVol. 90, no. 7, pp. 1151 \u2013 1163, 2002.  \n[3] R. Cucchiara, C. Grana, M. Piccardi, and A. Prati, \u201cDetecting moving \nobjects, ghosts and shadows in video streams\u201d, IEEE Trans. on Pattern \nAnalysis and Machine Intell., vol. 25, no. 10, Oct. 2003, pp. \n1337-1342.  \n[4] S.-C. Cheung and C. Kamath, \u201cRobust techniques for background \nsubtraction in urban traffic video\u201d, In Proc. SPIE, Electronic Imaging \nVideo Communications and Image Processing, Vol. 5308, San Jose, \nJan. 2004, pp. 881-892. \n[5] B. Han, D. Comaniciu, and L. Davis, \u201cSequential kernel density \n approximation through mode propagation: applications to background \n modeling\u201d, In Proc. ACCV -Asian Conf. on Computer Vision, 2004. \n[6] R.T. Collins, O. Amidi, and T. Kanade \u201cAn Active Camera System \nAcquiring Multi-View Video\u201d, In Proc. 2002 International \nConference on Image Processing (ICIP '02), pp. 517 \u2013 520.  \n[7] A. Memmon, P. Angelov, H. Ahmed, \u201cAn Approach to Real-Time \nColor-based Object Tracking\u201d, In Proc. 2006 International \nSymposium on Evolving Fuzzy Systems, 7-9 September, 2006, \nAmbleside, Lake District, UK, IEEE Press, pp.81-87.  \n[8] P. X. Liu, M. Q.-H. Meng (2004) Online Data-Driven Fuzzy Clustering \nwith Application to Real-Time Robotic Tracking, IEEE Transactions \non Fuzzy Systems, vol. 12, no. 4, pp. 516-523. \n[9] S. Ribari\u00fc, G. Adrinek, and S. \u0160egvi\u00fc, Real-time active visual tracking \nsystem, In Proc. 12th IEEE Mediterranean Electro-technical \nConference, Dubrovnik, Hrvatska, 2004, pp.231-234. \n[10] C. Rasmussen, K. Toyama, and G. D. Hager, Tracking Objects By \nColor Alone. Technical Report TR1114, Dept. of Computer Science \nYale University, June 1996. \n[11] T. Nakamura and T. Ogasawara, On-line Visual Learning Method \nfor Color Image Segmentation and Object Tracking, In Proc. IEEE\/RSJ \nIntern.  Conf. on Intelligent Robots and Syst., vol.1 pp. 221\u2013228, 1999. \n[12] C. Wren, A. Azerbayejani, T. Darnell, and A. Pentland, Pfinder: \nReal-time Tracking of the Human Body. IEEE trans. on PAMI., vol. \n19, no 7, pp. 780\u2013785, 1997.  \n[13] R. E. Kalman, A New Approach to linear filtering and prediction \nproblem, Transactions of the ASME, Ser. D,  Journal of Basic \nEngineering, vol. 82, pp.34-45, 1960. \n[14] P. Angelov, D. Filev, \u201cAn Approach to On-line Identification of \nTakagi-Sugeno Fuzzy Models\u201d, IEEE Trans. on System, Man, and \nCybernetics, part B - Cybernetics, vol.34, No1, pp.484-498. ISSN \n1094-6977, 2004.  \n[15] P. Angelov, X. Zhou, \u201cEvolving fuzzy systems from data streams in \nReal time,\u201d In Proc. 2006 International Symposium on Evolving Fuzzy \nSystems, Ambleside, Lake District, UK, IEEE Press, pp. 29-35, 2006. \n[16] I. Pavlidis, V. Morellas, P. Tsiamyrtzis, and S. Harp, \u201cUrban \n6surveillance systems: from the laboratory to the commercial world,\u201d \nIn Proc. of the IEEE, vol. 89, no. 10, pp. 1478 -1497, 2001. \n[17] R. R. Yager, D. P. Filev, \"Learning of fuzzy rules by mountain \nclustering, Proc. SPIE Conf. on Appl. of Fuzzy Logic Technology, \nBoston, MA, USA, pp. 246-254, 1993. \n[18] S. L. Chiu, \"Fuzzy model identification based on cluster \nestimation\", Journal of Intelligent and Fuzzy Systems, vol. 2, pp. \n267-278, 1994. \n[19] P. Angelov, \u201cAn Approach for Fuzzy Rule-base Adaptation using \nOn-line Clustering\u201d, International Journal of Approximate Reasoning, \nVol. 35 , No 3, March 2004, pp. 275-289. \n[20] E. Parzen, On estimation of a probability density function and \nmode, Annual Mathematical Statistics, vol. 33, pp.1065-1076, 1962.  \n[21] D. Specht, A general regression neural network, IEEE Transactions \non Neural Networks, vol. 2 , No 6, pp. 568-576, 1991. \n[22] C. Alzate, J. A. K. Suykens, \u201cImage segmentation using Weighted \nKernel PCA Approach to Spectral Clustering\u201d, In Proc. IEEE Intern. \nConf. on Comp. Intell. Applicat. for Image and Signal Processing, \nCIISP 2007, 1-5 April 2007, Honolulu, HI, USA, pp.220-225. \n[23] X. Zhou, P. Angelov, \u201cReal-Time joint Landmark Recognition and \nClassifier Generation by an Evolving Fuzzy System\u201d, In Proc. 2006 \nWorld Congress on Computational Intelligence, Vancouver, Canada \n16-21 July 2006, pp. pp.6314-6321, ISBN 0-7803-9489-5. \n[24] X. Zhou, P. Angelov, \u201cAutonomous Visual Self-localization in \nCompletely Unknown Environment using Evolving Fuzzy Rule-based \nClassifier\u201d, Proc. IEEE Intern. Conf. on Comp. Intelligence Applic. for \nDefense and Security, Honolulu, USA, 1-5 April 2007, pp. 131-138. \n[25] Haykin S., Adaptive Filter Theory, 4th ed. Upper Saddle RiverNJ, \nUSA, Prentice Hall, 2002. \n[26] Astrom K., B. Wittenmark, \u201cComputer Controlled Systems: Theory \nand Design,\u201d NJ USA, Prentice Hall, 1984. \n[27] M. Everett, P. Angelov, EvoMap: On-Chip Implementation of \nIntelligent Information Modelling using EVOlving MAPping, \nTechnical Report, 2005, Lancaster University, UK, pp.1-15. \n[28] P. Angelov, \u201cEvolving Rule-based Models: A Tool for Design of \nFlexible Adaptive Systems\u201d. Heidelberg, Germany: Springer, 2002. \n[29] T. Takagi, M. Sugeno, \"Fuzzy identification of systems and its \napplication to modeling and control\", IEEE Trans. on Systems, Man \nand Cybernetics, vol. 15, pp. 116-132, 1985. \n[30]http:\/\/www.cs.rutgers.edu\/~elgammal\/Research\/BGS\/research_bgs.ht\nm Date of access: 1st June 2007. \n[31] Z. Zhivkovic, F. van der Heijden, \u201cEfficient adaptive density \nestimation per image pixel for the task of background subtraction\u201d, \nPattern Recognition Letters, to appear, doi \n10.1016\/j.patrec.2005.11.005 \n[32] A. Hampapur, et al., \u201cSamrt Video Surveilance: Exploring the concept \nof multiscale spatiotemporal tracking\u201d, IEEE Signal Processing \nMagazine, March 2005, pp. 38-51. \n[33] G. Foresti, C. Micheloni, L. Snidaro, P. Remagnino, T. Elis, \u201cActive \nVideo-Based Surveilance System: The low-level image and video \nprocessing techniques needed for implementation\u201d, IEEE Signal \nProcessing Magazine, March 2005, pp.25-27. \n \n1464 2008 International Joint Conference on Neural Networks (IJCNN 2008)\n"}