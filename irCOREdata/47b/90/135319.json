{"doi":"10.1016\/j.ejps.2005.04.010","coreId":"135319","oai":"oai:bradscholars.brad.ac.uk:10454\/3011","identifiers":["oai:bradscholars.brad.ac.uk:10454\/3011","10.1016\/j.ejps.2005.04.010"],"title":"Optimisation of the predictive ability of artificial neural network (ANN) models: A comparison of three ANN programs and four classes of training algorithm.","authors":["Rowe, Raymond C.","Plumb, A.P.","York, Peter","Brown, M."],"enrichments":{"references":[],"documentType":{"type":null}},"contributors":[],"datePublished":"2005","abstract":"NoThe purpose of this study was to determine whether artificial neural network (ANN) programs implementing different backpropagation algorithms and default settings are capable of generating equivalent highly predictive models. Three ANN packages were used: INForm, CAD\/Chem and MATLAB. Twenty variants of gradient descent, conjugate gradient, quasi-Newton and Bayesian regularisation algorithms were used to train networks containing a single hidden layer of 3\u00bf12 nodes.\\ud\n\\ud\nAll INForm and CAD\/Chem models trained satisfactorily for tensile strength, disintegration time and percentage dissolution at 15, 30, 45 and 60 min. Similarly, acceptable training was obtained for MATLAB models using Bayesian regularisation. Training of MATLAB models with other algorithms was erratic. This effect was attributed to a tendency for the MATLAB implementation of the algorithms to attenuate training in local minima of the error surface. Predictive models for tablet capping and friability could not be generated.\\ud\n\\ud\nThe most predictive models from each ANN package varied with respect to the optimum network architecture and training algorithm. No significant differences were found in the predictive ability of these models. It is concluded that comparable models are obtainable from different ANN programs provided that both the network architecture and training algorithm are optimised. A broad strategy for optimisation of the predictive ability of an ANN model is proposed","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":null,"rawRecordXml":"<record><header><identifier>\n        \n            \n                oai:bradscholars.brad.ac.uk:10454\/3011<\/identifier><datestamp>\n                2016-08-18T17:06:12Z<\/datestamp><setSpec>\n                com_10454_152<\/setSpec><setSpec>\n                col_10454_6342<\/setSpec>\n            <\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:doc=\"http:\/\/www.lyncode.com\/xoai\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n            \nOptimisation of the predictive ability of artificial neural network (ANN) models: A comparison of three ANN programs and four classes of training algorithm.<\/dc:title><dc:creator>\nRowe, Raymond C.<\/dc:creator><dc:creator>\nPlumb, A.P.<\/dc:creator><dc:creator>\nYork, Peter<\/dc:creator><dc:creator>\nBrown, M.<\/dc:creator><dc:subject>\nArtificial neural network (ANN);<\/dc:subject><dc:subject>\nNetwork architecture<\/dc:subject><dc:subject>\nTraining algorithm<\/dc:subject><dc:subject>\nDirect compression tablet formulation<\/dc:subject><dc:description>\nNo<\/dc:description><dc:description>\nThe purpose of this study was to determine whether artificial neural network (ANN) programs implementing different backpropagation algorithms and default settings are capable of generating equivalent highly predictive models. Three ANN packages were used: INForm, CAD\/Chem and MATLAB. Twenty variants of gradient descent, conjugate gradient, quasi-Newton and Bayesian regularisation algorithms were used to train networks containing a single hidden layer of 3\u00bf12 nodes.\\ud\n\\ud\nAll INForm and CAD\/Chem models trained satisfactorily for tensile strength, disintegration time and percentage dissolution at 15, 30, 45 and 60 min. Similarly, acceptable training was obtained for MATLAB models using Bayesian regularisation. Training of MATLAB models with other algorithms was erratic. This effect was attributed to a tendency for the MATLAB implementation of the algorithms to attenuate training in local minima of the error surface. Predictive models for tablet capping and friability could not be generated.\\ud\n\\ud\nThe most predictive models from each ANN package varied with respect to the optimum network architecture and training algorithm. No significant differences were found in the predictive ability of these models. It is concluded that comparable models are obtainable from different ANN programs provided that both the network architecture and training algorithm are optimised. A broad strategy for optimisation of the predictive ability of an ANN model is proposed.<\/dc:description><dc:date>\n2009-07-14T14:21:32Z<\/dc:date><dc:date>\n2009-07-14T14:21:32Z<\/dc:date><dc:date>\n2005<\/dc:date><dc:type>\nArticle<\/dc:type><dc:type>\nNo full-text available in the repository<\/dc:type><dc:identifier>\nRowe, R.C., Plumb, A.P., York, P. and Brown, M. (2005)  Optimisation of the predictive ability of artificial neural network (ANN) models: A comparison of three ANN programs and four classes of training algorithm.  European Journal of Pharmaceutical Sciences. Vol. 25, No. 4-5, pp. 395-405.<\/dc:identifier><dc:identifier>\n90008331<\/dc:identifier><dc:identifier>\n900001090<\/dc:identifier><dc:identifier>\nhttp:\/\/hdl.handle.net\/10454\/3011<\/dc:identifier><dc:language>\nen<\/dc:language><dc:relation>\nhttp:\/\/dx.doi.org\/10.1016\/j.ejps.2005.04.010<\/dc:relation>\n<\/oai_dc:dc>\n<\/metadata>\n        <\/record>","journals":null,"language":null,"relations":["http:\/\/dx.doi.org\/10.1016\/j.ejps.2005.04.010"],"year":2005,"topics":["Artificial neural network (ANN);","Network architecture","Training algorithm","Direct compression tablet formulation"],"subject":["Article","No full-text available in the repository"],"fullText":null}