{"doi":"10.1007\/11617983_30","coreId":"102478","oai":"oai:epubs.surrey.ac.uk:1941","identifiers":["oai:epubs.surrey.ac.uk:1941","10.1007\/11617983_30"],"title":"Nested codes for constrained memory and for dirty paper","authors":["Schaathun, HG","Cohen, GD"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2006","abstract":"Dirty paper coding are relevant for wireless networks, multiuser channels, and digital watermarking. We show that the problem of dirty paper is essentially equivalent to some classes of constrained memories, and we explore the binary so-called nested codes, which are used for efficient coding and error-correction on such channels and memories. \u00a9 Springer-Verlag Berlin Heidelberg 2006","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:epubs.surrey.ac.uk:1941<\/identifier><datestamp>\n      2017-10-31T14:03:44Z<\/datestamp><setSpec>\n      74797065733D636F6E666572656E63655F6974656D<\/setSpec><setSpec>\n      6469766973696F6E733D656E67616E64706879736963616C736369656E636573:436F6D707574696E67<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/epubs.surrey.ac.uk\/1941\/<\/dc:relation><dc:title>\n        Nested codes for constrained memory and for dirty paper<\/dc:title><dc:creator>\n        Schaathun, HG<\/dc:creator><dc:creator>\n        Cohen, GD<\/dc:creator><dc:description>\n        Dirty paper coding are relevant for wireless networks, multiuser channels, and digital watermarking. We show that the problem of dirty paper is essentially equivalent to some classes of constrained memories, and we explore the binary so-called nested codes, which are used for efficient coding and error-correction on such channels and memories. \u00a9 Springer-Verlag Berlin Heidelberg 2006.<\/dc:description><dc:date>\n        2006<\/dc:date><dc:type>\n        Conference or Workshop Item<\/dc:type><dc:type>\n        NonPeerReviewed<\/dc:type><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/epubs.surrey.ac.uk\/1941\/1\/fulltext.pdf<\/dc:identifier><dc:identifier>\n          Schaathun, HG and Cohen, GD  (2006) Nested codes for constrained memory and for dirty paper       <\/dc:identifier><dc:relation>\n        10.1007\/11617983_30<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/epubs.surrey.ac.uk\/1941\/","10.1007\/11617983_30"],"year":2006,"topics":[],"subject":["Conference or Workshop Item","NonPeerReviewed"],"fullText":"Nested codes for constrained memory and for\ndirty paper\nHans Georg Schaathun1 and G\u00e9rard D. Cohen2\n1 Dept. Informatics,\nUniversity of Bergen\nPb. 7800\nN-5020 Bergen\nNorway\n2 Dept. Informatique et Reseaux,\nEcole Nationale Sup\u00e9rieure des T\u00e9l\u00e9communications\n46, rue Barrault,\nF-75634 Paris Cedex 13\nFrance\nAbstract Dirty paper coding are relevant for wireless networks, mul-\ntiuser channels, and digital watermarking. We show that the problem of\ndirty paper is essentially equivalent to some classes of constrained mem-\nories, and we explore the binary so-called nested codes, which are used\nfor e\ufb03cient coding and error-correction on such channels and memories.\nKeywords: dirty paper, constrained memory, nested codes, covering codes\nThe motivation of this paper is the dirty paper channel introduced by Costa\n[3]. This channel has received increased attention [4] in recent years, due to\napplications in wireless multiuser networks and digital \ufb01ngerprinting [5].\nWe show that the dirty paper channel is practically equivalent to writing on\nreluctant memories, and we make a few improvements on the existing results for\nsuch channels. Our interest is mainly in the binary dirty paper channel (BDP).\n1 Dirty paper and constrained memory coding\nThe dirty paper channel is depicted in Figure 1. There are two independent noise\nsources which are added to the transmitted signal to form the received signal.\nThe \ufb01rst noise vector, which we will call the state of the channel is known to\nthe sender but not to the receiver. The second noise vector, which we will refer\nto as noise is unknown to both.\nThe sender is subject to a power constraint ||x|| \u2264 P on the transmitted\nsignal. For a binary channel || \u00b7 || is usually the Hamming norm; for a continuous\nchannel it is usually the Euclidean norm.\nCosta [3] introduced this channel with Gaussian sources for both the state\nand the noise. His surprising result was that the channel capacity depends only on\nthe intensity of the noise; the intensity of the state does not change capacity. In\ns n??y ??y\nm\u2212\u2212\u2212\u2212\u2212\u2192 Enc. x\u2212\u2212\u2212\u2212\u2212\u2192 L y\u2212\u2212\u2212\u2212\u2212\u2192 L r\u2212\u2212\u2212\u2212\u2212\u2192 Dec. m\u02dc\u2212\u2212\u2212\u2212\u2212\u2192\nFigure 1. The dirty paper channel.\nmore recent years, his results have been generalised to other source distributions.\nWe will consider the binary dirty paper channel (BDP).\nIn a constrained memory, there are restrictions on writing to the memory,\nsuch that starting in one memory state, some states are reachable in one write\noperation and others are not. For each memory state, there is a feasible region\nof words which may be used to represent the next message. In this case the state\nis given by the previous message stored in memory.\nDirty paper coding and constrained memory coding are similar, in fact BDP\nchannels are practically equivalent to WRM (write reluctant memories) with\nerror-correction [2]. In WRM, one write operation cannot change more than a\ncertain number P of bits. This corresponds to the power constraint in BDP; if\ns is the state (previous contents), x is the change, and y = s+ x is the memory\ncontents after writing, then w(x) \u2264 P .\nThe state on dirty paper channels is externally given, whereas in constrained\nmemories it is the old codeword (with possible errors). The state, together with\npower constraints, de\ufb01nes the feasible region of vectors y which can be generated.\nFor BDP\/WRM, the feasible region is a Hamming sphere around the state.\nRemark 1. Occasionnally, in constrained memories, one assumes that s is a code-\nword with few errors, since nobody would write rubbish to the memory. We will\nnot make this assumption, for two reasons. Primarily, it does not extend to BDP.\nAlso, we know of no cases where results can be improved due to this assumption.\nFurthermore, by avoiding such assumption, the system can recover after an error\npattern which could not be corrected.\nExample 1. Another example of constrained memory is the Write Isolated Mem-\nory (WIM), where two consecutive memory bits cannot be changed in the same\noperation. In other words, the feasible region is the set {x+s : x = (x1, . . . , xn), xi =\n1\u21d2 xi\u22121 = xi+1 = 0}, where s is the memory state and x0 = xn+1 = 0 by con-\nvention.\nBDP (WRM) and WIM both fall into a class of channels, where the feasible\nregions are translation invariant, permitting some common techniques. By this\nwe mean that if Fs is the feasible region from s, then Fs\u2032 = Fs \u2212 s + s\u2032. Let us\ncall this class CCTIR (constrained channels with translation invariant regions).\n2 Some coding theory\nAn (n,M)q code C is an M -set of n-tuples over a q-ary alphabet. When q = 2\nwe may suppress the subscript. The Hamming distance d(x,y) is the number\nof positions where the two tuples di\ufb00er. The minimum distance d = d(C) of\nC is the least distance between two di\ufb00erent codewords. We say that C is an\n(n,M, d)q code. The covering radius r of C is the largest distance between a\nvector y \u2208 Qn and the code.\nThe problem of covering codes amounts to \ufb01nding codes minimising r given\nn and M , whereas the problem of error-correcting codes is about maximising d\ngiven n and M .\nWe also de\ufb01ne normalised measures, which will be useful when moving to\nasymptotic codes. We de\ufb01ne the rate logqM\/n, the distance \u03b4 = d\/n, and the\ncovering radius \u03c1 = r\/n.\n3 Codes for CCTIR\nIn order to make a successful code for CCTIR, we need for every state s and\nevery message m, to have at least one codeword x corresponding to m in the\nfeasible region of s. Furthermore, we require any capability for error-correction\nthat we may need. We will study e-error correcting CCTIR codes.\nLemma 1. For CCTIR, if x \u2208 Fy then y \u2208 Fx.\nLet Bi be the set of words corresponding to message i. We require that for\nany s, Fs \u2229Bi 6= \u2205. By the lemma above, this is equivalent to\u22c3\nb\u2208Bi\nFb = Fn, (1)\ni.e. that the feasible regions around the words of Bi cover the space. If the set\nof possible messages is i = 1, . . . ,M , then we de\ufb01ne\nCF =\nM\u22c3\ni=1\nBi.\nWhen the feasible regions are spheres of radius \u03c1, this is to say that Bi must be\na covering code of covering radius \u03c1 or smaller. For other feasible regions it is a\nmore general covering by F -shapes.\nIn order to correct up to e errors, we require that if i 6= j, then d(Bi, Bj) > 2e.\nIt is su\ufb03cient to require that CF has minimum distance 2e+1 or more; i.e. that\nCF is e-error correcting. Furthermore as a necessary condition, if there are two\ncodewords with distance at most 2e apart, they must fall in the same set Bi.\nIn a sense, we try to pack the space with coverings Bi such that we maintain\na minimum distance of 2e+ 1, a problem studied in [2].\nWe say that a CCTIR code (B1, . . . , BM ) is linear if CF is a linear e-error-\ncorrecting code, Bj is a subcode satisfying (1) for some j, and the Bi are cosets\nof Bj in CF . Clearly by linearity, Bi satis\ufb01es (1) whenever Bj does.\nLet an = #F0. For CCTIR, all the feasible regions clearly have the same\nsize.\nLemma 2. For an (n,M) CCTIR code, we have\nM \u2264 an\nLemma 3. For dirty paper codes, we have\nan = V (n,R) =\nn\u2211\ni=0\n(\nn\ni\n)\n.\nIn the case of WRM and dirty paper channels, a linear CCTIR code is also\ncalled a nested code. We call CF the \ufb01ne code and CC \u2286 CF the coarse code.\nThe nested code is the quotient C = CF \/CC , and we say that C is an [n,K; d1, \u03c1]\ncode, where K = kF \u2212 kC is the dimension of C. The following lemma is well\nknown.\nLemma 4 (Supercode lemma). For any [n,K; d1, \u03c1] nested code, we have\n\u03c1 \u2265 d1.\n4 Asymptotic existence\nDe\ufb01nition 1 (Entropy). The (binary) entropy of a discrete stochastic variable\nX drawn from a set X is de\ufb01ned as\nH(X) = \u2212\n\u2211\nx\u2208X\nP (X = x) logP (X = x).\nThe conditional entropy of X with respect to another discrete stochastic variable\nY from Y is\nH(X|Y ) = \u2212\n\u2211\ny\u2208Y\nP (Y = y)\n\u2211\nx\u2208X\nP (X = x|Y = y) logP (X = x|Y = y).\nThe following general theorem appears in [2].\nTheorem 1. For n large enough, there are \u03b8n-error correcting codes for CCTIR\nwith rate\n\u03ba(\u03b8) \u2265 \u03ba0 \u2212H(2\u03b8),\nwhere \u03ba0 is the maximum rate for a non-error-correcting code for the same con-\nstrained channel.\nThe proof is by greedy techniques, as follows.\nProof. We write\nS(B, i) =\n\u22c3\nb\u2208B\n{x : d(x,b \u2264 i}.\nFirst we make a code CC of rate 1 \u2212 \u03ba without error-correction. Let S0 =\nS(CC , 2\u03b8n\u2212 1).\nWe start with B = {0}, and construct a code CC by the following greedy\nalgorithm. In each step we take a random vector y \u2208 S\\S(B + CC , 2\u03b8n \u2212 1),\nand update B to be the linear span of y and the vectors of B. We proceed until\nS\\S(B+CC , 2\u03b8n\u2212 1) is empty. Since each word included in B excludes at most\n#S(CC , 2\u03b8n\u2212 1) elements from S0, we get that\n#B \u2265 2\nn\n#CC#S({0}, 2\u03b8n\u2212 1) \u2265\n2\u03ban\n#S({0}, 2\u03b8n\u2212 1) .\nAssymptotically, we have #B \u2248 2(\u03ba\u2212H(2\u03b8))n, Let CF = B + CC , so that C =\nCF \/CC \u2261 B. Clearly the rate of B and C is \u03ba\u2212H(2\u03b8) as required.\nIn the case of dirty paper channel, \u03ba0 = 1 \u2212 R\u03c1 where R\u03c1 is the minimum\nrate for a covering code with appropriate \u03c1.\nTheorem 2. For dirty paper codes with no error-correction, we can obtain rate\n\u03ba0 = H(\u03c1).\nObserve that whenever \u03c1 > \u03b4, we get asymptotic codes with non-zero rate\nfrom the above theorems. For \u03c1 = \u03b4, however, the guaranteed rate is just zero.\nProblem 1 Are there asymptotic families of nested codes with R > 0 and \u03c1 = d?\n5 Some small constructions\nParameters Coarse code Fine code\n[3, 1; 1, 1]\n\u00bb\n110\n101\n\u2013 24100010\n001\n35\n[3, 1; 2, 2]\n\u02c6\n110\n\u02dc \u00bb110\n011\n\u2013\n[3, 2; 1, 2]\n\u02c6\n110\n\u02dc 24100010\n001\n35\nTable 1. Some nested codes for n = 3.\nLemma 5. For any [n,K; 1, 1] nested code with even n, we have K \u2264 log n.\nParameters Coarse code Fine code\n[4, 2; 1, 1]\n\u00bb\n1110\n1001\n\u2013 2664\n1000\n0100\n0010\n0001\n3775\n[4, 2; 2, 2]\n\u02c6\n1111\n\u02dc 2411001010\n1001\n35\n[4, 3; 1, 2]\n\u02c6\n1111\n\u02dc 2664\n1000\n0100\n0010\n0001\n3775\n[5, 2; 1, 1]\n241110010011\n00110\n35 [5, 5, 1] full code\n[5, 4; 1, 2]\n\u02c6\n11111\n\u02dc\n[5, 5, 1] full code\n[5, 2; 2, 2]\n\u02c6\n11111\n\u02dc 241100010100\n10010\n35\n[6, 2; 2, 2]\n\u00bb\n111000\n000111\n\u2013 2664\n111000\n000111\n100100\n010010\n3775\n[6, 4; 1, 2]\n\u00bb\n111000\n000111\n\u2013\n[6, 6, 1] full code\n[6, 4; 2, 3]\n\u02c6\n111111\n\u02dc\n[6, 5, 2] even weight\nTable 2. Some nested codes for n = 4, 5, 6.\nProof. For a [n, kC ]1 covering code, we have kC \u2264 n\u2212 log n when n is even, and\nfor an [n, kF , 1] code, we have kF \u2264 n. Hence K = kF \u2212 kC \u2264 log n.\nLemma 6. There is a [2K \u2212 1,K; 1, 1] nested code for any K.\nProof. Let the coarse code be the [2K \u2212 1, 2K \u2212 1\u2212K, 3]1 Hamming code, and\nlet the \ufb01ne code be the [2K \u2212 1, 2K \u2212 1, 1] code.\nLemma 7. There are [2K ,K; 1, 1] and [2K \u2212 1,K; 1, 1] nested codes.\nProof. The \ufb01ne code is the [n, n, 1] full space. The coarse codes are the Hamming\ncodes, and the [2K , 2K \u2212 K, 1] direct sum of a Hamming code padded with a\nzero column, and the code generated by a single word of weight one.\nLemma 8. There are [2K ,K; 2K\u22121, 2K\u22121] and [2K \u2212 1,K; 2K\u22121 \u2212 1, 2K\u22121 \u2212 1]\nnested codes for any positive K.\nProof. Let the coarse code be the [2K , 1, 2K ] repetition code, and let the \ufb01ne code\nbe the [2K ,K, 2K\u22121] Reed-Muller code. The second set of parameters comes from\npuncturing the above code.\nLemma 9. If there is an [n,K; d, \u03c1] code, then there is an [n \u2212 1,K; d \u2212 1, \u03c1]\ncode by puncturing and an [n\u2212 1,K; d, \u03c1+ 1] code by shortening.\nProof. This follows easily from the standard results on puncturing and shorten-\ning of error-correcting and covering codes.\nLemma 10 ([2]). The [2m\u2212 1, 2m\u2212 1\u2212 2m, 5] BCH code has \u03c1 = 3 for m \u2265 3.\nCorollary 1. There is a [2m \u2212 1,m; 3, 3] nested code for every m \u2265 3.\nProof. The coarse code is the [2m \u2212 1, 2m \u2212 1 \u2212 2m, 5]3 BCH(2) code, and the\n\ufb01ne code is the Hamming code.\nLemma 11 ([2]). The [2m\u2212 1, 2m\u2212 1\u2212 3m, 7] BCH code has \u03c1 = 5 for m \u2265 4.\nCorollary 2. There is a [2m \u2212 1,m; 5, 5] nested code for every m \u2265 4.\nProof. The coarse code is the [2m \u2212 1, 2m \u2212 1 \u2212 3m, 7]5 BCH(3) code, and the\n\ufb01ne code is the [2m \u2212 1, 2m \u2212 1\u2212 2m, 5]3 BCH(2) code.\nLemma 12. There are [22m+1\u2212 2m, 2m+2; 22m\u2212 2m, 22m] and [22m+1\u2212 2m\u2212\n1, 2m+ 2; 22m \u2212 2m \u2212 1, 22m \u2212 1] nested codes\nProof. The coarse code is a repetition code. The \ufb01ne code is a [22m+1\u22122m, 2m+\n3, 22m \u2212 2m] code [1] or a punctured version of it.\nLemma 13. There is no [6, 4; 2, 2] nested code, so the [6, 3; 2, 2], [6, 4; 1, 2] and\n[6, 4; 2, 3] codes are optimal.\nProof. The smallest covering code of \u03c1 = 2 and n = 6 has kC = 2, so to get\nK = 4, we would need kF \u2265 6, which would give d = 1.\nParameters Coarse code Fine code\n[7, 3; 1, 1] [7, 4; 3]1 Hamming [7, 7, 1]\n[7, 3; 3, 3] [7, 1, 7]3 repetition [7, 4, 3] Hamming\n[8, 3; 1, 1]\n266664\n10001110\n01000110\n00101010\n00010110\n00000001\n377775 [8, 8, 1]\n[8, 3; 4, 4] [8, 1, 8]4 repetition [8, 4, 4] ext. Hamming\n[15, 4; 3, 3] [15, 7, 5]3 BCH(2) [15, 11, 3] Hamming\n[15, 2; 5, 5] [15, 5, 7]5 BCH(3) [15, 7, 5]3 BCH(2)\n[15, 2; 7, 7] [15, 1, 15]7 repetition [15, 3, 7] BCH(3)\n[15, 4; 7, 7] [15, 1, 15]7 repetition [15, 5, 7] punctured Reed-Muller\n[16, 6; 4, 6] [16, 5, 8]6 RM(1, 4) [16, 11, 4] RM(2, 4)\n[16, 4; 8, 8] [16, 1, 16]8 repetition [16, 5, 8] Reed-Muller\n[27, 6; 11, 13] [27, 1, 27]13 repetition [27, 7, 11] [1]\n[28, 6; 12, 14] [28, 1, 28]14 repetition [28, 7, 12] [1]\n[31, 5; 3, 3] [31, 21, 5]3 BCH(2) [31, 26, 3] Hamming\n[31, 5; 5, 5] [31, 16, 7]5 BCH(3) [31, 21, 5]3 BCH(2)\n[31, 5; 7, 7] [31, 11, 11]7 BCH(4) [31, 16, 7]5 BCH(3)\n[31, 5; 11, 11] [31, 6, 15]11 BCH(6) [31, 11, 11]7 BCH(4)\n[31, 5; 15, 15] [31, 1, 31]15 repetition [31, 6, 15] punctured Reed-Muller\n[32, 5; 2, 2] [32, 26, 4]2 RM(3, 5) [32, 31, 2] RM(4, 5)\n[32, 10; 4, 6] [32, 16, 8]6 RM(2, 5) [32, 26, 4] RM(3, 5)\n[32, 10; 8, 12] [32, 6, 16]12 RM(1, 5) [32, 16, 8] RM(2, 5)\n[36, 20; 4, 13] [36, 8, 16]\u03c1 \u03c1 \u2264 13 [36, 28, 4] C\u22a5C\n[49, 9; 20, 24] [49, 1, 49]24 repetition [49, 10, 20] [1]\n[63, 6; 1, 1] [63, 57, 3]1 BCH(1) [63, 63, 1] full code\n[63, 6; 3, 3] [63, 51, 5]3 BCH(2) [63, 57, 3]1 BCH(1)\n[63, 6; 5, 5] [63, 45, 7]5 BCH(3) [63, 51, 5]3 BCH(2)\n[63, 6; 7, 7] [63, 39, 9]7 BCH(4) [63, 45, 7]5 BCH(3)\n[63, 3; 9, 9] [63, 36, 11]9 BCH(5) [63, 39, 9]7 BCH(4)\n[64, 15; 4, 8] (u, u+ v) construction\n[64, 15; 16, 28] [64, 7, 32]28 RM(1, 6) [64, 22, 16] RM(2, 6)\nTable 3. Some nested codes for n \u2265 7.\n6 Some upper bounds on the nested code dimension\nLemma 14. For an [n,K; d, d] nested code, we have\n2K \u2264\n(\nn\nd\n)\n+ 1.\nProof. Consider the points of CC and the balls of radius \u03c1 = d around these\npoints. Because \u03c1 is the covering radius of CC , these balls cover the space. Since\nCF has minimum distance d = \u03c1, it can only contain points on the border of\nthese balls, besides the points of CC . Hence\n#CF \u2264 #CC \u00b7\n((\nn\nd\n)\n+ 1\n)\n,\nand hence\n#(CF \/CC) \u2264\n((\nn\nd\n)\n+ 1\n)\n,\nas required.\nWe have seen that this bound can be met with equality for \u03c1 = 1. For \u03c1 > 1\nexcept \u03c1 = n = 2 we have inequality; let's see this for \u03c1 = 2 \ufb01rst.\nProposition 1. For an [n,K; 2, 2] nested code with n > 2, we have\n2K <\n(\nn\n2\n)\n+ 1.\nProof. Suppose the bound were met with equality. Since CC is a covering code\nof \u03c1 = 2, we have\n2n \u2264 2kC\n(\n1 +\n(\nn\n1\n)\n+\n(\nn\n2\n))\n\u2264 2kC (2K + n) \u2264 2kF + 2kCn.\nFor all n > 2, we have\nn < 1 +\n(\nn\n2\n)\n,\nwhich is equal to 2K by assumption. This gives\n2n < 2kF+1,\nand clearly n \u2265 kF , so we get n = kF ; but then d = 1 < 2, giving a contradiction.\nWe do have degenerate [n, 1;n, n] nested codes for all n. They have only the\nzero word for CC , an [n, 1, n] repetition code for CF .\nProposition 2. For an [n,K; d, d] nested code, we have\n2K \u2264 A(n, d, d) + 1.\nIt is readily seen that this bound is stronger than Lemma 14 when \u03c1 > 2.\nProof. We start as we did proving Lemma 14 with the balls of radius \u03c1 around\nthe points of CC . The border of the ball around x are the points x + y where\ny has weight \u03c1. Obeying the distance requirement, the y that we choose for CF\nfrom this ball, will have to form a constant weight code of weight and distance\n\u03c1 = d.\nGeneralising, we get the following proposition, for which we ommit the proof.\nProposition 3. For an [n,K; d, \u03c1] nested code, we have\n2K \u2264 1 +\n\u03c1\u2211\nw=d\nA(n, d, w).\n7 Some constructions\nTheorem 3. Let U = UF \/UC and V = VF \/VC be [n,KU ; dU , \u03c1U ] and [n,KV ; dV , \u03c1V ]\nnested codes. Let Ui \u25e6 Vi denote the (u,u+ v) composition of UI and UV . Then\nwe can form a nested code C = U \u25e6 V = (UF \u25e6 VF )\/(UC \u25e6 UF ), and C is a\n[2n,KU +KV ; d, \u03c1] nested code with \u03c1 \u2264 \u03c1U + \u03c1V and d = min{2dV , dU}.\nThe proof is obvious from fundamental results on the parameters of the\ncomponent codes.\n8 Acknowledgements\nThe authors are grateful for codes contributed by Carl Bracken of Dublin, and\nfor the interest of and discussions with Wolfgang Willems of Magdeburg.\nReferences\n1. Carl Bracken. Private communication. 2004.\n2. G\u00e9rard Cohen, Iiro Honkala, Simon Litsyn, and Antoine Lobstein. Covering codes,\nvolume 54 of North-Holland Mathematical Library. North-Holland Publishing Co.,\nAmsterdam, 1997.\n3. Max H. M. Costa. Writing on dirty paper. IEEE Trans. Inform. Theory, 29(3):439\u0015\n441, 1983.\n4. Frank Kschischang and David Tse, editors. Proc. IEEE Intern. Symp. Inform.\nTheory, pages 533\u0015536. June 2004. A four-talk session on dirty paper coding.\n5. M.L. Miller, G.J. Doerr, and I.J. Cox. Applying informed coding and embedding to\ndesign a robust high-capacity watermark. IEEE Transactions on Image Processing,\n13(6):792\u0015807, June 2004.\n"}