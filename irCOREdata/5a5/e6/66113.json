{"doi":"10.1016\/j.ijar.2004.10.004","coreId":"66113","oai":"oai:dro.dur.ac.uk.OAI2:3107","identifiers":["oai:dro.dur.ac.uk.OAI2:3107","10.1016\/j.ijar.2004.10.004"],"title":"Dynamic programming for deterministic discrete-time systems with uncertain gain.","authors":["De Cooman, G.","Troffaes, M. C. M."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2005-06","abstract":"We generalise the optimisation technique of dynamic programming for discrete-time systems with an uncertain gain function. We assume that uncertainty about the gain function is described by an imprecise probability model, which generalises the well-known Bayesian, or precise, models. We compare various optimality criteria that can be associated with such a model, and which coincide in the precise case: maximality, robust optimality and maximinity. We show that (only) for the first two an optimal feedback can be constructed by solving a Bellman-like equation","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/66113.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/3107\/1\/3107.pdf","pdfHashValue":"29c11f818f486034a411ce8e9cd3287533f30c00","publisher":"Elsevier","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:3107<\/identifier><datestamp>\n      2011-08-24T12:25:01Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Dynamic programming for deterministic discrete-time systems with uncertain gain.<\/dc:title><dc:creator>\n        De Cooman, G.<\/dc:creator><dc:creator>\n        Troffaes, M. C. M.<\/dc:creator><dc:description>\n        We generalise the optimisation technique of dynamic programming for discrete-time systems with an uncertain gain function. We assume that uncertainty about the gain function is described by an imprecise probability model, which generalises the well-known Bayesian, or precise, models. We compare various optimality criteria that can be associated with such a model, and which coincide in the precise case: maximality, robust optimality and maximinity. We show that (only) for the first two an optimal feedback can be constructed by solving a Bellman-like equation. <\/dc:description><dc:subject>\n        Optimal control<\/dc:subject><dc:subject>\n         Dynamic programming<\/dc:subject><dc:subject>\n         Uncertainty<\/dc:subject><dc:subject>\n         Imprecise probabilities<\/dc:subject><dc:subject>\n         Lower previsions<\/dc:subject><dc:subject>\n         Sets of probabilities.<\/dc:subject><dc:publisher>\n        Elsevier<\/dc:publisher><dc:source>\n        International journal of approximate reasoning, 2005, Vol.39(2-3), pp.257-278 [Peer Reviewed Journal]<\/dc:source><dc:date>\n        2005-06<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:3107<\/dc:identifier><dc:identifier>\n        issn:0888-613X<\/dc:identifier><dc:identifier>\n        doi:10.1016\/j.ijar.2004.10.004<\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/3107\/<\/dc:identifier><dc:identifier>\n        http:\/\/dx.doi.org\/10.1016\/j.ijar.2004.10.004<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/3107\/1\/3107.pdf<\/dc:identifier><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["0888-613x","issn:0888-613X"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2005,"topics":["Optimal control","Dynamic programming","Uncertainty","Imprecise probabilities","Lower previsions","Sets of probabilities."],"subject":["Article","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n14 May 2009\nVersion of attached file:\nAccepted Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nDe Cooman, G. and Troffaes, M. C. M. (2005) \u2019Dynamic programming for deterministic discrete-time systems\nwith uncertain gain.\u2019, International journal of approximate reasoning., 39 (2-3). pp. 257-278.\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1016\/j.ijar.2004.10.004\nPublisher\u2019s copyright statement:\nAdditional information:\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\n \n \n \nDurham Research Online \n \nDeposited in DRO: \n14 May 2009 \n \nPeer-review status of attached file: \nPeer-reviewed \n \nPublication status of attached file: \nAccepted for publication \n \nCitation for published item: \nDe Cooman, G. and Troffaes, M. C. M. (2005) 'Dynamic programming for deterministic \ndiscrete-time systems with uncertain gain.', International journal of approximate reasoning., \n39 (2-3). pp. 257-278. \n \nFurther information on publisher\u2019s website: \nhttp:\/\/dx.doi.org\/10.1016\/j.ijar.2004.10.004 \n \n \n \n \n \n \n \n \n \n \n \n \nUse policy \n \nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior \npermission or charge, for personal research or study, educational, or not-for-profit purposes provided that : \n \n\uf0a7 a full bibliographic reference is made to the original source \n\uf0a7 a link is made to the metadata record in DRO \n\uf0a7 the full-text is not changed in any way \n \nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders. \n \nPlease consult the full DRO policy for further details. \n \nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom \nTel : +44 (0)191 334 2975 | Fax : +44 (0)191 334 2971 \nhttp:\/\/dro.dur.ac.uk \nGert de Cooman and Matthias C. M. Troffaes. Dynamic programming for\ndeterministic discrete-time systems with uncertain gain. International Journal of\nApproximate Reasoning, 39(2-3):257-278, Jun 2005.\nDynamic Programming for Deterministic\nDiscrete-Time Systems with Uncertain Gain\nGert de Cooman and Matthias C. M. Troffaes\nOnderzoeksgroep SYSTeMS, Universiteit Gent, Technologiepark \u2013 Zwijnaarde 914,\n9052 Zwijnaarde, Belgium\nAbstract\nWe generalise the optimisation technique of dynamic programming for discrete-\ntime systems with an uncertain gain function. We assume that uncertainty about\nthe gain function is described by an imprecise probability model, which generalises\nthe well-known Bayesian, or precise, models. We compare various optimality criteria\nthat can be associated with such a model, and which coincide in the precise case:\nmaximality, robust optimality and maximinity. We show that (only) for the first\ntwo an optimal feedback can be constructed by solving a Bellman-like equation.\nKey words: optimal control, dynamic programming, uncertainty, imprecise\nprobabilities, lower previsions, sets of probabilities\n1 Introduction to the Problem\nThe main objective in optimal control is to find out how a system can be influenced,\nor controlled, in such a way that its behaviour satisfies certain requirements, while\nat the same time maximising a given gain function. A very efficient method for\nsolving optimal control problems for discrete-time systems is the recursive dynamic\nprogramming technique, introduced by Richard Bellman [1].\nTo explain the ideas behind it, we refer to Figures 1 and 2. In Figure 1 we depict\na situation where a system can go from state a to state c through state b in three\nways: following the paths \u03b1\u03b2, \u03b1\u03b3 and \u03b1\u03b4. We denote the gains associated with these\npaths by J\u03b1\u03b2, J\u03b1\u03b3 and J\u03b1\u03b4 respectively. Assume that path \u03b1\u03b3 is optimal, meaning\nthat J\u03b1\u03b3 > J\u03b1\u03b2 and J\u03b1\u03b3 > J\u03b1\u03b4. Then it follows that path \u03b3 is the optimal way to go\n1 This paper presents research results of project G.0139.01 of the Fund for Scientific\nResearch, Flanders (Belgium), and of the Belgian Programme on Interuniversity Poles of\nAttraction initiated by the Belgian state, Prime Minister\u2019s Office for Science, Technology\nand Culture. The scientific responsibility rests with the authors.\n1\nr rra cb\n\u03b1\n\u03b2\n\u03b3\n\u03b4\nFig. 1. Principle of Optimality\nr\nr\nr\nr\nr\nr\nr\nr\nb\nc\nd\na e\n\u03bb\n\u03b1\n\u00b5\n\u03bd\n\u03b2\n\u03b3\n\u03b4\n\u000f\n\u03b7\nFig. 2. Dynamic Programming\nfrom b to c. To see this, observe that J\u03b1\u03bd = J\u03b1+J\u03bd for \u03bd \u2208 {\u03b2, \u03b3, \u03b4} (we shall assume\nthroughout that gains are additive along paths) and derive from the inequalities\nabove that J\u03b3 > J\u03b2 and J\u03b3 > J\u03b4. This simple observation, which Bellman called\nthe principle of optimality, forms the basis for the recursive technique of dynamic\nprogramming for solving an optimal control problem. To see how this is done in\nprinciple, consider the situation depicted in Figure 2. Suppose we want to find the\noptimal way to go from state a to state e. After one time step, we can reach the\nstates b, c and d from state a, and the optimal paths from these states to the final\nstate e are known to be \u03b1, \u03b3 and \u03b7, respectively. To find the optimal path from a to\ne, we only need to compare the costs J\u03bb+J\u03b1, J\u00b5+J\u03b3 and J\u03bd +J\u03b7 of the respective\ncandidate optimal paths \u03bb\u03b1, \u00b5\u03b3 and \u03bd\u03b7, since the principle of optimality tells us\nthat the paths \u03bb\u03b2, \u03bd\u03b4 and \u03bd\u000f cannot be optimal: if they were, then so would be\nthe paths \u03b2, \u03b4 and \u000f. This, written down in a more formal language, is what is\nessentially known as Bellman\u2019s equation. It allows us to solve an optimal control\nproblem fairly efficiently through a recursive procedure, by calculating optimal\npaths backwards from the final state.\nIn applications, it may happen that the gain function, which associates a gain with\nevery possible control action and the resulting behaviour of the system, is not well\nknown. This problem is most often treated by modelling the uncertainty about\nthe gain by means of a probability measure, and by maximising the expected gain\nunder this probability measure. Due to the linearity of the expectation operator,\nthis approach does not change the nature of the optimisation problem in any\nessential way, and the usual dynamic programming method can therefore still be\napplied.\nAs an example, consider the simple linear system described by\nxk+1 = axk + buk, k = 0, . . . , N \u2212 1 (1)\nwhere xk \u2208 R denote the system state and uk \u2208 R the control at time k, and\nwhere a and b are non-zero real numbers. Given an initial state x0 and a sequence\nu\u00b7 of successive controls u0, u1, . . . , uN\u22121, the systems goes through the successive\nstates x1, x2, . . . , xN determined by Eq. (1), and we assume that with this control\nthere is associated a gain\nJ(x0, u\u00b7, \u03c9) =\nN\u22121\u2211\nk=0\n[x2k + \u03c9u\n2\nk],\nwhere \u03c9 is some positive real constant. Solving the present optimal control problem\n2\nconsists in finding a control u\u00b7 that brings the system at time N in a given final\nstate xf , while at the same time maximising the gain J(x0, u\u00b7, \u03c9). The dynamic\nprogramming approach achieves this by reasoning backwards in time. First, the\ncontrol uN\u22121 is determined that maximises the gain\nx2N\u22121 + \u03c9u\n2\nN\u22121 =\n(\nxf \u2212 buN\u22121\na\n)2\n+ \u03c9u2N\u22121.\nThis control also determines a unique xN\u22121, and the procedure is then repeated\nby finding a control uN\u22122 that maximises the gain x2N\u22122 + \u03c9u\n2\nN\u22122, and so on . . .\nThe principle of optimality then ensures that the control u\u00b7 found in this recursive\nmanner indeed solves the optimal control problem. When \u03c9 is not well known, and\nonly its probability distribution is given, the optimal control problem is solved by\nmaximising the expected value of the gain, which can in this special example be\ndone by replacing \u03c9 with its expectation.\nIt has however been argued by various scholars (see [2, Chapter 5] for a detailed\ndiscussion with many references) that uncertainty cannot always be modelled ad-\nequately by (precise) probability measures, because, roughly speaking, there may\nnot be enough information available to identify a single probability measure. In\nthose cases, it is more appropriate to represent the available knowledge through a\nso-called imprecise probability model, e.g., by a coherent lower prevision, or what\nis mathematically equivalent, by a set of probability measures. For applications of\nthis approach, see for instance [3,4]. In the example above, it may for instance\nhappen that the probability distribution for \u03c9 is only known to belong to a given\nset: e.g., \u03c9 is normally distributed with mean zero, but the variance is only known\nto belong to an interval [\u03c32, \u03c32]; or \u03c9 itself is only known to belong to an interval\n[\u03c9, \u03c9]. 2\nTwo questions now arise naturally. First of all, how should we formulate the opti-\nmal control problem: what does it mean for a control to be optimal with respect\nto an uncertain gain function, where the uncertainty is represented through an\nimprecise probability model? In Section 2 we identify three different optimality\ncriteria, each with a different interpretation (although they coincide for precise\nprobability models), and we study the relations between them. Secondly, is it still\npossible to solve the corresponding optimal control problems using the ideas un-\nderlying Bellman\u2019s dynamic programming method? We show in Section 3 that this\nis the case for only two of the three optimality criteria we study: only for these a\ngeneralised principle of optimality holds, and the optimal controls are solutions of\nsuitably generalised Bellman-like equations. In order to arrive at this conclusion,\nwe study the properties that an abstract notion of optimality should satisfy for\nthe Bellman approach to work. To illustrate how our ideas can be implemented,\nwe present a numerical example in Section 4.\nWe recognise that other authors (see for instance [5,6,7,8,9]) have extended the\n2 This also covers the case where we want to find out how robust the optimal control\nsolution is against variations of \u03c9 within a given interval.\n3\ndynamic programming algorithm to systems with uncertain gain and\/or uncertain\ndynamics, where the uncertainty is modelled by an imprecise probability model.\nBut none of them seem to have questioned under what assumptions their gener-\nalised dynamic programming method leads to optimal paths. Here we approach\nthe problem from the opposite, and in our opinion, more logical side: one should\nfirst define a notion of optimality and investigate whether the dynamic program-\nming argument holds for it, rather than blindly \u201cgeneralise\u201d Bellman\u2019s algorithm\nwithout showing that it actually yields optimal controls.\nIn the remainder of this section, we introduce the basic systems-theoretic concepts\nand notation used in the rest of the paper.\n1.1 The System\nFor a and b in N, the set of natural numbers c that satisfy a \u2264 c \u2264 b is denoted\nby [a, b]. Let\nxk+1 = f(xk, uk, k)\ndescribe a discrete-time dynamical system with k \u2208 N, xk \u2208 X and uk \u2208 U . The\nset X is the state space (e.g., Rn, n \u2208 N \\ {0}), and the set U is the control space\n(e.g., Rm, m \u2208 N \\ {0}). The map f : X \u00d7 U \u00d7 N \u2192 X describes the evolution of\nthe state in time: given the state xk \u2208 X and the control uk \u2208 U at time k \u2208 N,\nit returns the next state xk+1 of the system. For practical reasons, we impose a\nfinal time N beyond which we are not interested in the dynamics of the system.\nMoreover, it may happen that not all states and controls are allowed at all times:\nwe demand that xk should belong to a set of admissible states Xk at every instant\nk \u2208 [0, N ], and that uk should belong to a set of admissible controls Uk at every\ninstant k \u2208 [0, N \u2212 1], where Xk \u2286 X and Uk \u2286 U are given. The set XN may be\nthought of as the set we want the state to end up in at time N .\n1.2 Paths\nA path is a triple (x, k, u\u00b7), where x \u2208 X is a state, k \u2208 [0, N ] a time instant, and\nu\u00b7 : [k,N\u22121]\u2192 U a sequence of controls. Such a path fixes a unique state trajectory\nx\u00b7 : [k,N ]\u2192 X , which is defined recursively through xk = x and x`+1 = f(x`, u`, `)\nfor every ` \u2208 [k,N \u2212 1]. It is said to be admissible if x` \u2208 X` for every ` \u2208 [k,N ]\nand u` \u2208 U` for every ` \u2208 [k,N\u22121]. We denote the unique map from the empty set\n\u2205 to U by u\u2205. If k = N , the control u\u00b7 does nothing: it is equal to u\u2205. The unique\npath starting and ending at time k = N in x \u2208 X is denoted by (x,N, u\u2205).\nThe set of admissible paths starting in the state x \u2208 Xk at time k \u2208 [0, N ] is\ndenoted by U(x, k), i.e.,\nU(x, k) = {(x, k, u\u00b7) : (x, k, u\u00b7) admissible path} .\n4\nFor example, U(x,N) = {(x,N, u\u2205)} whenever x \u2208 XN and U(x,N) = \u2205 otherwise.\nIf we consider a path with final time M different from N , then we write (x, k, u\u00b7)M\n(assume k \u2264 M \u2264 N). Observe that (x, k, u\u00b7)k can be identified with (x, k, u\u2205)k;\nit is the unique path (of length zero) starting and ending at time k in x. Let\n0 \u2264 k \u2264 ` \u2264 m. Two paths (x, k, u\u00b7)` and (y, `, v\u00b7)m can be concatenated if y = x`.\nThe concatenation is denoted by (x, k, u\u00b7, `, v\u00b7)m or by (x, k, u\u00b7)` \u2295 (y, `, v\u00b7)m. It\nrepresents the path that starts in state x at time k, and results from applying\ncontrol ui for times i \u2208 [k, `\u22121] and control vi for times i \u2208 [`,m\u22121]. In particular,\n(x, k, u\u00b7)` = (x, k, u\u00b7)k \u2295 (x, k, u\u00b7)` = (x, k, u\u00b7)` \u2295 (x`, `, u\u00b7)`.\nThe set of admissible paths starting in state x \u2208 Xk at time k \u2208 [0, N ] and ending\nat time ` \u2208 [k,N ] is denoted by U(x, k)`. In particular we have that U(x, k)k =\n{(x, k, u\u2205)k} if x \u2208 Xk, and U(x, k)k = \u2205 otherwise. Moreover, for any (x, k, u\u00b7)` \u2208\nU(x, k)` and any V \u2286 U(x`, `), we use the notation\n(x, k, u\u00b7)` \u2295 V = {(x, k, u\u00b7)` \u2295 (x`, `, v\u00b7) : (x`, `, v\u00b7) \u2208 V}.\n1.3 The Gain Function\nWe assume that applying the control action u \u2208 U to the system in state x \u2208 X\nat time k \u2208 [0, N \u2212 1] yields a real-valued gain g(x, u, k, \u03c9). Moreover, reaching\nthe final state x \u2208 X at time N also yields a gain h(x, \u03c9). The parameter \u03c9 \u2208 \u2126\nrepresents the (unknown) state of the world, and it is a device used to model that\nthe gains are not well known. If we knew that the real state of the world was \u03c9o,\nwe would know the gains to be g(x, u, k, \u03c9o) and h(x, \u03c9o). As it is, the real state of\nthe world is uncertain, and so are the gains, which could be considered as random\nvariables. It is important to note that the parameter \u03c9 only influences the gains;\nit has no effect on the system dynamics, which are assumed to be known perfectly\nwell.\nWe shall only consider the important case where the gains are additive along paths,\ni.e., with a path (x, k, u\u00b7) we associate a gain J(x, k, u\u00b7, \u03c9) given by:\nJ(x, k, u\u00b7, \u03c9) =\n\u2211N\u22121\ni=k g(xi, ui, i, \u03c9) + h(xN , \u03c9),\nfor any \u03c9 \u2208 \u2126 (gain additivity). If M < N , we also use the notation\nJ(x, k, u\u00b7, \u03c9)M =\n\u2211M\u22121\ni=k g(xi, ui, i, \u03c9).\nIt will be convenient to associate a zero gain with an empty control action: for\nk \u2208 [0, N ] we let J(x, k, u\u00b7, \u03c9)k = 0.\nThe main objective of optimal control can now be formulated as follows: given\nthat the system is in the initial state x \u2208 X at time k \u2208 [0, N ], find a control\nsequence u\u00b7 : [k,N \u22121]\u2192 U resulting in an admissible path (x, k, u\u00b7) such that the\n5\ncorresponding gain J(x, k, u\u00b7, \u03c9) is maximal. Moreover, we would like this control\nsequence u\u00b7 to be such that its value uk at time k is a function of x and k only,\nsince in that case the control can be realised through state feedback.\nIf \u03c9 is known, then the problem reduces to the classical problem of dynamic pro-\ngramming, first studied and solved by Bellman [1]. We shall assume here that\nthe available information about the true state of the world is modelled through a\ncoherent lower prevision P defined on the set L(\u2126) of gambles, or bounded real-\nvalued maps, on \u2126. A special case of this obtains when P is a linear prevision\nP . Linear previsions are the precise probability models; they can be interpreted\nas expectation operators associated with (finitely additive) probability measures,\nand they are previsions or fair prices in the sense of de Finetti [10]. We assume\nthat the reader is familiar with the basic ideas behind the theory of coherent lower\n(and linear) previsions (see [2] for more details).\nFor a given path (x, k, u\u00b7), the corresponding gain J(x, k, u\u00b7, \u03c9) can be seen as\na real-valued map on \u2126, which is denoted by J(x, k, u\u00b7) and is called the gain\ngamble associated with (x, k, u\u00b7). 3 In the same way we define the gain gambles\ng(xk, uk, k), h(xN) and J(x, k, u\u00b7)M . There is gain additivity: J(x, k, u\u00b7, `, v\u00b7)m =\nJ(x, k, u\u00b7)` + J(x`, `, v\u00b7)m for k \u2264 ` \u2264 m \u2264 N , and J(x, k, u\u00b7)k = 0. We denote by\nJ (x, k) the set of gain gambles for admissible paths from initial state x \u2208 Xk at\ntime k \u2208 [0, N ]:\nJ (x, k) = {J(x, k, u\u00b7) : (x, k, u\u00b7) \u2208 U(x, k)} .\n2 Optimality Criteria\n2.1 P-Maximality\nThe lower prevision P(X) of a gamble X has a behavioural interpretation as a\nsubject\u2019s supremum acceptable price for buying the gamble X: it is the highest\nvalue of \u00b5 such that the subject accepts the gamble X \u2212 x (i.e., accepts to buy X\nfor a price x) for all x < \u00b5. The conjugate upper prevision P(X) = \u2212P(\u2212X) of X\nis then the subject\u2019s infimum acceptable price for selling X. This way of looking\nat a coherent lower prevision P defined on the set L(\u2126) of all gambles allows us\nto define a strict partial order >P on L(\u2126) whose interpretation is that of strict\npreference.\nDefinition 1 For any gambles X and Y in L(\u2126) we say that X strictly dominates\nY , or that X is strictly preferred to Y (with respect to P), and we write X >P Y ,\n3 To simplify the discussion, we assume that this map is bounded. We have shown\nelsewhere how the boundedness requirement can be relaxed in the theory of coherent\nlower previsions [11].\n6\nif 4\nP(X \u2212 Y ) > 0 or (X \u2265 Y and X 6= Y ).\nIndeed, if X \u2265 Y and X 6= Y , then the subject should be willing to exchange Y\nfor X, since this can only improve his gain. On the other hand, P(X \u2212 Y ) > 0\nexpresses that the subject is willing to pay some strictly positive price to exchange\nY for X, which again means that he strictly prefers X to Y .\nIt is clear that we can also use the coherent lower prevision P to express a strict\npreference between any two paths (x, k, u\u00b7) and (x, k, v\u00b7), based on their gains: if\nJ(x, k, u\u00b7) >P J(x, k, v\u00b7) this means that the uncertain gain J(x, k, u\u00b7) is strictly\npreferred to the uncertain gain J(x, k, v\u00b7). We then say that the path (x, k, u\u00b7) is\nstrictly preferred to (x, k, v\u00b7), and we use the notation (x, k, u\u00b7) >P (x, k, v\u00b7).\nThe relation >P is anti-reflexive and transitive.\n5 It is therefore indeed a strict\npartial order on L(\u2126), and in particular also on J (x, k) and on U(x, k). But it is\ngenerally not linear: unless P is a linear prevision, there will typically be gambles\nX and Y such that P(X \u2212 Y ) \u2264 0 \u2264 P(X \u2212 Y ), and therefore X 6>PY and\nY 6>PX. Two paths need not be comparable with respect to this order, and it does\nnot always make sense to look for greatest elements, i.e., for paths that strictly\ndominate all the others. Rather, we should look for maximal, or undominated,\nelements: paths (x, k, u\u00b7) that are not dominated by any other path, meaning\nthat (x, k, v\u00b7)6>P(x, k, u\u00b7) for all paths (x, k, v\u00b7) in U(x, k). Observe that a maximal\ngamble X in a set K with respect to >P can be characterised as a maximal element\nof K with respect to \u2265 (i.e., it is point-wise undominated) such that P(X\u2212Y ) \u2265 0\nfor all Y \u2208 K. In case P is a linear prevision P , maximal gambles with respect to>P\nare precisely the point-wise undominated gambles whose prevision is maximal; they\nmaximise the expected gain. This motivates the following optimality definition.\nDefinition 2 Let k \u2208 [0, N ], x \u2208 Xk and V \u2286 U(x, k). A path (x, k, u\u2217\u00b7 ) in V\nis called P -maximal, or >P -optimal, in V if no path in V is strictly preferred to\n(x, k, u\u2217\u00b7 ), i.e., (x, k, u\u00b7)6>P(x, k, u\u2217\u00b7 ) for all (x, k, u\u00b7) \u2208 V. We denote the set of the\nP-maximal paths in V by opt>P (V). The operator opt>P is called the optimality\noperator induced by >P , associated with U(x, k).\nThe P -maximal paths in U(x, k) are precisely those admissible paths starting at\ntime k in state x for which the associated gain gamble is a maximal element of\nJ (x, k) with respect to the strict partial order >P . If we denote the set of these\n>P -maximal gain gambles in J (x, k) by opt>P (J (x, k)), then for all (x, k, u\u00b7) \u2208\nU(x, k):\n(x, k, u\u00b7) \u2208 opt>P (U(x, k)) \u21d0\u21d2 J(x, k, u\u00b7) \u2208 opt>P (J (x, k)) .\nP -maximal paths do not always exist: not every partially ordered set has maximal\nelements. A fairly general sufficient condition for the existence of P -maximal ele-\n4 The symbol \u201c\u2265\u201d denotes the point-wise order on gambles.\n5 Since P is coherent, we have P(X \u2212X) = 0 and P(Z \u2212X) \u2265 P(Z \u2212 Y ) +P(Y \u2212X).\n7\nments in J (x, k) (and hence in U(x, k)) is that J (x, k) should be compact 6 (and\nof course non-empty). This follows from a general result mentioned in [2, Sec-\ntion 3.9.2], which is also proven in Lemma 3 below. In fact, we use this lemma to\nprove a stronger result in Theorem 4, whose Corollary 5 turns out to be very impor-\ntant in showing that the dynamic programming approach works for P -maximality\n(see Section 3.2).\nIn order to prove Lemma 3 and Theorem 4, it is convenient to introduce the partial\npreorder (a reflexive and transitive relation) <P on L(\u2126) defined by:\nX <P Y \u21d4 P(X \u2212 Y ) \u2265 0.\nRecall that an element X of a subset K of L(\u2126) is a maximal element of K with\nrespect to <P if it is undominated, i.e., if and only if\n(\u2200Y \u2208 K)(Y <P X \u21d2 X <P Y ). (2)\nLemma 3 For any non-empty compact subset K of L(\u2126) the following statements\nhold.\n(i) If X is a maximal element of K with respect to <P , then P(X \u2212 Y ) \u2265 0 for\nall Y in K.\n(ii) For every X in K the subset\n\u2191PX =\n{\nY \u2208 K : Y <P X\n}\n= {Y \u2208 K : P(Y \u2212X) \u2265 0}\nof K is non-empty and compact.\n(iii) There is a maximal element of K with respect to <P .\n(iv) For every X in K there is a maximal element Y of K with respect to <P\nsuch that Y <P X.\n(v) For every X in K there is a maximal element Y of K with respect to the\npointwise order \u2265 such that Y \u2265 X.\n(vi) There is maximal element of K with respect to >P .\nPROOF. Assume that X is a maximal element of K with respect to <P . Consider\nY in K, then it follows from Condition (2) that P(Y \u2212X) < 0 or P(X \u2212 Y ) \u2265 0.\nIn both cases it follows that P(X \u2212 Y ) \u2265 0. This proves (i).\nIt is obvious that X <P X, so \u2191PX is non-empty. Consider a sequence (Xn) in\n\u2191PX that converges to some gamble X\u221e: sup\u03c9\u2208\u2126|X\u221e(\u03c9) \u2212 Xn(\u03c9)| \u2192 0. Since K\nis compact and therefore closed, we know that X\u221e \u2208 K. It now follows from the\ncoherence of P (see [2, Theorem 2.6.1]) and P(Xn \u2212X) \u2265 0 that\nP(X\u221e \u2212X) = P(X\u221e \u2212Xn +Xn \u2212X)\n\u2265 P(X\u221e \u2212Xn) + P(Xn \u2212X) \u2265 P(X\u221e \u2212Xn),\n6 In this paper, we always assume that L(\u2126) is provided with the supremum-norm\ntopology.\n8\nfor all n. Since \u2212|X\u221e \u2212Xn| \u2264 X\u221e \u2212Xn \u2264 |X\u221e \u2212Xn|, the coherence of P (again\nsee [2, Theorem 2.6.1]) also tells us that\n\u2212 P(|X\u221e \u2212Xn|) = P(\u2212|X\u221e \u2212Xn|)\n\u2264 P(X\u221e \u2212Xn) \u2264 P(|X\u221e \u2212Xn|) \u2264 P(|X\u221e \u2212Xn|),\nand that 0 \u2264 P(|X\u221e\u2212Xn|) \u2264 sup|X\u221e\u2212Xn|. This implies that P(X\u221e\u2212Xn)\u2192 0,\nand therefore P(X\u221e \u2212 X) \u2265 0, whence X\u221e \u2208 \u2191PX. This tells us that \u2191PX is a\nclosed subset of the compact K and therefore also compact, proving (ii).\nTo prove (iii), let K\u2032 be any subset of the non-empty compact set K that is linearly\nordered with respect to <P . If we can show that K\u2032 has an upper bound in K with\nrespect to <P , then we can infer from Zorn\u2019s lemma that K has a <P -maximal\nelement. Let then {X1, X2, . . . , Xn} be an arbitrary finite subset of K\u2032. We can\nassume without loss of generality that X1 <P X2 <P . . . <P Xn, and consequently\n\u2191PX1 \u2286 \u2191PX2 \u2286 \u00b7 \u00b7 \u00b7 \u2286 \u2191PXn. This implies that the intersection \u22c2nk=1 \u2191PXk = \u2191PX1\nof these up-sets is non-empty. We see that the collection\n{\n\u2191PX : X \u2208 K\u2032\n}\nof com-\npact and therefore closed subsets of the compact set K has the finite intersection\nproperty. Consequently, the intersection\n\u22c2\nX\u2208K\u2032 \u2191PX is non-empty as well, and this\nis the set of upper bounds of K\u2032 in K with respect to <P .\nTo prove (iv), combine (ii) and (iii) to show that the non-empty compact set \u2191PX\nhas a maximal element Y with respect to <P . It is then a trivial step to prove\nthat Y is also <P -maximal in K.\nThe fifth statement follows from the fourth: let P be the (coherent) so-called\nvacuous lower prevision, defined by P(X) = inf {X(\u03c9) : \u03c9 \u2208 \u2126}. Then the order\n<P is nothing but the pointwise order \u2265.\nWe now come to the last statement. By combining (i) and (iii), we know that there\nis some Yo in K such that P(Yo \u2212 X) \u2265 0 for all X \u2208 K. From (v) we infer that\nthere is some \u2265-maximal Y in K such that Y \u2265 Yo, and therefore (by coherence)\nP(Y \u2212 X) \u2265 P(Yo \u2212 X) \u2265 0 for all X \u2208 K. This means that Y is a maximal\nelement of K with respect to >P .\nTheorem 4 For every element X of a compact subset K of L(\u2126) that is not a\nmaximal element of K with respect to >P , there is some maximal element Y of K\nwith respect to >P such that Y >P X.\nPROOF. Consider an element X of K that is not >P -maximal in K. We may\nassume that X is \u2265-maximal in K. Indeed, if X is not \u2265-maximal then by\nLemma 3(v) there is some \u2265-maximal Z in K such that Z \u2265 X and Z 6= X,\nwhence Z >P X. If Z is >P -maximal in K then there is nothing left to prove.\nSo we are left with the case that Z is not >P -maximal. If we can prove for this\n\u2265-maximal Z that there is some >P -maximal Y in K such that Y >P Z then also\nY >P X and the proof is complete.\n9\nSince X is \u2265-maximal in K, there is some U in K such that P(U \u2212 X) > 0.\nBy Lemma 3(ii) and (vi) there is a >P -maximal element Y in \u2191PU . Since P(Y \u2212\nU) \u2265 0 we infer from the coherence of P (see [2, Theorem 2.6.1(e)]) that P(Y \u2212\nX) = P(Y \u2212 U + U \u2212 X) \u2265 P(Y \u2212 U) + P(U \u2212 X) > 0, whence Y >P X. It\nremains to prove that Y is also >P -maximal in K. Assume ex absurdo that there\nis some V in K such that V >P Y . Then there are two possibilities. If V \u2265 Y and\nV 6= Y then it follows from the coherence of P (see [2, Theorem 2.6.1(d)]) that\nP(V \u2212U) \u2265 P(Y \u2212U) \u2265 0, whence V \u2208 \u2191PU , a contradiction. If P(V \u2212Y ) > 0 then\nit follows from the coherence of P (see [2, Theorem 2.6.1(e)]) that P(V \u2212 U) =\nP(V \u2212 Y + Y \u2212 U) \u2265 P(V \u2212 Y ) + P(Y \u2212 U) > 0, whence V \u2208 \u2191PU , again a\ncontradiction.\nCorollary 5 Let k \u2208 [0, N ] and let x \u2208 Xk. If J (x, k) is compact then for every\nadmissible, non-P-maximal path (x, k, u\u00b7) in U(x, k) there is a P-maximal path\n(x, k, u\u2217\u00b7 ) in U(x, k) that is strictly preferred to it.\n2.2 P-Maximinity\nWe now turn to a different optimality criterion that can be associated with a lower\nprevision P . We use P to define another strict order on L(\u2126):\nDefinition 6 For any gambles X and Y in L(\u2126) we write X AP Y if\nP(X) > P(Y ) or (X \u2265 Y and X 6= Y ).\nAP induces a strict partial order on U(x, k), since it is anti-reflexive and transitive\non L(\u2126). A maximal element X of a subset K of L(\u2126) with respect to AP is\neasily seen to be a point-wise undominated element of K that maximises the lower\nprevision: P(X) \u2265 P(Y ) for all Y \u2208 K.\nWe can consider as optimal in U(x, k) those admissible paths (x, k, u\u00b7) for which\nthe associated gain gamble J(x, k, u\u00b7) is a maximal element of J (x, k) with re-\nspect to AP ; they are the paths (x, k, u\u00b7) that maximise the \u2018lower expected gain\u2019\nP(J(x, k, u\u00b7)) and whose gain gambles J(x, k, u\u00b7) are point-wise undominated.\nDefinition 7 Let k \u2208 [0, N ], x \u2208 Xk and V \u2286 U(x, k). A path (x, k, u\u2217\u00b7 ) in V is\ncalled P -maximin, or AP -optimal, in V if no path in V is strictly preferred to\n(x, k, u\u2217\u00b7 ), i.e., (x, k, u\u00b7) 6AP (x, k, u\u2217\u00b7 ) for all (x, k, u\u00b7) \u2208 V. We denote the set of the\nP-maximin paths in V by optAP (V). The operator optAP is called the optimality\noperator induced by AP , associated with U(x, k).\nProposition 8 P-maximinity implies P-maximality. For a linear prevision P,\nP-maximinity is equivalent to P-maximality.\n10\nPROOF. Consider a set of gambles K and assume that X is a maximal element\nof K with respect to AP . In order to prove that X is also a maximal element of\nK with respect to >P , it obviously suffices to show that P(X \u2212 Y ) \u2265 0 for all\nY \u2208 K. We know that P(X) \u2265 P(Y ) for any Y \u2208 K, and consequently, taking\ninto account coherence (see [2, Section 2.6.1(e)]):\nP(X \u2212 Y ) \u2265 P(X) + P(\u2212Y ) = P(X)\u2212 P(Y ) \u2265 0.\nIf P is a linear prevision P , assume that X is a maximal element of K with respect\nto >P . In order to prove that X is also a maximal element of K with respect to\nAP , it suffices to show that P(X) \u2265 P(Y ) for all Y \u2208 K. Since we know that for\nany Y \u2208 K, P(X\u2212Y ) \u2265 0, and that P(X\u2212Y ) = P(X)\u2212P(Y ), the desired result\nfollows at once.\nThe existence of maximal elements with respect to AP in an arbitrary set of\ngambles K is obviously not guaranteed. But if K is compact, then we may easily\ninfer from the continuity of any coherent lower prevision P , that the counterparts\nof Theorem 4 and Corollary 5 hold for AP .\n2.3 M-Maximality\nThere is a tendency, especially among robust Bayesians, to consider an imprecise\nprobability model as a compact convex set of linear previsions M\u2286 P(\u2126), where\nP(\u2126) is the set of all linear previsions on L(\u2126).M is assumed to contain the true,\nbut unknown, linear prevision PT that models the available information [12,13].\nA gamble X is then certain to be strictly preferred to a gamble Y under the true\nlinear prevision PT if and only if it is strictly preferred under all candidate models\nP \u2208 M. This observation leads to the definition of a \u2018robustified\u2019 strict partial\norder >M on L(\u2126).\nDefinition 9 X >M Y if X >P Y for all P \u2208M.\nSince M is assumed to be compact and convex, it is not difficult to show that\nthe strict partial orders >M and >P are one and the same, where the coher-\nent lower prevision P is the so-called lower envelope of M, defined by P(X) =\ninf {P(X) : P \u2208M} for all X \u2208 L(\u2126). 7 Conversely, given a coherent lower previ-\nsion P , the strict partial orders >M(P) and >P are identical, where\nM(P) = {P \u2208 P(\u2126) : (\u2200X \u2208 L(\u2126))(P(X ) \u2265 P(X ))}\nis the set of linear previsions that dominate P . These strict partial orders have the\nsame maximal elements, and lead to the same notion of optimality.\n7 Since M is compact, this infimum is actually achieved.\n11\nBut there is in the literature yet another notion of optimality that can be associated\nwith a compact convex set of linear previsionsM: a gambleX is considered optimal\nin a set of gamblesK if it is a maximal element ofK with respect to the strict partial\norder >P for some P \u2208M. This notion of optimality is called \u2018E-admissibility\u2019 by\nLevi [14, Section 4.8]. It does not generally coincide with the ones associated with\nthe strict partial orders >M and >P , unless the set K is convex [2, Section 3.9].\nWe are therefore led to consider a third notion of optimality, associated with a\nlower prevision P , or a set of linear previsions M.\nDefinition 10 Let x \u2208 X , k \u2208 [0, N ] and V \u2286 U(x, k). A path (x, k, u\u2217\u00b7 ) \u2208 V is\nsaid to be M-maximal in V if it is P-maximal in V for some P in M, i.e., if it is\n\u2265-maximal in V and maximises P(J(x, k, u\u00b7)) over V for some P \u2208 M. The set\nof all M-maximal elements of V is denoted by optM (V).\nInterestingly, for any set of paths V \u2286 U(x, k):\noptM (V) =\n\u22c3\nP\u2208M\nopt>P (V) . (3)\n3 Dynamic Programming\n3.1 A General Notion of Optimality\nSo far, we have discussed three different ways of associating optimal paths with a\nlower prevision P , all of which occur in the literature. We now propose to find out\nwhether, for these different types of optimality, we can use the ideas behind the dy-\nnamic programming method to solve the corresponding optimal control problems.\nTo do this, we take a closer look at Bellman\u2019s analysis as described in Section 1,\nand we investigate which properties a generic notion of optimality must satisfy for\nhis method to work. Let us therefore assume that there is some property, called\n\u2217-optimality, which a path in a given set of paths P either has or does not have.\nIf a path in P has this property, we say that it is \u2217-optimal in P . We shall denote\nthe set of the \u2217-optimal elements of P by opt\u2217 (P). By definition, opt\u2217 (P) \u2286 P .\nFurther on, we shall apply our findings to the various instances of \u2217-optimality\ndescribed above.\nConsider Figure 3, where we want to find the \u2217-optimal paths from state a to\nstate e. Suppose that after one time step, we can reach the states b, c and d from\nstate a. The \u2217-optimal paths from these states to the final state e are known to\nbe \u03b1, \u03b3, and \u03b4 and \u03b7, respectively. For the dynamic programming approach to\nwork, we need to be able to infer from this a generalised form of the Bellman\nequation, stating essentially that the \u2217-optimal paths from a to e, a priori given\nby opt\u2217 ({\u03bb\u03b1, \u03bb\u03b2, \u00b5\u03b3, \u03bd\u03b4, \u03bd\u000f, \u03bd\u03b7}), are actually also given by opt\u2217 ({\u03bb\u03b1, \u00b5\u03b3, \u03bd\u03b4, \u03bd\u03b7}),\ni.e., the \u2217-optimal paths in the set of concatenations of \u03bb, \u00b5 and \u03bd with the re-\nspective \u2217-optimal paths \u03b1, \u03b3, and \u03b4 and \u03b7. It is therefore necessary to exclude\n12\nr\nr\nr\nr\nr\nr\nr\nr\nb\nc\nd\na e\n\u03bb\n\u03b1\n\u00b5\n\u03bd\n\u03b2\n\u03b3\n\u03b4\n\u000f\n\u03b7\nFig. 3. A More General Type of Dynamic Programming\nthat the concatenations \u03bb\u03b2 and \u03bd\u000f with the non-\u2217-optimal paths \u03b2 and \u000f can be\n\u2217-optimal. This amounts to requiring that the operator opt\u2217 should satisfy some\nappropriate generalisation of Bellman\u2019s principle of optimality that will allow us\nto conclude that \u03bb\u03b2 and \u03bd\u000f cannot be \u2217-optimal because then \u03b2 and \u000f would be\n\u2217-optimal as well. Definition 13 below provides a precise general formulation.\nBut, perhaps surprisingly for someone familiar with the traditional form of dynamic\nprogramming, opt\u2217 should satisfy an additional property: the omission of the non-\n\u2217-optimal paths \u03bb\u03b2 and \u03bd\u000f from the set of candidate \u2217-optimal paths should not\nhave any effect on the actual \u2217-optimal paths: we need that\nopt\u2217 ({\u03bb\u03b1, \u03bb\u03b2, \u00b5\u03b3, \u03bd\u03b4, \u03bd\u000f, \u03bd\u03b7}) = opt\u2217 ({\u03bb\u03b1, \u00b5\u03b3, \u03bd\u03b4, \u03bd\u03b7}) .\nThis is obviously true for the simple type of optimality that we have looked at in\nSection 1, but it need not be true for the more abstract types that we want to\nconsider here. Equality will be guaranteed if opt\u2217 is insensitive to the omission of\nnon-\u2217-optimal elements from {\u03bb\u03b1, \u03bb\u03b2, \u00b5\u03b3, \u03bd\u03b4, \u03bd\u000f, \u03bd\u03b7}, in the following sense.\nDefinition 11 Consider a set S 6= \u2205 and an optimality operator opt\u2217 defined on\nthe set \u2118(S) of subsets of S such that opt\u2217 (T ) \u2286 T for all T \u2286 S. Elements of\nopt\u2217 (T ) are called \u2217-optimal in T . The optimality operator opt\u2217 is called insensi-\ntive to the omission of non-\u2217-optimal elements from S if opt\u2217 (S) = opt\u2217 (T ) for\nall T such that opt\u2217 (S) \u2286 T \u2286 S.\nThe following proposition gives an interesting sufficient condition for this insensi-\ntivity in case optimality is associated with a (family of) strict partial order(s): it\nsuffices that every non-optimal path is strictly dominated by an optimal one.\nProposition 12 Let S be a non-empty set provided with a family of strict partial\norders >j, j \u2208 J . Define for T \u2286 S, opt>j (T ) = {a \u2208 T : (\u2200b \u2208 T )(b 6>j a)} as the\nset of maximal elements of T with respect to >j, and let optJ (T ) =\n\u22c3\nj\u2208J opt>j (T ).\nThen opt>j , j \u2208 J and optJ are optimality operators. If for some j \u2208 J ,\n(\u2200a \u2208 S \\ opt>j (S))(\u2203b \u2208 opt>j (S))(b >j a), (4)\nthen opt>j is insensitive to the omission of non->j-optimal elements from S. If\nCondition (4) holds for all j \u2208 J , then optJ is insensitive to the omission of\nnon-J-optimal elements from S.\n13\nPROOF. Consider j in J , and assume that Condition (4) holds for this j. Let\nopt>j (S) \u2286 T \u2286 S, then we must prove that opt>j (S) = opt>j (T ). First of all,\nif a \u2208 opt>j (S) then b 6>j a for all b in S, and a fortiori for all b in T , whence\na \u2208 opt>j (T ). Consequently, opt>j (S) \u2286 opt>j (T ). Conversely, let a \u2208 opt>j (T )\nand assume ex absurdo that a 6\u2208 opt>j (S). It then follows from (4) that there\nis some c in opt>j (S) and therefore in T such that c >j a, which contradicts\na \u2208 opt>j (T ).\nNext, assume that (4) holds for all j \u2208 J . Let optJ (S) \u2286 T \u2286 S, then we must\nprove that optJ (S) = optJ (T ). Consider any j \u2208 J , then opt>j (S) \u2286 optJ (S) \u2286\nT \u2286 S, so we may infer from the first part of the proof that opt>j (S) = opt>j (T ).\nBy taking the union over all j \u2208 J , we find that indeed optJ (S) = optJ (T ).\nWe are now ready for a precise formulation of the dynamic programming approach\nfor solving optimal control problems associated with general types of optimality.\nWe assume that we have some type of optimality, called \u2217-optimality, that allows\nus to associate with the set of admissible paths U(x, k) starting at time k in initial\nstate x, an optimality operator opt\u2217 defined on the set \u2118(U(x, k)) of subsets of\nU(x, k). For each such subset V , opt\u2217 (V) is then the set of admissible paths that\nare \u2217-optimal in V . The principle of optimality states that the optimality operators\nassociated with the various U(x, k) should be related in a special way.\nDefinition 13 (Principle of Optimality) \u2217-optimality satisfies the principle of\noptimality if it holds for all k \u2208 [0, N ], x \u2208 Xk, ` \u2208 [k,N ] and (x, k, u\u00b7) in U(x, k)\nthat if (x, k, u\u00b7) is \u2217-optimal in U(x, k), then (x`, `, u\u00b7) is \u2217-optimal in U(x`, `).\nThis may also be expressed as:\nopt\u2217 (U(x, k)) \u2286\n\u22c3\n(x,k,u\u00b7)`\u2208U(x,k)`\n(x, k, u\u00b7)` \u2295 opt\u2217 (U(x`, `)) .\nThe Bellman equation now states that applying the optimality operator to the\nright hand side suffices to achieve equality. (Usually this is stated with ` = k+ 1.)\nTheorem 14 (Bellman Equation) Let k \u2208 [0, N ] and x \u2208 Xk. Assume that \u2217-\noptimality satisfies the principle of optimality, and that the optimality operator opt\u2217\nfor U(x, k) is insensitive to the omission of non-\u2217-optimal elements from U(x, k).\nThen for all ` \u2208 [k,N ]:\nopt\u2217 (U(x, k)) = opt\u2217\n\uf8eb\uf8ed \u22c3\n(x,k,u)`\u2208U(x,k)`\n(x, k, u)` \u2295 opt\u2217 (U(x`, `))\n\uf8f6\uf8f8 ,\nthat is, a path is \u2217-optimal if and only if it is a \u2217-optimal concatenation of an\nadmissible path (x, k, u\u00b7)` and a \u2217-optimal path of U(x`, `).\n14\nPROOF. Fix k in [0, N ], ` \u2208 [k,N ] and x \u2208 Xk. Define\nV1 =\n\u22c3\n(x,k,u)`\u2208U(x,k)`\n(x, k, u)` \u2295 opt\u2217 (U(x`, `)) , and,\nV2 =\n\u22c3\n(x,k,u)`\u2208U(x,k)`\n(x, k, u)` \u2295 (U(x`, `) \\ opt\u2217 (U(x`, `))) .\nObviously, U(x, k) = V1\u222aV2 and V1\u2229V2 = \u2205. We have to prove that opt\u2217 (U(x, k)) =\nopt\u2217 (V1). By the principle of optimality, no path in V2 is \u2217-optimal in U(x, k), so\nV2 \u2229 opt\u2217 (U(x, k)) = \u2205. This implies that opt\u2217 (U(x, k)) \u2286 V1 \u2286 U(x, k), and since\nopt\u2217 is assumed to be insensitive to the omission of non-\u2217-optimal elements from\nU(x, k), it follows that opt\u2217 (U(x, k)) = opt\u2217 (V1).\nLet us now apply these general results to the specific types of optimality introduced\nin the previous section. For all three optimality operators opt>P , optM and optAP ,\nwe shall check whether we can use a Bellman equation to solve the corresponding\noptimal control problem.\n3.2 P-Maximality\nWe first consider the optimality operator opt>P that selects from a set of gambles\n(or paths) S those gambles (or paths) that are the maximal elements of S with\nrespect to the strict partial order >P . The following lemma roughly states that\nthe preference amongst paths with respect to >P is preserved under concatena-\ntion and truncation. It yields a sufficient condition for the principle of optimality\nwith respect to P -maximality to hold. Moreover, the lemma, and the principle of\noptimality, do not necessarily hold for preference with respect to P -maximinity.\nLemma 15 Let k \u2208 [0, N ] and ` \u2208 [k,N ]. Consider the paths (x, k, u\u00b7)` in U(x, k)`\nand (x`, `, v\u00b7), (x`, `, w\u00b7) in U(x`, `). Then (x`, `, v\u00b7) >P (x`, `, w\u00b7) if and only if\n(x, k, u\u00b7)` \u2295 (x`, `, v\u00b7) >P (x, k, u\u00b7)` \u2295 (x`, `, w\u00b7).\nPROOF. Let X, Y and Z be gambles on \u2126. The statement is proven if we can\nshow that Y >P Z impliesX+Y >P X+Z. Assume that Y >P Z. If P(Y \u2212Z) > 0,\nthen P((X + Y ) \u2212 (X + Z)) = P(Y \u2212 Z) > 0. If Y \u2265 Z, then X + Y \u2265 X + Z,\nand finally, if Y 6= Z, then X + Y 6= X + Z. It follows that X + Y >P X + Z.\nProposition 16 (Principle of Optimality) Let k \u2208 [0, N ], x \u2208 Xk and (x, k, u\u2217\u00b7 ) \u2208\nU(x, k). If (x, k, u\u2217\u00b7 ) is P-maximal in U(x, k) then (x`, `, u\u2217\u00b7 ) is P-maximal in\nU(x`, `) for all ` \u2208 [k,N ].\n15\nPROOF. If (x`, `, u\n\u2217\n\u00b7 ) is not P -maximal, there is a path (x`, `, u\u00b7) such that\n(x`, `, u\u00b7) >P (x`, `, u\u2217\u00b7 ). By Lemma 15 we find that\n(x, k, u\u2217\u00b7 )` \u2295 (x`, `, u\u00b7) >P (x, k, u\u2217\u00b7 )` \u2295 (x`, `, u\u2217\u00b7 ) = (x, k, u\u2217\u00b7 ).\nThis means that (x, k, u\u2217\u00b7 )` \u2295 (x`, `, u\u00b7) is preferred to (x, k, u\u2217\u00b7 ), and therefore\n(x, k, u\u2217\u00b7 ) cannot be P -maximal, a contradiction.\nAs a direct consequence of Corollary 5 and Proposition 12, we see that if J (x, k) is\ncompact, then the optimality operator opt>P associated with U(x, k) is insensitive\nto the omission of non->P -optimal elements. Together with Proposition 16 and\nTheorem 14, this allows us to infer a Bellman equation for P -maximality.\nCorollary 17 Let k \u2208 [0, N ] and x \u2208 Xk. If J (x, k) is compact, then for all\n` \u2208 [k,N ]\nopt>P (U(x, k)) = opt>P\n\uf8eb\uf8ed \u22c3\n(x,k,u)`\u2208U(x,k)`\n(x, k, u)` \u2295 opt>P (U(x`, `))\n\uf8f6\uf8f8 , (5)\nthat is, a path is P-maximal if and only if it is a P-maximal concatenation of an\nadmissible path (x, k, u\u00b7)` and a P-maximal path of U(x`, `).\nCorollary 17 results in a procedure to calculate all P -maximal paths. Indeed,\nopt>P (U(x,N)) = {u\u2205} for every x \u2208 XN , and opt>P (U(x, k)) can be calculated re-\ncursively through Eq. (5). It also provides a method for constructing a P -maximal\nfeedback: for every x \u2208 Xk, choose any (x, k, u\u2217\u00b7 (x, k)) \u2208 opt>P (U(x, k)). Then\n\u03c6(x, k) = u\u2217k(x, k) realises a P -maximal feedback.\n3.3 M-Maximality\nWe now turn to the optimality operator optM, defined through (3). If we recall\nProposition 12, we see that optM is insensitive to the omission of non-M-maximal\nelements of U(x, k) whenever J (x, k) is compact. By Proposition 16, optM satisfies\nthe principle of optimality (indeed, if a path is M-maximal, then it must be P -\nmaximal for some P \u2208 M, and by the proposition any truncation of it is also\nP -maximal, hence also M-maximal). This means that the Bellman equation also\nholds forM-maximality under similar conditions as for P -maximality. As already\nmentioned in Section 2.3, both types of optimality coincide if J (x, k) is convex.\n3.4 P-Maximinity\nFinally, we come to the type of optimality associated with the strict partial order\nAP . It follows from Proposition 12 and the discussion at the end of Section 2.2\n16\nr\nr\nr\nr\na\nb\nc\nd\n\u03b1\n\u03b2\n\u03b3\nFig. 4. A Counterexample\nthat if J (x, k) is compact, the optimality operator optAP for U(x, k) is insensitive\nto the omission of non-AP -optimal paths from U(x, k). But, as the following coun-\nterexample shows, we cannot guarantee that the principle of optimality holds for\nAP -optimality, and therefore the dynamic programming approach may not work\nhere. Essentially, this is because the partial order AP is not a vector ordering on\nL(\u2126)\u2014it is not compatible with gain additivity: contrary to expected gains, lower\nexpected gains are not additive.\nExample 18 Consider the dynamical system depicted in Figure 4. Let \u2126 = {], [}\nand denote the gamble ] 7\u2192 x, [ 7\u2192 y by \u3008x, y\u3009. Let P be the vacuous lower\nprevision on \u2126, defined by P(\u3008x, y\u3009) = min{x, y}. Assume that J(\u03b1) = \u30082, 0\u3009,\nJ(\u03b2) = \u30080,\u22121\u3009 and J(\u03b3) = \u3008\u22122, 0\u3009 (there is zero gain associated with the final\nstate). Then \u03b1\u03b2 6AP \u03b1\u03b3: indeed, \u30082,\u22121\u3009 does not dominate \u30080, 0\u3009 point-wise, and\nP(\u30082,\u22121\u3009) = min{2,\u22121} 6> min{0, 0} = P(\u30080, 0\u3009) or equivalently \u30080, 0\u3009 maximises\nthe smallest expected gain. Hence, we find that \u03b1\u03b3 is P-maximin. But \u03b2 AP \u03b3:\nindeed, P(\u30080,\u22121\u3009) = min{0,\u22121} > min{\u22122, 0} = P(\u3008\u22122, 0\u3009), which means that \u03b3\nis not P-maximin. The \u201cprinciple of P-maximin optimality\u201d does not hold here.\nThe following theorem gives a sufficient condition for P -maximality to satisfy the\nprinciple of optimality. It seems that this condition is implicitly assumed to hold\nin most of the literature studying maximin-strategies by dynamic programming.\nThe idea that underlies this theorem is simple: the principle of optimality will hold\nif there is additivity of lower expected gains. In order to formulate the theorem\nin a way that is sufficiently general, we need to introduce a new concept. Assume\nthat the set \u2126 is a Cartesian product of non-empty sets \u21260, \u21261, . . . , \u2126N . Let P be\na lower prevision defined on L(\u2126). Then we call P externally additive relative to\n\u21260, \u21261, . . . , \u2126N if for all Xk in L(\u2126k), where k = 0, . . . , N , it holds that\nP\n(\nN\u2211\nk=0\nXk\n)\n=\nN\u2211\nk=0\nP(Xk),\nwhere we have identified gambles on the \u2126k with the corresponding gambles on \u2126\nthat only depend on the \u03c9k (their so-called cylindrical extensions).\nTheorem 19 Suppose that \u2126 = \u21260 \u00d7 \u21261 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 \u2126N and assume that the gain\ngambles g(xk, uk, k) are a function of \u03c9k only, and similarily, that h(xN) is a\nfunction of \u03c9N only. Let the coherent lower prevision P on L(\u2126) be externally\nadditive relative to the sets \u21260, \u21261, . . . , \u2126N , Then the principle of optimality holds\nfor P-maximinity.\n17\nPROOF. Under the conditions of the theorem, it holds that\nP(J(x, k, u\u00b7)) = P\n(\nN\u22121\u2211\nk=0\ng(xk, uk, k) + h(xN)\n)\n=\nN\u22121\u2211\nk=0\nP(g(xk, uk, k)) + P(h(xN)).\nThis tells us that from the perspective of optimal control, the system is equivalent\nto a classical optimal control system with precisely known gains g\u2032(xi, ui, i) :=\nP(g(xi, ui, i)) and h\n\u2032\nN(xN) := P(h(xN)). Hence, the principle of optimality holds.\nThe conditions of this theorem will of course not be satisfied in general, but they\nwill hold in a number of important special cases. Sometimes the structure of the\nproblem may impose some type of independence for the lower previsions that model\nthe gain uncertainty at different time points. This usually occurs when the system\nlends itself to a game-theoretic interpretation, see for instance [5,15]. We mention\nin passing that external additivity will typically be satisfied if the lower prevision P\non L(\u2126) is some type of independent product of marginal coherent lower previsions\nPk defined on the sets L(\u2126k). Special cases that lead to external additivity are\nfor instance the forward irrelevant product [16], the independent natural extension\n[2,17,18], the Kuznetsov extension [19,20] and the strong independent product [17],\nalso called the type-I product [2,18]. Finally, for a number of very simple impre-\ncise probability models, such as the ones used in Section 4, external additivity is\nimplicitly satisfied.\n3.5 Yet Another Type of Optimality\nWe end this discussion with another type of optimality associated with a strict\npartial order, sometimes called interval dominance, and suggested in a dynamic\nprogramming algorithm by Harmanec in [9, Definition 3.4]. In our setting (precisely\nknown system dynamics), its definition basically reduces to\nX >?P Y if P(X ) > P(Y ) or (X \u2265 Y and X 6= Y ).\nIt can be shown easily that if J (x, k) is compact, the optimality operator induced\nby >?P for U(x, k) is insensitive to the omission of non->?P -optimal paths from\nU(x, k). But, as the following counterexample shows, we cannot guarantee that\nthe principle of optimality holds for >?P -optimality, and therefore the dynamic\nprogramming approach may not work here. Again, this is because the partial\norder AP is not compatible with gain additivity. It also indicates that by solving\nthe Bellman-type equation advocated in [9], we will not necessarily get paths that\nare optimal in the sense described above.\nExample 20 Consider again the dynamical system depicted in Figure 4. As be-\nfore, let \u2126 = {], [}, let P be the vacuous lower prevision on \u2126, and denote the\ngamble ] 7\u2192 x, [ 7\u2192 y by \u3008x, y\u3009. Assume that J(\u03b1) = \u30082, 0\u3009, J(\u03b2) = \u30080, 0\u3009\nand J(\u03b3) = \u3008\u22121,\u22121\u3009 (there is zero gain associated with the final state). Then\n18\n\u03b1\u03b2 6>?P \u03b1\u03b3: indeed, \u30082, 0\u3009 does not dominate \u30081,\u22121\u3009 point-wise, and, P(\u30082, 0\u3009) =\nmin{2, 0} 6> max{1,\u22121} = P(\u30081,\u22121\u3009). Hence, we find that \u03b1\u03b3 is >?P -maximal.\nBut \u03b2 >?P \u03b3: indeed, \u30080, 0\u3009 dominates \u3008\u22121,\u22121\u3009 point-wise, which means that \u03b3 is\nnot >?P -maximal. The \u201cprinciple of >\n?\nP -maximal optimality\u201d does not hold for this\nexample.\n4 A numerical example\nSuppose we have a total amount of money x at our disposal, which we can invest\ninto two companies, denoted by 0 and 1. We denote our investment in company 0\nby u0, and in company 1 by u1. Observe that x, u0 and u1 are non-negative real\nnumbers, and u0 + u1 \u2264 x. The total gain is\nT (x, u0, u1) = \u03c90u0 + \u03c91u1 + \u03c92(x\u2212 u0 \u2212 u1),\nwhere \u03c90 > 0, \u03c91 > 0 are gain factors (for companies 0 and 1), and \u03c92 > 0 is the\ndevaluation factor (of the money we have not invested). We wish to maximise the\ngain, but, we are uncertain about \u03c90, \u03c91 and \u03c92.\n8 We know that \u03c90 = 1+g0+\u000f and\n\u03c91 = 1 + g1 + \u000f. g0 and g1 model the productivities of the companies, and \u000f models\neconomical variations that affect each company in the same way, such as the global\neconomical state. We do not make any assumption about the dependence between\ng0, g1, \u000f and \u03c92. We only know that g0 \u2208 [0.0, 0.3], g1 \u2208 [0.1, 0.2], \u000f \u2208 [\u22120.1, 0.2] and\n\u03c92 \u2208 [0.85, 0.95]. This leads to the following lower prevision on L(\u21260 \u00d7 \u21261 \u00d7 \u21262):\nP(X) = inf\n{\nX(1 + g0 + \u000f, 1 + g1 + \u000f, \u03c92) :\ng0 \u2208 [0.0, 0.3], g1 \u2208 [0.1, 0.2], \u000f \u2208 [\u22120.1, 0.2], \u03c92 \u2208 [0.85, 0.95]\n}\n.\nWe refer to [18] for a detailed discussion about why this lower prevision really\ncaptures the available information. We now wish to find all u0 and u1 such that the\ngain J(x, u0, u1) is P -maximal. Observe that this is a two-dimensional optimisation\nproblem.\nWe formulate this problem in terms of a dynamical system. If we define x0 = x\nand, recursively xk+1 = xk\u2212uk, the total gain is precisely equal to J(x, u\u00b7, 0), with\ng(xk, uk, k, \u03c9) = \u03c9kuk and h(x2, \u03c9) = \u03c92x2. Each state xk represents the money\nwe can invest in companies ` \u2265 k, and should therefore be non-negative. There is\ngain additivity, and the set of admissible gain gambles is compact. Corollary 17\napplies: we can solve this problem using dynamic programming.\nFor k = 1, we find that the control u1 = x1 is optimal from state x1 at time 1.\nIndeed, first observe that all controls are maximal with respect to the point-wise\n8 To ensure that gain gambles are bounded, we can assume that the \u03c9i belong to some\n(sufficiently large) bounded closed real intervals.\n19\norder. In that case, optimality of u1 is equivalent to P(J(x1, u1, 1)\u2212J(x1, v1, 1)) \u2265 0\nfor all v1. This holds iff\nsup\n{\n(1 + g1 + \u000f\u2212 \u03c92)(u1 \u2212 v1) :\ng1 \u2208 [0.1, 0.2], \u000f \u2208 [\u22120.1, 0.2], \u03c92 \u2208 [0.85, 0.95]\n}\n\u2265 0,\nand thus, iff u1 \u2265 v1 for all v1. Hence, optimal paths maximise u1. The highest u1\nwe can choose such that x2 is still non-negative is u1 = x1.\nFor k = 0, the dynamic programming argument says that we only have to consider\nconcatenations of (x0, u0, 0)1 with optimal paths from state x1 = x0\u2212u0, of which\nthere is only one, (x1, x1, 1), as we showed. Again all controls are maximal with\nrespect to the point-wise order. But\nP\n(\nJ((x0, u0, 0)1\u2295 (x0\u2212 u0, x0\u2212 u0, 1))\u2212 J((x0, v0, 0)1\u2295 (x0\u2212 v0, x0\u2212 v0, 1))\n)\n\u2265 0\nalso holds for any u0 and any v0. Indeed, the inequality is equivalent to\nsup\n{\n(g0 \u2212 g1)(u0 \u2212 v0) : g0 \u2208 [0.0, 0.3], g1 \u2208 [0.1, 0.2], \u000f \u2208 [\u22120.1, 0.2]\n}\n\u2265 0\nwhich obviously holds for any choice of u0 and v0. Thus, all paths (x0, u0, 0)1 \u2295\n(x0 \u2212 u0, x0 \u2212 u0, 1) are optimal.\nIn conclusion, the information implies that we should invest all money x, but we\ncannot infer how we should divide x over the two companies.\nBy our dynamic programming approach we have have managed to solve this two-\ndimensional optimisation problem by reducing it to two one-dimensional ones,\nwhich are each very easy to solve. In the more general case of uncertain investment\nwith n companies, we initially have a n-dimensional optimisation problem, and\ndynamic programming reduces this to n very simple one-dimensional optimisation\nproblems.\n5 Conclusion\nThe main conclusion of our work is that the method of dynamic programming can\nin principle be extended to deterministic systems with an uncertain gain, where\nthe uncertainty about the gain is modelled by a coherent lower prevision, or by a\nset of linear previsions (probability measures).\nBut our general study of what conditions a generalised notion of optimality should\nsatisfy for the Bellman approach to work is of some interest in itself too. In particu-\nlar, besides an obvious extension of the well-known principle of optimality, another\ncondition emerges that relates to the nature of the optimality operators per se:\nthe optimality of a path should be invariant under the omission of non-optimal\npaths from the set of paths under consideration. If optimality is induced by a\n20\nstrict partial ordering of paths, then this second condition is satisfied whenever\nthe existence of dominating optimal paths for non-optimal ones is guaranteed.\nAnother important observation is that, in contradistinction to P -maximality and\nM-maximality, the dynamic programming method cannot be used to solve opti-\nmisation problems corresponding to P -maximinity in general: for this notion the\nprinciple of optimality is not guaranteed to not hold, in particular when the ex-\nternal additivity property is not satisfied.\nIt is possible to refine our results by considering an additional equivalence rela-\ntion on paths expressing some notion of indifference\u2014relating, for instance, paths\nwith the same expected gain. This allows us to partition a set opt\u2217 (V) of optimal\nelements into equivalence classes of mutually indifferent paths. Any two paths in\nopt\u2217 (V) that belong to different equivalence classes are necessarily incomparable:\nthe available information, modelled through P , does not allow us to choose be-\ntween these two paths. A discussion of such matters presents no great conceptual\ndifficulties, but has been omitted from the present paper due to limitations of\nspace.\nThroughout the paper we have assumed the system dynamics to be deterministic,\nthat is, independent of \u03c9. This greatly simplifies the discussion, still encompasses\na large number of interesting applications, and does not suffer from the computa-\ntional problems often encountered when dealing with non-deterministic dynamical\nsystems\u2014simply because in general the number of possible (random) paths tends\nto grow exponentially with the size of the state space X . However, we should note\nthat dropping this assumption still leads to a Bellman-type equation, connecting\noperators of optimality associated with random states x : \u2126 \u2192 X . We intend to\npresent our results about and views on this issue elsewhere.\nReferences\n[1] R. Bellman, Dynamic Programming, Princeton University Press, Princeton, 1957.\n[2] P. Walley, Statistical Reasoning with Imprecise Probabilities, Chapman and Hall,\nLondon, 1991.\n[3] M. Cheve\u00b4, R. Congar, Optimal pollution control under imprecise environmental risk\nand irreversibility, Risk Decision and Policy 5 (2000) 151\u2013164.\n[4] L. V. Utkin, S. V. Gurov, Imprecise reliability for some new lifetime distribution\nclasses, Journal of Statistical Planning and Inference 105 (1) (2002) 215\u2013232.\n[5] J. K. Satia, J. Roy E. Lave, Markovian decision processes with uncertain transition\nprobabilities, Operations Research 21 (3) (1973) 728\u2013740.\n[6] C. C. White, H. K. Eldeib, Markov decision processes with imprecise transition\nprobabilities, Operations Research 42 (4) (1994) 739\u2013749.\n21\n[7] J. E. Smith, Generalized Chebychev inequalities: Theory and applications in decision\nanalysis, Operations Research 43 (5) (1995) 807\u2013825.\n[8] R. Givan, S. Leach, T. Dean, Bounded-parameter Markov decision processes,\nArtificial Intelligence 122 (2000) 71\u2013109.\n[9] D. Harmanec, Generalizing Markov decision processes to imprecise probabilities,\nJournal of Statistical Planning and Inference 105 (1) (2002) 199\u2013213.\n[10] B. De Finetti, Theory of Probability: A Critical Introductory Treatment, Wiley,\nNew York, 1974\u20135, two volumes.\n[11] G. de Cooman, M. C. M. Troffaes, Lower previsions for unbounded random variables,\ntechnical report, in progress (2004).\n[12] J. O. Berger, The robust Bayesian viewpoint, in: J. B. Kadane (Ed.), Robustness of\nBayesian Analyses, Elsevier Science, Amsterdam, 1984.\n[13] F. J. Giron, S. Rios, Quasi-Bayesian behaviour: A more realistic approach to decision\nmaking?, in: J. M. Bernardo, J. H. DeGroot, D. V. Lindley, A. F. M. Smith (Eds.),\nBayesian Statistics, University Press, Valencia, 1980, pp. 17\u201338.\n[14] I. Levi, The Enterprise of Knowledge. An Essay on Knowledge, Credal Probability,\nand Chance, MIT Press, Cambridge, 1983.\n[15] A. Nilim, L. E. Ghaoui, Robustness in Markov decision problems with uncertain\ntransition matrices, in: S. Thrun, L. Saul, B. Scho\u00a8lkopf (Eds.), Advances in Neural\nInformation Processing Systems 16, MIT Press, Cambridge, MA, 2004.\n[16] G. de Cooman, E. Miranda, A weak law of large numbers for coherent lower\nprevisions, in: Proceedings of the Tenth International Conference IPMU 2004\n(Information Processing and Management of Uncertainty in Knowledge-Based\nSystems, 4\u20139 July 2004, Perugia, Italy), Vol. 1, Editrice Universita` La Sapienza,\nRome, Italy, 2004, pp. 451\u2013458.\n[17] I. Couso, S. Moral, P. Walley, Examples of independence for imprecise probabilities,\nRisk Decision and Policy 5 (2000) 165\u2013181.\n[18] G. de Cooman, M. C. M. Troffaes, Coherent lower previsions in systems modelling:\nproducts and aggregation rules, Reliability Engineering and System Safety 85 (2004)\n113\u2013134.\n[19] V. P. Kuznetsov, Interval Statistical Models, Radio i Svyaz Publ., Moscow, 1991, in\nRussian.\n[20] F. G. Cozman, Computing lower expectations with Kuznetsov\u2019s independence\ncondition, in: J.-M. Bernard, T. Seidenfeld, M. Zaffalon (Eds.), ISIPTA \u201903 \u2013\nProceedings of the Third International Symposium on Imprecise Probabilities and\nTheir Applications, Carleton Scientific, 2003, pp. 117\u2013186.\n22\n"}