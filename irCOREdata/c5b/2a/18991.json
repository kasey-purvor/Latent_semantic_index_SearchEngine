{"doi":"10.1080\/00131910801934185","coreId":"18991","oai":"oai:eprints.bham.ac.uk:600","identifiers":["oai:eprints.bham.ac.uk:600","10.1080\/00131910801934185"],"title":"The value-added of primary schools: what is it really measuring?","authors":["Gorard, Stephen"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2008-05","abstract":"This paper compares the official value-added scores in 2005 for all primary schools in three adjacent LEAs in England with the raw-score Key Stage 2 results for the same schools. The correlation coefficient for the raw- and value-added scores of these 457 schools is around +0.75. Scatterplots show that there are no low attaining schools with average or higher value-added, and no high attaining schools with below average value-added. At least some of the remaining scatter is explained by the small size of some schools. Although some relationship between these measures is to be expected \u2013 so that schools adding considerable value would tend to have high examination outcome scores \u2013 the relationship shown is too strong for this explanation to be considered sufficient. Value-added analysis is intended to remove the link between a schools\u2019 intake scores and their raw-score outcomes at KS2. It should lead to an estimate of the differential progress made by pupils, assessed between schools. In fact, however, the relationship between value-added and raw scores is of the same size as the original relationship between intake scores and raw-scores that the value-added is intended to overcome. Therefore, however appealing the calculation of value-added figures is, their development is still at the stage where they are not ready to move from being a research tool to an instrument of judgement on schools. Such figures may mislead parents, governors and teachers and, even more importantly, they are being used in England by OFSTED to pre-determine the results of school inspections","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":"Taylor & Francis","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.bham.ac.uk:600<\/identifier><datestamp>\n      2011-12-20T11:22:46Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42:4C4232333631<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42:4C4231353031<\/setSpec><setSpec>\n      7375626A656374733D4C:4C31<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        The value-added of primary schools: what is it really measuring?<\/dc:title><dc:creator>\n        Gorard, Stephen<\/dc:creator><dc:subject>\n        LB2361 Curriculum<\/dc:subject><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LB1501 Primary Education<\/dc:subject><dc:subject>\n        L Education (General)<\/dc:subject><dc:description>\n        This paper compares the official value-added scores in 2005 for all primary schools in three adjacent LEAs in England with the raw-score Key Stage 2 results for the same schools. The correlation coefficient for the raw- and value-added scores of these 457 schools is around +0.75. Scatterplots show that there are no low attaining schools with average or higher value-added, and no high attaining schools with below average value-added. At least some of the remaining scatter is explained by the small size of some schools. Although some relationship between these measures is to be expected \u2013 so that schools adding considerable value would tend to have high examination outcome scores \u2013 the relationship shown is too strong for this explanation to be considered sufficient. Value-added analysis is intended to remove the link between a schools\u2019 intake scores and their raw-score outcomes at KS2. It should lead to an estimate of the differential progress made by pupils, assessed between schools. In fact, however, the relationship between value-added and raw scores is of the same size as the original relationship between intake scores and raw-scores that the value-added is intended to overcome. Therefore, however appealing the calculation of value-added figures is, their development is still at the stage where they are not ready to move from being a research tool to an instrument of judgement on schools. Such figures may mislead parents, governors and teachers and, even more importantly, they are being used in England by OFSTED to pre-determine the results of school inspections.<\/dc:description><dc:publisher>\n        Taylor & Francis<\/dc:publisher><dc:date>\n        2008-05<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.bham.ac.uk\/600\/1\/VA_primary_paper.pdf<\/dc:identifier><dc:relation>\n        public<\/dc:relation><dc:relation>\n        http:\/\/eprints.bham.ac.uk\/600\/1.hassmallThumbnailVersion\/VA_primary_paper.pdf<\/dc:relation><dc:relation>\n        http:\/\/dx.doi.org\/10.1080\/00131910801934185<\/dc:relation><dc:identifier>\n        Gorard, Stephen (2008) The value-added of primary schools: what is it really measuring? Educational Review, 60 (2). pp. 179-185. ISSN 0013-1911<\/dc:identifier><dc:relation>\n        http:\/\/eprints.bham.ac.uk\/600\/<\/dc:relation><dc:language>\n        English<\/dc:language><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["public","http:\/\/eprints.bham.ac.uk\/600\/1.hassmallThumbnailVersion\/VA_primary_paper.pdf","http:\/\/dx.doi.org\/10.1080\/00131910801934185","http:\/\/eprints.bham.ac.uk\/600\/"],"year":2008,"topics":["LB2361 Curriculum","LB Theory and practice of education","LB1501 Primary Education","L Education (General)"],"subject":["Article","PeerReviewed"],"fullText":" 1 \n \n \n \n 'This is an electronic post-print version of an article published in Educational Review \nVol. 60, No. 2 (May 2008): 179-185. Journal of Education Policy is available online \nat: http:\/\/www.tandf.co.uk\/journals\/titles\/0013-1911.asp.  \n \nURL to published version: http:\/\/dx.doi.org\/10.1080\/00131910801934185.  \n \n \nThe value-added of primary schools: what is it really measuring? \n \n \nStephen Gorard  \nSchool of Education \nUniversity of Birmingham \nB15 2TT \ns.gorard@bham.ac.uk \n \n \nAbstract \n \nThis paper compares the official value-added scores in 2005 for all primary schools in \nthree adjacent LEAs in England with the raw-score Key Stage 2 results for the same \nschools. The correlation coefficient for the raw- and value-added scores of these 457 \nschools is around +0.75. Scatterplots show that there are no low attaining schools with \naverage or higher value-added, and no high attaining schools with below average value-\nadded. At least some of the remaining scatter is explained by the small size of some \nschools. Although some relationship between these measures is to be expected \u2013 so that \nschools adding considerable value would tend to have high examination outcome scores \n\u2013 the relationship shown is too strong for this explanation to be considered sufficient. \nValue-added analysis is intended to remove the link between a schools\u2019 intake scores \nand their raw-score outcomes at KS2. It should lead to an estimate of the differential \nprogress made by pupils, assessed between schools. In fact, however, the relationship \nbetween value-added and raw scores is of the same size as the original relationship \nbetween intake scores and raw-scores that the value-added is intended to overcome. \nTherefore, however appealing the calculation of value-added figures is, their \ndevelopment is still at the stage where they are not ready to move from being a research \ntool to an instrument of judgement on schools. Such figures may mislead parents, \ngovernors and teachers and, even more importantly, they are being used in England by \nOFSTED to pre-determine the results of school inspections. \n \n \nIntroduction \n \nMuch has been written about the problems involved in making comparative claims \nabout the relative effectiveness of schools with equivalent pupils (Gorard 2000, 2001, \n2005). There are difficulties in assuming that the indicators of school outcomes are \ncomparable across time, place and curriculum area. There are also difficulties in \nequating outcomes scores for pupils at one age with scores at a later age. These \n 2 \ndifficulties are exacerbated by the limitations of the methods used to address \ncomparability. In general, education analysts do not conduct \u2018active\u2019 studies that \ninvolve allocating pupils to schools, teachers, or examinations for research purposes. \nFor a variety of practical and ethical reasons, analysts find themselves faced with the \nrather more \u2018passive\u2019 analysis of datasets, over which they have no control. The \nproblem with this \u2018post hoc dredging of sullen datasets\u2019 (Gorard 2006a) is that the \nstatistical methods usually involved were designed for use only in active research \n(Lunt 2004).  \n \nThe design of experimental approaches to research allows us to make observations of \ndifference or pattern in practice that can be directly related to a prior theory or \nhypothesis (Gorard 2002). A problem arises, however, when this logic of \nexperimentation is extended to other approaches, such as the regression analyses used \nto create value-added measures (Gorard 2006b). Without a controlled trial, the direct \nlink between a hypothesis and its testing in practice disappears, and is replaced by a \nmuch a weaker form of \u2018test\u2019, such as those based on probability and significance. \nThe results of these can be very misleading (Lunt 2004). For, in most research \nsituations, it is not sampling variation that is the key to understanding and unlocking \nthe process (Ziliak and McCloskey 2004). However, sampling variation is all that \ntraditional statistical analysis addresses, and often not very well at that (Gigerenzer \n2004). Researchers should be more concerned with developing and using indicators of \nthe scientific importance of their results, than with how well the results fit to a rather \narbitrary statistical model. For example, they could ask whether what they have found \nfits observations elsewhere, can be uncovered using a variety of different methods, \nwhether it looks right in practice, or what the dangers might be in assuming that it is \ntrue. \n \nThis paper illustrates these points \u2013 especially the need to be sceptical about results \nthat depend on only one method \u2013 with an important topical example. The \u2018raw\u2019 \nexamination scores produced in different schools are not so much a measure of the \nimpact of the schools as of the ability and outcome scores of their allotted pupils. In \norder to decide which schools are making differential progress with their pupils, the \nDfES in England is now producing value-added scores for each school. These value-\nadded scores attempt to measure the differential progress made by strictly equivalent \npupils in different schools.  \n \n \nMethods \n \nIn this \u2018value-added\u2019 analysis, the prior attainment of each pupil is taken into account, \nsuch that the published official figures reflect not the intake to the school but the \naverage progress made by pupils while in the school. The DfES value-added scores for \nthe average pupil progress from Key Stage 1 (KS1, the prior attainment of the pupil \naged 7 at primary school) to Key Stage 2 (attainment at age 11) in each secondary \nschool are calculated as follows (fuller details are available at DfES 2006).  \n \nMost independent schools, infant-only schools, pupil referral units and schools with \nless than five pupils in the age group are excluded. Otherwise, for the 2005 figures, all \npupils in an eligible school were included who were eligible for KS2, still on the school \nroll in May 2005, and with a matched KS1 score. Each pupil KS1 and KS2 outcome \n 3 \nwas awarded a point score (so that working towards a level 1 at KS1 is awarded 3 \npoints, and level 4 or more converts to 27 points). The scores for overall reading, \nwriting and mathematics at KS1, and the scores for English, mathematics and science at \nKS2, were then averaged for each pupil. A pupil\u2019s value-added score is calculated by \ncomparing their KS2 average with the median KS2 score for all pupils with the same \nKS1 score. Thus, in mainstream schools, a pupil with an average of level 1 at KS1 (9 \npoints) might be expected to attain an average of level 3 at KS2 (21 points), for \nexample. The value-added score for each school is the average of the value-added \nscores for all pupils meeting the definition above (with 100 added to this average to \neliminate negative values). 100 is near par, and a value-added score between 99.4 and \n101.2 is described as \u2018broadly average\u2019.  \n \nThis paper uses the Key Stage 2 (KS2) results for mainstream primary schools in \nEngland in 2005, and their published DfES value-added scores. The re-analysis \npresented here is based on all 457 primary schools with complete information in York, \nLeeds, and North Yorkshire.  Results are presented in scatterplot form, or as Pearson R \ncorrelation co-efficients - which can be squared to give an \u2018effect\u2019 size. The approach is \nvery similar to that used in Gorard (2006c), which demonstrated that value-added \nscores in secondary schools in England are no more independent of raw-scores than the \nraw-scores are independent of the schools\u2019 intake values. Paterson (1997) found \nsimilarly high correlations between raw scores and the results of regression analyses \nbased on pupils\u2019 prior qualification. \n \n \nThe same correlation appears \n \nFigure 1 shows that the same relationship, previously noted in the DfES value-added \nfigures for secondary schools in England and by Paterson (1997) in Scotland, also \nappears in the DfES value-added figures for primary schools. There is a very clear \nquasi-linear relationship between the KS2 raw-score for any primary school and its \neventual value-added score. All of the high value-added schools (e.g. above 101) have \nrelatively high raw scores (e.g. around 27 points or above). All of the low value-added \nschools (e.g. below 99) have relatively low raw scores (e.g. below 29 points).  \n \nFigure 1 \u2013 Crossplot of value-added scores against Key Stage 2 results, 457 primary \nschools, 2005 \n 4 \n96\n97\n98\n99\n100\n101\n102\n103\n104\n20 22 24 26 28 30 32 34\n \nNote: The graph shows the DfES KS1-KS2 value-added scores, and the average \npoints score per pupil at Key Stage 2 for all primary schools in York, Leeds, and \nNorth Yorkshire LEAs. \n \nBoth the KS1 and KS2 scores, of course, contain a considerable but unknown element \nof error. The Key Stage tests may have less than perfect validity in what they \npurportedly measure, candidates may make untypical mistakes in responding to \nquestions, some teachers and schools may condone \u2018sharp practice\u2019 in administering \nthe tests, some candidates will be missing, and some candidates will have missing \nscores. There may be mistakes in the marking, recording and computing of the KS2 \npoints per school. The marking is to a threshold in which the achievement of two \npupils just above and below a threshold may actually be closer than the achievement \nof two pupils awarded the same grade. The grades are converted to a points score, \nwhich changes the metric and may create additional distortions in the data. The value-\nadded scores are then created from these two imperfect sets of figures, and the value-\nadded model is only one of many possible, requiring a number of untestable analytical \nassumptions based on subjective judgements. This level of uncertainty in the result \ncould be sufficient to explain the apparent differences between the value-added scores \nof schools with similar raw-scores in Figure 1.  \n \nThe correlation between the primary schools\u2019 value-added score and their KS2 results \nis +0.74 (Pearson\u2019s R). One of the major reasons why this correlation is lower than \nthat previously published for secondary schools (Gorard 2006c) is that primary \nschools are generally much smaller, with fewer pupils in each cohort. Therefore, there \nis more volatility in the figures (or put another way, the measurement problems \noutlined above are more apparent \u2013 see also Tymms and Dean 2004). One way of \nassessing whether this is the correct interpretation is to examine the correlation for \nlarge and small primary schools separately. If the correlation is lower for small \nschools but larger for large schools then this is an indication that the volatility of \nsmall schools helps makes the correlation \u2018appear\u2019 smaller than it is at the secondary \nlevel.  \n \n 5 \nThis is what happens. The correlation between primary schools\u2019 value-added score \nand their KS2 results drops to +0.69 for the 353 schools with less than 50 pupils in the \ncohort, and to +0.67 for the 255 schools with less than 32 pupils, for example. The \ncorrelation rises to +0.76 for the 354 schools with more than 18 pupils, and to +0.78 \nfor the 258 schools with more than 30 pupils, for example. All of the schools with 50 \nor more pupils in the cohort had value-added scores in the narrow range of 98 to 102 \nand, in general, the schools with the most extreme value-added scores had very few \npupils. All of this suggests that the school-level value-added scores can be explained \nto a large extent by the actual level of attainment of pupils at KS2 (i.e. the raw \nscores), and the apparent differences (the width of the scatter in Figure 1) can be \nexplained by measurement error and the volatility of small numbers.1\n \n  \nSome commentators might suggest that Figure 1 actually shows two different kinds of \nregression. In addition to the bottom-left to top-right pattern discussed so far, there is \nalso a sequence of top-left to bottom-right \u2018lines\u2019. But there is no way of \ndistinguishing such a conceptual sequence from the scatter and volatility described \nabove. The appearance of the graph itself is affected by the scale chosen, and a visual \ncomparison of the two kinds of slopes is, therefore, not a reliable guide to the overall \npattern. If the pattern in Figure 1 had been close to a perfect diamond shape with \ncorners at (27, 103), (27, 97), (23, 100), and (31, 100) then the correlation would be \nzero, or very close to zero. If, on the other hand, there had been an appreciable \nnegative slope then the correlation would have been negative overall. But +0.74 is a \nvery high correlation \u2013 considerably higher than standard in the educational literature \n\u2013 representing an \u2018effect\u2019 size of 55%. It is also a positive correlation, representing the \npositive left-right slope while ignoring the negative one. \n \n \nDiscussion \n \nIf accepted, then the re-analysis above, coupled with the similar analysis of the results \nfor all secondary schools in England (Gorard 2006c), suggests two important kinds of \nconclusion. The first kind of conclusion that can be drawn is methodological. Many \nanalysts agree that value-added comparisons of the kind conducted so far by DfES are \nproblematic (e.g. Tymms and Dean 2004, Schagen 2006). Their usual response is to \ntry and make this complex analysis even more complex. But without confirmatory \nevidence of a different nature, and no sceptical consideration of the meaning of the \nmeasures involved, there is a danger that large-scale complex analyses such as those \nconsidered here are rhetorically misleading. In fact, the changes and differences \nidentified as school effects may be largely chance processes, with a greater random \nelement than traditional analysts allow (Pugh and Mangan 2003). How do we know \nthat the variation in value-added scores for different schools means anything at all? \nThere is no external standard or arbiter to which we can refer. The calculations look \nplausible enough, but no one had predicted the level of correlation found between \nraw- and value-added scores. In fact, on hearing of it, commentators at the DfES first \ndenied the correlation, and then attributed it to some peculiarity of schools in \nYorkshire, briefing the education minister in the House of Lords to state this in \n                                                 \n1 In fact, since a value-added score is, in essence, the difference between prior and subsequent \nattainment figures, one would expect around half of the variance in VA figures to be explained by \neither of these raw-scores, leading to two correlations of around 0.7 each.  \n \n 6 \nresponse to a query by another member (in Hansard \u2013 see \nhttp:\/\/www.publications.parliament.uk\/pa\/ld200506\/ldhansrd\/vo050620\/text\/50620w0\n2.htm). On being shown that the same relationship held for all secondary schools in \nEngland, the reply by Lord Adonis was not re-addressed, and the findings were \nsimply ignored.  \n \nMaking the official value-added analysis more complex, via the addition of contextual \ninformation about the pupils, may disguise but will not solve the problem highlighted \nin this paper. The additional complexity will reduce further the number of potential \ncritics able to understand the methods. The use of additional information about the \nsocial background of pupils is likely to decrease the scatter shown in Figure 1, making \nthe relationship between school intakes and school outcomes stronger. The inclusion \nof social background information in school performance figures will also have the \nunintended consequence that we will no longer be able to consider the extent to which \nschools do, or do not, compensate for differences in those backgrounds. At present, if \nVA worked, we could see whether schools were equally effective for rich and poor \npupils, by disaggregating the VA by eligibility for free school meals (FSM), for \nexample. Using contextualised VA with FSM factored into the calculation, it does not \nmake sense any longer to disaggregate by FSM. And the same point can be made \nabout ethnicity, language, and special need. Plans to make value-added analysis more \ncomplex, through the use of advanced regression techniques will also be \ncounterproductive. They will also reduce further the number of potential critics able \nto understand the methods, but can not overcome the problem of correlation noted \nhere.  \n \nThe percentage variation at school level, usually termed the \u2018school effect\u2019, is small \nand suggests incorrectly that schools are making little difference to their pupils. There \nis also the confusing situation that the same school may appear to be effective on one \nmeasure (such as attainment) but not another (such as dropout), or effective for one \nage group and not another. Therefore policies based on VA results and designed to \nimprove test performance for one age group can hurt performance in other areas \n(Rumberger and Palardy 2005). The solution to all of these issues is not a more \ncomplex value-added analysis. The solution lies in re-thinking what it is that we want \nvalue-added analysis to achieve. There are simpler and more scientific alternatives to \nmeasuring the impact of schools. One alternative suggested recently requires no \nconsideration of prior attainment or contextual variables, relying instead on the \ndiscontinuity of school years or grades to estimate the absolute effect of going to a \nschool in comparison to not going to school at all (Luyten 2006). \n \nThe second kind of conclusion from this paper is more practical. Until concerns about \nvalue-added analyses have been resolved, it is not reasonable to use them for practical \npurposes.2\n                                                 \n2 Clearly, there will never be an ideal measure able perfectly to summarise the performance of a school. \nThat is not the point. If accepted, what this paper shows is the DfES approach is nothing like a solution \nto the problem of measuring pupil progress independently of their raw-score attainment. It is neither \ngood enough, nor even the best approach currently available.  \n Parents cannot rely on them when choosing schools. School leaders cannot \nrely on them to judge the effectiveness of teachers or departments, and officials \ncannot rely on them to make decisions about the quality of education delivered in \nschools. Rather, what this re-analysis shows is that schools with a low-attaining pupil \n \n 7 \nintake have, ceteris paribus, low raw-scores at KS2, and that the \u2018value-added\u2019 scores \ndo almost nothing to overcome this clear pattern. Therefore, these value-added scores \nare not, as the DfES has claimed, independent of actual levels of raw-score \nattainment.  \n \nAn example of why this matters comes from the revised OFSTED light-touch school \ninspections in England. Inspectors from OFSTED are spending less time in schools, \nand making fewer lesson observations, on each inspection. The reduced reliance on \nprimary observation has, in the reports of some school leaders, led to an increased \nreliance on prior value-added analyses of the schools (Bald 2006). This has led to \nclear anomalies and \u2018bizarre judgements\u2019 such as a school being judged largely \u2018good\u2019 \nor \u2018outstanding\u2019 on observation, but being reported as merely \u2018satisfactory\u2019 because \nthe best outcome allowable by OFSTED was constrained by a relatively low prior \nvalue-added score (Mansell 2006a, Slater 2006). The increased reliance on \ncontextualised value-added (at time of writing), which is very sensitive to exclusions \nfor example, leads to some schools getting lower than expected inspection results \n(Mansell 2006b). As Paterson pointed out as early as 1997, pupil-level regression of \nthe kind now in use by the DfES is a fascinating and productive research tool, which \ncan be used to inform professional debate. But it should not yet be used directly as a \ntool for pupil, teacher or school assessment.  \n \n \nReferences \n \nBald, J. (2006) Inspection is now just a numbers game, Times Educational \nSupplement, 26\/5\/06, p.21 \nDfES (2006) Value-added technical information, \nhttp:\/\/www.dfes.gov.uk\/performancetables\/primary_03\/p5.shtml, accessed 15th \nMarch 2006 \nGigerenzer, G. (2004) Mindless statistics, American Economic Review, 33, 5, 587-606 \nGorard, S. (2000) 'Underachievement' is still an ugly word: reconsidering the relative \neffectiveness of schools in England and Wales, Journal of Education Policy, 15, \n5, 559-573 \nGorard, S. (2001) International comparisons of school effectiveness: a second \ncomponent of the 'crisis account'?, Comparative Education , 37, 3, 279-296 \nGorard, S. (2002) Fostering scepticism: the importance of warranting claims, \nEvaluation and Research in Education, 16, 3, 136-149 \nGorard, S. (2005) Academies as the \u2018future of schooling\u2019: is this an evidence-based \npolicy?, Journal of Education Policy, 20, 3, 369-377 \nGorard, S. (2006a) Towards a judgement-based statistical analysis, British Journal of \nSociology of Education, 27, 1, 67-80 \nGorard, S. (2006b) Using everyday numbers effectively in research, London: \nContinuum \nGorard, S. (2006c) Value-added is of little value, Journal of Educational Policy, 21, \n2, 233-241 \nLunt, P. (2004) The significance of the significance test controversy: comments on \n\u2018size matters\u2019, American Economic Review, 33, 5, 559-564 \nLuyten, H. (2006) An empirical assessment of the absolute effect of schooling: \nregression-discontinuity applied to TIMSS-95, Oxford Review of Education, 32, \n3, 397-429 \n 8 \nMansell, W. (2006a) Puzzle of new OFSTED ratings, Times Educational Supplement, \n9\/6\/06, p.6 \nMansell, W. (2006b) Shock of low score drives heads to resign, Times Educational \nSupplement, 9\/6\/06, p.6 \nPaterson, L. (1997) A commentary on methods currently being used in Scotland to \nevaluate schools statistically, pp. 298-312 in Watson, K., Modgil, C. and \nModgil, S. (Eds.) Educational dilemmas: debate and diversity, London: Cassell \nPugh, G. and Mangan, J. (2003) What\u2019s in a trend? A comment on Gray, Goldstein \nand Thomas (2001), British Educational Research Journal, 29, 1, 77-82 \nRumberger, R. and Palardy, G. (2005) Test scores, dropout rate, and transfer rates as \nalternative indicators of high school performance, American Educational \nResearch Journal, 42, 1, 3-42 \nSchagen, I. (2006) The use of standardized residuals to derive value-added measures \nof school performance, Educational Studies, 32, 2, 119-132 \nSlater, J. (2006) Anger as Ofsted\u2019s \u2018raised bar\u2019 bites, Times Educational Supplement, \n26\/5\/06, p.15 \nTymms, P. and Dean, C. (2004) Value-added in the primary school league tables: a \nreport for the National Association of Head Teachers, Durham: CEM Centre \nZiliak, S. and McCloskey, D. (2004) Significance redux, American Economic Review, \n33, 5, 665-675 \n \n"}