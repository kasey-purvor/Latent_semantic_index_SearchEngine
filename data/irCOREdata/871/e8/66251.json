{"doi":"10.1364\/AO.46.001089","coreId":"66251","oai":"oai:dro.dur.ac.uk.OAI2:2251","identifiers":["oai:dro.dur.ac.uk.OAI2:2251","10.1364\/AO.46.001089"],"title":"Durham extremely large telescope adaptive optics simulation platform.","authors":["Basden, A. G.","Butterley, T.","Myers, R. M.","Wilson, R. W."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2007-03-01","abstract":"Adaptive optics systems are essential on all large telescopes for which image quality is important. These are complex systems with many design parameters requiring optimization before good performance can be achieved. The simulation of adaptive optics systems is therefore necessary to categorize the expected performance. We describe an adaptive optics simulation platform, developed at Durham University, which can be used to simulate adaptive optics systems on the largest proposed future extremely large telescopes as well as on current systems. This platform is modular, object oriented, and has the benefit of hardware application acceleration that can be used to improve the simulation performance, essential for ensuring that the run time of a given simulation is acceptable. The simulation platform described here can be highly parallelized using parallelization techniques suited for adaptive optics simulation, while still offering the user complete control while the simulation is running. The results from the simulation of a ground layer adaptive optics system are provided as an example to demonstrate the flexibility of this simulation platform. \\u","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/66251.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/2251\/1\/2251.pdf","pdfHashValue":"0b080ebbc5ef7a76c49b7aaf13f965d55b9343ec","publisher":"Optical Society of America","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:2251<\/identifier><datestamp>\n      2017-03-10T15:04:24Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Durham extremely large telescope adaptive optics simulation platform.<\/dc:title><dc:creator>\n        Basden, A. G.<\/dc:creator><dc:creator>\n        Butterley, T.<\/dc:creator><dc:creator>\n        Myers, R. M.<\/dc:creator><dc:creator>\n        Wilson, R. W.<\/dc:creator><dc:description>\n        Adaptive optics systems are essential on all large telescopes for which image quality is important. These are complex systems with many design parameters requiring optimization before good performance can be achieved. The simulation of adaptive optics systems is therefore necessary to categorize the expected performance. We describe an adaptive optics simulation platform, developed at Durham University, which can be used to simulate adaptive optics systems on the largest proposed future extremely large telescopes as well as on current systems. This platform is modular, object oriented, and has the benefit of hardware application acceleration that can be used to improve the simulation performance, essential for ensuring that the run time of a given simulation is acceptable. The simulation platform described here can be highly parallelized using parallelization techniques suited for adaptive optics simulation, while still offering the user complete control while the simulation is running. The results from the simulation of a ground layer adaptive optics system are provided as an example to demonstrate the flexibility of this simulation platform. \\ud\n<\/dc:description><dc:subject>\n        Ocean optics<\/dc:subject><dc:subject>\n         Wave-front sensing<\/dc:subject><dc:subject>\n         Adaptive optics.\\ud\n<\/dc:subject><dc:publisher>\n        Optical Society of America<\/dc:publisher><dc:source>\n        Applied optics, 2007, Vol.46(7), pp.1089-1098 [Peer Reviewed Journal]<\/dc:source><dc:date>\n        2007-03-01<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:2251<\/dc:identifier><dc:identifier>\n        issn:0003-6935<\/dc:identifier><dc:identifier>\n        doi:10.1364\/AO.46.001089<\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/2251\/<\/dc:identifier><dc:identifier>\n        https:\/\/doi.org\/10.1364\/AO.46.001089<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/2251\/1\/2251.pdf<\/dc:identifier><dc:rights>\n        \u00a9 2007 Optical Society of America.\\ud\n\\ud\n<\/dc:rights><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["issn:0003-6935","0003-6935"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2007,"topics":["Ocean optics","Wave-front sensing","Adaptive optics.\\ud"],"subject":["Article","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n10 November 2010\nVersion of attached file:\nPublished Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nBasden, A. G. and Butterley, T. and Myers, R. M. and Wilson, R. W. (2007) \u2019Durham extremely large\ntelescope adaptive optics simulation platform.\u2019, Applied optics., 46 (7). pp. 1089-1098.\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1364\/AO.46.001089\nPublisher\u2019s copyright statement:\n2007 Optical Society of America.\nAdditional information:\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\nDurham extremely large telescope adaptive optics\nsimulation platform\nAlastair Basden, Timothy Butterley, Richard Myers, and Richard Wilson\nAdaptive optics systems are essential on all large telescopes for which image quality is important. These are\ncomplex systems with many design parameters requiring optimization before good performance can be\nachieved. The simulation of adaptive optics systems is therefore necessary to categorize the expected\nperformance. We describe an adaptive optics simulation platform, developed at Durham University,\nwhich can be used to simulate adaptive optics systems on the largest proposed future extremely large\ntelescopes as well as on current systems. This platform is modular, object oriented, and has the benefit\nof hardware application acceleration that can be used to improve the simulation performance, essential\nfor ensuring that the run time of a given simulation is acceptable. The simulation platform described here\ncan be highly parallelized using parallelization techniques suited for adaptive optics simulation, while\nstill offering the user complete control while the simulation is running. The results from the simulation\nof a ground layer adaptive optics system are provided as an example to demonstrate the flexibility of this\nsimulation platform. \u00a9 2007 Optical Society of America\nOCIS codes: 010.1080, 010.7350.\n1. Introduction\nAdaptive optics (AO) is a technology widely used in\noptical and infrared astronomy, and almost all large\nscience telescopes have an AO system. A large num-\nber of results have been obtained using AO systems,\nwhich would otherwise be impossible for seeing-\nlimited observations.1,2 New AO techniques are being\nstudied for novel applications such as wide-field high-\nresolution imaging3 and extra-solar planet finding.4\nThe simulation of an AO system is important as it\nhelps to determine how well the AO system will per-\nform. Such simulations are often necessary to deter-\nmine whether a given AO system will meet its design\nrequirements, thus allowing scientific goals to be\nmet. Additionally, new concepts can be modeled, and\nthe simulated performance of different AO tech-\nniques compared,5 allowing informed decisions to be\nmade when designing or upgrading an AO system\nand when optimizing the system design parameters.\nA full end-to-end AO simulation will typically in-\nvolve several stages.6 First, a representation of the\natmospheric turbulence is produced, typically by gen-\nerating simulated atmospheric phase screens, often by\nusing several different screens representing turbu-\nlence at different atmospheric heights. The aberrated\ncomplex wave amplitude at the telescope aperture is\nthen generated by modeling this atmospheric phase as\nseen from the telescope pupil. For a stratified atmo-\nspheric model this will involve propagating the atmo-\nspheric phase screens across the pupil to simulate the\neffect of the relative velocity of different atmospheric\nlayers. Thewavefront at the pupil is then passed to the\nsimulated AO system, which will typically include one\nor more wavefront sensors and deformable mirrors\nand a feedback algorithm for closed-loop operation.\nAdditionally, one or more science point-spread func-\ntions (PSFs) as corrected by the AO system are calcu-\nlated. Information about the AO system performance\nis computed from the PSFs, including quantities such\nas the Strehl ratio and encircled energy.\nThe computational requirements for AO simula-\ntion scale rapidly with telescope size, and thus sim-\nulation of the largest telescopes cannot be done\nwithout special techniques, some of which follow:\n1. Multiprocessor parallelization7,8 allows compu-\ntations to be spread acrossmultiple processors, though\nit can suffer from data bandwidth bottlenecks, as often\ndata cannot be transferred between processors at a\nThe authors are with the Centre for Advanced Instrumenta-\ntion, Department of Physics, Durham University, South Road,\nDurham DH1 3LE, UK. A. Basden\u2019s e-mail address is a.g.basden@\ndurham.ac.uk.\nReceived 24 April 2006; revised 19 October 2006; accepted 31\nOctober 2006; posted 2 November 2006 (Doc. ID 70199); published\n12 February 2007.\n0003-6935\/07\/071089-10$15.00\/0\n\u00a9 2007 Optical Society of America\n1 March 2007 \u0002 Vol. 46, No. 7 \u0002 APPLIED OPTICS 1089\nrate sufficient to keep them processing for a large\nproportion of the time.\n2. The use of dedicated hardware for algorithm ac-\nceleration9 can produce large performance improve-\nments, though it is somewhat inflexible.\n3. Analytical models can also be used,10 and these\ncan give rapid results, though they are not able to\nrepresent noise sources easily.\nHere we describe the approaches that we have\ntaken to implement an efficient and scalable simula-\ntion framework.\nAt Durham University we have been developing\nAO simulation codes for over 10 years.11 The code has\nrecently been rewritten to take advantage of new\nhardware, new software techniques, and to allow\nmuch greater scalability for advanced simulation of\nAO systems for extremely large telescopes (ELTs),\nincluding multiconjugate AO (MCAO) and extreme\nAO (XAO) systems.12\nThe Durham AO simulation platform uses the high-\nlevel programming language Python (currently, Py-\nthon 2.4), to select and link together C (American\nNational Standards Institute standard with the Gnu\nCompiler Collection (GCC) version 3.3), Python, and\nhardware accelerated algorithms, as well as third-\nparty modules, giving a great deal of flexibility. This\nallows us to rapidly prototype and develop new and\nexisting AO algorithms, and to prepare new AO sys-\ntem simulations quickly by using the Python code. The\nuse of C and hardware algorithms ensures that pro-\ncessor intensive parts of the simulation platform can\nbe implemented efficiently. The C and Python algo-\nrithms make use of optimized libraries including the\nFFTW (versions 2 and 3), the AMD core math library\n(version 3.5, for use on AMD platforms, including\nBLAS and LAPACK routines), the GNU scientific li-\nbrary (currently version 0.7), and the MPICH library\n(optimized for the Cray XD1). This ensures that high\nperformance can be achieved for computationally in-\ntensive algorithms. The hardware accelerated algo-\nrithms are implemented within field programmable\ngate arrays (FPGAs), which can be programmed to\nprovide impressive performance improvements over a\nstandard software implementation. The VHDL hard-\nware description language is used to program the\nFPGAs, using the Xilinx ISE 7.1 compiler.\nThe simulation software will run on most Unix-like\noperating systems, including Linux and Mac OS X.\nThe simulation platform hardware at Durham con-\nsists of a Cray XD1 supercomputer,13 which contains\nreprogrammable hardware for application accelera-\ntion as well as six dual Opteron processor nodes each\nwith 8 Gbits RAM. Additionally, a distributed cluster\nof conventional Unix workstations is connected by\ngigabit ethernet. For most simulation tasks, only the\nXD1 is required, though for large models, or when\nmultiple simulations are run simultaneously, the en-\ntire distributed cluster can be used. The simulation is\nprogrammed intelligently to make use of optimized\nlibraries and hardware acceleration when these are\navailable, and to use default library replacements\nwhen not (for example, the AMD core math library is\nnot available on a Mac OS X platform).\nThe simulation is object orientated, with high-level\nobjects (for example, a phase screen generation object\nand a wavefront sensing object) being connected to-\ngether, allowing data to pass between them in a\ndirection described by the user (for example, atmo-\nspheric phase screens may be passed to a deformable\nmirror object). The high-level simulation objects can\ncontain instances of lower-level objects, which are\ninternal to the simulation objects and used during\ncalculations, for example, a telescope pupil mask ob-\nject used to define which parts of the atmospheric\nphase screens are sampled by the wavefront sensor.\n2. Extremely Large Telescope Simulation\nRequirements\nWhen attempting to create a realistic simulation of\nan AO system on an ELT, a large amount of comput-\ning power, memory, and bandwidth will be required.\nThe Durham simulation platform provides these re-\nquirements by implementing several key technolo-\ngies.\nA. Multiple Processor Simulation Platform\nThe Durham simulation platform allows a simulation\nto comprise multiple processes, meaning that different\nparts of the simulation can run on different processors\nand even different computers. However, this means\nthat communication between the processes is essen-\ntial. To maximize the efficiency of the simulation, we\nuse a combination of shared memory access [where\nprocesses have access to the samememory, e.g., within\na symmetric multiprocessor (SMP) system] and mes-\nsage passing interface (MPI) communications where\nappropriate, and a simulation user has control over the\ntype of communication used.\n1. Shared Memory Access\nShared memory access allows multiple processes to\naccess the same region of computer memory. All\nprocesses can usually have read and write access to\nthis memory. Using shared memory allows a single\nmemory block to be shared between processes, thus\nreducing the overall memory requirements, and\nalso reducing the processor overhead, since produc-\ning an identical copy of the data for each different\nprocess is not then essential. Figure 1 is a schematic\nshowing how a typical shared memory system can\noperate. Shared memory buffers are created by us-\ning the Unix shm_open() function call and are\nmapped into a processor\u2019s virtual address space.\nStandard synchronization primitives such as sema-\nphores are used to ensure that no processes are\nreading the shared memory region while it is being\nwritten to and to ensure that only one process at a\ntime can write to the shared memory region.\nThe Durham simulation platform hides the use of\nsynchronization primitives (in this case semaphores)\nfrom the user (and simulation objects), such that the\nparallel processes will read and write to the shared\nmemory region only when it is appropriate to do so.\n1090 APPLIED OPTICS \u0002 Vol. 46, No. 7 \u0002 1 March 2007\nThis removes the possibility of data corruption, while\nproviding a simplified interface for the simulation\nprogrammer.\n2. Message Passing Interface Communication\nCommunication between distributed systems that do\nnot share memory requires copies of data sets to be\npassed between the systems. When the data set is\nlarge, or when a large number of small data sets are\npassed, a bottleneck can occur as processes will then\nspend a significant amount of time waiting for a data\nset to arrive or be sent. It is therefore essential that\nthe communication method used to transfer these\ndata sets be as efficient as possible, having both a low\nlatency (so that time is not wasted when sending\nsmall data sets) and a high sustained bandwidth (so\nthat large data sets can be sent in a minimum time).\nThe Durham simulation platform uses the MPI li-\nbrary for this communication as this allows data to be\npassed efficiently with only a small latency, particu-\nlarly on the XD1 system. The Cray XD1 has an opti-\nmized version of the MPI library, which is targeted to\nthe hardware architecture of this system, making ef-\nficient use of the RapidArray Transport interface, the\ncommercial high-bandwidth interconnect found in\nXD1 systems. Using the Durham system, we mea-\nsured a MPI communication latency of only 1.6 \u0002s and\na maximum sustained bandwidth of 1.4 Gbytes s\u00031\nbetween the computing nodes.\n3. Process Parallelization\nEach processor used for a given simulation will be\ngiven only one process to run to reduce context switch-\ning delays. Each of these processes will contain one or\nmore simulation objects, which are able to access the\nvirtual memory space of other objects within the pro-\ncess, making data transfer between these objects triv-\nial (e.g., the address of a data array can be passed). All\nsimulation objects are executed in a single thread, car-\nrying out their computations for each iteration in turn,\nwhich again reduces context switching delays.\nObjects in separate processes are able to pass data\nby using either MPI communications or shared mem-\nory as appropriate. When such communication is re-\nquired, a pair of high-level communication objects are\ncreated and are responsible for dealing with a partic-\nular communication link (MPI or shared memory).\nThese communication objects are then connected to\nthe simulation objects, which can then behave as\nif they were connected directly to the object with\nwhich they wish to communicate. Each simulation\nobject has a basic set of methods and data objects,\nwhich are viewable by other objects. The communi-\ncation objects then merely have to implement these\nmethods and data objects, transferring data as ap-\npropriate. The use of communication objects is trans-\nparent to the simulation objects, being handled by the\nsimulation framework.\nB. Hardware Acceleration\nThe Durham AO simulation platform is able to accel-\nerate specific parts of the AO simulation by using re-\nconfigurable logic hardware, FPGAs, and hence\nreducing the time taken for a given simulation to com-\nplete. These FPGAs are an integrated part of the Cray\nXD1 supercomputer,9 and, when used correctly, are\ncapable of reducing the execution time of some algo-\nrithms by 2 to 3 orders of magnitude14 while at the\nsame time, freeing the CPU for other operations. This\ngreatly improves the speed at which the simulation\ncan run and is essential for simulation of large AO\nsystems. Implementing algorithms within the FPGAs\nrequires knowledge of a hardware programming lan-\nguage, and so we have developed common libraries\nthat can be plugged into an existing simulation, for\nexample, a wavefront sensor pipeline. The simulation\nuser therefore requires no hardware knowledge, and\nyet can achieve significant impressive performance im-\nprovements using the hardware acceleration.\nC. Simulation Creation\nA user creates a new simulation by selecting and\nlinking together the various simulation modules as\nrequired, either graphically or in a text file. New\nmodules (for example, to investigate a new type of\nwavefront sensor or deformable mirror) can easily be\ncreated and added to the simulation with minimal\neffort. Once the simulation file has been set up, a\nparameter file is created, which contains all variables\nand configuration objects required by the simulation.\nThis parameter file is in XML format and allows an\nembedded Python code, which can be used to create\ncomplicated variables and objects. If suitably defined,\na cross-simulation parameter file could be created by\nusing a Python parser for the XML. The parameter\nfile can be created by using a graphic interface, which\nhas the capability to automatically create a skeleton\nparameter file from the simulation file, and then al-\nlow the user to adjust the default values of variables.\nThis allows a new simulation to be set up quickly by\nan inexperienced user.\nD. Simulation Control\nControl of a running simulation is achieved by con-\nnecting to it by using either the Python command line\nor graphic tools. This gives the users complete con-\ntrol over a simulation, allowing them to stop, start,\nFig. 1. Schematic showing how a typical shared memory system\noperates. Some processes will have read and write access to the\nmemory, while others will have read access only. Synchronization\nprimitives will be used to ensure that data are not read while being\nwritten and vice versa.\n1 March 2007 \u0002 Vol. 46, No. 7 \u0002 APPLIED OPTICS 1091\nand pause, as well as analyze (allowing them to cre-\nate plots of parts of the data chain, for example, sub-\naperture images) and change the current state of a\nsimulation (for example, changing the value of a vari-\nable or the contents of an array). This high degree of\nflexibility is achieved by allowing the users to send\ntext strings to the simulation, which are treated as\nPython code, and executed as a separate thread that\nhas access to the global name space. The user can\ntherefore access and alter any part of the simulation,\nand any requested data can be returned to the user\nfor further analysis. When a simulation comprises\nmore than one process, the user can connect to any or\nall of these processes.\nThis control facility is completely detachable from\nthe simulation and can be started and stopped with-\nout affecting simulation operation. It is also possible\nto have several users connected to the same simula-\ntion at any given time from anywhere that has inter-\nnet access to the computers running the simulation.\nFigure 2 shows a screen shot of the simulation control\nuser interface and demonstrates the powerful func-\ntionality that this provides through a simple inter-\nface, satisfying both novice and experienced users.\nThis simulation control capability is unique as it\nenables a user to implement new capability within a\nrunning simulation and to query all objects and vari-\nables, even if it was not envisaged that these should\nbe queried before the simulation was created. This\nhigh degree of flexibility is essential for ELT AO sys-\ntem simulation as simulation run times can typically\nbe measured in days.\nE. Parallelization Approaches\nWhen parallelizing any software, there is usually a\ntrade-off between the amount of processing done and\nthe amount of data that has to be passed between\nprocessors. A bottleneck may occur if the CPUs spend\na significant amount of time waiting for data, mean-\ning that the parallelization has not been efficient.\nIt is usually most efficient to create parallelized soft-\nware that sends as little data as possible between pro-\ncesses so that most time can be spent processing data.\nAt Durham, we typically parallelize our AO system\nsimulations by dividing parallel optical paths into sep-\narate processes, as shown in Fig. 3. Each optical path\nis virtually independent of the others, except that they\nall require inputs of atmospheric phase screen data\nand knowledge of any time-varying deformable mir-\nror surface shapes, and may (if part of the wavefront\ncorrection path) return new deformable mirror com-\nmands, or wavefront sensed values to be passed to\nother optical paths. By dividing the processes in this\nway, a minimum amount of time is spent waiting for\ndata, allowing the most efficient use of the CPUs to be\nmade. This will also allow a typical simulation (with\nseveral on- and off-axis science targets, and one or\nmore guide stars) to be parallelized into a similar num-\nber of processes as there are processors, allowing a\nsingle process to run on each processor.\nWhen all the parallel optical paths depend on one\nalgorithm, which generates data for the paths, for\nexample, atmospheric turbulence generation or re-\nconstruction of the deformable mirror commands\nfrom the wavefront sensor data, this algorithm can\nalso be parallelized by using a traditional parallel-\nization approach, by splitting the computation over\navailable processors, and passing the data as re-\nquired. Some optimized libraries, for example, the\nfastest Fourier transform in the west (FFTW) Fourier\ntransform library, use this technique. However, this\nparallelization approach is beneficial only for algo-\nrithms where the time spent transferring data is small\ncompared with the time spent processing the data.\n1. Simulation Scalability\nTo demonstrate the scalability of the AO simulation,\nwe have simulated a system with three wavefront\nsensors (32 \u0004 32 subapertures each), one science tar-\nFig. 2. Screen shot of the simulation control user interface. Nov-\nice users are able to control a simulation at the click of a button,\nwhile experienced users are able to query the simulation, obtain\nand display data, and alter the simulation state, including chang-\ning values and array contents.\nFig. 3. Example of the parallelization of parallel optical paths. No\ndata flow is required between these paths, except for the initial\nphase screens, meaning that minimal time is spent with the pro-\ncessors waiting for data to arrive.\n1092 APPLIED OPTICS \u0002 Vol. 46, No. 7 \u0002 1 March 2007\nget, and we assume that atmospheric turbulence is\nconcentrated in two layers. Table 1 provides details of\nthe different simulation objects required for this sim-\nulation and gives typical computation times for this\nexample. It should be noted that the computation\ntimes do not scale identically with simulation size,\nand so the ratio of computation times between differ-\nent algorithms is not constant for larger or smaller\nsimulations.\nWe demonstrate the strong scalability of the AO\nsimulation platform by keeping the simulation a fixed\nsize, but increasing the number of processors that are\nused. Table 2 shows the simulations parallelized by\nplacing different simulation objects on different pro-\ncessing nodes. For the small simulation used for this\ndemonstration, this type of parallelization can be\nsuboptimal, because the processing load can be poorly\nbalanced between processors. For example, when\nplaced on two processors, one of these will have two\nwavefront sensor objects, requiring approximately\ndouble the processing time of the other processor\n(with only one wavefront sensor object).\nBy parallelizing some of the simulation objects (in\nthis case the wavefront sensing objects), the compu-\ntational load can be spread more evenly across the\nprocessors, thus giving a better performance scaling\nwith computer system size. Table 3 shows the simu-\nlations parallelized by using parallelized wavefront\nsensing objects, allowing a better fit to a greater num-\nber of processors to be realized as the processing load\ncan be distributedmore evenly. The timing results for\nthese simulations are shown in Fig. 4. This figure\nshows that the simulation can scale well when it is\nwell suited to the number of processors; for example,\nusing three processors gives a simulation rate three\ntimes greater than one processor. However, when the\nsimulation is not well suited to the number of proces-\nsors (for example, two, four, five, and six processors in\nthe case when the individual simulation objects are\nunparallelized), the performance is suboptimal. If in-\ndividual objects are parallelized, the simulation can\nbe better fitted to the number of available processors,\nas the dotted curve in Fig. 4 shows. However, cur-\nrently, not all simulation objects can be parallelized.\nF. Extremely Large Telescope Simulation Suitability\nThe Durham AO simulation platform is suited for the\nsimulation of ELT scale AO systems. The XD1 super-\ncomputer has 8 Gbyte memory per computing node,\nallowing large phase screens, large numbers of wave-\nfront sensing elements (for example, Shack\u2013Hartmann\nsubapertures), and other data to be stored. The tight\nintegration of the FPGAs with memory and CPUs\nmeans that parts of the simulation can be accelerated\nby several orders of magnitude, and the high band-\nwidth and low latency connections between nodes\nallows data to be passed rapidly between parallelized\nprocesses. This simulation platform provides the ca-\npability for rapid simulation of AO systems on all\ncurrent telescopes and next generation ELTs.\n1. Extremely Large Telescope Simulation Details\nA simulation of a classical AO system on an ELT has\nbeen created to demonstrate the use of the AO sim-\nTable 1. Simulation Objects Used in a Study of the AO Simulation\nStrong Scalability\nSimulation Object\nSignificant\nAlgorithms\nComputation\nTime\ns\nInfinite phase screen\ngeneration\nMatrix operations 10\u00024\nAtmospheric pupil phase Matrix operations 7 \u0003 10\u00024\nDeformable mirror\nsimulation\nMatrix operations 0.03\nShack\u2013Hartmann\nsensor, slope\ncomputation\nFFT, matrix\noperations\n0.18\nWavefront\nreconstruction (SOR)\nMatrix operations 0.02\nScience image\ngeneration\nFFT, matrix\noperations\n0.02\nTable 2. How Simulation Objects can be Placed on Different Computing Nodes to Parallelize a Simulationa\nOne CPU Configuration Two CPUs Three CPUs Four CPUs\nCPU1 CPU1 CPU2 CPU1 CPU2 CPU3 CPU1 CPU2 CPU3 CPU4\n1. Phase screen (2 km) 1 4 2 4 1 2 1 4 5\n2. Phase screen (0 km) 2 5 3 7 5 3 3 7 8\n3. Telescope pupil phase (direction 1) 3 7 6 10 8 6 6 10 11\n4. Telescope pupil phase (direction 2) 6 8 9 12 11 13 9\n5. Telescope pupil phase (direction 3) 9 10 13 12\n6. Deformable mirror (direction 1) 13 11\n7. Deformable mirror (direction 2) 12\n8. Deformable mirror (direction 3)\n9. Wavefront sensor (direction 1)\n10. Wavefront sensor (direction 2)\n11. Wavefront sensor (direction 3)\n12. Wavefront reconstructor\n13. Science calculation (science image)\naThe first column gives a brief description of each object, whose numbers are then referred to in the other columns.\n1 March 2007 \u0002 Vol. 46, No. 7 \u0002 APPLIED OPTICS 1093\nulation platform. The key parameters of this simula-\ntion are detailed in Table 4. This simulation uses an\ninfinite phase screen generator with von Karman\nstatistics.15 A successive overrelaxation (SOR) wave-\nfront reconstructor is used, which means that it is not\nnecessary to create and invert an interaction matrix\nof the system. In a system of this size, a full interac-\ntion matrix could easily take more memory than\navailable on our Cray XD1 system, also taking a pro-\nhibitive length of time (days or weeks) to invert to\nobtain the control matrix, and so conventional wave-\nfront reconstruction is not an option. We are cur-\nrently implementing sparse matrix algorithms and\nFourier domain wavefront reconstruction algorithms\nthat will greatly reduce the memory and computation\nrequirements. An FPGA hardware accelerator is\nused for computation of the Shack\u2013Hartmann images\nand the spot centroid location algorithm. The high\nnumber of pixels per subaperture allows elongated\nShack\u2013Hartmann spots (e.g., from a laser guide star)\nto be analyzed. The simulation includes one wave-\nfront sensor and one science target. A more useful\nsimulation may include several wavefront sensors\nand several science targets, though these are not pre-\nsented here.\nThis simulation has been parallelized over five\nnodes of the Cray XD1, one node for each atmospheric\nlayer, one node for the science target, and one node to\ncombine the atmospheric layers to give the atmo-\nspheric phase at the telescope pupil, perform the sim-\nulation of the wavefront sensor, and reconstruct the\nwavefront allowing the deformable mirror surface to\nbe reshaped. Table 5 shows the relative time spent\ncomputing each of these algorithms, and it can be\nseen that by far themost computationally intensive is\nthe simulation of the science target (involving a\n8192 \u0004 8192 fast Fourier transform for each simula-\ntion time step). These timings are pessimistic (worse\ncase), as they include computation of all scientific\nparameters, including the Strehl ratio and enclosed\nenergy, which would typically only be performed\nTable 3. How the Parallelization of Simulation Objects can be used to Fit a Simulation to a Given Number of CPUsa\nOne CPU Two CPUs Three CPUs Four CPUs Six CPUs\nCPU1 CPU1 CPU2 CPU1 CPU2 CPU3 CPU1 CPU2 CPU3 CPU4 CPU1 CPU2 CPU3\n1 2 1 2 2 4 1 1 2 5 1 c 1 2 5\n3 4 4 5 3 7 5 3 4 8 10 d 3 4 8\n5 6 7 8 6 10 a 8 6 7 11 a 11 d 6 7 11 a\n7 8 3 10 c 9 a 10 b 11 a 9 a 9 d 11 b 9 a 10 a 11 b\n9 a 9 b 6 10 d 9 b 10 c 11 b 9 b 10 a 11 c 9 b 10 b\n9 c 9 d 9 a 11 a 9 c 10 d 11 c 9 c 10 b 12 13\n10 a 10 b 9 b 11 b 9 d 12 11 d 13 CPU4 CPU5 CPU6\n10 c 10 d 9 c 11 c 13 9 c 10 d 11 d\n11 a 11 b 9 d 11 d 9 d 10 d 11 d\n11 c 11 d 10 a 12 12\n12 13 10 b\n13\naThe simulation objects are denoted here by a, b, c, and d suffixes for a four-way parallelization of the wavefront sensing algorithm. The\nnumbers represent the simulation objects described in Table 2.\nFig. 4. Number of simulation time steps computed per unit time\n(simulation rate) when the simulation is parallelized over different\nnumbers of processors. The solid curve shows the case when indi-\nvidual objects are not parallelized, while the dotted curve shows\nthe case when the Shack\u2013Hartmann image creation and wavefront\nsensing algorithm are parallelized, providing a better fit to larger\nnumbers of processors. The simulation rate has been normalized to\nunity by the rate for an unparallelized simulation (with unparal-\nlelized simulation objects).\nTable 4. ELT Simulation Model Details\nSimulation Parameter Value\nTelescope primary 42 m\nAtmospheric layers 3 (0, 2, 10 km)\nWavefront sensors 1\nNumber of subapertures 256 \u0003 256 (16 cm per\nsubaperture)\nCCD pixels per subaperture 16 \u0003 16\nDeformable mirrors 1\nNumber of deformable mirror\nactuators\n256 \u0003 256\nAtmospheric resolution 1 cm of sky per phase\npixel\nPhase pixels for science image\ncreation\n4096 \u0003 4096\nGuide stars 1 (natural guide star)\n1094 APPLIED OPTICS \u0002 Vol. 46, No. 7 \u0002 1 March 2007\nevery 100 or so time steps. Without these calcula-\ntions, the science object takes less than 50 s to com-\npute and store the instantaneous PSF for this ELT\nsimulation. It should be noted that these timings do\nnot scale in the same way as a function of system size;\nfor example, when simulating a smaller telescope, the\ncomputation of the science image takes a signifi-\ncantly smaller fraction of processor time.\nFor the majority of the time, the other processors\nare idle, waiting for the science image algorithm to\ncomplete. Work is currently under way to place the\nbulk of this algorithm into hardware, which will re-\nsult in a significant performance improvement (a fac-\ntor of 10 times is expected). The computation time of\nthe science target simulation currently scales as\nO\u0003n2 log n\u0004, attributable to the large 2D fast Fourier\ntransform, where n is the linear size of the phase\nscreen (measured in pixels). This algorithm also uses\nthe most memory as it has to store a zero-padded\npupil phase (so that the Fourier transform is sampled\nat the Nyquist frequency) and both an instantaneous\nand integrated PSF. The memory requirements for\nthis algorithm scale asO\u0003n2\u0004 where n is the linear size\nof a phase screen, and over 5 Gbyte memory were\nrequired for this algorithm in the example here. With\nthe current hardware, it would be possible to create a\nsimulation with one more science object (on the cur-\nrently spare processing node), and approximately six\nmore wavefront sensing objects, to create (for exam-\nple) a MCAO simulation, without increasing the\nsimulation iteration time. This has not been imple-\nmented at the present time, as the MCAO wavefront\nreconstructor is not yet complete.\nThe planet finder instrument for the European\nSouthern Observatory ELT project is currently spec-\nified to have 200 \u0004 200 subapertures,16 and this is\none of the most demanding proposals. The simulation\ndemonstrated here is therefore of higher order (has a\nlarger number of subapertures) than all present and\nplanned astronomical AO systems.\n3. Simulation Results for Ground Layer Adaptive Optics\nA classical or single guide star AO system can produce\nonly a small corrected field of view, and isoplanatic\nerrors cause the image quality to quickly degrade from\nthe center of this field. When natural guide stars are\nused, the sky coverage for these AO systems is severely\nlimited, since it is difficult to find stars that are bright\nenough within each isoplanatic patch of sky. Ground\nlayer AO (GLAO) was proposed as a solution to this\nproblem, by applying a limited AO correction for a\nlarge field of view under any atmospheric conditions at\noptical and infrared wavelengths.17 A GLAO system is\nnot designed to produce diffraction-limited images, but\nimproves the concentration of the PSF by correcting\nonly the lowest turbulent atmospheric layers. Correc-\ntion is then virtually identical over the entire field of\nview since these layers are closer to the ground, while\nthe uncorrected higher layers degrade the spatial res-\nolution isoplanatically.\nAt Durham, we have implemented a GLAO simu-\nlation model by using the AO simulation framework\nfor corrected fields up to 15 arc min in size based\non high-resolution turbulence profiles taken at the\nGemini observatory,18 and some of the results are\npresented here to demonstrate an actual use of the\nsimulation. The Durham simulation model includes\ndetailed wavefront sensor (WFS) noise propagation\nand produces 2D PSFs and is used to quantify the\neffects of such noise on the PSF parameters across\nthe GLAO field for various seeing and noise condi-\ntions. The capabilities of this model are summarized\nas follows:\n1. The atmosphere can be modeled as any number\nof independently moving turbulent layers.\n2. Multiple laser beacons and guide stars can be\nmodeled.\n3. Multiple deformable mirrors of different types\ncan be modeled.\n4. Multiple wavefront sensors can be included, en-\ncompassing all main detector noise effects, pixela-\ntion, and atmospherically induced speckle.\n5. The science PSF can be sampled at any number\nof field points simultaneously.\nIt is a wholly independent code (not derived from\nany other simulation platform), but can be used subject\nto detailed cross-checks with other AO models.18 This\nchecking has been carried out as part of work for the\nGemini telescope consortium. The simulation can\nalso be used for situations where the atmosphere\ncannot be treated as stratified in layers, but as a 3D\nentity simply by implementing such a model. How-\never, this is not considered here.\nA. Durham Implementation\nA design for the GLAO system is shown in Fig. 5, and\nthis indicates that there are multiple guide stars and\nmultiple science sampling points where the AO sys-\ntem performance is categorized.\nWe have simulated a system with five laser guide\nstars, and four discrete atmospheric turbulence layers\nas shown in Table 6, assuming an 8 m telescope pri-\nmary mirror. The simulation takes samples of the sci-\nence field at a wavelength of 1250 nm at ten positions,\nTable 5. Time Spent in each Algorithm of the ELT Simulationa\nAlgorithm\nTime Taken\ns\nScience image and statistics 70\nAtmospheric pupil phase 6.1\nDeformable mirror 2.8\nWavefront sensing\n(Shack\u2013Hartmann sensor,\nslope computation)\n0.6\nWavefront reconstruction\n(SOR)\n2.0\nPhase screen generation 2.9 per layer\naThe time taken for each simulation iteration (corresponding to\n5 ms real time) was approximately 70 s, limited by the time to\nperform the science image computation.\n1 March 2007 \u0002 Vol. 46, No. 7 \u0002 APPLIED OPTICS 1095\non and off axis, as well as the uncorrected image, and\nuses these samples to categorize the performance of\nthe AO system, with parameters such as the Strehl\nratio and encircled energy being computed for each\nscience target location. The simulation uses a Shack\u2013\nHartmann wavefront sensor with 10 \u0004 10 subaper-\ntures, and assumes a generic deformable mirror to\nwhich combinations of Zernike modes are applied to\ngive the correct mirror shape at each time step (the\nfirst 54 Zernike modes were corrected). A typical lay-\nout of the science stars and guide stars is shown in\nFig. 6, as viewed from the telescope. The guide star\nangle from the on-axis direction can be varied be-\ntween 200 and 750 arc sec, and this can be used to\ninvestigate the degree of AO correction and the area\nover which this correction is achieved. The integrated\nseeing in these models was taken as 0.6 arc sec with\na Fried parameter of 0.17. An exposure time of 100 s\nwas used with a WFS integration time of 2 ms. The\nlaser guide stars were assumed to be of 13th magni-\ntude brightness.\nB. Parallelization Approaches\nThere are many ways in which a large simulation\nsuch as that presented here can be parallelized. The\noptimal parallelization approach will reduce the bot-\ntlenecks in data transferred between processes and\nminimize the amount of time in which processors are\nnot actively processing, while, fully utilizing as many\nprocessors as possible. As mentioned in Subsection\n2.E, when simulating an AO system it is possible to\nseparate the parallel optical paths from different\nguide stars and science targets onto different proces-\nsors, reducing the data transfer between processes.\nThis is the approach used here and presented as a\nflow chart in Fig. 7.\nC. Ground Layer Adaptive Optics Simulation Results\nWhen a number of guide stars are evenly spaced\nabout a circle (as viewed from the telescope), there\nwill be some atmospheric correction for starlight\npassing within this circle, but the degree of correction\nwill fall for starlight outside the circle. If the guide\nstar separation is reduced, better correction will be\nachieved over a smaller area. Conversely, if the sep-\naration is increased, a poorer correction will be\nachieved over a larger area.\nA GLAO system does not aim to achieve a high\ndegree of correction. Rather, a partial degree of correc-\ntion is achieved over a wide field of view, and the\nGLAO system is usually designed to be complemen-\ntary to more conventional AO systems or to be used\nwith integral field spectroscopy units. The correction\nFig. 6. Schematic of the relative positions of the laser guide stars\nand science sampling points used for the GLAO simulation. The\nscience sampling points (larger stars) are spaced uniformly\n150 arc sec apart, while the laser guide stars (smaller stars) are\npositioned equally around a circle with a diameter that can be\nvaried between 200 and 750 arc sec.\nFig. 7. Flow chart showing how the GLAO simulation is carried\nout using the Durham AO simulation platform. The algorithms in\ndifferent boxes are implemented on different processors, and ar-\nrows show the direction of data flow between the algorithms.\nTypically, there will be between five and ten science paths (to\ndetermine how the AO performance changes at different angles\nfrom the vertical axis), and between five and ten AO paths, de-\npending on the number of laser guide stars being used.\nFig. 5. Design of a GLAO system.\nTable 6. Atmospheric Model Details\nLayer height\u0002m 0 300 2000 10000\nWind speed\u0002ms\u00021 6 9 10 18\nRelative layer strengths 0.45 0.15 0.07 0.33\n1096 APPLIED OPTICS \u0002 Vol. 46, No. 7 \u0002 1 March 2007\nachieved from a GLAO system alone typically pro-\nduces Strehl ratios of only a few percent.\nThe results of an investigation into the effect of\nguide star separation are presented here, and Fig. 8\nshows that by moving the guide stars out from the\non-axis direction, the isoplanatic correction covers a\nlarger area with a smaller degree of correction, hence\ngiving a lower Strehl ratio. This decreases for fields\nfarther from the on-axis direction, but the rate of\nchange is dependent on the guide star separation.\nThe uncorrected Strehl ratio was approximately\n0.75%.\nThe FWHM as a function of angle from the on-axis\ndirection also displays the expected behavior, increas-\ning as the viewing angle is moved away from the axis.\nWhen the guide star separation is small, the FWHM is\nsmall close to the axis, increasing rapidly away from it,\nand when guide star separation is large, the FWHM is\ninitially larger, but increases slowly away from the\non-axis direction, as shown in Fig. 9. The uncorrected\nFWHM was 0.35 arc sec.\nThe GLAO simulation presented here has been com-\npared with other independent AO simulation codes18\nand has been found to be in agreement within the\nstatistical uncertainties.\n4. Conclusion\nWehave developed a newAO simulation capability at\nDurham for astronomical applications, and this plat-\nform is capable of extremely large telescope AO sys-\ntem simulation. This simulation platform is capable\nof using algorithms implemented within reconfigu-\nrable logic to provide hardware acceleration for the\nmost computationally intensive tasks.\nA simulation platform includes tools for creating\nand controlling the simulations, and optimal paral-\nlelization techniques specific to AO simulations have\nbeen discussed. The flexibility of the simulation plat-\nform as well as the ability to query and alter the\nstate of a running simulation make it unique. Addi-\ntionally, techniques used to parallelize a given sim-\nulation reducing the computation time have been\ndescribed, and these parallelization strategies are\nspecifically aimed at AO system simulation. The sim-\nulation platform has been tested against other inde-\npendent codes and was found to be in agreement with\nthose.\nWe have demonstrated a use of the AO simulation\nplatform for GLAO simulation and presented some of\nthe results obtained. These results show that the\nseparation of guide stars affects the achievable AO\ncorrection and the area over which this correction can\nbe achieved.\nReferences\n1. E. Gendron, A. Coustenis, P. Drossart, M. Combes, M. Hirtzig,\nF. Lacombe, D. Rouan, C. Collin, S. Pau, A.-M. Lagrange, D.\nMouillet, P. Rabou, T. Fusco, and G. Zins, \u201cVLT\u0002NACO adap-\ntive optics imaging of Titan,\u201d Astron. Astrophys. 417, L21\u2013L24\n(2004).\n2. E. Masciadri, R. Mundt, T. Henning, C. Alvarez, and D. Bar-\nrado y Navascu\u00e9s, \u201cA search for hot massive extrasolar planets\naround nearby young stars with the adaptive optics system\nNACO,\u201d Astrophys. J. 625, 1004\u20131018 (2005).\n3. E. Marchetti, R. Brast, B. Delabre, R. Donaldson, E. Fedrigo,\nC. Frank, N. N. Hubin, J. Kolb, M. Le Louarn, J. Lizon, S.\nOberti, R. Reiss, J. Santos, S. Tordo, R. Ragazzoni, C. Arcidia-\ncono, A. Baruffolo, E. Diolaiti, J. Farinato, and E. Vernet-\nViard, \u201cMAD status report,\u201d in Advancements in Adaptive\nOptics, D. B. Calia, B. L. Ellerbroek, and R. Ragazzoni, eds.,\nProc. SPIE 5490, 236\u2013247 (2004).\n4. D. Mouillet, A. M. Lagrange, J.-L. Beuzit, C. Moutou, M.\nSaisse, M. Ferrari, T. Fusco, and A. Boccaletti, \u201cHigh contrast\nimaging from the ground: VLT\u0002Planet Finder,\u201d in ASP Conf.\nSer. 321: Extrasolar Planets: Today and Tomorrow, pp. 39\u201346\n(2004).\n5. C. V\u00e9rinaud, M. Le Louarn, V. Korkiakoski, and M. Carbillet,\n\u201cAdaptive optics for high-contrast imaging: pyramid sensor\nversus spatially filtered Shack\u2013Hartmann sensor,\u201d Mon. Not.\nR. Astron. Soc. 357, L26\u2013L30 (2005).\n6. M. Carbillet, C. V\u00e9rinaud, B. Femen\u00eda, A. Riccardi, and L. Fini,\n\u201cModelling astronomical adaptive optics\u2014I. The software\nFig. 9. Change in the FWHM of corrected images as a function of\nangle from the on-axis direction. The solid curve is calculated with\na guide star angle of 700 arc sec from the axis, the dotted curve for\n550 arc sec, the dashed curve for 450 arc sec, and the dot\u2013dash\ncurve for 250 arc sec.\nFig. 8. Strehl ratio as a function of distance from the on-axis\ndirection. The solid curve is for a laser guide star separation of 700\narc sec from the on-axis direction, the dotted curve for a separation\nof 550 arc sec, the dashed curve for a separation of 450 arc sec, and\nthe dot\u2013dash curve for a separation of 250 arc sec. The points and\nerror bars are obtained from a sample of ten results for each point.\n1 March 2007 \u0002 Vol. 46, No. 7 \u0002 APPLIED OPTICS 1097\npackage CAOS,\u201d Mon. Not. R. Astron. Soc. 356, 1263\u20131275\n(2005).\n7. M. Le Louarn, C. Verinaud, V. Korkiakoski, and E. Fedrigo,\n\u201cParallel simulation tools for AO onELTs,\u201d inAdvancements in\nAdaptive Optics, D. B. Calia, B. L. Ellerbroek, and R. Ragaz-\nzoni, eds., Proc. SPIE 5490, pp. 705\u2013712 (2004).\n8. A. J. Ahmadia and B. L. Ellerbroek, \u201cParallelized simulation\ncode for multiconjugate adaptive optics,\u201d in Astronomical\nAdaptive Optics Systems and Applications, R. K. T. and M.\nLloyd-Hart, eds., Proc. SPIE 5169, pp. 218\u2013227 (2003).\n9. A. G. Basden, F. Ass\u00e9mat, T. Butterley, D. Geng, C. D. Saun-\nter, and R. W. Wilson, \u201cAcceleration of adaptive optics simu-\nlations using programmable logic,\u201d Mon. Not. R. Astron. Soc.\n364, 1413\u20131418 (2005).\n10. R. Conan, M. Le Louarn, J. Braud, E. Fedrigo, and N. N.\nHubin, \u201cResults of AO simulations for ELTs,\u201d in Future Giant\nTelescopes, J. Angel, P. Roger, and R. Gilmozzi, eds., Proc.\nSPIE 4840, pp. 393\u2013403 (2003).\n11. A. P. Doel, \u201cComparison of Shack\u2013Hartmann and curvature\nsensing for large telescopes,\u201d in Adaptive Optical Systems and\nApplications, R. K. Tyson and R. Q. Fugate, eds., Proc. SPIE\n2534, pp. 265\u2013276 (1995).\n12. A. P. G. Russell, T. G. Hawarden, E. Atad-Ettedgui, S. K.\nRamsay-Howat, A. Quirrenbach, R. Bacon, and R. M. Redfern,\n\u201cInstrumentation studies for a European extremely large tele-\nscope: a strawman instrument suite and implications for tele-\nscope design,\u201d in Emerging Optoelectronic Applications, G. E.\nJabbour and J. T. Rantala, eds., Proc. SPIE 5382, pp. 684\u2013698\n(2004).\n13. Cray, Cray XD1 Supercomputer, Cray, 1st ed. (2005), http:\/\/\nwww.cray.com\/products\/xd1\/.\n14. A. G. Basden, \u201cAdaptive optics simulation performance im-\nprovements using reconfigurable logic,\u201d Appl. Opt. 46, 900\u2013\n906 (2007).\n15. F. Ass\u00e9mat, R. Wilson, and E. Gendron, \u201cMethod for simulat-\ning infinitely long and non stationary phase screens with op-\ntimized memory storage,\u201d Opt. Express 14, 988\u2013999 (2006).\n16. R. M. Myers (r.m.myers@durham.ac.uk), Department of Phys-\nics, South Road, Durham DH1 3LE, UK (personal communi-\ncation, 2006).\n17. F. Rigaut, \u201cGround conjugate wide field adaptive optics for the\nELTs,\u201d in Beyond Conventional Adaptive Optics: A Conference\nDevoted to the Development of Adaptive Optics for Extremely\nLarge Telescopes. Proceedings of the Topical Meeting (Venice,\n2001) E. Vernet, R. Ragazzoni, S. Esposito, and N. Hubin eds.,\nGarching, Germany: European Southern Observatory, 2002\nESO Conference and Workshop Proceedings, 58, pp. 11\u201316\n(2002).\n18. D. R. Andersen, S. Stoesz, S. Morris, M. Lloyd-Hart, D. Cramp-\nton, T. Butterley, B. Ellerbroek, L. Jolissaint, M. Milton, R.\nMyers, K. Szeto, A. Tokovinin, J. Veran, and R. Wilson, \u201cPer-\nformance modeling of a wide-field ground-layer adaptive optics\nsystem,\u201d Pub. Astron. Soc. Pac. 118, 1574\u20131590 (2006).\n1098 APPLIED OPTICS \u0002 Vol. 46, No. 7 \u0002 1 March 2007\n"}