{"doi":"10.1109\/TCE.2007.381753","coreId":"102351","oai":"oai:epubs.surrey.ac.uk:1810","identifiers":["oai:epubs.surrey.ac.uk:1810","10.1109\/TCE.2007.381753"],"title":"A joint motion & disparity motion estimation technique for 3D integral video compression using evolutionary strategy","authors":["Adedoyin, S","Fernando, WAC","Aggoun, A"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2007-05-01","abstract":null,"downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:epubs.surrey.ac.uk:1810<\/identifier><datestamp>\n      2017-10-31T14:03:14Z<\/datestamp><setSpec>\n      74797065733D61727469636C65<\/setSpec><setSpec>\n      6469766973696F6E733D656E67616E64706879736963616C736369656E636573:656C656374726F6E6963656E67696E656572696E67:63637372<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/epubs.surrey.ac.uk\/1810\/<\/dc:relation><dc:title>\n        A joint motion & disparity motion estimation technique for 3D integral video compression using evolutionary strategy<\/dc:title><dc:creator>\n        Adedoyin, S<\/dc:creator><dc:creator>\n        Fernando, WAC<\/dc:creator><dc:creator>\n        Aggoun, A<\/dc:creator><dc:publisher>\n        IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC<\/dc:publisher><dc:date>\n        2007-05-01<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/epubs.surrey.ac.uk\/1810\/1\/fulltext.pdf<\/dc:identifier><dc:identifier>\n          Adedoyin, S, Fernando, WAC and Aggoun, A  (2007) A joint motion & disparity motion estimation technique for 3D integral video compression using evolutionary strategy   IEEE TRANSACTIONS ON CONSUMER ELECTRONICS, 53 (2).  pp. 732-739.      <\/dc:identifier><dc:relation>\n        10.1109\/TCE.2007.381753<\/dc:relation><dc:language>\n        English<\/dc:language><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/epubs.surrey.ac.uk\/1810\/","10.1109\/TCE.2007.381753"],"year":2007,"topics":[],"subject":["Article","PeerReviewed"],"fullText":"IEEE Transactions on Consumer Electronics, Vol. 53, No. 2, MAY 2007 \nContributed Paper \nManuscript received March 31, 2007                                         0098 3063\/07\/$20.00 \u00a9 2007 IEEE \n732\nA Joint Motion & Disparity Motion Estimation Technique for 3D \nIntegral Video Compression Using Evolutionary Strategy \nS. Adedoyin, W.A.C. Fernando, A.Aggoun  \n \nAbstract - 3D imaging techniques have the potential to establish a \nfuture mass-market in the fields of entertainment and \ncommunications. Integral imaging, which can capture true 3D \ncolor images with only one camera, has been seen as the right \ntechnology to offer stress-free viewing to audiences of more than \none person. Just like any digital video, 3D video sequences must \nalso be compressed in order to make it suitable for consumer \ndomain applications. However, ordinary compression techniques \nfound in state-of-the-art video coding standards such as H.264, \nMPEG-4 and MPEG-2 are not capable of producing enough \ncompression while preserving the 3D clues. Fortunately, a huge \namount of redundancies can be found in an integral video \nsequence in terms of motion and disparity. This paper discusses a \nnovel approach to use both motion and disparity information to \ncompress 3D integral video sequences. We propose to \ndecompose the integral video sequence down to viewpoint video \nsequences and jointly exploit motion and disparity redundancies \nto maximize the compression. We further propose an \noptimization technique based on evolutionary strategies to \nminimize the computational complexity of the joint motion-\ndisparity estimation. Experimental results demonstrate that Joint \nMotion and Disparity Estimation can achieve over 1 dB objective \nquality gain over normal motion estimation. Once combined with \nEvolutionary strategy, this can achieve up to 94% computational \ncost saving.1  \nI. INTRODUCTION \nReal world information, obtained by humans is three-dimensional \n(3D). There is growing evidence that 3D imaging techniques will \nhave the potential to establish a future mass-market in the fields of \nentertainment (television, video game) and communications \n(desktop video conferencing). In experimental user-trials, \nsubjective assessments have clearly demonstrated the increased \nimpact of 3D pictures compared to conventional flat-picture \ntechniques.It is reasonable, therefore, that we humans want an \nimaging system that produces pictures that are as natural and real \nas things we see and experience every day. Three-dimensional \nimaging and television (3DTV) are very promising approaches \nexpected to satisfy these desires [1]. \nTelevision that conveys real-time information has become an \nindispensable part of our lives. Real world information we obtain \nwith our eyes is three-dimensional (3D). It is reasonable, therefore, \nthat we humans want an imaging system that produces pictures that \nare as natural and real as things we see and experience every day. \nThree-dimensional television (3DTV) is a much explored but still \n \n   Steven Adedoyin is with the school of engineering and design of Brunel \nUniversity UB8 3PH, United Kingdom (steven.adedoyin@brunel.ac.uk) \nW.A.C Fernando is with the school of engineering and design of Brunel \nUniversity UB8 3PH, United Kingdom (anil.fernando@brunel.ac.uk) \nA.Aggoun is with the school of engineering and design of Brunel \nUniversity UB8 3PH, United Kingdom  (amar.aggoun@brunel.ac.uk) \nvery promising approach expected to satisfy such needs. To date a \nlarge number of three dimensional television recording and display \nsystems have been proposed, mostly autostereoscopic three \ndimensional displays using multiple separate image viewpoints. In \nmost of these designs, a lenticular sheet decoder or scanning \naperture is exploited to enable transmission of image data into the \nleft and right eyes by spatial or temporal multiplexing. However, \nsuch displays are not truly spatial since they exclude vertical \nparallax and rely upon the brain to fuse the two disparate images to \ncreate the 3D sensation. A fundamental limitation of stereo systems \nis that they tend to cause eye strain, fatigue and headaches after \nprolonged viewing as the users are required to simultaneously \nfocus to the screen plane whilst converging their eyes to a different \npoint in space, thus producing a very unnatural situation. \nTrue autostereoscopic 3D visualisation systems exhibiting \nparallax in all directions which allow accommodation and \nconvergence to work in unison are ideally required. Holographic \ntechniques, which demonstrate this characteristic are being \nresearched by different groups in an effort to produce full colour \nrealistic spatial images. The unsolved disadvantages of holographic \nmethods however, are the need for coherent radiation, their current \npoor colour rendering, the specialised environmental conditions, \nand huge information content. \nWe propose to use integral imaging, which can capture true 3D \ncolor images with only one camera and has been seen to offer \nstress-free viewing to audiences of more than one person. The \nimage data can be down sampled (to 50-60 dpi) whilst still being \nable to reconstruct the object space, albeit with reduced contrast \nand intensity. Electronic display of integral images, using \nunidirectional and omni-directional image decoders (cylindrical or \nspherical lens arrays), has been demonstrated by the group on a \nLCD panel with a resolution of 1024x768 pixels. This 3D imaging \nsystem has enormous implications for leisure and industrial \napplications and offers the potential of stress-free viewing to large \naudiences. To realise a 3D TV system, there is a need for high \nresolution display and capture devices along with high bandwidth \ncompression algorithms to reduce the transmission bit rate and a \nhigh bandwidth high quality digital transmission system.  \nTV based on 3D integral imaging video technology, that \nrequires only one camera, will be attractive to service providers \nbecause it will seamlessly provide the added value of three \ndimensional realism. This approach is attractive to service \nproviders because it avoids the cumbersome setting up of more \nthan one camera that other 3D capture techniques employ. 3D \nintegral imaging encoded video can be designed to be scalable with \n2D video and can be encoded efficiently so as to economically \nprovide attractive high value services over high value systems. \nThus, the development of a 3D integral imaging TV system will \nalso demonstrate how added-value broadband services of this type \ncan be delivered, providing benefit to designers of these type of \nAuthorized licensed use limited to: University of Surrey. Downloaded on August 21, 2009 at 06:40 from IEEE Xplore.  Restrictions apply. \nS. Adedoyin et al.: A Joint Motion & Disparity Motion Estimation Technique for 3D Integral Video Compression Using Evolutionary Strategy 733\nservices in the future. However, there are lots of work needs to be \ndone in this area specially in the compression of 3D integral \nimaging video. In this paper we propose an ES based motion \nestimation algorithm for 3D integral imaging video. Rest of the \npaper is organised as follows. In section 2, we summarize some \nrelated work on 3D integral imaging. The proposed ES based \nmotion estimation algorithm is presented in section 3.  Section 4 \npresents some simulation results and a detailed analysis. Finally, \nthe conclusions are given in section 5. \nII. RELATED WORK \nMany different approaches have been adopted in attempts to \nrealise free viewing 3D displays [2][3][4]. Several groups [5] \nhave demonstrated autostereoscopic 3D displays, which work \non the principle of presenting multiple images to the viewer by \nuse of temporal or spatial multiplexing of several discrete \nviewpoints to the eyes. However, the need for expensive multi-\ncamera capture systems and the complexity of the image \nprocessing electronics required to assemble the interlaced \nbanded images has prevented such systems from becoming \ncommercially available.  Furthermore, stereoscopic presentation \nof images does not create conditions in which viewer \naccommodation and convergence operate in unison and hence \nrely upon the brain to fuse the two disparate images to create \nthe 3D sensation. This factor can create disturbing \nphysiological effects after prolong viewing. \nTrue autostereoscopic 3D display systems should have \nparallax in all directions and present images, which allow \naccommodation and convergence to work in unison as in \nnormal viewing.  Integral imaging is a technique that is capable \nof creating and encoding a true volume spatial optical model of \nthe object scene in the form of a planar intensity distribution by \nusing unique optical components [6][7][8][9][10][11]. It is akin \nto holography in that 3D information recorded on a 2-D \nmedium can be replayed as a full 3D optical model, however, in \ncontrast to holography, coherent light sources are not required.  \nThis conveniently allows more conventional live capture and \ndisplay procedures to be adopted.  With recent progress in the \ntheory and microlens manufacturing, integral imaging is \nbecoming a practical and prospective 3D display technology \nand is attracting much interest in the 3D area.  A 3D integral \nimage is represented entirely by a planar intensity distribution, \nwhich may be recorded on to a photographic film for later \nelectronic scanning and processing or directly recorded as an \nintensity distribution using a CCD with a standard camera lens. \nDue to the large amount of data required to represent the \ncaptured 3D integral image with adequate resolution, it is \nnecessary to develop compression algorithms tailor to take \nadvantage of the characteristics of the recorded integral \nimage.  The planar intensity distribution representing an \nOmnidirectional Integral Image is comprised of 2D array of \nsub-images due to the structure of the microlens array used \nin the capture and replay. The structure of the recorded \nintegral image intensity distribution is such that a high cross \ncorrelation in a third domain, i.e. between the micro-images \nproduced by the recording microlens array, is present. This \nis due to the small angular disparity between adjacent \nmicrolenses. In order to maximise the efficiency of a \ncompression scheme for use with the integral image \nintensity distribution, both inter and intra sub-image \ncorrelation should be evaluated.  \nIn the last decade, a Three Dimensional Discrete Cosine \nTransform (3D-DCT) coding algorithm has been developed \nfor compression of still 3D integral images [12][13][14][15]. \nThe algorithm took advantage of the redundancies within the \nmicrolens sub-images (intra sub-image coding) and the \nredundancies between adjacent microlens sub-images (inter \nsub-image coding). The use of intra and inter sub-image \ncoding resulted in much better results than those obtained \nwith the baseline JPEG with respect to both image quality \nand compression ratio. The main advantage of using \ntransform coding is that integral 3D images are inherently \ndivided into small non-overlapping blocks referred to as \nmicrolens sub-images. This leads to high compression with \nless blocking artefacts.  \n \nA. The Development of Integral 3D Imaging \nThe first person who originated the term integral 3D \nimaging was Lippmann [16] in 1908. To record an integral \nphotograph Lippmann used a regularly spaced array of small \nlenslets closely packed together in contact with a \nphotographic emulsion. Each lenslet views the scene at a \nslightly different angle to its neighbour and therefore a scene \nis captured from many view points and parallax information \nis recorded. After processing, if the photographic \ntransparency is re-registered with the original recording \narray and illuminated by diffuse white light from the rear, \nthe object will be constructed in space by the intersection of \nray bundles emanating from each of the lenslets. It is the \nintegration of the pencil beams, which renders integral \nimaging unique and separates it from Gaussian imaging or \nholography. In replay the reconstructed image is \npseudoscopic (inverted in depth).  Optical and digital \ntechniques to convert the pseudoscopic image to an \northoscopic image have been proposed [6][7][8][9][10][11]. \n \nFigure 1 An advanced Integral Imaging system \nAuthorized licensed use limited to: University of Surrey. Downloaded on August 21, 2009 at 06:40 from IEEE Xplore.  Restrictions apply. \nIEEE Transactions on Consumer Electronics, Vol. 53, No. 2, MAY 2007 734\nAn optical configuration necessary to record one stage \northoscopic 3D integral images has been proposed [6-8] and is \nshown in figure 1. This employs a pair of microlens arrays \nplaced back to back and separated by their joint focal length, \nwhich produces spatial inversion. The arrangement allows a \npseudoscopic image to be transferred such that it can straddle \na separate microlens recording array (close imaging).  The \nrecording micro-lens array can be put anywhere in the \ntransferred image space to allow the desired effect to be \nachieved freely: The object can be entirely inside of the \ndisplay, outside of the display, or even straddling the display. \nThe space transfer imaging scheme offers the flexibility of \nrecording the object at a desired depth. \nThe system uses an additional lens array, which images the \nobject space around the plane of the microlens combination. \nThis arrangement has been termed a two-tier optical \ncombination.  Effectively the first macro array produces a \nnumber of pseudoscopic, laterally inverted, images around the \ndouble integral microlens screen. This image is transmitted \neffectively negating the sign of the input angle such that each \npoint in object space is returned to the same position in image \nspace. The arrangement performs pseudo phase conjugation, \ni.e. transfer of volumetric data in space. The image is \ntransmitted with equal lateral longitudinal magnification, and \nthe relative spatial co-ordinates, are preserved i.e. there is no \ninversion in the recorded image and no scale reduction in \ndepth. \nIt is possible to capture integral 3D images electronically \nusing a commercially available CCD array [6-8]. This form of \ncapture requires a high resolution CCD together with \nspecialised optical components to record the micro-images \nfields produced by precision micro-optics. The two-tier \nsystem shown in figure 1 has been used for the capture of the \nintegral images used in this work.  The object\/scene is \nrecorded on a film placed behind the recording microlens \narray through a rectangular aperture. The recorded data is then \nscanned using a high resolution scanner.  \nThe aperture greatly affects the characteristics of the micro-\nimages recorded. Since each micro-image is an image of the \nobject seen through the aperture independently, its shape and \nsize is determined by the aperture. If the field of a micro-\nimage is fully covered by the image, it is said to be fully-filled, \notherwise it is said to be under-filled or over-filled.  Under-\nfilled micro-images are caused by smaller aperture size than \nthe correct size, and lead to decreased effective viewing zone \nwith large part of the image area invalid when the image is \nreplayed. Over-filled micro-images on the other hand are \ncaused by bigger aperture size than the correct one. The \noverlapped micro-images lead to confusion in image \nreconstruction, thus degrade the replay quality.  The system is \nsetup to avoid over-filled micro-images to obtain a good 3D \nintegral image quality. \nThe system would record live images in a regular block pixel \npattern. The planar intensity distribution representing an \nintegral image is comprised of 2D array of M\u00b4M micro images \ndue to the structure of the microlens array used in the capture \nand replay. The resulting 3D images are termed \nOmnidirection Integral Images (OII) and have parallax in all \ndirections. The rectangular aperture at the front of the camera \nand the regular structure of the hexagonal microlenses array \nused in the hexagonal grid (recording microlens array) gives \nrise to a regular \u2018brick structure\u2019 in the intensity distribution.  \n \nIn this paper, the simulation work is carried out on \nunidirectional integral images (UII) which are obtained by \nusing a special case of the integral 3D imaging system where \n1D cylindrical microlens array is used for capture and replay \ninstead of a 2D array of microlenses.  The resulting images \ncontain parallax in the horizontal direction only.   Figure 2(a) \nshows an electronically captured unidirectional integral 3D \nimage and figure 2(b) shows a magnified section of the image.   \nThe M vertically running bands present in the planar intensity \ndistribution captured by the integral 3D camera are due to the \nregular structure of the 1D cylindrical microlens array used in \nthe capture process. \n  \n \n(a) \n \n(b) \nFigure 2  An electronically captured unidirectional integral \nimage a) Full.  b) Magnification \nAuthorized licensed use limited to: University of Surrey. Downloaded on August 21, 2009 at 06:40 from IEEE Xplore.  Restrictions apply. \nS. Adedoyin et al.: A Joint Motion & Disparity Motion Estimation Technique for 3D Integral Video Compression Using Evolutionary Strategy 735\nIII. Proposed 3D Video Encoder With ES-Based \nME \nEvolutionary computation (EC) theories were \ndeveloped originally from observing natural evolution of \nlife form. Because of this, the terminology surrounding \nthe field of EC is full of analogies with natural \nevolutionary process. It was particularly from Darwin\u2019s \ntheories [26] that the best techniques regarding the \noptimization, modelling and the control of unknown \nprocesses were developed. EC has long been exploited in \nthe video coding field. A very well known form of EC \ncalled Genetic Algorithm (GA) was used to perform \nimage registration as part of a larger Digital Subtraction \nAngiography (DAS) system [27][28]. Subsequently, GA \nsearch algorithm has been applied for motion estimation \n[29][30][31][32]. Hardware implementation of Four-Step \ngenetic search algorithm was proposed in [29].  \nSimilarly, the Evolutionary Strategies (ES) were \ndeveloped to solve technical optimization problems in \nvideo coding field. Thus, the motion and disparity \nestimation has been carried out using a (1+\u03bb) rudimentary \nES for stereoscopic video sequences, which includes \ncalculation of P- and B-frames, weighted prediction, joint \nmotion disparity estimation [33][34]. In this paper, we \napply ES to estimate the motion and inter-view disparity \nvectors in lenticular video coding. \nA. Coding structure \nEach viewpoint video sequence represents a unique \nrecording direction of the object scene. Hence the 3D \nintegral video sequence can be separated into its \nrespective distinctive viewpoint videos. Figure 3 \nillustrates the viewpoint extraction for a lenticular video \nof 4 distinctive viewpoints. Columns of pixels formed by \neach micro lens representing the similar view points are \nplaced near to each other to form the viewpoint images. \nThese viewpoint video sequences are used in the motion \ncompensation.  \n \n \nFigure 3 Viewpoint extraction  \nThe best motion vectors for each viewpoint can be \nfound by applying a conventional block-matching \nalgorithm. However such a technique would be too \ncomplex and time consuming. Since viewpoint images are \ncaptured by slightly different viewing angles, there is a \ngreat deal of redundancies. Therefore, it is advantageous \nto find a proper set of motion vectors only for a single \nviewpoint and utilize this correlation to minimize the \noverall coding complexity. \nCompression efficiency can be maximized if disparity \ncorrelations amongst the viewpoints are also considered. \nExploitation of such additional redundancies, however, \nincreases the computational complexity further. \nTherefore, there is a great deal of necessity for an \nefficient technique to exploit most of the existing \ncorrelations within lenticular video at an acceptable \ncomputational cost. Following this argument, we propose \nto motion compensate one of the middlemost viewpoints \nand the rest of the view points are motion and disparity \ncompensate jointly considering the motion compensated \nviewpoint as the base viewpoint. Proposed structure is \nillustrated in Figure 4. Please note that the figure \nrepresents only the first five view points. After coding the \nbase viewpoint (i.e. Viewpoint 5) with respect to the base \nviewpoint of the reference frame, viewpoint 3 is coded \ntaking the corresponding viewpoint from the reference \nframe and the reconstructed version of the base  \n \n \nFigure 4 Proposed motion and disparity estimation technique \nReference \nFrame \nCurrent \nFrame \nViewpoint 1 \nViewpoint 2 \nViewpoint 3 \nViewpoint 4 \nViewpoint 5  \nBase viewpoint \nMotion Prediction\nDisparity Prediction\n\u2026. \u2026. \nAuthorized licensed use limited to: University of Surrey. Downloaded on August 21, 2009 at 06:40 from IEEE Xplore.  Restrictions apply. \nIEEE Transactions on Consumer Electronics, Vol. 53, No. 2, MAY 2007 736\nviewpoint of the current frame as references. Subsequently, \nthe viewpoint 4 is coded taking reconstructed versions of the \nviewpoint 3 and 5 of the current frame and the viewpoint 4 of \nthe reference frame as references.  This process is repeated for \nthe other viewpoints in the sequence. \nUse of an exhaustive search technique such as full search \nwould result in an unacceptable computational complexity. \nWe propose to use evolutionary strategy to minimize the \ncomputational complexity. \nB. Proposed evolutionary strategy  \nES typically uses deterministic selection in which the worst \nsolutions are purged from the population based directly on \ntheir fitness function value. The (\u03bc+\u03bb)-Evolutionary Strategy \ndemonstrated in Figure 5 is used in this work with an \nincreasing level of imitation of biological evolution [35], \nwhere \u03bc means the total number of parents in previous \npopulation, and \u03bb stands for the number of offspring \ngenerated from mutated parents. \n \nFigure 5 (\u03bc+\u03bb)-Evolutionary Strategy-based motion estimation algorithm \nC. Chromosome representation \nEach chromosome represents three elements of a motion \nvector, i.e. the data for coordinates x and y and the reference \nframe (in case of a motion compensation) or viewpoint (in \ncase of a disparity compensation). Each element is described \nby 2 genes: object and strategy genes as shown in the Figure 6 \n(a). Object gene defines the actual value of the element. x and \ny represent the horizontal component of the motion\/disparity \nvector. reference represents whether the vector is a motion \nvector or a disparity vector and if it is a disparity vector, \nwhether it is upper viewpoint or the lower viewpoint. Values \nof the object genes determine how large the mutation is. The \nvalue of object gene of x and y are determined from the search \nwindow size. For example, if the search window is within the \nrange [-16, 16], then the value of object gene can take any \ninteger number inside of this range. The search window size \ndepends on the maximum motion vector size. Strategy gene \ndetermines whatever local or global search will be carried out. \nSmaller value of strategy gene more localised become the \nsearch process. The negative value defines the decrement of \nmutated gene and positive values respectively determine the \nincrement of the mutated gene. The strategy parameter \ndepends on the window size and can take any value up to its \nmaximum. In order to implement the local search, we choose \nto set the strategy parameter to values -1 or 1. \n \n(a) \n \n(b) \nFigure 6 Chromosome representation (a) General representation \nof chromosome; (b) an example of chromosome \nD. Fitness Function \nThe quality of the chromosome is defined by fitness \nfunction. Fitness function is calculated based on the Sum of \nAbsolute Difference (SAD). Each chromosome in newly \ngenerated population is evaluated using fitness function. \nE. Evolutionary strategy operators \nIn order to reduce the number of generations required to \nobtain the satisfactory solution, the initial population is \ngenerated from randomly generated chromosomes. However \nsince the argument of a large number of motion vectors is \nequal or closer to zero, (0, 0) motion and disparity vectors are \nalso included in the initial generation. Selection takes place \nonly amongst the offspring\u2019s (mutated values) and parents. \nThe size of population in the next generation is fixed and set \nto 20 individuals. The new population is generated from the \n20 best chromosomes from the previous population that \ncombines both parents and offspring as shown in Figure 1. \nMutation rate defines the percentage of genes to be mutated \nin a newly generated population. Mutation rate used in the \nexperiments was set to 8.5%. In general the value of the \nstrategy gene is generated randomly from the local search \nincrement window specified in advance. In our case, the \nstrategy gene value can vary within the range [-1, 1]. For \nexample, if the strategy gene in the x coordinate shown in \nFigure 6 (b) is chosen to be mutated, then its value will be \nchanged to 1. The new value of the object gene (if this gene \nhas been chosen to be mutated) is defined as following: \n spop\nnew\nop xxx +=  [1] \nwhere newopx  is the new value of mutated object gene for x \ncoordinate, xop and xsp are the values of object and strategy \ngenes for x coordinate respectively. Similarly the genes for y \ncoordinate and the reference are calculated. \nLet us consider an example shown in Figure 6 (b). The value \nof x coordinate is 10, if this gene has been chosen to \nparticipate in mutation, the value of this gene will be \n-1 10 1 -12 1 0 \nx y reference\nObject \ngene \nStrategy \ngene \nObject \ngene \nStrategy \ngene \nObject \ngene \nStrategy \ngene \nx y reference\nAuthorized licensed use limited to: University of Surrey. Downloaded on August 21, 2009 at 06:40 from IEEE Xplore.  Restrictions apply. \nS. Adedoyin et al.: A Joint Motion & Disparity Motion Estimation Technique for 3D Integral Video Compression Using Evolutionary Strategy 737\ndecremented by 1 and a new gene will be produced with an \nobject gene equaled to 9. In the case of the y coordinate with \nan object parameter of value -12, this will be incremented by \none, if it is chosen to be mutated and therefore will produce a \nnew gene with an object gene value of -11. The new reference \nwould be 1, if it is elected to be mutated.  \nMutation rate defines the percentage of genes to be mutated \nin the population. In the current work, 8.5% of the genes \nrandomly selected from the population are mutated. \n \nIV. RESULTS \nProposed joint motion and disparity estimation technique is \nimplemented in a 3D-DCT integral image codec based on the \narchitecture described in [12] for performance evaluation. An \nadaptive arithmetic coder is used for the entropy coding and \nthe quantizer step size ranged from 10 \u2013 50. The population \nsize of 30 was used for ES. The number of generation and the \nmutation rate are set to 10 and 0.17 respectively based on \npreliminary experimental results. Peak Signal-to-Noise Ratio \n(PSNR) is used to measure the objective quality.  \nFigure 7-10 show the objective quality comparison of the \nproposed ES based joint motion and disparity estimation \ntechnique (denoted as ES-JM&D) for the Room integral  \nvideo test sequence of image size 512\u00d7512 against three \nreference cases namely:  \n(i) Motion only full search (FS-MOTION) \u2013 motion \ncompensated prediction is used and the motion \nvectors are calculated using the full search \nalgorithm. \n(ii) Motion only ES search (ES-MOTION) \u2013 as above \nexcept full search is replaced with ES search. \n(iii) Joint motion and disparity full search (FS-JM&D) \n\u2013 joint motion and disparity compensated \nprediction is used with ES search. \nFigure 7, Figure 8 and Figure 9 depict the relative \nperformance of the above algorithms for selected viewpoints \nand Figure 10 does for the entire frame. Results show that FS-\nJM&D has outperformed FS-MOTION by over 1 dB. This is a \nclear evidence that the use of disparity redundancies together \nwith the motion can greatly improve the compression \nefficiency. However, as described in the section 3, this \nimprovement comes at an expense of multifold increment in \nthe computational complexity. This is where ES become \nhelpful. According to Table 1, ES gives an 84% decrease in \ncoding complexity over Motion Search and over 90% \ndecrease in coding complexity over disparity. It is clear that \nthe ES significantly minimizes the computational complexity. \nAccording to Figure 7-10, however, there is a slight objective \nquality penalty. Still the proposed technique has achieved over \n1 dB objective quality gain. \nTable 1 \nNumber of search points  \nFull search ES search \nComplexity \ngain \nMotion 1024 132 87.11% \nDisparity 3072 183 94.04% \nViewpoint 2\n25\n27\n29\n31\n33\n35\n37\n39\n41\n0.5 1 1.5 2 2.5 3\nBitrate (bpp)\nP\nS\nN\nR \n(d\nB\n)\nFS-MOTION\nFS-JM&D\nES-MOTION\nES-JM&D\n \nFigure 7 Objective quality comparison for the viewpoint 2  \n  \nViewpoint 4\n25\n27\n29\n31\n33\n35\n37\n39\n41\n0.5 1 1.5 2 2.5 3\nBitrate (bpp)\nP\nS\nNR\n (d\nB\n)\nFS-MOTION\nFS-JM&D\nES-MOTION\nES-JM&D\n \nFigure 8 Objective quality comparison for the viewpoint 4 \n \nV. CONCLUSION \nThis paper proposes a novel technique to exploit motion \nand disparity redundancies in 3D integral video sequence and \nlow complexity optimization technique. The integral video \nsequence is decomposed down to viewpoint video sequences \nand the motion and disparity redundancies are jointly \nexploited to maximize the compression efficiency. We further \nproposed an optimization technique based on evolutionary \nstrategies to minimize the computational complexity of the \njoint motion-disparity estimation. Experimental results show \nthat ES based joint motion and disparity estimation technique \nachieve over 1 dB objective quality gain while maintaining up \nto 94% computational cost saving. \nAuthorized licensed use limited to: University of Surrey. Downloaded on August 21, 2009 at 06:40 from IEEE Xplore.  Restrictions apply. \nIEEE Transactions on Consumer Electronics, Vol. 53, No. 2, MAY 2007 738\nViewpoint 8\n25\n27\n29\n31\n33\n35\n37\n39\n41\n0.5 1 1.5 2 2.5 3\nBitrate (bpp)\nPS\nNR\n (d\nB)\nFS-MOTION\nFS-JM&D\nES-MOTION\nES-JM&D\n \nFigure 9 Objective quality comparison for the viewpoint 8  \n \nIntegral\n25\n27\n29\n31\n33\n35\n37\n39\n41\n0.5 1 1.5 2 2.5 3\nBitrate (bpp)\nPS\nNR\n (d\nB)\nFS-MOTION\nFS-JM&D\nES-MOTION\nES-JM&D\n \nFigure 10 Objective quality comparison for the complete \nintegral video \nVI. REFERENCES \n[1] T. Motoki, H. Isono, and I. Yuyama, \u2018Present status of three-\ndimensional television research\u2019, Proc. IEEE, vol. 83, pp. 1009-\n1021 (1995). \n[2] T. Okoshi, \u2018Three-Dimensional Imaging Techniques\u2019, \nAcademic Press, Inc., London, UK. (1976). \n[3] S. A. Benton ed., \u2018Selected papers on three dimensional \ndisplays\u2019 SPIE Optical Engineering Press, Bellingham, Wash. \n(2001). \n[4] S. V. Vladimir, J.-Y. Son, B. Javidi, S.-K. Kim, D.-S. Kim, \n\u201cMoire minimization condition in three-dimensional image \ndisplays,\u201d Journal of display technology, Vol. 1, No. 2, pp. 347-\n353 (2005). \n[5] N.A. Dodgson, \u2018Autostereoscopic 3D Displays\u2019 IEEE Computer \nvol. 38(8), pp. 31 \u2013 36 (2005). \n[6] N. Davies et al., \u2018Three-dimensional imaging systems: A new \ndevelopment\u2019, Appl. Optics, vol. 27, pp. 4520-4528 (1988). \n[7] Monaleche S., Aggoun A., McCormick A., Davies N. and Kung \nS. Y., \u2018Analytical model of a 3d recording camera system using \ncircular and hexagonal based spherical microlenses\u2019, J. Opt. \nSoc. Am. A, Vol. 18(8), pp. 1814-1821 (2001). \n[8] Davis N., McCormick M., and Brewin M., \u2018Design and analysis \nof an image transfer system using microlens arrays\u2019, Optical \nEngineering, vol. 33(11), pp. 3624-3633 (1994).  \n[9] Martinez-Cuenca, R., Saavedra, G., Martinez-Corral, M., Javidi, \nB., \u2018Extended depth-of-field 3-D display and visualization by \ncombination of amplitude-modulated microlenses and \ndeconvolution tools\u2019  IEEE\/OSA Journal of display technology, \nVol. 1 (2),  pp. 321\u2013 327, (2005). \n[10] Okano F., Hoshino H., Arai J. and Yuyama I., \u2018Real-time \npickup method for a three-dimensional image based on integral \nphotography\u2019, Apply Optical, Vol. 36, pp.1598-1604 (1997). \n[11] M. Mart\u00ednez-Corral, B. Javidi, R. Mart\u00ednez-Cuenca and G. \nSaavedra, \u2018Formation of real, orthoscopic integral images by \nsmart pixel mapping\u2019, Optics Express 13, pp. 9175-9180 (2005) \n[12] A Aggoun: \u2018A 3D DCT Compression Algorithm For \nOmnidirectional Integral Images\u2019 ICASSP 2006. \n[13] Zaharia R., Aggoun A. and McCormick M., \u2018Adaptive 3D-DCT \ncompression algorithm for continuous parallax 3D integral \nimaging\u2019 Journal of Signal Processing: Image Communications, \nVol. 17(3), pp. 231-242 (2002). \n[14] R Zaharia, A Aggoun and M McCormick: \u2018Compression of full \nparallax colour Integral 3D TV Image Data based on sub-\nsampling of chrominance components\u2018 Proceedings of Data \nCompression Conference, DCC 2001, 27-29 March 2001, \nSnowbird, Utah, USA. IEEE Computer Society, 2001, ISBN 0-\n7695-1031-0, pp. 527. \n[15] Forman, M.C. and Aggoun, A., \u2018Quantisation strategies for \n3D_DCT based compression of full parallax 3D images\u2019, \nIPA97, Conf. Pub. No.443, IEE, (1997). \n[16] Lippmann, G. \u2018Eppreuves Reversibles Donnat Durelief\u2019, J. \nPhys. Paris 821 (1908). \n[17]  \n[18] W.U. ChungHong, \u201cDepth Measurement In Integral Images\u201d \nPhD Thesis, pp. 40-60, 2003. \n[19] Meriem Mazri and Amar Aggoun  \u201cCompression of 3D Integral \nImages using Wavelet decomposition\u201d Proc. SPIE Vol. 5150, \npp. 1181-1192,2003. \n[20] N. Davies, M. McCormick, and L. Yang, \u201cThree dimensional \nimaging systems: A new development\u201d, Appl. Opt., Vol. 27, pp. \n4520-4528, 1988. \n[21] N. Davies, M. McCormick, and M. Brewin, \u201cThe design and \nanalysis of an image transfer system using microlens arrays\u201d, \nOptical Engineering, Vol. 33, pp. 3624-3633, November , 1994. \n[22] M. C. Forman, \u201cCompression of Integral Three-Dimensional \nTelevision Pictures\u201d, PhD Thesis, January, 2000. \n[23] G. Lippmann, \u201cEpreuves reversibles\u201d, Photog. Integr. Comp. \nRend., Vol. 146, pp. 446-451, 1908. \n[24] H. E. Ives, \u201cOptical properties of a Lippmann lenticulated \nsheet\u201d, J. Opt. Soc. Amer., Vol. 21, pp. 171-176, 1931. \n[25] R. Zaharia, A. Aggoun and M. McCormick \u201cAdaptive 3D-DCT \ncompression algorithm for continuous parallax 3D integral \nimaging,\u201d Signal Processing: Image Communication, 17, pp. \n231-242, 2002. \n[26] Charles Darwin, \u201cThe Origin of Species: By Means of Natural \nSelection or the Preservation of Favoured Races in the Struggle \nfor Life (Bantam Classic),\u201d Bantam Classics, Reprint. 1999. \n[27] Fitzpatrick, J.M., Grefenstette, J.J. and Van-Gucht, D. \u201cImage \nregistration by genetic search,\u201d Proceedings of Southeastcon 84, \nLouisville, KY, 460-464, Apr 1984. \n[28] Grefenstette, J.J. and Fitzpatrick, J.M. \u201cGenetic search with \napproximate function evaluations,\u201d Proc. Intl. Conf. on Genetic \nAlgorithms and their Applications, Pittsburgh, PA, 112-120, Jul. \n1985. \n[29] Man F. So & Angus Wu, \u201cHardware Implementation of Four-\nStep Genetic Search Algorithm\u201d, IEEE Signal Processing \nAuthorized licensed use limited to: University of Surrey. Downloaded on August 21, 2009 at 06:40 from IEEE Xplore.  Restrictions apply. \nS. Adedoyin et al.: A Joint Motion & Disparity Motion Estimation Technique for 3D Integral Video Compression Using Evolutionary Strategy 739\nSociety 1999 Workshop on Multimedia Signal Processing, \nCopenhagen, Denmark, September 1999. \n[30] Xu Yuelei, Bi Duyan and Mao Baixin, \u201c A Genetic Search \nAlgorithm For Motion Estimation\u201d, Proceedings of 5th \nInternational Conference on Signal Processing Proceedings, \nBeijing, China, 2000. \n[31] Guanghua Qiu, Chaohuan Hou, \u201cA New Fast Algorithm for the \nEstimation of Block Motion Vectors\u201d, Proceedings of 3rd \nInternational Conference on Signal Processing, Beijing, China, \n1996. \n[32] Shen Li, Wei-pu Xu, Hui Wang, Nan-ning Zheng, \u201cA Novel \nFast Motion Estimation Method Based on Genetic Algorithm\u201d, \nProceedings of International Conference on Signal Processing, \nKobe, Japan, 1999. \n[33] K. Ponudurai, W.A.C. Fernando and K.K. Loo, \u201cJoint Motion \nand Disparity Estimation in Stereo Video Sequences Using \nEvolutionary Strategy,\u201d Proceedings of 1st Regional Conference \non ICT and E-Paradigms, Colombo, Sri Lanka, 2004. \n[34] K. Ponudurai, W.A.C. Fernando and K.K. Loo, \u201cJoint Motion \nand Disparity Estimation in Stereo Video Sequences Using \nEvolutionary Strategy,\u201d Proceedings of The Seventh \nInternational Symposium on Wireless Personal Multimedia \nCommunications, Abano Terme, Italy, September 2004. \n[35] Ingo Rechenberg, Evolutionsstrategie '94. Stuttgart: Frommann-\nHolzboog 1994. \n \n \nSteven Adedoyin is a research student at Brunel \nUniversity. He holds a BEng (Hons) in Microelectronics \nEngineering that was obtained from Brunel University. \nHis current research interest lies in 3D integral imaging, \n3D video coding and H.264. \n \n \n \n \nW.A.C. Fernando received the B.Sc. Engineering \ndegree (First class) in Electronic and \nTelecommunications Engineering from the University \nof Moratuwa, Sri Lanka in 1995 and the MEng degree \n(Distinction) in Telecommunications from Asian \nInstitute of Technology (AIT), Bangkok, Thailand in \n1997. He has completed his PhD at the Department of \nElectrical and Electronic Engineering, University of \nBristol, UK in February 2001. Currently, he is a lecture in signal processing at \nthe Brunel University, UK. Prior to that, he was an assistant professor in AIT. \nHis current research interests include digital image and video processing, \nintelligent video encoding, OFDM and CDMA for wireless channels, channel \ncoding and modulation schemes for wireless channels. He has published more \nthan 115 international papers on these areas. \n \nAmar Aggoun (M\u2019 99) received the \u201cIngenieur d\u2019Etat\u201d \ndegree in electronic engineering from Ecole Nationale \nPolytechnique of Algiers (ENPA) Algeria, in 1986 and \nthe PhD degree in compressed video signal processing \nfrom Nottingham University, UK, in 1991.  From \n1991-1993 he was with the Nottingham University as a \nresearch fellow in digital video signal processing. From \n1993-2005, he was with De Montfort University, UK, \nas a Principle lecturer in Electronic Engineering.  In \n2005, he joined the school of Design and Engineering at Brunel University \n(UK) as a Reader in 3D Imaging Technologies.  \nHis current research Interests include computer generation and live capture \nof 3D integral images, depth measurement and volumetric data reconstruction \nfrom 3D integral images, 3D video coding, computer vision, and real-time \ndigital image processing architectures. \nAuthorized licensed use limited to: University of Surrey. Downloaded on August 21, 2009 at 06:40 from IEEE Xplore.  Restrictions apply. \n"}