{"doi":"10.1109\/FPT.2005.1568531","coreId":"53962","oai":"oai:eprints.lincoln.ac.uk:2800","identifiers":["oai:eprints.lincoln.ac.uk:2800","10.1109\/FPT.2005.1568531"],"title":"A single-chip FPGA implementation of real-time adaptive background model","authors":["Appiah, Kofi","Hunter, Andrew"],"enrichments":{"references":[{"id":18448468,"title":"A Background Model Initialization Algorithm for video Surveillance.","authors":[],"date":"2001","doi":"10.1109\/iccv.2001.937598","raw":"D. Gutchess, M. Trajkovic, E. Cohen-Solal, D. Lyons, and A. K. Jain. A Background Model Initialization Algorithm for video Surveillance. IEEE, International Conference on Computer Vision, 2001.","cites":null},{"id":18448464,"title":"A Real Time Gesture Recognition System for Mobile Robots.","authors":[],"date":"2004","doi":null,"raw":"V. Bonato, A. Sanches, and M. Fernandes. A Real Time Gesture Recognition System for Mobile Robots. International Conference on Informatics in Control, Automation and Robotics, Portugal, August, 2004.","cites":null},{"id":18448478,"title":"Adaptive background mixture models for real-time tracking.","authors":[],"date":"1999","doi":"10.1109\/cvpr.1999.784637","raw":"C. Stauffer and W. E. L. Grimson. Adaptive background mixture models for real-time tracking. IEEE Conference on Computer Vision and Pattern Recognition, 1999.","cites":null},{"id":18448470,"title":"AN e w FPGA\/DSP-Based Parallel Architecture for RealTime Image Processing.","authors":[],"date":"2002","doi":null,"raw":"J.Batlle, J. Martin, P. Ridao, and J. Amat. AN e w FPGA\/DSP-Based Parallel Architecture for RealTime Image Processing. Elsevier Science Ltd., 2002.","cites":null},{"id":18448476,"title":"An FPGA Implementation of a Flexible, Parallel Image Processing Architecture Suitable for Embedded Vision Systems.","authors":[],"date":"2003","doi":"10.1109\/ipdps.2003.1213415","raw":"S. McBader and P. Lee. An FPGA Implementation of a Flexible, Parallel Image Processing Architecture Suitable for Embedded Vision Systems. Proceedings of the International Parallel and Distributed Processing Symposium, IPDPS\u201903, 2003.","cites":null},{"id":18448475,"title":"Comparison of Background extraction based intrusion detection algorithms.","authors":[],"date":"1996","doi":"10.1109\/icip.1996.559548","raw":"A. Makarov. Comparison of Background extraction based intrusion detection algorithms. IEEE Int. Conference on Image Processing, 1996.","cites":null},{"id":18448479,"title":"Ef\ufb01cient Image Filtering and Information Reduction","authors":[],"date":"2004","doi":"10.1109\/norchp.2004.1423823","raw":"J. Torresen, J. W. Bakke, and L. Sekanina. Ef\ufb01cient Image Filtering and Information Reduction in Recon\ufb01gurable Logic. Proceeding of the 22nd NORCHIP Conference, Norway, November, 2004.","cites":null},{"id":18448465,"title":"Exploiting color and topological features for region segmentation with recursive fuzzy C-means,","authors":[],"date":"2002","doi":"10.1109\/icpr.2002.1044869","raw":"R. Cucchiara, C. Grana, S. Seidenari, and G. Pellacani. Exploiting color and topological features for region segmentation with recursive fuzzy C-means, volume 11. 2002.","cites":null},{"id":18448463,"title":"FPGAImplementationofReal-TimeAdaptive Image Thresholding.","authors":[],"date":"2004","doi":null,"raw":"E.Ashari. FPGAImplementationofReal-TimeAdaptive Image Thresholding. SPIE\u2013The International Society for Optical Engineering, December, 2004.","cites":null},{"id":18448474,"title":"Implementation of a Target Recognition Application Using Pipelined Recon\ufb01gurable Hardware.","authors":[],"date":"2003","doi":null,"raw":"B. Levine, B. Colonna, T. Oblak, E. Hughes, M. Hoffelder, and H. Schmit. Implementation of a Target Recognition Application Using Pipelined Recon\ufb01gurable Hardware. Military and Aerospace Applications of Programmable Devices and Technologies International Conference, 2003.","cites":null},{"id":18448471,"title":"Implementing Image Processing Algorithms on FPGAs.","authors":[],"date":"2004","doi":"10.1109\/tencon.2005.301109","raw":"C. T. Johnston, K. T. Gribbon, and D. G. Bailey. Implementing Image Processing Algorithms on FPGAs. Proceedings of the eleventh electronics New Zealand Conference, ENZCON\u201904, Palmerston North, Nov, 2004.","cites":null},{"id":18448480,"title":"Increase Image Processing System Performance with FPGAs.","authors":[],"date":"2004","doi":null,"raw":"R. Williams. Increase Image Processing System Performance with FPGAs. Xcell Journal, Summer, 2004.","cites":null},{"id":18448466,"title":"Leveraging FPGA coprocessors to optimize automotive infotainment and telematics systems. Embedded Computing Design,","authors":[],"date":"2004","doi":null,"raw":"P. Ekas. Leveraging FPGA coprocessors to optimize automotive infotainment and telematics systems. Embedded Computing Design, Spring, 2004.","cites":null},{"id":18448467,"title":"Nonparametric Model for Background Subtraction.","authors":[],"date":"2000","doi":"10.1007\/3-540-45053-x_48","raw":"A. Elgammal, D. Harwood, and L. Davis. Nonparametric Model for Background Subtraction. Proceedings of the 6th European Conference on Computer Vision, Dublin, Ireland, 2000.","cites":null},{"id":18448477,"title":"Pareto Optimal Design of an FPGA-based Real-Time Watershed Image Segmentation. 15th Annual Workshop on circuits systems on signal processing,","authors":[],"date":"2004","doi":null,"raw":"M. Neuenhahn, H. Blume, and T. G. Noll. Pareto Optimal Design of an FPGA-based Real-Time Watershed Image Segmentation. 15th Annual Workshop on circuits systems on signal processing, November, 2004.","cites":null},{"id":18448481,"title":"P\ufb01nder: Real-time tracking of the human body.","authors":[],"date":"1997","doi":"10.1109\/34.598236","raw":"C. Wren, A. Azarbayejani, T. Darrel, and A. Pentland. P\ufb01nder: Real-time tracking of the human body. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1997.","cites":null},{"id":18448473,"title":"Smart Camera Based on Recon\ufb01gurable hardware Enables Diverse Real-time Applications.","authors":[],"date":"2004","doi":"10.1109\/fccm.2004.53","raw":"M. Leeser, S. Miller, and H. Yu. Smart Camera Based on Recon\ufb01gurable hardware Enables Diverse Real-time Applications. Proceedings of the 12th annual IEEE Symposium on Field-Programmable Custom Computing Machines (FCCM\u201904), 2004.","cites":null},{"id":18448469,"title":"W 4: Who? When? Where? What? A real time system for detecting and tracking people.","authors":[],"date":"1998","doi":"10.1109\/cvpr.1998.698720","raw":"I. Haritaoglu, D. Harwood, and L. Davis. W 4: Who? When? Where? What? A real time system for detecting and tracking people. IEEE Third International Conference on Automatic Face and Gesture, 1998.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2005-12-11","abstract":"This paper demonstrates the use of a single-chip\\ud\nFPGA for the extraction of highly accurate background\\ud\nmodels in real-time. The models are based\\ud\non 24-bit RGB values and 8-bit grayscale intensity\\ud\nvalues. Three background models are presented, all\\ud\nusing a camcorder, single FPGA chip, four blocks\\ud\nof RAM and a display unit. The architectures have\\ud\nbeen implemented and tested using a Panasonic NVDS60B\\ud\ndigital video camera connected to a Celoxica\\ud\nRC300 Prototyping Platform with a Xilinx Virtex\\ud\nII XC2v6000 FPGA and 4 banks of onboard RAM.\\ud\nThe novel FPGA architecture presented has the advantages\\ud\nof minimizing latency and the movement of\\ud\nlarge datasets, by conducting time critical processes\\ud\non BlockRAM. The systems operate at clock rates\\ud\nranging from 57MHz to 65MHz and are capable\\ud\nof performing pre-processing functions like temporal\\ud\nlow-pass filtering on standard frame size of 640X480\\ud\npixels at up to 210 frames per second","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/53962.pdf","fullTextIdentifier":"http:\/\/eprints.lincoln.ac.uk\/2800\/1\/SingleChipfpt05.pdf","pdfHashValue":"6b7d75a791227888a73983df29bd19289162867d","publisher":"IEEE","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lincoln.ac.uk:2800<\/identifier><datestamp>\n      2013-03-13T08:41:00Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F48:6A6163735F48363730<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47343030<\/setSpec><setSpec>\n      74797065733D636F6E666572656E63655F6974656D<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lincoln.ac.uk\/2800\/<\/dc:relation><dc:title>\n        A single-chip FPGA implementation of real-time adaptive background model<\/dc:title><dc:creator>\n        Appiah, Kofi<\/dc:creator><dc:creator>\n        Hunter, Andrew<\/dc:creator><dc:subject>\n        H670 Robotics and Cybernetics<\/dc:subject><dc:subject>\n        G400 Computer Science<\/dc:subject><dc:description>\n        This paper demonstrates the use of a single-chip\\ud\nFPGA for the extraction of highly accurate background\\ud\nmodels in real-time. The models are based\\ud\non 24-bit RGB values and 8-bit grayscale intensity\\ud\nvalues. Three background models are presented, all\\ud\nusing a camcorder, single FPGA chip, four blocks\\ud\nof RAM and a display unit. The architectures have\\ud\nbeen implemented and tested using a Panasonic NVDS60B\\ud\ndigital video camera connected to a Celoxica\\ud\nRC300 Prototyping Platform with a Xilinx Virtex\\ud\nII XC2v6000 FPGA and 4 banks of onboard RAM.\\ud\nThe novel FPGA architecture presented has the advantages\\ud\nof minimizing latency and the movement of\\ud\nlarge datasets, by conducting time critical processes\\ud\non BlockRAM. The systems operate at clock rates\\ud\nranging from 57MHz to 65MHz and are capable\\ud\nof performing pre-processing functions like temporal\\ud\nlow-pass filtering on standard frame size of 640X480\\ud\npixels at up to 210 frames per second.<\/dc:description><dc:publisher>\n        IEEE<\/dc:publisher><dc:date>\n        2005-12-11<\/dc:date><dc:type>\n        Conference or Workshop contribution<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lincoln.ac.uk\/2800\/1\/SingleChipfpt05.pdf<\/dc:identifier><dc:identifier>\n          Appiah, Kofi and Hunter, Andrew  (2005) A single-chip FPGA implementation of real-time adaptive background model.  In: IEEE 2005 Conference on Field-Programmable Technology (FPT' 05), December 11-14, 2005, National University of Singapore, Singapore.  <\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/FPT.2005.1568531<\/dc:relation><dc:relation>\n        10.1109\/FPT.2005.1568531<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lincoln.ac.uk\/2800\/","http:\/\/dx.doi.org\/10.1109\/FPT.2005.1568531","10.1109\/FPT.2005.1568531"],"year":2005,"topics":["H670 Robotics and Cybernetics","G400 Computer Science"],"subject":["Conference or Workshop contribution","PeerReviewed"],"fullText":"A Single-Chip FPGA Implementation of\nReal-time Adaptive Background Model\nKofi Appiah Andrew Hunter\nDepartment of Computing and Informatics\nFaculty of Technology, University of Lincoln\nLincoln, LN6 7TS, UK\n{kappiah, ahunter}@lincoln.ac.uk\nAbstract\nThis paper demonstrates the use of a single-chip\nFPGA for the extraction of highly accurate back-\nground models in real-time. The models are based\non 24-bit RGB values and 8-bit grayscale intensity\nvalues. Three background models are presented, all\nusing a camcorder, single FPGA chip, four blocks\nof RAM and a display unit. The architectures have\nbeen implemented and tested using a Panasonic NV-\nDS60B digital video camera connected to a Celox-\nica RC300 Prototyping Platform with a Xilinx Virtex\nII XC2v6000 FPGA and 4 banks of onboard RAM.\nThe novel FPGA architecture presented has the ad-\nvantages of minimizing latency and the movement of\nlarge datasets, by conducting time critical processes\non BlockRAM. The systems operate at clock rates\nranging from 57MHz to 65MHz and are capable\nof performing pre-processing functions like temporal\nlow-pass filtering on standard frame size of 640X480\npixels at up to 210 frames per second.\n1. Introduction\nWe demonstrate the use of Field Programmable\nGate Array (FPGA) for the extraction of accurate\nbackground models under variable lighting condi-\ntions in real-time. This implementation has a number\nof uses in embedded systems as well as automated vi-\nsual surveillance systems. Real-time image process-\ning is difficult to achieve on a serial processor, due\nto the movement of large data sets and complex op-\nerations that need to be performed on the image [9].\nAdvances in semiconductor technology makes it pos-\nsible to design complete embedded System-on-Chip\n(SoC) by combining sensor, signal processing and\nmemory onto a single substrate [13]. New embed-\nded vision systems have emerged as a result of this\nlevel of integration and are estimated to increase in\nthe coming years. The aim of computer vision sys-\ntems is to scan objects and make judgements on those\nobjects at rates much faster than human observers,\nthus the need to identify imaging functions that al-\nlow the computer to behave like a trained human\noperator[17].\nField Programmable Gate Array, a technology\nwhich has recently been made available to re-\nsearchers, is used as an alternative platform for\nspeedup in computer vision and digital image\nprocessing. The potential uses of FPGAs in ar-\neas like medical image processing, computational\nfluid dynamics, target recognition, embedded vi-\nsion systems, gesture recognition and automotive\ninfotainment have been demonstrated in [2] [4]\n[10] [11] [13]. Digital Image processing or computer\nvision algorithms can be broken down into three ma-\njor stages [8]: early processing, implemented by local\npixel-level functions; intermediate processing, which\nincludes segmentation, motion estimation and feature\nextraction; and late processing, including interpreta-\ntion and using statistical and artificial intelligence al-\ngorithms. Typically algorithm sophistication is con-\ncentrated in the later stages, but processing demands\ndominate in the early stages.\nVanderlei et al [2] used a simplified method in\nconverting Red, Green, and Blue (RGB) values into\nHue, Saturation, and Intensity (HSI) components, for\nextracting their binary region of interest (ROI) for\ntheir RAM-based neural network. The simplification\nwas necessary to achieve feasible FPGA implemen-\ntations. The choice of T1 and T2 (inferior and supe-\nrior thresholds respectively) as shown in equation 1 is\nnot very clear in their implementation. Elham Ashari\n[1] used an adaptive thresholding method to separate\nthe foreground and background pixels in gray level\nimages. Hardware implementation results in terms\nof visual performance, speed, and area consumption\nof the implementation is given, yet the algorithm is\napplication dependent and requires some training pe-\nriod for perfect segmentation. Jim et al [16] used an\nexperimental colour representation for filtering com-\nmon colours found in road signs. They used RGB val-\nues as opposed to HSV to avoid computationally ex-\npensive conversions as outputs of many cameras are\nin RGB mode.\nf(x, y) =\n{\n1 T1 \u2264 f(i, j) \u2264 T2\n0 otherwise (1)\nFrom the above analysis it becomes clear that most\nobject and activity recognition systems require some\nform of object extraction at their initial stages, a re-\nquirement which has not easily been achieved on re-\nconfigurable platforms such as FPGA. Background\nsubtraction is the commonly used method for extract-\ning moving targets in a scene. Even though it is ro-\nbust and less computationally expensive, it requires\nthe maintenance of a background model. The main-\ntained model can lead to accumulated errors if not up-\ndated over time. This paper presents three different\nreal-time adaptive background models, which have\nsuccessfully been implemented on FPGA with min-\nimal use of external memory.\nThe paper is organized as follows. Section 2\nbriefly describe some background modelling algo-\nrithms used for segmenting moving objects. Their\nadvantages and disadvantages for FPGA implemen-\ntation are also given. Section 3 gives details of the\nsystem showing how the various components and pe-\nripherals are connected. This is followed by our back-\nground modelling approach in section 4 with details\non how it is implemented on FPGA. Section 4 also\ngives results and analysis of each implementation. Fi-\nnally, we present a summary of our work and point\nout future directions.\n2. Previous Work\nThe first stage in processing for many video appli-\ncations is the segmentation of (usually) moving ob-\njects with significant difference in colour and shape\nfrom the background. Where the camera is station-\nary, a natural approach is to model the background\nand detect foreground object by differencing the cur-\nrent frame with the background. A wide and increas-\ning variety of techniques for background modelling\nhave been described; a good comparison is given by\nGutchess et al [6].\nThe most popular method is unimodal background\nmodelling, in which a single value is used to rep-\nresent a pixel, which has been widely used due to\nits relatively low computational cost and memory re-\nquirements [7] [18]. This technique gives a poor re-\nsults when used in modelling non-stationary back-\nground scenarios like waving trees, rain and snow.\nA more powerful alternative is to use a multimodal\nbackground representation, the most common variant\nof which is a mixture of Gaussians [5] [15]. However,\nthe computational demands make such techniques\nunpopular for real-time purposes; there are also dis-\nadvantages in multimodal techniques [5] [15] [18] in-\ncluding the blending effect, which causes a pixel to\nhave an intensity value which has never occurred at\nthat position (a side-effect of the smoothing used in\nthese techniques). Other techniques rely heavily on\nthe assumption that the most frequent intensity value\nduring the training period represents the background.\nThis assumption may well be false, causing the out-\nput to have a large error level.\n2.1 Grimson\u2019s Algorithm\nGrimson et al [15] introduced a multimodal ap-\nproach, modelling the values of each pixel as a mix-\nture of Gaussians. The background is modelled with\nthe most persistent intensity values. The algorithm\nhas two variants, colour and gray-scale: in this paper,\nwe concentrate on the gray-scale version. The prob-\nability of observing the current pixel value is given\nas:\nP (Xt) =\nk\u2211\ni=1\n\u03c9i,t\u03b7(Xt, \u03bci,t, \u03c3i,t) (2)\nWhere \u03bci,t, \u03c3i,t and \u03c9i,t are the respective mean,\nstandard deviation and weight parameters of the ith\nGaussians component of pixel X at time t. \u03b7 is a\nGaussian probability density function\n\u03b7(Xt, \u03bci,t, \u03c3i,t) =\n1\n\u03c3i,t\n\u221a\n2\u03c0\ne\n(Xt\u2212\u03bci,t)2\n2\u03c32\ni,t (3)\nA new pixel value is generally represented by one\nof the major components of the mixture model and\nused to update the model. For every new pixel value,\nXt, a check is conducted to match it with one of the\nK Gaussian distributions. A match is found when\nXt is within 2.5 standard deviation of a distribution.\nIf none of the K distributions match Xt, the least\nweighed distribution is replaced with a new distrib-\nution having Xt as mean, high variance and very low\nweight. The update equations are as follows:\nwi,t = wi,t\u22121 + \u03b1(mi,t \u2212 wi,t\u22121) (4)\nwhere \u03b1 is the learning rate and\nmi,t =\n{\n1 if there is a match\n0 otherwise (5)\n\u03bct = \u03bct\u22121 \u2212 \u03c1(Xt \u2212 \u03bct) (6)\n\u03c32t = (1\u2212 \u03c1)\u03c32t\u22121 + \u03c1(Xt \u2212 \u03bct)T (Xt \u2212 \u03bct) (7)\nOnly the matched distribution will have its mean and\nvariance updated, all others remain unchanged. For\n\u03c1 = \u03b1\u03b7(Xt|\u03bct, \u03c3t) (8)\nThe first B distributions (ordered by \u03c9k) are used as\na model of the background, where\nB = argb min(\nb\u2211\nk=1\n\u03c9k > T ). (9)\nThe threshold T is a measure of the minimum por-\ntion of the data that should be accounted for by the\nbackground.\n2.2 Temporal Low-Pass filtering Algorithm\nAleksej [12] introduced a method to avoid false\nalarms due to illumination, using a temporal filter to\nupdate the background model, while a global thresh-\nold value T was used to extract target regions. The\nbackground update he used is of the form\nB(k, l, n) =\n(p\u2212 c)\np\nB(k, l, n\u2212 1) + c\np\nI(k, l, n)\nwhere c is the number of consecutive frames during\nwhich a change is observed and is reset to zero each\ntime the new value becomes part of the background;\np is the adaptation time or insertion delay constant.\nThe moving target is extracted on a pixel level with\nthe following relation:\nf(k, l, n) =\n{\n1 |I(k, l, n)\u2212B(k, l, n)| > L\n0 otherwise\n(10)\nwhere f(k,l,n), B(k,l,n) and I(k,l,n) are the respective\nforeground, background and the grayscale intensity\nvalue of pixel (k,l) for the nth frame and L is the\nglobal threshold value.\nThe low-pass filtering algorithm is attractive for\ntwo reasons. First it is very simple and hence updat-\ning the background information is computationally\ncheap and memory consumption is minimal. The use\nof single global threshold value as well as a single\nmode makes it unattractive for scenes with varying\nlighting intensities. In contrast, Grimson\u2019s algorithm\n[15] is robust to outdoor environments where lighting\nintensity can suddenly change, and it handles multi-\nmodal backgrounds such as moving foliage (cyclical\nmotion) without manual initialisation. Unfortunately,\nthe use of floating-point numbers in all its update pa-\nrameters makes it computationally expensive, and un-\nsuitable for hardware implementation [1].\n3 Overview of Setup\nThe hardware system we present here is composed\nof a Panasonic NV-DS60B digital video camera, a\ndisplay unit and an FPGA prototyping board. The\ncamera is interlaced and runs at 50Hz, thus effec-\ntively transmitting at 25Hz in 24-bit RGB values of\nsize 768\u00d7567 in PAL format. The output of the cam-\nera is connected directly to the RC300 prototyping\nboard via the S-video input. The RC300 board is\npackaged with Xilinx Virtex II XC2v6000, 4 banks\nof ZBT SRAM totalling 32MBytes (thus 4 banks x\n2M x 36bits) and two DVI output ports. The outputs\nof the processed image and the background are dis-\nplayed on different VGAs for visual inspection. This\nconstrains the available memory resource.\nThe inability to read from and write to a single\nbank of RAM in a single clock cycle calls for the use\nof all available memory banks. Two banks hold cam-\nera data, whilst the other two hold background up-\ndate information. A control unit controls the RAM\nbank the camera data is written to and which bank\nthe VGA reads from. The two banks reserved for\ncamera data are swapped after every full frame is\nacquired. Similarly the RAM bank for the display\nunit is swapped. The architecture is such that, when\ndata from the camera is being written to bank \b1 the\nprocessing unit reads captured camera data from bank\n\b2 and processes it with stored background data from\nbank \b3. Concurrently, background data is written to\nbank \b4 after update. Figure 1 shows this architecture\nin detail, which is the same for all three implementa-\ntions.\nbank #1\nbank #2\nbank #3\nbank #4\nwrite image\nA B C D E F G H\nSELECTED\nON-LINE processor\nread image\nread background data\nwrite background data\nforeground\nbackground\nmodel\nFigure 1. The RAM switching architec-\nture\n4 Our Approach\nWe present here a novel hybrid background mod-\nelling algorithm that combines the attractive features\nof Grimson\u2019s algorithm [15] and the temporal low-\npass filtering [12], with appropriate modifications\nto improve segmentation of the foreground image,\nand to allow an efficient implementation on a recon-\nfigurable hardware platform such as Field Program-\nmable Gate Array (FPGA).\nFollowing Grimson [15], we maintain a number of\nclusters, each with weight wk, where 1 \u2264 k \u2264 K, for\nK clusters. Rather than modelling a Gaussian distrib-\nution, we maintain a model with a central value, ck of\n11-bits (8 bits integer part and 3 bits fractional part).\nWe use an implied global range, [ck \u2212 15, ck + 15],\nrather than explicitly modelling a range for each pixel\nbased on its variance as in [15]. The weights and cen-\ntral values of all the clusters are initialised to 0.\nA pixel X = I(i, j) (where X is 11-bit fixed-\npoint) from an image I is said to match a cluster, k,\nif X \u2265 ck \u2212 15 and X \u2264 ck + 15. The highest\nweight matching cluster is updated, if and only if its\nweight after the update will not exceed the maximum\nallowed value (i.e. wk \u2264 64, given the data width of\nthe weight as 6 bits). The update for the weight is as\nfollows:\nwk,t =\n{\n63\n64wk,t\u22121 +\n1\n64 for the matching cluster\n63\n64wk,t\u22121 otherwise (11)\nThe central values of all the clusters are also updated\nas follows:\nck,t,i,j =\n{\n7\n8ck,t\u22121,i,j +\n1\n8Xi,j matching cluster\nck,t\u22121,i,j otherwise\n(12)\nWhere ck,t,i,j is the central value for cluster k at time\nt for pixel (i, j)\nIf no matching cluster is found, then the least\nweighted cluster\u2019s central value, cK is replaced with\nX; its weight is reset to zero. The way we construct\nand maintain clusters make our approach gradually\nincorporate new background objects. This is similar\nto [12] and hence the insertion delay is 23 = 8 frames\nin our case.\nThe K distributions are ordered by weight, with\nthe most likely background distribution on top. Simi-\nlar to [15], the first B clusters are chosen as the back-\nground model, where\nB = argb min(\nb\u2211\nk=1\n\u03c9i > T ). (13)\nThe threshold T is a measure of the minimum portion\nof the data that should be accounted for by the back-\nground. The choice of T is very important, as a small\nT usually models a unimodal background whiles a\nhigher T models a multi-modal background.\nWe classify a pixel as foreground pixel based on\nthe following two conditions:\n1. If the intensity value of the pixel matches none\nof the K clusters.\n2. If the intensity value is assigned to the same\ncluster for two successive frames, and the inten-\nsity values X(t) and X(t \u2212 1) are both outside\nthe 40% mid-range [ck \u2212 6, ck + 6].\nThe second condition makes it possible to detect tar-\ngets with low contrast against the background, while\nmaintaining the concept of multimodal backgrounds.\nA typical example is a moving object with grayscale\nintensity close to that of the background, which\nwould be classified as background in [15]. This re-\nquires the maintenance of an extra frame, with val-\nues representing the recently processed background\nintensities.\n4.1 Gray-Scale Background Modelling\nIt is not always necessary to maintain a multi-\nmodal background. Many vision systems for hand-\nheld devices may have limited memory and may not\nrequire the maintenance of multi-modal backgrounds\nto operate correctly. For these reasons we demon-\nstrate how we can model background scenes in real\ntime on FPGA using a unimodal background. The al-\ngorithm used is similar to that described above with\nK = 1 and no associated weight. The central value is\nupdated on each processed frame and hence the back-\nground data can suffer from high deviation.\n4.1.1. FPGA System Design. The implementa-\ntion of the unimodal grayscale background modelling\nalgorithm is made up of six distinct processes all run-\nning in parallel. The first of these is the image capture\nblock which acquires pixels in RGB format, at cam-\nera rate of 25Hz and converts it to 8-bit grayscale in\na single cycle. This block has iterative mechanisms\nto acquire all pixels from the camera. Thus after ac-\nquiring pixel (i, j) this block iterates several times for\npixel (i + 1, j).\nThis iteration is necessary and possible as the\nclock rate of the design is much higher than the cam-\nera transmission rate. The successfully acquired pixel\nis sent to the memory write block. This block takes\ntwo cycles to write the camera data to external RAM.\nThe first cycle is used to enable memory write for ei-\nther RAM\b1 or RAM \b2. The data is written to the\nappropriate RAM, which is dependant on the RAM\nControl signal (RAM switch) in the second cycle.\nFigure 2 is a pictorial representation of the memory\nwriting pipeline in two clock cycles.\nTwo display units (VGA) are use for visual in-\nspection to display the background model and mov-\ning targets respectively. The two units have the same\nvertical blanking and hence it is possible to gener-\nate different outputs with a single data. The back-\nground data read from RAM\b3 (or RAM\b4 depending\non the RAM control) is used to extract the foreground\nimage, which is sent to the foreground display unit\n(FDU). Simultaneously, the background data is sent\nto the background display unit (BDU). This has been\nmade possible by reading pixels six cycles ahead of\ntheir display time.\nOut of these six cycle, the first cycle is used to en-\nable memory read for RAM \b2 and \b3 (or RAM\b1\nand \b4), camera and its corresponding background\ndata is made available in the second cycle. The 8-bit\ngrayscale intensity value is converted to 11-bit fixed-\npoint value in the third cycle. The fourth cycle is used\nto build a cluster around the 11-bit central value main-\ntained as the background value. This is followed by\nthe fifth and final stage, which estimates if the 11-\nbit grayscale value falls in this cluster. If the value\nfalls out of range of the cluster, the grayscale value is\nsent to the foreground display unit else zero (intensity\nvalue of zero) is sent.\nThe background value is simultaneously sent to\nthe background display unit. The fifth cycle is\nalso used to enable memory write for RAM\b4 (or\nRAM\b3). The final and sixth cycle displays the\nvalues on the display units and writes the updated\nbackground value to the external RAM. It should be\npointed out that the first six pixels of the display units\nare incorrect on reset.\ny\nx\nRAM#1\nRAM#2\n10\n10\nP\ni\np\ne\nli\nn\ne \nr\ne\ng\ni\ns\nt\ne\nr\n21\n24\nRAM Switch Write Enable\nRAM#2\nRAM#1\nWrite\n36\n36\n8\n \nFigure 2. The the 2 stage memory write\npipeline architecture\n4.1.2. Results and Analysis. The design de-\nscribed above runs at 64.81MHz as reported by the\nPlace and Route (PAR) tool. At this speed, the de-\nsign is able to process camera data at more than real-\ntime. The total latency from the time the camera\nsends the first pixel to the time the the pixel is avail-\nable for display on display units is approximately\n0.2\u03bcsec. This is because it takes 5 clock cycles to\nwrite a pixel from the camera, which runs at 25Hz.\nIt takes 6 clock cycles to update the background data\nand extract the region of interest for a given pixel.\nThus at a clock speed of 64.81MHz, the latency\nis approximately 0.2\u03bcsec. Table 1 summarizes the\nresource utilization of this implementation. When\nthe pipeline is full the FPGA produces result for a\npixel every clock cycle. Thus running at 64.81MHz,\nthe output of a pixel is ready every 15.429ns. At\nthis frequency we can effectively process 210fps of\nRS-170\/NTSC (640\u00d7480) frame size and 146fps of\nCCIR\/PAL (768\u00d7576) frame size.\n4.2 RGB Colour Background Modelling\nMany colour segmentation algorithms have been\nproposed in the past for skin lesion images, intru-\nsion detection and feature extraction. Very few of\nthese have successfully been implemented on hard-\nware platform for speed-up. This is due to a lack\nof resources to meet the real-time processing needs\nof these very complex and computationally expen-\nsive colour segmentation algorithms. The need for\nsuch algorithms running on dedicated systems like\nFPGAs is mentioned by Neuenhahn [14]. Real-time\ncolour image segmentation could be particularly use-\nful in many applications, especially in biomedical im-\nage analysis [3].\nResource Total Used Per.\nFlip Flops 1,112 out of 67,584 1%\n4 input LUTs 1,762 out of 67,584 2%\nBlock RAMs 0 out of 144 0%\nbonded IOBs 366 out of 824 44%\nOccupied Slices 1,156 out of 33,792 3%\nSSRAM (NTSC) 16 out of 256 Mbits 4.3%\nSSRAM (PAL) 11 out of 256 Mbits 6.3%\nTable 1. Resource utilization of the uni-\nmodal grayscale implementation, us-\ning XC2v6000, package ff1152 and speed\ngrade -4.\nIn this section we demonstrate how this challeng-\ning task in image processing can be achieved with\nsimplified yet robust algorithms, running at real-time\non reconfigurable computing platforms with minimal\nresources. The architecture we present here is sim-\nilar to that in section 4.1, but instead of using an 8-\nbit grayscale intensity value this architecture relies on\n24-bit RGB values. Again, the entire system is fitted\non a single FPGA chip with 4 banks of SRAM.\n P \nQ \n< \n= \n> \n11 bit comparator\n P \nQ \n< \n= \n> \n11 bit comparator\nS\n1\nS\n2\nD\nC ENB\nMultiplexer\nBlack\nFigure 3. The fifth stage of the six-stage\npipeline\n4.2.1. FPGA System Design. Similar to the\ngrayscale implementation, this architecture has six\nprocesses running in parallel. These are: the RAM\ncontrol; pixel reading from camera; writing camera\ndata to RAM; processing stored data for the output\ndevice and updating background data; writing to the\noutput device; and writing the update background\ndata back to RAM.\nThese processes take 12 clock cycles in total. The\nlongest process, the background updating and target\nextraction process, takes 5 cycles. The first cycle gen-\nerates memory addresses and reads the correspond-\ning background and camera values from the respec-\ntive RAM blocks. The length of the camera data is\n24bits (8 bits for each RGB channel) and that of the\nbackground is 33bits (11bits for each channel). The\nsecond cycle makes the addressed data available for\nprocessing. This is followed by extending the 3\u00d78\nbit RGB values into 11 bit fixed point values in the\nthird cycle. The fourth cycle is used to build a clus-\nter of width 30, each for the Red, Green and Blue\ncolour components. The fifth cycle is used to decide\nif all the colour components of the observed pixels\nbelong to the RGB clusters. The result of this de-\nfines the output to the VGA. If all colour components\nfall in their corresponding clusters, then the pixel is\ndisplayed as a background pixel else displayed as a\nforeground pixel. The last pipeline stage is depicted\nin figure 3, for the Red colour component, as the other\ntwo components have a similar structure.\n4.2.2. Results and Analysis. This design also\nruns at an impressive speed of 64.40MHz, about\n0.4MHz less than that of the grayscale implemen-\ntation. The percentage of the resources consumed\nby this implementation is the same as that of the\ngrayscale implementation, with the exception of 4 in-\nput LUTs (approximately 3% in this case). This can\nbe attributed to the increase in the number of pipeline\nregisters. The total latency from the time the cam-\nera sends the first pixel to the time the pixel is avail-\nable for display on display units is also 0.2\u03bcsec. The\npercentages of external RAM consumed by the de-\nsign for processing a PAL and NTSC frame sizes\nare 13% and 19% respectively. When the pipeline\nis full the FPGA produces result for a pixel every\nclock cycle, thus running at 64.40MHz, the output\nof a pixel is ready every 15.526ns. At this fre-\nquency we can effectively process 209fps of RS-\n170\/NTSC (640\u00d7480 colour) frame size and 145fps\nof CCIR\/PAL (768\u00d7576 colour) frame size.\n4.3 Bimodal Gray-Scale Background Mod-\nelling\nMultimodal background modelling is required to\nmodel scenes with non-stationary background ob-\njects, so reducing false positive alerts. To success-\nfully implement our algorithm on a Field Program-\nmable Gate Array, access to 17n bits of background\ndata is required every clock cycle, where n is the total\nnumber of background clusters and 17 bits is the total\nnumber of bits require for the cluster\u2019s weight (6 bits)\nand central value (11 bits). Having access to 36 bits in\na clock cycle, we can effectively implement a multi-\nCode Description\n000 cluster 0 highest wgt. & data out of range\n001 cluster 0 highest wgt. & data in range\n010 cluster 0 second wgt. & data out of range\n011 cluster 0 second wgt. & data in range\n100 cluster 1 highest wgt. & data out of range\n101 cluster 1 highest wgt. & in of range\n110 cluster 1 second wgt. & out of range\n111 cluster 1 second wgt. & in of range\nTable 2. Econding scheme for determin-\ning which cluster camera data belongs\nmodal background with n = 2. We demonstrate the\nsuccess of this implementation and how easily any n\ncan be increased based on the available resources.\n4.3.1. FPGA System Design. The design of the\nbimodal background is common with all the other ar-\nchitectures; it has six different processes running in\nparallel. The interesting part of the design is the eight\nstage pipeline for extracting moving targets and up-\ndating the background. The first two stages are used\nfor reading the camera and its corresponding back-\nground data from external RAM. The 8-bit grayscale\nvalue from the camera is converted into 11-bits fixed\npoint value in the third stage. To avoid deeply nested\nconditions as well as iteration in sorting the weights\nof the background clusters, different registers are\nused for the results of the sorted weights. This makes\nit possible to sort the weights in a single clock cycle\nin the fourth stage in the pipeline. Clusters are also\nbuilt on the various central values in this stage.\nThe fifth stage is used to determine if the cam-\nera data belongs to the highest weighted cluster. We\nuse an encoding scheme to know which cluster the\ncamera data belongs to. Table 2 gives the encod-\ning scheme used for the bimodal implementation and\ncan easily be used for multimodal implementations.\nThe weight and the background values are updates\nas in equations 11 and 12. If the camera data does\nnot match the highest weighted cluster, the appro-\npriate code is set and the camera data is compared\nwith the second weighted cluster in the sixth stage.\nThe pipeline stages will increase depending on the\nnumber of clusters used in the implementation. This\ncauses a delay in terms of setup and propagation time\nfor the pipeline registers but the speed gain as com-\npared to a deeply nested conditional statement is very\nsignificant.\nIn the seventh and final stage the lowest weighted\ncluster\u2019s central value is replaced with the 11bit fixed\npoint camera data and its weight set to zero if the data\ndoes not belong to any of the clusters. The foreground\nbit-map for the FDU is extracted in this same stage.\nFor the BDU, a grayscale value is displayed if the\nResource Total Used Per.\nFlip Flops 1,766 out of 67,584 2%\n4 input LUTs 3,347 out of 67,584 4%\nBlock RAMs 57 out of 144 39%\nbonded IOBs 366 out of 824 44%\nOccupied Slices 2,124 out of 33,792 6%\nSSRAM (NTSC) 37 out of 256 Mbits 14.4%\nSSRAM (PAL) 25 out of 256 Mbits 9.7%\nTable 3. Resource utilization of the bi-\nmodal grayscale implementation, us-\ning XC2v6000, package ff1152 and speed\ngrade -4.\npixel belongs to the highest weighted cluster or none\nof the clusters, and a red value is displayed if the pixel\nbelongs to the second-weighted cluster. This colour\ndisplay mode can also be used in a multimodal model.\nThe updated background and weights of the clusters\nare sent to the background updating processing unit\nto be written to external RAM. It takes 8 cycles to get\nthe first correct pixel displayed after reset.\n4.3.2. Results and Analysis. This design runs\nat 57.00MHz, about 7.81MHz less than that of the\nunimodal implementation. This has only been possi-\nble with the use of the encoding scheme, as an earlier\nimplementation could only run at maximum speed of\n25MHz due to the use of deeply nested condition for\nevaluating the cluster that the camera data belongs to.\nTo reduce noise due to the camera jitter, morphologi-\ncal opening is conducted on the foreground extracted.\nThis is entirely conducted on the BlockRAM avail-\nable on the FPGA. Effectively, this reduces the la-\ntency by a factor of 9 per pixel as it takes only one\ncycle to access data from the dual-port BlockRAM as\ncompared to two cycles for the external RAM. Table\n3 gives a summary of the resource utilization in this\ndesign. At 57MHz an output is ready every 17.543ns,\nwhen the pipeline is full.\n5. Experimental Results\nWe evaluate the performance of our approach\nagainst that of [15] using K = 3, thus 3 cluster in\nour case and 3 distributions in [15]. We use eight\nrandomly selected video sequences, four each from\noutdoor and indoor scenes. Manually marked frames\nare used as reference images for the sequences.\nOur result is based on pixel-wise errors against\nthe reference image, in terms of true positive(TP ),\ntrue negative(TN), false negative(FN) and false\npositive(FP ) pixels.\nThere are approximately 50 frames in each se-\nquence. Table 4 clearly shows the superiority of our\nalgorithm against that in [15] in terms of sensitivity.\nScene Our Approach (%)\nSENS. SPEC. PPV\nIn1 84.88 98.94 84.01\nOut1 83.18 99.87 94.79\nIn2 76.26 97.28 62.23\nOut2 76.87 99.31 92.50\nIn3 76.72 96.51 47.82\nOut3 81.39 99.56 58.03\nIn4 86.39 89.14 30.62\nOut4 56.35 99.06 48.53\nGrimson\u2019s (%)\nSENS. SPEC. PPV\nIn1 82.20 99.10 85.68\nOut1 80.99 99.85 93.50\nIn2 68.70 97.72 63.93\nOut2 64.33 99.22 89.75\nIn3 68.54 98.30 62.68\nOut3 75.58 99.70 65.40\nIn4 85.38 91.71 36.38\nOut4 45.96 99.38 53.91\nTable 4. Pixel errors evaluation results\nThe algorithm does produce more false positive er-\nrors; this is the side-effect of our approach in detect-\ning targets with low contrast against the background.\nHowever, in our target application false positive er-\nrors of the type reported are more acceptable than\nfalse negative errors, as subsystem tracking stages\ncan discard distracters such as shadows.\nThe evaluation parameters used in table 4 are de-\nfined as follows: Sensitivity (SENS.) is the propor-\ntion of positives that are correctly classified and it\u2019s\nexpressed as TP(TP+FN) , Specificity (SPEC.) is the\nproportion of negatives that are correctly classified\nand it\u2019s expressed as TN(TN+FP ) and Positive Predic-\ntive Values (PPV) is the proportion of cases classi-\nfied as positives that are actually positive and it\u2019s ex-\npressed as TP(TP+FP ) .\nFigure 4 gives sample images of the outputs gen-\nerated by the two algorithms. These are output re-\nsults of seven out of the eight sequences used in table\n4. Visual inspection of the images shows where our\napproach outperforms that of Grimson\u2019s. Grimson\u2019s\nalgorithm suffers from the foreground aperture prob-\nlem, but our approach with its frame-level processing\nsuffers minimally from this problem.\n6. Summary and Conclusions\nWe have demonstrated how a single chip FPGA\ncan effectively be used in modelling robust multi-\nmodal backgrounds in real-time. We have presented\narchitectures for modelling unimodal backgrounds\nusing grayscale intensity value, RGB colour values\nand bimodal backgrounds with grayscale values. The\nnovelty detector used is motivated by [12] and [15]\n  \n \n \n \n \n \n \n \n \n \n \n \nFigure 4. Sample outputs of the algo-\nrithms: Left shows our approach, mid-\ndle is the original frame and the right is\nGrimson\u2019s algorithm\nwith modifications to enable the extraction of tar-\ngets with low contrast. The processing speed and re-\nsources used in the implementation make room for\nother imaging algorithms for tracking and activity in-\nterpretation.\n7. References\n[1] E. Ashari. FPGA Implementation of Real-Time Adap-\ntive Image Thresholding. SPIE\u2013The International So-\nciety for Optical Engineering, December, 2004.\n[2] V. Bonato, A. Sanches, and M. Fernandes. A Real\nTime Gesture Recognition System for Mobile Robots.\nInternational Conference on Informatics in Control,\nAutomation and Robotics, Portugal, August, 2004.\n[3] R. Cucchiara, C. Grana, S. Seidenari, and G. Pella-\ncani. Exploiting color and topological features for re-\ngion segmentation with recursive fuzzy C-means, vol-\nume 11. 2002.\n[4] P. Ekas. Leveraging FPGA coprocessors to optimize\nautomotive infotainment and telematics systems. Em-\nbedded Computing Design, Spring, 2004.\n[5] A. Elgammal, D. Harwood, and L. Davis. Non-\nparametric Model for Background Subtraction. Pro-\nceedings of the 6th European Conference on Com-\nputer Vision, Dublin, Ireland, 2000.\n[6] D. Gutchess, M. Trajkovic, E. Cohen-Solal,\nD. Lyons, and A. K. Jain. A Background Model\nInitialization Algorithm for video Surveillance.\nIEEE, International Conference on Computer Vision,\n2001.\n[7] I. Haritaoglu, D. Harwood, and L. Davis. W 4: Who?\nWhen? Where? What? A real time system for detect-\ning and tracking people. IEEE Third International\nConference on Automatic Face and Gesture, 1998.\n[8] J.Batlle, J. Martin, P. Ridao, and J. Amat. A New\nFPGA\/DSP-Based Parallel Architecture for Real-\nTime Image Processing. Elsevier Science Ltd., 2002.\n[9] C. T. Johnston, K. T. Gribbon, and D. G. Bailey. Im-\nplementing Image Processing Algorithms on FPGAs.\nProceedings of the eleventh electronics New Zealand\nConference, ENZCON\u201904, Palmerston North, Nov,\n2004.\n[10] M. Leeser, S. Miller, and H. Yu. Smart Camera\nBased on Reconfigurable hardware Enables Diverse\nReal-time Applications. Proceedings of the 12th an-\nnual IEEE Symposium on Field-Programmable Cus-\ntom Computing Machines (FCCM\u201904), 2004.\n[11] B. Levine, B. Colonna, T. Oblak, E. Hughes, M. Hof-\nfelder, and H. Schmit. Implementation of a Target\nRecognition Application Using Pipelined Reconfig-\nurable Hardware. Military and Aerospace Applica-\ntions of Programmable Devices and Technologies In-\nternational Conference, 2003.\n[12] A. Makarov. Comparison of Background extraction\nbased intrusion detection algorithms. IEEE Int. Con-\nference on Image Processing, 1996.\n[13] S. McBader and P. Lee. An FPGA Implementation\nof a Flexible, Parallel Image Processing Architecture\nSuitable for Embedded Vision Systems. Proceedings\nof the International Parallel and Distributed Process-\ning Symposium, IPDPS\u201903, 2003.\n[14] M. Neuenhahn, H. Blume, and T. G. Noll. Pareto\nOptimal Design of an FPGA-based Real-Time Wa-\ntershed Image Segmentation. 15th Annual Workshop\non circuits systems on signal processing, November,\n2004.\n[15] C. Stauffer and W. E. L. Grimson. Adaptive back-\nground mixture models for real-time tracking. IEEE\nConference on Computer Vision and Pattern Recog-\nnition, 1999.\n[16] J. Torresen, J. W. Bakke, and L. Sekanina. Efficient\nImage Filtering and Information Reduction in Recon-\nfigurable Logic. Proceeding of the 22nd NORCHIP\nConference, Norway, November, 2004.\n[17] R. Williams. Increase Image Processing System Per-\nformance with FPGAs. Xcell Journal, Summer, 2004.\n[18] C. Wren, A. Azarbayejani, T. Darrel, and A. Pent-\nland. Pfinder: Real-time tracking of the human body.\nIEEE Transactions on Pattern Analysis and Machine\nIntelligence, 1997.\n"}