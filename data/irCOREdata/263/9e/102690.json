{"doi":"10.1109\/TCSVT.2009.2020336","coreId":"102690","oai":"oai:epubs.surrey.ac.uk:2272","identifiers":["oai:epubs.surrey.ac.uk:2272","10.1109\/TCSVT.2009.2020336"],"title":"A Temporal Subsampling Approach for Multiview Depth Map Compression","authors":["Ekmekcioglu, E","Worrall, ST","Kondoz, AM"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2009-08-01","abstract":null,"downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":"IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:epubs.surrey.ac.uk:2272<\/identifier><datestamp>\n      2017-10-31T14:04:52Z<\/datestamp><setSpec>\n      74797065733D61727469636C65<\/setSpec><setSpec>\n      6469766973696F6E733D656E67616E64706879736963616C736369656E636573:656C656374726F6E6963656E67696E656572696E67:63637372<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/epubs.surrey.ac.uk\/2272\/<\/dc:relation><dc:title>\n        A Temporal Subsampling Approach for Multiview Depth Map Compression<\/dc:title><dc:creator>\n        Ekmekcioglu, E<\/dc:creator><dc:creator>\n        Worrall, ST<\/dc:creator><dc:creator>\n        Kondoz, AM<\/dc:creator><dc:publisher>\n        IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC<\/dc:publisher><dc:date>\n        2009-08-01<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/epubs.surrey.ac.uk\/2272\/1\/SRF002126.pdf<\/dc:identifier><dc:identifier>\n          Ekmekcioglu, E, Worrall, ST and Kondoz, AM  (2009) A Temporal Subsampling Approach for Multiview Depth Map Compression   IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, 19 (8).  pp. 1209-1213.      <\/dc:identifier><dc:relation>\n        http:\/\/ieeexplore.ieee.org\/xpls\/abs_all.jsp?arnumber=4811997<\/dc:relation><dc:relation>\n        10.1109\/TCSVT.2009.2020336<\/dc:relation><dc:language>\n        English<\/dc:language><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/epubs.surrey.ac.uk\/2272\/","http:\/\/ieeexplore.ieee.org\/xpls\/abs_all.jsp?arnumber=4811997","10.1109\/TCSVT.2009.2020336"],"year":2009,"topics":[],"subject":["Article","PeerReviewed"],"fullText":"IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 19, NO. 8, AUGUST 2009 1209\nA Temporal Subsampling Approach for Multiview\nDepth Map Compression\nErhan Ekmekcioglu, Stewart T. Worrall, Member, IEEE, and Ahmet M. Kondoz, Member, IEEE\nAbstract\u2014 In this letter, a new method is proposed for mul-\ntiview depth map compression. It is intended to skip some\nparts of certain depth map viewpoints without encoding and\nto just predict those skipped parts by exploiting the multiview\ncorrespondences and some flags transmitted. It is targeted to save\nthe bit rate allocated for depth map sequences to a great extent.\nMultiview correspondences are exploited for each skipped depth\nmap frame by making use of the depth map frames belonging\nto neighboring views and captured at the same time instant.\nA prediction depth map frame is constructed block by block\non a free viewpoint qualitywise selective basis from a couple of\ncandidate predictors generated through the implicit and explicit\nusage of the 3-D scene geometry. Especially at lower bit rates,\ndropping higher temporal layers of certain depth map viewpoints\nand replacing them with corresponding predictors generated\nusing the proposed multiview aided approach save a great amount\nof bit rate for those depth map viewpoints. At the same time,\nthe perceived quality of the reconstructed stereoscopic videos is\nmaintained, which is proved through a set of subjective tests.\nIndex Terms\u2014 Depth map compression, multiview depth map\ncoding, video coding.\nI. INTRODUCTION\nMULTIVIEW CODINGwith depth map included hasbecome a very attractive research area, following the\nrapid developments in 3-D display technologies and image\nprocessing, where the depth map represents the relative dis-\ntance of each video object to the recording camera. These\ndevelopments make possible applications such as 3-D-TV [1]\nand free viewpoint TV (FTV) [2]. However, due to the\nlarge amount of visual and nonvisual (depth information)\ndata included in such systems, simulcast coding using exist-\ning video coding standards does not provide enough com-\npression for cost-effective distribution of multiview content.\nTherefore, it is a necessity to perform compression using\nmore advanced techniques in order to realize such systems.\nResearch has previously been carried out to compress both\ncolor and depth information exploiting particular multiview\ncorrelations that exist between different viewpoints of a mul-\ntiview set [3]\u2013[9].\nManuscript received April 14, 2008; revised October 29, 2008. First version\npublished April 7, 2009; current version published August 14, 2009. This\npaper was developed within VISNET II, a European Network of Excellence,\nfunded under the European Commission IST FP6 program. This paper was\nrecommended by Associate Editor L. Onural.\nThe authors are with the Multimedia and DSP Research Group (I-\nLab), Center for Communication Systems Research (CCSR), University\nof Surrey, GU2 7XH, Surrey, U.K. (e-mail: E.Ekmekcioglu@surrey.ac.uk;\nS.Worrall@surrey.ac.uk; A.Kondoz@surrey.ac.uk).\nColor versions of one or more of the figures in this paper are available\nonline at http:\/\/ieeexplore.ieee.org.\nDigital Object Identifier 10.1109\/TCSVT.2009.2020336\nDepth information can be encoded using reduced overhead\ncompared to the visual information. This is because the depth\nmap is monochromatic and also usually contains smoother and\nsimpler texture regions than the color information. Also, the\nperceived quality of the rendered images is affected more by\nthe color information than the depth map [10]. And, it is the\nperceived quality of the rendered images that are rendered\nusing the reconstructed depth map frames, but not the recon-\nstruction quality of depth map frames itself, which measures\nthe quality of depth map compression [11]. Taking additionally\ninto account the fact that multiview correspondences should be\nexploited in multiview depth map coding, a coding approach\nis proposed where the bit rate used for multiview depth\nmap coding can be further reduced by skipping one or more\ntemporal layers of selected depth map views (i.e., without\nencoding them but transmitting flags for them). Skipped depth\nmap frames for those views are replaced with predictions\ngenerated using solely the inter-view correspondences. It is\npossible to carry out the prediction process in both the\nencoder and the decoded sides identically, which is most\ndesired.\nFig. 1 shows a sample case with three views and a\ngroup of pictures (GOP) size of 8, where the depth map\nin the middle is encoded with inter-view prediction, after\ndropping one or more temporal layers and estimating the\ndropped frames using the novel predictors. In the case of\nthe color videos, such an approach would create crude,\nunsightly results because every single artifact in the predic-\ntion generation process (blocking, occlusion) would directly\nincur a perceptual loss in image quality. Hence, the con-\nventional multiview coding approach (MVC) is used for\ncolor videos in the proposed system. However, the depth\nmap frame is effective only in rendering free viewpoint or\nstereoscopic sequences, and has no visual value. In fact,\nblock artifacts and occlusions are tolerable on most parts of\nthe depth map frame, due to their negligible influence on\nrendering performance. Accordingly, the performance of the\ndepth map coding is evaluated using the quality of rendered\nimages.\nPredictions of depth map frames are generated block\nby block by selecting between two different prediction\ncandidates: one created through variable block size frame\ninterpolation from adjacent views\u2019 depth map frames, and\nthe other rendered using a 3-D warping technique, which\nwill be described in Section II. Selection is performed on\na fixed block size basis, where the prediction candidate\nblock yielding the best objective free viewpoint rendering\nquality with respect to the reference or ground truth is\n1051-8215\/$26.00 \u00a9 2009 IEEE\nAuthorized licensed use limited to: University of Surrey. Downloaded on December 4, 2009 at 06:53 from IEEE Xplore.  Restrictions apply. \n1210 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 19, NO. 8, AUGUST 2009\nselected as the prediction block. Free viewpoint rendering\nis realized using the 3-D warping method explained in\nSection II.\nGeneration of the predicted depth map frames for skipped\ntemporal layers is explained in Section II. Section III explains\nthe experimental setup and outlines the objective performance\nresults for depth map coding. Section IV provides the results\nof subjective assessment of stereoscopic view pairs generated\nusing reconstructed depth maps. Finally, Section V concludes\nthis letter.\nII. DEPTH IMAGE PREDICTOR GENERATION\nAt the frame positions where the corresponding tempo-\nral layer is dropped and the frame is not encoded with\nH.264\/MVC, the prediction depth map frame is generated. It\nis generated using two different prediction candidate depth\nmap frames generated through the exploitation of multiview\ncorrespondences.\nThe first candidate is generated by interpolating the middle\ndepth map frame from the two adjacent views\u2019 reconstructed\ndepth map frames (one left and one right). Interpolation\nis carried out on variable size blocks, based on a texture-\nhomogeneity criterion. This criterion is defined by how fre-\nquently the intensity values inside a certain block deviates\nfrom the median intensity value of the block. The median value\nrepresents the most frequently occurring intensity value in the\nblock. The reconstructed depth map frame under consideration\nin the adjacent view is taken first. Then, a depth map frame\nblock of size 128 \u00d7 128 is initially selected, and this block\nis divided into four iteratively, where the smallest possible\nblock size can be 32\u00d732. For each iteration, the mean square\nerror (MSE) is calculated between the selected depth block\nand the block filled only with an intensity value t as shown\nin (1)\nError = 1\nM2\nx\nM\u22121\u2211\ni=0\nM\u22121\u2211\nj=0\n\u2223\u2223depth_block(i, j) \u2212 t\u2223\u22232 (1)\nM refers to the size of the depth map frame block\n(depth_block) and t refers to the most frequently observed\nintensity value within the block in (1). The iterations are\ncarried out until the calculated MSE drops below a thresh-\nold value. Otherwise, the smallest possible block size is\nselected.\nAfter the block size determination process, a full-pixel\ndisparity vector is calculated for each block within a search\nwindow. The size in pixels of the window is determined\naccording to the base camera distance between the left neigh-\nbor and the right neighbor of the view under consideration.\nFor fast operation, it is assumed that the vertical distance\nbetween neighboring cameras is much less than the horizontal\ndifference between the cameras, and therefore the search\nwindow size in the vertical direction is restricted to only one-\ntenth of the search window size in the horizontal direction.\nAfter determining each block\u2019s full-pixel disparity vectors\nbetween the left and the right neighboring views, disparity\ncompensation takes place by dividing the available disparity\nFig. 1. Sample multiview depth map coding case with three views and a GOP\nsize of 8. For the view under concern (mid-view), the highest two temporal\nlayers are skipped and not coded. They are predicted using the inter-view\ncorrespondences.\nFig. 2. Block-based prediction candidate selection results for the first frame\nof Breakdancer test sequence (view nr. 1). White blocks indicate prediction via\ndisparity compensation and black blocks indicate prediction via 3-D warping.\nvectors by two, and moving the blocks using the halved dispar-\nity vectors. The reason for using the halved disparity vectors\nis that the view under consideration is positioned half way\nbetween its left and right neighbors. Disparity compensation\ntakes place twice, from the left toward the right neighbor and\nfrom the right toward the left neighbor. Both compensated\nimages are fused with equal weights into a final interpolated\nimage. Fusion is used to handle the occlusions that would be\ngenerated in case the disparity compensation is utilized from\na single direction.\nThe second candidate comes from 3-D warping left and\nright neighbor view depth map frames into the image coordi-\nnates of the view under consideration. Each depth map pixel\n(x, y) in the left and right neighbor depth map frames is first\nprojected into the 3-D world coordinates by\n[u, v, w]T = R(c)\u00b7A\u22121(c)\u00b7[x, y, 1]T \u00b7D[c, t, x, y]+T (c) (2)\nwhere, [u, v, w] is the world coordinate and c defines the\ncamera to be projected (left and right neighbors). R, T, A\ndefine the 3\u00d73 rotation matrix, the 3\u00d71 translation vector, and\nthe 3\u00d73 intrinsic matrix of the projected cameras, respectively.\nD[c, t, x, y] is the distance of the corresponding pixel (x, y)\nfrom the projected camera at time t . In the second step, the\nworld coordinate depth map pixels are warped into the 2-D\nimage coordinates of the view under consideration c by\n[x \u2032, y\u2032, z\u2032]T = A(c\u2032) \u00b7 R\u22121(c\u2032) \u00b7 {[u, v, w]T \u2212 T (c\u2032)} (3)\nwhere [(x \u2032\/z\u2032), (y\u2032\/z\u2032)] is the corresponding point in the image\nof the view under consideration. A depth buffer (Z buffer) is\nmaintained to prevent filling pixels with wrong depth values,\nespecially in case more than one depth value falls on to the\nsame pixel location in the target image coordinates.\nPrediction via 3-D warping is especially successful in pre-\ndicting smooth depth areas free of blocking artifacts. However,\nAuthorized licensed use limited to: University of Surrey. Downloaded on December 4, 2009 at 06:53 from IEEE Xplore.  Restrictions apply. \nEKMEKCIOGLU et al.: A TEMPORAL SUBSAMPLING APPROACH FOR MULTIVIEW DEPTH MAP COMPRESSION 1211\nTABLE I\nENCODER SETTINGS\nCodec JMVM 6.0\nEntropy coding CABAC\nMotion search range 96\nTemporal prediction structure Hierarchical B prediction\nTemporal GOP size 12\nFrame rate 25 frames\/s\nInter-view prediction I-B-P-B-P \u2026\nInter-view prediction\nselection for\nanchor\/non-anchor frames\nP-views: Enabled for anchor\nframes only\nB-views: Enabled for both\nanchor and non-anchor\nframes\nprediction via block-based disparity compensation performs\nbetter at strong depth transition areas by providing more robust\nprediction. This is due to the imperfection of the utilized\n3-D warping facility in the regions of strong disocclusion\nand image boundaries. A sample image in Fig. 2 shows\nthe resultant per block selection among the two candidates\nfor a frame in the Breakdancer test sequence. Blocks in\nblack correspond to predictors generated via 3-D warping,\nwhereas blocks in white correspond to predictors generated\nvia disparity compensation.\nThe selection between the two candidates is done for every\nblock of the prediction depth map frame, where the candidate\nachieving higher free viewpoint rendering quality with respect\nto the reference, which is generated through the uncompressed\ndepth map, is selected as the prediction block.\nIII. EXPERIMENTAL RESULTS\nDepth map coding experiments are conducted with the\ndraft multiview coding software Joint Multiview Video model\n(JMVM) version 6.0 [12]. Multiview depth map sequences\nfrom three test multiview video sequences, namely Ballet,\nBreakdancer [5], and Akko & Kayo, are used. First three views\nof each test sequence are selected for coding. Generic mul-\ntiview coding prediction structure with hierarchical B predic-\ntion [13] is utilized for fair comparison. Main encoder settings\nused throughout the experiments are shown in Table I. Adja-\ncent views\u2019 depth maps, view #0 and view #2, are encoded\nwith JMVM, i.e., view #0 is encoded without any inter-view\nprediction and view #2 is encoded using the reconstructed\ndepth map sequence of view #0 as a forward reference. For\nboth views, temporal prediction is enabled, and hierarchical B\nframe prediction with a GOP size of 12 is used. All depth map\nframes in four temporal layers are encoded. The depth map\nsequence for view #1, the middle view, is encoded using three\ndifferent schemes. In the first scheme, the depth map sequence\nof view #1 is encoded using MVC, i.e., the reconstructed depth\nmap sequences of view #0 and view #2 are used as inter-view\nreferences and all temporal layers are coded. In the second\nscheme, the depth map frames of the highest temporal layer\n(TL), i.e., TL 4, are skipped and not coded. In other words, the\ndepth map sequence is temporally subsampled. The skipped\nframes are predicted using the method explained in Section II,\nand only the overhead, consisting of the per-block single bit\nflags are transmitted directly, without the addition of further\nmotion\/disparity vectors and residual data. In the third scheme,\nin addition to the depth map frames in TL 4, the depth map\nframes in one less temporal layer, i.e., TL 3, are also skipped\nand predicted with the same method.\nTests are conducted at a wide range of depth map bit rates.\nDepth map compression performances for the intermediate\nviewpoint are calculated for both narrow baseline rendering\n(half of the eye distance in both directions) and wide baseline\nrendering (double the camera distance in both directions).\nFig. 3 shows the depth map compression performance\nresults for view #1, where the three schemes are plotted.\nBlock-based flag transmission cost is counted in the total\nrate. Fig. 3(a)\u2013(c) shows the results according to the narrow\nbaseline rendering quality and Fig. 3(d)\u2013(f) shows the results\naccording to the wide baseline rendering quality.\nThe results clearly show that, especially for lower depth\nmap rates, temporally subsampled coding with the proposed\nmethod can save a significant portion of the bit rate spent\nfor depth information (with both TL4 and TL3 dropped). On\nthe other hand, the loss in the objective quality of rendered\nnarrow baseline or wide baseline images is far less significant\n(less than 0.6 dB) when compared to the gain in bit rate. For\na better visualization, Fig. 4 shows a cropped section from\na compressed Breakdancer depth map frame (one is encoded\nnormally, the other one is predicted from adjacent depth map\nframes only) and the rendered free viewpoint images using\nthe corresponding reconstructed depth map frames. For better\ncomparsion, the rendering result using uncompressed depth\nmap frame is also plotted in the figure. It should be noted\nthat the perceived quality difference between the reconstructed\ndepth map frames is only reflected to a smaller percentage\nin rendered free viewpoint images, as expected. For higher\nbit rates however, subsampling does not turn out to yield as\noutperforming results as in the lower bit rate regions. Similarly,\nsince skipping all temporal layers lead to perceptible quality\nlevel change during viewpoint jump, it is intended maintain the\nperiodical presence of conventionally encoded temporal layers\nin the sequence, although the objective performance tend to\nbe sufficient at lower bit rates. It is left as a future study to\nimprove the proposed prediction method to handle skipping\nall layers without loss of perceptual quality during viewpoint\nswitching and hence leading to a viewpoint subsampling\nscheme.\nA second result reflects the fact that the effect of the\nproposed multiview depth map compression technique does\nnot change according to the position of the rendered free\nviewpoint image. Specifically, when observing the results for\nnarrow baseline rendering quality and wide baseline rendering\nquality, it is seen that the relative performance of the proposed\ndepth map compression technique with respect to MVC in both\ncases is not changed. The reconstructed depth map frames\nfrom the proposed depth map compression technique are\nrobust enough to be used in rendering free viewpoint images\nfar from the original base camera.\nAuthorized licensed use limited to: University of Surrey. Downloaded on December 4, 2009 at 06:53 from IEEE Xplore.  Restrictions apply. \n1212 IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 19, NO. 8, AUGUST 2009\n35\n20 70 120\n36\n37\n38\nAvg bit rate (kb\/s)\nA\nvg\n P\nSN\nR \n(dB\n)\nJMVM\nDrop layer 4\nDrop layer 3 and 4\n(a)\n32\n33\n34\n35\n40 90 140\nAvg bit rate (kb\/s)\nA\nvg\n P\nSN\nR \n(dB\n)\nJMVM\nDrop Layer 4\nDrop Layer 4 and Layer 3\n(b)\n37\n38\n39\n40\n41\n42\n43\n20\nA\nvg\n P\nSN\nR \n(dB\n)\nAvg bit rate (kb\/s)\nJMVM\nDrop layer 4\nDrop layer 3 and 4\n17012070\n(c)\n31\n32\n33\n34\nAvg bit rate (kb\/s)\nA\nvg\n P\nSN\nR \n(dB\n)\nJMVM\nDrop layer 4\nDrop layer 3 and 4\n20 70 120\n(d)\n28\n29\n30\n31\n32\n40 90 140\nAvg bit rate (kb\/s)\nA\nvg\n P\nSN\nR \n(dB\n)\nJMVM\nDrop Layer 4\nDrop Layer 4 and Layer 3\n(e)\n33\n34\n35\n36\n37\n38\n39\n40\nA\nvg\n P\nSN\nR \n(dB\n)\nJMVM\nDrop layer 4\nDrop layer 3 and 4\n20\nAvg bit rate (kb\/s)\n17012070\n(f)\nFig. 3. Narrow baseline rendering quality vs. middle view\u2019s depth map rate: (a) Breakdancer, (b) Ballet, and (c) Akko & Kayo. Wide baseline rendering\nquality vs. middle view\u2019s depth map rate: (d) Breakdancer, (e) Ballet, and (f) Akko & Kayo.\nconventionally coded frame\n(1464 bits)\nskipped (not coded) and predicted frame\n(192 bits)\nuncompressed depth map\nFig. 4. Cropped sections of three reconstructed depth map frames (left:\nMVC, middle: proposed, right: uncompressed) and the rendered images using\nthe according reconstructed depth map frames.\nIV. SUBJECTIVE TEST RESULTS\nTo verify that the predicted depth map frames in the\nskipped temporal layers do not cause perceptual loss of overall\nrendered image quality, a subjective test is conducted using\nthe Philips 3-D Solutions WOWvx 42-inch auto-stereoscopic\ndisplay. A stimulus comparison, the adjectival categorical\njudgement test method described in recommendation ITU-R\n142 kb\/s 108 kb\/s 60 kb\/s 46 kb\/s 114 kb\/s 86 kb\/s 47 kb\/s 35 kb\/s\nBALLET DEPTH SEQUENCE BREAKDANCER DEPTH SEQUENCE\nJMVM compression\n@ 167 kb\/s\nJMVM compression\n@ 71 kb\/s\nJMVM compression\n@ 137 kb\/s\nJMVM compression\n@ 59 kb\/s\nTL 4\ndropped\nTL 4 & TL 3\ndropped\nTL 4\ndropped\nTL 4\ndropped\nTL 4 & TL 3\ndropped\nTL 4 & TL 3\ndropped\nTL 4\ndropped\nTL 4 & TL 3\ndropped\nDifferential Mean Score Opinion with 95% confidence interval\n2\n1.5\n1\n0.5\n0\n\u00020.5\n\u00021\n\u00021.5\n\u00022\nFig. 5. Subjective test results on a differential mean score opinion scale for\nall test sequences used.\nBT.500-11 [14], is used. Fifteen professional subjects took part\nin the test. Breakdancer and Ballet test sequences are used\nonly in the test due to their high enough spatial resolution\nto create perceptual impact on the display used. In each\nparticular test, the subjects were asked to compare two stereo\nvideos, one of which is rendered using the depth map sequence\nAuthorized licensed use limited to: University of Surrey. Downloaded on December 4, 2009 at 06:53 from IEEE Xplore.  Restrictions apply. \nEKMEKCIOGLU et al.: A TEMPORAL SUBSAMPLING APPROACH FOR MULTIVIEW DEPTH MAP COMPRESSION 1213\nencoded using MVC and the other rendered using the depth\nmap sequence coded with the proposed method. Left-eye\nand right-eye views are generated by the display through\ndepth-image based rendering (DIBR) using the middle view\nand its reconstructed depth map as input sources. A compar-\nison is made in the sense of the overall stereo vision quality.\nUncompressed color video sequences are used in the tests so\nas to let the subjective test reflect the effects of depth map\ncoding only. The subjects did not know the order in which\nthe stereo sequences were shown to them. Fig. 5 shows the\nresults for both test sequences at different depth map coding\nrates on a differential mean score opinion (DMOS) scale. The\nvalues near zero (\u22120.5 to 0.5) indicate that there is hardly\nany difference in the perceived quality between the reference\nmethod and the proposed method. Negative values indicate\nthat the proposed method achieves better perceived quality.\nThe results indicate that the proposed depth map coding\nmethod does not significantly alter the overall stereoscopic\nviewing quality. Even though the depth map rate is reduced\nconsiderably by the proposed method, the overall stereoscopic\nperception does not significantly deviate from what it would\nbe, in case proper MVC was utilized.\nV. CONCLUSION\nThe scheme proposed in this letter aims to decrease the\noverhead assigned to the depth information, for selected view-\npoints, in multiview plus depth map video systems. Significant\nsavings in depth map rate can be achieved for these viewpoints,\nwhose depth maps can be well predicted using solely the inter-\nview correspondences. At the same time, the proposed depth\nmap frame prediction method, which avoids the complete\nMVC encoding process with Lagrangian optimization, does\nnot cause any loss in the perceived quality of stereo vision. The\ngenerality of the proposed approach is not altered when the\nsame hierarchical inter-view prediction structure is extended to\ncover more viewpoints. A major use of such a scheme would\nbe in adapting multiview plus depth map video transmission\nover narrow bandwidth channels. Also, such a method might\nbe well incorporated within a joint depth map\/color bit rate\nallocation scheme for optimized rate-distortion performance of\nmultiview plus depth map video systems.\nREFERENCES\n[1] L. Onural, \u201cTelevision in 3-D: What are the prospects?\u201d Proc. IEEE,\nvol. 95, no. 6, pp. 1143\u20131145, Jun. 2007.\n[2] M. Tanimoto, \u201cOverview of free viewpoint television,\u201d Signal Process.:\nImage Commun., vol. 21, no. 6, pp. 454\u2013461, Jul. 2006.\n[3] S. Shimizu, M. Kitahara, H. Kimata, K. Kamikura, and Y. Yashima,\n\u201cView scalable multiview video coding using 3-D warping with depth\nmap,\u201d IEEE Trans. Circuits Syst. Video Technol., vol. 17, no. 11, pp.\n1485\u20131495, Nov. 2007.\n[4] P. Merkle, \u201cMultiview video plus depth representation and coding,\u201d in\nProc. IEEE Int. Conf. Image Process., San Antonio, TX, Sep. 2007, pp.\nI-201\u2013I-204\n[5] C. L. Zitnick, \u201cHigh-quality video view interpolation using a layered\nrepresentation,\u201d ACM Siggraph Trans. Graph., vol. 23, no. 3, pp. 600\u2013\n608, Aug. 2004.\n[6] Y. Morvan, D. Farin, and P. H. N. de With, \u201cJoint depth\/texture bit-\nallocation for multiview video compression,\u201d in Proc. 26th Picture\nCoding Symp. (PCS \u201907), Portugal, Nov. 2007, pp. 1\u20134.\n[7] Y. Morvan, D. Farin, and P. H. N. de With, \u201cDepth-image compression\nbased on an R-D optimized quadtree decomposition for the transmission\nof multiview images,\u201d in Proc. IEEE Int. Conf. Image Process. 2007,\nSan Antonio, TX, Sep. 2007, pp. V-105\u2013V-108.\n[8] S. Yea and A. Vetro, \u201cRD-optimized view synthesis prediction for\nmultiview video coding,\u201d in Proc. IEEE Int. Conf. Image Process. 2007,\nSan Antonio, TX, Sep. 2007, pp. I-209\u2013I-212.\n[9] E. Ekmekcioglu, S. T. Worrall, and A. M. Kondoz, \u201cBit-rate adaptive\ndownsampling for the coding of multiview video with depth informa-\ntion,\u201d in Proc. 3DTV Conf.: True Vision Capture, Transmission Display\n3-D Video, Istanbul, Turkey, May 2008, pp. 137\u2013140.\n[10] C. T. E. R. Hewage, \u201cPrediction of stereoscopic video quality using\nobjective quality models of 2-D video,\u201d IET Electron. Lett., vol. 44,\nno. 16, pp. 963\u2013965, Jul. 2008.\n[11] R. Krishnamurthy, \u201cCompression and transmission of depth maps for\nimage-based rendering,\u201d in Proc. IEEE Int. Conf. Image Process.,\nThessalonica, Greece, Oct. 2001, pp. 828\u2013831.\n[12] Joint Multiview Video Model JMVM 6.0, ITU-T and ISO\/IEC JVT, Doc.\nJVT-Y207, Oct. 2007.\n[13] K. M\u00fcller, \u201cMultiview video coding based on H.264\/AVC using hierar-\nchical B-frames,\u201d in Proc. Picture Coding Symp. 2006, Beijing, China,\npp. 385\u2013390.\n[14] Methodology for the Subjective Assessment of the Quality of the Tele-\nvision Signals, ITU-R, Recommendation BT.500\u201311, 2002.\nAuthorized licensed use limited to: University of Surrey. Downloaded on December 4, 2009 at 06:53 from IEEE Xplore.  Restrictions apply. \n"}