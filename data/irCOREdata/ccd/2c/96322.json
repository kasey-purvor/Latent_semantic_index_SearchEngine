{"doi":"10.1007\/s00355-010-0453-x","coreId":"96322","oai":"oai:eprints.lse.ac.uk:29573","identifiers":["oai:eprints.lse.ac.uk:29573","10.1007\/s00355-010-0453-x"],"title":"Bayesian group belief","authors":["Dietrich, Franz"],"enrichments":{"references":[{"id":17284746,"title":"Scienti\ufb01c disagreement, unpublished","authors":[],"date":"1972","doi":null,"raw":"Bacharach, M. (1972) Scienti\ufb01c disagreement, unpublished manuscript Dietrich, F. (2004) Opinion Pooling under Asymmetric Information, working paper, Public Economics 0407002, EconWPA Dietrich, F. (2008) The premises of Condorcet\u2019s jury theorem are not simultaneously justi\ufb01ed, Episteme - a Journal of Social Epistemology 5(1): 56-73 Dietrich, F., and C. List (2007) Opinion pooling on general agendas, working paper, METEOR Research Memorandum 038, Maastricht University Fitelson, B. (2001) A Bayesian account of independent evidence with applications, Philosophy of Science 68 (Proceedings). S123 - S140 Genest, C. (1984) A characterization theorem for externally Bayesian groups, Ann. Statist. 12, p. 1100-1105 Genest, C. and J. V. Zidek (1986) Combining probability distributions: a critique and an annotated bibliography, Statist. Sci. 1, p. 114-148 Genest, C., K. J. McConway and M. J. Schervish (1986) Characterization of externally Bayesian pooling operators, Ann. Statist. 14, 487-501 Hild, M. (1998) The instability of ex post aggregation, Typescript Hylland, A. and R. Zeckhauser (1979) The impossibility of group decision making with separate aggregation of beliefs and values, Econometrica 47, p. 1321-36 32Je\ufb00rey, R. (1983) (\ufb01rst published 1965) The logic of decision, Chicago: Chicago University Press Lehrer, K. and C. Wagner (1981) Rational Consensus in Science and Society, Dordrecht: Reidel Levi, I. (1990) Pareto-unanimity and consensus, Journal of Philosophy 87 Madansky, A. (1964) Externally Bayesian groups, Technical Report RM-4141-PR, RAND Corporation McConway, K. (1978) The combination of experts\u2019 opinions in probability assessments: some theoretical considerations, Ph.D. thesis, University College London McConway, K. (1981) Marginalization and linear opinion pools, Jour. Amer. Statist. Assoc. 76, p. 410-414 Mongin, P. (1995) Consistent Bayesian aggregation, Journal of Economic Theory 66, p. 313-351 Mongin, P. (1998) The paradox of the Bayesian experts and state-dependent utility theory, Journal of Mathematical Economics 29, p. 331-61 Morris, P. A. (1974) Decision analysis expert use, Management Science 20, p. 1233-41 Pearl, J. (2000) Causality: Models, Reasoning and Inference, Cambridge: Cambridge University Press Pivato, M. (2008) The Discursive Dilemma and Probabilistic Judgement Aggregation, MPRA Paper 8412, University Library of Munich, Germany Risse, M. (2001) Instability of ex post aggregation in the Bolker\/Je\ufb00rey framework and related instability phenomena, Erkenntnis 55, p. 239-269 Risse, M. (2003) Bayesian group agents and two modes of aggregation, Synthese, forthcoming Savage, L. (1954) The foundations of statistics, New York: Wiley Schervish, M., T. Seidenfeld and J. Kadane (1991) Shared preferences and statedependent utilities, Management Science 37, p. 1575-89 Seidenfeld, T., J. Kadane and M. Schervish (1989) On the shared preferences of two Bayesian decision makers, Journal of Philosophy 86, p. 221-44 Wagner, C. G. (1982) Allocation, Lehrer models, and the consensus of probabilities, Theory and Decision 14, p. 207-220 33Wagner, C. (1985) \u201cOn the Formal Properties of Weighted Averaging as a Method of Aggregation\u201d, Synthese 62: 97-108","cites":null}],"documentType":{"type":0.7777777778}},"contributors":[],"datePublished":"2010-10","abstract":"If a group is modelled as a single Bayesian agent, what should its beliefs be? I propose an axiomatic model that connects group beliefs to beliefs of the group members. The group members may have different information, different prior beliefs and even different domains (algebras) within which they hold beliefs, accounting for differences in awareness and conceptualisation. As is shown, group beliefs can incorporate all information spread across individuals without individuals having to explicitly communicate their information (that may be too complex or personal to describe, or not describable in principle in the language). The group beliefs derived here take a simple multiplicative form if people's information is independent (and a more complex form if information overlaps arbitrarily). This form contrasts with familiar linear or geometric opinion pooling and the (Pareto) requirement of respecting unanimous beliefs","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/96322.pdf","fullTextIdentifier":"http:\/\/eprints.lse.ac.uk\/29573\/1\/Bayesian_group_belief_%28LSERO_version%29.pdf","pdfHashValue":"48cafdfb8105edf33ce86c86f2d4d422502363dd","publisher":"Springer","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lse.ac.uk:29573<\/identifier><datestamp>\n      2017-05-04T09:35:08Z<\/datestamp><setSpec>\n      74797065733D4445505453:4C53452D5048<\/setSpec><setSpec>\n      74797065733D43454E54524553:4C53455F52435F3134<\/setSpec><setSpec>\n      74797065733D43454E54524553:4C53455F52435F4C534543484F494345<\/setSpec><setSpec>\n      74797065733D434F4C4C53:4C53455F435F454F<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lse.ac.uk\/29573\/<\/dc:relation><dc:title>\n        Bayesian group belief<\/dc:title><dc:creator>\n        Dietrich, Franz<\/dc:creator><dc:subject>\n        B Philosophy (General)<\/dc:subject><dc:subject>\n        HB Economic Theory<\/dc:subject><dc:subject>\n        QA Mathematics<\/dc:subject><dc:description>\n        If a group is modelled as a single Bayesian agent, what should its beliefs be? I propose an axiomatic model that connects group beliefs to beliefs of the group members. The group members may have different information, different prior beliefs and even different domains (algebras) within which they hold beliefs, accounting for differences in awareness and conceptualisation. As is shown, group beliefs can incorporate all information spread across individuals without individuals having to explicitly communicate their information (that may be too complex or personal to describe, or not describable in principle in the language). The group beliefs derived here take a simple multiplicative form if people's information is independent (and a more complex form if information overlaps arbitrarily). This form contrasts with familiar linear or geometric opinion pooling and the (Pareto) requirement of respecting unanimous beliefs.<\/dc:description><dc:publisher>\n        Springer<\/dc:publisher><dc:date>\n        2010-10<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lse.ac.uk\/29573\/1\/Bayesian_group_belief_%28LSERO_version%29.pdf<\/dc:identifier><dc:identifier>\n          Dietrich, Franz  (2010) Bayesian group belief.  Social Choice and Welfare, 35 (4).  pp. 595-626.  ISSN 0176-1714     <\/dc:identifier><dc:relation>\n        http:\/\/www.springer.com\/economics\/economic+theory\/journal\/355<\/dc:relation><dc:relation>\n        10.1007\/s00355-010-0453-x<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lse.ac.uk\/29573\/","http:\/\/www.springer.com\/economics\/economic+theory\/journal\/355","10.1007\/s00355-010-0453-x"],"year":2010,"topics":["B Philosophy (General)","HB Economic Theory","QA Mathematics"],"subject":["Article","PeerReviewed"],"fullText":"  \nFranz Dietrich \nBayesian group belief \n \nArticle (Accepted version) \n(Refereed) \n \nOriginal citation: \nDietrich, Franz (2010) Bayesian group belief. Social choice and welfare, 35 (4). pp. 595-626. \nISSN 0176-1714 \n \nDOI: 10.1007\/s00355-010-0453-x  \n \n\u00a9 2010 The Author; published by Springer  \n \nThis version available at: http:\/\/eprints.lse.ac.uk\/29573\/\n \nAvailable in LSE Research Online: October 2010 \n \nLSE has developed LSE Research Online so that users may access research output of the \nSchool. Copyright \u00a9 and Moral Rights for the papers on this site are retained by the individual \nauthors and\/or other copyright owners. Users may download and\/or print one copy of any \narticle(s) in LSE Research Online to facilitate their private study or for non-commercial research. \nYou may not engage in further distribution of the material or use it for any profit-making activities \nor any commercial gain. You may freely distribute the URL (http:\/\/eprints.lse.ac.uk) of the LSE \nResearch Online website.  \n \nThis document is the author\u2019s final manuscript accepted version of the journal article, \nincorporating any revisions agreed during the peer review process.  Some differences between \nthis version and the published version may remain.  You are advised to consult the publisher\u2019s \nversion if you wish to cite from it. \nBayesian Group Belief\nFranz Dietrich1\nLondon School of Economics & Maastricht University\nMay 2008, revised March 2010\nAbstract. If a group is modelled as a single Bayesian agent, what should its be-\nliefs be? I propose an axiomatic model that connects group beliefs to beliefs of the\ngroup members. The group members may have different information, different prior\nbeliefs, and even different domains (algebras) within which they hold beliefs, account-\ning for differences in awareness and conceptualisation. As is shown, group beliefs can\nincorporate all information spread across individuals without individuals having to\nexplicitly communicate their information (that may be too complex or personal to\ndescribe, or not describable in principle in the language). The group beliefs derived\nhere take a simple multiplicative form if people\u2019s information is independent (and a\nmore complex form if information overlaps arbitrarily). This form contrasts with fa-\nmiliar linear or geometric opinion pooling and the (Pareto) requirement of respecting\nunanimous beliefs. JEL classification: D70, D71\nKeywords: Opinion pooling, Bayesianism, axiomatic approach, subjective probability\n1 Introduction\nSuppose a group is interested in whether a given hypothesis H is true. If every in-\ndividual assigns a probability of 70% to H, what probability should the group as\na whole assign to H? Is it exactly 70%, or perhaps more since different persons\nhave independently confirmed H? The answer, I will show, crucially depends on\nthe informational states of the individuals. If they have identical information, the\ncollective has good reasons to adopt people\u2019s unanimous 70% belief, following the\npopular (probabilistic) Pareto principle (e.g. Mongin (1995, 1998)). Under informa-\ntional asymmetry, by contrast, a possibly much higher or lower collective probability\nmay be appropriate, and the Pareto principle becomes problematic, or so I argue.\nThe above question is an instance of the classic opinion pooling\/aggregation prob-\nlem, with applications for instance in expert panels. In general, the beliefs of different\n1 I am very grateful for numerous helpful suggestions by a competent and diligent referee. This\npaper is based on my old unpublished paper \u2018Opinion Pooling under Asymmetric Information,\u2019 Public\nEconomics 0407002, EconWPA, 2004. Meanwhile, interesting related results have been obtained\nindependently by Marcus Pivato in his working paper \u2018The Discursive Dilemma and Probabilistic\nJudgement Aggregation,\u2019 MPRA Paper 8412, University Library of Munich, Germany, 2008.\n1\nindividuals need of course not coincide, and also more than one hypothesis may be\nunder consideration. The general goal is to merge many individuals\u2019 probability\nassignments to certain (exclusive and exhaustive) hypotheses into a single collect-\nive probability assignment to these hypotheses. The literature has proposed different\nnormative conditions on the aggregation rule, and has derived the class of rules satisfy-\ning these conditions. The two most prominent types of rules are linear and geometric\nrules. Denoting by \u03c01, ..., \u03c0n and \u03c0 the individual and collective probability assign-\nments (each assignment being a function that maps hypotheses to probabilities), a\nlinear rule defines \u03c0 as being a weighted arithmetic average\n\u2211n\ni=1wi\u03c0i, and a geomet-\nric rule defines \u03c0 as being proportional to a weighted geometric average\n\u220fn\ni=1 \u03c0\nwi\ni ,\nwhere w1, ..., wn \u2208 [0, 1] are fixed weights with sum 1. By contrast, our Bayesian\naxioms will lead to what I call multiplicative rules, which define \u03c0 as g\n\u220fn\ni=1 \u03c0i, the\nproduct of all (unweighted) individual function \u03c0i with some fixed function g. Lin-\near rules have been characterised (under additional technical assumptions) by the\nindependence or setwise function property (McConway (1981), Wagner (1982, 1985),\nDietrich and List (2007); see also Lehrer and Wagner (1981)), the marginalisation\nproperty (McConway (1981)), and (in a single-profile context) by the probabilistic\nanalogue of the Pareto principle (Mongin, (1995, 1998)); and geometric rules fam-\nously satisfy external Bayesianity as defined in Section 6 (e.g. McConway (1978),\nGenest (1984), Genest, McConway and Schervish (1986)). Still an excellent reference\nfor fundamental results on opinion pooling is Genest and Zidek\u2019s (1986) literature\nreview.\nI claim that the classic approach is problematic if, as in this paper, the goal of\nopinion pooling is taken to be information aggregation, i.e. if collective beliefs should\nincorporate all the information spread asymmetrically over the individuals. The clas-\nsic approach is more suitable if the goal is not information aggregation: the goal\nmight be not epistemic at all (e.g. fair representation), or it might be epistemic yet\nwith the disagreements between individuals caused not by differences in information\nbut by differences in interpretation of the same shared body of information.\nOne might at first suspect that classic pooling functions can account for informa-\ntional asymmetries by putting more weight on the beliefs of well-informed individuals.\nMore concretely, it is often suggested that in a linear and geometric rule (as defined\nabove) the weights wi of well-informed individuals should be higher. However, as\nGenest and Zidek (1986) put it, \u201cexpert weights do allow for some discrimination\n[...], but in vague, somewhat ill defined ways\u2019 (p. 120), and \u201cno definite indications\ncan be given concerning the choice or interpretation of the weights\u2019 (p. 118).\nTo concretely illustrate the difficulty that classic pooling functions have in aggreg-\nating information, consider again the introductory example. Suppose each individual\ni\u2019s subjective probability \u03c0i(H) = 0.7 is in fact the result of Bayesian conditioning\non some private information. What should the collective belief \u03c0(H) be? If the\nindividuals started from the same prior probability of H, all depends on how this\n2\nprior compares to 0.7: if the prior is below 0.7, say 0.5, then \u03c0(H) should intuitively\nexceed 0.7 because \u03c0(H) should incorporate the pooled information of many individu-\nals, where a single individual\u2019s information already suffices to push the probability\nof H up from a prior of 0.5 to a posterior of 0.7. By a similar argument, if H has a\ncommon prior above 0.7 then intuitively \u03c0(H) < 0.7, and if H has a common prior\nof exactly 0.7 then intuitively \u03c0(H) = 0.7. If people hold different prior beliefs of\nH, some below 0.7 and some above 0.7, then some individuals must have observed\ninformation in favour of H and the others information against H; so, intuitively, \u03c0(H)\nshould be higher than 0.7 if \u2018most\u2019 individuals had priors of H below 0.7 (hence, had\ninformation supporting H).\nThese considerations highlight that knowing just the individuals\u2019 current (i.e.\nposterior) opinions \u03c01, ..., \u03c0n does not suffice to determine a collective opinion \u03c0 that\nefficiently aggregates private information. But \u03c01, ..., \u03c0n are all that classic opinion\npooling takes into account in calculating \u03c0. This suggests that one should depart\nfrom the classic framework. As the above example lets one suspect, the collective\nopinion \u03c0 should be sensitive not just to people\u2019s posterior opinions \u03c01, ..., \u03c0n but\nalso their prior opinions.\nThis paper (which is based on my unpublished paper Dietrich 2004) presents an\naxiomatic framework that explicitly models the information states of the individuals.\nThe axioms lead (in the common prior case) to a unique formula for the collective\nprobability function; no weights or other parameters are needed to incorporate all\nindividual information into the collective beliefs. For the reason explained above,\nthe collective beliefs depend not just on people\u2019s actual (i.e. posterior) beliefs but\nalso their prior beliefs. This increased individual input is necessary and sufficient to\nefficiently aggregate information, which might come as a surprise. In short, knowing\nthe (complex) content of people\u2019s private information is not needed: knowing people\u2019s\nprior-posterior pairs suffices.\nAs an alternative to our approach, the supra-Bayesian approach might also be\nable to aggregate information efficiently; however, despite conceptual elegance, the\napproach suffers from some problems, among which practical infeasibility.2\nIn modelling both individuals and the collective as Bayesian rationals, our findings\nare also relevant to the theory of Bayesian aggregation, which aims to merge individual\nbeliefs\/values\/preferences satisfying Bayesian rationality conditions (in the sense of\nSavage (1954) or Jeffrey (1983)) into equally rational collective ones; for the ex ante\napproach, e.g. Seidenfeld et al. (1989), Broome (1990), Schervish et al. (1991) and\n2 In the supra-Bayesian approach (introduced by Morris\u2019 (1974) seminal work and extended in a\nlarge literature), collective beliefs are obtained as posterior probabilities (held by the real or vir-\ntual \u2018supra-Bayesian\u2019) conditional on the observed individual beliefs (treated as random events or\nevidence). This presupposes knowing (i) prior probabilities, and (ii) the likelihoods with which the\nindividuals make probability assignments. It is not clear where these prior probabilities and likeli-\nhoods can come from; reaching a compromise or consensus on them might involve a more complex\nopinion pooling problem than the original one.\n3\nMongin (1995, 1998); for the ex post approach, e.g. Hylland and Zeckhauser (1979),\nLevi (1990), Hild (1998) and Risse (2001); for an excellent overview, see Risse (2003).\nSection 2 presents the axiomatic model and derives the resulting aggregation rule.\nSection 3 gives a numerical example. Section 4 identifies our pooling formula as\na form of multiplicative opinion pooling. Sections 5 and 6 address the case of no\ncommon prior. Section 7 analyses the independent-information assumption made so\nfar. Section 8 generalises the aggregation rule to arbitrary information overlaps.\n2 An axiomatic model\nConsider a group of persons i = 1, ..., n (n \u2265 2) who need collective beliefs on certain\nhypotheses, represented as subsets H of a non-empty set \u2126 of possible worlds, i.e.\nworlds that are possible under the shared information. Throughout I call information\n(knowledge, an observation etc.) \u2018shared\u2019 if it is held by all group members. Let H be\nthe set of hypotheses H \u2286 \u2126 of interest, where H forms a finite or countably infinite\npartition of \u2126 and \u2205 \/\u2208 H. So, the hypotheses are mutually exclusive and exhaustive.\nA simple but frequent case is a binary problem H = {H,\u2126\\H}, where H might be\nthe hypothesis that the defendant in a court trial is guilty. In a non-binary case, H\nmight contain different hypotheses on the defendant\u2019s extent of guilt.\nIn practice, the hypotheses on which opinions are formed need not be represented\nas subsets of a set of worlds \u2126. This representation and indeed the set \u2126 are needed\nonly in the present formal framework, so that we can formulate axioms, and introduce\nfurther background objects (events and probability measures) which are needed in the\naxioms but do not appear when applying the resulting pooling formulas.\nI call an opinion (on H) any function f : H \u2192 (0, 1] with\n\u2211\nH\u2208H f(H) = 1\n(whereas probability measures are, as usual, defined on \u03c3-algebras of events3); let \u03a0\nbe the set of all these functions f .\nLet each individual i hold an opinion \u03c0i \u2208 \u03a0, and let the collective also hold\nan opinion \u03c0 \u2208 \u03a0. So far, this is entirely classical. Classical opinion pooling would\nproceed by placing conditions on how \u03c0 depends on \u03c01, ..., \u03c0n, resulting in a unique\nrelationship (e.g. \u03c0 = 1n\u03c01 + ... +\n1\nn\u03c0n) or a class of possible relationships (e.g. all\nlinear relationships).\n2.1 Simple case: common prior beliefs and a common belief domain\nBefore stating the axiomatic approach in full generality (that is, before allowing\nindividuals to hold different prior beliefs defined within different domains of events),\n3Any opinion uniquely extends to a probability measure defined on the \u03c3-algebra \u03c3(H) generated\nby H, and so we lose nothing by pooling opinions defined on H rather than probability measures\ndefined on \u03c3(H). By definition, opinions never assign zero probability to any hypothesis; this is\nmainly for technical convenience.\n4\nI sketch the approach in the simple base-line case. Suppose for the moment that any\nindividual i\u2019s opinion \u03c0i : H\u2192 (0, 1] is given by\n\u03c0i(H) = P (H|Ei) for all H \u2208 H,\nwhere for now P is a common prior probability measure (defined on an appropriate\n\u03c3-algebra over \u2126, for instance the power set P(\u2126)), and where Ei \u2286 \u2126 is individual i\u2019s\nprivate information with P (Ei) > 0. Suppose further that people hold independent\ninformation: E1, ..., En are independent conditional on any hypothesis H \u2208 H.\n4 We\nwould like to calculate a group opinion \u03c0. This group opinion should include all\ninformation spread over the individuals, i.e.,\n\u03c0(H) = P (H|E1 \u2229 ... \u2229En) for all H \u2208 H (1)\n(where one easily checks that (1) is well-defined, i.e., that P (E1 \u2229 ...\u2229En) > 0). One\napproach would be to ask all individuals i to \u2018tell\u2019 their private experience Ei, so that\nthe group could simply gather all experiences and calculate the conditional probabil-\nities (1). But this procedure may be unrealistic, as personal experience may be very\ncomplex and hard-to-communicate in normal language and limited time. (Another\nproblem, which we currently assume away by using a common belief domain, is that\nperson i\u2019s experience Ei may be an event of which the other persons have no prior\nbeliefs, or even no awareness or conceptualisation; asymmetries in awareness or con-\nceptualisation might indeed explain why different people make different experiences.)\nAssuming that private evidence cannot (or is not) communicated, can the beliefs\nin (1) be calculated at all? The following derivation gives a positive answer. Consider\na hypothesis H \u2208 H and the belief \u03c0(H) as defined by (1). Applying Bayes\u2019 rule and\nthen our independence assumption,\n\u03c0(H) =\nP (H)P (E1 \u2229 ... \u2229En|H)\u2211\nH\u2032\u2208H P (H\n\u2032)P (E1 \u2229 ... \u2229En|H \u2032)\n=\nP (H)P (E1|H) \u00b7 \u00b7 \u00b7P (En|H)\u2211\nH\u2032\u2208H P (H\n\u2032)P (E1|H \u2032) \u00b7 \u00b7 \u00b7P (En|H \u2032)\n.\n4Why do I assume that information is independent conditional on any hypothesis rather than\nunconditionally? Unconditional independence would be implausible. Suppose for instance that the\ninformation of individuals 1 and 2 both strongly correlate with the same hypothesis H in H. (In\na jury trial, the jurors 1 and 2 might each observe patterns in the defendant\u2019s behaviour which\nstrongly point towards the hypothesis of guilt.) Then E1 and E2 are usually not independent but\npositively correlated (P (E2|E1) > P (E2)), because learning E1 raises the probability of H , which\nin turn raises that of E2. More generally, since the evidences E1, ..., En tell something about the\nhypotheses, learning some of the Eis leads to revised probabilities of the hypotheses, which leads\nto revised probabilities of the other Eis. In short, the Eis are non-independent because they are\nmutually relevant via their relevance to hypotheses in H. This argument for non-independence is\nblocked once we condition on a hypothesis: conditional on a given hypothesis being true, evidences\nare not relevant to (i.e., do not bring new information about) hypotheses. If all existing probabilistic\ndependence between evidences goes \u2018via\u2019 the hypotheses, then conditioning on a hypothesis eliminates\nall sources of dependence, and the evidences become conditionally independent. Our (conditional)\nindependence assumption is analysed again below.\n5\nIn the numerator and the denominator, each factor of type P (Ei|H) can be rewritten\naccording to\nP (Ei|H) =\nP (H|Ei)P (Ei)\nP (H)\n=\n\u03c0i(H)P (Ei)\nP (H)\n.\nSubstituting this expression, we obtain\n\u03c0(H) =\nP (H)\u03c01(H)P (E1)P (H) \u00b7 \u00b7 \u00b7\n\u03c0n(H)P (En)\nP (H)\u2211\nH\u2032\u2208H P (H\n\u2032)\u03c01(H\n\u2032)P (E1)\nP (H\u2032) \u00b7 \u00b7 \u00b7\n\u03c0n(H\u2032)P (En)\nP (H\u2032)\n=\n\u03c01(H) \u00b7 \u00b7 \u00b7\u03c0n(H)\/P (H)\nn\u22121\u2211\nH\u2032\u2208H \u03c01(H\n\u2032) \u00b7 \u00b7 \u00b7\u03c0n(H \u2032)\/P (H \u2032)n\u22121\n.\nInterestingly, any private information Ei has dropped out altogether, so that the\ncollective opinion \u03c0 can be calculated solely on the basis of the revealed individual\nopinions \u03c01, ..., \u03c0n (and the fixed prior). Put differently, each individual information\nEi has been incorporated without disclosing it. In short, denoting by p the prior\nopinion P |H (i.e., the restriction of P to the hypotheses of interest), we have shown\nthat\n\u03c0 \u221d \u03c01 \u00b7 \u00b7 \u00b7\u03c0n\/p\nn\u22121.\nHere and throughout, I call functions f, g : H \u2192 R proportional, written f \u221d g, if\nthere exists a constant k \u000e= 0 such that f(H) = kg(H) for all H \u2208 H.\n2.2 General case: possibly distinct prior beliefs and belief domains\nAfter this preliminary analysis, let us start afresh, this time in full generality, and\nstating all assumptions as explicit axioms. Recall that we consider individual opinions\n\u03c01, ..., \u03c0n \u2208 \u03a0 and a collective opinion \u03c0 \u2208 \u03a0. The further elements introduced in the\npreliminary Section 2.1 (namely, P,E1, ..., En) are now re-introduced in their general\nand official form. For each person i there is (without having to be revealed):\n\u2022 an event Ei \u2286 \u2126, i\u2019s personal information;\n\u2022 a (\u2018prior\u2019) probability measure Pi representing i\u2019s beliefs based on the shared\ninformation (hence prior to observing Ei). Pi need not assign a probability to all\nevents in P(\u2126); rather, Pi is defined on some \u03c3-algebra Ai \u2286 P(\u2126), containing\nthe events on which i holds beliefs (whereas on other events i may lack beliefs,\nor even lack awareness or conceptualisation). But Ai should contain at least Ei\nand all hypotheses in H, where Pi(Ei) > 0 and Pi(H) > 0 for all H \u2208 H. The\nrestriction of i\u2019s prior belief Pi to H is called i\u2019s prior opinion. It is denoted by\npi (\u2208 \u03a0) and given by pi(H) = Pi(H) for all H \u2208 H.\nThese model resources allow us to state a standard rationality condition:\n6\nIndividual Bayesian Rationality (IBR) \u03c0i(H) = Pi(H|Ei) for each person i and\nhypothesis H \u2208 H.5\nA person i\u2019s belief domain Ai may fail to contain another person j\u2019s observation\nEj , and this for (at least) two reasons. First, the fact that j but not i observed Ej\nmay be due precisely to j having subjectively conceptualised Ej but i not having\ndone so; juror j in a trial may be the only juror to observe the suspicious smile on\nthe defendant\u2019s face because the other jurors i do not even know what a suspicious\nsmile would be like. Second, j\u2019s information Ej may be so detailed and complex that\nprior to j observing it, it belonged not even to j\u2019s own belief domain, let alone to i\u2019s;\nthat is, it was only while observing Ej that person j extended his prior beliefs to a\nlarger domain Aj containing Ej.\nFollowing the paradigm of social choice theory, I treat the collective as a separ-\nate virtual agent with its own beliefs. While this agent is typically a construction\n(i.e. there needn\u2019t exist any real individual holding these beliefs), the social choice\nparadigm requires it to be as rational as any real individual.6 \u2018Rationality\u2019 refers to\ndifferent things in different contexts (e.g. to transitivity of preferences in Arrovian\npreference aggregation, to von-Neumann-Morgenstern rationality in Harsanyi\u2019s The-\norem on group preferences over lotteries, to logical consistency in judgment aggreg-\nation, and so on). In the present context, it naturally refers to Bayesian rationality.\nTo formulate this, I suppose that there is\n\u2022 a (\u2018prior\u2019) probability measure P , representing collective beliefs based on people\u2019s\nshared information (hence not on their private information E1, ..., En). P is\ndefined on some \u03c3-algebra A \u2286 P(\u2126), the domain of the collective beliefs,\nwhich contains at least all private evidences E1, ..., En and all H \u2208 H, where\nP (E1 \u2229 ... \u2229 En) > 0 and P (H) > 0 for all H \u2208 H. The restriction of the\ncollective prior belief P to H is called the collective prior opinion; it is denoted\nby p (\u2208 \u03a0) and given by p(H) = P (H) for all H \u2208 H.\nA, P and p are the collective counterparts of Ai, Pi and pi. The collective coun-\nterpart of (IBR) is:\nCollective Bayesian Rationality (CBR) \u03c0(H) = P (H|E1 \u2229 ... \u2229 En) for each\nhypothesis H \u2208 H.\n5The conditional probability Pi(H|Ei) is well-defined because Ei,H \u2208 Ai and Pi(Ei) > 0. Our\nassumptions also take care that all other conditional probabilities used in this paper are well-defined.\n6The collective agent should be rational notably because it forms the basis for collective actions\nand decisions.\n7\nCondition (CBR) requires the collective opinion \u03c0 to incorporate all information\nspread over people: the shared information (contained in the prior P ) and all personal\ninformation (contained in E1, ..., En).\nWhile we have ensured, via (CBR), that the collective opinion uses all evidence\nscattered across individuals, we have done nothing so far to constrain the collective\nprior probability measure P (which underlies \u03c0). Indeed, P may so far be totally\ndisconnected from the individual prior probability measures P1, ..., Pn (which underlie\n\u03c01, ..., \u03c0n). The next condition does something to connect P to P1, ..., Pn. More\nprecisely, the condition ties the likelihood that the collective assigns to the various\nindividual evidences E1, ..., En to the individuals\u2019 own likelihood assessments:\nAccept People\u2019s Likelihood Assessments (APLA) For all persons i and hypo-\ntheses H \u2208 H, P (Ei|H) = Pi(Ei|H).\nThis principle requires the collective to take over i\u2019s own interpretation of i\u2019s in-\nformation Ei as given by i\u2019s likelihood assignments Pi(Ei|H), H \u2208 H. To motivate\nthis condition, let me first explain the context in a little more detail. In statistics,\nthe information that data contain on given hypotheses (as opposed to prior beliefs\non these hypotheses) is usually taken to be summarised in the data\u2019s likelihood func-\ntion, which maps any hypothesis to the data\u2019s probability given this hypothesis. For\ninstance, the information on humidity contained in a temperature measurement of\n20 degrees Celsius is given by the mapping that assigns to each potential humidity\nlevel the probability that temperature is 20 degrees Celsius given this humidity level.\nIn our case, the information contained in individual i\u2019s evidence Ei is summarised in\nEi\u2019s likelihood function, mapping any hypothesis H to Ei\u2019s probability given H. But\nhow large exactly is Ei\u2019s probability given H? For instance, how probable is it that\nthe defendant in a trial has a particular facial expression (Ei) given the hypothesis\nthat he is guilty (H)? The answer may be far from trivial, as one might come up\nwith various different interpretations of the same observation. Condition (APLA)\nrequires that the answer that the collective gives matches the answer that the indi-\nvidual who observed the evidence gives; that is, P (Ei|H) = Pi(Ei|H). What is the\nmotivation behind identifying P (Ei|H) with Pi(Ei|H)? Why not also take other per-\nsons\u2019 interpretations of Ei into account by defining P (Ei|H) as some compromise of\nP1(Ei|H), ..., Pn(Ei|H)? First, for reasons explained above, the persons j \u000e= imay not\neven hold beliefs on the unobserved event Ei (i.e., Ei \u000e\u2208 Aj), in which case Pj(Ei|H)\nis simply undefined. Second, assuming that the persons j \u000e= i do hold such beliefs\n(i.e., Ei \u2208 Aj), a \u2018likelihood compromise\u2019 could only be formed after each person j\nreveals Pj(Ei|H); which in turn supposes that first i communicates his informational\nbasis Ei in all detail to the rest of the group. This is not only at odds with the\npresent approach, but may also be infeasible: given the possible complexity of Ei\nand the limitations of language, time, i\u2019s ability to describe Ei, j\u2019s (j \u000e= i) ability to\nunderstand Ei, and so on, j could probably learn at most some approximation E\u02dci of\n8\nEi, and so j could at most provide j\u2019s likelihood of E\u02dci, which only approximates j\u2019s\nlikelihood of the true Ei (Pj(E\u02dci|H) \u2248 Pj(Ei|H)).\nThe next assumption is not a normative condition but rather an assumption on the\nenvironment: individuals receive independent information. This assumption will be\nanalysed (and relaxed) in later sections; see footnote 4 above for first considerations.\nFor now, I only mention that it is strong but very common. It is analogous to\n(i) independence assumptions on private information\/types in Bayesian games, (ii)\nthe independence condition in the literature on the Condorcet Jury Theorem (see\nDietrich 2008 for a critique of the condition), (iii) the Parental Markov Condition in\nthe theory of Bayesian networks (interpreting the true hypothesis in H as the parent\nof each information Ei in a Bayesian network; see Pearl 2000), and (iv) Fitelson\u2019s\n(2001) condition of confirmational independence.\nIndependent Information (Ind) For each hypothesis H \u2208 H, the personal obser-\nvations E1, ..., En are independent conditional on H.\n7\nI am ready to state the theorem. Recall that pi, \u03c0i is the pair of person i\u2019s prior\nand posterior opinion, and p, \u03c0 is the pair of the collective prior and posterior opinion.\nTheorem 1 Suppose individuals satisfy (IBR), information satisfies (Ind), and the\ncollective satisfies (CBR) and (APLA). Then the collective opinion \u03c0 is proportional\nto the collective prior opinion times all individual posterior-to-prior ratios:\n\u03c0 \u221d p\n\u03c01\np1\n\u00b7 \u00b7 \u00b7\n\u03c0n\npn\n.\nProof. Suppose (IBR), (CBR), (APLA) and (Ind) hold. For all H in H,\n\u03c0(H) = P (H|E1 \u2229 ... \u2229En) by (CBR)\n=\nP (E1 \u2229 ... \u2229En|H)p(H)\nP (E1 \u2229 ... \u2229En)\nby Bayes\u2019 rule\n= kP (E1 \u2229 ... \u2229En|H)p(H) for a constant k \u000e= 0\n= kP (E1|H) \u00b7 \u00b7 \u00b7P (En|H)p(H) by (Ind)\n= kP1(E1|H) \u00b7 \u00b7 \u00b7Pn(En|H)p(H) by (APLA)\n= k\nP1(H|E1)P1(E1)\np1(H)\n\u00b7 \u00b7 \u00b7\nPn(H|En)Pn(En)\npn(H)\np(H) by Bayes\u2019 rule\n= k\u2032\nP1(H|E1)\np1(H)\n\u00b7 \u00b7 \u00b7\nPn(H|En)\npn(H)\np(H) for a constant k\u2032 \u000e= 0\n= k\u2032\n\u03c01(H)\np1(H)\n\u00b7 \u00b7 \u00b7\n\u03c0n(H)\npn(H)\np(H) by (IBR). \u0001\nThree important remarks are due.\n7As usual, by \u2018independence\u2019 of events I mean full independence, not just pairwise independence.\n9\n1. As promised, the collective opinion \u03c0 is calculated without people having\nto communicate their arbitrarily complex informational bases Ei or their likelihoods\nP (Ei|H), H \u2208 H. In practice, all persons i submit their prior-posterior pairs pi, \u03c0i (or\njust their ratios \u03c0i\/pi), and then the collective opinion \u03c0 is calculated. So, compared\nto standard opinion pooling, we additionally require submission of prior opinions\np1, ..., pn, a complication that enables the incorporation of the individual information\nE1, ..., En into the collective opinion.\n2. Theorem 1\u2019s formula does not fully solve the aggregation problem since we do\nnot yet know how to determine the collective prior opinion p. Strategies to choose\np are presented in Sections 5 and 6. In practice, there is an alternative to having\nto choose p: one might use an approximation of Theorem 1\u2019s formula, defining the\ncollective opinion as\n\u03c0approx \u221d\n\u03c01\np1\n\u00b7 \u00b7 \u00b7\n\u03c0n\npn\n, (2)\nthe product of all posterior-to-prior ratios. When and why can \u03c0approx count as a\ngood approximation of Theorem 1\u2019s formula? Let me give some heuristic arguments.8\nOften, the function \u03c01p1 \u00b7 \u00b7 \u00b7\n\u03c0n\npn\nvaries considerably, i.e., assigns very different values to\nthe hypotheses H in H. Intuitively, this is because pooled information is often strong\nevidence for or against certain hypotheses. More formally, if sufficiently many of\nthe individual ratios \u03c0ipi vary at least moderately, the product\n\u03c01\np1\n\u00b7 \u00b7 \u00b7 \u03c0npn varies strongly\n(provided that the individual variations do not systematically cancel each other out).9\nWhenever the variation of \u03c01p1 \u00b7 \u00b7 \u00b7\n\u03c0n\npn\nis strong enough to \u2018outweigh\u2019 that of p (assuming\np should not vary very much), the function \u03c01p1 \u00b7 \u00b7 \u00b7\n\u03c0n\npn\nvaries roughly like p\u03c01p1 \u00b7 \u00b7 \u00b7\n\u03c0n\npn\n;\nand hence, the opinions \u03c0approx and \u03c0 (obtained by normalising the two functions so\nas to each sum to one) are roughly similar.\n3. Assume a unanimous posterior agreement \u03c01 = ... = \u03c0n (as in the intro-\nduction\u2019s example). Then only in special cases does \u03c0 equal \u03c01 = ... = \u03c0n, which\nshows that the unanimity\/Pareto principle often required in standard opinion pool-\ning is problematic under informational asymmetries. One such special case is that\n\u03c01 = ... = \u03c0n = p1 = ... = pn = p, so that none of the personal observations E1, ..., En\nconfirms or disconfirms any hypothesis, i.e., in essence, there is no informational\nasymmetry.\nAn important special case of Theorem 1 is that where people have managed to\nagree on how to interpret their shared information, i.e. where they hold a common\nprior opinion:\nCommon Prior (CP) p1 = ... = pn = p (i.e., the prior probability measures\nP1, ..., Pn, P agree on all hypotheses in H, though perhaps not elsewhere).\n8 I owe these thoughts to the helpful referee.\n9 If for instance most individual ratios peak at the same hypothesis (say, if most jurors believe the\ndefendant is guilty) then the product of ratios is likely to strongly peak at this hypothesis.\n10\nCorollary 1 Under the assumptions of Theorem 1 and (CP), the collective opinion\n\u03c0 is given by\n\u03c0 \u221d \u03c01 \u00b7 \u00b7 \u00b7\u03c0n\/p\nn\u22121\n1 .\nLet me make three remarks on this corollary.\n1. The corollary\u2019s formula differs in an important respect from Theorem 1\u2019s\nformula: the parameter p has been eliminated, and so the collective opinion \u03c0 is fully\ndetermined by the individual prior and posterior opinions. By contrast, if (CP) fails,\ni.e. if the group didn\u2019t manage to agree on how to interpret the shared information,\nTheorem 1\u2019s formula does not fully solve the aggregation problem, as we need a way\nto determine the collective prior p (see Sections 5 and 6).\n2. Condition (CP) can in fact be seen as the conjunction of two conditions. The\nfirst (descriptive) condition is that p1 = ... = pn, i.e. all persons i submit the same\nprior opinion. The second (normative) condition is that the unanimity (or Pareto)\nprinciple holds for the prior opinions, i.e. if all submit the same prior opinion, this\nbecomes the collective prior opinion. Applying a unanimity condition to prior opinions\nis far less problematic than doing so for the posterior opinions \u03c01, ..., \u03c0n, \u03c0, because\nprior opinions contain no informational asymmetry.\n3. According to a prominent view, held notably by Harsanyi, any inter-personal\ndifferences in beliefs between rational agents stem from different information (in\na suitably general sense of this word), never from different prior beliefs.10 If this\nview is correct, and if each opinion pi indeed incorporates no information except\nthe shared one which does not depend on i, then the pis must be identical. So,\nHarsanyi\u2019s view places us in the comfortable position of being able to assume (CP).\nHarsanyi\u2019s view is based on modelling all experiences \u2014 including early ones in life\nand perhaps prenatal ones \u2014 as information shaping beliefs via Bayesian conditioning.\nIf we think of i\u2019s private information Ei as containing all such experiences, then the\nevents Ei (and the underlying space \u2126) inevitably become highly complex. While\nthis by itself poses no problem (since E1, ..., En need not be revealed), at least one\ndifficulty arises. Even if Harsanyi were fundamentally right, people will in practice\noften not agree on a common prior opinion, if only because they do not \u2018remember\u2019 the\ncommon prior opinion that they used to hold at the prenatal stage. I personally do\nnot share Harsanyi\u2019s view. I believe in the possibility of genuinely non-information-\ndriven disagreements, and hence in the possibility that p1, ..., pn differ in spite of\nincorporating the same (shared) information.11\n10 I am grateful to the referee for raising this issue.\n11Harsanyi\u2019s claim is true nearly by definition if the notion of \u2018information\u2019 is purely technical and if\nthe claim is taken to be one about mathematically representing different probability measures as being\nobtained by conditioning from a common probability measure (defined on a suitably extended algebra\nof events). Under a so-extended notion of \u2018information\u2019, the whole process of personal deliberation\nneeded to form one\u2019s beliefs and to interpret one\u2019s information constitutes another large piece of\n(\u2018meta-\u2019)information. The current paper\u2019s notion of \u2018information\u2019 is not of this abstract kind. It\n11\n4. Instead of interpreting Ei as reflecting all of i\u2019s personal information, one\nmight re-interpret Ei as reflecting only that part of i\u2019s personal information which i\nhas incorporated rationally into his opinion (in the sense of Bayesian conditioning).\nThen (IBR) becomes true by definition. All not rationally incorporated personal\ninformation is then simply thrown away, i.e., not included in collective beliefs.12\n3 A numerical example for a simple case\nConsider the simple case of a binary problem H = {H,\u2126\\H} (H and \u2126\\H might\nmean that the defendant in a court trial is guilty resp. innocent, and persons might\nbe jurors). Suppose Common Prior (CP), i.e. p1 = ... = pn = p. By Theorem 1 (that\nis, by its corollary), the collective posterior of H is given by\n\u03c0H\n\u03c0H1 \u00b7 \u00b7 \u00b7\u03c0\nH\nn \/(p\nH)n\u22121\n\u03c0H1 \u00b7 \u00b7 \u00b7\u03c0\nH\nn \/(p\nH)n\u22121 + (1\u2212 \u03c0H1 ) \u00b7 \u00b7 \u00b7 (1\u2212 \u03c0\nH\nn )\/(1\u2212 p\nH)n\u22121\n=\n1\n1 + (1\/\u03c0H1 \u2212 1) \u00b7 \u00b7 \u00b7 (1\/\u03c0\nH\nn \u2212 1)\/(1\/p\nH \u2212 1)n\u22121\n,\n(3)\nwhere pH := p(H), \u03c0H := \u03c0(H) and \u03c0Hi := \u03c0i(H). For the case of only n = 2\nindividuals, in which the formula (3) for the collective posterior reduces to\n\u03c0H =\n\u03c0H1 \u03c0\nH\n2 \/p\nH\n\u03c0H1 \u03c0\nH\n2 \/p\nH + (1\u2212 \u03c0H1 )(1\u2212 \u03c0\nH\n2 )\/(1\u2212 p\nH)\n=\n1\n1 + (1\/\u03c0H1 \u2212 1)(1\/\u03c0\nH\n2 \u2212 1)\/(1\/p\nH \u2212 1)\n,\nTable 1 contains the values of the collective belief \u03c0H for all possible combinations of\nvalues of pH , \u03c0H1 , \u03c0\nH\n2 in the grid {0.1, 0.25, 0.5, 0.75, 0.9}.\n13 Note how drastically the\ngroup belief \u03c0H depends on the prior pH . By shifting pH below (above) the individual\nposteriors \u03c0Hi , the group belief \u03c0\nH quickly approaches 1 (0). The interpretation is\nthat if the posteriors \u03c0Hi are all to the same side of the prior, then the evidences\nis a substantive notion, under which it is possible that two opinions incorporate no (or the same)\ninformation and yet differ. If however Harsanyi\u2019s claim is taken to be not just about mathematical\nrepresentability but about the psychological reality of rational agents, then the claim is problematic.\nMany experiences in life (such as hearing a sound for the first time) have a content that was not\npreviously conceptualized by the agent, hence cannot belong to the algebra within which the agent\npreviously held beliefs. So the agent\u2019s new beliefs after the experience cannot stem from updating the\nold beliefs by Bayesian conditioning on this event. The topic of non-informational belief formation\ngoes beyond this paper (but will be developed in the paper \u2018A reason-based theory of rational belief\u2019\nwith Christian List).\n12 Instead of throwing this information away, one might ask people to incorporate it (in some\nnon-Bayesian ways, unfortunately) in the submitted prior opinions. This removes the informational\nsymmetry underlying the submitted prior opinions, which in turn affects the interpretation and\nplausibility of the analysis of later sections.\n13The entries are rounded results if three decimal digits are reported, and exact results else.\n12\npH :\n0.1 0.25 0.5 0.75 0.9\n0.1, 0.1 0.1 .036 .012 .004 .001\n0.25, 0.1 0.25 0.1 .036 .012 .004\n0.25, 0.25 0.5 0.25 0.1 .036 .012\n0.5, 0.1 0.5 0.25 0.1 .036 .012\n0.5, 0.25 0.75 0.5 0.25 0.1 .036\n0.5, 0.5 0.9 0.75 0.5 0.25 0.1\n0.75, 0.1 0.75 0.5 0.25 0.1 .036\n\u03c0H1 , \u03c0\nH\n2 : 0.75, 0.25 0.9 0.75 0.5 0.25 0.1\n0.75, 0.5 0.964 0.9 0.75 0.5 0.25\n0.75, 0.75 0.988 0.964 0.9 0.75 0.5\n0.9, 0.1 0.9 0.75 0.5 0.25 0.1\n0.9, 0.25 0.964 0.9 0.75 0.5 0.25\n0.9, 0.5 0.988 0.964 0.9 0.75 0.5\n0.9, 0.75 0.996 0.988 0.964 0.9 0.75\n0.9, 0.9 0.999 0.996 0.988 0.964 0.9\nTable 1: Collective probability \u03c0H = \u03c0(H) in dependence of the common prior\npH = p(H) and the individual posteriors \u03c0Hi = \u03c0i(H), for a group of size n = 2.\nEi all point into the same direction, so that their conjunction points even more into\nthat direction. But if the prior pH is somewhere in the middle of the posteriors\n\u03c0Hi , the group belief \u03c0\nH may be moderate. The interpretation is that if some of the\nposteriors \u03c0Hi are above the prior and others are below the prior, then the evidences Ei\npoint into different directions, and their conjunction need not strongly point into any\ndirection. The above formula for the group belief \u03c0H shows that it strictly increases\nas a function of each individual belief \u03c0Hi , but strictly decreases as a function of the\nprior belief pH . But how can one make sense of the group posterior \u03c0H depending\nnegatively on the prior pH? How could more prior support for H possibly reduce H\u2019s\nposterior probability? The answer is that by increasing the prior pH while keeping\nthe individual posteriors \u03c0Hi fixed one implicitly reduces the support that each of the\nevidences Ei gives to H; as a result, the collective posterior of H falls, intuitively\nbecause the reduced evidential support for H overcompensates the increased prior\nsupport.\n4 Multiplicative opinion pooling\nIf we treat the prior opinions p1, ..., pn, p as fixed parameters, the pooling formula\nof Theorem 1 depends just on \u03c01, ..., \u03c0n, hence defines a classic pooling function\nF : \u03a0n \u2192 \u03a0. Specifically, this pooling function is given by \u03c0 = g \u00b7\u03c01 \u00b7 \u00b7 \u00b7\u03c0n where g is\n13\na fixed function on H given by g \u221d p\/(p1 \u00b7 \u00b7 \u00b7 pn) (and in particular as g \u221d p\n1\u2212n under\nCommon Prior (CP)). So, our axioms lead to what one might call a multiplicative\nopinion pool. Formally, a (classic) opinion pool F : \u03a0n \u2192 \u03a0 is multiplicative if it is\ngiven by\nF (\u03c01, ..., \u03c0n) = g \u00b7 \u03c01 \u00b7 \u00b7 \u00b7\u03c0n for all \u03c01, ..., \u03c0n \u2208 \u03a0,\nfor some fixed function g : H \u2192 (0,\u221e). The simplest multiplicative rule is that in\nwhich g is a constant function, so that\nF (\u03c01, ..., \u03c0n) \u221d \u03c01 \u00b7 \u00b7 \u00b7\u03c0n for all \u03c01, ..., \u03c0n \u2208 \u03a0.\nNote how multiplicative opinion pools differ from the more common linear and geo-\nmetric opinion pools; these arise from different axiomatic systems that do not make\ninformation explicit.\nIn fact, our axioms not only imply that pooling is multiplicative: they characterise\nmultiplicative pooling if H is finite because every multiplicative rule can be obtained\nfrom suitable priors p1, ..., pn, p \u2208 \u03a0.\n14\n5 Choosing the collective prior p when there is no com-\nmon prior\nIf the interpretation of the shared information is controversial and hence (CP) fails,\nthe group needs to determine the collective prior p in Theorem 1\u2019s formula. At least\nthree strategies are imaginable. First, one might define p as a uniform or maximum-\nentropy prior if available. Second, someone, not necessarily a group member, may\nbe appointed to choose p, either by drawing on his own prior beliefs, or by taking\ninspiration from the submitted priors p1, ..., pn, or by using statistical estimation\ntechniques if available. These two solutions have obvious limitations, including some\nad-hoc-ness and a lack of democracy. A third alternative is to replace p by F (p1, ..., pn)\nand thus define the collective opinion by\n\u03c0 \u221d\n\u03c01\np1\n\u00b7 \u00b7 \u00b7\n\u03c0n\npn\nF (p1, ..., pn), (4)\nwhere F : \u03a0n \u2192 \u03a0 is a standard opinion pool. Note that F is used here not to\naggregate people\u2019s actual (posterior) opinions \u03c01, ..., \u03c0n but to aggregate their prior\nopinions p1, ..., pn, namely into a \u2018compromise prior\u2019. At first sight, one may wonder\nwhat is gained by formula (4) compared to the standard approach of defining \u03c0 =\nF (\u03c01, ..., \u03c0n) without having to care about priors p1, ..., pn. Does formula (4) not just\nshift the classic aggregation problem \u2014 pooling \u03c01, ..., \u03c0n into \u03c0 \u2014 towards an equally\n14For any multiplicative rule F : \u03a0n \u2192 \u03a0, say generated by the function g, if we (for instance)\ntake p1, ..., pn, p to be all identical and proportional to g\n\u22121\/(n\u22121), then g \u221d p\/(p1 \u00b7 \u00b7 \u00b7 pn), and hence\nthe multiplicative rule generated by g coincides with that arising in Theorem 1.\n14\ncomplex aggregation problem about priors \u2014 pooling p1, ..., pn into p? In an important\nrespect, pooling p1, ..., pn is simpler than pooling \u03c01, ..., \u03c0n: unlike \u03c01, ..., \u03c0n, the prior\nopinions p1, ..., pn involve no informational asymmetry since each pi is based on the\nsame (shared) information.15 Hence any disagreement between p1, ..., pn is due solely\nto different interpretations of that same body of information. This may facilitate\nthe choice of F . For instance, aggregation may be guided by the unanimity\/Pareto\nprinciple (which is problematic under informational asymmetry, as we have seen).\nFurther, aggregation may place equal weights on each of the priors p1, ..., pn (whereas\npooling \u03c01, ..., \u03c0n may involve the difficult and vague exercise of assigning more weight\nto better informed people). The literature\u2019s two most prominent types of opinion\npools F : \u03a0n \u2192 \u03a0 are\nlinear opinion pools: F (p1, ..., pn) = w1p1 + ...+wnpn,\ngeometric opinion pools: F (p1, ..., pn) \u221d p\nw1\n1 \u00b7 \u00b7 \u00b7 p\nwn\nn ,\nwith weights w1, ..., wi \u2208 [0, 1] that add up to 1 (where in the geometric pool the\nfactor of proportionality is chosen such that\n\u2211\nH\u2208H F (p1, ..., pn)(H) = 1). If F is a\nlinear resp. geometric opinion pool, our pooling formula (4) becomes\n\u03c0 =\n\u03c01\np1\n\u00b7 \u00b7 \u00b7\n\u03c0n\npn\n(w1p1 + ...+wnpn) (5)\nresp. \u03c0 \u221d\n\u03c01\np1\n\u00b7 \u00b7 \u00b7\n\u03c0n\npn\npw11 \u00b7 \u00b7 \u00b7 p\nwn\nn =\n\u03c01\np1\u2212w11\n\u00b7 \u00b7 \u00b7\n\u03c0n\np1\u2212wnn\n. (6)\nHow should the weights w1, ..., wn be chosen in practice? In general, unequal weights\nmay be justified either by different information states or by different competence,\ni.e. ability to interpret information. The former reason does not apply here, since\np1, ..., pn are by definition based on the same (shared) information. If, in addition,\ndifferences of competence are either inexistent, or unknown, or not to be taken into\naccount for reasons of procedural fairness, then equal weights w1 = ... = wn = 1\/n\nare justified, so that our pooling formula becomes\n\u03c0 =\n1\nn\n\u03c01\np1\n\u00b7 \u00b7 \u00b7\n\u03c0n\npn\n(p1 + ...+ pn) (7)\nresp. \u03c0 \u221d\n\u03c01\np\n1\u22121\/n\n1\n\u00b7 \u00b7 \u00b7\n\u03c0n\np\n1\u22121\/n\nn\n, (8)\nwhich is parameter-free, hence uniquely solves the aggregation problem.\n6 External and internal Bayesianity\nI now give an argument in defence of defining F in (4) as a geometric (or more\ngenerally, externally Bayesian) opinion pool, hence in defence of our pooling formulae\n15One might even argue that, while pooling p1, ..., pn into p is possible without using extra in-\nformation (due to the informational symmetry), pooling \u03c01, ..., \u03c0n into \u03c0 is impossible without extra\ninformation (such as p1, ..., pn).\n15\n(6) and (8). Note first that in (4) \u03c0 is a function of the vector (p1, \u03c01..., pn, \u03c0n) \u2208\n(\u03a0\u00d7\u03a0)n = \u03a02n, containing every person\u2019s prior and posterior.\nDefinition 1 A generalised opinion pool (\u2018GOP\u2019) or generalised probability aggreg-\nation rule is a function G : \u03a02n \u2192 \u03a0.\nUnlike a standard opinion pool F : \u03a0n \u2192 \u03a0, a GOP G also takes as inputs the pis,\ni.e. people\u2019s interpretations of the shared information. As shown above, our axioms\nimply that a GOP G should take the form (4), i.e. the form\nG(p1, \u03c01, ..., pn, \u03c0n) \u221d\n\u03c01\np1\n\u00b7 \u00b7 \u00b7\n\u03c0n\npn\nF (p1, ..., pn) (9)\nwhere F : \u03a0n \u2192 \u03a0 is a standard opinion pool that merges the priors p1, ..., pn.\nFrom a Bayesian perspective, two natural conditions may be imposed on a GOP,\nto be called external and internal Bayesianity. The former is an analogue of the\nequally-named classic condition for standard opinion pools F : it should not matter\nwhether information arrives before or after pooling, i.e. pooling should commute\nwith Bayesian updating. Formally, for every opinion p \u2208 \u03a0 and (likelihood) function\nl : H\u2192 (0, 1] the (updated) opinion pl \u2208 \u03a0 is defined by\npl(H) :=\nl(H)p(H)\u2211\nH\u2032\u2208H l(H\n\u2032)p(H \u2032)\n, in short pl \u221d lp. (10)\nHere, l is interpreted as a likelihood function P (E|.) for some observation E, so that\npl is a posterior probability. A standard opinion pool F : \u03a0n \u2192 \u03a0 is called externally\nBayesian if\nF (pl1, ..., p\nl\nn) = F (p1, ..., pn)\nl\nfor every profile (p1, ..., pn) \u2208 \u03a0\nn and (likelihood) function l : H \u2192 (0, 1] (Madansky\n(1964)). In particular, geometric opinion pools are externally Bayesian. An analogous\nconcept can be defined for GOPs:\nDefinition 2 A GOP G : \u03a02n \u2192 \u03a0 is called externally Bayesian if\nG(pl1, \u03c0\nl\n1, ..., p\nl\nn, \u03c0\nl\nn) = G(p1, \u03c01, ..., pn, \u03c0n)\nl\nfor every profile (p1, \u03c01, ..., pn, \u03c0n) \u2208 \u03a02n and (likelihood) function l : H\u2192 (0, 1].\nOn the left hand side of this equation not only all posteriors are updated (\u03c0li),\nbut also all priors (pli), because the incoming information is observed by everybody,\nhence part of the shared information, hence contained in the priors.\nWhile external Bayesianity requires that it be irrelevant whether pooling happens\nbefore or after updating, a different question is whether it matters who in the group\nhas observed a given information. Internal Bayesianity requires that it be irrelevant\nwhether every or just a single person obtains a given information:\n16\nDefinition 3 A GOP G : \u03a02n \u2192 \u03a0 is called internally Bayesian if, for each person\ni,\nG(p1, \u03c01, ..., pi\u22121, \u03c0i\u22121, pi, \u03c0\nl\ni, pi+1, \u03c0i+1, ..., pn, \u03c0n) = G(p\nl\n1, \u03c0\nl\n1, ..., p\nl\nn, \u03c0\nl\nn)\nfor every profile (p1, \u03c01, ..., pn, \u03c0n) \u2208 \u03a0\n2n and (likelihood) function l : H\u2192 (0, 1].\nOn the left hand side of this equation, i\u2019s prior is not updated (pi, not pli), be-\ncause the incoming information, being observed just by person i, is not part of the\nshared information, hence not reflected in any prior. Internal Bayesianity is based on\nthe idea that the collective probabilities should incorporate all information available\nsomewhere in the group, whether it is held by a single or every person. External and\ninternal Bayesianity together imply that, for each person i,\nG(p1, \u03c01, ..., pi\u22121, \u03c0i\u22121, pi, \u03c0\nl\ni, pi+1, \u03c0i+1, ..., pn, \u03c0n) = G(p1, \u03c01, ..., pn, \u03c0n)\nl\nfor every profile (p1, \u03c01, ..., pn, \u03c0n) \u2208 \u03a02n and (likelihood) function l : H\u2192 (0, 1].\nIt turns out that, if a GOP G takes the form (9), then external and internal\nBayesianity are in fact equivalent, and equivalent to external Bayesianity of F :\nTheorem 2 If a generalised opinion pool G : \u03a02n \u2192 \u03a0 has the form (9) where\nF : \u03a0n \u2192 \u03a0 is any opinion pool, the following conditions are equivalent:\n(i) G is externally Bayesian;\n(ii) G is internally Bayesian;\n(iii) F is externally Bayesian.\nSo, if one desires G to be externally or internally Bayesian, one is bound to use an\nexternally Bayesian opinion pool F in our pooling formula (9), for instance a geometric\nopinion pool F , which leads to pooling formula (6), hence to (8) in the equal-weight\ncase. There also exist more complex (non-geometric) externally Bayesian opinion\npools F, characterised in full generality by Genest, McConway, and Schervish (1986,\nTheorem 2.5), but geometric ones become the only solutions if |H| \u2265 3 and F has\nsome additional properties (see Genest, McConway, and Schervish (1986), Corollary\n4.5).\nProof. I show that (i) is equivalent with each of (ii) and (iii). By (9),\nG(pl1, \u03c0\nl\n1, ..., p\nl\nn, \u03c0\nl\nn) \u221d\n\u03c0l1\npl1\n\u00b7 \u00b7 \u00b7\n\u03c0ln\npln\nF (pl1, ..., p\nl\nn),\nand hence by (10)\nG(pl1, \u03c0\nl\n1, ..., p\nl\nn, \u03c0\nl\nn) \u221d\nl\u03c01\nlp1\n\u00b7 \u00b7 \u00b7\nl\u03c0n\nlpn\nF (pl1, ..., p\nl\nn) =\n\u03c01\np1\n\u00b7 \u00b7 \u00b7\n\u03c0n\npn\nF (pl1, ..., p\nl\nn). (11)\nOn the other hand, again by (9) and (10),\nG(p1, \u03c01, ..., pn, \u03c0n)\nl \u221d l\n\u03c01\np1\n\u00b7 \u00b7 \u00b7\n\u03c0n\npn\nF (p1, ..., pn) \u221d\n\u03c01\np1\n\u00b7 \u00b7 \u00b7\n\u03c0n\npn\nF (p1, ..., pn)\nl. (12)\n17\nRelations (11) and (12) together immediately imply that G is externally Bayesian if\nand only if F is externally Bayesian. Further, again by (9) and (10),\nG(p1, \u03c01, ..., pi\u22121, \u03c0i\u22121, pi, \u03c0\nl\ni, pi+1, \u03c0i+1, ..., pn, \u03c0n) \u221d l\n\u03c01\np1\n\u00b7 \u00b7 \u00b7\n\u03c0n\npn\nF (p1, ..., pn)\n\u221d\n\u03c01\np1\n\u00b7 \u00b7 \u00b7\n\u03c0n\npn\nF (p1, ..., pn)\nl.\nThis together with (11) implies that G is internally Bayesian if and only it F is\nexternally Bayesian. \u0001\n7 When is information independent, when not?\nLet us go back to Theorem 1\u2019s assumption of Independent Information (Ind). This\nassumption is often a useful idealisation, even in situations where it fails. But what\nexactly are these real situations where (Ind) fails? An important source for failure is\nwhat I call subgroup information, that is, information held by more than one but less\nthan all persons. I will prove that, under certain conditions, (Ind) holds if and only\nif there is no subgroup information.\nBy a person i\u2019s observation set I mean, informally, the (possibly quite enormous)\ncollection of i\u2019s relevant observations\/items of information. Formally, one may define\ni\u2019s observation set as a set Oi of non-empty observations O \u2286 \u2126.\n16 In the case of\na jury faced with hypotheses about the defendant\u2019s guilt, i\u2019s observation set might\ninclude the observations \u2018an insecure smile on the defendant\u2019s face\u2019, \u2018the defendant\u2019s\nfingerprint near the crime scene\u2019, \u2018two contradictory statements by witness x\u2019, etc.\nobservations of\nperson 1 only\nobservations of\nperson 2 only\nshared\nobservations\nobservations of\nperson 1 only observations of\nperson 2 only\nshared\nobservations\nobservations of\nperson 3 only\n! !\nFigure 1: Observation sets in a group of n = 2 perons (no subgroup information),\nand a group of n = 3 persons (with subgroup information marked by \"!\")\nFigure 1 shows observation sets, not sets of possible worlds A \u2286 \u2126. These two\nconcepts are in fact opposed to each other: the larger the observation set, the smal-\nler the corresponding set of worlds (in which the observations hold); the union of\n16An observation made by every person is represented by the sure event O = \u2126, because \u2126 is\ninterpreted as containing the worlds that are possible under shared information. Formally, O \u2208\nO1 \u2229 ... \u2229On implies O = \u2126.\n18\nobservation sets compares to the intersection of the sets of worlds. Formally, to an\nobservation set O corresponds the set of worlds \u2229O\u2208OO \u2286 \u2126 (interpreted as \u2126 if\nO = \u2205). Thus i\u2019s information Ei equals\nEi =\n\u22c2\nO\u2208Oi\\(O1\u2229...\u2229On)\nO,\nthe intersection of all of i\u2019s observations except from any shared one; by footnote 16,\nthis actually reduces to\nEi =\n\u22c2\nO\u2208Oi\nO.\nHere is the problem. Consider any observation contained in the observation sets\nof more than one but less than all persons i \u2014 something impossible in groups of size\nn = 2 but possible in larger groups, as illustrated by the \u2018!\u2019 fields in Figure 1. This\nobservation is not part of the shared information, but of the personal information Ei of\nmany individuals i. Such subgroup information typically creates positive correlations\nbetween the Eis in question. As a stylised example, consider a jury of n = 3 jurors\nfaced with the hypothesis of guilt of the defendant (H). All jurors have read the charge\n(shared information), and moreover juror 1 has listened to the first witness report and\nobserved the defendant\u2019s nervousness (E1), juror 2 has listened to the second witness\nreport and observed the defendant\u2019s smiles (E2), and juror 3 has listened to both\nwitness reports and had a private chat with the defendant (E3). Note the subgroup\ninformation of jurors 1 and 3, and that of jurors 2 and 3, which typically causes E3\nto be positively correlated with E1 and with E2. By contrast, individuals 1 and 2\ntogether have no subgroup information. This situation is depicted in Figure 1 on the\nright.\nTo formally clarify the relationship between subgroup information and independ-\nence violation, some preparation is needed.\nDefinition 4 A subgroup is a non-empty subset M of the group N := {1, ..., n}. A\nsubgroup is proper if it contains more than one but less than all persons.\nTo formalise the notion of subgroup information, suppose that to each subgroupM\nthere is a non-empty event EM \u2286 \u2126,M \u2019s exclusively shared information, representing\nall information held by each of and only the persons in M , where by assumption:\n\u2022 Ei =\n\u22c2\n{i}\u2286M\u2286N E\nM for all persons i (as i has observed thoseEM with i \u2208M);17\n\u2022 EN = \u2126 (as any world \u03c9 \u2208 \u2126 is assumed possible under the shared information);\n17Why not rather assume that Ei =\n\u22c2\n{i}\u2286M\u0003N E\nM , as Ei should not contain information held by\neverybody? In fact, both assumption are equivalent since by EN = \u2126 an additional intersection with\nEN has no effect.\n19\n\u2022 each EM belongs to A, the domain of the probability measure P (which holds\nin particular if A contains all subsets of \u2126).\nFor instance, the \u2018!\u2019 fields in Figure 1 (right) represent the observation sets cor-\nresponding to E{1,3} and E{2,3}. EM is interpretable as the intersection\u22c2\nO\u2208(\u2229i\u2208MOi)\\(\u222ai\/\u2208MOi)\nO\nof all observations O contained in each of the observation sets Oi, i \u2208M, but in none\nof the observation setsOi, i \/\u2208M (where this intersection is\u2126 if (\u2229i\u2208MOi)\\(\u222ai\/\u2208MOi) =\n\u2205).\nWhat we have to exclude is that a proper subgroup M exclusively shares inform-\nation; in other words, EM must be the no-information event \u2126:\nNo Subgroup Information (NoSI) Every proper subgroup M has no exclusively\nshared information, i.e. EM = \u2126 (or, more generally, P (EM) = 118).\nThis condition is empty if there are just n = 2 individuals, it requires E{1,2} =\nE{1,3} = E{2,3} = \u2126 if n = 3, and it requires the \u2018!\u2019 fields in Figure 1 to be empty.\nFinally, consider the following independence assumption:\n(Ind\u2217) The events EM , \u2205 \u000e= M \u2286 N, are (P -)independent conditional on each\nH \u2208 H.\n(Ind\u2217) is a more generally acceptable condition than (Ind) in that the EMs, unlike\nthe Eis, are based on non-overlapping observation sets. Indeed, a subgroup M \u2019s\nexclusively shared information EM , by the very meaning of \u2018exclusively\u2019, represents\ndifferent observations than any other subgroup\u2019s exclusively shared information.19\nTheorem 3 Assume (Ind\u2217). Then:\n(a) Independent Information (Ind) is equivalent to No Subgroup Information (NoSI);\n(b) specifically, if EM \u000e= \u2126 for proper subgroup M , then conditional on at least one\nH \u2208 H the personal observations Ei, i \u2208 M, are pairwise positively correlated\n(i.e. P (Ei \u2229Ej|H) > P (Ei|H)P (Ej|H) for any two distinct i, j \u2208M).\n18 \u2018P (EM) = 1\u2019 is equivalent to \u2018EM = \u2126\u2019 in the natural case that only the empty event in A\nhas zero probability. Strictly speaking, \u2018EM = \u2126\u2019 means \u2018no information\u2019 while \u2018P (EM) = 1\u2019 means\n\u2018essentially no information\u2019. I am grateful to the referee for suggesting to require \u2018P (EM) = 1\u2019\ninstead of \u2018EM = \u2126\u2019, thereby making it possible to state Theorem 3 without assuming that only the\nempty event in A has zero probability.\n19 (Ind\u2217) holds if the observations in O1 \u222a ... \u222aOn are mutually (conditionally) independent.\n20\nProof. Suppose (Ind*). I prove part (a); the proof includes a proof of part (b).\n(i) First, assume (NoSI). Each event E{i} coincides with Ei up to a set of prob-\nability zero, because\nEi =\n\u22c2\n{i}\u2286M\u2286N\nEM = E{i}\n\u22c2\uf8eb\uf8ed \u22c2\n{i}\u2286M\u2286N&|M|\u22652\nEM\n\uf8f6\uf8f8 ,\nin which P\n(\u22c2\n{i}\u2286M\u2286N&|M |\u22652E\nM\n)\n= 1 by (NoSI). So, as the events E{1}, ..., E{n}\nare independent conditional on any H \u2208 H by (Ind\u2217), also the events E1, ..., En are\nindependent conditional on any H \u2208 H.\n(ii) Now assume (NoSI) is violated, and let M\u2217 be a proper subgroup with\nP (EM\n\u2217\n) < 1. I show that the events Ei, i \u2208 M\n\u2217, are pairwise positively correl-\nated conditional on at least one H \u2208 H (which proves part (b) and also completes\nthe proof of part (a) since E1, ..., En are then not independent conditional on that\nH). Consider any distinct i, j \u2208 M\u2217. By P (EM\n\u2217\n) < 1 there exists an H \u2208 H with\nP (EM\n\u2217\n|H) < 1. Since Ei =\n\u22c2\n{i}\u2286M\u2286N E\nM and using (Ind\u2217), we have\nP (Ei|H) =\n\u220f\n{i}\u2286M\u2286N\nP (EM |H).\nThe analogous argument for j yields\nP (Ej|H) =\n\u220f\n{j}\u2286M\u2286N\nP (EM |H).\nSo,\nP (Ei|H)P (Ej|H) =\n\uf8ee\uf8f0 \u220f\n{i}\u2286M\u2286N\nP (EM |H)\n\uf8f9\uf8fb\u00d7\n\uf8ee\uf8f0 \u220f\n{j}\u2286M\u2286N\nP (EM |H)\n\uf8f9\uf8fb . (13)\nFurther, we have\nEi \u2229Ej =\n\uf8ee\uf8f0 \u22c2\n{i}\u2286M\u2286N\nEM\n\uf8f9\uf8fb\u22c2\uf8ee\uf8f0 \u22c2\n{j}\u2286M\u2286N\nEM\n\uf8f9\uf8fb\n=\n\uf8ee\uf8f0 \u22c2\n{i}\u2286M\u2286N\nEM\n\uf8f9\uf8fb\u22c2\uf8ee\uf8f0 \u22c2\n{j}\u2286M\u2286N\\{i}\nEM\n\uf8f9\uf8fb .\nSo, by (Ind\u2217),\nP (Ei \u2229Ej |H) =\n\uf8ee\uf8f0 \u220f\n{i}\u2286M\u2286N\nP (EM |H)\n\uf8f9\uf8fb\u00d7\n\uf8ee\uf8f0 \u220f\n{j}\u2286M\u2286N\\{i}\nP (EM |H)\n\uf8f9\uf8fb . (14)\n21\nThe relations (13) and (14) together entail\nP (Ei \u2229Ej|H) > P (Ei|H)P (Ej|H),\nbecause expression (13) equals expression (14) multiplied with the factor\u220f\n{i,j}\u2286M\u2286N\nP (EM |H),\nwhich is smaller than 1 since it contains the term P (EM\n\u2217\n|H) < 1. \u0001\n8 Opinion pooling in the presence of subgroup informa-\ntion\nOne may always try to \u2018remove\u2019 subgroup information through active information\nsharing prior to aggregation: all proper subgroups with exclusively shared information\ncommunicate this information to the rest of the group. In Figure 1, the observations in\neach \u2018!\u2019 field are communicated to the third person, and in the above jury example the\nsubgroups {1, 3} and {2, 3} communicate the exact content of the first resp. second\nwitness report to the third juror. Having thus removed any subgroup information,\n(NoSI) and hence (in view of Theorem 3) Independent Information (Ind) hold, so\nthat opinion pooling can proceed along the lines of Sections 2-5.\nBut suppose now that such information sharing is not feasible, e.g. due to the\ncomplexity of subgroup information. Then (NoSI) fails, and hence (Ind) fails, so\nthat we need to modify our pooling formula. It is at first not obvious whether and\nhow one can generalise Theorem 1 to arbitrary information overlaps, i.e. whether\nand how collective opinions can incorporate all information spread around the group.\nThe generalisation is possible, as will be seen. Roughly speaking, we have to replace\nTheorem 1\u2019s axioms of Individual Bayesian Rationality (IBR) and Independent In-\nformation (Ind) by corresponding axioms based on subgroups rather than individuals.\nTheorem 1\u2019s two other axioms, (APLA) and (CP), will not anymore appear explicitly,\nbut are build implicitly into the model, as explained in a moment. The adapted ax-\nioms will again lead to a unique collective opinion \u03c0, calculated in a somewhat more\ncomplicated way than in Theorem 1.\nFirst, let me state the new model ingredients, and compare them to the earlier\ningredients. As before, we have a non-empty set of possible worlds \u2126, partitioned into\na countable set H of non-empty hypotheses H. While Theorem 1\u2019s model contained\nfor every individual i a personal information Ei \u2286 \u2126, now for every subgroupM there\nis a non-empty event EM \u2286 \u2126, M \u2019s exclusively shared information, representing all\ninformation held by each of and only the persons in M . By assumption, EN = \u2126,\nreflecting that any world \u03c9 \u2208 \u2126 is possible under the shared information. From these\n22\nevents EM we can define each individual i\u2019s information as\nEi =\n\u22c2\n{i}\u2286M\u2286N\nEM ,\nrepresenting all information held at least by person i.\nThe earlier model contained every individual i\u2019s (prior) belief Pi; this is not any-\nmore needed here. Instead, I only assume a single probability measure P , defined\non some \u03c3-algebra A \u2286 P(\u2126) containing each EM and each hypothesis H \u2208 H. We\ninterpret P as capturing common prior beliefs.20 This assumption of common prior\nbeliefs is a simplification; it for instance implies that conditions such as (CP) and\n(APLA) above are built into the model, and hence will not have to appear explicitly.\nRecall further that in Theorem 1\u2019s model (in its common prior version) people\nprovide individual opinions \u03c01, ..., \u03c0n (reflecting \u2018individually shared\u2019 information) and\na common prior opinion p (reflecting the group\u2019s shared information). So, technically,\nthe earlier model contained the opinions \u03c01, ..., \u03c0n, p reflecting the shared information\nof the improper subgroups {1}, ..., {n}, N , respectively. Our new model adds to this\nthe opinions reflecting the shared information of certain proper subgroups M \u2286 N .\nMore precisely, in the new model at least those (proper or improper) subgroup which\nexclusively share information will need to provide an opinion. Formally, let M be a\nset of subgroups, containing at least those (proper or improper) subgroups M \u2286 N\nwith exclusively shared information, i.e. with EM \u000e= \u2126. Without loss of generality,\nlet N \u2208M.21 Each subgroup M inM submits an opinion \u03c0M \u2208 \u03a0, representing M \u2019s\nprobability assignments based on M \u2019s shared information (shared information need\nnot be exclusively shared, i.e. may be known to other persons too; see Definition\n5 below). Theorem 1\u2019s model (in the common prior version) is the special case\nthat M = {{1}, ..., {n}, N} (= {M : M is an improper subgroup}) with \u03c0{1} =\n\u03c01, ..., \u03c0{n} = \u03c0n, \u03c0N = p. In the last section\u2019s jury example with n = 3 individuals,\nwe may put\nM = {{1}, {2}, {3}, {1, 3}, {2, 3}, {1, 2, 3}}\nbecause {1, 2} has no exclusively shared information.\nIn practice, in addition to every individual i with {i} \u2208 M submitting an opinion\n\u03c0{i}, every non-singleton subgroup M \u2208M will have to \u2018sit together\u2019, find out about\nthe information it shares, and come up with an opinion \u03c0M based on this shared\ninformation.\nThe technique to calculate the (collective) opinion \u03c0 \u2208 \u03a0 from the submitted\nsubgroup opinions \u03c0M , M \u2208M, is recursive. Let me first illustrate it by an example.\n20More precisely, I do not mean to assume that every individual i holds a belief on all events in\nA. Rather i holds beliefs (at least) on a sub-\u03c3-algebra of A containing all hypotheses in H and those\nevents EM for which i \u2208M . i\u2019s beliefs on this sub-\u03c3-algebra are given by P .\n21One may always define M as containing all subgroups, but in practice this maximal choice\nadds unnecessary steps to the recursive pooling procedure introduced below. The minimal choice is\nM = {M : \u2205 \u000e=M \u0003 N and EM \u000e= \u2126} \u222a {N}.\n23\nExample. As in the last section\u2019s jury example, let there be n = 3 individuals and\nletM = {{1}, {2}, {3}, {1, 3}, {2, 3}, {1, 2, 3}}. So, functions \u03c0{1}, \u03c0{2}, \u03c0{3}, \u03c0{1,3}, \u03c0{2,3}\nand \u03c0{1,2,3} are submitted. The recursion works as follows, where I use a slightly sim-\nplified version of the later notation and give only informal justifications.\n\u2022 First, merge \u03c0{1,3} and \u03c0{2,3} into a function \u03c0{1,3},{2,3} that combines {1, 3}\u2019s\nshared information and {2, 3}\u2019s shared information. One may apply Corollary\n1\u2019s formula:\n\u03c0{1,3},{2,3} \u221d \u03c0{1,3}\u03c0{2,3}\/\u03c0{1,2,3}.\n(To see why \u03c0{1,2,3} can play the role of the prior opinion p in Corollary 1, recall\nthat p there represents the information shared by all individual opinions. The\ninformation shared by the opinions \u03c0{1,3} and \u03c0{2,3} is the information held by\n[1 and 3] and by [2 and 3]. This is equivalent to the information held by 1 and\n2 and 3, i.e. the information expressed in \u03c0{1,2,3}.)\n\u2022 Next, define \u03c0{1,2} as \u03c0{1,2,3}, because the subgroup {1, 2} does not exclusively\nshare any information and hence shares the same information as the larger\ngroup {1, 2, 3}.\n\u2022 Next, merge \u03c0{1} and \u03c0{2} into a function \u03c0{1},{2} that combines {1}\u2019s and {2}\u2019s\ninformation. One may apply Corollary 1\u2019s formula:\n\u03c0{1},{2} \u221d \u03c0{1}\u03c0{2}\/\u03c0{1,2}.\n(Why can \u03c0{1,2} play the role of p in Corollary 1, i.e. why does \u03c0{1,2} express\nthe information shared by \u03c0{1} and \u03c0{2}? The information shared by \u03c0{1} and\n\u03c0{3} is the information held by 1 and by 2, i.e. the information expressed in\n\u03c0{1,2}.)\n\u2022 Finally, merge \u03c0{1},{2} and \u03c0{3} into the function \u03c0 = \u03c0{1},{2},{3} that combines\n{1}\u2019s, {2}\u2019s and {3}\u2019s information. Again, one may apply Corollary 1\u2019s formula:\n\u03c0 = \u03c0{1},{2},{3} \u221d \u03c0{1},{2}\u03c0{3}\/\u03c0{1,3},{2,3}.\n(Why can \u03c0{1,3},{2,3} play the role of p in Corollary 1, i.e. why does \u03c0{1,3},{2,3}\nrepresent the information shared by \u03c0{1},{2} and \u03c0{3}? The information shared\nby \u03c0{1},{2} and \u03c0{3} is the information held by [1 or 2] and by 3. This is precisely\nthe information held by [1 and 3] or by [2 and 3], i.e. the information expressed\nin \u03c0{1,3},{2,3}.)\nNow I come to the formal treatment. Recall that i\u2019s information Ei is given by\nEi =\n\u22c2\n{i}\u2286M\u2286N\nEM ,\ni.e. i knows precisely the conjunction of what the subgroups containing i exclusively\nshare. This generalises as follows to:\n24\nDefinition 5 A subgroup M \u2019s shared information is defined as\nEM :=\n\u22c2\nM\u2286M \u2032\u2286N\nEM\n\u2032\n(the conjunction of all information exclusively shared by some supergroup of M).\nEM represents what is known to at least all members of M \u2014 as opposed to M \u2019s\nexclusively shared information EM , known exactly all members of M . Taking the\ncase of a singleton subgroup M = {i}, the event E{i} coincides with Ei. Also, note\nthat\nP (EM) > 0 and P (EM) > 0 for each subgroup M\nbecause\nP (EM), P (EM) \u2265 P\n\uf8eb\uf8ed \u22c2\n\u2205 \u000f=M \u2032\u2286N\nEM\n\u2032\n\uf8f6\uf8f8 = P (E1 \u2229 ... \u2229En) > 0.\nThe following condition translates Individual Bayesian Rationality (IBR) to sub-\ngroups in M:\nSubgroup Bayesian Rationality (SBR) \u03c0M(H) = P (H|EM) for every subgroup\nM \u2208M and hypothesis H \u2208 H.\nAs in Theorem 1, we would like the collective opinion to satisfy Collective Bayesian\nRationality (CBR); that is, we require that\n\u03c0(H) = P (H|E1 \u2229 ... \u2229En) for each hypothesis H \u2208 H,\na condition that may be rewritten in several equivalent ways since (by Definition 5)\nE1 \u2229 ... \u2229En = E{1} \u2229 ... \u2229E{n} =\n\u22c2\n\u2205 \u000f=M\u2286N\nEM =\n\u22c2\n\u2205 \u000f=M\u2286N\nEM .\nAs a technical tool to construct collective opinion \u03c0 satisfying (CBR), I need to\nintroduce opinions of abstract individuals.\nDefinition 6 An abstract individual is a non-empty set A of subgroups M ; its order\nis order(A) := min{|M | :M \u2208 A}, the size of a smallest subgroup in A.\nThe opinions \u03c0{1,3},{2,3}, \u03c0{1},{2}, ... defined in the example above are in fact the\nopinions of the abstract individuals {{1, 3}, {2, 3}}, {{1}, {2}}, ... More generally, I\ninterpret an abstract individual A as a hypothetical agent who knows the shared\ninformation of any subgroupM \u2208 A (and no more). For instance, A = {{1, 3}, {2, 3}}\nknows {1, 3}\u2019s shared information and {2, 3}\u2019s shared information. A\u2019s information\nis thus given by\n\u22c2\nM\u2208AEM . To get a concrete idea, note that the abstract agent\nA = {{1, 3}, {2, 3}} knows\n25\n\u2022 at least as much as the abstract agent {{1, 3}}, who knows all information that\n1 and 3 share (but no information that 2 and 3 share exclusively);\n\u2022 at least as much as the abstract agent {{1, 2, 3}}, who knows all information\nthat 1, 2 and 3 share (but no information that 1 and 3 share exclusively or that\n2 and 3 share exclusively);\n\u2022 at most as much as the abstract agent {{1}, {2}, {3}}, who knows all that 1 or 2\nor 3 knows (and hence all that two or three of these individuals know together).\nI will calculate for each abstract individual A an opinion \u03c0A \u2208 \u03a0 that reflects\nprecisely A\u2019s information \u2229M\u2208AEM , i.e. that satisfies\n\u03c0A(H) = P\n(\nH\n\u2223\u2223\u2223\u2223\u2223 \u22c2\nM\u2208A\nEM\n)\nfor each H \u2208 H. (15)\nSpecifically, I calculate \u03c0A by backward recursion over order(A): \u03c0A is calculated\nfirst for order(A) = n, then for order(A) = n \u2212 1, ..., then for order(A) = 1. This\nfinally yields \u03c0, since by (CBR) and (15)\n\u03c0 = P (.|E{1} \u2229 ... \u2229E{n}) = \u03c0A\nwhere A is the abstract individual {{1}, {2}, ..., {n}} of order 1. In the recursive\nconstruction, the main steps are to calculate from opinions \u03c0A and \u03c0A\u2217 of abstract\nindividuals A and A\u2217 the opinion \u03c0A\u222aA\u2217 of the abstract individual A \u222a A\n\u2217 whose\ninformation combines the information of A and A\u2217. To derive \u03c0A\u222aA\u2217 from \u03c0A and\n\u03c0A\u2217 , I generalise the formula of Theorem 1 to (two) abstract individuals. To do so,\nthe notion of shared information is crucial. What information do A and A\u2217 share?\nThey share precisely the information held by the abstract individual\nA \u2228A\u2217 := {M \u222aM\u2217 :M \u2208 A and M\u2217 \u2208 A\u2217}.\nThe reason is: the information A and A\u2217 share is precisely the information that A\nknows and A\u2217 knows, i.e. that some subgroup in A shares and some subgroup in A\u2217\nshares, i.e. that some union M \u222aM\u2217 with M \u2208 A and M\u2217 \u2208 A\u2217 shares. So, when\ncombining opinions \u03c0A and \u03c0A\u2217 , A\u2228A\n\u2217\u2019s opinion \u03c0A\u2228A\u2217 plays the role of the common\nprior p in Theorem 1. More precisely, the crucial result on how to combine opinions\nof abstract individuals states as follows (and is proved later):\nLemma 1 Assume (Ind\u2217). Consider abstract individuals B and C, form the abstract\nindividuals B \u2228C and B \u222aC. If \u03c0B, \u03c0C , \u03c0B\u2228C are opinions in \u03a0 given by (15), then\n\u2022 there is an opinion in \u03a0 proportional to \u03c0B\u03c0C\/\u03c0B\u2228C,\n22\n22Equivalently, the sum\n\u2211\nH\u2208H\n\u03c0B(H)\u03c0C(H)\/\u03c0B\u2228C(H) is finite. Indeed, a function f from H to\n(0,\u221e) (such as \u03c0B\u03c0C\/\u03c0B\u2228C) can be normalised to a function with sum 1 if and only if f has a finite\nsum.\n26\n\u2022 this opinion is the function \u03c0B\u222aC given by (15).\nThe formula in Lemma 1 guides us in assigning opinions to abstract individuals.\nThe assignment is recursive, with another nested recursion in \u2018Case 2\u2019:\nDefinition 7 Define the opinions \u03c0A \u2208 \u03a0 of abstract individual A by the following\nbackward recursion on order(A):\n\u2022 Assume order(A) = n. Then A = {N}. Define \u03c0A := \u03c0N .\n\u2022 Assume order(A) = k < n and assume \u03c0A\u2032 is already defined for order(A\n\u2032) > k.\nCase 1: |A| = 1. Then A = {M}. If M \u2208 M, define \u03c0A = \u03c0M . If M \/\u2208 M,\nconsider the abstract individual A\u2032 := {M\u222a{i} : i \/\u2208M} containing all subgroups\nwith exactly one person added to M (interpretation: A and A\u2032 have the same\ninformation by M \/\u2208M) and define \u03c0A := \u03c0A\u2032 (where \u03c0A\u2032 is already defined by\norder(A\u2032) = k + 1).\nCase 2: |A| > 1. Define \u03c0A by another recursion on |{M \u2208 A : |M | = k}|, the\nnumber of subgroups in A of size k:\n\u25e6 Assume |{M \u2208 A : |M | = k}| = 1. Then A = {M} \u222a A\u2217, where |M | = k\nand order(A\u2217) > k. Define \u03c0A by \u03c0A \u221d \u03c0{M}\u03c0A\u2217\/\u03c0{M}\u2228A\u2217 (where \u03c0{M}\nis already defined in case 1, and \u03c0A\u2217 and \u03c0{M}\u2228A\u2217 are already defined by\norder(A\u2217) > k and order({M} \u2228A\u2217) > k).\n\u25e6 Assume |{M \u2208 A : |M | = k}| = l > 1 and assume \u03c0A\u2217 is already defined\nfor all the A\u2217 such that |{M \u2208 A\u2217 : |M | = k}| < l (and order(A\u2217) = k).\nThen A = {M} \u222a A\u2217 with |M | = k and |{M\u2217 \u2208 A\u2217 : |M\u2217| = k}| = l \u2212 1.\nDefine \u03c0A by \u03c0A \u221d \u03c0{M}\u03c0A\u2217\/\u03c0{M}\u2228A\u2217 (where \u03c0{M} is already defined in\ncase 1, \u03c0A\u2217 is already defined by |{M\n\u2217 \u2208 A\u2217 : |M\u2217| = k}| = l \u2212 1, and\n\u03c0{M}\u2228A\u2217 is already defined by order({M} \u2228A\n\u2217) > k).\nOn the last recursion step we reach the opinions \u03c0A of abstract individuals of\norder 1, hence in particular the opinion of A = {{1}, ..., {n}}, and this is the desired\nopinion that incorporates the group\u2019s full information:\nTheorem 4 If subgroups satisfy (SBR), information satisfies (Ind\u2217), and the collect-\nive satisfies (CBR), then the collective opinion \u03c0 is given by \u03c0{{1},...,{n}}, the (recurs-\nively calculated) opinion of the abstract individual {{1}, ..., {n}}.\nThe procedure needed to obtain the collective opinion \u03c0 (= \u03c0{{1},...,{n}}) may\nhave a high complexity.23 How practically feasible is it? One should distinguish two\nseparate tasks: (i) first, each subgroup M in M has to form and submit an opinion\n23 I am grateful to the referee for drawing my attention to this point.\n27\n\u03c0M ; (ii) subsequently, the collective opinion \u03c0 has to be derived algorithmically from\nthe various subgroup opinions. Let me comment on each task.\nWhether task (i) is feasible in practice depends crucially on the number and size\nof subgroups in M, which in turn depends on how information is distributed across\npeople. In the worst case, every subgroup exclusively shares information. Here,M =\nP(N)\\{\u2205} and |M| = 2n\u22121, and the task becomes infeasible already for moderately\nlarge n. On the other hand, the task seems more feasible in situations where only\nrelatively few subgroups exclusively share information. Suppose for instance that,\nwhen pooling expert opinions relative to certain hypotheses about climate change,\nonly the following subgroups exclusively share information: each single expert, i.e.\neach singleton subgroup {i} \u2286 N ; a group of physicists M1 \u2286 N ; a group of biologists\nM2 \u2286 N ; and a group of meteorologists M3 \u2286 N . Then we may define M as\n{{1}, {2}, ..., {n},M1,M2,M3,N}, so that only |M| = n + 4 opinions have to be\nformed and submitted.\nTask (ii) involves an algorithm with a nested recursion; the overall number of steps\ngrows more than exponentially in n.24 So, for large n, task (ii) poses a feasibility\nproblem \u2014 even if |M| is small, i.e., if task (i) seems feasible. There is however\nan escape to this problem if M contains only relatively small proper subgroups. Let\nm := maxM\u2208M\\{N} |M | denote the maximal size that subgroups inM can have (apart\nfrom the improper subgroup N). A quick inspection of the algorithm in Definition 7\nshows that its backward recursion (which assigns opinions \u03c0A to abstract agents A) is\ntrivial until it reaches abstract agents of order m: all abstract agents of order k > m\nget assigned the opinion \u03c0A = \u03c0N . So a shortcut is possible: define \u03c0A as \u03c0N for all\nabstract agents A with order(A) > m, and start the backward recursion with those\nabstract agents A with order(A) = m.\nEven if both tasks (i) and (ii) turn out to be practically feasible, the very choice\nof M (before starting task (i)) may pose another high-complexity problem. Suppose\nM is chosen by surveying all subgroups one by one to find out which ones exclus-\nively share information (each subgroup might be asked to \u2018sit together\u2019 and look for\npotential information overlaps). Since there are 2n\u2212 1 subgroups in total, this would\nbecome infeasible already for moderately large n. However, no such problem arises\nif M can be specified without performing an explicit subgroup-by-subgroup examin-\nation. For instance, M might be specified by a social planner who knows from the\nstart that certain subgroups (say, those containing experts from different fields) do\nnot exclusively share any information, while the other subgroups might exclusively\nshare information.\nTurning now to the proof, I first show Lemma 1 and then Theorem 4.\nProof of Lemma 1. Assume (Ind\u2217). Let B,C be abstract individuals, and \u03c0B, \u03c0C ,\n24 In the algorithm, for each abstract agent A an opinion \u03c0A is calculated. There are 2\n2n\u22121 \u2212 1\nabstract agents in total. Hence, 22\nn\u22121 \u2212 1 opinions have to be calculated.\n28\n\u03c0B\u2228C , \u03c0B\u222aC \u2208 \u03a0. Suppose \u03c0B, \u03c0C , \u03c0B\u2228C satisfy (15). For all abstract individuals A,\nput\nA := {M \u2286 N :M \u2032 \u2286M for some M \u2032 \u2208 A},\nthe set of supergroups of subgroups in A. By (15), \u03c0B\u2228C = P (.|\n\u22c2\nM\u2208B\u2228C EM), where\nby Definition 5 \u22c2\nM\u2208B\u2228C\nEM =\n\u22c2\nM\u2208B\u2228C\n\u22c2\nM\u2286M \u2032\u2286N\nEM\n\u2032\n=\n\u22c2\nM\u2208B\u2228C\nEM .\nSo,\n\u03c0B\u2228C = P (.|E) with E :=\n\u22c2\nM\u2208B\u2228C\nEM . (16)\nAnalogously, by (15), \u03c0B = P (.|\n\u22c2\nM\u2208B EM), where by Definition 5\u22c2\nM\u2208B\nEM =\n\u22c2\nM\u2208B\n\u22c2\nM\u2286M \u2032\u2286N\nEM\n\u2032\n=\n\u22c2\nM\u2208B\nEM = EB \u2229E\nwith EB :=\n\u22c2\nM\u2208B\\B\u2228C E\nM . So \u03c0B = P (.|EB \u2229E), and hence by Bayes\u2019 rule\n\u03c0B \u221d P (.|E)P (EB|. \u2229E). (17)\nBy an analogous argument for C, we have\n\u03c0C \u221d P (.|E)P (EC |. \u2229E), (18)\nwhere EC :=\n\u22c2\nM\u2208C\\B\u2228C E\nM . By (16), (17) and (18) we have\n\u03c0B\u03c0C\/\u03c0B\u2228C \u221d [P (.|E)P (EB|. \u2229E)] [P (.|E)P (EC |. \u2229E)] \/P (.|E)\n= P (.|E)P (EB|. \u2229E)P (EC |. \u2229E). (19)\nNote that each of EB, EC , E is an intersection of a set of events of type E\nM , where the\nthree sets of EMs (corresponding to EB, EC , E, respectively) are pairwise disjoint.\nSo, as by (Ind*) all EMs are independent conditional on any H \u2208 H, so are the events\nEB, EC , E. Consider an H \u2208 H. As EB, EC , E are independent given H, the events\nEB, EC are independent given H \u2229E. So\nP (EB|. \u2229E)P (EC |. \u2229E) = P (EB \u2229EC |. \u2229E).\nSubstituting this into (19) and then applying Bayes\u2019 rule, we obtain\n\u03c0B\u03c0C\/\u03c0B\u2228C \u221d P (.|E)P (EB \u2229EC |. \u2229E) \u221d P (.|EB \u2229EC \u2229E) \u2208 \u03a0.\nNow suppose \u03c0B\u222aC = P (.|EB \u2229EC \u2229E). We may rewrite EB \u2229EC \u2229E as\u22c2\nM\u2208B\u222aC\nEM =\n\u22c2\nM\u2208B\u222aC\n\u22c2\nM\u2286M \u2032\u2286N\nEM =\n\u22c2\nM\u2208B\u222aC\nEM ,\n29\nand hence \u03c0B\u222aC equals P (.|\n\u22c2\nM\u2208B\u222aC EM), i.e. satisfies (15). \u0001\nProof of Theorem 4. Assume (SBR) and (Ind\u2217). By backward induction on the\norder of A I show that each abstract individual A has opinion \u03c0A satisfying (15). This\nin particular implies that {{1}, ..., {n}} has opinion\n\u03c0{{1},...,{n}}(H) = P (H|E1 \u2229 ... \u2229En) for each H \u2208 H,\nso that under (CBR) we have \u03c0 = \u03c0{{1},...,{n}}, as desired.\nDenote by A the set of abstract individuals A. The recursion proceeds as follows.\n\u2022 If order(A) = n, then A = {N}, and by definition \u03c0A = \u03c0N . So by (SBR)\n\u03c0A = P (.|EN) = P (.|\n\u22c2\nM\u2208AEM), as desired.\n\u2022 Now let order(A) = k < n, and assume (15) holds for all A\u2032 \u2208 A with\norder(A\u2032) > k. I have to show that \u03c0A = P (.|\n\u22c2\nM\u2208AEM).\nCase 1: |A| = 1. Then A = {M} with |M | = k. If M \u2208 M, then by definition\n\u03c0A = \u03c0M , so by (SBR) \u03c0A = P (.|EM) = P (.|\n\u22c2\nM \u2032\u2208AEM \u2032), as desired. Now as-\nsumeM \/\u2208M. Then by definition \u03c0A = \u03c0A\u2032 with A\n\u2032 := {M\u222a{i} : i \/\u2208M}. Since\norder(A\u2032) = k + 1, the induction hypothesis yields \u03c0A\u2032 = P (.|\n\u22c2\nM \u2032\u2208A\u2032 EM \u2032),\nhence \u03c0A = P (.|\n\u22c2\nM \u2032\u2208A\u2032 EM \u2032). So I have to show that\n\u22c2\nM \u2032\u2208A\u2032 EM \u2032 = EM . By\nDefinition 5,\nEM =\n\u22c2\nM\u2286M \u2032\u2286N\nEM\n\u2032\n= EM\n\u22c2\uf8f1\uf8f2\uf8f3 \u22c2\nM \u2032\u2208A\u2032\n\uf8ee\uf8f0 \u22c2\nM \u2032\u2286M \u2032\u2032\u2286N\nEM\n\u2032\u2032\n\uf8f9\uf8fb\n\uf8fc\uf8fd\uf8fe .\nIn this, EM = \u2126 (by M \/\u2208 M) and\n\u22c2\nM \u2032\u2286M \u2032\u2032\u2286N E\nM \u2032\u2032 = EM \u2032 (by Definition 5).\nSo EM =\n\u22c2\nM \u2032\u2208A\u2032 EM \u2032 , as desired.\nCase 2: |A| > 1. I show \u03c0A = P\n(\n.\n\u2223\u2223\u22c2\nM\u2208AEM\n)\nby induction on the number\n|{M \u2208 A : |M | = k}| of subgroups in A of size k.\n\u25e6 Let |{M \u2208 A : |M | = k}| = 1. Then A = {M} \u222a A\u2217 with |M | = k and\norder(A\u2217) > k. Then \u03c0A was defined as the function in \u03a0 proportional\nto \u03c0{M}\u03c0A\u2217\/\u03c0{M}\u2228A\u2217 ; let me show that (i) such a function does indeed\nexists and (ii) satisfies (15), as desired. Now, \u03c0{M} satisfies (15) by Case\n1, and \u03c0A\u2217 and \u03c0{M}\u2228A\u2217 satisfy (15) by order(A\n\u2217) > k and order({M} \u2228\nA\u2217) > k (and the k-induction hypothesis). So, by Lemma 1, the function\n\u03c0{M}\u03c0A\u2217\/\u03c0{M}\u2228A\u2217 is proportional to a function in \u03a0, so that \u03c0A is well-\ndefined. Also by Lemma 1, this function \u03c0A satisfies (15), as desired.\n\u25e6 Let |{M \u2208 A : |M | = k}| = l > 1, and assume A\u2217 satisfies (15) whenever\n|{M \u2208 A\u2217 : |M | = k}| < l (and order(A\u2217) = k). By definition, \u03c0A \u221d\n\u03c0{M}\u03c0A\u2217\/\u03c0{M}\u2228A\u2217 , where A = {M} \u222a A\n\u2217 with |M | = k and |{M\u2217 \u2208 A\u2217 :\n30\n|M\u2217| = k}| = l \u2212 1. Again, we have to show that \u03c0A is well-defined (i.e.\nthat \u03a0 indeed contains a function proportional to \u03c0{M}\u03c0A\u2217\/\u03c0{M}\u2228A\u2217) and\nsatisfies (15). \u03c0{M} satisfies (15) by Case 1, \u03c0A\u2217 satisfies (15) by |{M\n\u2217 \u2208\nA\u2217 : |M\u2217| = k}| = l \u2212 1 (and the l-induction hypothesis), and \u03c0{M}\u2228A\u2217\nsatisfies (15) by order({M} \u2228 A\u2217) > k (and the k-induction hypothesis).\nSo, by Lemma 1, \u03c0A is well-defined and satisfies (15). \u0001\n9 Conclusion\nThe above model interprets opinion pooling as information pooling: collective opin-\nions should build in the group\u2019s entire information, be it shared or personal. According\nto the pooling formulae I obtained, collective opinions should account for informa-\ntional asymmetries not by taking a standard weighted (linear or geometric) average\nof the individual opinions with higher weight assigned to better informed individuals\nbut by incorporating people\u2019s prior opinions in addition to their actual (i.e. posterior)\nopinions. In practice, people have either to agree on a common prior opinion p, i.e.\non how to interpret the shared information, or they have to submit their possibly\ndiverging prior opinions p1, ..., pn. Based on simple axioms, Theorem 1 shows how to\naggregate the (prior and posterior) opinions into a collective opinion. The formula\ndefines a multiplicative opinion pool: the collective opinion \u03c0 is the product of the\nindividual opinions \u03c01, ..., \u03c0n and a function g (which depends on prior opinions).\nMore precisely, Theorem 1 suggests that, based on individual opinions \u03c01, ..., \u03c0n,\nthe collective opinion \u03c0 should be defined by\n\u03c0 \u221d \u03c01 \u00b7 \u00b7 \u00b7\u03c0n\/p\nn\u22121\nif people agree on a common prior p, and by\n\u03c0 \u221d\n\u03c01\np1\n\u00b7 \u00b7 \u00b7\n\u03c0n\npn\nF (p1, ..., pn) (20)\nif people have arbitrary priors p1, ..., pn, where F is a standard opinion pool. I have\nsuggested that F should be anonymous (i.e. symmetric in its arguments) because\nthe prior opinions it pools are based on the same (shared) information, giving no\nindividual an informational superiority. More specifically, I have suggested to define\nF as unweighted geometric pooling, because this generates appealing properties shown\nin Theorem 2. This choice of F gives collective opinion the form\n\u03c0 \u221d\n\u03c01\np\n1\u22121\/n\n1\n\u00b7 \u00b7 \u00b7\n\u03c0n\np\n1\u22121\/n\nn\n.\nFortunately, not much depends on how we choose F in the pooling formula (20)\nif \u2014 as is frequently the case \u2014 \u03c01p1 \u00b7 \u00b7 \u00b7\n\u03c0n\npn\ndominates F (p1, ..., pn) (i.e., if the function\n\u03c01\np1\n\u00b7 \u00b7 \u00b7 \u03c0npn varies far more than the function F (p1, ..., pn) for \u2018reasonable\u2019 choices of F ).\n31\nIn such cases, one might in practice refrain from choosing F and simply define the\ncollective opinion as\n\u03c0 \u221d\n\u03c01\np1\n\u00b7 \u00b7 \u00b7\n\u03c0n\npn\n,\na particularly elegant pooling formula.\nA crucial axiom underlying these pooling formulas is that personal information\nis independent. By Theorem 3, independence is threatened by the possibility of\nsubgroup information, i.e. of information held by more than one but less than all\nindividuals. Theorem 4 therefore generalises the aggregation rule to arbitrary in-\nformation distributions (allowing for subgroup information). The generalisation is\nunique, but assumes that each subgroup with subgroup information agrees on how\nto interpret this information, a kind of common prior assumption. Dropping this\nassumption would have gone beyond the scope of this paper, but it might be an\ninteresting route for future research.\n10 References\nBacharach, M. (1972) Scientific disagreement, unpublished manuscript\nDietrich, F. (2004) Opinion Pooling under Asymmetric Information, working paper,\nPublic Economics 0407002, EconWPA\nDietrich, F. (2008) The premises of Condorcet\u2019s jury theorem are not simultaneously\njustified, Episteme - a Journal of Social Epistemology 5(1): 56-73\nDietrich, F., and C. List (2007) Opinion pooling on general agendas, working paper,\nMETEOR Research Memorandum 038, Maastricht University\nFitelson, B. (2001) A Bayesian account of independent evidence with applications,\nPhilosophy of Science 68 (Proceedings). S123 - S140\nGenest, C. (1984) A characterization theorem for externally Bayesian groups, Ann.\nStatist. 12, p. 1100-1105\nGenest, C. and J. V. Zidek (1986) Combining probability distributions: a critique\nand an annotated bibliography, Statist. Sci. 1, p. 114-148\nGenest, C., K. J. McConway and M. J. Schervish (1986) Characterization of extern-\nally Bayesian pooling operators, Ann. Statist. 14, 487-501\nHild, M. (1998) The instability of ex post aggregation, Typescript\nHylland, A. and R. Zeckhauser (1979) The impossibility of group decision making\nwith separate aggregation of beliefs and values, Econometrica 47, p. 1321-36\n32\nJeffrey, R. (1983) (first published 1965) The logic of decision, Chicago: Chicago\nUniversity Press\nLehrer, K. and C.Wagner (1981)Rational Consensus in Science and Society, Dordrecht:\nReidel\nLevi, I. (1990) Pareto-unanimity and consensus, Journal of Philosophy 87\nMadansky, A. (1964) Externally Bayesian groups, Technical Report RM-4141-PR,\nRAND Corporation\nMcConway, K. (1978) The combination of experts\u2019 opinions in probability assess-\nments: some theoretical considerations, Ph.D. thesis, University College London\nMcConway, K. (1981) Marginalization and linear opinion pools, Jour. Amer. Statist.\nAssoc. 76, p. 410-414\nMongin, P. (1995) Consistent Bayesian aggregation, Journal of Economic Theory\n66, p. 313-351\nMongin, P. (1998) The paradox of the Bayesian experts and state-dependent utility\ntheory, Journal of Mathematical Economics 29, p. 331-61\nMorris, P. A. (1974) Decision analysis expert use, Management Science 20, p. 1233-\n41\nPearl, J. (2000) Causality: Models, Reasoning and Inference, Cambridge: Cambridge\nUniversity Press\nPivato, M. (2008) The Discursive Dilemma and Probabilistic Judgement Aggrega-\ntion, MPRA Paper 8412, University Library of Munich, Germany\nRisse, M. (2001) Instability of ex post aggregation in the Bolker\/Jeffrey framework\nand related instability phenomena, Erkenntnis 55, p. 239-269\nRisse, M. (2003) Bayesian group agents and two modes of aggregation, Synthese,\nforthcoming\nSavage, L. (1954) The foundations of statistics, New York: Wiley\nSchervish, M., T. Seidenfeld and J. Kadane (1991) Shared preferences and state-\ndependent utilities, Management Science 37, p. 1575-89\nSeidenfeld, T., J. Kadane and M. Schervish (1989) On the shared preferences of two\nBayesian decision makers, Journal of Philosophy 86, p. 221-44\nWagner, C. G. (1982) Allocation, Lehrer models, and the consensus of probabilities,\nTheory and Decision 14, p. 207-220\n33\nWagner, C. (1985) \u201cOn the Formal Properties of Weighted Averaging as a Method\nof Aggregation\u201d, Synthese 62: 97-108\n34\n"}