{"doi":"10.1007\/s11222-006-8450-8","coreId":"71010","oai":"oai:eprints.lancs.ac.uk:8189","identifiers":["oai:eprints.lancs.ac.uk:8189","10.1007\/s11222-006-8450-8"],"title":"Exact and efficient Bayesian inference for multiple changepoint problems.","authors":["Fearnhead, Paul"],"enrichments":{"references":[{"id":16371916,"title":"A Bayesian analysis for change point problems.","authors":[],"date":"1993","doi":"10.2307\/2290726","raw":"Barry, D. and Hartigan, J. A. (1993). A Bayesian analysis for change point problems. Journal of the American Statistical Society 88, 309\u2013319.","cites":null},{"id":16371931,"title":"A Bayesian change-point analysis of electromyographic data: detecting muscle activation patterns and associated applications.","authors":[],"date":"2003","doi":"10.1093\/biostatistics\/4.1.143","raw":"Johnson, T. D., Elasho\ufb00, R. M. and Harkema, S. J. (2003). A Bayesian change-point analysis of electromyographic data: detecting muscle activation patterns and associated applications. Biostatistics 4, 143\u2013164.","cites":null},{"id":16371921,"title":"An improved particle \ufb01lter for non-linear problems.","authors":[],"date":"1999","doi":"10.1049\/ip-rsn:19990255","raw":"Carpenter, J., Cli\ufb00ord, P. and Fearnhead, P. (1999). An improved particle \ufb01lter for non-linear problems. IEE proceedings-Radar, Sonar and Navigation 146, 2\u20137.","cites":null},{"id":16371914,"title":"Bayes inference via Gibbs sampling of autoregressive time series subject to Markov mean and variance shifts.","authors":[],"date":"1993","doi":"10.2307\/1391303","raw":"Albert, J. H. and Chib, S. (1993). Bayes inference via Gibbs sampling of autoregressive time series subject to Markov mean and variance shifts. Journal of Business and Economic Statistics 11, 1\u201315.","cites":null},{"id":16371938,"title":"Bayesian analysis of a Poisson process with a change-point.","authors":[],"date":"1986","doi":"10.2307\/2336274","raw":"Raftery, A. E. and Akman, V. E. (1986). Bayesian analysis of a Poisson process with a change-point. Biometrika 73, 85\u201389.","cites":null},{"id":16371942,"title":"Bayesian binary segmentation procedure for a Poisson process with multiple changepoints.","authors":[],"date":"2001","doi":"10.1198\/106186001317243449","raw":"Yang, T. Y. and Kuo, L. (2001). Bayesian binary segmentation procedure for a Poisson process with multiple changepoints. Journal of Computational and Graphical Statistics 10, 772\u2013785.","cites":null},{"id":16371937,"title":"Bayesian curve \ufb01tting using MCMC with applications to signal segmentation.","authors":[],"date":"2002","doi":"10.1109\/78.984776","raw":"Punskaya, E., Andrieu, C., Doucet, A. and Fitzgerald, W. J. (2002). Bayesian curve \ufb01tting using MCMC with applications to signal segmentation. IEEE Transactions on Signal Processing 50, 747\u2013758.","cites":null},{"id":16371940,"title":"Bayesian methods for hidden Markov models: Recursive computing in the 21st century.","authors":[],"date":"2002","doi":"10.1198\/016214502753479464","raw":"Scott, S. L. (2002). Bayesian methods for hidden Markov models: Recursive computing in the 21st century. Journal of the American Statistical Association 97, 337\u2013351.","cites":null},{"id":16371935,"title":"Boundary detection through dynamic polygons.","authors":[],"date":"1998","doi":"10.1111\/1467-9868.00143","raw":"Pievatolo, A. and Green, P. J. (1998). Boundary detection through dynamic polygons. Journal of the Royal Statistical Society, Series B 60, 609\u2013626.","cites":null},{"id":16371939,"title":"Detection of onset of neuronal activity by allowing for heterogeneity in the change points.","authors":[],"date":"2002","doi":"10.1016\/s0165-0270(02)00275-3","raw":"Ritov, Y., Raz, A. and Bergman, H. (2002). Detection of onset of neuronal activity by allowing for heterogeneity in the change points. Journal of Neuroscience Methods 122, 25\u201342.","cites":null},{"id":16371933,"title":"Detection of undocumented changepoints: A revision of the two-phase regression model.","authors":[],"date":"2002","doi":"10.1175\/1520-0442(2002)015<2547:doucar>2.0.co;2","raw":"18Lund, R. and Reeves, J. (2002). Detection of undocumented changepoints: A revision of the two-phase regression model. Journal of Climate 15, 2547\u20132554.","cites":null},{"id":16371924,"title":"Direct simulation for mixture distributions: component weights and discrete distributions. submitted Available from http:\/\/www.maths.lancs.ac.uk\/\u223cfearnhea\/.","authors":[],"date":"2004","doi":null,"raw":"Fearnhead, P. (2004). Direct simulation for mixture distributions: component weights and discrete distributions. submitted Available from http:\/\/www.maths.lancs.ac.uk\/\u223cfearnhea\/.","cites":null},{"id":16371919,"title":"E\ufb03cient construction of reversible jump Markov chain Monte Carlo proposal distributions.","authors":[],"date":"2003","doi":"10.1111\/1467-9868.03711","raw":"Brooks, S. P., Giudici, P. and Roberts, G. O. (2003). E\ufb03cient construction of reversible jump Markov chain Monte Carlo proposal distributions. Journal of the Royal Statistical Society, series B 65, 3\u201339.","cites":null},{"id":16371943,"title":"Estimation of a noisy discrete-time step function: Bayes and empirical Bayes approaches.","authors":[],"date":"1984","doi":"10.1214\/aos\/1176346802","raw":"Yao, Y. (1984). Estimation of a noisy discrete-time step function: Bayes and empirical Bayes approaches. The Annals of Statistics 12, 1434\u20131447.","cites":null},{"id":16371926,"title":"Exact \ufb01ltering for partially-observed continuous-time Markov models.","authors":[],"date":"2004","doi":"10.1111\/j.1467-9868.2004.05561.x","raw":"Fearnhead, P. and Meligkotsidou, L. (2004). Exact \ufb01ltering for partially-observed continuous-time Markov models. Journal of the Royal Statistical Society, series B. To appear Available from http:\/\/www.maths.lancs.ac.uk\/\u223cfearnhea\/.","cites":null},{"id":16371936,"title":"Exact sampling with coupled Markov chains and applications to statistical mechanics.","authors":[],"date":"1996","doi":"10.1002\/(sici)1098-2418(199608\/09)9:1\/2<223::aid-rsa14>3.0.co;2-o","raw":"Propp, J. G. and Wilson, D. B. (1996). Exact sampling with coupled Markov chains and applications to statistical mechanics. Random Structures and Algorithms 9, 223\u2013252.","cites":null},{"id":16371930,"title":"Forecasting, stuctural time series and the Kalman \ufb01lter.","authors":[],"date":"1989","doi":"10.1017\/cbo9781107049994","raw":"Harvey, A. C. (1989). Forecasting, stuctural time series and the Kalman \ufb01lter. Cambridge University Press, Cambridge, UK.","cites":null},{"id":16371920,"title":"Hierarchical Bayesian analysis of changepoint problems.","authors":[],"date":"1992","doi":"10.2307\/2347570","raw":"17Carlin, B. P., Gelfand, A. E. and Smith, A. F. M. (1992). Hierarchical Bayesian analysis of changepoint problems. Applied Statistics 41, 389\u2013405.","cites":null},{"id":16371932,"title":"Monte Carlo strategies in scienti\ufb01c computing.","authors":[],"date":"2001","doi":"10.1007\/978-0-387-76371-2","raw":"Liu, J. S. (2001). Monte Carlo strategies in scienti\ufb01c computing. New York: Springer.","cites":null},{"id":16371918,"title":"Multiple changepoint \ufb01tting via quasilikelihood, with application to DNA sequence segmentation.","authors":[],"date":"2000","doi":"10.1093\/biomet\/87.2.301","raw":"Braun, J. V., Braun, R. K. and Muller, H. G. (2000). Multiple changepoint \ufb01tting via quasilikelihood, with application to DNA sequence segmentation. Biometrika 87, 301\u2013314.","cites":null},{"id":16371934,"title":"Numerical Bayesion Methods Applied to Signal Processing.","authors":[],"date":"1996","doi":"10.1007\/978-1-4612-0717-7","raw":"\u00b4 O Ruanaidh, J. J. K. and Fitzgerald, W. J. (1996). Numerical Bayesion Methods Applied to Signal Processing. New York: Springer.","cites":null},{"id":16371941,"title":"On the likelihood ratio test for a shift in location of normal populations.","authors":[],"date":"1979","doi":"10.2307\/2286336","raw":"Worsley, K. J. (1979). On the likelihood ratio test for a shift in location of normal populations. Journal of the American Statistical Association 74, 363\u2013367.","cites":null},{"id":16371925,"title":"Online inference for hidden Markov models.","authors":[],"date":"2003","doi":"10.1111\/1467-9868.00421","raw":"Fearnhead, P. and Cli\ufb00ord, P. (2003). Online inference for hidden Markov models. Journal of the Royal Statistical Society, Series B 65, 887\u2013899.","cites":null},{"id":16371929,"title":"Partition models.","authors":[],"date":"1990","doi":"10.1080\/03610929008830345","raw":"Hartigan, J. A. (1990). Partition models. Communications in Statistics 19, 2745\u20132756.","cites":null},{"id":16371915,"title":"Product partition models for change point problems.","authors":[],"date":"1992","doi":"10.1214\/aos\/1176348521","raw":"Barry, D. and Hartigan, J. A. (1992). Product partition models for change point problems. The Annals of Statistics 20, 260\u2013279.","cites":null},{"id":16371927,"title":"Reversible jump Markov chain Monte Carlo computation and Bayesian model determination.","authors":[],"date":"1995","doi":"10.2307\/2337340","raw":"Green, P. (1995). Reversible jump Markov chain Monte Carlo computation and Bayesian model determination. Biometrika 82, 711\u2013732.","cites":null},{"id":16371923,"title":"Sequential Monte Carlo methods in \ufb01lter theory.","authors":[],"date":"1998","doi":null,"raw":"Fearnhead, P. (1998). Sequential Monte Carlo methods in \ufb01lter theory. Ph.D. thesis, Oxford Unversity, available from http:\/\/www.maths.lancs.ac.uk\/\u223cfearnhea\/.","cites":null},{"id":16371917,"title":"Statistical methods for DNA sequence segmentation.","authors":[],"date":"1998","doi":"10.1214\/ss\/1028905933","raw":"Braun, J. V. and Muller, H. G. (1998). Statistical methods for DNA sequence segmentation. Statistical Science 13, 142\u2013162.","cites":null},{"id":16371922,"title":"Testing and locating changepoints with application to stock prices.","authors":[],"date":"1997","doi":"10.1080\/01621459.1997.10474026","raw":"Chen, J. and Gupta, A. K. (1997). Testing and locating changepoints with application to stock prices. Journal of the American Statistical Association 92, 739\u2013747.","cites":null},{"id":16371928,"title":"Trans-dimensional Markov chain Monte Carlo. In: Highly Structured Stochastic Systems","authors":[],"date":"2003","doi":null,"raw":"Green, P. J. (2003). Trans-dimensional Markov chain Monte Carlo. In: Highly Structured Stochastic Systems (eds. P. J. Green, N. L. Hjort and S. Richardson), Oxford University Press.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2006-06","abstract":"We demonstrate how to perform direct simulation from the posterior distribution of a class of multiple changepoint models where the number of changepoints is unknown. The class of models assumes independence between the posterior distribution of the parameters associated with segments of data between successive changepoints. This approach is based on the use of recursions, and is related to work on product partition models. The computational complexity of the approach is quadratic in the number of observations, but an approximate version, which introduces negligible error, and whose computational cost is roughly linear in the number of observations, is also possible. Our approach can be useful, for example within an MCMC algorithm, even when the independence assumptions do not hold. We demonstrate our approach on coal-mining disaster data and on well-log data. Our method can cope with a range of models, and exact simulation from the posterior distribution is possible in a matter of minutes","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/71010.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/8189\/1\/PScpt2.pdf","pdfHashValue":"5fe9451593780ebdd9261f9dcdaa3aa673d2bceb","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:8189<\/identifier><datestamp>\n      2018-01-24T03:16:56Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Exact and efficient Bayesian inference for multiple changepoint problems.<\/dc:title><dc:creator>\n        Fearnhead, Paul<\/dc:creator><dc:subject>\n        QA Mathematics<\/dc:subject><dc:description>\n        We demonstrate how to perform direct simulation from the posterior distribution of a class of multiple changepoint models where the number of changepoints is unknown. The class of models assumes independence between the posterior distribution of the parameters associated with segments of data between successive changepoints. This approach is based on the use of recursions, and is related to work on product partition models. The computational complexity of the approach is quadratic in the number of observations, but an approximate version, which introduces negligible error, and whose computational cost is roughly linear in the number of observations, is also possible. Our approach can be useful, for example within an MCMC algorithm, even when the independence assumptions do not hold. We demonstrate our approach on coal-mining disaster data and on well-log data. Our method can cope with a range of models, and exact simulation from the posterior distribution is possible in a matter of minutes.<\/dc:description><dc:date>\n        2006-06<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/8189\/1\/PScpt2.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1007\/s11222-006-8450-8<\/dc:relation><dc:identifier>\n        Fearnhead, Paul (2006) Exact and efficient Bayesian inference for multiple changepoint problems. Statistics and Computing, 16 (2). pp. 203-213. ISSN 0960-3174<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/8189\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1007\/s11222-006-8450-8","http:\/\/eprints.lancs.ac.uk\/8189\/"],"year":2006,"topics":["QA Mathematics"],"subject":["Journal Article","PeerReviewed"],"fullText":"Exact and Efficient Bayesian Inference for Multiple Changepoint\nproblems\nPaul Fearnhead\nDepartment of Mathematics and Statistics\nLancaster University\nSummary We demonstrate how to perform direct simulation from the posterior distri-\nbution of a class of multiple changepoint models where the number of changepoints is\nunknown. The class of models assumes independence between the posterior distribution\nof the parameters associated with segments of data between successive changepoints.\nThis approach is based on the use of recursions, and is related to work on product\npartition models. The computational complexity of the approach is quadratic in the\nnumber of observations, but an approximate version, which introduces negligible error,\nand whose computational cost is roughly linear in the number of observations, is also\npossible. Our approach can be useful, for example within an MCMC algorithm, even\nwhen the independence assumptions do not hold. We demonstrate our approach on\nwell-log data. Our method can cope with a range of models for this data, and exact\nsimulation from the posterior distribution is possible in a matter of minutes.\nKeywords Bayes factor, Forward-backward algorithm, Model choice, Perfect simula-\ntion, Reversible jump MCMC, Well-log data\n1 Introduction\nMany time-series models incorporate one, or multiple, changepoints. Some examples\ninclude Poisson processes with a piece-wise constant rate parameter (Raftery and Ak-\nman, 1986; Yang and Kuo, 2001; Ritov et al., 2002), changing linear regression models\n(Carlin et al., 1992; Lund and Reeves, 2002), Gaussian observations with varying mean\n(Worsley, 1979) or variance (Chen and Gupta, 1997; Johnson et al., 2003), and Markov\nmodels with time-varying transition matrices (Braun and Muller, 1998). Such models\nhave been used for modelling stock prices, muscle activation, climatic time-series, DNA\nsequences and neuronal activity in the brain, amongst many other applications\nIn this paper we consider Bayesian analysis for a class of multiple changepoint problems.\nWe call a period of time between two consecutive changepoints a segment. This class of\n1\nmodels assumes that the parameter values associated with each segment are independent\nfrom each other. Yang and Kuo (2001) comment that calculating the Bayes factors\nfor models with different numbers of changepoints is \u201cessentially infeasible for a large\nmodel with many changepoints\u201d. Our aim is to show that calculation of Bayes factors,\nand perfect sampling from the posterior distribution of changepoint locations, is both\npossible, and computationally inexpensive for the class of models we consider. While\nthis class of models may seem restrictive, recent examples of work on such models can\nfound in Johnson et al. (2003), Punskaya et al. (2002), and Braun et al. (2000).\nAlthough we use the phrase \u201cperfect simulation\u201d, we do not use coupling-from-the past\n(Propp and Wilson, 1996), or related ideas, which have become synonymous with this\nphrase. Instead, the work we present is closely related to work by Yao (1984), Barry\nand Hartigan (1992) and Barry and Hartigan (1993). These papers present efficient\nrecursions that allow the posterior probabilities of different numbers of changepoints,\nand the posterior mean of the parameters to be calculated. Despite the desirability of\nexact solutions, and the simplicity and computational efficiency of the recursions, these\nmethods are currently underused. We extend these methods to allow for direct simu-\nlation from the posterior distribution of the number and position of the changepoints,\nand to also perform inference conditional on the number of changepoints.\nMuch recent research for changepoint models is based on the use of MCMC. For models\nwith an unknown number of changepoints, a common approach is that of Green (1995).\nA set of models, each incorporating a different number of changepoints, are introduced,\nand reversible jump MCMC is used to explore the joint space of model and parameters.\nPotential difficulties of this approach include designing moves, particular ones between\ndifferent models, which enable the MCMC algorithm to mix well (for guidelines on de-\nsigning reversible jump MCMC algorithms see Brooks et al., 2003), and being able to\ndetect convergence of the algorithm. For example, in the analysis of the coal-mining\ndisaster data in Green (1995), the reversible jump MCMC algorithm had not converged.\nThe reanalysis of the data in Green (2003), using a reversible jump MCMC algorithm\nrun for 25 times as long, does fully explore the posterior distribution. The exact simu-\nlation method we describe here avoids any problems of needing to diagnose convergence\nof an MCMC algorithm.\nWe consider two classes of prior for the changepoint process. One, that of Green (1995),\ninvolves a prior on the number of changepoints, and then a conditional prior on their\n2\nposition. The other is based on modelling the changepoint process by a point process\n(Pievatolo and Green, 1998), and is a special case of a product-partion model (Hartigan,\n1990). This indirectly specifies a joint prior on the number and position of the change-\npoints. In both cases we assume that, conditional on the realisation of the changepoint\nprocess, the joint posterior distribution of the parameters is independent across the\nsegments of the time series. We also assume a conjugate prior for the parameters asso-\nciated with each segment. Under these two assumptions we derive a set of recursions\nto perform exact inference.\nThe recursions are similar to those of the Forward-Backward algorithm (see Scott, 2002,\nfor a review). Recent work has shown how such recursions can be used to perform exact\ninference for a range of problems (Fearnhead and Meligkotsidou, 2004; Fearnhead, 2004).\nThe assumption of independence between segments ensures the necessary Markov prop-\nerty that is required for Forward-Backward type recursions. For a data set consisting\nof observations at discrete times, 1, . . . , n, the recursions are based on calculating the\nprobability of the data from time t to time n, given a changepoint at time t, in terms\nof the equivalent probabilities at times t+ 1, . . . , n. Once these probabilities have been\ncalculated for all time-points, it is possible to directly simulate from the posterior dis-\ntribution of the time of the first changepoint, and then the conditional distribution of\nthe time of the second changepoint, given the first, and so on. The recursions can also\nbe used to perform exact inference conditional on the number of changepoints, and in\nsome cases to calculate the posterior distribution of the parameters that govern the\npoint process model for the changepoints.\nThe computational cost of the recursions increases quadratically with n. However an\napproximate version, which introduces negligible error, is possible. In limiting situations\nwhere the length of time series increases, and the number of changepoints is increasing\nlinearly with the number of observations, the computational cost increases roughly\nlinearly with n. (In the alternative limiting regime of more frequent observations, the\ncomputational cost remains quadratic in n.) The assumption of conjugate priors can\npotentially be relaxed, but with an increase in the computational cost. Essentially,\nlow-dimensional integrals that can be calculated analytically under conjugate priors\nwould need to be calculated numerically (for example see Section 4.3). Relaxation of\nthe independence assumption is more difficult, but our algorithm can still be used as a\nuseful tool for analysing such data. For example, the algorithm can be embedded in an\nMCMC algorithm, and we demonstrate such an approach on some real data.\n3\nThe outline of the paper is as follows. In Section 2 we introduce the two classes of\nchangepoint model that we consider. The recursions are derived and detailed in Section\n3. The resulting algorithm is demonstrated on well-log data in Section 4. We consider\na range of models for this data, and also demonstrate how our method can be used to\nanalyse the data when there is dependence between the parameters for each segment.\nThe paper concludes with a discussion.\n2 Models and Notation\nWe consider the following class of multiple changepoint models. Consider a sample\nof size n, y1, . . . , yn. Observation yi is obtained at time i, and we let yi:j denote the\nobservations from time i to time j inclusive.\nFirstly condition on m integer-valued changepoints, at points 0 < \u03c41 < \u03c42 < \u00b7 \u00b7 \u00b7 < \u03c4m <\nn. We let \u03c40 = 0 and \u03c4m+1 = n. Then the jth segment consists of the observations from\ntime \u03c4j\u22121 + 1 to time \u03c4j. We associate a (possibly vector-valued) parameter \u03b8j with\nthe jth segment for j = 1, . . . ,m+ 1. Conditional on the change-points and parameter\nvalues, the observations are independent; observation yi being drawn from a density\nf(yi|\u03b8j) if time i is in the jth segment.\nWe assume independent priors for the parameters associated with each segment. The\nprior for \u03b8j is denoted by pi(\u03b8j). Here, and throughout, we use pi(\u00b7) solely to denote a\nprior density; the argument making it clear as to which parameter the prior is for.\nWe assume that the changepoints occur at discrete time points, and consider two priors\nfor the changepoints. The first prior is based on a prior for the number of changepoints,\nand then a conditional prior on their positions. We will define this conditional prior\non the positions in terms of pim(\u03c4m) the prior for the last change point, and, for j =\n1, . . . ,m \u2212 1, pim(\u03c4j |\u03c4j+1), the prior for the position of the jth changepoint, given the\nposition of the (j + 1)st.\nThe second prior is obtained from a point process on the positive and negative integers.\nThe point process is specified by the probability mass function g(t) for the time between\ntwo succesive points. We assume that this time must be a strictly positive integer. We\nobserve the point process on the interval [1, n\u2212 1], and assume that changepoints occur\nat the positions of points in the point process. This prior is an example of a product-\npartition model.\n4\nIf G(t) =\n\u2211t\ns=1 g(s), is the distribution function of the distance between two succesive\npoints, and g0(t) is the mass function of the first point after 0, then the probability of\nm changepoints occuring at \u03c41, . . . , \u03c4m is\ng0(\u03c41)\n\uf8eb\n\uf8ed m\u220f\nj=2\ng(\u03c4j \u2212 \u03c4j\u22121)\n\uf8f6\n\uf8f8 (1 \u2212G(\u03c4m+1 \u2212 \u03c4m)).\nNatural choices for the distribution of the time between successive points are from the\nnegative binomial family. For a negative binomial distribution with parameters k, a\npositive integer, and p we have\ng(t) =\n\uf8eb\n\uf8ed t\u2212 k\nk \u2212 1\n\uf8f6\n\uf8f8 pk(1\u2212 p)t\u2212k g0(t) =\nk\u2211\ni=1\n\uf8eb\n\uf8ed t\u2212 i\ni\u2212 1\n\uf8f6\n\uf8f8 pi(1\u2212 p)t\u2212i\/k.\nThe negative binomial distribution can be thought of as a discrete version of the gamma\ndistribution (especially if p is small). If k = 1 then the negative binomial distribution\nis the geometric distribution, and the point process is Markov. Larger values of k can\nreduce the number of very short segments.\n3 Filtering Recursions\nWe first derive the recursions for analysing data under the point process prior for the\nchangepoints. We later derive recursions to perform inference conditional on the number\nof changepoints, and show how these can be used to perform inference under the other\nprior, and to perform inference about the parameters of the point process prior.\n3.1 Basic Recursions\nFor times s \u2265 t, define\nP (t, s) = Pr(yt:s|t, s in the same segment)\n=\n\u222b s\u220f\ni=t\nf(yi|\u03b8)pi(\u03b8)d\u03b8.\nWe will assume that the probabilities P (t, s) can be calculated for all t and s. In\npractice this will require conjugate priors on \u03b8, or, if \u03b8 is low-dimensional, that the\nrequired integration can be calculated numerically.\nWe next define for t = 2, . . . , n\nQ(t) = Pr(yt:n|changepoint at t\u2212 1),\n5\nwith Q(1) = Pr(y1:n). A set of recursions for calculating these probabilities are given\nby the following theorem.\nTheorem 1 Define the probabilities Q(t) and P (t, s) as above. Then for t = 2, . . . , n\nQ(t) =\nn\u22121\u2211\ns=t\nP (t, s)Q(s + 1)g(s + 1\u2212 t) + P (t, n)(1\u2212G(n \u2212 t)), (1)\nand\nQ(1) =\nn\u22121\u2211\ns=1\nP (1, s)Q(s + 1)g0(s) + P (1, n)(1 \u2212G0(n\u2212 1)), (2)\nwhere G0(t) =\n\u2211t\ns=1 g0(s).\nProof: We only prove Equation 1. Equation 2 can be derived similarly.\nFor notational convenience we drop the explicit conditioning on a changepoint at t\u2212 1\nin the following. Thus,\nQ(t) = Pr(yt:n)\n=\nn\u22121\u2211\ns=t\nPr(yt:n,next changepoint at s) + Pr(yt:n,no further changepoints).\nNow these probabilities can be calculated by the product of the prior probability on the\nchangepoints, and the probabilities of the observations from a single segment, P (t, s).\nThus\nPr(yt:n,next changepoint at s)\n= Pr(next changepoint at s) Pr(yt:s, ys+1:n|next changepoint at s)\n= g(s + 1\u2212 t) Pr(yt:s|t, s in same segment)Pr(ys+1:n|changepoint at s)\n= g(s + 1\u2212 t)P (t, s)Q(s + 1)\nSimilarly\nPr(yt:n,no further changepoints) = P (t, n)(1 \u2212G0(n\u2212 t)),\nas required. 2\nEquations 1 and 2 give recursions that can be used to calculate Q(t) in turn for t =\nn, . . . , 1. The evidence of the model is just Q(1). These equations are equivalent to\nthose of Barry and Hartigan (1992), and are based on the same idea as recursions of\nYao (1984).\n6\nThe computational complexity of the resulting algorithm is quadratic in n. However\noften only a small proportion of the terms on the right-hand side of (1) make an ap-\npreciable contribution to Q(t). This can happen when the data makes it almost certain\nthat a changepoint occurs before a given time-point. Thus the summation can often be\ntruncated with negligible error. We propose truncating the sum at term k when\nP (t, k)Q(s + 1)g(k + 1\u2212 t)\u2211k\ns=t P (t, s)Q(s+ 1)g(s + 1\u2212 t)\n(3)\nis less than some predetermined value, for example 10\u221210.\nIn the limiting regime of analysing a process over a longer time period, so that the\nnumber of changepoints will increase roughly linearly with the number of observations,\nn, the computational complexity of the resulting approximate set of recursions will be\nlinear in n. Essentially the average number of terms required in the right-hand side\nof (1) will be constant with t. Thus the average computational cost of one of the n\nrecursions will be independent of n.\n3.2 Perfect Simulation of Changepoints\nGiven the values of Q(t) for t = 1, . . . , n it is straightforward to simulate from the\nposterior distribution of the changepoints as follows.\nThe posterior distribution of the first changepoint is given by\nPr(\u03c41|y1:n) = Pr(y1:n, \u03c41)\/Pr(y1:n)\n= Pr(\u03c41) Pr(y1:\u03c41 |\u03c41) Pr(y\u03c41+1:n|\u03c41)\/Q(1)\n= P (1, \u03c41)Q(\u03c41 + 1)g0(\u03c41)\/Q(1),\nfor \u03c41 = 1, . . . , n\u22121. The probability of no further changepoint being P (1, n)(1\u2212G0(n\u2212\n1))\/Q(1).\nSimilarly the posterior distribution of the \u03c4j given \u03c4j\u22121 is\nPr(\u03c4j|\u03c4j\u22121, y1:n) = P (\u03c4j\u22121 + 1, \u03c4j)Q(\u03c4j + 1)g(\u03c4j \u2212 \u03c4j\u22121)\/Q(\u03c4j\u22121 + 1),\nfor \u03c4j = \u03c4j\u22121 + 1, . . . , n \u2212 1, and the probability of no further breakpoint is P (\u03c4j\u22121 +\n1, n)(1 \u2212G0(n\u2212 \u03c4j\u22121 \u2212 1))\/Q(\u03c4j\u22121 + 1).\nEfficient simulation of large samples of changepoints from the posterior distribution can\nbe done by simulating the samples concurrently, using the following algorithm. We\n7\ndenote the generic posterior distribution of the next changepoint, given a changepoint\nat t by Pr(\u03c4 |y1:n, t), which can be calculated as above.\n(1) For a sample of size M , initiate each of the M samples with a changepoint at\nt = 0.\n(2) For t = 0, . . . , n\u2212 2:\n(i) Calculate nt the number of whose last changepoint was at time t.\n(ii) If nt > 0 calculate the probability distribution Pr(\u03c4 |y1:n, t).\n(iii) Sample nt times from Pr(\u03c4 |y1:n, t) using Algorithm 1 of Carpenter et al.\n(1999) (see the Appendix). Use these values to update the nt samples of\nchangepoints which have a changepoint at t.\nThere are two advantages of this algorithm. The first is that the probability mass\nfunction Pr(\u03c4 |y1:n, t) need only be calculated once regardless of the number of samples\nrequired from it. If changepoints are sampled one at a time, then either these densities\nwill, potentially, need to be calculated for each sample, or they will need to be stored.\nStoring these mass functions can place large burdens on computational memory. The\nstorage requirements will be quadratic in n; by comparison the above algorithm has\nstorage requirements that are linear in n.\nThe second is that simulating a sample of size m from a general discrete mass function\ncan be achieved more efficiently than sampling m samples of size 1. Algorithm 1 of\nCarpenter et al. (1999) allows a sample of size m to be simulated with order n + m\neffort, rather than the nm effort of sampling m samples of size 1.\n3.3 Conditioning on the Number of Changepoints\nNow consider inference conditional on m changepoints. As in Section 2 we define the\nprior for the changepoints via pim(\u03c4m) and conditional probabilities of the form pim(\u03c4 +\nj|\u03c4j+1). We define P (s, t) as before, and for j = 1, . . . ,m, and t = j+1, . . . , n\u2212m\u22121+j,\nQ\n(m)\nj (t) = Pr(yt:n|\u03c4j = t\u2212 1, m changepoints).\nWe can derive the following set of recursions. For t = m+ 1, . . . , n\u2212 1,\nQ(m)m (t) = P (t, n)pim(\u03c4m = t\u2212 1).\n8\nFor j = 1, . . . ,m\u2212 1, and t = j + 1, . . . , n \u2212m\u2212 1 + j\nQ\n(m)\nj (t) =\nn\u2212m+j\u2211\ns=t\nP (t, s)Q\n(m)\nj+1(s + 1)pim(\u03c4j = t\u2212 1|\u03c4j+1 = s).\nFinally\nPr(y1:n|m changepoints) =\nn\u2212m\u2211\ns=1\nP (1, s)Q\n(m)\n1 (s + 1). (4)\nThese can be proved in a similar way to Theorem 1.\nIf the number of changepoints is unknown, with prior pi(m), then the posterior distri-\nbution of m can be calculated as\nPr(m|y1:n) \u221d pi(m) Pr(y1:n|m changepoints),\nwith the last term, the evidence for m changepoints, being calculated, for each m, using\nthe recursions.\nSimulation from the joint posterior distribution is possible by first simulatingM samples\nfrom Pr(m|y1:n). If the value m is sampled Nm times, then Nm samples from the\nposterior distribution of the changepoint positions, conditional on m changepoints, can\nbe obtained as described in Section 3.2. The only difference is that the conditional\ndistribution of \u03c4j given \u03c4j\u22121 is now\nPr(\u03c4j|\u03c4j\u22121, y1:n,m) = P (\u03c4j\u22121 + 1, \u03c4j)Q\n(m)\nj (\u03c4j + 1)pim(\u03c4j\u22121|\u03c4j)\/Q\n(m)\nj\u22121(\u03c4j\u22121).\nFinally, in the case of the Markov point process prior (that is, a geometric distribution\nfor the distance between changepoints), exact inference is possible even if the probability\nof a changepoint at any timepoint, p, is unknown. This is because, conditional on the\nnumber of changepoints, the positions are distributed uniformly along the interval,\nindependent of p. We can thus perform inference conditional on m changepoints. The\nprior for m is obtained by averaging pi(m|p) with respect to the prior for p.\n4 Well-log Data\nWe now consider the problem of detecting changepoints in well-log data. An example of\nwell-log data, which comes from O\u00b4 Ruanaidh and Fitzgerald (1996), is given in Figure\n1. The data consist of measurements of the nuclear-magnetic response of underground\nrocks. The data were obtained by lowering a probe into a bore-hole. Measurements\n9\n0 1000 2000 3000 4000\n80\n00\n0\n10\n00\n00\n12\n00\n00\n14\n00\n00\nTime\nN\nuc\nle\nar\n\u2212M\nag\nne\ntic\n re\nsp\non\nse\nFigure 1: The well-log data.\nwere taken at discrete timepoints by the probe as it was lowered through the hole. The\nunderlying signal is roughly piecewise constant, with each constant segment relates to\na single rock type (that has constant physical properties). The changepoints in the\nsignal occur each time a new rock type is encountered. Detecting the changepoints is\nimportant in oil-drilling; see the introduction of Fearnhead and Clifford (2003) for more\ndetails.\nThese data have been previously analysed by O\u00b4 Ruanaidh and Fitzgerald (1996), who\nused MCMC to fit a change-point model with a fixed number of changepoints; and\nby Fearnhead and Clifford (2003) who considered online analysis of the data using\nparticle filters. We performed a batch analysis of the data, but allowed for multiple\nchangepoints.\n4.1 Piecewise constant model\nInitially we consider analyse based on a model taken Fearnhead and Clifford (2003).\nWe assume a Markov point process prior, with p = 1\/250, for the changepoints. There\nare a number of outliers in the data which were removed before the data was analysed.\nFor a time t which belongs to segment i, we model a non-outlying observation, yt, by\nyt \u223c N(\u00b5i, \u03c3\n2),\n10\n36 38 40 42 44 46 48 50 52\n(a)\nNumber of Changepoints\nPo\nst\ner\nio\nr P\nro\nba\nbi\nlity\n0.\n00\n0.\n05\n0.\n10\n0.\n15\n(b)\nTime\nPr\nob\nab\nilit\ny\n0 1000 2000 3000 4000\n0\n1\n0 1000 2000 3000 4000\n10\n00\n00\n12\n00\n00\n14\n00\n00\n(c)\nTime\nN\nuc\nle\nar\n R\nes\npo\nns\ne\n0 1000 2000 3000 4000\n10\n00\n00\n12\n00\n00\n14\n00\n00\n(d)\nTime\nN\nuc\nle\nar\n R\nes\npo\nns\ne\nFigure 2: Results of analysis of the well-log data. Figures (a)\u2013(c) are a model with\np = 0.004: histogram of number of changepoints(a); posterior distribution of position\nof changepoints (b); and 20 simulations from the posterior distribution of the signal\n(c). Figure (d) shows 20 simulations from the posterior distribution of the signal when\np = 0.013.\nwhere \u00b5i is the mean associated with the ith segment, and we assume a common known\nvariance, \u03c32 = 25002. We assume that the segment means have independent normal\npriors with mean 115, 000 and variance 10, 0002. Conditional on the segment means,\nthe observations are independent.\nAll the parameter values are based on a simple analysis of the data. Ideally they would\nhave been obtained from analysing related data, but such data was not available. The\nonly difference in the model from that used by Fearnhead and Clifford (2003) is that the\noutliers are removed before the analysis. By comparison, Fearnhead and Clifford (2003)\ndetect outliers as the data is processed. O\u00b4 Ruanaidh and Fitzgerald (1996) also removed\noutliers before analysing the data. However they assumed that 13 changepoints were\npresent in the data, and they modelled the distribution of the observations as Laplacian.\nFearnhead (1998) shows that the normal model is more appropriate.\n11\nThe results of our analysis are shown in Figure 2(a)\u2013(c). These plots are for the pos-\nterior distribution of the number of changepoints and their positions (based on 10,000\nindependent simulations from the posterior), and 20 realisations of the underlying sig-\nnal. It took 26 seconds on a 3.4GHz PC to perform this analysis. The results we obtain\ndiffer substantially from previous analyses. The posterior distribution suggests around\n40 changepoints, which is roughly three times as many as assumed by O\u00b4 Ruanaidh and\nFitzgerald (1996). Fearnhead and Clifford (2003) only infer 16 changepoints. However\ntheirs is an online analysis, so inference at each timepoint is based solely on the ob-\nservations to that timepoint. Furthermore, they only inferred a changepoint when the\nposterior probability of changepoint within the last 10 time-points was greater then 0.9.\nThis is likely to produce a conservative estimate of the number of changepoints.\nThe posterior mean of the number of changepoints, 43, is much larger than the 8 to 26\nchangepoints we would expect under our prior. Using the methods of Section 3.3 we\ncan perform inference when p is unknown. If we assume a uniform prior for p, then the\nposterior mode is at p = 0.013. Some results of analysing the data with this value of p\nare shown in Figure 2(d). The posterior distribution for the number of changepoints is\nsubstantially different, with a posterior mean of 52 (not shown), but realisations from\nthe posterior distributions of the underlying signal are similar to those when p = 0.004.\nWe repeated our analysis using the approximate algorithm suggested in Section 3.1. We\ntruncated the sums in Equations (1) and (2), used to calculate the Q(t)s, when the value\nof (3) was less than 10\u221210. The resulting algorithm on average required sums of 222\nterms to be calculated for each Q(t); which compares with average sums of 2025 terms\nfor the exact algorithm. This is nine-fold reduction in the complexity of the algorithm.\nThe resulting approximation of the log evidence was correct to 4 decimal places, which\nsuggests that negligible errors were introduced.\n4.2 Inclusion of Hyperpriors\nWe now consider an extension of the above model where all parameters in our model were\nunknown, and we introduce hyperiors for them. This introduces dependence between\nthe segments, and our direct simulation algorithm has to be used with an MCMC\nscheme,\nWe used a uniform prior for p, and an improper prior for \u03c3, pi(\u03c3) \u221d 1\/\u03c3. We parame-\n12\nterised the prior for the segment means as\n\u00b5i \u223c N(\u03b7, \u03c4\n2\u03c32),\nand used improper hyperpriors on \u03b7 and \u03c4 : pi(\u03b7) \u221d 1 and pi(\u03c4) \u221d 1\/\u03c4 .\nWe analysed this model using MCMC. The MCMC algorithm used the following three\nupdates:\n(1) Update the changepoints conditional on \u03c3, p, \u03b7, and \u03c4 . We used an independent\nproposal from the true posterior distribution conditional on \u03c3 = 2, 330, p = 0.013,\n\u03b7 = 115000, and \u03c4 = 4.3.\n(2) Update \u03c3, p and the \u00b5is from their full conditional distribution given the change-\npoints and \u03b7 and \u03c4 .\n(3) Update \u03b7 and \u03c4 from their full-conditionals given the \u00b5is.\nEach of these moves satisfies detailed balance. Steps (2) and (3) are Gibbs steps, and\nthus the proposed values are always accepted. Step (1) is not a Gibbs step. Although\nit would be possible to make it so, there is a substantial overhead to calculating the\nposterior distribution of the changepoints at each iteration. Thus while this algorithm\nmay mix more slowly, a single iteration will be substantially quicker, and hence we hope\nit will be more efficient. In updating the changepoints in step (1) we throw away the\nsegment means. This is an example of collapsing (Liu, 2001, pages 146\u2013151 ), which\nusually improves the mixing of the Markov Chain.\nWe ran this Markov chain for 10,000 iterations. The acceptance probability of step (1)\nwas 61.8%. The 1-lag autocorrelation for each of the parameters was less than 0.03,\nwhich suggests that the chain is mixing extremely quickly.\nThe reason why this MCMC algorithm performs so well is because the posterior proba-\nbility of the parameters is concentrated in a small region of the parameter space. Over\nthis small region, the parameters are almost independent; the maximum absolute value\nof the correlation between any pair of parameters is 0.01. Furthermore, the conditional\ndistribution of the changepoints changes little over this range of parameter values, which\nmeans that the average acceptance probability in step (1) of the algorithm is high. This\nsituation is likely to occur in other situations where there is a large and informative\ndata set with many changepoints.\n13\n4.3 Alternative Models\nThe models used in the previous Sections are based around those previously used in the\nliterature for this data. However the realisations from the posterior distribution have\nmany more changepoints, and thus suggest many more rock strata, than is realistic. It\nappears that the piecewise constant model used is overly simplistic for the data, and\nthat this has resulted in the need for too many changepoints in order to fit the data.\nWe have considered numerous extensions to the model. Two possibilities are: (i) to\nallow different noise variances for different segments; and (ii) to model each segment\nusing a mean-shifted AR(1) model (Albert and Chib, 1993). Both of these models can\nbe analysed via our direct simulation method, though for (ii) we need to numerically\nintegrate out the autoregressive coefficient (this can be done in a similar way to that\ndescribed below). However neither of these extensions enable the data to be fit with\nsubstantially fewer change points (results not shown).\nInstead we consider the following state-space model for the data within a segment,\nwhere if t\u2212 1 and t both lie within segment i\n\u00b5t \u223c N(\u00b5t\u22121, \u03c4\n2\ni )\nyt \u223c N(\u00b5t, \u03c3\n2).\nThe initial \u00b5 value for each segment is drawn from the same independent normal priors\nas before. This is an extension of the piecewise constant model which allows the signal\nwithin a segment to perform a random walk. We allow the variance of the random walk\nto vary among segments, and assume a Gamma prior for \u03c4i with parameters 2 and 1\/40.\nThis prior places most probability mass on values of \u03c4i which lie in the interval [0, 150].\nThe idea of this model is that the random walk element can fit the small-scale variation\nin the underlying signal without the need to infer changepoints.\nIf \u03c4i were known for each segment then it would be straightforward to apply our direct\nsimulation method, using the Kalman Filter (Harvey, 1989) to integrate out the under-\nlying signal. To incorporate a prior on \u03c4i we resort to numerical integration to calculate\nthe P (t, s) values required by our algorithm. A simple, but adequate, approach to nu-\nmerical integration is based on using a grid of \u03c4i values, and we obtained such a grid as\nfollows. For a grid with K points, first simulate for k = 1, . . . ,K, a realisation, uk, of\na uniform random variable on [(k \u2212 1)\/K, k\/K]; then fix the kth grid point to be the\nukth quantile from the prior for \u03c4i.\n14\n12 14 16 18 20 22 24 26 28\n(a)\nNumber of Changepoints\nPo\nst\ner\nio\nr P\nro\nba\nbi\nlity\n0.\n00\n0.\n05\n0.\n10\n0.\n15\n(b)\nTime\nPr\nob\nab\nilit\ny\n0 1000 2000 3000 4000\n0\n1\n0 1000 2000 3000 4000\n10\n00\n00\n12\n00\n00\n14\n00\n00\n(c)\nTime\nN\nuc\nle\nar\n R\nes\npo\nns\ne\n0 1000 2000 3000 4000\n10\n00\n00\n12\n00\n00\n14\n00\n00\n(d)\nTime\nN\nuc\nle\nar\n R\nes\npo\nns\ne\nFigure 3: Results of analysis of the well-log data. using the state-space model: histogram\nof number of changepoints(a); posterior distribution of position of changepoints (b); and\nrealisations from the posterior distribution of the signal (c)\u2013(d). The results for (a) and\n(b) are based on 10,000 perfect simulations from the posterior distributions.\nIn practice we found that a grid of 100 points produced accurate results; and for such\na grid it took less than 19 minutes to simulate 10,000 draws from the joint posterior\ndistribution of changepoint positions on a 3.4GHz PC. The results of the analysis (as-\nsuming \u03c3 = 2, 500 and p = 0.004) are shown in Figure 3. This model gives more realistic\ninferences about the number and positions of the changepoints. Further refinements of\nthe model may be appropriate and could further improve the inferences, for example by\nchoosing a distribution for the segment lengths that does not allow very short segments,\nbut these are not considered here.\n5 Discussion\nWe have described ways in which recursions, based on the Forward-Backward algorithm,\ncan be used to perform Bayesian analysis of multiple changepoint problems. As men-\n15\ntioned previously, the work we present is closely related to work by Barry and Hartigan\n(1992). The main novelty of what we propose is that we demonstrate how the recur-\nsions can be used for perfect simulation from the posterior distribution of the number\nand position of change-points, and hence from the posterior distribution of the param-\neters. Presenting results from a Bayesian analysis via simulations from the posterior\ndistribution is both quicker than calculating the posterior means (as done by Barry\nand Hartigan, 1992, where the cost is cubic in the number of observations), and also\nencapsulates information about uncertainty about parameters, which is one of the ad-\nvantages of Bayesian inference. We have also extended the use of recursions to inference\nconditional on the number of changepoints.\nThe ability to simulate from posterior distributions also enables the algorithms we\npresent to be used in analysing more complex models, for example by embedding our\nalgorithm within an MCMC algorithm (see Section 4). While it may seem natural in\nsuch cases just to use standard MCMC algorithms, the use of direct simulation enabled\nus to construct an MCMC algorithm for the Well-log data that had exceptional mixing\nproperties.\nWhile the main focus of this paper is this new methodology, we have demonstrated\nin Section 4 some of the range of models that can be analysed by our algorithm. For\nthis data, the ability to produce draws from the joint posterior distribution of the\nchangepoint positions, despite there being around 50 changepoints, enables us to see\nsome inadequacies in an existing model. We were able to use out algorithm to analyse\na range of more complicated models, including a state-space model which appears more\nappropriate for the data. For this state-space model it was not possible to integrate\nout analytically all the parameters associated with each segment - however as there was\nonly a single univariate parameter that could not be integrated out analytically, direct\nsimulation was still possible using a simple numerical integration method.\nFinally we have shown how approximations to the set of recursions can be used which\ngreatly reduce the computational expense (particularly for large data sets with lots of\nchangepoints), but with negligible error. In the well-log example the computational\ncost was reduced by an order of magnitude. We imagine that the computational cost\nof this approximate algorithm will increase only linearly with the size of data, and thus\nthe algorithm could be used for analysing very large data sets.\nAcknowledgements I would like to thank Peter Green and two anonymous referees\n16\nfor helpful comments on an earlier version of this paper; and to thank Bill Fitzgerald\nfor sending me the Well-log data.\nAppendix\nTo simulate in linear time a sample of size n from a discrete distribution Pr(\u03c4), which\ntakes values of \u03c4 = 1, 2, . . .:\n1(a) for i = 1, . . . , n+1, simulate xi a realisation from an exponential distribution with\nrate parameter 1;\n1(b) Calculate S =\n\u2211n+1\ni=1 xi;\n1(c) Set u1 = x1\/S and for i = 2, . . . , n ui = ui\u22121 + xi\/S.\n2 Set Q = 0, U = u1, j = 1 and i = 1.\n3 If U < Q+Pr(\u03c4 = j) then output j and set U = U +ui+1 and i = i+1; otherwise\nset Q = Pr(\u03c4 = j) and j = j + 1. Repeat until i = n+ 1.\nReferences\nAlbert, J. H. and Chib, S. (1993). Bayes inference via Gibbs sampling of autoregressive\ntime series subject to Markov mean and variance shifts. Journal of Business and\nEconomic Statistics 11, 1\u201315.\nBarry, D. and Hartigan, J. A. (1992). Product partition models for change point prob-\nlems. The Annals of Statistics 20, 260\u2013279.\nBarry, D. and Hartigan, J. A. (1993). A Bayesian analysis for change point problems.\nJournal of the American Statistical Society 88, 309\u2013319.\nBraun, J. V. and Muller, H. G. (1998). Statistical methods for DNA sequence segmen-\ntation. Statistical Science 13, 142\u2013162.\nBraun, J. V., Braun, R. K. and Muller, H. G. (2000). Multiple changepoint fitting\nvia quasilikelihood, with application to DNA sequence segmentation. Biometrika 87,\n301\u2013314.\nBrooks, S. P., Giudici, P. and Roberts, G. O. (2003). Efficient construction of reversible\njump Markov chain Monte Carlo proposal distributions. Journal of the Royal Statis-\ntical Society, series B 65, 3\u201339.\n17\nCarlin, B. P., Gelfand, A. E. and Smith, A. F. M. (1992). Hierarchical Bayesian analysis\nof changepoint problems. Applied Statistics 41, 389\u2013405.\nCarpenter, J., Clifford, P. and Fearnhead, P. (1999). An improved particle filter for\nnon-linear problems. IEE proceedings-Radar, Sonar and Navigation 146, 2\u20137.\nChen, J. and Gupta, A. K. (1997). Testing and locating changepoints with application\nto stock prices. Journal of the American Statistical Association 92, 739\u2013747.\nFearnhead, P. (1998). Sequential Monte Carlo methods in filter theory . Ph.D. thesis,\nOxford Unversity, available from http:\/\/www.maths.lancs.ac.uk\/\u223cfearnhea\/.\nFearnhead, P. (2004). Direct simulation for mixture distributions: com-\nponent weights and discrete distributions. submitted Available from\nhttp:\/\/www.maths.lancs.ac.uk\/\u223cfearnhea\/.\nFearnhead, P. and Clifford, P. (2003). Online inference for hidden Markov models.\nJournal of the Royal Statistical Society, Series B 65, 887\u2013899.\nFearnhead, P. and Meligkotsidou, L. (2004). Exact filtering for partially-observed\ncontinuous-time Markov models. Journal of the Royal Statistical Society, series B.\nTo appear Available from http:\/\/www.maths.lancs.ac.uk\/\u223cfearnhea\/.\nGreen, P. (1995). Reversible jumpMarkov chain Monte Carlo computation and Bayesian\nmodel determination. Biometrika 82, 711\u2013732.\nGreen, P. J. (2003). Trans-dimensional Markov chain Monte Carlo. In: Highly Struc-\ntured Stochastic Systems (eds. P. J. Green, N. L. Hjort and S. Richardson), Oxford\nUniversity Press.\nHartigan, J. A. (1990). Partition models. Communications in Statistics 19, 2745\u20132756.\nHarvey, A. C. (1989). Forecasting, stuctural time series and the Kalman filter . Cam-\nbridge University Press, Cambridge, UK.\nJohnson, T. D., Elashoff, R. M. and Harkema, S. J. (2003). A Bayesian change-point\nanalysis of electromyographic data: detecting muscle activation patterns and associ-\nated applications. Biostatistics 4, 143\u2013164.\nLiu, J. S. (2001). Monte Carlo strategies in scientific computing . New York: Springer.\n18\nLund, R. and Reeves, J. (2002). Detection of undocumented changepoints: A revision\nof the two-phase regression model. Journal of Climate 15, 2547\u20132554.\nO\u00b4 Ruanaidh, J. J. K. and Fitzgerald, W. J. (1996). Numerical Bayesion Methods Applied\nto Signal Processing . New York: Springer.\nPievatolo, A. and Green, P. J. (1998). Boundary detection through dynamic polygons.\nJournal of the Royal Statistical Society, Series B 60, 609\u2013626.\nPropp, J. G. and Wilson, D. B. (1996). Exact sampling with coupled Markov chains and\napplications to statistical mechanics. Random Structures and Algorithms 9, 223\u2013252.\nPunskaya, E., Andrieu, C., Doucet, A. and Fitzgerald, W. J. (2002). Bayesian curve\nfitting using MCMC with applications to signal segmentation. IEEE Transactions on\nSignal Processing 50, 747\u2013758.\nRaftery, A. E. and Akman, V. E. (1986). Bayesian analysis of a Poisson process with a\nchange-point. Biometrika 73, 85\u201389.\nRitov, Y., Raz, A. and Bergman, H. (2002). Detection of onset of neuronal activity\nby allowing for heterogeneity in the change points. Journal of Neuroscience Methods\n122, 25\u201342.\nScott, S. L. (2002). Bayesian methods for hidden Markov models: Recursive computing\nin the 21st century. Journal of the American Statistical Association 97, 337\u2013351.\nWorsley, K. J. (1979). On the likelihood ratio test for a shift in location of normal\npopulations. Journal of the American Statistical Association 74, 363\u2013367.\nYang, T. Y. and Kuo, L. (2001). Bayesian binary segmentation procedure for a Poisson\nprocess with multiple changepoints. Journal of Computational and Graphical Statis-\ntics 10, 772\u2013785.\nYao, Y. (1984). Estimation of a noisy discrete-time step function: Bayes and empirical\nBayes approaches. The Annals of Statistics 12, 1434\u20131447.\n19\n"}