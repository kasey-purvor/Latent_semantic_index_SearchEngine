{"doi":"10.1007\/11523468_112","coreId":"66631","oai":"oai:dro.dur.ac.uk.OAI2:658","identifiers":["oai:dro.dur.ac.uk.OAI2:658","10.1007\/11523468_112"],"title":"Dynamic diffusion load balancing.","authors":["Berenbrink,  P.","Friedetzky,  T.","Martin,  R."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":["Caires,  L.","Italiano,  G. F.","Monteiro,  L.","Palamidessi,  C.","Yung,  M."],"datePublished":"2005-07","abstract":"We consider the problem of dynamic load balancing in arbitrary (connected) networks on n nodes. Our load generation model is such that during each round, n tasks are generated on arbitrary nodes, and then (possibly after some balancing) one task is deleted from every non-empty node. Notice that this model fully saturates the resources of the network in the sense that we generate just as many new tasks per round as the network is able to delete. We show that even in this situation the system is stable, in that the total load remains bounded (as a function of n alone) over time. Our proof only requires that the underlying \u201ccommunication\u201d graph be connected. (It of course also works if we generate less than n new tasks per round, but the major contribution of this paper is the fully saturated case.) We further show that the upper bound we obtain is asymptotically tight (up to a moderate multiplicative constant) by demonstrating a corresponding lower bound on the system load for the particular example of a linear array (or path). We also show some simple negative results (i.e., instability) for work-stealing based diffusion-type algorithms in this setting","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/66631.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/658\/1\/658.pdf","pdfHashValue":"e5977a62f541f99abe40d2c635d88b1a83627758","publisher":"Springer","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:658<\/identifier><datestamp>\n      2015-03-31T11:37:56Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Dynamic diffusion load balancing.<\/dc:title><dc:creator>\n        Berenbrink,  P.<\/dc:creator><dc:creator>\n        Friedetzky,  T.<\/dc:creator><dc:creator>\n        Martin,  R.<\/dc:creator><dc:description>\n        We consider the problem of dynamic load balancing in arbitrary (connected) networks on n nodes. Our load generation model is such that during each round, n tasks are generated on arbitrary nodes, and then (possibly after some balancing) one task is deleted from every non-empty node. Notice that this model fully saturates the resources of the network in the sense that we generate just as many new tasks per round as the network is able to delete. We show that even in this situation the system is stable, in that the total load remains bounded (as a function of n alone) over time. Our proof only requires that the underlying \u201ccommunication\u201d graph be connected. (It of course also works if we generate less than n new tasks per round, but the major contribution of this paper is the fully saturated case.) We further show that the upper bound we obtain is asymptotically tight (up to a moderate multiplicative constant) by demonstrating a corresponding lower bound on the system load for the particular example of a linear array (or path). We also show some simple negative results (i.e., instability) for work-stealing based diffusion-type algorithms in this setting.<\/dc:description><dc:subject>\n        Network<\/dc:subject><dc:subject>\n         Algorithm<\/dc:subject><dc:subject>\n         Load generation model.<\/dc:subject><dc:publisher>\n        Springer<\/dc:publisher><dc:source>\n        Caires,  L. & Italiano,  G. F. & Monteiro,  L. & Palamidessi,  C. & Yung,  M. (Eds.).  Automata, languages and programming : 32nd International Colloquium, ICALP 2005, 11-15 July 2005, Lisbon, Portugal ; proceedings . Berlin: Springer, pp. 1386-1398, Lecture notes in computer science(3580)<\/dc:source><dc:contributor>\n        Caires,  L.<\/dc:contributor><dc:contributor>\n        Italiano,  G. F.<\/dc:contributor><dc:contributor>\n        Monteiro,  L.<\/dc:contributor><dc:contributor>\n        Palamidessi,  C.<\/dc:contributor><dc:contributor>\n        Yung,  M.<\/dc:contributor><dc:date>\n        2005-07<\/dc:date><dc:type>\n        Book chapter<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:658<\/dc:identifier><dc:identifier>\n        issn:0302-9743<\/dc:identifier><dc:identifier>\n        issn: 1611-3349<\/dc:identifier><dc:identifier>\n        doi:10.1007\/11523468_112<\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/658\/<\/dc:identifier><dc:identifier>\n        http:\/\/dx.doi.org\/10.1007\/11523468_112<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/658\/1\/658.pdf<\/dc:identifier><dc:rights>\n        The final publication is available at Springer via http:\/\/dx.doi.org\/10.1007\/11523468_112<\/dc:rights><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["0302-9743"," 1611-3349","issn: 1611-3349","issn:0302-9743"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2005,"topics":["Network","Algorithm","Load generation model."],"subject":["Book chapter","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n04 November 2008\nVersion of attached file:\nAccepted Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nBerenbrink, P. and Friedetzky, T. and Martin, R. (2005) \u2019Dynamic diffusion load balancing.\u2019, in Automata,\nlanguages and programming : 32nd International Colloquium, ICALP 2005, 11-15 July 2005, Lisbon, Portugal\n; proceedings. Berlin: Springer, pp. 1386-1398. Lecture notes in computer science. (3580).\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1007\/11523468112\nPublisher\u2019s copyright statement:\nThe original publication is available at www.springerlink.com\nAdditional information:\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\nDynamic Diffusion Load Balancing\nPetra Berenbrink1, Tom Friedetzky2, and Russell Martin3 ?\n1 School of Computing Science\nSimon Fraser University\nBurnaby, B.C., V5A 1S6, Canada\n2 Department of Computer Science\nUniversity of Durham\nDurham, DH1 3LE, U.K.\n3 Department of Computer Science\nUniversity of Warwick\nCoventry, CV4 7AL, U.K.\nAbstract. We consider the problem of dynamic load balancing in arbitrary (connected) networks on n nodes. Our\nload generation model is such that during each round, n tasks are generated on arbitrary nodes, and then (possibly\nafter some balancing) one task is deleted from every non-empty node. Notice that this model fully saturates the\nresources of the network in the sense that we generate just as many new tasks per round as the network is able to\ndelete. We show that even in this situation the system is stable, in that the total load remains bounded (as a function\nof n alone) over time. Our proof only requires that the underlying \u201ccommunication\u201d graph be connected. (It of\ncourse also works if we generate less than n new tasks per round, but the major contribution of this paper is the\nfully saturated case.) We further show that the upper bound we obtain is asymptotically tight (up to a moderate\nmultiplicative constant) by demonstrating a corresponding lower bound on the system load for the particular exam-\nple of a linear array (or path). We also show some simple negative results (i.e., instability) for work-stealing based\ndiffusion-type algorithms in this setting.\n1 Introduction\nThe use of parallel and distributed computing is established in many areas of science, technology, and\nbusiness. One of the most crucial parameters of parallel machines is the efficient utilisation of resources.\nOf greatest importance here is an even distribution of the workload among the processors. In particular\napplications exposing some kind of \u201cirregularity\u201d require the use of a load balancing mechanism.\nA well known and much studied load balancing approach is the so-called diffusion load balancing, first\nintroduced by Cybenko and Boillat ([11], [10]). The algorithm works in synchronised rounds. The basic idea\nis that in every round, every processor p balances load with all its neighbours (independently, i.e., pair-wise).\nLet `p be the load of p, `q the load of some of p\u2019s neighbour q, and let \u2206 denote the maximum degree of\nthe underlying graph. In the discrete setting, p transfers max{0, b(`p \u2212 `q)\/(\u2206 + 1)c} tasks to q. Some of\nmany advantages of diffusion-type algorithms are the locality (no global knowledge regarding the overall\nload situation, or, in fact, anything except the strict neighbourhood of any vertex is needed), its simplicity,\nand its neighbourhood preservation (tasks tend to stay close to the processors where they are generated,\nwhich may help to maintain small communication overhead).\nThe diffusion load balancing algorithm has been thoroughly analysed for static scenarios, where each\nprocessor has some initial number of tasks, and the objective is to distribute this load evenly among the\nprocessors as quickly as possible. Much work has been done under the assumption that every edge is only\nallowed to forward one task per round [16, 17, 19] or when a constant number of tasks can be passed by each\nprocessor [15]. We refer to these scenarios as token distribution problems. In addition [12\u201314] have studied\nthe diffusion algorithm where tasks can be split arbitrarily.\n? A portion of this work was performed during a visit to the School of Computing Science at Simon Fraser University. Supported\nin part by the EPSRC grant \u201cDiscontinuous Behaviour in the Complexity of Randomized Algorithms\u201d.\nIn contrast to the static case of load balancing and token distribution, in the dynamic setting during\neach time step new tasks are generated (in some manner) on the set of processors, load is balanced amongst\nneighbours, then tasks are deleted from non-empty processors.\nMuch of the past work has studied the dynamic token distribution problem. Muthukrishnan and Rajara-\nman [18] studied a dynamic version where processors can forward a single task in each round. They assume\nan adversarial load generation model. The adversary is allowed to generate and to delete tokens from the\nnetwork in every round. The simple and elegant algorithm they consider is due to [2]: A node sends a task\nto its neighbour if the load difference between them is at least 2\u2206 + 1. They show that the system is stable\nif the load change in every subset S of the nodes minus a|S| is at most (1 \u2212 \u000f)e(S) for \u000f > 0. Here e(S)\nis the number of outgoing edges of S and a is the change in the average load. Their system is said to be\nstable if the deviation of the load of any processor from the average load can be bounded. Muthukrishnan\nand Rajaraman left open the question whether the system is also stable for \u000f = 0.\nAnshelevich, Kempe, and Kleinberg [4] gave a positive result for token distribution when \u000f = 0. They\nshowed that under the above load generation model no processor has more than average load\u00b1(2\u2206+ 1) \u00b7n.\nAnshelevich, et. al. also showed how their result can be generalized for edges that can forward c tokens per\ntime step. A node sends min{c, \u03c1} tasks to its neighbour if the load difference is at least 2\u2206c + \u03c1. In this\nsetting no processor has more than average load\u00b1(2\u2206+ 1)c \u00b7n as long as the load change in every subset S\nof the nodes minus a|S| is at most c \u00b7 e(S). Additionally, they showed that a generalisation of the algorithm\nis stable for two distinct types of jobs, and they extended their results to related flow problems.\nIn [6, 7] Awerbuch and Leighton use a variant of the token distribution model under the assumption\nthat tokens can be split into arbitrarily sized parts. They use a \u201cbalancing\u201d algorithm to approximate the\nmulti-commodity flow problem with capacitated edges. Their method is an iterative approach where flow\nis queued at the vertices of the graph. In each step, the commodity which has the largest excess is shipped\nfrom one vertex to another, and then new flow is injected into the system. In this balancing process, edge\ncapacities must always be respected. These edge capacities are analogous to the restrictions on the number\nof tasks that can be passed over any single edge in the token distribution problems. Furthermore, their model\ndoes not actually allow full use of those edge capacities, which is similar to the case in [18] where \u000f > 0\nwas required to ensure stability. The work in [1] and [5] expands the results of Awerbuch and Leighton for\npacket routing, but again in these cases only a constant number of tasks can be moved across any edge in a\nsingle time step.\nClearly the condition that processors can forward only a single task (or a constant number) per edge in\neach round significantly restricts the number and distribution of tasks that can be generated on (or deleted\nfrom) processors in each round and still obtain a stability result. Thus, in the results of [18] and [4] some\ndependence on the quantity e(S) (or some measure of the \u201cedge expansion\u201d) is to be expected.\nAnagnostopoulos et. al. [3] consider the setting where there are no restrictions on the number of tasks\nbalanced between processors in a time step, and they allow a broad range of injection models. Their protocol\nis similar to that studied in [15] for a static setting, but is not the typical diffusion load balancing procedure.\nIn their setting, in each step nodes are matched randomly with adjacent neighbours and matched nodes\nequalise their load. Hence, every processor is only involved in a single load balancing action. They show\nthat the system is stable as long as at most wn\u03bb tasks (in expectation) are generated in a time interval of\nlengthw, where \u03bb < 1. Their proof method unfortunately cannot be generalized to the case of full saturation\nwhen \u03bb = 1, which is the main focus of this paper.\n1.1 Our Results\nIn this paper we present the first analysis of the simple diffusion scheme for the dynamic load balancing\nproblem that allows full saturation of the resources. We assume that n new tasks are generated per round\nand, after load balancing, every non-empty processor deletes one task each round. (With small modifications\nour proofs will carry through to the case when we generate at most n tasks per round.) In contrast to [4] and\n[18], the newly generated tasks may be arbitrarily distributed among the nodes of the network, regardless of\nany \u201cedge expansion\u201d type of condition as in those models. For example, the tasks may always be generated\non the same processor, or all tasks may be generated on one processor but the processor can change from\nround to round, or alternatively, the tasks may be allocated at random each round. Note that, obviously,\nwithout load balancing the total number of tasks in the system may grow unboundedly with time (in the\nworst case, we generate n new tasks per step but delete only one).\nWe show that the system of processors is stable under the diffusion load balancing scheme and the\ngeneration model described above. By stable, we mean that the total load in the system does not grow with\ntime. In particular, we show that the total system load can be upper-bounded by O(\u2206n3), where \u2206 denotes\nthe maximum degree of the network. Furthermore, we present a simple, asymptotically matching lower\nbound when the network is a path.\nOur technique also captures a different scenario, similar to that in [4, 18], where stability is defined in\nterms of deviation of any processor\u2019s load from the average. In this scenario we have two separate phases, the\nfirst where tasks are generated on and\/or deleted from nodes, and the second where tasks are then balanced\namongst nodes. Let L\u00aft(S) denote the total load of the nodes in the set S after the task generation\/deletion\nphase, and Lt(S) denote the total load of S after the balancing step at time t. Assume that the genera-\ntion\/deletion phase satisfies the following condition:\n(L(S)t \u2212 L\u00af(S)t\u22121) \u2264 (avg(t)\u2212 avg(t\u2212 1)) \u00b7 |S|+ \u03c1\nwhere avg(t) denotes the average system load in step t. Then the total load of S can be bounded by |S| \u00b7\navg(t) + 5\u2206n\u03c1.\nFor both proofs of our results we use a potential function. Although the potential function we use looks\nsimilar to the one used in [4], the proof technique is very different. The proof method in [4] very much relies\nupon the restriction of their generation\/deletion model, where the number of tasks inserted into\/deleted from\na set S is bounded by a function of e(S), the number of edges that join the set S to its complement S\u00af. This,\ntogether with the bounded capacities on the edges of the graph, allows for a direct analysis of how the loads\nof sets might change in a single step of their process. The arbitrary distribution of tasks in our generation\nmodel and the unrestricted capacity of the edges in our network (i.e. unknown bounds on load transferred\ninto a set S in a single step) does not allow us to directly obtain similar results, so we need a different proof\nto show stability under our model.\nAnother approach to show our results would be to demonstrate an upper bound on the number of tasks\nthat can be moved over a single edge during any time step of our algorithm. If this is possible, the results\nin [4] could then be used to prove stability under either model that we have described. However, the authors\nof this paper feel that showing this result is not easier than the proof method we used, especially in the\nsecond model where the number of tasks inserted into the system at any time can be unbounded.\nIn the final part of our paper we discuss a different method of load balancing, one which is commonly\nreferred to as work stealing. In this framework, processors that are empty after task generation will balance\nwith processors that are not empty, but no other balancing actions are permitted.\nWe show that for this work-stealing protocol there are graphs for which the system cannot be stable for\na significant class of generation parameters. These results show that restricting balancing actions to empty\nprocessors is not sufficient in general.\nIn contrast, Berenbrink, Friedetzky, and Goldberg [8] showed stability of a work stealing algorithm\nunder a load generation model that is similar to many of those already mentioned. They consider a flexible\ndistribution of n generators among the nodes of the network, where each generator is allowed to generate\na task with probability strictly smaller than one. In this setting a very simple, parameterized work-stealing\nalgorithm achieves stability (in our sense) for a wide range of parameters. The important point to note is that\ntheir model applies only when the set of processors (and their communication linkages) forms a complete\ngraph, and their results only hold for the case where strictly less than n tasks (in expectation) are generated\nduring any time step.\nOur model is defined in the next section, and the formal definition of the diffusion approach to load\nbalancing is given following that.\n1.2 Our Model\nOur parallel system is modelled by a connected graph G = (V,E). The nodes V of the graph model\nour processors P = {P1, . . . , Pn} , and the edges E model the underlying communication structure. If two\nnodes are connected with each other, this means that the processors modelled by the nodes can communicate\ndirectly. For us, this means that they are allowed to exchange tasks. Nodes not connected by an edge have\nto communicate via message passing. Furthermore, let \u2206 be the maximum degree of the graph. We assume\nthat each processor maintains a queue in which yet-to-be-processed tasks are stored. One round looks as\nfollows:\n1. n generators are arbitrarily distributed over the processors, and each generator generates one task at the\nbeginning of every time round. For 1 \u2264 i \u2264 n, let kti = j if generator i is allocated to processor Pj in\nround t, and kti = 0 if the generator is not allocated to any processor in that round.\n2. Every processor balances its load with some or all its neighbours in the network (according to a well-\ndefined scheme for doing this operation).\n3. Every non-empty processor deletes one task.\nLet \u02c6`ti be the load of Pi directly after the load deletion phase in round t. A system is called stable if the\nnumber of tasks L\u02c6t(P) = \u2211ni=1 \u02c6`ti that are in the system at the end of round t does not grow with time, i.e.\nthe total load L\u02c6t(P) is bounded by a number that might depend on n, but not on the time t.\nWe will mainly focus on one load balancing method called the diffusion approach. Every processor is\nallowed to balance its load with all its neighbours. As mentioned previously, we briefly consider a second ap-\nproach in Section 4 where only empty processors are allowed to take load from their non-empty neighbours.\nWe call this second method the work stealing approach.\nDiffusion approach. We begin with a detailed description of the first approach, an integral variant of the\nFirst Order Diffusion scheme. Let \u00af`ti be the load of processor Pi directly before the load balancing phase,\nand `ti the load directly after the load balancing phase. Let \u03b1\nt\ni,j be the load that is to be sent from Pi to Pj in\nround t for (i, j) \u2208 E (\u03b1ti,j = 0 otherwise). Then \u03b1i,j and `i are calculated as follows:\n\u03b1ti,j := max\n{\n0,\n\u230a\n\u00af`t\ni \u2212 \u00af`tj\n2\u2206\n\u230b}\n`ti := \u00af`\nt\ni \u2212\n\u2211\n(i,j)\u2208E\n\u03b1ti,j +\n\u2211\n(j,i)\u2208E\n\u03b1tj,i\nTo compute \u02c6`ti, the load of processor Pi after load deletion, it remains to subtract one if `\nt\ni > 0, thus\n\u02c6`t\ni := max{0, `ti \u2212 1}.\nNote that the \u201cstandard\u201d diffusion approach divides \u00af`ti\u2212 \u00af`tj by \u2206+ 1 instead of 2\u2206. We need the 2\u2206 for\nour analysis.\nWe will now very briefly introduce our contributions. In Section 2, we prove Theorem 1, which states\nthat we can upper-bound the total system load by 3\u2206n3. This generalizes the results of [4] to the case of\nunbounded edge capacities and, hence, analyses the standard diffusion approach.\nTheorem 3 in Section 3 provides an asymptotically matching lower bound, showing that our upper bound\nis tight, up to a multiplicative constant.\nIn Section 4 we discuss the problem of combining the diffusion-approach with the work-stealing ap-\nproach and show that certain assumptions necessarily lead to instability.\n2 Analysis of the Dynamic Diffusion Algorithm\nIn this section we will show that the diffusion approach yields a stable system. Moreover, we are able to\nupper bound the maximum load that will be in the system by O(\u2206n3). Throughout, we assume that n \u2265 2\nand \u2206 \u2265 2.\nIn order to clarify the exposition, we first recall the notation we have already defined:\n\u2013 \u00af`ti denotes the load of processor Pi after we have generated tasks at the start of round t, but before load\nis balanced,\n\u2013 `ti is the load of processor Pi immediately after the load balancing phase, and\n\u2013 \u02c6`ti is the load of processor Pi after the task deletion phase of round t (i.e. at the very end of round t).\n\u2013 We will also use notation like L\u00aft(S) =\n\u2211\ni:Pi\u2208S\n\u00af`t\ni for a subset S \u2286 P , with similar definitions for\nLt(S) and L\u02c6t(S).\nWith this notation, our main result about the diffusion approach to load balancing is\nTheorem 1. Let n \u2265 2 denote the number of processors in the system, and an upper bound on the number\nof tasks that are generated during each time round. Let \u2206 \u2265 2 denote the maximum degree of the graph G\nthat specifies the communication linkages in the network. Then, starting with an empty system, for all t \u2265 1\nwe have\nL\u02c6t(P) =\nn\u2211\ni=1\n\u02c6`t\ni \u2264 3\u2206n3.\nWe will prove this theorem by first giving a series of preliminary results. The proof of Theorem 1 uses\na similar potential function as the one that was used in [4] (though what follows is very different). This idea\nis to prove an invariant that for all t \u2265 1, every subset S \u2286 P satisfies the following inequality:\nL\u02c6t(S) \u2264\nn\u2211\ni=n\u2212|S|+1\ni \u00b7 (4\u2206) \u00b7 n. (1)\nThen, Inequality (1) will immediately imply Theorem 1 (by taking S = P).\nWe will often have occasion to refer to the right hand side of Inequality (1) for many sets, so to make\nour proofs that follow easier to read, we define the following function f : {1, . . . , n} \u2192 N in this way\nf(k) =\nn\u2211\ni=n\u2212k+1\ni \u00b7 (4\u2206) \u00b7 n. (2)\nDefinition 1. In what follows, we will refer to sets as being bad after load generation in round t, or after\nthe load balancing phase of round t, etc., meaning that the load of the set at that particular time violates\nInequality (1). For example, if we say that a set S is bad after load generation in round t, we mean that\nL\u00aft(S) > f(|S|).\nConversely, we will also refer to a set as being good (after load generation, or load balancing, etc.) if it\nsatisfies Inequality (1) (at the time in question).\nThe first lemma states that if we consider any (non-empty) set S at the end of round t, there must have\nexisted a set S\u2032 so that the load of S\u2032 before load balancing was at least as large as the load of S after load\nbalancing, i.e. L\u00aft(S\u2032) \u2265 Lt(S) \u2265 L\u02c6t(S). The fact that might not be obvious is that we can assert that the\ntwo sets contain the same number of processors. This is the statement of the following lemma.\nLemma 1. Let \u2205 6= S \u2286 P denote an arbitrary subset of processors. There exists a set |S\u2032| such that\n1. |S\u2032| = |S|, and\n2. L\u00aft(S\u2032) \u2265 Lt(S).\nProof. The claim is clear if S = P , since in this case we have Lt(P) \u2265 L\u02c6t(P) and L\u00aft(P) = Lt(P). Taking\nS\u2032 = P then satisfies the conclusions of the theorem.\nSo we suppose that S is not the entire set of processors. In this case let Sin = {v : v \u2208 S and \u2203w 6\u2208\nS such that \u03b1twv > 0)}. In other words, Sin is the subset of S consisting of processors that received tasks\nfrom outside of S during load balancing.\nCase 1: Sin = \u2205. This case is essentially the same as when S = P . Since no processors in S received load\nfrom outside of S, the elements of S can only exchange load among themselves or send load to processors\noutside of S. Then it is clear that L\u00aft(S) \u2265 Lt(S), so taking S\u2032 = S again satisfies the desired conclusions.\nCase 2: Sin 6= \u2205. Let R = {w : w 6\u2208 S and \u2203v \u2208 Sin such that \u03b1twv > 0}. In other words, R is the set of\nnodes not in S that pushed tasks into S during load balancing. The main idea of what follows is that we are\ngoing to swap some elements of R for elements of Sin on a one-for-one basis to find the set S\u2032 we desire.\nMore formally, let Lin =\n\u2211\nw\u2208R,v\u2208Sin \u03b1\nt\nwv denote the total flow from R to S during load balancing. We\naim to find sets R1 \u2286 R and S1 \u2286 Sin with\n1. |R1| = |S1|, and\n2. L\u00aft(R1) \u2265 Lt(S1) + Lin + (flow from S1 to S\\S1).\nThen we will take S\u2032 = S\\S1\u222aR1. Our choice of the setR1 guarantees that S\u2032 will satisfy L\u00aft(S\u2032) \u2265 Lt(S),\nsince the elements of R1 account for all flow that enters S during load balancing, plus all flow that passes\nfrom elements in S1 to elements in S\\S1 as well.\nTo do this, let E1 = {(w, v) : w \u2208 R, v \u2208 Sin, \u03b1tvw > 0}. Consider an edge e1 = (w1, v1) \u2208 E0 where\n\u03b1te1 is largest. Then, from the definition of \u03b1\nt\nwv, we see that \u00af`\nt\nw1 \u2265 2\u2206\u03b1tw1v1+\u00af`tv1 . The key observation is that\nby choosing the largest edge, the expression \u00af`tw1 accounts for all possible load that v1 could have received\nduring load balancing, and all tasks that w1 pushes into the set S too (and any tasks that v1 might happen to\npass to other elements in S, since this is counted in the term \u00af`tv1). We set R1 := {w1} and S1 := {v1}, and\nE2 = E1\\ ({(w1, v\u2032) : v\u2032 \u2208 Sin} \u222a {(w\u2032, v1) : w\u2032 \u2208 R}).\nThen, we iteratively apply this argument, namely take a largest edge e2 = (w2, v2) \u2208 E2. (Note that\nw2 6= w1 and v2 6= v1.) The choice of largest edge then allows us to swap w2 for v2, again accounting for\nall tasks that w2 pushes into S during load balancing, all tasks that v2 receives, and any tasks that v2 passes\nto other elements in S. Then, we add w2 to R1, i.e. R1 := R1\u222a{w2}, add v2 to S1, so S1 := S1\u222a{v2}, and\ndelete the appropriate set of edges from E1. Thus, E2 = E1\\ ({(w2, v\u2032) : v\u2032 \u2208 Sin} \u222a {(w\u2032, v2) : w\u2032 \u2208 R}).\nWe continue to iterate this procedure, selecting an edge with largest \u03b1twv value, and performing an\nexchange as before, until we finish step k with a set Ek = \u2205. It is possible that this procedure terminates\nat a step when R1 = R or S1 = Sin (or both), or with one or both of R1, S1 being proper subsets of their\nrespective sets. In any case, we have constructed sets R1 and S1 (each with k \u2264 min{|Sin|, |R|} elements),\nso that by taking S\u2032 = (S\\S1) \u222aR1, this set S\u2032 satisfies the two conditions of the theorem.\n\u00bfFrom the previous lemma, we see that we have proven an inequality about the load of the sets of highest\nloaded processors, before and after load balancing (which, of course, need not be equal to each other). Thus\nwe can conclude the following result:\nCorollary 1. For i \u2208 [n], let M\u00af ti denote a set of i largest loaded processors before load balancing (in\nround t). Also let M ti denote a corresponding set of i largest loaded processors after load balancing. Then\nL\u00aft(M\u00af ti ) \u2265 Lt(M ti ).\nWe also conclude another result from Lemma 1.\nCorollary 2. Fix i \u2208 {1, . . . , n}. Suppose that every subset with i processors is good after the load gener-\nation phase of round t. Then, after the load balancing phase (and thus after the task deletion phase), every\nsubset with i processors is still good. (Of course, provided that M\u00af ti is good after load generation, we actually\nget the same conclusion from Corollary 1.)\nOur next result tells us that if a set is made bad by load generation, then the load balancing and deletion\nphases are sufficient to make that set good again.\nLemma 2. Suppose that at the end of round t, every set S \u2286 P satisfies (1). Further, suppose that after the\nload generation phase in round t + 1, there is some set S \u2286 P such that L\u00aft+1(S) > f(|S|). Then, at the\nend of round t+ 1, S again satisfies Inequality (1).\nProof. If there is more than one set S such that L\u00aft+1(S) > f(|S|), we may apply the argument that follows\nto each, so we fix one of the possible sets S. Suppose that x \u2208 {1, . . . n} denotes the number of tasks that\nwere injected into this set during load generation in round t+ 1.\nWe first show that\nif Pj \u2208 S then \u00af`t+1j \u2265 (n\u2212 |S|+ 1)(4\u2206)n\u2212 x. (3)\nIn the case when S = {Pi} for some i (that is, |S| = 1), this statement is clear, since we must have\n\u00af`t\ni > n(4\u2206)n to violate Inequality (1).\nWhen |S| \u2265 2 we can prove (3) by contradiction. So assume that some Pj \u2208 S satisfies \u00af`t+1j < (n\u2212|S|+\n1)(4\u2206)n\u2212 x. Since S was good before load generation, but not after, we know that L\u00aft+1(S)\u2212 f(|S|) > 0.\nThen, using that L\u00aft+1(S\\Pj) = L\u00aft+1(S)\u2212 \u00af`t+1j , and our assumption on \u00af`t+1j , we conclude\nL\u00aft+1(S\\Pj) > L\u00aft+1(S)\u2212 (n\u2212 |S|+ 1)(4\u2206)n+ x\nL\u00aft+1(S\\Pj)\u2212 f(|S\\Pj |) > L\u00aft+1(S)\u2212 f(|S\\Pj |)\u2212 (n\u2212 |S|+ 1)(4\u2206)n+ x\nL\u00aft+1(S\\Pj)\u2212 f(|S\\Pj |) > L\u00aft+1(S)\u2212 f(|S|) + x > x.\nSince we injected x tasks into S during the load generation phase of round t + 1, we know that\nL\u00aft+1(S\\Pj) \u2264 L\u02c6t(S\\Pj) + x. Putting this together with our last inequality above, we see that\nL\u02c6t(S\\Pj) + x\u2212 f(|S\\Pj |) \u2265 L\u00aft+1(S\\Pj)\u2212 f(|S\\Pj |) > x\n=\u21d2 L\u02c6t(S\\Pj)\u2212 f(|S\\Pj |) > 0.\nThis is a contradiction to the assumption stated in the hypothesis that all sets satisfied (1) at the end of round\nt. Hence, we conclude what we wanted to show, namely Inequality (3).\nIf S = P , then our lemma follows immediately. In this case, the lower bound in (3) is also a lower\nbound on the load of each processor after the load balancing phase, i.e. `ti \u2265 (4\u2206)n\u2212n > 0 for all Pi (since\nx = n when S = P). Thus, each processor will delete one task during the deletion phase. Since we injected\nat most n tasks into the system and deleted n tasks, the set S = P again satisfies (1), and we are done.\nSo, we now assume that S 6= P . Then, in a similar manner as before, we can show\nif Pj 6\u2208 S, then \u00af`t+1j \u2264 (n\u2212 |S|)(4\u2206)n+ n. (4)\nTo see this, again assume the contrary, so that some Pj 6\u2208 S satisfies \u00af`t+1j > (n\u2212 |S|)(4\u2206)n+ n. Then we\nhave the following inequalities\nL\u02c6t(S \u222a Pj) + n \u2265 L\u00aft+1(S \u222a Pj) (5)\nL\u00aft+1(S \u222a Pj)\u2212 f(|S \u222a Pj |) > L\u00aft+1(S)\u2212 f(|S|) + n. (6)\nInequality (5) holds simply because we insert n tasks into the system, and Inequality (6) follows by\nbreaking up the difference on the left hand side into constituent parts, and using our assumption about \u00af`t+1j .\nThese inequalities together imply\nL\u02c6t(S \u222a Pj)\u2212 f(|S \u222a Pj |) + n \u2265 L\u00aft+1(S)\u2212 f(|S|) + n (7)\nL\u02c6t(S \u222a Pj)\u2212 f(|S \u222a Pj |) \u2265 L\u00aft+1(S)\u2212 f(|S|) > 0. (8)\nThe final inequality in (8) comes from our assumption that L\u00aft+1(S) > f(|S|). Of course, (8) violates the\nhypothesis of the theorem stating that all sets satisfied Inequality (1) at the end of round t. Hence, we obtain\nthe upper bound on the load of elements not in S, as expressed in (4).\nThe rest of this lemma is a simple calculation. We first note that no load will be passed from P\\S into S\nduring the load balancing phase because of the load differences in the processors. Then, since our network\nG is connected, there must be an edge (i, j) with Pi \u2208 S and Pj 6\u2208 S. Using our bounds (3) and (4) for \u00af`ti\nand \u00af`tj , respectively, we find that\n\u03b1t+1ij \u2265\n\u00af`t+1\ni \u2212 \u00af`t+1j\n2\u2206\n\u2212 1 \u2265 4\u2206n\u2212 n\u2212 x\n2\u2206\n\u2212 1 \u2265 2n\u2212 n\n\u2206\n\u2212 1 \u2265 3\n2\nn\u2212 1.\nThe last two inequalities use the facts that x \u2264 n and \u2206 \u2265 2. We see this final ratio is at least n (with our\nassumption that n \u2265 2). Hence, during round t + 1, at most n tasks were injected into the set S during\nload generation, and at least n tasks were removed from S during the load balancing phase (and none were\ninserted into S during this phase). Therefore, after load balancing (and thus also after the task deletion phase)\nS again satisfies Inequality (1).\nLemma 2 tells us that if a set is made bad by the load generation phase, then the load balancing and\ndeletion phases are sufficient to make this set good. The essential task that remains to be shown is that load\nbalancing cannot, in some way, change a good set into a bad one. Corollary 2 tells us half the story. We need\na little more to cover all possible sets.\nLemma 3. Suppose that at all sets are good at the end of round t, but that after load generation in round\nt + 1, there exists a bad set S with |S| = i. Then after load balancing and deletion, there exists no bad set\nwith i processors.\nNow we are prepared to prove our main result.\nProof. [Theorem 1]\nWe prove this theorem by induction on t. Inequality (1) holds when t = 1, for however we inject the\nfirst n tasks into the system, all sets are good at the end of the first round.\nSo assume that at the end of round t, all sets are good. Fix i \u2208 {1, . . . , n}. If all sets of i processors are\ngood after the load generation phase, then from Corollary 2 they are all good at the end of round t+ 1.\nIf there is some bad set of i processors after load generation, then Lemmas 2 and 3 show that all sets of\nsize i are still good at the end of round t+ 1.\nFinally, it is not possible that during load balancing a (good or bad) set of i processors will lead to the\ncreation of a bad set of j( 6= i) processors. For suppose there is some bad set of j(6= i) processors at the\nend of round t+ 1. Lemma 1 tells us that there must exist a set of j processors that was bad before the load\nbalancing phase, but then Lemmas 2 and 3 again tell us that there is no bad set of j processors at the end\nof round t + 1, a contradiction to our assumption that there was a bad set of j processors at the end of the\nround.\nOn the first glance it might look as if the our proof strategy is overly complicated and that there is a\nmuch simpler proof. In the course of proving our result, we show that there is a gap of 4n\u2206 tasks between\na processor in the bad set S and a processor outside of the bad set before balancing whenever S is bad\nafter balancing. Hence, at least n tasks were sent away from S in this step and the invariant could not have\nbeen violated by S. But unfortunately it is possible to create a different bad set of processors during load\nbalancing (possibly with a different number of processors), and we have to discount this case too. Hence,\nwe have to show that if we can find a bad set after load balancing, then there was another bad set S\u2032 before\nload balancing, which leads us to a contradiction through our series of lemmas above.\n3 A Matching Lower Bound\nIn this section we provide a simple example that asymptotically matches the upper bound from Section 2.\nConsider the linear array G = (V,E) with V = {P0, . . . , Pn\u22121} and E = {(Pi, Pi+1)|0 \u2264 i < n \u2212 1}.\nFurthermore, suppose that during every time step, n new tasks are generated on processor Pn\u22121. The idea\nof the proof essentially follows from a few simple observations, which we state without formal proof.\nObservation 2\n1. Clearly, the system must be periodic since it is stable and there is a finite number of possible configura-\ntions it can be in, i.e., there is a \u201crun-in\u201d phase during which load is being built up (essentially, load is\nbeing distributed from processor Pn\u22121 to all other processors), followed by periodical behaviour.\n2. Another obvious fact is that once the system has finished the initial run-in phase, every processor must\ndelete one task in every round. If that were not the case, the system could not possibly be stable (we\nwould delete strictly fewer tasks that are generated per period, i.e., the system load would increase by\nat least one during every period).\n3. Suppose the period length is T . Then we see that once the system is periodic, during any T rounds,\nprocessor Pi (i > 0) must send exactly T \u00b7 i many tasks to processor Pi\u22121 (some of which gets spread to\nthe other processors Pi\u22122, . . . , P0), because that is just the number of tasks that processors Pi\u22121, . . . , P0\ndelete in T rounds. In other words, on average processor Pi sends i many tasks during any of those\nrounds (it does, in fact, send exactly i tasks to processor Pi\u22121, thus T = 1; more about that later).\n4. In our setting, load will never be sent toward processor Pn\u22121.\nTheorem 3 below implies that the preceding analysis of our algorithm is tight up to a multiplicative\nconstant, because the line graph has maximum degree \u2206 = 2, and thus we have an upper bound of O(n3)\non the system load.\nTheorem 3. The system described above on the linear array is stable with a total steady-state system load\nof \u0398(n3).\nProof. We begin by showing that processor Pi will never send more than i tasks to processor Pi\u22121; the\nproof is by induction on time. The claim is trivially true in round 1. Let \u03b1ti denote the number of tasks that\nprocessor Pi sends to processor Pi\u22121 in round t. (We may extend the definition to \u03b1tn = n and \u03b1t0 = 0 for\nall t.) Suppose the claim holds for some t\u22121 > 1, i.e., \u03b1t\u22121i \u2264 i for all i \u2208 {1, . . . , n\u22121}. Let `ti denote the\nload of processor Pi before the balancing in round t, 0 \u2264 i < n. \u00bf From Observation 2 (2), for large enough\nvalues of t we have `ti = `\nt\u22121\ni + \u03b1\nt\u22121\ni+1 \u2212 \u03b1t\u22121i \u2212 1 and `ti\u22121 = `t\u22121i\u22121 + \u03b1t\u22121i \u2212 \u03b1t\u22121i\u22121 \u2212 1. Using the facts that\n\u03b1t\u22121i =\n\u230a\n`t\u22121i \u2212 `t\u22121i\u22121\n4\n\u230b\nand\n`t\u22121i \u2212 `t\u22121i\u22121\n4\n\u2264\n\u230a\n`t\u22121i \u2212 `t\u22121i\u22121\n4\n\u230b\n+\n3\n4\n,\nwe can conclude that\n\u03b1ti =\n\u230a\n`ti \u2212 `ti\u22121\n2\u2206\n\u230b\n\u2264 `\nt\ni \u2212 `ti\u22121\n2\u2206\n=\n`ti \u2212 `ti\u22121\n4\n=\n(`t\u22121i + \u03b1\nt\u22121\ni+1 \u2212 \u03b1t\u22121i \u2212 1)\u2212 (`t\u22121i\u22121 + \u03b1t\u22121i \u2212 \u03b1t\u22121i\u22121 \u2212 1)\n4\n=\n`t\u22121i \u2212 `t\u22121i\u22121\n4\n+\n\u03b1t\u22121i+1 \u2212 2\u03b1t\u22121i + \u03b1t\u22121i\u22121\n4\n\u2264\n\u230a\n`t\u22121i \u2212 `t\u22121i\u22121\n4\n\u230b\n+\n3\n4\n+\n\u03b1t\u22121i+1 \u2212 2\u03b1t\u22121i + \u03b1t\u22121i\u22121\n4\n= \u03b1t\u22121i +\n3\n4\n+\n\u03b1t\u22121i+1 \u2212 2\u03b1t\u22121i + \u03b1t\u22121i\u22121\n4\n=\n2\u03b1t\u22121i + \u03b1\nt\u22121\ni+1 + \u03b1\nt\u22121\ni\u22121\n4\n+\n3\n4\n\u2264 2i+ (i+ 1) + (i\u2212 1)\n4\n+\n3\n4\n= i+\n3\n4\nFrom the above we know that processor Pi will never send more than i tasks to processor Pi\u22121 during\neach round (i.e. \u03b1ti \u2264 i since fractional tasks are not allowed in our model). However, in order to obtain\nstability, at least i tasks on average are necessary. Thus, we can conclude that once the system is \u201crun-in\u201d,\nprocessor Pi will always send i tasks to processor Pi\u22121, i.e., the system is in fact periodic with period length\nT = 1. Clearly, there are many possible fixed points with this property. However, since we are interested\nin a lower bound, we pick the one with smallest total load, i.e., the one in which processor P0 is empty at\nthe end of a round, receives one tasks from processor P1 in the next round, deletes it, and so on. Since a\nload difference of 2\u2206i = 4i implies i tasks being sent, this means that, directly before balancing, the load\nof processor Pi is\n\u2211i\nj=0 4j = 2i(i+ 1), and thus the total system load is\n\u2211n\u22121\ni=0 2i(i+ 1) = (2n\n3 \u2212 2n)\/3.\nTogether with the upper bound of 3\u2206n3 = 6n3 we get the statement of the theorem.\n4 Some Instability Results for Work Stealing\nIn this section we will consider a variation of our load balancing process where we may transfer tasks to\nempty processors only. This approach is similar to the diffusion approach, only the computation of the \u03b1ti,j\nis different. The value of \u03b1ti,j , the load that is sent from Pi to Pj , is larger than zero iff Pj is empty (and Pi\nnon-empty). This method is referred to as work stealing.\n\u03b1ti,j =\n{\nb \u00af`ti\u2206+1c : \u00af`tj = 0\n0 : otherwise\nNote that the bounds below also hold when we divide by 2\u2206 instead of\u2206+1. We use the above definition\nas worst case assumption. In [8] the authors showed that simple work stealing yields a stable system. They\nassumed that there are at most (1 \u2212 \u000f)n new tasks generated per round, for some \u000f \u2208 (0, 1]. The important\npoint to note is that in [8], the processor communication links correspond to a complete graph on n vertices.\nHere we will see that the work stealing method can fail (in the sense that the total load is unbounded over\ntime) if the graph is no longer the complete graph.\nWe consider the line network. In a line, we have an edge between node Pi and Pi+1 for 1 \u2264 i \u2264 n\u2212 1.\nHence, the maximum degree is 2.\nObservation 4 Assume we have n processors connected as a line and n generators are all on processor 1.\nThen the diffusion work stealing system is not stable.\nProof. Let us assume the system is in a state where P2 is empty and P1 has k tasks directly before the\nbalancing. Then it will transfer k\/3 tasks to P2 during the load balancing step. It is easy to see that it will\ntake at least\nt =\nk\n3(n\u2212 1) +\nn\u22122\u2211\ni=1\ni =\nk\n3(n\u2212 1) +\nn2 \u2212 4n\u2212 3\n2\ntime steps until P2 is empty again. To see that, assume that all other processors are empty. Then it takes\nn \u2212 2 steps until load will reach Pn, it takes n \u2212 3 time steps until load will reach Pn\u22121, and so on. In the\nmeantime, the load of P1 increases by t(n\u2212 1) tasks. Thus, the load of P1 after t steps is at least\nk \u2212 k\n3\n+\n(\nk\n3(n\u2212 1) +\nn2 \u2212 4n\u2212 3\n2\n)\n(n\u2212 1) \u2265 k.\nThis shows that the load of P1 increases between any two consecutive balancing actions.\nIn a similar manner, under adversarial injection schemes, it is easy to show that the work stealing proto-\ncol will not be stable for many classes of graphs, even under a randomised injection pattern. For example,\nwe can simply define the process in a way such that the expected load of a processor increases between two\nload balancing actions.\n5 A Different Model for Task Generation\/Deletion\nIn this section we define a load generation model similar to [18] and [4]. Rather than bounding the total\nnumber of tasks that are generated per round, we bound the load change in any subset of the processors.\nDuring each round, tasks can be added or deleted from processors, subject to the restriction in Inequality (9)\nbelow. The processors then balance load amongst themselves as before.\nIn the following, \u00af`ti (respectively, L\u00af\nt(S)) denotes the load of processor Pi (resp. the total load of all\nprocessors in set the S) after we have generated and deleted tasks, and `ti (resp. L\nt(S)) is the load of\nprocessor Pi (resp. the total load of all processors in the set S) immediately after the load balancing phase.\nLet avg(t) be the average load of the processors in round t after load generation and deletion, i.e. avg(t) =\n1\nn \u00b7\n\u2211n\ni=1\n\u00af`t\ni. Again, L\nt(P) denotes the total system load at the end of step t. One round looks now as follows:\n1. Tasks are generated and deleted according to the following generation restriction:\nL\u00aft(S)\u2212 Lt\u22121(S) \u2264 |S| \u00b7 (avg(t)\u2212 avg(t\u2212 1)) + n. (9)\n2. Every processor balances its load with some or all its neighbours in the network using the diffusion\noperation defined in Section 1.2.\nWe can show the following result (whose proof may be found in the appendix).\nTheorem 5. Let n \u2265 2 denote the number of processors in the system. Let \u2206 \u2265 2 denote the maximum\ndegree of the graph G that specifies the communication linkages in the network. Assume the load generation\nand deletion fulfills the generation restriction in (9). Then, starting with an empty system, for all t \u2265 1 and\nall S \u2286 P we have\nLt(S) \u2264 |S| \u00b7 avg(t) + 5\u2206n3.\nFurthermore, the maximum number of tasks per processor is avg(t) + 5\u2206n2.\n5.1 Further Extensions\nWe can easily generalize our results to other load generation processes, and the proofs of the following\nresults are much like those of Theorem 5 and can be found in the Appendix.\nTheorem 6. Let n \u2265 2 denote the number of processors in the system. Let \u2206 \u2265 2 denote the maximum\ndegree of the graph G that specifies the communication linkages in the network. Assume the load generation\nand deletion fulfills the generation restriction\nL\u00aft(S)\u2212 Lt\u22121(S) \u2264 |S| \u00b7 (avg(t)\u2212 avg(t\u2212 1)) +K.\nThen, starting with an empty system, for all t \u2265 1 and all S \u2286 P we have\nLt(S) \u2264 |S| \u00b7 avg(t) + 5\u2206nK.\nFurthermore, the maximum number of tasks per processor is avg(t) + 5\u2206nK.\nFurthermore, we can improve our results to a load generation model where the imbalance that we allow\nto be generated in any set depends on the number of outgoing edges.\nTheorem 7. Let n \u2265 2 denote the number of processors in the system. Let \u2206 \u2265 2 denote the maximum\ndegree of the graph G that specifies the communication linkages in the network. Let e(S) be the number of\noutgoing edges of the set S. Assume the load generation and deletion fulfills the generation restriction\nL\u00aft(S)\u2212 Lt\u22121(S) \u2264 |S| \u00b7 (avg(t)\u2212 avg(t\u2212 1)) +K \u00b7 e(S).\nThen, starting with an empty system, for all t \u2265 1 and all S \u2286 P we have\nLt(S) \u2264 |S| \u00b7 avg(t) + 5\u2206nK.\nFurthermore, the maximum number of tasks per processor is avg(t) + 5\u2206nK.\nReferences\n1. W. Aiello, E. Kushilevitz, R. Ostrovsky, and A. Rosen. Adaptive packet routing for bursty adversarial traffic. J. Computer\nand Systems Sciences 60 (2000), pp. 482\u2013509.\n2. W. Aiello, B. Awerbuch, B. Maggs, and S. Rao. Approximate load balancing on dynamic and asynchronous networks.\nProceedings of the 25th Annual ACM Symposium on Theory of Computing (STOC 1993), pp. 632\u2013641.\n3. A. Anagnostopoulos, A. Kirsch, and E. Upfal. Stability and efficiency of a random local load balancing protocol. Proceedings\nof the 44th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2003).\n4. E. Anshelevich, D. Kempe, and J. Kleinberg. Stability of load balancing algorithms in dynamic adversarial systems. Pro-\nceedings of the 34th Annual ACM Symposium on Theory of Computing (STOC 2002), pp. 399\u2013406.\n5. B. Awerbuch, P. Berenbrink, A. Brinkmann, and C. Scheideler. Simple routing strategies for adversarial systems. Proceedings\nof the 32nd Annual ACM Symposium on Theory of Computing (STOC 2001), pp. 158\u2013167.\n6. B. Awerbuch and T. Leighton. A simple local control algorithm for multi-commodity flow. Proceedings of the 34th IEEE\nSymposium on Foundations of Computer Science (FOCS 1993), pp. 459\u2013468.\n7. B. Awerbuch and T. Leighton. Improved approximation algorithms for the multi-commodity flow problem and local compet-\nitive routing in dynamic networks. Proceedings of the 26th Annual ACM Symposium on Theory of Computing (STOC 1994),\npp. 487\u2013496.\n8. P. Berenbrink, T. Friedetzky, and L.A. Goldberg. The natural work-stealing algorithm is stable. SIAM Journal of Computing,\nSICOMP 32 (2003), pp. 1260\u20131279.\n9. P. Berenbrink , T. Friedetzky , and E. W. Mayr. Parallel continuous randomized load balancing. Proceedings of the 10th\nAnnual ACM Symposium on Parallel Algorithms and Architectures (SPAA\u201998), 1998, pp.192-201 .\n10. J.E. Boillat. Load balancing and Poisson equation in a graph. Concurrency: Practice and Experiences 2 (1990), pp. 289\u2013313.\n11. G. Cybenko. Load balancing for distributed memory multiprocessors. J. Parallel and Distributed Computing 7 (1989), pp.\n279\u2013301.\n12. R. Diekmann, A. Frommer, and B. Monien. Efficient schemes for nearest neighbor load balancing. J. Parallel Computing 25\n(1999), pp. 789\u2013812.\n13. R. Elsa\u00a8sser and B. Monien. Load balancing of unit size tokens and expansion properties of graphs. Proceedings of the 15th\nAnnual ACM Symposium on Parallel Algorithms and Architectures (SPAA 2003), pp. 266\u2013273.\n14. R. Elsa\u00a8sser, B. Monien, and R. Preis. Diffusion schemes for load balancing on heterogeneous networks. Theory of Computing\nSystems 35 (2002), pp. 305\u2013320.\n15. B. Gosh and S. Muthukrishnan. Dynamic load balancing by random matchings. J. Computer and Systems Science, 53 (1996),\npp. 357-370.\n16. B. Ghosh, F.T. Leighton, B.M. Maggs, S. Muthukrishnan, C.G. Plaxton, R. Rajaraman, A.W. Richa, R.E. Tarjan, and D. Zuck-\nerman. Tight analyses of two local load balancing algorithms. Proceedings of the 27th Annual ACM Symposium on Theory\nof Computing (STOC 1995), pp. 548\u2013558.\n17. F.M. auf der Heide, B. Oesterdiekhoff, and R. Wanka. Strongly adaptive token distribution. Algorithmica 15 (1996), pp.\n413\u2013427.\n18. S. Muthukrishnan and R. Rajaraman. An adversarial model for distributed load balancing. Proceedings of the 10th Annual\nACM Symposium on Parallel Algorithms and Architectures (SPAA 1998), pp. 47\u201354.\n19. D. Peleg and E. Upfal. The token distribution problem. SIAM J. Computing 18 (1989), pp. 229\u2013243.\n20. Y. Rabani, A. Sinclair, and R. Wanka. Local divergence of Markov chains and the analysis of iterative load-balancing\nschemes. Proceedings of the 39th IEEE Symposium on Foundations of Computer Science (FOCS 1998), pp. 694\u2013703.\nAppendix\nProof of Lemma 3.\nProof. Without loss of generality, we can assume that S = M\u00af ti , the largest i processors. Lemma 2 tells us\nthat S is not bad at the end of round t + 1. We therefore have to show that we do not somehow change a\ngood set (of i processors) into a bad set during the load balancing phase. This proof is similar in flavor to\nthat of Lemma 1, except that the argument is somewhat more delicate in this case.\nSince we injected at most n tasks into the set S to change S from a good set into a bad set, we know\nthat L\u00aft+1(S)\u2212 n \u2264 f(|S|). Our goal now is to show that any set S\u2032 of i processors will satisfy Lt+1(S\u2032) \u2264\nL\u00aft+1(S)\u2212 n, meaning that S\u2032 is good after load balancing.\nSo with this mind, fix some set S\u2032 where |S\u2032| = i. We assume that S\u2032 6= S, otherwise by Lemma 2 there\nis nothing to prove. Define the following sets:\nScommon = S \u2229 S\u2032 Sold = S\\Scommon Snew = S\u2032\\Scommon.\nWe note that |Snew| = |Sold| \u2265 1. From our previous argument in Lemma 2, we know that the load\ndifference (after generation, but before balancing) of any pair of processors, one from S and one from P\\S,\nis at least 4\u2206n\u2212 2n.\nIn order to show our result, we will consider the load balancing actions of round t + 1 in three stages.\nWe first compute (and fix) the values of \u03b1t+1i,j . Then we proceed this way:\nStage 1. Internal load balancing actions among processors of S, and among processors of P\\S. After this\nstage, the load difference between a pair of processors, one from S and one from P\\S is still at least\n4\u2206n\u2212 2n.\nStage 2. Processors in Sold balance with those in Snew. This can only move load from Sold to Snew because\nof the high load difference between processors of these two sets.\nStage 3. All remaining load balancing actions are performed. Which ones remain? Because there are no\nbalancing actions from Snew \u2286 P\\S into Scommon \u2286 S, the only remaining ones are\n(a) Scommon to Snew,\n(b) Sold to P\\(S\u2032 \u222a Sold), and\n(c) Scommon to P\\(S\u2032 \u222a Sold).\nThe balancing actions of (a) and (b) do not change the load of S\u2032 = Scommon \u222a Snew, and those of (c) can\nonly decrease the load of S\u2032. Hence, if we can show the load of S\u2032 after Stage 2 is at most L\u00aft+1(S)\u2212n, then\nwe get the conclusion we want.\nTo this end, let L1(Snew) denote the load of Snew after Stage 1, and L2(Snew) the load after Stage 2\n(and similarly for other sets Sold, S, etc.). Let A =\n\u2211\nj\u2208Sold,k\u2208Snew \u03b1\nt+1\nj,k denote the total load transferred\nduring Stage 2 from Sold to Snew, and let B denote the load that remains in Sold after Stage 2. We note the\nfollowing equations hold:\nL2(S\u2032) = L2(S) + L2(Snew)\u2212 L2(Sold)\nL1(Sold) = A+B\nL2(Sold) = B\nL2(Snew) = L1(Snew) +A\nL2(S) = L1(S)\u2212A.\nAll of these equations together imply that\nL2(S\u2032) = L1(S)\u2212A+ L1(Snew) +A\u2212B\n= L1(S) + L1(Snew)\u2212B\n= L1(S) + L1(Snew) +A\u2212 L1(Sold).\nSince Stage 1 did not change the total load of S (so L1(S) = L\u00aft+1(S)), if we can show that\nL1(Snew) +A\u2212 L1(Sold) \u2264 \u2212n (10)\nwe obtain our desired result. Having arrived at the crux of the problem, we now demonstrate Inequality (10).\nFirst note that if, in fact, there are no edges from Sold to Snew, then A = 0. In this case, if we pair the\nvertices from Sold with those from Snew, then Inequality (10) follows immediately using the fact that the\nload difference of processors in Sold and Snew is at least 4\u2206n\u2212 2n.\nSuppose there is at least one edge from Sold to Snew. Because of the load difference of processors in\nSold and Snew, we see that any edge for which \u03b1t+1j,k is positive, we in fact have that \u03b1\nt+1\nj,k \u2265 n.\nConsider the subgraphG\u2032 that consists of processors in Sold and Snew and edges which were used to pass\nload from Sold to Snew during Stage 2. Choose an edge from G\u2032 such that the value of \u03b1t+1j,k is maximised.\nAssume (for simplicity) that j = 1 and k = 2. As in Lemma 1, we conclude that \u00af`t+11 \u2265 2\u2206\u03b1t+11,2 + \u00af`t+12 .\nDefine A1,2 =\n\u2211\nk\u2208Snew \u03b1\nt+1\n1,k +\n\u2211\nj\u2208Sold \u03b1\nt+1\nj,2 , the total flow out of P1 (into Snew) and into P2 (from Sold).\nSince \u03b1t+11,2 has maximum value over edges, we see that \u00af`\nt+1\n1 \u2265 2\u2206\u03b1t+11,2 + \u00af`t+12 \u2265 A1,2 + \u00af`t+12 . Hence,\nwe see that \u00af`t+12 + A1,2 \u2212 \u00af`t+11 \u2264 0. Indeed, if at least one of P1 and P2 has degree strictly smaller than\n\u2206 in G\u2032, this difference is smaller than or equal to \u2212n, which is what we want on the right hand side of\nInequality (10)!\nIn either case, consider the subgraph G\u2032\u2032 obtained from G\u2032 by deleting the processors P1, P2, and all\nedges adjacent to them. As before, if there are no edges, we can pair the remaining processors however we\nlike, and then we get the desired inequality. Otherwise, if we can show that L1(Snew\\P2) + (A \u2212 A1,2) \u2212\nL1(Sold\\P1) \u2264 \u2212n we again have shown Inequality (10).\nThe point is that we can proceed in an inductive manner as before, until we either find a pair Pj \u2208\nSold, Pk \u2208 Snew where Pj sent load to Pk during Stage 2 and one of Pj and Pk has degree (in the remaining\nsubgraph ofG\u2032) that is strictly less than\u2206 (in which case \u00af`t+1k +Aj,k\u2212 \u00af`t+1j \u2264 \u2212n), or we obtain a subgraph\nthat has processors remaining, but no edges (and in this case we pair up the remaining processors however\nwe like, and the large load difference between processors in the two sets gives us Inequality (10)). Whatever\noccurs, we can pair up processors in a one-to-one fashion to prove Inequality (10), and thus, our lemma.\nProof of Theorem 5.\nProof. The proof of this theorem follows the proof of Theorem 1. Here, we will concentrate on the parts\nthat have to be changed compared to that proof. We redefine f as follows.\nf(k) =\nn\u2211\ni=n\u2212k+1\ni \u00b7 (5\u2206) \u00b7 n. (11)\nOur new invariant is\nLt(S) \u2264 |S| \u00b7 avg(t) + f(|S|) = |S| \u00b7 avg(t) +\nn\u2211\ni=n\u2212|S|+1\ni \u00b7 (5\u2206) \u00b7 n. (12)\nSimilar to the previous section, we call a set S bad if Lt(S) > |S| \u00b7 avg(t) + f(|S|), and good otherwise.\nSince Lemma 1, Corollary 2, and Corollary 1 only depend on the load balancing scheme and not on the\nunderlying load generation and deletion, they still can be applied. Because Lemma 2 depends on the actual\nload of the processors and, therefore, on the load generation model, we have to adjust it. The new version is\npresented below.\nLemma 4. Suppose that at the end of round t, every set S \u2286 P satisfies (12). Further, suppose that after\nthe load generation and deletion phase in round t + 1, there is some set S \u2286 P such that L\u00aft+1(S) >\n|S| \u00b7 avg(t+ 1) + f(|S|). Then, at the end of round t+ 1, S again satisfies Inequality (12).\nProof. We only consider the parts of the proof that are different from the proof of Lemma 2.\nWe first show that\nif Pj \u2208 S then \u00af`t+1j \u2265 (n\u2212 |S|+ 1)(5\u2206)n+ avg(t+ 1)\u2212 n. (13)\nIn the case when S = {Pi} for some i (that is, |S| = 1), this statement is clear, since we must have\n\u00af`t+1\ni > n(5\u2206)n+ avg(t+ 1) to violate Inequality (12).\nAs in Lemma 2, when |S| \u2265 2 we can prove (13) by contradiction. So assume that some Pj \u2208 S satisfies\n\u00af`t+1\nj < (n\u2212 |S|+ 1)(5\u2206)n + avg(t + 1)\u2212 n. Since S was good before load generation, but not after, we\nknow that L\u00aft+1(S) \u2212 f(|S|) > |S| \u00b7 avg(t + 1). Then, using that L\u00aft+1(S\\Pj) = L\u00aft+1(S) \u2212 \u00af`t+1j , and our\nassumption on \u00af`t+1j , we conclude\nL\u00aft+1(S\\Pj) > L\u00aft+1(S)\u2212 (n\u2212 |S|+ 1)(5\u2206)n\u2212 avg(t+ 1) + n\nL\u00aft+1(S\\Pj)\u2212 f(|S\\Pj |) > L\u00aft+1(S)\u2212 f(|S\\Pj |)\u2212 (n\u2212 |S|+ 1)(5\u2206)n\u2212 avg(t+ 1) + n\nL\u00aft+1(S\\Pj)\u2212 f(|S\\Pj |) > L\u00aft+1(S)\u2212 f(|S|)\u2212 avg(t+ 1) + n > (|S| \u2212 1) \u00b7 avg(t+ 1) + n.\nOn the other hand, Inequality (9) tells us that\nL\u00aft+1(S\\Pj) \u2264 Lt(S\\Pj) + (|S| \u2212 1) \u00b7 (avg(t+ 1)\u2212 avg(t)) + n.\nPutting this together with our last inequality above, we see that\nLt(S\\Pj) + (|S| \u2212 1) \u00b7 (avg(t+ 1)\u2212 avg(t)) + n\u2212 f(|S\\Pj |) \u2265 L\u00aft+1(S\\Pj)\u2212 f(|S\\Pj |)\n\u2265 (|S| \u2212 1) \u00b7 avg(t+ 1) + n\n=\u21d2 Lt(S\\Pj)\u2212 f(|S\\Pj |) > (|S| \u2212 1) \u00b7 avg(t).\nThis is a contradiction to the hypothesis that all sets satisfied (12) at the end of round t. Hence, we conclude\nwhat we wanted to show, namely Inequality (13).\nWhen S = P , then our lemma follows immediately since the load of S is exactly n \u00b7 avg(t+ 1). hence,\nwe can assume that S 6= P . Then, in a similar manner as before, we can show\nif Pj 6\u2208 S, then \u00af`t+1j \u2264 (n\u2212 |S|)(5\u2206)n+ avg(t+ 1) + n. (14)\nTo see this, again assume the contrary, so that some Pj 6\u2208 S satisfies\n\u00af`t+1\nj > (n\u2212 |S|)(5\u2206)n+ avg(t+ 1) + n.\nThen we have the following inequalities\nLt(S \u222a Pj) + n+ (avg(t+ 1)\u2212 avg(t))(|S|+ 1) \u2265 L\u00aft+1(S \u222a Pj) (15)\nL\u00aft+1(S \u222a Pj)\u2212 f(|S \u222a Pj |) > L\u00aft+1(S)\u2212 f(|S|) + avg(t+ 1) + n. (16)\nInequality (15) is due to the generation restriction. Inequality (16) follows by breaking up the difference\non the left hand side into constituent parts, and using our assumption about \u00af`t+1j . These inequalities together\nimply\nLt(S \u222a Pj)\u2212 f(|S \u222a Pj |) + (avg(t+ 1)\u2212 avg(t))(|S|+ 1) + n \u2265 L\u00aft+1(S)\u2212 f(|S|) + avg(t+ 1) + n(17)\nLt(S \u222a Pj)\u2212 f(|S \u222a Pj |) + (avg(t+ 1)\u2212 avg(t))(|S|+ 1) \u2265 L\u00aft+1(S)\u2212 f(|S|) + avg(t+ 1) > 0.(18)\nLt(S \u222a Pj)\u2212 f(|S \u222a Pj |) + (avg(t+ 1)\u2212 avg(t))(|S|+ 1) \u2265 |S + 1| \u00b7 avg(t+ 1) (19)\nL\u02c6t(S \u222a Pj)\u2212 f(|S \u222a Pj |) \u2265 |S + 1| \u00b7 avg(t) (20)\n(21)\nInequality in (18) comes from our assumption that L\u00aft+1(S) > |S| \u00b7 avg(t+ 1) + f(|S|). Again, we have a\ncontradiction and obtain the upper bound on the load of elements not in S, as expressed in (14).\nAgain, we have a load difference of at least 5\u2206n between processors on S and processors not in S. The\nrest of this lemma is a simple calculation and can be done similar to the one in Lemma 4.\nLemma 3 only depends on the load difference of the processors and is still valid under the new load\ngeneration and deletion model. We have only to show that we still have\nL\u00aft+1(S)\u2212 n \u2264 |S| \u00b7 avg(t+ 1) + f(|S|),\ni.e. if we subtract n from the load of set S after load generation and deletion, set S is good again. This can\nbe done as follows.\nDue to the generation restriction, we know that the load generated in S is upper bounded by |S|\u00b7(avg(t+\n1)\u2212 avg(t)) + n. We know that Lt(S) \u2264 |S| \u00b7 avg(t) + f(|S|) since S was good at the end of round t. This\ngives us\nL\u00aft+1(S) \u2264 Lt(S) + |S| \u00b7 (avg(t+ 1)\u2212 avg(t)) + n \u2264 |S| \u00b7 avg(t) + f(|S|) + |S|(avg(t+ 1)\u2212 avg(t)) + n\nL\u00aft+1(S) \u2264 |S| \u00b7 avg(t+ 1) + f(|S|) + n\nL\u00aft+1(S)\u2212 n \u2264 |S| \u00b7 avg(t+ 1) + f(|S|).\nAlso, the remainder of the proof of Theorem 5 can be done similar to the proof of Theorem 1.\n"}