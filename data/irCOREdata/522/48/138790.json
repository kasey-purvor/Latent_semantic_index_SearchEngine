{"doi":"10.1016\/j.ins.2009.02.002","coreId":"138790","oai":"oai:dspace.lib.cranfield.ac.uk:1826\/3463","identifiers":["oai:dspace.lib.cranfield.ac.uk:1826\/3463","10.1016\/j.ins.2009.02.002"],"title":"Exploiting the performance gains of modern disk drives by enhancing data locality.","authors":["Deng, Yuhui"],"enrichments":{"references":[{"id":38095716,"title":"A comparison of file system workloads, in:","authors":[],"date":"2000","doi":null,"raw":". D. Roselli, J. R. Lorch and T. E. Anderson, A comparison of file system workloads, in: Proceedings of the USENIX Annual Technical Conference (Berkeley, CA), 2000, pp.41\u201354.","cites":null},{"id":38095704,"title":"A fast file system for UNIX,","authors":[],"date":"1984","doi":"10.1145\/989.990","raw":". M. McKusick, W. Joy, and S. Leffler, A fast file system for UNIX, ACM Trans. on Computer Systems 2(3) (1984) 181\u2013197.","cites":null},{"id":38095645,"title":"Adaptive block rearrangement,","authors":[],"date":"1995","doi":"10.1145\/201045.201046","raw":".  S. Aky\u00fcrek and K. Salem, Adaptive block rearrangement, ACM Transactions on Computer Systems 12(2) (1995) 89-121.","cites":null},{"id":38095710,"title":"Advances in disk technology: performance issues,","authors":[],"date":"1998","doi":"10.1109\/2.675641","raw":". S. W. Ng, Advances in disk technology: performance issues, Computer 31(5) (1998) 75-81.","cites":null},{"id":38095688,"title":"An efficient algorithm for mining frequent inter-transaction patterns,","authors":[],"date":"2007","doi":"10.1016\/j.ins.2007.03.007","raw":". A.  J. T.  Lee and  C. Wang, An  efficient  algorithm  for  mining  frequent  inter-transaction patterns, Information Sciences 177(17) (2007) 3453-3476.","cites":null},{"id":38095698,"title":"Aqueduct: online data migration with performance guarantees, in:","authors":[],"date":"2002","doi":null,"raw":". C. Lu, G. A. Alvarez, J. Wilkes, Aqueduct: online data migration with performance guarantees, in: Proceedings of the 1st USENIX Conference on File and Storage Technologies, 2002, pp.219-230.","cites":null},{"id":38095667,"title":"Blurring the line between oses and storage devices,","authors":[],"date":"2001","doi":null,"raw":".  G. Ganger, Blurring the line between oses and storage devices, Technical report, Carnegie Mellon University, December 2001.","cites":null},{"id":38095682,"title":"Caching strategies to improve disk system performance,","authors":[],"date":"1994","doi":"10.1109\/2.268884","raw":". R.  Karedla,  J.  S.  Love,  B.  G.  Wherry,  Caching  strategies  to  improve  disk  system  performance, Computer 27(3) (1994) 38 - 46.","cites":null},{"id":38095678,"title":"Characteristics of I\/O traffic in personal computer and server workloads,","authors":[],"date":"2003","doi":"10.1147\/sj.422.0347","raw":". W. W. Hsu and A. J. Smith, Characteristics of I\/O traffic in personal computer and server workloads, IBM Systems Journal 42(2) (2003) 347\u2013372.","cites":null},{"id":38095728,"title":"Clustering active disk data to improve disk performance,","authors":[],"date":null,"doi":null,"raw":". C. Staelin and H. Garcia-Molina, Clustering active disk data to improve disk performance, Tech. Rep. CS-TR-283-90, Dept. of Computer Science, Princeton University,1990.","cites":null},{"id":38095722,"title":"Designing computer systems with MEMS-based storage, in:","authors":[],"date":"2000","doi":"10.1145\/378995.378996","raw":". S.  W.  Schlosser,  J.  L.  Griffin,  D.  F.  Nagle,  G.  R.  Ganger,  Designing  computer  systems  with MEMS-based storage, in: Proceedings of the 9th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), 2000, pp.1\u201312.","cites":null},{"id":38095714,"title":"Disk drive level workload characterization, in:","authors":[],"date":"2006","doi":"10.1109\/iiswc.2009.5306787","raw":". A. Riska and E. Riedel, Disk drive level workload characterization, in: Proceedings of the USENIX Annual Technical Conference , Boston, 2006, pp. 97-103.","cites":null},{"id":38095718,"title":"Disk shuffling,","authors":[],"date":"1991","doi":"10.1109\/2.268881","raw":". C. Ruemmler and J. Wilkes, Disk shuffling, Technical report HPL-91-156, Hewlett-Packard Company, Palo Alto, CA, October 1991.","cites":null},{"id":38095665,"title":"Disk subsystem load balancing: disk striping vs. conventional data placement, in:","authors":[],"date":"1993","doi":"10.1109\/hicss.1993.270759","raw":".  G. R. Ganger, B. L. Worthington, R. Y. Hou, Y. N. Patt, Disk subsystem load balancing: disk striping vs. conventional data placement, in: Proceedings of the Hawaii International Conference on System Sciences, January 1993, pp. 40-49. ACCEPTED MANUSCRIPT","cites":null},{"id":38095661,"title":"EED: energy efficient disk drive architecture,","authors":[],"date":"2008","doi":"10.1016\/j.ins.2008.07.022","raw":".  Y. Deng, F. Wang, N. Helian, EED: energy efficient disk drive architecture, Information Sciences, 178(22) (2008) 4403-4417.","cites":null},{"id":38095670,"title":"Embedded inodes and explicit grouping: exploiting disk bandwidth for small files, in:","authors":[],"date":"1997","doi":null,"raw":". G. R. Ganger, M. F. Kaashoek, Embedded inodes and explicit grouping: exploiting disk bandwidth for small files, in: Proceedings of Annual USENIX Technical Conference (Anaheim, CA), January 1997, pp. 1-17.","cites":null},{"id":38095730,"title":"File size distribution on UNIX systems: then and now,","authors":[],"date":null,"doi":"10.1145\/1113361.1113364","raw":". A. S. Tanenbaum, J. N. Herder, H. Bos, File size distribution on UNIX systems: then and now, ACM SIGOPS Operating Systems Review, 40(1) (2006)100-104.","cites":null},{"id":38095700,"title":"Freeblock scheduling outside of disk firmware, in:","authors":[],"date":"2002","doi":null,"raw":". C. R. Lumb, J. Schindler, and  G. R. Ganger,  Freeblock scheduling outside of disk  firmware,  in: Proceedings of the 1st USENIX Conference on File and Storage Technologies, 2002, pp.275-288.","cites":null},{"id":38095680,"title":"FS2: dynamic data replication in free disk space for improving disk performance and energy consumption, in:","authors":[],"date":"2005","doi":"10.1145\/1095809.1095836","raw":". H. Huang, W. Hung, and K. G. Shin, FS2: dynamic data replication in free disk space for improving disk  performance  and  energy  consumption,  in:  Proceedings  of  the  20th  ACM  symposium  on Operating systems principles, 2005, pp.263-276.","cites":null},{"id":38095674,"title":"Hierarchical clustering of mixed data based on distance hierarchy,","authors":[],"date":"2007","doi":"10.1016\/j.ins.2007.05.003","raw":". C.  Hsu,  C.  Chen  and  Y.  Su,  Hierarchical  clustering  of  mixed  data  based  on  distance  hierarchy, Information Sciences 177(20) (2007) 4474-4492.","cites":null},{"id":38095692,"title":"Kernel class-wise locality preserving projection,","authors":[],"date":"2008","doi":"10.1016\/j.ins.2007.12.001","raw":". J. Li, J. Pan and S. Chu, Kernel class-wise locality preserving projection, Information Sciences 178(7) (2008) 1825-1835.","cites":null},{"id":38095694,"title":"Log-structured File System,","authors":[],"date":null,"doi":"10.1007\/978-1-4615-2221-8_4","raw":". Log-structured File System, http:\/\/log-file-system.area51.ipupdater.com\/.","cites":null},{"id":38095647,"title":"Measurements of a distributed file system, in:","authors":[],"date":"1991","doi":"10.1145\/121133.121164","raw":".  M. Baker,  J.  Hartman, M.  Kupfer, K.  Shirriff,  J.  Ousterhout,  Measurements  of  a distributed  file system, in: Proceedings of ACM Symposium on Operating Systems Principles, 1991, pp. 198\u2013212.","cites":null},{"id":38095690,"title":"Mining block correlations to improve storage performance,","authors":[],"date":"2005","doi":"10.1145\/1063786.1063790","raw":". Z.  Li,  Z.  Chen,  Y.  Zhou,  Mining  block  correlations  to  improve  storage  performance,  ACM Transactions on Storage 1(2) (2005) 213-245.","cites":null},{"id":38095731,"title":"My Cache or yours? making storage more exclusive, in:","authors":[],"date":"2002","doi":null,"raw":". T. M. Wong and J. Wilkes, My Cache or yours? making storage more exclusive, in: Proceedings of USENIX Annual Technical Conference (USENIX 2002), 2002, pp. 161\u2013175.","cites":null},{"id":38095706,"title":"Object-based storage,","authors":[],"date":"2003","doi":"10.1109\/mcom.2003.1222722","raw":". M. Mesnier, G. R. Ganger, E. Riedel, Object-based storage, IEEE Communications Magazine 41(8) (2003) 84 \u2013 90. ACCEPTED MANUSCRIPT","cites":null},{"id":38095708,"title":"Observing the effects of multi-zone disks, in:","authors":[],"date":"1997","doi":null,"raw":". R. V. Meter,  Observing  the  effects  of  multi-zone  disks,  in: Proceedings of  the USENIX  Annual Technical Conference, January 1997, pp.19-30.","cites":null},{"id":38095659,"title":"Optimal clustering size of small file access in network attached storage device, Parallel Processing Letters,","authors":[],"date":"2006","doi":"10.1142\/s0129626406002800","raw":".  Y. Deng, F. Wang, N. Helian, D. Feng, K. Zhou, Optimal clustering size of small file access in network attached storage device, Parallel Processing Letters, 16(4) (2006) 501-512.","cites":null},{"id":38095655,"title":"Reshaping access patterns for improving data locality, in:","authors":[],"date":"1996","doi":null,"raw":".  A.  J.  C.  Bik,  Reshaping  access  patterns  for  improving  data  locality,  in:  Proceedings  of  the  6th Workshop on Compilers for Parallel Computers, 1996, pp. 229-310.","cites":null},{"id":38095726,"title":"Seek distance dependent variable max VCM seek current to control thermal rise in VCM\u2019s.","authors":[],"date":null,"doi":null,"raw":". Seek  distance  dependent  variable  max  VCM  seek  current  to  control  thermal  rise  in  VCM\u2019s. http:\/\/www.patentstorm.us\/patents\/6724564-description.html.","cites":null},{"id":38095712,"title":"Storage hierarchies: Gaps, cliffs, and trends,","authors":[],"date":"1971","doi":"10.1109\/tmag.1971.1067228","raw":". E. Pugh, Storage hierarchies: Gaps, cliffs, and trends, IEEE Transactions on Magnetics 7(4) (1971) 810-814.","cites":null},{"id":38095676,"title":"The automatic improvement of locality in storage systems,","authors":[],"date":"2005","doi":"10.1145\/1113574.1113577","raw":". W. W. Hsu, A. J. Smith, H. C. Young, The automatic improvement of locality in storage systems, ACM Transactions on Computer Systems 23(4) (2005) 424\u2013473.","cites":null},{"id":38095657,"title":"The DiskSim simulation environment version 3.0 reference manual,","authors":[],"date":"2003","doi":null,"raw":".  J. S. Bucy and G. R. Ganger, The DiskSim simulation environment version 3.0 reference manual, Technical Report CMU-CS-03-102, January 2003.","cites":null},{"id":38095672,"title":"The performance impact of I\/O optimizations and disk improvements,","authors":[],"date":"2004","doi":"10.1147\/rd.482.0255","raw":". W. W. Hsu and A. J. Smith, The performance impact of I\/O optimizations and disk improvements, IBM Journal of Research and Development 48(2) (2004) 255\u2013289.","cites":null},{"id":38095702,"title":"The processor-memory bottleneck: problems and solutions,","authors":[],"date":"1999","doi":"10.1145\/357783.331677","raw":". N. R. Mahapatra and B. Venkatrao, The processor-memory bottleneck: problems and solutions, ACM Crossroads 5(3) (1999).","cites":null},{"id":38095696,"title":"Towards higher disk head utilization: extracting free bandwidth from busy disk drives, in:","authors":[],"date":null,"doi":null,"raw":". C. R. Lumb, J. Schindler, G. R. Ganger, D. F. Nagle, Towards higher disk head utilization: extracting free  bandwidth  from  busy  disk  drives,  in:  Proceedings  of  the  Fourth  Symposium  on  Operating Systems Design and Implementation(OSDI),2000, pp. 87\u2013102","cites":null},{"id":38095724,"title":"Track aligned extents: matching access patterns to disk drive characteristics,","authors":[],"date":"2002","doi":null,"raw":". J. Schindler, J. L. Griffin, C. R. Lumb, and G. R. Ganger, Track aligned extents: matching access patterns  to  disk  drive  characteristics,  in  Proceedings  of  Conf.  on  File  and  Storage  Technologies (FAST02), 2002, pp.259-274.","cites":null},{"id":38095684,"title":"Understanding the performance-temperature interactions in disk I\/O of server workloads, in:","authors":[],"date":"2006","doi":"10.1109\/hpca.2006.1598124","raw":". Y.  Kim,  S.  Gurumurthi,  A.  Sivasubramaniam,  Understanding  the  performance-temperature interactions in disk I\/O of server workloads, in: Proceedings of the 12th International Symposium on High-Performance Computer Architecture, 2006, pp.176-186.","cites":null},{"id":38095720,"title":"Unix disk access patterns, in:","authors":[],"date":"1993","doi":null,"raw":". C. Ruemmler, and J. Wilkes, Unix disk access patterns, in: Proceedings of the Winter 1993 USENIX Technical Conference , 1993, pp. 313-323.","cites":null},{"id":38095686,"title":"Zoned-RAID,","authors":[],"date":"2007","doi":"10.1145\/1227835.1227836","raw":". S. H. Kim, H. Zhu, R. Zimmermann, Zoned-RAID, ACM transactions on storage 3(1) (2007) 1-17.","cites":null}],"documentType":{"type":0.8888888889}},"contributors":[],"datePublished":"2009-06-27","abstract":"Due to the widening performance gap between RAM and disk drives, a large number of I\/O optimization methods have been proposed and designed to alleviate the impact of this gap. One of the most effective approaches of improving disk access performance is enhancing data locality. This is because the method could increase the hit ratio of disk cache and reduce the seek time and rotational latency. Disk drives have experienced dramatic development since the first disk drive was announced in 1956. This paper investigates some important characteristics of modern disk drives. Based on the characteristics and the observation that data access on disk drives is highly skewed, the frequently accessed data blocks and the correlated data blocks are clustered into objects and moved to the outer zones of a modern disk drive. The idea attempts to enhance spatial locality, improve the efficiency of aggressive sequential prefetch, and take advantage of Zoned Bit Recording (ZBR). An experimental simulation is employed to investigate the performance gains generated by the enhanced data locality. The performance gains are analyzed by breaking down the disk access time into seek time, rotational latency, data transfer time, and hit ratio of the disk cache. Experimental results provide useful insights into the performance behaviours of a modern disk drive with enhanced data locality","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/138790.pdf","fullTextIdentifier":"http:\/\/dx.doi.org\/10.1016\/j.ins.2009.02.002","pdfHashValue":"925b912845ea482fde2aa42542d2c3ca89f88c5f","publisher":"Elsevier","rawRecordXml":"<record><header><identifier>\noai:dspace.lib.cranfield.ac.uk:1826\/3463<\/identifier><datestamp>2009-07-11T14:09:53Z<\/datestamp><setSpec>hdl_1826_19<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>Exploiting the performance gains of modern disk drives by enhancing data locality.<\/dc:title><dc:creator>Deng, Yuhui<\/dc:creator><dc:subject>Disk drive<\/dc:subject><dc:subject>Data locality<\/dc:subject><dc:subject>Data access pattern<\/dc:subject><dc:subject>Block correlation<\/dc:subject><dc:subject>Data migration<\/dc:subject><dc:subject>Performance<\/dc:subject><dc:description>Due to the widening performance gap between RAM and disk drives, a large number of I\/O optimization methods have been proposed and designed to alleviate the impact of this gap. One of the most effective approaches of improving disk access performance is enhancing data locality. This is because the method could increase the hit ratio of disk cache and reduce the seek time and rotational latency. Disk drives have experienced dramatic development since the first disk drive was announced in 1956. This paper investigates some important characteristics of modern disk drives. Based on the characteristics and the observation that data access on disk drives is highly skewed, the frequently accessed data blocks and the correlated data blocks are clustered into objects and moved to the outer zones of a modern disk drive. The idea attempts to enhance spatial locality, improve the efficiency of aggressive sequential prefetch, and take advantage of Zoned Bit Recording (ZBR). An experimental simulation is employed to investigate the performance gains generated by the enhanced data locality. The performance gains are analyzed by breaking down the disk access time into seek time, rotational latency, data transfer time, and hit ratio of the disk cache. Experimental results provide useful insights into the performance behaviours of a modern disk drive with enhanced data locality.<\/dc:description><dc:publisher>Elsevier<\/dc:publisher><dc:date>2009-07-11T14:09:53Z<\/dc:date><dc:date>2009-07-11T14:09:53Z<\/dc:date><dc:date>2009-06-27<\/dc:date><dc:type>Article<\/dc:type><dc:identifier>Yuhui Deng, Exploiting the performance gains of modern disk drives by enhancing data locality, Information Sciences, Volume 179, Issue 14, Including Special Section - Linguistic Decision Making - Tools and Applications, 27 June 2009, Pages 2494-2511<\/dc:identifier><dc:identifier>0020-0255<\/dc:identifier><dc:identifier>http:\/\/dx.doi.org\/10.1016\/j.ins.2009.02.002<\/dc:identifier><dc:identifier>http:\/\/hdl.handle.net\/1826\/3463<\/dc:identifier><dc:language>en<\/dc:language><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["issn:0020-0255","0020-0255"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2009,"topics":["Disk drive","Data locality","Data access pattern","Block correlation","Data migration","Performance"],"subject":["Article"],"fullText":"Accepted Manuscript\nExploiting the Performance Gains of Modern Disk Drives by Enhancing Data\nLocality\nYuhui Deng\nPII: S0020-0255(09)00073-5\nDOI: 10.1016\/j.ins.2009.02.002\nReference: INS 8252\nTo appear in: Information Sciences\nReceived Date: 19 September 2007\nRevised Date: 22 January 2009\nAccepted Date: 1 February 2009\nPlease cite this article as: Y. Deng, Exploiting the Performance Gains of Modern Disk Drives by Enhancing Data\nLocality, Information Sciences (2009), doi: 10.1016\/j.ins.2009.02.002\nThis is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers\nwe are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and\nreview of the resulting proof before it is published in its final form. Please note that during the production process\nerrors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.\n  \n \nACCEPTED MANUSCRIPT \n \n 1 \nExploiting the Performance Gains  \nof Modern Disk Drives by Enhancing Data Locality  \n \nYuhui Deng,    \n \nCambridge-Cranfield High Performance Computing Facilities, \nCranfield University Campus, Bedfordshire MK430AL, United Kingdom \n \nEmail: deng_derek@emc.com;  yuhuid@hotmail.com \n \nDue to the widening performance gap between RAM and disk drives, a large number of I\/O optimization \nmethods have been proposed and designed to alleviate the impact of this gap. One of the most effective \napproaches of improving disk access performance is enhancing data locality. This is because the method \ncould increase the hit ratio of disk cache and reduce the seek time and rotational latency. Disk drives have \nexperienced dramatic development since the first disk drive was announced in 1956. This paper \ninvestigates some important characteristics of modern disk drives. Based on the characteristics and the \nobservation that data access on disk drives is highly skewed, the frequently accessed data blocks and the \ncorrelated data blocks are clustered into objects and moved to the outer zones of a modern disk drive. The \nidea attempts to enhance spatial locality, improve the efficiency of aggressive sequential prefetch, and take \nadvantage of Zoned Bit Recording (ZBR). An experimental simulation is employed to investigate the \nperformance gains generated by the enhanced data locality. The performance gains are analyzed by \nbreaking down the disk access time into seek time, rotational latency, data transfer time, and hit ratio of the \ndisk cache. Experimental results provide useful insights into the performance behaviours of a modern disk \ndrive with enhanced data locality. \n \nKey words: Disk drive; Data Locality; Data access pattern; Block correlation; Data migration; \nPerformance \n \n \n \n1. Introduction \nThe storage hierarchy in current computer architectures is designed to take advantage of data access \nlocality to improve overall performance. Each level of the hierarchy has higher speed, lower latency, and \nsmaller size than lower levels. For decades, the hierarchical arrangement has suffered from significant \nbandwidth and latency gaps among processor, RAM, and disk drive [27,32,38]. The performance gap \nbetween processor and RAM has been alleviated by fast cache memories. However, the performance gap \nof RAM to disk drive has been widened to 6 orders of magnitude in 2000 and will continue to widen by \nabout 50% per year [38].  \nSince the first disk drive was announced in 1956, disk drives have grown by over six orders of \nmagnitude in density and over four orders in performance [29]. Over the last decade, areal density, track \ndensity and linear density have achieved 100%, 50%, and 30% growth respectively. Revolutions Per \nMinute (RPM) has been increased from 3600 in 1981 to 15000 in 2000. Due to the significant growth in \n  \n \nACCEPTED MANUSCRIPT \n \n 2 \nboth the linear density and RPM, Internal Data Rate (IDR) has been growing at an exponential rate of 40% \neach year over the past 15 years [11]. Unfortunately, the basic mechanical architecture in disk drive has not \nchanged too much. Slowed by the mechanical delays, disk access time was improved only about 8% per \nyear [12]. Therefore, the disk I\/O subsystem is repeatedly identified as a major bottleneck to system \nperformance in many computing systems. The widening gap will be more serious when disk drives reache \nits physical limits due to the super paramagnetic effect. \nThe increasing performance gap between RAM and disk drive has long been a primary obstacle to \nimprove overall system performance. To alleviate the impact of this widening gap, a lot of research efforts \nhave been invested to improve disk access time. We only mention some of them which are related to our \nwork in this paper. Ruemmler and Wilkes [36] employed disk shuffling to move frequently accessed data \ninto the centre of a disk drive and organize the data into an organ pipe to reduce mean seek distances. They \nconstructed a repeatable simulation environment across a range of workloads and disk drive types for \ncomparing different shuffling algorithms. Their research indicated that the benefits are small to moderate, \nbut are likely to be much larger with file systems that do not do a good initial data placement. Aky\u00fcrek and \nSalem [1] presented an adaptive technique to copy a small number of frequently referenced disk blocks \nfrom their original locations to a reserved space near the middle of the disk. Their experiments showed that \nseek times are reduced 30% to 85% (depending on workloads) by adaptively rearranging about 3% of the \ndata on the disk drive. FS2 [16] dynamically places multiple copies of data in file system\u2019s free blocks \naccording to the disk access patterns observed at runtime to reduce the head positioning latencies. Because \none or more replicas can be accessed in addition to their original data block, choosing the nearest replica \nthat provides the fastest access can significantly improve disk I\/O performance. \nThe above methods can substantially reduce seek time or rotational latency. However, those \napproaches may not be effective on modern disk drives due to evolutional disk drive technologies. First of \nall, disk access time mainly consists of seek time, rotational latency and data transfer time. Due to the \nadvance of Voice Coil Motors (VCM) electric drive and the increasing track density, long distance seeks of \nmodern disk drives may not involve enormously more overhead than short ones. This results in a \nsignificant proportion decrease of seek time in disk access time. Secondly, due to geometric features, outer \ntracks on disk platters are much larger than the inner tracks. Modern disk drives employ a technique called \nZoned Bit Recording (ZBR) to take advantage of its geometric features to increase disk capacity by \nvarying the number of sectors per track with the distance from the spindle [30]. This characteristic results \nin much higher data transfer rate of outer zones than that of inner zones. Finally, the organ pipe is formed \nby placing the most frequently accessed cylinder in the middle of the disk drive, the next most frequently \naccessed cylinders on either side of the middle cylinder, and so on. This arrangement is provably optimal \nfor independent disk accesses [36]. However, block correlations are common semantic patterns in storage \nsystems, so the organ pipe arrangement could destroy the original block correlations [21]. The combination \nof these three reasons significantly counteracts the performance improvement of data reorganization.  \nThis paper explores the performance gains of data reorganization based on a modern disk drive. Some \nnew characteristics of modern disk drives, which are related to data reorganization, are reviewed. Based on \nthe characteristics, blocks, which are correlated to the frequently accessed block, are clustered into objects, \nand the objects are moved to the outer zones of the disk drive. The aggressive sequential prefetch of the \nmodern disk drive is enhanced and the block correlations remain due to the frequency based objects. Disk \naccess time is broken into four basic components: seek time, rotational latency, data transfer time, and hit \nratio of disk cache, each one is analyzed separately to determine its actual performance gains. \nExperimental results provide useful insights into the performance behaviour of block reorganization of a \n  \n \nACCEPTED MANUSCRIPT \n \n 3 \nmodern disk drive. \nThe remainder of this paper is organized as follows. An overview of modern disk drives is introduced \nin Section 2. Section 3 describes some important features of workload. The motivations of this paper are \ndepicted in section 4. Section 5 illustrates how to implement the block reorganization. The simulation \nenvironment and experimental validation are depicted in section 6. Section 7 concludes the paper with \nremarks on main contributions and indications. \n \n2. Modern Disk Drive Overview \nDisk access time \naccessT  is mainly composed of seek time seekT , rotational latency rotateT   and \ndata transfer time transferT . The seek time measures the time for the disk head to move to a specified track. \nWhen the disk head arrives at the required track, the time spent on rotating the required sector to appear \nunderneath the disk head is called rotational latency. The data transfer time is the amount of data divided \nby data transfer rate.  accessT  is expressed as follows: \naccessT = seekT + rotateT + transferT                           (1) \n2.1. Seek Time \nDisk head is driven by a VCM to move over the recording surface to seek a target sector. In order to \nreduce the heat dissipation of the VCM, the temperature of the coil in the VCM is controlled by selecting a \nfixed maximum current for the seek distances which exceed a threshold. This threshold is typically 35% of \na full stroke [40]. For a long seek distance which exceeds the threshold, the current in the coil reaches the \nmaximum value when the disk head reaches a nominal maximum velocity. At the end of an acceleration \nperiod, the current is removed from the coil, which incurs a coast period that maintains a nominal \nmaximum velocity. Then, the fixed maximum current is applied to the coil in an opposite direction to \ndecelerate the disk head. When the disk head reaches the target track, a procedure is triggered to verify the \ncurrent position. The time cost for the disk head to settle at the end of a seek is called settling time. \nTherefore, a seek time is composed of an acceleration time \nacct , a coast period coastt , a deceleration time \ndect , and a head settling time settlet . The acceleration time and deceleration time are proportional to the \nsquare root of the seek distance. The cost period is linear in the seek distance. For a short seek (e.g. single \ncylinder seek), the disk arm accelerates and decelerates without reaching the nominal maximum velocity.  \nAccording to the above discussion, for the seek distances which are shorter than the threshold, the \ndisk heads will never reach the nominal maximal velocity. This indicates that in this scenario there is no \ncost period no matter how fast a VCM is. The average seek time is generally taken to be the average time \nneeded to seek between two random blocks on the disks which is normally called average seek \ndistance averageD . The averageD  for a large number of random seeks is equal to a seek across 1\/3 of the \ndata zone which is shorter than the threshold. Therefore, the coast time of average seeks is zero. \nConsequently, we have an acceleration phase followed immediately by a deceleration phase [18], which \ncan be described as averageD =\n2)21( accacc ta \u00d7\u00d7 + 2)21( decdec ta \u00d7\u00d7 , where acca  is the acceleration \n  \n \nACCEPTED MANUSCRIPT \n \n 4 \nwhich is equal to the deceleration deca , and the acceleration time acct is equal to the deceleration time dect . \nWe assume that aaa decacc == , then we have: \nacct = dect =\na\nDaverage\n                           (2) \nThe average seek time seekT  is computed with the following equation: \nseekT =2\u00d7\na\nDaverage\n+\nsettlet                        (3) \nFor random small requests, seek time is a major component of disk access time, because the settling \ntime dominates the overall short seeks and the settling time has remained largely constant [1]. However, \nover the last decade, areal density has achieved 100% growth. This has resulted in 50% growth of track \ndensity measured in Tracks Per Inch (TPI), and 30% growth of linear density measured in Bits Per Inch \n(BPI) [11]. Due to the increased BPI, there are more sectors on a track, which means more sectors in a \ncylinder if the number of disk heads is not changed [31]. Within a certain range of data, a bigger cylinder \nmay impact the seek time in two ways: The first one is increasing the probability of reducing the number \nof seeks. When dealing with a certain amount of data, having a bigger cylinder raises the probability that \nthe next data request will be satisfied in the current cylinder, thus avoiding a seek completely. The second \none is reducing seek distance. If the size of each cylinder is increased, then an equal amount of data will \noccupy fewer cylinders compared with before. As a result, the seek distance is decreased. Both impacts \nresult in shorter seek time in terms of equation (3). \nLumb et al [24] investigated the impact of seek time, rotational latency, and data transfer time that add \nup to 100% of the disk head utilization for five modern disk drives which were sold on the market from \n1996 to 1999. Their investigation indicated that the faster seek of the Cheetah 18LP (average seek time \n5.2ms), relative to the Cheetah 9LP and Cheetah 4LP which have average seek time of 5.4ms and 7.7ms \nrespectively, resulted in lower seek components. The results also showed that as the request size of random \nworkload increased, larger request size yielded larger media transfer component, reduced the seek and \nrotational latency components by amortizing larger transfer over each positioning step. \n \n2.2. Rotational latency \nRotational latency depends on the RPM and the number of sectors that must pass underneath the disk \nhead. Traditionally, when the disk head arrives at a target track, it must wait for the disk platters to rotate \nuntil it reaches the first sector of the request before it begins to transfer data. The amount of time it takes \nfor the required sector to appear underneath the disk head is called rotational latency. If the disk head \nsettles above a sector which is one of the required sectors but not the first one, it will incur almost one \nrevolution to reach the first sector. \nZero-latency access, which is a new feature of modern disk drives, can start transferring data when \nthe disk head is positioned above any of the sectors in a request. If multiple contiguous sectors are required \nto be read, the disk head can read the sectors from the media into its buffer in any order with zero-latency \naccess support. The sectors in the buffer are assembled in ascending Logical Block Number (LBN) order \nand sent to the host. If exactly one track is required, the disk head can begin reading data as soon as the \n  \n \nACCEPTED MANUSCRIPT \n \n 5 \nseek is completed. It involves no rotational latency because all sectors on the track are needed. The same \nconcept applies to writes with a reverse procedure which moves the data from host memory to the disk \ncache before it can be written onto the media [39]. Therefore, the rotational latency decreases with the \ngrowth of the useful blocks in a track. \n \n2.3. Data Transfer Time and Zoned Bit Recording \n \nData transfer time is the amount of data divided by data transfer rate. This consists of two parts. The \nfirst part is an external data rate adopted to measure the transfer rate between memory and disk cache. The \nsecond part is employed to measure the transfer rate between disk cache and disk storage media, this part is \ncalled IDR. Due to the mechanical components in disk drives, the IDR is much lower than the external \ndata rate. Generally, the IDR is employed to measure the data transfer rate of disk drives because it is raw \ntransfer rate. The IDR depends on the combination of BPI and RPM. The BPI indicates how many bits can \nbe stored on a track, which in turn determines the number of sectors on a track. The data transfer time can \nbe calculated with following equation: \ntransferT =\ntrack\nrequest\nN\nN\n\u00d7\nRPM\n60\n                           (4) \nwhere trackN  denotes the number of sectors on a track, and requestN  is the data length of a request \nmeasured in sectors. \nDue to the geometric features, outer tracks on disk platters are much larger than inner tracks. Modern \ndisk drives employ a technique called ZBR, sometimes called Zoned Constant Angular Velocity (ZCAV), \nto take advantage of the geometric features to maximize disk capacity by varying the number of sectors per \ntrack with the distance from the spindle [30]. This technique groups tracks into zones based on their \ndistance from the spindle, and assigns each zone a different number of sectors per track. Outer zones are \nlonger and contain more sectors than the shorter inner zones. The ratio of the sectors of the outmost zone to \nthat of the innermost zone ranges from 1.43 to 1.58, according to the disk characteristics illustrated in [24]. \nIn terms of equation (4), for the same amount of data, the ZBR results in a much smaller data transfer time \nof the outer zones than that of the inner zones. \n \n2.4. Disk Cache \n \n Almost all modern disk drives employ a small amount of on-board cache (RAM) to speed up access \nto data on the disk drives [17]. Today's SDRAM has access time ranging from 7 to 10 nanoseconds. We \nassume that 512Byte data (one sector) needs to be accessed in a SDRAM. The SDRAM has 64 bit chip \nconfiguration and 10 nanoseconds access time. The data access overhead is about 6.4\u00d710-4 milliseconds. \nThis overhead is only 0.032% of the latest Hitachi Ultrastar 15K which has an average access time of 2 \nmilliseconds and an RPM of 15000. Because accessing data from cache is much faster than accessing from \ndisk media, disk cache can significantly improve performance by avoiding slow mechanical latencies if the \ndata access is satisfied from the disk cache (cache hit). Disk cache today can hold more data due to the \nincreasing cache size (Ultrastar 15K has 16MB disk cache), resulting in a higher hit ratio. As the hit ratio \ngrows, the benefits of reducing the seek time, rotational latency, and data transfer time decrease.  \nHsu and Smith [12] reported that disk cache in the megabyte range is sufficient. For a very large disk \n  \n \nACCEPTED MANUSCRIPT \n \n 6 \ncache, its hit ratio continues to slightly improve as the cache size is increased beyond 4% of the storage \nused. This indicates that if the disk cache size grows beyond a certain threshold, the increased cache only \nachieves a limited contribution to the hit ratio, which is not cost-effective. \nDisk cache works on the premise that the data in the cache will be reused often by temporarily \nholding data, thus reducing the number of physical access to the disk media [7]. To achieve this goal, \ncaches exploit the principles of spatial and temporal locality of reference. The spatial locality implies that \nif a block is referenced, then nearby blocks will also soon be accessed. The temporal locality implies that a \nreferenced block will tend to be referenced again in the near future. As the data locality improves, the hit \nratio of disk cache grows. Of all the I\/O optimizations that increase the efficiency of I\/Os, reducing the \nnumber of physical disk I\/Os by increasing the hit ratio of disk cache is the most effective method to \nimprove disk I\/O performance. \n \n3. Data Access Locality \n \nData locality is a measure of how well data can be selected, retrieved, compactly stored, and reused for \nsubsequent accesses. In general, there are two basic types of data locality: temporal, and spatial. The \ntemporal locality denotes that a data is accessed at one point in time will be accessed in the near future. \nThe temporal locality relies on the access patterns of different applications and can therefore change \ndynamically. The spatial locality defines that the probability of accessing a data is higher if a data near it \nwas just accessed. Unlike the temporal locality, the spatial locality is inherent in the data managed by a \nstorage system. It is relatively more stable and does not depend on applications, but rather on data \norganizations.  \n \n3.1. Data Access Pattern and Locality \n \nData locality is a property of both the access patterns of applications and data organization. Even \nthough reshaping access patterns can be employed to improve temporal locality [3], it is difficult to modify \nthe access patterns of those existing applications. Some methods have been proposed in the pattern \nrecognition community to preserve the locality when the original data is projected into a lower dimensional \nfeature space [22]. \nMany common access patterns approximate a Zipf-like distribution indicating that a few blocks are \nfrequently accessed, and others much less often [43]. Staelin and Garcia-Molina [41] observed that there \nwas a very high locality of reference on very large file systems (on the order of one tera byte). Some files \nin the file system have a much higher skew of access than others. The skew of disk I\/O access is often \nreferred to as 80\/20 rule of thumb, or in more extreme cases, 90\/10 Rule. The 80\/20 rule indicates that \ntwenty percent of storage resources receive eighty percent of I\/O accesses, while the other eighty percent \nof resources serve the remainder twenty percent I\/O accesses. Furthermore, the percentages are applied \nrecursively. For example, twenty percent of the twenty percent storage resources serve eighty percent of \nthe eighty percent I\/O accesses [8, 41]. \n The skew access patterns bring opportunities for block reorganization or migration. Generally, the \ntwenty percent of the storage resources which receive eighty percent of I\/O accesses are distributed across \nthe whole disk. If the twenty percent data blocks residing in the storage resources are packed together, the \nspatial locality would be enhanced and the frequently accessed blocks could remain in the disk cache \nlonger. \n  \n \nACCEPTED MANUSCRIPT \n \n 7 \n \n3.2. Block Correlations and Aggressive Prefetch \n \nBlock correlations commonly exist in storage systems. Two or more blocks are correlated if they are \nlinked together semantically. A lot of algorithms can be used to extract the correlations [13, 20, 21]. For \nexample, C-Miner [21] employs a data mining technique called frequent sequence mining to discover \nblock correlations in storage systems. These methods are very useful to exploit complex block correlations \nor extreme long-range dependence.  \nRiska and Riedel [34] reported that seek distances in some traces exhibit extreme long-range \ndependence. However, simple block correlations (e.g. spatial locality) are common patterns in storage \nsystems, because most of the correlated blocks are usually accessed very close to each other though it may \nnot be true for large files [35]. Fortunately, many studies indicate that the files in a file server are small \nfiles. Baker [2] reported that 80% of file accesses in file servers are less than 10KB. Nine years later, \nRoselli et al. [35] found that small files still comprised a large number of file accesses even though the \nnumber of accesses to large files had increased since the study in [2]. Riska and Riedel [34] measured the \ndisk drive workloads in systems representing enterprise, desktop, and consumer electronics environments. \nThey found that the common request size is 4KB across all traces, except the video streaming and game \nconsole which issue 128KB requests. Tanenbaum et al.[42] investigated the file size distribution on UNIX \nsystems in 1984 and 2005, respectively. They reported that the files which were smaller than 8KB was \n84.97% in 1984, and 69.96% in 2005. Also that 99.18% of files in 1984 and 90.84% of files in 2005 are \nsmaller than 64K, respectively. \nBlock correlations generally depend on how the file system above organizes the disk blocks. A lot of \neffort has been invested to optimally organize disk blocks, thus improving the access performance of small \nfiles. Fast File System (FFS) [28] determines the location of the last allocated block of its file and attempts \nto allocate the next contiguous disk block when a new block is allocated. When blocks of a file are \nclustered, multiple block transfers can be used to read\/write the file, therefore, reducing the number of disk \nI\/O and disk access latency. Co-located Fast File System (C-FFS) [10] adjacently clusters the data blocks \nof multiple small files especially the small files in the same directory and moves to and from the disk as a \nunit. C-FFS attempts to allocate a new block of a small file into an existing unit associated with the same \ndirectory. Reiserfs [33] uses balanced trees to store data and metadata. For extremely small files, the entire \nfile\u2019s data can be stored physically near the file\u2019s metadata, so that both can be retrieved together with little \nor no disk seeking time. Log-structured File System (LFS) [23] delays, remaps, and clusters all data blocks \ninto large, contiguous regions called segments on disks. LFS only writes large chunks to the disk, which \nexploits disk bandwidth for small files, metadata, and large files. Deng et al. [6] suggested clustering the \nsmall files as the size of the product of one cylinder size and disk number in a network attached system. \nThe above works illustrate that most of modern file systems tend to place correlated disk blocks close \nto each other and cluster blocks to reduce the number of disk I\/Os and disk access latency. Such behaviours \nagain confirm that simple locality is an inherent characteristic of disk drive workloads [37]. Based on the \nobservation that a block is usually semantically correlated to its neighbour blocks especially for small files \n(For example, if a file\u2019s blocks are allocated in a disk drive consecutively, these blocks are correlated to \neach other. Therefore, in some workloads, these blocks are likely accessed one after another.), most of the \nmodern disk drivers adopt aggressive sequential prefetch which takes advantage of the spatial locality to \nimprove disk I\/O performance. \n \n  \n \nACCEPTED MANUSCRIPT \n \n 8 \n4. Motivations \n \n \nFig. 1 Disk layout comparison with and without block reorganization \n \nDue to the highly skewed access patterns, if the frequently accessed blocks are distributed across the \nwhole disk drive, long seek distances from each other may be involved. Most modern disk drives employ \naggressive sequential prefetch to exploit spatial locality and improve I\/O access performance, but the effect \nof the prefetch could be significantly reduced in the above scenario, because a large number of blocks with \nlow access frequency could be prefetched to disk cache, thus reducing the hit ratio of the disk cache. \nMoving the frequently accessed data blocks to a small area to enhance the data locality can significantly \nimprove the effectiveness of the aggressive prefetch. Hsu et al.[14] recommended placing the reorganized \ndata blocks roughly at a 24-33% radial distance offset from the outer edge. The reason is that most of the \ndisk reads are either eliminated due to the more effective sequential prefetch, or can be satisfied from the \nreorganized area. They believed that the remaining disk reads tend to be uniformly distributed. Therefore, \nplacing the reorganized data blocks at the centre of the disk can reduce seek time significantly [1, 36]. \nHowever, due to the ZBR technique, the data transfer rate of the outmost zone is much higher than that of \nthe innermost zone (ranging from 43% to 58% in [24]). Please note that the disk drives in [24] are about \nten years old. Because of the increasing magnetic recording density, the ratio at present is much higher \nthan that. As discussed in section 3.1, most of the data accesses are highly skewed. For a skew of 90\/10, \n90% of the data accesses go to the hot areas. In the 90% data accesses, if we assume that 30% of the data \naccesses are absorbed by disk cache, the remaining 60% references have to access disk drive. If the hot \ndata blocks are placed in the out zones, the 60% references can take full advantage of the ZBR. Even \nthough the remaining 10% infrequent data accesses could distribute uniformly (It is arguable). On the \ncontrary, if the hot data blocks are placed at the centre of the disk, 60% of data accesses will lose the \nbenefits of ZBR, even though the placement can leverage the 10% infrequent data accesses to reduce the \nseek time to a certain degree. Therefore, we believe that the benefits achieved by putting the hot data \nblocks in the outer zones are bigger than placing the data blocks in the centre of a disk drive.  \nOur method is based on the observation that only a small portion of blocks are accessed frequently. \nWe divided a drive disk into a fast band and a slow band. The fast band is used to store the frequently \naccessed data on outer cylinders to take advantage of the ZBR. The size of fast band is determined as 10% \nof the disk drive capacity in terms of the 90\/10 ruler of thumb. Because most of the frequently accessed \nblocks are packed into the fast band, the spatial locality is enhanced. Due to the observation that a block is \nusually correlated to its neighbor blocks as discussed in section 3.2, a migration unit called object which \npacks the frequently accessed blocks and the correlated neighbour blocks together is adopted to maintain \nMax LBN \nOriginal Layout \nLBN 0 \nLBN 0 Max LBN \nReorganized \nLayout \nHot Blocks \nHot Blocks Correlated Blocks \nObject \nO1 O2 O3 \nO2 O1 O3 \n  \n \nACCEPTED MANUSCRIPT \n \n 9 \nthe block correlations. We clustered multiple adjacent blocks into one object and moved objects rather than \ncylinders, files or blocks to enhance the aggressive prefetch of modern disk drives. \nFor decades past, the most common storage interfaces (SCSI and IDE\/ATA), which expose storage \ncapacity as a linear array of fixed-sized blocks to file systems, mainly consist of simple read and write \ncommands. Data access for read and write is specified by a LBN and a data block length. Disk firmware is \nresponsible for translating LBN to physical location (C\/H\/S). Fig. 1 depicts the two disk layouts with and \nwithout block reorganization. In the upper figure, three objects with different access frequency are \ndistributed across the disk drive which is organized in terms of LBN. Each object consists of frequently \naccessed blocks (hot blocks in Fig. 1) and the correlated neighbour blocks. The bottom figure shows the \ndisk layout after object 2 and object 3 are migrated to the outer zones of the disk drive. Object 1 remains in \nthe original location even if it has the highest access frequency, because its location is in the fast band. The \nobject 3 has higher access frequency than that of object 2, and therefore the object 3 is moved to the \noutermost zone first, followed by the object 2. The upper figure illustrates that due to the long inter-object \ndistance between object 1 and the other objects, long seek distance would be incurred if the data accesses \nto the hot blocks contained in the objects are interleaved, and the aggressive prefetch in modern disk drives \ncould prefetch a large number of useless blocks which will not be accessed for a long time to the disk \ncache, thus decreasing the hit ratio of the disk cache. The bottom figure depicts a new disk layout after \nblock migration. The seek distances among object 1, object 2 and object 3 are reduced significantly.  \nBased on the above block arrangement, if the fast band receives most of the data accesses, we expect \nthe following benefits: \n(1) The disk head lingers over the fast band most of the time and the seek time is significantly \nreduced. \n(2) Zero-latency access of modern disk drives could achieve more benefit because the probability that \nmore useful blocks are accessed within one revolution is increased. Therefore, the rotational latency could \nbe reduced. \n(3) The data transfer time is decreased because most of the frequently accessed blocks are clustered in \nthe fast band to utilize the ZBR. \n(4) The spatial locality is enhanced because most of the frequently accessed blocks are packed in a \nrelatively narrow area. \n(5) Due to the enhanced spatial locality, the blocks of fast band could reside in the disk cache much \nlonger, thus increasing the hit ratio. \n(6)The number of physical disk I\/Os is decreased due to the increased hit ratio. \n(7)The block correlations are maintained because the migration unit packs the frequently accessed \nblocks and the correlated neighbour blocks together. \n \n5. Implementation \n \n  \n \nACCEPTED MANUSCRIPT \n \n 10 \n \nFig. 2 A schematic of the block reorganization \n \nOur Implementation is composed of a frequency tracker, a mapping table, and a data migration \nmechanism. The three major components are illustrated in Fig. 2. The frequency tracker monitors the \nstream of I\/O requests. Periodically, it produces or updates a mapping table which contains a list of \nfrequently accessed objects ordered by frequency. A newly incoming request is compared against the \nmapping table. The request will be redirected to a new location if the request is hit in the table. Otherwise, \nit goes to the original location. The data migration is triggered periodically in terms of the migration \nmechanism. The frequently accessed blocks and the correlated blocks will be moved to the fast band to \nenhance the spatial locality and take advantage of the ZBR. \n \n5.1. Frequency Tracker \n A data structure consisting of original object location, migrated object location, object access \nfrequency and unoccupied block number in the object is employed to track the object access frequency and \nconstruct the mapping table. The data structure takes 16 bytes per object. For a 9.1GB disk drive which has \n17938986 blocks (the disk we will use in the simulation) with an object size of 32 blocks, it takes \n(17938986\/32)*16=8969493Bytes=8.55MB storage capacity to track the whole disk drive. By analogy, a \n100 GB modern disk needs about 90 MB storage space to track all objects. Although the storage overhead \nis much smaller than the disk itself, the data structures should be maintained in memory so that they can be \naccessed with little overhead. Therefore, the storage capacity occupied by the data structures is important \nand should be kept as small as possible. A more space-efficient alternative is to maintain a small group of \ndata structures for objects that have been recently accessed frequently. According to the skewed access \npattern (10% storage resources receive 90% I\/O accesses), the maximum number of objects which should \nbe tracked is 10% of the whole objects. It is very easy to calculate that a 100GB disk drive needs about \n9MB memory capacity to track the frequency of the most frequently accessed objects. This is acceptable \nfor a modern computer system.  \nWe used two fixed length Least Recently Used (LRU) lists including a hot list and a recent list to \nidentify the most frequently and recently accessed objects. When the system receives a request, the \ncorresponding object will be recorded on the recent list. If the object on the recent list is accessed again in \na short period, the object will be promoted to the hot list. If the promoted object is already on the hot list, \nthe object will be moved to the head of the hot list. If the hot list is full, the last object on the hot list will \nbe degraded to the recent list. If the recent list is full, the last object on the recent list will be discarded, and \nthe fields of object location including the original location and the migrated location will be recorded on a \nMapping Table \nFast Band Slow Band \nFrequency Tracker \nTable Modification \nData Migration \nLBN 0 Max LBN \nData Request \n  \n \nACCEPTED MANUSCRIPT \n \n 11 \nmapping table. The objects on the hot list are the most frequently accessed objects over a period. However, \nthe sequence of the objects is not sorted in terms of the frequency. The most recently accessed object is \nalways on the head of the hot list, the next recently accessed object will follow the previous object. \nTherefore, compared with the frequency counting method, our method can further enhance the spatial \nlocality which indicates that the probability of accessing a data unit is higher if a data unit near it was just \naccessed. The method is very effective in our experiment. \nWhen the data migration mechanism is triggered, a process runing in background examines the hot \nlist. If the objects on the hot list are stored in the slow band, the objects will be moved to the fast band. If \nthe objects already exist in the fast band, the object locations will be kept unchanged. When the fast band \nruns out of its 90% capacity, the objects which are not on the hot list will be migrated to the slow band. \nThe fields of original object location and migrated object location in the data structure will be employed to \nrecord the objects location and construct the mapping table. \n \n5.2. Object Mapping Table \nData access for read and write is specified by a LBN (Logical Block Number) and a data block length. \nDisk firmware is responsible for translating LBN to physical location (i.e. cylinder, head, and sector). This \nhigh-level interface has enabled great portability, interoperability, and flexibility for storage devices and \ntheir vendors [9]. However, the narrow storage interface between file systems and storage hides details \nfrom both sides. Though both sides have made considerable advancement independently, the interface has \nlimited opportunities for whole system performance improvement due to lacking effective cooperation. \nA mapping table is employed to augment the interface in our implementation, because the mapping \ntable contains some hints of disk access patterns coming from the above file system. The mapping table \ncontains a list of frequently accessed objects, their original locations, and the locations after data moving. \nNewly incoming requests are compared against the list and redirected to the new location if the requested \nobject resides there. \n \n5.3. Data Migration Mechanism \n \n Data migration could have significant impact on the overall I\/O performance. The key point of a data \nmigration is to complete the data moving process in the shortest possible time with minimal performance \nimpact on the foreground applications, while guaranteeing QoS. It involves when and what blocks should \nbe migrated to where. \nBecause workloads tend to be bursty, there are enough idle periods for the system to analyze and \nreorganize the data blocks (e.g. perform the data migration daily) [1, 12, 14, 15]. Hsu and Smith [14] \nreported that there is a lot of time during which the storage system is relatively idle. They also proved that \ninfrequent (daily to weekly) block reorganization is sufficient to realize most of the benefit. Their \nexperiments confirm that the data migration takes only a small fraction of the idle time available between \nreorganizations. On the other hand, some intelligent algorithms which can eliminate or alleviate the impact \nof data migration have been proposed and developed. Aqueduct [25] uses a control-theoretical approach to \nstatistically guarantee a bound on the amount of impact on foreground work during a data migration, while \nstill accomplishing the data migration in a time period as short as possible. Lumb et al [26] proposed a free \nblock scheduling to replace a disk drive\u2019s rotational latency with useful background media transfers, \npotentially allowing the background disk I\/O to occur with no impact on foreground service times. The two \nmethods are supposed to further alleviate or eliminate the performance penalty of data migration. Because \n  \n \nACCEPTED MANUSCRIPT \n \n 12 \nthis paper is exploring the performance gains of block reorganization, not the data migration methodology, \nthe reader is referred to [25, 26] for a comprehensive understanding of data migration methodologies. \n \n6. Experimental Validation \n \nTable 1 Disk characteristics of Quantum Atlas 10K \nSize 9100 MB \nDisk cache size 2MB \nCylinders 10024 \nNumber of Zones 24 \nSectors per track of outermost zone 334 \nSectors per track of innermost zone 229 \nRotation speed (RPM) 10025  \nSingle cylinder seek time 1.24500 \nFull strobe seek time 10.82800 \nHead switch time 0.17600 \n \nA real implementation of the comprehensive and complicated system would be difficult and take an \nextremely long time. Trace driven simulation is a principal approach to evaluate the effectiveness of our \nproposed design, because it is much easier to change parameters and configurations in comparison with a \nreal implementation. The trace driven simulation is a form of event driven simulation in which the events \nare taken from a real system that operates under conditions similar to the ones being simulated. By using a \nsimulator and reference traces, we can evaluate the system in different environments and under a variety of \nworkloads.  \nDiskSim [4] is an efficient, accurate, highly configurable, and trace-driven disk system simulator. To \nexplore the performance gains of block reorganization based on modern disks, we enhanced DiskSim to \nmeasure the proposed method. Several experimentally validated disk models are distributed with Disksim. \nThe experimental results reported in this paper were generated by using the validated Quantum Atlas 10K \ndisk model. The disk drive is divided into 24 Zones in terms of the number of sectors per track, head skew, \ncylinder skew and number of spare sectors. The detailed disk characteristics are summarized in Table 1. \nTwo important metrics normally employed to measure I\/O performance are throughput and average \nresponse time. Throughput is the maximum number of I\/Os that can be satisfied in a given period by the \nsystem. Measuring the throughput with trace driven simulation is difficult because the workloads recorded \nin the trace are constant. Average response time includes both the time needed to serve the I\/O request and \nthe time spent on waiting or queuing for service. In this paper, we measure the average response time and \nbreak down the disk access time into four major components including seek time, rotational latency, data \ntransfer time, and the hit ration of disk cache, and analyze each component separately to determine its \nactual performance gains. \n \n6.1. Evaluating the impact of object size \n \nIn order to explore the impact of the object size, we employed a real trace ST1 which extracts 50,000 \nrequests from the cello99 trace [5] and modifies some fields in terms of the format requirements of the \nDisksim. The access type (read or write), request address, request size, and request arrival time are kept \n  \n \nACCEPTED MANUSCRIPT \n \n 13 \nunchanged to maintain the block correlations and data access pattern. The average request size of ST1 is \n7.2 KB. 65% requests of ST1 are read accesses. The skew of ST1 is 74\/10. It means that 74% I\/Os in ST1 \nconcentrate on 10% disk space. \n \n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\nBaseline 177 64 32 16 8\nObject Size (KB)\nA\nv\ner\nag\ne \nRe\nsp\no\nn\nse\n \nTi\nm\ne \n(m\ns)\n   \n30.00%\n32.00%\n34.00%\n36.00%\n38.00%\n40.00%\nBaseline 177 64 32 16 8\nObject Size (KB)\nD\nisk\n \nCa\nch\ne \nH\nit \nR\nat\nio\n \nFig. 3 Average response time with trace ST1        Fig. 4 Hit ratio of disk cache with trace ST1 \n \n34000\n34500\n35000\n35500\n36000\n36500\n37000\nBaseline 177 64 32 16 8\nObject Size (KB)\nTh\ne \nN\nu\nm\nbe\nr \no\nf P\nhy\nsic\nal \nD\nisk\n \nI\/O\n   \n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\nBaseline 177 64 32 16 8\nObject Size (KB)\nA\nv\ner\nag\ne \nSe\nek\n \nTi\nm\ne \n(m\ns)\n \n Fig. 5 Number of physical disk I\/Os with trace ST1      Fig. 6 Average seek time with trace ST1 \n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n1.6\n1.8\n2\n2.2\n2.4\n2.6\nBaseline 177 64 32 16 8\nObject Size (KB)\nA\nv\ner\nag\ne \nR\no\nta\ntio\nn\nal \nLa\nte\nn\ncy\n \n(m\ns)\n   \n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1.1\nBaseline 177 64 32 16 8\nObject Size (KB)\nA\nv\ner\nag\ne \nD\nat\na \nTr\nan\nsf\ner\n \nTi\nm\ne \n(m\ns)\n \n Fig. 7 Average rotational latency with trace ST1      Fig. 8 Average data transfer time with trace ST1 \n \n We used ST1 to do the first round of measurements. A set of object sizes ranging from 177KB to 8KB \nwere employed to investigate the optimal block correlations size by measuring the average response time \nincurred by the block reorganization. 177KB is the size of one segment of the disk cache extracted from \nthe Quantum Atlas 10K disk model. The leftmost bar in each figure from Fig. 3 to Fig. 8 denotes the \nperformance of the baseline system which does not employ any block reorganization. \n Fig. 3 shows the average response time of the system which uses different object size to migrate \nblocks against the baseline system. It illustrates that the performance improvement ranges from 5% to \n17.2% due to the block reorganization. According to the performance improvement, the optimal object \n  \n \nACCEPTED MANUSCRIPT \n \n 14 \nsize is 32KB. We believe that the object size maintains the block correlations of the baseline system very \nwell because it achieves the highest performance improvement. \n Fig. 4 illustrates the hit ratio of disk cache. Before the test, we expected to see a significant increase \nof the hit ratio because of two reasons. The first reason is that the spatial locality is enhanced by gathering \nfrequently accessed blocks. The second reason is that the prefetch could get more useful data which could \nstay in the cache much longer due to the high access frequency. However, as the object size decreases, the \nhit ratio also starts to decrease gradually and decrease acutely when the size reaches 16KB. The number of \nphysical disk I\/Os shown in Fig. 5 varies slightly when the object size is changed from 177KB to 32KB, \nwhereas it has a sharp increase when the object size reaches 16KB. We believe that the object size which is \nsmaller than 32KB could break the block correlations, accordingly incur more physical disk I\/Os and \ndecrease the hit ratio. Another reason is that the disk cache of the employed disk model in our simulation is \nonly 2MB. The small disk cache can not take full advantage of the enhanced data locality. The results in \nFig. 5 are consistent with that in Fig. 4. \n Fig. 6 depicts that the average seek time is significantly reduced when the object size is changed to \n32KB. The seek time is reduced 42.5% when the object size reaches 16KB. This validates our expectation \nthat the disk head lingers over the fast band most of the time, and the seek time is reduced due to the \nreduced seek distance. A basic requirement is a relatively small object size which can cluster the frequently \naccessed blocks compactly. \n Fig. 7 shows that the rotational latency achieves a slight performance improvement (less than 2.8%) \nwhen the object size varies. Fig. 8 illustrates that the data transfer time starts to decrease when the object \nsize is changed to 64KB. The data transfer time achieves the highest 2.5% performance improvement when \nthe object size is 32KB. These two measurements are not expected. According to equation (4), larger \nrequest size yields more performance gains at the data transfer time. Because the average request size of \nST1 is 7.2 KB, we decided to do the second round of test with another trace by varying the request size. \nThe object size in the second round of test is determined as 32KB because it maintains the basic block \ncorrelations. \n \n6.2 Evaluating the impact of request size \n \nSynthetic traces have the advantage of isolating specific behaviours which are not clearly expressed in \nthe real world traces. The trace characteristics can be varied as much as possible in order to cover a wide \nrange of different workloads. Therefore, we generated a synthetic trace named ST2 to investigate the \nimpact of the request size. ST2 consists of 25,000 requests which also were extracted from cello99 [5]. In \ncontrast to the ST1, we changed the request size of ST2 to explore the impact on the system performance. \nAs discussed in section 3.2, the biggest request size in the disk drive workloads is 128KB. Therefore, we \ninvestigated the request size of 8KB, 16KB, 32KB, 64KB, and 128KB, respectively. We believe bigger \nrequest sizes can obtain more benefit from the ZBR [19]. The skew of ST2 is 87\/10 which implies that \n87% I\/Os are accumulated in 10% disk space. \n \n  \n \nACCEPTED MANUSCRIPT \n \n 15 \n0%\n20%\n40%\n60%\n80%\n100%\n8 16 32 64 128\nRequest size(KB)\nPe\nrc\nen\nta\nge\n \no\nf d\nisk\n \nac\nce\nss\n \ntim\ne\nSeek time Rotational latency Data transfer time\n   \nFig. 9 Breakdown of disk access time with trace ST2 \n \nTable 2 Constitutions of disk access time with different request size \n 8KB 16KB 32KB 64KB 128KB \nSeek time 45.59% 40.97% 34.28% 24.77% 16.91% \nRotational latency 37.95% 33.16% 26.84% 18.72% 12.49% \nData transfer time 16.46% 25.87% 38.88% 56.51% 70.60% \n \nFig. 9 lists a breakdown of disk access time produced by ST2 with different request sizes. The test \nresults are based on the 10025 RPM Quantum Atlas 10K disk. It shows that with the increase of the \naverage request size, the portion of seek time and rotational latency are decreased, whereas the portion of \ndata transfer time is increased. This is reasonable because larger request size involves more data transfer \ntime. Table 2 shows when the average request size is changed, the percentages of each component \nincluding seek time, rotational latency, and data transfer time which contributes to the disk access time. \n \n0.00\n3.00\n6.00\n9.00\n12.00\n15.00\n18.00\nAverage\nresponse\ntime(ms)\nRequest size (KB)\nT1 4.16 5.74 8.92 15.44 16.36 \nT2 3.36 4.92 8.09 14.70 15.98 \n8 16 32 64 128\n   \n0.00%\n10.00%\n20.00%\n30.00%\n40.00%\nDisk cache\nhit ratio\nRequest size(KB)\nT1 38.65%35.60%32.96%27.36%22.12%\nT2 37.85%36.34%34.58%28.30%23.41%\n8 16 32 64 128\n \n   Fig. 10 Average response time with trace ST2    Fig. 11 Hit ratio of the disk cache with trace ST2 \n \n0\n4000\n8000\n12000\n16000\n20000\n24000\nNumber of\nphysical disk\nI\/Os\nRequest size(KB)\nT1 18198 18811 19481 20885 23289\nT2 18119 18556 18974 20603 22796\n8 16 32 64 128\n   \n0.00\n0.50\n1.00\n1.50\n2.00\n2.50\n3.00\n3.50\nAverage\nseek\ntime(ms)\nRequest size(KB)\nT1 3.08 3.02 2.96 2.83 2.79 \nT2 1.63 1.63 1.70 1.72 1.79 \n8 16 32 64 128\n \n  \n \nACCEPTED MANUSCRIPT \n \n 16 \nFig. 12 Number of physical disk I\/Os with trace ST2     Fig. 13 Average seek time with trace ST2 \n \n0.00\n1.00\n2.00\n3.00\nAverage\nrotational\nlatency(ms)\nRequest size(KB)\nT1 2.56 2.44 2.32 2.14 1.98 \nT2 2.56 2.47 2.35 2.16 2.07 \n8 16 32 64 128\n   \n0.00\n3.00\n6.00\n9.00\n12.00\n15.00\nAverage\ndata transfer\ntime(ms)\nRequest size(KB)\nT1 1.11 1.91 3.36 6.47 11.52 \nT2 1.08 1.85 3.43 6.62 12.01 \n8 16 32 64 128\n \nFig. 14 Average rotational latency with trace ST2      Fig. 15 Average data transfer time with trace ST2 \n  \nIn Fig. 10, Fig. 11, Fig. 12, Fig. 13, Fig. 14, and Fig. 15, T1 denotes the baseline system which does \nnot involve any data migration, T2 indicates the system which employs a 32KB object size to move data \nblocks. Fig. 10 illustrates that the average response time grows with the increase of the average request \nsize. It is reasonable because bigger request size produces more data transfer time which is a portion of the \naverage response time. When the request size is changed from 8KB to 16KB, 32KB, 64KB, and 128KB, \nthe average response times are improved 19%, 14%, 9%, 5%, and 2.3%, respectively. The measurement \nindicates that bigger request size achieves less performance improvement. The hit ratios of disk cache in \nFig. 11 shows a slight increase when the disk drive conducts data reorganization. Fig. 11 also demonstrates \nthat the hit ratios are decreased with the growth of request size. The number of physical disk I\/Os depicted \nin Fig. 12 follows the same trend as the hit ratio. This is reasonable because higher hit ratios imply less \nphysical disk I\/Os. The average seek time shown in Fig. 13 achieves significant reduction ranging from \n40% to 47% when the request size is changed from 128KB to 8KB. Fig. 13 also indicates that bigger \nrequest size achieves less seek time reduction, which is consistent with Fig. 10. However, the average \nrotational latency depicted in Fig. 14 and the average data transfer time illustrated in Fig. 15 are not \nexpected. The average rotational latency does not obtain any performance improvement. The highest \nperformance improvement of the average data transfer time is only 3.1%. When the request size is \nincreased to 64KB, a slight performance penalty of data transfer time is incurred due to the data \nreorganization. Fig. 14 indicates that the average rotational latency of both T1 and T2 are reduced with the \ngrowth of the request size. As discussed in section 2.2, due to zero-latency access, the rotational latency \ndecreases with the growth of useful blocks in a track. Therefore, bigger request size can take better \nadvantage of the zero-latency access, thus improving performance. Fig. 15 confirms the observations in \nFig. 9 and Table 2. \n \n  \n \nACCEPTED MANUSCRIPT \n \n 17 \n0.00\n4.00\n8.00\n12.00\n16.00\nAverage\ndisk access\ntime(ms)\nRequest size(KB)\nT1 6.75 7.37 8.63 11.44 15.21 \nT2 5.28 5.97 7.48 10.51 14.68 \n8 16 32 64 128\n  \n0.00\n2000.00\n4000.00\n6000.00\n8000.00\nBandwidth\n(KB\/s)\nRequest size(KB)\nT1 1922.0 2787.43585.8 4145.07852.7\nT2 2379.6 3250.23956.2 4354.98010.0\n8 16 32 64 128\n \n  Fig. 16 Average disk access time with trace ST2         Fig. 17 Bandwidth with trace ST2 \n \nThe above measurements show that the significant reduction of seek time is not dramatically reflected \nin the average response time. A very important reason is that the high hit ratio of disk cache reduces the \nimpact of seek time on the average response time. Therefore, we evaluated the disk access time illustrated \nin Fig. 16 which excludes the impact of disk cache. The performance improvements of different request \nsize are 22%, 19%, 13%, 8%, and 3.5%, respectively. It confirms the illustration in Fig. 10, and shows the \nsame implication that bigger request size achieves less performance improvement. The tests indicate that \nanother reason is because the average seek time is only a portion of the average response time. That\u2019s why \nthe impact of the reduced seek time on the average response time is alleviated. \nAccording to Fig. 15, the data transfer time does not achieve too much benefit from the ZBR. In order \nto investigate the reason, we measured the bandwidth because a different request size has different but \nstraightforward impact on the bandwidth. Fig. 17 depicts that when the average request size is changed to \n8KB, 16KB, 32KB, 64KB, and 128KB, the bandwidth achieves 24%, 17%, 10%, 5%, and 2% \nimprovement, respectively. It does illustrate that our method (T2 in Fig. 17) can take advantage of the ZBR \nto improve performance. However, it does not confirm the discussion in section 2.3 that bigger request size \nshould gain more benefits from the ZBR. The reason is that if the request size is bigger than the object size, \nthe request will be split into several sub-requests. Therefore, the block correlations could be destroyed. For \na fixed object size, the bigger the request size is, the higher the probability that the requests could be split. \nWe employed 32KB as the object size in the measurements of this section. That\u2019s why the performance \nimprovement decreases with the growth of the request size. We tracked the requests in the simulation, and \nobserved many split requests even when the request size is 8KB. It gives us an indication that though the \nobject can maintain the basic correlation among data blocks, the object size actually depends on the \nworkload. For example, 32KB is the optimal object size for ST1 which has an average request size of 7.2 \nKB. That\u2019s why in the test, 8KB request size achieves the maximal performance improvement. A dynamic \nalgorithm used to determine the optimal object size for different workloads could alleviate this problem. \n \n6.3 Evaluating the impact of data access pattern \n \nAs discussed in section 6.1 and 6.2, the traces ST1 and ST2 are highly skewed. We reorganized the \nfrequently accessed data blocks into a relatively small area, thus enhancing the data locality which \nsignificantly reduces the average seek time. However, the significant reduction of seek time is not reflected \nin the average response time. We believe that a very important reason is the high hit ratio of disk cache. \nTherefore, we constructed the third synthetic trace ST3 and the fourth synthetic trace ST4 which have \ndifferent skews by modifying the trace ST1. The skew of ST3 is 44\/10 which means 44% I\/Os go to 10% \n  \n \nACCEPTED MANUSCRIPT \n \n 18 \nstorage capacity. The request addresses in ST4 are distributed across the disk drive randomly. We believe \nthat the trace ST4 can simulate an extreme scenario of the online transaction processing, and the decreased \nskew can reduce the hit ratio of disk cache. \n \n0.00\n2.00\n4.00\n6.00\n8.00\n10.00\nST1 ST3 ST4\nTraces\nA\nv\ne\nra\nge\n \nre\nsp\no\nn\nse\n \ntim\ne\n(m\ns)\nT1 T2 T3\n    \n0\n0.1\n0.2\n0.3\n0.4\n0.5\nST1 ST3 ST4\nTraces\nH\nit \nra\ntio\n \no\nf d\nis\nk \nca\nch\ne\nT1 T2 T3\n \nFig. 18 Average response time                          Fig. 19 Hit ratio of disk cache \n \n0.00\n1.00\n2.00\n3.00\n4.00\n5.00\n6.00\nST1 ST3 ST4\nTraces\nA\nv\ne\nra\nge\n \nse\ne\nk \ntim\ne(m\ns)\nT1 T2 T3\n    \n0.00\n0.50\n1.00\n1.50\n2.00\n2.50\n3.00\nST1 ST3 ST4\nTraces\nA\nv\ner\nag\ne \nro\nta\ntio\nn\na\nl l\nat\nen\ncy\n(m\ns)\nT1 T2 T3\n \nFig. 20 Average seek time                          Fig. 21 Average rotational latency \n \n0.00\n0.20\n0.40\n0.60\n0.80\n1.00\n1.20\n1.40\nST1 ST3 ST4\nTraces\nA\nv\ner\nag\ne \nda\nta\n \ntra\nn\nsf\ner\n \ntim\ne(m\ns)\nT1 T2 T3\n \nFig. 22 Average data transfer time \n \nIn order to compare our method with the existing work, we implemented an organ pipe data layout in \nthe simulator. In this section, T1 denotes the baseline system which does not involve any data migration, \nT2 indicates the system which employs a 32KB object size to move the frequently accessed data blocks to \nthe fast band, and T3 implies that the moved data blocks are organized in an organ pipe style in the centre \nof disk drive. \nFig. 18 illustrates the average response time of the three systems measured by the three different traces. \nIt shows that when the traces ST1 and ST3 are employed to evaluate the systems, our method achieves the \n  \n \nACCEPTED MANUSCRIPT \n \n 19 \nbest performance, whereas T3 degrades the performance. The reason is because the sequentially accessed \ndata is split on either side of the organ pipe arrangement, and the disk arm has to seek back and forth \nacross the disk resulting in decreased performance [36]. It is interesting to observe that the system T3 \nobtains the best performance improvement (3.8%) when ST4 is adopted to evaluate the systems. This is \nbecause the requests in ST4 are independent. It means that the requests which are not reorganized are \ndistributed randomly across the disk drive. Placing the hot area in the middle of disk drive can reduce the \nseek time when the disk arm has to be moved to locate the remaining data blocks. \n Fig. 19 demonstrates the hit ratio of disk cache. As expected, the hit ratio is reduced when the skew is \ndecreased. When we used ST4 to compare the hit ratios, the actual values of T1, T2, and T3 are 0.004%, \n0.04%, and 0.003%, which is too low to be observed in Fig. 19. The low hit ratio is reasonable, because the \nrequests in ST4 do not contain any data locality including temporal locality and spatial locality. We also \ncan observe that when ST1 and ST3 are adopted to measure the systems, the hit ratio is decreased when the \ndata blocks are organized in an organ pipe style. \nFig. 20 depicts the average seek time. The experimental results show that our method does reduce the \naverage seek time significantly by using the traces ST1 and ST3, whereas the T3 incurs a growth of the \naverage seek time. This is because T3 destroys the block correlations. As expected, Fig. 20 also shows that \ncompared with our method, T3 obtains a better performance improvement when ST4 is adopted to measure \nthe system. This is because the requests in ST4 are independent. However, as discussed in section 3, most \nof the real data accesses show some locality. It means our method is much more effective than the organ \npipe in a real scenario. Fig. 21 and Fig. 22 do not show significant performance variation when different \ntraces are used to measure T1, T2, and T3. \n \n7. Discussion and Conclusion \n \n Since the first disk drive was announced in 1956, disk drives have experienced dramatic development, \nbut still lagged far behind the performance improvement of processor and RAM. A lot of I\/O optimization \nmethods have been devised to alleviate the gap between RAM and disk driver. One of the most effective \napproaches of improving disk access performance is increasing the hit ratio of disk cache and thus \nreducing the number of physical disk I\/Os. \n We reviewed some important characteristics of modern disk drives and disk access patterns in this \npaper. Attempting to take advantage of the characteristics, we clustered the frequently accessed blocks and \nthe correlated blocks into objects, and moved the objects to the outer zones of a modern disk drive to \nenhance its data locality. Synthetic trace driven simulation was adopted to break down disk access time \ninto seek time, rotational latency, data transfer time, and hit ration of disk cache, and investigate the \nperformance gains of each of them due to the enhanced locality. Experimental results give the following \nindications: \n(1) Data blocks are correlated with each other. When performing data reorganization, an optimal object \nsize which compactly cluster the frequently accessed data blocks with the correlated data blocks can \nachieve the best performance. Otherwise, the destroyed correlations can incur performance penalty. \n(2) By reorganizing the frequently accessed data blocks into a small area, average seek time achieves \nsignificant reduction due to the enhanced data locality. However, the reduction is not reflected dramatically \nin the response time because the high hit ratio of disk cache alleviates the impact. Even so, the reduction of \nseek time contributes most of the performance improvement, and the other components constitute a small \nportion of the improvement.  \n  \n \nACCEPTED MANUSCRIPT \n \n 20 \n(3) Zero-latency access does not obtain too much benefit from the enhanced spatial locality.  \n(4) ZBR does contribute to the performance improvement, though it is relatively small in comparison with \nthe contribution of seek time. \n(5) The enhanced spatial locality does not give too many opportunities to disk cache to improve the hit \nratio and reduce the number of physical disk I\/Os. This could be caused by the disk model employed in the \nsimulation. Quantum Atlas 10k is the latest model we can find in the Disksim. The model does support \nsome new characteristics such as zero-latency, ZBR, etc. However, a disk drive with 9.1GB storage \ncapacity and 2MB disk cache is old. It can not demonstrate other characteristics such as high TPI and DPI, \nand the small disk cache can not obtain too much performance gains from the enhanced spatial locality. We \nbelieve a disk model with higher storage capacity and bigger disk cache could illustrate more interesting \nexperimental results. For example, because long distance seeks of modern disk drive may not involve \nenormously more overhead than short ones, the performance improvement of seek time could decrease, \nwhile the bandwidth could further increase. \n(6) The data access pattern has an impact on our method. However, as discussed in section 3.1, the skewed \ndata access is a normal behaviour of the workloads in real world. Therefore, we believe that real \napplications can benefit from our method. \n \n \nAcknowledgements \nWe would like to thank the anonymous reviewers for helping us refine this paper. Their constructive \ncomments and suggestions are very helpful. Thanks also to my friend Michael Sandling and Andrew \nClemens who polished the language of this article. In addition, I am grateful to Prof. Witold Pedrycz for \ngiving me the opportunity to clarify my thoughts. \n \n \nReferences \n[1]. S. Aky\u00fcrek and K. Salem, Adaptive block rearrangement, ACM Transactions on Computer Systems \n12(2) (1995) 89-121. \n[2]. M. Baker, J. Hartman, M. Kupfer, K. Shirriff, J. Ousterhout, Measurements of a distributed file \nsystem, in: Proceedings of ACM Symposium on Operating Systems Principles, 1991, pp. 198\u2013212. \n[3]. A. J. C. Bik, Reshaping access patterns for improving data locality, in: Proceedings of the 6th \nWorkshop on Compilers for Parallel Computers, 1996, pp. 229-310. \n[4]. J. S. Bucy and G. R. Ganger, The DiskSim simulation environment version 3.0 reference manual, \nTechnical Report CMU-CS-03-102, January 2003. \n[5]. Cello 1999 traces, Storage Systems Program HP Laboratories, URL: \nhttp:\/\/tesla.hpl.hp.com\/public_software\/. \n[6]. Y. Deng, F. Wang, N. Helian, D. Feng, K. Zhou, Optimal clustering size of small file access in \nnetwork attached storage device, Parallel Processing Letters, 16(4) (2006) 501-512. \n[7]. Y. Deng, F. Wang, N. Helian, EED: energy efficient disk drive architecture, Information Sciences, \n178(22) (2008) 4403-4417. \n[8]. G. R. Ganger, B. L. Worthington, R. Y. Hou, Y. N. Patt, Disk subsystem load balancing: disk striping \nvs. conventional data placement, in: Proceedings of the Hawaii International Conference on System \nSciences, January 1993, pp. 40-49. \n  \n \nACCEPTED MANUSCRIPT \n \n 21 \n[9]. G. Ganger, Blurring the line between oses and storage devices, Technical report, Carnegie Mellon \nUniversity, December 2001. \n[10]. G. R. Ganger, M. F. Kaashoek, Embedded inodes and explicit grouping: exploiting disk bandwidth for \nsmall files, in: Proceedings of Annual USENIX Technical Conference (Anaheim, CA), January 1997, \npp. 1-17. \n[11]. Hitachi Global Storage Technologies \u2013 HDD Technology Overview Charts, \nhttp:\/\/www.hitachigst.com\/hdd\/technolo\/overview\/storagetechchart.html. \n[12]. W. W. Hsu and A. J. Smith, The performance impact of I\/O optimizations and disk improvements, \nIBM Journal of Research and Development 48(2) (2004) 255\u2013289. \n[13]. C. Hsu, C. Chen and Y. Su, Hierarchical clustering of mixed data based on distance hierarchy, \nInformation Sciences 177(20) (2007) 4474-4492. \n[14]. W. W. Hsu, A. J. Smith, H. C. Young, The automatic improvement of locality in storage systems, \nACM Transactions on Computer Systems 23(4) (2005) 424\u2013473. \n[15]. W. W. Hsu and A. J. Smith, Characteristics of I\/O traffic in personal computer and server workloads, \nIBM Systems Journal 42(2) (2003) 347\u2013372. \n[16]. H. Huang, W. Hung, and K. G. Shin, FS2: dynamic data replication in free disk space for improving \ndisk performance and energy consumption, in: Proceedings of the 20th ACM symposium on \nOperating systems principles, 2005, pp.263-276. \n[17]. R. Karedla, J. S. Love, B. G. Wherry, Caching strategies to improve disk system performance, \nComputer 27(3) (1994) 38 - 46.   \n[18]. Y. Kim, S. Gurumurthi, A. Sivasubramaniam, Understanding the performance-temperature \ninteractions in disk I\/O of server workloads, in: Proceedings of the 12th International Symposium on \nHigh-Performance Computer Architecture, 2006, pp.176-186. \n[19]. S. H. Kim, H. Zhu, R. Zimmermann, Zoned-RAID, ACM transactions on storage 3(1) (2007) 1-17. \n[20]. A. J. T. Lee and C. Wang, An efficient algorithm for mining frequent inter-transaction patterns, \nInformation Sciences 177(17) (2007) 3453-3476. \n[21]. Z. Li, Z. Chen, Y. Zhou, Mining block correlations to improve storage performance, ACM \nTransactions on Storage 1(2) (2005) 213-245. \n[22]. J. Li, J. Pan and S. Chu, Kernel class-wise locality preserving projection, Information Sciences 178(7) \n(2008) 1825-1835. \n[23]. Log-structured File System, http:\/\/log-file-system.area51.ipupdater.com\/. \n[24]. C. R. Lumb, J. Schindler, G. R. Ganger, D. F. Nagle, Towards higher disk head utilization: extracting \nfree bandwidth from busy disk drives, in: Proceedings of the Fourth Symposium on Operating \nSystems Design and Implementation(OSDI),2000, pp. 87\u2013102 \n[25]. C. Lu, G. A. Alvarez, J. Wilkes, Aqueduct: online data migration with performance guarantees, in: \nProceedings of the 1st USENIX Conference on File and Storage Technologies, 2002, pp.219-230. \n[26]. C. R. Lumb, J. Schindler, and G. R. Ganger, Freeblock scheduling outside of disk firmware, in: \nProceedings of the 1st USENIX Conference on File and Storage Technologies, 2002, pp.275-288. \n[27]. N. R. Mahapatra and B. Venkatrao, The processor-memory bottleneck: problems and solutions, ACM \nCrossroads 5(3) (1999). \n[28]. M. McKusick, W. Joy, and S. Leffler, A fast file system for UNIX, ACM Trans. on Computer Systems \n2(3) (1984) 181\u2013197. \n[29]. M. Mesnier, G. R. Ganger, E. Riedel, Object-based storage, IEEE Communications Magazine 41(8) \n(2003) 84 \u2013 90.  \n  \n \nACCEPTED MANUSCRIPT \n \n 22 \n[30]. R. V. Meter, Observing the effects of multi-zone disks, in: Proceedings of the USENIX Annual \nTechnical Conference, January 1997, pp.19-30. \n[31]. S. W. Ng, Advances in disk technology: performance issues, Computer 31(5) (1998) 75-81. \n[32]. E. Pugh, Storage hierarchies: Gaps, cliffs, and trends, IEEE Transactions on Magnetics 7(4) (1971) \n810-814. \n[33]. Reiserfs, http:\/\/www.namesys.com\/. \n[34]. A. Riska and E. Riedel, Disk drive level workload characterization, in: Proceedings of the USENIX \nAnnual Technical Conference , Boston, 2006, pp. 97-103. \n[35]. D. Roselli, J. R. Lorch and T. E. Anderson, A comparison of file system workloads, in: Proceedings of \nthe USENIX Annual Technical Conference (Berkeley, CA), 2000, pp.41\u201354. \n[36]. C. Ruemmler and J. Wilkes, Disk shuffling, Technical report HPL-91-156, Hewlett-Packard Company, \nPalo Alto, CA, October 1991. \n[37]. C. Ruemmler, and J. Wilkes, Unix disk access patterns, in: Proceedings of the Winter 1993 USENIX \nTechnical Conference , 1993, pp. 313-323.  \n[38]. S. W. Schlosser, J. L. Griffin, D. F. Nagle, G. R. Ganger, Designing computer systems with \nMEMS-based storage, in: Proceedings of the 9th International Conference on Architectural Support \nfor Programming Languages and Operating Systems (ASPLOS), 2000, pp.1\u201312. \n[39]. J. Schindler, J. L. Griffin, C. R. Lumb, and G. R. Ganger, Track aligned extents: matching access \npatterns to disk drive characteristics, in Proceedings of Conf. on File and Storage Technologies \n(FAST02), 2002, pp.259-274. \n[40]. Seek distance dependent variable max VCM seek current to control thermal rise in VCM\u2019s. \nhttp:\/\/www.patentstorm.us\/patents\/6724564-description.html. \n[41]. C. Staelin and H. Garcia-Molina, Clustering active disk data to improve disk performance, Tech. Rep. \nCS-TR-283-90, Dept. of Computer Science, Princeton University,1990. \n[42]. A. S. Tanenbaum, J. N. Herder, H. Bos, File size distribution on UNIX systems: then and now, ACM \nSIGOPS Operating Systems Review, 40(1) (2006)100-104.   \n[43]. T. M. Wong and J. Wilkes, My Cache or yours? making storage more exclusive, in: Proceedings of \nUSENIX Annual Technical Conference (USENIX 2002), 2002, pp. 161\u2013175. \n \n"}