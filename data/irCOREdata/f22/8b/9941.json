{"doi":"10.1145\/1390156.1390266","coreId":"9941","oai":"oai:www.era.lib.ed.ac.uk:1842\/4588","identifiers":["oai:www.era.lib.ed.ac.uk:1842\/4588","10.1145\/1390156.1390266"],"title":"On the quantitative analysis of Deep Belief Networks","authors":["Salakhutdinov, Ruslan","Murray, Iain"],"enrichments":{"references":[{"id":1034352,"title":"A fast learning algorithm for deep belief nets.","authors":[],"date":"2006","doi":"10.1162\/neco.2006.18.7.1527","raw":"Hinton, G. E., Osindero, S., & Teh, Y. W. (2006). A fast learning algorithm for deep belief nets. Neural Computation, 18, 1527\u2013 1554.","cites":null},{"id":1034429,"title":"A new class of upper bounds on the log partition function.","authors":[],"date":"2005","doi":"10.1109\/TIT.2005.850091","raw":"Wainwright, M. J., Jaakkola, T., & Willsky, A. S. (2005). A new class of upper bounds on the log partition function. IEEE Transactions on Information Theory, 51, 2313\u20132335.","cites":null},{"id":1034372,"title":"Annealed importance sampling.","authors":[],"date":"2001","doi":null,"raw":"Neal, R. M. (2001). Annealed importance sampling. Statistics and Computing, 11, 125\u2013139.","cites":null},{"id":1034435,"title":"Constructing free-energy approximations and generalized belief propagation algorithms.","authors":[],"date":"2005","doi":"10.1109\/TIT.2005.850085","raw":"Yedidia, J. S., Freeman, W. T., & Weiss, Y. (2005). Constructing free-energy approximations and generalized belief propagation algorithms. IEEE Transactions on Information Theory, 51, 2282\u20132312.","cites":null},{"id":1034381,"title":"Estimating ratios of normalizing constants using linked importance sampling","authors":[],"date":"2005","doi":null,"raw":"Neal, R. M. (2005). Estimating ratios of normalizing constants using linked importance sampling (Technical Report 0511). Department of Statistics, University of Toronto.","cites":null},{"id":1034421,"title":"Modeling human motion using binary latent variables.","authors":[],"date":"2006","doi":null,"raw":"Taylor, G. W., Hinton, G. E., & Roweis, S. T. (2006). Modeling human motion using binary latent variables. Advances in Neural Information Processing Systems. MIT Press.","cites":null},{"id":1034407,"title":"Modeling image patches with a directed hierarchy of Markov random \ufb01elds.","authors":[],"date":"2008","doi":null,"raw":"Osindero, S., & Hinton, G. (2008). Modeling image patches with a directed hierarchy of Markov random \ufb01elds. NIPS 20. Cambridge, MA: MIT Press.","cites":null},{"id":1034413,"title":"Nested sampling. Bayesian inference and maximum entropy methods","authors":[],"date":"2004","doi":"10.1007\/978-94-011-5028-6_1","raw":"Skilling, J. (2004). Nested sampling. Bayesian inference and maximum entropy methods in science and engineering, AIP Conference Proceeedings, 735, 395\u2013405.","cites":null},{"id":1034336,"title":"On contrastive divergence learning.","authors":[],"date":"2005","doi":null,"raw":"Carreira-Perpinan, M., & Hinton, G. (2005). On contrastive divergence learning. 10th Int. Workshop on Arti\ufb01cial Intelligence and Statistics (AISTATS\u20192005).","cites":null},{"id":1034366,"title":"Probabilistic inference using Markov chain Monte Carlo methods","authors":[],"date":"1993","doi":"10.1016\/S0025-5564(02)00109-8","raw":"Neal, R. M. (1993). Probabilistic inference using Markov chain Monte Carlo methods (Technical Report CRG-TR-93-1). Department of Computer Science, University of Toronto.","cites":null},{"id":1034346,"title":"Reducing the dimensionality of data with neural networks.","authors":[],"date":"2006","doi":"10.1126\/science.1127647","raw":"Hinton, & Salakhutdinov (2006). Reducing the dimensionality of data with neural networks. Science, 313, 504 \u2013 507.","cites":null},{"id":1034359,"title":"Representational power of restricted Boltzmann machines and deep belief networks.","authors":[],"date":"2008","doi":"10.1162\/neco.2008.04-07-510","raw":"LeRoux, N., & Bengio, Y. (2008). Representational power of restricted Boltzmann machines and deep belief networks. To appear in Neural Computation.","cites":null},{"id":1034408,"title":"Restricted Boltzmann machines for collaborative \ufb01ltering.","authors":[],"date":"2007","doi":"10.1145\/1273496.1273596","raw":"Salakhutdinov, R., Mnih, A., & Hinton, G. (2007). Restricted Boltzmann machines for collaborative \ufb01ltering. Proceedings of the Twenty-fourth International Conference (ICML 2004).","cites":null},{"id":1034333,"title":"Scaling learning algorithms towards AI. Large-Scale Kernel Machines.","authors":[],"date":"2007","doi":null,"raw":"Bengio, Y., & LeCun, Y. (2007). Scaling learning algorithms towards AI. Large-Scale Kernel Machines. MIT Press.","cites":null},{"id":1034341,"title":"The Rate Adapting Poisson (RAP) model for information retrieval and object recognition.","authors":[],"date":"2006","doi":"10.1145\/1143844.1143887","raw":"Gehler, P., Holub, A., & Welling, M. (2006). The Rate Adapting Poisson (RAP) model for information retrieval and object recognition. Proceedings of the 23rd International Conference on Machine Learning.","cites":null},{"id":1034350,"title":"Training products of experts by minimizing contrastive divergence.","authors":[],"date":"2002","doi":"10.1162\/089976602760128018","raw":"Hinton, G. E. (2002). Training products of experts by minimizing contrastive divergence. Neural Computation, 14, 1711\u20131800.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2008-07-09","abstract":"Deep Belief Networks (DBN\u2019s) are generative models that contain many layers of hidden variables. Efficient greedy algorithms for learning and approximate inference have allowed these models to be applied successfully in many application domains. The main building block of a DBN is a bipartite undirected graphical model called a restricted Boltzmann machine (RBM). Due to the presence of the partition function, model selection, complexity control, and exact maximum likelihood learning in RBM's are intractable. We show that Annealed Importance Sampling (AIS) can be used to efficiently estimate the partition function of an RBM, and we present a novel AIS scheme for comparing RBM's with different architectures. We further show how an AIS estimator, along with approximate inference, can be used to estimate a lower bound on the log-probability that a DBN model with multiple hidden layers assigns to the test data. This is, to our knowledge, the first step towards obtaining quantitative results that would allow us to directly assess the performance of Deep Belief Networks as generative models of data","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/9941.pdf","fullTextIdentifier":"http:\/\/www.cs.toronto.edu\/~rsalakhu\/papers\/dbn_ais.pdf","pdfHashValue":"f8a710dbd2af2523a55fbafd7505e9ca1ba72a5f","publisher":null,"rawRecordXml":"<record><header><identifier>\n        \n            \n                oai:www.era.lib.ed.ac.uk:1842\/4588<\/identifier><datestamp>\n                2011-05-25T09:59:43Z<\/datestamp><setSpec>\n                com_1842_102<\/setSpec><setSpec>\n                col_1842_3391<\/setSpec>\n            <\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:doc=\"http:\/\/www.lyncode.com\/xoai\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n            \nOn the quantitative analysis of Deep Belief Networks<\/dc:title><dc:creator>\nSalakhutdinov, Ruslan<\/dc:creator><dc:creator>\nMurray, Iain<\/dc:creator><dc:description>\nDeep Belief Networks (DBN\u2019s) are generative models that contain many layers of hidden variables. Efficient greedy algorithms for learning and approximate inference have allowed these models to be applied successfully in many application domains. The main building block of a DBN is a bipartite undirected graphical model called a restricted Boltzmann machine (RBM). Due to the presence of the partition function, model selection, complexity control, and exact maximum likelihood learning in RBM's are intractable. We show that Annealed Importance Sampling (AIS) can be used to efficiently estimate the partition function of an RBM, and we present a novel AIS scheme for comparing RBM's with different architectures. We further show how an AIS estimator, along with approximate inference, can be used to estimate a lower bound on the log-probability that a DBN model with multiple hidden layers assigns to the test data. This is, to our knowledge, the first step towards obtaining quantitative results that would allow us to directly assess the performance of Deep Belief Networks as generative models of data.<\/dc:description><dc:date>\n2011-01-13T17:24:49Z<\/dc:date><dc:date>\n2011-01-13T17:24:49Z<\/dc:date><dc:date>\n2008<\/dc:date><dc:date>\n2011-01-13T17:24:49Z<\/dc:date><dc:date>\n2008-07-05<\/dc:date><dc:date>\n2008-07-09<\/dc:date><dc:type>\nConference Paper<\/dc:type><dc:identifier>\n978-1-60558-205-4<\/dc:identifier><dc:identifier>\nhttp:\/\/www.cs.toronto.edu\/~rsalakhu\/papers\/dbn_ais.pdf<\/dc:identifier><dc:identifier>\nhttp:\/\/hdl.handle.net\/1842\/4588<\/dc:identifier><dc:identifier>\n10.1145\/1390156.1390266<\/dc:identifier><dc:language>\nen<\/dc:language>\n<\/oai_dc:dc>\n<\/metadata>\n        <\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2008,"topics":[],"subject":["Conference Paper"],"fullText":"On the Quantitative Analysis of Deep Belief Networks\nRuslan Salakhutdinov RSALAKHU@CS.TORONTO.EDU\nIain Murray MURRAY@CS.TORONTO.EDU\nDepartment of Computer Science, University of Toronto, Toronto, Ontario M5S 3G4, Canada\nAbstract\nDeep Belief Networks (DBN\u2019s) are generative\nmodels that contain many layers of hidden vari-\nables. Ef\ufb01cient greedy algorithms for learning\nand approximate inference have allowed these\nmodels to be applied successfully in many ap-\nplication domains. The main building block of\na DBN is a bipartite undirected graphical model\ncalled a restricted Boltzmann machine (RBM).\nDue to the presence of the partition function,\nmodel selection, complexity control, and exact\nmaximum likelihood learning in RBM\u2019s are in-\ntractable. We show that Annealed Importance\nSampling (AIS) can be used to ef\ufb01ciently es-\ntimate the partition function of an RBM, and\nwe present a novel AIS scheme for comparing\nRBM\u2019s with different architectures. We further\nshow how an AIS estimator, along with approx-\nimate inference, can be used to estimate a lower\nbound on the log-probability that a DBN model\nwith multiple hidden layers assigns to the test\ndata. This is, to our knowledge, the \ufb01rst step\ntowards obtaining quantitative results that would\nallow us to directly assess the performance of\nDeep Belief Networks as generative models of\ndata.\n1. Introduction\nDeep Belief Networks (DBN\u2019s), recently introduced by\nHintonet al. (2006)areprobabilisticgenerativemodelsthat\ncontain many layers of hidden variables, in which each\nlayer captures strong high-order correlations between the\nactivities of hidden features in the layer below. The main\nbreakthrough introduced by Hinton et al. was a greedy,\nlayer-by-layer unsupervised learning algorithm that allows\nef\ufb01cient training of these deep, hierarchical models. The\nlearning procedure also provides an ef\ufb01cient way of per-\nforming approximate inference, which makes the values of\nAppearing in Proceedings of the 25\nth International Conference\non Machine Learning, Helsinki, Finland, 2008. Copyright 2008\nby the author(s)\/owner(s).\nthe latent variables in the deepest layer easy to infer. These\ndeep generative models have been successfully applied in\nmany applicationdomains (Hinton& Salakhutdinov,2006;\nBengio & LeCun, 2007).\nThe main building block of a DBN is a bipartite undirected\ngraphical model called the Restricted Boltzmann Machine\n(RBM). RBM\u2019s, and their generalizations to exponential\nfamily models, have been successfully applied in collab-\norative \ufb01ltering (Salakhutdinov et al., 2007), information\nand image retrieval (Gehler et al., 2006), and time series\nmodeling (Taylor et al., 2006). A key feature of RBM\u2019s\nis that inference in these models is easy. An unfortunate\nlimitation is that the probability of data under the model is\nknown only up to a computationally intractable normaliz-\ning constant, known as the partition function. A good es-\ntimate of the partition function would be extremely helpful\nfor model selection and for controlling model complexity,\nwhich are important for making RBM\u2019s generalize well.\nThere has been extensive research on obtaining determin-\nistic approximations (Yedidia et al., 2005) or determin-\nistic upper bounds (Wainwright et al., 2005) on the log-\npartition function of arbitrary discrete Markov random\n\ufb01elds (MRF\u2019s). These variational methods rely critically\non an ability to approximate the entropy of the undirected\ngraphical model. However, for densely connected MRF\u2019s,\nsuch as RBM\u2019s, these methods are unlikely to perform\nwell. There have also been many developments in the\nuse of Monte Carlo methods for estimating the partition\nfunction, including Annealed Importance Sampling (AIS)\n(Neal, 2001), Nested Sampling (Skilling, 2004), and many\nothers (see e.g. Neal (1993)). In this paper we show how\none such method, AIS, by taking advantage of the bipartite\nstructure of an RBM, can be used to ef\ufb01ciently estimate\nits partition function. We further show that this estimator,\nalongwithapproximateinference,canbeusedtoestimatea\nlower bound on the log-probability that a DBN model with\nmultiple hidden layers assigns to training or test data. This\nresult allows us toassess theperformanceofDBN\u2019s as gen-\nerative models and to compare them to other probabilistic\nmodels, such as plain mixture models.On the Quantitative Analysis of Deep Belief Networks\n2. Restricted Boltzmann Machines\nA Restricted Boltzmann Machine is a particular type of\nMRF that has a two-layer architecture in which the visi-\nble, binary stochastic units v \u2208 {0,1}D are connected to\nhidden binary stochastic units h \u2208 {0,1}M. The energy of\nthe state {v,h} is:\nE(v,h;\u03b8) = \u2212\nD X\ni=1\nM X\nj=1\nWijvihj\u2212\nD X\ni=1\nbivi\u2212\nM X\nj=1\najhj, (1)\nwhere\u03b8 = {W,b,a} are the modelparameters: Wij repre-\nsents the symmetric interaction term between visible unit i\nandhiddenunit j; bi and aj are bias terms. The probability\nthat the model assigns to a visible vector v is:\np(v;\u03b8) =\np\u2217(v;\u03b8)\nZ(\u03b8)\n=\n1\nZ(\u03b8)\nX\nh\nexp(\u2212E(v,h;\u03b8)), (2)\nZ(\u03b8) =\nX\nv\nX\nh\nexp(\u2212E(v,h;\u03b8)), (3)\nwherep\u2217 denotesunnormalizedprobability,andZ(\u03b8) is the\npartition function or normalizingconstant. The conditional\ndistributions over hidden units h and visible vector v are\ngiven by logistic functions:\np(h|v) =\nY\nj\np(hj|v), p(v|h) =\nY\ni\np(vi|h) (4)\np(hj = 1|v) = \u03c3(\nX\ni\nWijvi + aj) (5)\np(vi = 1|h) = \u03c3(\nX\nj\nWijhj + bi), (6)\nwhere \u03c3(x) = 1\/(1+exp(\u2212x)). The derivative of the log-\nlikelihood with respect to the model parameter W can be\nobtained from Eq. 2:\n\u2202 lnp(v)\n\u2202Wij\n= EP0[vihj] \u2212 EPModel[vihj],\nwhere EP0[ ] denotes an expectation with respect to the\ndata distribution and EPModel[ ] is an expectation with re-\nspect to the distribution de\ufb01ned by the model. The ex-\npectation EPModel[ ] cannot be computed analytically. In\npractice learning is done by following an approximation\nto the gradient of a different objective function, called the\n\u201cContrastive Divergence\u201d (CD) (Hinton, 2002):\n\u2206Wij = \u01eb\n\uffff\nEP0[vihj] \u2212 EPT[vihj]\n\uffff\n. (7)\nThe expectationEPT[ ] represents a distribution of samples\nfrom running the Gibbs sampler (Eqs. 5, 6), initialized at\nthe data, for T full steps. Setting T = \u221e recovers maxi-\nmumlikelihoodlearning,althoughT is typicallyset to one.\nEven though CD learning may work well in practice, the\nproblem of model selection and complexity control still re-\nmains. Supposewe have two RBM\u2019s with parametervalues\n\u03b8A and \u03b8B. Suppose that each RBM has different num-\nber of hidden units and was trained using different learning\nrates and different numbers of CD steps. On the validation\nset, we are interested in calculating the ratio:\np(v;\u03b8A)\np(v;\u03b8B)\n=\np\u2217(v;\u03b8A)\np\u2217(v;\u03b8B)\nZ(\u03b8B)\nZ(\u03b8A)\n,\nwhich requires knowing the ratio of partition functions.\n3. Estimating Ratios of Partition Functions\nSuppose we have two distributions de\ufb01ned on some space\nV with probability density functions: pA(v) = p\u2217\nA(v)\/ZA\nand pB(v) = p\u2217\nB(v)\/ZB. One way to estimate the ra-\ntio of normalizing constants is to use a simple importance\nsampling (IS) method. Suppose that pA(v)  = 0 whenever\npB(v)  = 0:\nZB\nZA\n=\nR\np\u2217\nB(v)dv\nZA\n=\nZ\np\u2217\nB(v)\np\u2217\nA(v)\npA(v)dv = EpA\n\uffff\np\u2217\nB(v)\np\u2217\nA(v)\n\uffff\n.\nAssuming we can draw independent samples from pA, the\nunbiased estimate of the ratio of partition functions can be\nobtained by using a simple Monte Carlo approximation:\nZB\nZA\n\u2248\n1\nM\nM X\ni=1\np\u2217\nB(v(i))\np\u2217\nA(v(i))\n\u2261\n1\nM\nM X\ni=1\nw\n(i) = \u02c6 rIS, (8)\nwhere v(i) \u223c pA. If pA and pB are not close enough,\nthe estimator \u02c6 rIS will be very poor. In high-dimensional\nspaces, the variance of \u02c6 rIS will be very large (or possibly\nin\ufb01nite), unless pA is a near-perfect approximation to pB.\n3.1. Annealed Importance Sampling (AIS)\nSuppose that we can de\ufb01ne a sequence of intermediate\nprobability distributions: p0,...,pK, with p0 = pA and pK\n= pB, which satisfy the following conditions:\nC1 pk(v)  = 0 whenever pk+1(v)  = 0.\nC2 We must be able to easily evaluate the unnormalized\nprobability p\u2217\nk(v), \u2200v \u2208 V, k = 0,...,K.\nC3 For each k = 0,...,K\u22121, we must be able to draw\na sample v\u2032 given v using a Markov chain transition\noperator Tk(v\u2032;v) that leaves pk(v) invariant:\nZ\nTk(v\u2032;v)pk(v)dv = pk(v\u2032). (9)\nC4 We must be able to draw (preferably independent)\nsamples from pA.\nThe transition operatorsTk(v\u2032;v) represent the probability\ndensity of transitioning from state v to v\u2032. Constructing a\nsuitable sequence of intermediate probability distributionsOn the Quantitative Analysis of Deep Belief Networks\nwill dependon the problem. One general way to de\ufb01ne this\nsequence is to set:\npk(v) \u221d p\u2217\nA(v)1\u2212\u03b2kp\u2217\nB(v)\u03b2k, (10)\nwith 0 = \u03b20 < \u03b21 < ... < \u03b2K = 1 chosen by the user.\nOnce the sequence of intermediate distributions has been\nde\ufb01ned we have:\nAnnealed Importance Sampling (AIS) run:\n1. Generate v1,v2,...,vK as follows:\n\u2022 Sample v1 from pA = p0\n\u2022 Sample v2 given v1 using T1\n\u2022 ...\n\u2022 Sample vK given vK\u22121 using TK\u22121\n2. Set\nw\n(i) =\np\n\u2217\n1(v1)\np\u2217\n0(v1)\np\n\u2217\n2(v2)\np\u2217\n1(v2)\n...\np\n\u2217\nK\u22121(vK\u22121)\np\u2217\nK\u22122(vK\u22121)\np\n\u2217\nK(vK)\np\u2217\nK\u22121(vK)\nNote that there is no need to compute the normalizing con-\nstants of any intermediate distributions. After performing\nM runs of AIS, the importance weights w(i) can be substi-\ntutedintoEq.8to obtainanestimateoftheratioofpartition\nfunctions:\nZB\nZA\n\u2248\n1\nM\nM X\ni=1\nw(i) = \u02c6 rAIS. (11)\nNeal (2005) shows that for suf\ufb01ciently large number of in-\ntermediate distributions K, the variance of \u02c6 rAIS will be\nproportional to 1\/MK. Provided K is kept large, the total\namountof computationcan be split in any way between the\nnumber of intermediate distributions K and the number of\nannealing runs M without adversely affecting the accuracy\nof the estimator. If samples drawn from pA are indepen-\ndent, the number of AIS runs can be used to control the\nvariance in the estimate of \u02c6 rAIS:\nVar(\u02c6 rAIS) =\n1\nM\nVar(w(i)) \u2248\n\u02c6 s2\nM\n= \u02c6 \u03c32, (12)\nwhere \u02c6 s2 is estimated simply from the sample variance of\nthe importance weights.\n3.2. Ratios of Partition Functions of two RBM\u2019s\nSuppose we have two RBM\u2019s with parameter values \u03b8A =\n{W A,bA,aA} and \u03b8B = {W B,bB,aB} that de\ufb01ne prob-\nability distributions pA and pB over V \u2208 {0,1}D. Each\nRBM can have a different number of hidden units hA \u2208\n{0,1}MA and hB \u2208 {0,1}MB. The generic AIS interme-\ndiate distributions (Eq. 10) would be harderto sample from\nthanan RBM. Insteadwe introducethefollowingsequence\nof distributions for k = 0,...,K:\npk(v) =\np\u2217\nk(v)\nZk\n=\n1\nZk\nX\nh\nexp(\u2212Ek(v,h)), (13)\nwhere the energy function is given by:\nEk(v,h) = (1 \u2212 \u03b2k)E(v,hA;\u03b8A) + \u03b2kE(v,hB;\u03b8B), (14)\nwith 0 = \u03b20 < \u03b21 < ... < \u03b2K = 1. For i = 0, we have\n\u03b20 = 0 and so p0 = pA. Similarly, for i = K, we have\npK = pB. For the intermediate values of k, we will have\nsome interpolation between pA and pB.\nLet us now de\ufb01ne a Markov chain transition operator\nTk(v\u2032;v) that leaves pk(v) invariant. Using Eqs. 13, 14,\nit is straightforward to derive a block Gibbs sampler. The\nconditional distributions are given by logistic functions:\np(hA\nj = 1|v) = \u03c3\n\uffff\n(1 \u2212 \u03b2k)(\nX\ni\nW A\nijvi + aA\nj )\n\uffff\n(15)\np(hB\nj = 1|v) = \u03c3\n\uffff\n\u03b2k(\nX\ni\nW B\nij vi + aB\nj )\n\uffff\n(16)\np(v\u2032\ni = 1|h) = \u03c3\n\uffff\n(1 \u2212 \u03b2k)(\nX\nj\nW A\nijhA\nj + bA\ni )\n+ \u03b2k(\nX\nj\nW B\nij hB\nj + bB\ni )\n\uffff\n. (17)\nGiven v, Eqs. 15, 16 are used to stochastically activate hid-\nden units hA and hB. Eq. 17 is then used to draw a new\nsample v\u2032 as shownin Fig. 1 (left panel). Dueto thespecial\nstructure of RBM\u2019s, the cost of summing out h is linear in\nthe number of hidden units. We can therefore easily evalu-\nate:\np\u2217\nk(v) =\nX\nhA,hB\ne(1\u2212\u03b2k)E(v,h\nA;\u03b8A)+\u03b2kE(v,h\nB;\u03b8B)\n= e\n(1\u2212\u03b2k)\nP\ni b\nA\ni vi\nMA Y\nj=1\n(1 + e\n(1\u2212\u03b2k)(\nP\ni W\nA\nijvi+a\nA\nj ))\n\u00d7 e\n\u03b2k\nP\ni b\nB\ni vi\nMB Y\nj=1\n(1 + e\n\u03b2k(\nP\ni W\nB\nijvi+a\nB\nj )).\nWe will assume that the parameter values of each RBM\nsatisfy |\u03b8| < \u221e, in which case p(v) > 0 for all v \u2208 V.\nThis will ensure that condition C1 of the AIS procedure is\nalways satis\ufb01ed. We have already shown that conditions\nC2 and C3 are satis\ufb01ed. For condition C4, we can run\na blocked Gibbs sampler (Eqs. 5, 6) to generate samples\nfrom pA. These sample points will not be independent, but\nthe AIS estimator will still converge to the correct value,\nprovided our Markov chain is ergodic (Neal, 2001). How-\never, assessing the accuracy of this estimator can be dif\ufb01-\ncult, as it depends on both the variance of the importance\nweights and on autocorrelations in the Gibbs sampler.\n3.3. Estimating Partition Functions of RBM\u2019s\nThe partition function of an RBM can be found by \ufb01nding\nthe ratio to the normalizer for \u03b8A = {0,bA,aA}, an RBMOn the Quantitative Analysis of Deep Belief Networks\nW\nW\nv v\u2019\nh h\nModel B Model A\n\u03b2 (1\u2212   )W \u03b2\n\u03b2 (1\u2212   )W \u03b2\nB\nB\nA B\nk\nk k\nA\nA k P(v|h ,W ) Q(h |v,W )\nP(h ,h |W )\nh\nW\nv\nh\nv\nh W\nh\nh\n1 1 1 1\n1 2 2\nRBM 1\n1\n1\n2 2\n1\n2\nRBM\nFigure 1. Left: The Gibbs transition operator Tk(v\n\u2032;v) leaves pk(v) invariant when estimating the ratio of partition functions ZB\/ZA.\nMiddle: Recursive greedy learning consists of learning a stack of RBMs. Right: Two-layer DBN as a generative model.\nwith a zero weight matrix. From Eq. 3, we know:\nZA = 2MA\nY\ni\n(1 + ebi). (18)\nMoreover,\npA(v) =\nY\ni\npA(vi) =\nY\ni\n1\/(1 + e\u2212bi),\nso we can drawexact independentsamples fromthis \u201cbase-\nrate\u201d RBM. AIS in this case allows us to obtain an unbi-\nased estimate of the partition function ZB. This approach\nclosely resembles simulated annealing, since the interme-\ndiate distributions of Eq. 13 take form:\npk(v) =\nexp((1\u2212\u03b2k)vTbA)\nZk\nX\nhB\nexp(\u2212\u03b2kE(v,hB;\u03b8B)).\nWe gradually change \u03b2k (or inverse temperature) from 0\nto 1, annealing from a simple \u201cbase-rate\u201d model to the \ufb01nal\ncomplex model. The importance weights w(i) ensure that\nAIS produces correct estimates.\n4. Deep Belief Networks (DBN\u2019s)\nIn this section we brie\ufb02y review a greedy learning algo-\nrithm for training Deep Belief Networks. We then show\nhow to obtain an estimate of the lower bound on the log-\nprobability that the DBN assigns to the data.\n4.1. Greedy Learning of DBN\u2019s\nConsider learning a DBN with two layers of hidden fea-\ntures as shown in Fig. 1 (right panel). The greedy strategy\ndeveloped by Hinton et al. (2006) uses a stack of RBM\u2019s\n(Fig. 1, middle panel). We \ufb01rst train the bottom RBM with\nparameters W 1, as described in section 2.\nA key observation is that the RBM\u2019s joint distribution\np(v,h1|W 1) is identical to that of a DBN with second-\nlayerweightstiedtoW 2=W 1\u22a4. We nowconsideruntying\nand re\ufb01ning W 2, improving the \ufb01t to the training data.\nFor any approximating distribution Q(h1|v), the DBN\u2019s\nlog-likelihood has the following variational lower bound:\nlnp(v|W 1,W2) \u2265\nX\nh1\nQ(h1|v)\n\uffff\nlnp(h1|W 2) +\nlnp(v|h1,W1)\n\uffff\n+ H(Q(h1|v)), (19)\nwhere H( ) is the entropy functional. We set Q(h1|v) =\np(h1|v,W1) de\ufb01ned by the RBM (Eq. 5). Initially, when\nW 2 = W 1\u22a4, Q is the DBN\u2019s true factorial posterior over\nh1, and the bound is tight. Therefore, any increase in the\nbound will lead to an increase in the true likelihood of the\nmodel. Maximizing the bound of Eq. 19 with frozen W 1 is\nequivalent to maximizing:\nX\nh1\nQ(h1|v)lnp(h1|W 2). (20)\nThis is equivalent to training the second layer RBM with\nvectors drawn from Q(h1|v) as data.\nThis scheme can be extended by training a third RBM on\nh2 vectors drawn from the second RBM. If we initialize\nW 3=W 2\u22a4, we are guaranteedto improvethelowerbound\non the log-likelihood, though the log-likelihood itself can\nfall (Hinton et al., 2006). Repeating this greedy, layer-by-\nlayer training several times results in a deep, hierarchical\nmodel.\nRecursive Greedy Learning Procedure for the DBN.\n1. Fit parameters W\n1 of a 1-layer RBM to data.\n2. Freeze the parameter vector W\n1 and use samples from\np(h\n1|v,W\n1) as the data for training the next layer of\nbinary features with an RBM.\n3. Proceed recursively for as many layers as desired.\nIn practice, when adding a new layer l, we typically do not\ninitialize W l = W l\u22121\u22a4\n, so the number of hidden units of\nthe new RBM does not need to be the same as the number\nof the visible units of the lower-level RBM.\n4.2. Estimating Lower Bounds for DBN\u2019s\nConsider the same DBN model with two layers of hidden\nfeatures shown in Fig. 1. The model\u2019s joint distribution is:\np(v,h\n1,h\n2) = p(v|h\n1)p(h\n2,h\n1), (21)\nwhere p(v|h1) is de\ufb01ned by Eq. 6), and p(h1,h2) is the\njoint distribution de\ufb01ned by the second layer RBM. Note\nthat p(v|h1) is normalized.On the Quantitative Analysis of Deep Belief Networks\nBy explicitly summing out h2, we can easily evaluate an\nunnormalizedprobabilityp\u2217(v,h1)=Zp(v,h1). Using the\napproximating factorial distribution Q, which we get as a\nbyproduct of the greedy learning procedure, and the varia-\ntional lower bound of Eq. 19, we obtain:\nln\nX\nh1\np(v,h1) \u2265\nX\nh1\nQ(h1|v) lnp\u2217(v,h1)\n\u2212 lnZ + H(Q(h\n1|v)) = B(v). (22)\nThe entropy term H( ) can be computed analytically, since\nQ is factorial. The partitionfunctionZ is estimated by run-\nning AIS on the top-level RBM. And the expectation term\ncan be estimated by a simple Monte Carlo approximation:\nX\nh1\nQ(h1|v)lnp\u2217(v,h1) \u2248\n1\nM\nM X\ni=1\nlnp\u2217(v,h1(i)), (23)\nwhere h1(i) \u223c Q(h1|v). The variance of this Monte Carlo\nestimator will be proportional to 1\/M provided the vari-\nance of lnp\u2217(v,h1(i)) is \ufb01nite. In general, we will be in-\nterested in calculating the lower bound averaged over the\ntest set containing Nt samples, so\n1\nNt\nNt X\nn=1\nB(vn) \u2248\n1\nNt\nNt X\nn=1\n\uffff\n1\nM\nM X\ni=1\nlnp\u2217(vn,h1(i)) +\nH(Q(h1|vn))\n\uffff\n\u2212 ln \u02c6 Z = \u02c6 rB \u2212 ln \u02c6 Z = \u02c6 rBound. (24)\nIn this case the variance of the estimator induced by the\nMonte Carlo approximation will asymptotically scale as\n1\/(NtM). We will show in the experimental results sec-\ntion that the value of M can be small provided Nt is large.\nThe error of the overall estimator \u02c6 rBound in Eq. 24 will be\nmostly dominated by the error in the estimate of lnZ. In\nour experiments, we obtained unbiased estimates of \u02c6 Z and\nits standard deviation \u02c6 \u03c3 using Eqs. 11, 12. We report ln \u02c6 Z\nand ln( \u02c6 Z \u00b1 \u02c6 \u03c3).\nEstimatingthis lowerboundforDeepBeliefNetworkswith\nmore layers is now straightforward. Consider a DBN with\nL hidden layers. The model\u2019s joint distribution and its ap-\nproximate posterior distribution Q are given by:\np\n(v,h\n1,...,h\nL) = p(v|h\n1)...p(h\nL\u22122|h\nL\u22121)p(h\nL\u22121,h\nL)\nQ(h1,...,hL|v) = Q(h1|v)Q(h2|h1)...Q(hL|hL\u22121).\nThe bound can now be obtained by using Eq. 22. Note\nthat most of the computation resources will be spent on\nestimating the partition function Z of the top level RBM.\n5. Experimental Results\nInourexperimentswe usedthe MNISTdigitdataset, which\ncontains 60,000 training and 10,000 test images of ten\nhandwritten digits (0 to 9), with 28\u00d728 pixels. The dataset\nwas binarized: each pixel value was stochastically set to 1\nin proportion to its pixel intensity. Samples from the train-\ning set are shown in Fig. 2 (top left panel). Annealed im-\nportance sampling requires the \u03b2k that de\ufb01ne a sequence\nof intermediate distributions. In all of our experiments this\nsequence was chosen by quickly runninga few preliminary\nexperiments and picking the spacing of \u03b2k so as to mini-\nmize the log variance of the \ufb01nal importance weights. The\nbiases bA of a base-rate model (see Eq. 18) were set by\nmaximum likelihood, then smoothed to ensure that p(v) >\n0, \u2200 v \u2208 V. Code that can be used to reproduceexperimen-\ntal results is available at www.cs.toronto.edu\/\u223crsalakhu.\n5.1. Estimating partition functions of RBM\u2019s\nIn our \ufb01rst experiment we trained three RBM\u2019s on the\nMNIST digits. The \ufb01rst two RBM\u2019s had 25 hidden units\nand were learned using CD (section 2) with T=1 and T=3\nrespectively. We call these models CD1(25) and CD3(25).\nThe third RBM had 20 hidden units and was learned using\nCD with T=1. For all threemodels we can calculate the ex-\nact value of the partition function simply by summing out\nthe 784 visible units for each con\ufb01guration of the hiddens.\nForallthreemodelswe used500\u03b2k spaceduniformlyfrom\n0 to 0.5, 4,000 \u03b2k spaced uniformly from 0.5 to 0.9, and\n10,000 \u03b2k spaced uniformly from 0.9 to 1.0, with a total of\n14,500 intermediate distributions.\nTable 1 shows that for all three models, using only 10 AIS\nruns, we were able to obtain good estimates of partition\nfunctions in just 20 seconds on a Pentium Xeon 3.00GHz\nmachine. For model CD1(25), however, the variance of\nthe estimator was high, even with 100 AIS runs. However,\n\ufb01gure 3 (top row) reveals that as the number of annealing\nruns is increased, AIS can almost exactly recover the true\nvalue of the partition function across all three models.\nWe also estimated the ratio of normalizing constants of\ntwo RBM\u2019s that have different numbers of hidden units:\nCD1(20) and CD1(25). This estimator could be used to\ndo complexity control. In detail, using 100 AIS runs with\nuniform spacing of 10,000 \u03b2k, we obtained ln \u02c6 rAIS =\nln(ZCD1(20)\/ZCD1(25)) = \u221224.49 with an error estimate\nln(\u02c6 rAIS \u00b1 3\u02c6 \u03c3) = (\u221224.19,\u221224.93). Each sample from\nCD1(25) was generated by starting a Markov chain at the\nprevious sample and running it for 10,000 steps. Com-\npared to the true value of \u221224.18, this result suggests that\nour estimates may have a small systematic error due to the\nMarkov chain failing to visit some modes.\nOur second experiment consisted of training two more re-\nalistic models: CD1(500) and CD3(500). We used exactly\nthesamespacingof\u03b2k asbeforeandexactlythesamebase-\nrate model. Results are shown in table 1 (bottom row). For\neach model we were able to get what appears to be a ratherOn the Quantitative Analysis of Deep Belief Networks\nTraining samples MoB (100) Base-rate \u03b2 = 0 \u03b2 = 0.5 \u03b2 = 0.95 \u03b2 = 1.0\n\uffff \uffff The course of AIS run for model CD25(500)\nCD1(500) CD3(500) CD25(500) DBN-CD1 DBN-CD3 DBN-CD25\nFigure 2. Top row: Firsttwopanels show random samples fromthetrainingset and amixtureof Bernoullismodel with100 components.\nThe last 4 panels display the course of 16 AIS runs for CD25(500) model by starting from a simple base-rate model and annealing to the\n\ufb01nal complex model. Bottom row: Random samples generated from three RBM\u2019s and corresponding three DBN\u2019s models.\nTable 1. Results of estimating partition functions of RBM\u2019s along with the estimates of the average training and test log-probabilities.\nFor all models we used 14,500 intermediate distributions.\nAIS True\nEstimates\nTime\nAvg. Test log-prob. Avg. Train log-prob.\nRuns lnZ ln \u02c6 Z ln( \u02c6 Z \u00b1 \u02c6 \u03c3) ln( \u02c6 Z \u00b1 3\u02c6 \u03c3) (mins) true estimate true estimate\n100 CD1(25) 255.41 256.52 255.00, 257.10 0.0000,257.73 3.3 \u2212151.57 \u2212152.68 \u2212152.35 \u2212153.46\nCD3(25) 307.47 307.63 307.44, 307.79 306.91,308.05 3.3 \u2212143.03 \u2212143.20 \u2212143.94 \u2212144.11\nCD1(20) 279.59 279.57 279.43, 279.68 279.12,279.87 3.1 \u2212164.52 \u2212164.50 \u2212164.89 \u2212164.87\n100 CD1(500) \u2014 350.15 350.04, 350.25 349.77,350.42 10.4 \u2014 \u2212125.53 \u2014 \u2212122.86\nCD3(500) \u2014 280.09 279.99, 280.17 279.76,280.33 10.4 \u2014 \u2212105.50 \u2014 \u2212102.81\nCD25(500) \u2014 451.28 451.19, 451.37 450.97,451.52 10.4 \u2014 \u221286.34 \u2014 \u221283.10\naccurateestimateofZ. Of course,we arerelyingon anem-\npirical estimate of AIS\u2019s accuracy, which could potentially\nbemisleading. Nonetheless,Fig.3(bottomrow)showsthat\nas we increase the number of annealing runs, the value of\nthe estimator does not oscillate drastically.\nWhile performing these tests, we observed that contrastive\ndivergencelearningwith T=3 results in considerablybetter\ngenerative model than CD learning with T=1: the differ-\nence of 20 nats is striking! Clearly, the widely used prac-\ntice of CD learning with T=1 is a rather poor \u201csubstitute\u201d\nfor maximum likelihood learning. Inspired by this result,\nwe trained a model by starting with T=1, and gradually\nincreasing T to 25 during the course of CD training, as\nsuggested by (Carreira-Perpinan & Hinton, 2005). We call\nthis model CD25(500). Training this model was computa-\ntionally much more demanding. However, the estimate of\nthe average test log-probability for this model was about\n\u221286, which is 39 and 19 nats better than the CD1(500) and\nCD3(500) models respectively. Fig. 2 (bottom row) shows\nsamples generated from all three models by randomly ini-\ntializing binary states of the visible units and runningalter-\nnating Gibbs for 100,000 steps. Certainly, samples gener-\nated by CD25(500) look much more like the real handwrit-\nten digits, than either CD1(500) or CD3(500).\nWe also obtained an estimate of the log ratio of two parti-\ntion functions \u02c6 rAIS = lnZCD25(500)\/ZCD3(500) = 169.96,\nusing 10,000 \u03b2k and 100 annealing runs. The estimates of\nthe individual log-partitionfunctions were ln \u02c6 ZCD25(500)=\n451.28 and ln \u02c6 ZCD3(500) = 280.09, in which case the log\nratio is 451.28\u2212280.09=171.19. This is in agreement (to\nwithin threestandarddeviations)with the direct estimate of\nthe ratio, \u02c6 rAIS=169.96.\nFor a simple comparisonwe also trained several mixture of\nBernoullis models (see Fig. 2, top left panel) with 10, 100,\nand 500 components. The corresponding average test log-\nprobabilities were \u2212168.95, \u2212142.63, and \u2212137.64. The\ndata generated from the mixture model looks better than\nCD3(500), althoughour quantitiveresults reveal this is due\nto over-\ufb01tting. The RBM\u2019s make much better predictions.\n5.2. Estimating lower bounds for DBN\u2019s\nWe greedily trained three DBN models with two hidden\nlayers. The \ufb01rst model, called DBN-CD1, was greedilyOn the Quantitative Analysis of Deep Belief Networks\n 10    100   500  1000  10000\n252\n253\n254\n255\n256\n257\n258\n259\nNumber of AIS runs\nl\no\ng\n \nZ\n \n \nLarge Variance\n20 sec\n3.3 min\n17 min\n33 min\n5.5 hrs\nEstimated logZ\nTrue logZ\n 10    100   500  1000  10000\n304\n305\n306\n307\n308\n309\n310\nNumber of AIS runs\nl\no\ng\n \nZ\n \n \nEstimated logZ\nTrue logZ\n 10    100   500  1000  10000\n276\n277\n278\n279\n280\n281\n282\nNumber of AIS runs\nl\no\ng\n \nZ\n \n \nEstimated logZ\nTrue logZ\nCD1(25) CD3(25) CD1(20)\n 10    100   500  1000  10000\n347\n348\n349\n350\n351\n352\n353\nNumber of AIS runs\nl\no\ng\n \nZ\nLarge variance\n1.1 min\n10.4 min\n52 min 1.8 hrs 17.4 hrs\n 10    100   500  1000  10000\n277\n278\n279\n280\n281\n282\n283\nNumber of AIS runs\nl\no\ng\n \nZ\n 10    100   500  1000  10000\n448\n449\n450\n451\n452\n453\nNumber of AIS runs\nl\no\ng\n \nZ\nCD1(500) CD3(500) CD25(500)\nFigure 3. Estimates of the log-partition functions ln \u02c6 Z as we increase the number of annealing runs. The error bars show ln( \u02c6 Z \u00b1 3\u02c6 \u03c3).\nlearned by freezing the parameter vector of the CD1(500)\nmodel and learning the 2nd layer RBM with 2000 hidden\nunits using CD with T=1. Similarly, the other two models,\nDBN-CD3 and DBN-CD25, added 2000 hidden units on\ntop of CD3(500) and CD25(500), using CD with T=3 and\nT=25 respectively. Training the DBN\u2019s took roughly three\ntimes longer than the RBM\u2019s.\nTable 2 shows the results. We used 15,000 intermediate\ndistributions and 500 annealing runs to estimate the parti-\ntion function of the 2nd layer RBM. This took 2.3 hours.\nFurther sampling was required for the simple Monte Carlo\napproximation of Eq. 23. We used M=5 samples from\nthe approximating distribution Q(h|v) for each data vec-\ntor v. Setting M=100 did not make much difference. Ta-\nble 2 also reports the empirical error in the estimate of the\nlower bound \u02c6 rBound. From Eq. 24, we have Var(\u02c6 rBound) =\nVar(\u02c6 rB) + Var(ln \u02c6 Z), both of which are shown in table 2.\nNote that models DBN-CD1 and DBN-CD3 signi\ufb01cantly\noutperform their single layer counterparts: CD1(500) and\nCD3(500). Addingasecondlayerforthosetwo modelsim-\nproves model performance by at least 25 and 7 nats. This\ncorresponds to a dramatic improvement in the quality of\nsamples generated from the models (Fig. 2, bottom row).\nObserve that greedy learning of DBN\u2019s does not appear to\nsuffer severely from over\ufb01tting. For single layer models,\nthe difference between the estimates of training and test\nlog-probabilities was about 3 nats. For DBN\u2019s, the corre-\nsponding difference in the estimates of the lower bounds\nwas about 4 nats, even though adding a second layer intro-\nduced over twice as many (or one million) new parameters.\nTable 2. Results of estimating lower bounds \u02c6 rBound (Eq. 24) on\nthe average training and test log-probabilities for DBN\u2019s. On av-\nerage, the total error of the estimator is about \u00b1 2 nats.\nAvg. AIS error\nbound Error \u02c6 rB ln( \u02c6 Z \u00b1 3\u02c6 \u03c3)\nModel log-prob \u00b13 std \u2212ln \u02c6 Z\nTest DBN-CD1 \u2212100.64 \u00b10.77 \u22121.43,+0.57\nDBN-CD3 \u221298.29 \u00b10.75 \u22120.91,+0.31\nDBN-CD25 \u221286.22 \u00b10.67 \u22120.84,+0.65\nTrain DBN-CD1 \u221297.67 \u00b10.30 \u22121.43,+0.57\nDBN-CD3 \u221294.86 \u00b10.29 \u22120.91,+0.31\nDBN-CD25 \u221282.47 \u00b10.25 \u22120.84,+0.65\nThe result of our experiments for DBN-CD25, however,\nwas very different. For this model, on the test data we ob-\ntained \u02c6 rBound = \u221286.22. This is comparable to the esti-\nmate of \u221286.34 for the average test log-probability of the\nCD25(500) model. Clearly, we cannot con\ufb01dently assert\nthat DBN-CD25 is a better generative model compared to\nthe carefullytrained single layer RBM. This peculiar result\nalso supports previous claims that if the \ufb01rst level RBM al-\nready models data well, adding extra layers will not help\n(LeRoux & Bengio, 2008; Hinton et al., 2006). As an ad-\nditional test, instead of randomly initializing parameters of\nthe 2nd layer RBM, we initialized it by using the same pa-\nrameters as the 1st layer RBM but with hidden and visible\nunits switched (see Fig. 1). This initialization ensures that\nthe distribution over the visible units v de\ufb01ned by the two-\nlayer DBN is exactly the same as the distribution over v\nde\ufb01ned by the 1st layer RBM. Therefore, after learning\nparameters of the 2nd layer RBM, the lower bound on the\ntraining data log-likelihood can only improve. After care-On the Quantitative Analysis of Deep Belief Networks\nfully training the second level RBM, our estimate of the\nlower bound on the test log-probability was only \u221285.97.\nOnce again, we cannot con\ufb01dentlyclaim that adding an ex-\ntra layer in this case yields better generalization.\n6. Discussions\nThe original paper of Hinton et al. (2006) showed that for\nDBN\u2019s, each additional layer increases a lower bound (see\nEq. 19) on the log-probability of the training data, pro-\nvided the number of hidden units per layer does not de-\ncrease. However, assessing generalization performance of\nthese generative models is quite dif\ufb01cult, since it requires\nenumeration over an exponential number of terms. In this\npaper we developed an annealed importance sampling pro-\ncedure that takes advantage of the bipartite structure of the\nRBM. This can provide a good estimate of the partition\nfunctionin a reasonableamountof computertime. Further-\nmore, we showed that this estimator, along with approx-\nimate inference, can be used to obtain an estimate of the\nlower bound on the log-probabilityof the test data, thus al-\nlowingus to obtainsomequantitativeevaluationofthegen-\neralization performance of these deep hierarchical models.\nThere are some disadvantages to using AIS. There is a\nneed to specify the \u03b2k that de\ufb01ne a sequence of interme-\ndiate distributions. The number and the spacing of \u03b2k will\nbe problem dependent and will affect the variance of the\nestimator. We also have to rely on the empirical estimate of\nAIS accuracy, which could potentially be very misleading\n(Neal, 2001; Neal, 2005). Even though AIS provides an\nunbiased estimator of Z, it occasionally gives large overes-\ntimates and usually gives small underestimates, so in prac-\ntice, it is more likely to underestimate of the true value of\nthe partition function, which will result in an overestimate\nof the log-probability. But these drawbacks should not re-\nsult in disfavoring the use of AIS for RBM\u2019s and DBN\u2019s:\nit is much better to have a slightly unreliable estimate than\nno estimate at all, or an extremely indirect estimate, such\nas discriminative performance (Hinton et al., 2006).\nWe \ufb01nd AIS and otherstochastic methodsattractive as they\ncan just as easily be applied to undirectedgraphicalmodels\nthat generalize RBM\u2019s and DBN\u2019s to exponential family\ndistributions. This will allow future application to mod-\nels of real-valued data, such as image patches (Osindero &\nHinton, 2008), or count data (Gehler et al., 2006).\nAnother alternative would be to employ deterministic ap-\nproximations (Yedidia et al., 2005) or deterministic upper\nbounds (Wainwright et al., 2005) on the log-partition func-\ntion. However, for densely connected MRF\u2019s, we would\nnotexpectthesemethodstoworkwell. Indeed,preliminary\nresults suggest that these methods provide quite inaccurate\nestimates of (or very loose upper bounds on) the partition\nfunction, even for small RBM\u2019s when trained on real data.\nAcknowledgments\nWe thank Geoffrey Hinton and Radford Neal for many\nhelpful suggestions. This research was supported by\nNSERC and CFI. Iain Murray is supported by the govern-\nment of Canada.\nReferences\nBengio, Y., & LeCun, Y. (2007). Scaling learning algorithms to-\nwards AI. Large-Scale Kernel Machines. MIT Press.\nCarreira-Perpinan, M., & Hinton, G. (2005). On contrastive di-\nvergence learning. 10th Int. Workshop on Arti\ufb01cial Intelligence\nand Statistics (AISTATS\u20192005).\nGehler, P., Holub, A., & Welling, M. (2006). The Rate Adapt-\ning Poisson (RAP) model for information retrieval and object\nrecognition. Proceedings of the 23rd International Conference\non Machine Learning.\nHinton, & Salakhutdinov (2006). Reducing the dimensionality of\ndata with neural networks. Science, 313, 504 \u2013 507.\nHinton, G. E. (2002). Training products of experts by minimizing\ncontrastive divergence. Neural Computation, 14, 1711\u20131800.\nHinton, G. E., Osindero, S., & Teh, Y. W. (2006). A fast learning\nalgorithm for deep belief nets. Neural Computation, 18, 1527\u2013\n1554.\nLeRoux, N., & Bengio, Y. (2008). Representational power of\nrestricted Boltzmann machines and deep belief networks. To\nappear in Neural Computation.\nNeal, R. M. (1993). Probabilistic inference using Markov chain\nMonte Carlo methods (Technical Report CRG-TR-93-1). De-\npartment of Computer Science, University of Toronto.\nNeal, R. M. (2001). Annealed importance sampling. Statistics\nand Computing, 11, 125\u2013139.\nNeal, R. M. (2005). Estimating ratios of normalizing constants\nusing linked importance sampling (Technical Report 0511).\nDepartment of Statistics, University of Toronto.\nOsindero, S., & Hinton, G. (2008). Modeling image patches with\na directed hierarchy of Markov random \ufb01elds. NIPS 20. Cam-\nbridge, MA: MIT Press.\nSalakhutdinov, R., Mnih, A., & Hinton, G. (2007). Restricted\nBoltzmann machines for collaborative \ufb01ltering. Proceedings\nof the Twenty-fourth International Conference (ICML 2004).\nSkilling, J. (2004). Nested sampling. Bayesian inference and\nmaximum entropy methods in science and engineering, AIP\nConference Proceeedings, 735, 395\u2013405.\nTaylor, G. W., Hinton, G. E., & Roweis, S. T. (2006). Model-\ning human motion using binary latent variables. Advances in\nNeural Information Processing Systems. MIT Press.\nWainwright, M. J., Jaakkola, T., & Willsky, A. S. (2005). A\nnew class of upper bounds on the log partition function. IEEE\nTransactions on Information Theory, 51, 2313\u20132335.\nYedidia, J. S., Freeman, W. T., & Weiss, Y. (2005). Construct-\ning free-energy approximations and generalized belief propa-\ngation algorithms. IEEE Transactions on Information Theory,\n51, 2282\u20132312."}