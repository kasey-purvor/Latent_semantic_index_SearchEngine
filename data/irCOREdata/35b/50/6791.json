{"doi":"10.1080\/0968776970050105","coreId":"6791","oai":"oai:generic.eprints.org:228\/core5","identifiers":["oai:generic.eprints.org:228\/core5","10.1080\/0968776970050105"],"title":"Using computer\u2010based tests for information science","authors":["Callear, David","King, Terry"],"enrichments":{"references":[{"id":192998,"title":"A strategy for using computer based examinations'","authors":[],"date":"1996","doi":null,"raw":"Callear, D. (1996), 'A strategy for using computer based examinations' in Proceedings of the 4th Annual Conference on the Teaching of Computing, Dublin, August 1996, Dublin: University of Dublin, 144-8.","cites":null},{"id":193000,"title":"Issues arising during the continuous computer-aided assessment of large groups'","authors":[],"date":"1996","doi":null,"raw":"30An-] Volume 5 Number \/ King, T. (1996), 'Issues arising during the continuous computer-aided assessment of large groups' in Proceedings of the 4th Annual Conference on the Teaching of Computing, Dublin, August 1996, Dublin: University of Dublin, 83-6.","cites":null},{"id":193001,"title":"Summative and formative computerised assessment: the Luton experience', Papers of the Workshop Presentation at the Northumbria Assessment Conference,","authors":[],"date":"1996","doi":null,"raw":"Zakrewski, S. (1996), 'Summative and formative computerised assessment: the Luton experience', Papers of the Workshop Presentation at the Northumbria Assessment Conference, University of Northumbria at Newcastle, September 1996, Newcastle: University of Northumbria.","cites":null},{"id":192999,"title":"Using computer-aided assessment for objective testing in higher education: a case study at the University of Portsmouth',","authors":[],"date":"1995","doi":null,"raw":"King, T. (1995), 'Using computer-aided assessment for objective testing in higher education: a case study at the University of Portsmouth', SEDA Paper 88, Innovations in Computing Teaching, 123-8, Portsmouth: University of Portsmouth.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"1997","abstract":"Computer\u2010based tests have been used extensively in the Department of Information Science at the University of Portsmouth, both for end\u2010of\u2010course examinations and continuous assessment. This paper details the use of computer\u2010based objective testing as an innovative technique for traditional assessment, and the separate problems of continuous computer\u2010aided assessment. Results from three years of research have led to plans for future developments within the department, and the paper provides a checklist of considerations regarded as crucial","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/6791.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/228\/1\/ALT_J_Vol5_No1_1997_Using_computer_based_tests_for.pdf","pdfHashValue":"befefaed7b823416906444a711edc73b8c56cebe","publisher":"Universit of Wales Press","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:228<\/identifier><datestamp>\n      2011-04-04T09:22:24Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/228\/<\/dc:relation><dc:title>\n        Using computer\u2010based tests for information science<\/dc:title><dc:creator>\n        Callear, David<\/dc:creator><dc:creator>\n        King, Terry<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        Computer\u2010based tests have been used extensively in the Department of Information Science at the University of Portsmouth, both for end\u2010of\u2010course examinations and continuous assessment. This paper details the use of computer\u2010based objective testing as an innovative technique for traditional assessment, and the separate problems of continuous computer\u2010aided assessment. Results from three years of research have led to plans for future developments within the department, and the paper provides a checklist of considerations regarded as crucial.<\/dc:description><dc:publisher>\n        Universit of Wales Press<\/dc:publisher><dc:date>\n        1997<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/228\/1\/ALT_J_Vol5_No1_1997_Using_computer_based_tests_for.pdf<\/dc:identifier><dc:identifier>\n          Callear, David and King, Terry  (1997) Using computer\u2010based tests for information science.  Association for Learning Technology Journal, 5 (1).  pp. 27-32.  ISSN 0968-7769     <\/dc:identifier><dc:relation>\n        10.1080\/0968776970050105<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/228\/","10.1080\/0968776970050105"],"year":1997,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"Using computer-based tests\nfor information science\nDavid Callear and Terry King\nDepartment of Information Science, University of Portsmouth\nComputer-based tests have been used extensively in the Department of Information Science at the\nUniversity of Portsmouth, both for end-of-course examinations and continuous assessment. This paper\ndetails the use of computer-based objective testing as an innovative technique for traditional\nassessment, and the separate problems of continuous computer-aided assessment. Results from three\nyears of research have led to plans for future developments within the department, and the paper\nprovides a checklist of considerations regarded as crucial.\nComputer-based testing for traditional assessment\nEnd-of-unit examinations\nThe introduction of objective testing using computer software does not necessarily\nrepresent innovative assessment. Where tests occur as an add-on to a course, are time-\nconstrained, closed-book, invigilated, and where there is little (or no) feedback of results\nto the students, such testing is best regarded as an innovative technique for traditional\nsummative assessment. A computer-based examination of this nature using the\ncommercial software Question Mark has been operating for a number of years in the\nDepartment of Information Science at Portsmouth, in the second-year unit for Logic\nProgramming, with student numbers up to 160.\nComputer-based examinations, like written examinations and coursework, can test some\nthings and not others. Their big disadvantage is that answers to the questions have to be\nsimple, either straightforward choices or unambiguous character strings. It is not\npossible, in the present state of computer technology, to test a student's skill in extended,\nfree-flowing answers. Higher abilities (such as analysis, synthesis, evaluation, and\nextrapolation of knowledge into new areas) are the hardest to test - some would say\nimpossible to test - on a computer. But the contents of Table 1 show that it is possible to\nassess everything one might want to assess using a judicial mix of coursework and\ncomputer-based examination, and dispensing with a written examination.\n27\nDavid Calleor and Terry King Using computer-based tests for information science\nTable I: Different ways of assessing student abilities\nAbilities\nRecall of facts\nUnderstanding\nProblem-solving\nHigher abilities\nAdditional skills\nCoursework\nNo\nYes\nYes\nYes\nYes (all)\nWritten exams\nYes\nYes\nYes\nYes\nYes (some)\nComputer exams\nYes\nYes\nYes\nNo\nNo\nIn the Logic Programming unit, the computer-based examination is complemented by\ncoursework designed to encourage originality as, along with writing a program, students\nhave to write a description and present it as a mini-project, to test literacy and\npresentational skills (Callear, 1996).\nThe greatest advantages of computer-based examinations is that marking is done\nautomatically and immediately. The extent of time-saving when using computer-based\nexaminations is illustrated in Table 2.\nTable 2: Time-savings using computer-based examinations\nNumber of students Paper-based tests Computer-based tests Total time saving\n(preparation and marking) (preparation) per exam\nhours hours (hours)\n20 - 6 6 0\n60 16 6 10\n160 41 6 35\nIf the preparation time of objective tests is set against the marking time of an average\nLevel 2 paper (say, 15 minutes per paper), it is clear from the table that the cut-off point is\n20 students. After this, the time-saving becomes considerable. Furthermore, questions can\nbe reused because a student does not have a paper to take away. Such considerations are\nvery important in a system that is under increasing pressure. The other major advantage\nis that the marking is objective, with no personal element resulting from the lecturer's\nknowledge of a particular student.\nThe validity of computer-based examinations is often challenged, but issues such as the\n'guessing factor' in multiple-choice tests have been completely refuted by the national\nexamining boards, and the construction of a multiple-choice question paper with a\nrealistic number of graded questions offers a valid alternative to written examinations\n(Callear, 1996).\nContinuous assessment and in-class tests\nThe use of computer-based tests for continuous assessment has been the subject of a\nthree-year study at Portsmouth, involving up to 90 Year 1 students on the Introductory\nInformation Systems Analysis and Design unit of the HND in Computing (King, 1995;\nKing, 1996). These students received three or four computer-based tests during one\nsemester. The tests were generally run back-to-back, with one test being removed as\nanother test was made available. Although the running of the computer examinations\n28\nALT-) Volume 5 Number I\nseemed to have generated few practical problems, the availability of resources, especially\nstaff time, was a critical issue for continuous testing. As well as the creation of a question\nbank, staff workload for continuous computer-aided assessment (CAA) was generally\nfound to be considerable. For open-access tests, there were up to twenty tasks, involving\ntest control and student preparation and feedback, which needed to be co-ordinated at\nany one time. For summative tests, additional tasks could arise involving equipment\navailability, room booking, invigilation, and contingency planning.\nInnovative assessment using CAA\nAttempts were made while running the continuous computer-based test sequences to\nintroduce some innovative aspects to the assessment process. The tests were open-book,\nnon-invigilated, not time-constrained, and with feedback to the students so that they\ncould identify their areas of weakness. In later tests, the total scores were displayed\nimmediately at the end of each test, supported by printed reports such as detailed\nexception reports on incorrect responses. The most formative aspects of the trial centred\non the provision of valuable revision tests for self-assessment. The original tests were\nreissued with immediate feedback on each question and comprehensive assistance from\none-line hints, contextual help pages, and tutorial pages which could be consulted before\nanswering a question.\nFuture developments for computer-based testing\nFuture developments are likely to follow two routes:\nThe increased use of computer-based examinations and summative tests for large groups\nZakrewski (1996) from the University of Luton reports that 4,000 students were given\ncomputer-based examinations across eight different subject areas in the academic year\n1995\/6. In the Department of Information Science at Portsmouth, all students on first- or\nsecond-year units could be examined using computer-based examinations in about 20\nhours. Savings in marking time in only two final-year units could exceed 80 hours for one\nlecturer. Results would also be available immediately to examination boards, a great\nadvantage in the brief period available between Semesters 1 and 2.\nThe development of exploratory formative assessment as distinct from summative assessment\nThis may be done as informal self-assessment, with the results being entered by students\ninto portfolios or log books, or test completion may be participatory only. Such tests\nwould be feedback-driven and designed to maximize their usefulness to students in their\nlearning. It would include printed feedback reports giving students full information on\ntheir own responses. The delivery of test questions would not be randomized, which\nwould allow the use of graded questions, and would not disrupt the students' assessment\nof their own weak areas. These tests would contain or be incorporated with more overt\nforms of computer-based learning, and would be facilitated by a change of software to\nQuestion Mark Designer for Windows which would allow other applications to be run\nsimultaneously with the test, such as ToolBook for interactive material, the Internet for\ntutorial notes, Java- or JavaScript-based simulations and animations, and email packages\nfor contact with tutors or other students. The use of computer-based testing in this way\n29\nDavid Callear and Terry King Using computer-based tests for information science\nwould create a unified testing environment for the students, with summative tests being\nsimply a more rigorous extension of the formative mode.\nFacilitation of future developments\nNone of the above future developments is likely to be successful without being\nunderpinned by three factors:\nAdequate resourcing\nManagement commitment to computer-based testing in the form of adequate resourcing\nof the innovation is crucial to its long-term development. Commitment can be expressed\nin the form of support for the purchase and evaluation of suitable testing software and\nthe maintenance of software site licences, increased funding for computer hardware,\nlaboratories and staff development. Careful consideration should also be given to support\nthe development of a new academic technician post of CAA Officer to take on the\nresponsibility of link person between computer services and academic staff. A list of issues\nand tasks for consideration by a CAA Officer can be found in the checklist in the\nAppendix. Such a post would be vital for removing from academics the wide range of\nnon-academic tasks associated with computer-based testing.\nImproved technical support\nTo increase successfully the volume of testing, technical support for computer-based\ntesting will need to be formalized and given a higher profile, and an error-free testing\nenvironment provided. Such a move will require management action, but work within\ntechnical support will be made considerably easier by liaison with just one key CAA\nOfficer.\nAssuring quality in the testing process\nReliability will be underpinned by sound operational procedures, and test validity can be\nensured by the involvement of the course team in writing, grading, moderating and testing\nquestions. Where there is a need for a very large question bank and a fast changeover to\ncomputer-based examinations, the only feasible approach may be to have the questions\nwritten by an outside agency but passed to the course team for moderation.\nConclusion\nAfter some years developing expertise in successful CAA within Information Science, the\ndepartment is now in a position to capitalize on this knowledge. However, future\ndevelopments may be jeopardized if insufficient attention is paid to the creation of the\nassessment infrastructure which is vital for sound and high-quality testing.\nReferences\nCallear, D. (1996), 'A strategy for using computer based examinations' in Proceedings of\nthe 4th Annual Conference on the Teaching of Computing, Dublin, August 1996, Dublin:\nUniversity of Dublin, 144-8.\nKing, T. (1995), 'Using computer-aided assessment for objective testing in higher\neducation: a case study at the University of Portsmouth', SEDA Paper 88, Innovations in\nComputing Teaching, 123-8, Portsmouth: University of Portsmouth.\n30\nAn-] Volume 5 Number \/\nKing, T. (1996), 'Issues arising during the continuous computer-aided assessment of large\ngroups' in Proceedings of the 4th Annual Conference on the Teaching of Computing,\nDublin, August 1996, Dublin: University of Dublin, 83-6.\nZakrewski, S. (1996), 'Summative and formative computerised assessment: the Luton\nexperience', Papers of the Workshop Presentation at the Northumbria Assessment\nConference, University of Northumbria at Newcastle, September 1996, Newcastle:\nUniversity of Northumbria.\nAppendix: Checklist of issues and tasks for consideration by a\nCAA Officer\nQuality of staff support\n\u2022 advice and consultancy on question-writing\n\u2022 staff training on CAA software\n\u2022 staff assistance in entering tests to software\n\u2022 staff assistance in preparing graphics (scanning, etc.)\n\u2022 advice and consultancy on construction of question banks\n\u2022 creation of administrative guidelines and operational standards for staff\n\u2022 creation of test prints (for contingencies and other uses)\n\u2022 liaison with the administrators (over room-booking and invigilation)\n\u2022 liaison with computer services (to load and remove tests, retrieve answer files)\n\u2022 liaison with computer services to ensure network testing issue of feedback reports\n\u2022 initiate programme of evaluation of CAA software\nQuality of student support\n\u2022 student preparation for tests\n\u2022 operating instructions\n\u2022 warning (email, etc.)\n\u2022 issue feedback reports\nReliability of testing\n\u2022 clearly agreed and written procedures for test completion\n\u2022 written operational guidelines for technical support\n\u2022 written administrative guidelines\n31\nDavid Callear and Terry King Using computer-based tests for information science\n\u2022 agreed standards for question preparation\n\u2022 agreed standards for test-bank preparation and maintenance\n\u2022 contingencies in the result of systems failure\nValidity of testing\n\u2022 written documentation to support unit team involvement\n\u2022 written documentation to cover question moderation\n\u2022 consultation with external examiners\nAdequacy of technical support\n\u2022 one person with specific responsibility for CAA\n\u2022 does this person have adequate backup?\n\u2022 has this person been trained in aspects of CAA?\n\u2022 is this person trained in using the software?\n\u2022 is this person trained in operational procedures for CAA?\n\u2022 test-file naming centralized and standardized\n\u2022 adequate security of answer files\n\u2022 access to answer files standardized\nAdequacy of resources\n\u2022 hardware availability\n\u2022 adequate rooming\n\u2022 purchase of CAA software site licence\n\u2022 CAA software installation\n32\n"}