{"doi":"10.1016\/j.patrec.2009.03.003","coreId":"55516","oai":"oai:eprints.lincoln.ac.uk:1976","identifiers":["oai:eprints.lincoln.ac.uk:1976","10.1016\/j.patrec.2009.03.003"],"title":"Descriptive temporal template features for visual motion recognition","authors":["Meng, Hongying","Pears, Nick"],"enrichments":{"references":[{"id":18436768,"title":"A human action recognition system for embedded computer vision application. In: CVPR workshop on Embeded Computer Vision.","authors":[],"date":"2007","doi":"10.1109\/cvpr.2007.383420","raw":"Meng, H., Pears, N., Bailey, C., 2007a. A human action recognition system for embedded computer vision application. In: CVPR workshop on Embeded Computer Vision.","cites":null},{"id":18436770,"title":"A survey of advances in vision-based human motion capture and analysis.","authors":[],"date":"2006","doi":"10.1016\/j.cviu.2006.08.002","raw":"Moeslund, T., Hilton, A., Kruger, V., November 2006. A survey of advances in vision-based human motion capture and analysis. Comput. Vis. Image Underst. 103 (2-3), 90\u2013126.","cites":null},{"id":18436748,"title":"Actions as spacetime shapes. In: ICCV.","authors":[],"date":"2005","doi":"10.1109\/iccv.2005.28","raw":"Blank, M., Gorelick, L., Shechtman, E., Irani, M., Basri, R., 2005. Actions as spacetime shapes. In: ICCV. pp. 1395\u20131402.","cites":null},{"id":18436752,"title":"An Introduction to Support Vector Machines (and other kernel-based learning methods).","authors":[],"date":"2000","doi":"10.1017\/cbo9780511801389","raw":"Cristianini, N., Shawe-Taylor, J., 2000. An Introduction to Support Vector Machines (and other kernel-based learning methods). Cambridge University Press, Cambridge, UK.","cites":null},{"id":18436756,"title":"Behavior recognition via sparse spatio-temporal features. In:","authors":[],"date":"2005","doi":"10.1109\/vspets.2005.1570899","raw":"Doll\u00b4 ar, P., Rabaud, V., Cottrell, G., Belongie, S., October 2005. Behavior recognition via sparse spatio-temporal features. In: VS-PETS.","cites":null},{"id":18436776,"title":"Compressed domain real-time action recognition. In:","authors":[],"date":"2006","doi":"10.1109\/mmsp.2006.285263","raw":"Yeo, C., Ahammad, P., Ramchandran, K., Sastry, S., 2006. Compressed domain real-time action recognition. In: IEEE International Workshop on Multimedia 22Signal Processing (MMSP) - 2006. IEEE, Washington, DC, USA.","cites":null},{"id":18436775,"title":"Continuous gesture recognition using a sparse bayesian classi\ufb01er.","authors":[],"date":"2006","doi":"10.1109\/icpr.2006.411","raw":"Wong, S.-F., Cipolla, R., 2006. Continuous gesture recognition using a sparse bayesian classi\ufb01er. In: ICPR (1). pp. 1084\u20131087.","cites":null},{"id":18436762,"title":"Dining activity analysis using a hidden markov model.","authors":[],"date":"2004","doi":"10.1109\/icpr.2004.1334408","raw":"Gao, J., Hauptmann, A. G., Bharucha, A., Wactlar, H. D., 2004. Dining activity analysis using a hidden markov model. In: ICPR (2). pp. 915\u2013918.","cites":null},{"id":18436754,"title":"Hierarchical motion history images for recognizing human motion. In:","authors":[],"date":"2001","doi":"10.1109\/event.2001.938864","raw":"Davis, J. W., 2001. Hierarchical motion history images for recognizing human motion. In: IEEE Workshop on Detection and Recognition of Events in Video. pp. 39\u201346.","cites":null},{"id":18436773,"title":"High-speed human motion recognition based on a motion history image and an eigenspace.","authors":[],"date":"2006","doi":"10.1093\/ietisy\/e89-d.1.281","raw":"Ogata, T., Tan, J. K., Ishikawa, S., 2006. High-speed human motion recognition based on a motion history image and an eigenspace. IEICE Transactions on Information and Systems E89 (1), 281\u2013289.","cites":null},{"id":18436766,"title":"Human action classi\ufb01cation using SVM 2K classi\ufb01er on motion features.","authors":[],"date":"2006","doi":"10.1007\/11848035_61","raw":"Meng, H., Pears, N., Bailey, C., 2006a. Human action classi\ufb01cation using SVM 2K classi\ufb01er on motion features. In: LNCS. Vol. 4105. Istanbul, Turkey, pp. 458\u2013465.","cites":null},{"id":18436753,"title":"Human detection using oriented histograms of \ufb02ow and appearance.","authors":[],"date":"2006","doi":"10.1007\/11744047_33","raw":"Dalal, N., Triggs, B., Schmid, C., 2006. Human detection using oriented histograms of \ufb02ow and appearance. In: ECCV (2). pp. 428\u2013441.","cites":null},{"id":18436747,"title":"Human motion analysis: a review.","authors":[],"date":"1999","doi":"10.1006\/cviu.1998.0744","raw":"Aggarwal, J. K., Cai, Q., 1999. Human motion analysis: a review. Comput. Vis. Image Underst. 73 (3), 428\u2013440.","cites":null},{"id":18436774,"title":"Kernel-based recognition of human actions using spatiotemporal salient points. In:","authors":[],"date":"2006","doi":"10.1109\/cvprw.2006.114","raw":"Oikonomopoulos, A., Patras, I., Pantic, M., June 2006. Kernel-based recognition of human actions using spatiotemporal salient points. In: Proceedings of CVPR workshop 06. Vol. 3. pp. 151\u2013156. URL http:\/\/pubs.doc.ic.ac.uk\/Pantic-CVPR06-1\/ Schuldt, C., Laptev, I., Caputo, B., 2004. Recognizing human actions: a local SVM approach. In: ICPR. Cambridge, U.K. Stau\ufb00er, C., Grimson, W. E. L., 2000. Learning patterns of activity using real-time tracking. IEEE Transactions on Pattern Analysis and Machine Intelligence 22 (8), 747\u2013757. URL citeseer.ist.psu.edu\/stauffer00learning.html Weinland, D., Ronfard, R., Boyer, E., 2005. Motion history volumes for free viewpoint action recognition. In: IEEE International Workshop on modeling People and Human Interaction (PHI\u201905). URL http:\/\/perception.inrialpes.fr\/Publications\/2005\/WRB05 Wong, S.-F., Cipolla, R., 2005. Real-time adaptive hand motion recognition using a sparse bayesian classi\ufb01er. In: ICCV-HCI. pp. 170\u2013179.","cites":null},{"id":18436755,"title":"Minimal-latency human action recognition using reliable-inference.","authors":[],"date":"2006","doi":"10.1016\/j.imavis.2006.01.012","raw":"Davis, J. W., Tyagi, A., 2006. Minimal-latency human action recognition using reliable-inference. Image Vision Comput. 24 (5), 455\u2013472.","cites":null},{"id":18436769,"title":"Motion information combination for fast human action recognition. In:","authors":[],"date":"2007","doi":"10.1007\/978-3-540-89682-1_11","raw":"Meng, H., Pears, N., Bailey, C., March 2007b. Motion information combination for fast human action recognition. In: 2nd International Conference on Computer Vision Theory and Applications (VISAPP07). Barcelona, Spain.","cites":null},{"id":18436750,"title":"Motion segmentation and pose recognition with motion history gradients.","authors":[],"date":"2002","doi":"10.1109\/wacv.2000.895428","raw":"Bradski, G. R., Davis, J. W., 2002. Motion segmentation and pose recognition with motion history gradients. Mach. Vis. Appl. 13 (3), 174\u2013184.","cites":null},{"id":18436763,"title":"Quantifying and recognizing human movement patterns from monocular video images-part ii: applications to biometrics.","authors":[],"date":"2004","doi":"10.1109\/tcsvt.2003.821977","raw":"Green, R. D., Guan, L., 2004. Quantifying and recognizing human movement patterns from monocular video images-part ii: applications to biometrics. IEEE Trans. Circuits Syst. Video Techn. 14 (2), 191\u2013198.","cites":null},{"id":18436765,"title":"Real-time human action recognition on an embedded, recon\ufb01gurable video processing architecture.","authors":[],"date":"2008","doi":"10.1007\/s11554-008-0073-1","raw":"Meng, H., Freeman, M., Pears, N., Bailey, C., 2008. Real-time human action recognition on an embedded, recon\ufb01gurable video processing architecture. Journal of Real-Time Image Processing.","cites":null},{"id":18436771,"title":"Recognition of human activities using space dependent switched dynamical systems. In:","authors":[],"date":"2005","doi":"10.1109\/icip.2005.1530526","raw":"Nascimento, J. C., Figueiredo, M. A. T., Marques, J. S., 2005. Recognition of human activities using space dependent switched dynamical systems. In: IEEE Int. Conf. on Image Processing, ICIP.","cites":null},{"id":18436751,"title":"Recognition of human body motion using phase space constraints. In: ICCV.","authors":[],"date":"1995","doi":"10.1109\/iccv.1995.466880","raw":"Campbell, L. W., Bobick, A. F., 1995. Recognition of human body motion using phase space constraints. In: ICCV. pp. 624\u2013630.","cites":null},{"id":18436759,"title":"Recognizing action at a distance. In: ICCV.","authors":[],"date":"2003","doi":"10.1109\/iccv.2003.1238420","raw":"Efros, A. A., Berg, A. C., Mori, G., Malik, J., 2003. Recognizing action at a distance. In: ICCV. pp. 726\u2013733.","cites":null},{"id":18436767,"title":"Recognizing human actions based on motion information and SVM. In:","authors":[],"date":"2006","doi":"10.1049\/cp:20060648","raw":"Meng, H., Pears, N., Bailey, C., 2006b. Recognizing human actions based on motion information and SVM. In: 2nd IET International Conference on Intelligent Environments. IET, Athens, Greece, pp. 239\u2013245.","cites":null},{"id":18436761,"title":"State-space reconstruction in the presence of noise.","authors":[],"date":"1991","doi":"10.1007\/978-1-4899-2305-9_5","raw":"Farmer, J., Casdagli, M., Eubank, S., Gibson, J., 1991. State-space reconstruction in the presence of noise. Physics D 51, 52\u201398.","cites":null},{"id":18436764,"title":"The entire regularization path for the support vector machine.","authors":[],"date":"2004","doi":null,"raw":"Hastie, T., Rosset, S., Tibshirani, R., Zhu, J., 2004. The entire regularization path for the support vector machine. URL citeseer.ist.psu.edu\/hastie04entire.html Joachims, T., 1999. Making large-scale SVM learning practical. In: Advances in Kernel Methods - Support Vector Learning. MIT-Press, USA, oikonomopoulos, Antonios and Patras, Ioannis and Pantic, Maja eds. URL http:\/\/svmlight.joachims.org\/ 21Ke, Y., Sukthankar, R., Hebert., M., 2005. E\ufb03cient visual event detection using volumetric features. In: ICCV. pp. 166\u2013173, beijing, China, Oct. 15-21, 2005.","cites":null},{"id":18436749,"title":"The recognition of human movement using temporal templates.","authors":[],"date":"2001","doi":"10.1109\/cvpr.1997.609439","raw":"Bobick, A. F., Davis, J. W., 2001. The recognition of human movement using temporal templates. IEEE Trans. Pattern Anal. Mach. Intell. 23 (3), 257\u2013267.","cites":null},{"id":18436772,"title":"Unsupervised learning of human action categories using spatial-temporal words. In:","authors":[],"date":"2006","doi":"10.5244\/c.20.127","raw":"Niebles, J., Wang, H., Fei-Fei, L., 2006. Unsupervised learning of human action categories using spatial-temporal words. In: BMVC06. p. III:1249.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2009","abstract":"In this paper, a human action recognition system is proposed. The system is based on new, descriptive `temporal template' features in order to achieve high-speed recognition in real-time, embedded applications. The limitations of the well known `Motion History Image' (MHI) temporal template are addressed and a new `Motion History Histogram' (MHH) feature is proposed to capture more motion information in the video. MHH not only provides rich motion information, but also remains computationally inexpensive. To further improve classification performance, we combine both MHI and MHH into a low dimensional feature vector which is processed by a support vector machine (SVM). Experimental results show that our new representation can achieve a significant improvement in the performance of human action recognition over existing comparable methods, which use 2D temporal template based representations","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/55516.pdf","fullTextIdentifier":"http:\/\/eprints.lincoln.ac.uk\/1976\/1\/prlpahci07.pdf","pdfHashValue":"f4668b857c5ab086bb7cda6817120f6ccf5a1025","publisher":"Elsevier","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lincoln.ac.uk:1976<\/identifier><datestamp>\n      2013-03-13T08:33:20Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47373030<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47343430<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47373430<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lincoln.ac.uk\/1976\/<\/dc:relation><dc:title>\n        Descriptive temporal template features for visual motion recognition<\/dc:title><dc:creator>\n        Meng, Hongying<\/dc:creator><dc:creator>\n        Pears, Nick<\/dc:creator><dc:subject>\n        G700 Artificial Intelligence<\/dc:subject><dc:subject>\n        G440 Human-computer Interaction<\/dc:subject><dc:subject>\n        G740 Computer Vision<\/dc:subject><dc:description>\n        In this paper, a human action recognition system is proposed. The system is based on new, descriptive `temporal template' features in order to achieve high-speed recognition in real-time, embedded applications. The limitations of the well known `Motion History Image' (MHI) temporal template are addressed and a new `Motion History Histogram' (MHH) feature is proposed to capture more motion information in the video. MHH not only provides rich motion information, but also remains computationally inexpensive. To further improve classification performance, we combine both MHI and MHH into a low dimensional feature vector which is processed by a support vector machine (SVM). Experimental results show that our new representation can achieve a significant improvement in the performance of human action recognition over existing comparable methods, which use 2D temporal template based representations.<\/dc:description><dc:publisher>\n        Elsevier<\/dc:publisher><dc:date>\n        2009<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lincoln.ac.uk\/1976\/1\/prlpahci07.pdf<\/dc:identifier><dc:identifier>\n          Meng, Hongying and Pears, Nick  (2009) Descriptive temporal template features for visual motion recognition.  Pattern Recognition Letters, 30  (12).   pp. 1049-1058.  ISSN 1047-1160  <\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1016\/j.patrec.2009.03.003<\/dc:relation><dc:relation>\n        10.1016\/j.patrec.2009.03.003<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lincoln.ac.uk\/1976\/","http:\/\/dx.doi.org\/10.1016\/j.patrec.2009.03.003","10.1016\/j.patrec.2009.03.003"],"year":2009,"topics":["G700 Artificial Intelligence","G440 Human-computer Interaction","G740 Computer Vision"],"subject":["Article","PeerReviewed"],"fullText":"Descriptive temporal template features for\nvisual motion recognition\nHongying Meng1 and Nick Pears2\n1Department of Computing and Informatics, University of Lincoln, UK\n2Department of Computer Science, University of York, UK\nAbstract\nIn this paper, a human action recognition system is proposed. The system is based\non new, descriptive \u2018temporal template\u2019 features in order to achieve high-speed\nrecognition in real-time, embedded applications. The limitations of the well known\n\u2018Motion History Image\u2019 (MHI) temporal template are addressed and a new \u2018Motion\nHistory Histogram\u2019 (MHH) feature is proposed to capture more motion information\nin the video. MHH not only provides rich motion information, but also remains com-\nputationally inexpensive. To further improve classification performance, we combine\nboth MHI and MHH into a low dimensional feature vector which is processed by a\nsupport vector machine (SVM). Experimental results show that our new represen-\ntation can achieve a significant improvement in the performance of human action\nrecognition over existing comparable methods, which use 2D temporal template\nbased representations.\nKey words: Gesture recognition, Event recognition, Embedded vision, Motion\nanalysis, Machine learning\n1 Introduction\nIn this paper, we focus on the use of video for monitoring human events, with\na view to real-time, embedded implementations in security systems, human-\ncomputer interaction and intelligent environments. Such events may be pre-\nscribed gestures, or simple actions, such as walking in a particular direction\nacross the scene. In particular, our methods are designed to be appropriate\nfor deployment in a real-time, embedded context. In this sense, we aim for\ncompact, yet descriptive image-based (2D) motion representations, which are\nsimple to compute. Also we aim for low complexity classification algorithms,\nso that both feature extraction and classification may be implemented on our\nPreprint submitted to Elsevier 10 November 2008\nflexible stand-alone video processing architecture, which is based upon field-\nprogrammable gate arrays (FPGAs). A detailed description of this hardware\ncan be found in Meng et al. (2007a, 2008).\nGenerally, event detection in video has become an important computer vision\napplication, particularly in the context of activity classification (Aggarwal\nand Cai (1999)). For human activity or behaviour recognition, many efforts\nhave been concentrated on using state-space methods (Farmer et al. (1991)) to\nunderstand human motion sequences (Bobick and Davis (2001); Campbell and\nBobick (1995); Gao et al. (2004); Nascimento et al. (2005); Davis and Tyagi\n(2006)). However, these methods usually need intrinsic nonlinear models and\ndo not have a closed-form solution. Previous work on motion descriptors uses\npositions and velocities of human body parts (Green and Guan (2004)), but\nsuch information is often difficult to extract automatically during unrestricted\nhuman activities.\nAggarwal and Cai (1999) present an excellent overview of human motion anal-\nysis. Of the appearance based methods, template matching has gained increas-\ning interest ( Schuldt et al. (2004); Weinland et al. (2005); Ke et al. (2005);\nWong and Cipolla (2005); Dolla\u00b4r et al. (2005); Niebles et al. (2006); Ogata\net al. (2006); Dalal et al. (2006); Blank et al. (2005); Oikonomopoulos et al.\n(2006); Meng et al. (2006b,a); Wong and Cipolla (2006); Yeo et al. (2006)).\nAn example of a temporal template motion representation, the \u2018Motion His-\ntory Image\u2019 (MHI) (Bobick and Davis (2001)), is given in Fig. 1, where (a)\nis one frame from the original video clip and (b) is the MHI of this action. It\nis clear that MHI is an image, with the same size as the original frame, but\nwhich retains motion information associated with the action. To compute an\nMHI, we start by computing a foreground motion segmentation, which may\nbe done by simple frame differencing or more complex background modelling,\nsuch as Gaussian Mixture Models (Stauffer and Grimson (2000)). The MHI is\nthen generated as the weighted sum of past foreground segmentations and the\nweights decay back through time. Therefore, an MHI image contains the past\nforeground segmentations within itself, where most recent foreground motion\nis brighter than past ones. (This is mathematically defined in section 3.)\nWe have elected to further develop this \u2018temporal-template\u2019 approach. How-\never, given that the extracted motion information from temporal templates is\nappearance based, we have the limitations of (i) viewpoint dependence and\n(ii) loss of information in the projection from 3D to 2D. Both of these prob-\nlems are mitigated if the motion is predominantly parallel to the image plane.\nIn the case of gestures, these can be designed to be in an appropriate direc-\ntion relative to the camera. In the context of less deliberative, natural human\nmotion, the problem can only be solved by implementing multiple viewpoints\nand selecting the most appropriate data stream. This was a necessary compro-\nmise to allow us to implement a real-time embedded solution to our pattern\n2\nrecognition problem. Given that we have these restrictions, the problem that\nwe wished to solve was how to extract more information in order to improve\nthe state-of-the-art in temporal template based motion recognition approaches,\nwhile retaining a compact feature vector, that is easily deployed on embedded\nhardware.\nIn the following, we describe a human action recognition system based on a\nlinear Support Vector Machine (SVM) (Cristianini and Shawe-Taylor (2000)).\nWe address the limitations of the well-known Motion History Image (MHI)\n(Bobick and Davis (2001)), which is the pioneering work in temporal tem-\nplates, and propose a new feature, which we call the Motion History His-\ntogram (MHH). This representation expresses more motion information than\nthe MHI, but also remains inexpensive to compute. We show that the MHH\nand its derivatives outperform the MHI in terms of action recognition on a\nlarge public database. Finally, we extract a compact feature vector from the\nMHH and then combine it with a \u2018histogram of MHI\u2019 feature to give a better\nclassification performance than either feature type alone.\nThe rest of this paper is organized as follows: In section 2, we give an overview\nof related work. In section 3, we briefly review the MHI and its limitation. In\nsection 4, we give a detailed description of our new MHH temporal template\nfeatures, designed to overcome the MHI limitation, and their compact deriva-\ntives. In section 5, we discuss dimension reduction and feature combination,\napplied to MHI and MHH. In section 6, experimental results are evaluated\nand, finally, we present conclusions. The work presented here is an extension\nof our previous work (Meng et al. (2006b, 2007b)).\n2 Related work\nBobick and Davis (2001) pioneered the idea of temporal templates (Moeslund\net al. (2006)). They used Motion Energy Images (MEI) and MHI to recognize\nmany types of aerobics exercise. Bradski and Davis (2002) proposed the Mo-\ntion Gradient Orientation (MGO) to explicitly encode changes in an image\nintroduced by motion events.\nDavis (2001) also presented a useful hierarchical extension for computing a\nlocal motion field from the original MHI representation. The MHI was trans-\nformed into an image pyramid, permitting efficient fixed-size gradient masks to\nbe convolved at all levels of the pyramid, thus extracting motion information\nat a wide range of speeds. The hierarchical MHI approach remains a com-\nputationally inexpensive algorithm to represent, characterize, and recognize\nhuman motion in video.\n3\nSchuldt et al. (2004) proposed a method for recognizing complex motion pat-\nterns based on local space-time features in video and they integrated such\nrepresentations with SVM classification schemes for recognition.\nThe work of Efros et al. (2003) focuses on the case of low resolution video\nof human behaviours, targeting what they refer to as the 30 pixel man. In\nthis setting, they propose a spatio-temporal descriptor based on optical flow\nmeasurements, and apply it to recognize actions in ballet, tennis and football\ndatasets.\nWeinland et al. (2005) introduced Motion History Volumes (MHV) as a free-\nviewpoint representation for human actions in the case of multiple calibrated,\nand background-subtracted, video. They presented algorithms for computing,\naligning and comparing MHVs of different actions performed by different peo-\nple from a variety of viewpoints.\nKe et al. (2005) studied the use of volumetric features as an alternative to\nthe local descriptor approaches for event detection in video sequences. They\ngeneralized the notion of 2D box features to 3D spatio-temporal volumetric\nfeatures. They constructed a real-time event detector for each action of interest\nby learning a cascade of filters based on volumetric features that efficiently\nscanned video sequences in space and time.\nOgata et al. (2006) proposed Modified Motion History Images (MMHI) and\nused an eigenspace technique to realize high-speed recognition of six human\nmotions.\nWong and Cipolla (2005) proposed a new method to recognize primitive move-\nments based on MGO extraction and, later, used it for continuous gesture\nrecognition (Wong and Cipolla (2006)).\nRecently, Dalal et al. (2006) proposed Histogram of Oriented Gradient (HOG)\nappearance descriptors for image sequences and developed a detector for stand-\ning and moving people in video.\nDolla\u00b4r et al. (2005) proposed a similar method where they use a new spatio-\ntemporal interest point detector to obtain a global measurement instead of the\nlocal features in Efros et al. (2003). Niebles et al. (2006) also use spatial-time\ninterest points to extract spatial-temporal words as their features. Yeo et al.\n(2006) estimate motion vectors from optical flow and calculate frame-to-frame\nmotion similarity to analyse human action in video.\nBlank et al. (2005) regarded human actions as three dimensional shapes in-\nduced by silhouettes in space-time volume. They adopted an approach for\nanalyzing 2D shapes and generalized it to deal with volumetric space-time\naction shapes.\n4\nOikonomopoulos et al. (2006) introduced a sparse representation of image\nsequences as a collection of spatio-temporal events that were localized at points\nthat were salient both in space and time for human action recognition.\nWe note that, in some of these methods, the motion features employed are\nrelatively complex (Efros et al. (2003); Schuldt et al. (2004); Weinland et al.\n(2005); Niebles et al. (2006); Dalal et al. (2006); Dolla\u00b4r et al. (2005); Blank\net al. (2005); Oikonomopoulos et al. (2006); Ke et al. (2005); Yeo et al. (2006)),\nwhich implies significant computational cost when building the features. Some\nof them require tracking or other prohibitive computational cost processes\n(Bobick and Davis (2001); Bradski and Davis (2002); Davis (2001); Wong\nand Cipolla (2005, 2006); Ogata et al. (2006); Blank et al. (2005)), which,\nfor the time being, makes them not suitable for real-time embedded vision\napplications.\n3 The Motion History Image\nSeveral representations have been proposed to compact the whole motion se-\nquence of a video clip into one image to represent the motion. Such images\nare often termed \u2018temporal templates\u2019. The most popular \u2018temporal template\u2019\nmotion feature is the Motion History Image (MHI) (Bobick and Davis (2001)).\nWe now review this in outline and then we discuss its main limitation, that\nwe aim to overcome, by design of a new feature type.\nAn MHI is the weighted sum of past images and the weights decay back\nthrough time. Therefore, an MHI image contains the past images within itself,\nin which the most recent image is brighter than past ones. Normally, an MHI\nH\u03c4 (u, v, k) at time k and location (u, v) is defined by the following Eq. 1:\nH\u03c4 (u, v, k) = {\n\u03c4, if D(u, v, k) = 1\nmax{0, H\u03c4 (u, v, k \u2212 1)\u2212 1}, otherwise\n(1)\nwhere the motion mask D(u, v, k) is a binary image representing the fore-\nground motion, obtained from subtraction of frames, and \u03c4 is the maximum\nduration a motion is stored. In general, \u03c4 is chosen as the constant 255, allow-\ning the MHI to be easily represented as a gray scale image with one byte depth.\nThus an MHI pixel can have a range of values, whereas an MEI (motion en-\nergy image) is its binary version, that can easily be computed by thresholding\nH\u03c4 > 0.\nFig. 1 shows an example of the MHI motion features. Although we do not see\nthe original video clip of the action, we can still determine that the subject\n5\n          (a) Handwaving sample                       (b) MHI of Handwaving \n             \nFig. 1. Example of an MHI. (a) is one frame from the original hand waving action\nvideo clip and (b) is the MHI of this action. The vertical red line in (b) has the\npixels from (60, 11) to (60, 80).\nmoved both arms above their head. An MHI can be generated frame by frame\nand only three frames (previous frame, current frame and MHI) need to be\nstored at any one time. The final output associated with each action is an\nimage, which has the same size as the original input frames.\n3.1 Limitations of the MHI\nIn order to have a detailed look at the MHI, we have selected the pixels on\nthe red line in the MHI of Fig. 1 (b). If some action happened at frame k\non pixel (u, v), then D(u, v, k) = 1, otherwise D(u, v, k) = 0. The locations\nof these pixels are (60, 11), (60, 12), \u00b7 \u00b7 \u00b7 , (60, 80). For a pixel (u, v), the motion\nmask D(u, v, :) of this pixel is the binary sequence:\nD(u, v, :) = (b1, b2, \u00b7 \u00b7 \u00b7 , bN) , bi \u2208 {0, 1} (2)\nwhere N +1 is the total number of frames (as we need two images at the start\nof the sequence to generate b1).\nAll of the motion masks on the red line are shown in Fig. 2. Each row isD(u, v, :\n) for one fixed pixel (u, v) and a white block represents \u20181\u2019 and black block\nrepresents \u20180\u2019 in the sequences. The green line is the motion mark D(60, 50, :)\nand it has the following sequence (3):\n0000000001101000000000000000000000001010000 (3)\nFrom the definition of MHI in Eq. 1 it can be observed that, for each pixel\n(u, v), MHI actually retains the time since the last action occurred. That is,\nonly the last \u20181\u2019 in the sequence 3 is retained in the MHI at pixel (60, 50).\n6\n(60,11)\n(60,20)\n(60,30)\n(60,40)\n(60,50)\n(60,60)\n(60,70)\n(60,80)\n1 10 20 30 40\nP\nix\ne\nls\nNo. of frames\nFig. 2. D(:, :, :) on the red line of Fig. 1(b) is shown. Each row is D(u, v, :) for one\nfixed pixel (u, v). A white block represents \u20181\u2019 and a black block \u20180\u2019. The horizontal\ngreen line is the \u2018binary frame difference history\u2019 or \u2018motion mask\u2019 of pixel (60, 50)\nthrough time, ie, D(60, 50, :).\nIt is clear that previous \u20181\u2019s in the sequence, when some action occurred, are\nnot represented. It is also clear that almost all the pixels have more than one\n\u20181\u2019 in their sequence, implying that much motion information is lost when an\nMHI is generated.\n4 Motion History Histograms (MHH)\nThe above limitation of the MHI has motivated us to design a new represen-\ntation (the MHH) in which all of the information in the sequence is used and,\nyet, it remains compact and efficient to use.\nWe define the patterns Pi in the D(u, v, :) sequences, based on the number of\n7\nFig. 3. The patterns Pi in D(u, v, :). White and black blocks represent \u20181\u2019 and \u20180\u2019\nrespectively.\nconnected \u20181\u2019s, as shown in Fig. 3:\nP1 = 010\nP2 = 0110\nP3 = 01110\n...\nPM = 01 \u00b7 \u00b7 \u00b7 1\ufe38 \ufe37\ufe37 \ufe38\nM\n0\n(4)\nWe denote a subsequence CI,k by Eq. 5, where I and k are the indices of\nstarting and ending frames, and denote the set of all subsequences of D(u, v, :)\nas \u2126 {D(u, v, :)}. Then, for each pixel (u, v), we can count the number of\noccurrences of each specific pattern Pi in the sequence D(u, v, :), as shown in\nEq. 6, where 1 is the indicator function.\nCI,k = bI , bI+1, \u00b7 \u00b7 \u00b7 , bk, 1 \u2264 I < k \u2264 N (5)\nMHH(u, v, i) =\n\u2211\n(I,k) 1{CI,k=Pi|CI,k\u2208\u2126{D(u,v,:)}}\n1 \u2264 I < k \u2264 N, 1 \u2264 i \u2264M\n(6)\nFrom each pattern Pi, we can build a gray scale image and we call this its\n\u2018Histogram\u2019, since the bin value records the number of this pattern type. With\nall the patterns Pi, i = 1...M together, we collectively call them the \u2018Motion\nHistory Histograms\u2019(MHH) representation.\nThe computation of MHH is inexpensive and can be implemented by the\nfollowing procedure. D(u, v, k) is the binary sequences on pixel (u, v) that is\ncomputed by thresholding the differences between frame k and frame k \u2212 1.\nI(u, v) is a frame index that stands for the number of the starting frame of\na new pattern on pixel (u, v). At the beginning, I(u, v) = 1 for all (u, v).\n8\nAlgorithm (MHH)\nInput: Video clip f (u,v,k), u=1,...,U, v=1,...,V, frame k=0,1,...,N\nInitialisation: Pattern M, MHH(1:U,1:V,1:M)=0, I(1:U,1:V)=1\nFor k=1 to N (For 1) \n Compute: D(:,:,k)\nFor u=1 to U (For 2)\nFor v= 1 to V (For 3) \n          If  subsequence {D(u,v,I(u,v)),\u2026,D(u,v,k)}=Pi\nUpdate: MHH(u,v,i)=MHH(u,v,i)+1\n        End\n                    Update:  I(u,v)\n        End (For 3)\nEnd (For 2)\nEnd (For 1)\nOutput: MHH(1:U,1:V,1:M)\nThat means a new pattern starts from frame 1 for every pixel. I(u, v) will be\nupdated to I(u, v) = k while {D(u, v, I(u, v)), ..., D(u, v, k)} builds one of the\npatterns Pi (1 \u2264 i \u2264M) and, in this case, MHH(u, v, i) increases by 1.\nFor a pattern Pi, MHH(:, :, i) can be displayed as an image. In Fig. 4, four\npatterns P1, P2, P3 and P4 are shown, which were generated from the hand\nwaving action in Fig. 1. By comparing the MHH in Fig. 4 with the MHI\nin Fig. 1, it is interesting to find that the MHH decomposes the MHI into\ndifferent parts based on patterns. Unlike the hierarchical MHI described by\nDavis (2001), where only small size MHIs were obtained, MHH records the\nrich spatial information of an action.\nThe choice of the numberM depends on the video clips. In general, the bigger\ntheM is, the better the motion information will be. However, the values within\nthe MHH rapidly approach zero as M increases. In our experiment, no more\nthan half of the training data had the sixth pattern P6 and so we choseM = 5.\nFurthermore we note that a large M will increase the storage requirement for\nour hardware based system.\nWe can define the binary version of an MHH as MHHb , as shown in Eq. 7,\nand the motivation for this is given in the following section.\nMHHb(u, v, i) = {\n1, if MHH(u, v, i) > 0\n0, otherwise\n(7)\n9\n         (a) MHH(:,:,1)                                           (b) MHH(:,:,2)\n                  (c) MHH(:,:,3)                                           (d) MHH(:,:,4) \nFig. 4. MHH example. Four patterns P1, P2, P3 and P4 were selected. This results\nwere generated from the handwaving action in Fig. 1. Each pattern Pi, MHH(:, :, i)\nhas the same size as the original frame.\n5 Dimension reduction and feature combination\nAs previously stated, our work is targeted at embedded implementations us-\ning FPGA platforms. This requires us to use compact feature vectors, which\nretain discriminating information across the various classes of interest in a\nparticular application. Also, the process of compacting must be simple, in\norder to generate a small footprint (gate count) solution on our embedded de-\nvices. The following subsections describe how we employ histograms of both\nMHI and MHH representations, to compact their representations, and then\ncombine them via simple feature vector concatenation.\n5.1 MHI Histograms\nMHIs can be rendered as gray scale images, where a value of a pixel in the MHI\nrecords time information, namely when some motion most recently occurred at\nthis particular pixel location. Thus the histogram of MHI captures information\n10\nrepresenting the speed of the motion across the image. Since an MHI is an\nimage with values between 0 and 255, an MHI histogram is a vector of length\n256. An example of an MHI histogram is given at the bottom-left of Fig. 5.\nFor fast motions, many pixels in the MHI will have high values, whereas, for\nslow motions, only a few pixels in the MHI will have high values, thus these\ndifferent cases will have quite different MHI histograms.\n5.2 MHH Histograms : the Motion Geometric Distribution (MGD)\nThe size of the MHHb representation is rather large and we seek a more com-\npact representation, which captures the geometric distribution of the motion\nacross the image. Thus we sum each row of MHHb (for a given pattern, Pi) to\ngive a vector of size V rows. We obtain another vector by summing columns\nto give a vector of size U rows. Thus using all M levels in the binarised MHH\nhierarchy, we obtain a \u2018Motion Geometric Distribution\u2019 (MGD) vector of size\nM \u00d7 (U + V ), which is relatively compact, when compared to the size of the\noriginal MHH and MHI features. The MGD vector can thus be represented\nby the following Eq. 8:\nMGD = {\n\u2211\nuMHHb(u, v, i),\n\u2211\nv MHHb(u, v, i)}\ni = 1, 2, \u00b7 \u00b7 \u00b7 ,M\n(8)\nIn our work, we prefer to compute the MGD by using the MHHb feature instead\nof the MHH feature directly. From our experiments, it has been found that the\nvalues within the MHH decrease significantly for the large patterns. The values\nfor P4 and P5, for example, are much smaller than those of P1, P2 and P3. Thus,\nif we use the MHH directly to compute the MGD, a normalisation process is\nnecessary in order to treat all the patterns equally. However, this normalisation\nprocess is not an easy task for our hardware implementation because of limited\nmemory and the requirement to implement a floating-point processing ability.\nIn contrast, computation of the MGD from the MHHb feature does not need\na normalisation process and yet we are able to retain a good performance.\n5.3 Combining MHH and MHI histogram features\nIf different features are able to capture different discriminating properties of\na video clip, then a combination of such features is likely to improve classifi-\ncation performance over either feature alone. Indeed, this is what we find in\nthe following section, which describes our evaluation. Based on the simplicity\nrequirement of our embedded systems, our two feature vectors are combined\n11\nMGD\nHist_MHI\nFig. 5. Combination between MGD of the MHH and histogram of the MHI from\na same video example. The frame has the size of 160 \u00d7 120. MGD of MHH and\nhistogram of MHI have the size of (160 + 120)\u00d7 5 = 1400 and 255 respectively.\nin the simplest way by concatenating these two feature vectors into a higher\ndimensional vector. Fig. 5 shows an example of a combination between the\nMGD of the MHH and the histogram of the MHI from the same video.\n6 Evaluation of our system\n6.1 Experimental setup\nFor the evaluation of our system, we use a challenging human action recog-\nnition database, recorded by Schuldt et al. (2004), which is both large and\npublically available. It contains six types of human actions (walking, jogging,\nrunning, boxing, hand-waving and hand-clapping) performed several times by\n25 subjects in four different scenarios: outdoors (s1), outdoors with scale vari-\nation (s2), outdoors with different clothes (s3) and indoors (s4).\nThis database contains 2391 sequences. All sequences were taken over homoge-\nneous backgrounds with a static camera with 25Hz frame rate. The sequences\nwere down-sampled to the spatial resolution of 160\u00d7120 pixels and have a\ntime length of four seconds in average. To the best of our knowledge, this is\nthe largest video database with sequences of human actions taken over differ-\nent scenarios. All sequences were divided with respect to the subjects into a\ntraining set (8 persons), a validation set (8 persons) and a test set (9 persons).\nIn our experiment, the classifiers were trained on a training set while clas-\nsification results were obtained on the test set. In all our experiments, the\n12\n(a) \n(b) \n(c) \n(d) \n(e) \n(f) \nFig. 6. Six types of human action in the database: (a) walking (b) jogging (c) running\n(d) boxing (e) hand-clapping (f) hand-waving.\nsame parameters were used. The threshold in frame differencing was chosen\nas 25 and \u03c4 was chosen as 255 for MHI construction. The most suitable choice\nof the number of patterns M for MHH computation depends on the video\nclips and is a tradeoff between the compactness of the representation and the\nexpressiveness of the representation. Building a frequency histogram of the\npatterns extracted from the training clips indicates that no more than half\nof the training data had the sixth pattern. Thus the number of patterns was\nchosen to be M = 5.\nThe size of the MHI is 160\u00d7 120 = 19200, which is the same width as that of\nthe frames in the videos. In our experiment, the SVM is implemented using\nthe SVM light software (Joachims (1999)). In SVM training, choosing a good\nparameter C value is not so straightforward and can significantly affect classi-\nfication accuracy (Hastie et al. (2004)) but in order to keep our system simple,\nthe default value of C in SVM light is used in all the experiments.\nFig. 6 shows examples in each type of human action in this dataset. In order\nto compare our results with those in Ke et al. (2005) and Schuldt et al. (2004),\nwe use the exact same training set and testing set in our experiments. The\nonly difference is that we did not use the validation dataset in training. Our\nexperiments are carried out on all four different scenarios. In the same manner\nas Ke et al. (2005), each sequence is treated individually during the training\nand classification process. In all the following experiments, the parameters\nwere are the same.\n13\n    (1)                    \n    (2)                 \n    (3)          \n                   (a)                   (b)                     (c)                     (d)                   (e)                     (f)\nFig. 7. The (1) MHI, (2) MMHI and (3) MGO for the six actions in the dataset:\n(a) walking (b) jogging (c) running (d) boxing (e) hand-clapping (f) hand-waving\n6.2 Performance on single features\nWe have tested the performance of the fundamental motion features MHI,\nMMHI and MGO in our system. Fig. 7 shows these three motion features\nextracted from the action examples shown in Fig. 6. In order to keep our\nsystem simple for hardware implementation, we use the simplest method to\ntransform the motion features (MHI, MMHI and MGO) into a plain vector\nbased on the pixel scan order (row by row) to feed the SVM classifier.\nFirstly, we tested the system performance on the four different subsets of\nthe whole dataset. The results can be seen in Fig. 8. The correctly classified\npercentage on these data subsets indicates how many percent of the action\nclips in the testing set were correctly recognized by the system. It is clear that\nthe MHI feature gave the best classification performance in all the four subset\nwhile the MGO feature gave poor results for all four data subsets. We also can\nsee that subset s2 (outdoors with scale variation) is the most difficult subset\nin the whole dataset.\nFrom the experiments, it can be seen that this type of system can get rea-\nsonable results. The MHI based system looks better than the MMHI system\nin the experiments. The disadvantage for MMHI is that it can only work well\nin the case of an uncluttered and static background. If there is background\nmotion or noise, this will be recorded in the feature vector and will reduce the\nperformance of the classification.\nFor the whole dataset, the classification confusion matrix is a good measure\nfor the overall performance in this multi-class classification problem. Table 1\nshows the classification confusion matrix based on the method proposed in\nKe et al. (2005). Table 2 shows the confusion matrix obtained by our system\nbased on MHI. The confusion matrices show the motion label (vertical) versus\n14\n010\n20\n30\n40\n50\n60\n70\n80\ns1 s2 s3 s4\nC\no\nrr\ne\nc\ntl\ny\n c\nla\ns\ns\nif\nie\nd\n (\n%\n) MHI\nMMHI\nMGO\nFig. 8. Correctly classified percentage for separate data subset: s1 (outdoors), s2\n(outdoors with scale variation), s3 (outdoors with different clothes) and s4 (indoors).\nthe classification results (horizontal). Each cell (i, j) in the table shows the\npercentage of class i action being recognized as class j. Thus the main diagonal\nof the matrices show the percentage of correctly recognized actions, while the\nremaining cells show the percentages of misclassification. The trace of the\nmatrix shows the overall classification rate. In Table 1, the trace is 377.8\nand since there are six classes, then the overall mean classification rate is\n377.8\/6 = 63%.\nTable 1\nKe\u2019s confusion matrix (Ke et al. (2005)), trace=377.8, mean performance=63%\nWalk Jog Run Box Clap Wave\nWalk 80.6 11.1 8.3 0.0 0.0 0.0\nJog 30.6 36.2 33.3 0.0 0.0 0.0\nRun 2.8 25.0 44.4 0.0 27.8 0.0\nBox 0.0 2.8 11.1 69.4 11.1 5.6\nClap 0.0 0.0 5.6 36.1 55.6 2.8\nWave 0.0 5.6 0.0 2.8 0.0 91.7\nIn comparison with Ke\u2019s method, we use a simple MHI feature rather than\nlarge volumetric features in which the dimension of a feature vector might\nbe a billion, yet the performance of our system is marginally better on this\ndataset. Notice from Table 2 that the actions of walking, jogging and running\nare not well discriminated, as they are similar actions, performed at different\nspeeds, whereas the more distinctive actions of boxing, hand-waving and hand-\nclapping appear to be easier to classify.\nIn a second series of experiments, we tested low dimensional features, which\n15\nTable 2\nMHI\u2019s confusion matrix, trace=381.2, mean performance=63.5%\nWalk Jog Run Box Clap Wave\nWalk 53.5 27.1 16.7 0.0 0.0 2.8\nJog 46.5 34.7 16.7 0.7 0.0 1.4\nRun 34.7 28.5 36.1 0.0 0.0 0.7\nBox 0.0 0.0 0.0 88.8 2.8 8.4\nClap 0.0 0.0 0.0 7.6 87.5 4.9\nWave 0.0 0.0 0.0 8.3 11.1 80.6\nTable 3\nMHI S\u2019s confusion matrix, trace=377.7, mean performance=62.95%\nWalk Jog Run Box Clap Wave\nWalk 56.9 18.1 22.2 0.0 0.0 2.8\nJog 45.1 29.9 22.9 1.4 0.0 0.7\nRun 34.7 27.8 36.1 0.0 0.0 1.4\nBox 0.0 0.0 0.0 89.5 2.1 8.4\nClap 0.0 0.0 0.0 5.6 88.9 5.6\nWave 0.0 0.0 0.0 12.5 11.1 76.4\nTable 4\nHist. of MHI\u2019s confusion matrix, trace=328.6, mean performance=54.8%\nWalk Jog Run Box Clap Wave\nWalk 62.5 32.6 0.0 1.4 1.4 2.1\nJog 12.5 58.3 25.0 0.0 0.0 4.2\nRun 0.7 18.8 77.1 0.0 0.0 3.5\nBox 4.9 2.8 0.7 17.5 61.5 12.6\nClap 4.9 2.1 0.7 11.1 75.0 6.3\nWave 5.6 3.5 6.9 20.1 25.7 38.2\nare generated from fundamental motion features, such as MHI. Sub-sampling\nis easy to implement in hardware by any factor of 2 and this can be done in\nboth rows and columns of the motion feature. Tables 3 and 4 show the results\nbased on downsampling the MHI by a factor of 64 (a factor of 8 for both row\nand column) and the histogram of (subsampled) MHI respectively. It can be\nseen that subsampling the MHI to a significantly lower resolution does not\nhave an unduly detrimental effect on classification performance. This feature\n16\n  \n \n \n \n(a)        \n(b)        \n(c)        \n(d)        \n(e)        \n(f)        \n           (1) Video            (2) MHH(:,:,1)      (3) MHH(:,:,2)      (4) MHH(:,:,3)       (5) MHH(:,:,4)      (6)MHH(:,:,5) \nFig. 9. The six database human actions and associated MHH features: (a) walking\n(b) jogging (c) running (d) boxing (e) hand-clapping (f) hand-waving.\nperformed well in distinguishing the last three groups (box, clap, wave). On\nthe other hand, the histogram of MHI did not perform well in terms of overall\nperformance but has the power to distinguish the first three groups of action\n(walk, jog, run), which are similar shaped actions, executed at significantly dif-\nferent speeds. This demonstrates that these two features make different types\nof information more explicit to the classification process and this provides a\nstrong motivation for combining different feature types.\nFig. 9 shows examples in each type of human action and their associated MHH\nmotion features. For the MHH, it is hard to deal with the whole feature in our\ntarget hardware system as, with the number of patterns set to 5, the MHH\nhas a relatively high dimension of 5x160x120=96000. Thus, we constructed a\nsmaller sized MHH S by averaging the pixels in an 8 \u00d7 8 block, so that the\nsize of all MHH feature vectors is reduced to 20 \u00d7 15 \u00d7 5 = 1500. Our MGD\nfeature also has a small size of (160 + 120)\u00d7 5 = 1400.\nTable 5 and Table 6 show the results when using features MHH S and MGD\nrespectively. From these two tables, it is very clear that both MHH S and MGD\nimprove the overall performance, but they are failed to classify the \u2018jogging\u2019\nclass. The reason is that these video clips are quite similar to walking and\nrunning. It is hard to distinguish them correctly even by human observation.\nThis motivates us to consider the combination of the MGD with a feature\nthat is particularly sensitive to the speed of the motion, as is the case with\nthe \u2018histogram of MHI\u2019.\n17\nTable 5\nMHH S\u2019s confusion matrix, trace=417.3, mean performance=69.55%\nWalk Jog Run Box Clap Wave\nWalk 88.9 1.4 6.3 0.7 1.4 1.4\nJog 56.9 2.1 38.2 0.7 2.1 0.0\nRun 22.2 0.7 75.7 0.0 1.4 0.0\nBox 0.0 0.0 0.0 96.5 0.7 2.8\nClap 0.0 0.0 0.0 4.2 93.1 2.8\nWave 0.0 0.0 0.0 22.2 16.7 61.1\nTable 6\nMGD\u2019s confusion matrix, trace=432.6, mean performance=72.1%\nWalk Jog Run Box Clap Wave\nWalk 85.4 4.9 2.8 2.8 2.8 1.4\nJog 65.3 9.2 23.6 2.1 0.0 0.0\nRun 18.8 8.3 68.8 1.4 0.0 2.8\nBox 0.0 0.0 0.0 91.6 2.8 5.6\nClap 1.4 0.0 0.0 6.3 92.4 0.0\nWave 0.0 0.0 0.0 7.6 6.9 85.4\n6.3 Performance on combined features\nFrom the previous subsection, we found that different features had different\npowers in distinguishing classes of action. In order to overcome their own\nlimitations, we combine the MGD (derived from the MHH) and the histogram\nof the MHI, by concatenation. Table 7 shows the confusion matrix obtained\nfrom our system when these combined features were used. From this table, we\ncan see that the overall performance has a significant improvement over Ke\u2019s\nmethod, which is based on volumetric features. Note that a good performance,\naveraging over 80% in correct classifications, is achieved in distinguishing the\nsix actions in this challenging dataset.\nWe compared our results with other methods on this challenging dataset and\nwe summarize the correctly classified rates in Table 8. From this table, we can\nsee that MHH has made a significant improvement in comparison with MHI.\nFurthermore, the MGD feature gives a better performance than the MHH\nitself. The best performance, which gives significantly better classification re-\nsults, came from the combined feature, which is based on the histogram of the\nMHI and the MGD.\n18\nTable 7\nMGD & Hist. of MHI\u2019s confusion matrix, trace=481.9, mean performance=80.3%\nWalk Jog Run Box Clap Wave\nWalk 66.0 31.3 0.0 0.0 2.1 0.7\nJog 13.9 62.5 21.5 1.4 0.0 0.7\nRun 2.1 16.7 79.9 0.0 0.0 1.4\nBox 0.0 0.0 0.0 88.8 2.8 8.4\nClap 0.0 0.0 0.0 3.5 93.1 3.5\nWave 0.0 0.0 0.0 1.4 6.9 91.7\nTable 8\nOverall correctly classified rate (%) for all the methods on this open, challenging\ndataset.\nMethod Rate(%)\nSVM on local features (Schuldt et al. (2004))\u2217 71.7\nCascade of filters on volumetric features (Ke et al. (2005)) 63\nSVM on MHI (Meng et al. (2006b)) 63.5\nSVM 2K on MHI & MMHI (Meng et al. (2006a)) 65.3\nSVM on MHH S 69.6\nSVM on MGD 72.1\nSVM on HWT of MHI & Hist. of MHI (Meng et al. (2007b)) 70.9\nSVM on MGD & Hist. of MHI 80.3\nIt should be mentioned here that some performance results presented in the\nliterature (Dolla\u00b4r et al. (2005); Niebles et al. (2006); Yeo et al. (2006)) are\nquoted as higher than ours, when performing classification experiments on\n(parts of) this public dataset. However, we have not included these in Table\n8, because they are not directly comparable to ours, either because they have\nomitted to use all of the data, or because they have performed an easier\nclassification task, or both. For example, Dolla\u00b4r et al. (2005) achieved a correct\nclassification rate of 81.2%, but the evaluation omitted scenarios s2 and s4.\nOur earlier results, shown in Fig. 8, clearly indicate that scenario s2 is the most\ndifficult subset. This is also witnessed in the results of our best system (MGD\nand histogram of MHI), which give a mean performance of 80.4%, 63.5%,\n82.4% and 87% for the four scenarios s1, s2, s3 and s4 respectively. When we\ncombine data from the s1 and s3 subsets, as used by Dolla\u00b4r et al. (2005), and\nre-train our SVM classifiers on this combined data set, then we obtain a mean\nperformance of 82.8%, which is slightly better than the result of Dolla\u00b4r et al.\n(2005) (81.2%). Niebles et al. (2006) obtained similar results with 81.5% and\n19\nYeo et al. (2006) obtained 86.0%, but they did an easier task of classifying\neach complete sequence (containing 4 repetitions of same action) into one of\nsix classes, while our method was trained the same way as other papers(Ke\net al. (2005); Meng et al. (2006b,a, 2007b)), that is to detect a single instance\nof each action, within arbitrary sequences in the dataset. Furthermore, Yeo\net al. (2006) did not use the difficult subset 2 of the dataset, as was the case\nin Dolla\u00b4r et al. (2005).\n7 Conclusion and discussions\nIn this paper, we have proposed new, descriptive representations for human\naction recognition systems, which may easily be implemented in an embed-\nded computer vision context. The proposed method does not rely on accurate\ntracking as many other works do, since most tracking algorithms incur an\nextra computational cost for the system. Our system is based on simple \u2018tem-\nporal template\u2019 features in order to achieve high-speed recognition in real-time\nembedded applications.\nIn order to improve the state of the art performance in temporal template\nbased systems, we have proposed a new representation for motion informa-\ntion in video and this is called the Motion History Histogram (MHH). The\nrepresentation extends previous work on temporal template (MHI related) rep-\nresentations by additionally storing frequency information as the number of\ntimes motion is detected at every pixel, further categorized into the length of\neach motion. In essence, maintaining the number of contiguous motion frames\nremoves a significant limitation of MHI, which only encodes the time from the\nlast observed motion at every pixel. It can be used either independently, or\ncombined with information derived from the MHI, to give human action recog-\nnition systems with improved performance over existing comparable temporal\ntemplate based systems.\nIn terms of our work on combining features from MHI and MHH, we acknowl-\nedge that we have not generated a provably optimal feature combination.\nRather, we have selected two of the most promising feature types that we\nhave tested, that make explicit different aspects of the motion sequence, and\nwe have shown that the combined performance is better than either feature\nperformance alone. This suggests that further fruitful research would be to\nsearch for optimal combinations of temporal-template type features, for ex-\nample using a genetic algorithm, which is well suited to such a task.\nIn comparison with local SVM methods of Schuldt et al. (2004) and a cascade\nof filters on volumetric features of Ke et al. (2005), our feature vectors are\ncomputationally inexpensive. Even though we don\u2019t use a validation dataset\n20\nfor parameter tuning in SVM training, we have demonstrated a significant\nimprovement in recognition performance over other methods tested in a com-\nparable rigorous way on the large, public human action database (Schuldt\net al. (2004)).\nReferences\nAggarwal, J. K., Cai, Q., 1999. Human motion analysis: a review. Comput. Vis.\nImage Underst. 73 (3), 428\u2013440.\nBlank, M., Gorelick, L., Shechtman, E., Irani, M., Basri, R., 2005. Actions as space-\ntime shapes. In: ICCV. pp. 1395\u20131402.\nBobick, A. F., Davis, J. W., 2001. The recognition of human movement using tem-\nporal templates. IEEE Trans. Pattern Anal. Mach. Intell. 23 (3), 257\u2013267.\nBradski, G. R., Davis, J. W., 2002. Motion segmentation and pose recognition with\nmotion history gradients. Mach. Vis. Appl. 13 (3), 174\u2013184.\nCampbell, L. W., Bobick, A. F., 1995. Recognition of human body motion using\nphase space constraints. In: ICCV. pp. 624\u2013630.\nCristianini, N., Shawe-Taylor, J., 2000. An Introduction to Support Vector Ma-\nchines (and other kernel-based learning methods). Cambridge University Press,\nCambridge, UK.\nDalal, N., Triggs, B., Schmid, C., 2006. Human detection using oriented histograms\nof flow and appearance. In: ECCV (2). pp. 428\u2013441.\nDavis, J. W., 2001. Hierarchical motion history images for recognizing human mo-\ntion. In: IEEE Workshop on Detection and Recognition of Events in Video. pp.\n39\u201346.\nDavis, J. W., Tyagi, A., 2006. Minimal-latency human action recognition using\nreliable-inference. Image Vision Comput. 24 (5), 455\u2013472.\nDolla\u00b4r, P., Rabaud, V., Cottrell, G., Belongie, S., October 2005. Behavior recogni-\ntion via sparse spatio-temporal features. In: VS-PETS.\nEfros, A. A., Berg, A. C., Mori, G., Malik, J., 2003. Recognizing action at a distance.\nIn: ICCV. pp. 726\u2013733.\nFarmer, J., Casdagli, M., Eubank, S., Gibson, J., 1991. State-space reconstruction\nin the presence of noise. Physics D 51, 52\u201398.\nGao, J., Hauptmann, A. G., Bharucha, A., Wactlar, H. D., 2004. Dining activity\nanalysis using a hidden markov model. In: ICPR (2). pp. 915\u2013918.\nGreen, R. D., Guan, L., 2004. Quantifying and recognizing human movement pat-\nterns from monocular video images-part ii: applications to biometrics. IEEE\nTrans. Circuits Syst. Video Techn. 14 (2), 191\u2013198.\nHastie, T., Rosset, S., Tibshirani, R., Zhu, J., 2004. The entire regularization path\nfor the support vector machine.\nURL citeseer.ist.psu.edu\/hastie04entire.html\nJoachims, T., 1999. Making large-scale SVM learning practical. In: Advances in\nKernel Methods - Support Vector Learning. MIT-Press, USA, oikonomopoulos,\nAntonios and Patras, Ioannis and Pantic, Maja eds.\nURL http:\/\/svmlight.joachims.org\/\n21\nKe, Y., Sukthankar, R., Hebert., M., 2005. Efficient visual event detection using\nvolumetric features. In: ICCV. pp. 166\u2013173, beijing, China, Oct. 15-21, 2005.\nMeng, H., Freeman, M., Pears, N., Bailey, C., 2008. Real-time human action recog-\nnition on an embedded, reconfigurable video processing architecture. Journal of\nReal-Time Image Processing.\nMeng, H., Pears, N., Bailey, C., 2006a. Human action classification using SVM 2K\nclassifier on motion features. In: LNCS. Vol. 4105. Istanbul, Turkey, pp. 458\u2013465.\nMeng, H., Pears, N., Bailey, C., 2006b. Recognizing human actions based on mo-\ntion information and SVM. In: 2nd IET International Conference on Intelligent\nEnvironments. IET, Athens, Greece, pp. 239\u2013245.\nMeng, H., Pears, N., Bailey, C., 2007a. A human action recognition system for em-\nbedded computer vision application. In: CVPR workshop on Embeded Computer\nVision.\nMeng, H., Pears, N., Bailey, C., March 2007b. Motion information combination for\nfast human action recognition. In: 2nd International Conference on Computer\nVision Theory and Applications (VISAPP07). Barcelona, Spain.\nMoeslund, T., Hilton, A., Kruger, V., November 2006. A survey of advances in\nvision-based human motion capture and analysis. Comput. Vis. Image Underst.\n103 (2-3), 90\u2013126.\nNascimento, J. C., Figueiredo, M. A. T., Marques, J. S., 2005. Recognition of human\nactivities using space dependent switched dynamical systems. In: IEEE Int. Conf.\non Image Processing, ICIP.\nNiebles, J., Wang, H., Fei-Fei, L., 2006. Unsupervised learning of human action\ncategories using spatial-temporal words. In: BMVC06. p. III:1249.\nOgata, T., Tan, J. K., Ishikawa, S., 2006. High-speed human motion recognition\nbased on a motion history image and an eigenspace. IEICE Transactions on In-\nformation and Systems E89 (1), 281\u2013289.\nOikonomopoulos, A., Patras, I., Pantic, M., June 2006. Kernel-based recognition\nof human actions using spatiotemporal salient points. In: Proceedings of CVPR\nworkshop 06. Vol. 3. pp. 151\u2013156.\nURL http:\/\/pubs.doc.ic.ac.uk\/Pantic-CVPR06-1\/\nSchuldt, C., Laptev, I., Caputo, B., 2004. Recognizing human actions: a local SVM\napproach. In: ICPR. Cambridge, U.K.\nStauffer, C., Grimson, W. E. L., 2000. Learning patterns of activity using real-time\ntracking. IEEE Transactions on Pattern Analysis and Machine Intelligence 22 (8),\n747\u2013757.\nURL citeseer.ist.psu.edu\/stauffer00learning.html\nWeinland, D., Ronfard, R., Boyer, E., 2005. Motion history volumes for free view-\npoint action recognition. In: IEEE International Workshop on modeling People\nand Human Interaction (PHI\u201905).\nURL http:\/\/perception.inrialpes.fr\/Publications\/2005\/WRB05\nWong, S.-F., Cipolla, R., 2005. Real-time adaptive hand motion recognition using\na sparse bayesian classifier. In: ICCV-HCI. pp. 170\u2013179.\nWong, S.-F., Cipolla, R., 2006. Continuous gesture recognition using a sparse\nbayesian classifier. In: ICPR (1). pp. 1084\u20131087.\nYeo, C., Ahammad, P., Ramchandran, K., Sastry, S., 2006. Compressed domain\nreal-time action recognition. In: IEEE International Workshop on Multimedia\n22\nSignal Processing (MMSP) - 2006. IEEE, Washington, DC, USA.\n23\n"}