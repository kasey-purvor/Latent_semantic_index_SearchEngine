{"doi":"10.1016\/j.specom.2008.03.005","coreId":"9954","oai":"oai:www.era.lib.ed.ac.uk:1842\/3834","identifiers":["oai:www.era.lib.ed.ac.uk:1842\/3834","10.1016\/j.specom.2008.03.005"],"title":"A comparison of grapheme and phoneme-based units for Spanish spoken term detection","authors":["Tejedor, Javier","Wang, Dong","Frankel, Joe","King, Simon","Col\u00e1s, Jos\u00e9"],"enrichments":{"references":[{"id":19939227,"title":"A CTS task for meaningful fastturnaround experiments. In:","authors":[],"date":"2004","doi":null,"raw":"Chen, B., Cetin, O., Doddinton, G., Morgan, N., Ostendorf, M., Shinozaki, T., Zhu, Q., 2004. A CTS task for meaningful fastturnaround experiments. In: Proceedings of Rich Transcription Fall Workshop, Palisades, New York.","cites":null},{"id":19939248,"title":"A database for continuous speech recognition in a 1000 word domain. In:","authors":[],"date":"1998","doi":"10.1109\/icassp.1988.196669","raw":"Price, P.J., Fisher, W., Bernstein, J. 1998. A database for continuous speech recognition in a 1000 word domain. In: Proceedings of ICASSP, Vol. 1, pp. 651\u2013654.","cites":null},{"id":19939236,"title":"A fast lattice-based approach to vocabulary independent wordspotting. In:","authors":[],"date":"1994","doi":"10.1109\/icassp.1994.389277","raw":"James, D., Young, S., 1994. A fast lattice-based approach to vocabulary independent wordspotting. In: Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP-94), Vol. 1, pp. 465\u2013468.","cites":null},{"id":19939259,"title":"A hybrid word\/phoneme-based approach for improved vocabulary-independent search in spontaneous speech. In:","authors":[],"date":"2004","doi":"10.1109\/icassp.2004.1325970","raw":"Yu, P., Seide, F., 2004. A hybrid word\/phoneme-based approach for improved vocabulary-independent search in spontaneous speech. In: Proceedings of International Conference on Speech and Language Processing, pp. 635\u2013643.","cites":null},{"id":19939239,"title":"A keyword spotting approach based on pseudo N-gram language model. In:","authors":[],"date":"2004","doi":null,"raw":"Kim, J., Jung, H., Chung, H., 2004. A keyword spotting approach based on pseudo N-gram language model. In: Proceedings of SPECOM, pp. 156\u2013159.","cites":null},{"id":19939232,"title":"A study of phoneme and grapheme based context-dependent ASR systems. In:","authors":[],"date":"2007","doi":"10.1007\/978-3-540-78155-4_19","raw":"Dines, J., Doss, M.M., 2007. A study of phoneme and grapheme based context-dependent ASR systems. In: Proceedings of MLMI, Brno, Czech Republic.","cites":null},{"id":19939258,"title":"Acoustic indexing for multimedia retrieval and browsing. In:","authors":[],"date":"1997","doi":"10.1109\/icassp.1997.599600","raw":"Young, S., Brown, M., 1997. Acoustic indexing for multimedia retrieval and browsing. In: Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP-97), Vol. 1, pp. 199\u2013202.","cites":null},{"id":19939245,"title":"Albayzin speech database: design of the phonetic corpus. In:","authors":[],"date":"1993","doi":null,"raw":"Moreno, A., Poch, D., Bonafonte, A., Lleida, E., Llisterri, J., Marin \u02dco, J., Nadeu, C. 1993. Albayzin speech database: design of the phonetic corpus. In: Proceedings of Eurospeech, Vol. 1, pp. 653\u2013656.","cites":null},{"id":19939241,"title":"An experimental study of an audio indexing system for the web. In:","authors":[],"date":"2000","doi":null,"raw":"Logan, B., Moreno, P., Van Thong, J., Whittaker, E. 2000. An experimental study of an audio indexing system for the web. In: Proceedings of International Conference on Speech and Language Processing, Vol. 2, pp. 676\u2013679.","cites":null},{"id":19939254,"title":"Comparison of keyword spotting approaches for informal continuous speech. In:","authors":[],"date":"2005","doi":"10.1007\/11551874_39","raw":"Szoke, I., Schwarz, P., Matejka, P., Burget, L., Martin, K., Fapso, M., Cer- nocky, J., 2005. Comparison of keyword spotting approaches for informal continuous speech. In: Proceedings of Interspeech, Lisabon, Portugal, pp. 633\u2013636.","cites":null},{"id":19939250,"title":"Continuous hidden Markov modeling for speaker-independent word spotting. In:","authors":[],"date":"1989","doi":"10.1109\/icassp.1989.266505","raw":"Rohlicek, J., Russell, W., Roukos, S., Gish, H., 1989. Continuous hidden Markov modeling for speaker-independent word spotting. In: Proceedings of ICASSP, Vol. 1, pp. 627\u2013630, Glasgow, UK.","cites":null},{"id":19939252,"title":"Cross-language phonemisation in german text-to-speech synthesis. In:","authors":[],"date":"2003","doi":null,"raw":"Steigner, J., Schroder, M., 2003. Cross-language phonemisation in german text-to-speech synthesis. In: Proceedings of Interspeech.","cites":null},{"id":19939249,"title":"El comentario fonolo \u00b4gico y fone \u00b4tico de textos.","authors":[],"date":"1998","doi":null,"raw":"Quilis, A., 1998. El comentario fonolo \u00b4gico y fone \u00b4tico de textos. ARCO\/ LIBROS, S.A. Rohlicek, J., 1995. Modern methods of Speech Processing. Kluwer, Norwell MA.","cites":null},{"id":19939261,"title":"et al.\/Speech","authors":[],"date":"2008","doi":null,"raw":"J. Tejedor et al.\/Speech Communication 50 (2008) 980\u2013991 991","cites":null},{"id":19939251,"title":"Fast unconstrained audio search in numerous human languages. In:","authors":[],"date":"2007","doi":"10.1109\/icassp.2007.367167","raw":"Scott, J., Wintrode, J., Lee, M., 2007. Fast unconstrained audio search in numerous human languages. In: Proceedings of IEEE International 990 J. Tejedor et al.\/Speech Communication 50 (2008) 980\u2013991Conference on Acoustics, Speech, and Signal Processing (ICASSP-07), Honolulu, Hawai.","cites":null},{"id":19939225,"title":"Grama \u00b4tica de la lengua espan \u02dcola. Real Academia Espan \u02dcola. Coleccio \u00b4n Lebrija y Bello, Espasa Calpe.","authors":[],"date":"1995","doi":null,"raw":"Alarcos, E., 1995. Grama \u00b4tica de la lengua espan \u02dcola. Real Academia Espan \u02dcola. Coleccio \u00b4n Lebrija y Bello, Espasa Calpe.","cites":null},{"id":19939238,"title":"Grapheme based speech recognition. In:","authors":[],"date":"2003","doi":null,"raw":"Killer, M., Stuker, S., Schultz, T., 2003. Grapheme based speech recognition. In: Proceedings of Eurospeech.","cites":null},{"id":19939235,"title":"Indexing and search of multimodal information. In:","authors":[],"date":"1997","doi":"10.1109\/icassp.1997.599599","raw":"Hauptmann, A., Wactlar, H., 1997. Indexing and search of multimodal information. In: Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP-97), Vol. 1, pp. 195\u2013198.","cites":null},{"id":19939243,"title":"Joint decoding for phoneme-grapheme continuous speech recognition. In:","authors":[],"date":"2004","doi":"10.1109\/icassp.2004.1325951","raw":"Magiami-Doss, M., Bengio, S., Bourlard, H., 2004. Joint decoding for phoneme-grapheme continuous speech recognition. In: Proceedings of ICASSP, Montreal, Canada, pp. 177\u2013180.","cites":null},{"id":19939256,"title":"keywordspotting systembasedon \ufb01ller models, pseudo N-gram language model and a con\ufb01dence measure. In:","authors":[],"date":"2006","doi":null,"raw":"Tejedor,J., Cola \u00b4s, J., 2006.Spanish keywordspotting systembasedon \ufb01ller models, pseudo N-gram language model and a con\ufb01dence measure. In: Proceedings of IV Jornadas de Tecnolog\u0131 \u00b4a del Habla, pp. 255\u2013260.","cites":null},{"id":19939233,"title":"Lexical access to large vocabularies for speech recognition.","authors":[],"date":"1989","doi":"10.1109\/29.31268","raw":"Fissore, L., Laface, P., Micca, G., Pieraccini, R., 1989. Lexical access to large vocabularies for speech recognition. IEEE Trans. Acoust.","cites":null},{"id":19939230,"title":"Out-of-vocabulary word modeling and rejection for Spanish keyword spotting systems. In:","authors":[],"date":"2002","doi":"10.1007\/3-540-46016-0_17","raw":"Cuayahuitl, H., Serridge, B., 2002. Out-of-vocabulary word modeling and rejection for Spanish keyword spotting systems. In: Proceedings of MICAI, pp. 156\u2013165.","cites":null},{"id":19939240,"title":"Out-of-vocabulary word modelling and rejection for keyword spotting. In:","authors":[],"date":"1993","doi":null,"raw":"Lleida, E., Marino, J., Salavedra, J., Bonafonte, A., Monte, E., Mart\u0131 \u00b4nez, A. 1993. Out-of-vocabulary word modelling and rejection for keyword spotting. In: Proceedings of Eurospeech, pp. 1265\u20131268.","cites":null},{"id":19939242,"title":"Phoneme-grapheme based automatic speech recognition system. In:","authors":[],"date":"2003","doi":"10.1109\/asru.2003.1318410","raw":"Magiami-Doss, M., Stephenson, T.A., Bourlard, H., Bengio, S. 2003. Phoneme-grapheme based automatic speech recognition system. In: Proceedings of ASRU, pp. 94\u201398.","cites":null},{"id":19939257,"title":"Rapid yet accurate speech indexing using dynamic match lattice spotting.","authors":[],"date":"2007","doi":"10.1109\/tasl.2006.872615","raw":"Thambiratmann, K., Sridharan, S., 2007. Rapid yet accurate speech indexing using dynamic match lattice spotting. IEEE Trans. Audio Speech Process. 15 (1), 346\u2013357.","cites":null},{"id":19939244,"title":"Speech and language technologies for audio indexing and retrieval.","authors":[],"date":"2000","doi":"10.1109\/5.880087","raw":"Makhoul, J., Kubala, F., Leek, T., Liu, D., Nguyen, L., Schwartz, R., Srivastava, A., 2000. Speech and language technologies for audio indexing and retrieval. Proc. IEEE 88 (8), 1338\u20131353.","cites":null},{"id":19939255,"title":"Speech data retrieval system constructed on a universal phonetic code domain. In:","authors":[],"date":"2001","doi":"10.1109\/asru.2001.1034652","raw":"Tanaka, K., Itoh, Y., Kojima, H., Fujimura, N. 2001. Speech data retrieval system constructed on a universal phonetic code domain. In: Proceedings of IEEE Automatic Speech Recognition and Understanding, pp 323\u2013326.","cites":null},{"id":19939234,"title":"Speech\ufb01nd: advances in spoken document retrieval for a national gallery of the spoken word.","authors":[],"date":"2005","doi":"10.1109\/tsa.2005.852088","raw":"Hansen, J.R.H., Zhou, B., Seadle, M., Deller, J., Gurijala, A.R., Kurimo, M., Angkititrakul, P., 2005. Speech\ufb01nd: advances in spoken document retrieval for a national gallery of the spoken word. IEEE Trans. Acoust. Speech Signal Process. 13 (5), 712\u2013730.","cites":null},{"id":19939229,"title":"Telephone speech corpus development at CSLU. In:","authors":[],"date":"1994","doi":"10.3115\/1075812.1075821","raw":"Cole, R.A., Fanty, M., Noel, M., Lander, T., 1994. Telephone speech corpus development at CSLU. In: Proceedings of ICSLP, Yokohama, Japan, pp. 1815\u20131818.","cites":null},{"id":19939247,"title":"The spoken term detection (STD)","authors":[],"date":"2006","doi":null,"raw":"NIST (2006). The spoken term detection (STD) 2006 evaluation plan. National Institute of Standards and Technology, Gaithersburg, MD, USA, v10 ed. <http:\/\/www.nist.gov\/speech\/tests\/std>.","cites":null},{"id":19939260,"title":"Vocabulary independent indexing of spontaneous speech.","authors":[],"date":"2005","doi":"10.1109\/tsa.2005.851881","raw":"Yu, P., Chen, K., Ma, C., Seide, F., 2005. Vocabulary independent indexing of spontaneous speech. IEEE Trans. Acoust. Speech Signal Process. 13 (5), 635\u2013643.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-10-05T10:02:32Z","abstract":"The ever-increasing volume of audio data available online through the world wide web means that automatic methods for indexing and search are becoming essential. Hidden Markov model (HMM) keyword spotting and lattice search techniques are the two most common approaches used by such systems. In keyword spotting, models or templates are defined for each search term prior to accessing the speech and used to find matches. Lattice search (referred to as spoken term detection), uses a pre-indexing of speech data in terms of word or sub-word units, which can then quickly be searched for arbitrary terms without referring to the original audio. In both cases, the search term can be modelled in terms of sub-word units, typically phonemes. For in-vocabulary words (i.e. words that appear in the pronunciation dictionary), the letter-to-sound conversion systems are accepted to work well. However, for out-of-vocabulary (OOV) search terms, letter-to-sound conversion must be used to generate a pronunciation for the search term. This is usually a hard decision (i.e. not probabilistic and with no possibility of backtracking), and errors introduced at this step are difficult to recover from. We therefore propose the direct use of graphemes (i.e., letter-based sub-word units) for acoustic modelling. This is expected to work particularly well in languages such as Spanish, where despite the letter-to-sound mapping being very regular, the correspondence is not one-to-one, and there will be benefits from avoiding hard decisions at early stages of processing. In this article, we compare three approaches for Spanish keyword spotting or spoken term detection, and within each of these we compare acoustic modelling based on phone and grapheme units. Experiments were performed using the Spanish geographical-domain Albayzin corpus. Results achieved in the two approaches proposed for spoken term detection show us that trigrapheme units for acoustic modelling match or exceed the performance of phone-based acoustic models. In the method proposed for keyword spotting, the results achieved with each acoustic model are very similar","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/9954.pdf","fullTextIdentifier":"http:\/\/homepages.inf.ed.ac.uk\/dwang2\/public\/pdf\/STD_spanish_graphemes.a.pdf","pdfHashValue":"326c97c59e2827cec3e7fb41d37e2afea1b54639","publisher":"Elsevier","rawRecordXml":"<record><header><identifier>\n        \n            \n                oai:www.era.lib.ed.ac.uk:1842\/3834<\/identifier><datestamp>\n                2010-10-05T12:43:15Z<\/datestamp><setSpec>\n                com_1842_905<\/setSpec><setSpec>\n                com_1842_1255<\/setSpec><setSpec>\n                com_1842_154<\/setSpec><setSpec>\n                col_1842_906<\/setSpec><setSpec>\n                col_1842_3763<\/setSpec>\n            <\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:doc=\"http:\/\/www.lyncode.com\/xoai\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n            \nA comparison of grapheme and phoneme-based units for Spanish spoken term detection<\/dc:title><dc:creator>\nTejedor, Javier<\/dc:creator><dc:creator>\nWang, Dong<\/dc:creator><dc:creator>\nFrankel, Joe<\/dc:creator><dc:creator>\nKing, Simon<\/dc:creator><dc:creator>\nCol\u00e1s, Jos\u00e9<\/dc:creator><dc:description>\nThe ever-increasing volume of audio data available online through the world wide web means that automatic methods for indexing and search are becoming essential. Hidden Markov model (HMM) keyword spotting and lattice search techniques are the two most common approaches used by such systems. In keyword spotting, models or templates are defined for each search term prior to accessing the speech and used to find matches. Lattice search (referred to as spoken term detection), uses a pre-indexing of speech data in terms of word or sub-word units, which can then quickly be searched for arbitrary terms without referring to the original audio. In both cases, the search term can be modelled in terms of sub-word units, typically phonemes. For in-vocabulary words (i.e. words that appear in the pronunciation dictionary), the letter-to-sound conversion systems are accepted to work well. However, for out-of-vocabulary (OOV) search terms, letter-to-sound conversion must be used to generate a pronunciation for the search term. This is usually a hard decision (i.e. not probabilistic and with no possibility of backtracking), and errors introduced at this step are difficult to recover from. We therefore propose the direct use of graphemes (i.e., letter-based sub-word units) for acoustic modelling. This is expected to work particularly well in languages such as Spanish, where despite the letter-to-sound mapping being very regular, the correspondence is not one-to-one, and there will be benefits from avoiding hard decisions at early stages of processing. In this article, we compare three approaches for Spanish keyword spotting or spoken term detection, and within each of these we compare acoustic modelling based on phone and grapheme units. Experiments were performed using the Spanish geographical-domain Albayzin corpus. Results achieved in the two approaches proposed for spoken term detection show us that trigrapheme units for acoustic modelling match or exceed the performance of phone-based acoustic models. In the method proposed for keyword spotting, the results achieved with each acoustic model are very similar.<\/dc:description><dc:date>\n2010-10-05T10:02:31Z<\/dc:date><dc:date>\n2010-10-05T10:02:31Z<\/dc:date><dc:date>\n2008<\/dc:date><dc:date>\n2010-10-05T10:02:32Z<\/dc:date><dc:type>\nArticle<\/dc:type><dc:identifier>\nhttp:\/\/homepages.inf.ed.ac.uk\/dwang2\/public\/pdf\/STD_spanish_graphemes.a.pdf<\/dc:identifier><dc:identifier>\nhttp:\/\/hdl.handle.net\/1842\/3834<\/dc:identifier><dc:identifier>\ndoi:10.1016\/j.specom.2008.03.005<\/dc:identifier><dc:publisher>\nElsevier<\/dc:publisher>\n<\/oai_dc:dc>\n<\/metadata>\n        <\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2010,"topics":[],"subject":["Article"],"fullText":"A comparison of grapheme and phoneme-based units\nfor Spanish spoken term detection\nJavier Tejedor\na,b,*, Dong Wang\nb, Joe Frankel\nb, Simon King\nb, Jose \u00b4 Cola \u00b4s\na\naHuman Computer Technology Laboratory, Escuela Polite \u00b4cnica Superior UAM Avenue Francisco Toma \u00b4s y Valiente 11, 28049, Spain\nbCentre for Speech Technology Research, University of Edinburgh 2 Buccleuch Place, Edinburgh EH8 9LW, United Kingdom\nReceived 31 May 2007; received in revised form 19 February 2008; accepted 17 March 2008\nAbstract\nThe ever-increasing volume of audio data available online through the world wide web means that automatic methods for indexing\nand search are becoming essential. Hidden Markov model (HMM) keyword spotting and lattice search techniques are the two most com-\nmon approaches used by such systems. In keyword spotting, models or templates are de\ufb01ned for each search term prior to accessing the\nspeech and used to \ufb01nd matches. Lattice search (referred to as spoken term detection), uses a pre-indexing of speech data in terms of\nword or sub-word units, which can then quickly be searched for arbitrary terms without referring to the original audio.\nIn both cases, the search term can be modelled in terms of sub-word units, typically phonemes. For in-vocabulary words (i.e. words\nthat appear in the pronunciation dictionary), the letter-to-sound conversion systems are accepted to work well. However, for out-of-\nvocabulary (OOV) search terms, letter-to-sound conversion must be used to generate a pronunciation for the search term. This is usually\na hard decision (i.e. not probabilistic and with no possibility of backtracking), and errors introduced at this step are di\ufb03cult to recover\nfrom. We therefore propose the direct use of graphemes (i.e., letter-based sub-word units) for acoustic modelling. This is expected to\nwork particularly well in languages such as Spanish, where despite the letter-to-sound mapping being very regular, the correspondence\nis not one-to-one, and there will be bene\ufb01ts from avoiding hard decisions at early stages of processing.\nIn this article, we compare three approaches for Spanish keyword spotting or spoken term detection, and within each of these we\ncompare acoustic modelling based on phone and grapheme units. Experiments were performed using the Spanish geographical-domain\nALBAYZIN corpus. Results achieved in the two approaches proposed for spoken term detection show us that trigrapheme units for acous-\ntic modelling match or exceed the performance of phone-based acoustic models. In the method proposed for keyword spotting, the\nresults achieved with each acoustic model are very similar.\n\u0002 2008 Elsevier B.V. All rights reserved.\nKeywords: Spoken term detection; Keyword spotting; Graphemes; Spanish\n1. Introduction and motivation\nThe increasing amount of speech and multimedia data\nstored electronically has motivated the development of\ntechnologies that can provide automatic search for data\nmining and information retrieval. These technologies have\ndeveloped alongside large vocabulary continuous speech\nrecognition (LVCSR) and use many of the same\ntechniques.\n1.1. Keyword spotting and spoken term detection\nWe can broadly divide audio search approaches into\nkeyword spotting (KS), and lattice-based methods, which\nhave become known as spoken term detection (STD). For\n0167-6393\/$ - see front matter \u0002 2008 Elsevier B.V. All rights reserved.\ndoi:10.1016\/j.specom.2008.03.005\n* Corresponding author. Address: Human Computer Technology Lab-\noratory, Escuela Polite \u00b4cnica Superior UAM Avenue Francisco Toma \u00b4sy\nValiente 11, 28049, Spain.\nE-mail addresses: javier.tejedor@uam.es (J. Tejedor), dwang2@inf.e-\nd.ac.uk (D. Wang), joe@cstr.ed.ac.uk (J. Frankel), Simon.King@ed.ac.uk\n(S. King), jose.colas@uam.es (J. Cola \u00b4s).\nwww.elsevier.com\/locate\/specom\nAvailable online at www.sciencedirect.com\nSpeech Communication 50 (2008) 980\u2013991keyword spotting, the terms are de\ufb01ned in advance, and\nthen models or templates representing each term are used\nto \ufb01nd matches. The National Institute for Standards\nand Technology (NIST) introduced an STD evaluation in\n2006. In doing so, they de\ufb01ned the task of spoken term\ndetection as being a two-stage process in which the audio\nis \ufb01rst indexed according to word or sub-word units (e.g.\nphones), and then search is performed over the indexed\naudio. The indexing may be as a 1-best string, or as an\nN-best lattice.\nSpanish keyword spotting systems generally use hidden\nMarkov models (HMMs) of phone-based sub-word units\n(Lleida et al., 1993; Cuayahuitl and Serridge, 2002; Tejedor\nand Cola \u00b4s, 2006; Scott et al., 2007). In some cases, \ufb01ller\nmodels are incorporated which represent the non-keywords\nin the speech (Lleida et al., 1993; Cuayahuitl and Serridge,\n2002).\nLattice-based methods o\ufb00er signi\ufb01cantly faster search,\nas the speech is processed just once by the HMMs. A num-\nber of authors have taken the approach of searching for\nterms in the output of an LVCSR system (Hauptmann\nand Wactlar, 1997; Logan et al., 2000; Makhoul et al.,\n2000; Hansen et al., 2005), though a common \ufb01nding is\nthat these approaches yield high miss rates (i.e., low recall)\n(James and Young, 1994; Young and Brown, 1997; Tanaka\net al., 2001; Yu et al., 2005). Hybrid methods based on the\ncombination of keyword-spotting (which gives high recall)\nand sub-word lattice search have proven successful in com-\nbining the strengths of both methods (Yu and Seide, 2004;\nTejedor and Cola \u00b4s, 2006).\nIn this paper, we present three approaches. Two of\nthem are capable of spoken term detection, as they\nindex the speech in terms of sub-word units. The other\narchitecture can only perform keyword spotting, process-\ning the audio using a recognition network composed of\nword models (of the keywords) and \ufb01ller (garbage)\nmodels.\nOne of the problems which spoken-term detection must\novercome is dealing with out-of-vocabulary (OOV) search\nterms, where we de\ufb01ne OOV words to be those which do\nnot appear in the pronunciation lexicon. This is important,\nas the OOV rate for STD in applications such as multilan-\nguage surveillance, technical document database searching\nand news-story indexing tends to be higher than for tran-\nscription tasks due to a bias toward proper nouns and\nacronyms as search terms (Thambiratmann and Sridharan,\n2007).\nSearch within a word-based lattice is vocabulary-depen-\ndent, as only terms which appear in the LVCSR lexicon can\never appear in the output. Therefore it is common to build\nlattices and employ search over sub-word units in these\ncases. Similar ideas have been applied to open vocabulary\nkeyword spotting methods, for example HMM-based\nmethods with word models composed of sub-word units.\nHowever, these methods are at the cost of considerably\nslower query speed, as the speech must be re-searched for\neach new search term (Rohlicek, 1995).\nIn both cases, the search term is modelled in terms of\nsub-word units, typically phonemes, and for OOV search\nterms, letter-to-sound conversion must be used to generate\na pronunciation for the search term. This is usually a non-\nprobabilistic issue and a di\ufb03cult decision and errors intro-\nduced at this step are di\ufb03cult to recover from. We there-\nfore propose the direct use of graphemes (i.e., letter-\nbased sub-word units) for acoustic modelling.\nRather than enforcing a potentially hard decision on the\nsequence of phone units, the relationship between gra-\nphemes and sounds will then be modelled probabilistically\nby the acoustic models (HMMs) themselves, rather than by\nan external letter-to-sound model (such as a classi\ufb01cation\ntree, commonly used in text-to-speech synthesis).\nThis is expected to work particularly well in languages\nsuch as Spanish, where the letter-to-sound mapping is very\nregular. Whilst this regularity means that letter-to-sound\nconversion can be achieved more reliably than for some\nother languages (for example English), by modelling graph-\neme-based units directly we have the advantage of replac-\ning a potentially error-prone hard decision with a\nprobabilistic one which naturally accounts for this\nvariation.\n1.2. Grapheme-based automatic speech recognition (ASR)\nKiller et al. (2003) demonstrated that grapheme-based\nLVCSR systems for Spanish can achieve performance\nwhich is close to that of phone-based systems. In some\nother languages \u2013 notably English, the speech sounds are\nharder to predict accurately from the graphemes, so graph-\neme-based units typically perform worse than phone-based\nunits for acoustic modelling (Killer et al., 2003).\nHowever, Dines and Doss (2007) show that the use of\ngraphemes in English can yield competitive performance\nfor small to medium vocabulary tasks in automatic speech\nrecognition (ASR) systems. In experiments on the OGI\nNumbers95 task (Cole et al., 1994), a grapheme-based\nASR system was found to give similar performance to\nthe phone-based approach. However, on tasks of increased\ncomplexity, such as DARPA resource management (Price\net al., 1998) and continuous telephone speech (Chen\net al., 2004), the phone-based system gave lower error rates\nthan the grapheme system.\nDoss Magiami-Doss et al. (2003, 2004) also proposed\nthe use of a phone-grapheme based system that jointly\nmodels both the phone and grapheme sub-word units dur-\ning training. During decoding, recognition is performed\neither using one or both sub-word units. This was investi-\ngated in the framework of a hybrid hidden Markov\nmodel\/arti\ufb01cial neural network (HMM\/ANN) system.\nImprovements were obtained over a context-independent\nphone-based system using both sub-word units in recogni-\ntion in two di\ufb00erent tasks: isolated word recognition task\n(Magiami-Doss et al., 2003) and recognition of numbers\ntask (Magiami-Doss et al., 2004).\nJ. Tejedor et al.\/Speech Communication 50 (2008) 980\u2013991 9811.3. Motivation and organization of this paper\nGiven the performance of grapheme-based models for\nSpanish LVCSR and the potential advantages of grapheme\nover phone-based units for tasks involving OOVs, we pro-\npose that grapheme-based acoustic modelling can outper-\nform phone-based modelling for certain applications.\nIn this work, we compare grapheme-based sub-word\nunits (monographeme and trigrapheme models) with con-\nventional phone-based units (monophone and triphone\nmodels) for acoustic modelling using HMMs, in three dif-\nferent architectures for keyword spotting and spoken term\ndetection. Sections 2 and 3 de\ufb01ne the database and acoustic\nmodel con\ufb01gurations used. The three architectures are\ndescribed in Section 4. Section 5 de\ufb01nes the evaluation met-\nrics used, experimental results are presented in Sections 6\nand 7 concludes and suggests future work.\nThe novel aspects of this work are in the application of\ngrapheme-based units to acoustic modelling in keyword\nspotting and spoken term detection, and the con\ufb01dence\nmeasures introduced in the architectures presented in Sec-\ntions 4.1 and 4.3.\n2. The ALBAYZIN database\nThe experiments were performed using the Spanish geo-\ngraphical-domain ALBAYZIN corpus (Moreno et al., 1993)\nwhich contains utterances that incorporate the names of\nmountains, rivers, cities, etc. ALBAYZIN contains two sepa-\nrate sub-corpora: a phonetically rich component and a geo-\ngraphic corpus. Each of these is divided into training and\ntest sets. We used these 4 distinct, non-overlapping por-\ntions of the data as described by Table 1.\nThefoursetsareusedasfollows:Thephonetictrainingset\nwas used to train the acoustic models along with phone and\ngrapheme bigram language models. The STD development\nsetwasusedtotrainthelexicalaccessmoduleinarchitectures\n1and3,andtunethelanguagemodelscaleandinsertionpen-\nalty for the sub-word unit decoder in all three architectures.\nThephonetictestsetwasusedtodecidethenumberofGauss-\nianmixturecomponentsforalltypesofacousticmodels,and\nthe STD test set was used for \ufb01nal evaluation.\n3. HMM-based acoustic modelling\nThe input signal is sampled at 16 kHz and stored with 16\nbit precision. Mel-frequency cepstral coe\ufb03cients were com-\nputed at 10ms intervals within 25 ms Hamming windows.\nEnergy and \ufb01rst and second order derivatives were\nappended giving a series of 39-dimensional feature vectors.\nThe HTK v3.4 (Young et al., 2006) toolkit was used for\nthe feature extraction, acoustic modelling, and decoding\ndescribed in this paper.\n3.1. Phone models\nAn inventory of 47 allophones of Spanish (Quilis, 1998)\nwas used (as given in Appendix A), along with beginning\nand end of utterance silence models to build context-inde-\npendent (monophone) and context-dependent (triphone)\nsystems. All allophone and silence models had a conven-\ntional 3-state, left-to-right topology and there was an addi-\ntional short pause model which had a single emitting state\nand a skip transition.\nThe output distributions for the monophone system\nconsisted of 15-component Gaussian mixture models\n(GMM), and those in the triphone system used 11 compo-\nnents. In both cases, the number of mixture components\nwere chosen empirically based on phone accuracy on the\nphonetic test set. The triphone models were cross-word\nand were state-clustered using HTK\u2019s standard decision\ntree method with phonetically-motivated questions, which\nleads to 5632 shared states.\n3.2. A grapheme inventory for Spanish\nAlthough there is a simple relationship between spelling\nand sound in Spanish, care must be taken in de\ufb01ning the\ninventory of graphemes (Alarcos, 1995). We will use the\nterm \u2018\u2018grapheme\u201d to mean a single unit, which is a sequence\nof one or more letters, to be used for acoustic modelling.\nThis may not be precisely match the alphabet used for writ-\ning because we can expect better performance if we account\nfor a small number of language-speci\ufb01c special cases.\nThe letter \u2018\u2018h\u201d only a\ufb00ects the phonetic realisation when\nit appears in the combination \u2018\u2018ch\u201d, as in \u2018\u2018chaqueta\u201d\n(\u2018\u2018jacket\u201d) or \u2018\u2018Pancho\u201d (a proper name). \u2018\u2018ch\u201d is always\npronounced [tS]. Therefore \u2018\u2018ch\u201d is considered to be a\ngrapheme (digrapheme in this case) and the letter \u2018\u2018h\u201d\ncan be removed everywhere else. The only exceptions are\nin loanwords, such as \u2018\u2018Sa \u00b4hara\u201d (borrowed from Arabic)\nor \u2018\u2018hall\u201d (borrowed from English) where the \u2018\u2018h\u201d is pro-\nnounced somewhere along a [h]\u2013[v] continuum, depending\non the speaker. In the work presented here, we ignored the\nTable 1\nSpeci\ufb01cation of the training, development and testing sets for the ALBAYZIN database\nPhonetic corpus (orthographically transcribed and phonetically labelled) Geographic corpus (orthographically transcribed)\nTrain set NAME: Phonetic training set NAME: STD development set\nCONTAINS: 4800 phonetically balanced sentences from 164 speakers: 3 h and\n20 min\nCONTAINS: 4400 sentences from 88 speakers: 3 h and\n40 min\nTest set NAME: Phonetic test set NAME: STD test set\nCONTAINS: 2000 phonetically balanced sentences from 40 speakers: 1 h and\n40 min\nCONTAINS: 2400 sentences from 48 speakers: 2 h\n982 J. Tejedor et al.\/Speech Communication 50 (2008) 980\u2013991pronunciation of \u2018\u2018h\u201d in loanwords, because the corpus\nused for experimentation contains no loanwords.\nThe combination\u2018\u2018ll\u201d is pronounced [F] or [y], depend-\ning on context, and so is also considered a grapheme\n(digrapheme in this case) because its pronunciation is not\nrelated to that of its constituent letters. \u2018\u2018~ n\u201d is also consid-\nered a grapheme for the same reason (it is not an \u2018\u2018n\u201d plus a\n\u2018\u2018~\u201d). It is always pronounced [\u203a].\nThere are therefore a total of 28 grapheme units in our\nsystems: a, b, c, ch, d, e, f, g, i, j, k, l, ll, m, n, ~ n, o, p, q,\nr, s, t, u, v, w, x, y and z.\nThere are, of course, other letter combinations that\ncould be considered as single graphemes, such as \u2018\u2018rr\u201d,\nbut a balance must be struck between capturing these spe-\ncial cases of letter-to-sound relationships, and keeping the\ngrapheme inventory size small for statistical modelling rea-\nsons. The ideal grapheme inventory, even for a phoneti-\ncally simple language like Spanish, is not easily de\ufb01ned.\nThis is a challenge for grapheme-based modelling, and in\nfuture work we will consider automatic methods for avoid-\ning a sub-optimal manual choice of grapheme inventory.\n3.3. Grapheme models\nThe grapheme systems were built in an identical fashion\nto the phone-based systems; the only di\ufb00erences were in the\ninventory of sub-word units (Section 3.2) and the questions\nused for state clustering. The monographeme models used\nmixtures of Gaussians with 15 components, and the trigra-\npheme models used eight components. As with the phone\nmodels, these numbers were chosen empirically based on\ngrapheme accuracy on the phonetic test set. There are\n3575 shared states retained after clustering in the trigra-\npheme system.\nTo build state-tied context-dependent grapheme models\n(trigraphemes) requires a set of questions used to con-\nstruct the decision tree. There are three ways to generate\nthose questions: using only questions about single gra-\nphemes (\u2018\u2018singleton questions\u201d), converting from the ques-\ntions used to state-tie triphones (Section 3.1) according to\na phone-to-grapheme map, or generating questions from\ndata automatically. Killer and colleagues (Killer et al.,\n2003) reported that singleton questions give the best per-\nformance, so we used a singleton question set for state\ntying in our experiments.\n4. Three architectures for keyword spotting and spoken term\ndetection\nIn this work we compare three di\ufb00erent architectures:\n(1) Viterbi decoding is used to give the single most likely\nsequence of sub-word (phone\/grapheme) units. The\nkeyword is speci\ufb01ed in terms of sub-word units, and\na lexical access module is used to \ufb01nd exact or near\nmatches in the decoded output.\n(2) Viterbi decoding is used to produce an N-best sub-\nword (phone\/grapheme) lattice. An exact word-\nmatching procedure is applied to the lattice with the\nkeyword speci\ufb01ed in terms of sub-word units.\n(3) Hybrid system which combines a conventional key-\nword spotting system composed of keywords and \ufb01l-\nler models which account for the non-keywords, with\na sub-word decoder and lexical access module as in\n(1) in order to reduce the false alarm rate.\nThe three architectures (1)\u2013(3) are described below in\nSections 4.1, 4.2 and 4.3 respectively.\n4.1. Architecture 1: 1-best sub-word unit decoding + lexical\naccess\nThis architecture is illustrated in Fig. 1. The \ufb01rst pro-\ncessing step uses the HTK tool HVite to produce the sin-\ngle most likely (1-best) sequence of phones or graphemes,\nusing the HMM sets trained as described in Sections 3.1\nand 3.3 respectively. We refer to this as the sub-word unit\ndecoder, and the output is a sequence of U phone or graph-\neme sub-word units S \u00bcf s1;s2;...;sUg.\nDecoding incorporates a phone or grapheme bigram\nlanguage model (LM) which was trained on the phonetic\nor grapheme transcription of the phonetic training set,\nrespectively Fig. 2.\nThese sequences are then passed to the lexical access\nmodule which we describe in detail in the following\nsections.\nsub-word unit\ndecoder\nlexical access\nmodule\nspoken\nterms speech sub-word\nunit string\nFig. 1. The fast 1-best architecture.\nsub-word unit\ndecoder\nexact match\n+ threshold\nspoken\nterms speech sub-word\nunit lattice\nFig. 2. The sub-word lattice architecture.\nJ. Tejedor et al.\/Speech Communication 50 (2008) 980\u2013991 9834.1.1. Lexical access module \u2013 training the alignment costs\nEach keyword W is represented as a sequence of R\nphone or grapheme sub-word units W \u00bcf w1;w2;...;wRg,\nand search is performed within S, the output of the sub-\nword unit decoder. This approach is based on the dynamic\nprogramming algorithm proposed by Fissore et al. (1989).\nThe essence of the algorithm is to compute the cost of\nmatching each keyword W with the decoded output S.\nThe total cost is accumulated from the costs of four types\nof alignment error: substitution, insertion, deletion, and\ncontinuation. The \ufb01rst three of these are standard in\nASR decoding, and \u2018continuation\u2019 (Fissore et al., 1989)i s\nincluded in order to distinguish an insertion error from,\nfor example, hypothesising fbaag during a time interval\nin which the true phone sequence was fbag.\nDi\ufb00erent costs are associated with each type of align-\nment error and are estimated as follows:\nCsub\u00f0h;k\u00de\u00bc log\nNsub\u00f0h;k\u00de\nNtot\u00f0h\u00de\n\u00f01\u00de\nCins\u00f0h;k\u00de\u00bc log\nNins\u00f0h;k\u00de\nNtot\u00f0h\u00de\n\u00f02\u00de\nCdel\u00f0h\u00de\u00bc log\nNdel\u00f0h\u00de\nNtot\u00f0h\u00de\n\u00f03\u00de\nCcon\u00f0h;k\u00de\u00bc log\nNcon\u00f0h;k\u00de\nNtot\u00f0h\u00de\n\u00f04\u00de\nwhere we de\ufb01ne:\nNsub\u00f0h;k\u00de is the total substitutions of test symbol k for\nreference symbol h; Nins\u00f0h;k\u00de is the total insertions of test\nsymbol k after reference symbol h; Ndel\u00f0h\u00de is the total dele-\ntions of reference symbol h; Ncon\u00f0h;k\u00de is the total continu-\nations of test symbol k after h and Ntot\u00f0h\u00de is the total\noccurrences of reference symbol h, is given by\nNtot\u00f0h\u00de\u00bc\nX\nk\nNsub\u00f0h;k\u00de\u00feNins\u00f0h;k\u00de\u00feNcon\u00f0h;k\u00de \u00bd  \u00fe Ndel\u00f0h\u00de\n\u00f05\u00de\nThe costs are estimated by \ufb01rst producing 1-best hypothe-\nses for the training data using the sub-word unit decoder,\nand evaluating Eqs. (1)\u2013(4) against the reference transcript.\nIn order to develop a vocabulary-independent system, the\nfull vocabulary of the STD development set was used. This\nmeans that many of the training keywords would be unli-\nkely to appear in a practical application, though yields a\nmore general system.\n4.1.2. Lexical access module \u2013 \ufb01nding matches\nDynamic programming is used to calculate the overall\ncost of matching each keyword W against the hypothesised\nsequence S. Letting r and u be indices for the position\nwithin W and S, respectively, the local cost function\nG\u00f0r;u\u00de is calculated in recursively as:\nG\u00f0r;u\u00de\u00bc\nG\u00f0r   1;u   1\u00de\u00feCsub\u00f0wr;su\u00de\nG\u00f0r;u   1\u00de\u00feCins=con\u00f0wr;su\u00de\nG\u00f0r   1;u\u00de\u00feCdel\u00f0wr;su\u00de\n             \n\u00f06\u00de\nwhere\nCins=con\u00f0w\nr;s\nu\u00de\u00bc\nCins\u00f0wr;su\u00de if su \u00bc su 1\nCcon\u00f0wr;su\u00de otherwise\n        \u00f07\u00de\nThe keyword search over a length L hypothesised sequence\nof sub-word units progresses as follows:\n(1) For each keyword K, set the minimum window length\nto W\nmin\nK \u00bc NK=2 \u00fe 1, where NK is the number of sub-\nword units contained in the dictionary entry for key-\nword K. Set the maximum window length as\nW\nmax\nK \u00bc W\nmin\nK \u00fe NK.\n(2) Calculate the cost G each keyword K over each can-\ndidate window.\n(3) Sort keyword hypotheses according to G, removing\nany for which the cost G is greater than a threshold\nHGmax.\n(4) Remove overlapping keyword hypotheses: make a\npass through the sorted keyword hypotheses starting\nwith the highest-ranked keyword, removing all\nhypotheses with time-overlap greater than Hoverlap%.\n(5) Return all keyword hypotheses with cost less than\nGbest \u00fe HGbeam, where Gbest refers to the cost of the\nhighest-ranked keyword and HGbeam is beam width.\nThe thresholds HGmax, Hoverlap, and HGbeam, and the win-\ndow sizes W\nmin\nK and W\nmax\nK , are set on STD development set\nin order to give the desired trade-o\ufb00 of precision and recall.\nAs an example of the windowing in the grapheme-based\napproach, searching for the keyword madrid, which has a\ngrapheme transcription {madrid }, given a grapheme\ndecoder output of {maidriedaan }, the minimum\nand maximum windows are W\nmin\nK \u00bc 6=2 \u00fe 1 \u00bc 4a n d\nW\nmax\nK \u00bc 4 \u00fe 6 \u00bc 10. The cost G is therefore accumulated\nover the following candidate windows:\n{maid }, {maidr }, {maidri }, {maidrie },\n{maidried }, {maidrieda }, {maidried\naa }, {aidr }, {aidri }, ..., {ieda }\n4.2. Architecture 2: sub-word unit lattice + exact word\nmatching\nLattice search provides a natural extension to the 1-best\npath architecture above, and again search is based on sub-\nword (phone or grapheme) units.\nThe decoding process for the 1-best decoder from Sec-\ntion 4.1 was used, except that HVite was run in N-best\nmode. The resulting output were lattices generated from\nthe top N tokens in each state. An example grapheme lat-\ntice is shown in Fig. 3.\n4.2.1. Exact word matching in the lattice\nThe Viterbi algorithm provides an e\ufb03cient method to\n\ufb01nd all path fragments in the lattice that exactly match\nthe phone or grapheme string representing search terms.\n984 J. Tejedor et al.\/Speech Communication 50 (2008) 980\u2013991We use an implementation provided by collaborators at the\nBrno University of Technology (Szoke et al., 2005). Preli-\nminary work con\ufb01rmed the \ufb01nding of Szoke et al. (2005),\nthat given a suitably dense lattice, the accuracy improve-\nment from allowing non-exact matches was minimal, and\nthat N \u00bc 5 gave a suitably dense lattice. In the work\nreported here we set N \u00bc 5 and only consider exact\nmatches.\nFor each hypothesised keyword K which the search\nreturns, a con\ufb01dence score CK is calculated as follows:\nCK \u00bc La\u00f0K\u00de\u00feL\u00f0K\u00de\u00feLb\u00f0K\u00de Lbest \u00f08\u00de\nwhere:\n  La\u00f0K\u00de is the log likelihood of the best path from the lat-\ntice start to the node of the \ufb01rst phone or grapheme of\nK.\n  L\u00f0K\u00de is the log likelihood of keyword K, computed as\nthe sum of the acoustic log likelihood of its constituent\nphones or graphemes, plus the total language model log\nlikelihood for the sequence (weighted by the language\nmodel scale factor).\n  Lb\u00f0K\u00de is the log likelihood of the best path from the last\nnode of the last phone or grapheme of K to the end of\nthe lattice.\n  Lbest is the likelihood of the 1-best path over the com-\nplete lattice.\nLa\u00f0K\u00de and Lb\u00f0K\u00de are computed using standard forward-\nbackward recursions (Young et al., 2006). A threshold on\nthe con\ufb01dence score is set on the STD development set in\norder to reduce the false alarm rate and give the desired\nsystem performance.\n4.3. Architecture 3: hybrid word + sub-word system\nStandard word + \ufb01ller HMM-based keyword spotting\nas outlined in Section 1.1 above tends to give high hit rates\n(recall). In the third hybrid architecture, we propose com-\nbining such a system with a sub-word decoder method in\norder to reduce the false alarms, and so increase the preci-\nsion. The hybrid architecture is shown in Fig. 4.\nThe sub-word unit decoder is the same system as\ndescribed above in Section 4.1: decoding takes a phone\nor grapheme bigram and produces the most likely sequence\nof phone or grapheme units.\nThe keyword spotting module uses the same set of acous-\ntic models, though the bigram language model is replaced\nby a recognition network composed of words and \ufb01llers\nas shown in Fig. 5.\nAny transition between keywords and \ufb01ller models is\nallowed as well as self transitions for both keywords and\n\ufb01llers. This con\ufb01guration allows multiple keywords to\nappear in a single utterance and multiple instances of the\nM\nA\nD I\nR\nI\nE\nA\nD\n<s>\nT\nN E\nR\n<\/s>\nFig. 3. Illustration of a grapheme lattice.\nsub-word unit\ndecoder\nlexical access\nmodule\nkeywords\nspeech\nsub-word\nunit string\nsubstring\nselection\nselected\nstrings\nkeyword spotting\nmodule\nconfidence\nmeasure keywords\nFig. 4. The hybrid architecture.\nJ. Tejedor et al.\/Speech Communication 50 (2008) 980\u2013991 985same keyword in the same utterance. The keyword HMMs\nare constructed as concatenations of phone or grapheme\nHMMs, so no additional training is required. A pseudo\nN-gram language model, similar to the one proposed by\nKim et al. (2004) was used, in which probabilities are sim-\nply assigned to the two classes of keyword and \ufb01ller. The\nprobability for the keyword class was set to be 6 and 12\ntimes that of the \ufb01llers in the context-independent and con-\ntext-dependent systems, respectively. These ratios were\noptimized on the STD development set.\nThe set of hypothesised keyword matches and associ-\nated timings from the keyword spotting module are passed\nto the substring selection module which converts them into\ntheir corresponding phone or grapheme sequences as given\nby the sub-word unit decoder. These sub-word sequences are\nthen processed by the lexical access module as described in\nSections 4.1.1 and 4.1.2 above.\n4.3.1. Detecting false alarms using the lexical access module\nThe lexical access module determines the cost of match-\ning each dictionary word to the hypothesised sequence of\nphones or graphemes. Con\ufb01dence measures can be derived\nfrom both relative and absolute cost of keywords. For\nexample, if the second-best matching keyword has a cost\nwhich is close to that of the lowest cost keyword, then we\ncan assign low con\ufb01dence to the match. Similarly, if the\nabsolute cost for the best matching word is high, then we\nalso have low con\ufb01dence in this match.\nWe adapt this idea for detection of false alarms as fol-\nlows. The lexical access algorithm is run twice, \ufb01rst using\na set of costs estimated against the keywords which were\ncorrectly detected in the STD development set by the key-\nword spotting module. This identi\ufb01es a best matching word\nin the lexicon, along with its match cost Gbest. In the second\nrun of the lexical access algorithm, a set of costs trained on\nfalse alarms (FAs) produced by the keyword spotting mod-\nule when run on the STD development set, is used to return\nthe lowest-cost word GFA.\nIf the keywords corresponding to Gbest and GFA are the\nsame, and GFA   Gcorrect P a, a match is returned, as we\nconsider that the hypothesis is closer to a true keyword\nthan a false alarm. If the words associated with GFA and\nGbest di\ufb00er, or the di\ufb00erence falls below alpha, the match\nis rejected. The threshold a is tuned on the STD develop-\nment set.\n4.4. Vocabulary dependence\nThe sub-word lattice system described in Section 4.2 is\nthe most vocabulary independent, needing no knowledge\nof the keywords or indeed any word list at all during train-\ning (although a dictionary is required to convert the corpus\nword transcription into a sub-word unit transcription). The\n1-best system as described in Section 4.1 can be made inde-\npendent of the keyword list, but does need to know about\nthe corpus vocabulary during training of the lexical access\nmodule. The hybrid system of Section 4.3 is the most\nvocabulary-dependent system, and needs to know the cor-\npus vocabulary, the keyword list and have spoken exam-\nples of the keywords during training. It is expected that\nthe more vocabulary or corpus-dependent a system is, the\nbetter its performance should be.\n5. Evaluation metrics\nThe purpose of this research is to identify keywords\nwithin audio. Unlike ASR, which typically considers cor-\nrect recognition of all words equally important, we are\ninterested in the trade-o\ufb00 of precision and recall. We use\nthe following metrics to evaluate the systems presented in\nthis work.\nThe\ufb01gureofmerit(FOM)wasoriginallyde\ufb01nedbyRoh-\nliceketal.(1989)forthetaskofkeywordspotting.Itgivesthe\naverage detection rate over the range [1,10] false alarms per\nhourperkeyword.TheFOMvaluesforindividualkeywords\ncan be averaged in order to give an overall \ufb01gure.\nThe NIST STD 2006 evaluation plan (NIST, 2006)\nde\ufb01ned the metrics occurrence-weighted value (OCC) and\nactual term-weighted value (ATWV), both of which are spe-\nci\ufb01cally tailored to the task of spoken term detection.\nThese 2 metrics have been adopted and their description\nfollows.\nFor a given set of terms and some speech data, let\nNcorrect\u00f0t\u00de, NFA\u00f0t\u00de and Ntrue\u00f0t\u00de represent the number of cor-\nrect, false alarm, and actual occurrences of term t respec-\ntively. In addition, we denote the number of non-target\nterms (which gives the number of possibilities for incorrect\ndetection) as NNT\u00f0t\u00de. We also de\ufb01ne miss and false alarm\nprobabilities, Pmiss\u00f0t\u00de and PFA\u00f0t\u00de for each term t as:\nPmiss\u00f0t\u00de\u00bc1  \nNcorrect\u00f0t\u00de\nNtrue\u00f0t\u00de\n\u00f09\u00de\nPFA\u00f0t\u00de\u00bc\nNFA\u00f0t\u00de\nNNT\u00f0t\u00de\n\u00f010\u00de\nKeyword 1\nKeyword N\nFiller 1\nFiller M\nFig. 5. The recognition network used in the keyword spotting module of\nthe hybrid architecture.\n986 J. Tejedor et al.\/Speech Communication 50 (2008) 980\u2013991In order to tune the metrics to give a desired balance of\nprecision versus recall, a cost CFA for false alarms was de-\n\ufb01ned, along with a value V for correct detections.\nThe occurrence-weighted value is computed by accumu-\nlating a value for each correct detection and subtracting a\ncost for false alarms as follows:\nOCC \u00bc\nP\nt2terms\u00bdVNcorrect\u00f0t\u00de CFANFA\u00f0t\u00de \nP\nt2termsVNtrue\u00f0t\u00de\n\u00f011\u00de\nWhilst OCC gives a good indication of overall system per-\nformance, there is an inherent bias toward frequently-\noccurring terms.\nThe second NIST metric, the actual term-weighted value\n(ATWV) is arrived at by averaging a weighted sum of miss\nandfalsealarmprobabilities,Pmiss\u00f0t\u00deandPFA\u00f0t\u00de,overterms:\nATWV \u00bc 1  \nP\nt2terms\u00bdPmiss\u00f0t\u00de\u00febPFA\u00f0t\u00de \nP\nt2terms1\n\u00f012\u00de\nwhere b \u00bc C\nV \u00f0Pprior\u00f0t\u00de\n 1   1\u00de. The NIST evaluation scoring\ntool sets a uniform prior term probability Pprior\u00f0t\u00de\u00bc10\n 4,\nandtheratio C\nV tobe0.1withthee\ufb00ectthatthereisanempha-\nsis placed on recall compared to precision in the ratio 10:1.\nIn this work, we present results in terms of FOM and\nOCC. However, rather than giving the ATWV values\nwhich give point estimates of the miss and false alarm\nprobabilities, we present these results graphically in order\nto show the full range of operating points. For all results,\ntuning for the language model scale and insertion penalty\nis performed on STD development set according to the met-\nric which is used in evaluation. For all measures, higher\nvalues indicate better performance.\n6. Results\nThe experiments were performed on the ALBAYZIN data-\nbase, described in Section 2. A set of 80 keywords were cho-\nsen based on their high frequency of occurrence and\nsuitabilityassearchtermsforgeographical-domaininforma-\ntionretrieval,andevaluation(retrievingsearchterms)isper-\nformed on the STD test set. Signi\ufb01cance tests in the form of\npaired t-tests are used to compare systems, in order to deter-\nmine whether di\ufb00erences are consistent across search terms.\n6.1. Recognition accuracy\nWhilst phone or grapheme recognition is not the main\nfocus of this work, it is an important factor in STD\/KS per-\nformance. We present phone accuracy results in Table 2.\nFor both the phone and grapheme systems, performance\nis improved through the use of context-dependent models.\nThe grapheme recognition accuracy is higher, though this\nis expected as there are fewer graphemes than phones.\n6.2. Spoken term detection and keyword spotting results\nArchitecture 3 uses a standard keyword spotting module\nin combination with a sub-word-based con\ufb01dence measure.\nIn order to examine the gain due to the con\ufb01dence mea-\nsure, Table 3 presents results for the keyword spotting\nmodule in isolation.\nThese results show that the performance improvement\nin moving from context-independent to context-dependent\nmodels is greater for grapheme-based models than for\nphones. Paired t-tests show that there is no systematic dif-\nferences between the results of context-dependent phone\nand grapheme-based systems.\nTable 4 presents results in terms of FOM and OCC for\neach of the three architectures described above in Section 4.\nWe \ufb01rst note that comparing the results of the hybrid\narchitecture 3 with those in Table 3, the addition of the\ncon\ufb01dence measure leads to performance improvements\nfor each metric. However, it is only for the monographeme\nand triphone systems evaluated under the FOM metric that\nthe increases are statistically signi\ufb01cant (p < 0:01).\n6.2.1. Evaluation in terms of FOM\nTable 4 shows that for evaluation in terms of FOM,\ncontext-dependent models give the best performance for\nall architectures and for both phone and grapheme-based\nmodels. Signi\ufb01cance tests show that for the lattice-based\napproach of architecture 2, the grapheme-based systems\nTable 2\nPhone and grapheme recognition accuracy for both context-independent\nand dependent models\nMonophone\n(%)\nTriphone\n(%)\nMonographeme\n(%)\nTrigrapheme\n(%)\nRecognition\naccuracy\n63.9 68.2 75.2 79.1\nResults are presented on the phonetic test set.\nTable 3\nEvaluation of the keyword spotting module of architecture 3 in isolation\nKeyword spotting module\nMonophone Triphone Monographeme Trigrapheme\nFOM 65.9 68.3 61.0 67.6\nOCC 0.74 0.73 0.66 0.78\nResults are given in terms of FOM and OCC for both context dependent\nand independent models, using grapheme and phone units.\nTable 4\nResults in terms of FOM and OCC for the three architectures for context-\nindependent and -dependent phone and grapheme models\nMonophone Triphone Monographeme Trigrapheme\nFOM\nArchitecture 1 72.7 73.5 65.9 74.4\nArchitecture 2 44.0 47.1 58.1 64.0\nArchitecture 3 80.3 82.3 76.9 79.6\nOCC\nArchitecture 1 0.70 0.72 0.67 0.76\nArchitecture 2 0.40 0.42 0.53 0.61\nArchitecture 3 0.85 0.84 0.84 0.85\nFor all measures, higher values indicate better performance.\nJ. Tejedor et al.\/Speech Communication 50 (2008) 980\u2013991 987give consistent increases in performance over the best\nphone-based system with p < 0:01. Trigraphemes gave\nthe best performance on architecture 1, though this was\nnot found to be statistically signi\ufb01cant. For architecture\n3, the best results are found using phone-based models,\nthough the di\ufb00erence is not statistically signi\ufb01cant.\n6.2.2. Evaluation in terms of OCC\nWe \ufb01nd similar patterns where the evaluation is in terms\nof OCC, though the performance for the phone-based\nmodels does not improve by moving from context-indepen-\ndent to -dependent models. Graphemes give better perfor-\nmance than phones for architecture 2, shown to be\nsigni\ufb01cant with p < 0:01. For architecture 3, the results\nare very similar, and for architecture 1, the trigrapheme\ngives the highest performance, though the result is not sta-\ntistically signi\ufb01cant.\n6.2.3. Evaluation in terms of ATWV\nWe present detection error trade-o\ufb00 (DET) curves of the\nATWV performance for each of the three architectures in\nFig. 6. Each plot shows miss against false alarm probability\nfor context-independent and dependent models, for both\nphone and grapheme-based systems, giving an indication\nof the system performance at a number of operating points.\nThe DET curves for architecture 1 in Fig. 6 show that\nthe performances are quite similar for each of the systems,\nthough the trigrapheme models marginally outperform the\nothers for much of the range.\nFig. 6 shows the sizable performance gap between phone\nand grapheme-based models for the lattice-based architec-\nture 2, and that for most of the range, the trigrapheme sys-\ntem provides a lower bound. It is also showed that\nmonographeme system also outperforms both monophone\nand triphone systems.\nDET curves for the hybrid architecture are given in\nFig. 6, and show that the best performance is achieved\nby the monophone system.\n7. Conclusions and future\nOur results suggest that grapheme-based units perform\nat least as well as phone-based units for keyword spotting\nand spoken term detection, and that the relative perfor-\nmance of phone\/grapheme models varies according to the\narchitecture. The trends we observe when evaluating\naccording to FOM and OCC are similar, since both are\noccurrence-weighted measures, whereas ATWV is term-\nweighted and reveals di\ufb00erent aspects of the systems\u2019 per-\nformance. As expected, better results were found for\nvocabulary-dependant systems.\n7.1. Hybrid approach\nArchitecture 3, the hybrid system, which is the most\ncomplex and the most vocabulary dependent, gives the\noverall best performance for each type of sub-word unit,\nand for each evaluation metric. The DET curves in terms\nof ATWV metric in Fig. 6 show that the best performance\nis achieved by the monophone system. At the same time the\ndi\ufb00erence in FOM and OCC performance across the\n0 2 4 6 8\nx 10\n3\n0\n0.2\n0.4\n0.6\n0.8\n1\nP(miss)\nP\n(\nf\na\nl\ns\ne\n \na\nl\na\nr\nm\n)\nDET curve: 1best architecture\nmonophone\nmonographeme\ntriphone\ntrigrapheme\n0 0.005 0.01 0.015\n0\n0.2\n0.4\n0.6\n0.8\n1\nP(miss)\nP\n(\nf\na\nl\ns\ne\n \na\nl\na\nr\nm\n)\nDET curve: latticebased architecture\nmonophone\nmonographeme\ntriphone\ntrigrapheme\n0 1 2 3 4 5 6\nx 10\n4\n0\n0.2\n0.4\n0.6\n0.8\n1\nP(miss)\nP\n(\nf\na\nl\ns\ne\n \na\nl\na\nr\nm\n)\nDET curve: hybrid architecture\nmonophone\nmonographeme\ntriphone\ntrigrapheme\nFig. 6. DET curves showing miss against false alarm probability of each\narchitecture.\n988 J. Tejedor et al.\/Speech Communication 50 (2008) 980\u2013991di\ufb00erent acoustic models is not signi\ufb01cant. These results\nmay be attributed to the addition of other knowledge\nsources. These include the keyword network in the keyword\nspotting module and the empirically-trained costs in the lex-\nical access module, which makes it more robust to weaker\nacoustic models. However, this architecture cannot per-\nform spoken term detection (as de\ufb01ned by NIST) because\nit requires knowledge of the keywords when processing\nthe speech data.\n7.2. 1-Best approach\nArchitecture 1, the 1-best approach, is capable of spo-\nken term detection. Again, there is not signi\ufb01cant variation\nin performance across the four acoustic model types, pre-\nsumably because of the additional knowledge used in the\nform of the lexical access module. However, the DET\ncurves in terms of ATWV metric in Fig. 6 shows that the\ntrigrapheme models marginally outperform the others for\nmuch of the range.\n7.3. Lattice-based approach\nArchitecture 2, the lattice approach with no lexical\naccess module, is the most vocabulary and corpus indepen-\ndent system and conforms with the requirements of recent\nNIST evaluations. Under this architecture we \ufb01nd more\nmarked performance di\ufb00erences between the di\ufb00erent\nacoustic models. Our experiments give evidence that for\nthe lattice-based approach, grapheme-based systems out-\nperform equivalent phone-based methods.\nComparing the context-independent and context-depen-\ndent systems, we \ufb01nd that the grapheme-based approach\nbene\ufb01ts more from context-dependent modeling than the\nphoneme-based approach. This is expected, as a grapheme\nmay be pronounced quite di\ufb00erently according to context.\nBy comparison, context-dependent allophones belonging\nto the same central phone are typically subject to a smaller\ndegree of variation.\n7.4. Grapheme-based modelling\nWe consider that the power of the grapheme-based sys-\ntem on STD tasks, especially in the lattice-based architec-\nture, can be attributed to two factors. The \ufb01rst is the\nprobabilistic description of pronunciation variation in the\ngrapheme model, which helps represent all possible pro-\nnunciations of a search term in a single form. The second\nis its capacity to incorporate additional information,\nincluding both acoustic and phonological cues, in the lat-\ntice, thus improving the decision-making process in the\nsearch phase.\nGrapheme-based systems do not appear advantageous\nunder the 1-best and hybrid approaches of architectures 1\nand 3, where the single most likely phone or grapheme\nsequences are used rather than lattices for keyword search.\nGiven the increased acoustic variation associated with gra-\nphemes compared with phones, the advantage arises from\npostponing hard decisions and keeping multiple decoding\npaths alive. Furthermore, as stated above, the additional\nlinguistic information from the lexical access module may\ndiminish the relative performance of the di\ufb00erent acoustic\nmodels.\n7.5. Future work\nFuture work will include scaling up to larger tasks,\nwhich will necessitate development of language modelling\ntechniques. One advantage of grapheme-based sub-word\nunits is that very long span N-gram language models can\nbe trained directly from very large text corpora. However,\nproper smoothing of these language models will be essen-\ntial in order to retain the ability to model OOV search\nterms. Our goal is to build a full information retrieval sys-\ntem from the architectures presented in this work, incorpo-\nrating spoken term detection and keyword spotting. Within\nthis system, proper names will contribute to a high OOV\nrate, as will verbs, which present di\ufb03culties issue in Latin\nlanguages (Steigner and Schroder, 2003) such as Spanish.\nWe will also focus in multigrapheme-based systems in\norder to deal with the imported graphemes when the set\nof keywords to search for is composed of words borrowed\nfrom other languages.\nWe also intend to apply architecture 2 to other lan-\nguages and domains, initially English language meetings\ndata. Whilst letter-to-sound conversion is less regular\nfor English than Spanish, a grapheme-based approach\nwould still be desirable given the inherent \ufb02exibility in\ndealing with out of vocabulary terms. The key to this will\nbe in deriving an appropriate inventory of grapheme-\nbased units for English, and automated methods may\nbe required. Additionally, we are working on methods\nto replace the decision trees which map from CI to CD\ngrapheme units with probabilistic mappings. This will\nhave the e\ufb00ect of removing another hard decision from\nthe system, and improve the ability to model unusual\npronunciations.\nAcknowledgements\nThis work was mostly performed whilst JT was a visiting\nresearcher at the Centre for Speech Technology Research,\nUniversity of Edinburgh, and was partly funded by the\nSpanish Ministry of Science and Education (TIN 2005-\n06885). DW is a Fellow on the Edinburgh Speech Science\nand Technology (EdSST) interdisciplinary Marie Curie\ntraining programme. JF is funded by Scottish Enterprise\nunder the Edinburgh Stanford Link. SK is an EPSRC Ad-\nvanced Research Fellow. Many thanks to Igor Szoke and\ncolleagues in the Speech Processing Group of the Faculty\nof Information Technology at Brno University of Technol-\nogy for providing the lattice search tools.\nJ. Tejedor et al.\/Speech Communication 50 (2008) 980\u2013991 989Appendix A. Phone set\na,e,i,o,u vowel\nA,E,I,O,U stressed vowel\nan,en,in,on,un unstressed vowel between two nasals (i.e\nbetween n,m,~ n)\nAn,En,In,On,Un stressed vowel between two nasals (i.e\nbetween n,m,~ n)\nb plosive at the beginning of the word or\nafter a nasal (n,m,~ n)\nB fricative appears within a word if not\nafter a nasal\nT\/ corresponds to grapheme \u2018\u2018ch\u201d\nd plosive at the beginning of the word or\nafter a nasal (n,m,~ n)\nD fricative if it appears within a word, if\nnot after a nasal\nf corresponds to grapheme \u2018\u2018f\u201d\ng plosive at the beginning of the sentence\nor after a nasal (n,m,~ n)\nG fricative if it appears within a word, if\nnot after a nasal\nX sound as \u2018\u2018j\u201d\nj corresponds to grapheme \u2018\u2018i\u201d when \u2018\u2018i\u201d\nappears in a diphthong\nJ\/ corresponds to grapheme \u2018\u2018y\u201d at the\nbeginning of a word or after a nasal\n(n,m,~ n). When appearing after a word\nwhich \ufb01nishes in a vowel, changes to J\nJ corresponds to grapheme \u2018\u2018y\u201d all cases\nwhich are not considered in J\/\nk corresponds to grapheme \u2018\u2018k\u201d and\ngrapheme \u2018\u2018c\u201d when it does not sound as\n\u2018\u2018z\u201d\nl corresponds to grapheme \u2018\u2018l\u201d\nL corresponds to grapheme \u2018\u2018ll\u201d\nm corresponds to grapheme \u2018\u2018m\u201d\nn corresponds to grapheme \u2018\u2018n\u201d\nN corresponds to grapheme \u2018\u2018n\u201d when\nfollowing a vowel.\nNn corresponds to grapheme \u2018\u2018~ n\u201d\np corresponds to grapheme \u2018\u2018p\u201d\nr corresponds to grapheme \u2018\u2018r\u201d\nR corresponds to grapheme \u2018\u2018rr\u201d\ns corresponds to grapheme \u2018\u2018s\u201d\nt corresponds to grapheme \u2018\u2018t\u201d\nT corresponds to graphemes \u2018\u2018z\u201d or \u2018\u2018c\u201d\nwhen sounds as \u2018\u2018z\u201d\nw corresponds to grapheme \u2018\u2018u\u201d within a\ndiphthong\ngs corresponds to grapheme \u2018\u2018x\u201d\nReferences\nAlarcos, E., 1995. Grama \u00b4tica de la lengua espan \u02dcola. Real Academia\nEspan \u02dcola. Coleccio \u00b4n Lebrija y Bello, Espasa Calpe.\nChen, B., Cetin, O., Doddinton, G., Morgan, N., Ostendorf, M.,\nShinozaki, T., Zhu, Q., 2004. A CTS task for meaningful fast-\nturnaround experiments. In: Proceedings of Rich Transcription Fall\nWorkshop, Palisades, New York.\nCole, R.A., Fanty, M., Noel, M., Lander, T., 1994. Telephone speech\ncorpus development at CSLU. In: Proceedings of ICSLP, Yokohama,\nJapan, pp. 1815\u20131818.\nCuayahuitl, H., Serridge, B., 2002. Out-of-vocabulary word modeling and\nrejection for Spanish keyword spotting systems. In: Proceedings of\nMICAI, pp. 156\u2013165.\nDines, J., Doss, M.M., 2007. A study of phoneme and grapheme based\ncontext-dependent ASR systems. In: Proceedings of MLMI, Brno,\nCzech Republic.\nFissore, L., Laface, P., Micca, G., Pieraccini, R., 1989. Lexical access to\nlarge vocabularies for speech recognition. IEEE Trans. Acoust.\nSpeech, Signal Process. 37 (8), 1197\u20131213.\nHansen, J.R.H., Zhou, B., Seadle, M., Deller, J., Gurijala, A.R., Kurimo,\nM., Angkititrakul, P., 2005. Speech\ufb01nd: advances in spoken document\nretrieval for a national gallery of the spoken word. IEEE Trans.\nAcoust. Speech Signal Process. 13 (5), 712\u2013730.\nHauptmann, A., Wactlar, H., 1997. Indexing and search of multimodal\ninformation. In: Proceedings of IEEE International Conference on\nAcoustics, Speech, and Signal Processing (ICASSP-97), Vol. 1, pp.\n195\u2013198.\nJames, D., Young, S., 1994. A fast lattice-based approach to vocabulary\nindependent wordspotting. In: Proceedings of IEEE International\nConference on Acoustics, Speech, and Signal Processing (ICASSP-94),\nVol. 1, pp. 465\u2013468.\nKiller, M., Stuker, S., Schultz, T., 2003. Grapheme based speech\nrecognition. In: Proceedings of Eurospeech.\nKim, J., Jung, H., Chung, H., 2004. A keyword spotting approach based\non pseudo N-gram language model. In: Proceedings of SPECOM, pp.\n156\u2013159.\nLleida, E., Marino, J., Salavedra, J., Bonafonte, A., Monte, E., Mart\u0131 \u00b4nez,\nA. 1993. Out-of-vocabulary word modelling and rejection for keyword\nspotting. In: Proceedings of Eurospeech, pp. 1265\u20131268.\nLogan, B., Moreno, P., Van Thong, J., Whittaker, E. 2000. An\nexperimental study of an audio indexing system for the web. In:\nProceedings of International Conference on Speech and Language\nProcessing, Vol. 2, pp. 676\u2013679.\nMagiami-Doss, M., Stephenson, T.A., Bourlard, H., Bengio, S. 2003.\nPhoneme-grapheme based automatic speech recognition system. In:\nProceedings of ASRU, pp. 94\u201398.\nMagiami-Doss, M., Bengio, S., Bourlard, H., 2004. Joint decoding for\nphoneme-grapheme continuous speech recognition. In: Proceedings of\nICASSP, Montreal, Canada, pp. 177\u2013180.\nMakhoul, J., Kubala, F., Leek, T., Liu, D., Nguyen, L., Schwartz, R.,\nSrivastava, A., 2000. Speech and language technologies for audio\nindexing and retrieval. Proc. IEEE 88 (8), 1338\u20131353.\nMoreno, A., Poch, D., Bonafonte, A., Lleida, E., Llisterri, J.,\nMarin \u02dco, J., Nadeu, C. 1993. Albayzin speech database: design of\nthe phonetic corpus. In: Proceedings of Eurospeech, Vol. 1, pp.\n653\u2013656.\nNIST (2006). The spoken term detection (STD) 2006 evaluation plan.\nNational Institute of Standards and Technology, Gaithersburg, MD,\nUSA, v10 ed. <http:\/\/www.nist.gov\/speech\/tests\/std>.\nPrice, P.J., Fisher, W., Bernstein, J. 1998. A database for continuous\nspeech recognition in a 1000 word domain. In: Proceedings of\nICASSP, Vol. 1, pp. 651\u2013654.\nQuilis, A., 1998. El comentario fonolo \u00b4gico y fone \u00b4tico de textos. ARCO\/\nLIBROS, S.A.\nRohlicek, J., 1995. Modern methods of Speech Processing. Kluwer,\nNorwell MA.\nRohlicek, J., Russell, W., Roukos, S., Gish, H., 1989. Continuous hidden\nMarkov modeling for speaker-independent word spotting. In: Pro-\nceedings of ICASSP, Vol. 1, pp. 627\u2013630, Glasgow, UK.\nScott, J., Wintrode, J., Lee, M., 2007. Fast unconstrained audio search in\nnumerous human languages. In: Proceedings of IEEE International\n990 J. Tejedor et al.\/Speech Communication 50 (2008) 980\u2013991Conference on Acoustics, Speech, and Signal Processing (ICASSP-07),\nHonolulu, Hawai.\nSteigner, J., Schroder, M., 2003. Cross-language phonemisation in german\ntext-to-speech synthesis. In: Proceedings of Interspeech.\nSzoke, I., Schwarz, P., Matejka, P., Burget, L., Martin, K., Fapso, M.,\nCer- nocky, J., 2005. Comparison of keyword spotting approaches for\ninformal continuous speech. In: Proceedings of Interspeech, Lisabon,\nPortugal, pp. 633\u2013636.\nTanaka, K., Itoh, Y., Kojima, H., Fujimura, N. 2001. Speech data\nretrieval system constructed on a universal phonetic code domain. In:\nProceedings of IEEE Automatic Speech Recognition and Understand-\ning, pp 323\u2013326.\nTejedor,J., Cola \u00b4s, J., 2006.Spanish keywordspotting systembasedon \ufb01ller\nmodels, pseudo N-gram language model and a con\ufb01dence measure. In:\nProceedings of IV Jornadas de Tecnolog\u0131 \u00b4a del Habla, pp. 255\u2013260.\nThambiratmann, K., Sridharan, S., 2007. Rapid yet accurate speech\nindexing using dynamic match lattice spotting. IEEE Trans. Audio\nSpeech Process. 15 (1), 346\u2013357.\nYoung, S., Brown, M., 1997. Acoustic indexing for multimedia retrieval\nand browsing. In: Proceedings of IEEE International Conference on\nAcoustics, Speech, and Signal Processing (ICASSP-97), Vol. 1, pp.\n199\u2013202.\nYoung, S., Evermann, G., Hain, T., Kershaw, D., Moore, G., Odell, J.,\nOllason, D., Amnd Povey, D., Valtchev, V., Woodland, P., 2006. The\nHTK Book (for HTK Version 3.4). Microsoft Corp. and Cambridge\nUniversity Engineering Department.\nYu, P., Seide, F., 2004. A hybrid word\/phoneme-based approach for\nimproved vocabulary-independent search in spontaneous speech. In:\nProceedings of International Conference on Speech and Language\nProcessing, pp. 635\u2013643.\nYu, P., Chen, K., Ma, C., Seide, F., 2005. Vocabulary independent\nindexing of spontaneous speech. IEEE Trans. Acoust. Speech Signal\nProcess. 13 (5), 635\u2013643.\nJ. Tejedor et al.\/Speech Communication 50 (2008) 980\u2013991 991"}