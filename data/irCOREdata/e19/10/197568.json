{"doi":"10.1080\/03610918.2011.582561","coreId":"197568","oai":"oai:lra.le.ac.uk:2381\/9840","identifiers":["oai:lra.le.ac.uk:2381\/9840","10.1080\/03610918.2011.582561"],"title":"Infinite Variation Tempered Stable Ornstein-Uhlenbeck Processes with Discrete Observations","authors":["Kawai, Reiichiro","Masuda, Hiroki"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2011","abstract":"This is an electronic version of an article published in Communications in Statistics - Simulation and Computation, 2012, 41(1), pp. 125-139. Communications in Statistics - Simulation and Computation is available online at: www.tandfonline.comWe investigate transition law between consecutive observations of Ornstein\u2013\\ud\nUhlenbeck processes of infinite variation with tempered stable stationary\\ud\ndistribution. Thanks to the Markov autoregressive structure, the transition law can\\ud\nbe written in the exact sense as a convolution of three random components; a\\ud\ncompound Poisson distribution and two independent tempered stable distributions,\\ud\none with stability index in (0, 1) and the other with index in (1, 2). We discuss\\ud\nsimulation techniques for those three random elements. With the exact transition law\\ud\nand proposed simulation techniques, sample paths simulation proves significantly\\ud\nmore efficient, relative to the known approximative technique based on infinite shot\\ud\nnoise series representation of tempered stable L\u00e9vy processes.Peer-reviewedPost-prin","downloadUrl":"http:\/\/hdl.handle.net\/2381\/9840","fullTextIdentifier":"https:\/\/lra.le.ac.uk\/bitstream\/2381\/9840\/1\/ivtsousim.pdf","pdfHashValue":"1feb97ee1e7968758609c5cd1583e435529aef35","publisher":"Taylor & Francis","rawRecordXml":"<record><header><identifier>\n        \n            \n                oai:lra.le.ac.uk:2381\/9840<\/identifier><datestamp>\n                2012-09-15T01:45:06Z<\/datestamp><setSpec>\n                com_2381_445<\/setSpec><setSpec>\n                com_2381_9549<\/setSpec><setSpec>\n                col_2381_3823<\/setSpec>\n            <\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:doc=\"http:\/\/www.lyncode.com\/xoai\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n            \nInfinite Variation Tempered Stable Ornstein-Uhlenbeck Processes with Discrete Observations<\/dc:title><dc:creator>\nKawai, Reiichiro<\/dc:creator><dc:creator>\nMasuda, Hiroki<\/dc:creator><dc:subject>\nAcceptance-rejection sampling<\/dc:subject><dc:subject>\nL\u00e9vy process<\/dc:subject><dc:subject>\nOrnstein\u2013Uhlenbeck processes<\/dc:subject><dc:subject>\nSelf decomposability<\/dc:subject><dc:subject>\nTempered stable process<\/dc:subject><dc:subject>\nTransition law<\/dc:subject><dc:description>\nThis is an electronic version of an article published in Communications in Statistics - Simulation and Computation, 2012, 41(1), pp. 125-139. Communications in Statistics - Simulation and Computation is available online at: www.tandfonline.com<\/dc:description><dc:description>\nWe investigate transition law between consecutive observations of Ornstein\u2013\\ud\nUhlenbeck processes of infinite variation with tempered stable stationary\\ud\ndistribution. Thanks to the Markov autoregressive structure, the transition law can\\ud\nbe written in the exact sense as a convolution of three random components; a\\ud\ncompound Poisson distribution and two independent tempered stable distributions,\\ud\none with stability index in (0, 1) and the other with index in (1, 2). We discuss\\ud\nsimulation techniques for those three random elements. With the exact transition law\\ud\nand proposed simulation techniques, sample paths simulation proves significantly\\ud\nmore efficient, relative to the known approximative technique based on infinite shot\\ud\nnoise series representation of tempered stable L\u00e9vy processes.<\/dc:description><dc:description>\nPeer-reviewed<\/dc:description><dc:description>\nPost-print<\/dc:description><dc:date>\n2011-11-09T12:59:55Z<\/dc:date><dc:date>\n2012-09-15T01:45:06Z<\/dc:date><dc:date>\n2011<\/dc:date><dc:type>\nArticle<\/dc:type><dc:identifier>\nCommunications in Statistics - Simulation and Computation, 2012, 41(1), pp. 125-139.<\/dc:identifier><dc:identifier>\n0361-0918<\/dc:identifier><dc:identifier>\nhttp:\/\/www.tandfonline.com\/toc\/lssp20\/41\/1<\/dc:identifier><dc:identifier>\nhttp:\/\/hdl.handle.net\/2381\/9840<\/dc:identifier><dc:identifier>\n10.1080\/03610918.2011.582561<\/dc:identifier><dc:identifier>\n1532-4141<\/dc:identifier><dc:language>\nen<\/dc:language><dc:rights>\n\u00a9 Taylor & Francis 2011. Deposited with reference to the publisher's self-archiving policy available on the SHERPA\/RoMEO website.<\/dc:rights><dc:publisher>\nTaylor & Francis<\/dc:publisher>\n<\/oai_dc:dc>\n<\/metadata>\n        <\/record>","journals":[{"title":null,"identifiers":["0361-0918","issn:0361-0918","issn:1532-4141","1532-4141"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2011,"topics":["Acceptance-rejection sampling","L\u00e9vy process","Ornstein\u2013Uhlenbeck processes","Self decomposability","Tempered stable process","Transition law"],"subject":["Article"],"fullText":"Infinite Variation Tempered Stable Ornstein-Uhlenbeck Processes\nwith Discrete Observations\nREIICHIRO KAWAI\u0003AND HIROKI MASUDA\u2020\nAbstract\nWe investigate transition law between consecutive observations of Ornstein-Uhlenbeck processes of infinite variation\nwith tempered stable stationary distribution. Thanks to the Markov autoregressive structure, the transition law can be\nwritten in the exact sense as a convolution of three random components; a compound Poisson distribution and two\nindependent tempered stable distributions, one with stability index in (0;1) and the other with index in (1;2). We\ndiscuss simulation techniques for those three random elements. With the exact transition law and proposed simulation\ntechniques, sample paths simulation proves significantly more efficient, relative to the known approximative technique\nbased on infinite shot noise series representation of tempered stable Le\u00b4vy processes.\nKeywords: acceptance-rejection sampling, Le\u00b4vy process, Ornstein-Uhlenbeck processes, selfdecomposability, transi-\ntion law, tempered stable process.\n2010 Mathematics Subject Classification: 60J75, 62E15, 65C10, 68U20.\n1 Introduction\nThe class of non-Gaussian Ornstein-Uhlenbeck (OU, in short) processes is closely related to the selfdecomposability\nof the infinitely divisible distribution. Several interesting properties are known, such as the explicit relation between\nLe\u00b4vy measures of the stationary distribution and the underlying Le\u00b4vy process and the representation of entire trajectory\nbased on shot noise series representation of Le\u00b4vy processes, to mention just a few. (For details, see Section 17 of Sato\n[15], Masuda [11] and references therein.) Also, due to the growing practical interest, many authors have proposed\nstatistical inference methods for non-Gaussian OU processes. (See, for example, Brockwell et al. [4], Jongbloed et al.\n[8] and Sun and Zhang [16].)\nIn the class of non-Gaussian OU processes, the class of tempered stable OU (TS-OU, in short) process of finite\nvariation has been of particular interest from both theoretical and practical points of view. In terms of mathematical\ntractability, the transition law between consecutive observations can be written in the exact sense as a convolution of\none compound Poisson and one tempered stable distributions. It is known that exact, yet simple, simulation methods\nare available for both random elements. The particular setting of inverse-Gaussian OU processes was studied in Zhang\nand Zhang [18], while the general setting in [9]. Also, it was shown in Zhang and Zhang [19] that the transition law\nis selfdecomposable when the stability index is no less than 1=2. In practice, due to its distributional flexibility and\nthe positivity of sample paths, they have been used in financial economics and mathematical finance (for example,\nBarndorff-Nielsen and Shephard [2] and Benth et al. [3]).\nIn this paper, we study the class of TS-OU processes of infinite variation, that is, OU processes with a tempered\nstable stationary distribution with stability index in (1;2). This can be thought of as a natural alternative of finite\nvariation TS-OU processes, while the extension is not straightforward. In fact, the structure of transition law turns\nout to be significantly different, in the sense that for example, the transition law is a convolution of two independent\ntempered stable and one compound Poisson components. Also, the support of sample paths is necessarily the whole\nreal line, while only the positive half line in the finite variation setting if no negative jumps exist. We will here only\ndeal with a unilateral setting with no negative jumps. Nevertheless, the bilateral setting is also within our scope as\nit can be treated simply by superpositioning another similar convolution of three independent random components.\nIn addition, the bilateral framework can produce more distributional flexibility through combinations of positive and\nPublished in Communications in Statistics - Simulation and Computation (2012) 41(1) 125-139.\n\u0003Email Address: reiichiro.kawai@le.ac.uk. Postal Address: Department of Mathematics, University of Leicester, Leicester LE1 7RH, UK.\n\u2020Email Address: hiroki@math.kyushu-u.ac.jp. Postal Address: Graduate School of Mathematics, Kyushu University, Fukuoka 819-0395, Japan.\n1\nnegative jump components in terms of, for example, stability index and even finite and infinite variations. They may\nwiden the applicability of OU processes in a variety of fields.\nThe rest of this paper is organized as follows. Section 2 summarizes background material on stable and tempered\nstable distributions and on OU processes with tempered stable stationary distribution. In Section 3, we derive the\ntransition law in closed form, consisting of three random components; a compound Poisson distribution and two\nindependent tempered stable distributions, one with stability index a 2 (1;2), while the other with index a\u000012 (0;1).\nIn Section 4, we discuss simulation methods for the three random elements, all of which are based on acceptance-\nrejection sampling techniques. We also provide numerical results to illustrate the effectiveness of our exact transition\nlaw and proposed simulation techniques in sample paths generation, relative to the existing approximative method\nwith infinite shot noise series representation of tempered stable Le\u00b4vy processes. Finally, Section 5 concludes.\n2 Preliminaries\nLet us begin this preliminary section with the notations which will be used throughout the paper. We denote by\nR the one dimensional Euclidean space with the norm j \u0001 j and R+ := (0;+\u00a5). Let N be the collection of positive\nintegers with N0 := N[ f0g. We denote by L= the identity in law. We denote by G(a;b) the gamma distribution\nwith density ba=G(a)za\u00001e\u0000bz. We write fL(z) for a probability density function of a distribution L. (For example,\nfG(a;b)(z) = ba=G(a)za\u00001e\u0000bz.) We fix (W;F ;P) as our underlying probability space. Finally, note that G(\u0000s)< 0 for\ns 2 (0;1), while G(\u0000s)> 0 for s 2 (1;2).\n2.1 Spectrally Positive Stable Processes\nLet fL(s)t : t \u0015 0g be a totally positively skewed stable (Le\u00b4vy) process satisfying\nE\n\u0014\neiyL\n(s)\nt\n\u0015\n= exp\nh\ntaG(\u0000a)cos\n\u0010pa\n2\n\u0011\njyja\n\u0010\n1\u0000 i tan pa\n2\nsgn(y)\n\u0011i\n=\n8<:exp\nh\nt\nR\nR+\n\u0000\neiyz\u00001\u0001 aza+1 dzi ; if a 2 (0;1);\nexp\nh\nt\nR\nR+\n\u0000\neiyz\u00001\u0000 iyz\u0001 aza+1 dzi ; if a 2 (1;2); (2.1)\nwith some a> 0. Throughout this paper, we exclude the case a = 1. We write S(a ;a) :=L (L(s)1 ). The C\n+\u00a5-density\nof the distribution S(a;a) is given in the form of convergent power series\nfS(a;b)(x) =\n8><>:\n(\u0000aG(\u0000a))\u00001=a\np \u00e5\n+\u00a5\nk=1(\u00001)k\u00001 sin(pka)G(ka+1)k!\n\u0010\nx\n(\u0000aG(\u0000a))1=a\n\u0011\u0000ka\u00001\n; if a 2 (0;1);\n(aG(\u0000a))\u00001=a\np \u00e5\n+\u00a5\nk=1 sin\n\u0000\npk 1\u0000aa\n\u0001 G(k=a+1)\nk!\n\u0010\n\u0000 x\n(aG(\u0000a))1=a\n\u0011k\u00001\n; if a 2 (1;2):\n(2.2)\nNote that the above density is defined on R+ if a 2 (0;1), while on R if a 2 (1;2). It holds that for each t > 0,\nL (L(s)t ) = S(a; ta), and by the scaling property, L (t\u00001=aL\n(s)\nt ) = S(a;a). Note that the distribution S(a ; ta) has\ndensity t\u00001=a fS(a ;a)(t\u00001=ax). The distribution S(a;a) can be simulated in the exact sense through the well known\nrepresentation, due to Chambers et al. [5],\nS(a;a) L= (\u0000aG(\u0000a)cos(pa=2))1=a sin(aV +q)\n(cosV cosq)1=a\n\u0012\ncos((1\u0000a)V \u0000q)\nE\n\u0013 1\u0000a\na\n; (2.3)\nwhere q := arctan(tan(pa=2)), V is a uniform random variable on (\u0000p=2;p=2) and E is a standard exponential\nrandom variable independent of V . See Zolotarev [20] for more details on the stable distribution.\n2.2 Spectrally Positive Tempered Stable Processes\nLet fL(ts)t : t \u0015 0g be a centered and totally positively skewed tempered stable (Le\u00b4vy) process satisfying\nE\n\u0014\neiyL\n(ts)\nt\n\u0015\n= exp\n\u0014\nt\nZ\nR+\n\u0000\neiyz\u00001\u0000 iyz\u0001a e\u0000bz\nza+1\ndz\n\u0015\n= exp\n\u0002\ntaG(\u0000a)\u0000(b\u0000 iy)a \u0000ba + iyaba\u00001\u0001\u0003 :\n2\nNote that as indicated by \u201ccentered\u201d, the tempered stable process here is centered even in the case a 2 (0;1), unlike\nin the case of the stable process characterized by (2.1). As a matter of course, when a 2 (0;1), by adding back the\ncentering term as L(ts)t + tG(1\u0000a)aba\u00001, we can recover the associated tempered stable subordinator. Throughout the\npaper, we will use the notations\nTS(a;a;b) :=L (L(ts)1 ); (2.4)\nand\nTS0(a;a;b) :=L (L(ts)1 +G(1\u0000a)aba\u00001): (2.5)\nIt is known that\ne\u0000bz\nE\nh\ne\u0000bL\n(s)\n1\ni fS(a;a)(z) = e\u0000bz\u0000aG(\u0000a)ba fS(a ;a)(z) = fTS0(a ;a;b)(z): (2.6)\nThe class of tempered stable distributions was first proposed by Tweedie [17]. Several featuring properties of\ntempered stable distributions and processes were revealed by Rosin\u00b4ski [14], such as a stable-like behavior over short\nintervals, the absolute continuity with respect to its short-range limiting stable process, an aggregational Gaussianity\nand an infinite shot noise series representation in closed form\nn\nL(ts)t : t 2 [0;T ]\no\nL\n=\n(\n+\u00a5\n\u00e5\nk=1\n\"\"\u0012\naGk\nTa\n\u0013\u00001=a\n^ VkU\n1=a\nk\nb\n#\n1[0;t](Tk)\u0000\nt\nT\n\u0012\nak\nTa\n\u0013\u00001=a\n1(1;2)(a)\n#\n+\nt\nT\n\u0012\nTa\na\n\u00131=a\nz (1=a)1(1;2)(a)\u0000 tG(1\u0000a)aba\u00001 : t 2 [0;T ]\n)\n; (2.7)\nwhere fGkgk2N are arrival times of a standard Poisson process, fTkgk2N is a sequence of iid uniform random variables\non [0;T ], fVkgk2N is a sequence of iid standard exponential random variables and fUkgk2N is a sequence of iid uniform\nrandom variables on [0;1]. All those random sequences are mutually independent. Note that the kernel of series\nrepresentation is not unique. In fact, there are different series representations derived in Imai and Kawai [7] through\nthe thinning and rejection methods and yet another representation numerically through the inverse Le\u00b4vy measure\nmethod. (For details about the methods, see Rosin\u00b4ski [13].)\n2.3 Ornstein-Uhlenbeck Processes with Tempered Stable Stationary Distribution\nConsider the stochastic process fXt : t \u0015 0g defined in form of the stochastic differential equation\ndXt = l (m\u0000Xt)dt+dZl t ; (2.8)\nwhere l > 0, m 2 R and fZt : t \u0015 0g is a Le\u00b4vy process (not necessarily a subordinator), or in canonical form\nXt = e\u0000l tX0+m\n\u0010\n1\u0000 e\u0000l t\n\u0011\n+\nZ t\n0\ne\u0000l (t\u0000s)dZl s: (2.9)\nProcesses of this type are often called non-Gaussian OU processes, or Le\u00b4vy-driven OU processes. With fZt : t \u0015 0g\nbeing a subordinator, they have been used, for example, to model the squared volatility in a stochastic volatility model\nof Barndorff-Nielsen and Shephard [2], due to the non-negativity of sample paths.\nIn this paper, we consider the class of OU processes (2.8) where its invariant lawL (limt\"+\u00a5Xt) is TS(a;a;b)\u0003dm\nwith a 2 (1;2), where \u0003 denotes the convolution and dm is the degenerate distribution at m . The invariant law is clearly\nself-decomposable and has Le\u00b4vy density\nu(z) = a\ne\u0000bz\nza+1\n; z 2 R+: (2.10)\nIn fact, since the law has finite moments of every order due to the exponential tempering in (2.10), it follows that\nregardless of the choice of the parameter l > 0, there exists an ergodic Le\u00b4vy-driven OU process having TS(a;a;b)\u0003dm\nas its invariant law. (We refer the reader to Masuda [11] and the references therein for details about Le\u00b4vy-driven OU\nprocesses.) In particular, OU process with inverse Gaussian invariant law (a = 1=2) was applied in Benth [3] to\nstochastic volatility modeling of [2] for volatility and variance swap valuations.\n3\nLet w(z) be the Le\u00b4vy density of the marginal Z1 of the background driving Le\u00b4vy process. Since u(z) is differen-\ntiable, the Le\u00b4vy densities w(z) and u(z) are related by\nw(z) =\u0000u(z)\u0000 z \u00b6\n\u00b6 z\nu(z) = aa\ne\u0000bz\nza+1\n+ab\ne\u0000bz\nz(a\u00001)+1\n: (2.11)\n(See, for example, [2].) Therefore, on the one hand, if a 2 (0;1), then the underlying process fZt : t \u0015 0g is the\nsuperposition of a tempered stable process with TS0(a;aa ;b) and a compound Poisson process with Le\u00b4vy density\nabz\u0000ae\u0000bz. Sample paths can be written in the exact sense, using the infinite shot noise series representation (2.7) as\nfXt : t 2 [0;T ]g L=\n(\ne\u0000l tX0+m\n\u0010\n1\u0000 e\u0000l t\n\u0011\n+\n+\u00a5\n\u00e5\nk=1\ne\u0000l (t\u0000Tk)\n\"\u0012\nGk\naT\n\u0013\u00001=a\n^ VkU\n1=a\nk\nb\n#\n1[0;t] (Tk)\n+\n+\u00a5\n\u00e5\nk=1\neeGk\u0000l tGk1[0;l t]\u0010eGk\u0011 : t 2 [0;T ]\n)\n; (2.12)\nwhere feGkgk2N are arrival times of a standard Poisson process, independent of fGkgk2N, with intensity G(1\u0000a)aba(=R\nR+ abz\n\u0000ae\u0000bzdz), and fGkgk2N is a sequence of iid random variables with gamma distribution G(1\u0000a; b).\nOn the other hand, if a 2 (1;2), then the equation (2.11) implies that the underlying process fZt : t \u0015 0g is a\nsuperposition of two independent tempered stable processes with TS(a;aa;b) and TS(a \u0000 1;ab;b). Sample paths\ncan also be written in the exact sense with infinite shot noise series representation (2.7). This may however be not\nvery sensible, at least for the following three reasons; (i) there are too many random sequences to be generated, (ii) the\nseries representation for TS(a;aa;b) contains intricate centering terms as seen in (2.7), and (iii) the issue of truncation\nerror has to be addressed for two infinite shot noise series.\n3 Transition Law of Tempered Stable Ornstein-Uhlenbeck Processes of In-\nfinite Variation\nIn this section, we derive the transition law between consecutive observations of discrete time skeleton\nX0; XD; X2D; \u0001 \u0001 \u0001 ;\nof infinite variation TS-OU processes (2.8), with a fixed time stepsize D> 0. (In principle, the stepsize does not need\nto be equidistant and can be set different positive values for different steps.) The difference from the finite variation\nsetting [9, 19] lies in the integrability of Le\u00b4vy density of the transition law around the origin. As a consequence, the\nLe\u00b4vy density has to be decomposed twice to extract all infinite activity part, while only once in the finite variation\ncase. For better presentation, we will use the following notations\nw1;D(z) :=\n\u0010\n1\u0000 e\u0000alD\n\u0011 a\nza+1\ne\u0000bz;\nw2;D(z) := e\u0000alD\na\nza+1\n\u0010\ne\u0000bz\u0000 e\u0000belDz\n\u0011\n;\nw21;D(z) := e\u0000alD\n\u0010\nelD\u00001\n\u0011 ab\nz(a\u00001)+1\ne\u0000be\nlDz;\nw22;D(z) := ae\u0000alD\ne\u0000bz\u0000 e\u0000belDz\u0000b(elD\u00001)ze\u0000belDz\nza+1\n:\nTheorem 3.1. Fix D> 0. For each n 2 N0, it holds that given XnD,\nX(n+1)D\nL\n= e\u0000lDXnD+m\n\u0010\n1\u0000 e\u0000lD\n\u0011\n+Y01+Y02+\n NkD\n\u00e5\nk=1\nQk\u0000Dg\n!\n; (3.1)\nwhere\ng := aba\u00001G(1\u0000a)\n\u0010\ne\u0000alD\u0000 e\u0000lD\u0000\n\u0010\ne\u0000lD\u0000 e\u00002lD\n\u0011\n(1\u0000a)\n\u0011\n;\nand all the random elements are mutually independent and specified as\n4\n\u000f Y01 \u0018 TS(a;a(1\u0000 e\u0000alD);b),\n\u000f Y02 \u0018 TS(a\u00001;abe\u0000alD(elD\u00001);belD),\n\u000f NkD \u0018 Pois(kD) with kD := abaG(\u0000a)(a(1\u0000 e\u0000lD)+ e\u0000alD\u00001),\n\u000f fQkgk2N is a sequence of iid random variables in R+ with common probability density v1;D(z) := k\u00001D w22;D(z).\nMoreover, the transition law is selfdecomposable.\nProof. By the homogeneous Markovian autoregressive structure of (2.9), it holds that for each n 2 N0, given XnD,\nX(n+1)D = e\n\u0000lDXnD+m\n\u0010\n1\u0000 e\u0000lD\n\u0011\n+\nZ (n+1)D\nnD\ne\u0000l ((n+1)D\u0000s)dZl s\n=: e\u0000lDXnD+m\n\u0010\n1\u0000 e\u0000lD\n\u0011\n+ eD;n+1\nL\n= e\u0000lDXnD+m\n\u0010\n1\u0000 e\u0000lD\n\u0011\n+\nZ lD\n0\ne\u0000lD+sdZs;\nwhere the identity in law holds by the independence and stationarity of increments of the underlying Le\u00b4vy process\nfZt : t \u0015 0g. This implies that feD;kgk2N reduces to a sequence of iid random variables with common distribution\nFD := L (\nR lD\n0 e\n\u0000lD+sdZs). It thus suffices to investigate the conditional law L (XDjX0), that is, only of the first\nincrement. Note that by definition, this law is infinitely divisible.\nUsing the Le\u00b4vy-integral transform, we get the characteristic function of the distribution FD as\nbFD(y) = exp\u0014Z\nR+\n\u0000\neiyz\u00001\u0000 iyz\u0001wD(z)dz\u0015 ; (3.2)\nwhere\nwD(z) :=\nZ lD\n0\nesw(esz)ds:\nBy further computing wD(z), we get\nwD(z) =\na\nza+1\nZ lD\n0\n(a+besz)e\u0000ase\u0000be\nszds\n=\u0000 a\nza+1\nZ lD\n0\n\u00b6\n\u00b6 s\n\u0010\ne\u0000ase\u0000be\nsz\n\u0011\nds\n=\na\nza+1\n\u0010\ne\u0000bz\u0000 e\u0000alD\u0000belDz\n\u0011\n= w1;D(z)+w2;D(z):\nNote that w1;D and w2;D are positive functions on R+. Clearly, w1;D is the smooth Le\u00b4vy density of TS(a ;(1\u0000\ne\u0000alD)a;b). Note that w2;D(z) \u0018 abe\u0000alD(elD\u0000 1)z\u0000a as z # 0. This implies that w2;D is not integrable and thus\ncannot be a Le\u00b4vy density of compound Poisson components.\nLet us further decompose w2;D into two parts. We use the identity\ne\u0000x\u0000 e\u0000y = (y\u0000 x)e\u0000y+ e\u0000x (y\u0000 x)2H (y\u0000 x) ; y> x> 0: (3.3)\nwhere H(z) := z\u00002(1\u0000e\u0000z(1+ z)) is positive, bounded and strictly decreasing on R+ such that limz#0H(z) = 1=2. By\napplying this identity, we get\nw2;D(z) =e\u0000alD\n\u0010\nelD\u00001\n\u0011 ab\nz(a\u00001)+1\ne\u0000be\nlDz\n+ab2\n\u0010\nelD\u00001\n\u00112\ne\u0000alDz(2\u0000a)\u00001e\u0000bzH\n\u0010\nb\n\u0010\nelD\u00001\n\u0011\nz\n\u0011\n=w21;D(z)+w22;D(z):\nClearly, w21;D is the smooth Le\u00b4vy density of TS(a\u00001;abe\u0000alD(elD\u00001);belD).\n5\nNext, observe thatZ\nR+\nw22;D(z)dz= ae\u0000alD\nZ\nR+\nz\u00001\u0000a\n\u0010\ne\u0000bz\u0000 e\u0000belDz\u0000b\n\u0010\nelD\u00001\n\u0011\nze\u0000be\nlDz\n\u0011\ndz= kD:\nThis shows that w22;D serves as a Le\u00b4vy density of compound Poisson components. To realize the centering for the\ncompound Poisson distribution due to (3.2), we need to subtract the constant term\nR\nR+ zw22;D(z)dz= g , multiplied by\nthe time stepsize D.\nIt remains to show the selfdecomposability of the transition law. Define\nhD(z) :=\n1\nza\nZ lD\n0\n(a+besz)e\u0000as\u0000be\nszds; z 2 R+;\nso that wD(z) = ahD(z)=z. This function is obviously non-negative and is decreasing on R+, due to\nd\ndz\nhD(z) =\nZ lD\n0\nz\u00001\u0000ae\u0000as\u0000be\nsz \u0000\u0000(bes)2z2\u0000 (2a\u00001)besz\u0000a2\u0001ds (3.4)\n\u0014\u0000 a\n2\nz1+a\nZ lD\n0\ne\u0000as\u0000be\nszds< 0:\nHence, Corollary 15.11 of Sato [15] yields the claim. The proof is complete.\nRemark 3.2. It is worth noting that the selfdecomposability of the transition law holds for any a 2 (1;2) in the\ninfinite variation setting, while in the finite variation case, it holds only for the stability index of no less than 1=2. This\ndifference occurs due to the term 2a\u00001 in the integrand of (3.4), that is, the sign of 2a\u00001 changes at a = 1=2. (See\nZhang and Zhang [19] for the finite variation case.)\nRemark 3.3. It is difficult to provide an efficient simulation method for the distribution induced by the Le\u00b4vy density\nw2;D. Nevertheless, in the finite variation setting, that is, if a 2 (0;1), then w2;D is integrableZ\nR+\nw2;D(z)dz=\u0000abaG(\u0000a)\n\u0010\n1\u0000 e\u0000alD\n\u0011\n2 R+: (3.5)\nThis shows that w2;D acts as the smooth Le\u00b4vy density of a compound Poisson distribution. Moreover, there is no further\nneed to decompose w2;D, unlike we did in Theorem 3.1, since the corresponding Poisson distribution can be simulated\nin the exact sense through an acceptance-rejection sampling method, which will be presented in Section 4.1.\n4 Simulation Methods\nIt follows from Theorem 3.1 that to simulate sample paths of infinite variation TS-OU processes at discrete timings, it\nsuffices to simulate three random elements, that is, the tempered stable random variablesY01 andY02, and the compound\nPoisson random variable\u00e5\nNkD\nk=1Qk. In this section, we discuss simulation methods for those random elements. We begin\nwith relatively straightforward cases of Y02 and of the compound Poisson \u00e5\nNkD\nk=1Qk.\n4.1 Simulation of Tempered Stable with Stability Index a\u00001\nFirst, we present an exact simulation method for Y02 \u0018 TS(a\u00001;abe\u0000alD(elD\u00001);belD), which is centered. Hence,\nwe will first instead simulate TS0(a\u00001;abe\u0000alD(elD\u00001);belD), that is,\nY 002 := Y02+G(2\u0000a)aba\n\u0010\n1\u0000 e\u0000lD\n\u0011\n;\nwhich takes values solely in R+ and then subtract the added constant term. An efficient and exact simulation method\nfor the case a\u00001= 0:5, that is the inverse Gaussian, is well known due to Michael et al. [12]. For the general case of\na \u0000 1 2 (0;1), the best route would be acceptance-rejection sampling based on the representation (2.3) of the stable\ndistribution and the likelihood ratio of the two densities; for each z 2 R+,\nfTS0(a\u00001;abe\u0000alD(elD\u00001);belD)(z)\nfS(a\u00001;abe\u0000alD(elD\u00001))(z)\n= e\u0000be\nlDz\u0000G(1\u0000a)aba+1(elD\u00001) \u0014 e\u0000G(1\u0000a)aba+1(elD\u00001); (4.1)\n6\nwhere the density functions fS(a;a) and fTS0(a;a;b) are given respectively by (2.2) and (2.6). The random variable Y02\ncan then be generated in the exact sense by the following simple acceptance-rejection sampling algorithm.\nAlgorithm 1 (Y02 \u0018 TS(a\u00001;abe\u0000alD(elD\u00001);belD)):\nStep 1. GenerateU as uniform (0;1) and V , independent ofU , as S(a\u00001;abe\u0000alD(elD\u00001)) through the\nrepresentation (2.3).\nStep 2. IfU \u0014 e\u0000belDV , let Y02  V \u0000G(2\u0000a)aba(1\u0000 e\u0000lD). Otherwise, return to Step 1.\nThe acceptance rate at Step 2 of Algorithm 1 is given by\np1(D) := P\n\u0010\nU \u0014 e\u0000belDV\n\u0011\n= eG(1\u0000a)ab\na+1(elD\u00001):\nClearly, the algorithm works more efficiently when the acceptance rate p1(D) at Step 2 is closer to 1. Indeed, this\nhappens when D # 0.\nIt is, however, more practical to discuss the effectiveness on the work-normalized basis. Since the simulation of\nL (Y02) is exact through Algorithm 1, all we need to pay attention to is the computing time required to generate iid\nincrements from TS(a \u0000 1;abe\u0000alD(elD\u0000 1);belD) to fill each sample path. Since we are concerned with sample\npaths over a finite time horizon, by taking a smaller time stepsize D, the number of increments for each sample path\nincreases in proportion to 1=D. Next, in Algorithm 1, the number of trials until one acceptance has the geometric\ndistribution with success probability p1(D). The average time to get one sample from Algorithm 1 is thus proportional\nto 1=p1(D). Then, we find that as D # 0,\n1\nD \u0001 p1(D) '\n1\nD\n;\nwhich implies that the average computing time related to Y02 for each sample path increases in proportion to 1=D as\nD # 0.\nFor more details about related acceptance-rejection sampling methods, see Baeumer and Meerschaert [1], Devroye\n[6] and Kawai and Masuda [9].\n4.2 Simulation of Compound Poisson Component\nNext, we consider simulation of the compound Poisson component. Generation of the Poisson random variable NkD\nis straightforward and is thus omitted. We concentrate on generation of the random sequence fQkgk2N. Recall that\nL (Q1) has a probability density function v1;D(z) and that the function H in (3.3) is positive, bounded and strictly\ndecreasing on R+ with limz#0H(z) = 1=2:We can thus show that\nv1;D(z)\u0014 12kD ab\na\n\u0010\nelD\u00001\n\u00112\ne\u0000alDG(2\u0000a) fG(2\u0000a;b)(z) =: gD(z); (4.2)\nwhere fG(2\u0000a ;b)(z) = b2\u0000aG(2\u0000a)\u00001z(2\u0000a)\u00001e\u0000bz defined on R+. Then, it holds that\nv1;D(z)\ngD(z)\n=\n2\n(elD\u00001)2b2\ne\u0000bz\u0000 e\u0000belDz\u0000b(elD\u00001)ze\u0000belDz\nz2\n=: v2;D(z); z 2 R+:\nThis suggests the following acceptance-rejection sampling algorithm for generation of the random variable Q1.\nAlgorithm 2 (Q1 with probability density v1;D(z))\nStep 1: GenerateU as uniform (0;1) and V , independent ofU , as G(2\u0000a ;b).\nStep 2: IfU \u0014 v2;D(V ), let Q1  V . Otherwise, return to Step 1.\nThe acceptance rate at Step 2 is given by\np2(D) := P(U \u0014 v2;D(V )) = 2a(a\u00001)\na(1\u0000 e\u0000lD)+ e\u0000alD\u00001\ne\u0000alD(elD\u00001)2 :\nWe can show that the acceptance rate tends to 1 as D # 0.\n7\nSince the simulation of fQkgk2N is exact by Algorithm 2 as well, all we need to pay attention to is the computing\ntime required to generate iid increments from the distributionL (\u00e5\nNkD\nk=1Qk) to fill each sample path. Again, by taking a\nsmaller time stepsize D, the number of increments over a finite time horizon increases in proportion to 1=D. Next, the\naverage number of implementation of Algorithm 2 required to generate one sample from the distributionL (\u00e5\nNkD\nk=1Qk)\nis proportional to the intensity kD of the Poisson random variable NkD . The average time to get one sample from\nAlgorithm 2 is proportional to 1=p2(D). Therefore, we get\nkD\nD \u0001 p2(D) =\nabaG(\u0000a)a(a\u00001)e\u0000alD(elD\u00001)2\n2D\n' D;\nas D # 0. This implies that the computing time for simulation ofL (\u00e5NkDk=1Qk) decreases linearly in D, as D # 0.\n4.3 Simulation of Tempered Stable with Stability Index a\nWe have seen that the two distributionsL (Y02) andL (\u00e5\nNkD\nk=1Qk) can be simulated in the exact sense through acceptance-\nrejection sampling. To the best of our knowledge, the exact simulation of Y01 \u0018 TS(a;a(1\u0000 e\u0000alD);b) seems impos-\nsible in practice. For example, the series representation (2.7) looks like an exact method, while it is still approximative\nas soon as a finite truncation is performed. In this paper, amongst several possible approximative methods, we present\na method proposed by Baeumer and Meerschaert [1], which seems to be most suitable for our purpose. (See [10] for\na comparison among various approximative simulation methods.) Its procedure is outlined as follows.\nAlgorithm 3 (Approximation of Y01 \u0018 TS(a;a(1\u0000 e\u0000alD);b)):\nFix c> 0.\nStep 1. GenerateU as uniform (0;1) and V (D), independent ofU , as S(a;a(1\u0000 e\u0000alD)).\nStep 2. IfU \u0014 e\u0000b(V (D)+c), let Y 001;c  V (D). Otherwise, return to Step 1.\nStep 3. Return Y01;c  Y 001;c\u0000G(1\u0000a)a(1\u0000 e\u0000alD)ba\u00001.\nNote that the random variable Y 001;c in Step 2 approximates TS\n0(a;a(1\u0000 e\u0000alD);b), while Y01;c approximates\nTS(a;a(1\u0000 e\u0000alD);b). In fact, the constant shift \u0000G(1\u0000a)a(1\u0000 e\u0000alD)ba\u00001 in Step 3 accounts for the difference\nbetween (2.4) and (2.5).\nLet us briefly review basic properties of Algorithm 3 derived in [1]. The acceptance rate at Step 2 of Algorithm 3\nis\np3(D;c) := E\nh\ne\u0000b(V (D)+c)\n\f\fV (D)>\u0000ciP(V (D)>\u0000c)+P(V (D)\u0014\u0000c) :\nMoreover, we have\nP(Y 001;c \u0014 z) =\n1\np3(D;c)\n\u0012\nP(V (D)\u0014min(z;\u0000c))+\nZ z\nmin(z;\u0000c)\ne\u0000b(y+c) fS(a;a(1\u0000e\u0000alD))(y)dy\n\u0013\n;\nfL (Y 001;c)(z) =\n(\np3(D;c)\u00001 fS(a ;a(1\u0000e\u0000alD))(z); if z 2 (\u0000\u00a5;\u0000c];\np3(D;c)\u00001e\u0000b(z+c) fS(a;a(1\u0000e\u0000alD))(z); if z 2 (\u0000c;+\u00a5):\nIn view of the expression\nfTS0(a;a(1\u0000e\u0000alD);b)(z) = e\n\u0000b(z+a(1\u0000e\u0000alD)G(\u0000a)ba\u00001) fS(a;a(1\u0000e\u0000alD))(z); z 2 R;\nthe parameter c in Algorithm 3 acts as a truncation of the entire real line R to the domain on which the exponential\ntempering e\u0000bz is performed. To sum up, we get\nfL (Y01;c)(z) = fL (Y 001;c)(z+G(1\u0000a)a(1\u0000 e\n\u0000alD)ba\u00001); z 2 R:\nIt is also proved in Theorem 8 [1] that the density fL (Y01;c) converges in L\n1(R) to the true density fTS(a ;a(1\u0000e\u0000alD);b)\nas c \" +\u00a5, and consequently the Kolmogorov-Smirnov distance DKS(D;c) between two distributions L (Y01;c) and\n8\nTS(a;a(1\u0000 e\u0000alD);b) converges to zero as well, where\nDKS(D;c) := sup\nx2R\n\f\f\f\fZ x\u0000\u00a5\n\u0010\nfL (Y01;c)(z)\u0000 fTS(a;a(1\u0000e\u0000alD);b)(z)\n\u0011\ndz\n\f\f\f\f\n= sup\nx2R\n\f\f\f\fZ x\u0000\u00a5\n\u0010\nfL (Y 001;c)(z)\u0000 fTS0(a;a(1\u0000e\u0000alD);b)(z)\n\u0011\ndz\n\f\f\f\f\n= sup\nx2R\n\f\f\f\f\f\nZ x\n\u0000\u00a5\n \n1^ e\u0000b(z+c)\np3(D;c)\n\u0000 e\u0000b(z+a(1\u0000e\u0000alD)G(\u0000a)ba\u00001)\n!\nfS(a;a(1\u0000e\u0000alD))(z)dz\n\f\f\f\f\f :\nNevertheless, it is not sensible to simply look for a smaller distribution error by taking c \" +\u00a5, since then the con-\nsequent low acceptance rate makes Algorithm 3 extremely inefficient, due to limc\"+\u00a5 p3(D;c) = 0 for each D > 0.\nMeanwhile, it holds that for each c > 0, limD#0 p3(D;c) = e\u0000bc. Thus, there exists the issue of trade-off between the\ndistribution error and the computing effort in terms of D and c. Concerning the computing effort, on the one hand,\nwe wish to find (D;c) minimizing 1=(D \u0001 p3(D;c)), just as in the previous two subsections. It should be noted that\nasymptotic behaviors of p3(D;c) in D and c are difficult to analyze. Next, on the other hand, there exist several appro-\npriate criteria to measure the distribution error. Natural candidates include L1(R), L2(R)-distances between densities\nfL (Y01;c) and fTS(a;a(1\u0000e\u0000alD);b), while the Kolmogorov-Smirnov distance DKS(D;c) above is certainly valid. None\nof them are tractable in an explicit manner. For illustrative purpose, we present numerical results in Table 1 with\nparameter set (a;a;b;l ) = (1:8; 1:0; 1:0; 0:2) and D= 0:1.\nc 0.0 0.3 0.6 0.8 1.0 1.2 1.4 1.6\nDKS(D;c) 1.10E-1 6.35E-2 2.14E-2 7.00E-3 1.63E-3 2.69E-4 3.13E-5 2.51E-6\np3(D;c) 0.875 0.753 0.599 0.499 0.411 0.337 0.276 0.226\nTable 1: Numerical results of distribution error and acceptance rate for different truncation points c.\nWe draw in Figure 1 the resulting density functions fL (Y01;c) with a few different choices of c, together with\nthe true tempered stable density function fTS(a;a(1\u0000e\u0000alD);b). For clear comparison, we also provide vertical lines\nx = \u0000c\u0000G(1\u0000a)a(1\u0000 e\u0000alD)ba\u00001. (We need the constant shift here because the truncation c is performed on the\ndistribution TS0(a;a(1\u0000e\u0000alD);b), rather than on TS(a;a(1\u0000e\u0000alD);b).) Observe that two densities are sufficiently\nclose when c = 1:4. It is not quite worth setting c > 1:4 in this specific example, as the acceptance rate decreases\nstill steadily. In principle, one would expect a small b in any application, since otherwise an exponentially tailed\ndistribution would be chosen instead. This fact favors a relatively large acceptance rate p3(D;c).\n\u22121 0 1 2\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\n\u22121 0 1 2\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\n\u22121 0 1 2\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\nc= 0:0 c= 0:6 c= 1:4\nFigure 1: Comparison of two density functions fL (Y01;c) (solid) and fTS(a;a(1\u0000e\u0000alD);b) (dotted) under (a;a;b;l ) =\n(1:8; 1:0; 1:0; 0:2). The horizontal line indicates x=\u0000c\u0000G(1\u0000a)a(1\u0000 e\u0000alD)ba\u00001.\n9\n4.4 Sample Paths\nWe provide in Figure 2 typical sample paths of TS-OU processes of infinite variation with discrete observations, based\non the transition law we have obtained in Theorem 3.1 and the simulation methods described in Algorithm 1, 2 and 3.\nThe model parameters are set l = 0:2, m = 0, a = b = 1 and a = 1:2; 1:5 and 1:8. For simplicity, we set the initial\nstate X0 = 0, that is the mean of the stationary distribution TS(a;a;b)\u0003dm . Sample paths are generated over the time\ninterval either [0; 100] or [0; 200], where the time stepsize is kept D= 0:1 all over in common. This means that 1000\nand 2000 recursive increments are needed, respectively, for the intervals [0; 100] and [0; 200].\nIn our parameter setting, acceptance rates in Algorithm 1 (to generate one sample of Y02) are 0:889, 0:931 and\n0:891, respectively, while in Algorithm 2 (to generate one sample of Q1), acceptance rates are, respectively, 0:990,\n0:993 and 0:997. In Algorithm 3 (to generate one sample of Y01), we have chosen the truncation point c = 0:3, 0:6\nand 1:4, respectively, for a = 1:2, 1:5 and 1:8, on the basis of the criterion DKS(D;c)=(D \u0001 p3(D;c)), as discussed in\nSection 4.3. With the choice of truncation point c, acceptance rates in Algorithm 3 are 0:831, 0:588 and 0:276. Even\nin the case a = 1:8 with the lowest acceptance rate 0:276 in Algorithm 3, each sample path can be generated within\n0.1 second by Scilab software on a computer with recent regular spec. (The computing time can easily be reduced by\nemploying a low-level language, such as C.)\n5 Concluding Remarks\nWe have derived exact transition law between consecutive observations of TS-OU processes of infinite variation as\na convolution of three random components; a compound Poisson distribution and two tempered stable distributions,\none with stability index in (0;1) and the other with index in (1;2). We have adopted acceptance-rejection sampling\ntechniques to simulate exactly the compound Poisson component and the tempered stable distribution with index in\n(0;1). For simulation of the tempered stable distribution with index in (1;2), we have presented an approximative\nacceptance-rejection sampling method of [1] with discussion on the issue of trade-off between distribution error and\ncomputing time. Sample paths simulation is significantly more efficient with our explicit transition law and simulation\ntechniques, relative to the known approximative method based on infinite shot noise series representation of tempered\nstable Le\u00b4vy processes.\nAs mentioned in Section 4.3, we could think of several approximative simulation techniques for the tempered\nstable distribution with stability index in (1;2). Those techniques are investigated in [10]. Also, with the explicit\ntransition density functions of TS-OU processes, it is certainly worthwhile to investigate related statistical issues, such\nas the local asymptotic behavior of the likelihood ratio statistics, efficient parameter estimation, and so on. These\ntopics will be investigated in subsequent papers.\nAcknowledgements\nThe authors would like to appreciate an anonymous referee for valuable suggestions on the computation part.\nReferences\n[1] Baeumer, B., Meerschaert, M.M. (2010) Tempered stable Le\u00b4vy motion and transit super-diffusion, Journal of Computational and Applied\nMathematics, 233(10) 2438-2448.\n[2] Barndorff-Nielsen, O.E., Shephard, N. (2001) Non-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in financial eco-\nnomics (with discussion), J. R. Statist. Soc. B, 63(2) 167-241.\n[3] Benth, F.E., Gorth, M., Kufakunesu, R. (2007) Valuing volatility and variance swaps for a non-Gaussian Ornstein-Uhlenbeck stochastic\nvolatility model, Applied Mathematical Finance, 14(4) 347-363.\n[4] Brockwell, P.J., Davis, R.A., Yang, Y. (2007) Estimation for nonnegative Le\u00b4vy-driven Ornstein-Uhlenbeck processes, Journal of Applied\nProbability, 44(4) 977-989.\n[5] Chambers, J.M., Mallows, C.L., Stuck, B.W. (1976) A method for simulating stable random variables, Journal of the American Statistical\nAssociation, 71(354) 340-344.\n[6] Devroye, L. (2009) Random variate generation for exponential and polynomially tilted stable distributions, ACM Transactions on Modeling\nand Computer Simulation, 19(4) Article No. 18.\n[7] Imai, J., Kawai, R. (2011) On finite truncation of infinite shot noise series representation of tempered stable laws, Physica A, 390(23-24)\n4411-4425.\n10\n[8] Jongbloed, G., van der Meulen, F.H., van der Vaart, A.W. (2005) Nonparametric inference for Le\u00b4vy-driven Ornstein-Uhlenbeck processes,\nBernoulli, 11(5) 759-791.\n[9] Kawai, R., Masuda, H. (2011) Exact discrete sampling of finite variation tempered stable Ornstein-Uhlenbeck processes, Monte Carlo\nMethods and Applications, 17(3) 279-300.\n[10] Kawai, R., Masuda, H. (2011) On simulation of tempered stable random variates, Journal of Computational and Applied Mathematics,\n235(8) 2873-2887.\n[11] Masuda, H. (2004) On multidimensional Ornstein-Uhlenbeck processes driven by a general Le\u00b4vy process, Bernoulli, 10(1) 1-24.\n[12] Michael, J.R., Schucany, W.R., Haas, R.W. (1976) Generating random variates using transformations with multiple roots, The American\nStatistician, 30, 88-90.\n[13] Rosin\u00b4ski, J. (2001) Series representations of Le\u00b4vy processes from the perspective of point processes, In: Le\u00b4vy Processes - Theory and\nApplications, Eds. Barndorff-Nielsen, O.-E., Mikosch, T., Resnick, S.I., Birkha\u00a8user, 401-415.\n[14] Rosin\u00b4ski, J. (2007) Tempering stable processes, Stochastic Processes and their Applications, 117(6) 677-707.\n[15] Sato, K. (1999) Le\u00b4vy processes and infinitely divisible distributions, Cambridge University Press.\n[16] Sun, S., Zhang, X. (2009) Empirical likelihood estimation of discretely sampled processes of OU type, Science in China Series A: Mathe-\nmatics, 52(5) 908-931.\n[17] Tweedie, M.C.K. (1984) An index which distinguishes between some important exponential families, In: Statistics: Applications and New\nDirections: Proc. Indian Statistical Institute Golden Jubilee International Conference (eds. J. Ghosh and J. Roy) 579-604.\n[18] Zhang, S., Zhang, X. (2008) Exact simulation of IG-OU processes, Methodology and Computing in Applied Probability, 10(3) 337-355.\n[19] Zhang, S., Zhang, X. (2009) On the transition law of tempered stable Ornstein-Uhlenbeck processes, Journal of Applied Probability, 46(3)\n721-731.\n[20] Zolotarev, V.M. (1986) One-Dimensional Stable Distributions, American Mathematical Society, Providence, RI.\n11\n0 20 40 60 80 100\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n4\n0 50 100 150 200\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n4\na = 1:2, t 2 [0;100] a = 1:2, t 2 [0;200]\n0 20 40 60 80 100\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n4\n5\n0 50 100 150 200\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n4\n5\na = 1:5, t 2 [0;100] a = 1:5, t 2 [0;200]\n0 20 40 60 80 100\n\u22128\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n0 50 100 150 200\n\u22128\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\na = 1:8, t 2 [0;100] a = 1:8, t 2 [0;200]\nFigure 2: Typical sample paths of tempered stable Ornstein-Uhlenbeck processes through exact simulation algorithm.\nThe horizontal dotted lines indicate X0 = 0(= limt\"+\u00a5E[Xt ]).\n12\n"}