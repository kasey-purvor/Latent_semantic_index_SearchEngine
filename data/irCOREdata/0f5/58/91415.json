{"doi":"10.1145\/330560.330853","coreId":"91415","oai":"oai:eprints.lancs.ac.uk:11579","identifiers":["oai:eprints.lancs.ac.uk:11579","10.1145\/330560.330853"],"title":"Supporting Video in Heterogeneous Mobile Environments","authors":["Yeadon, Nick","Davies, Nigel","Friday, Adrian","Blair, Gordon S."],"enrichments":{"references":[{"id":16517704,"title":"1; \u201cInformation Technology - Coding of audio-visual objets\u201d, Working Draft (WD) ISO\/IEC","authors":[],"date":"1997","doi":null,"raw":"ISO  IEC  JTC  1;  \u201cInformation  Technology  -  Coding  of audio-visual objets\u201d, Working Draft (WD) ISO\/IEC 14496-1, 1997.","cites":null},{"id":16517701,"title":"1; \u201cInformation Technology - Coding of Moving Pictures and Associated Audio for Digital Storage Media up to about","authors":[],"date":"1993","doi":null,"raw":"ISO  IEC  JTC  1;  \u201cInformation  Technology  -  Coding  of Moving  Pictures  and  Associated  Audio  for  Digital  Storage Media  up  to  about  1.5Mbit\/s\u201d,  International  Standard  (IS) ISO\/IEC IS 11172, 1993.","cites":null},{"id":16517680,"title":"A Network Emulator to Support the Development of Adaptive Applications\u201d,","authors":[],"date":"1995","doi":null,"raw":"N.  Davies,  G.  Blair,  K.  Cheverst  and  A.  Friday,  \u201cA Network  Emulator  to  Support  the  Development  of  Adaptive Applications\u201d,  Proceedings  of  2nd  USENIX  symposium  on Mobile  and  Location  Independent  Computing,  Ann  Arbur, U.S.A., April 1995.","cites":null},{"id":16517269,"title":"Architectural Considerations for a New Generation of Protocols\u201d,","authors":[],"date":"1990","doi":null,"raw":"D.  Clark  and  D.  Tennenhouse,  \u201cArchitectural Considerations  for  a  New  Generation  of  Protocols\u201d, Proceedings  of  ACM  Sigcomm  '90,  Philadelphia, Pennsylvania, September 1990, pp. 200-208","cites":null},{"id":16517732,"title":"Development of WaveLAN, an ISM band wireless LAN\u201d,","authors":[],"date":"1993","doi":null,"raw":"B.  Tuch,  \u201cDevelopment  of  WaveLAN,  an  ISM  band wireless  LAN\u201d,  AT  &  T  Technical  Journal,  pp.  27-37, July\/August 1993.","cites":null},{"id":16517735,"title":"Filters: QoS Support Mechanisms for Multipeer Communications\u201d,","authors":[],"date":"1996","doi":null,"raw":"N.  Yeadon,  F.  Garcia,  D.  Hutchison  and  D.  Shepherd, \u201cFilters:  QoS  Support  Mechanisms  for  Multipeer Communications\u201d,  IEEE  Journal  on  Selected  Areas  in Computing  (JSAC)  special  issue  on  Distributed  Multimedia Systems  and  Technology,  Vol.  14,  No.  7,.  Pp.  1245-1262, September 1996.","cites":null},{"id":16517691,"title":"Guide context-sensitive mobile multimedia visitor guide, in collaboration with Lancaster Tourist Information, (EPSRC project GL\/L05280), further details: www.lancs.ac.uk\/computing\/research\/mpg\/most\/guide.html","authors":[],"date":null,"doi":null,"raw":"Guide context-sensitive mobile multimedia visitor guide, in collaboration with Lancaster Tourist Information, (EPSRC project GL\/L05280), further details: www.lancs.ac.uk\/computing\/research\/mpg\/most\/guide.html","cites":null},{"id":16517714,"title":"Mobile Multimedia collaborative project with Simoco International Ltd, Barclay Associates Ltd,","authors":[],"date":null,"doi":null,"raw":"Mobile  Multimedia  collaborative  project  with  Simoco International  Ltd,  Barclay  Associates  Ltd,  HW Communications  Ltd,  (EPSRC  LINK  PCP  project GR\/K82024), further details: www.lancs.ac.uk\/computing\/research\/mpg\/most\/mm_project.html","cites":null},{"id":16517723,"title":"Overview of the GSM System and Protocol Architecture\u201d,","authors":[],"date":"1993","doi":null,"raw":"M.  Rahnema,  \u201cOverview  of  the  GSM  System  and Protocol Architecture\u201d, IEEE Communication Magazine, Vol. 31, Issue. 4, pp. 92-100, April 1993.","cites":null},{"id":16517686,"title":"Programme DE\/RES - 0601, Subtechnical Committee (STC) RES 06, \u201cRadio Equipment Systems (RES); Trans-European Trunked Radio (TETRA); Voice plus Data (V+D), Designer's Guide\u201d,","authors":[],"date":"1995","doi":null,"raw":"ETSI  Work  Programme  DE\/RES  -  0601,  Subtechnical Committee (STC) RES 06, \u201cRadio Equipment Systems (RES); Trans-European  Trunked  Radio  (TETRA);  Voice  plus  Data (V+D), Designer's Guide\u201d, May 1995.","cites":null},{"id":16517727,"title":"RTP: A Transport Protocol for Real-Time Applications\u201d,","authors":[],"date":null,"doi":null,"raw":"H.  Schulzrinne,  S.  Casner,  R.  Frederick,  V.  Jacobson, \u201cRTP:  A  Transport  Protocol  for  Real-Time  Applications\u201d, Internet RFC 1889, 01\/25\/1996.","cites":null},{"id":16517264,"title":"Very Low Bitrate Videocoding using H.263 and Foreseen Extensions\u201d,","authors":[],"date":"1996","doi":null,"raw":"G.  Bjontegaard,  \u201cVery  Low  Bitrate  Videocoding  using H.263  and  Foreseen  Extensions\u201d,  Proceedings  of  European Conference  on  Multimedia  Applications,  Services  and Techniques (ECMAST \u201996), Louvain-la-Neuve, Belgium, May 1996, pp 825-838.","cites":null},{"id":16517710,"title":"Video Codec for Audiovisual Services at px64 kbits\u201d, International Telecommunications Union, Telecommunications Standardisation Sector, ITU-T Recommendation H.261,","authors":[],"date":"1993","doi":null,"raw":"ITU-T H.261, \u201cVideo Codec for Audiovisual Services at px64  kbits\u201d,  International  Telecommunications  Union, Telecommunications  Standardisation  Sector,  ITU-T Recommendation H.261, March 1993. [9]  ITU-T  H.263  \u201cVideo  coding  for  low  bit  rate communication\u201d  International  Telecommunications  Union, Telecommunications  Standardisation  Sector,  ITU-T Recommendation H.263 March 1996.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"1998-01","abstract":"High quality digital wireless networks and advances in multimedia compression schemes now permit the transmission of video streams over mobile networks. This opens up video communications to a new range of mobile users. We present our experiences of integrating very-low-bit-rate video encoding into a heterogeneous mobile environment as part of an ongoing project to provide multimedia support for the emergency services. This paper focuses on the features required to enable open working between a variety of applications, end-systems and networks and the performance of a very low bit rate encoder","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/91415.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/11579\/1\/SAC98%2D3.pdf","pdfHashValue":"9526adb774f8843a259607c1cdddfe67e5f66595","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:11579<\/identifier><datestamp>\n      2017-09-21T01:54:17Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D636F6E666572656E63655F6974656D<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Supporting Video in Heterogeneous Mobile Environments<\/dc:title><dc:creator>\n        Yeadon, Nick<\/dc:creator><dc:creator>\n        Davies, Nigel<\/dc:creator><dc:creator>\n        Friday, Adrian<\/dc:creator><dc:creator>\n        Blair, Gordon S.<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        High quality digital wireless networks and advances in multimedia compression schemes now permit the transmission of video streams over mobile networks. This opens up video communications to a new range of mobile users. We present our experiences of integrating very-low-bit-rate video encoding into a heterogeneous mobile environment as part of an ongoing project to provide multimedia support for the emergency services. This paper focuses on the features required to enable open working between a variety of applications, end-systems and networks and the performance of a very low bit rate encoder.<\/dc:description><dc:date>\n        1998-01<\/dc:date><dc:type>\n        Conference or Workshop Item<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/11579\/1\/SAC98%2D3.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1145\/330560.330853<\/dc:relation><dc:identifier>\n        Yeadon, Nick and Davies, Nigel and Friday, Adrian and Blair, Gordon S. (1998) Supporting Video in Heterogeneous Mobile Environments. In: ACM symposium on Applied Computing (SAC '98), 1998-02-271998-03-01.<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/11579\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1145\/330560.330853","http:\/\/eprints.lancs.ac.uk\/11579\/"],"year":1998,"topics":["QA75 Electronic computers. Computer science"],"subject":["Conference or Workshop Item","PeerReviewed"],"fullText":" SUPPORTING VIDEO IN HETEROGENEOUS MOBILE \nENVIRONMENTS \n \nNicholas Yeadon, Nigel Davies, Adrian Friday, Gordon Blair \nLancaster University, Lancaster, UK, LA1 4YR \n{njy, nigel, adrian, gordon}@comp.lancs.ac.uk \n \n \nKEYWORDS: H263, VLBR Video, Heterogeneity,  \nOverlay Networks.  \n  \nABSTRACT \nHigh quality digital wireless networks and advances in \nmultimedia compression schemes now permit the transmission \nof video streams over mobile networks. This opens up video \ncommunications to a new range of mobile users. We present \nour experiences of integrating very-low-bit-rate video encoding \ninto a heterogeneous mobile environment as part of an ongoing \nproject to provide multimedia support for the emergency \nservices. This paper focuses on the features required to enable \nopen working between a variety of applications, end-systems \nand networks and the performance of a very low bit rate \nencoder. \n1. INTRODUCTION \nAdvances in digital wireless technologies and improved \nmultimedia compression algorithms are enabling a \nconvergence of high requirement applications and low \ncapability networking. The higher throughput and lower error \nrates of the new range of digital wireless networks such as \nGSM [11], WaveLAN [13], and the emerging TETRA [4] \nprivate mobile radio standard provide the opportunity to deliver \nlow frame rate compressed video to mobile users.  \nIn parallel, improvements in processor power have enabled \nmany mobile computer systems to be able to source and sink \nmultimedia streams. This can be attributed to these powerful \ncomputers being able to perform complex and processor \nintensive compression functions, the emergence of specialised \ncodec hardware and advances in the development and \nstandardisation of compression algorithms. Currently under \nstandardisation are, the recently completed, H.263 [9] and, in \nfinal working draft, MPEG 4 [7] very low bit rate (VLBR) \nvideo compression schemes. These compression algorithms are \nintended for bit-rates less than 64Kbit\/s. \nThis paper describes a prototype system developed within our \nlaboratory which provides video conferencing and remote \nsurveillance functionality to mobile users equipped with pen \nbased tablets and GSM, TETRA or WaveLAN based \nnetworking. This prototype has been designed as part of a \nproject to develop a new range of services and collaborative \napplications for the future users of the recently standardised \nTErrestrial (formerly Trans European) Trunked RAdio system \n(TETRA), namely the emergency services, customs officials, \nutility workers and delivery and freight organisations. The \n \n \n \n \n \nsystem we have developed demonstrates the initial possibilities \nof the new technology. \nTogether with a detailed description of the application \nprototype the paper also describes the supporting infrastructure \nwhich addresses the technical issues of interworking between \nfixed and wireless users, video transmission and late entry, i.e. \nentering an already established data flow. Finally, the paper \nprovides performance measurements and reports on our \npractical experiences of using H.263 in a heterogeneous mobile \nenvironment. \n2. MOTIVATION  \nWe are currently undertaking a project to develop mobile \nmultimedia services and applications for members of the \nemergency services [10]. As part of this work we have \nconducted a requirements study which has identified the \nfollowing key requirements: \n\u2022 Establishment and management of multiple dynamic \ngroups. \n\u2022 Multiparty integrated voice and data transmission. \n\u2022 Exchange of multimedia information, including still \nimages, slow-scan and potentially real-time video. \n\u2022 Interoperability with other agencies and Emergency \nServices. \n\u2022 Support for interworking in heterogeneous \narchitectures. \n\u2022 Security of data and physical end-systems. \n\u2022 High levels of availability and dependability. \n\u2022 Automated locality and status reporting. \nThese requirements can be illustrated by considering the \nfollowing application scenario in which a major incident such \nas a bank robbery is taking place. In an ideal world, where all \nsystems are open and interoperable, the police would be able to \npatch directly into the surveillance cameras within the bank \nfrom their control centre using high-speed land-lines. The \nvideo images from these cameras could then be transmitted via \nappropriate interworking units (between the fixed and wireless \nnetwork) to police en route and attending the incident. The \nofficer(s) co-ordinating the operation could also control the \ncamera from which these images originate (e.g. panning or \ntilting the camera). The control centre would be able to capture \nan image of one of the robbers faces that they might relay in \ndigital form directly to police headquarters for identification. \nThe technology to realise such a scenario is now available, but \na significant remaining challenge is to integrate these \ntechnologies in a heterogeneous environment. In the above \nexample, high quality MPEG 1 or 2 compressed video may be \ntransmitted via a high speed ATM connection between the \nbank and the Police control centre. The control centre may \nutilise high capability workstations and mainframes as end-\nsystems to analyse and process the video stream. From there \nlow quality H.263 or MPEG 4 video may be transmitted via \nGSM or TETRA to mobile PCs at the scene of the crime. Once \ninside the bank, special response units may be able to receive \nbetter quality video on head up displays transmitted to them via \nthe building\u2019s own wireless LAN (e.g. WaveLAN). This \nscenario could quite easily be transposed to a number of \ndifferent domains each having the same characteristics of a \ngroup of users with disparate end-systems dynamically \nroaming across different capability overlay networks accessing \na multitude of information objects, (e.g. a tourist application \n[5]). \nWe have designed a demonstration system based on the \nscenario above that exploits current technology to deliver \ncompressed video at speeds and quality suitable for a range of \nwired and wireless networks. The next section gives an \nintroduction to the encoding scheme used to compress the \nvideo in the prototype system, namely H.263.  \n3.  H.263 VIDEO COMPRESSION STANDARD \nIn the application prototype described in the next section we \nhave made use of the freely available H.263 encoding and \ndecoding algorithms from Telenor R&D [1]. The following \nsection describes the key features of H.263 relevant to the work \npresented here. Some knowledge of existing compression \nschemes is assumed.  \nThe H.263 standard is not radically different from previous \nvideo compression standards such as MPEG 1 [6] and H.261 \n[8]; the main element of H.263 is still a block based discrete \ncosine transform (DCT). In many ways H.263 can be viewed as \nan enhanced or optimised H.261 scheme. The main differences \nlie in more efficient ways of coding DCT coefficients (with \nimproved adaptive Variable Length Code (VLC) tables), \nimproved entropy encoding and increased precision motion \ncompensation.  \n3.1. Overview of H.263 \nIn common with H.261, H.263 has two main frame types. In \nadditonal to H.261 it also has an optional third frame type. The \ntwo primary frames are either intra-coded frames (I-frames), \nencoded entirely without reference to any other frames, and \ninter-coded or predictive frames (P-frames), predicted off intra-\ncoded reference frames. The third type of frame is derived \nfrom MPEG bidirectionally predicted frames (B-frames) called \na PB frame. A PB-frame is two pictures coded as one unit. The \nfrequency at which I-frames are placed into a stream affects the \nresilience to the more catastrophic errors, such as total frame \nlosses; in effect a new I-frame flushes the decoding process.  \nPicture \nFormat  \nlum \npels\/line \nlum \nno of lines \nchrom \npels\/line  \nchrom \nno of \nlines \nsub-QCIF 128 96 64 48 \nQCIF 176 144 88 72 \nCIF 352 288 176 144 \n4CIF 704 576 352 288 \n16CIF 1408 1152 704 576 \nTable 1: H.263 standardised image formats \nAs with H.261, H.263 has a fixed set of frame sizes that can be \nencoded. H.263 has five image size formats: sub-QCIF (sub \nQuarter Common Interchange Format), QCIF, CIF (optional), \n4CIF (optional), and 16CIF (optional). In comparison H.261 \nonly supports Q-CIF and the optional CIF size images. The \nvarious image sizes (for the luminance and chrominance \ncomponents) are shown in Table 1 [9].  \n3.2. Symmetry of Coding \nThe H.263 codec has been designed to be relatively \nsymmetrical, in that the encoding process should not be \nsignificantly more complex or processor intensive than the \ndecoding process. This is to ensure its suitability for bi-\ndirectional visual communication (i.e. video conferencing).  \n3.3. Resilience \nAn important issue when considering H.263 for wireless \ncommunications is its sensitivity to bit-errors. Obviously as a \nvideo stream is compressed each bit becomes more important \nand the consequence of a single bit-error is of greater \nsignificance. There is a trade-off between the amount of \nredundancy that can be removed from a stream to compress it \nand the amount of redundancy that has to be added to the \nstream to protect it from bit-errors. An interworking gateway \n(see Section 4.3) could conceivably add and remove FEC \ninformation as video streams enter and leave a noisy \nenvironment, respectively. \n4. INTEGRATED TESTBED \nThis section describes our experimental architecture and its \ncomponent parts. This includes the infrastructure, a capture, \ncompression and delivery system, an interworking gateway and \na video transmission protocol. \nA prototype surveillance application has been developed to \ndemonstrate the various components of the system. The \napplication allows groups of users connected by both fixed and \nwireless networks to receive video from multiple surveillance \ncameras. The surveillance cameras are connected to the \ninfrastructure using a range of networking technologies (see \nSection 4.1). \n4.1. Infrastructure \nOur experimental infrastructure is shown in Figure 1. The \nsystem consists of a number of low-cost cameras (Connectix \nQuickCams, denoted by ) mounted in a surveillance \ncapacity connected to fixed and wireless end-systems. These \nend-systems perform video compression functions and act as \nvideo stream sources. \n \nFigure 1: Experimental Infrastructure  \nThe various machines are connected via a range of ethernet and \nATM fixed networks and WaveLAN and GSM networks. We \nalso are able to emulate a TETRA network using a public \ndomain wireless network emulator [3] while awaiting \nprototype TETRA equipment. \n4.2. Video Capture and Control  \nThe surveillance application employs the Telenor R&D H.263 \ncodec software [1]. The original codec software produced \nH.263 files from a raw video file. The H.263 encoding \nsoftware has been substantially altered to include source \ncapture directly from a camera, dynamic remote control of the \nencoding parameters, packetisation of individual compressed \nframes and rate paced transmission. A number of optimisations \nhave been made to improve compression time. For example, if \nthe encoder knows the camera only produces B&W images \nthen the encoding of the chrominance information can be \nperformed more rapidly. The decoder has also been \nsignificantly altered to issue connection requests, read \ncompressed video off the network (instead of from a file), \nmonitor its reception quality, and issue feedback control \nmessages to source and interworking units. The encoding \nsoftware currently runs on Linux based PCs while the decoder \noperates in both Linux and Windows NT environments. \n \n Figure 3: Screen shot of surveillance application  \nThe video source encoder can be controlled by any receiver \nthat has the correct permissions. Feedback messages are used \nto control typical camera adjustments, such as brightness, \ncontrast, etc., and also to zoom and pan the image \nelectronically. The zoom function allows a much larger image \nto be captured at source and the recipient to receive only the \narea of the picture of interest. A much smaller image is \ntherefore transmitted so reducing the bandwidth requirement.  \nThe surveillance application allows a number of streams to be \nreceived and displayed from separate encoders. Each stream is \ntransmitted and displayed as a sub-QCIF until the user selects a \nstream which may be of interest. The video stream is then \ndisplayed in the main, larger, window. The transmitted stream \nwill then be adjusted in accordance with the user\u2019s preferences. \nFor example, preference options include whether to switch to a \nfaster frame rate or to encode the frames with a higher \nresolution and screen size.  \n4.3. Interworking  \nConflicts in requirements will occur when there is a group of \nheterogeneous receivers within the same multipeer \ncommunication session. For example, a mobile receiver within \na multicast group may be experiencing high packet loss and so \nsignal the source to reduce its transmission rate, whereas within \nthe same session another receiver fixed to a wired network may \nbe receiving a perfect transmission and wish to increase the \nframe rate. This has been reported as the multicasting problem \n[14]. For these reasons we have a distinct interworking unit, \nwhich acts as a gateway between wired and fixed networks, \nperforming filtering [14] and adaptive functions where \nnecessary.  \nOur previous work has developed a number of filtering \nmechanisms that can dynamically adapt MPEG 1 and JPEG \ncompressed video streams [14]. This dynamic adaptation is \nperformed not at the source but at key locations within a \nmultipeer dissemination tree called filter servers. Filter agents \non each filter server perform operations on the compressed or \nsemi-uncompressed streams (i.e. operations are applied to the \nrun-length coded DCT coefficients to achieve fast and reactive \nprocessing). Typical filtering mechanisms include frame \ndropping, low-pass filtering, colour-reduction, requantization \nand transcoding.  \nSome of these filtering mechanisms have been integrated into \nthe present surveillance system and perform operations on \nH.263 compressed video. This permits users on a fixed high-\nspeed network to receive full frame-rate high quality video \nwhile other users on low-speed wireless networks may still \nparticipate in the same communication session but receive \nlower rate and quality images. Currently under development is \nan MPEG 1 to H.263 transcoder to enhance interworking \nfunctionality. \nThe interworking gateway is used to filter selected types of \npackets. If a high number of packets are being lost by the \nmobile system then this will produce a corrupt and distorted \nvideo sequence (as many frames are reliant on other frames \nbeing decoded). In such a case the surveillance application can \ninstruct the filtering gateway to discard all frames but the I-\nframes. This will reduce the bit-rate of the stream and produce \na stream that is more resilient to errors, since each frame is \nindependently encoded. Of course the frame is reduced but the \nimage quality will remain high. This does not affect the other \nrecipients on the fixed network. \n4.4. Video Transmission Scheme \nVideo transmission is achieved by using the Continuous Media \nProtocol (CMP) [14] over the fixed network and a Mobile-\nCMP over the wireless network. It includes an Application \nLevel Framing (ALF) [2] media assembly format (above UDP) \nand a rate control mechanism. As all communication is via \ndatagrams, connection state is maintained at the application \nlayer, i.e. at source agents, filter agents and client applications. \nLike the IETF Real-time Transport Protocol (RTP) [12] the \nCMP protocol is embedded into the application.  \nThe packetisation strategy for the Continuous Media Protocol \nis illustrated in Figure 4. This also shows the standard UDP \nheader. In the CMP header there are three important fields: the \npayload type, the sequence number and the time stamp.  \nThe payload type specifies the media type and also sub-types \nof that media. For example, the H.263 video media type used in \nthe prototype system consists of three sub-types: I-picture, P-\npicture and PB-picture. The sequence number identifies the \ntransmission order of the packets in the data stream. It can be \nused to detect lost packets or out-of-order packets. The time \nstamp is the local time (in \u00b5secs) at the source when the packet \nwas generated. Again, it can be used for detecting lost or \ndelayed packets and also for resynchronising incoming packets \nat the end-system.  \nIn the Mobile-CMP packetisation scheme the packet header is \nreduced to 1 or 2 bytes and a selective re-transmission scheme \nis used to enable the protocol to adapt to different media types\u2019 \nQoS requirements.  \n \nFigure 4: Packetisation of Continuous Media Data \nThere are two choices of rate control. The first and simplest is \na frame oriented rate control. That is, the frames are \ntransmitted from the source at regular intervals specified by the \nrequired frame rate. This method maintains the isochrony of \nthe media stream, but a potentially bursty traffic shape is \nproduced. \nThe second choice of rate control is implemented around a \ncredit-based flow control scheme. This method attempts to \nachieve a constant data rate from the source. An output stream \nis allocated a credit token, initially set to a level corresponding \nto the data rate required. When a packet is transmitted, the \ncredit is reduced by an amount related to the size of packet. \nThe credit then increases over time at a rate associated with the \ndata rate required. A packet can only be transmitted if there is \nenough credit available for that size of packet. This allows \nsmall packets to be transmitted frequently and larger packets to \nbe delayed until there are resources available. This method \nproduces a consistent flow of data but has the disadvantage of \nintroducing inter-frame jitter before the data has even left the \nsource. \nThe actual rate control and rate to use is specified by the \nsending entity. This rate can be dependent on information \ngleaned from the receivers. \nThe tagging of each packet with a sequence number and time-\nstamp allows each receiver to monitor the QoS it is receiving, \ni.e. it can detect packet loss, monitor data rates, packet jitter, \netc. By using this information the receivers can issue \nadaptation signals back to the source which may include rate \ncontrol or encoding parameter adjustments (e.g. changing the \nsource encoding parameters).  \n4.5. Late entry  \nIn a group communication it is typical for calls to be set up by \na small number of users and for other recipients to join the \ngroup session at a later time. This is called 'late entry'. When \ntransmitting compressed video where individual frames are \npredicted off other frames there is an inter-dependency \nbetween packets. This means that a user entering a group late \nmay need reference frames that have already been transmitted. \nWithout these reference frames the connection must wait until \nthe next independent object is transmitted, e.g. an intra coded \nreference frame. In some compression schemes, such as \nMPEG, other information is needed to initialise the decoder. \nThis information is carried in Sequence Headers and \ntransmitted very infrequently, sometimes only once. \nFortunately, the H.263 I-frame picture header contains all the \ninformation necessary to initialise a decoder.  \nIf late entry is a desired attribute of an H.263 based \ncommunication session then there are three approaches: I-\nframes must be transmitted relatively frequently and the user \nmust wait for then next reference to be transmitted, the last \nreference frame and any frames since may be transmitted again \nto a joining client to allow it to 'catch up', or the encoder may \nbe forced to generate a new I-frame. The first option may result \nin a larger stream over time due to the increased number of I-\nframes. The second and third options may cause a large burst \nwhen a client joins a group but it has the advantage that larger \nI-frames can be transmitted infrequently. The second option \nimplies there must be an object which caches previous frames \nand the decoder must be able to decode faster than the current \nstream is transmitting (or else it would never catch up).  \nLate entry is especially desirable in mobile environments \nbecause it allows quick recovery from burst errors, losses and \nconnection drop outs that may occur during handovers between \ncells and different overlay networks.  \nThe choice of schemes is dependent upon how dynamic a \ngroup is (i.e. how often clients join and leave a group) and how \nlong users are prepared to wait to start receiving video once \nconnected. The emergency services have stringent \nrequirements relating to the maximum mobile call set-up time \nin emergency situations (less than a quarter of a second for \nvoice traffic). As video communications becomes more \ncommon-place in this environment this may impinge on the \nschemes used for video transmission and encoding. \n5. PERFORMANCE EVALUATION AND ANALYSIS \nWe have conducted a performance evaluation of our system \nfocusing on the H.263 encoder and decoder. The evaluation \nprocedure used a 100MHz Pentium running the LINUX \noperating system to capture images from a greyscale Connectix \nQuickcam camera and then compress and transmit the video \nstream over WaveLAN to a mobile pen based mobile 486 DX2 \nPC running Windows 95. The tests were performed with the \nencoder in a variety of settings. The image sizes used were \nSub-QCIF, QCIF and CIF (see Section 3.1). The quantization \nfactor (which basically sets the image-quality-to-compression \nratio) was set to very high image quality (q=1), average\/normal \nuse (q=10) and low quality (q=31). The I-frame frequency was \nset to 1 in 10 and 1 in 100 frames. The encoder was also tested \nwith different input streams from the camera; the camera was \nleft stationary with no movement in the scene at all (in a \nsurveillance application possibly monitoring a warehouse this \nwould be the normal mode of operation) and the camera was \nmade to pan by physically oscillating from side to side at a rate \nof 7.5\u00b0\/sec (not to be confused with the software pan and zoom \ndescribed earlier).  \nAn important reasoning behind the evaluation stages was to \ntest the system in a real operational environment with varying \nlight conditions, random events, etc. The results predict a best \ncase scenario where the camera is continually monitoring the \nsame unchanging (though relatively complex) scene and worst \ncase, where a camera is panning with potentially (depending on \nthe pan period) a different image in every frame. The figures \nquoted are averages of measurements taken at different times \nover a two week trial period. \nTable 2 shows the speed at which the H.263 encoder can \ncapture, compress and transmit frames (running on a 100MHz \nPentium P100) and the associated bit-rate this produces. This is \nwhen the camera is stationary. \n \n \nQ=10 I-frame = 1\/10 I-frame= 1\/100 \nImage Size Frame \nRate \nBit         \nRate \nFrame \nRate \nBit  \nRate \nSub-QCIF 2.4 fps 3.2 Kbit\/s 2.2 fps 947 bit\/s \nQCIF 1.2 fps 4 Kbit\/s 1.2 fps 631 bit\/s \nCIF 0.29 fps 2.1 Kbit\/s 0.33 fps 422 bit\/s \nTable 2: Frame Rate to Bit Rate (stationary camera) \nTable 2 shows that the encoding process is relatively \nheavyweight. In fact, in high-speed networks the limiting factor \nis the time it takes to encode each frame and not the network \nthroughput. Obviously this is less of an issue in low-speed \nwireless networks. It can be seen from Table 2 that the \nfrequency at which an I-frame is placed into the data stream \nseverely affects the bit-rate. I-frames are much larger than \npredicted frames (P-frames) but provide increased error \nresilience and also reduce the time a user may have to wait for \nlate entry (see Section 4.5). This is of greater significance when \nthe image has little movement since much of the information \ncan be predicted from previous frames. The I-frames also \nprevent a gradual build up of decoding quality losses in such a \ncase.  \nThe slightly higher frame rate in Tables 2 and 3 when the I-\nframes are more frequent are caused by the reduced amount of \nprocessing needed to encode I-frames and hence, as the \nnetwork is not the bottleneck, this results in a higher encoding \nrate. \nQ=10 I-frame = 1\/10 I-frame = 1\/100 \nImage Frame Bit Frame  Bit \nSize Rate Rate Rate Rate \nSub-\nQCIF \n1.2 fps 3.3 Kbit\/s 1.1 fps 2.8 Kbit\/s \nQCIF 0.48 fps 3.3 Kbit\/s 0.5 fps 2.7 Kbit\/s \nCIF 0.12 fps 2.5 Kbit\/s 0.12 fps 1.65 Kbit\/s \nTable 3: Frame Rate to Bit Rate (panning camera) \n \nTable 3 show the case when the camera is panning. Obviously \nthe bit-rate produced is higher as there is significant movement \nin the scene. Also, the additional processing required to encode \nthe changes results in a slower frame rate. The performance \nwhen encoding the larger and slower pictures is made worse \nbecause the camera is oscillating quite fast in relation to the \nframe rate, hence the encoder will see this a much faster \nmotion. In fact, the image is usually completely different from \nthe previous image.  \nThe optional extensions described in the annexes of the H.263 \nstandard, were also tested. Surprisingly, the additional features \ndid not slow down the encoding process significantly. \nHowever, the bit-rate was reduced for each of the three \ndifferent size images as shown in Table 4. \nQ=10 I-frame = 1\/10  \nImage Size Frame Rate Bit-Rate \nSub-QCIF 1.1 fps 2.2 Kbit\/s \nQCIF 0.5 fps 2.2 Kbit\/s \nCIF 0.12 fps 1.9 Kbit\/s \nTable 4: Frame Rate to Bit Rate (panning camera with options \nAnnex D, E & F) \nThe final set of results shows the effect of using different \nquantization scales (quality factor). The previous results used \nthe quantization parameter set to the default value 10 (average \nquality). Table 5 shows the large increase in bit-rate if the \nhighest quality encoding is used. The frame rate also increases \nslightly when encoding with the highest quality as the \nprocessor does not have to quantize each DCT value, i.e. there \nis less processing perform. \nImage size=QCIF I-frame = 1\/10  \nQuantization Scale Frame Rate Bit-Rate \nQ=1 0.59 fps 56 Kbit\/s \nQ=10 0.48 fps 3.3 Kbit\/s \nQ=31 0.46 fps 2.7 Kbit\/s \nTable 5: Frame Rate to Bit Rate (panning camera) different  \nQ-Scales\n  Expected frame rate (panning camera) \nProfiles Max Data Rate Sub-QCIF QCIF CIF \nGSM 9.6 Kbit\/s 3.7 Fps 1.8 Fps 0.68 Fps \nTETRA 1 slot 7.2 Kbit\/s 2.8 Fps 1.3 Fps 0.5 Fps \nTETRA 2 slot 14.4 Kbit\/s 5.7 Fps 2.7 Fps 1 Fps \nTETRA 3 slot 21.6 Kbit\/s 8.5 Fps 4 Fps 1.5 Fps \nTETRA 4 slot 28.8 Kbit\/s 11.3 Fps 5.3 Fps 2.1 Fps \nWaveLAN 2 Mbit\/s 30 Fps (x26) 30 Fps (x12) 30 Fps (x 4) \nTable 6: User-selectable profiles for different network types.\nThe above results allow a set of profiles to be formulated \nwhich correspond to different network types. This set of \nprofiles is shown in Table 6. The table is a relatively simple \none that allows frame rate and image size to be traded-off at \ndifferent network capacities. The frame rates shown are the \nmaximum possible if the network bandwidth is the limiting \nfactor (with a quantization factor of 10). In our current \nencoding system this is not true and the processing capability \nof the end-system is the limiting factor (i.e. the encoding rate). \nIn such a case the quantization parameter may be adjusted so \nthat the data stream fills the communication channel, hence \nproducing higher quality images in the stream. From Table 5 it \ncan be seen that reducing the quantization factor may also \nincrease the frame rate. These additional factors may produce a \nmore complex cross-reference of attributes and trade-off \noptions. Error resilience could also add an additional element \nin a profile table.  \nIn the prototype application described earlier a roaming user \nmay specify a preference for larger image size or faster frame \nrate depending on the current use of the application. For \nexample, if a security officer were engaged in a WaveLAN \nsupported video call with an image size of QCIF and moved \noutside the coverage of his local area network then an \nalternative connection may be established via a slower GSM \nchannel. If the user has specified a preference for a frame rate \nover image size then the negotiation system would opt to drop \nthe image size to Sub-QCIF and maybe alter the quantization \nparameters in order to maintain a reasonably similar frame rate.  \nTable 6 can also be used to estimate the quality a user may \nexpect before establishing a call, and hence chose his medium \n(or number of channels in TETRA) appropriately.    \n6. CONCLUSIONS \nThis paper has reported on work carried out to develop \nprototype mobile multimedia applications to support members \nof the emergency services. In particular we have described a \nprototype video surveillance application and its associated \ninfrastructure. These enable groups of collaborating mobile \nusers to access H.263 encoded video streams produced by a \nnumber of fixed and mobile cameras. Strategies to deal with \nvideo transmission, QoS feedback and late entry are described. \nAn analysis of our prototype environment has enabled us to \nproduce an experimentally based table of expected video QoS \nfor a range of different networks. This table can be used both to \npredict the likely QoS the user can expect and to help specify \nadaptation policies in the face of QoS degradation. A further \nimportant result of our work is that we have demonstrated that \nin the majority of practical cases the quality of H.263 encoded \nvideo received by clients is governed by the performance of the \nend-system performing the compression which in a mobile \nenvironment is likely to be of low capability. \nOur future work in this area will be to develop a robust version \nof our application which will be used in a trial with members of \nthe emergency services. This will enable us to obtain subjective \nfeedback on the suitability of very-low-bit-rate encoded video \nin real application domains.  \n7. REFERENCES \n[1] G. Bjontegaard, \u201cVery Low Bitrate Videocoding using \nH.263 and Foreseen Extensions\u201d, Proceedings of European \nConference on Multimedia Applications, Services and \nTechniques (ECMAST \u201996), Louvain-la-Neuve, Belgium, May \n1996, pp 825-838.  \n[2] D. Clark and D. Tennenhouse, \u201cArchitectural \nConsiderations for a New Generation of Protocols\u201d, \nProceedings of ACM Sigcomm '90, Philadelphia, \nPennsylvania, September 1990, pp. 200-208 \n[3] N. Davies, G. Blair, K. Cheverst and A. Friday, \u201cA \nNetwork Emulator to Support the Development of Adaptive \nApplications\u201d, Proceedings of 2nd USENIX symposium on \nMobile and Location Independent Computing, Ann Arbur, \nU.S.A., April 1995. \n[4] ETSI Work Programme DE\/RES - 0601, Subtechnical \nCommittee (STC) RES 06, \u201cRadio Equipment Systems (RES); \nTrans-European Trunked Radio (TETRA); Voice plus Data \n(V+D), Designer's Guide\u201d, May 1995.  \n[5] Guide context-sensitive mobile multimedia visitor guide, in \ncollaboration with Lancaster Tourist Information, (EPSRC \nproject GL\/L05280), further details: \nwww.lancs.ac.uk\/computing\/research\/mpg\/most\/guide.html \n[6] ISO IEC JTC 1; \u201cInformation Technology - Coding of \nMoving Pictures and Associated Audio for Digital Storage \nMedia up to about 1.5Mbit\/s\u201d, International Standard (IS) \nISO\/IEC IS 11172, 1993. \n[7] ISO IEC JTC 1; \u201cInformation Technology - Coding of \naudio-visual objets\u201d, Working Draft (WD) ISO\/IEC 14496-1, \n1997. \n[8] ITU-T H.261, \u201cVideo Codec for Audiovisual Services at \npx64 kbits\u201d, International Telecommunications Union, \nTelecommunications Standardisation Sector, ITU-T \nRecommendation H.261, March 1993. \n[9] ITU-T H.263 \u201cVideo coding for low bit rate \ncommunication\u201d International Telecommunications Union, \nTelecommunications Standardisation Sector, ITU-T \nRecommendation H.263 March 1996. \n[10] Mobile Multimedia collaborative project with Simoco \nInternational Ltd, Barclay Associates Ltd, HW \nCommunications Ltd, (EPSRC LINK PCP project \nGR\/K82024), further details: \n www.lancs.ac.uk\/computing\/research\/mpg\/most\/mm_project.html \n [11] M. Rahnema, \u201cOverview of the GSM System and \nProtocol Architecture\u201d, IEEE Communication Magazine, Vol. \n31, Issue. 4, pp. 92-100, April 1993. \n[12] H. Schulzrinne, S. Casner, R. Frederick, V. Jacobson, \n\u201cRTP: A Transport Protocol for Real-Time Applications\u201d, \nInternet RFC 1889, 01\/25\/1996. \n[13] B. Tuch, \u201cDevelopment of WaveLAN, an ISM band \nwireless LAN\u201d, AT & T Technical Journal, pp. 27-37, \nJuly\/August 1993. \n[14] N. Yeadon, F. Garcia, D. Hutchison and D. Shepherd, \n\u201cFilters: QoS Support Mechanisms for Multipeer \nCommunications\u201d, IEEE Journal on Selected Areas in \nComputing (JSAC) special issue on Distributed Multimedia \nSystems and Technology, Vol. 14, No. 7,. Pp. 1245-1262, \nSeptember 1996. \n \n \n"}