{"doi":"10.1109\/TCE.2007.4429282","coreId":"102587","oai":"oai:epubs.surrey.ac.uk:2075","identifiers":["oai:epubs.surrey.ac.uk:2075","10.1109\/TCE.2007.4429282"],"title":"Motion and disparity estimation with self adapted evolutionary strategy in 3D video coding","authors":["Adedoyin, S","Fernando, WAC","Kondoz, KM","Aggoun, A"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2007-11","abstract":"Real world information, obtained by humans is three dimensional (3-D). In experimental user-trials, subjective assessments have clearly demonstrated the increased impact of 3-D pictures compared to conventional flat-picture techniques. It is reasonable, therefore, that we humans want an imaging system that produces pictures that are as natural and real as things we see and experience every day. Three-dimensional imaging and hence, 3-D television (3DTV) are very promising approaches expected to satisfy these desires. Integral imaging, which can capture true 3D color images with only one camera, has been seen as the right technology to offer stress-free viewing to audiences of more than one person. In this paper, we propose a novel approach to use Evolutionary Strategy (ES) for joint motion and disparity estimation to compress 3D integral video sequences. We propose to decompose the integral video sequence down to viewpoint video sequences and jointly exploit motion and disparity redundancies to maximize the compression using a self adapted ES. A half pixel refinement algorithm is then applied by interpolating macro blocks in the previous frame to further improve the video quality. Experimental results demonstrate that the proposed adaptable ES with Half Pixel Joint Motion and Disparity Estimation can up to 1.5 dB objective quality gain without any additional computational cost over our previous algorithm.1Furthermore, the proposed technique get similar objective quality compared to the full search algorithm by reducing the computational cost up to 90%. \u00a9 2007 IEEE","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:epubs.surrey.ac.uk:2075<\/identifier><datestamp>\n      2017-10-31T14:04:09Z<\/datestamp><setSpec>\n      74797065733D61727469636C65<\/setSpec><setSpec>\n      6469766973696F6E733D656E67616E64706879736963616C736369656E636573:656C656374726F6E6963656E67696E656572696E67:63637372<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/epubs.surrey.ac.uk\/2075\/<\/dc:relation><dc:title>\n        Motion and disparity estimation with self adapted evolutionary strategy in 3D video coding<\/dc:title><dc:creator>\n        Adedoyin, S<\/dc:creator><dc:creator>\n        Fernando, WAC<\/dc:creator><dc:creator>\n        Kondoz, KM<\/dc:creator><dc:creator>\n        Aggoun, A<\/dc:creator><dc:description>\n        Real world information, obtained by humans is three dimensional (3-D). In experimental user-trials, subjective assessments have clearly demonstrated the increased impact of 3-D pictures compared to conventional flat-picture techniques. It is reasonable, therefore, that we humans want an imaging system that produces pictures that are as natural and real as things we see and experience every day. Three-dimensional imaging and hence, 3-D television (3DTV) are very promising approaches expected to satisfy these desires. Integral imaging, which can capture true 3D color images with only one camera, has been seen as the right technology to offer stress-free viewing to audiences of more than one person. In this paper, we propose a novel approach to use Evolutionary Strategy (ES) for joint motion and disparity estimation to compress 3D integral video sequences. We propose to decompose the integral video sequence down to viewpoint video sequences and jointly exploit motion and disparity redundancies to maximize the compression using a self adapted ES. A half pixel refinement algorithm is then applied by interpolating macro blocks in the previous frame to further improve the video quality. Experimental results demonstrate that the proposed adaptable ES with Half Pixel Joint Motion and Disparity Estimation can up to 1.5 dB objective quality gain without any additional computational cost over our previous algorithm.1Furthermore, the proposed technique get similar objective quality compared to the full search algorithm by reducing the computational cost up to 90%. \u00a9 2007 IEEE.<\/dc:description><dc:date>\n        2007-11<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/epubs.surrey.ac.uk\/2075\/1\/SRF002654.pdf<\/dc:identifier><dc:identifier>\n          Adedoyin, S, Fernando, WAC, Kondoz, KM and Aggoun, A  (2007) Motion and disparity estimation with self adapted evolutionary strategy in 3D video coding   IEEE Transactions on Consumer Electronics, 53 (4).  pp. 1768-1775.      <\/dc:identifier><dc:relation>\n        http:\/\/ieeexplore.ieee.org\/xpls\/abs_all.jsp?arnumber=4429282<\/dc:relation><dc:relation>\n        10.1109\/TCE.2007.4429282<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/epubs.surrey.ac.uk\/2075\/","http:\/\/ieeexplore.ieee.org\/xpls\/abs_all.jsp?arnumber=4429282","10.1109\/TCE.2007.4429282"],"year":2007,"topics":[],"subject":["Article","PeerReviewed"],"fullText":"IEEE Transactions on Consumer Electronics, Vol. 53, No. 4, NOVEMBER 2007 \nContributed Paper \nManuscript received October 15, 2007                                       0098 3063\/07\/$20.00 \u00a9 2007 IEEE \n1768\nMotion and Disparity Estimation with Self Adapted  \nEvolutionary Strategy in 3D Video Coding \nS. Adedoyin, W.A.C. Fernando, A. Aggoun, K.M. Kondoz  \n \nAbstract - Real world information, obtained by humans is \nthree dimensional (3-D). In experimental user-trials, \nsubjective assessments have clearly demonstrated the \nincreased impact of 3-D pictures compared to \nconventional flat-picture techniques. It is reasonable, \ntherefore, that we humans want an imaging system that \nproduces pictures that are as natural and real as things \nwe see and experience every day. Three-dimensional \nimaging and hence, 3-D television (3DTV) are very \npromising approaches expected to satisfy these desires. \nIntegral imaging, which can capture true 3D color \nimages with only one camera, has been seen as the right \ntechnology to offer stress-free viewing to audiences of \nmore than one person.  \nIn this paper, we propose a novel approach to use \nEvolutionary Strategy (ES) for joint motion and disparity \nestimation to compress 3D integral video sequences. We \npropose to decompose the integral video sequence down \nto viewpoint video sequences and jointly exploit motion \nand disparity redundancies to maximize the compression \nusing a self adapted ES. A half pixel refinement algorithm \nis then applied by interpolating macro blocks in the \nprevious frame to further improve the video quality. \nExperimental results demonstrate that the proposed \nadaptable ES with Half Pixel Joint Motion and Disparity \nEstimation can up to 1.5 dB objective quality gain without \nany additional computational cost over our previous \nalgorithm.1Furthermore, the proposed technique get \nsimilar objective quality compared to the full search \nalgorithm by reducing the computational cost up to 90%. \n \nKeywords: 3D Video, Motion Estimation, Video coding \nI. INTRODUCTION \n  \nhere is growing evidence that three-dimensional (3D) \nimaging techniques will have the potential to \nestablish a future mass-market in the fields of (television, \nvideo game) and communications (desktop video \nconferencing). One much discussed application is 3D \n \n   Steven Adedoyin is with the University of Surrey, Guildford, GU2 7XH, \nUK (e-mail: S. Adedoyin @surrey.ac.uk).  \nW.A.C.Fernando is with the University of Surrey, Guildford, GU2 7XH, \nUK (e-mail: W.Fernando@surrey.ac.uk).  \nA.Aggoun is with the school of engineering and design of Brunel \nUniversity UB8 3PH, UK  (amar.aggoun@brunel.ac.uk) \nA.M. Kondoz  is with the University of Surrey, Guildford, GU2 7XH,UK \n(e-mail: A.Kondoz@surrey.ac.uk). \ntelevision. Many different approaches have been adopted \nin attempts to realize free viewing (auto-stereoscopic) 3D \nTV. True auto-stereoscopic 3D visualization systems \nexhibiting parallax in all directions, which allow \naccommodation and convergence to work in unison, are \nideally required. Holography, which demonstrates these \ncharacteristics, continues to be researched by different \ngroups in an effort to produce full color realistic spatial \nimages. However, the requirements for coherent light \nsources, dark room conditions and high mechanical \nstability during recording reduce the practical utility of \nholographic technique for general 3D spatial video \nimaging applications.  \nFollowing the introduction of digital broadcast, 3D TV \nis seen as the next major step forward [2][3].  Most of the \nresearch for the next generation of audio-visual systems \nhas centred around stereoscopic 3D imaging, which \nrequires multiple camera capture and multiple view \ndisplays [4].  In most of these designs, a lenticular sheet \ndecoder or scanning aperture is exploited to enable \ntransmission of image data into the left and right eyes by \nspatial or temporal multiplexing. However, such displays \nare not truly spatial since they exclude vertical parallax \nand rely upon the brain to fuse the two disparate 2D \nimages to create the 3D sensation. A fundamental \nlimitation of stereo systems is that they tend to cause eye \nstrain, fatigue and headaches after prolonged viewing as \nthe user is required to focus to the screen plane but \nconverge their eyes to a point in space, producing a very \nunnatural situation [5]. \nIntegral imaging is a technique that is capable of \ncreating and encoding a true volume spatial optical model \nof the object scene in the form of a planar intensity \ndistribution by using unique optical \ncomponents[6][7][8][9][10][11][12] \n[13][14]. It is akin to holography in that 3D information \nrecorded on a 2-D medium can be replayed as a full 3D \noptical model, however, in contrast to holography, \ncoherent light sources are not required.  This conveniently \nallows more conventional live capture and display \nprocedures to be adopted.  With recent progress in the \ntheory and microlens manufacturing, integral imaging is \nbecoming a practical and prospective 3D display \ntechnology and is attracting much interest in the 3D area \n[6][7][8][9][10][11][12][13][14].  \nTV based on 3D integral imaging video technology, \nthat requires only one camera, will be attractive to service \nT \nAuthorized licensed use limited to: University of Surrey. Downloaded on May 07,2010 at 15:17:52 UTC from IEEE Xplore.  Restrictions apply. \nS. Adedoyin et al.: Motion and Disparity Estimation with Self Adapted Evolutionary Strategy in 3D Video Coding 1769\nproviders because it will seamlessly provide the added \nvalue of three-dimensional realism. This approach is \nattractive to service providers because it avoids the \ncumbersome setting up of more than one camera that \nother 3D capture techniques employ. 3D integral imaging \nencoded video can be designed to be scalable with 2D \nvideo and can be encoded efficiently so as to \neconomically provide attractive high value services over \nhigh value systems. Thus, the development of a 3D \nintegral imaging TV system will also demonstrate how \nadded-value broadband services of this type can be \ndelivered, providing benefit to designers of these type of \nservices in the future. However, lots of work needs to be \ndone in this area specially in the compression of 3D \nintegral imaging video. \nIn 3D integral video compression motion estimations is \nthe most critical part as it takes up to 70%-90% of the \nencoding time and greatly reduces the amount of data \nneeded to represent a video sequence. An exhaustive \nblock matching algorithm is the conventional method \nused to conduct motion estimation. This searches possible \nposition within a specified area. The Full Search block \nmotion estimation (BMA) will find the best match by \nusing a matching criterion such as SAD to compare every \nblock to the current block. However such a method entails \na high computational cost since it involves several view \npoints.  \nIn this paper, we propose a joint motion and disparity \nmotion estimation with a half pixel refinement technique \nusing an adaptable evolutionary strategy for 3D integral \nimaging video. Rest of the paper is organised as follows. \nIn section 2, we summarize some related work on 3D \nintegral imaging. The proposed algorithm is presented in \nsection 3.  Section 4 presents some simulation results and \na detailed analysis. Finally, the conclusions are given in \nsection 5. \nII. RELATED WORK \nIn 1908 Lippmann introduced a system known as \nintegral photography[15].  This is a technique for creating \nfull color optical models, which exist in space \nindependently of the observer.  They are auto-\nstereoscopic and the images exhibit continuous parallax \nthroughout the viewing zone.  The continuous nature of \nthe images produced with this method eliminates the \neffect of 'cardboarding' (flattening of objects into discrete \ndepth planes) and flipping (a visible effect created by \nmoving between image fields) present in multi-view \nstereoscopic systems.  \nThe development of integral 3D imaging has been \ninhibited by: (i) the inherent difficulty in manufacturing \nhigh quality microlens arrays; (ii) orthoscopic integral 3D \nimaging is a two stage process; (iii) images rely upon the \nresolving capability of the microlens array; (iv) Okoshi \n[2] suggested a bandwidth of 42 GHz is necessary for \ntransmission; (v) published theory indicates that integral \n3D images will not reconstruct at the resolutions currently \nattainable in modern electronic display devices[3]. \nA three dimensional imaging arrangement based around \nLippmann\u2019s integral photography technique has been \nreported [6][7][8][9] and is shown in figure 1.  A single \n\u2018camera\u2019 unit encodes a true optical model of the scene as \na single flat intensity distribution suitable for electronic \ncapture. A flat panel display for example one using LCD \ntechnology is used to reproduce the captured intensity \nmodulated image and a microlens array re-integrates the \ncaptured rays to replay the original scene in full color and \nwith continuous parallax in all directions.  This system \noffers the potential for stress-free viewing by more than \none person.  \nIt is possible to capture integral 3D images \nelectronically using a commercially available CCD array. \nThis form of capture requires a high resolution CCD \ntogether with specialised optical components to record the \nmicro-images fields produced by precision micro-optics. \n \nFigure 1 An advanced Integral Imaging system \n The system would record live images in a regular block \npixel pattern. The planar intensity distribution representing \nan integral image is comprised of 2D array of MxM micro \nimages due to the structure of the microlens array used in \nthe capture and replay. The resulting 3D images are termed \nOmnidirection Integral Images (OII) and have parallax in \nall directions. The rectangular aperture at the front of the \ncamera and the regular structure of the hexagonal \nmicrolenses array used in the hexagonal grid (recording \nmicrolens array) gives rise to a regular \u2018brick structure\u2019 in \nthe intensity distribution.  \nIn this paper, the simulation work is carried out on \nunidirectional integral images (UII) which are obtained by \nusing a special case of the integral 3D imaging system \nwhere 1D cylindrical microlens array is used for capture \nAuthorized licensed use limited to: University of Surrey. Downloaded on May 07,2010 at 15:17:52 UTC from IEEE Xplore.  Restrictions apply. \nIEEE Transactions on Consumer Electronics, Vol. 53, No. 4, NOVEMBER 2007 1770\nand replay instead of a 2D array of microlenses.  The \nresulting images contain parallax in the horizontal direction \nonly. Figure 2(a) shows an electronically captured \nunidirectional integral 3D image and figure 2(b) shows a \nmagnified section of the image.   The M vertically running \nbands present in the planar intensity distribution captured \nby the integral 3D camera are due to the regular structure \nof the 1D cylindrical microlens array used in the capture \nprocess. \n \n(a) \n \n(b) \nFigure 2  An electronically captured unidirectional integral image a) \nFull.  b) Magnification \n \nDue to the large amount of data required to represent \nthe captured 3D integral image with adequate resolution, \nit is necessary to develop compression algorithms tailor to \ntake advantage of the characteristics of the recorded \nintegral image.  The planar intensity distribution \nrepresenting an Omnidirectional Integral Image is \ncomprised of 2D array of sub-images due to the structure \nof the microlens array used in the capture and replay. The \nstructure of the recorded integral image intensity \ndistribution is such that a high cross correlation in a third \ndomain, i.e. between the micro-images produced by the \nrecording microlens array, is present. This is due to the \nsmall angular disparity between adjacent microlenses. In \norder to maximise the efficiency of a compression scheme \nfor use with the integral image intensity distribution, both \ninter and intra sub-image correlation should be evaluated.  \nIn the last decade, a Three Dimensional Discrete Cosine \nTransform (3D-DCT) coding algorithm has been \ndeveloped for compression of still 3D integral images \n[19][20][21][22]. The algorithm took advantage of the \nredundancies within the microlens sub-images (intra sub-\nimage coding) and the redundancies between adjacent \nmicrolens sub-images (inter sub-image coding). The use \nof intra and inter sub-image coding resulted in much \nbetter results than those obtained with the baseline JPEG \nwith respect to both image quality and compression ratio. \nThe main advantage of using transform coding is that \nintegral 3D images are inherently divided into small non-\noverlapping blocks referred to as microlens sub-images. \nThis leads to high compression with less blocking \nartefacts.  \n \nIII. Proposed 3D Video Encoder \nIn 3D integral video, each viewpoint video sequence \nrepresents a unique recording direction of the object \nscene. Therefore, the 3D integral video sequence can be \nseparated into its respective distinctive viewpoint videos. \nFigure 3 illustrates the viewpoint extraction for a \nlenticular video of 4 distinctive viewpoints. Columns of \npixels formed by each micro lens representing the similar \nview points are placed near to each other to form the \nviewpoint images. These viewpoint video sequences have \nlots of correlations and can be exploited from the motion \nand disparity estimation.  \n \n \nFigure 3 Viewpoint extraction  \nAuthorized licensed use limited to: University of Surrey. Downloaded on May 07,2010 at 15:17:52 UTC from IEEE Xplore.  Restrictions apply. \nS. Adedoyin et al.: Motion and Disparity Estimation with Self Adapted Evolutionary Strategy in 3D Video Coding 1771\n \nFigure 4 Proposed motion and disparity estimation technique \nThe best motion vectors for each viewpoint can be \nfound by applying a conventional block-matching \nalgorithm. However such a technique would be too \ncomplex and time consuming. Since viewpoint images are \ncaptured by slightly different viewing angles, there is a \ngreat deal of redundancies. Therefore, it is advantageous \nto find a proper set of motion vectors only for a single \nviewpoint and utilize this correlation to minimize the \noverall coding complexity. Compression efficiency can be \nmaximized if disparity correlations amongst the \nviewpoints are also considered. Exploitation of such \nadditional redundancies, however, increases the \ncomputational complexity further. Therefore, there is a \ngreat deal of necessity for an efficient technique to \nexploit most of the existing correlations within lenticular \nvideo at an acceptable computational cost. Following this \nargument, we proposed a joint motion and disparity \nestimation algorithm based on ES to minimize the \ncomputational cost[1]. In this paper we extend the \nprevious work by considering self adaptation with half \npixel refinement to further improve the performance.  \nFigure 4 illustrates the basic coding architecture for \nmotion and disparity estimation. The middlemost \nviewpoint is considered as the base view point and the \nrest of the view points are motion and disparity \ncompensated jointly considering the motion compensated \nviewpoint as the base viewpoint. After coding the base \nviewpoint (i.e. Viewpoint 5) with respect to the base \nviewpoint of the reference frame, adjacent viewpoints are \ncoded taking the corresponding viewpoint from the \nreference frame and the reconstructed version of the base \nviewpoint of the current frame as references. This process \nis repeated for the other viewpoints in the sequence. \nIn this paper we use (\u03bc+\u03bb)-Evolutionary Strategy [26] \nlike in our previous paper [1]. In the proposed ES, each \nchromosome represents three elements of a motion vector. \nThese are x, y which defines the horizontal and vertical \ncomponents of a motion vector, and the reference which \ndefines if the vector resides in a previous frame (in case \nof motion compensation) or viewpoint (in case of a \ndisparity compensation). Each of these components has \nboth an object and strategy gene associated to it. The \nvalues of x and y are determined from the search window \nsize. For example, if the search window is within the \nrange [-8, 8], then the value of object gene can take any \ninteger number inside the range. Strategy gene determines \nif local or global search will be carried out. Smaller value \nof strategy gene mean the search becomes more localised. \nThe importance of a chromosome in a population is \ndefined by the fitness function. This function divides the \nstrongest from the weak. If a chromosome is deemed \nstrongest it will be the 1st selected chromosome for \nproducing offspring within the next generation, as objects \nwith good genes are considered to produce the finest \noffspring. The Fitness function is calculated based on the \nSum of Absolute Difference (SAD).  The Sum of \nAbsolute difference is a widely used search criterion for \nmotion estimation and is calculated by using the \nfollowing formula: \n \n))(,)((\n8\n1\n8\n1\n),(\n),,,(\njsyirx\ni\ni\nj\nj\njyix BA\nsryxSAD\n++++\n=\n=\n=\n=\n++ \u2212\n=\n\u2211\u2211  (1) \n \nWhere yx,  denote the target block A and sr,  donates \nthe offset from the block in the reference frame B.  The \nvertical and horizontal components of a given block type \nis represented by ji, . A chromosome in ES represents the \noffset. Each chromosome in newly generated population \nis evaluated using fitness function. The chromosome with \nthe lowest SAD will be the fittest individual within the \npopulation.  Evolutionary strategy operators have been \ndefined as in [1]. \nA. Population Generation  \nIn order to reduce the number of generations required to \nobtain a satisfactory solution, the initial population \nincludes a number of pre-defined chromosomes. This \nincludes the (0,0) motion vector, in case of zero biased \nmotion i.e. a static background. As all motion is not zero \nbiased, the population also includes the fittest \nchromosomes from spatial adjacent blocks, temporal \nadjacent blocks and includes the disparity related blocks.  \nReference \nFrame \nCurrent \nFrame \nViewpoint 1 \nViewpoint 2 \nViewpoint 3 \nViewpoint 4 \nViewpoint 5  \nBase viewpoint \nMotion Prediction\nDisparity Prediction\n\u2026. \u2026. \nAuthorized licensed use limited to: University of Surrey. Downloaded on May 07,2010 at 15:17:52 UTC from IEEE Xplore.  Restrictions apply. \nIEEE Transactions on Consumer Electronics, Vol. 53, No. 4, NOVEMBER 2007 1772\nThese blocks are selected as the motion vector correlations \nbetween these blocks tend to be similar. \nThe remaining members of the population are made up of \nrandomly generated chromosomes. Without the pre-\ndefined chromosomes the ES algorithm would still find \nthe correct match, but with an increase in the number of \ngenerations hence more search points.  \nThe size of initial population is set to 30, giving the ES \nalgorithm a good representation of adequate regions \nwithin the search window. During the selection process \nthe population size alternates, it combines the parents and \nthe offspring to determine which individuals are deemed \nunfit. These individuals are then purged from the \npopulation, before new offspring\u2019s are created.  \nB. Mutation \nThe Mutation rate defines the percentage of genes to be \nmutated in a newly generated population. These genes are \nrandomly chosen. The higher the mutation rate the higher \nthe number of offspring thus the higher the number of \nsearch points. Experimental results show that a mutation \nrate of 8.5% is sufficient enough. In general an offspring \nis denoted by,  \n  \n),0( nparent\nn\nparent\nn\noffspring xx \u03c3\u039d+=   (2) \n \nWhere  ),0( nparentN \u03c3  an M-dimensional vector of random \nGaussian numbers with zero mean and standard \ndeviations \u03c3.   noffspringx  represents a newly created object \ngene through mutation and nparentx  represents a randomly \nchosen gene from the population. In this illustration \u03c3 \nrepresents the strategy gene, which is dependant on the \nself-adaptation algorithm explained later.  \nC. Self Adaptation \nOne of the main benefits of ES is its self adaptation \ntechniques. Self adaptations benefits can be seen in other \nES research. Experimental results showed that the \nstrategy gene and mutation rates must decrease over time. \nGiven that large parts of the search space needs to be \nexplored to determine areas with low SAD\u2019s. As the \nsearch advances and the best possible values approach \nsmaller values for the strategy gene are needed to adjust \nchromosomes, so that the optimal match is found. \nThe method employed in alternating the strategy gene \nand ensuring that good regions is based on Rechenberg \n1\/5[26] success rule and Greenwood and Zhu Modified\u2019s \nSelf Adaptation [27]. Rechenberg 1\/5 success rule states \nthat the ratio of successful mutations (those in which the \nchild is fitter than the parent) to all mutations should be \n1\/5. Therefore if the ratio is greater than 1\/5 the strategy \ngene should be increased to find alternate good regions \nand if it is less the strategy gene is decreased to \nconcentrate the search more around the given object gene. \nA major stumbling block with the 1\/5 success rule is that \nit causes ES to become stuck at a local minimum. This is \nbecause over time the percentage of successful mutations \nwill drop below 1\/5. Thus causing 1\/5 success rule to \nreduce the strategy gene, which ensures a reduction in the \nsearch space, this in turn causes the search to be stuck on \na local minimum and miss the global minimum. In the \nproposed algorithm we used the following method shown \nin [27] to overcome this problem by modifying the \nstrategy gene. \n \n \n \n(3) \n \n \n \nsp  donates the number of total successful mutations \nover a number of generations. C is a constant its value is \n0.6, and it is used to increase or decrease the strategy \ngene, \u03c3new represents the modified strategy gene, and D \nrepresents half the diameter size of the search space. In \nthis case the search range is 8 therefore D will also equal \n8. D is used as an upper limit so that \u03c3 never exceeds it. If \nsp  is less than 1\/20 then the strategy gene is increased to \nstop the search from falling into local minima. \n \nD. Half Pixel Refinement \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n    After a block matching algorithm has found the best \nmatching motion vector, it is likely that a better match can be \nfound by searching the interpolated region surrounding the \nblock in the reference frame. This is known as sub sampling. \nHalf pixel refinement is a form of sub sampling.  Testing the 8 \nsurrounding sub pixel positions in the reference frame each \nwith a half pixel distance around the obtained motion vector \nFigure 5 Half pixel search positions around the pixel MV \n \n1 3 \n8 6 \n2 \n7 \n5 4 \nMV \nA A \nA A \nB \nC C \nB \n\u23aa\u23aa\u23a9\n\u23aa\u23aa\u23a8\n\u23a7\n=\n\u03c3,D)(\n\u03c3.c\n\u03c3\nc,D)(\n\u03c3 new\n2min\n\/min \u03c3\n20\/1\n5\/120\/1\n5\/1\n5\/1\n<\n<\u2264\n=\n>\ns\ns\ns\ns\npif\npif\npif\npif\nAuthorized licensed use limited to: University of Surrey. Downloaded on May 07,2010 at 15:17:52 UTC from IEEE Xplore.  Restrictions apply. \nS. Adedoyin et al.: Motion and Disparity Estimation with Self Adapted Evolutionary Strategy in 3D Video Coding 1773\nas illustrated in figure 5. These positions are acquired by using \nthe formula as described below. \n \n4\/)(8&6,3,1\n2\/)(54\n2\/)(7&2\nCBAMVPositions\nMVCPostions\nMVBPositions\n+++=\n+=+\n+=\n (4) \n \nThe formula illustrated above is then used to \nconstruct an interpolated block. The SAD of this block is \nthen compared to the original. If it is less, it is then \nchosen as the new motion vector. \n \nIV. RESULTS \nProposed adaptable ES with joint motion and disparity \nestimation algorithm is implemented in a 3D-DCT integral \nimage codec based on the architecture described in [19]. \nAn adaptive arithmetic coder is used for the entropy \ncoding and the quantizer step size ranged from 10 \u2013 50. A \npopulation size of 30 was used for ES. Peak Signal-to-\nNoise Ratio (PSNR) of the luminance signal and its bit \nrate are used to measure the objective quality. For all \nsimulations, all other simulation parameters have been \nselected as in [1]. \nFigures 6-10 show the objective quality comparison of \nthe proposed ES based joint motion and disparity \nestimation technique with half pixel refinement (denoted \nas proposed) for the \u2018Room\u2019 integral video test sequence \nof image size 512\u00d7512 against three reference cases \nnamely:  \n(i) Motion only full search (FS-MOTION) \u2013 \nmotion compensated prediction is used and the \nmotion vectors are calculated using the full \nsearch algorithm. \n(ii) Motion only ES search (ES-MOTION) \u2013 as \nabove except full search is replaced with ES \nsearch. \n(iii) Joint motion and disparity full search (FS-\nJM&D) \u2013 joint motion and disparity \ncompensated prediction. \n(iv) Joint motion and disparity with ES (ES-\nJM&D). \n \nFigure 6-8 depicts the relative performance of the \nabove algorithms for selected viewpoints and Figure 9-10 \ndoes for the entire frame. From Figure 6, it is clear that \nthe full search algorithm outperformed our previous \nalgorithm in PSNR even though the computational cost \nhas been reduced significantly [1]. However, Figure 7 and \n8 show that the algorithm proposed in this paper gives a \n1.5dB PSNR gain over FS-Motion and better quality over \nthe previously proposed FS-JM&D without adding any \ncomplexity to the algorithm. Figure 9-10 show similar \nperformance improvements for the whole sequence as \nwell. \n \nViewpoint 4\n25\n27\n29\n31\n33\n35\n37\n39\n41\n43\n0.2 0.4 0.6 0.8 1 1.2 1.4 1.6Bitrate (Bpp)\nPS\nN\nR\n (d\nB\n)\nFS-MOTION\nFS-JM&D\nES-MOTION\nES-JM&D\n \nFigure 6 Quality comparisons for viewpoint 4 with ES and full search \nViewpoint 6\n29\n31\n33\n35\n37\n39\n41\n43\n0.2 0.4 0.6 0.8 1 1.2 1.4 1.6Bitrate  (Bpp)\nPS\nN\nR\n (d\nB\n)\nFS-JM&D\nProposed\nFS-Motion\n \nFigure 7 Quality comparison for viewpoint 6 \n \nViewpoint 4\n25\n27\n29\n31\n33\n35\n37\n39\n41\n43\n0.2 0.4 0.6 0.8 1 1.2 1.4 1.6Bitrate  (Bpp)\nPS\nN\nR\n (d\nB\n)\nFS-Motion\nProposed\n \nFigure 8 Performance comparison of the proposed algorithm for \nViewpoint 4 \n \nAuthorized licensed use limited to: University of Surrey. Downloaded on May 07,2010 at 15:17:52 UTC from IEEE Xplore.  Restrictions apply. \nIEEE Transactions on Consumer Electronics, Vol. 53, No. 4, NOVEMBER 2007 1774\nIntegral\n29\n31\n33\n35\n37\n39\n41\n43\n0.2 0.4 0.6 0.8 1 1.2 1.4 1.6Bitrate (Bpp)\nPS\nN\nR\n (d\nB\n)\nFS-JM&D\nProposed\n \nFigure 9 Quality comparison of the average video sequence \n \nIntegral\n29\n31\n33\n35\n37\n39\n41\n43\n0.2 0.4 0.6 0.8 1 1.2 1.4 1.6Bitrate  (Bpp)\nPS\nN\nR\n (d\nB\n)\nFS-JM&D\nProposed\nFS-Motion\n \nFigure 10 Quality comparison of the average video sequence \n \nTable 1 Complexity comparison of the proposed algorithm  \nLooking at Figures 6-10 it can be seen that the proposed \nmethod gives better coding performance over FS-JM&D and \nES-JM&D. Furthermore, Table 1 shows that it can also reduce \nthe coding complexity by over 90% while increasing the video \nquality. \nV. CONCLUSION \nIn this paper, we propose a self-adaptable ES algorithm for \njoint motion and disparity estimation in 3D integral video to \nminimize the computational complexity. The video quality is \nfurther improved by half pixel refinement.  Experimental \nresults show that the proposed algorithm can achieve similar \nPSNR with 90% reduced complexity compared to the full \nsearch algorithm. It also gives better quality gain with better \ncoding performance over the previous method [1].  In future, \nwe will be considering early termination criteria for the ES to \nfurther improve the performance without significantly \nreducing the quality. \nREFERENCES \n[1] S. Adedoyin, W.A.C. Fernando, A. Aggoun,  \u201cA joint motion & \ndisparity motion estimation technique for 3d integral video compression \nusing evolutionary strategy\u201d IEEE Consumer Electronics, Volume: \n53, Issue: 2, page(s): 732-739, 2007. \n[2] T. Okoshi, T., \u2018Three-Dimensional imaging techniques\u2019, Academic \nPress, Inc., London, UK. 1976. \n[3] L. Onural, et. al., \u2018An assessment of 3dtv technologies\u2019 Proceedings of \nNAB Broadcast Engineering Conference, pp. 456-467, 2006. \n[4] N.A. Dodgson, \u2018Autostereoscopic 3d displays\u2019 IEEE Computer vol. \n38(8),  pp. 31 \u2013 36, 2005. \n[5] M. T.M. Lambooij, W. A. IJsselsteijn, I. Heynderickx: \u201cVisual \ndiscomfort in stereoscopic displays: a review\u201d Proc. of SPIE-IS&T \nElectronic Imaging, SPIE Vol. 6490, 2007. \n[6] N. Davies, M. McCormick and Li. Yang: \u201cThree-dimensional imaging \nsystems: a new development\u201d. Applied Optics. Vol 27, 4520, 1988.  \n[7] M. McCormick, N. Davies, A. Aggoun, M. Brewin: \u201cResolution \nrequirements for autostereoscopic full parallax 3d-tv\u201d. International \nBroadcasting Conference, Amsterdam, Sept. 94. IEE Conference \nPublication No.397, 1994.  \n[8] S. Manolache, A. Aggoun, M. McCormick, N. Davies, S.Y. Kung, \n\u201cAnalytical model of a three-dimensional integral image recording \nsystem that uses circular and hexagonal based spherical surface \nmicrolenses\u201d, Journal of the Optical Society of America. pt A, 18,No.7, \npp 1814-1821, Aug. 2001. \n[9] A. Aggoun: \u2018Pre-processing of integral images for 3d displays\u2019 IEEE \nJournal of Display Technology, Vol. 2. NO. 4, pp. 393-400, 2006. \n[10] J. Arai et. al.: \u201cGradient-index lens-array method based on real time \nintegral photography for three-dimensional images\u201d Applied Optics, No. \n11, pp. 2034-2045, 1998. \n[11] F. Okano et. al.: \u201cReal time pickup method for a three-dimensional \nimage based on integral photography\u201d Applied Optics, 36, No. 7, pp. \n1598-1603, 1997. \n[12] J.S. Jang and B. Javidi, \u201cFormation of orthoscopic three dimensional \nreal images in direct pickup one step integral imaging\u201d,  Optical \nEngineering, Vol. 42(7), pp. 1869-1870, 2003. \n[13] M. Mart\u00ednez-Corral, B. Javidi, R. Mart\u00ednez-Cuenca and G. Saavedra, \n\u201cFormation of real, orthoscopic integral images by smart pixel \nmapping\u201d, Optics Express 13, pp. 9175-9180, 2005. \n[14] B. Javidi, R. Mart\u00ednez-Cuenca, G. Saavedra, and M. Mart\u00ednez-Corral \n\u201cOrthoscopic, long-focal-depth integral imaging by hybrid method\u201d \nProc. of SPIE Vol. 6392, 639203, 2006. \n[15] G. Lippmann, \u2018Eppreuves reversibles donnat durelief\u2019, J. Phys. Paris \n821, 1908. \n[16] R. Martinez-Cuenca, G. Saavedra, M. Martinez-Corral,  B. Javidi, \n\u2018Extended depth-of-field 3-d display and visualization by combination \nof amplitude-modulated microlenses and deconvolution tools\u2019  \nIEEE\/OSA Journal of display technology, Vol. 1 (2),  pp. 321\u2013 327, \n2005. \n[17] F. Okano, H. Hoshino, J. Arai and I. Yuyama, \u2018Real-time pickup \nmethod for a three-dimensional image based on integral photography\u2019, \nApply Optical, Vol. 36, pp.1598-1604, 1997. \n[18] M. Mart\u00ednez-Corral, B. Javidi, R. Mart\u00ednez-Cuenca and G. Saavedra, \n\u2018Formation of real, orthoscopic integral images by smart pixel \nmapping\u2019, Optics Express 13, pp. 9175-9180, 2005. \n[19] A Aggoun: \u2018A 3D DCT compression algorithm for omnidirectional \nintegral images\u2019 ICASSP 2006. \n[20] R. Zaharia, A. Aggoun and M. McCormick, \u2018Adaptive 3d-dct \ncompression algorithm for continuous parallax 3d integral imaging\u2019 \nJournal of Signal Processing: Image Communications, Vol. 17(3), pp. \n231-242 2002. \n[21] R. Zaharia, A. Aggoun and M. McCormick: \u2018Compression of full \nparallax colour integral 3d tv image data based on sub-sampling of \nchrominance components\u2018 Proceedings of Data Compression \nConference, DCC 2001, Snowbird, Utah, USA. IEEE Computer \nSociety, ISBN 0-7695-1031-0, pp. 527,2001. \nNumber of search points  \nFull search Proposed \nComplexity \ngain \nMotion 1024 138 87.5% \nDisparity 3072 302 90.1% \nAuthorized licensed use limited to: University of Surrey. Downloaded on May 07,2010 at 15:17:52 UTC from IEEE Xplore.  Restrictions apply. \nS. Adedoyin et al.: Motion and Disparity Estimation with Self Adapted Evolutionary Strategy in 3D Video Coding 1775\n[22] Forman, M.C. and Aggoun, A., \u2018Quantisation strategies for 3d_dct \nbased compression of full parallax 3d images\u2019, IPA97, Conf. Pub. \nNo.443, IEE, 1997. \n[23] W.U. ChungHong, \u201cDepth Measurement in integral Images\u201d PhD \nThesis, pp. 40-60, 2003. \n[24] Charles Darwin, \u201cThe origin of species: by means of natural selection or \nthe preservation of favoured races in the struggle for life (bantam \nclassic),\u201d Bantam Classics, Reprint. 1999. \n[25] J.M. Fitzpatrick, J.J. Grefenstette, and D. Van-Gucht, \u201cImage \nregistration by genetic search,\u201d Proceedings of Southeastcon 84, \nLouisville, KY, 460-464, Apr 1984. \n[26] Ingo Rechenberg, Evolutionsstrategie '94. Stuttgart: Frommann-\nHolzboog 1994. \n[27] Garrison W. Greenwood, Qiji J. Zhu \u201cConvergence in evolutionary \nprograms with self-adaptation\u201d evolutionary computation, v.9 n.2, \np.147-157, June 2001. \n \n \nS. Adedoyin is a research student at University of Surrey, \nUK. He holds a BEng (Hons) in Microelectronics \nEngineering that was obtained from Brunel University, \nUK. His current research interest lies in 3D integral \nimaging, 3D video coding and H.264. \n \n \n \n \n \n \nW.A.C. Fernando received the B.Sc. Engineering degree \n(First class) in Electronic and Telecommunications \nEngineering from the University of Moratuwa, Sri Lanka \nin 1995 and the MEng degree (Distinction) in \nTelecommunications from Asian Institute of Technology \n(AIT), Bangkok, Thailand in 1997. He completed his \nPhD at the Department of Electrical and Electronic \nEngineering, University of Bristol, UK in February 2001. \nCurrently, he is a senior lecture in signal processing at the University of \nSurrey, UK. Prior to that, he was a senior lecturer in Brunel University, UK \nand an assistant professor in AIT. His current research interests include \nDistribute Video Coding (DVC), 3D video coding, intelligent video encoding \nfor wireless communications, OFDM and CDMA for wireless channels, \nchannel coding and modulation schemes for wireless channels. He has \npublished more than 150 international papers on these areas. He is a senior \nmember of IEEE and a fellow of the HEA, UK. He is also a member of the \nEPSRC College. \n \nA. Aggoun (M\u2019 99) received the \u201cIngenieur d\u2019Etat\u201d \ndegree in electronic engineering from Ecole Nationale \nPolytechnique of Algiers (ENPA) Algeria, in 1986 and \nthe PhD degree in compressed video signal processing \nfrom Nottingham University, UK, in 1991.  From 1991-\n1993 he was with the Nottingham University as a \nresearch fellow in digital video signal processing. From \n1993-2005, he was with De Montfort University, UK, as \na Principle lecturer in Electronic Engineering.  In 2005, \nhe joined the school of Design and Engineering at \nBrunel University (UK) as a Reader in 3D Imaging Technologies.  \nHis current research Interests include computer generation and live capture \nof 3D integral images, 3DTV, 3D visualisation, depth measurement and \nvolumetric data reconstruction from 3D integral images, 3D video coding, \ncomputer vision, and real-time digital image processing architectures. \n \nA.M. Kondoz received the B.Sc. (Hons.) degree in \nengineering, the M.Sc. degree in telematics, and the \nPh.D. in communication in 1983, 1984, and 1986, \nrespectively. He became a Lecturer in 1988, a Reader in \n1995, and then in 1996, a Professor in Multimedia \nCommunication Systems and deputy director of Centre \nfor Communication Systems Research (CCSR), \nUniversity of Surrey, Guildford, U.K. He has over 250 \npublications, including a book on low bit-rate speech coding and several book \nchapters. He has graduated more than 40 Ph.D. students in the areas of \nspeech\/image and signal processing and wireless multimedia \ncommunications, and has been a consultant for major wireless media terminal \ndevelopers and manufacturers. Prof Kondoz has been awarded several prizes, \nthe most significant of which are The Royal Television Societies\u2019 \nCommunications Innovation Award and The IEE Benefactors Premium \nAward. He has been on the Refereeing College for EPSRC and on the \nCanadian Research Councils. He is a member of the IEEE and the IET. \nAuthorized licensed use limited to: University of Surrey. Downloaded on May 07,2010 at 15:17:52 UTC from IEEE Xplore.  Restrictions apply. \n"}