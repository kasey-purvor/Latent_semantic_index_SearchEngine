{"doi":"10.1007\/s11257-010-9082-4","coreId":"177406","oai":"oai:aura.abdn.ac.uk:2164\/2218","identifiers":["oai:aura.abdn.ac.uk:2164\/2218","10.1007\/s11257-010-9082-4"],"title":"Layered evaluation of interactive adaptive systems : framework and formative methods","authors":["Paramythis, Alexandros","Weibelzahl, Stephan","Masthoff, Judith"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":["University of Aberdeen, Natural & Computing Sciences, Computing Science"],"datePublished":"2010-12-01","abstract":"Peer reviewedPostprin","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":null,"rawRecordXml":"<record><header><identifier>\n        \n            \n                oai:aura.abdn.ac.uk:2164\/2218<\/identifier><datestamp>\n                2018-01-02T00:04:27Z<\/datestamp><setSpec>\n                com_2164_673<\/setSpec><setSpec>\n                com_2164_370<\/setSpec><setSpec>\n                com_2164_331<\/setSpec><setSpec>\n                com_2164_705<\/setSpec><setSpec>\n                col_2164_674<\/setSpec><setSpec>\n                col_2164_706<\/setSpec>\n            <\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:doc=\"http:\/\/www.lyncode.com\/xoai\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n            \nLayered evaluation of interactive adaptive systems : framework and formative methods<\/dc:title><dc:creator>\nParamythis, Alexandros<\/dc:creator><dc:creator>\nWeibelzahl, Stephan<\/dc:creator><dc:creator>\nMasthoff, Judith<\/dc:creator><dc:contributor>\nUniversity of Aberdeen, Natural & Computing Sciences, Computing Science<\/dc:contributor><dc:subject>\nlayered evaluation<\/dc:subject><dc:subject>\nevaluation framework<\/dc:subject><dc:subject>\nformative evaluation methods<\/dc:subject><dc:subject>\ndesign<\/dc:subject><dc:subject>\nQA75 Electronic computers. Computer science<\/dc:subject><dc:subject>\nQA75<\/dc:subject><dc:description>\nPeer reviewed<\/dc:description><dc:description>\nPostprint<\/dc:description><dc:date>\n2012-01-26T11:25:02Z<\/dc:date><dc:date>\n2012-01-26T11:25:02Z<\/dc:date><dc:date>\n2010-12-01<\/dc:date><dc:type>\nJournal article<\/dc:type><dc:identifier>\nParamythis , A , Weibelzahl , S & Masthoff , J 2010 , ' Layered evaluation of interactive adaptive systems : framework and formative methods ' User Modelling and User-Adapted Interaction , vol 20 , no. 5 , pp. 383-453 . DOI: 10.1007\/s11257-010-9082-4<\/dc:identifier><dc:identifier>\n0924-1868<\/dc:identifier><dc:identifier>\nPURE: 3659174<\/dc:identifier><dc:identifier>\nPURE UUID: f39f4c66-824b-499b-aff0-49f04f5ee273<\/dc:identifier><dc:identifier>\nScopus: 78650729548<\/dc:identifier><dc:identifier>\nhttp:\/\/hdl.handle.net\/2164\/2218<\/dc:identifier><dc:identifier>\nhttp:\/\/dx.doi.org\/10.1007\/s11257-010-9082-4<\/dc:identifier><dc:language>\neng<\/dc:language><dc:relation>\nUser Modelling and User-Adapted Interaction<\/dc:relation><dc:rights>\nParamythis, A, Weibelzahl, S & Masthoff, J 2010, 'Layered evaluation of interactive adaptive systems: framework and formative methods ', User Modelling and User-Adapted Interaction, vol 20, no. 5, pp. 383-453. The final publication is available at www.springerlink.com, http:\/\/www.springerlink.com\/content\/p676118n878418x7\/ \u00a9 Springer Science+Business Media B.V. 2010<\/dc:rights><dc:format>\n71<\/dc:format>\n<\/oai_dc:dc>\n<\/metadata>\n        <\/record>","journals":[{"title":null,"identifiers":["issn:0924-1868","0924-1868"]}],"language":{"code":"en","id":9,"name":"English"},"relations":["User Modelling and User-Adapted Interaction"],"year":2010,"topics":["layered evaluation","evaluation framework","formative evaluation methods","design","QA75 Electronic computers. Computer science","QA75"],"subject":["Journal article"],"fullText":"Layered Evaluation of Interactive Adaptive Systems: \nFramework and Formative Methods \n \nAlexandros Paramythis \nInstitute for Information Processing and Microprocessor Technology (FIM) \nJohannes Kepler University Linz \nAltenbergerstr. 69 \nA-4040 Linz, AUSTRIA \n+43 732 2468 8442 \nalpar at fim.uni-linz.ac.at \nhttp:\/\/www.fim.uni-linz.ac.at\/staff\/paramythis\/ \n \nStephan Weibelzahl \nNational College of Ireland Dublin \nMayor Street IFSC \nDublin 1 \nIreland \n+353 1 4498 579 \nsweibelzahl at ncirl.ie \nhttp:\/\/www.weibelzahl.de\/ \n \nJudith Masthoff \nUniversity of Aberdeen \nAberdeen AB24 3UE \nScotland, UK \n+44 1224 272299 \nj.masthoff at abdn.ac.uk \nhttp:\/\/www.csd.abdn.ac.uk\/~jmasthof \n \n \nAbstract: The evaluation of interactive adaptive systems has long been acknowledged to be a \ncomplicated and demanding endeavour. Some promising approaches in the recent past have \nattempted tackling the problem of evaluating adaptivity by \u201cdecomposing\u201d and evaluating it \nin a \u201cpiece-wise\u201d manner. Separating the evaluation of different aspects can help to identify \nproblems in the adaptation process. This paper presents a framework that can be used to guide \nthe \u201clayered\u201d evaluation of adaptive systems, and a set of formative methods that have been \ntailored or specially developed for the evaluation of adaptivity. The proposed framework \nunifies previous approaches in the literature and has already been used, in various guises, in \nrecent research work. The presented methods are related to the layers in the framework and \nthe stages in the development lifecycle of interactive systems. The paper also discusses \npractical issues surrounding the employment of the above, and provides a brief overview of \ncomplementary and alternative approaches in the literature. \n \nKeywords: layered evaluation, evaluation framework, formative evaluation methods, design \n \n \n 1 \n1. Introduction \nThe importance and benefits of involving users in the design and evaluation of adaptive \nsystems has been advocated for a long time (Chin, 2001; Weibelzahl, 2001, 2005; Masthoff, \n2002; Gena 2005; Gena & Weibelzahl, 2007). In fact, user studies have become an integral \npart of papers published in the UMUAI journal, and indeed most papers published in the \nmajor conferences in the area. For example, a review of the last three years of UMUAI shows \nthat all papers (excluding surveys and special issue introductions) now contain evaluations, \ncompared to only one third when Chin (2001) surveyed UMUAI for the nine years preceding \n2001. Although this is most definitely indicative of increasing maturity in the field, we are far \nfrom having solved all related outstanding issues. This paper discusses some of these issues, \nand proposes a specific evaluation approach and methods for addressing them. \nFrom early on in the history of the field, it has been acknowledged that the evaluation of \ninteractive adaptive systems1 (IAS) is, in most cases, a complicated endeavour, that is \nsignificantly different to the evaluation of non-adaptive interactive systems (see e.g., \nTotterdell & Boyle, 1990). The differences are attributable to the nature of adaptivity and the \nimplications it has on interaction. In particular, a mainstay of evaluation approaches in \nHuman-Computer Interaction (HCI) is that an interactive system\u2019s state and behaviour are \nonly affected by direct and explicit actions of the user. This principle does not hold true in \nadaptive systems, however. The very aim of adaptivity is to imbue a system with the type of \nintelligence that allows it to actively take the initiative in supporting the users\u2019 activities, on \nthe basis of inferred information about the user and the interaction context, often derived from \nimplicit interaction cues. It is this capacity of adaptive systems to exhibit their own, not \ndirectly user-controlled behaviour that traditional evaluation approaches fail to address. \nMoreover, the adaptation process often takes time, as the system needs to learn about the \nuser\u2019s goals, knowledge or preferences, etc., before adaptation can take place. Thus, the \nobservation of any effects of adaptivity may require long-term, or even longitudinal studies, \nor be based on evaluation designs that explicitly account for that factor. Due largely to these \ndisparities between interactive systems in general and adaptive systems in particular, \nadaptation was not sufficiently addressed in early standardized evaluation frameworks \n(although, in some cases, it was a concern) (Stary & Totter, 1997).  \nTo remedy these problems, early approaches to the evaluation of IAS were in the direction \nof comparative assessments between adaptive and \u201cstatic\u201d systems. This gave rise to the \npopular, but potentially also problematic, \u201cwith and without\u201d adaptivity evaluation design, in \nwhich an adaptive instance of the system is compared with a non-adaptive one. This \nevaluation design has been used in several studies in the field, including, for example, Kaplan, \nFenwick, & Chen (1993), Boyle & Encarnacion (1994), Weber & Specht (1997), and \nBrusilovsky & Eklund (1998).  \nA partial summarization of the potential problems that Totterdell & Boyle (1990) \nassociate with comparing adaptive systems with static counterparts, or static instances of \nthemselves, is as follows: \n\u2212 Selection of non-adaptive controls: An adaptive system\u2019s behaviour can range over a set \nof possible states for any given dimension of adaptation. The question, therefore, is which \nof these states the evaluator should choose for the non-adaptive control. Where \nappropriate the state might be selected by best current practice. However, there may not \nalways be a plausible control, particularly if the system is a novel application. \nFurthermore, in all but the simplest situations there will be a very large space of potential \n                                                 \n1 We will be using the term \u201cinteractive adaptive systems\u201d throughout this paper to refer to systems that have an \ninteractive front-end and are capable of self-adaptation (applied to, or experienced through, the aforementioned \ninteractive front-end). We further assume that adaptation in such systems is based at least on the characteristics \nof users (treated individually or collectively), without excluding any other category of adaptation determinants. \n 2\nsystem states, which complicates the selection of one of these to serve as the \u201cbest\u201d non-\nadaptive state. Additionally, a non-adaptive instance of a system designed to be adaptive \nmay not be \u201coptimal\u201d in any way, if adaptation is properly designed into the system \n(H\u00f6\u00f6k, 2000). \n\u2212 Selection of equilibrium points: Another related problem is the selection of appropriate \n\u201cpoints of equilibrium\u201d in the evolution of the adaptive system\u2019s behaviour to compare \nagainst. This often needs to explicitly take into account an initial period of inefficiency \nduring which the system acquires a model of the user (and any other external factors that \nguide the system\u2019s behaviour), and also periods of \u201cflux\u201d during which changes in the \nuser\u2019s or system\u2019s behaviour have mutual effects that may lead to new points of \nequilibrium.  \n\u2212 Dynamics of adaptive behaviour: Adaptive systems often have to adapt to at least two \nmutually incompatible criteria (e.g., controllability vs. unobtrusiveness). Thus, \nenhancements brought about by adaptation and explainable in terms of information about \na particular user, group, etc., if applied to another user, group, or task, might instead have \ndetrimental effects. The evaluator then has to show, not only that adaptation is of benefit, \nbut also that there exist different \u201coptima\u201d in the environment, and that the system can \nfind them (e.g., different levels of trade-offs between controllability and unobtrusiveness \nthat would be \u201coptimum\u201d for a given user or category of users). Combining this \nrequirement with the fact that adaptive behaviour evolves over time, there is a \nmultiplicative effect on the number of states in which the system is, as Totterdell & Boyle \n(1990) term it, \u201ccompatibly adaptive\u201d to its environment; in a comparative assessment \napproach, all these states would ideally be targeted by evaluation.  \n \nFurther to the above, an implicit assumption of the comparative assessment approach is \nthat a system \u201cconverges\u201d to a state that can then be compared. This, however, leaves other \ndesirable attributes of adaptation unaccounted for, such as the system\u2019s capacity to detect \nchanges in its environment, and smoothly transition to new states of convergence, neither \nexhibiting oversensitivity to minor fluctuations, nor reacting so slowly as to cause long \nperiods of mismatch between its behaviour and its environment.  \nThese problems may be difficult to address in certain IAS domains, but do not, in fact, \nrender the employment of comparative approaches in studies prohibitive. Although, to the \nbest of our knowledge, such work has not as yet been reported in the literature, approaches \nthat would allow for a systematic selection of states to include in comparisons would be \nwithin reach of the research community.  \nOne point that requires further attention is that, when a comparative approach is \nemployed, then, by definition, the question asked is a variation of: \u201cis this (adaptive) version \nbetter than that (non-adaptive) version, in this particular respect?\u201d This is indeed a \nfundamental question and a defining one in establishing the \u201cvalue\u201d of adaptation in particular \nsettings. However, it may not provide sufficient insights in terms of the fine-grained effects of \nadaptive system behaviour, and the findings may not be readily generalisable beyond the \nspecific adaptation settings and behaviour of a single system. More specifically, when \nemploying comparative assessment without directly addressing specific aspects of adaptation, \nthe reasons behind the \u201csuccess\u201d, or \u201cfailure\u201d of adaptation can only be traced back to the \ninitial hypotheses of the adaptive system design. In other words, it may not be possible to \nascertain why, and under what conditions, a particular type of adaptation may be employed \ntowards a specific goal. This constraint may be prohibitive in cases where evaluation is \nintended to derive design knowledge that can be fed back into the system\u2019s development \nprocess. In short, then, we can say that comparative assessment can be potentially very useful, \nprobably even more so in the context of evaluating the system against the overall goal that \nadaptation was introduced to achieve. However, when it is applied at that level, it may not be \n 3 \nable to offer the type of insight necessary for attaining and validating adaptation design \nknowledge.  \nA major characteristic of evaluations that was alluded to above is their goal. A widely \naccepted coarse classification uses an evaluation\u2019s goal to distinguish between formative and \nsummative evaluations (Scriven, 1996). Formative evaluation aims to identify shortcomings \nor errors in a system in order to further improve it and to guide the system design and \ndevelopment. In contrast, summative evaluation aims to determine the value or impact of a \nsystem. Formative evaluation goes hand-in-hand with the HCI principle of involving users as \nearly as possible in the design process, and is vital in discovering what and how to improve in \nan interactive system (Gould & Lewis, 1985; Shneiderman, 1998). Whereas summative \nevaluation is well established and in wide use, the same is not true of formative evaluation. \nMost user-based assessments of IAS in the literature report only summative evaluations, \naiming to establish the extent to which the use of an adaptation method has improved the \nsystem, or the extent to which the user modelling is accurate. Some recent notable exceptions \nof papers that include formative studies that appeared in UMUAI include (Stock et al., 2007), \n(Carmagnola et al., 2008) and (Porayska-Pomsta, Mavrikis & Pain, 2008), while some others \nmention that a formative study has preceded the summative one, but do not report its results \n(e.g., Kosba, Dimitrova & Boyle, 2007).  \nAlthough the inherent difficulties in the evaluation of adaptation, as discussed thus far, \nhave been well understood for quite some time, no satisfactory solutions or principled \nalternative approaches emerged until the beginning of the last decade. During that time, \nempirical studies that evaluated IAS remained few, and, more often than not, provided \nambiguous results. In the last ten years, the evaluation of IAS started receiving considerable \nrenewed attention. This has been due, in part, to the increasing utilization of adaptivity \nmethods and techniques in a wide range of application domains, but also due to the desire to \nacquire a solid design basis for adaptation, unattainable until the largely unsolved problems \ninvolved were addressed (see, e.g., Brusilovsky & Eklund, 1998; H\u00f6\u00f6k, 2000; Chin, 2001; \nMasthoff, 2002).  \nThis last decade has seen the introduction of a number of promising attempts at tackling \nthe problem of evaluating IAS, sharing one main idea: to treat adaptation not as a singular, \nopaque process, but, rather, \u201cbreak it down\u201d into its constituents and evaluate each of these \nconstituents separately where necessary and feasible. These approaches became known under \nthe moniker of \u201clayered\u201d evaluation of adaptive systems.  \nAn oft-cited example of the application of the related principles and their potential \nbenefits are two studies of the same system, one following a layered evaluation approach and \none not. The first study on the effects of adaptive link annotation (described in Brusilovsky & \nEklund, 1998) demonstrates well the problems that can arise when evaluating an adaptive \nsystem. This study treated the adaptation process as a \u201cmonolithic\u201d entity and aimed to assess \nit as a whole. Specifically, the goal of that experiment was to assess the impact of (link-\noriented) adaptive navigation support (ANS) on students\u2019 learning and on their paths through \nthe learning space. Contrary to expectations, the study failed to show any statistically \nsignificant differences between the versions with and without ANS. Although the authors did \nperform additional analysis and offered some potential justifications for their findings, the \nmatter remained largely inconclusive. A revisited interpretation of the initial study was then \npresented (Brusilovsky, Karagiannidis, & Sampson, 2001), which decomposed the adaptation \nprocess into two layers that were evaluated separately. This study demonstrated that whereas \nthe user models created were sufficiently accurate, the adaptations applied on the basis of \nthese models were likely not appropriate for the target population.  \nThe above and other propositions on how layered evaluation of IAS can be approached \nhave been directly or indirectly in use for some time now. This paper attempts to unify and \norganize the principles of layered evaluation, as these emerge from the different propositions \n 4\nand related work in the literature, into a framework that is based on a decomposition model of \nthe adaptation process that identifies five stages or layers in the process. It also presents an \narray of evaluation methods that can be used in association with the proposed framework. \nMore specifically, the rest of the paper is structured as follows: We start by outlining the \nhistory of layered evaluation, and the underpinnings of the specific framework presented \nherein (Section  2). Following that, we present the proposed framework, providing a rationale \nand a basis for the evaluation of each of the identified layers, and propose a number of generic \ncriteria that can be evaluated in relation to each layer (Section  3). We then provide an \nextensive overview of evaluation methods that can be tailored, or have been specifically \ndeveloped to cater for the idiosyncrasies of evaluating adaptive systems; we focus on methods \nsuitable for formative evaluation, and relate these to the proposed framework\u2019s layers, and the \nstages in the development lifecycle of interactive systems (Section  4). We then address \npractical issues related to the employment of the framework, including the derivation of \napplication domain- and adaptation type- specific criteria, the tailoring of the layered \napproach to suit individual evaluation requirements, and the selection of appropriate \nevaluation methods for different layers and development stages (Section  5). We next turn our \nattention to limitations of the framework and the general evaluation approach put forward in \nthis paper, including the evaluation methods presented, and list some of the complementary \nand alternative approaches that can be used to address these shortcomings (Section  6). \nFinally, we discuss the impact of layered evaluation in the literature thus far, potential \nbenefits of its application, and related work in the literature (Section  7). \n2. History and Underpinnings of Layered Evaluation \nThe seeds of the idea of decomposing adaptation for evaluation purposes can be traced back \nto Totterdell & Boyle (1990), who propose that a number of adaptation metrics be related to \ndifferent components of a logical model of adaptive user interfaces, to provide what amounts \nto adaptation-oriented design feedback. Furthermore, Totterdell & Boyle (1990) present two \ntypes of assessment performed to validate what is termed \u201csuccess of the user model\u201d (note \nthat, in their case, the \u201cuser model\u201d is also responsible for adaptation decision making): \u201cTwo \ntypes of assessment were made of the user model: an assessment of the accuracy of the \nmodel\u2019s inferences about user difficulties; and an assessment of the effectiveness of the \nchanges made at the interface.\u201d (Totterdell & Boyle, 1990, p. 180) \nThis main idea remained dormant for several years, but was revived and further pursued in \nthe past decade, in an attempt to resolve the problems encountered when employing methods \nand techniques intended for \u201ctraditional\u201d interactive systems to their adaptive counterparts.  \nAs already mentioned, Brusilovsky et al. (2001, p.3) advocated layered evaluation, \u201cwhere \nthe success of adaptation is decomposed into, and evaluated at, different layers, reflecting the \nmain phases of adaptation [\u2026]\u201d (see Figure 1). The authors describe the identified layers \nthusly (Brusilovsky et al., 2001) (emphasis by the authors): \n\u2212 In the interaction assessment layer, only the assessment phase is being evaluated. That is, \nthe question here can be stated as:  \u201care the conclusions drawn by the system concerning \nthe characteristics of the user-computer interaction valid?\u201d or \u201care the user\u2019s \ncharacteristics being successfully detected by the system and stored in the user model?\u201d \n\u2212 In the adaptation decision making layer, only the adaptation decision making is being \nevaluated. That is, the question here can be stated as: \u201care the adaptation decisions valid \nand meaningful, for selected assessment results?\u201d \nSimultaneously with the aforementioned idea, two related evaluation frameworks were \nproposed. The first was a process-based framework presented by Weibelzahl (2001), which \ndiscerned four layers that refer to the information processing steps within the adaptation \n 5 \nprocess (Figure 2 \u2013 note that in this figure the steps are represented by arrows, whereas, in the \nrest of the figures in this section, they are represented by rectangular \u201cnodes\u201d):  \n\u2212 Evaluation of input data (Step 1 in Figure 2), refers to the evaluation of the reliability and \nexternal validity of the input data acquisition process, as well as of the acquired data itself.   \n\u2212 Evaluation of the inference mechanism (Step 2 in Figure 2), addresses the evaluation of \nthe validity of user properties inferred from the input data previously collected. \n\u2212 Evaluation of the adaptation decisions (Step 3 in Figure 2), deals with determining \nwhether adaptation decisions made are optimal, determined through the comparison of \npossible alternative decisions based on the same specific set of inferred user properties. \n\u2212 Evaluation of the total interaction (Step 4 in Figure 2), finally, is geared towards the \nsummative assessment of adaptation, and distinguishes between the evaluation of system \nbehaviour (including factors such as the frequency of adaptation), and the evaluation of \nuser behaviour (as affected by adaptation) and the system\u2019s overall usability. \n \n \nINSERT FIGURE 1 ABOUT HERE \n \n \nINSERT FIGURE 2 ABOUT HERE \n \n \nThis framework has a very clear focus on the empirical evaluation of IAS and has been \napplied in practice to different adaptive learning courses, including several studies with \nthousands of users (Weibelzahl & Weber, 2003). \nThe second framework proposed around the same time by Paramythis, Totter and \nStephanidis (2001) adopts a more engineering-oriented perspective in the identification of \nlayers (termed \u201cmodules\u201d in the respective paper), focusing in more detail on the different \ncomponents involved in the adaptation process (Figure 3). The framework identifies the \nfollowing stages\/components of adaptation in adaptive user interfaces: \n\u2212 Interaction monitoring, encapsulates the collection of input data. \n\u2212 Interpretation\/inferences, refers to inferences drawn upon the collected input data. \n\u2212 Modelling, refers to the population of user- context- and other dynamic models, as well as \nto the utilization of any static models (e.g., a domain- or task- model) \n\u2212 Adaptation decision making, captures the process of making high-level adaptation \ndecisions (e.g., identify products that are likely of interest to the user), on the basis of the \navailable models. \n\u2212 Applying adaptations, refers to \u201cinstantiating\u201d adaptation decisions into the system (e.g., \nshowing a panel with the list of recommended products, or promoting them in a list \nincluding other products). \n \n \nINSERT FIGURE 3 ABOUT HERE \n \n \nBased on these, the framework then goes on to suggest evaluation \u201cmodules\u201d that address \nthe evaluation of these adaptation stages in isolation or in combination. This framework also \ndiscusses the issue of formative vs. summative evaluation, and makes some initial suggestions \nas to which (of the then existing) methods and tools might be appropriate for the evaluation of \ndifferent adaptation modules, in order to elicit input for the development process.  \nThe frameworks discussed thus far have several significant differences, both in the stages \nof the adaptation process they seek to highlight and address, and in the evaluation approaches \n 6\nthey (implicitly or explicitly) advocate. However, there is inarguably also a lot of common \nground: the premise of all these frameworks is that adaptation needs to be decomposed, so \nthat its comprising stages\/elements can be assessed\/evaluated in isolation. Their main \nconceptual differences lie with the decomposition models used, and, in particular, with the \nmodels\u2019 perspectives on adaptation, as well as with the adopted level of granularity. Figure 4 \nprovides a pictorial representation of the differences and relations between the \ndecompositions proposed by these three frameworks. \n \nINSERT FIGURE 4 ABOUT HERE \n  \nParamythis & Weibelzahl (2005) presented the first steps of an effort to merge or unify the \ncommon themes of these frameworks. These efforts towards a unification of the alternative \npropositions, culminating into the framework proposed in this paper, are based on the \nintroduction of a model of decomposition with the widest possible applicability on existing \nand forthcoming IAS, making few assumptions about implementation and architectural \nproperties of the system, but, at the same time, offering a concrete enough guide to evaluation \nactivities. \nTo arrive at the desired decomposition model, we have examined not only the previously \nproposed frameworks, but also the common properties of existing models and architectures \nfor adaptation. Although relatively young, the field of IAS is abundant with conceptual, \narchitectural, and functional models of adaptation, spanning a large range of theoretical \napproaches to adaptation, types of adaptation supported, component technologies, etc. (see, \ne.g., Totterdell & Rautenbach, 1990; Oppermann, 1994; De Bra, Houben, & Wu, 1999; \nJameson, 2001; Koch & Wirsing, 2002; Knutov, De Bra, & Pechenizkiy, 2009). This \npluralism is further compounded by the existence of domain- and \u201cplatform\u201d2- specific \nmodels\/architectures, which cannot be easily generalised or extended in their coverage. For \nexample, several reference models have been developed for adaptive hypermedia (e.g., De \nBra et al., 1999; Koch & Wirsing, 2002; Ohene-Djan, 2002), but currently these are not \ngenerally applicable to adaptive systems and are rather intended to support software engineers \nin developing systems.  \nIn examining adaptation models in the literature, we have restricted ourselves to the \nprocess-oriented ones (as opposed, for instance, to component-oriented ones), so as to allow \nfor the maximum possible degree of flexibility in terms of how adaptation is implemented \n(where, in fact, approaches proliferate). We concentrate here on three models (or \narchitectures) of adaptive systems that have been proposed in the literature:  \n\u2212 A very early process-oriented architecture was put forward by Totterdell and Rautenbach \n(1990) (Figure 5), which was the basis for the framework proposed by Paramythis et al. \n(2001) (see also Figure 3). This model relates major architectural elements of adaptive \nuser interfaces in a multi-step adaptation process involving the collection of input data \n(Interaction cues), the creation\/utilization of models (User\/Task Models), and the \nselection of appropriate adaptive interventions (User Interface Variants), all on the basis \nof the underlying Adaptive Theory. \n\u2212 Another proposal by Oppermann (1995) describes adaptive systems as consisting of three \nparts: an afferential, an inferential and an efferential component. According to \n(Oppermann, 1995, p. 6), \u201c[t]his nomenclature borrows from a simple physiological \nmodel of an organism with an afferential subsystem of sensors and nerves for internal and \nexternal stimuli, with an inferential subsystem of processors to interpret the incoming \n                                                 \n2 The term \u201cplatform\u201d is used here in its general sense. Exemplifying this use, we would categorise, for instance, \nthe \u201cWeb\u201d as one such platform, quite distinctly from the \u201cdesktop\u201d platform.  \n 7 \ninformation, and with an efferential subsystem of effectors and muscles to respond to the \nstimuli\u201d.  \n\u2212 More recently, Jameson (2001) presented a \u201cgeneral schema\u201d for the processing in user-\nadaptive systems (Figure 6), which can be informally described as follows (Jameson, \n2008, p. 433): \u201cA user-adaptive system makes use of some type of information about the \ncurrent individual user, such as the products that the user has bought. In the process of \nuser model acquisition, the system performs some type of learning and\/or inference on the \nbasis of the information about the user in order to arrive at some sort of user model, which \nin general concerns only limited aspects of the user (such as her interest in particular types \nof product). In the process of user model application, the system applies the user model to \nthe relevant features of the current situation in order to determine how to adapt its \nbehaviour to the user.\u201d \n \nINSERT FIGURE 5 ABOUT HERE \n \nINSERT FIGURE 6 ABOUT HERE \n \n \nThese models represent different points of view, and focus on different aspects of \nadaptation. One important similarity that they do have, though, is that they do not attempt to \nbe prescriptive in terms of the modules\/components that make up an adaptive system. Instead, \nthey focus, directly or indirectly, on the \u201csteps\u201d or stages of the adaptation process in \ninteractive adaptive systems. \nEven more importantly, the models under discussion exhibit a number of common \ncharacteristics: \n\u2212 They commence with the collection and interpretation of \u201cobservation data\u201d, which, in \nthese models, relate mainly to the user\u2019s behaviour (see \u201cInteraction Cues\u201d in Figure 5, \nand \u201cInformation about the user\u201d in Figure 6).  \n\u2212 In all three cases, there is an \u201cinference\u201d step, which results in the creation or updating of \ncorresponding models, on the basis of the observed data (see \u201cUser\/Task Models\u201d in \nFigure 5, and \u201cUser model acquisition\u201d in Figure 6). Typically, this involves the \nemployment of an intelligent mechanism that infers user-, context-, etc., characteristics \nfrom the raw data. \n\u2212 Split between Oppermann\u2019s (1995) \u201cinferential\u201d and \u201cefferential\u201d steps, and represented \nindividually in the other two models (see \u201cUser Interface Variants\u201d in Figure 5, and \u201cUser \nmodel application\u201d in Figure 6), is the task of making decisions as to how the system \nshould be adapted, i.e., how the system behaviour should be changed.  \n \nThe identified common characteristics of the above models, coupled with the precursor \nwork on layered evaluation frameworks for IAS, form a solid basis for the proposal described \nin detail in the next section.  \n3. The Proposed Evaluation Framework  \n3.1. A Model for \u201cDecomposing\u201d Adaptation \nAs already discussed, a comprehensive, yet not prescriptive, model of adaptation is of \nparamount importance to the framework at hand. We have composed this model by factoring \nout and enriching the common elements of previous attempts and the related models outlined \nin the previous sections. Its foundations lie on the identification of three rough categories of \n\u201cactivities\u201d in the adaptation process in an IAS: observing and interpreting (user) input; \n 8\nadjusting internal models that evolve on the basis of that input; and, using the up-to-date \nmodels to determine the system\u2019s adaptive behaviour. This rough set has been elaborated \nupon and refined to better capture elements of the adaptation process that may need to be \nassessed. The resulting model is depicted in Figure 7. Briefly, the main layers of adaptation \nidentified are (Figure 7):  \n(a) Collection of input data (CID) refers to the assembly of user interaction data, along with \nany other data (available, e.g., through non-interactive sensors) relating to the interaction \ncontext. \n(b) Interpretation of the collected data (ID) is the step in which the raw input data previously \ncollected acquire meaning for the system. \n(c) Modelling of the current state of the \u201cworld\u201d (MW) refers to derivation of new knowledge \nabout the user, the interaction context, etc., as well as the subsequent introduction of that \nknowledge in the \u201cdynamic\u201d models of the IAS. \n(d) Deciding upon adaptation (DA) is the step in which the IAS decides upon the necessity \nof, as well as the required type of, adaptations, given a particular state of the \u201cworld\u201d, as \nexpressed in the various models maintained by the system. \n(e) Finally, applying (or instantiating) adaptation (AA) refers to the actual introduction of \nadaptations in the user-system interaction, on the basis of the related decisions.  \n \n \nINSERT FIGURE 7 ABOUT HERE \n \n \nIt is argued that each of these adaptation layers needs to be evaluated explicitly, although \nnot all layers can be \u201cisolated\u201d and evaluated separately in all systems. Furthermore, the \nnature of the IAS will necessarily dictate the relevance of each of these layers. \nBefore we move on with the discussion of each of the layers, it is important to make some \npreliminary remarks on the rest of the elements that appear in the model. Firstly, the figure \ncontains several elements, \u201cinternal\u201d to the IAS (\u201cstatic\u201d and \u201cdynamic\u201d models, and \nadaptation theory). These are briefly described below. \nThe models potentially maintained by the IAS are separated into two broad categories. \nThe first category groups together the IAS\u2019s \u201cstatic\u201d models (comprising, for instance, the \nsystem model, the task model, the application model, etc.) These are often implicit, i.e., there \ndoes not necessarily exist an explicit representation of them in the IAS; rather, they may be \n\u201cdispersed\u201d in the form of domain knowledge throughout the system. In several cases, of \ncourse, explicit representations do exist and are actively used in deciding upon adaptations \n(e.g., in the case of user plan recognition, a task model is a necessity). This first category of \ninternal IAS models is used, again implicitly or explicitly, when interpreting input data. \nConsider as an example the case of an adaptive, Web-based course delivery system; the fact \nthat the user has requested a specific URL may be interpreted by the system as a request for \nviewing\/reading the contents of the corresponding organization of learning material(s). \nThe second category groups together the IAS\u2019s \u201cdynamic\u201d models (comprising, for \ninstance, the user model3, the context model4, a representation of the interaction history, etc.) \n                                                 \n3 It should be noted that, in some categories of adaptive systems, the user model is created once and does not \nevolve over time. In these cases, one might categorize the user model with the static models of the system. \nNevertheless, these models can still be treated as dynamic, since they refer to individual users and are not \n\u201cshared\u201d among users (as is the case, for instance, with a system\u2019s task model). \n4 The term \u201cinteraction context\u201d (often shortened to \u201ccontext\u201d in this paper) is used to refer to all information \nrelating to an interactive episode that is not directly related to an individual user. This interpretation of the term \nfollows the definition of context given by Dey & Abowd (2000), but diverges in that users engaged in direct \ninteraction with a system are considered (and modelled) separately from other dimensions of context. Therefore, \n 9 \nThese are models that are updated by the IAS, on the basis of new knowledge that it derives \nfrom the interpretation of the input data. They are, typically, the main determinant for \nadaptation decisions, and can be used in various ways in the decision-making process (for \nexample, a user model can be used to decide upon adaptations for a specific user, or be \ncombined with models of other users to provide support for decisions based on the \ncharacteristics, behaviour, etc., of entire groups of users). \nFigure 7 also introduces an entity termed \u201cadaptive theory\u201d. The term itself is borrowed \nfrom (Totterdell & Rautenbach, 1990) and is used to refer to the theory that underlies \nadaptations in the system (see also Figure 5). The word theory is not used here in its formal \nsense, but rather to represent the totality of adaptation goals\/objectives that drive adaptation in \nthe IAS. In several systems, the adaptive theory is dispersed into possibly independent \nadaptation \u201crules\u201d which are themselves \u201ctriggered\u201d by the contents of the IAS\u2019s models (e.g., \nthe user model). \nFinally, arrows are used in the figure to denote potential flows of information. Although \nsome of the depicted flows will be typical in certain categories of IAS, or in certain \napplication domains, only part of them are usually present in any one system. For example, \nthe flow from the adaptation decision layer, to the \u201cadaptive theory\u201d entity, exists only in IAS \nthat have a, so-called, second adaptation cycle (Totterdell & Rautenbach, 1990) - i.e., systems \nwhich are capable of assessing their own adaptation decisions and modifying their adaptation \nstrategies. \nNote that the above described elements are not part of the model itself. Their inclusion in \nthe figure is solely intended to facilitate understanding of the model and support related \ndiscussions. The proposed decomposition model (and, consequently, the proposed \nframework) is neither based upon, nor presupposes the presence of any of the models \nidentified in the figure (with the possible exception of the user model, or its equivalent). \nFurther, we explicitly do not assume specific approaches to intelligence, or decision making, \nalthough the depiction of the model might suggest that. In fact, different approaches along the \nabove lines might lead to different groupings of the layers, which, for instance, may happen \ncollectively, or have but rudimentary manifestations in an IAS. The subsequent discussion of \nthe adaptation decomposition model is explicitly based on these provisions. \n3.2. Layered Evaluation of Adaptation \nIn this section we will present in more detail each of the layers that appear in the model and \ndiscuss whether they need to be evaluated (in isolation or combination) and with what \nobjectives. To this end, we will also introduce specific evaluation criteria that can potentially \nserve as \u201cguides\u201d for their respective evaluation \u201clayers\u201d. Criteria that we believe are \napplicable to all layers are discussed separately, after the layers themselves. Discussions \nconcerning assessment methods that might be appropriate for evaluating the proposed criteria, \nas well as the scope and practical use of the framework and its relation to the specific \napplication domain of the IAS, are deferred until later sections. Table 1 is intended to act as a \n\u201cguide\u201d to the rest of this section, and provides a collective overview of the layers and the \nproposed criteria for each of them, along with the formative evaluation methods that may be \napplicable in each case. To facilitate reading, the relevant portions of the table are repeated at \nthe end of the discussion of each layer. \n \nINSERT TABLE 1 ABOUT HERE \n                                                                                                                                                        \nthe interaction context is characterised, for example, by: features and capabilities of access terminals, \ncharacteristics of network connections, the user\u2019s current location, current date\/time, etc. \n 10\n3.2.1. Collection of Input Data \nThe \u201cinput\u201d data that an interactive system collects is predominantly derived from the user\u2019s \ninteraction with it, i.e., it comes from direct interaction of the user with the system\u2019s user \ninterface, or interactive front end5. Data in this category include the user\u2019s pression of a \nbutton, selection of a link, etc. It is important to note at this point that input data of this nature \ndoes not necessarily carry any semantic information. It is in the next layer, and with the \nassistance of (implicit or explicit) application- and task- models that this low-level data will \nacquire \u201cmeaning\u201d for the system. \nIn addition to the traditional input data that an IAS may derive from direct user \ninteraction, there exists a host of additional information that may be available to the system, \nfrom \u201csensors\u201d not directly or explicitly manipulated by the user. For example, in an adaptive \nenvironment, the user\u2019s position, direction of movement, gestures, direction of gaze, etc. may \nalso be available (Zimmermann & Lorenz, 2008); smart environments such as smart home or \nsmart offices often rely on a variety of sensors. The accuracy of these sensors needs to be \nconsidered, before the resulting data can be used for further inferences. For example, the \nthermometer measuring the office temperature in an intelligent office environment had an \nerror of about 2\u00b0C (Cheverst et al., 2005) and the positioning system in an adaptive museum \nguide provided a resolution of 5cm and 5\u00b0 in terms of orientation (Zimmermann, Specht, & \nLorenz, 2005). In fact, the quality of the input data can be evaluated in a systematic way. \nUsing the example of an adaptive museum guide, Schmidt, Zukerman, & Albrecht (2009) \ndescribe a framework how the impact of uncertainty in sensing technology in physical spaces \ncan be investigated. With more and more sensor data available for user modelling, a number \nof IAS have been described that take advantage of such potentially useful information as the \nambient noise in the user\u2019s environment (Cheverst et al., 2005), the presence of the user in \nfront of the interaction \u201cterminal\u201d (Oliver & Horvitz, 2005), the very fact that the user is \nlooking at the screen or not, and even the user\u2019s affective state inferred from physiological \nsensors (Cooper et al., 2009). In many cases, the accuracy of these sensors seems to be taken \nfor granted. In fact there may be no need to evaluate this layer if previous studies have shown \nthat the data is reliable or if it is safe to assume that the data is reliable. Presence of data in \nthis category may not directly affect user modelling itself, but most certainly does affect the \ninterpretation of user-related data, or may even be used to model the broader context of \ninteraction. \nThe nature of the sensors and the way in which their input is used will typically determine \nwhat other criteria may need to be assessed. For instance, excessive latency in a GPS sensor \nmay result in the system adapting to a geographical context that \u201clags behind\u201d the user\u2019s \ncurrent one, and a low sampling rate for an accelerometer may have adverse effects in a \nmobile guide that adapts its output to whether its user is on the move or stationary. It should \nalso be noted that certain categories of sensors, especially ones not normally employed in \ninteractive situations (e.g., ones related to a person\u2019s physical well being), may well pose \nsensor-and context- specific considerations and may require the introduction of respective \n(possibly entirely custom) criteria. \nIt is noteworthy that undetected problems in this layer may have \u201ccascading\u201d effects in \nother layers. Returning to a previous example, treating a user\u2019s position in a physical space, as \nrelayed by sensors, as entirely accurate, may lead to problematic interpretations of the users\u2019 \ninterests in relation to objects within that physical space. In contrast, when the level of \ninaccuracy that should be anticipated is known, it can well be integrated into the adaptation \n                                                 \n5 In the rest of this paper we will be using the term \u201cinteractive front end\u201d, rather than \u201cuser interface\u201d. This is \ndone to explicitly denote a potentially richer interactive experience than the one afforded by today\u2019s WIMP, \nkeyboard- and mouse- based user interfaces, as well as to avoid misinterpretations that may result from the \ntypical association of the term \u201cuser interface\u201d to desktop-based interaction. \n 11 \nmodels of subsequent layers, as demonstrated by Schmidt, Zukerman, & Albrecht (2009). \nThese \u201ccascading\u201d effects can arguably occur between most pairs of subsequent layers, but \nare most often neglected in this layer. \nIn synthesis, either of the categories of data discussed (i.e., originating from the user, or \nfrom non-interactive sensors) is subject to \u201ctechnical\u201d assessments which would determine \nwhether factors such as accuracy, latency, sampling rate, etc. are appropriate for the system at \nhand. Given the assumption that \u201craw\u201d input data does not carry semantic value by itself, such \nassessments may be all that is necessary at this level. A summary of this layer is given in \nTable 2. \n \n \nINSERT TABLE 2 ABOUT HERE \n \n \n3.2.2. Interpretation of the Collected Data \nWhat is far more interesting and challenging in terms of evaluation is the layer of \ninterpretation of the input data. According to the proposed model, this is the very layer at \nwhich input data acquire \u201cmeaning\u201d of relevance to the system. It should be noted that the \ndistinction between this stage and the collection of the input data may seem somewhat \nartificial as far as current practice is concerned. It is usually the case that input data is \nretrieved and interpreted in one step. The separation here is not intended as a proposal for a \nnew engineering paradigm or implementation approach; rather, it seeks to explicitly identify \nand conceptually dissociate the two stages, thus making it possible to discuss them in \nisolation.  \nThe interpretation process may be straightforward, in those cases that there exists a direct, \none-to-one mapping between the raw input data and their semantically meaningful \ncounterparts. Examples include the retrieval of a user\u2019s position (when the latter is regarded in \nits strict geographical confines), the identification of a user action in the context of a task, etc. \nWhen the interpretation is unambiguous, and independently of whether it employs any of the \nsystem\u2019s \u201cstatic\u201d models, it can be assessed objectively and in a user-independent manner. \nFor instance, an adaptive user support system (Encarna\u00e7\u00e3o & Stoev, 1999) might exploit \naction sequences; registering the number of sessions that the user completed or the number of \nthe user\u2019s undo actions is probably highly reliable. There is no subjective judgment or other \nnoise involved in this observation. \nPotential problems arise when: (a) the interpretation makes use of assumptions, or (b) the \ninterpretation requires some level of inference. Assumptions and inferences are quite \ncommonly employed in existing IAS, mainly due to the lack of additional data that can better \ndescribe the context of interaction. A typical example is how adaptive Web-based information \nsystems consider a node in the hypermedia space \u201cvisited\u201d, \u201clearned\u201d, \u201cof (no) interest\u201d, etc., \non the basis of how long the user spent on viewing the respective page. Another example is \nsensor data in intelligent homes, which is particularly difficult to interpret (Sixsmith, 2000). \nAlthough considerable work has gone into developing and proving principles upon which \n\u201ceducated\u201d assumptions or inferences can be drawn (see, e.g., Goecks & Shavlik, 2000; \nClaypool, Le, Wased, & Brown, 2001; Spada, S\u00e1nchez-Monta\u00f1\u00e9s, Paredes, & Carro, 2008), \nthese are always dependant on the IAS\u2019s application domain, deployment context, etc. \nA criterion that may need to be addressed at this stage is the validity of the interpretations \n(at least in cases where the interpretations are not straightforward, as discussed above). An \nexample where validity plays a central role would be an adaptive news broker (Billsus & \nPazzani, 1999): Users might provide feedback about a specific news story by selecting one of \nfour categories: interesting, not interesting, I already know this, and tell me more about this. \n 12\nBut the user\u2019s answer depends on many uncontrollable factors. Users might read the story \nonly roughly and might overlook some interesting new facts. Or they might read the same \nstory somewhere else afterwards. Or, just for the moment, they might not be interested in this \nkind of stories. Several other threats to validity do arise here, and further inferences might be \nhighly biased if the data quality is neither assured nor considered in this process. \nRevisiting the example of the \u201cstate\u201d of pages that have been viewed by a user, we could \nnow further identify it as a problem of validity of the interpreted data. If the system\u2019s \nassumption (or inference) that a page\u2019s content is \u201cknown\u201d by the user is erroneous, this could \nlead to entirely unexpected and unacceptable adaptations, although the rest of the adaptation \ncycle may be flawless. \nSome systems attempt to compensate for potentially erroneous interpretations by \nexplicitly incorporating in them the concept of uncertainty. For example, an IAS might assign \ninterpretations a probability, which might even be related to similar\/related interpretations \nmade in the past. Carmichael, Kay, & Kummerfeld (2005) show how the inconsistency of \nsensor data can be modelled. Modelling the uncertainty can help to identify distortions in \nsensor data (Schmidt et al., 2009).  It is claimed that the proposed evaluation criteria for this \nstage of adaptation are valid in this case as well, although their scope may need to be \nadjusted. \nWhen considering the IAS behaviour from the user\u2019s perspective, we can identify two \nadditional criteria that may need to be addressed at this layer, namely predictability of the \nsystem\u2019s interpretations and scrutability of the system\u2019s interpretations. Jameson (2008) \ndefines predictability in this context to represent the extent to which users can predict the \neffects of their actions. We specialize the definition for the needs of applying this criterion to \nthis layer, and constrain it specifically to the system\u2019s interpretation of user actions. When \nusers have a wrong mental model of the principles of this interpretation, there is a very real \ndanger that they will try to modify their behaviour to influence the system\u2019s interpretations \nwith unpredictable effects (see, e.g., Zaslow, 2002). \nThe second user-oriented criterion proposed is that of scrutability (Kay, 2000). The term \nscrutability is typically employed in user modelling to signify that every user\u2019s model can be \ninspected and altered by its owner. The goal is to enable users to determine themselves what \nis modelled about them and how adaptations based on their models will be conducted. In the \ncontext of this layer, the relevant dimension of scrutability that applies is the capacity of users \nto determine (i.e., inspect and control) how (or even whether) specific actions of theirs are \ninterpreted by the system.  \nIt is worth discussing at this point the notion of interaction between evaluation criteria. For \nexample, making a system thoroughly scrutable may directly contribute to the system\u2019s \npredictability from the user\u2019s perspective. Although in the preceding example the interaction \nis contributory, we will later encounter cases where attempting to maximize one criterion will \nhave potential adverse effects on others. It is very important to have a clear picture of such \ninteractions between criteria when evaluating a particular adaptive system, and, if possible, to \ndecide beforehand what types and levels of trade-offs between \u201ccompeting\u201d criteria are \nacceptable. \nA summary of this layer is given in Table 3. \n \n \nINSERT TABLE 3 ABOUT HERE \n \n \n 13 \n3.2.3. Modelling of the Current State of the \u201cWorld\u201d \nThis stage of the proposed model concerns the derivation of new knowledge about the user, \nthe user\u2019s group, the interaction context, etc., as well as the subsequent introduction of that \nknowledge in the \u201cdynamic\u201d models of the IAS. There is a definite overlap between this stage \nand the interpretation of the input data; in fact, in several cases, there is no \u201csecond-level \ninference\u201d in adaptive systems, which may simply go from interpreting the input data to \nrepresenting those interpretations in an appropriate model. However, more often than not, IAS \ndo employ second-level inference, mainly in the direction of relating the interpreted input to \nthe current state of the dynamic models, as a basis for deciding the next \u201cstate\u201d of those \nmodels. Inferences can be derived in many different ways ranging from simple rule based \nalgorithms to Bayesian Networks, or Case-Based Reasoning systems. \nThe main evaluation criterion for this stage is validity of the interpretations\/inferences. \nThis refers to whether the inferences\/interpretations reflect the actual state of the entity being \nmodelled. Whereas, in many cases, this can be determined objectively and in a user-\nindependent manner, this is not always true. For example, an IAS\u2019s inference on a user\u2019s \ninterest in a particular piece or category of information (e.g., a tourist information system \nmight infer user interest in visiting sites of historical interest), can only be judged on the \nsubjective basis of the individual whom the inference concerns. In the context of \nrecommender systems, validity is usually tested by n-fold cross validation (see Section  4.3.3) \nof a given dataset (Degemmis, Lops, & Semeraro, 2007; Berkovsky, Kuflik, & Ricci, 2008; \nde Campos, Fern\u00e1ndez-Luna, Huete, & Rueda-Morales, 2009). While this setting makes it \neasy to compare and benchmark different modelling mechanisms and algorithms, such \nstandardized datasets are not available in many other domains. In these cases, validity may be \nassessed through external criteria such as expert ratings (Suebnukarn & Haddawy, 2006) or \nprediction of a user\u2019s behaviour or performance (Yudelson, Medvedeva, & Crowley, 2008). \nBeyond validity, it is argued that predictability and scrutability also need to be evaluated \nin this layer, although from a slightly different perspective than the one adopted in the \nprevious layer. Specifically, predictability, in this case, refers to whether users are capable of \npredicting the system\u2019s modelling behaviour, given the system\u2019s interpretation of their \nactions. Similarly, scrutability, in this case, refers to the users\u2019 capacity to inspect and modify \nthe user model itself (as opposed to the processes leading to its creation, or the ones involving \nits utilization). It is in fact this context of scrutiny and tailoring that the term is usually \nemployed to convey.  \nWhile the above three criteria (and especially validity) are inarguably the most important \nones for this stage, there are a number of lower-level criteria that address the modelling \nprocess in further detail. It is important to note that, in most cases, it only makes sense to \nproceed with these criteria after reasonable levels of validity have been ascertained. The \nproposed lower-level criteria are: (a) comprehensiveness of the model; (b) conciseness of the \nmodel; (c) precision of the model; and, (d) sensitivity of the modelling process.  \nThe first criterion, comprehensiveness of the model, is derived from information theory \nand is intended to identify the degree to which the IAS\u2019s model is capable of representing in \nits entirety the inferred\/interpreted information about the entity being modelled. In other \nwords, this criterion is concerned with how well the model can capture all the knowledge that \nis produced by the system within this particular adaptation stage (for instance, whether there \nare any properties that should be modelled, but are not modelled). Apparently, this is a \ncriterion addressing the \u201cstructure\u201d of the model and its representational power, as these relate \nto the inference process itself. Consider for example an adaptive learning system which offers \nlearners, for every concept to be learned, a main description, a set of examples, and a set of \nself-tests to take. If the system\u2019s learner model is only capable of representing concepts as \n\u201cknown\u201d or \u201cnot known\u201d(or even a range of possible values between these two extremes) \nrather than modelling the learner\u2019s interactions in more detail, then it will be impossible to \n 14\ntake advantage of the rich set of interactions available (and learning states possible) in further \nadapting the system to the user.  \nThe second lower-level criterion, conciseness of the model, is \u201csymmetric\u201d to the first, \nand seeks to identify properties of the entity being modelled, which can be represented in the \nmodel, but cannot be inferred from interaction (and, thus, do not need to be modelled). This \ncriterion is only relevant if the presence of such redundant \u201cattributes\u201d has adverse effects on \nthe system\u2019s design or run-time operation (e.g., if the system\u2019s complexity is unnecessarily \nincreased or run-time behaviour is impacted). Returning to the example of the adaptive \nlearning system in the previous paragraph, consider the case where the user model is capable \nof representing learning progress in a fine grained way, including explicit representations of \nsub-elements of concepts, such as examples, tests, etc. If these sub-elements are not well \nstructured in the system (e.g., not semantically distinguished within pages, necessitating that \nassumptions are made as to whether the learner has encountered them or not, in which order, \netc.) then it will not be possible to populate the respective entries in the model dependably. \nThis might be a problem if computational effort is expended in deriving the poorly \nsubstantiated entries, and, even more so, if the system\u2019s adaptation logic uses these values as \nif they were always present and dependable. \nThe criterion of precision of a model is again derived from information theory, and is \nconcerned with the level of precision with which aspects of the user, context, etc., are \nmodelled. For example, using a three-point scale to represent a person\u2019s knowledge of a given \ntopic is certainly different than using an expanded seven-point scale, or a percentage. An \nalternative way to think about this criterion is that it is concerned with whether properties are \nmodelled with enough detail. Whereas a high level of precision is, in general, a desired \nproperty of IAS, pursuing it may lead to redundancies, without necessarily increasing the \ncomprehensiveness of a model. An important differentiating characteristic between the \ncriteria of comprehensiveness and precision is that the first is mostly concerned with entire \naspects of the entity being modelled, while the second addresses the \u201cgranularity\u201d of the \nmodel and the level of precision that can be afforded.  \nLastly, the criterion of sensitivity seeks to identify, on the one hand, how fast the \nmodelling process converges to a comprehensive and accurate representation of the entity \nbeing modelled, and, on the other hand, the effects that fluctuations in the input data have on \ntheir respective models. Evidently, the desiderata are to: quickly arrive at a model that \nsufficiently represents the outside \u201cworld\u201d (including the user), addressing in the process any \n\u201ccold-start\u201d problems (Schein, Popescul, Ungar, & Pennock, 2002); avoid \u201cchase effects\u201d that \nmay result from the system\u2019s being too sensitive; and, avoid unnecessary latencies between an \nevident change in the modelled entity and the propagation of that change into the model. This \nis a very delicate subject that needs to be approached with great care both in terms of \nadaptation design and in terms of evaluation. To better comprehend the complexity of the \nparticular criterion, consider the example of a system that tries to \u201cunderstand\u201d whether the \nuser\u2019s lack of interest in a previously appealing subject, is temporary or the result of a more \npermanent shift in the user\u2019s interests.  \nA summary of this layer is given in Table 4. \n \n \nINSERT TABLE 4 ABOUT HERE \n \n \n 15 \n3.2.4. Deciding upon Adaptation \nDuring this adaptation stage the IAS decides upon the necessity of, as well as the required \ntype of, adaptations, given a particular \u201cstate\u201d (as the latter is expressed in the various models \nmaintained by the system, or directly from input data). \nUsually there are several possibilities of adaptation given the same user properties. \nBesides the way in which the system usually adapts, it is often possible to ignore the user \nmodel completely, or to use a single stereotype for all users. Furthermore, for most systems, \nthere are even more adaptation behaviours possible. For instance, a product recommendation \nsystem might have inferred a strong preference for a specific product. It might now either \nrecommend this product to the customer, only limit the possible selection to this product, \nindicate that there is a suggestion without naming it, or even recommend another product \nrandomly. Comparing these alternatives might help to explore a kind of baseline that indicates \nwhat usual (non-intelligent) behaviour could achieve and whether adaptation really has \nadvantages. As already discussed, one should be careful when using comparative analysis, \nespecially if the \u201cstatic\u201d system compared against is a \u201cwithout adaptation\u201d version of the \nsystem being evaluated.   \nThere is a very clear distinction between this stage and the next (see \u201cApplying \nAdaptation Decisions\u201d below). This separation can already be seen, for example, in the model \nof Oppermann (1995), where the inferential component (where, among other things, \nadaptation decisions are made) is separated from the efferential component (where adaptation \ndecisions are applied). Again this is not necessarily a distinction that exists in practice; it is \nrather a way of facilitating the conceptualisation of the steps that are involved in the \nderivation and application of adaptation decisions, and which are often overlooked in \nevaluating, leading to questionable evaluation results. One \u201crule of thumb\u201d that we propose \nfor the separation between these stages is that, decisions made at this stage, are mainly at the \nsemantic and syntactic level of interaction; any further decisions made while effecting \nadaptation should belong to the lower syntactic, or to the lexical\/physical level of interaction6.  \nThe goal in making this seemingly artificial distinction is to foster the separation of the \nadaptation theory (i.e., the foundation of the logic that drives adaptation) from decisions \n(made at design- or run- time) that represent a typical interaction design task, rather than a \nparticular adaptation artefact. To return to our very first example again: A decision to guide \nlearners would belong to this level; the same is true for more detailed versions of that \ndecision, such as \u201cto guide the learner by augmenting links in-place as they appear in the \ntext\u201d. All other lower-level decisions (e.g., colour and adornments used to augment links) \nwould belong to the next level though. \nThe primary aim of this evaluation step is to determine whether the adaptation decisions \nmade are the optimal ones, given that the user\u2019s (and, more generally, the \u201cworld\u2019s\u201d) \nproperties have been inferred correctly. We propose the following evaluation criteria for \nassessing the system towards this end: (a) necessity of adaptation; (b) appropriateness of \nadaptation; and (c) subjective acceptance of adaptation (i.e., does the user think that the \nadaptation is both necessary and appropriate?). \nThe necessity criterion is concerned with whether a decided upon adaptation is indeed \nrequired, given a specific interaction state, as this is represented in the system\u2019s various \n                                                 \n6 The terms \u201clexical\u201d, \u201csyntactic\u201d and \u201csemantic\u201d refer to the three levels at which human-computer interaction \noccurs (Hoppe, Tauber, & Ziegler, 1986; Ziegler & Bullinger, 1991): The lexical level of interaction (also \nreferred to as physical), which concerns the structure, presentation attributes, and actual behavior of the input \/ \noutput interaction elements that make up the user interface; it is at this level that interaction physically takes \nplace. The syntactic level of interaction, which concerns the structure and syntax of the dialogue between the \nuser and the computer, through which the application semantics are made accessible to the user (e.g., specific \ninteraction steps taken by the user, method of accomplishing tasks). The semantic level of interaction, which \ninvolves conveying the system functionality and domain-specific facilities to the end-user. \n 16\nmodels. This criterion is often directly related to the theory underlying the system\u2019s adaptive \nbehaviour, as it addresses the very point at which specific states of the system\u2019s \u201cworld \nmodel\u201d are linked to (at least) high-level strategies for remedying identified problems, or \ncapitalizing upon identified opportunities to support users in their interaction with the system \n(or with elements of their environment if the system\u2019s role is a mediating one). For example, \nin a system that seeks to automate commonly performed user tasks, the necessity criterion \nwould need to be applied to all cases where the system identifies an action sequence the \nautomation of which it believes will benefit the user. \nHaving established the need to adapt, one can then move on to the appropriateness of the \ndecision made, i.e., is the adaptation decided upon one that can cater for the requirements \nposed by the current interaction context? For example, N\u00fcckles, Winter, Wittwer, Herbert, & \nH\u00fcbner (2006) observed expert behaviour and how it was influenced by the availability of \nadditional knowledge to them. Essentially, they were able to demonstrate which adaptation \ndecisions were taken by experts given different user models.  \nFinally, subjective acceptance of adaptation decisions refers to the user\u2019s perception of \nwhether a decided upon adaptation is both required and appropriate. This criterion is \ncomplementary to the ones discussed thus far, in that it specifically urges evaluators to \nconsider not only the objective dimensions of an adaptation decision, but also its direct effects \nas perceived by end users. It may well be the case that users are uncomfortable with a system \ndecision, even if it is ultimately to their benefit (e.g., if it makes obvious a particular non-\ncomplimentary social trait of the user). Subjective acceptance is of particular importance \nwhen a lack of transparency may affect the user\u2019s trust in the system (Cramer et al., 2008) or \ncompromise the user\u2019s privacy, e.g., in a group recommendation situation (Masthoff & Gatt, \n2006). \nWhen feasible and desirable, a number of more fine-grained user-oriented criteria can also \nbe considered for this layer. To start with, predictability of the system\u2019s adaptive behaviour, \non the basis of its model of the \u201cworld\u201d is an essential element in many domains of adaptivity. \nComplementary to predictability is the criterion of scrutability of the system\u2019s adaptive \nbehaviour. As was the case in the preceding layer, these criteria reappear, but the perspective \nhas again shifted to capture the portion of the adaptation process that is covered by this layer.  \nA criterion that is applicable in both this layer and the next is that of breadth of \nexperience. Jameson (2008) argues that, especially in IAS that support the user in some form \nof information acquisition, the system\u2019s adaptive behaviour may prevent the user from \nexperiencing the full range of items, products, functionalities, etc., that are available. This is \nrelated to serendipity, a criterion often applied for the evaluation of recommender systems \n(McNee, Riedle, & Konstan, 2006), and intended to convey the extent to which users make \npleasant new discoveries when using the system. Jameson (2008) points out that a reduction \nof the breadth of experience is especially likely if the system relies more heavily than is \nnecessary on an incomplete user model. Although this is quite possible, in the context of this \nframework this criterion is intended to identify decisions that have the described detrimental \neffects that are based on theoretically sufficiently populated and valid models.  \nIt is noteworthy that we have here another clear example of interaction between proposed \ncriteria. Specifically, some of the common methods used in systems to mitigate the \ndiminishing of the breadth of experience, such as the systematic proposition of items that are \nnot dictated by the current user model in a recommender system (see, e.g., Ziegler, McNee, \nKonstan, & Lausen, 2005), may have direct impact on the predictability of the system\u2019s \nbehaviour. It is again recommended that evaluators explore such interactions, especially for \nnovel criteria they add to the assessment of individual layers or the system as a whole, and \nthat they ensure that the system\u2019s design priorities in this respect are reflected appropriately in \nthe evaluation design.  \nA summary of this layer is given in Table 5. \n 17 \n \n \nINSERT TABLE 5 ABOUT HERE \n \n \n3.2.5. Applying Adaptation Decisions \nThis stage refers to the actual introduction of adaptations in the user-system interaction, on \nthe basis of the related decisions. Although typically subsumed by adaptation decision making \nin the literature, this stage may be varied independently of the decision making process, e.g., \nto account for different adaptation strategies. More importantly, this stage usually \u201chides\u201d a \nlevel of adaptation (i.e., the transformation of possibly high-level adaptation decisions to a \n\u201cconcrete\u201d form experienced by the user), which only too often, and in several cases \nmistakenly in the authors\u2019 opinion, gets evaluated in tandem with the higher-level decision \nmaking stage. \nThe evaluation criteria that are applicable at this stage depend very much on the type of \nadaptation effected. In most cases, traditional evaluation criteria, such as usability, will be \nhighly relevant (Gena & Weibelzahl, 2007). The identification of these criteria can only be \nperformed on a case-by-case basis.  \nHowever, there are a number of adaptation-specific criteria that are largely independent of \nthe type of adaptation and could be assessed at this stage. We propose that the following be \nconsidered as a minimum: timeliness of adaptation (i.e., is the decided upon adaptation \napplied in a timely manner - e.g., not too late?); unobtrusiveness of the adaptation (i.e., how \nobtrusive, or obstructive is the application of an adaptation, with respect to the users' main \ninteraction tasks); and, user control over the adaptation (i.e., can the user disallow, retract, or \neven disregard an adaptation?). The last criterion is a specialization of controllability, which \nis discussed in detail in Section  3.2.7; it is repeated here explicitly to emphasize the role of the \ncurrent layer in the users\u2019 perception of their control over the system. Furthermore, all of the \naforementioned criteria can be thought of as directly contributing towards the acceptance of \nthe adaptation by the user. \nCriteria that have been suggested for prior layers and also have bearing on the application \nof adaptation decisions are breadth of experience and predictability of the system\u2019s adaptive \nbehaviour. In terms of the former, assessment can address the extent to which the way in \nwhich adaptations are applied precludes (or makes less likely) that users will experience \ncertain aspects of the system. In terms of the latter, assessment may address whether \nmodifications (incurred by adaptivity) at the physical and syntactic levels of interaction are \ndeemed predictable by the user. \nThe evaluation of this stage should be approached judiciously, and any related evaluation \nactivity should be designed very carefully to measure only the relevant criteria. The difficulty \nin doing so arises from the fact that the users \u201cexperience\u201d the grand total of the system\u2019s \nadaptive behaviour through the adaptations that are effected (and of which they are aware). \nHeuristic evaluations by experts in terms of usability criteria can help to detect issues with the \napplication of the adaptation decision at early stages of the development cycle (Carmagnola et \nal., 2008). An alternative approach that is more demanding on evaluation (and possibly also \non development) resources, but enables the straightforward participation of end users in the \nevaluation activities, is the comparison of alternative manifestations of adaptation decisions. \nIn such a scenario, two versions of the adaptive system would be in comparison, and they \nwould differ only in how specific adaptation decisions are effected. \nA summary of this layer is given in Table 6. \n \n \n 18\nINSERT TABLE 6 ABOUT HERE \n \n \n3.2.6. Evaluating Adaptation as a Whole \nThe \u201cpiece-wise\u201d evaluation of adaptation, as proposed in this paper, can provide valuable \ninsight into the individual adaptation stages through which an IAS goes. However, what is \nstill missing is the \u201cbig picture\u201d \u2013 the evaluation of the primary adaptation theory (or \ntheories). For example, the basis of adaptation in an adaptive learning system might be that \nguiding learners through the available material, decreases learning time and increases \nretention time of learned material.  \nTo assert whether such high-level theories (or, seen from a different perspective, \nhypotheses) hold true, one needs metrics that transcend the layered evaluation of adaptation as \nthis has been discussed so far. Such metrics must adequately capture the application- and \nadaptation- domains, to be able to more holistically assess the \u201csuccess of adaptation\u201d. This \nrole cannot be fulfilled by the stage-based evaluation criteria proposed in preceding sections, \nas these are \u201cdomain-agnostic\u201d, i.e., they make no assumptions, but also no provisions, for \nany particular application domain.  \nBrowne, Norman and Riches (1990) have proposed that this problem be addressed by: (a) \narticulating and assessing against the system\u2019s objectives, and\/or (b) assessing indirectly \nagainst the underlying theory. In the first case, the evaluation is centred around the \nidentification of the objectives that the system aspires to attain (e.g., to speed up the user\u2019s \ninteraction with the system, or to decrease the user\u2019s error rate, or to increase user satisfaction, \netc.) According to Browne et al. (1990) many of the objectives of an adaptive system can be \nexpressed as lists of purposes, which, in turn, can be loosely interpreted as the collection of \n\u201creasons\u201d that led to the introduction of adaptation in the system, in the first place. Metrics \nand assessment methods can then be devised to measure the extent to which the stated \nobjectives are met. These metrics might either be subjective, such as perception of and \nsatisfaction with the system (Zimmermann & Lorenz, 2008) or may be objective, such as task \ncompletion time or number of steps required (Bontcheva & Wilks, 2005). \nFollowing the above approach may not be equally straightforward when the success of the \nsystem in obtaining its objective is related only indirectly to the aspect of the user interaction \nthat the system is attempting to improve. The means for attaining the objective may rest on an \nuntested theory. For example, adaptation could be based on a theory that attempts to decrease \nerror rates in order to increase user satisfaction. In order to test this theory it is essential that \nerror rate data be collected (even though this does not reflect the objective of the whole \nsystem), and associated with evaluation results regarding subjective user satisfaction. In this \ncase, error rates would serve as an indirect metric towards assessing the adaptive theory. \nRelated to this concept is that of a \u201cmediator variable\u201d in statistics; the mediator variable is \ntypically intended to concretize and\/or operationalise the relationship between an independent \nand a dependent variable. When applying this approach evaluators are cautioned that it is \nsometimes very complicated to establish causal relationships between variables in an \nempirically rigorous manner (Green, Ha, & Bullock, 2010).   \nApparently, the formulation\/selection of metrics in both of the preceding cases is domain- \nand system- dependent. The establishment of such metrics needs to take place at design time, \nand their assessment must be planned well in advance, as there is a distinct possibility that \nrelated measurements may require monitoring aspects of the system\u2019s or the user\u2019s behaviour \nwhich are not part of the primary adaptation cycle. The selection of appropriate \nevaluation\/assessment methods and instruments depends, naturally, on the very nature of \nderived metrics. For instance, one would approach in entirely different ways metrics related to \ninteraction speed, from those related to user satisfaction, or retention of learning material. \n 19 \nThe discussion until now may have led readers to the conclusion that the assessment of \ninteraction as a whole cannot be approached in a domain-independent way. However, this is \nnot necessarily the case. If we accept that there are adaptation goals that are shared by IAS in \ndifferent domains, then we could also formulate metrics that go beyond individual domains. \nFor example, Weibelzahl (2003) proposes as a general goal of adaptation the simplification of \nthe interaction process, and goes on to introduce the metric of behavioural complexity as a \nmeans of assessing against the stated goal.  \nA summary of this layer is given in Table 7. \n \nINSERT TABLE 7 ABOUT HERE \n \n \n3.2.7. Criteria Applicable in Most Layers \nBeyond the criteria introduced for the individual layers there are some that apply to most, or \neven all layers. Namely these comprise privacy, transparency and controllability. \nPrivacy has been identified as a challenge to adaptive systems (Jameson, 2003) due to the \npotential tension between the use of personal data for personalization and the user\u2019s need for \nand concern of privacy (Kobsa, 2007). In fact, privacy is a complex issue that cannot be \naddressed by a single solution. All layers are affected by this issue. Starting with the \nCollection of Input Data layer, it may be necessary to evaluate whether users are willing to \nprovide a certain type of information (Ackerman, Cranor & Reagle, 1999) or whether the data \nis allowed to be collected under certain legislation (Wang, Chen & Kobsa, 2006). In regard to \nthe MW layer it may be evaluated whether the information in the user model is stored in a \nsecure way. In regard to the DA layer, it may be evaluated whether the adaptation may \npotentially disclose information about the user to other users. \nOn a similar note, the criterion transparency may need to be evaluated with respect to \nseveral or all of the layers. In general, it is desirable that a user of an adaptive system can \nunderstand why the system has made a particular adaptation or recommendation and how the \nsystem\u2019s adaptive mechanisms work (Jameson, 2003). Accordingly, depending on the system \ndomain and application it may be important that the user is aware which information is \ncollected (CID layer), which inferences are drawn (MW layer) or why a certain adaptation has \nbeen chosen (DA layer). Transparency is closely coupled though not identical to scrutability \n(Kay, 2000). A scrutable system allows users to inspect their user model and to change it, e.g., \nin order to remove inaccuracies. Mapped onto the layered approach it may be evaluated \nwhether a user can undo or change system interpretations (ID layer), undo or change user \nmodelling actions (MW layer), or undo and change adaptation decisions (DA layer). \nDepending on the IAS at hand, it may well be that scrutability subsumes transparency in those \nlayers that it is applicable. Care should be exercised when making this assumption, however, \nsince transparency is typically understood more expansively and is thus applicable to more \nlayers than scrutability. \nControllability in this context refers to the user\u2019s perceived ability to regulate, control, and \noperate the product (Zhang, Rau & Salvendy, 2007). Users feel in control if the system \nbehaviour can be strongly influenced by the actions of the user (Norman, 1994; Winter, \nWagner & Deissenboeck, 2008). The term is sometimes also used to refer to the system \nproperty of the ability to move a system around in its entire configuration space (Ogata, 2009) \nwhich is obviously related but not identical to the user\u2019s subjective perception of being able to \ndo so.  \n 20\nIn the design of adaptive systems, the \u201cability of the user to determine the nature and \ntiming of the adaptation\u201d (Jameson & Schwarzkopf, 2002, p.193) is a key issue, because in \nmany systems the adaptation is triggered implicitly by user actions and while some users want \nto control each aspect of adaptivity, others may have less desire to do so. Controllability can \nbe evaluated in all layers, but is of particular importance for the following three layers: When \nModelling the Current State of the World users need to feel in control that they can influence \nwhat the system thinks about them. When Deciding upon Adaptation users should be able to \ncontrol which decision is taken. When Applying Adaptation Decisions users should be able to \ncontrol how the adaptation is implemented. The specific system goals and domain may \ndetermine how important controllability is considered for system success. While in adaptive \nlearning systems a high level of controllability seems to be desirable (Kay, 2001), it may be \nless so with recommender systems or agent-based systems (Trewin, 2000). As in the case of \ntransparency, there are commonalities between the criteria of controllability and scrutability, \nand similar caveats apply as controllability does not necessarily imply a satisfactory level of \nscrutability, but the opposite is usually true. \nFurther to the above, there may also exist functional criteria that are of importance to the \nsuccess of adaptivity and need to be subjected to user-based evaluation. For example, whereas \nalgorithm complexity is something that can be studied independently (see, e.g., Domshlak & \nJoachims, 2007), it may be necessary to assess in real-world conditions the efficiency of a \nsystem as perceived by its users. Assessment of this kind of criteria may sometimes require \nthe simulation of scale (e.g., in the number of users in the target population), which may be \naddressed with hybrid approaches, such as the employment of simulated users (see Section \n 4.3.2). \n4. Methods for the Formative Evaluation of IAS \nHaving presented the proposed evaluation framework for IAS, we now proceed to discuss a \nnumber of evaluation methods that can be used in conjunction with the framework. Returning \nto the differentiation between formative and summative evaluation, we would like to point out \nthat, due to the nature of layered evaluation, this paper focuses almost exclusively on \nformative methods. This bias is due to the fact that, whereas summative evaluation methods \nare well established and in wide use, the same is not true of methods that are formative in \nnature and have been specifically tailored or developed to cater for the distinct nature of IAS. \nThis section describes some of these methods and their application in the proposed \nframework. \nSeveral publications already discuss and\/or provide overviews of evaluation methods for \nadaptive systems (for example, Chin, 2001; Gena, 2005; Gena & Weibelzahl, 2007; van \nVelsen, van der Geest, Klaassen & Steehouder, 2008).  \nTo start with, Chin (2001) presented a detailed discussion of factors that need to be \nconsidered when planning an empirical evaluation of an adaptive system (termed a \u201cuser test\u201d \nin this paper). Chin (2001) placed emphasis on producing rigorous experiments that are well-\ncontrolled, use appropriate statistics, and are reported in sufficient detail. The discussion was \nrestricted to summative evaluations (and is, in fact, an excellent guide for evaluators \ninterested primarily in this type of studies). Qualitative methods were only briefly presented \nand their employment in adaptive systems not directly addressed. \nGena (2005) and Gena & Weibelzahl (2007) provide a comprehensive overview of \nevaluation methods for the adaptive web, derived from research in HCI. Inspired by Preece, \nRogers, Sharp & Benyon (1994) and Dix, Finlay, Abowd & Beale (1998), Gena (2005) \nclassifies these methods into: (a) collection of users\u2019 opinions, (b) observing and monitoring \nusage, (c) predictive evaluation, (d) formative evaluation and (e) experiments and tests. This \nclassification lacks clarity due to overlaps: for example, observing usage is often done while \n 21 \ndoing an experiment, and predictive evaluation is often of a formative nature. In later work \n(Gena & Weibelzahl, 2007) this is rectified and a distinction is made between data collection \nmethods (including the collection of user opinions and user observation methods) and \nevaluation methods. Gena & Weibelzahl (2007) classified methods according to: (a) the \nfactors the methods are most suitable to generate and evaluate (e.g., user satisfaction); (b) \napplicability conditions (e.g., a prototype and presence of expert evaluators); and, (c) their \nadvantages and disadvantages. Both papers provide an overview of layered evaluation \napproaches, however the link between the general HCI methods discussed and the evaluation \nof layers in an adaptive system is not made.  \nVan Velsen et al. (2008) review user-centred evaluation studies of adaptive and adaptable \nsystems. Rather than providing a framework for evaluation, they have taken a descriptive \napproach: mapping the current user centred evaluation practice, reflecting on its weaknesses \nand providing suggestions for improvement (e.g., the need to report think-aloud protocols in \nmore detail than current practice). Most of the methods discussed would be qualified as data \ncollection methods (as identified in Gena & Weibelzahl, 2007), rather than evaluation \nmethods; of these, questionnaires were identified as the most popular method in user-centred \nstudies of IAS. \nIn contrast to these existing overviews, this section provides an overview of methods \nspecifically tailored or developed for the evaluation of IAS. This overview is not intended to \nbe exhaustive (for more comprehensive accounts of general HCI evaluation methods readers \nare referred to Maguire, 2001, and Gena & Weibelzahl, 2007). As already discussed, the focus \nwill be on formative evaluation methods, and on relating these to the layered evaluation of \nIAS. Furthermore, methods will be related to phases in the life-cycle of system development, \nas this arguably largely determines their applicability\/suitability, and, ultimately, selection.  \nIn particular, this section distinguishes three development phases in which a layer may be \nevaluated: specification, design, and implementation. In contrast to the phases proposed by \nGena and Weibelzahl (2007), the specification phase excludes the earlier period, in which the \npurpose and goals of a system may be unclear, necessitating investigations of the \ncharacteristics of users, tasks and contexts of use. The assumption made here is that such \ninvestigations, if necessary, have taken place prior to the specification phase. In contrast to \nthe phases proposed by van Velsen et al. (2008), no distinction is made between high-fidelity \nprototypes and full systems. Arguably, formative evaluation is needed even when a system \nhas been fully implemented. Perhaps even more importantly, most of the evaluation methods \nthat can be used with a fully implemented system are also applicable with a high fidelity \nprototype. \nThe following sub-sections discuss and provide examples from the literature for the \nmethods that can be applied in each of the above three phases for the different layers. As \nformative evaluation aims to inform and improve system design, this blurs the distinction \nbetween design and evaluation. Actually, in user-centred\/participatory design, users are \ninvolved from the start, and design and evaluation go hand in hand.  Therefore, methods \nsuitable for the specification phase have also been included. \n 22\n d in detail in the subsections below and in \nsection  5.3. \n \nhis section can be used when the general functionality that corresponds \ntput. For instance, when considering the AA layer of an adaptive \nnews website, participants may be given examples of the layer\u2019s input such as \u201cemphasize \nfootball news, de-emphasize cricket news\u201d (i.e., the adaptation decisions made by the \npre  to discuss how the (de-)emphasizing should be \ninstantiated. A discussion may ensue of the relative merits of emphasizing through bigger \nfonts, re-ordering the news\u2019 list, adding star annotations, etc. Table 9 provides additional \nexa\nINSERT TABLE 9 ABOUT HERE \nTable 8 provides an overview of the methods, categorizing them on the basis of when, \nhow, and by whom the evaluation is done. It also shows which layers the method is \nparticularly suitable for. This table will be explaine\n \nINSERT TABLE 8 ABOUT HERE \n \n \nTo conserve space, the terms \u201cinput to a layer\u201d and \u201coutput of a layer\u201d will be used to \nrefer, respectively, to the input and output data of the portion of an adaptive system\u2019s \nfunctionality that corresponds to one of the framework\u2019s layers (and may match a particular \nsystem component, although this is not necessary). \n \n4.1. Methods for the Specification Phase \nMethods described in t\nto a layer in the proposed framework has been decided, but no design exists yet. In other \nwords, the system\u2019s input, and the desired kind of output for that stage are known, but the way \nin which the input will be transformed into output is not (or not fully).  \nThree methods are particularly useful in this phase: focus groups, the user-as-wizard, and \ndata mining. Which of these methods is most suitable depends on the nature of the layer and \nthe availability of data.  If the task under evaluation is one that humans may be good at, then \nthe user-as-wizard and focus groups methods are appropriate. If it is possible to obtain a \ndataset that maps input onto ideal output, then data mining may be appropriate. \n4.1.1. Focus Groups \nFocus groups are a type of interview conducted on groups. Participants provide their opinions \non issues in an informal group setting, facilitated by a moderator (Krueger & Casey, 2009). \nThis method is typically used early in the development process, to gather user requirements or \nobtain initial feedback on designs and prototypes. It produces rich qualitative data about what \nusers want and (dis-)like. The informal setting encourages discussion. Normally, multiple \nfocus groups are held on the same issues to avoid bias due to participant selection and group \ndynamics in one particular group. \nWhen focus groups are used in the specification phase, participants are told what kind of \ninput the layer would have and may be given examples of this input. They discuss how the \nlayer should produce its ou\nceding DA layer). They are asked\nmples of how focus groups can be used in the specification phase. The \u201cInput\u201d column \nshows what participants are told about the layer\u2019s input. The \u201cTask (Question to group)\u201d \ncolumn shows what the participants are asked to discuss. Where available, examples from real \nstudies are provided. If no reference is given, the example is hypothetical and included for \nillustrative purposes only. \n \n \n 23 \n \n \nFocus groups are suitable if humans are good at the task under evaluation. Often, this \nmeans that they are more appropriate for evaluations addressing the later layers in the \nadaptation process (i.e., MW, DA, AA). Even when participants seem capable of and are \nvocal in discussing how a layer should operate, results need to be used with caution. \nParticipants\u2019 subjective opinions may well be wrong. This is expressed in the well-known \ndesign mantra \u201cusers are not designers and designers are not users\u201d.  \nFor more hands-on information on how to run focus groups see (Krueger & Casey, 2009). \n4.1.2. The-User-as-Wizard \nThe user-as-wizard is a method introduced specifically to provide a structured way for using \nhumans to inspire the algorithms needed in an adaptive system. This method was first fully \ndescribed by Masthoff (2006), though it had been implicitly (partly) applied before (e.g., \nMasthoff, 2004; Masthoff & Gatt, 2006; de Rosis, Mazzotta, Miceli & Poggi, 2006; N\u00fcckles \net al., 2006). It integrates ideas from both Contextual Design and the Wizard-of-Oz methods. \nking them questions, as users\u2019 behaviour is often instinctive \nand\nsed when developing dialogue systems \ntha\nat humans tend to be good at adaptation, and, thus, \nobserving them in the role of the wizard may provide useful input in designing the system\u2019s \nIn this method, participants are given exactly the same information as the \n consolidation stage, this understanding is consolidated by participants \njud\nContextual design is an ethnographic method, in which users are observed in their work \nplace, to find out how they go about their work, in what environment, using which artefacts \n(Beyer & Holtzblatt, 1997). The idea is that users are the experts in their tasks and that \nobserving them is better than as\n their knowledge tacit. For example, Anderson, Boyle & Yost (1985) based their geometry \ntutor on observations of the strategies employed by teachers. However, observing experts in \ntheir normal setting is not ideal either, as the experts may use background and contextual \nknowledge that are not available to a system. Also, such studies are limited to situations that \noccur in the real-world setting. Finally, they are limited to the design of the system as a \nwhole, rather than individual adaptation layers.   \nWizard-of-Oz (see Section  4.3.1) is a method used in user tests in which a system designer \nplays the role of the system. It has, for example, been u\nt use speech recognition, to be able to evaluate the interactions without having to worry \nabout the quality of the speech. \nIn the user-as-wizard method, participants take the role of the wizard, and they are left \ncompletely free to perform the wizard\u2019s task, without giving them a script to follow. The \nrationale behind this method is th\nadaptive behaviour. \nsystem would have under the layer being evaluated. They deal with fictional users rather than \nreal ones. This allows studying multiple participants dealing with the same user and \ncontrolling exactly what information participants get.   \nThe method consists of two stages. In the first stage, the exploration stage, participants \ntake the role of the adaptive system, or, most frequently, of functionality that corresponds to a \nparticular layer. This investigates how humans perform the task that needs to be performed. In \nthe second stage, the\nging the performance of others.  \n \nExploration Stage \nParticipants are given a scenario describing a fictional user and their intentions (task). Using \nfictional users is a well-known technique in user-centred design, where so-called personas are \ndeveloped as examples of user classes (Cooper, 1999; Grudin & Pruitt, 2002). Similarly, \nscenarios\u2014stories of a persona doing a task\u2014are used extensively (Carroll, 2000). Personas \nand scenarios are also used in cognitive walkthroughs (Wharton, Rieman, Lewis & Polson, \n 24\nIn this stage, participants are given the task the adaptive system is supposed to perform. \n a scenario that describes 7-year-old Mary visiting a museum, and \ny have to adapt. They will automatically base their recommendations on \nwh\nroviding criteria on which to judge adaptation; and, (b) how they went \nabo\nample, suppose a navigation \nsup\ne \nthe\nnt to find out participants\u2019 reasons for their judgments. Similar \nbservational methods can be used as in the exploration phase. \ntanders were unable to discriminate between \ndialogue moves of the ITS and a human tutor. However, depending on the layer\u2019s task, we \nma want it to outperform humans (e.g., when detecting patterns in a user\u2019s typing \nbeh performance that is below human performance (e.g., \nwhen recommending books). \n \nFor instance, consider\nindicates that Mary likes horses, flowers, and the colour pink. A task given to participants \ncould be to recommend three paintings for Mary to view. There is no need to instruct \nparticipants that the\nat they know about Mary. Crucial to the success of the method at this stage is finding out \nthe participants\u2019 reasons for their decisions and actions, as this reflects: (a) what participants \nfound important, p\nut the task, providing inspiration for the adaptation algorithm. The same observational \nmethods as for a user study can be used (see Section  4.3.1). The above process may be \nrepeated for several scenarios.  \n \nConsolidation Stage \nThe consolidation stage verifies the acceptability of the human performance and determines in \nwhat respects it can be improved. Participants should not have been involved in the \nexploration stage.  \nFirst, participants are given: (a) a scenario involving a fictional user and their intentions; \nand, (b) an associated task. The scenarios and tasks used are typically the same as in the \nexploration stage. \nNext, participants are shown a performance on this task for this scenario. This can be a \nhuman performance (as from the exploration stage), or it can be a system performance (e.g., \nusing an algorithm based on the exploration stage). For ex\nport system needs to create a hierarchy of items of interest to the user. In the exploration \nstage, participants have produced such hierarchies. In the consolidation stage, participants are \nshown some of those hierarchies, and some hierarchies produced by an algorithm. Participants \nare not told whether the performance was by a human or system. They are then asked to judg\n quality of task performance (in the example, how good the hierarchies are), potentially on \nmultiple criteria. These criteria may be based on factors found to be important through \nobservations of participants in the exploration phase (e.g., participants wanted hierarchies to \nbe balanced in depth), or from input from system designers, or from indications in the \nliterature (e.g., discussions on breadth versus depth in the literature). Again, as in the \nexploration stage, it is importa\no\nThis procedure may be repeated for several task performances, presented in randomized \norder. Judgments of human performance may be interspersed with judgements of system \nperformance. Note that this resembles a Turing test (Turing, 1950), in that we could say that \nour system performs well, if participants judge it as well as they judge human performance. \nPerson & Graesser (2002), for instance, used a Turing test to evaluate the naturalness of \ndialogue moves of an ITS, finding that bys\ny \naviour), or may still be satisfied with \nExamples of the use of the method and its stages are presented in Table 10. The \u201cInput\u201d \ncolumn shows what participants are told about the layer\u2019s input, in the form of a scenario. The \n\u201cTask\u201d column shows what the participants are asked to do (i.e., producing the layer\u2019s output \nin the exploration stage, and judging the layer\u2019s output in the consolidation stage). The \n 25 \n\u201cObservational method\u201d column indicates what method was used to find out why participants \nacted \/ made their decisions the way they did. \n \n \nINSERT TABLE 10 ABOUT HERE \n \nA limitation of the user-as-wizard method is that it is not suitable for tasks that humans are \ntation algorithms on human performance is only sensible if humans \nhich \nfea\n that predicts learners\u2019 knowledge from their behaviour, one \ncou\nthe input (sensor data), they may use information unavailable to the component, such as facial \nexpressions. So, this can be used even when humans are not good at the task targeted by the \neva  desired output using richer input. When using this \napproach, at least two annotators are required and one should report to what extent they agree \n(e.g., using Cohen\u2019s kappa). \nTable 11 shows examples of the use of data mining in the specification phase. The \u201cGold \ndard used (combinations of layer input and ideal layer \nbad at. Basing adap\nperform well. Some tasks are inherently more difficult for humans than for computers. For \nexample, humans tend to be bad at processing large amounts of data. For such tasks, they may \nhave difficulty not just deciding what to do, but also judging performance. As in the case of \nfocus groups, this means that this method is most suitable for the later layers of the \nframework (i.e., MW, DA, AA). Another limitation is that participants\u2019 judgments may not \nalways correspond with what would be best for users. For instance, in a study of a medical \nreporting system, it was found that while doctors said they preferred graphs, they actually \nperformed better with texts (Law et al., 2005). For this reason, a normal user test will still be \nneeded. The user-as-wizard method is only intended as an initial step in the design process.  \n \n4.1.3. Data Mining \nData mining can be a very useful formative evaluation method in the specification phase if \nrepresentative data is available showing which inputs should result in which outputs. Data \nmining techniques can inform the layer\u2019s design by discovering patterns; for example, w\ntures of the input are important to predict the output accurately (Mobasher, 2007; \nMobasher & Tuzhilin, 2009). There are three ways in which such ideal, gold-standard, output \ndata can be obtained.  \nFirstly, the ideal outputs could be part of an existing dataset. For example, when designing \na system component that predicts a user\u2019s movie rating based on their ratings for other \nmovies, we can use the MovieLens dataset (Herlocker, Konstan, Terveen & Riedl, 2004). If \nthis data includes a particular user\u2019s rating for the movie, then this will be the ideal output for \nthe layer when it receives as input the other data for that user. \nSecondly, the ideal outputs may be measured directly in a special study. For example, \nwhen designing an algorithm\nld have learners interact with the system, gather behavioural data (the algorithm\u2019s input) \nand administer a test to measure the learners\u2019 knowledge (the algorithm\u2019s ideal output).  \nThirdly, a special study can be set up to indirectly measure the ideal outputs. For example, \nwhen designing a component that predicts learners\u2019 emotional state from sensor readings, one \ncould have learners interact with the system, gather sensor data, and use human observers to \nannotate observed emotions over time. This differs from the user-as-wizard method, as the \nobservers are not really performing the component\u2019s task: instead of deciding based solely on \nluation, but are good at producing the\nstandard\u201d column shows the gold stan\noutput). The table also shows how this gold standard was obtained, as this is the most difficult \naspect of using data mining. The availability of gold-standard output data is also used to \nevaluate designed and implemented systems, see cross-validation below (Section  4.3.3). For \n 26\nBLE 11 ABOUT HERE \n \nhich may have \nbee\n of how the system and its parts will work. It is assumed \nthat no (full) implementation of the functionality corresponding to the layer exists yet.  In \nribed here, a user test (a method typically associated with the \n is \napp\nprevention\u201d. For \u201cLearner control\u201d, they \n \n4.2. Methods for the Design Phase \nMethods in this section may be applied when the design has been (partly) completed. Initially \nin this phase, ideas will exist of how different system components will work, w\nn illustrated through storyboards showing what output the components will produce given \ncertain input.  Later in this phase, full algorithms and\/or graphical user interfaces (GUIs) will \nhave been designed, providing clarity\naddition to the methods desc\nimplementation phase) may also be applicable, by using a wizard-of-Oz technique (see \nSection  4.3.1). \n4.2.1. Focus Groups \nWhile focus groups would most frequently be used in the specification phase, this method\nlicable in the design phase too. The main difference is that participants are shown the \nsystem\u2019s input and output for the layer under evaluation, and discuss the output\u2019s \nappropriateness. Table 9 shows examples of focus groups in the design phase.  \nThe main limitation of using focus groups in the design phase is that they gather \nsubjective opinions only; as mentioned above in the user-as-wizard section, what people say \nthey like might not be best for them. \n4.2.2. Heuristic Evaluation \nIn a traditional heuristic evaluation, usability experts judge a system\u2019s user interface against a \nset of criteria. The most popular heuristics in usability testing are Nielsen\u2019s heuristics \n(Nielsen, 1994a): ten broad guidelines based on a factor analysis of common usability \nproblems. When evaluating a layer of an adaptive system, the experts need to be given \nexamples of the layer\u2019s input and resulting output. They also need appropriate heuristics.   \nCarmagnola et al. (2008) report a heuristic evaluation using the heuristics associated with \nJameson\u2019s (2003, 2005) five usability challenges for adaptive systems: predictability and \ntransparency, controllability, unobtrusiveness, privacy, and breadth of experience. These \nchallenges have been proposed in this paper as criteria to be used when evaluating a specific \nlayer, or the system as a whole (see section  3.2). Jameson linked these goals to frequently \nencountered properties of adaptive systems that may have detrimental effects in attaining \nthese goals. Accordingly, he proposed compensatory and preventive measures. Later, \nJameson (2009) extended the original five challenges into nine \u201cusability side effects\u201d of \nadaptive systems. This list no longer includes unobtrusiveness, but adds: need to switch \napplications or devices, need to teach the system, unsatisfactory aesthetics or timing, need for \nlearning by the user, and imperfect system performance.  \nMagoulas, Chen & Papanikolaou (2003) proposed an integration of Nielsen\u2019s original \nusability heuristics with layered evaluation for adaptive learning environments. For each \nlayer, they selected a subset of Nielsen\u2019s heuristics which they deemed particularly \nappropriate, and added more detailed criteria for these heuristics. For example, for the ID \nlayer, they selected \u201cLearner control\u201d and \u201cError \n 27 \nadded criteria such as \u201cthe same content is presented in various formats according to the \nlea et of heuristics for a layer appears promising, and it \nma s sense to have more detailed criteria for heuristics in the context of a layer. However, \nthe heuristics they selected for each layer and the criteria proposed do not always seem \napp\n \n HERE \nhe control is appropriate for this step, and will \npro\ne user will \nbe \nel. If there is a \nGUI which allows users to modify their user model directly, then a cognitive walkthrough can \nll be able to change a given user model to a \nModifications to the method may often be required to suit the evaluation of an IAS. In \nparticular, experts will likely need to be provided with multiple action sequences per task; \nrning profile\u201d. The idea of a specific s\nke\nropriate. In the example given, the criteria do not seem to address learner control directly, \nbut rather automatic adaptation to the learner profile.  \nTable 12 shows examples of how the criteria introduced in Section 3 can act as heuristics \nappropriate for layered evaluation. It also shows sample questions that can be asked about the \nlayers to judge how well they perform on the criteria. The table is not intended to provide the \ndefinitive set of heuristics for the evaluation of IAS, though it provides a good starting point. \nIt is a challenge for the IAS community to produce such a set, and, in particular, to base such \na set on an analysis of common problems discovered in IAS and the layers of IAS (similarly \nto Nielsen\u2019s factor analysis of usability problems).  \n \nINSERT TABLE 12 ABOUT\n \n \nHeuristic evaluation can in principle be applied to every layer, as long as appropriate \nheuristics have been agreed upon. The experts need to have expertise in heuristic evaluation, \nneed to understand the meaning of the particular heuristics and questions used, and need to \nunderstand the layer\u2019s input and output. However, experts are not real users, so results need to \nbe treated with caution. In addition, trade-offs between different heuristics may be required \n(as already mentioned in the discussion of the criteria in Section 3). For example, making the \nsystem status more visible may reduce unobtrusiveness. So, depending on the task and \ndomain, some heuristics may be of lower priority than others, and the relative importance of \nheuristics for a particular IAS may need to be considered. \nFor more information on how to conduct heuristic evaluations see (Nielsen, 1994a). \n4.2.3. Cognitive Walkthrough \nA cognitive walkthrough (Wharton et al., 1994) focuses on learnability: usability experts \nwork through typical user tasks, and decide for each action whether a novice user might \nencounter difficulties. They use the correct action sequence to accomplish each task. For each \naction, they keep four questions in mind: will users expect to do this, will they notice the \ncontrol (e.g., button), will they recognize t\ngress be apparent once it has been used. This method is most suitable for the evaluation of \nlayers that have direct or indirect effects on the GUI, i.e., the DA+AA layers or the system as \na whole. For example, consider the evaluation of the DA+AA layers of an ITS which \nannotates lessons with traffic light icons based on whether the learner is ready to learn them \n(after a knowledge test). A cognitive walkthrough can be used to evaluate if a novic\nable to select the optimal sequence of lessons to reach a particular learning goal. \nSometimes it may be possible to evaluate earlier layers of the framework, such as the MW \nlayer. For example, consider the evaluation of the scrutability of a user mod\nbe used to evaluate whether a novice user wi\nparticular desired state. If there is no such GUI, a cognitive walkthrough can be used only if: \n(a) there is a GUI to provide input to the user model (e.g., rate news stories); and, (b) the \nmodelling algorithm has been designed such that a correct action sequence can be identified \n(difficult when Machine Learning is used).  \n 28\nafter all, the system\u2019s behaviour may well change depending on the user. Also, a cognitive \nlity to be \neva\nresponding to an evaluation layer has been implemented, it can be \nence task-based user \nater).  \nare \net al. (2006) asked \n, while thinking-\nwalkthrough (when applied unmodified) typically looks at the first time a user does a task, \nignoring that the system may change over time, after learning more about the user. \nUnfortunately, there is a complete lack of reported cognitive walkthroughs in the IAS \ncommunity, and therefore, no table with further examples has been provided. For more \ninformation on how to run cognitive walkthroughs see (Wharton et al., 1994). \n4.3. Methods for the Implementation Phase \nMethods in this section can be used when a prototype of the system functiona\nluated has been implemented. This may be a limited prototype which can only deal with a \nsubset of inputs, or a full implementation. \n.3.1. User Tests 4\nOnce the functionality cor\ntested by real users. Typically, users are given well-defined tasks to do; h\ntest will be used to identify the most common type of user test. Measurements are made of \nusers\u2019 performance (e.g., how fast they learn in an ITS) and opinions. Observational methods \nare used to identify the cause of problems. The main difficulty of testing an individual layer \nof adaptation is that it may be hard for participants to provide the kind of input required, \nnecessitating the presence of special interactive facilities to support the  process (alternatives \ninclude doing indirect user tests, or employing simulated users, as discussed l\nOther pitfalls for the empirical evaluation of adaptive systems have been noted (Chin, \n2001; Masthoff, 2002; Gena & Weibelzahl, 2007; Tintarev & Masthoff, 2009), but these \nnot specific to the layered evaluation of adaptive systems and are therefore not repeated here.    \n \nObservational Methods \nDifferent observational methods can be used in a user test, such as:  \n\u2212 Thinking-aloud. Participants are asked to verbalize their thinking while performing a task \n(Ericsson & Simon, 1993; Lewis, 1982; Nielsen, 1993). N\u00fcckles \nexperts to think-aloud when deciding what explanation would be best, given the learner\u2019s \nknowledge level. D\u2019Mello, Craig, Sullins & Graesser (2006) used a variant called emote-\naloud: learners verbally expressed their emotions. Porayska-Pomsta et al. (2008) \nsuggested asking learners to describe what they are thinking and feeling. \n\u2212 Co-discovery. Participants work together with somebody they know well, and their \nnaturally arising discussion exposes their thinking (O\u2019Malley, Draper & Riley, 1984).  \n\u2212 Retrospective testing. Using an interview or questionnaire, participants report their \nthoughts after the task has finished, possibly while watching a video of their actions \n(Nielsen, 1994b). The latter is also called retrospective thinking-aloud\naloud during the tasks is sometimes called concurrent thinking-aloud. \n\u2212 Coaching. Participants are encouraged to ask questions when they encounter problems, \nhelp is provided and notes are made of these issues (Nielsen, 1994b). \nSome changes to the observational methods may be needed when evaluating an IAS. For \ninstance, when investigating usability it is normally stressed that participants are not to be \naided (unless using coaching), and not to be asked direct questions during the task as these \nmay guide them. However, when evaluating an adaptive system this may cause problems. For \ninstance, users may not even notice the adaptation occurring, which may make it necessary to \ninterrupt them, and ask them about it explicitly. For example, when evaluating scrutability, \nand participants fail to notice the scrutability tool (as happened in Czarkowski, 2006), it may \nbe good to lead them to it (making a note to improve its visibility). Alternatively, adaptivity-\nrelated activities may be incorporated in the tasks to alleviate this problem.  \n 29 \nFurther to the above, the normal limitations of observational methods apply also when \nevaluating IAS. It is often claimed that both thinking-aloud and co-discovery may interfere \nwith participants\u2019 cognitive processes, slowing them down and making them behave differ-\nently than they normally would (as also noted by Chin, 2001). However, the impact may de-\npend on how strictly Ericsson and Simon\u2019s (1993) principles for thinking-aloud are followed. \nAd\ng & Schellens (2004) found that participants enjoyed co-dis-\ncov\nesting may lead to participants justifying their errors, and being insincere. Ret-\nspective testing may suffer from participants not being able to recall why they did things. \nal. (2003) found that concurrent and retrospective thinking-aloud \nerface. This technique is used for rapid \npro\n is fully implemented.  \nWizard-of-Oz has been used in the evaluation of adaptive systems for a long time. For \n, Greenberg & Mander (1993) used wizard-of-Oz to prototype an intelligent \nhering to these principles, classic thinking-aloud aims at verbalisation without mental \nprocessing, only prompting by \u201ckeep talking\u201d, not establishing personal contact or directing \nthe participant\u2019s attention. Usability studies often use a more relaxed approach, which may \nlead to mental processing and interference with task performance (Ericsson & Simon, 1993). \nEven classic thinking-aloud has in some studies been found to decrease task performance (van \nden Haak, de Jong & Schellens, 2003) and increase task duration (Hertzum, Hansen & Ander-\nsen, 2009), though it has also been found to have little effect on participants\u2019 behaviour and \nmental processes (Hertzum et al., 2009). In contrast, relaxed thinking-aloud clearly changed \nbehaviour and increased perceived mental workload (Hertzum et al., 2009). A change of be-\nhaviour is even worse when evaluating an IAS, as it may influence the adaptation taking \nplace. Based on the above, the classic variant of thinking-aloud would be preferable for \nevaluating an IAS. The coaching method clearly changes task performance as participants can \nask for help, and may, therefore, be inappropriate for IAS. \nThinking-aloud also requires training, and is less natural than co-discovery and coaching. \nA study by van den Haak, de Jon\nery more than both concurrent and retrospective thinking-aloud. However, co-discovery \nmay be less natural\/suitable when a system is supposed to adapt to an individual user (unless a \nuser model is provided, as in the indirect experiments discussed below). Thinking-aloud and \nretrospective t\nro\nHowever, van den Haak et \nprotocols revealed comparable sets of usability problems. Given the reduction in task per-\nformance for concurrent thinking-aloud, they argued in favour of retrospective thinking-\naloud, while noting that it may be less suitable for more complicated tasks. In contrast, van \nden Haak et al. (2004) argued in favour of concurrent thinking-aloud, as it is less resource in-\ntensive than retrospective thinking-aloud (which requires twice the amount of time) and co-\ndiscovery (which requires twice the number of participants). They did not find a difference in \ntask performance in that study.  \nThe best observational method is likely to depend on the available resources (time and \nnumber of participants), the task type and complexity, the type of participants (importance of \nparticipant enjoyment and naturalness), and the importance of avoiding changes in participant \nbehaviour.     \n \nWizard-of-Oz Technique \nIf a layer\u2019s functionality has not been implemented yet, it may still be possible to do a user \ntest by using a wizard-of-Oz technique (Gould, Conti & Hovanyecz, 1982). A human \n\u201cwizard\u201d (somebody from the design team) simulates the system\u2019s intelligence and interacts \nwith the user through a real or mock computer int\ntotyping when a system is too costly or difficult to build (Wilson & Rosenberg, 1988). The \nwizards tend to follow a precise script. Participants are typically unaware that a wizard is \nused, and believe the system\nexample, Maulsby\nagent. Several recent UMUAI papers report on wizard-of-Oz studies (Miettinen & Oulasvirta, \n2007; Batliner, Steidl, Hacker & N\u00f6th , 2008; Damiano, Gena, Lombardo, Nunnari & Pizzo, \n2008; Conati & Maclaren, 2009). For example, Miettinen and Oulasvirta (2007) used wizard-\n 30\nof-Oz to simulate the system functionality that corresponds to the CID \/ ID layers: sensors \nwere simulated by human codings of data. In a layered evaluation, wizard-of-Oz can also be \nuseful to simulate layers preceding the one being evaluated, to ensure these work perfectly \nand to enable the evaluation of a layer in isolation. A wizard could also help users to provide \ninput for a layer that has no user interface normally. \n09), wizard-of-Oz is impractical for large-\nwed \nto f\nation takes time, often \ntoo\nt, standard user tests will be called direct.  \n an indirect user test, participants perform the task on behalf of somebody else, rather \nthan for themselves. This allows the evaluator to control the characteristics of the person for \nwh ding the time delay otherwise needed for initializing \nand populating the user model from actual user interactions with the system. Importantly, an \nindirect experiment also ensures that the input to a layer is perfect, making it very suitable for \nlay\nocus on a particular behaviour of the system that did not always \noccur and wanted to remove extraneous factors from the evaluation. Indirect user tests are less \nnd the results may therefore be less reliable.   \ninancial and temporal \nterm\nsessions. It may be hard to get users that can participate long enough for the adaptation to be \nAs noted by Walker, Rummel & Koedinger (20\nscale research as it creates uncertainty as to whether different facilitators acting as wizards \nmay have different effects. \n \nPlay-with-Layer \nPlay-with-layer is a variant of a user test in which participants are not given tasks, but allo\nreely explore the system or layer. They freely input data as if coming from the preceding \nlayer in the adaptation process, and judge the output. There are several ways of judging a \nsystem\u2019s behaviour for a particular layer. Firstly, it can be judged against criteria. Secondly, a \nquestionnaire or interview can be used to obtain participants\u2019 opinions. Finally, it may be \npossible to use objective measures, for example the frequency of occurrences of certain \nevents, such as adaptations.  \n \nIndirect User Test \nA problem with using a user test for an adaptive system is that adapt\n much time to be able for the system to adapt during a typical one-hour experiment. One \nsolution is to focus on evaluating the later layers in the framework, with the user model \nprovided (by, or to, the participants). When the user model is provided to the participants, this \ncomes down to an indirect user test. In contras\nIn\nom participants perform the task, avoi\nered evaluations. George, Zukerman & Niemann (2007) used an indirect experiment \nbecause they wanted to f\nnatural for participants, a\nTable 13 shows examples of both standard user tests (task-based, direct), and indirect and \nplay-with-layer variants. It shows the input of the layer, the task performed by participants \n(for standard and indirect user tests), the measurement and observational methods used, and \nthe criteria that the layer is evaluated against. \n \n \nINSERT TABLE 13 ABOUT HERE \n \n \nFor more information on how to conduct, design and report user tests see (Robson, 1994; \nDumas & Loring, 2008). \n4.3.2. Simulated Users \nA general problem with user tests is that they tend to be costly in both f\ns. This may be further hampered by difficulties in recruiting a sufficient number of users. \nThe situation is even worse when evaluating an adaptive system. Adaptation takes time, so a \nuser study may need a long duration or even be longitudinal, with users taking part in multiple \n 31 \nful\n model. Or, as \nwe have seen, even when it is possible for the user to provide the input, special interaction \nfacilities may need to be implemented for this purpose. For these reasons, the simulated users \nme  of users instead of real users. \n usability testing, model-based testing has been proposed as a way to quickly test \nsystems without the need for real users. Methods such as GOMS (Card, Thomas & Newell, \n198\nhat if those assumptions are \nwrong? For example, the simulated voices used for evaluation by Chickering & Paek (2007) \n to train the baseline model. So, what if these simulated voices were \ngiven layer. The other part (called the \ntes\nly tested. Recruitment is further complicated by the need for many different types of users \nto fully measure the impact of adaptation, the accuracy of user modelling, etc. In addition, \ncomparatively more formative testing is probably needed in IAS, as they tend to contain \nintelligent algorithms, and many adaptation alternatives to compare. Finally, when evaluating \nan individual layer, it may be hard for users to provide the layer\u2019s input. For example, when \nevaluating the DA layer, it may be difficult for users to provide a correct user\nthod is based on computational models\nIn\n3) are used as a basis for implementing simulated users (e.g., using a probabilistic model). \nMurray (1993) proposed the use of simulated students for formative ITS evaluation. This \nmethod has since been used by many ITS researchers (e.g., VanLehn, Niu, Siler & Gertner, \n1998; MacLaren & Koedinger, 2002; Mill\u00e1n & P\u00e9rez de la Cruz, 2002; Guzm\u00e1n, Conejo & \nP\u00e9rez-de-la-Cruz, 2007). It has also been used in the evaluation of other types of adaptive \nsystems. Table 14 shows example studies. It shows the layer\u2019s input produced by simulated \nusers, the measurements taken, and the criteria on which the layer is evaluated. \n \n \nINSERT TABLE 14 ABOUT HERE \n \n \nThe advantage of using simulated users is that different aspects of adaptation can be tested \nrapidly, and that the system inputs for the different layers can be controlled. The main \nproblem is that the models used for building the simulated users are likely to be based on the \nsame assumptions that underlie the adaptive system\u2019s design. W\nare a subset of those used\nunrealistic? A second issue is that modelling static user behaviour differs from modelling \nadaptive user behaviour. A model that accurately captures user behaviour when the system is \nstatic, does not necessarily accurately predict how users will behave when a system adapts. \nFinally, despite their usefulness in formative evaluations, simulated users will not be able to \nprovide qualitative feedback, or provide subjective opinions on vital aspects of the system \n(e.g., aesthetics, feeling of trust). We therefore advocate using simulated users initially to gain \nrapid insight, and reverting to real users to validate findings. Indeed, most papers mentioned \nin Table 14 report on additional studies with real users to either validate the simulation \nmodels or to validate the findings of the simulations (Masthoff, 2002; Masthoff & Gatt, 2006; \nGuzm\u00e1n et al., 2007; Hollink, van Someren & Wielinga, 2007).  \n4.3.3. Cross-Validation \nThis method is appropriate for validating the accuracy of a layer\u2019s output if there exists a \ngold-standard: representative data showing which inputs should result in which outputs (see \nSection  4.1.3). The data is split into two parts: one part (called the training data) is used to \ninform the design of the system\u2019s functionality for a \nt data) is used to verify the accuracy of the (potentially implemented) design. To avoid \naccidental effects caused by the way the data is split, more rigorous forms of this approach \ntend to be applied, such as k-fold cross-validation (Kohavi, 1995): the data is split into k \nsegments, and at any time k-1 segments form the training data, with the remaining segment \nacting as test data. This is repeated k times, with each segment in turn acting as test data.  \n 32\nCross-validation is by far the most frequently used method in UMUAI papers of the last \nthree years: it was used 20 times in the period 2007-2009 (compared to only 12 times in all \nthe preceding years). This effect, however, may be partly due to the special issues on Data \nMining and Personalization (Mobasher & Tuzhilin, 2009) and Statistical and Probabilistic \nMethods for User Modelling (Albrecht & Zuckerman, 2007).  Table 11 shows examples of \ncross-validation. Note that components constructed on the basis of results derived from data \nmin\nThere is a question about whether this method has a place in this paper, given our stated \nion. The method in itself is perfectly valid; however, evaluators \ne, the criteria proposed in section  3.2.7, or the criterion \u201cbreadth of \n \ne provides some \no assist in the design and \neva\n\u2212 Objective Metric: captures the objective of the adaptive system (e.g., decrease error rate). \ning are normally evaluated using cross-validation. \nemphasis on formative evaluat\nthat use it tend to only apply this method and then report on the accuracy of the evaluated \ncomponent. Therefore, evaluations based on this method tend to be completely summative, \nwithout any formative insights. In our opinion, this does not have to be the case. The \naccuracies achieved tend to be quite far from 100%, and one wonders whether it would not be \npossible to analyse in what kind of cases the aspect evaluated is sub-optimal, so that at least \nsome insight is gained into when it works well and when it needs improving. Another \nlimitation of the method is that it only investigates accuracy (be it in all its forms, such as \nMAE, precision, recall, ROC) and sometimes efficiency, and there are many other criteria that \nmay need evaluating. Finally, this method\u2019s need for gold-standard output normally makes it \nunsuitable for the DA and AA layers. \nFor more information on how to use cross-validation see (Witten & Frank, 2005). \n5. Using the Framework  \nHaving discussed the framework itself and formative evaluation methods that can be used in \nassociation with it, in this section we turn our attention to practical issues related to the \nemployment of the framework. Firstly, we discuss how the application domain and type of \nadaptation employed may affect evaluation, and specifically the selection and \noperationalisation of assessment criteria. We then concentrate on the evaluation of layers in \ncombination for the needs of particular systems and evaluation studies. This is, finally, \nfollowed by a synthetic view over the evaluation methods presented above, offering \npreliminary guidance for selecting a method (or methods) for specific evaluation settings.  \n5.1. Considering the Application and Adaptation Domain \nSection  3.2 discussed a number of criteria that can be used when employing layered \nevaluation. These were selected on the basis of their generality and wide applicability, and \nare, in their majority, layer-specific. However, for all but the most trivial cases, there will be \nattributes of adaptation that are \u201ccross-cutting concerns\u201d over more than one (or even all) \nlayers (see, for instanc\nexperience\u201d argued to be applicable both when deciding upon, and when applying adaptations\n\u2013 sections  3.2.4 and  3.2.5 respectively). Often, what these attributes are depends on the \napplication domain and the type(s) of adaptation supported. Their identification and \noperationalisation is not always a straightforward task, but the literatur\nguidelines that can assist towards this end. \nOne approach which can be used to guide the selection of criteria comes from Browne et \nal. (1990) who propose that a number of \u201cmetrics\u201d be defined t\nluation of adaptive systems. Totterdell & Boyle (1990) provide a more detailed account of \nhow these metrics can be used to drive the assessment of adaptation. Note that the word \n\u201cmetrics\u201d, as used in the preceding publications does not necessarily refer to measurable \nindices in a system, but rather operationalised discrete elements of the system\u2019s adaptive \nbehaviour. Of the proposed metrics, some are of direct relevance to the discussion here \n(Browne et al., 1990): \n 33 \n\u2212 Theory Assessment Metric: required when the success of the system in obtaining its \nobjective is related only indirectly to the aspect of interaction that the system is attempting \nto improve (e.g., increase user satisfaction through reduced error rates).  \n\u2212 Trigger Metric: describes the aspect of user interaction on which the adaptation is based. \n\u2212 Recommendation Metric: provides a description of the output of the theory-based part of \nthe system (i.e., the adaptation decisions made by the system). \nTotterdell & Boyle (1990) argue that by specifying and assessing these metrics in relation \nto one another, one can answer many questions about the functioning of an adaptive system. It \nis further argued here that the Objective- and Theory Assessment- metrics in particular, can \nserve as a guide for defining criteria that permeate the evaluation of individual layers or the \nsystem as a whole. Consider, for instance, a system that controls temperature and lighting in a \nhouse. For such a system, the Objective Metric may be associated with the automatically \nbitants. The Theory Assessment Metric would then possibly \nants have to exert to attain the desired temperature and \n\u2019s initiative in modifying these settings. Note that \nn design; these high-level metrics would then \ntities that, in turn, can be derived through the \nn methods and data collection instruments. \nA second approach which can be used to guide the selection of criteria, and is along \nsimilar lines to the specification of metrics, is to focus on the dimensions of adaptation in a \nsy  constituents, to arrive at the operationalisation of \nattributes that need to be assessed during evaluation. Knutov et al. (2009) identify six \nquestions that, when answered, can provide a reasonably complete definition of adaptation in \na sy\ntion activities start from this very dimension to \n as a whole. The integration of \nen by the proposed framework can take place from \n                                                \nachieved comfort level of the inha\naddress the effort levels that the inhabit\nlighting settings, in relation to the system\nthese are but the first steps towards an evaluatio\nhave to be broken down to measurable quan\napplication of selected evaluatio\nstem, including its determinants and\nstem, as well as the ways in which they relate to each other (Figure 8)7: \n\u2212 What can we adapt? (What?) \n\u2212 What can we adapt to? (To What?) \n\u2212 Why do we need adaptation? (Why?) \n\u2212 Where can we apply adaptation? (Where?)  \n\u2212 When can we apply adaptation? (When?) \n\u2212 How do we adapt? (How?) \n \n \nINSERT FIGURE 8 ABOUT HERE \n \n \nThere are apparent correspondences between the metrics proposed by Totterdell & Boyle \n(1990) and the questions\/dimensions put forward by Knutov et al. (2009). Perhaps the most \nimportant such correspondence is that between the Theory Assessment metric and the \nAdaptation goals (Why?), which is usually what an evaluation of an IAS sets out to assess in \nthe first place. It is recommended that evalua\ndefine measurable criteria for individual layers and the system\nsuch criteria into an evaluation process driv\ntwo complementary perspectives: (a) evaluators can specify the layers for which the defined \ncriteria are relevant and incorporate them into their evaluation design; (b) the criteria, when \nthey represent cross-cutting concerns, may also determine what combinations of layers (a \nsubject addressed in the subsequent subsection) may be addressed to get a more holistic \npicture of the system.  \n \n7 Knutov et al. (2009) restrict their analysis to Adaptive Hypermedia Systems, but the questions and their \ninterrelations are arguably more generally applicable to most classes of IAS.  \n 34\nBoth propositions put forward here are intended to facilitate the process of formalizing the \nunderlying design decisions in an IAS, so as to enable the derivation of the domain-specific \ncriteria that will be used to assess these decisions. Of utility in this context may be other \nevaluation frameworks that propose complementary or alternative approaches to layered \nevaluation, and are discussed in Section  6.3. \n5.2. Evaluating Layers in Combination \nWhen presenting the evaluation layers, it was often remarked that evaluating them in isolation \nmay not be feasible due to the nature of adaptivity in the system, the system\u2019s architecture, \netc. In addition to such practical considerations, one may also have to observe organizational \nand resource constraints that may apply in the evaluation. For instance, a system may be \nsufficiently complex that evaluating each layer in isolation would require an amount of \nresources not readily available. When such constraints exist, or when assessment criteria need \nto be evaluated across layers as discussed above, it may be necessary to evaluate layers in \ncombination. This section discusses potential combinations of layers and considerations for \ntheir employment. \nStarting from the end of the adaptation process, a combination that is often made in the \nliterature is between the layers of deciding upon (the type of) adaptations, and the layer of \neffecting the said adaptations in the interactive front end. This combination is often motivated \nby the fact that most adaptive systems do not support alternative manifestations of adaptation \ndecisions at the syntactic and lexical levels of interaction. For instance, an adaptive learning \nsystem usually supports only one way of denoting links are \u201cready to read\u201d. Although, in \ngeneral, this combination is a reasonable one, evaluators should be careful when drawing \nconclusions about a system\u2019s adaptive theory from results thusly derived. This is especially \ntrue in the case that results are negative, since this could be attributed either to a faulty \nhypothesis serving as the basis of adaptation, or to an inappropriate incarnation of the \nadaptation decision at the physical level of interaction. This may be the case, for instance, \nwith the results reported by Brusilovsky et al. (2001), where the authors ensured the validity \nof the user model, and concluded that the identified problems must lie with the adaptation \ntheory \u2013 but did not separately check whether alternative manifestation of adaptive navigation \nsupport might have led to better results. At the opposite end of the spectrum, even if the IAS \ndoes distinguish between the two layers, it is possible to treat them jointly in terms of \nevaluation by: (a) enumerating all the possible concrete manifestations an adaptation decision \nmay have, and (b) treating each decision - concrete instance pair as a distinct decision. \nAnother combination often made in the literature merges together the first three layers of \nthe proposed framework, treating the collection of input data, its interpretation, and the \nmodelling of the resulting knowledge as a single step or a single stage in the adaptation \nprocess. Again, this is in many cases a reasonable combination, but may suppress the true \norigin of identified problems. Consider the case of a personalized museum guide, in which \nvisitations of artefacts in the museum\u2019s physical space are used to infer the visitors\u2019 interests \nin different styles, epochs, artists, etc. If the evaluation of the first three layers in combination \nshows that the user model only poorly represents the users\u2019 real interests, what should that be \nattributed to? The system\u2019s component that determines a person\u2019s position and direction of \nsight in the museum\u2019s rooms? The algorithm that translates a series of positions into \u201cvisits\u201d? \nThe assumption that visitors will only stand in front of artefacts that fall within their interests? \nThe extrapolation of common characteristics between the visited artefacts? In an evaluation \nthat merges together the first three layers, such questions may be impossible to answer with \nany certainty.  \nA combination that is potentially less challenging than the aforementioned one merges \ntogether only the first two layers of the proposed framework, namely the collection of input \ndata, and its interpretation. This can be entirely straightforward in situations where the \n 35 \ninteraction data assembled is unambiguous, and\/or represents the entirety of data observable \nby the system. In such cases, the only processing that occurs and may, therefore, result in \nerrors, is concentrated in the interpretation of the collected data. If, however, this premise \ndoes not hold, this combination is susceptible to the same kind of problems discussed above. \nIt should be noted that by adopting two of the layer combinations discussed above,  \nnamely treating the first three of the proposed framework\u2019s layers as one and the last two \nlikewise, we effectively arrive at the two-layer decomposition proposed by Karagiannidis & \nSampson (2000). Employment of the two-layered evaluation approach is a major step forward \nndividual adaptation steps, and \narding the layers (or \nthe\nction of the evaluation methods most appropriate for the evaluation \nicipants, as they are the ones who \n This could be input that is normally gathered over a long period of \nfrom traditional practices that make no attempt at assessing i\ncould be considered the most minimalistic decomposition plausible for evaluating adaptation.  \nIn summary, combining layers is a reasonable approach under certain circumstances, and \npossibly the only feasible one in some cases. When employing it, however, researchers and \npractitioners should exercise additional caution when: (a) using criteria that are meant for the \nevaluation of individual layers (and whose semantics may be diffused when merging layers); \nand, more generally, (b) planning the evaluation to prevent the occurrence of unattributable \neffects. All potential difficulties that arise when merging layers can be traced back to the fact \nthat the individual layers still exist, but are \u201chidden\u201d (as are their effects on adaptation) from \nthe perspective of the evaluator. A thorough understanding of this fact and its repercussions \nis, in the authors\u2019 opinion, a prerequisite for the successful application of the layered \nevaluation approach with combined layers. \n5.3. Selecting Evaluation Methods for Layered Evaluation \nIn the planning of evaluation studies, once decisions have been made reg\nir combinations) that need to be assessed, and the criteria this will be done against, the next \nissue to tackle is the sele\nsettings. The presentation of methods for the formative evaluation of IAS in Section  4 has \nadopted the explicit assumption that the most appropriate evaluation method(s) in a given \nsituation will depend on the development phase. Other factors to be considered include who \nwill be involved and which data is available.  \nFrom the overview of methods it is clear that the evaluation can either involve users, \nexperts, or simulated users. Users are the most realistic part\nwill end up using the system. Experts may be required when the layer\u2019s input and\/or output is \ndifficult to understand for ordinary users (e.g., for an IAS using a decision-theoretic model to \ndecide upon adaptations). Experts may also have a better understanding of evaluation criteria \n(as required for example for heuristic evaluations and cognitive walkthroughs). Simulated \nusers may allow for rapid and controlled testing of multiple alternatives.  \nEvaluation methods also differ in terms of the input and output data for the component(s) \nevaluated in each layer: \n\u2212 The layer\u2019s input. The input data to the component(s) that embody the functionalities that \na layer is intended to assess can be either given to the participating end users or experts, or \ndecided by themselves.\ntime, for example a user model that has been built up over a period of weeks. Allowing \nthe participants to decide the input may require the development of special interaction \nfacilities for this purpose, as most layers will lack this.  \n\u2212 The layer\u2019s output. Similarly to the input, we can either provide the output data of the \ncomponent(s) corresponding to a layer to the participants, or let them produce that output \nthemselves. Presenting such output may require effort, as most components involved in \nthe adaptation process will not normally have a front end (interactive or otherwise). \nAnother problem is that it may be hard to differentiate between outputs intended to be \nassessed at two layers (e.g., it may be hard to consider the outputs of the Apply \nAdaptation and Decide upon Adaptation layers separately). \n 36\n compare the output with a gold standard (the ideal output for the \norresponding input).  \nable 8 (see Section  4, page 63) provided an overview of the methods discussed in this \npap ts. Figure 9 puts together a set of rules of thumb that \nevaluators can follow, summarizing the discussion and propositions made in Section  4. The \ndiamonds (\u25ca) indicate questions that guide the selection of methods. For example, the first \nquestion is what the development stage is, and depending on the answer different methods \nning, user as wizard, and focus group are \nther questions. For example, the later two \nINSERT FIGURE 9 ABOUT HERE \nternative \nTh\nc\nT\ner and how they differ on these aspec\napply. If it is the specification stage, then data mi\npossibilities. Which of these is best depends on fur\nmethods are only suitable if it is a task humans are good at. Note that the organization in \nFigure 9 is only partial, and is intended to facilitate decision making, but not necessarily to be \nthe sole driving force in this respect. After all, the best method will also depend on the criteria \none wants to evaluate. For example, the simulated users method is not suitable to evaluate \nsubjective acceptability. \n \n \n \n \n6. Limitations and Alternative Approaches \nIn this section we discuss the framework\u2019s scope, focusing on areas that restrict its \napplicability. Following that, we provide a brief account the potential limitations of formative \nevaluation, and how the discussion of formative evaluation methods is also relevant to \nsummative evaluation. The section closes with a discussion of complementary and al\nframeworks and approaches that have been proposed for the evaluation of IAS, and that one \nmay want to consider in order to alleviate some of the discussed shortcomings of the proposed \napproach. \n6.1. Scope and Limitations of the Proposed Framework \nThe proposed framework is intended to be applicable to as wide a range of IAS as possible, \nindependently of their application domain, type and purpose of adaptation, etc. It is meant to \nguide the design at different stages of the development lifecycle of an adaptive system. The \nframework itself is intentionally not prescriptive in terms of evaluation methods, techniques, \nand data collection approaches, but strives to provide guidance for evaluators to make \ninformed decisions on these matters. Although it can be readily used to inform the design of \nsummative studies of specific aspects of an adaptive system\u2019s behaviour, it has been primarily \nconceived to facilitate the planning and undertaking of principled formative studies.  \ne framework does exhibit a number of limitations that should be taken into account when \napplying it. These relate primarily to the applicability of layers in certain types of IAS, \naspects of adaptation not directly addressed by the framework, and, arguably, the feasibility \n(in terms of temporal and resource constraints) of applying the framework in its entirety. \nIt has already been discussed that some of the proposed layers may not be possible to \nevaluate separately in a system, or, for that matter, may not even exist \u2212 a fact probably \nobvious for the case of the first and last layers in the framework, but not exclusive to them. \nFor instance, for systems that use inference mechanisms which relate input data and \nadaptation decisions directly \u2212as sometimes found in machine learning systems (Krogs\u00e6ter, \nOppermann, & Thomas, 1994; Pohl, 1997, 1999)\u2212, the modelling layer might not be \n 37 \napplicable in isolation. One possible way of mitigating this type of problems, namely the \ncombination of layers so that the resulting adaptation process stages (and corresponding \nlay\nss, \ncom\nective of evaluation. For certain systems it may be possible, for example, to \ntrea\nc. \nnother limitation lies in the breadth of the framework. Applying all layers and criteria to \na single system, potentially at various stages of the development process, is next to \nwork is meant to inform and guide study design \nan implicit suggestion of a superiority of formative evaluation. \ners) better reflect the system\u2019s actual operation, has been discussed in detail in Section  5.2.  \nAt a different level, the framework deliberately does not address the evaluation of meta-\nadaptivity. The term is used here to refer to systems that are capable of assessing and \nmodifying their own adaptive behaviour, learning, in the process, to identify situations in \nwhich different adaptations are best applied. Although there are different forms and levels of \nsophistication of meta-adaptivity, some of which not even computationally possible yet \n(Totterdell & Rautenbach, 1990), all of these share one characteristic in common: they require \nthat a system be capable of self-evaluating its own adaptive behaviour. In more detail, this \nrefers to the run-time assessment of the effects of decided upon and effected adaptations, with \nthe intent of evaluating their \u201csuccess\u201d (i.e., whether the goals underlying their introduction \nhave been met). This stage is referred to as \u201csecond-level adaptation\u201d in Totterdell & \nRautenbach (1990) and may further involve the modification of aspects of the lower-level \nadaptation cycle (e.g., by enabling or disabling rules in rule-based adaptation, or by altering \nthe \u201cweight\u201d of alternatives, in decision theory-based adaptation).  \nThe evaluation of meta-adaptivity is, as one might expect, a complicated matter. \nPractically, it necessitates the consideration of an additional second-level adaptation proce\nprising:  identification\/isolation of the effects of applied adaptations on the user\u2019s \nbehaviour; comparison between said effects and the ones intended or desired; and, potentially, \nselection and application of alternative sets of behaviours. A plausible evaluation approach \nmay involve ensuring that the system shares the same views as the users with regards to the \n\u201csuccess\u201d, or \u201cfailure\u201d of adaptations. Seen from a different perspective, if an IAS assesses \nand modifies lower-level adaptation \u201cstrategies\u201d, then what needs to be evaluated is whether \nany such modifications are optimal from the perspective of the user. Although, from an \nengineering standpoint, the IAS component(s) involved in \u201cadapting the adapter\u201d operate at a \nmeta-level with respect to the rest of the IAS components, this distinction may not be relevant \nfrom the persp\nt \u201cmeta-adaptations\u201d as just another type of adaptation. This would mean that meta-level \nadaptations are amenable to the same treatment as first-level ones, and can thus be included in \nthe layered evaluation as this has been described so far. To the best of our knowledge, there \ndo not yet exist proposals in the literature for generically addressing this challenging topi\nA\nimpossible. As mentioned earlier, the frame\ndecisions. It may neither be feasible nor necessary to apply all layers and criteria. For \nexample, the Collection of Input Data layer has not been addressed in evaluation studies of \nmany systems. If there are no obvious shortcomings in this layer, the evaluation of other \nlayers may take priority. Nevertheless (and this is an important implication of the layered \napproach), due to the implicit dependencies of layers, evaluators need to be aware that a \nproblem identified in a higher layer might just be the symptom of problems introduced at \nlower layers.  \nThe above are some of the limitations of the framework\u2019s scope, but not necessarily the \nonly ones. We fully expect that there will exist evaluation settings and system features that \nmay render the framework inapplicable. We encourage evaluators to critically consider the \nframework in those cases, and, where applicable, modify and extend it to fit their needs. \n \nL6.2. \nThis paper has focused almost exclusively on formative methods. This is not to be interpreted \nas a preference or indeed as \nimitations of Formative Evaluation \n 38\nTh\ning what they are supposed to measure (e.g. how much \nersonalization helps a student to learn) rather than being hindered by lower-level \nng properly. Formative studies can also \ne verified later through well-controlled \ner has shown how \nformative evaluation methods can be adapted to cope with layered evaluation and the \nplicable to summative evaluation. \ne relative merits of formative versus summative evaluation have been hotly debated (e.g. \nCronbach et al., 1980; Scriven 1981, 1991; Chen, 1996). Some of the limitations mentioned \nfor formative evaluation are that: \n\u2212 Formative evaluations may be more time- and labour- intensive compared to most \nforms of summative evaluation due to relying more on qualitative methods.  \n\u2212 Formative evaluations do not seek to generalize, so may be more limited in their \nfindings. \n\u2212 Formative evaluations are not necessarily as carefully controlled; they are typically not \naimed at producing scientific proof. \n\u2212 Formative evaluations may be less suitable for comparisons, as they do not necessarily \nproduce an objective measure of \u201cgoodness\u201d. \n\u2212 Formative evaluations may be less independent, with more involvement of the design \nteam. \nThese limitations do not necessarily always hold: they depend on how the formative \nstudies are set up. Additionally, methods are not necessarily either formative or summative in \nnature; a single study may be used both to determine the system\u2019s value and how to further \nimprove it. In fact, Scriven (1991) argued that it is a fallacy that formative and summative \nstudies are intrinsically different. The mantra \u201cWhen the cook tastes the soup, that\u2019s \nformative; when the guests taste the soup, that\u2019s summative\u201d (R. Stake, as quoted in Scriven, \n1991, p. 19) shows that the same method (tasting the soup) can be both formative and \nsummative depending on when it is used and for what purpose. So, the methods presented in \nthis paper are not necessarily restricted to formative evaluations. Indeed, two of the methods \ndescribed are arguably the most popular ones for summative evaluation of IAS (namely user-\ntests and cross-validation). However, the paper provided a formative perspective, for example \nfor user tests, emphasising observational methods rather than the reporting of statistics.  \nSummative studies have an important role to play in demonstrating the success of the \nultimate goal of the adaptive system, and their value should, thus, not be underestimated. An \nimportant role of formative studies is to produce better summative studies. For example, by \nimproving system components through formative studies, it can be ensured that the \nsummative studies are measur\np\ncomponents (such as learner modelling) not worki\nproduce a qualitative understanding which can b\nsummative studies.  \nOn a final note on the subject, summative evaluation is not restricted to evaluating the \nsystem as a whole. It is possible to perform summative evaluations using the layered \nevaluation framework: evaluating the \u201cvalue\u201d of individual layers. This pap\nevaluation of an adaptive system. Much of this is equally ap\nFor example, for summative evaluation, it is just as important to ensure that the input received \nfrom the lower layers is accurate. Evaluators are urged to consider how the factors covered \nhere may influence the design of studies and the selection of data collection instruments for \nsummative assessment of IAS. \n \n6.3. Complementary and Alternative Approaches \nAt this point it is worth briefly recounting some of the complementary, as well as alternative \napproaches to the evaluation of adaptation that have been proposed in the literature. Broadly \nspeaking, some of them focus on the identification of criteria, while others address \ncomplementary aspects of adaptive systems to those of layered evaluation. \n 39 \n6.3.1. Identifying Appropriate Evaluation Criteria for IAS \nThe first framework that was designed to identify appropriate criteria was introduced by \nTobar (2003). The approach is based on a so-called map which integrates different design \nperspectives to facilitate the understanding of adaptation assessment and design. Tobar\u2019s \nproposed framework is targeted towards the identification of specific adaptation features that \nneed to be assessed, the establishment of criteria for the assessment, and the generation of \nevaluation plans on this basis.  \nA more recent approach called AnAmeter, proposed by Tarpin-Bernard, Marfisi-\none proposed by Tobar \ng the procedural means \ndescription of the system at hand. Although this framework is still at the \near\ndaptation as a whole. \nhe evaluation of open learner models and their scrutability is addressed in the SMILI \nframework proposed by Bull and Kay (2007).  As we have briefly seen, although scrutability \nge in the adaptation process, it has major implications in the evaluation of \ncially if users are able to modify the contents of their personal models (e.g., \nSchottman, and Habieb-Mammar (2009), is somewhat related to the \n(2003), but has important differentiations as well. Instead of prescribin\nfor identifying adaptation features for assessment, AnAmeter provides a relatively exhaustive \nenumeration of potential adaptation constituents and determinants in an IAS in a tabular form. \nEvaluators can characterize the adaptivity and use the resulting table to determine exactly \nwhat needs to be assessed. This facilitates the identification of potential conflicts and \ncorrelations (e.g., where the same determinant affects several constituents). This framework is \nalso unique in that it attempts to summarize and quantify the \u201cdegree\u201d of adaptation in a \nsystem, and in that it is supported by a web-based tool that enables evaluators to interactively \nmanage the tabular \nly stages of its development, it appears to bear promise in structuring the adaptation space \nin an easy to understand way. It would also be interesting to see future work examining the \nextent to which this approach can be used in conjunction with layered evaluation. \n6.3.2. Addressing Complementary Aspects of the Evaluation of IAS \nHerder (2003) proposed a utility-based approach to complement the layered evaluation \nprocess. The basic idea is that the added value of an adaptive system can be expressed by a \nutility function that maps selected, measurable criteria with respect to the performance of the \nadaptive system to a quantitative representation. If one would compare an adaptive system \nwith its non-adaptive counterpart, the value of adaptation is the difference in utility between \nthe two systems. Herder (2003) argues that the main advantage of the layered evaluation \napproach in this context is that it separates the utility function in several functions in a \nprincipled manner. \nMagoulas et al. (2003) argue about the need to develop an educational-evaluation model \nand a methodology that include usability testing as standard procedure capable to determine \nthe impact of adaptation on learners\u2019 behaviour in an educational environment. As described \nearlier, they introduce modifications to the standard heuristic evaluation approach and \naugment it with criteria that diagnose potential usability problems related to adaptation, \nsubsequently integrating it into the layered evaluation approach. In contrast to Jameson\u2019s \n(2003) generic usability challenges these heuristics are formulated for the specific case of \nadaptive educational systems. This not only narrows their applicability but also seems to \nintroduce some unnecessary assumptions about the system and the adaptation in particular. As \nshown in Section  4, Heuristic Evaluation can be used to assess several different layers, and \nmay in particular be useful to evaluate the a\nT\nis not itself a sta\nother stages, espe\ninaccuracies in the model may be attributable to the user\u2019s intervention, rather than to the \nsystem\u2019s derivation of incorrect assumptions). The SMILI framework allows evaluators to \ncharacterize the scrutability of a system along a set of seven different purposes of scrutability \nsuch as an increase of the user models accuracy, or the facilitation of reflection. Different \nelements of the system are then rated against these purposes in order to identify useful \n 40\npotential evaluations, i.e., those that provide evidence of the performance of the system on \none or more central purposes of the system. While the framework proposed here does address \nscrutability to some extent (as a criterion), the SMILI framework is by far more explicit and \ndetailed as far as scrutability is concerned. \n \n7. Discussion \nThe main postulation of layered evaluation of IAS is that adaptation needs to be decomposed \nand assessed in layers in order to be evaluated effectively. Since the first introduction of the \nterm in 2000, the scientific community has adopted this concept in planning and conducting \nempirical studies. Many authors explicitly refer back to the foundational papers published on \nthe topic to justify experimental designs, to provide rationale for goals or structure of their \nevaluation studies (Arruabarrena, P\u00e9rez, L\u00f3pez-Cuadrado, Guti\u00e9rrez, & Vadillo, 2002; \nOrtigosa & Carro, 2003; Petrelli & Not, 2005;  Cena et al., 2006; Goren-Bar, Graziola, \nPianesi, & Zancanaro, 2006; Glahn, Specht, & Koper, 2007; Kosba, Dimitrova, & R. Boyle, \n2007; Nguyen & Santos Jr, 2007;  Stock et al., 2007; Carmagnola et al., 2008; Limongelli, \nSci\nreaffirms the claim that the evaluation of adaptive systems \nimp\nexp\narrone, & Vaste, 2008; Ley, Kump, Maas, Maiden, & Albert, 2009; Popescu, 2009; Santos \n& Boticario, 2009), or to demonstrate methodological shortcomings of existing studies \n(Masthoff, 2002; Gena, 2005; Brusilovsky, Farzan, & Ahn, 2006; Yang & Huo, 2008; Brown, \nBrailsford, Fisher, & Moore, 2009). The fact that layered evaluation received such a high \nlevel of attention in the literature \nlicates some inherent difficulties. \nThe benefits of layered evaluation are perhaps representatively illustrated by a set of \nstudies of a mobile adaptive multimedia guide system for museums called PEACH (Stock & \nZancanaro, 2007). PEACH records the visitors\u2019 movements through the museum and collects \nlicit feedback about items seen. Based on this data, PEACH provides recommendations \nand a personalised report presented through a live-like agent. PEACH has been evaluated in a \nnumber of different empirical studies involving the running system respectively prototypes of \nthe system. The studies can be associated with different evaluation layers. \nIn regard to data collection, the user can express preferences through a so-called \u201clike-o-\nmeter\u201d. A field study with 140 users showed that visitors are willing to provide their \nfeedback. They understood how to use the feedback system and provided a sufficient number \nof ratings (Stock et al., 2007). The study provided evidence that the tool is effective in \ncollecting feedback from visitors. \nIn regard to the modelling of users, the movements of visitors in the museum were \nrecorded and categorized into different behaviour patterns (Zancanaro, Kuflik, Boger, Goren-\nBar, & Goldwasser, 2007). Clustering algorithms confirmed existing qualitative ethnographic \nfindings on visitor behaviour. \nIn regard to the adaptation decision, a study was designed to explore which \u201cadaptivity \ndimensions\u201d would be accepted by users, i.e., are presentations that rely on one characteristic \nin the user model preferred over decisions that rely on different characteristics? In a \nlaboratory study, users were presented with two simulated systems, one being adaptive and \nthe other non-adaptive. After expressing their preference for one of the versions, they were \nasked to give reasons for their preference in regard to the four dimensions the system can \nadapt to (location, interest, follow-up, history). The study yielded insights with respect to the \ndimensions of adaptation which may be accepted by different user groups (Goren-Bar et al., \n2006).  \nIn regard to the instantiation of adaptation it should be noted that the user interface of the \nmuseum guide evolved over several user-centred design cycles (Goren-Bar et al., 2005). One \nof the interface components on the mobile device is a life-like character that presents \n 41 \ninformation and engages the user. The effectiveness of this character in attracting the visitors\u2019 \nattention was tested in a study with an early prototype (Kruppa & Aslan, 2005). \nThe combination of these studies of PEACH and the improvements made based on their \nresults contributed to the successful deployment of a full adaptive system in a real-world \nenvironment. \nThe evaluation framework proposed here is centred around a decomposition model that \nidentifies five distinct stages in the adaptation process that should be evaluated as individual \n(or\nevaluating an \nIAS\nme task. The framework thus aims to serve both as an \nins\n in a variety of application domains. \nhe many methods available in the field of HCI, to \ninv\ne \nadaptive systems field. The best method to employ at any one time will primarily depend on \nwhen the evaluation takes place (with respect to the system\u2019s development lifecycle) and the \ncharacteristics of the layer under consideration. We have addressed this topic, as well as two \nother areas where the application of the framework necessitates that domain- and system- \n combined) layers. An important point we would like to make about the proposed \ndecomposition is that it is neither the only one feasible, nor, necessarily, the most appropriate \none for all types of assessment of IAS one might want to perform. For instance, it would be \npossible to decompose adaptation on the basis of the software components involved in a \nsystem\u2019s implementation. Furthermore, even if one takes a process-based approach to the \ndecomposition, it is not necessary that the same level of granularity be employed. Our \nproposal tries to strike a balance between, on the one hand, identifying all the individual \nclusters of steps involved in that process, and, on the other hand, having a manageable set of \ncoherent and assessable \u201ctargets\u201d. A related point that merits attention is that \n in a layered fashion (irrespectively of whether the proposed model is followed), does not \ndirectly address \u201ccross-cutting\u201d evaluation concerns, which have implications on all \nadaptation stages. Evaluators are still required to ensure that such concerns are individually \nintegrated into the evaluation activities of each stage.  \nThe proposed framework\u2019s target audience includes potentially most of the actors \ninvolved in the development of adaptive systems (e.g., usability experts, system designers, \nevaluators), as the framework may be employed from different perspectives. While a \npractitioner might use it to improve an existing system, a researcher might apply the \nframework to several systems in order to compare the quality of different modelling or \ninference approaches for the sa\ntrument to be used for the principled design of evaluation studies of IAS, but also as the \ncommon ground between disciplines for the derivation of concrete, validated design \nknowledge for different types of adaptation\nAnother goal of the framework is to facilitate the integration of evaluation activities in the \niterative design of IAS. Evaluation can (and, arguably, should) take place throughout a \nsystem\u2019s development, from early on to inspire the design of adaptive behaviour, up until and \nincluding the implementation and deployment of a system. In this context, the results of \nformative evaluation can be quite important in terms of system evolution: most often, \nevaluations are not just intended to investigate how good a layer (or system) is, but seek \ninsights over what causes problems and why. On the other hand, summative evaluation of \neither individual layers or a system as a whole are also of paramount importance, as they offer \na solid basis for generalization of findings, and foster theory development, which has been a \nperennial goal of the IAS field.  \nSkill is required in isolating and evaluating (combinations of) layers in a system\u2019s adaptive \nbehaviour. We have shown examples of how this can be done, several of them grounded on \nevaluation work reported in the literature.  \nNormally, multiple evaluation methods will be used during the development of an IAS. \nAdaptive systems can clearly benefit from t\nolve users in system design and evaluation. This paper has shown how these traditional \nmethods need to be tailored to suit the particular requirements of adaptivity in the user-system \ninteraction. It has also described some methods (e.g., User-as-Wizard) that are specific to th\n 42\nspecific characteristics be taken into account: (a) the potential combination of layers for the \nIn closing, the concepts behind layered evaluation have already had a significant impact \ntion of IAS. It is our hope that this paper will foster the wider adoption of the \ntribute to an increase in the number and quality of studies in the field. \nAck\nof a\nRef\nAck\nAlbr  I. (Eds.) (2007). Statistical and probabilistic methods for user \nArru\nve systems for education. In:  \n, Johnstown, PA. \nBerk of user models for enhanced \nBey\nBills\n (Banff, Canada), Vienna: Springer, pp. 98-\nBoy\npurposes of the evaluation, and (b) the derivation of metrics\/criteria that reflect the goals of \nemploying adaptation in a system in the first place, and can be used to assess its performance \nat different stages of the adaptation cycle. Our express aim in pursuing the above goals has \nbeen to remain non-prescriptive, yet provide a sufficiently holistic approach so that it can be \nreadily employed in the evaluation of IAS. \non the evalua\napproach and will con\nnowledgements \nWe would like to thank the anonymous reviewers and the UMUAI editor who have helped us \nto substantially improve the quality of the paper. We would also like to thank the participants \nof the several workshops and tutorials that we have organized on the subject of the evaluation \ndaptive systems for their constructive input to the evolution of the work presented here.  \nerences \nerman, M. S., Cranor, L. F., & Reagle, J. (1999). Privacy in e-commerce: examining user \nscenarios and privacy preferences. In: 1st ACM Conference on Electronic Commerce, \nDenver, CO: ACM, pp. 1-8. \necht, D., & Zuckerman,\nmodeling. Special Issue, User Modeling and User Adapted Interaction, 17(1-2), 1-215. \nAnderson, J. R., Boyle, C. F., & Yost, G. (1985). The geometry tutor. In: 9th International \nJoint Conference on Artificial Intelligence (Los Angeles, CA), San Francisco: Morgan \nKaufmann, pp. 1-7. \nabarrena, R., P\u00e9rez, T., L\u00f3pez-Cu adrado, J., Guti\u00e9rrez, J., & Vadillo, J. (2002). On \nevaluating adapti 2nd International Conference on Adaptive \nHypermedia and Adaptive Web-Based Systems (Malaga, Spain), LNCS 2347, Berlin: \nSpringer, pp. 363-367. \nvan Barneveld, J. and van Setten, M. (2003). Involving users in the design of user interfaces \nfor TV recommender systems. In: 3rd Workshop on Personalization in Future TV at \nUM03\nBatliner, A., Steidl, S., Hacker, C., & N\u00f6th, E. (2008). Private emotions versus social \ninteraction: a data-driven approach towards analysing emotion in speech, User Modeling \nand User Adapted Interaction, 18(1-2), 175\u2013206. \novsky, S., Kuflik, T., & Ricci, F. (2008). Mediation \npersonalization in recommender systems. User Modeling and User-Adapted Interaction, \n18(3), 245\u2013286. \ner, H. & Holtzblatt, K. (1997). Contextual design: Defining customer-centred systems. \nSan Francisco: Morgan Kaufmann. \nus, D., & Pazzani, M. (1999). A hybrid user model for news story classification. In: 7th \nInternational Conference on User Modeling\n108. \nBontcheva, K., & Wilks, Y. (2005). Tailoring automatically generated hypertext. User \nModeling and User-Adapted Interaction, 15(1-2), 135-168. \nle, C., & Encarnacion, A. O. (1994). MetaDoc: An adaptive hypertext reading system. \nUser Modeling and User-Adapted Interaction, 4(1), 1-19. \nBrown, E. J., Brailsford, T. J., Fisher, T., & Moore, A. (2009). Evaluating learning style \npersonalization in adaptive systems: Quantitative methods and approaches. IEEE \n 43 \nTransactions on Learning Technologies, 2(1), 10-22.  \nBrowne, D., Norman, M., & Riches, D. (1990). Why build adaptive systems. In D. Browne, P. \nBrusilovsky, P., & Eklund, J. (1998). A study of user model based link annotation in \nBrus\nd services. In: 1  Workshop on Empirical Evaluation of \nBrus  evaluation of adaptive search. In: \nBull r in: The SMILI\u263a open \nde C J., Huete, J., & Rueda-Morales, M. (2009). Managing \n(3), 207-242. \nCarm rtassa, O., Gena, C., Goy, A., Torre, I., Toso, A., & \npted Interaction, 18(5), 497-538. \nCena, F., Console, L., Gena, C., Goy, A., Levi, G., Modeo, S., & Torre, I. (2006). Integrating \nent control \nChin ical evaluation of user models and user-adapted systems. User \nClay\n. \nCoo e running the asylum. Indianapolis: Macmillan. \n9). \nTotterdell, & M. Norman (Eds.), Adaptive user interfaces. London: Academic Press, pp. \n15-57. \neducational hypermedia. Journal of Universal Computer Science, 4(4), 429\u2013448. \nilovsky, P., Karagiannidis, C., & Sampson, D. (2001). The benefits of layered evaluation \nof adaptive applications an st\nAdaptive Systems at UM2001, Sonthofen, Germany, pp. 1\u20138. \nilovsky, P., Farzan, R., & Ahn, J. (2006). Layered\nWorkshop on Evaluating Exploratory Search Systems at SIGIR06, Seattle, WA, pp. 11-\n13. \n, S., & Kay, J. (2007). Student models that invite the learne\nlearner modelling framework. International Journal of Artificial Intelligence in \nEducation, 17(2), 89-120. \nampos, L., Fern\u00e1ndez-Luna, \nuncertainty in group recommending processes. User Modeling and User-Adapted \nInteraction, 19\nCard, S.K., Thomas, T.P., & Newell, A. (1983). The Psychology of Human-Computer \nInteraction, London: Lawrence Erbaum Associates. \nagnola, F., Cena, F., Console, L., Co\nVernero, F. (2008). Tag-based user modeling for social multi-device adaptive guides. \nUser Modeling and User-Ada\nCarmichael, D. J., Kay, J., & Kummerfeld, B. (2005). Consistent modelling of users, devices \nand sensors in a ubiquitous computing environment. User Modeling and User-Adapted \nInteraction, 15(3-4), 197-234. \nCarroll, J. M. (2000). Five reasons for scenario-based design. Interacting with Computers, \n13(1), 43-60. \nheterogeneous adaptation techniques to build a flexible and usable mobile tourist guide. \nAI Communications, 19(4), 369-384. \nChen, H.T. (1996), A comprehensive typology for program evaluation. American Journal of \nEvaluation, 17(2), 121-130. \nCheverst, K., Byun, H. E., Fitton, D., Sas, C., Kray, C., & Villar, N. (2005). Exploring issues \nof user model transparency and proactive behaviour in an office environm\nsystem. User Modeling and User-Adapted Interaction, 15(3-4), 235-273. \nChickering, D.M, & Paek, T. (2007). Personalizing influence diagrams: applying online \nlearning strategies to dialogue management. User Modeling and User Adapted \nInteraction, 17(1-2), 71\u201391. \n, D. (2001). Empir\nModeling and User-Adapted Interaction, 11(1-2), 181-194. \npool, M., Le, P., Wased, M., & Brown, D. (2001). Implicit interest indicators. In: \nInternational Conference on Intelligent User Interfaces, Santa Fe, NM: ACM, pp. 33-40\nConati, C., & MacLaren, H. (2009). Empirically building and evaluating a probabilistic model \nof user affect. User Modeling and User Adapted Interaction, 19(3), 267\u2013303. \nper, A. (1999). The inmates ar\nCooper, D., Arroyo, I., Woolf, B., Muldner, K., Burleson, W., & Christopherson, R. (200\nSensors model student self concept in the classroom. In: 1st International Conference on \nUser Modeling, Adaptation, and Personalization (Trento, Italy), LNCS 5535, Berlin: \nSpringer, pp. 30-41. \nCramer, H., Evers, V., Ramlal, S., van Someren, M., Rutledge, L., Stash, N., Aroyo, L., & \n 44\nWielinga, B. (2008). The effects of transparency on trust in and acceptance of a content-\nbased art recommender. User Modeling and User-Adapted Interaction, 18(5), 455-496. \nDam\nodeling and User \nDe 99). AHAM: a Dexter-based reference model for \nDeg ommender that \nDey, A. K., & Abowd, G. D. (2000). Towards a better understanding of context and context-\n, 16(1), 3\u201328. \nct from conversational cues. User Modeling and \nDom\n-Adapted Interaction, 17(1-2), 41-69. \n  \nForb  relative impact of student affect on \nGena, C. (2005). Methods and techniques for the evaluation of user-adaptive systems. The \nGen\ntional Conference on Adaptive Hypermedia and Adaptive \nGen\nl (Eds.), The adaptive web: Methods and strategies of \nGeo\n User Adapted Interaction, \nGlah\nCronbach, L.J., Ambron, S.R., Dornbusch, S.M., Hess, R.D., Hornik, R.C., Philips, D.C., \nWalker, D.F., & Weiner, S.S. (1980). Toward reform of program evaluation. San \nFrancisco, CA: Jossey-Bass.  \nCzarkowski, M. (2006) A scrutable adaptive hypertext. PhD Thesis, University of Sydney. \niano, R., Gena, C., Lombardo, V., Nunnari, F., & Pizzo, A. (2008). A stroll with Carletto: \nadaptation in drama-based tours with virtual characters. User M\nAdapted Interaction, 18(5), 417\u2013453. \nBra, P., Houben, G., & Wu, H. (19\nadaptive hypermedia. In: 10th ACM Conference on Hypertext and Hypermedia, \nDarmstadt, Germany: ACM, pp. 147-156. \nemmis, M., Lops, P., & Semeraro, G. (2007). A content-collaborative rec\nexploits WordNet-based user profiles for neighborhood formation. User Modeling and \nUser-Adapted Interaction, 17(3), 217-255. \nawareness. In: Workshop on the What, Who, Where, When, and How of Context-\nAwareness at CHI 2000, The Hague, The Netherlands,  pp. 304\u2013307. \nDix, A., Finlay, J., Abowd, G., & Beale, R. (1998) Human Computer Interaction, 2nd Ed. \nEnglewood Cliffs, NJ: Prentice-Hall. \nD\u2019Mello, S.K., Craig, S.D., Sullins, J., & Graesser, A.C. (2006). Predicting affective states \nthrough an emote-aloud procedure from AutoTutor\u2019s mixed-initiative dialogue. \nInternational Journal of Artificial Intelligence in Education\nD\u2019Mello, S.K., Craig, S.D., Witherspoon,A., McDaniel, B., & Graesser, A.C. (2008). \nAutomatic detection of learner\u2019s affe\nUser-Adapted Interaction, 18(1-2), 45-80. \nshlak, C., & Joachims, T. (2007). Efficient and non-parametric reasoning over user \npreferences. User Modeling and User\nDumas, J.S. and Loring, B.A. (2008). Moderating usability tests: principles and practices for \ninteracting. San Francisco: Morgan Kaufmann.\nEncarna\u00e7\u00e3o, L., & Stoev, S. (1999). Application-independent intelligent user support system \nexploiting action-sequence based user modeling. 7th International Conference on User \nModeling (Banff, Canada), Vienna: Springer, pp. 245-254. \nEricsson, K.A., & Simon, H.A. (1993). Protocol analysis: Verbal reports as data. Revised \nedition. Cambridge, MA: MIT Press. \nes-Riley, K., Rotaru, M., & Litman, D.J. (2008). The\nperformance models in a spoken dialogue tutoring system. User Modeling and User-\nAdapted Interaction, 18(1-2), 11-43. \nKnowledge Engineering Review, 20(1), 1-37.  \na, C., & Ardissono, L., (2004). Intelligent support to the retrieval of information about \nhydric resources. 3rd Interna\nWeb-Based Systems, LNCS 3137, Berlin: Springer, pp. 126-135. \na, C., & Weibelzahl, S. (2007). Usability engineering for the adaptive web. In P. \nBrusilovsky, A. Kobsa, & W. Nejd\nweb personalization, Berlin: Springer, pp. 720-762.  \nrge, S., Zukerman, I., & Niemann, M. (2007). Inferences, suppositions and explanatory \nextensions in argument interpretation. User Modeling and\n17(5), 439-474. \nn, C., Specht, M., & Koper, R. (2007). Smart indicators on learning interactions. In: 2nd  \nEuropean Conference on Technology Enhanced Learning (Crete, Greece), LNCS 4753, \n 45 \nBerlin: Springer, pp. 56-70.  \nGoecks, J., & Shavlik, J. (2000). Learning users' interests by unobtrusively observing their \nnormal behavior. In: 5th International Conference on Intelligent User Interfaces, New \nOrleans, LA: ACM, pp. 129-132.  \nGoren-Bar, D., Graziola, I., Pianesi, F., & Zancanaro, M. (2006). The influence of personality \nfactors on visitor attitudes towards adaptivity dimensions for mobile museum guides. \nGor\nguide. In: 1  \nGould, J., Conti, J., & Hovanyecz, T. (1982). Composing letters with a simulated listening \nGreen, D. P., Ha, S. E., & Bullock, J. G. (2010). Enough already about \u201cblack box\u201d \nGru opment: An \n, 339-351. \nparison. Interacting with Computers, 16(6), 1153-1170.  \nptive Systems at UM2003, Johnstown, PA, USA. pp. 25-30.  \n Systems, 22(1), 5\u201353. \npport. User Modeling and User Adapted Interaction, \nH\u00f6\u00f6\nUser Modeling and User-Adapted Interaction, 16(1), 31-62. \nen-Bar, D., Graziola, I., Rocchi, C., Pianesi, F., Stock, O., & Zancanaro, M. (2005). \nDesigning and redesigning an affective interface for an adaptive museum st\nInternational Conference on Affective Computing and Intelligent Interaction (Beijing, \nChina), LNCS 3784, Berlin: Springer, pp. 939-946. \ntypewriter. In: 1st ACM Conference on Human Factors in Computer Systems (CHI),  \nGaithersburg, MD: ACM, pp. 367-370. \nGould, J. D., & Lewis, C. (1985). Designing for usability: key principles and what designers \nthink. Communications of the ACM, 28(3), 300-311. \nexperiments: Studying mediation is more difficult than most scholars suppose. ANNALS \nof the American Academy of Political and Social Science, 628(1), 200-208. \ndin, J., & Pruitt, J. (2002). Personas, participatory design, and product devel\ninfrastructure for engagement. In: Participatory Design Conference, Malm\u00f6, Sweden: \nACM, pp. 144-161. \nGuzm\u00e1n, E., Conejo, R., & P\u00e9rez-de-la-Cruz, J.-L. (2007). Adaptive testing for hierarchical \nstudent models. User Modeling and User-Adapted Interaction, 17(1-2), 119\u2013157. \nvan den Haak, M.J., de Jong, M.D.T., & Schellens, P.J. (2003). Retrospective vs. concurrent \nthink-aloud protocols: Testing the usability of an online library catalogue. Behaviour & \nInformation Technology, 22(5)\nvan den Haak, M.J.,  de Jong, M.D.T., & Schellens, P.J. (2004). Employing think-aloud \nprotocols and constructive interaction to test the usability of online library catalogues: a \nmethodological com\nHerder, E. (2003). Utility-based evaluation of adaptive systems. In: 2nd Workshop on \nEmpirical Evaluation of Ada\nHerlocker, J.L., Konstan, J.A., Terveen, L.G., & Riedl, J.T. (2004). Evaluating collaborative \nfiltering recommender systems. ACM Transactions on Information\nHertzum, M., Hansen, K., Andersen, H.H.K. (2009). Scrutinizing usability evaluation: Does \nthinking aloud affect behaviour and mental workload? Behaviour & Information \nTechnology, 28(2), 165-181.  \nHollink, V., van Someren, M., & Wielinga, B. (2007). Discovering stages in web navigation \nfor problem-oriented navigation su\n17(1-2), 183\u2013214. \nk, K. (2000). Steps to take before intelligent user interfaces become real. Interacting with \nComputers, 12(4), 409-426. \nHoppe, H., Tauber, M., & Ziegler, J. (1986). A survey of models and formal description \nmethods in HCI with example applications. ESPRIT Project, 385. \nHorvitz, E., & Paek, T. (2007). Complementary computing: Policies for transferring callers \nfrom dialog systems to human receptionists. User Modeling and User-Adapted \nInteraction, 17(1-2), 159-182. \nJameson, A. (2001). Systems that adapt to their users: An integrative perspective. \nSaarbr\u00fccken: Saarland University. \nJameson, A. (2003). Adaptive interfaces and agents. In: J.A. Jacko & A. Sears (Eds.), The \nhuman-computer interaction handbook: Fundamentals, evolving technologies and \n 46\nemerging applications, Hillsdale, NJ: L. Erlbaum Associates, pp. 305-330.  \nJam , A., & Schwarzkopf, E. (2002). Pros and cons of controllability: An empirical study. \ns (M\u00e1laga, Spain), LNCS 2347, Berlin: Springer, pp. 193-202. \nKaragiannidis, C., & Sampson, D. (2000). Layered evaluation of adaptive applications and \nKay t models and scrutability. 5  International Conference on \nKay 1(1-2), 111-\nKnu\n Review of Hypermedia and \nKob\n adaptive web: Methods and strategies of web personalization, Berlin: \nKoc\na, Spain), LNCS 2347, Berlin: Springer, pp. 213-222.  \nnce on Artificial Intelligence (Montr\u00e9al, \nKos\n413. \np. 97-125. \nKruppa, M., & Aslan, I. (2005). Parallel presentations for heterogeneous user groups \u2013 an \nlin: Springer, \nLaw\nal Monitoring and Computing, 19(3), \nLek\nJameson, A. (2005). User modeling meets usability goals. In: 10th International Conference \non User Modeling (Edinburgh, UK),  LNAI 3538, Berlin: Springer, pp. 1-3.  \nJameson, A. (2008). Adaptive user interfaces and agents. In A. Sears & J. Jacko (Eds.), The \nhuman-computer interaction handbook: Fundamentals, evolving technologies and \nemerging applications, 2nd Ed., Boca Raton, FL: CRC Press, pp. 433-458. \nJameson, A. (2009). Understanding and dealing with usability side effects of intelligent \nprocessing. AI Magazine, 30(4), 23-40. \neson\nIn: 2nd International Conference on Adaptive Hypermedia and Adaptive Web-Based \nSystem\nKaplan, C., Fenwick, J., & Chen, J. (1993). Adaptive hypertext navigation based on user \ngoals and context. User Modeling and User-Adapted Interaction, 3(3), 193-220. \nservices. In: 1st International Conference on Adaptive Hypermedia and Adaptive Web-\nBased Systems (Trento, Italy), LNCS 1892, Berlin: Springer, pp. 343-346.  \n, J. (2000). Stereotypes, studen th\nIntelligent Tutoring Systems (Montr\u00e9al, Canada), LNCS 1839, Berlin: Springer, pp. 19-\n30. \n, J. (2001). Learner control. User Modeling and User-Adapted Interaction, 1\n127. \ntov, E., De Bra, P., & Pechenizkiy, M. (2009). AH - 12 years later: a comprehensive \nsurvey of adaptive hypermedia methods and techniques. New\nMultimedia, 15(1), 5-38.  \nsa, A. (2007). Privacy-enhanced web personalization. In: P. Brusilovsky, A. Kobsa, & W. \nNejdl (Eds.), The\nSpringer-Verlag, pp. 628-670. \nh, N., & Wirsing, M. (2002). The Munich reference model for adaptive hypermedia \napplications. 2nd International Conference on Adaptive Hypermedia and Adaptive Web-\nBased Systems (M\u00e1lag\nKohavi, R. (1995). A study of cross-validation and bootstrap for accuracy estimation and \nmodel selection. 14th International Joint Confere\nCanada), San Francisco: Morgan Kaufmann, pp. 1137-1145. \nba, E., Dimitrova, V., & Boyle, R. (2007). Adaptive feedback generation to support \nteachers in web-based distance education. User Modeling and User-Adapted Interaction, \n17(4), 379-\nKrogs\u00e6ter, M., Oppermann, R., & Thomas, C. G. (1994). A user interface integrating \nadaptability and adaptivity. In: R. Oppermann (Ed.), Adaptive user support: ergonomic \ndesign of manually and automatically adaptable software, Hillsdale, NJ: Lawrence \nErlbaum, p\nKrueger, R., & Casey, M. (2009). Focus groups: A practical guide for applied research. 4th \nEd., Los Angeles: Sage Publications.  \ninitial user study. In: 4th International Conference on Intelligent Technologies for \nInteractive Entertainment (Madonna di Campiglio, Italy), LNAI 3814, Ber\npp. 54-63. \n A., Freer, Y., Hunter, J., Logie, R., McIntosh, N., & Quinn, J. (2005). A comparison of \ngraphical and textual presentations of time series data to support medical decision making \nin the neonatal intensive care unit, Journal of Clinic\n183-194. \nakos, G., & Giaglis, G. (2007). A hybrid approach for improving predictive accuracy of \n 47 \ncollaborative filtering algorithms. User Modeling and User-Adapted Interaction, 17(1-2), \n5-40. \nLewis C. (1982). Using the \u2018thinking-aloud\u2019 method in cognitive interface design. Research \nreport RC9265; IBM T.J. Watson Research Center, Yorktown Heights, NY \n, T., Kump, B., Maas, A., Maiden, N., & Albert, D. (2009). Evaluating the adaptatLey ion of a \ning styles in web-based education. In: 5  \nonal Conference \nMag\n 2  Workshop on Empirical \nMasthoff, J. (2002). The evaluation of adaptive systems. In: N. Patel (Ed.), Adaptive \nMas\nser Adapted Interaction, 14(1), 37-85. \n60-469. \nMas of \nnments, Newcastle, UK. \nACM, pp. 277-284. \nss, Montr\u00e9al, Canada. \nMill  modeling \nMon  (2008). What do you want to know? Investigating the \nlearning system before the prototype is ready: A paper-based lab study. In: 1st \nInternational Conference on User Modeling, Adaptation, and Personalization (Trento, \nItaly), LNCS 5535, Berlin: Springer, pp. 331-336.  \nLimongelli, C., Sciarrone, F., & Vaste, G. (2008). LS-Plan : An effective combination of \ndynamic courseware generation and learn th\nInternational Conference on Adaptive Hypermedia and Adaptive Web-Based Systems \n(Hannover, Germany), LNCS 5149, Berlin: Springer, pp. 133-142.  \nMacLaren, B., & Koedinger, K. (2002). When and why does mastery learning work: \nInstructional experiments with ACT-R \"SimStudents\". In: 6th Internati\non Intelligent Tutoring Systems, (Biarritz, France), Berlin: Springer, pp. 355-366. \noulas, G. D., Chen, S. Y., & Papanikolaou, K. A. (2003). Integrating layered and \nheuristic evaluation for adaptive learning environments. In: nd\nEvaluation of Adaptive Systems at UM2003,  Johnstown, PA, pp. 5-14. \nMaguire, M. (2001). Methods to support human-centred design. International Journal \nHuman-Computer Studies, 55(4), 587- 634. \nEvolutionary Information Systems, London: Idea Group Publishing, pp. 329-347. \nthoff, J. (2004).  Group modeling: Selecting a sequence of television items to suit a group \nof viewers. User Modeling and U\nMasthoff, J. (2006). The user as wizard: A method for early involvement in the design and \nevaluation of adaptive systems. In: 5th Workshop on User-Centred Design and Evaluation \nof Adaptive Systems at AH06, Dublin, Ireland, pp. 4\nMasthoff, J. (unpublished). Automatically constructing good hierarchies: HCI meets AI. \nthoff, J., & Gatt, A. (2006). In pursuit of satisfaction and the prevention \nembarrassment: Affective state in group recommender systems. User Modeling and \nUser-Adapted Interaction, 16(3-4), 281-319. \nMasthoff, J., Vasconcelos, W.W., Aitken, C., & Correa da Silva, F.S. (2007). Agent-based \ngroup modelling for ambient intelligence, AISB Symposium on Affective Smart \nEnviro\nMaulsby, D., Greenberg, S., & Mander, R. (1993) Prototyping an intelligent agent through \nwizard of Oz. In: 10th ACM Conference on Human Factors in Computing Systems, \nAmsterdam, The Netherlands: \nMcNee, S.M., Riedl, J., & Konstan, J.A. (2006). Being accurate is not enough: How accuracy \nmetrics have hurt recommender systems. In: CHI work in progre\nMiettinen, M., & Oulasvirta, A. (2007). Predicting time-sharing in mobile interaction, User \nModeling and User Adapted Interaction, 17(5), 475\u2013510. \n\u00e1n, E., & P\u00e9rez de la Cruz, J.L. (2002). Diagnosis algorithm for student\ndiagnosis and its evaluation. User Modeling and User Adapted Interaction, 12(2\u20133), 281\u2013\n330.  \nMobasher, B. (2007). Data mining for web personalization. In: P. Brusilovsky, A. Kobsa, & \nW. Nejdl (Eds.), The adaptive web: Methods and strategies of web personalization, \nBerlin: Springer, pp. 90-135.  \nMobasher, B., & Tuzhilin, A. (Eds.) (2009). Special issue on data mining for personalization. \nUser Modeling and User Adapted Interaction, 19 (1-2), 1-166.  \ncur, W., Masthoff, J., & Reiter, E.\ninformation requirements of patient supporters. In: 21th IEEE International Symposium \n 48\non Computer-Based Medical Systems, Jyv\u00e4skyl\u00e4, Finland: IEEE, pp. 443-448. \nMurray, T. (1993). Formative qualitative evaluation for \u2018exploratory\u2019 ITS research. \nNgu r's position to persuasive \n), \nNguyen, H., & Santos Jr, E. (2007). An evaluation of the accuracy of capturing user intent for \nNiel sts. In: \nNiel\n Wiley & Sons, pp. 25-64. \nNor\nraction, 16(2), 87-127.  \nOhene-Djan, J. (2002). Ownership transfer via personalisation as a value-adding strategy for \nOliv n of HMMs and dynamic bayesian networks for \nO'M n: A method for \nOpp aptability. International Journal of Human-\nOpp\ntically adaptable software. Hillsdale, NJ: \nOrti\nIn: 9  International Conference on User \nPaek\nling and \nPara\n Workshop on Empirical Evaluation of Adaptive Systems \nPara valuation \n Springer, pp. 821-830. \nInternational Journal on Artificial Intelligence in Education, 4 (2-3), 179\u2013207. \nyen, H., Masthoff, J. & Edwards, P. (2007). Modelling a receive\narguments. In: 2nd International Conference on Persuasive Technology (Palo Alto, CA\nLNCS 4744, Berlin: Springer, pp. 271-282. \ninformation retrieval. In: International Conference on Artificial Intelligence, Las Vegas, \nNV: CSREA Press, pp. 341-350.  \nsen, J. (1993). Evaluating the thinking-aloud technique for use by computer scienti\nH.R. Hartson & D. Hix (Eds). Advances in human-computer interaction, Vol.3, pp. 69-\n82. \nsen, J. (1994a). Heuristic evaluation. In: J. Nielsen & R.L. Mack (Eds.), Usability \ninspection methods, New York: John\nNielsen, J. (1994b). Usability engineering, 2nd Ed., San Francisco, CA: Morgan Kaufmann. \nman, D. A. (1994). How might people interact with agents. Communications of the ACM, \n37(7), 68-71. \nN\u00fcckles, M., Winter, A., Wittwer, J., Herbert, M., & H\u00fcbner, S. (2006). How do experts adapt \ntheir explanations to a layperson\u2019s knowledge in asynchronous communication? An \nexperimental study. User Modeling and User-Adapted Inte\nOgata, K. (2009). Modern control engineering, 5th Ed., Upper Saddle River, NJ: Prentice Hall. \nweb-based education. In: Workshop on Adaptive Systems for Web-Based Education at \nAH2002, M\u00e1laga, Spain, pp. 27-41. \ner, N., & Horvitz, E. (2005). A compariso\nrecognizing office activities. In: 10th International Conference on User Modeling \n(Edinburgh, UK), LNCS 3538, Berlin: Springer, pp. 199-209.  \nalley, C.E., Draper, S.W., & Riley, M.S. (1984). Constructive interactio\nstudying human-computer-human interaction. In: 1st International Conference on \nHuman-Computer Interaction, Honolulu, HI, pp. 269-274 \nermann, R. (1994). Adaptively supported ad\nComputer Studies, 40(3), 455-472. \nermann, R. (1995). Introduction. In: R. Oppermann (Ed.), Adaptive user support: \nErgonomic design of manually and automa\nLawrence Erlbaum Associates, pp. 1-13. \ngosa, A., & Carro, R. M. (2003). The continuous empirical evaluation approach: \nEvaluating adaptive web-based courses. th\nModeling (Johnstown, PA), LNCS 2702, Berlin: Springer, pp. 163-167.  \n, T., & Chickering, D.M. (2007). Improving command and control speech recognition on \nmobile devices: using predictive user models for language modelling. User Mode\nUser-Adapted Interaction, 17(1-2), 93-117. \nmythis, A., Totter, A., & Stephanidis, C. (2001). A modular approach to the evaluation of \nadaptive user interfaces. In: 1st\nat  UM2001, Sonthofen, Germany, pp. 9\u201324. \nmythis, A., & Weibelzahl, S. (2005). A decomposition model for the layered e\nof interactive adaptive systems. In: 10th International Conference on User Modeling \n(Edinburgh, UK), LNCS 3538, pp 438-442. \nPerson N.K., & Graesser A.C. (2002). Tutoring research group human or computer? \nAutoTutor in a bystander Turing test. In: 6th International Conference on Intelligent \nTutoring Systems (Biarritz, France), LNCS 2363, Berlin:\nPetrelli, D., & Not, E. (2005). User-centred design of flexible hypermedia for a mobile guide: \n 49 \nReflections on the HyperAudio experience. User Modeling and User-Adapted \nInteraction, 15(3-4), 303-338. \n Interaction, Amsterdam: Elsevier, pp. 27-30.  \nPop\n Web-Based Learning (Aachen, \nPora\nUser Adapted Interaction, 18(1-2), \nPree\nRob iment, design, and statistics in psychology. 3  Ed. London: Penguin. \nSantos, O., & Boticario, J. (2009). Guiding learners in learning management systems through \npp. 596-601. \nation Retrieval(Tampere, Finland). New York: \nSchm ukerman, I., & Albrecht, D. (2009). Assessing the impact of measurement \neducation: At quarter century. Chicago, IL: \nScri\nSern zheimer\u2019s disease for \nShn ning the user interface: Strategies for effective human-\nSixs system. Journal of \nSpada, D., S\u00e1nchez-Monta\u00f1\u00e9s, M., Paredes, P., & Carro, R. (2008). Towards inferring \nPohl, W. (1997). LaboUr - Machine learning for user modeling. In: 7th International \nConference on Human-Computer\nPohl, W. (1999). Logic-based representation and reasoning for user modeling shell systems. \nUser Modeling and User-Adapted Interaction, 9(3), 217-282. \nescu, E. (2009). Evaluating the impact of adaptation to learning styles in a web-based \neducational system. In: 8th International Conference on\nGermany), LNCS 5686, Berlin: Springer, pp. 343-352.  \nyska-Pomsta, K., Mavrikis, M., & Pain, H. (2008).  Diagnosing and acting on student \naffect: the tutor\u2019s perspective. User Modeling and \n125-173. \nce, J., Rogers, Y., Sharp, H., & Benyon, D. (1994). Human\u2013Computer Interaction. \nReading, MA: Addison-Wesley. \nson, C. (1994). Exper rd\nde Rosis, F., Mazzotta, I., Miceli, M., & Poggi, I. (2006). Persuasion artifices to promote \nwellbeing. In: 1st International Conference on Persuasive Technology (Eindhoven, The \nNetherlands),  LNCS 3962, Berlin: Springer, pp. 84-95.  \nrecommendations. In: 4th European Conference on Technology Enhanced Learning (Nice, \nFrance), LNCS 5794, Berlin: Springer, \nSchein, A.I., Popescul, A., Ungar, L.H., & Pennock, D. M. (2002). Methods and metrics for \ncold-start collaborative filtering. In: 25th Annual international ACM SIGIR Conference \non Research and Development in Inform\nACM, pp. 253-260.  \nidt, D., Z\nuncertainty on user models in spatial domains. 1st International Conference on User \nModeling, Adaptation, and Personalization (Trento, Italy), LNCS 5535, Berlin: Springer, \npp. 210-222.  \nScriven, M. (1981). Produce evaluation. In N.L. Smith (Ed.), New techniques for evaluation. \nBeverly Hills, CA: Sage, pp. 121-126. \nScriven, M. (1991). Beyond formative and summative evaluation. In G.W. McLaughlin and \nD.C. Phillips (Eds.), Evaluation and \nUniversity of Chicago Press, pp. 19-64. \nven, M. (1996). Types of evaluation and types of evaluator. Evaluation Practice, 17(2), \n151-162. \na, A. Pigot, H., & Rialle, V. (2007). Modeling the progression of Al\ncognitive assistance in smart homes. User Modeling and User-Adapted Interaction, \n17(4), 415\u2013438. \neiderman, B. (1998). Desig\ncomputer interaction. Reading, MA: Addison Wesley. \nmith, A. J. (2000). An evaluation of an intelligent home monitoring \nTelemedicine and Telecare, 6(2), 63-72. \nsequential-global dimension of learning styles from mouse movement patterns. In: 5th \nInternational Conference on Adaptive Hypermedia and Adaptive Web-Based Systems \n(Hannover, Germany), LNCS 5149, Berlin: Springer, pp. 337-340.  \nStary, C., & Totter, A. (1997). How to integrate concepts of the design and the evaluation of \nadaptable and adaptive user interfaces. 3rd ERCIM Workshop on User Interfaces for All, \nObernai, France, pp. 68-75. \nStamou, S., & Ntoulas, A. (2009). Search personalization through query and page topical \n 50\nanalysis. User Modeling and User-Adapted Interaction, 19(1), 5-33.   \nk, O., & Zancanaro, M. (2007). PEACH - Intelligent interfaces for muStoc seum visits. Berlin: \nStoc\nser-Adapted Interaction, 17(3), 257-304.  \n11-248. \nence on Adaptive \nTintarev, N., & Masthoff, J. (2007). Effective explanations of recommendations: User-\nTintarev, N., & Masthoff, J. (2009). Evaluating recommender explanations: Problems \nTob ation framework. 2  Workshop on Empirical \nTott aptive systems. In D. Browne, P. \nmic Press, pp. \nTott\n. \n \nTuri Mind, 59, 433-460. \nLNCS 1452, Berlin: Springer, pp. 434\u2013\nvan \ns: a literature review. The Knowledge \nWal  A research framework for \n, 19(5), 387-431. \nersonalized \nWeb\n International Conference on User Modeling (Chia \nSpringer. \nk, O., Zancanaro, M., Busetta, P., Callaway, C., Kr\u00fcger, A., Kruppa, M., Kuflik, T., et al. \n(2007). Adaptive, intelligent presentation of information for the museum visitor in \nPEACH. User Modeling and U\nSuebnukarn, S., & Haddawy, P. (2006). Modeling individual and collaborative problem-\nsolving in medical problem-based learning. User Modeling and User-Adapted \nInteraction, 16(3-4), 2\nTarpin-Bernard, F., Marfisi-Schottman, I., & Habieb-Mammar, H. (2009). AnAmeter: The \nfirst steps to evaluating adaptation. 6th Workshop on User-Centred Design and Evaluation \nof Adaptive Systems at UMAP2009, Trento, Italy: CEUR, pp. 11-20.  \nTintarev, N., & Masthoff, J. (2008). The effectiveness of personalized movie explanations: \nAn experiment using commercial meta-data. In: 5th International Confer\nHypermedia and Adaptive Web-Based Systems (Hannover, Germany), LNCS 5149, \nBerlin: Springer, pp. 204-213. \ncentered design. In: ACM conference on Recommender systems, Minneapolis, MN: \nACM, pp. 153-156.  \nexperienced and lessons learned for the evaluation of adaptive systems. 6th Workshop on \nUser Centered Design and Evaluation at UMAP09, Trento, Italy: CEUR, pp. 54-63. \nar, C. M. (2003). Yet another evalu nd\nEvaluation of Adaptive Systems at UM2003, Johnstown, PA, pp. 15-24. \nerdell, P., & Boyle, E. (1990). The evaluation of ad\nTotterdell, & M. Norman (Eds.), Adaptive user interfaces, London: Acade\n161-194. \nerdell, P., & Rautenbach, P. (1990). Adaptation as a problem of design. In D. Browne, P. \nTotterdell, & M. Norman (Eds.), Adaptive user interfaces, London: Academic Press, pp. \n61-84\nTotterdell, P., Rautenbach, P., Wilkinson, A., & Anderson, S. (1990). Adaptive interface \ntechniques. In D. Browne, P. Totterdell, & M. Norman (Eds.), Adaptive user interfaces, \nLondon: Academic Press, pp. 131-160.\nTrewin, S. (2000). Configuration agents, control and privacy. In: ACM Conference on \nUniversal Usability, Arlington, VA: ACM, pp. 9-16.  \nng, A. (1950). Computing machinery and intelligence. \nVanLehn, K., Niu, Z., Siler, S. & Gertner, A.S. (1998). Student modeling from conventional \ntest data: a Bayesian approach without priors. In: 5th International Conference on \nIntelligent Tutoring Systems (Montr\u00e9al, Canada), \n443  \nVelsen, L., van der Geest, T, Klaassen, R, Steehouder, M. (2008). User-centered \nevaluation of adaptive and adaptable system\nEngineering Review, 23(3), 261-281.  \nker, E., Rummel, N. & Koedinger, K.R, (2009). CTRL:\nproviding adaptive collaborative learning support. User Modeling and User Adapted \nInteraction\nWang, Y., Chen, Z., & Kobsa, A. (2006). A collection and systematization of international \nprivacy laws, with special consideration of internationally operating p\nwebsites, http:\/\/www.ics.uci.edu\/~kobsa\/privacy \ner, G., & Specht, M. (1997). User modeling and adaptive navigation support in \nWWW-based tutoring systems. In: 6th\n 51 \nLaguna, Italy), Vienna: Springer, pp. 289-300. \nbelzahl, S. (2001). Evaluation of adaptive systems. In: 8th International Conference on \nUser Modeling, LNCS 2109, Berlin: Springer,\nWei\n pp. 292-294. \nWeibelzahl, S. (2005). Problems and pitfalls in evaluating adaptive systems.  In: 4  Workshop \nWei\nns, pp. 105-141. \nier, pp. \nWin\nrlin: Springer, pp. 106-122. \nufmann. \nZan  & Goldwasser, D. (2007). Analyzing \niegler, C.-N., McNee, S.M., Konstan, J. A., & Lausen, G. (2005). Improving \nrecommendation lists through topic diversification. In: 14th International World Wide \nrenz, A. (2008). LISTEN: A user-adaptive audio-augmented museum \nWeibelzahl, S. (2003). Evaluation of adaptive systems. PhD Thesis, University of Trier, \nGermany. \nth\non the Evaluation of Adaptive Systems at UM'05, Edinburgh, UK, pp. 57-66. \nbelzahl, S., & Weber, G. (2003). Evaluating the inference mechanism of adaptive learning \nsystems. In: 9th International Conference of User Modeling (Johnstown, PA), LNCS \n2702, Berlin: Springer, pp. 154-168.  \nWharton, C., Rieman, J., Lewis, C., & Polson, P. (1994). The cognitive walkthrough method: \nA practitioner's guide. In: J.Nielsen.and R.L.Mack (Eds.) Usability inspection methods. \nNew York: John Wiley & So\nWilson, J., & Rosenberg, D. (1988). Rapid prototyping for user interface design. In: M. \nHelander (Ed.), Handbook of human-computer interaction, Amsterdam: Elsev\n859-875. \nter, S., Wagner, S., & Deissenboeck, F. (2008). A comprehensive model of usability. In: \nEngineering Interactive Systems Conference, LNCS 4940, Be\nWitten, I.A. & Frank, E. (2005). Data mining: Practical machine learning tools and \ntechniques. 2nd Ed. Amsterdam: Morgan Ka\nYang, D., & Huo, H. (2008). Assessment on the adaptivity of adaptive systems. In: \nInternational Conference on Management of e-Commerce and e-Government, Nanchang, \nChina: IEEE, pp. 437-440.  \nYudelson, M., Medvedeva, O., & Crowley, R. (2008). A multifactor approach to student \nmodel evaluation. User Modeling and User-Adapted Interaction, 18(4), 349-382. \ncanaro, M., Kuflik, T., Boger, Z., Goren-Bar, D.,\nmuseum visitors\u2019 behavior patterns. In: 11th International Conference on User Modeling \n(Corfu, Greece), LNCS 4511, Berlin: Springer, pp. 238-246. \nZaslow, J. (2002). If TiVo thinks you are gay, here's how to set it straight. The Wall Street \nJournal, sect. A, p. 1, November 26, 2002.  \nZiegler, J., & Bullinger, H. J. (1991). Formal models and techniques in human-computer \ninteraction. In: B. Shackel & S.J. Richardson (Eds.), Human factors for informatics \nusability, Cambridge, UK: Cambridge University Press, pp. 183-206. \nZ\nWeb Conference, Chiba, Japan: ACM, pp. 22\u201332. \nZimmermann, A., & Lo\nguide. User Modeling and User-Adapted Interaction, 18(5), 389-416. \nZimmermann, A., Specht, M., & Lorenz, A. (2005). Personalization and context management. \nUser Modeling and User-Adapted Interaction, 15(3-4), 275-302. \nZhang, T., Rau, P., & Salvendy, G. (2007). Developing instrument for handset usability \nevaluation: A survey study. In: 12th International Conference on Human-Computer \nInteraction (Beijing, China), LNCS 4550, Berlin: Springer, pp. 662-671. \n \nuthor Biographies A\nDr. Alexandros Paramythis received his Ph.D. in the area of Adaptive Systems from the \nJohannes Kepler University (Linz, Austria) where he is currently employed as a researcher. \nHe has long-standing experience in the design and development of adaptive systems, gained \nthrough participation in several research projects in the field. His research interests lie in the \n 52\nareas of evaluation of adaptive systems, adaptive support for personalized and collaborative \nlearning, and meta-adaptivity. Together with Dr. Stephan Weibelzahl and Dr. Judith \nMasthoff, he has organized six workshops and delivered two tutorials on the subject of the \nvaluation of adaptive systems, and maintains the community website on evaluating adaptive \ner Experience and blended learning.  \nr. Judith Masthoff is a Senior Lecturer in Computing Science at the University of \nAberdeen. She received her Ph.D. from the Eindhoven University of Technology on a thesis \nthat described an agent-based adaptive instruction system (awarded 1997 SNS bank prize for \nbest applied thesis of the university in that year). Her research interests lie in the areas of \nintelligent user interfaces, group recommender systems, persuasive technology, the evaluation \nof adaptive systems, personalized time-based media and automated diagrammatic reasoning.  \nShe is also involved in public engagement with science, most recently in The Joking \nComputer project. She currently co-leads the Computing Science discipline at the University \nof Aberdeen. \n \n \ne\nsystems, EASy-Hub (http:\/\/www.easy-hub.org). \n \nDr. Stephan Weibelzahl is a Lecturer in the School of Computing at National College of \nIreland, Dublin. He is also Principal Investigator of the National e-Learning Laboratory. \nStephan has longstanding experience in the evaluation of adaptive systems. In his PhD thesis \nhe tried to integrate current research on this topic and explored suitable evaluation methods \nand criteria. He has published on the problems arising in the area and adequate methods to \naddress these problems. His research interests lie in adaptive learning systems, adaptation to \nmotivation, Us\n \nD\n 53 \nAppendix (Figures and Tables) \n \n \nmonitoring\nmechanism\ninteraction\nassessment\nadaptation\ndecision making\nadaptive \napplication\nadaptation \nmechanism\nlow-level monitoring \ninformation\n- keystrokes\n- tasks initiated, completed,\ncancelled, etc\n- answers to quizzes\nadaptation decisions\n- display pop-up help window\n- re-structure hyper-space\n- provide details on learning\nconcept\nhigh-level assessment\nconclusions:\n- user is disoriented\n- user is unable to \ncomplete task\n- student has not\nunderstood concept  \nFigure 1: Adaptation decomposed into two high-level phases: interaction assessment and adaptation \ndecision making (Brusilovsky et al., 2001) \n \n \n \n \nFigure 2: Four-layered decomposition model in the evaluation framework proposed by Weibelzahl (2003). \n \n \n \n 54\nuser\ninput\noutput\nin\nte\nra\nct\nio\nn \nm\non\nito\nrin\ng\ninterpretation \/ inferences\nmodelling\n ...\nadaptation decision \nmaking\napplying \nadaptations\ntransparent models &\nadaptation \u201crationale\u201d\nexplicitly provided\nknowledge\ncontext\nplans\nuser\nau\nto\nm\nat\nic\n a\nda\npt\nat\nio\nn \nas\nse\nss\nm\nen\nt\n Legend:\nstandard stages and components standard connections between components\noptional stages and components optional connections between components  \nFigure 3: Decomposition model for \u201cmodular\u201d evaluation of adaptive user interfaces  \n(Paramythis et al., 2001) \n \n \n \n Brusilovsky et al., 2001 Paramythis et al., 2001 Weibelzahl, 2001 \nInteraction monitoring Evaluation of input data \nInterpretation \/ inferences Interaction assessment  \nModelling \nEvaluation of the inference \nmechanism \nAdaptation decision \nmaking \nA\nda\npt\nat\nio\nn \npr\noc\nes\ns \nAdaptation decision \nmaking  \nApplying adaptations \nEvaluation of the \nadaptation decisions \n   Evaluation of the total \ninteraction \n \nFigure 4: Comparison of the adaptation decomposition models in the three frameworks presented by \nBrusilovsky et al. (2001), Paramythis et al. (2001) and Weibelzahl (2001). \n \n 55 \n \nFigure 5: Logical two-level architecture of adaptation;  \nadapted from (Totterdell, Rautenbach, Wilkinson, & Anderson, 1990) \n \n \n \n \n \nFigure 6: General schema for the processing in a user-adaptive system; adapted from (Jameson, 2008). \n (dotted arrows: use of information; solid arrows: production of results.) \n \n \n \n \n 56\n \n \nFigure 7: The adaptation decomposition model underlying the proposed evaluation framework. \n \n 57 \n \nFigure 8: Classification of Adaptive Hypermedia methods and techniques, adaptation process highlights \n(Knutov et al., 2009) \n \n 58\n \nFigure 9: A partial organization of rules of thumb into a decision process for the selection of methods to use in the layered evaluation of IAS\nTable 1: An overview of layers and related criteria, along with methods that can be used for their evaluation. \nLayer Goal Evaluation criteria Evaluation methods \nCollection of \nInput Data \n(CID) \nCheck quality of \nraw input data \nAccuracy, latency, \nsampling rate \nData Mining (see 4.1.3);  \nPlay with Layer (see 4.3.1);  \nSimulated Users (see 4.3.2);  \nCross-Validation (see 4.3.3) \nInterpretation \nof the Collected \nData (ID) \nCheck that input \ndata is inter-\npreted correctly \nValidity of interpreta-\ntions, predictability, \nscrutability \nData Mining (see 4.1.3);  \nHeuristic Evaluation (see 4.2.2); \nPlay with Layer (see 4.3.1); \nSimulated Users (see 4.3.2);  \nCross Validation (see 4.3.3) \n \nModelling the \nCurrent State \nof the \u201cWorld\u201d \n(MW) \nCheck that con-\nstructed models \nrepresent real \nworld \nPrimary Criteria: Valid-\nity of interpretations or \ninferences, scrutability, \npredictability; Secon-\ndary Criteria: Concise-\nness, comprehensive-\nness, precision, sensi-\ntivity \nFocus Group (see 4.1.1; 4.2.1); \nUser-as-Wizard (see 4.1.2);  \nData Mining (see 4.1.3);  \nHeuristic Evaluation (see 4.2.2); \nPlay with Layer (see 4.3.1); \nSimulated Users (see 4.3.2); \nCross-Validation (see 4.3.3) \nDeciding upon \nAdaptation \n(DA) \nDetermine \nwhether the ad-\naptation deci-\nsions made are \nthe optimal ones \nNecessity of adapta-\ntion, appropriateness \nof adaptation, subjec-\ntive acceptance of ad-\naptation, predictability, \nscrutability, breadth of \nexperience \nFocus Group (see 4.1.1; 4.2.1); \nUser-as-Wizard (see 4.1.2); \nHeuristic Evaluation (see 4.2.2); \nCognitive Walkthrough (4.2.3);  \nSimulated Users (see 4.3.2);  \nPlay with Layer (see 4.3.1);  \nUser Test (see 4.3.1)  \nApplying \nAdaptation \nDecisions (AA) \nDetermine \nwhether the im-\nplementation of \nthe adaptation \ndecisions made \nis optimal  \nUsability criteria, time-\nliness, unobtrusive-\nness, controllability, \nacceptance by user, \npredictability, breadth \nof experience \nFocus Group (see 4.1.1);  \nUser-as-Wizard (see 4.1.2); \nHeuristic Evaluation (see 4.2.2); \nCognitive Walkthrough (4.2.3);  \nUser Test (see 4.3.1);  \nPlay with Layer (see 4.3.1) \nEvaluating \nAdaptation as a \nWhole \nEvaluate the \noverall adapta-\ntion theory,  \nmay be either \nformative or \nsummative \nSpecific for system\u2019s \nobjectives or underly-\ning theory \nHeuristic Evaluation (see 4.2.2); \nCognitive Walkthrough (see 4.2.3); \nUser Test (see 4.3.1); \nPlay with Layer (see 4.3.1) \nAll layers --- Privacy, transparency, \ncontrollability \nFocus Group (see 4.1.1; 4.2.1); \nCognitive Walkthrough (see 4.2.3); \nHeuristic Evaluation (see 4.2.2); \nUser Test (see 4.3.1)  \n \n \n \nTable 2: Summary of Collection of Input Data Layer \nCollection of Input Data (CID) \nGoal Check quality of raw input data \nEvaluation criteria Accuracy, latency, sampling rate, etc. \nEvaluation methods  Data Mining (see 4.1.3); Play with Layer (see 4.3.1); Simulated \nUsers (see 4.3.2); Cross-Validation (see 4.3.3) \n 61 \n \nTable 3: Summary of Interpretation of the Collected Data Layer \nInterpretation of the Collected Data (ID) \nGoal Check that input data is interpreted correctly \nEvaluation criteria Validity of interpretations, predictability (of system\u2019s \ninterpretations), scrutability (of system\u2019s interpretations) \nEvaluation methods  Data Mining (see 4.1.3); Heuristic Evaluation (see 4.2.2); Play \nwith Layer (see 4.3.1); Simulated Users (see 4.3.2); Cross \nValidation (see 4.3.3) \n  \n \nTable 4: Summary of the Modelling the Current State of the \u201cWorld\u201d Layer \nModelling the Current State of the \u201cWorld\u201d (MW) \nGoal Check that constructed models represent real world \nPrimary evaluation criteria Validity of interpretations or inferences, predictability (of \nsystem\u2019s modelling behaviour), scrutability (of user model) \nSecondary evaluation criteria  Comprehensiveness, conciseness, precision, sensitivity \nEvaluation methods  Focus Group (see 4.1.1; 4.2.1); User-as-Wizard (see 4.1.2); \nData Mining (see 4.1.3); Heuristic Evaluation (see 4.2.2); Play \nwith Layer (see 4.3.1); Simulated Users (see 4.3.2); Cross-\nValidation (see 4.3.3) \n \n \nTable 5: Summary of the Deciding upon Adaptation Layer \nDeciding upon Adaptation (DA) \nGoal Determine whether the adaptation decisions made are the \noptimal ones  \nEvaluation criteria Necessity of adaptation, appropriateness of adaptation, \nsubjective acceptance of adaptation, predictability (of system\u2019s \nadaptive behaviour), scrutability (of system\u2019s behaviour), breadth \nof experience \nEvaluation methods  Focus Group (see 4.1.1; 4.2.1); User-as-Wizard (see 4.1.2); \nHeuristic Evaluation (see 4.2.2); Cognitive Walkthrough (4.2.3); \nSimulated Users (see 4.3.2); Play with Layer (see 4.3.1);  \nUser Test (see 4.3.1) \n \n 62\n \nTable 6: Summary of the Applying Adaptation Decisions Layer \nApplying Adaptation Decisions (AA) \nGoal Determine whether the implementation of the adaptation \ndecisions made is the optimal one \nEvaluation criteria Usability criteria, timeliness, unobtrusiveness, user control, \nacceptance by user, predictability (of system\u2019s adaptive \nbehaviour), breadth of experience \nEvaluation methods  Focus Group (see 4.1.1); User-as-Wizard (see 4.1.2); \nHeuristic Evaluation (see 4.2.2); Cognitive Walkthrough \n(4.2.3); User Test (see 4.3.1); Play with Layer (see 4.3.1) \n \n \nTable 7: Summary of the Evaluating Adaptation as a Whole Layer \nEvaluating Adaptation as a Whole \nGoal Summative evaluation of the adaptation theory \nEvaluation criteria Specific for system\u2019s objectives or underlying theory \nEvaluation methods  Heuristic Evaluation (see 4.2.2); Cognitive Walkthrough (see \n4.2.3); User Test (see 4.3.1); Play with Layer (see 4.3.1) \n \n 63 \n \nTable 8: Overview of formative evaluation methods for IAS, against a number of selection dimensions.  \nHow Method When \nLayer\u2019s \nInput \nLayer\u2019s \nOutput \nQuality  \nassessed \nBy \nWhom \nWhich \nlayers* \nFocus Group Opinions Users \/ Experts \nMW, DA, \nAA \nUser-as-\nWizard \nCriteria or \nGold-standard \nUsers \/ \nExperts \nMW, DA, \nAA \nData Mining \nSpecification \n \nProduced \n \nGold-standard Experts CID, ID, MW \nFocus Group Opinions Users \/ Experts \nMW, DA, \nAA \nCognitive \nWalkthrough Criteria Experts \nDA+AA, \nWhole \nHeuristic \nEvaluation \nDesign \n \nGiven \n \nCriteria Experts Any \nUser-Study Given or Produced \nOpinions or \nCriteria or \nPerformance \nUsers \/ \nExperts \nDA, AA, \nWhole \nPlay with \nlayer Produced \nOpinions or \nCriteria \nUsers \/  \nExperts Any \nSimulated \nusers \nImplementation \n \n(or Design and \nWizard-of-Oz) \nProduced Criteria or Gold-standard \nSimulated \nUsers \nCID, ID, \nMW, DA \nCross \nValidation Implementation Given \nGiven \nGold-standard Experts CID, ID, MW \n \n* CID = Collect Input Data,   ID = Interpret Data,   MW = Model the current state of the world, \n                       DA = Decide upon Adaptation,   AA = Apply (or Instantiate) Adaptation. \nTable 9. Examples of using focus groups in the specification and design phases. \nPhase Layer Input Task (Question to group) \nMW layer of an ITS, which infers the learner\u2019s \nemotional state from test results and sensor \ndata.  \nThe learner has answered 70% of \nquestions correctly on the last two \ntests. He is leaning forwards.  \nWhat do you think the learner\u2019s emotional state is?  \nAA layer of a news recommender, \ninstantiating (de)emphasis. \nNeed to emphasize football news and \ndeemphasize cricket. \nHow do you think the emphasizing\/deemphasizing \nshould be done?  \nDA+AA8 layers of a recommender, deciding \nwhat features to use to explain a movie\u2019s \nsuitability (Tintarev & Masthoff, 2007). \nNone provided, participants used \ntheir knowledge about their own likes \nand dislikes and what was important \nto them. \nHow would they like to be recommended or dissuaded \nfrom watching particular movies  \nDA+AA layers of a recommender, deciding \nand instantiating how recommendations are \npresented (van Barneveld & van Setten, \n2003). \nNone provided. Participants were \ngiven some general background on \nwhat a TV recommender system \ndoes.  \nUsers produced mock-ups of the way \nrecommendations could be presented and explained. \nSpecification \nDA layer of an adaptive health information \nsystem, deciding what to tell the patient\u2019s \nclose friends. (Moncur et al., 2008). \nScenario: \u201cImagine that you are the \nclose friend of someone whose baby \nwas admitted to Neonatal Intensive \nCare after it was born recently\u201d.  \nAsked to do a \u2018card-sorting\u2019 exercise: given cards \nshowing suggested information items, they were asked \nto reach a consensus on what heading to place each \ncard under (e.g., \u201cEssential Information\u201d, \u201cNot \nneeded\u201d). \nDA+AA layers: overall look of an adaptive \npublic administration website (Gena & \nArdissono, 2004). \nNone provided. System mock-ups were provided. \nWhat do you (dis)like? How can it be improved? \nAA layer of a museum guide, selecting the \ncharacter to present the narrative (Damiano \net al., 2008). \nSome background on what the role of \ncharacter was going to be. \nFour possible characters for the museum guide were \nshown. What are your opinions on the characters, and \nwhich is the best one? \nDesign \nDA+AA layers of a recommender, deciding \nhow to explain (Tintarev & Masthoff, 2008). \nNone provided. Screenshots of different ways of explaining   \nrecommendations were provided. Which are best and \nhow to further improve them? \n                                                 \n8 We use the notation XX+YY to refer to cases where two layers are evaluated in combination, a subject that we return to in detail in section  5.2. \nTable 10. Examples of using the user-as-wizard  \nLayer Stage Input (Scenario) Task Observational method \nExpl. \nJohn, Mary, and Adam are going to watch \nclips together. A table shows their liking \nfor each of the clips.  \nDecide which five clips the group should \nwatch.  Justify \nDA layer of a group recommender \nsystem (Masthoff, 2004), selecting a \nsequence of items adapted to a group of \nusers. As part of this it has to aggregate \nindividual ratings. Cons. \nSlight variation on the scenario above: we \nused \u201cYou and two friends (Friend 1 and \nFriend 2)\u201d.  \nJudge your and your friends\u2019 satisfaction if \nshown a particular sequence of clips. \nRepeated for three sequences.  \nJustify \nDA+AA layers of a support system \n(N\u00fcckles et al., 2006). Expl. \nExperts interacted with a real layperson \nand were given information about their \nknowledge level. \nWrite instructional explanations of computer \nand internet issues in response to queries \nasked by the layperson. \nThink-aloud. \nDA+AA layers of a system convincing \npeople to eat more healthily (de Rosis et \nal., 2006), deciding on arguments and \nmessage structure. \nExpl. \nA story about a fictional friend, with \ndetails about her personality, goals, \nhabits, and healthy eating facts.  \nConstruct a message to convince this friend \nto eat more healthily.  None \nMW layer of a group recommender, \nmodelling the effect on satisfaction of \nanother member\u2019s emotion (Masthoff & \nGatt, 2006).  \nExpl. \nThink of somebody [who meets some \nrelationship criterion]. Assume you are \nwatching TV together. You are enjoying \nthe program a bit.  \nJudge how it would impact your satisfaction \nto know that the other person is really \nhating\/liking it.  \nNone \nMW layer of a persuasive system, \nmodelling how a user\u2019s attitude changes \nwhen presented with an argument \n(Nguyen et al., 2007).  \nExpl. Participants were told about Adam and his current position on nuclear power. \nJudge how a particular argument would \nchange Adam\u2019s position. Justify \nExpl. \nParticipants were given a set of items of \ninterest to a user.  \n \nConstruct a suitable textbook hierarchy to \ncontain the items, inventing titles for \nchapters, sections etc. \nCo-discovery DA+AA layers of a navigation support \nsystem,  deciding how to group items of \ninterest to the user together in a \nhierarchy and how to name groups \n(Masthoff, unpubl.)  Cons. \nHierarchies were shown, most produced \nin the Exploration Stage, others \ncomputer-generated.  \nJudge the hierarchy on given criteria. \nExplain what they disliked most. Justify \nTable 11. Examples of data mining and cross-validation.  \nMethod Layer Gold standard How the gold standard is obtained \nMW layer of a recommender that assigns users to \nlifestyles based on on-line behaviour (Lekakos & \nGiaglis, 2007). Learns classification rules. \nKnown lifestyles for a set of \nusers plus these users\u2019 on-line \nbehavioural data. \nHad a portion of the population complete a \npsychographic questionnaire that allows them to be \nclassified into lifestyle segments. \nMW layer of a speech recognition system that \npredicts the next user command based on past \nbehaviour and context of use (Paek & Chickering, \n2007). Learns decision tree. \nKnown speech commands \ngiven by users for whom we \nalso have contextual and past \ndata. \nCollected data from existing users: speech commands \n(transcribed), times of commands, personal data \ncontained on devices at the time of each command, \nwhat types of commands users had enabled. \nData Mining \nand  \nCross-\nvalidation \nID layer of an ITS which infers a learner\u2019s emotions \nat any given time from conversational features (D\u2019 \nMello et al., 2008). Learns which dialogue features \npredict which affective state (regression). \nKnown emotional state for \nlearners for whom we also \nhave conversational data. \nCollected conversational data in interactions with the \nITS, and afterwards measured the learners\u2019 emotions \n(in 20s intervals) using videos showing screen content, \nfacial expressions, and speech. This was done through \nself-rating, and rating by peers and trained judges.  \nMW layer of a recommender that infers a utility \nfunction (able to decide on the best k items) from \nusers\u2019 qualitative preference statements (Domshlak \n& Joachims, 2007). \nKnown ratings and therefore \nknown preference orders for \nusers for whom we have \npreference statements. \nUses EachMovie and MovieLens datasets which \ncontain movie ratings and attributes\/genres. As \npreference statements were unavailable, these were \ngenerated using a decision tree learning algorithm. \nMW layer of a search support system that infers a \nuser\u2019s preference for topics from click patterns \n(Stamou & Ntoulas, 2009). \nKnown interest in topics for \nsearchers for whom we also \nhave click & relevance data. \nCollected data from Google query stream. Users \nprovided their general interest in topics for each query \nand rated the relevance of visited pages.  \nMW layer of a spoken dialogue tutoring system, \nwhich infers learning from, amongst others, \naffective state (Forbes-Riley et al., 2008).  \nKnown learning for learners for \nwhom we also have perfect \ninput data. \nMeasured learning through a post-test. For the training, \ncorpora of learner interactions were used, which had \nbeen annotated with affective states, turn correctness \nand discourse structure. \nMW layer of a movie recommender system \n(Degemmis et al., 2007). \nKnown ratings for movies. Uses EachMovie dataset which contains movie ratings \nand movie attributes. \nCross-\nvalidation \nMW layer of a dialogue system, predicting a \nsession\u2019s outcome (Horvitz & Paek, 2007).  \nKnown outcomes of callers\u2019 \nsessions.  \nUsed data from the legacy system. \n 66\nTable 12. Examples of proposed evaluation criteria and questions that can be used in heuristic evaluation \nCriteria Example Questions \nTransparency \/ \nComprehensibility \n\u2212 Does the user know and understand what the system has captured (CID), interpreted (ID) and modelled (MW), and why; what \nadaptation decisions it has taken and why (DA); and how adaptation has happened (AA)? \nPredictability \u2212 Is the user able to predict what the effect of their actions will be on the system\u2019s beliefs (ID, MW) and decisions (DA)?  \n\u2212 Is the adaptation not making the user experience too inconsistent? (AA) \n\u2212 Are users asked to approve major changes to the system\u2019s appearance\/functioning? (DA, AA) \n\u2212 Does the system follow the conventions of applications the user normally uses? (AA) \n\u2212 Are adaptations done in a way that fits with user\u2019s expectations from the real world? (AA) \nPrivacy \u2212 Is the user informed about the kind of data captured about them (CID), the type of inferences drawn (ID, MW), and the way this data is \nstored and used (ID, MW, DA)?  \n\u2212 Is the user able to decide the kind of data captured about them (CID), inferences allowed to be drawn (ID, MW), adaptations shown \n(AA), which data is stored (ID, MW) and what it is used for (DA)?  \n\u2212 Is personal data protected in a way similar to the real world? (ID, MW) \nControllability \/ \nScrutability \n\u2212 Can the user undo or change system interpretations (ID), user modelling actions (MW), adaptation decisions (DA)? \n\u2212 Can the user influence how adaptations are applied (AA), and how inferences (MW) and decisions (DA) are made, e.g., by setting \nparameters that control the system\u2019s behaviour? \nBreadth of \nExperience \/ \nSerendipity \n\u2212 Is the user still able to access material that the system thought was less suitable for them (AA)? \n\u2212 Does the system allow users to make unexpected pleasant discoveries, rather than restricting experience (DA \/ AA)? \nUnobtrusiveness  \u2212 Are explanations of system\u2019s actions not disturbing the user unnecessarily and too often? (AA) \n\u2212 Is the user\u2019s approval of system actions not sought too often, when it is not really needed? (ID, MW, DA) \nTimeliness \u2212 Is the timing of system actions (e.g., messages) appropriately adapted to the users\u2019 activities and context? (DA, AA) \nAesthetics \u2212 Are automatic changes to the system\u2019s appearance aesthetically pleasing? (AA) \nAppropriateness \/ \nNecessity \n \n\u2212 How necessary was the action the system decided upon? (this and the next question should be posed for individual actions, rather \nthan collectively)  (DA) \n\u2212 How appropriate was the action the system decided upon given the interaction state (and history) and the system\u2019s adaptive theory? \n(this question is not intended to assess whether the theory is valid, but whether the action is consistent with the theory\u2019s relevant \npremises) (DA) \n 67 \nTable 13. Examples of standard user tests (task-based and direct), and indirect and play-with-layer variants.  \n Layer Input Task Measurement Observational Method Criteria \nMW layer of  a \nrecommender system \nProduced by \nparticipants \nConvince it \nyou hate \ncricket. \nDo they succeed and if so, how \nquickly? What causes problems? Co-discovery \nTransparency \nScrutability Standard \n \n(task-\nbased,  \ndirect) \nDA+AA layers of a \nrecommender deciding how \nto explain (Tintarev & \nMasthoff, 2007) \nParticipants set their own \nuser model, via a \nspecially made GUI. \nDecide how \nmuch you \nlike a \nmovie. \nRatings of the explanations on \nvarious criteria \n \nJustification Effectiveness Persuasiveness \nDA+AA layers of an ITS \nthat annotates lesson links \nTold about a learner, and \nthat the system had \nadapted. \nSelect a \nlesson to \nsuit this \nlearner. \nDo they enjoy using the system, \ntrust it, make appropriate and fast \ndecisions? What causes confusion?\nCo-discovery \nSatisfaction, \nEffectiveness, \nEfficiency \nTrust \nIndirect \nDA+AA layers of a museum \nguide, which decides what \nto tell the user (Goren-Bar \net al., 2006) \nTold about a visitor, and \nshown videos of aspects \nof interaction with 2 \nguides. \nRate \nguides on \naspects, \npick best. \nJustifications were analysed for \nstatements related to the criteria. \nAnalysed relation between \npersonality and preferences. \nJustification \nAcceptability \nTransparency \nUsability \nMemorisability \nCID layer of a news \nrecommender, deciding \nwhat the user looks at \nParticipants look at \ndifferent parts of the \nscreen. \nHow accurately and fast it picks up \nwhat they look at. Requires a GUI \nshowing the layer\u2019s output. \nNone Accuracy Efficiency \nID layer of a recommender \ndeciding interest based on \nwhat users look at \nParticipants position the \nmouse on items they look \nat. \nDo participants agree with the \ninferred interests?  Retrospective \nAccuracy \nAcceptability \nDA layer of recommender, \ndeciding music based on \nusers present and moods \n(Masthoff et al., 2007) \nParticipants set users\u2019 \nmusic preferences and \nsimulate users entering \nand exiting. \nSimulator shows the individuals\u2019 \nmood based on music played so \nfar. Participants judge the \ndecisions. \nJustification Effectiveness \nPlay with \nLayer \nOverall experience of a \nmuseum guide (Stock et \nal., 2007) \nParticipants use the guide \nin a real museum setting. \n \n \n \n \nQuestionnaires Retrospective \nEase of use \nIntention to use \nInvolvement \n 68\nTable 14. Examples of the use of simulated users.  \nLayer Input (through Simulation) Measurement Criteria \nDA layer of an ITS, deciding what \nword-pair to teach next   (Masthoff, \n2002). \nAnswers to practice items produced by simulated students, \nbased on models of learning proposed in the literature. \nSimulations with varying models and parameter values.  \nHow many correct responses the \nsimulated learners get on average \non a test, for different variants of \nthe DA layer.  \nEffectiveness \nDA layer of a Group \nRecommender, deciding which \nmusic item to play next (Masthoff & \nGatt, 2006). \nAffective state produced by simulated users. Simulations \nwith varying parameter values. \nHow the simulated users\u2019 feel at \nany moment based on decisions \nmade by different variants of the \nDA layer.  \nEffectiveness (to \nkeep individuals \nsatisfied) \nMW layer of an ITS, inferring \nknowledge based on replies to \nquestions (Guzm\u00e1n et al., 2007).  \nAnswers to test items produced by simulated students with \nknown prior knowledge levels.   \nComparing real knowledge with \ninferred knowledge. Measuring \ntime. \nAccuracy \nEfficiency \nMW layer of a navigation support \nsystem, which divides a website\u2019s \npages on the basis of user logs into \nsets that correspond to navigation \nstages (Hollink et al., 2007). \nNavigation log files produced by simulated navigators (finite \nstate automata modelling transition between navigation \nstages). \nHow often did the algorithm \ndiscover the right number of \nstages? \nAccuracy \nDA layer of a cognitive assistance \nsystem, deciding when to assist \n(Serna et al., 2007). \nActions and mistakes when performing a cooking task by \nsimulated people with Alzheimer\u2019s disease. The simulation \nmodel is parameterized according to the different stages of \nthe disease.  \nCan measure impact of \nassistance provided on number of \nmistakes made. Not really \ncovered yet in this study. \nEffectiveness \nMW layer in a dialogue \nmanagement system, personalising \na baseline model to a voice \n(Chickering & Paek, 2007). \nSpeech commands produced by simulated voices with \nvaried values for parameters. \nCompared accuracy of different \nstrategies for personalizing the \nmodel.  \nAccuracy \n \n 69 \n"}