{"doi":"10.1109\/ICCSIT.2009.5234439","coreId":"54263","oai":"oai:eprints.lincoln.ac.uk:2670","identifiers":["oai:eprints.lincoln.ac.uk:2670","10.1109\/ICCSIT.2009.5234439"],"title":"Near range path navigation using LGMD visual neural networks","authors":["Yue, Shigang","Rind, F. Claire"],"enrichments":{"references":[{"id":942745,"title":"A bio-inspired visual collision detection mechanism for cars: combining insect inspired neurons to create a robust system.","authors":[],"date":"2007","doi":"10.1016\/j.biosystems.2006.09.010","raw":"Stafford, R., Santer R.D. & Rind, F.C. A bio-inspired visual collision detection mechanism for cars: combining insect inspired neurons to create a robust system. BioSystems, 87, 162-169, 2007.","cites":null},{"id":944187,"title":"A bioinspired visual collision detection mechanism for cars: optimisation of a model of a locust neuron to a novel environment.","authors":[],"date":"2006","doi":"10.1016\/j.neucom.2005.06.017","raw":"Yue, S., Rind, F.C. Keil, M.S., Cuadri, J. & Stafford, R. A bioinspired visual collision detection mechanism for cars: optimisation of a model of a locust neuron to a novel environment. Neurocomputing, vol.69 (13-15), 1591-1598, 2006.","cites":null},{"id":943402,"title":"A Collision detection system for a mobile robot inspired by locust visual system.","authors":[],"date":"2005","doi":"10.1109\/robot.2005.1570705","raw":"Yue, S. & Rind, F.C.  A Collision detection system for a mobile robot inspired by locust visual system. IEEE Int. Conf. on Robotics and Automation, Spain, Barcelona, Apr.18-21, 2005, 3843-3848, 2005.","cites":null},{"id":938300,"title":"A silicon implementation of the fly's optomotor control system.","authors":[],"date":"2000","doi":"10.1162\/089976600300014944","raw":"Harrison, R.R., & Koch, C. A silicon implementation of the fly's optomotor control system. Neural Computation, vol.12, 2291-2304, 2000.","cites":null},{"id":944627,"title":"A synthetic vision system using directionally selective motion detectors to recognize collision.","authors":[],"date":"2007","doi":"10.1162\/artl.2007.13.2.93","raw":"Yue, S. & Rind, F.C. A synthetic vision system using directionally selective motion detectors to recognize collision. Artificial Life, vol.13 (2), 93-122, 2007.","cites":null},{"id":939882,"title":"Anology integrated circuit for motion detection against moving background based on the insect visual system.","authors":[],"date":"2004","doi":"10.1007\/s10043-004-0024-4","raw":"Nishio, K., Yonezu, H., Kariyawasam, A.B., Yoshikawa, Y. Sawa, S. & Furukawa, Y., Anology integrated circuit for motion detection against moving background based on the insect visual system. Optical Review, vol.11, 1, 24-33, 2004.","cites":null},{"id":941773,"title":"Bioinspired sensors: from insect eyes to robot vision. Frontiers in Neuroscience: Methods in Insect Sensory Neuroscience,","authors":[],"date":null,"doi":"10.1201\/9781420039429.ch8","raw":"Rind, F.C. Bioinspired sensors: from insect eyes to robot vision. Frontiers in Neuroscience: Methods in Insect Sensory Neuroscience, Christensen T.A. (Eds.), CRC Press Boca Raton, London, New York,","cites":null},{"id":939080,"title":"Biologically Inspired Visual Odometer for Navigation of a Flying Robot. Robotics and Autonomous Systems,","authors":[],"date":"2003","doi":"10.1016\/s0921-8890(03)00070-8","raw":"Iida, F.  Biologically Inspired Visual Odometer for Navigation of a Flying Robot. Robotics and Autonomous Systems, vol.44\/3-4, 201-208, 2003.","cites":null},{"id":943191,"title":"Can robots learn to see?.","authors":[],"date":"1999","doi":"10.1016\/S0967-0661(99)00029-5","raw":"Wichert,G. Can robots learn to see?. Control Engineering Practice, vol.7, pp.783-795, 1999.","cites":null},{"id":936699,"title":"Collision avoidance using a model of the locust LGMD neuron. Robotics and Automonous Systems,","authors":[],"date":"2000","doi":"10.1016\/s0921-8890(99)00063-9","raw":"Blanchard, M. Rind, F.C. & Verschure, P.F.M.J. Collision avoidance using a model of the locust LGMD neuron. Robotics and Automonous Systems, vol.30, 17-38. 2000.","cites":null},{"id":943980,"title":"Collision detection in complex dynamic scenes using a LGMD based visual neural network with feature enhancement.","authors":[],"date":"2006","doi":"10.1109\/TNN.2006.873286","raw":"Yue, S. & Rind, F.C. Collision detection in complex dynamic scenes using a LGMD based visual neural network with feature enhancement. IEEE Transactions on Neural Networks, May, vol.17 (3), 705-716, 2006.","cites":null},{"id":945097,"title":"Competence comparison of collision sensitive neural systems in dynamic environments. Artificial Life, 2008a (under review).","authors":[],"date":null,"doi":null,"raw":"Yue. S. & Rind, F.C. Competence comparison of collision sensitive neural systems in dynamic environments. Artificial Life, 2008a (under review).","cites":null},{"id":945325,"title":"Exploring organization of direction selective neural networks for road collision prediction.","authors":[],"date":null,"doi":null,"raw":"Yue, S. & Rind, F.C. Exploring organization of direction selective neural networks for road collision prediction. IEEE Trans. Intelligent Transport Systems, 2008b (under review).","cites":null},{"id":942163,"title":"Gliding behaviour elicited by lateral looming stimuli in flying locusts.","authors":[],"date":"2005","doi":"10.1007\/s00359-004-0572-x","raw":"Santer,R.D., Simmons,P.J. & Rind, F.C. Gliding behaviour elicited by lateral looming stimuli in flying locusts. Journal of Comparative Physiology, vol.191, 61-73, 2005.","cites":null},{"id":936949,"title":"Learning and understanding dynamic scene activities: a review.","authors":[],"date":"2003","doi":"10.1016\/s0262-8856(02)00127-0","raw":"Buxton, H. Learning and understanding dynamic scene activities: a review. Image and Vision Computing, vol.21, 125-136, 2003.","cites":null},{"id":944836,"title":"Locust-like emergent escape behaviour of mobile robots emerged with bilateral pair of visual LGMD\/DCMD, Autonomous Robots,","authors":[],"date":"2007","doi":"10.1007\/s10514-009-9157-4","raw":"Yue, S., Santer, R., Yamawaki, Y., & Rind F.C., Locust-like emergent escape behaviour of mobile robots emerged with bilateral pair of visual LGMD\/DCMD, Autonomous Robots, 2007 (under review).","cites":null},{"id":941537,"title":"Locust\u2019s looming detectors for robot sensors. Sensors and Sensing","authors":[],"date":"2003","doi":"10.1007\/978-3-7091-6025-1_17","raw":"Rind, F.C. Santer, R.D.J., Blanchard, M. & Verschure, P.F.M.J. Locust\u2019s looming detectors for robot sensors. Sensors and Sensing in Biology and Engineering, FG Barth, JAC Humphrey, and TW Secomb (Eds.), Spinger-Verlag, Wien, New York, 2003.","cites":null},{"id":941302,"title":"Motion detectors in the locust visual system: from biology to robot sensors.","authors":[],"date":"2002","doi":"10.1002\/jemt.10029","raw":"Rind, F.C. Motion detectors in the locust visual system: from biology to robot sensors. Microscopy Research and Technique, vol.56, 256-269, 2002.","cites":null},{"id":940532,"title":"Neural network based on the input organization of an identified neuron signaling impending collision.","authors":[],"date":"1996","doi":null,"raw":"Rind, F.C. & Bramwell, D.I.  Neural network based on the input organization of an identified neuron signaling impending collision. Journal of Neurophysiology, vol.75, 967\u2013 985, 1996.","cites":null},{"id":939380,"title":"Neuromorphic vision sensors.","authors":[],"date":"2000","doi":"10.3390\/s8095352","raw":"Indiveri, G. & Douglas, R. Neuromorphic vision sensors. Science, vol.288, 1189-1190, 2000.","cites":null},{"id":939626,"title":"Obstacle detection and terrain classification for autonomous off-road navigation. Autonomous Robots,","authors":[],"date":"2005","doi":"10.1023\/b:auro.0000047286.62481.1d","raw":"Manduchi, R. Castano, A. Talukder A. & Matthies, L. Obstacle detection and terrain classification for autonomous off-road navigation. Autonomous Robots, vol.18, 81-102, 2005.","cites":null},{"id":938816,"title":"On robots and flies: modelling the visual orientating behaviour of flies. Robotics and Autonomous Systems,","authors":[],"date":"1999","doi":"10.1016\/s0921-8890(99)00055-x","raw":"Huber, S.A. Franz M.O. & Buelthoff, H.H. On robots and flies: modelling the visual orientating behaviour of flies. Robotics and Autonomous Systems, vol.29, 227-242, 1999.","cites":null},{"id":940773,"title":"Orthopteran DCMD neuron: A reevaluation of responses to moving objects. I. Selective responses to approaching objects.","authors":[],"date":"1992","doi":null,"raw":"Rind, F.C. Simmons, P.J. Orthopteran DCMD neuron: A reevaluation of responses to moving objects. I. Selective responses to approaching objects.  Journal of Neurophysiology, vol.68, 1654\u20131666, 1992.","cites":null},{"id":942508,"title":"Orthopteran DCMD neuron: A reevaluation of responses to moving objects. II. Critical cues for detecting approaching objects.","authors":[],"date":"1992","doi":null,"raw":"Simmons, P.J., Rind, F.C. Orthopteran DCMD neuron: A reevaluation of responses to moving objects. II. Critical cues for detecting approaching objects. Journal of Neurophysiology, vol.68, 1667\u20131682, 1992.","cites":null},{"id":942890,"title":"Reafferent or redundant: integration of phonotaxis and optomotor behaviour in crickets and robots. Adaptive behaviour,","authors":[],"date":"2003","doi":"10.1177\/1059712303113001","raw":"Webb, B. & Reeve, R. Reafferent or redundant: integration of phonotaxis and optomotor behaviour in crickets and robots. Adaptive behaviour, vol.11 (3), 137-158. 2003.","cites":null},{"id":938067,"title":"Redestrian protection systems: issues, survey, and challenges.","authors":[],"date":"2007","doi":"10.1109\/tits.2007.903444","raw":"Gandhi, T. and Trivedi, M.M. Redestrian protection systems: issues, survey, and challenges. IEEE Transactions on Intelligent Transport Systems, vol.8, no.3, 413-430, 2007.","cites":null},{"id":942392,"title":"Response of the locust descending contralateral movement detector neuron to rapidly approaching and withdrawing visual stimuli.","authors":[],"date":"1977","doi":"10.1139\/z77-179","raw":"Schlotterer, G.R. Response of the locust descending contralateral movement detector neuron to rapidly approaching and withdrawing visual stimuli. Canadian Journal of Zoology, vol.55, 1372\u20131376, 1977.","cites":null},{"id":937588,"title":"Robot navigation using panoramic tracking.","authors":[],"date":"2004","doi":"10.1016\/j.patcog.2004.02.017","raw":"Fiala, M. & Basu, A. Robot navigation using panoramic tracking. Pattern Recognition, vol.37, 2195-2215, 2004.","cites":null},{"id":940086,"title":"Rover navigation using stereo ego-motion. Robotics and Autonomous Systems,","authors":[],"date":"2003","doi":"10.1016\/s0921-8890(03)00004-6","raw":"Olson,C.F., Matthies, L.H., Schoppers, M. and Maimone, M.W. Rover navigation using stereo ego-motion. Robotics and Autonomous Systems, vol.43, 215-229, 2003.","cites":null},{"id":941039,"title":"Seeing what is coming: Building collision sensitive neurons. Trends in Neurosciences,","authors":[],"date":"1999","doi":"10.1016\/s0166-2236(98)01332-0","raw":"Rind, F.C. & Simmons, P.J.  Seeing what is coming: Building collision sensitive neurons. Trends in Neurosciences, vol.22, 215-220, 1999.","cites":null},{"id":938570,"title":"Systemtheorische analyse der zeit-, reihenfolgen- und vorzeichenauswertung bei der bewegungsperzeption des ruesselkaefers chlorophanus. Zeitschrift fuer Naturforschung,","authors":[],"date":null,"doi":null,"raw":"Hassenstein, B., & Reichardt, W. Systemtheorische analyse der zeit-, reihenfolgen- und vorzeichenauswertung bei der bewegungsperzeption des ruesselkaefers chlorophanus. Zeitschrift fuer Naturforschung, vol.11b, 513-524.","cites":null},{"id":940309,"title":"The anatomy of a locust visual interneurone: The descending contralateral movement detector.","authors":[],"date":"1974","doi":"10.1007\/bf00698057","raw":"O\u2019Shea, M. Rowell, C.H.F., Williams, J.L.D.  The anatomy of a locust visual interneurone: The descending contralateral movement detector.  Journal of Exp. Biology, vol.60, 1\u201312, 1974.","cites":null},{"id":941912,"title":"The neuronal basis of a sensory analyser, the acridid movement detector system .IV. The preference for small field stimuli.","authors":[],"date":"1977","doi":null,"raw":"Rowell, C.H.F. O\u2019Shea, M.  Williams, J.L.  The neuronal basis of a sensory analyser, the acridid movement detector system .IV. The preference for small field stimuli. J. Experimental Biology, vol.68, 157-185, 1977.","cites":null},{"id":937503,"title":"Vision for mobile robot navigation: a survey.","authors":[],"date":"2002","doi":"10.1109\/34.982903","raw":"DeSouza, G.N. & Kak, A.C. Vision for mobile robot navigation: a survey.  IEEE Transactions on Pattern Analysis and Machine Intelligence, vol.24 (2), 237-67, 2002.","cites":null},{"id":937809,"title":"Visual guidance based on optic flow: a biorobotic approach.","authors":[],"date":"2004","doi":"10.1016\/j.jphysparis.2004.06.002","raw":"Franceschini, N. Visual guidance based on optic flow: a biorobotic approach. Journal of Physiology Paris, vol.98, pp281-292, 2004.","cites":null},{"id":944392,"title":"Visual motion pattern extraction and fusion for collision detection in complex dynamic scenes. Computer Vision and Image Understanding,","authors":[],"date":null,"doi":"10.1016\/j.cviu.2006.07.002","raw":"Yue, S. & Rind, F.C. Visual motion pattern extraction and fusion for collision detection in complex dynamic scenes. Computer Vision and Image Understanding, vol.104 (1), 48-60, 2006b.","cites":null},{"id":937186,"title":"Visually mediated motor planning in the escape response of drosophila.","authors":[],"date":"2008","doi":"10.1016\/j.cub.2008.07.094","raw":"Card, G. & Dickinson, M.H. Visually mediated motor planning in the escape response of drosophila. Current Biology, vol.18(17), 1-8, 2008.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2009-08","abstract":"In this paper, we proposed a method for near range path navigation for a mobile robot by using a pair of biologically\\ud\ninspired visual neural network \u2013 lobula giant movement detector (LGMD). In the proposed binocular style visual system, each LGMD processes images covering a part of the wide field of view and extracts relevant visual cues as its output. The outputs from the two LGMDs are compared and translated into executable motor commands to control the wheels of the robot in real time. Stronger signal from the LGMD in one side pushes the robot away from this side step by step; therefore, the robot can navigate in a visual environment naturally with the proposed vision system. Our experiments showed that this bio-inspired system worked well in different scenarios","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/54263.pdf","fullTextIdentifier":"http:\/\/eprints.lincoln.ac.uk\/2670\/1\/2009-icainn-Yue%26Rind-2lgmd-final.pdf","pdfHashValue":"b35ac2406969e6e4be28265ddd5e3a140a58621f","publisher":"IEEE","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lincoln.ac.uk:2670<\/identifier><datestamp>\n      2017-12-05T11:23:09Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47343030<\/setSpec><setSpec>\n      74797065733D636F6E666572656E63655F6974656D<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lincoln.ac.uk\/2670\/<\/dc:relation><dc:title>\n        Near range path navigation using LGMD visual neural networks<\/dc:title><dc:creator>\n        Yue, Shigang<\/dc:creator><dc:creator>\n        Rind, F. Claire<\/dc:creator><dc:subject>\n        G400 Computer Science<\/dc:subject><dc:description>\n        In this paper, we proposed a method for near range path navigation for a mobile robot by using a pair of biologically\\ud\ninspired visual neural network \u2013 lobula giant movement detector (LGMD). In the proposed binocular style visual system, each LGMD processes images covering a part of the wide field of view and extracts relevant visual cues as its output. The outputs from the two LGMDs are compared and translated into executable motor commands to control the wheels of the robot in real time. Stronger signal from the LGMD in one side pushes the robot away from this side step by step; therefore, the robot can navigate in a visual environment naturally with the proposed vision system. Our experiments showed that this bio-inspired system worked well in different scenarios.<\/dc:description><dc:publisher>\n        IEEE<\/dc:publisher><dc:date>\n        2009-08<\/dc:date><dc:type>\n        Conference or Workshop contribution<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lincoln.ac.uk\/2670\/1\/2009-icainn-Yue%26Rind-2lgmd-final.pdf<\/dc:identifier><dc:identifier>\n          Yue, Shigang and Rind, F. Claire  (2009) Near range path navigation using LGMD visual neural networks.  In: 2009 2nd IEEE International Conference on Computer Science and Information Technology, 2009, August, Beijing, China.  <\/dc:identifier><dc:relation>\n        http:\/\/www.computer.org\/portal\/web\/csdl\/doi\/10.1109\/ICCSIT.2009.5234439<\/dc:relation><dc:relation>\n        10.1109\/ICCSIT.2009.5234439<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lincoln.ac.uk\/2670\/","http:\/\/www.computer.org\/portal\/web\/csdl\/doi\/10.1109\/ICCSIT.2009.5234439","10.1109\/ICCSIT.2009.5234439"],"year":2009,"topics":["G400 Computer Science"],"subject":["Conference or Workshop contribution","PeerReviewed"],"fullText":" Near Range Path Navigation Using LGMD Visual Neural Networks \n \nShigang Yue \nDept of Computing and Informatics \nUniversity of Lincoln \nLincoln, LN6 7TS, UK \n e-mail: syue@lincoln.ac.uk \nF. Claire Rind \nBuilding, School of Biology  \nUniversity of Newcastle \nNewcastle upon Tyne, NE1 7RU, UK \ne-mail: claire.rind@ncl.ac.uk\n \n \nAbstract\u2014 in this paper, we proposed a method for near range \npath navigation for a mobile robot by using a pair of biologically \ninspired visual neural network \u2013 lobula giant movement detector \n(LGMD). In the proposed binocular style visual system, each \nLGMD processes images covering a part of the wide field of view \nand extracts relevant visual cues as its output. The outputs from \nthe two LGMDs are compared and translated into executable \nmotor commands to control the wheels of the robot in real time. \nStronger signal from the LGMD in one side pushes the robot \naway from this side step by step; therefore, the robot can navigate \nin a visual environment naturally with the proposed vision \nsystem. Our experiments showed that this bio-inspired system \nworked well in different scenarios. \nKeywords- visual neural network, LGMD, path navigation \nI.  INTRODUCTION \nAutonomous mobile robots have long been expected to \npossess the ability to explore their paths in a dynamic \nenvironment and interact with dynamic moving objects \neffectively and free of collisions. However, it is still difficult \nfor a mobile robot to run autonomously without collision in \ncomplex environments using vision only (Wichert 1999, \nIndiveri and Douglas 2000, DeSouza and Kak 2002, Buxton \n2003). Even with several kinds of sensors - including visual, \nultrasound, infra-red, laser, and mini-radar to provide more \nenvironmental information, a robot may still encounter \ndifficulties in interpret the plenty of data collected in real \ntime with limited computing power (e.g., Everett 1995, \nOlson et. al. 2003, Manduchi et. al. 2005), especially in a \nhuman centred urban environment (e.g., Gandhi and Trivedi \n2007, Urban Challenge 2007). Effectively and cheaply \nextract relevant visual cues from the visual field and turn \nthese cues into executable motor commands are critical for a \nrobot exploring its local environment or interact with \ndynamic objects free of collision. However, both are difficult \ntasks because of the visual complexity. \nAnimals often possess incredible visual systems to detect \nand react to the changes of their environment appropriately. \nMechanisms revealed in animal visual systems provide \nunique and robust solutions for robotic problems such as \nnavigation, collision avoidance, landing and source target \ntracing etc. (e.g., Blenchard & Rind 1999 and 2000, Huber \net. al. 1999, Harrison & Koch 2000, Webb & Reeve 2003, \nIida 2003, Nishio et. al. 2004, Franceschini 2004, Yue & \nRind 2006, 2007, 2008a and 2008b). In locusts, it is believed \nthat the LGMDs, and their postsynaptic partners, the \ndescending contralateral movement detectors (DCMDs) \n(Schlotterer, 1977, Rind and Simmons, 1992, 1999, Gabbiani \net.al. 2004, 2006, Santer et al., 2005, 2006) may play an \nimportant role for collision avoidance (Rind 2000, Rind et. \nal. 2003), as demonstrated with simulated models either in \nsingle to trigger collision avoidance (Blanchard et. al 1999, \n2000, Rind and Bramwell 1996, Santer et. al. 2004, Yue and \nRind 2005, 2006, Stafford et.al. 2007) or in pair to control \nescape directions (Yue et.al. 2007).  As a visual neural \nnetwork, LGMD is ideal for extracting collision visual cues \nfrom dynamic scenes. These extracted cues can then be \ninterpreted into executable motor commands directly to drive \na mobile robot. Inspired by the binocular vision of many \nanimal species, we propose a LGMD based vision system \nthat works in a binocular style to generate near range path \nnavigation behavior.  \n \n \n \n \nFigure 1.  The schematic illustration of the vision system for a mobile \nrobot with a pair of LGMDs to process images covering overlapped fields \nof view parallelly. \n \nIn this paper, we build a system with a pair of LGMDs \nand a simple motor system. We feed the pair of LGMDs with \ninput images from a panoramic visual camera and integrate \nthe vision system with the real time control system of a \nmobile robot through a motor system. We test the navigation \nability of the robot equipped with presented system by \n\u2500 \nMotor system \ncarrying out several experiments in a robotic laboratory. We \nexpect the system enables a mobile robot moving within a \nvisual environment efficiently while creating a reasonable \npath without collision.  \nIn the following parts, the proposed system is described \nin detail in section 2.  Experiments and results are related in \nsection 3. Discussion is presented in section 4 and conclusion \nis drawn in section 5. \n \nII. THE VISUAL NEURAL SYSTEM \nThe vision system in the study includes two visual \nLGMD neural networks to extract relevant visual cues and a \nmotor system to use the inputs from the two LGMDs to \ncontrol the two robotic wheels. The vision system is \nintegrated to a Khapera mobile robot (k-team, Lausanne, \nSwitzerland) with a panoramic camera capturing live images. \nThe whole system is schematically illustrated in Figure 1. \n \nA. The LGMD \nThe LGMD model (Figure 1) used in this study is based \non the neural network described in Yue & Rind (2005) with \nminor simplification. The LGMD model responds selectively \nto movement in depth or looming. The left and right LGMD \nare identical but pick up different input images. The left \nLGMD is fed with image from the left side hemisphere while \nthe right LGMD takes the right side images as its input. \nThe LGMD network is composed of four layers of cells - \nphotoreceptor (P), excitatory (E), inhibitory (I) and summing \n(S), and a single cells LGMD. \nIn the first layer of the neural network are the \nphotoreceptor P cells which are arranged in a matrix to \ncapture the luminance change, \n),(),(),( 1 yxLyxLyxP fff \u2212\u2212=                   (1) \nwhere Pf(x,y) is the change in luminance corresponding to \npixel (x,y) at frame f; x and y are the pixel coordinates, Lf and \nLf-1 are the luminance, subscript f denotes the current frame \nand f-1 denotes the previous frame.  \nThe outputs of the P cells form the inputs to two separate \ncell types in the next layer. One type is the excitatory cells, \nthrough which excitation is passed directly. The second cell \ntype is the lateral inhibition cells, which pass inhibition, after \na 1 image frame delay. The strength of inhibition delivered \nto a cell in this layer is given by, \n)0,(),,(),(),( 1 =\u2260++= \u2212\n\u2212= \u2212=\n\u2211\u2211 iifjijiwjyixPyxI Ifn\nni\nn\nnj\nf (2) \nwhere If(x,y) is the inhibition corresponding to pixel (x,y) at \nthe current frame f, wI(i, j) are the local inhibition weights, \nand n defines the size of the inhibited area. In the \nexperiments, the local inhibition weights are set to 25% for \nthe inhibition from the four direct neighbouring cells and \n12.5% for the inhibition from the diagonal neighbouring \ncells; and n was set to 1. \nExcitation from the E cells and inhibition from the I cells \nare summed by the S cells in layer 3 of the network using the \nfollowing equation: \nIfff WyxIyxPyxS )),(),((),( \u2212=                    (3) \nwhere WI is the global inhibition weight. Excitation that \nexceeds a threshold value is passed to the LGMD cell: \n\uf8f3\uf8f2\n\uf8f1\n<\n\u2265=\nrf\nrff\nf TyxSif\nTyxSifyxS\nyxS\n),(0\n),(),(\n),(~                (4) \nwhere Tr is the threshold. \nThe excitation of the LGMD cell Uf, is the summation of \nall the excitation in the S cells as described by the following \nequation, \n\u2211\u2211\n= =\n=\nk\nx\nl\ny\nff yxSU\n1 1\n),(~(                                (5) \nThe excitation Uf is then performed a sigmoid \ntransformation, \n1)1(\n1 \u2212\u2212 \u2212+= cellf nUf eu                              (6) \nwhere ncell is the total number of the cells in the S layer. The \nsigmoid excitation uf varies within [0.5, 1] and forms the \ninput of the following motion controller. \nB. The binocular style vision to motor system \nOnce the cues have been extracted by the left and right \nLGMD in real time, it can be used to trigger a robot\u2019s \nbehaviour, such as escape behaviour (e.g., Blanchard et.al. \n2000, Yue and Rind 2005, 2006) via different types of motor \nneural networks (Yue et. al. 2007). In this study, we tend to \nlink the extracted visual cues directly to the motor system to \ngenerate navigation behavior. Since the panoramic vision \ncan only provide near range visual information, the \nnavigation behavior can only be affected by those objects \nclose to and seen by the robot.  \nTo react promptly to the surrounding in the wide range \nfield of view, the difference between the outputs of left and \nright LGMD is used to generate motor command via the \nmotor system. The difference between the left and right side \nLGMD reflects the changing local environment on both \nsides. These new generated motor commands are executed \nimmediately by the left and right wheels.  \nThe signal interpreter can be a simple proportional, PID, \nfuzzy system, neural networks or other adaptive controller \nwhich translates signal to motor control command frame by \nframe. A proportional controller is selected for its simplicity. \nIt translates output signal from the left and right LGMD to \nexecutable motor command for the robot.  \nThe outputs of the right and left LGMD are compared \nand written as,  \nr\nf\nl\nf uue \u2212=                                        (7) \nThe ideal situation is \u2013 left and right LGMD\u2019s outputs are \nequal, i.e. e is zero. In this situation, the robot moves \nforward\/backward in straight lines. We use e as the \ncompensation signal fed into the controller. \nThe output speed u at time t (or frame f) of the controller \ncan be, \n)(eKu cf =                                       (8) \nwhere u is the speed coefficient of the robot\u2019s left and right \nwheels, Kc is the proportional gain of the controller. \nThe motor commands from the interpreter sent to the left \nand right wheels are calculated and rounded as following \nbefore execution, \n)( 0 vuuroundu fll ++= \u03ba                      (9) \n)( 0 vuuroundu frr +\u2212= \u03ba                   (10) \nwhere u0 is a constant represents the robot\u2019s initial motion \npattern, v represents the contribution from other sources, \nround( ) means round the value to a nearest integer. Note, the \ntwo motor commands ul and ur are computed at each frame \nand fed to the motor system immediately without delay. \nAccording to the above equations, the robot moves in a \nspecific pattern defined by u0 if there is no or very little \ndifference between the left and right LGMD excitation level \nand no or very little other inputs. Otherwise, the motion \npattern should be the outcome of all the inputs.  \n \n20 40 60 80 100 120 140 160 180 200\n20\n40\n60\n80\n100\n120\n140\n \n(a) \n10 20 30 40 50 60 70 80 90 100\n10\n20\n30\n40\n10 20 30 40 50 60 70 80 90 100\n10\n20\n30\n40\n \n(b)                                                  (c) \nFigure 2.  Example images from the panoramic camera (a); the \ntransformed image fed to left side LGMD, the field of view is about 100 \ndegrees (b); the image fed to right side LGMD, the field of view is about \n100 degrees (c). There is 20 degrees overlapped area which covers the front \npart of the vision. \nC. Image transformation \nPanoramic images captured by the robot\u2019s CCD camera \n(Figure 2, top) is transformed into normal images (Figure 2 \nbottom) in real time before split and fed to the LGMDs. The \ntransformation is done using a program written in Matlab\u00ae \n(Mathworks, USA). The transformation involves in \nrearranging pixels to a different coordination system, i.e., \nfrom a Polar to a Cartesian coordination system. The grey \nscale at each pixel remains unchanged after transformation. \nThe size of the image fed to the left and right LGMDs are \nwith 100 pixels in horizontal and 42 in vertical, covering 100 \ndegree field of view in each side with 20 degrees overlapping \narea. The two wide range images fed to left\/right LGMD \nshould allow the system responding to the changing scenes \nin the front hemisphere. \nD. Implementation  \nThe LGMD neural networks and the motor command \ngenerator were written in Matlab\u00ae (MathWorks, USA) and \nrun on a PC. Inputs to the paired LGMD neural networks are \nfrom the panoramic CCD camera of a Khepera mobile robot. \nCommunication between the PC and the robot is via a serial \nport through a RS232 cable.  \nImages captured by the robot\u2019s CCD camera (example \nshown in Figure 2, top) are transformed and fed to the \nLGMDs and processed in real time. The size of the whole \nimage transformed from panoramic image is 360 pixels in \nhorizontal and 42 pixels in vertical. The frame rate is about 8 \nframes per second. As mentioned above, the size of the \nimages fed to the left or right LGMD is 100 pixels in \nhorizontal and 42 in vertical, covering about 180 degrees \nfield of view to the front (Figure 2 b and c). The overlapping \narea is about 20 degrees.  \nThe constant u0 is set to 6, which means the robot \nmoving speed is at 4.8cm\/s forward for each wheel. Default \nacceleration profile is used in the experiment. The global \ninhibition weight WI is set to 0.3 and threshold Tr is set to 15 \nin the experiments. The proportional gain Kc is set to 15 \nunless stated differently. Each experiment lasts for about 8 \nseconds. \nAll the experiments are conducted in a robotic laboratory. \nFurther background is not excluded. The robot may see other \nobjects further away. Illumination condition was not \ndeliberately controlled.  \n \nIII. EXPERIMENTS AND RESULTS \nTo check the feasibility of the system, we put the robot at \none end of an arena with two walls \u2013 one curved block wall \non one side and sparse blocks on other side. The robot is \nexpected to move for about 8 seconds and stop. We repeated \nthe experiments several times with the robot started from \ndifferent entrances of the arena; results are shown in Figure \n3. We found that the vision system works well providing \nrelevant information for the motor system to yield reasonable \nnavigation paths between the two columns of blocks, \nalthough the initial positions and orientations of the robot in \nthe above five trials were quite different, as indicated in \nFigure 3. \n \n \n  \n \nFigure 3.  The navigation paths of the robot. Two trials with initial \norientation towards one end of the arena surrounded by one curved blocks \nwall and several sparsely aligned blocks(left); three trials with initial \norientation towards the other end of the path in the same environment as in \nthe left side image (right). Images were from the first frame of two trials \nand trajectories were overlaid after being extracted from recorded video \nclips. \nExperiments were also carried out to investigate the \nimpact of individual object on the VISION system. Details \nof the two trails with or without a hand in the environment \nwere shown in Figure 4 and Figure 5. The robot conducted a \nquite different movement (as evidenced from its path) when \na hand was presented on its way (Figure 4). The difference of \nexcitation from left and right LGMD at the two trials were \nshown in Figure 5, together with the outputs from the \ncommand generator to the left and right wheels. It was found \nthat the robot changed its direction to its right side at the first \npart of the journey, and turn back to its left side at the last \nhalf of this journey when the hand was presented (Figure 5, \na, left) by adjusting the speed of its left and right wheels \n(Figure 5, a, right). For the trial without hand on its path, the \nspeed of the wheels performed differently, especially around \nthe critical time from frame 10 to 30 (Figure 5, b). \n \n \n \nFigure 4.  The movement trajectories of the robot in two different trials, \n(left) without object on its path; (right) with an object \u2013 a hand, on its way. \nImage was from the first frame of each trial and trajectories were overlaid \nafter being extracted from recorded video clips. Both of the trajectories \nwere overlaid to the first frame of images for easy comparison. \n \n0 10 20 30 40 50 60\n-0.5\n-0.4\n-0.3\n-0.2\n-0.1\n0\n0.1\n0.2\n0.3\nframe\ndi\nffe\nre\nnc\ne \nin\n e\nxc\nita\ntio\nn\n0 10 20 30 40 50 60\n0\n2\n4\n6\n8\n10\n12\nframe\nsp\nee\nd \nva\nlu\ne \nto\n e\nac\nh \nw\nhe\nel\nleft wheel\nright wheel\n \n(a) \n0 10 20 30 40 50 60\n-0.3\n-0.2\n-0.1\n0\n0.1\n0.2\n0.3\n0.4\nframe\ndi\nffe\nre\nnc\ne \nin\n e\nxc\nita\ntio\nn\n0 10 20 30 40 50 60\n0\n2\n4\n6\n8\n10\n12\nframe\nsp\nee\nd \nva\nlu\ne \nto\n e\nac\nh \nw\nhe\nel\nleft wheel\nright wheel\n \n(b) \nFigure 5.  Left column, the difference in excitation between the left and \nright LGMD; right column, the different output speed to left and right \nwheels. (a) The trial with the hand in the environment (Figure 4); (b) the \ntrial without the hand (Figure 4). \nIV. DISCUSSIONS \nSince the camera was mounted on the mobile platform, \nthe constant movements and frequent direction changes \nshould result in sharp changes between successive images. \nThese sharp changes may bring difficulties in image \ninterpretation or may further enhance unstable direction \nchanges. However, our experiments showed that it was not a \nproblem for motion sensitive neural network based vision \nsystem. With a simple motor system, the whole system \nworked well and stable. On the other hand, our experiments \nsuggests a similar way of using extracted visual cues to \ndirect fly course in a biological system may also be feasible. \nThe presented LGMD based vision system picked up \ndifferential images only and computed the excitation level to \nguide the robot\u2019s movements. It worked well regardless the \ncomplexity resulted from surrounding objects\u2019 size, shape, \ncolour or other physical characteristics.  The robustness of \nthese motion sensitive neural networks has been \ndemonstrated in different applications (Yue and Rind, 2006, \n2007). Therefore, the proposed system is ideal to be \nimplemented to mobile robotic systems to enable basic local \npath planning. \nFlies demonstrated motor planning ability mediated by \nvisual information in the escape responses (Card & \nDickinson 2008). Elementary motion detector (EMD), which \nwas proposed to explain a visual processing mechanism in \nfly (Hassenstein & Reichardt 1956), has also been applied to \nrobots for navigation (e.g., Francessini 2004 and Harrison \nand Koch 2000) or as speed odometer to guide fly robots\u2019 \n(Iida 2003). However, the EMD model can be limited to \ncertain range of speed. It is not clear if an EMD model still \nworks when the projected retinal image speed is faster than \npredicted. The presented LGMD based visual networks are \nable to respond to image changes in a whole visual field and \naccommodate large variety of retinal image speeds. With the \npresented system, faster optical flow results in stronger \nexcitation, and therefore, causes bigger turning speed and \nacute movement that enables the robot to run away from the \nfaster image change.  \nIn the future, we are going to combine a bio-inspired \ntarget tracking algorithm with the vision system to enable a \nmobile robot interact with dynamic world better. \n \nV. CONCLUSION \nIn the above sections, we presented a binocular style \nvision system that enables a mobile robot exploring local \npaths effectively using visual input only. The vision system \nconsists of a pair of LGMD visual neural networks and a \nsimple motor system. The vision system shares the \ndistinctive feature of motion sensitive neural networks \u2013 \nprocessing visual images effectively in real time regardless \nthe colour, shape, physical characteristics of surrounding \nobjects. Our experiments showed that the system worked \nwell in different scenarios. The system can be further \nimplemented to other mobile robot platforms for local path \nexploring, motion planning and interacting. \nACKNOWLEDGMENT \nThis research is partly supported by EU IST (FET) 2001-\n38097. \n \nREFERENCES \n \n[1] Blanchard, M. Rind, F.C. & Verschure, P.F.M.J. Collision avoidance \nusing a model of the locust LGMD neuron. Robotics and Automonous \nSystems, vol.30, 17-38. 2000. \n[2] Buxton, H. Learning and understanding dynamic scene activities: a \nreview. Image and Vision Computing, vol.21, 125-136, 2003. \n[3] Card, G. & Dickinson, M.H. Visually mediated motor planning in the \nescape response of drosophila. Current Biology, vol.18(17), 1-8, \n2008. \n[4] DeSouza, G.N. & Kak, A.C. Vision for mobile robot navigation: a \nsurvey. IEEE Transactions on Pattern Analysis and Machine \nIntelligence, vol.24 (2), 237-67, 2002. \n[5] http:\/\/www.darpa.mil\/grandchallenge\/index.asp  \n[6] Fiala, M. & Basu, A. Robot navigation using panoramic tracking. \nPattern Recognition, vol.37, 2195-2215, 2004. \n[7] Franceschini, N. Visual guidance based on optic flow: a biorobotic \napproach. Journal of Physiology Paris, vol.98, pp281-292, 2004. \n[8] Gandhi, T. and Trivedi, M.M. Redestrian protection systems: issues, \nsurvey, and challenges. IEEE Transactions on Intelligent Transport \nSystems, vol.8, no.3, 413-430, 2007. \n[9] Harrison, R.R., & Koch, C. A silicon implementation of the fly's \noptomotor control system. Neural Computation, vol.12, 2291-2304, \n2000. \n[10] Hassenstein, B., & Reichardt, W. Systemtheorische analyse der zeit-, \nreihenfolgen- und vorzeichenauswertung bei der \nbewegungsperzeption des ruesselkaefers chlorophanus. Zeitschrift \nfuer Naturforschung, vol.11b, 513-524.  \n[11] Huber, S.A. Franz M.O. & Buelthoff, H.H. On robots and flies: \nmodelling the visual orientating behaviour of flies. Robotics and \nAutonomous Systems, vol.29, 227-242, 1999.  \n[12] Iida, F.  Biologically Inspired Visual Odometer for Navigation of a \nFlying Robot. Robotics and Autonomous Systems, vol.44\/3-4, 201-\n208, 2003. \n[13] Indiveri, G. & Douglas, R. Neuromorphic vision sensors. Science, \nvol.288, 1189-1190, 2000. \n[14] Manduchi, R. Castano, A. Talukder A. & Matthies, L. Obstacle \ndetection and terrain classification for autonomous off-road \nnavigation. Autonomous Robots, vol.18, 81-102, 2005. \n[15] Nishio, K., Yonezu, H., Kariyawasam, A.B., Yoshikawa, Y. Sawa, S. \n& Furukawa, Y., Anology integrated circuit for motion detection \nagainst moving background based on the insect visual system. Optical \nReview, vol.11, 1, 24-33, 2004. \n[16] Olson,C.F., Matthies, L.H., Schoppers, M. and Maimone, M.W. \nRover navigation using stereo ego-motion. Robotics and Autonomous \nSystems, vol.43, 215-229, 2003. \n[17] O\u2019Shea, M. Rowell, C.H.F., Williams, J.L.D.  The anatomy of a \nlocust visual interneurone: The descending contralateral movement \ndetector.  Journal of Exp. Biology, vol.60, 1\u201312, 1974. \n[18] Rind, F.C. & Bramwell, D.I.  Neural network based on the input \norganization of an identified neuron signaling impending collision.  \nJournal of Neurophysiology, vol.75, 967\u2013 985, 1996. \n[19] Rind, F.C. Simmons, P.J. Orthopteran DCMD neuron: A reevaluation \nof responses to moving objects. I. Selective responses to approaching \nobjects.  Journal of Neurophysiology, vol.68, 1654\u20131666, 1992. \n[20] Rind, F.C. & Simmons, P.J.  Seeing what is coming: Building \ncollision sensitive neurons. Trends in Neurosciences, vol.22, 215-220, \n1999. \n[21] Rind, F.C. Motion detectors in the locust visual system: from biology \nto robot sensors. Microscopy Research and Technique, vol.56, 256-\n269, 2002. \n[22] Rind, F.C. Santer, R.D.J., Blanchard, M. & Verschure, P.F.M.J. \nLocust\u2019s looming detectors for robot sensors. Sensors and Sensing in \nBiology and Engineering, FG Barth, JAC Humphrey, and TW \nSecomb (Eds.), Spinger-Verlag, Wien, New York, 2003. \n[23] Rind, F.C. Bioinspired sensors: from insect eyes to robot vision. \nFrontiers in Neuroscience: Methods in Insect Sensory Neuroscience, \nChristensen T.A. (Eds.), CRC Press Boca Raton, London, New York, \n2005 \n[24] Rowell, C.H.F. O\u2019Shea, M.  Williams, J.L.  The neuronal basis of a \nsensory analyser, the acridid movement detector system .IV. The \npreference for small field stimuli. J. Experimental Biology, vol.68, \n157-185, 1977. \n[25] Santer,R.D., Simmons,P.J. & Rind, F.C. Gliding behaviour elicited by \nlateral looming stimuli in flying locusts. Journal of Comparative \nPhysiology, vol.191, 61-73, 2005. \n[26] Schlotterer, G.R. Response of the locust descending contralateral \nmovement detector neuron to rapidly approaching and withdrawing \nvisual stimuli. Canadian Journal of Zoology, vol.55, 1372\u20131376, \n1977. \n[27] Simmons, P.J., Rind, F.C. Orthopteran DCMD neuron: A \nreevaluation of responses to moving objects. II. Critical cues for \ndetecting approaching objects. Journal of Neurophysiology, vol.68, \n1667\u20131682, 1992. \n[28] Stafford, R., Santer R.D. & Rind, F.C. A bio-inspired visual collision \ndetection mechanism for cars: combining insect inspired neurons to \ncreate a robust system. BioSystems, 87, 162-169, 2007. \n[29] Webb, B. & Reeve, R. Reafferent or redundant: integration of \nphonotaxis and optomotor behaviour in crickets and robots. Adaptive \nbehaviour, vol.11 (3), 137-158. 2003. \n[30] Wichert,G. Can robots learn to see?. Control Engineering Practice, \nvol.7, pp.783-795, 1999. \n[31] Yue, S. & Rind, F.C.  A Collision detection system for a mobile robot \ninspired by locust visual system. IEEE Int. Conf. on Robotics and \nAutomation, Spain, Barcelona, Apr.18-21, 2005, 3843-3848, 2005.  \n[32] Yue, S. & Rind, F.C. Collision detection in complex dynamic scenes \nusing a LGMD based visual neural network with feature \nenhancement. IEEE Transactions on Neural Networks, May, vol.17 \n(3), 705-716, 2006. \n[33] Yue, S., Rind, F.C. Keil, M.S., Cuadri, J. & Stafford, R. A bio-\ninspired visual collision detection mechanism for cars: optimisation \nof a model of a locust neuron to a novel environment.  \nNeurocomputing, vol.69 (13-15), 1591-1598, 2006. \n[34] Yue, S. & Rind, F.C. Visual motion pattern extraction and fusion for \ncollision detection in complex dynamic scenes. Computer Vision and \nImage Understanding, vol.104 (1), 48-60, 2006b. \n[35] Yue, S. & Rind, F.C. A synthetic vision system using directionally \nselective motion detectors to recognize collision. Artificial Life, \nvol.13 (2), 93-122, 2007. \n[36] Yue, S., Santer, R., Yamawaki, Y., & Rind F.C., Locust-like \nemergent escape behaviour of mobile robots emerged with bilateral \npair of visual LGMD\/DCMD, Autonomous Robots, 2007 (under \nreview). \n[37] Yue. S. & Rind, F.C. Competence comparison of collision sensitive \nneural systems in dynamic environments. Artificial Life, 2008a (under \nreview). \n[38] Yue, S. & Rind, F.C. Exploring organization of direction selective \nneural networks for road collision prediction. IEEE Trans. Intelligent \nTransport Systems, 2008b (under review).  \n \n"}