{"doi":"10.1080\/09687760108656774","coreId":"14099","oai":"oai:generic.eprints.org:753\/core5","identifiers":["oai:generic.eprints.org:753\/core5","10.1080\/09687760108656774"],"title":"A toolkit for supporting evaluation","authors":["Conole, Gr\u00e1inne","Crew, Ed","Oliver, Martin","Harvey, Jen"],"enrichments":{"references":[{"id":1042046,"title":"9 Number I HEFCE","authors":[],"date":"1996","doi":null,"raw":null,"cites":null},{"id":194843,"title":"A pedagogical framework for embedding C and IT into the curriculum',","authors":[],"date":"1998","doi":"10.1080\/0968776980060202","raw":"Conole, G. and Oliver, M. (1998), 'A pedagogical framework for embedding C and IT into the curriculum', ALT-J, 6 (2), 4-16.","cites":null},{"id":1042050,"title":"Assessing and enhancing quality using toolkits',","authors":[],"date":"2000","doi":"10.1108\/09684880010312677","raw":"Oliver, M. and Conole, G. (2000), 'Assessing and enhancing quality using toolkits', Journal of Quality Assurance  in Education,  8 (1), 32-7. Oliver,  M.,  MacBean,  J.,  Conole, G.  and Harvey, J.  (forthcoming), 'Using a  toolkit to support the evaluation of learning technology', Journal of Computer Assisted Learning.","cites":null},{"id":1042048,"title":"Evaluating Communication and Information Technologies: a toolkit for practitioners',","authors":[],"date":"1998","doi":null,"raw":"Oliver,  M.  and  Conole,  G.  (1998),  'Evaluating  Communication  and  Information Technologies: a toolkit for practitioners', Active Learning,  8, 3-8.","cites":null},{"id":1876869,"title":"Evaluation of the teaching and learning technology programme',","authors":[],"date":"1996","doi":null,"raw":"48 AI~-]  Volume  9 Number  I HEFCE  (1996),  'Evaluation  of  the  teaching  and  learning  technology  programme', Coopers and Lybrand report, HEFCE M21196, h ttp.\/\/www, niss. ac. ukleducation\/hefcelpub96\/m21_96,  h tml.","cites":null},{"id":1876875,"title":"From theory to practice: a model and project structure for toolkit development',","authors":[],"date":"1999","doi":null,"raw":"Oliver, M. and Conole, G. (1999a), 'From theory to practice: a model and project structure for toolkit development', BP ELT Report No. 12, University of North London.","cites":null},{"id":1042049,"title":"From theory to practice: amodel and project structure for toolkit development',","authors":[],"date":"1999","doi":null,"raw":null,"cites":null},{"id":1042047,"title":"Teaching and Learning Technology Programme Phase 3: invitation to bid',","authors":[],"date":"1997","doi":null,"raw":"HEFCE (1997a),  'Teaching and Learning Technology Programme Phase 3: invitation to bid', HEFCE,  14\/97, TLTP, Bristol. HEFCE  (1997b),  'Information  technology  assisted  teaching  and  learning  in  higher education' (ITATL report), Bristol: HEFCE Research series.","cites":null},{"id":1042051,"title":"The development of a generic framework for accrediting professional development in C&IT',","authors":[],"date":"1999","doi":null,"raw":"Phelps, J.,  Oliver,  M.,  Bailey,  E  and Jenkins, A.  (1999),  'The development of a  generic framework for accrediting professional development in C&IT', EFFECTS report No. 2, University of North London.","cites":null},{"id":194845,"title":"The LTDI Evaluation Cookbook, Learning Technology Dissemination Initiative.","authors":[],"date":"1998","doi":null,"raw":"Harvey, J.  (1998), The LTDI Evaluation  Cookbook,  Learning Technology Dissemination Initiative.","cites":null},{"id":194844,"title":"Toolkits as an approach to evaluating and using learning materials',","authors":[],"date":"2000","doi":"10.1109\/iwalt.2000.890582","raw":"Conole, G., Oliver, M. and Harvey, J. (2000), 'Toolkits as an approach to evaluating and using learning materials', ASCILITE 2000, Coifs Harbour, NSW, Australia.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2001","abstract":"This paper describes the development of a Web-based evaluation toolkit that supports practitioners in the effective and appropriate use and evaluation of learning materials. An outline of the basic toolkit architecture and design will be described, along with extracts of some of the formative feedback received on the use and value of the toolkit during a range of user trials which were carried out as part of the project development work. The paper will conclude with potential applications for such toolkits and recommendations for future developmen","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/14099.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/753\/1\/ALT_J%2DVol09_No1_2001_A_toolkit_for_supporting_evalu.pdf","pdfHashValue":"346e1662686b9172ae931c814e306d97dca7a494","publisher":"Taylor and Francis Ltd","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:753<\/identifier><datestamp>\n      2011-04-04T08:59:50Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/753\/<\/dc:relation><dc:title>\n        A toolkit for supporting evaluation<\/dc:title><dc:creator>\n        Conole, Gr\u00e1inne<\/dc:creator><dc:creator>\n        Crew, Ed<\/dc:creator><dc:creator>\n        Oliver, Martin<\/dc:creator><dc:creator>\n        Harvey, Jen<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        This paper describes the development of a Web-based evaluation toolkit that supports practitioners in the effective and appropriate use and evaluation of learning materials. An outline of the basic toolkit architecture and design will be described, along with extracts of some of the formative feedback received on the use and value of the toolkit during a range of user trials which were carried out as part of the project development work. The paper will conclude with potential applications for such toolkits and recommendations for future development<\/dc:description><dc:publisher>\n        Taylor and Francis Ltd<\/dc:publisher><dc:date>\n        2001<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/753\/1\/ALT_J-Vol09_No1_2001_A_toolkit_for_supporting_evalu.pdf<\/dc:identifier><dc:identifier>\n          Conole, Gr\u00e1inne and Crew, Ed and Oliver, Martin and Harvey, Jen  (2001) A toolkit for supporting evaluation.  Association for Learning Technology Journal, 9 (1).  pp. 38-49.  ISSN 0968-7769 (print)\/1741-1629 (online)     <\/dc:identifier><dc:relation>\n        10.1080\/09687760108656774<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/753\/","10.1080\/09687760108656774"],"year":2001,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"A toolkit for supporting evaluation \nG~inne Conole,* Ed Crew,* Martin Oliver** and Jen Harvey*** \n*University of Bristol, **University College London \n***Dublin Institute of Technology \nemail: g.conole@bristol.ac.uk \nThis paper describes the development of a Web-based evaluation toolkit that supports \npractitioners in the effective and appropriate use and evaluation of learning materials. \nAn outline of the basic toolkit architecture and design will be described, along with \nextracts of some of the formative feedback received on the use and value of the toolkit \nduring a range of user trials which were carried out as part of the project development \nwork. The paper will conclude with potential applications for such toolkits and \nrecommendations for future development. \nIntroduction \nDespite the current consensus that dapting and reusing existing learning resources, \nincluding Information and Communication Technology (ICT), is a good thing, examples \nof this practice are few and far between. This can be traced to a number of factors. In \nparticular, the 'not invented here' syndrome (HEFCE, 1996) is no doubt still present. \nHowever, more important is the issue of the time and skills required to retrieve, evaluate \nand then adapt materials. This is compounded by the fact that identifying suitable \nresources in the first place can be complex (although the growth of subject-specific \ninformation gateways, portals and guidelines to resources will go some way towards \nalleviating this problem). Some of the barriers to uptake that have been identified include: \n\u2022 the quantity of material, of varying quality, now available through the Web; \n\u2022 the problem of retrieving appropriate materials; \n\u2022 the difficulty of adapting other peoples' materials; \n\u2022 the issue of ownership and copyright; \n\u2022 integration of new materials, including issues of style, definition and level into existing \ncourses; \n38 \nAIt-J Volume 9 Number I \n\u2022 staff having the appropriate educational nd\/or technical skills required to evaluate, \nadapt and integrate materials; \n\u2022 concern about he currency of materials, particularly accuracy and whether it is up to \ndate. \nFurthermore, few academic staff have had the opportunity to develop the prerequisite \nexpertise to design and implement an effective strategy for acquisition, use and evaluation \nof either new materials or innovative methods of delivery. In order to provide these \npractitioners with support and encouragement, easy-to-use guidelines and resources are \nrequired. Not only must these be based on sound, tested pedagogic theories, but first and \nforemost they must be practical for academics using (or evaluating the use of) learning \ntechnologies. This paper will describe an evaluation toolkit that addresses these concerns \nby providing simple step-by-step guidance on the development of effective valuation \nstrategies for learning and teaching. \nBackground: providing support and guidance for decision-making \nResources for supporting decision-making range from highly restrictive 'templates' or \n'wizards', which provide step-by-step guidance but little possibility of user-adaptation, \nthrough to 'theoretical frameworks', which provide a context and scope for the work but \nleave the user to devise their own strategy for implementation. Between these xtremes lies \na range of resources, including checklists, guidelines and step-by-step tutorials. This \nproject set out to develop a type of resource which lies approximately half way along this \ncontinuum, referred to here as a 'toolkit'. \nToolkits include an expert model of a system derived from recognized theory and best \npractice. This is used to provide a structured process, supporting the implementation f \nperformance monitoring systems. Furthermore, by providing a common conceptual \nframework (particularly one in which multiple interpretations of terms can be negotiated \nand agreed), it becomes possible to define and establish standards. \nA more detailed escription and definition of frameworks, toolkits and wizards, supported \nby illustrative xamples, is provided elsewhere (Conole, Oliver, and Harvey, 2000). \nAn outline of the toolkit \nThe 'Evaluation Toolkit for Practitioners' project is funded by JISC as part of the Committee \nfor Awareness, Liaison and Training's remit. It has developed and tested a Web-based toolkit \nthat helps practitioners, irrespective of their current degree of expertise, to evaluate their \nselection and use of learning materials. It provides a structured resource that can be used to \nplan, scope and cost an evaluation. By providing progressively more detailed information on \nparticular topics, the user can follow up relevant issues when and if this is required. Secondly, \nby providing a simple, logically 'organized structure the toolkit reduces the time required to \nplan work of this type. The aim is that the toolkit will be used iteratively, with progressively \nmore detailed analysis occurring once initial feedback and information has been received. \nThe development of he toolkit was built on a set of underlying assumptions. It should: \n\u2022 be easy to use for practitioners and provide demonstrable b nefit; \n39 \nGr6inne Conole et al A toolkit for supporting evaluation \n\u2022 integrate guidance, but not be prescriptive; \n\u2022 be adaptable and easy to customize to the local context; \n\u2022 enable access to a comprehensive r source of relevant material. \nThe toolkit in its current format is designed to be introduced initially as part of a \nstructured face-to-face workshop session. Users can register for a username and password \nand are then given access to their own working area of the toolkit. They can then build up \na portfolio of different evaluations to suit their needs. Each user also has access to a set of \npredefined case study evaluations that they can use for guidance or as a model that can be \ncustomized to create their own evaluation plan. \nThe toolkit is run online from a server at the University of Bristol (http:\/\/www.ltss. \nbris.ac.ukljcalt\/). Users have access to their own private workspace within the toolkit, and \ncan store any number of evaluations within this area. Summaries of each evaluation can be \nprinted out in a number of formats, either by evaluation topic or as a summary of the \nwhole evaluation. The toolkit is an SQL-database-driven Web application, which provides \na tailored interface to a large resource of content and external links. The basic framework \nconsists of over 200 files, which include scripts to generate appropriate HTML pages of \ncontent and resources. It is designed so that output depends on user input with results \nbeing displayed 'on the fly' in a user-orientated manner. The resource includes embedded \nlinks to a large range of relevant external resources as well as a dynamic database of \ntoolkit resources. The database of resources can be adapted and is designed to be \nexpandable to include new resources or information as required. In addition, the toolkit is \ndesigned so that users can adapt and define personal requirements, categories or content. \nThe resource also includes a feature that allows users to share content through \ncontribution of evaluation case studies to a general pool of resources. \nToolkit  sect ions \nThe toolkit consists of three components: i) Evaluation Planner, ii) Evaluation Advisor \nand iii) Evaluation Presenter. There are also links to three databases on data capture \nmethods, data analysis and presentation techniques. A key feature of the resource is that it \nemploys a filtration process to recommend relevant materials to the user at appropriate \nstages of the evaluation. Each section also contains substantial l nks to related material, \nsuch as The LTDI Evaluation Cookbook (Harvey, 1998) and other Web resources. \nAlthough the Toolkit is presented in three sequenced sections, the modular format of the \nstructure is designed to enable use of the different components when and if required uring \nan ongoing evaluation study. This means that a user can access their personal workspace \nand retrieve, amend and develop their evaluation as they are carrying out the work rather \nthan having to complete all the sections before undertaking the work. \nEvaluation Planner \nThe first section of the toolkit helps users to define the scope of their evaluation and is \ndivided into seven main stages. The content is linked with a series of questions, guidelines \nand exercises upported by user input fields, which guide users in planning their evaluation \nstrategy and clarifying their reasons for particular selections. \nThe section also supports users in the identification of the intended audience for the \n40 \nAIt-j Volume 9 Number I \nevaluation (stakeholder analysis) and the definition of the core evaluation question(s). It\nfocuses on defining the complexity or scale of the proposed evaluation, giving particular \nattention to associated time and resource implications (both financial and human). Users \nare expected to give some thought at this stage to defining evaluation tasks, roles and \nresponsibilities. The output from Planner includes an evaluation strategy and \nimplementation guide. This provides the starting-point for the communication plan in the \nEvaluation Presenter section of the toolkit. \nAfter the introductory 'About Planner' stage, the users are asked to specify the focus of the \nevaluation in the 'What are you evaluating' section. The user is given a predefined list of \npotential evaluation objects to choose from (Figure 1). They are also able to add or edit \ntheir own entries. \nThe 16 evaluation objects you have to choose from \nCourse materials Could be self-contained static resources \nEvaluation might focus on eitt~er design or use \nCourse programmes Could be a complete programme of study \nMight be at module or course level \nCould involve evaluation of all aspects of the learning experience \nNight be a longitudinal study \nExternal resources Could be to assess whether or not they are appropriate to be included in a course \nCould include some evaluation of the associated costs and benefits \nExternally-funded This could be either a developmental or research project \nproject \nThe evaluation might be an assessment of the success of the project or might be more \nformative, with the intention being to improve the project through the valuation results. \nNew innovation in This might be where you have replaced or supplemented your teaching with some use of ICT or \nlearning and teaching migrd be where you have enhanced the course by moving from individual to group-based \nIo~rr, ihn \nFigure I: Suggested evaluation objects, \nThis process of guiding the user through a set of choices, whilst maintaining user freedom \nby allowing them to customize or add to the entries, is an integral feature throughout the \ntoolkit. In a similar fashion, the user is guided through the process of thinking about their \nreasons for carrying out the evaluation (and any associated resource implications) and are \nasked to describe the context for the study. These sections consist of a mixture of user- \ninput boxes and choice lists (Figure 2). \nThe user-input boxes are associated with questions designed to prompt reflection on the \nevaluation design process. At any stage during the evaluation, the user can return to earlier \nsteps and edit entries, to support this reflective approach and encourage iterative \ndevelopment of the evaluation plan. \n41 \nGrdinne Conole et al A toolkit for supporting evaluation \nI Reeour~n \n2 S\u00a2lle \n3 T~dt~m~ \nI~ ~sre 8 group ~\u00a2~ame~s or ~r~ \n85~C~ ~ b'~is? IHE\" ~ l J~ or nO e~r~e~q~ of \nIt lhere 8re lee~r~e~ or users ess~iet~d. I~ I~.  T~ ~aNe~en Nil be \n'~  ~ne m+ud\u00a5, g~v~ d~i~ o~the n~er  \nand 1~r loveFo~\u00a2k~ro~d ec~per~e~e. \nV~no will be d~ng I~ eqalu~orff \nV '~ res~rce~ ~ you got a~'aiI~ie? \nt~ ~r~ a bud~to  ~or~e ~valua[tor~ \na~Kdes? \nv~t  you ere ~aluatI~, for ~xamp~  i~ ineeds to be ~ ished.  but ~ s \nIf ~r~le~r~, bo~ r~ar~ l~amer ho~r~ \u00a2lo~s i~ \nr~ne~i~ \nIs ~ set[ \u00a2or~im~I or ~r~egr~e~ o~.~er \nI~arning ~v~Iies? \n~en are yo~ p~r~iro {e ~ndert~ke ~his \n~rk  and ~en d~s it need to be \n~ompt~ed? \nFigure 2: Examples of questions prompting reflection on the context of the study. \nLecturer  - ~ ~mesa~mg \nI~0 releValnce W Relevanca to Lecturer - TimesaVing \nx'- \nYou may want to use one of the comparisons below: j j~  \n[ \nFigure 3: Suggestions of di~erent ways to rephrase the evaluation question. \n42 \nAIt-J Volume 9 Number I \nParticularly important in this process are the section summaries and the overall final \nevaluation plan. Once the user has clarified the nature of the evaluation and the reasons for \ndoing it, they are asked to look at the stakeholders involved in the process. As before, the \nuser is given the option of a predefined set of potential stakeholders and of the concerns \nthat each of these might wish to have addressed. Users are encouraged tolimit the number \nof stakeholders and concerns to a maximum of three to ensure that a management \nevaluation plan can be devised. \nThe next step guides the user through the derivation of evaluation questions to meet he \nindividual stakeholder concerns. Previous research as shown that practitioners find this \nstep surprisingly difficult (Oliver and Conole, 2000) and that a structured approach to \nderiving the question can support hem in this process. The user works through a number \nof alternative ways of thinking about each stakeholder concern, guided by a set of \nquestion types and potential question stems (Figure 3). \nHaving worked through this stage the user defines the final questions for each of the \nconcerns by combination of appropriate parts of the preparatory questions. The screen is \ndesigned to allow the user to iterate between questions for the different stakeholder \nconcerns to encourage a holistic approach to the evaluation. The final step in Evaluation \nPlanner gives a summary of the user input; presenting an overview of this information \nallows the user to consider the design at a strategic level, giving them early feedback on \nwhether or not the study will meet heir needs. Initial formative valuation of the toolkit \nsuggests hat this summary step does indeed encourage r flective practice (Oliver, McBean, \nConole and Harvey, forthcoming). \nEvaluation Advisor \nThis section covers the planning of the implementation aspects of the evaluation. It uses \nthe output from Evaluation Planner as a starting-point, guiding the user through the \nStakeholders: Lecturer ledurer \nConcern Timesaving Research \nStudent \nUsability \nQuestion: , L \n\u2022 7 \nlsorn'e ~udent_~_l b \u00b0ne t IAll stu.ent~ \nI '~.!~ o. _~ I~s.~.0~.t~o,!~ I \nI System log data, Cognitive walk \nConcept maps, Focus through\/think aloud protocols, Confidence logs, In-course \nGroups, Unstructured Designing experiments & pre- and experiment, Performance test \nI interviews posttesting, Resource Reflective logs\/student \nQuestionnaires, Split screen video, diaries \nTrials \n= \nFigure 4: Examples of the questions used to elicit information From users. \n43 \nGrdinne Conole t al A toolkit for supporting evaluation \nchoices of data collection and analysis required in the evaluation process. It links closely to \na knowledge base of relevant external material, such as The LTDI Evaluation Cookbook \nand other esources. \nHaving devised a clear idea of the strategic direction of their evaluation, the Evaluation \nAdvisor section guides the user through the process of identifying appropriate data \ncollection and analysis for the evaluation. After an initial introduction to the section \n(About the Advisor), the user is asked to provide information about the evaluation \nquestions relevant to the stakeholders identified in the previous ection, which they now \nwish to address (Figure 4). \nFrom this information, appropriate methods for capturing data are suggested by the \nToolkit as outlined below (Figure 5). \nDefinition \nAn unobstrusive mettled for collecting data on student use of sol,  rare by collecting and \nrecording student activity- such as key strokes, menu selection, and mouse clicks \nAvisual representation of associated ideas or concepts showing the relationships or links \nbetween them. \nConfidence logs involve a series of statements about aspects of a course; students are \nexpected to say how confident he'~ are (using a Ukert scale) about each of these. The logs are \nadministered as a suwe~, and may be used once (in order to get a \"snapshot\" of class \nconfidence about s particular aspect of the course) or repeatedly (to get a longitudinal profile of \nhow confidence develops overtime). Confidence logs were used bythe TILT project as part era \nsuite of methods designed to assess howwe[I learning technologywas integrated into \nteaching and learning. However, the dea of assessing confidence using Likert scales as a part \nof suweys has been used for manyyears. \nFigure 5: An example of the range of suggestions made by the toolkJt. \nBehind this process the programme has mapped potential answers to these questions to the \ndifferent types of data capture method. In this way it is possible to provide the user with a \nfiltered set of suggested methods. However, this filtration does not preclude the users from \nexploring the wider range of data capture methods. The filtration is designed to provide a \nlayered view of the details about data capture. There is a substantial body of associated \ninformation available about each of the data capture methods and this layered approach is\ndesigned to help guide the user through this information in a digestible format. For \nexample, some users of the programme will already have a very clear idea of what focus \ngroups can offer as a data capture method, while others will have little background \nknowledge. \nOnce confident of the different data capture methods and their uses, users choose which of \nthe methods will be appropriate for their study. Their choice is then used as a starting-point \nfor selecting techniques to analyse any data collected. The next stage again begins with a \nset of questions designed to explore the purpose of the data collection. In a similar fashion \ndata analytical methods are mapped to the question responses and the users can either \nchoose to accept he returned methods r explore other possibilities in more detail. At the \n44 \nAIt-j Volume 9 Number I \nend of the section there is a summary of the user's input, providing an overview of this \ninformation. As previously outlined, this aims to encourage the user to consider the \nimplementation f the evaluation holistically and then reflect on the design. \nEvaluation Presenter \nThis third section looks at the dissemination of the findings of the evaluation to the \nstakeholders identified in Evaluation Planner. It argets the report format to the audience \nidentified by the evaluation planner and then provides tructured templates that take the \noutput from Evaluation Advisor and present it according to the most appropriate media \ntypes (Report format, Web site, PowerPoint presentation, peer p suasion\/oral communica- \ntion and research-paper fo mat). \nBefore considering the presentation techniques users work through a 'closing the loop' \nstep. This stage prompts them to build in reflective checks to consider whether their results \nare valid as well as draw up an evaluation plan. In particular users are asked to think about \ntheir evaluation presentation plans in light of the original stakeholders identified in the \nPlanner section and their associated concerns. Relevant information from the previous two \nsections is then pulled into this section. This stage is important as the toolkit attempts to \nensure that users have focused their presentation outputs to reflect the interests of their \npotential audience. As with the data capture and analysis section, the potential set of \npresentation tools is prefaced by a set of questions designed to fit the tools to the user \nrequirements. For example, if the requirement is to disseminate the findings of the study \nquickly, an email or Web site is more likely to be returned as a suggested presentation \nmethod than a detailed peer-reviewed journal article. Subsequent questions help the users \nto draw up a communication plan, considering issues like the message to be communicated, \nopportunities for dissemination and the resources required. \nThe final part of Evaluation Planner gives a full summary of all the information given and \ndecisions that the user has taken within all three sections of the toolkit. This acts as an \nEvaluation Plan for the study. Again this is designed to encourage reflection on the whole \nprocess and acts as a valuable one-to-two page summary plan of the process. This can also \nact as a working document that will allow users to manage the future implementation of \nthe study. The formative evaluation shows that this step is considered a valuable 'reward' \nby the users after the considerable effort involved in designing the study working through \nthe toolkit. \nFurther elements of the toolkit \nThe toolkit also contains background etails on the resource itself, the overall architecture, \nand reasons why users might find the toolkit useful. All the associated resources used \nthroughout the toolkit are grouped in the resource section and can be searched in a number \nof ways. Users can also add their own resources. \nOne of the most overwhelming aspects of the feedback from the formative valuation was \nthat users would welcome a quick route through the toolkit. This is possible via the \nEvaluation Finder section. Users who have completed an evaluation can choose to make \nthis publicly available for other users to copy and customize. This means that users can \npick an off-the-shelf Web site design evaluation and with minimum effort adapt it to their \nown requirements and local context. It is hoped that a rich database of these evaluation \n45 \nGr6inne Conole et dl A toolkit for supporting evaluation \ncase studies will be developed. At a more local level, as each user is assigned their own \nprivate area of the toolkit they can build up a set of their own evaluation case studies. \nThese can be developed and adapted over time or might be used within a local setting with, \nfor example, a set of colleagues across a department. \nFormative evaluation of the toolkit \nThe evaluation of the toolkit was designed toprovide feedback on the usability of the toolkit \nand to assess its potential impact on practice. A more detailed outline of the evaluation is\ndescribed elsewhere (Oliver, MacBean, Conole and Harvey, 2000) and only illustrative \nextracts will be described here. The methodology included observational studies and a \nfollow-up workshop of new users. In the initial stage usability trials in the form of cognitive \nwalk-through were carried out at University of Bristol, University College London and the \nDublin Institute of Technology. These were used to improve the design and layout of the \ntoolkit and help the content developers to identify areas that required further work and \nimprovement. Following a think-aloud protocol thirteen participants across the three sites \nparticipated in this part of the study. In each case a researcher observed and recorded the \nuser's activities. At the end of the session users also provided feedback on the overall use and \nvalue of the toolkit, good and bad aspects, and whether or not the format of the toolkit had \ninfluenced the way in which they had approached the evaluation planning process. \nFeedback from these trials was used to improve the toolkit and in particular the overall \nstructure and navigation of the resource. Once an updated version of the toolkit had been \nproduced, taking into account all the feedback from the initial usability trails, the second \nphase of the evaluation was carried out. This took the form of a workshop of 'critical \nfriends' comprising a range of potential users (lecturers, managers, researchers, taff in \nuniversity support services or national centres). During this one-day workshop, the \nparticipants worked through the toolkit section by section. They were asked to keep a \nrecord of their activities and in particular to jot down any significant findings or errors \nencountered. At appropriate points the group were drawn together to discuss progress and \nin particular good and bad features of the toolkit. The workshop concluded with a general \nsumming up of their experiences with using the toolkit and its potential value and use. \nSummary of the evaluation studies \nMuch early feedback on the toolkit concerned its usability. The navigation, for example, \ncaused problems for some users. \nIt's not clear to me at what point I go on, so I'm taking the view that there's enough \nintelligence in here [the toolkit] to stop me if there's omething missing. (Workshop) \nThe 'change selection' buttons at each stage are not clear. (Study U1) \n(Note that for this and subsequent quotes, the coding refers to the university where the \nstudy took place, U for London, B for Bristol and D for Dublin, and an assigned \nnumber for each participant). \nSimilarly, some users found some of the terminology confusing. \n'Evaluation Object' could be replaced by 'Focus of the Evaluation'. (Study D5) \nScope? What do you mean by scope? (Workshop) \n46 \nAlto] Volume 9 Number I \nIt was evident hat many of the users were frustrated by the early navigational structure \nand layout, as this was fundamentally impeding their progress and hampering their \nunderstanding of the issues and concepts being described in the program, as the followirLg \nquote illustrates: 'At the core is probably a sound idea but it is so obscured by the language \nand interface' (Study D3). The toolkit was iterafively refined in light of such comments, \nwith the result that the usability of the toolkit was markedly improved by the end of the \nproject. \nFeedback on the content and potential usefulness of the toolkit was generally positive. \nUsers clearly gained benefit from working through it and recognized that it was a rich \nresource of material. However, there was some concern that the toolkit was deceptive in \nterms of its size and ease of use. In particular, it was recommended that users should be \nmade more clearly aware of the time required to complete ach section (approximately one \nhour) and the level of detail and concentration required if the user was to gain optimal \nvalue from using the resource. \nI really liked [it] making me think aboUt the purposes of evaluation. I've completely \nchanged my view of the evaluation by working through this. (Study B3) \nI haven't had to produce an evaluation plan ever before - so in that sense it was \nextremely helpful as it guided me through the process, explained some background i eas \nand suggested other sources of help. (Study U2) \nFurther feedback was derived from the 'critical friends' workshop and the final steering \ngroup meeting. Some users expressed concern that, to be utilized effectively, the user really \nneeded to spend a good few hours working through the toolkit and that for smaller \nevaluations this was impractical. In response to this, simpler 'quick' routes through the \ntoolkit, along with templates for standard evaluation processes, are planned. However, \noverall, feedback has been positive and the general consensus was that the toolkit had the \npotential to be a very valuable resource. The follow-on workshop echoed many of the \ncomments made in the observational studies. It was encouraging to note that the \nimprovements made to the toolkit, as the result of initial feedback, were noticeable in the \nway that the workshop users worked through the resource much more asily, although \nfurther improvements to the navigation structure will still need to be made. Perhaps most \nencouraging was a general view that the toolkit encouraged reflective practice and that it \nwould build into a valuable resource for individuals as they developed a library of their \nown evaluation plans for a variety of activities. The workshop organizers also noted that \nthe use of the toolkit as a framework for an evaluation workshop significantly increased \nthe degree of evaluation knowledge imparted to participants. (In related face-to-face \nworkshops based on a paper version of a precursor to the toolkit, participants generally \nonly managed to work through a third of the material in the same amount of time.) \nEqually important was the feedback indicating that participants valued the experience of \ngoing through the various development stages involved in creating their own plans with the \noption of being able to reuse and adapt plans developed by other users of the toolkit. The \nability to store their thought processes and answers to different sections online for \nconsideration at a later date was also particularly welcomed. \n47 \nGr(Jinne Conole et al A toolkit for supporting evaluation \nConclusions \nThe work described in this paper has outlined the development of an online evaluation \ntoolkit, which is designed to provide easy-to-use guidance and help for practitioners \ninterested indeveloping evaluation plans to support aspects of their learning and teaching. \nIt has described the philosophy behind the development of the toolkit, the component \nsections of a toolkit for evaluation, and feedback from a formative valuation of this \nresource. \nFeedback from the trials was positive, with many of the users reporting that the toolkit \nhelped guide them through their evaluation process as well as provide them with useful \nlinks to further information and support. However, these initial studies demonstrated the \nneed for additional trials and development work, as well as a series of workshops to \nintroduce the resource to potential groups of users. Other areas of work suggested could \ninclude: \n\u2022 case studies covering 'standard' types of evaluation, perhaps provided by subject \nexperts acting as consultants. The consultants would attend an initial workshop on \nusing the toolkit.and then use the toolkit o generate the case studies. Studies of interest \ninclude the evaluation of Web sites, externally funded projects, a learning and teaching \ninnovation, a strategy document, a resource or a Virtual Learning Environment; \n\u2022 user trials to build on the limited but valuable information gained during the short \ntime-scale of the project. Studies with a more diverse group of users, concentrating on\nthe ways that they use the resource and the aspects they find most useful, would \nimprove the value and relevance of the toolkit itself. They would also help define the \nkey factors for success in producing toolkits and hence help define specifications for \nfuture related resources of this kind; \n\u2022 adaptation of the existing resource for other user groups, such as further education; \n\u2022 extension of the toolkit to cover other areas of learning and teaching such as \ncurriculum development, media selection, assessment or quality assurance. \nToolkits clearly represent a valuable type of resource for staff such as educational \ndevelopers and learning technologists who work and support academics engaged in the \ndevelopment of learning and teaching practice. The work described within this paper \nwould support he idea that the development of an online toolkit is an effective way of \nsupporting the planning phases involved in carrying out an evaluation study. \nReferences \nConole, G. and Oliver, M. (1998), 'A pedagogical framework for embedding C and IT into \nthe curriculum', ALT-J, 6 (2), 4-16. \nConole, G., Oliver, M. and Harvey, J. (2000), 'Toolkits as an approach to evaluating and \nusing learning materials', ASCILITE 2000, Coifs Harbour, NSW, Australia. \nHarvey, J. (1998), The LTDI Evaluation Cookbook, Learning Technology Dissemination \nInitiative. \n48 \nAI~-] Volume 9 Number I \nHEFCE (1996), 'Evaluation of the teaching and learning technology programme', \nCoopers and Lybrand report, HEFCE M21196, \nh ttp.\/\/www, niss. ac. ukleducation\/hefcelpub96\/m21_96, h tml. \nHEFCE (1997a), 'Teaching and Learning Technology Programme Phase 3: invitation to \nbid', HEFCE, 14\/97, TLTP, Bristol. \nHEFCE (1997b), 'Information technology assisted teaching and learning in higher \neducation' (ITATL report), Bristol: HEFCE Research series. \nOliver, M. and Conole, G. (1998), 'Evaluating Communication and Information \nTechnologies: a toolkit for practitioners', Active Learning, 8, 3-8. \nOliver, M. and Conole, G. (1999a), 'From theory to practice: amodel and project structure \nfor toolkit development', BP ELT Report No. 12, University of North London. \nOliver, M. and Conole, G. (2000), 'Assessing and enhancing quality using toolkits', Journal \nof Quality Assurance in Education, 8 (1), 32-7. \nOliver, M., MacBean, J., Conole, G. and Harvey, J. (forthcoming), 'Using a toolkit to \nsupport he evaluation of learning technology', Journal of Computer Assisted Learning. \nPhelps, J., Oliver, M., Bailey, E and Jenkins, A. (1999), 'The development of a generic \nframework for accrediting professional development in C&IT', EFFECTS report No. 2, \nUniversity of North London. \n49 \n"}