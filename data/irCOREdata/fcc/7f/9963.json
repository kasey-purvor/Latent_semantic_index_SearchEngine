{"doi":"10.1145\/1015330.1015358","coreId":"9963","oai":"oai:www.era.lib.ed.ac.uk:1842\/3694","identifiers":["oai:www.era.lib.ed.ac.uk:1842\/3694","10.1145\/1015330.1015358"],"title":"The Bayesian Backfitting Relevance Vector Machine","authors":["D'Souza, Aaron","Vijayakumar, Sethu","Schaal, Stefan"],"enrichments":{"references":[{"id":19939483,"title":"An O(n) algorithm for incremental real time learning in high dimensional space.","authors":[],"date":"2000","doi":null,"raw":"Vijayakumar, S., & Schaal, S. (2000). An O(n) algorithm for incremental real time learning in high dimensional space. Proceedings of the Seventeenth International Conference on Machine Learning (ICML2000) (pp. 1079{ 1086). Stanford, CA.","cites":null},{"id":19939473,"title":"Bayesian learning for neural networks.","authors":[],"date":"1994","doi":"10.1007\/978-1-4612-0745-0","raw":"Neal, R. M. (1994). Bayesian learning for neural networks. Doctoral dissertation, Dept. of Computer Science, University of Toronto.","cites":null},{"id":19939460,"title":"Bayesian parameter estimation via variational methods.","authors":[],"date":"2000","doi":null,"raw":"Jaakkola, T. S., & Jordan, M. I. (2000). Bayesian parameter estimation via variational methods. Statistics and Computing, 10, 25{37.","cites":null},{"id":19939462,"title":"Comparison of approximate methods for handling hyperparameters.","authors":[],"date":"1999","doi":"10.1162\/089976699300016331","raw":"MacKay, D. J. C. (1999). Comparison of approximate methods for handling hyperparameters. Neural Computation, 11, 1035{1068.","cites":null},{"id":19939455,"title":"Dimensionality reduction for supervised learning using reproducing kernel Hilbert spaces.","authors":[],"date":"2004","doi":null,"raw":"Fukumizu, K., Bach, F. R., & Jordan, M. I. (2004). Dimensionality reduction for supervised learning using reproducing kernel Hilbert spaces. Journal of Machine Learning Research, 5, 73{99.","cites":null},{"id":19939481,"title":"Fast marginal likelihood maximization for sparse Bayesian models.","authors":[],"date":"2003","doi":null,"raw":"Tipping, M. E., & Faul, A. C. (2003). Fast marginal likelihood maximization for sparse Bayesian models. Proceedings of the Ninth International Workshop on Articial Intelligence and Statistics.","cites":null},{"id":19939485,"title":"Gaussian processes for regression.","authors":[],"date":"1996","doi":null,"raw":"Williams, C. K. I., & Rasmussen, C. E. (1996). Gaussian processes for regression. Advances in Neural Information Processing Systems 8 (pp. 514{520). Cambridge, MA: MIT Press.","cites":null},{"id":19939458,"title":"Generalized additive models.","authors":[],"date":"1990","doi":"10.1214\/ss\/1177013604","raw":"Hastie, T. J., & Tibshirani, R. J. (1990). Generalized additive models. No. 43 in Monographs on Statistics and Applied Probability. Chapman & Hall.","cites":null},{"id":19939478,"title":"Local dimensionality reduction.","authors":[],"date":"1998","doi":"10.1007\/s11063-009-9098-0","raw":"Schaal, S., Vijayakumar, S., & Atkeson, C. G. (1998). Local dimensionality reduction. Advances in Neural Information Processing Systems 10 (pp. 633{639). Cambridge, MA: MIT Press.","cites":null},{"id":19939475,"title":"Numerical recipes in C: The art of scienti computing.","authors":[],"date":"1992","doi":"10.1016\/s0003-2670(00)82860-3","raw":"Press, W. H., Teukolsky, S. A., Vetterling, W. T., & Flannery, B. P. (1992). Numerical recipes in C: The art of scientic computing. Cambridge University Press. 2 edition.","cites":null},{"id":19939464,"title":"Principal component regression in exploratory statistical research.","authors":[],"date":"1965","doi":"10.2307\/2283149","raw":"Massey, W. F. (1965). Principal component regression in exploratory statistical research. Journal of the American Statistical Association, 60, 234{246.","cites":null},{"id":19939504,"title":"Soft modeling by latent variables: The nonlinear iterative partial least squares approach. In","authors":[],"date":"1975","doi":"10.1080\/0031383770210103","raw":"Wold, H. (1975). Soft modeling by latent variables: The nonlinear iterative partial least squares approach. In J. Gani (Ed.), Perspectives in probability and statistics, papers in honour of M. S. Bartlett, 520{540. London: Academic Press.","cites":null},{"id":19939480,"title":"Sparse Bayesian learning and the relevance vector machine.","authors":[],"date":"2001","doi":"10.1109\/icdm.2012.58","raw":"Tipping, M. E. (2001). Sparse Bayesian learning and the relevance vector machine. Journal of Machine Learning Research, 1, 211{244.","cites":null},{"id":19939454,"title":"Sparse representation for Gaussian process models.","authors":[],"date":"2001","doi":null,"raw":"Csat o, L., & Opper, M. (2001). Sparse representation for Gaussian process models. In (Leen et al., 2001), 444{ 450.","cites":null},{"id":19939502,"title":"Using the Nystr om method to speed up kernel machines.","authors":[],"date":"2001","doi":null,"raw":"Williams, C. K. I., & Seeger, M. (2001). Using the Nystr om method to speed up kernel machines. In (Leen et al., 2001), 682{688.","cites":null},{"id":19939456,"title":"Variational inference for Bayesian mixtures of factor analysers.","authors":[],"date":"2000","doi":null,"raw":"Ghahramani, Z., & Beal, M. J. (2000). Variational inference for Bayesian mixtures of factor analysers. Advances in Neural Information Processing Systems 12 (pp. 509{ 514). Cambridge, MA: MIT Press.","cites":null},{"id":19939453,"title":"Variational relevance vector machine.","authors":[],"date":"2000","doi":null,"raw":"Bishop, C. M., & Tipping, M. E. (2000). Variational relevance vector machine. Proceedings of the 16th Conference on Uncertainty in Articial Intelligence (pp. 46{ 53). Morgan Kaufmann Publishers.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-08-31T13:37:31Z","abstract":"Traditional non-parametric statistical learning\\ud\ntechniques are often computationally attractive,\\ud\nbut lack the same generalization and\\ud\nmodel selection abilities as state-of-the-art\\ud\nBayesian algorithms which, however, are usually\\ud\ncomputationally prohibitive. This paper\\ud\nmakes several important contributions that\\ud\nallow Bayesian learning to scale to more complex,\\ud\nreal-world learning scenarios. Firstly,\\ud\nwe show that back tting | a traditional\\ud\nnon-parametric, yet highly e cient regression\\ud\ntool | can be derived in a novel formulation\\ud\nwithin an expectation maximization\\ud\n(EM) framework and thus can  nally\\ud\nbe given a probabilistic interpretation. Secondly,\\ud\nwe show that the general framework\\ud\nof sparse Bayesian learning and in particular\\ud\nthe relevance vector machine (RVM), can\\ud\nbe derived as a highly e cient algorithm using\\ud\na Bayesian version of back tting at its\\ud\ncore. As we demonstrate on several regression\\ud\nand classi cation benchmarks, Bayesian\\ud\nback tting o ers a compelling alternative to\\ud\ncurrent regression methods, especially when\\ud\nthe size and dimensionality of the data challenge\\ud\ncomputational resources","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/9963.pdf","fullTextIdentifier":"http:\/\/homepages.inf.ed.ac.uk\/svijayak\/publications\/dsouza-ICML2004.pdf","pdfHashValue":"7febecfcfa587db1780f10897cbeed4bf24d02ac","publisher":"ACM Press","rawRecordXml":"<record><header><identifier>\n        \n            \n                oai:www.era.lib.ed.ac.uk:1842\/3694<\/identifier><datestamp>\n                2010-08-31T15:15:16Z<\/datestamp><setSpec>\n                com_1842_102<\/setSpec><setSpec>\n                col_1842_3391<\/setSpec>\n            <\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:doc=\"http:\/\/www.lyncode.com\/xoai\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n            \nThe Bayesian Backfitting Relevance Vector Machine<\/dc:title><dc:creator>\nD'Souza, Aaron<\/dc:creator><dc:creator>\nVijayakumar, Sethu<\/dc:creator><dc:creator>\nSchaal, Stefan<\/dc:creator><dc:subject>\nBackfitting<\/dc:subject><dc:description>\nTraditional non-parametric statistical learning\\ud\ntechniques are often computationally attractive,\\ud\nbut lack the same generalization and\\ud\nmodel selection abilities as state-of-the-art\\ud\nBayesian algorithms which, however, are usually\\ud\ncomputationally prohibitive. This paper\\ud\nmakes several important contributions that\\ud\nallow Bayesian learning to scale to more complex,\\ud\nreal-world learning scenarios. Firstly,\\ud\nwe show that back tting | a traditional\\ud\nnon-parametric, yet highly e cient regression\\ud\ntool | can be derived in a novel formulation\\ud\nwithin an expectation maximization\\ud\n(EM) framework and thus can  nally\\ud\nbe given a probabilistic interpretation. Secondly,\\ud\nwe show that the general framework\\ud\nof sparse Bayesian learning and in particular\\ud\nthe relevance vector machine (RVM), can\\ud\nbe derived as a highly e cient algorithm using\\ud\na Bayesian version of back tting at its\\ud\ncore. As we demonstrate on several regression\\ud\nand classi cation benchmarks, Bayesian\\ud\nback tting o ers a compelling alternative to\\ud\ncurrent regression methods, especially when\\ud\nthe size and dimensionality of the data challenge\\ud\ncomputational resources.<\/dc:description><dc:date>\n2010-08-31T13:37:30Z<\/dc:date><dc:date>\n2010-08-31T13:37:30Z<\/dc:date><dc:date>\n2004-07<\/dc:date><dc:date>\n2010-08-31T13:37:31Z<\/dc:date><dc:type>\nConference Paper<\/dc:type><dc:identifier>\n0377<\/dc:identifier><dc:identifier>\nhttp:\/\/homepages.inf.ed.ac.uk\/svijayak\/publications\/dsouza-ICML2004.pdf<\/dc:identifier><dc:identifier>\nhttp:\/\/hdl.handle.net\/1842\/3694<\/dc:identifier><dc:identifier>\nhttp:\/\/doi.acm.org\/10.1145\/1015330.1015358<\/dc:identifier><dc:publisher>\nACM Press<\/dc:publisher>\n<\/oai_dc:dc>\n<\/metadata>\n        <\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2010,"topics":["Backfitting"],"subject":["Conference Paper"],"fullText":"The Bayesian Back\ftting Relevance Vector Machine\nAaron D'Souza adsouza@usc.edu\nUniversity of Southern California, Los Angeles, CA 90089, USA\nSethu Vijayakumar sethu.vijayakumar@ed.ac.uk\nUniversity of Edinburgh, Edinburgh EH9 3JZ, UK\nStefan Schaal sschaal@usc.edu\nUniversity of Southern California, Los Angeles, CA 90089, USA and ATR Computational Neuroscience Labora-\ntory, Kyoto, Japan\nAbstract\nTraditional non-parametric statistical learn-\ning techniques are often computationally at-\ntractive, but lack the same generalization and\nmodel selection abilities as state-of-the-art\nBayesian algorithms which, however, are usu-\nally computationally prohibitive. This paper\nmakes several important contributions that\nallow Bayesian learning to scale to more com-\nplex, real-world learning scenarios. Firstly,\nwe show that back\ftting | a traditional\nnon-parametric, yet highly e\u000ecient regres-\nsion tool | can be derived in a novel for-\nmulation within an expectation maximiza-\ntion (EM) framework and thus can \fnally\nbe given a probabilistic interpretation. Sec-\nondly, we show that the general framework\nof sparse Bayesian learning and in particu-\nlar the relevance vector machine (RVM), can\nbe derived as a highly e\u000ecient algorithm us-\ning a Bayesian version of back\ftting at its\ncore. As we demonstrate on several regres-\nsion and classi\fcation benchmarks, Bayesian\nback\ftting o\u000bers a compelling alternative to\ncurrent regression methods, especially when\nthe size and dimensionality of the data chal-\nlenge computational resources.\n1. Introduction\nReal-world data, for instance obtained from neuro-\nscience, chemometrics, data mining, or sensor-rich en-\nvironments, is frequently extremely high-dimensional,\nAppearing in Proceedings of the 21\nst International Confer-\nence on Machine Learning, Ban\u000b, Canada, 2004. Copyright\n2004 by the \frst author.\nseverely underconstrained (few data points), even in-\nterspersed with large amounts of irrelevant and\/or\nredundant features. Combined with the inevitable\nmeasurement noise, e\u000ecient learning from such data\nstill poses signi\fcant challenges to state-of-the-art su-\npervised learning algorithms, even in linear settings.\nWhile traditional statistical techniques (e.g. partial\nleast squares (PLS) regression, back\ftting) for super-\nvised learning are often quite e\u000ecient and robust for\nthese problems, they lack a probabilistic interpreta-\ntion and cannot easily provide measures like predic-\ntive distributions or the evidence of data as needed\nfor model selection. On the other hand, while recent\nalgorithms in supervised learning compute such infor-\nmation, they lack computational e\u000eciency as, for in-\nstance, in Gaussian process regression or support vec-\ntor learning. The goal of this paper is to introduce a\nnew algorithm that exploits the best of both worlds\nby developing a probabilistic formulation of a classical\nnon-parametric non-probabilistic regression algorithm.\nAs will be demonstrated, this algorithm can greatly\nimprove the computational e\u000eciency of the modern\nframework of sparse Bayesian learning, including fea-\nture detection and automatic relevance determination,\nand allow this technique to be applied for very high di-\nmensional problems.\n1.1. Sparse Bayesian Learning: The Relevance\nVector Machine\nThe relevance vector machine was introduced by\nBishop and Tipping (2000) as an alternative to the\npopular support vector regression (SVR) method. The\nRVM operates in a framework similar to generalized\nlinear regression, but uses the following generative\nmodel:\ny(x;b) =\nN X\ni=1\nbik(x;xi) + \u000f (1)where k(x;xi) is a bivariate kernel function centered\non each of the N training data points xi, and b = \u0002\nb1 ::: bN\n\u0003T\nis a vector of regression coe\u000ecients.\nAs in SVR, the goal of the RVM is to accurately pre-\ndict the target function, while retaining as few basis\nfunctions as possible in the linear combination. This\nis achieved through the framework of sparse Bayesian\nlearning and the introduction of prior distributions\nover the precisions \u000bi of each element of b:\np(b;\u000b) =\nN Y\ni=1\nNormal\n\u0000\nbi;0;\u000b\n\u00001\ni\n\u0001\nGamma(\u000bi;a\u000b;b\u000b)\n(2)\nb\n1\nb\n2\np\n(\nb\n1\n,\nb\n2\n)\nFigure 1. Marginal prior over regression coe\u000ecients\nThis form of prior results in the marginal distribution\nover b being a product of Student-t distributions as\nshown (for a 2-dimensional b) in Fig. 1, and thus favors\nsparse solutions that lie along the (hyper-)spines of the\ndistribution.\nApproximate analytical solutions for the RVM can be\nobtained by the Laplace method (Tipping, 2001) or\nby using factorial variational approximations (Bishop\n& Tipping, 2000). However, in both these methods\neach update of the hyperparameters \u000b requires the\nre-estimation of the posterior distribution of b via an\nO(N3) Cholesky decomposition. As the number of\ndata points increases, the RVM faces a similar explo-\nsion of computational requirements as that observed\nin Gaussian processes and support vector machines,\nsince each new data point adds an extra \\dimension\"\nto the input vector. As will be shown in the next sec-\ntion, there are several alternatives for performing this\nregression step e\u000eciently. In particular, our introduc-\ntion of a probabilistic version of back\ftting in this pa-\nper will show that we can achieve orders of magnitude\nimprovement in the performance of the RVM, allowing\nthe back\ftting-RVM to tackle signi\fcantly larger data\nsets than previous methods.\n1.2. High-Dimensional Regression\nAlgorithms for high-dimensional regression usually fall\ninto one of two categories:\n1. Those that try to \fnd a low-dimensional repre-\nsentation of the data which captures the salient\ninformation required to perform the regression.\n2. Those that deal with the complete dimensionality,\nbut structure computations as e\u000eciently as pos-\nsible (such as performing successive inexpensive\nunivariate regressions).\nIn the former category, methods like Principal Com-\nponent Regression (PCR) and Factor Regression (FR)\ncan be used to \fnd a low-dimensional representation\nof the input data (Massey, 1965). Unfortunately, these\nmethods are purely variance based, and do not take the\noutput data into account when determining the rele-\nvant input dimensions. Thus, directions in input space\nwhich have large variance will be retained even if they\nhave no in\ruence on the prediction at all (Schaal et al.,\n1998). This drawback can be somewhat alleviated by\nperforming the dimensionality reduction on the joint\nspace of input and output data, and then conditioning\non the observed input. Joint-space principal compo-\nnent regression (JPCR), and joint-space factor analy-\nsis for regression (JFR) are two such methods (Schaal\net al., 1998), and more recently, the use of reproducing\nkernel Hilbert spaces (Fukumizu et al., 2004). The di-\nmensionality reduction nevertheless typically requires\nexpensive manipulation of covariance matrices of the\ndata | an operation typically cubic in the assumed\nlatent dimensionality.\nMethods like partial least squares (PLS) (Wold, 1975)\nand back\ftting (Hastie & Tibshirani, 1990) fall into\nthe second category mentioned above (i.e. algorithms\nthat structure computation e\u000eciently). While PLS\nperforms computationally inexpensive univariate re-\ngressions, along projection directions chosen according\nto correlation between input and output, back\ftting\ncreates fake supervised targets for successive inexpen-\nsive univariate regressions along each input dimension\n(see Algorithm 1). This e\u000bectively decouples inference\nin each individual dimension leading to a highly e\u000e-\ncient (albeit iterative) algorithm which can be shown\nto be a generalized Gauss-Seidel procedure (Hastie &\nTibshirani, 1990).\nAlthough computationally extremely e\u000ecient, back\ft-\nting comes with a series of drawbacks, the most signif-\nicant being that it has no probabilistic interpretation.\nThis makes it di\u000ecult to insert into the framework of\ncurrent research in Bayesian statistical learning which\nemphasizes model selection, and the estimation of con-\n\fdence intervals. Another potential pitfall is that in\neven the simplest case of linear regression, back\ftting\nprovides no guarantees of convergence (Press et al.,\n1992). In Sec. 2, we will show that a simple modi\fca-\ntion to the standard graphical model for linear regres-1: Init: X = [x1;:::;xN]T;y =\n\u0002\ny1;:::;yN\n\u0003\n;gm;i =\ngm(xi;\u0012m);gm = [gm;1;:::;gm;N]T\n2: repeat\n3: for m = 1 to d do\n4: rm   y \u0000\nP\nk6=m gk \/\/fake target\n5: \u0012m   argmin\u0012m kgm \u0000 rmk2\n6: end for\n7: until convergence of \u0012m\nAlgorithm 1: The back\ftting algorithm works with a\nlinear combination of basis functions gm(xi;\u0012m) that\nare iteratively updated to \ft fake targets formed by\npartial residuals.\nsion allows us to derive a probabilistic version of back-\n\ftting which is guaranteed to converge by virtue of\nthe convergence properties of the EM algorithm. Sub-\nsequently, this allows us to augment the model with\nappropriate prior distributions to enable an automatic\ndetermination of which input dimensions are relevant\nto the regression. This in turn gives us the founda-\ntion for our reformulation of sparse Bayesian learning\nin Sec. 4.\n2. Probabilistic Back\ftting\nBy introducing the notion of fake supervised targets,\nback\ftting decouples the inference in each input di-\nmension, creating an e\u000ecient regression algorithm.\nThis section shows that by treating these supervised\ntargets as hidden variables in an EM algorithm, we\ncan derive a probabilistic version of back\ftting, which\nprovides the same computational advantages as tra-\nditional back\ftting, but with the added bonus of a\nprobabilistic interpretation, and convergence proper-\nties that stem from its EM formulation.\nFig. 2(a) shows the graphical model for generalized\nlinear regression, according to the following equation:\ny(x) =\nd X\nm=1\nbmfm(x;\u0012m) + \u000f\ni.e., multiple predictors fm(x;\u0012m) (where 1 \u0014 m \u0014 d)\nthat are generated by an adjustable non-linear trans-\nformation with parameters \u0012m and are fed linearly\nto an output y by an inner product with a regres-\nsion vector b =\n\u0002\nb1 b2 \u0001\u0001\u0001 bd\n\u0003T\nplus additive\nnoise \u000f. It is easy to see that the optimal estimate\nof the regression parameters (in the least-squares or\nmaximum-likelihood sense) is given by the Ordinary\nLeast Squares (OLS) solution bOLS =\n\u0000\nFTF\n\u0001\u00001\nFTy,\nwhere F denotes a matrix whose rows contain the\nfm(xi) of all the training data points f(xi;yi)g\nN\ni=1.\nWith a growing number of fan-in variables in the\ngraphical model (or equivalently, an increasing in-\nput dimensionality d), evaluation of the OLS solution\nbecomes increasingly computationally expensive (ap-\nproximately O(d3)) and numerically brittle.\nConsider the introduction of a random variable zim\nwhich is analogous to the output of the gm func-\ntion of Algorithm 1, where we de\fne gm(x;\u0012m) =\nbmfm(x;\u0012m). For the derivation of our algorithm, we\nassume that zim is conditionally normally distributed,\nzimjxi \u0018 Normal(zim;gm(xi); zm). The introduction\nof the zim variables modi\fes the graphical model to\nthat in Fig. 2(b), which we can formally describe for\nevery data point i as follows:\nyijzi \u0018 Normal\n\u0000\nyi;1Tzi; y\n\u0001\nzimjxi \u0018 Normal(zim;bmfm(xi); zm)\nwhere 1 = [1;1;:::;1]T. It needs to be emphasized\nthat now, the regression coe\u000ecients bm are behind the\nfan-in of the graphical model.\nGiven the data set D = fxi;yig\nN\ni=1, and the graphical\nmodel of Fig. 2(b), we wish to estimate the parame-\nters bm and (possibly) optimize the individual func-\ntions fm(x;\u0012m) with respect to the parameters \u0012m.\nThis is easily formulated as an EM algorithm, which\nmaximizes the incomplete log likelihood logp(yjX):\nlogp(yjX) = \u0000\nN\n2\nlog y \u0000\n1\n2\nN X\ni=1\n\u0000\nyi \u0000 bTf(xi)\n\u00012\n+ const (3)\nby maximizing the expected complete log likelihood\nhlogp(y;ZjX)i, where:\nlogp(y;ZjX) = \u0000\nN\n2\nlog y \u0000\n1\n2 y\nN X\ni=1\n\u0000\nyi \u0000 1Tzi\n\u00012\n\u0000\nd X\nm=1\n\"\nN\n2\nlog zm+\n1\n2 zm\nN X\ni=1\n(zim\u0000bmfm(xi;\u0012m))\n2\n#\n+ const (4)\nAs this maximization is solely based on standard ma-\nnipulations of normal distributions, we omit deriva-\ntions and just summarize the EM update equations\nfor bm and the noise variances  y and  zm as follows:\nM-Step :\nbm =\nPN\ni=1 hzimifm(xi)\nPN\ni=1 fm(xi)2\n y =\n1\nN\nN X\ni=1\n\u0010\nyi \u0000 1\nT hzii\n\u00112\n+ 1\nT\u0006z1\n zm =\n1\nN\nN X\ni=1\n(hzimi \u0000 bmfm(xi))\n2 + \u001b\n2\nzmN\nfd(xi)\nf2(xi)\nf1(xi)\nb yi\n(a) Standard lin-\near regression\nN\nzi2\nzi1\nzid\nf1(xi)\nf2(xi)\nfd(xi)\nb1\nb2\nbd\nyi\n(b) Probabilistic back\ft-\nting\nN\nzi2\nzi1\nzid\nf1(xi)\nf2(xi)\nfd(xi)\nb1\nb2\nbd\n\u000b1\n\u000b2\n\u000bd\nyi\n(c) Back\ftting with ARD\nFigure 2. Graphical models for back\ftting. Circular nodes represent random variables, with a double circle denoting\nobserved variables. Square nodes denote point estimated parameters.\nE-Step :\n1\nT\u0006z1 =\n \nd X\nm=1\n zm\n!\"\n1 \u0000\n1\ns\n \nd X\nm=1\n zm\n!#\n\u001b\n2\nzm =  zm\n\u0012\n1 \u0000\n1\ns\n zm\n\u0013\nhzimi = bmfm(xi) +\n1\ns\n zm\n\u0010\nyi \u0000 b\nTf(xi)\n\u0011\nwhere we de\fne s =  y +\nPd\nm=1  zm, and \u0006z =\nCov(zjy;X). In addition, the parameters \u0012m\nof each function fm can be updated by setting PN\ni=1 (hzimi \u0000 bmfm (xi;\u0012m))\n@fm(xi;\u0012m)\n@\u0012m = 0 and solv-\ning for \u0012m. As this step depends on the particular\nchoice of fm, e.g., splines, kernel smoothers, paramet-\nric models, etc., we will not pursue it any further in\nthis paper and just note that any statistical approxi-\nmation mechanism could be used.\nTwo items in the above EM algorithm are of special\ninterest. First, all equations are algorithmically O(d)\nwhere d is the number of predictor functions fm. Sec-\nond, if we substitute the expression for hzimi in the\nmaximization equation for bm we get the following up-\ndate equation:\nb\n(n+1)\nm = b\n(n)\nm +\n zm\ns\nPN\ni=1\n\u0010\nyi\u0000\nPd\nk=1 b\n(n)\nk fk(xi)\n\u0011\nfm(xi)\nPN\ni=1 fm(xi)2\n(5)\nThus each EM cycle updates the mth regression coe\u000e-\ncient by an amount proportional to the correlation be-\ntween the mth predictor and the residual error. Hence\nthe residual can be interpreted as forming a \\fake tar-\nget\" for the mth branch of the fan-in, which is similar\nto the way PLS regresses residual errors against indi-\nvidual input projections | indeed, our algorithm can\nalso be interpreted as a probabilistic version of PLS.\nAs the next section shows, this enables us to place this\nalgorithm in the context of back\ftting.\n2.1. Interpreting the EM Solution as\nProbabilistic Back\ftting\nIn the context of understanding Eq. (5) as Probabilis-\ntic Back\ftting, we note that back\ftting can be viewed\nas a formal Gauss-Seidel algorithm; an equivalence\nthat becomes exact in the special case of linear mod-\nels (Hastie & Tibshirani, 1990). For the linear system\nFTFb = FTy, the Gauss-Seidel updates for the indi-\nvidual bm are:\nbm =\nPN\ni=1\n\u0010\nyi \u0000\nPd\nk6=m bkfk(xi)\n\u0011\nfm(xi)\nPN\ni=1 fm(xi)2 (6)\nA well-known extension to the Gauss-Seidel algorithm\ncalled successive relaxation adds a fraction (1 \u0000 !) of\nbm to the update and giving us:\nb(n+1)\nm = (1 \u0000 !)b(n)\nm\n+ !\nPN\ni=1\n\u0010\nyi \u0000\nPd\nk6=m bkfk(xi)\n\u0011\nfm(xi)\nPN\ni=1 fm(xi)2 (7)\nwhich has improved convergence rates for overrelax-\nation (1 < ! < 2), or improved stability for under-\nrelaxation (0 < ! < 1). For ! = 1, the standard\nGauss-Seidel\/back\ftting of Eq. (6) is recovered. Set-\nting ! = !m =  zm=s in Eq. (7), it can be shown that\n(after some algebraic rearrangement,) we obtain ex-\nactly our EM update in Eq. (5), i.e., we indeed derive\na probabilistic version of back\ftting as an underrelax-\nation method.\nNotably, the original back\ftting procedure makes no\nguarantees about convergence. However, it is easy to\nshow that due to the convergence properties of EM,\nthe probabilistic back\ftting procedure is guaranteed\nto converge to an OLS solution.\nWe note that in general, there exist other algorithms\nthat can iteratively arrive at a solution to a linear sys-tem of equations such as the method of conjugate gra-\ndients, which can also be related algorithmically to Ja-\ncobi iterations and Gauss-Seidel relaxation methods.\nImportantly, our formulation shows that the rich fam-\nily of methods that can be related to the back\ftting\nalgorithm, and that until now did not have a proba-\nbilistic derivation, can now be represented within the\nprobabilistic framework of an iterative EM algorithm.\nThis is an important stepping stone, since | as the\nnext section shows | these algorithms may now ben-\ne\ft from the model regularizing features of Bayesian\ninference.\n3. Bayesian Back\ftting\nModifying Fig. 2(b) slightly, we now place individual\nprecision variables \u000bm over each of the regression pa-\nrameters bm, resulting in Fig. 2(c). This model struc-\nture can be captured by the following set of prior dis-\ntributions (c.f. Eq. (2)):\np(bj\u000b) =\nd Y\nm=1\n\u0010\u000bm\n2\u0019\n\u00111=2\nexp\nn\n\u0000\n\u000bm\n2\nb2\nm\no\np(\u000b) =\nd Y\nm=1\nba\u000b\n\u000b\n\u0000(a\u000b)\n\u000b(a\u000b\u00001)\nm exp(\u0000b\u000b\u000bm)\n(8)\nUsing a factorial variational approximation\n(e.g. Ghahramani & Beal, 2000), we can derive\nthe modi\fed update equations for the variables in\nthe model. Due to space constraints, we omit the\nderivation, and only summarize the update equations\nfor the mean of b and \u000b:\nhbmi\n(n+1) =\n  PN\ni=1 fm(xi)2\nPN\ni=1 fm(xi)2 +  zm h\u000bmi\n!\nhbmi\n(n)\n+\n zm\nPN\ni=1\n\u0012\nyi \u0000 hbi\n(n)T\nf(xi)\n\u0013\nfm(xi)\ns\n\u0010PN\ni=1 fm(xi)2 +  zm h\u000bmi\n\u0011 (9)\nh\u000bmi =\n2a\u000b + 1\n2b\u000b + hb2\nmi\n(10)\nComparing Eqs. (9) and (5) we see that in the absence\nof a correlation between the current input dimension\nand the residual error, the \frst term of Eq. (9) causes\nthe current regression coe\u000ecient to decay. This results\nin a regression solution which regularizes over the num-\nber of retained input dimensions in the \fnal regression\nvector, similar to Automatic Relevance Determination\n(ARD) (Neal, 1994). As an aside, it is useful to note\nthat if we chose to put a single precision variable over\nthe entire regression vector b then our model reduces\nto ridge regression, with the Bayesian model selection\nprocess providing an automatic tuning of the ridge pa-\nrameter.\n3.1. Bayesian Back\ftting Evaluation\nRather than immediately derive the back\ftting RVM,\nwe will momentarily digress to underscore the e\u000ecacy\nof Bayesian back\ftting as a robust and e\u000ecient linear\nregression procedure. We compare the use of PLS and\nBayesian back\ftting as described in Sec. 3 to analyze\nthe following real-world data set collected from neuro-\nscience. Our choice of PLS for comparison was moti-\nvated by the fact that this is a well-studied algorithm\nthat also has O(d) complexity, and is widely used on\ndata sets in chemometrics with similar properties. The\ndata set consists of simultaneous recordings (2400 data\npoints) of \fring-rate coded activity in 71 motor corti-\ncal neurons and the EMG of 11 muscles. The goal is\nto determine which neurons are responsible for the ac-\ntivity of each muscle. The relationship between neural\nand muscle activity is assumed to be linear, such that\nthe basis functions in back\ftting are simply a copy of\nthe respective input dimensions, i.e. fm(x) = xm.\nA brute-force study (conducted by our research collab-\norators) painstakingly considered every possible com-\nbination of neurons (up to groups of 20 for computa-\ntional reasons, i.e. even this reduced analysis required\nseveral weeks of computation on a 30-node cluster\ncomputer), to determine the optimal neuron-muscle\ncorrelation as measured on various validation sets.\nThis study provided us with a baseline neuron-muscle\ncorrelation matrix that we hoped to duplicate with\nPLS and Bayesian back\ftting, although with much re-\nduced computational e\u000bort.\nBayes. back. PLS baseline\nneuron match 93.6% 18% |\nnMSE 0.8446 1.77 0.84\nTable 1. Results on the neuron-muscle data set\nThe results shown in Table 1 demonstrate two points:\n\u000f The relevant neurons found by Bayesian back\ft-\nting contained over 93% of the neurons found by\nthe baseline study, while PLS fails to \fnd com-\nparable correlations. The neuron match in back-\n\ftting is easily inferred from the resulting magni-\ntude of the precision parameters \u000b, while for PLS,\nthe neuron match was inferred based on the sub-\nspace spanned by the projections that PLS em-\nployed.\n\u000f The regression accuracy of Bayesian back\ftting\n(as determined by 8-fold cross-validation), is com-parable to that of the baseline study, while PLS'\nfailure to \fnd the correct correlations causes it\nto have signi\fcantly higher generalization errors.\nThe analysis for both back\ftting and PLS was\ncarried out using the same validation sets as those\nused for the baseline analysis.\nThe performance of Bayesian back\ftting on this par-\nticularly di\u000ecult data set shows that it is a viable\nalternative to traditional generalized linear regression\ntools. Even with the additional Bayesian inference for\nARD, it maintains its algorithmic e\u000eciency since no\nmatrix inversion is required.\nAs an aside it is useful to note that Bayesian back-\n\ftting and PLS required of the order of 8 hours of\ncomputation on a standard PC1 (compared with sev-\neral weeks on a cluster for the brute-force study), and\nevaluated the contributions of all 71 neurons.\n4. Bayesian Back\ftting RVM\nUntil now, we have chosen not to comment on the\nnature of the basis functions fm(x) in our model. Let\nus now switch to the RVM framework in which we\ncreate N basis functions by centering a bivariate kernel\nfunction k(x;x0) on each individual data point. This\nimplies:\nfm(\u0001) = k(\u0001;xm)\nfor 1 \u0014 m \u0014 d and where we now have d = N. Notice\nthat this transformation makes our back\ftting model\nof Fig. 2(c) equivalent to the RVM model discussed in\nSec. 1.1, with the notable di\u000berence that back\ftting al-\nlows a signi\fcant advantage over the standard RVM in\ncomputational complexity. Note however, that while\nthe computational complexity of a back\ftting update\nis linear in the dimensionality of the problem, it is also\nlinear in the number of data points i.e. O(Nd). When\ncast into the RVM framework, setting d = N makes\nthis complexity O(N2). In particular we would like to\nstress the following:\n\u000f At each update of the \u000bm hyperparameters, the\nRVM requires an O(N3) Cholesky decomposition\nto re-estimate the regression parameters, while\ndiscarding the estimate at the previous iteration.\nIn the back\ftting-RVM however, the existing esti-\nmate of the regression parameters provides a good\nstarting estimate, allowing the update to complete\nin just a handful of O(N2) iterations (\u0018 10 iter-\nations were su\u000ecient in our simulations). The\nsaving in computation is especially evident when\n1Pentium IV class machine, 1.7GHz\n\u221210 \u22128 \u22126 \u22124 \u22122 0 2 4 6 8 10\n\u22120.4\n\u22120.2\n0\n0.2\n0.4\n0.6\n0.8\n1 true\napproximated\ntraining\nrelevant\nFigure 3. Fitting the sinc function using back\ftting-RVM.\nthe number of data points (and hence the e\u000bective\ndimensionality) is large, and in situations where\nthe hyperparameters require many updates before\nconvergence.\n\u000f In the initial computations within the graphical\nmodel, it seems wasteful to spend large amounts\nof computation on estimating parameters accu-\nrately, when surrounding parameters (and hyper-\nparameters) have not converged. One can struc-\nture the back\ftting updates to work with partially\nconverged estimates, such that the brunt of com-\nputation is only expended to accurately estimate\na variable when one is more con\fdent about the\nvariables in its Markov blanket.\nFig. 3 shows back\ftting-RVM used to \ft a toy data\nset generated using the 1-dimensional sinc function\nsin(x)=x, using the Gaussian kernel:\nk(xi;xj) = exp\nn\n\u0000\u0015(xi \u0000 xj)\n2\no\nfor \u0015 > 0. Even though back\ftting-RVM is an order of\nmagnitude faster than the standard RVM, it su\u000bers no\npenalty in generalization error or its ability to sparsify\nthe set of basis functions. We note that Tipping (2001)\nproposes an optimization of the distance metric \u0015 that\nis based on gradient ascent in the log likelihood. Such\na gradient can also be computed for back\ftting as:\n@ hlogp(y;ZjX)i\n@\u0015\n=\nN X\nj=1\nbj\n zj\nN X\ni=1\n(hziji \u0000 bjkij)(xi \u0000 xj)\n2 kij\nwhere we have abbreviated kij = k(xi;xj). Based\non our experience however, we would like to caution\nagainst unconstrained maximization of the likelihood,\nespecially over distance metrics. Instead, we would\nrecommend the route taken in the Gaussian process\ncommunity, which is to treat these variables as hyper-\nparameters, and place prior distributions over them.Exact solutions being typically intractable, we can ei-\nther optimize them by using maximum a posteriori es-\ntimates (MacKay, 1999), or by Monte Carlo techniques\n(Williams & Rasmussen, 1996).\nWe note that there are several \\optimizations\" that are\nsuggested in (Tipping, 2001; Tipping & Faul, 2003).\nThese include pruning the basis functions when their\nprecision variables dictate that they are unneeded, as\nwell as adopting a greedy (but potentially suboptimal)\nstrategy in which the algorithm starts with a single\nbasis function and adds candidates as necessary. We\nwould like to emphasize that our implementation of\nthe back\ftting-RVM performs neither of these opti-\nmizations, although it is trivial to introduce them into\nour framework as well.\n4.1. Back\ftting-RVM Evaluation\nTo evaluate the generalization ability of back\ftting-\nRVM, we compared it to other state-of-the art regres-\nsion tools on the popular benchmark Boston housing\nand Abalone data sets2. For each data set, a randomly\nselected 20% of the data set was used as test data and\nthe remainder for training.\nRVM SVR GP LWPR bRVM\nSinc 0.0132 0.0178 0.0136 0.0124 0.0130\nBoston 0.0882 0.1115 0.0806 0.0846 0.0837\nAbalone 0.4591 0.4830 0.4440 0.4056 0.4473\nTable 2. nMSE on benchmark data sets\nTable 2 shows the normalized mean squared er-\nrors on the test sets averaged over 100 experiments.\nThe algorithms compared were the standard rele-\nvance vector machine (RVM), support vector regres-\nsion (SVR)3, Gaussian process (GP) regression, locally\nweighted projection regression (LWPR) (Vijayakumar\n& Schaal, 2000), and our back\ftting-RVM (bRVM).\nBoth back\ftting-RVM and its standard counterpart\nused Gaussian kernels with distance metrics optimized\nby 5-fold cross-validation. The Gaussian process algo-\nrithm used RBF covariance function with automatic\nhyperparameter optimization. As Table 2 shows,\nback\ftting-RVM provides an extremely competitive\nsolution in terms of generalization ability when com-\npared to other popular regression methods.\nRVM SVR bRVM\nSinc 6.7 45.2 4.8\nBoston 39 142.8 57.4\nAbalone 437 1320 368\nTable 3. \\Relevant\" vectors retained\nFor the 3 methods (RVM, SVR, and bRVM) that fo-\n2Both available from the UCI repository\n3RVM and SVR results adapted from (Tipping, 2001)\ncus on a \\sparsi\fcation\" of the set of basis functions,\nwe compared the average number of basis functions re-\ntained on two data sets: the Boston housing, and sinc\ndata sets. To aid comparison, data for the sinc bench-\nmark was generated using a method identical to that\nspeci\fed in (Tipping, 2001). Table 3 shows the aver-\nage number of vectors retained in the \fnal solution on\nthese data sets.\nRVM bRVM N d\nSinc 18.71s 6.24s 100 1\nBoston 372s 155s 481 13\nAbalone 2767s 428s 3341 10\nTable 4. Relative computation time\nThe above experiments demonstrate that back\ftting-\nRVM is a competitive regression solution when com-\npared to other current state-of-the-art statistical meth-\nods, both in its generalization ability, and in its e\u000ecacy\nas a sparse Bayesian learning algorithm. However, the\nmain advantage of back\ftting-RVM is apparent only\nwhen we examine its relative computation time. Ta-\nble 4 gives the average execution time (in seconds)\nrequired by the RVM, and back\ftting-RVM for con-\nvergence of their regression parameter estimates (to\n5 signi\fcant digits) on the sinc, Boston housing, and\nAbalone data sets. The table also shows the number\nof training data points, and their dimensionality. Note\nthat the number of O(N2) updates to b per update cy-\ncle of the hyperparameters is very small (\u0018 10), since\nthe solution from the previous update cycle is a very\ngood starting point for the iterations of the next cy-\ncle. The results demonstrate that the back\ftting-RVM\ncan signi\fcantly gain from the iterative nature of the\nBayesian back\ftting generalized linear regression pro-\ncedure.\n5. Discussion\nGiven the form y =\nPN\ni=1 bik(\u0001;xi) of the RVM solu-\ntion, it is natural to make a connection to Gaussian\nprocesses, which also express the solution as a linear\ncombination of basis functions centered at the train-\ning data points. Indeed, retaining only the relevant\nvectors amounts to a sparsi\fcation of the Gaussian\nprocess. Our algorithm is equivalent to pruning the\nset of basis functions, as is also achieved using the\nNystr om method (Williams & Seeger, 2001). Other\nvariants exist such as the growing\/replacement solu-\ntion of Csat\u0013 o and Opper (2001), which generalizes well\nto online learning scenarios.\nThis paper makes two essential contributions. Firstly,\nwe have demonstrated that the class of traditionally\nnon-Bayesian, yet highly e\u000ecient iterative linear re-\ngression methods like back\ftting, can be derived fromthe framework of the EM algorithm. We have derived\nBayesian back\ftting, which retains linear complexity\nin the dimensionality of the input data, even while\nperforming ARD-like model selection. On its own,\nBayesian back\ftting provides a very general frame-\nwork for generalized linear regression, which is nu-\nmerically robust and is able to handle extremely high-\ndimensional datasets. At the expense of being an iter-\native algorithm, it is a viable drop-in replacement for\nalgorithms such as PLS, stepwise regression, singular\nvalue decomposition regression, and others mentioned\nin Sec. 1.2. While requiring the assumption of Gaus-\nsian distributions at certain steps of the derivation of\nBayesian back\ftting, our experience with its use on\ndata sets that violate these assumptions has shown no\nsigni\fcant degradation in performance or generaliza-\ntion ability.\nSecondly, we have shown that the framework of sparse\nBayesian learning can bene\ft immensely from a prob-\nabilistic formulation of this iterative class of methods.\nIn particular, the popular relevance vector machine\ncan be derived from the framework of Bayesian back-\n\ftting. This back\ftting-RVM has signi\fcant compu-\ntational advantages over its conventional counterpart,\nwithout sacri\fcing generalization and model regular-\nization ability. Although the examples presented here\nfocus on regression, it is easy to see that by using sim-\nilar variational extensions (Jaakkola & Jordan, 2000)\nas those used in (Bishop & Tipping, 2000), the ap-\nplicability of the back\ftting-RVM can be extended to\nclassi\fcation tasks as well.\nAcknowledgments\nThis research was supported in part by National Sci-\nence Foundation grants ECS-0325383, IIS-0312802,\nIIS-0082995, ECS-0326095, ANI-0224419, a NASA\ngrant AC#98-516, an AFOSR grant on Intelligent\nControl, the ERATO Kawato Dynamic Brain Project\nfunded by the Japanese Science and Technology\nAgency, and the ATR Computational Neuroscience\nLaboratories.\nReferences\nBishop, C. M., & Tipping, M. E. (2000). Variational rel-\nevance vector machine. Proceedings of the 16th Confer-\nence on Uncertainty in Arti\fcial Intelligence (pp. 46{\n53). Morgan Kaufmann Publishers.\nCsat\u0013 o, L., & Opper, M. (2001). Sparse representation for\nGaussian process models. In (Leen et al., 2001), 444{\n450.\nFukumizu, K., Bach, F. R., & Jordan, M. I. (2004). Di-\nmensionality reduction for supervised learning using re-\nproducing kernel Hilbert spaces. Journal of Machine\nLearning Research, 5, 73{99.\nGhahramani, Z., & Beal, M. J. (2000). Variational infer-\nence for Bayesian mixtures of factor analysers. Advances\nin Neural Information Processing Systems 12 (pp. 509{\n514). Cambridge, MA: MIT Press.\nHastie, T. J., & Tibshirani, R. J. (1990). Generalized ad-\nditive models. No. 43 in Monographs on Statistics and\nApplied Probability. Chapman & Hall.\nJaakkola, T. S., & Jordan, M. I. (2000). Bayesian param-\neter estimation via variational methods. Statistics and\nComputing, 10, 25{37.\nLeen, T. K., Diettrich, T. G., & Tresp, V. (Eds.). (2001).\nAdvances in neural information processing systems 13,\nvol. 13. Cambridge, MA: MIT Press.\nMacKay, D. J. C. (1999). Comparison of approximate\nmethods for handling hyperparameters. Neural Com-\nputation, 11, 1035{1068.\nMassey, W. F. (1965). Principal component regression in\nexploratory statistical research. Journal of the American\nStatistical Association, 60, 234{246.\nNeal, R. M. (1994). Bayesian learning for neural networks.\nDoctoral dissertation, Dept. of Computer Science, Uni-\nversity of Toronto.\nPress, W. H., Teukolsky, S. A., Vetterling, W. T., & Flan-\nnery, B. P. (1992). Numerical recipes in C: The art of\nscienti\fc computing. Cambridge University Press. 2 edi-\ntion.\nSchaal, S., Vijayakumar, S., & Atkeson, C. G. (1998). Lo-\ncal dimensionality reduction. Advances in Neural In-\nformation Processing Systems 10 (pp. 633{639). Cam-\nbridge, MA: MIT Press.\nTipping, M. E. (2001). Sparse Bayesian learning and the\nrelevance vector machine. Journal of Machine Learning\nResearch, 1, 211{244.\nTipping, M. E., & Faul, A. C. (2003). Fast marginal likeli-\nhood maximization for sparse Bayesian models. Proceed-\nings of the Ninth International Workshop on Arti\fcial\nIntelligence and Statistics.\nVijayakumar, S., & Schaal, S. (2000). An O(n) algorithm\nfor incremental real time learning in high dimensional\nspace. Proceedings of the Seventeenth International Con-\nference on Machine Learning (ICML2000) (pp. 1079{\n1086). Stanford, CA.\nWilliams, C. K. I., & Rasmussen, C. E. (1996). Gaussian\nprocesses for regression. Advances in Neural Information\nProcessing Systems 8 (pp. 514{520). Cambridge, MA:\nMIT Press.\nWilliams, C. K. I., & Seeger, M. (2001). Using the Nystr om\nmethod to speed up kernel machines. In (Leen et al.,\n2001), 682{688.\nWold, H. (1975). Soft modeling by latent variables: The\nnonlinear iterative partial least squares approach. In\nJ. Gani (Ed.), Perspectives in probability and statistics,\npapers in honour of M. S. Bartlett, 520{540. London:\nAcademic Press."}