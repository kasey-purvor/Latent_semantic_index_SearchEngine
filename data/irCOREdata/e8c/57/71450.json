{"doi":"10.1109\/ICDMW.2006.74","coreId":"71450","oai":"oai:eprints.lancs.ac.uk:935","identifiers":["oai:eprints.lancs.ac.uk:935","10.1109\/ICDMW.2006.74"],"title":"Evolving extended naive Bayes classifiers","authors":["Klawonn, Frank","Angelov, Plamen"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2006-12","abstract":"Naive Bayes classifiers are a very simple, but often effective tool for classification problems, although they are based on independence assumptions that do not hold in most cases. Extended naive Bayes classifiers also rely on independence assumptions, but break them down to artificial subclasses, in this way becoming more powerful than ordinary naive Bayes classifiers. Since the involved computations for Bayes classifiers are basically generalised mean value calculations, they easily render themselves to incremental and online learning. However, for the extended naive Bayes classifiers it is necessary, to choose and construct the subclasses, a problem whose answer is not obvious, especially in the case of online learning. In this paper we propose an evolving extended naive Bayes classifier that can learn and evolve in an online manner (c) IEEE Pres","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/71450.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/935\/1\/klawonn_angelov.pdf","pdfHashValue":"f75af29c28e95592166cb9267894c6aeb1126a0d","publisher":"IEEE","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:935<\/identifier><datestamp>\n      2018-01-24T02:07:15Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D626F6F6B5F73656374696F6E<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Evolving extended naive Bayes classifiers<\/dc:title><dc:creator>\n        Klawonn, Frank<\/dc:creator><dc:creator>\n        Angelov, Plamen<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        Naive Bayes classifiers are a very simple, but often effective tool for classification problems, although they are based on independence assumptions that do not hold in most cases. Extended naive Bayes classifiers also rely on independence assumptions, but break them down to artificial subclasses, in this way becoming more powerful than ordinary naive Bayes classifiers. Since the involved computations for Bayes classifiers are basically generalised mean value calculations, they easily render themselves to incremental and online learning. However, for the extended naive Bayes classifiers it is necessary, to choose and construct the subclasses, a problem whose answer is not obvious, especially in the case of online learning. In this paper we propose an evolving extended naive Bayes classifier that can learn and evolve in an online manner (c) IEEE Press<\/dc:description><dc:publisher>\n        IEEE<\/dc:publisher><dc:date>\n        2006-12<\/dc:date><dc:type>\n        Contribution in Book\/Report\/Proceedings<\/dc:type><dc:type>\n        NonPeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/935\/1\/klawonn_angelov.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/ICDMW.2006.74<\/dc:relation><dc:identifier>\n        Klawonn, Frank and Angelov, Plamen (2006) Evolving extended naive Bayes classifiers. In: Data Mining Workshops, 2006. ICDM Workshops 2006. Sixth IEEE International Conference on. IEEE, pp. 643-647. ISBN 0-7695-2702-7<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/935\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1109\/ICDMW.2006.74","http:\/\/eprints.lancs.ac.uk\/935\/"],"year":2006,"topics":["QA75 Electronic computers. Computer science"],"subject":["Contribution in Book\/Report\/Proceedings","NonPeerReviewed"],"fullText":"Evolving Extended Na\u0131\u00a8ve Bayes Classifiers\nFrank Klawonn\nDepartment of Computer Science\nUniversity of Applied Sciences BS\/WF\nSalzdahlumer Str. 46\/48\nD-38302 Wolfenbuettel, Germany\nf.klawonn@fh-wolfenbuettel.de\nPlamen Angelov\nDepartment of Communication Systems\nInfoLab21\nLancaster University\nSouth Drive, Lancaster LA1 4WA, UK\np.angelov@lancaster.ac.uk\nAbstract\nNa\u0131\u00a8ve Bayes classifiers are a very simple tool for classi-\nfication problems, although they are based on independence\nassumptions that do not hold in most cases. Extended na\u0131\u00a8ve\nBayes classifiers also rely on independence assumption, but\nbreak them down to artificial subclasses, in this way becom-\ning more powerful than ordinary na\u0131\u00a8ve Bayes classifiers.\nSince the involved computations for Bayes classifier are ba-\nsically generalised mean value calculations, they easily ren-\nder themselves to incremental and online learning. How-\never, for the extended na\u0131\u00a8ve Bayes classifiers it is necessary,\nto choose and construct the subclasses, a problem whose\nanswer is not obvious, especially in for online learning. In\nthis paper we propose an evolving extended na\u0131\u00a8ve Bayes\nclassifiers that can learn and evolve in an online manner.\n1. Introduction\nIn recent years, in many application fields of data anal-\nysis and mining it was realised that there is a strong need\nfor analysing and mining streaming data. Streaming data\ndiffer from the classical paradigm of data analysis and min-\ning in the sense that the data set to be analysed is not com-\npletely available at the start of the analysis. Data arrive as\na stream so that more and more data become available over\ntime. In most applications it is not possible to wait until\na large amount of data has been collected for the analysis.\nInstead, the analysis should be started as soon as possible,\neven with a small data set. However, when more data arrive,\nthe analysis should not be re-started completely, but should\nbe carried out in an incremental fashion. This means that\nnew or modified algorithms are needed that can work in an\nincremental way. In most cases, it is also impossible to store\nthe full history of the data stream. Therefore, an algorithm\nfor streaming data should only rely on a small, simplified\nexcerpt from the original data stream that contains the nec-\nessary information for the analysis.\nWhen a data stream is analysed in a purely incremen-\ntal fashion, it is assumed that the underlying model of the\ndata and its parameters do not change over time, which\nturns out to be an unrealistic assumption in many applica-\ntions. Therefore, a proper analysis of a data stream must\nbe able to evolve over time, i.e. it must be able to adjust its\nmodel structure and the corresponding parameter set, when\nchanges in the data stream occur.\nIn this paper we focus on evolving classifiers. This\nmeans we assume that the data stream consists of a num-\nber of input attributes that are used for the prediction of an\nadditional categorical attribute whose values are the classes.\nThe correct classification is not available at the time, when\nthe input data arrive and the prediction is needed. How-\never, in order to be able to train the evolving classifier, we\nassume that the correct classification will be available at a\nlater stage. For instance, when we want to predict whether\nit will rain the next day, the prediction must be based on\nknown measurements available before the next day. How-\never, after the next day, it is known whether the prediction\nwas correct or not, i.e. whether it has been raining or not.\nMany traditional classifiers try to minimize the classifi-\ncation error directly. This means in most cases that the full\ninformation from a training data set is needed to build such\nclassifiers. However, in an evolving setting it is not realis-\ntic to assume that all historical data can be stored. Although\nthere are approaches to evolving, classification performance\ndriven classifiers, they tend to be quite complex. Na\u0131\u00a8ve\nBayes classifiers do not suffer from this problem, they are\ndistribution driven. Distributions and their parameters are\nvery easy to track and to be adapted in an evolving fashion.\nHowever, a standard na\u0131\u00a8ve Bayes classifier is very limited\nand not well-suited for more difficult classification prob-\nlems, especially since only one simple joint probability dis-\ntribution is used per class. Extended na\u0131\u00a8ve Bayes classifiers\nallow the introduction of artificial subclasses for each class\nin order to achieve a better performance. Another advan-\ntage of extended na\u0131\u00a8ve Bayes classifiers is that they can be\ninterpreted as a rule-based fuzzy classifier so that their clas-\nsification behaviour is easier to understand for non-experts.\nThe evolving extended na\u0131\u00a8ve Bayes classifier proposed\nin this paper can handle continuous as well as categorical\nattributes. Updating the parameters of the probability dis-\ntribution can be carried out in a purely incremental fashion,\nbut it is also possible to neglect the influence of classifica-\ntions that were learned a long time. The more interesting\nevolving part of the classifier is the introduction of new ar-\ntificial subclasses in order to improve the performance of\nthe classifier.\nSection 2 briefly reviews the concepts of standard and\nextended na\u0131\u00a8ve Bayes classifiers and their relation to fuzzy\nclassifiers. How incremental learning and evolving strate-\ngies can be applied to extended na\u0131\u00a8ve Bayes classifiers is\ndiscussed in section 3, followed by a brief example in sec-\ntion 4. We conclude the paper by outlining future work,\nespecially ideas for simplifying the extended na\u0131\u00a8ve Bayes\nclassifiers in an evolving fashion.\n2. The Framework of Extended Na\u0131\u00a8ve Bayes\nClassifiers\nSupervised classification refers to the problem to predict\nthe value of a categorical attribute, the so-called class, based\non the values of other attributes that can be categorical, or-\ndinal or continuous. A typical setting for supervised classi-\nfication is where a data set of classified data objects is avail-\nable and the task is to construct the classifier based on this\ndata set. Usually the data set is split up into a training and a\ntest set, even multiple times in the case of cross-validation,\nin order to better judge the prediction quality of the classi-\nfier for unknown data. Typically, the misclassification rate\nis used as an indicator for the quality of the classifier. How-\never, this is only a special case of a general cost or loss ma-\ntrix that specifies the estimated average costs that will result\nfrom misclassifications of an object from each true class \u0000\nto any other class \u0001 . The misclassification rate simply uses\na cost matrix with ones everywhere, except in the diago-\nnal. The costs in the diagonal, i.e. the losses for correct\nclassifications are assumed to be zero. The costs for mis-\nclassifications can differ extremely so that simply counting\nthe misclassifications might mislead the classifier. If, for in-\nstance, we want to predict whether a component of a safety\ncritical system like an aeroplane will work without failure\nduring the next operation, the costs for misclassifying the\ncomponent as faulty, although it would work, are the costs\nfor exchanging the component. The costs for misclassifying\na faulty component as correct can cause the death of many\npeople as \u201dcosts\u201d.\nA large variety of classifiers exist in the literature which\ncannot be mentioned here completely. Linear discriminant\nanalysis is a very simple statistical classifier that is easy to\nconstruct. Decision trees are very popular because they are\neasy to understand according to their simple rule-like struc-\nture. The same applies to fuzzy classifiers. Both of them\nrely on more complex learning algorithms. Na\u0131\u00a8ve Bayes\nclassifiers, that will be the focus of this paper, are also very\nelementary probabilistic models. They are Bayesian net-\nworks with an extremely simple structure. The learning al-\ngorithm for na\u0131\u00a8ve Bayes classifiers is very simple and they\ncan also be interpreted easily due to the \u2013 sometimes unre-\nalistic \u2013 underlying independence assumptions.\n2.1. Na\u0131\u00a8ve Bayes Classifiers\nA Bayes classifiers exploits the Bayes rule from statis-\ntics: \u0002\u0004\u0003\u0006\u0005\b\u0007 \t\u000b\n\r\f \u0002\u0004\u0003\u0006\t\u000e\u0007 \u0005\u000f\n\u0011\u0010\u0012\u0002\u0004\u0003\u0006\u0005\u000f\n\n\u0002\u0004\u0003\u0006\t\u000b\n (1)\n\u0005\nis the hypothesis, in the case of classification any value\nof the categorical attribute that we want to predict.\n\t\nis the\nevidence, i.e. the information from the observed attributes\nthat we want to exploit to predict the considered categorical\nattribute. When the misclassification rate is used as an indi-\ncator for the quality of the classifier, then simply the class\n\u0013 yielding the highest posterior probability\n\u0002\u0004\u0003\u0006\u0005\u0014\f\n\u0013\n\u0007 \t\u000b\n\nis predicted. If a cost matrix \u0015 for misclassifications is\nknown, the prediction tries to minimize the expected loss.\nIf the categorical attribute to be predicted can take values\nfrom the finite set of classes \u0016 , then the entry \u0015\n\u0003\n\u0013\u0018\u0017\u0019\u0013ff\u001a\n\n\nfor\nany \u0013\u0018\u0017\u0019\u0013fi\u001affifl \u0016 in the cost matrix stands for the costs that will\nresult, when predicting \u0013\u0012\u001a instead of the correct class \u0013 . Of\ncourse, we always assume \u0015\n\u0003\n\u0013\u0018\u0017\u0019\u0013\n\n\u001f\f! \nfor any \u0013\"fl \u0016 . Cor-\nrect predictions do not cause any loss. The expected loss\ngiven evidence\n\t\nand predicting class \u0013 is\nloss\n\u0003\n\u0013\n\u0007 \t\u000b\n#\f $\n%'&)(+*\n\u0002\u0004\u0003\n\u0013\n\u001a\n\u0007 \t\u000b\n\u0011\u0010\n\u0015\n\u0003\n\u0013\n\u001a\n\u0017\u0019\u0013\n\n\n\f\n$\n%\n&\n(+*\n\u0002\u0004\u0003\u0006\t\u000e\u0007\n\u0013fi\u001a\n\n\u0011\u0010\u0012\u0002\u0004\u0003\n\u0013fi\u001a\n\n\n\u0002\u0004\u0003\u0006\t\u000b\n\n\u0010\n\u0015\n\u0003\n\u0013\n\u001a\n\u0017\u0019\u0013\n\n-,\n(2)\nIn this case, the class \u0013 yielding the lowest value in (2) is\npredicted by the Bayes classifier.\nNote that in both equations (1) and (2), the probability\u0002\u0004\u0003\u0006\t\u000b\n\nof the specific evidence does not have any influence\non the predicted class \u0013 , since\n\u0002\u0004\u0003\u0006\t\u000b\n\ncan be considered as a\nconstant factor independent of \u0013 . Therefore, for the decision\nin (1), it is sufficient to consider the likelihoods (unnormal-\nized probabilities).\n\u0003\u0006\u0005\b\u0007 \t\u000b\n\r\f\/\u0002\u0004\u0003\u0006\t\u000e\u0007 \u0005\u000f\n\u0011\u0010\u0012\u0002\u0004\u0003\u0006\u0005\u000f\n\n(3)\nand in (4) we only need the relative expected losses\nrloss\n\u0003\n\u0013\n\u0007 \t\u000b\n0\f\n$\n%'&)(+*\n.\n\u0003\n\u0013\n\u001a\n\u0007 \t\u000b\n\u0011\u0010\n\u0015\n\u0003\n\u0013\n\u001a\n\u0017\u0019\u0013\n\n\n\f$\n%\n&\n(+*\n\u0002\u0004\u0003\u0006\t\u000e\u0007\n\u0013\n\u001a\n\n \u0010ff\u0002\u0004\u0003\n\u0013\n\u001a\n\n\u0011\u0010\n\u0015\n\u0003\n\u0013\n\u001a\n\u0017\u0019\u0013\n\n-,\n(4)\nThe evidence\n\t\nrepresents the measured or given val-\nues of the attributes that we exploit for the prediction of the\nconsidered categorical attribute. For a na\u0131\u00a8ve Bayes clas-\nsifier (see for instance [15]) it is assumed that these at-\ntributes are independent given the classes. This means that,\nif\n\u0000\n\u0000\n\u0017\n,\u0012,\u0012,\n\u0017\n\u0000\u0002\u0001\nare the attributes used for prediction, we have\n\u0002\u0004\u0003\n\u0000\n\u0000\n\f\n\u0003\n\u0000\n\u0017\n,\u0012,\u0012,\n\u0017\n\u0000\u0002\u0001\n\f\n\u0003\n\u0001\n\u0007\n\u0013\n\n \f\n\u0002\u0004\u0003\n\u0000\n\u0000\n\f\n\u0003\n\u0000\n\u0007\n\u0013\n\n \u0010 ,\u0012,\u0012,+\u0010ff\u0002\u0004\u0003\n\u0000\u0002\u0001\n\f\n\u0003\n\u0001\n\u0007\n\u0013\n\n\n(5)\nIn order to apply a na\u0131\u00a8ve Bayes classifier, the probabilities\nin (5) must be known. In case that the attributes \u0000 \u0000 \u0017\n,\u0012,\u0012,\n\u0017\n\u0000\u0002\u0001\nare categorical attributes, these probabilities can be esti-\nmated based on the corresponding relative frequencies in\nthe available training data set.\nFor a continuous attribute\n\u0000\u0002\u0004\n, it is necessary to estimate\nits (conditional) probability density function (pdf) \u0005\u0007\u0006\t\b\u000b\n %\nfrom the training data set. Once we have an estimation for\nthe pdf\n\u0005\f\u0006\t\b\u000b\n\n% , the corresponding value\n\u0005\r\u0006\t\b\u000e\n\n%\n\u0003\n\u0003\n\u0004\n\n\nis used in\n(5) instead of the probability\n\u0002\u0004\u0003\n\u0000\u0002\u0004\n\f\n\u0003\n\u0004\n\u0007\n\u0013\n\n\n. This means the\ncomputed value\n\u0002\u0004\u0003\n\u0000\n\u0000\n\f\n\u0003\n\u0000\n\u0017\n,\u0012,\u0012,\n\u0017\n\u0000\u0002\u0001\n\f\n\u0003\n\u0001\n\u0007\n\u0013\n\n\nis no longer a\nprobability, but a likelihood, i.e. an unnormalized probabil-\nity.\nIn order to estimate the pdf\n\u0005\r\u0006\t\b\u000b\n\n% , it is usually assumed\nthat\n\u0005\f\u0006\t\b\u000e\n\n% belongs to a class of parametric distributions, so\nthat only the corresponding parameters of the pdf have to\nbe estimated. A very typical assumption is that\n\u0000\u000f\u0004\n\u0007\n\u0013 is nor-\nmally distributed with unknown mean \u0010 \u0004\n\n\n% and unknown\nvariance \u0011\t\u0012\u0004 \n\n%\n. Such parameters can be estimated from the\ntraining data set using the corresponding well-known for-\nmulae from statistics.\n2.2. Extended Na\u0131\u00a8ve Bayes Classifiers\nThe underlying model of a na\u0131\u00a8ve Bayes classifier as-\nsumes that each class is characterised by a probability dis-\ntribution and for each class this probability distribution is\nsimply the product of it marginals. Figure 1 shows the nor-\nmal distributions for the well known iris data set [3] learned\nby a na\u0131\u00a8ve Bayes classifier. The categorical attribute to be\npredicted has three different values for the iris data set. Four\ncontinuous attributes are used for the prediction. For each\nof the three classes, the na\u0131\u00a8ve Bayes classifier computes a\nfour-dimensional normal distribution (with diagonal covari-\nance matrices), whose marginal distributions are shown in\nfigure 1.\nThe classification performance will work out well in\ncases like the iris data set, when the distribution of the data\ncan be described roughly by a (multinomial) normal distri-\nbution for each class. If, however, the data objects of one\nFigure 1. The normal distributions for the\nthree classes for each of the four attributes\nfor the iris data set.\nclass do not form a kind of compact cluster, but are dis-\ntributed over different separated clusters, then a na\u0131\u00a8ve Bayes\nclassifier might fail completely.\nIn this sense a na\u0131\u00a8ve Bayes classifier can be viewed as\na specific form of a fuzzy classifier using exactly one rule\nper class [10]. For this purpose the probability distribu-\ntions must be scaled in such a way that they never yield\nvalues greater than one, in order to interpret them as fuzzy\nsets. This leads to a constant scaling factor that does not\ninfluence the classification decision. The fuzzy sets (scaled\ndistributions) for one class are aggregated by the product\noperator. Typically, fuzzy classifiers use the minimum for\nthis operation. However, the minimum often leads to se-\nvere restrictions [14] and the product is a possible alterna-\ntive. The prior probabilities for the classes correspond to\nrule weights.\nIn order to overcome this restriction that a na\u0131\u00a8ve Bayes\nclassifier using normal distributions for continuous at-\ntributes assumes a unimodal distribution of the data for each\nclass, the introduction of pseudo-classes was proposed in\n[8]. In the sense of a fuzzy classifier, more than one rule is\nallowed for each class. In terms of a Bayes classifier, each\nclass is represented by a number of artificial subclasses.\nIn order to classify a data object, such an extended na\u0131\u00a8ve\nBayes classifier will first compute the posterior probabili-\nties (likelihoods) for each pseudo-class in the same way as\na standard na\u0131\u00a8ve Bayes classifier. The posterior probabili-\nties (likelihoods) for the actual classes are simply obtained\nas the sums of the posterior probabilities (likelihoods) of the\ncorresponding pseudo-classes.\nAlthough the classification of new data object is obvi-\nous for an extended na\u0131\u00a8ve Bayes classifier, it is not clear\nat all, how to estimate the prior probabilities and the cor-\nresponding probability distributions for the pseudo-classes.\nThe problem during the training phase or construction of the\nextended na\u0131\u00a8ve Bayes classifier is to decide to which of its\nsubclasses an object of a specific class should be assigned.\nThe problem will be treated in the following section.\n3. Incremental Learning and Evolving Ex-\ntended Na\u0131\u00a8ve Bayes Classifiers\nOnline learning from data streams should neither involve\nstoring all historic data nor re-initiating the learning proce-\ndure from scratch, when new data objects arrive. Learn-\ning strategies that rely directly on the information given by\nwrong and correct classifications of the single data objects\nare usually not well-suited for online learning and need spe-\ncial adaptations. A typical example for classifiers working\non this basis are most of the fuzzy classifiers [9]. Learning\nin decision trees is mainly based on a suitable impurity mea-\nsure like entropy. Since entropy is based on discrete proba-\nbility distributions in the case of decision trees, these proba-\nbility distributions can be updated in an incremental fashion\nand corresponding approaches to online learning for deci-\nsion trees can be derived [6, 7]. However, this idea causes\nproblems, when continuous attributes are considered for a\ndecision tree and the splitting\/discretisation of the contin-\nuous attributes should be carried out in an online fashion\nwithout storing all the data. For probabilistic models, when\nthey use some general weighted mean concept to estimate\ntheir parameters, online learning is very easy to be imple-\nmented, as we will see in the following.\n3.1. Advantages of Weighted Mean Concepts\nA mean value\n\u0000 \u0001\u0003\u0002\u0005\u0004 \u0006\b\u0007\n\f\n\u0000\n\u0006\n\t\n\u0006\n\u0004 \u000b\n\u0000\n\u0001\n\u0004\ncan be updated easily\nin an incremental manner by\n\u0000 \u0001\u0003\u0002\n\u0006\b\f\n\u0000\n\f \r\n\r\u000f\u000e\u0011\u0010\n\u0000 \u0001\u0003\u0002\n\u0006\n\u000e\n\u0010\n\r\u000f\u000e\u0011\u0010\n\u0001\n\u0006\b\f\n\u0000\n,\n(6)\nTo update a mean value, it is sufficient to know the mean\nvalue of the previous observations, the new observation and\nthe number of observations. Note that the concept of a mean\nvalue, respectively its estimation, is much more general. It\napplies also to derived concepts, especially to statistical mo-\nments. This fact can be exploited to compute variances and\ncovariances in an online fashion. For the empirical variance,\nwe have\n\u0012\n\u0011\n\u0012\n\u0006\n\f \u0010\n\r\u0014\u0013\u0015\u0010\n\u0006\n$\n\u0004 \u000b\n\u0000\n\u0003\n\u0001\n\u0004\n\u0013\n\u0000 \u0001\u0003\u0002\n\u0006\n\n\n\u0012\n\f \r\n\r\u0014\u0013\u0015\u0010\u0017\u0016\n\u0000 \u0001\n\u0012\n\u0002\n\u0006\n\u0013\n\u0000 \u0001\u0003\u0002\n\u0012\n\u0006\u0019\u0018\n,\n(7)\nThis means, the variance only involves the computation of\nthe two means\n\u0000 \u0001\n\u0012\n\u0002\n\u0006\nand\n\u0000 \u0001\u0003\u0002\n\u0006\n, i.e. the first and second mo-\nment.\nThe general idea of the Bayesian approach in statis-\ntics is to update probabilities based on new observations.\nTherefore, it is no wonder that there exist many incremen-\ntal learning approaches in the spirit of the general Bayesian\nidea [1, 5]. These concepts are applied to many statisti-\ncal methods like linear discriminant analysis [11], but also\nto Bayesian networks [4], classifiers [13] and especially to\nna\u0131\u00a8ve Bayes classifiers [12].\nSo far, we have only focused on incremental learning.\nEvolving systems do not only learn in an online fashion, but\nalso adapt and change their structure while new data arrive.\nIt is also important to notice changes in the data stream. A\nsimple incremental strategy as (6) will slow down the learn-\ning process with an increasing number of data. The under-\nlying assumption in (6) is that all data objects contribute\nthe same information to the actual system or classifier, no\nmatter how old these data objects are.\n(6) is nothing else than a specific convex combination of\nthe previous mean\n\u0000 \u0001\u0003\u0002\n\u0006\nand the new observation\n\u0001\n\u0006\b\f\n\u0000\n. One\ncan also think of other convex combinations, for instance\n\u0000 \u0001\u0003\u0002\n\u0006\b\f\n\u0000\n\f \u0003\n\u0010\u001a\u0013fiff\n\n\u0011\u0010\n\u0000 \u0001\u0003\u0002\n\u0006\n\u000eflff\n\u0010\n\u0001\n\u0006\b\f\n\u0000 (8)\nwith a fixed constant\nff\nfl\n\u0000\n \n\u0017\n\u0010\n\u0002\n. This corresponds to the\nwell known idea of exponential decay of the information.\nAfter ffi new observations the contribution of an observation\nto the mean will decrease in an exponential fashion to\n\u0003\n\u0010\u001a\u0013\nff\n\n\n\u0001\n\u0010\nff\n. In terms of the Bayesian idea, this means that the\nprobability for a new observation equal or similar to the last\none is\nff\n, whereas the probability for an observation equal\nor similar to one that lies ffi steps back is\n\u0003\n\u0010\u001f\u0013fiff\n\n\n\u0001\n\u0010\nff\n.\nOne can also realise a sliding window concept of size ffi\nfor the mean easily, if it is possible to store a fixed number of\nffi data objects for the classifier. In the sense of the Bayesian\napproach this means that new observations are expected to\nfollow the same distribution as the previous ffi ones.\n3.2. Incremental Learning for Extended Na\u0131\u00a8ve\nBayes Classifiers\nThe above mentioned concepts can be applied to a na\u0131\u00a8ve\nBayes classifier in a straight forward manner. The prior\nprobabilities\n\u0002\u0004\u0003\n\u0013\n\n\nfor the classes are nothing else than mean\nvalues for counting variables, so that (6) can be used. The\nsame applies to the conditional probabilities\n\u0002\u0004\u0003\n\u0000\u000f\u0004\n\u0007\n\u0013\n\n\nfor\ndiscrete attributes\n\u0000\u0002\u0004\n.\nFor continuous attributes it is necessary to assume a cer-\ntain type of distribution. When the distribution is deter-\nmined by a finite number of its moments, then again (6)\nis applicable. For instance, in the case of normal distribu-\ntions, we can estimate the first and second moment (as spe-\ncific mean values) in an incremental fashion and compute\nthe variance from (7). It should be noted that it is not nec-\nessary to consider complicated distributions, for instance\nGaussian mixtures. The reason is that we use an extended\nna\u0131\u00a8ve Bayes classifier and a Gaussian mixture could be rep-\nresented by usual normal distributions for a corresponding\nnumber of pseudo-classes.\nWhen the incremental update is carried out according to\n(6), incremental learning will yield the same result as batch\nlearning. Of course, we can also apply the \u201dexponential for-\ngetting\u201d (8) or a sliding window concept that are not equiv-\nalent to batch learning, since they treat the data in an asym-\nmetric way.\nThere is one open question for the extended na\u0131\u00a8ve Bayes\nclassifier that we have not answered so far. It is not clear\nwhich class to choose for updating the corresponding prob-\nability distributions. For a standard na\u0131\u00a8ve Bayes classifier,\nthe probability distributions for the class of the new ob-\nserved object are updated. But since in an extended na\u0131\u00a8ve\nBayes classifier one class might be represented by a number\nof pseudo-classes, we have to choose one of the pseudo-\nclasses. In case, the extended na\u0131\u00a8ve Bayes classifier has\npredicted the correct class for the new observed object, we\nupdate the probability distributions of the corresponding\npseudo-class that had yielded the highest likelihood. We\ncould also choose this pseudo-class in case of a misclassi-\nfication. Note that in this case, we would not choose the\npseudo-class with the highest likelihood from all pseudo-\nclasses, but only from those that represent the correct class.\nHowever, we slightly deviate from this concept in case of\nmisclassification. The reason is the way we construct the\npseudo-classes in an online fashion as described in the fol-\nlowing.\n3.3. Evolving Extended Na\u0131\u00a8ve Bayes Classifiers\nSo far, we have only described incremental learning but\nno adaptation that is expected from an evolving system. We\ninitialise the extended na\u0131\u00a8ve Bayes classifier as a standard\nna\u0131\u00a8ve Bayes classifier, i.e. we start with one pseudo-class\nper class. New pseudo-classes are introduced, when the\nmisclassification rate or the average loss is too high. Since\nthe misclassification rate and the average loss per class are\nalso weighted mean concepts, they can be tracked in an on-\nline fashion as well.\nWhen then misclassification rate or the average loss is\ntoo high and we decide to introduce a new pseudo-class, we\nassign the pseudo-class to the class for which we have the\nhighest misclassification rate or the highest average loss, re-\nspectively. We also have to specify initial probability distri-\nbutions for the attributes for the new pseudo-class and also\nthe prior probability for the new pseudo-class.\nFor the introduction of new pseudo-classes, we apply the\nfollowing strategy. When we create the initial na\u0131\u00a8ve Bayes\nclassifier, we already introduce for each class a second hid-\nden pseudo-class. These pseudo-classes are not used for\nprediction. But we update the probability distributions for a\nhidden pseudo-class each time a misclassification occurred\nfor an object of the associated class. In this way, the hid-\nden pseudo-classes can already start to adapt their proba-\nbility distributions to those objects that are misclassified,\nalthough they do not participate in the classification pro-\ncess. When a hidden pseudo-class is added to the actual\npseudo-classes, because of an unacceptable high misclassi-\nfication rate or average loss for the associated class, we sim-\nply use the probability distributions for the attributes that\nwere computed for the hidden class. The prior probability\nfor the new pseudo-class is calculated as follows. Note that\nwe cannot simply use the tracked prior probability of the\nhidden pseudo-class, since otherwise the sum of the prior\nprobabilities of all classes would exceed one. The prior\nprobability of the associated class is the sum of all prior\nprobabilities of the corresponding pseudo-classes. Now we\ninclude the corresponding hidden class, without increasing\nthe prior probability of the associated class. Assume that\nthe pseudo-classes associated with the corresponding class\nhave prior probabilities \u0000 \u0000 \u0017\n,\u0012,\u0012,\n\u0017\u0001\u0000\u0003\u0002 and that the prior prob-\nability for the hidden pseudo-class was calculated as \u0000\u0004\u0002\n\f\n\u0000\n.\nThis means that the prior probability of the associated class\nis \t\n\u0002\n\u0004 \u000b\n\u0000\n\u0000\n\u0004\n. We define the new prior probabilities for the\npseudo-classes by\n\u0000\n(new)\u0004\n\f\n\u0000\n\u0004\n\u0010\n\u0005\n\u0003\u0007\u0006 \f\n\u0010\n\u0017\n,\u0012,\u0012,\n\u0017\t\b\n\u000e\u0011\u0010\n\n\nwhere\n\u0005\n\f\n\t\n\u0002\n\u0004 \u000b\n\u0000\n\u0000\n\u0004\n\t\n\u0002\n\f\n\u0000\n\u0004 \u000b\n\u0000\n\u0000\n\u0004\n,\nThis will give at least a small chance to the added pseudo-\nclass to yield the highest likelihood. When we move a hid-\nden pseudo-class to the extended na\u0131\u00a8ve Bayes classifier, we\nintroduce a new hidden pseudo-class associated with the\nsame class to the hidden pseudo-classes. The prior prob-\nabilities of this new hidden pseudo-class are initialised with\nstandard parameters (uniform distributions for categorical\nattributes, standard normal distributions for continuous at-\ntributes. The prior probability is set to zero.\nAs mentioned above, when a new data object is classi-\nfied correctly, we update the probability distributions of the\npseudo-classes of the extended na\u0131\u00a8ve Bayes classifier yield-\ning the highest likelihood. When the new object is classi-\nfied incorrectly by the extended na\u0131\u00a8ve Bayes classifier, we\nupdate the probability distributions of that pseudo-class as-\nsociated with the correct class that was last added to the\nextended na\u0131\u00a8ve Bayes classifier.\nWhen we have moved a hidden pseudo-class to the ex-\ntended na\u0131\u00a8ve Bayes classifier, we do not immediately move\nanother hidden pseudo-class to the extended na\u0131\u00a8ve Bayes\nclassifier after the next observation, since in most cases the\nmisclassification rate or average loss will not drop imme-\ndiately after introducing a hidden pseudo-class to the ex-\ntended na\u0131\u00a8ve Bayes classifier. Before we move another hid-\nden pseudo-class to the extended na\u0131\u00a8ve Bayes classifier, we\nwait a fixed number of new observations in order to give the\nmodified classifier a chance to adapt its parameters.\n4. An Application Example\nIn order to illustrate how our approach works, we con-\nsider a modified version of the iris data set. The original\niris data set contains three classes that are roughly grouped\ninto three clusters. We artificially join two of the classes, so\nthat one class is represented by two clusters. We then apply\nour incremental evolving extended na\u0131\u00a8ve Bayes classifier to\na stream of randomly drawn samples from the iris data set.\nThe classifier constructs four pseudo-classes, two for each\nclass, i.e. one more than expected. Figure 2 shows the cor-\nresponding normal distributions. Although the distributions\ndo not completely look like one would expect, the misclas-\nsification rate is quite low (4%). The left normal distribution\nfor the first attribute and the corresponding pseudo-class is\nalmost not used by the classifier.\n5. Conclusions\nIn this paper, we have proposed an extended version of a\nna\u0131\u00a8ve Bayes classifier that is able to learn in an incremental\nfashion and to extend its structure automatically, when the\ndata from the data stream cannot be classified well enough.\nFuture work will include concepts to reduce the number of\npseudo-classes. Pseudo-classes with an extremely low prior\nprobability can be removed. Also pseudo-classes with sim-\nilar probability distributions can be joint together. Here a\n\u0000\n\u0012 -test could be applied.\nWe also plan to incorporate ideas from semi-supervised\nonline learning [2], in case the classification is not available\nfor all data.\nFigure 2. The normal distributions for the four\npseudo-classes resulting from the modified\niris data set.\nReferences\n[1] J. Anderson and M. Matessa. Explorations of an incremen-\ntal, Bayesian algorithm for categorization. Machine Learn-\ning, 9:275\u2013308, 1992.\n[2] P. Angelov and D. Filev. Flexible modells with evolv-\ning structure. International Journal on Intelligent Systems,\n19:327\u2013340, 2004.\n[3] C. Blake and C. Merz. UCI repos-\nitory of machine learning databases.\nhttp:\/\/www.ics.uci.edu\/ \u0001 mlearn\/MLRepository.html,\n1998.\n[4] N. Friedman and M. Goldszmidt. Sequential update of\nBayesian network structure. In Proc. 13th Conference on\nUncertainty in Artificial Intelligence, pages 165\u2013174, San\nFrancisco, 1997. Morgan Kaufmann.\n[5] J. Gama. Iterative Bayes. Theoretical Computer Science,\n292:417\u2013430, 2003.\n[6] D. Kalles and T. Morris. Effcient incremental induction of\ndecision trees. Machine Learning, 24:231\u2013242, 1996.\n[7] D. Kalles and A. Papagelis. Stable decision trees: Using lo-\ncal anarchy for effcient incremental learning. International\nJournal on Artificial Intelligence Tools, 9:79\u201395, 2000.\n[8] A. Klose. Partially Supervised Learning of Fuzzy Classi-\nfication Rules. Ph.D. thesis, Otto-von-Guericke-Universita\u00a8t\nMagdeburg, 2004.\n[9] D. Nauck, F. Klawonn, and R. Kruse. Neuro-Fuzzy Systems.\nWiley, Chichester, 1997.\n[10] A. Nu\u00a8rnberger, C. Borgelt, and A. Klose. Improving naive\nBayes classifiers using neuro-fuzzy learning. In Proc. 6th\nInternational Conference on Neural Information Processing\n(ICONIP99), page 154159, Perth, 1999.\n[11] S. Pang, S. Ozawa, and N. Kasabo. Incremental linear dis-\ncriminant analysis for classification of data streams. IEEE\nTransactions on Systems, Man, and Cybernetics \u2013 Part B,\n35:905\u2013914, 2005.\n[12] J. Roure. Incremental learning of tree augmented naive\nBayes classifiers. In F. Garijo, J. Riquelme, and M. Toro,\neditors, Proc. 8th Ibero-American Conference of Artificial\nIntelligence: IBERAMIA 2002, pages 32\u201341, Berlin, 2002.\nSpringer.\n[13] M. Singh and M. Gregory. Efficient learning of selective\nBayesian network classifiers. In L. Saitta, editor, Proc. 13th\nInternational Conference on Machine Learning, pages 453\u2013\n461, San Francisco, 1996. Morgan Kaufmann.\n[14] B. von Schmidt and F. Klawonn. Fuzzy max-min classifiers\ndecide locally on the basis of two attributes. Mathware and\nSoft Computing, 6:91\u2013108, 1999.\n[15] I. Witten and E. Frank. Data Mining. Morgan Kaufmann,\nSan Francisco, 2nd edition, 2005.\n"}