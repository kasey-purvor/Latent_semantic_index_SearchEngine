{"doi":"10.1007\/s11222-009-9129-8","coreId":"69507","oai":"oai:eprints.lancs.ac.uk:26391","identifiers":["oai:eprints.lancs.ac.uk:26391","10.1007\/s11222-009-9129-8"],"title":"Probabilistic relabelling strategies for the label switching problem in Bayesian mixture models.","authors":["Sperrin, Matthew","Jaki, Thomas","Wit, Ernst"],"enrichments":{"references":[{"id":1007840,"title":"Approaches to the label-switching problem of classi\ufb01cation, based on partitionspace relabeling and label-invariant visualization.","authors":[],"date":"2006","doi":null,"raw":"D. Farrar. Approaches to the label-switching problem of classi\ufb01cation, based on partitionspace relabeling and label-invariant visualization. Technical report, Statistical consulting center and department of statistics, Virginia Polytechnic, 2006. J. Geweke. Interpretation and inference in mixture models: simple MCMC works. Comp.","cites":null},{"id":1009159,"title":"Bayesian \ufb01nite mixtures with an unknown number of components: The allocation sampler.","authors":[],"date":"2007","doi":"10.1007\/s11222-006-9014-7","raw":"A. Nobile and A. T. Fearnside. Bayesian \ufb01nite mixtures with an unknown number of components: The allocation sampler. Stat. and Comput., 17(2):147\u2013162, 2007.","cites":null},{"id":1008423,"title":"Bayesian Inference for Mixture Models via Monte Carlo. PhD thesis,","authors":[],"date":"2005","doi":null,"raw":"A. Jasra. Bayesian Inference for Mixture Models via Monte Carlo. PhD thesis, Imperial College London, 2005.","cites":null},{"id":1010347,"title":"Bayesian Methods for Mixtures of Normal Distributions.","authors":[],"date":"1997","doi":null,"raw":"M. Stephens. Bayesian Methods for Mixtures of Normal Distributions. PhD thesis, University of Oxford, 1997a.","cites":null},{"id":1007094,"title":"Computational and inferential di\ufb03culties with mixture posterior distributions.","authors":[],"date":"2000","doi":"10.2307\/2669477","raw":"G. Celeux, M. Hurn, and C. P. Robert. Computational and inferential di\ufb03culties with mixture posterior distributions. J. Am. Stat. Assoc., 95:957\u2013970, 2000.","cites":null},{"id":1010126,"title":"Dealing with label-switching in mixture models.","authors":[],"date":"2000","doi":"10.1111\/1467-9868.00265","raw":"M. Stephens. Dealing with label-switching in mixture models. J. R. Stat. Soc. Ser. B, 62: 795\u2013809, 2000.","cites":null},{"id":1010634,"title":"Discussion of On Bayesian analysis of mixtures with an unknown number of components.","authors":[],"date":"1997","doi":"10.1111\/1467-9868.00095","raw":"M. Stephens. Discussion of On Bayesian analysis of mixtures with an unknown number of components. J. R. Stat. Soc. Ser. B, 59:768\u2013769, 1997b.","cites":null},{"id":1008128,"title":"Estimating mixtures of regressions.","authors":[],"date":"2007","doi":"10.1016\/j.csda.2006.08.014","raw":"Stat. and Data Analysis, 51:3529\u20133550, 2007. M. A. Hurn, A. Justel, and C. P. Robert. Estimating mixtures of regressions. J. Comp.","cites":null},{"id":1007692,"title":"Estimation of \ufb01nite mixture distributions through Bayesian sampling.","authors":[],"date":"1994","doi":null,"raw":"J. Diebolt and C. P. Robert. Estimation of \ufb01nite mixture distributions through Bayesian sampling. J. R. Stat. Soc. Ser. B, 56:363\u2013375, 1994.","cites":null},{"id":1008902,"title":"Finite Mixture Models.","authors":[],"date":"2000","doi":"10.1016\/0923-5965(95)00039-9","raw":"G. McLachlan and D. Peel. Finite Mixture Models. Wiley, 2000.","cites":null},{"id":1006785,"title":"Likelihood and Bayesian analysis of mixtures.","authors":[],"date":"2001","doi":"10.1191\/147108201128212","raw":"M. Aitkin. Likelihood and Bayesian analysis of mixtures. Stat. Model., 1:287\u2013304, 2001. G. Celeux and J. Diebolt. The SEM algorithm: a probabilistic teacher algorithm derived from the em algorithm for the mixture problem. Comput. Stat. Q., 2:73\u201382, 1985.","cites":null},{"id":1008578,"title":"Markov chain monte carlo methods and the label switching problem in Bayesian mixture modelling.","authors":[],"date":"2005","doi":"10.1214\/088342305000000016","raw":"A. Jasra, C. C. Holmes, and D. A. Stephens. Markov chain monte carlo methods and the label switching problem in Bayesian mixture modelling. Stat. Sci., 20:50\u201367, 2005.REFERENCES 17 J. M. Marin, K. L. Mengersen, and C. P. Robert. Bayesian modelling and inference on mixtures of distributions. Elsevier, 2005.","cites":null},{"id":1007416,"title":"Maximum likelihood from incomplete data via the EM algorithm (with discussion).","authors":[],"date":"1977","doi":null,"raw":"A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm (with discussion). J. R. Stat. Soc. Ser. B, 39:1\u201338, 1977.","cites":null},{"id":1009819,"title":"On Bayesian analysis of mixtures with an unknown number of components.","authors":[],"date":"1997","doi":"10.1111\/1467-9868.00095","raw":"S. Richardson and P. J. Green. On Bayesian analysis of mixtures with an unknown number of components. J. R. Stat. Soc. Ser. B, 59:758\u2013764, 1997. With discussion.","cites":null},{"id":1009790,"title":"Probes of large-scale structure in the Corona Borealis region.","authors":[],"date":"1986","doi":"10.1086\/114257","raw":"M. Postman, J. P. Huchra, and M. J. Geller. Probes of large-scale structure in the Corona Borealis region. Astro. J., 92:1238\u20131246, 1986.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-07","abstract":"The label switching problem is caused by the likelihood of a Bayesian mixture model being invariant to permutations of the labels. The permutation can change multiple times between Markov Chain Monte Carlo (MCMC) iterations making it di\ufb03cult to infer component-speci\ufb01c parameters of the model. Various so-called \u2018relabelling\u2019 strategies exist with the goal to \u2018undo\u2019 the label switches that have occurred to enable estimation of functions that depend on component-speci\ufb01c parameters. Most existing approaches rely upon specifying a loss function, and relabelling by minimising its posterior expected loss. In this paper we develop probabilistic approaches to relabelling that allow estimation and incorporation of the uncertainty in the relabelling process. Variants of the probabilistic relabelling algorithm are introduced and compared to existing loss function based methods. We demonstrate that the idea of probabilistic relabelling can be expressed in a rigorous framework based on the EM algorithm","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/69507.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/26391\/1\/labels%2Dsc%2Dtechrep%2Dapr09.pdf","pdfHashValue":"190865713fe3689d83be3fc7e795da55149d0acc","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:26391<\/identifier><datestamp>\n      2018-01-24T02:48:15Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Probabilistic relabelling strategies for the label switching problem in Bayesian mixture models.<\/dc:title><dc:creator>\n        Sperrin, Matthew<\/dc:creator><dc:creator>\n        Jaki, Thomas<\/dc:creator><dc:creator>\n        Wit, Ernst<\/dc:creator><dc:subject>\n        QA Mathematics<\/dc:subject><dc:description>\n        The label switching problem is caused by the likelihood of a Bayesian mixture model being invariant to permutations of the labels. The permutation can change multiple times between Markov Chain Monte Carlo (MCMC) iterations making it di\ufb03cult to infer component-speci\ufb01c parameters of the model. Various so-called \u2018relabelling\u2019 strategies exist with the goal to \u2018undo\u2019 the label switches that have occurred to enable estimation of functions that depend on component-speci\ufb01c parameters. Most existing approaches rely upon specifying a loss function, and relabelling by minimising its posterior expected loss. In this paper we develop probabilistic approaches to relabelling that allow estimation and incorporation of the uncertainty in the relabelling process. Variants of the probabilistic relabelling algorithm are introduced and compared to existing loss function based methods. We demonstrate that the idea of probabilistic relabelling can be expressed in a rigorous framework based on the EM algorithm.<\/dc:description><dc:date>\n        2010-07<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/26391\/1\/labels%2Dsc%2Dtechrep%2Dapr09.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1007\/s11222-009-9129-8<\/dc:relation><dc:identifier>\n        Sperrin, Matthew and Jaki, Thomas and Wit, Ernst (2010) Probabilistic relabelling strategies for the label switching problem in Bayesian mixture models. Statistics and Computing, 20 (3). pp. 357-366. ISSN 0960-3174<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/26391\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1007\/s11222-009-9129-8","http:\/\/eprints.lancs.ac.uk\/26391\/"],"year":2010,"topics":["QA Mathematics"],"subject":["Journal Article","PeerReviewed"],"fullText":"Probabilistic relabelling strategies for the label switching\nproblem in Bayesian mixture models\nM. Sperrin T. Jaki E. Wit\nMay 6, 2009\nAbstract\nThe label switching problem is caused by the likelihood of a Bayesian mixture model\nbeing invariant to permutations of the labels. The permutation can change multiple\ntimes between Markov Chain Monte Carlo (MCMC) iterations making it difficult to in-\nfer component-specific parameters of the model. Various so-called \u2018relabelling\u2019 strategies\nexist with the goal to \u2018undo\u2019 the label switches that have occurred to enable estimation\nof functions that depend on component-specific parameters. Most existing approaches\nrely upon specifying a loss function, and relabelling by minimising its posterior expected\nloss. In this paper we develop probabilistic approaches to relabelling that allow esti-\nmation and incorporation of the uncertainty in the relabelling process. Variants of the\nprobabilistic relabelling algorithm are introduced and compared to existing loss func-\ntion based methods. We demonstrate that the idea of probabilistic relabelling can be\nexpressed in a rigorous framework based on the EM algorithm.\nKeywords: Bayesian, Identifiability, Label Switching, MCMC, Mixture Model.\n1 Introduction\nMixture models have been used as tools to model heterogeneity for over 100 years. Devel-\nopments in Markov Chain Monte Carlo (MCMC) methods [e.g. Diebolt and Robert, 1994]\nopened the door for mixture models in a Bayesian framework as they allow efficient ex-\nploration of posterior and predictive surfaces of these models. The use of these Bayesian\nmixture models has given rise to new problems, particularly when estimating component-\nspecific parameters of the model and interpreting marginal posterior densities.\nThe label switching problem arises as the components of the Bayesian mixture model\ncan be ordered arbitrarily. During one run of an MCMC sampler, the order of components\ncan change multiple times between iterations. To obtain a meaningful interpretation of the\ncomponents it is necessary to account for these changes, which has been called relabelling\n[e.g. Stephens, 2000]. Various functions of interest, such as recovery of the full mixture\nposterior and its associated moments, may be invariant to the labelling permutations. For\nthis type of inference, the label switching problem need not concern us. On many occasions,\nhowever, it is of interest to infer parameters that are specific to individual components of\n1\n2. Relabelling Strategies 2\nthe mixture model. This may be because the components of the model have some interpre-\ntation, in the sense of a one-to-one correspondence to true components in the population,\nor alternatively we may be using mixture models to carry out semi-parametric density\nestimation, and the purpose of the relabelling is to provide coherent estimates of the com-\nponents that make up the density estimate. In either case, we must find methods to \u2018relabel\u2019\nthe results of an MCMC run so that the components are in the same order at each iteration.\nA wide array of strategies exist in the literature for \u2018relabelling\u2019 MCMC output in an\nattempt to remove the label switching problem \u2014 we divide them here into three categories.\nIdentifiability constraints involve relabelling the output of the MCMC sampler so that the\nposterior obtained satisfies a constraint on the component parameters. The constraint is\nchosen such that exactly one relabelling satisfies the constraint at each iteration of the sam-\npler. Deterministic relabelling algorithms select a relabelling at each iteration of the MCMC\noutput that minimises the posterior expectation of some loss function. Naturally, a variety\nof loss functions have been considered by different authors. Probabilistic approaches are a\nrelatively new idea in which one acknowledges that there is uncertainty in the relabelling\nthat should be selected on each iteration of the MCMC output. In contrast, both identifi-\nability constraint and deterministic relabelling algorithms assume that the relabelling that\nhas been carried out is \u2018correct\u2019.\nThe contribution of this paper is to develop and extend the idea of probabilistic rela-\nbelling, which was introduced originally in Jasra [2005]. We frame probabilistic relabelling\nas an application of the EM algorithm, where the missing data is the order that the com-\nponents are in at each iteration of the MCMC. Two novel probabilistic algorithms based\non the stochastic EM (SEM) are developed.\nWe will proceed, in Section 2, by briefly describing some of the relabelling algorithms\ncurrently available, before we introduce new strategies for probabilistic relabelling. Section\n3 evaluates the performance of the strategies on observed as well as simulated data. We\nconclude with a discussion of the advantages and disadvantages of the various methods and\nsome future directions in Section 4.\n2 Relabelling Strategies\nSuppose n observations y1, . . . , yn are taken from a K-component mixture distribution\nwhere all the components have the same distributional form, with mixture-specific pa-\nrameters \u03b8 = (\u03b81, . . . ,\u03b8K), global parameters \u03b7 and mixing weights pi, summarised by\n\u03b3 = (pi1, . . . , piK ;\u03b81, . . . ,\u03b8K ;\u03b7). The mixture distribution for a single observation Yi is then\ngiven by\np (yi|\u03b3) =\nK\u2211\nk=1\npikfk (yi|\u03b8k,\u03b7)\n2. Relabelling Strategies 3\nwith K \u2265 1, pik > 0 (k = 1, 2, . . . ,K),\n\u2211K\nk=1 pik = 1 and fk(\u00b7|\u03b8k,\u03b7) is a density function\nparametrised by \u03b8k and \u03b7. For convenience we introduce latent variables zi, i = 1, . . . , n,\nwhere {zi = k} indicates membership of the observation yi to class k, with for i = 1, . . . , n,\nzi\ni.i.d.\u223c Multinomial{1, (pi1, . . . , piK) }\nConditional on belonging to class k, observation Yi will be distributed according to fk(\u00b7|\u03b8k,\u03b7),\nYi|(zi = k) \u223c fk(\u00b7|\u03b8k,\u03b7)\nEach zi is then an unknown categorical variable that denotes the subpopulation from which\nobservation yi originates. Bayesian inference can be conducted on such a model using\nMCMC (Diebolt and Robert [1994]). This proceeds, on each iteration r, by drawing a\nvector of component memberships z(r), and parameter estimates \u03b3(r), from the posterior.\nThroughout this paper, for ease of illustration we will assume that each fk(\u00b7) is a normal\ndistribution with mean \u00b5k and variance \u03c32k. For the priors we will use the hierarchical\n\u2018random beta\u2019 model in Richardson and Green [1997], following their suggestions on the\nhyperparameter choices. For the number of components K we use a Poisson(1) prior as\nargued for in Nobile and Fearnside [2007].\nLet SK denote the set of all permutations on {1, 2, . . . ,K}. The label switching problem\narises because the likelihood\np (y|\u03b3) =\nn\u220f\ni=1\n{\nK\u2211\nk=1\npi\u03bd(k)f\u03bd(k)\n(\nyi|\u03b8\u03bd(k),\u03b7\n)}\nis identical for all \u03bd \u2208 SK . If exchangeable priors are used (containing no component-\nspecific information) then the posterior has the same property, resulting in the posterior\nsurface having K! symmetric modes, each associated with a different labelling permutation\n\u03bd \u2208 SK . This is problematic because each iteration of the MCMC sampler r, r = 1, . . . , R,\nhas an associated permutation \u03bd(r) \u2208 SK . Then for r1 6= r2, it may be that \u03bd(r1) 6= \u03bd(r2),\ni.e. the sampler can move from one mode to another between iterations. This makes an\nergodic average estimate of a component-specific parameter, e.g.\nE[\u03b81] \u2248 1\nR\nR\u2211\nr=1\n\u03b8\n(r)\n1 (1)\nsomewhat meaningless. Indeed, if the chain is in equilibrium, then the estimate of E[\u03b8k]\nshould be the same for all k, since such a chain explores equally all the symmetric modes.\nThe idea of relabelling the MCMC output is to account for the permutations \u03bd(r), r =\n1, . . . , R, in such a way that an ergodic average estimate such as (1) is made meaningful.\nOf course, we generally have limited data, and can never say with certainty whether we\ntruly have agreement \u03bd(r1) = \u03bd(r2), for r1 6= r2. Indeed, in our view the \u03bd(r)s are themselves\nparameters with associated uncertainty. Define a relabelled posterior, q(\u00b7), as the posterior\n2.1 Identifiability Constraints 4\ndensity that we obtain when we attempt to account for the permutations \u03bd(r), r = 1, . . . , R\nacross the iterations of an MCMC sampler. This is not unique \u2014 firstly there are K!\nversions of it that correspond to applying a permutation \u03bd to the entire output of an MCMC\nto yield an equivalent answer. Secondly, we accept that it is not possible to find the \u2018correct\u2019\nrelabelled posterior due to the uncertainty in estimating the \u03bd(r)s \u2014 we approximate this\nby the various relabelling methods considered in this paper. A version of the relabelled\nposterior is then useful when one conducts component-specific inference.\n2.1 Identifiability Constraints\nThe first efforts to deal with the label-switching problem involve placing an Identifiability\nConstraint (IC) on the parameter space [e.g. McLachlan and Peel, 2000]. The idea is to\ndefine a restricted parameter space A such that there exists a unique permutation \u03bd\u2217 \u2208 SK\nthat satisfies\n(\u03b8\u03bd\u2217(1), . . . , \u03b8\u03bd\u2217(K)) \u2208 A, for component-specific parameters \u03b8k, k = 1, . . . ,K. The simplest\nexample in the normal distribution case is the constraint \u00b51 < \u00b52 < . . . < \u00b5K , or equally\nthe same constraint on the mixture proportions, or the component variances. More sophis-\nticated alternatives can be found, for example, in Marin et al. [2005].\nThis approach is simple and works well in many situations. Proposition 3.1 of Stephens\n[1997a] demonstrates that the relabelling for such a strategy can be carried out after the\nMCMC has run, provided the priors are exchangeable. Geweke [2007] notes that use of\nthe IC leads, asymptotically in n, to the correct marginals for the true parameter vector \u03b8\nbeing recovered, provided \u03b8 \u2208 A. Nevertheless, for finite n it is found that the parameter\nestimates are \u2018pushed apart\u2019, that is the difference between the parameters of adjacent com-\nponents is typically over-estimated [McLachlan and Peel, 2000]. This is a consequence of\nthe fact that we are effectively imposing a-priori that the joint prior of \u03b8 must satisfy the\nconstraint, despite originally imposing exchangeable priors, suggesting we know nothing to\ndistinguish the components of the mixture model. Moreover, it can be difficult to find a\nsensible subspace ,A, when the mixture-specific parameters are multidimensional.\n2.2 Deterministic Relabelling Algorithms\nThe idea of the relabelling algorithm is that we believe that the permutations \u03bd(r1) and\n\u03bd(r2) match (for r1 6= r2; r1, r2 \u2208 {1, 2, . . . , R}) when a characteristic about the r1th iteration\nunder permutation \u03bdr1 is \u2018close\u2019 to that characteristic of the r2th iteration under permutation\n\u03bdr2 . There is a vast literature on the application of such algorithms to the label switching\nproblem, all considering different characteristics about each iteration on which to measure\ncloseness, and how one does measure closeness. Stephens [1997a] and Celeux et al. [2000]\ngive methods where the characteristic is the estimates of the parameters on each iteration\nr, \u03b8(r). Stephens [2000] produces a method in which the characteristic is the matrix of\nallocation probabilities of the observations to each component of the mixture, P (r) whilst\nNobile and Fearnside [2007] measure closeness in the allocation vector Z(r).\n2.3 Probabilistic Relabelling Algorithms 5\nCall the characteristic on which we measure closeness C, and the measure of closeness\nbetween two characteristics at iterations r1 and r2 as L(C(r1), C(r2)), which is a loss function\nthat is large when the discrepancy between C(r1) and C(r2) is large. When we apply a\npermutation \u03bd(r) to iteration r we will write \u03bd(r)(C(r)).\nWe are not interested per se in pairwise closeness, but closeness of the characteristics\nacross the entire MCMC sample, {C(1), . . . , C(R)}, as we wish the entire sample to be rela-\nbelled \u2018correctly\u2019. To take this into account in an efficient manner, many of the relabelling\nalgorithms adopt a K-means style approach, which can be described in a general manner\nas follows:\n1. Choose C to minimise\n\u2211R\nr=1 L\n{\nC, \u03bd(r)(C(r))\n}\n. In the K-means analogy, view C as\nthe centroids of the clusters. In common with this analogy, C is usually calculated as\nthe ergodic average of the characteristics C(r), r = 1, . . . , R.\n2. For r = 1, . . . , R choose \u03bd(r) to minimize L\n{\nC, \u03bd(r)(C(r))\n}\n, which is equivalent to\nallocating the observations to the clusters. Stephens [2000] demonstrates that it is\nusually possible to achieve this quickly, using a variant of the transportation algorithm.\n3. Repeat 1 and 2 until an optimal solution is reached.\nThe algorithm should be run from multiple starting positions (initial permutations of the\nMCMC iterations) as it is only guaranteed to converge to a local maximum rather than\nthe global maximum (e.g. Stephens [2000]). The approach corresponds to minimising the\napproximate posterior expectation of the loss function L, with the approximation arising\nfrom averaging over the MCMC output. The iterative nature of the algorithm means that\nit must be run after the MCMC has completed.\nICs and relabelling algorithms have very similar goals, in that they assign meaning to\neach of the components. For example, under the IC considered above when we talk about the\nfirst component we mean \u2018the component with the smallest mean\u2019. Relabelling algorithms\nattempt to give components meaning by enforcing some form of stable behaviour between\niterations of the MCMC. If the goal of the inference is parameter estimation, it seems\nsensible to use an algorithm that stabilises the relabelled posterior of the parameters, using\nfor example the algorithm of Stephens [1997a]. Farrar [2006], however, takes the opposing\nview that one should relabel using a different feature than the one of statistical interest,\ne.g. relabel based on component allocations when interested in parameter estimates.\nA separate class of algorithms are the label invariant loss function approaches introduced\nby Celeux et al. [2000]. Here, the idea is to measure closeness between iterations of the\nMCMC without relying on labelling information. For example, one could consider pairwise\ncomparison of the allocation of observations to components [Hurn et al., 2003].\n2.3 Probabilistic Relabelling Algorithms\nProbabilistic relabelling is first introduced by Jasra [2005]. The idea is that the permutation\n\u03bd(r) that is associated with the rth iteration of the MCMC sampler is unknown. Therefore,\n2.3 Probabilistic Relabelling Algorithms 6\nthe permutation can be viewed as having the discrete density gr(\u03bd(r);\u03b3, y) over \u03bd(r) \u2208 SK ,\nconditional on the data, y, and the full vector of parameters, \u03b3. Jasra [2005] then shows,\nusing the strong law of large numbers, that one can estimate a quantity of interest h(\u00b7) via\nh(\u03b3) =\n1\nR\nR\u2211\nr=1\n\u2211\n\u03bd(r)\u2208SK\nh\n{\n\u03bd(r)(\u03b3(r))\n}\ng\u02c6r(\u03bd(r); \u03b3\u02c6, y) (2)\nwhere \u03bd(r)(\u03b3(r)) represents the parameter vector with the component specific parameters\npermuted by \u03bd(r). The function of interest h(\u00b7) may depend additionally or alternatively on\nthe allocation vector z.\nTo use this approach we need a way to estimate gr(\u00b7), and we also need to know in\nadvance the vector of true parameters \u03b3. Jasra [2005] gives various suggestions on how\neach of these issues may be dealt with. For example, the parameters \u03b3 can be derived\nby averaging over a small number of iterations from the MCMC, determined by eye not\nto have switched labels. The permutation densities gr(\u00b7) are derived by estimating the\nposterior surface of the relabelled posterior using again a small number of iterations where\nthe labels are deemed not to have switched. This uses a normal approximation, and the\nidea of estimating the relabelled posterior to deal with label switching was first suggested\nby Stephens [1997b].\nNext we introduce a novel approach to probabilistic relabelling, in which gr(\u00b7) and \u03b3\nare estimated in an iterative fashion. An EM-type approach is adopted, where the missing\ndata are the permutations {\u03bd(r), r = 1, . . . , R} applied at each stage. The permutation\ndensities, gr(\u00b7), are estimated by conditioning only on the data, y, the current estimate of\nthe parameters, \u03b3, and the current allocation vector, z(r). Letting Srk = {i : z(r)i = k} be\nthe set of indices of the observations belonging, before permutation, to the kth parameter\nat iteration r, we calculate\ng\u02c6r(\u03bd(r); \u03b3\u02c6, y, z(r)) \u221d\nK\u220f\nk=1\n\u220f\ni\u2208Srk\np\u02c6i\u03bd(k)f\u03bd(k)\n(\nyi|\u03b8\u02c6\u03bd(k), \u03b7\u02c6\n)\n(3)\nwhere the right hand side corresponds to the allocated likelihood. So rather than using a\nnormal approximation to the surface of the relabelled posterior, gr(\u00b7) is estimated based\non the allocated likelihood of the data under each permutation, the current estimate of the\nparameters (permuted according to the permutation under consideration) and the current\nallocation vector z(r). Finally g\u02c6r(\u00b7) is normalised to sum to one over all possible permuta-\ntions. A detailed derivation of (3) is given in the Appendix.\nThe usual application of the EM algorithm [Dempster et al., 1977] to the mixture prob-\nlem views the available data as the observations, and the missing data the membership of\nthe observations to the various components. The framework introduced here, on the other\nhand, can be interpreted as an EM algorithm where the available data are the output from\nthe MCMC sampler, and the missing data are the permutations {\u03bd(r), r = 1, . . . , R} ap-\nplied at each stage. One could loosely consider the approaches suggested by Jasra [2005] as\ncorresponding to a single iteration of such an EM algorithm, with sensible starting values\n2.3 Probabilistic Relabelling Algorithms 7\nchosen. We propose now a variety of extensions and alternatives that stem from placing\nprobabilistic relabelling in this framework. We suggest first an iterative EM algorithm,\nwhich proceeds, after initialising estimates of the parameters \u03b3 using, for example, an IC,\nby:\nE Step Estimate the densities {gr(\u00b7), r = 1, . . . , R} using the current estimate of \u03b3, via\n(3).\nM step Update estimates of \u03b3 using (2), with appropriate choices of h(\u00b7). For example,\nthe component weight pi1 may be updated by\np\u02c6i1 =\n1\nR\nR\u2211\nr=1\n\u2211\n\u03bd(r)\u2208SK\npi\n(r)\n1 g\u02c6r(\u03bd\n(r); \u03b3\u02c6, y,z(r))\nAs with all EM-type algorithms, convergence to the global maximum is not guaranteed\n\u2014 local modes or saddle points may instead be found. Therefore it is advised to use\nmultiple starting points (different estimates of \u03b3). We call this EM approach \u2018EMP\u2019 (EM\nbased probabilistic relabelling).\nA popular alternative to the EM algorithm is the stochastic EM algorithm (SEM)\n[Celeux and Diebolt, 1985]. This introduces an extra step \u2018the S step\u2019, where the missing\ndata is simulated from its estimated density. This constitutes drawing \u03bd(r) multinomially\nfrom the discrete density gr(\u00b7). The randomness that this modification introduces helps to\navoid the algorithm getting caught in local modes, and provides faster convergence. Addi-\ntionally, the convergence of the SEM does not depend on the starting position [Celeux and\nDiebolt, 1985]. A SEM-type probabilistic relabelling strategy is as follows:\nE step Estimate the densities {gr(\u00b7), r = 1, . . . , R} using the current estimate of \u03b3, via (3).\nS step Simulate values for the permutations {\u03bd(r), r = 1, . . . , R} by drawing multinomially\nfrom the corresponding densities gr(\u00b7), calling these simulated permutations \u03bd\u02dc(r), r =\n1, . . . , R.\nM step Update estimates of \u03b3 by taking ergodic averages over the sample after accounting\nfor the permutations \u03bd\u02dc(r), r = 1, . . . , R. For example, the component weight pi1 may\nbe updated by\np\u02c6i1 =\n1\nR\nR\u2211\nr=1\npi\n(r)\n1\nafter the inverse of \u03bd\u02dc(r) has been applied at each r.\nWe call this approach \u2018SEMP\u2019 (SEM based probabilistic relabelling).\nA final alternative that we suggest acknowledges that \u03b3 is itself unknown. We con-\nsider estimating the permutation densities gr(\u00b7), r = 1, . . . , R without conditioning on \u03b3 by\n2.4 Comments 8\nintegrating \u03b3 out with respect to its relabelled posterior, q(\u03b3):\ng\u02c6r(\u03bd(r); y, z(r)) \u221d\n\u222b K\u220f\nk=1\n\u220f\ni\u2208Srk\np\u02c6i\u03bd(k)f\u03bd(k)\n(\nyi|\u03b8\u02c6\u03bd(k), \u03b7\u02c6\n)\nq(\u03b3)d\u03b3\u02c6\nand approximate the integral by the Monte Carlo estimate over the MCMC sample\ng\u02c6r(\u03bd(r); y, z(r)) \u221d 1\nR\nR\u2211\nr=1\n\uf8f1\uf8f2\uf8f3\nK\u220f\nk=1\n\u220f\ni\u2208Srk\npi\n(r)\n\u03bd(k)f\u03bd(k)\n(\nyi|\u03b8(r)\u03bd(k),\u03b7(r)\n)\uf8fc\uf8fd\uf8fe (4)\nThis leads to the algorithm\nE step Estimate the densities {gr(\u00b7), r = 1, . . . , R} using the current estimate of the rela-\nbelled posterior density of \u03b3, via (4).\nS step Simulate values for the permutations {\u03bd(r), r = 1, . . . , R} by drawing multinomially\nfrom the corresponding densities gr(\u00b7), calling these simulated permutations \u03bd\u02dc(r), r =\n1, . . . , R.\nM step Estimate the relabelled posterior density, q(\u03b3), using the output from the MCMC\nand the current estimates \u03bd\u02dc(r), r = 1, . . . , R.\nThe M step is, therefore, fundamentally different from a usual EM or SEM algorithm \u2014\nwe estimate an entire posterior rather than point estimates of the parameters. We call this\napproach \u2018SEMUP\u2019 (SEM based unconditional probabilistic relabelling).\n2.4 Comments\nFor the remainder of the paper we will consider seven different relabelling strategies, the IC,\nthree deterministic relabelling algorithms and the three variants of probabilistic relabelling\nwe introduced in the previous section. The notation used for the methods is defined in\nTable 1.\nNotation Method Source\nIC Identifiability constraint McLachlan and Peel [2000]\nPL Parameter relabelling algorithm Stephens [1997a]\nCPL Class Probability relabelling algorithm Stephens [2000]\nAL Allocation vector relabelling algorithm Nobile and Fearnside [2007]\nEMP EM probabilistic Section 2.3\nSEMP SEM probabilistic Section 2.3\nSEMUP SEM unconditional probabilistic Section 2.3\nTable 1: Relabelling algorithms evaluated\nOne of the disadvantages of relabelling algorithms and ICs is that they apply a specific\npermutation \u03bd(r) at each iteration r, with no indication on how uncertain we are that this\n2.4 Comments 9\nparticular permutation is \u2018correct\u2019. Using probabilistic methods, uncertainty in relabelling\ncan be quantified by how close to one the probability of the most likely permutation being\ncorrect, max\n\u03bd(r)\n{\ng\u02c6r(\u03bd(r); \u03b3\u02c6, y, z(r))\n}\n, is, for each iteration of the MCMC.\nA further advantage of probabilistic relabelling is the improved recovery of the posterior\ntails, which are often truncated using other methods. Consider 50 simulated observations\nfrom 0.5N(0, 1) + 0.5N(2, 1). Figure 1 compares the marginal posteriors for \u00b51 (defined as\nthe component mean with smallest ergodic average) under the PL and SEMP methods, as-\nsuming that all parameters in the model are unknown. The distributions are quite different\nin shape with the right hand tail being truncated for the PL algorithm in comparison to the\nSEMP method, which is compensated by a higher peak. Similar results are observed in all\nthe probabilistic methods. This clearly shows the superior ability of probabilistic relabelling\nto recover posterior tails.\n\u22122 \u22121 0 1 2 3 4\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\nFigure 1: Graphs showing the posteriors for \u00b51 (defined as the \u00b5 with smallest ergodic\naverage) of the two component mixture model 0.5N(0, 1) + 0.5N(2, 1), for the PL (solid\nline) and SEMP (dashed line) algorithms.\n3. Comparison of Methods 10\n3 Comparison of Methods\nTo evaluate the proposed algorithms we will now compare them to existing methods on\nobserved and simulated data. The seven relabelling strategies that will be compared are\ngiven in Table 1.\n3.1 The Galaxy Data\nFor the initial comparison we investigate the galaxy data which consist of the velocities of\n82 different galaxies [Postman et al., 1986]. A histogram of the data is given in Fig. 2. This\ndataset has become the benchmark for testing different methods for analysis of mixture\ndata. See Jasra et al. [2005] for a recent investigation into the galaxy data in the mixture\nmodelling context.\nVelocity\nFr\neq\nue\nnc\ny\n10 15 20 25 30 35\n0\n5\n10\n15\n20\nFigure 2: Histogram of the velocities of 82 galaxies.\nAn MCMC run on this data spends at least 10% of its iterations in each of K =3, 4 and\n5 clusters suggesting that any of these choices could be sensible. We refer to Aitkin [2001]\nfor an interesting summary of the differing posteriors for K achieved using different, but\n3.1 The Galaxy Data 11\napparently similar, methods on this dataset. Here we will look in detail at the relabelling\nalgorithms applied to the K = 5 case. As this is a single data set, it is feasible to use all of\nthe output points from the MCMC for the SEMUP algorithm.\nA remarkable stability between the different methods can be found as they recover al-\nmost identical values to each other for all the parameters. Table 2 shows an example of\nthese results, the component mean of the fourth component, \u00b54. The mean changes between\nmethods, which is due to the difference in dealing with tails of the relabelled posterior by the\nvarious methods. Looking at the \u03b1-quantiles this is further illustrated by the fact that q0.05\nand q0.95 are rather different between the methods. This suggests that there are adjacent\ncomponents that are poorly separated. For a parameter in a well-separated component,\nsuch as the first component that accounts for the observations in the left-hand peak, almost\nidentical results for the \u03b1-quantiles are observed for each relabelling method.\nMethod Mean Posterior Quantiles\nq0.05 q0.25 q0.50 q0.75 q0.95\nIC 23.92 21.81 22.56 23.01 23.58 32.51\nPL 22.60 21.33 22.04 22.65 23.09 23.65\nCPL 22.39 21.09 21.83 22.43 23.00 23.45\nAL 22.49 21.20 21.94 22.55 23.04 23.62\nEMP 23.92 21.40 22.09 22.68 23.13 24.13\nSEMP 22.60 21.25 22.00 22.63 23.09 24.09\nSEMUP 23.37 16.39 22.21 22.88 23.44 34.60\nTable 2: Summary of estimated \u00b54 for different relabelling methods across all iterations of\nthe MCMC with K = 5. Here, \u00b54 is defined as the mean with the fourth smallest ergodic\naverage.\nConsequently the only major difference between the algorithms can be found in the\nvariance for the estimates of each parameter as the allocation of component estimates from\nthe tails has a large bearing on the estimated variances of the parameters.\nFigure 3 gives the probabilities of the two most likely permutations (calculated from (3))\nfor the Galaxy data with the number of components K = 3, 4, 5, for 100 thinned iterations\nin each case. We have used the SEMP relabelling procedure. For K = 3 and 4, there is little\nor no uncertainty over which permutation of the labels is selected. For K = 5, however,\nit is often the case that there are two permutations with reasonable probabilities of being\nselected. This suggests that there are two components that are virtually indistinguishable,\nwhich implies that it may be beneficial to merge them. In this way, there is potential to\nuse this method to help choose the number of components K.\n3.2 Simulated Data 12\n0 20 40 60 80 100\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\npr\nob\nab\nilit\ny\n0 20 40 60 80 100\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\npr\nob\nab\nilit\ny\n0 20 40 60 80 100\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\nIteration\npr\nob\nab\nilit\ny\nFigure 3: Graphs showing the probabilities of the most likely (solid line) and second most\nlikely (dashed line) permutations at 100 iterations of the MCMC sampler for the galaxy\ndata. The three graphs represent models with differing numbers of components \u2014 K = 3\n(top), K=4 (middle) and K=5 (bottom).\n3.2 Simulated Data\nFor a more thorough evaluation of the different relabelling algorithms we now turn to\nsimulated data. We investigated different simulations in which we draw n observations from\npiN(0, 1)+ (1\u2212pi)N(\u00b52, \u03c322), for various combinations of (n, pi, \u00b52, \u03c322). Each combination is\nrepeated 100 times and the results are averaged over these repeats in order to remove the\nimpact of individual data sets. Since it is computationally not feasible to use the SEMUP\nalgorithm with all available iterations of the MCMC we set the number of posterior points\nto 100 for use in (4). As well as giving estimates of parameters, we give a measure of\ncloseness of the estimated mixture distribution to the true density that we have simulated\nfrom, by simulating 106 values from the true density and estimating the Kullback-Leibler\ndistance via\n\u03d5 =\n104\n106\n106\u2211\ni=1\nlog\n{\nf(xi; \u03b8true)\/f(xi; \u03b8\u02c6)\n}\n3.2 Simulated Data 13\nwhere \u03b8\u02c6 is estimated via the various relabelling methods, and we have rescaled by 104 from\na usual average to give more readable results.\nFor situations where the difference between two components is large, that is when either\n\u00b52 was very different from zero or \u03c322 was very different from 1 (e.g. \u03c3\n2\n2 = 0.1 or \u03c3\n2\n2 = 10),\nall relabelling algorithms, unsurprisingly, performed well as label switching occurs rarely.\nWe therefore omit the details of these simulations and focus on situations where the two\ncomponents are very similar. Tables 3-5 provide the details of some of the most interesting\nsituations considered.\nFor the case where (pi, \u00b52, \u03c322) = (0.5, 2, 1) and the sample size is varied as n = 50 and\nn = 100 (Table 3) it is immediately striking that for all relabelling algorithms except the IC,\nthe estimates of \u00b51 and \u00b52 are pushed toward each other with the effect being strongest for\nthe CPL and AL methods, and a moderate effect for the probabilistic strategies. Further,\nfor all relabelling methods, the variances are severely over-estimated and neither feature is\nimproved by an increase sample size, even when raised to n = 500 (not shown).\nBoth of these problems can be attributed to posterior weight on the possibility of both\ncomponents being in the middle of the dataset with similar means and different variances.\nThis solution, however, yields a rather different interpretation of the components than the\none used to generate the data. The high standard deviation of the simulations indicates a\nhigh uncertainty in the \u2018correct\u2019 interpretation of the mixture distribution. In terms of the\npredictive error \u03d5, PL and the probabilistic approaches are best performers in both sample\nsizes.\nWhen looking at the results for very similar components (\u00b52 = 0.1, Table 4) we see the\nconverse feature of the average estimates of \u00b51 and \u00b52 being pushed apart from each other.\nThis is caused by the components being virtually indistinguishable so the MCMC responds\nby moving one component excessively to the left and the other excessively to the right.\nThese opposing results are an illustration of the limitations of using ergodic average esti-\nmates for the parameters. For this situation interestingly the CPL and AL method perform\nbetter than the other methods, while probabilistic relabelling methods are in the middle.\nIt is also interesting to see that, contrary to the previous set of situations, the estimates\nof the variance are more or less on target for all algorithms considered. In this case, the\npredictive error \u03d5 is minimised by CPL and AL, although SEMUP performs fairly well.\nIn Table 5 the components are more distinguishable (\u00b52 = 2), but the mixing weights are\nrather different, with pi = 0.1. In this case, \u00b51 and \u03c31 are both severely over-estimated while\n\u00b52 and \u03c32 are estimated accurately for all relabelling strategies with none of the methods\nappearing to be superior to the others. Additionally pi is also over-estimated strongly which\ncan be attributed to the asymmetry in the posterior distribution. The predictive error \u03d5 is\nsmallest for the PL method while it is largest for the IC.\n3.2 Simulated Data 14\nn \u03b8 IC PL CPL AL EMP SEMP SEMUP\n\u00b51 \u22120.02 (0.21) 0.19 (0.12) 0.64 (0.19) 0.65 (0.18) 0.20 (0.09) 0.20 (0.16) 0.34 (0.16)\n\u00b52 1.90 (0.12) 1.69 (0.03) 1.24 (0.09) 1.23 (0.08) 1.69 (0.01) 1.68 (0.06) 1.54 (0.06)\n50 \u03c321 1.63 (0.39) 1.62 (0.31) 1.58 (0.51) 1.58 (0.51) 1.62 (0.37) 1.63 (0.38) 1.66 (0.39)\n\u03c322 1.61 (0.47) 1.62 (0.55) 1.66 (0.34) 1.66 (0.34) 1.62 (0.48) 1.61 (0.48) 1.58 (0.46)\npi 0.50 (0.06) 0.51 (0.09) 0.49 (0.38) 0.49 (0.39) 0.51 (0.11) 0.46 (0.08) 0.47 (0.11)\n\u03d5 205 97 166 169 96 86 83\n\u00b51 0.07 (0.18) 0.23 (0.23) 0.58 (0.36) 0.58 (0.36) 0.27 (0.28) 0.28 (0.25) 0.30 (0.26)\n\u00b52 1.91 (0.19) 1.75 (0.23) 1.39 (0.39) 1.40 (0.38) 1.71 (0.30) 1.70 (0.29) 1.68 (0.27)\n100 \u03c321 1.52 (0.39) 1.52 (0.39) 1.50 (0.39) 1.50 (0.39) 1.51 (0.35) 1.52 (0.39) 1.52 (0.36)\n\u03c322 1.47 (0.36) 1.47 (0.37) 1.49 (0.36) 1.49 (0.36) 1.49 (0.40) 1.47 (0.35) 1.47 (0.39)\npi 0.50 (0.05) 0.50 (0.07) 0.49 (0.24) 0.49 (0.24) 0.50 (0.09) 0.50 (0.09) 0.49 (0.09)\n\u03d5 110 65 188 184 66 67 70\nTable 3: Average parameter estimates over 100 iterations for different relabelling strategies\nwhen (pi, \u00b51, \u00b52, \u03c321, \u03c3\n2\n2) = (0.5, 0, 2, 1, 1) for n = 50 and n = 100. Values in parentheses give\nthe standard deviations of the estimates.\n\u03b8 IC PL CPL AL EMP SEMP SEMUP\n\u00b51 \u22120.60 (0.24) \u22120.42 (0.28) \u22120.22 (0.30) \u22120.21 (0.30) \u22120.47 (0.30) \u22120.44 (0.28) \u22120.36 (0.29)\n\u00b52 0.67 (0.22) 0.47 (0.24) 0.27 (0.26) 0.27 (0.26) 0.52 (0.25) 0.50 (0.25) 0.42 (0.25)\n\u03c321 0.95 (0.25) 0.95 (0.31) 0.94 (0.26) 0.94 (0.26) 0.95 (0.24) 0.95 (0.27) 0.96 (0.28)\n\u03c322 0.92 (0.23) 0.91 (0.29) 0.92 (0.23) 0.92 (0.23) 0.92 (0.23) 0.91 (0.24) 0.91 (0.24)\npi 0.49 (0.09) 0.49 (0.15) 0.47 (0.29) 0.47 (0.29) 0.49 (0.14) 0.48 (0.14) 0.50 (0.15)\n\u03d5 211 37 0.4 0.4 66 50 18\nTable 4: Average parameter estimates over 100 iterations for different relabelling strategies\nwhen (pi, \u00b51, \u00b52, \u03c321, \u03c3\n2\n2) = (0.5, 0, 0.1, 1, 1) for n = 100. Values in parentheses give the\nstandard deviations of the estimates.\n\u03b8 IC PL CPL AL EMP SEMP SEMUP\n\u00b51 0.75 (0.40) 0.91 (0.47) 1.07 (0.55) 1.08 (0.55) 0.85 (0.45) 0.91 (0.49) 0.95 (0.49)\n\u00b52 2.32 (0.20) 2.17 (0.20) 2.00 (0.21) 1.99 (0.21) 2.22 (0.23) 2.16 (0.23) 2.12 (0.20)\n\u03c321 1.39 (0.36) 1.46 (0.46) 1.36 (0.43) 1.35 (0.42) 1.35 (0.36) 1.39 (0.38) 1.38 (0.41)\n\u03c322 1.02 (0.29) 0.95 (0.25) 1.05 (0.26) 1.05 (0.27) 1.05 (0.31) 1.02 (0.29) 1.02 (0.32)\npi 0.39 (0.10) 0.36 (0.12) 0.28 (0.18) 0.28 (0.18) 0.38 (0.12) 0.39 (0.13) 0.40 (0.14)\n\u03d5 184 51 57 59 125 106 110\nTable 5: Average parameter estimates over 100 iterations for different relabelling strategies\nwhen (pi, \u00b51, \u00b52, \u03c321, \u03c3\n2\n2) = (0.1, 0, 2, 1, 1) for n = 100. Values in parentheses give the standard\ndeviations of the estimates.\nOverall the results indicate that none of the methods compared are performing uni-\nformly better than any of the others leaving the ultimate decision on which method to use\nto the user. The CPL and AL methods are unstable in terms of the predictive error \u03d5 as\nthey perform well when the components are very hard to distinguish, but show poor perfor-\nmance when the components are more separated. Consistent results for \u03d5 are obtained for\n4. Discussion 15\nthe PL and the probabilistic methods. Based on these results it is, however, evident that\nthe use of ergodic averages can often be detrimental. Due to the large variation in the pa-\nrameter estimates we believe the SEMUP method is more appropriate as it is probabilistic\nand moreover avoids conditioning on the parameter estimates. It does, however, depend on\nthe accuracy of the approximation in (4) through the value of R.\n4 Discussion\nIn this paper we have developed a new class of probabilistic methods for the label switching\nproblem in Bayesian mixture models. The main advantages of these approaches are on the\none hand that the tails of the posterior distributions are recovered and on the other hand\nuncertainty associated with relabelling can be incorporated, features that are not present\nfor deterministic relabelling algorithms. The computation time of the probabilistic meth-\nods are either substantially lower than or on par with the existing deterministic methods\nwith the exception of the IC. It is shown through analysis of an observed dataset as well as\nsimulation that the parameter estimates obtained by probabilistic relabelling are virtually\nthe same as for the deterministic approaches suggesting that the above advantages come\nwithout any loss.\nWe also introduce an algorithm for probabilistic relabelling, called SEMUP, that does\nnot rely on ergodic average estimates of parameters as we integrate over a relabelled poste-\nrior. Although there is some additional computation required to approximate the relevant\nintegral that also introduced a trade-off between speed and accuracy, the additional time\nwas found to be reasonable for single datasets.\nDuring the evaluation of the methods it was pointed out that some information about\nthe choice of K, the number of components, can be derived from probabilistic relabelling al-\ngorithms. Although the full extent of the relevance of probabilistic relabelling for choosing\nK is still to be evaluated carefully, it does show promise. The uncertainty in the rela-\nbelling can be used as an indication that too many components are in the model, since high\nuncertainty in relabelling suggests that there is ambiguity between adjacent components,\nimplying that it may be better to merge them. Further work will need to be done to get a\nbetter understanding of this.\nA Derivation of (3)\nFirst, gr(\u03bdr; \u03b3\u02c6, y, z(r)) is defined as the probability that permutation \u03bdr is \u2018correct\u2019, given\nthe data y, the current estimate of the parameters \u03b3\u02c6, and the allocation vector z(r), for the\nrth iteration of the sampler. In an abuse of notation when we write \u03bdr henceforth we mean\n\u2018permutation \u03bdr is correct\u2019.\nREFERENCES 16\nThen\nP[\u03bdr|y, z(r), \u03b3\u02c6] = P[y|\u03bdr, z\n(r), \u03b3\u02c6]P[z(r)|\u03bdr, \u03b3\u02c6]P[\u03bdr|\u03b3\u02c6]\nP[y|\u03b3\u02c6, z(r)]P[z(r)|\u03b3\u02c6]\nNow, the terms in the denominator do not depend on \u03bdr, and we assume that each permu-\ntation is equally likely, so we are left with\nP[\u03bdr|y, z(r), \u03b3\u02c6] \u221dP[y|\u03bdr, z(r), \u03b3\u02c6]P[z(r)|\u03bdr, \u03b3\u02c6]\n=\nn\u220f\ni=1\nP[yi|\u03bdr, z(r)i , \u03b3\u02c6]P[z(r)i |\u03bdr, \u03b3\u02c6]\n=\nK\u220f\nk=1\n\u220f\ni\u2208Srk\np\u02c6i\u03bd(k)f\u03bd(k)\n(\nyi|\u03b8\u02c6\u03bd(k), \u03b7\u02c6\n)\nwhich is the form given in (3).\nReferences\nM. Aitkin. Likelihood and Bayesian analysis of mixtures. Stat. Model., 1:287\u2013304, 2001.\nG. Celeux and J. Diebolt. The SEM algorithm: a probabilistic teacher algorithm derived\nfrom the em algorithm for the mixture problem. Comput. Stat. Q., 2:73\u201382, 1985.\nG. Celeux, M. Hurn, and C. P. Robert. Computational and inferential difficulties with\nmixture posterior distributions. J. Am. Stat. Assoc., 95:957\u2013970, 2000.\nA. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data\nvia the EM algorithm (with discussion). J. R. Stat. Soc. Ser. B, 39:1\u201338, 1977.\nJ. Diebolt and C. P. Robert. Estimation of finite mixture distributions through Bayesian\nsampling. J. R. Stat. Soc. Ser. B, 56:363\u2013375, 1994.\nD. Farrar. Approaches to the label-switching problem of classification, based on partition-\nspace relabeling and label-invariant visualization. Technical report, Statistical consulting\ncenter and department of statistics, Virginia Polytechnic, 2006.\nJ. Geweke. Interpretation and inference in mixture models: simple MCMC works. Comp.\nStat. and Data Analysis, 51:3529\u20133550, 2007.\nM. A. Hurn, A. Justel, and C. P. Robert. Estimating mixtures of regressions. J. Comp.\nGraph. Stat., 12:55\u201379, 2003.\nA. Jasra. Bayesian Inference for Mixture Models via Monte Carlo. PhD thesis, Imperial\nCollege London, 2005.\nA. Jasra, C. C. Holmes, and D. A. Stephens. Markov chain monte carlo methods and the\nlabel switching problem in Bayesian mixture modelling. Stat. Sci., 20:50\u201367, 2005.\nREFERENCES 17\nJ. M. Marin, K. L. Mengersen, and C. P. Robert. Bayesian modelling and inference on\nmixtures of distributions. Elsevier, 2005.\nG. McLachlan and D. Peel. Finite Mixture Models. Wiley, 2000.\nA. Nobile and A. T. Fearnside. Bayesian finite mixtures with an unknown number of\ncomponents: The allocation sampler. Stat. and Comput., 17(2):147\u2013162, 2007.\nM. Postman, J. P. Huchra, and M. J. Geller. Probes of large-scale structure in the Corona\nBorealis region. Astro. J., 92:1238\u20131246, 1986.\nS. Richardson and P. J. Green. On Bayesian analysis of mixtures with an unknown number\nof components. J. R. Stat. Soc. Ser. B, 59:758\u2013764, 1997. With discussion.\nM. Stephens. Dealing with label-switching in mixture models. J. R. Stat. Soc. Ser. B, 62:\n795\u2013809, 2000.\nM. Stephens. Bayesian Methods for Mixtures of Normal Distributions. PhD thesis, Univer-\nsity of Oxford, 1997a.\nM. Stephens. Discussion of On Bayesian analysis of mixtures with an unknown number of\ncomponents. J. R. Stat. Soc. Ser. B, 59:768\u2013769, 1997b.\n"}