{"doi":"10.1093\/biomet","coreId":"216933","oai":"oai:eprints.lse.ac.uk:31549","identifiers":["oai:eprints.lse.ac.uk:31549","10.1093\/biomet"],"title":"Estimation of latent factors for  high-dimensional time series","authors":["Lam, Clifford","Yao, Qiwei","Bathia, Neil"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2011-12","abstract":"This paper deals with the dimension reduction of high-dimensional time series based on common factors. In particular we allow the dimension of time series p to be as large as, or even larger than, the sample size n. The estimation of the factor loading matrix and the factor process itself is carried out via an eigenanalysis of a p \u00a3 p non-negative de\u00afnite matrix. We show that when all the factors are strong in the sense that the norm of each column in the factor loading matrix is of the order p1=2, the estimator of the factor loading matrix is weakly consistent in L2-norm with the convergence rate independent of p. This result exhibits clearly that the `curse' is canceled out by the `blessing' of dimensionality. We also establish the asymptotic properties of the estimation when factors are not strong. The proposed method together with their asymptotic properties are further illustrated in a simulation study. An application to an implied volatility data set, together with a trading strategy derived from the \u00aftted factor model, is also reported","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/216933.pdf","fullTextIdentifier":"http:\/\/eprints.lse.ac.uk\/31549\/1\/__Libfile_repository_Content_Lam%2C%20C_Estimation%20of%20Latent%20Factors_Estimation%20of%20latent%20factors%20for%20high%28lsero%29.pdf","pdfHashValue":"75c1b2c076a25601bd64eaa5482c1fee3e4cff33","publisher":"Oxford University Press","rawRecordXml":"<record><header><identifier>\n  \n    \n    \n      oai:eprints.lse.ac.uk:31549<\/identifier><datestamp>\n      2014-05-30T10:46:56Z<\/datestamp><setSpec>\n      74797065733D4445505453:4C53452D5354<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lse.ac.uk\/31549\/<\/dc:relation><dc:title>\n        Estimation of latent factors for  high-dimensional time series<\/dc:title><dc:creator>\n        Lam, Clifford<\/dc:creator><dc:creator>\n        Yao, Qiwei<\/dc:creator><dc:creator>\n        Bathia, Neil<\/dc:creator><dc:subject>\n        HA Statistics<\/dc:subject><dc:description>\n        This paper deals with the dimension reduction of high-dimensional time series based on common factors. In particular we allow the dimension of time series p to be as large as, or even larger than, the sample size n. The estimation of the factor loading matrix and the factor process itself is carried out via an eigenanalysis of a p \u00a3 p non-negative de\u00afnite matrix. We show that when all the factors are strong in the sense that the norm of each column in the factor loading matrix is of the order p1=2, the estimator of the factor loading matrix is weakly consistent in L2-norm with the convergence rate independent of p. This result exhibits clearly that the `curse' is canceled out by the `blessing' of dimensionality. We also establish the asymptotic properties of the estimation when factors are not strong. The proposed method together with their asymptotic properties are further illustrated in a simulation study. An application to an implied volatility data set, together with a trading strategy derived from the \u00aftted factor model, is also reported.<\/dc:description><dc:publisher>\n        Oxford University Press<\/dc:publisher><dc:date>\n        2011-12<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lse.ac.uk\/31549\/1\/__Libfile_repository_Content_Lam%2C%20C_Estimation%20of%20Latent%20Factors_Estimation%20of%20latent%20factors%20for%20high%28lsero%29.pdf<\/dc:identifier><dc:identifier>\n          Lam, Clifford and Yao, Qiwei and Bathia, Neil  (2011) Estimation of latent factors for high-dimensional time series.  Biometrika, 98 (4).  pp. 901-18.  ISSN 0006-3444     <\/dc:identifier><dc:relation>\n        http:\/\/biomet.oxfordjournals.org\/<\/dc:relation><dc:relation>\n        10.1093\/biomet\/asr048<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lse.ac.uk\/31549\/","http:\/\/biomet.oxfordjournals.org\/","10.1093\/biomet\/asr048"],"year":2011,"topics":["HA Statistics"],"subject":["Article","PeerReviewed"],"fullText":"  \nClifford Lam, Qiwei Yao and Neil Bathia \nEstimation of latent factors for high-\ndimensional time series \n \nArticle (Accepted version) \n(Refereed) \n \n \n \nOriginal citation: \n \nLam, Clifford and Yao, Qiwei and Bathia, Neil (2011) Estimation of latent factors for high-dimensional time \nseries. Biometrika , 98 (4). pp. 901-18. ISSN 0006-3444  \n DOI: 10.1093\/biomet\/asr048 \n \n\u00a9 2011 Biometrika Trust \n \nThis version available at: http:\/\/eprints.lse.ac.uk\/31549\/ \nAvailable in LSE Research Online: January 2013  \n \nLSE has developed LSE Research Online so that users may access research output of the \nSchool. Copyright \u00a9 and Moral Rights for the papers on this site are retained by the individual \nauthors and\/or other copyright owners. Users may download and\/or print one copy of any \narticle(s) in LSE Research Online to facilitate their private study or for non-commercial research. \nYou may not engage in further distribution of the material or use it for any profit-making activities \nor any commercial gain. You may freely distribute the URL (http:\/\/eprints.lse.ac.uk) of the LSE \nResearch Online website.  \n \nThis document is the author\u2019s final manuscript accepted version of the journal article, \nincorporating any revisions agreed during the peer review process.  Some differences between \nthis version and the published version may remain.  You are advised to consult the publisher\u2019s \nversion if you wish to cite from it. \n \n \nEstimation of Latent Factors for\nHigh-Dimensional Time Series\nBy Clifford Lam, Qiwei Yao and Neil Bathia\nDepartment of Statistics, London School of Economics and Political Science\nHoughton Street, London WC2A 2AE, U.K.\nc.lam2@lse.ac.uk q.yao@lse.ac.uk n.bathia@lse.ac.uk\nSummary\nThis paper deals with the dimension reduction of high-dimensional time\nseries based on common factors. In particular we allow the dimension of time\nseries p to be as large as, or even larger than, the sample size n. The estima-\ntion of the factor loading matrix and the factor process itself is carried out via\nan eigenanalysis of a p\u00d7 p non-negative definite matrix. We show that when\nall the factors are strong in the sense that the norm of each column in the\nfactor loading matrix is of the order p1\/2, the estimator of the factor loading\nmatrix is weakly consistent in L2-norm with the convergence rate indepen-\ndent of p. This result exhibits clearly that the \u2018curse\u2019 is canceled out by the\n\u2018blessing\u2019 of dimensionality. We also establish the asymptotic properties of the\nestimation when factors are not strong. The proposed method together with\ntheir asymptotic properties are further illustrated in a simulation study. An\napplication to an implied volatility data set, together with a trading strategy\nderived from the fitted factor model, is also reported.\nShort Title: Estimation of Large Latent Time Series Factors.\nSome key words: Convergence in L2-norm; Curse and blessing of dimensionality; Dimen-\nsion reduction; Eigenanalysis; Factor model.\n1\n1 Introduction\nIn the modern information age, analysis of large data sets is an integral part of both\nscientific research and practical problem-solving. In particular, high-dimensional time se-\nries analysis is commonplace in many fields including, among others, finance, economics,\nenvironmental and medical studies. For example, understanding the dynamics of the\nreturns of large number of assets is the key to asset pricing, portfolio allocation, and\nrisk management. Panel time series are frequently encountered in studying economic and\nbusiness phenomena. Environmental time series are often of a high dimension because\nof the large number of indices monitored across many different locations. However the\nstandard multiple time series models such as vector AR or vector ARMA are not prac-\ntically viable when the dimension of time series p is high, as the number of parameters\ninvolved is in the order of p2. Furthermore, one may face a serious model-identification\nproblem in a vector ARMA model. In fact the vector ARMA model has hardly been used\nin practice without further regularization in its matrix coefficients. Therefore dimension-\nreduction is an important step in order to achieve an efficient and effective analysis of\nhigh-dimensional time series data. In relation to the dimension-reduction for independent\nobservations, the added challenge here is to retain the dynamical structure of time series.\nModeling by common factors is one of the most frequently used methods to achieve\ndimension-reduction in analyzing multiple time series. Early attempts in this direc-\ntion include Anderson (1963), Priestley et al. (1974), Brillinger (1981) and Pen\u02dca and Box\n(1987). To deal with the situations when the number of time series p is as large as, or\neven larger than, the length of the time series n, more recent efforts focus on the inference\nwhen p goes to infinity together with n. See, e.g. Chamberlain and Rothschild (1983),\nChamberlain (1983), Bai (2003) Forni et al. (2000, 2004, 2005). Furthermore, in analyz-\ning economic and financial phenomena, most econometric factor models seek to identify\nthe common factors such that each of them affects the dynamics of most the original p\ntime series. These common factors are separated from the so-called idiosyncratic noise\ncomponents; each idiosyncratic noise component may at most affect the dynamics of a\nfew original time series. Note that an idiosyncratic noise series is not necessarily white\nnoise. The rigorous definition of the common factors and the idiosyncratic noise can\nonly be established asymptotically when the number of time series p goes to infinity; see\nChamberlain and Rothschild (1983) and Chamberlain (1983). Hence those econometric\nfactor models are only asymptotically identifiable when p \u2192 \u221e. See also Forni et al.\n2\n(2000).\nWe adopt a different and more statistical approach in this paper from a dimension-\nreduction point of view. Our model is similar to those in Pen\u02dca and Box (1987), Bai and Ng\n(2002), Pen\u02dca and Poncela (2006), and Pan and Yao (2008), and we consider the inference\nwhen p is as large as, or even larger than, n. Different from the aforementioned econo-\nmetric factor models, we decompose the p-dimensional time series into two parts: the\ndynamic part driven by low-dimensional factors and the static part which is a vector\nwhite noise. Furthermore, we allow the future factors to depend on past (white) noise.\nSuch a conceptually simple decomposition is convenient for both model identification and\nstatistical inference. In fact, the model is identifiable for any finite p. Furthermore the\nestimation for the factor loading matrix and the factor process itself is equivalent to an\neigenanalysis of a p\u00d7 p non-negative definite matrix. Therefore it is applicable when p is\nin the order of a few thousands. Our approach is rooted in the same idea on which the\nmethods of Pen\u02dca and Poncela (2006) and Pan and Yao (2008) were based. However, our\nmethod is radically different and is substantially simpler. For example, Pen\u02dca and Poncela\n(2006) requires the computation of the inverse of the sample covariance matrix for the\ndata, which is computationally costly when p is large, and is invalid when p > n. (See also\nPen\u02dca and Box (1987).) Moreover, in contrast to performing eigenanalysis for one autoco-\nvariance matrix each time, our method only requires to perform one single eigenanalysis\non a matrix function of several autocovariance matrices, and it augments the information\non the dynamics along different lags. The method of Pan and Yao (2008) involves solv-\ning several nonlinear optimization problems, which is designed to handle non-stationary\nfactors and is only feasible for moderately large p. Our approach identifies factors based\non the autocorrelation structure of the data, which, we argue, is more relevant than the\nleast squares approach advocated by Bai and Ng (2002) and Bai (2003) in the context of\nidentifying time series factors.\nThe major theoretical contribution of this paper is to reveal an interesting and some-\nhow intriguing feature in factor modeling: the estimator for the factor loading matrix\nof the original p-dimensional time series converges at a rate independent of p, provided\nthat all the factors are strong in the sense that the norm of each column in the factor\nloading matrix is of order p1\/2. Our simulation indicates that the estimation errors are\nindeed independent of p. This result exhibits clearly that the \u2018curse\u2019 is canceled out by\nthe \u2018blessing\u2019 in dimensionality. In the presence of weak factors, the convergence rate of\n3\nthe estimated factor loading matrix depends on p. In spite of this, we have shown that\nthe optimal convergence rate is obtained under some additional conditions on the white\nnoise, which include Gaussian white noise as a special case.\nAlthough we focus on stationary processes only in this paper, our approach is still\nrelevant for the nonstationary processes for which a generalized autocovariance matrix is\nwell-defined; see remark 1(v) in section 3.\nThe rest of the paper is organized as follows. The model, its presentational issues and\nthe estimation method are presented in section 2. Section 3 introduces the asymptotic\nproperties of the proposed estimation method. Our simulation results are presented in\nsection 4. A detailed analysis of a set of implied volatility data is reported in section 5.\nAll technical proofs are relegated to section 6.\n2 Models and estimation methodology\n2.1 Factor models\nLet y1, \u00b7 \u00b7 \u00b7 ,yn be n p\u00d7 1 successive observations from a vector time series process. The\nfactor model assumes\nyt = Axt + \u00b2t, (2.1)\nwhere {xt} is a r \u00d7 1 unobserved factor time series which is assumed to be strictly\nstationary with finite first two moments, A is a p \u00d7 r unknown constant factor loading\nmatrix, r(\u2264 p) is the number of factors, and {\u00b2t} is a white noise with mean 0 and\ncovariance matrix \u03a3\u00b2.\nWe introduce some notation first. For k \u2265 0, let \u03a3x(k) = Cov(xt+k,xt), \u03a3x,\u00b2(k) =\nCov(xt+k, \u00b2t), and\n\u03a3\u02dcx(k) =\n1\nn\u2212 k\nn\u2212k\u2211\nt=1\n(xt+k \u2212 x\u00af)(xt \u2212 x\u00af)T , \u03a3\u02dcx,\u00b2(k) = 1\nn\u2212 k\nn\u2212k\u2211\nt=1\n(xt+k \u2212 x\u00af)(\u00b2t \u2212 \u00b2\u00af)T ,\nwhere x\u00af = n\u22121\n\u2211n\nt=1 xt, \u00b2\u00af = n\n\u22121\u2211n\nt=1 \u00b2t. The autocovariance matrices \u03a3\u00b2(k), \u03a3\u00b2,x(k),\nand their sample versions are defined in a similar manner. Some assumptions on model\n(2.1) are now in order.\n(A) No linear combination of the components of xt is white noise.\n(B) For k = 0, 1, \u00b7 \u00b7 \u00b7 , k0, where k0 \u2265 1 is a small positive integer, \u03a3x(k) is full-ranked.\n4\n(C) For k \u2265 0, the cross autocovariance matrix \u03a3x,\u00b2(k) and the covariance matrix \u03a3\u00b2\nhave elements of order O(1).\n(D) Cov(\u00b2t,xs) = 0 for all s \u2264 t.\n(E) yt is strictly stationary and \u03c8-mixing with the mixing coefficients \u03c8(\u00b7) satisfying\nthe condition that\n\u2211\nt\u22651 t\u03c8(t)\n1\/2 <\u221e. Furthermore E{\u2016yt\u20164} <\u221e.\nAssumption (A) is natural, as all the white noise linear combinations of xt should be\nabsorbed into \u00b2t. It implies that there exists at least one k \u2265 1 for which \u03a3x(k) is full\nranked. Assumption (B) strengthens this statement for all 1 \u2264 k \u2264 k0, which entails that\nthe non-negative definite matrix L, defined in (2.4) below, has only r positive eigenvalues.\nAssumption (C) is also a natural condition which ensures each element in \u03a3x,\u00b2(k) and \u03a3\u00b2\nbehaves normally when p increases. Assumption (D) relaxes independence assumption\nbetween {xt} and {\u00b2t}, which is present in most factor model literature. It allows future\nfactors to be correlated with past white noise. Finally, assumption (E) is not the weakest\npossible. The \u03c8-mixing condition may be replaced by the \u03b1-mixing condition at the\nexpenses of more lengthy technical argument.\nIn this paper, we always assume that the number of factors r is known and fixed.\nIt is reasonable to assume r fixed while p \u2192 \u221e, as model (2.1) is practically use-\nful only when r \u00bf p. There is a large body of literature on the determination of r.\nSee, for example, Bai and Ng (2002, 2007), Hallin and Li\u02c7ska (2007), Pan and Yao (2008),\nBathia et al. (2010) and Lam and Yao (2010). We use the information criterion proposed\nby Bai and Ng (2002) to determine r in our numerical examples in section 4.\n2.2 Identifiability and factor strength\nModel (2.1) is unchanged if we replace the pair (A,xt) on the RHS by (AH,H\n\u22121xt) for\nany invertible H. However the linear space spanned by the columns of A, denoted by\nM(A) and called the factor loading space, is uniquely defined by (2.1). Note M(A) =\nM(AH) for any invertible H. Once such an A is specified, the factor process xt is\nuniquely defined accordingly. We see the lack of uniqueness of A as an advantage, as we\nmay choose a particular A which facilitates our estimation in a simple and convenient\nmanner. Before we specify explicitly such an A in section 2.3 below, we introduce an\nindex \u03b4 to measure the strength of the factors. We always use the notation a \u00b3 b to\ndenote a = OP (b) and b = OP (a).\n5\n(F) A = (a1 \u00b7 \u00b7 \u00b7 ar) such that \u2016ai\u20162 \u00b3 p1\u2212\u03b4, i = 1, \u00b7 \u00b7 \u00b7 , r, 0 \u2264 \u03b4 \u2264 1.\n(G) For each i = 1, \u00b7 \u00b7 \u00b7 , r and \u03b4 given in (F), min\u03b8j ,j 6=i \u2016ai \u2212\n\u2211\nj 6=i \u03b8jaj\u20162 \u00b3 p1\u2212\u03b4.\nWhen \u03b4 = 0 in assumption (F), the corresponding factors are called strong factors since\nit includes the case where each element of ai is O(1), implying that the factors are shared\n(strongly) by the majority of the p time series. When \u03b4 > 0, the factors are called\nweak factors. In fact the smaller the \u03b4 is, the stronger the factors are. This definition is\ndifferent from Chudik et al. (2009) which defined the strength of factors by the finiteness\nof the mean absolute values of the component of ai. One advantage of using index \u03b4 is to\nlink the convergence rates of the estimated factors explicitly to the strength of factors.\nIn fact the convergence is slower in the presence of weak factors. Assumptions (F) and\n(G) together ensure that all r factors in the model are of the equal strength \u03b4.\nTo facilitate our estimation, we use the QR decomposition A = QR to normalize the\nfactor loading matrix, so that (2.1) becomes\nyt = QRxt + \u00b2t = Qft + \u00b2t, (2.2)\nwhere ft = Rxt, and Q\nTQ = Ir. Note that the pair (Q, ft) in the above model can be\nreplaced by (QU,UT ft) for any r \u00d7 r orthogonal matrix U. In the following section we\nwill specify explicitly such a Q to be used in our estimation.\n2.3 Estimation\nFor k \u2265 1, model (2.2) implies that\n\u03a3y(k) = Cov(yt+k,yt) = Q\u03a3f (k)Q\nT +Q\u03a3f ,\u00b2(k), (2.3)\nwhere \u03a3f (k) = Cov(ft+k, ft) and \u03a3f ,\u00b2(k) = Cov(ft+k, \u00b2t). For k0 \u2265 1 given in condition\n(B), define\nL =\nk0\u2211\nk=1\n\u03a3y(k)\u03a3y(k)\nT\n= Q\n( k0\u2211\nk=1\n{\u03a3f (k)QT +\u03a3f ,\u00b2(k)}{\u03a3f (k)QT +\u03a3f ,\u00b2(k)}T\n)\nQT .\n(2.4)\nObviously L is a p \u00d7 p non-negative definite matrix. Now we are ready to specify the\nfactor loading matrix Q to be used in our estimation. Apply the spectral decomposition\n6\nto the positive-definite matrix sandwiched by Q and QT on the RHS of (2.4), i.e.\nk0\u2211\nk=1\n{\u03a3f (k)QT +\u03a3f ,\u00b2(k)}{\u03a3f (k)QT +\u03a3f ,\u00b2(k)}T = UDUT ,\nwhereU is an r\u00d7r orthogonal matrix, andD is a diagonal matrix with the elements on the\nmain diagonal in descending order. This leads to L = QUDUTQT . As UTQTQU = Ir,\nthe columns of QU are the eigenvectors of L corresponding to its r non-zero eigenvalues.\nWe take QU as the Q to be used in our inference, i.e.\nthe columns of the factor loading matrix Q are the r orthonormal eigenvectors\nof the matrix L corresponding to its r non-zero eigenvalues, and the columns\nare arranged such that the corresponding eigenvalues are in the descending\norder.\nA natural estimator for the Q specified above is defined as Q\u0302 = (q\u03021, \u00b7 \u00b7 \u00b7 , q\u0302r), where\nq\u0302i is the eigenvector of L\u02dc corresponding to its i-th largest eigenvalue, q\u03021, \u00b7 \u00b7 \u00b7 , q\u0302r are\northonormal, and\nL\u02dc =\nk0\u2211\nk=1\n\u03a3\u02dcy(k)\u03a3\u02dcy(k)\nT , \u03a3\u02dcy(k) =\n1\nn\u2212 k\nn\u2212k\u2211\nt=1\n(yt+k \u2212 y\u00af)(yt \u2212 y\u00af)T , (2.5)\nwhere y\u00af = n\u22121\n\u2211n\nt=1 yt.\nConsequently, we estimate the factors and the residuals respectively by\nf\u0302t = Q\u0302\nTyt, et = yt \u2212 Q\u0302f\u0302t = (Ip \u2212 Q\u0302Q\u0302T )yt. (2.6)\n3 Asymptotic theory\nIn this section we present the rates of convergence for the estimators Q\u0302 for model (2.2),\nand also for the estimated factor Q\u0302f\u0302t. It goes without saying explicitly that we may\nreplace some q\u0302j by \u2212q\u0302j in order to match the direction of qj. Denote by \u2016M\u2016 the\nspectral norm of a matrix M (i.e. the positive square root of the maximum eigenvalue\nof MMT ), and denote by \u2016M\u2016min the positive square root of the minimum eigenvalue of\nMMT or MTM , whichever is a smaller matrix. For model (2.2), define\n\u03bamin = min\n1\u2264k\u2264k0\n\u2016\u03a3f ,\u00b2(k)\u2016min, \u03bamax = max\n1\u2264k\u2264k0\n\u2016\u03a3f ,\u00b2(k)\u2016.\nBoth \u03bamax and \u03bamin may be viewed as the measures of the strength of the cross-correlation\nbetween the factor process and the white noise.\n7\nTheorem 1 Let assumptions (A) - (G) hold, and the r positive eigenvalues of matrix L,\ndefined in (2.4), be distinct. Then,\n(i) \u2016Q\u0302\u2212Q\u2016 = OP (p\u03b4n\u22121\/2) provided \u03bamax = o(p1\u2212\u03b4) and p\u03b4n\u22121\/2 = o(1), and\n(ii) \u2016Q\u0302\u2212Q\u2016 = OP (\u03ba\u22122min\u03bamax \u00b7pn\u22121\/2) provided p1\u2212\u03b4 = o(\u03bamin) and \u03ba\u22122min\u03bamax \u00b7pn\u22121\/2 =\no(1).\nRemark 1. (i) When all the factors are strong (i.e. \u03b4 = 0), Theorem 1(i) reduces to\n\u2016Q\u0302 \u2212Q\u2016 = OP (n\u22121\/2) provided \u03bamax\/p \u2192 0. The standard root-n rate might look too\ngood to be true, as the dimension p goes to infinity together with the sample size n. But\nthis is the case when \u2018blessing of dimensionality\u2019 is at its clearest. Note that the strong\nfactors pool together the information from most, if not all, of the original p component\nseries. When p increases, the curse of dimensionality is offset by the increase of the\ninformation from more component series. The condition \u03bamax\/p \u2192 0 is very mild. It\nimplies that the linear dependence between the factors and the white noise is not too\nstrong.\n(ii) When \u03b4 > 0, Theorem 1(i) shows that the stronger the factors are, the faster the\nconvergence rate is. The condition \u03bamax = o(p\n1\u2212\u03b4) ensures that the matrix \u03a3f (k)QT +\n\u03a3f ,\u00b2(k), in (2.4), is dominated by the first term.\n(iii) Theorem 1(ii) represents the cases that there are strong cross-correlations between\nthe factors and the white noise, as \u03bamin\/p\n1\u2212\u03b4 \u2192 \u221e. However this does not necessarily\nimply a slow convergence rate in estimating Q. For instance, when \u03bamax \u00b3 p1\u2212\u03b4\/2 \u00b3 \u03bamin\n(see Lemma 1 in section 6 below), \u2016Q\u0302 \u2212Q\u2016 = OP (p\u03b4\/2n\u22121\/2). This convergence rate is\neven faster than the rate p\u03b4n\u22121\/2. This is not surprising, as we assume that r is known\nand we estimate Q by extracting the information on the autocorrelation of the data,\nincluding the cross-autocorrelation between {ft} and {\u00b2t}. See the definition of L in\n(2.4). However, this may create difficulties for estimating r; see the relevant asymptotic\nresults in Lam and Yao (2010).\n(iv) The assumption that all the non-zero eigenvalues of L are different is not essential,\nand is merely introduced to simplify the presentation in the sense that Theorem 1 now\ncan deal with the convergence of the estimator for Q directly. Otherwise a discrepancy\nmeasure for two linear spaces has to be introduced in order to make statements on the\nconvergence rate of the estimator for the factor loading space M(A); see Pan and Yao\n(2008).\n8\n(v) Theorem 1 can be extended to the cases when the factor xt in model (2.1) is\nnon-stationary, provided that a generalized sample (auto)covariance matrix\nn\u2212\u03b1\nn\u2212k\u2211\nt=1\n(xt+k \u2212 x\u00af)(xt \u2212 x\u00af)T\nconverges weakly, where \u03b1 > 1 is a constant. This weak convergence has been estab-\nlished when, for example, {xt} is a two times integrated process (i.e. {xt} is I(2)) by\nPen\u02dca and Poncela (2006). It can also be proved for other processes with linear trends,\nrandom walk or long memories. In this paper we do not pursue further in this direction.\nSome conditions in Theorem 1 may be too restrictive. For instance when p \u00b3 n,\nTheorem 1(i) requires \u03b4 < 1\/2. This rules out the cases in the presence of weaker factors\nwith \u03b4 \u2265 1\/2. The convergence rates in Theorem 1 are also not optimal. They can be\nfurther improved under additional assumptions on \u00b2t as follows. Note that in particular,\nboth assumptions (H) and (I) are fulfilled when \u00b2t are independent and N(0, \u03c3\n2Ip). See\nalso Pe\u00b4che\u00b4 (2009).\n(H) Let \u00b2jt denote the j-th component of \u00b2t. Then \u00b2jt are independent for different t\nand j, and have mean 0 and common variance \u03c32 <\u221e.\n(I) The distribution of each \u00b2jt is symmetric. Furthermore E(\u00b2\n2k+1\njt ) = 0, and E(\u00b2\n2k\njt ) \u2264\n(\u03c4k)k for all 1 \u2264 j \u2264 p and t, k \u2265 1, where \u03c4 > 0 is a constant independent of j, t, k.\nTheorem 2 In addition to the assumptions of Theorem 1, we assume (H) and (I). If\nn = O(p), then\n(i) \u2016Q\u0302\u2212Q\u2016 = OP (p\u03b4\/2n\u22121\/2) provided \u03bamax = o(p1\u2212\u03b4) and p\u03b4\/2n\u22121\/2 = o(1), and\n(ii) \u2016Q\u0302 \u2212 Q\u2016 = OP (\u03ba\u22122min\u03bamax \u00b7 p1\u2212\u03b4\/2n\u22121\/2) provided p1\u2212\u03b4 = o(\u03bamin) and \u03ba\u22122min\u03bamax \u00b7\np1\u2212\u03b4\/2n\u22121\/2 = o(1).\nBy comparing with Theorem 1, the rates provided in Theorem 2 are improved by a\nfactor p\u2212\u03b4\/2. This also relaxes the condition on the strength of the factors. For instance,\nwhen p \u00b3 n, Theorem 2(i) only requires \u03b4 < 1 while Theorem 2(i) requires \u03b4 < 1\/2.\nTheorem 3 If all the eigenvalues of \u03a3\u00b2 are uniformly bounded from infinity (as p\u2192\u221e),\nit holds that\np\u22121\/2\u2016Q\u0302f\u0302t \u2212Axt\u2016 = p\u22121\/2\u2016Q\u0302f\u0302t \u2212Qft\u2016 = OP (p\u2212\u03b4\/2\u2016Q\u0302\u2212Q\u2016+ p\u22121\/2). (3.7)\n9\nTheorem 3 specifies the convergence rate for the estimated factors. When all factors are\nstrong (i.e. \u03b4 = 0), both Theorems 1 and 2 imply \u2016Q\u0302\u2212Q\u2016 = OP (n\u22121\/2). Now it follows\nTheorem 3 that\np\u22121\/2\u2016Q\u0302f\u0302t \u2212Axt\u2016 = OP (n\u22121\/2 + p\u22121\/2). (3.8)\nThis is the optimal convergence rate specified in Theorem 3 of Bai (2003). This opti-\nmal rate is still attained when the factors are weaker (i.e. \u03b4 > 0) but the white noise\nfulfils assumptions (H) and (I), as then Theorem 2(i) implies \u2016Q\u0302\u2212Q\u2016 = OP (p\u03b4\/2n\u22121\/2).\nPlugging this into the RHS of (3.7), we obtain (3.8).\n4 Simulation\nIn this section, we illustrate our estimation method and their properties via two simulated\nexamples.\nExample 1. We start with a simple one factor model\nyt = Axt + \u00b2t, \u00b2tj \u223c i.i.d. N(0, 22),\nwhere the factor loading matrix A is a p\u00d7 1 vector with 2 cos(2pii\/p) as its i-th element,\nand the factor time series is defined as xt = 0.9xt\u22121 + \u03b7t, where \u03b7t are independent\nN(0, 22) random variables. Hence we have a strong factor for this model with \u03b4 = 0.\nWe set n = 200, 500 and p = 20, 180, 400, 1000. For each (n, p) combination, we generate\nfrom the model 50 samples and calculate the estimation errors. The results are listed in\nTable 1 below. Table 1 indicates clearly that the estimation error in L2 norm for Q\u0302 is\nindependent of p, as shown in Theorem 1(i) with \u03b4 = 0.\n\u2016Q\u0302\u2212Q\u2016 n = 200 n = 500\np = 20 .022(.005) .014(.003)\np = 180 .023(.004) .014(.002)\np = 400 .022(.004) .014(.002)\np = 1000 .023(.004) .014(.002)\nTable 1: Means and standard errors (in brackets) of \u2016Q\u0302\u2212Q\u2016 for Example 1.\nExample 2. We consider model (2.1) with three factors now (r = 3). We compare the\nperformance of our estimators with the principle component (PC) method of Bai and Ng\n10\n(2002) under two different scenarios: (I) \u00b2t \u223c N(0, Ip), and (II) \u00b2t \u223c N(0,\u03a3\u00b2), where \u03a3\u00b2\nhas elements \u03c3ij following the fractional Gaussian noise, defined by\n\u03c3ij =\n1\n2\n((|i\u2212 j|+ 1)2H \u2212 2|i\u2212 j|2H + (|i\u2212 j| \u2212 1)2H),\nand H \u2208 [0.5, 1] is the Hurst parameter. We set H = 0.9 in (II) to simulate strong\ncross-sectional dependence for the elements of \u00b2t, which violates the weak cross-sectional\ndependence assumption in Bai and Ng (2002), but is allowed in our setting.\nThe factors are defined by\nx1,t = \u22120.8x1,t\u22121 + 0.9e1,t\u22121 + e1,t,\nx2,t = \u22120.7x2,t\u22121 + 0.85e2,t\u22121 + e2,t,\nx3,t = 0.8x2,t \u2212 0.5x3,t\u22121 + e3,t,\nwhere ei,t are independent N(0, 1) random variables. For each column of A, we generate\nthe first p\/2 elements randomly from the U(\u22122, 2) distribution; the rest are set to zero.\nThis increases the difficulty in detecting the signals from the factors. We then adjust the\nstrength of the factors by normalizing the columns, setting ai\/p\n\u03b4i\/2 as the i-th column of\nA with \u03b42 = \u03b43.\nWe estimate Q\u0302 either using the true number of factors r = 3, or r\u0302 obtained by\nminimizing the BIC type of information criterion proposed in Bai and Ng (2002):\nr\u0302 = arg minkIC(k) = arg mink log\n(\np\u22121n\u22121\np\u2211\nj=1\n\u2016\u00b2\u0302j\u20162\n)\n+ k\n(\np+ n\npn\n)\nlog\n(\npn\np+ n\n)\n.\nWe set n = 100, 200, 400 and p = 200, 400, 800. We use k0 = 4 in the definition of L\u02dc\nin (2.5). The first factor has strength index \u03b41 and the last two factors have strength\nindex \u03b42. For each combination of (n, p, \u03b41, \u03b42), we replicate the simulation 100 times, and\ncalculate the mean and the standard deviation of the root-mean-square error (RMSE):\nRMSE =\n(\u2211n\nt=1 \u2016Q\u0302f\u0302t \u2212Qft\u20162\npn\n)1\/2\n.\nWe also use y\u0302\n(1)\nn = Q\u0302f\u0302\n(1)\nn to forecast the factor Qft, where f\u0302\n(1)\nn is the one-step predictor\nfor fn derived from a fitted AR(4) model based on f\u03021, \u00b7 \u00b7 \u00b7 , f\u0302n\u22121. We then calculate the\nmean and standard deviation of the factor forecast error (FFE) and the forecast error\n(FE):\nFFE = p\u22121\/2\u2016y\u0302(1)n \u2212Qfn\u2016, FE = p\u22121\/2\u2016y\u0302(1)n \u2212 yn\u2016.\n11\n(I): \u00b2t \u223c N(0, Ip) PC method Our method\n\u03b41 = \u03b42 = 0 r\u0302 RMSE FFE FE r\u0302 RMSE FFE FE\nn=100 p=200 3(0) .21(.005) 1.55(.76) 1.87(.62) 3.0(.2) .28(.02) 1.54(.75) 1.87(.62)\np=400 3(0) .19(.003) 1.61(.77) 1.93(.65) 3.1(.3) .27(.02) 1.61(.77) 1.94(.66)\np=800 3(0) .18(.003) 1.61(.82) 1.95(.71) 3.1(.3) .26(.02) 1.64(.87) 1.97(.76)\nn=200 p=200 3(0) .17(.004) 1.58(.74) 1.90(.61) 3(0) .21(.008) 1.58(.74) 1.90(.61)\np=400 3(0) .15(.003) 1.44(.71) 1.80(.59) 3(0) .19(.01) 1.44(.70) 1.80(.59)\np=800 3(0) .14(.001) 1.28(.64) 1.67(.51) 3(0) .18(.01) 1.28(.64) 1.67(.51)\nn=400 p=200 3(0) .15(.003) 1.47(.74) 1.82(.62) 3(0) .17(.004) 1.47(.74) 1.82(.62)\np=400 3(0) .12(.002) 1.59(.73) 1.92(.62) 3(0) .15(.004) 1.59(.73) 1.92(.62)\np=800 3(0) .11(.001) 1.37(.61) 1.73(.50) 3(0) .13(.004) 1.37(.61) 1.73(.50)\nTable 2: Means and standard deviations (in brackets) of estimation errors and forecast\nerrors for Example 2: \u00b2t \u223c N(0, Ip), and all three factors are strong (\u03b41 = \u03b42 = 0).\nIt is clear from Table 2 that the information criterion for estimating r performed very\nwell on both methods under scenario (I). The PC method performs better in estimating\nthe factors, reflected by the smaller RMSE in most cases. As n increases the RMSE\nfor the two methods become closer. Moreover, the two methods perform equally well in\nterms of the forecast errors. Table 3 shows the results under the same scenario when\nthe factors have different strength indices \u03b41 and \u03b42, and p = 2n. It is clear that r is\nestimated very well even in the presence of weak factors, and the relative performance of\nthe two methods is about the same as in Table 2.\nTable 4 shows the results under scenario (II). The information criterion leads to over-\nestimation of r for both methods, with a more adverse effect for the PC. Our method\noutperforms the PC method under this scenario in general for all the measures RMSE,\nFFE and FE. This is well-expected since {\u00b2t} exhibits strong cross-sectional dependence,\nwhich violates the condition imposed in Bai and Ng (2002).\nSince r is overestimated in all cases in Table 4, we repeat the simulation with the\nnumber of factors set at the true value r = 3. The results are reported in Table 5. Our\nmethod outperforms the PC method in all cases except when \u03b41 = \u03b42 = 0. When all\nfactors are strong, the PC method can pick up the signals from all the three factors and\nstill gives better performance. However, in the presence of weaker factors coupled with\nstrong cross-sectional dependence of {\u00b2t}, the PC method cannot identify correctly the\nsignals from all the factors, evidenced by the sharp increase in RMSE and their large\nstandard deviations. On the other hand, our method can detect the presence of weaker\nfactors even under strong cross-sectional dependence.\n12\nWe repeat the above experiments with all the components of \u00b2t being i.i.d and each fol-\nlows an AR(1) with parameter \u03c6 = 0.2. This creates weak serial correlation in {\u00b2t}. The\nresults are very similar to that in Table 2, and are therefore omitted. This demonstrates\nthat weak serial correlation in {\u00b2t} does not affect the performance of both methods.\n(I): \u00b2t \u223c N(0, Ip) PC method Our method\n\u03b41 = 0, \u03b42 = 1\/4 r\u0302 RMSE FFE FE r\u0302 RMSE FFE FE\n(n, p) = (100, 200) 3(0) .21(.006) 1.20(.55) 1.61(.43) 3.0(.1) .28(.02) 1.22(.54) 1.62(.43)\n(n, p) = (200, 400) 3(0) .15(.003) 1.01(.44) 1.45(.32) 3(0) .19(.01) 1.01(.44) 1.46(.32)\n(n, p) = (400, 800) 3(0) .11(.002) .89(.42) 1.37(.29) 3(0) .13(.005) .89(.42) 1.38(.29)\n\u03b41 = 1\/4, \u03b42 = 0 r\u0302 RMSE FFE FE r\u0302 RMSE FFE FE\n(n, p) = (100, 200) 3(0) .21(.005) 1.31(.62) 1.68(.49) 3.0(.1) .28(.02) 1.31(.61) 1.68(.49)\n(n, p) = (200, 400) 3(0) .15(.003) 1.38(.80) 1.76(.64) 3(0) .19(.01) 1.38(.80) 1.77(.65)\n(n, p) = (400, 800) 3(0) .11(.001) 1.30(.72) 1.69(.58) 3(0) .13(.004) 1.30(.72) 1.69(.58)\n\u03b41 = 0, \u03b42 = 1\/2 r\u0302 RMSE FFE FE r\u0302 RMSE FFE FE\n(n, p) = (100, 200) 3.0(.2) .22(.01) .79(.42) 1.30(.30) 3.0(.2) .27(.02) .80(.41) 1.30(.29)\n(n, p) = (200, 400) 3(0) .15(.003) .77(.49) 1.32(.34) 3(0) .19(.008) .78(.49) 1.32(.33)\n(n, p) = (400, 800) 3(0) .11(.002) .64(.41) 1.23(.27) 3(0) .13(.004) .64(.41) 1.23(.27)\nTable 3: Means and standard deviations (in brackets) of estimation errors and forecast\nerrors for Example 2: \u00b2t \u223c N(0, Ip).\n(II): \u00b2t \u223c N(0,\u03a3\u00b2) PC method Our method\n\u03b41 = 0, \u03b42 = 0 r\u0302 RMSE FFE FE r\u0302 RMSE FFE FE\n(n, p) = (100, 200) 6.7(.6) .71(.04) 1.84(.79) 2.14(.69) 4.8(1.0) .63(.05) 1.73(.80) 2.03(.71)\n(n, p) = (200, 400) 8.4(.6) .68(.02) 1.55(.69) 1.88(.61) 5.2(1.3) .57(.04) 1.48(.63) 1.82(.55)\n(n, p) = (400, 800) 11.2(.7) .66(.02) 1.42(.76) 1.82(.63) 7.0(1.8) .54(.03) 1.35(.74) 1.77(.59)\n\u03b41 = 0, \u03b42 = 1\/4 r\u0302 RMSE FFE FE r\u0302 RMSE FFE FE\n(n, p) = (100, 200) 6.8(.7) .72(.04) 1.18(.53) 1.57(.46) 4.7(.7) .64(.05) 1.07(.45) 1.48(.38)\n(n, p) = (200, 400) 8.3(.6) .68(.02) 1.02(.46) 1.44(.37) 5.4(1.0) .58(.03) .96(.40) 1.40(.30)\n(n, p) = (400, 800) 11.3(.6) .66(.02) .98(.43) 1.43(.35) 6.1(1.0) .54(.02) .93(.42) 1.40(.33)\n\u03b41 = 1\/4, \u03b42 = 0 r\u0302 RMSE FFE FE r\u0302 RMSE FFE FE\n(n, p) = (100, 200) 6.6(.6) .71(.04) 1.47(.70) 1.83(.62) 4.7(.9) .63(.05) 1.30(.65) 1.71(.57)\n(n, p) = (200, 400) 8.4(.6) .69(.02) 1.42(.80) 1.78(.68) 5.9(1.4) .59(.03) 1.38(.78) 1.74(.66)\n(n, p) = (400, 800) 11.2(.6) .66(.02) 1.25(.65) 1.64(.54) 7.3(1.3) .55(.03) 1.25(.65) 1.64(.53)\n\u03b41 = 0, \u03b42 = 1\/2 r\u0302 RMSE FFE FE r\u0302 RMSE FFE FE\n(n, p) = (100, 200) 6.7(.7) .71(.03) .96(.46) 1.42(.43) 4.7(1.0) .64(.04) .96(.47) 1.42(.44)\n(n, p) = (200, 400) 8.4(.7) .68(.03) .84(.46) 1.30(.37) 5.6(1.1) .59(.03) .82(.46) 1.29(.35)\n(n, p) = (400, 800) 11.2(.7) .66(.02) .88(.55) 1.36(.44) 10.7(4.1) .60(.05) .90(.56) 1.37(.45)\nTable 4: Means and standard deviations (in brackets) of estimation errors and forecast\nerrors for Example 2: \u00b2t \u223c N(0,\u03a3\u00b2).\n13\n(II): \u00b2t \u223c N(0,\u03a3\u00b2) PC method Our method\n\u03b41 = 0, \u03b42 = 0 RMSE FFE FE RMSE FFE FE\n(n, p) = (100, 200) .28(.08) 1.58(.74) 1.93(.62) .34(.07) 1.58(.74) 1.93(.63)\n(n, p) = (200, 400) .16(.02) 1.44(.66) 1.80(.57) .20(.02) 1.44(.65) 1.80(.56)\n(n, p) = (400, 800) .11(.02) 1.35(.71) 1.76(.56) .14(.02) 1.35(.71) 1.76(.56)\n\u03b41 = 0, \u03b42 = 1\/4 RMSE FFE FE RMSE FFE FE\n(n, p) = (100, 200) .71(.13) 1.10(.43) 1.49(.39) .40(.15) 1.02(.45) 1.45(.37)\n(n, p) = (200, 400) .68(.11) 0.99(.43) 1.43(.31) .23(.05) 0.91(.42) 1.37(.30)\n(n, p) = (400, 800) .63(.12) 1.00(.41) 1.45(.34) .15(.02) 0.92(.40) 1.39(.32)\n\u03b41 = 1\/4, \u03b42 = 0 RMSE FFE FE RMSE FFE FE\n(n, p) = (100, 200) .55(.21) 1.21(.57) 1.63(.48) .36(.10) 1.18(.58) 1.60(.48)\n(n, p) = (200, 400) .54(.21) 1.31(.74) 1.68(.62) .22(.05) 1.26(.75) 1.65(.62)\n(n, p) = (400, 800) .52(.22) 1.23(.67) 1.62(.54) .15(.03) 1.18(.66) 1.59(.53)\n\u03b41 = 0, \u03b42 = 1\/2 RMSE FFE FE RMSE FFE FE\n(n, p) = (100, 200) .65(.03) .93(.43) 1.40(.40) .59(.10) .93(.42) 1.40(.39)\n(n, p) = (200, 400) .60(.03) .82(.44) 1.29(.34) .51(.10) .81(.44) 1.29(.34)\n(n, p) = (400, 800) .56(.02) .86(.56) 1.36(.45) .42(.12) .85(.56) 1.35(.44)\nTable 5: Means and standard deviations (in brackets) of estimation errors and forecast\nerrors for Example 2: \u00b2t \u223c N(0,\u03a3\u00b2), and the number of factors is fixed at r = 3.\n5 Data Analysis : Implied Volatility Surfaces\nWe illustrate our method by modeling the dynamic behavior of IBM, Microsoft and Dell\nimplied volatility surfaces through the period 03\/01\/2006 \u2212 29\/12\/2006 (250 days in\ntotal). The data was obtained from OptionMetrics via the WRDS database. For each\nday t we observe the implied volatility Wt(ui, vj) computed from call options. Here ui is\nthe time to maturity, taking values 30, 60, 91, 122, 152, 182, 273, 365, 547 and 730 for\ni = 1, \u00b7 \u00b7 \u00b7 , pu = 10 respectively, and vj is the delta, taking values 0.2, 0.25, 0.3, 0.35, 0.4,\n0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, and 0.8 for j = 1, \u00b7 \u00b7 \u00b7 , pv = 13 respectively. We collect\nthese implied volatilities in the matrixWt = (Wt(ui, vi)) \u2208 Rpu\u00d7pv . Figure 1 displays the\nmean volatility surface of IBM, Microsoft and Dell in this period. It shows clearly that\nthe implied volatilities surfaces are not flat. Indeed any cross-section in the maturity or\ndelta axis display the well documented volatility smile.\nIt is a well documented stylized fact that implied volatilities are non-stationary (see\nCont and da Fonseca (1988), Fengler et al. (2007) and Park et al. (2009) amongst oth-\ners). Indeed, when applying the Dickey-Fuller test to each of the univariate time series\nWt(ui, vi), none of the pu \u00d7 pv = 130 nulls of unit roots could be rejected at the 10%\nlevel. Of course we should treat the results of these tests with some caution since we\n14\nFigure 1: Mean implied volatility surfaces.\nare performing a large number of hypothesis tests, but even still the evidence in favor of\nunit roots is overwhelming. Therefore, instead of working with Wt directly, we choose\nto work with \u2206Wt = Wt \u2212Wt\u22121. Our observations are then yt = vec{\u2206Wt}, where\nfor any matrix M = (m1, . . . ,mpv) \u2208 Rpu\u00d7pv , vec{M} = (mT1 , . . . ,mTpv)T \u2208 Rpupv . Note\nthat yt is now defined over 04\/01\/2006 \u2212 29\/12\/2006 since we lose an observation due\nto differencing. Hence altogether there are 249 time points, and the dimension of yt is\np = pv \u00d7 pu = 130.\nWe perform the factor model estimation on a rolling window of length 100 days,\ndefined from the i-th day to the (i + 99)-th day for i = 1, \u00b7 \u00b7 \u00b7 , 150. The length of\nthe window is chosen so that the stationarity assumption of the data is approximately\nsatisfied. For each window, we compare our method with the PC method by estimating\nthe factor loading matrix and the factor series. For the i-th window, we use an AR model\nto forecast the (i + 100)-th value of the estimated factor series x\n(1)\ni+100, so as to obtain a\none-step ahead forecast y\n(1)\ni+100 = A\u0302x\n(1)\ni+100 for yi+100. We then calculate the forecast error\nfor the (i+ 100)-th day defined by\nFE = p\u22121\/2\u2016y(1)i+100 \u2212 yi+100\u2016.\n5.1 Estimation results\nIn forming the matrix L\u02dc for each window, we take k0 = 5 in (2.5) , taking advantage\nthat the autocorrelations are not weak even at higher lags, though similar results (not\nreported here) are obtained for smaller k0.\nFigure 2 displays the average of each ordered eigenvalue over the 150 windows. The\nleft hand side shows the average of the largest to the average of the tenth largest eigenvalue\nof L\u02dc for Dell, IBM and Microsoft for our method, whereas the right hand side shows the\n15\nFigure 2: Averages of ordered eigenvalues of L\u02dc over the 150 windows. Left: Ten largest.\nRight: Second to eleventh largest.\nsecond to eleventh largest. We obtain similar results for the Bai and Ng (2002) procedure\nand thus the corresponding graph is not shown.\nFrom this diagram it is apparent that there is one eigenvalue that is much larger than\nthe others for all three companies for each window. We have done automatic selection\nfor the number of factors for each window using the IC criterion by Bai and Ng (2002)\nintroduced in Example 2 in section 4, and a one factor model is consistently obtained for\neach window and for each company. Hence both methods choose a one factor model over\nthe 150 windows.\nFigure 3: The cumulative FE over the 150 windows. Red dotted: Bai and Ng (2002)\nprocedure. Green: Taking forecast y\n(1)\nt+1 to be yt. Black: Our method.\n16\n(a) Dell (b) IBM (c) Microsoft\nFigure 4: Plot of return against the estimated factor for the first window. Black circle:\nOur method. Red \u201c+\u201d: PC method.\nFigure 3 displays the cumulative FE over the 150 windows for each method. We choose\na benchmark procedure (green line in each plot), where we just treat today\u2019s value as\nthe one-step ahead forecast. Except for Dell where the PC method is doing marginally\nbetter, our method consistently outperforms the benchmark procedure and is better than\nthe PC method for IBM and Microsoft.\n5.2 A simple trading exercise\nWe use the one-step ahead forecast above to forecast the next day return of the three\nstocks. Figure 4 shows the plots of return against the estimated factor for all three\ncompanies for the data in the first window. Simple linear regression suggests that the\nslope of the regression lines are significant. Hence we can plug in the one-step ahead\nforecast of the factor into the estimated linear function to estimate the next day return.\nAll other windows for the three companies show linear pattern with similar plots, and\nhence we can do this for all the 150 windows.\nAfter forecasting the return of the (t + 1)-th day, if it is higher than that of the t-th\nday, we buy $1; otherwise, we sell $1. Ignoring all trading costs, the accumulated return\nis calculated at the end of the whole time period. This is done for our method and the PC\nmethod. For the benchmark procedure, we calculated the average of the price of a stock\nfor the past 5 days, and compare that to the price today. If the average is higher than\nthe price today, we sell $1; otherwise we buy $1. We have two more similar benchmark\n17\nprocedures, which look at the average price of the past 10 and 15 days respectively.\nFigure 5 shows the results for the three companies. Our method outperforms others\nfor IBM and Microsoft. For Dell, both the returns of our method and the PC method\nstay flat for around the first 100 days, and then gradually go up to perform similarly to\nother moving average benchmarks in the end.\n(a) Dell (b) IBM\n(c) Microsoft\nFigure 5: Plot of accumulated returns over time. Black: Our method. Black dotted: PC\nmethod. Red, Green and Blue lines are respectively benchmark procedure looking at 5,\n10 and 15 days moving average price.\n18\n6 Proofs\nBefore proving the theorems in section 3, we need to have three lemmas.\nLemma 1 Under model (2.2) with assumptions (A) - (G) in sections 2.1 and 2.2, we\nhave\n\u2016\u03a3f (k)\u2016 \u00b3 p1\u2212\u03b4 \u00b3 \u2016\u03a3f (k)\u2016min, \u2016\u03a3f ,\u00b2(k)\u2016 = O(p1\u2212\u03b4\/2).\nProof. Model (2.2) is an equivalent representation of model (2.1), where\nyt = Axt + \u00b2t = Qft + \u00b2t,\nwith A = QR and ft = Rxt. With assumptions (F) and (G), the diagonal entries of R\nare all asymptotic to p\n1\u2212\u03b4\n2 (which is the order of \u2016ai\u2016), and the off-diagonal entries are of\nsmaller order. Hence, as r is a constant, using\n\u2016R\u2016 = max\n\u2016u\u2016=1\n\u2016Ru\u2016, \u2016R\u2016min = min\u2016u\u2016=1 \u2016Ru\u2016,\nwe can conclude that\n\u2016R\u2016 \u00b3 p 1\u2212\u03b42 \u00b3 \u2016R\u2016min.\nThis, together with \u03a3f (k) = Cov(ft+k, ft) = Cov(Rxt+k,Rxt) = R\u03a3x(k)R\nT for k =\n1, \u00b7 \u00b7 \u00b7 , k0, implies\np1\u2212\u03b4 \u00b3 \u2016R\u20162min \u00b7 \u2016\u03a3x(k)\u2016min \u2264 \u2016\u03a3f (k)\u2016min \u2264 \u2016\u03a3f (k)\u2016 \u2264 \u2016R\u20162 \u00b7 \u2016\u03a3x(k)\u2016 \u00b3 p1\u2212\u03b4,\nwhere we used assumption (B) to arrive at \u2016\u03a3x(k)\u2016 \u00b3 1 \u00b3 \u2016\u03a3x(k)\u2016min, so that\n\u2016\u03a3f (k)\u2016 \u00b3 p1\u2212\u03b4 \u00b3 \u2016\u03a3f (k)\u2016min.\nWe used the inequality \u2016AB\u2016min \u2265 \u2016A\u2016min \u00b7 \u2016B\u2016min for any square matrices A and B,\nwhich can be proved by noting\n\u2016AB\u2016min = min\nu6=0\nuTBTATABu\n\u2016u\u20162 \u2265 minu6=0\n(Bu)TATA(Bu)\n\u2016Bu\u20162 \u00b7\n\u2016Bu\u20162\n\u2016u\u20162\n\u2265 min\nw 6=0\nwTATAw\n\u2016w\u20162 \u00b7minu6=0\n\u2016Bu\u20162\n\u2016u\u20162 = \u2016A\u2016min \u00b7 \u2016B\u2016min. (6.1)\nFinally, using assumption (C) that \u03a3x,\u00b2(k) = O(1) elementwisely, and that it has\nrp \u00b3 p elements, we have\n\u2016\u03a3f ,\u00b2(k)\u2016 = \u2016R\u03a3x,\u00b2(k)\u2016 \u2264 \u2016R\u2016 \u00b7 \u2016\u03a3x,\u00b2(k)\u2016F = O(p 1\u2212\u03b42 ) \u00b7O(p1\/2) = O(p1\u2212\u03b4\/2),\nwhere \u2016M\u2016F := trace(MMT ) denotes the Frobenius norm of the matrix M . \u00a4\n19\nLemma 2 Under model (2.2) and assumption (E) in section 2.1, we have for 0 \u2264 k \u2264 k0,\n\u2016\u03a3\u02dcf (k)\u2212\u03a3f (k)\u2016 = OP (p1\u2212\u03b4n\u22121\/2), \u2016\u03a3\u02dc\u00b2(k)\u2212\u03a3\u00b2(k)\u2016 = OP (pn\u22121\/2),\n\u2016\u03a3\u02dcf ,\u00b2(k)\u2212\u03a3f ,\u00b2(k)\u2016 = OP (p1\u2212\u03b4\/2n\u22121\/2) = \u2016\u03a3\u02dc\u00b2,f (k)\u2212\u03a3\u00b2,f (k)\u2016,\nMoreover, \u2016ft\u20162 = OP (p1\u2212\u03b4) for all integers t \u2265 0.\nProof. From (2.1) and (2.2), we have the relation ft = Rxt, where R is an upper\ntriangular matrix with \u2016R\u2016 \u00b3 p 1\u2212\u03b42 \u00b3 \u2016R\u2016min (see the proof of Lemma 1). Then we\nimmediately have \u2016ft\u20162 \u2264 \u2016R\u20162 \u00b7 \u2016xt\u20162 = OP (p1\u2212\u03b4r) = OP (p1\u2212\u03b4).\nAlso, the covariance matrix and the sample covariance matrix for {ft} are respectively\n\u03a3f (k) = R\u03a3x(k)R\nT , \u03a3\u02dcf (k) = R\u03a3\u02dcx(k)R\nT .\nHence\n\u2016\u03a3\u02dcf (k)\u2212\u03a3f (k)\u2016 \u2264 \u2016R\u20162 \u00b7 \u2016\u03a3\u02dcx(k)\u2212\u03a3x(k)\u2016\n= O(p1\u2212\u03b4) \u00b7OP (n\u22121\/2 \u00b7 r)\n= OP (p\n1\u2212\u03b4n\u22121\/2),\nwhich is the rate specified in the lemma. We used the fact that the matrix \u03a3\u02dcx(k)\u2212\u03a3x(k)\nhas r2 elements, with elementwise rate of convergence being O(n\u22121\/2) which is implied by\nassumption (E) and that {\u00b2t} is white noise. Other rates can be derived similarly using\nthe Frobenius norm as an upper bound. \u00a4\nThe following is Theorem 8.1.10 in Golub and Van Loan (1996), which is stated ex-\nplicitly since our main theorems are based on this. See Johnstone and Arthur (2009)\nalso.\nLemma 3 Suppose A and A+ E are n\u00d7 n symmetric matrices and that\nQ = [Q1 Q2] (Q1 is n\u00d7 r, Q2 is n\u00d7 (n\u2212 r))\nis an orthogonal matrix such that span(Q1) is an invariant subspace for A (that is,\nA\u00b7 span(Q1) \u2282 span(A)). Partition the matrices QTAQ and QTEQ as follows:\nQTAQ =\n(\nD1 0\n0 D2\n)\nQTEQ =\n(\nE11 E\nT\n21\nE21 E22\n)\n.\n20\nIf sep(D1,D2) := min\u03bb\u2208\u03bb(D1), \u00b5\u2208\u03bb(D2) |\u03bb\u2212\u00b5| > 0, where \u03bb(M) denotes the set of eigenval-\nues of the matrix M , and\n\u2016E\u2016 \u2264 sep(D1,D2)\n5\n,\nthen there exists a matrix P \u2208 R(n\u2212r)\u00d7r with\n\u2016P\u2016 \u2264 4\nsep(D1,D2)\n\u2016E21\u2016\nsuch that the columns of Q\u03021 = (Q1+Q2P)(I+P\nTP)\u22121\/2 define an orthonormal basis for\na subspace that is invariant for A+ E.\nIn the proofs thereafter, we use \u2297 to denote the Kronecker product of matrices, and\n\u03c3j(M) to denote the j-th singular value of the matrix M . Hence \u03c31(M) = \u2016M\u2016. We use\n\u03bbj(M) to denote the j-th largest eigenvalue of M .\nProof of Theorem 1. Under model (2.2), we have shown in section 2.3 that we have\nLQU = QUD. Since U is an orthogonal matrix, we have\nyt = Qft + \u00b2t = (QU)(U\nT ft) + \u00b2t,\nso that we can replaceQU withQ andUT ft with ft in the model, thus making LQ = QD,\nwhere now D is diagonal with\nD =\nk0\u2211\nk=1\n{\u03a3f (k)QT +\u03a3f ,\u00b2(k)}{\u03a3f (k)QT +\u03a3f ,\u00b2(k)}T .\nIf B is an orthogonal complement of Q, then LB = 0, and(\nQT\nBT\n)\nL(Q B) =\n(\nD 0\n0 0\n)\n, (6.2)\nwith sep(D,0) = \u03bbmin(D) (see Lemma 3 for the definition of the function sep). We now\nfind the order of \u03bbmin(D).\nTo this end, define\nWf (k0) = (\u03a3f (1), \u00b7 \u00b7 \u00b7 ,\u03a3f (k0)), Wf ,\u00b2(k0) = (\u03a3f ,\u00b2(1), \u00b7 \u00b7 \u00b7 ,\u03a3f ,\u00b2(k0)),\nso that we have D = (Wf (k0)(Ik0 \u2297 QT ) +Wf ,\u00b2(k0))(Wf (k0)(Ik0 \u2297 QT ) +Wf ,\u00b2(k0))T .\nHence, assuming first that \u03bamax = o(p\n1\u2212\u03b4), we have\n\u03bbmin(D) = {\u03c3r(Wf (k0)(Ik0 \u2297QT ) +Wf ,\u00b2(k0))}2\n\u2265 {\u03c3r(Wf (k0)(Ik0 \u2297QT ))\u2212 \u03c31(Wf ,\u00b2(k0))}2\n= {\u03c3r(Wf (k0))\u2212 \u03c31(Wf ,\u00b2(k0))}2\n\u00b3 \u03c3r(Wf (k0))2 \u00b3 p2\u22122\u03b4,\n21\nwhere we use \u2016\u03a3f (k)\u2016min \u00b3 p1\u2212\u03b4 from Lemma 1. On the other hand, if p1\u2212\u03b4 = o(\u03bamin),\nthen we have\n\u03bbmin(D) \u2265 {\u03c3r(Wf ,\u00b2(k0))\u2212 \u03c31(Wf (k0)(Ik0 \u2297QT ))}2\n= {\u03c3r(Wf ,\u00b2(k0))\u2212 \u03c31(Wf (k0))}2\n\u00b3 \u03c3r(Wf ,\u00b2(k0))2 \u00b3 \u03ba2min.\nHence we have\nmax(\u03ba2min, p\n2\u22122\u03b4) = O(\u03bbmin(D)). (6.3)\nNext, we need to find \u2016EL\u2016, where we define EL = L\u02dc \u2212 L, with L\u02dc defined in (2.5).\nThen it is easy to see that\n\u2016EL\u2016 \u2264\nk0\u2211\nk=1\n{\n\u2016\u03a3\u02dcy(k)\u2212\u03a3y(k)\u20162 + 2\u2016\u03a3y(k)\u2016 \u00b7 \u2016\u03a3\u02dcy(k)\u2212\u03a3y(k)\u2016\n}\n. (6.4)\nConsider for k \u2265 1, using the results from Lemma 1,\n\u2016\u03a3y(k)\u2016 = \u2016Q\u03a3f (k)QT +Q\u03a3f ,\u00b2(k)\u2016 \u2264 \u2016\u03a3f (k)\u2016+ \u2016\u03a3f ,\u00b2(k)\u2016 = O(p1\u2212\u03b4 + \u03bamax). (6.5)\nAlso, for k = 1, \u00b7 \u00b7 \u00b7 , k0, using the results in Lemma 2,\n\u2016\u03a3\u02dcy(k)\u2212\u03a3y(k)\u2016 \u2264\u2016\u03a3\u02dcf (k)\u2212\u03a3f (k)\u2016+ 2\u2016\u03a3\u02dcf ,\u00b2(k)\u2212\u03a3f ,\u00b2(k)\u2016+ \u2016\u03a3\u02dc\u00b2(k)\u2016\n=OP (p\n1\u2212\u03b4n\u22121\/2 + p1\u2212\u03b4\/2n\u22121\/2 + \u2016\u03a3\u02dc\u00b2(k)\u2016)\n=OP (p\n1\u2212\u03b4\/2n\u22121\/2 + \u2016\u03a3\u02dc\u00b2(k)\u2016).\n(6.6)\nWithout further assumptions on {\u00b2t}, we have \u2016\u03a3\u02dc\u00b2(k)\u2016 \u2264 \u2016\u03a3\u02dc\u00b2(k)\u2016F = OP (pn\u22121\/2), which\nimplies from (6.6) that\n\u2016\u03a3\u02dcy(k)\u2212\u03a3y(k)\u2016 = OP (pn\u22121\/2). (6.7)\nWith (6.5) and (6.7), we can easily see from (6.4) that\n\u2016EL\u2016 = OP (p2\u2212\u03b4n\u22121\/2 + \u03bamax \u00b7 pn\u22121\/2). (6.8)\nFinally, no matter \u03bamax = o(p\n1\u2212\u03b4) or p1\u2212\u03b4 = o(\u03bamin), we have from (6.8) and (6.3)\nthat\n\u2016EL\u2016 = OP (p2\u2212\u03b4n\u22121\/2 + \u03bamax \u00b7 pn\u22121\/2) = oP (max(p2\u22122\u03b4, \u03ba2min))\n= OP (\u03bbmin(D)) = OP (sep(D,0)),\n22\nsince we assumed hn = p\n\u03b4n\u22121\/2 = o(1) in the former case, or \u03ba\u22122min\u03bamax \u00b7 pn\u22121\/2 = o(1) in\nthe latter. Hence for sufficient large n, we have \u2016EL\u2016 \u2264 sep(D,0)\/5. This allows us to\napply Lemma 3 to conclude that there exists a matrix P \u2208 R(p\u2212r)\u00d7r such that\n\u2016P\u2016 \u2264 4\nsep(D,0)\n\u2016(EL)21\u2016 \u2264 4\nsep(D, 0)\n\u2016EL\u2016,\nand Q\u0302 = (Q+BP)(I+PTP)\u22121\/2 is an estimator for Q. Then we have\n\u2016Q\u0302\u2212Q\u2016 = \u2016(Q(I\u2212 (I+PTP)1\/2) +BP)(I+PTP)\u22121\/2\u2016\n\u2264 \u2016I\u2212 (I+PTP)1\/2\u2016+ \u2016P\u2016\n\u2264 2\u2016P\u2016,\nand using (6.3) and (6.8),\n\u2016P\u2016 = OP\n(\np2\u2212\u03b4n\u22121\/2 + \u03bamax \u00b7 pn\u22121\/2\nmax(\u03ba2min, p\n2\u22122\u03b4)\n)\n=\n{\nOP (p\n\u03b4n\u22121\/2), if \u03bamax = o(p1\u2212\u03b4);\nOP (\u03ba\n\u22122\nmin\u03bamax \u00b7 pn\u22121\/2), if p1\u2212\u03b4 = o(\u03bamin).\nThis completes the proof of the theorem. \u00a4\nProof of Theorem 2. Under assumptions (H) and (I), if we can show that\n\u2016\u03a3\u02dc\u00b2(k)\u2016 = OP (pn\u22121), (6.9)\nthen (6.6) becomes\n\u2016\u03a3\u02dcy(k)\u2212\u03a3y(k)\u2016 = OP (p1\u2212\u03b4\/2n\u22121\/2 + pn\u22121) = OP (p1\u2212\u03b4\/2n\u22121\/2),\nwhere we use the assumption p\u03b4\/2n\u22121\/2 = o(1). This rate is smaller than that in (6.7) by\na factor of p\u03b4\/2, which carries to other parts of the proof of Theorem 1, so that the final\nrates are all smaller by a factor of p\u03b4\/2. Hence, it remains to show (6.9).\nTo this end, define 1k the column vector of k ones, and\nEr,s = (\u00b2r, \u00b7 \u00b7 \u00b7 , \u00b2s) for r \u2264 s.\nSince the asymptotic behavior of the three sample means\n\u00b2\u00af = n\u22121E1,n1n, (n\u2212 k)\u22121E1,n\u2212k1n\u2212k, (n\u2212 k)\u22121Ek+1,n1n\u2212k\nare exactly the same as k is finite and {\u00b2t} is stationary, in this proof we take the sample\nlag-k autocovariance matrix for {\u00b2t} to be\n\u03a3\u02dc\u00b2(k) = n\n\u22121(Ek+1,n \u2212 (n\u2212 k)\u22121Ek+1,n1n\u2212k1Tn\u2212k)(E1,n\u2212k \u2212 (n\u2212 k\u22121E1,n\u2212k1n\u2212k1Tn\u2212k))T\n= n\u22121Ek+1,nTn\u2212kET1,n\u2212k,\n23\nwhere Tj = Ij \u2212 j\u221211j1\u2032j. Then under conditions (H) and (I),\n\u2016\u03a3\u02dc\u00b2(k)\u2016 \u2264 \u2016n\u22121\/2Ek+1,n\u2016 \u00b7 \u2016Tn\u2212k\u2016 \u00b7 \u2016n\u22121\/2E1,n\u2212k\u2016\n= \u03bb\n1\/2\n1 (n\n\u22121ETk+1,nEk+1,n) \u00b7 \u03bb1\/21 (n\u22121ET1,n\u2212kE1,n\u2212k)\n= OP ((1 + (pn\n\u22121)1\/2) \u00b7 (1 + (pn\u22121)1\/2))\n= OP (pn\n\u22121),\nwhere the second last line follows from Theorem 1.3 of Pe\u00b4che\u00b4 (2009) for the covariance\nmatrices n\u22121ETk+1,nEk+1,n and n\n\u22121ET1,n\u2212kE1,n\u2212k, and the last line follows from the assump-\ntion n = O(p). This completes the proof of the theorem. \u00a4\nProof of Theorem 3. Consider\nQ\u0302f\u0302t \u2212Qft = Q\u0302Q\u0302Tyt \u2212Qft = Q\u0302Q\u0302TQft \u2212Qft + Q\u0302Q\u0302T\u00b2t\n= (Q\u0302Q\u0302T \u2212QQT )ft + Q\u0302(Q\u0302\u2212Q)T\u00b2t + Q\u0302QT\u00b2t\n:= K1 +K2 +K3.\nUsing Lemma 2, we have\n\u2016K1\u2016 = OP (\u2016Q\u0302\u2212Q\u2016 \u00b7 \u2016ft\u2016) = OP (p 1\u2212\u03b42 \u2016Q\u0302\u2212Q\u2016).\nAlso, since \u2016Q\u0302\u2212Q\u2016 = oP (1) and \u2016Q\u2016 = 1, we have K2 dominated by K3 in probability.\nHence we only need to consider K3. Now consider for Q = (q1, \u00b7 \u00b7 \u00b7 ,qr), the random\nvariable qTj \u00b2t, with\nE(qTj \u00b2t) = 0, Var(q\nT\nj \u00b2t) = q\nT\nj \u03a3\u00b2qj \u2264 \u03bbmax(\u03a3\u00b2) < c <\u221e\nfor j = 1, \u00b7 \u00b7 \u00b7 , r by assumption, where c is a constant independent of n and r. Hence\nqTj \u00b2t = OP (1). We then have\n\u2016K3\u2016 = \u2016Q\u0302QT\u00b2t\u2016 \u2264 \u2016QT\u00b2t\u2016 =\nr\u2211\nj=1\n(qTj \u00b2t)\n2 = OP (1).\nHence p\u22121\/2\u2016Q\u0302f\u0302t \u2212Qft\u2016 = OP (p\u2212\u03b4\/2\u2016Q\u0302\u2212Q\u2016+ p\u22121\/2), which completes the proof of the\ntheorem. \u00a4\n24\nReferences\nAnderson, T. (1963). The use of factor analysis in the statistical analysis of multiple time\nseries. Psychometrika 28, 1\u201325.\nBai, J. (2003). Inferential theory for factor models of large dimensions. Econometrica 71,\n135\u2013171.\nBai, J. and S. Ng (2002). Determining the number of factors in approximate factor\nmodels. Econometrica 70, 191\u2013221.\nBai, J. and S. Ng (2007). Determining the number of primitive shocks in factor models.\nJournal of Business & Economic Statistics 25, 52\u201360.\nBathia, N., Q. Yao, and F. Zieglemann (2010). Identifying the finite dimensionality of\ncurve time series. Ann. Statist., to appear.\nBrillinger, D. (1981). Time Series Data Analysis and Theory (Extended ed.). San Fran-\ncisco: Holden-Day.\nChamberlain, G. (1983). Funds, factors, and diversification in arbitrage pricing models.\nEconometrica 51, 1305\u20131323.\nChamberlain, G. and M. Rothschild (1983). Arbitrage, factor structure, and mean-\nvariance analysis on large asset markets. Econometrica 51, 1281\u20131304.\nChudik, A., M. H. Pesaran, and E. Tosetti (2009). Weak and strong cross section depen-\ndence and estimation of large panels. Manuscript.\nCont, R. and J. da Fonseca (1988). Dynamics of implied volatility surfaces. Quantitative\nFinance 2, 45\u201360.\nFengler, M., W. Hardle, and E. Mammen (2007). A dynamic semiparametric factor model\nfor implied volatility string dynamics. Journal of Econometrics 5, 189\u2013218.\nForni, M., M. Hallin, M. Lippi, and L. Reichlin (2000). The generalized dynamic-factor\nmodel: identification and estimation. The Review of Economics and Statist. 82, 540\u2013\n554.\n25\nForni, M., M. Hallin, M. Lippi, and L. Reichlin (2004). The generalized dynamic-factor\nmodel: consistency and rates. J. of Econometrics 119, 231\u2013255.\nForni, M., M. Hallin, M. Lippi, and L. Reichlin (2005). The generalized dynamic factor\nmodel: One-sided estimation and forecasting. J. Amer. Statist. Assoc. 100, 830\u2013840.\nGolub, G. and C. Van Loan (1996). Matrix Computations (3rd ed.). Johns Hopkins\nUniversity Press.\nHallin, M. and R. Li\u02c7ska (2007). Determining the number of factors in the general dynamic\nfactor model. J. Amer. Statist. Assoc. 102, 603\u2013617.\nJohnstone, I. and Y. Arthur (2009). On consistency and sparsity for principal components\nanalysis in high dimensions. J. Amer. Statist. Assoc. 104, 682\u2013693.\nLam, C. and Q. Yao (2010). Factor modelling for high dimensional time series.\nManuscript.\nPan, J. and Q. Yao (2008). Modelling multiple time series via common factors.\nBiometrika 95, 365\u2013379.\nPark, B., E. Mammen, W. Hardle, and S. Borak (2009). Modelling dynamic semipara-\nmetric factor models. J. Amer. Statist. Assoc.. forthcoming.\nPen\u02dca, D. and G. Box (1987). Identifying a simplifying structure in time series.\nJ. Amer. Statist. Assoc. 82, 836\u2013843.\nPen\u02dca, D. and P. Poncela (2006). Nonstationay dynamic factor analysis. Journal of\nStatistical Planning and Inference 136, 1237\u20131257.\nPe\u00b4che\u00b4, S. (2009). Universality results for the largest eigenvalues of some sample covariance\nmatrix ensembles. Probab. Theory Relat. Fields 143, 481\u2013516.\nPriestley, M., T. Rao, and J. Tong (1974). Applications of principal component analysis\nand factor analysis in the identification of multivariable systems. IEEE Trans. Automat.\nControl 19, 703\u2013704.\n26\n"}