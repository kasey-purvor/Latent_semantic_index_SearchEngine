{"doi":"10.1145\/1996461.1996531","coreId":"10949","oai":"oai:eprints.lancs.ac.uk:39956","identifiers":["oai:eprints.lancs.ac.uk:39956","10.1145\/1996461.1996531"],"title":"Estimating scale using depth from focus for mobile augmented reality.","authors":["\u010copi\u010d Pucihar, Klen","Coulton, Paul"],"enrichments":{"references":[{"id":16396816,"title":"An Iterative Image Registration Technique with an Application to Stereo Vision. in IJCAI81,","authors":[],"date":"1981","doi":null,"raw":"Lucas,  B.D.  and  Kanade,  T.,  An  Iterative  Image Registration Technique with an Application to Stereo Vision. in IJCAI81, (1981), 674-679.","cites":null},{"id":16396819,"title":"Augmented reality camera tracking with homographies.","authors":[],"date":null,"doi":"10.1109\/mcg.2002.1046627","raw":"Prince,  S.J.D.,  Xu,  K.  and  Cheok,  A.D.  Augmented reality  camera  tracking  with  homographies.  Ieee Computer Graphics and Applications, 22 (6). 39-45.","cites":null},{"id":16396831,"title":"Depth from focusing and defocusing. in Computer Vision and Pattern Recognition,","authors":[],"date":"1993","doi":"10.1109\/cvpr.1993.340977","raw":"Xiong, Y. and Shafer, S.A., Depth from focusing and defocusing.  in  Computer  Vision  and  Pattern Recognition, 1993. Proceedings CVPR '93., 1993 IEEE Computer Society Conference on, (1993), 68-73.","cites":null},{"id":16396817,"title":"FastSLAM 2.0: An Improved Particle Filtering Algorithm for Simultaneous Localization and Mapping","authors":[],"date":"2003","doi":"10.1109\/robot.2003.1241885","raw":"Montemerlo,  M.  and  Thrun,  S.  FastSLAM  2.0:  An Improved  Particle  Filtering  Algorithm  for Simultaneous Localization and Mapping that Provably Converges,  Conference  on  Artificial  Intelligence (ICCV\u201907), Rio de Janeiro, 2003, 1151\u20131156.","cites":null},{"id":16396823,"title":"Focusing Techniques.","authors":[],"date":null,"doi":"10.1117\/12.132073","raw":"Subbarao,  M.,  Choi,  T.  and  Nikzad,  A.  Focusing Techniques. Optical Engineering, 32 (11). 2824-2836.","cites":null},{"id":16396820,"title":"Good features to track Computer Vision and Pattern Recognition,","authors":[],"date":"1994","doi":"10.1109\/cvpr.1994.323794","raw":"Shi,  J.  and  Tomasi,  C.  Good  features  to  track Computer  Vision  and  Pattern  Recognition,  1994. Proceedings CVPR '94., 1994 IEEE Computer Society Conference on, 1994, 593 -600.","cites":null},{"id":16396821,"title":"Markerless tracking using planar structures in the scene Augmented Reality,","authors":[],"date":"2000","doi":"10.1109\/isar.2000.880935","raw":"Simon,  G.,  Fitzgibbon,  A.W.  and  Zisserman,  A. Markerless  tracking  using  planar  structures  in  the scene  Augmented  Reality,  2000.  (ISAR  2000). Proceedings. IEEE and ACM International Symposium on, 2000, 120 -128.","cites":null},{"id":16396815,"title":"Monocular model-based 3D tracking of rigid objects.","authors":[],"date":null,"doi":"10.1561\/0600000001","raw":"Lepetit,  V.  and  Fua,  P.  Monocular  model-based  3D tracking  of  rigid  objects.  Found.  Trends.  Comput. Graph. Vis., 1 (1). 1-89.","cites":null},{"id":16396806,"title":"MonoSLAM: Real-Time Single Camera SLAM.","authors":[],"date":null,"doi":"10.1109\/tpami.2007.1049","raw":"Davison, A.J., Reid, I.D., Molton, N.D. and Stasse, O. MonoSLAM:  Real-Time  Single  Camera  SLAM. Pattern  Analysis  and  Machine  Intelligence,  IEEE Transactions on, 29 (6). 1052-1067.","cites":null},{"id":16396812,"title":"Multiple View Geometry in Computer Vision.","authors":[],"date":"2004","doi":"10.1017\/cbo9780511811685.013","raw":"Hartley,  R.I.  and  Zisserman,  A.  Multiple  View Geometry in Computer Vision. Cambridge University Press, ISBN: 0521540518, 2004.","cites":null},{"id":16396822,"title":"On the Representation and Estimation of Spatial Uncertainty.","authors":[],"date":null,"doi":"10.1177\/027836498600500404","raw":"Smith, R.C. and Cheeseman, P. On the Representation and  Estimation  of  Spatial  Uncertainty.  The International Journal of Robotics Research, 5 (4). 56-","cites":null},{"id":16396813,"title":"Parallel Tracking and Mapping for Small AR Workspaces","authors":[],"date":"2007","doi":"10.1109\/ismar.2007.4538852","raw":"Klein,  G.  and  Murray,  D.  Parallel  Tracking  and Mapping for Small AR Workspaces Proc. Sixth IEEE and  ACM  International  Symposium  on  Mixed  and Augmented Reality (ISMAR'07), Nara, Japan, 2007.","cites":null},{"id":16396814,"title":"Parallel Tracking and Mapping on a Camera Phone","authors":[],"date":"2009","doi":"10.1109\/ismar.2009.5336495","raw":"Klein,  G.  and  Murray,  D.  Parallel  Tracking  and Mapping on a Camera Phone Proc. Eigth IEEE and ACM  International  Symposium  on  Mixed  and Augmented Reality (ISMAR'09), Orlando, 2009.","cites":null},{"id":16396829,"title":"Pose tracking from natural features on mobile phones","authors":[],"date":"2008","doi":"10.1109\/ismar.2008.4637338","raw":"Wagner, D., Reitmayr, G., Mulloni, A., Drummond, T. and  Schmalstieg,  D.  Pose  tracking  from  natural features on mobile phones ISMAR '08: Proceedings of the 7th IEEE\/ACM International Symposium on Mixed and  Augmented  Reality,  IEEE  Computer  Society, Washington, DC, USA, 2008, 125-134.","cites":null},{"id":16396810,"title":"Presentation of paper: Scalable Monocular SLAM,","authors":[],"date":"2006","doi":null,"raw":"Ethan, E. and Tom, D. Presentation of paper: Scalable Monocular SLAM, 2006.","cites":null},{"id":16396809,"title":"Scalable Monocular SLAM. in Computer Vision and Pattern Recognition,","authors":[],"date":"2006","doi":"10.1109\/cvpr.2006.263","raw":"Eade,  E.  and  Drummond,  T.,  Scalable  Monocular SLAM. in Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on, (2006), 469-476.","cites":null},{"id":16396818,"title":"Shape from focus: an effective approach for rough surfaces. in Robotics and Automation,","authors":[],"date":"1990","doi":"10.1109\/robot.1990.125976","raw":"Nayar, S.K. and Nakagawa, Y., Shape from focus: an effective approach for rough surfaces. in Robotics and Automation,  1990.  Proceedings.,  1990  IEEE International Conference on, (1990), 218-225 vol.212.","cites":null},{"id":16396811,"title":"Shape recognition and pose estimation for mobile augmented reality","authors":[],"date":"2009","doi":"10.1109\/ismar.2009.5336498","raw":"Hagbi, N., Bergig, O., El-Sana, J. and Billinghurst, M. Shape  recognition  and  pose  estimation  for  mobile augmented reality Proceedings of the 2009 8th IEEE International  Symposium  on  Mixed  and  Augmented Reality, IEEE Computer Society, 2009, 65-71.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2011-06","abstract":"Whilst there has been considerable progress in augmented reality over recent years it has principally been related to either marker based or apriori mapped systems which limits its opportunity for wide scale deployment. Recent advances in marker-less systems that have no apriori information using techniques borrowed from robotic vision are now finding their way into mobile augmented reality and are producing exciting results. However, unlike marker based and apriori tracking systems these techniques are independent of scale which is a vital component in ensuring that augmented objects are contextually sensitive to the environment they are projected upon. In this paper we address the problem of scale by adapting a Depth From Focus (DFF) technique, which has previously been limited to high-end cameras to a commercial mobile phone. The results clearly show that the technique is viable and with the ever-improving quality of camera phone optics, add considerably to the enhancement of mobile augmented reality solutions. Further as it simple require a platfrom with an auto-focusing camera the solution is applicable to other AR platforms","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/10949.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/39956\/1\/Estimating_Scale_using_Depth_From_Focus_for_Mobile_Augmented_Reality_DRAFT.pdf","pdfHashValue":"97848229335786ca39dd560efabdf68e68369931","publisher":"ACM","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:39956<\/identifier><datestamp>\n      2018-01-24T02:04:16Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D626F6F6B5F73656374696F6E<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Estimating scale using depth from focus for mobile augmented reality.<\/dc:title><dc:creator>\n        \u010copi\u010d Pucihar, Klen<\/dc:creator><dc:creator>\n        Coulton, Paul<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        Whilst there has been considerable progress in augmented reality over recent years it has principally been related to either marker based or apriori mapped systems which limits its opportunity for wide scale deployment. Recent advances in marker-less systems that have no apriori information using techniques borrowed from robotic vision are now finding their way into mobile augmented reality and are producing exciting results. However, unlike marker based and apriori tracking systems these techniques are independent of scale which is a vital component in ensuring that augmented objects are contextually sensitive to the environment they are projected upon. In this paper we address the problem of scale by adapting a Depth From Focus (DFF) technique, which has previously been limited to high-end cameras to a commercial mobile phone. The results clearly show that the technique is viable and with the ever-improving quality of camera phone optics, add considerably to the enhancement of mobile augmented reality solutions. Further as it simple require a platfrom with an auto-focusing camera the solution is applicable to other AR platforms.<\/dc:description><dc:publisher>\n        ACM<\/dc:publisher><dc:date>\n        2011-06<\/dc:date><dc:type>\n        Contribution in Book\/Report\/Proceedings<\/dc:type><dc:type>\n        NonPeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/39956\/1\/Estimating_Scale_using_Depth_From_Focus_for_Mobile_Augmented_Reality_DRAFT.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1145\/1996461.1996531<\/dc:relation><dc:identifier>\n        \u010copi\u010d Pucihar, Klen and Coulton, Paul (2011) Estimating scale using depth from focus for mobile augmented reality. In: Proceedings of the 3rd ACM SIGCHI symposium on Engineering interactive computing systems. ACM, pp. 1-6. ISBN 978-1-4503-0670-6<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/39956\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1145\/1996461.1996531","http:\/\/eprints.lancs.ac.uk\/39956\/"],"year":2011,"topics":["QA75 Electronic computers. Computer science"],"subject":["Contribution in Book\/Report\/Proceedings","NonPeerReviewed"],"fullText":"  \nEstimating Scale using Depth From Focus for Mobile \nAugmented Reality  \n \nKlen \u010copi\u010d Pucihar \nSchool of Computing and Communications \nInfoLab21, Lancaster University \nLancaster LA1 4WA UK \n+44 1524 510393 \nk.copicpucihar@lancaster.as.uk \nPaul Coulton \nSchool of Computing and Communications  \nInfoLab21, Lancaster University  \nLancaster LA1 4WA UK \n+44 1524 510393 \np.coulton@lancaster.ac.uk \n \nABSTRACT \nWhilst there has been considerable progress in augmented \nreality over recent years it has principally been related to \neither marker based or apriori mapped systems which limits \nits opportunity for wide scale deployment. Recent advances \nin marker-less systems that have no apriori information \nusing techniques borrowed from robotic vision are now \nfinding their way into mobile augmented reality and are \nproducing exciting results. However, unlike marker based \nand apriori tracking systems these techniques are \nindependent of scale which is a vital component in ensuring \nthat augmented objects are contextually sensitive to the \nenvironment they are projected upon. In this paper we \naddress the problem of scale by adapting a Depth From \nFocus (DFF) technique, which has previously been limited \nto high-end cameras to a commercial mobile phone. The \nresults clearly show that the technique is viable and with \nthe ever-improving quality of camera phone optics, add \nconsiderably to the enhancement of mobile augmented \nreality solutions. Further as it simple require a platfrom  \nwith an auto-focusing camera the solution is applicable to \nother AR platforms. \nKeywords \nMobile, scale, metric scale, camera, phone, augmented \nreality. \nINTRODUCTION \nOne of the main challenges of Augmented Reality (AR) \nsystems is camera tracking, which can be implemented \nusing fiducial markers or natural-features. In fiducial based \nsystems the scale ambiguity is not present as it can be \neasily derived by using markers of a known size, whereas \nin the natural feature based systems it is only possible if the \nsystem is of informed type where the apriori knowledge of \nthe view being studied is available i.e. where a database of \nlandmarks forming the map is created offline and the map \ncreation process introduces metric scale. In the case of \nnatural feature tracking where the 3D map is created online \nfrom natural features alone, the scale is unknown because it \nis impossible to determine the scale of the scene based on a \nsequence of images alone [5].  \nIn fiducial marker and apriori feature tracking systems \nscale ambiguity is not a problem although such systems \noffer limited prospects of large scale deployment as they \nwould require either wide scale augmentation of our \nphysical space with fiducial markers or wide scale 3D \nmapping of our physical environment. The alternative \noptions are maker-less AR systems that use online tracking \napproaches without apriori information the method of map \ncreation and camera pose estimation can vary from the \nmodel-based to move-matching approaches. \nIn the case of online model-based approach the camera \npose is always estimated by comparing the initial frame \nwith the current camera frame. The initial frame is an \nimage taken directly from above the plane, or one that is \nsynthetically un-projected from additional sensor \ninformation, by which perspective distortions of camera \nprojections are removed and the extracted landmarks can be \nused as an object model of the plane in the scene. As the \nsame initial fame is always taken for pose estimation, such \nsystem is not incremental and does not have problems with \ndrift or loop closures [1]. However, such systems are \nlimited to planar scenes, as landmarks not lying on a plane \ncannot be initialized from only one observation thus  \nmaking extraction of the depth information using stereo \nvision impossible. Furthermore, as the initial frame is \nalways used for the camera pose estimation, all newly \nadded features need to be referenced to the initial frame, \nwhich in practice means long term maps where features are \ntracked over a long period of time.  \nAR systems that use this approach, but differ in the sense \nthat their maps are created offline have been created [12] \n[17], however, there is no reason why such systems could \nnot be modified to act as uninformed tracking systems \nwhich would improve their use flexibility, but at the same \ntime introduce the scale ambiguity. One such system \nrunning on a mobile phone is Nestor [4] in which curves of \nplanar shapes are used for tracking and  shape \nidentification. The shapes are added to the database of \nknown shapes by the method described above, and are then \nused as natural features for camera pose extraction as well \n \nLEAVE BLANK THE LAST 2.5 cm (1\u201d) OF THE LEFT \nCOLUMN ON THE FIRST PAGE FOR THE \nCOPYRIGHT NOTICE. \n \n \n  \nas to select 3D objects for augmentation.  The drawback of \nthis system is that it is also ambiguous up to scale. \nIn the later case of move-matching techniques, the camera \npose is updated based on the frame-to-frame movement of \ntracked features. Such system [14] is incremental as after \neach frame is acquired the camera pose update from the \nprevious frame is computed. This approach does not \nrequire long-term feature tracking and with it the \nrequirement to maintain long-term maps, which makes it \nmore flexible and faster as computationally expensive \nbundle adjustment of the map is not required. Furthermore \nit also enables depth estimation and with it the camera pose \nextraction from non-planar surfaces. However, as the \nmethod is incremental, it is hard to avoid drift, which also \nintroduces the problem of loop closures [1]. In case of non-\nplanar surfaces, one of the main problems is the \ninitialization of the system where the camera pose and the \nmap environment are unknown. To solve this problem, the \nSimultaneous Localization and Mapping (SLAM) \ntechnique were developed in the field of robotic exploration \nand were later adopted by AR systems. \nTwo such SLAM algorithms, EKF-SLAM [15] and \nFastSLAM2.0 [10] both use incremental mapping methods \nand were later adapted for hand held cameras [1] [2]. In \nthese systems, the map is initialized by a fiducial marker \nthrough which the scale of the map becomes available. \nHowever in case of Eade and Drummond SLAM \nimplementation [2] the map can also be initialized without \nthe necessity of a marker but in this case the scale again \nbecomes unknown. Note after the map initialization, \nnatural features are used for expanding the map and \ntracking the camera pose. \nFurther developments of single hand held camera tracking \nwere achieved using the Parallel Tracking and Mapping \nalgorithm (PTAM) [6], which differentiates from others by \nseparating the mapping and tracking tasks. In PTAM \nbundle adjustment is used as an alternative to incremental \nmapping in which long-term maps are created and features \nare frequently revisited. The map initialization is done with \nfive-point stereo algorithm or in the later versions by \nhomography decompositions. In both of these cases the \nmetric scale is unknown if no additional information is \navailable. \nCurrently the only presented alternative for estimating scale \nis performed during the process of stereo map initialization \nas demonstrated in PTAM [6] whereby users were asked to \nprovide first two keyframes of the map by moving the \ncamera sideways for approximately 10 cm during from \nwhich the metric scale of the map could be estimated as \nadditional information was introduced to the captured video \nstream. However, according to Klein and Murray, this map \ninitialisation method proved to be problematic as users \ntended to use pure rotation rather then lateral movement, \nthus the correct map initialization was heavily dependent \non users understanding of the stereo baseline requirements \n[7]. Furthermore, introduction of scale in this manner is \nsubjective as the user camera movement is approximate and \nsubjectively assessed. \nTo date a highly modified variation of PTAM for the \niPhone is the only implementation of six degrees of \nfreedom camera tracking SLAM on a mobile phone where, \naccording to Klein and Murray, stereo initialization was \ndetermined inadequate not only because of the introduction \nof the user error previously defined, but due to the \nlimitations of the mobile phone platform, in particular the \nlimited computational power and narrow camera field of \nview [7]. In the alternative map initialization, Klein and \nMurray, ask the user to only provide the first key-frame, \ntherefore, the previously defined additional information is \nlost. This means that currently there are no marker-less \nmobile AR systems that provide an estimate of scale.  \nIn this paper we introduce a possible solution for providing \nmetric scale for marker-less AR systems with no apriori \ninformation by utilizing the Depth From Focus (DFF) \ntechnique. In the following section the theoretical \nbackground of the method will be presented followed by \nthe design patterns section where two different generic \nscale implementations will be discussed. The solutions are \nthen analyzed through an empirical study of a specific \nimplemented on a commercially available mobile phone the \nNokia N900. Note that the proposed solution is platform, as \nwell as operating system, independent and could therefore \nbe implemented on any auto-focusing system where access \nto the camera driver is available. Finally the \nimplementation of a demo application will be presented \nfollowed by the conclusions and further work. \nTHEORETICAL FRAMEWORK \nDigital cameras are generally auto-focused by searching for \nthe lens position that gives the \u2018best\u2019 focused image, thus \nthe lens position is dependent on the distance to the object \nas shown in Figure 1.  If the focused lens position and the \nfocal distance of the lens are known, the thin Gaussian lens \nequation (1) can be used to calculate DFF i.e. the distance \nto the object u.  \n \n! \n1\/ f = 1\/u +1\/v  (1) \nThis method has been mainly used in the domain of robotic \nvision as an alternative to stereo depth recovery. One of the \nmain choices with this method is which focus measure to \nuse in order to identify the best lens position [18]. An ideal \nfocus measure is described as unimodal and monotonic in \nthat it should have only one maximum at the point where \nthe image is in focus [11] [16]. However, in practice any \nfocal measure has many local maximums, therefore, the \nglobal peak of the focal measure is not easy to find. \nFurthermore, as it has been observed by [11] and [18], not \nonly the texture and contrast of the scene, but also the depth \nof field (DOF) influence the maximums of focus measure \nfunction. It is preferable to have good texture with high \ncontrast as well as the smallest possible DOF, which can be \nachieved by using the maximal focal distance of the camera \nas well as maximal aperture. Furthermore, with bigger focal \ndistance the lens movement for focusing the image is \n  \nbigger which is expected to increases the resolution and \nprecision of the lens positioning system. \n \nFigure 1: Image formation in a convex lens \nIn order to calculate the scale unit s of the scene, one needs \nto know the distance to the object plane u, the vertical or \nhorizontal cameras field of view\n! \n\"  and picture height or \nwidth in pixels 2h\u2019. The calculation of scale unit s is then \nbased on simple trigonometry as shown in equation (2). In \ncase of the augmented reality application, the user would \nneed to focus on the plane where at least two map points \nare present. After defining the scale between two map \npoints the scale of the whole map is known.  \n \n! \nh = u tan \" \/ 2( ) # s = h[mm] \/h'[ px] (2) \nThe measurable depth of DFF system is theoretical limited \nby the hyperfocal distance, which is a defined as a minimal \nobject distance at which we need to focus in order to \nconsider the points at infinity to be in focus. However, as \nDOF needs to be as small as possible in order to achieve \nreliable and accurate results, such distance lies much closer. \nThe relation between the size of DOF and hyperfocal \ndistance is inversely proportional with the ratio between \nobject and hyperfocal distance, therefore the accuracy of \nthe system is expected to be better in the close up range. In \ncase of an AR application with the ability to expand the \nmap, the scale estimation only imposes the limitation for \nclose up system initialization, which can then expand the \nmap to desirable proportions. Therefore, the size of the AR \nworkspace is not limited by the requirement of a close-up \ninitialization.  \nFrom the previous discussion it is obvious that the scale \ncould be introduced to uninformed marker-less AR systems \nif the system has a camera with auto-focusing capability \nand allows access to the camera driver. In the following \nsection the generic implementation for scale estimation will \nbe designed. \nDESIGN PATTERNS FOR GENERIC IMPLMENTATION \nThe motor count captured from the camera driver is \nassumed to represent the relative distance of the lens in the \nmotor step domain. In order to use motor count with the \nGaussian lens equation (1), the conversion to absolute \ndistance in metric space (on Figure 1 shown as v) is \nrequired. An alternative is to capture measurements across \nthe whole focusing range and define an approximation \nfunction that will define the transformation from motor \ncount to object distance. This research analyzes both cases \nas it has some significant implications for the user \ninteraction requirements as well as the flexibility of the \nsystem. \nIn the first mode of operation, the camera system is \nassumed to be unknown. In order to convert the motor \ncount to object distance, the lens equation (1) can be used, \nhowever, as already indicated, the lens movement interval \nis usually unknown. The only information available about \nthe lens position is its motor count, which needs to be \nconverted to lens distance v in metric space.  \nThe proposed solution is to focus the camera at the object, \nat two different, but known distances. The first \nmeasurement should be taken close to the minimal \nfocusable object distance, and the second at approximately \none sixth of the hyperfocal distance. The range is \nperformed in the close up region of the camera as the \naccuracy of DFF system is expected to decline with object \ndistance and it is important to ensure that the calibration of \nthe motor step values is made in a way to best fit the lens \nequation in the close up region. As the distances to the \nobject are known, the theoretical lens position can be \ncalculated using the equation (1) by which the motor step \nunit is defined. The difference between the current motor \ncount value m and the minimal motor count value \n! \nm0  make \nthe conversion of motor step count to metric space possible \nby equation (3). \n \n! \nv = v0 + \"m #unit \"m = m $m0  (3) \nIn the second mode of operation, the assumption of a \nknown camera model is made, which enables the use of an \napproximation function to transform the motor count step \nto object distance. No user calibration is therefore required \nfor this mode of operation, however, this would limit its \napplicability to known camera models for which the \napproximation curve has been predefined. In the next \nsection, we will make a case study of the scale estimation \naccuracy using standard camera phone Nokia N900 running \nMaemo operating system. It is important to note, that the \nproposed solution is not limited to the specific operating \nsystem nor the device. In the following section the \nproposed solutions will be analyzed through an empirical \nstudy of a specific implemented on mobile phone the Nokia \nN900. \nEMPERICAL STUDY OF SCALE ESTIMATION \nThe data presented in this section was captured with four \nphones where auto-focusing was performed by two \nfocusing algorithms, namely, the native camera application \nalgorithm and by utilising the \u2018gstreamer\u2019 library. The \nphone camera used is a 5-mega pixels camera with Carl. \nZeiss optics with a focal length of 5.2 millimetres, aperture \nf\/2.8 and a horizontal field of view of 56 degrees.\n  \n  \n \nFigure 2: The left graph shows the average lens position in relation to object distance in mode one operation. The graph in the \nmiddle shows the average lens position in motor step space in relation to object distance for mode two operation and exponential \napproximation function (3). To the right, the screen shot of the Metre application measuring the laptop is shown.\nIn all measurements the same randomly selected A3 colour \nposter with good contrast and texture was used. The \nmeasurements were repeated 20 times at each given \ndistance. In case of \u2018gstreamer\u2019 library, the phone was \nfocused at an object at random distance before capturing \neach measurement. In the case of native application the \nmeasurements were taken in a sequence starting at minimal \nobject distance. Using the data analysis of the \u2018gstreamer\u2019 \ndataset, a decision was made to only capture values in the \nrange between 70 and 500 millimetres in the next \nexperiment, as results above this distance were considered \nto be to unreliable. The motor count value was assumed to \nbe represented by the \u2018V4L2_CID_ FOCUS_ABSOLUTE\u2019 \nvariable of the Video4Linux2 camera driver and was \ncaptured after each successful focusing. \nDFF Accuracy Using Gaussian Lens Equation (Mode 1) \nIn order to analyze how well the Gaussian lens equation (1) \nfits the captured data, the average values of measured lens \ndisplacements of each phone were plotted alongside the \ntheoretical values obtained using the lens equation function \nand are shown in the left graph of . The camera was \ncalibrated at object distances of 70 and 250 millimeters. \nAlthough the shape of the theoretical curve runs relatively \nclose to the captured data set it is still considerably \ndifferent. As expected, the dataset is best described close to \nthe value of \n! \nv0  which is the lens position of the far point \nused in the calibration procedure of mode one operation. \nFurthermore, it can be observed that the two focusing \nalgorithms produce similar results and that the deviation of \nresults between four different phones is small. The results \nprove that the assumptions made are correct and that the \ncaptured value from the camera driver is interpreted \ncorrectly. \nThe accuracy of depth measurement can be best described \nwith the relative depth error (shown on in the left graph of \nFigure 3), which is also the relative error of the scale \nintroduced to the AR system because the only variable in \nscale calculation of equation (2) is the object distance. The \nmaximal relative depth error at distances below 300 \nmillimetres ranges from 9.5 up to 15.8 percent, which is \ncompared to results acquired with precise camera systems \n(0.098% acquired at 1.2 meters) still very high [18]. \nHowever, it should be taken into account that the focal \nlength, lens mechanics and quality of such high precision \ncamera systems limit the direct comparability to the mobile \nphone camera. These limitations could be also seen as one \nof the reasons for deviation of the dataset from the \ntheoretical lens equation. Furthermore, as discussed in the \nmethod section, some of these parameters have significant \neffect on the focus measure that is a crucial component of \nthe auto-focusing accuracy.  \nDFF Accuracy Using Approximation (Mode 2) \nIn order to improve the accuracy of the system and to \nremove need for user calibration, the mode two solution \nproposed the use of approximation function for mapping \nthe transformation from motor count space to object \ndistance. The exponential approximation curve (3) was \ndetermined from the average data set of all measurements \ntaken by the \u2018gstreamer\u2019 focusing algorithm. To make the \nfunction best represent the data at a close range, only \nmeasurements up to 400 millimetres were considered. \n \n! \nv(u) = a \/u + b  (3) \nThe centre graph of  shows that this new curve better \nrepresents the measurements, especially in the close up \nregion. The maximal relative depth error can be seen in the \ncentre graph of Figure 3 and shows that the maximal \nrelative depth error does not drop but stays at comparable \nlevels to mode one operation, however it is obvious, that \nthe approximation function describes the data far more \nconsistently as in the case of lens equation because the \nrelative depth error graph does not show the distinctive \nminima at the calibration value\n! \nv0 that can be observed in \nmode one operation. It can also be seen that the standard \ndeviation of results between different phones seams to have \ngrown compared to mode one operation. This is due to the \nfact that motor count values have not been normalized as in \nthe case of mode one operation. \nTo explore the full potential of the system, a decision was \nmade to analyze behaviour of the system where a simple \none step calibration was added to the mode two operation. \nAs it is easier for the user to calibrate the device at the \nclose up region, the user is asked to calibrate their phone \nclose to the minimal focusable distance, in our case at 70 \nmillimetres.  The exponential approximation curve needs to\n  \n   \nFigure 3: The graphs in this section all show Relative Depth Error of object distance. In the graph to the left mode one operation \nresults are shown, followed by the mode two results and finally on the right the results for a single step user calibration of mode \ntwo are presented.  Each line of the graph represents one of the 4 phones., and the two colours denote different focusing algorithms. \nbe recalculated for the measurements which are normalized \nat the maximum motor count captured by user calibration. \nAgain only the measurements captured with \u2018gstreamer\u2019 \nfocusing algorithm in the region up to 400 millimetres were \nused. In operation each time the new motor count value is \ncaptured, it is subtracted from the user calibrated maximal \nmotor count value and converted to distance using the new \nfitting function. By doing this the maximal relative depth \nerror shown at the middle of Figure 3 has dropped to a \nrange between 6.5 to 12.2 percent.  \nIf the two different focusing algorithms are compared, their \nperformance does not differ significantly in the close up \nregion, however this is not the case in the regions further \naway from the camera. The accuracy of the native camera \nfocusing algorithm in distant regions is better then what \nwould be expected. The possible reason for this could be \nthe already mentioned difference in the procedure how the \ntwo experiments were executed. Contrary to the \n\u2018gstreamer\u2019 measurement, the native camera, was not \nrefocused at a randomly distant object before capturing \neach measurement, but was rather capturing the \nmeasurements in a sequence.  \nIt is important to note that the captured measurements were \ntaken under the controlled environment in good lighting \nconditions with good focusing surface, with no user factor \nerror, therefore, the accuracy in real world scenario could \nbe expected to decrease. Furthermore, as incremental \nSLAM techniques continuously update the map and camera \npose with increments, the overall scale could be affected by \naccruing the local scale errors [3], however, this would not \nbe the case in SLAM approaches where batch methods are \nused to maintain long term maps. Furthermore, accruing of \nscale error would also not be present in the marker-less \nobject-based tracking systems as those systems are not \nincremental and drift is not a problem. \nTo sum up, this data analysis shows that the proposed \nsolution is valid and can produce reasonable scale \nestimation with relative error raging from 6.5 to 12.2 \npercent in the region between 70 and 300 millimetres. In \nthe following section, a demo application called Metre will \nbe discussed in order to demonstrate a use case of the \nproposed solution. \nAPPLICATION SPECIFIC IMPLEMENTATION \nTo highlight the technique rather than content a simpler \napplication providing scale to a captured picture was \ndeveloped. The demo application is called Metre and \nenables users to measure objects on a taken picture. The \napplication was implemented on Nokia N900 phone where \nthe tracking part of the application was implemented using \nthe OpenCV library, the video capturing and auto-focusing \nwere implementation using \u201cgstreamer\u201d library and the \nscale was initialized by the solution of mode two operation.  \nIn order to increase the maximal size of the objects and to \nimprove accuracy of the measurement, the application \nenables users to introduce scale close to the object they \nwant to measure and then move back to get the full view of \nthe object. In the scale initialization process, two natural \nfeatures that are chosen based on Shi and Tomasi good \ncorner definition [13], are being tracked using optical flow, \nwhich is calculated in the small window region of selected \npoints by the Pyramid Lucas-Kanade algorithm [9]. As the \ndistance between the two points is known from the scale \ninitialization step the scale is known as long as the two \nfeatures are successfully tracked. The screen shot of the \napplication can be seen in the right corner of .  \nCONCLUSION AND FUTURE WORK \nThe results show that auto-focusing capability of the \ncamera phone can be used to effectively introduce the scale \nestimate into the marker-less AR workspace without apriori \ninformation. However, currently the method is limited to \nthe close up initialization (in our case distances up to 300 \nmillimeters) as in this region the maximal relative scale \nerror is expected to stay in the range of 6.5-12.2 percent.  \nThe limitation in range and accuracy is mainly due to the \nsmall focal length (5.2 millimetres) of a camera phone, \nwhich results in short hyperfocal distance and therefore a \nsmall DFF range. It was discovered that the region up to \n1\/9 of the hyperfocal distance was to be accurate enough. It \nis important to note that marker-less AR systems which \ncreate 3D maps online have the potential to dynamically \nexpand these maps. This means the requirement of a close \nup initialization is only necessary at the start of the \nmapping process after which the map can be expand to \ndesired proportions. Furthermore, range capability \n  \nlimitations are likely to be overcome by the next generation \ncamera phones in which the focal distance is expected to \nraise by the introduction of optical zoom lenses.  \nHowever, it is important to identify that the ideal AR \nplatform would use a camera with a wide field of view, \nwhich in practice means even smaller focal distances than \nthe one used in this case study. Furthermore, most camera \npose tracking systems use a camera projection model where \nthe intrinsic parameters are assumed to be known and fixed \n[8]. As zooming changes the intrinsic parameters of the \ncamera, it is not permitted. However, this problem could be \novercome, by moving the zoom back to the original \nposition after initializing for scale.  \nIn the future, a fully featured uninformed marker-less \naugmented reality application with the proposed scale \nestimation will be implemented in order to explore the user \ninteraction and to test applications that could take \nadvantage of the newly added scale information. Finally, as \nthe proposed scale estimation is device and platform \nindependent, a more detailed feasibility study for \nimplementing the proposed system on other suitable AR \nplatforms should be preformed. \nTo conclude, the proposed method can be used to introduce \nscale into marker less AR systems without the requirement \nof apiori knowledge of the workspace, however, such scale \nestimation is currently limited to a small close up range. \nNevertheless, as AR systems have potential to dynamically \nexpend their maps, the close up initialization does not limit \nthe size of their workspace. Furthermore, by introducing \nbetter lens optics, and optical zoom lenses to mobile \ndevices, the accuracy and range will inevitably improve. \nACKNOWLEDGMENTS \nThe authors would like to thank Nokia for the provision of \nsoftware and hardware to the Mobile Radicals research \ngroup at Lancaster University which was used in this \nproject. \nREFERENCES \n1. Davison, A.J., Reid, I.D., Molton, N.D. and Stasse, O. \nMonoSLAM: Real-Time Single Camera SLAM. \nPattern Analysis and Machine Intelligence, IEEE \nTransactions on, 29 (6). 1052-1067. \n2. Eade, E. and Drummond, T., Scalable Monocular \nSLAM. in Computer Vision and Pattern Recognition, \n2006 IEEE Computer Society Conference on, (2006), \n469-476. \n3. Ethan, E. and Tom, D. Presentation of paper: Scalable \nMonocular SLAM, 2006. \n4. Hagbi, N., Bergig, O., El-Sana, J. and Billinghurst, M. \nShape recognition and pose estimation for mobile \naugmented reality Proceedings of the 2009 8th IEEE \nInternational Symposium on Mixed and Augmented \nReality, IEEE Computer Society, 2009, 65-71. \n5. Hartley, R.I. and Zisserman, A. Multiple View \nGeometry in Computer Vision. Cambridge University \nPress, ISBN: 0521540518, 2004. \n6. Klein, G. and Murray, D. Parallel Tracking and \nMapping for Small AR Workspaces Proc. Sixth IEEE \nand ACM International Symposium on Mixed and \nAugmented Reality (ISMAR'07), Nara, Japan, 2007. \n7. Klein, G. and Murray, D. Parallel Tracking and \nMapping on a Camera Phone Proc. Eigth IEEE and \nACM International Symposium on Mixed and \nAugmented Reality (ISMAR'09), Orlando, 2009. \n8. Lepetit, V. and Fua, P. Monocular model-based 3D \ntracking of rigid objects. Found. Trends. Comput. \nGraph. Vis., 1 (1). 1-89. \n9. Lucas, B.D. and Kanade, T., An Iterative Image \nRegistration Technique with an Application to Stereo \nVision. in IJCAI81, (1981), 674-679. \n10. Montemerlo, M. and Thrun, S. FastSLAM 2.0: An \nImproved Particle Filtering Algorithm for \nSimultaneous Localization and Mapping that Provably \nConverges, Conference on Artificial Intelligence \n(ICCV\u201907), Rio de Janeiro, 2003, 1151\u20131156. \n11. Nayar, S.K. and Nakagawa, Y., Shape from focus: an \neffective approach for rough surfaces. in Robotics and \nAutomation, 1990. Proceedings., 1990 IEEE \nInternational Conference on, (1990), 218-225 vol.212. \n12. Prince, S.J.D., Xu, K. and Cheok, A.D. Augmented \nreality camera tracking with homographies. Ieee \nComputer Graphics and Applications, 22 (6). 39-45. \n13. Shi, J. and Tomasi, C. Good features to track \nComputer Vision and Pattern Recognition, 1994. \nProceedings CVPR '94., 1994 IEEE Computer Society \nConference on, 1994, 593 -600. \n14. Simon, G., Fitzgibbon, A.W. and Zisserman, A. \nMarkerless tracking using planar structures in the \nscene Augmented Reality, 2000. (ISAR 2000). \nProceedings. IEEE and ACM International Symposium \non, 2000, 120 -128. \n15. Smith, R.C. and Cheeseman, P. On the Representation \nand Estimation of Spatial Uncertainty. The \nInternational Journal of Robotics Research, 5 (4). 56-\n68. \n16. Subbarao, M., Choi, T. and Nikzad, A. Focusing \nTechniques. Optical Engineering, 32 (11). 2824-2836. \n17. Wagner, D., Reitmayr, G., Mulloni, A., Drummond, T. \nand Schmalstieg, D. Pose tracking from natural \nfeatures on mobile phones ISMAR '08: Proceedings of \nthe 7th IEEE\/ACM International Symposium on Mixed \nand Augmented Reality, IEEE Computer Society, \nWashington, DC, USA, 2008, 125-134. \n18. Xiong, Y. and Shafer, S.A., Depth from focusing and \ndefocusing. in Computer Vision and Pattern \nRecognition, 1993. Proceedings CVPR '93., 1993 IEEE \nComputer Society Conference on, (1993), 68-73. \n \n"}