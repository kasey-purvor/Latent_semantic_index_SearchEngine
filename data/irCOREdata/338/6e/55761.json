{"doi":"10.1016\/j.robot.2006.04.013","coreId":"55761","oai":"oai:eprints.lincoln.ac.uk:1848","identifiers":["oai:eprints.lincoln.ac.uk:1848","10.1016\/j.robot.2006.04.013"],"title":"Improved data association and occlusion handling for vision-based people tracking by mobile robots","authors":["Cielniak, Grzegorz","Duckett, Tom","Lilienthal, J. Achim"],"enrichments":{"references":[{"id":18438650,"title":"A decision-theoretic generalization of on-line learning and an application to boosting,\u201d","authors":[],"date":"1995","doi":"10.1006\/jcss.1997.1504","raw":"Y. Freund and R. E. Schapire, \u201cA decision-theoretic generalization of on-line learning and an application to boosting,\u201d in Computational Learning Theory: Eurocolt. Springer-Verlag, 1995, pp. 23\u201337.","cites":null},{"id":18438657,"title":"An MCMC-based particle \ufb01lter for tracking multiple interacting targets,\u201d in","authors":[],"date":"2004","doi":"10.1007\/978-3-540-24673-2_23","raw":"Z. Khan, T. Balch, and F. Dellaert, \u201cAn MCMC-based particle \ufb01lter for tracking multiple interacting targets,\u201d in Proc. ECCV, 2004.","cites":null},{"id":18438661,"title":"Color indexing,\u201d","authors":[],"date":"1991","doi":"10.1007\/bf00130487","raw":"M. Swain and D. Ballard, \u201cColor indexing,\u201d International Journal of Computer Vision, vol. 7, pp. 11\u201332, 1991.","cites":null},{"id":18438652,"title":"Condensation \u2013 conditional density propagation for visual tracking,\u201d","authors":[],"date":"1998","doi":"10.1007\/bfb0015549","raw":"M. Isard and A. Blake, \u201cCondensation \u2013 conditional density propagation for visual tracking,\u201d International Journal of Computer Vision, vol. 29, no. 1, pp. 5\u201328, 1998.","cites":null},{"id":18438659,"title":"Evaluating multi-object tracking,\u201d","authors":[],"date":"2005","doi":"10.1109\/cvpr.2005.453","raw":"K. Smith, D. Gatica-Perez, J. M. Odobez, and S. Ba, \u201cEvaluating multi-object tracking,\u201d in Workshop on Empirical Evaluation Methods in Computer Vision, San Diego, CA, USA, 2005.","cites":null},{"id":18438647,"title":"Face tracking and hand gesture recognition for human-robot interaction,\u201d in","authors":[],"date":"2004","doi":"10.1109\/robot.2004.1308101","raw":"L. Br` ethes, P. Menezes, F. Lerasle, and J. Hayet, \u201cFace tracking and hand gesture recognition for human-robot interaction,\u201d in Proc. IEEE ICRA, New Orleans, LA, USA, 2004, pp. 1901\u20131906.","cites":null},{"id":18438665,"title":"Kr\u00a8 ose, \u201cKeeping track of humans: have i seen this person before?\u201d in","authors":[],"date":"2005","doi":"10.1109\/robot.2005.1570420","raw":"W. Zajdel, Z. Zivkovic, and B. J. A. Kr\u00a8 ose, \u201cKeeping track of humans: have i seen this person before?\u201d in Proc. IEEE ICRA, Barcelona, Spain, 2005.","cites":null},{"id":18438656,"title":"Monte Carlo data association for multiple target tracking,\u201d in","authors":[],"date":"2001","doi":"10.1049\/ic:20010239","raw":"R. Karlsson and F. Gustafsson, \u201cMonte Carlo data association for multiple target tracking,\u201d in In IEEE Target tracking: Algorithms and applications, The Netherlands, 2001.","cites":null},{"id":18438651,"title":"Novel approach to nonlinear\/non-Gaussian Bayesian state estimation,\u201d","authors":[],"date":"1993","doi":"10.1049\/ip-f-2.1993.0015","raw":"N. J. Gordon, D. J. Salmond, and A. F. M. Smith, \u201cNovel approach to nonlinear\/non-Gaussian Bayesian state estimation,\u201d Proc. Inst. Elect. Eng. F, vol. 140, no. 2, pp. 107\u2013113, April 1993.","cites":null},{"id":18438648,"title":"People tracking by mobile robots using thermal and colour vision,\u201d","authors":[],"date":"2007","doi":"10.1016\/j.robot.2006.04.013","raw":"G. Cielniak, \u201cPeople tracking by mobile robots using thermal and colour vision,\u201d Ph.D. dissertation, \u00a8 Orebro University, April 2007.","cites":null},{"id":18438663,"title":"Rapid object detection using a boosted cascade of simple features,\u201d in","authors":[],"date":"2001","doi":"10.1109\/cvpr.2001.990517","raw":"P. Viola and M. Jones, \u201cRapid object detection using a boosted cascade of simple features,\u201d in Proc. IEEE CVPR, 2001.","cites":null},{"id":18438662,"title":"Real-time people tracking for mobile robots using thermal vision,\u201d","authors":[],"date":"2006","doi":"10.1016\/j.robot.2006.04.013","raw":"A. Treptow, G. Cielniak, and T. Duckett, \u201cReal-time people tracking for mobile robots using thermal vision,\u201d Robotics and Autonomous Systems, vol. 54, no. 9, pp. 729\u2013739, 2006.","cites":null},{"id":18438664,"title":"Sensor fusion for vision and sonar based people tracking on a mobile service robot,\u201d","authors":[],"date":"2002","doi":null,"raw":"T. Wilhelm, H. J. B\u00a8 ohme, and H. M. Gross, \u201cSensor fusion for vision and sonar based people tracking on a mobile service robot,\u201d in Int. Workshop on Dynamic Perception, Bohum, Germany, 2002, pp. 315\u2013 320.","cites":null},{"id":18438660,"title":"Similarity of color images,\u201d","authors":[],"date":"1995","doi":"10.1117\/12.205308","raw":"M. A. Stricker and M. Orengo, \u201cSimilarity of color images,\u201d in Storage and Retrieval for Image and Video Databases, 1995, pp. 381\u2013392.","cites":null},{"id":18438649,"title":"Tools and techniques for video performance evaluation,\u201d in","authors":[],"date":"2000","doi":"10.1109\/icpr.2000.902888","raw":"D. S. Doermann and D. Mihalcik, \u201cTools and techniques for video performance evaluation,\u201d in Proc. ICPR, vol. 4, Barcelona, Spain, 2000, pp. 4167\u20134170.","cites":null},{"id":18438658,"title":"Tracking multiple moving objects with a mobile robot,\u201d in","authors":[],"date":"2001","doi":"10.1109\/cvpr.2001.990499","raw":"D. Schulz, W. Burgard, D. Fox, and A. B. Cremers, \u201cTracking multiple moving objects with a mobile robot,\u201d in Proc. IEEE CVPR, 2001.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2007-10","abstract":"This paper presents an approach for tracking multiple persons using a combination of colour and thermal vision sensors on a mobile robot. First, an adaptive colour model is incorporated into the measurement model of the tracker. Second, a new approach for detecting occlusions is introduced, using a machine learning classifier for pairwise comparison of persons (classifying which one is in front of the other). Third, explicit occlusion handling is then incorporated into the tracker","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/55761.pdf","fullTextIdentifier":"http:\/\/eprints.lincoln.ac.uk\/1848\/1\/cielniak07improved.pdf","pdfHashValue":"53466a374dd8c0c49a4e2bf5bf6a54f52a963122","publisher":"IEEE","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lincoln.ac.uk:1848<\/identifier><datestamp>\n      2013-03-13T08:31:55Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F48:6A6163735F48363730<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F48:6A6163735F48363731<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47373430<\/setSpec><setSpec>\n      74797065733D636F6E666572656E63655F6974656D<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lincoln.ac.uk\/1848\/<\/dc:relation><dc:title>\n        Improved data association and occlusion handling for vision-based people tracking by mobile robots<\/dc:title><dc:creator>\n        Cielniak, Grzegorz<\/dc:creator><dc:creator>\n        Duckett, Tom<\/dc:creator><dc:creator>\n        Lilienthal, J. Achim<\/dc:creator><dc:subject>\n        H670 Robotics and Cybernetics<\/dc:subject><dc:subject>\n        H671 Robotics<\/dc:subject><dc:subject>\n        G740 Computer Vision<\/dc:subject><dc:description>\n        This paper presents an approach for tracking multiple persons using a combination of colour and thermal vision sensors on a mobile robot. First, an adaptive colour model is incorporated into the measurement model of the tracker. Second, a new approach for detecting occlusions is introduced, using a machine learning classifier for pairwise comparison of persons (classifying which one is in front of the other). Third, explicit occlusion handling is then incorporated into the tracker.<\/dc:description><dc:publisher>\n        IEEE<\/dc:publisher><dc:date>\n        2007-10<\/dc:date><dc:type>\n        Conference or Workshop contribution<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lincoln.ac.uk\/1848\/1\/cielniak07improved.pdf<\/dc:identifier><dc:identifier>\n          Cielniak, Grzegorz and Duckett, Tom and Lilienthal, J. Achim  (2007) Improved data association and occlusion handling for vision-based people tracking by mobile robots.  In: IEEE\/RSJ International Conference on Intelligent Robots and Systems (IROS), October 29 - November 2, San Diego, CA, USA.  <\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/IROS.2007.4399507<\/dc:relation><dc:relation>\n        10.1016\/j.robot.2006.04.013<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lincoln.ac.uk\/1848\/","http:\/\/dx.doi.org\/10.1109\/IROS.2007.4399507","10.1016\/j.robot.2006.04.013"],"year":2007,"topics":["H670 Robotics and Cybernetics","H671 Robotics","G740 Computer Vision"],"subject":["Conference or Workshop contribution","PeerReviewed"],"fullText":"Improved Data Association and Occlusion Handling\nfor Vision-Based People Tracking by Mobile Robots\nGrzegorz Cielniak\u2020, Tom Duckett\u2020 and Achim J. Lilienthal\u2217\n\u2020 Department of Computing and Informatics\nUniversity of Lincoln\nLN6 7TS Lincoln, United Kingdom\ngcielniak@lincoln.ac.uk,\ntduckett@lincoln.ac.uk\n\u2217 Centre for Applied Autonomous Sensor Systems\nO\u00a8rebro University\nSE-701 82 O\u00a8rebro, Sweden\nachim.lilienthal@tech.oru.se\nAbstract\u2014This paper presents an approach for tracking\nmultiple persons using a combination of colour and thermal\nvision sensors on a mobile robot. First, an adaptive colour model\nis incorporated into the measurement model of the tracker.\nSecond, a new approach for detecting occlusions is introduced,\nusing a machine learning classifier for pairwise comparison of\npersons (classifying which one is in front of the other). Third,\nexplicit occlusion handling is then incorporated into the tracker.\nI. INTRODUCTION\nThis paper presents a vision-based people tracking system\nallowing a mobile robot to detect and localise people in\nits surroundings, which uses a combination of thermal and\ncolour information (see [2] for further details). The approach\nis based on an existing tracking system for thermal im-\nages [13]. While thermal vision is good for detecting people,\nit can be very difficult to maintain the correct association\nbetween different observations and persons, especially where\nthey occlude one another. To further improve tracking of\nmultiple persons, this paper introduces three main improve-\nments to the system:\n\u2022 incorporation of an adaptive colour model into the\nmeasurement model of the tracker to improve data\nassociation, using the integral image representation to\nspeed up processing,\n\u2022 explicit detection of occlusions, using a machine learn-\ning algorithm AdaBoost for pairwise comparison of\npersons (classifying which one is in front of the other),\nand\n\u2022 integration of occlusion handling into the particle filter.\nMany approaches for people tracking on mobile platform\nare based on skin colour and face recognition (e.g., [15], [1]).\nHowever these methods require persons to be close to and\nfacing the robot so that their hands or faces are visible. The\nsystem in [9] uses a laser sensor to track multiple persons.\nIt is based on a particle filter and JPDAF data association.\nIt uses a global representation of the environment, requires\nthresholded sensor data and deals with occlusions of non-\ninteracting persons only. In contrast our system uses sensor\ncoordinates, incorporates unthresholded data and can reason\nabout occlusions of interacting persons. The work of [16]\npresents a robotic system that tracks and re-identifies persons\nwhen they re-appear on the scene. However the tracking\nprocedure is realised by a Baysian network that grows rapidly\nand requires storage of all data, and is therefore limited for\nuse in on-line applications.\nII. EXPERIMENTAL SET-UP\nWe used an ActivMedia PeopleBot robot (Fig. 1) equipped\nwith different sensors, including a colour pan-tilt-zoom cam-\nera (VC-C4R, Canon) and thermal camera (Thermal Tracer\nTS7302, NEC), and an Intel Pentium III processor (850\nMHz). The colour and thermal camera are mounted close to\neach other to allow for easy combination of the information\n(see Section IV-A). In our set-up the visible range on the\ngrey-scale thermal image was equivalent to the temperature\nrange from 24 to 36 \u25e6C.\nThe robot was operated in an indoor environment (a\ncorridor and lab room). Persons taking part in the exper-\niments were asked to walk in front of the robot while it\nperformed a corridor following behaviour or while the robot\nwas stationary. At the same time, image data were collected\nwith a frequency of 15Hz. The resolution of both thermal\nand colour images was 320\u00d7 240 pixels.\nIII. BASIC TRACKER USING THERMAL VISION\nA. Tracking a Single Person\nOur system uses a particle filter to provide an efficient\nsolution to the estimation problem despite the high dimen-\nsionality of the state space. The particle filter performs both\ndetection and tracking simultaneously without exhaustive\nsearch of the state space. Moreover the measurements are\nincorporated directly into the tracking framework without\nany preprocessing such as thresholding that could cause loss\nof information.\nThe posterior probability p(xt|z1:t) of the system be-\ning in state xt given a history of measurements z1:t is\napproximated by a set of N weighted samples such that\nSt = {xit, wit}, i = 1, . . . , N. Each xit describes a possible\nstate together with a weight wit which is proportional to the\nlikelihood that the system is in this state. We use a standard\nSampling Importance Resampling (SIR) filter [5] starting\nwith a uniform initial distribution. The resampling step was\ncolour camera\nthermal camera\nFig. 1. ActivMedia PeopleBot robot equipped with a thermal camera and\na standard camera (left). Example of an image from the colour camera\n(right-top) and thermal camera (right-bottom).\nimplemented using the systematic resampling algorithm. The\ndynamic model used in the particle filter is a movement with\nconstant velocity plus small random changes.\nB. Tracking Multiple Persons\nThe above method is extended to the multi-person case\nby detecting new persons incrementally as they appear while\nmaintaining existing tracks of persons. This system uses a set\nof independent particle filters to track different persons. To\nassign new filters to new persons we use a sequential detector\nconsisting of a set of N randomly initialised particles. These\nparticles are used to \u201ccatch\u201d a new person entering the scene.\nTo avoid multiple detections in the same or similar regions,\nthe weight of detection particles is penalised by a factor\n\u03c8d < 1 in cases where particles cross already detected areas.\nThe weight update equation for the ith detection particle is\nmodified to wit \u221d p(zt|xt = xit)\u03c8, where \u03c8 = \u03c8d if particle\ni overlaps with other detected regions and \u03c8 = 1 otherwise.\nThus already existing filters naturally limit the search space\nfor the detector. Detection occurs when the average fitness of\nthe particles exceeds a certain threshold for a few consecutive\nframes (3 in our experiments). Then the particles from the\ndetector are used to initialise a new tracker before being re-\ninitialised for detection of the next new person.\nA solution based on independent tracking filters is com-\nputationally inexpensive and appropriate for on-line applica-\ntions, but suffers in cases when tracked persons are too close\nto each other. To reduce these problems we explicitly model\ninteractions between persons by penalising the weights of\nparticles that intersect with other detected regions. The\nweight update equation for established tracking filters is\nchanged to wit \u221d p(zt|xt = xit)\u03c8, where \u03c8 = e(\u2212\u03c1gim)\nand gim expresses the amount of overlap between particle\ni and region m, which is multiplied by a factor \u03c1 in the\nexponent of the penalty term. This solution is similar to the\ninteraction model proposed by [8], where the authors propose\na Random Markov Field using a joint state space repre-\nsentation. The treatment of interactions in both approaches\nhas the drawback that in the case of occlusions weaker\nfilters disappear. Motion information could help here only\nin specific situations where persons are just passing by each\nw\nh\nh\/2\nd\n(x,y)\n\u0394\n6\n\u0394\n1\n\u0394\n2\n\u0394\n3\n\u0394\n4\n\u0394\n5\n\u0394\n7\nFig. 2. The elliptic measurement model for thermal images. Model\nparameters are shown on the left. Division of ellipses into 7 regions is\nshown on the right.\nother at sufficient speeds. However this is not the case in\nsituations where people stop to talk, shake hands, walk in\ngroups, etc.\nC. Elliptic Contour Model\nThe measurement model used by our thermal tracker is\na contour model consisting of two ellipses: one describes\nthe position of the body part and the other measures the\nposition of the head part (Fig. 2). Thus we obtain a 9-\ndimensional state vector: xt = (x, y, w, h, d, vx, vy, vw, vh)\nwhere (x, y) is the mid-point of the body ellipse with width\nw and height h. The height of the head is calculated by\ndividing h by a constant factor. The displacement of the\nmiddle of the head part from the middle of the body ellipse\nis described by d. We also model velocities of the body part\nas (vx, vy, vw, vh). The velocity of the d component has very\nnoisy characteristics and is therefore not taken into account.\nTo calculate the importance weight wit of a sample i with\nstate xit we divide the ellipses into m = 7 different regions\n(see Fig. 2) and for each region j the image gradient \u2206ij\nbetween pixels in the inner and outer parts of the ellipse\nis calculated. The gradient is maximal if the ellipses fit\nthe contour of a person in the image data. A fitness value\nf i for each sample i is then calculated as the sum of all\ngradients multiplied with individual weights \u03b1j for each\nregion: f i =\n\u2211m\nj=1 \u03b1j\u2206\ni\nj . The weights \u03b1j sum to one and\nare chosen such that the shoulder parts have lower weight to\nminimize the measurement error that occurs due to different\narm positions. The fitness value is finally scaled to values in\n[0, 1] in order to represent a likelihood:\npg(zt|xit) =\nexp(\u03ba \u00b7 (f i \u2212 \u03b8))\nexp(\u03ba \u00b7 (f i \u2212 \u03b8)) + exp(\u03ba \u00b7 (\u03b8 \u2212 f i)) , (1)\nwhere \u03b8 denotes a fitness threshold and the value of \u03ba defines\nthe slope of the likelihood function.\nWhen the mean gradient value from Eq. 1 is greater than\n0.5 then a person is considered to be detected. We also\ncheck the uncertainty of the estimate [7] to avoid detections\nin wrong regions when the posterior is multi-modal (e.g.\nfor multiple persons). This approach is similar to the work\nby Isard and Blake [6] for tracking people in a greyscale\nimage. However, they use a spline model of the head and\nshoulder contour which cannot be applied in situations where\nthe person is far away or visible in a side view, because there\na) b)\nFig. 3. Rectangular features: a) thermal image b) colour image with regions\ncorresponding to different body parts from which colour information is\nextracted.\nwill be no recognisable head-shoulder contour. The elliptic\ncontour model used here is able to cope with these situations.\nIV. ADAPTIVE COLOUR MODEL\nA. Colour representation\nSince the baseline between cameras is small compared to\nthe distance to persons, it is possible to align the thermal\nand colour images by affine transformation. We then use\nan efficient colour representation proposed in [11] based\non the first three moments (mean, variance and skewness)\nof the colour distribution. This representation was shown\nto be more effective than histogram methods (e.g., [12])\nin the domain of image indexing. To include information\nabout the spatial layout of the colour we divided the region\ncorresponding to a person\u2019s body into rectangular sub-areas\nfrom which we calculate the colour statistics (see Fig. 3b).\nThe position and size of these regions is determined from\nthe information provided by the elliptic contour model.\nB. Colour likelihood\nThe appearance model based on colour moments is created\nevery time a new detection occurs, i.e. a new track is\ninitialised in the thermal image. By using the affine transfor-\nmation we are able to determine the region corresponding to\na person on the colour image (see Fig. 3). From three rectan-\ngular regions corresponding to the person\u2019s head, torso and\nlegs we collect colour statistics ct of the first three moments\n(m1,m2,m3) for three colour channels (R,G,B). Finally\nwe obtain a feature vector ct of size 3\u00d73\u00d73 = 27. To make\nthe model more robust to changing light conditions we adapt\nit while a person is tracked. In our implementation we store\ncolour statistics from the last nk frames and calculate their\nmean value. The parameter nk influences the robustness and\nadaptivity of the colour model. In our experiments nk = 10\ncorresponding to 0.7 s. We use Euclidean distance to measure\nthe similarity between the model c?t and region of interest\nct. Finally, the likelihood model for colour information is\npc(zt|xt) = exp\n(\u2212\u03bbd2t ) , (2)\nwhere \u03bb is a parameter that determines the shape of the\ncolour likelihood. Since \u03bb scales the distance, higher values\nof \u03bb mean that the colour-based likelihood model is more\npeaked, thus having more importance when combined with\nthe gradient information from the ellipse model.\nC. Rapid rectangular features\nThe simple features based on the colour moments can\nbe rapidly calculated using an integral image representation\n[14]. The estimators for the first three moments of the\ncolour distribution can be obtained by means of k statistics\ncalculated using sums of the rth powers of the colour data:\nSr =\nx+w\u2211\ni=x\ny+h\u2211\nj=y\nIr(i, j), (3)\nwhere I(i, j) is a pixel value of the colour image se-\nlected from the rectangular region specified by coordinates\n{x, y, x+w, y+h}. Each Sr can be quickly calculated using\nthe integral image representation. The first three k-statistics\nare obtained as\nk1 = S1\/n, (4)\nk2 =\nnS2 \u2212 S21\nn(n\u2212 1) , (5)\nk3 =\n2S31 \u2212 3nS1S2 + n2S3\nn(n\u2212 1)(n\u2212 2) , (6)\nwhere n = w\u00d7h. Finally the normalised values of estimators\nfor mean m1, variance m2 and skewness m3 can be obtained\nas m1 = k1, m2 = k2\/k1 and m3 = k3\/k\n3\n2\n2 . The\nnormalisation is performed to balance the influence of each\nmoment on the final score.\nD. Combining thermal and colour information\nIf we assume that the likelihoods for the gradient model\npg(zt|xt) (Eq. 1) and colour model pc(zt|xt) (Eq. 2) are\nindependent then the data fusion can be realised by taking a\nproduct of these two likelihoods\np(zt|xt) = pg(zt|xt)pc(zt|xt). (7)\nThe parameters \u03ba, \u03b8 (gradient model) and \u03bb (colour model)\nspecify the shape of the gradient and colour likelihood\nfunctions, thus specifying the importance of the respective\nfeatures. The influence of possible correlations between\ncolour and thermal distributions should be investigated more\nthoroughly in future work.\nWhen a person is not detected, a colour model cannot be\nbuilt and only gradient information can be used to update\nthe weight of the particles of a single tracking filter as wit =\npg(zt|xit)\u03c8. However as soon as a person is detected the\ncolour model can be created and the weight update equation\nchanges to:\nwit = pg(zt|xit)pc(zt|xit)\u03c8, i = 1, . . . , N. (8)\nNote that the sequential detector relies only on gradient\ninformation from the thermal image.\nV. OCCLUSION DETECTION WITH ADABOOST\nTo detect occlusions we propose an approach that sorts the\norder of all persons in the image according to pairwise com-\nparisons. The proposed occlusion detector specifies which\none of two overlapping persons is in front. The order of\nthe persons from front-to-back is then determined by a sort\nprocedure requiring MO \u00b7 log(MO) comparisons where MO\nspecifies the number of overlapping persons.\nThere are several features that could indicate the order\nof overlapping persons in the image, from which we have\nchosen a set of three thermal and three colour features.\nThe first feature chosen is the strength (i.e., mean gradient\nvalue) of a tracking filter, since a person for which the\ncorresponding tracker indicates a higher confidence is more\nlikely to be in front. This feature is, however, very noisy and\naffected by many factors such as movement of the camera,\nambient temperature, etc. The top and bottom of the elliptic\nmodel can also indicate the depth of a person since closer\npersons appear taller and closer to the upper and bottom\nborder of the image. However the bottom part can be cut\nwhen persons stand too close to the camera. The top of a\nperson\u2019s head is a more reliable feature, though it is affected\nby the different height of persons. Another set of features\nis the colour similarity of the region corresponding to a\nperson. We have chosen three such regions including the\noverlapping, non-overlapping and whole areas of a person.\nOccluded persons should have lower similarity values.\nWe use the AdaBoost (Adaptive Boosting) classification\nalgorithm [4] for selecting the best combination of features\nto detect occlusions. AdaBoost combines results from so-\ncalled \u201cweak\u201d classifiers ht(x) into one \u201cstrong\u201d classifier\nH(x) = sign(f(x)) as f(x) =\n\u2211T\nt=1 \u03b1tht(x), where T\nis the number of weak classifiers and \u03b1t is an importance\nweight given to each \u201cweak\u201d classifier ht(x) according to\nthe performance during the iterative learning process (see\n[14] for details). During learning focus is put on the training\nexamples which were most difficult to classify (this process\nis called \u201cboosting\u201d). As a result we obtain a final classifier\nthat performs better than any of the weak classifiers alone.\nFollowing [14] we use simple weak classifiers based on a\nsingle-valued feature fj(x)\nhj(x) =\n{\n1 : pjfj(x) < pj\u03b8j\n0 : otherwise, (9)\nwhere \u03b8j is a threshold and pj = {\u22121, 1} is a parity\nindicator determining the direction of the inequality sign.\nDuring the training procedure optimal values of \u03b8j and pj\nare determined by minimising the number of misclassified\ntraining examples.\nIn addition, we use weak classifiers based on a weighted\ncombination of features fj(x) =\n\u2211G\ni=1 \u03b1ifi(x), where \u03b1i\nspecifies the weight for an input feature fi(x) (G = 2 in our\nexperiments). We discretise possible weight values \u03b1i from\nthe range {\u22121, 1} into Nf fractions. As a result we obtain a\nsufficient number of different weak classifiers for selection\nby the boosting algorithm.\nVI. OCCLUSION HANDLING\nThe learned occlusion detector can be used to improve\ntracking performance during occlusion. It is used in two\ndifferent ways: first, to alter the penalising policy between\nthe trackers (as described in Section III), and second, to re-\nidentify occluded persons when they reappear.\nOur interaction model for tracking multiple persons allows\ntracking of people that overlap to a certain degree. This is\nachieved by modifying the interaction factor \u03c1 to prevent\ntarget fetching (i.e., to prevent two filters in close proxim-\nity from collapsing around the same tracked object). The\nproposed pairwise occlusion detector is used to determine\nwhich of the tracking filters is occluded. We consider two\npossible situations: partial occlusion and total occlusion.\nDuring partial occlusion, some part of a person is still\nvisible. However, the gradient along the contour is disturbed,\nwhich can cause a quick disappearance of the tracker. To\navoid this we change the penalty equation to \u03c8 = e(\u2212\u03c1ogij)\nwhere the penalty term \u03c1o < \u03c1. Interaction with other\nfilters (non-overlapping with this pair) remains unchanged.\nWhen the head contour of a person becomes occluded the\ncorresponding tracker is considered to be totally occluded.\nThis means that we can only guess the true position of this\nperson. We assume that the state of the occluded person is\nthe same as the state of the occluding person. No penalty\nis considered for the occluded tracker. We keep particles of\nthe totally occluded tracker for a short time (we use a value\nof 8 frames here) in situations when quick occlusions occur\nand the velocity of particles may allow resolution of this\nocclusion. However after this time has elapsed the particles\nof the tracker are removed and the only information kept\nis the colour model. When a new person is detected this\ninformation is used to match the colour model to all occluded\ntrackers. If the colour model is most similar to the closest\noccluded tracker then the detected person is considered to\nbe an occluded one. Otherwise the person is considered to\nbe a new person. To avoid situations where the occluded\ntracker stays forever behind the occluding one, we also\nspecify a maximum duration of occlusion (in our case 10 s).\nThis minimises errors in the case where an occluded person\ndisappears from the scene in some other way (e.g., through\na door or a corridor behind an occluding person) or in cases\nof missed assignments to newly detected persons.\nVII. EXPERIMENTS\nA. Evaluation\nOur system was tested on the data collected by the robot\nduring several runs. In total we collected 11 tracks using\ncorridor following and 42 tracks with a stationary robot. In\ntotal we obtained 53 different tracks including 12 different\npersons (5607 images containing at least one person and\n6769 images in total). To obtain the ground truth data we\nused a flood-fill segmentation algorithm corrected afterwards\nby hand using the ViPER-GT tool [3]. We considered only\na bounding box around a person. The top and bottom edges\nwere determined from the contours of the head and feet\nwhile the sides were specified by the maximum width of\nthe torso (without arms). The cases when persons appeared\ntoo close (< 3m) to or too far (> 10m) from the robot were\nnot taken into account. The size of the bounding box was\nspecified as 2 \u00b7width and 3.5 \u00b7height of the elliptic contour\ndetection localisation\nrecall NR\nNT\n|AT\u2229AR|\n|AT |\nprecision NR\nNC\n|AT\u2229AR|\n|AC |\naccuracy 2\u00b7NR\nNT+NC\n2\u00b7|AT\u2229AR|\n|AT |+|AC |\nTABLE I\nDETECTION AND LOCALISATION METRICS.\nmodel, an approximation to the proportions of the human\nbody. Bounding boxes from the ground truth data are referred\nto as targets and those from the tracker as candidates.\nWe use two kinds of metrics that indicate the quality of\nthe tracking procedure: detection metrics (counting persons)\nand localisation metrics (area matching). Each type of metric\nis further divided into three statistics: recall, precision and\naccuracy. Recall indicates true positives (\u201chits\u201d), precision\nindicates the level of false alarms, and accuracy is a combina-\ntion of both recall and precision (see Table I). These metrics\nallow thorough testing of the properties and performance of\nthe tracker as in [3] and [10].\nA candidate is considered to be correctly detected if the\noverlap ratio between candidate and target bounding boxes\nis greater than 50%. Detection metrics take into account the\nnumber of correctly detected candidates NR in one frame\nand compare it with the number of targets NT and number\nof all candidates NC . The final result is a weighted average\nof all frames. Localisation metrics express relations between\nareas corresponding to correctly detected candidates AR, all\ncandidates AC and targets AT . The final result is a weighted\naverage of all frames. All of the metrics are normalised to\ngive percentages.\nB. Training of the AdaBoost classifier\nWe extracted the described thermal and colour features\nfrom the collected data. We considered only cases when\ntwo or more people were overlapping. Moreover since the\nbehaviour of the tracker without proper occlusion handling\nis unpredictable after a total occlusion occurs, we took\nonly those examples that preceded the moment of the total\nocclusion. During the occlusions, the colour models of the\nrespective persons were not updated. In this way we obtained\n121 positive and 121 negative examples giving a total of 242\nexamples.\nWe created additional weak classifiers based on weighted\nsums of pairs of features with 20 fractions giving, in the\ncase of all six thermal and colour features used, 1200 new\nweak classifiers. We used 60% of randomly selected input\nexamples as a training set and the remaining part as a test\nset. Each training procedure was repeated 10 times.\nC. Results\nFig. 4 shows the tracking performance using only thermal\ngradient information, with additional colour information, and\nwith both colour information and explicit occlusion han-\ndling. Each experiment was repeated 10 times with different\nrecall precision accuracy\n0\n20\n40\n60\n80\n100\ndetection metrics\nr a\nt e\n \n[ %\n]\nrecall precision accuracy\n0\n20\n40\n60\n80\nlocalisation metrics\nr a\nt e\n \n[ %\n]\ngradient\n+ colour\n+ occ. detector\ngradient\n+ colour\n+ occ. detector\nFig. 4. Detection and localisation metrics for tracking multiple persons\nwithout and with colour information and with occlusion handling procedure.\nFig. 5. Selected thermal images from the sequence showing the output from\nthe tracker before, during and after the occlusion of three simultaneously\ntracked persons. The bounding boxes corresponding to occluded persons are\nmarked by a dotted line.\nrandom variations in the particle filter for each trial using\nN = 1000 particles per filter. The system parameters were\noptimised individually using an area accuracy metric as the\nperformance criterion. Both detection and localisation met-\nrics indicate a significant improvement when using additional\ncolour information (p < 0.01). This leads to more precise\nestimates and decreases the number of cases where the\ntracker loses track of a person. The overall accuracy (84.2%\nin detection and 68.7% in localisation) however is affected\nby low recall values. Adding the occlusion detector gives\nan increase of 6.8% in area recall metrics and 3.1% in area\naccuracy metrics. The output from the tracker can be seen\nin Fig. 5.\nThe strong classifier learned from the combination of\nthermal and colour features was able to predict correctly in\naround 89% of all cases (see Table II). This gives a sig-\nnificant advantage over classification results obtained when\nthermal and colour features were used separately (p < 0.01).\nThermal features provided significantly better results than\ncolour features alone.\nThe most reliable features are the top of a person\u2019s\nhead, colour similarity of the whole region and of the non-\noverlapping area. Weak classifiers based on combinations of\nFeature type Results [%]\nthermal 76.39\u00b1 4.49\ncolour 69.07\u00b1 1.94\nboth 89.38\u00b1 2.48\nTABLE II\nCLASSIFICATION RESULTS FOR DIFFERENT FEATURE TYPES.\nPlatform Model\ngradient colour I colour III\n[ms] [ms] [ms]\nrobot int. image - 5.12 16.09\n0.85 GHz 1000 samples 33.37 50.24 68.79\nmodern PC int. image - 2.09 4.90\n2.00 GHz 1000 samples 13.52 17.66 25.89\nTABLE III\nAVERAGE PROCESSING TIME NEEDED TO CALCULATE 1000 SAMPLES\nUSING DIFFERENT MEASUREMENTS MODELS. LABEL \u201cCOLOUR I\u201d AND\n\u201cCOLOUR III\u201d CORRESPOND TO A COLOUR REPRESENTATION USING THE\nFIRST MOMENT AND THE FIRST THREE MOMENTS RESPECTIVELY.\nthese features had the highest importance. Other features also\ncontributed to the final classifier (e.g., the position of the\nbottom of the elliptic model) even though their individual\nperformance was relatively poor.\nTable III presents the average processing time needed for\ncalculation of 1000 samples when using different colour\nrepresentations. It takes about two times longer to calcu-\nlate one step of the tracking procedure when using all\nthree moments compared to the tracker based on thermal\ninformation only (around 30Hz on a 2.00 GHz processor\nwhen using 1000 samples). A good trade-off between time\nrequirements and performance of the tracker for our set-\nup is a representation using just the first moment of the\ncolour distribution (46% more time compared to the gradient\nbased tracker). The overall performance of the tracker based\non this representation is about 2% lower than the variant\nusing the three colour moments. When tracking multiple\npersons, additional processing time is required for calculation\nof penalty terms for the detector and individual tracking\nfilters. In our case tracking one person required around 8%\nextra time for the detector and in the case of four persons\naround 36% extra time is needed for calculation of penalty\nterms between the trackers.\nVIII. CONCLUSIONS AND FUTURE WORK\nFrom the viewpoint of a typical service robot, it can be\nvery difficult to keep track of which observation corresponds\nto which person, due to the unpredictable appearance and\nsocial behaviour of humans. We believe that the question\nof how to handle occlusions is impossible to answer in\na general way, i.e. independent of a particular application.\nHowever our solution demonstrates that it is plausible to\ndeal with occlusions to some extent and through experiments\nwe showed that this increases the overall performance of\nthe tracker. Such a solution has obvious pitfalls that should\nbe considered in future work such as proper handling of\nmisclassification errors, wrong assignments after occlusions,\nuniformly dressed people, etc. A mobile robot itself could be\nused to check if the occluded person is really behind another\nperson by taking appropriate actions. Recognition of human\nbehaviour could also help to solve this kind of problem.\nREFERENCES\n[1] L. Bre`thes, P. Menezes, F. Lerasle, and J. Hayet, \u201cFace tracking and\nhand gesture recognition for human-robot interaction,\u201d in Proc. IEEE\nICRA, New Orleans, LA, USA, 2004, pp. 1901\u20131906.\n[2] G. Cielniak, \u201cPeople tracking by mobile robots using thermal and\ncolour vision,\u201d Ph.D. dissertation, O\u00a8rebro University, April 2007.\n[3] D. S. Doermann and D. Mihalcik, \u201cTools and techniques for video\nperformance evaluation,\u201d in Proc. ICPR, vol. 4, Barcelona, Spain,\n2000, pp. 4167\u20134170.\n[4] Y. Freund and R. E. Schapire, \u201cA decision-theoretic generalization of\non-line learning and an application to boosting,\u201d in Computational\nLearning Theory: Eurocolt. Springer-Verlag, 1995, pp. 23\u201337.\n[5] N. J. Gordon, D. J. Salmond, and A. F. M. Smith, \u201cNovel approach to\nnonlinear\/non-Gaussian Bayesian state estimation,\u201d Proc. Inst. Elect.\nEng. F, vol. 140, no. 2, pp. 107\u2013113, April 1993.\n[6] M. Isard and A. Blake, \u201cCondensation \u2013 conditional density propa-\ngation for visual tracking,\u201d International Journal of Computer Vision,\nvol. 29, no. 1, pp. 5\u201328, 1998.\n[7] R. Karlsson and F. Gustafsson, \u201cMonte Carlo data association for\nmultiple target tracking,\u201d in In IEEE Target tracking: Algorithms and\napplications, The Netherlands, 2001.\n[8] Z. Khan, T. Balch, and F. Dellaert, \u201cAn MCMC-based particle filter\nfor tracking multiple interacting targets,\u201d in Proc. ECCV, 2004.\n[9] D. Schulz, W. Burgard, D. Fox, and A. B. Cremers, \u201cTracking multiple\nmoving objects with a mobile robot,\u201d in Proc. IEEE CVPR, 2001.\n[10] K. Smith, D. Gatica-Perez, J. M. Odobez, and S. Ba, \u201cEvaluating\nmulti-object tracking,\u201d in Workshop on Empirical Evaluation Methods\nin Computer Vision, San Diego, CA, USA, 2005.\n[11] M. A. Stricker and M. Orengo, \u201cSimilarity of color images,\u201d in Storage\nand Retrieval for Image and Video Databases, 1995, pp. 381\u2013392.\n[12] M. Swain and D. Ballard, \u201cColor indexing,\u201d International Journal of\nComputer Vision, vol. 7, pp. 11\u201332, 1991.\n[13] A. Treptow, G. Cielniak, and T. Duckett, \u201cReal-time people tracking\nfor mobile robots using thermal vision,\u201d Robotics and Autonomous\nSystems, vol. 54, no. 9, pp. 729\u2013739, 2006.\n[14] P. Viola and M. Jones, \u201cRapid object detection using a boosted cascade\nof simple features,\u201d in Proc. IEEE CVPR, 2001.\n[15] T. Wilhelm, H. J. Bo\u00a8hme, and H. M. Gross, \u201cSensor fusion for vision\nand sonar based people tracking on a mobile service robot,\u201d in Int.\nWorkshop on Dynamic Perception, Bohum, Germany, 2002, pp. 315\u2013\n320.\n[16] W. Zajdel, Z. Zivkovic, and B. J. A. Kro\u00a8se, \u201cKeeping track of humans:\nhave i seen this person before?\u201d in Proc. IEEE ICRA, Barcelona, Spain,\n2005.\n"}