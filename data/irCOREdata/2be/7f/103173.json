{"doi":"10.1007\/11551188_32","coreId":"103173","oai":"oai:epubs.surrey.ac.uk:3031","identifiers":["oai:epubs.surrey.ac.uk:3031","10.1007\/11551188_32"],"title":"Configuration of neural networks for the analysis of seasonal time series","authors":["Taskaya-Temizel, T","Casey, MC","Singh, S","Singh, M","Apte, C","Perner, P"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2005-01-01","abstract":null,"downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:epubs.surrey.ac.uk:3031<\/identifier><datestamp>\n      2017-10-31T14:07:35Z<\/datestamp><setSpec>\n      74797065733D636F6E666572656E63655F6974656D<\/setSpec><setSpec>\n      6469766973696F6E733D656E67616E64706879736963616C736369656E636573:436F6D707574696E67<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/epubs.surrey.ac.uk\/3031\/<\/dc:relation><dc:title>\n        Configuration of neural networks for the analysis of seasonal time series<\/dc:title><dc:creator>\n        Taskaya-Temizel, T<\/dc:creator><dc:creator>\n        Casey, MC<\/dc:creator><dc:creator>\n        Singh, S<\/dc:creator><dc:creator>\n        Singh, M<\/dc:creator><dc:creator>\n        Apte, C<\/dc:creator><dc:creator>\n        Perner, P<\/dc:creator><dc:date>\n        2005-01-01<\/dc:date><dc:type>\n        Conference or Workshop Item<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        attached<\/dc:rights><dc:identifier>\n        http:\/\/epubs.surrey.ac.uk\/3031\/2\/2005_taskaya-temizel_casey_configuration_of_neural_networks_for_the_analysis_of_seasonal_time_series.pdf<\/dc:identifier><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/epubs.surrey.ac.uk\/3031\/4\/licence.txt<\/dc:identifier><dc:identifier>\n          Taskaya-Temizel, T, Casey, MC, Singh, S, Singh, M, Apte, C and Perner, P  (2005) Configuration of neural networks for the analysis of seasonal time series  In: 3rd International Conference on Advances in Pattern Recognition, 2005-08-22 - 2005-08-25, Bath, ENGLAND.     <\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1007\/11551188_32<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/epubs.surrey.ac.uk\/3031\/","http:\/\/dx.doi.org\/10.1007\/11551188_32"],"year":2005,"topics":[],"subject":["Conference or Workshop Item","PeerReviewed"],"fullText":"Configuration of Neural Networks for the\nAnalysis of Seasonal Time Series\nT.Taskaya-Temizel and M.C.Casey\nt.taskaya,m.casey@surrey.ac.uk\nDepartment of Computing, School of Electronics and Physical Sciences,\nUniversity of Surrey, Guildford GU2 7XH,UK\nAbstract. Time series often exhibit periodical patterns that can be\nanalysed by conventional statistical techniques. These techniques rely\nupon an appropriate choice of model parameters that are often difficult\nto determine. Whilst neural networks also require an appropriate param-\neter configuration, they offer a way in which non-linear patterns may\nbe modelled. However, evidence from a limited number of experiments\nhas been used to argue that periodical patterns cannot be modelled us-\ning such networks. In this paper, we present a method to overcome the\nperceived limitations of this approach by determining the configuration\nparameters of a time delayed neural network from the seasonal data it\nis being used to model. Our method uses a fast Fourier transform to\ncalculate the number of input tapped delays, with results demonstrating\nimproved performance as compared to that of other linear and hybrid\nseasonal modelling techniques.\n1 Introduction\nIn time series analysis, the recognition of patterns is important to facilitate the\nestimation of future values. This is especially evident for financial time series\nforecasting, where techniques such as technical and regression analyses have\nbeen developed that rely upon the identification of different temporal patterns\n[1]. In particular, regression analysis, whose application area is not only limited\nto financial forecasting [2\u20134], relies on the identification of patterns within the\nseries, such as trend and seasonality. These patterns can be modelled using sta-\ntistical techniques, such as autoregressive (AR) variants, but constructing such\nmodels is often difficult. Whilst the application of neural networks to time series\nanalysis remains controversial [1, 4], they appear to offer improved performance,\nfor example, when used in hybrid models [5]. In hybrid models applied to sea-\nsonal data series, seasonality is first decomposed using techniques such as linear\nfilters [1]. One such method is the application of the autocorrelation function to\ndetermine the input lags used to build a linear AR model, and in particular only\nthe significant lags are selected [3]. However there is evidence to suggest that us-\ning just these selected lags does not give optimal linear models [6]. Furthermore,\nit is unclear whether the lags obtained from the autocorrelation function are\nsufficient for use in a non-linear model, such as a neural network. In this paper\nwe describe a method to configure a neural network to model time series that\nexhibit cyclic behaviour using a more appropriate selection of input lags. When\napplied to a time delayed neural network (TDNN), our method demonstrates\nsimilar performance compared to more complex hybrid modelling techniques.\nIn statistics, periodical variations are treated in two different ways. First, if\nperiodical patterns change stochastically during time, one can apply seasonal\ndifferencing to eliminate seasonality. Second, if behaviour of the periodicity is\ndeterministic, models can be applied by taking into account the type of the vari-\nation, such as whether the changes are in additive or multiplicative form. Linear\nprocesses such as AR models, seasonal AR models and the Holt-Winters method\nare among the few seasonal modelling techniques that have proven successful [1,\n3]. However, to model non-linear patterns, appropriate non-linear techniques are\nrequired, such as neural networks.\nNon-linear neural networks are capable of extracting complex patterns in\ntime series successfully to some degree [2, 7], although identifying whether non-\nlinear models are required remains difficult [1]. A well-known technique to per-\nform temporal processing is to use memory of past input and activation values\nwithin the network to allow it to identify temporal patterns. These TDNN mod-\nels [7, 8] are widely used because of their simplicity, either on their own [9\u201311],\nor in hybrid models such as with an autoregressive integrated moving average\nmodel (ARIMA) [5, 12, 13]. Nevertheless, it has been reported that such hybrids\ndo not necessarily outperform single models [11].\nThe limitations of neural networks are essentially an inability to cope with\nchanges in mean and variance [9], which can be attributed, for example, to\ntrend and exponential seasonality. The mean and variance of a series may be\nstabilised using techniques such as differencing and the Box-Cox transformation\n[14]. Similarly, it has been argued that periodical patterns should be removed\nprior to modelling with a neural network [12, 13, 15, 16]. However, it has been\nshown that certain periodical patterns can be successfully modelled using neural\nnetworks if the network is configured appropriately, and the time series pre-\nprocessed to stabilise the mean [10].\nThe studies so far have focused on model selection tools designed for building\nlinear AR models, which are then used to construct a TDNN for cyclic series\n[9, 12]. For example, Cottrell [9] used Akaike and Bayesian Information Criteria\nto find an optimum TDNN. For the Sunspot data, their results suggested an\narchitecture with four input delays and four hidden neurons. Despite this, the\nSunspot data exhibits an 11 year cycle, and results suggest that a minimum lag\nof 11 is required (for example, [2, 17]). In contrast to Cottrell\u2019s approach, Zhang\nand Qi [12] used the autocorrelation function to configure a TDNN. First, they\nobtained the significant lags within the series based on the an analysis of the\nautocorrelation. Then they employed these lags to construct the input delays\nof TDNN. In this paper we describe a method, which is an extension of that\ndescribed in [10], for selecting the input delays using a fast Fourier transform,\ncomparing our results with Zhang and Qi\u2019s.\n2 Configuring TDNN for Seasonal Time Series\nA TDNN is a variant of the multi-layer perceptron in which the inputs to any\nnode i can consist of the outputs of the earlier nodes generated between the\ncurrent step t and step t \u2212 d, where d \u2208 Z+ and \u2200d < t. Here, the activation\nfunction for node i at time t is:\nyi(t) = f\n\uf8eb\uf8ed M\u2211\nj=1\nT\u2211\nd=1\nwij(t\u2212 d)yj(t\u2212 d)\n\uf8f6\uf8f8 (1)\nwhere yi(t) is the output of node i at time t, wij(t) is the connection weight\nbetween node i and j at time t, T is the number of tapped delays, M is the\nnumber of nodes connected to node i from preceding layer, and f is the activation\nfunction, typically the logistic sigmoid. In this paper, we consider the case when\nwe have tapped delays in the input layer only: an IDNN.\nIn order to set the number of delays in the input layer, we obtain the cycle\ninformation from the training data using a fast Fourier transform, which some-\ntimes gives similar results to that of applying the autocorrelation function, but\nis more convenient to use within a systematic method. In the choice of the cycle\ninformation, we consider the dominant cycles within the data as determined by\nthe outliers in the amplitude response. Then we construct the TDNN with the\nnumber of tapped delays IL equal to each of the extracted dominant cycles,\nchoosing the final configuration based upon the best performing network on the\nvalidation data set. Specifically:\n1. Given a time series {xi}Ki=1, create training {xi}TRi=1, validation {xi}V Li=TR+1\nand test data sets {xi}Ki=V L+1, where i is the time index.\n2. Stabilise the mean of the series by computing the first-order difference.\nx\n\u2032\ni+1 = xi+1 \u2212 xi (2)\nwhere x\n\u2032\nis the stabilised time series.\n3. Estimate the number of input tapped delays IL using the dominant cycle\ninformation in the differenced series:\n(a) Compute the fast Fourier transform of {x\u2032i}V Li=1:\nXi =\nV L\u2211\nj=1\nx\n\u2032\njw\n(j\u22121)(i\u22121)\nV L (3)\nwhere wV L = e(\u22122pii)\/V L is a V Lth root of unity.\n(b) Let R0i = |Xi| be the amplitude response of Xi.\n(c) Discard the periods that are greater than V L\/2.\n(d) Set j = 1.\nLet S be a set and set S = \u000b.\nLet Pi be the set of periods.\nWhile not finished\nCompute the mean \u00b5j\u22121 and standard deviation \u03c3j\u22121 of R\nj\u22121\ni\nExtract the outliers Pi, where R\nj\u22121\ni > \u00b5j\u22121 + 3\u03c3j\u22121\nSet S = S \u222a Pi.\nSet Rji be the amplitude response without the outliers Pi.\nSet j = j + 1.\nIf P = \u000b exit loop.\nEnd while\n(e) Set the number of input tapped delays IL to the closest integer value\n\u00b12 [12] for each period within S. The best number will be selected ex-\nperimentally from these according to the test set performance.\n4. Restrict the number of nodes in the output layer to unity and set the hidden\nlayer size\nH \u2264 (IL+ 1)\n2\n(4)\n5. Normalize the series using the z-score to improve training in the network.\nzi =\nx\n\u2032\ni \u2212 x\u00af\u2032\n\u03c3x\u2032\n(5)\nNote that the outliers P in Step 3d are rounded integer periods.\n3 Experiments and Results\nTo evaluate this method, we chose four industrial production series (from Fed-\neral Reserve Board [18]): consumer goods (starting January 1970), durable goods\n(starting January 1947), fuels (starting January 1954), and total industrial pro-\nduction (starting January 1947)and five U.S. Census Bureau series [19] (starting\nJanuary 1992): retail, hardware, clothing, furniture, and bookstore, all ending in\nDecember 2001 [12]. Each of these monthly data sets exhibit strong seasonalities\nthat are difficult to predict.\nIn order to compare the selected architectures using the method described\nwith the performance of a TDNN in general, we conducted a number of ex-\nperiments with network configurations of 2i : 2j : 1, where 1 \u2264 i \u2264 33 and\n1 \u2264 j \u2264 16, a total of 528 networks. Each network was configured to use a hy-\nperbolic tangent activation function for the hidden layer and a linear function for\nthe output layer. Training was performed using the gradient descent algorithm\nfor a maximum of 20,000 epochs, with initial learning rate parameter \u03bb = 0.1,\nincreased by 1.05% if the training error decreased, otherwise decreased by 0.7%,\nif the training error increased by over 4%. Each configuration was tested with\n30 different random initial conditions to provide an average root mean square\nerror (RMSE). The testing data set was used to determine which was the best\narchitecture once training was complete.\n3.1 Results\nFor each of the selected input delay sizes IL per data set, Table 1 shows the\narchitecture selected by our algorithm and the best performing network within\nthe trials. In five data sets (FR Fuels, FR Total Production, USBC Bookstore,\nUSBC Clothing, USBC Retail, USBC Hardware), we obtained the best per-\nforming architectures among our trials using our method. In the other four data\nsets, the method did not pick up the best performing architecture. However, the\nselected architectures were amongst the top ten within the trials.\nTable 1. The selected method\u2019s performances compared with the performances of the\nbest network configuration obtained from the trials. Bold values show the correctly\nidentified TDNN configurations by the algorithm. Our algorithm finds the best config-\nurations on five out of nine data sets.\nData Sets\nSelected Model Best Model\nConfig RMSE Config RMSE\nFR Consumer Goods 16:02:01 1.25 \u00b1 0.17 24:02:01 1.07 \u00b1 0.89\nFR Durable Goods 14:06:01 3.15 \u00b1 0.51 12:16:01 2.91 \u00b1 0.34\nFR Total Production 42:02:01 0.99 \u00b1 0.05 42:02:01 0.99 \u00b1 0.05\nFR Fuels 30:02:01 1.73 \u00b1 0.12 32:02:01 1.64 \u00b1 0.13\nUSBC Bookstore 12:02:01 91.51 \u00b1 10.40 12:02:01 91.51 \u00b1 10.40\nUSBC Clothing 14:02:01 378.71 \u00b1 68.29 14:02:01 378.71 \u00b1 68.29\nUSBC Furniture 14:02:01 175.02 \u00b1 10.57 48:02:01 161.09 \u00b1 22.63\nUSBC Retail 14:02:01 634.72 \u00b1 30.97 14:02:01 634.72 \u00b1 30.97\nUSBC Hardware 12:04:01 37.97 \u00b1 9.34 12:04:01 37.97 \u00b1 9.34\nIn order to understand whether the method selects near optimum parameters\nfor the TDNN, we compared the results given in Table 1 with those for each of\nthe 528 networks constructed over 30 trials. Figure 1 shows the results for each\nof these networks for the FR total production data set. Part (a) shows the\ndominant cycles determined by the method, part (b) shows the performance of\neach of the 528 networks, with the number of neurons within the hidden layer\non the x-axis, and the number of input tapped delays on the y-axis. The shading\nshows the average RMSE, with the dark areas showing the lowest values. Here\nwe see that the best performing architecture is that with 2 hidden neurons and\napproximately 42 input tapped delays. This corresponds well with our method,\nwhich selects 43 as one of the dominant periods, and with the best performing\narchitecture using 42 input delays, within our bounds of \u00b12, as suggested by\n[12]. Apart from FR durable goods, a TDNN has optimal performance when the\nhidden layer size is set between 2 and 4. We can therefore see that our method\nprovides a way in which the dominant cycle information can be successfully used\nto construct a near-optimum TDNN to model seasonal time series.\nIn Table 2, we compared our best fit results among the selected networks with\nthe best fit of the hybrid ARIMA models constructed by Zhang and Qi [12].\n(a) (b)\n0 10 20 30 40 50 60\n0\n1\n2\n3\n4\n5\n6\nx 104\nPo\nw\ner\nPeriod (Months\/Cycle)\n1\n1.2\n1.4\n1.6\n1.8\n2\n2.2\n2.4\n2 6 10 14 18 22 26 30\n2\n6\n10\n14\n18\n22\n26\n30\n34\n38\n42\n46\n50\n54\n58\n62\n66\nHidden Layer\nIn\npu\nt L\nay\ner\nTotal Production Testing RMSE\nFig. 1. (a) shows the dominant cycles, which are automatically picked up by the algo-\nrithm, (b) shows the error surface of the TDNN on \u2019Total Production\u2019 time series.\nFor the hybrid architecture, our TDNN models outperformed in six data sets\n(FR Durable, FR Total Production, USBC Bookstore, USBC Clothing, USBC\nRetail, USBC Hardware). Zhang and Qi constructed the networks by taking into\nconsideration the correlation structure of the series. Based on the outcome of\ntheir analysis, they considered ten time lags: 1-4, 12-14, 24, 25, and 36, where\n12, 24, and 36 months apart are highly correlated. The number of hidden nodes\nvaried between 2 and 14 with an increment of 2. For example, the best neural\nnetwork configuration for the USBC retail series was 36:12:1, where 36 shows the\nmaximal lagged term. Their findings showed that the input layer should comprise\nat least a maximal lag of 12 (5 input nodes) for all series. More specifically, they\nreported that for the detrended data using polynomial fitting, the maximal lags\nwere identified as 13 for USBC retail, 36 for FR consumer goods, and 14 for\ndurable goods. For the detrended and deseasonalised data, they used a maximal\nlag of 1 for USBC retail, 4 for FR consumer goods and 4 for durable goods.\nThey concluded that they found large discrepancies among the hidden layer\nnodes. However, we found that neural networks with a small number of hidden\nnodes perform significantly better than ones with a large number of hidden nodes\nin our earlier study [11]. We observed that the networks trained with continuous\nlag information outperform networks without.\n4 Conclusion\nWe have described an algorithm that can be used to find the optimum TDNN\nconfiguration for modelling seasonal time series. The method described selects a\nnumber of candidate architectures that include those that are the best perform-\ning for this technique compared to existing results. The results also demonstrate\nthat a TDNN model can produce comparable performance to other hybrid mod-\nels. One advantage with this approach is that the performance of an ARIMA\nTable 2. The best fit selected architectures compared with the performance of TDNN\nand hybrid architecture constructed by Zhang and Qi [12]. Bold values indicate the\nminimum RMSE obtained per data set. Our TDNN architectures outperform in all\ndata sets compared to Zhang and Qi\u2019s TDNN architectures. They also outperform on\nfive out of nine data sets compared to their hybrid architectures.\nData Sets Selected Model RMSE TDNN [13] ARIMA-NN[13]\nFR Consumer Goods 18:04:01 0.91 1.48 0.68\nFR Durable Goods 12:02:01 2.16 5.98 3.63\nFR Total Production 42:02:01 0.77 1.62 0.85\nFR Fuels 30:04:01 1.30 1.83 0.81\nUSBC Bookstore 12:02:01 71.10 170.49 88.74\nUSBC Clothing 14:02:01 277.34 1117.72 315.43\nUSBC Furniture 14:06:01 144.32 226.68 99.45\nUSBC Retail 14:02:01 546.58 1785.77 975.55\nUSBC Hardware 12:02:01 19.76 105.12 49.17\nneural network hybrid is likely to degrade due to overfitting [11]. In this case it\ntherefore appears that using relatively simple models can improve performance.\nOne restriction to our method is that it can only model stationary seasonal\ntime series. For example, our method cannot model series in which the amplitude\nof the cycle increases constantly over time. To be able to model such a series with\nneural networks, first either the series should be stabilised using an appropriate\ntransformation, or a seasonal AR model should be used. Furthermore, the poorer\nperformance of our method on three data sets (FR Consumer Goods, FR Fuels\nand USBC Furniture) requires further investigation to determine whether there\nare any particular characteristics of these that affects our method.\nIn evaluating the method through comparison of different network configu-\nrations, we note that as the number of free parameters increases in the network\n(the number of input delays and hidden neurons), that the model is likely to\noverfit both to the training and validation data sets giving poor generalisation.\nExperimentally this tells us that the input layer size should be set to less than\nten percent of the total data set size in order to achieve improved results, but\nfurther investigation is required to formalise this. Similarly, we also note that\na TDNN generally has optimal performance on the selected data sets when the\nhidden layer size is set between 2 and 4, commensurate with our previous work\n[10, 11].\nAcknowledgements\nWe are grateful to the Fingrid project [RES-149-25-0028], which provided us a\nGrid environment comprising 24 machines to run our simulations.\nReferences\n1. Chatfield, C.: The Analysis of Time Series. Sixth edn. Texts in Statistical Science.\nChapman & Hall, USA (2004)\n2. Rementeria, S., Olabe, X.B.: Predicting sunspots with a self-configuring neural\nsystem. In: Proceedings of the 8th Conference on Information Processing and\nManagement of Uncertainty in Knowledge-Based Systems (IPMU 2000). (2000)\n3. Box, G., Jenkins, G.: Time Series Analysis: forecasting and control. Texts in\nStatistical Science. Holden Day (1970)\n4. Faraway, J., Chatfield, C.: Time series forecasting with neural networks: A com-\nparative study using the airline data. Applied Statistics 47 (1998) 231\u2013250\n5. Zhang, G.P.: Time series forecasting using a hybrid ARIMA and neural network\nmodel. Neurocomputing 50 (2003) 159\u2013175\n6. Rao, T.S., Sabr, M.M.: An introduction to bispectral analysis and bilinear time\nseries models. Lecture Notes in Statistics 24 (1984)\n7. Wan, E.: Time series prediction by using a connectionist network with internal\ndelay lines. In Weigend, A., Gershenfeld, N., eds.: Time Series Prediction: Fore-\ncasting the Future and Understanding the Past. SFI Studies in the Sciences of\nComplexity, Addison Wesley (1994) 1883\u20131888\n8. Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., Lang, K.J.: Phoneme recog-\nnition using time-delay neural networks. IEEE Transactions on Acoustics, Speech\nand Signal Processing 37 (1989) 328\u2013339\n9. Cottrell, M., Girard, B., Girard, Y., Mangeas, M., Muller, C.: Neural modeling for\ntime series:a statistical stepwise method for weight elimination. IEEE Transactions\non Neural Networks 6 (1995) 1355\u20131364\n10. Taskaya-Temizel, T., Casey, M.C., Ahmad, K.: Pre-processing inputs for optimally-\nconfigured time-delay neural networks. IEE Electronic Letters 41 (2005) 198\u2013200\n11. Taskaya-Temizel, T., Ahmad, K.: Are ARIMA neural network hybrids better than\nsingle models? In: Proceedings of International Joint Conference on Neural Net-\nworks (IJCNN 2005), Montre\u00b4al, Canada (2005) to appear\n12. Zhang, G.P., Qi, M.: Neural network forecasting for seasonal and trend time series.\nEuropean Journal of Operational Research 160 (2005) 501\u2013514\n13. Tseng, F.M., Yu, H.C., Tzeng, G.H.: Combining neural network model with sea-\nsonal time series ARIMA model. Technological Forecasting and Social Change 69\n(2002) 71\u201387\n14. Box, G.E.P., Cox, D.R.: An analysis of transformations. JRSS B 26 (1996) 211\u2013246\n15. Virili, F., Freisleben, B.: Nonstationarity and data preprocessing for neural network\npredictions of an economic time series. In: Proceedings of International Joint\nConference on Neural Networks (IJCNN 2000), Como, Italy (2000) 5129\u20135136\n16. Nelson, M., Hill, T., Remus, W., O\u2019Connor, M.: Time series forecasting using\nneural networks: Should the data be deseasonalized first? Journal of Forecasting\n18 (1999) 359\u2013367\n17. Weigend, A., Gershenfeld, N.A., eds.: Time Series Prediction: Forecasting the\nFuture and Understanding the Past. Addison-Wesley, Reading, MA (1993)\n18. Federal Reserve Time Serial Data. http:\/\/www.federalreserve.gov\/releases\/\nG17\/table1 2.htm (2005) Last accessed: Jan.2005.\n19. U.S. Census Bureau Monthly Trade and Food Services Data Sets. http:\/\/www.\ncensus.gov\/mrts\/www\/mrts.html (2005) Last accessed: Jan.2005.\n"}