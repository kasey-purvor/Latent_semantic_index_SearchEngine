{"doi":"10.1088\/1742-5468","coreId":"68773","oai":"oai:eprints.lancs.ac.uk:31229","identifiers":["oai:eprints.lancs.ac.uk:31229","10.1088\/1742-5468"],"title":"Inferential framework for non-stationary dynamics:theory and applications","authors":["Duggento, A.","Luchinsky, D. G.","Smelyanskiy, V. N.","McCkintock, P. V. E."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2009","abstract":"An extended Bayesian inference framework is presented, aiming to infer time-varying parameters in non-stationary nonlinear stochastic dynamical systems. The convergence of the method is discussed. The performance of the technique is studied using, as an example, signal reconstruction for a system of neurons modeled by FitzHugh\u2013Nagumo oscillators: it is applied to reconstruction of the model parameters and elements of the measurement matrix, as well as to inference of the time-varying parameters of the non stationary system. It is shown that the proposed approach is able to reconstruct unmeasured (hidden) variables of the system, to determine the model parameters, to detect stepwise changes of control parameters for each oscillator and to track the continuous evolution of the control parameters in the adiabatic limit","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/68773.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/31229\/1\/2jstatPostPrint.pdf","pdfHashValue":"af7c312c4a5cc28719eddcca2ed5b5608e29a8fd","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:31229<\/identifier><datestamp>\n      2018-01-24T02:55:01Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5143<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Inferential framework for non-stationary dynamics:theory and applications<\/dc:title><dc:creator>\n        Duggento, A.<\/dc:creator><dc:creator>\n        Luchinsky, D. G.<\/dc:creator><dc:creator>\n        Smelyanskiy, V. N.<\/dc:creator><dc:creator>\n        McCkintock, P. V. E.<\/dc:creator><dc:subject>\n        QC Physics<\/dc:subject><dc:description>\n        An extended Bayesian inference framework is presented, aiming to infer time-varying parameters in non-stationary nonlinear stochastic dynamical systems. The convergence of the method is discussed. The performance of the technique is studied using, as an example, signal reconstruction for a system of neurons modeled by FitzHugh\u2013Nagumo oscillators: it is applied to reconstruction of the model parameters and elements of the measurement matrix, as well as to inference of the time-varying parameters of the non stationary system. It is shown that the proposed approach is able to reconstruct unmeasured (hidden) variables of the system, to determine the model parameters, to detect stepwise changes of control parameters for each oscillator and to track the continuous evolution of the control parameters in the adiabatic limit.<\/dc:description><dc:date>\n        2009<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/31229\/1\/2jstatPostPrint.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1088\/1742-5468\/2009\/01\/P01025<\/dc:relation><dc:identifier>\n        Duggento, A. and Luchinsky, D. G. and Smelyanskiy, V. N. and McCkintock, P. V. E. (2009) Inferential framework for non-stationary dynamics:theory and applications. Journal of Statistical Mechanics: Theory and Experiment, 2009 (1).<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/31229\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":null,"relations":["http:\/\/dx.doi.org\/10.1088\/1742-5468\/2009\/01\/P01025","http:\/\/eprints.lancs.ac.uk\/31229\/"],"year":2009,"topics":["QC Physics"],"subject":["Journal Article","PeerReviewed"],"fullText":"Inferential framework for nonstationary dynamics:\ntheory and applications\nAndrea Duggento1, Dmitri G. Luchinsky1,2,\nVadim N. Smelyanskiy2, Peter V. E. McClintock1\n1Department of Physics, Lancaster University, Lancaster LA1 4YB, UK\n2NASA Ames Research Center, Mail Stop 269-2, Moffett Field, CA 94035, USA\nE-mail: a.duggento@lancaster.ac.uk\nAbstract. An extended Bayesian inference framework is presented, aiming to infer\ntime-varying parameters in nonstationary nonlinear stochastic dynamical systems.\nThe convergence of the method is discussed. The performance of the technique is\nstudied using, as an example, signal reconstruction for a system of neurons modeled by\nFitzHugh-Nagumo oscillators: it is applied to reconstruction of the model parameters\nand elements of the measurement matrix, as well as to inference of the time-varying\nparameters of the non-stationary system. It is shown that the proposed approach is\nable to reconstruct unmeasured (hidden) variables of the system, to determine the\nmodel parameters, to detect stepwise changes of control parameters for each oscillator,\nand to track the continuous evolution of the control parameters in the adiabatic limit.\nPublished in J. Stat. Mech. P01025 (2009).\nInferential framework for nonstationary dynamics: theory and applications 2\n1. Introduction\nThe inference of parameters from time-series, for models based on stochastic nonlinear\ndynamical systems, is an open field that is currently attracting much attention on\naccount of its importance and wide applicability. Such models can reproduce a diversity\nof complex phenomena in technology and nature, providing the information needed\nfor diagnosis of faults, prognosis of future conditions, or control, in e.g. reactors [1],\nhelioseismology [2], physiology [3] and neuroscience [4].\nMuch effort has been devoted to the solution of the problem of parameter inference\nunder different conditions [3, 5, 6, 7, 8, 9, 10, 11, 12, 13]. However, time variation of the\ncontrol parameters in non-stationary systems, and how the inference algorithm can be\nadjusted to accommodate them, has not yet been widely explored. It is therefore highly\ndesirable to extend the inferential framework in this direction, hoping to encompass\nalmost-real-time tracking of time-varying parameters.\nThe algorithms developed earlier all have their own particular advantages and\ndisadvantages. The disadvantages include e.g. the requirement of extensive numerical\nsimulation [9, 10, 13], or the need for very large amount of data [5, 7] (cf. econometric\nseries analysis [14]). Their adaptation for parameter-tracking in non-stationary\nstochastic nonlinear systems cannot be done in any obvious way. Moreover, most\ncalculations of flows produce biased estimates due to their lack of a term related to\nthe Jacobian of transformation from stochastic to deterministic variables. This is a\nvery important issue, because this term gives [15] a leading-order contribution to the\ninference results in the presence of strong dynamical noise. In our own earlier work we\nintroduced an analytic solution of the dynamical inference problem [15, 16] based on\nBayesian statistics and a path-integral formulation of stochastic dynamics. As a direct\nresult, fast, unbiased estimation of model parameters became possible. The technique\nalso provides optimal compensation for dynamical noise.\nIn the present paper we extend the Bayesian framework, allowing us to infer\ninformation encoded in the time-varying control parameters of a nonlinear non-\nstationary system, almost in real time. The parameter-tracking algorithm is effectively\nembedded into the learning inferential framework, enabling us to reconstruct both the\nmodel parameters and also the unmeasured (hidden) variables of the system. Such an\ninferential framework can have a wide range of interdisciplinary applications, including\nnanosensors [17], physiology [18], and aerospace [19, 20].\nWe consider an application of the scheme to a model of physiological signalling.\nIn particular, we demonstrate how the technique might be used for the analysis of\nsignals from neuronal systems. Their dynamics have not yet been well understood,\nand the highly nonlinear and non-stationary nature of their behavior makes it difficult\nto apply standard techniques for the reliable inference of control parameters. Internal\nand measurement noises in these systems strongly affect their dynamics, and the time\nvariation of the control parameters is directly related to information coding. We show\nthat our approach is able to decode the time evolution of the control parameters\nInferential framework for nonstationary dynamics: theory and applications 3\nin a system of neurons modeled as a set of FitzHugh-Nagumo (FHN) equations\n[21, 22, 23, 24], including detection of large stepwise changes for either oscillator\nand continuous variation in the adiabatic limit. We illustrate this ability by our\nreconstruction of the system parameters assuming that the original parameters of the\nmodel are unknown, that only one coordinate of each oscillator is available for recording,\nand that the measurements are mixed by a measurement matrix.\nThe paper is organized as follows. Sec. 2 presents the general approach of the\nBayesian inferential framework, and the main idea of the inferential framework for non-\nstationary dynamics. In Sec. 2.2 the theory of Bayesian inference for a system of L\nFHN oscillators is developed, providing the basis for physiological applications; and in\nSec. 2.3 the case of nonstationary dynamics is discussed. Sec. 3 presents simulation\nresults. In Sec. 4 results are summarized and conclusions are drawn.\n2. Bayesian inferential framework for non-stationary dynamics\n2.1. A general approach\nThe fundamental problem in dynamical inference can be defined as follows. An M -\ndimensional time-series of observational data Y = {yn \u2261 y(tn)} (tn = nh) is provided,\nand the time variation of the unknown model parameters and the unknown dynamical\ntrajectory M =\n{\nc,b, D\u02c6, M\u02c6, {xn}\n}\nis to be inferred, under the conditions that the\nunderlying dynamics can be described by a set of L-dimensional (L \u2265 M) stochastic\ndifferential equations in the form\nx\u02d9(t) = f(x(t)|c) +\n\u221a\nD\u02c6\u03be(t), (1)\nand that the observations Y are related to the actual unknown dynamical variables\nX = {xn \u2261 x(tn)} via the following measurement equation\ny(t) = g(x(t)|b) +\n\u221a\nM\u02c6\u03b7(t). (2)\nHere g(x|b) is a measurement function, \u03be(t) and \u03b7(t) are L- and M -dimensional\nGaussian white noises, and D\u02c6 and M\u02c6 are L \u00d7 L and M \u00d7M dimensional dynamical\nand measurement diffusion matrices respectively. It is assumed that the sampling is\ndense enough to use the Euler mid-point discretization; in this case Eqs. (1),(2) can be\ndiscretized in the form\nxn+1 = xn + h f(x\n\u2217\nn|c) +\n\u221a\nh D\u02c6\u03ben,\nyn = g(xn|b) +\n\u221a\nM\u02c6\u03b7n,\n}\n(3)\nwhere x\u2217n = (xn+1 + xn)\/2.\nIn Bayesian statistics a given prior density \u03c1prior(M) that encloses expert knowledge\nof the unknown parameters and the likelihood function \u2113(Y|M), the probability density\nto observe {yn(t)} given choice M of the dynamical model, are both employed to\nInferential framework for nonstationary dynamics: theory and applications 4\ncalculate the so-called posterior density \u03c1post(M|Y) of the unknown parameters M\nconditioned on observations through the use of Bayes\u2019 theorem\n\u03c1post(M|Y) = \u2113(Y|M) \u03c1prior(M)\u222b\n\u2113(Y|M) \u03c1prior(M)dM (4)\nThe construction of the likelihood is of great importance because it contains all the\napproximations of the theory. For independent white Gaussian noise sources, the\nlikelihood is given by a product over n of the probability to observe yn+1 at each time,\n(see [25, 15]) and the minus log-likelihood function S = Sdyn + Smeas = \u2212 ln \u2113(Y|M)\ncan be written as\nS =\nN\n2\nln |D\u02c6|+ h\n2\nN\u22121\u2211\nn=0\n{\n\u2202(f(xn)|c)k\n\u2202xk\n. + [x\u02d9n \u2212 f(x\u2217n|c)]T D\u02c6\u22121 [x\u02d9n \u2212 f(x\u2217n|c)]\n}\n+\nN\n2\nln |M\u02c6|+ 1\n2\nN\u2211\nn=1\n[yn \u2212 g(xn|b)]T M\u02c6\u22121 [yn \u2212 g(yn,xn|b)] + (5)\n+ (L + M)N ln(2\u03c0h),\nwhere x\u02d9n =\nxn+1\u2212xn\nh\nand summation over k is implicit in the term \u2202(f(xn)|c)k\n\u2202xk\n. Here Sdyn\nand Smeas are the dynamical (the first two terms in the first line) and measurement\n(next three terms) parts of the minus log-likelihood function.\nAn explicit calculation with an algorithm capable of minimizing Eq. (5) has been\nprovided in [25]. Basically it consists of an iterative optimization of S in the space of\ndynamical paths {xn} and in the space of parameters\n{\nc,b, D\u02c6, M\u02c6\n}\n.\nLet assume for now that the hidden dynamical variables {xn} are given and let focus\nour attenction only on the minimization of Sdyn. The key point is the parameterization\nof the fields in respect of the parameter vector c:\nf(x|c) = F\u02c6(x) c, (6)\nwhere matrix F\u02c6 takes the form\nF\u02c6 =\n\uf8ee\n\uf8ef\uf8f0\n\uf8eb\n\uf8ec\uf8ed\n\u03c61 . . . 0\n...\n. . .\n...\n0 . . . \u03c61\n\uf8f6\n\uf8f7\uf8f8 . . .\n\uf8eb\n\uf8ec\uf8ed\n\u03c6F . . . 0\n...\n. . .\n...\n0 . . . \u03c6F\n\uf8f6\n\uf8f7\uf8f8\n\uf8f9\n\uf8fa\uf8fb , (7)\nand {\u03c6i} are the F -dimensional sets of arbitrary base functions.\nWith this linear parameterization of f , and given that the log-likelihood quadratic\nin f (see Eq. 5), we obtain a quadratic log-likelihood in respect of the vector parameter\nc. Hence, using a multivariate normal distribution for the prior probability immediately\nleads to a multivariate normal distribution for the posterior. This is highly desirable\nfor two reasons: (i) from one side a Gaussian posterior is computationally extremely\nconvenient because there is the guarantee of a unique maximum, the mean vector\nand covariance matrix completely characterize the distribution and give us the most\nsignificant information; (ii) but most of all the multivariate normal posterior can be\nused again as a prior in presence of a new block of data and knowledge about the\nsystem can easily be updated. This last feature is essential for any real time application\nInferential framework for nonstationary dynamics: theory and applications 5\nbecause it ensures that the complexity of the algorithm does not change with the length\nof the input data-stream.\nWith a multivariate normal distribution as a prior for parameters c, with mean c\u00af,\nand covariances \u039e\u02c6\u22121prior, the stationary point of Sdyn is calculating recursively with the\nfollowing equations (cf. [15]):\n\u3008D\u02c6\u3009 = h\nN\nN\u22121\u2211\nn=0\n[\nx\u02d9n \u2212 F\u02c6n c\u00af\n] [\nx\u02d9n \u2212 F\u02c6n c\u00af\n]T\n, (8)\n\u3008c\u3009 = \u039e\u02c6\u22121X (D\u02c6)wX (D\u02c6), (9)\nwX (D\u02c6) = \u039e\u02c6\n\u22121\npriorc\u00af + h\nN\u22121\u2211\nn=0\n[\nF\u02c6Tn D\n\u22121 x\u02d9n \u2212 v(xn)\n2\n]\n, (10)\n\u039e\u02c6X (D\u02c6) = \u039e\u02c6prior + h\nN\u22121\u2211\nn=0\nF\u02c6Tn D\u02c6\n\u22121 F\u02c6n, (11)\nwhere F\u02c6n \u2261 F\u02c6(xn), and the components of the vector v(x), are\nvm(x) =\nL\u2211\nl=1\n\u2202Flm(x)\n\u2202xl\n, m = 1, . . . , F. (12)\nIn absence of any prior knowledge about the system, a non informative prior can be\nused: the limit of an infinitely large normal distribution is set with the computational\ninitial value of \u039e\u02c6prior = 0 and c\u00afprior = 0.\nAt this point an optimization technique for reconstruction of the driving dynamics of\nX is needed. The problem is far too vast to be reviewed in any detail in the present work.\nSuggestions for reconstruction of the best path in the dynamical space range from the\nMarkov Chain Monte Carlo (MCMC) techniques [26], to the extended Kalman filter [10],\nto the Langevin method of sampling the posterior [27]. To bound our discussion of how\nto tackle the problem in the case of non-stationay dynamics we will not dig into any\noptimization technique in the space of observable variables. Therefore, in the latter, a\nphysiological example will be considered where the observational variable y is noiseless\nand has the same dimension of the dynamics-driving variable.\n2.2. System of FitzHugh-Nagumo oscillators\nTo appreciate how non-stationary Bayesian inference could be applied, we want to\nprovide a physiologically relevant example, and so we consider a typical situation of\nneuron readout. We will simulate neurons firing at the rate of \u223c5-10 s\u22121 and our\nexplicit aim will be to reconstruct the varying parameters with a correlation of \u223c500-\n1000 ms. This means that we need a computational inference delay shorter that 500 ms,\ncorresponding to a few periods of firing of the action potential. To model this spiking\nbehavior we use the FitzHugh-Nagumo system in the form\nv\u02d9j = \u2212vj (vj \u2212 \u03b1j) (vj \u2212 1)\u2212 qj + \u03b7j +\n\u221a\nDji \u03bei,\nq\u02d9j = \u2212\u03b2 qj + \u03b3j vj, (13)\nInferential framework for nonstationary dynamics: theory and applications 6\n-0.5\n 0\n 0.5\n 1\n 1.5\n 2\n0.0 0.2 0.4 0.6 0.8 1.0\nv 1\n(t) \/\n v 2(\nt)\nt (s)\n(a)\n-0.8\n 0\n 0.8\n 1.6\n 2.4\n 3.2\n0.0 0.2 0.4 0.6 0.8 1.0\ny 1(\nt) \/ \ny 2(\nt)\nt (s)\n(b)\nFigure 1. Time-series data generated by the model (13), (14) before and after mixing,\nfor the parameters given in Table 1. Parameters \u03b71 and \u03b72 fluctuate between 0.35 and\n0.45. The (blue) solid lines show v1(t) and y1(t), and the (red) dotted lines show v2(t)\nand y2(t).\n\u3008\u03bej(t) \u03bei(t\u2032)\u3009 = \u03b4i j \u03b4(t\u2212 t\u2032), j = 1 : L.\nThis system (13) represents the simplified dynamics of L non-interacting neurons [22],\nwhere vj models the membrane potentials and qj are slow recovery variables. Parameters\n\u03b7i control the potential threshold for the self-excited dynamics, controlling the firing rate,\nand they will be considered as time-varying parameters. Realistically, the membrane\npotential is difficult to measure from each single neuron, so we will assume that the\nvariables vi remain unobserved, and observable readout consists of mixed variables yi\ndefined as\nyi = Xij vj . (14)\nwhere Xij is an unknown invertible mixing matrix whose coefficients have to be inferred.\nExamples of noisy signals before and after mixing are presented in Fig. 1. Often in\nsuch systems the measurement noise is negligible, and we therefore assume no noise\nin Eq. (14). This assumption will also allow us to avoid global optimization and to\nestimate better the performance of the Bayesian inference itself. Our task is to infer\nthe set of model parameters M = {\u03b7i, \u03b1i, qi(0), \u03b3i, Dij, Xij}, with on-line tracking of the\ntime-varying parameters {\u03b7i} for each neuron, starting from the time series data {yi}.\nFollowing [32], a convenient way to treat this problem is by integration of the slow\nrecovery variable qi of equations in (13),\nqj(t) = \u03b3j\n\u222b t\n0\nd\u03c4e\u2212\u03b2 (t\u2212\u03c4)vj(\u03c4) + e\n\u2212\u03b2 tqj(0). (15)\nand to substitute (15) into the top equation in (13), obtaining\nv\u02d9j = \u2212\u03b1jvj + (1 + \u03b1j)v2j \u2212 v3j + \u03b7j (16)\n\u2212 \u03b3j\n\u222b t\n0\nd\u03c4e\u2212\u03b2 (t\u2212\u03c4)vj(\u03c4)\u2212 e\u2212\u03b2 tqj(0) +\n\u221a\nDij\u03bej.\nInferential framework for nonstationary dynamics: theory and applications 7\nwith qj(0) as a set of initial coordinates for the unobservable variable qj(t), thus\nreducing the reconstruction of unobservable variables qj(t) to the inference of the L\ninitial conditions qj(0).\nThe variables vj(t) are also not observed so, using Eq. (14) and substituting\nv = X\u22121y into (16), we obtain in vector notation an explicit form for the dynamics of\nthe readout variable:\ny\u02d9 = X\u03b1\n(\nX\u22121y\n)\n+ X(1 + \u03b1)\n(\nX\u22121y\n)2\n+ X\n(\nX\u22121 y\n)3\n+ (17)\n+ e\u2212\u03b2tXq0 \u2212\n\u222b t\n0\ne\u03b2(t\u2212\u03c4)X\u03b3\n(\nX\u22121y\n)\nd\u03c4 + X\u03b7 + X\n\u221a\nD\u03be(t),\nwhere q0 = q(t = 0), \u03b1 and \u03b3 are matrices with \u03b1i and \u03b3i on the respective diagonals,\nand\n(\nX\u22121 y\n)n\n=\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ed\n(\u2211L\ni=1 x1iyi\n)n\n. . . 0\n...\n. . .\n...\n0 . . .\n(\u2211L\ni=1 xLiyi\n)n\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f8 .\nHere xij are elements of the inverse matrix X\n\u22121. The dynamics represented in Eq. (17)\ncan be described by a set of base functions for the L dimensional readout y and their\nrespective coefficients. The minimal set of base functions for Eq. (17) is\n\u03c6(x) = {1, y1, ..., yL, y21, y1y2, ..., y1yL, y22,\ny2y3, ..., y2yL, ..., y\n3\n1, y\n2\n1y2, ..., y\n2\n1yL, y\n3\n2, y\n2\n2y1, ..., (18)\ny22yL, ..., y\n2\nLyL\u22121 , y\n3\nL ,\u03a61, ...,\u03a6L , e\n\u2212\u03b2t}\nwhere \u03a6i is defined as\n\u03a6i \u2261\n\u222b t\n0\nyi(\u03c4)e\n\u03b2(\u03c4\u2212t) d\u03c4.\nIntroducing a new notation for the coefficients of the transformed dynamics one can\nwrite Eq. (17) as\ny\u02d9i = \u03b7\u02dci + \u03b1\u02dcijyj + b\u02dcik1k2 yk1yk2 + c\u02dcik1k2 yk1y\n2\nk2\n+ e\u2212\u03b2 tq\u02dci \u2212\n\u222b t\n0\ne\u03b2(t\u2212\u03c4)\u03b3ij yjd\u03c4 +\n\u221a\nD\u02dcij\u03bej(t), (19)\nWe notice that the number of base functions N\u03c6 for the mixed dynamic is much\nlarger than the number of polynomial terms in Eq. (13)\nN\u03c6 = 2 + 2L +\nL(L + 1)\n2\n+ L2 ,\nand that increases as L2 with the number of systems.\nAlso, due to the symmetries of the system that have been introduced with the\nsubstitution of the dynamics, the number of unknown coefficients of the system (17)\nNc is much larger than the number of parameters in the original dynamics and\nNc = N\u03c6 \u00d7 L + L2 + L(L+1)2 .\nInferential framework for nonstationary dynamics: theory and applications 8\nFrom Eq. (19) it is clear that the parameter set\nM\u02dc = {\u03b7\u02dci, \u03b1\u02dcij, b\u02dcijk, c\u02dcijkl, \u03b3\u02dcij, q\u02dci(0), D\u02dcij}\nobtained by inferring the coefficients for the base functions of y differs from the original\nM = {\u03b7i, \u03b1i, bi, ci, \u03b3i, qi(0), Dij, Xij} .\nThe explicit relation between the two set of parameters M\u02dc and M can be worked out\nby comparing Eqs. (16), (17) and (19):\n\u03b7\u02dci = Xij \u03b7j, (20)\n\u03b1\u02dcij = Xim \u03b1m\n(\nX\u22121\n)\nmj\n, (21)\nb\u02dcijl = Xim (1 + \u03b1m)\n(\nX\u22121\n)\nmj\n(\nX\u22121\n)\nml\n, (22)\nc\u02dcjklm = Xji\n(\nX\u22121\n)\nik\n(\nX\u22121\n)\nil\n(\nX\u22121\n)\nim\n, (23)\n\u03b3\u02dci l = Xij \u03b3j\n(\nX\u22121\n)\njl\n, (24)\nq\u02dci = Xij qj, (25)\u221a\nD\u02dcij = Xik\n\u221a\nDkj. (26)\nThe inferential algorithm discussed in previous sections is able to select the optimal\nposterior density in respect of the parameters of the transformed dynamics M\u02dc. There\nis no bijective function from M\u02dc to M because we are going from a space larger in\ndimensions to a smaller one; the selection of the best original parameter cannot be done\nvia algebraic calculations. The problem is a constrained search for the maximum of the\nposterior distribution within the set of original parameters. In practice, the log-posterior\ni.e. a multivariate parabolic function, standard non-linear least-squares technique are\nparticularly efficient [29]. The problem of reconstruction of the original parameters M\ncan be stated in the following terms: we have the explicit transformation rule from\nM to M\u02dc and we have the explicit density probability in respect of M\u02dc (the inferred\nposterior). We have to find the maximum of the posterior subject to the constraint\nrepresented by M. Employing a least-squares algorithm for this task means: (i) to\nchoose an initial evaluation of M0; (ii) to improve the initial condition in the direction\nwhere the probability of the corresponding M\u02dc0 is highest; and (iii) repeat the steps\ntill convergence is achieved. In the simulations that we carried out, the choice of the\ninitial M0 had no influence on the convergence. The initial guess of X is arbitrary set\nas the unitary matrix. The initial values for \u03b1i can safely be chosen at random between\none order of magnitude greater or smaller than the coefficients \u03b1\u02dcij. In an FHN model,\nthe coefficients bi have no degree of freedom, and the relation bi = \u03b1i + 1 must hold,\nsuggesting the natural initial condition for bi. An analogous argument suggests that we\ninitialise ci = \u22121 accordingly to the FHN dynamics. Details of the application of this\ntechnique for the recovery of the M parameter set have been explained in [32]. We note\nthat the redundancy of this approach lets one introduce unknown parameters for the\ncouplings between the FHN systems and the reconstruction of these parameters will bis\nthen a trivial extension.\nInferential framework for nonstationary dynamics: theory and applications 9\n2.3. Non-stationary dynamics: a stepwise approach\nThe main idea of our approach consists of considering the parameters to be stationary\nwithin reasonably short time-segments, infering parameters for this step. For each block\nof data k the posterior probability density ppost(c) can be used as a prior for the next\nblock k + 1. Since our posterior probability is a multivariate Gaussian distribution\ncharacterized by a mean vector cpost and a covariance matrix \u039e\n\u22121\npost, it is easy to modify\nthis last quantity to take into account that some parameters, being non-stationary, could\nhave been drifted when considering the next block.\nThis approximation, where slowly-varying parameters can be assumed constant,\nwill be regarded as an adiabatic approximation. To consider it as a valid starting point,\nsome preliminary considerations are in order.\nThere are multiple timescales involved in the inferential problem and, when dealing\nwith non-stationary dynamics, the two most significant ones are the timescale that\ndefines the evolution of parameters \u03c4param, and the timescale at which we acquire\ninformation through the measurement \u03c4meas. The first one can be regarded as an\napproximate measure of the biggest change of the dynamical parameters\nmin\ni\n[\nci\n(\ndci\ndt\n)\u22121]\n\u2248 \u03c4param\nand it is an intrinsic property of the system. Conversely, the informational timescale is\nlimited by the sampling frequency h and the number Nstep of sampling points needed\nto infer the form of the vector field with the desired precision. Thus\nhNstep \u2248 \u03c4meas.\nIn order to consider parameters to be constant during one step, the timescale condition\n\u03c4param \u226b \u03c4meas\nmust hold. So the question can be posed in this way: how fast is the parameter\nconvergence, and how small can Ninf be for the timescale condition to hold? In the\nfollowing paragraphs we consider the inference of a set of FHN oscillators as a possible\napplication of this inference in short steps. Because the convergence of parameters\nis crucial in relation to the first timescale, it will be kept as the central point of the\ndiscussion.\n3. Simulation results\n3.1. Convergence speed for parameter inference\nWe are mainly interested in the qualitative behavior of convergence of parameters and\nalthough what been presented so far applies to any number of FHN oscillators, we now\nrestrict ourselves to a pair of oscillators. We have analysed the convergence of the model\nInferential framework for nonstationary dynamics: theory and applications 10\nparameters as a function of T = hN , where h is the sampling time-step and N is the\nnumber of points in a block of data. Synthetic data for the 2 FHN oscillators have\nbeen generated using the Heun scheme [31] to integrate the model (13), (14) and the\nparameters as been chosen as in Table 1. Only the time-series data y1(t) and y2(t)\nhave been used as an input for the algorithm. Coefficients of the dynamics in Eq. (19)\nhave been inferred; use of these parameters has been made to reconstruct the original\nparameters for the dynamics and the coefficients of the mixing matrix. Typical results\nof the inference of the parameters and the results of the reconstruction of the mixing\nmatrix and the coefficients for the original dynamics are shown in Table 2. It can be\nseen that convergence of order of 1.5% or lower is achieved within 30000 points, which\ncorrespond to less than 1 s at a sampling rate of 35 kHz. To investigate the qualitative\nbehavior of the information gained as a function of number of available data-points we\nhave carried out the following test: we used a data stream 45000 points long and we\ndivided it in 9 blocks of data with 5000 points in each block. The division of the data-\nstream into several blocks is meant to simulate the learning process of the inferential\nmachine: the posterior information of each block is used as a prior for the following block.\nThe reconstruction of parameters has been done after each block with the information\navailable at that moment. We repeated this procedure for 1000 runs : in each run a\nrandom data sequence is generated from random initial conditions.It is then inferred\nand the means of the parameters distribution are recorded. After each block we have a\nstatistical distribution of inferred parameters coming from a different realization of the\ndynamics. The result of the test is presented in Fig. 2.\nMatrix \u039e\u02c6 in Eq.(11) is the inverse of the covariance matrix for parameters in the\nposterior normal density and gives a measures of how sharply-peaked this distribution is\nabout its mean value. For a qualitative comparison between the parameters\u2019 convergence\nand the evolution of the biggest eigenvalues \u03bbi of \u039e\u02c6\n\u22121 see [30, 32].\nNext, we consider the efficiency of the method under non-stationary conditions.\n3.2. Non-stationary dynamics\nLet us assume that we have to deal with some non-stationary parameters. In particular\nwe will take \u03b7i to be the varying parameters, keeping all the others fixed. To show\ncapability in dealing with this situation we perform three different tests. In the first\ntest the \u03b71 and \u03b72 change at random and in a step-like manner, and no other information\n\u03b11= 0.35 \u03b71= 0.4\n\u03b12= 0.20 \u03b72= 0.3\n\u03b31= 0.0153 \u03b2= 0.0151\n\u03b32= 0.0153\nd11= 0.0002 d12= 0.00007\nd22= 0.0002 d21= 0.00007\nx11= 1.7 x12= 0.8\nx22= 0.2 x21= 0.9\nTable 1. Parameter values of the model (13), (14) used to generate stationary time-\nseries data.\nInferential framework for nonstationary dynamics: theory and applications 11\nparameter real inferred % rel. error\n\u03b7\u02dc1 0.9200 0.924384 0.476522\n\u03b7\u02dc2 0.3500 0.351001 0.28600\nb\u02dc222 1.7550 1.758011 0.171567\nb\u02dc112 -2.1086 -2.114731 0.290762\nX11 1.7 1.686459 0.796526\nX12 0.8 0.794263 0.717092\nX21 0.2 0.196746 1.626811\nX22 0.9 0.898222 0.197610\n\u03b71 0.4 0.406227 1.556788\n\u03b72 0.3 0.302462 0.820660\n\u03b11 -0.35 -0.351992 0.569082\n\u03b12 -0.2 -0.200376 0.188228\nb1 1.35 1.357427 0.550145\nb2 1.2 1.203863 0.321885\nc1 -1.0 -0.999520 0.047957\nc2 -1.0 -0.999114 0.088582\nTable 2. Inferred values of some of the mixed dynamics coefficients (first four lines),\nand values of some of the reconstructed parameters. The inference is based on 30000\npoints. The actual values (second column) are compared with the inferred values (third\ncolumn). Relative errors are given in the last column.\n 0.8\n 0.9\n 1\n 1.1\n 1.2\n 0  0.5  1\n\u03b7~ 1\nt (s)\n(a)\n 0\n 0.5\n 1\n 1.5\n 0  0.5  1\nX 1\n2\nt (s)\n(b)\nFigure 2. Typical example of parameter convergence and reconstruction of original\ncoefficients as a function of signal length. Convergences of parameter \u03b7\u02dc1 (a) and of\nthe reconstruction of an element of the mixing matrix (b) are shown. The first point\ncorresponds to a block of 5000 data points; each successive point after that corresponds\nto an additional 5000 data, as discussed in the text. Vertical bars show the standard\ndeviations of the inferred values, calculated over 1000 realizations.\nis assumed; in the second test we demonstrate that inference of stepwise change of \u03b7i\nare detected much faster if we assume knowledge of the other parameters; and in the\nthird test \u03b71 and \u03b72 are allowed to change continuously and information about the other\nparameters is assumed to be limited. In discussing these examples we do not aim to\ncover all possible cases that might arise. Rather we selected these three tests in the\nInferential framework for nonstationary dynamics: theory and applications 12\nhope of providing an overall view of how to apply the present algorithm and of how it\nbehaves under different circumstances.\nFirst test Parameters \u03b71 and \u03b72 change at random in time in a step-like manner, and\nremain constant between steps. The time interval between steps is approximately 5\nperiods of firing of the action potential. It contains one block of data with 20000 points,\ncorresponding to approximately 0.55 s. Other parameters of the model are fixed but\nassumed unknown (the actual values are given in Table 1). At each step we infer all the\nparameters. Their initial values are assumed to be zero and their initial dispersion to\nbe infinity as already discussed above. Fig. 3 presents the inference results.\nSecond test Parameters \u03b71 and \u03b72 change at random as in the previous test, but now\nthe other parameters of the model are fixed at known values. The time interval between\nsteps is only 1000 points, corresponding to approximately 0.03 s. At each step we infer\nonly parameters \u03b71 and \u03b72. From Fig. 4 it is clear that when other parameters are\nknown, the time required for inferring \u03b7i is smaller by two orders of magnitude than in\nthe case when all parameters have to be inferred.\n 0.2\n 0.4\n 0  1  2  3\n\u03b7 1\nt (sec)\n(a)\n 0\n 1.5\n 3\n 0  1  2  3\nx 1\nt (sec)\n(b)\n 0.2\n 0.4\n 0.6\n 0  1  2  3\nq 1\nt (sec)\n(c)\nFigure 3. Inference of the parameters of two uncoupled FHN systems mixed by the\nmeasurement matrix. It is assumed that \u03b71 and \u03b72 change step-wise while all other\nparameters of the system are fixed and unknown. (a) The inferred values of \u03b71 (dashed\nred lines) are compared with their true values (full blue lines). (b) Measured mixed\nvalues of the coordinate x1(t). (c) Inferred values of the coordinate q1(t) (red dotted\nline) are compared with its true values of (blue solid line). The other parameters are\nfixed at the values given in Table 1. The noise amplitude was\n\u221a\nd1 =\n\u221a\nd2 = .01225.\nThird test In our final test, we infer smoothly varying parameters \u03b71 and \u03b72 with added\nnoise, without knowing any other parameters of the model. The partial information on\nparameters is simulated by inferring parameters from the first block (with 30000 points)\nof stationary dynamics; then for all other blocks of data we use acquired information\nto fix the model parameters constant at their inferred values, and we track in time\nonly variations of the control parameters \u03b7i. Each block of data (except the first one)\ncontains 12000 points and has a time length of t \u2248 0.34 sec. The inferred time evolution\nof the control parameters \u03b7i is compared with its true variation in Fig. 5. It is evident\nfrom the figure that the method allows us to infer the unknown constant parameters\nInferential framework for nonstationary dynamics: theory and applications 13\n 0.2\n 0.4\n 0  0.5  1  1.5\n\u03b7 1\nt (s)\n(a)\n 0\n 1.5\n 3\n 0  0.5  1  1.5\nx 1\nt (s)\n(b)\n 0.2\n 0.4\n 0.6\n 0  0.5  1  1.5\nq 1\nt (s)\n(c)\nFigure 4. Inference of the model parameters of two uncoupled FHN systems mixed\nby the measurement matrix. It is assumed that \u03b71 and \u03b72 change step-wise while all\nother parameters of the system are fixed and known. (a) Inferred values of \u03b71 (short\nelements of red dashed line) are compared with their true values (short elements of full\nblue line) as a function of time. (b) The time-trace of the measured coordinate x1(t).\n(c) The time-trace of the inferred coordinate q\u02dc1(t) (red dotted line) is compared with\nits true value q1(t) (blue solid line). The values of the other parameters are fixed, as\ngiven in Table 1. The noise amplitude was\n\u221a\nd1 =\n\u221a\nd2 = .01225.\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0  1  2\n\u03b7 1\nt (s)\n(a)\n-0.5\n 0.6\n 1.7\n 2.8\n 0  1  2\nx 1\nt (s)\n(b)\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0  1  2\nq 1\nt (s)\n(c)\nFigure 5. Inference of \u03b71 and \u03b72, while smoothly varying in the presence of noise.\nNo prior knowledge of the model parameters is assumed. (a) The inferred values of\n\u03b71 (dashed red lines) are compared with their true values (full blue lines). (b) The\nmeasured time-trace of the mixed coordinate x1(t). (c) The inferred time-trace of the\nmixed coordinate q\u02dc1(t) (dashed red line) is compared with its true value q1(t) (full blue\nline). The values of the other parameters are given in Table 1. The noise amplitude\nwas\n\u221a\nd1 =\n\u221a\nd2 = .01225.\nof the model, and then also to use this information to track in time the non-stationary\ncontrol parameters of the system with a time resolution of the order of 0.3 sec.\n4. Conclusion\nThe inferential framework presented above is an extended version of the that proposed\nin [15] . It is based on Bayesian statistics and a path-integral formulation of stochastic\nnonlinear dynamics, and it allows reconstruction of the parameters of the dynamical and\nmeasurement models from noise-corrupted time series data with subsequent fast tracking\nof time-varying control parameters. The presented algorithm is open to modification:\nad hoc external information can always be added without changing the theoretical\nframework.\nInferential framework for nonstationary dynamics: theory and applications 14\nThe convergence speed of the method is an important issue for fast-tracking of\nthe parameters. We have discussed the convergence of the method in the parameter\nspace which is important issue in this context. We also we provided an overview of\nhow the reconstruction machine might be modified, depending on availability of system\ninformation.\nWe tested the performance of our scheme on simulations of physiological signals,\nmodeling the action potentials of an array of neurons as a set of L noisy FHN\noscillators. The readout was assumed mixed by an unknown measurement matrix. We\nhave established that the method does indeed facilitate on-line tracking of key control\nparameters with a suitable time resolution. The results are achieved by embedding the\nfast on-line tracking of the control parameters within a Bayesian learning framework\nfor the more slowly varying coefficients of the system. To simplify the analysis of\nperformance, we neglected measurement noise, but suitable plug-in techniques exist\nfor dealing with this extra noise source without any alteration of the core method.\nThe use of our inferential framework is very practical and multidisciplinary. With ad\nhoc adjustments on a per-case base, its natural employment is within any field where\ninferential capabilities are needed for a non-linear dynamical system.\nReferences\n[1] H. Konno and K. Hayashi, Annals of Nuclear Energy 23, 35 (1996).\n[2] J. Christensen-Dalsgaard, Rev. Mod. Phys. 74, 1073 (2002).\n[3] V. N. Smelyanskiy, D. G. Luchinsky, A. Stefanovska, and P. V. E. McClintock, Phys. Rev. Lett.\n94, 098101 (2005).\n[4] E. Izhikevich, Dynamical Systems in Neuroscience: The Geometry of Excitability and Bursting.\n(MIT Press, Cambridge, MA, 2006).\n[5] S. Siegert, R. Friedrich, and J. Peinke, Physics Letters A 243, 275 (1998).\n[6] P. E. McSharry and L. A. Smith, Phys. Rev. Lett. 83, 4285 (1999).\n[7] R. Friedrich et al., Phys. Lett. A 271, 217 (2000).\n[8] J. P. M. Heald and J. Stark, Phys. Rev. Lett. 84, 2366 (2000).\n[9] R. Meyer and N. Christensen, Phys. Rev. E 62, 3535 (2000).\n[10] R. Meyer and N. Christensen, Phys. Rev. E 65, 016206 (2001).\n[11] J.-M. Fullana and M. Rossi, Phys. Rev. E 65, 031107 (2002).\n[12] M. Siefert, A. Kittel, R. Friedrich, and J. Peinke, Europhys. Lett. 61, 466 (2003).\n[13] F. Watanabe, H. Konno, and S. Kanemoto, Annals of Nuclear Energy 31, 375 (2004).\n[14] W. Enders, Applied Econometric Time Series, 2nd Edition (Wiley, Hoboken, NJ, 2004).\n[15] V. N. Smelyanskiy, D. G. Luchinsky, D. A. Timucin, and A. Bandrivskyy, Phys. Rev. E 72, 026202\n(2005).\n[16] V. N. Smelyanskiy, D. G. Luchinsky, A. Stefanovska, and P. V. E. McClintock, Phys. Rev. Lett.\n94, 098101 (2005).\n[17] R. W. Z. Turner, K.L.; Baskaran, Decision and Control, 2003. Proceedings. 42nd IEEE Conference\non 3, 2650 (9-12 Dec. 2003).\n[18] A. B. Schwartz, X. T. Cui, D. J. Weber, and D. W. Moran, Neuron 52, 205 (2006).\n[19] V. V. Osipov, D. G. Luchinsky, V. N. Smelyanskiy, and D. A. Timucin, Proc.\nAIAA\/ASME\/SAE\/ASEE Joint Propulsion Conf. and Exhibit, AIAA Conference Proceedings\n(AIAA, Cincinnati, OH, 2007), AIAA 2007-5823 (2007).\nInferential framework for nonstationary dynamics: theory and applications 15\n[20] D. G. Luchinsky et al, Proc. AIAA Infotech@Aerospace 2007 Conf. and Exhibit, AIAA\nConference Proceedings (AIAA, Robnert Park, CA, 2007), AIAA 2007-2829 (2007).\n[21] R. FitzHugh, Biophys. J. 1, 445 (1961).\n[22] J. Nagumo, S. Animoto, and S. Yoshizawa, Proc. Inst. Radio Engineers 50, 2061 (1962).\n[23] A. T. Winfree, The Geometry of Biological Time (Springer-Verlag, New York, 1980).\n[24] E. V. Pankratova, A. V. Polovinkin, and B. Spagnolo, Phys. Lett. A 344, 45\u201350 (2005).\n[25] D. G. Luchinsky, V. N. Smelyanskiy, and J. Smith, in Unsolved Problems of Noise and Fluctuations,\nVol. 800 of AIP Conference Proceedings, ed. by L. Reggiani et al. (AIP, Melville, NY, 2005), pp.\n539\u2013545.\n[26] C. Calder, M. Lavine, P. Mu\u00a8ller, and J. S. Clark, Ecology 84, 1395 (2003).\n[27] A. Apte, M. Hairer, A. M. Stuart, and J. Voss, Physica (Utrecht) D230, 50 (2007).\n[28] D. G. Luchinsky, V. N. Smelyanskiy, M. Millonas, and P. V. E. McClintock, in Noise in Complex\nSystems and Stochastic Dynamics III, Vol. 5845 of (SPIE), ed. L. B. Kish et al (SPIE,\nBellingham, WA, 2005), pp. 173\u2013181.\n[29] W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling, Numerical Recipes in\nFORTRAN: The Art of Scientific Computing, 2 edn. (Cambridge University Press, 1992).\n[30] D. G. Luchinsky et al, Phys. Rev. E 77, 061105 (2008).\n[31] R. Mannella, Intern. J. Mod. Phys. C 13, 1177 (2002).\n[32] A. Duggento et al, Phys. Rev. E 77, 061106 (2008).\n"}