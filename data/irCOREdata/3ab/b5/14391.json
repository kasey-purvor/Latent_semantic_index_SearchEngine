{"doi":"10.1080\/09687760500479977","coreId":"14391","oai":"oai:generic.eprints.org:108\/core5","identifiers":["oai:generic.eprints.org:108\/core5","10.1080\/09687760500479977"],"title":"An exploration of the potential of Automatic Speech Recognition to assist and enable receptive communication in higher education","authors":["Wald, Mike"],"enrichments":{"references":[{"id":201886,"title":"Accessibility, transcription, and access everywhere,","authors":[],"date":"2005","doi":"10.1147\/sj.443.0589","raw":"Bain, K., Basson, S., Faisman, A. & Kanevsky, D. (2005) Accessibility, transcription, and access everywhere,  IBM Systems Journal,  44(3), 589\u2013603. Available online at: http:\/\/ www.research.ibm.com\/journal\/sj\/443\/bain.pdf (accessed 1 November 2005).","cites":null},{"id":201895,"title":"Automatic processing of spoken and written lecture material, paper presented at","authors":[],"date":"2005","doi":null,"raw":"Hazen, T. J. & Barzilay, R. (2005) Automatic processing of spoken and written lecture material, paper presented at Proceedings of Speech Technologies: Captioning, Transcription and Beyond, IBM T.J. Watson Research Center, New York. Available online at: http:\/\/www.nynj.avios.org\/ Proceedings.htm (accessed 1 November 2005).","cites":null},{"id":201888,"title":"Automatic Speech Recognition and receptive communication 19","authors":[],"date":"2002","doi":null,"raw":null,"cites":null},{"id":1043550,"title":"Computer speech recognition as an assistive device for deaf and hard of hearing people, paper presented at \u2018Challenge of Change: Beyond the Horizon\u2019","authors":[],"date":"1996","doi":null,"raw":"Robison, J. & Jensema, C. (1996) Computer speech recognition as an assistive device for deaf and hard of hearing people, paper presented at \u2018Challenge of Change: Beyond the Horizon\u2019 Proceedings from Seventh Biennial Conference on Postsecondary Education for Persons who are Deaf or Hard of Hearing, April. Available online at: http:\/\/sunsite.utk.edu\/cod\/pec\/products\/1996\/robison.pdf (accessed 11 November 2005).","cites":null},{"id":456940,"title":"Developments in technology to increase access to education for deaf and hard of hearing students, paper presented at","authors":[],"date":"2000","doi":null,"raw":"Wald, M. (2000) Developments in technology to increase access to education for deaf and hard of hearing students, paper presented at Proceedings of CSUN Conference Technology and Persons20 M. Wald with Disabilities, California State University Northridge. Available online at: http:\/\/ www.csun.edu\/cod\/conf\/2000\/proceedings\/0218Wald.htm (accessed 11 November).","cites":null},{"id":201892,"title":"Dyslexia and technology, in:","authors":[],"date":"2002","doi":"10.1080\/17483100601178492","raw":"Draffan, E. (2002) Dyslexia and technology, in: L. Phipps, A. Sutherland & J. Seale (Eds) Access all areas: disability, technology and learning (York, JISC TechDis and Oxford, ALT), 24\u201328.","cites":null},{"id":201891,"title":"Evaluating speech-to-text communication access providers: a quality assurance issue, paper presented at PEPNet 2002: Diverse Voices, One Goal","authors":[],"date":"2002","doi":null,"raw":"Downs, S., Davis, C., Thomas, C. & Colwell, J. (2002) Evaluating speech-to-text communication access providers: a quality assurance issue, paper presented at PEPNet 2002: Diverse Voices, One Goal Proceedings from Biennial Conference on Postsecondary Education for Persons who are Deaf or Hard of Hearing, 10\u201313 April. Available online at: http:\/\/sunsite.utk.edu\/cod\/pec\/products\/2002\/downs.pdf (accessed 11 November 2005).","cites":null},{"id":456941,"title":"Hearing disability and technology, in:","authors":[],"date":"2002","doi":null,"raw":"Wald, M. (2002) Hearing disability and technology, in: L. Phipps, A. Sutherland & J. Seale (Eds) Access all areas: disability, technology and learning (York, JISC TechDis and Oxford, ALT), 19\u201323.","cites":null},{"id":1043547,"title":"Issues paper: teaching and learning infrastructure in higher education: report to the HEFCE. Available online at: http:\/\/www.hefce.ac.uk\/Pubs\/hefce\/2002\/02_31.htm (accessed 26","authors":[],"date":"2002","doi":null,"raw":"JM Consulting (2002) Issues paper: teaching and learning infrastructure in higher education: report to the HEFCE.  Available online at: http:\/\/www.hefce.ac.uk\/Pubs\/hefce\/2002\/02_31.htm (accessed 26 May 2005).","cites":null},{"id":201894,"title":"JISC technology and standards watch: improvements in hands-free access to computers. Available online at: http:\/\/www.jisc.ac.uk\/uploaded_documents\/tsw_02-06.doc (accessed 20","authors":[],"date":"2002","doi":null,"raw":"Hargrave-Wright, J. (2002) JISC technology and standards watch: improvements in hands-free access to computers.  Available online at: http:\/\/www.jisc.ac.uk\/uploaded_documents\/tsw_02-06.doc (accessed 20 July 2005).","cites":null},{"id":1043549,"title":"Liberated learning initiative innovative technology and inclusion: current issues and future directions for liberated learning research.","authors":[],"date":"2003","doi":null,"raw":"Leitch, D. & MacMillan, T. (2003) Liberated learning initiative innovative technology and inclusion: current issues and future directions for liberated learning research. Year IV report, Saint Mary\u2019s University, Nova Scotia. Available online at: http:\/\/www.liberatedlearning.com\/research\/ Year%20IV%20research%20report%202003.pdf (accessed 11 November 2005).","cites":null},{"id":201889,"title":"Making chalk and talk accessible,","authors":[],"date":"2002","doi":"10.1145\/960201.957227","raw":"Bennett, S., Hewitt, J., Kraithman, D. & Britton, C. (2002) Making chalk and talk accessible, ACM SIGCAPH Computers and the Physically Handicapped, 73\u201374, 119\u2013125.","cites":null},{"id":456938,"title":"Perceptions of hearingimpaired college students towards real-time speech to print: real time graphic display and other educational support services, The Volta Review,","authors":[],"date":"1988","doi":null,"raw":"Stinson, M., Ross Stuckless, E., Henderson, J. & Miller, L. (1988) Perceptions of hearingimpaired college students towards real-time speech to print: real time graphic display and other educational support services, The Volta Review, 90, 336\u2013348.","cites":null},{"id":201890,"title":"Phonetic searching applied to on-line distance learning modules. Available online at: http:\/\/www.imtc.gatech.edu\/news\/multimedia\/ spe2002_paper.pdf (accessed 1","authors":[],"date":"2002","doi":null,"raw":"Clements, M., Robertson, S. & Miller, M. S. (2002) Phonetic searching applied to on-line distance learning modules.  Available online at: http:\/\/www.imtc.gatech.edu\/news\/multimedia\/ spe2002_paper.pdf (accessed 1 November 2005).","cites":null},{"id":201887,"title":"Speech recognition in university classrooms,","authors":[],"date":"2002","doi":"10.1145\/638249.638284","raw":"Bain, K., Basson, S. & Wald, M. (2002) Speech recognition in university classrooms, Proceedings of the Fifth International ACM SIGCAPH Conference on Assistive Technologies (New York, NY, ACM Press), 192\u2013196.Automatic Speech Recognition and receptive communication 19 Banes, D. & Seale, J. (2002) Accessibility and inclusivity in further and higher education: an overview, in: L. Phipps, A. Sutherland & J. Seale (Eds) Access all areas: disability, technology and learning (York, JISC TechDis and Oxford, ALT), 1\u20135.","cites":null},{"id":1043548,"title":"Speech-based real-time subtitling services,","authors":[],"date":"2004","doi":"10.1023\/b:ijst.0000037071.39044.cc","raw":"Lambourne, A., Hewitt, J., Lyon, C. & Warren, S. (2004) Speech-based real-time subtitling services, International Journal of Speech Technology, 7, 269\u2013279.","cites":null},{"id":201893,"title":"The C-Print speech-to-text system for communication access and learning, paper presented at","authors":[],"date":"2003","doi":null,"raw":"Francis, P. M. & Stinson, M. (2003) The C-Print speech-to-text system for communication access and learning, paper presented at Proceedings of CSUN Conference Technology and Persons with Disabilities, California State University Northridge. Available online at http:\/\/www.csun.edu\/ cod\/conf\/2003\/proceedings\/157.htm (accessed 11 November 2005).","cites":null},{"id":1043546,"title":"The Special Educational Needs and Disability Act. Available online at: http:\/\/ www.legislation.hmso.gov.uk\/acts\/acts2001\/20010010.htm (accessed 5","authors":[],"date":"2001","doi":null,"raw":"HMSO (2001)  The Special Educational Needs and Disability Act.  Available online at: http:\/\/ www.legislation.hmso.gov.uk\/acts\/acts2001\/20010010.htm (accessed 5 October 2005).","cites":null},{"id":456937,"title":"Virtual signing. Available online at: http:\/\/www.rnid.org.uk\/howwehelp\/","authors":[],"date":"2005","doi":null,"raw":"RNID (2005)  Virtual signing.  Available online at: http:\/\/www.rnid.org.uk\/howwehelp\/ research_and_technology\/communication_and_broadcasting\/virtual_signing\/ (accessed 19 July 2005).","cites":null},{"id":456939,"title":"Web content accessibility guidelines version 1.0. Available online at: http:\/\/www.w3.org\/ TR\/WCAG10\/ (accessed 26","authors":[],"date":"1999","doi":null,"raw":"WAI (1999) Web content accessibility guidelines version 1.0. Available online at: http:\/\/www.w3.org\/ TR\/WCAG10\/ (accessed 26 May 2005).","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2006","abstract":"The potential use of Automatic Speech Recognition to assist receptive communication is explored. The opportunities and challenges that this technology presents students and staff to provide captioning of speech online or in classrooms for deaf or hard of hearing students and assist blind, visually impaired or dyslexic learners to read and search learning material more readily by augmenting synthetic speech with natural recorded real speech is also discussed and evaluated. The automatic provision of online lecture notes, synchronised with speech, enables staff and students to focus on learning and teaching issues, while also benefiting learners unable to attend the lecture or who find it difficult or impossible to take notes at the same time as listening, watching and thinking","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/14391.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/108\/1\/ALT_J_Vol14_No1_2006_An%20exploration%20of%20the%20potentia.pdf","pdfHashValue":"1592b005a061941599c7aac1e366717f44fd8bef","publisher":"Routledge","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:108<\/identifier><datestamp>\n      2011-04-04T09:28:44Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/108\/<\/dc:relation><dc:title>\n        An exploration of the potential of Automatic Speech Recognition to assist and enable receptive communication in higher education<\/dc:title><dc:creator>\n        Wald, Mike<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        The potential use of Automatic Speech Recognition to assist receptive communication is explored. The opportunities and challenges that this technology presents students and staff to provide captioning of speech online or in classrooms for deaf or hard of hearing students and assist blind, visually impaired or dyslexic learners to read and search learning material more readily by augmenting synthetic speech with natural recorded real speech is also discussed and evaluated. The automatic provision of online lecture notes, synchronised with speech, enables staff and students to focus on learning and teaching issues, while also benefiting learners unable to attend the lecture or who find it difficult or impossible to take notes at the same time as listening, watching and thinking.<\/dc:description><dc:publisher>\n        Routledge<\/dc:publisher><dc:date>\n        2006<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/108\/1\/ALT_J_Vol14_No1_2006_An%20exploration%20of%20the%20potentia.pdf<\/dc:identifier><dc:identifier>\n          Wald, Mike  (2006) An exploration of the potential of Automatic Speech Recognition to assist and enable receptive communication in higher education.  Association for Learning Technology Journal, 14 (1).  pp. 9-20.  ISSN 0968-7769     <\/dc:identifier><dc:relation>\n        10.1080\/09687760500479977<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/108\/","10.1080\/09687760500479977"],"year":2006,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"ALT-J, Research in Learning Technology\nVol. 14, No. 1, March 2006, pp. 9\u201320\nISSN 0968-7769 (print)\/ISSN 1741-1629 (online)\/06\/010009\u201312\n\u00a9 2006 Association for Learning Technology\nDOI: 10.1080\/09687760500479977\nAn exploration of the potential of \nAutomatic Speech Recognition to assist \nand enable receptive communication in \nhigher education\nMike Wald*\nUniversity of Southampton, UK\nTaylor and Francis LtdCALT_A_147980.sgm10.1080\/09687760500479977ALT-J, Research in Learning Technology0968 7769 (pri t)\/1741-1629 (onli e)Original Article2 06 & Fran is4 000Ma ch 20 6MikeW l.Wald@ oto .ac.uk\nThe potential use of Automatic Speech Recognition to assist receptive communication is explored.\nThe opportunities and challenges that this technology presents students and staff to provide\ncaptioning of speech online or in classrooms for deaf or hard of hearing students and assist blind,\nvisually impaired or dyslexic learners to read and search learning material more readily by augment-\ning synthetic speech with natural recorded real speech is also discussed and evaluated. The auto-\nmatic provision of online lecture notes, synchronised with speech, enables staff and students to\nfocus on learning and teaching issues, while also benefiting learners unable to attend the lecture or\nwho find it difficult or impossible to take notes at the same time as listening, watching and thinking.\nIntroduction\nStudents in the United Kingdom who find it difficult or impossible to write using a\nkeyboard may use Automatic Speech Recognition (ASR) to assist or enable their\nwritten expressive communication (Banes & Seale, 2002; Draffan, 2002; Hargrave-\nWright, 2002). In a report to the English Higher Education Funding Council it was\nnoted that one of the \u2018key issues for teaching\u2019 with regard to information and commu-\nnications technology was the opportunities for such technologies as speech recogni-\ntion software: \nThe importance of this development is that it will change the nature of interaction with\ncomputers. Word commands will make it easier to operate a computer, particularly for\npeople with relatively low literacy skills. This, in turn, will have major implications for the\ndesign of learning materials. (JM Consulting, 2002)\n*School of Electronics and Computer Science, University of Southampton, Highfield, Southampton\nSO17 1BJ, UK. Email: M.Wald@soton.ac.uk\n10 M. Wald\nNo mention was made, however, of the use of ASR to assist students who find it diffi-\ncult or impossible to understand speech, with their receptive communication of\nspeech in class or online.\nUK Disability Discrimination Legislation states that reasonable adjustments\nshould be made to ensure that disabled students are not disadvantaged (HMSO,\n2001), and so it would appear reasonable to expect that adjustments should be made\nto ensure that multimedia materials including speech are accessible if a simple and\ninexpensive method to achieve this was available. Since providing a text transcript of\na video does not necessarily provide equivalent information for a disabled learner, the\ngovernment-funded Skills for Access website,1 which describes itself as \u2018the compre-\nhensive guide to creating accessible multimedia for e-learning\u2019, currently advises that\nthe most desirable accessibility solution is to: \n[\u2026] provide the video with text captions for all spoken and other audio content [\u2026]There\nis no \u2018reasonable\u2019 reason for not captioning video clips from a \u2018widening access\u2019 point of view.\nand that if resource limitations prohibit providing a reasonable alternative experience\nfor those who cannot hear the video, the \u2018reasonable adjustment\u2019 argument: \n[\u2026] begs the question: should you be using video clips at all?\nThe Skills for Access website reports that it took a number of skilled workers many\ntens of hours to caption the video clips used on its site. As video and speech become\nmore common components of online learning materials, the need for captioned\nmultimedia with synchronised speech and text, as recommended by the Web Content\nAccessibility Guidelines (WAI, 1999), can be expected to increase, and so finding an\naffordable method of captioning will become more important to help support a\n\u2018reasonable adjustment\u2019.\nThis paper explores how using ASR can help provide a cost-effective way to assist\nand enable receptive communication, help ensure e-learning is accessible and\nenhance the quality of learning and teaching.\nUse of captions and transcription in education\nDeaf and hard of hearing people can find it difficult to follow speech through hearing\nalone, or to take notes while they are lip-reading or watching a sign-language inter-\npreter. Although summarised notetaking and sign language interpreting is currently\navailable, notetakers can only record a small fraction of what is being said while\nqualified sign language interpreters with a good understanding of the relevant higher\neducation subject content are in very scarce supply (RNID, 2005): \nThere will never be enough sign language interpreters to meet the needs of deaf and hard\nof hearing people, and those who work with them.\nSome deaf and hard of hearing students may also not have the necessary higher\neducation subject-specific sign language skills. Students may consequently find it\ndifficult to study in a higher education environment or to obtain the qualifications\nrequired to enter higher education.\nAutomatic Speech Recognition and receptive communication 11\nStinson et al. (1988) reported that deaf or hard of hearing students at Rochester\nInstitute of Technology who had good reading and writing proficiency preferred real-\ntime verbatim text displays (i.e. similar to television subtitles\/captions) to interpreting\nand\/or notetaking. They have therefore developed the use of ASR re-voicing for the\nC-Print system in classrooms (Francis & Stinson, 2003), where the \u2018notetaker\u2019\nrepeats what the lecturer is saying into a special microphone \u2018mask\u2019 that reduces the\nsound heard by others: \nAn extensive program of research has provided evidence that the C-Print system works\neffectively in public school and postsecondary educational settings.\nRobison and Jensema (1996) identified the value of speech recognition to over-\ncome the difficulties that sign language interpreting had with foreign languages and\nspecialist subject vocabulary, for which there are no signs: \nFingerspelling words such as these slows down the interpreting process while potentially\ncreating confusion if the interpreter or student is not familiar with the correct spelling.\nDowns et al. (2002) identifies the potential of speech recognition in comparison to\nsummary transcription services and students reporting programmes unable to keep\nup with the information flow in the classroom: \nThe deaf or hard of hearing consumer is not aware, necessarily, whether or not s\/he is\ngetting the entirety of the message.\nAlthough UK government funding is available to deaf and hard of hearing students\nin higher education for interpreting or notetaking services, real-time captioning has\nnot yet been used because of the shortage of trained stenographers wishing to work\nin universities. Since universities in the United Kingdom do not have direct respon-\nsibility for funding or providing interpreting or notetaking services, there would\nappear to be less incentive for them to investigate the use of ASR in classrooms as\ncompared with universities in Canada, Australia and the United States.\nASR offers the potential to provide automatic real-time verbatim captioning for\ndeaf and hard of hearing students or for any user of systems when speech is not\navailable, suitable or audible. Students, especially those whose first language is not\nEnglish, may also find it easier to follow the captions and transcript than to follow the\nspeech of the lecturer who may have a dialect, accent or not have English as their first\nlanguage.\nIn lectures\/classes students can spend much of their time and mental effort trying\nto take notes. This is a very difficult skill to master for any student, especially if the\nmaterial is new and they are unsure of the key points, as it is difficult to simulta-\nneously listen to what the lecturer is saying, read what is on the screen, think care-\nfully about it and write concise and useful notes. The automatic provision of a live\nverbatim displayed transcript of what the teacher is saying, archived as accessible\nlecture notes, would therefore enable staff and students to concentrate on learning\nand teaching issues (e.g. students could be asked searching questions in the\nknowledge that they had the time to think) as well as benefiting students who find\nit difficult or impossible to take notes at the same time as listening, watching and\n12 M. Wald\nthinking or those who are unable to attend the lecture (e.g. for mental or physical\nhealth reasons). Lecturers would also have the flexibility to stray from a pre-prepared\n\u2018script\u2019, safe in the knowledge that their spontaneous communications will be\n\u2018captured\u2019 permanently.\nEnhancing teaching and learning through reflection\nPoor oral presentation skills of teachers can affect all students, but is particularly an\nadded disadvantage for disabled students and students whose first language is not\nEnglish. Using ASR to capture all presentations in synchronised and transcribed form\nallows teachers to monitor and review what they have said and reflect on it to improve\ntheir teaching and the quality of their spoken communication.\nAccess to preferred modality of communication\nTeachers may have preferred teaching styles involving the spoken or written word\nthat may differ from learners\u2019 preferred learning styles (e.g. teacher prefers spoken\ncommunication, while student prefers reading). Speech, text and images have\ncommunication qualities and strengths that may be appropriate for different content,\ntasks, learning styles and preferences. Speech can express feelings that are difficult to\nconvey through text (e.g. presence, attitudes, interest, emotion and tone) and that\ncannot be reproduced through speech synthesis. Images can communicate informa-\ntion permanently and holistically. and simplify complex information and portray\nmoods and relationships. Students can usually read much faster than a teacher speaks\nand so find it possible to switch between listening and reading. When a student\nbecomes distracted or loses focus it is easy to miss or forget what has been said,\nwhereas text reduces the memory demands of spoken language by providing a lasting\nwritten record that can be reread.\nBenefits of synchronised multimedia for learning and teaching\nSynchronising multimedia means that text, speech and images can be linked together\nby the stored timing of information, and this enables all the communication qualities\nand strengths of speech, text and images to be available as appropriate for different\ncontent, tasks, learning styles and preferences. Some students, for example, may find\nthe more colloquial style of verbatim-transcribed text from spontaneous speech,\neasier to follow than an academic written style.\nCreating synchronised multimedia\nSynchronised multimedia learning and teaching materials can offer many benefits\nfor students but can be difficult to create access, manage and exploit. Tools that\nsynchronise pre-prepared text and corresponding audio files, either for the produc-\ntion of electronic books (e.g. Dolphin2) based on the DAISY3 specifications or for\nAutomatic Speech Recognition and receptive communication 13\nthe captioning of multimedia (e.g. MAGpie4) using, for example, the Synchronized\nMultimedia Integration Language,5 are not normally suitable or cost-effective for\nuse by teachers for the \u2018everyday\u2019 production of learning materials with accessible\nspeech captions or transcriptions. This is because they depend on either a teacher\nreading a prepared script aloud, which can make a presentation less natural sound-\ning and therefore less effective, or on obtaining a written transcript of the lecture,\nwhich is expensive and time consuming to produce. ASR can improve the usability\nand accessibility of e-learning through the cost-effective production of synchronised\nand captioned multimedia.\nAdvantages of recorded speech compared with synthetic speech\nSynchronised speech and text can assist blind, visually impaired or dyslexic learn-\ners to read and search text-based learning material more readily by augmenting\nunnatural synthetic speech with natural recorded real speech. Although speech\nsynthesis can provide access to some text based materials for blind, visually\nimpaired or dyslexic learners, it can be difficult and unpleasant to listen to for long\nperiods and cannot match synchronised real recorded speech in conveying \u2018peda-\ngogical presence\u2019, attitudes, interest, emotion and tone, and communicating words\nin a foreign language and descriptions of pictures, equations, tables, diagrams, and\nso on.\nASR feasibility trials\nFeasibility trials using existing commercially available ASR software to provide a\nreal-time verbatim displayed transcript in lectures for deaf students in 1998 by the\nauthor in the United Kingdom (Wald, 2000) and St Mary\u2019s University, Nova\nScotia in Canada identified that standard speech recognition software (e.g. Dragon,\nViaVoice [Scansoft,6 2005]) was unsuitable as it required the dictation of punctua-\ntion, which does not occur naturally in spontaneous speech in lectures. The soft-\nware also stored the speech synchronised with text in proprietary non-standard\nformats for editing purposes only\u2014so that when the text was edited, speech and\nsynchronisation could be lost. Without the dictation of punctuation, the ASR soft-\nware produced a continuous unbroken stream of text that was very difficult to read\nand comprehend. Attempts to insert punctuation by hand in real time proved\nunsuccessful. The trials, however, showed that reasonable accuracy could be\nachieved by interested and committed lecturers who spoke very clearly and care-\nfully after extensively training the system to their voice by reading the training\nscripts and teaching the system any new vocabulary that was not already in the\ndictionary.\nBased on these feasibility trials the international Liberated Learning Collaboration\nwas established by Saint Mary\u2019s University, Nova Scotia, Canada in 1999, and since\nthen the author has continued to work with IBM and Liberated Learning to investi-\ngate how ASR can make speech more accessible.\n14 M. Wald\nAutomatic formatting\nIt is very difficult to usefully automatically punctuate transcribed spontaneous\nspeech as ASR systems can only recognise words and cannot understand the\nconcepts being conveyed. Further investigations and trials demonstrated it was\npossible to develop an ASR application that automatically formatted the transcrip-\ntion by breaking up the continuous stream of text based on the length of the pauses\/\nsilences in the speech stream. Since people do not naturally spontaneously speak in\ncomplete sentences, attempts to insert conventional punctuation (e.g. a comma for\na shorter pause and a fullstop for a longer pause) in the same way as normal written\ntext did not provide a very readable and comprehensible display of the speech. A\nmore readable approach was achieved by providing a visual indication of pauses\nshowing how the speaker grouped words together (e.g. one new line for a short\npause and two for a long pause; it is, however, possible to select any symbols as\npause markers).\nText created automatically from spontaneous speech using ASR usually has a\nmore colloquial style than academic written text and, although students may prefer\nthis, some teachers had some concerns that this would make it appear that they\nhad poor writing skills. Editors were therefore used to correct and punctuate the\ntranscripts before making them available to students online after the lectures.\nHowever, lecturers\u2019 concerns that a transcript of their spontaneous utterances will\nnot look as good as carefully prepared and hand-crafted written notes can be met\nwith the response that students at present can tape a lecture and then get it\ntranscribed. Students are capable of understanding the different purposes and\nexpectations of a verbatim transcript of spontaneous speech and pre-prepared writ-\nten notes.\nThe \u2018Liberated Learning\u2019 concept\nThe potential of using ASR to provide automatic captioning of speech in higher\neducation classrooms has now been demonstrated in \u2018Liberated Learning\u2019 classrooms\nin the United States, Canada and Australia (Bain et al., 2002; Wald, 2002; Leitch &\nMacMillan, 2003). Lecturers spend time developing their ASR voice profile by train-\ning the ASR software to understand the way they speak. This involves speaking the\nenrolment scripts, adding new vocabulary not in the system\u2019s dictionary and training\nthe system to correct errors it has already made so that it does not make them in the\nfuture. Lecturers wear wireless microphones, providing the freedom to move around\nas they are talking, while the text is displayed in real time on a screen using a data\nprojector so students can simultaneously see and hear the lecture as it is delivered.\nAfter the lecture the text is edited for errors and made available for students on the\nInternet.\nTo make the Liberated Learning vision a reality, the prototype ASR application\n\u2018Lecturer\u2019, developed in 2000 in collaboration with IBM, was superseded the follow-\ning year by \u2018IBM ViaScribe\u2019. Both applications used the ViaVoice \u2018engine\u2019 and its\nAutomatic Speech Recognition and receptive communication 15\ncorresponding training of voice and language models, and automatically provided\ntext displayed in a window and stored for later reference synchronised with the\nspeech. ViaScribe used a standard file format (e.g. SMIL) enabling synchronised\naudio and the corresponding text transcript and slides to be viewed on an Internet\nbrowser or through media players that support the SMIL 2.0 standard for accessible\nmultimedia.\nViaScribe7 can automatically produce a synchronised captioned transcription of\nspontaneous speech using automatically triggered formatting from live lectures, or in\nthe office, or even, using speaker-independent recognition, from recorded speech files\non a website (Bain et al., 2005).\nAccuracy\nStudies to date have shown that it has proved difficult to obtain an accuracy of\nover 85% in all higher education classroom environments directly, from the speech\nof all teachers (Leitch & MacMillan, 2003). It was also observed that lecturers\u2019\nASR accuracy rates were lower in classes compared with those achieved in the\noffice environment. This has also been noted elsewhere (Bennett et al., 2002).\nInformal investigations have suggested this might be because the rate of delivery\nvaried more in a live classroom situation than in the office, resulting in the ends of\nwords being run into the start of subsequent words. It is important to note that the\nstandardised statistical measurement of recognition accuracy by noting recogni-\ntion \u2018errors\u2019 does not necessarily mean that the error affected readability or under-\nstanding (e.g. substitution of \u2018the\u2019 for \u2018a\u2019). It is difficult, however, to devise a\nstandard measure for ASR accuracy that takes readability and comprehension into\naccount.\nStudent and teacher feedback\nDetailed feedback (Leitch & Macmillan, 2003) from 44 students with a wide\nrange of physical, sensory and cognitive disabilities and interviews with lecturers\nshowed that both students and teachers generally liked the Liberated Learning\nconcept and felt it improved teaching and learning as long as the text was reason-\nably accurate (i.e. >85%). Many students developed strategies to cope with errors\nin the text and the majority of students used the text as an additional resource to\nverify and clarify what they heard (e.g. 87% of students surveyed reported\nwatching the screen in class, 69% reported comparing their own notes with the\ndigitised text and 63% reported retrieving the online notes). Typical comments\nwere: \nIt gives you something to compare your notes to and if you miss a class the notes are still\naccessible.\nIt\u2019s very helpful when the lecturer moves on while you\u2019re still writing down a point as you\ncan look at the screen.\n16 M. Wald\nCoping with multiple speakers\nIn Liberated Learning classrooms, lecturers repeated questions from students so\nthey appeared transcribed on the screen. In interactive group sessions, in order that\ncontributions, questions and comments from all speakers could be transcribed\ndirectly into text, each speaker would at present need to have their own separate\npersonal ASR system trained to their voice.\nCurrent and planned developments\nLiberated Learning research and development has continued to try improving the\nusability and performance through training users, simplifying the interface and\nimproving the display readability. In addition to continuing classroom trials in the\nUSA, Canada and Australia, new trials will occur in the United Kingdom, China\nand Japan. Research and development also continues on developing the ASR appli-\ncation. MIT is a member of the Liberated Learning collaboration and is working to\nshare information to assist the incorporation of speech recognition technology into\nMIT OpenCourseWare to help students find and review lecture materials (Hazen &\nBarzilay, 2005).\nImproving accuracy through editing and\/or re-voicing\nAlthough it can be expected that developments in ASR will continue to improve accu-\nracy rates, the use of a human intermediary to improve accuracy through re-voicing\nand\/or correcting mistakes in real time as they are made by the ASR software could,\nwhere necessary, help compensate for some of ASR\u2019s current limitations\nIt is possible to edit errors in the synchronised speech and text to insert, delete or\namend the text with the timings being automatically adjusted. For example, an\n\u2018editor\u2019 correcting 15 words per minute would improve the accuracy of the\ntranscribed text from 80% to 90% for a speaker talking at 150 words per minute. Not\nall errors are equally important, and so the editor can use their initiative to prioritise\nthose that most affect readability and understanding.\nAn experienced trained \u2018re-voicer\u2019 using ASR by repeating very carefully and\nclearly what has been said can improve accuracy over the original speaker using ASR\nwhere the original speech is not of sufficient volume\/quality or when the system is not\ntrained (e.g. telephone, Internet, television, indistinct speaker, multiple speakers,\nmeetings, panels, audience questions). Re-voiced ASR is sometimes used for live\ntelevision subtitling in the United Kingdom (Lambourne et al., 2004) and in class-\nrooms and courtrooms in the United States (Francis & Stinson, 2003) using a mask\nto reduce background noise and disturbance to others.\nWhile one person acting as both the re-voicer and editor could attempt to create\nreal-time edited re-voiced text, this would be more problematic if a lecturer\nattempted to edit ASR errors while they were giving their lecture. However, a person\nediting their own ASR errors to increase accuracy might be more acceptable when\nusing ASR to communicate one-to-one with a deaf person.\nAutomatic Speech Recognition and receptive communication 17\nImproving usability and performance\nCurrent unrestricted vocabulary ASR systems normally are speaker dependent and\nso require the speaker to train the system to the way they speak, any special vocab-\nulary they use and the words they most commonly employ when writing. This\nnormally involves initially reading aloud from a provided training script, providing\nwritten documents to analyse, and then continuing to improve accuracy by improv-\ning the voice and language models by correcting existing words that are not recogn-\nised and adding any new vocabulary not in the dictionary. Current research\nincludes a new integrated speech recognition engine (\u2018Lecturer\u2019 and \u2018ViaScribe\u2019\nrequired the ViaVoice ASR engine) and providing \u2018pre-trained\u2019 voice models (the\nmost probable speech sounds corresponding to the acoustic waveform) and\nlanguage models (the most probable words spoken corresponding to the phonetic\nspeech sounds) from samples of speech, so the user does not need to spend the\ntime reading training scripts to improve the voice or language models. This should\nalso help ensure better accuracy for a speaker\u2019s specialist subject vocabularies and\nalso spoken spontaneous speech structures, which will differ from their more formal\nwritten structures. Speaker independent systems currently usually generate lower\naccuracy than trained models but systems can improve accuracy with exposure to\nthe speaker\u2019s voice.\nPersonalised displays\nLiberated Learning\u2019s research has shown that while projecting the text onto a large\nscreen in the classroom has been used successfully, it is clear that in many situations\nan individual personalised and customisable display would be preferable or essential.\nAn application is therefore being developed to provide users with their own personal\ndisplay on their own web-enabled wireless systems (e.g. computers, PDAs, mobile\nphones, etc.) customised to their preferences (e.g. font, size, colour, text formatting\nand scrolling).\nHighlighting, annotation and manipulation of synchronised material\nSince it would take students a long time to read through a verbatim transcript after a\nlecture and summarise it for future use, it would be valuable for students to be able\nto create an annotated summary for themselves in real time through highlighting,\nselecting and saving key sections of the transcribed text and adding their own words\ntime linked with the synchronised transcript.\nManaging, searching and indexing multimedia\nIt is difficult to search multimedia materials (e.g. speech, video, PowerPoint files),\nand using ASR to synchronise speech with transcribed text files can assist learners and\nteachers to manipulate, index, bookmark, manage and search for online digital\n18 M. Wald\nmultimedia resources that include speech by means of the synchronised text. Stan-\ndard synchronised multimedia streams do not currently offer a simple way to achieve\nthis.\nConclusion\nIt would appear to be reasonable to expect educational material produced by staff and\nstudents to be accessible to disabled students whenever possible, and audiovisual\nmaterial in particular can benefit from captioning. Screen readers using speech\nsynthesis can provide access to many materials but it will also sometimes be helpful\nto provide real synchronised speech. ASR enables academic staff to take a proactive\nrather than a reactive approach to teaching students with disabilities by providing\npractical, economic methods to make their teaching accessible and assist learners to\nmanage and search online digital multimedia resources. This can improve the quality\nof education for all students because the automatic provision of accessible synchro-\nnised lecture notes enables students to concentrate on learning and enables teachers\nto monitor and review what they said and reflect on it to improve their teaching.\nThe only ASR application that is currently being used in classrooms to provide a\nreal-time synchronised and editable transcription would appear to be IBM ViaScribe;\ntherefore, to further research and develop the use of ASR, applications need to\ncontinue to be developed for use by researchers, staff and students. For example\nViaScribe automatically produces a phonetic transcription, and this could be used for\n\u2018phonetic searching\u2019 (Clements et al., 2002) without the need to correct ASR errors\nin the transcript. Phonetic searching is faster than searching the original speech and\ncan also help overcome ASR \u2018out of vocabulary\u2019 errors that occur when words spoken\nare not known to the ASR system, as it searches for words based on their phonetic\nsounds not their spelling.\nNotes\n1. Skills for Access: http:\/\/www.skillsforaccess.org.uk\n2. Dolphin: http:\/\/www.dolphinaudiopublishing.com\/products\/EasePublisher\/index.htm\n3. DAISY: http:\/\/www.daisy.org\n4. MAGpie: http:\/\/ncam.wgbh.org\/webaccess\/magpie\/\n5. SMIL: http:\/\/www.w3.org\/AudioVideo\/\n6. Scansoft: http:\/\/www.nuance.com\/\n7. ViaScribe: http:\/\/www-306.ibm.com\/able\/solution_offerings\/ViaScribe.html\nReferences\nBain, K., Basson, S., Faisman, A. & Kanevsky, D. (2005) Accessibility, transcription, and access\neverywhere, IBM Systems Journal, 44(3), 589\u2013603. Available online at: http:\/\/\nwww.research.ibm.com\/journal\/sj\/443\/bain.pdf (accessed 1 November 2005).\nBain, K., Basson, S. & Wald, M. (2002) Speech recognition in university classrooms, Proceedings\nof the Fifth International ACM SIGCAPH Conference on Assistive Technologies (New York, NY,\nACM Press), 192\u2013196.\nAutomatic Speech Recognition and receptive communication 19\nBanes, D. & Seale, J. (2002) Accessibility and inclusivity in further and higher education: an over-\nview, in: L. Phipps, A. Sutherland & J. Seale (Eds) Access all areas: disability, technology and\nlearning (York, JISC TechDis and Oxford, ALT), 1\u20135.\nBennett, S., Hewitt, J., Kraithman, D. & Britton, C. (2002) Making chalk and talk accessible,\nACM SIGCAPH Computers and the Physically Handicapped, 73\u201374, 119\u2013125.\nClements, M., Robertson, S. & Miller, M. S. (2002) Phonetic searching applied to on-line distance\nlearning modules. Available online at: http:\/\/www.imtc.gatech.edu\/news\/multimedia\/\nspe2002_paper.pdf (accessed 1 November 2005).\nDowns, S., Davis, C., Thomas, C. & Colwell, J. (2002) Evaluating speech-to-text communication\naccess providers: a quality assurance issue, paper presented at PEPNet 2002: Diverse Voices,\nOne Goal Proceedings from Biennial Conference on Postsecondary Education for Persons who are\nDeaf or Hard of Hearing, 10\u201313 April. Available online at: http:\/\/sunsite.utk.edu\/cod\/pec\/prod-\nucts\/2002\/downs.pdf (accessed 11 November 2005).\nDraffan, E. (2002) Dyslexia and technology, in: L. Phipps, A. Sutherland & J. Seale (Eds) Access\nall areas: disability, technology and learning (York, JISC TechDis and Oxford, ALT), 24\u201328.\nFrancis, P. M. & Stinson, M. (2003) The C-Print speech-to-text system for communication access\nand learning, paper presented at Proceedings of CSUN Conference Technology and Persons with\nDisabilities, California State University Northridge. Available online at http:\/\/www.csun.edu\/\ncod\/conf\/2003\/proceedings\/157.htm (accessed 11 November 2005).\nHargrave-Wright, J. (2002) JISC technology and standards watch: improvements in hands-free access to\ncomputers. Available online at: http:\/\/www.jisc.ac.uk\/uploaded_documents\/tsw_02-06.doc\n(accessed 20 July 2005).\nHazen, T. J. & Barzilay, R. (2005) Automatic processing of spoken and written lecture material,\npaper presented at Proceedings of Speech Technologies: Captioning, Transcription and Beyond,\nIBM T.J. Watson Research Center, New York. Available online at: http:\/\/www.nynj.avios.org\/\nProceedings.htm (accessed 1 November 2005).\nHMSO (2001) The Special Educational Needs and Disability Act. Available online at: http:\/\/\nwww.legislation.hmso.gov.uk\/acts\/acts2001\/20010010.htm (accessed 5 October 2005).\nJM Consulting (2002) Issues paper: teaching and learning infrastructure in higher education: report to\nthe HEFCE. Available online at: http:\/\/www.hefce.ac.uk\/Pubs\/hefce\/2002\/02_31.htm\n(accessed 26 May 2005).\nLambourne, A., Hewitt, J., Lyon, C. & Warren, S. (2004) Speech-based real-time subtitling\nservices, International Journal of Speech Technology, 7, 269\u2013279.\nLeitch, D. & MacMillan, T. (2003) Liberated learning initiative innovative technology and inclusion:\ncurrent issues and future directions for liberated learning research. Year IV report, Saint Mary\u2019s\nUniversity, Nova Scotia. Available online at: http:\/\/www.liberatedlearning.com\/research\/\nYear%20IV%20research%20report%202003.pdf (accessed 11 November 2005).\nRobison, J. & Jensema, C. (1996) Computer speech recognition as an assistive device for deaf and\nhard of hearing people, paper presented at \u2018Challenge of Change: Beyond the Horizon\u2019 Proceed-\nings from Seventh Biennial Conference on Postsecondary Education for Persons who are Deaf or Hard\nof Hearing, April. Available online at: http:\/\/sunsite.utk.edu\/cod\/pec\/products\/1996\/robi-\nson.pdf (accessed 11 November 2005).\nRNID (2005) Virtual signing. Available online at: http:\/\/www.rnid.org.uk\/howwehelp\/\nresearch_and_technology\/communication_and_broadcasting\/virtual_signing\/ (accessed 19\nJuly 2005).\nStinson, M., Ross Stuckless, E., Henderson, J. & Miller, L. (1988) Perceptions of hearing-\nimpaired college students towards real-time speech to print: real time graphic display and\nother educational support services, The Volta Review, 90, 336\u2013348.\nWAI (1999) Web content accessibility guidelines version 1.0. Available online at: http:\/\/www.w3.org\/\nTR\/WCAG10\/ (accessed 26 May 2005).\nWald, M. (2000) Developments in technology to increase access to education for deaf and hard of\nhearing students, paper presented at Proceedings of CSUN Conference Technology and Persons\n20 M. Wald\nwith Disabilities, California State University Northridge. Available online at: http:\/\/\nwww.csun.edu\/cod\/conf\/2000\/proceedings\/0218Wald.htm (accessed 11 November).\nWald, M. (2002) Hearing disability and technology, in: L. Phipps, A. Sutherland & J. Seale (Eds)\nAccess all areas: disability, technology and learning (York, JISC TechDis and Oxford, ALT),\n19\u201323.\n"}