{"doi":"10.1080\/0968776960040307","coreId":"14329","oai":"oai:generic.eprints.org:222\/core5","identifiers":["oai:generic.eprints.org:222\/core5","10.1080\/0968776960040307"],"title":"Evaluation in a project life\u2010cycle: The hypermedia CAMILLE project","authors":["Chanter, Thierry"],"enrichments":{"references":[{"id":200651,"title":"(in press), 'Creating interactive multimedia CALLware: the CAMILLE experience',","authors":[],"date":"1995","doi":null,"raw":"Emery, C, Ingraham, B., Chanier, T. and Brouwer, J. (in press), 'Creating interactive multimedia CALLware: the CAMILLE experience', Eurocall '95 conference, Valencia, Spain, September, 1995.","cites":null},{"id":200646,"title":"(in press), 'Integrating CALL in the negotiated learner-centred curriculum: a case study',","authors":[],"date":"1995","doi":null,"raw":"Blin, F. (in press), 'Integrating CALL in the negotiated learner-centred curriculum: a case study', Eurocall '95 Conference, Valencia, Spain, September, 1995.","cites":null},{"id":200652,"title":"Adult language-learning styles and strategies in an intensive training setting',","authors":[],"date":"1990","doi":"10.2307\/327627","raw":"Ehrman, M. and Oxford, R. (1990), 'Adult language-learning styles and strategies in an intensive training setting', Modern Language Journal, 74 (3), 311-27.","cites":null},{"id":454496,"title":"An approach to the evaluation of hypermedia',","authors":[],"date":"1991","doi":"10.1016\/0360-1315(91)90067-2","raw":"Knussen, C, Tanner, G. R. and Kibby, M. R. (1991), 'An approach to the evaluation of hypermedia', Computers and Education, 17 (1), 13-24.","cites":null},{"id":200654,"title":"Autonomie et apprentissage auto-dirig\u00e9: quelques sujets de reflexion',","authors":[],"date":"1991","doi":null,"raw":"Holec, H. (1991), 'Autonomie et apprentissage auto-dirig\u00e9: quelques sujets de reflexion', in Les Auto-apprentissages, Actes des 6\u00e8mes Rencontres de l'ASDIFLE, Paris: Les Cahiers de l'ASDIFLE, 2, pp. 23-33.","cites":null},{"id":200655,"title":"CAMILLE: a European project to develop language training for different purposes, in various languages on a common hypermedia framework',","authors":[],"date":"1994","doi":"10.1016\/0360-1315(94)90038-8","raw":"Ingraham, B., Chanier, T. and Emery, C. (1994), 'CAMILLE: a European project to develop language training for different purposes, in various languages on a common hypermedia framework', Computers and Education, 23 (1\/2), 107-15.","cites":null},{"id":200649,"title":"De l'EAO aux NTF: utiliser l'ordinateur pour la formation,","authors":[],"date":"1992","doi":null,"raw":"Demaizi\u00e8re, F. and Dubuisson, C. (1992), De l'EAO aux NTF: utiliser l'ordinateur pour la formation, Paris: Ophrys.","cites":null},{"id":200648,"title":"D\u00e9finir les objectifs de l'\u00e9ducation,","authors":[],"date":"1984","doi":null,"raw":"de Landsheere, V. and G. (1984), D\u00e9finir les objectifs de l'\u00e9ducation, Paris: PUF.","cites":null},{"id":454498,"title":"Evaluation methodologies for intelligent tutoring systems',","authors":[],"date":"1993","doi":null,"raw":"Mark, M. A. and Greer, J. E. (1993). 'Evaluation methodologies for intelligent tutoring systems', Journal of Artificial Intelligence in Education, 4 (2\/3), 129-53.","cites":null},{"id":200653,"title":"Fundamentals of Non-verbal Behaviour, Cambridge and Paris:","authors":[],"date":"1991","doi":null,"raw":"Feldman, R. S. and Rim\u00e9, B. (1991), Fundamentals of Non-verbal Behaviour, Cambridge and Paris: CUP and Editions de la Maison des Sciences de l'Homme.","cites":null},{"id":200647,"title":"Learning a second language for specific purposes within a hypermedia framework',","authors":[],"date":"1996","doi":"10.1080\/0958822960090102","raw":"Chanier, T. (1996), 'Learning a second language for specific purposes within a hypermedia framework', Computer-Assisted Language Learning, 9 (1), 3-43.","cites":null},{"id":454499,"title":"Learning Strategies in Second Language Acquisition,","authors":[],"date":"1990","doi":"10.2307\/415153","raw":"O'Malley, J. M. and Chamot, A. U. (1990), Learning Strategies in Second Language Acquisition, Cambridge: CUP. Pothier, M. (in press), 'Travailler en France: un environnement informatique hypermedia pour l'auto-apprentissage sur objectifs sp\u00e9cifiques', Revue de Phon\u00e9tique Appliqu\u00e9e.","cites":null},{"id":200650,"title":"The Study of Second Language Acquisition,","authors":[],"date":"1994","doi":"10.1017\/s0272263100014479","raw":"Ellis, R. (1994), The Study of Second Language Acquisition, Oxford: OUP.","cites":null},{"id":454497,"title":"Valuer les apprentisages dans une approche communicative,","authors":[],"date":"1992","doi":null,"raw":"Lussier, D. (1992), Valuer les apprentisages dans une approche communicative, Paris: Hachette.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"1996","abstract":"In the CAL literature, the issue of integrating evaluation into the life\u2010cycle of a project has often been recommended but less frequently reported, at least for large\u2010scale hypermedia environments. Indeed, CAL developers face a difficult problem because effective evaluation needs to satisfy the potentially conflicting demands of a variety of audiences (teachers, administrators, the research community, sponsors, etc.). This paper first examines some of the various forms of evaluation adopted by different kinds of audiences. It then reports on evaluations, formative as well as summative, set up by the European CAMILLE project teams in four countries during a large\u2010scale courseware development project. It stresses the advantages, despite drawbacks and pitfalls, for CAL developers to systematically undertake evaluation. Lastly, it points out some general outcomes concerning learning issues of interest to teachers, trainers and educational advisers. These include topics such as the impact of multimedia, of learner variability and learner autonomy on the effectiveness of learning with respect to language skills","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/14329.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/222\/1\/ALT_J_Vol4_No3_1996_Evaluation_in_a_project_life%E2%80%90c.pdf","pdfHashValue":"a44f28f248bdfa0836d85f817f906641557c1f13","publisher":"Universit of Wales Press","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:222<\/identifier><datestamp>\n      2011-04-04T09:22:55Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/222\/<\/dc:relation><dc:title>\n        Evaluation in a project life\u2010cycle: The hypermedia CAMILLE project<\/dc:title><dc:creator>\n        Chanter, Thierry<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        In the CAL literature, the issue of integrating evaluation into the life\u2010cycle of a project has often been recommended but less frequently reported, at least for large\u2010scale hypermedia environments. Indeed, CAL developers face a difficult problem because effective evaluation needs to satisfy the potentially conflicting demands of a variety of audiences (teachers, administrators, the research community, sponsors, etc.). This paper first examines some of the various forms of evaluation adopted by different kinds of audiences. It then reports on evaluations, formative as well as summative, set up by the European CAMILLE project teams in four countries during a large\u2010scale courseware development project. It stresses the advantages, despite drawbacks and pitfalls, for CAL developers to systematically undertake evaluation. Lastly, it points out some general outcomes concerning learning issues of interest to teachers, trainers and educational advisers. These include topics such as the impact of multimedia, of learner variability and learner autonomy on the effectiveness of learning with respect to language skills.<\/dc:description><dc:publisher>\n        Universit of Wales Press<\/dc:publisher><dc:date>\n        1996<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/222\/1\/ALT_J_Vol4_No3_1996_Evaluation_in_a_project_life%E2%80%90c.pdf<\/dc:identifier><dc:identifier>\n          Chanter, Thierry  (1996) Evaluation in a project life\u2010cycle: The hypermedia CAMILLE project.  Association for Learning Technology Journal, 4 (3).  pp. 54-68.  ISSN 0968-7769     <\/dc:identifier><dc:relation>\n        10.1080\/0968776960040307<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/222\/","10.1080\/0968776960040307"],"year":1996,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"Evaluation in a project life-cycle:\nthe hypermedia CAMILLE project\nThierry Chanter\nUniversit\u00e9 de Clermont II and Universit\u00e9 de Paris V\nIn the CAL literature, the issue of integrating evaluation into the life-cycle of a project has often been\nrecommended but less frequently reported, at least for large-scale hypermedia environments. Indeed,\nCAL developers face a difficult problem because effective evaluation needs to satisfy the potentially\nconflicting demands of a variety of audiences (teachers, administrators, the research community,\nsponsors, etc.). This paper first examines some of the various forms of evaluation adopted by different\nkinds of audiences. It then reports on evaluations, formative as well as summative, set up by the\nEuropean CAMILLE project teams in four countries during a large-scale courseware development\nproject. It stresses the advantages, despite drawbacks and pitfalls, for CAL developers to\nsystematically undertake evaluation. Lastly, it points out some general outcomes concerning learning\nissues of interest to teachers, trainers and educational advisers. These include topics such as the impact\nof multimedia, of learner variability and learner autonomy on the effectiveness of learning with respect\nto language skills.\nIntroduction\nThis paper reports on a series of evaluations undertaken in the countries which\nparticipated in the CAMILLE project.1 The principal aim of this European project has\nbeen the development and delivery of hypermedia courseware in Dutch, Spanish and\nFrench. The courseware encompasses the training of general linguistic competencies for\nbeginners (Dutch and Spanish) as well as competencies related to the use of language for\nspecific purposes (French). The target audience includes students in science or business,\nand technicians or engineers from SMEs (Small and Medium Enterprises - small\nbusinesses). This report may be of interest to two kinds of reader of this journal, as\nfollows.\n\u2022 Each one of our packages exploit the full range of hypertextual and multimedia\nfacilities currently provided by standard computing platforms. Furthermore, each\npackage offers learners a large-scale learning environment capable of supporting\nautonomous study. Consequently, these preliminary outcomes relating to the way\n54\nALT-J Volume 4 Number 3\nCAMILLE has been practically used by learners and to its effectiveness are of\npotential interest to teachers, trainers and educational advisers.\n\u2022 The various experiments conducted by the teams and integrated into the process of\nsoftware development will be of interest to Computer-Aided Learning (CAL)\ndevelopers in general. Indeed, within the CAL literature, the issue of integrating\nevaluation into the life of a project, i.e. either in the course of the development or at\nthe end of it, has often been recommended but much less frequently reported, at least\nfor this type of environment. The paper discusses the constraints, advantages and\ndrawbacks of actually adopting such a procedure.\nIn order to make clear both the nature of the experiments undertaken within CAMILLE\nand the significance of the results obtained, a brief preamble on evaluation is necessary.\nThe term evaluation is widely used by various groups connected with Computer-Assisted\nLanguage Learning (CALL) but frequently approached from very different perspectives,\nand this can leave the reporting of results open to misinterpretation. At one end of the\nspectrum there is an increasing pressure on researchers and developers to adopt more\nmethodological and scientific procedures, and, at the other end, educational advisers and\nexecutives constantly require concrete and positive results before extending their support\nto CALL. CAMILLE is one project, among an increasing number of others, which has\nhad to try to make these potentially contradictory viewpoints coexist.\nBelow I describe various aspects of evaluation in language learning and in CAL. After\nthis, I set out the initial requirements and achievements of the CAMILLE project and\nintroduce the common features of the different experiments. This is followed by detailed\nevaluations made in two countries, and a report of the main general outcomes, summing\nup our experience of managing evaluation as an integral part of a project life-cycle.\nPreamble on evaluation\nIn order to delimit the framework adopted in this research, this section presents the\nprincipal functions of evaluation, the initial questions in the design process, the\noverlapping forms of evaluation, and the evaluation procedure.\nFunctions of evaluation\nFor almost thirty years, a distinction has been frequently made between two principal\nfunctions of evaluation: formative evaluation and summative evaluation. This distinction\nexists in language teaching (Lussier, 1992) as well as in CAL (Knussen et al, 1991;\nDemaiziere and Dubuisson, 1992; Mark and Greer, 1993), but they are differently\ninterpreted.\nIn language teaching, formative evaluation consists in regularly diagnosing the learner's\nstate of knowledge, abilities, attitudes. It is undertaken for learners in order to let them\nknow their current position with respect to their final goals; and for teachers to gain\ninformation that may lead them to adjust and adapt their teaching before the end of the\ncourse. In CAL, formative evaluation also occurs before the end of the implementation\nphase. It is intended to help the designers review their progress towards achieving the\ngoals of an educational innovation. It is set up by designers, and involves a few learners\nwho are carefully observed in order to assess whether they use the software as intended.\n55\nThieny Chanter Evaluation in a project life-cycle: the hypermedia CAMILLE project\nSuch aspects as interface, human-machine interaction, learner strategies, hardware\nconfiguration and computing architecture are observed with rather informal methods.\nThis process brings both detailed and general information, which may lead to surface\nchanges (correction of bugs) or more profound changes in the design and the\ndevelopment. It also provides insights into the way the courseware will perform when\nintegrated into a real-life learning situation.\nIn language teaching, summative evaluation comes at the conclusion of a course, or a\nprogramme, in order to measure the level of proficiency acquired by a learner with respect\nto normative goals explicitly fixed by the learning institution. It is a global measure which\ncompares the performance of learners. It is intended to certify learners in order to give\nthem credits, to recommend an orientation, or to check the effectiveness of the course or\nprogramme. In CAL, summative evaluation is concerned with the evaluation of\ncompleted systems. Its purpose is to measure the effectiveness of an innovation in terms\nof its stated aims. It is intended for trainers, centres and designers to assess the suitability\nof the software for certain tasks and users, or to compare it with other products already in\nuse. In both cases, summative evaluation has to be undertaken in real-learning settings,\nand to involve a larger number of subjects than formative evaluation.\nSince the central topic of this paper is the role of evaluation in developing a software\npackage, I will adopt the CAL standpoint rather than that of language teaching.\nMoreover, since the computing environments developed in the CAMILLE project have\nbeen designed to support autonomous learning, some aspects of the language-teaching\nmodel would be inappropriate. However, beyond the discrepancies between CAL and\nlanguage-teaching models of evaluation, there is a common feature which distinguishes\nthem from the issue of assessment. Evaluation is not a judgemental but a decision-making\nprocess. Since outcomes may be interpreted by various audiences (e.g. designers, teachers,\ninstitutions) in order to make lasting changes, the framework for setting up an evaluation\nand its procedure will be examined hereafter.\nInitial questions in the design of an evaluation\nEvaluating a language program, or any piece of CALL software, is a complex process.\nThere follow some key questions (taken from Nunan, 1992, chapter 9) that should be\nanswered before starting any evaluation.\nObjectives: What is the purpose and who is the audience of the evaluation (for whom is it\nmade)?\nMethodology: What principles of procedure should guide the evaluation? What tools,\ntechniques, and instruments are appropriate?\nMaterial constraints: Who should carry out the evaluation? When should it be carried\nout? What is the time-frame and budget?\nRelease: How should the evaluation be reported?\nIt may seem obvious that it is extremely important to clarify, from the beginning, the\ngoals of the evaluation. However, it is not a straightforward task. Let us consider an\ninnovation. Relationships are not clear at all between the original working hypotheses of\ndesigners, the actual achievement, and the selection of precise experimental variables: a\n56\nAIT-J Volume 4 Number 3\nshift may have appeared between the starting and end points; an innovation may have\nunexpected effects (it may not raise the level of proficiency, but the learner's motivation);\ncomparisons with other existing learning environments may be problematic simply\nbecause they are so different. For example, determining a scale for measuring\neffectiveness with respect to communicative goals and specific purposes was an expected\noutcome, in itself, in the CAMILLE project. Fixing the objective of an evaluation is again\nnot always easy when the audience is diversified: designers, teachers, administrators, and\nfunding bodies often have different perceptions.\nIf learning objectives need to be elicited, they also need to be associated with precise\nforms of evaluation which are themselves associated with different methodological\napproaches. Below, I extract overlapping forms of evaluation from one (Knussen et al,\n1991) out of many possible presentations.\n\u2022 Experimental. A limited number of clearly defined variables are scientifically measured,\nusually based on statistical inferences. The laboratory is the traditional setting for\nevaluations which generally have a formative function. If such a form is considered as\nmore scientific, its relevance to real learning settings is problematic.\n\u2022 Research and developmental. The purpose is to apply quasi-experimental\nmethodologies, including pre- and post-tests, in situations closer to real learning\nsettings. This form, which many evaluations of CAL systems try to adopt, also requires\nclear statements of measurable objectives. They may be easier to guarantee in scientific\nor industrial environments than in educational ones. They more often concern\nsummative than formative functions.\n\u2022 Illuminative. Isolating variables and associated parameters, as well as quantifying\nmeasures, is hard to achieve in real learning settings, especially if estimation of the\nimpact of social factors and the participant's views on the meaning of educational\ninnovations are at stake. Consequently, methods, essentially qualitative and usually\nbased on observations and interviews, are applied to 'illuminate' important factors\nrather than to test hypotheses. Associated pitfalls here range from the risk of\nobservers' obtrusiveness to findings which cannot be generalized to apply to other\nsettings.\n\u2022 Teacher as researcher. Since teachers play a prominent role in the integration of CALL\nsystems into the curriculum, it seems natural to let them take charge of the evaluations.\nBiases (e.g. subjectivity, role-conflict, work overload) introduced by this form suggest\nthat it should be used only in addition to other approaches.\n\u2022 Case studies. Understanding the effects of situational and personal factors in the use of\ninnovative software is generally based on the detailed study of a restricted number of\nlearners. However, a generalization of the findings to other situations may be difficult.\nIn CAMILLE, two forms of evaluation have mainly been used: the research and\ndevelopmental one, and the illuminative one.\nOnce the form of the evaluation has been determined, material constraints need to be\nappreciated before performing the evaluation procedure. The first step of the procedure\nconsists in designing the whole evaluation. The initial task of the second step is the\n57\nThierry Chanier Evaluation in a project life-cycle: the hypermedia CAMILLE project\nconstruction of the instruments: materials for the tests, questionnaires and forms, and\nextra materials for the control group, if necessary. Data collection and analysis follow,\naccording to the methodological approach chosen. The third step, drafting the report,\nlearning and deciding from it, may not be the last one. In formative evaluation, immediate\ndecisions may be taken, followed by changes which then will be measured a second time.\nThe variety of tasks and their co-ordination create genuine obstacles for the successful\ncompletion of a project. Who will the evaluators be? What is the time-frame and the\nbudget? This may explain why many CALL developers seem reluctant to include an\nevaluation procedure as part of their project.\nThe last issue, raised in the initial questions, refers to the release: how is the evaluation to\nbe reported? On one hand, evaluation is often described as a public act which should be\nopen to inspection. On the other, unsatisfactory findings, and\/or disagreements between\nparticipants, may impede the publication of a final report. Alternatively, interesting\nfindings may be over-generalized if the final conclusions are not clearly delimited.\nGeneral aspects of the CAMILLE evaluation\nThe European LINGUA CAMILLE project started in 1993 and will finish this year.\nDescriptions of the project and of its theoretical standpoints can be found in Ingraham et\nal (1994); Chanier (1996); and Pothier (in press). In this section I recall only its initial\nrequirements and its main achievements. From there, I examine the purposes and\naudiences of the various evaluations, the common features shared by the different\nexperiments, and details of evaluations.\nInitial requirements of the whole project\nThe CAMILLE project is aimed at conducting a large-scale experiment touching on\nissues arising from both pedagogical and software-engineering viewpoints.\nFrom a pedagogical viewpoint, hypermedia technologies are often presented as an\nopportunity to enhance language learning. Although these factors are often assumed to\nplay an important role in the acquisition process, as yet there have been no large-scale\nexperiments based on their use. CAMILLE was thus seen as an opportunity to undertake\nsuch an examination in a multi-cultural environment. The objective was the construction\nof an environment that would provide learners with all the tools and information, short\nof a live teacher, that they might need to undertake a specific level of course in the target\nlanguage. One consequence was the integration of books\/resources (on lexicon, culture,\nfunction, grammar) with the textbook (the course proper) on the same desktop. Another\nconsequence was the mode of its use and of its integration into a whole curriculum:\nCAMILLE was designed to be used by well-motivated adults, who may or may not be\nengaged in formal education or training and who may or may not have access to a tutor.\nThus the emphasis was on autonomy.\nFrom a software-engineering viewpoint, hypermedia programming tools are often\nrecommended as an opportunity to speed up courseware development, and therefore make\nCAL a realistic complement for training learners in and out of the academic world. But\nproduction of courseware in hypermedia also dramatically increases the number of skills\nrequired, and up to now our experience in reusing modules of software, or shared knowledge\n58\nVolume 4 Number 3\nfor large-scale software, is still very limited. The CAMILLE project was supposed to help to\ngain a clearer understanding of trans-national courseware development. As a starting point,\nit was decided to use a common template for development, a template which consisted of a\nsoftware and hardware platform, created by our British partners in 1991\/92. The\neffectiveness of the software-engineering viewpoint was enforced by the decision to launch a\ncommercial release at the end of the project, i.e. at the beginning of 1996.\nMain achievements\nA few months before the conclusion of the project, the main courses finalized or near\ncompletion are as follows.\n\u2022 Espanol Interactivo, Interactif Nederlands, and France Interactive. These three packages\nare respectively designed for the training of general linguistic competencies for\nbeginners in Spanish, Dutch, and French, and developed in Spain, The Netherlands,\nand the UK.\n\u2022 Travailler en France. This package has been designed for the training of competencies\nrelated to the use of Language for Specific Purposes (LSP) for intermediate-advanced\nlevel French, and developed in France.\nEach package includes two CDs which run on a standard, basic hypermedia PC platform,\nnamely the international standard MPC2. This has the minimal equipment to play full-\nmotion video, and offers good quality for recording and playing sounds. Each disc\ngathers approximately 30 minutes of original video, plus other oral, graphical and textual\ndata, on top of which are built resources, and several dozen activities, which offer more\nthan 20 hours of study to the learner.\nWhile, at present, debugging and some coding processes are still under way, CAMILLE\npartners are fixing the legal aspects in order to start the commercial release of the most\nadvanced courses.\nCommon features of the evaluations\nFollowing the general framework discussed above, I review here the common features of\nthe evaluations undertaken by all the CAMILLE partners.\nThree kinds of audiences with their respective purposes can be distinguished.\nThe first encompasses the European Union and publishers, as external (to the CALL\ncommunity) actors which intervene in the project life. The former (the EU) partially\nfunded CAMILLE (actually for less than a quarter of the total budget), added its own\nrequirements, and annually examined achievements before deciding any extension of\nfunding. The latter (the publishers) have recently undergone internal restructuring in\norder to be prepared to release multimedia software. Most limit the major risks linked to\ninnovations by expecting developments to be supported by small, recent private ventures.\nFurthermore, they are not accustomed to dealing with academic institutions. For them\nall, evaluations were intended to assert our reliability, by proving that learners could turn\ntheir hand to our courseware in real settings, by convincing them that academics could\nchallenge private companies and be more transparent when performing evaluations as\npublic acts open to inspection.\nThe second kind of audience is the CALL community, which includes teachers and\n59\nThierry Charter Evaluation in a project life-cycle; the hypermedia CAMILLE project\nresearchers. The pedagogical perspectives outlined above needed to be made explicit. The\npurpose here is twofold: firstly, measuring what kinds of language skills multimedia\ntechnologies can help practise, what sorts of learning strategies are performed in\nhypermedia environments, and how effective autonomous learning is in various settings;\nsecondly, scaling effectiveness with respect to communicative goals and specific purposes.\nThe latter point refers to the problem of finding criteria by which educational objectives\ncan be measured: how can we assess the learner's ability to master knowledge and skills,\nmobilized around the specific purposes of each piece of courseware, to transfer them and\ncreate new pieces of discourse (cf. de Landsheere's trilogy, 1984)?\nDevelopers constitute the third kind of audience. The purpose here was to appreciate to\nwhat extent formative evaluation is necessary for adapting and debugging the learning\nenvironment, to perform summative evaluation to clarify the software goals (i.e. exactly\nidentify what can be measured) and to determine the constraints and overheads brought\nupon the whole project.\nThe different CAMILLE research teams set up evaluations, located either in their own\ninstitutions or in neighbouring ones. This happened over 14 months (1994-1995) during\nimplementation, or at the end of large parts of it. No extra budget, nor extra human\nresources, were available. The results of these evaluations are being reported in three\ndifferent ways: the final report to the European Union, conferences such as Eurocall\n(Emery et al, in press) and academic papers.\nDetails of the evaluations\nIn the CAMILLE project, objectives and methodologies varied from one research team to\nanother. As an illustration, in this section I detail evaluations undertaken on Interactif\nNederlands and Travailler en France. Results drawn from France Interactive and Espanol\nInteractivo are included in the general outcomes presented in the next section.\nEvaluation at HEBO (De Haagse Hogeschool, The Netherlands)\nThe Dutch CAMILLE team performed both formative and summative evaluations. The\nformative side of the evaluation was designed as a two-round experiment. As soon as data\nwas analysed, changes were made and new experiments were based on the modified\nsoftware. The aim of the summative evaluation was to compare the software with local\nclassroom learning. This second side directly interested the HEBO managers and the local\nteachers. The school supervised more than a thousand university Dutch or foreign\nstudents who needed intensive training in several languages for professional purposes\n(legal or business). It offered a strong integration of CALL into the curriculum: nearly\n50% of the students' work time in language learning was organized around free access to\ncomputers. Heads of the school consider the familiarization with the Dutch language and\nsociety by foreign students as an important factor of integration into a country where\nthey are spending several years. Of course, Dutch is not a 'survival' language (learners can\neasily talk English and be understood by anyone in everyday life), but attendance in\nDutch classrooms is strongly encouraged, though not mandatory, and learners' credits\ncan easily be transferred. It was thus decided that learners who learned Dutch only\nthrough Interactif Nederlands would take the same oral examination as the other learners\nof Dutch who attended classroom sessions.\n60\nAa-j Volume 4 Number 3\nThe experiment took place at the HEBO in the multimedia, free-access room. Evaluators\nused a network version of the software on computers with the recommended hardware\nconfiguration. Sixty local students were involved, on a voluntary basis. They were true\nbeginners in Dutch, but experienced language learners (Dutch often being their third\nlanguage) had a low motivation for learning Dutch, and only basic experience with\ncomputers. Learning tasks were organized around half of the software, which represented\n30 hours of work, distributed over 10 weeks, with free-access conditions. Learners had to\nfill in questionnaires and were interviewed at the end of each session. Evaluators also\nmade non-systematic observations. Data from 14 students was analysed for the formative\npart of the evaluation. This analysis will not be detailed here, but the lessons evaluators\nlearned from this experiment will be mixed with the other general outcomes in the next\nsection. The final examination was organized by the usual Dutch teachers, not by the\nevaluators. Marks and teachers' comments on the CALL group showed that results were\nneither better nor worse than usual. Since the timing and the assessment procedures were\nthe same as for the live course, the software would appear to be efficient in this sort of\nsituation and with these types of learners.\nEvaluations in CAVI LAM (France)\nFormative and summative evaluations of Module 1 of Travailler en France were\norganized at two different stages of the project: the formative evaluation in October 1994,\nat the very end of the development of the prototype of Module 1, and the summative one\nin June 1995, after changes and debugging had been finished on Module 1 and while\nModule 2 was under development. Before considering the details of these evaluations, it\nwill be helpful to consider certain common features\nLocal students, who were following full-time language training periods of 1 to 6 months\nin length, participated in the evaluation. They were between 21 and 47 years of age, with\nan average age of 25, coming from various continents and cultures. All were intermediate\n(200 hours) or advanced (400 hours) learners of French, with French often being their\nthird language. They had good professional motivation for learning, either because they\nalready had a job, or were seeking work where the mastering of specific skills in French\nwas important, or because they wanted to attend French universities. They had a mixed\nexperience with computers, some being almost computer-illiterate as they came from\ncountries where computers are not part of the work or study environment.\nBoth evaluations were undertaken with Module 1, where the specific purpose is to learn\nhow to apply for a job in France. This makes a noticeable difference from other\nCAMILLE courses which are for general purposes (Chanier, 1996). The module is built\naround one main task: making a job application. Knowledge bases and activities allow\nlearners to fulfil the task and immerse them in a socio-cultural context which determines\nthe architecture of the software. The story-line of the module presents two characters who\nare very different in nature and who encounter a series of representative situations, for\nexample how to find appropriate information and acquire experience in the employment\nmarket; how to write a letter of application and a CV in the French way; how to make an\nappointment on the telephone; how to handle an interview. Linguistic knowledge and\nactivities have been designed from the task context, but do not have top priority.\nThe learning tasks require a total amount of 20 to 25 hours' work over three weeks. The\n61\nThierry Oionier Evaluation in a project life-cycle: the hypermedia CAMILLE project\nlearners used the software during the time usually allocated for practical work in their\ntraining, and had further opportunities for free access.\nFormative evaluation\nOne purpose of the formative evaluation was to measure the performance of the\ncourseware. The second one focused on how effectively the kinds of activities and\nresources available matched the learners' strategies and interests. The sample population\nwas limited to five volunteers because we wanted one of our observers always to be\npresent. They could work alone or in a group. The observer, who acted in a non-obtrusive\nway, either video- or audio-recorded all the sessions, and took detailed notes on the\nlearners' moves, selections and timing. Learners filled in pre- and post-questionnaires and\nhad a form to fill in at the end of every session, followed by a short interview.\nThrough this procedure we were able to collect detailed information about the learners'\nbehaviour and reactions as well as their (positive) comments. All this helped us to make\nsubsequent adaptations. Details are discussed below, but one point is worth mentioning at\nthis stage because it relates to the LSP aspect of our software. Even when learners were not\ndirectly, personally concerned with seeking a job, they all (even subjects of the summative\nevaluation) indicated that the experience provided important discoveries concerning socio-\ncultural aspects of the target-language country, and of its everyday native language.\nApprehending variations in the target language and links between language and complex\nsituations encountered daily by natives is an efficient way of raising language awareness; as\nsuch, it is an important aspect of second-language learning.\nSummative evaluation\nThe purposes of the summative evaluation were threefold:\n\u2022 assessment of the suitability of the first LSP courseware with respect to the local\nlearners,\n\u2022 comparison with autonomous (audio + paper) learning,\n\u2022 measurement of the impact of hypermedia CALL on vocabulary learning.\nFor this second experiment, the audience was not limited to the project team. The\nCAVILAM staff were also interested in the outcomes, and took over the supervision of\nthe learning task, acting as counsellors. The project team only handled the various tests.\nSubjects were divided into two groups on a voluntary basis: group 1 (Gl), the paper-and\naudio-based group, comprised six people; group 2 (G2), the CALL one, seven. For Gl we\nextracted large parts of textual data contained in the software activities and resources, and\nall the sounds of the dialogues. They then had a document and audio-cassettes to work\nwith. They also had access to paper-based dictionaries available in the language laboratory.\nWe prepared pre- and post-questionnaires, the post-questionnaire contents being different\nfor Gl and G2. We also translated into French and administrated the SILL (Strategy\nInventory for Language Learning test: Ehrman and Oxford, 1990) which allows learners\nclearly to indicate which sort of strategies they usually apply when learning a language\ngenerally. Results show to what extent they use (and are aware of using) appropriate\nstrategies for remembering more effectively, using mental processes, compensating for\nmissing knowledge, organizing and evaluating their learning, managing emotions, and\n62\nALT-J Volume 4 Number 3\nlearning with others. Subjects also passed a pre- and a post-test on vocabulary (pre- and\npost-tests were identical) and a post-test to assess communicative competence in the same\ndomain. For the latter one, called the main post-test, we created original aural and textual\nmaterials. Subjects had to write their answers and essays. The main post-test had three\nparts: an aural comprehension of an interview which included subjective appreciation of\nthe applicant's situation; a comprehension and a written production of part of the\nexchanges in a dialogue on the telephone; and the writing of a letter of application for a\npost-profile described in an advertisement. This test was not ready when the experiment\nstarted, so we could only use it as a post-test.\nThe two groups appeared not to be equally balanced. Analysis of subjects' answers in G2,\nthe computer-based group, showed that they used more varied strategies and were more\nself-conscious of the way they usually learned. They proved to have better lexical\nknowledge than Gl in the pre-test. Both groups progressed in this domain, Gl slightly\nmore than G2. This may not be very surprising since the lexical test was difficult (the\nemphasis was put upon the relationship between words and phrases, and collocations;\nsemantic relationships, grammatical structures and relational constraints of lexical\nphrases were required to be understood). Within this context, progression in subjects with\nlower-level knowledge is easier. As regards the main post-test, there was not much\ndifference between Gl and G2. This result is not easy to explain since samples were\nlimited in both groups. However, we noticed that Gl behaved as if they were competing\nagainst G2. The learners in Gl did, however, have to find by themselves extra resources\nwhich were easily available in the software: for example, we observed that Gl learners\nfrequently used dictionaries. Gl strongly protested against their learning materials which\nthey found boring, while G2 found much interest in the software. Learning may have\nbeen a harder process for Gl, but both groups satisfactorily learned, and passed their\nexams (vocabulary and main post-tests), which was what we were expecting.\nConclusions\nGeneral outcomes\nMultimedia\nThe activities which the learners rated most highly were the video-based and the audio-\nbased activities, in order of preference. When asked to evaluate the activities upon quality\nalone, this order of preference was reversed. When learners found quality of sound\nunsatisfactory, they expressed their view strongly, although they never complained about\nthe definition or size of the video material. This result supports our original decision to\nuse the basic MPC2 standard, since limitations to the quality of video are less important\ngiven the range of functions we assigned to video. In CAMILLE, as in other CALL\nenvironments, video is primarily used:\n\u2022 to put language into context, thus to raise motivation;\n\u2022 to support the interpretation of the linguistic contents of utterances: in simulation\nactivities, looking at the speaker's face may bring information on the pragmatic\ncontents of the message (happiness, irony, discontent, etc.), and when pronunciation\nactivities are essential, as in the first lessons of the Dutch course, focusing on the\nspeaker's lip movements facilitates comprehension and production of phonemes.\n63\nThierry Chanter Evaluation in a project life-cycle: the hypermedia CAMILLE project\nFor such functions, the video supports sound. This means that when use of video is\nsuppressed (for example, in some telephone-based activities where we wanted to increase\nthe level of difficulty), the linguistic content is still comprehensible, provided that the\nquality of the sound is very good.\nHowever, learning a language is not reducible to purely linguistic knowledge. Kinesics\nand proxemics are also very important (Feldman and Rive, 1991). In real communication\nsettings, the hearer not only interprets the speaker's message from its linguistic content,\nbut also from his\/her gestures, location, etc. In situations such as interviews or\nnegotiations, the issue not only relies on what is said but also fundamentally on the\npredisposition of the various parties - a predisposition which will, be interpreted\naccording to a protocol of behaviour and gestures. In foreign-language learning, these\naspects are never neglected in live courses. If we want to do as well in CALL, we need, to\nstudy other functional uses of video. In one module of the French for Business\ncourseware, we have designed three activities on gestures, which can either support the\nverbal message or completely replace it. However, no experiments have yet been made\nconcerning this new type of activity because its development was completed only after the\nevaluation phase.\nAs regards sound, the results of our experiments showed that we had underestimated the\npotential of simple technologies which allow recording and producing sounds of high\nquality. In CAMILLE, it is possible for the learner to record him\/herself in almost every\nactivity. In some of them, self-recording is an accessory, but in others (like simulation\nactivities) it is essential. Experiments showed that even if all learners regarded it as\nimportant to have self-recording facilities, there was a large discrepancy between the way\nthey claimed to use these resources and the extent to which they actually did. This can be\nexplained by learners' lack of self-assurance, and by the lack of explicit stress in the first\nversions of our software on this important and preliminary step for the support of oral\nproduction skills. We have now switched to simple solutions such as adding signposts and\ninteractive comments in relevant activities, and in the general learners' follow up. In fact,\nthe CAL environment must indicate to every user the importance of adopting effective,\ninteractive strategies, such as re-recording oneself several times and making a (subjective)\ncomparison with the model (as we observed some learners actually doing).\nLearner variability\nIn all the questionnaires, learners almost unanimously expressed their preference for\ninteractive activities compared to more passive ones, but they disagreed about which ones\nthey considered better (with the exception of simulations, which were always highly\nrated). Learners also often stressed the fact that even if they found communicative\nactivities attractive, basic linguistic activities, on grammar or vocabulary, should not be\nforgotten. In the case of InteractifNederlands, for example, this led to an adjustment of\nthe balance between both types of activities by adding new, more linguistically oriented,\ntasks. The learners' reaction was not necessarily a plea for activities of a 'traditional'\nnature. Linguistic activities can be designed in new ways. Thus learners found our\npresentation of vocabulary knowledge in lexical networks in Travailler en France very\nappealing.\nLearner variability appeared not only in opinions but also in ways of working with the\n64\nALT-] Volume 4 Number 3\ncourseware. Learners followed very different routes in the scheduling of their overall\nwork: some undertook activities strictly in the order suggested by our presentation; others\ntook a quick overview of the whole contents and of the various kinds of activities (which\nwere signposted), then started with the ones they preferred. Learners also performed\nactivities in very different ways, some trying to finish them quickly without paying much\nattention to instructions or without looking at the associated resources (they generally\nthen got stuck and had to restart), others self-monitoring their task by first carefully\nconsidering in which order to proceed, looking at the cues and available resources. Some\nwere systematic, relying on repetitions of self-recording and exhausting the various\npossible alternatives. Some systematically took notes before actually performing any\nactivity. Some verbalized their thoughts and reactions, whereas others were almost\ncompletely silent. When group-work occurred, and when skills were complementary\ninside the group, effective collaboration took place with one taking over the interaction\nwith the system, while a second controlled the planning, or negotiated the knowledge.\nThis learner variability is an important positive outcome. Disagreement on attractiveness\nof activities showed that everyone found their own interest. Variations in the way of using\nthe software happened according to learners' personal characteristics. Whatever our\nwishes may be in expecting learners to follow a particular route, individual variability\nremains the rule in language learning (Ellis, 1994). One of the advantages of multimedia\nlearning environments is the support they can give to these individual variations by\noffering different types of activity, practice of different linguistic skills, flexible\nnavigation, access to resources of various kinds, and note-taking.\nAutonomy\nCAMILLE has been designed for an audience of learners who are typically clients,\nprofessionals with clear demands and for whom flexibility and swiftness are essential\ncriteria. Sample populations involved in our evaluations mostly corresponded to this\nprofile. Furthermore, nearly all were experienced learners, either advanced learners of a\nsecond language, or beginners in a third. They were aware of their own preferred learning\nstrategies, and used software in an autonomous way, evaluators and teachers, when\npresent, being merely observers.\nFrom the learners' answers, and from our own observations, it is possible to underline the\npoints which follow.\n\u2022 When we developed our software, we recognized the need to distinguish between activities\nand resources, but also the need for resources to be tightly linked to activities in order to\nmake essential extra knowledge readily available within self-contained courses (Chanier,\n1996). The fact that learners did use these extra resources suggests that significant time and\nenergy should be allocated to their development in such hypermedia environments.\n\u2022 Software can be self-contained, but learners will still be looking for discussion and\nfeedback with experts. It is still an open question whether these experts should be\nteachers acting as guides or counsellors, or native speakers.\n\u2022 Self-access has been, as far as possible, the rule. Learners have made it a basic\nrequirement. Insufficient provision of equipment and flexible access time within\ninstitutions may jeopardize the whole learning procedure.\n65\nThierry Charter Evaluation in a project life-cycle: the hypermedia CAMILLE project\n\u2022 Autonomous learning situations have been explored only in training institutions.\nLearners said they were willing to work alone, and to work at home. We have yet to\ninvestigate how this might affect learning outcomes. Experimenting with access at\nwork is yet another possible approach that should be considered.\nNot surprisingly, the types of learners with whom we were concerned reacted very\npositively. They appeared to master the essential three domains for managing one's own\nlearning (Holec, 1990): methodological aspects, linguistic aspects and cultural\nbackground. We have collected no data for generalizing these outcomes to other types of\nlearners. The experiment undertaken in Teesside with true second-language beginners,\nlacking the self-assurance and motivation, was not conclusive. Blin (in press) has also\nremarked that an insufficient level of confidence in using computers for language-learning\npurposes (which never appeared to be a problem with our experienced learners) may\nrepresent a major element in the learner's decisions to under-use computers as opposed to\nother materials in self-access centres.\nEffectiveness and language skills\nIt is now time to come back to the question of the effectiveness of such hypermedia\nsoftware with respect to the four language skills. As pointed out above, the technology we\nrelied on is more adequate for practising aural (listening) and written comprehension than\naural and written productions. Learners passed two summative tests, as described earlier,\nin HEBO and in the CAVILAM. The former test was completely based on aural skills\nand thus included aural production. The latter test encompassed aural comprehension,\nwritten comprehension and written production.\nIn order to appreciate these results correctly, it should be remembered that the\nevaluations involved small samples, related to specific types of learners, and in both\nplaces quality of results was not much better than that of more traditional approaches,\nlive courses or audio-cassette methods. This quality is satisfactory because we were not\nexpecting computing-learning environments to be much more efficient, but to represent\nan effective alternative which can be taken into account in autonomous learning\nsituations, an alternative which possesses other advantages discussed in the previous\nsections. Another open question is whether or not our results can be generalized to all\nexperienced learners.\nEvaluation as part of the development process\nThe main goal of formative evaluation is to measure the performance of the courseware.\nIt is a necessity for adapting the software, for debugging it, and for collecting essential\ninformation on timing etc., information which can then also help in preparing the user-\nmanual delivered with the software. The procedure must involve real learners belonging\nto the target audience, and should be set up long before the end of the development. The\nelapsed time between the final release of the software and the evaluation phase is often as\nlong as the duration of the development of the first version of the software which served\nin the evaluation. In general, a reduced protocol is sufficient, but if research questions are\nat stake, an extended protocol is necessary for setting up case studies. The whole\nevaluation procedure then becomes much more complex.\nThe purpose of summative evaluation is to measure the effectiveness of the courseware in\nterms of its stated aims. We have pointed out several caveats: the summative evaluation is\n66\nALT-] Volume 4 Number 3\ntime-consuming; it requires adequate means for achieving it; many partners are often\ninvolved; and its results or its abandonment may be used against the project. Since it\nrepresents a real risk for the whole project, the first question which should be answered before\nmaking any decision is: who are the audience? Who really wants to know the outcomes?\nNevertheless, the organization of summative evaluations by project teams should happen\nmore frequently. They are important for the research and pedagogical communities for\ndeontological reasons, as follows.\n\u2022 They help to clarify the functional differences between the various sorts of software reports\nand the evaluation reports. For most software, the only accessible reports are commercial\nreports, written by publishers, or software reviews, written by external teachers or\nresearchers. These reports may bring useful information, but they have the disadvantage of\noften being labelled as 'evaluations'. Confusion with reports based on experiments\ninvolving real learners and following a methodological procedure should be avoided.\n\u2022 They minimize over-generalizations, either pro or con. An evaluation has specific aims.\nResults can be interpreted only with respect to the restricted parameters which have been\ntested. Unfortunately, papers are still published which either present evaluations as being\naimed at definitely stating the superiority of CAL over other learning methods, without\ndefining parameters such as types of learners, types of skills, levels of proficiency, nor\nactual learning situations, or which, when they make explicit their restricted purposes, do\nnot incorporate any detailed information. It is then impossible for readers correctly to\ninterpret their results, and to undertake other evaluations in order to verify them.\n\u2022 They reinforce the idea that, in an evaluation, not only the software may be tested, but\nalso the learning situations. A limited piece of software can be very useful, and on the\ncontrary, a wonderful language package can be misused, depending on its integration\ninto the curriculum, its access conditions, hardware configurations, etc.\n\u2022 They may offer instruments for measuring various aspects of so-called communicative\ncompetence. Such references support the dialogue between designers and the Second\nLanguage Acquisition community.\nPotentially, a summative evaluation is also of direct interest for the project team itself. It\nrepresents an efficient way of clarifying the final aims of the software, and of estimating\nthe inevitable shift between the initial hypotheses and the reality of the achievements.\nMeasuring tools make it possible to elicit how and on what grounds designers want their\ninnovation to be estimated. Since evaluation is a cumulative process, it forms a starting\npoint from which other researchers are able to set up new experiments in order to extend\nthe initial measures. Tests can also be adequately joined to the software delivery in order\nto let learners evaluate themselves at the end of their training period.\nNote\n1 CAMILLE (which stands for Computer-Aided Multimedia Interactive Language\nLearning Environment) has partly been financed by the European LINGUA Programme.\nMembers of the CAMILLE Consortium are: The University of Teesside; Universite\nBlaise Pascal and Universite d'Auvergne, Clermont-Ferrand; De Haagse Hogeschool,\nThe Hague; and Universidad Politecnica, Valencia.\n67\nThierry Chanier Evaluation in a project life-cycle the hypermedia CAMILLE project\nAcknowledgements\nI would like to thank all the teachers and researchers who participated in the CAMILLE\nevaluations, particularly Ana Gimeno Sanz in Universidad Polit\u00e9cnica de Valencia, Jan\nBrouwer in De Haagse Hogeschool, Janina Emery, Chris Emery and Bruce Ingraham in\nthe University of Teesside, the CAVILAM staff in Vichy, Maguy Pothier, Paul Lotin and\nJ\u00e9r\u00f4me Oilier in the Universit\u00e9 of Clermont II.\nReferences\nBlin, F. (in press), 'Integrating CALL in the negotiated learner-centred curriculum: a case\nstudy', Eurocall '95 Conference, Valencia, Spain, September, 1995.\nChanier, T. (1996), 'Learning a second language for specific purposes within a\nhypermedia framework', Computer-Assisted Language Learning, 9 (1), 3-43.\nde Landsheere, V. and G. (1984), D\u00e9finir les objectifs de l'\u00e9ducation, Paris: PUF.\nDemaizi\u00e8re, F. and Dubuisson, C. (1992), De l'EAO aux NTF: utiliser l'ordinateur pour la\nformation, Paris: Ophrys.\nEllis, R. (1994), The Study of Second Language Acquisition, Oxford: OUP.\nEmery, C, Ingraham, B., Chanier, T. and Brouwer, J. (in press), 'Creating interactive\nmultimedia CALLware: the CAMILLE experience', Eurocall '95 conference, Valencia,\nSpain, September, 1995.\nEhrman, M. and Oxford, R. (1990), 'Adult language-learning styles and strategies in an\nintensive training setting', Modern Language Journal, 74 (3), 311-27.\nFeldman, R. S. and Rim\u00e9, B. (1991), Fundamentals of Non-verbal Behaviour, Cambridge\nand Paris: CUP and Editions de la Maison des Sciences de l'Homme.\nHolec, H. (1991), 'Autonomie et apprentissage auto-dirig\u00e9: quelques sujets de reflexion',\nin Les Auto-apprentissages, Actes des 6\u00e8mes Rencontres de l'ASDIFLE, Paris: Les Cahiers\nde l'ASDIFLE, 2, pp. 23-33.\nIngraham, B., Chanier, T. and Emery, C. (1994), 'CAMILLE: a European project to\ndevelop language training for different purposes, in various languages on a common\nhypermedia framework', Computers and Education, 23 (1\/2), 107-15.\nKnussen, C , Tanner, G. R. and Kibby, M. R. (1991), 'An approach to the evaluation of\nhypermedia', Computers and Education, 17 (1), 13-24.\nLussier, D. (1992), Valuer les apprentisages dans une approche communicative, Paris: Hachette.\nMark, M. A. and Greer, J. E. (1993). 'Evaluation methodologies for intelligent tutoring\nsystems', Journal of Artificial Intelligence in Education, 4 (2\/3), 129-53.\nNunan, D. (1992), Research Methods in Language Learning, Cambridge: CUP.\nO'Malley, J. M. and Chamot, A. U. (1990), Learning Strategies in Second Language\nAcquisition, Cambridge: CUP.\nPothier, M. (in press), 'Travailler en France: un environnement informatique hypermedia\npour l'auto-apprentissage sur objectifs sp\u00e9cifiques', Revue de Phon\u00e9tique Appliqu\u00e9e.\n68\n"}