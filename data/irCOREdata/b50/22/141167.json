{"doi":"10.1016\/j.patcog.2008.04.001","coreId":"141167","oai":"oai:dspace.lib.cranfield.ac.uk:1826\/6868","identifiers":["oai:dspace.lib.cranfield.ac.uk:1826\/6868","10.1016\/j.patcog.2008.04.001"],"title":"General support vector representation machine for one-class classification of\nnon-stationary classes","authors":["Camci, Fatih","Chinnam, R. B."],"enrichments":{"references":[{"id":37929376,"title":"A bootstrap-like rejection mechanism for multilayer perceptron networks\u201d, II Simposio Brasileiro de Redes Neurais, S\u00e3o Carlos-SP,","authors":[],"date":null,"doi":null,"raw":"G.C. Vasconcelos, \u201cA bootstrap-like rejection mechanism for multilayer perceptron networks\u201d, II Simposio Brasileiro de Redes Neurais, S\u00e3o Carlos-SP, Brazil, pp. 167-172, 1995.34","cites":null},{"id":37929383,"title":"A kernel-distance-based multivariate control chart using support vector methods.","authors":[],"date":"2003","doi":"10.1080\/1352816031000075224","raw":"R. Sun, and F. Tsung, A kernel-distance-based multivariate control chart using support vector methods. International Journal of Production Research, 2003, 41, 2975-2989.","cites":null},{"id":37929374,"title":"An Approach To Novelty Detection Applied To The Classification Of Image Regions,","authors":[],"date":"2004","doi":"10.1109\/tkde.2004.1269665","raw":"S. Singh and M. Markou, An Approach To Novelty Detection Applied To The Classification Of Image Regions, IEEE Transactions on Knowledge And Data Engineering, vol. 16, no.4, pp. 396-407, 2004","cites":null},{"id":37929388,"title":"An introduction to kernel-based learning algorithms\u201d,.","authors":[],"date":"2001","doi":"10.1109\/72.914517","raw":"K.-R. M\u00fcller, S. Mika, G. R\u00e4tsch, K. Tsuda, and B. Sch\u00f6lkopf. \u201cAn introduction to kernel-based learning algorithms\u201d,. IEEE Neural Networks, vol.12, no.2, pp.181-201, 2001.","cites":null},{"id":37929393,"title":"An introduction to support vector machines and other kernel-based learning methods,","authors":[],"date":"2000","doi":"10.1017\/cbo9780511801389","raw":"N. Cristianini and J. S. Taylor, An introduction to support vector machines and other kernel-based learning methods, Cambridge University Press, 2000, pp.122-125.","cites":null},{"id":37929368,"title":"Automated segmentation of multiple sclerosis lesions by model outlier detection\u201d,","authors":[],"date":"2001","doi":"10.1109\/42.938237","raw":"K. Van Leemput, F. Maes, D. Vandermeulen, A. Colchester, and P. Suetens, \u201cAutomated segmentation of multiple sclerosis lesions by model outlier detection\u201d, Medical Imaging IEEE Transactions, vol.20, no.8 pp.677-688, 2001.","cites":null},{"id":37929363,"title":"Automatic assessment of scintmammographic images using a novelty filter\u201d, in","authors":[],"date":"1995","doi":null,"raw":"M. Costa and L. Moura, \u201cAutomatic assessment of scintmammographic images using a novelty filter\u201d, in Proc. 19th Annual Symposium on Computer Applications in Medical Care, PA, 1995, pp. 537-541.","cites":null},{"id":37929396,"title":"Change Detection in Time Series Data Using Wavelet Footprints\u201d,","authors":[],"date":"2005","doi":"10.1007\/11535331_8","raw":"Sharifzadeh M., Azmoodeh F., and Shahabi C., \u201cChange Detection in Time Series Data Using Wavelet Footprints\u201d, Lecture Notes in Computer Science, Vol. 3633, 2005, pp.127-144.","cites":null},{"id":37929360,"title":"Choosing an appropriate model for novelty detection\u201d, in","authors":[],"date":"1997","doi":"10.1049\/cp:19970712","raw":"T. Nairac, C. Corbet, R. Ripley, N. Townsend, and L. Tarassenko, \u201cChoosing an appropriate model for novelty detection\u201d, in Proc. of 5th International Conference on Artificial Neural Networks, UK, 1997, pp.117-122.","cites":null},{"id":37929365,"title":"Detecting attacks on networks\u201d,","authors":[],"date":"1997","doi":"10.1109\/2.642762","raw":"C. Herringshaw, \u201cDetecting attacks on networks\u201d, Computer, vol.30, no.12, pp.16-17, 1997","cites":null},{"id":37929392,"title":"Development and benchmarking of multivariate statistical process control tools for a semiconductor etch process; improving robustness through model updating\u201d, in","authors":[],"date":"1997","doi":null,"raw":"V. B. Gallagher, R. M. Wise, S. W. Butler, D. D. White, and G. G Barna, \u201cDevelopment and benchmarking of multivariate statistical process control tools for a semiconductor etch process; improving robustness through model updating\u201d, in Proc. International Symposium on Advanced Control of Chemical Processes, Banff, Canada 1997, pp.149-161.","cites":null},{"id":37929395,"title":"Event Detection from Time Series Data\u201d, in","authors":[],"date":"1999","doi":"10.1145\/312129.312190","raw":"V. Guralnik and J. Srivastava, \u201cEvent Detection from Time Series Data\u201d, in Proc. of ACMSIGKDD International Conference Knowledge Discovery and Data Mining, 1999, pp. 33-42.","cites":null},{"id":37929372,"title":"Experimental validation of structural health monitoring methodology I: novelty detection on a laboratory structure\u201d,","authors":[],"date":"2003","doi":"10.1006\/jsvi.2002.5168","raw":"K. Worden, G. Manson, and D. J. Allman, \u201cExperimental validation of structural health monitoring methodology I: novelty detection on a laboratory structure\u201d, Journal of Sound and Vibration, vol.259, no.2, pp.323-343, 2003.","cites":null},{"id":37929367,"title":"Experimental validation of structural health monitoring methodology\u201d,","authors":[],"date":"2003","doi":"10.1006\/jsvi.2002.5168","raw":"K. Worden and G. Manson, \u201cExperimental validation of structural health monitoring methodology\u201d, Journal of Sound Vibration, vol.259, no.2, pp.345-363, 2003.","cites":null},{"id":37929387,"title":"Knowledge Discovery and Data Mining - The Info-Fuzzy Network (IFN)","authors":[],"date":"2000","doi":"10.1007\/978-1-4757-3296-2","raw":"O. Maimon and M. Last, Knowledge Discovery and Data Mining - The Info-Fuzzy Network (IFN) Methodology, Kluwer Academic Publishers, December 2000.","cites":null},{"id":37929385,"title":"Learning in the Presence of Concept Drift and Hidden Contexts\u201d,","authors":[],"date":"1996","doi":"10.1007\/bf00116900","raw":"G. Widmer and M. Kubat, \u201cLearning in the Presence of Concept Drift and Hidden Contexts\u201d, Machine Learning, Vol. 23, No. 1, pp. 69-101, 1996.","cites":null},{"id":37929373,"title":"Milne \u201cOnline unsupervised outlier detection using finite mixtures with discounting learning algorithms\u201d,","authors":[],"date":"2004","doi":"10.1023\/b:dami.0000023676.72185.7c","raw":"K. Yamanishi, J. Takeuchi, G. Williams,, P. Milne \u201cOnline unsupervised outlier detection using finite mixtures with discounting learning algorithms\u201d, Data Mining and Knowledge Discovery, vol. 8, pp. 275\u2013300, 2004","cites":null},{"id":37929362,"title":"Novelty detection for the identification of masses in mammograms\u201d, in","authors":[],"date":"1995","doi":"10.1049\/cp:19950597","raw":"L. Tarassenko, P. Hayton, N. Cerneaz, and M. Brady, \u201cNovelty detection for the identification of masses in mammograms\u201d, in Proc. 4th International Conference on Artificial Neural Networks, London, UK, 1995, pp. 442-447.","cites":null},{"id":37929377,"title":"Novelty detection: a review-part 1: neural network based approaches,","authors":[],"date":"2003","doi":"10.1016\/j.sigpro.2003.07.018","raw":"Markou M. and Singh S., Novelty detection: a review-part 1: neural network based approaches, Signal Processing, Vol. 83, No. 12,:pp. 2499 - 2521, 2003","cites":null},{"id":37929371,"title":"On the Choice of Smoothing Parameters for Parzen Estimators of Probability Density Functions\u201d.","authors":[],"date":"1976","doi":"10.1109\/tc.1976.1674577","raw":"P. W. Duin: \u201cOn the Choice of Smoothing Parameters for Parzen Estimators of Probability Density Functions\u201d. IEEE Trans. Computers, vol.25, no.11, pp.1175-1179, 1976.","cites":null},{"id":37929375,"title":"One Class Classification\u201d,","authors":[],"date":"2001","doi":"10.1109\/icpr.2004.1334542","raw":"D. M. Tax, \u201cOne Class Classification\u201d, Ph.D. dissertation, Delft Technical University, 2001.","cites":null},{"id":37929386,"title":"Online Classification of Nonstationary Data Streams\u201d,","authors":[],"date":"2002","doi":null,"raw":"M. Last, \u201cOnline Classification of Nonstationary Data Streams\u201d, Intelligent Data Analysis, Vol. 6, No. 2, pp. 129- 147, 2002.","cites":null},{"id":37929378,"title":"Online novelty detection on temporal sequences\u201d, in","authors":[],"date":"2003","doi":"10.1145\/956750.956828","raw":"J. Ma and S. Perkins, \u201cOnline novelty detection on temporal sequences\u201d, in Proc. of International Conference on Knowledge Discovery and Data Mining, Washington DC, 2003, pp. 417-423.","cites":null},{"id":37929390,"title":"Recursive PCA for adaptive process monitoring\u201d,","authors":[],"date":"2000","doi":"10.1016\/s0959-1524(00)00022-6","raw":"W. Li, H. Yue, S. Valle-Cervantes, and J. Qin, \u201cRecursive PCA for adaptive process monitoring\u201d, Journal of Process Control , vol.10, no.5, pp.471-486, 2000.","cites":null},{"id":37929370,"title":"Small Sample Size Effects in Statistical Pattern Recognition: Recommendations for Practitioners\u201d, Pattern Analysis and Machine Intelligence,","authors":[],"date":"1991","doi":"10.1109\/34.75512","raw":"S. J. Raudys and A. K. Jain, \u201cSmall Sample Size Effects in Statistical Pattern Recognition: Recommendations for Practitioners\u201d, Pattern Analysis and Machine Intelligence, IEEE Transactions, vol.13, no.3, pp.252-264, 1991.","cites":null},{"id":37929389,"title":"Statistical learning theory,","authors":[],"date":"1998","doi":"10.1007\/978-1-4757-3264-1","raw":"V. Vapnik, Statistical learning theory, Wiley, 1998, pp.401-440.","cites":null},{"id":37929358,"title":"Support vector domain description\u201d,","authors":[],"date":"1999","doi":"10.1016\/s0167-8655(99)00087-2","raw":"D. M. Tax and R. Duin, \u201cSupport vector domain description\u201d, Pattern Recognition Letters, vol.20, no.11-13, pp 1191-1199, 1999.","cites":null},{"id":37929382,"title":"Support vector machines for class representation and discrimination\u201d,","authors":[],"date":"2003","doi":"10.1109\/ijcnn.2003.1223940","raw":"C. Yuan and D. Casanent, \u201cSupport vector machines for class representation and discrimination\u201d, International Joint Conference on Neural Networks, Portland, 2003 pp. 1610-1615.","cites":null},{"id":37929379,"title":"SV Estimation of a Distribution\u2019s Support\u201d, in","authors":[],"date":"1999","doi":null,"raw":"B. Sch\u00f6lkopf, R. Williamson, A. Smola, and J. S. Taylor, \u201cSV Estimation of a Distribution\u2019s Support\u201d, in Proc. NIPS\u201999, 1999.","cites":null},{"id":37929394,"title":"Unifying Framework for Detecting Outliers and Change Points from Non-Stationary Time Series Data\u201d,","authors":[],"date":"2006","doi":"10.1145\/775047.775148","raw":"K. Yamanishi and J. Takeuchi \u201cUnifying Framework for Detecting Outliers and Change Points from Non-Stationary Time Series Data\u201d, IEEE Transact\u0131ons on Knowledge and Data Eng\u0131neer\u0131ng, Vol. 18, No. 4, April 2006.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2008-10-31T00:00:00Z","abstract":"Novelty detection, also referred to as one-class classification, is the process\nof detecting 'abnormal' behavior in a system by learning the 'normal' behavior.\nNovelty detection has been of particular interest to researchers in domains\nwhere it is difficult or expensive to find examples of abnormal behavior (such\nas in medical\/equipment diagnosis and IT network surveillance). Effective\nrepresentation of normal data is of primary interest in pursuing one-class\nclassification. While the literature offers several methods for one-class\nclassification, very few methods can support representation of non-stationary\nclasses without making stringent assumptions about the class distribution. This\npaper proposes a one-class classification method for non-stationary classes\nusing a modified support vector machine and an efficient online version for\nreducing computational time. The presented method is applied to several\nsimulated datasets and actual data from a drilling machine. In addition, we\npresent comparison results with other methods that demonstrate its superior\nperformance. (C) 2008 Elsevier Ltd. All rights reserved","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/141167.pdf","fullTextIdentifier":"http:\/\/dx.doi.org\/10.1016\/j.patcog.2008.04.001","pdfHashValue":"46b645f39318968085c79c101782636c6bfe8bb8","publisher":"Elsevier Science B.V., Amsterdam.","rawRecordXml":"<record><header><identifier>\noai:dspace.lib.cranfield.ac.uk:1826\/6868<\/identifier><datestamp>2012-02-03T11:17:51Z<\/datestamp><setSpec>hdl_1826_24<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>General support vector representation machine for one-class classification of\nnon-stationary classes<\/dc:title><dc:creator>Camci, Fatih<\/dc:creator><dc:creator>Chinnam, R. B.<\/dc:creator><dc:subject>Novelty detection<\/dc:subject><dc:subject>One-class classification<\/dc:subject><dc:subject>Support vector machine<\/dc:subject><dc:subject>Non-stationary classes<\/dc:subject><dc:subject>Non-stationary processes<\/dc:subject><dc:subject>Online training<\/dc:subject><dc:subject>Outlier detection<\/dc:subject><dc:description>Novelty detection, also referred to as one-class classification, is the process\nof detecting 'abnormal' behavior in a system by learning the 'normal' behavior.\nNovelty detection has been of particular interest to researchers in domains\nwhere it is difficult or expensive to find examples of abnormal behavior (such\nas in medical\/equipment diagnosis and IT network surveillance). Effective\nrepresentation of normal data is of primary interest in pursuing one-class\nclassification. While the literature offers several methods for one-class\nclassification, very few methods can support representation of non-stationary\nclasses without making stringent assumptions about the class distribution. This\npaper proposes a one-class classification method for non-stationary classes\nusing a modified support vector machine and an efficient online version for\nreducing computational time. The presented method is applied to several\nsimulated datasets and actual data from a drilling machine. In addition, we\npresent comparison results with other methods that demonstrate its superior\nperformance. (C) 2008 Elsevier Ltd. All rights reserved.<\/dc:description><dc:publisher>Elsevier Science B.V., Amsterdam.<\/dc:publisher><dc:date>2012-01-23T23:00:57Z<\/dc:date><dc:date>2012-01-23T23:00:57Z<\/dc:date><dc:date>2008-10-31T00:00:00Z<\/dc:date><dc:type>Article<\/dc:type><dc:identifier>Fatih Camcia, Ratna Babu Chinnam, General support vector representation machine for one-class classification of\nnon-stationary classes, Pattern Recognition, Volume 41, Issue 10, October 2008, Pages 3021\u20133034.<\/dc:identifier><dc:identifier>0031-3203<\/dc:identifier><dc:identifier>http:\/\/dx.doi.org\/10.1016\/j.patcog.2008.04.001<\/dc:identifier><dc:identifier>http:\/\/dspace.lib.cranfield.ac.uk\/handle\/1826\/6868<\/dc:identifier><dc:language>en_UK<\/dc:language><dc:rights>NOTICE: this is the author\u2019s version of a work that was accepted for publication in Pattern Recognition. Changes resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms may not be reflected in this document. Changes may have been made to this work since it was submitted for publication. A definitive version was subsequently published in Pattern Recognition, VOL 41, ISSUE 10, (2008) DOI:10.1016\/j.patcog.2008.04.001<\/dc:rights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["0031-3203","issn:0031-3203"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2008,"topics":["Novelty detection","One-class classification","Support vector machine","Non-stationary classes","Non-stationary processes","Online training","Outlier detection"],"subject":["Article"],"fullText":"1Pattern Recognition, Volume 41, Issue 10, October 2008, Pages 3021-3034\nGeneral Support Vector Representation Machine\nfor One-Class Classification of Non-Stationary Classes\nFatih Camci, Assistant Professor\nDepartment of Computer Engineering\nFatih University\nBuyukcekmece Istanbul, 34500 Turkey\nRatna Babu Chinnam, Associate Professor*\nDepartment of Industrial & Manufacturing Engineering\nWayne State University\n4815 Fourth Street, Detroit, MI 48202, USA\nAbstract\u2014Novelty detection, also referred to as one-class classification, is the process of detecting\n\u2018abnormal\u2019 behavior in a system by learning the \u2018normal\u2019 behavior. Novelty detection has been of\nparticular interest to researchers in domains where it is difficult or expensive to find examples of\nabnormal behavior (such as in medical\/equipment diagnosis and IT network surveillance). Effective\nrepresentation of normal data is of primary interest in pursuing one-class classification. While the\nliterature offers several methods for one-class classification, very few methods can support\nrepresentation of non-stationary classes without making stringent assumptions about the class\ndistribution. This paper proposes a one-class classification method for non-stationary classes using a\nmodified Support Vector Machine and an efficient on-line version for reducing computational time. The\npresented method is applied to several simulated datasets and actual data from a drilling machine. In\naddition, we present comparison results with other methods that demonstrate its superior performance.\nIndex Terms\u2014Novelty Detection, One-class Classification, Support Vector Machine, Non-stationary\nClasses, Non-stationary processes, On-line Training, Outlier Detection\n2* Corresponding Author: Tel: +313-577 4846; Fax: +313-578-5902; E-mail: r_chinnam@wayne.edu\n3List of Symbols:\nr : Radius of a hyper-sphere\nc : Center of a hyper-sphere\nix : Training data point i\nC : Penalty for misclassification\ni\uf078 : Distance of misclassified data point i from the boundary of the sphere\n,\u03b1 \u03b3 : Lagrange multipliers\n( , )i iK x x : Kernel density\n( )\uf06a x : Nonlinear transformation from the input space to the feature space\nm : Dimensionality of the feature space\n2\n\uf073 : Variance\ndn : Average nearest neighbor distance\n\uf077i : Importance (weight) of data point i\n\uf044 : Forgetting factor (0 1)\uf0a3 \uf03c\uf044\nct : Time of collection of the most recent data point\ncm : Measure of support vector closeness to the boundary\nid : Distance between data point i and the closest support vector\nD : Maximum distance between any data point and its closest support vector\nv : Vector of data points in the boundary list\nF\n\uf0b6\n: Overall fitness value of a given Gaussian kernel \uf073 parameter\n1,2\uf06c : Weighting factors for age and closeness measures, respectively\nit : Time of collection of the\nthi data point\nSVN : Number of support vectors\nI. INTRODUCTION\nDistinguishing one object class from others is the main task of most classification systems. However,\nthere are occasions when the chief task of the classifier is to distinguish the object class from the non-\nobject class, leading to the so called novelty detection or one-class classification [1]. More precisely,\nnovelty detection is the process of learning the normality of a system by fitting a model to the set of\nnormal examples and labeling unseen data as normal or abnormal according to its novelty score [2].\nWhile there are several reasons why this problem arises, the most important reason is the general\ndifficulty (attributed to lack of resources or time or cost) or even impossibility of collecting enough\nexamples for the different abnormal classes to facilitate adequate representation. For example, in\n4medical diagnostics, it is common to employ one-class classifiers to differentiate healthy patients from\nnon-healthy patients (for example through mammograms for breast-cancer detection) and then resort to\nexperienced diagnosticians and physicians to identify the particular condition [3, 4]. Another example\ncomes from the domain of equipment condition monitoring (for the purpose of optimal maintenance). It\nis common practice to only characterize behavior of healthy equipment and raise an alarm when novel\nbehaviors are detected. The prevalence of this practice is often attributed to difficulty in collecting\nrepresentative examples of certain equipment failure modes (for example in rotary equipment) and out-\nof-control states (for example in semiconductor equipment). Other examples include network\nsurveillance and computer security [5]. In summary, the essential difference between conventional\nclassification and one-class classification is that availability of data for all classes is not necessary in the\nlatter case. While conventional classification methods perform well for cases where there are enough\nexamples for all classes, they are ineffective for those cases where data are unavailable for some classes.\nThis article proposes a one-class representation method using a modified support vector machine for\nrepresentation of stationary as well as non-stationary classes. We give the previous work in section 2,\ndiscuss General Support Vector Representation Machine (GSVRM) in section 3, on-line training in\nsection 4, results from experiments in section 5, and finally conclude in section 6.\nII. PREVIOUS WORK\nThe majority of approaches for novelty detection can be roughly grouped into two categories: statistical\nmethods and computational intelligence based methods. Statistical methods can be parametric or non-\nparametric. Parametric methods often involve estimation of the probability density of the assumed\ndistribution, and in turn calculation of outlier probability for novelty detection [6, 7, 8]. The\neffectiveness of this method is limited by the degree to which the assumption regarding the distribution is\nsatisfied and is generally considered unsuitable for many real-world applications. Most popular non-\nparametric methods are either based on k-nearest neighbor logic or kernel density estimation (such as\nParzen windows and Gaussian mixture models) [9, 10]. In Parzen windows, a Gaussian kernel is used for\n5each pattern, whereas fewer kernels are needed for building Gaussian Mixture Models (GMM). The main\nlimitation of these methods is the computational inefficiency. In addition, these approaches cannot take\nadvantage of possible existing examples from abnormal classes.\nMajority of computational intelligence based novelty detection methods include auto-associators, multi-\nlayer perceptrons (MLP), self organizing map (SOM), and support vector machines (SVM). In employing\nauto-associators, basically a feed-forward neural network, a threshold for the output error is set in order\nto identify the abnormality [8]. SOM is an unsupervised learning technique and based on the idea of\nneuron competition for a given input pattern (i.e., competitive learning). In performing novelty detection,\nSOM is first learned utilizing normal data, and then, some form of a distance between winner neuron of a\ngiven input pattern and neurons that represent normal data is used as novelty score [10,11]. Multilayer\nperceptron (MLP) neural networks have also been employed for novelty detection. They label a pattern\nas novel if all the output neurons give low confidence on being the winner. Setting the threshold for\nconfidence might be easier by modifying the output of the neuron by using a softmax function [12] or by\ntraining the neural network to reduce the mutual information [13]. In the latter case, the output\ndistribution is mapped to a Gaussian distribution with a circular decision boundary, which is easy to\nthreshold for novelty detection. Bootstrap type rejection mechanism is also used for novelty detection in\nMLP [14, 15]. This method is based on the idea of training the network with negative examples (i.e.,\nnovel events) rather than normal data. Several support vector machine based methods have been\nproposed in the literature for addressing the task of novelty detection. Ma and Perkins proposed Support\nVector Regression for one-class classification [16]. Generally, the assumption of independent\noccurrence of novel events, made by these methods, does not hold in real-world datasets. Sch\u00f6lkopf et\nal. [17] proposed the v-Support Vector Classifier that places a hyper-plane in the transformed space such\nthat it separates the normal dataset from the origin with maximum margin. David Tax proposed a similar\nmethod that creates a closed boundary around the data instead of a hyper-plane and labeled it Support\nVector Data Description (SVDD) [13]. The method proposed in this paper is inspired from SVDD, hence\nit will be discussed in detail in the following section. Yuan and Casasent proposed Support Vector\n6Representation Machine (SVRM), a method somewhat similar to SVDD, that offers an effective\nprocedure for estimating the \uf073 parameter of the Gaussian kernel [18]. Kernel-distance-based novelty\ndetection methods have been developed using SVDD and RSVM [19]. Nearest Neighbor Data Descriptor\n(NNDD) is another proposed method that rejects the data by comparing the distances between nearest\nneighbors [13].\nCertainly, most of the novelty detection methods in the literature, including the ones mentioned above,\nassume that the class is stationary1. However, the real-world data are usually collected over a period and\nstatistical properties of the data may change within the time the data is collected. SmartSifter [11] is a\nmethod developed for novelty detection of non-stationary data. A histogram density with a number of\ncells is used to represent a probability density over the domain of categorical variables, which are\nhierarchically structured. A finite mixture model within each cell is then used to represent the probability\ndensity over the domain of continuous variables. SmartSifter gradually discounts the effects of past\nexamples. A score for each input is assigned as an indication of novelty, in which high score indicates a\nnovel event. However, SmartSifter employs a probabilistic model for representation of data, which limits\nits implementation in real-world settings. Smartsifter is extended to two methods named ChangeFinder\n(CF) and SC in [28] in order to deal with time-series data and detect change points. These methods are\ncompared with GS method, which is an event detection method for time-series data presented in [29].\nOur method, GSVRM, will be compared with these methods in the experiments section (section 5). Note\nthat change point detection in time-series and novelty detection in non-stationary data are not exactly\nsame problems; even though there exists significant similarity. Change points are time positions in the\ntime-series data where the \u201clocal trend\u201d has changed (i.e., points of discontinuities between adjacent\nsegments) [30]. In general, the expectation in dealing with change point detection problems is that the\nunderlying process is stationary, and the task is to detect the point of discontinuity or change. On the\ncontrary, our proposed method allows the underlying process to be non-stationary while aiming to detect\nchanges in the non-stationary process. Thus, we can apply our proposed novelty detection method to\n7\u2018change point detection\u2019 problems, however, change point detection methods in time-series may not be\napplicable to novelty detection problems in non-stationary data.\nThere exist some classification methods that handle non-stationary data such as FLORA [20], decision\ntree based methods (e.g., CART, ID3, C4.5) [21], info-fuzzy network [22]. However, these methods aim\ntwo or multi-class classification or clustering and are not appropriate to implement for novelty detection,\nwhich is essentially a one-class classification problem.\nThe proposed GSVRM methods are explicitly developed in order to detect novel events from non-\nstationary as well as stationary data.\nSupport Vector Domain Description (SVDD)\nSupport Vector Domain Description (SVDD) [1] identifies a sphere with minimum volume that captures\nthe given normal data set. The sphere volume is characterized with its center c and radius r.\nMinimization of the volume is achieved by minimizing r2, which represents structural error [23]:\nMin 2r (1)\nSubject to: 2 2i r i\uf02d \uf0a3 \uf022x c , ix : thi data point (2)\nThe above equations do not allow any data to fall outside of the sphere. In order to make provision\nwithin the model for potential outliers within the training set, a penalty cost function is introduced as\nfollows (for data that lie outside of the sphere):\nMin 2 i\ni\nr C \uf078\uf02b \uf0e5 (3)\nSubject to: 2 2i ir \uf078\uf02d \uf0a3 \uf02bx c , 0i\uf078 \uf0b3 i\uf022 (4)\nwhere C is the coefficient of penalty for each outlier (also referred to as the regularization parameter)\nand i\uf078 is the distance between the thi data point and the hyper-sphere. This is a quadratic optimization\nproblem and can be solved efficiently by introducing Lagrange multipliers for constraints [24].\nLagrangian formulation of the problem also gives the advantage that training data appear in the form of\ndot products between vectors [23]:\n1 Stationarity of a data set guarantees time invariant statistical properties, such as mean, variance, or spectrum, whereas\n8\uf07b \uf07d\n2\n2\n( , , , , )\n( 2 )\ni\ni\ni i i i i i i\ni i\nL r r C\nr\n\uf078\n\uf061 \uf078 \uf067 \uf078\n\uf03d \uf02b \uf02d\n\uf02b \uf02d \uf0d7 \uf02d \uf0d7 \uf02b \uf0d7 \uf02d\n\uf0e5\n\uf0e5 \uf0e5\nc \u03be \u03b1 \u03b3\nx x c x c c\n(5)\nwhere i\uf067 and i\uf061 are Lagrange multipliers, 0i\uf067 \uf0b3 , 0i\uf061 \uf0b3 , and i i\uf0d7x x is inner product of ix and ix . Note\nthat for each training data point ix , a corresponding i\uf061 and i\uf067 are defined. L has to be maximized with\nrespect to r , c , and \u03be , and maximized with respect to \u03b1 and \u03b3 .\nTaking the derivatives of (5) with respect to r , c , \u03be , and equating them to zero, we obtain the following\nconstraints:\ni i\ni\n\uf061\uf03d\uf0e5c x (6)\n0i iC \uf061 \uf067\uf02d \uf02d \uf03d i\uf022 (7)\n1i\ni\n\uf061 \uf03d\uf0e5 (8)\nGiven that 0i\uf067 \uf0b3 , 0i\uf061 \uf0b3 , constraint (7) can be rewritten as:\n0 i C\uf061\uf0a3 \uf0a3 i\uf022 (9)\nThe following quadratic programming equations can be obtained by substituting (6), (7), (8), and (9) in\n(5).\nMax\n,\n( ) ( )i i i i j i j\ni i j\n\uf061 \uf061 \uf061\uf0d7 \uf02d \uf0d7\uf0e5 \uf0e5x x x x (10)\nSubject to: 0 i C\uf061\uf0a3 \uf0a3 i\uf022 (11)\n1i\ni\n\uf061 \uf03d\uf0e5 (12)\nStandard algorithms exist for solving this problem. The above Lagrange formulation also allows further\ninterpretation of the values of \u03b1 . If necessary, the Lagrange multipliers ( i\uf061 , i\uf067 ) will take a value of zero\nin order to make the corresponding constraint term zero in (5). Thus, formulation of GSVRM satisfies\nthe Karush-Kuhn-Tucker (KKT) conditions for achieving a global optimal solution. Noting that\ni iC \uf061 \uf067\uf03d \uf02b , if one of the multipliers becomes zero, the other takes on a value of C . When a data point ix\nstatistical properties of a non-stationary dataset change in time\n9is inside the sphere, the corresponding i\uf061 will be equal to zero. If it is outside of the sphere, i.e. 0i\uf078 \uf03e ,\ni\uf067 will be zero resulting in i\uf061 to be C . When the data point is at the boundary, i\uf061 and i\uf067 will be\nbetween zero and C to satisfy (8). The quadratic programming solution often yields a few data points\nwith a non-zero i\uf061 value, called support vectors. What is of particular interest is that, in general, support\nvectors can effectively represent the data while remaining sparse. In general, a sphere in the original\ninput space may not represent the dataset well enough. Hence, data ought to be transformed to a higher\ndimensional feature space where it can be effectively represented using a hyper-sphere. By employing\nthe so called kernel trick, one may use the inner-product kernel ( , )i jK x x to construct the optimal hyper-\nsphere in the higher dimensional feature space without having to consider the feature space itself (which\ncan be extremely large) in explicit form [24]. This kernel trick makes SVMs computationally efficient.\nThe inner-product kernel is a special case of the Mercer\u2019s theorem and is defined as follows:\n0\n( , ) ( ) ( )\n( ) ( )\nT T\ni j i\nm\nj j i\nj\nK\ni\uf06a \uf06a\n\uf03d\n\uf03d\n\uf03d \uf022\uf0e5\nx x x x\nx x\n\uf06a \uf06a\n(13)\nwhere 1{ ( )}mj j\uf06a \uf03dx denote a set of nonlinear transformations from the input space to the feature space and\nm is the dimensionality of the feature space. Thus, the dot product in (10) is replaced by a Kernel\nfunction, leading us once again to the following quadratic programming problem:\nMax\n,\n( , ) ( , )i i i i j i j\ni i j\nK K\uf061 \uf061 \uf061\uf02d\uf0e5 \uf0e5x x x x (14)\nSubject to 0 i C\uf061\uf0a3 \uf0a3 i\uf022 (15)\n1i\ni\n\uf061 \uf03d\uf0e5 (16)\nThis problem can be solved rather easily using well established quadratic programming algorithms, and\nwill lead to representation of \u201cnormal\u201d data. The assessment of whether a data point is inside or outside\nthe SVDD hyper-sphere is based on the sign of the following function:\n\uf028 \uf029 \uf028 \uf029 \uf028 \uf029 \uf028 \uf029\n\uf0f7\n\uf0f7\n\uf0f8\n\uf0f6\n\uf0e7\n\uf0e7\n\uf0e8\n\uf0e6\n\uf02d\uf02b\uf02d\uf03d \uf0e5\uf0e5\n\uf03d\uf03d\nxxKxxKxxKrsignxf\nm\ni\nii\nm\nji\njiji ,,2,\n11,\n2\n\uf061\uf061\uf061 (17)\n10\nPositive (negative) sign implies that the distance of the data point to the center of the hyper-sphere is less\n(greater) than the radius of the hyper-sphere. The radius of the hyper-sphere, r , can be calculated using\nany support vector and making the distance within sign function of (17) to 0.\nIII. GENERAL SUPPORT VECTOR REPRESENTATION MACHINE\nAs stated earlier in section 1, most data domain description methods assume a stationary process,\nincluding SVDD [1] and SVRM [18]. This may not be the case for many real-world applications. Some\nexamples of non-stationary processes include catalyst deactivation, equipment behavior with age, sensor\nand process drifting, and fault conditions [25, 26]. For example, Figure 1 illustrates the non-stationary\nvibratory behavior of a mechanical pump in the space of the two most dominant principal spectral\ncomponents. It should also be pointed out that while this pump shows evolution or trajectory in the\nprincipal spectral space, there were no mechanical faults, and hence, this represents \u201cnormal\u201d\nevolutionary behavior of the pump. Figure 2 shows contour plots of two most dominant principal\ncomponents of vibration sensor data collected from a pump using temporal, spectral, and energy domain\nfeatures. It is evident from the figures that the data does not follow any particular parametric\ndistribution. In this section, we will discuss a new one-class classification method called the General\nSupport Vector Representation Machine (GSVRM) inspired from SVRM [18] and SVDD [1] for\nrepresentation of stationary and non-stationary processes.\nFigure 1: Non-stationary vibratory behavior of a mechanical pump captured in the space of the two dominant\nprincipal spectral components of vibration sensor data collected at 12.5kHz for 0.5 sec at 4 hr intervals over a time\nspan of 3 months.\n500 1000\n0\n500\n1st Principal Component\n2n\nd\nP\nrin\nci\npa\nlC\nom\npo\nne\nnt\n\u2013 Recent\n\u2013 Old\n11\nFigure 2: Contour plots of two most dominant principal components of vibration sensor collected from a pump: a)\ntemporal domain, b) spectral domain, and c) energy domain.\nSimilar to SVDD, the proposed GSVRM is formulized to minimize the volume of the hyper-sphere that\ncaptures the normal data. To explicitly support the non-stationary nature of the data, we introduce the\nnotion of \u2018weight\u2019 or \u2018importance\u2019 of a data point in determining the boundary representation based on its\n\u2018age\u2019, i.e., how old the data point is. These weights are defined as follows:\n(1 ) c it ti i\uf077 \uf06c\n\uf02d\n\uf03d \uf02d \uf022 (18)\nwhere \uf06c is the forgetting factor (0 1)\uf06c\uf0a3 \uf03c , ct is time of the \u201clatest\u201d data point, and it is the time of data\npoint i . The GSVRM procedure incorporates these weights by modifying (3) as follows:\nMin 2 i i\ni\nr C \uf077 \uf078\uf02b \uf0e5 (19)\nSubject to: 2 2i ir \uf078\uf02d \uf0a3 \uf02bx c , 0i\uf078 \uf0b3 i\uf022 (20)\nThe dual of the Lagrangian results in the following:\nMax\n,\n( , ) ( , )i i i i j i j\ni i j\nK K\uf061 \uf061 \uf061\uf02d\uf0e5 \uf0e5x x x x (21)\nSubject to: 0 i iC\uf061 \uf077\uf0a3 \uf0a3 (22)\n1i\ni\n\uf061 \uf03d\uf0e5 (23)\na. 2 PCs \u2013 Temporal Domain\n-1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8\n-0.6\n-0.4\n-0.2\n0\n0.2\n0.4\n-1 -0.5 0 0.5 1\n-0.8\n-0.6\n-0.4\n-0.2\n0\n0.2\n0.4\n0.6\nKernel density contours\nb. 2 PCs \u2013 Spectral Domain\n-2.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2\n-2\n-1.5\n-1\n-0.5\n0\n0.5\n1\nc. 2 PCs \u2013 Energy Domain\n12\nGSVRM differs from SVDD in formulation and optimal \uf073 calculation. Each i\uf061 will be limited by a\ndifferent upper bound. Similar to the discussion in SVDD, \u03b1 is interpreted as follows: data point lies\ninside the GSVRM boundary if 0i\uf061 \uf03d ; outside the GSVRM boundary if i iC\uf061 \uf077\uf03d ; and on the boundary if\n0 i iC\uf061 \uf077\uf03c \uf03c . GSVRM also satisfies the Karush-Kuhn-Tucker (KKT) conditions.\nEven though different kernel functions may be used, Gaussian kernel has been shown to be one of the\nmost effective kernel functions for Support Vector Data Descriptor and Support Vector Representation\nMachine [1, 18]. Tax has reported results with several kernel functions but in the end has focused on the\nGaussian Kernel for one-class classification [1]. Given that the inspiration for our method comes from\nthese methods, we too employ the following Gaussian Kernel:\n2\n2( , ) exp\ni j\ni jK\n\uf073\n\uf0e6 \uf0f6\uf02d \uf02d\n\uf0e7 \uf0f7\uf03d\n\uf0e7 \uf0f7\uf0e7 \uf0f7\n\uf0e8 \uf0f8\nx x\nx x i j\uf022 \uf0b9 (24)\nSVDD utilizes the Gaussian kernel to transform the data to a higher dimensional space and finds the\nsmallest hyper-sphere that captures the data in this space. Proper assignment of parameter \uf073 is critical\nfor classification accuracy and could be provided by the user or optimized iteratively [24]. In SVDD, \uf073\nis calculated based on the user-given expected error rate. The ratio of number of support vectors to the\ntotal number of data points is defined as expected error rate. Different \uf073 values are empirically\nimplemented and the one that results close approximation to the number of expected support vectors is\nselected. SVRM on the contrary utilizes the Gaussian kernel to create a reference vector from the origin\nof the Gaussian hyper-sphere so that the inner-product of the reference vector and normal data vectors\nexceeds a threshold. SVRM also employs a different procedure for estimating \uf073 parameter of the\nkernel. It first locates the data points close to the class boundary by constructing local SVRMs for each\ndata point. Once the boundary points are identified (to form a boundary list), it tries different \uf073 values\nfor the global SVRM and chooses a value that results in good agreement between support vectors of the\nglobal SVRM and the points in the boundary list.\n13\nGSVRM strictly employs the Gaussian kernel. GSVRM also identifies the boundary points and evaluate\n\uf073 based on fitness function. The procedure is as follows:\n\u2212 Boundary List Identification\n\uf0b7 Calculate the average nearest neighbor distance, denoted by dn , between all the data points in the\ndata set.\n\uf0b7 For each data point, construct a local GSVRM utilizing data within a sphere of radius 2 dn\uf0b4 . In\nbuilding the local GSVRM, average distance of the data within the local sphere to their mean is\nemployed as \uf073 for the Gaussian kernel.\n\uf0b7 For each local GSVRM, construct an inner local GSVRM (hyper-sphere) by employing a radius\nsmaller than that suggested by the quadratic programming solution for the local GSVRM. The\nparameter that controls the reduction in radius (i.e., reduction %) is pre-specified by the user.\n\uf0b7 If the data point is rejected by the inner local GSVRM (meaning lies outside), it is added into the\nboundary list. Figure 3 illustrates this procedure for determination of boundary list. If there is a\nsingle data point within the local GSVRM hyper-sphere, it is not added to the boundary list for it\nmight be an outlier.2\n\u2212 Optimization of \uf073\n\uf0b7 Global GSVRMs (i.e., GSVRMs that represent all data) are constructed using different \uf073\nvalues, the range spanning from the smallest nearest neighbor distance ( min\uf073 ) to the largest\nnearest neighbor distance ( max\uf073 ) in the data set. The value that gives the \u201cbest fit\u201d is chosen to be\nthe optimal \uf073 based on the overall fitness ( F\n\uf0b6\n). The iteration is stopped when F\n\uf0b6\nis maximized\nor becomes 1, which means all the support vectors are in the boundary list and all data in the\nboundary list are support vectors. Overall fitness is calculated by combining age and closeness\nmeasures of all support vectors as follows:\n2 For computational savings, data points that lie within the boundary of any other data point\u2019s inner local GSVRM are not\nconsidered for entry into the boundary list. Experimental results reveal that this results in computational savings around 50%.\n14\n\uf028 \uf0291 2\n1\nSVN\ni i\ni\nSV\ncm\nF\nN\n\uf077\n\uf03d\n\uf0b6\n\uf02b\n\uf03d\n\uf0e5 \uf06c \uf06c\n(25)\nwhere, 1,2\uf06c denote weighting factors for age and closeness measures, respectively, and SVN the\nnumber of support vectors. F\n\uf0b6\napproaches to 1 as the representation gets better ( 0 1F\n\uf0b6\n\uf0a3 \uf0a3 ).\nFigure 3: Determination of class boundary: Data points on the boundary will be rejected by local inner GSVRMs.\nThe fitness function (i.e., F\n\uf0b6\n) is defined by two parameters: closeness ( cm ) and age ( i\uf077 ). Closeness\nassesses \u201ccloseness of support vectors to the boundary list\u201d and age assesses the \u201cimpact of age of the\nsupport vectors on overall solution\u201d. The closer the support vectors to the boundary and\/or the younger\nthe support vectors, the higher the fitness value. The two parameters are discussed in the following\nparagraphs.\nCloseness:\nIn the context of a global GSVRM, smaller \uf073 values yield more representing points (i.e., support\nvectors) and a tighter hyper-sphere, whereas larger values give fewer support vectors and result in a\nbigger hyper-sphere. The goal is to identify a value for \uf073 that results in good agreement between the\nsupport vector list of the global GSVRM and the boundary list resulting from local GSVRMs. In\ngeneral, smaller \uf073 values result in a global support vector list that is a \u201csuperset\u201d of the boundary list,\nwith some points that are not part of the boundary list. On the contrary, larger \uf073 values result in a global\nsupport vector list that is a \u201csubset\u201d of the boundary list. The key being to achieve agreement between\nBoundary Point\nInterior Point\nLocal GSVRM\nInner Local GSVRM\nLocal Sphere\nof radius 2x dn\n15\nthe two sets. In assessing this agreement, GSVRM computes the fitness of a \uf073 value by employing the\ncloseness parameter. Closeness, ( cm ) is in essence a function of the ratio of the distance from a support\nvector to the closest boundary list point and the maximum of shortest distances between data points and\ntheir closest boundary list points:\n1 \/i icm d D\uf03d \uf02d (26)\nmax( )D \uf03d d min( ( , ))i id dis\uf03d x v (27)\nwhere v denotes the vector of data points in the boundary list. Closeness measure reaches a maximum\nwhen the individual support vectors are in the boundary list or are near the boundary.\nFigure 4 illustrates the influence of different cm levels on the quality of representation, using an example\ndataset. From the figure, it appears that B* offers the best representation, whereas representation A has\ntoo many support vectors and representations C, D, and E have too few support vectors (many boundary\npoints are not support vectors). In this example, the fitness value reaches 1, the possible highest value,\nfor representations B, C, D, and E. However, GSVRM starts its iterative search with small \uf073 values, and\nis stopped when the fitness value reaches 1 for the first time, leading to representation B* in this\nexample.\nFigure 4: Influence of \uf056 on GSVRM representation.\nOnce the optimal \uf073 value is calculated based on the fitness function, one can construct \u2018inner\u2019 and\n\u2018outer\u2019 boundary representations by correspondingly changing the radius of the GSVRM hyper-sphere.\nFigure 5 illustrates this procedure for the same dataset from Figure 4. It is clear that as the radius is\nchanged, the overall geometric shape is maintained while the scale changes.\nA\nB*\nC D E\n16\nFigure 5: Influence of GSVRM hyper-sphere radius on boundary representation ( 0)\uf056 \uf03d .\nAge:\nIn order to handle non-stationary data, the \u201cage\u201d concept is also introduced into the fitness function.\nThe age of a data point is measured by the weight parameter ( i\uf077 ), which is a function of forgetting\nfactor. Thus, the age parameter forces the support vectors to be as young as possible so as to be able to\neffectively track the non-stationary process. Note that the overlap between new data and previous data\nduring evolution is allowed with the possibility of having two age values for overlapping data points.\nGSWRM can also be applied to stationary data by setting the i\uf077 value to zero, which will force the\nfitness function to represent only the closeness of support vectors to the geometric boundary. In\nsummary, the fitness function identifies a value for \uf073 that yields young support vectors close to the\nboundary.\nLearning from abnormal data\nAs mentioned earlier, one-class classification is important in cases where there are not enough\nexamples from abnormal classes. The question \u201chow can the model benefit from any existing abnormal\nexamples?\u201d is very logical. SVDD incorporates these examples in the model [13]. Existing examples\nfrom abnormal classes are particularly important when they are \u201cwithin normal data\u201d. The objective\nfunction can incorporate these examples as follows:\nOuter GSVRM\nInner GSVRM\n17\nMin 2 i i o j j\ni j\nr C C\uf077 \uf078 \uf077 \uf078\uf02b \uf02b\uf0e5 \uf0e5 (28)\nSubject to:\n2\ni ir \uf078\uf02d \uf0a3 \uf02bx c (29)\n2\nj jr \uf078\uf02d \uf0b3 \uf02dx c (30)\nwhere oC is the penalty value for an abnormal data point inside the representation boundary.\nThe Lagrange formulation of the problem is as follows:\n\uf07b \uf07d\n2\n2\n( , , , , )\n( 2 )\ni i o j j i i\ni j i\nj j i i i i i\nj i\nL r r C C\nr\n\uf077 \uf078 \uf077 \uf078 \uf067 \uf078\n\uf067 \uf078 \uf061 \uf078\n\uf03d \uf02b \uf02b \uf02d\n\uf02d \uf02d \uf02b \uf02d \uf0d7 \uf02d \uf0d7 \uf02b \uf0d7\n\uf0e5 \uf0e5 \uf0e5\n\uf0e5 \uf0e5\nc \u03be \u03b1 \u03b3\nx x c x c c\n(31)\nAfter taking the derivatives, we obtain the following equations:\n1i i\ni i\n\uf061 \uf061\uf02d \uf03d\uf0e5 \uf0e5 (32)\ni i j j\ni j\n\uf061 \uf061\uf03d \uf02d\uf0e5 \uf0e5c x x (33)\nSubstituting (32) and (33) in (31), leads us to following Lagrange formulation:\n,\n, ,\n,\n( , , , , ) ( ) ( ) ( )\n( ) ( )\n( )\ni i i i i i i j i j\ni I i J i j I\ni j i j i j i j\ni I j J i J j I\ni j i j\ni j J\nL r \uf061 \uf061 \uf061 \uf061\n\uf061 \uf061 \uf061 \uf061\n\uf061 \uf061\n\uf0ce \uf0ce \uf0ce\n\uf0ce \uf0ce \uf0ce \uf0ce\n\uf0ce\n\uf03d \uf0d7 \uf02d \uf0d7 \uf02d \uf0d7\n\uf02b \uf0d7 \uf02b \uf0d7\n\uf02d \uf0d7\n\uf0e5 \uf0e5 \uf0e5\n\uf0e5 \uf0e5\n\uf0e5\nc \u03be \u03b1 \u03b3 x x x x x x\nx x x x\nx x\n(34)\nwhere, I is class of normal examples and J is class of abnormal examples. This formulation can be\nsimplified by labeling abnormal classes as \u201c-1\u201d and normal class as \u201c+1\u201d.\n1\n1\ni\ni\ni\nx I\ny\nx J\n\uf0ce\uf0ec\n\uf03d \uf0ed\n\uf02d \uf0ce\uf0ee\n(35)\n'\ni i iy\uf061 \uf061\uf03d (36)\nSubstituting (35) and (36) in (34) results in the following:\nMax ' ' '\n{ , } , { , }\n( , , , , ) ( ) ( )i i i i j i j\ni I J i j I J\nL r \uf061 \uf061 \uf061\n\uf0ce \uf0ce\n\uf03d \uf0d7 \uf02d \uf0d7\uf0e5 \uf0e5c \u03be \u03b1 \u03b3 x x x x (37)\n18\n0 i iC\uf061 \uf077\uf0a3 \uf0a3 (38)\n0 j o jC\uf061 \uf077\uf0a3 \uf0a3 (39)\n1i\ni\n\uf061 \uf03d\uf0e5 (40)\nAs can be seen from equation (37), the formulation essentially remains the same with an additional\nconstraint for the corresponding Lagrange multiplier value of the abnormal example. The next section\ndescribes the online training process.\nIV. ON-LINE TRAINING\nOn-line training can be defined as the process of continuous training as new data becomes available using\nrelatively small effort by adding new data to the previously trained system instead of starting the training\nfrom scratch. Provision for on-line training is an important attribute for many models. The need for an\non-line training procedure is obvious, especially for machine monitoring sort of applications where\nresponse time is of the essence. As noted earlier, GSVRM optimizes the sigma parameter of the\nGaussian kernel (i.e., \uf073 ) using the boundary list (i.e., the list of data points that are located on the\nboundary of the dataset). Since the fitness value of sigma depends on the boundary list, any change in the\nboundary list will affect the optimal value for \uf073 , and obviously, the GSVRM. Thus, adaptation of\nGSVRM to incorporate newly available data depends on changes in the boundary list. An efficient\nprocess for carrying out this adaptation on-line is addressed here.\nFigure 6: New data point is located strictly inside the dataset.\nx\nNew data\npoint\n19\nLet us first discuss how the boundary list may be affected by a new data point. There are two possible\nchanges in the boundary list: addition to and deletion from the list. In \u2018addition\u2019, the new data point will\nbe added to the boundary list, only if it is in the boundary. If the data point is located strictly inside the\nexisting dataset; neither the boundary list nor the GSVRM will be updated as illustrated in Figure 6. If\nthe new data point is located in the boundary of the dataset as illustrated in Figure 7.a, the boundary list\nis updated instead of re-calculating the entire boundary points from scratch by adding the new data point\nto the list. This addition may cause some existing boundary list points to no longer qualify for the list,\ntriggering the need for their deletion.\nHence, there are two tasks to be carried out when a new data point becomes available in online training:\nFirst, we need to check whether the new data point is a boundary point. If so, it is added to the boundary\nlist. Otherwise, nothing will be changed and online training will end for this data point. Secondly, we\nneed to check any possible deletion from the boundary list. For the first task, a local GSVRM is created\nfor the new data point. If the new point lies outside its inner local GSVRM, as in Figure 7.b (dashed line\nboundary belong to local inner GSVRM), it means the data point is located in the boundary and is added\nto the boundary list. The second task is to identify any points for deletion from the boundary list. Data\npoints that are left strictly inside the dataset because of new data points are deleted from the boundary list\nas illustrated in Figure 7.b (new data point leaves the top boundary point to be strictly inside the dataset).\nIn order to identify these data points, local inner GSVRMs are created for data points within the vicinity\nof the new data point and in the boundary list. If a data point falls inside the boundary of any of these\ninner local GSVRMs, it means the data point is no longer in the boundary any is removed from the\nboundary list, as in Figure 7.b.\n20\nFigure 7: a) New data point is located in the boundary of the dataset b) Boundary list is updated c) GSVRM is\nupdated.\nFigure 8: GSVRM ALGORITHM.\nOnce the boundary list is updated, new best \uf073 value is searched by increasing (decreasing) current \uf073\nvalue as long as overall fitness value gets better. Increasing or decreasing \uf073 too much will lead to\nreduced overall fitness, as mentioned under \u2018closeness\u2019 subsection in section III. Increment rate in each\nstep is defined as ratio of the difference between the smallest nearest neighbor distance ( min\uf073 ) and the\nlargest nearest neighbor distance ( max\uf073 ) in the dataset to 50. The \uf073 value that gives the highest fitness\nBoundary list is\nupdated\nGSVRM is\nupdated with\nnew sigma\n(a)\nx\nNew data\npoint\n(b) (c)\nBoundary list points\nInterior points\nStep 1: Determination of data points close to boundary (i.e., creation of boundary list).\n1.1 Calculate the average nearest neighbor distance ( dn )\n1.2 For each data point i, construct a local GSVRM with those data points inside the sphere with radius of 2 \uf0b4 dn\nand centered at the current data point i\n1.3 Calculate an inner GSVRM with reduced threshold\n1.4 If the data point is rejected by the inner GSVRM (meaning outside the threshold boundary), add the point to\nthe boundary list\n1.5 Choose the next data point and go to step 1.2.\nStep 2: Calculate the global GSVRM using an optimal \uf073 . The \uf073 value that gives the highest fitness value is chosen to\nbe the optimal \uf073 .\nStep 3: When a new data point is available, update the boundary points as follows:\n3.1 Construct a local GSVRM for the new data point as in Step 1.\n3.2 Calculate inner GSVRM with a reduced threshold.\n3.3 Add the new data point to the boundary list if the data point is rejected by inner GSVRM.\n3.4 Implement steps 3.1 and 3.2 for all the data points within the local SVRM sphere of the new data point and the\nboundary list.\n3.5 Remove the data point from the boundary list if it is inside one of the inner GSVRM\n3.5 Any points that are not rejected should be removed from the boundary list.\n3.6 Check for better fitness of the global GSVRM by increasing and decreasing \uf073 . If a different \uf073 leads to better\nfitness, update \uf073 .\n3.7 Calculate minimum volume hyper-sphere using updated boundary points.\n21\nvalue is chosen to be the new optimal \uf073 value. Figure 7.c illustrates the updated boundary of GSVRM\nby updating \uf073 using the new boundary list.\nThe complete GSVRM algorithm is outlined in Figure 8.\nExperimental results from implementation of GSVRM are discussed in the next section.\nV. EXPERIMENTS\nThe proposed one-class classification method (GSVRM) is tested using multiple datasets, both simulated\nand real-world, in one, two- and three-dimensional spaces. First five datasets are simulated datasets in\ntwo and three dimensional spaces. The sixth dataset is illustrated and used by Yamanishi and Takeuchi in\n[28] to demonstrate the effectiveness of SmartSifter [11]. GSVRM will also be applied to this dataset and\nthe results will be compared several existing methods. The last dataset involves thrust and torque signals\ncollected from a drilling machine, an example from the real-world.\nThe properties of the first five datasets are illustrated in Table 1. The parameter levels employed for\nconstructing the GSVRMs are reported in Table 2. All experiments dealing with non-stationary data in\nthe first five datasets employed a forgetting factor of \uf06c = 0.03. Obviously, the results are sensitive to\nthis factor. In general, the forgetting factor should be based on subject matter experience and\/or data\nanalysis. The higher the non-stationary nature of the data, the larger should be the forgetting factor. In\nall our experiments, the C parameter is set at 1.03 so that penalty values for most recent data come close\nto unity, ensuring that recent data are well represented. Weighting factors for both age and closeness\nmeasures ( 1,2\uf06c ) are both set to be 0.5. Reduction percentage for creation of inner GSVRM is set to be\n0.7% for all datasets (i.e., the radius for inner local GSVRM is 0.993 times the radius of GSVRM). All\nexperiments are conducted on a PC with a Pentium III processor running at 700 MHz. All experiments\ninvolved a Gaussian kernel.\n22\nTable 1: Properties of experimental datasets.\nDataset #1 #2 #3 #4 #5\nDimensionality 2 2 2 2 3\nStationarity YES\nNO \u2013\nstraight line\nmovement\nNO \u2013\narc\nmovement\nNO \u2013\nstraight line\nmovement\nYES\nGeometry\nGaussian\n[0 0]\uf03d\u03bc\n1 .8\n.8 1\n\uf0e9 \uf0f9\n\uf03d \uf0ea \uf0fa\n\uf0eb \uf0fb\n\u03a3\nGaussian\n[0 0]\uf03d\u03bc\n1 .8\n.8 1\n\uf0e9 \uf0f9\n\uf03d \uf0ea \uf0fa\n\uf0eb \uf0fb\n\u03a3\nGaussian\n[0 0]\uf03d\u03bc\n1 .8\n.8 1\n\uf0e9 \uf0f9\n\uf03d \uf0ea \uf0fa\n\uf0eb \uf0fb\n\u03a3\nLetter\n\u201cC\u201d\nHelical\n\u201cC\u201d\nTable 2: Parameters for building GSVRM.\nDataset #1 #2 #3 #4 #5\nParameters\nC 1.03 1.03 1.03 1.03 1.03\n\uf06c 0 .03 .03 .03 0\n1w - .5 .5 .5 -\n2w - .5 .5 .5 -\nFigure 9 illustrates the estimated GSVRM boundary and an actual 99 percentile contour plot for dataset\n#1. Given the geometric similarity between the GSVRM boundaries and the probability contour, one can\nconclude that GSVRM is reasonably effective in describing the data.\nIn the case of dataset #2, the class moves along a straight line. If one were to ignore the non-stationary\nnature of the process, all data will receive equal importance and the GSVRM will try to include all data\nwithin the hyper-sphere. As can be seen from Figure 10, the resulting GSVRM boundary does a good\njob of representing the non-stationary process and old data are allowed to fall outside the boundary.\nAs can be seen from Figure 11, in the case of dataset #3, the mean moves along an arc of a circle.\nGSVRM once again does an effective job representing the dynamics of the non-stationary process.\nFigure 12 presents results for dataset #4. This dataset is \u201cC\u201d shaped data that move on a straight line.\nGSVRM is once again effective in representing the non-stationary process in spite of the complexity of\nthe class boundary. As seen from the figure, the most recent data, which are in \u201cC\u201d shape on the right\nend of the line, are detected successfully.\n23\nThe primary objective in building the last dataset (i.e., the helical \u201cC\u201d dataset #5) was to assess the\nscalability of the proposed GSVRM to higher dimensional data, in terms of computational efficiency.\nThis dataset in the three-dimensional space is constructed by adding an additional dimension to dataset\n#4, where the new variable increases in value linearly from the first data point to the last data point,\nresulting in a helical \u201cC\u201d.\nFigure 9: Results from GSVRM for dataset #1.\n-5 0 5 10 15 20 25\n-5\n0\n5\n10\n15\n20\n25\nFigure 10: Results from GSVRM for non-stationary dataset #2.\n-0.15 -0.1 -0.05 0 0.05 0.1\n-0.14\n-0.12\n-0.1\n-0.08\n-0.06\n-0.04\n-0.02\n0\n0.02\nFigure 11: Results from GSVRM for non-stationary dataset #3.\n24\n-0.1 -0.05 0 0.05 0.1 0.15 0.2 0.25\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\n0.2\n0.25\nInitial\nMiddle\nLast\nSVND\nFigure 12: Results from GSVRM for dataset #4.\nGiven the curse of dimensionality, one would expect to obtain more support vectors for higher\ndimensional datasets. Since capacity control in a Support Vector Machine is obtained by controlling the\nnumber of support vectors [27], a model that results in number of support vectors that equal or near the\nnumber of data points is susceptible to high misclassification probability. Several experiments have been\ncarried out here to assess these properties for the proposed GSVRM method.\nTable 3 reports the number of support vectors for two- dimensional datasets (i.e., \u201cC\u201d) and three-\ndimensional datasets (i.e., helical \u201cC\u201d), as a function of dataset size. While there is evidence that the\nmethod can handle higher dimensional datasets, there is clear evidence that the number of support\nvectors increases with data dimensionality. Given the difficulty in producing three-dimensional\nvisualizations, we only report the number support vectors and the computational time for constructing the\nGSVRM.\nTable 3: Scalability of GSVRM method to higher dimensions\nmeasured in terms of number of support vectors.\nDataset Size\n124 174 224 274 324\nD\nim\nen\nsi\non\nal\nity Letter\u201cC\u201d in\n2D 9 12 12 10 12\nHelical\n\u201cC\u201d in\n3D 23 17 28 26 33\nFigure 13 reports the training computation time for the two- and three-dimensional datasets from Table 3.\nIt is possible to implement GSVRM to higher dimensional problems, however, as with most\n25\nclassification methods, computation time increases with the increase in dataset size and dimensionality.\nComputational time can be reduced by initially applying GSVRM to a small fraction of the dataset and\nonline-training for the rest of the dataset. Online training and its computational time is discussed in the\nfollowing sub-sections.\nFigure 13: GSVRM computational time (in seconds) versus dataset size\nin two- and three-dimensional data.\nOn-line Training\nThe procedure for on-line training is discussed in section 5. Checking for a better \uf073 value every time a\nnew data point becomes available may not be necessary. We chose to check for a better \uf073 value once\nevery 5 new data points are available. The optimal batch size would depend on how fast the process is\nmoving and by how much the class density distribution is changing. Figure 14 shows results from\nimplementation of on-line GSVRM for dataset #2. It is evident that the GSVRM representation tracks\nthe data. \u201c+\u201d represents the data points used initially for training the GSVRM and \u201c \uf067\u201d represents data\npoints subsequently added to the dataset one by one. GSVRM is updated after every five new data points\nbecome available.\nDataset Size\n124 174 224 274 324\nC\nom\npu\nta\ntio\nna\nlt\nim\ne\n(s\nec\non\nds\n)\n26\n-5 0 5 10 15 20 25\n-5\n0\n5\n10\n15\n20\n25\nFigure 14: On-line GSVRM using second dataset.\n-0.15 -0.1 -0.05 0 0.05 0.1\n-0.2\n-0.15\n-0.1\n-0.05\n0\n0.05\n0.1\n0.15\nFigure 15: Online WSND using C shape dataset\nOn-line GSVRM method is also tested using the dataset #3. Initially, the first half (represented by \u2018+\u2019)\nof the \u201cC\u201d shaped dataset is used to initialize the process. The second half (represented by \u2018 \uf067\u2019) of the\ndata is added one by one. Boundary list is updated after each data point is added, and \uf073 value is updated\nafter every five data points are added. Figure 15 shows the moving hyper-sphere of the data. As seen\nfrom the graph, the hyper-sphere seems to do an effective job in tracking the data.\nComparison of GSVRM with Other Methods\nAs mentioned earlier, the sixth dataset to be reported with GSVRM is generated by Yamanishi and\nTakeuchi in [28] to demonstrate the effectiveness of CF and SC, which are extended from SmartSifter\n[11]. A second order AR model is used by them to generate the dataset:\n1 10.6 0.5t t t tx x x \uf065\uf02d \uf02d\uf03d \uf02d \uf02b , )1,0(Nt \uf0bb\uf065 (41)\n27\n0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000\n-10\n0\n10\n20\n30\n40\n50\nFigure 16: Shifting mean simulated AR dataset [28].\nThe mean of the data is shifted by ( ( ) 10x x\uf044 \uf03d \uf02d ) at the 1000thi\uf0b4 data point ( 1, 2,3,...9i \uf03d ). The data\nas displayed in Figure 16 is non-stationary and CF and SC aim to detect the shifts correctly with\nminimum false alarm. The results from another change point detection method, GS, by Guralnik and\nSrivastava [28], will also be compared with GSVRM.\nThe effectiveness of the detection method is quantified through a benefit function as outlined in Eq. (42).\nThis benefit function rewards early detection while penalizing false alarms. It is expected to detect the\nshift within 20 data points after the shift. False alarm is the ratio of false alarms to the total number of\nalarms.\n( ) 1 ( *) \/ 20benefit t t t\uf03d \uf02d \uf02d (42)\nEven though ( )x\uf044 is defined as 10 x\uf02d , it is stated in [28] that it is fixed at 5 ( ( ) 5x\uf044 \uf03d ) during\nexperimentation. We too employ the same setting for all our experiments. Figure 17 displays the results\nfrom classification of all the dataset points using GSVRM with on-line training. Normal data is classified\nas belonging to class \u201c1\u201d, whereas change points are identified as belonging to class \u201c-1\u201d. All the change\npoints except 6000th and 8000th data points are detected immediately. Change in 6000th and 8000th data\npoints are detected with only 1 point delay (6001st and 8001st data points). Figure 18 displays benefit vs.\nfalse alarm rate values for GSVRM, CF, SC, and GS. Several GSVRMs with different radius values are\napplied to the dataset and four of them are displayed in the figure (displayed as \u2018o\u2019). Top left corner of\n28\nthe figure gives the best results with few false alarms and immediate detection of changes. As seen from\nthe figure, our method, GSVRM, highly outperforms the other methods.\n0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000\n-1\n1\nData\nN\nor\nm\nal\n-A\nbn\nor\nm\nal\nC\nla\nss\nClassification of 10000 data points as normal - abnormal\n1 : Normal Class\n-1 : Abnormal Class - Change detection\nData points identified as abnormal classes:\n1000,\n2000, 2001,\n3000,\n4000,\n5000,\n6001,\n7000, 7001\n8001\n9000, 9001\n10000\nFigure 17: Classification of dataset points.\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nComparison of GSVRM with other methods\nFalse Alarm Rate\nA\nve\nra\nge\nd\nB\nen\nef\nit\nSC\nCF\nGSVRM\nGS\nFigure 18: Comparison of GSVRM with other methods.\nThe quantitative comparison of computational times for GSVRM and other methods cannot be achieved\nsince they are not reported explicitly. However, it is reported that computational times of CF, SC, and GS\n29\nare in the order of )(nO , )( 2nO , and )( 2nO (where n is the size of the dataset). The computational time\nof GSVRM for one time training is in the order of )( 2nO , since it involves quadratic optimization.\nHowever, online training significantly reduces it. Remember that each data point is associated with a\nweight value defining its importance as in Eq. (18) and i\uf061 defining being inside or outside of the sphere\nas in Eq. (38) and (39). Since 0 i iC\uf061 \uf077\uf0a3 \uf0a3 , data points with very low iC\uf077 value can be ignored during\ntraining. The computational time in each step of on-line training is in the order of )( 2mO , where m is\nthe number of data points with high enough iC\uf077 value (\n310\uf02d\uf03e in our experiment). Since this is\nrepeated for n data points, the computational time is in the order of )( 2nmO .\nDrilling Process Dataset\nThe last dataset involves actual data collected from a drilling machine for the purposes of monitoring the\ncutting tool (i.e., the drill-bit). The experimental setup consisted of a HAAS VF-1 CNC Machine, a\nworkstation with LabVIEW software for signal processing, a Kistler 9257B piezo-dynamometer for\nmeasuring thrust-force and torque, and a NI PCI-MIO-16XE-10 card for data acquisition. Stainless steel\nbars with a thickness of 0.25 inches were used as specimens for tests. The drill-bits used consisted of\nhigh-speed twist drill-bits with two flutes and were operated under the following conditions without any\ncoolant: feed-rate of 4.5 inches-per-minute (ipm) and spindle-speed of 800 revolutions-per-minute (rpm).\nThe thrust-force and torque data were collected for each hole from the time instant the drill-bit penetrated\nthe work piece through the time instant the drill-bit protruded from the other side of the work piece. The\ndata was collected at 250 Hz, which is considered adequate to capture cutting tool dynamics in terms of\nthrust-force and torque. The number of data points collected for a hole changed between 380 and 460.\nData from each hole was reduced to 24 RMS (root mean square) values. For illustration, Fig. 19.a plots\nthis data collected for drill-bit #5. As seen from the figure, the data is non-stationary both within the hole\n(attributable to varying material removal rate as the drill-bit penetrates the work piece) as well as across\n30\nthe holes (attributable to wear). Scatter plot of standardized thrust-force and torque are displayed in Fig.\n19.b.\n-2.5 -2 -1.5 -1 -0.5 0 0.5 1\n-2\n-1.5\n-1\n-0.5\n0\n0.5\n1\nNon-stationary data within a hole\nTo\nrq\nue\nThrust-force\nHole\nends Hole starts\nFigure 19: a) Thrust-force and Torque data from drill-bit #5.\nb)Joint plot of standardized thrust-force and torque during a particular hole.\nThe data from the first and last couple of holes differ from the intermediate holes, since they represent\nthe \u201cbrand new\u201d and \u201cclose to failure\u201d cases. 15 holes from intermediate holes of two drill-bits are\nselected (from 3rd through 8th holes from 1st drill-bit and from 2nd through 10th holes from 2nd drill-bit).\nThe data from 15 holes (24x15=360 data points) are mapped based on time in the hole in order to model\nthe movement of the data within the hole. In other words, first data of all holes are taken first, second\nlater, till 24th data within all holes. Initial GSVRM is trained with first 15 data points among 360 data\npoints, and then online training is applied to the remaining thrust-torque dataset (345 data points) by\nintroducing data points one by one. Initial GSVRM and the progress of updated GSVRM with new\navailable data are displayed Figure 20. Sign \u2018+\u2019 represents the available data, whereas sign \u2018.\u2019 represents\nthe future or too old data. As seen from the figure, GSVRM is able to effectively track and capture the\ndata in this non-stationary process. Some of the data represented by \u2018+\u2019 are left outside of GSVRM, since\nthey are old (they are represented by \u2018.\u2019 when they become too old). This totally depends on the\nforgetting factor (\uf06c ) and may be changed as desired.\nTime\nThrust force \u2013\nNewtons\nTorque \u2013\nNewton meters\na hole\n31\n-2 -1.5 -1 -0.5 0 0.5 1 1.5\n-2\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\nInitial GSVRM\nThrust-force\nTo\nrq\nue\n-2 -1.5 -1 -0.5 0 0.5 1 1.5\n-2\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\nAfter 45th data\nTo\nrq\nue\nThrust-force\n-2 -1.5 -1 -0.5 0 0.5 1 1.5\n-2\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\nAfter 63th data\nThrust-force\nTo\nrq\nue\n-2 -1.5 -1 -0.5 0 0.5 1 1.5\n-2\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\nAfter 231th data\nThrust-force\nTo\nrq\nue\n-2 -1.5 -1 -0.5 0 0.5 1 1.5\n-2\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\n251th Data\nThrust-force\nTo\nrq\nue\n-2 -1.5 -1 -0.5 0 0.5 1 1.5\n-2\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\nAfter 291th Data\nThrust-force\nTo\nrq\nue\n-2 -1.5 -1 -0.5 0 0.5 1 1.5\n-2\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\nAfter 309th Data\nThrust-force\nTo\nrq\nue\n-2 -1.5 -1 -0.5 0 0.5 1 1.5\n-2\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\nAfter 321th data\nThrust-force\nTo\nrq\nue\n32\n-2 -1.5 -1 -0.5 0 0.5 1 1.5\n-2\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\nAfter 357th Data\nThrust-force\nTo\nrq\nue\nFigure 20: Progression of GSVRM with online training for thrust-force and torque data.\nVI. CONCLUSION\nOne-class classification methods have been of particular interest to researchers in domains where it is\ndifficult or expensive to find examples of abnormal behavior (such as in medical\/machine diagnosis and\nIT network surveillance). This paper proposes a novel one-class classification method named General\nSupport Vector Representation Machine (GSVRM) for stationary as well as non-stationary classes. The\nmethod does not make any strong assumptions regarding the cluster data density. In representing the\n\u2018normal\u2019 class, GSVRM essentially minimizes the volume of the hyper-sphere in the Gaussian kernel\nspace that encapsulates normal data, while making an explicit provision for incorporating any data\navailable from \u2018abnormal\u2019 classes. The GSVRM offers the ability to represent non-stationary classes by\nmaking a provision for assigning different weights (or degrees of importance) to data points as a function\nof their \u2018age\u2019. GSVRM formulation still remains to be a quadratic programming formulation and meets\nthe KKT optimality conditions, allowing use of existing solvers to arrive at a global optimal solution.\nExperimental evaluation reveals that the proposed method can effectively represent both stationary and\nnon-stationary classes. An efficient on-line version of the GSVRM is also proposed. GSVRM is applied\nto several simulated datasets and actual data collected from a drilling machine. GSVRM highly\noutperforms the other methods and comparison results are reported in the paper.\nACKNOWLEDGEMENTS\nThis research is partially funded by the US National Science Foundation under grant DMI-0300132.\n33\nREFERENCES\n[1] D. M. Tax and R. Duin, \u201cSupport vector domain description\u201d, Pattern Recognition Letters, vol.20,\nno.11-13, pp 1191-1199, 1999.\n[2] T. Nairac, C. Corbet, R. Ripley, N. Townsend, and L. Tarassenko, \u201cChoosing an appropriate model\nfor novelty detection\u201d, in Proc. of 5th International Conference on Artificial Neural Networks, UK,\n1997, pp.117-122.\n[3] L. Tarassenko, P. Hayton, N. Cerneaz, and M. Brady, \u201cNovelty detection for the identification of\nmasses in mammograms\u201d, in Proc. 4th International Conference on Artificial Neural Networks,\nLondon, UK, 1995, pp. 442-447.\n[4] M. Costa and L. Moura, \u201cAutomatic assessment of scintmammographic images using a novelty\nfilter\u201d, in Proc. 19th Annual Symposium on Computer Applications in Medical Care, PA, 1995, pp.\n537-541.\n[5] C. Herringshaw, \u201cDetecting attacks on networks\u201d, Computer, vol.30, no.12, pp.16-17, 1997\n[6] K. Worden and G. Manson, \u201cExperimental validation of structural health monitoring\nmethodology\u201d, Journal of Sound Vibration, vol.259, no.2, pp.345-363, 2003.\n[7] K. Van Leemput, F. Maes, D. Vandermeulen, A. Colchester, and P. Suetens, \u201cAutomated\nsegmentation of multiple sclerosis lesions by model outlier detection\u201d, Medical Imaging IEEE\nTransactions, vol.20, no.8 pp.677-688, 2001.\n[8] S. J. Raudys and A. K. Jain, \u201cSmall Sample Size Effects in Statistical Pattern Recognition:\nRecommendations for Practitioners\u201d, Pattern Analysis and Machine Intelligence, IEEE\nTransactions, vol.13, no.3, pp.252-264, 1991.\n[9] P. W. Duin: \u201cOn the Choice of Smoothing Parameters for Parzen Estimators of Probability Density\nFunctions\u201d. IEEE Trans. Computers, vol.25, no.11, pp.1175-1179, 1976.\n[10] K. Worden, G. Manson, and D. J. Allman, \u201cExperimental validation of structural health monitoring\nmethodology I: novelty detection on a laboratory structure\u201d, Journal of Sound and Vibration,\nvol.259, no.2, pp.323-343, 2003.\n[11] K. Yamanishi, J. Takeuchi, G. Williams,, P. Milne \u201cOnline unsupervised outlier detection using\nfinite mixtures with discounting learning algorithms\u201d, Data Mining and Knowledge Discovery, vol.\n8, pp. 275\u2013300, 2004\n[12] S. Singh and M. Markou, An Approach To Novelty Detection Applied To The Classification Of\nImage Regions, IEEE Transactions on Knowledge And Data Engineering, vol. 16, no.4, pp. 396-\n407, 2004\n[13] D. M. Tax, \u201cOne Class Classification\u201d, Ph.D. dissertation, Delft Technical University, 2001.\n[14] G.C. Vasconcelos, \u201cA bootstrap-like rejection mechanism for multilayer perceptron networks\u201d, II\nSimposio Brasileiro de Redes Neurais, S\u00e3o Carlos-SP, Brazil, pp. 167-172, 1995.\n34\n[15] Markou M. and Singh S., Novelty detection: a review-part 1: neural network based approaches,\nSignal Processing, Vol. 83, No. 12,:pp. 2499 - 2521, 2003\n[16] J. Ma and S. Perkins, \u201cOnline novelty detection on temporal sequences\u201d, in Proc. of International\nConference on Knowledge Discovery and Data Mining, Washington DC, 2003, pp. 417-423.\n[17] B. Sch\u00f6lkopf, R. Williamson, A. Smola, and J. S. Taylor, \u201cSV Estimation of a Distribution\u2019s\nSupport\u201d, in Proc. NIPS\u201999, 1999.\n[18] C. Yuan and D. Casanent, \u201cSupport vector machines for class representation and discrimination\u201d,\nInternational Joint Conference on Neural Networks, Portland, 2003 pp. 1610-1615.\n[19] R. Sun, and F. Tsung, A kernel-distance-based multivariate control chart using support vector\nmethods. International Journal of Production Research, 2003, 41, 2975-2989.\n[20] G. Widmer and M. Kubat, \u201cLearning in the Presence of Concept Drift and Hidden Contexts\u201d,\nMachine Learning, Vol. 23, No. 1, pp. 69-101, 1996.\n[21] M. Last, \u201cOnline Classification of Nonstationary Data Streams\u201d, Intelligent Data Analysis, Vol. 6,\nNo. 2, pp. 129- 147, 2002.\n[22] O. Maimon and M. Last, Knowledge Discovery and Data Mining - The Info-Fuzzy Network (IFN)\nMethodology, Kluwer Academic Publishers, December 2000.\n[23] K.-R. M\u00fcller, S. Mika, G. R\u00e4tsch, K. Tsuda, and B. Sch\u00f6lkopf. \u201cAn introduction to kernel-based\nlearning algorithms\u201d,. IEEE Neural Networks, vol.12, no.2, pp.181-201, 2001.\n[24] V. Vapnik, Statistical learning theory, Wiley, 1998, pp.401-440.\n[25] W. Li, H. Yue, S. Valle-Cervantes, and J. Qin, \u201cRecursive PCA for adaptive process monitoring\u201d,\nJournal of Process Control , vol.10, no.5, pp.471-486, 2000.\n[26] V. B. Gallagher, R. M. Wise, S. W. Butler, D. D. White, and G. G Barna, \u201cDevelopment and\nbenchmarking of multivariate statistical process control tools for a semiconductor etch process;\nimproving robustness through model updating\u201d, in Proc. International Symposium on Advanced\nControl of Chemical Processes, Banff, Canada 1997, pp.149-161.\n[27] N. Cristianini and J. S. Taylor, An introduction to support vector machines and other kernel-based\nlearning methods, Cambridge University Press, 2000, pp.122-125.\n[28] K. Yamanishi and J. Takeuchi \u201cUnifying Framework for Detecting Outliers and Change Points\nfrom Non-Stationary Time Series Data\u201d, IEEE Transact\u0131ons on Knowledge and Data Eng\u0131neer\u0131ng,\nVol. 18, No. 4, April 2006.\n[29] V. Guralnik and J. Srivastava, \u201cEvent Detection from Time Series Data\u201d, in Proc. of ACM-\nSIGKDD International Conference Knowledge Discovery and Data Mining, 1999, pp. 33-42.\n[30] Sharifzadeh M., Azmoodeh F., and Shahabi C., \u201cChange Detection in Time Series Data Using\nWavelet Footprints\u201d, Lecture Notes in Computer Science, Vol. 3633, 2005, pp.127-144.\n"}