{"doi":"10.1016\/j.cag.2003.08.005","coreId":"70403","oai":"oai:eprints.lancs.ac.uk:12229","identifiers":["oai:eprints.lancs.ac.uk:12229","10.1016\/j.cag.2003.08.005"],"title":"Interacting with Proactive Community Displays","authors":["Villar, Nicolas","Kortuem, Gerd","Gellersen, Hans","Schmidt, Albrecht"],"enrichments":{"references":[{"id":16300529,"title":"Teleporting \u2013 making applications mobile.\u201d","authors":[],"date":"1994","doi":"10.1109\/wmcsa.1994.36","raw":"F. Bennett, T. Richardson, and A. Harter. \u201cTeleporting \u2013 making applications mobile.\u201d Proc. Of the IEEE Workshop on Mobile Computing Systems and Applications, pages 82-84, Santa Cruz, California, Dec. 1994. IEEE Computer Society Press.","cites":null},{"id":16300533,"title":"The Gesture Pendant: A Self-illuminating, Wearable, Infrared Computer Vision system for Home Automation Control and","authors":[],"date":"2000","doi":"10.1109\/iswc.2000.888469","raw":"T. Starner, J. Auxier, D. Ashbrook and M. Gandy.  \u201cThe Gesture Pendant: A Self-illuminating, Wearable, Infrared Computer Vision system for Home Automation Control and Medical Monitoring.\u201d International Symposium on Werable Computers, Atlanta, GA: IEEE Computer Society, pp. 87-94 (2000).","cites":null},{"id":16300538,"title":"Aware Community Portals: Shared Information Appliances for Transitional Spaces.\u201d CHI","authors":[],"date":"2000","doi":"10.1007\/s007790170034","raw":"N. Sawhney, S. Wheeler, and C. Schmandt. \u201cAware Community Portals: Shared Information Appliances for Transitional Spaces.\u201d CHI 2000, April , 2000.","cites":null},{"id":16300541,"title":"The Active Badge Location System,\u201d Olivetti Research Laboratory,","authors":[],"date":"1992","doi":"10.1145\/128756.128759","raw":"R. Want, \u201cThe Active Badge Location System,\u201d Olivetti Research Laboratory, ACM Trans. Information Sys. Vol. 10, no 1, 1992, pp.91-102","cites":null},{"id":16300545,"title":"Context Awareness in Systems with Limited Resources&quot;.","authors":[],"date":"2002","doi":null,"raw":"O. Cakmakci, J. Coutaz, K. Van Laerhoven, and H.-W. Gellersen. &quot;Context Awareness in Systems with Limited Resources&quot;. In Proc. of the third workshop on Artificial Intelligence in Mobile Systems (AIMS), ECAI 2002, Lyon, France. 2002. pp. 21-29.","cites":null},{"id":16300550,"title":"The Smart-Its Project.","authors":[],"date":null,"doi":null,"raw":"The Smart-Its Project. http:\/\/www.smart-its.org","cites":null},{"id":16300554,"title":"Smart Rooms,","authors":[],"date":"1996","doi":"10.1038\/scientificamerican0496-68","raw":"A. Pentland. Smart Rooms, Sci Amer., April 1996.","cites":null},{"id":16300558,"title":"Consortium for Informatics and Mathematics. Ambient Intelligence,","authors":[],"date":"2001","doi":null,"raw":"European Research Consortium for Informatics and Mathematics. Ambient Intelligence, ERCIM News No.47, Oct. 2001. http:\/\/www.ercim.org\/publication\/ Ercim_News\/enw47\/","cites":null},{"id":16300561,"title":"Adaptive Interfaces and Agents. In","authors":[],"date":"2002","doi":"10.1201\/9781420088861.ch6","raw":"A. Jameson. Adaptive Interfaces and Agents. In J.A. Jacko, A. Sears (Eds.) The Human-Computer Interaction Handbook: Fundamentals, Evolving Technologies and Emerging Applications. Lawrence Erlbaum Assoc, 2002","cites":null},{"id":16300566,"title":"Wearable Computing Meets Ubiquitous Computing: Reaping the best of both worlds.","authors":[],"date":"1999","doi":"10.1109\/iswc.1999.806695","raw":"B.J. Rhodes, N. Minar and J. Weaver. Wearable Computing Meets Ubiquitous Computing: Reaping the best of both worlds. In Proc. of 3rd Intl. Symp. on Wearable Computers (ISWC '99), San Francisco, CA, Oct. 1999.","cites":null},{"id":16300568,"title":"Collaboration Using Multiple PDAs Connected to a PC.","authors":[],"date":"1998","doi":"10.1145\/289444.289503","raw":"B.A. Myers, H. Stiel, and R. Gargiulo. Collaboration Using Multiple PDAs Connected to a PC. In Proc. CSCW'98: ACM Conference on Computer-Supported Cooperative Work, Nov 1998, Seattle, pp. 285-294.","cites":null},{"id":16300571,"title":"PDAs and Shared Public Displays: Making Personal Information Public, and Public Information Personal.","authors":[],"date":"1999","doi":"10.1007\/bf01305320","raw":"Greenberg, S., Boyle, M. and LaBerge, J. PDAs and Shared Public Displays: Making Personal Information Public, and Public Information Personal. Personal Technologies, Vol.3, No.1, March 1999, pp. 54-64, Springer-Verlag.","cites":null},{"id":16300575,"title":"Implementing a Sentient Computing System.","authors":[],"date":"2001","doi":"10.1109\/2.940013","raw":"M. Addlesee, R. Curwen, S. Hodges, J. Newman, P. Steggles, A. Ward, and A. Hopper. Implementing a Sentient Computing System. IEEE Computer, Vol 34, No 8, pp. 50-56, August 2001.","cites":null},{"id":16300577,"title":"A Distributed Location System for the Active Office .","authors":[],"date":"1994","doi":"10.1109\/65.260080","raw":"A. Harter and A. Hopper, A Distributed Location System for the Active Office . IEEE Network, Vol. 8, No. 1, January 1994.","cites":null},{"id":16300579,"title":"Teleporting -Making Applications Mobile.","authors":[],"date":"1994","doi":"10.1109\/wmcsa.1994.36","raw":"F. Bennett, T. Richardson and A. Harter. Teleporting -Making Applications Mobile. Proc. WMCSA\u201994, Workshop on Mobile Computing Systems and Applications, Santa Cruz, Dec. 1994.","cites":null},{"id":16300581,"title":"GestureWrist and GesturePad: Unobtrusive Wearable Interaction Devices.","authors":[],"date":"2001","doi":"10.1109\/iswc.2001.962092","raw":"J. Rekimoto. GestureWrist and GesturePad: Unobtrusive Wearable Interaction Devices. Proc. ISWC\u201901, Fifth Intl Symposium on Wearable Computers, October 2001, Zurich, Switzerland.","cites":null},{"id":16300583,"title":"The Gesture Pendant: A Self-illuminating, Wearable, Infrared Computer Vision system for Home Automation Control and Medical Monitoring.","authors":[],"date":"2000","doi":"10.1109\/iswc.2000.888469","raw":"T. Starner, J. Auxier, D. Ashbrook and M. Gandy.  The Gesture Pendant: A Self-illuminating, Wearable, Infrared Computer Vision system for Home Automation Control and Medical Monitoring. Proc. ISWC 2000, Intl. Symp. on Wearable Computers, Oct 2000, Atlanta, GA, pp. 87-94 (2000).","cites":null},{"id":16300586,"title":"Evolution of a Reactive Environment.","authors":[],"date":"1995","doi":"10.1145\/223904.223926","raw":"J.R. Cooperstock, K. Tanikoshi, G. Beirne, T. Narine and W. Buxton. Evolution of a Reactive Environment. Proc. of CHI'95, Conference on Human Factors in Computing Systems, Denver, May 1995.","cites":null},{"id":16300588,"title":"Reactive environments: Throwing away your keyboard and mouse,","authors":[],"date":"1997","doi":"10.1145\/260750.260774","raw":"Cooperstock, J., Fels, S., Buxton, W. & Smith, K. Reactive environments: Throwing away your keyboard and mouse, CACM 40(9), 1997, pp. 65-73.","cites":null},{"id":16300589,"title":"Multi-Camera Multi-Person Tracking for EasyLiving.","authors":[],"date":"2000","doi":"10.1109\/vs.2000.856852","raw":"J. Krumm, S. Harris, B. Meyers, B. Brumitt, M. Hale, S. Shafer. Multi-Camera Multi-Person Tracking for EasyLiving. IEEE Workshop on Visual Surveillance, July 2000.","cites":null},{"id":16300592,"title":"Face-responsive interfaces: from direct manipulation to perceptive presence,","authors":[],"date":"2002","doi":"10.1007\/3-540-45809-3_10","raw":"T. Darrell, K. Tollmar, F. Bentley, N. Checka, L.-P. Morency, A. Rahimi, and A. Oh. Face-responsive interfaces: from direct manipulation to perceptive presence, Proc. Ubicomp 2002,  Intl. Conf. on Ubiquitous Computing, Gothenburg, Oct 2002.","cites":null},{"id":16300628,"title":"An Inertial Measurement Framework for Gesture Recognition and Applications.","authors":[],"date":"2001","doi":"10.1007\/3-540-47873-6_2","raw":"A.Y.  Benbasat  and J. A. Paradiso. An Inertial Measurement Framework for Gesture Recognition and Applications. Proc. GW 2001, International Gesture Workshop, London, UK, 2001.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2003-01","abstract":"In this paper we describe the design and architecture of an adaptive proactive environment in which information, which reflects the communal interests of current inhabitants, is proactively displayed on large- scale public displays. Adaptation is achieved through implicit communication between the environment and personal sensor devices worn by users. These devices, called Pendle, serve two purposes: they store and make available to the environment user preferences, and they allow users to override the environment\u2019s proactive behavior by means of simple gestures. The result is a smooth integration of environment-controlled interaction (experienced by the user as implicit interaction, triggered by their presence) and user-controlled explicit interaction. Initial results show that user-controlled adaptation leads to an engaging user experience that is unobtrusive and not distracting","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/70403.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/12229\/1\/InteractingCommunityDisplays_IMC2003.pdf","pdfHashValue":"31406aaaef0709e8da6c63f6b8fda90fac4c6e32","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:12229<\/identifier><datestamp>\n      2018-01-24T00:04:27Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Interacting with Proactive Community Displays<\/dc:title><dc:creator>\n        Villar, Nicolas<\/dc:creator><dc:creator>\n        Kortuem, Gerd<\/dc:creator><dc:creator>\n        Gellersen, Hans<\/dc:creator><dc:creator>\n        Schmidt, Albrecht<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        In this paper we describe the design and architecture of an adaptive proactive environment in which information, which reflects the communal interests of current inhabitants, is proactively displayed on large- scale public displays. Adaptation is achieved through implicit communication between the environment and personal sensor devices worn by users. These devices, called Pendle, serve two purposes: they store and make available to the environment user preferences, and they allow users to override the environment\u2019s proactive behavior by means of simple gestures. The result is a smooth integration of environment-controlled interaction (experienced by the user as implicit interaction, triggered by their presence) and user-controlled explicit interaction. Initial results show that user-controlled adaptation leads to an engaging user experience that is unobtrusive and not distracting.<\/dc:description><dc:date>\n        2003-01<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:relation>\n        http:\/\/dx.doi.org\/10.1016\/j.cag.2003.08.005<\/dc:relation><dc:identifier>\n        Villar, Nicolas and Kortuem, Gerd and Gellersen, Hans and Schmidt, Albrecht (2003) Interacting with Proactive Community Displays. Computers and Graphics, 27 (6). pp. 849-857.<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/12229\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1016\/j.cag.2003.08.005","http:\/\/eprints.lancs.ac.uk\/12229\/"],"year":2003,"topics":["QA75 Electronic computers. Computer science"],"subject":["Journal Article","PeerReviewed"],"fullText":"Interacting with Proactive Community Displays \n \n \nNicolas Villar, Albrecht Schmidt, Gerd Kortuem, Hans-Werner Gellersen \nComputing Department \nLancaster University \nLancaster LA1 4YR \nUnited Kingdom \n{villar, albrecht, kortuem, hwg}@comp.lancs.ac.uk \n \nAbstract \nIn this paper we describe the design and architecture \nof an adaptive proactive environment in which \ninformation, which reflects the communal interests of \ncurrent inhabitants, is proactively displayed on large-\nscale public displays. Adaptation is achieved through \nimplicit communication between the environment and \npersonal sensor devices worn by users. These devices, \ncalled Pendle, serve two purposes: they store and make \navailable to the environment user preferences, and they \nallow users to override the environment\u2019s proactive \nbehavior by means of simple gestures. The result is a \nsmooth integration of environment-controlled interaction \n(experienced by the user as implicit interaction, triggered \nby their presence) and user-controlled explicit \ninteraction. Initial results show that user-controlled \nadaptation leads to an engaging user experience that is \nunobtrusive and not distracting. \n1. Introduction \nCommunal environments offer great opportunities \nfor informal interaction and communication. However, \nthe informative aspects of the environment - in the shape \nof posters, screen displays or anything able to display \ninformation \u2013 do not always make best use of their \ndisplay real-estate. Traditional static media such as \nposters quickly become outdated, their informative \npurpose outlived as they become commonplace. Screen \nsavers often occupy computer screens that are left on, \nleaving unused a space that could be useful in displaying \ninformation unobtrusively in the periphery of the \nenvironment. We looked at some of the communal spaces \nin our department and found that although some of them \nwere equipped with valuable display resources, such as \nprojection equipment and large plasma screens, this \nequipment remained unused as gaining access to \ninformation on these displays was complicated. \nAs a result, we became interested in looking at how \nwe could facilitate interaction between mobile users and \ndisplay resources in their ambient environment. Our goal \nis to enable users to take better advantage of display \nresources that are available in their environment. To that \neffect, we developed a proactive environment where \nindividual users and groups of users can interact \nimplicitly and explicitly with display services in their \nvicinity by means of small, personalized, wearable \ndevices, called Pendle (Figure 1). A Pendle mediates \nbetween user and proactive environment. It serves two \npurposes: it holds personal information to implicitly \ntrigger proactive behavior in the environment, and it \nprovides a gesture-based interface for the user to \nexplicitly modify the environment\u2019s.  \nThe environment contains several large-scale public \ndisplays that proactively display information from the \nweb that is relevant to the users near them. This is done \nin a spontaneous fashion, in a manner similar to MIT\u2019s \nCommunity Portals [3], although our design relies on the \ninternet as a source of information and the content to \ndisplay is chosen based on keywords that represent \nindividual users\u2019 interests. Similarly also to the \nTeleporting system developed at the Olivetti Research \nLaboratory [1], our system allows users to quickly gain \naccess to personal information throughout different \ndisplays and environments on demand.  They are able to \ninteract with the displays and get fast access to a small \nnumber of pre-selected internet. The result of our design \nis a smooth integration of environment-controlled \ninteraction (experienced by the user as implicit \ninteraction, triggered by their presence) and user-\ncontrolled explicit interaction. \n2. Concept and System Architecture \nThe starting point of our design is the idea to utilize \nexisting screen real-estate of public displays and desktop \ncomputers as proactive display space.  \nOur main design goals are: \n\u2022 Displays show content which reflects the \ncommunal interests of people in their vicinity \n\u2022 The content on the displays changes without \nusers intervention \n\u2022 Displays are stable over a significant period of \ntime, the change should be not recognized by the \nuser \n\u2022 Upon explicit interaction the display space \nshould be available for particular information \npre-selected by the user \nTo realize this several issues have to be addressed, in \nparticular: \n\u2022 How will users have an influence on the system \n\u2013 implicitly as well as explicitly? \n\u2022 How to find the content to display? \n\u2022 How to give the user means for interacting with \nsuch an environment? \n\u2022 What kind of interaction is useful and \nappropriate? \nIn the following we describe a system that was built \nto explore these issues and to investigate the potential \nbenefits for users. \n2.1 Components and Architecture  \nThe basic design relies on components in the \nenvironment that are connected to the Internet and on  \nPendle wearable devices that communicate wirelessly \nwith the environment. A sketch of the proactive \nenvironment architecture is given in Figure 2. \n \n2.2.1. Display services.  Display services are the visible \npart of the proactive environment. To implement display \nservices, it is necessary to implement a system that is \ncapable of displaying different types of content and also \nallows control over what content is displayed. The \nphysical output media is preferably a large screen or a \nprojection; however, we recognized that in many settings \na standard workstation, that would usually be displaying \na screen saver when idle, could also be used. Additionally \nto the screen output, audio output is also provided by the \ndisplay service. \nThe following content types are supported by the \ndisplay services: \n\u2022 Web-pages \n\u2022 PDF documents \n\u2022 Images \n\u2022 PowerPoint Slides \n\u2022 Audio streams and files \n\u2022 Videos streams and files \n\u2022 Television \n \n2.2.2. Pendle wearable device. The content of the \ndisplay services adapt to the users that are currently in the \nenvironment or which have recently been in the \nenvironment. Furthermore, users are able to explicitly \ninteract with the environment. In our design, we use a \nminimal wearable device which was explicitly designed \nto mediate between user and proactive environment.  \nThe Pendle device incorporates storage, wireless \ncommunication, and simple gesture recognition. With \nthese basic properties it can store the user\u2019s interests and \nit provides an easily accessible explicit interface to the \nenvironment. It is designed as a badge which can be worn \non a ribbon around the neck (Figure 1, Figure 4). \nTwo types of information are stored on the Pendle. \nThe first is a set of keywords which reflect the user\u2019s \ninterest.  The second is a set of URLs which the user likes \nquick and easy access to on public displays. \nTo allow the environment to proactively adapt to the \nusers, the Pendle broadcasts continuously search words \nto the environment. This is a type of implicit interaction \nwith the environment. When the Pendle is explicitly \nmoved by the user gestures are recognized which allows \nthe displaying of the URLs stored on the Pendle. The \nPendle is described in more detail in Chapter 3. \n \n \n \nFigure 2. Architecture of the  \nProactive Environment. \n \nFigure 1. The Pendle Wearable Device \n2.2.3. Finding content. At the centre of the system is a \ncomponent that transforms the information that is \ncontinuously broadcasted from the wearable devices into \nmeaningful content, which can then be shown on the \ndisplay services.  \nThe basic function of this component is to relate \nmessages from the wearable device to meaningful URLs \nwhich can be displayed. Dependent on the messages \nreceived this may only be forwarding a URL received at a \nparticular location to a selected screen. If, instead, \nkeywords are received from the wearable devices, the \nsystem uses available information sources - such as a \nsearch engine - and finds appropriate content. This \ncontent relates to the set of search terms received in a \nparticular spatial area (e.g. a room) in the building. \n \n2.2.4. Receivers, Connectors and Gateways. To realize \nsuch a system, the information that is wirelessly \ntransmitted from the wearable devices has to be handed \nto a module which finds appropriate content, and this \ncontent description has to be given to the appropriate \ndisplay service.  \nThe physical location where the information from the \nwearable device is received, as well as the physical \nlocation of the display service is of great importance. In \ncontrast, the module which relates received messages to \nmeaningful content can be situated anywhere.  \nTo facilitate communication, receivers for the \nwireless communication are connected to the local \nnetwork. Messages received are then extended by the \nphysical location of where they were received and \nbroadcasted into the local network as UDP packets. \nThe module that relates information to content \nreceives these messages resolves them into an URL and \nsends it to the most appropriate display service. The \ncontent pointed to by this URL is then fetched directly by \nthe display service using its available network \nconnection. \n2.2  Data Flow \nThe data flow between Pendles and infrastructure is \nshown in Figure 3. Users wearing the device broadcast \nchosen keywords or URLs. These messages are picked up \nby receivers in the area which is covered by the wireless \ntransmission. Each receiver broadcasts the messages \nreceived, extended by an ID and the physical location to \nthe local Ethernet. In the network one or more modules \nreceive the messages collected by the receivers and can \ndetermine whether or not to use them and to transform \nthem into valid URLs. After creating a valid URL, a \ndisplay service (or a set of them) is selected and a request \nto display the URL is sent. The valid URL is then \nreceived by a display service and - depending on the state \nof the service at that time - the URL content is fetched \nand displayed. \n \n \n \nFigure 3. Component interaction \n \nIn the following sections we describe in more detail \nthe design and implementation of the wearable device, \nthe content finder, and the display service. \n3. The Pendle Wearable Device \nWe designed a wearable device - called a Pendle - to \nserve as mediator between users and proactive \nenvironments. By wearing it the user influences the \nenvironment\u2019s proactive behavior. It also serves as a point \nof interaction by providing a simple user interface to \nservices in the environment. The name Pendle allures to \nits realization in the form-factor of a pendant, as well as \nthe fact that environment services are dependant on this \ndevice in order to operate. \n3.1 Concept and Basic Operation \nThe Pendle is a personal wearable device. It stores a \nuser\u2019s predefined information which it can wirelessly \ntransmit to the environment when it is being worn. This \nenables services to \u2018sense\u2019 the user\u2019s presence and \npreferences. An interface for explicit interaction with \nthese services is provided through gesture recognition \nfeature built into the device. Gestures can be associated \nwith commands that can be performed on specific \nservices. \nWe chose to realize the Pendle as a badge on a \nribbon that can comfortably be worn around the neck. \nThe Pendle is a self-contained device with a small size \nand weight in order to make it a relatively unobtrusive \nitem to wear, and the ribbon allows the user to \ncomfortably hold and manipulate the object while \nwearing it.  \n  \n \nFigure 4. User interacting with  \nProactive Displays \n \nUsers interact implicitly with the public displays \nservices in the environment simply by wearing their \nPendles. They can manipulate it by performing gestures \nto explicitly control the displays and to access the content \nof a predefined set of information. \nAlthough similar to the Gesture Pendant [2] in \nconcept and form-factor, we wanted to use the Pendle as \nmore than an artifact that provides interaction with our \nsystem. We wanted to use the fact that users would wear \nthe Pendle as a way to measure some of their implicit \nactions and therefore allow the system to proactively \nrespond. \n3.2 Interaction Modes \nThe Pendle is a device for interacting with services \nprovided by an augmented environment. It supports three \ninteraction modes: \n \nInactive: If the device is not being worn it is inactive. \nIn this mode no interaction takes place between the \ndevice and the environment. \n \nImplicit: In implicit mode the Pendle wirelessly \ntransmits information chosen by the user to the \nenvironment. This mode is in effect as long as the \ndevice is being worn by the user (unless the explicit \nmode becomes active). The environment services use \nthe information contained in the profile to adapt their \nbehavior to suit the user. \n \nExplicit: The explicit mode is in effect whenever the \nuser performs gestures with the device. Each gesture \nrepresents a specific command. As soon as the Pendle \nrecognizes a gesture, it transmits the corresponding \ncommand to the environment.   \n4. The webPendle Application \nWe developed an application called webPendle \nwhere the Pendle device and related infrastructure is used \nto create interesting proactive\/interactive informative \nenvironments. In our application display resources \u2013 such \nas large plasma screens or PC monitors - in the \nenvironment proactively display information from the \nweb that is relevant to the users near them. This has to be \ndone in a way that the system implicitly displays websites \nthat might be interesting, while also allowing the user to \nexplicitly request a particular website of their choice. \n4.1 Keywords to enable implicit behavior \nIn order to provide a satisfactory user experience \nwhile the system behaves proactively, it is important that \nthe system has a good idea of a user\u2019s likes and dislikes. \nIn order to do this, the user describes herself as a set of \nkeywords to be stored on the Pendle. These keywords, \nalong with the keywords of other users in the \nenvironment, will be used to find content that should \nreflect the communal interests. \n4.2 Bookmarks for explicit interaction \nThe users are also able to use the Pendle to call up \npredefined URLs to be displayed, similar to accessing \nwebsites through a web-browser\u2019s \u2018favorites\u2019  or \nbookmark list. In theory this list could be linked and \nupdated from the user\u2019s web browser on their PC, \nallowing them to carry their bookmarks with them. \n4.3  The meaning of gestures \nTo provide a vehicle for explicit interaction with the \nsystem, we assigned the following meanings to simple \ngestures that can be performed with the Pendle: \n\u2022 Wearing the Pendle marks the beginning of the \nimplicit interaction. \n\u2022 Picking up the Pendle marks the beginning of \nthe explicit interaction. \n\u2022 Holding up the Pendle causes display services in \nthe vicinity to display the next favorite URL \nstored in the its memory \n\u2022 Shaking the Pendle removes URLs that are \ncurrently on display.  \n\u2022 Letting go of the Pendle marks the end of the \nexplicit interaction. \n\u2022 Taking off the Pendle marks the end of the \nimplicit interaction. \nBy limiting the number of gestures to a small number, we \nare able to recognize these with almost perfect accuracy \nwhile requiring a minimal device implementation. This is \nfurther described in section 5.1. \n4.4 Displaying content \n     The display service moderates what content is shown, \nand for how long it is displayed.  It displays both \nexplicitly requested and implicitly (keyword-based) \nURLs. By default, the display service acts in a proactive \nmanner, while explicit requests for URLs cause \ninteractive behavior. \n     URLs that have been marked by the content-finder as \nbeing the result of a keyword search are fetched from the \ninternet by the display service and displayed for at least \ntwo minutes. After this period, the content is replaced by \nnew incoming URLs. This is done in order to maintain a \ndynamic informative environment that is highly relevant \nto users in the vicinity, while at the same time ensuring \nthat the information is not changed so frequently as to \nprove distracting.  \nURLs that are marked as being the result of a user \nmaking an explicit gesture have priority over implicit-\nURLs, and will display immediately upon reception of a \nrequest. Also, a \u201cremove URL\u201d command (shaking the \nPendle) or a user leaving the environment causes any \ncontent explicitly requested by that user to disappear and \nthe service goes back to displaying keyword-based \nURLs.  \nIn the case when the URL points to a media clip, the \nmedia clip will be played in its entirety and then the \nservice will go back to displaying the last static URL \nreceived. \n5. Implementation \nThis section describes our implementation of the \nwebPendle application as was deployed in a communal \nrecreation area of our department. \n5.1 Pendle Implementation \nWe implemented a prototype of the Pendle using a \nmini Smart-Its device coupled with a sensor board [6] \n(Figure 5, Figure 6). The Smart-It core provides a PIC \nmicrocontroller capable of storing and running small \nprograms. It also has 8K of non-volatile RAM through an \nI2C connection, a wireless transceiver and a 3V lithium \ncell. \nThe smart-its core board is augmented through an \nadd-on board with two sensors: a touch sensor (QT110) \nand an ADXL202 that provides two axis of acceleration. \nThe combined components are 45mm x 45mm x 20mm, \nweighing about 40g. \nWe use the acceleration sensor to measure both static \nand dynamic acceleration. Dynamic acceleration is \ncaused by the user or Pendle moving, and is useful for \ndetermining the implicit context (sitting, moving) and \nalso some explicit gestures, like shaking the Pendle. \nStatic acceleration refers to the reading that gravity \nproduces on the sensor. The touch sensor is used to \ndetermine when the user is holding the Pendle, important \nfor establishing where the explicit interaction begins and \nends. \n \n \n \nFigure 5. Pendle implementation using a Smart-\nIts Device with Sensor add-on. \n \nAs the Pendle is designed to be able to work \nindependently from its environment, algorithms cannot \nrely on off-board processing and therefore must be kept \nminimal. A peak-based feature extraction method similar \nto the one described in [21] is used for this task. By \nsupporting only a small number of possible gestures it is \npossible to detect these with very high accuracy. \n \nMini Sensor AddOn\nAcceleration\nSensor\nTouch\nSensor\nMini Smart-Its Core\nMemoryWirelessTransceiver PIC MCU\nGesture\nBadge\nSystem\nArchitecture\n  \n \nFigure 6. Pendle Device Architecture. \n \nThe non-volatile memory on the smart-its core is \nused to store the keywords and URLs as defined by the \nuser. Although we considered the possibility of using the \nPendle as a pointer to a place on the network or internet \nwhere this information could be stored - in a manner \nsimilar to the Active Badge [4] - we chose to store the \ndata directly on the Pendle. We liked the idea of users \ncarrying around the data in their physical devices as it \nallowed for an easier use of the Pendle over different \nenvironments. It also facilitates some of the future \nimprovements we have considered for the system, such as \nthe ability to exchange URLs or keywords with other \nusers without the need for an internet connection to be \npresent, as exchange of information is possible between \nPendles without the need for external infrastructure. \nOur system also differs from the Active Badge in that \nit operates anonymously.  Particularly in proactive \noperation, there is no need to differentiate between \nindividual users as the information displayed is the result \nof the shared interests of the people in the environment. \nPseudonyms or IDs are assigned to every Pendle to allow \nfor explicit control, but they are not necessarily linked to \na user\u2019s actual identity. The anonymity option was \nintroduced to deal with the privacy concerns that users of \na system like this are likely to have. \n5.2 Receivers  \nReceivers are the link between the wireless \ncommunication used by the Pendles and the local \nEthernet network used by the rest of the components. As \nsuch, they are deployed throughout the environment \nwhere the display services reside.  \nThe receivers in our implementation are Smart-Its \ndevices connected to PCs already present in the \nenvironment that provides the Ethernet connectivity. We \nare also experimenting with devices that can directly \ninterface with the LAN, removing the need for a PC. The \nsoftware running in the MCU of the device simply \nreceives wireless transmissions, augments them with the \nID of the receiver and broadcasts it onto the network to \nbe picked up by the Content-Finder module. \n5.3 Content-Finders \nWe implemented the content-finder as a program that \nresides on a machine connected to the same local network \nas the receivers and display services. Its physical location \nis not important. \nThe program listens on a defined network port for \nany broadcasts from the receivers. We chose to use UDP \nas the communications protocol as it easily enables us to \nbroadcast the information to several content-finders on \nthe same network. As it is intended for use over a local \narea network, we expect no problems in terms of packet \nloss. \nThe content-finder uses a table that maps receiver \nID\u2019s to available display services. It is used to determine \nwhich display services to send particular commands to. \nThis is initially a static mapping, but a possible \nimprovement would be to examine ways in which this \ntable could be dynamically updated as new devices \nbecome available on the network. In our implementation, \nthe table simply maps receivers to display services in \ntheir same physical location. \nIn the case of URLs or commands being received, \nthey are simply forwarded on to the display services \ndetailed on the available services table. These are marked \nas being the result of an explicit interaction, to \ndifferentiate them from implicit-URLs to allow the \ndisplay services to recognize them as such and deal with \nthem accordingly. \nHowever, any keywords that are received are placed \ninto an array. They are maintained in the array as long as \nthey are periodically resubmitted by the Pendle, \notherwise their validity expires five minutes after the last \nsubmission, and they are removed. This is done to \nmaintain a dynamic and highly relevant keyword pool, \nreflective of the chosen interests of people in the \nenvironment or ones that have recently been in the \nenvironment. \nWe then faced the task of converting these keywords \ninto valid URLs that could be retrieved and rendered by \nthe display services.  Our first approach was to randomly \nsubmit a few keywords from the pool to the Google \nsearch engine (using the Google Web API). The most \nrelevant URL to be found \u2013 according to Google\u2019s \nranking system \u2013 is chosen to be displayed. This resulted \nin varied if somewhat random and sometimes unrelated \ncontent being displayed. Other approaches were also \nexamined, for example the algorithm outlined in Table 1 \nwhich yielded more relevant results.  \n \n \n1. Randomly select 10 keywords from \nthe pool \n \n2. Submit keywords to search engine \n \n3. If no URL is retrieved, go to \nstep 5. \n \n4. If URL has low relevancy score, \ngo to step 5. Else, go to step 6. \n \n5. Remove one keyword from \nsubmission list. Go to step 2. \n \n6. Fetch and display URL. \n \nTable 1. Content-finding Algorithm \n \nBy implementing different keyword-to-URL \ntranslation methodologies, the system can be changed \neither to implicitly display highly relevant content for one \nor a few of the users in the environment, or more vaguely \nrelevant content for a wider number of users.  \nThis program is intended to operate autonomously \nand require no front-end or administration. However the \nlocation handling and search strategies can be modified to \nfine-tune the behavior of the system. \n5.4   Display Services \nWe developed a program to encapsulate the services \noffered by both the Microsoft Internet Explorer Control \nand the Microsoft Media Player Control in a single \napplication. Our program is able to differentiate between \nimplicitly and explicitly requested URLs\/commands, \nmaintain a list of recently-accessed URLs, and display \ndifferent types of content using the most appropriate of \nthe two applications. We found that by doing this, we \nwere able to display any of the content-types we were \ninterested in accessing. Adobe Acrobat documents are \naccessed through the Internet Explorer control. Also, \nwithin our institution major television channels are \navailable as streaming video, so there was no need to \ntreat this type of media different form a video stream \nwhich is dealt with the Media Player control. \n6. Discussion \nProactive environments currently receive \nconsiderable interest, as the research field moves forward \nfrom early demonstrations of smart rooms [7] to large \ninitiatives investigating ambient intelligence [8]. One \npromiment example of proactive environments is the \nReactive Room project, which took a thorough HCI \nperspective on augmented environment concerning itself \nwith issues of predictability and controllability [17,18]. In \nthe Reactive Room project, a room used for video \nconferencing was made aware of the context of both \nusers and objects in the room for the purpose of relieving \nthe user of the burden of controlling the objects.  \nYet pure proactive environments, in which users \nhave no influence over the environment\u2019s behavior, are \nproblematic for two reasons. First, pure proactive \nenvironments are not likely to facilitate any significant \npersonalization of the user experience. Secondly, many \nscenarios of proactive environments appear to be \nunrealistic and even undesirable from a user experience \nperspective. They tend to assume users will want their \nenvironments to act on their behalf and that users will \nagree with the proactive behaviours they exhibit. \nHowever, HCI studies of adaptive interactive systems \nhave consistently emphasized controllability as \nfundamental usability issue [9].  \nWe address the problem of pure proactive \nenvironments by introducing the Pendle personalized \nwearable device. A core aspect of the presented research \nis to integrate the distinct advantages of personal \nwearable devices with those that proactive environments \noffer. The benefits of such integration have previously \nbeen discussed by Rhodes et al. who also sketched a \nvariety of application scenarios [10]. Their work \nhighlights the distinct advantages of wearable vs. \nubiquitous facilities and their combination (e.g. \npersonalization and localization). Our work takes this \nforward with the focus on the interactions afforded by a \ncombination of personal interaction devices and proactive \nenvironments. There has been further work that \ninvestigates interactions between personal devices and \nenvironment-based facilities, however generally with a \nfocus on explicit interaction (e.g. [11] on use of personal \ndevices to control shared displays, and [12] on interactive \napplications migrating across personal and public \ndevices). \nA different emphasis in combination of wearable and \nenvironment-based technologies for interactive services \nis largely explored in many ubiquitous computing \nprojects (e.g. [13]). Here, the focus is generally on \nenvironment-based services that integrate wearable \ncomponents such as Active Badges [14] for identification \nand localization of users, for example to allow users to \nsummon their remote desktops to nearby displays [15]. \nKey to our approach is to foresee a wearable device \nthat provides for casual interaction on the basis of an \neasy-to-use repertoire of hand gestures. In related work, \nStarner et al have proposed a wearable gesture interface, \nlike ours in the form-factor of a pendant [16]. Their \nGesture Pendant is designed for explicit environment \ncontrol with user-definable gestures performed in front of \nthe pendant. Gesture recognition is based on computer \nvision, requiring significantly more computational \nresources than provided in our compact device design. \nRekimoto proposed a simple gesture input technique that \nis based on a wrist-mounted device with acceleration \nsensor and sensor electrodes [17]. GestureWrist can \nrecognize several variations of gestures. However as it is \ndesigned to be always on, i.e. not foreseeing an explicit \ntrigger mechanism, it can yield unintended recognitions. \nWe believe our approach affords significantly more \ncasual interaction (\u2018fingering a device worn around the \nneck\u2019) and lower cognitive load (no hands-ear \ncoordination). \nMany projects in the area of proactive environments \nemploy computer vision infrastructure for external \nobservation of users. This raises concerns with respect to \nintrusion on privacy that we believe need to be carefully \nweighed. Examples are the EasyLiving project visually \ntracking users [19] and work of Darrel et al proposing \nface detection in augmented environments [20]. We \naddress the privacy problem by basing environment \nadaptation and personalization on information willingly \nand explicitly provided by users. The use of personal \ndevice that are under control of the user at all times give \nthe user control over what kind of information is \ndisclosed at which time.  \n7. Conclusion \nIn this paper we have proposed a proactive \nenvironment controlled by personal wearable devices, \nThe Pendle device serves as mediator between user and \nproactive environment. It provides the user with control \nand influence over their environment\u2019s proactive \nbehaviour on the basis of a simple user interface that \nlends itself to casual interaction and a smooth integration \nof environment-controlled interaction (experienced by the \nuser as implicit interaction, triggered by their presence) \nand user-controlled interaction (i.e. explicit interaction to \ndirectly manipulate the behaviour of the environment). \nThe result is an unobtrusive adaptation of \ninformation displays according to the interests of nearby. \nAt the same time, the Pendle device enables users to \noverride proactive behaviors to quickly and easily access \ncontent on the internet. As such, the system improves the \ninteraction between the users and the valuable displays \nservices in their vicinity without making it obtrusive or \ndistracting to do so, while at the same time creating a \nmore interesting space to inhabit. \nAs it stands, the system allows for exploration into \nseveral interesting ways in which it can be improved. In \nparticular, it would be interesting to examine different \nmethodologies for locating the most appropriate displays \nservices or more advanced algorithms for finding relevant \ncontent to display.   \nReferences \n[1]  F. Bennett, T. Richardson, and A. Harter. \u201cTeleporting \u2013 \nmaking applications mobile.\u201d Proc. Of the IEEE Workshop on \nMobile Computing Systems and Applications, pages 82-84, \nSanta Cruz, California, Dec. 1994. IEEE Computer Society \nPress. \n[2]  T. Starner, J. Auxier, D. Ashbrook and M. Gandy.  \u201cThe \nGesture Pendant: A Self-illuminating, Wearable, Infrared \nComputer Vision system for Home Automation Control and \nMedical Monitoring.\u201d International Symposium on Werable \nComputers, Atlanta, GA: IEEE Computer Society, pp. 87-94 \n(2000). \n[3]  N. Sawhney, S. Wheeler, and C. Schmandt. \u201cAware \nCommunity Portals: Shared Information Appliances for \nTransitional Spaces.\u201d CHI 2000, April , 2000. \n[4] R. Want, \u201cThe Active Badge Location System,\u201d Olivetti \nResearch Laboratory, ACM Trans. Information Sys. Vol. 10, no \n1, 1992, pp.91-102 \n[5]  O. Cakmakci, J. Coutaz, K. Van Laerhoven, and H.-W. \nGellersen. \"Context Awareness in Systems with Limited \nResources\". In Proc. of the third workshop on Artificial \nIntelligence in Mobile Systems (AIMS), ECAI 2002, Lyon, \nFrance. 2002. pp. 21-29. \n[6]  The Smart-Its Project. http:\/\/www.smart-its.org  \n[7] A. Pentland. Smart Rooms, Sci Amer., April 1996. \n[8] European Research Consortium for Informatics and \nMathematics. Ambient Intelligence, ERCIM News No.47, Oct. \n2001. http:\/\/www.ercim.org\/publication\/ Ercim_News\/enw47\/ \n[9] A. Jameson. Adaptive Interfaces and Agents. In J.A. Jacko, \nA. Sears (Eds.) The Human-Computer Interaction Handbook: \nFundamentals, Evolving Technologies and Emerging \nApplications. Lawrence Erlbaum Assoc, 2002 \n[10] B.J. Rhodes, N. Minar and J. Weaver. Wearable \nComputing Meets Ubiquitous Computing: Reaping the best of \nboth worlds. In Proc. of 3rd Intl. Symp. on Wearable Computers \n(ISWC '99), San Francisco, CA, Oct. 1999. \n[11] B.A. Myers, H. Stiel, and R. Gargiulo. Collaboration \nUsing Multiple PDAs Connected to a PC. In Proc. CSCW'98: \nACM Conference on Computer-Supported Cooperative Work, \nNov 1998, Seattle, pp. 285-294. \n[12] Greenberg, S., Boyle, M. and LaBerge, J. PDAs and \nShared Public Displays: Making Personal Information Public, \nand Public Information Personal. Personal Technologies, Vol.3, \nNo.1, March 1999, pp. 54-64, Springer-Verlag. \n[13] M. Addlesee, R. Curwen, S. Hodges, J. Newman, P. \nSteggles, A. Ward, and A. Hopper. Implementing a Sentient \nComputing System. IEEE Computer, Vol 34, No 8, pp. 50-56, \nAugust 2001. \n[14] A. Harter and A. Hopper, A Distributed Location System \nfor the Active Office . IEEE Network, Vol. 8, No. 1, January \n1994. \n[15] F. Bennett, T. Richardson and A. Harter. Teleporting - \nMaking Applications Mobile. Proc. WMCSA\u201994, Workshop on \nMobile Computing Systems and Applications, Santa Cruz, Dec. \n1994. \n[17] J. Rekimoto. GestureWrist and GesturePad: Unobtrusive \nWearable Interaction Devices. Proc. ISWC\u201901, Fifth Intl \nSymposium on Wearable Computers, October 2001, Zurich, \nSwitzerland. \n[16] T. Starner, J. Auxier, D. Ashbrook and M. Gandy.  The \nGesture Pendant: A Self-illuminating, Wearable, Infrared \nComputer Vision system for Home Automation Control and \nMedical Monitoring. Proc. ISWC 2000, Intl. Symp. on \nWearable Computers, Oct 2000, Atlanta, GA, pp. 87-94 (2000). \n[17] J.R. Cooperstock, K. Tanikoshi, G. Beirne, T. Narine and \nW. Buxton. Evolution of a Reactive Environment. Proc. of \nCHI'95, Conference on Human Factors in Computing Systems, \nDenver, May 1995.  \n[18] Cooperstock, J., Fels, S., Buxton, W. & Smith, K. Reactive \nenvironments: Throwing away your keyboard and mouse, \nCACM 40(9), 1997, pp. 65-73. \n [19] J. Krumm, S. Harris, B. Meyers, B. Brumitt, M. Hale, S. \nShafer. Multi-Camera Multi-Person Tracking for EasyLiving. \nIEEE Workshop on Visual Surveillance, July 2000. \n[20] T. Darrell, K. Tollmar, F. Bentley, N. Checka, L.-P. \nMorency, A. Rahimi, and A. Oh. Face-responsive interfaces: \nfrom direct manipulation to perceptive presence, Proc. \nUbicomp 2002, Intl. Conf. on Ubiquitous Computing, \nGothenburg, Oct 2002. \n[21] A.Y. Benbasat and J. A. Paradiso. An Inertial \nMeasurement Framework for Gesture Recognition and \nApplications. Proc. GW 2001, International Gesture Workshop, \nLondon, UK, 2001. \n"}