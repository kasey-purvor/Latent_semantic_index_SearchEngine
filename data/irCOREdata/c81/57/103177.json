{"doi":"10.1109\/IJCNN.2010.5596839","coreId":"103177","oai":"oai:epubs.surrey.ac.uk:3036","identifiers":["oai:epubs.surrey.ac.uk:3036","10.1109\/IJCNN.2010.5596839"],"title":"Simulating the Effects of Cortical Feedback in the Superior Colliculus with Topographic Maps","authors":["Pavlou, A","Casey, MC"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":null,"abstract":"The superior colliculus (SC) is a neural structure found in mammalian brains that acts as a sensory hub through which visual, auditory and somatosensory inputs are integrated. This integration is used to orient the eye's fovea towards a prominent stimulus, independently of which sensory modality it was detected in. A recently observed aspect of this integration is that it is moderated by cortical feedback. As a key sensorimotor function integrating low-level sensory information moderated by the cortex, studying the SC may therefore enable us to understand how natural systems prioritize sensory computation in real-time, possibly as a result of task dependent feedback. In this paper, we focus on such a biological model. From a computational perspective, understanding this combination of bottom-up processing with top-down moderation in a model is therefore appealing. We present for the first time a behavioral model of the SC which combines the development of unisensory and multisensory representations with simulated cortical feedback. Our model demonstrates how unisensory maps can be aligned and integrated automatically into a multisensory representation. Results demonstrate that our model can capture the basic properties of the SC, and in particular they show the influence of the simulated cortical feedback on multisensory responses, reproducing the observed multisensory enhancement and suppression phenomena compared to biological studies. This suggests that our unified competitive learning approach may successfully be used to represent spatial processing that is moderated by task, and hence could be more widely applied to other, task dependent processing","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:epubs.surrey.ac.uk:3036<\/identifier><datestamp>\n      2017-10-31T14:07:37Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      74797065733D636F6E666572656E63655F6974656D<\/setSpec><setSpec>\n      6469766973696F6E733D656E67616E64706879736963616C736369656E636573:436F6D707574696E67<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/epubs.surrey.ac.uk\/3036\/<\/dc:relation><dc:title>\n        Simulating the Effects of Cortical Feedback in the Superior Colliculus with Topographic Maps<\/dc:title><dc:creator>\n        Pavlou, A<\/dc:creator><dc:creator>\n        Casey, MC<\/dc:creator><dc:description>\n        The superior colliculus (SC) is a neural structure found in mammalian brains that acts as a sensory hub through which visual, auditory and somatosensory inputs are integrated. This integration is used to orient the eye's fovea towards a prominent stimulus, independently of which sensory modality it was detected in. A recently observed aspect of this integration is that it is moderated by cortical feedback. As a key sensorimotor function integrating low-level sensory information moderated by the cortex, studying the SC may therefore enable us to understand how natural systems prioritize sensory computation in real-time, possibly as a result of task dependent feedback. In this paper, we focus on such a biological model. From a computational perspective, understanding this combination of bottom-up processing with top-down moderation in a model is therefore appealing. We present for the first time a behavioral model of the SC which combines the development of unisensory and multisensory representations with simulated cortical feedback. Our model demonstrates how unisensory maps can be aligned and integrated automatically into a multisensory representation. Results demonstrate that our model can capture the basic properties of the SC, and in particular they show the influence of the simulated cortical feedback on multisensory responses, reproducing the observed multisensory enhancement and suppression phenomena compared to biological studies. This suggests that our unified competitive learning approach may successfully be used to represent spatial processing that is moderated by task, and hence could be more widely applied to other, task dependent processing.<\/dc:description><dc:type>\n        Conference or Workshop Item<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        attached<\/dc:rights><dc:identifier>\n        http:\/\/epubs.surrey.ac.uk\/3036\/2\/2010_pavlou_casey_simulating_cortical_feedback.pdf<\/dc:identifier><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/epubs.surrey.ac.uk\/3036\/4\/licence.txt<\/dc:identifier><dc:identifier>\n          Pavlou, A and Casey, MC   Simulating the Effects of Cortical Feedback in the Superior Colliculus with Topographic Maps  In: International Joint Conference on Neural Networks (IJCNN) 2010, 2010-07-18 - 2010-07-23, Barcelona.     <\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/IJCNN.2010.5596839<\/dc:relation><dc:relation>\n        10.1109\/IJCNN.2010.5596839<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/epubs.surrey.ac.uk\/3036\/","http:\/\/dx.doi.org\/10.1109\/IJCNN.2010.5596839","10.1109\/IJCNN.2010.5596839"],"year":null,"topics":[],"subject":["Conference or Workshop Item","PeerReviewed"],"fullText":" \n \n \nSimulating the Effects of Cortical Feedback in the Superior \nColliculus with Topographic Maps \nAthanasios Pavlou and Matthew Casey \n \nAbstract\u2014The superior colliculus (SC) is a neural structure \nfound in mammalian brains that acts as a sensory hub through \nwhich visual, auditory and somatosensory inputs are \nintegrated. This integration is used to orient the eye's fovea \ntowards a prominent stimulus, independently of which sensory \nmodality it was detected in. A recently observed aspect of this \nintegration is that it is moderated by cortical feedback. As a \nkey sensorimotor function integrating low-level sensory \ninformation moderated by the cortex, studying the SC may \ntherefore enable us to understand how natural systems \nprioritize sensory computation in real-time, possibly as a result \nof task dependent feedback.  In this paper, we focus on such a \nbiological model.  From a computational perspective, \nunderstanding this combination of bottom-up processing with \ntop-down moderation in a model is therefore appealing. We \npresent for the first time a behavioral model of the SC which \ncombines the development of unisensory and multisensory \nrepresentations with simulated cortical feedback.  Our model \ndemonstrates how unisensory maps can be aligned and \nintegrated automatically into a multisensory representation. \nResults demonstrate that our model can capture the basic \nproperties of the SC, and in particular they show the influence \nof the simulated cortical feedback on multisensory responses, \nreproducing the observed multisensory enhancement and \nsuppression phenomena compared to biological studies.  This \nsuggests that our unified competitive learning approach may \nsuccessfully be used to represent spatial processing that is \nmoderated by task, and hence could be more widely applied to \nother, task dependent processing. \nI. INTRODUCTION \nhe superior colliculus (SC) is a neural structure found in \nmammalian brains that acts a sensory hub through \nwhich visual, auditory and somatosensory inputs are \nintegrated creating multisensory representations of the \nsensory space [1]. The resulting multisensory topographic \nmap is used to direct eye saccades and head movements [2] \nso that the fovea is focused on a stimulus rapidly. In \nparticular, the SC prioritizes its output to respond to stimuli \nfrom the same spatial location, but registered from different \nsenses (multisensory enhancement), whereas asynchronous \nor spatially unrelated multisensory inputs have lower \npriority (suppression). Interestingly, while this processing to \nshift gaze occurs in the midbrain, prior to cortical \nprocessing, the enhancement or suppression of multisensory \nstimuli is affected by descending afferents from cortical \nareas, such as the visual, auditory and somatosensory \ndivisions of the anterior ectosylvian sulcus in the cat [3,4]. \nThe influence of these afferent connections is beginning to \nsuggest that the integration of sensory stimuli in the SC \ndepends upon behavioral context [4], demonstrating again \nthe important nature of feedback within the brain (see [5,6] \nfor two examples in vision). \n \nManuscript received January 26, 2010. A. Pavlou and M. C. Casey are \nwith the Department of Computing, University of Surrey, Guildford, Surrey, \nGU2 7XH, UK, (e-mail: athanasios.pavlou@surrey.ac.uk, \nm.casey@surrey.ac.uk). \n \n \nFig. 1.  Sensory and cortical information flows in the SC.  The superficial \nlayer of the SC is directly connected to the optic tract, while the deep layers \nreceive auditory (from the inferior colliculus) and somatosensory input.  \nCortical feedback moderates the processing in the deep layer and influences \nthe motor outputs. \n \nAs a mechanism for prioritizing the use of limited sensory \nresources, the SC is an evolutionary stable structure in \nvertebrates. It consists of a laminated structure of superficial \nand deep layers [2].  The superficial layers of the SC process \nvisual stimuli directly from the optic tract to form a \ntopographic map representing visual space.  In the deep \nlayers, auditory (from the inferior colliculus) and \nsomatosensory representations of space are combined with \nthe visual map to form a multisensory representation.  This \nis used to form appropriate motor instructions to orient the \neyes, head and body to focus on a selected stimulus, \nmoderated by cortical feedback (Figure 1). \nAs a sensorimotor structure which is evolutionary stable, \nand which is moderated by cortical feedback, the SC is an \ninteresting part of the brain to study computationally. As \nwell as being sufficiently understood to permit the \ndevelopment of a computational model, the SC exhibits key \nprinciples that we wish to embody within artificial agents: \nseamless sensory integration coupled with motor control, \nT \n \n \n \nmoderated by behavioral context. If we can understand and \nduplicate the key principles of how the SC can achieve these \ntasks, not only can we use the model to improve our \nneurophysiological knowledge, but we can perhaps also \ndevelop new computational paradigms for artificial agents \nwhich are capable of operating in real-time on complex, \nreal-world stimuli. \nA milestone towards this goal is to achieve the \ndevelopment of coordinate systems for each of the sensory \nmodalities as well as their successful alignment and \nintegration, moderated by cortical feedback. However, as \nyet, no sufficiently detailed model of the SC has been \ndeveloped to achieve all of these. For example, until \nrecently, most models have focused on the simulation of \nmultisensory effects observed within the deep layers of the \nSC [7-9], rather than on the unisensory space.  However, \nthree recent models have looked more closely at unisensory \nrepresentations [10,11] and development [12], respectively. \nMagosso et al [10] considered the spatial properties of the \nsensory spaces, but not from a developmental perspective. \nThey provided a modularized architecture of three neural \nnetworks. The first two represented unimodal visual and \nauditory neurons whilst the third represented multisensory \nneurons in the SC. Their model accounted for a number of \nbehavioral multisensory phenomena within the SC, however \nit would be difficult to employ for practical use within \nartificial agents because it lacks the capability of developing \ncoordinates which can then be aligned in later processing \nstages. A similar model was developed by Casey and Pavlou \n[11], who demonstrated how two unisensory maps could be \ncoupled together through association to develop and align \ncoordinate systems.  Although their model was able to \ndevelop unisensory spaces and align their coordinates to the \nmultisensory space, it employed multiple learning \nalgorithms compromising its biological plausibility. Also, \nthe multisensory map of their model was the product of a \nmanual addition of the translated (aligned) unisensory maps \nand it did not explore how integration could be developed, \nor how the cortex can influence enhancement or \nsuppression. \nIn contrast, Huo and Murray [12] used Spike Timing \nDependent Plasticity (STDP) to model alignment (or rather \nrealignment) of audio and visual maps in the SC with \nfeedback.  Their model consisted of representations for the \nretina, inferior colliculus (IC), which is responsible for \ngenerating an audio representation of space, and the SC.  In \nthe SC, a model interneuron was used to initiate learning in \nthe IC. While this model perhaps lacks the scale of the \nprevious two more practical models, it does provide a \nbiologically plausible representation of plasticity that is \nrobust to changing environmental conditions.  Furthermore, \nit demonstrates the dependency and feedback between \ndifferent parts of the midbrain: the IC and the SC.  However, \nalthough this model does demonstrate the key characteristics \nof adaptive integration through feedback, it does not \nsufficiently demonstrate how unisensory maps can develop \nand be aligned to produce multisensory integration \nmoderated through cortical feedback. \nIn this paper, we report the development and evaluation of \na biological model which brings together the key principles \nof the SC into a single architecture using a generalized \nassociative learning model. The model includes: 1) \ndevelopment of unimodal topographic maps, 2) alignment of \nthe unimodal maps and development of a multisensory \nrepresentation under a common learning algorithm; 3) \nmultisensory response suppression upon presentation of \nspatially non-coincident stimuli and 4) the effects of \nenabling and disabling cortical feedback on multisensory \nresponses. While this model is evaluated on abstract stimuli \nin the same way as all previous models, it does demonstrate \nhow a simple neuronal model can be used to process and \nintegrate sensory stimuli seamlessly with the essential \ninfluence of the cortex, as compared to biological data. \nII. MODEL SPECIFICATION \nOur modeling approach of the superior colliculus is \ncomprised of three distinct layers. The architectural design \nsimulates the actual SC structure which demonstrates \nincreasing complexity from unisensory to multisensory \nneurons as information flows from the superficial to the \ndeep layers. A similar approach with two unisensory maps \nbeing connected to a third multisensory map was also \nfollowed by Magosso et al [10]. Our architecture is a \nmodification of the layered approach used by Armony et al \n[13], who used associative learning within a layered \nmodularized architecture to model fear conditioning in the \namygdala. Here, we extended this approach to be able to \nlearn topography using Hebbian learning [11]. This results \nin modules at lower and higher levels forming their own \ncoordinate systems, which is a key property of the SC. \n \nFig. 2.  Schematic of the superior colliculus model. \n \nFigure 2 shows a schematic of the model, which consists \nof four layers. The first layer is the input representing the \nvisual and auditory modality. The second is the unisensory \nlayer where we have the auditory and the visual map. We \nnote that for simplicity we do not implement a \nsomatosensory input or map. Each of these maps is trained \n \n \n \nusing representations of their respective modality; the \nauditory input is only connected to the auditory map and the \nvisual input to the visual map. The third layer represents a \ncortical map.  While this is clearly a simplification of the \nextensive processing that may take place in several of the \nhigher unisensory and multisensory cortical areas, here we \nare interested in what influence such a layer may have on \nforming a multisensory representation.  Finally, the fourth \nlayer is the integration map, which represents the \nmultisensory space, moderated by the cortical feedback.  \nThe cortical map receives inputs from both the auditory and \nvisual maps, whilst the multisensory map receives inputs \nfrom the cortical, auditory and visual maps. The layered \nrepresentation allows us to study the behavior of the model \nin two phases: a) the unisensory phase where we can \nobserve the development of topographic representations of \nthe input space, b) the integration phase where we can study \nmultisensory and cortical feedback effects. \nEach of our modules is a grid of fully connected neurons. \nTraining occurs per layer, so that the unisensory maps are \ndeveloped first before continuing on to the integration maps. \nThis is because we want first to achieve topographic \norganization of the sensory input. Then, the cortical map is \ntrained using outputs from the auditory and visual maps. \nFinally, the multisensory map is trained using outputs from \nthe cortical, auditory and visual maps. A neuron at location \n(i,j) on the grid outputs y given an m-dimensional input x: \n\u2211\n=\n=\nm\nk\nkijkij twxu\n1\n)(\n \n(1) \n\u23aa\u23a9\n\u23aa\u23a8\n\u23a7\n\u2212\n<\u2212=\n)(\n)( if)(\nwinij\nwinijij\nij yuf\nthccuf\ny  (2) \n\u23aa\u23a9\n\u23aa\u23a8\n\u23a7\n\u2264\n<<\n\u2265\n=\n00\n10\n11\nu\nuu\nu\nyij\n (3) \nHere, wkij(t) represents the weight from input k for neuron \n(i,j) in the grid at time step t \u2265 0. In equation 2 we consider a \nneuron within the winner area if its location (i,j) falls within \nthe current radius value h(t) of the winning neuron. This \nimplementation of lateral inhibition between neurons in the \nmap is similar to Kohonen's SOM [14], and is sufficient to \npromote competition and allows the map to organize. We \nindicate cij and cwin the grid coordinates of the neuron (i,j) \nand the winning neuron respectively. The activation value of \nthe winning neuron ywin = maxijf(uij) inhibits neurons found \noutside of the winning area. During training, the winning \nneuron\u2019s area and the learning rate are reduced per epoch by \na Gaussian neighborhood and logarithmic radius function: \n\u239f\u239f\u23a0\n\u239e\n\u239c\u239c\u239d\n\u239b\u2212\n\u2212+= 22\n)\/(\nminmaxmin )()( s\ne\nr\ntt\nerrrth  \n(4) \n\u239f\u239f\u23a0\n\u239e\n\u239c\u239c\u239d\n\u239b\u2212\n\u2212+= 2\n2\n2\n)\/(\nminmaxmin )()( s\ne\nl\ntt\nelllt\u03b5  (5) \n \nBy doing so, each neuron\u2019s weights are gradually tuned to \nrespond to inputs depending on their location in the input \nspace. Here, rmin, rmax and lmin, lmax represent the minimum \nand maximum radii for the neighborhood and learning rate \nrespectively with rs and ls being the bandwidth. The \nneighborhood and learning rate are updated per epoch thus \nthe time step te for the Gaussian functions (4) and (5) is \nequal to the data set size. \nWe use the same set of parameters throughout all maps to \nensure consistency during development. The auditory, \ncortical and multisensory maps have a size of 20 by 15 while \nthe visual map a size of 10 by 10. These particular sizes \nwere chosen for computational efficiency. A smaller visual \nmap size is selected since the visual space representation is \nsmaller than the auditory. In this way, the rmax is equal to the \ngrid width which would be 20 for the auditory, cortical, and \nmultisensory map whilst 10 for the visual map. The \nminimum radius rmin is 3 for all modules. We set the \nbandwidth to rs = 300, and the maximum and minimum \nlearning rates to lmax = 0.1 and lmin = 0.001, respectively. \nTraining lasts for 700 epochs with all stimuli being \nrandomly presented per epoch. These parameters were \nselected by an in depth analysis [15]. The Hebbian rule \nvariation we employ is used to update each weight. After the \nweight update we normalize all weights to avoid exponential \ngrowth. \nijkkijkij yxttwtw )()()1(\n' \u03b5+=+  (6) \n)1(\n)1()1(\n1\n'\n'\n+\n+=+ \u2211 = tw\ntwtw m\nl lij\nkij\nkij\n \n(7) \nIII. EXPERIMENTS AND EVALUATION \nWe will first demonstrate the development of the unisensory \nmaps and the creation of coordinate systems for each. Then \nwe will evaluate whether the maps have developed a \nmagnified central area, thus reflecting the dense (fovea) and \nnon-dense (peripheral) areas of the input representations. \nFollowing this, we will examine coordinate alignment of the \nhigher levels of the simulated cortical map as well as the \nmultisensory map. Based on the developed topographic \nproperties we will introduce example test data sets that \ncomprise coincident, non-coincident and multi-stimuli data \nto evaluate the responses with respect to multisensory \nenhancement and suppression. Finally, by enabling and \ndisabling cortical input to the multisensory map we will \nevaluate the effect that the former has on the manifestation \nof multisensory responses inspired by the observations of \nStein and Meredith [1] from experiments on cats.  \nThe input is a simplified representation of the visual and \nauditory space corresponding to spatially related stimuli. \nThe inputs are in the form of Gaussian activity patterns (cf. \n[16]). \nSuch inputs provide us with a simple stimuli that is flexible \nenough to capture key properties, such as signal strength \n \n \n \n(amplitude) and location. This helps us to obtain a clear \nunderstanding of the model's behavior. Each Gaussian \nactivation pattern is centered at random intervals within a \nsensory space of elevation i and azimuth j. The values of an \ninput x for a stimulus centered at (c,d) are given from: \n\u239f\u239f\u23a0\n\u239e\n\u239c\u239c\u239d\n\u239b \u2212+\u2212\u2212\n= 2\n22\n2\n)()(\n\u03c3\u03bb\ndjci\nij ex  \n(8) \nwith \u03bb being the amplitude and \u03c3 the bandwidth. We \nintroduce two pairs of bandwidth and amplitude per \nmodality in order to represent dense and non-dense regions, \nas used in [11]. Greater bandwidth with smaller width \nactivations are used for the dense, whilst lower bandwidth \nwith larger width for non-dense firing regions. In this way \nwe capture accuracy of stimulus representation depending \non its location on the sensory space. This is based on \nsensory stimuli occurring approximately within the centre of \nthe sensory space corresponding with the fovea [1]. An \nexample of this input representation is seen in Figure 3. \n \n \n \nFig. 3.  A stripe of six auditory inputs. Each Gaussian activation pattern is a \nsingle input for the auditory (unisensory) map. The two patterns in the \ncentre fall within the dense region hence they have higher amplitudes and \nsmaller widths compared to the remaining four in the non-dense areas. \nA. Development of Unisensory Maps \nIn this phase we train the modality specific maps. Each map \nis trained with its own data set. Both data sets used are \nGaussian activation patterns. We use a 25 by 13 auditory \nand a 9 by 13 visual space that correspond to an azimuth and \nelevation range of [-180, 180], [-90, 90] and [-90, 90], \n[-65, 55], respectively. These degrees correspond to \napproximate actual elevation and azimuth of sensory spaces \n[1]. The range of the input space results from using a \ndiscrete step size of 15. We choose this as a computational \nefficient size of input examples. \nEach input contains a dense and non-dense area \nrepresenting fovea and peripheral areas of stimuli encoding. \nFor the auditory input this area is within azimuth [-180, 180] \nand elevation [-30, 30] whilst for the visual with azimuth \n[-15, 15] and elevation [-15, 15] as observed by [1]. Thus, \nthe dense areas fall within azimuth units 1-25 and elevation \nunits 5-9 for the auditory modality, while approximately \nfrom azimuth 6-8 and elevation 4-6 for the visual modality. \nWe note that for the visual modality the units correspond to \nan azimuth of [-15, 15] and elevation of [-10, 20] due to the \nstep size of 15. In this way, dense area coverage is 38.5% of \nthe auditory and 7% of the visual unit space. The full details \nof the input specification can be seen in Table 1. \nThe number of training examples (6000 for auditory and \n2400 for visual) was empirically chosen in order to achieve \nsufficient overlap, crucial for associative learning \napproaches, as the input centers are randomly presented \nwithin each of the respective input spaces. The number of \nauditory examples is larger than the visual examples \nreflecting the size difference of the auditory and visual input \nspaces.  \nIn order to examine organization, we recorded the \nlocations of the maximally activated neurons when inputting \nstripes of stimuli according to azimuth and elevation. These \nstimuli were different to the training set. As an example to \nillustrate the map organization, for the auditory maps we \nchose three input stripes between [-90, -45], [-30, 30] and \n[45, 90] of azimuth [-180, 180]. Also, for an elevation of \n[-90, 90] we used example azimuth stripes of [-180, -60] \n[-45, 60] and [75, 180]. \nTABLE I \nTRAINING AND TESTING DATA FOR UNISENSORY MAPS \nModalities Auditory Visual \nInput Space \nNon-dense  Elevation [-90,-30); (30, 90] [-65,-15); (15, 55] \n Azimuth [-180, 180] [-90,-15); (15, 90] \n Gaussian \u03bb = 0.5 \u03c3 = 30 \u03bb = 0.5 \u03c3 = 30 \nDense  Elevation [-30, 30] [-15, 15] \n Azimuth [-180. 180] [-15. 15] \n Gaussian \u03bb = 1 \u03c3 = 15 \u03bb = 1 \u03c3 = 15 \nInput dimension 325 (13 by 25) 117 (9 by 13) \nUnisensory Training and Testing \nWhole area examples 4200 2100 \nAdditional dense examples 1800 300 \nTotal examples 6000 2400 \nAs seen in Figure 4a, each auditory output stripe obtained \na distinct location on the map. Our results show that 52.3% \nof the total number of maximally activated units belonged to \nthe stripe of azimuth [-180, 180] and elevation [-30, 30], \nwhich corresponds to the auditory dense area. The map \ntherefore has a magnified representation of the input dense \narea on the auditory map since it only covers 38.5% of the \ninput space. Similarly, we repeated the same process by \npresenting the visual maps with visual stimuli stripes (Figure \n4b). The maximally activated number of neurons for the \ninput dense area comprised 15%, whilst the input dense area \nwas 7% of the total input space. The results from both maps \nshows that the unisensory maps can topographically \norganize the training examples (auditory and visual \nexamples respectively) whilst also manifesting the \nmagnification effect observed in the actual neural structure. \nB. Multisensory Integration \nIn this section we evaluate the integration map responses. \nWe are particularly interested in the multisensory map \nresponses when presented with a) topographically coincident \nmultisensory information (i.e. an auditory and visual \nstimulus originating from the same location) b) non-\ncoincident stimuli, and c) unimodal stimuli. This is because \n \n \n \nFig. 4.  Maximum neuron activation stripes of auditory and visual map: a) auditory map outputs according to elevation, and b) visual map outputs \naccording to azimuth. The clusters on the maps formed by neighboring coordinate stripes are indicative of the developed topographical organizations. The \ndashed white rectangles indicate the approximate locations on the maps where the example inputs of the auditory and visual fovea are encoded. \na) b) \nit has been established that coincident stimuli induce higher \nresponses (enhancement) compared to non-coincident \nstimuli, which are often suppressed. In particular, we \nexamine what the effect is of the cortical input to such \nresponses as it is hypothesized that the feedback controls the \nresponses of multisensory stimuli [4,17]. The integration \nmaps are trained using outputs from the unisensory maps. \nTABLE II \nTRAINING AND TESTING DATA FOR INTEGRATION MAPS \nIntegration Maps Training \nType Azimuth Elevation Auditory Visual \nCoincident [-90, 90] [-65, 65] 1500 1500 \nNon-coincident [-90, 90] [-65, 65] 500 500 \nOnly Auditory [-180, 180] [-90, 90] 500 * \nOnly Visual [-90, 90] [-65, 65] * 500 \nTotal Concurrent Presentations 3000 \n* zero inputs \nFor this phase we create a new training data set. It \ncontains a mixture of stimuli that fall: a) within identical \nlocations for both input spaces (coincident input), b) within \nthe input spaces of both modalities but in different locations \n(non-coincident), c) within the auditory space only with zero \ninputs from the visual modality, d) within the visual input \nspace only, with zero inputs from the auditory modality. By \nzero input we define an example with zero activation to \nindicate the absence of a stimulus. The aim of this data set is \nto establish whether the integration maps are able to \norganize and produce multisensory responses. For this \nreason 50% of the examples used were coincident whilst the \nrest were of the remaining three types (17% each), obtaining \na total of 3000 training examples (Table 2). We empirically \nchose these percentages in order to demonstrate that the \nsimultaneous presentation of non-coincident input is not \nadequate to disrupt the map organizations even if they \ncomprised 50% of the total input examples. The inputs were \npresented to the unisensory maps. Their outputs were used \nto first train the cortical map. After training of the latter \nfinished, we concatenated the outputs of the cortical and the \nunisensory maps and used them to train the multisensory \nmap. \nFor testing we use examples with integer center \ncoordinates. We do this in order to simplify the \ninterpretation of results since examples at distinct locations \nhave maximum amplitude of 1. Therefore, the term \ncoincident examples from this point onwards refer to 117 \nexample pairs (auditory and visual) that fall within the same \nlocations of the auditory and visual maps. This number is \nderived from the size of the visual input space (13 by 9). \nSince the visual space is smaller than that of the auditory, \nwe use the auditory spatial parameters for the multisensory \nspace. The non-coincident inputs are examples that fall \nwithin the same space boundaries as the coincident but the \npairs are not of the same location. Particularly, we are using \npatterns with distinct centers originating from opposite ends \non the common space of the auditory and visual maps, \nconverging towards the centre. Thus, we have again 117 \nexamples, with example 59 being the only coincident input \nas it is the crossing-point of the modalities\u2019 patterns (Figure \n5). By employing this structure for the non-coincident data \nwe can observe in a controlled way how the coincident \ncompare to the non-coincident activations, as they have in \ncommon one part per example pair (with the other part at a \nknown location). \n \n \n \nFig.5. Maximum neuron activations corresponding to coincident and non-coincident stimuli. The white arrows indicate the direction of consecutive input \npairs: (a) the first of the non-coincident input examples, (b) cross-over point of non-coincident input examples, and (c) the last non-coincident example. \na) b) c) \nFig. 6. Maximum neuron activations for coincident (red line) and non-coincident data (blue line) for the multisensory map with cortex enabled. Test data \nwere 117 examples having distinct centers on the auditory and visual space. \n0\n0.05\n0.1\n0.15\n0.2\n0.25\n1 6 11 16 21 26 31 36 41 46 51 56 61 66 71 76 81 86 91 96 101 106 111 116\nMultisensory - Coincident Multisensory - Non Coincident\n7\nExample 59\nFor the auditory only inputs we use distinct centers \nthroughout the entire auditory input space (25 by 13) whilst \nsetting zero amplitude inputs at the corresponding visual \nspace. This yields a total of 325 auditory examples. \nSimilarly for the visual only inputs, we use distinct centers \nthroughout the entire visual space (9 by 13), with the \nauditory inputs set to zero amplitudes. \nWe proceed to compare the multisensory map activations \nwhen presented with coincident and non coincident inputs. \nAs seen in Figure 6 the coincident inputs induce higher \noutputs than the non coincident ones. This is attributed to \nthe multisensory map developing weight formations that \nrespond maximally to coincident stimuli. In other words, \nnon-coincident inputs did not fall within the developed \nneuron weights and this resulted in an overall reduced \nneuron response. Throughout the examples, coincident \nvalues are higher (mean 0.138) compared to the non-\ncoincident (mean 0.108) for the multisensory map (with \nstatistical significance p < 3.757e-011). This verifies the fact \nthat the map learns to respond maximally to coincident \nauditory-visual stimuli whilst showing suppressed \nactivations to non coincident ones. In this way we were able \nto reproduce a well known phenomenon of the SC where \nlocally or temporally disparate stimuli demonstrate \nsuppressed activations [2]. We note that in Figure 6, \nexample 59 is the crossing point of the non-coincident data \nwe used, and hence the activation values are the same. \nOur final task was to investigate whether the multisensory \nmap\u2019s outputs were affected by enabling and disabling the \ncortex. This is shown in Figure 7 where the outputs of the \n117 locally coincident multisensory inputs (audio and \nvisual) are presented. We observe that activations drop when \nthe cortex input is disabled from a mean of 0.138 to a mean \nof 0.112 (p < 1.7868e-009). This shows that the magnitude \nof the multisensory map\u2019s activations is dependent on the \ninput received by the cortex module. Neurobiological \nfindings also support that by disabling connectivity of \ncortical areas to the SC, such as the anterior ectosylvian \nsulcus (AES) and the rostral lateral suprasylvian sulcus \n(rLS), effects multisensory enhancement whilst leaving \nrelatively unaltered modality specific responses [17,18]. \nAlvarado et al. [17] is one of the latest studies to have \nshown this by measuring responses of a multisensory neuron \nwhen presented with cross-modal (auditory and visual) \ncoincident stimuli. They demonstrated that the \nsuperadditivity phenomenon of a multisensory neuron (using \nthree different intensity levels of the visual stimulus \neffectiveness) is lost when the AES and rLS are deactivated, \nand reappear when they are reactivated. Although in our \nexperiment we did not use different effectiveness levels and \nwere not concerned with the level of enhancement \n(additivity or superadditivitty) we managed to reproduce \nmaximal responses only when the cortex map was enabled. \nThis is summarized in Table III compared to the responses \nfrom [17], where enhancement changes between +10% to -\n59.06% are recorded after deactivating and activating AES \nand rLS. This was also apparent in our model with an \nactivation change of -18.84% after disabling the cortex \nmodule and comparing the multisensory map responses to \nthe ones recorded when it was enabled. \n \n \n \n0\n0.05\n0.1\n0.15\n0.2\n1 7 13 19 25 31 37 43 49 55 61 67 73 79 85 91 97 103 109 115\n0.25\nMultisensory (Cortex On) - Coincident Multisensory (Cortex Off) - Coincident\n7\nFig.7. Maximum neuron activations of multisensory map for coincident audio-visual data with cortex on (red line) and off (blue line). Activating the \ncortex (red) clearly produces higher responses compared to it being deactivated (blue). Test data were 117 examples having distinct centres on the \nauditory and visual space. \nTABLE III \nRESPONSE COMPARISON TO ALVARADO ET AL. [4,17] RESULTS \nModality Specific Multisensory Neurons Unisensory Neurons \nAlvarado et al. 2009 [4] \nStimuli Visual 1 Visual 2 Visual 1 Visual 2 \nDeactivate FAES and AEV \nIncrease 7.32% 0% 6.45% 3.23% \nNo Change 24.39% 28.57% 83.87% 90.32% \nDecrease 68.29% 71.43% 9.68% 6.45% \nOur Results \nMultisensory map activations percentage change after turning off cortex \nAuditory activations magnitude within unisensory space -13.11% \nAuditory activations magnitude within multisensory space -18.78% \nCross Modal Multisensory Neuron Response Magnitude \nAlvarado et al. 2007 [17] \nVisual + Auditory Stimuli (Using alternate visual stimulus effectiveness) \nDeactivated AES and rLS Enhancement between 48% to 70% \nActivated AES and rLS Enhancement between  43% to 171% \nEnhancement Change Between +10% to -59.06% \nOur Results \nMultisensory map activations percentage change after turning off cortex \nVisual-auditory activations magnitude with cortex module off -18.84% \nTo further illustrate the correspondence of our model with \nneurobiological recordings (summarized in Table III) we \nillustrate the multisensory map\u2019s responses when presenting \nit with auditory input only. Inputs are located at distinct \nlocations throughout the whole auditory input space (325 \nexamples of distinct centers resulting from a 25 by 13 \nauditory space). We observed that auditory input responses \nof the multisensory map that fell within the unisensory space \nwere effected less (-13.11%) by turning the cortex off \n(Table III). On the other hand, greater activation differences \n(-18.78%) were observed for auditory inputs falling within \nthe multisensory space (Figure 8). Thus the overall mean \nvalue of the activations with the cortex on and off (0.048 \nand 0.040 respectively with p < 0.021) was mostly affected \nby the multisensory rather than the unisensory space \nactivations. A similar effect was observed when presenting \nonly visual inputs (mean values of 0.050 and 0.042 having \nthe cortex on and off respectively, with p < 0.001). \nTherefore our model\u2019s outputs are comparable with \nneurobiological findings after cortical area deactivation \n(FAES and AEV) where multisensory neurons responded \nmore (decrease) and unisensory neurons less (no change) \neven to presentations of a single modality stimulus [4,17] \n(Table III).  \nIV. CONCLUSION \nIn this paper we presented a model of the superficial and \ndeep layers of the SC using a modularized associative \nlearning architecture. Our model provides a complete \nfunctional representation of the SC, which includes \nunisensory topographic maps and coordinate alignment of \nthese maps to form a multisensory space. We demonstrated \nhow topographic unisensory maps can be developed for the \nauditory and visual modalities using abstract stimuli. \nAlthough the organization was not perfect, clusters of \nspatially similar stimuli are formed clearly within each map. \nThis includes greater representation of the dense (fovea) and \nnon-dense (peripheral) areas of each sensory input, \ncommensurate with biological data. Having developed these \nunisensory representations, we then input these to a simple \nrepresentation of cortical processing.  This, together with the \nunisensory map outputs, was then used to form a \nmultisensory map that exhibited behavior found in the SC. \nIn particular, when a non-spatially coincident stimuli is input \nto the auditory and visual maps, the multisensory signal is \nsuppressed, while spatially coincident stimuli are enhanced. \nWhile the enhancement demonstrated in the model is not \nsuperadditive as found in the real SC, neither suppression or \nenhancement is achieved without cortical feedback. Here, \nwe found that the cortex only affected multisensory map \nresponses for inputs that fell within the multisensory space. \nThese findings reflect some of the core properties of the SC. \nThis model for the first time shows how each aspect of \nthe SC may be implemented using a uniform architecture. \nOur model demonstrates how unisensory representations of \nthe auditory and visual spaces can be developed through a \nprocess of competition.  These topographic maps, which \nhave their own coordinate systems, are aligned through the \nsame process of learning to form a multisensory space.  This \nspace in turn is moderated by cortical feedback which is \nessential to cause increased activtations for coincident \nmultimodal, versus non-coincident or unimodal stimuli, \nalbeit without the property of inverse effectiveness (cf. \n[19]). \nWith more and more complex models of the brain and \nsensory processing now being developed (cf. [20]), we must \nalso ask what this model contributes computationally.  For \nthe most part, modeling the computational properties of \n \n \n \nFig. 8. Maximum neuron activations of the multisensory map for auditory data only with cortex on (red line) and off (blue line). The cortex effects \nresponses of auditory inputs to multisensory neurons only when falling within the multisensory space. For inputs that fall within the unisensory space, no \nchanges were observed by having the cortex on or off. Test data were 325 examples having distinct centers on the auditory space.  \n0\n0.05\n0.1\n0.15\n0.2\n1 24 47 70 93 116 139 162 185 208 231 254 277 300 323\nMultisensory (Cortex On) - Auditory Stimuli Only Multisensory (Cortex Off) - Auditory Stimuli Only\n325\nUnisensory Space\n(Auditory Modality)\nUnisensory Space\n(Auditory Modality)\nApproximate Unisensory Space (Auditory Modality)\nsensory processing has focused on cortical columns, which \nintegrate various levels of processing with feedback.  \nTypically, such models treat low level structures simply, \neither as inputs (the retina) or to route signals (the \nthalamus).  From a computational perspective, models of the \nSC demonstrate that there is more to computational \nmodelling than the cortex.  Although the cortex is clearly an \nimportant area to study and model, we must not forget the \nrole that the midbrain and other lower structures play, \nparticularly because they seem to be an integral part of \nprocessing, as demonstrated through the likes of cortical \nfeedback to these structures.  In our model, we have \ndemonstrated how such low level structures can perform \nimportant and non-trivial functions which are important \nenough to be actively influenced by cortical processing.  \nThrough a unified, if simplistic learning paradigm, we have \nshown how different processing levels in the brain can be \nsimulated.  The next stage of this work is to expose this \nmodel to real-world stimuli to determine if it can process \nmore complex inputs of images from video and audio from \nmicrophones in real-time. \nREFERENCES \n[1] B.E.Stein and M.A.Meredith, The Merging of the Senses. Cambridge, \nMA.: A Bradford Book, MIT Press, 1993. \n[2] A.J.King, \"The Superior Colliculus,\" Current Biology, vol. 14, pp. \nR335-R338, 2004. \n[3] B.E.Stein, W.Jiang and T.R.Stanford, \"Multisensory Integration in \nSingle Neurons of the Midbrain,\" G.A.Calvert, C.Spence, and \nB.E.Stein, Ed. Cambridge, MA.: A Bradford Book, MIT Press, 2004, \npp. 243-264. \n[4] J.C.Alvarado, T.R.Stanford, B.A.Rowland, J.W.Vaughan and \nB.E.Stein, \"Multisensory Integration in the Superior Colliculus \nRequires Synergy among Corticocollicular Inputs,\" The Journal of \nNeuroscience, vol. 29, pp. 6580-6592, 2009. \n[5] M.Corbetta and G.L.Shulman, \"Control of Goal-directed and \nStimulus-driven Attention in the Brain,\" Nature Reviews \nNeuroscience, vol. 3, pp. 215-229, 2002. \n[6] L.A.Notman, P.T.Sowden and E.\u00d6zgen, \"The Nature of Learned \nCategorical Perception Effects: A Psychophysical Approach,\" \nCognition, vol. 95, pp. B1-B14, 2005. \n[7] S.Grossberg, K.Roberts, M.Aguilar and D.Bullock, \"A Neuronal \nModel of Multimodal Adaptive Saccadic Eye Movement Control by \nSuperior Colliculus,\" Journal of Neuroscience, vol. 17, pp. 9706-\n9725, 1997. \n[8] T.J.Anastasio, P.E.Patton and K.Belkacem-Boussaid, \"Using Bayes' \nRule to Model Multisensory Enhancement in the Superior Colliculus,\" \nNeural Computation, vol. 12, pp. 1165-1187, 2000. \n[9] P.E.Patton and T.J.Anastasio, \"Modeling Cross-Modal Enhancement \nand Modality-Specific Suppression in Multisensory Neurons,\" Neural \nComputation, vol. 15, pp. 783-810, 2003. \n[10] E.Magosso, C.Cuppini, A.Serino, G.Di Pellegrino and M.Ursino, \"A \nTheoretical Study of Multisensory Integration in the Superior \nColliculus by a Neural Network Model,\" Neural Networks, vol. 21, pp. \n817-829, 2008. \n[11] M.C.Casey and A.Pavlou, \"A Behavioral Model of Sensory Alignment \nin the Superficial and Deep Layers of the Superior Colliculus,\" in \nProceedings of the International Joint Conference on Neural \nNetworks (IJCNN) 2008, 2008, pp. 2751-2756. \n[12] J.Huo and A.Murray, \"The Adaptation of Visual and Auditory \nIntegration in the Barn Owl Superior Colliculus with Spike Timing \nDependent Plasticity,\" Neural Networks, vol. 22, pp. 913-921, 2009. \n[13] J.L.Armony, D.Servan-Schreiber, J.D.Cohen and J.E.LeDoux, \n\"Computational Modeling of Emotion: Explorations Through the \nAnatomy and Physiology of Fear Conditioning,\" Trends in Cognitive \nSciences, vol. 1, pp. 28-34, 1997. \n[14] T.Kohonen, \"Self-Organized Formation of Topologically Correct \nFeature Maps,\" Biological Cybernetics, vol. 43, pp. 59-69, 1982. \n[15] A.Pavlou, \"Computational Modelling of Visual Fear Conditioning,\" \nUnpublished doctoral thesis, University of Surrey, Guildford, UK, \n2009. \n[16] J.L.McClelland, A.Thomas, B.D.McCandliss and J.A.Fiez, \n\"Understanding Failures of Learning: Hebbian Learning, Competition \nfor Representational Space and Some Preliminary Experimental Data,\" \nJ.Reggia, E.Ruppin, and D.Glanzman, Ed. The Netherlands: Elsevier \nScience BV, 1999, pp. 75-80. \n[17] J.C.Alvarado, T.R.Stanford, J.W.Vaughan and B.E.Stein, \"Cortex \nMediates Multisensory but not Unisensory Integration in Superior \nColliculus,\" Journal of Neuroscience, vol. 27, pp. 12775-12786, 2007. \n[18] M.T.Wallace and B.E.Stein, \"Cross-modal Synthesis in the Midbrain \nDepends on Input from Cortex,\" Journal of Neurophysiology, vol. 71, \npp. 429-432, 1994. \n[19] B.A.Rowland, T.R.Stanford and B.E.Stein, \"A Model of the Neural \nMechanisms Underlying Multisensory Integration in the Superior \nColliculus,\" Perception, vol. 36, pp. 1431-1443, 2007. \n[20] E.Izhikevich and G.M.Edelman, \"Large-Scale Model of Mammalian \nThalamocortical Systems,\" Proceedings of the National Academy of \nSciences of the USA, vol. 105, pp. 3593-3598, 2008. \n \n \n"}