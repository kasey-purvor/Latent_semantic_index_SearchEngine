{"doi":"10.1109\/STEP.2003.8","coreId":"102524","oai":"oai:epubs.surrey.ac.uk:1987","identifiers":["oai:epubs.surrey.ac.uk:1987","10.1109\/STEP.2003.8"],"title":"Effects of virtual development on product quality: exploring defect causes","authors":["Jacobs, J. C.","van Moll, J. H.","Krause, P. J.","Kusters, R. J.","Trienekens, J. J. M."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2003-09-19","abstract":"<p>This paper explores the effects of virtual development on product quality, from the viewpoint of \"conformance to specifications\". Specifically, causes of defect injection and non- or late-detection are explored. Because of the practical difficulties of obtaining hard project-specific defect data, an approach was taken that relied upon accumulated expert knowledge. The accumulated expert knowledge based approach was found to be a practical alternative to an in-depth defect causal analysis on a per-project basis. Defect injection causes seem to be concentrated in the requirements specification phases. Defect dispersion is likely to increase, as requirements specifications are input for derived requirements specifications in multiple, related sub-projects. Similarly, a concentration of causes for the non- or late detection of defects was found in the Integration Test phases. Virtual development increases the likelihood of defects in the end product because of the increased likelihood of defect dispersion, because of new virtual development related defect causes, and because causes already existing in co-located development are more likely to occur.<\/p","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:epubs.surrey.ac.uk:1987<\/identifier><datestamp>\n      2017-10-31T14:03:55Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      74797065733D636F6E666572656E63655F6974656D<\/setSpec><setSpec>\n      6469766973696F6E733D656E67616E64706879736963616C736369656E636573:436F6D707574696E67<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/epubs.surrey.ac.uk\/1987\/<\/dc:relation><dc:title>\n        Effects of virtual development on product quality: exploring defect causes<\/dc:title><dc:creator>\n        Jacobs, J. C.<\/dc:creator><dc:creator>\n        van Moll, J. H.<\/dc:creator><dc:creator>\n        Krause, P. J.<\/dc:creator><dc:creator>\n        Kusters, R. J.<\/dc:creator><dc:creator>\n        Trienekens, J. J. M.<\/dc:creator><dc:description>\n        <p>This paper explores the effects of virtual development on product quality, from the viewpoint of \"conformance to specifications\". Specifically, causes of defect injection and non- or late-detection are explored. Because of the practical difficulties of obtaining hard project-specific defect data, an approach was taken that relied upon accumulated expert knowledge. The accumulated expert knowledge based approach was found to be a practical alternative to an in-depth defect causal analysis on a per-project basis. Defect injection causes seem to be concentrated in the requirements specification phases. Defect dispersion is likely to increase, as requirements specifications are input for derived requirements specifications in multiple, related sub-projects. Similarly, a concentration of causes for the non- or late detection of defects was found in the Integration Test phases. Virtual development increases the likelihood of defects in the end product because of the increased likelihood of defect dispersion, because of new virtual development related defect causes, and because causes already existing in co-located development are more likely to occur.<\/p><\/dc:description><dc:date>\n        2003-09-19<\/dc:date><dc:type>\n        Conference or Workshop Item<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/epubs.surrey.ac.uk\/1987\/1\/fulltext.pdf<\/dc:identifier><dc:identifier>\n          Jacobs, J. C., van Moll, J. H., Krause, P. J., Kusters, R. J. and Trienekens, J. J. M.  (2003) Effects of virtual development on product quality: exploring defect causes  In: Eleventh Annual International Workshop on Software Technology and Engineering Practice.     <\/dc:identifier><dc:relation>\n        10.1109\/STEP.2003.8<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/epubs.surrey.ac.uk\/1987\/","10.1109\/STEP.2003.8"],"year":2003,"topics":[],"subject":["Conference or Workshop Item","PeerReviewed"],"fullText":"Effects of Virtual Development on Product Quality:\nExploring Defect Causes\nJ.C. Jacobs\nPhilips Semiconductors\nEindhoven,The Netherlands\njef.jacobs@philips.com\nJ.H. van Moll\nPhilips Semiconductors & Sioux\nTechnical Software Development\nEindhoven, The Netherlands\njan.van.moll@philips.com\nP.J. Krause\nUniversity of Surrey\nGuildford, UK\np.krause@eim.surrey.ac.uk\nR.J. Kusters\nEindhoven University of\nTechnology\nEindhoven, The Netherlands\nr.j.kusters@tm.tue.nl\nJ.J.M. Trienekens\nEindhoven University of\nTechnology\nEindhoven, The Netherlands\nj.j.m.trienekens@tm.tue.nl\nAbstract\nThis paper explores the effects of virtual development\non product quality, from the viewpoint of \"conformance\nto specifications\". Specifically, causes of defect injection\nand non- or late-detection are explored. Because of the\npractical difficulties of obtaining hard project-specific\ndefect data, an approach was taken that relied upon\naccumulated expert knowledge. The accumulated expert\nknowledge based approach was found to be a practical\nalternative to an in-depth defect causal analysis on a\nper-project basis. Defect injection causes seem to be\nconcentrated in the Requirements Specification phases.\nDefect dispersion is likely to increase, as requirements\nspecifications are input for derived requirements speci-\nfications in multiple, related sub-projects. Similarly, a\nconcentration of causes for the non- or late detection of\ndefects was found in the Integration Test phases. Virtual\ndevelopment increases the likelihood of defects in the\nend product because of the increased likelihood of defect\ndispersion, because of new virtual development related\ndefect  causes, and because causes already existing in\nco-located development are more likely to occur.\nKeywords: Virtual development, Product Quality,\nDefect injection, Defect detection, Defect Causal\nAnalysis\n1 Introduction\nThe objective of this paper is to investigate the\npossible effects of virtual development on the quality of\nthe delivered product, in particular the exploration of\ndefect causes. Virtual development is the development of\na product (or product family) by a virtual team. A virtual\nteam is a team distributed across space, time, and\norganization boundaries, and linked by webs of\ninteractive technology [13]. We prefer the term \u201cvirtual\ndevelopment\u201d over the term \u201cglobal software\ndevelopment\u201d as used by Karolak [12], Carmel [3] and\nHerbsleb et al. [9], because the scope of development is\nnot necessarily restricted to software only, and because\nthe development team need not necessarily be scattered\naround the globe. Being in another building or on a\ndifferent floor of the same building, or even at the other\nend of a corridor, can be  sufficient to label it as global\ndevelopment [9].\nBoth Karolak and Carmel describe the issues that\ncause virtual development of products to be much more\ncomplex than even the most complex project managed\nentirely in house [3, 12]. They also suggest possible\nsolution strategies, derived from case studies in virtual\ndevelopment projects. The works of Karolak and Carmel\nare focused on the managerial and collaboration aspects\nof the organization and execution of virtual development\nprojects. The emphasis is on timely delivery of the\nproduct within budget. In this paper, we address the\neffects  of virtual development on product quality, as this\nis not or only marginally addressed in frequently cited\nliterature on virtual development, e.g. [3, 9, 12].\nHowever, product quality is a complex and multi-faceted\nconcept, pointed out already in 1982 by Garvin [6]. He\nidentified five different views of quality: the\ntranscendental view, the manufacturing view, the\nproduct view, the user's view, and the value-for-money\nview. In the context of this paper, we will consider the\nmanufacturing view, usually encapsulated in the phrase\n\u201cconformance to specification\u201d, as our base view on\nproduct quality. Non-conformances to specifications\nProceedings of the Eleventh Annual International Workshop on Software Technology and Engineering Practice (STEP\u201904) \n0-7695-2218-1\/04 $20.00 \u00a9 2004 IEEE\n2(which we shall call defects) will typically have a\nnegative impact on product quality whatever the point of\nview.  \nSection 2 introduces virtual development and its\nspecific problems. It leads to the recognition of four risk\ncategories that, given the unique aspects of virtual\ndevelopment, are crucial to success or failure of virtual\ndevelopment projects. Section 3 reports on an\nexplorative investigation into the effects of virtual\ndevelopment on product quality. Practical problems to\nget hard defect data forced an approach relying upon\naccumulated expert knowledge concerning defect causes\nin virtual development projects. In \"Defect Causal\nAnalysis\"-like meetings a team of experts took a\nlifecycle-centric view on virtual development to address\ntypical causes of the injection and non- or late-detection\nof defects. Section 4 discusses the suitability of the\nalternative approach and the findings of the explorative\ninvestigation. Finally, section 4 summarizes the\nconclusions.\n2 Problem Areas of Virtual Development\nTo investigate the effects of virtual development on\nproduct quality,  the associated problems and risks as\nreported by Carmel [3] and Karolak [12] can be used as\na  point of departure. Carmel performed a case study, the\nGlobally Dispersed Software Development (GDSD)\nstudy, to find the aspects in which global development\ndiffers from traditional, entirely in-house development\n[3]. The GDSD study concerned 17 software companies\nengaged in virtual development of products. Eventually,\nCarmel recognises the following three unique aspects:\nDistance between development sites has a direct\nimpact on project control, coordination and\ncommunication\nTime zone differences between development sites\nmake it even harder to communicate, impacting project\ncontrol and coordination\nCultural differences between development sites may\nlead to mistrust, mis-communication and lack of\ncohesion.\nOn the basis of these findings, Carmel identified five\nproblem areas that act as \u201ccentrifugal forces, driving the\nglobal development team apart\u201d.  We interpret Carmel\u2019s\nstatement of problem areas that are \u201cdriving the global\ndevelopment team apart\u201d as problem areas that\npotentially threaten the delivery of the product in time,\nwithin budget and with the specified or implicitly\nexpected product quality. The problem areas that Carmel\nrecognized are:\n(1) geographic dispersion, (2) control and\ncoordination breakdown, (3) loss of communication\nrichness, (4) loss of \u201cteamness\u201d and (5) cultural\ndifferences.\nWe interpret Carmel\u2019s unique aspects as causes and\nthe problems as their effects. In this view, the problem\narea \u201cGeographic dispersion\u201d seems peculiar, as it is a\ndirect implication of the \u201cdistance\u201d aspect. Hammar et\nal. also came to the same conclusion [8]. These authors\nreplaced the problem area \u201cGeographic dispersion\u201d by\n\u201cDifferences in knowledge\u201d, that they consider to be an\neffect caused by Carmel\u2019s unique aspects Distance and\nCulture. However, distance or cultural differences do not\nnecessarily cause differences in knowledge.\nKarolak\u2019s work [12] has a lot in common with that of\nCarmel [3]. However, Karolak uses the word \u201crisk\u201d\nwhere Carmel uses the word  \u201cproblem\u201d (or problem\narea). We prefer the term \u201crisk\u201d, because it implies that\nthe issue it addresses might be a problem, not that it\nnecessarily is a problem.\nKarolak distinguishes three risk categories:\nOrganizational risks, concerning decision authorities,\nresponsibilities, tasks and project structure, impacting\nproject control, coordination and team behavior,\nTechnical risks, concerning methods and tools used\nto solve technical problems, impacting development\nmethodology, architectural choices and eventually\nproduct quality,\nCommunication risks, that may lead to mistrust,\nmisinterpretations and inadequate communications.\nAlthough Karolak and Carmel view virtual or global\ndevelopment from a different perspective, they both\narrive at more or less the same risks or problem areas.\nNevertheless, to study the effects of virtual development\non product quality, we favor Karalok's risk view,\nbecause we consider the risk categories more\nimplication-neutral and coherent.\nIn one aspect Karolak and Carmel differ markedly:\nwhere Karolak considers technical aspects as a potential\nrisk, Carmel seems to consider them as a solution. A\npossible explanation of these seemingly opposed views\nmay be found in a study by Maidantchik et al. [14]. They\nreport the experiences of managing a global\ndevelopment project, in which advanced technology was\nused to minimize, or even eliminate some of the risks.\nThe collaborating groups that together formed the virtual\ndevelopment team differed in process maturity levels (as\nmeasured by CMM). They realized that for low maturity\norganizations it might be difficult or even impossible to\nintroduce advanced methods and technology. For\norganizations with a higher level of process maturity,\nadvanced methods and technology can be a solution,\nwhile for low maturity organizations the same methods\nand technology can be a problem. The study by\nMaidantchik et al. identifies the differences between\nsoftware processes used by collaborating groups and\nassociated process maturities as a risk category for\nvirtual development projects [14]. Earlier, McMahon\nalready warned for the potential danger of differences in\nprocesses and process maturities of collaborating parties\nProceedings of the Eleventh Annual International Workshop on Software Technology and Engineering Practice (STEP\u201904) \n0-7695-2218-1\/04 $20.00 \u00a9 2004 IEEE\n3in distributed development [16]. The observations of\nthese authors are in line with our own experiences in\nvirtual development. Consequently, we consider process\nrisks as an additional risk category for virtual\ndevelopment.\nKarolak\u2019s risk categories apply to both virtual and\nco-located projects. However, the likelihood of risks\noccurring in virtual development projects is greater [12],\ndue to the three unique aspects of virtual development as\nidentified by Carmel [3]. This is equally so for the\nadditional process risk category.\nIn first instance, we wanted to investigate in-depth\nthe causes of defects in a number of virtual development\nprojects. However, early in the preparation stage of the\ninvestigation it became apparent that such an approach\nwas unfeasible:  either the necessary defect data for\ndetermining defect root causes is  unavailable, or\norganizations refuse to provide them. A similar\nobservation has been reported by Chulani [5]. Moreover,\na quick scan learned that organizations limit the extent of\ndefect classification and analysis to only collect defect\ndata for solving problems at hand; root cause analysis to\neventually prevent defects from recurring in the future is\nhardly practiced.  If applied at all, defect classification\nschemes like the Hewlett Packard Scheme [7], the IEEE\n1044 Standard Classification for Software Anomalies\n[10] or Orthogonal Defect Classification [4] are rarely\nemployed on such a scale that their scope includes the\nentire virtual development context (i.e. the entire set of\nrelated projects contributing to the development of the\nproduct).\nThese practical problems forced us to explore an\napproach to our investigation of defects, alternative to\nthat of an in-depth causal analysis on a per-project basis.\nThe problem of lack of hard data was also faced by\nBriand et al., albeit in a different context [1]. They\nsuccessfully investigated the cost-effectiveness of\ninspections by relying upon expert judgments. Likewise,\nour alternative approach is based upon accumulated\nexpert knowledge concerning defect causes in virtual\ndevelopment projects.\n3 An Exploratory Investigation of Defect\nCauses\n3.1 Investigation Goals\nApart from the goal of determining the suitability of\nthe approach, this exploratory investigation aims at\nanswering the research questions:\n1. What are typical causes for the injection of\ndefects in virtual development? These may\neither be (a) \u2018new\u2019 causes, i.e. additional to\ncauses already present in co-located\ndevelopment, or (b) causes also present in co-\nlocated development but with a much higher\nprobability of occurrence in a virtual\ndevelopment context.\n2. What are typical causes for non- or late-\ndetection of defects in virtual development? I.e.\nwhy is it that defects are not detected at all or\nlate?\nWe use the word defect here, as a generic term for\nany discrepancy between:\n- product information specifying the product's\nbehavior and the behavior requested by the\nproduct development principal\n- actual product behavior and the specification of\nits behavior\n- information intended for the verification  &\nvalidation of the product  and its specified and\nrequested behavior.\n3.2 Investigation Approach\nOutline.   Six analysis meetings, with selected experts as\nparticipants, were conducted to identify causes for defect\ninjection and non- or late-detection. The meetings very\nmuch resembled the causal analysis meetings as seen in\nthe Defect Causal Analysis (DCA) process [2] and\nDefect Prevention Process (DPP) [15]. A major\ndifference was the way in which defects to be analyzed\nwere gathered, as shown in figure 1. Instead of selecting\na sample from a project\u2019s problem database as in regular\nDCA, an inventory was made of defect types and\nassociated causes on basis of the accumulated experience\nof the participating experts.\nFigure 1. Context of the Causal Analysis Meeting:\nStandard DCA (left) vs. this investigation (right).\nSelection of participants. Participants in the analysis\nmeetings were carefully selected on the basis of their\nprofessional background and expertise. Each participant\nSoftware\nProduction\nSoftware\nTesting\nCausal\nAnalysis\nMeeting\nProblems to fix Problems identified\nSample of\nproblems\nRecommended actions\nSoftware\nProblem\ndatabase\nProduct\nDevelopment\nProjects\n(industry-wide)\nProduct\nTesting\nProjects\n(industry-wide)\nDefects identified\nby expert experience\nDefects identified\nby expert experience\nDefects\n(industry\n-wide)\nSoftware\nCausal\nAnalysis\nMeeting\n Causes for injection\nand non (or late) detection\nProceedings of the Eleventh Annual International Workshop on Software Technology and Engineering Practice (STEP\u201904) \n0-7695-2218-1\/04 $20.00 \u00a9 2004 IEEE\n4had to have over three years of experience in the area of\nvirtual development.\nThey had to be directly involved in product testing,\ntechnical product support, defect causal analysis or\nproject management. Of these areas, defect causal\nanalysis experience was a prerequisite for participation.\nAn additional criterion was that they had been directly\ninvolved in multiple virtual development projects in the\nlast two years. To ensure a sufficiently wide experience\nbase, we have set the minimum number of experts to\nfive. Actually, six experts participated. The selected\nexperts, all at senior level, included two test managers,\none test architect, one software architect, one project\nmanager and one service engineer. They had all acquired\nexperience in multiple organizations, and so\ncorrespondingly their accumulated knowledge about\ndefect causes covered multiple organizations.\nModeratorship.  A moderator chaired and guided the\nanalysis meetings. His tasks included introducing the\nmeeting participants to the purpose and set-up of the\nmeeting. To safeguard the duration of the meetings, he\nalso intervened in discussions preventing them getting\ntoo extensive. At the end of a meeting, the moderator\nevaluated the meeting.\nMeeting and analysis process.  In each meeting, a\nlifecycle-centric view on product creation was taken. A\nlifecycle-centric view was reported valuable for the\ninvestigation of the effects of virtual development on\nproduct quality: virtual development projects are\ntypically structured as a hierarchy of lifecycles,\nreflecting the decomposition of projects into sub-projects\n[17]. An example of a generic V-lifecycle, deployed in a\nvirtual development setting is given in figure 2.\nFigure 2. Generic V-lifecycle as deployed in a typical\nvirtual development context.\nFor each lifecycle phase, the participants discussed\nabout typical defects arising in that specific phase,\nadversely affecting product quality. Subsequently,\npossible causes were identified for those defects of\nwhich there was a substantiated opinion that their\ninjection (or non-detection) is significantly influenced by\nthe nature of the project (i.e. in a virtual development\ncontext). Identification of causes was supported by\nbrainstorming about what possibly could go wrong in the\narea of communication, process, organization and\ntechnology (i.e. the risk categories). Substantiation was\nto be provided either by statements found in literature on\ndefect detection and prevention or by the participants\u2019\nown experiences gained from the outcomes of defect\ncausal analyses in earlier projects. By using the cause-\neffect graphing technique [11], the participants tried to\nsystematically identify all possible causes for injection\nand non-detection. Causes were assigned to one of the\nrisk categories: communication, process, organization or\ntechnology. Only in case of irresolvable doubts, it was\nallowed to assign the cause to multiple risk categories.\nTo ensure focus and attention from the participants, the\nduration of the meetings was limited to a maximum of\nthree hours.  Six analysis meetings were held, each with\na different combination of the six experts. The number of\nparticipants in a meeting was deliberately kept small, as\nto avoid negative group effects like cognitive inertia,\ndominations and production blocking [18]. Each meeting\nbuilt upon the results obtained in previous meetings. In\nthis way, the collection of causes was gradually\nreviewed, refined and inter-subjectively extended. At the\nend of a meeting, an evaluation was held in which the\nexperts were invited to give their opinion about the\nanalysis process and how they perceived the results.\n3.3 Investigation Results\nFigure 3 and 4 show examples of the cause-effect\ngraphing performed during the causal analysis meetings.\nFigure 3 is an analysis example of defect injection\nshowing causes for the injection of specification defects\n(e.g. wrong, missing, unclear). Figure 4 is an analysis\nexample of non-detection and lists causes for defects not\nbeing detected at system testing. For illustrative purposes\nthe cause-effect graphs have been simplified.\nTechnical\nRequirements\nSpecification\nHigh-level\nDesign\nDetailed\nDesign\nImplementation\nUnit Test\nIntegration\nTest\nSystem\nTest\nSystem\nArchitectural\nDesign\nSystem\nTechnical\nRequirements\nCustomer\nRequirements\nSpecification\nIntegration\nTest\nSystem Test\nAcceptance\nTest\nSYSTEM\nSUB-SYSTEMS\nProject X\nProject Y\nProject Z\nProceedings of the Eleventh Annual International Workshop on Software Technology and Engineering Practice (STEP\u201904) \n0-7695-2218-1\/04 $20.00 \u00a9 2004 IEEE\n5Figure 3. Example of a fish-bone diagram showing\ncauses for injection of specification defects.\nFigure 4. Example of fish-bone diagram showing\ncauses for non- or late-detection at system testing.\nA summary of the analysis results for the injection of\ndefects is given in Appendix 1, answering research\nquestion 1: What are typical causes for the injection of\ndefects in virtual development? The rows represent the\nvarious lifecycle phases given in Figure 2, while the\ncolumns represent the risk categories. Each table cell\ncontains potential causes for the injection of defects\nduring the corresponding lifecycle phase.\nThe table given in Appendix 2 has a similar\nconstruction as the table given in Appendix 1, but here\nthe cells contain causes for non- or late-detection of\ndefects during the corresponding lifecycle phase. It\ncontains answers to research question 2: What are\ntypical causes for non-  or late detection of defects in\nvirtual  development? Note that the same cause can\nappear in multiple cells. However, the cause is mostly\nonly mentioned in the cell where it was found to be most\nsignificant (i.e. present in practice). Also note explicitly\nthat causes that would be appearing in co-located\ndevelopment as well are left out. Causes like these are\nonly mentioned if their likelihood of occurrence was\nconsidered to be higher in a virtual development context\nthan in a co-located development context.\n4 Discussion\n4.1 Discussion of the approach\nThe alternative approach yielded tangible\ninformation about defect causes in virtual developments.\nThe evaluations, held at the end of each analysis\nmeeting, learned that the participating experts considered\nthe results representative for defect causes in virtual\ndevelopment projects. In later meetings, experts\nrecognized and confirmed the causes that had been\nidentified in previous meetings by different experts,\nwithout any exception.\nAn issue discussed was whether people can retrieve\ninformation easily and reliably from long term memory.\nThe prevailing opinion was that the systematic lifecycle-\nbased brainstorming, the extended cause-effect\nreasoning, the usage of risk categories and the\ninteraction of participants with different viewpoints\neffectively stimulated the retrieval of long-term memory\ninformation. Participants independently expressed their\nconfidence in the completeness of the results. We\nconclude that determination of defect causes based upon\naccumulated expert knowledge can be considered as a\npractical and valid alternative to an in-depth defect\ncausal analysis on a per-project basis.\nFuture application of this approach may benefit from\nthe following observations:\nParticipants Involvement. Overall, the participating\nexperts exhibited great interest in the investigation and\nwere highly motivated to take part in the analysis\nmeetings. The opportunity to brainstorm in-depth about\nthe root causes of defects was perceived as a strong\nmotivator: in actual projects there is hardly any\npossibility to do so, because of time and cost constraints.\nSome of the experts also mentioned that the discussions\nwith fellow-participants contributed favorably to their\nunderstanding of the problems encountered in virtual\ndevelopment. All expressed the interest in receiving a\ncopy of the final investigation report.\nRole of the Moderator.  Strong moderatorship was\nneeded to ensure focus in the analysis meetings.\nParticipants tended to continue discussing issues\ndeviating from the analysis goals. A recurrent issue\nSPECIFICATION\nDEFECT\nCOMMUNICATION PROCESS\nTECHNOLOGYORGANISATION\nBad traceability\nof requirements\nover projects\nNo change control\nauthority installed\nChanges in\nrequirements not\ncommunicated to\nrelated projects\nUnclear responsibilities\nbetween projects for\nimplementation of\nrequirements\nConflicting\nrequirements between\nprojects\nImplicit assumptions\nnot communicated to\nother projects\nDifferent interpretation of\nimplicit requirements\nDelayed\ncommunication of\nchanges in\nrequirements to\nrelated projects\nPoor\norganisation\nUnjust trust in\nexpertise of other\nproject's staff\nNo support for\nreviewing in distributed\nprojects\nDEFECTS NOT\nDETECTED AT\nSYSTEM TESTING\nCOMMUNICATION PROCESS\nTECHNOLOGYORGANISATION\nBlind spots\nin test coverage\nNo deliberation on test\napproach between\nprojects\nUnjust reuse\nof test specs\nfrom other\nprojects\nChanges in test object\nnot communicated by\nother projects\nBad regression testing\nNo stakeholder\ninvolvementof related\nprojects\nLocally insufficient\ntechnology\navailable\nInsufficient local\ndomain knowledge\nUnjust trust in\ntest effectiveness of\nother project's staff\nResponsibilities not\nassigned\nInadequate test\norganization\nBad duplicate of test\nenvironment of related\nproject\nNo representative\nenvironment available\nInsufficient local\nknowledge about\nenvironment\nInadequate provision\nof test information\nHiding policy\nBad test specs\nDifferent interpretation\nof test results\nUnjust trust in pretesting by\nother projects\nUnclear test\nconditions of\nother projects\nProceedings of the Eleventh Annual International Workshop on Software Technology and Engineering Practice (STEP\u201904) \n0-7695-2218-1\/04 $20.00 \u00a9 2004 IEEE\n6concerned the division of defect-finding responsibilities\nbetween developers and testers. Another issue tending to\nextensive discussions was the question of whether\ncertain defect types can be detected at all in a specific\nlifecycle phase.\nCause-Effect Graphing.  Cause-effect graphing was\nfound to be a useful tool for experienced based root-\ncause determination. However, cause-effect graphing\nturned out to be a laborious process, because of the\nlength and inter-relation of the cause-effect chains. A\ncause can be the effect of another cause (or a\ncombination of other causes). Eventually, a root cause\nmight be a complex interaction of elements pertaining to\none or more of the risk categories. This makes the\nassignment of a cause to one single risk category at least\ndisputable. Either objective discrimination criteria are\nneeded for assigning a cause to a single risk category, or\nit should be clearly allowed to assign causes to multiple\nrisk categories.\n4.2 Discussion of the results\nInjection of defects.   Previously, Van Moll et al. [17]\nreported a case study indicating that transitions between\nlifecycles of sub-projects are particularly sensitive to\ndefect injection. While their study focused on the\nlocations of defect injections, the current study explores\nthe causes of defect injections at the transitions. The data\nfrom the current investigation amplifies the finding that\nthe transitions between lifecycles of sub-projects from\nvirtual development projects are defect sensitive. The\ntable in Appendix 1 shows a concentration of causes in\nthe Technical Requirements Specification and\nIntegration Test phase (of system project X). The\nTechnical Requirements Specification shows a relatively\nhigh number of causes, and may inherently be more\nsensitive to injection than other phases. As in this phase,\ninformation is being processed that has been transferred\nfrom one context (that of Project X) to another context\n(that of Project Y), the phase is said to be situated at a\ntransition between lifecycles. A relatively high number\nof potential causes increases the likelihood of defects\nbeing injected at such lifecycle transitions.\nDefect injection occurring in the Technical\nRequirements Specification phase can be considered\nsevere as in an actual virtual development project, a\ngiven Requirements Specification is a source for derived\nRequirements Specifications to multiple sub-projects.\nThis means that defects are likely to disperse in a virtual\ndevelopment context.\nRegardless of the project context (i.e. virtual or co-\nlocated), defects are typically injected in either\nrequirements, design or implementation phases. In\nvirtual development, no new types of defects (i.e.\nexclusively occurring in virtual development context) are\nto be expected. Rather, virtual development increases the\nlikelihood of defect injection because new defect causes\noccur and causes already existing in co-located\ndevelopment are more likely. Consequently, dispersion\nof defects as well as an increased likelihood of injection\nmay lead to a higher number of defects in the delivered\nproduct.\nDefect injection is significantly increased in\nsituations where changes in the requirements, design or\nimplementation are being handled. Proper handling\nincludes change control authority, impact analysis and\nthe communication of changes. It was observed that a\nmultitude of causes relates to the handling of changes.\nAn example is that changes are not, or not clearly\ncommunicated to the appropriate parties involved or are\nnot unanimously agreed upon. While non-adequate\nhandling of changes is already severe in co-located\ndevelopment, virtual development projects even seem to\naggravate the effects, resulting in additional defect\ninjection causes.\nNon-detection or late-detection of defects. Causes of\nnon- or late-detection are concentrated in the Integration\nTest phase of the system-level project (project X). At\nthis lifecycle transition, the components developed in the\nsub-projects are integrated and subsequently tested.\nDispersed defects (especially from the Requirements\nPhase transitions) will become painfully visible here.\nAppendix 2 shows that defects that should have been\nfound earlier, are causing integration difficulties and\nproject delays. Lacking or insufficient test coordination\n(over the entire virtual development project) seems to be\nthe major cause of non or late-detection of defects.\nProject delays threaten the execution of a proper\nintegration test. Projects tend to rely upon the subsequent\nsystem test as a fall-back, not or insufficiently realizing\nthat certain types of integration related defects cannot be\ndetected at this later phase.\nThe danger of non- or late-detection of defects\nespecially lurks in situations of unclarity about test\ncoordination. Test coordination includes the action of\ndistributing test focus over the product by the various\nparties involved, the assigning of test responsibilities and\nprocessing of test results.  Even in co-located projects,\ninsufficient attention for test coordination and test\napproach by project management often results in\nproblems with product quality. In virtual development\nprojects the effects of lacking or insufficient test\ncoordination seem to be aggravated, resulting in\nadditional causes for non- or late-detection.\n5 Conclusions\nIn this paper, we have explored defect causes in\nproducts developed by virtual teams. Because of the\npractical difficulties of obtaining hard project-specific\nProceedings of the Eleventh Annual International Workshop on Software Technology and Engineering Practice (STEP\u201904) \n0-7695-2218-1\/04 $20.00 \u00a9 2004 IEEE\n7defect data, an approach was taken that relied upon\naccumulated expert knowledge. This approach was\nfound to be a practical alternative to an in-depth defect\ncausal analysis on a per-project basis.\nIn causal analysis-like meetings, experts identified\ncauses for defect injection and non- or late detection of\ndefects, considering the individual phases in a hierarchy\nof related projects constituting a virtual project. Causes\nwere assigned to risk categories Communication,\nProcess, Organization and Technology.\nCauses for defect injection were primarily found at\nthe Technical Requirements Specification  phases around\nthe transitions from one project to another, early in the\nlifecycle. Causes for non- or late detection of defects\nwere primarily found at the Integration Test phases\nsituated at the transitions from one project to another,\nlate in the lifecycle.\nA main limitation of this study is the relatively small\nnumber of participating experts. Furthermore, the results\ndepend on expert judgement, assuming that people can\nretrieve information reliably from long term memory,\nand that the influence of negative group effects is\nnegligible. However, we don't have indications that these\nlimitations invalidate the results.\nAcknowledgements\nWe owe many thanks to all experts who participated\nin our analysis meetings. They sacrificed many hours of\ntheir free time, discussing and analyzing defect causes\nand contributing to the execution of our investigation.\nWe are also indebted to Maggie Larragy (Philips\nSemiconductors-RTG, Einhoven, The Netherlands) for\nher helpful comments.\nReferences\n[1] Briand, L.C.,Freimut, B., and Vollei, F.,\nAssessing the Cost-Effectiveness of Inspections\nby Combining Project Data and Expert Opinion,\nProceedings of the 11th International\nSymposium on Software Reliability\nEngineering, San Jose, 2000, pp. 124-135.\n[2] Card, D., Learning from our Mistakes with\nDefect Causal Analysis, IEEE Software, Jan.\n1998, pp. 56-63.\n[3] Carmel, E., Global Software Teams:\nCollaborating Across Borders and Time Zones,\nPrentice Hall PTR, 1998.\n[4] Chillarege, R.,  Bhandari, I., Chaar, J., Halliday,\nM. , Moebus, D., Ray, B., and Wong,\nM.,Orthogonal Defect Classification-A concept\nfor in-process measurements, IEEE Transactions\non Software Engineering, vol.18, Nov. 1992, pp.\n43-956.\n[5] Chulani, S., Bayesian Analysis of Software Cost\nand Quality Models, PhD Dissertation,\nUniversity of Southern California, May 1999.\n[6] Garvin, D., What does \u201cProduct Quality\u201d really\nmean? Sloan Management Review, Fall 1984,\npp. 25-43.\n[7] Grady, R.B., Practical Software Metrics for\nProject Management and Process Improvement,\nPrentice Hall, 1992.\n[8] Hammar, M., Heverius, J., Centrifugal Forces in\nGlobal Software Development \u2013 Applying the\nHammerius Model in the Collaboration between\nIFS Sweden and IFS Sri Lanka, Master Thesis,\nUniversity of Link\u00f6ping, Sweden, 2000.\n[9] Herbsleb, J.D., and Moitra, D., Global Software\nDevelopment, IEEE Software, March\/April\n2001, pp. 16-20.\n[10] IEEE std 1044-1993. IEEE Standard\nClassification for Software Anomolies, 1993.\n[11] Ishikawa, K., Guide to Quality Control, White\nPlains, N.Y., Quality Resource, 1971, 1989\n[12] Karolak, D.W., Global Software Development ,\nIEEE CS, Los Alamitos, CA, USA, 1998.\n[13] Lonchamp, J., Collaboration Flow Management:\na New Paradigm for Virtual Team Support,\nDEXA 2002, Aix-en-Provence, 2002.\n[14] Maidantchick , C., and da Rocha, A.R.C.,\nManaging a worldwide software process.\nProceedings International Workshop on Global\nSoftware Development, ICSE 2002, Orlando,\nFlorida, USA, 2002.\n[15] Mays, R.G., Jones, C.L., Holloway, G.J., and\nStudinski, D.P., Experiences with Defect\nPrevention, IBM Systems Journal, vol.29, no. 1,\n1990, pp. 4-32.\n[16] McMahon, P.E., Distributed Development:\nInsights, Challenges, and Solutions, Crosstalk,\nNovember 2001, pp. 4-9.\n[17] Moll, J.H. van, Jacobs, J.C., Kusters, R.J., and\nTrienekens, J.J.M., Defect Detection Oriented\nLifecycle Modeling in Complex Product\nDevelopment, Information and Software\nTechnology, vol.46, no. 10, 2004, pp. 665-675\n[18] Nunamaker, J.F., Dennis, A.R., Valacich, J.S.,\nVogel. D.R., George, J.F., Group Support\nSystems Research: Experience from the Lab and\nField, In: L.M. Jessup and J.S. Valacich (Eds),\nGroup Support Systems, New York, MacMillan\nPublishing Company, 1993.\nProceedings of the Eleventh Annual International Workshop on Software Technology and Engineering Practice (STEP\u201904) \n0-7695-2218-1\/04 $20.00 \u00a9 2004 IEEE\n8Appendix 1: Identification of causes for defect injection\nProject* Injection at Communication Process Organization Technology\nX Customer Requirements\nSpecification\nX System Technical\nRequirements\nChange control\nauthority over-\nconcentrated in one\nproject. Impact analysis\nof changes on other\nprojects difficult.\nX System Architectural\nDesign\nNo involvement of\nstakeholders of related\nprojects when creating\ndesign\nNo general agreement\non error handling made\nY Technical Requirements\nSpecification\nPeople have different\ninterpretation of\nimplicit requirements\nUnjust trust in expertise\nof other project\u2019s staff\n(e.g. have the experts do\nthe work, despite\nmissing specs and trust\non their expertise)\nExpert sheltering (don\u2019t\ntell us how to do it, we\nare the experts)\nLack of trust in other\nprojects, resulting in\ninformation hiding\nimplicit assumptions\nnot communicated to\nother projects\nchanges in requirements\nnot communicated to\nrelated projects\nunclear responsibilities\nbetween projects for the\nimplementation of\nrequirements\nBad traceability of\nrequirements over\nprojects\nInteraction between\nproduct parts not clear\n(different assumptions\nmade)\nOrganizational structure\ndelays communication\nof changes in\nrequirements to related\nprojects\nNo change control\nauthority installed\nNo support for\nreviewing in distributed\nprojects\nY High-level Design Higher-level (system)\ndesign decisions not\ncommunicated to\nrelated projects\nHigher-level (system)\ndesign decisions not\nclear\nInteraction between\nproduct parts not clear\n(different assumptions\nmade)\nY Implementation Improper base-lining\nover projects\nUsage of different\nimplementation\nstandards by other\nprojects\nConventions used by\nother projects are\nunclear (e.g. error code\nranges)\nIncorrect interpretation\nof test code by other\nprojects\nNo instant access to\nother projects\nimplementation\ninformation (e.g. code,\ndocumentation)\nX Integration Test Additional defects as\nside-effects of\nworkarounds needed to\nbypass integration\nproblems\nAdditional defects as\nside-effects of hasty\nfixes by integration\nteam because actual\nAdditional defects as\nside-effects of duplicate\nsolving of problems by\nmultiple projects\n(unclear responsibilities\nfor problem solving)\nProceedings of the Eleventh Annual International Workshop on Software Technology and Engineering Practice (STEP\u201904) \n0-7695-2218-1\/04 $20.00 \u00a9 2004 IEEE\n9problem solvers from\nprojects are not\navailable at time and\nlocation of integration\n- Operational\nDocumentation\n(e.g. service manual,\noperating manual,\ninstallation manual, user\nmanual)\nNo contact possible\nbetween development\nand operations: difficult\nto compile user\ndocumentation.\nIncorrect reuse of parts\nof existing\ndocumentation.\nInput from other\nprojects (e.g.\ndevelopment\ndocumentation) is\nunclear\nCoverage of user\ndocumentation for the\nend product is poor, due\nto each project defining\nits documentation in\nisolation\nResponsibility for\nintegral user\ndocumentation not\ndefined\nInsufficient knowledge\n(domain, product) to\ncreate documentation\nNo environment\navailable assisting in\ncompiling user\ndocumentation.\n* Refers  to the generic lifecycle given in Figure 2.\nAppendix 2: Identification of causes for non-detection or late-detection of defects\nProject* Non-detection at Communication Process Organization Technology\nX Customer Requirements\nSpecification\nX System Technical\nRequirements\nX System Architectural\nDesign\nY Technical\nRequirements\nSpecification\nOverall review strategy\nnot established\nNo stakeholder\ninvolvement of related\nprojects in review of\nspecifications\nReviewers not trained in\nreviewing of document\nhierarchies\nY Unit Test Stubs and drivers to\nsimulate other units\nbased on incorrect\nassumptions\nY System Test Incorrect assumption\nthat certain aspects will\nbe tested by other\nproject\nRelevant stakeholders\nof other projects not\ninvolved in reviewing\nof test specifications\nX Integration Test Unreported problems by\nother projects prevent\nadequate integration\ntesting. (delay->forced\nskipping of tests)\nDeliberate deviations\nfrom agreed integration\napproach not\ncommunicated to other\nprojects.\nHidden test features in\nobjects not\ncommunicated by other\nprojects. Features not\nused.\nProblem solvers not\npresent at time and\nlocation of integration.\nIncorrect assumptions\nregarding integration\nplan (delay->forced\nskipping of tests)\nIntegration approach\nnot defined resulting in\ndelay or order of\ndelivery conflicts.\nForced skipping of tests\nObjects to be integrated\nnot available at planned\ntime. (delay->forced\nskipping of tests)\nStakeholders of other\nprojects not involved in\nreviewing of test\nspecifications\nResponsibility for\nintegration testing not\nexplicitly defined.\nPreviously informal\nresolution of problems\nin objects to be\nintegrated (direct\ncontact between\ndevelopers in different\nprojects)\nUnclear who is\nresponsible for solving\nproblem encountered.\nDuplicate solving by\nvarious projects-\n>additional defects\n(delay->forced skipping\nof tests)\nConcealed shutting\ndown of functionality\nby other projects\u2019\nobjects, unknown to\nintegration tester.\nUndocumented\n(incomplete) self tests\nby other projects\u2019\nobjects.\nConcealed simulators in\nobjects delivered by\nother projects,\ngenerating output data\nincorrectly assumed to\nbe all right.\nBehavior of simulators\ncreated by others\nprojects unknown or\nProceedings of the Eleventh Annual International Workshop on Software Technology and Engineering Practice (STEP\u201904) \n0-7695-2218-1\/04 $20.00 \u00a9 2004 IEEE\n10\nCauses delays-\n>skipping tests.\nSolving undocumented\nlast-minute changes in\ntest objects interface\n(delay->forced skipping\nof tests)\nOveremphasis of\ntesting. Testing only\nthose objects\nexperienced as\nproblematic by the\nintegrator at the cost of\nother objects.\nToo much focus on\n\u2018positive testing\u2019 after\nsevere integration\nproblems. Negative\nscenarios not executed.\nNo coupling of earlier\nreview results to\nintegration test strategy\n(inadequate testing)\nChange control\nauthority over-\nconcentrated in one\nproject. Impact analysis\nof changes on other\nprojects difficult.\nmisinterpreted.\nNo general agreement\nby all projects on design\nfor testability\nNo infrastructure\npresent for integration\nIntegration platform not\ndefined\/unclear (delay-\n>forced skipping of\ntests)\nTest software not\navailable or unclear.\nOther integration\nplatform used.\nX System Test Unjust trust in detection\neffectiveness of other\nproject\u2019s staff\nDifficulties in\nunderstanding test\nresults produced when\ntesting objects created\nin other projects\nNot-repaired defects not\ncommunicated by other\nprojects. Test results\nand test object behavior\nunjustly assumed to be\ncorrect.\nBlind spots in test\ncoverage. No agreement\non test approach is\nmade with other\nprojects.\nStakeholders of other\nprojects not involved in\nreviewing of test\nspecifications.\nMissing cross-\nverification of solved\ndefects by other projects\nInadequate\/no\nregression testing by\nother projects while no\nregression test done\nhere.\nUnjustified reuse of\nother projects\u2019 test\nspecifications. Specs\ninadequate.\nResidual test code\nincorrectly assumed as\nreal code by other\nprojects.\nResponsibility for final\nsystem testing not\nexplicitly defined.\nTesters not having a\nproduct focus. Difficult\nto create adequate test\nspecifications for\nsystem parts they are\nunfamiliar with.\nAutomated tests not\nclear\/undocumented\nTest automation tool\nused in other projects\nnot available\nNo adequate test\nenvironment available\nfor objects produced by\nother projects (e.g. too\nexpensive)\nDifferences in test\nenvironments between\nprojects (e.g. in\nenvironmental\nconditions like\nhumidity, temparature)\nTest conditions of other\nprojects unclear. No\nregression testing\npossible.\nNo possibility to\nremotely monitor tests\ndone in other projects\nTest input data used by\nother projects is\nunclear\/unavailable.\nX Acceptance Test Local differences in\nusage of product.\nFunctionality by\ninfrequent scenarios not\ncovered.\nWrong stakeholders\ninvolved in execution of\nacceptance test\n* Refers  to the generic lifecycle given in Figure 2.\nProceedings of the Eleventh Annual International Workshop on Software Technology and Engineering Practice (STEP\u201904) \n0-7695-2218-1\/04 $20.00 \u00a9 2004 IEEE\n"}