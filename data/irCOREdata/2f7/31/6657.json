{"doi":"10.1080\/0968776980060205","coreId":"6657","oai":"oai:generic.eprints.org:273\/core5","identifiers":["oai:generic.eprints.org:273\/core5","10.1080\/0968776980060205"],"title":"A unique style of computer\u2010assisted assessment","authors":["Thelwall, Mike"],"enrichments":{"references":[{"id":192691,"title":"A knowledge-based diagnostic test of basic mathematical skills',","authors":[],"date":"1997","doi":"10.1016\/s0360-1315(97)00001-8","raw":"Appleby J., Samuels P. and Treasure-Jones, T. (1997), 'A knowledge-based diagnostic test of basic mathematical skills', Computers in Education, 28 (2) 113-31. Broadnet (n.d.), The Broadnet Project, http:\/\/www.broadnet.co.uk.","cites":null},{"id":1041869,"title":"A Study of Student Use of the Mathematics Computerised Assessment Test, internal report,","authors":[],"date":"1998","doi":null,"raw":"Thelwall, M. (1998), A Study of Student Use of the Mathematics Computerised Assessment Test, internal report, Wolverhampton University.","cites":null},{"id":1041868,"title":"A Study of Student Use of the Statistics Computerised Assessment Test, internal report,","authors":[],"date":"1997","doi":null,"raw":"Thelwall, M. (1997), A Study of Student Use of the Statistics Computerised Assessment Test, internal report, Wolverhampton University.","cites":null},{"id":192694,"title":"Automatic assessment of elementary Standard ML programs using Ceilidh',","authors":[],"date":"1997","doi":"10.1046\/j.1365-2729.1997.00012.x","raw":"Foubister, S.P., Michaelson, G.J. and Tomes, N. (1997), 'Automatic assessment of elementary Standard ML programs using Ceilidh', Journal of Computer-Assisted Learning, 13 (2), 99-108. Knowledge (n.d.), The Knowledge Space Project and ALEKS, http:\/\/aleks.uci.edu\/ and http:\/\/www.spaces.uci.edu\/. Online Exercises (n.d.), http:\/\/math.uc.edu\/WWW-test\/demo\/demo.html.","cites":null},{"id":1041870,"title":"Computer-assisted assignments in a large physics class',","authors":[],"date":"1996","doi":"10.1016\/0360-1315(96)00003-6","raw":"Thoennessen, M. and Harrison, M.J. (1996), 'Computer-assisted assignments in a large physics class', Computers in Education, 27 (2).","cites":null},{"id":192692,"title":"Computer-based assessment: some issues for consideration',","authors":[],"date":"1994","doi":null,"raw":"56ALT-} Volume 6 Number 2 Bull, J. (1994), 'Computer-based assessment: some issues for consideration', Active Learning 1, 18-21.","cites":null},{"id":192695,"title":"Using computer-assisted assessment: time saver or sophisticated distraction?',","authors":[],"date":"1994","doi":null,"raw":"Stephens, D. (1994), 'Using computer-assisted assessment: time saver or sophisticated distraction?', Active Learning 1, 11-15.","cites":null},{"id":192693,"title":"Using computer-based tests for information science',","authors":[],"date":"1997","doi":"10.1080\/0968776970050105","raw":"Callear, D. and King, T. (1997), 'Using computer-based tests for information science', ALTJ, 5 (1), 27-32.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"1998","abstract":"This paper examines a project at the University of Wolverhampton that has been producing its own unique style of computerized test for several years. The tests are all designed to deliver a different set of questions each time they are run, a fact which enables many of them to double as learning resources. Most of the tests are used for both formative and summative assessments on Level 1 modules, in conjunction with more traditional assessment methods","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/6657.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/273\/1\/ALT_J_Vol6_No2_1998_A_unique_style_of_computer_ass.pdf","pdfHashValue":"a222506360140bb6d69f05aae217e6e1c3d03053","publisher":"University of Wales Press","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:273<\/identifier><datestamp>\n      2011-04-04T08:50:07Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/273\/<\/dc:relation><dc:title>\n        A unique style of computer\u2010assisted assessment<\/dc:title><dc:creator>\n        Thelwall, Mike<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        This paper examines a project at the University of Wolverhampton that has been producing its own unique style of computerized test for several years. The tests are all designed to deliver a different set of questions each time they are run, a fact which enables many of them to double as learning resources. Most of the tests are used for both formative and summative assessments on Level 1 modules, in conjunction with more traditional assessment methods.<\/dc:description><dc:publisher>\n        University of Wales Press<\/dc:publisher><dc:date>\n        1998<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/273\/1\/ALT_J_Vol6_No2_1998_A_unique_style_of_computer_ass.pdf<\/dc:identifier><dc:identifier>\n          Thelwall, Mike  (1998) A unique style of computer\u2010assisted assessment.  Association for Learning Technology Journal, 6 (2).  pp. 49-57.  ISSN 0968-7769     <\/dc:identifier><dc:relation>\n        10.1080\/0968776980060205<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/273\/","10.1080\/0968776980060205"],"year":1998,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"A unique style of computer-assisted assessment\nMike Thelwall\nSchool of Computing and Information Technology, University of Wolverhampton. Email:\ncm 1993@wlv.ac.uk\nThis paper examines a project at the University of Wolverhampton that has been\nproducing its own unique style of computerized test for several years. The tests are all\ndesigned to deliver a different set of questions each time they are run, a fact which\nenables many of them to double as learning resources. Most of the tests are used for\nboth formative and summative assessments on Level 1 modules, in conjunction with\nmore traditional assessment methods.\nIntroduction\nThe Computer-Based Assessment project at the University of Wolverhampton has\nproduced a number of tests built around a common framework. There are three main tests\nthat are used by about 800 students per year, and they replace written tests. There are also\ntwo diagnostic tests that are not part of any formal assessment but are used by about 400\nstudents per year. We have produced numerous special versions of these tests for short\ncourses, different teaching techniques, and one for a Broadnet Online training module\n(Broadnet, n.d.). The tests are all written with 80,000 permutations built in to allow them\nto be made available for students to use and practise on at all times without compromising\nsecurity. In fact, there are no serious security concerns because of the random factors, and\nso the same tests are used every year without the need to be hidden or rewritten. Similar\nadvantages have been found on other projects using random factors; see for example\nThoennessen and Harrison (1996).\nThe project has been producing random-based computerized tests for modules for several\nyears now. It began with a single statistics test, the success of which led to the use of the\ntechnology for the production of others. Our criteria for accepting a module for a\ncomputerized test are that it must have a large number of students and that the tests should\nbe able to be used for a number of years. We have produced five random-based PC tests so\nfar; two of which have a number of versions for different modules and situations. Three of\nthese replace written tests in maths, stats and IT, while the other two are diagnostic tests for\nthe numeracy and computing skills of new students.\n49\nMike Thehvall A unique style of computer-assisted assessment\nThe main features common to all the tests are as follows.\n\u2022 Detailed feedback is given at the end of each test.\n\u2022 Random factors are built in so that each use of the program produces a different set of\nquestions.\n\u2022 The assessment grade is shown on-screen at the end of the test (for students and\ninstructors) and is saved automatically to a common database together with\ncomprehensive information about the test. Lecturers using a Web browser can access\nthis data to analyse individual student performances or to produce summary statistics.\n\u2022 Once written, the same test can be used every year without modification.\n\u2022 The majority of questions are not multiple choice or questions expecting keyword answers.\nOn a technical level the tests have a number of distinctive attributes, as follows.\n\u2022 They run in Windows versions 3.1 and higher.\n\u2022 They are written in a programming language rather than with a specialist quiz\nauthoring tool such as Question Mark Designer.\n\u2022 Each test consists of one program file and two standard library files, all of which fit on\na single floppy disk. There is no data file, the data being stored in the program.\n\u2022 Data is saved using Internet protocols, a method that works on any computer\nconnected to the Internet, even a student's home PC.\nAt the start of the project we were tempted to use the Web to run the tests, but rejected this\nas not yet giving the very high reliability needed for large-scale summative assessments.\nOthers have managed to use the Web for assessing technical subjects in a different way (see\nOnline Exercises, n.d).\nAlthough the same technique and randomization are used for the production of these tests,\nthey fall into three different groups: assessment tests available all the time; assessment tests\navailable only for the actual test; and diagnostic tests which do not count as part of the\nassessment for any module. The maths and stats tests that are covered in most detail here\nare available at all times for students to practise on. They count for assessment only when\ntaken in the actual test session. The IT test is password-protected, and the students use it\nonly for the actual assessment test. It was thought that the nature of the module meant\nthat there would be no benefit to students from practising it. This was mainly because they\nwould end up practising the test in the workshop time instead of using the package they\nwere supposed to be learning. The other two tests are diagnostic tests for numeracy and\ncomputing which do not count towards assessment. These ask questions depending on the\nanswers to previous questions.\nThe method of producing the tests has been to obtain a number of past test papers for the\nmodule from the appropriate lecturer and to produce a prototype computer test based on\nthese. This is then shown to the module leader for comment and amendments. We found\nthat this method was more practical than involving lecturers in the production of the\nmaterial at an earlier stage as they often were uncertain about what was possible for their\nmodule or how to introduce the randomization.\nso\nALT-J Volume 6 Number 2\nA comparison of the computerized tests and their written\npredecessors\nThe tests produced by the project matched the written tests that they replaced to varying\ndegrees. All assessed the same general learning outcomes as their written counterparts.\nSome types of question from the original written versions could not be asked in the same\nway, but many others could. For example, the computerized stats test questions looked\nvery similar to the paper versions but most of the IT test questions changed completely,\nalthough testing broadly the same knowledge. There was an attempt to make the\ncomputerized questions smaller or to break them up into parts so that there would not be\nquestions or parts of questions with large all-or-none mark allocations. Questions with\nlarge mark allocations that could be lost with a single mistake would be unfair to the\nstudents.\nThe exercise of rewriting the tests for the new medium was an interesting one, helping us to\nrethink assessment criteria and to produce new types of questions. All except one of the\ntests ended up with at least one question that was some form of multiple choice. Despite\nthis, multiple-choice questions were not extensively used in any test as it took more time to\nproduce a sufficiently large random selection of them than with other types of question.\nI shall discuss the particular characteristics of each test in turn.\nThe statistics test\nThis test was probably the closest match to the written version, with a similar number and\nstyle of questions. Most questions in the original test described a situation, gave some data,\nthen asked for statistical calculations and a report on the results. This was done in the same\nway in the computerized version except that a report was not asked for; instead a multiple-\nchoice question concerning the conclusions of the report was included on the same screen\nas the boxes for the answers to the calculations. Some of the aspects of the report that were\nnot tested in this way were tested in additional multiple-choice questions later in the test.\nAlthough the learning outcome of being able to write an entire report was not tested, all\nthe individual component parts of the report were asked for in one question or another.\nFigure 1 shows a sample question from the test.\nThe Information Technology test\nThis produced the greatest change from the written to the computerized version, a fact due\nto the nature of the subject material; I believe that a computerized test is more natural for\nthis situation. The original paper test was based on short written answers that were\nawkward for computer-marking. We did, however, devise a method of marking short\nanswers with deductive logic. An example of this approach is: 'If they have put the correct\nanswer, then they must have used words A, B and C or D and E, so we will mark it correct\nif it has A, B and C or D and E in it'. This worked surprisingly well. One of the questions\nis: 'Describe how to start Word from the Windows Main Menu screen'. There is a box for\nthe answer to be entered, and the marking scheme will give full marks if the answer\ncontains the words: 'double', click' and 'icon', or the words 'click'; 'icon' and 'return', or\nthe words: 'click'; 'icon' and 'enter'. It is hard to enter a correct answer without using one\nof the three sets of words above. Any students who did so would be caught in the safety net\nof showing their answer and marks to the instructor at the end of the test to query why\nthey were marked wrong, and to get their marks restored, but this turned out to be a rare\n51\nMike Thelwall A unique style of computer-assisted assessment\n- Question Number 2 out ol 9. Worth 8 marks out of 48.- Test type Practise\n :\nA scientist claims that Bigwing butterflies have a 580 day lifespan. A sample of IE\nbutterflies weie taken and it was found that the sample mean was 572.5 and the\nsample variance was 38. Set up and test null and alternative hypotheses to check the\nscientist's claim.\n-Enter the answers in the boxes below. Enter the numbers onry.-\nThe test\nstatistic\nThe tabulated\nstatistic at the\n5* level\n3.3 The tabulated\nstatistic at\nthe IX level\n4.2\n-Chose the option which best describes the conclusion based on your statistics.-\nC There is no evidence to reject the null hypothesis. Accept the null hypothesis.\nt\u00bb jThcre is some evidence to reject the null hypothesis and accept the alternative hypothesis.!\nC There is strong evidence to reject the null hypothesis and accept the alternative hypothesis,\nr None ol the above.\nContents Last Question {tact Question\nFigure \/: An example of a question from the test\nJl A window HHE3I\nMove this window about 5cm to\nthe right and leave it there.\nClick on the OK button when\nyou have done this.\nFigure 2: A test of the ability to move a window\nevent. For this question, the students lose marks if they use any of the words 'drag', 'type'\nor 'menu' as these could not be used in a correct answer except in very obscure cases.\nNo serious complaints were made about this system from the students, even after they had\nseen transcripts of their tests. A check for words which could not possibly be included in a\ncorrect answer is made in the short-answer questions to stop students simply writing down\nas many key words as they can think of. We allocated a third of the marks on the paper to\nthese short-answer questions, and invented new types of question for the remaining two\n52\nALT-J Volume 6 Number 2\nthirds. These questions were mostly of types that could not be asked in a written test, for\nexample a question testing the ability to move a window with the mouse (see Figure 2)\nThis summative test counted for half of the mark for the first topic of the module, the\nother half going on continuous-assessment exercises. This is a combination similar to that\nused in an IT module at Loughborough, where a computerized multiple-choice test was\nused in addition to an essay (Stephens, 1994).\nThe maths test\nThis test replaced a written calculus and algebra test to which it was broadly similar. The\nunusual feature of it was the need for extra interfaces to display maths and to accept\nmathematical inputs from students. This made it a rather more complicated product than\nthe other tests, although it still received a high usability rating from the students.\nThis question also required a more complicated interface than needed for the other tests.\nThe answer is an algebraic expression that has to be entered with special syntax, then the\nbox at the bottom (see Figure 3) is used to check that the syntax is correct.\n- Question Number 3 out of 23. Worth 4 maiks out of 69. Type of test Real\nSimplify the following expression as much as possible.\n3 5\nr\n-8 4 r 2 -e l *r |y r J\nEnter your answer in the box below. Click on OK or\npress return when done. OK\nCheck your answer in the box below after clicking on the OK button.\nContents Last Question I Next Question\nFigure 3: The interface for the maths test\n53\nMike Thelwall A unique style of computer-assisted assessment\nThe computing diagnostic test\nComputer-aided assessment tests are widely used in computing, for example at Portsmouth\n(Callear and King, 1997). We produced a computing diagnostic test in response to a\nrequest from members of staff who had seen other tests in action. An introductory\nprogramming module at Wolverhampton has used a written programming-skill test for\nseveral years to allocate students to streamed tutorials. We produced a computerized\nversion of this to save marking time and to allow the test to be extended without creating\nextra marking. The computer test was again very similar to the written version, but we\npartly abandoned the questions asking the students to produce a program in any\nprogramming language to achieve a given task. Automated program marking is widely\nused, for example the Ceilidh system (Foubister et al, 1997), but devising a system to work\nfor a wide range of programming languages would have been very time-consuming. The\nprogramming question was still asked, but was left unmarked, the students being given the\noption of asking a live tutor to mark it on their transcript if they felt that the mark on the\nrest of the test was too low.\nQuestions on the related skill of program comprehension were included, partially to fill the\ngap. The question transcripts were all saved to enable the tutors to judge whether the\nabsence of marks for these questions was seriously skewing the results of the test. This did\nnot seem to be the case.\nThe numeracy diagnostic test\nThis is the only test in our portfolio that did not replace a written test. It was used in three\nmaths modules where it was felt that a diagnostic test was necessary. The need for\ndiagnostic testing had been previously identified but not yet actioned. Maths diagnostic\ntests have been written by others (Appleby et al, 1997; Knowledge, n.d.) with a better\ntheoretical framework, but those we tested had data-saving methods incompatible with our\nnetworks.\nAnalysis of student usage of the maths and statistics tests\nI conducted studies on the maths and statistics tests in the first semester of the 1997\/98\nyear, and wrote two internal reports (Thelwall, 1997; 1998). I used three sources of\ninformation on which to base the reports. These were the computer logs of the tests, a\nquestionnaire given to the students, and a number of informal interviews. Close analysis of\nthe data is problematical for a number of reasons. One such is that a previous study I\ncarried out showed a high correlation between students' attitudes to the test and their\ngrades on it. This could be due to an appreciation of how to use the test leading to a better\nmark, or the feel-good factor from doing well on the test. It was probably a mixture of the\ntwo.\nThe following sections summarize the general findings of the reports that are appropriate\nto this paper.\nStudent usage of the statistics test\nWe discovered that the 168 students who took the test were practising it three times on\naverage before taking it for a grade. They found it easy to use and trusted it to mark their\nwork. The students believed that the test helped them to learn and motivated them to do\nmore revision. The overall high grades on the test reflected this belief. The printed\n54\nALT-J Volume 6 Number 2\nfeedback sheet given at the end of the test was particularly popular. Figure 4 shows an\nexample of feedback on one question.\nFigure 4: An\nexample of feedback\non a question\n4) A biologist was interested in the weights of male and\nfemale Lappin\nRabbits. She weighed a random sample of both and\ncalculated the following\nsample statistics.\nSample Sample Sample\nSize Mean Variance\nMale 12 3.1 1.46\nFemale 11 4.6 1.61\nTest a suitable hypothesis.\nYour pooled variance was 22. The correct answer was\n1.531.\nYour test statistic was 34. It was wrong. The correct\nanswer for YOUR\npooled variance was -.766. The correct answer was -2.904.\nYour tabulated statistic at the 5% level was 4.432. The\ncorrect answer was\n2.08.\nYour tabulated statistic at the 1% level was 5.654.\ncorrect answer was\n2.831.\nThe\nBased on the numbers you entered your conclusion was false.\n0 out of 10\nThe feedback given is enough for the students to take away and sit the test again at home.\nMany did just that.\nThe test was a positive experience for most students, motivating them and helping them to\nlearn. In fact, I believe that it was more successful at teaching than the specially written\ncomputerized tutorial they also use.\nStudent usage of the maths test\nThe maths test was more technical and more complicated for the students than the stats\ntest. In order to enter some of the answers, students had to learn some syntax for entering\nmathematical expressions. Despite this, the conclusions from the same data sources as the\nstats test were broadly similar. The students practised slightly less on average, just under\nthree times, but still felt that they had learned from this practice.\nOther tests\nThe other programs written for the project were not designed to be used for practise for\nvarious reasons, although they do contain the same random factors and are built using the\nsame technology. Practising would be inappropriate on the diagnostic tests and formal\nassessments. As mentioned above, the IT test is password-protected, and this is to prevent\npractising, but it does count for assessment. No formal studies have been conducted on this\ntest, but I believe that it would get a lower user-satisfaction rating than the other tests\npartly because it does not allow practising.\n55\nMike Thelwatl A unique style of computer-assisted assessment\nStaff attitudes to the tests\nOne of the main aims of the project was to gain staff acceptance of computer-assisted\nassessment by getting round some of the problems which have been identified (Bull, 1994).\nThere are two main incentives for lecturers to commission one of our tests: to save marking\ntime, and to have an extra learning resource for the students. Once written, the tests require\nvery little attention. Problems that need to be fixed are sometimes identified, and syllabus\nchanges also cause alterations to the test. If no changes are wanted, the programs stay on\nthe student networks ready for the next run of the module. Staff did not raise any serious\nconcerns about the ability of the programs to assess their students, although a few issues\nwere raised. The short-answer questions were the only problematic ones, the non-\ntransparent marking technique baffling instructors and students on occasion. A more\nserious problem was that many of the weaker students took too much time on these\nquestions and did not manage to answer all the others. For next year, there will be fewer\nquestions of this type, and they will all be at the end of the test.\nAnother of the original objectives of the project was to gain long-term use of\ncomputerized testing. None of the tests has yet been dropped from a module, a fact that I\nsee as going some way towards the achievement of this objective. I attribute this staying\npower to successes with students as well as ease of use for staff. In fact, it is true that\ndropping a computerized test will result in an increase in work, both from the need to write\na new test and from having to mark it by hand.\nConclusions\nThe project has produced programs that have stood the test of time, and is continuing to\nproduce more with the same methodology. So far, each test has been easier and quicker to\nproduce than the previous one. The random base allows the tests to have long lifetimes and\navoid security problems in most cases. It also allows them to be successful as learning\nresources and thus helps them to be popular with both staff and students.\nI do not believe that there is a significant difference between the ability of the written and\ncomputerized versions of tests to assess skills and knowledge. A computerized test, seen in\nisolation from the way that it is used, is in general a somewhat blunter instrument. It is\nunable to cope easily with such things as bad spelling, less able to give 'method marks', and\nis restricted in the type of questions it can ask. These problems have been circumvented to\na large extent by the use of appropriate question designs. The tests also contain additional\nfeatures in the feedback generated and in the ability for students to have marked practices\nthat are hard to achieve with written tests.\nThe tests are available for others to use without charge from the Web site\n(http:\/\/cba.scit.wlv.ac.uk\/).\nReferences\nAppleby J., Samuels P. and Treasure-Jones, T. (1997), 'A knowledge-based diagnostic test of\nbasic mathematical skills', Computers in Education, 28 (2) 113-31.\nBroadnet (n.d.), The Broadnet Project, http:\/\/www.broadnet.co.uk.\n56\nALT-} Volume 6 Number 2\nBull, J. (1994), 'Computer-based assessment: some issues for consideration', Active\nLearning 1, 18-21.\nCallear, D. and King, T. (1997), 'Using computer-based tests for information science', ALT-\nJ, 5 (1), 27-32.\nFoubister, S.P., Michaelson, G.J. and Tomes, N. (1997), 'Automatic assessment of\nelementary Standard ML programs using Ceilidh', Journal of Computer-Assisted Learning,\n13 (2), 99-108.\nKnowledge (n.d.), The Knowledge Space Project and ALEKS, http:\/\/aleks.uci.edu\/ and\nhttp:\/\/www.spaces.uci.edu\/.\nOnline Exercises (n.d.), http:\/\/math.uc.edu\/WWW-test\/demo\/demo.html.\nStephens, D. (1994), 'Using computer-assisted assessment: time saver or sophisticated\ndistraction?', Active Learning 1, 11-15.\nThelwall, M. (1997), A Study of Student Use of the Statistics Computerised Assessment\nTest, internal report, Wolverhampton University.\nThelwall, M. (1998), A Study of Student Use of the Mathematics Computerised Assessment\nTest, internal report, Wolverhampton University.\nThoennessen, M. and Harrison, M.J. (1996), 'Computer-assisted assignments in a large\nphysics class', Computers in Education, 27 (2).\n57\n"}