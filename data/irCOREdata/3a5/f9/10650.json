{"doi":"10.1214\/10-BA524REJ","coreId":"10650","oai":"oai:dro.dur.ac.uk.OAI2:8088","identifiers":["oai:dro.dur.ac.uk.OAI2:8088","10.1214\/10-BA524REJ"],"title":"Rejoinder - Galaxy Formation : a Bayesian uncertainty analysis.","authors":["Vernon,  Ian","Goldstein,  Michael","Bower,  Richard G."],"enrichments":{"references":[{"id":646765,"title":"Handbook of Bayesian Analysis.","authors":[],"date":"2007","doi":null,"raw":null,"cites":null},{"id":646764,"title":"Pressure matching for hydrocarbon reservoirs: a case study in the use of Bayes linear strategies for large computer experiments.&quot;","authors":[],"date":"1997","doi":null,"raw":null,"cites":null},{"id":646766,"title":"The Coyote Universe II: Cosmological Models and Precision Emulation of the Nonlinear Matter Power Spectrum.&quot;","authors":[],"date":"2009","doi":"10.1088\/0004-637X\/705\/1\/156","raw":null,"cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-12-01","abstract":"We thank the discussants David Poole, Pritam Ranjan, Earl Lawrence, David Higdon, and David van Dyk for their commentaries on our paper, and for raising many interesting points for discussion, ranging from practical questions related to the imple- mentation of our methodology to fundamental issues of the role and purpose of Bayesian analysis in science","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/10650.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/8088\/1\/8088.pdf","pdfHashValue":"9df1c35e4369015114d0704381b9539615cfa7a4","publisher":"International Society for Bayesian Analysis","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:8088<\/identifier><datestamp>\n      2011-04-06T09:30:26Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Rejoinder - Galaxy Formation : a Bayesian uncertainty analysis.<\/dc:title><dc:creator>\n        Vernon,  Ian<\/dc:creator><dc:creator>\n        Goldstein,  Michael<\/dc:creator><dc:creator>\n        Bower,  Richard G.<\/dc:creator><dc:description>\n        We thank the discussants David Poole, Pritam Ranjan, Earl Lawrence, David Higdon, and David van Dyk for their commentaries on our paper, and for raising many interesting points for discussion, ranging from practical questions related to the imple- mentation of our methodology to fundamental issues of the role and purpose of Bayesian analysis in science.<\/dc:description><dc:subject>\n        Computer models<\/dc:subject><dc:subject>\n         Uncertainty analysis<\/dc:subject><dc:subject>\n         Model discrepancy<\/dc:subject><dc:subject>\n         History matching<\/dc:subject><dc:subject>\n         Bayes linear analysis<\/dc:subject><dc:subject>\n         Galaxy formation<\/dc:subject><dc:subject>\n         Galform.<\/dc:subject><dc:publisher>\n        International Society for Bayesian Analysis<\/dc:publisher><dc:source>\n        Bayesian analysis, 2010, Vol.05(04), pp.697-708 [Peer Reviewed Journal]<\/dc:source><dc:date>\n        2010-12-01<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:8088<\/dc:identifier><dc:identifier>\n        issn:1936-0975<\/dc:identifier><dc:identifier>\n        issn: 1931-6690<\/dc:identifier><dc:identifier>\n        doi:10.1214\/10-BA524REJ<\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/8088\/<\/dc:identifier><dc:identifier>\n        http:\/\/ba.stat.cmu.edu\/vol05is04.php<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/8088\/1\/8088.pdf<\/dc:identifier><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":[" 1931-6690","issn: 1931-6690","1936-0975","issn:1936-0975"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2010,"topics":["Computer models","Uncertainty analysis","Model discrepancy","History matching","Bayes linear analysis","Galaxy formation","Galform."],"subject":["Article","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n06 April 2011\nVersion of attached file:\nPublished Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nVernon, Ian and Goldstein, Michael and Bower, Richard G. (2010) \u2019Rejoinder - Galaxy Formation : a\nBayesian uncertainty analysis.\u2019, Bayesian analysis., 05 (04). pp. 697-708.\nFurther information on publisher\u2019s website:\nhttp:\/\/ba.stat.cmu.edu\/vol05is04.php\nPublisher\u2019s copyright statement:\nAdditional information:\nThis is the Rejoinder to the 4 discussants\u2019 comments to our invited discussion paper Galaxy Formation: a Bayesian\nUncertainty Analysis.\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\nBayesian Analysis (2010) 5, Number 4, pp. 697\u2013708\nRejoinder\nIan Vernon\u2217, Michael Goldstein\u2020 and Richard G. Bower\u2021\nWe thank the discussants David Poole, Pritam Ranjan, Earl Lawrence, David Hig-\ndon, and David van Dyk for their commentaries on our paper, and for raising many\ninteresting points for discussion, ranging from practical questions related to the imple-\nmentation of our methodology to fundamental issues of the role and purpose of Bayesian\nanalysis in science. We respond to each of the discussants as follows.\n1 Response to David Poole\nPoole agrees that often a fully Bayesian analysis for computer models can be difficult\nand that simplifications such as a the Bayes Linear approach described in our paper are\noften helpful. He then goes on to discuss another technique known as Bayesian Melding,\nwhereby prior information regarding both the inputs and outputs of the computer model\nfunction are amalgamated into a single prior over x. He remarks that such an approach\nwould most likely not be suitable for use on the Galform model due to computational\nreasons (as melding normally requires complete knowledge of the function f(x)). We\nwould state that while it is possible that these computational issues could be resolved\nby the appropriate use of emulators within the melding calculation, there are unresolved\nissues about the validity of the melding calculations, consideration of which would have\ntaken us beyond the remit of the study.\nPoole then asks \u201cwhat one loses by doing a Bayes Linear approach?\u201d compared to\na hypothetical fully Bayesian analysis. We respond that (as we discuss toward the end\nof section 3.3), if a fully Bayesian approach were feasible, in that we were prepared\nto spend the considerable amount of extra time and effort to construct and document\nrealistic joint priors over the input space, the model discrepancy and all other quantities\nof interest, and if such priors contained extra physical information that was defensible\nto other cosmology experts in the field, then we would be able to perform a more\ndetailed fully Bayesian inference which would reflect the additional physical information\ncontained within the prior specification.\nHowever, an elicitation for such complex objects would present substantial concep-\ntual and practical difficulty, and hence we perform a Bayes Linear analysis which may\nbe viewed as a pragmatic compromise to such an arduous analysis, as it only requires\nexpert assessments over means and variances. Were we confident enough to obtain\nmore detailed expert judgements, then we can incorporate these within the Bayes Lin-\near framework also (by writing down covariances of more complex objects e.g. higher\norder quantities (Goldstein and Wooff (2007))). We choose the Bayes Linear method to\n\u2217Department of Mathematical Sciences, Durham University, Science Laboratories, Durham, UK,\nmailto:i.r.vernon@durham.ac.uk\n\u2020Department of Mathematical Sciences, Durham University, Science Laboratories, Durham, UK,\nmailto:michael.goldstein@durham.ac.uk\n\u2021Department of Physics, Durham University, Science Laboratories, Durham, UK, mailto:r.g.\nbower@durham.ac.uk\nc\u00a9 2010 International Society for Bayesian Analysis DOI:10.1214\/10-BA524REJ\n698 Rejoinder\nobtain meaningful answers in reasonable time using reasonable effort. See the rejoinder\nto Lawrence and Higdon and to van Dyk for further comparisons between the Bayes\nLinear and full Bayes approaches.\nWhat we would suggest as unnecessary is the far too common form of Bayesian anal-\nysis whereby priors are chosen that have little physical insight and are mainly of forms\nthat provide mathematical convenience for the subsequent challenging fully Bayesian\ncalculations. We have reservations about the value and meaning of such calculations.\nWere the ensuing calculations simple and transparent, then such an approach, even so,\ncould have considerable exploratory value, but current Bayesian technology does not\nmake this easy.\n2 Response to Pritam Ranjan\nRanjan asks about the choices we have made in the history matching process. As\ndescribed in this paper and summarised in section 3, the history matching approach\ninvolves emulating appropriate collections of outputs of the computer model, and com-\nbining this with observational data in order to eliminate portions of the input parameter\nspace. Specifically, in this application we chose a subset of 7 outputs of the Galform\ncomputer model to compare to the data. This subset increased from 7 to 11 outputs in\nlater waves (these are shown as the vertical lines in figures 12 and 13). Ranjan provides\nan interesting discussion over this reduction of the data, and asks for a formal technique\nto pick a subset of outputs that are sufficient to capture most of the characteristics of\nthe functional response of the Galform model. There are techniques for this purpose,\nand one that we have used in similar applications is Principal Variables (see Cumming\nand Wooff (2007); Cumming and Goldstein (2009a)).\nHowever, it should be noted that in the first few waves of our study we do not seek\nto identify a fully sufficient set of outputs. We only use a small number of outputs\nthat a) are straightforward to emulate and b) are informative enough to allow us to cut\nout large portions of the input space (see section 4.2). Once the input space has been\nreduced, outputs become easier to emulate accurately for reasons discussed in section\n3 and demonstrated in section 7: essentially it is because we are \u2018zooming in\u2019 on a\nlocally smooth function. We can then emulate a larger set of outputs to further reduce\nthe input space. This iterative strategy is a major strength of our approach and results\ndirectly from our aim of removing implausible (or \u2018bad\u2019) points as opposed to identifying\n\u2018good\u2019 points. At the end of our analysis we performed a large set of Wave 5 runs to\ncheck if the runs that match our 11 outputs also match the other outputs that were not\nconsidered. These runs are shown in figures 12 and 13 (bottom right panel) and confirm\ngood matches across all outputs of interest, suggesting that the final set of 11 outputs\nwas indeed sufficient to an acceptable degree.\nRanjan proposes an alternative approach that involves emulating and minimising a\nfunction g(x): a global measure that depends on all outputs, which would equal zero\nif there were an exact match to the data. While this approach may work in some\napplications, it does have the following disadvantages compared to our method.\nI. Vernon, M. Goldstein and R. G. Bower 699\nActive Inputs: In our approach we initially emulate individual outputs that mainly\ndepend only on a small set of active inputs (e.g. in Wave 1 each output was emulated\nusing sets containing only 5 active inputs) thus greatly simplifying the emulation and\nanalysis. This benefit has been exploited in several applications (see Craig et al. (1997);\nCumming and Goldstein (2009a,b)) where the union of active inputs may be large, but\nthe number used for any particular output is small. In contrast, despite g(x) giving\nscalar output, it will usually depend on all inputs to the function f(x) and hence will\nbe a very complex, high dimensional function. It is usually far easier to emulate a few\nlow dimensional functions than one high dimensional one.\nCapturing Physical Dependencies: Emulating individual outputs has a further\nsignificant advantage. As discussed in section 3.4, we prefer to build more structure into\nthe mean function or regression part of the emulator for several reasons. Perhaps the\nmost important is that the individual outputs of many physical models, and of Galform\nin particular, exhibit strong and physically interpretable monotonicities with respect to\nthe inputs, which are naturally expressed through the mean function, resulting in more\naccurate emulation. The g(x) function would have no such monotonicities, being most\nlikely a complex surface with many local minima, and such emulation advantages would\nbe lost.\nPhysical Interpretation: There is also the question of the physical interpretation\nof the emulators that our approach allows: the results of emulation of individual outputs\ncan be checked against expert knowledge and can often help the scientist further under-\nstand the behaviour of the model. This again would be lost if we were to analyse only\ng(x). Our method also allows a more nuanced approach to assessing model adequacy,\nwhich we take full advantage of, as we discuss in the response to van Dyk.\nRanjan then suggests recasting our implausibility approach in terms of an expected\nimprovement (EI) criterion. We do fear that the form of an EI algorithm may miss\nsome of the more appealing aspects of our approach, namely that we do not attempt\nto emulate accurately over the whole input space, only to emulate certain outputs\nsufficiently accurately to be able to discard large regions of input space. Also, we never\nexpress criteria for acceptable inputs (until the Wave 5 runs at the end of the analysis)\nand only work in terms of non-implausible (i.e. not currently ruled out) inputs. It is\nunclear if one could devise an EI criterion that incorporates these attributes.\nAt each wave the current non-implausible volume of input space was checked for\nconnectedness. Ranjan rightly asks whether each of these volumes are also convex in\nstructure. This is an interesting point and we intend to investigate this possibility in\nfuture work, as our history matching method could easily produce highly non-convex\nregions of input space after each wave for certain applications. As concerns Galform,\nthe majority of projections of the non-implausible regions into 2, 3 and 4 dimensional\nsubspaces suggested a convex (or almost convex) shape. The major part of each of our\nemulators is based on polynomials that can be fitted over non-convex regions (although\none has to be careful of the usual traps of fitting over a non-orthogonal design). The\nGaussian process part of our emulators have shorter correlation length parameters (as\nthe regression terms take up the global behaviour), and hence we only need to worry\n700 Rejoinder\nabout more localised non-convexity, compared to say using a full Gaussian process emu-\nlator with simple mean function. These considerations combined with healthy amounts\nof diagnostics to test each of the emulator\u2019s performances (200 diagnostic runs were\ndone at each wave), would suggest that non-convexity is not a significant problem here.\nRanjan\u2019s suggested solution, at each wave to use all previous runs from all previous\nwaves to construct an emulator over the whole original input volume, does address the\nnon-convex problem, but for an unacceptable cost. This loses one of the fundamental\nmotivations for our approach which is that it is generally easier to emulate the function\nover smaller volumes. This is discussed in section 3 and demonstrated in section 7 and\nis mainly due to the polynomial terms becoming better approximations to the smooth\nfunction f(x), and the higher density of points allowing the Gaussian process terms to\nbecome more accurate.\nAs to the query raised concerning the practical methodology for constructing the\nindividual emulators, we strongly believe that use of both active variables and a de-\ntailed mean function is advantageous in the majority of computer model applications.\nHowever, there is flexibility over the choice of techniques used. As we had a reasonably\nlarge number of Galform runs, it was felt that traditional model selection techniques\nwere adequate for our purposes. If we had a substantially smaller set of runs then\nwe would have to employ a more careful and formal Bayesian style approach for such\nselection.\nIn response to the question about the principles behind our design choices, in order\nto construct, for example, the Wave 2 design, we decided that computational resources\nwould allow the evaluation of approximately 1400 to 1450 runs. We used a latin hy-\npercube of size 9500 and rejected all the points that did not satisfy our implausibility\ncutoffs. We chose this size of design as we knew (from experimenting with our emula-\ntors and implausibility measures to determine the approximate volume remaining after\nwave 1) that the number of runs that survive should lie in the range 1400 to 1450.\nSimilar rules were used for subsequent waves.\nThis leads us to the total number of runs used in this analysis. As this was an\napplication at the cutting edge of investigations into Galaxy formation, we felt it wise\nto use the substantial computer time available to evaluate as many runs as was feasible\n(1000+) for each wave. That is, the relatively large number of runs used is a reflection\nof the computer resources available and the importance of the project. It is clear that\nwe could have performed this analysis with fewer runs, but it was thought best to err on\nthe side of caution. Exactly how many evaluations would have been required to achieve\nthe same goal is an interesting question, and leads to the obvious design issue of how\nto proportion a fixed quota of runs between various future waves. We intend to look at\nthis important topic in future work.\n3 Response to Earl Lawrence and David Higdon\nLawrence and Higdon (LH) requested our Wave 2 data, which was composed of 1414 runs\nthat were restricted to the non-implausible region defined by the Wave 1 implausibility\nI. Vernon, M. Goldstein and R. G. Bower 701\ncutoffs (given by equation (22)). We are pleased that they were able to apply their\nmethodology as described in Higdon et al. (2008) to this data set, and that they have\nproduced some interesting, if provisional, results in time for this discussion.\nIt seems that the fully Bayesian approach is more \u201caggressive\u201d in reducing input\nspace than the Bayes linear version, but it would at this stage appear to be too ag-\ngressive. Comparison with figure 11 which shows a marginal plot of the set of Wave 5\nruns, coloured by implausibility, shows several acceptable (green) runs that are clearly\noutside of the hpd region given by LH\u2019s analysis (see for example the Vhotdisk : \u03b1hot,\nthe \u03b1cool : \u03b1hot, the \u000f\u22121? : pyield and the Vhotdisk : \u03b1reheat projections). That is to say,\nsome of the acceptable runs which are of interest to the cosmologists (the corresponding\nluminosity functions of which are shown in the bottom-right panel of figures 12 and 13)\ncould be excluded by this analysis.\nLawrence and Higdon urge caution in making direct comparisons between the two\napproaches based on the marginal posterior densities given in figure 1 of their discussion\npaper, as the two cases are not equivalent. For example a major difference is that the\nposterior as calculated by LH used the model discrepancy \u03a6E conditioned on a fixed\nvalue of the parameters a, b and c (see equation (20)). The implausibility approach\nexplored the full ranges of the parameters a, b and c as specified by the expert and\ngiven by equation (21), by only discarding an input x as implausible if it failed the\nimplausibility cutoffs for all values of a, b and c. Incorporating the uncertainty on the\nparameters a, b and c into LH\u2019s approach could possibly lead to a more diffuse posterior\nand make a comparison between the approaches easier (and may go someway toward\nsolving the Wave 5 run problem outlined above). As LH state, the differences in the\nemulation approaches used will also muddy this comparison.\nA general problem is that is it not at all obvious that the posterior shown in LH\u2019s\nfigure 1 (which was generated using the Wave 2 runs) still respects the Wave 1 con-\nstraints that make the analysis meaningful. That is to say, are there parts of LH\u2019s\nposterior that give significant probability to inputs that were previously ruled out by\nthe Wave 1 constraint? LH\u2019s assessment can only be based on an extrapolation from\na subspace covered by the wave 2 runs whereas the wave 1 elimination was based on\nfunction evaluations more local to that region. There is no guarantee that this issue is\navoided simply because the 2-dimension marginals of LH\u2019s figure 1 are seen to be within\nthe 2-dimensional projected non-implausible region of figure 10, say.\nLH then go on to describe their impressive work on the Coyote Universe (Heitmann\net al. (2009)), where, as they state, the main computational effort went into finding\nsuitably smooth representations of the power law (see LH\u2019s figure 4), while retaining\nthe vital physical features (known as the baryonic acoustic oscillations), important for\nunderstanding structure formation. They use PCA to capture the behaviour of all\noutputs of the computer model. PCA is widely used in the computer model literature,\nand has many obvious benefits. However, it has some disadvantages too. Often the most\nimportant principal components will depend on all of the active inputs to the computer\nmodel and hence emulating them may be difficult. This should be compared with the\nPrincipal Variables approach discussed in the response to Ranjan above, where each\n702 Rejoinder\nprincipal variable may only depend on a small subset of the active inputs. Principal\nvariables have similar properties to principal components in their ability to largely\nreconstruct the entire output set.\nAlthough our approach differs from LH\u2019s due to the use of Bayes linear methods\nas opposed to a more fully Bayesian treatment, another fundamental difference is that\nbetween history matching and calibration. LH are performing calibration and hence\nassume there exists a single input (the \u201cbest input\u201d x+, as defined in section 3), and\nsubsequently try to calculate the posterior distribution for the single point x+. In the\nhistory matching approach we ask the more general question: which inputs x are not\nobviously inconsistent with the notion of x+? It is not surprising that these approaches\ngive different but related answers. In particular the statement that there is a unique\nbest input may often sharpen the credible intervals, as compared to a Bayes linear or\neven fully Bayesian history match. We discuss this difference in more detail in the\nresponse to van Dyk below.\nWe highlight the above differences between their approach and that of our own to\nemphasize the many subtleties involved in such computer model analysis. We would\nrecommend to anyone attempting a fully Bayesian calibration of such a complex com-\nputer model, to consider preceding it by a history match. (Our understanding is that\nthis is why LH asked for the wave 2 runs rather than those of wave 1). A history match\nshould be performed to identify if there are any acceptable matches and their location in\ninput space (which is often of great interest to the modeller), and then a fully Bayesian\ncalibration should be performed only over the restricted input region defined by the\nhistory match. Performing an MCMC algorithm whilst respecting various complex con-\nstraints as provided by the history match may present some interesting challenges: see\nour concerns about this above and in the response to van Dyk.\nThis combined process is equivalent to cutting out the often large regions of input\nspace that would have extremely low posterior probability, before proceeding with the\nBayesian analysis, and should result in a highly accurate approximation to the posterior\ndistribution. We intend to explore this, powerful strategy, in future work.\n4 Response to David van Dyk\nWe thank David van Dyk for his comments, and for the opportunity to expand further on\nsome aspects of our work. His comments raise interesting questions about the meaning,\nthe potential and the limitations of Bayesian investigations within fundamental science.\nWe deal with his points in turn.\nThe first point raised is a query about quality of fit in figure 14. This figure (which\nshows new types of outputs not considered in this work) is included purely to show the\nnext stage of the matching process. As we have now identified the region of input space\nconsistent with both the bj and K luminosity function observed data, we are now free\nto move around this region, exploring the effect on the new outputs shown in figure\n14. In this way, we view history matching as an ongoing process, notably simpler than\nattempting to incorporate all data constraints simultaneously.\nI. Vernon, M. Goldstein and R. G. Bower 703\nVan Dyk proceeds to give a description of history matching in terms of \u201cstandard\nstatistical methodology\u201d using the log-likelihood L(\u03b8|Y ). While there are similarities\nbetween this description and our methods, van Dyk seriously oversimplifies some crucial\nfeatures.\n\u03b8 does not exist: Most importantly, we do not assume that there exists a single\n\u201ctrue\u201d input \u03b8 (or best input x+), as even though the inputs to the Galform model\nare related to real physical quantities, they are not themselves physical. Van Dyk\u2019s\nsummary assumes that \u03b8 exists as a true but unknown quantity. However, the actual\nsituation is that \u03b8 is largely a model construct and that as the Galform model evolves\nover future generations, the interpretation of the various elements of \u03b8 will change, with\nsome even ceasing to exist. As mentioned in the response to Lawrence and Higdon, not\nassuming the existence of \u03b8 changes the questions that one might ask. We ask only if\nthere are any values of the inputs x that are not inconsistent with the concept of such a\nbest input \u03b8. We feel that it is essential to establish the answer to this question, before\nconsidering whether a calibration is appropriate.\nImplausibility Measure is not a log-likelihood: Even ignoring this issue, the\nlog-likelihood that van Dyk writes down is not comparable to the problem we analyse as\nhe has ignored the \u03b8 dependance of \u03c3i, which comes from the use of emulators and is of\ngreat importance in this context. That is to say, a more appropriate comparison would\nbe if Yi \u223c N(\u00b5i(\u03b8), \u03c32i (\u03b8)), where \u00b5i(\u03b8) and \u03c3i(\u03b8) both depend on \u03b8. In this case the log-\nlikelihood is now, ignoring constants, L(\u03b8|Y ) = \u2212 12 log |\u03a3(\u03b8)|\u2212\n\u2211n\ni=1(Yi\u2212\u00b5i(\u03b8))2\/2\u03c32i (\u03b8),\nwhere in this simple illustration the matrix \u03a3(\u03b8) = diag \u03c321 , .., \u03c3\n2\nn. Thus L(\u03b8|Y ) contains\nan additional \u2212 12 log |\u03a3(\u03b8)| term which does not feature in our implausibility measures,\nas this term comes directly from the full distributional assumption of normality, which\nwe do not make.\nIn summary our approach is both mathematically distinct and different in funda-\nmental interpretation from the interpretation van Dyk suggests, and from most of the\napproaches used in the literature. As discussed below, much of the computer model\nterminology is used to highlight these essential differences.\nIn the section entitled \u201cEmploying the Common Statistical Framework for\nComputer Modelling\u201d, it is remarked that \u201c(the authors) aim to find the values of\nthe parameter that result in the best fit to the data\u201d. We in fact aim for the opposite: to\nfind the input parameters x that are clearly not good fits to the considered data, given\ncurrent knowledge of the computer simulator f(x). We then discard these inputs, leaving\na hopefully non-empty set of input parameters that are deemed non-implausible. As we\nperform more runs of the computer model, or bring into consideration more outputs,\nthis set will decrease in size. At no point are we trying to find the \u201cbest\u201d set of input\nparameters, just those that give acceptable matches.\nThis essential difference between discarding \u2018bad\u2019 inputs and searching for \u2018good\u2019\ninputs is critical, coming directly from the lack of a best input assumption, and is the\nreason for the power of our sequential approach. It is far easier to emulate a small\nset of outputs of the function in order to determine that large parts of the input space\nare implausible (and continue this process iteratively), than it is to attempt to identify\n704 Rejoinder\nthe \u201cbest fits\u201d which would require emulation of all outputs, use of all observed data\nand careful modelling of every part of the problem. Even if we are searching for good\nmatches, it is a sensible way to simplify massively the calculations by first eliminating\nbad matches.\nOften the different terminology in use in the computer model literature has arisen due\nto the subtly different problems presented in this area. For example, history match-\ning, the process of iteratively discarding implausible inputs as outlined above, is not\nequivalent to \u2018model checking\u2019 which refers to confirming all aspect of the stochastic\nformulation. History matching is a process applied to the computer model (i.e. the\nfunction f(x)) itself. The term \u2018calibration\u2019 is a statistical term widely used in inverse\nregression problems, of which the calibration of a computer model through use of an\nemulator can be viewed as a direct generalisation.\nAs discussed above, an implausibility measure is not the same as the likelihood. The\nformer is only informative regarding unacceptable (i.e. implausible) inputs and says\nnothing about possible good inputs (a low implausible value I(x) should be interpreted\nas \u201cx is not ruled out yet\u201d): it is deliberately not normed. The likelihood comes from\ncarefully modelling all aspects of the data (a sometimes arduous task in computer model\nproblems), and is informative regarding both \u2018good\u2019 and \u2018bad\u2019 inputs. The likelihood\nin computer model applications is unfortunately often non-robust. We are somewhat\nsurprised as to van Dyk\u2019s comments regarding the use of the term \u201cinputs\u201d. We use\nthe term in the mathematical sense to refer to the inputs x to the Galform function\nf(x), and initially introduce them in the earlier sections as \u201cinput parameters\u201d to avoid\nconfusion (see for example the abstract, section 1, section 2 and specifically table 1).\nThus they are distinguished from the other parameters in the problem e.g. those used\nin fitting the emulators: \u03b2ij , \u03c3i, \u03c9i and \u03b8i.\nThis is not just a case of different terminology, but rather a case of labelling the\nspecific types of problems faced in a computer model analysis that are often quite\ndifferent from those encountered in other statistical applications. Indeed, it is possible\nto go further and remark that (as van Dyk does point out), statisticians can learn from\nsome of the ideas presented in the computer model literature. Specifically, the reasoning\nleading to the clear and upfront acknowledgement that the computer model is not a\nperfect representation of reality could also be applied to any statistical model too (as is\ndiscussed in Goldstein (2010)). As van Dyk remarks, such computer models are often\nembedded within large statistical models to enable analysis of complex systems. Care\nmust be taken in such situations not to oversimplify: the danger is that the computer\nmodel is treated as reality, and no model discrepancy is used.\nVan Dyk next argues that the multivariate implausibility measure I(x) given by\nequation (16) should be considered superior to IM (x) (and implicitly I2M (x) and I3M (x))\nwhich are the first, second and third highest univariate implausibilities corresponding to\neach individual output, respectively, and given by equations (13), (14) and (15). Again,\nas we are not attempting to identify likely values of the parameters, we would argue that\nthe choice between univariate and multivariate measures is highly problem dependent,\nand that in many cases one can pay too high a price for using the multivariate measure\nI. Vernon, M. Goldstein and R. G. Bower 705\nI(x) too early. This measure is very sensitive to possible failings of the emulators (it\ngenerally requires the construction of an accurate multivariate emulator, which is often\ndifficult). It also requires a full multivariate model discrepancy specification which can\nbe both hard to elicit and to document (see for example equations (20) and (21) and the\naccompanying discussions). For these reasons, we recommend using the conceptually\nsimpler, easier to elicit, and more robust measures I2M (x) and I3M (x) to reduce the\ninput parameter space in the initial waves. Then, as we have done in the current work,\nI(x) can be brought in for use at a later wave, when emulation is easier. It would of\ncourse be interesting, if possible, to track the changes in I(x) as one progresses from\nwave 1 onward.\nIn the section \u201cWhat does it mean to be a Bayesian\u201d van Dyk gives some\ninteresting views about the Bayesian paradigm, many of which we agree with. Having\nsaid this, the statement \u201cthe Bayes Linear approach is based on Bayes Theorem\u201d is\nmisleading: as is discussed at the end of section 3.3, the Bayes Linear approach is a\ngeneralisation of Bayes Theorem, based on taking expectation rather than probability\nas the natural primitive for the subjectivist theory (see De Finetti (1974) and Goldstein\n(2006) for more details). We would agree that one of the major benefits of a Bayesian\nanalysis is that \u201cit is a principled analysis that fully accounts for the complexities of the\nunderlying distributions and avoids the old and often unrealistic Gaussian assumptions\u201d,\nbut only provided serious effort is put into capturing the beliefs of the expert, and\ninto creating a realistic statistical model, so that the underlying distributions used\nhave actual physical meaning, as opposed to being based on arbitrary assumptions or\nmathematical simplicity. Our perspective is that often this level of detailed elicitation\nis infeasible, in which case we would rather make a simpler specification based on means\nand variances and proceed with a tractable Bayes Linear analysis, than make arbitrary\nassumptions as to the forms of several distributions and try to proceed with an often\ndifficult and (in the case of computer models) highly non-robust Bayesian analysis.\nWe now turn to the discussion of expert judgement within our analysis. First it\nshould be noted that the judgements asserting that the quantities in equations (1)\nand (2) are uncorrelated actually represented the assessments made by the expert af-\nter extensive discussion and consideration. We also note that similar assumptions are\nstandard throughout the field of computer models and used in countless papers (e.g\nKennedy and O\u2019Hagan (2001)), often with a much stronger assumption of full indepen-\ndence. It is possible to consider the structural beliefs about model discrepancy even\nmore carefully (as is discussed in Goldstein and Rougier (2009)), but we considered the\ncurrent assessment adequate for the purposes of this study.\nThe most significant area of our analysis which involves expert judgement is that of\nthe model discrepancy \u000fmd which represents the difference between the model output and\nreality itself. This was broken down into three contributions, two of which were assessed\nusing further computer model runs, while the third component \u03a6E came directly from\nRichard Bower\u2019s expert judgements. In a Bayesian analysis of any form, it is impossible\nto address the issue of model discrepancy meaningfully without using expert judgements\nof this kind. Crucially, in this project we had the benefit of prolonged contact with the\nexpert resulting from regular monthly meetings for over two years. The judgements\n706 Rejoinder\nthemselves were formed from many considerations of the possible deficiencies of the\nmodel (missing physics, approximate models of certain processes, inaccurate dark matter\nsimulations) only some of which we were able to discuss in the paper due to length\nrestrictions. The judgements were made by an expert fully versed in the meaning and\nimpact of \u03a6E . In this work, the model discrepancy should be understood in terms of the\nexpert\u2019s tolerance for what would be classified as an acceptable run, and therefore the\nmodel discrepancy is no longer an uncertain quantity to be estimated using standard\nstatistical methods.\nThe assessment as to whether the Galform model is an adequate model of Galaxy\nFormation can only come from expert judgement. Van Dyk is clearly concerned that\n\u201cthe expert assessments determine the final outcome\u201d. This misunderstanding arises\ndue to the belief that the Galform model can be deemed acceptable by some abstract\nnotion of \u2018right\u2019 or \u2018wrong\u2019. This is an assertion that is disconnected from the way in\nwhich scientists view models of this complexity. Scientists are always concerned both\nwith incremental and paradigmatic improvements to their model. If, for example, we\nfound no acceptable runs, we would increase the tolerance as represented by the model\ndiscrepancy, to determine how large it would have to be to obtain some \u2018acceptable\u2019\nruns. This would be informative for the scientists as it would give some measure of how\ninadequate the model is, and show the impact of the missing (or incorrect) physics in\nthe current model formulation. Such measures are crucial in helping scientists to assess\nwhether incremental modifications will be adequate to deal with the observed discrep-\nancy between the model output and physical observations. Note that, our approach\nexplicitly incorporates a sensitivity analysis as relates to the expert assessment of \u03a6E .\nAs can be seen in equations (20) and (21) the Var(\u03a6E) is parameterised by three pa-\nrameters a, b and c that the expert was unwilling to assign specific values. Therefore we\nexplored the impact on our implausibility measures of varying the parameters a, b and\nc over ranges agreed by the expert and given by equation (21). We only discarded an\ninput x as implausible if it failed the implausibility cutoffs for every value of a, b and c in\nthe given ranges. We also performed sensitivity analysis on the choice of implausibility\ncutoffs imposed using various sets of diagnostic runs. A benefit of our approach is that\nsuch sensitivity analysis can be relatively straightforward.\nVan Dyk states \u201cin my view even a Bayesian analysis must work hard to minimise\nits assumptions and must be absolutely upfront about the impact of its subjective as-\nsessments on the final analysis.\u201d We could not agree with this statement more, provided\nwe are clear about the distinction between assumptions which are by necessity some-\nwhat arbitrary, and subjective assessments, which reflect the careful and knowledgeable\nassertions of experts. Rather than making several unjustifiable distributional assump-\ntions (which are necessarily somewhat arbitrary and correspond to an infinite number of\nprobabilistic assertions) whose impact on the posterior is in many cases extremely hard\nto assess, there often are advantages in using a Bayes linear style approach involving\nonly a relatively small number of first and second order quantities, where subjective\nassessments on quantities such as the model discrepancy enter clearly, and the effects\nof which are easy to demonstrate. Having said all this, we would be interested to see\na full elicitation as applied to Galform, followed by a Bayesian inference in which each\nI. Vernon, M. Goldstein and R. G. Bower 707\ndistributional statement is robustly defended.\nIn the section entitled \u201cIf it looks like a Duck...\u201d the implausibility measures are\nagain equated to the Gaussian log-likelihood, which as discussed above, is an inappro-\npriate comparison. For example, if we had judged Gaussian distributional assumptions\nto be appropriate, we would have used much tighter credible volumes and cutoffs as we\nwould not have had to appeal to Pukelsheim\u2019s 3\u03c3 rule (the very powerful result that\nfor any unimodal, continuous distribution more than 95% of its probability lies within\n\u00b13\u03c3) and instead used a more familiar 2\u03c3 rule (as incidentally it appears LH have done).\nSuch differences are always present when comparing a Bayes Linear approach to a full\nGaussian specification in any example.\nVan Dyk then asks directly about the differences between the two approaches in\nthis application: they are substantial as we now describe. A fully Gaussian Bayesian\nanalysis would attempt to construct a likelihood based on all available data points\nand an assertion of a unique true value for \u03b8. It would further depend on constructing\nmultivariate emulators to represent jointly every single output of the computer model for\nwhich there is data, even if some of these outputs are very difficult to emulate accurately\nover the whole input space. The modelling assumptions and emulator construction that\nwould go into building this multimodal likelihood would need to be highly accurate,\notherwise it would result in any inference being extremely non-robust. The approach\nwe describe in this work avoids both the assertion of the unique true value of the\nparameters and much of the difficulty in the modelling and analysis. We deal with\nimplausibility measures that are far more robust being functions of small numbers of\noutputs (outputs which are chosen for their ease of emulation). At each wave we do not\nneed to model any highly multimodal likelihood: all we need to do is draw a conservative\ncontour around the low implausibility input points. As demonstrated in section 7, at\neach wave the emulators improve in accuracy and we can consider more output data\nconstraints when necessary. Note that as we introduce the constraints from the data\nsequentially, even a large number of constraints is often straightforward to incorporate,\nunlike for an MCMC algorithm for which it may be problematic.\nIn \u201cThe scientific objective\u201d van Dyk asks some questions about the goal of\nthis analysis. The appropriateness of the analysis depends upon the context of the\nscientific question. If we are looking at well defined physical quantities (location, age,\nmetal content) then often estimating these quantities would be the appropriate choice\nof analysis. However the Galform input parameters are more abstract, and only make\nsense in terms of the model and its applicability. In this case we are trying to assess\nwhether Galform can shed light onto the physical world, or at least contribute to that\nlarge question. An approach where we perform a Bayesian calibration for the best\ninput x+, may become appropriate when the model has been shown to be sufficiently\naccurate for all intended purposes. Provided that the history match has revealed the\nmodel does appear to be sufficiently accurate, the discussant\u2019s suggestions in the section\n\u201cThe Final Analysis\u201d may be helpful for achieving such a Bayesian calibration. As a\ngeneral principal, carrying out a history match as a first stage in the Bayesian calibration\nexercise will usually be very useful in greatly restricting the volume of parameter space\nthat needs to be explored by the MCMC algorithm.\n708 Rejoinder\nReferences\nCraig, P. S., Goldstein, M., Seheult, A. H., and Smith, J. A. (1997). \u201cPressure matching\nfor hydrocarbon reservoirs: a case study in the use of Bayes linear strategies for large\ncomputer experiments.\u201d In Gatsonis, C., Hodges, J. S., Kass, R. E., McCulloch,\nR., Rossi, P., and Singpurwalla, N. D. (eds.), Case Studies in Bayesian Statistics,\nvolume 3, 36\u201393. New York: Springer-Verlag. 699\nCumming, J. A. and Goldstein, M. (2009a). \u201cBayes linear uncertainty analysis for oil\nreservoirs based on multiscale computer experiments.\u201d In O\u2019Hagan, A. and West, M.\n(eds.), Handbook of Bayesian Analysis. Oxford, UK: Oxford University Press. 698,\n699\n\u2014 (2009b). \u201cSmall Sample Bayesian Designs for Complex High-Dimensional Models\nBased on Information Gained Using Fast Approximations.\u201d Technometrics, 51(4):\n366\u2013376. 699\nCumming, J. A. and Wooff, D. A. (2007). \u201cDimension reduction via principal variables.\u201d\nComputational Statistics & Data Analysis, 52(3): 550\u2013565. 698\nDe Finetti, B. (1974). Theory of Probability , volume 1. London: Wiley. 705\nGoldstein, M. (2006). \u201cSubjective Bayesian Analysis: Principles and Practice.\u201d Bayesian\nAnalysis, 1(3): 403\u2013420. 705\n\u2014 (2010). \u201cExternal Bayesian analysis for computer simulators.\u201d In Bernardo, J. M.,\nBayarri, M. J., Berger, J. O., Dawid, A. P., Heckerman, D., Smith, A. F. M., and\nWest, M. (eds.), To appear in Bayesian Statistics 9. Oxford University Press. 704\nGoldstein, M. and Rougier, J. C. (2009). \u201cReified Bayesian modelling and inference for\nphysical systems (with Discussion).\u201d Journal of Statistical Planning and Inference,\n139(3): 1221\u20131239. 705\nGoldstein, M. and Wooff, D. A. (2007). Bayes Linear Statistics: Theory and Methods.\nChichester: Wiley. 697\nHeitmann, K., Higdon, D., et al. (2009). \u201cThe Coyote Universe II: Cosmological Models\nand Precision Emulation of the Nonlinear Matter Power Spectrum.\u201d Astrophys. J.,\n705(1): 156\u2013174. 701\nHigdon, D., Gattiker, J., Williams, B., and Rightley, M. (2008). \u201cComputer Model\nCalibration Using High-Dimensional Output.\u201d Journal of the American Statistical\nAssociation, 103(482): 570\u2013583. 701\nKennedy, M. C. and O\u2019Hagan, A. (2001). \u201cBayesian calibration of computer models.\u201d\nJournal of the Royal Statistical Society, Series B, 63(3): 425\u2013464. 705\n"}