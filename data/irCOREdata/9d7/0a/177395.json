{"doi":"10.1007\/978-3-642-12805-9","coreId":"177395","oai":"oai:aura.abdn.ac.uk:2164\/2204","identifiers":["oai:aura.abdn.ac.uk:2164\/2204","10.1007\/978-3-642-12805-9"],"title":"Arguing Using Opponent Models","authors":["Oren, Nir","Norman, Timothy J."],"enrichments":{"references":[{"id":4852,"title":"An argumentation inspired heuristic for resolving normative con\ufb02ict.","authors":[],"date":"2008","doi":null,"raw":"N. Oren, M. Luck, S. Miles, and T. J. Norman. An argumentation inspired heuristic for resolving normative con\ufb02ict. In Proceedings of The Fifth Workshop on Coordination, Organizations, Institutions, and Norms in Agent Systems (COIN@AAMAS-08), pages 41\u201356, Estoril, Portugal, 2008.","cites":null},{"id":4854,"title":"Arguing with con\ufb01dential information.","authors":[],"date":"2006","doi":"10.1016\/j.artint.2007.04.006","raw":"N. Oren, T. J. Norman, and A. Preece. Arguing with con\ufb01dential information. In Proceedings of the 18th European Conference on Arti\ufb01cial Intelligence, pages 280\u2013284, Riva del Garda, Italy, August 2006.","cites":null},{"id":4858,"title":"Computational Logic: Logic Programming and Beyond. Essays In Honour of","authors":[],"date":"2002","doi":"10.1007\/3-540-45632-5_14","raw":"H. Prakken and G. Sartor. Computational Logic: Logic Programming and Beyond. Essays In Honour of Robert A. Kowalski, Part II, volume 2048 of LNCS, pages 342\u2013380. Springer-Verlag, 2002.","cites":null},{"id":4851,"title":"Dialogue game theory for intelligent tutoring systems.","authors":[],"date":"1993","doi":null,"raw":"D. Moore. Dialogue game theory for intelligent tutoring systems. PhD thesis, Leeds Metropolitan University, 1993.","cites":null},{"id":4850,"title":"Dialogue games in multi-agent systems.","authors":[],"date":"2002","doi":"10.1007\/s10458-009-9108-7","raw":"P. McBurney and S. Parsons. Dialogue games in multi-agent systems. Informal Logic, 22(3):257\u2013274, 2002.","cites":null},{"id":4844,"title":"editors. Computational Models of Argument:","authors":[],"date":"2008","doi":null,"raw":"P. Besnard, S. Doutre, and A. Hunter, editors. Computational Models of Argument: Proceedings of COMMA 2008, Toulouse, France, May 28-30, 2008, volume 172 of Frontiers in Arti\ufb01cial Intelligence and Applications. IOS Press, 2008.","cites":null},{"id":4843,"title":"Generation and evaluation of di\ufb00erent types of arguments in negotiation.","authors":[],"date":"2004","doi":null,"raw":"L. Amgoud and H. Prade. Generation and evaluation of di\ufb00erent types of arguments in negotiation. In Proceedings of the 10th International Workshop on Non-monotonic Reasoning, 2004.","cites":null},{"id":4860,"title":"Heuristics in argumentation: A game theory investigation.","authors":[],"date":null,"doi":"10.2139\/ssrn.1317349","raw":"R. Riveret, H. Prakken, A. Rotolo, and G. Sartor. Heuristics in argumentation: A game theory investigation. In Besnard et al. [4], pages 324\u2013335.","cites":null},{"id":4845,"title":"Incorporating opponent models into adversary search. In","authors":[],"date":"1996","doi":null,"raw":"D. Carmel and S. Markovitch. Incorporating opponent models into adversary search. In In Proceedings of the Thirteenth National Conference on Arti\ufb01cial Intelligence, pages 120\u2013125. AAAI, 1996.","cites":null},{"id":4855,"title":"Loose lips sink ships: a heuristic for argumentation.","authors":[],"date":"2006","doi":null,"raw":"N. Oren, T. J. Norman, and A. Preece. Loose lips sink ships: a heuristic for argumentation. In Proceedings of the Third International Workshop on Argumentation in Multi-Agent Systems, pages 121\u2013134, Hakodate, Japan, May 2006.16. S. Parsons, P. McBurney, E. Sklar, and M. Wooldridge. On the relevance of utterances in formal inter-agent dialogues. In AAMAS \u201907: Proceedings of the 6th international joint conference on Autonomous agents and multiagent systems, pages 1\u20138, New York, NY, USA, 2007. ACM.","cites":null},{"id":4859,"title":"Mechanism design for abstract argumentation.","authors":[],"date":"2008","doi":"10.1007\/978-0-387-98197-0_16","raw":"I. Rahwan and K. Larson. Mechanism design for abstract argumentation. In Proceedings of AAMAS 2008, 2008.","cites":null},{"id":4846,"title":"Model-based learning of interaction strategies in multi-agent systems.","authors":[],"date":"1998","doi":"10.1080\/095281398146789","raw":"D. Carmel and S. Markovitch. Model-based learning of interaction strategies in multi-agent systems. Journal of Experimental and Theoretical Arti\ufb01cial Intelligence, 10(3):309\u2013332, 1998.","cites":null},{"id":4848,"title":"On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games.","authors":[],"date":"1995","doi":"10.1016\/0004-3702(94)00041-X","raw":"P. M. Dung. On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games. Arti\ufb01cial Intelligence, 77(2):321\u2013357, 1995.","cites":null},{"id":4841,"title":"On the bipolarity in argumentation frameworks.","authors":[],"date":"2004","doi":"10.1002\/int.20307","raw":"L. Amgoud, C. Cayrol, and M.-C. Lagasquie-Schiex. On the bipolarity in argumentation frameworks. In Proceedings of the 10th International Workshop on Non-monotonic Reasoning, pages 1\u20139, Whistler, Canada, 2004.","cites":null},{"id":4856,"title":"On the relevance of utterances in formal inter-agent dialogues.","authors":[],"date":"2007","doi":"10.1145\/1329125.1329416","raw":null,"cites":null},{"id":4847,"title":"Probabilistic opponent-model search. Information Sciences,","authors":[],"date":"2001","doi":"10.1016\/j.tcs.2005.09.049","raw":"H. H. L. M. Donkers, J. W. H. M. Uiterwijk, and H. J. van den Herik. Probabilistic opponent-model search. Information Sciences, 135(3-4):123\u2013149, 2001.","cites":null},{"id":4862,"title":"Programming a computer for playing chess.","authors":[],"date":"1950","doi":"10.1038\/scientificamerican0250-48","raw":"C. E. Shannon. Programming a computer for playing chess. Philosophical Magazine, 41:256\u2013275, 1950.","cites":null},{"id":4849,"title":"Progressive defeat paths in abstract argumentation frameworks.","authors":[],"date":"2006","doi":"10.1007\/11766247_21","raw":"D. C. Mart\u00b4 \u0131nez, A. J. Garc\u00b4 \u0131a, and G. R. Simari. Progressive defeat paths in abstract argumentation frameworks. In L. Lamontagne and M. Marchand, editors, Canadian Conference on AI, volume 4013 of Lecture Notes in Computer Science, pages 242\u2013253. Springer, 2006.","cites":null},{"id":4857,"title":"Relating protocols for dynamic dispute with logics for defeasible argumentation.","authors":[],"date":"2001","doi":"10.1007\/978-94-017-0456-4_3","raw":"H. Prakken. Relating protocols for dynamic dispute with logics for defeasible argumentation. Synthese, 127:187\u2013219, 2001.","cites":null},{"id":4853,"title":"Semantics for evidence-based argumentation.","authors":[],"date":null,"doi":"10.1007\/978-3-540-75526-5_10","raw":"N. Oren and T. J. Norman. Semantics for evidence-based argumentation. In Besnard et al. [4], pages 276\u2013284.","cites":null},{"id":4842,"title":"Strategical considerations for argumentative agents (preliminary report).","authors":[],"date":"2002","doi":null,"raw":"L. Amgoud and N. Maudet. Strategical considerations for argumentative agents (preliminary report). In Proceedings of the 9th International Workshop on Nonmonotonic Reasoning, pages 399\u2013407, 2002.","cites":null},{"id":4861,"title":"Success chances in argument games:a probabilistic approach to legal disputes.","authors":[],"date":"2007","doi":"10.2139\/ssrn.1100672","raw":"R. Riveret, N. Rotolo, S. G, H. Prakken, and B. Roth. Success chances in argument games:a probabilistic approach to legal disputes. In Proceedings of the 20th Anniversary International Conference on Legal Knowledge and Information Systems (Jurix 2007), pages 99\u2013108, Amsterdam, The Netherlands, 2007.","cites":null}],"documentType":{"type":1}},"contributors":["University of Aberdeen, Natural & Computing Sciences, Computing Science"],"datePublished":"01-01-20","abstract":"Peer reviewedPostprin","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":null,"rawRecordXml":"<record><header><identifier>\n        \n            \n                oai:aura.abdn.ac.uk:2164\/2204<\/identifier><datestamp>\n                2018-01-05T22:30:07Z<\/datestamp><setSpec>\n                com_2164_673<\/setSpec><setSpec>\n                com_2164_370<\/setSpec><setSpec>\n                com_2164_331<\/setSpec><setSpec>\n                com_2164_705<\/setSpec><setSpec>\n                col_2164_674<\/setSpec><setSpec>\n                col_2164_706<\/setSpec>\n            <\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:doc=\"http:\/\/www.lyncode.com\/xoai\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n            \nArguing Using Opponent Models<\/dc:title><dc:creator>\nOren, Nir<\/dc:creator><dc:creator>\nNorman, Timothy J.<\/dc:creator><dc:contributor>\nUniversity of Aberdeen, Natural & Computing Sciences, Computing Science<\/dc:contributor><dc:subject>\nargumentation<\/dc:subject><dc:subject>\nARGUMENTATION<\/dc:subject><dc:subject>\nQA75 Electronic computers. Computer science<\/dc:subject><dc:subject>\nQA75<\/dc:subject><dc:description>\nPeer reviewed<\/dc:description><dc:description>\nPostprint<\/dc:description><dc:date>\n2012-01-01T00:22:00Z<\/dc:date><dc:date>\n2012-01-01T00:22:00Z<\/dc:date><dc:date>\n2010<\/dc:date><dc:date>\n01-01-20<\/dc:date><dc:type>\nJournal article<\/dc:type><dc:identifier>\nOren , N & Norman , T J 2010 , ' Arguing Using Opponent Models ' Lecture Notes in Computer Science , vol 6057 , pp. 160-174 . DOI: 10.1007\/978-3-642-12805-9<\/dc:identifier><dc:identifier>\n0302-9743<\/dc:identifier><dc:identifier>\nPURE: 3088243<\/dc:identifier><dc:identifier>\nPURE UUID: c33be38a-5be6-4ee1-b22e-8190f5051103<\/dc:identifier><dc:identifier>\nWOS: 000280471900010<\/dc:identifier><dc:identifier>\nScopus: 77954809331<\/dc:identifier><dc:identifier>\nhttp:\/\/hdl.handle.net\/2164\/2204<\/dc:identifier><dc:identifier>\nhttp:\/\/dx.doi.org\/10.1007\/978-3-642-12805-9<\/dc:identifier><dc:language>\neng<\/dc:language><dc:relation>\nLecture Notes in Computer Science<\/dc:relation><dc:rights>\nOren, N & Norman, TJ 2010, 'Arguing Using Opponent Models' Lecture Notes in Computer Science, vol 6057, pp. 160-174. The original publication is available at www.springerlink.com doi: 10.1007\/978-3-642-12805-9<\/dc:rights><dc:format>\n15<\/dc:format>\n<\/oai_dc:dc>\n<\/metadata>\n        <\/record>","journals":[{"title":null,"identifiers":["0302-9743","issn:0302-9743"]}],"language":{"code":"en","id":9,"name":"English"},"relations":["Lecture Notes in Computer Science"],"year":1,"topics":["argumentation","ARGUMENTATION","QA75 Electronic computers. Computer science","QA75"],"subject":["Journal article"],"fullText":"Arguing Using Opponent Models\nNir Oren1 and Timothy J. Norman2\n1 Dept. of Computer Science\nKing\u2019s College London\nStrand, London\nWC2R 2LS, United Kingdom\nnir.oren@kcl.ac.uk\n2 Dept. of Computing Science\nUniversity of Aberdeen\nAberdeen\nAB24 3UE\nScotland\nt.j.norman@abdn.ac.uk\nAbstract. While researchers have looked at many aspects of argumen-\ntation, an area often neglected is that of argumentation strategies. That\nis, given multiple possible arguments that an agent can put forth, which\nshould be selected in what circumstances. In this paper we propose a\nheuristic that implements one such strategy. The heuristic is built around\nopponent modelling, and operates by selecting the line of argument that\nyields maximal utility, based on the opponent\u2019s expected response, as\ncomputed by the opponent model. An opponent model may be recur-\nsive, with the opponent modelling of the agent captured by the original\nagent\u2019s opponent model. Computing the utility for each possible line of\nargument is thus done using a variant of M* search, which in itself is\nan enhancement of min-max search. After describing the M* algorithm\nwe show how it may be adapted to the argumentation domain, and then\nstudy what enhancements are possible for more specific types of dialogue.\nFinally, we discuss how this heuristic may be extended in future work,\nand its relevance to argumentation theory in general.\n1 Introduction\nArgumentation has emerged as a powerful reasoning mechanism in many do-\nmains. Applications tend to revolve around either the logical form of argument,\nidentifying when an argument is, in some sense, acceptable. Some extension of\nDung\u2019s seminal argument framework [8] is typically used in such applications,\nand domains have included negotiation [3] and normative conflict detection [12].\nAnother use of argumentation makes use of the dialogue level, and focuses on the\ninteraction between different parties as the dialogue progresses. It is important\nto note that these two approaches are not mutually exclusive, but rather com-\nplementary. In describing argumentation systems, Prakken [18] identified four\nlayers with which an argument framework must concern itself. These are the\nlogical, dialectic, procedural, and heuristic layers. The first strand of research\nprimarily concerns itself with the logical and dialectic layers, while the second\nfocuses on the procedural level. As Prakken notes, little work has dealt with the\nheuristic layer, which, among other things, deals with how agents should decide\nwhich argument to advance at different points in the dialogue.\nIn this paper, we focus on the heuristic layer, examining argument strategies.\nIn other words, we attempt to determine what argument an agent should advance\nin order to achieve a certain goal. Previous work on the topic includes a utility\nbased approach with one step lookahead [14], while another tack has focused on\ngame theory and mechanism design [19, 20] to ensure that dialogues are strategy\nfree. Here, we look at how an agent may decide what utterance to advance by\nmaking use of opponent modelling. That is, given that agent \u03b1 has a belief\nabout agent \u03b2\u2019s knowledge and goals, we determine what argument the agent\nshould advance to achieve its aims. Informally, we examine possible utterances\nfrom each position in the dialogue, creating a tree of possible dialogues. We then\nmake use of techniques adapted from computer game playing, namely opponent\nmodel search and the M* algorithm [5], which is itself an extension of standard\nmin-max search [22].\nAs an example of such a search, consider two agents (\u03b1 and \u03b2) participating\nin a dialogue regarding whether to invade a small country. Agent \u03b1 would like to\npersuade \u03b2 that this is a prudent course of action. One possible line of argument\nis that the country has weapons of mass destruction, and that invading it will\nprevent the use of these weapons. If \u03b1 knows that \u03b2 cannot determine whether\nthe country has such weapons, and that \u03b2 may not ask \u03b1 where it obtained\nits information from, then \u03b1 will use this line of argument in the dialogue.\nHowever, if it knows that \u03b2 knows that the country has no WMDs, it will not\nattempt to use this line of argument. Thus, \u03b1 can make use of its model of \u03b2\u2019s\nknowledge in deciding what arguments to advance as part of a dialogue. The\nuse of opponent modelling as an argument strategy has clear applications to\nmany different scenarios, including persuasion type dialogues, and the domain\nof automated negotiation.\nThis paper\u2019s main contribution is a description of how a dialogue participant\nmay decide what arguments to advance given that it has a model of the other\ndialogue party. We also show how the M* algorithm may be adapted for the\nargumentation domain, and provide some domain dependant enhancements to\nthe search process, which results in a pruning of the dialogue tree.\nAfter describing the M* algorithm in more detail in the next section, we\nshow how M* can be adapted for the argumentation domain. Our initial effort,\ndescribed in Section 3.1 deals with very general situations, and we show how our\napproach can be specialised for more specific types of dialogues and situations\nin Sections 3.2 and 3.3. Finally, we discuss a number of possible enhancements\nto our approach, and describe additional related work.\n2 Background\nIn this section, we examine work in the field of opponent modelling. When partic-\nipating in a competitive event in which strategy may affect the outcome, human\nplayers typically adjust their strategy in response to the opponent(s) they face.\nOn the other hand, artificial agents typically construct plans in which they as-\nsume that their opponent will play in the way most damaging to their goals,\nresulting in algorithms such as min-max search, and many of the results of game\ntheory. Such approaches typically also assume that the opponent will use the\nsame strategy as the player.\nBy making use of opponent modelling, it is possible to represent opponents\nwith different goals and strategies to the agent doing the modelling. Such an\napproach raises a number of interesting possibilities. First, it allows for situations\nsuch as swindles and traps. The former occurs when a player may, due to an\nopponent\u2019s weakness, play a non-optimal move and still win, while the latter\nallows a player to play a weak move under the assumption that the opponent\nwill view it as a strong move. Second, by explicitly modelling the opponent\u2019s\ngoals, it is easier to deal with non-zero sum games. Other advantages gained by\nmaking use of strategies built around opponent modelling are described in [5].\nThe M* algorithm is similar to the min-max search algorithm, but makes\nuse of an explicit opponent model to more accurately calculate the utility of a\nposition. An opponent model (described as a player in [5]) is a pair \u3008f,O\u3009 where\nf is an evaluation function, and O is an opponent model. The latter may also\ntake on the special value of NIL, representing the case where no opponent model\nexists.\nIn this paper, we make use of a slightly modified form of this algorithm,\nbut before describing this modified algorithm, we examine the original, which is\nshown in Algorithm 1.\nInformally, given a board position, a search depth, and an opponent model\n(consisting of the opponent\u2019s evaluation function and another opponent model),\nthe algorithm returns the best move (and its value) by recursively computing\nwhat move the best move for the opponent would be for the given opponent\nmodel (line 11), based on what the opponent thinks the player\u2019s best move\nwould be (line 10), and so on, until the maximal depth of search is reached. At\nthis point, the utility of the current move can be computed (lines 2 and 8). The\nMoveGen function in line 5 is dependant on the rules of the game, and generates\nall possible moves from the current board position pos. It should be noted that\nit is easy to represent standard min-max search using the M* algorithm by using\nan opponent model of the form (f, (\u2212f, (f, . . .))) to the depth of the search. It\nshould also be noted that the algorithm assumes that the depth to which the\nopponent is modelled is greater than (or, more usually equal to) the depth to\nwhich the search takes place.\nAlgorithm 1 M\u2217(pos, depth, fpl, oppModel)\nRequire: A board position pos\nRequire: A search depth depth\nRequire: A position evaluation function fpl\nRequire: An opponent model oppModel = {fopl, oppModelo}\n1: if depth=0 then\n2: return (NIL, fpl(pos))\n3: else\n4: maxUtil = \u2212\u221e\n5: PossibleMoves =MoveGen(pos)\n6: for all move \u2208 PossibleMoves do\n7: if depth=1 then\n8: playUtil = fpl(move)\n9: else\n10: \u3008oppMove, oppUtil\u3009 =M\u2217(move, depth\u2212 1, fopl, oppModelo)\n11: \u3008playMove, playUtil\u3009 =M\u2217(oppMove, depth\u2212 2, \u3008fpl, oppModel\u3009)\n12: end if\n13: if playUtil > maxUtil then\n14: maxUtil = playUtil\n15: maxMove = move\n16: end if\n17: end for\n18: end if\n19: return (maxMove,maxUtil)\n3 Approach\nArgument strategies are employed by agents taking part in a dialogue. Argu-\nments are thus added over time, and the goal of an agent\u2019s strategy involves\ndeciding which utterance to make. For dialogue to take place, participants must\nagree on the rules governing the dialogue. These include not only the rules of\nthe dialogue game itself [10], such as whether turn taking exists, what utter-\nances may be made at which point, and the like, but also the rules governing\nthe interactions between arguments; for example stating that modus ponens is\nvalid.\n3.1 Opponent Modelling for General Dialogues\nWe begin by assuming a very general type of dialogue game similar in spirit to the\none proposed in [17]. This game is represented by agents advancing arguments.\nArguments may take the form of more specific utterances such as \u201cassert x\u201d, but\nsuch utterances are captured in abstract form, by being represented as arguments\nthemselves. Thus, for example, an agent may make an argument a, and if another\nagent questions this argument (for example, by asking \u201cwhy a?\u201d), this could be\nviewed as an argument b which attacks a. A response could then be an argument\nc, attacking b, etc. We denote this set of abstract arguments Args.\nA dialogue represents the utterances made by an agent. We define it very\nsimply as follows:\nDefinition 1. (Dialogue) A dialogue is an ordered set of arguments\nDialogue = {a, b, . . .}\nsuch that Dialogue \u2286 Args.\nFor convenience, and without loss of generality, we assume that only two\ndialogue participants exist, and that they take turns when making utterances.\nWe do not therefore need to associate a dialogue participant with an utterance\nin our representation of a dialogue.\nIn order to abstract the rules governing a dialogue, we assume the existence\nof a generator function allowing us to compute the set of legal possible moves in\na dialogue. This function is equivalent to the MoveGen function in Algorithm 1.\nDefinition 2. (Move Generation) The function\nlegalMoves : 2Dialogue \u2192 2Args\ntakes in a dialogue and returns the set of arguments that may be uttered by an\nagent at that point in the dialogue.\nNote that only a single argument may be advanced by an agent during its\nturn (as this argument may actually encapsulate other arguments). Also, note\nthat the legal moves function does not explicitly depend on the player making\nan utterance (though such information may be implicitly found by examining\nthe dialogue\u2019s current length). The legalMoves function identifies legal moves\nrather than possible moves for a dialogue. That is, moves that are sanctioned by\nthe rules of the game, rather than those moves that an agent may actually make\nbased on factors such as its knowledge and goals.\nAt this point, we must define the structure of dialogue participants (also\nreferred to as agents). An agent participating in a dialogue has a knowledge\nbase identifying the arguments it is aware of, some goals it is trying to achieve,\nand an opponent model.\nDefinition 3. (Agent) An agent is a tuple\n\u3008KB,Goals,Opp\u3009\nwhere KB \u2286 Args is a knowledge base containing those arguments known by an\nagent, Goals is the agent\u2019s goal function, and is described in Definition 4, while\nOpp is an opponent model as detailed in Definition 5.\nAn agent participates in a dialogue to achieve certain goals. Different dia-\nlogues may meet these goals to a greater or lesser extent. An agent may thus\nassign a utility to a dialogue, and this is modelled by the Goals function. This\nutility may depend on many factors, including the arguments introduced within\nthe dialogue, the agent introducing these arguments, the arguments deemed ad-\nmissible by some argumentation semantics at some stage of the dialogue, and\nthe like.\nDefinition 4. (Goals) A Goals function takes in a dialogue, and returns its\nutility.\nGoals : 2Dialogue \u2192 R\nAn opponent model is meant to represent an agent, and thus looks very\nsimilar to Definition 3. However, we must also handle the situation where an\nagent has no recursive model of its opponent. We start by representing this\nsituation, and then recursively defining more complex, nested opponent models.\nDefinition 5. (Opponent Model) An opponent model is incrementally defined\nas follows:\n\u2013 \u3008KB,Goals, \u2205\u3009 is an opponent model\n\u2013 \u3008KB,Goals,Opp\u3009, where Opp is an opponent model.\nHere, KB \u2286 Args and Goals : 2Dialogue \u2192 R are a knowledge base and goal\nfunction respectively.\nGiven an agent with some knowledge base, goal function and opponent model,\ntogether with a legalMoves function representing the dialogue game\u2019s rules, an\nagent may decide what utterance to make (i.e. what argument to advance) by\nfollowing the slightly modified version of M*, called M\u2217gd shown in Algorithm 2.\nApart from the different specification of an agent, line 5 ensures that an agent\nnot only uses moves generated by the legalMoves function, but filters these to\nensure that it only uses moves that it believes are possible (according to its\nknowledge base). Also, note that on line 10, the opponent model is treated as\nan agent when passed as a parameter to the algorithm.\nThe M\u2217gd algorithm is very general, in the sense that it does not make use of\nany of the properties of argument that would differentiate it from other types of\nsearch. In the remainder of this section, we take a closer look at the legalMoves\nand Goals functions, showing how they may be specialised to represent more\nspecific dialogues, and how these specialisations may aid in pruning the possible\ndialogue tree.\n3.2 Pruning by Legal Moves\nOne of the focuses of argumentation research examines how groups of arguments\ninteract. Typically, this interaction is built around the notion of attack, and\nmore controversially support, between arguments. The arguments advanced by\n(rational) agents in the course of a dialogue are aimed at achieving their goals,\neither by building up support for, or attacking other arguments [2].\nVarious semantics for argument frameworks have been proposed [8, 13]. These\nsemantics, when given a set of arguments and the interactions between them,\nidentify which sets of arguments may be viewed as, in some sense, consistent\nwith each other. In most cases, it makes little sense for an agent to introduce an\nargument which would not be deemed consistent, and so we begin by showing\nhow this notion may be added to the agent\u2019s reasoning process.\nFiltering the set of arguments that may be advanced is the task of the\nlegalMoves function. We must, therefore, specialise this function as follows:\nAlgorithm 2 M\u2217gd(dia, depth, agent, legalMoves)\nRequire: A dialogue dia\nRequire: A search depth depth\nRequire: An agent agent = \u3008KB,Goals, oppModel\u3009\nRequire: A move generation function legalMoves\n1: if depth=0 then\n2: return (NIL,Goals(dia))\n3: else\n4: maxUtil = \u2212\u221e\n5: PossibleMoves = legalMoves(dia) \u2229KB\n6: for all move \u2208 PossibleMoves do\n7: if depth=1 then\n8: playUtil = Goals(dia \u222a {move})\n9: else\n10: \u3008oppMove, oppUtil\u3009 =M\u2217gd(dia\u222a{move}, depth\u22121, oppModel, legalMoves)\n11: \u3008playMove, playUtil\u3009 =M\u2217gd(dia \u222a {move, oppMove}, depth \u2212 2 ,\nagent , legalMoves)\n12: end if\n13: if playUtil > maxUtil then\n14: maxUtil = playUtil\n15: maxMove = move\n16: end if\n17: end for\n18: end if\n19: return (maxMove,maxUtil)\n\u2013 Introduce the notion of an argument system; rather than having the ar-\nguments Args in isolation, we define an argument system consisting of\nArgs and a set of relations between subsets (or elements) of Args. For\nexample, a Dung-style argument system is a tuple \u3008Args,Attacks\u3009 where\nAttacks \u2286 Args\u00d7Args.\n\u2013 Add a semantics under which the concept of \u201cadditional information\u201d can be\njudged. In the case of a Dung-style argument system, these may be preferred,\ngrounded, stable, or some other semantics.\nDefinition 6. (The Legal Moves Function for Argument Systems) The\nlegalMoves function for an argument system AS is a function\nlegalMoves : AS \u00d7 Semantics\u00d7Dialogue\u2192 2Args\nThis function takes in an argument system AS, together with an associated se-\nmantics Semantics, and a dialogue, and returns a set of possible arguments.\nFor example, in a Dung-style argument system, the legalMoves function\ncould be defined as follows3:\n3 Note that this definition means that only an argument relevant (in the sense of\nDefinition 7) to some argument may be introduced in this dialogue game. However,\nlegalMoves(AS ,DSemantics,Dialogue) ={pass}\u222a\n{a|a \u2208 Args and\nDSemantics(\u3008Dialogue \u222a {a},Attacks\u3009)\n6= DSemantics(\u3008Dialogue,Attacks\u3009)}\nWhere pass is a move meaning no utterance is made4, AS = \u3008Args,Attacks\u3009\nand DSemantics : 2AS \u2192 2Args captures the appropriate Dung-style preferred,\ngrounded or stable semantics.\nThis legalMoves function requires that a legal move be one that changes the\nconclusions that can be drawn from the dialogue if it were to terminate at this\npoint. In other words, a legal move is one that changes the set of arguments that\nare contained in the grounded, preferred or whatever extension that is required\nby the semantics. As a consequence of this, the dialogue tree of moves excludes\nthose that do not alter the accepted set of arguments, and so this function\nensures that a dialogue will terminate (assuming a finite set of arguments).\nIt is important to note that the legalMoves function captures both the rules\nof the dialogue game\u2019s locutions, and the contents of these locutions, mixing the\ndialogue game\u2019s syntax with its semantics.\n3.3 Filtering Using Relevance\nOften, a dialogue participant\u2019s goals revolve around having the other parties\naccept, or reject a single argument. In such a situation, arguments that are not\ndirectly relevant to this single argument may be ignored when searching the\npossible move tree. This notion is related, but different to the idea presented in\nthe previous section. The legalMoves function captures all legal moves according\nto a dialogue. A relevant move is a legal move that also affects the agent\u2019s goals.\nWe define one argument as relevant to another if it can affect its status in some\nway.\nDefinition 7. (Relevance) Given\n\u2013 Two arguments a, b\n\u2013 The set of all arguments Args\n\u2013 A semantics for argument, which is able to determine the status of an ar-\ngument given a set of arguments using the function Semantics(c,X) where\nc \u2208 Args and X \u2286 Args\nWe say that a is relevant to b (in the context of some argument system) if\n\u2203X \u2286 Args such that a \/\u2208 X and Semantics(b,X) 6= Semantics(b,X \u222a {a}).\nWe may write relevant(a, b) to indicate that a is relevant to b.\na relevance-aware agent (Definition 8) considers arguments relevant to its goals,\nmaintaining the distinction between legal moves and moves relevant to it. Note also\nthat this legalMoves function allows an agent to introduce arguments that attack\nits own arguments, which may not be allowed in some dialogues.\n4 A dialogue game typically ends after all agents consecutively pass.\nIn dialogues where an agent may pass to end the game, a pass move is also\nconsidered relevant. In such games, relevant(pass, A) for any A \u2208 Args also\nholds.\nWhile the legalMoves function may make use of the underlying argument\nsystem\u2019s semantics, the determination of relevance of an argument must employ\nthe system\u2019s semantics.\nWithin a Dung-style argument framework, an argument is relevant if there\nis a path within the argument graph between it and some other argument. This\nmakes sense intuitively, as the argument has the potential to affect the other\nargument\u2019s status (by directly, or indirectly attacking, or reinstating it).\nSupport is another way of having one argument affect another. In many dia-\nlogue types, an argument\u2019s premises have to be introduced before the argument\nitself may be used. A number of semantics have been proposed to deal with\nsupport [13, 1, 9], and any are appropriate for the purposes of this algorithm5\nAs mentioned above, the notion of relevance requires that an agent be able\nto specifically identify its goals. We extend the notion of an agent to capture\nthis as follows:\nDefinition 8. (Relevance-Aware Agent) A relevance-aware agent is a tuple\n\u3008KB,Goals,GoalArguments,Opp\u3009\nwhere KB \u2286 Args, Goals : 2Dialogue \u2192 R and Opp is an opponent model as in\nDefinition 3, and GoalArguments \u2286 Args.\nAs discussed below, a relevance-aware agent utilises Algorithm 3 to consider\nonly arguments which are relevant to its GoalArguments in its search. If the\nGoalArguments set is small (in comparison to the arguments returned by the\nlegalMoves function), this may lead to a considerable pruning of the search\ntree. If we assume a single argument within GoalArguments, then the notion\nof relevance presented here is equivalent to R1 relevance in the work of Parsons\net al. [16].\nAs the number of GoalArguments increase, the usefulness of the relevance\noptimization decreases, as more arguments typically become relevant. In order\nfor the relevance based approach to function, the Goals function must also be\nconstrained.\nSince the utility of a dialogue is only evaluated once the tree is expanded to\nthe maximum search depth, we cannot simply alter the agent\u2019s Goal function to\n5 It should be noted that the dialogues resulting from a framework such as [13] are\nvery different to those obtained from a Dung style framework. In the latter, the\ndialogue would begin with the goal arguments being introduced, and attacks on\nthose arguments (and attacks on those attacks) introduced in subsequent moves.\nHowever, the former approach requires that all arguments be supported, meaning\nthat the dialogue would progress by first introducing arguments as premises, and\nthen build on these premises with additional arguments, until the goal arguments\nare introduced. Attacks on introduced arguments may still occur at any time.\ninclude the notion of relevance. Instead, an agent must filter the set of possible\nmoves to be evaluated based on its goals. To do this, a relevance-aware agent\ninvokes Algorithm 3 using\nM\u2217rel(dia, depth, \u3008KB,Goals,Opp\u3009, GoalArguments, legalMoves)\nWhere dia, depth and legalMoves are dependent on the dialogue and the\nagent\u2019s capabilities.\nIt is clear that the relevant moves form a subset of the legal moves for a\ndialogue. Thus, for all g \u2208 GoalArguments\u22c3\ng\n{a|a \u2208 Args and relevant(a, g)} \u2286 legalMoves(AS, Semantics,Dialogue)\nIf this subset relation is strict, the notion of relevance is useful in pruning the\nsearch tree. However, even in this case, we still filter by using the legalMoves\nfunction (line 5) before filtering by relevance, as we assume that this operation\nis computationally cheaper than relevance filtering.\nLines 6\u201310 filter out irrelevant moves. It should be noted that the relevance-\naware agent\u2019s GoalArguments are used at all depths of this algorithm, as any\nmove that does not affect these goals can be ignored by the agent. This follows\nfrom the idea that if the other party makes (what are considered) irrelevant\narguments, these have no effects on the arguments introduced by the agent.\nAs a simple example, consider the relevance-aware agent\n\u3008{a, c}, {a}, {a}, \u3008{b}, {\u00aca}, \u3008{d}, {a}, {}\u3009\u3009\u3009\nHere, \u00aca represents a goal that a is not deemed acceptable according to\nthe game\u2019s semantics. We may assume that only a single argument may be\nintroduced by an agent during its turn (or that it may pass), and assume Dung-\nstyle argument system with grounded semantics.\nAS = ({a, b, c, d}, {(b, a), (d, b)})\nFor simplicity, we represent goals as single arguments rather than dialogues;\nwe assume that all dialogues containing the goal argument are worth 10 utility,\nless one utility for every argument the agent advances.\nThen at the first level of argument, arguments a, c, and passing are all legal\nmoves according to the agent\u2019s knowledge base and semantics. However, a and\npass are the only relevant moves, and the agent must then determine what\nthe opponent will play in response to a. According to its opponent model, the\nopponent could play b. However, argument c is not relevant, and the agent will\nthus not consider whether the opponent will play it or not. The algorithm shows\nthat if the opponent plays b, it believes that the agent will play d (even though\nit does not actually know d). Since playing a move will cost the opponent utility,\nthe agent believes that the opponent will pass.\nAlgorithm 3 M\u2217rel(dia, depth, agent, goalArguments, legalMoves)\nRequire: A dialogue dia\nRequire: A search depth depth\nRequire: An agent agent = \u3008KB,Goals, oppModel\u3009\nRequire: A set of arguments goalArguments\nRequire: A move generation function legalMoves\n1: if depth = 0 then\n2: return (NIL,Goals(dia))\n3: else\n4: maxUtil = \u2212\u221e\n5: PossibleMoves = legalMoves(dia) \u2229KB\n6: for all move \u2208 PossibleMoves do\n7: if @a \u2208 goalArguments such that relevant(move, a) then\n8: PossibleMoves = PossibleMoves\\move\n9: end if\n10: end for\n11: for all move \u2208 PossibleMoves do\n12: if depth = 1 then\n13: playUtil = Goals(dia \u222a {move})\n14: else\n15: \u3008oppMove, oppUtil\u3009 =M\u2217rel(dia \u222a {move}, depth\u2212 1,\noppModel , goalArguments, legalMoves)\n16: \u3008playMove, playUtil\u3009 =M\u2217rel(dia \u222a {move, oppMove}, depth \u2212 2 ,\nagent , goalArguments, legalMoves)\n17: end if\n18: if playUtil > maxUtil then\n19: maxUtil = playUtil\n20: maxMove = move\n21: end if\n22: end for\n23: end if\n24: return (maxMove,maxUtil)\nIf instead, the agent passes, it will gain no utility, while the opponent would\nalso pass, and would gain 10 utility. Thus, the agent will utter a, and if its\nopponent model is accurate, will win the game when the opponent passes.\nGiven perfect information, the agent should not have won this game. Fur-\nthermore, the agent gains 10 utility, while the opponent gains 0 utility.\n4 Discussion\nThis paper is built around the idea of an agent being able to assign utility to\na dialogue. While the assignment of positive utility to a dialogue represents the\nagent meeting some of its goals, negative utility can arise from a number of\ndialogue-dependent situations. These obviously include the agent not meeting\nits goals, and, more interestingly, may occur when some other argument is in-\ntroduced (c.f. [15]), or when the agent actually makes use of an argument (for\nexample, as a premise to one of its own arguments). Work such as [14] examines\nsome of these utility assignment approaches in more detail, but performs only\none step lookahead when selecting an utterance.\nAs discussed in Section 2, it is possible to model a min-max opponent by\nmaking use of an opponent model of the form (f, (\u2212f, (f, . . .))) to the depth\nof the search. Constructing a min-max opponent model for the argumentation\ndomain requires incorporating a knowledge base into the opponent model. Two\nnatural choices for an opponent\u2019s knowledge base arise. First, it could be identical\nto the original agent\u2019s knowledge base. Second, it could contain all arguments in\nthe system. Since min-max is inherently pessimistic, the latter approach makes\nsense for the opponent\u2019s model, while the former knowledge base would represent\nthe opponent\u2019s model of the agent. It has been shown that an agent making use\nof theM\u2217 strategy will perform no worse than an agent utilising min-max search\n[5], and this result can be trivially mapped to our extensions of the strategy.\nThe worst case computational complexity of the M\u2217 algorithm, and thus\nour heuristic is bounded by (b + 1)d\u22121 where b represents the branching fac-\ntor, and d the search depth. Clearly, computational techniques that prune the\ntree are important if the algorithm is to be used in real world situations. The\ntechniques described in Section 3.2 and 3.3 reduce the branching factor, and\nperform pruning of the dialogue tree respectively. Furthermore, depending on\nthe form of the utility function, an adaptation of \u03b1\u03b2-pruning may be applied to\nM\u2217[5]. This adaptation is particularly applicable to persuasion dialogues, where\nan agent meeting its goals usually means that its opponent fails to meet their\ngoals. Other techniques, such as transposition tables can also yield significant\ncomputational savings, even when the utility function is relatively complex.\nMoore [11] suggested that any strategy for argument must meet three criteria,\nnamely to maintain the focus of the dispute, build its point of view, or attack the\nopponent\u2019s one, and select an argument that fulfils the previous two objectives.\nThe inclusion of relevance, and the filtering of the legal moves function allows an\nagent to include these criteria within its reasoning process. Clearly, any utterance\nmade as a result of a reasoning process meeting Moore\u2019s criteria will itself meet\nthe criteria.\nAdditional work dealing with argument strategy includes the work of Riveret\n[20, 21], who, like us, builds a tree of dialogues, and computes game theoretic\nequilibria to determine which move an agent should make. However, he assumes\nthat the agent\u2019s knowledge and goals are perfectly known, an assumption which\ndoes not hold in many situations. Riveret\u2019s work is more general than ours in\none sense, namely in that the dialogues they investigate do not assume that\nagents make a single utterance during their turn, or indeed, take turns in an\norderly manner. However, our approach can easily be extended to include this\ngeneralisation. Rahwan [19] has also examined the notion of strategy within\nargument, but focused on showing how dialogues may be designed so as to make\nthem strategy-proof.\nWe have begun implementing and evaluating the performance of agents mak-\ning use of opponent modelling as part of their argument strategy. Our initial re-\nsults indicate that this approach outperforms techniques such as min-max search,\nbut still need to investigate issues such as the effects of errors in the opponent\nmodel on the quality of the strategy, and the tradeoffs between search time and\nsearch quality when increasing the search depth.\nIt is clear that the introduction of an argument by an opponent may cause an\nupdate in our knowledge base or opponent model (for example, if an argument\na supports an argument b, which in turn supports argument c and we believe\nthat our opponent\u2019s goal is argument c, the introduction of a should cause us\nto believe that our opponent knows b). Our algorithms cater for such updates\nimplicitly; the agent\u2019s opponent model at the appropriate depth of the expanded\ndialogue tree should include the fact that the opponent knows b. In the future,\nwe intend to provide more constraints on the form of the agent and its opponent\nmodel, capturing notions such as knowledge\/belief revision.\nAnother area of future work involves extending our algorithms to make use\nof probabilistic opponent modelling [7]. In this paper, we assumed that an agent\nhad a single model of its opponent. However, probabilistic opponent modelling\nwould allow an agent to reason with imperfect information about its opponent,\na situation that commonly arises in real world situations. The addition of prob-\nabilistic opponent modelling opens up many exciting avenues for research. For\nexample, an agent may have two strategies open to it; one that will yield it a\nhigh utility, but assumes that there is a high chance that its opponent does not\nknow a key fact, and the other yielding less utility, but being much safer. In\nsuch a situation, the agent must take into account the risk of making certain\nutterances, and we intend to investigate how such considerations can form part\nof an agent\u2019s argument strategy.\nFinally, we have assumed throughout the paper that an agent has some oppo-\nnent model, with an arbitrary evaluation function and knowledge base. However,\nwe have not examined how an agent may learn an opponent model during the\ncourse of a dialogue, or over repeated interactions with an opponent. Some work\non this topic exists in the context of game playing [6], and we intend to apply it\nto the argumentation domain.\n5 Conclusions\nIn this paper, we showed how an agent may decide on which utterances to\nadvance in the course of an argument by making use of an opponent model. By\nmaking use of an opponent model, the agent can discover lines of argument that\nmay take advantage of its opponent\u2019s beliefs and goals. Such lines of argument\nwould not be discovered by a more na\u00a8\u0131ve strategy, such as min-max, which would\nassume that the opponent\u2019s goals are diametrically opposed to its own.\nWe also showed how various aspects of the argumentation domain, namely\nthe notion of relevance, and the interactions between arguments, may be used to\nreduce the computational complexity of searching for an argument while making\nuse of opponent modelling.\nTo our knowledge, no work on argument strategies has yet utilised the no-\ntion of an opponent model to the same extent as we have. The introduction of\nopponent modelling opens up a number of avenues for future research, and also\nallows for the creation of new, and novel strategies for argument.\nReferences\n1. L. Amgoud, C. Cayrol, and M.-C. Lagasquie-Schiex. On the bipolarity in ar-\ngumentation frameworks. In Proceedings of the 10th International Workshop on\nNon-monotonic Reasoning, pages 1\u20139, Whistler, Canada, 2004.\n2. L. Amgoud and N. Maudet. Strategical considerations for argumentative agents\n(preliminary report). In Proceedings of the 9th International Workshop on Non-\nmonotonic Reasoning, pages 399\u2013407, 2002.\n3. L. Amgoud and H. Prade. Generation and evaluation of different types of ar-\nguments in negotiation. In Proceedings of the 10th International Workshop on\nNon-monotonic Reasoning, 2004.\n4. P. Besnard, S. Doutre, and A. Hunter, editors. Computational Models of Argument:\nProceedings of COMMA 2008, Toulouse, France, May 28-30, 2008, volume 172 of\nFrontiers in Artificial Intelligence and Applications. IOS Press, 2008.\n5. D. Carmel and S. Markovitch. Incorporating opponent models into adversary\nsearch. In In Proceedings of the Thirteenth National Conference on Artificial In-\ntelligence, pages 120\u2013125. AAAI, 1996.\n6. D. Carmel and S. Markovitch. Model-based learning of interaction strategies in\nmulti-agent systems. Journal of Experimental and Theoretical Artificial Intelli-\ngence, 10(3):309\u2013332, 1998.\n7. H. H. L. M. Donkers, J. W. H. M. Uiterwijk, and H. J. van den Herik. Probabilistic\nopponent-model search. Information Sciences, 135(3-4):123\u2013149, 2001.\n8. P. M. Dung. On the acceptability of arguments and its fundamental role in non-\nmonotonic reasoning, logic programming and n-person games. Artificial Intelli-\ngence, 77(2):321\u2013357, 1995.\n9. D. C. Mart\u00b4\u0131nez, A. J. Garc\u00b4\u0131a, and G. R. Simari. Progressive defeat paths in\nabstract argumentation frameworks. In L. Lamontagne and M. Marchand, editors,\nCanadian Conference on AI, volume 4013 of Lecture Notes in Computer Science,\npages 242\u2013253. Springer, 2006.\n10. P. McBurney and S. Parsons. Dialogue games in multi-agent systems. Informal\nLogic, 22(3):257\u2013274, 2002.\n11. D. Moore. Dialogue game theory for intelligent tutoring systems. PhD thesis, Leeds\nMetropolitan University, 1993.\n12. N. Oren, M. Luck, S. Miles, and T. J. Norman. An argumentation inspired\nheuristic for resolving normative conflict. In Proceedings of The Fifth Work-\nshop on Coordination, Organizations, Institutions, and Norms in Agent Systems\n(COIN@AAMAS-08), pages 41\u201356, Estoril, Portugal, 2008.\n13. N. Oren and T. J. Norman. Semantics for evidence-based argumentation. In\nBesnard et al. [4], pages 276\u2013284.\n14. N. Oren, T. J. Norman, and A. Preece. Arguing with confidential information.\nIn Proceedings of the 18th European Conference on Artificial Intelligence, pages\n280\u2013284, Riva del Garda, Italy, August 2006.\n15. N. Oren, T. J. Norman, and A. Preece. Loose lips sink ships: a heuristic for argu-\nmentation. In Proceedings of the Third International Workshop on Argumentation\nin Multi-Agent Systems, pages 121\u2013134, Hakodate, Japan, May 2006.\n16. S. Parsons, P. McBurney, E. Sklar, and M. Wooldridge. On the relevance of utter-\nances in formal inter-agent dialogues. In AAMAS \u201907: Proceedings of the 6th in-\nternational joint conference on Autonomous agents and multiagent systems, pages\n1\u20138, New York, NY, USA, 2007. ACM.\n17. H. Prakken. Relating protocols for dynamic dispute with logics for defeasible\nargumentation. Synthese, 127:187\u2013219, 2001.\n18. H. Prakken and G. Sartor. Computational Logic: Logic Programming and Beyond.\nEssays In Honour of Robert A. Kowalski, Part II, volume 2048 of LNCS, pages\n342\u2013380. Springer-Verlag, 2002.\n19. I. Rahwan and K. Larson. Mechanism design for abstract argumentation. In\nProceedings of AAMAS 2008, 2008.\n20. R. Riveret, H. Prakken, A. Rotolo, and G. Sartor. Heuristics in argumentation: A\ngame theory investigation. In Besnard et al. [4], pages 324\u2013335.\n21. R. Riveret, N. Rotolo, S. G, H. Prakken, and B. Roth. Success chances in argument\ngames:a probabilistic approach to legal disputes. In Proceedings of the 20th An-\nniversary International Conference on Legal Knowledge and Information Systems\n(Jurix 2007), pages 99\u2013108, Amsterdam, The Netherlands, 2007.\n22. C. E. Shannon. Programming a computer for playing chess. Philosophical Maga-\nzine, 41:256\u2013275, 1950.\n"}