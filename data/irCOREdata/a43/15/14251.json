{"doi":"10.1080\/0968776000080307","coreId":"14251","oai":"oai:generic.eprints.org:342\/core5","identifiers":["oai:generic.eprints.org:342\/core5","10.1080\/0968776000080307"],"title":"Evaluating learning and teaching technologies in further education","authors":["Jones, Ann","Barnard, Jane","Calder, Judith","Scanlon, Eileen","Thompson, Julie"],"enrichments":{"references":[{"id":1881097,"title":"65Ann Jones et al Evaluating learning and teaching technologies in further education","authors":[],"date":"1997","doi":null,"raw":"65Ann Jones et al Evaluating learning and teaching technologies in further education Draper, S. W. (1997), 'Prospects for summative evaluation of CAL in higher education', ALT-J, 5 (1), 33-9.","cites":null},{"id":453039,"title":"A framework for evaluating the use of educational technology',","authors":[],"date":"1997","doi":null,"raw":"Oliver, M. (1997), 'A framework for evaluating the use of educational technology', BP ELT Report No. 1, Learning and Teaching Innovation and Development (LaTID), London: University of North London.","cites":null},{"id":198623,"title":"CAL evaluation: future directions',","authors":[],"date":"1997","doi":"10.1080\/0968776970050107","raw":"Gunn, C. (1997), 'CAL evaluation: future directions', ALT-J, 5 (1), 40-7.","cites":null},{"id":198619,"title":"Computers in FE Biology: a study of how teachers' classroom practice can be affected by different software',","authors":[],"date":"1999","doi":null,"raw":"Barnard, J. (1999), 'Computers in FE Biology: a study of how teachers' classroom practice can be affected by different software', Educational Technology and Society, 2 (4).","cites":null},{"id":198621,"title":"et al Evaluating learning and teaching technologies in further education","authors":[],"date":"1997","doi":"10.3402\/rlt.v8i3.12005","raw":null,"cites":null},{"id":198625,"title":"Evaluating CAL at the Open University: 15 years on',","authors":[],"date":"1996","doi":"10.1016\/0360-1315(95)00064-x","raw":"Jones, A., Scanlon, E., Tosunoglu, C., Butcher, P., Greenberg, J., Murphy, P. and Ross, S. (1996), 'Evaluating CAL at the Open University: 15 years on', Computers and Education, 26 (1-3), 5-15.","cites":null},{"id":1043029,"title":"Evaluating communication and information technologies: a toolkit for practitioners',","authors":[],"date":"1998","doi":null,"raw":"Oliver, M. and Conole, G. (1998), 'Evaluating communication and information technologies: a toolkit for practitioners', Active Learning, 8, July, 3-8.","cites":null},{"id":1043028,"title":"Evaluation using ethnography: context, content and collaboration',","authors":[],"date":"1998","doi":null,"raw":"Jones, C. (1998) 'Evaluation using ethnography: context, content and collaboration', in M. Oliver (ed.), Innovation in the Evaluation of Learning Technology, London: University of North London, 87-100.","cites":null},{"id":198622,"title":"Integrative evaluation: an emerging role for classroom studies of CAL',","authors":[],"date":"1996","doi":"10.1016\/0360-1315(95)00068-2","raw":"Draper, S. W., Brown, M. I., Henderson, F. P. and McAteer, E. (1996), 'Integrative evaluation: an emerging role for classroom studies of CAL', Computers in Education, 1-3, 17-32.","cites":null},{"id":453040,"title":"ITMA's approach to classroom observation',","authors":[],"date":"1988","doi":null,"raw":"Phillips, R. (1988), 'ITMA's approach to classroom observation', in A. Jones and P. Scrimshaw (eds.), Computers in Education, Milton Keynes: Open University Press, 5-13.","cites":null},{"id":453041,"title":"Learning with computers: experiences from an evaluation project',","authors":[],"date":"1998","doi":"10.1016\/s0360-1315(97)00073-0","raw":"Scanlon, E., Tosunoglu, C., Jones, A., Butcher, P., Ross, S., Greenberg, J., Taylor J. and Murphy, P. (1998), 'Learning with computers: experiences from an evaluation project', Computers and Education, 30 (1\/2), 9-14.","cites":null},{"id":198620,"title":"Programme Evaluation and Quality: A Comprehensive Guide to Setting up an Evaluation System,","authors":[],"date":"1994","doi":null,"raw":"Calder, J. (1994), Programme Evaluation and Quality: A Comprehensive Guide to Setting up an Evaluation System, Kogan Page\/Open University.","cites":null},{"id":1043027,"title":"Reflections on a model for evaluating learning technologies',","authors":[],"date":"1998","doi":null,"raw":"Jones, A., Scanlon, E. and Blake, C. (1998), 'Reflections on a model for evaluating learning technologies', in M. Oliver (ed.), Innovation in the Evaluation of Learning Technology, London: University of North London, 25-41.","cites":null},{"id":198624,"title":"Report of the Learning and Technology Committee, Further Education Funding Council.","authors":[],"date":"1996","doi":null,"raw":"Higginson, G. (1996), Report of the Learning and Technology Committee, Further Education Funding Council.","cites":null},{"id":453042,"title":"The Stationery Office","authors":[],"date":"1998","doi":"10.5860\/choice.35-4220","raw":"The Stationery Office (1998), The Learning Age: A Renaissance for a New Britain, UK Government Green Paper.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2000","abstract":"With the current emphasis on quality assessment and the role of evaluation in quality assessment, it is likely that teachers in post\u2010compulsory education will increasingly be expected to evaluate their teaching, especially when making changes to their teaching methods. In Further Education (FE), there have been a number of developments to foster the use of Information and Learning Technologies (ILT), following the publication of the Higginson Report in 1996. However, there is some evidence that the adoption of ILT has been patchy","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/14251.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/342\/1\/ALT_J_Vol8_No3_2000_Evaluating%20learning%20and%20teachi.pdf","pdfHashValue":"da32d63ea2c359a22626f9bd3bf3adedcf651715","publisher":"University of Wales Press","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:342<\/identifier><datestamp>\n      2011-04-04T09:14:29Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/342\/<\/dc:relation><dc:title>\n        Evaluating learning and teaching technologies in further education<\/dc:title><dc:creator>\n        Jones, Ann<\/dc:creator><dc:creator>\n        Barnard, Jane<\/dc:creator><dc:creator>\n        Calder, Judith<\/dc:creator><dc:creator>\n        Scanlon, Eileen<\/dc:creator><dc:creator>\n        Thompson, Julie<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        With the current emphasis on quality assessment and the role of evaluation in quality assessment, it is likely that teachers in post\u2010compulsory education will increasingly be expected to evaluate their teaching, especially when making changes to their teaching methods. In Further Education (FE), there have been a number of developments to foster the use of Information and Learning Technologies (ILT), following the publication of the Higginson Report in 1996. However, there is some evidence that the adoption of ILT has been patchy.<\/dc:description><dc:publisher>\n        University of Wales Press<\/dc:publisher><dc:date>\n        2000<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/342\/1\/ALT_J_Vol8_No3_2000_Evaluating%20learning%20and%20teachi.pdf<\/dc:identifier><dc:identifier>\n          Jones, Ann and Barnard, Jane and Calder, Judith and Scanlon, Eileen and Thompson, Julie  (2000) Evaluating learning and teaching technologies in further education.  Association for Learning Technology Journal, 8 (3).  pp. 56-66.  ISSN 0968-7769     <\/dc:identifier><dc:relation>\n        10.1080\/0968776000080307<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/342\/","10.1080\/0968776000080307"],"year":2000,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"Evaluating learning and teaching technologies in\nfurther education\nAnn Jones, Jane Barnard, Judith Calder, Eileen Scanlon and JulieThompson\nInstitute of Educational Technology, Open University\nemail: a.c.jones@open.ac.uk\nWith the current emphasis on quality assessment and the role of evaluation in quality\nassessment, it is likely that teachers in post-compulsory education will increasingly be\nexpected to evaluate their teaching, especially when making changes to their teaching\nmethods. In Further Education (FE), there have been a number of developments to\nfoster the use of Information and Learning Technologies (ILT), following the publica-\ntion of the Higginson Report in 1996. However, there is some evidence that the adoption\nof ILT has been patchy.\nThis paper reports on a project funded by the Further Education Development Agency to\ndevelop evaluation tools for use by FE colleges to evaluate their use of ILT. One of the\nmain challenges for the project team was to produce a tool that could be used by time-\npressed practitioners with little or no experience of evaluation for use with very diverse\nprojects and students. The paper discusses this challenge, the approach to developing the\ntool that was adopted, the findings from the project and the implications of these\nfindings.\nIntroduction\nThere is currently an unprecedented interest in the use of technologies for supporting\nteaching and learning. In post-compulsory education, the current Government's\ncommitment to increasing access to Lifelong Learning is expressed through a number of\ninitiatives that also affect the further education (FE) sector. For example, in The Learning\nAge: A Renaissance for a New Britain (Stationery Office, 1998) the government outlines its\nproposal to expand the scale, scope and nature of both further and higher education. The\nLearning Age follows a number of such government papers that emphasize the importance\nof Information and Communication Technologies (ICTs) or Information and Learning\nTechnologies (ILTs) in FE and HE.\n56\nALT-] Volume 8 Number 3\nFor several years, HE and FE have been exploring the use of ICTs as a way of serving an\nexpanding student population and allowing more flexibility in study patterns. This has\nresulted in the growth of flexible and distance education for lifelong learning and in the\nincreased use of new technologies for supporting teaching and learning. In the FE sector,\nthe report of the Learning and Technology committee of the FEFC (Higginson, 1996)\nrecommended a series of strategic developments to foster the use of information and\ncommunication technologies across the sector. The first response to Higginson's\nrecommendations was the funding by FEFC of a national staff development programme,\nQUILT (Quality in Information and Learning Technologies) managed by the Further\nEducation Development Agency (FEDA) in conjunction with the British Educational\nCommunications and Technology Agency (Becta).\nIn this context, evaluation is crucial. It offers an opportunity to investigate what is actually\nhappening with the use of learning technologies and how the reality of the use maps onto\nthe potential.\nThis paper begins by briefly reviewing three recent approaches to evaluating ILT in HE. It\nthen reports on a project, funded by FEDA, to develop a tool for FE colleges to evaluate\ntheir use of teaching and learning technologies: the project was entitled Learning with ILT.\nThe paper discusses how the team that worked on this project approached the task, the\nprocess of developing the tool, issues that have arisen from the project and the implications\nfor evaluating teaching and learning technologies in FE.\nModels of evaluation of ILT\nThis section briefly considers three recent approaches to evaluating ILT: two approaches\nfor evaluating ILT in HE and the development of an evaluation toolkit for HE lecturers\nand then draws out some principles which were used in the Learning with ILT project.\nCIAO!\nThe CIAO! framework was developed at the Open University (OU) for evaluating the use\nof ILT in OU courses (Jones, Scanlon, Tosunoglu, Butcher, Greenberg, Murphy and Ross,\n1996). It has three main dimensions: context, interactions and outcomes, and is intended to\nprovide a variety of approaches to be considered for particular evaluations. The import-\nance of considering the whole learning experience is reflected in the dimension of context\nwhich includes, for example, how the ILT component fits within the course; the context of\nuse (at home, in a classroom, by individuals or groups) and how it is supported by human\nteaching. One important aspect of context is the designer's rationale in introducing the\ntechnology. Knowing the rationale enables us to decide on appropriate and achievable\nevaluation questions. For example, in a fourth-level resource-based social history course,\nthe rationale was to allow the student to 'be' a researcher by having access to data and\nresources on the CD-ROM. The course then needed to support students in using these\nresources. This was reflected in the evaluation which included questions about the student's\nexperience 'as a researcher', the integration of the media and navigation of the resource.\nThe interactions dimension refers to students' interactions with the software, which are\nimportant for understanding more about the learning processes. The outcomes dimension\nincludes students' perceptions and attitudes, which can be crucial in determining whether\ncomputer packages are ever used. A wide range of methods can be used to collect data on\n57\nAnn Jones et o\/ Evaluating learning and teaching technologies in further education\nthese different dimensions, including questionnaires, interviews, logs of computer usage\nand students' work.\nIntegrative evaluation\nThis model was developed by Draper, Brown, Henderson and McAteer (1996), members of\nthe TILT (Teaching with Independent Learning Technologies) project for evaluating a\ndiverse range of CAL in use in a university setting.1 It is a student-centred approach with\nan emphasis on observations in real classrooms, and aims to improve teaching and learning\nby enhancing the integration of ILT into the overall situation. It has an outer method and\nan inner method. The outer method starts with one or more meetings between evaluators,\nteachers and developers in order to gather information and establish parameters such as\nthe teachers' evaluation goals, the learning aims and objectives to be studied and other\nfeatures of the teaching situation.\nThe course material is then examined in order to identify where assessments, learning\nquizzes or other measures of learning gains could be incorporated and a design for the\nevaluation is finalized. The model emphasizes a cycle of teacher involvement and the joint\nestablishment of goals, aims and the other central features cited above. The inner method is\nabout the instruments to be used. \u2022 The range here is similar to that in the CIAO!\nframework, including, for example, questionnaires, observations, student confidence logs\nand interviews. Further information and discussion about these instruments and their use\ncan be found in Draper et al. (1996).\nELT project framework\nThe ELT project at UNL, funded by BP (Oliver and Conole, 1998) has developed a toolkit\nto guide staff through an evaluation. It has three main steps: selecting a methodology,\ngathering data and analysing data. For each of these, the proposed evaluation is reviewed\nin terms of criteria that help the user to decide which approach to adopt. The final toolkit\nincludes information on applying methods and pointers to literature that discusses their\nuse in evaluations and salient case studies. Again, the focus on resources, assessment\ncriteria, learning outcomes and background information on the courses and subjects is\nseen as important for inclusion in the evaluation tools, as is the need to involve\npractitioners.\nPrinciples for evaluation of ILT\nThese three models have been applied to very different HE contexts. However, they tend to\nagree on a number of points, which are supported in the literature on evaluating ILT and\nthe wider literature on educational evaluations.\n\u2022 Many factors are known to affect the successful use of ILT: as these are hard to\ndetermine in advance or to control, experimental approaches are not usually\nappropriate. (The issue of which approach to take is widely discussed in the literature;\nfor example, Gunn (1997), Jones, Scanlon and Blake (1998) and Draper (1997) discuss\nthe problems of trying to apply experimental approaches in this area whilst Oliver\n(1997) discusses the difficulties of comparative approaches).\n\u2022 It is important to determine the aims or rationale for the use of ILT in order to decide\nthe appropriate focus for the evaluation.\n58\nALT-J Volume 8 Number 3\n\u2022 Evaluation should involve the learners who are intending to use the technology, and\nshould take account of the whole learning situation. The context surrounding the use\nof the technology is crucial (Gunn, 1997; Phillips, 1988; and Jones, 1998).\n\u2022 Practitioners, teachers and designers should be involved throughout the process (see\nDraper, 1996; Phillips, 1988).\n\u2022 There should be ways (such as open-ended questions\/interviews) of finding out about\nunanticipated issues (Parlett and Hamilton, 1987).\nThese guidelines are supported by much of the literature, although, as in many areas, the\nliterature can be contentious. However, the team members were also drawing on their own\nexperience of evaluations over a considerable period of time. These became the principles\nthat informed the development and testing of the evaluation tools developed for use in FE\ncolleges in the Learning with ILT project.\nThe Learning with ILT project and ILT in FE\nThe project formed a third part of one of the QUILT programmes mentioned in the\nintroduction and was funded by FEDA. Two phases were already complete and included a\nliterature review, the initial development of a set of evaluation tools, visits to nine colleges\nand interviews with staff.\nThe aim of this project, the third phase, was to produce an evaluation tool that would\nprovide FE colleges with a mechanism for evaluating ILT projects. The brief was that this\nphase should build on phase 2 (which the project team had not been involved in) by\napplying the tools and methods to six to eight case studies. The timing and resourcing was\ntightly constrained: the project was to run over a year, to include several dissemination\nseminars and envisaged to take up to eighty days' work.\nThe research team that successfully tendered for the project was from the Institute of\nEducational Technology at the Open University. They had experience of evaluating policy\nand large-scale implementation of ILT and of research into attitudes towards new\ntechnologies (see, for example, Jones, Kirkup and Kirkwood, 1993; Calder, 1994) and of\nformative and summative evaluations of ILTs in context (for example, Scanlon, Tosunoglu,\nJones, Butcher, Ross, Greenberg, Taylor and Murphy, 1998). The team also included two\nresearchers who were not full-time university staff, with experience of teaching,\nmanagement and research into using ILT in FE.\nFE take up of ILT\nFollowing the Higginson report, the QUILT programme aimed to:\n\u2022 support change by individuals and institutions;\n\u2022 reach 50,000 staff in the FE sector and college governors;\n\u2022 provide activities from awareness raising to skills training.\nDespite such initiatives, by 1998 there was some evidence that the take up of ILT had not\nbeen as widespread as hoped and that the use of technology in colleges varied widely\nbetween curriculum areas (FEFC, 1998). During the 1980s and 1990s, much of the FE\ninvestment in technology had gone towards the building up of central resources (Gray and\n59\nAnn Jones et al Evaluating learning and teaching technologies in further education\nWarrender, 1995). However, a study by Barnard (1999) of FE biology teachers found that\nthe use of central resources made it difficult for them to use computers flexibly in planning\ntheir curriculum work and also led to access difficulties.\nGiven this apparent gap between the objectives for ILT and the reality in colleges,\nevaluation has a crucial role to play in finding out what is happening 'on the ground' and\ninforming future policy.\nDeveloping the evaluation materials\nChallenges and constraints\nThe main challenge facing the team was how to adapt our experience of evaluation and\nfindings from the literature to this particular context. The 'principles' discussed earlier\nwere all derived from evaluations carried out in real contexts, and were therefore followed\nas far as possible. However, in this project two particular criteria applied:\n\u2022 Firstly, the tools to be developed were to be used by FE practitioners - managers and\nteachers in FE colleges working in a pressured environment. Therefore we needed tools\nthat could be used 'off the shelf that did not require a high level of expertise in\nanalysis, and that did not take too much time to use and analyse.\n\u2022 Secondly, there was a wide range of college initiatives using ILT with very different\naims and objectives. The projects involved different subject areas and students of\ndifferent ages, experience and skill levels. The tools needed to be applicable to all of\nthese situations.\nTrying to work within these constraints raised some interesting tensions with the principles.\nThe first criterion was a guiding factor in the decision to base the evaluation tool on\nquestionnaires, and although these can have some limitations (for example, they tend to be\n'outcome-orientated') it was felt that they could provide the most flexible methods of\ngathering data. They could either 'stand alone' or be supported by other sources of\ninformation (such as course documents, test results, student logs\/diaries) which the\nrespondents might want to cite as evidence for their responses. Questionnaires were also\nlikely to be familiar to managers, lecturers and learners and would not require a high level\nof expertise to analyse.\nThe second constraint meant that the project team could not determine the aims of each\nILT project in order to decide where to focus the evaluation. Given our second principle -\n'It is important to determine the aims or rationale for the use of ILT in order to decide the\nappropriate focus for the evaluation' - this was potentially problematic. A tool was needed\nthat was generic enough to cope with very diverse situations, and yet could take account of\nthe particular context in which it was operating. The decision was therefore made to\ndevelop questionnaire templates that could be customized by the users for use in particular\nevaluations. An additional advantage was that those running the evaluation would need to\nconsider carefully the rationale for the project, and this activity, as well modifying the\nquestionnaire, should allow the evaluators themselves to take ownership of the tool.\nThe other evaluation principles discussed earlier (in the section on 'Principles for\nevaluation of ILT') fitted the context of the project well. It had already been decided that\nsix case study colleges would be involved, and this was consistent with evaluation involving\n60\nALT-J Volume 8 Number 3\nthe 'end users'. But this principle also emphasizes that evaluation should take account of\nthe whole learning situation, and the context surrounding the use of the technology.\nHaving customizable questionnaire templates allowed for the colleges' own issues to be\nconsidered, and open-ended questions allowed for the reporting of unanticipated issues.\nFE teachers and managers were involved throughout the process, by adopting an iterative\ncycle of development and modification.\nFinally, it was important that the tool should be used! Although questionnaires are\nreasonably quick and straightforward to use, this was an extra activity in an already full\ntimetable and was most likely to happen if it could also feed into an existing activity or\nserve another function. We therefore involved a self-inspection consultant who discussed\nhow the tool could help colleges provide evidence about their activities in this area in their\nself-inspection. Again, this was part of the cycle of development and modification, which is\ndiscussed following the next section.\nThe case study colleges and the projects\nSix colleges were involved in piloting the evaluation tools and each college was visited three\ntimes. The criteria for selecting the colleges were that they should be enthusiastic about\nparticipating and generally working well with ILT, as the tool was at an early stage of\ndevelopment. Within these constraints, the aim was to include colleges in different areas of\nthe country (England and Wales were included), of different size and type (including a\nsixth-form college) and in urban and rural locations.\nThe ILT projects in which the colleges were engaged also varied considerably. They\nincluded projects on online careers advice; resource based A-level delivery; the educational\nuse of IT for teachers; the use of authoring packages to put materials online; a multimedia\nkey skill project for disaffected 14-16-year-olds and staff development in ILT. The brief\nsketch below is included to give a flavour of two of the case study colleges:\nCollege A is a sixth-form college, with 2,000 students, mainly 16-19 years old. It has three\nteaching faculties - arts and communication, humanities and business studies, and\nmathematics, science and technology - and a strong commitment to resource-based\nlearning. Centralized computing facilities include a large learning resource centre and three\nsubject-specific resource bases staffed by library staff and one teacher from the appropriate\narea.\nThere are a number of large-scale cross-college ILT projects, two of which were used in\ntrialling the evaluation tools. The member of staff involved in the evaluation project was\nresponsible for the technical aspects of these projects. One project involved careers\nsoftware and was aimed at 700 first-year students on two-year A-level or advanced GNVQ\ncourses. The students were to work in tutor group sessions, as part of an eight-week series\nof careers sessions, and the software was designed to replace a one-hour-per-week session\nby the careers staff. The second project provided Internet access to 100 staff machines and\n300 student machines. .The member of staff responsible was keen to evaluate the impact of\nthis policy decision on students' work practices.\nCollege B is one of the largest FE providers in its region, with four sites, 5,000 students and\n500 staff. Its wide range of courses includes leisure, construction, art .and design,\nengineering, business management, and manufacturing technology. It has a number of\n61\nAnn Jones et ol Evaluating learning and teaching technologies in further education\nsubject-based workshops and a general IT support unit, available to students on a drop-in\nbasis.\nHere the selected project was a multimedia skills project for disaffected 14-16 year olds\nwho attended school but spent one day a week at the college for their GNVQ Foundation\nCourse in Manufacturing. About fourteen students were involved and they used key skills\nmaterial which had been developed from paper-based versions by an authoring technician\nusing Authorware Version 5.\nInvolving the practitioners: cycles of development and modification\nThe approach adopted drew on a multi-phase iterative model of formative evaluation, that\nhad been applied to the development of stand-alone teacher training materials (Jones,\nKirkup and Kirkwood, 1993). It also drew on the model that FEDA had previously\napplied, in particular the use of consultation and dissemination seminars.\nThree customizable questionnaire templates were developed: for managers, for lecturers\nand for students\/learners. These would allow the evaluators to collect information from the\nmajor stakeholders in the projects and also to triangulate the feedback in order to increase\nconfidence in the findings.\nThe project had three phases, each of which included visits by the researchers to the\ncolleges, at Which the colleges fed back and discussed their use of and reactions to the\nquestionnaires. In the first phase an initial draft of the tool was developed. Following an\ninitial visit to the colleges to discuss the ILT projects where the tool would be used,\nfeedback was obtained on the tool from the colleges and from FEDA. In the second phase,\nthe tool was modified following feedback on the draft tool and given to colleges to trial.\nFeedback from the trials was obtained at a second visit to the colleges and a consultation\nseminar was held to which all the participating colleges were invited. An interim report was\nproduced during this phase and again further modifications were made to the tool. In the\nthird and final phase, the colleges used the modified tools and the data (and the colleges'\nanalyses) was discussed at the final visits. Following analysis of the trials, a draft final\nreport was written and this was sent to participants at the dissemination seminar, who were\ninvolved in relevant work in both HE and FE. The dissemination seminar allowed the\nproject to be presented and provided feedback from the participants.\nFindings from the development of the evaluation tool\nThe analysis framework and briefing sheet\nIt became clear from the second visits that the three separate strands of the evaluation\nprocess needed more coherence. An analysis template was therefore developed so that the\nthree questionnaires (learner, manager, and lecturer) could be compared and critical areas\ncould be highlighted and cross-referenced.\nThe second visits also revealed the need for one individual, the manager, to take\nresponsibility for customizing and administering the questionnaires and analysis of the\nresponses. The manager's briefing sheet was developed and outlined the best way to\napproach the analysis through analysis of the learner questionnaire first and then analysis\nof the lecture and manager questionnaires together so that learner responses could\nilluminate the staff responses. The study participants were asked to trial the analysis\n62\nALT-J Volume 8 Number 3\ntemplate before the third visit and their comments were on the whole very positive. Several\nwere surprised at the patterns that emerged and felt that these allowed them to propose\nimportant modifications to their ILT projects. The overall message was that the template\nwas robust as it stood - and that problems specific to individual projects should be\naddressed by customizing.\nOther feedback from stage three\nThe intention was that the colleges would initially use the questionnaires in the second\nphase of the project, but at this point, only two projects were sufficiently advanced to be\nevaluated, and the analysis framework and the briefing sheet had not been developed. So\nwhilst the feedback from phase two was useful, it was not very detailed. Full feedback and\nanalysis had to wait until phase three. The rest of this section outlines further findings\nfrom this phase of the project.\nUsing the generic templates and customization\nThe challenge of using a generic questionnaire to probe into specific situations, persisted,\nboth for the developers and users. For example, the first section in the learner\nquestionnaire included questions about the project (for example, its purpose and how it\nfitted in). There were problems in formulating questions on 'outcomes' for this section, and\nthese stemmed from the very variable aims of the colleges' ILT projects, which meant that\nit was difficult to ask specific relatively closed questions about the students' learning\nexperience. Some of the feedback on the learner questionnaire in phase two had suggested\nthat more questions should be included that related specifically to student learning. These\nwere introduced in phase three and included, for example, a question that asked\nrespondents whether 'system X' had helped them to find useful information or practise\nuseful.skills. The intention here, as explained in the guidance notes, was that the term\n'system X' would be replaced by the name of the appropriate software or project. However,\nthe feedback revealed that very few of the colleges customized their templates to do this.\nMost colleges used the questionnaires as they were and so in these cases this question still\nreferred to 'system X'.\nIn many instances, the lack of customization was not problematic and the generalized\nquestions addressed the information that the colleges needed. However, in two cases the\nparticipants realized when they reached the analysis stage that the key questions had not\nbeen asked. This highlights the importance of customizing - in particular with the learner\nquestionnaire - so that colleges can address the specific aims and objectives of their\nprojects. As a result, the final manual outlined the importance of customization and, in\nparticular, of ensuring that the questionnaire reflected the specific aims of the project\nbeing evaluated.\nAnother finding related to this issue, was that colleges had different experiences in using\nthe questionnaire: sections and approaches that were generally found helpful were\noccasionally seen as having limited importance by particular colleges (for example, access)\nas they were not sufficiently salient to the project or the college. Again, customization\nwould address this.\nDifferent stakeholders' experiences\nThe manager questionnaires were completed thoroughly and they reported that they had\nfound the process of thinking through the strategic aims salutary - often projects had been\n63\nAnn Jones et al Evaluating learning and teaching technologies in further education\nintroduced so quickly that teams had not had time to reflect on exactly what they were\ntrying to achieve. By contrast, lecturer responses varied considerably in their quality and\nreflected some resistance to using ILT. It seemed that those questionnaires that were com-\npleted more thoroughly reflected good communication and\/or promoted good commun-\nication between managers and learners, and a shared ownership of projects. One manager\nin particular noted that the evaluation process had been invaluable in acting as a\nframework to support discussion and develop coherence within his team. Students some-\ntimes had difficulties with answering questions about the purpose of the ILT activities. The\nlecturers had also had similar difficulties in phase two of the project where they had left the\nstrategic aims section blank in their questionnaires. In the final version, in section three,\nrather than asking what the strategic aims were, it was decided to ask whether they had\nseen the strategic aims. This also yielded information about their involvement in and\nownership of the project.\nDiscussion and implications\nAs indicated above, the anticipated customization did not happen. Time may have been a\nfactor: it was a major constraint and throughout the project the team was aware that the\ncolleges were finding it difficult to keep up with their planned implementation timetables.\nAlso, the lack of customization may reflect the fact that the same individuals were involved in\nsuccessive trails and on the first round we had asked them not to customize the templates.\nBoth lecturers and learners were at times unsure about the purpose of the projects they\nwere evaluating, and, as discussed earlier, we could not include questions about each\nproject's particular objectives because each was different. Three factors may be relevant to\nthe lecturers' difficulties in answering questions about the aims of the project. First, there\nwas some feedback that the design of the questionnaire needed modification and it was\nmodified during the project. Secondly, such problems sometimes reflected the lecturers'\nlack of ownership and understanding of the projects particularly where respondents had\njoined the project once it was under way and had not been involved with setting it up.\nThirdly, a few lecturers appeared to have more general difficulties in articulating aims and\nobjectives.\nThe process of trialling revealed the need for the person overseeing the evaluation process\nto be responsible for customizing the questionnaires and to have a strong awareness of the\naims and objectives of the project.\nThe feedback from the colleges strongly supports the principle that users must be involved\nin the development cycle. Without such an involvement, crucial information such as the\ndifficulties that colleges had with determining the aims and objectives of the projects and\nthe importance of customization (and that colleges often did not readily do this) would\nhave been missed. There were also a number of significant changes between the first and\nfinal drafts of the learner questionnaire templates, which supports the importance of\niterative trialling in this way, and the final version of the manual stressed that customizing\nis crucial to the success of the evaluation procedure.\nConclusions\nWhen new approaches to teaching are adopted, evaluation is particularly important and\nalthough it has often been an 'afterthought' in the past, evaluation is increasingly viewed as\n64\nALT-J Volume 8 Number 3\npart of any new project. Other changes are also under way. Whereas an investigation of an\ninnovative project may previously have been carried out by outside evaluators, with the\ncurrent emphasis on quality assessment, teachers in post-compulsory education are likely\nto be increasingly engaged in evaluation. It is likely that such teacher-evaluators, unlike\nexternal evaluators, will not have any appropriate research or evaluation experience and are\ncertainly likely to be short of time.\nThe Learning with ILT project's aim was to develop a generic evaluation tool that could be\nused in these circumstances. Whilst the constraints were challenging, it did prove possible\nto follow the guidelines outlined earlier and to produce a tool that was usable. However, we\nwould emphasize the need for tailoring if a generic tool is to be used successfully in very\ndifferent contexts.\nIt is also important that the need for support when starting to use evaluation tools such as\nthis should not be underestimated. The case study colleges were keen to take part, and had\nthe 'spur' of visits from the researchers to keep them going. Even so, time was a major\nconstraint, so it would be very easy for colleges that did not have the process timetabled in\nthis way to let other priorities take over. Guidance in using the evaluation tool as part of\nthe self-inspection process was important in providing a rationale and motivation for using\nthe tool. It is too early to know whether colleges will take on board such evaluation\nprocesses once they are on their own, but it is clear that having another reason for doing\nthe evaluation is particularly important when resources are limited.\nAcknowl edgem ents\nWe would like to acknowledge the input of all the members of the team who were involved\nat various stages of the project: Andrew Morris, Tony Tait, Kevin Donovan and Jill\nAttewell from FEDA, and Ann Jones, Jane Barnard, Judith Calder, Eileen Scanlon, Julie\nThompson and Mary Thorpe from the Open University. FEDA funded the project and\nhave published a manual based on the project outcomes entitled 'Evaluating ICT Projects\nand Strategies in Teaching and Learning' (in August 2000).\nWe would particularly like to thank the colleges involved who gave us their reflections, time\nand access to their staff on top of their busy workloads. We also very much appreciated the\ninput of the dissemination seminar participants who gave us very helpful comments and\nfeedback which are reflected in the final report (see above), and Pam Frame for discussing\nthe role of the evaluation tools in the self-assessment process.\nNote\n1\n The TILT project was funded through the TLTP (Teaching and Learning Technology\nProgramme) by the UK university funding bodies and also by the University of Glasgow.\nReferences\nBarnard, J. (1999), 'Computers in FE Biology: a study of how teachers' classroom practice\ncan be affected by different software', Educational Technology and Society, 2 (4).\nCalder, J. (1994), Programme Evaluation and Quality: A Comprehensive Guide to Setting up\nan Evaluation System, Kogan Page\/Open University.\n65\nAnn Jones et al Evaluating learning and teaching technologies in further education\nDraper, S. W. (1997), 'Prospects for summative evaluation of CAL in higher education',\nALT-J, 5 (1), 33-9.\nDraper, S. W., Brown, M. I., Henderson, F. P. and McAteer, E. (1996), 'Integrative\nevaluation: an emerging role for classroom studies of CAL', Computers in Education, 1-3,\n17-32.\nGray, L. and Warrender, A. (1995), Learning and Technology in Further Education Colleges,\nFEDA.\nGunn, C. (1997), 'CAL evaluation: future directions', ALT-J, 5 (1), 40-7.\nHigginson, G. (1996), Report of the Learning and Technology Committee, Further\nEducation Funding Council.\nJones, A., Kirkup, G. and Kirkwood, A. (1993), Personal Computers for Distance Learning,\nPaul Chapman.\nJones, A., Scanlon, E., Tosunoglu, C., Butcher, P., Greenberg, J., Murphy, P. and Ross, S.\n(1996), 'Evaluating CAL at the Open University: 15 years on', Computers and Education,\n26 (1-3), 5-15.\nJones, A., Scanlon, E. and Blake, C. (1998), 'Reflections on a model for evaluating learning\ntechnologies', in M. Oliver (ed.), Innovation in the Evaluation of Learning Technology,\nLondon: University of North London, 25-41.\nJones, C. (1998) 'Evaluation using ethnography: context, content and collaboration', in\nM. Oliver (ed.), Innovation in the Evaluation of Learning Technology, London: University\nof North London, 87-100.\nOliver, M. and Conole, G. (1998), 'Evaluating communication and information\ntechnologies: a toolkit for practitioners', Active Learning, 8, July, 3-8.\nOliver, M. (1997), 'A framework for evaluating the use of educational technology', BP ELT\nReport No. 1, Learning and Teaching Innovation and Development (LaTID), London:\nUniversity of North London.\nPhillips, R. (1988), 'ITMA's approach to classroom observation', in A. Jones and\nP. Scrimshaw (eds.), Computers in Education, Milton Keynes: Open University Press, 5-13.\nScanlon, E., Tosunoglu, C., Jones, A., Butcher, P., Ross, S., Greenberg, J., Taylor J. and\nMurphy, P. (1998), 'Learning with computers: experiences from an evaluation project',\nComputers and Education, 30 (1\/2), 9-14.\nThe Stationery Office (1998), The Learning Age: A Renaissance for a New Britain, UK\nGovernment Green Paper.\n66\n"}