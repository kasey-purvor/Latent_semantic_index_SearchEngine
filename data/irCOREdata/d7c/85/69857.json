{"doi":"10.1016\/j.csda.2009.02.024","coreId":"69857","oai":"oai:eprints.lancs.ac.uk:20799","identifiers":["oai:eprints.lancs.ac.uk:20799","10.1016\/j.csda.2009.02.024"],"title":"Structural components in functional data.","authors":["Park, Juhyun","Gasser, Theo","Rousson, Valentin"],"enrichments":{"references":[{"id":16337639,"title":"Analysis of linear growth using a mathematical model ii. from 3 to 21 years of age.","authors":[],"date":"1987","doi":"10.1111\/j.1651-2227.1987.tb17122.x","raw":"Karlberg, J., J. G. Fryer, I. Engstr\u00a8 om, and P. Karlberg (1987). Analysis of linear growth using a mathematical model ii. from 3 to 21 years of age. Acta Paediatrica Scandinavica 337, 12\u201329.","cites":null},{"id":16337645,"title":"Applied Functional Data Analysis: Methods and Case Studies.","authors":[],"date":"2002","doi":"10.1007\/b98886","raw":"Ramsay, J. and B. Silverman (2002). Applied Functional Data Analysis: Methods and Case Studies. Springer: New York.","cites":null},{"id":16337629,"title":"Asymptotic theory for the principal component analysis of a vector random function: Some applications to statistical inference.","authors":[],"date":"1982","doi":"10.1016\/0047-259x(82)90088-4","raw":"Dauxois, J., A. Pousse, and Y. Romain (1982). Asymptotic theory for the principal component analysis of a vector random function: Some applications to statistical inference. Journal of multivariate analysis 12, 136\u2013154.","cites":null},{"id":16337635,"title":"Criteria for evaluating dimension-reducing components for multivariate data.","authors":[],"date":"2004","doi":"10.1198\/0003130042863","raw":"Gervini, D. and V. Rousson (2004). Criteria for evaluating dimension-reducing components for multivariate data. The American Statistician 58, 72\u201376.","cites":null},{"id":16337672,"title":"Functional data analysis for sparse longitudinal data.","authors":[],"date":"2005","doi":"10.1198\/016214504000001745","raw":"Yao, F., H. G. M\u00a8 uller, and J. L. Wang (2005). Functional data analysis for sparse longitudinal data. J. Am. Statist. Ass. 100, 577\u2013590. 192 7 12 17 21 age (years) h e i g h t 2 7 12 17 21 age (years) v e l o c i t y","cites":null},{"id":16337643,"title":"Functional Data Analysis.","authors":[],"date":"1997","doi":"10.1007\/978-1-4757-7107-7","raw":"Ramsay, J. and B. Silverman (1997). Functional Data Analysis. Springer\u2013Verlag: New York.","cites":null},{"id":16337624,"title":"Kernel-based functional principal components.","authors":[],"date":"2000","doi":"10.1016\/s0167-7152(00)00014-6","raw":"Boente, G. and R. Fraiman (2000). Kernel-based functional principal components. Statistics & Probability Letters 48, 335\u2013345.","cites":null},{"id":16337647,"title":"Linear Statistical Inference and its Applications.","authors":[],"date":"1968","doi":"10.2307\/1907503","raw":"Rao, C. R. (1968). Linear Statistical Inference and its Applications. Wiley: New York.","cites":null},{"id":16337665,"title":"Local polynomial mixed-e\ufb00ects models for longitudinal data.","authors":[],"date":"2002","doi":"10.1198\/016214502388618672","raw":"Wu, H. and J. Zhang (2002). Local polynomial mixed-e\ufb00ects models for longitudinal data. J. Am. Statist. Ass. 97(459), 883\u2013897.","cites":null},{"id":16337626,"title":"Modelization, nonparametric estimation and prediction for continuous time processes.","authors":[],"date":"1991","doi":"10.1007\/978-94-011-3222-0_38","raw":"Bosq, D. (1991). Modelization, nonparametric estimation and prediction for continuous time processes. In G. Roussas (Ed.), Nonparametric Function Estimation and Related Topics, Nato Asi series, pp. 509\u2013529. Klewer Academic: Dordrecht.","cites":null},{"id":16337631,"title":"Nonparametric estimation of covariance structure in longitudinal data.","authors":[],"date":"1998","doi":"10.2307\/3109751","raw":"Diggle, P. J. and A. P. Verbyla (1998). Nonparametric estimation of covariance structure in longitudinal data. Biometrics 54, 401\u2013415.","cites":null},{"id":16337627,"title":"Nonparametric estimation of smoothed principal components analysis of sampled noisy functions.","authors":[],"date":"2000","doi":"10.1080\/10485250008832820","raw":"Cardot, H. (2000). Nonparametric estimation of smoothed principal components analysis of sampled noisy functions. Journal of Nonparametric Statistics 12, 503\u2013 538.","cites":null},{"id":16337633,"title":"Nonparametric regression analysis of growth curves.","authors":[],"date":"1984","doi":"10.1214\/aos\/1176346402","raw":"Gasser, T., H. G. M\u00a8 uller, W. K\u00a8 ohler, L. Molinari, and A. Prader (1984). Nonparametric regression analysis of growth curves. Ann. Statist. 12, 210\u2013229.","cites":null},{"id":16337657,"title":"Nonparametric regression analysis of longitudinal data.","authors":[],"date":"1998","doi":"10.2307\/2670055","raw":"Staniswalis, J. and J. Lee (1998). Nonparametric regression analysis of longitudinal data. J. Am. Statist. Ass. 93, 1403\u20131418.","cites":null},{"id":16337637,"title":"On the modeling of human growth.","authors":[],"date":"1987","doi":"10.1002\/sim.4780060210","raw":"Karlberg, J. (1987). On the modeling of human growth. Statist. Med. 6, 185\u2013192.","cites":null},{"id":16337660,"title":"Shape-invariant modeling of human growth.","authors":[],"date":"1980","doi":"10.1080\/03014468000004641","raw":"St\u00a8 utzle, W., T. Gasser, L. Molinari, R. H. Largo, A. Prader, and P. J. Huber (1980). Shape-invariant modeling of human growth. Annals of Human Biology 7, 507\u2013528.","cites":null},{"id":16337668,"title":"Shrinkage estimation for functional principal component scores with application to the population kinetics of plasma folate.","authors":[],"date":"2003","doi":"10.1111\/1541-0420.00078","raw":"Yao, F., H. G. M\u00a8 uller, A. J. Cli\ufb00ord, S. R. Dueker, J. Follett, Y. Lin, B. A. Buchholz, and J. S. Vogel (2003). Shrinkage estimation for functional principal component scores with application to the population kinetics of plasma folate. Biometrics 59, 676\u2013685.","cites":null},{"id":16337650,"title":"Simple component analysis.","authors":[],"date":"2004","doi":"10.1002\/bimj.200490287","raw":"Rousson, V. and T. Gasser (2004). Simple component analysis. Appl. Statist. 53, 539\u2013555.","cites":null},{"id":16337654,"title":"Smoothed functional principal component analysis by choice of norm.","authors":[],"date":"1996","doi":"10.1214\/aos\/1033066196","raw":"Silverman, B. W. (1996). Smoothed functional principal component analysis by choice of norm. Ann. Statist. 24, 1\u201324.","cites":null},{"id":16337641,"title":"Statistical tools to analyze data representing a sample of curves.","authors":[],"date":"1992","doi":"10.1214\/aos\/1176348769","raw":"18Kneip, A. and T. Gasser (1992). Statistical tools to analyze data representing a sample of curves. Ann. Statist. 20, 1266\u20131305.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2009-07-01","abstract":"Analyzing functional data often leads to finding common factors, for which functional principal component analysis proves to be a useful tool to summarize and characterize the random variation in a function space. The representation in terms of eigenfunctions is optimal in the sense of L2 approximation. However, the eigenfunctions are not always directed towards an interesting and interpretable direction in the context of functional data and thus could obscure the underlying structure. To overcome such difficulty, an alternative to functional principal component analysis is proposed that produces directed components which may be more informative and easier to interpret. These structural components are similar to principal components, but are adapted to situations in which the domain of the function may be decomposed into disjoint intervals such that there is effectively independence between intervals and positive correlation within intervals. The approach is demonstrated with synthetic examples as well as real data. Properties for special cases are also studied","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/69857.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/20799\/1\/sca_Jul07.pdf","pdfHashValue":"25759d958abb62a8b22707caf9ce627d0c82b836","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:20799<\/identifier><datestamp>\n      2018-01-24T02:34:57Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Structural components in functional data.<\/dc:title><dc:creator>\n        Park, Juhyun<\/dc:creator><dc:creator>\n        Gasser, Theo<\/dc:creator><dc:creator>\n        Rousson, Valentin<\/dc:creator><dc:subject>\n        QA Mathematics<\/dc:subject><dc:description>\n        Analyzing functional data often leads to finding common factors, for which functional principal component analysis proves to be a useful tool to summarize and characterize the random variation in a function space. The representation in terms of eigenfunctions is optimal in the sense of L2 approximation. However, the eigenfunctions are not always directed towards an interesting and interpretable direction in the context of functional data and thus could obscure the underlying structure. To overcome such difficulty, an alternative to functional principal component analysis is proposed that produces directed components which may be more informative and easier to interpret. These structural components are similar to principal components, but are adapted to situations in which the domain of the function may be decomposed into disjoint intervals such that there is effectively independence between intervals and positive correlation within intervals. The approach is demonstrated with synthetic examples as well as real data. Properties for special cases are also studied.<\/dc:description><dc:date>\n        2009-07-01<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/20799\/1\/sca_Jul07.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1016\/j.csda.2009.02.024<\/dc:relation><dc:identifier>\n        Park, Juhyun and Gasser, Theo and Rousson, Valentin (2009) Structural components in functional data. Computational Statistics and Data Analysis, 53 (9). pp. 3452-3465. ISSN 0167-9473<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/20799\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1016\/j.csda.2009.02.024","http:\/\/eprints.lancs.ac.uk\/20799\/"],"year":2009,"topics":["QA Mathematics"],"subject":["Journal Article","PeerReviewed"],"fullText":"Structural Components in Functional Data\nJuhyun Park\u2217, Theo Gasser and Valentin Rousson\nDepartment of Biostatistics\nUniversity of Zu\u00a8rich\nJuly 24, 2007\nAbstract\nAnalyzing functional data often leads to finding common factors, for which\nfunctional principal components analysis proves to be a useful tool to summa-\nrize and characterize the random variation in a function space. The represen-\ntation in terms of eigenfunctions is optimal in the sense of L2 approximation.\nHowever, the eigenfuntions are not always directed towards an interesting and\ninterpretable direction in the context of functional data and thus could obscure\nthe underlying structure. This paper proposes an alternative to functional\nprincipal component analysis that produces directed components which may\nbe more informative and easier to interpret. These structural components are\nsimilar to principal components, but are adapted to situations in which the do-\nmain of the function may be decomposed into disjoint intervals such that there\nis effectively independence between intervals and positive correlation within\nintervals. The approach is demonstrated with examples as well as real data.\nProperties for special cases are also studied.\nKeywords: Functional data analysis, Functional principal component anal-\nysis, PCA, Longitudinal data, Smoothing\n1 INTRODUCTION\nRepeated measurements in the form of curves are increasingly common in the fields\nof biomedicine and physical sciences. Examples include blood pressure profiles over\n24 hours, evoked brain potentials and growth curves. Individual measurements are\n\u2217Address for correspondence: Juhyun Park, Department of Mathematics and Statistics, Lancaster\nUniversity, Lancaster, LA1 4YF, U.K. Email: juhyun.park@lancaster.ac.uk\n1\ntaken at consecutive time points and repeatedly observed for different subjects. Such\nmeasurements are generally called functional data.\nFigure 1 about here.\nAn example of children growth data is shown in Figure 1. These are height\nmeasurements (left) and velocity curves (right) of 10 boys at ages between 2 and\n21 years (Gasser et al., 1984). Velocity curves are estimated nonparametrically with\nGasser-Mu\u00a8ller estimator. In order to perform a functional data analysis, these curves\nwill be aligned to eliminate time variability, known as registration. In this paper, we\nshall assume that registration, if necessary, has been carried out and we shall treat\nthe registered curves as raw data. For details on registration, we refer to Ramsay and\nSilverman (1997, 2002).\nUsually the sample of curves is assumed to have some homogeneous structure\nin the functional shape, while allowing for individual variability. This variability is\ncommonly charaterized with a few components. Let yij be the jth observation on the\nith individual at time tj and consider the following regression model\nyij \u2261 yi(tj) = \u00b5(tj) +\nK\u2211\nk=1\n\u03beik\u03c6k(tj) + \u000fij , i = 1, \u00b7 \u00b7 \u00b7 , n; j = 1, \u00b7 \u00b7 \u00b7 , T ,\nwhere \u000fij are independent errors. Here the design points are assumed the same for\neach subject. This is not an essential assumption but it will simplify presentation in\nSection 2. From now on, such \u03c6ks are referred to as components.\nWhen viewing each curve as an independent realization of a stochastic process\nX(t) in function space, the model has an optimal representation when the common\nfunctions {\u03c6k} and the coefficients {\u03beik} are derived from eigenfunctions and eigen-\nvalues of the covariance function of X. These are known as functional principal\ncomponents. Such models appear in the context of both longitudinal and functional\ndata analysis. In longitudinal data analysis, the random coefficients \u03beik are often\nassociated with random effects with known covariates and may have sparse design\npoints. See also Yao et al. (2005). While the components are prespecified in the lon-\ngitudinal models, they may be completely determined by the data in functional data\nanalysis. However, situation may arise when some restriction would be useful without\nfully specifying the components. In particular, when some variability shows certain\nstructure along the time axis, it is beneficial to have components that reflect such\na phenomenon. For example, whether the growth process experiences a qualitative\nchange in time would be of interest.\nWe mainly focus on nonnegative covariance functions, although in practice we\nwould tolerate small negative entries to extract the essential features of a basically\npositive relationship.\n2\nFigure 2 about here.\nConsider a simple scenario with two underlying subprocesses that have almost\nnon\u2013overlapping support as shown in Figure 2. Assume that a sample of curves is\ngenerated from the sum of these processes when 1) the two processes are independent\nand when 2) they are positively highly correlated. In the latter case, it can be viewed\nas being one homogeneous process.\nFigure 3 about here.\nFigure 3 shows the result of functional PCA for these two cases. The left column\ncorresponds to the nearly uncorrelated case and the right column to the positively\ncorrelated case. The first components shown in the top row both suggest an overall\nlevel of the process as major source of variability (in our terminology these are block\ncomponents) and the second components, in the second row, suggest the difference\nbetween the first and the second processes as the next source (we shall call these\ndifference components). Although the percentage of variance differs, functional PCA\nprovides qualitatively the same answer whether there are two independent subpro-\ncesses or only one process.\nFigure 4 about here.\nOn the other hand, the covariance structure, seen as contour plots in top pan-\nels of Figure 4, clearly indicate that the observed process is approximated by two\nsubprocesses in the former case and by one process in the latter case. Below are the\ncorresponding correlation functions. Note that the minimum correlation for the latter\nis 0.69, compared to 0.2 for the former.\nIn this paper, we shall define structural components to reflect this information\nboth quantitatively and qualitatively. We introduce block components and difference\ncomponents to make a distinction between the two cases. In Section 2, we introduce\na general framework to obtain block components that reflect the underlying structure.\nWe use the simulated examples to illustrate the procedure. Statistical properties are\nstudied in Section 3. Numerical performance is evaluated in Section 4. In particular,\ncomparison to functional PCA and its Varimax rotation is made in simulation studies.\nApplication to real data is also included\n2 METHODOLOGY\nConsider a stochastic process X(t) with compact support T = [0, T ]. Denote the\nmean function by \u00b5(t) and the covariance function by \u03b3(s, t) = cov(X(s), X(t)) and\nassume that\n3\n(A1) \u03b3(s, t) \u2265 0 for all s, t \u2208 T .\nProcesses with predominantly positive covariances are frequent in functional data and\nin many cases this property should be satisfied to a good approximation.\nWe shall decompose a stochastic process into a system of components \u03b21, \u00b7 \u00b7 \u00b7 ,\u03b2q\nas in the regression model of Section 1. Similar in spirit to decomposition of analy-\nsis of variance, where major variability is captured by main effect and contrasts, we\nmodel the process with block components and difference components. These consti-\ntute structural components in our model. Structural components were introduced in\nthe multivariate context by Rousson and Gasser (2004). Formal definitions will be\nintroduced below.\n2.1 Block and difference components\nA component is called a block component if \u03b2(t) \u2265 0 (or \u03b2(t) \u2264 0) for all t \u2208 T , the\ndomain where it is strictly positive (strictly negative) being connected. A difference\ncomponent is an element of nonblock components, i.e. where the sgn(\u03b2) is not\nconstant. A simple example of difference component is seen when the domain where\nit is strictly positive is connected and when the domain where it is strictly negative is\nconnected. For identifiability we assume that\n\u222b\n\u03b22(t) dt = 1. We are mainly interested\nin deriving block components but difference components could be of further interest.\nFor example, we would like to have two block components in the case of a sum of\ntwo uncorrelated subprocesses, but to have only one block component in the case of\na sum of two highly correlated subprocesses (see Figure 2).\n2.2 Correlation between components\nEach component function is associated with a random variable Xk =\n\u222b\n\u03b2k(t)X(t) dt.\nWe measure the correlation between two components \u03b2k and \u03b2l by the random vari-\nables induced by them. Define\nCorr(\u03b2k,\u03b2l) =\n\u222b \u222b\n\u03b2k(s)\u03b3(s, t)\u03b2l(t) ds dt\u221a\u222b \u222b\n\u03b2k(s)\u03b3(s, t)\u03b2k(t) ds dt\n\u221a\u222b \u222b\n\u03b2l(s)\u03b3(s, t)\u03b2l(t) ds dt\nThus, two components are said to be uncorrelated if\n\u222b\n\u03b2k(s)\u03b3(s, t)\u03b2l(t) ds dt = 0.\nOn the other hand, two components are said to be orthogonal if\n\u222b\n\u03b2k(t)\u03b2l(t) dt = 0.\nFunctional principal components is the only system of components which is orthog-\nonal and uncorrelated. In our approach, components may be non-orthogonal and\/or\ncorrelated. To avoid that components share too much information, we shall concen-\ntrate on systems where the maximal correlation\nC = max\nk 6=l\nCorr(\u03b2k,\u03b2l)\n4\nis smaller than some threshold Cmax.\n2.3 Variance extracted by a system of components\nWe assess optimality of components by corrected sum of variances explained by com-\nponents. Following Gervini and Rousson (2004), we consider\nV ar(\u03b21) +\nq\u2211\nk=2\nV ar(\u03b2k|\u03b21, \u00b7 \u00b7 \u00b7 ,\u03b2k\u22121) ,\nwhere\nV ar(\u03b2k|\u03b21, \u00b7 \u00b7 \u00b7 ,\u03b2k\u22121) =\n\u222b \u222b\n\u03b2k(s)\u03b3(k\u22121)(s, t)\u03b2k(t) ds dt .\nHere \u03b3(k\u22121) is the residual covariance function subtracting linear prediction in terms\nof (\u03b21, \u00b7 \u00b7 \u00b7 ,\u03b2k\u22121). Normalization with respect to principal components \u03c61, \u00b7 \u00b7 \u00b7 , \u03c6q\nmeasures relative loss of optimality:\nO =\nV ar(\u03b21) +\n\u2211q\nk=2 V ar(\u03b2k|\u03b21, \u00b7 \u00b7 \u00b7 ,\u03b2k\u22121)\nV ar(\u03c61) +\n\u2211q\nk=2 V ar(\u03c6k|\u03c61, \u00b7 \u00b7 \u00b7 , \u03c6k\u22121)\n.\nThis criterion has been introduced in the multivariate context and it has been shown\nthat O \u2264 1, equality holding if and only if the \u03b2k are principal components. It is\ndesigned for penalizing systems of components which are correlated. The \u201cprice to\npay\u201d for replacing principal components with a suboptimal system which is better\ninterpretable, for example, a system with more than one block component, can then\nbe quantified. This criterion hence allows to compare different candidate component\nmodels. Note that this criterion is not symmetric with respect to the order of com-\nponents. For structural components, we order first block components by decreasing\nvariance, and then difference components by decreasing variance.\n2.4 Problem statement\nNow we can state our problem formally. We are interested in a system \u03b21, \u00b7 \u00b7 \u00b7 ,\u03b2q\nwhich maximizes\n(1) the number of block components Nb\n(2) the optimality criterion O\nunder the constraint that the maximum correlation C \u2264 Cmax. Thus, if there is no\nsystem with Nb > 1 and C \u2264 Cmax, then the solution to this problem is given by\n5\nprincipal components. Otherwise the solution is in general different from principal\ncomponents.\nIn subsequent sections, a stepwise approach for estimating the components from a\nsample of curves is proposed. These are based on the covariance function estimator.\n2.5 Estimation of covariance function\nEstimating the covariance function \u03b3 is closely related to estimating functional prin-\ncipal components. Diggle and Verbyla (1998), Staniswalis and Lee (1998), Wu and\nZhang (2002) and Yao et al. (2003) among many others have used kernel type smooth-\ning estimator. Alternatively, a prespecified basis function can be used as in Silverman\n(1996). The differences center around the different smoothing techniques adopted.\nOur approach can encompass different versions of covariance function estimators\nand the subtle differences would not be a major issue for the purpose of our analysis.\nThe common mean function is estimated simply by taking the average of the data\nat design points without smoothing followed by subtraction from data points. The\nremaining curve corresponds to subject specific variation, subject to a measurement\nerror. At this stage individual (kernel) smoothing is applied before computing the\ncovariance function. To avoid any systematic bias caused by different smoothing\nparameters, the same bandwidth is used. Our estimator of the covariance function is\nthe covariance function of the smoothed residual process.\n2.6 Estimating block components\nIn this section, we explain how to estimate the block components, as well as their\nnumber Nb. For this, we first consider all possible partitions of [0, T ] into two disjoint\nintervals [0, t] and [t, T ]. Let \u03b1t and \u03b2t be the leading eigenfunctions of the covari-\nance functions restricted on these intervals. In general, these functions are positive,\notherwise, we approximate \u03b1t and \u03b2t to be positive by setting zero where they are\nnegative so that we extract a basically positive relationship. We extend \u03b1t and \u03b2t\nto the whole interval [0, T ] by adding zero on the other interval. These functions\nmay have discontinuity at t, which is of measure zero. For each separation point t,\nwe evaluate the criteria C and O, obtaining functions C(t) and O(t). If the corre-\nlation C(t) is larger than Cmax throughout T , the solution to our problem stated in\nSection 2.4 is given by the principal components. Otherwise, we consider the two\nblock components \u03b1t, \u03b2t that maximize O(t) under the domain where C(t) \u2264 Cmax.\nAlternatively, we may choose the separation point t using specific knowledge from\nthe field of application (some values may make more \u201csense\u201d than others). We then\ntry to split the block components obtained into two further block components using\na similar algorithm. This sequential approach ends when it is no longer possible to\n6\nsplit the block components without surpassing the correlation threshold. In practice,\nthe algorithm often stops with Nb = 1 or Nb = 2 block components. In these cases,\nthe system obtained is considered the solution to our problem. If there are more than\ntwo block components, however, this sequential algorithm may miss it. In order to\nbe sure to find the solution, one would need to use a global algorithm investigating\nall possible partitions of [0, T ] into three or more disjoint intervals.\nRemark 1: In our analysis, we allow discontinuity at t for \u03b1 and \u03b2. It is possible,\nthough, to slightly modify the function so that it becomes continuous or differentiable.\nFor example, to obtain a continuous function, linear interpolation at the border may\nbe implemented. To obtain a smooth function, a constrained smoothing may be ap-\nplied with boundary condition. However, because an additional refinement procedure\ncan be arbitrary and does not influence the conclusion, we do not pursue this topic\nfurther.\nFigure 5 about here.\nThe procedure is illustrated with the examples shown in Figure 3. Again, panels\nfrom the left column correspond to two subprocesses and those to the right to one\nhomogeneous process. The functions C(t) and O(t) are seen in the top panels of\nFigure 5. For two subprocesses, the correlation C(t) is found below the cut-off value\nCmax = 0.3 on a whole interval around 0, and the maximum of the optimality criterion\nO(t) within that interval is found at t0 = \u22120.04. This leads to two block-components\n(lower left panel). For one homogeneous process, the correlation function C(t) stays\nwell above the cut-off value, and we hence define only one block component (lower\nright panel).\n2.7 Adding difference components\nOnce block components are defined, we may add difference components to the sys-\ntem. This is done in order to increase the percentage of variance extracted and to\nobtain further structural information. For each i, define a residual process ri(t) by\nsubtracting its linear prediction in terms of \u03b2k, k = 1, \u00b7 \u00b7 \u00b7 , Nb as\nri(tj) = yi(tj)\u2212 \u00b5(tj)\u2212\nNb\u2211\nk=1\n\u03b8ik\u03b2k(tj) .\nDenote the covariance function of the residual process by \u03b3r and apply principal\ncomponents analysis. Alternatively, the residual dispersion matrix subtracting its\nbest linear prediction in terms of the \u03b2k can be directly obtained from\n\u0393r = \u0393\u2212 \u0393B(B\u2032\u0393B)\u22121B\u2032\u0393 ,\n7\nwhere \u0393 = {\u03b3(ti, tj)} and B = {\u03b2i(tj)} are matrices evaluated at design points. See\nRao (1968).\n3 Properties\nTo establish statistical properties of the proposed procedure, we shall restrict our\nattention to two important situations. If there are two subprocesses, the procedure\nshould be able to detect it by defining two block components. If there is only one\nprocess, we should come up with one block component. The following assumptions\nwill be used.\n(A2) The covariance function \u03b3 is continuous, strictly positive-definite, and the trace\nof \u03b3,\n\u222b\n\u03b3(u, u) du, is finite.\n(A3) All eigenvalues \u03bbj have multiplicity 1, so that \u03bb1 > \u03bb2 > \u00b7 \u00b7 \u00b7 > 0.\n(A4) supu,v\u2208T |\u03b3\u02c6(u, v)\u2212 \u03b3(u, v)| \u2192 0 in probability .\nAssumption (A2) is standard. When some eigenvalues have multiplicity greater\nthan 1, the corresponding eigenfunctions are not uniquely defined and thus one needs\nto deal with the subspace generated by the eigenvectors as in Dauxois et al. (1982)\nand Boente and Fraiman (2000). Because our criteria only require the leading eigen-\nfunction for each partition, we assume that it is uniquely defined as in (A3). Our\nfocus is not so much on the covariance function estimator as on the behaviour of\ncriteria functions based on it, as long as it is (uniformly) consistent. For example,\nkernel-based smoothing estimators used in Staniswalis and Lee (1998) and Yao et al.\n(2005), similar to our estimator, satisfy (A4). For roughness penalty approach, see\nSilverman (1996) and Cardot (2000). Thus, any reasonable covariance function esti-\nmator could be incorporated in the procedure and the properties stated below would\nbe equally applicable. Proofs are found in the Appendix.\nBelow we write C\u02c6(t) and O\u02c6(t) for the respective estimators of C(t) and O(t) cal-\nculated with \u03b3\u02c6. Theorem 1 shows that these estimators are consistent. In particular,\none can estimate consistently the minimum value of C(t). This will be needed to\nshow the consistency of our procedure in subsequent sections.\nTheorem 1 Assume (A2)-(A4). Then, we have\nsup\nt\n|C\u02c6(t)\u2212 C(t)| \u2192 0 in probability ,\nsup\nt\n|O\u02c6(t)\u2212O(t)| \u2192 0 in probability ,\nas n\u2192\u221e.\n8\n3.1 Case of two subprocesses\nConsider the case of two subprocesses. Equivalently, suppose that the covariance\nfunction has an ideal partition with two blocks.\n(A1\u2032) For a fixed t0, \u03b3(u, v) = 0 for (u\u2212 t0)(v\u2212 t0) < 0. Otherwise, \u03b3(u, v) \u2265 0, where\n0 < c1 \u2264\n\u222b T\nt0\n\u03b3(u, u) du\u222b t0\n0\n\u03b3(u, u) du\n\u2264 c2 <\u221e ,\nfor some positive constants c1 and c2.\nIt turns out that one cannot detect this cut-point t0 based on the correlation criterion\nC(t) alone. The reason for this is that this function does not have a unique minimum,\nbut is equal to zero on an interval. Thus we look for the point where the optimality\ncriterion O(t) reaches maximum on that interval.\nLemma 1 Assume (A1\u2032), (A2)\u2212 (A3) Then, there exists a neighborhood of t0, N (t0)\nsuch that\nC(t) = 0 , for all t \u2208 N (t0) ,\nand\nO(t0) \u2265 O(t) , for all t \u2208 T .\nMoreover, the maximizer is unique.\nTheorem 2 Assume (A1\u2032), (A2)\u2212 (A4). Define for given \u03c4 > 0\nt\u02c60 = arg max\nt:|C\u02c6(t)|\u2264\u03c4\nO\u02c6(t) .\nSuppose that for large enough N , t\u02c60 is uniquely defined for n \u2265 N as n \u2192 \u221e with\nprobability tending to one. Then\nt\u02c60 \u2192 t0 in probability .\nIn summary, in such an ideal case as assumed in (A1\u2032), our procedure will con-\nsistently define block components which correspond to the blocks in the covariance\nfunction.\n9\n3.2 Case of one process\nNow we consider the case of one homogeneous process. In terms of the covariance\nfunction, this means that\n(A1\u2032\u2032) \u03b3(u,v)\n(\u03b3(u,u)\u03b3(v,v))1\/2\n\u2265 c > 0 , for all u, v \u2208 T .\nHere we are mainly concerned with c relatively large, for example c = 0.5 or larger.\nThe following lemma shows that the correlation criterion C(t) will in turn be larger\nthan this threshold c. Its corollary states that this will also happen consistently in\nthe sample.\nLemma 2 Assume (A1\u2032\u2032), (A2)\u2212 (A3). Then, we have\nC(t) \u2265 c , for all t \u2208 T .\nCorollary 1 Assume (A1\u2032\u2032), (A2) \u2212 (A4). Then, there exists a sequence cn that\nconverges to c as n\u2192\u221e such that\nC\u02c6(t) \u2265 cn in probability .\nTherefore, in case (A1\u2032\u2032) holds, our procedure will consistently define only one\nblock-component as soon as threshold c is larger than the pre-specified cut-off value\nCmax.\n4 Numerical performance\nNumerical performance was studied through simulation and application to three real\ndata sets including growth data shown in Figure 1. Further examples are weather\ndata and gait data from Ramsay and Silverman (1997). Motivating examples shown\nin Figure 2 serve as the basis of simulation studies.\n4.1 Simulation studies\nWe compare our method (SCA) to functional PCA and its varimax rotation (imple-\nmented in Matlab) to assess differences with respect to the three criteria proposed in\nSection 2: number of block components (Nb), correlation(C) and optimality(O). We\nconsider the following model\nyi(tj) = \u03b1i1\n1\u221a\n2pi0.3\nexp\n(\n\u2212(tj + 0.5)\n2\n0.3\n)\n+ \u03b1i2\n1\u221a\n2pi0.3\nexp\n(\n\u2212(tj \u2212 0.5)\n2\n0.3\n)\n,\n10\nTable 1: Comparison of methods for approximate block structure (left) and nonblock\nstructure (right).\n\u03c312 = 0.0 \u03c312 = 0.5\nNb Nb(10%) Opt Corr Nb Nb(10%) Opt Corr\nSCA 1.95 1.95 0.98 0.04 1.0 1.0 1.0 0.0\nFPCA 0.97 1.41 1.0 0.0 1.0 1.0 1.0 0.0\nVarimax 0 2.0 0.99 0.06 0 2.0 0.78 0.70\nwhere\n(\u03b1i1, \u03b1i2) \u223c iid Normal\n(\n0,\n(\n\u03c311 \u03c312\n\u03c312 \u03c322\n))\n.\nWe are mainly interested in the dependence of Nb on the underlying covariance struc-\nture. We set \u03c311 = 0.8, \u03c322 = 0.7 and \u03c312 between 0 and 0.5. Results below are based\non 1000 simulations, where 50 random curves are observed at 101 equally spaced\ndesign points on [-1,1]. For SCA, we use Cmax = 0.3.\nTable 1 presents average results for \u03c312 = 0.0 and \u03c312 = 0.5. The former case cor-\nresponds to two independent subprocesses (with almost non-overlapping supports),\nwhereas the latter case corresponds to one homogeneous process. Thus, a good so-\nlution according to our criteria should have Nb = 2 and Nb = 1, respectively. While\nthis is mostly achieved by SCA (Nb being on average 1.95 and 1), FPCA has only one\nblock component in either cases (Nb being on average 0.97 and 1). Strictly speaking,\nVarimax had no block component (Nb = 0). However, it often produces components\nwhich resemble block components. If we relax a bit our criterion to allow small per-\nturbation, say 10% of squared norm, in counting the number of block components\nas\nNb(10%) =\nq\u2211\nk=1\nI\n(\nmin\n{\u222b\n\u03b2k>0\n\u03b22k(t) dt,\n\u222b\n\u03b2k<0\n\u03b22k(t) dt\n}\n< .1\n)\nthen Varimax has 2 block components in both cases (Nb(10%) = 2). Thus, neither\nFPCA nor Varimax could distinguish between one and two subprocesses, whereas\nSCA mostly does. Moreover, the average optimalty of SCA is larger than 0.98 in\nboth cases, the average correlation between components remaining smaller than 0.04.\nBy way of contrast, the average optimality of Varimax is only 0.78 in the one process\ncase, the average correlation between components being as high as 0.7.\nFurther simulations point into the same direction; details can be obtained from\nthe first author.\n11\n4.2 Real data examples\nNow we present application to three real data sets.\nGrowth data: The procedure is applied to the growth data shown in Figure 1.\nIt has been conjectured (Karlberg, 1987; Karlberg et al., 1987) that during growth\nthere are at least two (or three if including infancy) nearly non\u2013overlapping periods\nin which onset of secretion of new hormones promotes different phase of growth. This\nprompted the use of different parametric forms for different periods, however, phase\nchanges are rather manually identified based on individual growth curves. Stu\u00a8tzle\net al. (1980) adopted a semiparametric approach, where two almost non-overlapping\nadditive components are estimated by nonlinear regression via shape\u2013invariant mod-\neling. It is an interesting question whether our method can identify the separation\nbetween the different phases.\nFor 120 boys and 112 girls, measurements were taken from age 0 to age 20, half-\nyearly around puberty and yearly otherwise. Because of the huge variability in in-\nfancy, the analysis is restricted to age 2 to 20, for which two subprocesses could be\npostulated. The analysis is based on the velocity trajectories, estimated directly from\nthe raw data using Gasser-Mu\u00a8ller estimator. A registration step is implemented based\non landmarks (Gasser et al., 1984; Kneip and Gasser, 1992). The pattern is similar\nfor both sexes but the timing of the pubertal growth spurt, corresponding to a peak\nin the velocity curves, occurs earlier and is smaller for girls than for boys.\nFigure 6 about here.\nThe top left panel of Figure 6 shows 20 samples of smoothed velocity curves for\nboys after registration, together with the mean function (thick line). The middle left\npanel shows the correlation criterion (solid line) and optimality criterion (dashed line).\nThe curves are evaluated at the original data points and linearly interpolated. As the\ncorrelation becomes negative after age 9.9, the absolute correlation is plotted. Observe\nthat the correlation stays low for a wide range of values, suggesting the presence\nof independent subprocesses. Putting the correlation threshold to Cmax = 0.3, the\noptimality criterion is maximized at age 11.8. Thus, our procedure define a first block\ncomponent between ages 2 and 11.8, and a second block component between ages 11.8\nand 21, representing two roughly independent growth subprocesses. Alternatively,\nsince optimality is almost constant in the age range for which correlation is low, one\nmay select another age to separate subprocesses (in agreement with an expert in the\nfield) with practically no loss in optimality.\nIn particular, since the correlation function has a clear minimum at age 9.9, and\nsince the optimality criterion is almost as high at 9.9 as at 11.8, age 9.9 in this example\nis also a natural estimate to separate blocks. In the bottom panel are added block\n12\ncomponents based on this separation (t0 = 9.9). These two block components cannot\nbe splitted into further blocks without surpassing the correlation threshold. Thus,\nwe conclude that two subprocesses, but not more, are present in the data. Similar\nconclusions can be drawn for girls (not shown).\nWeather data: Our second example is the monthly mean temperature of 35 Cana-\ndian weather stations from Ramsay and Silverman (1997) shown in the top right panel\nof Figure 6. The mean is estimated by kernel smoothing with a bandwidth 0.4, set\nby visual inspection. Unlike the growth process, the climate process is expected to be\nhomogeneous across time and regions. The correlation criterion shown in the middle\nright panel confirms this hypothesis, as the function remains very high, close to 1, on\nthe whole range. Thus, our procedure selects in this example only a single block com-\nponent to indicate that no structural change occurs and that the underlying process is\nhomogeneous. In that case, components suggested by our procedure are the same as\nthose produced by functional PCA, and the first two of them are shown in the bottom\nright panel. The same conclusions hold for the daily temperature measurements.\nGait data: The procedure is now applied to gait data, concentrating on the knee.\nThese consist of the angles formed by the hip and knee of 39 children over a gait cycle\n(Ramsay and Silverman 1997). This is an interesting example because it is not clear\nin advance whether the underlying process consists of one homogeneous process or\nnot. As a registration step is required, landmark registration with maxima is used.\nFigure 7 about here.\nThe smoothed curves of registered data are shown in the upper left panel of Figure 7,\nwith the mean curve in thick line. The corresponding correlation and optimality\ncriteria are shown in the upper right panel. In contrast to the first two examples, the\ncorrelation function is neither negligible, nor very high, but is around 0.5 on a large\ndomain. When applied with a cut-off value of Cmax = 0.3, our procedure selects a\nsingle block component, as is shown in the bottom right panel. However, using a more\nliberal value of Cmax, a solution with two block components is conceivable for which\noptimality is still above 80%, compared to functional principal components. The\ntwo block components obtained are shown in the lower left panel, which separate the\nearlier preparation movement from the later major movement. It would be interesting\nto discuss with specialists of the field the merits of the two solutions.\n5 Appendix\nProof of Theorem 1 We use the following lemma.\n13\nLemma 3 (p.147, Rudin (1976)) The sequence of functions {fn} defined on E, a\nsubset of metric space, converges uniformly on E if and only if for every \u000f > 0, there\nexists an integer N such that m \u2265 N, n \u2265 N implies\n|fn(t)\u2212 fm(t)| < \u000f\nfor each t \u2208 E.\nWe first introduce some notations. Define the bounded linear operator associated\nwith the covariance function \u0393 : L2[0, T ]\u2192 L2[0, T ] as\n(\u0393f)(u) =\n\u222b T\n0\n\u03b3(u, v)f(v) dv for all f \u2208 L2([0, T ]) ,\nwith norm ||\u0393||L = sup||f ||\u22641 ||\u0393f ||, where || \u00b7 || is the usual norm in the space L2[0, T ],\ndistinguished from || \u00b7 ||2 for the norm in L2([0, T ] \u00d7 [0, T ]). Similarly, the empirical\ncovariance operator \u0393\u02c6 will be defined through the estimated covariance function \u03b3\u02c6.\nThen it holds that\n||\u0393||L \u2264 ||\u03b3||2 . (1)\nNote that, instead of C\u02c6, we write Cn for the estimator of C based on n observations.\nFor each t, denote by \u03b1n,t and \u03b2n,t the extended leading eigenfunctions for each\npartial covariance function, which are defined on the whole interval. Then Cn can be\nwritten as\nCn(t) =\n(\u03b1n,t,\u0393n\u03b2n,t)\u221a\n(\u03b1n,t,\u0393n\u03b1n,t)(\u03b2n,t,\u0393n\u03b2n,t)\n. (2)\nIn view of Lemma 3, we will study the behaviour of |Cn(t) \u2212 Cm(t)| for each t and\nsuppress the dependence on t in \u03b1,\u03b1n,\u03b2 and \u03b2n from now on. First observe that\n|(\u03b1n,\u0393n\u03b2n)\u2212 (\u03b1m,\u0393m\u03b2m)|\n\u2264 |(\u03b1n, (\u0393n \u2212 \u0393m)\u03b2n)|+ |(\u03b1n \u2212\u03b1m,\u0393m\u03b2n)|+ |(\u03b1m,\u0393m(\u03b2n \u2212 \u03b2m))|\n\u2264 ||\u03b1n||||(\u0393n \u2212 \u0393m)\u03b2n||+ ||\u03b1n \u2212\u03b1m||||\u0393m\u03b2n||+ ||\u03b1m||||\u0393m(\u03b2n \u2212 \u03b2m)||\n\u2264 ||\u03b1n||||\u0393n \u2212 \u0393m||L||\u03b2n||+ ||\u03b1n \u2212\u03b1m||||\u0393m||L||\u03b2n||+ ||\u03b1m||||\u0393m||L||\u03b2n \u2212 \u03b2m||\n\u2264 ||\u03b1n||||\u03b3n \u2212 \u03b3m||2||\u03b2n||+ ||\u03b1n \u2212\u03b1m||||\u03b3m||2||\u03b2n||+ ||\u03b1m||||\u03b3m||2||\u03b2n \u2212 \u03b2m|| ,\nwhere the last inequality follows from (1). Note that (A4) implies that ||\u03b3n\u2212\u03b3||2 \u2192 0\nin probability. Thus, by Lemma 3, it is enough to show that ||\u03b1n\u2212\u03b1|| and ||\u03b2n\u2212\u03b2||\nconverge. In fact, the uniform convergence of the covariance function implies the\nuniform convergence of the corresponding eigenfunctions. According to lemma 3.1 of\nBosq (1991), the jth eigenfunction estimator of \u03c6j satisfies\n||\u03c6\u02c6j \u2212 \u03c6j|| \u2264 aj||\u0393\u02c6\u2212 \u0393||L ,\n14\nwhere\na1 = 2\n\u221a\n2(\u03bb1 \u2212 \u03bb2)\u22121\naj = 2\n\u221a\n2max{(\u03bbj\u22121 \u2212 \u03bbj)\u22121, (\u03bbj \u2212 \u03bbj+1)\u22121} if j \u2265 2 .\n(Remark: The original theorem is proved under non-smoothed empirical covariance\noperator and its extension to a kernel-smoothed covariance operator is referred to\nBoente and Fraiman (2000).) From (1), we may write it as\n||\u03c6\u02c6j \u2212 \u03c6j|| \u2264 aj||\u03b3\u02c6 \u2212 \u03b3||2 .\nThis property still can be applied to our situation where for each t, \u03b1n is not an eigen-\nfunction of \u0393n but an eigenfunction of \u0393n restricted to [0, t], for if the full covariance\nfunction converges uniformly, so does its restriction on the subinterval. Write \u03b3n,t for\nthe corresponding covariance function associated with \u03b1n and \u03b3n,\u2212t for the covariance\nfunction associated with \u03b2n.\n||\u03b1n \u2212\u03b1|| \u2264 a1,t||\u03b3n,t \u2212 \u03b3t||2 = a1,t\n\u221a\u222b t\n0\n\u222b t\n0\n|\u03b3n(u, v)\u2212 \u03b3(u, v)|2 du dv\n\u2264 a1,t\n\u221a\u222b T\n0\n\u222b T\n0\n|\u03b3n(u, v)\u2212 \u03b3(u, v)|2 du dv\n= a1,t||\u03b3n \u2212 \u03b3||2 ,\nwhere a1,t is the constant a1 calculated from \u03b3n,t. Similar result can be derived\nfor ||\u03b2n \u2212 \u03b2|| with an appropriate constant b1,\u2212t. This leads to the convergence of\n(\u03b1n,\u0393n\u03b2n), in particular, that of (\u03b1n,\u0393n\u03b1n) and (\u03b2n,\u0393n\u03b2n). It follows from (2) and\nLemma 3 that Cn converges uniformly. On the other hand, it can be seen that Cn(t)\nconverges to C(t) for each t. Therefore, we conclude that Cn converges to C uniformly.\nBecause O is also a functional of the covariance function and its eigenfunctions, similar\nargument applies to O.\nProof of Lemma 1 For a given t, the leading eigenfunctions for each partition\nsatisfy:\n(\u0393t\u03b1)(u) =\n\u222b t\n0\n\u03b3(u, v)\u03b1(v) dv = \u03bb1(t)\u03b1(u) ,\n(\u0393t\u03b2)(u) =\n\u222b T\nt\n\u03b3(u, v)\u03b2(v) dv = \u03bb2(t)\u03b2(u) ,\n15\nHere, for simplicity of notation, we suppress the dependence on t in \u03b1 and \u03b2. Suppose\nthat t \u2264 t0 and consider \u03b2. Because of condition (A1\u2032), only one of the following\nequations will be used to produce the leading eigenfunction \u03b2.\u222b t0\nt\n\u03b3(u, v)\u03b21(v) dv = \u03bb2,1(t)\u03b21(u) t \u2264 u \u2264 t0\u222b T\nt0\n\u03b3(u, v)\u03b22(v) dv = \u03bb2,2(t)\u03b22(u) t0 \u2264 u \u2264 T\nObserve that \u03bb2,2(t) does not depend on t, while\n\u03bb2,1(t) =\n\u222b t0\nt\n\u222b t0\nt\n\u03b21(u)\u03b3(u, v)\u03b21(v) du dv\n\u2264\n\u221a\u222b t0\nt\n\u222b t0\nt\n\u03b3(u, v)2 du dv\n\u2264\n\u222b t0\nt\n\u03b3(u, u) du \u2264 sup\nu\u2208[t,t0]\n|\u03b3(u, u)||t\u2212 t0| .\nAs a consequence, \u03bb2,1(t) \u2192 0 and \u03bb2,2(t) = \u03bb2,2(t0) as t \u2192 t0. So, there exist a\nneighborhood N (t0) such that for all t \u2208 N (t0),\u03b2(u) = \u03b22(u), t0 \u2264 u \u2264 T and 0\notherwise. Therefore,\u222b T\n0\n\u222b T\n0\n\u03b1(u)\u03b3(u, v)\u03b2(v) du dv\n=\n\u222b t0\n0\n\u222b T\n0\n\u03b1(u)\u03b3(u, v)\u03b2(v) du dv +\n\u222b T\nt0\n\u222b T\n0\n\u03b1(u)\u03b3(u, v)\u03b2(v) du dv\n=\n\u222b t0\n0\n\u222b t\n0\n\u03b1(u)\u03b3(u, v)\u03b2(v) du dv +\n\u222b T\nt0\n\u222b t\n0\n\u03b1(u)\u03b3(u, v)\u03b2(v) du dv ,\nwhere the first term is zero because \u03b2(u) = 0 for 0 \u2264 u \u2264 t0 and the second term is\nzero because of (A1\u2032). Symmetric arguments apply to \u03b1 when t > t0. Therefore, this\nimplies that C(t) = 0 for all t \u2208 N (t0). Regarding O, note that under the assumption\n(A1\u2032), the two block components separated at t0 are indeed eigenfunctions of the\ncovariance function \u03b3, thus the weight functions of principal components. Because O\nmeasures the sum of variability and the principal components uniquely maximize the\nsum of variability, it follows that O(t0) is the unique maximum.\nProof of Theorem 2 We prove that the negation of the claim contradicts to\nthe argument below. To make the dependence on n of the estimator clear, write the\n16\nnth criteria functions as Cn and On. All arguments below hold in probability without\nmaking explicit references. Observe that t0 is uniquely defined for two block structure\n(A1\u2032). Then for every \u03b4 > 0, there exists some \u000f = \u000f(\u03b4) > 0 such that\nO(t) + \u000f < O(t0)\u2212 \u000f , if |t\u2212 t0| > \u03b4 .\nBecause of uniform convergence of On, we have for every \u000f1 > 0 and for all t, there\nexists n0 = n0(\u000f1) \u2265 N such that for all n \u2265 n0,\n|On(t)\u2212O(t)| < \u000f . (3)\nThen, for every \u03b4 > 0, there exists n0 = n0(\u000f(\u03b4)) \u2265 N such that for all n \u2265 n0,\nOn(t) < O(t0)\u2212 \u000f(\u03b4) , if |t\u2212 t0| > \u03b4 . (4)\nThis will contradict to the negation of the argument. To see why, suppose that the\nclaim is false. Define for n\nt0,n = argmax\nt\nOn(t) .\nThen for some \u03b40 > 0, there exists n \u2265 n\u02dc, n \u2265 N for all n\u02dc such that\n|t0,n \u2212 t0| > \u03b40 , and On(t0,n) \u2265 On(t0) .\nGiven \u000f0 = \u000f(\u03b40), choose n\u02dc = n0(\u000f0) that satisfies (3). Then, there exists n \u2265 n0 \u2265 N\nsuch that\nOn(t0,n) > O(t0)\u2212 \u000f(\u03b40) , if |t0,n \u2212 t0| > \u03b40 ,\nwhich is contradictory to (4).\nNow suppose that Cn is also used and t0,n is defined as\nt0,n = arg max\nt:|Cn(t)|\u2264\u03c4\nOn(t) ,\nfor some \u03c4 > 0. Note that C(t0) = 0. Then given \u03c4 and for all t \u2208 N (t0), there exists\nn1 such that for all n \u2265 n1 \u2265 N ,\n|Cn(t)| \u2264 \u03c4 .\nIf we replace \u000f by min(\u000f, \u03c4) and n0 by max(n0, n1) in the above argument, the same\nholds true.\nProof of Lemma 2 It follows from (A1\u2032\u2032) combined with Cauchy-Schwarz inqual-\nity.\n17\nProof of Corollary 1 Because supt |C\u02c6(t)\u2212 C(t)| \u2192 0 in probability, with prob-\nability tending to 1, for every \u000f and for every t, there exists n0 \u2265 N such that for all\nn \u2265 n0,\n|Cn(t)\u2212 C(t)| < \u000f .\nBecause C(t) \u2265 c, this implies that\nCn(t) \u2265 c\u2212 \u000f .\nFor \u000fn \u2192 0, take cn = c\u2212 \u000fn.\nReferences\nBoente, G. and R. Fraiman (2000). Kernel-based functional principal components.\nStatistics & Probability Letters 48, 335\u2013345.\nBosq, D. (1991). Modelization, nonparametric estimation and prediction for continu-\nous time processes. In G. Roussas (Ed.), Nonparametric Function Estimation and\nRelated Topics, Nato Asi series, pp. 509\u2013529. Klewer Academic: Dordrecht.\nCardot, H. (2000). Nonparametric estimation of smoothed principal components\nanalysis of sampled noisy functions. Journal of Nonparametric Statistics 12, 503\u2013\n538.\nDauxois, J., A. Pousse, and Y. Romain (1982). Asymptotic theory for the principal\ncomponent analysis of a vector random function: Some applications to statistical\ninference. Journal of multivariate analysis 12, 136\u2013154.\nDiggle, P. J. and A. P. Verbyla (1998). Nonparametric estimation of covariance\nstructure in longitudinal data. Biometrics 54, 401\u2013415.\nGasser, T., H. G. Mu\u00a8ller, W. Ko\u00a8hler, L. Molinari, and A. Prader (1984). Nonpara-\nmetric regression analysis of growth curves. Ann. Statist. 12, 210\u2013229.\nGervini, D. and V. Rousson (2004). Criteria for evaluating dimension-reducing com-\nponents for multivariate data. The American Statistician 58, 72\u201376.\nKarlberg, J. (1987). On the modeling of human growth. Statist. Med. 6, 185\u2013192.\nKarlberg, J., J. G. Fryer, I. Engstro\u00a8m, and P. Karlberg (1987). Analysis of linear\ngrowth using a mathematical model ii. from 3 to 21 years of age. Acta Paediatrica\nScandinavica 337, 12\u201329.\n18\nKneip, A. and T. Gasser (1992). Statistical tools to analyze data representing a\nsample of curves. Ann. Statist. 20, 1266\u20131305.\nRamsay, J. and B. Silverman (1997). Functional Data Analysis. Springer\u2013Verlag:\nNew York.\nRamsay, J. and B. Silverman (2002). Applied Functional Data Analysis: Methods and\nCase Studies. Springer: New York.\nRao, C. R. (1968). Linear Statistical Inference and its Applications. Wiley: New\nYork.\nRousson, V. and T. Gasser (2004). Simple component analysis. Appl. Statist. 53,\n539\u2013555.\nRudin, W. (1976). Principles of Mathematical Analysis (3rd ed.). McGraw\u2013Hill.\nSilverman, B. W. (1996). Smoothed functional principal component analysis by choice\nof norm. Ann. Statist. 24, 1\u201324.\nStaniswalis, J. and J. Lee (1998). Nonparametric regression analysis of longitudinal\ndata. J. Am. Statist. Ass. 93, 1403\u20131418.\nStu\u00a8tzle, W., T. Gasser, L. Molinari, R. H. Largo, A. Prader, and P. J. Huber (1980).\nShape-invariant modeling of human growth. Annals of Human Biology 7, 507\u2013528.\nWu, H. and J. Zhang (2002). Local polynomial mixed-effects models for longitudinal\ndata. J. Am. Statist. Ass. 97 (459), 883\u2013897.\nYao, F., H. G. Mu\u00a8ller, A. J. Clifford, S. R. Dueker, J. Follett, Y. Lin, B. A. Buchholz,\nand J. S. Vogel (2003). Shrinkage estimation for functional principal component\nscores with application to the population kinetics of plasma folate. Biometrics 59,\n676\u2013685.\nYao, F., H. G. Mu\u00a8ller, and J. L. Wang (2005). Functional data analysis for sparse\nlongitudinal data. J. Am. Statist. Ass. 100, 577\u2013590.\n19\n2 7 12 17 21\n90\n120\n150\n180\nage (years)\nhe\nig\nht\n2 7 12 17 21\n0\n5\n10\n15\nage (years)\nve\nlo\nci\nty\nFigure 1: Height growth curves (left) and velocity curves (right) for 10 boys. Velocity\ncurves are estimated nonparametrically with Gasser-Mu\u00a8ller estimator. These curves\ncan be aligned to eliminate time variability.\n20\n\u22121 \u22120.5 0 0.5 1\n\u22122\n\u22121\n0\n1\n2\n3\n\u22121 \u22120.5 0 0.5 1\n\u22122\n\u22121\n0\n1\n2\n3\nFigure 2: Example of curves that have almost non\u2013overlapping support. Data are\nconstructed by adding up these two curves.\n21\n\u22121 \u22120.5 0 0.5 1\n\u22120.2\n\u22120.1\n0\n0.1\n0.2\nI. Two subprocesses\nPC 1: 63%\n\u22121 \u22120.5 0 0.5 1\n\u22120.2\n\u22120.1\n0\n0.1\n0.2\nII. One process\nPC 1: 86.4%\n\u22121 \u22120.5 0 0.5 1\n\u22120.2\n\u22120.1\n0\n0.1\n0.2\nPC 2: 37%\n\u22121 \u22120.5 0 0.5 1\n\u22120.2\n\u22120.1\n0\n0.1\n0.2\nPC 2: 13.6%\nFigure 3: Functional PCA for two subprocesses (left, shown in Figure 2) and one\nprocess (right). Both provide qualitatively the same answer.\n22\n\u22121 \u22120.5 0 0.5 1\n\u22121\n\u22120.8\n\u22120.6\n\u22120.4\n\u22120.2\n0\n0.2\n0.4\n0.6\n0.8\n1\n0.2 0.4 0.6 0.8 1\n\u22121 \u22120.5 0 0.5 1\n\u22121\n\u22120.8\n\u22120.6\n\u22120.4\n\u22120.2\n0\n0.2\n0.4\n0.6\n0.8\n1\n0.2 0.4 0.6 0.8 1 1.2\nFigure 4: Contour plots of covariance (top) and correlation (bottom) function. Dis-\ntinct structures between two subprocesses (left) and one process (right) are visualized.\nThe higher, the darker. Ranges in numbers are added in the bottom of each plot.\n23\n\u22121 \u22120.5 0 0.5 1\n0\n0.2\n0.4\n0.6\n0.8\n1\nII. One process\nCorr\nOpt\n\u22121 \u22120.5 0 0.5 1\n\u22120.2\n\u22120.1\n0\n0.1\n0.2\n\u22121 \u22120.5 0 0.5 1\n0\n0.2\n0.4\n0.6\n0.8\n1\nI. Two subprocesses\n\u22121 \u22120.5 0 0.5 1\n\u22120.2\n\u22120.1\n0\n0.1\n0.2\nt = \u22120.04\nCorr\nOpt\nFigure 5: Top row shows diagnostic plots for two subprocesses (left) and one pro-\ncess (right) cases, shown in Figure 4. For the left are suggested two block components,\nseparated at t = \u2212.04 and one block component for the right. Bottom row shows\nselected first two components.\n24\n2 7 12 17 21\n0\n5\n10\n15\nI. Growth data\n2 7 12 17 21\n0\n0.5\n1\nCorr\nOpt\n2 7 12 17 21\n0\n0.1\n0.2\nt = 9.9\nage (years)\n1 3 6 9 12\n\u221224\n\u22128\n8\n24\nII. Weather data\n1 3 6 9 12\n0\n0.5\n1\nCorr\nOpt\n1 3 6 9 12\n\u22120.1\n0\n0.1\nmonth\nFigure 6: Analysis of structural components for growth curves (left) and weather data\n(right). Criteria curves are shown in the middle panel, indicating two subprocesses\nin the left and one process in the right. Suggested two components are drawn in the\nbottom.\n25\n0 0.5 1\n0\n40\n80\n0 0.5 1\n0\n0.5\n1\nCorr\nOpt\n0 0.5 1\n\u22120.1\n0\n0.1\nt = 0.47\n0 0.5 1\n\u22120.1\n0\n0.1\nFigure 7: Registered gait data (knee) in the upper left panel with criteria curves in the\nright. Vertical line indicates possible separation of block components with relatively\nhigh correlation (0.5). Two block components are shown in lower left, compared to\ntwo principal components in lower right panel. Block components are approximately\n85% optimal against principal components, as is indicated by dashed curve in top\nright panel.\n26\n"}