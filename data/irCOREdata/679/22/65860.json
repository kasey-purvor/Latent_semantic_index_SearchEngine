{"doi":"10.1109\/ICSM.2007.4362628","coreId":"65860","oai":"oai:dro.dur.ac.uk.OAI2:4389","identifiers":["oai:dro.dur.ac.uk.OAI2:4389","10.1109\/ICSM.2007.4362628"],"title":"Reducing regression test size by exclusion.","authors":["Gallagher,  K.","Hall, T.","Black, S."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":["Tahvildari, L.","Canfora, G."],"datePublished":"2007-10-01","abstract":"Operational software is constantly evolving. Regression testing is used to identify the unintended consequences of evolutionary changes. As most changes affect only a small proportion of the system, the challenge is to ensure that the regression test set is both safe (all relevant tests are used) and unclusive (only relevant tests are used). Previous approaches to reducing test sets struggle to find safe and inclusive tests by looking only at the changed code. We use decomposition program slicing to safely reduce the size of regression test sets by  identifying those parts of a system that could not have been affected by a  change; this information will then direct the selection of regression tests by eliminating tests that are not  relevant to the change. The technique properly accounts for additions and deletions of code. \\ud\n\\ud\nWe extend and use Rothermel and Harrold\u2019s framework for measuring the safety of regression test sets and  introduce new safety and precision  measures that do not require a priori knowledge of the exact number\\ud\nof modification-revealing tests. We then analytically evaluate and compare our techniques for producing reduced regression test sets","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/65860.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/4389\/1\/4389.pdf","pdfHashValue":"79562b184c9508fc2c3aaf1637bd3293ed27681c","publisher":"IEEE","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:4389<\/identifier><datestamp>\n      2011-09-06T10:57:09Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Reducing regression test size by exclusion.<\/dc:title><dc:creator>\n        Gallagher,  K.<\/dc:creator><dc:creator>\n        Hall, T.<\/dc:creator><dc:creator>\n        Black, S.<\/dc:creator><dc:description>\n        Operational software is constantly evolving. Regression testing is used to identify the unintended consequences of evolutionary changes. As most changes affect only a small proportion of the system, the challenge is to ensure that the regression test set is both safe (all relevant tests are used) and unclusive (only relevant tests are used). Previous approaches to reducing test sets struggle to find safe and inclusive tests by looking only at the changed code. We use decomposition program slicing to safely reduce the size of regression test sets by  identifying those parts of a system that could not have been affected by a  change; this information will then direct the selection of regression tests by eliminating tests that are not  relevant to the change. The technique properly accounts for additions and deletions of code. \\ud\n\\ud\nWe extend and use Rothermel and Harrold\u2019s framework for measuring the safety of regression test sets and  introduce new safety and precision  measures that do not require a priori knowledge of the exact number\\ud\nof modification-revealing tests. We then analytically evaluate and compare our techniques for producing reduced regression test sets.<\/dc:description><dc:publisher>\n        IEEE<\/dc:publisher><dc:source>\n        Tahvildari, L. & Canfora, G. (Eds.). (2007). IEEE International Conference on Software Maintenance, ICSM 2007, 2-5 October 2007, Paris ; proceedings. New York: IEEE, pp. 154-163<\/dc:source><dc:contributor>\n        Tahvildari, L.<\/dc:contributor><dc:contributor>\n        Canfora, G.<\/dc:contributor><dc:date>\n        2007-10-01<\/dc:date><dc:type>\n        Book chapter<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:4389<\/dc:identifier><dc:identifier>\n        doi:10.1109\/ICSM.2007.4362628<\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/4389\/<\/dc:identifier><dc:identifier>\n        http:\/\/dx.doi.org\/10.1109\/ICSM.2007.4362628<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/4389\/1\/4389.pdf<\/dc:identifier><dc:rights>\n        \u00a92007 IEEE. This material is presented to ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors or by other copyright holders. All persons copying this information are expected to adhere to the terms and constraints invoked by each author's copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder\\ud\n\\ud\nPersonal use of this material is permitted. However, permission to reprint\/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must be obtained from the IEEE.\\ud\n<\/dc:rights><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2007,"topics":[],"subject":["Book chapter","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n26 March 2010\nVersion of attached file:\nPublished Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nGallagher, K. and Hall, T. and Black, S. (2007) \u2019Reducing regression test size by exclusion.\u2019, in IEEE\nInternational Conference on Software Maintenance, ICSM 2007, 2-5 October 2007, Paris ; proceedings. New\nYork: IEEE, pp. 154-163.\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1109\/ICSM.2007.4362628\nPublisher\u2019s copyright statement:\n2007 IEEE. This material is presented to ensure timely dissemination of scholarly and technical work. Copyright and\nall rights therein are retained by authors or by other copyright holders. All persons copying this information are\nexpected to adhere to the terms and constraints invoked by each author\u2019s copyright. In most cases, these works may\nnot be reposted without the explicit permission of the copyright holder Personal use of this material is permitted.\nHowever, permission to reprint\/republish this material for advertising or promotional purposes or for creating new\ncollective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in\nother works must be obtained from the IEEE.\nAdditional information:\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\n  \n \nDurham Research Online \n \nDeposited in DRO: \n26 March 2010 \n \nPeer-review status: \nPeer-reviewed \n \nPublication status: \nPublished version \n \nCitation for published item: \nGallagher, K. and Hall, T. and Black, S. (2007) 'Reducing regression test size by exclusion.', \nin IEEE International Conference on Software Maintenance, ICSM 2007, 2-5 October 2007, \nParis ; proceedings. New York: IEEE, pp. 154-163. \n \nFurther information on publisher\u2019s website: \nhttp:\/\/dx.doi.org\/10.1109\/ICSM.2007.4362628 \n \nPublisher\u2019s copyright statement: \n\u00a92007 IEEE. This material is presented to ensure timely dissemination of scholarly and \ntechnical work. Copyright and all rights therein are retained by authors or by other copyright \nholders. All persons copying this information are expected to adhere to the terms and \nconstraints invoked by each author's copyright. In most cases, these works may not be \nreposted without the explicit permission of the copyright holder \n \nPersonal use of this material is permitted. However, permission to reprint\/republish this \nmaterial for advertising or promotional purposes or for creating new collective works for \nresale or redistribution to servers or lists, or to reuse any copyrighted component of this work \nin other works must be obtained from the IEEE. \n \n \nUse policy \n \nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior \npermission or charge, for personal research or study, educational, or not-for-profit purposes provided that : \n \n\uf0a7 a full bibliographic reference is made to the original source \n\uf0a7 a link is made to the metadata record in DRO \n\uf0a7 the full-text is not changed in any way \n \nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders. \n \nPlease consult the full DRO policy for further details. \n \nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom \nTel : +44 (0)191 334 2975 | Fax : +44 (0)191 334 2971 \nhttp:\/\/dro.dur.ac.uk \nReducing Regression Test Size by Exclusion\nKeith Gallagher Tracy Hall Sue Black\nCentre for Software Maintenance School of Computer Science Harrow School\nand Evolution of Computer Science\nDurham University University of Hertfordshire University of Westminster\nSouth Road College Lane Watford Road\nDurham DH1 3LE Hertfordshire AL10 9AB Harrow HA1 3TP\nUnited Kingdom United Kingdom United Kingdom\nk.b.gallagher@durham.ac.uk t.hall@herts.ac.uk s.black3@westminster.ac.uk\nAbstract\nOperational software is constantly evolving. Regres-\nsion testing is used to identify the unintended conse-\nquences of evolutionary changes. As most changes af-\nfect only a small proportion of the system, the chal-\nlenge is to ensure that the regression test set is both\nsafe (all relevant tests are used) and inclusive (only\nrelevant tests are used). Previous approaches to re-\nducing test sets struggle to find safe and inclusive tests\nby looking only at the changed code. We use decom-\nposition program slicing to safely reduce the size of\nregression test sets by identifying those parts of a sys-\ntem that could not have been affected by a change; this\ninformation will then direct the selection of regression\ntests by eliminating tests that are not relevant to the\nchange. The technique properly accounts for additions\nand deletions of code.\nWe extend and use Rothermel and Harrold\u2019s frame-\nwork for measuring the safety of regression test sets\nand introduce a new safety and precision measures that\ndo not require a priori knowledge of the exact number\nof modification revealing tests. We then analytically\nevaluate and compare our techniques for producing re-\nduced regression test sets.\n1 Introduction\nMichael Cusumano, writing in the July 2006 issue\nof Communications of the ACM regarding Microsoft\u2019s\nMarch 2006 announcement that the delivery of the\nnew Vista operating system (known internally in Mi-\ncrosoft as \u201cLonghorn\u201d) would be (again) delayed to\nearly 2007, notes:\nMaking even small changes in one part of the\nproduct led to unpredictable and destabiliz-\ning consequences in other parts since most of\nthe components were tied together in com-\nplex and unpredictable ways. Even 4,000\nor so software developers and an equivalent\nnumber of testers was not enough to get\nLonghorn working [3].\nMaking a change to existing software, whether\nin production or development, system generates two\nproblems. The first is validating that the change cor-\nrect. The second and more difficult problem is de-\ntermining that no error has been inadvertently intro-\nduced. Ostrand, et al., found a 50-80% probably of in-\ntroducing an error while making a change to code [9].\nRegression testing is the method to uncover the un-\nintended side effects of making changes to software.\nRegression testing can uncover the unintended side ef-\nfects of making changes to software. It demonstrates,\ninsofar as any testing method can, that no changes\nhave rippled into other parts of the system.\nThe main difficulty of regression testing is deter-\nmining test sets are both safe and inclusive. Inclusive\nmeans that only tests that could possibly uncover an\nerror are used. Safe means to include all tests that\ncould possibly uncover an error. (Simply running all\ntests would clearly be safe, but not inclusive.) A good\nset of regression tests is inclusive and safe: the suite\ncovers all parts of the system that may have been af-\nfected by a change, but does not cover parts of the sys-\ntem that could not have been affected by that change.\nWe are interested in finding safe and inclusive re-\ngression tests. Moreover, most changes are relatively\nsmall with respect to the size of the entire system, so\nwhile the test set to determine that the change is cor-\nrect could be quite small, the number of regression test\nrequired could be relatively large. In an informal ap-\nplication of Amdahl\u2019s Law that small improvements in\nlarge efforts may surpass large improvements in small\nefforts, we aim to turn the problem of inclusion and\nsafety on its head by finding tests that could be ex-\ncluded, and excluded in such a way that running the\nexcluded test would not show any detectable change\nin the program\u2019s behaviour.\nThis approach has the decided advantage of always\nbeing safe. The cost is a loss of precision; some inef-\nfectual tests may be run. There is always a cost asso-\nciated with selecting regression tests. It is not difficult\nto imagine that in some cases the the cost of finding\nthe regression tests may exceed the cost of merely re-\nrunning all the tests. Our technique proposes, that in\nthe larger scheme, running a few unnecessary tests is\na small price to pay. It is certainly better to run a few\nunnecessary tests than to miss a necessary one.\n1.1 Contribution and Impact\nWe demonstrate how decomposition slicing [7, 8]\ncan enable the reduction of regression test set size by\ntest elimination. Decomposition slicing is a program\nslicing technique that identifies all lines of code that\nare related to a variable of interest and thus provides\na technique to identify unchanged parts of the system.\nThe technique can be compared to other regression\ntest set inclusion techniques by extending an exist-\ning framework for the comparison of inclusion tech-\nniques [10]. There are two contributions to reducing\nregression test set size. The first is alluded to in the\nCusumano quotation above: change testing is expen-\nsive and any help in reducing the effort is significant.\nThe second effect subtle: an a priori estimate of the\nsize of regression tests needed could direct the style of\nthe change (even, perhaps, changing the priority of a\nchange request due to regression test cost).\n1.2 Outline\nAfter this introductory section, the paper is orga-\nnized as follows. Section 2 give the background, dis-\ncussing the conceptual framework for the subsequent\npresentation previous work that we build on. Section\n3 re-frames the conceptual framework in what we be-\nlieve is a more coherent way; the new-found coherency\nallows an easy extension to the framework. Section 4\ndescribes our new technique and analyzes in the re-\nvised framework. Section 5 gives some examples and\nSection 6 summarizes the contribution and details fu-\nture directions.\n2 Background\nThe aim of regression testing is to validate un-\nmodified parts of software systems, to make sure that\nthe modification process does not introduce any new\nerrors and provide confidence that the modifications\nare correct. Regression testing is a very necessary,\nbut very expensive, software development and main-\ntenance activity. Ensuring that an accurate regression\ntest suite is chosen is of critical importance in reducing\nthe cost of software engineering activities.\nRegression test selection techniques attempt to re-\nduce the amount of time needed to retest a modi-\nfied program by selecting a subset of the existing test\nsuite. Techniques such as path analysis, program de-\npendence, cluster identification and program slicing\nare used to varying effect to reduce the amount of\ntests that need to be run thus reducing the amount of\neffort and therefore cost.\nRegression testing is not about validating that the\nchange is correct; that the change is correct is the baili-\nwick of the maintenance engineer and not in the scope\nof this paper. However, in subsequent versions of a\nsystem, tests that were created to validate a change\nin the system become part of the regression test suite.\nA new feature that is added to a system must also be\nprotected against unintended change.\n2.1 Regression Test Selection Analysis\nRothermel and Harrold [10] (hereafter referred to as\n\u201cRH96\u201d) describe a framework for evaluating regres-\nsion test selection techniques in terms of inclusiveness,\nprecision, efficiency and generality. Table 1 summa-\nrizes their framework.\nThe analysis framework described in RH96 is func-\ntional. Tests for code fragments that are syntacti-\ncally changed, but not semantically changed, would\nbe classified as non-modification-revealing. Thus any\nregression test selection technique that would select\nthe tests for such a code fragment would be consid-\nered as imprecise. For instance, consider the example\nof Figure 1, taken from RH96. While it is a bit con-\ntrived, it does show a technical side of the definition\nof precision. Since the code is functionally equivalent,\nany regression technique that selects tests for this code\nis imprecise.\nIn RH96, a modification-revealing test (hereafter\n\u201cmr-test\u201d) is one upon which P and P\u2032 differ. Inclu-\nsiveness is the measure of the number of modification-\nrevealing tests that regression test selection technique\nM finds, relative to the total number of modification-\nrevealing tests. (Our emphasis.) The selection tech-\nMeasure Description\nInclusiveness The measure of the extent to which a technique chooses the tests that will cause\nthe modified program to produce different output than the original program and thereby\nexpose faults caused by modifications\nSafety is a property of inclusiveness.\nPrecision The measure of the ability of a technique to avoid choosing tests that will not cause\nthe modified program to produce different output than the original program.\nNo property of precision is given in [10].\nEfficiency The computational cost, and thus, practicality, of a technique.\nGenerality The ability of a technique to handle realistic and diverse language constructs,\narbitrarily complex code modifications, and realistic testing applications\nTable 1: Summary of Rothermel and Harrold Terminology. Efficiency and generality are shown for completeness,\nbut are not a focus of this work.\nread (x) read (x)\nif ( x <= 0 ) if ( x <= 0 )\nif ( x == 0 ) if ( x == 0 )\nprint ( x + 2 ) print ( x + 2 )\nexit exit\nend else\nprint ( x + 3 )\nexit\nend\nend end\nprint ( x + 3 ) print ( x + 3 )\nexit exit\nFigure 1: Semantically Equivalent Code Fragments.\nnique is safe when inclusiveness is exactly 1. Note\nthat since safety measures only modification-revealing\ntests, that it is bounded above by 1.\nA non-modification-revealing test (hereafter \u201cnmr-\ntest\u201d), where P and P\u2032 are the same, for a given\ntest. In RH96, precision is the measure of the num-\nber of nmr-tests that regression test selection tech-\nnique N excludes, relative to the total number of non-\nmodification-revealing tests. (Our emphasis.) The se-\nlection technique is precise when the ratio is 1, and\nlike safety, bounded above by 1.\nIn RH96, inclusiveness and precision are comple-\nmentary measures, and safety is a property of the mea-\nsure inclusiveness. When investigating these ideas, we\napplied some simple software metric analysis. \u201cMea-\nsurement is the process by which numbers . . . are as-\nsigned to attributes of entities . . . \u201d [5]. (Our empha-\nsis.) We found the following adjustments in the ter-\nminology helpful. The entities that we will measure\nwill be techniques for obtaining regression tests. The\nattribute that we will measure will be inclusiveness\n\/ exclusiveness. The measure will be safety (for in-\nclusiveness) and precision (for exclusiveness). Table 2\nsummarizes our changes. A singular advantage to this\nre-casting is that now we can apply safety measures to\nexclusive techniques and precision measures to inclu-\nsive techniques. More discussion of this idea follows,\nbut at this point we note that one implication is that\nprecision and safety are no longer bounded above by\n1.\nOur definition of precision is different from RH96;\nthey use precision to measure what we call the at-\ntribute of exclusion and have no term analogous to\nsafety. We use safety to measure inclusion; and we use\nprecision to measure exclusion. The change in termi-\nnology from RH96 is so that there are pairs of comple-\nmentary terms: inclusion and exclusion as attributes;\nand safety and precision as measures. RH96 has in-\nclusion and precision as complementary measurement\nterms, and no complementary term for safety.\nRH96 defines inclusiveness, etc, in terms of mr-\ntests. They point out that other definitions may be\nappropriate. For instance, data-flow techniques can\nbe used to define definition-use-inclusiveness.\n2.2 Regression Test Selection Techniques\nThis subsection surveys a two techniques for select-\ning regression tests. It is by no means complete; it\ndoes however, relate to the technique we present. Both\ntechniques are used for selections. The reason that\nAttribute Description\nInclusiveness The extent to which a technique chooses the tests that will cause the modified\nprogram to produce different output than the original program and thereby\nexpose faults caused by modifications\nInclusiveness is measured by safety.\nExclusiveness The ability of a technique to avoid choosing tests that will not cause the modified\nprogram to produce different output than the original program.\nExclusiveness is measured by precision.\nTable 2: Our Changes to the Rothermel and Harrold Terminology\nthese were chosen is that the first uses dynamic infor-\nmation that is in turn attached to the static source.\nThe second uses program slicing.\n2.2.1 Textual Differencing\nVokolos and Frankl [11] use textual differencing, which\ncompares program sources. Source statements are\nmapped to the basic block in which they are contained;\nthe basic blocks are mapped to the test cases which\nexecute them. When a textual difference is observed\nin a basic block, the tests that execute that block are\nselected. The textual differencing technique is robust\nenough to observe changes in basic block structure;\nin this case, a basic block at a shallower nesting level\nthan execute the changed code is used to determine\nwhich tests to select.\nThe empirical evidence reported by Vokolos and\nFrankl is promising. The reductions are significant\nand easily obtained. The tool used to construct the\ntest set reduction, Pythia, was a straightforward cob-\nbling of Unix tools, a decided advantage. They do not\nreport, and evidently had no way of determining, if\nany regression tests were missed.\nChen, et al., [2] use a similar technique in Test-\nTube. Instead of using basic blocks, a` la Vokolos and\nFrankl, they identify which macros, functions, types\nand variables are covered by a test, then select tests\nthat correspond to a changed entity.\nIt is interesting to note that these techniques are\nconsidered imprecise in the RH96 framework. The test\nfor the changed code illustrated in Figure 1 would be\nselected. It seems that this is exactly what a tester\nwants to know. It may be that function of code frag-\nment has not changed, but many software evolution\nactivities do not require, and in some instances, do\nnot permit a changed functionality. However, non-\nfunctional changes have to be tested, too.\n2.2.2 Semantics Guided Selection\nBinkley [1] uses a semantics-guided selection tech-\nnique. Semantics in this case is the sequence of states\nof computation through which a program passes. The\nprogram must be \u201crolled out\u201d (all procedure calls ap-\npropriately substituted in-line). Once the semantics\nis clarified, the differences between a certified and\na modified variant of a program can be determined.\nThe technique then selects tests that exercise compo-\nnents of certified that have similar semantics (state\ntraces) in modified. Some care must be exercised in\ndefining \u201csimilar semantics.\u201d The test set selection\nis guided by calling context slicing, a technique for\ncalculating precise interprocedural slices. The calling\ncontext slices are used to find common execution pat-\nterns, which in turn are used to find the similar se-\nmantics. The outcome is that tests which are possibly\nmodification revealing are selected.\n2.3 Decomposition Slicing\nProgram slicing is a software analysis technique\nthat extracts program statements relevant to a par-\nticular computation. Informally, a slice provides the\nanswer to the question \u201cwhat program statements po-\ntentially affect the computation of variable v at state-\nment s?\u201d While conventional slices capture the value\nof a variable at a particular program point, a decom-\nposition slice captures all computations of a variable\nand is independent of program location [7, 8]. Decom-\nposition slicing can be used to partition a program\ninto three parts as described in the Table 3:\nFor software testing, only independent and depen-\ndent statements i.e., the decomposition slice taken\nwith respect to v are of interest. The complement\nstatements cannot be affected by the change. While\na program has as many decomposition slices as it has\nvariables, many of the decomposition slices on differ-\nProgram parts Includes statements which are . . .\nIndependent in the decomposition slice taken with respect to v that are\nnot in any other decomposition slice.\nDependent in the decomposition slice taken with respect to v that\nare in another decomposition slice.\nComplement not independent, i.e., statements in some other decomposition slice, (but not v\u2019s).\nTable 3: Classification of Decomposition Slice Variables. Assume the computation of variable v is the variable of\ninterest\nent variables are exactly the same [6]. Empirical anal-\nysis showed that reductions by equivalent decomposi-\ntion slices range from 9-87% with a mean percentage\nbetween 50% and 60% ( p < 0.005 with 95% confi-\ndence). This means that any test that exercises any\nvariable in a cluster will satisfy every variable in that\ncluster and a tester may be assured that all the vari-\nables in the cluster are covered by one test. Test se-\nlection is tremendously simplified by this clustering\nfeature of decomposition slicing.\nFigure 1.5 illustrates the reductions that are possi-\nble. The graphs in Figure 1.5 are decomposition slice\ngraphs of a differencing program. The nodes are par-\ntially ordered by set inclusion. A lower node is prop-\nerly contained in any node above it that is is con-\nnected to. The graph on the left has 77 nodes and\n271 edges, from a source size of about 700 lines. The\ngraph quickly grows into unwieldy proportions. The\ngraph on the right of Figure 1.5 is the reduced graph\nshowing only computationally equivalent nodes, with\nonly 34 nodes and 43 edges. Another advantage to the\n(visual) reduction is the easy estimate of the size do\nthe change and regression test effort. Any node that\nhas a change must have all the nodes that contain it\n(i.e., are above it and connected in the graph) tested\nalso [8].\nUnlike decomposition slicing, other slicing ap-\nproaches do not identify all additional statements (i.e.\nthe dependent part) of the slice that is related to the\noriginal change. Consequently these other slicing ap-\nproaches are all unsafe as inclusion techniques. Using\nslicing as an exclusion technique is automatically safe.\nThe advantage of a decomposition slicing based ap-\nproach for test exclusion is two-fold: any variable that\nis in the complement part is automatically excluded,\nas are all the variables in its computationally equiva-\nlent set.\n3 Extension\nChanges are typically small with respect to the size\nof the entire system. The natural inference is that the\nmr-test set is also proportionally small. As the set of\nnmr-tests is thus proportionally large, even small ad-\nditions to its size will have a large payoff. Reducing\nthe number of regression tests, by excluding ones that\nwill show no change, can have immediate practical im-\npact.\n3.1 Example\nWe start with an example. Suppose we have two\nways of obtaining regression tests. Technique M se-\nlects tests to be run and so is an inclusive technique.\nTechnique N omits tests that do not need to be run\nand so is an exclusive technique.\nNow suppose test suite T contains 1000 tests; for\nchange from P to P\u2032, suppose there are 200 mr-tests\nand 800 nmr-tests. If regression test selection tech-\nnique M selects 150 of the 200 mr-tests, its inclu-\nsiveness has a safety measure of 150\/200 = 0.75; if\na regression test selection technique excludes N 700\nnmr-tests, it has a precision measure of 700\/800 =\n0.875.\nIt is safe to run the 300 tests that are not excluded\nby N. It is not inclusive in the RH96 measurement\nsense; in this case the RH96 inclusive measure cannot\nbe applied because we do not actually know that there\nare 200 mr tests. If we somehow has the knowledge of\nthe actual number of mr tests the ratio would be > 1,\nnot possible in the RH96 framework. The true cost is\nrunning the 100 extra tests that we know to be nmr.\nA cost \/ benefit calculation will determine whether or\nnot this is suitable.\nSuppose each technique is increased by 10%. We\nnow have 165 mr-tests and 770 nmr-tests. The safety\nof M is 165\/200 = 0.825. this safety measure is still\nunknown to us, for the exact number of mr-tests is not\nknown. With 770 nmr-tests, the precision measure of\nN is given by 770\/800 = 0.9625. If we now use the\nFigure 1.5.\nsafe 230 test not excluded by N as a regression test\nsuite, our cost drops significantly, but we still don\u2019t\nknow exactly how much.\nIn general, let t be total number of tests, n be the\nnumber of mr-tests, s be the number of nmr-tests; let m\nbe the number of mr-tests found by inclusive technique\nM and let r be the number of nmr-tests found by\ntechnique exclusive technique N. While mr- and nmr-\ntests are always arithmetically complementary, safety\nand precision are precisely related only when they are\nequal to 1. When a selection technique is safe, m = n;\nwhen an exclusion technique is precise, r = s. That\nis, a safe selection technique is precise and a precise\nselection technique is safe. Also note that when n and\ns are known, the regression testing problem is solved.\nIndeed, the problem is finding that partition.\n3.2 New Measures\nFigure 2 shows a simplified diagram of the RH96\nframework. Figure 2 shows all possible regression tests\nare decomposed into all modification revealing (MR)\nand all non-modification revealing (NMR) tests. An\ninclusive selection technique (M) identifies a sub-set\nof modification revealing tests (mr). Inclusiveness is\ncalculated as mr \/ MR and safe inclusive test sets\nare when mr = MR. Similarly a exclusion-based se-\nlection technique (N) will find a sub-set of all non-\nmodification revealing tests (nmr). RH96 identifies\nonly modification revealing test sets they do not fur-\nther decompose the measurement of precision.\nThe premise of the measures and crucial to the ex-\nample above is the a priori knowledge of m and n, the\nactual number of mr- and nmr-tests. If these values\nare not known, how can the precision and safety be\nestimated?\n\u0002\n\u0003\n\u0004\n\u0005\n\u0002\n\u0003\n\u0004\u0006\n\u0007\n\b\n\t\nMR NMR\nmr nmr\nALL TESTS\n\u0002\n\u0002\n\u0002\n\u0002\n\u0002\n\u0002\n\u0002\n\u0002\u0003\n\u0004\n\u0004\n\u0004\n\u0004\n\u0004\n\u0004\u0005\nIncluded by technique M\nExcluded by technique N\nFigure 2: The set of all tests is partitioned into MR\nand NMR sets. A particular inclusion or exclusion\ntechnique does not find all the tests that should be\nincluded \/ excluded.\nTo estimate safety and precision when m and n are\nnot known, we propose using exclusion to measure an\ninclusive technique\u2019s safety, and inclusion to measure\nan exclusive technique\u2019s precision.\nThus we define the safety of inclusive technique M\nwith respect to exclusive technique N as the ratio of the\nnumber of tests found by inclusive technique M to the\nnumber of tests that exclusive technique N does not\nfind. We call this e-safety(M,N).\nIn the example above, inclusive technique M se-\nlects 150 tests; exclusive technique N misses 300 tests.\nThus e-safety(M,N) is 150\/300 = 0.5\nSimilarly, we define i-precision(N,M) as the ratio of\nthe number of tests found by exclusive technique N to\nthe number of tests that inclusive technique M does\nnot find. In the example, i-precision(N,M) is 700\/850\n= 0.82. An e-safe technique is safe when N is precise;\nan i-precise technique is precise when M is safe.\nContinuing with the example, when inclusive tech-\nnique M finds 10% more (15) mr-tests e-safety(M,N)\nis 165\/300 = 0.55; and i-precision(N,M) is 700\/815 =\n0.86. When exclusive technique N finds 10% more\n70 nmr-tests e-safety(M,N) is 150\/230 = 0.65,; i-\nprecision(N,M) is 770\/850 = 0.91.\nThe examples show an application of Amdahl\u2019s law.\nBy improving the actual value of nmr, the number of\nnon-modification-revealing tests, we can have an com-\nputable upper bound on MR, the (unknown) number\nof modification-revealing tests. What we have is a\npractical way, an engineering way, a way to work safely\nin the presence of unknown data. Referring back to\nFigure 2, the technique and the associated measures,\nare trying to enlarge the \u201cbubble\u201d that is labeled nmr.\n4 A New Technique\nAn exclusion-based approach is likely to be more ef-\nfective than an inclusion-based approach in two ways.\nFirst, in terms of safety, as it is possible to more con-\nfidently identify all non-modification revealing tests\nwhere it is not possible to confidently identify all mod-\nification revealing tests. Second, in terms of the im-\npact of this approach, as changes are typically small\nwith respect to the size of the entire system, conse-\nquently the modification revealing test set is also pro-\nportionally small. As the set of non-modification re-\nvealing tests is thus proportionally large, even small\nadditions to its size will have a large payoff. Reducing\nthe number of regression tests, by excluding ones that\nwill show no change, can have immediate practical im-\npact. Thus, determining ways to exclude tests should\nhave a large payoff.\nThe technique is reasonably straight-forward:\n1. Decompose and Reduce System Version n. Con-\nstruct the decomposition slices for the system un-\nder consideration. Note that the decomposition\nslice graph does not need to be constructed (al-\nthough it would be nice to have); we only need the\ndecomposition slices and the slices equivalences.\n2. Match Tests with Code. Use techniques like those\nof Vokolos and Chen to connect test cases to de-\ncomposition slices. Note that this yields both and\ndecomposition of the test cases, just as the slices\nform a decomposition; an amalgamation of of\nequivalent test cases are also obtained by match-\ning the test cases with the equivalent decomposi-\ntion slice cluster.\n3. Decompose and Reduce System Version n+1. As\nin step 1. The systems will readily compare. Ob-\ntain the tests for decomposition slice clusters that\nremain unchanged.\n4. Use tests that remain after removing those ob-\ntained in step 3. Any tests for unchanged code\nare not needed.\nThe advantage to the tester of partitioning both the\ncode and the tests should not be underestimated. A\ntest that satisfies any testing criteria for one element\nin the decomposition slice cluster will satisfy the same\ncriteria em for every slice in the cluster.\nWhat we have done is similar to Vokolos in that we\nmatch test to code, but in our case, we use the compu-\ntational element embodied in the decomposition slice.\nThe advantage of slicing is a clear win here. A code\nblock may be changed that has a ripple effect out into\nthe computation contained in another file. Slicing will\nlocate the affect. The same argument applies to the\ntechniques of Chen.\nWe use slicing in a much simpler way than Bink-\nley. We do not need to roll-out the code and look for\nsemantic differences. We can statically examine the\ncode (Binkley also does his analysis statically), per-\nform some elementary set operations, and have the\nresult.\n4.1 Measuring\nUsing this method to obtain regression meshes\nnicely with measurement concepts presented is Sec-\ntion 3.2. We know how many test we have; we can\ncount the size of the set of tests that we do not use;\nthis set is nmr in Figure 2. We can measure the e-\nsafety of any other selection technique with respect to\nwhile ( ++x < 0 ) while ( ++x < 0 )\n{ {\nwhile ( ++x < 0 ) while ( ++x < 0 )\n{ {}\nwhile ( ++x < 0 ) while ( ++x < 0 )\n{} {}\nwhile ( ++x < 0 )\n{}\n} }\n} }\nprint ( x ) print ( x )\nFigure 3: \u201cPathological\u201d while statements.\nthat number of nmr-tests. In the technique we have\npresented, we do not even bother to look for mr-tests.\nWe just use what is left as an approximation to MR.\nHowever, with an approximation of MR in hand, we\ncan compare empirically the various regression test se-\nlection techniques.\nOur technique can also be \u201cinverted.\u201d Instead of\nlocating the tests for the unchanged decomposition\nslice cluster and tossing those out of the regression test\nsuite, we can locate the tests for the changed clusters\nand investigate the idea that they are sufficient as a\nregression test suite.\nEssentially, we have invented another kind of inclu-\nsiveness, exclusiveness. As RH96 pointed out and we\nnoted at the end of Section 2.1.\n5 Examples\nRH96 has some examples: some revolve around\nstatement deletion which is a singularly difficult prob-\nlem in regression test selection; others are pathological\nin the sense that they help clarify the technical def-\ninitions. We go through these examples in turn to\ndemonstrate our technique.\nWe begin by noting that the change in noted in\nFigure 1 would have its tests included in the regression\nsuite, because the code has changed. This is just as\nthe techniques of Vokolos and Chen.\nA similar argument applies to the code of Figure 3.\nRH96 notes that both fragments output 1 on input\n0, while on input -2, the left fragment outputs 1 and\nthe right fragment outputs 3. A functional regression\ntest method would be imprecise if the first test were\nselected. Our slice and test method appropriately se-\nlects all tests for this fragment.\nif P then if P then\na = 2 a = 2\nb = 3\nend end\nFigure 4: Adding Code\ncall PutTermGFX()\ncall DrawLine(x,y) call DrawLine(x,y)\nFigure 5: Deleted Function Call\nThe code of Figure 4 nicley illustrates our tech-\nnique. In the (unlikely) scenario that the is the only\nchange to the system is the addition of the assignment\nb = 3, the tests that exercise the fragment on the right\nwould be eliminated, as they can have no effect on b.\nIn decomposition slicing terminology, the fragment on\nthe right properly extend the decomposition slice on\nP; the decompositin slice on a is not involved in the\nchange. It is the maintainers job to see that the new\ncode is correct according to its new specification; this\ntask is not part of regression testing.\nThe code of Figure 5 is interesting. From its evident\ncontext, the pair of decomposition slices for the two\nfunction calls do not interfere. The tests that exer-\ncise the call to PutTermGFX() would not be excluded,\nand therefore run; the tests that exercise the (evi-\ndently) unmodified function DrawLine(x,y)would be\nexcluded, and not used. This is a strict application of\nthe technique; in reality, one can presume that the\nDrawLine(x,y) function accesses information that is\nprocessed by PutTermGFX(). (It\u2019s hard to image a\ndrawing function not accessing the terminal.) Because\nthe the decomposition slices would show the computa-\ntional relationship between these functions, our tech-\nnique would succeed.\nThe example of Figure 6 is a modication to the\ndecomposition slice on a. Tests that exercise the re-\nmaining code on the right are included, as the decom-\nposition slice on a was modified.\n6 Summary and Future Work\nThere are five main areas of novelty in this presen-\ntation:\n1. Exclusion. Previous work in the area has used\ntest inclusion rather than test exclusion as a basis\nfor reducing regression test sets. We believe that\nour novel approach is significantly more effective\nif P then if P then\ndo_something do_something\na = 2\nend end\nif Q then if Q then\ndo_something do_something\nprint a\nend end\nFigure 6: Changed Decompositin Slice\nat reducing the size of regression test sets than\nprevious approaches.\n2. Decomposition slicing. No previous work has\nused decomposition to direct regression testing.\nDecomposition slicing uniquely exploits compu-\ntational equivalence to safely identify reduced\ntestable components. This means that a decom-\nposition slicing directed approach to determining\nexcluded test sets allows more precision in test set\nreduction than previous approaches.\n3. Additions and deletions. Both Additions and\ndeletions are correctly accounted for.\n4. Extended analysis framework. Our work extends\nthe measurement framework proposed originally\nby Rothermel and Harrold in RH96.\n5. New practical metrics. We are able to establish\nan empirical baseline for the comparison of re-\ngression test seletion technique\nThis approach can be used very quickly and easily\nby commercial companies to reduce their regression\ntest sets.\n6.1 Continuing Work\nThe Software-artifact Infrastructure Repository\n(SIR) [4] is a repository of software-related artifacts\ndesigned to support rigorous, controlled experimenta-\ntion with program analysis and software testing tech-\nniques. It was created to support the understanding\nand assessment of software testing and regression test-\ning techniques and as such is an invaluable resource for\nour continuing work. The repository contains many\nJava and C software systems in multiple versions, to-\ngether with supporting artifacts such as test suites,\nfault data, and scripts. It also includes documenta-\ntion on how to use these objects in experimentation,\nsupporting tools that facilitate experimentation, and\ninformation on the processes used to select, organize,\nand enhance the artifacts, and supporting tools that\nhelp with these processes. We will use the SIR as an\nempirical test-bed.\nReferences\n[1] D. Binkley. Semantics guided regression test cost\nreduction. IEEE Transactions on Software Engi-\nneering, 23(8):498\u2013516, 1997.\n[2] Y.-F. Chen, D. Rosenblum, and K.-P. Vo. Test-\ntube: A system for selective regression testing. In\n16th International Conference on Software Engi-\nneering, pages 211\u2013220, 1994.\n[3] M. Cusumano. What road ahead for Microsoft\nand Windows? Commun. ACM, 49(7):21\u201323,\n2006.\n[4] H. Do, S. G. Elbaum, and G. Rothermel. Sup-\nporting controlled experimentation with testing\ntechniques: An infrastructure and its potential\nimpact. Empirical Software Engineering: An In-\nternational Journal, 10(4):405\u2013435, 2005.\n[5] N. Fenton and S. L. Pfleeger. Software Metrics:\na rigorous and practical approach. PWC, Second\nedition, 1997.\n[6] K. Gallagher and D. Binkley. An empirical study\nof computation equivalence as determined by de-\ncomposition slice equivalence. In Proceedings of\nthe 10th Working Conference on Reverse Engi-\nneering, WCRE\u201303, pages 316 \u2013 322, 2003. ISBN:\n0-7965-2087-8.\n[7] K. Gallagher, M. Harman, and S. Danicic. Guar-\nanteed inconsistency avoidance during software\nevolution. Journal of Software Maintenance and\nEvolution: Research and Practice, 15(6):393\u2013415,\n2003. ISSN: 1532-060X.\n[8] K. B. Gallagher and J. R. Lyle. Using program\nslicing in software maintenance. IEEE Trans-\nactions on Software Engineering, 17(8), August\n1991.\n[9] T. Ostrand, E. Weyuker, and R. M. Bell. Pre-\ndicting the location and number of faults in large\nsoftware systems. IEEE Transactions on Software\nEngineering, 31(4):340\u2013355, 2005.\n[10] G. Rothermel and M. J. Harrold. Analyzing re-\ngression test selection techniques. IEEE Trans-\nactions on Software Engineering, 22(8):529\u2013551,\n1996.\n[11] F. Vokolos and P. Frankl. Empirical evaluations\nof the textual differencing regression testing tech-\nnique. In IEEE Conference on Software Mainte-\nnance, 1998, pages 44\u201353, 1998.\n"}