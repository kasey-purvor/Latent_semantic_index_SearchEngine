{"doi":"10.1016\/j.knosys.2007.11.007","coreId":"69323","oai":"oai:eprints.lancs.ac.uk:27223","identifiers":["oai:eprints.lancs.ac.uk:27223","10.1016\/j.knosys.2007.11.007"],"title":"Evolving a dynamic predictive coding mechanism for novelty detection","authors":["Haggett, Simon J.","Chu, Dominique F.","Marshall, Ian W."],"enrichments":{"references":[{"id":989730,"title":"A self-organising network that grows when required. Neural Netw.","authors":[],"date":"2002","doi":null,"raw":null,"cites":null},{"id":990842,"title":"An Introduction to Genetic Algorithms. 6th edn.","authors":[],"date":null,"doi":null,"raw":null,"cites":null},{"id":991497,"title":"Automatic feature selection in neuroevolution. In:","authors":[],"date":"2005","doi":null,"raw":null,"cites":null},{"id":989318,"title":"Detecting novel features of an environment using habiutation. In:","authors":[],"date":"2000","doi":null,"raw":null,"cites":null},{"id":990331,"title":"Dynamic predictive coding by the retina. Nature","authors":[],"date":"2005","doi":"10.1038\/nature03689","raw":null,"cites":null},{"id":991257,"title":"Efficient Evolution of Neural Networks Through Complexification.","authors":[],"date":"2004","doi":null,"raw":null,"cites":null},{"id":989982,"title":"Image compression with neural networks - a survey. Signal Process., Image Commun.","authors":[],"date":"1999","doi":"10.1016\/S0923-5965(98)00041-1","raw":null,"cites":null},{"id":990617,"title":"In: The Synaptic Organization of the Brain. 3rd edn.","authors":[],"date":"1990","doi":null,"raw":null,"cites":null},{"id":990962,"title":"Machine Learning. McGraw-Hill Higher Education,","authors":[],"date":"1997","doi":null,"raw":null,"cites":null},{"id":989017,"title":"Novelty detection: a review part 1: statistical approaches. Signal Process.","authors":[],"date":"2003","doi":null,"raw":null,"cites":null},{"id":988774,"title":"Novelty detection: a review part 2: neural network based approaches. Signal Process.","authors":[],"date":"2003","doi":null,"raw":null,"cites":null},{"id":989449,"title":"On-line novelty detection for autonomous mobile robots. Robotics and Autonomous Systems","authors":[],"date":"2005","doi":null,"raw":null,"cites":null}],"documentType":{"type":null}},"contributors":[],"datePublished":"2008-04","abstract":"Novelty detection is a machine learning technique which identifies new or unknown information in data sets. We present our current work on the construction of a new novelty detector based on a dynamical version of predictive coding. We compare three evolutionary algorithms, a simple genetic algorithm, NEAT and FS-NEAT, for the task of optimising the structure of an illustrative dynamic predictive coding neural network to improve its performance over stimuli from a number of artificially generated visual environments. We find that NEAT performs more reliably than the other two algorithms in this task and evolves the network with the highest fitness. However, both NEAT and FS-NEAT fail to evolve a network with a significantly higher fitness than the best network evolved by the simple genetic algorithm. The best network evolved demonstrates a more consistent performance over a broader range of inputs than the original network. We also examine the robustness of this network to noise and find that it handles low levels reasonably well, but is outperformed by the illustrative network when the level of noise is increased","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:27223<\/identifier><datestamp>\n      2018-01-24T02:50:32Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Evolving a dynamic predictive coding mechanism for novelty detection<\/dc:title><dc:creator>\n        Haggett, Simon J.<\/dc:creator><dc:creator>\n        Chu, Dominique F.<\/dc:creator><dc:creator>\n        Marshall, Ian W.<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        Novelty detection is a machine learning technique which identifies new or unknown information in data sets. We present our current work on the construction of a new novelty detector based on a dynamical version of predictive coding. We compare three evolutionary algorithms, a simple genetic algorithm, NEAT and FS-NEAT, for the task of optimising the structure of an illustrative dynamic predictive coding neural network to improve its performance over stimuli from a number of artificially generated visual environments. We find that NEAT performs more reliably than the other two algorithms in this task and evolves the network with the highest fitness. However, both NEAT and FS-NEAT fail to evolve a network with a significantly higher fitness than the best network evolved by the simple genetic algorithm. The best network evolved demonstrates a more consistent performance over a broader range of inputs than the original network. We also examine the robustness of this network to noise and find that it handles low levels reasonably well, but is outperformed by the illustrative network when the level of noise is increased.<\/dc:description><dc:date>\n        2008-04<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:relation>\n        http:\/\/dx.doi.org\/10.1016\/j.knosys.2007.11.007<\/dc:relation><dc:identifier>\n        Haggett, Simon J. and Chu, Dominique F. and Marshall, Ian W. (2008) Evolving a dynamic predictive coding mechanism for novelty detection. International Journal of Knowledge-Based and Intelligent Engineering Systems, 21 (3). pp. 217-224.<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/27223\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1016\/j.knosys.2007.11.007","http:\/\/eprints.lancs.ac.uk\/27223\/"],"year":2008,"topics":["QA75 Electronic computers. Computer science"],"subject":["Journal Article","PeerReviewed"],"fullText":"Evolving a Dynamic Predictive Coding\nMechanism for Novelty Detection\nSimon J Haggett\nComputing Laboratory, University of Kent\nCanterbury, UK\nsimon.haggett@acm.org\nDominique F Chu\nComputing Laboratory, University of Kent\nCanterbury, UK\nD.F.Chu@kent.ac.uk\nIan W Marshall\nLancaster Environment Centre, Lancaster University\nLancaster, UK\nI.W.Marshall@lancaster.ac.uk\nAbstract\nNovelty detection is a machine learning technique which identifies new or\nunknown information in large data sets. We present our current work on\nthe construction of a new novelty detector based on a dynamical version\nof predictive coding. We compare three evolutionary algorithms, a simple\ngenetic algorithm, NEAT and FS-NEAT, for the task of optimising the\nstructure of an illustrative dynamic predictive coding neural network to\nimprove its performance over stimuli from a number of artificially gen-\nerated visual environments. We find that NEAT performs more reliably\nthan the other two algorithms in this task and evolves the network with\nthe highest fitness. However, both NEAT and FS-NEAT fail to evolve a\nnetwork with a significantly higher fitness than the best network evolved\nby the simple genetic algorithm. The best network evolved demonstrates\na more consistent performance over a broader range of inputs than the\noriginal illustrative network. We also examine the robustness of this net-\nwork to noise and find that it handles low levels reasonably well, but is\noutperformed by the illustrative network when the level of noise is in-\ncreased.\n1 Introduction\nA novelty detector is a machine learning system that identifies new or unknown\ndata that it was not aware of during its training phase [1]. Novelty detection is\nimportant in practical applications where large data sets are being processed.\nIn a sensor array which continuously monitors an unpredictable environment,\n1\na constant stream of data is produced by each sensor. A large proportion of\nthis data will be redundant since it describes information already known. The\ncrucial information is that which indicates that change has occurred, since this\nmay require some action to take place. Novelty detection can be used in this\ncase to highlight such data.\nBecause novelty detection is an extremely challenging task, there are cur-\nrently a number of different approaches [2]. A comprehensive survey of ap-\nproaches using neural networks is given by [1]. Marsland et al. [3] propose a\nnovelty detector which employs a Habituating Self Organising Map (HSOM). An\ninput layer is connected to a clustering layer which represents the feature space.\nEach input vector is classified by associating it with a neuron in the clustering\nlayer as follows. The neuron in the clustering layer with the smallest distance to\nthe input vector (where distance is defined by the sum of the squared difference\nbetween each element of the input vector and the value held in the given node\nin the clustering layer) fires for that input vector. Each node in the clustering\nlayer is connected to the output neuron via a habituable synapse, which not\nonly weakens (habituates) when the corresponding node in the clustering layer\nfires frequently, but also strengthens (dishabituates) when the node fires infre-\nquently. The value of the output neuron for a given input vector indicates the\nnovelty of that input vector. Habituation enables the network to learn on-line\nand cope with changing environments. However, since the size of the network\nremains fixed one limitation is that on-line learning can cause saturation in the\nnetwork. This is when all synapses habituate, resulting in novel input vectors\nbeing misclassified as normal [4]. Marsland et al. [5, 4] extend this work by\nproposing the \u2019Grow When Required\u2019 (GWR) network as an improvement to\nthe HSOM. The GWR network is a new type of clustering map that allows new\nnodes to be created as required, thus overcoming the problem of saturation seen\nin the HSOM.\nPredictive coding is a technique used in areas such as image and speech\ncompression [6]. In image compression, this technique attempts to predict the\nvalue of a given pixel based on the values of neighbouring pixels. A difference\nsignal, holding the difference between the predicted and observed values of the\ngiven pixel, is then used to represent that pixel. When the predicted value is\nclose to the observed value, this difference signal will have a smaller magnitude,\nwhich means that it can be represented more compactly. In turn, this allows\nthe image as a whole to be represented in a compressed form. If we assume\nthat novel values are likely to be unpredictable, then the difference signal can\nbe used to determine the novelty of the observed value.\nHosoya et al. [7] propose a possible neural network model of circuits in the\nretina. This model performs a dynamical version of predictive coding in that\nit adapts online to the changing visual scene. As animals move through their\nenvironment, they tend to encounter visual scenes which differ strongly in their\nstatistical properties. For example, the visual scene in a woodland environment\nis likely to have strong correlation between vertically separated points and weak\ncorrelation between horizontally separated points, whilst the visual scene in\na sandy environment, such as a desert or beach, is likely to have correlation\n2\nbetween points only in small localities. By adapting to the changing image\nstatistics, a predictive coder is able to maintain its efficiency as the visual scene\nchanges.\nWe wish to develop a new novelty detector which is based on dynamic pre-\ndictive coding. This form of predictive coding is a promising model to base a\nnovelty detector on because of its capability to learn on-line the current norm\nconditions and adapt to changes in these conditions. Unlike Marslands GWR\n[4, 5], which is also capable of on-line learning, this method of novelty detection\nwill learn the statistical relationships between values and report novelty when\nthose relationships change. Inputs are classified depending on the statistical\nrelationships between their elements, as opposed to Euclidean distance to a fea-\nture prototype. Therefore, inputs which are represented by multiple classes in\nGWR may be represented by a single class in our proposed approach.\nIn this paper, we present our current work on the construction of a new\nnovelty detector based on dynamic predictive coding. We compare three evolu-\ntionary algorithms for the task of evolving a neural network structure to give an\noptimal performance over stimuli from a number of artificially generated visual\nenvironments. We also examine the robustness of the evolved structure when\nnoise is introduced to its inputs. The remainder of this paper is organised as fol-\nlows. In section 2, we describe a neural network capable of dynamic predictive\ncoding and identify a limitation of this network. Section 3 describes three evo-\nlutionary algorithms used to search for a neural network structure which gives\nan optimal performance according to two basic criteria. In section 4, we present\nexperimental results from using these algorithms and examine the robustness of\nthe best evolved network to random noise. Section 5 discusses these results and\nthe comparative performance of the evolutionary algorithms. Finally, section 6\nconcludes this paper and briefly outlines how we plan to continue this work.\n2 Novelty Detection using Dynamic Predictive\nCoding\nThe neural network model proposed by Hosoya et al. [7] is a feedforward network\nwhich represents a neural circuit found in the retina [8]. In this model, input\nneurons connect to output neurons via fixed weight synapses and\/or modifiable\nsynapses. The fixed-weight synapse from input xj to output yi is represented\nas bij , and the modifiable synapse is represented as aij . The output yi of the\ni-th output neuron is given by equation 1. At each output neuron, the network\nattempts to predict the sum of the inputs received through fixed synapses and\nsubtract this prediction from the neurons input. The modifiable synapse weights\nare modulated according to the anti-hebbian learning rule shown in Equation 2\nto form this prediction.\nyi =\n\u2211\nj\n(bij + aij)xj (1)\n3\ndaij\ndt\n=\n\u2212aij \u2212 \u03b2 \u3008yixj\u3009\n\u03c4\n\u03c4, \u03b2 > 0 (2)\n\u3008yixj\u3009 is the time-averaged correlation between input xj and output yi and\nis sampled over m previous time steps up to the current time step t:\n\u3008yixj\u3009 =\n1\nm\nt\u2211\nk=t\u2212m\nyi (k)xj (k) (3)\nWhere yi (k) and xj (k) are the values of yi and xj respectively at time\nk. This anti-hebbian learning rule causes the modifiable synapses to weaken\nwhen the activity at the presynaptic and postsynaptic neurons is correlated and\nstrengthen when the activity is anti-correlated. The parameters \u03b2 and \u03c4 control\nthe networks sensitivity to the correlation signal and the rates of learning and\ndecay respectively.\nTo illustrate this model, the following single layer example neural network\nwas given by [7]. Consider a 4x4 pixel greyscale image where each pixel pro-\nvides a single input to the network. The network has a single output neuron\nwhich aims to predict the sum of the centre 2x2 pixels given the correlational\nrelationships between those pixels and the neighbouring \u2019surround\u2019 pixels. All\npixels are connected to the output neuron by modifiable synapses. In addition,\nthe 2x2 centre pixels are also connected to the output neuron via fixed-weight\nsynapses with weight 1. In the networks initial state, each modifiable synapse\nhas a weight of 0, meaning that the output of the network is initially the sum\nof the centre 2x2 pixels. Over time, the modifiable synapses have their weights\nupdated by the anti-hebbian learning rule (Equation 2) such that a prediction\nof the sum of the 2x2 pixels is formed and subtracted from the observed sum.\nHosoya et al. [7] demonstrate the operation of this illustrative neural net-\nwork over a number of artificially generated visual \u2019environments\u2019. Four of\nthese environments were flickering greyscale images with perfect correlational\nrelationships and one of these environments, titled \u201crandom\u201d, was a flickering\nenvironment with no correlation between pixels. The four environments with\nperfect correlational relationships were a flickering uniform field, a flickering\ncheckerboard pattern and flickering vertical and horizontal bars. Each pixel was\nupdated every u timesteps with an independently drawn value from a standard\nnormal distribution. Finally, a \u2019none\u2019 environment was used which was defined\nas a steady grey screen. Figure 1 illustrates the environments. To analyse the\nperformance of the network, Hosoya et al. [7] observed how the sensitivity of\nthe output neuron to the uniform, checkerboard, vertical bar and horizontal bar\nenvironments varied during the simulation. Sensitivity of the output neuron y\nto a given environment E is defined by [7] as the square root of the averaged\nvariance of y, taken over stimuli from environment E (equation 5 in section 3).\nWe use this example network as the basis for a novelty detector utilising\ndynamic predictive coding. To improve our understanding and identify any\nlimitations, we constructed a simulation in which we observed how this network\nresponded when showed a series of visual environments. In this simulation, each\n4\nFigure 1: The artificial environments specified in [7].\nenvironment was shown for 2500 time steps. After every time step, the weights\nof the network were frozen and the variance of the output neuron var(y) sampled\nfor stimuli from each environment. After every 100 time steps, these variances\nwere averaged and the sensitivity to each environment calculated. We used a\nvalue of 0.4 for \u03b2 and a value of 500 for \u03c4 , both determined experimentally.\nThe parameter u was set to 1 to give maximum flicker. The simulation was\nimplemented in GNU C, using the GNU Scientific Library (GSL).\nWe also introduced two new diagonal environments (shown in figure 2) and\nobserved how the network responded to these. As with the existing environ-\nments, each diagonal environment was shown to the network for 2500 time\nsteps.\nFigure 2: The new diagonal environments.\nTo verify our implementation of this example network, we first tested it with\nthe original environments used in [7]. Figure 3 illustrates how the networks\nsensitivity to the uniform environment varies during the time course of the\nsimulation. In this and subsequent graphs, sensitivity is scaled such that a\nsensitivity of 1 is defined by the standard deviation of the output of the network\nin its unadapted state. The network was shown each environment in the order\nimplied by the horizontal axis.\nWhen the network was shown the \u201crandom\u201d environment, its sensitivity\nto the uniform environment fell slightly. This behaviour was observed for all\nenvironments and is also demonstrated in [7]. When the network adapted to\nthe uniform environment, its sensitivity to that environment fell considerably.\n5\nFigure 3: The sensitivity graph produced by the illustrative neural network\ngiven in [7]. This shows how sensitivity to the uniform environment varies\nduring the time course of the simulation. A sensitivity close to zero indicates\nthat the environment is known and that the output of the network is small.\nConversely, a sensitivity close to one indicates that the environment is novel\nand that the output is close to the sum of the intensities in the centre patch.\nIn this state, the network considers the uniform environment to be known and\ntherefore not novel. When the network was subsequently allowed to adapt\nto the checkerboard environment, its sensitivity to the uniform environment\nrecovered. The network forgets about the uniform environment and classifies it\nas novel again. Figure 4 illustrates how the networks sensitivity to all original\nenvironments varied through this simulation.\nWe also performed a simulation using the diagonal environments. Figure\n5 shows how the networks sensitivity varied through the simulation. As the\nnetwork adapts to the uniform, checkerboard, vertical bar and horizontal bar\nenvironments, its sensitivity to the diagonal environments falls to approximately\n0.75. As the network adapts to one diagonal environment, the sensitivity to the\nother diagonal environment rises above 1.0. Here, the output of the network\nis greater than the sum of the centre 2x2 patch. Since the goal of predictive\ncoding is to compress the observed value, this behaviour is undesired.\nTo explain this result, we considered the original network when adapted to\na non-diagonal environment. In this case, sensitivity to diagonal environments\nis reduced because similarities between the diagonal and non-diagonal environ-\nments are used to form a partial prediction (this can also be seen vice-versa\nas the network adapts to the diagonal environments). If the original network\nis adapted to a diagonal environment, sensitivity to the alternative diagonal\nenvironment rises above 1. This was caused by a limitation of the network in\nhandling symmetry between environments.\n6\nFigure 4: The sensitivity graph produced by the illustrative neural network given\nin [7]. This shows how sensitivity to the uniform, checkerboard, vertical bar and\nhorizontal bar environments vary as the network adapts to each environment\nduring the time course of the simulation.\n3 Optimising Structure using Evolution\nTo optimise the neural network, we searched for a structure which gave the best\nperformance according to two basic criteria of novelty detection. We wished\nto find a solution which (a) maximises the difference in sensitivity between\nknown and novel environments and (b) remains at a similar level of sensitivity\nfor all novel environments. We first attempted to construct a new structure\nby hand but this proved to be time consuming and none of the networks we\ndeveloped gave any significant improvement in performance. We then considered\na genetic algorithm (GA) based approach, since such an approach is good in\ncases when the search space is not well understood [9]. We compared three GA\u2019s\nfor this task; a simple textbook-based genetic algorithm, and two neuroevolution\nmethods which are specially designed to evolve neural network structure. All\nGA\u2019s were implemented using GNU C and GSL.\nThe simple genetic algorithm is based on that described by [10]. Each gene\nis represented by the quadruple (inID, outID, weight, type), which is in turn\nencoded as a 25 character bitstring. A genome holds a collection of these genes\nand thus has a connection-centric view of the neural network. Point mutation\nand single-point crossover are both used, as well as a \u2019gene-replicate\u2019 and \u2019gene-\nremove\u2019 mutation (to allow the addition or removal of new structural elements).\nThe first neuroevolution method used was NeuroEvolution of Augmenting\nTopologies (NEAT), proposed by Stanley [11]. NEAT is specifically designed\nto evolve neural networks. New structure is introduced gradually so as min-\nimise the dimensionality of the search space. Speciation is used to encourage\ndiversity and help prevent bloat from occurring (solutions containing unneces-\n7\nFigure 5: The sensitivity graph produced by the illustrative neural network. In\naddition to the environments seen in figure 4, this graph shows the networks\nresponse to the left diagonal and right diagonal environments.\nsary elements). In crossover, NEAT uses historical markings to discover which\ngenes in two genomes match and which do not, solving the competing conven-\ntions problem (whereby two identical solutions have different genetic represen-\ntations). The competing conventions problem is important to solve because\ncrossover of similar solutions with different representations is likely to produce\ndamaged offspring [11].\nWe also used a variation of NEAT proposed by Whiteson et al. [12], Feature\nSelective NEAT (FS-NEAT). Unlike NEAT, which usually starts with a pop-\nulation of fully connected networks, FS-NEAT starts with a population where\neach network has only a single connection between a randomly chosen input\nand output. This allows FS-NEAT to begin its search in a space of an even\nlower dimensionality. FS-NEAT then proceeds in the same manner as NEAT.\nExperiments conducted in an autonomous car racing simulation showed that\nFS-NEAT was capable of outperforming NEAT in evolving solutions that both\nscored higher and were also less complex in terms of their structure [12].\nAll three approaches use the same measure of fitness, based on our perfor-\nmance criteria stated above. We define the following fitness function over N\nenvironments (not including \u2019none\u2019 or \u2019random\u2019):\nF (c) =\nN\u2211\ni=1\n1\nN \u2212 1\n\uf8eb\n\uf8ed\nN\u2211\nj=1, j 6=i\nSc(i, j)\n\uf8f6\n\uf8f8\u2212 Sc(i, i) (4)\nThe sensitivity of a network to a given environment E is defined as the\nsquare root of the variance of the output neuron y averaged over stimuli from\nenvironment E [7]:\nSE =\n\u221a\n\u3008var(y)\u3009E (5)\n8\nWe then define Sc(i, j) as the sensitivity of candidate network c to envi-\nronment j when adapted to environment i. To encourage sensitivity to novel\nenvironments to remain at a similar level, candidates which allow sensitivity to\nany environment to rise above 1 should be punished. However, such candidates\nshould not simply be awarded zero fitness since they may yet evolve into good\nsolutions. Also, sensitivity values greater than 1 that have a small distance from\n1 should be awarded a higher fitness than those with a large distance. From\nexperimentation, we found that the following non-linear adjustment gave the\nbest results:\nSc(i, j) =\n{\n2.0\u2212 Sc(i, j)4 Sc(i, j) > 1.0\nSc(i, j) otherwise\n(6)\nAfter this adjustment, Sc(i, j) may be negative. However, the lowest fitness\nthe network can achieve when adapted to a single environment i is constrained\nto 0. Thus, a network performing badly when adapted to one environment but\nnot when adapted to another is punished for its poor performance only.\nWhen normalising sensitivity for these networks, we maintain the maximum\nsensitivity value defined earlier, i.e. the square root of the variance of the sum\nof the 2x2 centre patch.\n4 Results\nFigure 6: The sensitivity graph produced by the best network found by NEAT.\nTable 1 shows how the sensitivity of the best network evolved by the simple\nGA varies when it is unadapted (only shown the \u201cnone\u201d environment) and\nwhen it is adapted to the environments with perfect correlational relationships.\nTables 2 and 3 show these results for the best networks evolved by NEAT and\nFS-NEAT respectively. The highest scoring network overall was that found\n9\nEnvironment Sensitivity To\nAdapted Uniform Checker Vertical Horizontal LDiag RDiag\nNone 0.831 0.829 0.845 0.830 0.835 0.850\nUniform 0.134 0.785 0.884 0.783 0.697 0.976\nChecker 0.792 0.129 0.796 0.895 0.996 0.704\nVertical 0.881 0.777 0.134 0.790 0.700 0.974\nHorizontal 0.793 0.899 0.796 0.118 0.983 0.700\nLDiag 0.697 0.987 0.698 0.970 0.138 0.415\nRDiag 0.987 0.692 0.996 0.691 0.390 0.102\nTable 1: Sensitivity of the best network evolved by the simple GA to the uni-\nform, checkerboard, vertical bar, horizontal bar and diagonal environments as\nthe network adapts to each environment. The sensitivity to the adapted envi-\nronment is highlighted in bold.\nEnvironment Sensitivity To\nAdapted Uniform Checker Vertical Horizontal LDiag RDiag\nNone 0.871 0.854 0.873 0.853 0.857 0.854\nUniform 0.127 0.902 0.894 0.896 0.988 0.605\nChecker 0.893 0.113 0.897 0.905 0.997 0.592\nVertical 0.895 0.875 0.112 0.891 0.612 0.987\nHorizontal 0.900 0.892 0.895 0.117 0.598 1.004\nLDiag 0.987 0.999 0.609 0.599 0.125 0.506\nRDiag 0.610 0.617 0.996 1.006 0.510 0.120\nTable 2: Sensitivity of the best network evolved by NEAT to the uniform,\ncheckerboard, vertical bar, horizontal bar and diagonal environments as the\nnetwork adapts to each environment. The sensitivity to the adapted environ-\nment is highlighted in bold.\nEnvironment Sensitivity To\nAdapted Uniform Checker Vertical Horizontal LDiag RDiag\nNone 0.684 0.687 0.682 0.686 0.683 0.675\nUniform 0.041 1.026 0.661 1.023 0.673 1.005\nChecker 1.031 0.063 1.046 0.675 0.319 0.690\nVertical 0.669 1.002 0.045 0.993 0.683 0.351\nHorizontal 1.009 0.678 1.013 0.042 0.993 0.658\nLDiag 0.672 0.340 0.682 0.990 0.031 0.350\nRDiag 1.013 0.669 0.341 0.663 0.342 0.044\nTable 3: Sensitivity of the best network evolved by FS-NEAT to the uniform,\ncheckerboard, vertical bar, horizontal bar and diagonal environments as the net-\nwork adapts to each environment. The sensitivity to the adapted environment\nis highlighted in bold.\n10\nby NEAT. Figure 6 shows the sensitivity graph produced by this best overall\nnetwork.\nIn each experiment, 10 runs of the GA were executed, with each run per-\nforming 500 generations of evolution and returning the best network evolved\nduring that time. The best network from the 10 runs was taken to be the best\nnetwork for that method. Table 4 shows the average and best performances\nof each GA method along with the average complexity of the solutions found.\nTable 5 shows the average hidden node and synapse count of solutions produced\nby each method.\nMethod Average\nFitness\nStandard\nDeviation\nBest Fitness Worst Fitness\nSimple GA 3.537 1.581 4.122 3.405\nNEAT 4.169 0.245 4.208 4.096\nFS-NEAT 3.708 2.235 4.167 3.532\nTable 4: Average, best and worst fitness observed for each GA method over 10\n500-generation runs.\nMethod Average\nHidden\nNodes\nAverage\nFixed-\nWeight\nSynapses\nAverage\nAdaptive\nSynapses\nUnstim.\nNodes\nIneffective\nHidden\nNodes\nSimple GA 7.7 (9.76) 16.5 (1.9) 4.6 (1.07) 3.1 (4.51) 6.8 (9.17)\nNEAT 3.6 (2.84) 18 (2.87) 8.2 (4.02) 0.0 (0.0) 0.0 (0.0)\nFS-NEAT 8.1 (4.01) 9.7 (4.69) 14.1 (5.34) 0.0 (0.0) 0.0 (0.0)\nTable 5: Average hidden node, synapse count and ineffective\/unstimulated node\ncount for solutions produced by each GA method over 10 500-generation runs.\nNumbers in brackets represent standard deviation. Unstimulated nodes are\nthose which do not receive any input. Ineffective hidden nodes are those which\ndo not influence the output neuron.\nComparing the GA approaches, we can see that the fitness of their best\nnetworks are all at a similar level. The average fitness after 500 generations was\nhighest for NEAT (4.169) with a standard deviation of 0.245, demonstrating a\nmore consistent performance. Looking at the complexity of networks evolved,\nNEAT tended to evolve networks with fewer hidden nodes, but with a similar\naverage synapse count to that seen for the simple GA. FS-NEAT tended to\nevolve networks with more adaptive synapses. The networks evolved by the\nsimple GA also showed evidence of bloat in that they had both nodes which\nhave no inputs (unstimulated nodes) and hidden nodes which do not connect\nto, or influence in any way, the output neuron. However, such obsolete structure\ncan easily be pruned from the network.\n11\nFigure 7: Fitness of the best evolved network as noise is applied to its inputs,\nwith noise factor k varied from 0.0 to 5.0\nIn many practical applications, data is likely to contain a noise component.\nTherefore, a novelty detector used in such applications should be resilient to\nnoise. To investigate this, we tested the performance of the best evolved net-\nwork when independent noise was introduced to each input. The noise com-\nponent was a value drawn independently from a standard normal distribution\nand scaled by a constant factor k. When normalising sensitivity, we took into\naccount the level of noise being added to the stimulus. We varied k from 0.0\nto 5.0, with a step size of 0.2. Figure 7 shows how the fitness of both the orig-\ninal illustrative neural network and the best evolved network varies as noise is\nintroduced. This reduces in a non-linear fashion for both networks as the noise\nfactor k is increased. The best evolved network is able to distinguish between\nnovel and known environments until k = 0.8, after which it struggles to reliably\ndifferentiate between the diagonal environments. At k = 1.6, the best evolved\nnetwork is unable to distinguish between any of the environments. The original\nnetwork is unable to distinguish between environments at k = 2.2.\n5 Discussion\nThe networks evolved by the GA methods show a more consistent handling of\na broader range of environments than either the example neural network given\nby [7] or any of the networks we designed by hand. The network with the high-\nest fitness, found using NEAT, has a complexity similar to that of the example\nnetwork. For most novel environments, this networks sensitivity remains at a\nsimilar level throughout the simulation. Unlike the example network, this net-\nwork does not see a dramatic increase in sensitivity to one diagonal environment\nwhen adapted to the alternative diagonal environment. This is important as it\nensures a more consistent separation between novel and known environments.\n12\nHowever, a drop in sensitivity to the diagonal environments is still observed\nwhen the network is adapted to non-diagonal environments, and vice versa.\nThis is again caused by the network being able to form partial predictions of\nstimuli due to similarities between environments.\nInterestingly, the structure of the best network has demonstrated, from a\npredictive coding standpoint, a change in function. The fixed-weight synapses\nbetween the centre patch inputs and the output neuron have been removed and a\nnew fixed-weight synapse introduced connecting a single peripheral input to the\noutput neuron. Thus, the network has changed from predicting the sum of the\ncentre patch to predicting the selected periphery input. However, from a novelty\ndetection perspective this network retains the intended function of indicating\nthe novelty of a given input vector. The high-scoring candidates from all three\nGA approaches demonstrate similar changes in network structure. This may\nindicate that a network which preserves the function of predicting the sum of\nthe centre patch does not exist.\nA surprising result is the comparatively high best fitness score achieved\nby the simple GA. For this problem, the best network evolved by the simple\nGA was of a similar fitness to the best networks evolved by both NEAT and\nFS-NEAT. This is despite the relative complexity of these two neuroevolution\ntechniques, compared to the simple GA. Improvements can easily be made to\nthe simple GA, such as adding new nodes using a similar method to that used\nby NEAT to reduce the observed bloating of networks. Whilst NEAT has been\ndemonstrated to give a more reliable performance, investigating the effect of\nsuch improvements to the performance of the simple GA would be instructive.\nThe best network presented in section 4 has been shown to cope reasonably\nwell with noise. At k = 0.8, the variance of the noise component was 0.8\ntimes the variance of the signal component. Despite this, the network was\nstill capable of distinguishing between known (adapted) and novel (unadapted)\nenvironments. However, with increasing noise, the network is shown to perform\nworse than the original network in terms of the fitness criteria defined in section\n3. This demonstrates that performance over noisy data should be considered\nby the GA approaches when searching for new neural network structures.\n6 Conclusions and Future Work\nWe have described a neural network used by [7] to illustrate dynamic predictive\ncoding and identified a limitation of this network. For the task of optimising\nthe structure of the network, we have demonstrated that whilst NEAT outper-\nforms two other evolutionary algorithms, it does not produce a solution which\nis significantly better than that produced by a simple genetic algorithm. The\noptimised network evolved by NEAT distinguishes more consistently between a\nbroader range of environments than either the original neural network or any\nof the networks we designed by hand. It also performs well when a low level of\nnoise is introduced but its performance degrades quickly as this noise increases.\nThis is because noise sensitivity was not part of the fitness criteria used by the\n13\nGA approaches. Since we plan to test this approach to novelty detection on\ndata that is inherently noisy, this will need to be addressed in future work.\nIn order to extend this work, we plan to investigate using a neural net-\nwork, capable of dynamic predictive coding, for novelty detection over recorded\nweather data. A vector of metrics would be passed to the network, which would\nthen learn the correlational relationships between those metrics and detect nov-\nelty when these relationships change. We consider it likely that the structure\nof the neural network used will need to be evolved to give optimal performance\nin this task, and this process of evolution will need to take into consideration\nnoise applied to the data.\nReferences\n[1] Markou, M., Singh, S. Novelty detection: a review part 2: neural network\nbased approaches. Signal Process. 2003; 83:2499\u20132521\n[2] Markou, M., Singh, S. Novelty detection: a review part 1: statistical ap-\nproaches. Signal Process. 2003; 83:2481\u20132497\n[3] Marsland, S., Nehmzow, U., Shapiro, J.: Detecting novel features of an en-\nvironment using habiutation. In: Proc. Simulation of Adaptive Behaviour,\nMIT Press, 2000, 189\u2013198\n[4] Marsland, S., Nehmzow, U., Shapiro, J. On-line novelty detection for au-\ntonomous mobile robots. Robotics and Autonomous Systems 2005; 51:191\u2013\n206\n[5] Marsland, S., Shapiro, J., Nehmzow, U. A self-organising network that\ngrows when required. Neural Netw. 2002; 15:1041\u20131058\n[6] Jiang, J. Image compression with neural networks - a survey. Signal Pro-\ncess., Image Commun. 1999; 14:737\u2013760\n[7] Hosoya, T., Baccus, S.A., Meister, M. Dynamic predictive coding by the\nretina. Nature 2005; 436:71\u201377\n[8] Stirling, P.: Retina. In: The Synaptic Organization of the Brain. 3rd edn.\nOxford University Press, 1990, 170\u2013213\n[9] Mitchell, M. An Introduction to Genetic Algorithms. 6th edn. MIT Press,\n1999\n[10] Mitchell, T. Machine Learning. McGraw-Hill Higher Education, 1997\n[11] Stanley, K.O. Efficient Evolution of Neural Networks Through Complexifi-\ncation. PhD thesis, University of Texas at Austin, 2004\n14\n[12] Whiteson, S., Stone, P., Stanley, K.O., Miikkulainen, R., Kohl, N.: Au-\ntomatic feature selection in neuroevolution. In: Proceedings of the 2005\nconference on Genetic and evolutionary computation, New York, NY, USA,\nACM Press, 2005, 1225\u20131232\n15\n"}