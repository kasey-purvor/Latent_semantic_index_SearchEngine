{"doi":"10.3758\/bf03193553","coreId":"66252","oai":"oai:dro.dur.ac.uk.OAI2:2238","identifiers":["oai:dro.dur.ac.uk.OAI2:2238","10.3758\/bf03193553"],"title":"Asymmetric interference between sex and emotion in face perception.","authors":["Atkinson, A.P.","Tipples, J.","Burt, D.M.","Young, A.W."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2005-10-01","abstract":"Previous research with speeded-response interference tasks modeled on the Garner paradigm has demonstrated that task-irrelevant variations in either emotional expression or facial speech do not interfere with identity judgments, but irrelevant variations in identity do interfere with expression and facial speech judgments. Sex, like identity, is a relatively invariant aspect of faces. Drawing on a recent model of face processing according to which invariant and changeable aspects of faces are represented in separate neurological systems, we predicted asymmetric interference between sex and emotion classification. The results of Experiment 1, in which the Garner paradigm was employed, confirmed this prediction: Emotion classifications were influenced by the sex of the faces, but sex classifications remained relatively unaffected by facial expression. A second experiment, in which the difficulty of the tasks was equated, corroborated these findings, indicating that differences in processing speed cannot account for the asymmetric relationship between facial emotion and sex processing. A third experiment revealed the same pattern of asymmetric interference through the use of a variant of the Simon paradigm. To the extent that Garner interference and Simon interference indicate interactions at perceptual and response-selection stages of processing, respectively, a challenge for face processing models is to show how the same asymmetric pattern of interference could occur at these different stages. The implications of these findings for the functional independence of the different components of face processing are discussed","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/66252.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/2238\/1\/2238.pdf","pdfHashValue":"d234633488d691b98c253cfbe87e9ace0dadda44","publisher":"Psychonomic Society","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:2238<\/identifier><datestamp>\n      2017-03-10T15:05:16Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Asymmetric interference between sex and emotion in face perception.<\/dc:title><dc:creator>\n        Atkinson, A.P.<\/dc:creator><dc:creator>\n        Tipples, J.<\/dc:creator><dc:creator>\n        Burt, D.M.<\/dc:creator><dc:creator>\n        Young, A.W.<\/dc:creator><dc:description>\n        Previous research with speeded-response interference tasks modeled on the Garner paradigm has demonstrated that task-irrelevant variations in either emotional expression or facial speech do not interfere with identity judgments, but irrelevant variations in identity do interfere with expression and facial speech judgments. Sex, like identity, is a relatively invariant aspect of faces. Drawing on a recent model of face processing according to which invariant and changeable aspects of faces are represented in separate neurological systems, we predicted asymmetric interference between sex and emotion classification. The results of Experiment 1, in which the Garner paradigm was employed, confirmed this prediction: Emotion classifications were influenced by the sex of the faces, but sex classifications remained relatively unaffected by facial expression. A second experiment, in which the difficulty of the tasks was equated, corroborated these findings, indicating that differences in processing speed cannot account for the asymmetric relationship between facial emotion and sex processing. A third experiment revealed the same pattern of asymmetric interference through the use of a variant of the Simon paradigm. To the extent that Garner interference and Simon interference indicate interactions at perceptual and response-selection stages of processing, respectively, a challenge for face processing models is to show how the same asymmetric pattern of interference could occur at these different stages. The implications of these findings for the functional independence of the different components of face processing are discussed.<\/dc:description><dc:publisher>\n        Psychonomic Society<\/dc:publisher><dc:source>\n        Perception & psychophysics, 2005, Vol.67(7), pp.1199-1213 [Peer Reviewed Journal]<\/dc:source><dc:date>\n        2005-10-01<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:2238<\/dc:identifier><dc:identifier>\n        issn:0031-5117<\/dc:identifier><dc:identifier>\n        issn: 1532-5962<\/dc:identifier><dc:identifier>\n        doi:10.3758\/bf03193553<\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/2238\/<\/dc:identifier><dc:identifier>\n        https:\/\/doi.org\/10.3758\/bf03193553<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/2238\/1\/2238.pdf<\/dc:identifier><dc:rights>\n        \u00a9 Copyright 2005 Psychonomic Society, Inc.<\/dc:rights><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":[" 1532-5962","issn: 1532-5962","issn:0031-5117","0031-5117"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2005,"topics":[],"subject":["Article","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n04 November 2010\nVersion of attached file:\nPublished Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nAtkinson, A.P. and Tipples, J. and Burt, D.M. and Young, A.W. (2005) \u2019Asymmetric interference between sex\nand emotion in face perception.\u2019, Perception psychophysics., 67 (7). pp. 1199-1213.\nFurther information on publisher\u2019s website:\nhttp:\/\/app.psychonomic-journals.org\/content\/67\/7\/1199.abstract\nPublisher\u2019s copyright statement:\nCopyright 2005 Psychonomic Society, Inc.\nAdditional information:\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\nPerception & Psychophysics\n2005, 67 (7), 1199-1213\nThe Components of Face Processing and Their \nFunctional Independence\nA common conception of face perception is that its \ndifferent component processes are to some extent func-\ntionally independent of each other (see, e.g., Bruce & \nYoung, 1986; Ellis, 1989; Young, 1998). There is plenty \nof evidence for this view, from dissociations following \nbrain injury (see, e.g., Campbell, Landis, & Regard, 1986; \nHumphreys, Donnelly, & Riddoch, 1993; Parry, Young, \nSaul, & Moss, 1991; Young, Newcombe, de Haan, Small, \n& Hay, 1993) to the results of speeded judgment tasks \nin normal participants (see, e.g., Bruce, Ellis, Gibling, \n& Young, 1987; Campbell, Brooks, de Haan, & Roberts, \n1996; Young, McWeeny, Hay, & Ellis, 1986). Evidence \nfrom neurophysiological studies of nonhuman primates \nand functional brain-imaging studies in humans indicates \nthat this functional separation is underpinned by anatomi-\ncally separable components of a distributed face-processing \nsystem (see, e.g., Hasselmo, Rolls, & Baylis, 1989; Puce, \nAllison, Bentin, Gore, & McCarthy, 1998; see Haxby, \nHoffman, & Gobbini, 2000, for a review).\nAlthough our appearances change gradually over the \nyears, facial identity is invariant relative to the rapid \nchanges that occur in our faces when we speak or emote. \nIn their model of face perception, Bruce and Young (1986) \nproposed that, subsequent to an initial common stage of \nstructural processing, distinct sets of processes are in-\nvolved in recognizing different aspects of faces, includ-\ning identity, emotion, and speech. More recently, Haxby \net al. (2000) have proposed a model of face processing \n 1199 Copyright 2005 Psychonomic Society, Inc.\nA.P.A. was supported by a Leverhulme Trust Research Fellowship \nand a grant from the McDonnell Project in Philosophy and the Neu-\nrosciences (funded by the James S. McDonnell Foundation; see www\n.sfu.ca\/neurophilosophy). J.T. and A.W.Y. were supported by MRC Co-\noperative Group Component Grant G9812945. We are grateful to Cathy \nBlacklock, Sefton Loftus, Kathryn Jones, and Laura Hallett for helping \nwith data collection, and to the referees for helpful comments on an ear-\nlier version of this article. Correspondence concerning this article should \nbe addressed to A. P. Atkinson, Department of Psychology, University of \nDurham, Science Laboratories, South Road, Durham DH1 3LE, England \n(e-mail: a.p.atkinson@durham.ac.uk).\nAsymmetric interference between sex and \nemotion in face perception\nANTHONY P. ATKINSON\nUniversity of Durham, Durham, England\nJASON TIPPLES\nUniversity of Hull, Hull, England\nD. MICHAEL BURT\nUniversity of Durham, Durham, England\nand\nANDREW W. YOUNG\nUniversity of York, York, England\nPrevious research with speeded-response interference tasks modeled on the Garner paradigm has \ndemonstrated that task-irrelevant variations in either emotional expression or facial speech do not \ninterfere with identity judgments, but irrelevant variations in identity do interfere with expression and \nfacial speech judgments. Sex, like identity, is a relatively invariant aspect of faces. Drawing on a recent \nmodel of face processing according to which invariant and changeable aspects of faces are represented \nin separate neurological systems, we predicted asymmetric interference between sex and emotion \nclassification. The results of Experiment 1, in which the Garner paradigm was employed, confirmed \nthis prediction: Emotion classifications were influenced by the sex of the faces, but sex classifications \nremained relatively unaffected by facial expression. A second experiment, in which the difficulty of \nthe tasks was equated, corroborated these findings, indicating that differences in processing speed \ncannot account for the asymmetric relationship between facial emotion and sex processing. A third \nexperiment revealed the same pattern of asymmetric interference through the use of a variant of the \nSimon paradigm. To the extent that Garner interference and Simon interference indicate interactions \nat perceptual and response-selection stages of processing, respectively, a challenge for face process-\ning models is to show how the same asymmetric pattern of interference could occur at these different \nstages. The implications of these findings for the functional independence of the different components \nof face processing are discussed.\n1200    ATKINSON, TIPPLES, BURT, AND YOUNG\nthat shares some elements of Bruce and Young\u2019s model \nbut fits the various components of face processing into \ntwo broadly defined streams. On the basis of neuroimag-\ning and electrophysiological research, Haxby et al. pro-\nposed one route, leading from inferior occipital cortex \nto inferotemporal cortex, in which relatively invariant \naspects of faces are represented, and another route, lead-\ning from inferior occipital cortex to superior temporal \ncortex, in which changeable aspects of faces resulting \nfrom movement of the facial features are represented. (In \nthis article, we shall use the general term expressions as \nshorthand for the changes in the shapes of facial features \ncreated by muscle movements involved in talking and \nemoting.) These two processing streams are regarded as \nbeing largely functionally independent of each other, al-\nthough Haxby et al.\u2019s account of the model does allow for \na modicum of interaction. Furthermore, the model posits \nthat specific face perception functions are performed by \nthe coordinated interaction of each of these two streams \nwith other neural regions. For example, the perception of \nfacial emotion involves the occipital to superior temporal \nstream operating in concert with regions involved in rep-\nresenting emotional information, such as the amygdala \nand the insula, whereas the perception of identity involves \nthe occipital to inferotemporal stream operating in con-\ncert with more anterior temporal regions.\nFace Processing Interference With the Garner \nParadigm\nThe results of some studies in which a speeded-judgment \ninterference task was employed have been interpreted as \nindicating that certain components of face processing can \ninteract and so are not completely independent. Using the \nGarner paradigm (Garner, 1976), Schweinberger and col-\nleagues found that facial identity judgments remain unaf-\nfected by task-irrelevant variations in either facial expres-\nsion or facial speech, yet variations in facial identity did \ninfluence judgments about facial expression and facial \nspeech (Schweinberger, Burton, & Kelly, 1999; Schwein-\nberger & Soukup, 1998; see also Baudouin, Martin, Ti-\nberghien, Verlut, & Franck, 2002).\nIn the Garner paradigm, participants are required to \nclassify exemplars of a stimulus, such as a face, along a \nparticular dimension while a second, task-irrelevant di-\nmension of the stimulus is varied. The stimuli are presented \nin three different conditions, defined by the relationship \nbetween the task-relevant and task-irrelevant dimensions. \nIn the control condition, the irrelevant dimension is held \nconstant across variations in the relevant dimension. For \nexample, all faces might express the same emotion when \nthe task is to judge identity, and all faces might be of the \nsame person when the task is to judge facial expression. \nIn the orthogonal condition, both the irrelevant and rel-\nevant dimensions are varied (e.g., two people express each \nof two different emotions). In the correlated condition, \nchanges in the irrelevant dimension covary with changes \nin the relevant dimension (e.g., Person A always expresses \nfear and Person B always expresses happiness). The logic \nof the Garner paradigm is that a difference in response \ntimes (RTs) to the stimuli across these three conditions is \nevidence that the processing of the different stimulus di-\nmensions is not independent; in other words, it is evidence \nthat participants are unable to pay selective attention to \none dimension without the other dimension\u2019s influencing \ntheir responses. The strongest evidence of independence \nis longer RTs in the orthogonal than in the control condi-\ntion. Shorter RTs in the correlated condition than in the \ncontrol condition, indicating facilitation of classification \n(\u201credundancy gain\u201d), are consistent with but not strongly \nindicative of nonindependent or integral processing (for \ndiscussion, see Eimas, Tartter, Miller, & Keuthen, 1978; \nGreen & Kuhl, 1991; Schweinberger & Soukup, 1998).\nIn Schweinberger and Soukup\u2019s (1998) first two ex-\nperiments, participants classified computer-presented \nphotographs of faces with respect to either identity (Per-\nson A or Person B) or expression (happy or sad) while \nignoring the other, irrelevant dimension. RTs for identity \njudgments did not differ across the control, orthogonal, \nand correlated conditions. RTs for expression judgments, \non the other hand, were significantly different across \nthe three conditions; specifically, RTs for the correlated \ncondition were reliably shorter than those for the control \ncondition, which in turn were reliably shorter than RTs \nfor the orthogonal condition. In subsequent experiments, \nSchweinberger and Soukup found a similar pattern of re-\nsults when identity and facial speech judgments were com-\npared. Identity judgments were not affected by variation \nin facial speech postures; however, when judging facial \nspeech, participants were reliably faster in the correlated \ncondition than in the control condition and reliably faster \nin the control condition than in the orthogonal condition. \nThese asymmetric dependencies between the processing \nof facial identity, emotion, and speech indicate that ob-\nservers are able to attend and respond to the identity of \nfaces while ignoring emotional and speech expressions, \nbut they are unable to ignore identity when attending and \nresponding to either emotional expression or speech pos-\nture (Schweinberger & Soukup, 1998).\nAn alternative explanation for Schweinberger and \nSoukup\u2019s (1998) findings is that the interference effect \nis due simply to a difference in the processing speeds of \nthe two types of information, as is indicated by different \noverall RTs for the two tasks. Such a speed-of-processing \naccount predicts that faster tasks can interfere with slower \ntasks, but the opposite is not true (see the introduction to \nExperiment 2 for a more detailed discussion). This ex-\nplanation is consistent with Schweinberger and Soukup\u2019s \nresults insofar as participants were faster to classify iden-\ntity than they were to classify emotion (a finding that is \nalso consistent with electrophysiological evidence that \nidentity processing occurs substantially earlier than ex-\npression processing: M\u00fcnte et al., 1998). In order to test \nthis alternative explanation, Schweinberger et al. (1999) \nmanipulated the difficulty of identity- and emotion-judg-\nment tasks by employing a computer morphing technique \nto create face stimuli that blended, to varying degrees, \nASYMMETRIC INTERFERENCE IN FACE PERCEPTION    1201\neither the identities of two faces within a given emotion or \ntwo emotions (happy and angry) within a given identity. \nDifficult stimuli were defined as 71:29 and 29:71 blends \nacross the relevant dimension, and easy stimuli were de-\nfined as the endpoints of these continua (100:0 and 0:100). \nThe irrelevant dimension was not morphed; it remained \nfixed at the endpoints. Employing these stimuli in Garner \ntasks, Schweinberger et al. replicated Schweinberger and \nSoukup\u2019s finding that variation in identity interfered with \nclassifications of emotional expression, but variation in \nemotional expression did not interfere with classifications \nof identity. Importantly, they found that this asymmetric \npattern of interference held even when identity was more \ndifficult (and therefore took longer) to discriminate than \nemotion.\nAlso using the Garner paradigm, Baudouin et al. \n(2002) have demonstrated that schizophrenic individu-\nals, as well as healthy controls, show this asymmetric \ninteraction between the processing of facial expression \nand that of identity, despite the fact that schizophrenics \ntend to have impaired face-processing skills, especially in \nemotion recognition. Such impairments were manifest in \nthe RTs of Baudouin et al.\u2019s participants but not in their \naccuracy. Although the schizophrenic patients performed \njust as accurately as the healthy controls on the expres-\nsion and identity classification tasks, the schizophrenics \nwere considerably slower than the controls in every con-\ndition in each task. Moreover, this RT deficit was greater \nfor emotion classification than for identity classification. \nNevertheless, a typical pattern of Garner interference was \nobtained with both the schizophrenics and the healthy \nparticipants without any significant difference between \nthe two groups. Variation in identity increased RTs for \nclassification of facial expression, but variation in facial \nexpression did not influence RTs for classification of \nidentity.\nInterpreting Asymmetric Interference on Facial \nVariants of the Garner Paradigm\nOne explanation that Schweinberger et al. (1999) of-\nfered for their findings of asymmetric Garner interfer-\nence is that identity information can provide a reference \nfor computing information about facial speech and emo-\ntion, but the opposite is not the case. Presumably, it is \nrather less likely that information about emotional and \nspeech expressions can provide references for comput-\ning identity, for the very reason that such information is \nchangeable. Cast in terms of Haxby et al.\u2019s (2000) neu-\nrological model, we can think of Schweinberger et al.\u2019s \nresults as indicating some form of interaction between \nneurological systems that code invariant aspects of faces \nand those that encode changeable aspects. Note that this \nconception of Haxby et al.\u2019s model implies that the con-\nnection between these two systems is one-way, or at least \nis considerably stronger in one direction than in the other. \nAlthough Haxby et al.\u2019s depiction of their model shows a \ntwo-way arrow linking the distinct systems in which in-\nvariant and changeable aspects of faces are represented, \nthis is an assumption for which no evidence is presented. \nMoreover, Haxby et al. remark, \u201cThe degree of separa-\ntion between the functional roles played by the different \nregions . . . is unclear\u201d (p. 231). Their only suggestion of \na possible interaction between these two systems is a one-\nway interaction, similar to our own proposal\u2014namely, \nthat the system that encodes invariant information about \nfaces \u201cmay play a supportive role in the perception of ex-\npression, perhaps because different individuals can have \ncharacteristic expressions, such as a crooked smile or a \nwry grin, that we associate uniquely with them\u201d (p. 231).\nThe sex of faces, like their identity, is a relatively in-\nvariant property. Thus, given our interpretations of Haxby \net al.\u2019s (2000) model and of the aforementioned findings \nof asymmetric Garner interference between facial iden-\ntity and facial emotion judgments, we would expect task-\nirrelevant variations in emotional expression to interfere \nwith sex judgments, but we would not expect irrelevant \nvariations in sex to interfere with expression judgments. \nHowever, Le Gal and Bruce (2002) used the Garner \nparadigm to examine the relationship between facial ex-\npression and sex processing and found little evidence of \ninterference between these two dimensions. In two experi-\nments in this study, participants judged either the sex of \nfaces or whether the faces expressed anger or surprise. \nIn the first of these experiments, RTs to judge the sex of \nfaces did not differ significantly among the correlated, \ncontrol, and orthogonal conditions. RTs to judge emo-\ntional expression did not differ between the control and \northogonal conditions, although RTs were significantly \nshorter in the correlated condition than in these two con-\nditions. This relationship was examined again in a second \nexperiment, in which photographs of faces were cropped \nto remove the hairline and jawline in order to increase the \ndifficulty of the sex classification task to approximately \nequal that of the emotion classification task, as indicated \nby equivalent overall RTs for the two tasks. For sex clas-\nsifications, there were no significant differences among \nthe three conditions. For expression classifications, RTs \nin the correlated condition were reliably shorter than RTs \nin the control condition, but there was no significant dif-\nference between the orthogonal and control conditions. In \ncontrast to these speeded-judgment findings, however, a \nprior rating experiment showed that variation in emotional \nexpression affected the perceived relative femininity or \nmasculinity of faces; surprised expressions increased the \njudged femininity of faces and decreased their judged \nmasculinity relative to angry expressions. These findings \nprovide, in Le Gal and Bruce\u2019s view, \u201cqualified support\u201d \n(p. 242) for the independence of facial emotion and sex \nprocessing.\nThe Present Study\nTo examine further the utility of Haxby et al.\u2019s (2000) \nmodel, we investigated the interaction between sex (a par-\nadigmatic example of invariance) and (paradigmatically \nchangeable) emotional expression using two different \nspeeded-response interference paradigms. If an invariant \n1202    ATKINSON, TIPPLES, BURT, AND YOUNG\naspect of facial information\u2014identity or sex\u2014can pro-\nvide references for our judgments about more change-\nable aspects of faces, including emotional and speech \ninformation, then we would expect to find asymmetric \ninterference effects in experiments in which judgments \nabout the sex of a face are pitted against judgments about \nits particular emotional expression. In Experiments 1 \nand 2, we employed a variant of the Garner paradigm in \nwhich participants classified the emotion or sex of faces \nwhose irrelevant dimension (their sex in the case of emo-\ntion judgments, or their expressions in the case of sex \njudgments) either varied or remained constant. In Experi-\nment 3, we sought additional evidence from the Simon \nparadigm (Simon, 1990; Simon & Acosta, 1982; Simon \n& Rudell, 1967). In our facial variant of the Simon para-\ndigm, which we adapted from De Houwer, Hermans, and \nEelen (1998), participants classified the emotional ex-\npression or sex of faces by saying a word that was either \nsemantically congruent or semantically incongruent with \nthe irrelevant dimension of the face. The Simon paradigm \nis widely thought to involve interference at an output \nstage of processing rather than at an earlier perceptual \nstage (see, e.g., Kornblum & Lee, 1995; Lu & Proctor, \n1995; Umilt\u00e0 & Nicoletti, 1990, 1992). Given that Garner \ninterference effects are sometimes interpreted in terms \nof perceptual interaction, a finding of the same pattern \nof asymmetric interference with the facial variants of the \nGarner and Simon paradigms would raise questions about \nthe location of the interference and about whether the two \ntasks in fact tap the same source of interference. We shall \nreturn to these issues in the General Discussion.\nEXPERIMENT 1\nMethod\nParticipants. Thirty-two students from University College Win-\nchester were recruited for Experiments 1A and 1B and were paid for \ntheir participation. Sixteen of these students (12 female) took part \nin Experiment 1A (age range, 19\u201326 years; mean age, 21.1 years), \nand the other 16 (12 female) took part in Experiment 1B (age range, \n18\u201335 years; mean age, 20.9 years). All of the participants had nor-\nmal or corrected-to-normal eyesight.\nStimuli and Apparatus. Sixteen face stimuli were used in this \nexperiment, in the form of digitized black-and-white photographs. \nEight of the pictures showed different people with fearful facial ex-\npressions, and the remaining pictures showed the same individuals \nwith happy facial expressions. Each set of fearful and happy faces \nconsisted of four pictures of men and four pictures of women.\nEach face stimulus was a caricatured version of a face from the \nEkman and Friesen (1976) pictures of facial affect, taken from \nthe FEEST database of facial expression stimuli (Young, Perrett, \nCalder, Sprengelmeyer, & Ekman, 2002). The expressions selected \nwere computer caricatured by 50% using the techniques described \nby Calder, Young, Rowland, and Perrett (1997). Stimuli from the \nFEEST were chosen so that the expressions from which they were \nderived would be good exemplars of basic emotion categories, in-\ncluding those of fear and happiness. Not only are the original Ekman \nand Friesen pictures among the most used and thus well-normed sets \nof facial affect stimuli, but, in addition, their caricatured versions \nare identified more quickly than their noncaricatured counterparts \nwith no loss in accuracy (Calder et al., 1997), and people perceive \ncaricatured expressions as more emotionally intense than their \nnoncaricatured counterparts (Benson, Campbell, Harris, Frank, & \nTov\u00e9e, 1999; Calder et al., 2000). Another important feature of the \nselected stimuli was that the faces were cropped around the hair-\nline, eliminating the possibility that participants might base their \nsex judgments on hairstyle. Our intention in using cropped hairlines \nwas to make the sex-judgment task harder than usual, and by using \ncaricatured expressions we hoped to make the expression-judgment \ntask easier than usual. This was done to counter as much as possible \nthe tendency for sex classification to be an easier task than emotion \nclassification.\nAn Apple Macintosh PowerPC 8100\/80 computer, running Su-\nperlab 1.74 experimental software, controlled the stimulus displays \nand recorded RTs. All the faces appeared in the center of a uniform \ngray background, which covered the whole screen of the 12-in. color \nmonitor in grayscale mode. When presented on the screen, the faces \nhad a mean height of 10 cm (range, 9\u201311 cm) and a mean width of \n7.2 cm (range, 6\u20138 cm), subtending angles of 4.8\u00ba vertically and 3.4\u00ba \nhorizontally at a viewing distance of 60 cm. Manual responses were \nrecorded via the computer keyboard.\nDesign. The aim of Experiment 1A was to examine whether the \nsex of a face would influence judgments of its emotional expression, \nwhereas the aim of Experiment 1B was to examine whether a face\u2019s \nemotional expression would influence judgments of its sex. Since \nthe contrast between orthogonal and control conditions is critical \nfor establishing integral versus independent processing in the Gar-\nner task, with the correlated condition being of less theoretical in-\nterest, we chose to simplify the experimental design by including \nonly orthogonal and control conditions. Thus, in Experiment 1A we \ncompared RTs for classifying expression (happy vs. fearful) when \nthe faces were all of the same sex with RTs for classifying expres-\nsion when the faces depicted males and females in equal propor-\ntions. Similarly, in Experiment 1B we compared RTs for classifying \nfaces as male or female when the faces all had the same expression \nwith RTs for classifying sex when the faces had an equal mixture of \nhappy and fearful expressions.\nThe set of 16 faces provided 8 stimuli for classifying the ex-\npressions of male faces (4 happy and 4 fearful) and 8 stimuli for \nclassifying the expressions of female faces (4 happy and 4 fearful). \nSimilarly, these 16 faces provided 8 stimuli for classifying the sex \nof happy faces (4 male and 4 female) and 8 stimuli for classify-\ning the sex of fearful faces (4 male and 4 female). For the tasks of \nclassifying the expressions of faces that were either male or female \n(mixed sex) and classifying the sex of faces that were either happy \nor fearful (mixed emotion), 16 stimuli were available. Thus, in order \nto ensure an equal number of stimuli in each block of trials, the 16 \nstimuli were divided into two matched blocks for the orthogonal \nconditions (i.e., mixed sex in Experiment 1A and mixed emotion \nin Experiment 1B).\nFor Experiment 1A, each orthogonal (mixed-sex) block consisted \nof one male (E.M. or J.J.) and one female (C. or M.O.), each with \nboth happy and fearful expressions, and the control (single-sex) \nblocks consisted of two males (P.E. and W.F.) or two females (M.F. \nand S.W.), each with both happy and fearful expressions. Similarly, \nfor Experiment 1B each orthogonal (mixed-emotion) block con-\nsisted of one male (E.M. or J.J.) and one female (C. or M.O.), each \nwith both happy and fearful expressions, and each control (single-\nemotion) block consisted of two males (P.E. and W.F.) and two fe-\nmales (M.F. and S.W.) showing either all happy expressions or all \nfearful expressions. For both experiments, each of the four different \nstimuli within each block was presented eight times, yielding a total \nof 32 trials per block. All trials within each block were presented in \na pseudorandom order.\nEight different versions of Experiment 1A and eight different ver-\nsions of Experiment 1B were created, and the participants were ran-\ndomly assigned to one of these 16 versions. These different versions \nof the experiments were created in order to control for (1) the order \nof presentation of the four blocks within an experiment (four differ-\nASYMMETRIC INTERFERENCE IN FACE PERCEPTION    1203\nent orders determined by a Latin square design) and (2) the stimulus\u2013\nresponse mapping\u2014that is, the way in which the two response keys \nwere matched with the responses (\u201cfearful\u201d and \u201chappy\u201d in Experi-\nment 1A, and \u201cmale\u201d and \u201cfemale\u201d in Experiment 1B).\nProcedure. The participants were seated in a quiet room in front \nof the computer, approximately 60 cm from the monitor screen. \nWritten instructions were presented on the screen and summarized \norally by the experimenter. Each trial began with a fixation cross \n(\u0002), which appeared in the center of the screen for 1,500 msec. \nThe fixation cross was followed immediately by a stimulus face, \nwhich also appeared in the center of the screen. The face remained \non the screen until the participant\u2019s keypress triggered its offset, \nat which point the fixation cross appeared again, followed by the \nnext stimulus. The response keys were \u201cz\u201d and \u201c\/,\u201d which were \nlocated 17 cm apart on the row of the keyboard second from the \nbottom. The participants were asked to respond as quickly and ac-\ncurately as possible to the emotional expression displayed by the \nface (for Experiment 1A) or to the face\u2019s sex (for Experiment 1B) \nand to ignore other aspects of the face. The experiments began with \na practice block consisting of each of the 16 faces presented once, \nin pseudorandom order.\nResults\nIncorrect responses accounted for 2.8% of the data \noverall. Outliers, defined as RTs greater than 3,000 msec, \naccounted for only two datapoints, one in each task; these \nwere excluded from the RT and error analyses.\nResponse times. The mean correct RTs were initially \nanalyzed in a mixed-design ANOVA with task (sex clas-\nsification vs. emotion classification) as the between-\nsubjects variable and condition (orthogonal vs. control) as \nthe repeated measures variable. There were main effects \nof task [F(1,30) \u0003 15.01, p \u0004 .005; \u03b72 \u0003 .33] and condi-\ntion [F(1,30) \u0003 9.48, p \u0004 .01; \u03b72 \u0003 .24]. The main effect \nof task indicated that, averaged across condition, the par-\nticipants were reliably slower to judge the emotions (M \u0003 \n668 msec, SEM \u0003 23) than they were to judge the sex \n(M \u0003 544 msec, SEM \u0003 23) of the faces. The main effect \nof condition indicated that, averaged across tasks, the par-\nticipants were reliably slower in the orthogonal condition \n(M \u0003 622 msec, SEM \u0003 18) than in the control condition \n(M \u0003 590 msec, SEM \u0003 15). Of greater theoretical inter-\nest was the task \u0005 condition interaction [F(1,30) \u0003 6.59, \np \u0004 .05; \u03b72 \u0003 .18]. This interaction is depicted in Figure 1, \nwhich indicates that there was a clear effect of condition \nfor emotional expression classifications but not for sex \nclassifications. This was confirmed by simple main effect \nanalyses of the effect of condition for each task separately. \nWhen the participants were required to judge the emotion \nof the faces, they were slower to respond in orthogonal \nblocks (M \u0003 698 msec, SEM \u0003 26) than in control blocks \n[M \u0003 638 msec, SEM \u0003 22; F(1,15) \u0003 22.19, p \u0004 .005; \n\u03b72 \u0003 .59]. However, when the participants were required \nto judge the sex of the faces, there was no difference be-\ntween responses to orthogonal blocks (M \u0003 546 msec, \nSEM \u0003 26) and those to control blocks [M \u0003 541 msec, \nSEM \u0003 22; F(1,15) \u0003 .10, p \u0006 .5; \u03b72 \u0003 .007].\nIn order to compare RTs for every combination of each \nrelevant and each irrelevant dimension, separate ANOVAs \nwere carried out for each task, with the repeated measures \nvariables of condition, sex, and expression. For expres-\nsion classifications, there were significant main effects \nof condition [F(1,15) \u0003 20.7, p \u0004 .001; \u03b72 \u0003 .58] and \nemotion [F(1,15) \u0003 10.17, p \u0004 .01; \u03b72 \u0003 .404], but no \nsignificant interactions. The new finding revealed by this \nANOVA is that the participants were slower to judge fear-\nful facial expressions as fearful (M \u0003 704 msec, SEM \u0003 \n37) than they were to judge happy facial expressions as \nhappy (M \u0003 640 msec, SEM \u0003 24), averaged across con-\ndition. For sex classifications, no main effects of condi-\ntion, sex, or expression reached significance. No interac-\nFigure 1. Mean response times (RTs) for classifying the emotion (Experi-\nment 1A) and sex (Experiment 1B) of faces in the control and orthogonal condi-\ntions of the Garner paradigm.\n1204    ATKINSON, TIPPLES, BURT, AND YOUNG\ntions reached significance either, but the emotion \u0005 sex \ninteraction approached significance [F(1,15) \u0003 4.4, p \u0003 \n.053; \u03b72 \u0003 .227]. Inspection of Figure 2 reveals that the \nparticipants were somewhat slower to judge a fearful male \nface as male than they were to judge a happy male face as \nmale, but there was essentially no difference in their RTs \nto judging fearful versus happy female faces as female.\nErrors. A subsidiary set of analyses was used to exam-\nine error rates. Two separate one-way repeated measures \nANOVAs were conducted to examine the percentage of \nerrors for each block type in each task. For emotion classi-\nfication, error rates were higher for the orthogonal blocks \n(M \u0003 5.67%, SEM \u0003 1.33) than for the control blocks \n(M \u0003 1.95%, SEM \u0003 0.56) [F(1,15) \u0003 12.12, p \u0004 .01; \n\u03b72 \u0003 .45]. For sex classification, error rates were not sig-\nnificantly different between the orthogonal (M \u0003 2.05%, \nSEM \u0003 0.51) and the control (M \u0003 1.56%, SEM \u0003 0.47) \nblocks [F(1,15) \u0003 0.63, p \u0006 .1; \u03b72 \u0003 .04].\nDiscussion\nThe main result of this experiment was that RTs were \nlonger in the orthogonal than in the control condition of \nthe emotion classification task, whereas there was no \nsignificant difference in RTs between the orthogonal and \ncontrol conditions for sex classification. The error rates \nfollowed the same pattern, with more errors in the orthog-\nonal condition than in the control condition for emotion \nclassification but not for sex classification, thus confirm-\ning that the RT results were not due to a speed\u2013accuracy \ntrade-off. Despite the lack of an overall interference effect \nfor sex classification, the more finely grained RT analysis \nof the intratask relationships yielded some evidence to \nsuggest that a face\u2019s expression can influence sex classi-\nfication; RTs to male faces portraying fearful expressions \ntended to be longer than RTs to male faces portraying \nhappy expressions.\nOn the standard interpretation of Garner interference, \nthe overall asymmetric interference effect obtained in Ex-\nperiment 1 can be construed as evidence that although \npeople are able to attend selectively to the sex of faces \nwhile ignoring variations in facial expression, they are \nless able to attend selectively to facial expressions while \nignoring variations in the sex of the faces. (However, the \nnearly significant emotion \u0005 sex interaction for sex clas-\nsification suggests that for the male faces in this experi-\nment, the participants could not entirely ignore variations \nin emotional expression.)\nEXPERIMENT 2\nOur second experiment had two aims. One was to rep-\nlicate our finding of asymmetric Garner interference be-\ntween sex and emotion classification, given that Le Gal \nand Bruce (2002) did not find such an effect. The other \naim of this experiment was to examine whether or not this \nasymmetric interference would still be evident when the \nsex and emotion classification tasks were more equal in \ndifficulty, which would argue against an account of the \nasymmetric relationship in terms of relative processing \nspeeds.\nAccording to a speed-of-processing account, the asym-\nmetric pattern of interference is at least partially a result \nof the mismatch in task difficulty (Le Gal & Bruce, 2002; \nMelara & Mounts, 1993; Schweinberger et al., 1999), as \nis indicated by longer RTs and higher error scores for \nemotion classifications in comparison with sex classifi-\ncations. The basic argument would run as follows: Infor-\nmation concerning the sex of faces (being the more dis-\ncriminable dimension) is computed before the emotional \nexpression (the less discriminable dimension) has been \ndetermined. Consequently, when the task is to classify \nsex in the orthogonal condition of the Garner paradigm, a \nFigure 2. Mean response times (RTs) for classifying the sex of faces (Experi-\nment 1B) as a function of the emotion on the faces.\nASYMMETRIC INTERFERENCE IN FACE PERCEPTION    1205\nresponse can usually be initiated before information about \nthe face\u2019s emotional expression has been fully processed. \nBut when the task is to classify emotion in the orthogo-\nnal condition, the sex of the face is determined before a \nresponse regarding its emotion can be initiated, and thus \ninformation regarding the sex of the face is available as a \nreference for processing information regarding its emo-\ntional expression.\nAttempts to equate or reverse the initial difficulty of \nthe paired tasks in facial variants of the Garner paradigm \ndo not provide strong support for such an argument, how-\never. Le Gal and Bruce (2002) did not find any significant \ninterference effects between sex and emotion classifica-\ntions in an experiment in which both classifications were \nassumed to be approximately equal in difficulty, as is evi-\ndenced by approximately equal mean RTs in the two tasks \n(in fact, sex classifications were marginally though not \nsignificantly slower than emotion classifications). This is \nadmittedly not strong evidence against the mismatch-in-\ndifficulty argument for sex versus emotion classification, \nsince Le Gal and Bruce found no interference effect in an \nexperiment in which emotion classifications took longer \n(i.e., were more difficult) than sex classifications. Yet a \nmismatch in task difficulty is unlikely to be able to account \nentirely for our own finding of an asymmetric relationship \nbetween facial emotion and sex classification. As was noted \nearlier, Schweinberger et al. (1999) found that equating and \neven reversing task difficulty did not eliminate the same \nasymmetric pattern of interference between identity and \nexpression classifications. This suggests that we should \nalso expect equating or reversing task difficulty to preserve \nour interference effects, given our assumptions that invari-\nant and changeable aspects of faces are represented in dis-\ntinct processing routes and that the sex of faces, like their \nidentity, is a relatively invariant property.\nDespite this reason for doubting a relative speed-of-\nprocessing account of our asymmetric interference effect, \nit behoved us to test directly the validity of such an ac-\ncount. Consequently, we attempted to vary the difficulty of \n(and thus the RTs for) the sex and emotion classification \ntasks in Experiment 2 by morphing the faces across the \ntask-relevant dimension (cf. Schweinberger et al., 1999).\nMethod\nParticipants. Thirty-two students from the University of Dur-\nham were recruited; 16 (8 female; age range, 19\u201321 years; mean \nage, 20.1 years) took part in Experiment 2A, whereas the other 16 \n(8 female; age range, 19\u201322 years; mean age, 20.9 years) took part \nin Experiment 2B. All of the participants had normal or corrected-\nto-normal eyesight.\nStimuli and Apparatus. The bases for the stimulus set in Ex-\nperiment 2 were faces from the FEEST set (Young et al., 2002) \ncomprising four males (E.M., J.J., P.E., and W.F.) and four females \n(C., M.F., M.O., and N.R.) with both fearful and happy caricatured \n(\u000250%) expressions. (Thus, the faces were the same as those used \nin Experiment 1, with the exception of one of the female faces.) Two \nsets of morphed continua were created from these original faces. For \nExperiment 2A (emotion classification), each of the eight identities \nwas morphed across emotion (e.g., J.J.\u2019s face morphed from happy to \nfearful) with identity and thus sex held constant. For Experiment 2B \n(sex classification), the faces were morphed across sex and thus \nidentity, with expression held constant, so that one particular female \nface and one particular male face were morphed together with their \nfearful expressions and again with their happy expressions. (C. was \npaired with W.F., M.F. with E.M., M.O. with J.J., and N.R. with P.E.) \nThe choice of the particular female and male faces to be morphed \ntogether was based on the criterion that they should have as many \nFacial Action Coding System (Ekman & Friesen, 1978) action units \nin common as possible for each emotion (action unit data obtained \nfrom Young et al., 2002). Six equidistant morph levels were selected \nfor the morphed continua of emotion and sex: 75:25, 65:35, 55:45, \n45:55, 35:65, and 25:75 (the ratios expressing proportions of ini-\ntial image to final image). This resulted in a total of 48 different \nstimuli in each set, each of which was presented four times during \nthe course of the experiment, resulting in a total of 192 trials for \nExperiment 2A and 192 trials for Experiment 2B.\nThe stimulus presentation and recording of RTs was controlled by \nSuperlab 1.74 experimental software running on an Apple Macintosh \nG3 computer with a 19-in. color monitor in grayscale mode. The \ndimensions of the stimuli on the screen were the same as those in \nExperiment 1. Manual responses were recorded via the computer \nkeyboard.\nDesign. For reasons similar to those that apply to Experiment 1, \ntwo orthogonal and two control conditions were created for each \nof Experiments 2A and 2B. For Experiment 2A, each orthogonal \n(mixed-sex) block consisted of one female and one male (C. with \nW.F. or M.O. with J.J.), each morphed across emotion, and the con-\ntrol (single-sex) blocks consisted of two females (M.F. and N.R.) \nor two males (E.M. and P.E.), each morphed across emotion. For \nExperiment 2B, each orthogonal (mixed-emotion) block consisted \nof one female\u2013male pair (C. with W.F. or M.O. with J.J.) morphed \nacross identity (and thus sex) for their fearful expressions and again \nfor their happy expressions, whereas the control (single-emotion) \nblocks consisted of two female\u2013male pairs (M.F. with E.M. and N.R. \nwith P.E.) morphed across either their happy or their fearful expres-\nsions. For both Experiments 2A and 2B, within each block, each of \nthe 12 different stimuli (2 morphed continua \u0005 6 morph levels) was \npresented four times, yielding a total of 48 trials per block. All trials \nwithin each block were presented in a pseudorandom order.\nAs in Experiment 1, eight different versions of Experiment 2A \nand eight different versions of Experiment 2B were created, per-\nmitting counterbalancing of block order and stimulus\u2013response \nmapping. The participants were randomly assigned to 1 of these 16 \ndifferent versions.\nProcedure. The procedural details were the same as those for Ex-\nperiment 1, except that this time the practice block consisted of each \nof the eight morphed continua presented twice, once at the 75:25 \nmorph level and once at the 25:75 morph level, in pseudorandom \norder. As in Experiment 1, the task was to respond as quickly and \naccurately as possible to the emotional expression displayed by each \nface (for Experiment 2A) or to the face\u2019s sex (for Experiment 2B) \nwhile ignoring other aspects of the face.\nResults\nClassification performance. Figure 3 depicts the \npercentages of \u201cfearful\u201d and \u201cmale\u201d responses for emo-\ntion and sex classification, respectively, as a function of \nmorph level and experimental condition. With respect \nto the control and orthogonal conditions of the emotion \nclassification task, it can be seen that except for the most \nambiguous morph levels (i.e., those with either 55:45 or \n45:55 proportions of the two emotions), the stimuli were \nvery consistently classified as displaying either fear or \n1206    ATKINSON, TIPPLES, BURT, AND YOUNG\nhappiness. With respect to the control and orthogonal \nconditions of the sex classification task, on the other \nhand, the stimuli were not as consistently classified as \nmale or female, even for the morph levels toward the end-\npoints of the continua. These data were initially subjected \nto an ANOVA across tasks, with repeated measures on \nthe variables of condition and morph level. This mixed-\ndesign ANOVA revealed a significant main effect of con-\ndition [F(1,30) \u0003 4.4, p \u0004 .05; \u03b72 \u0003 .13] and a highly \nsignificant main effect of morph level [F(1.7,50.9) \u0003 \n228.97, p \u0004 .001; \u03b72 \u0003 .88, Greenhouse\u2013Geisser cor-\nrected]. The main effect of condition reflected the fact \nthat the mean ratio of \u201cfemale\u201d to \u201cmale\u201d or \u201chappy\u201d to \n\u201cfearful\u201d responses was reliably greater in the control \nconditions (M \u0003 57.4:42.6, SEM \u0003 1.8) than in the or-\nthogonal conditions (M \u0003 54.0:46.0, SEM \u0003 1.3). Pair-\nwise comparisons (Bonferonni corrected, \u03b1 \u0003 .05) for \nthe variable of morph level revealed that, averaged over \ntask and condition, all morph levels differed significantly \nfrom each other. The main effect of task approached sig-\nnificance [F(1,30) \u0003 3.89, p \u0003 .058; \u03b72 \u0003 .12], reflect-\ning a tendency for the mean ratio of \u201cfemale\u201d to \u201cmale\u201d \nresponses in the sex classification task (M \u0003 58.4:41.6, \nSEM \u0003 1.9) to be greater than the mean ratio of \u201chappy\u201d \nto \u201cfearful\u201d responses in the emotion classification task \n(M \u0003 53.0:47.0, SEM \u0003 1.9). These main effects were \nmodified by a significant task \u0005 morph level interaction \n[F(5,150) \u0003 26.53, p \u0004 .001; \u03b72 \u0003 .47], which was in \nturn modified by a significant task \u0005 condition \u0005 morph \nlevel interaction [F(5,150) \u0003 7.01, p \u0004 .001; \u03b72 \u0003 .19].\nTo follow up on this significant three-way interaction, \nseparate ANOVAs were conducted for each task, with re-\npeated measures on the variables of condition and morph \nlevel. There were highly significant main effects of morph \nlevel for both emotion classification [F(2.17,32.56) \u0003 \n481.17, p \u0004 .001; \u03b72 \u0003 .97] and sex classification \n[F(1.34,20.02) \u0003 34.53, p \u0004 .001; \u03b72 \u0003 .7; Greenhouse\u2013\nGeisser corrected in both cases]. Pairwise comparisons \n(Bonferonni corrected, \u03b1 \u0003 .05) for the variable of morph \nlevel revealed that, for emotion classification, all morph \nlevels differed significantly from each other except for \nlevels 35:65 and 25:75, whereas for sex classification, \nmorph levels 55:45 and 45:55 did not differ significantly \nfrom each other. There was no main effect of condition \nfor either sex or emotion classification, although in the \ncase of the latter there was a nonsignificant trend toward \na higher mean ratio of \u201chappiness\u201d responses to \u201cfear-\nful\u201d responses in the control (M \u0003 54.2:45.8, SEM \u0003 1.4) \nthan in the orthogonal (M \u0003 51.8:48.2, SEM \u0003 1.3) con-\nditions [F(1,15) \u0003 4.13, p \u0003 .06; \u03b72 \u0003 .22]. There were \nsignificant condition \u0005 morph interactions for both emo-\ntion classification [F(2.92,43.81) \u0003 3.55, p \u0004 .05; \u03b72 \u0003 \n.19, Greenhouse\u2013Geisser corrected] and sex classifica-\ntion [F(5,75) \u0003 5.19, p \u0004 .001; \u03b72 \u0003 .26]. As Figure 3 \nindicates, these interactions reflected differences between \nthe control and orthogonal conditions for some but not \nall morph levels, which was confirmed by simple main-\neffects analyses. For emotion classification, the consis-\ntency of response was higher in the control than in the \northogonal conditions for morph levels 75:25, 45:55, and \n35:65, whereas for sex classification the consistency of \nresponse was higher in the orthogonal than in the control \nconditions for morph levels 75:25 and 65:35 [all ps \u0004 \n.05; morph level 55:45 approached significance for sex \nclassification ( p \u0003 .057)].\nClassification response times. In order to analyze the \nRTs to stimuli of different levels of difficulty on the task-\nrelevant dimension, we collapsed the data from the six \nFigure 3. Percentages of \u201cfearful\u201d or \u201cmale\u201d responses, depending on task \nand condition in Experiment 2, as a function of morph level (happy:fearful, or \nfemale:male).\nASYMMETRIC INTERFERENCE IN FACE PERCEPTION    1207\nindividual morph levels into three levels of difficulty: 1 \n(75:25 and 25:75), 2 (65:35 and 35:65), and 3 (55:45 and \n45:55). Since our primary goal was to select certain lev-\nels of morphing to test whether asymmetric interference \nwould still be evident when task difficulty was equated, \nand because it is not very meaningful to analyze RTs at \nnear-chance levels of performance, we entered into the \nRT analyses the data for difficulty levels 1 and 2 only. We \ninitially performed an ANOVA across tasks, with repeated \nmeasures on the variables of condition (control vs. or-\nthogonal) and difficulty level (1 vs. 2). There was no main \neffect of task [F(1,30) \u0003 0.06, p \u0006 .5; \u03b72 \u0003 .002], indicat-\ning that RTs for the classification of sex (M \u0003 900 msec, \nSEM \u0003 68) were similar to those for the classification of \nemotion (M \u0003 877 msec, SEM \u0003 68). The main effects of \ncondition and difficulty did not reach significance either \n(all Fs \u0004 0.2). However, there was a significant task \u0005 \ndifficulty interaction [F(1,30) \u0003 7.47, p \u0004 .05; \u03b72 \u0003 .2], \na significant task \u0005 condition interaction [F(1,30) \u0003 \n15.67, p \u0004 .001; \u03b72 \u0003 .34], and a significant condition \u0005 \ndifficulty interaction [F(1,30) \u0003 4.24, p \u0004 .05; \u03b72 \u0003 .12]. \nThese two-way interactions were modified by a signifi-\ncant task \u0005 difficulty \u0005 condition interaction [F(1,30) \u0003 \n4.78, p \u0004 .05; \u03b72 \u0003 .14].\nTo analyze these interactions further, we performed \nseparate ANOVAs for each task, with repeated measures \non the variables of condition (control vs. orthogonal), dif-\nficulty level (1 [easier] vs. 2 [more difficult]), and irrel-\nevant dimension of the face (male vs. female in the case of \nemotion classification, and fear vs. happiness in the case \nof sex classification). For emotion classification, there \nwas a significant main effect of condition [F(1,15) \u0003 \n15.22, p \u0004 .005; \u03b72 \u0003 .5], reflecting the expected re-\nsult\u2014namely, that RTs in the orthogonal condition (M \u0003 \n914 msec, SEM \u0003 46) were reliably longer than RTs in \nthe control condition (M \u0003 835 msec, SEM \u0003 42). There \nwas also a significant main effect of difficulty [F(1,15) \u0003 \n6.97, p \u0004 .05; \u03b72 \u0003 .32], reflecting the fact that the par-\nticipants were slower to respond to morphed faces of \nDifficulty Level 2 (M \u0003 906 msec, SEM \u0003 49) than to \nmorphed faces of Difficulty Level 1 (M \u0003 842 msec, \nSEM \u0003 40). There was no significant main effect of the \nsex of the face, and none of the interactions was signifi-\ncant (all Fs \u0004 1).\nFor sex classification, there was a significant main effect \nof condition [F(1,15) \u0003 5.21, p \u0004 .05; \u03b72 \u0003 .26], reflecting \nthe fact that, overall, RTs in the orthogonal condition (M \u0003 \n856 msec, SEM \u0003 74) were reliably shorter than RTs in the \ncontrol condition (M \u0003 933 msec, SEM \u0003 98). There was \nalso a significant main effect of difficulty [F(1,15) \u0003 7.86, \np \u0004 .05; \u03b72 \u0003 .34], which was due to the fact that, overall, \nthe participants were faster to respond to morphed faces \nof Difficulty Level 2 (M \u0003 867 msec, SEM \u0003 80) than \nto morphed faces of Difficulty Level 1 (M \u0003 922 msec, \nSEM \u0003 90). These main effects were modified by a sig-\nnificant condition \u0005 difficulty interaction [F(1,15) \u0003 8.95, \np \u0004 .01; \u03b72 \u0003 .37]. Simple main-effects analyses for this \nsignificant interaction revealed that RTs for faces of Dif-\nficulty Level 1 were significantly shorter in the orthogonal \ncondition (M \u0003 846 msec, SEM \u0003 75) than in the con-\ntrol condition [M \u0003 1,000 msec, SEM \u0003 108; F(1,15) \u0003 \n11.56, p \u0004 .005; \u03b72 \u0003 .44], but that for faces of Difficulty \nLevel 2 there was no significant difference in RTs between \nthe control (M \u0003 896 msec, SEM \u0003 89) and the orthogonal \n(M \u0003 859 msec, SEM \u0003 79) conditions [F(1,15) \u0003 1.14, \np \u0006 .1; \u03b72 \u0003 .07].\nDiscussion\nMorphing the faces across the task-relevant dimension \nclearly affected the difficulty of the corresponding task. \nThe more the faces were blended on the relevant dimen-\nsion, the more difficult the task became, as is indicated \nby both longer RTs and the decrease in consistency of the \nparticipants\u2019 responses. Crucially, these manipulations of \ntask difficulty had the effect of equating the overall RTs \nacross the emotion and sex classification tasks. Neverthe-\nless, we still found an asymmetric pattern of Garner inter-\nference. The direction of this asymmetric interference was \nthe same as that found in Experiment 1\u2014that is, variation \nin the sex of the faces interfered with emotion classifica-\ntion performance more than variation in expression inter-\nfered with sex classification performance. Although sex \nclassifications remained relatively unaffected by changes \nin facial expression, there was some evidence of an in-\nteraction, insofar as RTs were reliably shorter in the or-\nthogonal condition than in the control condition for faces \nof Difficulty Level 1 (75:25 and 25:75 morphs). In sum, \nthe results of this experiment suggest that differences in \nprocessing speed cannot account for the asymmetric rela-\ntionship between facial emotion and sex processing.\nEXPERIMENT 3\nExperiments 1 and 2 demonstrated the value of distin-\nguishing the processing of invariant aspects of faces from \nthat of changeable aspects in interpreting asymmetric in-\nterference on facial variants of the Garner paradigm. The \npurpose of Experiment 3 was to determine whether the im-\nplementation of a different interference paradigm\u2014a facial \nvariant of De Houwer et al.\u2019s (1998) affective Simon para-\ndigm\u2014would converge on the same pattern of findings.\nDe Houwer et al. (1998) investigated the effect of an \nirrelevant dimension on speeded forced-choice judgments \nabout faces by using a variant of the affective Simon para-\ndigm (De Houwer, Crombez, Baeyens, & Hermans, 2001; \nDe Houwer & Eelen, 1998). Like the Garner paradigm, \nthe Simon paradigm requires participants to respond to \nstimuli along a particular dimension while a second di-\nmension of the stimulus is varied. However, unlike in the \nGarner paradigm, the required response is related to the \nirrelevant dimension. In De Houwer et al.\u2019s (1998) study, \nparticipants classified the sex or identity of faces with a \nverbal response that was either congruent or incongruent \nwith the valence of the facial expressions. The stimuli were \nphotographs of two male and two female faces with both \npositive (happy) and negative (fearful, angry, disgusted, \n1208    ATKINSON, TIPPLES, BURT, AND YOUNG\nand sad) expressions. In the sex classification task, half \nof the participants were asked to say \u201cpositive\u201d if the face \nwas male and \u201cnegative\u201d if the face was female, whereas \nthe remaining participants were asked to give the converse \nresponses (i.e., \u201cnegative\u201d to male faces and \u201cpositive\u201d to \nfemale faces). In the identification task, the participants \nwere asked to say \u201cpositive\u201d to two of the four stimulus \nfaces (one male and one female) and \u201cnegative\u201d to the \nother two faces. (The female and male faces assigned the \n\u201cpositive\u201d and \u201cnegative\u201d responses were determined in \na prior learning phase of the experiment.) In both tasks, \nthe participants were told to ignore the expressions on \nthe faces.\nVocal RTs were reliably shorter on congruent than on \nincongruent trials in the identity classification task in \nDe Houwer et al.\u2019s (1998) study, but not in their sex classi-\nfication task. That is, for identity but not sex classification, \nthe participants were faster when the valence of the facial \nexpression matched the valence of the correct response \n(\u201cpositive\u201d to faces with positive emotional expressions \nand \u201cnegative\u201d to faces with negative emotional expres-\nsions) than when they did not match. Although there was \nno significant congruency effect in the sex classification \ntask, RTs were shorter for the congruent than for the in-\ncongruent trials in both instances of this task. De Houwer \net al. (1998) speculated that further studies with more sta-\ntistical power might show this trend to be significant.\nOur modified version of De Houwer et al.\u2019s (1998) task \nemployed exactly the same faces as those used in Experi-\nment 1. In contrast to Experiments 1 and 2, the required \nresponse in Experiment 3 was vocal and semantically re-\nlated to the irrelevant dimension. Participants classified \nemotional expression either by saying \u201cmale\u201d for happy \nfaces and \u201cfemale\u201d for fearful faces or by saying the con-\nverse (i.e., \u201cfemale\u201d for happy faces and \u201cmale\u201d for fear-\nful faces), and they classified the sex of the faces either \nby saying \u201cfearful\u201d for male faces and \u201chappy\u201d for female \nfaces or by saying the converse. Thus, our Simon task is \nanalogous to a Stroop task (MacLeod, 1991; Stroop, 1935) \ninsofar as the relevant and the irrelevant dimensions in the \nSimon task can be semantically congruent (e.g., requiring \nresponses of \u201cmale\u201d to happy male faces and \u201cfemale\u201d to \nfearful female faces) or semantically incongruent (e.g., \nrequiring responses of \u201cmale\u201d to happy female faces and \n\u201cfemale\u201d to fearful male faces). The two paradigms dif-\nfer to the extent that, in the Stroop but not in the Simon \nparadigm, the relevant stimulus dimension overlaps with \nthe response set and the irrelevant feature dimension (see \nKornblum & Lee, 1995).\nMethod\nParticipants. Twenty-eight psychology students (15 female) \nfrom University College Winchester took part in the experiment. \nAll were 19 to 45 years of age (M \u0003 26.2 years) and had normal or \ncorrected-to-normal eyesight.\nStimuli and Apparatus. In Experiment 3, we employed the \nsame stimuli as in Experiment 1\u2014that is, 16 digitized black-and-\nwhite photographs of four male and four female faces from the \nFEEST database of facial expression stimuli (Young et al., 2002), \nwith both happy and fearful caricatured expressions. These stimuli \nwere presented in the same dimensions and on the same computer \nand monitor as they were in Experiment 1, but this time the stimu-\nlus presentation and recording of RTs was controlled by PsyScope \n1.2.4 experimental software (Cohen, MacWhinney, Flatt, & Provost, \n1993). Vocal responses were recorded via a headset microphone \nconnected to the computer via a response box and the keyboard.\nDesign. Each participant undertook two tasks: sex classification \nand emotion classification. Half of the participants classified the \nsex of the faces first and then classified the emotion of the faces; the \nother half of the participants completed the tasks in the reverse order. \nFor sex classification, half of the participants were required to re-\nspond \u201chappy\u201d to male faces and \u201cfearful\u201d to female faces, whereas \nthe remaining participants were asked to respond \u201cfearful\u201d to male \nfaces and \u201chappy\u201d to female faces. For emotion classification, half \nof the participants were required to respond \u201cmale\u201d to happy faces \nand \u201cfemale\u201d to fearful faces, whereas the remaining participants \nwere asked to respond \u201cmale\u201d to fearful faces and \u201cfemale\u201d to happy \nfaces. Four versions of each experiment were created\u2014one for each \ncombination of task order and response mapping\u2014and the partici-\npants were randomly assigned to them. Within each task, there were \nfive blocks of 16 trials. Each of the 16 faces was presented once in \neach block, in a pseudorandom order.\nProcedure. The participants were seated in a quiet room in front \nof the computer, approximately 60 cm from the monitor screen. \nWritten instructions were presented on the screen and summarized \norally by the experimenter. Each trial began with the warning GET \nREADY! presented in the center of the screen for 1,000 msec. This \nwas followed by a 1,000-msec blank screen interval, at the end of \nwhich a stimulus face appeared in the center of the screen. The face \nstimulus remained on the screen until the participant\u2019s vocal re-\nsponse triggered its offset. Following a 200-msec interval, a central \nfixation cross appeared in the center of the screen until its offset \nwas triggered by the experimenter\u2019s pressing one of three keys on \nthe keyboard, which coded the vocal response and any erroneous \nactivation of the voice key. An interval of 1,000 msec preceded the \nbeginning of the next trial. The participants were asked to respond as \nquickly and accurately as possible to either the sex or the emotional \nexpression of the faces, depending on the task, and to ignore other \naspects of the faces.\nResults\nResponse times. Incorrect responses and trials in \nwhich the vocal response failed to activate the voice key \naccounted for 9.2% of the total responses and were not \nincluded in the RT analyses. Mean correct RTs were ini-\ntially analyzed with a two-way ANOVA, with task (emo-\ntion classification vs. sex classification) and congruency \n(congruent vs. incongruent) as repeated measures vari-\nables. There were main effects of task [F(1,27) \u0003 39.98, \np \u0004 .001; \u03b72 \u0003 .6] and congruency [F(1,27) \u0003 28.27, \np \u0004 .001; \u03b72 \u0003 .51]. The main effect of task showed that \nresponses were slower for emotion classification (M \u0003 \n900 msec, SEM \u0003 23) than for sex classification (M \u0003 \n773 msec, SEM \u0003 20). The main effect of congruency \nshowed that responses were slower on incongruent (M \u0003 \n852 msec, SEM \u0003 19) than on congruent (M \u0003 821 msec, \nSEM \u0003 19) trials. These main effects were modified by \na significant task \u0005 congruency interaction [F(1,27) \u0003 \n4.69, p \u0004 .05; \u03b72 \u0003 .15], which is depicted in Figure 4. As \ncan be seen from this figure, the participants were slower \nto respond in the incongruent conditions than in the con-\ngruent conditions when classifying emotion, but not when \nASYMMETRIC INTERFERENCE IN FACE PERCEPTION    1209\nthey were classifying sex; furthermore, they were faster \nin classifying emotion than in classifying sex. This was \nconfirmed by simple main-effect analyses of congruence \nfor each task separately. When judging emotional expres-\nsions, the participants were significantly slower when the \ncorrect responses were incongruent with the irrelevant di-\nmension (i.e., the sex) of the face (M \u0003 925 msec, SEM \u0003 \n24) than when they were congruent [M \u0003 875 msec, \nSEM \u0003 23; F(1,27) \u0003 27.4, p \u0004 .001; \u03b72 \u0003 .5]. In the \nsex classification task, there was no significant difference \nbetween RTs to incongruent (M \u0003 779 msec, SEM \u0003 22) \nand to congruent [M \u0003 768 msec, SEM \u0003 19; F(1,27) \u0003 \n1.02, p \u0006 .1; \u03b72 \u0003 .036] stimuli.\nIt was not possible to compare RTs for every combi-\nnation of each relevant and each irrelevant dimension \nfor each task in this experiment, as was done for Experi-\nment 1. This is because in Experiment 3 the participants \nwere not presented with all possible combinations of each \nrelevant and each irrelevant dimension in each condition. \nConsider, for example, those participants who classified \nemotion by saying \u201cmale\u201d for happy faces and \u201cfemale\u201d \nfor fearful faces and classified sex by saying \u201chappy\u201d for \nmale faces and \u201cfearful\u201d for female faces. These partici-\npants saw only happy male and fearful female faces in \nthe congruent conditions of both the emotion classifica-\ntion and sex classification tasks, and they saw only fearful \nmale and happy female faces in the incongruent condi-\ntions of both tasks.\nErrors. We examined error rates through a subsidiary \nset of analyses. A two-way repeated measures ANOVA \nwith variables of task (emotion classification vs. sex clas-\nsification) and congruency (congruent vs. incongruent) \nwas conducted to examine the percentage of errors for \neach condition in each task. There were main effects of \ntask [F(1,27) \u0003 9.68, p \u0004 .01; \u03b72 \u0003 .26] and congruency \n[F(1,27) \u0003 26.29, p \u0004 .001; \u03b72 \u0003 .49]. The main effect \nof task indicated that error rates were reliably higher for \nemotion classification (M \u0003 11.4%, SEM \u0003 1.5) than for \nsex classification (M \u0003 6.0%, SEM \u0003 1.1). The main ef-\nfect of congruency indicated that error rates were reliably \nhigher in the incongruent condition (M \u0003 10.8%, SEM \u0003 \n1.2) than in the congruent condition (M \u0003 6.6%, SEM \u0003 \n1.0). These main effects were modified by a significant \ntask \u0005 congruency interaction [F(1,27) \u0003 12.71, p \u0004 \n.005; \u03b72 \u0003 .32], which is depicted in Figure 5. As this fig-\nure indicates, the participants made approximately twice \nas many errors in the incongruent (M \u0003 15.1%, SEM \u0003 \n2.1) than in the congruent (M \u0003 7.7%, SEM \u0003 1.2) condi-\ntion when classifying emotion, but when classifying sex \nthey showed essentially no difference in their error rates \nbetween these two conditions.\nDiscussion\nThe participants were significantly slower to classify \nemotion when the required response (\u201cmale\u201d or \u201cfemale\u201d) \nwas incongruent with the sex of the face in comparison \nwith when the response and the sex of the face were con-\ngruent. No such significant congruency effect was evident \nfor sex classification, however. The error rates followed \nthe same pattern, with more errors in the incongruent than \nin the congruent condition for emotion classification but \nnot for sex classification, confirming that the RT results \nwere not due to a speed\u2013accuracy trade-off. Thus, we have \nfound a facial variant of a semantic Simon effect (De Hou-\nwer, 1998) but not of an affective Simon effect\u2014at least \nnot in a sex classification task, which corroborates De \nHouwer et al.\u2019s (1998) finding. This asymmetry in the in-\nteractions between the processing of the emotion and the \nsex of faces mirrors the asymmetric dependency found in \nExperiments 1 and 2.\nFigure 4. Mean response times (RTs) for classifying the emotion and sex of \nfaces in the incongruent and congruent conditions (Simon paradigm) of Ex-\nperiment 3. \n1210    ATKINSON, TIPPLES, BURT, AND YOUNG\nIt is worth noting that the sex classification task in Ex-\nperiment 3 was completed more slowly than the emotion \nclassification task in Experiment 1 (773 vs. 668 msec). \nThis too counts against a purely speed-based account of \nthe asymmetric interference, because the shorter RTs in \nthe Experiment 1 emotion classification task than in the \nExperiment 3 sex classification task indicate that infor-\nmation about emotion must have been available during \nthe sex classification task in Experiment 3, yet there was \nno interference in the latter task.\nGENERAL DISCUSSION\nThis study provides convergent evidence, from two \ndifferent interference paradigms, of an asymmetric rela-\ntionship between the processes underlying performance \non facial emotion and sex classification tasks. Emotion \nclassification was significantly influenced by irrelevant \nvariations in the sex of the faces (Experiments 1 and 2) \nand by the semantic relatedness of the vocal response to \nthe sex of the faces (Experiment 3). Sex classification, \non the other hand, was less influenced by both irrelevant \nvariations in facial expression and the semantic related-\nness of the vocal response to the facial expression. Nev-\nertheless, Experiments 1 and 2 provided evidence that in-\nformation concerning emotional expressions did impinge \nto a limited extent on sex classification performance. Spe-\ncifically, the participants in Experiment 1 were slower to \nclassify male faces as male when those faces portrayed \nfearful expressions in comparison with when they por-\ntrayed happy expressions, and in Experiment 2, sex classi-\nfications were slower in the control than in the orthogonal \nconditions for faces morphed across sex at the easiest of \nthe difficulty levels.\nThe lack of Garner interference between sex and emo-\ntion classification in Le Gal and Bruce\u2019s (2002) study \nis not consistent with our results. There are several pos-\nsible reasons for the different findings in the two studies, \nincluding the fact that Le Gal and Bruce employed sur-\nprised and angry expressions whereas we used expres-\nsions of fear and happiness. Although we have no specific \nreason to suspect that different emotions might produce \ndifferent patterns of Garner interference, an avenue for \nfuture research will be to examine the robustness of the \nasymmetric pattern of interference by conducting studies \nin which different combinations of emotional expressions \nare employed.\nHaxby et al.\u2019s (2000) face-processing model provides a \nuseful framework for interpreting the findings of asym-\nmetric dependencies between identity processing and \nfacial and speech expression processing, as reported by \nSchweinberger and Soukup (1998), Schweinberger et al. \n(1999), and Baudouin et al. (2002). In this model, invariant \nand changeable aspects of faces are represented through \ndistinct processing routes. According to the interpreta-\ntion of Haxby et al.\u2019s model that we presented in the intro-\nduction, asymmetric Garner interference indicates some \nform of interaction between these distinct neurological \nsystems. The direction of the asymmetric dependencies \nis consistent with the further plausible assumption that \nidentity information (which is paradigmatically invariant) \nis much more likely to provide a reference for computing \ninformation about facial speech and emotion (which is \nparadigmatically changeable) than vice versa. The sex of \nfaces, like their identity, is a relatively invariant property. \nMoreover, whereas the extent to which sex processing and \nidentity processing are independent has been a matter of \ndebate (see Young, 1998, for discussion) and a degree of \nFigure 5. Mean percentage errors for classifying the emotion and sex of faces \nin the incongruent and congruent conditions (Simon paradigm) of Experi-\nment 3.\nASYMMETRIC INTERFERENCE IN FACE PERCEPTION    1211\nindependence has been convincingly established (Bruce \net al., 1987), a variety of recent evidence suggests that \nthe sex and identity of faces are processed through a single \nroute (see, e.g., Calder, Burton, Miller, Young, & Akamatsu, \n2001; Dubois et al., 1999; Ganel & Goshen-Gottstein, \n2002; Goshen-Gottstein & Ganel, 2000), as Haxby et al.\u2019s \n(2000) conception of involvement of a neurological path-\nway in the analysis of invariant properties of faces would \nsuggest. Thus, our finding of asymmetric Garner interfer-\nence between sex and emotion classification is consistent \nwith this particular reading of Haxby et al.\u2019s model.\nOur explanation of the asymmetry of the interference on \nfacial variants of the Garner paradigm in terms of Haxby \net al.\u2019s (2000) model has so far relied on the intuition that \nchangeable properties of faces, by their very nature, pro-\nvide a less stable basis for further computations and are \nmore context sensitive than relatively invariant properties \nof faces. There is a growing body of evidence consistent \nwith this intuition. Our judgments of others\u2019 emotional \nstates are influenced by our expectations regarding those \npeople and the social contexts in which the emotions are \nexpressed (see, e.g., Ekman & Friesen, 1971; Hess & Kir-\nouac, 2000; Kirouac & Hess, 1999). For example, the ex-\npectation (at least in Western cultures) is that females will \ntend to be more facially expressive, express fear and sad-\nness more readily, and smile more frequently than males, \nwhereas males will tend to express anger more readily \nthan females (Brody & Hall, 2000; Kring & Gordon, \n1998; LaFrance, Hecht, & Paluck, 2003). Our suggestion \nis that the human brain\u2019s expression-processing systems \nare sensitive to such expectations insofar as they can draw \nupon information regarding the sex and identity of the \nperson displaying the emotion.\nAccording to Schweinberger and Soukup (1998) and \nSchweinberger et al. (1999), interactions at a perceptual \nlevel of processing are the presumed cause of asymmet-\nric Garner interference in face processing. Similarly, Le \nGal and Bruce (2002; see especially p. 242) assumed that \nit is the absence of interactions at a perceptual level of \nprocessing that explains their finding of no Garner inter-\nference between expression classification and sex classi-\nfication. We have indicated how our conception of Haxby \net al.\u2019s (2000) model of face processing is consistent with \nsuch an interpretation, insofar as it suggests that such an \ninteraction occurs between neurologically separate sys-\ntems that represent the changeable properties of faces on \none hand, and the relatively invariant ones on the other. \nShortly, we will elucidate how this model is nevertheless \ncloser in spirit and detail to models of face processing \nthat posit parallel and independent processes than it is to \nparadigmatic \u201cparallel-contingent\u201d models.\nThe possibility of interaction at an even later stage of \nprocessing is suggested by the results of our third experi-\nment, in which we used a facial variant of the Simon para-\ndigm. Given the similarity of the Simon and Stroop para-\ndigms, theoretical accounts of Stroop interference clearly \nhave the potential to explain findings of interference on \nfacial variants of the Simon paradigm. A common aspect \nof theoretical accounts of Stroop interference is an appeal \nto response competition: The extent to which process-\ning of the task-irrelevant dimension and that of the task-\nrelevant dimensions compete for control of the partici-\npant\u2019s response depends on the degree of relatedness or \noverlap between the required response and the stimulus \ndimensions (see, e.g., Kornblum & Lee, 1995; MacLeod, \n1991). Thus, the interference effect observed in Experi-\nment 3 suggests that the locus of this Simon interference \nis at a response-selection stage of processing.\nGiven that the Simon paradigm employed in Experi-\nment 3 yielded the same asymmetric pattern of interfer-\nence as that observed with the Garner paradigm in Experi-\nments 1 and 2, the challenge is to find a model that can \naccount parsimoniously for this finding of convergence. \nAlthough an explanation in terms of response conflict \nmight be appropriate for findings of Simon interference, \nit is difficult to see how Garner interference effects could \nbe due to response conflicts, given that in that paradigm \nthe irrelevant dimension is not mapped onto the same re-\nsponses as is the relevant dimension. We note, though, \nthat it is possible that an interaction between two or more \nstimulus dimensions can occur at a response-selection \nstage of processing for reasons other than response conflict. \nFor example, the interested reader might want to consider \nNosofsky and Palmeri\u2019s (1997a, 1997b) \u201cexemplar-based \nrandom walk model\u201d of stimulus categorization, which ac-\ncounts for Garner interference effects in terms of post-\nperceptual operations at a memory-retrieval or response-\nselection stage of processing, and how this model might \nbe extended to account for the asymmetric pattern of in-\nterference on facial variants of the Garner paradigm.\nWe have been promoting the utility of Haxby et al.\u2019s \n(2000) model of face processing, our interpretation of \nwhich is consistent with the findings of asymmetric Gar-\nner interference between the processing of a face\u2019s sex \nand identity on the one hand, and the processing of facial \nemotion and speech expressions on the other. This model \nlocates the source of that interaction at a perceptual stage \nof processing. To the extent that Simon interference indi-\ncates interaction at a later, response-selection stage of pro-\ncessing, Haxby et al.\u2019s model would need to be extended \nin order to account for our finding of such an effect. Be \nthat as it may, our finding of the same pattern of asym-\nmetric interference in both our Garner and our Simon \nexperiments indicates the pervasiveness of the principle \nthat invariant dimensions of a stimulus are more useful \nreferents for computing information about changeable as-\npects of that stimulus than vice versa. Moreover, Haxby \net al.\u2019s model preserves a high degree of functional sepa-\nration between anatomically distinct processing routes, \nwith only a relatively small degree of dependence or con-\ntingency, and then mostly in one direction and likely not \nmandatory. This model is a parallel-contingent model, but \none that emphasizes the \u201cparallel\u201d much more than the \n\u201ccontingent.\u201d It is thus closer in spirit and detail to mod-\nels that posit parallel and independent processes, such as \nthat of Bruce and Young (1986), than to the paradigms \n1212    ATKINSON, TIPPLES, BURT, AND YOUNG\nof parallel-contingent models (see discussion and refer-\nences in Schweinberger et al., 1999, p. 1113). As such, \nHaxby et al.\u2019s model is still consistent with a range of \nfindings indicative of a high degree of functional separa-\ntion (if not complete independence) between certain com-\nponents of face processing, which we mentioned in the \nintroduction\u2014namely, between those components that \nencode changeable properties of faces versus those that \nencode invariant ones. Consider, for example, neuropsy-\nchological dissociations between identity and expression \nprocessing. Haxby et al.\u2019s model is consistent with pre-\nserved expression processing despite impaired identity \nprocessing (due to damage to inferior but not to superior \ntemporal cortex) and with preserved identity processing \ndespite impaired expression processing (due to damage \nto superior but not to inferior temporal cortex). This is \nbecause there is relatively minimal functional reliance \nbetween the two anatomically separate components in the \ncore system of Haxby et al.\u2019s model, in comparison with \nthe extent to which each of these components relies on \nthe contributions of different components in the extended \nsystem. One intriguing prediction of this interpretation, \ngiven our aforementioned assumption about the direction \nof interaction between these components, is that those \nprosopagnosics who have preserved expression recogni-\ntion abilities might be expected not to be able to use facial \nidentity information (and perhaps not even information \nabout the sex of faces) in their judgments of facial expres-\nsion; such patients might not show the asymmetric pattern \nof interference in the facial variant of the Garner task, for \nexample.\nConclusion\nWe have provided convergent evidence from two dif-\nferent interference paradigms that facial emotion clas-\nsification can be significantly influenced by the sex of \nfaces, but sex classification remains relatively impervi-\nous to facial emotion. These results are consistent with \nthe hypothesis that information about invariant aspects of \nfaces is much more likely to influence the processing of \nchangeable aspects of faces than information about their \nchangeable aspects is to influence the processing of their \ninvariant aspects. The findings of asymmetric interfer-\nence on facial variants of the Garner paradigm, including \nthose reported here, suggest interactions at a perceptual \nlevel of processing, whereas our finding of asymmetric \ninterference in the facial variant of the Simon paradigm \nsuggests interaction at a memory-retrieval or response-\nselection stage of processing. A challenge for face pro-\ncessing models is therefore to show how the same asym-\nmetric pattern of interference could occur at different \nstages of processing or, alternatively, how interactions at \na single stage of processing could account for the same \nasymmetric pattern of interference across two paradigms \nthat place apparently rather different processing demands \non the subject. Although Haxby et al.\u2019s (2000) model does \nnot obviously explain the Simon interference effect, a \nstrong point of this model is that it can account for asym-\nmetric Garner interference while preserving a high degree \nof functional separation between the processing routes for \ninvariant and for changeable aspects of faces. Future re-\nsearch could capitalize on the ability of electrophysiologi-\ncal recordings to determine not just the relative timing of \ndifferent perceptual judgments about faces (M\u00fcnte et al., \n1998), but also their interactions (Rahman, Sommer, & \nSchweinberger, 2002), to address the issue of the locus of \ninterference in the different paradigms.\nREFERENCES\nBaudouin, J. Y., Martin, F., Tiberghien, G., Verlut, I., & Franck, N. \n(2002). Selective attention to facial emotion and identity in schizo-\nphrenia. Neuropsychologia, 40, 503-511.\nBenson, P. J., Campbell, R., Harris, T., Frank, M. G., & Tov\u00e9e, M. J. \n(1999). Enhancing images of facial expressions. Perception & Psy-\nchophysics, 61, 259-274.\nBrody, L. R., & Hall, J. A. (2000). Gender, emotion, and expression. \nIn M. Lewis & J. M. Haviland-Jones (Eds.), Handbook of emotions \n(2nd ed., pp. 338-349). New York: Guilford.\nBruce, V., Ellis, H., Gibling, F., & Young, A. (1987). Parallel pro-\ncessing of the sex and familiarity of faces. Canadian Journal of Psy-\nchology, 41, 510-520.\nBruce, V., & Young, A. (1986). Understanding face recognition. Brit-\nish Journal of Psychology, 77, 305-327.\nCalder, A. J., Burton, A. M., Miller, P., Young, A. W., & Aka-\nmatsu, S. (2001). A principal component analysis of facial expres-\nsions. Vision Research, 41, 1179-1208.\nCalder, A. J., Rowland, D., Young, A. W., Nimmo-Smith, I., \nKeane, J., & Perrett, D. I. (2000). Caricaturing facial expressions. \nCognition, 76, 105-146.\nCalder, A. J., Young, A. W., Rowland, D., & Perrett, D. I. (1997). \nComputer-enhanced emotion in facial expressions. Proceedings of \nthe Royal Society of London: Series B, 264, 919-925.\nCampbell, R., Brooks, B., de Haan, E., & Roberts, T. (1996). Disso-\nciating face processing skills: Decision about lip-read speech, expres-\nsion, and identity. Quarterly Journal of Experimental Psychology, \n49A, 295-314.\nCampbell, R., Landis, T., & Regard, M. (1986). Face recognition and \nlip-reading: A neurological dissociation. Brain, 109, 509-521.\nCohen, J., MacWhinney, B., Flatt, M., & Provost, J. (1993). Psy-\nScope: An interactive graphic system for designing and controlling \nexperiments in the psychology laboratory using Macintosh comput-\ners. Behavior Research Methods, Instruments, & Computers, 25, \n257-271.\nDe Houwer, J. (1998). The semantic Simon effect. Quarterly Journal \nof Experimental Psychology, 51A, 683-688.\nDe Houwer, J., Crombez, G., Baeyens, F., & Hermans, D. (2001). \nOn the generality of the affective Simon effect. Cognition & Emotion, \n15, 189-206.\nDe Houwer, J., & Eelen, P. (1998). An affective variant of the Simon \nparadigm. Cognition & Emotion, 12, 45-61.\nDe Houwer, J., Hermans, D., & Eelen, P. (1998). Affective Simon \neffects using facial expressions as affective stimuli. Zeitschrift f\u00fcr \nExperimentelle & Angewandte Psychologie, 45, 88-98.\nDubois, S., Rossion, B., Schiltz, C., Bodart, J. M., Michel, C., \nBruyer, R., & Crommelinck, M. (1999). Effect of familiarity on \nthe processing of human faces. NeuroImage, 9, 278-289.\nEimas, P. D., Tartter, V. C., Miller, J. L., & Keuthen, N. J. (1978). \nAsymmetric dependencies in processing phonetic features. Percep-\ntion & Psychophysics, 23, 12-20.\nEkman, P., & Friesen, W. V. (1971). Constants across cultures in the \nface and emotion. Journal of Personality & Social Psychology, 17, \n124-129.\nEkman, P., & Friesen, W. V. (1976). Pictures of facial affect. Palo Alto, \nCA: Consulting Psychologists Press.\nEkman, P., & Friesen, W. V. (1978). The facial action coding system. \nPalo Alto, CA: Consulting Psychologists Press.\nASYMMETRIC INTERFERENCE IN FACE PERCEPTION    1213\nEllis, A. W. (1989). Neuro-cognitive processing of faces and voices. \nIn A. W. Young & H. D. Ellis (Eds.), Handbook of research on face \nprocessing (pp. 207-215). Amsterdam: North-Holland.\nGanel, T., & Goshen-Gottstein, Y. (2002). Perceptual integrality of \nsex and identity of faces: Further evidence for the single-route hy-\npothesis. Journal of Experimental Psychology: Human Perception & \nPerformance, 28, 854-867.\nGarner, W. R. (1976). Interaction of stimulus dimensions in concept \nand choice processes. Cognitive Psychology, 8, 98-123.\nGoshen-Gottstein, Y., & Ganel, T. (2000). Repetition priming for \nfamiliar and unfamiliar faces in a sex-judgment task: Evidence for a \ncommon route for the processing of sex and identity. Journal of Ex-\nperimental Psychology: Learning, Memory, & Cognition, 26, 1198-\n1214.\nGreen, K. P., & Kuhl, P. K. (1991). Integral processing of visual place \nand auditory voicing information during phonetic perception. Journal \nof Experimental Psychology: Human Perception & Performance, 17, \n278-288.\nHasselmo, M. E., Rolls, E. T., & Baylis, G. C. (1989). The role of ex-\npression and identity in the face-selective responses of neurons in the \ntemporal visual cortex of the monkey. Behavioural Brain Research, \n32, 203-218.\nHaxby, J. V., Hoffman, E. A., & Gobbini, M. I. (2000). The distributed \nhuman neural system for face perception. Trends in Cognitive Sci-\nences, 4, 223-233.\nHess, U., & Kirouac, G. (2000). Emotional expression in groups. In \nM. Lewis & J. M. Haviland-Jones (Eds.), Handbook of emotions (2nd \ned., pp. 368-381). New York: Guilford.\nHumphreys, G. W., Donnelly, N., & Riddoch, M. J. (1993). Expres-\nsion is computed separately from facial identity, and it is computed \nseparately for moving and static faces: Neuropsychological evidence. \nNeuropsychologia, 31, 173-181.\nKirouac, G., & Hess, U. (1999). Group membership and the decoding \nof nonverbal behavior. In P. Philippot, R. S. Feldman, & E. J. Coats \n(Eds.), The social context of nonverbal behavior (pp. 182-210). Cam-\nbridge: Cambridge University Press.\nKornblum, S., & Lee, J. W. (1995). Stimulus\u2013response compatibil-\nity with relevant and irrelevant stimulus dimensions that do and do \nnot overlap with the response. Journal of Experimental Psychology: \nHuman Perception & Performance, 21, 855-875.\nKring, A. M., & Gordon, A. H. (1998). Sex differences in emotion: \nExpression, experience, and physiology. Journal of Personality & \nSocial Psychology, 74, 686-703.\nLaFrance, M., Hecht, M. A., & Paluck, E. L. (2003). The contingent \nsmile: A meta-analysis of sex differences in smiling. Psychological \nBulletin, 129, 305-334.\nLe Gal, P. M., & Bruce, V. (2002). Evaluating the independence of sex \nand expression in judgments of faces. Perception & Psychophysics, \n64, 230-243.\nLu, C.-H., & Proctor, R. W. (1995). The influence of irrelevant loca-\ntion information on performance: A review of the Simon and spatial \nStroop effects. Psychonomic Bulletin & Review, 2, 174-207.\nMacLeod, C. M. (1991). Half a century of research on the Stroop effect: \nAn integrative review. Psychological Bulletin, 109, 163-203.\nMelara, R. D., & Mounts, J. R. W. (1993). Selective attention to \nStroop dimensions: Effects of baseline discriminability, response \nmode, and practice. Memory & Cognition, 21, 627-645.\nM\u00fcnte, T. F., Brack, M., Grootheer, O., Wieringa, B. M., \nMatzke, M., & Johannes, S. (1998). Brain potentials reveal the \ntiming of face identity and expression judgments. Neuroscience Re-\nsearch, 30, 25-34.\nNosofsky, R. M., & Palmeri, T. J. (1997a). Comparing exemplar-\nretrieval and decision-bound models of speeded perceptual classifica-\ntion. Perception & Psychophysics, 59, 1027-1048.\nNosofsky, R. M., & Palmeri, T. J. (1997b). An exemplar-based ran-\ndom walk model of speeded classification. Psychological Review, \n104, 266-300.\nParry, F. M., Young, A. W., Saul, J. S., & Moss, A. (1991). Dissocia-\nble face processing impairments after brain injury. Journal of Clinical \n& Experimental Neuropsychology, 13, 545-558.\nPuce, A., Allison, T., Bentin, S., Gore, J. C., & McCarthy, G. \n(1998). Temporal cortex activation in humans viewing eye and mouth \nmovements. Journal of Neuroscience, 18, 2188-2199.\nRahman, R. A., Sommer, W., & Schweinberger, S. R. (2002). Brain-\npotential evidence for the time course of access to biographical facts \nand names of familiar persons. Journal of Experimental Psychology: \nLearning, Memory, & Cognition, 28, 366-373.\nSchweinberger, S. R., Burton, A. M., & Kelly, S. W. (1999). \nAsymmetric dependencies in perceiving identity and emotion: Ex-\nperiments with morphed faces. Perception & Psychophysics, 61, \n1102-1115.\nSchweinberger, S. R., & Soukup, G. R. (1998). Asymmetric rela-\ntionships among perceptions of facial identity, emotion, and facial \nspeech. Journal of Experimental Psychology: Human Perception & \nPerformance, 24, 1748-1765.\nSimon, J. R. (1990). The effects of an irrelevant directional cue on \nhuman information processing. In R. W. Proctor & T. Gilmour Reeve \n(Eds.), Stimulus\u2013response compatibility: An integrated perspective \n(pp. 31-86). Amsterdam: North-Holland.\nSimon, J. R., & Acosta, E., Jr. (1982). Effect of irrelevant information \non the processing of relevant information: Facilitation and\/or interfer-\nence? The influence of experimental design. Perception & Psycho-\nphysics, 31, 383-388.\nSimon, J. R., & Rudell, A. P. (1967). Auditory S\u2013R compatibility: The \neffect of an irrelevant cue on information processing. Journal of Ap-\nplied Psychology, 51, 300-304.\nStroop, J. R. (1935). Studies of interference in serial verbal reactions. \nJournal of Experimental Psychology, 18, 643-662.\nUmilt\u00e0, C., & Nicoletti, R. (1990). Spatial stimulus\u2013response com-\npatibility. In R. W. Proctor & T. G. Reeve (Eds.), Stimulus\u2013response \ncompatibility: An integrated perspective (pp. 89-116). Amsterdam: \nNorth-Holland.\nUmilt\u00e0, C., & Nicoletti, R. (1992). An integrated model of the Simon \neffect. In J. Alegria, D. Holender, J. Junca de Morais, & M. Radeau \n(Eds.), Analytic approaches to human cognition (pp. 331-350). Am-\nsterdam: North-Holland.\nYoung, A. W. (1998). Face and mind. Oxford: Oxford University Press.\nYoung, A. W., McWeeny, K. H., Hay, D. C., & Ellis, A. W. (1986). \nMatching familiar and unfamiliar faces on identity and expression. \nPsychological Research, 48, 63-68.\nYoung, A. W., Newcombe, F., de Haan, E. H. F., Small, M., & Hay, \nD. C. (1993). Face perception after brain injury: Selective impair-\nments affecting identity and expression. Brain, 116, 941-959.\nYoung, A. W., Perrett, D. I., Calder, A. J., Sprengelmeyer, R., & \nEkman, P. (2002). Facial expressions of emotion: Stimuli and tests \n(FEEST). Bury St. Edmunds: Thames Valley Test Company.\n(Manuscript received June 30, 2003;\nrevision accepted for publication February 11, 2005.)\n"}