{"doi":"10.1016\/j.cam.2010.12.014","coreId":"196839","oai":"oai:lra.le.ac.uk:2381\/9043","identifiers":["oai:lra.le.ac.uk:2381\/9043","10.1016\/j.cam.2010.12.014"],"title":"On Simulation of Tempered Stable Random Variates","authors":["Kawai, Reiichiro","Masuda, Hiroki"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-12-21","abstract":"Various simulation methods for tempered stable random variates with stability index greater than one are investigated with a view towards practical implementation, in particular cases of very small scale parameter, which correspond to increments of a tempered stable L\u00e9vy process with a very short stepsize. Methods under consideration are based on acceptance\u2013rejection sampling, a Gaussian approximation of a small jump component, and infinite shot noise series representations. Numerical results are presented to discuss advantages, limitations and trade-off issues between approximation error and required computing effort. With a given computing budget, an approximative acceptance\u2013rejection sampling technique Baeumer and Meerschaert (2009) [11] is both most efficient and handiest in the case of very small scale parameter and moreover, any desired level of accuracy may be attained with a small amount of additional computing effort","downloadUrl":"http:\/\/www.sciencedirect.com\/science\/journal\/03770427,","fullTextIdentifier":"https:\/\/lra.le.ac.uk\/bitstream\/2381\/9043\/1\/tssim.pdf","pdfHashValue":"ef6557f39a1149e1e66611d83d25d5bd58fe0391","publisher":"Elsevier","rawRecordXml":"<record><header><identifier>\n        \n            \n                oai:lra.le.ac.uk:2381\/9043<\/identifier><datestamp>\n                2015-12-11T10:46:46Z<\/datestamp><setSpec>\n                com_2381_445<\/setSpec><setSpec>\n                com_2381_9549<\/setSpec><setSpec>\n                col_2381_3823<\/setSpec>\n            <\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:doc=\"http:\/\/www.lyncode.com\/xoai\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n            \nOn Simulation of Tempered Stable Random Variates<\/dc:title><dc:creator>\nKawai, Reiichiro<\/dc:creator><dc:creator>\nMasuda, Hiroki<\/dc:creator><dc:subject>\nAcceptance\u2013rejection sampling<\/dc:subject><dc:subject>\nCompound Poisson<\/dc:subject><dc:subject>\nGaussian approximation<\/dc:subject><dc:subject>\nInfinite shot noise series<\/dc:subject><dc:subject>\nTempered stable distribution<\/dc:subject><dc:subject>\nCharacteristic function<\/dc:subject><dc:description>\nVarious simulation methods for tempered stable random variates with stability index greater than one are investigated with a view towards practical implementation, in particular cases of very small scale parameter, which correspond to increments of a tempered stable L\u00e9vy process with a very short stepsize. Methods under consideration are based on acceptance\u2013rejection sampling, a Gaussian approximation of a small jump component, and infinite shot noise series representations. Numerical results are presented to discuss advantages, limitations and trade-off issues between approximation error and required computing effort. With a given computing budget, an approximative acceptance\u2013rejection sampling technique Baeumer and Meerschaert (2009) [11] is both most efficient and handiest in the case of very small scale parameter and moreover, any desired level of accuracy may be attained with a small amount of additional computing effort.<\/dc:description><dc:date>\n2011-02-08T10:39:02Z<\/dc:date><dc:date>\n2011-02-08T10:39:02Z<\/dc:date><dc:date>\n2010-12-21<\/dc:date><dc:type>\nArticle<\/dc:type><dc:identifier>\nJournal of Computational and Applied Mathematics, 2011, 235 (8), pp. 2873-2887<\/dc:identifier><dc:identifier>\n0377-0427<\/dc:identifier><dc:identifier>\nhttp:\/\/www.sciencedirect.com\/science\/article\/pii\/S0377042710006643<\/dc:identifier><dc:identifier>\nhttp:\/\/hdl.handle.net\/2381\/9043<\/dc:identifier><dc:identifier>\n10.1016\/j.cam.2010.12.014<\/dc:identifier><dc:language>\nen<\/dc:language><dc:rights>\nThis is the author\u2019s final draft of the paper published as Journal of Computational and Applied Mathematics, 2011, 235 (8), pp. 2873-2887.  The final published version is available at http:\/\/www.sciencedirect.com\/science\/journal\/03770427, Doi: 10.1016\/j.cam.2010.12.014.<\/dc:rights><dc:publisher>\nElsevier<\/dc:publisher>\n<\/oai_dc:dc>\n<\/metadata>\n        <\/record>","journals":[{"title":null,"identifiers":["0377-0427","issn:0377-0427"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2010,"topics":["Acceptance\u2013rejection sampling","Compound Poisson","Gaussian approximation","Infinite shot noise series","Tempered stable distribution","Characteristic function"],"subject":["Article"],"fullText":"On Simulation of Tempered Stable Random Variates\nREIICHIRO KAWAI\u0003AND HIROKI MASUDA\u2020\nAbstract\nVarious simulation methods for tempered stable random variates with stability index greater than one\nare investigated with a view towards practical implementation, in particular cases of very small scale\nparameter, which correspond to increments of a tempered stable Le\u00b4vy process with a very short stepsize.\nMethods under consideration are based on acceptance-rejection sampling, a Gaussian approximation of a\nsmall jump component, and infinite shot noise series representations. Numerical results are presented to\ndiscuss advantages, limitations and trade-off issues between approximation error and required computing\neffort. With a given computing budget, an approximative acceptance-rejection sampling technique [2]\nis both most efficient and handiest in the case of very small scale parameter and moreover, any desired\nlevel of accuracy may be attained with a small amount of additional computing effort.\nKeywords: acceptance-rejection sampling, compound Poisson, Gaussian approximation, infinite shot\nnoise series, tempered stable distribution, characteristic function.\n2010 Mathematics Subject Classification: 65C10, 68U20, 60E07, 60B10.\n1 Introduction\nThe class of tempered stable distributions was first proposed by Tweedie [27]. Several featuring properties of\ntempered stable laws and processes were revealed by Rosin\u00b4ski [23], such as a stable-like behavior over short\nintervals, the absolute continuity with respect to its short-range limiting stable process, an aggregational\nGaussianity and an infinite shot noise series representation in closed form. Tempered stable distributions\nand processes have been used in a variety of applications, such as statistical physics [17, 20], mathematical\nfinance [4], financial econometrics [3] and mathematical biology [21], to mention just a few. Simulation of\nthe tempered stable distribution have thus been of great practical interest, in particular, for validation and\nestimation purposes.\nOn the one hand, it is well known that their increments with stability index smaller than one can be sim-\nulated exactly through, either single or double, acceptance-rejection sampling. (See Section 2.2 for more\ndetails.) To the best of our knowledge, on the other hand, there exist no practically exact simulation meth-\nods for tempered stable random variates with stability index greater than one. The purpose of this paper\nis to investigate and weigh various possible simulation techniques and discuss their advantages, limitations\nand trade-off between approximation error and computing effort, with a full view towards practical imple-\nmentation. We pay particular attention to the case of very small scale parameters, which corresponds to\nincrements of tempered stable Le\u00b4vy processes with a very short stepsize. This was motivated by application\nto approximation of Le\u00b4vy driven stochastic differential equations such as the Euler scheme, for which we\nhave to have many small time independent increments of the Le\u00b4vy process. Moreover, the generation is\nextremely important in simulation experiments concerning statistics, such as parameter estimation, for high\nPublished in Journal of Computational and Applied Mathematics (2011) 235(8) 2873-2887.\n\u0003Email Address: reiichiro.kawai@gmail.com. Postal Address: Department of Mathematics, University of Leicester, Leicester\nLE1 7RH, UK.\n\u2020Email Address: hiroki@math.kyushu-u.ac.jp. Postal Address: Graduate School of Mathematics, Kyushu University, Fukuoka\n819-0395, Japan.\n1\nfrequently observed Le\u00b4vy driven processes. The importance of our comparative study is clearly reinforced\nby ever-increasing practical demand of statistical inference for processes with jumps and wide applicability\nof the tempered stable process in modeling. (See, for example, Carr et al. [4], Masuda [18, 19], Todorov\nand Tauchen [26], and the references therein.) This work was initiated when the approximative simulation\nmethod of Section 3.2 due to Baeumer and Meerschaert [2] came to our attention, as we have long been\naware of the fact that simulation methods with the Gaussian approximation of Asmussen and Rosin\u00b4ski [1]\nare not as efficient as often claimed in the literature, due to the unaddressed practical trade-off between the\naccuracy of the normal approximation of small jumps and the required computing time to generate the com-\npound Poisson component for large jumps. In this paper, we illustrate through numerous numerical results\nthat simulation methods based on the Gaussian approximation are not necessarily the best and that different\ntechniques are worth testing whenever available.\nThe rest of this paper is organized as follows. After summarizing background material in brief on stable\nand tempered stable distributions in Section 2, we discuss acceptance-rejection sampling methods of De-\nvroye [7] and Baeumer and Meerschaert [2]. The method of [7] yields an exact simulation in principle, but\nrequires very time-consuming numerical integration for each fundamental quantity, while the approach of\n[2] provides an approximative, yet very handy and efficient, simulation method. In Section 4, we investigate\nsimulation methods based on a suitable decomposition of the Le\u00b4vy measure into small and large jump com-\nponents. We apply the well known Gaussian approximation of Asmussen and Rosin\u00b4ski [1] to the small jump\ncomponent. We also propose further compound Poisson extraction schemes on the small jump component.\nIn Section 5, we discuss yet another simulation method based on infinite shot noise series representations. In\nprinciple, only the infinite shot noise series provides an exact simulation method for tempered stable Le\u00b4vy\nprocesses since it simulates complete information of sample paths, that is, size, direction and timing of every\nsingle jump. A closed form of such a series representation is given in Rosin\u00b4ski [23] (first introduced in his\ndiscussion section of the article [3]). From a computational point of view, however, the form of infinite sum\nhas raised important issues of finite truncation to be addressed. (See Imai and Kawai [12].)\n2 Preliminaries\nLet us begin this preliminary section with the notations which will be used throughout the paper. We\ndenote by R the one dimensional Euclidean space with the norm j \u0001 j, R+ := (0;+\u00a5) and R\u0000 := (\u0000\u00a5;0).\nLet N be the collection of positive integers with N0 := N[ f0g. We denote by L= and L!, respectively,\nidentity and convergence in law. We write fL(z) for a smooth probability density function of a distribution\nL. We fix (W;F ;P) as our underlying probability space. We denote by C\u00a5b (R; R) by the class of infinitely\ndifferentiable functions from R to R which, together with all their derivatives, are bounded. Finally, the\ngamma function G(s) :=\nR +\u00a5\n0 x\ns\u00001e\u0000xdx, s> 0, can be extended to negative s thanks to analytic continuation\nand G(s+1) = sG(s). In particular, G(\u0000s)< 0 for s 2 (0;1), while G(\u0000s)> 0 for s 2 (1;2).\n2.1 Spectrally Positive Stable Processes\nLet fL(s)t : t \u0015 0g be a totally positively skewed stable (Le\u00b4vy) process satisfying\nE\nh\neiyL\n(s)\nt\ni\n= exp\nh\ntaG(\u0000a)cos\n\u0010pa\n2\n\u0011\njyja\n\u0010\n1\u0000 i tan pa\n2\nsgn(y)\n\u0011i\n=\n8<:exp\nh\nt\nR\nR+\n\u0000\neiyz\u00001\u0001 aza+1 dzi ; if a 2 (0;1);\nexp\nh\nt\nR\nR+\n\u0000\neiyz\u00001\u0000 iyz\u0001 aza+1 dzi ; if a 2 (1;2); (2.1)\nwith some a > 0. Throughout this paper, we exclude the case a = 1. We write S(a;a) :=L (L(s)1 ). Note\nthat the random variable L(s)1 takes values only in R+ if a 2 (0;1), while in R if a 2 (1;2). It holds that\n2\nfor each t > 0, L (L(s)t ) = S(a; ta), and by the scaling property, L (t\u00001=aL\n(s)\nt ) = S(a;a). Note that the\ndistribution S(a; ta) has density t\u00001=a fS(a;a)(t\u00001=ax). The distribution S(a;a) can be simulated in the exact\nsense through the well known representation, due to Chambers et al. [5],\nS(a;a) L= (\u0000aG(\u0000a)cos(pa=2))1=a sin(aU+q)\n(cosU cosq)1=a\n\u0012\ncos((1\u0000a)U\u0000q)\nE\n\u0013 1\u0000a\na\n; (2.2)\nwhere q := arctan(tan(pa=2)), U is a uniform random variable on (\u0000p=2;p=2) and E is a standard expo-\nnential random variable independent ofU . See Zolotarev [28] for complete details on the stable distribution.\n2.2 Spectrally Positive Tempered Stable Processes\nLet fL(ts)t : t \u0015 0g be a centered and totally positively skewed tempered stable (Le\u00b4vy) process satisfying\nE\nh\neiyL\n(ts)\nt\ni\n= exp\n\u0014\nt\nZ\nR+\n\u0000\neiyz\u00001\u0000 iyz\u0001a e\u0000bz\nza+1\ndz\n\u0015\n= exp\n\u0002\ntaG(\u0000a)\u0000(b\u0000 iy)a \u0000ba + iyaba\u00001\u0001\u0003 :\nWhen a 2 (0;1), by adding back the centering term as L(ts)t + tG(1\u0000a)aba\u00001, we can recover the tempered\nstable subordinator. Throughout the paper, we will use the notations\nTS(a;a;b) :=L\n\u0010\nL(ts)1\n\u0011\n; (2.3)\nand\nTS0(a;a;b) :=L\n\u0010\nL(ts)1 +G(1\u0000a)aba\u00001\n\u0011\n: (2.4)\nIt is known that\ne\u0000bz\nE\nh\ne\u0000bL\n(s)\n1\ni fS(a;a)(z) = e\u0000bz\u0000aG(\u0000a)ba fS(a;a)(z) = fTS0(a;a;b)(z); (2.5)\nand clearly\nfTS(a;a;b)(z) = fTS0(a;a;b)(z\u0000G(1\u0000a)aba\u00001)\n= e\u0000bz\u0000a(a+1)G(\u0000a)b\na\nfS(a;a)(z\u0000G(1\u0000a)aba\u00001): (2.6)\nThose hold for every a 2 (0;1)[ (1;2).\nLet us first focus on the case a 2 (0;1) with\nE\nh\neiy(L\n(ts)\nt +tG(1\u0000a)aba\u00001)\ni\n= exp\n\u0014\nt\nZ\nR+\n\u0000\neiyz\u00001\u0001a e\u0000bz\nza+1\ndz\n\u0015\n= exp [taG(\u0000a)((b\u0000 iy)a \u0000ba)] :\n(Note that this never holds for a 2 (1;2).) Based upon this fact and the density function (2.5), it is well\nknown (for example, [2, 8]) that when a 2 (0;1), the tempered stable distribution TS0(a;a;b) can be simu-\nlated exactly through acceptance-rejection sampling as follows.\nAlgorithm 0;\nStep 1. GenerateU as uniform (0;1) and V as S(a;a).\nStep 2. IfU \u0014 e\u0000bV , exit with V . Otherwise, return to Step 1.\nFor example, this exact acceptance-rejection sampling paves the way for efficient simulation of tempered\nstable Ornstein-Uhlenbeck processes. In particular, the acceptance rate increases to 1 as D # 0 and decreases\nto eaG(\u0000a)ba as D tends to infinity. (For more details, see Kawai and Masuda [13, 14].) Moreover, another\nexact double rejection method is developed in [8] based on the Zolotarev integral representation [28] of the\n3\ndensity function fS(a;a)(z). This double rejection is not handy anymore but has an interesting feature that\nit is more efficient than Algorithm 0 when D is large. Although Algorithm 0 is not necessarily the most\nefficient, it is still an exact and very handy method. For this reason, in this paper, we do not consider the\nrange a 2 (0;1).\nIn what follows, we concentrate on the case a 2 (1;2). In particular, Algorithm 0 cannot be simply\nextended to the range a 2 (1;2), due to the support of the whole real line R where the exponential tilting\nexplodes at either positive or negative infinity. (See Section 3.2 for details.) Without loss of generality, we\nfocus on a centered and totally positively skewed tempered stable random variate X(D) satisfying\njD(y) := E\nh\neiyX(D)\ni\n= exp\n\u0014\nD\nZ\nR+\n\u0000\neiyz\u00001\u0000 iyz\u0001a e\u0000bz\nza+1\ndz\n\u0015\n= exp\n\u0002\nDaG(\u0000a)\u0000(b\u0000 iy)a \u0000ba + iyaba\u00001\u0001\u0003 : (2.7)\nLet us note that simulation of increments of general infinite-variation tempered stable Le\u00b4vy processes (with\nbilateral jumps) is within our scope, as this can be done simply through a convolution of two independent\ntempered stable random variables totally skewed in the opposite directions. (See Remark 3.1 for a related\ndiscussion.)\nRemark 2.1. Let L(s)t and L\n(ts)\nt +tG(1\u0000a)aba\u00001 be random variables respectively with distributions S(a; ta)\nunder the probability measure Q and TS0(a; ta;b) under P. It is a straightforward application of Theorem\n33.3 of Sato [24] to evaluate an expected value related to tempered stable random variables by the density\ntransform\nEP\nh\nF\n\u0010\nL(ts)t\n\u0011i\n= EQ\n\u0014\ndP\ndQ\n\f\f\nG\nF\n\u0010\nL(s)t\n\u0011\u0015\n; (2.8)\nwithF :R!R such that EP[jF(L(ts)t )j]<+\u00a5. Here, the Radon-Nykodym derivative is given in closed form\n(dP=dQ)jG = e\u0000bL\n(s)\nt =EQ[e\u0000bL\n(s)\nt ], Q-a:s:, where G is the minimal s -field generated by the random variable\nL(s)t . The equality (2.8) is valid for every a 2 (0;1)[ (1;2). Evaluation of expectations based on (2.8) does\nnot require simulation of L(ts)t , but only requires simulation of L\n(s)\nt , which is simple through the representa-\ntion (2.2). This density transform formulation is found useful in the computation of Greeks under an asset\nprice model driven by tempered stable processes. (See Kawai and Takeuchi [15] for details.) However, this\nformulation is not valid for simulation of replications but only valid for evaluation of expectations.\n3 Acceptance-Rejection Sampling\nIn this section, we discuss two (one exact and the other approximative) acceptance-rejection sampling tech-\nniques for simulation of tempered stable random variables.\n3.1 Exact Sampling Using Density Function\nIt holds by the well known result of Devroye [7] that for z 2 R,\nfTS(a;Da;b)(z)\u0014min\n\u0014\nC1(D);\nC2(D)\nz2\n\u0015\n:= qD(z); (3.1)\nwhere\nC1(D) :=\n1\n2p\nZ\nR\njjD(y)jdy; C2(D) := 12p\nZ\nR\n\f\fj 00D(y)\f\fdy; (3.2)\nand that\nC3(D) :=\nZ\nR\nqD(z) = 4\np\nC1(D)C2(D):\n4\nLet U1 and U2 be iid uniform random variables on (\u00001;+1). It is also shown that the random variable V\ndefined by\nV :=\ns\nC2(D)\nC1(D)\nU1\nU2\n(3.3)\nhas density (C3(D))\u00001qD(z). Based on the above facts, we can employ a simulation method based on\nacceptance-rejection sampling as follows.\nAlgorithm 1;\nStep 1. Generate V as (3.3) andU asU(0; 1) independent of V . If jV j<pC2(D)=C1(D), then go to Step 3.\nStep 2. If\nC2(D)U < fTS(a;Da;b)(V )V 2;\nthen exit with Y1(D) V . Otherwise, go to Step 1.\nStep 3. If\nC1(D)U < fTS(a;Da;b)(V );\nthen exit with Y1(D) V . Otherwise, go to Step 1.\nThis is an exact simulation algorithm, that is, Y1(D)\nL\n= X(D). The expected number of times Step 1 is\nexecuted is C3(D), while the acceptance rate at Step 1 is given by\np1(D) :=\n1\nC3(D)\n:\nAs discussed in Remark 4 of [8], Algorithm 1 has already been enhanced in terms of constant shift in the\nsense that the tempered stable distribution is centered in our setting (2.7).\nBy recalling (2.7) and observing that\nj 00D(y) =\u0000jD(y)Da\nh\nDaG(1\u0000a)2 \u0000ba\u00001\u0000 (b\u0000 iy)a\u00001\u00012+G(2\u0000a)(b\u0000 iy)a\u00002i ;\nit seems difficult to obtainC1(D) andC2(D) in closed form. We thus need to computeC1(D) andC2(D) based\non (3.2) through some numerical integration techniques. Note that numerical integration here does not have\nto be extremely accurate, as long as the inequality (3.1) holds true.\nThe important point in question is how to prepare fTS(a;Da;b)(V ) in Step 2 and 3 of Algorithm 1, where as\nmentioned, the density fTS(a;Da;b)(z) is unavailable in closed form for any a 2 (1;2). One straightforward\napproach is to compute the density by the Fourier inverse of the characteristic function (2.7), while the\nother is to compute the density fS(a;Da)(z) of the associated stable distribution in order to use the relation\n(2.6). It would be more sensible to take the latter route since some math tools provide a function that\nreturns density values of the stable distribution, such as dstable in R language. With the help of such\nexisting functions, we may either (i) compute fS(a;Da)(V \u0000DG(1\u0000a)aba\u00001) whenever required for each V ,\n(ii) compute fS(a;Da)(vk) at several pre-selected points fvkgk2N and use interpolation for each replication,\nor could be (iii) a combination of them. Note that they are, strictly speaking, both approximative since\nnumerical integration is used. In principle, the choice is up to how many replications to be generated.\nTo discuss the efficiency of this numerical approach, the key quantities are the constants C1(D) and\nC2(D) and the acceptance rate p1(D). As D # 0, it tends to be more expensive to compute C1(D) due to\nlimD#0 jjD(y)j = 1 for each y 2 R, while less expensive to compute C2(D) due to limD#0 jj 00D(y)j = 0. It\nhowever seems difficult to discuss the computing time required for both in total. We report in Table 1 the\nacceptance rate p1(D) for various parameter settings. It seems safe to conclude that Algorithm 1 tends to be\nmore efficient (i) with a larger D, (ii) with a larger a , and (iii) with a larger b. In other words, Algorithm\n1 is more efficient when the tempered stable distribution is closer to a Gaussian distribution. (It is known\nthat the tempered stable distribution approaches to a Gaussian distribution with larger D, b and a , while it\n5\nb a D= 0:001 D= 0:010 D= 0:100 D= 1:000\n1.2 0.280 0.317 0.382 0.483\n0:1 1.5 0.483 0.499 0.529 0.573\n1.8 0.615 0.618 0.624 0.631\n1.2 0.328 0.400 0.505 0.597\n1:0 1.5 0.512 0.550 0.596 0.626\n1.8 0.623 0.629 0.634 0.636\n1.2 0.350 0.435 0.544 0.615\n2:0 1.5 0.527 0.571 0.612 0.632\n1.8 0.627 0.632 0.635 0.637\nTable 1: Numerical results of the acceptance rate p1(D) of Algorithm 1 under various parameter settings.\nWe fix a= 1 here.\nis closer to a stable distribution with smaller D and b. See, for example, Section 3 of [23].) We also provide\nin Figure 1 comparisons of the density fTS(a;Da;b)(z) and its bounding function qD(z) in the inequality (3.1).\nIn conclusion, it seems sensible to employ this approach for simulation of increments over a longer time\nstepsize, but not for simulation of small increments, for example in approximation of stochastic differential\nequations.\n\u22120.10 0.00 0.10\n0\n20\n40\n60\n\u22120.6 \u22120.2 0.2 0.6\n0\n1\n2\n3\n4\n\u22124 \u22122 0 2 40\n.0\n0.\n3\n0.\n6\n(a;D;b) = (1:2;0:001;0:1) (a;D;b) = (1:5;0:01;1:0) (a;D;b) = (1:8;0:1;2:0)\np1(D) = 0:280 p1(D) = 0:550 p1(D) = 0:635\nFigure 1: Comparison of fTS(a;Da;b)(z) (solid line) and qD(z) (dotted line) in the inequality (3.1). We fix\na= 1 here.\nRemark 3.1. For simulation of the bilateral tempered stable distribution, that is, with a characteristic func-\ntion\ny 7! exp\n\"\nD\nZ\nR0\n\u0000\neiyz\u00001\u0000 iyz\u0001 a+ e\u0000b+zza++11R+(z)+a\u0000 e\u0000b\u0000jzjjzja\u0000+11R\u0000(z)\n!\ndz\n#\n;\nwe need to implement Algorithm 1 at least twice; once for the positive component and the other for the\nnegative. This is so because the simple relation (2.6) does not hold simultaneously for both the positive and\nnegative components.\n3.2 Approximative Sampling with Stable Proposal Distribution\nThe second acceptance-rejection sampling is an approximative method of [2]. Let us first state the algorithm.\n6\nAlgorithm 2;\nStep 0. Fix c> 0.\nStep 1. GenerateU as uniform (0;1) and V (D) as S(a;Da).\nStep 2. IfU \u0014 e\u0000b(V (D)+c), exit with Y2(D;c) V (D)\u0000DG(1\u0000a)aba\u00001. Otherwise, return to Step 1.\nThis is not an exact simulation algorithm, that is, L (Y2(D;c)) 6= TS(a; Da; b) for any c 2 R+, due to the\nsupport of the whole real line R, rather than the half line R+ for a 2 (0;1). The constant shift \u0000DG(1\u0000\na)aba\u00001 in Step 2 accounts for the difference between (2.3) and (2.4). Basic properties of Algorithm 2 are\ndiscussed in [2]. The acceptance rate at Step 2 of Algorithm 2 is\np2(D;c) := E\nh\ne\u0000b(V (D)+c);V (D)>\u0000c\ni\n+P(V (D)\u0014\u0000c) :\nThe distribution function P(Y2(D;c)\u0014 z) and a density function fL (Y2(D;c))(z) are given by\nP(Y2(D;c)\u0014 z) = 1p2(D;c)\n\u0012\nP(V (D)\u0014min(x;\u0000c))+\nZ z\nmin(x;\u0000c)\ne\u0000b(y+c) fS(a;Da)(y)dy\n\u0013\n;\nfL (Y2(D;c))(z) =\n(\np2(D;c)\u00001 fS(a;Da)(z); if z 2 (\u0000\u00a5;\u0000c];\np2(D;c)\u00001e\u0000b(z+c) fS(a;Da)(z); if z 2 (\u0000c;+\u00a5):\nThe parameter c in Algorithm 2 acts as a truncation of the entire real line R to the domain on which the\nexponential tempering e\u0000bz is performed. It is also proved in Theorem 8 [2] that the density fL (Y2(D;c))(z)\nconverges in L1(R) to its target density fTS(a ;Da;b)(z) as c \" +\u00a5, and as a consequence, the Kolmogorov-\nSmirnov distance DKS(D;c) := DKS(L (Y2(D;c));TS(a;Da;b)) converges to zero as well. Nevertheless,\nit is not sensible to simply aim at a smaller distribution error by taking c \" +\u00a5, since then Algorithm 3\nbecomes extremely inefficient due to the low acceptance rate, that is, for each D > 0, limc\"+\u00a5 p2(D;c) = 0.\n(Note also that for each c> 0, limD#0 p2(D;c) = 1.)\nConcerning the computing effort, as before, we wish to find c maximizing p2(D;c). Asymptotic be-\nhaviors of p2(D;c) with respect to c are difficult to obtain in closed form. Next, it is not clear how\nto choose an appropriate criterion to measure the distribution error. Natural candidates include L1(R)-\nand L2(R)-distances between fL (Y2(D;c))(z) and fTS(a;Da;b)(z), while the Kolmogorov-Smirnov distance\nDKS(D;c) := DKS(L (Y2(D;c));TS(a;Da;b)) is certainly valid as well. None of them are tractable in an\nexplicit manner. Let us present in Table 2 numerical results of the quantity DKS(D;c)=p2(D;c) for different\nvalues of c. We only provide results for a single parameter set (a;a;b) = (1:5; 1:0; 1:0) and D = 0:1 and\nD= 0:01.\nD= 0:01 D= 0:10\nc DKS(D;c) p2(D;c) DKS(D;c)p2(D;c) c DKS(D;c) p2(D;c)\nDKS(D;c)\np2(D;c)\n0.00 2.48E-2 0.954 2.60E-2 0.0 1.26E-1 0.878 1.43E-1\n0.06 1.58E-2 0.931 1.69E-2 0.6 3.68E-2 0.661 5.57E-2\n0.12 6.28E-3 0.896 7.02E-3 0.8 1.29E-2 0.560 2.31E-2\n0.13 5.03E-3 0.889 5.66E-3 1.0 2.65E-3 0.464 5.71E-3\n0.14 4.73E-3 0.881 5.37E-3 1.1 9.48E-4 0.421 2.25E-3\n0.15 4.73E-3 0.874 5.42E-3 1.2 9.10E-4 0.381 2.39E-3\n0.18 4.73E-3 0.850 5.57E-3 1.3 9.10E-4 0.345 2.64E-3\nTable 2: Numerical results of distribution error and acceptance rate for different levels c.\nIt can be observed that the Kolmogorov-Smirnov distanceDKS(D;c) decreases in c, while the acceptance\nrate p2(D;c) has to decrease as well. The quantities DKS(D;c)=p2(D;c) indicate that choosing c greater\n7\nthan around 0:14 for D = 0:01 and 1:1 for D = 0:1 would not help in total, just as can be seen from the\nfact that DKS(D;c) does not improve anymore, while the acceptance rate p2(D;c) still gets worse. Let\nus however remind again that the Kolmogorov-Smirnov distance is simply one of various candidates as a\nmeasure of distribution error and the best choice of the parameter c may be different for different criteria.\nFinally, we draw in Figure 2 some resulting density functions fL (Y2(D;c))(z) with different choices of c when\nD= 0:1, together with the target tempered stable density function fTS(a;Da;b)(z). We do not provide figures\nfor D= 0:01, while two densities are almost indistinguishable even with a very small c> 0.\n\u22121.5 0.0 1.0 2.00\n.0\n0.\n4\n0.\n8\n1.\n2\n\u22121.5 0.0 1.0 2.00\n.0\n0.\n4\n0.\n8\n1.\n2\n\u22121.5 0.0 1.0 2.00\n.0\n0.\n4\n0.\n8\n1.\n2\nc= 0:0, p2(D;c) = 0:878 c= 0:6, p2(D;c) = 0:661 c= 1:1, p2(D;c) = 0:421\nFigure 2: Comparison of two density functions fL (Y2(D;c))(z) (solid line) and fTS(a;Da;b)(z) (dotted line)\nunder (a;a;b) = (1:5; 1:0; 1:0) and D= 0:10. The horizontal line indicates x=\u0000c\u0000DG(1\u0000a)aba\u00001.\nThe implementation of Algorithm 2 is very simple and requires no computation of a density function\nunlike in Algorithm 1. In particular, when D is small, the acceptance rate remains remarkably high while\nthe distribution error is negligible. We may find an optimal parameter value of c instantaneously through a\nstandard numerical approach, such as the Nelder-Mead direct search method. In total, this algorithm would\nbe a better choice than Algorithm 1 for simulation of the tempered stable distribution from a practical point\nof view.\nRemark 3.2. The Zolotarev integral representation is known [28] even for a 2 (1;2), but has to be expressed\nseparately on the positive and negative domains. It thus seems difficult to develop a double rejection method\n[8] of practical use.\n4 Decomposition into Small and Large Jump Components\nIn this section, we consider approximative simulation methods based on decomposition into a small jump\ncomponent and the remaining large jump component. To be more precise, write\ngD(z) := Da\ne\u0000bz\nza+1\n; z 2 R+;\nand decompose the characteristic exponent\nR\nR+(e\niyz\u0000 1\u0000 iyz)Daz\u0000a\u00001e\u0000bzdz into three independent com-\nponents as\nlnE\nh\neiyX(D)\ni\n=\nZ\nR+\n(eiyz\u00001\u0000 iyz)gD(z)dz\n=\nZ e\n0\n(eiyz\u00001\u0000 iyz)gD(z)dz+\nZ +\u00a5\ne\n(eiyz\u00001)gD(z)dz\u0000 iy\nZ +\u00a5\ne\nzgD(z)dz;\n=:h(1)e;D(y)+h\n(2)\ne;D(y)\u0000h(3)e;D(y);\n8\nfor some e > 0. First, the component h(3)e;D(y) clearly corresponds to a constant as\nh(3)e;D(y) = iy\nDa\na\u00001\n\u0010\ne1\u0000ae\u0000be \u0000ba\u00001G(2\u0000a;be)\n\u0011\n=: iyqe;D:\nIn what follows, we use the notations H(1)e;D and H\n(2)\ne;D for random variables satisfying\nlnE\n\u0014\neiyH\n(k)\ne;D\n\u0015\n= h(k)e;D(y); k = 1;2;\nand call H(1)e;D the small jump component and H\n(2)\ne;D the large jump component. It holds that for each e > 0\nand D> 0,\nX(D) L= H(1)e;D+H\n(2)\ne;D\u0000qe;D:\n4.1 Simulation of Large Jump Component\nWe first discuss simulation of the large jump component H(2)e;D. This component is compound Poisson with\nintensity\nxe;D :=\nZ +\u00a5\ne\nDa\ne\u0000bz\nza+1\ndz=\nDa\na(a\u00001)\n\u0012\n(a\u00001)e\u0000be\nea\n\u0000 be\n\u0000be\nea\u00001\n+baG(2\u0000a;be)\n\u0013\n: (4.1)\n4.1.1 Straightforward Compound Poisson Simulation\nThe straightforward method is based on the summation of iid suitable random variables through\nH(2)e;D\nL\n=\nNe;D\n\u00e5\nk=1\nYk;\nwhere Ne;D is a Poisson random variate with intensity xe;D and fYkgk2N is a sequence of iid random variables\nwith common probability density\n1\nxe;D\nDa\ne\u0000bz\nza+1\n; z 2 (e;+\u00a5):\nThis concept is indeed straightforward, while never as handy as often claimed in the literature, for mainly\ntwo reasons. First, when the truncation e is chosen too small (which is in principle desirable), the compound\nPoisson intensity xe;D explodes of order e\u0000a . Also, we need to deal with numerical integration and inversion\nof the imcomplete gamma function for the common distribution of the random sequence fYkgk2N.\n4.1.2 Acceptance-Rejection Sampling with Pareto Proposal Distribution\nWe here present an exact simulation technique for the large jump component H(2)e;D, which gets around the\naforementioned numerical integration and inversion. Notice that the Le\u00b4vy density of the component is\nbounded from above as\nDa\ne\u0000bz\nza+1\n\u0014 Dae\n\u0000be\naea\n\u0012\naea\nza+1\n\u0013\n=: x (0)e;D\naea\nza+1\n; z 2 (e;+\u00a5); (4.2)\nwhere aeaz\u0000a\u00001 serves as a Pareto probability density function on (e ;+\u00a5). This Pareto random variable\ncan easily be simulated as eU\u00001=a , where U \u0018U(0;1). Suppose that e and D are set such that x (0)e;D \u0015 1.\nThen, we can employ acceptance-rejection sampling for simulating the compound Poisson component as\nfollows.\n9\nAlgorithm 3;\nStep 1. GenerateU1 andU2 as independent uniform (0;1) and let V  eU\u00001=a2 .\nStep 2. IfU1 \u0014 e\u0000b(V\u0000e), exit with Y3(D) V . Otherwise, return to Step 1.\nThe acceptance rate at Step 2 of Algorithm 3 is 1=x (0)e;D. The expected number of times Step 1 is executed\nis thus x (0)e;D, while the expected number of times Algorithm 3 is executed for simulation of H\n(2)\ne;D is xe;D.\nTherefore, Step 1 will be executed xe;Dx\n(0)\ne;D times on average. Due to the explosion of this expected execu-\ntation time for ideally small e , it is difficult at this stage to claim that the discussed simulation of large jump\ncomponent is useful, although it is exact and straightforwad to implement. It also seems difficult to find a\nsimulation method for the jump component dominantly better than Algorithm 3.\nIn what follows, we assume that the compound Poisson component H(2)e;D is always simulated through\nAlgorithm 3 and will say that the expected time required for simulation of H(2)e;D is xe;Dx\n(0)\ne;D.\n4.2 Simulation of Small Jump Component\nIn this section, we turn to simulation of the small jump component H(1)e;D. As we have just observed, simula-\ntion of large jump component is exact but computationally very demanding. Hence, in order to convince the\nuser of the significance of the decomposition into small and large jump components for simulation purposes,\nit is necessary to have a very efficient and nearly exact method for small jump component.\nThroughout this subsection, we will use the notation\nkk(e;D) :=\nZ e\n0\nzkgD(z)dz=\nDa\nbk\u0000a\ng(k\u0000a;be):\nClearly, since the random variable H(1)e;D consists of infinitely many jumps, the compound Poisson simula-\ntion never applies. To investigate approximative simulation techniques for H(1)e;D, let us derive its first three\nmoments\nE\nh\nH(1)e;D\ni\n= 0;\nVar\n\u0010\nH(1)e;D\n\u0011\n=\nZ e\n0\nz2gD(z)dz= k2(e;D) =: s2e;D;\nE\n\u0014\u0010\nH(1)e;D\u0000E\nh\nH(1)e;D\ni\u00113\u0015\n=\nZ e\n0\nz3gD(z)dz= k3(e ;D):\nNote also that for each D> 0, kk(e;D)\u0018 ek\u0000aDa=(k\u0000a), as e # 0.\nFirst, based on the zero mean, it would be a valid approximation to simply replace H(1)e ;D by the mean\nvalue 0. The expected time required for this approximative simulation of X(D) remains same as the one\nrequired for simulation of H(2)e;D and is thus xe;Dx\n(0)\ne;D. For evaluation of weak approximation error, let us\nadopt the framework of Signahl [25] It holds by the Taylor theorem that for each f 2C\u00a5b (R;R),\nE [ f (X(D))]\u0000E\nh\nf\n\u0010\nH(2)e;D\u0000qe;D\n\u0011i\n=\ns2e;D\n2\nE\nh\nf 00\n\u0010\nH(2)e;D\u0000qe;D\n\u0011i\n+\nk3(e;D)\n6\nE\nh\nf 000\n\u0010\nH(2)e;D\u0000qe;D\n\u0011i\n+ \u0001 \u0001 \u0001 :\nHence, we get for each D> 0,\f\f\fE [ f (X(D))]\u0000Eh f \u0010H(2)e;D\u0000qe;D\u0011i\f\f\f= O\u0000e2\u0000a\u0001 ;\nas e # 0.\n10\nNext, it would be better to replaceH(1)e;D by with a normal random variable Ze;D, where Ze;D\u0018N (0;s2e;D).\nLet us write X (0)(e ;D) := Ze;D+H\n(2)\ne;D\u0000qe;D. This Gaussian approximation was justified in [1, 6] in a rigorous\nmanner and holds true in this case since s2e;D=e2 \u0018 Dae\u0000a=(2\u0000a) \" +\u00a5, as e # 0. By the addition of this\nGaussian component, the expected time required for simulation of X (0)(e ;D) is increased by 1 and is thus\nt(0)e;D := 1+xe;Dx\n(0)\ne;D: (4.3)\nTaking into account the undesirable limit lime#0 xe;Dx\n(0)\ne;D = +\u00a5, the addition of the Gaussian component is\nnegligible in terms of computing effort. Now, it holds by the Taylor theorem that for each f 2C\u00a5b (R;R),\nE\nh\nf\n\u0010\nX (0)(e;D)\n\u0011i\n\u0000E [ f (X(D))]\n= E\nh\u0010\nZe;D\u0000H(1)e;D\n\u0011\nf 0 (X(D))\ni\n+\n1\n2\nE\n\u0014\u0010\nZe;D\u0000H(1)e;D\n\u00112\nf 00 (X(D))\n\u0015\n+ \u0001 \u0001 \u0001 :\nBy further Taylor expansions and using the knowledge ofL (Ze;D), we get\f\f\fEh f \u0010X (0)(e;D)\u0011i\u0000E [ f (X(D))]\f\f\f\u0018 k3(e ;D)\n6\n\f\f\fEh f 000\u0010H(2)e;D\u0000qe;D\u0011i\f\f\f ;\nas e # 0. Recall that the true distribution has characteristic function jD(y). Meanwhile, it is straightforward\nthat the approximation has characteristic function\nj(0)e;D(y) := E\nh\neiyX\n(0)(e;D)\ni\n= jD(y)exp\n\u0014\n\u0000\nZ e\n0\n\u0012\neiyz\u00001\u0000 iyz+ 1\n2\ny2z2\n\u0013\ngD(z)dz\n\u0015\n= jD(y)exp\n\u0014\n\u0000jyj\n3\n3!\nZ e\n0\nh(yz)z3gD(z)dz\n\u0015\n;\nwhere h is a function from R to C satisfying jh(x)j \u0014 1 for x 2 R. Therefore, it holds by the Parseval\ntheorem that\nr(0)e;D :=\nZ\nR\n\f\f\f fTS(a;Da;b)(z)\u0000 fL (X (0)(e;D))(z)\f\f\f2 dz\n=\n1\n2p\nZ\nR\n\f\f\fjD(y)\u0000j(0)e;D(y)\f\f\f2 dy (4.4)\n\u0018 1\n2p\nZ\nR\njjD(y)j2\n\f\f\f\f jyj33!\nZ e\n0\nh(yz)z3gD(z)dz\n\f\f\f\f2 dy\n\u0014 k3(e;D)\n2\n72p\nZ\nR\n\f\fy3jD(y)\f\f2 dy= O\u0000e6\u00002a\u0001 ;\nwhere all the asymptotics hold when e # 0.\n4.2.1 Further Compound Poisson of Constant Density\nThe discussed Gaussian approximation is clearly very handy. It is then natural to ask whether the approxi-\nmation error can be reduced without a significant increase in computing time. A straightforward approach\nis to decompose the Le\u00b4vy density gD(z) over (0;e) into two independent components g\n(a)\ne;D(z) := gD(e) and\ngD(z)\u0000g(a)e;D(z). Accordingly, we write\nH(1)e;D = J\n(a)\ne;D+K\n(a)\ne;D\n11\nfor this decomposition. The first component J(a)e;D corresponds to the constant Le\u00b4vy density g\n(a)\ne;D(e) and is\nthus compound Poisson with intensity\nx (a)e;D :=\nDae\u0000be\nea\n; (4.5)\nwith iid densityU(0;e). This compound Poisson component should be centered and can be simulated in the\nexact sense as\nJ(a)e;D  \nN(a)e;D\n\u00e5\nk=1\neUk\u0000 Dae\n\u0000be\n2ea\u00001\n:\nThe remaining component K(a)e;D, corresponding to the Le\u00b4vy density gD(z)\u0000 g(a)e;D(z), is still centered, is of\ninfinite activity and is thus approximated by a normal random variable. Here, we define for each k 2 N,\nk(a)k (e ;D) :=\nZ e\n0\nzk\n\u0010\ngD(z)\u0000g(a)e;D(z)\n\u0011\ndz\n=\nDa\nbk\u0000a\ng(k\u0000a;be)\u0000 Dae\n\u0000be\n(k+1)ea\u0000k\n\u0018 Daek\u0000a 1+a\n(k\u0000a)(k+1) ;\nas e # 0. The Gaussian approximation can easily be justified with a variance k(a)2 (e;D). Let Z(a)e;D be a normal\nrandom variable with mean zero and a variance k(a)2 (e;D). In a similar manner to the previous case, it holds\nby the Taylor theorem that for each f 2C\u00a5b (R;R),\nE\nh\nf\n\u0010\nX (a)(e ;D)\n\u0011i\n\u0000E [ f (X(D))]\n= E\nh\u0010\nZ(a)e;D\u0000K(a)e;D\n\u0011\nf 0 (X(D))\ni\n+\n1\n2\nE\n\u0014\u0010\nZ(a)e;D\u0000K(a)e;D\n\u00112\nf 00 (X(D))\n\u0015\n+ \u0001 \u0001 \u0001 ;\nwhere\nX (a)(e;D) := Z(a)e;D+ J\n(a)\ne;D+H\n(2)\ne;D\u0000qe;D:\nBy further Taylor expansions of f 0 and f 00 and using the fact that Z(a)e;D is Gaussian, we get\f\f\fEh f \u0010X (a)(e ;D)\u0011i\u0000E [ f (X(D))]\f\f\f\u0018 k(a)3 (e;D)\n6\n\f\f\fEh f 000\u0010H(2)e ;D\u0000qe;D\u0011i\f\f\f ;\nas e # 0. Therefore, by further introducing this compound Poisson J(a)e;D, we can reduce the weak error by a\nfactor of\nlim\ne#0\nk(a)3 (e;D)\nk3(e;D)\n=\n1+a\n4\n2\n\u0012\n1\n2\n;\n3\n4\n\u0013\n:\nAs before, it holds by the Parseval theorem that\nr(a)e;D :=\n1\n2p\nZ\nR\n\f\f\fjD(y)\u0000j(a)e;D(y)\f\f\f2 dy (4.6)\n=\n1\n2p\nZ\nR\njjD(y)j2\n\f\f\f\fexp\u0014\u0000Z e0 \u0000eiyz\u00001\u0000 iyz\u0001\n\u0010\ngD(z)\u0000g(a)e;D(z)\n\u0011\ndz\n\u0015\n\u00001\n\f\f\f\f2 dy\n\u0018 1\n2p\nZ\nR\njjD(y)j2\n\f\f\f\f jyj33!\nZ e\n0\nh(yz)z3\n\u0010\ngD(z)\u0000g(a)e;D(z)\n\u0011\ndz\n\f\f\f\f2 dy\n\u0014\n\u0010\nk(a)3 (e;D)\n\u00112\n72p\nZ\nR\n\f\fy3jD(y)\f\f2 dy;\n12\nwhere all the asymptotics hold when e # 0. Clearly, the expected time required for simulation of X (a)(e;D)\nis given by\nt(a)e;D := 1+x\n(a)\ne;D+xe;Dx\n(0)\ne;D: (4.7)\nRemark 4.1. It is straightforward that the compound Poisson component J(a)e;D and the large jump component\nH(2)e;D of Section 4.1 can be simulated exactly as a single compound Poisson random variable. Define\nl(a)e;D :=\nx (a)e;D\nx (a)e;D+xe;D\n; r(a)e;D :=\nxe;D\nx (a)e;D+xe;D\n;\nwhere x (a)e;D and xe;D are defined, respectively, in (4.5) and (4.1). Clearly, l\n(a)\ne;D+ r\n(a)\ne;D = 1. Then, it holds that\nJ(a)e;D+H\n(2)\ne;D\nL\n=\neN(a)e;D\n\u00e5\nk=1\nY (a)k \u0000Dae\u0000be2ea\u00001;\nwhere eN(a)e;D is a Poisson random variable with intensity x (a)e;D+xe;D and fY (a)k gk2N is a sequence of iid random\nvariables with common distribution which can be simulated exactly as follows.\nAlgorithm 3(a);\nStep 1. GenerateU1  U(0;1).\nStep 2. IfU1 \u0014 l(a)e;D, then exit with eU1=l(a)e;D.\nStep 3. GenerateU2  U(0;1) and let V  e((U1\u0000 l(a)e;D)=r(a)e;D)\u00001=a . IfU1 \u0014 e\u0000b(V\u0000e), then exit with V .\nOtherwise, go to Step 1.\nWe can show that the expected total time (number of implementations of Step 1) for simulation of J(a)e;D+H\n(2)\ne;D\nis \u0010\nx (a)e;D+xe;Dx\n(0)\ne;D\n\u0011 r(a)e;D\u00101\u00001=x (0)e;D\u0011\u0010\n1\u0000 r(a)e;D\n\u0010\n1\u00001=x (0)e;D\n\u0011\u0011 ;\nwhile the expected total time for simulation of J(a)e;D and H\n(2)\ne;D separately is x\n(a)\ne;D+xe;Dx\n(0)\ne;D. We can also show\nthat the use of Algorithm 3(a) helps reduce computing effort if e is sufficiently large, while increases it by a\nfactor of 2 as e # 0. We thus do not consider using this algorithm as our interest is mainly in a small e .\n4.2.2 Further Compound Poisson of Exploding but Integrable Density\nBefore proceeding to numerical experiments, let us consider further taking more compound Poisson mass,\nwhich can be simulated exactly, out of the small jump component (and then approximate the reminder by\nGaussian). To this end, we extract the density\ng(b)e;D(z) := Da\ne\u0000be\ne1+d\n1\nza\u0000d\n; (4.8)\nfor some d 2 (a\u00001;a), from the Le\u00b4vy density gD(z) over (0;e). As before, we decompose as\nH(1)e;D = J\n(b)\ne;D+K\n(b)\ne;D;\n13\nwhere J(b)e;D indicates the centered compound Poisson random variable corresponding to the density (4.8),\nwhile the K(b)e;D is the remaining infinite activity component to be approximated by Gaussian. The compound\nPoisson intensity is given by\nx (b)e;D :=\nZ e\n0\ng(b)e;D(z)dz=\nDae\u0000be\n(d \u0000a+1)ea : (4.9)\nNote here that this is independent of d in the sense of asymptotics of e # 0. We can derive that J(b)e;D can be\nsimulated exactly as\nJ(b)e;D  \nN(b)e;D\n\u00e5\nk=1\n(eaUk)\n1\nd\u0000a \u0000 e\u0000a d\u0000a\u00001d\u0000a ae\n\u0000be(d \u0000a)\n(d \u0000a+1)2 ;\nwhere N(b)e;D is a Poisson random variable with intensity x\n(b)\ne;D and fUkg is a sequence of iid uniform random\nvariables on (0;1) as before. Using\nk(b)k (e;D) :=\nZ e\n0\nzk\n\u0010\ngD(z)\u0000g(b)e;D(z)\n\u0011\ndz\n=\nDa\nbk\u0000a\ng(k\u0000a;be)\u0000 Dae\n\u0000be\n(k+1+d \u0000a)ea\u0000k\n\u0018 Dae\nk\u0000a\n(k\u0000a)(k+1+d \u0000a) ;\nas e # 0, we can derive\f\f\fEh f \u0010X (b)(e ;D)\u0011i\u0000E [ f (X(D))]\f\f\f\u0018 k(b)3 (e ;D)\n6\n\f\f\fEh f 000\u0010H(2)e;D\u0000qe;D\u0011i\f\f\f ;\nas e # 0, where\nX (b)(e;D) := Z(b)e;D+ J\n(b)\ne;D+H\n(2)\ne;D\u0000qe;D:\nTherefore, by further introducing the compound Poisson J(b)e;D, we can reduce the weak error by a factor of\nlim\ne#0\nk(b)3 (e;D)\nk3(e ;D)\n=\n1\n4+d \u0000a :\nSince d can be taken arbitrarily in (a\u00001;a), we can improve the approximation error down by a factor of\n1=4, compared to the simplest Gaussian approximation. Moreover, comparing with the one introduced in\nSection 4.2.1, we get\nlim\ne#0\nk(b)3 (e;D)\nk(a)3 (e;D)\n=\n4\n(1+a)(4+d \u0000a) :\nHence, we can further improve the approximation error by up to a factor of 1=(1+a). As before, it holds\nby the Parseval theorem that\nr(b)e;D :=\n1\n2p\nZ\nR\n\f\f\fjD(y)\u0000j(b)e;D(y)\f\f\f2 dy (4.10)\n=\n1\n2p\nZ\nR\njjD(y)j2\n\f\f\f\fexp\u0014\u0000Z e0 \u0000eiyz\u00001\u0000 iyz\u0001\n\u0010\ngD(z)\u0000g(b)e;D(z)\n\u0011\ndz\n\u0015\n\u00001\n\f\f\f\f2 dy\n\u0018 1\n2p\nZ\nR\njjD(y)j2\n\f\f\f\f jyj33!\nZ e\n0\nh(yz)z3\n\u0010\ngD(z)\u0000g(b)e;D(z)\n\u0011\ndz\n\f\f\f\f2 dy\n\u0014\n\u0010\nk(b)3 (e ;D)\n\u00112\n72p\nZ\nR\n\f\fy3jD(y)\f\f2 dy;\n14\nwhere all the asymptotics hold when e # 0. Clearly, the expected time required for simulation of X (b)(e;D)\nis given by\nt(b)e;D := 1+x\n(b)\ne;D+xe;Dx\n(0)\ne;D: (4.11)\nWe are in a position to present some numerical results and discuss whether the decomposition into\nsmall and large jump components is useful for simulation purpose. From a practical point of view, we\npresent in Table 3 the quantities r(0)e;D, r\n(a)\ne;D and r\n(b)\ne;D, defined respectively by (4.4), (4.6) and (4.10), for the\napproximation error, and t(0)e;D, t\n(a)\ne;D and t\n(b)\ne;D, defined respectively by (4.3), (4.7) and (4.11), for the required\ncomputing time. We computed r(0)e;D, r\n(a)\ne;D and r\n(b)\ne;D as precisely as possible by numerical integration of their\ndefinitions and did not use their asymptotic upper bounds. To check a relatively extreme setting for the\ntechnique of Section 4.2.2, we set d = a \u0000 1+ 0:1 in (4.8). In addition, to compare with Algorithm 2 of\nSection 3.2, we fix (a; a; b) = (1:5; 1:0; 1:0) and present the corresponding quantities (\nR\nR j fTS(a;Da;b)(z)\u0000\nfL (Y2(D;c))(z)j2dz)1=2 and 1=p2(D;c) of Algorithm 2. In short, in the decomposition framework, a lot of\nadditional computing effort is required for improvement in approximation error, either by taking a smaller\ntruncation e or by introducing the techniques of Section 4.2.1 and 4.2.2. To achieve a similar level of\napproximation error to the method of Section 3.2, the decomposition framework requires an extraordinary\nlarger computing effort.\nD= 0:01\napproximation error computing time Algorithm 2\ne r(0)e;D r\n(a)\ne;D r\n(b)\ne;D t\n(0)\ne;D t\n(a)\ne;D t\n(b)\ne;D c error time\n0.033 3.48E-2 2.21E-2 7.79E-3 2.15E+0 3.77E+0 1.83E+1 0.00 8.83E-2 1.04E+0\n0.030 3.03E-2 1.92E-2 6.74E-3 2.55E+0 4.41E+0 2.21E+1 0.12 2.55E-2 1.11E+0\n0.020 1.68E-2 1.06E-2 3.65E-3 6.33E+0 9.80E+0 4.10E+1 0.14 1.66E-2 1.13E+0\n0.010 6.01E-3 3.76E-3 1.28E-3 4.46E+1 5.45E+1 1.44E+2 0.18 5.26E-3 1.17E+0\n0.005 2.13E-3 1.33E-3 4.52E-4 3.53E+2 3.81E+2 6.34E+2 0.20 2.54E-3 1.19E+0\n0.003 9.91E-4 6.20E-4 2.10E-4 1.64E+3 1.70E+3 2.24E+3 0.24 4.88E-4 1.24E+0\n0.001 1.91E-4 1.19E-4 4.02E-5 4.44E+4 4.47E+4 4.75E+4 0.30 2.02E-4 1.32E+0\nD= 0:1\napproximation error computing time Algorithm 2\ne r(0)e;D r\n(a)\ne;D r\n(b)\ne;D t\n(0)\ne;D t\n(a)\ne;D t\n(b)\ne;D c error time\n0.130 1.59E-2 1.03E-2 4.05E-3 2.50E+0 4.38E+0 2.12E+1 0.00 2.02E-1 1.14E+0\n0.120 1.43E-2 9.22E-3 3.57E-3 2.96E+0 5.09E+0 2.43E+1 0.60 6.42E-2 1.51E+0\n0.100 1.10E-2 7.08E-3 2.68E-3 4.55E+0 7.42E+0 3.32E+1 0.80 2.35E-2 1.78E+0\n0.010 3.71E-4 2.33E-4 7.94E-5 4.36E+3 4.46E+3 5.35E+3 1.10 1.93E-3 2.37E+0\n0.005 1.32E-4 8.24E-4 2.79E-5 3.52E+4 3.55E+4 3.80E+4 1.30 1.63E-4 2.90E+0\n0.001 1.18E-5 7.40E-6 2.50E-6 4.44E+6 4.44E+6 4.47E+6 1.50 9.72E-6 3.54E+0\n0.0005 4.20E-6 2.60E-6 9.00E-7 3.55E+7 3.55E+7 3.56E+7 1.80 5.71E-6 4.78E+0\nTable 3: Numerical results of the approximation error and the required computing time\n5 Infinite Shot Noise Series Representation with Finite Truncation\nIt is known that every infinitely divisible random variable admits a shot noise series representation, and\nthat the series is infinite if Le\u00b4vy measure is infinite. Here, we discuss in brief such representations of\nthe tempered stable distribution with a view towards simulation. Fix (l ;l1;l2) 2 R3+ and define g(D) :=\n(Da=a)\u00001=az (1=a)\u0000DG(1\u0000a)aba\u00001 where z detnoes the Riemann zeta function. We denote by fGkgk2N\narrival times of a standard Poisson process, by fUkgk2N a sequence of iid uniform random variables on\n[0;1], by fE(1)k gk2N and fE(4)k gk2N sequences of iid standard exponential random variables, by fE(2)k gk2N\n15\na sequence of iid exponential random variables with rate bl1, and by fE(3)k gk2N a sequence of iid gamma\nrandom variables with shape l1 and scale (bl2)\u00001. Finally, define\n \u0000q (s) := inf\n\u001a\nr > 0 :\nZ +\u00a5\nr\na\ne\u0000bz\nza+1\ndz> s\n\u001b\n; s> 0:\nNote that this function cannot be written in closed form. Then, the tempered stable random variable X(D)\ncan be written as\nX(D)\u0000 g(D) L=\n+\u00a5\n\u00e5\nk=1\n\"\n \u0000q\n\u0012\nGk\nD\n\u0013\n\u0000\n\u0012\nak\nDa\n\u0013\u00001=a#\n(5.1)\nL\n=\n+\u00a5\n\u00e5\nk=1\n\"\"\u0012\naGk\nDa\n\u0013\u00001=a\n^ E\n(1)\nk U\n1=a\nk\nb\n#\n\u0000\n\u0012\nak\nDa\n\u0013\u00001=a#\n(5.2)\nL\n=\n+\u00a5\n\u00e5\nk=1\n\"\nE(2)k 1\n \nGk \u0014 Da\nlb(E(2)k )a+1\ne\u0000b(1\u0000l )E\n(2)\nk\n!\n\u0000\n\u0012\nak\nDa\n\u0013\u00001=a#\n(5.3)\nL\n=\n+\u00a5\n\u00e5\nk=1\n\"\nE(3)k 1\n \nGk \u0014 Da\n(bl2)l1(E\n(3)\nk )\na+l1\ne\u0000b(1\u0000l2)E\n(3)\nk\n!\n\u0000\n\u0012\nak\nDa\n\u0013\u00001=a#\n(5.4)\nL\n=\n+\u00a5\n\u00e5\nk=1\n\"\u0012\naGk\nDa\n\u0013\u00001=a\n1\n \u0012\naGk\nDa\n\u0013\u00001=a\n\u0014 E\n(4)\nk\nb\n!\n\u0000\n\u0012\nak\nDa\n\u0013\u00001=a#\n: (5.5)\nThe representation (5.1) is derived with the inverse Le\u00b4vy measure method due to Ferguson and Klass [9]\nand LePage [16]. The representation (5.2) is derived with the generalized shot noise method due to [23],\nwhile the others (5.3)-(5.5) are due to Imai and Kawai [12] and are derived with the thinning method [22]\nfor (5.3) and (5.4) and the rejection method [22] for (5.5). It is obviously insensible to generate the above\ninfinite sum for sample simulation of only one increment X(D), while it may make sense to generate many\niid replications of X(D). (See [10] for some techniques for computation of expectation involving series\nrepresentations.) Suppose we wish to generate N of them, N = 100000 say. Let T = ND and let fTkgk2N\nbe a sequence of iid uniform random variables on [0;T ]. Then, a tempered stable Le\u00b4vy process admits the\nfollowing series representation (based on (5.1), for example)\nn\nL(ts)t : t 2 [0;T ]\no\nL\n=\n(\n+\u00a5\n\u00e5\nk=1\n\"\n \u0000q\n\u0012\nGk\nT\n\u0013\n1[0;t](Tk)\u0000\nt\nT\n\u0012\nak\nTa\n\u0013\u00001=a#\n+\nt\nT\ng(T ) : t 2 [0;T ]\n)\n: (5.6)\nIts increments fL(ts)kD \u0000L(ts)(k\u00001)Dgk=1;:::;N with equidistant stepsize D form a sequence of iid random variables\nwith common lawL (X(D)) for every k = 1; : : : ;N.\nLet us discuss in brief an issue of trade-off between computing time and finite trunction of the infinite\nsum. To this end, we take the representation (5.1) with the finite truncation fk 2 N : Gk \u0014 ng, as this\ncombination reveals a meaningful probabilistic structure to explain a duality to the decomposition employed\nin Section 4. Let nn denote the Le\u00b4vy measure of the infinitely divisible random variable defined by\n\u00e5\nfk2N:Gk\u0014ng\n\"\n \u0000q (Gk)\u0000\n\u0012\nak\na\n\u0013\u00001=a#\n+ g(1); (5.7)\nand let n := n+\u00a5 denote the original Le\u00b4vy measure. The decomposition of Section 4 is to divide the original\nLe\u00b4vy measure n into the small jump component n j(0;e] and the large jump component n j(e;+\u00a5), while the\nseries representation (5.7) simulates the compound Poisson component nn = n j( \u0000q (n);+\u00a5) and discards the\nrest n j(0; \u0000q (n)] near the origin. Hence, the discussion of Section 4 about the heavy computation load required\n16\nfor the compound Poisson component for large jumps applies to the series representation (5.7) as soon as\nthe threshold e of Section 4 is replaced with  \u0000q (n). Namely, the simulation methods discussed in Section\n4.1 are to be replaced by the series representation (5.7), while the small jump component should be treated\nthrough the techniques of Section 4.2. In addition, although a single sample path of the tempered stable\nLe\u00b4vy process (5.6) can provide N iid random variables, it should not be ignored that additional computing\nof an indexed search is required for the uniform dispersion of jumps based on the indicator 1((k\u00001)D;kD](\u0001)\nin (5.6). It is also known [11, 12] that the representation (5.1) can express more variability of randomness\nthan any others (5.2)-(5.5) under the common finite truncation fk 2 N : Gk \u0014 ng. For those reasons, as\nfar as tempered stable random variables are concerned, this framework does not improve the situation of\nSection 4 in any significant manner. Let us close this section with remarking that the choice of threshold is\nan important issue to be addressed whenever the decomposition of Section 4 and\/or the series representation\nof this section are the only possibility.\n6 Concluding Remarks\nIn this paper, we have investigated various simulation methods of tempered stable laws with stability index\ngreater than one, with primal interest in simulation of increments X(D) over a very short stepsize D > 0:\na suitable setting for approximation of stochastic differential equations through the Euler scheme. From a\npractical point of view, we have sought a simulation recipe of a good balance between computational load\nand approximation error, together with implementation ease. Results can be summarized as follows.\n\u000f Themodel-free acceptance-rejection sampling method of [8] provides an exact simulation method,\nin principle, but requires a lot of computing effort for computing density values. This method\nexhibits quite low acceptance rate when D is small and the stability index a is close to 2, that is,\nwhen the target is close to Gaussian.\n\u000f The acceptance-rejection sampling of [2] is approximative yet very handy with both very small\ncomputing time and approximation error. Finding an optimal value of the tuning parameter is\nrelatively straightforward and is required only once in advance.\n\u000f The Gaussian approximation of [1] provides a different route to approximative simulation. We\nhave shown that in this framework, the approximation error can be made very small by either\nsimulating more large jump component or simulating more mass of the small jump component\nas compound Poisson random variables, while an extraordinary large amount of computing effort\nis additionally required for an improvement in approximation error, which may be a drawback in\npractice when thousands of tempered stable variates of small scale are needed.\n\u000f Infinite shot noise series representations of tempered stable laws can also be used for simulation.\nEven after a finite truncation of infinite shot noise series, a large computing effort needs to be\npaid to attain satisfactory approximation error. The trade-off between approximation error and\ncomputing time based upon the finite truncation has in principal a duality relation to the framework\nof the Gaussian approximation.\nIn conclusion, with a given computing budget, the approximative acceptance-rejection sampling of [2] is\nboth most efficient and handiest based on numerical assessment of accuracy for simulation of increments\nin small time. The decomposition and the series representation are hardly competitive against Baeumer-\nMeerschaert as computation load for the compound Poisson part is too heavy. However, the choice of the\nthreshold is certainly an important issue for simulation of infinitely divisible laws for which the decomposi-\ntion and\/or the series representation are the only possibility.\n17\nAcknowledgement\nThe authors would like to thank the anonymous referees for various helpful comments and suggestions.\nResearch of HM is supported in part by Grant-in-Aid for Young Scientists (B) 20740061, Japan.\nReferences\n[1] Asmussen, S., Rosin\u00b4ski, J. (2001) Approximation of small jumps of Le\u00b4vy processes with a view towards simulation, Journal\nof Applied Probability, 38(2) 482-493.\n[2] Baeumer, B., Meerschaert, M.M. (2009) Tempered stable Le\u00b4vy motion and transit super-diffusion, Journal of Computational\nand Applied Mathematics, 223(10) 2438-2448.\n[3] Barndorff-Nielsen, O.E., Shephard, N. (2001) Non-Gaussian Ornstein-Uhlenbeck-based models and some of their uses in\nfinancial economics (with discussion), J. R. Statist. Soc. B, 63(2) 167-241.\n[4] Carr, P., Geman, H., Madan, D.B., Yor, M. (2002) The fine structure of asset returns: An empirical investigation, Journal of\nBusiness, 75, 303-325.\n[5] Chambers, J.M., Mallows, C.L., Stuck, B.W. (1976) A method for simulating stable random variables, Journal of the Amer-\nican Statistical Association, 71(354) 340-344.\n[6] Cohen, S., Rosin\u00b4ski, J. (2007) Gaussian approximation of multivariate Le\u00b4vy processes with applications to simulation of\ntempered stable processes, Bernoulli, 13(1) 195-210.\n[7] Devroye, L. (1981) On the computer generation of random variables with a given characteristic function, Computers and\nMathematics with Applications, 7(6) 547-552.\n[8] Devroye, L. (2009) Random variate generation for exponential and polynomially tilted stable distributions, ACM Transac-\ntions on Modeling and Computer Simulation, 19(4) Article No. 18.\n[9] Ferguson, T.S., Klass, M.J. (1972) A representation of independent increment processes with Gaussian components, Annals\nof Mathematical Statistics, 43(5) 1634-1643.\n[10] Imai, J., Kawai, R. (2010) Quasi-Monte Carlo method for infinitely divisible random vectors via series representations, SIAM\nJournal on Scientific Computing, 32(4) 1879-1897.\n[11] Imai, J., Kawai, R. (2010) Numerical inverse Le\u00b4vy measure method for infinite shot noise series representation, available at\nhttp:\/\/sites.google.com\/site\/reiichirokawai\/\n[12] Imai, J., Kawai, R. (2010) On finite truncation of infinite shot noise series representation of tempered stable laws, available\nat http:\/\/sites.google.com\/site\/reiichirokawai\/\n[13] Kawai, R., Masuda, H. (2009) Exact simulation of finite variation tempered stable Ornstein-Uhlenbeck processes, available\nat http:\/\/hdl.handle.net\/2324\/15635\n[14] Kawai, R., Masuda, H. (2010) Infinite variation tempered stable Ornstein-Uhlenbeck processes with discrete observations,\navailable at http:\/\/hdl.handle.net\/2324\/16248\n[15] Kawai, R., Takeuchi, A. (2009) Computation of Greeks for asset price dynamics driven by stable and tempered stable\nprocesses, available at https:\/\/sites.google.com\/site\/reiichirokawai\/\n[16] LePage, R. (1980) Multidimensional infinitely divisible variables and processes II, In: Lecture Notes in Mathematics 860,\nSpringer-Verlag, Berlin, New York, Heidelberg, 279-284.\n[17] Mantegna, R.N., Stanley, H.E. (1994) Stochastic process with ultraslow convergence to a Gaussian: The truncated Le\u00b4vy\nflights, Physical Review Letters, 73, 2946-2949.\n[18] Masuda, H. (2010) Approximate self-weighted LAD estimation of discretely observed ergodic Ornstein-Uhlenbeck pro-\ncesses. Electronic Journal of Statistics, 4, 525\u2013565.\n[19] Masuda, H. (2010) Approximate quadratic estimating function for discretely observed Le\u00b4vy driven SDEs with ap-\nplication to a noise normality test, MI Preprint Series, Kyushu University, 2010-21, available at https:\/\/qir.kyushu-\nu.ac.jp\/dspace\/bitstream\/2324\/17028\/1\/MI2010-21.pdf\n[20] Novikov, E.A. (1994) Infinitely divisible distributions in turbulence, Physical Review E, 50, R3303-R3305.\n[21] Palmer, K.J., Ridout, M.S., Morgan, B.J.T. (2008) Modelling cell generation times by using the tempered stable distribution,\nJournal of the Royal Statistical Society: Series C (Applied Statistics), 57(4) 379-397.\n[22] Rosin\u00b4ski, J. (2001) Series representations of Le\u00b4vy processes from the perspective of point processes, In: Le\u00b4vy Processes -\nTheory and Applications, Eds. Barndorff-Nielsen, O.-E., Mikosch, T., Resnick, S.I., Birkha\u00a8user, 401-415.\n[23] Rosin\u00b4ski, J. (2007) Tempering stable processes, Stochastic Processes and their Applications, 117(6) 677-707.\n18\n[24] Sato, K. (1999) Le\u00b4vy Processes and Infinitely Divisible Distributions, Cambridge University Press.\n[25] Signahl, M. (2003) On error rates in normal approximations and simulation schemes for Le\u00b4vy processes, Stochastic Models,\n19(3) 287-298.\n[26] Todorov ,V., Tauchen, G. (2010) Limit theorems for power variations of pure-jump processes with application to activity\nestimation, to appear in Annals of Applied Probability.\n[27] Tweedie, M.C.K. (1984) An index which distinguishes between some important exponential families, In: Statistics: Appli-\ncations and New Directions: Proc. Indian Statistical Institute Golden Jubilee International Conference (eds. J. Ghosh and J.\nRoy) 579-604.\n[28] Zolotarev, V.M. (1986) One-Dimensional Stable Distributions, American Mathematical Society, Providence, RI.\n19\n"}