{"doi":"10.1016\/j.sigpro.2008.06.013","coreId":"136541","oai":"oai:bradscholars.brad.ac.uk:10454\/4105","identifiers":["oai:bradscholars.brad.ac.uk:10454\/4105","10.1016\/j.sigpro.2008.06.013"],"title":"Recognition of off-line printed Arabic text using Hidden Markov Models.","authors":["Al-Muhtaseb, Husni Abdulghani","Mahmoud, Sabri A.","Qahwaji, Rami S.R."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2008","abstract":"yesThis paper describes a technique for automatic recognition of off-line printed Arabic text using Hidden Markov Models. In this work different sizes of overlapping and non-overlapping hierarchical windows are used to generate 16 features from each vertical sliding strip. Eight different Arabic fonts were used for testing (viz. Arial, Tahoma, Akhbar, Thuluth, Naskh, Simplified Arabic, Andalus, and Traditional Arabic). It was experimentally proven that different fonts have their highest recognition rates at different numbers of states (5 or 7) and codebook sizes (128 or 256).\\ud\n\\ud\nArabic text is cursive, and each character may have up to four different shapes based on its location in a word. This research work considered each shape as a different class, resulting in a total of 126 classes (compared to 28 Arabic letters). The achieved average recognition rates were between 98.08% and 99.89% for the eight experimental fonts.\\ud\n\\ud\nThe main contributions of this work are the novel hierarchical sliding window technique using only 16 features for each sliding window, considering each shape of Arabic characters as a separate class, bypassing the need for segmenting Arabic text, and its applicability to other languages","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/136541.pdf","fullTextIdentifier":"https:\/\/bradscholars.brad.ac.uk\/bitstream\/10454\/4105\/3\/Recognition.pdf","pdfHashValue":"ae49df99ffb7fd73114ac62d9643730cc105410a","publisher":"Elsevier","rawRecordXml":"<record><header><identifier>\n        \n            \n                oai:bradscholars.brad.ac.uk:10454\/4105<\/identifier><datestamp>\n                2016-09-09T15:03:54Z<\/datestamp><setSpec>\n                com_10454_413<\/setSpec><setSpec>\n                col_10454_6345<\/setSpec>\n            <\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:doc=\"http:\/\/www.lyncode.com\/xoai\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n            \nRecognition of off-line printed Arabic text using Hidden Markov Models.<\/dc:title><dc:creator>\nAl-Muhtaseb, Husni Abdulghani<\/dc:creator><dc:creator>\nMahmoud, Sabri A.<\/dc:creator><dc:creator>\nQahwaji, Rami S.R.<\/dc:creator><dc:subject>\nSignal Processing<\/dc:subject><dc:subject>\nOCR<\/dc:subject><dc:subject>\nFeature extraction<\/dc:subject><dc:subject>\nArabic text recognition<\/dc:subject><dc:subject>\nHidden Markov Models (HMM)<\/dc:subject><dc:subject>\nOmni font recognition<\/dc:subject><dc:description>\nyes<\/dc:description><dc:description>\nThis paper describes a technique for automatic recognition of off-line printed Arabic text using Hidden Markov Models. In this work different sizes of overlapping and non-overlapping hierarchical windows are used to generate 16 features from each vertical sliding strip. Eight different Arabic fonts were used for testing (viz. Arial, Tahoma, Akhbar, Thuluth, Naskh, Simplified Arabic, Andalus, and Traditional Arabic). It was experimentally proven that different fonts have their highest recognition rates at different numbers of states (5 or 7) and codebook sizes (128 or 256).\\ud\n\\ud\nArabic text is cursive, and each character may have up to four different shapes based on its location in a word. This research work considered each shape as a different class, resulting in a total of 126 classes (compared to 28 Arabic letters). The achieved average recognition rates were between 98.08% and 99.89% for the eight experimental fonts.\\ud\n\\ud\nThe main contributions of this work are the novel hierarchical sliding window technique using only 16 features for each sliding window, considering each shape of Arabic characters as a separate class, bypassing the need for segmenting Arabic text, and its applicability to other languages.<\/dc:description><dc:date>\n2009-12-15T16:02:15Z<\/dc:date><dc:date>\n2009-12-15T16:02:15Z<\/dc:date><dc:date>\n27\/06\/2008<\/dc:date><dc:date>\n2008<\/dc:date><dc:type>\nArticle<\/dc:type><dc:type>\nAccepted Manuscript<\/dc:type><dc:identifier>\nAl-Muhtaseb, H. A., Mahmoud, S. A. and Qahwaji, R. S. R. (2008). Recognition of off-line printed Arabic text using Hidden Markov Models. Signal Processing, Vol. 88, No. 12, pp. 2902-2912.<\/dc:identifier><dc:identifier>\n900013770<\/dc:identifier><dc:identifier>\nhttp:\/\/hdl.handle.net\/10454\/4105<\/dc:identifier><dc:language>\nen<\/dc:language><dc:relation>\nhttp:\/\/dx.doi.org\/doi:10.1016\/j.sigpro.2008.06.013<\/dc:relation><dc:rights>\n\u00a9 2008 Elsevier. Reproduced in accordance with the publisher's self-archiving policy.<\/dc:rights><dc:publisher>\nElsevier<\/dc:publisher>\n<\/oai_dc:dc>\n<\/metadata>\n        <\/record>","journals":null,"language":null,"relations":["http:\/\/dx.doi.org\/doi:10.1016\/j.sigpro.2008.06.013"],"year":2008,"topics":["Signal Processing","OCR","Feature extraction","Arabic text recognition","Hidden Markov Models (HMM)","Omni font recognition"],"subject":["Article","Accepted Manuscript"],"fullText":"  \nThe University of Bradford Institutional \nRepository \nhttp:\/\/bradscholars.brad.ac.uk \nThis work is made available online in accordance with publisher policies. Please \nrefer to the repository record for this item and our Policy Document available from \nthe repository home page for further information. \nTo see the final version of this work please visit the publisher\u2019s website. Where \navailable access to the published online version may require a subscription. \nAuthor(s): Al-Muhtaseb, Husni A., Mahmoud, Sabri A. and Qahwaji, Rami S. R. \nTitle: Recognition of off-line printed Arabic text using Hidden Markov \nModels. \nPublication year: 2008. \nJournal: Signal Processing. \nPublisher:  Elsevier. \nLink to original published version:  \nhttp:\/\/dx.doi.org\/doi:10.1016\/j.sigpro.2008.06.013 \nCitation: Al-Muhtaseb, H. A., Mahmoud, S. A. and Qahwaji, R. S. R. (2008). \nRecognition of off-line printed Arabic text using Hidden Markov Models. Signal \nProcessing, Vol. 88, No. 12, pp. 2902-2912. \nCopyright statement: \u00a9 2008 Elsevier. Reproduced in accordance with the \npublisher's self-archiving policy. \nRecognition of Off-line Printed Arabic Text  \nUsing Hidden Markov Models \nHusni A. Al-Muhtaseb, Sabri A. Mahmoud,  \nInformation and Computer Science Department, King Fahd University of Petroleum & Minerals, Dhahran 31261, \nSaudi Arabia.  e-mail: muhtaseb@kfupm.edu.sa, smasaad@kfupm.edu.sa. \nand  \nRami S. Qahwaji \nElectronic Imaging and media communications department, University of Bradford, Bradford, UK  \ne-mail: R.S.R.Qahwaji@brad.ac.uk \nAbstract \nThis paper describes a technique for automatic recognition of off-line printed Arabic text \nusing Hidden Markov Models. In this work different sizes of overlapping and non-overlapping \nhierarchical windows are used to generate 16 features from each vertical sliding strip. Eight \ndifferent Arabic fonts were used for testing (viz. Arial, Tahoma, Akhbar, Thuluth, Naskh, \nSimplified Arabic, Andalus, and Traditional Arabic). It was experimentally proven that different \nfonts have their highest recognition rates at different numbers of states (5 or 7) and codebook \nsizes (128 or 256).  \nArabic text is cursive, and each character may have up to 4 different shapes based on its \nlocation in a word. This research work considered each shape as a different class resulting in a \ntotal of 126 classes (compared to 28 Arabic letters). The achieved average recognition rates were \nbetween 98.08% and 99.89% for the eight experimental fonts.  \nThe main contributions of this work are the novel hierarchical sliding window technique \nusing only 16 features for each sliding window, considering each shape of Arabic characters as a \nseparate class, bypassing the need for segmenting Arabic text, and its applicability to other \nlanguages. \n   \nKey words: Arabic text recognition, Hidden Markov Models, Feature extraction, Omni font recognition. \n2 \n \n  \n1. Introduction \nOptical text (cursive) recognition, including handwritten text, is receiving renewed extensive \nresearch after the success in optical character recognition. Arabic text recognition, which was not \nresearched as thoroughly as Latin, Japanese, or Chinese, is receiving a renewed interest not only \nfrom Arabic-speaking researchers but also from non-Arabic-speaking researchers. Samples of \nthis research work are given in the references [1-10]. This has resulted in the improvement of the \nstate of the art in Arabic text recognition in recent years. Higher recognition rates were reported \nand more practical data is being used for testing new techniques. In addition to the traditional \napplications like check verification in banks, office automation, and postal address reading, there \nis a large interest in searching scanned documents that are available on the internet and for \nsearching handwritten manuscripts. \n Reference may be made to [11-14] for surveys on Arabic Optical Text Recognition. Lorigo \nand Govindaraju addressed off-line Arabic handwriting recognition in [15]. Trenkle et al \naddressed advances in Arabic text recognition in [16] and Srihari and Ball presented an \ninteresting and useful assessment of Arabic handwriting recognition technology [17]. They \ndiscussed the state of the art in off-line Arabic handwriting recognition, specified the most \nneeded data, and discussed the technology gaps in Arabic handwriting recognition. \nDue to the advantages of Hidden Markov Models (HMM) many researchers have used them \nfor Arabic text recognition [18-32]. HMM offer several advantages. To name a few, there is no \nneed for segmenting the Arabic cursive text, they are resistant to noise, they can tolerate \nvariations in writing, and the HMM tools are freely available. Some researchers used HMM for \nhandwriting word recognition [18, 19, 27, 28, 31]. Other researchers used it for text recognition \n[22, 23, 30, 32]. HMM was used for off-line Arabic handwritten digit recognition [33, 34] and \nfor character recognition in [30, 35]. The techniques used in [33-34] are based on extracting \ndifferent types of features of each digit as a whole, not on the sliding window principles used by \nthe majority of researchers using HMM. For their technique to be applicable to Arabic text \nrecognition, it has to be preceded by a segmentation step which is error-prone. Bazzi et al \npresented a system for bilingual text recognition (English\/Arabic) [22, 36] using the sliding \nwindow principles and extracting different types of features. Dehghani et al used it for online \n3 \n \nhandwritten Persian characters in [35] and for handwritten Farsi (Arabic) word recognition in \n[37].  \nOther researchers addressed the different stages of an Arabic text recognition system. \nExamples include: a database for Arabic handwritten text recognition in [38], a database for \nArabic handwritten checks in [39], preprocessing methods in [40], segmenting of Arabic text in \n[41], different types of features are used in [42-44], and multiple classifiers in [45, 46].  \nIt is worth mentioning that no generally accepted database for Arabic text recognition is \nfreely available for researchers. Hence different researchers of Arabic text recognition use \ndifferent data, and hence the recognition rates of the different techniques may not be comparable.  \nThis raises the need for researchers to make their data available for other researchers as a first \nstep and to work on producing a comprehensive database for Arabic text recognition. In this \nrespect we will make our data available for other interested researchers. \nIn this work, we followed the sliding window principle used by many researcher to extract \nthe features to be used with the HMM [18, 20, 21, 23, 25, 47]. In our work we employed the \nsliding window technique used with HMM in speech recognition.  Researchers, using the sliding \nwindow principle, differ in the number of features, type of features, window sizes, window \noverlapping, and HMM parameters.  \nArabic text is cursive and is written from right to left. The Arabic alphabet has 28 basic \nletters, as shown in Figure 1. An Arabic letter may have up to four basic different shapes \ndepending on the position of the letter in the word: whether it is a standalone, initial, terminal, or \nmedial form. Letters of a word may overlap vertically with or without touching. Different Arabic \nletters have different sizes (height and width). Letters in a word can have short vowels \n(diacritics). These diacritics are written as strokes, placed either on top of, or below, the letters. \nA different diacritic on a letter may change the meaning of a word. Each diacritic has its own \ncode as a separate letter when it is considered in a digital text. Readers of Arabic are accustomed \nto reading un-vocalized text by deducing the meaning from context. Figure 2 shows some of the \ncharacteristics of Arabic text related to character recognition. It shows a base line, overlapping \nletters, diacritics, and three shapes of the Lam character (terminal, medial, and initial). \n                                                      \n \n4 \n \nFigure 1: basic letters of Arabic. \n \n \nFigure 2. An example of an Arabic sentence indicating some characteristics of Arabic text.  \n \nIn this paper we addressed the automatic recognition of off-line printed Arabic text. We used \nArab Standardization and Metrology Organization (ASMO) character sets ASMO-449, ASMO-\n708 and ISO 8859-6 which define 36 Arabic letters. We also added four forms of Lam-Alef ( ) \nwhich is a sequence of two letters written as one set (hence resulting in 40 letters). Most of the \nletters can take four different shapes (from 1 to 4) but one letter has only one shape while others \nhave two. Hence, the total number of shapes is 126.  \nTable 1 shows the basic Arabic letters with their categories. We group them into 3 different \nclasses according to the number of shapes each letter takes. Class 1 consists of a single shape of \nthe Hamza which comes in stand-alone state (Number 1 in Table 1). Hamza does not connect \nwith any other letter. The second class (medial category) presents the letters that can come either \nstandalone or connected only from the right (Numbers 2-5, 7, 9, 15-18, and 35-39 in Table 1). \nThe third class (Class 4) consists of the letters that can be connected from either side or both \nsides, and can also appear as standalone (Numbers 6, 8, 10-14, 19-33, and 40 in Table 1).   \n5 \n \n \nno \nStand-\nalone \nTerminal Medial Initial Shapes Class \n1         1 1 \n2           2 2 \n3           2 2 \n4           2 2 \n5           2 2 \n6             4 3 \n7           2 2 \n8             4 3 \n9           2 2 \n10             4 3 \n11             4 3 \n12             4 3 \n13             4 3 \n14             4 3 \n15           2 2 \n16           2 2 \n17           2 2 \n18           2 2 \n19             4 3 \n20             4 3 \n21           4 3 \n22           4 3 \n23             4 3 \n24             4 3 \n25             4 3 \n26             4 3 \n27             4 3 \n28             4 3 \n29             4 3 \n30             4 3 \n31             4 3 \n32             4 3 \n33             4 3 \n34           2 2 \n35          2 2 \n36          2 2 \n37          2 2 \n38          2 2 \n39           2 2 \n40             4 3 \n \n \nTable 1. Shapes of Arabic alphabets \nAlthough an Arabic letter can have up to 4 different shapes, each letter is saved using only \none code. A computer built-in driver uses contextual analysis to decide the right shape to display, \ndepending on the previous and next characters. Presenting each Arabic letter with a single unique \n6 \n \ncode irrespective of its shape and position is an international standard that helps a lot in \nsearching, sorting, communications, and information retrieval. \nSeveral aspects of our technique resulted in the high recognition rates. Our technique is based \non a novel hierarchical sliding window technique which is reported for the first time in the \nliterature. We represent each sliding strip by 16 features from one type of simple features for \neach sliding window, while in [22, 36] 80 features of four types of features are used. Our \ntechnique considers each shape of an Arabic character as a separate class (not combining \nmultiple shapes in one class as is done by other researchers). The number of classes thus \nbecomes 126 compared with 40 classes if all the shapes of a character are considered as separate \nclasses. Our technique bypasses the need for segmentation of Arabic text, which is error-prone, \nand is applicable to other languages. \nThis paper is organized as follows. Section 2 addresses data preparation. Feature extraction is \naddressed in Section 3, where the details of the feature extraction phase are reported. Hidden \nMarkov Models is addressed in Section 4. Training, recognition, and experimental results are \naddressed in Section 5. Finally the conclusions are presented in Section 6.  \n2.  Data preparation \nThe data used in this work was extracted from the books of Saheh Al-Bukhari and Saheh \nMuslem [48, 49]. The text of the books represents samples of Standard Arabic.  The extracted \ndata consists of 2766 lines of text, consisting of 46062 words totaling 224109 characters \nincluding spaces. The average word length of the text is 3.93 characters. The length of the \nsmallest line is 43 characters. The longest line has 89 characters.  \nEight files with the same text were created, each with one of the eight used fonts (viz. Arial, \nTahoma, Akhbar, Thuluth, Naskh, Simplified Arabic, Andalus, and Traditional Arabic). We \nconsidered each shape as a separate class for recognition, as each shape of the same character is \ndifferent from the other shapes of the same character.  Out of the 2766 lines, the first 2500 lines \nwere used for training, and the remaining 266 were used for testing, in order to have enough \nsamples of each class for training. Table 2 shows a sample for each font. \n7 \n \nFont Name Sample \nArial \n \nTahoma  \nAkhbar  \nThuluth \n \nNaskh \n \nSimplified Arabic  \nTraditional Arabic  \nAndalus  \nTable 2. Samples of all fonts used.  \nFor each file the text was formatted to appear as a white font color in a black background. \nMoreover, each image in the \u201etif\u201f file has been side-reversed through a mirroring tool to speed up \nthe training and recognition testing processes as shown in Figure (3). The same effect can be \ndone by changing the index so that the window will slide from right to left. However, what we \nare suggesting is more efficient. The images of the text lines were all normalized to have a height \nof 80 pixels. \n \nFigure 3 An example of a side reversed line using a mirroring tool. \nFifteen more lines of text were added to assure the inclusion of a sufficient number of all \nshapes of Arabic letters. These lines consist of 5 copies of the minimal Arabic script that has \nbeen prepared by the authors for preparing databases and benchmarks for Arabic text recognition \nresearch [50].  \nWe prepared two dictionary books for each font. The first one represents the dictionary to be \nused in training and testing, where we coded each shape of a letter by its unique code. The \nsecond one includes coded Arabic characters, using English characters as the Hidden Markov \nModel Tool does not accept Arabic text as a dictionary.   \n8 \n \n3. Feature Extraction \n A technique based on the sliding window principle was implemented to extract text features. \nA window with variable width and height was used. Horizontal and vertical overlapping \nwindows were experimented with. In many experiments we tried different values for the window \nwidth and height, vertical and horizontal overlapping. Then different types of windows were \nutilized to get more features of each vertical segment and to decide on the most proper window \nsize and the number of overlapping cells vertically and horizontally. The direction of the text line \nis considered as the feature extraction axis. Figure 4 shows the sliding window technique used in \nthis research.  \n \nFigure 4 Areas used for feature extraction and sliding windows. \nStarting from the first pixel of the text line image, a vertical segment of 3 pixels width and a \ntext line of height (TLH) is taken. A window of 3 pixels width and TLH\/8 height was used to \nestimate the number of black pixels in the windows of the first level of the hierarchical structure. \nEight vertically non-overlapping windows are used to estimate the first 8 features (features 1 to \n \nFigure 7a. The main eight areas used for feature extraction visualized on an image line \n \n \nFigure 7 Sliding window. \n \n \n9 \n \n8). Four additional features (features 9 to 12) are estimated from four vertically non-overlapping \nwindows of 3 pixels width and TLH\/4 height (windows of the second level of the hierarchical \nstructure). Then an overlapping window with 3 pixels width and TLH\/2 height (windows of the \nthird level of the hierarchical structure) with an overlapping of TLH\/4 is used to calculate three \nfeatures (features 13 to 15). The last feature (feature 16) is found by estimating the number of \nblack pixels in the whole vertical segment (the window of the fourth level of the hierarchical \nstructure). Hence, 16 features were extracted for each horizontal slide. To calculate the following \nfeatures, the vertical window is moved horizontally, keeping an overlap of one pixel. Sixteen \nfeatures were extracted from each vertical strip and served as a feature vector in the training \nand\/or testing processes. It has to be noted that the window size and vertical and horizontal \noverlapping are made settable, and hence different features may be extracted using different \nwindow sizes and vertical and horizontal overlapping. The advantages of our technique are: \nextracting a small number of one type of features; implementing different sizes of windows; \nusing a hierarchical structure of windows for the same vertical strip; bypassing the need for \nsegmentation of Arabic text; and applicability to other languages. These sixteen features have \nbeen chosen after extensive experimental testing. Table 3 illustrates features and windows used \nin the feature extraction phase.   \nFeatures \nF16 \nFeatures \nF15 \nFeatures \nF3 to F4 \nFeatures \nF9 to F12 \nFeatures \nF1 to F8 \nF16 =  \nF13 + F14 \n F14 =  \nF11 + F12 \nF12 =  \nF7 + F8 \nF8 (sum of black pixels in \n8\nth\n vertical rectangle) \nF7 (sum of black pixels in \n7\nth\n vertical rectangle) \nF15 =  \nF10 + F11 \nF11 =  \nF5 + F6 \nF6 (sum of black pixels in \n6\nth\n vertical rectangle) \nF5 (sum of black pixels in \n5\nth\n vertical rectangle) \nF13 =  \nF9 + F10 \nF10 =  \nF3 + F4 \nF4 (sum of black pixels in \n4\nth\n vertical rectangle) \nF3 (sum of black pixels in \n3\nrd\n vertical rectangle) \n F9 =  \nF1 + F2 \nF2 (sum of black pixels in \n2\nnd\n vertical rectangle) \nF1 (sum of black pixels in \n1\nst\n vertical rectangle) \nTable 3 illustrates features and windows used in the feature extraction phase.   \n10 \n \n4. Hidden Markov Model (HMM) \nSeveral research papers have been published using HMM for text recognition [18, 22, 30, 32, \n51-53]. In order to use HMM several researchers computed the feature vectors as a function of \nan independent variable. This simulates the use of HMM in speech recognition where sliding \nframes\/windows are used. The same technique is utilized in off-line text recognition where the \nindependent variable is in the direction of the line length [22, 36]. In this paper we extract the \nfeatures of an Arabic text by using the sliding windows principle to calculate the features based \non sliding vertical strip which covers parts of the character. However, our technique differs from \nthe general trend by implementing a hierarchical window structure with different window sizes \nand horizontal and vertical overlapping. In addition, we extract only 16 simple features (of one \ntype) per vertical strip compared to 80 features (four types of features) used in [22, 36]. As was \ndone in [22, 36] we bypass the need for segmenting Arabic text, and our technique is applicable \nto other languages. We use the same HMM classifier without modification as implemented in \nHTK [54]. However, we implement our own parameters of the HMM. We allowed transition to \nthe current, the next, and the following states only. This structure allows nonlinear variations in \nthe horizontal position. HTK models the feature vector with mixture of Gaussians. It uses the \nViterbi algorithm in the recognition phase which searches for the most likely sequence of a \ncharacter given the input feature vector. \nIn this paper a left-to-right HMM for our Arabic handwritten text recognition is implemented. \nFigure 5 displays the case of a 7-state HMM, showing that we allowed transition to the current, \nthe next, and the following states only. This is in line with several research studies using HMM \n[22, 36]. This model allows relatively large variations in the horizontal position of the Arabic \ntext. The sequence of state transition in the training and testing of the model is related to each \ntext segment feature observations. In this work we have experimented with using different \nnumbers of states and dictionary sizes, and selected the best performing ones. Although each \ncharacter model could have a different number of states, we decided to adopt the same number of \nstates for all characters in a font. However, the number of states and dictionary sizes for each \nfont, in relation to the best recognition rates for each font, are different for each font. \nIn this work, each Arabic text segment is represented by a 16-dimensional feature vector as \ndescribed earlier.   \n11 \n \nFigure 5 Seven-state Hidden Markov Model (HMM) \n5. Training and Recognition \nIn order to have enough samples of each class for each font, in the training phase, 2500 lines \nwere used for training and the remaining 266 lines in testing. There is no overlap between \ntraining and testing samples. A file that contains the feature vectors of each line was prepared. \nThe feature vector contains the sixteen features extracted for each vertical strip of the image of \nthe text line by the method described previously. All feature vectors of the vertical strips of the \nline are concatenated to give the feature vectors of the text line. The group of the feature files of \nthe first 2500 lines represents the observation list for training. The group of the remaining 266 \nfeature files represents the observation list for testing. \n5.1 Training \nA large number of trials were conducted to find the most suitable combinations of the number \nof suitable states and codebook sizes. Different combinations of the number of states and size of \ncodebook were tested. The states that were experimented with range from 3 to 15. The sizes of \ncodebook that were experimented with are 32, 64, 128, 192, 256, 320, 384, and 512. Table 4 \nshows the best combinations we experimentally found to give the best recognition rates for each \nfont. \nFont Name Number of Sates Codebook size \nArial 5 256 \nTahoma 7 128 \nAkhbar 5 256 \nThuluth 7 128 \nNaskh 7 128 \nSimplified Arabic 7 128 \nTraditional Arabic 7 256 \nAndalus 7 256 \nTable 4 combinations of number of states and size of codebook used for different fonts. \n12 \n \n5.2 Classification \nThe results of testing 266 lines are summarized in Table 5. The table also shows the effect of \nhaving a unique code for each shape of each character in the classification phase (Columns 2 & \n3) and then combining the shapes of the same character into one code (Columns 4 & 5). In all \ncases there are improvements in both correctness and accuracy in combining the different shapes \nof the character after recognition into one code. The following two equations were used to \ncalculate correctness and accuracy. \n \n \n \nTable 6 summarizes the classification results for the Arial font. The results of all other fonts \nare summarized in Table 7. As the resultant confusion matrices are too large to display in row \nformat (at least 126 rows X 126 columns are needed), we summarize the confusion matrix in a \nmore informative way by collapsing all different shapes of the same character into one entry and \nby listing error details for each character. This will actually be the result after converting the \nrecognized text from the unique coding of each shape to the unique coding of each character \n(which is done by the contextual analysis module, a tool we built for this purpose). \nThe following subsections discuss the classification results for the Arial font and a summary \nof the results for all other fonts are summarized in Table 7 which shows the average correctness \nand accuracy for all fonts (viz. Arial, Tahoma, Akhbar, Thuluth, Naskh, Simplified Arabic, \nAndalus, and Traditional Arabic). \nTable 5 Summary of Results per font type with and without shape expansion \n With Expanded shapes With Collapsed shapes \nText font \n% of \nCorrectness \n% of \nAccuracy \n% of \nCorrectness \n% of \nAccuracy \nArial 99.89 99.85 99.94 99.90 \nTahoma 99.80 99.57 99.92 99.68 \nAkhbar 99.33 99.25 99.43 99.34 \n13 \n \nThulth 98.08 98.02 98.85 98.78 \nNaskh 98.12 98.02 98.19 98.09 \nSimplified Arabic 99.69 99.55 99.84 99.70 \nTraditional Arabic 98.85 98.81 98.87 98.83 \nAndalus 98.92 96.83 99.99 97.86 \n \na. Arial font Classification \nTable 6 shows the classifications results for the Arial font. The correctness percentage was \n99.94 and the accuracy percentage was 99.90. Only four letters out of 43 had some errors. The \nletter   has been substituted by the letter   four times out of 234 instances. The only difference \nbetween the two characters is the dot in the body of the letter  . The second error consists of two \nreplacements of the letter   by the letter   out of 665 instances. The third error was substituting \nthe ligature   by a blank four times out of 40. The fourth error was substituting the ligature \u0644\u0644\u0647 \nonce by   out of 491 times. Other than the substitutions, 10 insertions were added (two of them \nwere blanks). The blank problems were reported by several researchers including [22]. \nTable 6 Classification Results for Arial Font. \nLet Samples Correct  Errors \n% \nRecognition \n% \nError  \nDel Ins Correctness Accuracy Error Details \nSil 532 532 0 100.0 0.0 0 0 100.0 100.0   \n\u0621 83 83 0 100.0 0.0 0 0 100.0 100.0   \n\u0622 10 8 2 80.0 20.0 0 0 80.0 80.0   \n\u0623 484 484 0 100.0 0.0 0 0 100.0 100.0   \n\u0624 14 14 0 100.0 0.0 0 0 100.0 100.0   \n\u0625 157 157 0 100.0 0.0 0 0 100.0 100.0   \n\u0626 43 43 0 100.0 0.0 0 0 100.0 100.0   \n\u0627 2114 2114 0 100.0 0.0 0 1 100.0 100.0   \n\u0628 409 409 0 100.0 0.0 0 0 100.0 100.0   \n\u0629 234 234 0 100.0 0.0 0 0 100.0 100.0   \n\u062a 420 420 0 100.0 0.0 0 0 100.0 100.0   \n\u062b 124 124 0 100.0 0.0 0 0 100.0 100.0   \n\u062c 170 170 0 100.0 0.0 0 0 100.0 100.0   \n\u062d 234 230 4 98.3 1.7 0 0 98.3 98.3 4    \n\u062e 113 113 0 100.0 0.0 0 0 100.0 100.0   \n\u062f 344 344 0 100.0 0.0 0 1 100.0 99.7   \n\u0630 97 97 0 100.0 0.0 0 0 100.0 100.0   \n\u0631 702 702 0 100.0 0.0 0 0 100.0 100.0   \n\u0632 46 46 0 100.0 0.0 0 0 100.0 100.0   \n\u0633 640 640 0 100.0 0.0 0 0 100.0 100.0   \n\u0634 119 119 0 100.0 0.0 0 0 100.0 100.0   \n\u0635 415 415 0 100.0 0.0 0 0 100.0 100.0   \n14 \n \nLet Samples Correct  Errors \n% \nRecognition \n% \nError  \nDel Ins Correctness Accuracy Error Details \n\u0636 93 93 0 100.0 0.0 0 0 100.0 100.0   \n\u0637 68 68 0 100.0 0.0 0 0 100.0 100.0   \n\u0638 15 15 0 100.0 0.0 0 0 100.0 100.0   \n\u0639 818 818 0 100.0 0.0 0 0 100.0 100.0   \n\u063a 44 44 0 100.0 0.0 0 0 100.0 100.0   \n\u0641 495 495 0 100.0 0.0 0 0 100.0 100.0   \n\u0642 467 467 0 100.0 0.0 0 0 100.0 100.0   \n\u0643 288 288 0 100.0 0.0 0 0 100.0 100.0   \n\u0644 2136 2136 0 100.0 0.0 0 2 100.0 99.9   \n\u0645 1005 1005 0 100.0 0.0 0 0 100.0 100.0   \n\u0646 1023 1023 0 100.0 0.0 0 0 100.0 100.0   \n\u0647 665 663 2 99.7 0.3 0 0 99.7 99.7 2    \n\u0648 937 937 0 100.0 0.0 0 0 100.0 100.0   \n\u0644\u0622 5 5 0 100.0 0.0 0 0 100.0 100.0   \n\u0644\u0623 40 36 4 90.0 10.0 0 0 90.0 90.0 4 Blnk  \n\u0644\u0625 14 14 0 100.0 0.0 0 0 100.0 100.0   \n\u0644\u0627 207 207 0 100.0 0.0 0 4 100.0 98.1   \n\u0649 413 413 0 100.0 0.0 0 0 100.0 100.0   \n\u064a 1159 1159 0 100.0 0.0 0 0 100.0 100.0   \nBlnk 4637 4637 0 100.0 0.0 0 2 100.0 100.0   \n\u0644\u0644\u0647 491 490 1     0 0 99.8 99.8 1    \nIns   10          \n1   1   2   4   \n2 Blnk  \nTotal 22524 22511 13 99.94 0.06 0 10 99.94 99.90  \nb. Classification of other fonts \nTable 7 summarizes the results of Arial, Tahoma, Akhbar, Thuluth, Naskh, Simplified \nArabic, Andalus, and Traditional Arabic fonts. Arial font was included for comparison purposes. \nThe table shows the average correctness and accuracy of all these fonts. \n15 \n \n \n Arial Tahoma Akhbar Thuluth Naskh \nSimplified \nArabic \nTraditional \nArabic \nAndalus \nLe\nt \nCorrectn\ness \nAccura\ncy \nCorrectn\ness \nAccura\ncy \nCorrectn\ness \nAccura\ncy \nCorrectn\ness \nAccura\ncy \nCorrectn\ness \nAccura\ncy \nCorrectn\ness \nAccura\ncy \nCorrectn\ness \nAccura\ncy \nCorrectne\nss \nAccura\ncy \nS 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n  100 100 100 98.8 96.3 96.3 100 97.4 100 100 100 100 100 100 100 100 \n  80 80 100 100 60 60 100 100 100 100 40 40 100 100 100 100 \n  100 100 100 100 99.6 99.6 99.8 99.8 100 100 96.3 96.3 98.5 98.5 100 100 \n  100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n  100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n  100 100 100 100 100 100 100 100 97.7 97.7 100 100 100 100 100 100 \n  100 100 100 99.9 99.1 99 100 100 98.5 98.4 100 99.8 98.7 98.5 100 100 \n  100 100 100 100 99.3 99.3 93.9 93.9 89.3 88.4 100 100 92.5 92.5 100 99.8 \n  100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n  100 100 99.8 99.8 100 100 97.8 97.6 98.8 98.8 99.8 99.8 99.1 99.1 100 100 \n  100 100 100 100 98.4 92.6 100 100 95.1 95.1 100 100 99.2 99.2 100 100 \n  100 100 99.4 99.4 100 100 97.1 97.1 96.5 96.5 100 100 91.8 91.8 100 100 \n  98.3 98.3 94.4 94.4 100 100 82 82 87.6 87.6 100 100 84.2 84.2 100 100 \n  100 100 100 100 100 100 98.2 98.2 90.3 89.4 100 100 98.2 97.4 100 100 \n  100 99.7 100 99.1 100 99.7 99.1 98.8 100 100 100 99.7 99.7 99.4 100 99.7 \n  100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n  100 100 100 100 100 100 94.9 94.9 98.6 98.6 100 100 99.9 99.9 100 100 \n  100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n  100 100 100 100 99.8 98.9 99.8 99.8 99.2 99.2 100 100 99.7 99.7 100 100 \n  100 100 100 100 99.2 99.2 99.2 99.2 100 100 100 100 100 100 100 100 \n  100 100 100 100 100 100 99.8 99.8 98.8 98.8 100 100 98.8 98.8 100 100 \n  100 100 100 100 100 100 98.9 98.9 100 100 100 100 100 100 100 100 \n  100 100 98.5 97.1 100 100 97.1 97.1 97.1 97.1 100 100 100 100 100 100 \n  100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n  100 100 100 100 99.8 99.8 99.5 99.5 98.7 98.7 100 100 99.9 99.9 100 100 \n  100 100 100 100 97.7 97.7 97.7 97.7 100 100 100 100 100 100 100 100 \n  100 100 100 100 99.6 99.6 100 99.2 99.6 99.6 100 100 100 100 100 100 \n  100 100 100 100 98.9 98.9 100 100 99.1 99.1 99.8 99.8 100 100 100 100 \n  100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n  100 99.9 100 97.9 98.1 97.9 99.8 99.7 98.2 98.1 100 99.7 99.6 99.6 100 77.7 \n  100 100 100 100 99.9 99.9 98.2 98.1 90 89 100 100 94.1 94 100 100 \n  100 100 100 100 99.4 99.3 99.1 99.1 96.7 96.5 99.9 99.9 98.1 98 100 100 \n  99.7 99.7 100 100 99.7 99.7 99.1 99.1 99.7 99.7 100 100 99.1 99.1 100 100 \n  100 100 100 100 100 100 99.8 99.8 99.9 99.9 100 100 100 100 100 100 \n  100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n  90 90 100 100 100 100 100 100 97.5 97.5 100 100 100 100 100 100 \n  100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 \n  100 98.1 100 100 100 100 100 100 99 99 100 100 100 100 100 100 \n  100 100 100 100 100 100 89.1 89.1 99.5 99.5 98.6 98.6 98.6 98.6 100 100 \n  100 100 100 100 98.4 98.4 99.7 99.7 97.7 97.7 100 100 98.4 98.4 100 100 \nB 100 100 100 100 100 100 99.3 99.2 99.4 99.4 100 99.6 99.9 99.9 99.9 99.9 \n\u0644\u0644\u0647 99.8 99.8 99.8 99.8 99.8 99.8 99.8 99.8 99.8 99.8 99.8 99.8 99.8 99.8 100 100 \nT 99.9 99.9 99.9 99.7 99.4 99.3 98.9 98.8 98.2 98.1 99.8 99.7 98.9 98.8 100 97.9 \nTable 7 summarizes the results of Arial, Tahoma, Akhbar, Thuluth, Naskh, Simplified Arabic, \nAndalus, and Traditional Arabic fonts. \n16 \n \n \nc. Do the suggested features work for English? \nAlthough features presented in this work were designed for Arabic script, the question of \nwhether the same features would work for English arises. To validate this matter, an English text \nof 1230 lines was prepared using Microsoft Sans Serifs font as an example. The text file is \nconverted into an array of image lines. The same features used for Arabic text images were \nextracted for the English text. The first 1100 lines of the English text images were used for \ntraining, and the remaining 130 lines were used for testing. The classification results were \n98.92% for the correctness and 98.90% for the accuracy. Out of 4921 characters there were two \ndeletions, 51 substitutions, and one insertion. This shows the applicability of our features for \nEnglish text as well. All these errors are summarized in Table 8. The table contains the letters \nthat had classification errors. All other letters were correctly classified. It is to be noted that we \napplied the same model for Arabic text recognition without change or enhancement for a proof \nof concept. \n m n o Blank Capital i Del \nSmall L 0 0 0 0 0 2 \nn 16  0 0 0 0 \nr 2 3 0 0 0 0 \nt 0 0 0 13 0 0 \n\/ 0 0 0 0 1 0 \nc 0 0 16 0 0 0 \nIns 0 0 0 1 0  \nTable 8. All errors appeared in the English test. \n6. Conclusions \nThis paper presents a technique for automatic recognition of off-line Arabic text recognition \nbased on estimating simple and effective features that are suitable for use with the HMM (which \nis normally employed for speech recognition). We analyzed the performance of the HMM with \ndifferent numbers of features, different sizes of sliding windows, different numbers of states and \ndifferent dictionary sizes. We applied the technique for eight Arabic fonts (viz. Arial, Tahoma, \nAkhbar, Thuluth, Naskh, Simplified Arabic, Andalus, and Traditional Arabic). After a large \n17 \n \nnumber of experiments, we selected the number of features, the number of states and the \ndictionary size for each font according to each font's highest recognition rate. The technique is \nscale- and translation-invariant. The experimental results indicate the effectiveness of the \nproposed technique in the automatic recognition of off-line Arabic text with different types of \nfonts. \nA database of 2766 lines was used in the training and testing phase. 2500 lines were used in \ntraining and the remaining 266 in testing. The experimental results, discussed earlier, show the \neffectives of our features. We used a small number of simple and effective features that can be \ncomputed quickly. This was repeated for all vertical strips with an overlap of one pixel. Only \nsixteen features were extracted from each vertical strip of the text line image. We applied our \ntechnique to eight different Arabic fonts. They all gave acceptable recognition rates (accuracy \npercentages were: Arial 99.9, Tahoma 99.68, Akhbar 99.34, Thuluth 98.78, Naskh 98.09, \nSimplified Arabic 99.7, Traditional Arabic 98.83, Andalus 97.86).  \nSeveral aspects of our technique resulted in the high recognition rates. Our technique is based \non a novel hierarchical sliding window technique with overlapping and nonoverlapping windows \nwhich is reported for the first time in the literature. We represent each sliding strip by 16 features \nfrom one type of simple features for each sliding window, while other researchers used 80 \nfeatures of four types of features (viz. intensity, vertical and horizontal derivative, and local \nscope and correlation) [22,36]. To the knowledge of the researchers, no other researchers have \nincluded the following letters\/ligatures in their classifications: ( ,  ,  ,  ,  ,  , and \u0644\u0644\u0647). We \nconsidered each shape of an Arabic character as a separate class, not combining multiple shapes \nin one class as is done by other researchers. The number of classes became 126 compared with \n40 classes if all the shapes of a character are considered as separate classes. This technique does \nnot require segmentation of Arabic cursive text which is known to be problematic where an error \nin segmentation results in more errors in recognitions. Hence, using this technique, segmentation \nwas a by-product of our technique. Finally, the presented technique is language independent. \n \nThe researchers are currently exploring the use of more elaborate data and testing the system \non Omni font. In addition, they are exploring post-processing techniques to enhance the \n18 \n \nrecognition rates further as they feel that the extracted features and the classifier have done an \nexcellent job in the classification phase.  \nAcknowledgment \nWe would like to thank the referees for their constructive criticism and stimulating remarks. The \nmodification of the original manuscript to address those remarks improved the revised manuscript \nconsiderably. The first and second authors would like to thank King Fahd University of Petroleum and \nMinerals for supporting this research work. This work is partially supported by KFUPM internal project \nnumber IN060337.  \nReferences \n[1] M.Arivazhagan, H.Srinivasan, S.Srihari, A statistical approach to line Segmentation in \nhandwritten documents, in: Proceedings of SPIE, 2007. \n[2]  R. Davidson and R. Hopely, \u201cArabic and Persian OCR Training and Test Data Sets,\u201d \nProc. Symp. Document Image Understanding Technology, pp. 303-307, 1997. \n[3]  J.Femiani, M.Phielipp, A.Razdan, A System for Discriminating Handwriting from \nMachine Print on Noisy Arabic Datasets, in: SDIUT 05: Proceedings of the 2005 \nSymposium on Document Image Understanding Technology, CollegePark, \nMaryland,2005. \n[4]  A. Gillies, E. Erlandson, J. Trenkle and S. Schlosser, \u201cArabic Text Recognition System,\u201d \nProc. Symp. Document Image Understanding Technology, 1999. \n[5] J. Jin, H. Wang, X. Ding and L. Peng, \u201cPrinted Arabic Document Recognition System,\u201d \nProc. SPIE-IS&T Electronic Imaging, vol. 5676, pp. 48-55, 2005. \n[6]  G. Kim, V. Govindaraju and S. Srihari, \u201cArchitecture for Handwritten Text Recognition \nSystems,\u201d Advances in Handwriting Recognition, Series in Machine Perception and \nArtificial Intelligence, pp. 163-172, 1999. \n[7]  L.Lorigo, V.Govindaraju, Segmentation and pre-recognition of Arabic Hand writing, in: \nICDAR 05: Proceedings of the Ninth International Conference on Document Analysis \nand Recognition, vol.2, IEEE Computer Society, Seoul, Korea, 2005, pp. 605-609. \n[8] T.Sari, L.Souici, M.Sellami, O-line Handwritten Arabic Character Segmentation \nAlgorithm: ACSA, in:Proceedings of the Eighth International Workshop on Frontiers in \nHandwriting Recognition,2002. \n[9] L. Souici-Meslati and M. Sellami, \u201cA Hybrid Approach for Arabic Literal Amounts \nRecognition,\u201d The Arabian J. Science and Eng., vol. 29, pp. 177-194, 2004. \n19 \n \n[10] S.Srihari, G.Ball, H.Srinivasan, Versatile Search of Scanned Arabic Handwriting, in: \nSACH06: Summit on Arabic and Chinese Handwriting, 2006. \n[11] B. Al-Badr,  S. Mahmoud, \u201cSurvey and Bibliography of Arabic Optical Text \nRecognition,\u201d J. of Signal Processing, Vol. 41, No.1, pp.49-77 (Jan. 1995). \n[12] M. Khorsheed, \u201cOff-line Arabic Character Recognition \u2013 A Review,\u201d Pattern Analysiss \n& Applications, 5:31-45, 2002.  \n[13] A. Eldin and A. Nouh, \u201cArabic Character Recognition: A Survey,\u201d Proc. SPIE Conf. \nOptical Pattern Recognition, pp. 331-340, 1998. \n[14]  N. Amara, F.Bouslama, Classication of Arabic script using multiple Sources of \ninformation: State of the art and perspectives, International Journal On Document \nAnalysis and Recognition 5(4)(2005),195-212. \n[15] L. Lorigo, V. Govindaraju, \u201cOffline Arabic Handwriting Recognition: A Survey\u201d, EEE \nTrans. Pattern Analysis and Machine Intelligence, vol. 28, no. 5, pp. 712-724,May 2006. \n[16] J. Trenkle, A. Gillies, E. Erlandson, S. Schlosser and S. Cavin, \u201cAdvances in Arabic Text \nRecognition,\u201d Proc. Symp. Document Image Understanding Technology, 2001. \n[17]  S. Srihari and G. Ball, An Assessment of Arabic Handwriting Recognition Technology, \nTR-03-07 report, University at Buffalo, The State University of New York, 2007. \n[18] S. Almaadeed, C. Higgens and D. Elliman, \u201cRecognition of Off-Line Handwritten Arabic \nWords Using Hidden Markov Model Approach,\u201d Proc. 16th Int\"l Conf. Pattern \nRecognition, vol. 3, pp. 481-484, 2002. \n[19]   S.Almaadeed, C.Higgens, D.Elliman, On-Line Recognition of Handwritten Arabic \nWords Using Multiple Hidden Markov Models, Knowledge-Based Systems 17(2004), \n75-79, 2004. \n[20] S. Al-Qahtani and M. Khorsheed, \u201cAn Omni-Font HTK-Based Arabic Recognition \nSystem,\u201d Proc. Eighth IASTED Int\"l Conf. Artificial Intelligence and Soft Computing, \n2004. \n[21]  S. Al-Qahtani and M. Khorsheed, \u201cA HTK-Based System to Recognise Arabic Script,\u201d \nProc. Fourth IASTED Int\"l Conf. Visualization, Imaging, and Image Processing, 2004. \n[22]  I. Bazzi, R. Schwartz and J. Makhoul, \u201cAn Omnifont Open-Vocabulary OCR System for \nEnglish and Arabic,\u201d IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 21, pp. \n495-504, 1999. \n[23]  H. Bunke, S. Bengio and A. Vinciarelli, \u201cOff-Line Recognition of Unconstrained \nHandwritten Texts Using HMMS and Statistical Language Models,\u201d IEEE Trans. Pattern \nAnalysis and Machine Intelligence, vol. 26, pp. 709-720, 2004. \n[24]  R.El-Hajj, L.Likforman-Sulem, C.Mokbel, Arabic Hand writing Recognition Using \nBaseline Dependant Features and Hidden Markov Modeling, in: Proc. 9th Intl Conf. \nDocument Analysis and Recognition, ICDAR 05, 2005, pp. 893-897. \n[25]  M. Khorsheed, Recognising Hand written Arabic Manuscripts Using a Single Hidden \nMarkov Model, Pattern Recognition Letters 24(2003), 2235-2242. \n20 \n \n[26]  H. Miled, N. Amara, Planar Markov Modeling for Arabic Writing Recognition: \nAdvancement State, in: Proc. Intl Conf. Document Analysis and Recognition, pp. 69-73, \n2001. \n[27] M.Pechwitz, V.Margner, HMM based approach for handwritten Arabic word Recognition \nusing the IFN\/ENIT-database ,in: ICDAR03:  Proceedings of The Seventh International \nConference on Document Analysis and Recognition, IEEE Computer Society, Edinburgh, \nScotland, pp. 890-894, 2003, 2003. \n[28] R.Safabakhsh, P.Adibi, Nastaaligh Handwritten Word Recognition Using a Continuous-\nDensity Variable-Duration HMM, The Arabian J. Science and Eng. 30(2005),95-118, \n2005. \n[29] S.Touj, N. Amara, H.Amiri, Arabic Handwritten Words Recognition Based On a Planar \nHidden Markov Model., International Arab Journal of Information Technology \n2(4)(2005),318-325. \n[30] A. Hassin, X. Tang, J. Liu, and W. Zhao, Printed Arabic character recognition using \nHMM, Journal of Computer Science and Technology,  Volume 19 Issue 4, July 2004, pp. \n538-543.  \n[31]  M. Mohamed and P. Gader, \u201cHandwritten Word Recognition Using Segmentation-Free \nHidden Markov Modeling and Segmentation-Based Dynamic Programming Techniques,\u201d \nIEEE Trans. Pattern Analysis and Machine Intelligence, vol. 18, no. 5, pp. 548-554, May \n1996. \n[32]  J. Hu, S. . Lim, M. Brown, Writer independent on-line handwriting recognition using an \nHMM approach, Pattern Recognition 33 (2000) 133-147. \n[33]  S. Mahmoud, Recognition of writer-independent Off-line Handwritten Arabic (Indian) \nNumerals Using Hidden Markov Models, Accepted for publication in the Journal of \nSignal Processing. \n[34] S. Mahmoud, M. Abu-Amara, Recognition of Handwritten Arabic (Indian) Numerals \nUsing Radon Transform, submitted for publication. \n[35] A.Dehghani, F.Shabani, P.Nava, On-Line Recognition of Isolated Persian Handwritten \nCharacters Using Multiple Hidden Markov Models, in: Proc. Intl Conf. Information \nTechnology: Coding and Computing, pp. 506-510, 2001. \n[36] I. Bazzi, C. LaPre, J. Makhoul, and R. Schwartz, \u201cOmnifont and Unlimited Vocabulary \nOCR for English and Arabic,\u201d Proc. Int\u201fl Conf. Document Analysis and Recognition, vol. \n2, pp. 842-846, Ulm, Germany, 1997. \n[37]  M.Dehghan, K.Faez, M.Ahmadi, M.Shridhar, Hand written Farsi(Arabic) Word \nRecognition: A Holistic Approach Using Discrete HMM, Pattern Recognition \n34(2001),1057-1065. \n[38] S. Almaadeed, D. Elliman and C. Higgins, \u201cA Data Base for Arabic Handwritten Text \nRecognition Research,\u201d Proc. Eighth Int\"l Workshop Frontiers in Handwriting \nRecognition, pp. 485-489, 2002. \n[39]  Y. Al-Ohali, M. Cheriet and C. Suen, \u201cDatabases for Recognition of Handwritten Arabic \nCheques,\u201d Pattern Recognition, vol. 36, pp. 111-121, 2003. \n21 \n \n[40]  F.Farooq, V.Govindaraju, M. Perrone, Pre-processing methods for Handwritten Arabic \ndocuments, in: ICDAR 05: Proceedings of the Ninth International Conference on \nDocument Analysis and Recognition,vol.1, IEEE Computer Society, Seoul, Korea, . 267-\n271, 2005. \n[41]  R.Haraty, A.Hamid, Segmenting Hand written Arabic Text, in: Proc. Intl Conf. \nComputer Science, Software Eng., Information Technology, e-Business, And \nApplications, 2002. \n[42]  A.Amin, Recognition of Hand-Printed Characters Based on Structural Description and \nInductive Logic Programming, Pattern Recognition Letters 24(2003),3187-3196. \n[43]  M.Fahmy, S. Ali, Automatic Recognition of Hand written Arabic Characters Using Their \nGeometrical Features, Studies in Informatics and Control J.10. \n[44]  S.Mozaari, K.Faez, M.Ziaratban, Structural decomposition and statistical Description of \nFarsi\/Arabic hand written numeric characters, In: ICDAR05: Proceedings of the Ninth \nInternational Conference on Document Analysis and Recognition, vol.1,IEEE Computer \nSociety, Seoul, Korea, 2005. \n[45]  N.Farah, L.Souici, L.Farah, M.Sellami, Arabic Words Recognition with Classiers \nCombination: An Application to Literal Amounts, in: Proc. Artificial Intelligence: \nMethodology, Systems, and Applications, 2004. \n[46] N. Farah, A. Ennaji, T. Khadir and M. Sellami, \u201cBenefits of Multi-Classifier Systems for \nArabic Handwritten Words Recognition,\u201d Proc. Int\"l Conf. Document Analysis and \nRecognition, pp. 222-226, 2005. \n[47] M. Shahrezea, K. Faez, A. Khotanzad, Recognition of handwritten Persian\/Arabic \nnumerals by shadow coding and an edited probabilistic neural network. In: Proceedings \nof the International Conference on Image Processing, vol. 3, pp. 436\u2013439,1995. \n[48] M. Al-Bukhari, \"Al-Jame' Al-Saheeh (Sahih Al-Bukhari)\", Dar Al-Jeel, Beirut, 2005 (in \nArabic). \n[49] M. Al-Naysabouri,  \"Al-Jame' Al-Saheeh (Sahih Muslim)\", Dar Al-Jeel, Beirut, 2006 (in \nArabic). \n[50] H. Al-Muhtaseb, S. Mahmoud, R. Qahwaji, \u201cA Novel Minimal Arabic Script for \nPreparing Databases and Benchmarks for Arabic Text Recognition Research\u201d, To be \npublished, 2008. \n[51]  S. Almaadeed, C. Higgens, and D. Elliman, \u201cRecognition of Off-line Handwritten \nArabic Words using Hidden Markov Model Approach,\u201d ICPR 2002, Quebec City, \nAugust 2002, pp. 481-484. \n[52] M. Mohamed and P. Gader, \u201cHandwritten Word Recognition Using Segmentation-Free \nHidden Markov Modeling and Segmentation-Based Dynamic Programming Techniques,\u201d \nIEEE Trans. Pattern Analysis and Machine Intelligence, vol. 18, no. 5, pp. 548-554, May \n1996. \n[53] HTK Speech Recognition Toolkit, http:\/\/htk.eng.cam.ac.uk\/ \n"}