{"doi":"10.1016\/j.ress.2004.03.007","coreId":"66122","oai":"oai:dro.dur.ac.uk.OAI2:3058","identifiers":["oai:dro.dur.ac.uk.OAI2:3058","10.1016\/j.ress.2004.03.007"],"title":"Coherent lower previsions in systems modelling : products and aggregation rules.","authors":["De Cooman, G.","Troffaes, M. C. M."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2004-09","abstract":"We discuss why coherent lower previsions provide a good uncertainty model for solving generic uncertainty problems involving possibly conflicting expert information. We study various ways of combining expert assessments on different domains, such as natural extension, independent natural extension and the type-I product, as well as on common domains, such as conjunction and disjunction. We provide each of these with a clear interpretation, and we study how they are related. Observing that in combining expert assessments no information is available about the order in which they should be combined, we suggest that the final result should be independent of the order of combination. The rules of combination we study here satisfy this requirement","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/66122.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/3058\/1\/3058.pdf","pdfHashValue":"5b2358629853f9589000134599752f0f9f659c36","publisher":"Elsevier","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:3058<\/identifier><datestamp>\n      2011-08-24T12:22:23Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Coherent lower previsions in systems modelling : products and aggregation rules.<\/dc:title><dc:creator>\n        De Cooman, G.<\/dc:creator><dc:creator>\n        Troffaes, M. C. M.<\/dc:creator><dc:description>\n        We discuss why coherent lower previsions provide a good uncertainty model for solving generic uncertainty problems involving possibly conflicting expert information. We study various ways of combining expert assessments on different domains, such as natural extension, independent natural extension and the type-I product, as well as on common domains, such as conjunction and disjunction. We provide each of these with a clear interpretation, and we study how they are related. Observing that in combining expert assessments no information is available about the order in which they should be combined, we suggest that the final result should be independent of the order of combination. The rules of combination we study here satisfy this requirement. <\/dc:description><dc:subject>\n        Expert information<\/dc:subject><dc:subject>\n         Coherent lower previsions<\/dc:subject><dc:subject>\n         Natural extension<\/dc:subject><dc:subject>\n         Independence<\/dc:subject><dc:subject>\n         Type-I product<\/dc:subject><dc:subject>\n         Marginal extension<\/dc:subject><dc:subject>\n         Conjunction<\/dc:subject><dc:subject>\n         Disjunction.<\/dc:subject><dc:publisher>\n        Elsevier<\/dc:publisher><dc:source>\n        Reliability engineering & system safety, 2004, Vol.85(1-3), pp.113-134 [Peer Reviewed Journal]<\/dc:source><dc:date>\n        2004-09<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:3058<\/dc:identifier><dc:identifier>\n        issn:0951-8320<\/dc:identifier><dc:identifier>\n        doi:10.1016\/j.ress.2004.03.007<\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/3058\/<\/dc:identifier><dc:identifier>\n        http:\/\/dx.doi.org\/10.1016\/j.ress.2004.03.007<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/3058\/1\/3058.pdf<\/dc:identifier><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["issn:0951-8320","0951-8320"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2004,"topics":["Expert information","Coherent lower previsions","Natural extension","Independence","Type-I product","Marginal extension","Conjunction","Disjunction."],"subject":["Article","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n14 May 2009\nVersion of attached file:\nAccepted Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nDe Cooman, G. and Troffaes, M. C. M. (2004) \u2019Coherent lower previsions in systems modelling : products and\naggregation rules.\u2019, Reliability engineering system safety., 85 (1-3). pp. 113-134.\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1016\/j.ress.2004.03.007\nPublisher\u2019s copyright statement:\nAdditional information:\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\n \n \n \nDurham Research Online \n \nDeposited in DRO: \n14 May 2009 \n \nPeer-review status of attached file: \nPeer-reviewed \n \nPublication status of attached file: \nAccepted for publication \n \nCitation for published item: \nDe Cooman, G. and Troffaes, M. C. M. (2004) 'Coherent lower previsions in systems \nmodelling : products and aggregation rules.', Reliability engineering & system safety., 85 (1-\n3). pp. 113-134. \n \nFurther information on publisher\u2019s website: \nhttp:\/\/dx.doi.org\/10.1016\/j.ress.2004.03.007 \n \n \n \n \n \n \n \n \n \n \n \nUse policy \n \nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior \npermission or charge, for personal research or study, educational, or not-for-profit purposes provided that : \n \n\uf0a7 a full bibliographic reference is made to the original source \n\uf0a7 a link is made to the metadata record in DRO \n\uf0a7 the full-text is not changed in any way \n \nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders. \n \nPlease consult the full DRO policy for further details. \n \nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom \nTel : +44 (0)191 334 2975 | Fax : +44 (0)191 334 2971 \nhttp:\/\/dro.dur.ac.uk \nGert de Cooman and Matthias C. M. Troffaes. Coherent lower previsions in systems modelling: products and\naggregation rules. Reliability Engineering and System Safety, 85(1-3):113-134, 2004.\nCOHERENT LOWER PREVISIONS IN SYSTEMS MODELLING:\nPRODUCTS AND AGGREGATION RULES\nGERT DE COOMAN AND MATTHIAS C. M. TROFFAES\nABSTRACT. We discuss why coherent lower previsions provide a good uncertainty model for solving generic uncertainty prob-\nlems involving possibly conflicting expert information. We study various ways of combining expert assessments on different\ndomains, such as natural extension, independent natural extension and the type-I product, as well as on common domains, such as\nconjunction and disjunction. We provide each of these with a clear interpretation, and we study how they are related. Observing\nthat in combining expert assessments no information is available about the order in which they should be combined, we suggest\nthat the final result should be independent of the order of combination. The rules of combination we study here satisfy this\nrequirement.\n1. INTRODUCTION\nIn [11], two problem sets are presented, whose challenge consists in modelling diverse kinds of uncertainty about a\nsystem\u2019s parameters, sometimes from different and possibly conflicting sources, and inferring from that an uncertainty\nmodel for the system\u2019s output. In the present paper, we concentrate on the first of these problem sets, because we believe\nit is simpler and at the same time presents the same modelling challenges as the second one. It can be briefly sketched as\nfollows.\nThe system response y is a function f of two continuous parameters a and b, given by\ny = f(a, b) = (a+ b)a.\nThe parameters a and b are non-negative real numbers, and consequently, so is the system output y. The task is to give\na model for the uncertainty about y, given additional information about the values that a and b assume. This additional\ninformation is different in each of the following six problems for this set. In modelling the available information about a\nand b, it should be kept in mind that they are assumed to be epistemically independent, i.e., information about the value\nthat one parameter assumes does not influence our knowledge and beliefs about the value of the other one.\nProblem 1. a and b assume a value in the respective closed intervals:\nA = [0.1, 1.0] and B = [0.0, 1.0].\nProblem 2. a assumes a value in the closed interval A, and for b there are four independent and equally credible sources\nof information, each of them stating that b belongs to a closed interval Bj (i = 1, . . . , 4). There are three different cases.\n2a) The intervals Bj are consonant, or nested:\nA = [0.1, 1.0] and B1 = [0.6, 0.8], B2 = [0.4, 0.85], B3 = [0.2, 0.9], B4 = [0.0, 1.0].\n2b) The intervals Bj are consistent, i.e., they have a non-empty intersection:\nA = [0.1, 1.0] and B1 = [0.6, 0.9], B2 = [0.4, 0.8], B3 = [0.1, 0.7], B4 = [0.0, 1.0].\n2c) the intervals Bj are inconsistent or conflicting, i.e., they have an empty intersection:\nA = [0.1, 1.0] and B1 = [0.6, 0.8], B2 = [0.5, 0.7], B3 = [0.1, 0.4], B4 = [0.0, 1.0].\nProblem 3. For a there are three independent and equally credible sources of information, and for b there are four. There\nare three different cases, where, with obvious notations,\n3a) the intervals Ai and Bj are consonant:\nA1 = [0.5, 0.7], A2 = [0.3, 0.8], A3 = [0.1, 1.0]\nB1 = [0.6, 0.6], B2 = [0.4, 0.85], B3 = [0.2, 0.9], B4 = [0.0, 1.0];\n3b) the intervals Ai and Bj are consistent:\nA1 = [0.5, 1.0], A2 = [0.2, 0.7], A3 = [0.1, 0.6]\nB1 = [0.6, 0.6], B2 = [0.4, 0.8], B3 = [0.1, 0.7], B4 = [0.0, 1.0];\n3c) the intervals Ai and Bj are conflicting:\nA1 = [0.8, 1.0], A2 = [0.5, 0.7], A3 = [0.1, 0.4]\nB1 = [0.8, 1.0], B2 = [0.5, 0.7], B3 = [0.1, 0.4], B4 = [0.0, 0.2].\nKey words and phrases. expert information, coherent lower previsions, natural extension, independence, type-I product, marginal extension, con-\njunction, disjunction.\nEmail: gert.decooman@rug.ac.be, matthias.troffaes@rug.ac.be.\nAddress: Onderzoeksgroep SYSTeMS, Technologiepark Zwijnaarde 914, 9052 Zwijnaarde, Belgium.\n1\nProblem 4. a belongs to the closed interval A, and b is log-normally distributed, i.e., ln b \u223c N(\u00b5, \u03c3), where \u00b5 and \u03c3 are\nknown to belong to the respective closed intervals M and S, where\nA = [0.1, 1.0],M = [0.0, 1.0] and S = [0.1, 0.5].\nProblem 5. There are three independent and equally credible sources of information for a, each specifying a closed\ninterval that a belongs to. There are also three independent and equally credible sources of information for b, each stating\nthat b is log-normally distributed, i.e., ln b \u223c N(\u00b5, \u03c3), and each specifying closed intervals that \u00b5 and \u03c3 belong to. There\nare three different cases, where, with obvious notations,\n5a) the intervals Ai, Mj and Sk are consonant:\nA1 = [0.5, 0.7], A2 = [0.3, 0.8], A3 = [0.1, 1.0]\nM1 = [0.6, 0.8],M2 = [0.2, 0.9],M3 = [0.0, 1.0]\nS1 = [0.3, 0.4], S2 = [0.2, 0.45], S3 = [0.1, 0.5];\n5b) the intervals Ai, Mj and Sk are consistent:\nA1 = [0.5, 1.0], A2 = [0.2, 0.7], A3 = [0.1, 0.6]\nM1 = [0.6, 0.9],M2 = [0.1, 0.7],M3 = [0.0, 1.0]\nS1 = [0.3, 0.45], S2 = [0.15, 0.35], S3 = [0.1, 0.5];\n5c) the intervals Ai, Mj and Sk are conflicting:\nA1 = [0.8, 1.0], A2 = [0.5, 0.7], A3 = [0.1, 0.4]\nM1 = [0.6, 0.8],M2 = [0.1, 0.4],M3 = [0.0, 1.0]\nS1 = [0.4, 0.5], S2 = [0.25, 0.35], S3 = [0.1, 0.2].\nProblem 6. a belongs to the closed interval A, and b is log-normally distributed, i.e., ln b \u223c N(\u00b5, \u03c3), with known\nparameters \u00b5 and \u03c3:\nA = [0.1, 1.0], \u00b5 = 0.5 and \u03c3 = 0.5.\nBelow, we propose to use Walley\u2019s imprecise probability models, or coherent lower previsions [14], in order to repre-\nsent the available information about the parameters a and b, and to infer a model for the uncertainty about the output y.\nWe have good reasons for preferring these models to a number of their very popular alternatives, such as Bayesian prob-\nabilities and belief functions. First of all, unlike belief functions, imprecise probability models have an operationalisable\ndefinition and a definite interpretation in terms of a subject\u2019s behaviour. In this respect, they are very much like Bayesian\nmodels, and they are also required to satisfy a number of rationality requirements, such as avoiding sure loss and coher-\nence. But they allow for more generality: roughly speaking, it is not claimed that all uncertainty should be represented\nby probability measures, but rather by sets of probability measures. In our view, this makes imprecise probability models\nmore widely applicable and more realistic than their Bayesian counterparts. For an extensive discussion of these issues,\nwe refer to [14, Chapter 5].\nMoreover, we are convinced that a model cannot be considered separately from how it is to be used. In many cases,\nmodels are used to (help somebody) make decisions, such as deciding which action to take, but also deciding which\nestimate to prefer, or which inference to make. Like their Bayesian counterparts, but unlike belief functions, imprecise\nprobability models come with a full-fledged decision theory that is closely linked with their behavioural interpretation:\nroughly speaking, these models reflect a subject\u2019s behaviour in certain situations, which, through requirements of consis-\ntency or rationality, has implications for how his behaviour should be in other situations (for more information, see [14,\nSection 3.9]). The imprecise probability models that we shall derive for the value of the system output y, can be used to\nchoose between actions whose outcome depends on the actual value of y. Although we concentrate on modelling itself,\nwe nevertheless feel that this additional aspect of our models should be mentioned.\nAnd finally, imprecise probability models include Bayesian probabilities and belief functions as special cases, as they\ndo a number of other models in the literature, such as 2-monotone capacities [1], possibility measures [2, 7, 16], convex\nsets of probability measures [10], comparative and modal probabilities (see for instance [15]).\nIn deriving a model for the uncertainty about the system parameter y, there are three steps to be taken: 1) finding an\nimprecise probability model for the given information about the parameter a and about the parameter b, taken separately;\n2) combining these separate models into a joint model for the values of a and b; and 3) deriving from this joint model an\nimprecise probability model for the uncertainty about y.\nStep 1 is discussed in Section 2. Section 3 deals with Step 2, which can also be described more technically as forming\nproducts from independent marginals; and Step 3 is discussed in Section 4. We want to point out here that Problems 1,\n4 and 6 are conceptually simpler than Problems 2, 3 and 5, because the latter also involve aggregating information from\ndifferent sources or experts. The results in Sections 2\u20134 do not deal with this aggregation problem, and therefore only\nallow us to solve Problems 1, 4 and 6, which is done in Section 5. The problem of aggregation in Step 1 is dealt with in\nSection 6, and the solutions to Problems 2, 3 and 5 are presented in Section 7.\nIn solving these problems, we have had to derive a number of new results about imprecise probability models, for\nwhich we have provided detailed proofs. As these are fairly technical and not always essential for understanding the main\nargument, and as they sometimes use other results proven elsewhere, we advise the reader with a limited knowledge of\nthe theory of imprecise probabilities to simply skip them.\n2\n2. IMPRECISE PROBABILITY MODELS FOR THE VALUE OF A PARAMETER\n2.1. Importance of a common mathematical model. Each problem in the set provides expert assessments for the values\nof two parameters, a and b. It is important to note that each of the expert assessments deals with a single parameter.\nMoreover, it is either of the\n\u2022 vacuous type, such as the interval information: \u2018a belongs to A = [0.1, 0.9]\u2019,\n\u2022 Bayesian type, such as \u2018b has the log-normal distribution with parameters \u00b5 = 0.5 and \u03c3 = 0.5\u2019, or\n\u2022 Bayesian type with vacuous parameters, such as \u2018b has a log-normal distribution with parameters \u00b5 belonging to\nthe interval M = [0.0, 1.0], and \u03c3 belonging to the interval S = [0.1, 0.5]\u2019.\nAssessments of the last type are hierarchical: an assessment of the variable b is made through an assessment about\nvariables \u00b5 and \u03c3 and an assessment about the variable b conditional on the variables \u00b5 and \u03c3.\nA first step toward combining these assessments is to express them using mathematical models of the same type. This\nshould allow us to deal with all sources of information in a uniform way. We shall argue in this section that all the\ngiven expert assessments can be modelled by specific imprecise probability models, called coherent lower previsions. But\nbefore we do that, let us first explain briefly what those models are. For a more detailed discussion, and many of the\ntechnical results used in the proofs further on, we refer to Walley\u2019s important book on the subject [14].\n2.2. Coherent lower previsions: a behavioural uncertainty model.\n2.2.1. The behavioural definition. Let us consider an agent who is uncertain about something, say, the value of the\nvariable a that takes values in a setA . A gamble is a bounded mapping fromA toR, and it is interpreted as an uncertain\nreward: if some \u03b1 in A would turn out to be the true value of the variable a then the agent would receive the amount\nX(\u03b1), expressed in units of some (predetermined) linear utility. Gambles play a similar part in the theory of imprecise\nprobabilities as events do in the classical, or Bayesian, theory of probability. In fact, any event can be interpreted as a very\nsimple gamble that only allows the modeller to distinguish between two situations: the event either occurs, or it doesn\u2019t,\nand the reward depends only on whether or not it does. So, an event, modelled as a subset A of the space A of possible\nparameter values, corresponds to a gamble IA (its indicator) that yields one unit of utility if it occurs, i.e., if a \u2208 A, and\nzero units if it doesn\u2019t, i.e., if a \u2208 {A, where {A denotes the set-theoretic complement of A. In other words, there is a\nnatural correspondence between events and zero-one-valued gambles. The concept of a gamble can therefore be seen as a\ngeneralisation of the concept of an event. The set of all gambles associated with the variable a is denoted byL (A ). It is\na real linear space under the point-wise addition of gambles and the scalar point-wise multiplication of gambles with real\nnumbers.\nThe information the agent has about the value of the parameter a will lead him to accept or reject transactions whose\nreward depends on this value, and we can formulate a model for his uncertainty by looking at a specific type of transaction:\nbuying gambles. The agent\u2019s lower prevision (or supremum acceptable buying price, or lower expectation) P(X) for a\ngamble X is the greatest real number s such that he is disposed to buy the gamble X for any price strictly lower than s.\nIf the agent assesses a supremum acceptable buying price for every gamble X in some subsetK ofL (\u2126), the resulting\nmapping P : K \u2192 R is called the agent\u2019s lower prevision. P will denote the conjugate upper prevision of P . It is defined\nby P(X) = \u2212P(\u2212X) for every X \u2208 \u2212K . P(X) represents the agent\u2019s infimum acceptable price for selling the gamble\nX .\nLower and upper previsions for gambles are a natural generalisation of probabilities for events. Indeed, any assessment\nof a probability of an event can be translated into an assessment of a supremum buying price and an infimum selling price\nfor a zero-one-valued gamble. Suppose that the probability of the event A is known to be p. The reward we expect from\nIA is then equal to 0 \u00b7 (1\u2212 p) + 1 \u00b7 p = p. Therefore, we are willing to buy IA for any price less than p, and we are willing\nto sell IA for any price greater than p. We infer that P(IA) = P(IA) = p. The power of lower and upper previsions,\ncompared to classical probability theory, is that lower and upper previsions allow for far more generality. In particular,\nthe theory does not require that your supremum buying price should be equal to your infimum selling price.\nAs we have already suggested in the Introduction, a particular benefit from this way of modelling available information\n(or uncertainty) is that it leads naturally to a theory of decision making under uncertainty. For example, making a particular\ndecision d from a set D of alternatives is behaviourally equivalent to accepting a gamble Xd, which represents the\n(possibly negative) utility received as a function of the value of the (unknown) parameter a of the decision problem. The\nagent should strictly prefer one action d1 over an alternative d2 if P(Xd1 \u2212Xd2) > 0: this means that he is willing to pay\nsome strictly positive amount of utility for exchanging the rewards of making decision d2 with those of making decision\nd1. More details on decision making with imprecise probability models can be found in [14, Section 3.9]. For a discussion\nof optimal control and dynamic programming in connection with imprecise probability models, we refer to [4].\n2.2.2. Coherence. Since a lower prevision P represents an agent\u2019s commitments to act in certain ways\u2014to buy gambles\nX in its domainK up to certain prices P(X)\u2014it should satisfy a number of requirements that ensure that his behaviour\nis rational. The strongest such rationality criterion is that of coherence. It is easiest to understand and define if the domain\nK is the set of all gambles L (A ). A lower prevision P on L (A ) is called coherent if it satisfies the following three\nrequirements.\nAccepting sure gains: The agent should always be willing to buy a gamble X for a price equal to the lowest\npossible reward he may expect from X , that is, inf[X]. Hence, it should hold that\nQ(X) \u2265 inf[X] for all gambles X.\n3\nPositive homogeneity: Next, since we are working with a linear utility, buying prices should be independent of the\nchoice of the scale of the utility. Mathematically, this means that\nQ(\u03bbX) = \u03bbQ(X) for each gamble X and \u03bb > 0.\nSuperadditivity: Finally, since we are working with a linear utility, if the agent is willing to buy X for price Q(X)\nand Y for price Q(Y ), he should be willing to buy X + Y for at least Q(X) +Q(Y ), whence:\nQ(X + Y ) \u2265 Q(X) +Q(Y ) for all gambles X and Y .\nA lower prevision P on an arbitrary domain K \u2286 L (A ) is called coherent if it is the restriction of\u2014can be extended\nto\u2014some coherent lower prevision onL (A ).\n2.2.3. Avoiding sure loss. There is a weaker rationality criterion, called avoiding sure loss, that is of interest for the\ndevelopment in this paper. A lower prevision P defined on a set of gambles K \u2286 L (A ) avoids sure loss if it is point-\nwise dominated by some coherent lower prevision, i.e. if there is a coherent lower prevision Q onL (A ) such that for all\ngambles X inK , P(X) \u2264 Q(X).\nIt can be shown that, if this criterion is not satisfied, there are gambles X1, . . . , Xn inK such that\n(1) sup\n\u03b1\u2208A\n[\nn\u2211\nk=1\n[Xk(\u03b1)\u2212 P(Xk)]\n]\n< 0,\ni.e., the combination of the transactions in which the gambles Xk are bought for a price P(Xk) leads to a loss, whatever\nthe actual value of the parameter a. This means that the assessments in P are clearly unacceptable. For this reason,\navoiding sure loss is a minimal but stringent requirement that an agent\u2019s lower prevision should satisfy!\n2.2.4. Natural extension. If a lower prevision P on a set of gambles K avoids sure loss (which we have argued should\nalways at least be the case), then it has a dominating coherent lower prevision onL (A ), and it is not difficult to see that\nit has a point-wise smallest dominating lower prevision.1 This lower prevision E is called the natural extension of P : it\nis the most conservative correction of P to a coherent lower prevision onL (A ).\nThe natural extension E of P can also be calculated as follows: for any gamble X ,\nE (X) = sup\nn\u2208N\n\u03bbi\u22650\nYi\u2208K\n{\n\u03b3 : X \u2212 \u03b3 \u2265\nn\u2211\ni=1\n\u03bbi [Yi(\u03b1)\u2212 P(Yi)]\n}\n= sup\nn\u2208N\n\u03bbi\u22650\nYi\u2208K\ninf\n\u03b1\u2208A\n[\nX(\u03b1)\u2212\nn\u2211\ni=1\n\u03bbi[Yi(\u03b1)\u2212 P(Yi)]\n]\n.(2)\nIf the domainK of P is a linear subspace ofL (A ), then is is easy to show that\n(3) E (X) = sup\nY \u2208K\ninf\n\u03b1\u2208A\n[X(\u03b1)\u2212 [Y (\u03b1)\u2212 P(Y )]] .\nIf P is actually coherent, then it obviously coincides with its natural extension E on its domain K : E is the most\nconservative extension of P to a coherent lower prevision onL (A ).\nNatural extension is a very important tool in imprecise probability theory, as it allows any lower prevision that avoids\nsure loss to be corrected into a coherent lower prevision, and any coherent lower prevision to be extended to the set\nof all gambles, with minimal behavioural implications! The Bayesian counterpart of natural extension is de Finetti\u2019s\nfundamental theorem of probability [5]. To give an example of how de Finetti\u2019s theorem relates to natural extension,\nobserve for instance that from any probability measure \u00b5 on a \u03c3-field F on \u2126, which gives probabilities \u00b5(A) for all\nevents A inF , we obtain a coherent lower prevision P defined by P(IA) = P(IA) = \u00b5(A) for all A \u2208 F . The natural\nextension of this lower prevision P coincides exactly with the Lebesgue integral with respect to \u00b5, on all \u00b5-measurable\ngambles.\n2.2.5. Linear previsions. This section focuses on the important issue of how classical theory of probability is embedded\nin the theory of coherent lower previsions, and how the theory of coherent lower previsions can interpreted in terms of\nclassical probability theory. We establish that, without loss of generality, experts may represent their information using\nsets of classical probability models, rather than assessing supremum buying prices directly (which one is simpler depends\non the application).\nIf an agent has little or no relevant information about the outcome of the gamble X , his infimum acceptable price\nP(X) for selling the X will typically be substantially higher than his supremum acceptable price P(X) for buying it.\nThe bid-ask spread P(X) \u2212 P(X) is a measure for the amount of imprecision in the agent\u2019s behavioural dispositions\ntoward the gamble X . The more relevant information the agent has about the outcome of X , the closer P(X) and P(X)\nwill move to each other. If it should happen that P(X) = P(X), then this common value is denoted by P(X) and it is\ncalled the agent\u2019s fair price, or prevision, for the gamble X .\n1To prove this, use the definition of a coherent lower prevision to verify that the point-wise infimum of any number of coherent lower previsions that\ndominate P is still a coherent lower prevision that dominates P .\n4\nAs we hinted at before, in the Bayesian theory of uncertainty, championed by de Finetti [5, 6], it is assumed that an\nagent can always give a fair price for a gamble, whatever information he may have about its value. This questionable\nassumption is not made in imprecise probability theory, but it should be obvious that a precise, or Bayesian, model\nis nothing but a special case of an imprecise probability model, where the agent\u2019s lower and upper previsions happen to\ncoincide. In particular, de Finetti\u2019s rationality requirements for fair prices are nothing but particular cases of the coherence\nrequirements for lower (and upper) previsions.\nA coherent lower prevision P on L (A ) that is self-conjugate in the sense that P(X) = P(X) = P(X) for all X\nin L (A ), is called a linear prevision. It can be characterised alternatively as a real linear functional on the linear space\nL (A ), that is moreover positive (if X \u2265 0 then P(X) \u2265 0) and has unit norm (P(1) = 1). A linear prevision on an\narbitrary set of gamblesK is the restriction toK of some linear prevision onL (A ). We shall denote byP(A ) the set of\nall linear previsions onL (A ). As stated before, these linear previsions are exactly the same thing as de Finetti\u2019s coherent\nprevisions, i.e., the Bayesian, or precise, probability models. In general, a linear prevision P onK has no unique linear\nextension to all of L (A ): its natural extension is generally imprecise. This fact is emphasised by de Finetti\u2019s so-called\nfundamental theorem of probability which gives bounds for linear extensions of linear previsions. It is interesting to note\nthat these bounds correspond exactly to the values obtained by natural extension.\nThere is another interesting connection between lower and linear previsions. LetM(P) be the set of all linear previ-\nsions P onL (A ) that dominate the lower prevision P on its domainK : P(X) \u2265 P(X) for allX \u2208 K . Then P avoids\nsure loss if and only ifM(P) is non-empty, i.e., if P has a dominating linear prevision. P is coherent if and only if\nP(X) = min {P(X) : P \u2208M(P)}\nfor all X inK , i.e., if P is the lower envelope ofM(P). And, finally, if P avoids sure loss then the natural extension E\nis given by\nE (X) = min {P(X) : P \u2208M(P)}\nfor all X in L (A ). Moreover, any lower envelope of a set of linear previsions is a coherent lower prevision: it is\neasily checked using the definition of coherence that the point-wise infimum of a set of coherent lower previsions (and in\nparticular linear previsions) is a coherent lower prevision.\n2.2.6. Lower probabilities. A gamble X on A can be seen as a very general risky investment that yields a possibly dif-\nferent return X(\u03b1) for each possible value \u03b1 of the parameter a. In more traditional approaches to uncertainty modelling,\nit is common to work with a restricted class of quite simple gambles, that only allow the modeller to distinguish between\ntwo situations: zero-one-valued gambles, which correspond to events in A as we argued before. Let A be an event in A\nand let IA be its corresponding gamble. An agent\u2019s lower prevision P(IA) for this gamble is also denoted by P(A) and is\ncalled his lower probability for the event A: lower probabilities are simply lower previsions for zero-one-valued gambles.\nP(A) can also be interpreted as an agent\u2019s supremum acceptable rate for betting on the occurrence of the eventA. Similar\nconsiderations holds for upper probabilities. It should be mentioned here that most of the lower and upper probabilities in\nthe literature, such as precise probability measures, 2-monotone capacities, belief functions, and possibility measures, are\nin this way special cases of coherent lower and upper previsions [3, 14].\nWe prefer to work with the more general notion of gambles, rather than events. In fact, whereas the languages of events\nand of gambles are equally powerful when dealing with precise, or Bayesian, probabilities or previsions [5], it has been\nshown [14] that this is no longer the case for imprecise models: we need the more powerful language of gambles in the\nmore general theory of imprecise probability.\nThis concludes our brief survey of the theory of coherent lower previsions. We are now ready to apply the tools this\ntheory provides to modelling the expert assessments of the values of the variables a and b in the problems described in the\nIntroduction.\n2.3. Vacuous information. Let us first consider assessments of the type \u2018a assumes a value in a subset A of A \u2019. This\ntype of assessment includes the various instances of interval information present in the problem set\u2014one may for example\nthink of A as the positive real line and of A as a closed interval.\nThis type of information can be represented by the so-called vacuous lower prevision relative to A, which will be\ndenoted by PA, and is given by\nPA(X) = inf\n\u03b1\u2208A\nX(\u03b1),\nfor all gambles X inL (A ). This is a coherent lower prevision onL (A ) and its conjugate upper prevision is given by\nPA(X) = sup\n\u03b1\u2208A\nX(\u03b1).\nThere are several lines of reasoning to motivate that this lower prevision indeed is the appropriate model for the given\ninformation. First of all, if the agent knows that a belongs to A, and nothing more, he should be willing to buy a gamble\nX for any price s strictly lower than inf\u03b1\u2208AX(\u03b1) because doing so results in a sure gain; but he should not be willing to\npay a price t strictly higher than that, because then there is some \u03b1 \u2208 A such that t > X(\u03b1), and for all the agent knows,\n\u03b1 might be the actual value of the parameter a!\n5\nA second justification for PA is that it is the natural extension of the single precise probability assessment P(A) = 1.\nUsing Eq. (2), we find indeed that:\nsup\n\u03bb\u22650\ninf\n\u03b1\u2208A\n[X(\u03b1)\u2212 \u03bb[IA(\u03b1)\u2212 P(A)]] = sup\n\u03bb\u22650\nmin\n{\ninf\n\u03b1\u2208A\nX(a), inf\n\u03b1\u2208{A\n[X(\u03b1) + \u03bb]\n}\n= inf\n\u03b1\u2208A\nX(\u03b1) = PA(X).\nThis shows that the vacuous lower prevision relative to A follows uniquely from the single assessment that the agent\u2019s\nprobability of event A is equal to 1, or equivalently, that the agent is practically certain that a belongs to A (since he is\nprepared to bet at all odds on the occurrence of A).\nYet another way of justifying PA is the following. Consider a gamble X . Take any linear prevision P such that\nP(A) = 1. De Finetti\u2019s fundamental theorem of probability imposes bounds on P(X), i.e., states that P(X) belongs\nto some interval. The union of all these intervals over all linear previsions P such that P(A) = 1 is exactly given by\n[PA(X),PA(X)].\n2.4. Bayesian information. Let us now look at the expert assessment \u2018the parameter b is log-normally distributed with\nparameters \u00b5 and \u03c3\u2019. This is a special case of a very common type of assessment, stating that a continuous real random\nvariable b taking values in a subset B of the set of real numbers R has (cumulative) distribution function F , or density\nfunction \u03c6, with respect to some measure \u00b5 on the reals, such as the Lebesgue measure \u03bb.\nIt is well-known (see for instance [5]) that specifying such a model is equivalent to specifying a linear prevision P on\na setF of gambles that are measurable with respect to some \u03c3-algebra onB, where for each such gamble X ,2\nP(X) =\n\u222b\nB\nXdF =\n\u222b\nB\nX\u03c6 d\u00b5\nis the expectation of X . For the problem set under study here, B is the positive real line, \u00b5 is the Lebesgue measure \u03bb,\nF is the linear space of Lebesgue-measurable gambles, and \u03c6 is the log-normal density function.\nSince linear previsions are special types of coherent lower previsions, Bayesian models fit perfectly into imprecise\nprobability theory. This is also the case for the so-called robust Bayesian models that we deal with next.\n2.5. Bayesian information with vacuous parameters.\n2.5.1. Robust Bayesian models. The Bayesian approach has the disadvantage that the probability density \u03c6 must be\nknown exactly. In some situations this is not realistic: there may be a class \u03a6 of probability densities \u03c6, each compatible\nwith the given information, and different choices within this class may lead to completely different results: inferences\nbased on particular choices of \u03c6 will then not be robust, and will not with any confidence reflect the information that is\nactually available.\nThe theory of imprecise probabilities deals with this situation in a straightforward manner: it associates with the set \u03a6\nof density functions a coherent3 lower prevision P that is the lower envelope of the linear previsions associated with the\ndensity functions \u03c6 \u2208 \u03a6: for each measurable gamble X ,\n(4) P(X) = inf\n\u03c6\u2208\u03a6\n\u222b\nB\nX\u03c6 d\u00b5.\nThis ensures that our agent will accept to buy a gamble X only for prices that are lower than any of the prices\n\u222b\nBX\u03c6 d\u00b5\ncorresponding to specific choices of \u03c6 in \u03a6; any conclusions we may draw from the model will be automatically robust.\nWe have shown in Section 2.2.5 that, in a very specific sense, coherent lower previsions are mathematically equivalent\nwith sets of linear previsions. This way of looking at coherent lower previsions is sometimes referred to as the Bayesian\nsensitivity analysis interpretation, or robust Bayesian interpretation of such lower previsions.\n2.5.2. An alternative justification. There is another way of deriving Eq. (4), which, in our opinion, provides a better\njustification for associating a lower prevision with a set of density functions.\nAssume that the available information about a variable b allows us to specify for it a Bayesian model P(\u00b7|\u03b8) that\ndepends on some additional variable \u03b8 \u2208 \u0398. However, the true value of the variable \u03b8 is only known to belong to some\nnon-empty subset \u03980 of \u0398. We thus have the following information about the value of the variable b.\n(i) Conditionally on \u03b8, a probability density \u03c6\u03b8 for b. The inferred conditional linear prevision P(\u00b7|\u03d1), given by\nP(X|\u03d1) =\n\u222b\nB\nX\u03c6\u03d1 d\u00b5\nfor each measurable gamble X , would describe the agent\u2019s behavioural dispositions toward gambles onB if \u03d1 were\nthe true value of the parameter \u03b8. This conditional linear prevision P(\u00b7|\u03d1) is also called a sampling model.\n(ii) Vacuous information about the variable \u03b8, described by a vacuous lower prevision relative to \u03980, P\u03980 , on L (\u0398),\nwhere \u03980 \u2286 \u0398 and \u03980 6= \u2205. The coherent lower prevision P\u03980 is also called the prior.\n2To see that P really is a linear prevision, observe that by the Hahn-Banach theorem P can be extended to a linear prevision on the whole space\nL (B). Generally speaking, such an extension will not be unique, or equivalently, the natural extension of P to L (B) will be imprecise (a coherent\nlower prevision that is not a linear prevision).\n3A lower envelope of linear previsions is always a coherent lower prevision, see Section 2.2.5.\n6\nFor a given measurable gamble X onB, we shall denote by P(X|\u0398) the gamble on \u0398 that assumes the value P(X|\u03d1) in\nthe element \u03d1 of \u0398.\nWalley\u2019s marginal extension theorem [14, Theorem 6.7.2] then tells us that the natural extension P of the lower previ-\nsion P\u03980 and the conditional linear prevision P(\u00b7|\u0398) is given by\nP(X) = P\u03980(P(X|\u0398)) = inf\u03d1\u2208\u03980 P(X|\u03d1) = inf\u03d1\u2208\u03980\n\u222b\nB\nX\u03c6\u03d1 d\u00b5,\nfor all measurable gamblesX . P(X) is the supremum acceptable buying price forX that can be inferred from the agent\u2019s\nassessments P\u03980 and P(\u00b7|\u0398), through arguments of coherence alone! P is the smallest (most conservative) coherent\nextension to measurable gambles onB of both P(\u00b7|\u0398) and P\u03980 .\nFor a full motivation of marginal extension we refer to [14, Chapter 6]. But it may be interesting to observe that it\ngeneralises Kolmogorov\u2019s definition P(B) = E(Pu(B)) of conditional probability [9, Chapter V, Equations (1\u20133), pp. 47\u2013\n9], where Pu(B) is the probability of event B conditional on the outcome of a random variable u, E is the expectation of\na gamble depending on the outcome of the random variable u and P(B) is the probability of the event B.\nFor the problem set under study, the sampling model is a log-normal distribution with parameters \u00b5 and \u03c3; we shall\ndenote the set of possible values for parameter \u00b5 by M and for parameter \u03c3 by S . Thus, we have that \u0398 = M \u00d7 S\nand \u03b8 = (\u00b5, \u03c3). Each expert expresses his information about \u00b5 and \u03c3 through a vacuous lower prevision on L (M ) or\nL (S ), relative to some subset (closed interval) ofM orS , respectively.\nWhat we still need, however, in order to be able to apply the marginal extension theorem, is a way to combine the\nseparate lower previsions onM andS into a joint lower prevision on \u0398 = M \u00d7S . If we can do that, we shall be able\nto arrive at a lower prevision modelling the available information about the parameter b. We have seen above that we are\nalso able to find a lower prevision modelling the available information about parameter a. But here again, we still have\nto combine these separate lower previsions into a joint lower prevision modelling the available information about a and b\ntaken together. We conclude that in order to proceed, we need a way to form so-called products of lower previsions. This\nis the subject of the next section.\n3. PRODUCTS\nAssume that the information about the value that the variable a assumes inA is modelled by a coherent lower prevision\nP . To make things as easy as possible and as complicated as necessary, its domain is assumed to be a linear space\nFA \u2286 L (A ) of gambles on A \u2014 for instance, the set L (A ), or the linear space of all gambles that are measurable\nwith respect to some \u03c3-field on A . Similarly, the information about the variable b is represented by a coherent lower\nprevision Q defined on a linear space of gamblesFB \u2286 L (B).\nWe now want to find a way to combine the information about a and that about b into information about the values that\nthe variable (a, b) takes in the product space A \u00d7B. In other words, we want to find a way to combine P and Q into a\ncoherent product lower prevision P whose marginals are P and Q . This is made more clear in the following definition.\nWe denote by FA \u00d7FB the linear space of those gambles Z on A \u00d7B whose partial maps belong to FA and FB,\ni.e., such that for all \u03b1 \u2208 A and \u03b2 \u2208 B, Z(\u03b1, \u00b7) \u2208 FB and Z(\u00b7, \u03b2) \u2208 FA . If X belongs to FA then it can also be\nconsidered as a gamble on A \u00d7B that is constant on B, and which therefore belongs to FA \u00d7FB. A similar remark\ncan be made about gambles Y inFB.\nDefinition 1. A coherent lower previsionR whose domainF includesFA \u00d7FB is called a product of the coherent lower\nprevisions P andQ if it has these lower previsions as its marginals, i.e., if for allX inFA and Y inFB, R(X) = P(X)\nand R(Y ) = Q(Y ).\nWe shall consider three different ways to define a product of P and Q .\n3.1. Natural extension. The natural extension P \u00d7NEQ of P and Q is defined as the smallest coherent lower prevision\nonFA \u00d7FB that has marginals P andQ . It is, in other words, their least-committal or most conservative product. Since\nwe assume the domainsFA andFB to be linear spaces, we find, using Eq. (3), that for any gamble Z inFA \u00d7FB:\n(5)\n(\nP \u00d7NEQ\n)\n(Z) = sup\nX\u2208FA ,Y \u2208FB\ninf\n(\u03b1,\u03b2)\u2208A\u00d7B\n[\nZ(\u03b1, \u03b2)\u2212 [X(\u03b1)\u2212 P(X)]\u2212 [Y (\u03b2)\u2212Q(Y )]] .\nIn forming the natural extension, no assumption is made about the possible independence of the variables a and b. Making\nsuch an additional assumption generally leads to products that dominate the natural extension, and are therefore less\nconservative. We look at two types of independent products, each with a different interpretation.\n3.2. Independent natural extension. The independent natural extension of P and Q is denoted by P \u00d7INEQ and\ndefined as the smallest coherent lower prevision onFA \u00d7FB that has marginals P and Q , also taking into account the\nextra assessment that the variables a and b are epistemically independent, i.e., that additional knowledge about the value\nthat a assumes in A does not affect our knowledge about the value that b assumes inB, and vice versa. It can be shown\nusing the results and ideas in [14, Chapters 6\u20139] that for any gamble Z inFA \u00d7FB:\n(6)\n(\nP \u00d7INEQ\n)\n(Z) = sup\nX\u2208FA ,Y \u2208FB\ninf\n(\u03b1,\u03b2)\u2208A\u00d7B\n[\nZ(\u03b1, \u03b2)\u2212 [X(\u03b1, \u03b2)\u2212 P(X(\u00b7, \u03b2))]\u2212 [Y (\u03b1, \u03b2)\u2212Q(Y (\u03b1, \u00b7))]],\n7\nwhere we have introduced the notations\nFA = {X \u2208 L (A \u00d7B) : (\u2200\u03b2 \u2208 B)(X(\u00b7, \u03b2) \u2208 FA )}\nFB = {Y \u2208 L (A \u00d7B) : (\u2200\u03b1 \u2208 A )(Y (\u03b1, \u00b7) \u2208 FB)} .\nObserve thatFA \u00d7FB = FA \u2229FB,L (A )\u00d7FB = FB andFA \u00d7L (B) = FA .\n3.3. Type-I product. There is another way to define an independent product of P and Q that is compatible with the\nBayesian sensitivity analysis interpretation of a lower prevision. On this view, the uncertainty about the variable a is\nideally described by a linear prevision PT ; the only problem is that for some reason, we are not able to uniquely identify\nit. Specifying a coherent lower prevision P is then tantamount to stating that the unknown PT should belong to the set\nM(P) = {P \u2208 P(A ) : (\u2200X \u2208 FA )(P(X) \u2265 P(X))}\nof those linear previsions on A that dominate the lower prevision P on its domain FA . Similar considerations hold\nfor the uncertainty about the variable b, its ideal precise model QT , and the coherent lower prevision Q . If a and b are\nindependent, then the ideal model describing the uncertainty about the value of the joint variable (a, b) is the independent\nproduct PT \u00d7QT , i.e., the linear prevision defined on gambles Z on A \u00d7B by\n(PT \u00d7QT ) (Z) = PT (QT (Z)) = QT (PT (Z)) ,\nwhere by QT (Z) we denote the gamble on A taking the value QT (Z(\u03b1, \u00b7)) in \u03b1 \u2208 A and similarly for PT (Z).\nIt then seems appropriate to define the type-I product P \u00d7TIQ of the lower previsions P and Q as the lower envelope\nof all compatible independent linear products (see also Corollary 1 further on):\n(7)\n(\nP \u00d7TIQ\n)\n(Z) = inf\n{\n(P \u00d7Q)(Z) : P \u2208M(P) and Q \u2208M(Q)}\nfor all Z inFA \u00d7FB.\n3.4. Products with a vacuous lower prevision. In general, we have that(\nP \u00d7NEQ\n)\n(Z) \u2264 (P \u00d7INEQ) (Z) \u2264 (P \u00d7TIQ) (Z)\nfor all gambles Z inFA \u00d7FB, but it turns out that, if at least one of the lower previsions P or Q is vacuous, then their\nindependent natural extension and their type-I product coincide! This is the case for all of the problems in the problem\nset described in the Introduction. If both P and Q are vacuous, then all three products coincide. This is made evident by\nthe following theorem and proposition.\nTheorem 1. Consider a non-empty subset A of A , let Q be a coherent lower prevision defined on a linear space of\ngamblesFB, and let PA be the vacuous lower prevision relative to A, defined onL (A ) by\nPA(X) = inf\n\u03b1\u2208A\nX(\u03b1),\nfor all gambles X on A . Then the natural extension of PA and Q is given by(\nPA\u00d7NEQ\n)\n(Z) = E\n(\ninf\n\u03b1\u2208A\nZ(\u03b1, \u00b7)\n)\nfor all gambles Z in FB = L (A ) \u00d7 FB, where E is the natural extension of Q to the set L (B). Moreover, the\nindependent natural extension and the type-I product of PA and Q coincide and are given for all Z inFB by(\nPA\u00d7INEQ\n)\n(Z) =\n(\nPA\u00d7TIQ\n)\n(Z) = inf\n\u03b1\u2208A\nQ(Z(\u03b1, \u00b7)).\nProof. Let us first look at the natural extension of PA and Q . For any Z inFB we find that(\nP \u00d7NEQ\n)\n(Z) = sup\nY \u2208FB\nsup\nX\u2208FA\ninf\n\u03b2\u2208B\ninf\n\u03b1\u2208A\n[\nZ(\u03b1, \u03b2)\u2212 [X(\u03b1)\u2212 P(X)]\u2212 [Y (\u03b2)\u2212Q(Y )]]\n\u2264 sup\nY \u2208FB\ninf\n\u03b2\u2208B\nsup\nX\u2208FA\ninf\n\u03b1\u2208A\n[\nZ(\u03b1, \u03b2)\u2212 [X(\u03b1)\u2212 P(X)]\u2212 [Y (\u03b2)\u2212Q(Y )]]\nand using the fact that the coherent PA coincides onL (A ) with its natural extension, given by Eq. (3),\n= sup\nY \u2208FB\ninf\n\u03b2\u2208B\nPA\n(\nZ(\u00b7, \u03b2)\u2212 [Y (\u03b2)\u2212Q(Y )])\n= sup\nY \u2208FB\ninf\n\u03b2\u2208B\n[\ninf\n\u03b1\u2208A\nZ(\u03b1, \u03b2)\u2212 [Y (\u03b2)\u2212Q(Y )]\n]\nand invoking Eq. (3) for the natural extension E of Q ,\n= E\n(\ninf\n\u03b1\u2208A\nZ(\u03b1, \u00b7)\n)\n.\nTo prove the converse inequality, let X = \u2212\u03bb[1 \u2212 IA] in the definition of the natural extension, where \u03bb is some real\nnumber satisfying\n\u03bb \u2265 sup\n(\u03b1,\u03b2)\u2208A\u00d7B\nZ(\u03b1, \u03b2)\u2212 inf\n(\u03b1,\u03b2)\u2208A\u00d7B\nZ(\u03b1, \u03b2).\n8\nFor this choice of \u03bb it is easy to prove that for all \u03b2 \u2208 B:\n(8) \u03bb+ inf\n\u03b1 6\u2208A\nZ(\u03b1, \u03b2) \u2265 inf\n\u03b1\u2208A\nZ(\u03b1, \u03b2)\nIt then follows, since PA(X) = 0, that(\nP \u00d7NEQ\n)\n(Z) \u2265 sup\nY \u2208FB\ninf\n\u03b2\u2208B\ninf\n\u03b1\u2208A\n[\nZ(\u03b1, \u03b2) + \u03bb[1\u2212 IA(\u03b1)]\u2212 [Y (\u03b2)\u2212Q(Y )]\n]\n= sup\nY \u2208FB\ninf\n\u03b2\u2208B\n[\nmin\n{\ninf\n\u03b1\u2208A\nZ(\u03b1, \u03b2), inf\n\u03b16\u2208A\nZ(\u03b1, \u03b2) + \u03bb\n}\n\u2212 [Y (\u03b2)\u2212Q(Y )]\n]\nand taking into account the inequality (8),\n= sup\nY \u2208FB\ninf\n\u03b2\u2208B\n[\ninf\n\u03b1\u2208A\nZ(\u03b1, \u03b2)\u2212 [Y (\u03b2)\u2212Q(Y )]\n]\nand using Eq. (3) for the natural extension E of Q ,\n= E\n(\ninf\n\u03b1\u2208A\nZ(\u03b1, \u00b7)\n)\n.\nWe now turn to the type-I product of PA and Q :(\nP \u00d7TIQ\n)\n(Z) = inf\nQ\u2208M(Q)\ninf\nP\u2208M(PA)\nP(Q(Z))\n= inf\nQ\u2208M(Q)\nPA(Q(Z))\n= inf\nQ\u2208M(Q)\ninf\n\u03b1\u2208A\nQ(Z(\u03b1, \u00b7))\n= inf\n\u03b1\u2208A\ninf\nQ\u2208M(Q)\nQ(Z(\u03b1, \u00b7))\n= inf\n\u03b1\u2208A\nQ(Z(\u03b1, \u00b7)).\nTo conclude the proof, we consider the independent natural extension of PA andQ . For ease of notation, denote by R the\nlower prevision defined on the set of gambles L (B) by R(Z) = inf\u03b1\u2208A E (Z(\u03b1, \u00b7)), where, as before, E is the natural\nextension of Q . It is not difficult to prove that R is coherent. Consequently, R(\u00b7) \u2265 inf[\u00b7], and therefore(\nPA\u00d7INEQ\n)\n(Z) \u2264 sup\nX\u2208FA ,Y \u2208FB\nR\n(\nZ \u2212 [X \u2212 PA(X)]\u2212 [Y \u2212Q(Y )]\n)\nand from the coherence (superadditivity) of R\n\u2264 sup\nX\u2208FA ,Y \u2208FB\n[\nR(Z)\u2212 R(X \u2212 PA(X))\u2212 R(Y \u2212Q(Y ))\n]\n.\nAt the same time, we deduce from the coherence of E that\nR (X \u2212 PA(X)) = inf\n\u03b1\u2208A\nE\n(\nX(\u03b1, \u00b7)\u2212 inf\n\u03b3\u2208A\nX(\u03b3, \u00b7)\n)\n\u2265 0\nand similarly, since Q and its natural extension E coincide onFB,\nR\n(\nY \u2212Q(Y )) = inf\n\u03b1\u2208A\nQ\n(\nY (\u03b1, \u00b7)\u2212Q(Y (\u03b1, \u00b7))) = 0,\nwhence immediately\n(\nPA\u00d7INEQ\n)\n(Z) \u2264 R(Z). To prove the converse inequality, let in the definition for the indepen-\ndent natural extension Y = Z \u2208 FB and X = Q(Z) \u2208 L (A ). \u0003\nProposition 1. Let A be a non-empty subset of A and B a non-empty subset ofB. Then for all Z inL (A \u00d7B),\n(PA\u00d7NE PB) (Z) = (PA\u00d7INE PB) (Z) = (PA\u00d7TI PB) (Z) = PA\u00d7B(Z) = inf\n(\u03b1,\u03b2)\u2208A\u00d7B\nZ(\u03b1, \u03b2).\nProof. For any Z inL (A \u00d7B) we find using Theorem 1 that, since PA is a vacuous lower prevision,\n(PA\u00d7NE PB) (Z) = PB\n(\ninf\n\u03b1\u2208A\nZ(\u03b1, \u00b7)\n)\n= inf\n\u03b2\u2208B\ninf\n\u03b1\u2208A\nZ(\u03b1, \u03b2) = inf\n(\u03b1,\u03b2)\u2208A\u00d7B\nZ(a, b) = PA\u00d7B(Z),\nand similarly,\n(PA\u00d7INE PB) (Z) = (PA\u00d7TI PB) (Z) = inf\n\u03b1\u2208A\nPB(Z(\u03b1, \u00b7)) = inf\n\u03b1\u2208A\ninf\n\u03b2\u2208B\nZ(\u03b1, \u03b2) = inf\n(\u03b1,\u03b2)\u2208A\u00d7B\nZ(a, b) = PA\u00d7B(Z). \u0003\n9\n3.5. Products with a linear prevision. If at least one of the lower previsions P or Q is a linear prevision, which is\nthe case for Problem 6 described in the Introduction, then their independent natural extension and their type-I product\ncoincide as well.\nTheorem 2. Let P be a coherent lower prevision defined on a linear space of gambles FA \u2286 L (A ), and let Q be a\nlinear prevision defined onL (B). Then for all gambles Z inFA = FA \u00d7L (B),\n(P \u00d7INEQ) (Z) = (P \u00d7TIQ) (Z) = E (Q(Z)),\nwhere E is the natural extension of P to L (A ), and where Q(Z) denotes the gamble on A whose value in \u03b1 \u2208 A is\ngiven by Q(Z(\u03b1, \u00b7)).\nProof. Let Z be any gamble inFA . We begin with the last equality. SinceM(Q) = {Q}, we see that indeed\n(P \u00d7TIQ) (Z) = inf\nP\u2208M(P)\nP(Q(Z)) = E (Q(Z)).\nTo prove the first equality, apply Eq. (6) to see that\n(P \u00d7INEQ) (Z) = sup\nX\u2208FA ,Y \u2208L (B)\ninf [Z \u2212 [X \u2212 P(X)]\u2212 [Y \u2212Q(Y )]] .\nIf we make the particular choice Y = Z in this supremum, and apply Eq. (3) for the natural extension E of P , we find\nthat\n(P \u00d7INEQ) (Z) \u2265 sup\nX\u2208FA\ninf [Q(Z) + [X \u2212 P(X)]] = E (Q(Z)).\nTo prove the converse inequality, recall that it follows from the coherence of P and Q that for any gamble U on A \u00d7B,\nE (Q(U)) \u2265 inf[U ], whence\n(P \u00d7INEQ) (Z) = sup\nX\u2208FA ,Y \u2208L (B)\ninf [Z \u2212 [X \u2212 P(X)]\u2212 [Y \u2212Q(Y )]]\n\u2264 sup\nX\u2208FA ,Y \u2208L (B)\nE (Q (Z \u2212 [X \u2212 P(X)]\u2212 [Y \u2212Q(Y )]))\n\u2264 sup\nX\u2208FA ,Y \u2208L (B)\n[E (Q(Z))\u2212 E (Q(X \u2212 P(X)))\u2212 E (Q(Y \u2212Q(Y )))]\nIt is easy to see that, by Lemma 1,\nE (Q(X \u2212 P(X))) = E (Q(X))\u2212Q(P(X)) \u2265 0,\nand also that\nE (Q(Y \u2212Q(Y ))) = 0\nwhence indeed also (P \u00d7INEQ) (Z) \u2264 E (Q(Z)). \u0003\nLemma 1. Let P be a coherent lower prevision defined on L (A ), and let Q be a linear prevision defined on L (B).\nThen for all gambles Z inL (A \u00d7B):\nP(Q(Z)) \u2265 Q(P(Z)).\nProof. For any Z in L (A \u00d7B) and any P inM(P) we have that P(Z) \u2264 P(Z), whence Q(P(Z)) \u2264 Q(P(Z)) =\nP(Q(Z)), and consequently\nQ(P(Z)) \u2264 inf\nP\u2208M(P)\nP(Q(Z)) = P(Q(Z)). \u0003\nCorollary 1. Let P be a linear prevision defined onL (A ) and let Q be a linear prevision defined onL (B). Then\nP \u00d7INEQ = P \u00d7TIQ = P \u00d7Q .\nIt may happen, as in some of the problems sketched in the Introduction, that the variable b is assumed to have a\nprecise probability model QT , whose parameters are not well known. This means that the lower prevision Q has the\nBayesian sensitivity analysis interpretation. If no such additional assumption is made for the variable a, then the lower\nprevision P does not have this interpretation. In this case, if the ideal QT were known, the lower prevision describing\nthe joint information would be given by P \u00d7INEQT , taking into account the epistemic independence of a and b, and the\nappropriate product of P and Q is then given by\ninf\nQ\u2208M(Q)\n(P \u00d7INEQ) (Z) = inf\nQ\u2208M(Q)\nE (Q(Z)) = inf\nQ\u2208M(Q)\ninf\nP\u2208M(P)\nP(Q(Z)) =\n(\nP \u00d7TIQ\n)\n(Z)\nfor any gamble Z, taking into account Theorem 2. In other words, as soon as the lower prevision for at least one of the two\nepistemically independent variables a and b has the Bayesian sensitivity analysis interpretation, the appropriate product\nto use is the type-I product. If none of the models for these variables has the Bayesian sensitivity analysis interpretation,\ni.e., if no additional assumption is made that their model is precise but not well known, we should use the independent\nnatural extension to form independent products. Fortunately, since it will turn out that at least one of the lower previsions\nin the problem set is always of the vacuous type, we may deduce from Theorem 1 that both types of independent products\ncoincide, and we need therefore in the rest of this paper not really be concerned with these subtleties of interpretation.\n10\n4. INFERENCE\nIn our line of reasoning so far, we have taken the necessary steps to ensure that we can model the available information\nabout the parameters a and b, or actually their joint value (a, b), by a coherent lower prevision P on some linear spaceF\nof gambles on A \u00d7B. The final step to take is the transformation of P into a coherent lower prevision Py on the set of\npossible values Y of the parameter y, using the functional relationship y = f(a, b) between (a, b) and y.\nThis can be achieved quite easily using the following heuristic course of reasoning. Consider a gamble U on Y . If\n(a, b) assumes the value (\u03b1, \u03b2) in A \u00d7 B, then y assumes the value f(\u03b1, \u03b2), and consequently U assumes the value\nU(f(\u03b1, \u03b2)). This means that the gamble U on Y can be interpreted as a gamble U \u25e6 f onA \u00d7B, whose lower prevision\nis P(U \u25e6 f), provided that U \u25e6 f belongs to the linear space\nFy = {U \u2208 L (Y ) : U \u25e6 f \u2208 F}\nof gambles on Y . This leads immediately to the definition of the lower prevision Py(\u00b7) = P( \u00b7 \u25e6 f) onFy .\nThis course of reasoning can be given more weight by arguments of coherence. If we know that the variable (a, b)\nassumes the value (\u03b1, \u03b2), then it is absolutely certain that the variable y assumes the value f(\u03b1, \u03b2), and this can be\nmodelled by a degenerate linear prevision all of whose probability mass lies in f(\u03b1, \u03b2): for any gamble U on Y ,\nP(U |\u03b1, \u03b2) = U(f(\u03b1, \u03b2)).\nThe functional relationship f between (a, b) and y can therefore be represented by the conditional linear previsions\nP(\u00b7|\u03b1, \u03b2) for all (\u03b1, \u03b2) in A \u00d7B, or with the notation established in Section 2.5.2, by the conditional linear prevision\nP(\u00b7|A \u00d7B) defined on the setL (Y ) of all gambles on Y . Together with the prior P , it leads, through natural extension\n(Walley\u2019s marginal extension theorem [14, Theorem 6.7.2], see also the similar course of reasoning in Section 2.5.2), to\nthe lower prevision Py , defined on the set of gamblesFy by\n(9) Py(U) = P(P(U |A \u00d7B)) = P(U \u25e6 f),\nfor all U inFy . Py(U) is the smallest (most conservative) supremum acceptable price for buying the gamble U that can\nbe inferred from the lower prevision P and the functional relationship f , using only arguments of coherence!\nObserve that, if U is the indicator IC of some subset C of Y , then it is clear that (IC \u25e6 f)(\u03b1, \u03b2) equals one if and only\nif f(\u03b1, \u03b2) \u2208 C, or equivalently, (\u03b1, \u03b2) \u2208 f\u22121(C), and that it equals zero elsewhere:\nIC \u25e6 f = If\u22121(C),\nwhich tells us that Py(C) = P(f\n\u22121(C)). In other words, Eq. (9) is the appropriate generalisation to lower previsions and\nto gambles of the notion of a probability measure induced by the map f !\n5. SOLUTIONS TO PROBLEMS 1, 4 AND 6\nWe are now ready to apply the results derived so far to the solution of the problems not involving combination of\nassessments from different experts.\n5.1. Solution to Problem 1. We only know that a and b assume values in the respective closed intervals A = [0.1, 1.0]\nand B = [0.0, 1.0]. Hence, we have the vacuous lower prevision PA on L (A ) and the vacuous lower prevision PB on\nL (B). Using Proposition 1, we find that the lower prevision P representing the available information about the value of\n(a, b) is given by:\nP(Z) = (PA\u00d7INE PB) (Z) = (PA\u00d7TI PB) (Z) = inf\n(\u03b1,\u03b2)\u2208A\u00d7B\nZ(\u03b1, \u03b2),\nfor each Z in L (A \u00d7B). Eq. (9) tells us that the lower prevision Py representing the available information about the\noutput y is then given by\nPy(U) = inf\n(\u03b1,\u03b2)\u2208A\u00d7B\nU(f(\u03b1, \u03b2)),\nfor all gambles U on Y . This lower prevision can be used as a starting point for further inference and decision problems\ninvolving the value of the output y. As an illustration, we calculate the lower prevision Py(1Y ) = P(f) and upper\nprevision Py(1Y ) = P(f) of the output y, where 1Y is the identity map on Y .4 We get\nP(f) = 0.692201 and P(f) = 2.\n5.2. Solution to Problem 4. We know that a assumes a value in the closed interval A = [0.1, 1.0], and that b is log-\nnormally distributed, ln b \u223c N(\u00b5, \u03c3), with parameters \u00b5 \u2208 M = [0.0, 1.0] and \u03c3 \u2208 S = [0.1, 0.5]. Hence, we have as\nappropriate models the vacuous lower prevision PA on L (A ) and, as argued in Section 2.5, a lower envelope over M\nand S:\nQ(X) = inf\n(\u00b5,\u03c3)\u2208M\u00d7S\n\u222b\nB\nX\u03c6\u00b5,\u03c3d\u03bb = inf\n\u00b5\u2208M\ninf\n\u03c3\u2208S\n\u222b\nB\nX\u03c6\u00b5,\u03c3d\u03bb,\nfor each integrable gambleX onB, with \u03c6\u00b5,\u03c3 the log-normal distribution and \u03bb the Lebesgue measure on the reals. Since\nthe model Q for the parameter b has the Bayesian sensitivity analysis interpretation, we have argued in Section 3.5 that\n4The reader will perhaps object that 1Y is not bounded, and therefore not a gamble. But we have shown elsewhere [12, 13] that this difficulty can\nbe circumvented, and we shall not go any deeper into this matter here.\n11\nthe appropriate product to use is the type-I product. But since PA is vacuous, this product coincides with the independent\nnatural extension, by Theorem 1, and it is given by\nP(Z) =\n(\nPA\u00d7TIQ\n)\n(Z) =\n(\nPA\u00d7INEQ\n)\n(Z) = inf\n\u03b1\u2208A\ninf\n\u00b5\u2208M\ninf\n\u03c3\u2208S\n\u222b\nB\nZ(\u03b1, \u00b7)\u03c6\u00b5,\u03c3d\u03bb,\nfor each gamble Z \u2208 L (B) such that Z(\u03b1, \u00b7) is integrable for every \u03b1 \u2208 A . The lower prevision Py representing the\navailable information about the output y is then given by\nPy(U) = inf\n\u03b1\u2208A\ninf\n\u00b5\u2208M\ninf\n\u03c3\u2208S\n\u222b\nB\nU (f(\u03b1, \u00b7))\u03c6\u00b5,\u03c3d\u03bb,\nfor all gambles U on Y such that U (f(\u03b1, \u00b7)) is integrable for all \u03b1 \u2208 A . In particular, using the monotonicity of the\nintegral\n\u222b\nB f(\u03b1, \u00b7)\u03c6\u00b5,\u03c3d\u03bb with respect to \u03b1, \u00b5 and \u03c3, we easily find the following values for the lower and the upper\nprevision of the output y:\nP(f) = 1.00966 and P(f) = 4.08022.\n5.3. Solution to Problem 6. We know that a assumes a value in the closed interval A = [0.1, 1.0], and that b is log-\nnormally distributed, ln b \u223c N(\u00b5, \u03c3), with parameters \u00b5 = 0.5 and \u03c3 = 0.5. Hence, we have as appropriate models the\nvacuous lower prevision PA onL (A ) and the linear prevision\nQ(X) =\n\u222b\nB\nX\u03c6\u00b5,\u03c3d\u03bb,\nfor each integrable gamble X on B, with \u03c6\u00b5,\u03c3 the log-normal distribution and \u03bb the Lebesgue measure on the reals.\nBy Theorem 1, or alternatively by Theorem 2, the independent natural extension and the type-I product of PA and Q\ncoincide, and the lower prevision P representing the available information about the parameters (a, b) is given by:\n(PA\u00d7INEQ) (Z) = (PA\u00d7TIQ) (Z) = inf\n\u03b1\u2208A\n\u222b\nB\nZ(\u03b1, \u00b7)\u03c6\u00b5,\u03c3d\u03bb\nfor each gamble Z \u2208 L (B) such that Z(\u03b1, \u00b7) is integrable for every \u03b1 \u2208 A . The available information about the output\ny is modelled by the lower prevision Py , where\nPy(U) = inf\n\u03b1\u2208A\n\u222b\nB\nU (f(\u03b1, \u00b7))\u03c6\u00b5,\u03c3d\u03bb\nfor all gambles U on Y such that U (f(\u03b1, \u00b7)) is integrable for all \u03b1 in A . In particular, using the monotonicity of\u222b\nB f(\u03b1, \u00b7)\u03c6\u00b5,\u03c3d\u03bb with respect to \u03b1, we find for the lower and upper prevision of the output y:\nP(f) = 1.05939 and P(f) = 2.86825.\n6. COMBINATION OF ASSESSMENTS\nWhereas Problems 1, 4 and 6 involve only a single assessment for each parameter, problems 2, 3 and 5 also involve\ndifferent and possibly conflicting assessments about the same parameter. In order to solve these problems, we now aim at\nfinding ways of combining multiple lower previsions defined on gambles on the same parameter space into a single lower\nprevision. To arrive at a joint uncertainty model for all the parameters we can combine lower previsions on each parameter\nseparately and apply one of the product rules described in Section 3. We could alternatively first construct all products\nbetween lower previsions on different parameters, and then combine these products. In general, even more scenarios are\npossible.\nHowever, the order in which we apply combination and product rules is not prescribed, and we therefore should demand\nthat this order has no influence on the final result. We call this the order of combination invariance principle: the joint\nshould not depend on the order in which combinations and products are applied. Our approach satisfies this principle (see\nPropositions 4, 5, 6 and 7).\nIn order to fix terminology and notation, suppose we have n (male) agents, called the experts. Their assessments about\nthe value that a parameter \u03c9 assumes in a set of possible values \u2126 are expressed through coherent lower previsions Pk on\nsome subsetKk ofL (\u2126), for k = 1, . . . , n. We show how these lower previsions can be combined into a single coherent\nlower prevision defined on the set of gamblesL (\u2126).\n6.1. Consistency and conjunction. Consider a new (female) agent, called the modeller. She wishes to aggregate all the\nassessments Pk to a single lower prevision PM defined onL (\u2126). Say that she trusts an expert\u2019s assessment Pk whenever\nshe is willing to accept every decision he makes, that is, whenever she is willing to accept his price s for buying X as\nher price for buying X , and this for each gamble X \u2208 Kk and each buying price s < Pk(X). We immediately have the\nfollowing result.\nTheorem 3. The modeller trusts an expert\u2019s assessment Pk if and only if her coherent lower prevision PM point-wise\ndominates Pk on its domainKk.\nThis leads to the notion of consistency of the expert assessments Pk.\nDefinition 2. If there is at least one coherent lower prevision that point-wise dominates the coherent lower previsions Pk\non their respective domainsKk for k = 1, . . . , n, then these lower previsions Pk are called consistent.\n12\nIt is not difficult to show that if the Pk are consistent, then there is (point-wise) smallest coherent lower prevision that\ndominates all the Pk on their respective domains Kk. This coherent lower prevision is called the conjunction of the Pk\nand is denoted by unk=1Pk. By Theorem 3, it is the smallest, or most conservative, coherent lower prevision on L (\u2126)\nthat the modeller can have such that she still trusts each of the experts. The conjunction of two consistent coherent lower\nprevisions P1 and P2 is also denoted by P1 u P2. By the theorem given below, the conjunction is an associative and\ncommutative operator, and hence, it satisfies the order of combination invariance principle.\nTheorem 4. Consider, for each of the coherent lower previsions Pk defined on the set Kk, the set of dominating linear\nprevisions\nM(Pk) = {P \u2208 P(\u2126): (\u2200X \u2208 Kk)(Pk(X) \u2264 P(X))} .\nThen the (Pk)\nn\nk=1 are consistent if and only if\nn\u22c2\nk=1\nM(Pk) 6= \u2205.\nIn that case, the conjunction unk=1Pk is the lower envelope of this intersection: for all gambles X on \u2126,\n(unk=1Pk) (X) = inf\n{\nP(X) : P \u2208\nn\u22c2\nk=1\nM(Pk)\n}\n.\nProof. The first part of the theorem is immediate if we recall that a coherent lower prevision is always dominated by some\nlinear prevision, and that a linear prevision is in particular a coherent lower prevision.\nTo prove the second part, assume that the (Pk)\nn\nk=1 are consistent. Then it suffices to show that\nM(unk=1Pk) =\nn\u22c2\nk=1\nM(Pk).\nAssume that the linear prevision P belongs to\n\u22c2n\nk=1M(Pk). Then it dominates all of the Pk on their domains, and\ntherefore, since it is in particular a coherent lower prevision, it also dominates smallest coherent lower prevision unk=1Pk\nthat dominates all of the Pk on their domains. So P belongs toM(unk=1Pk).\nConversely, assume that P belongs toM(unk=1Pk), i.e., it dominates unk=1Pk. Since unk=1Pk dominates all of the\nPk on their domains, so does P , and consequently P is an element of\n\u22c2n\nk=1M(Pk). \u0003\nTheorem 5. Assume that the domain Kk of the coherent lower prevision Pk is a linear space for each k \u2208 {1, . . . , n}.\nConsider the map E assigning to each gamble Z on \u2126 the (possibly infinite) real number\n(10) E (Z) = sup\nXk\u2208L (\u2126)\n{\n\u03b1 \u2208 R : Z \u2212 \u03b1 \u2265\nn\u2211\nk=1\n[Xk \u2212 Pk(Xk)]\n}\n,\nIf E (Z) = +\u221e for some Z (and hence for all Z), then the lower previsions (Pk)nk=1 are inconsistent. Otherwise, the\nconjunction unk=1Pk coincides with E on all gambles on \u2126.\nProof. We first show that E dominates Pk on Kk, for all k \u2208 {1, . . . , n}. To see this, let Z \u2208 Kk and let Xk = Z and\nX` = 0 for all ` \u2208 {1, . . . , n} \\ {k} in Eq. (10).\nNext, let F be another coherent lower prevision that dominates each Pk on its domainKk. Then we show that F also\ndominates E . Indeed, for any gamble Z on \u2126,\nE (Z) = sup\nXk\u2208Kk\n{\n\u03b1 \u2208 R : Z \u2212 \u03b1 \u2265\nn\u2211\nk=1\n[Xk \u2212 Pk(Xk)]\n}\n\u2264 sup\nXk\u2208Kk\n{\n\u03b1 \u2208 R : \u03b1 \u2264 F\n(\nZ \u2212\nn\u2211\nk=1\n[Xk \u2212 Pk(Xk)]\n)}\n\u2264 sup\nXk\u2208Kk\n[\nF (Z)\u2212\nn\u2211\nk=1\nF (Xk \u2212 Pk(Xk))\n]\n\u2264 F (Z),\nwhere we used the coherence of F and the fact that F (Xk) \u2265 P(Xk) for all Xk \u2208 Kk.\nIt is now easily checked that provided that E is everywhere finite, it is a coherent lower prevision, and therefore\ncoincides with the conjunction.\nTo complete the proof, assume first that the conjunction exists. Denote this conjunction by F . Then F is coherent and\ndominates each P i. But then F must dominate E too, as we already showed before. Hence, if the conjunction exists,\nE (Z ) \u2264 F (Z ) 6= +\u221e for all gambles Z on \u2126, or equivalently, if E (Z ) = +\u221e for some gamble Z on \u2126 then the\nconjunction does not exist.\nFinally, if E (Z ) = +\u221e for some gamble Z on \u2126, then it must hold that\n(\u2200\u03b1 \u2208 R)(\u2203Xk \u2208 Kk)\n(\nZ \u2212 \u03b1 \u2265\nn\u2211\nk=1\n[Xk \u2212 Pk(Xk)]\n)\n.\n13\nNow, observe that since gambles are is bounded, this condition is equivalent to\n(\u2200\u03b1 \u2208 R)(\u2203Xk \u2208 Kk)\n(\nY \u2212 \u03b1 \u2265\nn\u2211\nk=1\n[Xk \u2212 Pk(Xk)]\n)\n,\nfor any other gamble Y on \u2126. Hence, it holds indeed that E (Z) = +\u221e for some gamble Z if and only if E (Y ) = +\u221e\nfor any gamble Y . \u0003\nIf the consistent coherent lower previsions Pk have a common domainK then the conjunction E can also be derived\nas the natural extension toL (A \u00d7B) of the lower prevision PC defined onK by\nPC(X) =\nn\nmax\nk=1\nPk(X), for all X \u2208 K .\nObserve that the Pk are consistent if and only if PC avoids sure loss.\nLet us now take a closer look at a number of interesting special cases.\n6.2. Conjunction of vacuous lower previsions. If the modeller trusts all the experts\u2019 assessments \u2018\u03c9 \u2208 T1\u2019, . . . , \u2018\u03c9 \u2208\nTn\u2019 then she should at least conclude that \u2018\u03c9 \u2208\n\u22c2\ni\u2208I Ti\u2019. This is the essence of the following proposition.\nProposition 2. Consider the non-empty subsets Tk, k = 1, . . . , n of \u2126, and the associated vacuous lower previsions PTk\non L (\u2126). Let T =\n\u22c2n\nk=1 Tk. Then the PTk are consistent if and only if T 6= \u2205. If the PTk are consistent, then their\nconjunction is equal to the vacuous lower prevision PT on \u2126 relative to the intersection T .\nProof. First, assume that T = \u2205. Then the PTk cannot be consistent. Indeed, assume that there is a coherent lower\nprevision R that point-wise dominates them. Since\n\u22c3\ni\u2208I {Ti = \u2126, it follows from the coherence of R that\n\u22121 = R(\u22121) = R\n(\n\u2212\n\u2211\ni\u2208I\nI{Ti\n)\n\u2265\n\u2211\ni\u2208I\nR\n(\u2212I{Ti) \u2265\u2211\ni\u2208I\nPTi\n(\u2212I{Ti) = 0,\na contradiction. This means that the PTk are inconsistent.\nConversely, assume that T 6= \u2205. Then PT is a coherent lower prevision, and it point-wise dominates all the PTk : for\neach k and each X inL (\u2126),\nPT (X) = inf\n\u03c9\u2208T\nX(\u03c9) \u2265 inf\n\u03c9\u2208Tk\nX(\u03c9) = PTk(X).\nThis means that the PTk are consistent. Now let R be any coherent lower prevision on \u2126 that dominates all the PTk . It\nnow only remains to show that R \u2265 PT . Indeed, let X be any gamble. Then it is always possible to find \u03bbk \u2265 0 such that\ninf\n[\nX +\nn\u2211\nk=1\n\u03bbk {Tk\n]\n= min\n{\ninf\n\u03c9\u2208T\nX(\u03c9),\nn\nmin\nk=1\n[\n\u03bbk + inf\n\u03c9\u2208{Tk\nX(\u03c9)\n]}\n= inf\n\u03c9\u2208T\nX(\u03c9) = PT (X).\nConsequently, taking into account the coherence of R, and the fact that R\n(\u2212I{Tk) \u2265 PTk (\u2212I{Tk) = 0, whence also\nR\n(\u2212\u2211nk=1 \u03bbkI{Tk) \u2265 0, and consequently\nR(X) \u2265 R(X)\u2212 R\n(\n\u2212\nn\u2211\nk=1\n\u03bbkI{Tk\n)\n= R(X) + R\n(\nn\u2211\nk=1\n\u03bbkI{Tk\n)\n\u2265 R\n(\nX +\nn\u2211\nk=1\n\u03bbkI{Tk\n)\n\u2265 inf\n[\nX +\nn\u2211\nk=1\n\u03bbkI{Tk\n]\n= PT (X).\nThus PT is indeed the smallest coherent lower prevision that point-wise dominates all of the Pk. \u0003\nProposition 3. Let P be any coherent lower prevision with domain K and let P\u2126 the vacuous lower prevision relative\nto \u2126. Then P u P\u2126 = E , where E is the natural extension of P toL (\u2126).\nProof. The proof is immediate if we recall that any coherent lower prevision P point-wise dominates P\u2126 on its domain\nK . \u0003\n6.3. Products of conjunctions. The conjunction distributes over the two independent products we have defined previ-\nously. So, using conjunction to combine expert assessments on common domains, and independent natural extension\nor type-I product to combine expert assessments on different domains, the order of combination invariance principle is\nsatisfied.\nProposition 4. For each k = 1,. . . ,n and each ` = 1,. . . ,m, let Pk be a coherent lower prevision defined on the\nlinear subspace Fk of L (A ) and let Q` be a coherent lower prevision defined on the linear subspace G` of L (B).\nThen consistency of (Pk \u00d7INEQ`)k,` is equivalent to consistency of both (Pk)nk=1 and (Q`)m`=1, and in such a case the\nfollowing equality holds:\n(11) uk,`\n(\nPk \u00d7INEQ`\n)\n=\n(\nunk=1Pk\n)\n\u00d7INE\n(\num`=1Q`\n)\n.\n14\nProof. Let us, for convenience, define the inconsistent lower prevision on \u2126 as I (Z) = +\u221e for all gambles Z on \u2126.\nIn this way, the conjunction is always defined and the formula for conjunction, Eq. (10), always holds, whether the\nassessments are consistent or not (if they are not, it is equal to I ). Also let the independent natural extension be the\ninconsistent lower prevision whenever at least one of the factors is the inconsistent lower prevision. Then both sides of\nEq. (11) are well defined and if we can prove equality, equivalence of consistency follows naturally.\nLet P denote the conjunction of (Pk)\nn\nk=1 (i.e., the usual conjunction if they are consistent, and +\u221e otherwise).\nSimilarly, let Q denote the conjunction of (Q\n`\n)n`=1.\nLet Z be any gamble on \u2126 = A \u00d7B. Using the definition of independent natural extension (Eq. (6)), we have that\n(12)\n(\nP \u00d7INEQ\n)\n(Z) = sup\nX ,Y\u2208L (\u2126)\n{\n\u03b3 \u2208 R : Z \u2212 \u03b3 \u2265 X \u2212 P(X ) +Y \u2212Q(Y )}\nNote that this equation holds whether both (Pk)\nn\nk=1 and (Q`)\nm\n`=1 are consistent or not because the right hand side is +\u221e\n(independently of Z) in case of inconsistency, conforming with our definition of the inconsistent lower prevision.\nFrom the formula for conjunction (Eq. (10)), we know that, for any X \u2208 L (\u2126), and any \u03b2 \u2208 B,\nP(X(\u00b7, \u03b2)) = sup\nXk\u2208Fk\n{\n\u03b7 \u2208 R : X(\u00b7, \u03b2)\u2212 \u03b7 \u2265\nn\u2211\nk=1\n[Xk \u2212 Pk(Xk)]\n}\n(13)\nAgain, this holds whether the (Pk)\nn\nk=1 are consistent or not. We now consider two cases.\n(1) If both (Pk)\nn\nk=1 and (Q`)\nm\n`=1 are consistent, it is instructive to rewrite Eq. (13) as follows:\n(14)\n(\u2200X \u2208 L (\u2126))(\u2200\u03b2 \u2208 B)(\u2200\u000f > 0)(\u2200k)(\u2203Uk,X,\u03b2,\u000f \u2208 Fk)\n(\nX(\u00b7, \u03b2)\u2212 P(X(\u00b7, \u03b2)) + \u000f \u2265\nn\u2211\nk=1\n[Uk,X,\u03b2,\u000f \u2212 Pk(Uk,X,\u03b2,\u000f)]\n)\nUsing (14), and a similar expression for Q(Y (a, \u00b7)), we find that\n(15) (\u2203X,Y \u2208 L (\u2126))(Z \u2212 \u03b3 \u2265 X \u2212 P(X) + Y \u2212Q(Y ))\nimplies that\n(16) (\u2200\u000f > 0)(\u2203Xk \u2208 F k)(\u2203Y` \u2208 G `)\n(\nZ \u2212 \u03b3 \u2265\nn\u2211\nk=1\n[Xk \u2212 Pk(Xk)]\u2212 \u000f+\nm\u2211\n`=1\n[\nY` \u2212Q`(Y`)\n]\n\u2212 \u000f\n)\n,\nfor any gamble Z on \u2126 and any \u03b3 \u2208 R. From this implication, (15) =\u21d2 (16), we may infer that the right hand\nside of (12) is less than or equal to\n(17) 2\u000f+ sup\nXk\u2208Fk\nY`\u2208G `\n{\n\u03b3 \u2208 R : Z \u2212 \u03b3 \u2265\nn\u2211\nk=1\n[Xk \u2212 Pk(Xk)] +\nm\u2211\n`=1\n[\nY` \u2212Q`(Y`)\n]}\n,\nfor every \u000f > 0. Now also observe that\n(18) (\u2203Xk \u2208 F k)(\u2203Y` \u2208 G `)\n(\nZ \u2212 \u03b3 \u2265\nn\u2211\nk=1\n[Xk \u2212 Pk(Xk)] +\nm\u2211\n`=1\n[\nY` \u2212Q`(Y`)\n])\nimplies (15). Indeed, take X =\n\u2211n\nk=1Xk and Y =\n\u2211n\nk=1 Y`, and use the fact that P \u2265 Pk onF k and Q \u2265 Q`\non G `, e.g.:\nn\u2211\nk=1\n[Xk \u2212 Pk(Xk)] = X \u2212\nn\u2211\nk=1\nPk(Xk) \u2265 X \u2212\nn\u2211\nk=1\nP(Xk)\n\u2265 X \u2212 P\n(\nn\u2211\nk=1\nXk\n)\n= X \u2212 P(X).\nFrom this implication, (18) =\u21d2 (15), we may infer that the right hand side of (12) is greater or equal to\n(19) sup\nXk\u2208Fk\nY`\u2208G `\n{\n\u03b3 \u2208 R : Z \u2212 \u03b3 \u2265\nn\u2211\nk=1\n[Xk \u2212 Pk(Xk)] +\nm\u2211\n`=1\n[\nY` \u2212Q`(Y`)\n]}\n.\nThus, since that the right hand side of (12) is less or equal to (17) for every \u000f > 0, and greater or equal to (19),\nwe must conclude that it is equal to (19).\n(2) In case that the (Pk)\nn\nk=1 are inconsistent, we may rewrite Eq. (13) as\n(\u2200\u03b2 \u2208 B)(\u2200\u03b7 \u2208 R)(\u2200k)(\u2203Uk,X,\u03b2,\u03b7 \u2208 Fk)\n(\nZ(\u00b7, \u03b2)\u2212 \u03b7 \u2265\nn\u2211\nk=1\n[Uk,X,\u03b2,\u03b7 \u2212 Pk(Uk,X,\u03b2,\u03b7)]\n)\n.\nIn particular, taking Xk(\u00b7, \u03b2) = Uk,X,\u03b2,\u03b7 and Y` = 0, we find that\n(\u2200\u03b3 \u2208 R)(\u2200k)(\u2203Xk \u2208 F k)(\u2200`)(\u2203Y` \u2208 G `)\n(\nZ \u2212 \u03b3 \u2265\nn\u2211\nk=1\n[Xk \u2212 Pk(Xk)] +\nm\u2211\n`=1\n[\nY` \u2212Q`(Y`)\n])\n.\n15\nHence, in this case, (19) is equal to +\u221e, which is also equal to the right hand side of (12).\n(3) A similar argument shows that the equality also holds if the (Q\n`\n)m`=1 are inconsistent.\nHence, we showed that, in all cases, (19) is equal to the right hand side of (12)\nNow, by the definition of independent natural extension, equation (6), it holds that(\nPk \u00d7INEQ`\n)\n(Zk`) = sup\nX\u2208Fk\nY \u2208G `\n{\n\u03b3k` \u2208 R : Zk` \u2212 \u03b3k` \u2265 X \u2212 Pk(X) + Y \u2212Q`(Y )\n}\n.\nfor any gamble Zk` on \u2126. It is instructive to rewrite this equality as follows:\n(20) (\u2200\u000f > 0)(\u2203U\u000f \u2208 F k)(\u2203V\u000f \u2208 G `)\n(\nZk` \u2212\n(\nPk \u00d7INEQ`\n)\n(Zk`) + \u000f \u2265 U\u000f \u2212 Pk(U\u000f) + V\u000f \u2212Q`(V\u000f)\n)\n.\nUsing (20) we find that\n(21) (\u2203Zk` \u2208 L (\u2126))\n\uf8eb\uf8edZ \u2212 \u03b3 \u2265\u2211\nk,`\n[\nZk` \u2212\n(\nPk \u00d7INEQ`\n)\n(Zk`)\n]\uf8f6\uf8f8\nimplies that\n(\u2200\u000f > 0)(\u2203Xk` \u2208 F k)(\u2203Yk` \u2208 G `)\n\uf8eb\uf8edZ \u2212 \u03b3 \u2265\u2211\nk,`\n[\nXk` \u2212 Pk(Xk`) + Yk` \u2212Q`(Yk`)\u2212 \u000f\n]\uf8f6\uf8f8 ,\nwhich implies that\n(22) (\u2200\u000f > 0)(\u2203Xk \u2208 F k)(\u2203Yk \u2208 G `)\n(\nZ \u2212 \u03b3 + nm\u000f \u2265\nn\u2211\nk=1\n[Xk \u2212 Pk(Xk)] +\nm\u2211\n`=1\n[\nY` \u2212Q`(Y`)\n])\n.\nIndeed, take Xk =\n\u2211m\n`=1Xk` and Y` =\n\u2211n\nk=1 Yk`, and use the fact that, e.g.,\nm\u2211\n`=1\n[Xk` \u2212 Pk(Xk`)] = Xk \u2212\nm\u2211\n`=1\nPk(Xk`) \u2265 Xk \u2212 Pk\n(\nm\u2211\n`=1\nXk`\n)\n= Xk \u2212 Pk(Xk)\nNow, the implication (21) =\u21d2 (22) implies that (19) is greater or equal to\n(23) \u2212 nm\u000f+ sup\nZk`\u2208L (\u2126)\n\uf8f1\uf8f2\uf8f3\u03b3 \u2208 R : Z \u2212 \u03b3 \u2265\u2211\nk,`\n[\nZk` \u2212\n(\nPk \u00d7INEQ`\n)\n(Zk`)\n]\uf8fc\uf8fd\uf8fe\nfor any \u000f > 0. Also\n(24) (\u2203Xk \u2208 F k)(\u2203Y` \u2208 G `)\n(\nZ \u2212 \u03b3 \u2265\nn\u2211\nk=1\n[Xk \u2212 Pk(Xk)] +\nm\u2211\n`=1\n[\nY` \u2212Q`(Y`)\n])\nimplies (21). Indeed, taking Zk` = 1m (Xk \u2212 Pk(Xk)) + 1n\n(\nY` \u2212Q`(Y`)\n)\nwe find that(\nPk \u00d7INEQ`\n)\n(Zk`) \u2265 1\nm\n(\nPk \u00d7INEQ`\n)\n(Xk \u2212 Pk(Xk)) +\n1\nn\n(\nPk \u00d7INEQ`\n)\n(Y` \u2212Q`(Y`)) \u2265 0.\nThe last inequality follows from the fact that, e.g.,(\nPk \u00d7INEQ`\n)\n(Xk \u2212 Pk(Xk)) = sup\nX\u2208Fk\nY \u2208G `\n{\n\u03b7 \u2208 R : Xk \u2212 Pk(Xk)\u2212 \u03b7 \u2265 X \u2212 Pk(X) + Y \u2212Q`(Y )\n}\n\u2265 sup {\u03b7 \u2208 R : Xk \u2212 Pk(Xk)\u2212 \u03b7 \u2265 Xk \u2212 Pk(Xk)} = 0.\nHence, for this choice of Zk` we have that\nn\u2211\nk=1\n[Xk \u2212 Pk(Xk)] +\nm\u2211\n`=1\n[\nY` \u2212Q`(Y`)\n]\n=\n\u2211\nk,`\nZkl \u2265\n\u2211\nk,`\n[\nZk` \u2212\n(\nPk \u00d7INEQ`\n)\n(Zk`)\n]\n,\nwhich establishes (24) =\u21d2 (21). This implication implies that (19) is less or equal to\n(25) sup\nZk`\u2208L (\u2126)\n\uf8f1\uf8f2\uf8f3\u03b3 \u2208 R : Z \u2212 \u03b3 \u2265\u2211\nk,`\n[\nZk` \u2212\n(\nPk \u00d7INEQ`\n)\n(Zk`)\n]\uf8fc\uf8fd\uf8fe\nThus, if (19) is greater or equal to (23) for all \u000f > 0 and less or equal to (25), then it must be equal to (25). But (25) is\nexactly equal to uk,`\n(\nPk \u00d7INEQ`\n)\n(Z), no matter whether the (Pk \u00d7INEQ`)k,` are consistent or not. \u0003\n16\nProposition 5. Let Pk be a coherent lower prevision defined on a linear subspace Fk of L (A ), k = 1, . . . , n, and let\nQ\n`\nbe a coherent lower previsions defined on a linear subspace G` of L (B), ` = 1, . . . ,m. Then the (Pk \u00d7TIQ`)k,`\nare consistent if and only if both the (Pk)\nn\nk=1 and (Q`)\nm\n`=1 are, and in such a case, the following equality holds:\n(26) uk,`\n(\nPk \u00d7TIQ`\n)\n= (unk=1Pk)\u00d7TI\n(\num`=1Q`\n)\n.\nProof. The proof is immediate if we recall that(\nn\u22c2\nk=1\nM(Pk)\n)\n\u00d7\n(\nm\u22c2\n`=1\nM(Q\n`\n)\n)\n=\nn\u22c2\nk=1\nm\u22c2\n`=1\n[\nM(Pk)\u00d7M(Q`)\n]\n.\nThe equality holds too whenever either side is equal to the empty set; this establishes equivalence of consistency. \u0003\n6.4. Combination of conflicting assessments. If the assessments (Pk)nk=1 are conflicting\u2014if there exists no conjunction\u2014\nthen there is no coherent way to accept every decision of every expert, since the modeller incurs a sure loss if she would do\nso: using Theorem 5 it is easily established that in case of inconsistency there are gambles Xk \u2208 Kk such that (compare\nwith Eq. (1))\n(27) sup\n\u03c9\u2208\u2126\n[\nn\u2211\nk=1\n[Xk(\u03c9)\u2212 Pk(Xk)]\n]\n< 0,\ni.e., the combination of the transactions in which the gambles Xk are bought for a price Pk(Xk) leads to a loss, whatever\nthe actual value of the parameter a. Blindly accepting decisions of all the experts (Pk)\nn\nk=1 is clearly unacceptable in case\nof inconsistency.\nThis issue can be resolved using hierarchical models. In order to avoid sure loss, this procedure must somehow involve\nthe weakening of some of the decisions of at least some of the experts. This can be done for instance by only accepting\nparticular decisions at some prescribed rate r strictly less than one. But we do not know how much the experts\u2019 decisions\ncan be weakened.\nWe propose a simpler and more straightforward method, based on the principle that there should be at least one of the\nexperts that provides a reasonable model. Briefly, in case of inconsistency the modeller is certain that some of the experts\u2019\nassessments (Pk)\nn\nk=1 cannot be trusted, but she does not necessarily know which ones. Therefore, instead of looking for\nan aggregate that implies trust to all experts\u2014a conjunction\u2014we now look for an aggregate that is trusted by all experts.\nThis is reasonable if we assume that at least one of the experts has a reasonable model.\nSay that an expert trusts the modeller whenever he is willing to accept every decision she makes, that is, whenever he\nis willing to accept her price s for buying X as his price for buying X , and this for each gamble X on \u2126 and each buying\nprice s < PM (X). We have the following result.\nTheorem 6. The modeller is trusted by an expert if and only if the natural extension Ek of his assessment Pk point-wise\ndominates her coherent lower prevision PM onL (\u2126).\nWe need to consider the natural extension Ek because Pk may not be defined on all gambles, which may complicate\ncomparison with PM .\nNow define the disjunction of (Pk)\nn\nk=1 as the largest, or least conservative, coherent lower prevision that the modeller\ncan have, such that all of the experts still agree with her decisions. By Theorem 6, this is the (point-wise) largest coherent\nlower prevision that is dominated by all of the experts\u2019 natural extensions (Ek)\nn\nk=1. This way of combining lower\nprevisions is also called the unanimity rule [14, Section 4.3.9].\nIt is easy to see that the disjunction always exists (see Theorem 7 below). We shall denote the disjunction of n\ncoherent lower previsions (Pk)\nn\nk=1 by unionsqnk=1Pk, and the disjunction of two coherent lower lower previsions P1 and P2\nby P1 unionsq P2. By the next theorem, the disjunction is an associative and commutative operator, and hence, it satisfies the\norder of combination invariance principle.\nTheorem 7. Let Ek be the natural extension of the coherent lower prevision Pk, and letM(Pk) be its set of dominating\nlinear previsions, for each k \u2208 {1, . . . , n}. Then for each gamble Z on \u2126:\n(28) (unionsqnk=1Pk) (Z) =\nn\nmin\nk=1\nEk(Z) = inf\n{\nP(Z) : P \u2208\nn\u22c3\nk=1\nM(Pk)\n}\n.\nProof. For notational convenience, let E = minnk=1 Ek. It is obvious that E is the largest functional on L (\u2126) that is\npoint-wise dominated by all the Ek. Since it is a point-wise minimum of coherent lower previsions, E is a coherent lower\nprevision, which proves the first equality. The proof of the second equality is now immediate: for each gamble Z on \u2126,\ninf\n{\nP(Z) : P \u2208\nn\u22c3\nk=1\nM(Pk)\n}\n= inf\nn\u22c3\nk=1\n{P(Z) : P \u2208M(Pk)} =\nn\nmin\nk=1\ninf {P(Z) : P \u2208M(Pk)} =\nn\nmin\nk=1\nEk(Z). \u0003\nObviously, if the modeller only accepts those decisions which are supported by all the experts, her model is going to\nbe at most as precise as the least precise expert only. Disjunction, in contradistinction to conjunction, aims at reconciling\nall the experts\u2019 assessments. Therefore, if the assessments are highly conflicting, any reconciliation of them will be highly\nimprecise. This is illustrated by Eq. (28). We therefore suggest the following general strategy:\n17\n(i) if the experts are consistent, the modeller takes their conjunction unk=1Pk as her coherent lower prevision,\n(ii) if they are not, this points to the fact that the modeller cannot trust all of the experts, so she should try to find out\nwhat experts she really can trust, and then discard those experts which she does not trust,\n(iii) if the remaining experts, say (Pk)\nn\u2032\nk=1 with n\n\u2032 \u2264 n, are consistent, she takes their conjunction un\u2032k=1Pk as her\ncoherent lower prevision,\n(iv) if there is still inconsistency, she reconciles the remaining experts by taking the disjunction unionsqn\u2032k=1Pk of the remaining\nassessments as her coherent lower prevision.\nDiscarding experts will lead to less precise results in case of consistency (smaller conjunction), but may increase precision\nin case of inconsistency (larger disjunction). There is no unique solution in case of inconsistency: everything depends on\nhow much information is available about the reliability of the given information.\n6.5. Products of disjunctions. The disjunction also distributes over the two independent products we have introduced.\nHence, we obtain order of combination invariance for disjunction and independent natural extension, and disjunction and\nthe type-I product.\nProposition 6. For all k = 1, . . . , n, let Pk be a coherent lower prevision defined on the linear subspace Fk of L (A )\ncontaining all constant gambles, and for all ` = 1, . . . ,m, let Q\n`\nbe a coherent lower prevision defined on the linear\nsubspace G` ofL (B) containing all constant gambles. Then the following equality holds:\n(29) unionsqk,`\n(\nPk \u00d7INEQ`\n)\n=\n(\nunionsqnk=1Pk\n)\n\u00d7INE\n(\nunionsqm`=1Q`\n)\n.\nProof. Consider a gamble Z on A \u00d7B. Using Eqs. (28) and (6) we find that(\nunionsqnk=1Pk\n)\n\u00d7INE\n(\nunionsqm`=1Q`\n)\n(Z) = sup\nX,Y \u2208L (A\u00d7B)\n{\n\u03b3 : Z \u2212 \u03b3 \u2265 X \u2212\n(\nn\nmin\nk=1\nEk\n)\n(X) + Y \u2212\n(\nm\nmin\n`=1\nF `\n)\n(Y )\n}\n,\nwhere Ek is the natural extension of Pk and F ` is the natural extension of Q`. Now observe that\n(30) Z \u2212 \u03b3 \u2265 X \u2212\n(\nn\nmin\nk=1\nEk\n)\n(X) + Y \u2212\n(\nm\nmin\n`=1\nF `\n)\n(Y )\nis equivalent to\n(\u2200(\u03b1, \u03b2) \u2208 A \u00d7B)\n(\nZ(\u03b1, \u03b2)\u2212 \u03b3 \u2265 X(\u03b1, \u03b2)\u2212\nn\nmin\nk=1\nEk(X(\u00b7, \u03b2)) + Y (\u03b1, \u03b2)\u2212\nm\nmin\n`=1\nF `(Y (\u03b1, \u00b7))\n)\n,\nwhich is also equivalent to\n(\u2200(\u03b1, \u03b2) \u2208 A \u00d7B) (\u2200k, `) (Z(\u03b1, \u03b2)\u2212 \u03b3 \u2265 X(\u03b1, \u03b2)\u2212 Ek(X(\u00b7, \u03b2)) + Y (\u03b1, \u03b2)\u2212 F `(Y (\u03b1, \u00b7))) ,\nwhich can also be written as\n(31) (\u2200k, `) (Z \u2212 \u03b3 \u2265 X \u2212 Ek(X) + Y \u2212 F k(Y )) .\nUsing the equivalence of (30) and (31), we find that(\nunionsqnk=1Pk\n)\n\u00d7INE\n(\nunionsqm`=1Q`\n)\n(Z) = sup\nX,Y \u2208L (\u2126)\n{\u03b3 : (\u2200k, `) (Z \u2212 \u03b3 \u2265 X \u2212 Ek(X) + Y \u2212 F `(Y ))} .\nNext, observe that, since Pk(X) = Ek(X) for X \u2208 F k, and similarly for Q`,\n(32) (\u2200k, `) (\u2203Xk \u2208 F k, Y` \u2208 G `) (Z \u2212 \u03b3 \u2265 Xk \u2212 Pk(Xk) + Y` \u2212Q`(Y`))\nimplies that\n(33) (\u2203X,Y \u2208 L (A \u00d7B)) (\u2200k, `) (Z \u2212 \u03b3 \u2265 X \u2212 Ek(X) + Y \u2212 F `(Y )) .\nIndeed, let X = 1n+m\n\u2211n\nk=1Xk and Y =\n1\nn+m\n\u2211m\n`=1 Y`, and observe that for this choice\nEk(X) = Pk(X) \u2265\n1\nn+m\nn\u2211\nk=1\nPk(Xk) and F `(Y ) = Q`(Y ) \u2265\n1\nn+m\nm\u2211\n`=1\nQ\n`\n(Y`).\nfor all k and `. Thus, if (32) is satisfied then\nZ \u2212 \u03b3 \u2265\n\u2211\nk,`\n[Xk \u2212 Pk(X) + Y \u2212 P `(Y )] \u2265 X \u2212 Ek(X) + Y \u2212 F `(Y ),\nfor all k and `, which means that (33) must hold too. Consequently,\n(34)(\nunionsqnk=1Pk\n)\n\u00d7INE\n(\nunionsqm`=1Q`\n)\n(Z) \u2265 sup\n{\n\u03b3 : (\u2200k, `) (\u2203Xk \u2208 F k, Y` \u2208 G `) (Z \u2212 \u03b3 \u2265 Xk \u2212 Pk(Xk) + Y` \u2212Q`(Y`))}.\nTo prove the converse inequality, we recall that (see [14, Section 3.1.4])\nEk(X(\u00b7, \u03b2)) = sup\nU\u2208Fk\nU\u2264X(\u00b7,\u03b2)\nPk(U(\u00b7, \u03b2)) = sup\nU\u2208Fk\nU\u2264X\nPk(U(\u00b7, \u03b2))\n18\nand similarly\nF `(Y (\u03b1, \u00b7)) = sup\nV \u2208G`\nV\u2264Y (\u03b1,\u00b7)\nQ\n`\n(V (\u03b1, \u00b7)) = sup\nV \u2208G `\nV\u2264Y\nQ\n`\n(V (\u03b1, \u00b7)).\nConsequently,\n(\u2200\u000f > 0)(\u2200k, `)(\u2200X,Y \u2208 L (A \u00d7B)) (\u2203U\u000f,k,X \u2208 F k) (\u2203V\u000f,`,Y \u2208 G `)(\nU\u000f,k,X \u2264 X , Pk(U\u000f,k,X) + \u000f \u2265 Ek(X), V\u000f,`,Y \u2264 Y and Q`(V\u000f,`,Y ) + \u000f \u2265 F `(Y )\n)\n.\nBut this means that (33) implies that\n(\u2200\u000f > 0)(\u2200k, `) (\u2203Xk \u2208 F k) (\u2203Y` \u2208 G `) (Z \u2212 \u03b3 \u2265 Xk \u2212 Pk(Xk)\u2212 \u000f+ Y` \u2212Q`(Y`)\u2212 \u000f)\n[identify Xk with U\u000f,k,X and Y` with V\u000f,`,Y ]. From this implication we infer that(\nunionsqnk=1Pk\n)\n\u00d7INE\n(\nunionsqm`=1Q`\n)\n(Z) \u2264 2\u000f+sup\n{\n\u03b3 : (\u2200k, `) (\u2203Xk \u2208 F k) (\u2203Y` \u2208 G `) (Z \u2212 \u03b3 \u2265 Xk \u2212 Pk(Xk) + Y` \u2212Q`(Y`))}\nfor all \u000f > 0, and hence also for \u000f = 0. We may thus infer that(\nunionsqnk=1Pk\n)\n\u00d7INE\n(\nunionsqm`=1Q`\n)\n(Z) = sup\n{\n\u03b3 : (\u2200k, `) (\u2203Xk \u2208 F k) (\u2203Y` \u2208 G `) (Z \u2212 \u03b1 \u2265 Xk \u2212 Pk(Xk) + Y` \u2212Q`(Y`))}.\nIf we now define the Ak,` as the subsets{\n\u03b1 \u2208 R : (\u2203Xk \u2208 F k) (\u2203Y` \u2208 G `) (Z \u2212 \u03b1 \u2265 Xk \u2212 Pk(Xk) + Y` \u2212Q`(Y`))}\nof the reals, then we can rewrite this as(\nunionsqnk=1Pk\n)\n\u00d7INE\n(\nunionsqm`=1Q`\n)\n(Z) = sup\n\u22c2\nk,`\nAk,`.\nSince the Ak,` are down-sets [if \u03b11 \u2208 Ak,` and \u03b12 \u2264 \u03b11 then also \u03b12 \u2208 Ak,`], it follows that\nsup\n\u22c2\nk,`\nAk,` = min\nk,`\nsupAk,`.\nNow observe that the right hand side is equal to\n(\nunionsqk,`\n(\nPk \u00d7INEQ`\n))\n(Z). \u0003\nProposition 7. Let Pk be a coherent lower prevision defined on the linear subspaceFk ofL (A ), k = 1, . . . , n, and let\nQ\n`\nbe a coherent lower prevision defined on the linear subspace G` ofL (B), ` = 1, . . . ,m. Then the following equality\nholds:\n(35) unionsqk,`\n(\nPk \u00d7TIQ`\n)\n= (unionsqnk=1Pk)\u00d7TI\n(\nunionsqm`=1Q`\n)\n.\nProof. The proof is an immediate consequence of Theorem 7 if we recall that(\nn\u22c3\nk=1\nM(Pk)\n)\n\u00d7\n(\nm\u22c3\n`=1\nM(Q\n`\n)\n)\n=\nn\u22c3\nk=1\nm\u22c3\n`=1\n[\nM(Pk)\u00d7M(Q`)\n]\n. \u0003\n7. SOLUTIONS TO PROBLEMS 2, 3 AND 5\nThe additional difficulty in Problems 2, 3 and 5 is that there are a number of experts, or sources, giving information\nabout the parameters a and b. In the formulation of the problem set [11], nothing is said about how these experts are related:\nan expert for a and an expert for b might be same person, or they may be different people. We take the independence\nassumption in [11] stating that \u201cknowledge about the value of one parameter implies nothing about the value of the other\u201d\nto refer to all the experts: their assessments about the values of one parameter, are in no way influenced by (their own or\nother experts\u2019) assessments about the values of the other parameter. This means that for each a-expert and each b-expert,\nwe can take an independent product of the lower previsions modelling their assessments, and then combine these products\nusing conjunction, or if there is inconsistency, using disjunction. But the results in Propositions 4, 5, 6 and 7 tell us that\nwe get the same result if we first combine, for each parameter, the information about its value coming from the different\nexperts, and then take an independent product: our approach satisfies the order of combination invariance principle.\n7.1. Solution to Problem 2. The argument in Section 2.3 tells us that the expert assessment A = [a1, a2] is described by\nthe coherent vacuous lower prevision PA defined onL (A ), and that the expert assessments Bj = [b\nj\n1, b\nj\n2] are described\nby the coherent vacuous lower previsions PBj defined onL (B) (j = 1, . . . , 4). Let us first look at the case that the PBj\nare consistent, or equivalently, by Proposition 2, that\n\u22c24\nj=1Bj 6= \u2205. We then find for the lower prevision P modelling the\navailable information about the pair (a, b) that, again using Proposition 2 and also Propositions 1, 4 and 5:\nP = u4j=1\n(\nPA\u00d7INE PBj\n)\n= u4j=1\n(\nPA\u00d7TI PBj\n)\n= PA\u00d7INE\n(\nu4j=1PBj\n)\n= PA\u00d7TI\n(\nu4j=1PBj\n)\n= PA\u00d7(T4j=1 Bj).\nThe available information about the output y is then represented by the coherent lower prevision Py , where\nPy(U) = inf\n\u03b1\u2208A\ninf\n\u03b2\u2208T4j=1 Bj U(f(\u03b1, \u03b2)),\n19\nfor all gambles U on Y . As an illustration, we can calculate the lower prevision P(f) and the upper prevision P(f) of\nthe system output y, for the given subproblems 2a and 2b.\nProblem 2a. With A\u00d7 \u22294j=1Bj = [0.1, 1.0]\u00d7 [0.6, 0.8], we find that\nP(f) = 0.956196 and P(f) = 1.8.\nProblem 2b. With A\u00d7 \u22294j=1Bj = [0.1, 1.0]\u00d7 [0.6, 0.7], we find that\nP(f) = 0.956196 and P(f) = 1.7.\nIn Problem 2c, the lower previsions PB1 , . . . , PB4 on L (B) are inconsistent, as B1 \u2229 B2 \u2229 B3 = \u2205. Applying\ndisjunction, this leads to the following lower prevision P , using Propositions 1, 6 and 7,\nP = unionsq4j=1\n(\nPA\u00d7INE PBj\n)\n= unionsq4j=1\n(\nPA\u00d7TI PBj\n)\n= PA\u00d7INE\n(\nunionsq4j=1PBj\n)\n= PA\u00d7TI\n(\nunionsq4j=1PBj\n)\n= PA\u00d7(S4j=1 Bj).\nThe information about the output y is now represented by the coherent lower prevision Py , where\nPy(U) = inf\n\u03b1\u2208A\ninf\n\u03b2\u2208S4j=1 Bj U(f(\u03b1, \u03b2)),\nfor all gambles U on Y . As an illustration, we can calculate the lower prevision P(f) and the upper prevision P(f) of\nthe system output y.\nProblem 2c. With A\u00d7\u22c34j=1Bj = [0.1, 1.0]\u00d7 [0.0, 1.0], we find that\nP(f) = 0.692201 and P(f) = 2.\n7.2. Solution to Problem 3. Using the results of Section 2.3, the expert assessments Ai = [ai1, ai2] are described by the\ncoherent vacuous lower previsions PAi defined on L (A ) (i = 1, . . . , 3), and the expert assessments Bj = [b\nj\n1, b\nj\n2] by\nthe coherent vacuous lower previsions PBj defined onL (B) (j = 1, . . . , 4). We first consider the case that the PAi and\nPBj are consistent, or equivalently, by Proposition 2, that\n\u22c23\ni=1Ai 6= \u2205 and\n\u22c24\nj=1Bj 6= \u2205. We then find for the lower\nprevision P modelling the available information about the pair (a, b) that, using Proposition 2 and Propositions 1, 4 and 5:\nP = ui=1,\u00b7\u00b7\u00b7 ,3\nj=1,\u00b7\u00b7\u00b7 ,4\n(\nPAi \u00d7INE PBj\n)\n= ui=1,\u00b7\u00b7\u00b7 ,3\nj=1,\u00b7\u00b7\u00b7 ,4\n(\nPAi \u00d7TI PBj\n)\n=\n(u3i=1PAi)\u00d7INE (u4j=1PBj) = (u3i=1PAi)\u00d7TI (u4j=1PBj)\n= P(T3i=1 Ai)\u00d7(T4j=1 Bj).\nThe available information about the output y is represented by the coherent lower prevision Py , where\nPy(U) = inf\n\u03b1\u2208T3i=1 Ai inf\u03b2\u2208T4j=1 Bj U(f(\u03b1, \u03b2)),\nfor all gambles U on Y . As an illustration, we give the lower prevision P(f) and the upper prevision P(f) of the output,\nfor the subproblems 3a and 3b.\nProblem 3a. For (\n\u22c23\ni=1Ai)\u00d7 (\n\u22c24\nj=1Bj) = [0.5, 0.7]\u00d7 {0.6}, we find\nP(f) = 1.04881 and P(f) = 1.2016.\nProblem 3b. For (\n\u22c23\ni=1Ai)\u00d7 (\n\u22c24\nj=1Bj) = [0.5, 0.6]\u00d7 {0.6}, we find that\nP(f) = 1.04881 and P(f) = 1.1156.\nIn Problem 3c, the lower previsions corresponding with the different expert assessments PAi and PBj are conflicting,\nsince\n\u22c23\ni=1Ai =\n\u22c24\nj=1Bj = \u2205. The modeller has no additional information regarding the reliability of any of the sources;\nshe considers all of the assessments equally credible and applies the disjunction rule to all of them:\nP = unionsqi=1,\u00b7\u00b7\u00b7 ,3\nj=1,\u00b7\u00b7\u00b7 ,4\n(\nPAi \u00d7INE PBj\n)\n= unionsqi=1,\u00b7\u00b7\u00b7 ,3\nj=1,\u00b7\u00b7\u00b7 ,4\n(\nPAi \u00d7TI PBj\n)\n=\n(unionsq3i=1PAi)\u00d7INE (unionsq4j=1PBj) = (unionsq3i=1PAi)\u00d7TI (unionsq4j=1PBj)\n= P(S3i=1 Ai)\u00d7(S4j=1 Bj).\nThe available information about the output y is represented by the coherent lower prevision Py , where\nPy(U) = inf\n\u03b1\u2208S3i=1 Ai inf\u03b2\u2208S4j=1 Bj U(f(\u03b1, \u03b2)),\nfor all gambles U on Y . As an illustration, we give the lower prevision P(f) and the upper prevision P(f) of the output.\nProblem 3c. We find that\nP(f) = 0.692201 and P(f) = 2.\n20\n7.3. Solution to Problem 5. Using the results of Section 2.3, the assessments Ai = [ai, ai] are described by the vacuous\nlower previsions PAi onL (A ) (i = 1, . . . , 3), the assessments Mj = [\u00b5\nj\n1, \u00b5\nj\n2] by the vacuous lower previsions PMj on\nL (M ) (j = 1, . . . , 3), and the assessments Sk = [\u03c3k1 , \u03c3\nk\n2 ] by the vacuous lower previsionsPSk onL (S ) (k = 1, . . . , 3).\nInformation about the value of the parameter b in B is derived from information about \u00b5 and \u03c3 through the common\nsampling model, ln b \u223c N(\u00b5, \u03c3), and using the marginal extension theorem (see Section 2.5.2).\nLet us first concentrate on the information about the parameter b. When combining the assessments about the values\nof the parameters \u00b5 and \u03c3, there are in principle two possible ways to proceed:\n(i) first obtain, for each expert, the marginal extensions based on his assessments of the values of \u00b5 and \u03c3, and then\ncombine these marginal extensions;\n(ii) first combine the different experts\u2019 assessments about the values of \u00b5 and \u03c3, and then calculate the marginal exten-\nsion.\nThe two strategies are not equivalent; in fact, for the types of combination we have studied, it turns out that the second\nstrategy gives more precise results (i.e., the resulting buying prices are higher). This is not entirely unexpected, as the\nsecond strategy leads to more precise results because it involves additional assumptions: the experts use the same sampling\nmodel and make assessments about the same sampling model parameters. Since this is precisely the case in the problem\nunder study, we have opted for the second strategy.\nWe first consider Problems 5a and 5b, where the lower previsions PAi , the lower previsions PMj , and the lower\nprevisions PSk are consistent, because\n\u22c23\ni=1Ai 6= \u2205,\n\u22c23\nj=1Mj 6= \u2205 and\n\u22c23\nk=1 Sk 6= \u2205, see Proposition 2.\nWe can then model the information about the parameters \u00b5 and \u03c3 by the following lower prevision onL (M \u00d7S ):\nuj=1,\u00b7\u00b7\u00b7 ,3\nk=1,\u00b7\u00b7\u00b7 ,3\n(\nPMj \u00d7INE PSk\n)\n= uj=1,\u00b7\u00b7\u00b7 ,3\nk=1,\u00b7\u00b7\u00b7 ,3\n(\nPMj \u00d7TI PSk\n)\n=\n(u3j=1PMi)\u00d7INE (u3k=1PSk) = (u3j=1PMj)\u00d7TI (u3k=1PSk) = P(T3j=1Mj)\u00d7(T3k=1 Sk).\nUsing the results of Section 2.5, we find that the information about the value of the parameter b can then be modelled by\nthe lower prevision Q , where\nQ(Y ) = inf\n\u00b5\u2208T3j=1Mj inf\u03c3\u2208T3k=1 Sk\n\u222b\nB\nY \u03c6\u00b5,\u03c3d\u03bb,\nfor all integrable gambles Y onB. This lower prevision has the Bayesian sensitivity analysis interpretation.\nBy Proposition 2, the conjunction of the sources Ai is the vacuous lower prevision PT3\ni=1 Ai\n. By Proposition 1, the\ntype-I product of this conjunction with the lower prevision Q coincides with their independent natural extension, and it is\ngiven by the lower prevision P , where\nP(Z) = inf\n\u03b1\u2208T3i=1 Ai inf\u00b5\u2208T3j=1Mj inf\u03c3\u2208T3k=1 Sk\n\u222b\nB\nZ(\u03b1, \u00b7)\u03c6\u00b5,\u03c3(\u00b7)d\u03bb\nfor all gambles Z on A \u00d7B such that Z(\u03b1, \u00b7) is integrable for all \u03b1 in A .\nThe available information about the output y is represented by the coherent lower prevision Py , where\nPy(U) = inf\n\u03b1\u2208T3i=1 Ai inf\u00b5\u2208T3j=1Mj inf\u03c3\u2208T3k=1 Sk\n\u222b\nB\nU (f(\u03b1, \u00b7))\u03c6\u00b5,\u03c3(\u00b7)d\u03bb\nfor all gambles U on Y such that U (f(\u03b1, \u00b7)) is integrable for all \u03b1 in A . As an illustration, we give the lower prevision\nP(f) and the upper prevision P(f) of the system output y.\nProblem 5a. With\n\u22c23\ni=1Ai = [0.5, 0.7],\n\u22c23\nj=1Mj = [0.6, 0.8] and\n\u22c23\nk=1 Sk = [0.3, 0.4] we find that\nP(f) = 1.54027 and P(f) = 2.19107.\nProblem 5b. With\n\u22c23\ni=1Ai = [0.5, 0.6],\n\u22c23\nj=1Mj = [0.6, 0.7] and\n\u22c23\nk=1 Sk = [0.3, 0.35] we find that\nP(f) = 1.54027 and P(f) = 1.81496.\nIn Problem 5c, all the experts give conflicting information about the parameters a, \u00b5 and \u03c3, as\n\u22c23\ni=1Ai =\n\u22c23\nj=1Mj =\u22c23\nk=1 Sk = \u2205, see Proposition 2. As discussed above, we first combine the expert assessments M1, . . . , M3 and S1,\n. . . , S3. Both \u00b5-sources and \u03c3-sources are conflicting, so we combine them separately using disjunction and take their\nindependent product. We find the following lower prevision onL (M \u00d7S ), representing the available information about\n(\u00b5, \u03c3):\nP (S3i=1Mi)\u00d7(S3i=1 Si).\nThis lower prevision has the Bayesian sensitivity analysis interpretation.\nThe sources A1, . . . , A3 are conflicting too, so we use disjunction. We find the lower prevision PS3\ni=1 Ai\non L (A ).\nApplying the marginal extension theorem, and taking the independent product\u2014independent natural extension and type-I\nproduct coincide\u2014we eventually find that the available information about (a, b) can be modelled by the lower prevision\nP , where\nP(Z) = inf\n\u03b1\u2208S3i=1 Ai inf\u00b5\u2208S3j=1Mj inf\u03c3\u2208S3k=1 Sk\n\u222b\nB\nZ(\u03b1, \u00b7)\u03c6\u00b5,\u03c3(\u00b7)d\u03bb\n21\nfor all gambles Z on A \u00d7B such that Z(\u03b1, \u00b7) is integrable for all \u03b1 in A .\nThe available information about the output y is represented by the coherent lower prevision Py , where\nPy(U) = inf\n\u03b1\u2208S3i=1 Ai inf\u00b5\u2208S3j=1Mj inf\u03c3\u2208S3k=1 Sk\n\u222b\nB\nU (f(\u03b1, \u00b7))\u03c6\u00b5,\u03c3(\u00b7)d\u03bb\nfor all gambles U on Y such that U (f(\u03b1, \u00b7)) is integrable for all \u03b1 in A . As an illustration, we give the lower prevision\nP(f) and the upper prevision P(f) of the system output y.\nProblem 5c. For\n\u22c33\ni=1Ai = [0.1, 1.0],\n\u22c33\nj=1Mj = [0.0, 1.0] and\n\u22c33\nk=1 Sk = [0.1, 0.2]\u222a [0.25, 0.35]\u222a [0.4, 0.5], we find\nthat\nP(f) = 1.00966 and P(f) = 4.08022.\n8. CONCLUSION\nWe have argued extensively that the theory of coherent lower previsions is eminently suited for solving the first set of\nproblems posed in [11]. For easy reference, and in order to allow easy comparison with other solution methods, we have\nlisted the calculated lower and upper previsions P(f) and P(f) for the output y in Table 1. We want to emphasise that\nthese numbers are by no means the only information that our models provide. In fact the lower previsions Py we have\ngiven as solutions in Sections 5 and 7 contain much more information than just these two numbers, and they can also be\nused to in decision making and estimation problems. But we feel a more extensive discussion of these issues to be beyond\nthe scope of the modelling challenges presented in [11].\nproblem P(f) P(f)\n1 0.692201 2.0\n2a 0.956196 1.8\n2b 0.956196 1.7\n2c 0.692201 2.0\n3a 1.04881 1.2016\n3b 1.04881 1.1156\n3c 0.692201 2.0\n4 1.00966 4.08022\n5a 1.54027 2.19107\n5b 1.54027 1.81496\n5c 1.00966 4.08022\n6 1.05939 2.86825\nTABLE 1. Lower and upper previsions for the output y calculated using our imprecise probability mod-\nels for the available information\nREFERENCES\n[1] G. Choquet. Theory of capacities. Annales de l\u2019Institut Fourier, 5:131\u2013295, 1953\u20131954.\n[2] G. de Cooman. Possibility theory I\u2013III. International Journal of General Systems, 25:291\u2013371, 1997.\n[3] G. de Cooman and D. Aeyels. Supremum preserving upper probabilities. Information Sciences, 118:173\u2013212, 1999.\n[4] G. de Cooman and M. Troffaes. Dynamic programming under imprecise gain. submitted to the Third International Symposium on Imprecise\nProbabilities and Their Applications (ISIPTA \u201903). 2003.\n[5] B. de Finetti. Theory of Probability. John Wiley & Sons, Chichester, 1974\u20131975. English Translation of Teoria delle Probabilita`.\n[6] Bruno de Finetti. Sul significato soggettivo della probabilita`. Fundamenta Mathematicae, 17:298\u2013329, 1931.\n[7] D. Dubois and H. Prade. Possibility Theory. Plenum Press, New York, 1988.\n[8] A. N. Kolmogorov. Grundbegriffe der Wahrscheinlichkeitsrechnung. Springer, Berlin, 1933.\n[9] A. N. Kolmogorov. Foundations of Probability. Chelsea Publishing Co., New York, 1950. English translation of [8].\n[10] I. Levi. The Enterprise of Knowledge. MIT Press, London, 1980.\n[11] W. L. Oberkampf, J. C. Helton, C. A. Joslyn, S. F. Wojtkiewicz, and S. Ferson. Challenge problems: uncertainty in system response given uncertain\nparameters. Reliability Engineering and System Safety. (this issue).\n[12] M. C. M. Troffaes and G. de Cooman. Extension of coherent lower previsions to unbounded random variables. In Proceedings of the Ninth\nInternational Conference IPMU 2002 (Information Processing and Management of Uncertainty in Knowledge-Based SystemsJuly 1\u20135, Annecy,\nFrance), pages 735\u201342. ESIA \u2013 Universite\u00b4 de Savoie, 2002.\n[13] M. C. M. Troffaes and G. de Cooman. Lower previsions for unbounded random variables. In P. Grzegorzewski, O. Hryniewicz, and M. A\u00b4ngeles Gil,\neditors, Soft Methods in Probability, Statistics and Data Analysis, Advances in Soft Computing, pages 146\u2013155. Physica-Verlag, New York, 2002.\n[14] P. Walley. Statistical Reasoning with Imprecise Probabilities. Chapman and Hall, London, 1991.\n[15] P. Walley and T. L. Fine. Varieties of modal (classificatory) and comparative probability. Synthese, 41:321\u2013374, 1979.\n[16] L. A. Zadeh. Fuzzy sets as a basis for a theory of possibility. Fuzzy Sets and Systems, 1:3\u201328, 1978.\n22\n"}