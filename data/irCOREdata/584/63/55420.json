{"doi":"10.1016\/j.robot.2007.07.001","coreId":"55420","oai":"oai:eprints.lincoln.ac.uk:2103","identifiers":["oai:eprints.lincoln.ac.uk:2103","10.1016\/j.robot.2007.07.001"],"title":"Appearance-based localization for mobile robots using digital zoom and visual compass","authors":["Bellotto, Nicola","Burn, Kevin","Fletcher, Eric","Wermter, Stefan"],"enrichments":{"references":[{"id":18435222,"title":"A mobile robot that learns its place,","authors":[],"date":"1997","doi":"10.1162\/neco.1997.9.3.683","raw":"S. Oore, G. E. Hinton, G. Dudek, A mobile robot that learns its place, Neural Computation 9(3) (1997), pp. 683-699.","cites":null},{"id":18435201,"title":"A New Omnidirectional Vision Sensor for Monte-Carlo Localization, in:","authors":[],"date":"2004","doi":"10.1007\/978-3-540-32256-6_8","raw":"E. Menegatti, A. Pretto, E. Pagello, A New Omnidirectional Vision Sensor for Monte-Carlo Localization, in: Proc. of RoboCup Symposium, 2004, pp. 97-109.","cites":null},{"id":18435215,"title":"Active global localization for a mobile robot using multiple hypothesis tracking,","authors":[],"date":"2001","doi":"10.1109\/70.964673","raw":"P.  Jensfelt,  S.  Kristensen,  Active  global  localization  for  a  mobile  robot  using  multiple hypothesis tracking, IEEE Trans. on Robotics and Automation, 17(5) (2001), pp. 748-760.","cites":null},{"id":18435197,"title":"Alfred: The Robot Waiter Who Remembers You, in:","authors":[],"date":"1999","doi":null,"raw":"B.A. Maxwell, L.A. Meeden, N. Addo, L. Brown, P. Dickson, J. Ng, S. Olshfski, E. Silk, J. Wales, Alfred: The Robot Waiter Who Remembers You, in: Proc. of the AAAI Workshop on Robotics, Florida, USA, 1999.","cites":null},{"id":18435208,"title":"An experimental Comparison of Localization Methods Continued, in:","authors":[],"date":"2002","doi":"10.1109\/irds.2002.1041432","raw":"J.S. Gutmann, D. Fox, An experimental Comparison of Localization Methods Continued, in: Proc. of the IEEE\/RSJ Int. Conf. on Intelligent Robots and Systems (IROS\u201902), 2002, pp. 454-459.","cites":null},{"id":18435207,"title":"An Experimental Comparison of Localization Methods, in:","authors":[],"date":"1998","doi":"10.1109\/iros.1998.727280","raw":"J.S. Gutmann, W. Burgard, D. Fox, K. Konolige, An Experimental Comparison of Localization Methods, in: Proc. of the Int. Conf. on Intelligent Robots and Systems (IROS\u201998), 1998, pp. 736-743.","cites":null},{"id":18435209,"title":"An Experimental Comparison of Localization Methods, the MHL Sessions, in:","authors":[],"date":"2003","doi":"10.1109\/iros.2003.1250757","raw":"S. Kristensen, P. Jensfelt, An Experimental Comparison of Localization Methods, the MHL Sessions, in: Proc. of the IEEE\/RSJ Int. Conf. on Intelligent Robot and Systems (IROS\u201903), 2003, pp. 992-997.","cites":null},{"id":18435202,"title":"Appearance-Based Place Recognition for Topological Localization, in:","authors":[],"date":"2000","doi":"10.1109\/robot.2000.844734","raw":"I. Ulrich, I. Nourbakhsh, Appearance-Based Place Recognition for Topological Localization, in: Proc. of the IEEE Int. Conf. on Robotics and Automation (ICRA\u201900), San Francisco, USA, 2000, pp. 1023-1029. Pre-print version accepted for publication on Robotics and Autonomous Systems, Vol. 56, Issue 2, pp.143-156, February 2008","cites":null},{"id":18435221,"title":"Bayesian Landmark Learning for Mobile Robot Localization,","authors":[],"date":"1998","doi":null,"raw":"S. Thrun, Bayesian Landmark Learning for Mobile Robot Localization, Machine Learning 33 (1998), pp. 41-76.","cites":null},{"id":18435211,"title":"Computer Vision,","authors":[],"date":"1982","doi":"10.1007\/978-0-387-31439-6_273","raw":"D. H. Ballard, C. M. Brown, Computer Vision, Prentice Hall, New York, 1982.","cites":null},{"id":18435206,"title":"Context-based vision system for place and object recognition,","authors":[],"date":"2003","doi":"10.1109\/iccv.2003.1238354","raw":"A. Torralba, K. P. Murphy, W. T. Freeman, M. A. Rubin, Context-based vision system for place and object recognition, IEEE Int. Conf. on Computer Vision, France, 2003.","cites":null},{"id":18435204,"title":"Distinctive Image Features from Scale-Invariant Keypoints,","authors":[],"date":"2004","doi":"10.1023\/b:visi.0000029664.99615.94","raw":"D. G. Lowe, Distinctive Image Features from Scale-Invariant Keypoints, International Journal of Computer Vision, 60, 2 (2004), pp. 91-110.","cites":null},{"id":18435216,"title":"Estimating the absolute position of a mobile robot using position probability grids, in:","authors":[],"date":"1996","doi":"10.1109\/eurbot.1996.551874","raw":"W. Burgard, D. Fox, D. Hennig, T. Schmidt, Estimating the absolute position of a mobile robot using position probability grids, in: Proc. of the 13th National Conf. on Artificial Intelligence (AAAI\u201996), Portland, Oregon, USA, 1996, pp. 896-901.","cites":null},{"id":18435196,"title":"Experiences with an interactive museum tour-guide robot,","authors":[],"date":"1999","doi":"10.1016\/s0004-3702(99)00070-3","raw":"W. Burgard, A.B. Cremers, D. Fox, D. H\u00e4hnel, G. Lakemeyer, D. Schulz, W. Steiner, S. Thrun, Experiences  with  an  interactive  museum  tour-guide  robot,  Artificial  Intelligence  114  (1-2) (1999), pp. 3-55.","cites":null},{"id":18435220,"title":"Finding landmarks for mobile robot navigation, in:","authors":[],"date":"1998","doi":"10.1109\/robot.1998.677210","raw":"S. Thrun, Finding landmarks for mobile robot navigation, in: Proc. of the IEEE Int. Conf. on Robotics and Automation (ICRA\u201998), 1998, pp. 958-963.","cites":null},{"id":18435214,"title":"Global localization and topological map learning for robot navigation, in","authors":[],"date":"2002","doi":null,"raw":"D.  Filliat, J. A. Meyer, Global localization and topological map learning for robot navigation, in Proc. of the 7th Int. Conf. on Simulation of Adaptive Behavior (SAB\u201902), 2002, pp. 131-140.","cites":null},{"id":18435210,"title":"Graphics gems IV,","authors":[],"date":"1994","doi":"10.1016\/b978-0-12-336156-1.50003-3","raw":"K. Zuidervel, Graphics gems IV, Academic Press Professional, Inc., San Diego, 1994.","cites":null},{"id":18435198,"title":"Indoor Robot Navigation with Single Camera Vision, in:","authors":[],"date":"2002","doi":null,"raw":"G. Gini, A. Marchi, Indoor Robot Navigation with Single Camera Vision, in: Proc. of Pattern Recognition in Information Systems (PRIS 2002), 2002, pp. 67-76.","cites":null},{"id":18435213,"title":"Markov Localization: A Probabilistic Framework for Mobile Robot Localization and Navigation. Doctoral Thesis.","authors":[],"date":"1998","doi":null,"raw":"D.  Fox, Markov Localization: A Probabilistic Framework for Mobile Robot Localization and Navigation. Doctoral Thesis. Institute of Computer Science III, University of Bonn, Germany, 1998.","cites":null},{"id":18435219,"title":"Maximum likelihood from incomplete data via the EM algorithm,","authors":[],"date":"1977","doi":null,"raw":"A.P. Dempster, N.M. Laird, D.B. Rubin, Maximum likelihood from incomplete data via the EM algorithm, Journal of the Royal Statistical Society, vol. 39 B (1977), pp. 1-38.","cites":null},{"id":18435195,"title":"MINERVA: A Second-Generation Museum TourGuide Robot, in:","authors":[],"date":"1999","doi":"10.1109\/robot.1999.770401","raw":"S.  Thrun,  M.  Bennewitz,  W.  Burgard,  A.B.  Cremers,  F.  Dellaert,  D.  Fox,  D.  H\u00e4hnel,  C. Rosenberg, N. Roy, J. Schulte, D. Schulz, MINERVA: A Second-Generation Museum TourGuide Robot, in: Proc. of the IEEE Int. Conf. on Robotics and Automation (ICRA\u201999), 1999, v. 3, pp. 1999-2005.","cites":null},{"id":18435223,"title":"Mobile robot self-localization using occupancy histograms and a mixture of Gaussian location hypotheses,","authors":[],"date":"2001","doi":"10.1016\/s0921-8890(00)00116-0","raw":"T.  Duckett,  U.  Nehmzow,  Mobile  robot self-localization  using  occupancy  histograms  and  a mixture of Gaussian location hypotheses, Robotics and Autonomous Systems 34 (2001), pp. 117-129.","cites":null},{"id":18435218,"title":"Sensor fusion in certainty grids for mobile robots,","authors":[],"date":"1988","doi":"10.1007\/978-3-642-74567-6_19","raw":"H.P. Moravec, Sensor fusion in certainty grids for mobile robots, AI Magazine (1988) pp. 61-74.","cites":null},{"id":18435203,"title":"Topological Localization for Mobile Robots using Omni-directional Vision and Local Features, in","authors":[],"date":"2004","doi":"10.1109\/robot.2005.1570627","raw":"H. Andreasson, T. Duckett, Topological Localization for Mobile Robots using Omni-directional Vision and Local Features, in Proc. of the 5th Symposium on Intelligent Autonomous Vehicles (IAV), Portugal, 2004.","cites":null},{"id":18435200,"title":"Vision-based Robot Localization using Sporadic Features, in:","authors":[],"date":"2001","doi":"10.1007\/3-540-44690-7_5","raw":"S. Enderle, H. Folkerts, M. Ritter, S. Sablatn\u00f6g, G. Kraetzshmar, G. Palm, Vision-based Robot Localization  using  Sporadic  Features,  in:  Proc.  of  the  Int.  Workshop  on  Robot  Vision, Auckland, New Zealand, 2001, pp. 35-42.","cites":null},{"id":18435224,"title":"Visual compass, in","authors":[],"date":"2004","doi":"10.1002\/rob.20159","raw":"F. Labrosse, Visual compass, in Proc. of Towards Autonomous Robotic Systems (TAROS\u201904), 2004, University of Essex, Colchester, UK. Nicola Bellotto received his Laurea degree in Electronic Engineering from the University of Padua in Italy. He is currently a PhD candidate in robotics at the University of Essex in UK. His doctoral thesis focuses on  Multisensor  Data  Fusion  for  Human-Robot  Interaction.  Other research  interests  include  robot  localization,  computer  vision  and embedded  systems  programming.  Before  joining  the  Human  Centred Robotics  Group  in  Essex,  he  has  been  an  active  member  of  the Intelligent Autonomous System Laboratory in Padua and of the Centre for  Hybrid  Intelligent  Systems  at  the  University  of  Sunderland.  He gained also several years of professional experience in Italy and UK as embedded systems programmer and software developer for entertainment robotics. Kevin  Burn  received  a  BSc  degree  in  Mechanical  Engineering  from Newcastle University in 1984 and an MEng from Durham University in 1986. After working in the power industry for several years he returned to Newcastle University as a Research Associate and was awarded his PhD in 1994 for research into robotics and teleoperator systems. He is now  a  Senior  Lecturer  at  Sunderland  University  in  the  School  of Computing and Technology, where his research interests are in robotics and intelligent systems. Professor Eric Fletcher graduated from the University of Hull with an honours degree in Physics and Pure Mathematics in 1965. He obtained a Ph.D. in Geophysics from the University of Newcastle upon Tyne in 1975. Subsequently he did research in Solid State Physics specialising in  Auger  Spectroscopy  of  Glass  surfaces.  In  1986  he  transferred  to Computer  Science  initially  specialising  in  Mathematical  Modelling Simulation  and  Decision  Support  Systems.  In  recent  years  he  has concentrated on image analysis applied to biological systems and traffic flow monitoring. He has supervised 14 Ph.D. students and has over 80 publications in Computer Science and Solid State Physics. In December 2003 he retired from the full time post of Professor of Applied Computing at the University of Sunderland and is now Emeritus Professor of Computing and is continuing his interests in image analysis. Professor Stefan Wermter holds the Chair in Intelligent Systems at the University  of  Sunderland,  UK  and  is  the  Director  of  the  Centre  for Hybrid  Intelligent  Systems.  His  research  interests  are  in  Intelligent Systems, Neural Networks, Cognitive Neuroscience, Hybrid Systems, Language Processing, and Learning Robots. He has an MSc from the University of Massachusetts, USA and a PhD and Habilitation from the University of Hamburg, Germany, both in Computer Science and was a Research Scientist at Berkeley, USA before joining the University of Sunderland in 1998. Professor Wermter has written or edited five books and  published  about  130  articles  on  this  research  area,  including  books  like  &quot;Hybrid Connectionist  Natural  Language  Processing&quot;  or  &quot;Connectionist,  Statistical,  and  Symbolic Approaches  to  Learning  for  Natural  Language  Processing&quot;,  &quot;Hybrid  Neural  Systems&quot;, &quot;Emergent  Neural  Computational  Architectures  based  on  Neuroscience&quot;  and  \u201cBiomimetic Neural Learning for Intelligent Robots\u201d.","cites":null},{"id":18435199,"title":"Visual Self-Localization for Indoor Mobile Robots Using Natural Lines, in:","authors":[],"date":"2003","doi":"10.1109\/iros.2003.1248817","raw":"N.X. Dao, B.J. You, S.R. Oh, M. Hwangbo, Visual Self-Localization for Indoor Mobile Robots Using Natural Lines, in: Proc. of the IEEE\/RSJ Int. Conf. on Intelligent Robots and Systems (IROS\u201903), Las Vegas, USA, 2003, pp.1252-1257.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2008","abstract":"This paper describes a localization system for mobile robots moving in dynamic indoor environments, which uses probabilistic integration of visual appearance and odometry information. The approach is based on a novel image matching algorithm for appearance-based place recognition that integrates digital zooming, to extend the area of application, and a visual compass. Ambiguous information used for recognizing places is resolved with multiple hypothesis tracking and a selection procedure inspired by Markov localization. This enables the system to deal with perceptual aliasing or absence of reliable sensor data. It has been implemented on a robot operating in an office scenario and the robustness of the approach demonstrated experimentally","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/55420.pdf","fullTextIdentifier":"http:\/\/eprints.lincoln.ac.uk\/2103\/2\/ras05_preprint.pdf","pdfHashValue":"f623ac13bdd5679d469bd89cac1ceeeef4febbd7","publisher":"Elsevier","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lincoln.ac.uk:2103<\/identifier><datestamp>\n      2013-11-18T12:15:34Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F48:6A6163735F48363731<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lincoln.ac.uk\/2103\/<\/dc:relation><dc:title>\n        Appearance-based localization for mobile robots using digital zoom and visual compass<\/dc:title><dc:creator>\n        Bellotto, Nicola<\/dc:creator><dc:creator>\n        Burn, Kevin<\/dc:creator><dc:creator>\n        Fletcher, Eric<\/dc:creator><dc:creator>\n        Wermter, Stefan<\/dc:creator><dc:subject>\n        H671 Robotics<\/dc:subject><dc:description>\n        This paper describes a localization system for mobile robots moving in dynamic indoor environments, which uses probabilistic integration of visual appearance and odometry information. The approach is based on a novel image matching algorithm for appearance-based place recognition that integrates digital zooming, to extend the area of application, and a visual compass. Ambiguous information used for recognizing places is resolved with multiple hypothesis tracking and a selection procedure inspired by Markov localization. This enables the system to deal with perceptual aliasing or absence of reliable sensor data. It has been implemented on a robot operating in an office scenario and the robustness of the approach demonstrated experimentally.<\/dc:description><dc:publisher>\n        Elsevier<\/dc:publisher><dc:date>\n        2008<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lincoln.ac.uk\/2103\/2\/ras05_preprint.pdf<\/dc:identifier><dc:identifier>\n          Bellotto, Nicola and Burn, Kevin and Fletcher, Eric and Wermter, Stefan  (2008) Appearance-based localization for mobile robots using digital zoom and visual compass.  Robotics and Autonomous Systems, 56  (2).   pp. 143-156.  ISSN 0921-8890  <\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1016\/j.robot.2007.07.001<\/dc:relation><dc:relation>\n        10.1016\/j.robot.2007.07.001<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lincoln.ac.uk\/2103\/","http:\/\/dx.doi.org\/10.1016\/j.robot.2007.07.001","10.1016\/j.robot.2007.07.001"],"year":2008,"topics":["H671 Robotics"],"subject":["Article","PeerReviewed"],"fullText":"Pre-print version accepted for publication on Robotics and Autonomous Systems, Vol. 56, Issue 2, pp.143-156, February 2008 \n 1 \nAppearance-based localization for mobile robots  \nusing digital zoom and visual compass \n \nN. Bellottoa,*, K. Burnb, E. Fletcherb, S. Wermterb \na\n Human Centred Robotics Group, Department of Computer Science, \nUniversity of Essex, Wivenhoe Park, Colchester CO4 3SQ, UK \nb\n Centre for Hybrid Intelligent Systems, School of Computing and Technology,  \nUniversity of Sunderland, St Peter\u2019s Way, Sunderland SR6 0DD, UK \n \nAbstract \nThis paper describes a localization system for mobile robots moving in dynamic indoor \nenvironments, which uses probabilistic integration of visual appearance and odometry \ninformation. The approach is based on a novel image matching algorithm for appearance-\nbased place recognition that integrates digital zooming, to extend the area of application, and \na visual compass. Ambiguous information used for recognizing places is resolved with \nmultiple hypothesis tracking and a selection procedure inspired by Markov localization. This \nenables the system to deal with perceptual aliasing or absence of reliable sensor data. It has \nbeen implemented on a robot operating in an office scenario and the robustness of the \napproach demonstrated experimentally. \n \nKeywords: appearance-based localization; digital zoom; visual compass; Markov localization. \n \n \n1. Introduction \n \nIn mobile robotics, localization plays a fundamental role for the navigation task, since it is \nnecessary for every kind of path-planning. In order to achieve a goal, an autonomous mobile \nrobot must be able to localize itself within the environment where it is acting and relatively to \nthe target destination. \nThe main objective of this article is to illustrate the implementation of a new map-based \nlocalization system for a mobile robot operating in an indoor environment where it is not \nnecessary to know the exact, absolute position. Instead, a topological localization is the most \nappropriate solution. We developed a new visual place recognition algorithm that does not \nneed any specific landmark. In particular, the novelty introduced by such algorithm is the use \nof digital zooming to improve the capability of recognizing places. The same algorithm is also \nused for reconstructing panoramic images from the place of interest, combining a sequence of \nsnapshots taken with the camera. Such images, together with approximate coordinates of the \ntopological locations, form the map used by the robot. Furthermore, when the robot is located \nin one of the mapped places, it can also estimate its absolute orientation using vision, thanks \nto an original visual compass system. The place recognition process is then followed by a \nprocedure that resolves cases of perceptual aliasing or absence of reliable sensor information. \nThe system keeps track of a set of hypotheses and for each update step chooses the most \nplausible with an approach inspired by Markov Localization. From experiments carried out in \na typical office scenario, the method shows to be robust even in case of dynamic \nenvironments and locations poor of features. \n                                                 \n*\n Corresponding author. \n  E-mail address: nbello@essex.ac.uk (N. Bellotto). \n 2 \nThe remainder of the article is structured as follows: in Section 2 we report a brief \nliterature review; Section 3 and Section 4 describe respectively the place recognition and the \nmultiple hypothesis localization; in Section 5 we present some experimental results and \nfinally we conclude in Section 6 with a summary and some recommendations. \n \n \n2. Related work \n \nIn recent years there have been increasing numbers of robot applications where \nlocalization is an essential part of the navigation system. Well known examples include the \ntour-guide robots Rhino and Minerva [1, 2], or the robot-waiter Alfred [3], which used \ndifferent approaches and sensors for localization. With Rhino, for example, perceptions were \nbased upon sonar and laser range sensors, whilst Minerva made use of lasers plus an \nadditional camera directed towards the ceiling, so the observed scene was mostly static. In \ncontrast, Alfred used vision with artificial landmarks to recognize places of interest. \nOther localization approaches making use of vision have been presented in recent years. \nGini and Marchi [4] used a robot equipped with a unidirectional camera pointing ahead and \ntowards the floor. Their basic hypothesis was that the floor had a uniform texture so that after \ncamera calibration, it was possible to reconstruct a local map from images. Localization was \nthen the result of a comparison between the current local map and a pre-recorded global map. \nThe solution of Dao et al. [5] was based on a natural landmark model and a robust tracking \nalgorithm. The landmark model contained sets of three or more natural lines such as \nbaselines, door edges and linear edges of tables or chairs. The localization depended on an \nalgorithm that allowed the robot to determine its absolute position from a single landmark. \nSeveral recent approaches have made use of Monte Carlo localization [6, 8]. It has been \ndemonstrated that this technique is reliable and, at the same time, keeps the processing time \nlow. Indeed, Monte Carlo localization has been successfully applied in the RoboCup four-\nlegged league, where the Sony dog\u2019s hardware has critical limitations. For example, Enderle \net al. [6] implemented a Monte Carlo approach for vision-based localization that made use of \nsporadic features, extracted from the images of the robot\u2019s unidirectional camera. The \nprobability of being in a certain location was calculated against an internal model of the \nenvironment where the robot moved. Experiments proved that the method was reliable \nenough, even with a restricted number of image samples, and was improved drastically by \nincreasing the number of features. Tests in a typical office environment were also promising.  \nMenegatti et al. described another application of Monte Carlo localization in the \nRoboCup context [7]. In this case, the video input came from an omni-directional sensor; the \nimages were processed in a way to simulate a laser scanner, using the distances from points \nwith color intensity transitions. Even here the localization system made use of an internal \nrepresentation of the football field. Ulrich and Nourbakhsh also used an omni-directional \ncamera for topological localization [8]. They presented an appearance-based place recognition \nscheme that used only panoramic vision, without any odometry information. Color images \nwere classified in real-time with nearest-neighbor learning, image histogram matching and a \nsimple voting scheme. Andreasson and Duckett illustrated another system in [9] that \nperformed topological localization using images from an omni-directional camera. Their \nmethod searched for the best matching place among a database of pre-recorded panoramic \nimages, extracting and using modified SIFT features [10]. An interesting approach was also \nthe context-based visual recognition implemented by Torralba et al. [11], which made use of \nlow resolution images from a wearable camera to extract texture features and their spatial \nlayout. Training was done with hand-labeled image sequences taken in the environment to \nPre-print version accepted for publication on Robotics and Autonomous Systems, Vol. 56, Issue 2, pp.143-156, February 2008 \n 3 \nmap, then localization was performed using two parallel Hidden Markov Models (HMMs) for \nboth place recognition and categorization, the latter useful also for the identification of \nunexplored environments. \nNumerous techniques have finally been devised to resolve the ambiguity that arises in \nsensory perception, irrespective of the device employed. No observation indeed is immune \nfrom noise and errors, originating in both the sensor and the surrounding environment. A wide \nrange of localization systems have been tested and compared in the works of [12-14], \ncovering methods based on Extended Kalman Filtering (EKF), Markov Localization (ML), a \ncombination of the two (ML-EKF), Monte Carlo Localization (MCL) and Multi Hypotheses \nLocalization (MHL). The results of these experiments have been used as a basis for \nmotivating the most suitable localization approach for our application.  \n \n \n3. Place recognition \n \nIn this section, we describe a new method to recognize a position amongst a finite set of \npossible locations. This set is basically a topological map of the environment provided by the \nuser and each place is identified by a point in the Cartesian space and a panoramic image of \nthe scene observed from that point. The procedure is based on the comparison of a new \nimage, taken by the robot\u2019s camera, with all the panoramic images of the map. A measure of \nthe match\u2019s quality is assigned to each comparison using a novel image-matching algorithm \n(or IMA). Basically, this process constitutes the place recognition, which is an essential part of \nour localization. \n \n3.1 Image matching algorithm \nTypically, for indoor environments, most of the relevant changes occurring in an image \nare due to objects or people moving with respect to a horizontal plane. A person walking, a \nchair moving, a door opening or closing: all of these examples can be thought as \u201ccolumns\u201d \nmoving horizontally along an image of the original scene. The algorithm described in this \nsection arises from this simple consideration. The principal idea is to divide the new image \ninto several column regions, called \u201cslots\u201d, and then compare each of them with a stored \nimage of the original scene. \nConsider the new image Inew, single channel, of width Wnew. This is divided into Ns slots \nhaving width Wslot = Wnew \/ Ns. One slot is referred to as slotn, with n = 1, \u2026, Ns. Also, \nconsider a reference image Iref, single channel, of width Wref \u2265 Wnew. The images Inew and Iref \nhave the same height. A region of Iref, delimited by the pixel columns cleft and cright, is referred \nto as Iref [cleft , cright]; the columns cleft and cright belong to this region. The two image structures \nare illustrated in Fig. 1. \n \nW new \n  \nW slot \n  \nW ref \n  \nI ref [c left  , c right ]   \nslot1 slot2 slot3 slot4 \n \nFig. 1  Example of Inew (divided into four slots) and Iref \n \n 4 \n \nVAL[1], VAL[2], VAL[3], \u2026   \nslo\nt n\n \n \nFig. 2  Slot of Inew shifted and compared along Iref by NCC \n \nThe measure of the similarity between a slot of the new image and a region of the stored \none is given by a function based on the Normalized Correlation Coefficient [15] and called \nNCC. Given a new slotn and a reference image Iref, the NCC matching function compares slotn \nwith all the regions Iref[c, c + Wslot \u2013 1], where c = 1, \u2026, Wref (if slotn falls over the right \nbound of Iref, it restarts from the beginning) After each comparison, a value between 0 and 1 is \nstored inside an array VAL of length Wref, as explained also in Fig. 2 (note that the original \nNormalized Correlation Coefficient varies between \u20131 and 1, so we actually rescale it to fit \nbetween 0 and 1). For example, if the slot\u2019s width is Wslot = 10, the assignment VAL[5] = 0.7 \nmeans that the similarity between slotn and the region Iref [5, 15] measures 0.7. \nThe actual IMA can be divided into two parts: the first apply NCC to find, for each pixel \ncolumn, which is the slot that matches best; the second determines the position that gives the \nmaximum match for the whole sequence of Ns slots. The algorithm is described by the \npseudo-code in Table 1 (note that the highlighted \u201celse if\u201d condition is for the \nreconstruction of panoramic images explained in Section 3.2). \n \nTable 1  Image Matching Algorithm (IMA) \n\/* fist part: slot matching *\/ \nVAL[Wref] = {0, \u2026, 0} \nMATCH_SLOT[Wref] = {0, \u2026, 0} \nMATCH_VAL[Wref] = {0, \u2026, 0} \nfor n = 1 to Ns \n NCC(slotn , Iref , VAL) \n for c = 1 to Wref \n  if VAL[c] > MATCH_VAL[c] \n   MATCH_SLOT[c] = n \n   MATCH_VAL[c] = VAL[c] \n  end if \n end for \nend for \n\/* second part: best match extraction *\/ \nBEST_MATCH = 0 \nCOL = 1 \nfor c = 1 to Wref \n SUM = 0 \n for n = 1 to Ns \n  i = c + (n \u2013 1) Wslot \n  if MATCH_SLOT[i] = n \n   SUM = SUM + MATCH_VAL[i] \n  else if MATCH_VAL[i] = 0.5 \n   SUM = SUM + SUM \/ (n \u2013 1) \n  end if \n end for \n if SUM > BEST_MATCH \n  BEST_MATCH = SUM \nPre-print version accepted for publication on Robotics and Autonomous Systems, Vol. 56, Issue 2, pp.143-156, February 2008 \n 5 \n  COL = c \n end if \nend for \nBEST_MATCH = BEST_MATCH \/ Ns \nEND \n \n3.2 Panoramic image \nFor every place in the environment, a panoramic image can be also reconstructed using \nthe IMA algorithm. Initially, the panorama is just a black image and the relative similarities \nreturned by NCC measure exactly 0.5. With a simple \u201celse if\u201d condition in the second part \nof the IMA, highlighted in Table 1, this situation can be handled and used for the correct \ninsertion of a new image in the panoramic view. Basically, whenever a slot is compared with \na black zone, the assigned matching-value is the mean of the previous comparisons. Of course \nthis is valid only if a sequence of snapshots, taken during a clock-wise rotation, is inserted in \nthe exact order, from left to right. The input images are also filtered using a Contrast Limited \nAdaptive Histogram Equalization (CLAHE) [16] in order to increase the number of \ndistinguishable features for scenes not well illuminated. The insertion of new images \ncontinues until the whole panorama is filled. An example of reconstruction is shown in Fig. 3. \n \n \nFig. 3  Panoramic image reconstruction \n \n3.3 Heading angle extraction \nAn important feature of the IMA is the capacity to extract the position, inside a panoramic \nimage, where a new snapshot matches best. This position is given by the value COL (see \nTable 1), which is the left pixel column of the region on Iref where the best match occurs. If \nCOL = 1 corresponds to the zero direction on a panoramic image having width Wref (and \nconsidering a clock-wise versus), then the displacement angle \u03b1 of the camera is simply given \nby the following expression: \nrefW\nCOL 12 \u2212\u22c5= pi\u03b1  (1) \nTherefore, if all the panoramic images have been reconstructed with a common angle of \nreference, \u03b1 can be used to estimate the robot\u2019s heading. Its precision is normally good \nenough to be used as a \u201cvisual compass\u201d and correct the odometry\u2019s heading angle, as \nexplained in Section 4.6 and demonstrated experimentally in Section 5.2. \n 6 \n3.4 Enhancement with digital zooming \nThe place recognition method described so far suffers from the problem of sensitivity to \nthe distance from the original point where the panoramic image has been constructed. This \nmeans that, moving the robot away from that point, the output of IMA quickly decreases. To \nsolve this problem, digital zoom was integrated to enlarge the detection area. \nDigital zoom can be implemented using bilinear interpolation and explained starting from \nthe well known pin-hole camera model [17]. This is shown in Fig. 4a for a given object of \nheight H and distance D = X \u2013 x0 from the camera, for which the next relations can be written: \nD\nH\nf\nh\n=  ( )DsD\nH\nf\nh\n,\u03c1\n\u03c1\n\u2212\n=\n\u22c5\n (2) \nwhere h is the height projected on the image plane, f is the focal length of the camera, \u03c1 is the \nzoom factor and s(\u03c1, D) = x' \u2013 x0 is the \u201cvirtual\u201d shift from the original position. After simple \npassages, the latter can be expressed as follows: \n\uf8f7\uf8f7\n\uf8f8\n\uf8f6\n\uf8ec\uf8ec\n\uf8ed\n\uf8eb\n\u2212\u22c5=\n\u03c1\n\u03c1 11),( DDs  (3) \nGiven a panoramic image of a place at position P0(x0, y0) and moving the robot along a \nrectilinear path on an interval [x0 \u2212 \u2206x, x0 + \u2206x], IMA returns values that can be approximately \nrepresented with a centrally peaked distribution, as empirically showed in Section 5.3. To \nexpand the interval where the IMA\u2019s output is higher, the input image from the camera can be \ndigitally zoomed. More precisely, after a normal comparison, the image is zoomed-in and \ncompared again, then zoomed-out and compared once more. Theoretically, including these \nnew comparisons means adding a couple of new peaked curves to the original one. In order to \nhave the same absolute shift |s(\u03c1in, D)| \u2261 |s(\u03c1out, D)| for both the zoom-in and the zoom-out, \nthe following relationship can be easily derived (note that \u03c1in > 1 and 0 < \u03c1out < 1): \n12 \u2212\n=\nin\nin\nout \u03c1\n\u03c1\u03c1  (4) \nThe combination of the three IMA\u2019s outputs is shown in Fig. 4b, where xZin = x0 + s(\u03c1in, D) \nand xZout = x0 + s(\u03c1out, D). The actual output considered for place recognition is the maximum \nof the three curves, as specified also by the pseudo-code in Table 2. \n \n \nx 0 \n  \nx' h \n  \n\u03c1 \u22c5 h \n  \nH  \nX \n  \nf \n  \nf \n  \ns(\u03c1, D) \n    \nx0 xZout xZin x0\u2212\u2206x x0+\u2206x \nIM\nA \no\nu\ntp\nu\nt \nPosition \n0 \n1 \n \n(a)                (b) \nFig. 4  Virtual shift model and IMA\u2019s output using digital zoom \nPre-print version accepted for publication on Robotics and Autonomous Systems, Vol. 56, Issue 2, pp.143-156, February 2008 \n 7 \nTable 2  Digital zoom enhancement for the IMA \n\/* image matching without zoom *\/ \nI \u2190 Inew \nM = IMA(I, Iref) \nBEST_MATCH = M \n\/* image matching with zoom-in *\/ \nI \u2190 zoom-in of Inew \nM = IMA(I, Iref) \nif M > BEST_MATCH \n  BEST_MATCH = M \nend if \n\/* image matching with zoom-out *\/ \nI \u2190 zoom-out of Inew \nM = IMA(I, Iref) \nif M > BEST_MATCH \n  BEST_MATCH = M \nend if \nEND \n \nUnfortunately, in a real environment things are more complicate \u2013 scenes (and objects \nwithin them) always have different distances from the point from which they are observed. \nThe width of the curve in Fig. 4 could be altered significantly changing the direction of \nobservation since the distance of a new scene can be different from a previous one, \ninfluencing therefore xZin and xZout. Because the virtual shift in (3), for a fixed zoom factor \u03c1, \nchanges linearly with the distance D, the region where the recognition holds can be presumed \nto depend somehow on the shape of the room. For example, consider an observation point P \nwithin a small empty room, as illustrated in Fig. 5a. The crosses indicate the displacements \ngiven by the zoom-in when observing in the direction of the arrows; the squares are the \nrelative displacements for the zoom-out. In Fig. 5b the two sets of points for the zoom-in and \nthe zoom-out, obtained by a full rotation about P, are represented by the solid and the dashed \nsquares respectively. If we fix a proper threshold on Fig. 4, over which the IMA output is \nconsidered valid (for example 0.5), we can draw a region for P where the recognition holds, \nas illustrated in Fig. 5c. This region is given by the union of two rectangular areas, one for the \nzoom-in and one for the zoom-out, which are extensions of the previous in Fig. 5b. Indicating \nalso the robot\u2019s position and orientation with a versor, the zoom-out rectangle contains all \nversors pointing to P, while the zoom-in rectangle contains all versors pointing in the opposite \ndirection. \n \n \nP \n    \n \nP \n    \n \nP \n \n(a)          (b)          (c) \nFig. 5  Place recognition with digital zoom \n \nThe observations above suggest some care must be taken when choosing the places to \nrecognize (topological nodes of the map) and the zoom factor to use. In particular, if the \nnodes are too close to each other or if the zoom is too much, the risk of overlaps amongst \n 8 \nthem and the probability of perceptual aliasing increase. Moreover, since the zoom-based \nrecognition works best when observing along those directions passing through the area\u2019s point \nof reference (point P in Fig. 5), a small zoom factor is preferable. In this way, the limited area \nextension increases the probability to be correctly aligned. \n \n \n4. Multiple hypothesis localization \n \nThe main problem using image-based place recognition for localization arises when two \nor more places look very similar and are therefore difficult to distinguish. This is known as \nperceptual aliasing and affects not just vision-based applications, but many other systems \nemploying sensors that provide information about the perceived world (e.g. sonar, laser, etc.). \nIt occurs frequently in indoor environments with similar rooms and furniture like offices. \nThe IMA procedure, described in the previous Section 3.1, is normally able to distinguish \ndifferent places because it considers a significant amount of information coming from the \nvision input. Nevertheless, cases of perceptual aliasing may occur because of occlusions or \nchanges in the scenes originally memorized. To handle this kind of uncertainty, we adopt an \nalgorithm inspired by Markov localization [18]. It starts with a series of hypotheses generated \nby the place-recognition procedure and then chooses the most likely according to the previous \nhypotheses and to the last robot\u2019s movement. \n \n4.1 Notations and assumptions \nLet the state (i.e. position) of the robot at time t be represented by a triplet < xt, yt, \u03d5t >, \nwhere xt and yt are the Cartesian coordinates of the robot\u2019s location and \u03d5 t is its heading \nangle. The couple (xt, yt) belongs to a finite set of two-dimensional points, which is the \ntopological map. The heading angle \u03d5t has continuous values inside the interval [0, 2pi). \nTherefore, the entire set S of possible states contains an infinite number of elements. \nTo make the problem computationally feasible, certain assumptions are imposed. It is \nassumed that the probability distribution at time t of the robot being in a certain position  \n< xt, yt, \u03d5 t > is completely contained in a sub-set St \u2282 S. The elements of St are all the \npositions for which the IMA, at time t, returns a matching-value higher than a certain \nthreshold, plus an additional \u201cvirtual\u201d position given by the odometry. That is, the real \nposition of the robot is always supposed to be one of those recognized by the place \nrecognition or calculated using odometric information; this is justified by the fact that, most \nof the times, the correct position is in effect one of the best recognized with the IMA. The \nnumber of possible states so generated is limited by the nodes of the topological map; \ntherefore, St is a numerable set. \nIn the following sections, the set St is referred to with the letter D and called the \ndestinations\u2019 set (elements d\u2208D). The set St\u22121 is referred to with the letter O and called the \norigins\u2019 set (elements o\u2208O). The set D of destinations at time t becomes then the set O of \norigins at time t + 1. Also, to distinguish the \u201clocal\u201d probability distribution from the one used \nin Markov localization, the word belief is substituted with activity, as in [19]. The believes \nBel(st) and Bel(st\u22121) become then the activities Act(d) and Act(o) respectively (activity of the \ndestination and activity of the origin). \n \n \nPre-print version accepted for publication on Robotics and Autonomous Systems, Vol. 56, Issue 2, pp.143-156, February 2008 \n 9 \n4.2 Virtual destination hypothesis \nThe assumption of considering only the destinations given by the last observation, i.e. the \nIMA output, would be too restrictive. To be sure that the set of estimated positions contains in \neffect the right one, the threshold on the visual-recognition with the IMA should be very high. \nThis would limit the possibility of considering good hypotheses just because some changes in \nthe environment, temporary or permanent, have reduced their distinctiveness. On the other \nhand, a low threshold increases the number of possible destinations but also the probability to \nchoose the wrong one. Even worse is the case when none of the current hypotheses are \ncorrect. To handle this kind of ambiguity, sometimes a \u201czero hypothesis\u201d is used when all the \nother hypotheses are considered wrong. In [20], for example, the authors have a finite set of \nhypotheses generated by new observations, updated simultaneously using Kalman filters. The \nzero hypothesis is used to close the probability space and is kept up-to-date considering the \nuncertainty of the observations. When the probability of such a hypothesis is higher than the \nothers, the robot is in a state of indecision. \nIn our approach, it was found useful to insert a \u201cvirtual destination\u201d, that is, a topological \nnode of the map that is near to the position given by the odometry. In practice, the virtual \ndestination is the closest node, in terms of Euclidian distance, to the previous winning \ndestination plus the last odometric displacement. The heading angle of this new hypothesis is \nalso given by the odometry. The term \u201cvirtual\u201d is used because it is assigned a matching-\nvalue, like all the other destinations generated by an observation, and then treated the same \nway. The assigned matching-value is equal to the threshold chosen for generating the other \nhypotheses, as if an additional place was recognized by the IMA with the least acceptable \nmatch. Finally, at the next update step, the \u201cvirtual-destination\u201d becomes the \u201cvirtual-origin\u201d. \n \n4.3 Action model \nThe first component of Markov localization is the action model. Using the notation \nintroduced before and simply calling a the action a\n t\u22121, the model can be written as follows: \n),|(),|( 11 aodPassP ttt \u2261\u2212\u2212  (5) \nThis expresses the probability that a destination d is reached by performing the action a \nfrom the origin o. This probability is estimated taking into account the location and the \nheading angle of the robot. The action a is simply the displacement given by odometry. In this \nwork, no sophisticated models were used for handling the cumulative errors typical of \nodometry; indeed, its information is always relative to the previous estimated state and \ncorresponds to a short path. Therefore, it is considered reliable enough for being directly used \nin our action model, as described below. \nLet Qo be the position of the origin o (with heading angle \u03d5o) and Qa the position reached \nfrom Qo after the execution of a. Also, Qd is the position of the destination hypothesis d. \nUsing the quantities illustrated in Fig. 6, the action model is calculated as follows: \n)()(),|( \u03d5\u03d5 \u2206\u22c5\u2206= glgaodP l  (6) \nwhere gl(\u2206l) and g\u03d5 (\u2206\u03d5) are two Gaussians: \n2\nmax\n2\n2\nmax2\n1)( l\nl\nl el\nlg \u2206\n\u2206\n\u2212\n\u22c5\n\u2206\u22c5\n=\u2206\npi\n          \n2\nmax\n2\n2\nmax2\n1)( \u03d5\n\u03d5\n\u03d5 \u03d5pi\n\u03d5 \u2206\n\u2206\n\u2212\n\u22c5\n\u2206\u22c5\n=\u2206 eg  (7) \nThe quantities \u2206lmax and \u2206\u03d5max are respectively the maximum \u2206l and \u2206\u03d5 calculated \nbetween the current origin and all the destination hypotheses. \n 10 \n \n\u2206l\nQo \n\u03d5a \n\u03d5d \nQd \nQa \n\u2206\u03d5 \n\u03d5o \n \nFig. 6  Parameters for the action model \n \n4.4 Sensor model \nIn many localization systems, the environment is sensed through low-dimensional \ndevices, like sonar or laser, for which accurate models are already available  \n[21, 22]. Other approaches instead use vision to calculate the robot\u2019s position with respect to \nsome particular features. In [7], for example, an omni-directional image is processed using a \nray-tracing method, simulating a laser range sensor that returns distances of chromatic-\ntransition features. Even in this case, an accurate model is provided, the parameters of which \nare extracted by a modified EM algorithm [23] applied to a set of 2000 sample images. There \nare also other approaches where the sensor models are learned with neural networks using \ndata from both vision and sonar [24-26]. \nThe data given by our image-based place recognition, that is, the IMA\u2019s matching-value, \ndiffers from all the above-mentioned approaches. The sensor model is implicitly \u201cincluded\u201d in \nthe pre-recorded panoramic image, in a way conceptually similar to [27]. Ideally, a new image \nwould return 1 in case of perfect match with a portion of the panorama and would decrease to \n0 as the match deteriorates. Therefore, given the current state, the probability of the \nobservation can be considered the matching-value calculated by the IMA. With the notation \nintroduced earlier and calling v the observation vt, the sensor model can be written as follows: \n)()|()|( dMATCHdvPsvP tt =\u2261  (8) \nwhere MATCH(d) is the value of the variable BEST_MATCH in Table 1 (or Table 2, if \nenhanced by digital zoom) for the destination d. \n \n4.5 Update of the activities \nActivities are updated with the same formulae of Markov localization, but taking into \naccount our previous assumptions. Thus, given a set of destinations d\u2208D and origins o\u2208O, the \nprocedure for calculating new activities, using (6) and (8), is the following: \n1) Prediction: \u2211\n\u2208\n=\u2032\nOo\noActaodPdP )(),|()(  (9) \n2) Update: )()|()( dPdvPdP \u2032=\u2032\u2032  (10) \n3) Normalization: \n\u2211\n\u2208\n\u2032\u2032\n\u2032\u2032\n=\nDd\ndP\ndPdAct )(\n)()(  (11) \n4.6 Odometry reset and visual compass \n \nPre-print version accepted for publication on Robotics and Autonomous Systems, Vol. 56, Issue 2, pp.143-156, February 2008 \n 11 \nAn important role in the selection of the current destination is played by odometry. \nIndeed, the prediction step (9) makes use of the action model (6), which strongly depends on \nthe odometry\u2019s information. This is reset every time an update of the topological position has \nbeen performed. In general, the fact that the topological area is reasonably small, if compared \nto the distance between two consecutive destinations, reduces the effect of the error \nintroduced by the reset. On the other hand, the advantage is significant, since it fixes a limit to \nthe cumulative error of the dead reckoning. \nThe heading angle has a double importance: it affects directly the action model and, since \nrelated to the internal frame of reference of the robot, it influences also the parameter \u2206l. In \nmany applications, instead of considering the heading angle computed using encoders, an \nexternal magnetic compass is mounted on the robot [27, 28]. This has the advantage of being \nindependent from the cumulative errors of odometry, since it gives an absolute direction for \nNorth. Unfortunately, such a device is not immune from errors, which are mainly due to \nmetallic objects in the proximity of the robot. \nIn our approach, another original solution was chosen. Since for every new environment \nan up-to-date map and fresh panoramic images are needed, the latter are always constructed \nstarting from the same direction. This permits the robot to recover its absolute heading using \nequation (1), like if provided with a \u201cvisual compass\u201d. A similar approach, based instead on \nan omni-directional camera, has been recently implemented also in [29]. To limit the error \ncases of using a wrong panoramic image, the odometry\u2019s heading angle is corrected only \nwhen the matching-value of the estimated destination is higher than a given threshold. \n \n4.7 Localization algorithm \nIn order to reduce the computational expense, the whole localization algorithm is executed \nonly after the robot has moved a certain distance or has rotated through a minimum angle. \nThis also has the advantage of effectively generating new different destinations (i.e. different \nstates), reducing instances of failure. The localization algorithm is summarized in the pseudo-\ncode of Table 3. The value \u03b5M is the threshold used for extracting the destinations with the \nbest matching-values; \u03b5\u03d5 is the threshold on the matching-value for correcting the odometry\u2019s \nheading (\u03b5\u03d5 \u2265 \u03b5M); both the quantities \u03b5M and \u03b5\u03d5  are determined empirically. The destination \nwith the higher activity is d*, which corresponds to the estimated state < x*, y*, \u03d5* >; finally, \no\n*\n is the most likely origin, that is, the d* extracted at the previous time-step. \n \nTable 3  Localization algorithm \nCalculate the location given by o* plus odometry displacement \nFind the topological position d0\u2208D closer to such a location \nUse IMA to compare the new image with all the panoramic images in the map \nExtract all the possible destinations d\u2208D with matching-value MATCH(d) > \u03b5M \nif no new destinations are generated with IMA \n  Return the topological position d0 \n  END \nend if \nfor each d\u2208D \n  for each o\u2208O \n   Calculate \u2206l and \u2206\u03d5 \n   Keep track of \u2206lmax and \u2206\u03d5max \n  end for \nend for \n 12 \nfor each d\u2208D \n  Calculate P(d | o, a) with (6) \n  Update the activity (9, 10) \n  Keep track of the destination d* with the higher activity \nend for \nNormalize the activities (11) \nReset the odometry\u2019s coordinates \nif MATCH(d*) > \u03b5\u03d5 \n  Set the odometry\u2019s heading to the angle \u03d5* \nend if \nThe destinations become the next origins, O \u2190 D \nReturn d* \nEND \n \n \n5. Experimental results \n \nIn this section we present the results of experiments conducted in the Neuro-Robots \nLaboratory at the University of Sunderland. This consists of a room approximately 6\u00d76m2, \nwith typical office furniture, and an adjacent corridor connected through a small hall. Along \ntwo sides of the office there are large windows, resulting in particularly challenging light \nconditions. The robot used is an ActivMedia PeopleBot (Fig. 7) provided with a perspective \ncamera and an on-board computer Pentium III 700MHz with 256MB of RAM. Grey-scale \nimages with resolution 72\u00d758 pixels were used and the number of slots chosen for IMA was 8, \nwith thresholds \u03b5M = 0.5 and \u03b5\u03d5 = 0.6. The whole localization system, implemented in C++ \nwithout any particular optimization, worked in real-time on the robot\u2019s computer. The \ntopological position was recovered whenever the robot moved 0.5m or rotated 10\u00b0; the \nmaximum update frequency was 2Hz, which is normally adequate for the tasks of a service \nrobot. We mapped up to 15 locations for the experiments here presented, but the system was \nstill fast enough in other tests with more than 20 different locations. Our approach is therefore \nfeasible for real-time localization in small and medium indoor environments, although larger \nareas could also be covered if more recent and fast hardware was available. \n \n \nFig. 7  Mobile robot used for the experiments \nPre-print version accepted for publication on Robotics and Autonomous Systems, Vol. 56, Issue 2, pp.143-156, February 2008 \n 13 \n5.1 Place recognition performance \nIn this section we present results of the matching algorithm applied to panoramic images. \nFig. 8 shows a panoramic image reconstructed from snapshots taken in the center of the \nlaboratory, using the procedure illustrated in Section 3.2. In particular, we used 12 snapshots \ntaken at intervals of ~30\u00b0. Note that the robot can be rotated quickly, therefore the whole \npanorama\u2019s reconstruction takes less than 1 minute; the algorithm indeed is capable of \naligning the sequence of snapshots correctly, even if the angle step varies of several degrees. \n \n \nFig. 8  Panoramic image of reference \n \nA few moments later, after the panorama was recorded and the software reset, the robot \nwas made to perform a complete rotation on the same point, approximately at 10\u00b0\/s. The \nrelative output of the IMA is the solid line in Fig. 9. It can be seen that the match has a mean \nvalue greater than 0.8. The worst cases, for which IMA returned a value of approximately 0.7, \ncorrespond to the cupboard (~100\u00b0, right part of the panorama) and the shelves (~350\u00b0, left \npart of the panorama). This is probably due to a combination of imprecision in the panoramic \nimage and errors caused by changes in the perspective. On the same graph, it is also \nillustrated the output of a second turn, when the person seating in front of the desk moved \naway. The relative change can be observed on the dashed line of Fig. 9, where the output \ndecreases at about 270\u00b0 (direction where the person was). It is important to note that, even if \nthe output decreased, the position inside the panoramic image, relative to the best match, was \nstill correct, and so it was the heading angle of the visual compass. \nTo test the robustness of the place recognition, we performed a similar experiment the day \nafter using the same panoramic image. Also, to make the experiment more challenging, \nduring the observation a person was walking around the robot about one meter far. The result \nis shown in Fig. 10, where the new matching output (dashed line, relative to the one-day old \npanoramic image) is compared to the previous one (solid line). Despite the small decrease due \nto different light conditions and objects\u2019 position, the main loss of quality is due to the \nabsence of the person sitting on a chair (at ~300\u00b0). In particular, the arrow on the graph \nindicates a point where the estimated position inside the panorama was completely wrong. \nThe four points A, B, C, D are relative instead to the instants when the person, walking \naround the robot, was occluding the camera\u2019s view. \nThe last result about the IMA applied to panoramic images is perhaps the most important. \nAs its main purpose is distinguishing different locations, we wanted to compare the result \nobtained in the last case (old panoramic image and occluding person) with the output obtained \nfrom another location, in the same room but one meter far from the original position. The \nresulting output is represented by the solid line in Fig. 11 and compared to the previous case, \nwhich is the dashed line. Although the output in the new location is not very low, in general it \nis well distinguishable from that one obtained from the original position. Failure cases, like \nthe overlap indicated by an arrow in Fig. 11 (at ~315\u00b0), are situations of perceptual aliasing. \nHere it is clear the need of additional information for resolving the ambiguity, i.e. integrating \nodometry and previous states with a Markov-like approach, as explained in Section 4. \n \n 14 \n0    90 180 270 360\n0\n0.2\n0.4\n0.6\n0.8\n1\nrotation [deg]\nIM\nA\n \nFig. 9  IMA\u2019s outputs for panoramic image \n \n0    90 180 270 360\n0\n0.2\n0.4\n0.6\n0.8\n1\nrotation [deg]\nIM\nA\nA \nB C\nD \n \nFig. 10  IMA\u2019s output the day after with occlusions \n \n0    90 180 270 360\n0\n0.2\n0.4\n0.6\n0.8\n1\nrotation [deg]\nIM\nA\n \nFig. 11  IMA\u2019s output from a different location \n \n5.2 Correction of the visual compass \nThis section illustrates some results regarding the heading angle extraction using the IMA \nand equation (1). In a first experiment, the robot rotated around a position where a panoramic \nimage was previously reconstructed. Data was collected for the heading angle given by \nodometry and by the vision during 10 consecutive rotations, measuring at intervals of 45\u00b0. \nFig. 12 shows the final results. The real angle is on the abscissa and the heading angle \nmeasured by the robot is on the ordinate; the dashed line refers to the odometry and the solid \none is the angle extracted using the IMA as visual compass. It is clear that the angle given by \nodometry becomes unreliable after a few rotations due to the internal cumulative error. At the \nseventh rotation, the odometry\u2019s error already reached \u221245\u00b0 with respect to the real direction. \nInstead, the error of the heading angle given by the visual compass is always between \u00b110\u00b0, \nwithout suffering of any cumulative error. \n \nPre-print version accepted for publication on Robotics and Autonomous Systems, Vol. 56, Issue 2, pp.143-156, February 2008 \n 15 \n0 1 2 3 4 5 6 7 8 9 10\n\u2212180\u00b0\n0\n180\u00b0\nrotation \/ 360\u00b0\nhe\nad\nin\ng \nan\ngl\ne [\nde\ng]\n \nFig. 12  Comparison  of heading angle from odometry and visual compass \n \nAlthough the precision of the visual compass is not sufficient to give a perfect measure of \nthe robot\u2019s orientation, it is still good enough to correct from time to time the odometry and \nhelp with the localization. This is demonstrated for example with the following experiment. \nThe robot performed 10 rounds following the path of Fig. 13a, which shows a schematic of \nthe laboratory and eight topological nodes on a grid of 1m\u00d71m per square. The coordinates \ngiven by the odometry during the robot\u2019s motion are illustrated in Fig. 13b. Because of the \ncumulative error affecting the odometry, the points are spread in the room instead of being \nconcentrated in proximity of the path. Fig. 13c instead shows the same points after the \ncorrection of the visual compass as part of the localization process. It can be noted that the \nnew distribution is much closer to the real path followed by the robot, despite some outliers \ndue to odometry reset or localization errors. \n \n1 \n7 6 \n5 4 \n3 \n2 8 \n                       \n(a)           (b)          (c) \nFig. 13  Reference path and odometry correction with visual compass \n \n5.3 Effect of the digital zoom \nThis section demonstrates that the use of digital zoom increases the place recognition \ncapability, enabling the robot to identify not just an exact point in the environment, but the \nwhole of the surrounding area. The following results are relative to a normal single image of \nreference, rather than a panoramic one, in order to reduce the noise and avoid wrong matches. \nThe same principles however are still valid when using panoramic images. \nIn Fig. 14, the observed scene and relevant graphs for three different zoom factors are \nillustrated. The distance of the robot from the wall in the middle of the scene was about 4m; \nthe robot moved from \u20131m to +1m with respect to the original position. The variation of the \nIMA\u2019s output is shown on the graphs, where the dashed line is the reference, without any \nzoom, and the solid line is relative to the current zoom factor. \n \n 16 \n \n\u22121000 \u2212500 0 500 1000\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\ndistance [mm]\nIM\nA\nzoom 10%  \nfactor 1.1\n \n \n\u22121000 \u2212500 0 500 1000\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\ndistance [mm]\nIM\nA\nzoom 20%  \nfactor 1.2\n \n\u22121000 \u2212500 0 500 1000\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\ndistance [mm]\nIM\nA\nzoom 30%  \nfactor 1.3\n \nFig. 14  IMA\u2019s performances varying the digital zoom \n \nTwo important considerations arise from observation of the graphs. First, the output is \nsimilar to the combination of three different peaked distributions, as anticipated. \nUnfortunately, the amplitude of the external peaks decreases considerably when the zoom \nfactor augments. Despite the fact that, when translating, it is not easy to keep the robot \nconstantly on the same direction, the main reason for this decrease is the loss of resolution \nimplicit in the zoom process. The second concerns the gaps between the external picks and \nthat one in the middle. From these it can be seen that the internal (local) minimum goes \nquickly below 0.5 already with a zoom of 20%. This is because the peaked curves are not very \nwide and the distance from the observed scene is quite long \u2013 recall from (3) that the virtual \ndisplacement obtained with the digital zoom is directly proportional to this distance. Note also \nthat the formula given in (3) was an approximation to an ideal case, but in the real world the \nvirtual displacement is influenced by several other factors. For example, in case of a zoom of \n20% (\u03c1in = 1.2) the hypothetical displacement \u2206x for a distance D = 4m should be 0.67m; in \npractice, the graph shows two external peaked curves not further than 0.5m from the origin. \nAgain, the higher the zoom factor, the bigger the error. \nWith the next result, we would like to demonstrate also how the distance of the observed \nscene influences the effect of the digital zoom on the recognition area. In Section 3.4 we \nstated that the shape of the recognized region depends on the environment because of the \nlinear relation (3) between virtual shift and distance of the scene. According to that, we would \nexpect a reduction on the width of the recognition\u2019s curve when observing a closer scene. \nTherefore we repeated the same test illustrated above, but this time placing the robot just 1m \nfar from the closest obstacles. The result for a zoom of 10% is illustrated in Fig. 15. \nComparing this new graph with the previous one in Fig. 14 (for zoom 10%), it is evident that \nthe recognition interval becomes smaller, and this is due to the decrease of the relative virtual \ndisplacement for the current observation. In general then, the region where the zoom-based \nrecognition holds becomes thinner (wider) in the direction of a closer (farther) scene. \nPre-print version accepted for publication on Robotics and Autonomous Systems, Vol. 56, Issue 2, pp.143-156, February 2008 \n 17 \n \n \n\u22121000 \u2212500 0 500 1000\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\ndistance [mm]\nIM\nA\nzoom 10%  \nfactor 1.1\n \nFig. 15  IMA and digital zoom on a closer scene \n \nApart from these limitations, if compared to the output without any zoom, even a value of \n10% is a big improvement of the place recognition. For the next localization experiment, this \nwas the value used. First, we disabled the digital zoom procedure on the localization system \nand moved the robot along the path of Fig. 13a, carefully passing through the centers of each \nof the eight topological nodes. After one turn and 19 updates of the localization, we had an \nerror due to the estimation of a wrong place, which was however an adjacent node of the \ncorrect one (at less than 1.5m, about the length of a square\u2019s diagonal on the grid of Fig. 13a). \nStill without digital zoom, the robot performed a new turn following the same path, but this \ntime avoiding the centers of the nodes. As expected, the number of localization updates \ndecreased to 14 because the IMA output was often below the threshold \u03b5M. Furthermore, 4 of \nthose updates were wrong, with a couple of estimations involving nodes more than 1.5m far \nfrom the correct locations. Finally, we moved the robot on the same path, avoiding again the \nnodes of the centers but making use of the digital zoom procedure. As reported in Table 4, in \nthis case the localization succeeded, generating 22 correct estimations without any error. \nThese results show that the digital zoom was an essential part of the localization system; \ntherefore it was always used in the following experiments. \n \nTable 4  Errors with and without digital zoom \nCase no zoom & center no zoom & no center zoom & no center \nUpdate steps 19 14 22 \nErrors due to adjacent nodes: 1 2 0 \nErrors due to distant nodes: 0 2 0 \nTotal number of errors: 1 4 0 \n \n5.4 Localization performances in a dynamic environment \nTo test the localization system in a dynamic environment, updated panoramas were used, \nall reconstructed the same day. The robot performed 10 rounds following the same path with \neight nodes in Fig. 13a. During the experiments, the robot was always avoiding the exact \ncentres of the topological position, so to force the use of the digital zoom for place \nrecognition. In the meanwhile, two people were continuously moving around the robot, \nsometimes walking or standing in front of the camera and sometimes simply sitting on chairs. \nExamples of such situations are shown in the robot\u2019s snapshots of Fig. 16. \n 18 \n       \n(a)          (b)          (c) \nFig. 16  Snapshots of error cases \n \nThe results presented in Table 5 are encouraging; indeed there were only 3 incorrect \nlocalizations out of a total of 253 update steps. Two of them happened because of people \nobstructing the scene, so the robot computed that it was at node 3 instead of the correct \npositions of nodes 5 and 6. The reduced video information obtained from the real positions \nhas not been sufficient to resolve the perceptual aliasing, even with the odometry\u2019s help. The \nrelative robot\u2019s snapshots are shown in Fig. 16a and 16b. In the third error case, the robot was \nat node 7, but the localization estimated node 1, although nobody was obstructing the view at \nthat moment. The reason is probably the poor quantity of features in that particular scene, \nwhich can be seen in Fig. 16c. However, the three errors, in terms of distance from the correct \nnode, were all less than 1.5m; that is, .in the worst case the estimation was a topological node \nadjacent to the correct one. \nFor comparison, in Table 5 are reported also the error cases for the same localization \nexperiment using only place recognition, without the Markov updating process. As we can \nsee, the performance decrease is considerable, with a total number of 37 localization errors. \nAmong these, 34 were assigned to adjacent nodes, less than 1.5m far from the correct \nposition, instead 3 errors involved more distant nodes. From these results, it is obvious the \nbenefit given by the Markov update to the localization system. \n \nTable 5  Errors with and without Markov update \nCase with Markov update without Markov update \nUpdate steps 253 253 \nErrors due to adjacent nodes: 3 34 \nErrors due to distant nodes: 0 3 \nTotal number of errors: 3 37 \n \n \n5.5 Localization in a bigger environment \nTo show the performance of the localization system in a bigger environment, we used an \nadjacent corridor, connected to the laboratory through a small entry and both already mapped \nseveral days before. The new locations are poor of features and quite narrow, just 2\u00d72m2 for \nthe entry and about 2\u00d710m2 for the corridor, both illuminated by artificial light. Fig. 17 shows \na panoramic image taken from the corridor and Fig. 18 illustrates the new map. \n \n \nFig. 17  Panoramic image of the corridor (from node 13 of the map) \nPre-print version accepted for publication on Robotics and Autonomous Systems, Vol. 56, Issue 2, pp.143-156, February 2008 \n 19 \n1 \n7 6 \n5 4 \n3 \n2 8 \n9 \n10 11 12 13 14 15 \n \nFig. 18  Map of laboratory and corridor with reference path. The gray line is the odometry. \n \nThe robot followed the new path drawn in Fig. 18, starting from node 1 until node 15, at \nthe end of the corridor, and then back to node 1. In the figure is also illustrated the path given \nby the odometry when performing such a trip, showing clearly the effects of its cumulative \nerror. Like for the previous case, the robot always avoided the exact center of the topological \nplaces; also, all the parameters of the localization algorithm (slots, zoom, thresholds, etc.) \nwere the same adopted for the laboratory. We repeated the trial 3 times, collecting data for a \ntotal number of 202 update steps, and the results are reported in Table 6. We can notice an \nincrement of the error cases, which still represent however a small percentage of the total \nsteps\u2019 number. The localization\u2019s failures were distributed on the whole environment, with a \nrelatively high concentration (4 errors) inside the entry room, at node 10. The reason is that \nthe panoramic image of this room was taken with two doors closed, while during the \nexperiment the same doors were open. In such a small place, these doors took almost half of \nthe panoramic image and their state influenced much the recognition performance. However, \nthe localization in general was very reliable and the few errors were always limited to \nadjacent nodes, less than 1.5m far from the correct ones. \n \nTable 6  Errors in a bigger environment \nUpdate steps 202 \nErrors due to adjacent nodes: 9 \nErrors due to distant nodes: 0 \nTotal number of errors: 9 \n \n \n 20 \n6. Conclusions \n \nAn appearance-based localization system for indoor environments has been developed, \nmaking use of a simple unidirectional camera and odometry information. The approach is \nstrongly based on a novel place recognition algorithm (IMA) enhanced by digital zoom. The \nsame algorithm also permits the generation of panoramic images used for mapping the \nenvironment and, from these, to estimate the absolute robot orientation with a visual compass. \nThe latter in particular gave promising results, considering also the hardware limitations that \nhad to be dealt with. Finally, within a probabilistic framework, odometry is integrated with \nthe visual information to resolve cases of ambiguity. The experiments presented show the \nrobustness of the approach, even in case of dynamic environments, making the localization \nsuitable for service-robot applications. \nThere are two main topics that should be explored in the future: (i) the automatic update \nof panoramic images, and (ii) the use of incremental digital zoom. The first would boost the \nplace recognition and would be a natural step towards a complete system of self-localization \nand map-learning (SLAM). The second is an innovative technique that has just been \nintroduced, but which shows great potential to improve the effectiveness of the localization. \nBy adding incremental digital zoom to the frame captured by the camera, \u201coff node\u201d map \nlocations could be identified. It would be worthwhile extending this technique to support \ninterpolation of location between map nodes. \n \n \nReferences \n \n[1] S. Thrun, M. Bennewitz, W. Burgard, A.B. Cremers, F. Dellaert, D. Fox, D. H\u00e4hnel, C. \nRosenberg, N. Roy, J. Schulte, D. Schulz, MINERVA: A Second-Generation Museum Tour-\nGuide Robot, in: Proc. of the IEEE Int. Conf. on Robotics and Automation (ICRA\u201999), 1999, v. \n3, pp. 1999-2005. \n[2] W. Burgard, A.B. Cremers, D. Fox, D. H\u00e4hnel, G. Lakemeyer, D. Schulz, W. Steiner, S. Thrun, \nExperiences with an interactive museum tour-guide robot, Artificial Intelligence 114 (1-2) \n(1999), pp. 3-55. \n[3] B.A. Maxwell, L.A. Meeden, N. Addo, L. Brown, P. Dickson, J. Ng, S. Olshfski, E. Silk, J. \nWales, Alfred: The Robot Waiter Who Remembers You, in: Proc. of the AAAI Workshop on \nRobotics, Florida, USA, 1999. \n[4] G. Gini, A. Marchi, Indoor Robot Navigation with Single Camera Vision, in: Proc. of Pattern \nRecognition in Information Systems (PRIS 2002), 2002, pp. 67-76. \n[5] N.X. Dao, B.J. You, S.R. Oh, M. Hwangbo, Visual Self-Localization for Indoor Mobile Robots \nUsing Natural Lines, in: Proc. of the IEEE\/RSJ Int. Conf. on Intelligent Robots and Systems \n(IROS\u201903), Las Vegas, USA, 2003, pp.1252-1257. \n[6] S. Enderle, H. Folkerts, M. Ritter, S. Sablatn\u00f6g, G. Kraetzshmar, G. Palm, Vision-based Robot \nLocalization using Sporadic Features, in: Proc. of the Int. Workshop on Robot Vision, \nAuckland, New Zealand, 2001, pp. 35-42. \n[7] E. Menegatti, A. Pretto, E. Pagello, A New Omnidirectional Vision Sensor for Monte-Carlo \nLocalization, in: Proc. of RoboCup Symposium, 2004, pp. 97-109. \n[8] I. Ulrich, I. Nourbakhsh, Appearance-Based Place Recognition for Topological Localization, in: \nProc. of the IEEE Int. Conf. on Robotics and Automation (ICRA\u201900), San Francisco, USA, \n2000, pp. 1023-1029. \nPre-print version accepted for publication on Robotics and Autonomous Systems, Vol. 56, Issue 2, pp.143-156, February 2008 \n 21 \n[9] H. Andreasson, T. Duckett, Topological Localization for Mobile Robots using Omni-directional \nVision and Local Features, in Proc. of the 5th Symposium on Intelligent Autonomous Vehicles \n(IAV), Portugal, 2004. \n[10] D. G. Lowe, Distinctive Image Features from Scale-Invariant Keypoints, International Journal \nof Computer Vision, 60, 2 (2004), pp. 91-110. \n[11] A. Torralba, K. P. Murphy, W. T. Freeman, M. A. Rubin, Context-based vision system for place \nand object recognition, IEEE Int. Conf. on Computer Vision, France, 2003. \n[12] J.S. Gutmann, W. Burgard, D. Fox, K. Konolige, An Experimental Comparison of Localization \nMethods, in: Proc. of the Int. Conf. on Intelligent Robots and Systems (IROS\u201998), 1998, pp. \n736-743. \n[13] J.S. Gutmann, D. Fox, An experimental Comparison of Localization Methods Continued, in: \nProc. of the IEEE\/RSJ Int. Conf. on Intelligent Robots and Systems (IROS\u201902), 2002, pp. 454-\n459. \n[14] S. Kristensen, P. Jensfelt, An Experimental Comparison of Localization Methods, the MHL \nSessions, in: Proc. of the IEEE\/RSJ Int. Conf. on Intelligent Robot and Systems (IROS\u201903), \n2003, pp. 992-997. \n[15] A.C. Bovik, C.W. Chen, D.B. Goldgof, T.S. Huang, Advances in Image Processing and \nUnderstanding, World Scientific, Singapore, 2002. \n[16] K. Zuidervel, Graphics gems IV, Academic Press Professional, Inc., San Diego, 1994. \n[17] D. H. Ballard, C. M. Brown, Computer Vision, Prentice Hall, New York, 1982. \n[18] D.  Fox, Markov Localization: A Probabilistic Framework for Mobile Robot Localization and \nNavigation. Doctoral Thesis. Institute of Computer Science III, University of Bonn, Germany, \n1998. \n[19] D.  Filliat, J. A. Meyer, Global localization and topological map learning for robot navigation, in \nProc. of the 7th Int. Conf. on Simulation of Adaptive Behavior (SAB\u201902), 2002, pp. 131-140. \n[20] P. Jensfelt, S. Kristensen, Active global localization for a mobile robot using multiple \nhypothesis tracking, IEEE Trans. on Robotics and Automation, 17(5) (2001), pp. 748-760. \n[21] W. Burgard, D. Fox, D. Hennig, T. Schmidt, Estimating the absolute position of a mobile robot \nusing position probability grids, in: Proc. of the 13th National Conf. on Artificial Intelligence \n(AAAI\u201996), Portland, Oregon, USA, 1996, pp. 896-901. \n[22] H.P. Moravec, Sensor fusion in certainty grids for mobile robots, AI Magazine (1988) pp. 61-74. \n[23] A.P. Dempster, N.M. Laird, D.B. Rubin, Maximum likelihood from incomplete data via the EM \nalgorithm, Journal of the Royal Statistical Society, vol. 39 B (1977), pp. 1-38. \n[24] S. Thrun, Finding landmarks for mobile robot navigation, in: Proc. of the IEEE Int. Conf. on \nRobotics and Automation (ICRA\u201998), 1998, pp. 958-963. \n[25] S. Thrun, Bayesian Landmark Learning for Mobile Robot Localization, Machine Learning 33 \n(1998), pp. 41-76. \n[26] S. Oore, G. E. Hinton, G. Dudek, A mobile robot that learns its place, Neural Computation 9(3) \n(1997), pp. 683-699. \n[27] T. Duckett, U. Nehmzow, Mobile robot self-localization using occupancy histograms and a \nmixture of Gaussian location hypotheses, Robotics and Autonomous Systems 34 (2001), pp. \n117-129. \n[28] T. Duckett, S. Marsland, J. Shapiro, Fast, On-Line Learning of Globally Consistent Maps, \nAutonomous Robots 12 (2002), pp. 287-300. \n[29] F. Labrosse, Visual compass, in Proc. of Towards Autonomous Robotic Systems (TAROS\u201904), \n2004, University of Essex, Colchester, UK. \n \n 22 \nNicola Bellotto received his Laurea degree in Electronic Engineering \nfrom the University of Padua in Italy. He is currently a PhD candidate \nin robotics at the University of Essex in UK. His doctoral thesis focuses \non Multisensor Data Fusion for Human-Robot Interaction. Other \nresearch interests include robot localization, computer vision and \nembedded systems programming. Before joining the Human Centred \nRobotics Group in Essex, he has been an active member of the \nIntelligent Autonomous System Laboratory in Padua and of the Centre \nfor Hybrid Intelligent Systems at the University of Sunderland. He \ngained also several years of professional experience in Italy and UK as embedded systems \nprogrammer and software developer for entertainment robotics. \n \nKevin Burn received a BSc degree in Mechanical Engineering from \nNewcastle University in 1984 and an MEng from Durham University in \n1986. After working in the power industry for several years he returned \nto Newcastle University as a Research Associate and was awarded his \nPhD in 1994 for research into robotics and teleoperator systems. He is \nnow a Senior Lecturer at Sunderland University in the School of \nComputing and Technology, where his research interests are in robotics \nand intelligent systems. \n \n \nProfessor Eric Fletcher graduated from the University of Hull with an \nhonours degree in Physics and Pure Mathematics in 1965. He obtained a \nPh.D. in Geophysics from the University of Newcastle upon Tyne in \n1975. Subsequently he did research in Solid State Physics specialising \nin Auger Spectroscopy of Glass surfaces. In 1986 he transferred to \nComputer Science initially specialising in Mathematical Modelling \nSimulation and Decision Support Systems. In recent years he has \nconcentrated on image analysis applied to biological systems and traffic \nflow monitoring. He has supervised 14 Ph.D. students and has over 80 \npublications in Computer Science and Solid State Physics. In December 2003 he retired from \nthe full time post of Professor of Applied Computing at the University of Sunderland and is \nnow Emeritus Professor of Computing and is continuing his interests in image analysis. \n \nProfessor Stefan Wermter holds the Chair in Intelligent Systems at the \nUniversity of Sunderland, UK and is the Director of the Centre for \nHybrid Intelligent Systems. His research interests are in Intelligent \nSystems, Neural Networks, Cognitive Neuroscience, Hybrid Systems, \nLanguage Processing, and Learning Robots. He has an MSc from the \nUniversity of Massachusetts, USA and a PhD and Habilitation from the \nUniversity of Hamburg, Germany, both in Computer Science and was a \nResearch Scientist at Berkeley, USA before joining the University of \nSunderland in 1998. Professor Wermter has written or edited five books \nand published about 130 articles on this research area, including books like \"Hybrid \nConnectionist Natural Language Processing\" or \"Connectionist, Statistical, and Symbolic \nApproaches to Learning for Natural Language Processing\", \"Hybrid Neural Systems\", \n\"Emergent Neural Computational Architectures based on Neuroscience\" and \u201cBiomimetic \nNeural Learning for Intelligent Robots\u201d. \n"}