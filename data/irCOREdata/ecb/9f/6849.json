{"doi":"10.1080\/0968776960040206","coreId":"6849","oai":"oai:generic.eprints.org:213\/core5","identifiers":["oai:generic.eprints.org:213\/core5","10.1080\/0968776960040206"],"title":"A computer\u2010aided continuous assessment system","authors":["Turton, B. C. H."],"enrichments":{"references":[{"id":193192,"title":"Academics' use of courseware materials: a survey',","authors":[],"date":"1994","doi":"10.3402\/rlt.v1i1.9462","raw":"Laurillard, D., Swift, B. and Darby, J. (1994), 'Academics' use of courseware materials: a survey', Association for Learning Technology Journal, 1 (1), 4-15.","cites":null},{"id":193190,"title":"Early experiences of computer-aided assessment when teaching computer programming',","authors":[],"date":"1994","doi":"10.1080\/0968776930010206","raw":"Benford, S., Burke, E., Foxley, E., Gutteridge, N. and Mohd Zin, A. (1994), 'Early experiences of computer-aided assessment when teaching computer programming', Association for Learning Technology Journal, 1 (2), 55-70.","cites":null},{"id":193189,"title":"Teaching and learning in an expanding higher education system: executive summary',","authors":[],"date":"1993","doi":null,"raw":"Committee of Scottish University Principals (1993), Teaching and learning in an expanding higher education system: executive summary', The CTISS File, 15, 5-6.","cites":null},{"id":193191,"title":"The database format question: an alternative to multiple choice and free format for computer-based testing',","authors":[],"date":"1990","doi":"10.1016\/0360-1315(90)90032-3","raw":"Fry, J. (1990), 'The database format question: an alternative to multiple choice and free format for computer-based testing', Computers and Education, 14 (5), 391-401.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"1996","abstract":"A high\u2010quality assessment system should have the following attributes: rapid feedback to the students, appropriate and detailed feedback, and an effective grading system which provides an accurate overall grade as well as information which identifies the student's weak areas. As stafflstudent ratios worsen, providing such a system will become more difficult and consequently computer assistance in this task is becoming more attractive. This paper describes a Computer\u2010Aided Assessment (CAA) system based on a modified version of the multiple\u2010choice questionnaire. The CAA has been designed to be used in continuous assessment, with features that discourage plagiarism and provide appropriate feedback Over a hundred students were tested using this CAA and the results were compared with a more traditional assessment system. In addition, questionnaires were used to assess the student's reaction to the CAA. The results were highly satisfactory, and a more advanced version of the original software is under consideration","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/6849.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/213\/1\/ALT_J_Vol4_No2_1996_A_computer_aided_continuous_as.pdf","pdfHashValue":"9d6c94d18fe112ede6f01e8c10af864fcd56d5cb","publisher":"Universit of Wales Press","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:213<\/identifier><datestamp>\n      2011-04-04T09:23:07Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/213\/<\/dc:relation><dc:title>\n        A computer\u2010aided continuous assessment system<\/dc:title><dc:creator>\n        Turton, B. C. H.<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        A high\u2010quality assessment system should have the following attributes: rapid feedback to the students, appropriate and detailed feedback, and an effective grading system which provides an accurate overall grade as well as information which identifies the student's weak areas. As stafflstudent ratios worsen, providing such a system will become more difficult and consequently computer assistance in this task is becoming more attractive. This paper describes a Computer\u2010Aided Assessment (CAA) system based on a modified version of the multiple\u2010choice questionnaire. The CAA has been designed to be used in continuous assessment, with features that discourage plagiarism and provide appropriate feedback Over a hundred students were tested using this CAA and the results were compared with a more traditional assessment system. In addition, questionnaires were used to assess the student's reaction to the CAA. The results were highly satisfactory, and a more advanced version of the original software is under consideration.<\/dc:description><dc:publisher>\n        Universit of Wales Press<\/dc:publisher><dc:date>\n        1996<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/213\/1\/ALT_J_Vol4_No2_1996_A_computer_aided_continuous_as.pdf<\/dc:identifier><dc:identifier>\n          Turton, B. C. H.  (1996) A computer\u2010aided continuous assessment system.  Association for Learning Technology Journal, 4 (2).  pp. 48-60.  ISSN 0968-7769     <\/dc:identifier><dc:relation>\n        10.1080\/0968776960040206<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/213\/","10.1080\/0968776960040206"],"year":1996,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"A computer-aided continuous\nassessment system\nB. C. H. Turton\nSchool of Engineering University of Wales Cardiff\nA high-quality assessment system should have the following attributes: rapid feedback to the students,\nappropriate and detailed feedback, and an effective grading system which provides an accurate overall\ngrade as well as information which identifies the student's weak areas. As stafflstudent ratios worsen,\nproviding such a system will become more difficult and consequently computer assistance in this task is\nbecoming more attractive. This paper describes a Computer-Aided Assessment (CAA) system based\non a modified version of the multiple-choice questionnaire. The CAA has been designed to be used in\ncontinuous assessment, with features that discourage plagiarism and provide appropriate feedback\nOver a hundred students were tested using this CAA and the results were compared with a more\ntraditional assessment system. In addition, questionnaires were used to assess the student's reaction to\nthe CAA. The results were highly satisfactory, and a more advanced version of the original software is\nunder consideration.\nIntroduction\nUniversities within the United Kingdom have had to cope with a massive expansion in\nundergraduate student numbers over the last five years (Committee of Scottish University\nPrincipals, 1993; CVCP Briefing Note, 1994). In addition, there has been a move towards\nmodularization and a closer monitoring of a student's progress throughout the year. Since\nthe price\/performance ratio of computer systems has continued to improve, Computer-\nAssisted Learning (CAL) has become an attractive option. (Fry, 1990; Benford et al, 1994;\nLaurillard et al, 1994). To this end, the Universities Funding Council (UFQ has funded the\nTeaching and Learning Technology Programme (TLTP). However universities also have a\nduty to assess as well as to teach. This paper describes a Computer-Aided Assessment\n(CAA) system capable of assisting in grading students and providing feedback. In this\nparticular case, a continuously assessed course (Low-Level Languages) of over 100 students\nis considered. Typically, three man-days are required to mark one assessed piece of\ncoursework from the students in this class. Any feedback on how the questions were dealt\nwith by the student are of necessity brief. Most of the feedback is provided in a tutorial\nsession that covers the pitfalls encountered by the majority of the students.\n48\nALT-J Volume 4 Number 2\nA CAA solution was sought which covered the following points:\n\u2022 rapid feedback;\n\u2022 appropriate feedback;\n\u2022 breakdown of the strengths and weaknesses of the student;\n\u2022 breakdown of the strengths and weaknesses of the class\n\u2022 a system that makes plagiarism difficult;\n\u2022 effective assessment of the student;\n\u2022 reasonable resource implications.\nRapid and appropriate feedback is clearly necessary for a course lasting only 12 weeks.\nAny assessment method will obviously be more useful for teaching purposes if it can form\na profile of the student and the class. Plagiarism must be guarded against since\ncontinuous assessment precludes examination conditions. All these conditions must be\nmet within the resources available.\nThe Computer-Aided Assessment system\nIn order to facilitate rapid marking, a fixed set of solutions was deemed to be the only\ncheap and effective means. Traditionally, this has been thought to be a rather limited\nsystem as feedback can be poor. In order to combat this problem, each available choice\nwithin a question was designed to capture a particular misunderstanding of the subject.\nThis discipline was highly informative as it requires the examiner to consider the common\nerrors made by students. By storing information on common mistakes, a detailed\nexplanation can be given to the student which deals with the specific area of\nmisunderstanding that led him or her to choose a particular, inappropriate answer. An\nexplanation of the correct answer is also given to ensure that those who have guessed the\nanswer can still benefit from the correct explanation.\nNam\u00ab .\nPaper No.\nSfiiH\u00abntNo..\nStudent\n\/\nsits\nStudent\nExam Paper\nClass\nPaper No.\ncontains ^\nOue.stion No.\n1 \/Paper\nQuestions *\" are on\nAnswer Text-\nVeracity\nQuestion\nCategory\nJ\nisoftyj\nQuestion\n\/\n\"Category No.\n.Category Text\nje\n[Ouestion No.\n.Question Text\n\u2022.Category No.\nhas the answer\nAnswer Answpr Nn\n..Question No\nFigure \/: Entity Relationship diagram for CAA\nT\nExplanation\n49\n8. C H. Turton A computer-aided continuous assessment system\nEach question must also be classified so that a summary can be made of the student, and\nclasses, for different categories. Such classification enables the computer to create\nsummaries for students and classes. Figure 1 shows an entity\/relationship diagram for the\nsituation as described so far.\nThe key disadvantage of this system is its susceptibility to plagiarism. Unlike the case with\ntraditional assessments, plagiarism would be impossible to detect between students taking\nthe same paper. An alternative would be to set different questions for each student, but it\nwould be impractical to design a paper with 10 different questions for over 100 students.\nOne solution would be a bank of multiple-choice questions from which 10 questions are\nselected for each student; however, this bank would have to be very large, of the order of\n100 questions, to ensure that on average any two papers would have no more than one\nquestion in common. In addition it would be difficult to ensure that a large range of\nquestions maintained roughly the same difficulty and subject coverage.\nThe solution adopted was to have a small bank of questions from which 10 were selected\nfor each student. Each question has a number of different ways of being asked, a number\nof correct answers, and a number of false answers. The question asked of the students is\none instance of the text for that question with one true answer, a number of randomly\nselected false answers from the set of false answers to that question, and a 'don't know'\nanswer. This combines the advantages of having a small set of questions yet ensures a\nvariety of combinations such that plagiarism is discouraged. For example, a simple\nquestion designed to check a student's ability to perform base conversion could be stored\nas follows:\nChoice\nQuestion version 1: Which value is equivalent to the number 1125.\nQuestion version 2: AM 0123 = 0, select Zfrom the choices below.\nAnswer (true) 1000002\nAnswer (true) 447\nAnswer (false) 11210 or 101210 (student failed to perform conversion, guesswork)\nAnswer (false) 3323 (student failed to notice digit range 0-2 for base 3)\nAnswer (false) 25g (student transposed the digits)\nQuestion\nThere are twelve combinations for a simple three-option multiple-choice questionnaire;\none possibility is:\nX-10123 = 0, select Zfrom the choices below.\n(a)256\n(b)447\n(c)3323\n50\nALT-J Volume 4 Number 2\nAssume q questions are required for m students, with c choices per question not including\n'don't know'. A formula will be given for each case followed by a numeric result assuming\n100 students, 10 questions and four choices.\nScenario 1: Entirely different questions for each student\nProbability of two papers not duplicating any question\nNumber of questions to be prepared = q.m\nNumber of answers to be prepared = q.m.c\n= 1.00\nExample = 1000 questions\nExample = 10000 answers\nScenario 2: A bank of questions {b questions in the bank, assume 6=100 for the example)\nProbability of two papers sharing s questions, =\nresults for the example numbers.\nNumber of questions to be prepared = b\nNumber of answers to be prepared = b.c\nI\n bCq. Figure 2 shows the\nExample =100 questions\nExample = 400 answers\n(4 per question)\n\u2022aI\n0 1 2 3 4 5 6 7 8 9 10\nNo. of shared questions\nfigure 2: Probability that two papers will share zero or more questions\nScenario 3: In this scenario, a distinction will be drawn between the whole question\n(questions and choices) and the text of the question itself (not including choices). The\nwhole question will be referred to as the 'question', and the text of the question itself will\nbe referred to as the 'question-text'. In this scenario, there is a bank of b questions with n\nversions of the question-text for each question. There are t true and \/ false versions of\nanswers to that question from which c choices are given to the student, only one of which\nis true. A variety of question-texts and answers can be used to create a single question\nwith c choices of answer. Clearly some combinations of answers and question-texts will be\nso similar that the questions are for practical purposes identical, while other combinations\nwill be very different. In order to provide a fair comparison with the other techniques,\nquestions which are very similar should be considered to be the same question. Two\nmethods of grouping questions together as being essentially identical can be defined:\n51\n8. C H. Turton A computer-aided continuous assessment system\nOption (i): If two questions share the same 'true answer' choice and the same question-\ntext, they Will be deemed to be essentially the same question.\nOption (ii): If two questions share the same question-text and choices, in any order, they\nwill be deemed to be essentially the same question.\nThe first option assumes that a student is cheating by acquiring someone else's 'perfect'\npaper in addition to his or her own, and so can copy where the question-text and the true\nanswer appear on both papers. The second option is strictly correct in that the questions\nare different if the choices or question-text are different. The author of this paper prefers\noption (i) as a measure of similarity, as it more closely reflects the possibility of\nplagiarizing another student's work. In the equation that follows, the number of distinct\nversions of question is denoted by the symbol v. This number can be calculated as follows:\nfor option (i) v=t.n; for option (ii) v=t . (^i)C\/. n (see Figure 3). The value of v will be\nsubstituted in the probability formula given later.\nI\n\u20228\n-Option i)\nOption ii)\n\u20224-\n0 1 2 3 4 5 6 7 8 9 1 0\nNo. of shared questions\nFigure 3: Probability of shared questions {b=25, q=IO, v=4, option i) v=4, option ii) v=80}\nThe formula for calculating the probability of two papers having i questions in common,\nof which s are the same variant and consequently can be easily copied, requires some\nexplanation. Figure 4 represents the simple case where there is only one version of each\nquestion.\nIn this case^ the formula is simply G ^ C ^ .\n qCs)\/hCr The reader must remember that there\nis a bank of b questions of which q are on the paper which is being used for plagiarism\npurposes. So b-qCq-s represents the number of ways of choosing (q-s) non-duplicated\nquestions from the possible (b-q) non-duplicated questions in the question bank.\n qCs\nrepresents the number of ways of choosing j-duplicate questions from the set of q\nquestions which could be duplicated. So the total number of ways of producing a paper\nwhich has exactly s duplicated questions is (j^qCq_s. ?CV). Since the total number of ways\n52\nALT-J Volume 4 Number 2\nBank of b questions\n1 1 1 ,\nq questions on the first paper which the\n. 1 - 1 ;\ns same questions\nI - A\nstudent is attempting to copy from\nPaper 1\n1 ' ! * | j Paper 2\nq questions on the paper to be\ncompleted\nFigure 4: Pictorial representation of questions chosen from a bank\nof producing q questions from b is jC9, the probability of doing so by chance must be\n(j>-qCq-s \u2022 qCs)fbCq- In scenario 3 this is complicated by the fact that even if a question is\nduplicated, it may be a different variant to the one chosen on the original paper.\nConsequently, it will be assumed that of the q questions, i are the same question and s of\nthem are the same variant of the same question.\nBank of b questions\ni \u00bb 1 M. - i .\u2022*\nq questions on the first paper which the student is attempting to copy from\n\\ l 1\ni shared questions ^-shared\nquestions\nsame\nversion\nPaper 1\n\u2022 ; | - v ] Paper 2\nq questions on the paper to be\ncompleted\nFigure 5: Pictorial representation of questions chosen from a bank with different versions of question\nThe formula must now be split into three sections: the number of ways of choosing (q-i)\nnon-duplicated questions from (b-q) questions, multiplied by the number of ways of\nselecting (is) duplicate questions but different variants from q questions, times the\nprobability of that event, multiplied by the number of ways of selecting s questions from\nthe remaining (q-(i-s)) questions, times the probability of that event. Now the equation\nmust be summed for all possible values of \/, and divided by the total number of ways of\nchoosing q questions from b. This gives a probability of any two papers sharing s\n'identical' questions of:\n6. C H. Turton A computer-aided continuous assessment system\nAssume for this example that there are two versions (\u00ab=2) of the question-text for each\nquestion, two true answers (f=2), and six false ones (f=6) from which four options (c=4)\nare chosen. Finally, let the bank have 25 questions (6=25). A particular question may\nonly appear once on a particular paper, irrespective of the way it is expressed.\nNumber of questions to be prepared = b.q Example = 50 questions\n(25 questions each with two different question-texts)\nNumber of answers to be prepared = b.(t+f) Example = 200 answers\n(Eight for any one question, six false, two true)\nIn order to ensure that the frequency of a particular choice being on a question is the\nsame for both true and false answers, t <=f\/ (c-1) which is true for the example given.\nOnce this bank of questions and answers has been created, there is no requirement to\ncreate a new bank for the next year, as comparison of papers between students from\ndifferent years will not easily yield the correct answers.\nThis method of producing question papers from a bank of questions can be organized as\ndepicted in the Entity Relationship (E\/R) diagram shown in Figure 6. The key differences\nbetween the previous E\/R diagram and this one is the different versions of texts for\nquestions, multiple versions of answers, and an individual paper for each student.\nr^ateg orv No.\nCategory TexL-\nAnswer Text.\nExplanation\u2014\nAnswer\nQuestion No.\nPaper Question No\u00bb t\nPaner No.\nident ChoiceJfo \"^\nSou\nStudent\nexamined by\nStudent\nAnswers\n\\\nientNc\nwhich picks\nAnswer No\n_Veracity\ns contained in\n\\ Panri-Nn\nPaper\/Answer\nOptions\nf \\\nis answered by\nPar\u00abrNo has the choice:\nPaper Paper\/Question\n>PaperName\nis written as\n\/\nQuestion\nCategory\ncategorises ^\n1 \\\nhas the variants\n\"^\nQuestion\nText\nPpper Question No.\n^Question choice No.\n^Answer No.\nPaper No.\n.Question No.\nPaper question No.\n\\juestion Version No.\nQuestion No\nOnestion Version No.\nQuestion Text\n7 V\nCategory No. Qimstion No.\nFigure 6: Entity Relationship diagram for individual multiple-choice question papers\nImplementation\nThe software for this project was written in Borland C++ by P. Morgan (UWCC) using a\nmodified version of the earlier E\/R diagram which could be implemented more efficiently.\nEach entity was stored as a separate file. The modified E\/R diagram is depicted in Figure\n7 using the same notation as found in Figure 6. A breakdown of the menu structure can\nbe found in Figure 8.\n54\nALT-] Volume 4 Number 2\nAnswer Text-\nExplanation\nQuestion No.\nAnswer\nPaper Question No.,\nStudent No_\nStudent Choice No.\"\nStudent\nAnswers\nName_J\nClass 1\nwhich picks\n_Answer_Njj\n|__Veracity\nis contained in\nPaper\/Answer\nOptions\nproduces\nStudent No.\nhas the choices\nStudents\nsits\nStudent No.\n_Pat\u00abtQuestion No.\nkOuestion choice No.\n\"Answer No.\n.Student No.\n^taper\/Question\nis written as\nCategory No. 1\nCategory TextJ\nQuestion\nCategory\ncategorises\n[.Question No.\n.Paper question No.\n.OuestionJJo,\nQuestion\nText\n[Xategory No.\n.Question Text\nition Version\nFigure 7: Modified version of an individual multiple-choice question paper\nFigure 8: Menu structure for an implementation\n55\n8. C H. Turton A computer-aided continuous assessment system\nIn order to allow a high degree of flexibility in the system, the mark scheme can be\nselected from a number of alternatives. For example:\n1 - Correct Answer\n1 - Correct Answer\n0 - False answer 0 - Don't know\n\u2014l\/(c\u20141) - False answer 0 - Don't know\nwhere c is the number of choices for a multiple-choice answer.\n\u2022 1 - Correct Answer -1 - False answer 0 - Don't know\nIn this paper the results are based on the second option, which is a negative marking\nscheme.\nResults\nOne hundred and thirteen students were tested using ten questions from a bank of fifteen\nquestions. Each question had the following choices: one question-text from two choices,\none true answer from two choices, five false answers from seven choices and a Don't\nknow option. Questions were allocated in a random order on the paper, and choices\nallocated randomly within a question. The probability of a number of questions (s) being\nduplicated when comparing two individual papers agreed with the analytic result given\nearlier (see Figure 9). The two options given in this figure relate to two separate ways of\ndefining 'different' papers. If the definition for option (i), as described earlier in this\npaper, is used, then the probability of finding less than or equal to three shared questions\nwhen comparing two papers is over 90%. Alternatively, the definition in option (ii) would\ndescribe the chance of finding one or fewer shared questions between two papers as over\n90%. Both definitions are reasonable, therefore both results are given. An example of the\ntype of question produced for the student's questionnaire is given in Figure 10.\nl - \u2022\n0.9 -r\n0.8 --\"\n0.7 - -\n0.6 -j- '.\n0.5\n0.4\n0.3\n0.2\n0.1\n0\n-Option i)\nOption II)\n- I -\n3 4 5 6 7\nNo. of shared questions\n10\nFigure 9: Probability of shared questions {b=l5, q=IO, option i) v=tn=4, option ii) v=t(c-l)Cfn=84]\n56\nAu-J Volume 4 Number 2\nSTUDENTS NAME : MR XXXX XXXXX XXXX HS10\nCLASS NAME : LLL93 )\nMARKING SCHEME : [Right = (+1.0) ] [wrong = (-0.2)][Dont Know = (0.0]\n[Question d ]\nGiven the following machine code and the table below, state the number\nplaced by the program at location $42, given that memory location (0041)\nProgram:(7F 00 40)(DE 40)(A6 50)(97 42)(3F)\n3 Hex\nTable: Memory Address (Hex)\n0050\n0051\n0052\n0053\n0054\n0055\n0056\n0057\nEntry (Hex) | Entry (Decimal)\n00\n01\n04\n09\n10\n19\n24\n31\nI 01\n4\n9\n16\n25\n36\n49\nAnswer\nAnswe:\nAnswe:\nAnswe:\nAnswe:\nAnswe:\n(1)\n(2)\n(3)\n(4)\n(5)\n(6)\nOutput from program = (10) Hex\nOutput from program = (31) Hex\nOutput from the program is impossible to determine as it is\noutside the range of the table.\nOutput from program = (36) Decimal\nOutput from program = (34) Decimal\nOutput from program = 1001 binary.\nAnswer (7) (DONT KNOW)\nfigure \/ 0: Example question from the multiple-choice questionnaire\nSix categories of question were available from which to generate the report. These\ncategories were used to identify 'weak' areas for tutorial work. Finally, the results were\ncorrelated with those previously obtained from a traditional examination, and they\nproduced a correlation coefficient of 0.923 for 111 degrees of freedom and two variables.\nThis indicates a high degree of positive correlation between the results.\nFeedback was also obtained from the students. They were not told how the\nquestionnaires were generated, so the plagiarism question refers to their impression from\ntheir papers, not their understanding of the technique. An example of the questionnaire is\ngiven in Figure 11, with shaded boxes to indicate the percentage of students choosing the\noption indicated. Figure 12 shows the percentage results graphically.\nIn addition to the results shown in the figures, the following comments, derived from the\ncomments section of the questionnaire, are worth highlighting.\n\u2022 Students found that multiple-choice questions specifically designed with options that\ntested their ability to understand subtle differences and exploited common student\nmisunderstandings were very difficult to answer. One commented that this forced them\nto go to the textbooks.\n57\nB. C H. Turton A computer-aided continuous assessment system\nMultiple Choice Questionnaire (14\/1\/94)\nThe following questionnaire is designed to elicit your reaction to using multiple\nchoice papers with mixed questions and computer generated explanations.\nPlease tick one box and place any relevant comments at the bottom of the page.\nDid you find the\nmultiple choice paper\nDid you prefer the\nMCQ paper to\ntraditional methods\nIn your opinion would\nPlagiarism be\nDid you think the\nFeedback was\nVery Difficult Reasonable Easy Very\nDifficult Easy\nMuch\nPreferred\nVery\nDifficult\nPreferred Made little Disliked Very\nDifference much\nDisliked\nDifficult Moderate Easy Very\nEasy\nMuch better Better than Average Poor Very Poor\nthan normal normal\nDid you think that the\nspeed of response\n(feedback time) was\nExcellent Good Reasonable Poor Very Poor\nI I\nDo you think as a\nteaching tool this form\nof MCQ was\nExcellent Good Reasonable Poor Very Poor\nJ\nDo you think as an\nassessment tool this\nform of MCQ was\nExcellent Good Reasonable Poor Very Poor\nCOMMENTS\nKEY - indicates percentage of students ticking the option shown (50% uptake of questionnaire)\nj 10% 1 5% | 0%\nfigure \/ \/ : Student questionnaire\n58\nALT-J Volume 4 Number 2\nDifficulty Preference Plagarism Feedback Response Teaching Assessment\nQuestion Choices\nFigure \/ 2: Bar chart showing the percentage results for the questionnaire\n\u2022 The assessment was criticized for using negative marking and not being able to give\npart-marks for working out. This indicates that the students' opinion on the multiple-\nchoice questionnaire as an assessment tool may partly be based on their opinion of\nnegative marking.\n\u2022 The computer-generated answers only referred to one version of the question text.\nSome students found this confusing when trying to understand the feedback papers.\nConclusion\nAccording to the questionnaire results, the class seemed to prefer this form of assessment\ndespite finding it difficult. Plagiarism did not seem to be a problem, and the response time\nwas good. However, they would have preferred a system that did not use negative\nmarking, and would have liked clearer explanation sheets. Of particular interest is the\nstudents' opinion of this CAA as a teaching tool. Students believed this CAA system was\na better teaching tool than assessment tool, according to the questionnaire. This statistic\nis backed up by comments on how the CAA paper forced them to study their books and\nwhich also indicated their dislike of an assessment method that used negative marking.\nClearly, this particular study achieved its goals and provides an effective means of\nassessing students and providing effective feedback. However, three man-days were spent\non developing the questions by a lecturer experienced in setting appropriate questions for\nthis subject. So no saving of time can be realized unless very large classes (>100) are\nassessed or the questions can be re-used. Due to the nature of this system, such questions\ncould be re-used provided a student did not collect multiple explanation sheets from a\nprevious year. Arguably, a student who reads through explanations for many different\nforms of questions and answers is likely to spend more time learning than the one who\nstudies his or her course notes. Naturally, the explanations could be withheld, thus\ntreating the students from several years as one class for the purposes of the results\nobtained. But this defeats the feedback aspects of the system. Certainly, this system does\n59\nB. C H. Turton A computer-aided continuous assessment system\nnot represent a perfect solution, as considerable effort has to be put into the 'false'\nanswers in order properly to test and inform the students. Nor can this be used as.a\nsystem to replace traditional assessment, as traditional assessment and tutorials provide\nthe means for identifying common misconceptions. Nonetheless it is a powerful new tool\nin the hands of a lecturer. The next stage in developing this system will be to provide a\nnetworked version to remove unnecessary paperwork and to rewrite the system so that it\nbecomes part of a standard database. These developments should allow greater flexibility\nand greater ease of use.\nReferences\nCommittee of Scottish University Principals (1993), Teaching and learning in an\nexpanding higher education system: executive summary', The CTISS File, 15, 5-6.\nCVCP Briefing Note (1994), Funding of Undergraduates and University Teaching, London,\nCVCP Publications.\nBenford, S., Burke, E., Foxley, E., Gutteridge, N. and Mohd Zin, A. (1994), 'Early\nexperiences of computer-aided assessment when teaching computer programming',\nAssociation for Learning Technology Journal, 1 (2), 55-70.\nFry, J. (1990), 'The database format question: an alternative to multiple choice and free\nformat for computer-based testing', Computers and Education, 14 (5), 391-401.\nLaurillard, D., Swift, B. and Darby, J. (1994), 'Academics' use of courseware materials: a\nsurvey', Association for Learning Technology Journal, 1 (1), 4-15.\n60\n"}