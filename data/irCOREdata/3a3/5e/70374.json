{"doi":"10.1049\/ic:20030158","coreId":"70374","oai":"oai:eprints.lancs.ac.uk:12328","identifiers":["oai:eprints.lancs.ac.uk:12328","10.1049\/ic:20030158"],"title":"Towards a Wearable Inertial Sensor Network","authors":["Van Laerhoven, Kristof","Gellersen, Hans","Kern, Nicky","Schiele, Bernt"],"enrichments":{"references":[{"id":16318034,"title":"Context Awareness by Analysing Accelerometer Data.","authors":[],"date":"2000","doi":"10.1109\/iswc.2000.888488","raw":"Randell, C, and Muller, H. 2000. Context Awareness by Analysing Accelerometer Data. In Blair MacIntyre and Bob Iannucci, editors, The Fourth International Symposium on Wearable Computers, IEEE Computer Society, October 2000, pp. 175-176.","cites":null},{"id":16318017,"title":"Context Awareness in Systems with Limited Resources.","authors":[],"date":"2002","doi":null,"raw":"Cakmakci, O., Coutaz, J., Van Laerhoven, K., and Gellersen, H.-W. 2002. Context Awareness in Systems with Limited Resources. In Proc. of the third workshop on Artificial Intelligence in Mobile Systems (AIMS), ECAI 2002, Lyon, France. pp. 21-29.","cites":null},{"id":16318032,"title":"D\u00e9tection des activit\u00e9s quotidiennes \u00e0 l'aide des s\u00e9parateurs \u00e0 Vaste Marge.","authors":[],"date":"2003","doi":null,"raw":"Loosli, G., Canu, S. Rakotomamonjy, A. 2003. D\u00e9tection des activit\u00e9s quotidiennes \u00e0 l'aide des s\u00e9parateurs \u00e0 Vaste Marge. RJCIA, France, pp. 139-152.","cites":null},{"id":16318037,"title":"Hierarchical Recognition of Intentional Human Gestures for Sports Video Annotation.","authors":[],"date":"2002","doi":"10.1109\/icpr.2002.1048493","raw":"Chambers, G., Venkatesh, S., West, G., Bui, H. 2002. Hierarchical Recognition of Intentional Human Gestures for Sports Video Annotation. In Proceedings of the 16 th IEEE Conference on Pattern Recognition, pp. 1082 -1085 vol.2","cites":null},{"id":16318030,"title":"Indoor navigation using a diverse set of cheap, wearable sensors.","authors":[],"date":"1999","doi":"10.1109\/iswc.1999.806640","raw":"Golding, A.R. and Lesh, N. 1999. Indoor navigation using a diverse set of cheap, wearable sensors. In Proceedings of the First International Symposium on Wearable Computers, pp. 29-36.","cites":null},{"id":16318035,"title":"Multi-Sensor ContextAware Clothing&quot;.","authors":[],"date":"2002","doi":"10.1109\/iswc.2002.1167218","raw":"Van Laerhoven, K., Schmidt, A., and Gellersen, H.-W. 2002. &quot;Multi-Sensor ContextAware Clothing&quot;. In Proceedings of the sixth International Symposium on Wearable Computers, ISWC 2002, Seattle, WA. IEEE Press, pp. 49-57.","cites":null},{"id":16318028,"title":"Wearable Sensing to Annotate Meeting Recordings&quot;.","authors":[],"date":"2002","doi":"10.1109\/iswc.2002.1167247","raw":"N. Kern, B. Schiele, H. Junker, P. Lukowicz, and G. Tr\u00f6ster. 2002. &quot;Wearable Sensing to Annotate Meeting Recordings&quot;. In Proceedings of the sixth International Symposium on Wearable Computers, ISWC 2002, Seattle, WA. IEEE Press, pp. 186-194.","cites":null},{"id":16318026,"title":"Wearable Sensor Badge & Sensor Jacket for Context Awareness.","authors":[],"date":"1999","doi":"10.1109\/iswc.1999.806681","raw":"Farringdon, J., Moore, A., Tilbury, N., Church, J., and Biemond, P. 1999. Wearable Sensor Badge & Sensor Jacket for Context Awareness. In Proc. of the Third International Symposium on Wearable Computers, ISWC\u201999, San Francisco, pp.107-113.","cites":null},{"id":16318036,"title":"What shall we teach our pants?","authors":[],"date":"2000","doi":"10.1109\/iswc.2000.888468","raw":"Van Laerhoven, K., and Cakmakci, O. 2000. What shall we teach our pants? In Proc. of the fourth International Symposium on Wearable Computers, ISWC 2000, Atlanta, GA. IEEE Press, 2000, pp.77-83.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2003-09","abstract":"Abstract. Wearable inertial sensors have become an inexpensive option to measure the movements and positions of a person. Other techniques that use environmental sensors such as ultrasound trackers or vision-based methods need full line of sight or a local setup, and it is complicated to access this data from a wearable computer\u2019s perspective. However, a body-centric approach where sensor data is acquired and processed locally, has a need for appropriate algorithms that have to operate under restricted resources. The objective of this paper is to give an overview of algorithms that abstract inertial data from bodyworn sensors, illustrated using data from state-ofthe-art wearable multi-accelerometer prototypes","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/70374.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/12328\/1\/eurowearable_2003b.pdf","pdfHashValue":"bedac8c54c7b3740ab7cf62cd1ceba10b1422d22","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:12328<\/identifier><datestamp>\n      2017-12-19T00:06:25Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D636F6E666572656E63655F6974656D<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Towards a Wearable Inertial Sensor Network<\/dc:title><dc:creator>\n        Van Laerhoven, Kristof<\/dc:creator><dc:creator>\n        Gellersen, Hans<\/dc:creator><dc:creator>\n        Kern, Nicky<\/dc:creator><dc:creator>\n        Schiele, Bernt<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        Abstract. Wearable inertial sensors have become an inexpensive option to measure the movements and positions of a person. Other techniques that use environmental sensors such as ultrasound trackers or vision-based methods need full line of sight or a local setup, and it is complicated to access this data from a wearable computer\u2019s perspective. However, a body-centric approach where sensor data is acquired and processed locally, has a need for appropriate algorithms that have to operate under restricted resources. The objective of this paper is to give an overview of algorithms that abstract inertial data from bodyworn sensors, illustrated using data from state-ofthe-art wearable multi-accelerometer prototypes.<\/dc:description><dc:date>\n        2003-09<\/dc:date><dc:type>\n        Contribution to Conference<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/12328\/1\/eurowearable_2003b.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1049\/ic:20030158<\/dc:relation><dc:identifier>\n        Van Laerhoven, Kristof and Gellersen, Hans and Kern, Nicky and Schiele, Bernt (2003) Towards a Wearable Inertial Sensor Network. In: IEE Eurowearable 2003, 1900-01-01.<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/12328\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1049\/ic:20030158","http:\/\/eprints.lancs.ac.uk\/12328\/"],"year":2003,"topics":["QA75 Electronic computers. Computer science"],"subject":["Contribution to Conference","PeerReviewed"],"fullText":"TOWARDS A WEARABLE INERTIAL SENSOR NETWORK \n \n \nKristof Van Laerhoven\u2020, Nicky Kern\u2302, Hans-Werner Gellersen\u2020, Bernt Schiele\u2302 \n \n\u2020 Computing Department, Lancaster University, LA15SQ Lancaster, United Kingdom \n\u2302 Perceptual Computing and Computer Vision Group, ETHZ, 8092 Zurich, Switzerland \n \n \n \nAbstract. Wearable inertial sensors have become \nan inexpensive option to measure the movements \nand positions of a person. Other techniques that \nuse environmental sensors such as ultrasound \ntrackers or vision-based methods need full line of \nsight or a local setup, and it is complicated to \naccess this data from a wearable computer\u2019s \nperspective. However, a body-centric approach \nwhere sensor data is acquired and processed \nlocally, has a need for appropriate algorithms that \nhave to operate under restricted resources. The \nobjective of this paper is to give an overview of \nalgorithms that abstract inertial data from body-\nworn sensors, illustrated using data from state-of-\nthe-art wearable multi-accelerometer prototypes.  \n \n \nINTRODUCTION \n \nTo date, wearable computers are primarily used in \nspecific areas, and are certainly not focused to a \nlarge audience like the one for mobile phones or \nhandheld computers. Producing tiny, light-weight \ncomputers alone is not enough to make wearable \ncomputers break through to a wider market.  \n \nThe properties of wearable computing (as stated in \n(12) for example), indicate several advantages \nover computing devices that follow the desktop \nparadigm: a computer that is always on, which is \nclose to the wearer and has ways to get input \nwithout user interaction, has a far better chance of \ngetting to know its owner than regular computers. \nWe therefore assume that making a computing \nsystem that \u2018perceives\u2019 what its wearer is doing, \nwould be a step forward for wearable computing. \n \nAugmenting a computer with sensors in order to \ngive it some level of insight in what is happening, \nhas been a well-researched topic in AI and robotics \nand has resurfaced about a decade ago in the field \nof human-computer interaction (HCI) as context \nawareness. The goal is to take the strain away \nfrom user-interaction and let the computer make \ndecisions autonomously, based on information that \nwas acquired independently from the user via the \nsensors.  \n \nResearch at Philips (2) concentrated on integration \nof many small sensors into clothing, while \naccelerometers were also discovered to be ideal \nsensors to distinguish basic activities of the \nwearable computer user (6,8). \n \nA range of applications exists in the medical \ndomain, where a patient\u2019s activities might be \nmonitored in combination with other sensing \n(knowing someone\u2019s activity is crucial in clarifying \nwhy the heart rate is suddenly higher, for instance) \nor for keeping track of a person\u2019s posture in \nergonomic applications. Wearable computing \nscenarios use activity-aware applications in other \nsettings, like meetings (4) or tracking (6). Other \nfuture directions might include deducing tell-tale \nsigns for emotions in an individual\u2019s posture and \ngestures. \n \n \nRATIONALE \n \nThis section clarifies two design choices that were \nmade not only by the authors, but also in similar \nwearable computing research.  \n \n \nWhy Accelerometers? \n \nWe limit the type of sensor in our research to the \nacceleration sensor or accelerometer. This sensor \ncan be thought of as a ball that is attached to two \nsprings on opposite sides, and which is placed in a \ntube to limit its movement in two directions, as \ndepicted in Figure 1. Measuring the position of the \nball within the tube is in this metaphor the output of \nthe sensor: shaking the tube to the left and right \nwill move the ball\u2019s position, but tilting it will do so \nas well (but in a lesser degree). These two effects \nare called dynamic and static acceleration \nrespectively.  \n \n \nFigure 1. Metaphor for the acceleration sensor, using \nthe location of a ball in a tube that is attached to two \nsprings.  \nSprings \nTube Ball \nAdvantages of a body-network of accelerometers \ninclude mainly the cost, power consumption and \nsize of this type of sensor. Other available sensors \nthat might replace or contribute to accelerometers \nfor measuring a person\u2019s body movements, are \neither more expensive, bigger in size, or require \nmore battery power at the time of writing this \npaper.  \n \n \nWhy So Many? \n \nThe goal of the sensors is to detect motion and \nposition of a person\u2019s body that can be abstracted \nto a more abstract activity, like sitting, running, or \nwaving. Early research (4, 6, 8) used merely a \ncouple of well-placed accelerometers to distinguish \nas many of these activities as possible. Among the \nsolutions were placing one or two accelerometers \non one hip, or just above one of the knees.  \n \nThis approach is far from error-proof, however: if \nthe sensors are just above the knee for instance, \nlying on the ground would produce the same \nsensor values as sitting, and so would lifting the \nleg the sensors are attached to. A solution would \nbe to attach two more sensors on the other leg, but \nlying horizontally would still look like 'sitting down' \nto the system. We can go on like this and even \nafter adding many more sensors of the same type, \nstill come up with situations that make the system \nincorrectly 'detect' sitting. \n \nHaving the sensors distributed over the wearer\u2019s \nbody is therefore a straightforward solution in order \nto make sure that every aspect of the body\u2019s \nmovements and positions will be registered. \n \n \nHARDWARE PLATFORMS \n \nAlthough the heart of this paper is about the \nalgorithms that can make abstractions from a vast \namount of worn sensors, it is necessary to give \ncertain characteristics of the hardware platforms \nthat produce the data. As this paper was a joint \neffort by two institutions, two distinct, custom-built \nmulti-accelerometer platforms were used. \n \n \nThe Lancaster Sensor Hardware \n \nThe Lancaster multi-accelerometer platform (9) \nhas 30 accelerometers, which are all linked to one \nmicrocontroller unit that sends the output straight \nto a wearable computer\u2019s serial port. Rather than \nusing straps to fasten the sensors to the wearer\u2019s \nbody, the choice was made to embed the \naccelerometers in trousers and a lab coat. Sensors \nare grouped per two (in physical packages of the \naccelerometers). \nThe ETH Zurich Sensor Hardware   \n \nThe platform built at ETH Zurich was designed as \na more modular, straps-based harness. It is based \non smart-its (5), a modular multi-purpose miniature \ncomputing platform. Groups of six sensor modules \nare connected to a smart-it via multiplexers, with \neach module containing four accelerometers, \nwhich can be combined (experiments in this paper \nused two of these groups, thus giving a total of 2 \ntimes 4 sensors per 6 modules = 48 \naccelerometers).  \n \n \nCHALLENGES \n \n \nNoise \n \nThe main difficulty when translating raw sensor \ndata from sensors in clothing to an activity \ndescription is the presence of noise. We \ndifferentiate three types of noise: \n \nSensor Noise. This is the distortion of what the \nactual source looks like (e.g., movement) in the \nsignal that the sensor produces. This includes \nnoise in the electronic circuits around the sensor, \nand noise introduced during the measurement. \nEvery hardware sensor produces a signal that \ncontains a certain amount of noise.  \n \nSensor Distance Noise. We wish to measure \nmovement and position (and in the future maybe \nother properties) of a person\u2019s body. The sensors, \nhowever, are embedded in the person\u2019s clothing. \nFor free-hanging clothing such as skirts, this type \nof noise will evidently be large, but even tight \ngarments can shift while moving.  \n \nTime-Domain Noise. The sampling of sensor data \nmight also fluctuate in time, resulting in noise in the \ntime dimension as well. This might complicate \nthings for recognition of gesture-based motion \nwhere patterns in time need to be predicted, or \nwhere they shift or drift over time (e.g., as walking \npatterns change slightly as one gets tired). \n \nAn important factor in both discussed wearable \nsensor systems is the high number of sensors. \nThis is inevitable as the objective is to measure the \nwearer\u2019s activity by looking at the motions the body \nis making. The result: algorithms have to be able to \ncombine or fuse all this data.   \n \nThe algorithms that process the acceleration data \nbecome slower and less effective as the number of \naccelerometers increases. This problem is \ngenerally known as the \u2018curse of dimensionality\u2019, \nand is a common obstacle for multi-sensor \nsystems.  \n   \nFigure 2. The Lancaster Platform (right) and the ETH \nZurich Platform (left). Both were used to capture activity \ndata that were then analyzed at both sites.  \n \nFusion of sensor data \n \nSensor and\/or feature selection is the most used \nmethod to solve this problem. Not all sensors are \nequally important to detect a certain activity; \nwalking is mainly detected by leg movement and \nwriting by the arm\u2019s motions, for instance.  \n \n \nOVERVIEW OF ALGORITHMS \n \nThis section gives a short overview on common \nalgorithms that have been used in the past to \nprocess data from wearable sensors.  \n \n \nPreprocessing \n \nBasic statistics, such as the minimum, maximum, \naverage or (co)variance over a certain interval \nmake ideal descriptors for acceleration data. The \nmain advantage is reduction of the data streams: \nusing the aforementioned four values to describe a \nstream of 100 values, for instance, will make things \neasier for the algorithms that have to further \nabstract this information.  \n \nPeaks in the signals of the accelerometer signals \ncan be expected to reveal a great deal more than \nthe basic statistics discussed before. A promising \nmethod is to detect and characterize peaks as \nsoon as they occur. The accelerometer traces can \nbe parsed by first recognizing activity by \nthresholding the size and length of a running \nvariance. Then, the available peaks within that \nactivity region are measured and roughly \nclassified. \n \n \nClustering \/ Topographic Mapping \n \nKohonen Self-Organizing Map. Unsupervised \nneural networks, such as the Kohonen Self-\nOrganizing Map, have been applied extensively to \nanalyse the data from wearable sensors, in (9) and \n(7) for instance. These algorithms map the \nincoming sensor data to a grid-like map, where \nsimilar signals are mapped close to each other on \nthe map, and dissimilar signals are mapped far \naway from each other.   \n \n \nClassification \n \nSupport Vector machines. Loosli et al. (7) use \nsupport vector machines (SVMs) to improve on the \nmethod in (10), using a selection procedure of \nparameters.  \n \nBayesian Classification. Bayesian classification is \na simple classification algorithm based on Bayes\u2019 \nrule from basic probability theory. It requires \nlabeled training data for the classification. Bayes\u2019 \nrule states that the probability of a given activity a, \ngiven a feature vector x, can be calculated as \nfollows:  \n)(\n)()|()|(\nxp\napaxpxap =  \np(a) denotes the a-priori probability of the given \nactivity. The a-priori probability p(x) of the data is \nonly used for normalization. It can often be ignored \nby simply choosing the activity with the highest \nlikelihood as the final classification. The likelihood \np(x|a) has to be computed from labeled training \ndata. \n \nGraphical Models. Graphical models such as \nMarkov Models are commonly used in combination \nwith previously discussed methods to do additional \nconsistency checks. The Markov Chain was used \nin (10) for instance to track sequences of activities, \nsuch as \u201csitting\u201d \u2013 \u201cstanding\u201d \u2013 \u201cwalking\u201d \u2013 \n\u201crunning\u201d, and assign probabilities to these \ntransitions. Kern et al. (4) have used Hidden \nMarkov Models to recognize similar activities. \nChambers et al. (11) have used HMMs to \nrecognize Kung-Fu gestures. \n \n \nSTUDY: CROSS USAGE OF DATASETS \n \nThis section describes the results of a cross \ncomparison test between the data sets from \nauthors\u2019 hardware and algorithms, testing data \nfrom both platforms on both sites\u2019 algorithms. This \nis possible since the nature of both hardware \nconfigurations is similar enough, using similar \nsensors and having similar specs. The benefits for \nthis approach include an extra check on the \nreliability of the data, but more importantly a check \nfor the generality of the algorithms (as they were \nnot prepared for the data).  \n \nThe intention of this study is not to find out which \nplatform or algorithms are superior (this study is \ntoo small for that), but to illustrate the authors\u2019 \nmethods using each other\u2019s \u2018unrehearsed\u2019 data \nand find similarities and common ground in the \nauthors\u2019 research.  \n \n \nDatasets \n \nWe recorded two data sets, one with each \nhardware platform.  \n \nETH data set. The ETH data set is a continuous \ndata set lasting ca. 18min and containing the \nactivities: sitting, standing, walking, stairs up, and \nstairs down. The sensors are attached to shoulder, \nelbow, wrist, hip, knee, and ankle on both sides of \nthe body (see figure 2).  \n \nLancaster data set.  For facilitating further \ncomparison, the Lancaster data set was roughly \nmodelled on the ETH data set, and consists of \nseveral activities like lying down on a bed, sitting, \nstanding, walking, stairs up, and stairs down. The \naccelerometers are attached to the shoulders, \nupper arms, lower arms, hips, above the knees, \nbelow the knees, and above the ankles on both \nsides, plus an additional one on the chest (two \nperpendicular sensors per location, see figure 2).  \n \n \nIllustration of Methodologies \u2013 Mapping \n \nLancaster\u2019s topographic mapping-based method \ncombines topographic mapping algorithms, putting \nsimilar sensor data together in categories, with a \nMarkov Chain model that keeps track of \nsequences of these categories. Before the \nclustering, however, some pre-processing is done \non the data. Apart from simple statistics (minimum, \nmaximum, average and variances per sensor over \na window of 50 samples), basic peak-based \ninformation per sensor was assembled as well over \na varying window.  \n \nPeak-based Feature Extraction. For detecting the \npeaks, a two-step algorithm was used that first \ndetects \u2018area of activity\u2019 in the data, in which it then \nstarts analysing the peaks per sensor. This method \ngenerally works well, although certain limitations \napply: the time frame for the area of activity is in \npractice relatively short (in the order of seconds), \nand it is not possible to track peaks over multiple \ndimensions.    \n \nTopographic Mapping. A combination of the \nKohonen Self-Organizing Map, with sub-\nhierarchies of k-means clustering layers per cell in \nthe Kohonen Map (as described in 10) was then \nutilised to further analyse the features (basic \nstatistics, plus the peak information if the most \nrecent sensor data was part of an area of activity). \nIt is possible to give visual feedback during and \nafter the run through the data set: Figure 4 shows \nsuch a visualisation of the created map for the ETH \ndata set by the approach based on topographic \nmapping.  \n \n \n200 400 600 800 1000 1200 1400 1600 1800 2000\n0\n200\n400\n600\n800\ndata set, 2000 samples, 4 channels, 10-frame, \u03bb=550\n1\n2\n3\n4\nact\n5 10 15 20\n520\n540\n560\n580\ncurrent activity region\n1 2 3 4\n-20\n-10\n0\n10\nindividual channels\nglobal area: (99           150         168.1          81.7)     position: 1174     size: 20\nunfil tered peaks:                                                                          \n1        -43.1         0.15        -0.85         27.9        -5.55        21.45            \n2         -8.5           75        -66.5                                                   \n3       -18.35        84.05        -65.7                                                   \n4       -23.25        39.15        -0.65         0.35       -16.95         1.35            P\nea\nk \nIn\nfo\nrm\nat\nio\nn\n \n \nFigure 3. Peak extraction on the ETH data set, part of \nwhich is plotted in the top part of the graph, with a \nspecific area of activity visualised in the middle two plots, \nand information on the detected peaks in the lower part. \nOnly part of the full data set, and only four sensors are \nvisualised for keeping the figure readable.  \n \n \n1 \n2 \n3 4 \n5 \n \n \nFigure 4. Visualization of the Kohonen Map on the ETH \ndata set (known as the U-Matrix), where cells in plateaus \nof the same colour belong to the various activities in the \ndata set. These different activity plateaus are in fact \ngenerated clusters that have been assigned a number in \norder of occurrence (1-5). The following clusters belong \nto the activities: 1 \u2013 sitting, 2 \u2013 standing, 3 comprises \nboth walking up and down the stairs, 4 \u2013 walking down \nthe stairs, and 5 - walking. \n \nTable 1 shows the final recognition rate over the \nentire ETH data set. As this algorithm does one \npass over the data, a lot of the errors occur in the \ninitial phases whenever a new activity presents \nitself. Apart from a weak performance in \ndistinguishing the \u201cwalking up the stairs\u201d and \n\u201cwalking down the stairs\u201d activities, most other \nerrors occur in transitions between activities.  \n \n \n Recognized Activity \n Sit Stand Walk Lying Stairs Up \nStairs \nDown \nSit 93.22 0.1 2.16 0.05 2.51 1.96 \nStand 0.14 93.78 4.8 0.04 0.92 0.31 \nWalk 2.74 4.49 74.98 3.77 4.95 9.06 \nLying 0.19 0.24 1.94 92.62 2.28 2.72 \nStairs \nUp 0.18 0.54 16.82 0.92 45.52 36.02 \nStairs \nDown 0.41 0.76 14.87 1.03 34.5 48.41 \n \n \n Recognized Activity \n Sit Stand Walk Stairs \nUp \nStairs \nDown \nSit 92.28 0 3.84 1.2 2.67 \nStand 0 93.11 2.15 1.94 2.78 \nWalk 1.62 0.56 73.2 12.68 11.94 \nStairs \nUp 3.23 2.66 16.64 38.83 38.63 \nStairs \nDown 1.28 1.14 8.79 30.88 57.9 \nTable 1: Recognition Results of the Lancaster (top) and \nETH (bottom) data sets using classification on the map \nthat was generated by the Kohonen-based algorithm. \nThe overall recognition is 71.05% \n \nMarkov Chain. It made less sense to generate a \nMarkov Chain, as the transitions of activities in the \ndata set are not occurring often enough to extract \na valuable model. This method is mainly aimed at \nproviding a second opinion when the system is \nused for longer periods.  \n \n \nIllustration of Methodologies \u2013 Naive Bayes \n \nThe naive Bayes approach uses labelled data to \nrecognize clearly defined activities. A running \nmean and variance, computed over the last 50 \ndata vectors, is used as features.  \nLancaster Data Set. The Lancaster data set is \ndivided into three sequences, comprising of the \nactivities lying-sitting-standing-sitting-lying (bed), \nsitting-standing-walking-stairs (sit), and stairs \ndown-up (stairs). Three Bayes\u2019 classifiers are \ntrained on the entire sequences. A classification is \ndone for every feature vector.  \n \n \n Recognized Activity \n Bed Sit Stairs \nBed 92.30 7.61 0.08 \nSit 1.73 98.20 0.06 \nStairs 1.69 10.71 87.59 \n \nTable 2: Recognition Results of the Lancaster Data Set \nusing Bayes\u2019 Classification \n \n Recognized Activity \n Sit Stand Walk Stairs \nUp \nStairs \nDown\nSit 99.27 0.34 0.33 0.04 0 \nStand 2.37 96.23 1.39 0 0 \nWalk 2.12 6.48 87.54 0.32 3.52 \nStairs \nUp 4.44 0.03 18.79 72.98 3.72 \nStairs \nDown 4.20 0 15.47 0.01 80.30 \nTable 3: Recognition Results of the ETH data set using \nBayes' Classification. The overall recognition rate is \n87.26%. \nTable 2 shows the resulting confusion matrix of the \nrecognition. The overall recognition rate is 92.7%. \nThe confusion between sit and stairs is due to the \nfact that they both contain walking up and down \nstairs. The same applies to confusion between bed \nand sit, which both contain sitting and standing. \n \nETH Data Set. Five Naive Bayes classifiers have \nbeen trained on the ETH data set. Table 3 \nsummarizes the recognition results. Apart from the \nactivities stairs up and stairs down, the recognition \nrate is between 87-99%. The principal confusion is \nbetween walking and stairs up\/down, which is \nmainly due to the similarity of the three activities. \nCompared to the topographic mapping approach, \nthe overall recognition rate is improved by 16%. \n \nCONCLUSIONS \n \nWe aimed at giving an overview of up to date \nresearch on multi-sensor wearable platforms, \nspecifically focusing on the algorithms that interpret \nthe many streams of data these produce, with a \ncloser view at the approaches of this paper\u2019s \nauthors.  This was illustrated by applying the \nalgorithms on data sets that were created using \neach others\u2019 sensing platforms. Both algorithms \nhave proven that they are suitable for practical use \nbefore, and performed well in recognizing a variety \nof different activities.  \n \nBoth approaches deal differently with noise. The \nETH platform has the accelerometers strapped \ntightly to the body to avoid noise being created by \nclothing (which is present in the Lancaster \nplatform). On the algorithms\u2019 side, noise is mainly \nreduced by using adequate pre-processing, but \nalso by using algorithms that are known to operate \nwell for noisy data. \n \nRather than proving or disproving superiority in the \nchoice of algorithms, the two discussed \napproaches proved to be built for slightly different \napplication domains. The topographic mapping \napproach of automatically clustering the data is \nwell suited for applications with activities that are \nnot well defined, that can change over time, or for \nwhich there is no labelled training data available. It \nallows capturing activities that are relatively \narbitrary and even can be chosen by the user. The \nBayesian approach requires labelled training data \nand hence needs more effort in the data \nacquisition phase. Given these labels however, it is \npossible to recognize activities such as walking, \nsitting, standing, lying in bed, etc. with high \naccuracies. In applications where high recognition \nrates are for a selection of predefined activities, \nsuch a supervised approach is usually optimal. \n \nIn conclusion, we stress that progress in this \nparticular area of multi-accelerometer activity \ntracking is progressing rapidly. Multi-sensor \nhardware platforms such as the ones presented \nhere were only constructed in the last year, using \ncomponents that only recently have shrunk to \ncomfortable sizes. As new iterations of both \nhardware and algorithms will be put through, we \nexpect to see reliable versions that eventually will \ngo beyond lab experiments and become \nappropriate for a wider use in wearable computing. \n \n \nACKNOWLEDGEMENTS \n \nThis research was in part funded by Equator, an \nEPSRC funded IRC project, and the Smart-Its \nProject, funded by the Commission of the \nEuropean Union (contract IST-2000-25428). \n \n \nREFERENCES \n \n1. Cakmakci, O., Coutaz, J., Van Laerhoven, K., \nand Gellersen, H.-W. 2002. Context \nAwareness in Systems with Limited \nResources. In Proc. of the third workshop on \nArtificial Intelligence in Mobile Systems \n(AIMS), ECAI 2002, Lyon, France. pp. 21-29. \n2. Farringdon, J., Moore, A., Tilbury, N., Church, \nJ., and Biemond, P. 1999. Wearable Sensor \nBadge & Sensor Jacket for Context \nAwareness. In Proc. of the Third International \nSymposium on Wearable Computers, \nISWC\u201999, San Francisco, pp.107-113. \n3. Gellersen, H.-W., Schmidt, A. and Beigl, M. \n2002. Multi-Sensor Context-Awareness in \nMobile Devices and Smart Artifacts, in Mobile \nNetworks and Applications (MONET), Oct \n2002. \n4. N. Kern, B. Schiele, H. Junker, P. Lukowicz, \nand G. Tr\u00f6ster. 2002. \"Wearable Sensing to \nAnnotate Meeting Recordings\". In \nProceedings of the sixth International \nSymposium on Wearable Computers, ISWC \n2002, Seattle, WA. IEEE Press, pp. 186-194. \n5. Smart-Its: \nhttp:\/\/ubicomp.lancs.ac.uk\/twiki\/bin\/viewauth\/\nSmartits\/WebHome \n6. Golding, A.R. and Lesh, N. 1999. Indoor \nnavigation using a diverse set of cheap, \nwearable sensors. In Proceedings of the First \nInternational Symposium on Wearable \nComputers, pp. 29-36. \n7. Loosli, G., Canu, S. Rakotomamonjy, A. 2003.  \nD\u00e9tection des activit\u00e9s quotidiennes \u00e0 l'aide \ndes s\u00e9parateurs \u00e0 Vaste Marge. RJCIA, \nFrance, pp. 139-152. \n8. Randell, C, and Muller, H. 2000. Context \nAwareness by Analysing Accelerometer Data. \nIn Blair MacIntyre and Bob Iannucci, editors, \nThe Fourth International Symposium on \nWearable Computers, IEEE Computer \nSociety, October 2000, pp. 175-176. \n9. Van Laerhoven, K., Schmidt, A., and \nGellersen, H.-W. 2002. \"Multi-Sensor Context-\nAware Clothing\". In Proceedings of the sixth \nInternational Symposium on Wearable \nComputers, ISWC 2002, Seattle, WA. IEEE \nPress, pp. 49-57.  \n10. Van Laerhoven, K., and Cakmakci, O. 2000. \nWhat shall we teach our pants? In Proc. of the \nfourth International Symposium on Wearable \nComputers, ISWC 2000, Atlanta, GA. IEEE \nPress, 2000, pp.77-83. \n11. Chambers, G., Venkatesh, S., West, G., Bui, \nH. 2002. Hierarchical Recognition of \nIntentional Human Gestures for Sports Video \nAnnotation. In Proceedings of the 16th IEEE \nConference on Pattern Recognition, pp. 1082 \n-1085 vol.2 \n12. Wearables FAQ at MIT, online: \nhttp:\/\/wearables.www.media.mit.edu\/projects\/\nwearables\/FAQ\/ \n"}