{"doi":"10.3115\/1613692.1613695","coreId":"70238","oai":"oai:eprints.lancs.ac.uk:12818","identifiers":["oai:eprints.lancs.ac.uk:12818","10.3115\/1613692.1613695"],"title":"Measuring MWE compositionality using semantic annotation","authors":["Piao, Scott","Rayson, Paul","Mudraya, Olga","Wilson, Andrew","Garside, Roger"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2006-07","abstract":"This paper reports on an experiment in which we explore a new approach to the automatic measurement of multi-word expression (MWE) compositionality. We propose an algorithm which ranks MWEs by their compositionality relative to a semantic field taxonomy based on the Lancaster English semantic lexicon (Piao et al., 2005a). The semantic information provided by the lexicon is used for measuring the semantic distance between a MWE and its constituent words. The algorithm is evaluated both on 89 manually ranked MWEs and on McCarthy et al's (2003) manually ranked phrasal verbs. We compared the output of our tool with human judgments using Spearman's rank-order correlation coefficient. Our evaluation shows that the automatic ranking of the majority of our test data (86.52%) has strong to moderate correlation with the manual ranking while wide discrepancy is found for a small number of MWEs. Our algorithm also obtained a correlation of 0.3544 with manual ranking on McCarthy et al's test data, which is comparable or better than most of the measures they tested. This experiment demonstrates that a semantic lexicon can assist in MWE compositionality measurement in addition to statistical algorithms","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/70238.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/12818\/1\/W06_1202.pdf","pdfHashValue":"6c0679f3786ab2254419b9e7f2ad8ee524e7c9db","publisher":"Association for Computational Linguistics","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:12818<\/identifier><datestamp>\n      2017-10-31T00:06:37Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D626F6F6B5F73656374696F6E<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Measuring MWE compositionality using semantic annotation<\/dc:title><dc:creator>\n        Piao, Scott<\/dc:creator><dc:creator>\n        Rayson, Paul<\/dc:creator><dc:creator>\n        Mudraya, Olga<\/dc:creator><dc:creator>\n        Wilson, Andrew<\/dc:creator><dc:creator>\n        Garside, Roger<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        This paper reports on an experiment in which we explore a new approach to the automatic measurement of multi-word expression (MWE) compositionality. We propose an algorithm which ranks MWEs by their compositionality relative to a semantic field taxonomy based on the Lancaster English semantic lexicon (Piao et al., 2005a). The semantic information provided by the lexicon is used for measuring the semantic distance between a MWE and its constituent words. The algorithm is evaluated both on 89 manually ranked MWEs and on McCarthy et al's (2003) manually ranked phrasal verbs. We compared the output of our tool with human judgments using Spearman's rank-order correlation coefficient. Our evaluation shows that the automatic ranking of the majority of our test data (86.52%) has strong to moderate correlation with the manual ranking while wide discrepancy is found for a small number of MWEs. Our algorithm also obtained a correlation of 0.3544 with manual ranking on McCarthy et al's test data, which is comparable or better than most of the measures they tested. This experiment demonstrates that a semantic lexicon can assist in MWE compositionality measurement in addition to statistical algorithms.<\/dc:description><dc:publisher>\n        Association for Computational Linguistics<\/dc:publisher><dc:date>\n        2006-07<\/dc:date><dc:type>\n        Contribution in Book\/Report\/Proceedings<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/12818\/1\/W06_1202.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.3115\/1613692.1613695<\/dc:relation><dc:identifier>\n        Piao, Scott and Rayson, Paul and Mudraya, Olga and Wilson, Andrew and Garside, Roger (2006) Measuring MWE compositionality using semantic annotation. In: MWE '06 Proceedings of the Workshop on Multiword Expressions. Association for Computational Linguistics, Stroudsburg, pp. 2-11. ISBN 1932432841<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/12818\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":null,"relations":["http:\/\/dx.doi.org\/10.3115\/1613692.1613695","http:\/\/eprints.lancs.ac.uk\/12818\/"],"year":2006,"topics":["QA75 Electronic computers. Computer science"],"subject":["Contribution in Book\/Report\/Proceedings","PeerReviewed"],"fullText":"Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 2\u201311,\nSydney, July 2006. c\u00a92006 Association for Computational Linguistics\nMeasuring MWE Compositionality Using Semantic Annotation \nScott S. L. Piao1, Paul Rayson1, Olga Mudraya2, Andrew Wilson2 and Roger Garside1 \n \n1Computing Department \nLancaster University \nLancaster, UK \n{s.piao, p.rayson, r.garside}@lancaster.ac.uk \n2Dept. of Linguistics and EL \nLancaster University \nLancaster, UK \n{o.mudraya, a.wilson}@lancaster.ac.uk \n \n \nAbstract \nThis paper reports on an experiment in \nwhich we explore a new approach to the \nautomatic measurement of multi-word \nexpression (MWE) compositionality. We \npropose an algorithm which ranks MWEs \nby their compositionality relative to a \nsemantic field taxonomy based on the \nLancaster English semantic lexicon (Piao \net al., 2005a). The semantic information \nprovided by the lexicon is used for meas-\nuring the semantic distance between a \nMWE and its constituent words. The al-\ngorithm is evaluated both on 89 manually \nranked MWEs and on McCarthy et al\u2019s \n(2003) manually ranked phrasal verbs. \nWe compared the output of our tool with \nhuman judgments using Spearman\u2019s \nrank-order correlation coefficient. Our \nevaluation shows that the automatic rank-\ning of the majority of our test data \n(86.52%) has strong to moderate correla-\ntion with the manual ranking while wide \ndiscrepancy is found for a small number \nof MWEs. Our algorithm also obtained a \ncorrelation of 0.3544 with manual rank-\ning on McCarthy et al\u2019s test data, which \nis comparable or better than most of the \nmeasures they tested. This experiment \ndemonstrates that a semantic lexicon can \nassist in MWE compositionality meas-\nurement in addition to statistical algo-\nrithms. \n1 Introduction \nOver the past few years, compositionality and \ndecomposability of MWEs have become impor-\ntant issues in NLP research. Lin (1999) argues \nthat \u201cnon-compositional expressions need to be \ntreated differently than other phrases in many \nstatistical or corpus\u2013based NLP methods\u201d. Com-\npositionality means that \u201cthe meaning of the \nwhole can be strictly predicted from the meaning \nof the parts\u201d (Manning & Sch\u00fctze, 2000). On the \nother hand, decomposability is a metric of the \ndegree to which the meaning of a MWE can be \nassigned to its parts (Nunberg, 1994; Riehemann, \n2001; Sag et al., 2002). These two concepts are \nclosely related. Venkatapathy and Joshi (2005) \nsuggest that \u201can expression is likely to be rela-\ntively more compositional if it is decomposable\u201d. \nWhile there exist various definitions for \nMWEs, they are generally defined as cohesive \nlexemes that cross word boundaries (Sag et al., \n2002; Copestake et al., 2002; Calzolari et al., \n2002; Baldwin et al., 2003), which include \nnominal compounds, phrasal verbs, idioms, col-\nlocations etc. Compositionality is a critical crite-\nrion cutting across different definitions for ex-\ntracting and classifying MWEs. While semantics \nof certain types of MWEs are non-compositional, \nlike idioms \u201ckick the bucket\u201d and \u201chot dog\u201d, \nsome others can have highly compositional se-\nmantics like the expressions \u201ctraffic light\u201d and \n\u201caudio tape\u201d. \nAutomatic measurement of MWE composi-\ntionality can have a number of applications. One \nof the often quoted applications is for machine \ntranslation (Melamed, 1997; Hwang & Sasaki, \n2005), in which non-compositional MWEs need \nspecial treatment. For instance, the translation of \na highly compositional MWE can possibly be \ninferred from the translations of its constituent \nwords, whereas it is impossible for non-\ncompositional MWEs, for which we need to \nidentify the translation equivalent for the MWEs \nas a whole. \nIn this paper, we explore a new method of \nautomatically estimating the compositionality of \nMWEs using lexical semantic information, \nsourced from the Lancaster semantic lexicon \n(Piao et al., 2005a) that is employed in the \nUSAS1 tagger (Rayson et al., 2004). This is a \n                                                 \n1 UCREL Semantic Analysis System \n2\nlarge lexical resource which contains nearly \n55,000 single-word entries and over 18,800 \nMWE entries. In this lexicon, each MWE2 and \nthe words it contains are mapped to their poten-\ntial semantic categories using a semantic field \ntaxonomy of 232 categories. An evaluation of \nlexical coverage on the BNC corpus showed that \nthe lexical coverage of this lexicon reaches \n98.49% for modern English (Piao et al., 2004).  \nSuch a large-scale semantic lexical resource al-\nlows us to examine the semantics of many \nMWEs and their constituent words conveniently \nwithout resorting to large corpus data. Our ex-\nperiment demonstrates that such a lexical re-\nsource provides an additional approach for auto-\nmatically estimating the compositionality of \nMWEs. \nOne may question the necessity of measuring \ncompositionality of manually selected MWEs. \nThe truth is, even if the semantic lexicon under \nconsideration was compiled manually, it does not \nexclusively consist of non-compositional MWEs \nlike idioms. Built for practical discourse analysis, \nit contains many MWEs which are highly com-\npositional but depict certain entities or semantic \nconcepts. This research forms part of a larger \neffort to extend lexical resources for semantic \ntagging. Techniques are described elsewhere \n(e.g. Piao et al., 2005b) for finding new candi-\ndate MWE from corpora. The next stage of the \nwork is to semi-automatically classify these can-\ndidates using an existing semantic field taxon-\nomy and, to assist this task, we need to investi-\ngate patterns of compositionality. \n2 Related Work  \nIn recent years, various approaches have been \nproposed to the analysis of MWE compositional-\nity. Many of the suggested approaches employ \nstatistical algorithms. \nOne of the earliest studies in this area was re-\nported by Lin (1999) who assumes that \u201cnon-\ncompositional phrases have a significantly dif-\nferent mutual information value than the phrases \nthat are similar to their literal meanings\u201d and \nproposed to identify non-compositional MWEs \nin a corpus based on distributional characteristics \nof MWEs. Bannard et al. (2003) tested tech-\nniques using statistical models to infer the mean-\ning of verb-particle constructions (VPCs), focus-\n                                                 \n2 In this lexicon, many MWEs are encoded as templates, \nsuch as driv*_* {Np\/P*\/J*\/R*} mad_JJ,  which represent \nvariational forms of a single MWE, For further details, see \nRayson et al., 2004.  \ning on prepositional particles. They tested four \nmethods over four compositional classification \ntasks, reporting that, on all tasks, at least one of \nthe four methods offers an improvement in preci-\nsion over the baseline they used. \nMcCarthy et al. (2003) suggested that compo-\nsitional phrasal verbs should have similar \nneighbours as for their simplex verbs. They \ntested various measures using the nearest \nneighbours of phrasal verbs and their simplex \ncounterparts, and reported that some of the \nmeasures produced results which show signifi-\ncant correlation with human judgments. Baldwin \net al. (2003) proposed a LSA-based model for \nmeasuring the decomposability of MWEs by ex-\namining the similarity between them and their \nconstituent words, with higher similarity indicat-\ning the greater decomposability.  They evaluated \ntheir model on English noun-noun compounds \nand verb-particles by examining the correlation \nof the results with similarities and hyponymy \nvalues in WordNet. They reported that the LSA \ntechnique performs better on the low-frequency \nitems than on more frequent items. Venkatapathy \nand Joshi (2005) measured relative composition-\nality of collocations having verb-noun pattern \nusing a SVM (Support Vector Machine) based \nranking function. They integrated seven various \ncollocational and contextual features using their \nranking function, and evaluated it against manu-\nally ranked test data. They reported that the SVM \nbased method produces significantly better re-\nsults compared to methods based on individual \nfeatures. \nThe approaches mentioned above invariably \ndepend on a variety of statistical contextual in-\nformation extracted from large corpus data. In-\nevitably, such statistical information can be af-\nfected by various uncontrollable \u201cnoise\u201d, and \nhence there is a limitation to purely statistical \napproaches. \nIn this paper, we contend that a manually \ncompiled semantic lexical resource can have an \nimportant part to play in measuring the composi-\ntionality of MWEs. While any approach based on \na specific lexical resource may lack generality, it \ncan complement purely statistical approaches by \nimporting human expert knowledge into the \nprocess. Particularly, if such a resource has a \nhigh lexical coverage, which is true in our case, \nit becomes much more useful for dealing with \ngeneral English. It should be emphasized that we \npropose our semantic lexical-based approach not \nas a substitute for the statistical approaches. \n3\nRather we propose it as a potential complement \nto them.   \nIn the following sections, we describe our ex-\nperiment and explore this approach to the issue \nof automatic estimation of MWE compositional-\nity. \n3 Measuring MWE compositionality \nwith semantic field information \nIn this section, we propose an algorithm for \nautomatically measuring MWE compositionality \nbased on the Lancaster semantic lexicon. In this \nlexicon, the semantic field of each word and \nMWE is encoded in the form of semantic tags. \nWe contend that the compositionality of a MWE \ncan be estimated by measuring the distance be-\ntween semantic fields of an MWE and its con-\nstituent words based on the semantic field infor-\nmation available from the lexicon. \nThe lexicon employs a taxonomy containing \n21 major semantic fields which are further di-\nvided into 232 sub-categories. 3  Tags are de-\nsigned to denote the semantic fields using letters \nand digits. For instance, tag N3.2 denotes the \ncategory of {SIZE} and Q4.1 denotes {media: \nNewspapers}. Each entry in the lexicon maps a \nword or MWE to its potential semantic field \ncategory\/ies. More often than not, a lexical item \nis mapped to multiple semantic categories, re-\nflecting its potential multiple senses. In such \ncases, the tags are arranged by the order of like-\nlihood of meanings, with the most prominent one \nat the head of the list. For example, the word \n\u201cmass\u201d is mapped to tags N5, N3.5, S9, S5 and \nB2, which denote its potential semantic fields of \n{QUANTITIES},  {MEASUREMENT: \nWEIGHT}, {RELIGION AND SUPERNATU-\nRAL}, {GROUPS AND AFFILIATION} and \n{HEALTH AND DISEASE}. \n The lexicon provides direct access to the se-\nmantic field information for large number of \nMWEs and their constituent words. Furthermore, \nthe lexicon was analysed and classified manually \nby a team of linguists based on the analysis of \ncorpus data and consultation of printed and elec-\ntronic corpus-based dictionaries, ensuring a high \nlevel of consistency and accuracy of the semantic \nanalysis.  \nIn our context, we interpret the task of measur-\ning the compositionality of MWEs as examining \nthe distance between the semantic tag of a MWE \nand the semantic tags of its constituent words. \n                                                 \n3 For the complete semantic tagset, see website: \nhttp:\/\/www.comp.lancs.ac.uk\/ucrel\/usas\/ \nGiven a MWE M and its constituent words wi (i \n= 1, .., n), the compositionality D can be meas-\nured by multiplying the semantic distance SD \nbetween M and each of its constituent words wi. \nIn practice, the square root of the product is used \nas the score in order to reduce the range of actual \nD-scores, as shown below: \n \n(1)   \u220f\n=\n=\nn\ni\niwMSDMD\n1\n),()(  \n \nwhere D-score ranges between [0, 1], with 1 in-\ndicating the strongest compositionality and 0 the \nweakest compositionality. \nIn the semantic lexicon, as the semantic in-\nformation of function words is limited, they are \nclassified into a single grammatical bin (denoted \nby tag Z5). In our algorithm, they are excluded \nfrom the measuring process by using a stop word \nlist. Therefore, only the content constituent \nwords are involved in measuring the composi-\ntionality. Although function words may form an \nimportant part of many MWEs, such as phrasal \nverbs, because our algorithm solely relies on se-\nmantic field information, we assume they can be \nignored.  \n The semantic distance between a MWE and \nany of its constituent words is calculated by \nquantifying the similarity between their semantic \nfield categories. In detail, if the MWE and a con-\nstituent word do not share any of the major 21 \nsemantic domains, the SD is assigned a small \nvalue \u03bb.4 If they do, three possible cases are con-\nsidered: \n \nCase a. If they share the same tag, and the con-\nstituent word has only one tag, then SD \nis one. \nCase b. If they share a tag or tags, but the con-\nstituent words have multiple candidate \ntags, then SD is weighted using a vari-\nable \u03b1 based on the position of the \nmatched tag in the candidate list as well \nas the number of candidate tags. \nCase c. If they share a major category, but their \ntags fall into different sub-categories \n(denoted by the trailing digits following \na letter), SD is further weighted using a \n                                                 \n4 We avoid using zero here in order to avoid producing se-\nmantic distance of zero indiscriminately when any one of \nthe constituent words produces zero distance regardless of \nother constituent words. \n4\nvariable \u03b2 which reflects the difference \nof the sub-categories. \nWith respect to weight \u03b1, suppose L is the \nnumber of candidate tags of the constituent word \nunder consideration, N is the position of the spe-\ncific tag in the candidate list (the position starts \nfrom the top with N=1), then the weight \u03b1 is cal-\nculated as \n \n(2)  2\n1\nL\nNL +\u2212=\u03b1 , \n \nwhere N=1, 2, \u2026, n and N<=L. Ranging between \n[1, 0), \u03b1 takes into account both the location of \nthe matched tag in the candidate tag list and the \nnumber of candidate tags. This weight penalises \nthe words having more candidate semantic tags \nby giving a lower value for their higher degree of \nambiguity. As either L or N increases, the \u03b1-\nvalue decreases.       \nRegarding the case c), where the tags share the \nsame head letter but different digit codes, i.e. \nthey are from the same major category but in \ndifferent sub-categories, the weight \u03b2 is calcu-\nlated based on the number of sub-categories they \nshare. As we mentioned earlier, a semantic tag \nconsists of an initial letter and some trailing dig-\nits divided by points, e.g. S1.1.2 {RECIPROC-\nITY}, S1.1.3 {PARTICIPATION}, S1.1.4 {DE-\nSERVE} etc. If we let T1, T2 be a pair of semantic \ntags with the same initial letters, which have ki \nand kj trailing digit codes (denoting the number \nof sub-division layers) respectively and share n \ndigit codes from the left, or from the top layer, \nthen \u03b2 is calculated as follows: \n \n(3)   \nk\nn=\u03b2 ; \n(4)   . ),max( ji kkk =\n \nwhere \u03b2 ranges between (0, 1). In fact, the cur-\nrent USAS taxonomy allows only the maximum \nthree layers of sub-division, therefore \u03b2 has one \nof three possible scores: 0.500 (1\/2), 0.333 (1\/3) \nand 0.666 (2\/3). In order to avoid producing zero \nscores, if the pair of tags do not share any digit \ncodes except the head letter, then n is given a \nsmall value of 0.5. \nCombining all of the weighting scores, the \nsemantic distance SD in formula (1) is calculated \nas follows: \n \n(5)  ( )\n\u23aa\u23aa\n\u23aa\n\u23a9\n\u23aa\u23aa\n\u23aa\n\u23a8\n\u23a7\n=\n\u220f\n\u220f\n=\n=\n.  then   c), if\n;  then   b), if\n1;  then   a), if\n;   then   matches,   tagno if\n,\n1\n1\nn\ni\nii\nn\ni\niiwMSD\n\u03b2\u03b1\n\u03b1\n\u03bb\n \nwhere \u03bb is given a small value of 0.001 for our \nexperiment5. \nSome MWEs and single words in the lexicon \nare assigned with combined semantic categories \nwhich are considered to be inseparable, as shown \nbelow: \npetrol_NN1 station_NN1 M3\/H1 \nwhere the slash means that this MWE falls under \nthe categories of M3 {VEHICLES AND TRANS-\nPORTS ON LAND} and H1 {ARCHITECTURE \nAND KINDS OF HOUSES AND BUILDINGS} \nat the same time. For such cases, criss-cross \ncomparisons between all possible tag pairs are \ncarried out in order to find the optimal match \nbetween the tags of the MWE and its constituent \nwords. \nBy way of further explanation, the word \n\u201cbrush\u201d as a verb has candidate semantic tags of \nB4 {CLEANING AND PERSONAL CARE} and \nA1.1.1 {GENERAL ACTION, MAKING} etc. On \nthe other hand, the phrasal verb \u201cbrush down\u201d \nmay fall under either B4 category with the sense \nof cleaning or G2.2 category {ETHICS} with the \nsense of reprimand. When we apply our algo-\nrithm to it, we get the D-score of 1.0000 for the \nsense of cleaning, indicating a high degree of \ncompositionality, whereas we get a low D-score \nof 0.0032 for the sense of reprimand, indicating \na low degree of compositionality. Note that the \nword \u201cdown\u201d in this MWE is filtered out as it is \na functional word. \nThe above example has only a single constitu-\nent content word. In practice, many MWEs have \nmore complex structures than this example. In \norder to test the performance of our algorithm, \nwe compared its output against human judgments \nof compositionality, as reported in the following \nsection. \n4 Manually Ranking MWEs for \nEvaluation \nIn order to evaluate the performance of our \ntool against human judgment, we prepared a list \n                                                 \n5 As long as \u03bb is small enough, it does not affect the ranking \nof D-scores. \n5\nof 89 MWEs6 and asked human raters to rank \nthem via a website. The list includes six MWEs \nwith multiple senses, and these were treated as \nseparate MWE. The Lancaster MWE lexicon has \nbeen compiled manually by expert linguists, \ntherefore we assume that every item in this lexi-\ncon is a true MWE, although we acknowledge \nthat some errors may exist. \nFollowing McCarthy et al.\u2019s approach, we \nasked the human raters to assign each MWE a \nnumber ranging between 0 (opaque) and 10 \n(fully compositional). Both native and non-native \nspeakers are involved, but only the data from \nnative speakers are used in this evaluation. As a \nresult, three groups of raters were involved in the \nexperiment.  Group 1 (6 people) rated MWEs \nwith indexes of 1-30, Group 2 (4 people) rated \nMWEs with indexes of 31-59 and Group 3 (five \npeople) rated MWEs with indexes of 6-89. \nIn order to test the level of agreement between \nthe raters, we used the procedures provided in \nthe 'irr' package for R (Gamer, 2005). With this \ntool, the average intraclass correlation coefficient \n(ICC) was calculated for each group of raters \nusing a two-way agreement model (Shrout & \nFleiss, 1979). As a result, all ICCs exceeded 0.7 \nand were significant at the 95% confidence level, \nindicating an acceptable level of agreement be-\ntween raters. For Group 1, the ICC was 0.894 \n(95% ci = 0.807 < ICC < 0.948), for Group 2 it \nwas 0.9 (95% ci=0.783<ICC<0.956) and for \nGroup 3 it was 0.886 (95% ci =  0.762 < ICC < \n0.948). \nBased on this test, we conclude that the man-\nual ranking of the MWEs is reliable and is suit-\nable to be used in our evaluation. Source data for \nthe human judgements is available from our \nwebsite in spreadsheet form7. \n5 Evaluation \nIn our evaluation, we focused on testing the \nperformance of the D-score against human rat-\ners\u2019 judgment on ranking different MWEs by \ntheir degree of compositionality, as well as dis-\ntinguishing the different degrees of composition-\nality for each sense in the case of multiple tags.  \nThe first step of the evaluation was to imple-\nment the algorithm in a program and run the tool \non the 89 test MWEs we prepared. Fig. 1 illus-\ntrates the D-score distribution in a bar chart. As \nshown by the chart, the algorithm produces a \nwidely dispersed distribution of D-scores across \n                                                 \n6 Selected at random from the Lancaster semantic lexicon. \n7 http:\/\/ucrel.lancs.ac.uk\/projects\/assist\/ \nthe sample MWEs, ranging from 0.000032 to \n1.000000. For example, the tool assigned the \nscore of 1.0 to the FOOD sense and 0.001 to the \nTHIEF senses of \u201ctea leaf\u201d successfully distin-\nguishing the different degrees of compositional-\nity of these two senses. \n \nMWE Compositionality Distribution\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1 6 11 16 21 26 31 36 41 46 51 56 61 66 71 76 81 86\n89 MWEs\nD\n-s\nco\nre\n \n \nFig 1: D-score distribution across 89 sample \nMWEs \n \nAs shown in Fig. 1, some MWEs share the \nsame scores, reflecting the limitation of the num-\nber of ranks that our algorithm can produce as \nwell as the limited amount of semantic informa-\ntion available from a lexicon. Nonetheless, the \nalgorithm produced 45 different scores which \nranked the MWEs into 45 groups (see the steps \nin the figure). Compared to the eleven scores \nused by the human raters, this provides a fine-\ngrained ranking of the compositionality.   \nThe primary issue in our evaluation is the ex-\ntent to which the automatic ranking of the MWEs \ncorrelates with the manual ranking of them. As \ndescribed in the previous section, we created a \nlist of 89 manually ranked MWEs for this pur-\npose. Since we are mainly interested in the ranks \nrather than the actual scores, we examined the \ncorrelation between the automatic and manual \nrankings using Spearman\u2019s correlation coeffi-\ncient. (For the full ranking list, see Appendix). \nIn the manually created list, each MWE was \nranked by 3-6 human raters. In order to create a \nunified single test data of human ranking, we \ncalculated the average of the human ranks for \neach MWE. For example, if two human raters \ngive ranks 3 and 4 to a MWE, then its rank is \n(3+4)\/2=3.5. Next, the MWEs are sorted by the \naveraged ranks in descending order to obtain the \ncombined ranks of the MWEs. Finally, we sorted \nthe MWEs by the D-score in the same way to \nobtain a parallel list of automatic ranks. For the \ncalculation of Spearman\u2019s correlation coefficient, \nif n MWEs are tied to a score (either D-score or \nthe average manual ranks), their ranks were ad-\n6\njusted by dividing the sum of their ranks by the \nnumber of MWEs involved. Fig. 2 illustrates the \ncorrespondence between the adjusted automatic \nand manual rankings. \n \nAuto vs. Manual Ranks Comparison\n(n=89, rho=0.2572)\n0\n20\n40\n60\n80\n100\n0 20 40 60 80 100\nauto ranks\nm\nan\nua\nl r\nan\nks\n \n \nFig. 2: Scatterplot of automatic vs. manual \nranking. \n \nAs shown in Fig. 2, the overall correlation seems \nquite weak. In the automatic ranking, quite a few \nMWEs are tied up to three ranks, illustrated by \nthe vertically aligned points. The precise correla-\ntion between the automatic and manual rankings \nwas calculated using the function provided in R \nfor Windows 2.2.1.  Spearman's rank correlation \n(rho) for these data was 0.2572 (p=0.01495), \nindicating a significant though rather weak posi-\ntive relationship. \nIn order to find the factors causing this weak \ncorrelation, we tested the correlation for those \nMWEs whose rank differences were less than 20, \n30, 40 and 50 respectively. We are interested to \nfind out how many of them fall under each of the \ncategories and which of their features affected \nthe performance of the algorithm. As a result, we \nfound 43, 54, 66 and 77 MWEs fall under these \ncategories respectively, which yield different \ncorrelation scores, as shown in Table 1.  \n \nnumb of \nMWEs \nPercent \n(%) \nRank \ndiff \nrho-\nscore \nSig. \n43 48.31 <20 0.9149 P<0.001 \n54 60.67 <30 0.8321 P<0.001 \n66 74.16 <40 0.7016 P<0.001 \n77 86.52 <50 0.5084 P<0.001 \n89 (total) 100.00 <=73 0.2572 P<0.02 \n \nTable 1: Correlation coefficients corresponding \ndifferent rank differences. \n \nAs we expected, the rho decreases as the rank \ndifference increases, but all of the four categories \ncontaining a total of 77 MWEs (86.52%) show \nreasonably high correlations, with the minimum \nscore of 0.5084. 8 In particular, 66 of them \n(74.16%), whose ranking differences are less \nthan 40, demonstrate a strong correlation with \nrho-score 0.7016, as illustrated by Fig. 3 \n \nScatterPlot of Auto vs. Man Ranks for 66 MWEs\n(rank_diff < 40)\n0\n20\n40\n60\n80\n100\n0 20 40 60 80 10auto ranks\nm\nan\n ra\nnk\ns\n0\n \n \nFig 3: ScatterPlot for 66 MWEs (rank_diff < \n40) which shows a strong correlation \n \nOur manual examination shows that the algo-\nrithm generally pushes the highly compositional \nand non-compositional MWEs towards opposite \nends of the spectrum of the D-score. For example, \nthose assigned with score 1 include \u201caid worker\u201d, \n\u201caudio tape\u201d and \u201cunemployment figure\u201d. On the \nother hand, MWEs such as \u201ctea leaf\u201d (meaning \nthief), \u201ckick the bucket\u201d and \u201chot dog\u201d are given \na low score of 0.001. We assume these two \ngroups of MWEs are generally treated as highly \ncompositional and opaque MWEs respectively. \nHowever, the algorithm could be improved. A \nmajor problem found is that the algorithm pun-\nishes longer MWEs which contain function \nwords. For example, \u201cmake an appearance\u201d is \nscored 0.000114 by the algorithm, but when the \narticle \u201can\u201d is removed, it gets a higher score \n0.003608. Similarly, when the preposition \u201cup\u201d \nis removed from \u201ckeep up appearances\u201d, it gets \n0.014907 compared to the original 0.000471, \nwhich would push up their rank much higher. To \naddress this problem, the algorithm needs to be \nrefined to minimise the impact of the function \nwords to the scoring process. \nOur analysis also reveals that 12 MWEs with \nrank differences (between automatic and manual \nranking) greater than 50 results in a degraded \noverall correlation. Table 2 lists these words, in \nwhich the higher ranks indicate higher composi-\ntionality.  \n \n                                                 \n8 Salkind (2004: 88) suggests that r-score ranges 0.4~0.6, \n0.6~0.8 and 0.8~1.0 indicate moderate, strong and very \nstrong correlations respectively. \n7\nMWE Sem. Tag9 Auto \nrank \nManual \nrank \nplough into A9- 53.5 3 \nBloody Mary F2 53.5 2 \npillow fight K6 26 80.5 \nlollipop lady M3\/S2 70 15 \ncradle snatcher S3.2\/T3\/S2 73.5 17.5 \ngo bananas X5.2+++ 65 8.5 \nmake an appearance S1.1.3+ 2 58.5 \nkeep up appearances A8\/S1.1.1 4 61 \nsandwich course P1 69 11.5 \ngo bananas B2-\/X1 68 10 \nEskimo roll M4 71.5 5 \nin other words Z4 12.5 83 \n \nTable 2: Twelve MWEs having rank differences \ngreater than 50. \n \nLet us take \u201cpillow fight\u201d as an example. The \nwhole expression is given the semantic tag K6, \nwhereas neither \u201cpillow\u201d nor \u201cfight\u201d as individ-\nual word is given this tag. In the lexicon, \u201cpil-\nlow\u201d is classified as H5 {FURNITURE AND \nHOUSEHOLD FITTINGS} and \u201cfight\u201d is as-\nsigned to four semantic categories including S8- \n{HINDERING}, X8+ {HELPING}, E3- {VIO-\nLENT\/ANGRY}, and K5.1 {SPORTS}. For this \nreason, the automatic score of this MWE is as \nlow as 0.003953 on the scale of [0, 1]. On the \ncontrary, human raters judged the meaning of \nthis expression to be fairly transparent, giving it \na high score of 8.5 on the scale of [0, 10]. Similar \ncontrasts occurred with the majority of the \nMWEs with rank differences greater than 50, \nwhich are responsible for weakening the overall \ncorrelation. \nAnother interesting case we noticed is the \nMWE \u201cpass away\u201d. This MWE has two major \nsenses in the semantic lexicon L1- {DIE} and \nT2- {END} which were ranked separately. Re-\nmarkably, they were ranked in the opposite order \nby human raters and the algorithm. Human raters \nfelt that the sense DIE is less idiomatic, or more \ncompositional, than END, while the algorithm \nindicated otherwise. The explanation of this \nagain lies in the semantic classification of the \nlexicon, where \u201cpass\u201d as a single word contains \nthe sense T2- but not L1-. Consequently, the \nautomatic score for \u201cpass away\u201d with the sense \n                                                 \n                                                \n9 Semantic tags occurring in Table 2: A8 (seem), A9 (giving \npossession), B2 (health and disease), F2 (drink), K6 (chil-\ndren\u2019s games and toys), M3 (land transport), M4 (swim-\nming), P1 (education), S1.1.1 (social actions), S1.1.3 (par-\nticipation), S2 (people), S3.2 (relationship), T3 (time: age), \nX1 (psychological actions), X5.2 (excited), Z4 (discourse \nbin) \nL1- is much lower (0.001) than that with the \nsense of T2- (0.007071). \nIn order to evaluate our algorithm in compari-\nson with previous work, we also tested it on the \nmanual ranking list created by McCarthy et al \n(2003).10 We found that 79 of the 116 phrasal \nverbs in that list are included in the Lancaster \nsemantic lexicon. We applied our algorithm on \nthose 79 items to compare the automatic ranks \nagainst the average manual ranks using the \nSpearman\u2019s rank correlation coefficient (rho). As \na result, we obtained rho=0.3544 with signifi-\ncance level of p=0.001357. This result is compa-\nrable with or better than most measures reported \nby McCarthy et al (2003). \n6 Discussion \nThe algorithm we propose in this paper is dif-\nferent from previous proposed statistical methods \nin that it employs a semantic lexical resource in \nwhich the semantic field information is directly \naccessible for both MWEs and their constituent \nwords. Often, typical statistical algorithms meas-\nure the semantic distance between MWEs and \ntheir constituent words by comparing their con-\ntexts comprising co-occurrence words in near \ncontext extracted from large corpora, such as \nBaldwin et al\u2019s algorithm (2003). \nWhen we consider the definition of the com-\npositionality as the extent to which the meaning \nof the MWE can be guessed based on that of its \nconstituent words, a semantic lexical resource \nwhich maps MWEs and words to their semantic \nfeatures provides a practical way of measuring \nthe MWE compositionality. The Lancaster se-\nmantic lexicon is one such lexical resource \nwhich allows us to have direct access to semantic \nfield information of large number of MWE and \nsingle words. Our experiment demonstrates the \npotential value of such semantic lexical resources \nfor the automatic measurement of MWE compo-\nsitionality. Compared to statistical algorithms \nwhich can be affected by a variety of un-\ncontrollable factors, such as size and domain of \ncorpora, etc., an expert-compiled semantic lexi-\ncal resource can provide much more reliable and \n\u201cclean\u201d lexical semantic information. \nHowever, we do not suggest that algorithms \nbased on semantic lexical resources can substi-\ntute corpus-based statistical algorithms. Rather, \nwe suggest it as a complement to existing statis-\ntical algorithms. As the errors of our algorithm \n \n10This list is available at website: \nhttp:\/\/mwe.stanford.edu\/resources\/  \n8\nreveal, the semantic information provided by the \nlexicon alone may not be rich enough for a very \nfine-grained distinction of MWE compositional-\nity. In order to obtain better results, this algo-\nrithm needs to be combined with statistical tech-\nniques. \nA limitation of our approach is language-\ndependency. In order to port our algorithm to \nlanguages other than English, one needs to build \nsimilar semantic lexicon in those languages. \nHowever, similar semantic lexical resources are \nalready under construction for some other lan-\nguages, including Finnish and Russian (L\u00f6fberg \net al., 2005; Sharoff et al., 2006), which will al-\nlow us to port our algorithm to those languages. \n7 Conclusion \nIn this paper, we explored an algorithm based \non a semantic lexicon for automatically measur-\ning the compositionality of MWEs. In our \nevaluation, the output of this algorithm showed \nmoderate correlation with a manual ranking. We \nclaim that semantic lexical resources provide \nanother approach for automatically measuring \nMWE compositionality in addition to the exist-\ning statistical algorithms. Although our results \nare not yet conclusive due to the moderate scale \nof the test data, our evaluation demonstrates the \npotential of lexicon-based approaches for the \ntask of compositional analysis. We foresee, by \ncombining our approach with statistical algo-\nrithms, that further improvement can be ex-\npected. \n8 Acknowledgement \nThe work reported in this paper was carried \nout within the UK-EPSRC-funded ASSIST Pro-\nject (Ref. EP\/C004574). \nReferences  \nTimothy Baldwin, Colin Bannard, Takaaki Tanaka, \nand Dominic Widdows. 2003. An Empirical Model \nof Multiword Expression Compositionality. In \nProc. of the ACL-2003 Workshop on Multiword \nExpressions: Analysis, Acquisition and Treatment,  \npages 89-96, Sapporo, Japan. \nColin Bannard, Timothy Baldwin, and Alex Las-\ncarides. 2003. A statistical approach to the seman-\ntics of verb-particles. In Proc. of the ACL2003 \nWorkshop on Multiword Expressions: Analysis, \nAcquisition and Treatment, pages 65\u201372, Sapporo. \nNicoletta Calzolari, Charles Fillmore, Ralph Grish-\nman, Nancy Ide, Alessandro Lenci, Catherine \nMacLeod, and Antonio Zampolli. 2002. Towards \nbest practice for multiword expressions in compu-\ntational lexicons. In Proc. of the Third Interna-\ntional Conference on Language Resources and \nEvaluation (LREC 2002), pages 1934\u20131940, Las \nPalmas, Canary Islands. \nAnn Copestake, Fabre Lambeau, Aline Villavicencio, \nFrancis Bond, Timothy Baldwin, Ivan A. Sag, and \nDan Flickinger. 2002. Multiword expressions: Lin-\nguistic precision and reusability. In Proc. of the \nThird International Conference on Language Re-\nsources and Evaluation (LREC 2002), pages 1941\u2013\n1947, Las Palmas, Canary Islands. \nMatthias  Gamer. 2005. The irr Package: Various Co-\nefficients of Interrater Reliability and Agreement. \nVersion 0.61 of 11 October 2005.  Available from:   \ncran.r-project.org\/src\/contrib\/Descriptions\/irr.html \nDekang Lin. 1999. Automatic identification of non-\ncompositional phrases. In Proc. of the 37th Annual \nMeeting of the ACL, pages 317\u2013324, College Park, \nUSA. \nLaura L\u00f6fberg, Scott Piao, Paul Rayson, Jukka-Pekka \nJuntunen, Asko Nyk\u00e4nen, and Krista Varantola. \n2005. A semantic tagger for the Finnish language. \nIn Proc. of the Corpus Linguistics 2005 conference, \nBirmingham, UK. \nChristopher D. Manning and Hinrich Sch\u00fctze. 2000. \nFoundations of Statistical Natural Language Proc-\nessing. The MIT Press, Cambridge, Massachusetts. \nDiana McCarthy, Bill Keller, and John Carroll. 2003. \nDetecting a continuum of compositionality in \nphrasal verbs. In Proc. of the ACL-2003 Workshop \non Multiword Expressions: Analysis, Acquisition \nand Treatment, pages 73\u201380, Sapporo, Japan. \nDan Melamed. 1997. Automatic discovery of non-\ncompositional compounds in parallel data. In Proc. \nof the 2nd Conference on Empirical Methods in \nNatural Language Processing , Providence, USA. \nGeoffrey Nunberg, Ivan A. Sag, and Tom Wasow. \n1994. Idioms. Language, 70: 491\u2013538. \nScott S.L. Piao, Paul Rayson, Dawn Archer and Tony \nMcEnery. 2004. Evaluating Lexical Resources for \na Semantic Tagger. In Proc. of LREC-04, pages \n499\u2013502, Lisbon, Portugal. \nScott S.L. Piao, Dawn Archer, Olga Mudraya, Paul \nRayson, Roger Garside, Tony McEnery and An-\ndrew Wilson. 2005a. A Large Semantic Lexicon \nfor Corpus Annotation. In Proc. of the Corpus Lin-\nguistics Conference 2005, Birmingham, UK. \nScott S.L. Piao., Paul Rayson, Dawn Archer, Tony \nMcEnery. 2005b. Comparing and combining a se-\nmantic tagger and a statistical tool for MWE ex-\ntraction. Computer Speech and Language, 19, 4: \n378\u2013397. \n9\nPaul Rayson, Dawn Archer, Scott Piao, and Tony \nMcEnery. 2004. The UCREL Semantic Analysis \nSystem. In Proc. of LREC-04 Workshop: Beyond \nNamed Entity Recognition Semantic Labeling for \nNLP Tasks, pages 7\u201312, Lisbon, Portugal. \nSusanne Riehemann. 2001. A Constructional Ap-\nproach to Idioms and Word Formation. Ph.D. the-\nsis, Stanford University, Stanford. \n Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann \nCopestake, and Dan Flickinger. 2002. Multiword \nExpressions: A Pain in the Neck for NLP. In Proc. \nof the 3rd International Conference on Intelligent \nText Processing and Computational Linguistics \n(CICLing-2002), pages 1\u201315, Mexico City, Mexico. \nNeil J. Salkind. 2004. Statistics for People Who Hate \nStatistics. Sage: Thousand Oakes, US. \nSerge Sharoff, Bogdan Babych, Paul Rayson, Olga \nMudraya and Scott Piao. 2006. ASSIST: Auto-\nmated semantic assistance for translators. Proceed-\nings of EACL 2006, pages 139\u2013142, Trento, Italy. \nPatrick E. Shrout and Joseph L. Fleiss. 1979. Intra-\nclass Correlations: Uses in Assessing Rater Reli-\nability. Psychological Bulletin (2), 420\u2013428. \nSriram Venkatapathy and Aravind K. Joshi. 2005. \nMeasuring the relative compositionality of verb-\nnoun (V-N) collocations by integrating features. In \nProc. of Human Language Technology Conference \nand Conference on Empirical Methods in Natural \nLanguage Processing (HLT\/EMNLP 2005), pages \n899\u2013906, Vancouver, Canada. \n \nAppendix: Manual vs. Automatic Ranks \nof Sample MWEs \nThe table below shows the human and auto-\nmatic rankings of 89 sample MWEs. The MWEs \nare sorted in ascending order by manual average \nranks. The top items are supposed to be the most \ncompositional ones. For example, according to \nthe manual ranking, facial expression is the most \ncompositional MWE while tea leaf is the most \nopaque one. This table also shows that some \nMWEs are tied up with the same ranks. For the \ndefinitions of the full semantic tagset, see web-\nsite http:\/\/www.comp.lancs.ac.uk\/ucrel\/usas\/. \n \nMWE Tag Sem tag Man \nrank \nAuto. \nrank \nfacial expression B1 1 9 \naid worker S8\/S2 2 4 \naudio tape K3 3.5 4 \nleisure activities K1 3.5 36.5 \nadvance warning T4\/Q2.2 5 36.5 \nliving space H2 6 51 \nin other words Z4 7 77.5 \nunemployment fig-\nures \nI3.1\/N5 8 4 \ncamera angle Q4.3 9.5 45 \npillow fight K6 9.5 64 \nyouth club S5\/T3 11.5 4 \npetrol station M3\/H1 11.5 36.5 \npalm tree L3 13 9 \nrule book G2.1\/Q4.1 14 4 \nball boy K5.1\/S2.2 15 13 \ngoal keeper K5.1\/S2 16.5 4 \nkick in E3- 16.5 36.5 \nventilation shaft H2 18 47 \ndirectory enquiries Q1.3 19 14 \nphone box Q1.3\/H1 21 18.5 \nlose balance M1 21 53 \nbend the rules A1.7 21 54.5 \nbig nose X7\/X2.4 23 67 \nquantity control N5\/A1.7 24 11.5 \nact of God S9 25 36.5 \nair bag A15\/M3 26 62.5 \nmind stretching A12 27 59 \nplain clothes B5 28 36.5 \nkeep up appearances A8\/S1.1.1 29 86 \nexamining board P1 30 23 \nopen mind X6 31.5 49 \nmake an appearance S1.1.3+ 31.5 88 \ncable television Q4.3 33 15 \nking size N3.2 34 36.5 \naction point X7 35 61 \nkeep tight rein on A1.7 36 28 \nnoughts and crosses K5.2 37 77.5 \ntea leaf L3\/F2 38 4 \nsingle minded X5.1 39.5 77.5 \nwindow dressing I2.2 39.5 77.5 \nstreet girl G1.2\/S5 42 36.5 \njust over the horizon S3.2\/S2.1 42 60 \npressure group T1.1.3 42 16.5 \nair proof O4.1 44.5 57.5 \nheart of gold S1.2.2 44.5 77.5 \nlose heart X5.2 46 26 \nfood for thought X2.1\/X5.1 47 89 \nplay part S8 48 68 \nlook down on S1.2.3 49 77.5 \narm twisting Q2.2 50 36.5 \ntake into account A1.8 51 69 \nkidney bean F1 52 9 \ncome alive A3+ 53 52 \nbreak new ground T3\/T2 54 54 \nmake up to S1.1.2 55 65 \nby virtue of C1 56.5 36.5 \nsnap shot A2.2 56.5 27 \npass away L1- 58 77.5 \nlong face E4.1 59 77.5 \nbossy boots S1.2.3\/S2 60 77.5 \nplough into M1\/A1.1.2 61 11.5 \nkick in T2+ 62 50 \nanimal magnetism S1.2 63 55.5 \nsixth former P1\/S2 64 77.5 \npull the strings S7.1 65 62.5 \ncouch potato A1.1.1\/S2 66 77.5 \nthink tank S5\/X2.1 67 36.5 \ncome alive X5.2+ 68 24 \nhot dog F1 69 77.5 \ncheap shot G2.2-\/Q2.2 70 66 \n10\nrock and roll K2 71 48 \nbright as a button S3.2\/T3\/S2 72.5 87 \ncradle snatcher X9.1+ 72.5 16.5 \nalpha wave B1 74 77.5 \nlollipop lady M3\/S2 75 20 \npass away X5.2+ 76.5 57.5 \nplough into T2- 76.5 36.5 \npiece of cake P1 78.5 77.5 \nsandwich course A12 78.5 21 \ngo bananas B2-\/X1 80 22 \ngo bananas X5.2+++ 81.5 36.5 \ngo bananas E3- 81.5 25 \nkick the bucket L1 83 77.5 \non the wagon F2 84 36.5 \nEskimo roll M4 85 18.5 \nacid house K2 86 46 \nplough into A9- 87 36.5 \nBloody Mary F2 88 36.5 \ntea leaf G2.1-\/S2mf 89 77.5 \n \n11\n"}