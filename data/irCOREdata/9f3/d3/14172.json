{"doi":"10.1080\/0968776042000259573","coreId":"14172","oai":"oai:generic.eprints.org:611\/core5","identifiers":["oai:generic.eprints.org:611\/core5","10.1080\/0968776042000259573"],"title":"Dont write, just mark: the validity of assessing student ability via their computerized peer-marking of an essay rather than their creation of an essay","authors":["Davies, Phil"],"enrichments":{"references":[{"id":196404,"title":"(2003a) Peer-assessment: no marks required just feedback?\u2014evaluating the quality of computerized peer-feedback compared with computerized peer-marking, in:","authors":[],"date":null,"doi":null,"raw":"Davies, P (2003a) Peer-assessment: no marks required just feedback?\u2014evaluating the quality of computerized peer-feedback compared with computerized peer-marking, in: J. Cook & D.","cites":null},{"id":196397,"title":"A taxonomy of educational objectives: handbook of cognitive domain","authors":[],"date":"1956","doi":"10.1177\/001316445601600310","raw":"Bloom, B. (1956) A taxonomy of educational objectives: handbook of cognitive domain (New York, McKay).","cites":null},{"id":196405,"title":"Communities of Practice, Research","authors":[],"date":"2003","doi":null,"raw":"McConnell (Eds)  Communities of Practice, Research Proceedings of the 10th  Association for Learning Technology Conference (ALT-C 2003), University of Sheffield and Sheffield Hallam University, 8\u201310 September.","cites":null},{"id":196401,"title":"Computerized peer assessment,","authors":[],"date":"2000","doi":"10.1080\/135580000750052955","raw":"Davies, P. (2000) Computerized peer assessment, Innovations in Education and Training International, 37(4), 346\u2013355.","cites":null},{"id":449764,"title":"Deepening computer programming skills by using Web-based peer assessment,","authors":[],"date":"2003","doi":"10.1109\/icalt.2003.1215052","raw":"Sitthiworachart, J. & Joy, M. (2003) Deepening computer programming skills by using Web-based peer assessment, 4th Annual LTSN-ICS Conference, Galway.","cites":null},{"id":196400,"title":"Feedback for web-based assignments,","authors":[],"date":"2001","doi":"10.1046\/j.0266-4909.2001.00185.x","raw":"Collis, B., De Boer, W. & Slotman, K. (2001) Feedback for web-based assignments, Journal of Computer Assisted Learning, 17, 306\u2013313.","cites":null},{"id":449552,"title":"Improving feedback to and from students, in: P. Knight (Ed.) Assessment for learning in higher education","authors":[],"date":"1995","doi":null,"raw":"Falchikov, N. (1995) Improving feedback to and from students, in: P. Knight (Ed.) Assessment for learning in higher education (London, Kogan Page).","cites":null},{"id":449763,"title":"In search of fairness: an application of multi-reviewer anonymous peer review in a large class,","authors":[],"date":"2002","doi":"10.1080\/03098770220129451","raw":"Robinson, J. (2002) In search of fairness: an application of multi-reviewer anonymous peer review in a large class, Journal of Further and Higher Education, 26(2), 183\u2013192.","cites":null},{"id":449762,"title":"Patterns in student\u2013student commenting,","authors":[],"date":"2002","doi":"10.1109\/te.2002.1024619","raw":"Rada, R. & Hu, K. (2002) Patterns in student\u2013student commenting,  IEEE Transactions on Education, 45(3), 262\u2013267.","cites":null},{"id":196398,"title":"Peer assessment: principles, a case, and computer support, paper presented at the LTSN-ICS workshop in Computer Assisted Assessment,","authors":[],"date":"2001","doi":null,"raw":"Bostock, S. (2001) Peer assessment: principles, a case, and computer support, paper presented at the LTSN-ICS workshop in Computer Assisted Assessment, Warwick University, 5\u20136 April. Available online at: http:\/\/www.keele.ac.uk\/depts\/cd\/Stephen_Bostock\/docs\/ltsnicsapril 2001.pdf.","cites":null},{"id":449553,"title":"Peer feedback marking: developing peer assessment,","authors":[],"date":"1995","doi":"10.1080\/1355800950320212","raw":"Falchikov, N. (1995a) Peer feedback marking: developing peer assessment,  Innovations in Education and Training International, 32(2), 175\u2013187.","cites":null},{"id":196399,"title":"Peer learning and assessment,","authors":[],"date":"1999","doi":"10.1080\/0260293990240405","raw":"Boud, D., Cohen, R. & Sampson, J. (1999) Peer learning and assessment,  Assessment and Evaluation in Higher Education, 24(4), 413\u2013426.","cites":null},{"id":1042429,"title":"Peer, self and tutor assessment: relative reliabilities,","authors":[],"date":"1994","doi":"10.1080\/03075079412331382153","raw":"Stefani, L. A. J. (1994) Peer, self and tutor assessment: relative reliabilities, Studies in Higher Education, 19(1), 69\u201375.","cites":null},{"id":449760,"title":"Self, peer and tutor assessment of text online: design, delivery and analysis,","authors":[],"date":"2003","doi":null,"raw":"Parsons, R. (2003) Self, peer and tutor assessment of text online: design, delivery and analysis, in: J. Christie (Ed.) Proceedings of 7th International CAA Conference, July, 315\u2013326.","cites":null},{"id":449554,"title":"Student peer assessment in higher education: a meta-analysis comparing peer and teacher marks,","authors":[],"date":"2000","doi":"10.3102\/00346543070003287","raw":"Falchikov, N. & Goldfinch, J. (2000) Student peer assessment in higher education: a meta-analysis comparing peer and teacher marks, Review of Educational Research, 70(3), 287\u2013322.","cites":null},{"id":449761,"title":"The art of assessing,","authors":[],"date":"1995","doi":null,"raw":"Race, P. (1995) The art of assessing, The New Academic, 4(3) and 5(1).","cites":null},{"id":449555,"title":"The automated peer-assisted assessment of programming skills, in: P. Chalk (Ed.)","authors":[],"date":"2004","doi":"10.1109\/itre.2004.1393651","raw":"Lewis, S. F. & Davies, P. (2004) The automated peer-assisted assessment of programming skills, in: P. Chalk (Ed.) Proceedings of the 8th JAVA and The Internet in the Computing Curriculum Conference JICC8, January.","cites":null},{"id":196403,"title":"There\u2019s no confidence in multiple choice testing, in: M. Danson & C. Eabry (Eds)","authors":[],"date":"2002","doi":null,"raw":"Davies, P. (2002b) There\u2019s no confidence in multiple choice testing, in: M. Danson & C. Eabry (Eds) Proceedings of the 6th Annual CAA Conference, Loughborough, July, 119\u2013130.Assessing student ability via peer-marking of an essay 277 Davies, P. (2003) Closing the communications loop on the computerized peer-assessment of essays, ALT-J, 11(1), 41\u201354.","cites":null},{"id":196396,"title":"Towards electronically assisted peer assessment: a case study,","authors":[],"date":"2001","doi":"10.1080\/09687760108656773","raw":"Bhalerao, A. & Ward, A. (2001) Towards electronically assisted peer assessment: a case study, ALT-J, 9(1), 26\u201337.","cites":null},{"id":196402,"title":"Using student reflective self-assessment for awarding degree classifications,","authors":[],"date":"2002","doi":"10.1080\/13558000210161034","raw":"Davies, P. (2002a) Using student reflective self-assessment for awarding degree classifications, Innovations in Education and Teaching International, 39(4), 307\u2013319.","cites":null},{"id":449759,"title":"Web-based peer assessment: feedback for students with various thinking-styles,","authors":[],"date":"2001","doi":"10.1046\/j.0266-4909.2001.00198.x","raw":"Lin, S. S. J., Liu, E. Z. F. & Yuan, S. M. (2001) Web-based peer assessment: feedback for students with various thinking-styles, Journal of Computer Assisted Learning, 17, 430\u2013432.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2004","abstract":"This paper reports on a case study that evaluates the validity of assessing students via a computerized peer-marking process, rather than on their production of an essay in a particular subject area. The study assesses the higher-order skills shown by a student in marking and providing consistent feedback on an essay. In order to evaluate the suitability of this method of assessment in judging a students ability, their results in performing this peer-marking process are correlated against their results in a number of computerized multiple-choice exercises and also the production of an essay in a cognate area of the subject being undertaken. The results overall show a correlation of the expected results in all three areas of assessment being undertaken, rated by the final grades of the students undertaking the assessment. The results produced by quantifying the quality of the marking and commenting of the students is found to map well to the overall expectations of the results produced for the cohort of students. It is also shown that the higher performing students achieve a greater improvement in their overall marks by performing the marking process than those students of a lower quality. This appears to support previous claims that awarding a 'mark for marking' rewards the demonstration of higher order skills of assessment. Finally, note is made of the impact that such an assessment method can have upon eradicating the possibility of plagiarism","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/14172.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/611\/1\/ALT_J_Vol12_No3_2004_Dont%20write%2C%20just%20mark_%20the%20val.pdf","pdfHashValue":"6e8029edd8429f5745a23290a5e6add9fa8e23aa","publisher":"Taylor and Francis Ltd","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:611<\/identifier><datestamp>\n      2011-04-04T09:05:18Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/611\/<\/dc:relation><dc:title>\n        Dont write, just mark: the validity of assessing student ability via their computerized peer-marking of an essay rather than their creation of an essay<\/dc:title><dc:creator>\n        Davies, Phil<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        This paper reports on a case study that evaluates the validity of assessing students via a computerized peer-marking process, rather than on their production of an essay in a particular subject area. The study assesses the higher-order skills shown by a student in marking and providing consistent feedback on an essay. In order to evaluate the suitability of this method of assessment in judging a students ability, their results in performing this peer-marking process are correlated against their results in a number of computerized multiple-choice exercises and also the production of an essay in a cognate area of the subject being undertaken. The results overall show a correlation of the expected results in all three areas of assessment being undertaken, rated by the final grades of the students undertaking the assessment. The results produced by quantifying the quality of the marking and commenting of the students is found to map well to the overall expectations of the results produced for the cohort of students. It is also shown that the higher performing students achieve a greater improvement in their overall marks by performing the marking process than those students of a lower quality. This appears to support previous claims that awarding a 'mark for marking' rewards the demonstration of higher order skills of assessment. Finally, note is made of the impact that such an assessment method can have upon eradicating the possibility of plagiarism.<\/dc:description><dc:publisher>\n        Taylor and Francis Ltd<\/dc:publisher><dc:date>\n        2004<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/611\/1\/ALT_J_Vol12_No3_2004_Dont%20write%2C%20just%20mark_%20the%20val.pdf<\/dc:identifier><dc:identifier>\n          Davies, Phil  (2004) Dont write, just mark: the validity of assessing student ability via their computerized peer-marking of an essay rather than their creation of an essay.  Association for Learning Technology Journal, 12 (3).  pp. 261-277.  ISSN 0968-7769 (print)\/1741-1629 (online)     <\/dc:identifier><dc:relation>\n        10.1080\/0968776042000259573<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/611\/","10.1080\/0968776042000259573"],"year":2004,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"ALT-J, Research in Learning Technology\nVol. 12, No. 3, September 2004\nISSN 0968\u20137769 (print)\/ISSN 1741\u20131629 (online)\/04\/030261\u201317\n\u00a9 2004 Association for Learning Technology\nDOI: 10.1080\/0968776042000259573\nDon\u2019t write, just mark: the validity of \nassessing student ability via their \ncomputerized peer-marking of an essay \nrather than their creation of an essay\nPhil Davies*\nUniversity of Glamorgan, UK\nTaylor and Francis LtdCALT120306.sgm10.1080\/ 968776042000259573ALT-J, Research in Learning Technology0968 7769 (pri t)\/1741-1629 (onli e)Original Article2 04ssoci tion for Learning Techno ogy23 000Sept mber 20 4Ph lDavi sSch ol of Co putingUniversity of G amorganPontypriddMid-Glamorgan Wouth WalesCF37 1DLpdavies@glam.ac.uk\nThis paper reports on a case study that evaluates the validity of assessing students via a\ncomputerized peer-marking process, rather than on their production of an essay in a particular\nsubject area. The study assesses the higher-order skills shown by a student in marking and providing\nconsistent feedback on an essay. In order to evaluate the suitability of this method of assessment in\njudging a student\u2019s ability, their results in performing this peer-marking process are correlated\nagainst their results in a number of computerized multiple-choice exercises and also the production\nof an essay in a cognate area of the subject being undertaken. The results overall show a correlation\nof the expected results in all three areas of assessment being undertaken, rated by the final grades\nof the students undertaking the assessment. The results produced by quantifying the quality of the\nmarking and commenting of the students is found to map well to the overall expectations of the\nresults produced for the cohort of students. It is also shown that the higher performing students\nachieve a greater improvement in their overall marks by performing the marking process than those\nstudents of a lower quality. This appears to support previous claims that awarding a \u2018mark for\nmarking\u2019 rewards the demonstration of higher order skills of assessment. Finally, note is made of\nthe impact that such an assessment method can have upon eradicating the possibility of plagiarism.\nIntroduction\nDavies (Davies, 2002a) reported a student comment \u2018I learnt more from marking the\nwork of others than from writing my own report\u2019. If this were to be true then does it\nnegate the need for students to create their own essays? This forms the basis of this\nstudy in making use of a computerized peer-assessment environment in order to\n*School of Computing, University of Glamorgan, Pontypridd, Mid-Glamorgan CF37 1DL, UK.\nEmail: pdavies@glam.ac.uk\n262 P. Davies\nassess a students\u2019 knowledge in a particular subject area, rather than making them\nwrite an essay in this area.\nThe benefits of peer-assessment are well documented with respect to student and\nstaff benefits (Falchikov, 1995b; Boud et al., 1999). Part of the negativity that\nsurrounds peer-assessment is the need for \u2018proof\u2019 that the end mark produced by the\npeers is equitable with respect to the mark that would have been provided by a tutor\nif they had performed the marking process (Falchikov & Goldfinch, 2000).\nNumerous studies have reported back favorable results with regard to the equity of\nstudent and tutor marking (Stefani, 1994). The majority of the research previously\ncarried out in the area of peer-assessment has tended to concentrate on the marking\naspect of the process. This generation of marks is only part of an assessment process\nwith feedback being of utmost importance (Collis et al., 2001). Some previous studies\nhave attempted to concentrate on the comments provided by the peers rather than\nmerely just the marks (Falchikov, 1995a). Assessing whether the quantification of the\nquality of peer-comments can also be used as a method for assessing the quality of an\nessay is a relatively new area of research (Davies, 2003a).\nIn recent years computerized systems have been introduced in order to provide\nmanaged environments to support the use of peer-assessment systems (Davies, 2000;\nBhalero & Ward, 2001; Bostock, 2001; Lin et al., 2001; Parsons, 2003) aimed at\nassessing the quality of essays and reports. Systems are now also being created that\nare attempting to utilize the inherent benefits of peer-assessment in other areas such\nas computer programming (Sitthiworachart & Joy, 2003; Lewis & Davies, 2004).\nThe higher-order skill of evaluation (Bloom, 1956) is an area that would be\nexpected to be assessed and used to identify the \u2018better\u2019 students. The method of\npeer-assessment detailed in this paper attempts to address this issue, and facilitate a\nmethod whereby these \u2018better\u2019 students will be able to demonstrate these skills and\nscore higher. Lin et al. (2001) noted that \u2018high executive students contributed\nsubstantially better feedback than the low executive thinkers\u2019. It will be interesting to\nascertain whether this maps to their ability in the marking of the work.\nRobinson (2002) suggests that as much as a third of the feedback provided by\nstudents was \u2018inadequate\u2019. Therefore students producing such feedback will receive\nfewer marks than those students who show consistency within both their marking and\ncommenting. Rada and Hu (2002) suggest that students should receive credit for\ndoing good comments. This was also taken account of in the study reported here.\nEssays are considered as a means of assessing the subjective skills of a student.\nHowever, lecturer marking can be highly subjective, \u2018Essays are demonstrably the\nform of assessment where the dangers of subjective marking are greatest\u2019 (Race,\n1995). If the attribution of marks for peer-marking as presented in this study is to be\nof any value, then the student\u2019s overall subjectivity should be removed (high and low\nmarking and commenting), and their ability in showing consistency of evaluative\nskills should be what is rewarded.\nDue to the experimental nature of this exercise, other assessments were included\nwithin the overall summative assessment process, namely objective tests (a series of\nMCQ tests including confidence testing) (Davies, 2002b) and also the creation and\nAssessing student ability via peer-marking of an essay 263\nsubmission of an essay of their choice (accompanied by a presentation) within the\nsubject area of the module (Networks and Internet Architectures). This ensured that\nthe assessment of the learning outcomes of the module was fully covered. It was\ndecided that in order to attain full student engagement within all aspects of the assess-\nment process, there would be an equal contribution of marks from each of the assess-\nment methods that contribute to the final summative grade.\nBackground\nOne of the aims of this work was to replace essay writing with peer assessment. The\nstudents were directed to research a particular area of study within a module, and\nthen used this to evaluate the work of a previous student. Part of the study was to\ninvestigate how a mark for the demonstration of evaluative skills can be mapped to an\nactual standard marking scale?\nThe study was undertaken during the Autumn Term 2003, within the School of\nComputing at the University of Glamorgan. The assessment was aimed at a cohort of\n34 students studying on the Post Higher National Diploma (HND) course of study.\nThis course is designed for students who have previously attained a HND and are\nusing this Post HND course as a bridging year to increase their credits to a level\nwhereby they are able to enter into the final year of studies on the Bachelor of Science\nHonours course (BSc Hons). The students undertaking the module Networks and\nInternet Architectures (NIA) were from a broad range of previous named HND\nawards varying from Network Administration, Computer Studies to Information\nSystems and Business.\nAt the start of the module students were presented with a series of questions on\ncomputer networking. From this, the area of N-tier architectures was highlighted as\none which they had not covered. This area is traditionally covered within the final\nyear module in Distributed Systems and Enterprise Networks (DSandEN). In the\nacademic year 2002\u20132003, an essay had been set to the final year group in this area\nof N-tier Architectures. From this, 39 essays were available, each on average had been\nmarked by their peers six times and a compensated peer-mark having been previously\nderived. Rada et al note (Rada et al., 2002) that \u2018Students (as well as teachers) may\nmanifest bias, and a student may unfairly evaluate another student\u2019s work\u2019. By having\na number of markings this bias is removed. Also the compensation processes\nemployed minimizes this aspect of the peer-generated grades.\nThe NIA students were provided with web references on the basics of N-tier Archi-\ntectures. They were also provided with the same assessment pro-forma that had been\ngiven to the DSandEN students. To ensure that they perform the peer-marking\nprocess as closely to the DSandEN students, they were able to use the CAP Menu\ndriven marking tool (Figure 1). The DSandEN students had also made use of the\nanonymous communications facilities of the CAP system (Davies, 2003), however as\nthese students had already graduated the previous year, this facility was not available\nto the NIA students.\nFigure 1. CAP marking via pull-down menu\n264 P. Davies\nMethodology\nThe essays to be marked were from the last year\u2019s final year. The compensated peer-\ngenerated marks for each of these essays had already been recorded. The marking of\nthese essays had been undertaken making use of the pull-down menu driven CAP\nsystem (Figure 1).\nLin et al. (2001) note that \u2018some students complain that holistic peer feedback was\noften too vague or useless\u2019. By using this menu-driven system for marking, greater\nspecificity can be generated within the marking process. Also students make use of\nthe menu system as a scaffold for their own commenting (Davies, 2003a). Assessment\nvia this tool provides both the ability to mark and comment upon an essay. In this way\ntwo quantified values can be generated and compared, i.e. the peer-mark and the\nfeedback-index. Analyses of previous student commenting indicates that while doing\nthe marking, students include free-text comments as well as using the menu driven\nmarking system, therefore the feedback-index used included these comments as well\nas those generated by the menu system. Use was made of the \u2018Mark-Up\u2019 tool (Figure\n2) in order to create feedback-indexes for each essay marking.\nFigure 2. Mark-up tool for quantifying commentsThe NIA students as part of their requirements within this assessment were asked\nto mark six essays each. The average feedback-index was created for each essay previ-\nously marked by the DSandEN students for comparative purposes (Figure 3).\nFigure 3. Creation of feedback index for DSandEN essaysA compensation process was included to take into account of both high and low\ncommenting\/marking, as there is not necessarily a direct correlation between the\nmark produced and the comments. On one occasion a mark of 90% was allocated by\nFigure 1. CAP marking via pull-down menu\nAssessing student ability via peer-marking of an essay 265\na student, who then went on to heavily criticize almost every aspect of the report\n(Davies, 2000).\nIn order to assess the validity of using these feedback indexes, a comparison\nbetween the average peer-marks and feedback indexes was performed for these\nFigure 2. Mark-up tool for quantifying comments\nFigure 3. Creation of feedback index for DSandEN essays\n266 P. Davies\nessays. If there is validity within these \u2018scores\u2019 for an essay, then they are used as a\nmeasure against the peer-marking and commenting produced by each of the post-\nHND students.\nBy producing average differences within the markings of a Post HND student and\nthose of the essays, a measure of a student\u2019s ability in evaluating the essays is\nproduced (Figure 4). These deviations in marking and commenting are used to\nproduce a grade for the Post HND student both representing their marking and\ncommenting ability.\nFigure 4. Quantification of differences in NIA student markingsTo produce an average difference for the marking, the peer mark produced by the\nDSandEN students has the mark given by the NIA student taken away from it + or \u2212\nX. The average of the differences can then be calculated, i.e. (X1) + (X2) \u2026 and then\nsubsequently be divided by the total number of markings giving AV. This figure\nrepresents the average differences from the perfect 0. However, in doing this no\naccount is taken of the consistency of a student\u2019s marking. Therefore, the absolute\ndifferences are calculated from this average and divided by the total markings, i.e.\n\u03a3(AV \u2013 [difference for each marking]) \/ number of markings. This numerical value\nnow represents the consistency of the marker. A similar process is performed in order\nto produce a numerical value to represent the consistency of the marker with regard\nto the differences of the feedback indexes (associated with comments).\nThese differences are then mapped via some form of linear structure in order to\ngenerate a valid grade for a marker. How the allocation of this absolute grade will be\nproduced from this consistency mark will be presented later in this paper.\nFigure 4. Quantification of differences in NIA student markings\nAssessing student ability via peer-marking of an essay 267\nResults and analysis\nTo use the DSandEN results as a \u2018marker\u2019 for the NIA students marking, there must\nbe a confidence that these initial results are valid. Of the original essays 10% had been\ncross-marked and there was a maximum of 5% variation in any of the compensated\npeer-marks awarded which was considered acceptable. The qualitative assessment of\nthe comments generated within the marking process was not previously generated.\nMaking use of the Markup tool (Figure 2), a feedback index was created for each\nessay (Figure 3) as marked by the DSandEN students. The correlation of these grades\nto the marks is shown in Table 1.\nHowever, some students tend to over- or under-mark. This is addressed by using\nthe compensated peer-mark. Table 2 shows the effect of using the compensated\nfeedback indexes for the DSandEN student markings. There is a significant positive\ncorrelation between the average feedback indexes and the average peer marks\nawarded for the essays (Tables 1 and 2). This improves by performing the compen-\nsation process (positive correlation of over 0.9).\nMore importantly the average standard deviations produced for each average\nreduces from 4.33 to 3.61. This shows that the range of marks within each category\nTable 1. Mapping of non-compensated DSandEN marks to comments\nF\/Index \u22125 \u22124 \u22123 \u22122 \u22121 0 0 1 2 3 4 5 6 7\n39 46 58 60 60 69 63 67 63 70 73 76 80\n54 45 59 58 68 67 68 69 68 73\n60 65 67 60 69 70\n59 60 62 76\n50 62 75\n62 68\n71\nAvg 39 50 0 51.5 57.6 60 64 63.7 64 66.7 71.3 70.5 74.5 80\nSt Dev 5.66 9.19 4.28 5.57 3.14 3.56 3.21 3.04 3.54 2.12\nTable 2. Mapping of compensated DSandEN marks to comments\nF\/In \u22125 \u22124 \u22123 \u22122 \u22121 0 0 1 2 3 4 5 6 7 8 9\n39 54 58 50 60 60 68 63 63 71 76 73 80\n46 58 59 62 69 67 67 69 68 73\n60 45 65 60 68 71 68 76\n59 60 67 62 70 69\n62 75\nAvg 42.5 0 54 58.7 53.3 60 61.8 65.2 65 67.8 70.4 72 74 0 0 80\nSt D 4.95 1.15 6.95 2.36 3.96 2.94 3.59 2.79 5.66 1.73\n268 P. Davies\nof the feedback indexes is reduced. Also noticeable (Table 3) is the fact that the top\nessay mark has moved along the linear scale significantly with regard to the feedback\nreceived by performing the compensation process. This positive correlation maps well\nwith the previous results (Davies, 2003a). Due to this correlation it is fair to assume\nthat the feedback indexes and the peer-marks produced for the DSandEN essays are\ntrue measures of the quality of the work produced, and can be used as controls for the\nassessment of the NIA students. It was decided to permit the NIA students to\nperform their marking and commenting of the essays making use of the CAP marking\nsystem (with the same menu driven commentings) as had been used by the previous\nyear\u2019s DSandEN students.\nThe average mark for the essays produced and marked by the DSandEN students\nwas 63.52%, with a standard deviation of 8.69. The average mark produced for the\nsame essays, as marked by the NIA students was 58.75%, with a standard deviation\nof 12.71 (only 5 out of 34 markers on average over-marked). A positive correlation of\n0.77 existed between the average compensated marks generated for the essays by the\nDSandEN students, with the average marks produced by the NIA students. Looking\nat the feedback indexes generated by the NIA students, on average their feedback was\n\u22121.37 (only 8 out of the 34 markers over-commented) compared with the feedback\nproduced by the DSandEN students.\nTable 4 shows the results of the NIA markings and comments. On examining\nthe use of the menu-driven comments and the free-text comments, a number of\nstudents tended to make use of both facilities integrated together. It was therefore\ndecided that the feedback index produced for each marking should also include\nthese free text responses by again making use of the Markup (Figure 2)\nApplication. Also included in Table 4 are the gradings (0\u20135) for this and other\nassessments within the study. Table 4 shows the average absolute differences (as\ndiscussed previously) produced by the students for both their marking and\ncommenting.\nTo allocate a final grade, three assessments were specified. Therefore the differ-\nences produced via the peer-marking process by the NIA students needed to be quan-\ntified. In trying to allocate marks in a linear manner, the following grading was\ndecided upon (Table 5). In previous years of this module the average mark produced\nfor the students has been between 55\u201360%. Therefore it was assumed reasonable, as\nthere were no indicators that this year\u2019s NIA students were any different to previous\nyears, that a similar average would be produced within this cohort. A range of 0-5\nmarks was decided upon, and linear scales were determined for each aspect of the\nassessment based on these previous year\u2019s expectations.\nTable 3. Comparison of averages of non-compensated and compensated average feedback \nindexes\nF\/Index \u22125 \u22124 \u22123 \u22122 \u22121 0 0 1 2 3 4 5 6 7 8 9\nAvg T1 39 50 51.5 57.6 60 64 63.7 64 66.7 71.3 70.5 74.5 80\nAvg T2 42.5 54 58.7 53.3 60 61.8 65.2 65 67.8 70.4 72 74 80\nAssessing student ability via peer-marking of an essay 269\nT\nab\nle\n 4\n.\nR\nes\nul\nts\n F\nor\n N\nIA\n s\ntu\nde\nnt\ns \nin\n a\nll \nA\nss\nes\nsm\nen\nts\n (\nm\nap\npe\nd \nto\n li\nne\nar\n s\nca\nle\n 0\n\u20135\n)\nS\ntu\nde\nnt\nA\nve\nra\nge\n \nco\nm\npe\nns\nat\ned\n \ndi\nff\ner\nen\nce\n in\n \nm\nar\nki\nng\nG\nra\nde\n \n0\u2013\n5 \nfo\nr \nm\nar\nki\nng\nA\nve\nra\nge\n \ndi\nff\ner\nen\nce\n \n(j\nus\nt u\nsi\nng\n \nm\nen\nu \ndr\niv\nen\n)\nG\nra\nde\n 0\n\u20135\n \nfo\nr \nm\nen\nu \nco\nm\nm\nen\nts\nA\nve\nra\nge\n \ndi\nff\ner\nen\nce\n \n(u\nsi\nng\n fr\nee\n \nte\nxt\n +\n \nm\nen\nu)\nG\nra\nde\n 0\n\u20135\n \nfo\nr F\nre\ne-\nte\nxt\n \nco\nm\nm\nen\nts\nD\nif\nfe\nre\nnc\ne \nin\n fe\ned\nba\nck\n \nm\nen\nu\/\nfr\nee\n \nte\nxt\nC\nom\nbi\nne\nd \ngr\nad\ne \nfo\nr \nm\nar\nki\nng\n a\nnd\n \nfr\nee\n-t\nex\nt \nco\nm\nm\nen\nti\nng\n \/ \n20\nC\nom\nbi\nne\nd \ngr\nad\ne \nfo\nr \nm\nar\nki\nng\n a\nnd\n \nfr\nee\n-t\nex\nt \nco\nm\nm\nen\nti\nng\n %\nM\nC\nQ\n \nra\nng\ne \n0\u2013\n5\nE\nss\nay\n \npr\nod\nuc\ned\n \n0\u2013\n5\nT\not\nal\n \nba\nse\nd \nup\non\n \/5\n \neq\nua\nl\nR\nes\nul\nts\n \nba\nse\nd \non\n \nM\nC\nQ\n a\nnd\n \nes\nsa\ny\nA\nve\nra\nge\n \nti\nm\ne \nta\nke\nn \nto\n m\nar\nk \nan\n \nes\nsa\ny \n(m\nin\nut\nes\n)\n1\n4.\n8\n4\n2.\n54\n4\n2.\n8\n4\n\u2212\n0.\n2\n16\n80\n3.\n4\n3.\n3\n70\n.9\n66\n.3\n35\n.5\n2\n18\n.2\n1\n9.\n5\n1\n10\n.3\n0\n\u2212\n0.\n8\n2\n10\n1.\n6\n3.\n5\n37\n.4\n51\n.1\n32\n.7\n3\n8.\n3\n3\n4.\n5\n3\n5.\n0\n3\n\u2212\n0.\n6\n12\n60\n3.\n6\n2.\n1\n58\n.4\n57\n.7\n54\n.5\n4\n6.\n3\n4\n4.\n3\n3\n5.\n0\n3\n\u2212\n0.\n8\n14\n70\n2.\n6\n2.\n6\n58\n.1\n52\n.2\n38\n.3\n5\n5.\n7\n4\n2.\n1\n4\n4.\n0\n4\n\u2212\n1.\n9\n16\n80\n3.\n4\n2.\n6\n66\n.9\n60\n.4\n33\n.2\n6\n4.\n3\n4\n2.\n6\n4\n3.\n6\n4\n\u2212\n1.\n0\n16\n80\n3.\n7\n3\n71\n.0\n66\n.5\n30\n.2\n7\n16\n.0\n1\n4.\n2\n3\n7.\n3\n2\n\u2212\n3.\n1\n6\n30\n2.\n6\n2.\n1\n41\n.2\n46\n.8\n55\n.8\n8\n3.\n3\n5\n1.\n7\n5\n2.\n7\n4\n\u2212\n1.\n0\n18\n90\n1.\n9\n2\n56\n.3\n39\n.5\n61\n.0\n9\n4.\n2\n4\n2.\n5\n4\n6.\n1\n2\n\u2212\n3.\n6\n12\n60\n3.\n4\n2.\n6\n60\n.4\n60\n.6\n49\n.6\n10\n11\n.5\n3\n5.\n1\n3\n5.\n8\n3\n\u2212\n0.\n7\n12\n60\n3.\n3\n2.\n8\n60\n.5\n60\n.8\n35\n.2\n11\n10\n.3\n3\n3.\n5\n4\n4.\n5\n3\n\u2212\n1.\n0\n12\n60\n3.\n3\n3.\n1\n62\n.8\n64\n.2\n36\n.3\n12\n11\n.9\n3\n3.\n2\n4\n2.\n8\n4\n0.\n4\n14\n70\n2.\n2\n3\n57\n.9\n51\n.9\n17\n.0\n13\n16\n.5\n1\n8.\n3\n1\n9.\n3\n1\n\u2212\n1.\n0\n4\n20\n2.\n6\n2.\n8\n42\n.3\n53\n.5\n41\n.8\n14\n8.\n9\n3\n5.\n7\n3\n6.\n2\n2\n\u2212\n0.\n6\n10\n50\n1.\n5\n2.\n1\n41\n.2\n36\n.7\n68\n.6\n15\n19\n.5\n1\n9.\n1\n1\n8.\n9\n1\n0.\n2\n4\n20\n3.\n6\n1.\n8\n42\n.4\n53\n.6\n47\n.3\n16\n9.\n7\n3\n4.\n7\n3\n3.\n9\n4\n0.\n8\n14\n70\n1.\n3\n2.\n3\n47\n.0\n35\n.4\n16\n.0\n17\n11\n.0\n3\n4.\n6\n3\n7.\n6\n2\n\u2212\n3.\n0\n10\n50\n2.\n6\n2.\n8\n52\n.0\n53\n.1\n65\n.2\n18\n12\n.2\n2\n2.\n4\n4\n5.\n5\n3\n\u2212\n3.\n2\n10\n50\n3.\n4\n3.\n1\n59\n.6\n64\n.4\n33\n.7\n19\n9.\n3\n3\n5.\n7\n3\n8.\n4\n1\n\u2212\n2.\n8\n8\n40\n2.\n1\n2.\n3\n42\n.2\n43\n.2\n40\n.3\n20\n15\n.2\n2\n2.\n2\n4\n2.\n8\n4\n\u2212\n0.\n7\n12\n60\n2.\n8\n2.\n6\n56\n.5\n54\n.7\n58\n.0\n21\n15\n.1\n2\n3.\n3\n4\n5.\n4\n3\n\u2212\n2.\n0\n10\n50\n3.\n4\n2.\n6\n57\n.2\n60\n.7\n40\n.2\n22\n9.\n6\n3\n4.\n1\n3\n5.\n9\n3\n\u2212\n1.\n7\n12\n60\n2.\n3\n2.\n9\n54\n.8\n52\n.1\n48\n.5\n23\n2.\n7\n5\n4.\n8\n3\n7.\n9\n2\n\u2212\n3.\n1\n14\n70\n3.\n4\n2.\n5\n62\n.5\n58\n.7\n34\n.5\n24\n7.\n6\n4\n4.\n7\n3\n3.\n7\n4\n1.\n0\n16\n80\n3.\n4\n1.\n9\n61\n.5\n52\n.3\n34\n.3\n25\n11\n.0\n3\n3.\n3\n4\n4.\n6\n3\n\u2212\n1.\n3\n12\n60\n3.\n0\n2.\n6\n57\n.5\n56\n.2\n33\n.4\n26\n10\n.3\n3\n3.\n9\n4\n4.\n7\n3\n\u2212\n0.\n8\n12\n60\n1.\n4\n0\n29\n.2\n13\n.8\n21\n.2\n270 P. Davies\nT\nab\nle\n 4\n.\nC\non\ntin\nue\nd\nS\ntu\nde\nnt\nA\nve\nra\nge\n \nco\nm\npe\nns\nat\ned\n \ndi\nff\ner\nen\nce\n in\n \nm\nar\nki\nng\nG\nra\nde\n \n0\u2013\n5 \nfo\nr \nm\nar\nki\nng\nA\nve\nra\nge\n \ndi\nff\ner\nen\nce\n \n(j\nus\nt u\nsi\nng\n \nm\nen\nu \ndr\niv\nen\n)\nG\nra\nde\n 0\n\u20135\n \nfo\nr \nm\nen\nu \nco\nm\nm\nen\nts\nA\nve\nra\nge\n \ndi\nff\ner\nen\nce\n \n(u\nsi\nng\n fr\nee\n \nte\nxt\n +\n \nm\nen\nu)\nG\nra\nde\n 0\n\u20135\n \nfo\nr \nF\nre\ne-\nte\nxt\n \nco\nm\nm\nen\nts\nD\nif\nfe\nre\nnc\ne \nin\n fe\ned\nba\nck\n \nm\nen\nu\/\nfr\nee\n \nte\nxt\nC\nom\nbi\nne\nd \ngr\nad\ne \nfo\nr \nm\nar\nki\nng\n a\nnd\n \nfr\nee\n-t\nex\nt \nco\nm\nm\nen\nti\nng\n \/ \n20\nC\nom\nbi\nne\nd \ngr\nad\ne \nfo\nr \nm\nar\nki\nng\n a\nnd\n \nfr\nee\n-t\nex\nt \nco\nm\nm\nen\nti\nng\n %\nM\nC\nQ\n \nra\nng\ne \n0\u2013\n5\nE\nss\nay\n \npr\nod\nuc\ned\n \n0\u2013\n5\nT\not\nal\n \nba\nse\nd \nup\non\n \/5\n \neq\nua\nl\nR\nes\nul\nts\n \nba\nse\nd \non\n \nM\nC\nQ\n a\nnd\n \nes\nsa\ny\nA\nve\nra\nge\n \nti\nm\ne \nta\nke\nn \nto\n m\nar\nk \nan\n \nes\nsa\ny \n(m\nin\nut\nes\n)\n27\n8.\n2\n3\n2.\n1\n4\n2.\n2\n4\n\u2212\n0.\n2\n14\n70\n2.\n6\n3.\n4\n63\n.3\n60\n.0\n35\n.7\n28\n10\n.3\n3\n3.\n0\n4\n3.\n5\n4\n\u2212\n0.\n6\n14\n70\n2.\n6\n2.\n9\n59\n.6\n54\n.4\n44\n.0\n29\n9.\n2\n3\n4.\n2\n3\n5.\n9\n3\n\u2212\n1.\n7\n12\n60\n4.\n0\n3.\n4\n69\n.4\n74\n.1\n43\n.0\n30\n17\n.6\n1\n4.\n3\n3\n6.\n3\n2\n\u2212\n2.\n0\n6\n30\n3.\n2\n3.\n1\n52\n.4\n63\n.6\n41\n.3\n31\n6.\n5\n4\n7.\n2\n2\n6.\n2\n2\n1.\n0\n12\n60\n1.\n9\n0\n32\n.7\n19\n.1\n49\n.5\n32\n4.\n7\n4\n2.\n2\n4\n2.\n0\n4\n0.\n2\n16\n80\n1.\n7\n2.\n1\n52\n.2\n38\n.4\n42\n.7\n33\n4.\n0\n4\n1.\n8\n5\n6.\n2\n2\n\u2212\n4.\n3\n12\n60\n3.\n7\n3.\n1\n65\n.4\n68\n.0\n28\n.0\n0\n34\n11\n.3\n3\n7.\n0\n2\n8.\n8\n1\n\u2212\n1.\n75\n8\n40\n3.\n6\n2.\n9\n56\n.5\n64\n.7\n81\n.6\nA\nvg\n9.\n9\n3\n4.\n2\n3.\n3\n5.\n5\n2.\n8\n\u2212\n1.\n2\n11\n.5\n57\n.6\n2.\n8\n2.\n5\n54\n.7\n53\n.2\n42\n.0\nS\nt \nD\nev\n4.\n6\n0.\n7\n3.\n2\n1.\n4\n4.\n3\n2.\n1\n1.\n0\n5.\n7\n28\n.3\n0.\n2\n0.\n3\n10\n.1\n1.\n1\n32\n.6\nAssessing student ability via peer-marking of an essay 271\nA comparison of average feedback differences produced the results shown in Table 6.\nThe overall mark for the peer-marking aspect of the coursework was 2.88\/5 with a\nstandard deviation of 1.41 (using menu + free text as the feedback score). As a\npercentage this evaluates to 57.6% (within expectations from previous years). The\nmark for the essay and presentation was slightly lower than was expected (50%).\nHowever, during the presentations it was noted that the NIA students\u2019 ability in\ndeveloping an essay of their own was in general quite poor. This skill may be one that\nis developed throughout the course of the Post HND course.\nIn the past it was considered that the more time taken to mark an essay results in a\nmore detailed and precise marking and commenting. The average time taken to mark\nthe essays was 42 minutes (Table 4). This varied considerably between students with\na standard deviation of 32.6 for the times taken. The times taken to mark an individ-\nual essay ranged from 10 to 104 minutes. The student with the highest average time\nfor marking was 81 minutes (Number 34) yet he only received 1 for his quality of\ncommenting, whereas the student with the lowest average time for marking 16\nminutes (Number 12), received 2.8 for commenting, with both receiving 3 for consis-\ntency of marking. From viewing the figures there was no significant correlation\nbetween the time taken and the grade awarded for the peer-marking processes.\nFor a true comparison to be made between the various assessment methods, then\nthey needed to be graded in a consistent manner (Table 5). A composite final grade\nwas produced on the basis of equal weightings of the three methods of assessment.\nCorrelation between essay\/MCQ combined grade and the final assessment grade\nincluding the peer-assessment was 0.80. This would suggest a good match of the\nTable 5. Mapping to common assessment scale 0\u20135\nMarks Marking difference Feedback difference Mapping for MCQ and essays\n5 <4 <2 90\u2013100%\n4 <8 <4 80\u201389%\n3 <12 <6 60\u201379%\n2 <16 <8 40\u201359%\n1 <20 <10 20\u201339%\n0 20 or more 10 or more 0\u201319%\nTable 6. Average results for assessments (0\u20135)\nMethod of assessment Average \/5 Std Dev Avg as %\nScore for peer marking 3.0 0.71\nScore for feedback just menu driven 3.29 1.41\nScore for feedback with menu + free text 2.76 2.12\nScore for MCQ 2.8 0.15 56%\nScore for essay + presentation 2.52 0.27 50%\n272 P. Davies\nstudents within the grade awarded for the peer-marking process. Table 7 shows the\nfrequency distributions of the number of students within each grading.\nIn ordering the students via their final grade awarded there is on average a consis-\ntency of performance against the three methods of assessment used (Table 8).\nIf based only on MCQ and essay (50\/50 split) and removing the peer-marking\nprocess, the final average would have been 53.20% (with a standard deviation of\n1.13). This would have had an effect of increasing the final overall average by 1.48%.\nTable 7. Frequency distributions of results\n0 1 2 3 4 5\nMarking differences 0 5 3 15 9 2\nFeedback on menu only 0 3 2 13 14 2\nFeedback on menu + free text 1 4 8 10 11 0\nFinal mark (based on menu) 0 3 3 20 7 1\nFinal mark (based on text + menu) 1 4 6 17 6 0\nMCQ tests 0 7 10 16 1 0\nProduce own essay and presentation 2 2 20 10 0 0\nFinal based on 50\/50 MCQ and essay only 2 4 16 12 0 0\nAssessing student ability via peer-marking of an essay 273\nBased upon the final results produced by all three assessment processes, it is interest-\ning to note which students performed best at the peer-marking process, i.e. displayed\nthe greatest improvement by demonstrating evaluative skills. Table 9 indicates that\nthe claim at the outset of this study that the higher order students will be rewarded\nappears to be vindicated.\nThe results are affected by the fact that one of the two students in the 30\u201339%\ngroup did not submit an essay. Without this student it would have been \u221213.69%.\nAlso the student in the 20\u201329% category did not submit an essay. The results indicate\non average a correlation between the results produces across the three methods of\nassessment and also the rewarding of higher order skills as demonstrated by the\n\u2018brighter\u2019 students.\nStudent feedback\nSeventeen responses were received from the 34 students (50%) who undertook this\nassessment process. Of the students who replied they all stated that it was the first\ntime that any of them had used any form of peer-assessment. The students were asked\nwhether they had prepared in a different way prior to the marking than if they were\ngoing to write an essay themselves. 14 out of the 17 stated they had prepared in\nexactly the same way as they would have normally. Those who felt that they had\nprepared in a different manner generally made the point that they had performed\nTable 9. Improvement based on performing peer-marking process\nBased on all three results Effect on student\u2019s final % by including peer-marking process\n70% + +4.53%\n60\u201369% +1.51%\n50\u201359% +1.82%\n40\u201349% \u22122.16%\n30\u201339% \u22120.03%\n20\u201329% +15.45\nTable 8. Frequency distribution of NIA student results (final grades)\nFinal overall % mark Number of students Average MCQ Average essay Average peer-marking\n70% + 2 3.52 3.13 4.0\n60\u201369% 9 3.39 2.82 3.33\n50\u201359% 14 2.78 2.67 3.00\n40\u201349% 6 2.28 2.21 1.92\n30\u201339% 2 1.75 1.75 1.75\n20\u201329% 1 1.38 0 3.00\nNote: student 20\u201329% did not submit an essay.\n274 P. Davies\nmuch more research. They felt that a much better understanding of the subject area\nwas required prior to the actual assessment process taking place. One student made\nnote of the fact that their research continued throughout the marking process and\nthey had often looked up areas that they didn\u2019t understand whilst progressing through\nthe marking process. This might result in a problem for this student being able to\nmaintain a standard throughout their marking process.\nThe students were asked how they felt about marking their peers. A number high-\nlighted \u2018an apprehension\u2019 in undertaking this method of assessment initially, but this\nlessened throughout the course of the marking process, and a number commented \u2018it\nreally became enjoyable\u2019. A number of the students expressed their concerns that they\n\u2018didn\u2019t know how many marks to give or take away\u2019. This feeling of uncertainty was\nexpressed by most of the respondents.\nSince these Post HND students are deemed to be at level two of a degree scheme,\nit was felt that it would be interesting to assess their thoughts on the quality of work\nproduced by a level three student. The common thread of comments related to\n\u2018amazed at the wide range of work I had to mark\u2019. The feedback reported back that\nthere were few \u2018average\u2019 essays but they tended to be \u2018either excellent or poor\u2019.\nThe students were questioned as to whether there were particular areas of the\nessays that they had found to be good or bad. The areas that were reported to be good\nwere the use of examples, good grammar throughout, and where students had\nattempted to provide significant personal conclusions. The main areas that were\nreported as being poor concerned the referencing of the material. The markers\ncommented on how difficult it had been in many essays \u2018to find out where they\u2019ve got\ntheir information from\u2019. On being questioned on whether they the marker had a good\nunderstanding of the subject area, 15 out of the 17 respondents answered in the posi-\ntive. Comments such as \u2018I\u2019ve a much better understanding than if I\u2019d written my own\u2019\nand \u2018I now have the confidence to explain this subject to my classmates\u2019 were very\npositive. The two students who did not feel they had a good understanding both said\nthat their knowledge was still \u2018very general\u2019.\nOn seeking improvements to the method used, the common comment from the\nstudents was that they required better initial guidance concerning such matters as the\nmarking scheme, how to \u2018judge a good from a bad piece of work\u2019 and what to look for\nwhen marking. With regard to the CAP system one student suggested \u2018the pull down\nmenu comments are inserted in alphabetical order\u2019. Two students suggested that a\ngood essay be provided, with the marks given and also the comments. If this were the\ncase then the subjectivity of the peer-marker may well be weakened.\nWith regard to not doing an essay of their own, no student felt they\u2019d been disad-\nvantaged in any way. A number felt that they\u2019d improved their knowledge signifi-\ncantly in the subject area because \u2018really had to learn the stuff, not just write about it\u2019.\nTwo students were pleased that \u2018the hassle of writing an essay has been removed\u2019.\nOverall the feedback was very positive towards this method of learning through\nassessing.\nA general comment made concerned the students not knowing \u2018how will I get my\nmarks\u2019. Even though it was explained up front that they would be judged on their\nAssessing student ability via peer-marking of an essay 275\nconsistency of feedback and marking, the visualisation of what was considered \u2018good\u2019\nand \u2018bad\u2019 was difficult for the students to judge.\nA fact that has been noted in the past as a possible negative against peer-assessment\nis the problem of students not knowing or following the university rules associated\nwith the identification of plagiarism. This is identified in the following \u2018quite a lot of\ncopy and pasting (plagiarism), but it was referenced\u2019?\nConclusions\nThis study aimed to assess whether students could be judged on their knowledge in\na particular subject area by marking rather than creating an essay. The results indicate\nthat this is possible and that student feedback is positive.\nFrom the analysis of the results it has been noticeable that not all students perform\nat the same level in the various assessment types undertaken (though the average\nshows good mapping). This indicates that it is important to assess via a variety of\nmethods to ensure students are assessed fairly.\nRemoving the ability to include free-text responses within the peer-assessment\nprocess removes the need for employing the Markup utility, and hence speeds up the\nprovision of awarding a \u2018mark for marking\u2019. However, from student feedback and the\ntutor\u2019s analysis of the feedback, there is an indication that the inclusion of free-text\nresponses as well as the menu-driven system encourages students to include emphasis\nand further subjectivity within their comments. It has also been noted that the inter-\npretation of a comment may differ from student to student. On the menu-driven\nsystem the comment \u2018Overall a fair report\u2019 might be deemed to be a negative\ncomment and this interpretation problem is currently being researched, students are\nnow able to develop an augmented database to include their own comments as well\nas the standard comments. Each comment is given a weighting by the student thus\nproducing a greater degree of subjective based feedback. The results from this\nresearch will be reported upon in the future.\nOne of the main advantages of this form of assessment is the elimination of plagia-\nrism. The students are not able to copy material off the web (as they do not produce\ntheir own essays) and can not copy off their peers in the marking process as the essays\nare randomly selected. An interesting question arises is what procedures should be\nundertaken if a student in peer-marking identifies plagiarism within a student\u2019s essay,\nwhen that student may have already been awarded their qualification. From the\nessays produced and presentations of Post HND students, the author noted that the\nstudents lacked the level of skills of expression and referencing that would be\nexpected of a level three student. These skills may well be collected via further expe-\nrience at a higher level of study and augmented through the duration of this Post\nHND year of study. If the NIA students had been required to have written an essay\nthemselves in this area of study they may well have received a lower grade. Would this\nhave been a fair reflection on their knowledge in the subject area?\nIn producing the mappings in Table 5, it is noted that the linear scale 0\u20135 is used\nto represent a student\u2019s achievement in a particular item of assessment. This is based\n276 P. Davies\nupon expectations based upon previous student cohorts. It is questionable whether\nthese mappings could be used across all modules, years, courses, etc. Also from the\nauthor\u2019s experience it has been noted that different ethnic backgrounds have varying\nexpectations, e.g. a student from one culture may consider 80% poor, whilst another\nmight considered this good.\nWhat this study has shown is the knowledge acquisition process need not be limited\nto the development of an essay, but can be enhanced via peer assessment. This study\nhighlights the need for a compensated grade. This is illustrated by the example of the\noriginal marking and commenting for one DSandEN student, who was awarded a\nmark for their own work of 71%, but who consistently under-marked by an average\nof 2% per essay. However, on looking at his commenting, noting the feedback index\nscales shown previously of \u22125 to +9, his average under-commenting was \u22125.16. This\nmeans that just looking at his comments would have a very negative response. If their\nessay was only to be judged on a numeric representation of the commenting then\npossibly a good essay would receive a very low mark. For these methods of peer-\nassessment to be utilized then it is clear that the need for the computerisation of the\nprocesses is essential for management purposes.\nTo summarize this study has produced positive results, both statistically and from\nstudent feedback. The results do support the fact that the \u2018brighter\u2019 students achieve\nmore by utilizing their higher order skills. The method of assessment reported is capa-\nble of augmenting the assessment process, however the author would not make any\nclaims concerning replacing the need for student essay production completely within\na course of study. A final question that arises from this study is at what stage of the\neducational process or at what age group could this method of assessment be deemed\nacceptable? This is an area that will provide future work before any assumptions may\nbe formulated.\nReferences\nBhalerao, A. & Ward, A. (2001) Towards electronically assisted peer assessment: a case study,\nALT-J, 9(1), 26\u201337.\nBloom, B. (1956) A taxonomy of educational objectives: handbook of cognitive domain (New York,\nMcKay).\nBostock, S. (2001) Peer assessment: principles, a case, and computer support, paper presented at\nthe LTSN-ICS workshop in Computer Assisted Assessment, Warwick University, 5\u20136 April. Avail-\nable online at: http:\/\/www.keele.ac.uk\/depts\/cd\/Stephen_Bostock\/docs\/ltsnicsapril 2001.pdf.\nBoud, D., Cohen, R. & Sampson, J. (1999) Peer learning and assessment, Assessment and\nEvaluation in Higher Education, 24(4), 413\u2013426.\nCollis, B., De Boer, W. & Slotman, K. (2001) Feedback for web-based assignments, Journal of\nComputer Assisted Learning, 17, 306\u2013313.\nDavies, P. (2000) Computerized peer assessment, Innovations in Education and Training Interna-\ntional, 37(4), 346\u2013355.\nDavies, P. (2002a) Using student reflective self-assessment for awarding degree classifications,\nInnovations in Education and Teaching International, 39(4), 307\u2013319.\nDavies, P. (2002b) There\u2019s no confidence in multiple choice testing, in: M. Danson & C. Eabry\n(Eds) Proceedings of the 6th Annual CAA Conference, Loughborough, July, 119\u2013130.\nAssessing student ability via peer-marking of an essay 277\nDavies, P. (2003) Closing the communications loop on the computerized peer-assessment of\nessays, ALT-J, 11(1), 41\u201354.\nDavies, P (2003a) Peer-assessment: no marks required just feedback?\u2014evaluating the quality of\ncomputerized peer-feedback compared with computerized peer-marking, in: J. Cook & D.\nMcConnell (Eds) Communities of Practice, Research Proceedings of the 10th Association for\nLearning Technology Conference (ALT-C 2003), University of Sheffield and Sheffield Hallam\nUniversity, 8\u201310 September.\nFalchikov, N. (1995) Improving feedback to and from students, in: P. Knight (Ed.) Assessment for\nlearning in higher education (London, Kogan Page).\nFalchikov, N. (1995a) Peer feedback marking: developing peer assessment, Innovations in\nEducation and Training International, 32(2), 175\u2013187.\nFalchikov, N. & Goldfinch, J. (2000) Student peer assessment in higher education: a meta-analysis\ncomparing peer and teacher marks, Review of Educational Research, 70(3), 287\u2013322.\nLewis, S. F. & Davies, P. (2004) The automated peer-assisted assessment of programming skills,\nin: P. Chalk (Ed.) Proceedings of the 8th JAVA and The Internet in the Computing Curriculum\nConference JICC8, January.\nLin, S. S. J., Liu, E. Z. F. & Yuan, S. M. (2001) Web-based peer assessment: feedback for\nstudents with various thinking-styles, Journal of Computer Assisted Learning, 17, 430\u2013432.\nParsons, R. (2003) Self, peer and tutor assessment of text online: design, delivery and analysis, in:\nJ. Christie (Ed.) Proceedings of 7th International CAA Conference, July, 315\u2013326.\nRace, P. (1995) The art of assessing, The New Academic, 4(3) and 5(1).\nRada, R. & Hu, K. (2002) Patterns in student\u2013student commenting, IEEE Transactions on\nEducation, 45(3), 262\u2013267.\nRobinson, J. (2002) In search of fairness: an application of multi-reviewer anonymous peer review\nin a large class, Journal of Further and Higher Education, 26(2), 183\u2013192.\nSitthiworachart, J. & Joy, M. (2003) Deepening computer programming skills by using Web-based\npeer assessment, 4th Annual LTSN-ICS Conference, Galway.\nStefani, L. A. J. (1994) Peer, self and tutor assessment: relative reliabilities, Studies in Higher\nEducation, 19(1), 69\u201375.\n"}