{"doi":"10.1007\/978-3-540-28651-6_112","coreId":"103171","oai":"oai:epubs.surrey.ac.uk:3029","identifiers":["oai:epubs.surrey.ac.uk:3029","10.1007\/978-3-540-28651-6_112"],"title":"In-situ learning in multi-net systems","authors":["Casey, M","Ahmad, K","Yang, ZR","Everson, R","Yin, H"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2004-01-01","abstract":null,"downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:epubs.surrey.ac.uk:3029<\/identifier><datestamp>\n      2017-10-31T14:07:35Z<\/datestamp><setSpec>\n      74797065733D636F6E666572656E63655F6974656D<\/setSpec><setSpec>\n      6469766973696F6E733D656E67616E64706879736963616C736369656E636573:436F6D707574696E67<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/epubs.surrey.ac.uk\/3029\/<\/dc:relation><dc:title>\n        In-situ learning in multi-net systems<\/dc:title><dc:creator>\n        Casey, M<\/dc:creator><dc:creator>\n        Ahmad, K<\/dc:creator><dc:creator>\n        Yang, ZR<\/dc:creator><dc:creator>\n        Everson, R<\/dc:creator><dc:creator>\n        Yin, H<\/dc:creator><dc:date>\n        2004-01-01<\/dc:date><dc:type>\n        Conference or Workshop Item<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        attached<\/dc:rights><dc:identifier>\n        http:\/\/epubs.surrey.ac.uk\/3029\/2\/2004_casey_ahmad_in-situ_learning_in_multi-net_systems.pdf<\/dc:identifier><dc:format>\n        text<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/epubs.surrey.ac.uk\/3029\/4\/licence.txt<\/dc:identifier><dc:identifier>\n          Casey, M, Ahmad, K, Yang, ZR, Everson, R and Yin, H  (2004) In-situ learning in multi-net systems  In: 5th International Conference on Intelligent Data Engineering and Automated Learning (IDEAL 2004), 2004-08-25 - 2004-08-27, Execter, ENGLAND.     <\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1007\/978-3-540-28651-6_112<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/epubs.surrey.ac.uk\/3029\/","http:\/\/dx.doi.org\/10.1007\/978-3-540-28651-6_112"],"year":2004,"topics":[],"subject":["Conference or Workshop Item","PeerReviewed"],"fullText":"In-situ Learning in Multi-net Systems \nMatthew Casey and Khurshid Ahmad \nDepartment of Computing, School of Electronics and Physical Sciences, \nUniversity of Surrey, Guildford, Surrey, GU2 7XH, UK \n{m.casey, k.ahmad}@surrey.ac.uk \nAbstract.  Multiple classifier systems based on neural networks can give im-\nproved generalisation performance as compared with single classifier systems.  \nWe examine collaboration in multi-net systems through in-situ learning, explor-\ning how generalisation can be improved through the simultaneous learning in \nnetworks and their combination.  We present two in-situ trained systems; first, \none based upon the simple ensemble, combining supervised networks in paral-\nlel, and second, a combination of unsupervised and supervised networks in se-\nquence.  Results for these are compared with existing approaches, demonstrat-\ning that in-situ trained systems perform better than similar pre-trained systems. \n1 Introduction \nThe task of classifying data has been tackled by a number of different techniques.  \nOne such approach is the use of mixture models, which uses a combination of models \nto summarise a data set comprising a number of modes.  Such mixture models are \n\u2018parsimonious in the sense that they typically combine distributions that are simple \nand relatively well-understood\u2019 [5] (p.267), of which the mixture-of-experts (ME) \nmodel is a good example.  Mixture models are based on the assumption that each con-\nstituent of the mixture can classify one segment of the input, and that the combination \nis able to classify most, if not all, of the input.  Such combinations appear intuitive, \nand have been used on a number of pattern recognition tasks, such as identity [6] and \nhandwriting recognition [16].  The disadvantage with mixture models is the increase \nin processing time caused by multiple components, however they have a degree of \nelegance in that they combine a number of \u2018simple\u2019 classifiers. \nThe constituent classifier neural networks of a multiple classifier combination are \nfurther distinguished as either ensemble or modular; the former refers to a set of re-\ndundant networks, whilst the later has no redundancy (of which ME is an example).  \nSuch multi-net systems (see papers in [12]) typically combine networks in parallel, but \nthe sequential combination of networks has also had some success [10].  Whether in \nparallel or in sequence, each constituent network of a multi-net system is combined \nusing prior knowledge of how the combination is affected, exemplified by the pre-\ntraining of networks before combination.  The question here is whether techniques \nsuch as ME, which can learn how to combine networks, offers any improvement over \nindividually trained systems?  In the context of multiple classifier systems, it is im-\nportant to look at this in-situ learning, defined as the simultaneous training of the con-\n2      Matthew Casey and Khurshid Ahmad \nstituent networks, which \u2018provides an opportunity for the individual networks to in-\nteract\u2019 [9] (p.222).  In this paper we evaluate the use of in-situ learning in the parallel \nand sequential combination of networks to help assess this as a general approach to \nlearning in multi-net systems. \n2 In-situ Learning in Multi-net Systems \nIn this paper we consider two multi-net systems that exploit in-situ learning [3].  The \nfirst is a simple ensemble (SE) trained in conjunction with early stopping techniques: \nthe simple learning ensemble (SLE).  The second is a novel system consisting of a \ngroup of unsupervised networks and a single supervised network that are trained in \nsequence: sequential learning modules (SLM). \nSimple Learning Ensemble: There have been two contrasting examples of in-situ \nlearning in ensembles.  Liu and Yao [8] defined the negative correlation learning al-\ngorithm for ensembles that trains networks in-situ using a modified learning rule with \na penalty term, whereas Wanas, Hodge and Kamel\u2019s [14] multi-net system combines \npartially pre-trained networks before continuing training in-situ.  Whilst we agree \nwith Liu and Yao that in-situ learning is important, our work differs from theirs and \nWanas et al\u2019s in two respects: first we use the same data set to train all of the net-\nworks, rather than using data sampling, and second we use early stopping to promote \ngeneralisation through assessing the combined performance of the ensemble, instead \nof introducing a penalty term to the error function, exploiting the interaction between \nnetworks [9].  Our approach is based upon the SE, but with each network trained in-\nsitu.  We use the generalisation loss [11] early stopping metric to control the amount \nof training based upon the measured generalisation performance. \nSequential Learning Modules: Sequential in-situ learning is a difficult area to de-\nvelop for supervised classification because it depends upon having an appropriate er-\nror to propagate back through each network in sequence.  This issue is apparent in the \ndevelopment of multi-layer, single network systems, where an algorithm such as \nbackpropagation is required to assign error to hidden neurons.  Bottou and Gallinari \n[2] discussed how error can be assigned to sequential networks in multi-net systems, \nbut assumed that each such network used supervised learning.  Our approach is to use \nunsupervised networks in sequence coupled with in-situ learning so that no such error \nis required, only an appropriate input to each network.  We employ networks that use \nunsupervised learning in all but the last network to give an overall supervised system, \nbut which does not propagate back error.  This approach also allows unsupervised \ntechniques to be used to give a definite classification through the assignment of a \nclass by the last network. \n3 Evaluating In-situ Learning with Classification \nThe classification of an arbitrary set of objects is regarded as an important exemplar \nof learnt behaviour.  We use well-known data sets [1], which have been used exten-\nIn-situ Learning in Multi-net Systems      3 \nsively in benchmarking the performance of classification systems, observing the be-\nhaviour of the proposed systems.  We use the artificial MONK\u2019s problems [13] to test \ngeneralisation capability, whilst the Wisconsin Breast Cancer Database (WBCD) [15] \nis used to test pattern separation capability using real-life data (Table 1). \nTable 1.  Details of data sets used for experiments.  For the MONK\u2019s problems, the validation \ndata set includes the training data, which is also used for testing. \nData Set Input Output Training Validation Testing Examples\/Class % Notes \nMONK 1 6 1 124 432 - 50:50  \nMONK 2 6 1 169 432 - 67:33  \nMONK 3 6 1 122 432 - 47:53 5% misclassified \nWBCD 9 2 349 175 175 66:34 16 missing values \nSLE systems consisting of from 2 to 20 multi-layer perceptrons (MLPs) trained us-\ning backpropagation were constructed to determine the effect of ensemble complexity \non generalisation performance.  Each network within the ensemble had the same net-\nwork topology, but to generate diversity in the networks, each was initialised with dif-\nferent random real number weights selected using a normal probability distribution \nwith mean 0, standard deviation 1.  The backpropagation with momentum algorithm \nwas used with the Logistic Sigmoid activation function, using a constant learning rate \nof 0.1 and momentum of 0.9. \nFor the SLM systems, we restrict ourselves to combining a self-organising map \n(SOM) [7] and a single layer network employing the delta learning rule.  Neither of \nthese is capable of solving a non-linearly separable classification problem; our hy-\npothesis is that an in-situ trained combination of these can solve these more complex \nproblems.  The basic SOM algorithm was used on a rectangular map of neurons, with \na Gaussian neighbourhood and exponential learning rate.  To ensure that the output of \nthe SOM can be combined with the single layer network, the output is converted into \na vector by concatenating the winning values from each of the neurons, with \u20181\u2019 asso-\nciated with the winning neuron and \u20180\u2019 for all other neurons.  The single layer net-\nwork using the delta learning rule had a constant learning rate of 0.1, and a binary \nthreshold activation function. \nTable 2.  The number of input, hidden and output nodes per data set for each of the constituent \nnetworks used for the single network and ensemble systems (hidden nodes selected as in [13]). \nSystem MONK 1 MONK 2 MONK 3 WBCD \nMLP \nMLP (ES) \nSE (ES) \nSLE (ES) \n6-3-1 6-2-1 6-4-1 9-5-2 \nIn order to understand the generalisation performance of the SLE and SLM sys-\ntems, we compare the percentage test responses against those generated for single \nMLPs trained with and without early stopping, as well as simple ensembles formed \nfrom 2 to 20 MLPs pre-trained with early stopping.  The architecture used for the \nvarious systems is shown in Table 2 and Table 3.  Each of the systems underwent 100 \n4      Matthew Casey and Khurshid Ahmad \ntrials to estimate the mean performance, training either for a fixed 1000 epochs, or \nwith early stopping (ES) for a maximum of 1000 epochs. \nTable 3.  The different architectures used for the SLM system, shown as the topology of the \nSOM and the single layer network.  For the SOM this is the number of inputs and nodes in the \nmap.  For the single layer network this is the number of input and output nodes.  \nSystem MONK 1 MONK 2 MONK 3 WBCD \n6-5x5: 25-1 6-5x5: 25-1 6-5x5: 25-1 9-5x5: 25-2 \n6-10x10: 100-1 6-10x10: 100-1 6-10x10: 100-1 9-10x10: 100-2 SLM \n6-20x20: 400-1 6-20x20: 400-1 6-20x20: 400-1 9-20x20 400-2 \n3.1 Experimental Results \nFor each of the benchmark data sets, Table 4 shows the percentage mean number of \ncorrect test responses for the MLP, SE, SLE and SLM systems.  Only the configura-\ntion of each system giving the highest mean test percentage is shown. \nTable 4.  Results for systems with the highest mean test response, with the number of networks \n\/ SOM configuration and mean test response, with standard deviation. \nMONK 1 MONK 2 MONK 3 WBCD System \nNets Test % Nets Test % Nets Test % Nets Test % \nMLP 1 84.44 \u00b1 12.15 1 66.29 \u00b1 35.21 1 83.39 \u00b1 47.57 1 95.90 \u00b1 3.93\nMLP (ES) 1 57.13 \u00b1 8.74 1 65.21 \u00b1 2.68 1 63.10 \u00b1 6.83 1 82.34 \u00b1 9.61\nSE (ES) 3 55.75 \u00b1 7.70 18 66.25 \u00b1 0.81 18 66.03 \u00b1 23.10 20 91.94 \u00b1 1.69\nSLE (ES) 20 90.21 \u00b1 6.16 20 69.49 \u00b1 1.24 19 78.57 \u00b1 4.69 20 92.95 \u00b1 1.06\nSLM 10x10 75.63 \u00b1 4.78 20x20 75.09 \u00b1 26.06 10x10 84.10 \u00b1 1.76 20x20 97.63 \u00b1 0.83\nFirst we note that for the MONK 1 and 2, the SLE system gives a comparatively \nbetter generalisation performance when a relatively large number of networks are \ncombined, with the performance of the SE decreasing with successively more net-\nworks.  Here a more complex in-situ trained system gives better generalisation, in \ncontrast to the far less complex pre-trained system.  For MONK 3 and WBCD, the \nSLE improves upon the early stopping MLP and SE systems, but not the fixed MLP \ntrained for 1000 epochs.  The improvement in generalisation performance can be at-\ntributed to the increased training times experienced by the SLE algorithm with in-\ncreasing numbers of networks as compared with the MLP with early stopping sys-\ntems.  For example, for MONK 1 with 2 networks, the maximum number of epochs is \n27 (excluding outliers), which increases to 521 epochs for 20 networks.  However, all \nthese are less than the fixed 1000 epochs for the MLP systems, yet give a similar level \nof performance. \nFor the SLM system, we note that the sequential combination of networks success-\nfully learns to solve each non-linearly separable task.  This is perhaps surprising given \nthat neither is individually capable, and despite the somewhat complex nature of the \nSLM systems with relatively high numbers of neurons.  For MONK 2, 3 and WBCD, \nthe SLM system out-performs the other single network and multi-net systems.  For \nIn-situ Learning in Multi-net Systems      5 \nMONK 1 the results are better than both the SE and MLP with early stopping, but do \nnot improve upon the SLE or fixed MLP. \nThe results for the SLM also show how the number of neurons within the SOM af-\nfects the overall performance of the system, perhaps in a similar way to the number of \nhidden neurons in an MLP.  Here, increasing the map size tends to give both im-\nproved training and generalisation performance, reaching a peak commensurate with \nover-fitting.  For MONK 1 the response for the 10x10 map is better than for the \n20x20 map, despite giving a 100% training response, as compared with the 10x10 re-\nsponse of 89.65%.  Furthermore, increasing the map size also produces more reliable \nsolutions in that the standard deviation decreases, whilst still maintaining a similar \nlevel of generalisation performance. \n3.2 Discussion \nThese preliminary results are encouraging, and demonstrate that in-situ learning in \nparallel and sequential combinations of networks can give improved generalisation \nperformance, as demonstrated by the results for SLE, and especially the SLM sys-\ntems.  Putting these into context with other reported results shows that they compare \nwell, but it is recognised that some further investigation is required. \nFor the MONK\u2019s problems, optimal generalisation results have been reported with \n100%, 100% and 97.2% for MONK 1, 2 and 3 respectively [13].  For the SLE sys-\ntems the maximum values are 98.4%, 74.5% and 83.1%, and for the SLM systems \n84.7%, 81.0% and 87.5%, showing that, whilst there is a small spread of values, fur-\nther tuning is required to improve the maximum.  Here, of interest is the way in which \nthe results demonstrate the use of unsupervised learning in a modular system, giving a \nsignificant improvement in generalisation as compared with existing supervised tech-\nniques (MONK 2 and 3).  For the WBCD data set, the SLM system with a mean of \n97.63% again out-performs the SE, and is comparable to other multi-net systems such \nas AdaBoost with 97.6% [4].  Further work is required to assess the properties of \nthese techniques with other data sets, and especially how the combination of unsuper-\nvised and supervised learning can be further exploited for classification tasks. \n4 Conclusion \nIn this paper we have explored whether the use of simultaneous, in-situ learning in \nmulti-net systems can provide improved generalisation in classification tasks.  In par-\nticular, we have presented results for in-situ learning in an ensemble of redundant \nnetworks, and the in-situ learning in a sequential system, the latter of which builds \nupon the principle that \u2018simple\u2019 networks combined in a modular system are parsimo-\nnious, through the combination of supervised and unsupervised techniques. \nAcknowledgements.  The authors would like to thank Antony Browne and the two \nanonymous reviewers for their helpful comments. \n6      Matthew Casey and Khurshid Ahmad \nReferences \n1.  Blake,C.L. & Merz,C.J. UCI Repository of Machine Learning Databases.  \nhttp:\/\/www.ics.uci.edu\/~mlearn\/MLRepository.html.  Irvine, CA.: University of California, \nIrvine, Department of Information and Computer Sciences, 1998. \n2.  Bottou, L. & Gallinari, P.  A Framework for the Cooperation of Learning Algorithms.  In \nLippmann, R.P., Moody, J.E. & Touretzky, D.S. (Ed), Advances in Neural Information \nProcessing Systems, vol. 3, pp. 781-788, 1991. \n3.  Casey, M.C.  Integrated Learning in Multi-net Systems.  Unpublished doctoral thesis.  \nGuildford, UK: University of Surrey, 2004. \n4.  Drucker, H.  Boosting Using Neural Networks.  In Sharkey, A. J. C. (Ed), Combining Artifi-\ncial Neural Nets: Ensemble and Modular Multi-Net Systems, pp. 51-78.  London: Springer-\nVerlag, 1999. \n5.  Jacobs, R.A. & Tanner, M.  Mixtures of X.  In Sharkey, A. J. C. (Ed), Combining Artificial \nNeural Nets: Ensemble and Modular Multi-Net Systems, pp. 267-295.  Berlin, Heidelberg, \nNew York: Springer-Verlag, 1999. \n6.  Kittler, J., Hatef, M., Duin, R.P.W. & Matas, J.  On Combining Classifiers.  IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence, vol. 20(3), pp. 226-239, 1998. \n7.  Kohonen, T.  Self-Organized Formation of Topologically Correct Feature Maps.  Biological \nCybernetics, vol. 43, pp. 59-69, 1982. \n8.  Liu, Y. & Yao, X.  Ensemble Learning via Negative Correlation.  Neural Networks, vol. \n12(10), pp. 1399-1404, 1999. \n9.  Liu, Y., Yao, X., Zhao, Q. & Higuchi, T.  An Experimental Comparison of Neural Network \nEnsemble Learning Methods on Decision Boundaries.  Proceedings of the 2002 Interna-\ntional Joint Conference on Neural Networks (IJCNN'02), vol. 1, pp. 221-226.  Los Alami-\ntos, CA: IEEE Computer Society Press, 2002. \n10. Partridge, D. & Griffith, N.  Multiple Classifier Systems: Software Engineered, Automati-\ncally Modular Leading to a Taxonomic Overview.  Pattern Analysis and Applications, vol. \n5(2), pp. 180-188, 2002. \n11. Prechelt, L.  Early Stopping - But When?  In Orr, G. B. & M\u00fcller, K-R. (Ed), Neural Net-\nworks: Tricks of the Trade, 1524, pp. 55-69.  Berlin, Heidelberg, New York: Springer-\nVerlag, 1996. \n12. Sharkey, A.J.C.  Multi-Net Systems.  In Sharkey, A. J. C. (Ed), Combining Artificial Neural \nNets: Ensemble and Modular Multi-Net Systems, pp. 1-30.  London: Springer-Verlag, 1999. \n13. Thrun, S.B., Bala, J., Bloedorn, E., Bratko, I., Cestnik, B., Cheng, J., De Jong, K., Dzeroski, \nS., Fahlman, S.E., Fisher, D., Hamann, R., Kaufman, K., Keller, S., Kononenko, I., \nKreuziger, J., Michalski, R.S., Mitchell, T., Pachowicz, P., Reich, Y., Vafaie, H., van de \nWelde, W., Wenzel, W., Wnek, J. & Zhang, J.  The MONK's Problems: A Performance \nComparison of Different Learning Algorithms.  Technical Report CMU-CS-91-197.  Pitts-\nburgh, PA.: Carnegie-Mellon University, Computer Science Department, 1991. \n14. Wanas, N.M., Hodge, L. & Kamel, M.S.  Adaptive Training Algorithm for an Ensemble of \nNetworks.  Proceedings of the 2001 International Joint Conference on Neural Networks \n(IJCNN'01), vol. 4, pp. 2590-2595.  Los Alamitos, CA.: IEEE Computer Society Press, \n2001. \n15. Wolberg, W.H. & Mangasarian, O.L.  Multisurface Method of Pattern Separation for Medi-\ncal Diagnosis Applied to Breast Cytology.  Proceedings of the National Academy of Sci-\nences, USA, vol. 87(23), pp. 9193-9196, 1990. \n16. Xu, L., Krzyzak, A. & Suen, C.Y.  Several Methods for Combining Multiple Classifiers and \nTheir Applications in Handwritten Character Recognition.  IEEE Transactions on Systems, \nMan, and Cybernetics, vol. 22(3), pp. 418-435, 1992. \n"}