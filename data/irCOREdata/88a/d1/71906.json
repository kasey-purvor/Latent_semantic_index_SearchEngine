{"doi":"10.1007\/s10676-005-4583-2","coreId":"71906","oai":"oai:eprints.lancs.ac.uk:114","identifiers":["oai:eprints.lancs.ac.uk:114","10.1007\/s10676-005-4583-2"],"title":"Disclosive ethics and information technology: disclosing facial recognition systems","authors":["Introna, Lucas"],"enrichments":{"references":[{"id":16453804,"title":"The De-scription of Technical Objects.","authors":[],"date":"1992","doi":null,"raw":"M. Akrich. The De-scription of Technical Objects. In W.E. Bijker and J. Law, editors, Shaping Technology\/Building Society, pp. 205\u2013224. MIT Press, Cambridege, 1992.","cites":null},{"id":16453808,"title":"Your Face is Not a Bar Code: Arguments Against Face Recognition","authors":[],"date":"2003","doi":null,"raw":"P.E. Agre. Your Face is Not a Bar Code: Arguments Against Face Recognition in Public Places, [Online], 2003. Available: http:\/\/dlis.gseis.ucla.edu\/pagre, [2003, May 25].","cites":null},{"id":16453814,"title":"Social Choice about Privacy: Intelligent Vehicle-Highway Systems in the United States. In","authors":[],"date":"1997","doi":"10.1108\/09593849410076825","raw":"P. Agre and C. Mailloux. Social Choice about Privacy: Intelligent Vehicle-Highway Systems in the United States. In B. Friedman, editor, Human Values and Design of Computer Technology, Cambridge University Press, Cambridge, 1997.","cites":null},{"id":16453820,"title":"Disclosive Computer Ethics.","authors":[],"date":"2000","doi":"10.1145\/572260.572264","raw":"Philip. Brey. Disclosive Computer Ethics. Computers and Society, 30(4): 10\u201316, 2000.","cites":null},{"id":16453824,"title":"Some Elements of a Sociology of Translation: Domestication of the Scallops and the Fishermen of St Brieuc Bay. In","authors":[],"date":"1986","doi":null,"raw":"Michel. Callon. Some Elements of a Sociology of Translation: Domestication of the Scallops and the Fishermen of St Brieuc Bay. In John. Law, editor, Power, Action and Belief, pp. 196\u2013233. Routledge & Kegan Paul, London, 1986.","cites":null},{"id":16453830,"title":"Against Ethics.","authors":[],"date":"1993","doi":"10.1177\/019145379702300405","raw":"John D. Caputo, Against Ethics. Indiana University Press, Indianapolis, 1993.","cites":null},{"id":16453835,"title":"The Ethics of Deconstruction: Derrida and Levinas. 2nd edn.","authors":[],"date":"1999","doi":null,"raw":"Simon. Critchley, The Ethics of Deconstruction: Derrida and Levinas. 2nd edn. Edinburgh University Press, Edinburgh, 1999.","cites":null},{"id":16453841,"title":"Questioning Technology. London and","authors":[],"date":"1999","doi":null,"raw":"Andrew. Feenberg, Questioning Technology. London and New York, Routledge, 1999. N. Furl, J.P. Phillips and A.J. O\u2019Toole. Face Recognition Algorithms and the Other-race E\ufb00ect: Computational Mechanisms for a Developmental Contact Hypothesis.","cites":null},{"id":16453846,"title":"Facial Recognition Vendor Test","authors":[],"date":"2002","doi":"10.1007\/springerreference_184088","raw":"Facial Recognition Vendor Test 2002 (FRTV2002), [Online], Available: http:\/\/www.frvt.org\/FRVT2000\/ default.htm, [2003, Aug.1]. M. Foucault, Discipline and Punish, The Birth of the Prison.","cites":null},{"id":16453851,"title":"Don\u2019t Count on Face-recognition Technology to Catch Terrorists.","authors":[],"date":"2002","doi":null,"raw":"S. Garpinkle. Don\u2019t Count on Face-recognition Technology to Catch Terrorists. Discover, 23(9): 17\u201320, 2002.","cites":null},{"id":16453855,"title":"A statistical Assessment of Subject Factors in the PCA Recognition of Human Faces, [Online],","authors":[],"date":"2003","doi":"10.1109\/cvprw.2003.10088","raw":"G. Givens, J.R. Beveridge, B.A. Draper, and D. Bolme. A statistical Assessment of Subject Factors in the PCA Recognition of Human Faces, [Online], 2003. Available: http:\/\/www.cs.colostate.edu\/evalfacerec\/papers\/csusacv03.","cites":null},{"id":16453861,"title":"Digitizing Surveillance: Categorization, Space and Inequality.","authors":[],"date":"2003","doi":"10.1177\/0261018303023002006","raw":"DIS ISCLOSIVE CLOSIVE ETHICS AND THICS AND INFORMATION NFORMATION TECHNOLOGY ECHNOLOGY 85S. Graham and D. Wood. Digitizing Surveillance: Categorization, Space and Inequality. Critical Social Policy, 20(2): 227\u2013248, 2003.","cites":null},{"id":16453864,"title":"Quo vadis Face Recognition?, [Online],","authors":[],"date":"2001","doi":null,"raw":"R. Gross, J. Shi, and J.F. Cohn. Quo vadis Face Recognition?, [Online], 2001a. Available: http:\/\/dagwood.vs am.ri.cmu.edu\/ralph\/Publications\/QuoVadisFR.pdf, [2003, July 10].","cites":null},{"id":16453869,"title":"Oppression, Resistance and Information Technology: Some Thoughts on Design and Values, Design for Values: Ethical, Social and Political","authors":[],"date":"1998","doi":null,"raw":"L.D. Introna. Oppression, Resistance and Information Technology: Some Thoughts on Design and Values, Design for Values: Ethical, Social and Political Dimensions of Information Technology workshop sponsored by the NSF DIMACS held at Princeton University, USA, February 27-March 1, 1998.","cites":null},{"id":16453876,"title":"The Internet as a Democratic Medium: Why the Politics of Search Engines Matters.","authors":[],"date":"2000","doi":"10.1080\/01972240050133634","raw":"L.D. Introna and H. Nissenbaum. The Internet as a Democratic Medium: Why the Politics of Search Engines Matters. Information Society, 16(3): 169\u2013185, 2000.","cites":null},{"id":16453880,"title":"The trial.","authors":[],"date":"1925","doi":"10.1515\/9783110205992.211","raw":"F. Kafka, The trial. Penguin Books Ltd., London, England, 1925. R. Kemp, N. Towell and G. Pike. When Seeing Should not be Believing: Photographs, Credit Cards and Fraud.","cites":null},{"id":16453885,"title":"Face the FactsFacial recognition technology\u2019s troubled past \u2013 and troubling future.","authors":[],"date":"1997","doi":null,"raw":"Applied Cognitive Psychology, 11: 211\u2013222, 1997. D. Kopel and M. Krause. Face the FactsFacial recognition technology\u2019s troubled past \u2013 and troubling future.","cites":null},{"id":16453890,"title":"Available: http:\/\/www.reason.com\/0210\/ fe.dk.face.shtml,","authors":[],"date":"2003","doi":null,"raw":"[Online] 2003. Available: http:\/\/www.reason.com\/0210\/ fe.dk.face.shtml, [2003, Nov. 18]. B. Latour. Technology is Society Made Durable. In J.","cites":null},{"id":16453895,"title":"Where are the Missing Masses? The Sociology of a Few Mundane Artefacts.","authors":[],"date":"1992","doi":null,"raw":"B. Latour. Where are the Missing Masses? The Sociology of a Few Mundane Artefacts. In W.L.J. Bijker, editors, Shaping Technology\/Building Society, pp. 225\u2013258. MIT Press, London, 1992.","cites":null},{"id":16453900,"title":"The Sociology of Monsters:","authors":[],"date":"1991","doi":"10.4324\/9780203862834","raw":"John. Law, The Sociology of Monsters: Essays on Power, Technology and Domination. Routledge, London, 1991. D. Lyon, Surveillance Society, Monitoring Everyday Life.","cites":null},{"id":16453904,"title":"Surveillance After","authors":[],"date":"2001","doi":"10.5153\/sro.643","raw":"D. Lyon. Surveillance After September 11, 2001, [Online], 2002. Available: http:\/\/www.\ufb01ne.lett.hiroshima-u.ac.jp\/ lyon\/lyon2.html, [2003, July.10].","cites":null},{"id":16453908,"title":"The Engineering of Social Control: The search for the silver Bullet.","authors":[],"date":"1995","doi":null,"raw":"G.T. Marx. The Engineering of Social Control: The search for the silver Bullet. In J. Hagen and R. Peterson, editors, Grime and Inequality, pp. 225\u201346. Stanford University Press, Stanford, CA, 1995.","cites":null},{"id":16453912,"title":"Robo cop: Some of Britain\u2019s 2.5 million CCTV cameras are being hooked up to a facial recognition system designed to identify known criminals. But does it work,","authors":[],"date":"2002","doi":null,"raw":"J. Meek. Robo cop: Some of Britain\u2019s 2.5 million CCTV cameras are being hooked up to a facial recognition system designed to identify known criminals. But does it work, Guardian, June 13, 2002.","cites":null},{"id":16453916,"title":"The Maximum Surveillance Society, The Rise of CCTV.","authors":[],"date":"1999","doi":null,"raw":"C. Norris and G. Armstrong, The Maximum Surveillance Society, The Rise of CCTV. Berg, New York, USA, 1999.","cites":null},{"id":16453921,"title":"Facial Recognition System Considered For US Airports,","authors":[],"date":"2001","doi":null,"raw":"R. O\u2019Harrow Jr. Facial Recognition System Considered For US Airports, Washington Post, Monday, September 24, 2001, Page A14.","cites":null},{"id":16453926,"title":"Face Recognition Vendor Test","authors":[],"date":"2002","doi":"10.1109\/amfg.2003.1240822","raw":"P. Phillips, P. Grother, R. Micheals, D.M. Blackburn, E. Tabassi, and J.M. Bone. Face Recognition Vendor Test 2002: Overview and Summary, [Online], 2003. Available: http:\/\/www.biometricsinstitute.org\/bi\/ FaceRecognitionVendorTest2002.pdf, [2003, May 30].","cites":null},{"id":16453931,"title":"Drawing a Blank: The Failure of Facial Recognition Technology in","authors":[],"date":"2002","doi":null,"raw":"J. Stanley and B. Steinhardt. Drawing a Blank: The Failure of Facial Recognition Technology in Tampa, Florida, ACLU Special Report, 2002.","cites":null},{"id":16453934,"title":"Do Artefacts Have Politics.","authors":[],"date":"1980","doi":null,"raw":"Landon. Winner. Do Artefacts Have Politics. Daedalus, 109: 121\u2013136, 1980. J. Woodward, C. Horn, J. Gatune and A. Thomas.","cites":null},{"id":16453938,"title":"Biometrics: A Look at Facal Recognition, Documented Brie\ufb01ng prepared for the Virginia State Crime Commission,","authors":[],"date":"2003","doi":null,"raw":"Biometrics: A Look at Facal Recognition, Documented Brie\ufb01ng prepared for the Virginia State Crime Commission, 2003 (available at http:\/\/www.rand.org). LUCAS UCAS D. INTRONA NTRONA 86","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2005","abstract":"This paper is an attempt to present disclosive ethics as a framework for computer and information ethics \u00e2\ufffd\ufffd in line with the suggestions by Brey, but also in quite a different manner. The potential of such an approach is demonstrated through a disclosive analysis of facial recognition systems. The paper argues that the politics of information technology is a particularly powerful politics since information technology is an opaque technology \u00e2\ufffd\ufffd i.e. relatively closed to scrutiny. It presents the design of technology as a process of closure in which design and use decisions become black-boxed and progressively enclosed in increasingly complex sociotechnical networks. It further argues for a disclosive ethics that aims to disclose the nondisclosure of politics by claiming a place for ethics in every actual operation of power \u00e2\ufffd\ufffd as manifested in actual design and use decisions and practices. It also proposes that disclosive ethics would aim to trace and disclose the intentional and emerging enclosure of politics from the very minute technical detail through to social practices and complex social-technical networks. The paper then proceeds to do a disclosive analysis of facial recognition systems. This analysis discloses that seemingly trivial biases in recognition rates of FRSs can emerge as very significant political acts when these systems become used in practice","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/71906.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/114\/1\/Disclosive_Ethics_FRS.pdf","pdfHashValue":"73dbf524e735263c139dbd73f992fba02fdfd493","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:114<\/identifier><datestamp>\n      2018-01-24T00:04:05Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Disclosive ethics and information technology: disclosing facial recognition systems<\/dc:title><dc:creator>\n        Introna, Lucas<\/dc:creator><dc:description>\n        This paper is an attempt to present disclosive ethics as a framework for computer and information ethics \u00e2\ufffd\ufffd in line with the suggestions by Brey, but also in quite a different manner. The potential of such an approach is demonstrated through a disclosive analysis of facial recognition systems. The paper argues that the politics of information technology is a particularly powerful politics since information technology is an opaque technology \u00e2\ufffd\ufffd i.e. relatively closed to scrutiny. It presents the design of technology as a process of closure in which design and use decisions become black-boxed and progressively enclosed in increasingly complex sociotechnical networks. It further argues for a disclosive ethics that aims to disclose the nondisclosure of politics by claiming a place for ethics in every actual operation of power \u00e2\ufffd\ufffd as manifested in actual design and use decisions and practices. It also proposes that disclosive ethics would aim to trace and disclose the intentional and emerging enclosure of politics from the very minute technical detail through to social practices and complex social-technical networks. The paper then proceeds to do a disclosive analysis of facial recognition systems. This analysis discloses that seemingly trivial biases in recognition rates of FRSs can emerge as very significant political acts when these systems become used in practice.<\/dc:description><dc:date>\n        2005<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:relation>\n        http:\/\/dx.doi.org\/10.1007\/s10676-005-4583-2<\/dc:relation><dc:identifier>\n        Introna, Lucas (2005) Disclosive ethics and information technology: disclosing facial recognition systems. Ethics and Information Technology, 7 (2). pp. 75-86. ISSN 1388-1957<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/114\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1007\/s10676-005-4583-2","http:\/\/eprints.lancs.ac.uk\/114\/"],"year":2005,"topics":[],"subject":["Journal Article","PeerReviewed"],"fullText":"Disclosive ethics and information technology: disclosing facial recognition systems\nLucas D. Introna\nCenter for the Study of Technology and Organisation, Lancaster University Management School, Lancaster, LA1 4YX, UK\nE-mail: l.introna@lancaster.ac.uk\nAbstract. This paper is an attempt to present disclosive ethics as a framework for computer and information\nethics \u2013 in line with the suggestions by Brey, but also in quite a different manner. The potential of such an\napproach is demonstrated through a disclosive analysis of facial recognition systems. The paper argues that the\npolitics of information technology is a particularly powerful politics since information technology is an opaque\ntechnology \u2013 i.e. relatively closed to scrutiny. It presents the design of technology as a process of closure in\nwhich design and use decisions become black-boxed and progressively enclosed in increasingly complex socio-\ntechnical networks. It further argues for a disclosive ethics that aims to disclose the nondisclosure of politics by\nclaiming a place for ethics in every actual operation of power \u2013 as manifested in actual design and use decisions\nand practices. It also proposes that disclosive ethics would aim to trace and disclose the intentional and\nemerging enclosure of politics from the very minute technical detail through to social practices and complex\nsocial-technical networks. The paper then proceeds to do a disclosive analysis of facial recognition systems. This\nanalysis discloses that seemingly trivial biases in recognition rates of FRSs can emerge as very significant\npolitical acts when these systems become used in practice.\nKey words: biases, disclosive ethics, facial recognition systems, false positives, information technology, politics\nIntroduction\nIt would not be controversial to claim that informa-\ntion technology has become ubiquitous, invading all\naspects of human existence. Most everyday technol-\nogies depend on microprocessors for their ongoing\noperation. Most organisations have become entirely\nreliant on their information technology infrastruc-\nture. Indeed information technology seems to be a\nvery cost-efficient way to solve many of the problems\nfacing an increasingly complex society. One can\nalmost say it has become a default technology for\nsolving a whole raft of technical and social problems.\nIt have become synonymous with societies view of\nmodernisation and progress. In this paper we will\nconsider facial recognition systems as one example of\nsuch a search for solutions.\nHowever, this reliance on information technology\nalso brings with it many new and different kinds of\nproblems. In particular, for our purposes, ethical\nconcerns of a different order. We would argue that\ninformation technology is mostly not evident, obvi-\nous, transparent or open to inspection by the ordin-\nary everyday person affected by it (Brey 2000). It is\nrather obscure, subsumed and black-boxed in ways\nthat only makes its \u2018surface\u2019 available for inspection.\nImbedded in the software and hardware code of these\nsystems are complex rules of logic and categorisation\nthat may have material consequences for those using\nit, or for the production of social order more gener-\nally (Introna and Nissenbaum 2000; Feenberg 1999;\nLatour 1992). However, often these remain obscured\nexcept for those experts that designed these systems \u2013\nand sometimes even not to them as we shall see in our\nanalysis of facial recognition systems below. Simply\nput: they are most often closed boxes unavailable for\nour individual or collective inspection and scrutiny.\nThis problem of \u2018closure\u2019 is made more acute by the\nfact that these systems are often treated as neutral\ntools that simply \u2018do the job\u2019 they were designed to\ndo. Differently put, we do not generally attribute\nvalues and choices to tools or artefacts but rather to\npeople. Nevertheless, Winner (1980) and Latour\n(1991, 1992) has shown convincingly that these tools\nhave inscribed in them value choices that may or may\nnot be very significant to those using them or affected\nby them \u2013 i.e. software programmes are political in as\nmuch as the rules of logic and categorisation they\ndepend on reflect or included curtain interests and\nnot others. Enclosed in these \u2018boxes\u2019 may be signifi-\ncant political programmes, unavailable or closed off\nfrom our critical and ethical gaze.\nPaper prepared for the Technology and Ethics Workshop\nat Twente\nEthics and Information Technology (2005) 7:75\u201386 \u0001 Springer 2005\nDOI 10.1007\/s10676-005-4583-2\nMany authors have realised this and have done a\nvariety of analysis to disclose the particular ways in\nwhich these technologies have become enrolled in\nvarious political programmes for the production of\nsocial order (Callon 1986; Latour 1991, 1992; Law\n1991). However, in this paper we would like to ask a\ndifferent question \u2013 the normative or ethical question.\nHow can we approach information technology as an\nethical problem? In response to this question we will\npropose, in accord with Philip Brey (2000), but in a\nrather different way, that the first principle of an\ninformation technology ethics should be disclosure.\nThus, we want to propose a form of disclosive ethics\nas a framework for information technology ethics.\nWe will aim to show how this may work in doing a\ndisclosive analysis of facial recognition systems.\nThus, this paper will have three parts: First, we will\ndiscuss the question of the politics of information\ntechnology in general; second, we will present our\nunderstanding of disclosive ethics and its relation to\npolitics; and finally, we will do a disclosive analysis of\nfacial recognition systems.\nThe politics of (information) technology as closure\nThe process of designing technology is as much a\nprocess of closing down alternatives as it is a process\nof the opening up of possibilities. In order for the\ntechnology to produce its intended outcome it needs\nto enforce its \u2018scripts\u2019 on its users. Its designers has to\nmake assumptions about users and the use context\nand often build these assumptions into the very\nmateriality of their artefacts. These artefacts then\nfunction as sub-plots in larger social scripts aimed at\n\u2018making society durable\u2019 \u2013 plots (and sub-plots) that\nare supposed to generate durable social order in\nwhich some ways of being are privileged and others\nare not. It is this closure that is an implicit part of\ntechnology design and use that is of interest to us. Let\nus consider this closure in more detail.\nThe micro-politics of the artefact\nTechnology is political (Winner 1980). By this we\nmean that technology, by its very design, includes\ncertain interests and excludes others. We are not\nsuggesting that this is always an explicit politics. In\nfact it is mostly implicit and part of a very mundane\nprocess of trying to solve practical problems. For\nexample, the ATM bank machine assumes a partic-\nular person in front of it. It assumes a person that is\nable to see the screen, read it, remember and enter a\nPIN code, etc. It is not difficult to imagine a whole\nsection of society that does not conform with this\nassumption. If you are blind, in a wheelchair, have\nproblem remembering, or unable to enter a PIN,\nbecause of disability, then your interest in getting\naccess to your account will be excluded by the actual\ndesign of the ATM. This \u2018closure\u2019 may not be obvious\nto the designers of ATMs as they may see their task\nas trying simply to solve a basic problem of making\nbanking transactions more efficient and accessible. In\ntheir minds they often design for the \u2018average\u2019 cus-\ntomer doing average transactions. And they are\nmostly right \u2013 but if they are not, then their biases can\nbecome profoundly stubborn. In some senses quite\nirreversible. Where does the excluded go to appeal\nwhen they are faced with a stubborn and mute object\nsuch as an ATM? Maybe they can work around it, by\ngoing into the branch for example. This may be\npossible. However, this exclusion becomes all the\nmore significant if banks start to close branches or\ncharge for an over-the-counter transaction (as some\nbanks are doing). Thus, as the micro-politics of the\nATM becomes tied to, and multiplied through other\nexclusionary social practice, trivial injustice soon\nmultiply into what may seem to be a coherent and\nintentional strategy of exclusion (Introna and Nis-\nsenbaum 2000; Agre and Mailloux 1997). Yet there is\noften nobody there that \u2018authored\u2019 it as such (Fou-\ncault 1975; Kafka 1925).\nThus, the politics of technology is more than the\npolitics of this or that artefact. Rather these artefacts\nfunction as nodes, or links, in a dynamic socio-tech-\nnical network kept in place by a multiplicity of arte-\nfacts, agreements, alliances, conventions, translations,\nprocedures, threats, and so forth: in short by rela-\ntionships of power and discipline (Callon 1986). Some\nare stable, even irreversible; some are dynamic and\nfragile. Analytically we can isolate and describe these\nnetworks (see Law 1991 for examples). However, as\nwe survey the landscape of networks we cannot locate,\nin any obvious manner, where they begin nor where\nthey end. Indeed we cannot with any degree of cer-\ntainty separate the purely social from the purely\ntechnical means from ends, cause from effect, designer\nfrom user, winners from losers, and so on.\nIn these complex and dynamic socio-technical\nnetworks ATMs, doors, locks, keys, cameras, algo-\nrithms, etc. function as political \u2018locations\u2019 where\nvalues and interests are negotiated and ultimately\n\u2018inscribed\u2019 into the very materiality of the things\nthemselves \u2013 thereby rendering these values and\ninterests more or less permanent (Akrich 1992;\nCallon 1986; Latour 1991, 1992; Law 1991). Through\nthese inscriptions, which may be more or less suc-\ncessful, those that encounter and use these inscribed\nartefacts become, wittingly or unwittingly, enrolled\ninto particular programmes, or scripts for action.\nLUCAS D. INTRONA76\nObviously, neither the artefacts nor those that draw\nupon them simply except these inscriptions and en-\nrolments as inevitable or unavoidable. In the flow of\neveryday life artefacts often get lost, break down, and\nneed to be maintained. Furthermore, those that draw\nupon them use them in unintended ways, ignoring or\ndeliberately \u2018misreading\u2019 the script the objects may\nendeavour to impose. Nevertheless, to the degree that\nthese enrolments are successful, the consequences of\nsuch enrolments can result in more or less profound\nclosures that ought to be scrutinised. We would claim\nthat the politics of artefacts is much more mundane\nand much more powerful than most other politics, yet\nit is often enclosed in such as way to evade our\nscrutiny. This is particularly true for information\ntechnology in which closure is much more powerful\nas the closure is itself closed off.\nOn the silent politics of the software algorithm\nHaving argued that technology is political, we now\nwant to claim that the politics of information tech-\nnology (in the form of software algorithms) is, in a\nsense, of a different order (Graham and Wood 2003).\nWe want to contend that scrutinising information\ntechnology is particularly problematic since infor-\nmation technology, in particular algorithms, is what\nwe would term an opaque technology as opposed to a\ntransparent technology (Introna 1998). Obviously we\ndo not see this distinction as a dichotomy but rather\nas a continuum. As an attempt to draw this distinc-\ntion some aspects are highlighted in Table 1 below.\nFacial recognition algorithms, which we will dis-\ncuss below, is a particularly good example of a opa-\nque technology. The facial recognition capability can\nbe imbedded into existing CCTV networks, making\nits operation impossible to detect. Furthermore, it is\npassive in its operation. It requires no participation\nor consent from its targets \u2013 it is \u2018non-intrusive,\ncontact-free process\u2019 (Woodward et al. 2003: 7). Its\napplication is flexible. It can as easily be used by a\nsupermarket to monitor potential shoplifters (as was\nproposed and later abandoned, by the Borders\nbookstore), by casinos to track potential fraudsters,\nby law enforcement to monitor spectators at a Super\nBowl match (as was done in Tampa, Florida), or used\nfor identifying \u2018terrorists\u2019 at airports (as is currently\nin operation at various US airports). However, most\nimportant of all is the obscurity of its operation.\nMost of the software algorithms at the heart of\nfacial recognition systems (and other information\ntechnology products) are propriety software objects.\nThus, it is very difficult to get access to them for\ninspection and scrutiny. More specifically, however,\neven if you can go through the code line by line, it is\nimpossible to inspect that code in operation, as it\nbecomes implemented through multiple layers of\ntranslation for its execution. At the most basic level\nwe have electric currents flowing through silicon\nchips, at the highest level we have programme\ninstructions, yet it is almost impossible to trace the\nconnection between these as it is being executed.\nThus, it is virtually impossible to know if the code\nyou inspected is the code being executed, when exe-\ncuted. In short, software algorithms are operationally\nobscure.\nIt is our argument that the opaque and \u2018silent\u2019\nnature of digital technology makes it particularly\ndifficult for society to scrutinise it. Furthermore, this\ninability to scrutinise creates unprecedented oppor-\ntunities for this silent and \u2018invisible\u2019 micro-politics to\nbecome pervasive (Graham and Wood 2003). Thus, a\nprofound sort of micro-politics can emerge as these\nopaque (closed) algorithms become enclosed in the\nsocial-technical infrastructure of everyday life. We\ntend to have extensive community consultation and\nimpact studies when we build a new motorway.\nHowever, we tend not to do this when we install\nCCTV in public places or when we install facial rec-\nognition systems in public spaces such as airports,\nshopping malls, etc. To put is simply: most informed\npeople tend to understand the cost (economic, per-\nsonal, social, environmental) of more transparent\ntechnologies such as a motorway, or a motorcar, or\nmaybe even cloning. However, we would argue that\nthey do not often understand the \u2018cost\u2019 of the more\nopaque information technologies that increasingly\npervade our everyday life. We will aim to disclose this\nTable 1. Opaque versus transparent technology\nOpaque technology is: Transparent technology is:\nEmbedded\/hidden On the \u2018surface\u2019\/conspicuous\nPassive operation (limited user involvement, often automatic) Active operation (fair user involvement, often manual)\nApplication flexibility (open ended) Application stability (firm)\nObscure in its operation\/outcome Transparent in its operation\/outcome\nMobile (soft-ware) Located (hard-ware)\nDISCLOSIVE ETHICS AND INFORMATION TECHNOLOGY 77\nin the case of facial recognition systems below. Before\nwe do this we want to give an account of what we\nmean by this \u2018disclosure\u2019 of disclosive ethics.\nDisclosive ethics as the \u2018other\u2019 side of politics\nEthics is always and already the \u2018other\u2019 side of politics\n(Critchley 1999). When we use the term \u2018politics\u2019\n(with a small \u2018p\u2019) \u2013 as indicated above \u2013 we refer to\nthe actual operation of power in serving or enclosing\nparticular interests, and not others. For politics to\nfunction as politics it seeks closure \u2013 one could say\n\u2018enrolment\u2019 in the actor network theory language.\nDecisions (and technologies) need to be made and\nprogrammes (and technologies) need to be imple-\nmented. Without closure politics cannot be effective\nas a programme of action and change. Obviously, if\nthe interests of the many are included \u2013 in the\nenclosure as it were \u2013 then we might say that it is a\n\u2018good\u2019 politics (such as democracy). If the interests of\nonly a few are included we might say it is a \u2018bad\u2019\npolitics (such as totalitarianism). Nevertheless, all\npolitical events of enclosing are violent as they always\ninclude and exclude as their condition of operation.\nIt is the excluded \u2013 the other on the \u2018outside\u2019 as it\nwere \u2013 that is the concern of ethics. Thus, every\npolitical action has, always and immediately, tied to\nits very operation an ethical question or concern \u2013 it\nis the other side of politics. When making this claim it\nis clear that for us ethics (with a small \u2018e\u2019) is not\nethical theory or moral reasoning about how we\nought live (Caputo 1993). It is rather the question of\nthe actual operation of closure in which the interests\nof some become excluded as an implicit part of the\nmaterial operation of power \u2013 in plans, programmes,\ntechnologies and the like. More particularly, we are\nconcerned with the way in which the interest of\nsome become excluded through the operation of\nclosure as an implicit and essential part of the design\nof information technology and its operation in social-\ntechnical networks.\nAs those concerned with ethics, we can see the\noperation of this \u2018closure\u2019 or \u2018enclosure\u2019 in many\nrelated ways. We can see it operating as already\n\u2018closed\u2019 from the start \u2013 where the voices (or interests)\nof some are shut out from the design process and use\ncontext from the start. We can also see it as an\nongoing operation of \u2018closing\u2019 \u2013 where the possibility\nfor suggesting or requesting alternatives are pro-\ngressively excluded. We can also see it as an ongoing\noperation of \u2018enclosing\u2019 \u2013 where the design decisions\nbecome progressively \u2018black-boxed\u2019 so as to be inac-\ncessible for further scrutiny. And finally, we can see it\nas \u2018enclosed\u2019 in as much as the artefacts become\nsubsumed into larger socio-technical networks from\nwhich it becomes difficult to \u2018unentangle\u2019 or scruti-\nnise. Fundamental to all these senses of closure is\n\u2018\u2018the event of closure [as] a delimitation which\nshows the double appartenance of an inside and an\noutside...\u2019\u2019 (Critchley 1999: 63).\nWe need to acknowledge that politics \u2013 or the\noperation of closure \u2013 is fundamental to the ongoing\nproduction of social order. Decisions have to be\nmade, technologies have to be designed and imple-\nmented, as part of the ongoing ordering of society.\nAgendas cannot be kept open forever, designs cannot\nbe discussed and considered indefinitely. Thus, we are\nnot suggesting an end to politics as the operation of\nclosure. Closure is a pragmatic condition for life.\nEqually, we are not arguing that the question of\nethics can, and ought to be, \u2018divorced\u2019 from politics.\nEthics cannot escape politics. The concern of ethics is\nalways and already also a political concern. To\nchoose, propose or argue for certain values \u2013 such as\njustice, autonomy, democracy and privacy as sug-\ngested by Brey (2000) \u2013 is already a political act of\nclosure. We may all agree with these values as they\nmight seem to serve our interests, or not. Neverthe-\nless, one could argue that they are very anthropo-\ncentric and potentially excludes the claims of many\nothers \u2013 animals, nature, the environment, things, etc.\nIf ethics cannot escape politics then it is equally\ntrue that politics cannot escape ethics. This is our\nstarting point \u2013 a powerful one in our view. The\ndesign or use of information technology is not mor-\nally wrong as such. The moral wrongdoing is rather\nthe nondisclosure of the closure or the operation of\npolitics as if ethics does not matter \u2013 whether it is\nintended or not. We know that power is most effec-\ntive when it hides itself (Foucault 1975). Thus, power\nhas a very good reason to seek and maintain non-\ndisclosure. Disclosive ethics takes as its moral\nimperative the disclosure of this nondisclosure \u2013 the\npresumption that politics can operate without regard\nto ethics \u2013 as well as the disclosure of all attempts at\nclosing or enclosing that are implicitly part of the\ndesign and use of information technology in the\npursuit of social order.\nObviously at a curtain level design is rather a\npragmatic question. However, it is our contention\nthat many seemingly pragmatic or technical decisions\nmay have very important and profound consequences\nfor those excluded \u2013 as we will show below. This is\nthe important task of disclosive ethics. Not merely to\nlook at this or that artefact but to trace all the moral\nimplications (of closure) from what seems to be\nsimple pragmatic or technical decisions \u2013 at the level\nof code, algorithms, and the like \u2013 through to social\npractices, and ultimately, to the production of\nLUCAS D. INTRONA78\nparticular social orders, rather than others. For dis-\nclosive ethics it is the way in which these seemingly\npragmatic attempts at closing and enclosing connect\ntogether to deliver particular social orders that\nexcludes some and not others \u2013 irrespective of whe-\nther this was intended by the designers, or not. Indeed\nit will be our argument that in the design of complex\nsocio-technical networks these exclusionary or\nenclosing possibilities often do not surface as a con-\nsideration when making this or that particular design\ndecision. Furthermore, these exclusionary possibili-\nties often emerge as a systemic effect or outcome with\nno particular \u2018author\u2019 in charge of the script as such.\nIndeed this is what we intend to show in the dis-\nclosing of facial recognition systems below. In sum-\nmary, disclosive ethics operates with two principles:\n(a) To disclose the nondisclosure of politics by\nclaiming a place for ethics as being always and\nimmediately present in every actual operation of\npower\n(b) To trace and disclose the intentional or uni-\nternational enclosure of values and interests from\nevery minute technical detail through to social\npractices and complex social-technical networks.\nWe will now turn our attention to a disclosive anal-\nysis of facial recognition systems in order to disclose\nits politics and the way in which these may emerge in\nthe social practices of securing identity.\nDisclosing facial recognition systems\nGetting a digital face: the facial recognition system\nFigure 1 below depicts the typical way that a facial\nrecognition system (FRS) system can be made oper-\national.\nThe first step is the capturing of a face image. This\nwould normally be done using a still or video camera.\nAs such it can be incorporated into existing \u2018passive\u2019\nCCTV systems. However, locating a face image in the\nfield of vision is not a trivial matter at all. The\neffectiveness of the whole system is dependent on the\nquality of the captured face image. The face image is\npassed to the recognition software for recognition\n(identification or verification). This would normally\ninvolve a number of steps such as normalising the\nface image and then creating a \u2018template\u2019 of \u2018print\u2019 to\nbe compared to those in the database. If there is a\n\u2018match\u2019 then an alarm would solicit an operator\u2019s\nattention to verify the match and initiate the appro-\npriate action. The match can either be a true match\nwhich would lead to investigative action or it might\nbe a \u2018false positive\u2019 which means the recognition\nalgorithm made a mistake and the alarm would be\ncancelled. Each element of the system can be located\nat different locations within a network, making it easy\nfor a single operator to respond to a variety of\nsystems.\nFor our analysis we want to concentrate on steps\ntwo and three of the system. We want to scrutinise\nthe FR algorithms, the image database (also called\nthe gallery) and the operators. At each of these points\nimportant decisions are made which may have\nimportant ethical and political implications.\nFacial recognition algorithms and reduction\nResearch in software algorithms for facial recogni-\ntion has been ongoing for the last 30 years or so\n(Gross et al. 2001a). However, advances in informa-\ntion technology and statistical methods have given\nimpetus to this development with seemingly excellent\nrecognition results and low error rates \u2013 at least in\nideal laboratory conditions. It is possible to identify\nFigure 1. Overview of FRS (Source: Face Recognition, Vendor Test 2002).\nDISCLOSIVE ETHICS AND INFORMATION TECHNOLOGY 79\ntwo main categories of algorithms according to Gross\net al. (2001a):\nThe image template algorithms. These algorithms\nuse a template-based method to calculate the corre-\nlation between a face and one or more standard\ntemplates to estimate the face identity. These stan-\ndard templates tend to capture the global features of\na gallery of face images. Thus, the individual face\nidentity is the difference between (or deviation from)\nthe general or \u2018standard\u2019 face. This is an intuitive\napproach since we, as humans tend to look for dis-\ntinctive features (or differences from the general)\nwhen we identify individuals. Some of the methods\nused are: Support Vector Machines (SVM), Principal\nComponent Analysis (PCA), Neural Networks,\nKernel Methods, etc. The most commercially known\ntemplate based algorithm is the MIT Bayesian Ei-\ngenface technique, which has been developed with the\nPCA method. During various tests conducted in\n1996, its performance was consistently near the top\ncompared to other available at the time.\nThe geometry feature-based algorithms. These\nmethods capture the local facial features and their\ngeometric relationships. They often locate anchor\npoints at key facial features (eyes, nose, mouth, etc.),\nconnect these points to form a net and then measure\nthe distances and angles of the net to create a unique\nface \u2018print\u2019. The most often cited of these is the\ntechnique known as Local Feature Analysis (LFA),\nwhich is used in the Identix (formerly known as\nVisionics) face recognition system called FaceIt. The\nLFA method, in contrast to the PCA technique, is\nless sensitive to variations in lighting, skin tone, eye\nglasses, facial expression, hair style, and individual\u2019s\npose up to 35 degrees.\nThe commonality in both of these groups of\ntechniques is the issue of reduction. In order to be\nefficient in processing and storage the actual face\nimage gets reduced to a numerical representation (as\nsmall as 84 bytes or 84 individual characters in the\ncase of FaceIt). With this reduction certain infor-\nmation is disregarded (as incidental or irrelevant) at\nthe expense of others. It is here that we need to focus\nour analysis. What is the consequences of the process\nof reduction. It would be best to understand this\nthrough some detailed study of the logic and opera-\ntion of these algorithms in diverse settings with\ndiverse databases. This has not yet being done (not\neven in the Facial Recognition Vendor Tests of 2002\n(FRVT 2002) which has been the most comprehen-\nsive thus far). Nevertheless, with our limited knowl-\nedge we can make some logical conclusions and then\nsee how these may play out in the FRVT 2002 eval-\nuations. How will the reduction, effect the perfor-\nmance of these algorithms?\n\u2022 Template based algorithms. In these algorithms\ncertain biases become built into the standard\ntemplate. It obviously depends on the gallery used\nto create the standard template as well as the range\nof potential variations within a population. For\nexample, because minorities tend to deviate the\nmost from the standard template they might\nbecome easier to recognise.\n\u2022 Feature based algorithms. These algorithms do not\nhave an initial bias. However, because of the\nreduction the \u2018face prints\u2019 generated are in close\nproximity to each other. Thus, as the gallery\ndatabase increases more and more face prints are\ngenerated in ever diminishing proximity, thereby\nmaking the discrimination required for the recog-\nnition task more difficult. Therefore, the operation\nof the system deteriorates rapidly as the database\nincreases (this is also true for template based\nalgorithms). It also makes the system dependent\non good quality face images. The implication of\nthis is that the system will operate at its best with a\nsmall database and good quality face capture, such\nas an operator assisted face capture (reintroducing\nthe operator bias). In addition to this, it will tend\nto be better at identifying those that are more\ndistinctive, or less similar, to those already in the\ndatabase (such as minorities).\nThus, in both cases we would expect some form of\nbias to emerge as a result of the reduction. Is this\nconclusion borne out by the performance of these\nalgorithms in the FRVT? Let us now consider the\nresults of these evaluations.\nThe evaluations: reduction, operation and error\nThe most significant evaluation of FRSs happened\nwith the Facial Recognition Vendor Tests of 2002\n(Phillips et al. 2003). These test were independent\ntests sponsored by a host of organizations such as\nDefense Advanced Research Projects Agency\n(DARPA), the Department of State and the Federal\nBureau of Investigation. This evaluation followed in\nthe footsteps of the earlier FRVT of 2000 and the\nFERET evaluations of 1994, 95 and 96. In the FRVT\n2002 ten FRS vendors participated in the evaluations.\nThe FRVT of 2002 were more significant than any of\nthe previous evaluations because of:\n\u2022 The use of a large database (37,437 individuals)\n\u2022 The use of a medium size database of outdoor and\nvideo images\n\u2022 Some attention given to demographics\nThe large database (referred to as the HCInt data set)\nis a subset of a much larger database which was\nLUCAS D. INTRONA80\nprovided by the Visa Services Directorate, Bureau of\nConsular Affairs of the U.S. Department of State. The\nHCInt data set consisted of 121,589 images of 37,437\nindividuals with at least three images of each person.\nAll individuals were from theMexican non-immigrant\nvisa archive. The images were typical visa application\ntype photographs with a universally uniform back-\nground, all gathered in a relatively consistent manner.\nThe medium size database consisted of a number\noutdoor and video images from various sources.\nFigure 2 below gives an indication of the images in\nthe database. The top row contains images taken\nindoors and the bottom contains outdoor images\ntaken on the same day. Notice the quality of the\noutdoor images. The face is consistently located in\nthe frame and similar in orientation to the indoor\nimages.\nFor the identification task an image of an\nunknown person is provided to a system (assumed to\nbe in the database). The system then compares the\nunknown image (called the probe image) to the\ndatabase of known people. The results of this com-\nparison are then presented by the system, to an\noperator, in a ranked listing of the top n \u2018candidates\u2019\n(referred to as the \u2018rank\u2019, typically anywhere from 1\nto 50). If the correct image is somewhere in the top n,\nthen the system is considered to have performed the\nidentification task correctly. Figure 3 below indicates\nthe performance at rank 1, 10 and 50 for the three top\nperformers in the evaluation.\nWith the very good images from the large database\n(37,437 images) the identification performance of the\nbest system at rank one is 73% at a false accept rate\nof 1%. There is a tradeoff between the recognition\nrates and the level of \u2018false accepts\u2019 (incorrect iden-\ntification) one is prepared to accept, the false accept\nrate. If you are prepared to accept a higher false\naccept rate then the recognition performance can go\nup. However, this will give you more cases of false\nidentification to deal with. This rate is normally a\nthreshold parameter that can be set by the operators\nof the system.\nWhat are the factors that can detract from this\n\u2018ideal\u2019 performance? There might bemany. The FRVT\n2002 considered three of the most important ones:\n\u2022 Indoor versus outdoor images\n\u2022 The time delay between the database image and\nthe probe image\n\u2022 The size of the database\nThe identification performance drops dramatically\nwhen outdoor images are used \u2013 in spite of the fact\nthat they can be judge as relatively good \u2013 as indi-\ncated above. One would not expect a typical video\ncamera to get this quality of image all the time. For\nthe best systems the recognition rate for faces cap-\ntured outdoors (i.e. less than ideal circumstances) was\nonly 50% at a false accept rate of 1%. Thus, as the\nreport concluded: \u2018\u2018face recognition from outdoor\nimagery remains a research challenge area.\u2019\u2019 The\nmain reason for this problem is that the algorithm\ncannot distinguish between the change in tone, at the\nFigure 2. Indoor and outdoor images from the medium\ndata base. (from FRVT2002 report, p. 16).\nFigure 3. Performance at rank 1, 10 and 50 for the three top performers in the evaluation (from FRVT 2002, Overview and\nSummary, p. 9).\nDISCLOSIVE ETHICS AND INFORMATION TECHNOLOGY 81\npixel level, caused by a relatively dark shadow, versus\nsuch a change caused by a facial feature. As such it\nstarts to code shadows as facial features. The impact\nof this on the identification may be severe if it hap-\npens to be in certain key areas of the face.\nAs one would expect, the identification perfor-\nmance also decreases as time laps increases between\nthe acquisition of the database image and the newly\ncaptured probe image presented to a system. FRVT\n2002 found that for the top systems, performance\ndegraded at approximately 5% points per year. It is\nnot unusual of the security establishment to have a\nrelatively old photograph of a suspect. Thus, a two\nyear old photograph will take 10% off the identifi-\ncation performance. A study by the US National\nInstitute of Standards and Technology found that\ntwo sets of mugshots taken 18 months apart pro-\nduced a recognition rate of only 57% (Brooks 2002).\nGross et al. (2001: 17) found an even more dramatic\ndeterioration. In their evaluation, the performance\ndropped by 20% in recognition rate for images just\ntwo weeks apart. Obviously these evaluations are not\ndirectly comparable. Nevertheless, there is a clear\nindication that there may be a significant deteriora-\ntion when there is a time gap between the database\nimage and the probe image.\nWhat about the size of the database? For the best\nsystem, \u2018\u2018the top-rank identification rate was 85% on\na database of 800 people, 83% on a database of\n1,600, and 73% on a database of 37,437. For every\ndoubling of database size, performance decreases by\ntwo to three overall percentage points\u2019\u2019 (Phillips et al.\n2003: 21). What would this mean for extremely large\ndatabases? For example, the UK fingerprint database\nconsists of approximately 5.5 million records. If one\nhad a similar size \u2018mugshot\u2019 database how will the\nalgorithms perform in identifying a probe image in\nthat database? If one takes the decrease to be 2.5%\nfor every doubling of the database, and use 73% at\n37,437 as the baseline, then one would expect the\nidentification performance to be approximately 55%\nin ideal conditions and as low as 32% in less than\nideal conditions.\nTo conclude this discussion we can imagine a very\nplausible scenario where we have a large database,\nless than ideal image due to factors such as variable\nillumination, outdoor conditions, poor camera angle,\netc. and the probe image is relatively old, a year or\ntwo. Under these conditions the probability to be\nrecognized is very low, unless one sets the false accept\nrate to a much higher level, which means that there is\na risk that a high number of individual may be sub-\njected to scrutiny for the sake of a few potential\nidentifications. What will be the implications of this\nfor practice? We will take up this point again below.\nObviously, we do not know how these factors would\nact together and they are not necessarily cumulative.\nNevertheless, it seems reasonable to believe that there\nwill be some interaction that would lead to some\ncumulative affect.\nSuch a conclusion can make sense of the Tampa\nPolice Department case reported by ACLU (Stanley\nand Steinhardt 2002) as well as the Palm Beach\nInternational Airport also reported by the ACLU. In\nthe Tampa case the system was abandoned because of\nall the false positive alarms it generated. As far as it\ncould be ascertained it did not make one single\npositive identification. In the Palm Beach Airport\ncase the system achieved a mere 47% correct identi-\nfications of a group of 15 volunteers using a database\nof 250 images (Brooks 2002). In Newham, UK, the\npolice admitted that the FaceIt system had, in its\ntwo years of operation, not made a single positive\nidentification, in spite of working with a small data-\nbase. One could argue that there might not have been\nthe potential for a match to be made as none of the\nindividual in the database actually appeared in the\nstreet. Nevertheless, the system could not identify a\nGuardian journalist, placed in the database, that\nintentionally presented himself in the two zones\ncovered by the system (Meek 2002). These cases\nindicate the complexity of real world scenarios. We\nnow want to move to the focal concern of this paper\nnamely the question of biases in the algorithms\nthemselves.\nReduction and biased code\nThe most surprising outcome \u2013 for those involved \u2013\nof the FRVT 2002 is the realization that the algo-\nrithms displayed particular identification biases.\nFirst, recognition rates for males were higher than\nfemales. For the top systems, identification rates for\nmales were 6\u20139% points higher than that of females.\nFor the best system, identification performance on\nmales was 78% and for females was 79%. Second,\nrecognition rates for older people were higher than\nyounger people. For 18\u201322 year olds the average\nidentification rate for the top systems was 62%, and\nfor 38\u201342 year olds was 74%. For every 10 years\nincrease in age, on average performance increases\napproximately 5% through to age 63. Unfortunately,\nthey could not check race as the large data set con-\nsisted of mostly Mexican non-immigrant visa appli-\ncants. However, research by Givens et al. (2003),\nusing PCA algorithms, has confirmed the biases in\nthe FRVT 2002 (except for the gender bias) and also\nfound a significant race bias. This was confirmed\nusing balanced databases and controlling for other\nfactors. They concluded that: \u2018\u2018Asians are easier [to\nLUCAS D. INTRONA82\nrecognize] than whites, African-Americans are easier\nthan whites, other race members are easier than\nwhites, old people are easier than young people, other\nskin people are easier to recognize than clear skin\npeople...\u2019\u2019 (p. 8). Their results are indicated in\nFigure 4 below.\nThese results were also found in another context\nby Furl, Phillips and O\u2019Toole (2002) in their study of\nrecognition performance by 13 different algorithms.\nOne can legitimately ask whether these differences,\nprobably in the order of 5\u201310%, really makes a dif-\nference? Are they not rather trivial? We would argue\nthat taken by themselves they may seem rather trivial.\nHowever, as we argued earlier on, it is when these\ntrivial differences become incorporated into a net-\nwork of practices that they may become extremely\nimportant. This is what we now want to explore: the\npolitics of the digital face as it becomes imbedded in\npractices.\nClosure and the ethics of the digital face\nFRSs: efficient, effective and neutral\nMany security analysts see FRSs as the ideal bio-\nmetric to deal with the new emerging security envi-\nronment (post 11 September). They claim that it is\nefficient (FaceIt only requires a single 733 Mhz Pen-\ntium PC to run) and effective, often quoting close to\n80% recognition rates from the FRVT 2002 evalua-\ntion while leaving out of the discussion issues of the\nquality of the images used in the FRVT, the size of\nthe database, the elapsed time between database\nimage and probe image, etc. But most of all they\nclaim that these systems \u2018\u2018performs equally well on all\nraces and both genders. Does not matter if popula-\ntion is homogeneous or heterogeneous in facial\nappearance\u2019\u2019 (Faceit technical specification1). This\nclaim is not only made by the suppliers of FRSs such\nas Identix and Imagis Technologies. It is also echoed\nin various security forums: \u2018\u2018Face recognition is\ncompletely oblivious to differences in appearance as a\nresult of race or gender differences and is a highly\nrobust Biometrics\u2019\u20192 Even the critical scholar Gary\nMarx (1995: 238) argued that algorithmic surveillance\nprovides the possibility of eliminating discrimination.\nThe question is not whether these claims are correct\nor not. One could argue that in a certain sense they\nare correct. The significance of these claims is the way\nthey frame the technology. It presents the technology\nitself as neutral and unproblematic. More than this it\npresents the technology as a solution to the problem\nof terrorism. Atick of Identix claimed, in the wake of\nthe 9\/11 attacks, that with FaceIt the US has the\n\u2018\u2018ability to turn all of these cameras around the\ncountry into a national shield\u2019\u2019 (O\u2019Harrow 2001). He\nmight argue that in the face of terrorism \u2018minor\u2019\ninjustices (biases in the algorithms) and loss of pri-\nvacy is a small price to pay for security. This may be\nso, although we would disagree.\nNevertheless, our main concern is that these\narguments present the technical artefacts in isolation\nwith disregard to the socio-technical networks within\nwhich they will become enclosed. As argued above, it\nis not just the micro-politics of the artefact that is the\nissue. It is how these become multiplied and magni-\nfied as they become tied to other social practices that\nis of significance. We need to disclose the \u2018network\neffects\u2019, as it were, of the micro-politics of artefacts.\nThis is especially so for opaque digital technology.\nThere is every reason to believe that the silent and\nnon-invasiveness of FRSs make it highly desirable as\na biometric for digital surveillance. It is therefore\nimportant that this technology becomes disclosed for\nits potential politics in the socio-technical network of\ndigital surveillance. Thus, not just as isolated soft-\nware objects as was done in the FRVTs but in its\nmultiplicity of implementations and practices. We\nwould claim it is here where the seemingly trivial\nexclusions may become very important as they\nbecome incorporated into actual practices.\nFRSs and the production of suspects\nThere is an urgent need for an in-depth study of FRSs\nin practice (as has been done with CCTV by Norris\nand Armstrong (1999) and others). However, since we\ncurrently only have a limited number of systems in\noperation and due to the sensitivity of these imple-\nmentations it is unlikely that we would be able to do\nso in the near future. Thus, in the face of this limita-\ntion, we propose to outline what we consider to be a\nhighly probable scenario of how these digital closures\nmay become incorporated into other practices that\nwould render these seemingly trivial biases significant.\nBased on the FRVT of 2002 we know that,\nalthough FRSs have the capability to achieve a 70\u2013\n85% accuracy rate, this is only in ideal circumstances.\nThe system\u2019s performance degrades significantly in an\nuncontrolled \u2018face-in-the-crowd\u2019 environment, with a\nlarge database, and where there is an elapsed time\nbetween the database image and the probe image. This\nwould seem to us to be a usual rather than an unusual\nsituation. What will happen if the system\u2019s perfor-\nmance degrades under these rather usual conditions?\n1 http:\/\/www.identix.com\/newsroom\/news_biometrics_face_acc.html\n2 http:\/\/www.ats-computers.com\/biometrics\/face.html http:\/\/\nwww.biocom.tv\/BIOMETRICS_types.htm\nDISCLOSIVE ETHICS AND INFORMATION TECHNOLOGY 83\nWe would propose that two possibilities are most\nlikely. First, it is possible that the operators will\nbecome so used to false positives that they will start\nto treat all alarms as false positives thereby rendering\nthe system useless. Alternatively, they may deal with\nit by increasing the identification threshold (request-\ning the system to reduce the number of false posi-\ntives). This will obviously also increase the false\nnegatives, thereby raising all sorts of questions about\nthe value of the system into question. However, more\nimportant to us, with an increased threshold small\ndifferences in identifiability (the biases outlined\nabove) will mean that those that are easier to identify\nby the algorithms (African-Americans, Asians, dark\nskinned persons and older people) will have a greater\nprobability of being scrutinised. If the alarm is an\nactual positive recognition then one could argue that\nnothing is lost. However, it also means that these\ngroups would be subjected to a higher probability of\nscrutiny as false positives, i.e. mistaken identity.\nMoreover, we would propose that this scrutiny will\nbe more intense as it would be based on the\nassumption that the system is working at a higher\nlevel and therefore would be more accurate. In such a\ncase existing biases, against the usual suspects (such\nas minorities), will tend to come into play (Norris and\nArmstrong 1999). The operators may even override\ntheir own judgements as they may think that the\nsystem under such high conditions of operation must\n\u2018see something\u2019 that they do not. This is highly likely\nas humans are not generally very good at facial rec-\nognition in pressurised situations as was indicated in\na study by Kemp et al. (1997). Thus, under these\nconditions the bias group (African-Americans,\nAsians, dark skinned persons and older people) may\nbe subjected to disproportionate scrutiny, thereby\ncreating a new type of \u2018digital divide\u2019 (Jupp in\nGraham and Wood, 2003: 234).\nHow likely is this scenario? We believe it to be\nmore likely than we presume. We have only the fol-\nlowing anecdotal evidence reported in the Discover\nMagazine of an installation at the Fresno Yosemite\nInternational Airport to suggest:\n\u2018\u2018[The system] generates about one false positive for\nevery 750 passengers scanned, says Pelco vice\npresident Ron Cadle. Shortly after the system was\ninstalled, a man who looked as if he might be from\nthe Middle East set the system off. \u2018\u2018The gentleman\nwas detained by the FBI, and he ended up spending\nthe night,\u2019\u2019 says Cadle. \u2018\u2018We put him up in a hotel,\nand he caught his flight the next day.\u2019\u2019 (Garpinkle\n2002, p. 19 \u2013 emphasis added)\nTo produce only one false positive per 700 passengers\nthe system had to operate with a very restricted false\npositive rate, thereby suggesting that an alarm must\n\u2018mean something\u2019. Notice that one of the false posi-\ntives was a man supposedly from \u2018Middle Eastern\u2019\nFigure 4. From Givens et al. (2003) indicating which factor make it harder or easier to correctly identify a probe image\npresented to a system.\nLUCAS D. INTRONA84\norigin. The individual was detained and questioned\nby the FBI because he \u2018\u2018looked as if he might be from\nthe Middle East\u2019\u2019 in spite of the fact that he was\nobviously a false positive. There could be many\nexplanations for this action. Nevertheless, it is likely\nthat they may have decided to detain him \u2018just in\ncase\u2019 the system saw something they did not see. This\nis likely in a situation where a human operator must\nmake a decision. We know from research that\nhumans find it very difficult to identify individual\nfrom other ethnic groups (Kemp et al. 1997), exactly\nthe group that we would expect to emerge as likely\nfalse positives. In these moments of uncertainty, the\nFRSs may be taken as more authoritative than\nthe humans involved. This case clearly demonstrates\nthe scenario we outline above. Our disclosive analysis\nhas demonstrated that seemingly trivial differences in\nrecognition rates, within the algorithm, can indeed\nhave important political (ethical) implications for\nsome when it becomes incorporated into a whole set\nof socio-technical surveillance practices.\nOne might imagine that in an environment where\nthere is an acute sense of vulnerability it would not be\nunreasonable to store these false positives in a data-\nbase \u2018just in case\u2019 (Lyon 2001, 2002). These false\npositive may then become targets for further scrutiny.\nWhy? Just because they have features that make them\nmore distinctive. We are not saying that this will\nhappen. We are merely trying to indicate how seem-\ningly trivial \u2018technical issues\u2019 can add up to strong\npolitical ideologies at the expense of some for the\nsake of others. This is the issue of the politics \u2013 and\nethics \u2013 of FRSs. This is particularly dangerous pol-\nitics in the case of silent and opaque technologies\nsuch as FRSs. Obviously more in-depth study of\nactual installations are required.\nThere is no doubt in our minds that facial bio-\nmetric is a very important part of the future security\ninfrastructure. Kopel and Krause (2003) reports that:\n\u2018\u2018As of June 2001 the Departments of Justice and\nDefence had given about $21.3 million and $24.7\nmillion, respectively, to the research and development\nof FRSs.\u2019\u2019 Its efficiency, ease of implementation and\ninvisible nature make it and ideal biometric. We\nbelieve, we have demonstrated that there are many\naspects of this opaque technology that still needs to\nbe disclosed (see Agre 2003 for more indications of\nwhat might be disclosed).\nNevertheless, this disclosive analysis of facial rec-\nognition systems is not complete. We have not looked\nat those that have been excluded from the start. For\nexample, the fact that most of the research in FRSs\nare sponsored by US government agencies, who has\nbeen excluded through this mechanism? We have not\nconsidered the way in which equally valid other\nalternatives have become progressively excluded.\nWhat other ways of securing is possible? More\nimportantly, we have not disclosed ourselves as those\ndoing the disclosing. How does this analysis itself\nenclose? Indeed, disclosive ethics is an infinite task.\nWe believe it is worth doing even if there is no clear\nguidelines and no clear end. This is in our view not a\nflaw but rather its strength. It expects every closure to\nbe disclosed irrespective of where it emanates from \u2013\nthat is why it is disclosive.\nReferences\nM. Akrich. The De-scription of Technical Objects. In W.E.\nBijker and J. Law, editors, Shaping Technology\/Building\nSociety, pp. 205\u2013224. MIT Press, Cambridege, 1992.\nP.E. Agre. Your Face is Not a Bar Code: Arguments\nAgainst Face Recognition in Public Places, [Online],\n2003. Available: http:\/\/dlis.gseis.ucla.edu\/pagre, [2003,\nMay 25].\nP. Agre and C. Mailloux. Social Choice about Privacy:\nIntelligent Vehicle-Highway Systems in the United\nStates. In B. Friedman, editor, Human Values and Design\nof Computer Technology, Cambridge University Press,\nCambridge, 1997.\nBrooks, Michael. \u2018\u2018Face-off\u2019\u2019, New Scientist, Vol. 175, Issue\n2399, 9\/7\/2002.\nPhilip. Brey. Disclosive Computer Ethics. Computers and\nSociety, 30(4): 10\u201316, 2000.\nMichel. Callon. Some Elements of a Sociology of Trans-\nlation: Domestication of the Scallops and the Fishermen\nof St Brieuc Bay. In John. Law, editor, Power, Action and\nBelief, pp. 196\u2013233. Routledge & Kegan Paul, London,\n1986.\nJohn D. Caputo, Against Ethics. Indiana University Press,\nIndianapolis, 1993.\nSimon. Critchley, The Ethics of Deconstruction: Derrida and\nLevinas. 2nd edn. Edinburgh University Press, Edin-\nburgh, 1999.\nAndrew. Feenberg, Questioning Technology. London and\nNew York, Routledge, 1999.\nN. Furl, J.P. Phillips and A.J. O\u2019Toole. Face Recognition\nAlgorithms and the Other-race Effect: Computational\nMechanisms for a Developmental Contact Hypothesis.\nCognitive Science, 26: 797\u2013815, 2002.\nFacial Recognition Vendor Test 2002 (FRTV2002),\n[Online], Available: http:\/\/www.frvt.org\/FRVT2000\/\ndefault.htm, [2003, Aug.1].\nM. Foucault, Discipline and Punish, The Birth of the Prison.\nPenguin Books Ltd., London, UK, 1975.\nS. Garpinkle. Don\u2019t Count on Face-recognition Technol-\nogy to Catch Terrorists. Discover, 23(9): 17\u201320, 2002.\nG. Givens, J.R. Beveridge, B.A. Draper, and D. Bolme. A\nstatistical Assessment of Subject Factors in the PCA\nRecognition of Human Faces, [Online], 2003. Available:\nhttp:\/\/www.cs.colostate.edu\/evalfacerec\/papers\/csusacv03.\npdf,[2003, July.10].\nDISCLOSIVE ETHICS AND INFORMATION TECHNOLOGY 85\nS. Graham and D. Wood. Digitizing Surveillance: Catego-\nrization, Space and Inequality. Critical Social Policy,\n20(2): 227\u2013248, 2003.\nR. Gross, J. Shi, and J.F. Cohn. Quo vadis Face Recog-\nnition?, [Online], 2001a. Available: http:\/\/dagwood.vs\nam.ri.cmu.edu\/ralph\/Publications\/QuoVadisFR.pdf,\n[2003, July 10].\nL.D. Introna. Oppression, Resistance and Information\nTechnology: Some Thoughts on Design and Values,\nDesign for Values: Ethical, Social and Political Dimen-\nsions of Information Technology workshop sponsored\nby the NSF DIMACS held at Princeton University,\nUSA, February 27-March 1, 1998.\nL.D. Introna and H. Nissenbaum. The Internet as a\nDemocratic Medium: Why the Politics of Search Engines\nMatters. Information Society, 16(3): 169\u2013185, 2000.\nF. Kafka, The trial. Penguin Books Ltd., London, England,\n1925.\nR. Kemp, N. Towell and G. Pike. When Seeing Should not\nbe Believing: Photographs, Credit Cards and Fraud.\nApplied Cognitive Psychology, 11: 211\u2013222, 1997.\nD. Kopel and M. Krause. Face the FactsFacial recognition\ntechnology\u2019s troubled past \u2013 and troubling future.\n[Online] 2003. Available: http:\/\/www.reason.com\/0210\/\nfe.dk.face.shtml, [2003, Nov. 18].\nB. Latour. Technology is Society Made Durable. In J.\nLaw, editor, A Sociology of Monsters: Essays on Power,\nTechnology and Domination, pp. 103\u2013131. Routledge,\nLondon, 1991.\nB. Latour. Where are the Missing Masses? The Sociology\nof a Few Mundane Artefacts. In W.L.J. Bijker, editors,\nShaping Technology\/Building Society, pp. 225\u2013258. MIT\nPress, London, 1992.\nJohn. Law, The Sociology of Monsters: Essays on Power,\nTechnology and Domination. Routledge, London, 1991.\nD. Lyon, Surveillance Society, Monitoring Everyday Life.\nOpen University Press, Philadelphia, USA, 2001.\nD. Lyon. Surveillance After September 11, 2001, [Online],\n2002. Available: http:\/\/www.fine.lett.hiroshima-u.ac.jp\/\nlyon\/lyon2.html, [2003, July.10].\nG.T. Marx. The Engineering of Social Control: The search\nfor the silver Bullet. In J. Hagen and R. Peterson, editors,\nGrime and Inequality, pp. 225\u201346. Stanford University\nPress, Stanford, CA, 1995.\nJ. Meek. Robo cop: Some of Britain\u2019s 2.5 million CCTV\ncameras are being hooked up to a facial recognition\nsystem designed to identify known criminals. But does it\nwork, Guardian, June 13, 2002.\nC. Norris and G. Armstrong, The Maximum Surveillance\nSociety, The Rise of CCTV. Berg, New York, USA, 1999.\nR. O\u2019Harrow Jr. Facial Recognition System Considered\nFor US Airports, Washington Post, Monday, September\n24, 2001, Page A14.\nP. Phillips, P. Grother, R. Micheals, D.M. Blackburn, E.\nTabassi, and J.M. Bone. Face Recognition Vendor Test\n2002: Overview and Summary, [Online], 2003. Available:\nhttp:\/\/www.biometricsinstitute.org\/bi\/ FaceRecognition-\nVendorTest2002.pdf, [2003, May 30].\nJ. Stanley and B. Steinhardt. Drawing a Blank: The Failure\nof Facial Recognition Technology in Tampa, Florida,\nACLU Special Report, 2002.\nLandon. Winner. Do Artefacts Have Politics. Daedalus,\n109: 121\u2013136, 1980.\nJ. Woodward, C. Horn, J. Gatune and A. Thomas.\nBiometrics: A Look at Facal Recognition, Documented\nBriefing prepared for the Virginia State Crime Commis-\nsion, 2003 (available at http:\/\/www.rand.org).\nLUCAS D. INTRONA86\n"}