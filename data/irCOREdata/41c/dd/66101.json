{"doi":"10.1214\/105051606000000079","coreId":"66101","oai":"oai:dro.dur.ac.uk.OAI2:3182","identifiers":["oai:dro.dur.ac.uk.OAI2:3182","10.1214\/105051606000000079"],"title":"Positive recurrence of processes associated to crystal growth models.","authors":["Andjel,  A.","Menshikov,  M. V.","Sisko,  V."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2006-10-02","abstract":"We show that certain Markov jump processes associated to crystal growth models are positive recurrent when the parameters satisfy a rather natural condition","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/66101.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/3182\/1\/3182.pdf","pdfHashValue":"a504d7acdad303730ea7737f1de8a1796ec841ec","publisher":"Institute of Mathematical Statistics","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:3182<\/identifier><datestamp>\n      2017-09-25T10:09:48Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Positive recurrence of processes associated to crystal growth models.<\/dc:title><dc:creator>\n        Andjel,  A.<\/dc:creator><dc:creator>\n        Menshikov,  M. V.<\/dc:creator><dc:creator>\n        Sisko,  V.<\/dc:creator><dc:description>\n        We show that certain Markov jump processes associated to crystal growth models are positive recurrent when the parameters satisfy a rather natural condition. <\/dc:description><dc:subject>\n        Positive recurrent<\/dc:subject><dc:subject>\n         Markov chain<\/dc:subject><dc:subject>\n         Invariant measure<\/dc:subject><dc:subject>\n         Exponentially decaying tails.<\/dc:subject><dc:publisher>\n        Institute of Mathematical Statistics<\/dc:publisher><dc:source>\n        Annals of applied probability, 2006, Vol.16(3), pp.1059-1085 [Peer Reviewed Journal]<\/dc:source><dc:date>\n        2006-10-02<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:3182<\/dc:identifier><dc:identifier>\n        issn:1050-5164<\/dc:identifier><dc:identifier>\n        doi:10.1214\/105051606000000079<\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/3182\/<\/dc:identifier><dc:identifier>\n        https:\/\/doi.org\/10.1214\/105051606000000079<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/3182\/1\/3182.pdf<\/dc:identifier><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["issn:1050-5164","1050-5164"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2006,"topics":["Positive recurrent","Markov chain","Invariant measure","Exponentially decaying tails."],"subject":["Article","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n17 May 2010\nVersion of attached file:\nPublished Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nAndjel, A. and Menshikov, M. V. and Sisko, V. (2006) \u2019Positive recurrence of processes associated to crystal\ngrowth models.\u2019, Annals of applied probability., 16 (3). pp. 1059-1085.\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1214\/105051606000000079\nPublisher\u2019s copyright statement:\nAdditional information:\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\nThe Annals of Applied Probability\n2006, Vol. 16, No. 3, 1059\u20131085\nDOI: 10.1214\/105051606000000079\n\u00a9 Institute of Mathematical Statistics, 2006\nPOSITIVE RECURRENCE OF PROCESSES ASSOCIATED TO\nCRYSTAL GROWTH MODELS\nBY E. D. ANDJEL, M. V. MENSHIKOV1 AND V. V. SISKO2\nUniversit\u00e9 de Provence, University of Durham and Universidade de S\u00e3o Paulo\nWe show that certain Markov jump processes associated to crystal\ngrowth models are positive recurrent when the parameters satisfy a rather\nnatural condition.\n1. Introduction. Gates and Westcott studied some Markov processes repre-\nsenting crystal growth models. In these models particles accumulate on a finite set\nof sites. The first two theorems in the present paper study the model obtained when\nthis set of sites is one-dimensional. To define the process associated to that model,\nlet\nr(a, b, c) =\n\uf8f1\uf8f4\uf8f2\n\uf8f4\uf8f3\n\u03b22, if b < min{a, c},\n\u03b21, if min{a, c} \u2264 b < max{a, c},\n\u03b20, if max{a, c} \u2264 b,\nand for each n \u2265 2, let Xn(t) = (Xn1(t), . . . ,Xnn(t)) be a Markov jump process\non {Z+}n such that\nXni (t) \u2192 Xni (t) + 1, 1 \u2264 i \u2264 n,\nat rate\nr\n(\nXni\u22121(t),Xni (t),Xni+1(t)\n)\n.\nWhile these rates are well defined if 1 < i < n, their expression for i = 1 or i = n\ndepends on which boundary conditions we adopt. In this paper we will consider\ntwo boundary conditions:\n(a) Zero boundary conditions: Xn0(t) = Xnn+1(t) = 0 for all t \u2265 0.\n(b) Periodic boundary conditions: Xn0(t) = Xnn(t) and Xnn+1(t) = Xn1(t) for all\nt \u2265 0.\nWe now define for 1 \u2264 i \u2264 n\n\u0002iX\nn(t) = Xni (t) \u2212 Xni+1(t).\nReceived April 2005; revised November 2005.\n1Supported in part by FAPESP (2004\/13610\u20132).\n2Supported by FAPESP (2003\/00847\u20131).\nAMS 2000 subject classifications. Primary 60J20; secondary 60G55, 60K25, 60J10, 80A30.\nKey words and phrases. Positive recurrent, Markov chain, invariant measure, exponentially de-\ncaying tails.\n1059\n1060 E. D. ANDJEL, M. V. MENSHIKOV AND V. V. SISKO\nLet\nr\u02dc(u, v) =\n\uf8f1\uf8f4\uf8f2\n\uf8f4\uf8f3\n\u03b22, if min{u, v} > 0,\n\u03b21, if min{u, v} \u2264 0 < max{u, v},\n\u03b20, if max{u, v} \u2264 0.\nThen, r(a, b, c) = r\u02dc(a \u2212 b, c \u2212 b). Therefore, the process(\n\u00021X\nn(t), . . . ,\u0002nX\nn(t)\n)\nis Markovian too.\nGates and Westcott in [4] considered this process under periodic boundary con-\nditions and parameters \u03b2 such that\n0 < \u03b20 < \u03b21 < \u03b22.(1.1)\nThere, they proved that it is positive recurrent if either n \u2264 4 or\n(n \u2212 1)2\u03b20 < \u03b22.\nIn an earlier paper [3], the same authors studied the case in which the parameters \u03b2\nsatisfy (1.1) and \u03b21 = 12(\u03b20 + \u03b22). For this case, they gave for any n an explicit\nexpression for a stationary measure of the process, thus, proving its positive recur-\nrence.\nIn this paper we show that (1.1) is a sufficient condition for positive recurrence\nfor all values of n. More explicitly, we prove the following:\nTHEOREM 1.1. Let \u03b20, \u03b21 and \u03b22 satisfy (1.1). Then, for periodic boundary\nconditions and for any n \u2265 2, the process (\u00021Xn(t), . . . ,\u0002nXn(t)) is positive re-\ncurrent and its unique invariant measure has exponentially decaying tails.\nTHEOREM 1.2. Let \u03b20, \u03b21 and \u03b22 satisfy (1.1). Then, for zero boundary con-\nditions and for any n \u2265 2, the process (\u00021Xn(t), . . . ,\u0002n\u22121Xn(t)) is positive re-\ncurrent and its unique invariant measure has exponentially decaying tails.\nIn the statement of the last theorem the process considered is(\n\u00021X\nn(t), . . . ,\u0002n\u22121Xn(t)\n)\nrather than (\n\u00021X\nn(t), . . . ,\u0002nX\nn(t)\n)\n,\nbecause for zero boundary conditions, we clearly have limt \u0002nXn(t) = \u221e. Note\nthat the former process is Markovian too.\nIn Section 3 we prove that all the processes considered and starting from\n(0, . . . ,0) are such that\nsup\ni=1,...,n\u22121,\nt\u22650\nP\n(|\u0002iXn(t)| \u2265 k)\nCRYSTAL GROWTH MODELS 1061\ndecays exponentially in k. It is then easy to conclude that Theorems 1.1 and 1.2\nhold. The proof is first given for zero boundary conditions and relies on an induc-\ntion on n, some coupling arguments and Dol\u00e9ans exponential martingales.\nWhen \u03b21 = 12(\u03b20 + \u03b22), the invariant measure given in [3] is such that the dis-\ntribution of \u0002iXn is the same for all n. It is therefore natural to ask if, under\ncondition (1.1), there is a lower bound of the constant associated with the expo-\nnential decay of the invariant measure that is valid for all n. Unfortunately, our\nproof does not provide such a bound and its existence remains an open problem.\nIn Section 2 we prove the positive recurrence of a class of discrete time Markov\nchains. The precise result is stated in Theorem 1.3. A consequence of this theorem\nis that the processes considered by Theorems 1.1 and 1.2 are positive recurrent (see\nExamples 1 and 2 below). This proof does not give any information about the decay\nof the invariant measure, but we believe that many readers will be interested in it\nbecause it is much shorter than the proof given in Section 3 and because it treats\nmany models which are not covered by Theorems 1.1 and 1.2 (see Example 3).\nTo describe the class of Markov chains considered in Section 2, let G = (V ,E)\nbe a connected nonoriented finite graph, where V = {1, . . . , n}. The elements of V\nare seen as different columns, on which particles accumulate with time. A Markov\nchain \u03b6t , t \u2265 0 from that class will have as state space Y = Zn\u22121 and its transition\nprobability matrix will by denoted by Q. The ith coordinate of the chain represents\nthe difference between the number of particles in columns i and n.\nIn Theorem 1.3 we impose some conditions on Q, using the following func-\ntions:\nfij (y) =\n\uf8f1\uf8f4\uf8f2\n\uf8f4\uf8f3\ny(i)\u2212 y(j), if i < n, j < n,\ny(i), if i < n, j = n,\n\u2212y(j), if i = n, j < n,\nwhere y \u2208 Y and 1 \u2264 i, j \u2264 n. Hence, in all cases fij (y) is the algebraic difference\nbetween the number of particles in columns i and j .\nFor i = 1, . . . , n, let ei \u2208 Y be the vector that we have to add to an element of Y\nwhen column i increases by one unit, that is,\ne1 = (1,0, . . . ,0), . . . , en\u22121 = (0,0, . . . ,1), en = (\u22121, . . . ,\u22121,\u22121).\nWe now state our third theorem:\nTHEOREM 1.3. Consider a graph G = (V ,E) and a Markov chain \u03b6t with\nstate space Y = Zn\u22121. Let \u03b4 and M be constants such that \u03b4,M \u2208 (0,1). Suppose\nthat, for the transition probability matrix Q of \u03b6t , the following conditions hold:\n(i) Q(x,y) = 0 unless y = x or y = x + ei for some 1 \u2264 i \u2264 n.\n(ii) infy\u2208Y Q(y, y + ei) \u2265 \u03b4 for all 1 \u2264 i \u2264 n.\n(iii) Suppose that y \u2208 Y , {i, j} \u2208 E, and fij (y) > 0. Then\nQ(y,y + ei) \u2264 Q(y,y + ej ).\n1062 E. D. ANDJEL, M. V. MENSHIKOV AND V. V. SISKO\n(iv) Suppose that y \u2208 Y , i \u2208 V and for all \u0005 such that {i, \u0005} \u2208 E, we have\nfi\u0005(y) > 0. Then\nQ(y,y + ei) \u2264 Q(y,y + e\u0005) \u2212 M\nfor any \u0005 as above.\nThen, the Markov chain \u03b6t is positive recurrent.\nWe end this section with three examples of Markov chains to which this the-\norem can be applied. All these examples can be seen as crystal growth models.\nThe first and second examples treat the embedded chains appearing in Theorems\n1.2 and 1.1, respectively. The third example does not require the graph to have a\none-dimensional structure.\nEXAMPLE 1. Adopting the same notation as in Theorem 1.2, consider the\nprocess\nZ(t) = (Z1(t), . . . ,Zn\u22121(t)),\nwhere\nZi(t) =\nn\u2212i\u2211\nk=1\n\u0002n\u2212kXn(t), i = 1, . . . , n\u2212 1.\nThis process is Markovian because it is obtained applying a one to one map to\nanother Markovian process. The embedded Markov chain of the process Z(t) has\nas state space Y = Zn\u22121 and its probability transition matrix Q is such that, for all\ny \u2208 Y , we have Q(y, z) = 0 unless z = y + ei for some i = 1, . . . , n.\nNow, let G = (V ,E) be given by\nV = {1, . . . , n}, E = {{i, i + 1} : i = 1, . . . , n\u2212 1},\nthen, using (1.1), it is easy to check that this chain satisfies all the hypothesis of\nTheorem 1.3. Hence, it is positive recurrent. This implies that the process Z(t)\nis positive recurrent because its rates are bounded. Since this process is obtained\napplying a one to one map to the process(\n\u00021X\nn(t), . . . ,\u0002n\u22121Xn(t)\n)\n,\nthis last process is positive recurrent too.\nEXAMPLE 2. In this example (\u00021Xn(t), . . . ,\u0002nXn(t)) is as in Theorem 1.1.\nWe start noting that in this case\n\u2211n\ni=1 \u0002iXn(t) \u2261 0. Therefore, (\u00021Xn(t), . . . ,\n\u0002n\u22121Xn(t)) is also Markovian and it suffices to show that this last process is pos-\nitive recurrent. This is done almost exactly as in the previous example, the only\ndifference being that now\nE = {{i, i + 1} : i = 1, . . . , n\u2212 1}\u222a {1, n}.\nCRYSTAL GROWTH MODELS 1063\nEXAMPLE 3. Suppose that condition (1.1) is satisfied and that \u03b22 \u2264 1. Then,\nlet G = (V ,E) be any admissible graph, that is, it is nonoriented, finite and con-\nnected. As in the other examples, we take V = {1, . . . , n}. Then, define the matrix\nof transition probabilities as follows: for y \u2208 Y and i = 1, . . . , n, let\nQ(y,y + ei) =\n\uf8f1\uf8f4\uf8f2\n\uf8f4\uf8f3\n\u03b20\/n, if fi\u0005(y) > 0 for all \u0005 such that {i, \u0005} \u2208 E,\n\u03b22\/n, if fi\u0005(y) \u2264 0 for all \u0005 such that {i, \u0005} \u2208 E,\n\u03b21\/n, otherwise,\nand let\nQ(y,y) = 1 \u2212\nn\u2211\ni=1\nQ(y,y + ei).\nIt is now easy to see that the conditions of Theorem 1.3 are satisfied with M =\n(\u03b21 \u2212 \u03b20)\/n and \u03b4 = \u03b20\/n.\n2. Positive recurrence. To prove Theorem 1.3, we will apply the following\nvariation of Foster\u2019s theorem:\nTHEOREM 2.1. Let \u03b6t be an irreducible Markov chain on a countable state\nspace Y . Then, \u03b6t is positive recurrent if there exist a positive function f defined\non Y , a bounded strictly positive integer-valued function k also defined on Y and\na finite subset A of Y such that the following inequalities hold:\nE\n(\nf\n(\n\u03b6t+k(\u03b6t )\n)\u2212 f (\u03b6t )|\u03b6t = y) \u2264 \u22121, y \/\u2208 A,(2.1)\nE\n(\nf\n(\n\u03b6t+k(\u03b6t )\n)|\u03b6t = y)< \u221e, y \u2208 A.(2.2)\nThis theorem follows easily from Theorem 2.2.4 of [2]. Although in that refer-\nence the Markov chain is also assumed to be aperiodic, this extra condition is not\nneeded if we are only interested in positive recurrence instead of ergodicity.\nThroughout this section we adopt the same notation as in Theorem 1.3, assume\nits hypothesis and let\np = |E| (i.e., the cardinality of E).\nWe start defining the functions f and k to which we will apply Theorem 2.1. Let\nf (y) = \u2211\n{i,j}\u2208E\nf 2ij (y).\nWe need the following lemmas.\nLEMMA 2.1. For all {i, j} \u2208 E and all y \u2208 Y , we have\nE\n(\nf 2ij (\u03b6t+1) \u2212 f 2ij (\u03b6t )|\u03b6t = y\n)\u2264 1.\n1064 E. D. ANDJEL, M. V. MENSHIKOV AND V. V. SISKO\nPROOF. The lemma is a consequence of the following observations:\n(i) |fij (\u03b6t+1) \u2212 fij (\u03b6t )| \u2264 1 almost surely,\n(ii) from condition (iii) of Theorem 1.3, it follows that, for \u03b6t such that\n|fij (\u03b6t )| \u2265 1, we have\nP\n(|fij (\u03b6t+1)| = |fij (\u03b6t )| + 1)\u2264 P(|fij (\u03b6t+1)| = |fij (\u03b6t )| \u2212 1). \u0001\nLEMMA 2.2. Suppose C \u2265 1+p2M and let\nD = {y \u2208 Y : |fij (y)| > C, {i, j} \u2208 E}.\nThen inequality (2.1) holds for k \u2261 1 and any y \u2208 D.\nIn words, the set D is the set of all states of the Markov chain \u03b6t such that,\nfor every i, j \u2208 {1, . . . , n} such that i and j are neighbor columns, we have that\nthe absolute value of the difference between heights of columns i and j is greater\nthan C.\nPROOF OF LEMMA 2.2. For a given y \u2208 D, let u be in the set of the columns\nthat have the maximal number of particles and let v be such that {u, v} \u2208 E. We\nwrite\nE\n(\nf (\u03b6t+1) \u2212 f (\u03b6t )|\u03b6t = y)\n= E(f 2uv(\u03b6t+1) \u2212 f 2uv(\u03b6t )|\u03b6t = y)\n+ \u2211\n{i,j}\u2208E,\n{i,j}\b={u,v}\nE\n(\nf 2ij (\u03b6t+1) \u2212 f 2ij (\u03b6t )|\u03b6t = y\n)\n.\nIt is easy to see that fuv(y) = |fuv(y)| > C. From condition (iv) of Theorem 1.3, it\nfollows that the first term in the right-hand side is bounded above by \u22122CM + 1.\nBy Lemma 2.1, the second term is less than or equal to p \u2212 1. Putting the bounds\ntogether, we complete the proof. \u0001\nNote that the number of elements in the set Y \\ D is infinite. Therefore, we are\nnot ready yet to apply Theorem 2.1 and finish the proof of Theorem 1.3.\nBefore the formulation of the last lemma we need some notation. Let\n0 < C1 < C2 < \u00b7 \u00b7 \u00b7 < Cp < \u221e.\nThe values of these constants will be determined later. Let\nD0 = {y \u2208 Y : |fij (y)| < Cp, {i, j} \u2208 E},\nD1 = {y \u2208 Y : |fij (y)| \u2265 C1, {i, j} \u2208 E},\nDm = D\u2032m \u2229 D\u2032\u2032m, m = 2, . . . , p,\nCRYSTAL GROWTH MODELS 1065\nwhere\nD\u2032m =\n{\ny \u2208 Y : there exists {u, v} \u2208 E such that |fuv(y)| \u2265 Cm}\nand\nD\u2032\u2032m =\n{\ny \u2208 Y : for any {i, j} \u2208 E we have |fij (y)| \/\u2208 [Cm\u22121,Cm)}.\nSince E has p elements, we have\nY = D0 \u222a D1 \u222a D2 \u222a \u00b7 \u00b7 \u00b7 \u222a Dp.\nThe following lemma constitutes the main step in the proof of Theorem 1.3.\nLEMMA 2.3. There exist positive constants C1, . . . ,Cp and positive integers\nk1, . . . , kp such that, for every m \u2208 {1, . . . , p}, inequality (2.1) holds for y \u2208 Dm if\nk(y) = km.\nPROOF. We take an increasing sequence of integers Cm such that\nC1 \u2265 1 + p2M ,\nCm \u2265 max\n{\npCm\u22121,\n1 + p + p2Cm\u22121\nM\u03b4pCm\u22121\n}\n, m = 2, . . . , p,\nand let\nkm =\n{1, if m = 1,\n1 + pCm\u22121, if m = 2, . . . , p.\nRecall that the sets Dm depend on the constants Cm. By Lemma 2.2, inequal-\nity (2.1) holds for y \u2208 D1 if k(y) = k1.\nSuppose that y \u2208 Dm for some m = 2, . . . , p. Let \u0005 be in the set of the columns\nthat have the maximal number of particles. Consider the subgraph G\u02dc = (V , E\u02dc) of\nthe graph G = (V ,E), where\nE\u02dc = {{i, j} \u2208 E : |fij (y)| < Cm\u22121}.\nBy O denote the largest connected subgraph of G\u02dc containing \u0005.\nLet us prove that O \b= V . Assume the converse. Recall that |E| = p. Then, for\nany {i, j} \u2208 E, we have |fij (y)| < pCm\u22121 \u2264 Cm. This contradicts y \u2208 D\u2032m.\nAlso, since the graph G is connected, we see that there exists {u, v} \u2208 E such\nthat u \u2208 O , v \/\u2208 O , and |fuv(y)| \u2265 Cm\u22121. Since y \u2208 D\u2032\u2032m, we have |fuv(y)| \u2265 Cm.\nLet us prove that fuv(y) \u2265 Cm. Assume the converse. Then we have fuv(y) \u2264\n\u2212Cm, and therefore,\nf\u0005v(y) = f\u0005u(y) + fuv(y) \u2264 (p \u2212 1)Cm\u22121 \u2212 Cm < 0.\nThis contradicts the fact that the column \u0005 has the largest number of particles.\n1066 E. D. ANDJEL, M. V. MENSHIKOV AND V. V. SISKO\nSuppose that y\u02dc is obtained from y by adding pCm\u22121 particles to the column u.\nIt is easy to see that for y\u02dc the column u has at least one more particle than any\nother column.\nTo complete the proof of the lemma, write\nE\n(\nf\n(\n\u03b6t+km\n)\u2212 f (\u03b6t )|\u03b6t = y)\n= E(f (\u03b6t+km\u22121)\u2212 f (\u03b6t )|\u03b6t = y)\n+\u2211\nz\u2208Y\nE\n(\nf\n(\n\u03b6t+km\n)\u2212 f (\u03b6t+km\u22121)|\u03b6t+km\u22121 = z)Q(km\u22121)yz .\nBy Lemma 2.1, the first term of the right-hand side is bounded above by p(km\u22121).\nFor the second term write\u2211\nz\u2208Y\nE\n(\nf\n(\n\u03b6t+km\n)\u2212 f (\u03b6t+km\u22121)|\u03b6t+km\u22121 = z)Q(km\u22121)yz\n= \u2211\nz\u2208Y\n\u2211\n{i,j}\u2208E,\n{i,j}\b={u,v}\nE\n(\nf 2ij\n(\n\u03b6t+km\n)\u2212 f 2ij (\u03b6t+km\u22121)|\u03b6t+km\u22121 = z)Q(km\u22121)yz\n+\u2211\nz\u2208Y\nE\n(\nf 2uv\n(\n\u03b6t+km\n)\u2212 f 2uv(\u03b6t+km\u22121)|\u03b6t+km\u22121 = z)Q(km\u22121)yz ,\nwhere, by Lemma 2.1, the first term of the right-hand side is bounded above by\np \u2212 1, while the second term can be written as\u2211\nz\u2208Y,z \b=y\u02dc\nE\n(\nf 2uv\n(\n\u03b6t+km\n)\u2212 f 2uv(\u03b6t+km\u22121)|\u03b6t+km\u22121 = z)Q(km\u22121)yz\n+ E(f 2uv(\u03b6t+km)\u2212 f 2uv(\u03b6t+km\u22121)|\u03b6t+km\u22121 = y\u02dc)Q(km\u22121)yy\u02dc ,\nwhich, by Lemma 2.1 and conditions (iii) and (iv) of Theorem 1.3, is less than or\nequal to \u2211\nz\u2208Y,z \b=y\u02dc\nQ(km\u22121)yz + (1 \u2212 2CmM)Q(km\u22121)yy\u02dc \u2264 1 \u2212 2CmM\u03b4km\u22121.\nThe last inequality follows from condition (ii) of Theorem 1.3 and the fact that it\nis possible for \u03b6t to reach y\u02dc from y in km \u2212 1 steps.\nPutting these bounds together, we get\nE\n(\nf\n(\n\u03b6t+km\n)\u2212 f (\u03b6t )|\u03b6t = y)\u2264 p(km \u2212 1) + (p \u2212 1) + 1 \u2212 2CmM\u03b4km\u22121,\nwhich is less than or equal to \u22121 by our choice of the sequences km and Cm. \u0001\nWe can now complete the proof of Theorem 1.3: Let A = D0, which is finite.\nThen define the function k as follows: k(y) = 1 if y \u2208 D0 \u222a D1 and k(y) = km if\ny \u2208 Dm \\ (D0 \u222a D1 \u222a \u00b7 \u00b7 \u00b7 \u222a Dm\u22121), m = 2, . . . , p.\nCRYSTAL GROWTH MODELS 1067\nInequality (2.2) follows from the fact that the function k is bounded and the transi-\ntion matrix Q is such that, for any x, Q(x,y) = 0 except for finitely many values\nof y. Since inequality (2.1) follows from Lemma 2.3, we see that the hypotheses\nof Theorem 2.1 are fulfilled.\nREMARK. Theorem 1.3 is still valid if we replace condition (iv) by the fol-\nlowing condition:\n(iv\u2032) Suppose that y \u2208 Y , i \u2208 V and for all \u0005 such that {i, \u0005} \u2208 E, we have\nfi\u0005(y) < 0. Then\nQ(y,y + ei) \u2265 Q(y,y + e\u0005) + M\nfor any \u0005 as above.\nThe proof of the theorem requires the following minor modifications:\n(a) In the proof of Lemma 2.2 the column u is in the set of the columns that\nhave the minimal number of particles.\n(b) In the proof of Lemma 2.3 let \u0005 be one of the columns having the smallest\nnumber of particles. Then, fuv(y) \u2264 \u2212Cm. Suppose that y\u02dc is obtained from y\nby adding at least pCm\u22121 particles to each column that is a neighbor of u and\nbelongs to the set O . Then for y\u02dc the column u has at least one particle less than\nany neighbor column. Also, we take an increasing sequence of integers Cm such\nthat\nC1 \u2265 1 + p2M ,\nCm \u2265 max\n{\npCm\u22121,\n1 + p + p3Cm\u22121\nM\u03b4p\n2Cm\u22121\n}\n, m = 2, . . . , p,\nand let\nkm =\n{1, if m = 1,\n1 + p2Cm\u22121, if m = 2, . . . , p.\n3. Exponential decay of the invariant distribution. We start this section in-\ntroducing some notation.\nFor 0 \u2264 s \u2264 t and 1 \u2264 j \u2264 n, let\n\u0002\ns,t\nj (X\nn) = Xnj (t) \u2212 Xnj (s)\nand for 1 \u2264 j \u2264 n \u2212 1, let\nDj(X\nn(t)) = sup\n1\u2264i\u2264j\n\u0002iX\nn(t).\nLet PX be the probability associated to this process with initial condition X. If\nthis initial condition is Xn(0) = (0, . . . ,0), we will write P0.\n1068 E. D. ANDJEL, M. V. MENSHIKOV AND V. V. SISKO\nThroughout this section Ta, T \u2032b and T \u2032\u2032c will be independent Poisson r.v. with pa-\nrameters a, b and c, respectively, and Ci and Ki will be strictly positive constants.\nCoupling techniques will be often used in this section. These techniques allow\nus to compare two different processes or two versions of the same process starting\nfrom different initial configurations and we will assume that the reader is familiar\nwith them.\n3.1. Construction of the processes, coupling. The inductive argument used in\nthis section requires a simultaneous construction on the same probability space of\nthe processes for all values of n and for the two types of boundary conditions. This\nis done as follows:\nLet (N0,j ,N1,j ,N2,j ; j \u2208 N) be a collection of independent Poisson processes\nwhose parameters are \u03b20 for N0,j , \u03b21 \u2212 \u03b20 for N1,j and \u03b22 \u2212 \u03b21 for N2,j . We\nlet Xnj (\u00b7) increase by 1 at time t if one of the following conditions are satisfied:\n(1) r(Xnj\u22121(t\u2212),Xnj (t\u2212),Xnj+1(t\u2212)) = \u03b20 and N0,j jumps at time t ,\n(2) r(Xnj\u22121(t\u2212),Xnj (t\u2212),Xnj+1(t\u2212)) = \u03b21 and N0,j + N1,j jumps at time t ,\n(3) r(Xnj\u22121(t\u2212),Xnj (t\u2212),Xnj+1(t\u2212)) = \u03b22 and N0,j + N1,j + N2,j jumps at\ntime t .\nThe following lemma is an immediate consequence of our simultaneous con-\nstruction of all processes. It allows us to compare two processes with different and\nnot necessarily deterministic initial conditions.\nLEMMA 3.1. Suppose Xn(\u00b7) and X\u02dcn(\u00b7) are versions of the process with zero\nboundary conditions constructed as above (with the same Poisson processes):\n(a) If P(Xnj (0) \u2265 X\u02dcnj (0)) = 1 for all 1 \u2264 j \u2264 n, then\nP\n(\nXnj (t) \u2265 X\u02dcnj (t),\u2200 t \u2265 0\n)= 1 for all 1 \u2264 j \u2264 n.\n(b) If P(Xnj (0) \u2212 X\u02dcnj (0) = k) = 1 for all 1 \u2264 j \u2264 n, then\nP\n(\nXnj (t) \u2212 X\u02dcnj (t) = k,\u2200 t \u2265 0\n)= 1 for all 1 \u2264 j \u2264 n.\nREMARK. Suppose that j < n, t > 0 and let X \u2208 {Z+}n be such that\n\u0002jX \u2265 0. Let Xn(\u00b7) be the process with zero boundary conditions starting from X\nand let Xj(\u00b7) be the process with zero boundary conditions starting from the re-\nstriction of X to the first j coordinates. Then, with the above simultaneous con-\nstruction of Xn(\u00b7) and Xj(\u00b7), we have\n{\u0002jXn(s) \u2265 0,\u22000 \u2264 s \u2264 t} \u2282 {Xni (s) = Xji (s),\u22000 \u2264 s \u2264 t, i \u2264 j}.\nCRYSTAL GROWTH MODELS 1069\n3.2. An auxiliary process and some exponential martingales. We introduce an\nauxiliary process on {Z+}r \u00d7 {Z+}r \u00d7 {Z+}r+1, which we denote\n(Xr,Zr,Xr+1).\nThe first and third marginal of this process evolve as processes with zero boundary\nconditions in {Z+}r and in {Z+}r+1 respectively and their jumps follow the same\ncollection of Poisson processes (N0,j ,N1,j ,N2,j ; j \u2208 N). The second marginal\nperforms the same jumps as Xr and at any given time t , also increases all its\ncoordinates simultaneously by one unit when one of the following two conditions\nis satisfied:\n(i) Xr+1r+1(t\u2212) > Xr+1r (t\u2212), Xr+1r\u22121(t\u2212) \u2264 Xr+1r (t\u2212) and the process N1,rjumps at time t .\n(ii) Xr+1r+1(t\u2212) > Xr+1r (t\u2212), Xr+1r\u22121(t\u2212) > Xr+1r (t\u2212) and the process N2,rjumps at time t .\nWhen any of these conditions is satisfied, it may happen that the r th coordinate\nof the Xr process also jumps at time t . If that is the case, it is understood that\nthe Zr process performs both jumps. This means that Zrr increases by two units,\nwhile all the other coordinates of Zr increase by one unit.\nThe following lemma is a consequence from the construction of this process.\nLEMMA 3.2. Suppose the auxiliary process starts with all its 3r + 1 coordi-\nnates equal to 0, then\nP\n(\nXri (t) \u2264 Xr+1i (t) \u2264 Zri (t),\u22001 \u2264 i \u2264 r, t \u2265 0\n)= 1(3.1)\nand\nP\n(\nZri (t) \u2212 Xri (t) = Zr1(t) \u2212 Xr1(t),\u22002 \u2264 i \u2264 r, t \u2265 0\n)= 1.(3.2)\nPROOF. One just needs to check that if the initial condition is such that\nXri (0) \u2264 Xr+1i (0) \u2264 Zri (0) \u22001 \u2264 i \u2264 r\nand\nZri (0) \u2212 Xri (0) = Zr1(0) \u2212 Xr1(0) \u22002 \u2264 i \u2264 r,\nthen no jump of the process can break any of these inequalities. This is straight-\nforward except for the inequality involving the r th coordinate of Zr and Xr+1.\nHowever, since the coordinates of Xr+1 can only make jumps of one unit, we may\nassume that Xr+1r = Zrr and when this happens, conditions (i) and (ii) guarantee\nthat any jump of the r th coordinate of Xr+1 due to its (r + 1)st coordinate is\nsimultaneous to a jump of all the coordinates of Zr . \u0001\nWe will later need the following:\n1070 E. D. ANDJEL, M. V. MENSHIKOV AND V. V. SISKO\nLEMMA 3.3. Suppose the auxiliary process starts with all its 3r + 1 coordi-\nnates equal to 0, let\nus = r(Xr+1r\u22121(s),Xr+1r (s),Xr+1r+1(s))\u2212 r(Xr+1r\u22121(s),Xr+1r (s),0),\nand let\nvs = r(Xr+1r (s),Xr+1r+1(s),0),\nthen, for all \u03b1 \u2265 0,\n(1 + \u03b1)Zrr (t)\u2212Xrr (t) exp\n(\n\u2212\u03b1\n\u222b t\n0\nus ds\n)\n(3.3)\nand\n(1 + \u03b1)Xr+1r+1(t) exp\n(\n\u2212\u03b1\n\u222b t\n0\nvs ds\n)\n(3.4)\nare martingales.\nPROOF. We will apply Theorem T2 in page 165 of [1] and use as much as\npossible the notation of that reference. Consider the point process Zrr (t) \u2212 Xrr (t)\nand let Ft be the \u03c3 -algebra generated by the Poisson processes Ni,j , 0 \u2264 i \u2264 2,\n1 \u2264 j \u2264 r+1. Then, the intensity of this point process is theFt -predictable process\n\u03bbt = lims\u2191t us . Let \u00b5s \u2261 1 + \u03b1. Since \u03bb and \u00b5 are bounded, condition (2.1) in that\nreference is satisfied. It then follows from the referred theorem that\n(1 + \u03b1)Zrr (t)\u2212Xrr (t) exp\n(\n\u2212\u03b1\n\u222b t\n0\nus ds\n)\n(3.5)\nis a local martingale. Since for 0 \u2264 s \u2264 t , Zrr (s) \u2212 Xrr (s) is increasing, positive\nand bounded by a Poisson random variable of parameter (\u03b22 \u2212 \u03b20)t , it is also a\nmartingale. A similar argument shows that (3.4) is a martingale too. \u0001\n3.3. Zero boundary conditions. In this subsection we consider the process\nwith zero boundary conditions. In several parts of our proofs the following ob-\nservations will play an important role. Suppose n \u2208 N, 0 < i < n, and 0 \u2264 s < t ,\nthen\n{\u00021Xn(u) \u2265 0,\u2200u \u2208 [s, t]} \u2282 {\u0002s,t1 (Xn) = N0,1(t) \u2212 N0,1(s)}(3.6)\nand\n{\u0002iXn(u) > 0,\u2200u \u2208 [s, t]}\n\u2282 {\u0002s,ti+1(Xn) \u2265 N0,i+1(t) \u2212 N0,i+1(s)+ N1,i+1(t) \u2212 N1,i+1(s)}.\n(3.7)\nThe strategy of the proof is to proceed by induction in the number of coordi-\nnates. The initial lemma provides the result we need when we have two coordi-\nnates. The second lemma takes advantage of the boundary effects to show that\nthe first and last coordinates are unlikely to be much higher than there respec-\ntive neighbors. This provides the initial statement for the induction in index i per-\nformed in the proof of Theorem 3.1.\nCRYSTAL GROWTH MODELS 1071\nLEMMA 3.4. There exist \u03b12, \u03b32 > 0, and 0 < d2 < \u03b21 such that\nP0\n(|\u00021X2(t)| \u2265 k)\u2264 exp(\u2212\u03b12k) \u2200 k \u2208 N, t \u2265 0,\nand\nP0\n(\nX22(t) \u2265 d2t\n)\u2264 exp(\u2212\u03b32t) \u2200 t \u2265 0.\nPROOF. First note that |\u00021X2(t)| is a continuous time birth and death process\non Z+, such that:\n\u2022 0 \u2192 1 at rate 2\u03b20,\n\u2022 n \u2192 n + 1 at rate \u03b20 if n \u2265 1,\n\u2022 n \u2192 n \u2212 1 at rate \u03b21 if n \u2265 1.\nSince \u03b20 < \u03b21, this birth and death process is positive recurrent and its invariant\nmeasure has an exponentially decaying tail. It is then easy to couple two versions\nof this process, one starting from its invariant distribution and the other one from\nthe point mass at 0 in such a way that, with probability 1, the former is at all\ntimes above the latter. This proves the first assertion of the lemma. For the sec-\nond assertion, note that the process X21(t) + X22(t) increases by one unit at a rate\nwhich is bounded above by \u03b20 + \u03b21. Therefore, it can be coupled with a Poisson\nprocess Z(t) of parameter \u03b20 + \u03b21 in such a way that\nP\n(\nX21(t) + X22(t) \u2264 Z(t)\n)= 1.\nThe second assertion then follows from the first assertion, the inequality\n2X22(t) \u2264 X21(t) + X22(t) + |\u00021X2(t)|\nand standard large deviations estimates for Z(t). \u0001\nLEMMA 3.5. There exist C\u2032 and \u03b1\u2032 > 0 such that, for all n \u2208 N, k \u2208 N, and\nt \u2265 0, we have\nP0\n(\n\u00021X\nn(t) \u2265 k)\u2264 C\u2032 exp(\u2212\u03b1\u2032k)(3.8)\nand\nP0\n(\u2212\u0002n\u22121Xn(t) \u2265 k)\u2264 C\u2032 exp(\u2212\u03b1\u2032k).(3.9)\nPROOF. Since the process starting from X \u2261 0 is invariant under the map\n(x1, x2, . . . , xn\u22121, xn) \u2192 (xn, xn\u22121, . . . , x2, x1),\nunder P0 the random variables \u00021Xn(t) and \u2212\u0002n\u22121Xn(t) have the same distrib-\nution. Hence, it suffices to prove (3.8). We assume without loss of generality that\nk \u2265 4\u03b21, since this additional condition can be dropped adjusting the constants C\u2032\nand \u03b1\u2032. Moreover, we also assume that\nt >\nk\n2\u03b21\n(3.10)\n1072 E. D. ANDJEL, M. V. MENSHIKOV AND V. V. SISKO\nbecause when this inequality fails, we have\nP0\n(\n\u00021X\nn(t) \u2265 k)\u2264 P((N0,1 + N1,1)(t) \u2265 k)\u2264 P(Tk\/2 \u2265 k),\nwhich decays exponentially in k. Let\n\u03c4t = sup{s : 0 \u2264 s \u2264 t such that \u00021Xn(s) = 0},\nand let\n\u0005 = min{q \u2208 N : (t \u2212 q)\u03b21 \u2264 k\/2}.\nIt follows from this definition and our assumptions on k and t that\nt \u2212 \u0005 \u2265 1,\nand\n2(t \u2212 \u0005)\u03b21 \u2264 k \u2264 2(t \u2212 \u0005 + 1)\u03b21 \u2264 4(t \u2212 \u0005)\u03b21.(3.11)\nWe will show that both\nP0\n(\n\u00021X\nn(t) \u2265 k, \u03c4t > \u0005)\nand\nP0\n(\n\u00021X\nn(t) \u2265 k, \u03c4t \u2264 \u0005)\ndecay exponentially in k with constants which depend on \u03b20 and \u03b21 but not on t\nor n.\nFor the first of these terms, write\nP0\n(\n\u00021X\nn(t) \u2265 k, \u03c4t > \u0005)\n\u2264 P0(\u0002\u0005,t1 (Xn) \u2265 k)\n\u2264 P0((N0,1 + N1,1)(t) \u2212 (N0,1 + N1,1)(\u0005) \u2265 k)\n\u2264 P(T\u03b21(t\u2212\u0005) \u2265 k)\n\u2264 P(Tk\/2 \u2265 k),\nwhich decays exponentially in k.\nFor the second term, let m \u2264 \u0005 and note that\n{\u03c4t \u2208 [m \u2212 1,m),\u00021Xn(t) > 0} \u2282 {\u00021Xn(s) > 0,\u2200 s \u2208 [m, t]}.\nHence, we deduce from (3.6) that on {\u03c4t \u2208 [m \u2212 1,m),\u00021Xn(t) > 0} we have\n\u0002\nm\u22121,t\n1 (X\nn) = \u0002m\u22121,m1 (Xn) + \u0002m,t1 (Xn)\n\u2264 N0,1(t) \u2212 N0,1(m) + (N0,1 + N1,1)(m)\n\u2212 (N0,1 + N1,1)(m \u2212 1),\nCRYSTAL GROWTH MODELS 1073\nand from (3.7) that\n\u0002\nm,t\n2 (X\nn) \u2265 (N0,2 + N1,2)(t) \u2212 (N0,2 + N1,2)(m).\nSince on {\u00021Xn(t) > 0, \u03c4t \u2208 [m\u2212 1,m)} we also have\n\u0002\nm\u22121,t\n1 (X\nn) > \u0002\nm,t\n2 (X\nn),\nwe can write\nP0\n(\n\u00021X\nn(t) \u2265 k, \u03c4t \u2208 [m \u2212 1,m))\n\u2264 P0(\u00021Xn(t) > 0, \u03c4t \u2208 [m \u2212 1,m))\n\u2264 P(N0,1(t) \u2212 N0,1(m) + (N0,1 + N1,1)(m) \u2212 (N0,1 + N1,1)(m \u2212 1)\n\u2265 (N0,2 + N1,2)(t) \u2212 (N0,2 + N1,2)(m))\n= P(T\u03b20(t\u2212m)+\u03b20+\u03b21 \u2265 T \u2032\u03b21(t\u2212m)\n)\n,\nwhich decays exponentially in t \u2212 m. Adding this over 1 \u2264 m \u2264 \u0005, we obtain that\nP0\n(\n\u00021X\nn(t) \u2265 k, \u03c4t \u2264 \u0005)\ndecays exponentially in t \u2212 \u0005. The lemma now follows from (3.11). \u0001\nWe will now state and prove the main result of this section. Theorem 1.2 follows\nimmediately from it.\nTHEOREM 3.1. For all n \u2265 2, there exist \u03b1n > 0 and an such that\nP0\n(|\u0002iXn(t)| \u2265 k)\u2264 an exp(\u2212\u03b1nk) \u2200 t \u2265 0,1 \u2264 i \u2264 n \u2212 1, k \u2208 N,(3.12)\nand there exist \u03b3n > 0, bn, and 0 < dn < \u03b21 such that\nP0\n(\nXnn(t) \u2265 dnt\n)\u2264 bn exp(\u2212\u03b3nt) \u2200 t \u2265 0.(3.13)\nThe second inequality of the conclusion in this theorem says that the coordi-\nnates next to the boundary grow at a speed which is strictly smaller than \u03b21. This\nwill play an important role in the inductive step: In time intervals on which co-\nordinate r remains higher than coordinate r + 1, the first r coordinates behave as\na process with r coordinates and zero boundary conditions. Hence, coordinate r\ngrows at a rate which is strictly smaller than \u03b21, while coordinate r + 1 grows at\nleast at that rate. This implies that coordinate r is unlikely to remain higher than\ncoordinate r + 1 for a long time.\nPROOF OF THEOREM 3.1. We proceed by induction on n. For n = 2, (3.12)\nand (3.13) follow from Lemma 3.4. Suppose that (3.12) and (3.13) hold for n \u2208\n{1, . . . , r}.\n1074 E. D. ANDJEL, M. V. MENSHIKOV AND V. V. SISKO\nWe start proving the following statement: For 1 \u2264 i \u2264 r , there exists \u03b1\u2032i > 0\nand c\u2032i such that\nP0\n(\n\u0002iX\nr+1(t) \u2265 k)\u2264 c\u2032i exp(\u2212\u03b1\u2032ik) \u2200 t \u2265 0, k \u2208 N.(3.14)\nThe proof of (3.14) is done by induction on i. For i = 1, (3.14) holds by\nLemma 3.5. We now suppose that (3.14) holds for i \u2208 {1, . . . , j \u2212 1}, where\n1 \u2264 j \u2212 1 \u2264 r \u2212 1. Note that, for these values of j , by the inductive hypothesis\non n, (3.13) holds for n = j . Let t \u2265 1, let\n\u03c4t = sup{s : 0 \u2264 s \u2264 t such that \u0002jXr+1(s) = 0},\n\u0005 \u2208 N \u2229 [0, t], k \u2208 N, and let L be large enough to satisfy\ndj + (r + 1)\/L < \u03b21\n[here dj is the constant in (3.13) with n = j ]. Then,\nP0\n(\n\u0002jX\nr+1(t) > 0, \u03c4t \u2208 [\u0005 \u2212 1, \u0005))\n\u2264 P0(\u0002jXr+1(s) > 0,\u2200 s \u2208 [\u0005, t],Dj (Xr+1(\u0005)) \u2264 (t \u2212 \u0005)\/L)\n+ P0(\u0002jXr+1(\u0005) > (t \u2212 \u0005)\/L, \u03c4t \u2208 [\u0005 \u2212 1, \u0005))\n+ P0(Dj\u22121(Xr+1(\u0005)) > (t \u2212 \u0005)\/L).\n(3.15)\nWe will now show that each of the three terms of the right-hand side above is\nbounded above by expressions of the form C exp(\u2212K(t \u2212 \u0005)).\nFor the third term, this follows by the inductive hypothesis on i.\nFor the the second term, note that\n{\u0002jXr+1(\u0005) > (t \u2212 \u0005)\/L, \u03c4t \u2208 [\u0005 \u2212 1, \u0005)} \u2282 {\u0002\u0005\u22121,\u0005j (Xr+1) \u2265 (t \u2212 \u0005)\/L}.\nTherefore,\nP0\n(\n\u0002jX\nr+1(\u0005) > (t \u2212 \u0005)\/L, \u03c4t \u2208 [\u0005 \u2212 1, \u0005))\n\u2264 P((N0,j + N1,j + N2,j )(\u0005) \u2212 (N0,j + N1,j + N2,j )(\u0005 \u2212 1) > (t \u2212 \u0005)\/L)\n= P(T\u03b22 > (t \u2212 \u0005)\/L),\nwhich decays exponentially in t \u2212 \u0005.\nFor the first term, write\n{\u0002jXr+1(s) > 0,\u2200 s \u2208 [\u0005, t],Dj (Xr+1(\u0005)) \u2264 (t \u2212 \u0005)\/L}\n\u2282 {\u0002jXr+1(t) > 0,Dj (Xr+1(\u0005)) \u2264 (t \u2212 \u0005)\/L}\n\u2282 {\u0002jXr+1(t) > 0,\u0002jXr+1(\u0005) \u2264 (t \u2212 \u0005)\/L}\n\u2282 {\u0002\u0005,tj (Xr+1) > \u0002\u0005,tj+1(Xr+1) \u2212 (t \u2212 \u0005)\/L}\n\u2282 {\u0002\u0005,tj (Xr+1) > (dj + r\/L)(t \u2212 \u0005)}\n\u222a {\u0002\u0005,tj+1(Xr+1) < (dj + (r + 1)\/L)(t \u2212 \u0005)}.\nCRYSTAL GROWTH MODELS 1075\nHence,\n{\u0002jXr+1(s) > 0,\u2200 s \u2208 [\u0005, t],Dj (Xr+1(\u0005)) \u2264 (t \u2212 \u0005)\/L}\n\u2282 {Dj(Xr+1(\u0005)) \u2264 (t \u2212 \u0005)\/L}\n\u2229 ({\u0002\u0005,tj (Xr+1) > (dj + r\/L)(t \u2212 \u0005),\u0002jXr+1(s) > 0,\u2200 s \u2208 [\u0005, t]}\n\u222a {\u0002\u0005,tj+1(Xr+1) < (dj + (r + 1)\/L)(t \u2212 \u0005),\u0002jXr+1(s) > 0,\n\u2200 s \u2208 [\u0005, t]}).\nTherefore, applying the Markov property at time \u0005 and letting\nAm,j = {X \u2208 {Z+}m :Dj(X) \u2264 (t \u2212 \u0005)\/L},\nwhere j < m \u2208 N, we get\nP0\n(\n\u0002jX\nr+1(s) > 0,\u2200 s \u2208 [\u0005, t],Dj (Xr+1(\u0005)) \u2264 (t \u2212 \u0005)\/L)\n\u2264 sup\nX\u2208Ar+1,j\nPX\n(\n\u0002\n0,t\u2212\u0005\nj (X\nr+1) > (dj + r\/L)(t \u2212 \u0005),\n\u0002jX\nr+1(s) > 0,\u2200 s \u2208 [0, t \u2212 \u0005])(3.16)\n+ sup\nX\u2208Ar+1,j\nPX\n(\n\u0002\n0,t\u2212\u0005\nj+1 (X\nr+1) <\n(\ndj + (r + 1)\/L)(t \u2212 \u0005),\n\u0002jX\nr+1(s) > 0,\u2200 s \u2208 [0, t \u2212 \u0005]).\nFrom the remark following Lemma 3.1, we get that the first term of the right-\nhand side of (3.16) is bounded by\nsup\nX\u2208Aj,j\u22121\nPX\n(\n\u0002\n0,t\u2212\u0005\nj (X\nj ) > (dj + r\/L)(t \u2212 \u0005)),\nwhile, by (3.7), the second term is bounded above by\nP\n(\n(N0,j+1 + N1,j+1)(t \u2212 \u0005) \u2264 (dj + (r + 1)\/L)(t \u2212 \u0005)).\nFor X \u2208 Aj,j\u22121, let X\u2032 be the element of {Z+}j whose coordinates are all equal\nto max{X1, . . . ,Xj }. Note that the value of these coordinates is bounded above by\nXj + (j \u22121)(t \u2212 \u0005)\/L. Therefore, by parts (a) and (b) of Lemma 3.1, the first term\nis bounded above by\nPX\u2032\n(\n\u0002\n0,t\u2212\u0005\nj (X\nj ) >\n(\ndj + r\/L \u2212 (j \u2212 1)\/L)(t \u2212 \u0005))\n= P0(\u00020,t\u2212\u0005j (Xj ) > (dj + r\/L \u2212 (j \u2212 1)\/L)(t \u2212 \u0005))\n\u2264 P0(\u00020,t\u2212\u0005j (Xj ) > dj (t \u2212 \u0005))\n= P0(Xjj (t \u2212 \u0005) > dj (t \u2212 \u0005))\n\u2264 bj exp(\u2212\u03b3j (t \u2212 \u0005)),\n1076 E. D. ANDJEL, M. V. MENSHIKOV AND V. V. SISKO\nwhere the last inequality follows from the inductive hypothesis on n (recall that\nj \u2264 r). Since the Poisson process N0,j+1 + N1,j+1 has parameter \u03b21 > dj + (r +\n1)\/L, we get that the second term of the right-hand side of (3.16) also decays\nexponentially in t \u2212 \u0005. This completes the proof that the first term of the right-\nhand side of (3.15) decays exponentially.\nSince the three terms of the right-hand side of (3.15) decay exponentially in\nt \u2212 \u0005, we have proved that there exist constants C1 and K1 > 0 such that\nP0\n(\n\u0002jX\nr+1(t) > 0, \u03c4t \u2208 [\u0005 \u2212 1, \u0005))\u2264 C1 exp(\u2212K1(t \u2212 \u0005))\nholds for all t \u2265 1, \u0005 \u2264 t , and \u0005 \u2208 N. Therefore, substituting \u0005\u2032 for \u0005 and summing\non 1 \u2264 \u0005\u2032 \u2264 \u0005, we get\nP0\n(\n\u0002jX\nr+1(t) > 0, \u03c4t \u2264 \u0005)\n\u2264 C2 exp(\u2212K2(t \u2212 \u0005)) \u2200 t \u2265 1, \u0005 \u2264 t, \u0005 \u2208 N.(3.17)\nWe will now complete the inductive step for (3.14). We may assume that\nt \u2265 k\n2\u03b22\n,(3.18)\nsince otherwise\nP0\n(\n\u0002jX\nr+1(t) \u2265 k)\n\u2264 P((N0,j + N1,j + N2,j )(t) \u2265 k)\n\u2264 P(Tk\/2 \u2265 k),\nwhich decays exponentially in k with a constant which does not depend on t . And\narguing as in the proof of Lemma 3.5, we may also assume that k > 4\u03b22. Let\n\u0005 = min{q \u2208 N : (t \u2212 q)\u03b22 \u2264 k\/2}.\nThen, it follows from our assumptions on k and t that\nt \u2212 \u0005 \u2265 1(3.19)\nand\nk \u2264 2(t \u2212 \u0005 + 1)\u03b22 \u2264 4(t \u2212 \u0005)\u03b22.(3.20)\nNow write\nP0\n(\n\u0002jX\nr+1(t) \u2265 k)\n\u2264 P0(\u0002jXr+1(t) > 0, \u03c4t \u2264 \u0005)\n+ P0(\u0002jXr+1(t) \u2265 k, \u03c4t > \u0005).\nBy (3.17), the first term is bounded above by\nC2 exp\n(\u2212K2(t \u2212 \u0005))\u2264 C2 exp\n(\n\u2212 K2\n4\u03b22\nk\n)\n,\nCRYSTAL GROWTH MODELS 1077\nand the second term is bounded above by\nP0\n(\n\u0002\n\u0005,t\nj (X\nr+1) \u2265 k)\u2264 P(T\u03b22(t\u2212\u0005) \u2265 k)\u2264 P(Tk\/2 \u2265 k).\nTherefore, both terms decay exponentially in k with constants which do not depend\non t . Thus, (3.14) holds for i = j . Hence, by induction, it holds for all i, and by\nsymmetry, it follows that (3.12) holds for n = r + 1. It remains to prove that (3.13)\nalso holds for that n.\nLet us and vs be as in Lemma 3.3. Then, for all s \u2265 0, we have\n0 \u2264 us \u2264 max{\u03b22 \u2212 \u03b21, \u03b21 \u2212 \u03b20} \u2264 \u03b22 \u2212 \u03b20, 0 \u2264 vs \u2264 \u03b21,\nand that us > 0 implies Xr+1r+1(s) > Xr+1r (s), which in turn implies vs = \u03b20. Hence,\nif 0 \u2264 \u03b4 \u2264 1 and t \u2265 0, then either\u222b t\n0\nus ds \u2264 (\u03b22 \u2212 \u03b20)\u03b4t\nor \u222b t\n0\nvs ds \u2264 [\u03b20\u03b4 + \u03b21(1 \u2212 \u03b4)]t.\nLet\n\u03b3 \u2208 (0, \u03b21 \u2212 dr),\nthen pick \u03b4 \u2208 (0,1) and \u03b7 > 0 such that\n\u03b3 \u2212 \u03b4(\u03b22 \u2212 \u03b20) > 0, \u03b7 + \u03b3 < \u03b21 \u2212 dr\nand\n\u03b7 < \u03b4(\u03b21 \u2212 \u03b20).\nLet (Xr,Zr,Xr+1) be the auxiliary process defined in Section 3.2 and let P0 be the\nprobability associated to this process when it starts with all its coordinates equal\nto 0. Then, write\nP0\n(\nZrr (t) \u2212 Xrr (t) \u2265 \u03b3 t,Xr+1r+1(t) \u2265 (\u03b21 \u2212 \u03b7)t\n)\n\u2264 P0\n(\nZrr (t) \u2212 Xrr (t) \u2265 \u03b3 t,\n\u222b t\n0\nus ds \u2264 (\u03b22 \u2212 \u03b20)\u03b4t\n)\n+ P0\n(\nXr+1r+1(t) \u2265 (\u03b21 \u2212 \u03b7)t,\n\u222b t\n0\nvs ds \u2264 \u03b20\u03b4 + \u03b21(1 \u2212 \u03b4)t\n)\n\u2264 P0\n(\nZrr (t) \u2212 Xrr (t) \u2212\n\u222b t\n0\nus ds \u2265 \u03b3 \u2212 \u03b4(\u03b22 \u2212 \u03b20)t\n)\n+ P0\n(\nXr+1r+1(t) \u2212\n\u222b t\n0\nvs ds \u2265 [\u03b4(\u03b21 \u2212 \u03b20) \u2212 \u03b7]t\n)\n.\n1078 E. D. ANDJEL, M. V. MENSHIKOV AND V. V. SISKO\nLet\nc = min{\u03b3 \u2212 \u03b4(\u03b22 \u2212 \u03b20), \u03b4(\u03b21 \u2212 \u03b20) \u2212 \u03b7},\nthen c > 0 and\nP0\n(\nZrr (t) \u2212 Xrr (t) \u2265 \u03b3 t,Xr+1r+1(t) \u2265 (\u03b21 \u2212 \u03b7)t\n)\n\u2264 P0\n(\nZrr (t) \u2212 Xrr (t) \u2212\n\u222b t\n0\nus ds \u2265 ct\n)\n+ P0\n(\nXr+1r+1(t) \u2212\n\u222b t\n0\nvs ds \u2265 ct\n)\n.\n(3.21)\nLet \u03c1 > 0 be such that\n\u03b5 =: (1 \u2212 \u03c1)c \u2212 \u03c1\u03b22 > 0\nand let \u03b1 > 0 be such that\n\u03b1(1 \u2212 \u03c1) \u2264 ln(1 + \u03b1).\nFrom Lemma 3.3, we have\nE\n(\nexp\n[\nln(1 + \u03b1)(Zrr (t) \u2212 Xrr (t))\u2212 \u03b1\n\u222b t\n0\nus ds\n])\n= 1.\nTherefore,\nE\n(\nexp\n[\n\u03b1\n(\n(1 \u2212 \u03c1)(Zrr (t) \u2212 Xrr (t))\u2212 \u03b1\n\u222b t\n0\nus ds\n)])\n\u2264 1,\nwhich, by Chebyshev\u2019s inequality, implies that\nP\n(\n(1 \u2212 \u03c1)(Zrr (t) \u2212 Xrr (t))\u2212\n\u222b t\n0\nus ds \u2265 a\n)\n\u2264 exp(\u2212\u03b1a) \u2200a > 0.\nHence,\nP0\n((\nZrr (t) \u2212 Xrr (t)\n)\u2212\n\u222b t\n0\nus ds \u2265 ct\n)\n= P0\n(\n(1 \u2212 \u03c1)(Zrr (t) \u2212 Xrr (t))\u2212\n\u222b t\n0\nus ds \u2265 (1 \u2212 \u03c1)ct \u2212 \u03c1\n\u222b t\n0\nus ds\n)\n\u2264 P0\n(\n(1 \u2212 \u03c1)(Zrr (t) \u2212 Xrr (t))\u2212\n\u222b t\n0\nus ds \u2265 ((1 \u2212 \u03c1)c \u2212 \u03c1\u03b22)t\n)\n\u2264 exp(\u2212\u03b1\u03b5t).\n(3.22)\nUsing the second martingale provided by Lemma 3.3,\n(1 + \u03b1)Xr+1r+1(t) exp\n(\n\u2212\u03b1\n\u222b t\n0\nvs ds\n)\nCRYSTAL GROWTH MODELS 1079\nand proceeding as above, we get\nP0\n(\nXr+1r+1(t) \u2212\n\u222b t\n0\nvs ds \u2265 ct\n)\n\u2264 exp(\u2212\u03b1\u03b5\u2032t),(3.23)\nwhere \u03b5\u2032 = (1 \u2212 \u03c1)c \u2212 \u03c1\u03b21 > \u03b5. It now follows from (3.21), (3.22) and (3.23) that\nP0\n(\nZrr (t) \u2212 Xrr (t) \u2265 \u03b3 t,Xr+1r+1(t) \u2265 (\u03b21 \u2212 \u03b7)t\n)\u2264 2 exp(\u2212\u03b1\u03b5t) \u2200 t \u2265 0.\nHowever, by the inductive hypothesis on n, we have\nP0\n(\nXrr (t) \u2265 dr t\n)\u2264 bn exp(\u2212\u03b3r t),\ntherefore, there exist C6 and K6 > 0 such that\nP0\n(\nZrr (t) \u2265 (dr + \u03b3 )t,Xr+1r+1(t) \u2265 (\u03b21 \u2212 \u03b7)t\n)\n\u2264 C6 exp(\u2212K6t) \u2200 t \u2265 0.\n(3.24)\nSince Xr+1r (t) \u2264 Zrr (t), we have\nP0\n(\nZrr (t) \u2264 (dr + \u03b3 )t,Xr+1r+1(t) \u2265 (\u03b21 \u2212 \u03b7)t\n)\n\u2264 P0(Xr+1r (t) \u2264 (dr + \u03b3 )t,Xr+1r+1(t) \u2265 (\u03b21 \u2212 \u03b7)t)\n\u2264 P0(Xr+1r+1(t) \u2212 Xr+1r (t) \u2265 (\u03b21 \u2212 \u03b7 \u2212 \u03b3 \u2212 dr)t).\nSince \u03b21 \u2212\u03b7\u2212\u03b3 \u2212dr > 0, it follows from Lemma 3.5 that there exist constants C7\nand K7 > 0 such that\nP0\n(\nZrr (t) \u2264 (dr + \u03b3 )t,Xr+1r+1(t) \u2265 (\u03b21 \u2212 \u03b7)t\n)\n\u2264 C7 exp(\u2212K7t) \u2200 t \u2265 0.\n(3.25)\nBy (3.24) and (3.25), we get C8 and K8 > 0 such that\nP0\n(\nXr+1r+1(t) \u2265 (\u03b21 \u2212 \u03b7)t\n)\u2264 C8 exp(\u2212K8t) \u2200 t \u2265 0.\nHence, (3.13) holds for n = r + 1 and the induction in n is complete. \u0001\n3.4. Periodic boundary conditions. We denote by\nYn(t) = (Yn1 (t), . . . , Y nn (t))\nthe process with periodic boundary conditions on {Z+}n.\nFor this process, we define for 1 \u2264 i \u2264 n\n\u0002iY\nn(t) = Yni (t) \u2212 Yni+1(t),\nwhere, by convention, Ynn+1(t) = Yn1 (t). Note that the process(\n\u00021Y\nn(t), . . . ,\u0002nY\nn(t)\n)\nis Markovian too and\n\u2211n\ni=1 \u0002iYn(t) = 0.\n1080 E. D. ANDJEL, M. V. MENSHIKOV AND V. V. SISKO\nFor 0 \u2264 s \u2264 t and 1 \u2264 j \u2264 n, let\n\u0002\ns,t\nj (Y\nn) = Ynj (t) \u2212 Ynj (s).\nSince\n\u2211n\ni=1 \u0002iYn(t) = 0 and the semigroup of the process commutes with the\nmap\n(y1, y2, . . . , yn) \u2192 (y2, . . . , yn, y1),\nTheorem 1.1 follows from the following:\nTHEOREM 3.2. For all 1 \u2264 i \u2264 n\u2212 1, there exist positive C\u00afi and K\u00afi > 0 such\nthat\nP0\n(\nmin{\u2212\u0002nYn(t),\u0002iY n(t)} \u2265 k)\u2264 C\u00afi exp(\u2212K\u00afik) \u2200 t \u2265 0, k \u2208 N.\nThe idea of the proof is that as long as \u2212\u0002n and \u0002i are positive, coordinates\n1, . . . , i behave as a process with zero boundary conditions. By Theorem 3.1, they\ngrow at a rate which is strictly smaller than \u03b21. But coordinates i+1 and n grow at\nleast at that rate. Therefore, \u2212\u0002n and \u0002i are unlikely to remain positive too long.\nPROOF OF THEOREM 3.2. As in the proof of Theorem 3.1, it suffices to prove\nthe result under the additional assumptions k \u2265 4\u03b22 and t > k2\u03b22 . We proceed by\ninduction on i.\nFirst step. i = 1. Let\n\u03c4t = sup{s \u2208 [0, t] :\u0002nYn(s) = 0 or \u00021Yn(s) = 0},\nand let\n\u0005 = min{q \u2208 N : (t \u2212 q)\u03b22 \u2264 k\/2}.\nThen\nP0\n(\nmin{\u2212\u0002nYn(t),\u00021Yn(t)} \u2265 k, \u03c4t \u2265 \u0005)\n\u2264 P((N0,1 + N1,1 + N2,1)(t) \u2212 (N0,1 + N1,1 + N2,1)(\u0005) \u2265 k)\n= P(T\u03b22(t\u2212\u0005) \u2265 k)\u2264 P(Tk\/2 \u2265 k)\n\u2264 C5 exp(\u2212K5k).\n(3.26)\nFor m \u2264 \u0005, write\nP0\n(\nmin{\u2212\u0002nYn(t),\u00021Yn(t)} \u2265 k, \u03c4t \u2208 [m\u2212 1,m))\n\u2264 P0(min{\u2212\u0002nYn(s),\u00021Yn(s)} > 0,\u2200 s \u2208 [m, t], \u03c4t \u2208 [m \u2212 1,m)).\nBut on the set{\nmin{\u2212\u0002nYn(s),\u00021Yn(s)} > 0,\u2200 s \u2208 [m, t], \u03c4t \u2208 [m\u2212 1,m)}\nCRYSTAL GROWTH MODELS 1081\nwe have\nN0,1(t) \u2212 N0,1(m \u2212 1) \u2265 \u0002\u03c4t ,t1 (Y n) \u2265 min{\u0002\u03c4t ,t2 (Y n),\u0002\u03c4t ,tn (Y n)}\n\u2265 min{\u0002m,t2 (Y n),\u0002m,tn (Y n)}.\nSince on that set we also have\n\u0002\n\u03c4t ,t\n2 (Y\nn) \u2265 (N0,2 + N1,2)(t) \u2212 (N0,2 + N1,2)(m)\nand\n\u0002\u03c4t ,tn (Y\nn) \u2265 (N0,n + N1,n)(t) \u2212 (N0,n + N1,n)(m),\nwe get\nP0\n(\nmin{\u2212\u0002nYn(t),\u00021Yn(t)} \u2265 k, \u03c4t \u2208 [m\u2212 1,m))\n\u2264 P(N0,1(t) \u2212 N0,1(m \u2212 1)\n\u2265 [(N0,2 + N1,2)(t) \u2212 (N0,2 + N1,2)(m)]\n\u2227 [(N0,n + N1,n)(t) \u2212 (N0,n + N1,n)(m)])\n= P(T\u03b20(t\u2212m+1) > T \u2032\u03b21(t\u2212m) \u2227 T \u2032\u2032\u03b21(t\u2212m)\n)\n\u2264 2 P(T\u03b20(t\u2212m+1) > T \u2032\u03b21(t\u2212m)\n)\n.\nAs \u03b20 < \u03b21, this decays exponentially in t \u2212 m. Thus, for some constants C6 and\nK6 > 0, we have\nP0\n(\nmin{\u2212\u0002nYn(t),\u00021Yn(t)} \u2265 k, \u03c4t \u2208 [m \u2212 1,m))\n\u2264 C6 exp(\u2212K6(t \u2212 m)).\nSumming over 1 \u2264 m \u2264 \u0005, we get\nP0\n(\nmin{\u2212\u0002nYn(t),\u00021Yn(t)} \u2265 k, \u03c4t < \u0005)\u2264 C7 exp(\u2212K7(t \u2212 \u0005)).\nThis, (3.20) and (3.26) imply the result for i = 1.\nSecond step. Suppose the result holds for 1 \u2264 i \u2264 r \u2212 1 < n \u2212 1. Now let\n\u03c4t = sup{s \u2208 [0, t] :\u0002nYn(t) = 0 or \u0002rYn(t) = 0}\nand as in the first step, let\n\u0005 = min{q \u2208 N : (t \u2212 q)\u03b22 \u2264 k\/2}.\nThen\nP0\n(\nmin{\u2212\u0002nYn(t),\u0002rY n(t)} \u2265 k, \u03c4t \u2265 \u0005)\n\u2264 P((N0,1 + N1,1 + N2,1)(t) \u2212 (N0,1 + N1,1 + N2,1)(\u0005) \u2265 k)\n+ P((N0,r + N1,r + N2,r )(t) \u2212 (N0,r + N1,r + N2,r )(\u0005) \u2265 k)\n\u2264 2 P(T\u03b22(t\u2212\u0005) \u2265 k)\u2264 2 P(Tk\/2 \u2265 k)\n\u2264 C8 exp(\u2212K8k).\n(3.27)\n1082 E. D. ANDJEL, M. V. MENSHIKOV AND V. V. SISKO\nLet m \u2208 N be less than or equal to \u0005. We wish to show that\nP0\n(\nmin{\u2212\u0002nYn(t),\u0002rY n(t)} \u2265 k, \u03c4t \u2208 [m \u2212 1,m))\ndecays exponentially in t \u2212 m. To do so, first note that if\nmin{\u2212\u0002nYn(t),\u0002rY n(t)} \u2265 k,\nthen either \u2212\u0002nYn(\u03c4t ) = 1 or \u0002rYn(\u03c4t ) = 1. Therefore,\nP0\n(\nmin{\u2212\u0002nYn(t),\u0002rY n(t)} \u2265 k, \u03c4t \u2208 [m \u2212 1,m))\n\u2264 P0(min{\u2212\u0002nYn(t),\u0002rY n(t)} \u2265 k, \u03c4t \u2208 [m \u2212 1,m),\u2212\u0002nYn(\u03c4t ) = 1)\n+ P0(min{\u2212\u0002nYn(t),\u0002rY n(t)} \u2265 k, \u03c4t \u2208 [m \u2212 1,m),\u0002rY n(\u03c4t ) = 1)\n= 2 P0(min{\u2212\u0002nYn(t),\u0002rY n(t)} \u2265 k, \u03c4t \u2208 [m\u2212 1,m),\u0002rY n(\u03c4t ) = 1),\nwhere the equality follows from the facts that the process is invariant under the\nmap\n(y1, . . . , yn) \u2192 (yr , yr\u22121, . . . , y1, yn, yn\u22121, . . . , yr+1)\nand the initial configuration is a fixed point of that map.\nLet \u03b1 > 0 be such that 2\u03b1+dr < \u03b21 (dr is the same constant as in Theorem 3.1)\nand let\nE0 = {min{\u2212\u0002nYn(t),\u0002rY n(t)} \u2265 k, \u03c4t \u2208 [m\u2212 1,m),\u0002rY n(\u03c4t ) = 1},\nE1 = {max{Yn1 (\u03c4t ), . . . , Y nr (\u03c4t )} \u2265 max{Yn1 (\u03c4t ), Y nr (\u03c4t )} + \u03b1(t \u2212 m)},\nE2 = {max{Yn1 (\u03c4t ), . . . , Y nr (\u03c4t )} \u2264 Ynr (\u03c4t ) + \u03b1(t \u2212 m)},\nE3 = {max{Yn1 (\u03c4t ), . . . , Y nr (\u03c4t )} \u2264 Yn1 (\u03c4t ) + \u03b1(t \u2212 m),\n\u2212 \u0002nYn(\u03c4t ) < \u03b1(t \u2212 m)}\nE4 = {Ynr (\u03c4t ) + \u03b1(t \u2212 m) \u2264 max{Yn1 (\u03c4t ), . . . , Y nr (\u03c4t )} \u2264 Yn1 (\u03c4t ) + \u03b1(t \u2212 m),\n\u2212\u0002nYn(\u03c4t ) \u2265 \u03b1(t \u2212 m)}.\nSince at least one of the last four events must occur, we have\nP0(E0) \u2264 P0(E0 \u2229 E1) + P0(E0 \u2229 E2) + P0(E0 \u2229 E3) + P0(E0 \u2229 E4).(3.28)\nWe will now show that the four terms of the right-hand side of (3.28) decay expo-\nnentially in t \u2212 m.\nFirst term. This term is bounded above by\nP0\n(\nmax{Yn1 (\u03c4t ), . . . , Y nr (\u03c4t )} \u2265 max{Yn1 (\u03c4t ), Y nr (\u03c4t )} + \u03b1(t \u2212 m),\n\u03c4t \u2208 [m\u2212 1,m))\n\u2264 P0(max{Yn1 (m), . . . , Y nr (m)} \u2265 max{Yn1 (m),Y nr (m)} + \u03b1(t \u2212 m)\/2)\n+ P(\u0002m\u22121,m1 (Y n) \u2265 \u03b1(t \u2212 m)\/2)+ P(\u0002m\u22121,mr (Y n) \u2265 \u03b1(t \u2212 m)\/2).\n(3.29)\nCRYSTAL GROWTH MODELS 1083\nBut\nmax{Yn1 (m), . . . , Y nr (m)} \u2265 max{Yn1 (m),Y nr (m)} + \u03b1(t \u2212 m)\/2\nimplies that there exist 1 \u2264 i1 < i2 \u2264 r \u2212 1 such that\n\u2212\u0002i1Yn(m) \u2265\n\u03b1\n2r\n(t \u2212 m)\nand\n\u0002i2Y\nn(m) \u2265 \u03b1\n2r\n(t \u2212 m).\nHence, the first term of the right-hand side of (3.29) decays exponentially in t \u2212m\nby the inductive hypothesis and the invariance of the process under the map\n(y1, . . . , yn) \u2192 (yi1+1, . . . , yn, y1, . . . , yi1).\nSince the second and third terms trivially share this property, the same happens to\nthe left-hand side of (3.29).\nSecond term. On the set A = E0 \u2229 E2, the coordinates Yn1 , . . . , Y nr jump in\nthe time interval (\u03c4t , t] at the same rates as the coordinates of the process with\nzero boundary conditions (Xr1, . . . ,Xrr ). Therefore, if we start this last process at\ntime m \u2212 1 with all its coordinates equal to\nmax{Yn1 (\u03c4t ), . . . , Y nr (\u03c4t )},\nwe can couple it with the Yn process in such a way that on the set A we have\nYni (s) \u2264 Xri (s) for all s \u2208 [\u03c4t , t] and all 1 \u2264 i \u2264 r . Therefore, on A we have\nXrr (t) \u2265 Ynr (t) \u2265 Ynr+1(t) + k \u2265 Ynr+1(t) + 1\nand\nXrr (m \u2212 1) = max{Yn1 (\u03c4t ), . . . , Y nr (\u03c4t )}\n\u2264 Ynr (\u03c4t ) + \u03b1(t \u2212 m)\n= Ynr+1(\u03c4t ) + 1 + \u03b1(t \u2212 m)\n\u2264 Ynr+1(m) + 1 + \u03b1(t \u2212 m),\nwhich imply\n\u0002m\u22121,tr (Xr) + \u03b1(t \u2212 m) \u2265 \u0002m,tr+1(Y n).\nSince on A\n\u0002\nm,t\nr+1(Y\nn) \u2265 (N0,r+1 + N1,r+1)(t) \u2212 (N0,r+1 + N1,r+1)(m),\nwe must also have\n\u0002m\u22121,tr (Xr) + \u03b1(t \u2212 m) \u2265 (N0,r+1 + N1,r+1)(t) \u2212 (N0,r+1 + N1,r+1)(m).\n1084 E. D. ANDJEL, M. V. MENSHIKOV AND V. V. SISKO\nHence,\nA \u2282 A1 \u222a A2,\nwhere\nA1 = {\u0002m\u22121,tr (Xr) \u2265 dr(t \u2212 m)}\nand\nA2 = {(N0,r+1 + N1,r+1)(t) \u2212 (N0,r+1 + N1,r+1)(m) \u2264 (dr + \u03b1)(t \u2212 m)}.\nSince \u0002m\u22121,tr (Xr) is distributed as Xrr (t \u2212 m + 1) under P0, the probability of A1\ndecays exponentially in t \u2212m by Theorem 3.1, and the probability of A2 does the\nsame because \u03b21 > dr + 2\u03b1 > dr + \u03b1.\nThird term. Let\nB = {min{\u2212\u0002nYn(t),\u0002rY n(t)} \u2265 k, \u03c4t \u2208 [m\u2212 1,m)}\u2229 E3.\nOn B the coordinates Yn1 , . . . , Y nr jump in the time interval (\u03c4t , t] at the same rates\nas the coordinates of the process with zero boundary conditions (Xr1, . . . ,Xrr ).\nTherefore, if we start this last process at time m \u2212 1 with all its coordinates equal\nto max{Yn1 (\u03c4t ), . . . , Y nr (\u03c4t )}, we can couple it with the Yn process in such a way\nthat on the set B we have Yni (s) \u2264 Xri (s) for all s \u2208 [\u03c4t , t] and all 1 \u2264 i \u2264 r .\nTherefore, on B we have\nXr1(t) \u2265 Yn1 (t) \u2265 Ynn (t)\nand\nXr1(m\u2212 1) = max{Yn1 (\u03c4t ), . . . , Y nr (\u03c4t )}\n\u2264 Yn1 (\u03c4t ) + \u03b1(t \u2212 m) \u2264 Ynn (\u03c4t ) + 2\u03b1(t \u2212 m)\n\u2264 Ynn (m) + 2\u03b1(t \u2212 m).\nHence, on B we also have\n\u0002\nm\u22121,t\n1 (X\nr) + 2\u03b1(t \u2212 m) \u2265 \u0002m,tn (Y n).\nProceeding as we did for the second term, we get\nB \u2282 {\u0002m\u22121,t1 (Xr) \u2265 dr(t \u2212 m)}\n\u222a {(N0,n + N1,n)(t) \u2212 (N0,n + N1,n)(m) \u2264 (dr + 2\u03b1)(t \u2212 m)}\nand both sets have probabilities which decay exponentially in t \u2212 m.\nCRYSTAL GROWTH MODELS 1085\nFourth term. Since\nmax{Yn1 (\u03c4t ), . . . , Y nr (\u03c4t )} \u2265 Ynr (\u03c4t ) + \u03b1(t \u2212 m),\nthere exists 1 \u2264 i \u2264 r \u2212 1 such that\n\u0002iY\nn(\u03c4t ) \u2265 \u03b1\nr\n(t \u2212 m).\nSince in this case we also have \u2212\u0002nYn(\u03c4t ) \u2265 \u03b1(t \u2212m) considering the coordinates\n1, . . . , i, we can proceed as for the first term to show that the fourth term also\ndecays exponentially in t \u2212 m.\nHence, there exists constants such that\nP0\n(\nmin{\u2212\u0002nYn(t),\u0002rY n(t)} \u2265 k, \u03c4t \u2208 [m \u2212 1,m))\n\u2264 C9 exp(\u2212K9(t \u2212 m)).\nAdding this on 1 \u2264 m \u2264 \u0005, we obtain\nP0\n(\nmin{\u2212\u0002nYn(t),\u0002rY n(t)} \u2265 k, \u03c4t \u2264 \u0005)\n\u2264 C10 exp(\u2212K10(t \u2212 \u0005)).\nThis, (3.20) and (3.27) complete the inductive step. \u0001\nAcknowledgment. We are grateful to the referee for his very careful reading\nand useful comments.\nREFERENCES\n[1] BR\u00c9MAUD, P. (1981). Point Processes and Queues. Springer, New York. MR0636252\n[2] FAYOLLE, G., MALYSHEV, V. A. and MENSHIKOV, M. V. (1995). Topics in Constructive\nTheory of Countable Markov Chains. Cambridge Univ. Press. MR1331145\n[3] GATES, D. J. and WESTCOTT, M. (1988). Kinetics of polymer crystallization I. Discete and\ncontinuum models. Proc. Roy. London Soc. Ser. A 416 443\u2013461. MR0941479\n[4] GATES, D. J. and WESTCOTT, M. (1993). Markov models of steady crystal growth. Ann. Appl.\nProbab. 3 339\u2013355. MR1221155\nE. D. ANDJEL\nCMI\nUNIVERSIT\u00c9 DE PROVENCE\n39 RUE JOLIOT CURIE\n13453 MARSEILLE CEDEX 13\nFRANCE\nE-MAIL: andjel@cmi.univ-mrs.fr\nM. V. MENSHIKOV\nDEPARTMENT OF MATHEMATICAL SCIENCES\nUNIVERSITY OF DURHAM\nSOUTH ROAD, DURHAM DH1 3LE\nUK\nE-MAIL: Mikhail.Menshikov@durham.ac.uk\nV. V. SISKO\nDEPARTAMENTO DE ESTAT\u00cdSTICA\nINSTITUTO DE MATEM\u00c1TICA E ESTAT\u00cdSTICA\nUNIVERSIDADE DE S\u00c3O PAULO\nRUA DO MAT\u00c3O 1010\nCEP 05508-090, S\u00c3O PAULO, SP\nBRAZIL\nE-MAIL: valentin@ime.usp.br\n"}