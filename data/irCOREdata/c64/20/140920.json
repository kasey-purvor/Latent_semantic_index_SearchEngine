{"doi":"10.1109\/TII.2009.2032654","coreId":"140920","oai":"oai:dspace.lib.cranfield.ac.uk:1826\/5063","identifiers":["oai:dspace.lib.cranfield.ac.uk:1826\/5063","10.1109\/TII.2009.2032654"],"title":"Nonlinear Dynamic Process Monitoring Using Canonical Variate Analysis and Kernel\nDensity Estimations","authors":["Odiowei, P. P.","Cao, Yi"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-02-05T00:00:00Z","abstract":"The Principal Component Analysis (PCA) and the Partial Least Squares (PLS) are\ntwo commonly used techniques for process monitoring. Both PCA and PLS assume\nthat the data to be analysed are not self-correlated i.e. time-independent.\nHowever, most industrial processes are dynamic so that the assumption of time-\nindependence made by the PCA and the PLS is invalid in nature. Dynamic\nextensions to PCA and PLS, so called DPCA and DPLS, have been developed to\naddress this problem, however, unsatisfactorily. Nevertheless, the Canonical\nVariate Analysis (CVA) is a state-space-based monitoring tool, hence is more\nsuitable for dynamic monitoring than DPCA and DPLS. The CVA is a linear tool and\ntraditionally for simplicity, the upper control limit (UCL) of monitoring\nmetrics associated with the CVA is derived based on a Gaussian assumption.\nHowever, most industrial processes are nonlinear and the Gaussian assumption is\ninvalid for such processes so that CVA with a UCL based on this assumption may\nnot be able to correctly identify underlying faults. In this work, a new\nmonitoring technique using the CVA with UCLs derived from the estimated\nprobability density function through kernel density estimations (KDEs) is\nproposed and applied to the simulated nonlinear Tennessee Eastman Process Plant.\nThe proposed CVA with KDE approach is able to significantly improve the\nmonitoring performance and detect faults earlier when compared to other methods\nalso examined in this study","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/140920.pdf","fullTextIdentifier":"http:\/\/dx.doi.org\/10.1109\/TII.2009.2032654","pdfHashValue":"888da590795c92ab59b9d7c6bfe413d8862243c1","publisher":"IEEE","rawRecordXml":"<record><header><identifier>\noai:dspace.lib.cranfield.ac.uk:1826\/5063<\/identifier><datestamp>2012-03-09T09:23:53Z<\/datestamp><setSpec>hdl_1826_19<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>Nonlinear Dynamic Process Monitoring Using Canonical Variate Analysis and Kernel\nDensity Estimations<\/dc:title><dc:creator>Odiowei, P. P.<\/dc:creator><dc:creator>Cao, Yi<\/dc:creator><dc:subject>Canonical variate analysis (CVA) kernel density estimation (KDE) probability density function (PDF) process monitoring principal component analysis identification pls<\/dc:subject><dc:description>The Principal Component Analysis (PCA) and the Partial Least Squares (PLS) are\ntwo commonly used techniques for process monitoring. Both PCA and PLS assume\nthat the data to be analysed are not self-correlated i.e. time-independent.\nHowever, most industrial processes are dynamic so that the assumption of time-\nindependence made by the PCA and the PLS is invalid in nature. Dynamic\nextensions to PCA and PLS, so called DPCA and DPLS, have been developed to\naddress this problem, however, unsatisfactorily. Nevertheless, the Canonical\nVariate Analysis (CVA) is a state-space-based monitoring tool, hence is more\nsuitable for dynamic monitoring than DPCA and DPLS. The CVA is a linear tool and\ntraditionally for simplicity, the upper control limit (UCL) of monitoring\nmetrics associated with the CVA is derived based on a Gaussian assumption.\nHowever, most industrial processes are nonlinear and the Gaussian assumption is\ninvalid for such processes so that CVA with a UCL based on this assumption may\nnot be able to correctly identify underlying faults. In this work, a new\nmonitoring technique using the CVA with UCLs derived from the estimated\nprobability density function through kernel density estimations (KDEs) is\nproposed and applied to the simulated nonlinear Tennessee Eastman Process Plant.\nThe proposed CVA with KDE approach is able to significantly improve the\nmonitoring performance and detect faults earlier when compared to other methods\nalso examined in this study.<\/dc:description><dc:publisher>IEEE<\/dc:publisher><dc:date>2011-11-13T23:07:05Z<\/dc:date><dc:date>2011-11-13T23:07:05Z<\/dc:date><dc:date>2010-02-05T00:00:00Z<\/dc:date><dc:type>Article<\/dc:type><dc:identifier>1551-3203<\/dc:identifier><dc:identifier>http:\/\/dx.doi.org\/10.1109\/TII.2009.2032654<\/dc:identifier><dc:identifier>http:\/\/dspace.lib.cranfield.ac.uk\/handle\/1826\/5063<\/dc:identifier><dc:language>en_UK<\/dc:language><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["issn:1551-3203","1551-3203"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2010,"topics":["Canonical variate analysis (CVA) kernel density estimation (KDE) probability density function (PDF) process monitoring principal component analysis identification pls"],"subject":["Article"],"fullText":"Nonlinear Dynamic Process Monitoring using Canonical \nVariate Analysis and Kernel Density Estimations \n \nP.P. Odiowei and Y. Cao* \n \nSchool of Engineering, Cranfield University, Bedford, MK43 0AL, UK \n \nAbstract \nThe Principal Component Analysis (PCA) and the Partial Least Squares (PLS) are \ntwo commonly used techniques for process monitoring. Both PCA and PLS assume \nthat the data to be analysed are not self-correlated i.e. time-independent. However, \nmost industrial processes are dynamic so that the assumptions of time-independence \nmade by the PCA and the PLS are invalid in nature. Dynamic extensions to PCA and \nPLS, so called DPCA and DPLS, have been developed to address this problem, \nhowever, unsatisfactorily. Nevertheless, the Canonical Variate Analysis (CVA) is a \nstate-space based monitoring tool, hence is more suitable for dynamic monitoring than \nDPCA and DPLS. The CVA is a linear tool and traditionally for simplicity, the upper \ncontrol limit (UCL) of monitoring metrics associated with the CVA is derived based \non a Gaussian assumption. However, most industrial processes are non-linear and the \nGaussian assumption is invalid for such processes so that CVA with a UCL based on \nthis assumption may not be able to correctly identify underlying faults. In this work, a \nnew monitoring technique using the CVA with UCLs derived from the estimated \nprobability density function through kernel density estimations (KDE) is proposed \n                                                 \n* Corresponding author, email address: y.cao@cranfield.ac.uk \n 1\nand applied to the simulated nonlinear Tennessee Eastman Process Plant. The \nproposed CVA with KDE approach is able to significantly improve the monitoring \nperformance and detect faults earlier when compared to other methods also examined \nin this study. \n \nKeywords: Canonical Variate Analysis, Probability Density Function, Kernel \nDensity Estimation, Process Monitoring  \n \n1. INTRODUCTION \nProcess monitoring is essential to maintain high quality products as well as process \nsafety. Widely applied process monitoring techniques like the PCA and the PLS rely \non static models, which assume that the observations are time independent and follow \na Gaussian distribution. However, the assumptions of time-independence and \nnormality are invalid for most chemical processes because variables driven by noise \nand disturbances are strongly auto-correlated and most plants are nonlinear in nature. \nTherefore, the static PCA and PLS based approaches are inappropriate to monitor \nsuch nonlinear dynamic processes.  \n \nTo extend PCA applications to dynamic systems, Ku et al.1 presented a study of PCA \non lagged variables to develop dynamic models and Multivariate Statistical Process \nMonitoring (MSPM) tools for dynamic continuous processes. In this so called \nDynamic PCA (DPCA) approach, Ku et al.1 used parallel analysis to determine the \nnumber of time-lagged value for the process variables as well as the number of \nprincipal components to retain in the DPCA model. Although dynamic models are \ndeveloped in DPCA and faults are detected, diagnosis of abnormal behaviour is more \n 2\ncomplicated with DPCA given that lagged variables are involved2. It is also reported \nthat principal components extracted in this way are not necessarily the minimal \ndynamic representations3. Furthermore, Komulainen4 extended PLS applications to \ndynamic systems, in a similar way to the DPCA, for the monitoring of an online \nindustrial dearomatization process. The extended PLS approach is known as the \nDynamic PLS (DPLS). Although the DPLS technique was reported to be efficient for \nfault detection, like the DPCA, the capability of the DPLS to identify dynamic faults \nis still questionable because the way of the DPCA and DPLS to represent a dynamic \nsystem is not efficient and may not be able to capture some important dynamic \nbehaviours of the system. .   \n \nMore recently, monitoring techniques based on Canonical Variate Analysis (CVA) \nhave been developed with UCLs derived based on the Gaussian assumption5,6,7. CVA \nwas first introduced in 1936 by Hotelling7, adopted for use in dynamic systems for a \nlimited class of processes by Akaike in 19757,8 and adapted to general linear systems \nby Larimore in 19838. CVA is a state space based MSPM method, hence is more \nappropriate for dynamic process monitoring.  \n \nNorvalis et al.7 developed a process monitoring and fault diagnosis tool that combined \ncanonical variate state space (CVSS) models with knowledge based systems (KBS) \nfor monitoring multivariate process operations.  Faults were detected using the CVSS \nmodels and then UCLs derived based on the Gaussian assumption while diagnosis \nwas based on the KBS. The efficiency of the technique was illustrated by monitoring \nsimulated data of a polymerisation reactor system.  \n \n 3\nJuan and Fei6 employed CVA for fault detection based on Hotelling\u2019s T2 charts to \nmonitor a chemical separation plant. The results from the study illustrated a good \nperformance of the statistical model based on CVA. Furthermore, it was demonstrated \nthat the precision of the CVA model improved with an increase in the length of the \ndata employed for the CVA analysis.  \n \nDifferent from the above mentioned studies, Chiang et al.5 employed canonical \nvariate analysis to include the input and output variables for the estimation of the state \nspace variable. From the estimated state space variable, UCLs of T2 and Q metrics \nwere determined to judge whether or not those processes were in-control.  \n \nThe T2 and Q metrics are widely employed with various MSPM \ntechniques1,3,5,9,10,11,12. For linear MSPM techniques, such as PCA, PLS and CVA, \ntraditionally, UCLs of the T2 and Q metrics are estimated based on an assumption that \nthe latent or state variables follow a Gaussian distribution. However, most industrial \nprocesses are nonlinear. For such processes, although the distribution of stochastic \nsources might be Gaussian, such as measurement noises and normally distributed \ndisturbances, the distribution of process variables, in general, will be non-Gaussian. In \nsuch a case, the UCL estimated based on the Gaussian assumption is unable to \ncorrectly identify underlying faults.  \n \nThe problem of monitoring non-Gaussian processes can be addressed by directly \nestimating the underlying probability density function (PDF) of the T2 and Q metrics \nthrough the kernel density estimation (KDE) to derive the correct UCL13,14.  \n \n 4\nMartin and Morris13 presented an overview of multivariate process monitoring \ntechniques using the PCA and the PLS with T2 and M2 metrics for process \nmonitoring. The control limit of M2 metric was estimated based on the PDF, \ncombining techniques of standard bootstrap and kernel density estimations to \novercome the limitations of the T2 metric mentioned above. Both methodologies were \napplied to a continuous polyethylene reactor and a polymerisation reactor to \ndemonstrate the efficiencies of both methodologies and the M2 metric was reported to \nbe a more efficient process monitoring tool than the T2 metric.   \n \nChen et al.14 adopted several KDE approaches in association with PCA for process \nmonitoring. A gas melter process was used as the case study and it was demonstrated \nthat the KDEs could obtain nonparametric empirical density function as a tool for a \nmore efficient process monitoring. Their emphasis was to demonstrate the efficiencies \nof three different density estimators which were verified based on the \nmisclassification rates at given confidence intervals.  \n \nIn order to use the linear dynamic tools, such as the CVA to monitor nonlinear \ndynamic processes, the limitation of the Gaussian assumption based T2 and Q metrics \nmentioned above has to be addressed. In this paper, KDE is employed in association \nwith the CVA resulting in a new extension of the CVA algorithm, the \u2018CVA with \nKDE\u2019 for process monitoring. To achieve this, a CVA model is firstly estimated from \nthe so called past and future variables constructed from the collected process data. \nFrom the estimated CVA model, the T2 and Q metrics are then calculated and the \nKDE is employed to estimate the PDF of these T2 and Q metrics calculated. UCLs are \nthen determined based on the estimated PDF for a given confidence bound. For \n 5\ncomparison, different monitoring algorithms; DPCA and DPLS with and without \nKDE as well as CVA with and without KDE have been applied to the simulated \nnonlinear Tennessee Eastman Process Plant in the present study. Results show that the \nmonitoring performance is significantly improved by using the \u2018CVA with KDE\u2019 \napproach compared with other monitoring algorithms aforementioned. Although the \nCVA is a linear model, in this study, the CVA is employed to monitor a nonlinear \ndynamic process plant. Hence, this study is described as nonlinear dynamic process \nmonitoring.   \n \nThe rest of the paper is organised as follows: Section 2 explains the CVA model while \nsection 3 describes monitoring metrics and their UCLs derived through Kernel \nDensity Estimations. The procedure of CVA with KDE is then summarised in section \n4. Section 5 describes the case study whilst the results of the case study are presented \nand discussed in section 6. Finally, the work is concluded in section 7. \n \n2. CANONICAL VARIATE ANALYSIS \nCanonical Variate Analysis (CVA) is a linear dimension reduction technique to \nconstruct a minimum state space model for dynamic process monitoring. This section \napplies the linear CVA algorithm to a nonlinear dynamic plant for identifying state \nvariables directly from the process measurements.  \n \nAssume the nonlinear dynamic plant under consideration represented as follows. \nkkk\nkkk\nx vgy\nwxfx\n+=\n+=+\n)(\n)(1                                                                                                       (1) \n 6\nwhere   and  are state and measurement vectors respectively, nR\u2208kx mR\u2208ky )(\u22c5f  \nand  are unknown nonlinear functions, while  and  are plant disturbances and \nmeasurement noise vectors respectively. It is clear that such an unknown nonlinear \ndynamic system is generally difficult to deal with for monitoring. However, at a stable \nnormal operating point, the nonlinear plant can be approximated by a linear stochastic \nstate space model as follows; \n)(\u22c5g kw kv\nk\nk\n\u03b7Cxy\n\u03b5Axx\nkk\nk1k\n+=\n+=+                                                                                                            (2) \nwhere  and  are unknown state and output matrices respectively while  and  \nare collective modelling errors partially due to the underlying nonlinearity of the plant \nwhich has not been included in the linear model, as well as associated with process \ndisturbance and measurement noise,  and  respectively. Due to the unknown \nnonlinearity, the collective modelling errors,  and  generally will be non-\nGaussian although  and  might be normally distributed processes. This is the \nmain difference of this work from other CVA based approaches reported in literature. \nInstead of dealing with the unknown nonlinear system (1) directly, in this work, the \napproximated linear state space model given in (2) is considered through the standard \nCVA approach. Although the linear model (2) is easier to deal with than the nonlinear \nsystem (1), the collective errors  and  have to be treated as non-Gaussian \nprocesses. This leads to the direct PDF estimation of the associated T2 and Q metrics \nthrough the KDE approach explained in section 3.  \nA C k\u03b5 k\u03b7\nkw kv\nk\u03b5 k\u03b7\nkw kv\nk\u03b5 k\u03b7\n \n 7\nIn the CVA approach, firstly, the measurement vector  is expanded by  past and \nfuture measurements to give the past and future observation vectors y  and  \nrespectively. \nky q\nk,p kf ,y\nkpkpkp\nmq\nqk\nk\nk\nkp R ,,,\n2\n1\n,\n~, yyy\ny\ny\ny\ny \u2212=\u2208\n\u23a5\u23a5\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n=\n\u2212\n\u2212\n\u2212\nM                                                                  (3)                             \n                                                    \nkfkfkf\nmq\nqk\nk\nk\nkf R ,,,\n1\n1\n,\n~, yyy\ny\ny\ny\ny \u2212=\u2208\n\u23a5\u23a5\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n=\n\u2212+\n+\nM                                                             (4)                          \nwhere kp,y  and kf ,y  are the sample means of  and  respectively, and the \nproducts of  represents the lengths of the past and future observation vectors \nrespectively. The length of the past and future observations can be determined by \nchecking the autocorrelation of the square sum of the process variables such that the \ncorrelation can be neglected when the time distance is larger than the number of lags \ndetermined.  \nkp,y kf ,y\nmq\n \nThese past and future observations are stochastic processes. Their sample-based \ncovariance and cross-covariance matrices can be estimated through the truncated \nHankel matrices as follows; \n1\n1\n1 )1(~~)1(: \u2212\n+\n+=\n\u2212 \u2212=\u2212= \u2211 MM TppMq\nqk\nT\np,kp,kpp YYyy\u03a3                                                         (5) \n \n1\n1\n1 )1(~~)1(: \u2212\n+\n+=\n\u2212 \u2212=\u2212= \u2211 MM TffMq\nqk\nT\nf,kf,kff YYyy\u03a3      (6) \n 8\n 1\n1\n1 )1(~~)1(: \u2212\n+\n+=\n\u2212 \u2212=\u2212= \u2211 MM TpfMq\nqk\nT\np,kf,kfp YYyy\u03a3      (7) \n \nwhere  and  are past and future truncated pY fY M -column Hankel matrices \nrespectively, and defined as follows.  \nMmq\nMqpqpqpp R\n\u00d7\n+++ \u2208= ]~~~[ ,2,1, yyyY L        (8) \nMmq\nMqfqfqff R\n\u00d7\n+++ \u2208= ]~~~[ ,2,1, yyyY L        (9) \nFor a set of measurements with total  observations, the last element of  in (3) \nis , whilst the last element of  in (4) should be . Therefore, the maximum \nnumber of columns of these Hankel matrices is  \nN\nM+\n1, +qpy\n1y qf ,y Ny\n12 +\u2212= qNM                    (10)  \n \nThe CVA aims to find the best linear combinations,   )~( ,kf\nT ya  and )~( ,kp\nT yb  of the \nfuture and past observations so that the correlation between these combinations is \nmaximised. The correlation can be represented as follows: \n( ) ( ) 2\/12\/1),( b\u03a3ba\u03a3a\nb\u03a3a\nba\npp\nT\nff\nT\nfp\nT\nfp =\u03c1                  (11) \nLet  and . The optimization problem can be casted as: a\u03a3u 2\/1ff= b\u03a3v 2\/1pp=\n( )\n1\n1s.t.\nmax 2\/12\/1\n=\n=\n\u2212\u2212\nvv\nuu\nv\u03a3\u03a3\u03a3u\nvu,\nT\nT\nppfpff\nT\n                  (12) \nAccording to linear algebra theory, the solution,  and  are left and right singular \nvectors of the scaled Hankel matrix,  and the maximal correlation \nu\nfp\u03a3\nv\n2\/12\/1: \u2212\u2212= ppff \u03a3\u03a3H\n 9\n),(max , baba fp\u03c1\u03c3 =\nH\n is the corresponding singular value of . If the rank of the scaled \nHankel matrix,  is r, then there are \nH\nr  non-zero singular values, rii K,2,1, =\u03c3  in \nthe descending order and correspondingly r  pairs of the left and right singular \nvectors,  and  for . Singular values and vectors can be collected in \nthe following matrix form of the singular value decomposition (SVD). \niu iv\nppfpff \u03a3\u03a3\u03a3H == \u2212\u2212 2\/12\/1\n[ ]r \u2208= L21 uuuU\n~y\nT\nr\nT\nT\nkp\nT\nr\nT\nT\nk\n2\n1\n,\n2\n1\n~\nv\nv\nv\ny\nb\nb\nb\nz\n\u23a2\u23a2\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n=\n\u23a5\u23a5\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n=\nMM\npp\nT \u2212 \u2208= 2\/1\u03a3VJ\nri ,,2,1 K=\nTUDV\nrmq\u00d7 =, 1vV\nT\nkppp ,\n2\/1 ~ Vy =\u2212\nmqr\u00d7\n                                                                                     (13) \nwhere \n[ ] rrmqRR \u00d7\u00d7\n\u23a2\u23a2\n\u23a2\u23a2\n\u23a3\n\u23a1\n\u2208\n\u03c3\nML\n0\n0\n,\n1\nr\nr\nR\u2208\n\u23a5\u23a5\n\u23a5\u23a5\n\u23a6\n\u23a4\n\u03c3\n\u03c3\nL\nMOM\nL\nL\n0\n0\n00\n2\nmq\nrv\n=\n=D2v\npp\n\/1\u2212\n   \nFurthermore, the canonical variates can be directly estimated from the past \nobservation vector  as illustrated in (14). kp,\nkpkp ,,\n2 ~~ yJy\u03a3\u03a3\n\u23a5\u23a5\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n                                             (14)   \nwhere  is the transformation matrix, which transforms the -\ndimensional past measurements to the \nR\nr -dimensional canonical variates. These \ncanonical variates are normalised with a unit sample covariance. \nI\u03a3VVy\u03a3z =\u239c\u239c\u239d\n\u239b\n\u2212\u2212\n\u2212\n+\n+=\n\u2212\n=\n\u2211 \/T\/ppMq\nqk\nT\np,k\n\/\npp\nT\nq\nk\nT\nk MM\n221\n1\n21 ~\n1\n1\n1\n1  VVV\u03a3 ==\u2212 T\/pppp 21\na\nal space ba\ngnitude of the singular values, the first \nn canonical variates \n\u03a3\u2212pp\n1\nand\n\u03a3\u239f\u239f\u23a0\n\u239e\nate sp\n into the state space \na\ny p,k~\nccording to the m\nV=z\n+\n+\n\u2211M\nq\nk\n1\nFrom equation (14), the canonical vari ace spanned by all the estim ted \ncanonical variates can be separated  the residu sed \non the order of the system. A\nn dominant singular values are determined and the corresponding \n 10\nretained as the state variables where rn < . In addition, the remaining  \ncanonical variates are said to be in the residual space. Equation (15) below shows the \nentire canonical variate space spanned by the state variables and the \nresidual canonical variates ( .  \n)( nr \u2212\n)( rk R\u2208z\n)nrk R\n\u2212\u2208\n)( nk R\u2208x\nd\nTT\nk\nT\nkk ][ dxz =                                                                                                              (15) \nThe state variables  are a subset of the canonical variates  estimated in (14). \nHence the state variable like the canonical variates is defined as a linear combination \nof the past observation vector , \n)( kx )( kz\nkp,\n~y kpxk ,~yJx = , where  with \nconsisting of the first n columns of  defined in (13). \n2\/1\u2212= ppTx \u03a3VxJ\nxV V\n \nLike the canonical variates, the state variables also have the unit covariance. Once the \nstates of the system are determined, the state and output matrices,  and C  can then \nbe estimated through linear least squares regression. However, the determination of \nthe state and output matrices A and C  will be omitted from the rest of the paper since \nthese matrices will not be used in this work.  \nA\n \nThe variation of state variables can be represented by the T2 metric. Another \ncommonly used monitoring metric is the Q metric which measures the total sum of \nsquare errors of the variations in the residual space. The estimation and use of the T2 \nand Q metrics are explained in the next section. \n \n3.  CONTROL LIMIT THROUGH KERNEL DENSITY ESTIMATIONS \n 11\nTraditionally, it was assumed that  and  are normally distributed, as well as the \nstate, measurement and residual vectors, , and  since a linear combination of \nmultivariate Gaussian variables is also normally distributed.  \nk\u03b5 k\u03b7\nkx yk ke\n \nFor  samples of data, the number of samples of the states available is N M , given in \n(10). For the normally distributed n -dimensional state vector, , with x M  samples, \n,  , the T2 statistic defined in (16) can be used to test whether the \nmean \nkx M,,2,1 K=k\n\u03bc  of  is at the desired target . x \u03c4\n( ) ( ) MkT kTkk ,,2,1,12 K=\u2212\u2212= \u2212 \u03c4xS\u03c4x                 (16) \nwhere  is the estimated covariance of . If S x \u03c4\u03bc = , then , where ),(~2 nMnFCT \u2212\nnMMnM(MC )1)(1() +\u2212\u2212= . Therefore, the system (2) can be monitored by \nplotting  against time, , along with a UCL, T  corresponding to a \nsignificance level, \n2\nkT k )(\n2\nUCL \u03b1\n\u03b1 , that has the probability, ( ) \u03b1\u03b1 =)(> 2UCLT2TP k .  \n \nEquation (16) can be simplified as the state covariance matrix, IS = . Furthermore, \nsince the past and future observations, kp ,~y  and kf ,~y  have zero means, the desired \ntarget for the state is .  With these simplifications in place, the T2 metric for the \nstate space is represented in (17). \n0\u03c4 =\nk\nT\nkkT xx=2                     (17) \nThe corresponding UCL  for a significance level )(2 \u03b1UCLT \u03b1  is derived as follows  \n)(\n)(\n)1()( ,\n2\n2 \u03b1\u03b1 nMnCUL FnMM\nMnT \u2212\u2212\n\u2212=                                                                                (18) \n 12\nwhere )(, \u03b1baF  is the critical value of the F-distribution with a  and b  degrees of \nfreedom for a significance level \u03b1. By comparing  against T  in real-time, an \nabnormal condition is then determined when T . \n2\nkT\nUCLT>\n)\u03b1(2UCL\n)(2 \u03b1k\n \nThe Q metric is introduced to test the significance level of the prediction error \nrepresented in the scaled past observation space. According to (14), the prediction \nerror for the scaled past measurement and the corresponding Q metric are then \ndefined in (19) and (20) respectively. \n( ) kpkpppTxxk ,,2\/1 ~~ yFy\u03a3VVIe =\u2212= \u2212                                                                              (19)                               \nk\nT\nkkQ ee=                                                                                                                   (20) \nGiven a level of significance,\u03b1 , also based on the assumption of normality, the \nthreshold, )(\u03b1UCLQ  of the Q-metric for the PCA is estimated by Jackson and \nMudholkar15 as \n0\n1\n2\n1\n002\n1\n20\n1\n)1(\n1\n2\n)(\nh\nUCL\nhhchQ\n\u23a5\u23a5\u23a6\n\u23a4\n\u23a2\u23a2\u23a3\n\u23a1 \u2212++= \u03b8\n\u03b8\n\u03b8\n\u03b8\u03b8\u03b1 \u03b1                                                          (21)     \nwhere , \u2211\n+=\n=\nr\nnj\ni\nji\n1\n\u03bb\u03b8 2\n2\n31\n0 3\n2\n1 \u03b8\n\u03b8\u03b8\u2212=h  , and c\u03b1 is the normal deviate corresponding to (1 \u2013 \n\u03b1) percentile. For the PCA, in (21), j\u03bb  is the j th eigenvalue of the covariance of the \nmeasured data. For the CVA error represented in (19), it should be the covariance of \nthe scaled past observations, kp,pp\n2\/1 ~y\u03a3\u2212 , i.e.  \nrj\nMq\nqk\nT\nkpppkpppj ,,2,1,1)~)(~(\n1\n,\n2\/1\n,\n2\/1 K==\u239f\u239f\u23a0\n\u239e\n\u239c\u239c\u239d\n\u239b= \u2211+\n+=\n\u2212\u2212 y\u03a3y\u03a3\u03bb\u03bb  \n 13\nTherefore, the calculation of )(\u03b1UCLQ  can be simplified by letting )( nri \u2212=\u03b8  and \n in (21). By comparing  against 3\/10 =h kQ )(\u03b1UCLQ  in real-time, an abnormal \ncondition is determined when )(\u03b1UCLk QQ > . \n \nBoth control limits in (18) and (21) are based on the assumptions that the state \nvariables and prediction errors are Gaussian. However, when the collective modelling \nerrors,  and  of the system (2) are non-Gaussian processes, this assumption is not \nvalid. Hence,  and \nk\u03b5 k\u03b7\n2\nUCLT )(\u03b1 )(\u03b1UCLQ  derived above can no longer be used as control \nlimits for real-time monitoring. One solution to this issue is to estimate the PDF \ndirectly for these T2 and Q metrics through a non-parametric approach13,14. Amongst \nvarious PDF estimating approaches, the kernel density estimation (KDE) approach13,14 \nis selected for this work. The KDE is a well established approach to estimate the PDF \nparticularly for univariate random processes16. Therefore, it is particularly suitable for \nthe T2 and Q metrics which are univariate although the underlying processes are \nmultivariate. Assume x  is a random variable and its density function is denoted \nby .  This means that  )(xp\n\u222b\n\u221e\u2212\n=<\nb\ndxxpbxP )()(                                                                                                  (22) \nTherefore, by knowing , an appropriate control limit can be determined for a \nspecific confidence bound, \n)(xp\n\u03b1  using (22). The estimation of the probability density \nfunction  at point )(\u02c6 xp x  through the kernel function, )(\u22c5K  is defined as follows.  \n)(1)(\u02c6\n1 h\nxxK\nMh\nxp k\nM\nk\n\u2212= \u2211\n=\n                                                                                        (23) \n 14\nwhere kx , Mk ,,2,1 K=  are samples of x  and h  is the bandwidth. The bandwidth \nselection in KDE is an important issue because selecting a bandwidth too small will \nresult in the density estimator being too rough, a phenomenon known as under-\nsmoothed while selecting a bandwidth too big will result in the density estimator \nbeing too flat. There is no single perfect way to determine the bandwidth. However, a \nrough estimation of the optim  subject to minimising the \napproximation of  \n.  \nT\n and \nal bandwidth opth\n the mean integrated square error can be derived in (24), where \u03c3 is\nthe standard deviation17\n5\/1\nopt 06.1\n\u2212= Nh \u03c3                                                                                                       (24) \nBy replacing kx  with \n2\nkT  and kQ  obtained in equations (17) and (20) respectively, the \nabove KDE approach is able to estimate the underlying PDFs of the 2 and Q metrics. \nThe corresponding control limits, )(2UCL \u03b1T )(\u03b1UCLQ\ngiven confidence bound, \n can then be obtained from \nthe PDFs of the T2 and Q metrics for a \u03b1  by solving the \nfollowing equations respectively. \n                             (25)  \nthe state space but not necessa f significance in the error \nspace, vice versa.  Therefore, in this work, a fault is then identified (\n\u222b\n\u222b\n\u221e\u2212 =\n=\n)(\n)( 22\n)(\n)(\n2\n\u03b1\n\u03b1\n\u03b1\n\u03b1\nUCL\nUCL\nQ\nT\ndQQp\ndTTp\nThe T2 and Q metrics are complementary. A fault may cause a significant deviation in \nry results in a similar level o\n\u221e\u2212  \n=kF 1) if either \n)(2 \u03b1UCLk TT >  or )(2 \u03b1UCLk QQ >  conditions are satisfied, i.e. \n( ) ( ))()(22 \u03b1\u03b1 UCLkUCLkk QQTTF >\u2295>=                 (26  \nwhere \u2295  represents a logical OR operation.  By using the fault detection condition \n(26), the\n)\nmonitoring performance becomes insensitive to the number of states,  n   \n 15\nsince any ignored variances in the T2 metric by reducing  will be recovered by Q \nA using KDEs for nonlinear dynamic process monitoring is proposed to identify \nnderlying faults subject to non\nproposed CVA with KDE algorithm is illustrated in the flo chart presented in Figure \n1. \n \nn\n-Gaussian processes. The step by step procedure of the \nw\nmetric. \n \n4. CVA with KDE Algorithm \nBy summarising the analysis presented in the previous sections, a new extension of \nCV\nu\n 16\nFigure 1: Flowchart of the CVA with KDE algorithm, (a) Off-line modelling \nprocedure,  (b) Real-time monitoring procedure \n(a) (b) \nArbitrary point (k): Determine the arbitrary \npoint (k) which will serve as the current time. \nCollect Real-time Monitoring Data: \nThe real-time monitoring data is \narranged in the similar way as the data \nfor off-line modelling.  \nPast and Future output vectors:  \nTT\npk\nT\nk\nT\nk\nT\nkkp ],...,,[ 321, \u2212\u2212\u2212\u2212= yyyyy  \nTT\nfk\nT\nk\nT\nk\nT\nkkf ],...,,[ 121, \u2212+++= yyyyy                    \nCanonical Variate & State Vector \nFormation for Real-time  Data:  \nThe state and residual of realtime data is \ncontinuously calculated by multiplying \nthe matrix  and F  by the past \nmeasurments of the real-time data \nrespectively. \nxJ\nkp,x\n~yk Jx =  and \nkp,k\n~yFe =    \n \nCovariance matrices of the past and future \noutput vectors:  1)1( \u2212\u2212= MTpppp YY\u03a3\n1)1( \u2212\u2212= MTffff YY\u03a3 , \u03a3  1)1( \u2212\u2212= MTpffp YY\nPDF Estimation: )(1)(\u02c6\n1 h\nxxK\nMh\nxp k\nM\nk\n\u2212= \u2211\n=\n for \n and  2kT=kx kQ=kx\nScale the Hankel matrix, Perform SVD & \nDetermine Order (n) \nT\/\nppfp\n\/\nff UDV\u03a3\u03a3\u03a3H == \u2212\u2212 2121  \nFormation of T2 and Q values: the T2 \nand Q values were estimated from the \nstate variables and prediction error of the \nreal-time data.  \nk\nT\nkk xxT =2  and  kTkk eeQ =\nMonitoring:  A fault is detected if  \n ( ) ( ))()(22 \u03b1\u03b1 UCLkUCLkk QQTTF >\u2295>=  \n \nis true. \nEstimate the T2 and Q metrics:  \nkkT xx\nT\nk=2  and kkQ eeTk=                                 \nState variable and prediction error estimation:  \n kpx ,~yJxk =  ,  kkQ eeTk=\nCalculate transformation matrices: \n2\/1\u2212= ppTxx \u03a3VJ , ( ) 2\/1\u2212\u2212= ppTxx \u03a3VVIF                  \nControl Limit Estimation: For given\u03b1 , solve  \n\u03b1\u03b1 =\u222b \u221e\u2212 )( 22\n2\n)(UCL\nT\nkk dTTp\n\u222b \u221e\u2212 =)( )(\u03b1 \u03b1UCLQ kk dQQp\n and \n \n 17\n5. Case Study - Tennessee Eastman Process Plant \nThe Tennessee Eastman Process (TEP) plant18 has 5 main units which are the reactor, \ncondenser, separator, stripper and compressor5,18. Streams of the plant consists of 8 \ncomponents; A, B, C, D, E, F, G and H. Components A, B and C are gaseous \nreactants which were fed to the reactor to form products G and H. The TEP data used \nfor this work consists of two blocks; the training and test data blocks. Each block has \n21 data sets corresponding to the normal operation (Fault 0) and 20 fault operations \n(Fault 1 \u2013 Fault 20). The sampling time for most of the process variables in the TEP \nplant is 3 minutes. A total of 52 measurements are collected for each data set of \nlength, N=960 representing 48-hour operation with a sampling rate of 3 minutes. \nHowever, 19 of the 52 measurements, 14 of them sampled at 6 minute interval and 5 \nof them sampled in every 15 minutes, have not been included in this study due to the \nmeasurement time delay. Different from the work reported by Chiang15, 11 \nmanipulated variables are treated the same as other measured variables because under \nfeedback control, these variables are not independent any more. The simulation time \nof each operation run in the test data block is 48 hours and the various faults are \nintroduced only after 8 hours. This means that for each of the faults, the process is in-\ncontrol for the first 8 simulation hours before the process gets out of control at the \nintroduction of the fault. All twenty faults have been studied in this work. Also in this \npaper, the normal operating process data will be referred to as the training data. A \ngraphical description of the TEP Plant is shown in Figure 2 while a brief description \nof the twenty TEP Faults is presented in Table 1.  \n 18\n Figure 2 Graphical Description of the TEP Plant  \n \n \n \n \n \n \n \n \n \n \n \n \n \n 19\n \nTable 1. Brief Description of TEP Plant Faults \n \nFault ID Description Type \n1 A\/C Feed Ratio, B Composition Constant (Stream 4) Step \n2 An increase in B while A\/C Feed ratio is constant (stream 4) Step \n3 D Feed Temperature (Stream 2) Step \n4 Reactor Cooling Water Inlet Temperature Step \n5 Condenser Cooling Water Inlet Temperature Step \n6 A loss in Feed A (stream 1) Step \n7 C Header Pressure Loss \u2013 Reader Availability (Stream 4) Step \n8 A,B,C Feed Composition (Stream 4) Random \n9 D Feed Temperature (Stream 2) Random \n10 C Feed Temperature (Stream 4) Random \n11 Reactor Cooling Water Inlet Temperature Random \n12 Condenser Cooling Water Inlet Temperature Random \n13 Reaction Kinetics Drift \n14 Reaction Cooling Water Valve Sticking \n15 Condenser Cooling Water Valve Sticking \n16 Unknown Unknown\n17 Unknown Unknown\n18 Unknown Unknown\n19 Unknown Unknown\n20 Unknown Unknown\n \n \n 20\n6. Monitoring Performance \nThe monitoring performance in this study is assessed based on the percentage \nreliability which is defined as the percentage of the samples outside the control \nlimits19 within the last 40 hour faulty operation.  Hence a monitoring technique is said \nto be better than another technique if the percentage reliability of this technique is \nnumerically higher than the percentage reliability of another. Also, the monitoring \nperformance is assessed by the detection delay which is the time period it takes to \ndetect a fault after the introduction of the fault. The false alarm rate was also \ninvestigated. The monitoring performance of the proposed CVA with KDE is \ncompared with the performance of the DPCA and DPLS with and without KDE as \nwell as CVA without KDE using all twenty faults described above. The 99% \nconfidence interval is adopted in this study.  \n0 5 10 15 20 25\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n1\nLag\nS\nam\npl\ne \nA\nut\noc\nor\nre\nla\ntio\nn\nSample Autocorrelation Function (ACF)\n \nFigure 3. Autocorrelation function of the summed squares of all measurements. \n 21\nThe variability of the training data is characterised by the extracted canonical variate \nstate space model. Firstly, the number of time lags for past and future observations is \ndetermined from the autocorrelation function of the summed squares of all \nmeasurements as shown in Figure 3 against \u00b15% confidence bounds. The \nautocorrelation function indicates that the maximum number of significant lags in this \nstudy is 16. Hence both p and f are set to 16. The length of the past and future \nobservations ( ) is 528 according to (3) and (4). The number of columns of the \ntruncated Hankel matrices according to (10) is \nmq\n929=M . The singular value \ndecomposition is then performed on the scaled Hankel matrix as in (13).  \n \n0 100 200 300 400 500 600\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nNormalised Singular Values\nN\nor\nm\nal\nis\ned\n S\nin\ngu\nla\nr V\nal\nue\ns\nNumber of States\n \nFigure 4 Normalised Singular Values from the Scaled Hankel Matrix \n \nSeveral ways have been suggested to determine the order ( ) of the system for CVA \nbased approaches amongst which the dominant singular values3,5 and the Akaike \nn\n 22\nInformation Criterion (AIC)6 are most widely adopted. The former method was \nadopted in this study to determine the order of the system. The singular values from \nthe scaled Hankel were normalised to have the values ranging between 0 and 1 and \nthen the order determined based on the dominant normalised singular values. For the \nTEP case study, it is noticed that the singular values of the scaled Hankel matrix H  in \n(13) decrease slowly. If  is determined from these singular values, it will be \nunrealistically large as indicated in Figure 4 (a), which shows the normalised sum of \nsquares of residual singular values against the number of states. As mentioned in \nsection 3, the value of  is not important to monitoring performance for this work due \nto the fault detection condition (26) adopted. Hence, a more realistic number of \nsingular values, n = 26 represented by circles in Figure 4 are employed to represent \nthe model space. Also, to make a fair comparison of the proposed technique with the \nother techniques considered, the process variables, the number of lag and the order to \ndetermine the dimension of the latent variables are the same for all the approaches \ncompared. The monitoring criterion mentioned above is applied to all the other \nmethods considered.   \nn\nn\n \n6.1 Reliability Comparison \nThe superiority of the CVA with KDE over other techniques considered in this paper \nis demonstrated in Table 2.  Over all the faults compared, the CVA achieves the best \nperformance in terms of reliability. Both CVA techniques are able to improve the \nmonitoring performance for most TEP faults comparing with the DPCA, DPCA with \nKDE, DPLS and DPLS with KDE techniques. Nevertheless, the proposed CVA with \nKDE technique is able to further improve the reliability for faults that are more \ndifficult to detect such as Faults 3 and 9. Faults 3 and 9 are more difficult to detect \n 23\nbecause these faults have very little effect on the corresponding process \nmeasurements. For such faults, the performance of the CVA with KDE is significantly \nbetter than that of the CVA. All KDE approaches achieve the reliability higher than or \nthe same as their non-KDE counterparts as indicated in Table 2. This is due to the \nnonlinear and non-Gaussian features of the plant, which justify the necessity of this \nwork. \n \nTable 2. Performance based on % Reliability of all Algorithms (99%) \n \nFaults CVA+KDE \n(%) \nCVA \n(%) \nDPCA+KDE\n(%) \nDPCA \n(%) \nDPLS+KDE \n(%) \nDPLS \n(%) \n1 99.75 99.75 99.38 99.25 99.25 99.25 \n2 99.5 98.5 98 97.88 98.13 98 \n3 73.03 37.2 0 0 0.2497 0 \n4 99.88 99.88 99.88 99.88 99.88 99.88 \n5 99.88 99.88 29.09 27.84 28.21 26.47 \n6 99.88 99.88 99.88 99.88 99.88 99.88 \n7 99.88 99.88 99.88 99.88 99.88 99.88 \n8 98.88 98.75 97.25 97.13 97 97 \n9 92.26 75.28 0.2497 0 0.2497 0 \n10 96.63 96.25 39.08 28.21 36.83 29.46 \n11 99.38 99.38 98.88 98.63 97.88 97.75 \n12 99.5 99.5 98.13 98.13 98 98 \n13 96.13 96.13 95.01 95.01 94.76 94.76 \n14 99.88 99.75 99.75 99.75 99.75 99.75 \n15 99.5 99.5 0.1248 0 0.1248 0 \n16 99.13 99.13 35.83 26.22 26.97 21.6 \n17 98.13 98.13 97.75 97.75 97.75 97.75 \n18 99.25 99.25 98.63 98.5 98.63 98.5 \n19 99.88 99.88 90.51 87.02 84.64 79.28 \n20 97.63 97.25 79.15 76.9 73.91 71.41 \n \n \n6.2 Detection Delay Comparison \nThe detection delays for the CVA with KDE and other techniques considered are \npresented in Table 3. As shown in Table 3, the CVA with KDE approach is able to \ndetect most of these faults earlier than other techniques. This means operators have \n 24\nmore time to take safety measures to counteract occurring faults if the proposed CVA \nwith KDE approach is adopted. Again, all KDE associated approach achieve detection \ndelay less than or the same as their non-KDE counterparts due to the same reason \naforementioned. \nAlso investigated is the false alarm rates for all the faults and no false alarm has been \nobserved for all faults and all approaches studied.  \n \nTable 3. Detection Delay for all the Algorithms \n \nFaults CVA +KDE CVA  DPCA+KDE DPCA DPLS+KDE DPLS \n1 9 9 18 21 21 21 \n2 15 15 51 54 48 51 \n3 15 39 - - 1125 - \n4 6 6 6 6 6 6 \n5 6 6 12 12 12 12 \n6 6 6 6 6 6 6 \n7 6 6 6 6 6 6 \n8 30 33 69 72 75 75 \n9 33 45 2115 - 1125 - \n10 84 93 210 210 219 219 \n11 18 18 24 24 24 24 \n12 15 15 48 48 51 51 \n13 96 96 123 123 129 129 \n14 6 9 9 9 9 9 \n15 15 15 1140 - 1125 - \n16 24 24 111 111 216 219 \n17 48 48 57 57 57 57 \n18 21 21 36 39 36 39 \n19 6 6 36 39 36 42 \n20 60 69 120 123 123 123 \n    \n \n6.3 Monitoring Chart Comparison of Fault 9 \nTo appreciate the superior performance achieved by the new CVA with KDE \napproach, the T2 and Q monitoring charts of all approaches for Fault 9 are presented \nin Figure 5. In Figure 5, sub-figures in the left column and the right column are for the \nT2 and Q charts respectively; whilst the first, second and third rows are for CVA, \n 25\nDPCA and DPLS approaches respectively. Upper control limits obtained based on the \nGaussian assumption are represented as dashed lines, whilst the UCLs determined by \nthe KDE approach are shown in dash-dot lines. \n   \n0 10 20 30 40\n20\n40\n60\n80\n(a) CVA T2\nT2\n0 10 20 30 40\n400\n500\n600\n700\n800\n(b) CVA Q\nQ\n0 10 20 30 40\n0\n20\n40\n60\n80\n(c) DPCA T2\nT2\n0 10 20 30 40\n200\n250\n300\n350\n400\n(d) DPCA Q\nQ\n0 10 20 30 40\n10\n20\n30\n40\n50\n60\n(e) DPLS T2\nT2\ntime, hour\n0 10 20 30 40\n200\n250\n300\n350\n400\n450\nQ\n(f) DPLS Q\ntime, hour\n \nFigure 5.  Fault 9 monitoring chats, solid: metrics, dashed: KDE based UCL, dash-\ndot: Gaussian assumption based UCL.   \n \nFigure 5 clearly indicates that only the CVA model is able to reveal the difference in \ndynamic behaviour between the normal operation and the operation with fault 9. Both \n 26\nT2 and Q metrics produced by the DPCA and the DPLS approaches have no \nidentifiable difference between the normal and faulty operations. Furthermore, the \nCVA with KDE approach gives tighter UCLs for both metrics resulting in a higher \npercentage of reliability and earlier fault detection than the traditional CVA approach.  \n \n7 Conclusions \nTo deal with fault monitoring for nonlinear dynamic processes, the linear state-space \nmodel based CVA approach is extended by directly estimating the underlying PDF of \nthe associated T2 and Q metrics to derive more appropriate control limits for these \nmonitoring metrics. This leads to the new CVA with KDE algorithm proposed for \nnonlinear dynamic process monitoring. The proposed approach is applied to the \nTennessee Eastman Process. The monitoring performance of the proposed CVA with \nKDE is compared with that of the DPCA and DPLS with and without KDE as well as \nCVA without KDE techniques. The percentage reliability and the detection delays \nwere adopted to assess and compare the monitoring performance of the proposed \napproach with that of all other techniques considered in this study. Although some of \nthe faults are commonly detected by all the techniques considered, the outstanding \nsuperiority of the CVA with KDE is demonstrated in those faults that are not easily \ndetectable. For such faults, the proposed CVA with KDE has higher percentage \nreliability than other techniques considered. In addition, the proposed CVA with KDE \nis able to detect faults earlier than other techniques considered. Hence, the CVA with \nKDE is a more efficient tool than the DPCA and the DPLS with and without KDE as \nwell as the CVA without KDE for nonlinear dynamic process monitoring.  \n \n \n 27\nReferences \n1. W. F. Ku, H. R. Storer and C. Georgakis. Disturbance Detection and \nIsolation by Dynamic Principal Component Analysis. Chemometrics and \nIntelligent Laboratory Systems, 1995, pp 179 - 196. \n2. T. J. Richard, K. Uwe and E. C. Jonathan. Dynamic Multivariate Statistical \nProcess Control Using Subspace Identification. Journal of Process Control \nVol. 14, 2004, pp 279 - 292. \n3. A. Negiz and A. Cinar. Monitoring of Multivariable Dynamic Processes \nand Sensor Auditing. Journal of Process Control, Vol. 8, No. 56, 1998, pp \n375 - 380.  \n4. T. Komulainen, M. Sourander and S. Jamsa-Jounela. An Online \nApplication of Dynamic PLS to a Dearomatization Process. Computers \nand Chemical Engineering, Vol. 28, 2004, pp 2611 - 2619. \n5. L. H Chiang, E. L. Russell and R. D. Braatz. Fault Detection and \nDiagnosis in Industrial Systems. Springer, London, 2001, pp 85-98, 103-\n109. \n6. L. Juan and L. Fei. Statistical Modelling of Dynamic Multivariate Process \nUsing Canonical Variate Analysis. Proceedings of IEEE International \nConference of Information and Automation, Colombo, Sri Lanka, 15 -17, \nDecember 2006, pp 218 - 221.  \n7. A. Norvalis, A. Negiz, J. DeCicco and A. Cinar. Intelligent Process \nMonitoring by Interfacing Knowledge-Based systems and Multivariate \nStatistical Monitoring. Journal of Process Control, Vol. 10, 2000, pp 341 - \n350. \n 28\n8. C. D. Schaper, W. E. Larimore, D. E. Seborg and D. A. Mellichamp. \nIdentification of Chemical Processes Using Canonical Variate Analysis. \nComputers Chemical Engineering, Vol. 18, No. 1, 1994, pp 55 - 69. \n9. A. Negiz and A. Cinar. PLS, Balanced and Canonical Variate Realization \nTechniques for Identifying VARMA models in State Space. Chemometrics \nand Intelligent Laboratory Systems, Vol. 38, 1997, pp 209 - 221. \n10. A. Chiuso and G. Picci. Asymptotic Variance of Subspace Estimates. \nProceedings of the 48th IEEE Conference on Decision and Control, Vol. 4, \nDec 2001, Orlando, Florida USA, pp 3910 - 3915. \n11.  N. F. Hunter Jr. Comparing CVA and ERA in Transfer Function \n            Measurement for Lithography Applications. Proceedings of the American \n            Control Conference, Vol. 2, 2-4 June 1999, San Diego, California, USA  \n            pp 1171 - 1175.  \n12. W .E Larimore. Statistical Optimality and Canonical Variate Analysis \nSystem Identification. Signal Processing, Vol. 52, 1996, pp 131 - 144. \n13. E. B. Martin and A. J. Morris. Non-Parametric Confidence Bounds for \nProcess Performance Monitoring Charts. Journal of Process Control, Vol. \n6, No. 6, 1996, pp 349 - 358. \n14. Q. Chen, P. Goulding, D. Sandoz and R. Wynne. Application of Kernel \nDensity Estimates to Condition Monitoring for Process Industries. \nProceedings of the American Control Conference, Vol. 6, 21-26 June \n1998, Philadelphia, PA, USA, pp 3312 - 3316. \n15. J. E. Jackson and G. S. Modholkar. Control Procedures for Residuals \nAssociated with Principal Component Analysis. Technometric, Vol. 21, \n1979, pp 341 - 349. \n 29\n16. A. W. Bowman and A. Azzalini. Applied Smoothing Techniques for Data \nAnalysis, The Kernel Approach with S-Plu Illustrations, Clarendon Press, \nOxford, 1997. \n17. S. Xiaoping and A. Sonali. Kernel Density Estimation For an Anomaly \nBased Intrusion Detection System. Proceedings of the 2006 World \nCongress in Computer Science, Computer Engineering and Applied \nComputing, Las Vegas Nevada 26 \u2013 29 June, 2006, pp 161 - 167.  \n18. J. J. Downs and E. F. Vogel. A Plant-wide Industrial Process Control \nProblem. Computers and Chemical Engineering, Vol. 17, No. 3, 1993, pp \n245 - 255. \n19. M. Kano, K. Nagao, S. Hasebe, I. Hashimoto, H. Ohno, R. Strauss and B. \nR. Bakshi. Comparison of Multivariate Statistical Process Monitoring \nMethods with Applications to the Eastman Challenge Problem. Computers \nand Chemical Engineering, Vol. 26, 2002, pp 161 - 174.  \n20. A. Simoglou, E. B. Martin and A. J. Morris. Statistical Performance \nMonitoring of Dynamic Multivariate Processes Using State Space \nModelling. Computers and Chemical Engineering, Vol. 26, 2002, pp 909 - \n920. \n21. W. E. Larimore. System Identification Reduced Order Filtering and \nModelling via Canonical Correlation Analysis. Proceedings of the \nAmerican Control Conference, San Francisco, USA, 1983, pp 445 - 451. \n22. B. C. Juricek, D. E. Seborg and W. E. Larimore .W .E. Fault Detection \nUsing Canonical Variate Analysis. Ind. Eng. Chem. Res, Vol. 43, 2004, pp \n458 - 474. \n23. T. Chen, J. Morris and E. Martin. Probability Density Estimation  \n 30\n 31\n                  via an Infinite Gaussian Mixture Model: Application to Statistical Process \n                  Monitoring. Appl. Statist, Vol. 55, Part 5, 2006, pp. 699 - 715.  \n24. A. Simoglou, E. B. Martin and A. J. Morris. A Comparison of Canonical \nVariate Analysis and Partial Least Squares for the Identification of \nDynamic Processes. Proceedings of the American Control Conference, 2-4 \nJune, 1999, San Diego, California, USA, pp 832 - 837. \n25. E. Tatara and A. Cinar. An Intelligent System for Multivariate Statistical \nProcess Monitoring and Diagnosis. Instrumentation, Systems and \nAutomation Society, Vol. 41, 2002, pp 255 - 270. \n26. A. C. Rencher. Methods of Multivariate Analysis. A Wiley-Interscience \nPublication. John Wiley and Sons, INC. New York, 1934, pp 132 - 146. \n27. M. Kano, K. Nagao, S. Hasebe, I. Hashimoto, H. Ohno, R. Strauss and B. \nR. Bakshi. Comparison of Multivariate Statistical Process Monitoring \nMethods with Applications to the Eastman Challenge Problem. Computers \nand Chemical Engineering, Vol. 26, 2002, pp 161 - 174.  \n \n \n \n \n \n"}