{"doi":"10.1109\/FUZZY.2007.4295393","coreId":"69984","oai":"oai:eprints.lancs.ac.uk:19217","identifiers":["oai:eprints.lancs.ac.uk:19217","10.1109\/FUZZY.2007.4295393"],"title":"Evolving single- and multi-model fuzzy classifiers with FLEXFIS-class","authors":["Lughofer, E.","Angelov, Plamen","Zhou, Xiaowei"],"enrichments":{"references":[{"id":16320168,"title":"Advanced Methods in Neural Computing.","authors":[],"date":"1993","doi":"10.1017\/s0269888900007372","raw":"P. Wasserman, Advanced Methods in Neural Computing. New York: Van Nostrand Reinhold, 1993.","cites":null},{"id":16320140,"title":"Advances in eeg signals classi\ufb01cation via dependant hmm models and evolving fuzzy classi\ufb01ers,\u201d","authors":[],"date":"2006","doi":"10.1016\/j.compbiomed.2005.09.006","raw":"C. Xydeas, P. Angelov, S. Chiao, and M. Reoullas, \u201cAdvances in eeg signals classi\ufb01cation via dependant hmm models and evolving fuzzy classi\ufb01ers,\u201d International Journal on Computers in Biology and Medicine, special issue on Intelligent Technologies for Bio-Informatics and Medicine, vol. 36, no. 10, pp. 1064\u20131083, 2006.","cites":null},{"id":16320152,"title":"An approach to online identi\ufb01cation of TakagiSugeno fuzzy models,\u201d","authors":[],"date":"2004","doi":"10.1109\/tsmcb.2003.817053","raw":"P. Angelov and D. Filev, \u201cAn approach to online identi\ufb01cation of TakagiSugeno fuzzy models,\u201d IEEE Trans. on Systems, Man and Cybernetics, part B, vol. 34, no. 1, pp. 484\u2013498, 2004.","cites":null},{"id":16320158,"title":"Applied Regression Analysis. Probability and Mathematical Statistics.","authors":[],"date":"1981","doi":"10.2307\/2985274","raw":"N. Draper and H. Smith, Applied Regression Analysis. Probability and Mathematical Statistics. New York: John Wiley & Sons, 1981.","cites":null},{"id":16320167,"title":"Classi\ufb01cation and Regression Trees. Boca Raton: Chapman and Hall,","authors":[],"date":"1993","doi":"10.2307\/2530946","raw":"L. Breiman, J. Friedman, C. Stone, and R. Olshen, Classi\ufb01cation and Regression Trees. Boca Raton: Chapman and Hall, 1993.","cites":null},{"id":16320126,"title":"Creating fuzzy rules for image classi\ufb01cation using biased data clustering,\u201d","authors":[],"date":"1999","doi":"10.1117\/12.341077","raw":"R. Santos, E. Dougherty, and J. A. Jaakko, \u201cCreating fuzzy rules for image classi\ufb01cation using biased data clustering,\u201d in SPIE proceedings series (SPIE proc. ser.) International Society for Optical Engineering proceedings series. Society of Photo-Optical Instrumentation Engineers, Bellingham, WA, 1999, pp. 151\u2013159.","cites":null},{"id":16320160,"title":"Data-driven evolving fuzzy systems using ets and \ufb02ex\ufb01s: Comparative analysis,\u201d to appear in","authors":[],"date":"2007","doi":"10.1080\/03081070701500059","raw":"P. Angelov and E. Lughofer, \u201cData-driven evolving fuzzy systems using ets and \ufb02ex\ufb01s: Comparative analysis,\u201d to appear in International Journal of General Systems, 2007.","cites":null},{"id":16320136,"title":"DENFIS: Dynamic evolving neural-fuzzy inference system and its application for time-series prediction,\u201d","authors":[],"date":"2002","doi":"10.1109\/91.995117","raw":"N. K. Kasabov and Q. Song, \u201cDENFIS: Dynamic evolving neural-fuzzy inference system and its application for time-series prediction,\u201d IEEE Trans. on Fuzzy Systems, vol. 10, no. 2, pp. 144\u2013154, 2002.","cites":null},{"id":16320141,"title":"Evolving fuzzy rule-based classi\ufb01ers,\u201d in","authors":[],"date":"2007","doi":"10.1109\/ciisp.2007.369172","raw":"P. Angelov, X. Zhou, and F. Klawonn, \u201cEvolving fuzzy rule-based classi\ufb01ers,\u201d in 2007 IEEE International Conference on Computational Intelligence Application for Signal and Image Processing, Honolulu, Hawaii, USA, 2007, to appear.","cites":null},{"id":16320165,"title":"Evolving fuzzy systems from data streams in real-time,\u201d","authors":[],"date":"2006","doi":"10.1109\/isefs.2006.251157","raw":"P. Angelov and X.-W. Zhou, \u201cEvolving fuzzy systems from data streams in real-time,\u201d in 2006 International Symposium on Evolving Fuzzy Systems, 2006, pp. 26\u201332.","cites":null},{"id":16320138,"title":"FLEXFIS: A variant for incremental learning of Takagi-Sugeno fuzzy systems,\u201d","authors":[],"date":"2005","doi":"10.1109\/fuzzy.2005.1452516","raw":"E. Lughofer and E. Klement, \u201cFLEXFIS: A variant for incremental learning of Takagi-Sugeno fuzzy systems,\u201d in Proceedings of FUZZIEEE 2005, Reno, Nevada, U.S.A., 2005, pp. 915\u2013920.","cites":null},{"id":16320128,"title":"From approximative to descriptive fuzzy classi\ufb01ers,\u201d","authors":[],"date":"2002","doi":"10.1109\/tfuzz.2002.800687","raw":"J. Q. S. Marin-Blazquez, \u201cFrom approximative to descriptive fuzzy classi\ufb01ers,\u201d IEEE Transactions on Fuzzy Systems, vol. 10, no. 4, pp. 484\u2013497, 2002.","cites":null},{"id":16320155,"title":"Fuzzy basis functions, universal approximation and orthogonal least-squares learning,\u201d","authors":[],"date":"1992","doi":"10.1109\/72.159070","raw":"L. Wang and J. Mendel, \u201cFuzzy basis functions, universal approximation and orthogonal least-squares learning,\u201d IEEE Trans. Neural Networks, vol. 3, no. 5, pp. 807\u2013814, 1992.","cites":null},{"id":16320148,"title":"Fuzzy identi\ufb01cation of systems and its applications to modeling and control,\u201d","authors":[],"date":"1985","doi":"10.1016\/b978-1-4832-1450-4.50045-6","raw":"T. Takagi and M. Sugeno, \u201cFuzzy identi\ufb01cation of systems and its applications to modeling and control,\u201d IEEE Trans. on Systems, Man and Cybernetics, vol. 15, no. 1, pp. 116\u2013132, 1985.","cites":null},{"id":16320132,"title":"Generating classi\ufb01cation rules with the neuro\u2013fuzzy system NEFCLASS,\u201d in","authors":[],"date":"1996","doi":"10.1109\/nafips.1996.534779","raw":"D. Nauck, U. Nauck, and R. Kruse, \u201cGenerating classi\ufb01cation rules with the neuro\u2013fuzzy system NEFCLASS,\u201d in Proc. Biennial Conference of the North American Fuzzy Information Processing Society NAFIPS\u201996, Berkeley, 1996.","cites":null},{"id":16320124,"title":"Handoff algorithms based on fuzzy classi\ufb01ers,\u201d","authors":[],"date":"2000","doi":"10.1109\/25.901898","raw":"H. Maturino-Lozoya, D. Munoz-Rodriguez, F. Jaimes-Romera, and H. Taw\ufb01k, \u201cHandoff algorithms based on fuzzy classi\ufb01ers,\u201d IEEE Transactions on Vehicular Technology, vol. 49, no. 6, pp. 2286\u20132294, 2000.","cites":null},{"id":16320143,"title":"Incremental learning of fuzzy basis function networks with a modi\ufb01ed version of vector quantization,\u201d","authors":[],"date":"2006","doi":null,"raw":"E. Lughofer and U. Bodenhofer, \u201cIncremental learning of fuzzy basis function networks with a modi\ufb01ed version of vector quantization,\u201d in Proceedings of IPMU 2006, volume 1, Paris, France, 2006, pp. 56\u201363.","cites":null},{"id":16320131,"title":"Learning fuzzy classi\ufb01cation rules from data,\u201d","authors":[],"date":"2003","doi":"10.1007\/978-3-7908-1829-1_13","raw":"J. Roubos, M. Setnes, and J. Abonyi, \u201cLearning fuzzy classi\ufb01cation rules from data,\u201d Information SciencesInformatics and Computer Science: An International Journal, vol. 150, pp. 77\u201393, 2003.","cites":null},{"id":16320129,"title":"Nefclass-x a soft computing tool to build readable fuzzy classi\ufb01ers,\u201d","authors":[],"date":"1998","doi":"10.1007\/10720181_6","raw":"D. Nauck and R. Kruse, \u201cNefclass-x a soft computing tool to build readable fuzzy classi\ufb01ers,\u201d BT Technology Journal, vol. 16, no. 3, pp. 180\u2013190, 1998.","cites":null},{"id":16320161,"title":"Online adaptation of Takagi-Sugeno fuzzy inference systems,\u201d","authors":[],"date":"2003","doi":"10.1109\/fuzzy.2006.1682014","raw":"E. Lughofer and E. Klement, \u201cOnline adaptation of Takagi-Sugeno fuzzy inference systems,\u201d in Proceedings of CESA\u20192003\u2014IMACS Multiconference, Lille, France, 2003, CD-Rom, paper S1-R-00-0175.","cites":null},{"id":16320134,"title":"Sequential adaptive fuzzy inference system (sa\ufb01s) for nonlinear system identi\ufb01cation and prediction,\u201d Fuzzy Sets and Systems,","authors":[],"date":"2006","doi":"10.1016\/j.fss.2005.12.011","raw":"N. S. H.-J. Rong, G.-B. Huang, and P. Saratchandran, \u201cSequential adaptive fuzzy inference system (sa\ufb01s) for nonlinear system identi\ufb01cation and prediction,\u201d Fuzzy Sets and Systems, vol. 157, no. 9, pp. 1260\u20131275, 2006.","cites":null},{"id":16320163,"title":"Simpl eTS: A simpli\ufb01ed method for learning evolving Takagi-Sugeno fuzzy models,\u201d","authors":[],"date":"2005","doi":"10.1109\/fuzzy.2005.1452543","raw":"P. Angelov and D. Filev, \u201cSimpl eTS: A simpli\ufb01ed method for learning evolving Takagi-Sugeno fuzzy models,\u201d in Proceedings of FUZZ-IEEE 2005, Reno, Nevada, U.S.A., 2005, pp. 1068\u20131073.","cites":null},{"id":16320149,"title":"System Identi\ufb01cation: Theory for the User. Upper Saddle River,","authors":[],"date":"1999","doi":null,"raw":"L. Ljung, System Identi\ufb01cation: Theory for the User. Upper Saddle River, New Jersey 07458: Prentice Hall PTR, Prentic Hall Inc., 1999.","cites":null},{"id":16320157,"title":"The Elements of Statistical Learning: Data Mining, Inference and Prediction.","authors":[],"date":"2001","doi":"10.1111\/j.1541-0420.2010.01516.x","raw":"T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning: Data Mining, Inference and Prediction. New York, Berlin, Heidelberg, Germany: Springer Verlag, 2001.","cites":null},{"id":16320153,"title":"The problem of concept drift: de\ufb01nitions and related work,\u201d","authors":[],"date":"2004","doi":null,"raw":"A. Tsymbal, \u201cThe problem of concept drift: de\ufb01nitions and related work,\u201d Department of Computer Science, Trinity College Dublin, Ireland, Tech. Rep. TCD-CS-2004-15, 2004.","cites":null},{"id":16320146,"title":"Vector quantization,\u201d","authors":[],"date":"1984","doi":"10.1007\/978-1-4615-3626-0_16","raw":"R. Gray, \u201cVector quantization,\u201d IEEE ASSP Magazine, pp. 4\u201329, 1984.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2007-07-24","abstract":"[2] R. Santos, E. Dougherty, and J. A. Jaakko, \u201cCreating fuzzy rules for image classification using biased data clustering,\u201d in SPIE proceedings series (SPIE proc. ser.) International Society for Optical Engineering proceedings series. Society of Photo-Optical Instrumentation Engineers, Bellingham, WA, 1999, pp. 151\u2013159. [3] J. Q. S. Marin-Blazquez, \u201cFrom approximative to descriptive fuzzy classifiers,\u201d IEEE Transactions on Fuzzy Systems, vol. 10, no. 4, pp. 484\u2013497, 2002. [4] D. Nauck and R. Kruse, \u201cNefclass-x a soft computing tool to build readable fuzzy classifiers,\u201d BT Technology Journal, vol. 16, no. 3, pp. 180\u2013190, 1998. [5] J. Roubos, M. Setnes, and J. Abonyi, \u201cLearning fuzzy classification rules from data,\u201d Information SciencesInformatics and Computer Science: An International Journal, vol. 150, pp. 77\u201393, 2003. [6] D. Nauck, U. Nauck, and R. Kruse, \u201cGenerating classification rules with the neuro\u2013fuzzy system NEFCLASS,\u201d in Proc. Biennial Conference of the North American Fuzzy Information Processing Society NAFIPS\u201996, Berkeley, 1996. [7] N. S. H.-J. Rong, G.-B. Huang, and P. Saratchandran, \u201cSequential adaptive fuzzy inference system (safis) for nonlinear system identification and prediction,\u201d Fuzzy Sets and Systems, vol. 157, no. 9, pp. 1260\u20131275, 2006. [8] N. K. Kasabov and Q. Song, \u201cDENFIS: Dynamic evolving neural-fuzzy inference system and its application for time-series prediction,\u201d IEEE Trans. on Fuzzy Systems, vol. 10, no. 2, pp. 144\u2013154, 2002. [9] E. Lughofer and E. Klement, \u201cFLEXFIS: A variant for incremental learning of Takagi-Sugeno fuzzy systems,\u201d in Proceedings of FUZZIEEE 2005, Reno, Nevada, U.S.A., 2005, pp. 915\u2013920. [10] C. Xydeas, P. Angelov, S. Chiao, and M. Reoullas, \u201cAdvances in eeg signals classification via dependant hmm models and evolving fuzzy classifiers,\u201d International Journal on Computers in Biology and Medicine, special issue on Intelligent Technologies for Bio-Informatics and Medicine, vol. 36, no. 10, pp. 1064\u20131083, 2006. [11] P. Angelov, X. Zhou, and F. Klawonn, \u201cEvolving fuzzy rule-based classifiers,\u201d in 2007 IEEE International Conference on Computational Intelligence Application for Signal and Image Processing, Honolulu, Hawaii, USA, 2007, to appear. [12] E. Lughofer and U. Bodenhofer, \u201cIncremental learning of fuzzy basis function networks with a modified version of vector quantization,\u201d in Proceedings of IPMU 2006, volume 1, Paris, France, 2006, pp. 56\u201363. [13] R. Gray, \u201cVector quantization,\u201d IEEE ASSP Magazine, pp. 4\u201329, 1984. [14] T. Takagi and M. Sugeno, \u201cFuzzy identification of systems and its applications to modeling and control,\u201d IEEE Trans. on Systems, Man and Cybernetics, vol. 15, no. 1, pp. 116\u2013132, 1985. [15] L. Ljung, System Identification: Theory for the User. Upper Saddle River, New Jersey 07458: Prentice Hall PTR, Prentic Hall Inc., 1999. [16] P. Angelov and D. Filev, \u201cAn approach to online identification of Takagi- Sugeno fuzzy models,\u201d IEEE Trans. on Systems, Man and Cybernetics, part B, vol. 34, no. 1, pp. 484\u2013498, 2004. [17] A. Tsymbal, \u201cThe problem of concept drift: definitions and related work,\u201d Department of Computer Science, Trinity College Dublin, Ireland, Tech. Rep. TCD-CS-2004-15, 2004. [18] L. Wang and J. Mendel, \u201cFuzzy basis functions, universal approximation and orthogonal least-squares learning,\u201d IEEE Trans. Neural Networks, vol. 3, no. 5, pp. 807\u2013814, 1992. [19] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning: Data Mining, Inference and Prediction. New York, Berlin, Heidelberg, Germany: Springer Verlag, 2001. [20] N. Draper and H. Smith, Applied Regression Analysis. Probability and Mathematical Statistics. New York: John Wiley & Sons, 1981. [21] P. Angelov and E. Lughofer, \u201cData-driven evolving fuzzy systems using ets and flexfis: Comparative analysis,\u201d to appear in International Journal of General Systems, 2007. [22] E. Lughofer and E. Klement, \u201cOnline adaptation of Takagi-Sugeno fuzzy inference systems,\u201d in Proceedings of CESA\u20192003\u2014IMACS Multiconference, Lille, France, 2003, CD-Rom, paper S1-R-00-0175. [23] P. Angelov and D. Filev, \u201cSimpl eTS: A simplified method for learning evolving Takagi-Sugeno fuzzy models,\u201d in Proceedings of FUZZ-IEEE 2005, Reno, Nevada, U.S.A., 2005, pp. 1068\u20131073. [24] P. Angelov and X.-W. Zhou, \u201cEvolving fuzzy systems from data streams in real-time,\u201d in 2006 International Symposium on Evolving Fuzzy Systems, 2006, pp. 26\u201332. [25] L. Breiman, J. Friedman, C. Stone, and R. Olshen, Classification and Regression Trees. Boca Raton: Chapman and Hall, 1993. [26] P. Wasserman, Advanced Methods in Neural Computing. New York: Van Nostrand Reinhold, 1993","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/69984.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/19217\/1\/1123.pdf","pdfHashValue":"57f6d850265f851f55d8c6a5aa3308b34a26199e","publisher":"IEEE","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:19217<\/identifier><datestamp>\n      2018-01-24T02:07:10Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D626F6F6B5F73656374696F6E<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Evolving single- and multi-model fuzzy classifiers with FLEXFIS-class<\/dc:title><dc:creator>\n        Lughofer, E.<\/dc:creator><dc:creator>\n        Angelov, Plamen<\/dc:creator><dc:creator>\n        Zhou, Xiaowei<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        [2] R. Santos, E. Dougherty, and J. A. Jaakko, \u201cCreating fuzzy rules for image classification using biased data clustering,\u201d in SPIE proceedings series (SPIE proc. ser.) International Society for Optical Engineering proceedings series. Society of Photo-Optical Instrumentation Engineers, Bellingham, WA, 1999, pp. 151\u2013159. [3] J. Q. S. Marin-Blazquez, \u201cFrom approximative to descriptive fuzzy classifiers,\u201d IEEE Transactions on Fuzzy Systems, vol. 10, no. 4, pp. 484\u2013497, 2002. [4] D. Nauck and R. Kruse, \u201cNefclass-x a soft computing tool to build readable fuzzy classifiers,\u201d BT Technology Journal, vol. 16, no. 3, pp. 180\u2013190, 1998. [5] J. Roubos, M. Setnes, and J. Abonyi, \u201cLearning fuzzy classification rules from data,\u201d Information SciencesInformatics and Computer Science: An International Journal, vol. 150, pp. 77\u201393, 2003. [6] D. Nauck, U. Nauck, and R. Kruse, \u201cGenerating classification rules with the neuro\u2013fuzzy system NEFCLASS,\u201d in Proc. Biennial Conference of the North American Fuzzy Information Processing Society NAFIPS\u201996, Berkeley, 1996. [7] N. S. H.-J. Rong, G.-B. Huang, and P. Saratchandran, \u201cSequential adaptive fuzzy inference system (safis) for nonlinear system identification and prediction,\u201d Fuzzy Sets and Systems, vol. 157, no. 9, pp. 1260\u20131275, 2006. [8] N. K. Kasabov and Q. Song, \u201cDENFIS: Dynamic evolving neural-fuzzy inference system and its application for time-series prediction,\u201d IEEE Trans. on Fuzzy Systems, vol. 10, no. 2, pp. 144\u2013154, 2002. [9] E. Lughofer and E. Klement, \u201cFLEXFIS: A variant for incremental learning of Takagi-Sugeno fuzzy systems,\u201d in Proceedings of FUZZIEEE 2005, Reno, Nevada, U.S.A., 2005, pp. 915\u2013920. [10] C. Xydeas, P. Angelov, S. Chiao, and M. Reoullas, \u201cAdvances in eeg signals classification via dependant hmm models and evolving fuzzy classifiers,\u201d International Journal on Computers in Biology and Medicine, special issue on Intelligent Technologies for Bio-Informatics and Medicine, vol. 36, no. 10, pp. 1064\u20131083, 2006. [11] P. Angelov, X. Zhou, and F. Klawonn, \u201cEvolving fuzzy rule-based classifiers,\u201d in 2007 IEEE International Conference on Computational Intelligence Application for Signal and Image Processing, Honolulu, Hawaii, USA, 2007, to appear. [12] E. Lughofer and U. Bodenhofer, \u201cIncremental learning of fuzzy basis function networks with a modified version of vector quantization,\u201d in Proceedings of IPMU 2006, volume 1, Paris, France, 2006, pp. 56\u201363. [13] R. Gray, \u201cVector quantization,\u201d IEEE ASSP Magazine, pp. 4\u201329, 1984. [14] T. Takagi and M. Sugeno, \u201cFuzzy identification of systems and its applications to modeling and control,\u201d IEEE Trans. on Systems, Man and Cybernetics, vol. 15, no. 1, pp. 116\u2013132, 1985. [15] L. Ljung, System Identification: Theory for the User. Upper Saddle River, New Jersey 07458: Prentice Hall PTR, Prentic Hall Inc., 1999. [16] P. Angelov and D. Filev, \u201cAn approach to online identification of Takagi- Sugeno fuzzy models,\u201d IEEE Trans. on Systems, Man and Cybernetics, part B, vol. 34, no. 1, pp. 484\u2013498, 2004. [17] A. Tsymbal, \u201cThe problem of concept drift: definitions and related work,\u201d Department of Computer Science, Trinity College Dublin, Ireland, Tech. Rep. TCD-CS-2004-15, 2004. [18] L. Wang and J. Mendel, \u201cFuzzy basis functions, universal approximation and orthogonal least-squares learning,\u201d IEEE Trans. Neural Networks, vol. 3, no. 5, pp. 807\u2013814, 1992. [19] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning: Data Mining, Inference and Prediction. New York, Berlin, Heidelberg, Germany: Springer Verlag, 2001. [20] N. Draper and H. Smith, Applied Regression Analysis. Probability and Mathematical Statistics. New York: John Wiley & Sons, 1981. [21] P. Angelov and E. Lughofer, \u201cData-driven evolving fuzzy systems using ets and flexfis: Comparative analysis,\u201d to appear in International Journal of General Systems, 2007. [22] E. Lughofer and E. Klement, \u201cOnline adaptation of Takagi-Sugeno fuzzy inference systems,\u201d in Proceedings of CESA\u20192003\u2014IMACS Multiconference, Lille, France, 2003, CD-Rom, paper S1-R-00-0175. [23] P. Angelov and D. Filev, \u201cSimpl eTS: A simplified method for learning evolving Takagi-Sugeno fuzzy models,\u201d in Proceedings of FUZZ-IEEE 2005, Reno, Nevada, U.S.A., 2005, pp. 1068\u20131073. [24] P. Angelov and X.-W. Zhou, \u201cEvolving fuzzy systems from data streams in real-time,\u201d in 2006 International Symposium on Evolving Fuzzy Systems, 2006, pp. 26\u201332. [25] L. Breiman, J. Friedman, C. Stone, and R. Olshen, Classification and Regression Trees. Boca Raton: Chapman and Hall, 1993. [26] P. Wasserman, Advanced Methods in Neural Computing. New York: Van Nostrand Reinhold, 1993.<\/dc:description><dc:publisher>\n        IEEE<\/dc:publisher><dc:date>\n        2007-07-24<\/dc:date><dc:type>\n        Contribution in Book\/Report\/Proceedings<\/dc:type><dc:type>\n        NonPeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/19217\/1\/1123.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/FUZZY.2007.4295393<\/dc:relation><dc:identifier>\n        Lughofer, E. and Angelov, Plamen and Zhou, Xiaowei (2007) Evolving single- and multi-model fuzzy classifiers with FLEXFIS-class. In: Fuzzy Systems Conference, 2007. FUZZ-IEEE 2007. IEEE International. IEEE, pp. 363-368. ISBN 1-4244-1209-9<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/19217\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1109\/FUZZY.2007.4295393","http:\/\/eprints.lancs.ac.uk\/19217\/"],"year":2007,"topics":["QA75 Electronic computers. Computer science"],"subject":["Contribution in Book\/Report\/Proceedings","NonPeerReviewed"],"fullText":"Evolving Single- And Multi-Model Fuzzy\nClassifiers with FLEXFIS-Class\nEdwin Lughofer, Plamen Angelov and Xiaowei Zhou\nAbstract\u2014In this paper a new method for training single-\nmodel and multi-model fuzzy classifiers incrementally and adap-\ntively is proposed, which is called FLEXFIS-Class. The evolving\nscheme for the single-model case exploits a conventional zero-\norder fuzzy classification model architecture with Gaussian fuzzy\nsets in the rules antecedents, crisp class labels in the rule\nconsequents and rule weights standing for confidence values in\nthe class labels. In the multi-model case FLEXFIS-Class exploits\nthe idea of regression by an indicator matrix to evolve a Takagi-\nSugeno fuzzy model for each separate class and combines the\nsingle models\u2019 predictions to a final classification statement. The\npaper includes a technique for increasing the prediction quality,\nwhenever a drift in a data stream occurs. An empirical analysis\nwill be given based on an online, adaptive image classification\nframework, where images showing production items should be\nclassified into good or bad ones. This analysis will include the\ncomparison of evolving single- and multi-model fuzzy classifiers\nwith conventional batch modelling approaches with respect to\nachieved prediction accuracy on new online data. It will also\nbe shown that multi-model architecture can outperform conven-\ntional single-model architecture (\u2019classical\u2019 fuzzy classification\nmodels) for all data sets with respect to prediction accuracy.\nIndex Terms\u2014evolving fuzzy classifiers, single- and multi-\nmodel architecture, incremental training, regression by indicator\nmatrix, process safety, data drift, image classification framework\nI. INTRODUCTION\nIn the contemporary industrial systems data-driven fuzzy\nclassifiers are applied for decision making [1], fault detection\nor image classification tasks [2] quite often. The reason is\nnot only a high predictive quality [3], which can be achieved\nwhen applying these types of classifiers, but also the good\ntransparency in the form of linguistic rules that shows under-\nstandable dependencies between the features [4]. This is an\nessential point e.g. for finding reasons of a faulty system state.\nFurthermore, confidence values for models\u2019 decisions can be\ncalculated easily based on fulfillment degrees of the rules.\nThese confidence values usually support the operators, as they\npoint out the trustworthiness of the classifier\u2019s decisions. In\ncase when the data is recorded online with a high frequency,\nthe training of fuzzy classifiers has to be carried out in an\nincremental and evolving manner on a sample-per-sample\nbasis, because a complete rebuilding of the classifier with\nall the recorded data contradicts with the online demands.\nEdwin Lughofer is with the Department of Knowledge-based Mathematical\nSystems, Johannes Kepler University of Linz, A-4040 Linz, Austria (e-mail:\nedwin.lughofer@jku.at)\nPlamen Angelov and Xiaowei Zhou are with Department of Communica-\ntion Systems, Infolab21, Lancaster University, Lancaster, LA1 4WA, United\nKingdom (e-mail: p.angelov@lancaster.ac.uk and x.zhou3@lancaster.ac.uk )\nMoreover, some operators may overrule a decision of the\nclassifier during online operation. Another reason that requires\nincremental and evolving learning techniques is the usually\nhuge size of the data bases.\nA number of papers exist that consider fuzzy rule-based\nclassifiers [5] [6]. To our best knowledge, the evolving fuzzy\nand neuro-fuzzy modelling techniques such as SAFIS [7],\nDENFIS [8], FLEXFIS [9] etc. has not yet been applied\nto classification. The only incremental and evolving fuzzy\napproaches for classification that we are aware of are [10] and\n[11]. The first one introduces the eClass method which applies\nall-in-one fuzzy rule-base which contains sub-rule-bases per\nclass and uses zero order (singleton) consequents. This concept\nis taken further in [11] where a first order multi-input-multi-\noutput (MIMO) Takagi-Sugeno non-linear model is used.\nIn this paper a different approach is proposed, FLEXFIS-\nClass, which uses both, multi-model architecture based on\nthe idea of regression by an indicator matrix (FLEXFIS-\nClass MM) and single-model architecture (\u2019classical\u2019 fuzzy\nclassification models) (FLEXFIS-Class SM). Both variants of\nFLEXFIS-Class are basically deduced from FLEXFIS [9],\nwhich serves as an evolving method for building up fuzzy\nregression models fully automatically and adaptively with new\nincoming data points (from measurement signals, data streams\netc.). For both model architectures, the evolving mechanism\nfor the rule antecedent parts takes place in the clusters space\nby using an evolving version of vector quantization [12]\n(the original VQ in [13]), including cluster evolution, an\nalternative winner selection strategy and updates of cluster\nsurfaces synchronously to their centres. In the single-model\ncase the evolution of the consequents (=single class labels)\nand rule weights is based on a plurality choice and relative\nfrequency of classes in the different clusters (rules), which\nboth can be updated sample-wise (Section II). In the multi-\nmodel case one Takagi-Sugeno fuzzy regression model [14] is\ntrained for each separate class and their continuous outputs are\naggregated to an overall classification statements (Section III).\nThe incremental training of the consequents in the TS models\n(hyper-planes) is carried out by exploiting recursive weighted\nleast squares [15] (also applied for consequent adaptation in\n[16]). Improving the fuzzy classifiers towards approximation\naccuracy (dealing with drifts in online data streams [17]) is\ndescribed in Section IV. The paper is concluded in Section\nV with an evaluation of the proposed approaches within an\nonline adaptive image classification framework and based\non a pen-digit recognition data set from the UCI-repository.\nThis evaluation includes a comparison of the impact of the\ntwo model architectures on prediction accuracy and model\n1-4244-1210-2\/07\/$25.00 \u00a92007 IEEE.\n363\ncomplexity as well as a comparison with other well-known\nbatch modelling methods.\nII. FLEXFIS-CLASS USING (CLASSICAL)\nSINGLE-MODEL ARCHITECTURE\nFor the single-model architecture case we exploit the \u2019clas-\nsical\u2019 architecture for fuzzy classifiers [5] [6], where the ith\nrule is defined in the following way:\nRulei : IF x1 IS \u00b5i1 AND...AND xp IS \u00b5ip THEN yi = ci\n(1)\nwhere p is the dimensionality of the input space, \u00b5i1, ..., \u00b5ip\nare the p antecedent fuzzy sets and ci is the crisp output class\nlabel from the set {1, ...,K} with K the number of classes.\nNow, for each new incoming sample ~x, the final crisp class\nlabel is obtained by taking the class label of that rule with the\nhighest activation degree, i.e. by calculating\ny = ci\u2217 with i\u2217 = argmax1\u2264i\u2264C \u00b5i (2)\nwith C the number of rules and \u00b5i the activation degree of\nrule i defined by (using product t-norm):\n\u00b5i =\np\u220f\ni=1\n\u00b5ij(xj)\nFurthermore, we introduce K rule weights wi1, ..., wiK ,\nwhich denote the confidence of the ith rule in all the K\nclasses, whereas the highest weight corresponds to the output\nclass ci. Note that confidence values are of great help for a\nbetter interpretation of the model\u2019s final output and quite often\ndemanded by operators in an industrial system, especially in\ncase of incorrect model\u2019s feedback. Based on the rule weights\nan overall confidence conf of the overall output class label\ny = k \u2208 {1, ...,K} is calculated through the following\nweighted average:\nconf =\n\u2211C\ni=1 wik\u00b5i\u2211C\ni=1 \u00b5i\n(3)\nNote that in case of a two-class classification task, the single-\nmodel architecture can be made even slimer, as the rule\nweights can be directly encoded in the consequent part of\nthe rules, serving as continuous output values between 0 =\nclass#1 and 1 = class#2. If for example a rule has consequent\nvalue of 0.6, this means that it is confident in class#2 with a\ndegree of 0.6 and in class#1 with a degree of 1\u2212 0.6 = 0.4.\nConsequently, if the value of the rule with highest activation\ndegree is greater or equal 0.5 the current data sample belongs\nto class#2, otherwise to class#1.\nThe adaptive and evolving training procedure for the rules\u2019\nantecedent parts is done with the help of an incremental\nand evolving clustering procedure that is modified, evolving\nversion of VQ [12] (eVQ), whose algorithm is summarised\nin the flowchart of Figure 1 (assuming normalized features).\nAfter each new incoming sample the updated\/generated cluster\nin the high-dimensional data space is projected onto the axes\nto form one-dimensional Gaussian fuzzy sets, i.e. \u00b5ij(xj) =\ne\n\u2212 12\n(xj\u2212cij)2\n\u03c32\nij . The fuzzy sets projected from one cluster on\nFig. 1. Workflow of evolving version of vector quantization [12] (assuming\nnormalized features)\neach axis form the antecedent part of one rule. In this sense,\nantecedent parts are permanently updated and new rules and\nfuzzy sets are born, whenever a new cluster is born according\nto Step 7a in Figure 1. Please note that the class labels are\nincluded during the whole learning procedure. The crisp and\nunique class label in the consequent part of the ith rule is\nelicited by counting the relative proportions of all the samples\nwhich formed the corresponding cluster to all the K classes\nand taking the maximal value of these proportions:\nci =\nK\nmax\nk=1\n(wik) =\nK\nmax\nk=1\n(\nnik\nni\n) (4)\nThese values can be updated by counting ni, the number\nof samples, which formed the antecedent part of the ith\nrule and nik the number of samples forming the antecedent\npart of the ith rule and falling into class k. The K rule\nweights wi1, ..., wiK are also obtained through (4). Combining\nevolving antecedent and consequent learning yields FLEXFIS-\nClass with single-model architecture, FLEXFIS-Class SM:\nAlgorithm 1: FLEXFIS-CLASS SM\n1) CollectN data points sufficient for the actual dimension-\nality of the problem, estimate the ranges of all features\nfrom these N points.\n2) Generate an initial fuzzy classifier with these N points\nby applying eVQ as in Fig. 1 (with features normalized\nby their ranges) and eliciting the consequent labels as\nin (4) for all clusters (=rules).\n3) Take the next incoming sample ~x, elicit its class label,\nsay k; either the data is pre-labelled or it can be for\n364\ninstance labelled by calculating (2) when using all the\nfuzzy rules obtained so far and obtaining feedback from\nan operator if the class output is correct.\n4) Proceed through evolving VQ as demonstrated in Figure\n1 from Step 3 on without going back to Step 2 (with\nfeatures normalized by their ranges).\n5) Project updated or newly generated cluster onto the input\nfeature axes, forming the complete antecedent part of\nthe corresponding rule. Hereby, the jth dimension of\nthe cluster prototype and width is assigned to the center\nand width of the Gaussian fuzzy set in the jth premise\npart.\n6) If a new rule is born, set ni = 1 and nik = 1.\n7) Else Update the corresponding consequent label and\nconfidence values in all K classes by setting ni = ni+1\nand nik = nik + 1 and using (4).\n8) Update ranges of features\n9) If new incoming data points are still available then goto\nStep 1; otherwise stop.\nIII. FLEXFIS-CLASS USING MULTI-MODEL\nARCHITECTURE\nFLEXFIS-Class MM is based on multiple Takagi-Sugeno\nfuzzy regression models [14] using a K indicator matrices for\nthe K classes which are generated from the original classifica-\ntion matrix, containing different features in different columns\nand a label entry in the last column. This is done for the ith\nmatrix by setting each label entry to 1, if the corresponding\nrow belongs to the ith class, otherwise it is set to 0. The\nfeature columns remain the same for all indicator matrices.\nIn this sense, it is guaranteed that the regression surface is\nforced towards 1 in the region of those samples belonging to\nits corresponding class and otherwise it approaches 0.\nThe overall output from the fuzzy classifier is calculated by\ninferencing a new multi-dimensional sample ~x through the K\nTakagi-Sugeno models (fuzzy basis function networks [18], as\na product t-norm in connection with Gaussian fuzzy sets):\nf\u02c6m(~x) =\nC\u2211\ni=1\nli\u03a8i(~x) m = 1, ...,K (5)\nwith the normalized membership functions\n\u03a8i(~x) =\ne\n\u2212 12\n\u2211p\nj=1\n(xj\u2212cij)2\n\u03c32\nij\u2211C\nk=1 e\n\u2212 12\n\u2211p\nj=1\n(xj\u2212ckj)2\n\u03c32\nkj\n(6)\nand consequent functions\nli = wi0 + wi1x1 + wi2x2 + ...+ wipxp (7)\nand then eliciting that model producing the maximal output\nand taking the corresponding class as label response, hence\ny = class(~x) = argmaxm=1,...,K f\u02c6m(~x) (8)\nIn [19] it is maintained that regression by an indicator\nmatrix only works well on two-class problems (0\/1) and can\nhave problems with multi-class problems, as then a complete\nmasking of one class by two or more others may happen.\nHowever, opposed to linear regression by an indicator ma-\ntrix [20], the TS fuzzy models are non-linear, i.e. not only\nthe (locally linear) rule consequents but also the non-linear\nantecedent parts are trained based on the indicator matrix\ninformation, i.e. by taking the label column as the (regressing)\ntarget variable. In this sense, the approximation behaviour is\nnon-linear which forces the surface of a model fk going to 0\nmore rapidly in regions apart from class k samples as in the\ncase of inflexible linear hyper-planes, therefore the masking\nis much weaker than in the pure linear case. In Section V-\nB an empirical verification of this point will be given based\non a high-dimensional data set containing 10 classes, where\nthe classification accuracy obtained by conventional batch\nmodelling approaches (which do not suffer by the masking\nproblem) can be approximated with FLEXFIS-Class MM quite\nwell, whereas linear regression by an indicator matrix is far\nbehind. An overall confidence value is elicited by normalizing\nthe maximal output value by the sum of the output values from\nall K models.\nNow the problem remains how to generate the K Takagi-\nSugeno fuzzy models in incremental and evolving manner.\nThis is done here with the original version of FLEXFIS [9],\nextended in [12] (FLEXFIS-MOD), compared with eTS in [21].\nThe basics of this algorithm are:\n\u2022 It applies eVQ [12] for incremental clustering, see Figure\n1\n\u2022 The updated cluster centers and surfaces are projected\nonto the axes after each incremental learning step in order\nto adapt the fuzzy sets and rules.\n\u2022 It uses recursive local learning by exploiting recursive\nweighted least squares [15], [16] for the linear consequent\nparameters.\nThe FLEXFIS-Class MM algorithm for building an evolving\nmulti-model fuzzy classifier can be summarized in the follow-\ning pseudo-code:\nAlgorithm 2: FLEXFIS-CLASS MM\n1) Collect N data points sufficient for the actual dimen-\nsionality of the problem, estimate ranges of all features\nfrom the N points.\n2) Generate K (initial) TS fuzzy models for K classes\npresent with N data points by using batch mode FLEX-\nFIS [9], i.e. eVQ as in Figure 1 (with normalized\nfeatures by their ranges) and axis-projection first and\nleast squares afterwards.\n3) Take the next incoming data point ~x; elicit its class label,\nsay k; either the data is pre-labelled or it can be for\ninstance labelled by calculating (2) when using all the\nfuzzy rules obtained so far and obtaining feedback from\nan operator if the class output is correct.\n4) Update ranges of features\n5) Update the kth TS fuzzy model by taking y = 1 as\nresponse (target) value and using FLEXFIS(-MOD) [9]\n[12]\n6) Update all other TS fuzzy models by taking y = 0 as\nresponse (target) value and using FLEXFIS(-MOD) [9]\n[12]\n7) If new incoming data points are still available then goto\n365\nStep 3; otherwise stop.\nIV. TRACKING DRIFT IN THE DATA\nMore accuracy can be achieved for specific data streams,\nwhenever a drift in the data appears after some time. For\nachieving a smooth tracking of a drift in the data, an expo-\nnential forgetting of older data points over time is applied. In\nFLEXFIS-Class MM this forgetting takes place exclusively in\nthe linear consequent parameters of all the TS fuzzy models\nsynchronously. This can be achieved by introducing a so-called\nforgetting factor \u03bb and incorporating them into the recursive\nweighted least squares formula [22] (here for the ith rule of\nany model):\n~\u02c6wi(k + 1) = ~\u02c6wi(k) + \u03b3(k)(y(k + 1)\u2212 ~rT (k + 1) ~\u02c6wi(k)) (9)\n\u03b3(k) =\nPi(k)~r(k + 1)\n\u03bb\n\u03a8i(~x(k+1))\n+ ~rT (k + 1)Pi(k)~r(k + 1)\n(10)\nPi(k + 1) = (I \u2212 \u03b3(k)~rT (k + 1))Pi(k) 1\n\u03bb\n(11)\nwith Pi(k) = (Ri(k)TQi(k)Ri(k))\u22121 the inverse weighted\nHesse matrix and ~r(k + 1) = [1 x1(k + 1) x2(k +\n1) . . . xp(k+1)]T the regressor values of the (k+1)th data\npoint. Note that in the case of \u03bb = 1 no forgetting takes\nplace, while with decreasing \u03bb the forgetting gets stronger\nand stronger. In this way, drifts in the trajectory of the TS\nfuzzy regression models can be tracked [21], which changes\nthe overall output of the fuzzy classifier more rapidly. For\nrecognizing a drift in the data the cluster ageing strategy\nas introduced in [23] and further developed in [24] can be\nexploited, as then slopes of the rule age curves tend to increase.\nA verification example for this point will be demonstrated in\nSection V-A.\nWhen using single-model architecture the forgetting of the\nconsequent class labels can be achieved by using a window\nwith fixed size, over which ni and nik are counted. Depending\non the window size, the forgetting is faster or slower. However,\nthis requires a batch storage of data samples in a buffer with\nthe same size as the chosen window. A better alternative is to\nfix a value \u000f and add this for each counting cycle, i.e. after N\nsamples belonging to the ith rule we get\nni =\nN\u2211\ni=1\n(1 + (i\u2212 1)\u000f) (12)\nThe same is carried out for nik. Depending on the size of \u000f\nolder points are forgotten faster or slower. For instance, when\nsetting \u000f = 1 and the first three points belonging to the ith\ncluster (=rule) fall into class#1 and the next two into class#2,\nthen we get ni = 1+2+3+4+5 = 15 and ni1 = 1+2+3 = 6\nwhile ni2 = 4 + 5 = 9, hence ci = 2, as the last two data\nsamples count more than the first three.\nV. APPLICATION EXAMPLES\nIn this section an evaluation of the two evolving classi-\nfication approaches, FLEXFIS-Class SM and FLEXFIS-Class\nMM, within two applications is demonstrated: a generic online\nadaptive image classification framework (from an EU-project)\nTABLE I\nCOMPARISON OF FLEXFIS-Class SM AND MM WITH BATCH CLASSIFIERS\nWHEN APPLYING THEM ONTO CD IMPRINT DATA\nMethod MC Rate Agg.\n\/ No. of Rules\n\/ No. of Inputs\n\/ Training Time\nFLEXFIS-Class SM 16.77% \/ 39 \/ 17 \/ 1.98s\nFLEXFIS-Class MM 9.02% \/ 62 \/ 17 \/ 3.56s\nFLEXFIS-Class MM + forg. 8.76% \/ 62 \/ 17 \/ 3.56s\nCART [25] 8.76%\nProbabilistic NN [26] 9.54%\nand recognition of pen-based hand-written digits. This eval-\nuation includes tests on the accuracy and complexity of the\nevolved classifiers. Furthermore, the impact on the accuracy\nof forgetting consequent parameters in the multi-model case\nwhen a drift in the data set is observed, will be pointed out.\nA. Image Classification Framework\nIn this section an application example is given, which in-\ncludes an automatically self-reconfigurable and adaptive fault\ndetection framework for images which classifies each image\nas good or bad, and evolves the classifier upon operator\u2019s\nfeedback and the data. The images are taken from an online\nproduction process with a high frequency with the aim to\nsupervise the system, as they may show errors in a production\nprocess. This framework including pre-processing, segmenta-\ntion and classification is shown in Fig. 2.\nIn principle, each type of image may be processed through\nthe classification framework as shown in Fig. 2. The only\nassumption is that a master image is available: the purpose\nis to generate deviation images by subtracting newly recorded\nimages from the master one in order to be able to classify\nthe image into good or bad (depending on the structure and\ncharacteristics of the deviation pixels). For the evaluation of\nour approaches we applied image data from a CD-imprint\nproduction process, where faults due to weak colours, wrong\npalettes etc. should be detected within a process frequency of\nabout one Hz. The data stream comprises 1164 images that\nwere recorded one by one. Out of these images 776 served as\ntraining samples for building up the classification models in\nan evolving manner and the remaining 388 samples (test data)\nwere used only for classification only (without an adaptation\nof the classification models, in order to be able to compare\nwith the batch modelling approaches). 17 aggregated features\nwere extracted, describing the distribution, density, shape etc.\nof the pixel fragments in the deviation images. The miss-\nclassification rates on this test data set are demonstrated in\nTable I. From this table it can be recognized, that FLEXFIS-\nClass MM can compete with two well-known batch modelling\nmethods, the decision tree-based classification method CART\nwith optimal pruning strategy [25] and possibilistic neural\nnetworks [26], with respect to classification accuracy. By\ntaking into account that the two renowned approaches access\nthe whole data set information at once, the applicability\nand worthiness of FLEXFIS-Class MM should be clearly\n366\nFig. 2. Classification framework for classifying images into good and bad\nunderlined. From this table it is also clear that FLEXFIS-\nClass MM significantly outperforms FLEXFIS-Class SM in\nterms of accuracy. This is because Takagi-Sugeno models offer\nmore flexibility, when regressing on the indicator matrix, than\nclassical classification models which strongly depend on the\nincremental clustering procedure for evolving the antecedent\nparts of the rules. Regarding time complexity, FLEXFIS-Class\nSM is superior to FLEXFIS-Class MM. This is because in\nFLEXFIS-Class MM two TS fuzzy models need to be updated\nsynchronously (one for class \u2019bad\u2019, one for class \u2019good\u2019), there\nare more consequent parameters to update (p+1 per rule versus\n1 per rule in case of FLEXFIS-Class SM) and the adaptation\nof the consequent parameters itself is much more complex, as\na parameter vector and an inverse matrix needs to be updated,\nwhereas in FLEXFIS-Class SM updating is done by simple\ncounting. However, both can cope with the on-line demand\nof this system, as the image data is usually loaded with a\nfrequency of about 1 Hz, whereas the training time in Table I\nis listed for the whole 776 training data samples.\nThe evolution of the accuracy over time was examined,\nwhen building up the classifier with FLEXFIS-Class MM step\nby step. Therefore, the fuzzy classification model was trained\nin incremental manner with 100, 200, 300, ..., 776 (all) data\nsamples and the development of the miss-classification rate\ntracked, see Figure 3. One can see that the classifier improves\nwith the number of samples, which shows the consistency\nwith respect to the amount of information provided by the\noperator. Furthermore, cluster ageing as introduced in [23] and\n[24] was applied for detecting a potential drift in the data set\n(indicating that one or more operating conditions changed).\nThis can be used as a trigger for exponential forgetting of\nolder data points in the linear consequent adaptation within\nFLEXFIS-Class MM (see Section IV). In fact, it turned out\nthat a small drift occurred for the last 80 data samples (as the\nslope of the age curves tend to increase), hence a forgetting\nwith \u03bb = 0.99 was initiated for the last 80 points. The results\nare listed in Row 3 of Table I; it can be seen that a some\ndecrease of the miss-classification rate can be achieved. The\ncomplexity and computational performance of the classifier\nare practically the same without forgetting, (only a single\nparameter value is included in the weighted RLS update for\nlinear consequent parameters, which does not affect the rule\nevolution and adaptation part). It should be also mentioned\nFig. 3. Performance of FLEXFIS-Class MM on CD print data with increasing\nnumber of (online) training samples\nthat a forgetting applied on the complete second half of the\ntraining data set worsened the miss-classification rates for both\ndata sets. This indicates that a correct detection of a drift to\ntrigger a forgetting is essential, when intending to improve the\naccuracy of the fuzzy classifier.\nB. Pen-Based Recognition of Handwritten Digits\nThis data set from the UCI repository1 was created by\ncollecting 250 samples from 44 writers, which was generated\nwith the use of a pressure sensitive tablet with an integrated\nLCD display and a cordless stylus. The data set contains\n16 features, 7494 training data samples and 3498 test data\nsamples, containing ten different classes (for the ten digits)\nwhich are almost equally distributed in both, training and test\ndata set. Principally, this data set is an off-line batch data\nset. However, we simulate it as an on-line pseudo-stream by\nperforming a loading of data samples and evolve the fuzzy\nclassifiers with each new incoming point separately.\nTable II shows the results when applying the two model\narchitectures for this data set. The conclusion is similar to\nthat one for the image data set: FLEXFIS-Class SM is more\ntransparent and less accurate, while FLEXFIS-Class MM pro-\nduced ten TS fuzzy models with in sum 61 rules in total, which\n1http:\/\/www.ics.uci.edu\/ mlearn\/MLRepository.html\n367\nTABLE II\nCOMPARISON OF EVOLVING VARIANTS TO WELL-KNOWN BATCH\nCLASSIFICATION METHODS ON HANDWRITTEN DIGITS DATA\nMethod Accuracy\n\/ No. of Rules \/ No. of Rules\nFLEXFIS-Class SM 88.77% \/ 16\nFLEXFIS-Class MM 96.23% \/ 61\nCART [25] 97.68%\nk-NN 97.40%\nLin. regr. with indicator mat. 82.0%\nis hardly interpretable, but could achieve an accuracy of over\n96% which comes quite close to the results of the decision\ntree classifier CART [25] as well as of k-NN. Taking into\naccount that the results for CART and k-NN were produced\nwith the best parameter grid search procedure (for CART\noptimal pruning strategy, for k-NN the parameter k was varied\nfrom 1 to 20) while FLEXFIS-Class MM was started with the\ndefault parameter setting, it underlines even more the strength\nof FLEXFIS-Class MM on this data set. Compared to linear\nregression by an indicator matrix, which usually suffers from\nthe masking problem when applied to a multi-classification\ntask, FLEXFIS-Class MM can significantly outperform this\nmethod.\nVI. CONCLUSION AND OUTLOOK\nTwo variants for evolving fuzzy classification schemes\nwere presented, FLEXFIS-Class SM based on single-model\narchitecture and FLEXFIS-Class MM based on multi-model\narchitecture. The key issues of their algorithms were pointed\nout in detail in Sections II and III, whereas a concept for\nreacting on drifts in the data was demonstrated in Section IV,\nimproving the accuracy of the model as shown in Section V-A.\nIn the evaluation section two applications were described: i)\nclassifying images into good or bad; ii) pen-based recognition\nof handwritten digits were described. Both methods produced\nreliable results; while FLEXFIS-Class SM produces a slim\nand transparent fuzzy model (with low number of rules and\nunique models), FLEXFIS-Class MM generates more complex\nmodels, but could achieve a higher accuracy, especially for\nthe CD imprint data set. Furthermore, a reliable connection of\ncluster\/rule ageing with forgetting of consequent parameters\n(as done in FLEXFIS-Class MM) could be achieved, triggering\na benefit in terms of a better accuracy of the resulting classifier.\nACKNOWLEDGEMENTS\nThis work was supported by the European Commission\n(project Contract No. STRP016429, acronym DynaVis). This\npublication reflects only the author\u2019s view.\nREFERENCES\n[1] H. Maturino-Lozoya, D. Munoz-Rodriguez, F. Jaimes-Romera, and\nH. Tawfik, \u201cHandoff algorithms based on fuzzy classifiers,\u201d IEEE\nTransactions on Vehicular Technology, vol. 49, no. 6, pp. 2286\u20132294,\n2000.\n[2] R. Santos, E. Dougherty, and J. A. Jaakko, \u201cCreating fuzzy rules for\nimage classification using biased data clustering,\u201d in SPIE proceedings\nseries (SPIE proc. ser.) International Society for Optical Engineering\nproceedings series. Society of Photo-Optical Instrumentation Engineers,\nBellingham, WA, 1999, pp. 151\u2013159.\n[3] J. Q. S. Marin-Blazquez, \u201cFrom approximative to descriptive fuzzy\nclassifiers,\u201d IEEE Transactions on Fuzzy Systems, vol. 10, no. 4, pp.\n484\u2013497, 2002.\n[4] D. Nauck and R. Kruse, \u201cNefclass-x a soft computing tool to build\nreadable fuzzy classifiers,\u201d BT Technology Journal, vol. 16, no. 3, pp.\n180\u2013190, 1998.\n[5] J. Roubos, M. Setnes, and J. Abonyi, \u201cLearning fuzzy classification rules\nfrom data,\u201d Information SciencesInformatics and Computer Science: An\nInternational Journal, vol. 150, pp. 77\u201393, 2003.\n[6] D. Nauck, U. Nauck, and R. Kruse, \u201cGenerating classification rules with\nthe neuro\u2013fuzzy system NEFCLASS,\u201d in Proc. Biennial Conference of\nthe North American Fuzzy Information Processing Society NAFIPS\u201996,\nBerkeley, 1996.\n[7] N. S. H.-J. Rong, G.-B. Huang, and P. Saratchandran, \u201cSequential adap-\ntive fuzzy inference system (safis) for nonlinear system identification\nand prediction,\u201d Fuzzy Sets and Systems, vol. 157, no. 9, pp. 1260\u20131275,\n2006.\n[8] N. K. Kasabov and Q. Song, \u201cDENFIS: Dynamic evolving neural-fuzzy\ninference system and its application for time-series prediction,\u201d IEEE\nTrans. on Fuzzy Systems, vol. 10, no. 2, pp. 144\u2013154, 2002.\n[9] E. Lughofer and E. Klement, \u201cFLEXFIS: A variant for incremental\nlearning of Takagi-Sugeno fuzzy systems,\u201d in Proceedings of FUZZ-\nIEEE 2005, Reno, Nevada, U.S.A., 2005, pp. 915\u2013920.\n[10] C. Xydeas, P. Angelov, S. Chiao, and M. Reoullas, \u201cAdvances in\neeg signals classification via dependant hmm models and evolving\nfuzzy classifiers,\u201d International Journal on Computers in Biology and\nMedicine, special issue on Intelligent Technologies for Bio-Informatics\nand Medicine, vol. 36, no. 10, pp. 1064\u20131083, 2006.\n[11] P. Angelov, X. Zhou, and F. Klawonn, \u201cEvolving fuzzy rule-based\nclassifiers,\u201d in 2007 IEEE International Conference on Computational\nIntelligence Application for Signal and Image Processing, Honolulu,\nHawaii, USA, 2007, to appear.\n[12] E. Lughofer and U. Bodenhofer, \u201cIncremental learning of fuzzy basis\nfunction networks with a modified version of vector quantization,\u201d in\nProceedings of IPMU 2006, volume 1, Paris, France, 2006, pp. 56\u201363.\n[13] R. Gray, \u201cVector quantization,\u201d IEEE ASSP Magazine, pp. 4\u201329, 1984.\n[14] T. Takagi and M. Sugeno, \u201cFuzzy identification of systems and its\napplications to modeling and control,\u201d IEEE Trans. on Systems, Man\nand Cybernetics, vol. 15, no. 1, pp. 116\u2013132, 1985.\n[15] L. Ljung, System Identification: Theory for the User. Upper Saddle\nRiver, New Jersey 07458: Prentice Hall PTR, Prentic Hall Inc., 1999.\n[16] P. Angelov and D. Filev, \u201cAn approach to online identification of Takagi-\nSugeno fuzzy models,\u201d IEEE Trans. on Systems, Man and Cybernetics,\npart B, vol. 34, no. 1, pp. 484\u2013498, 2004.\n[17] A. Tsymbal, \u201cThe problem of concept drift: definitions and related\nwork,\u201d Department of Computer Science, Trinity College Dublin, Ire-\nland, Tech. Rep. TCD-CS-2004-15, 2004.\n[18] L. Wang and J. Mendel, \u201cFuzzy basis functions, universal approximation\nand orthogonal least-squares learning,\u201d IEEE Trans. Neural Networks,\nvol. 3, no. 5, pp. 807\u2013814, 1992.\n[19] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical\nLearning: Data Mining, Inference and Prediction. New York, Berlin,\nHeidelberg, Germany: Springer Verlag, 2001.\n[20] N. Draper and H. Smith, Applied Regression Analysis. Probability and\nMathematical Statistics. New York: John Wiley & Sons, 1981.\n[21] P. Angelov and E. Lughofer, \u201cData-driven evolving fuzzy systems using\nets and flexfis: Comparative analysis,\u201d to appear in International Journal\nof General Systems, 2007.\n[22] E. Lughofer and E. Klement, \u201cOnline adaptation of Takagi-Sugeno fuzzy\ninference systems,\u201d in Proceedings of CESA\u20192003\u2014IMACS Multiconfer-\nence, Lille, France, 2003, CD-Rom, paper S1-R-00-0175.\n[23] P. Angelov and D. Filev, \u201cSimpl eTS: A simplified method for learning\nevolving Takagi-Sugeno fuzzy models,\u201d in Proceedings of FUZZ-IEEE\n2005, Reno, Nevada, U.S.A., 2005, pp. 1068\u20131073.\n[24] P. Angelov and X.-W. Zhou, \u201cEvolving fuzzy systems from data streams\nin real-time,\u201d in 2006 International Symposium on Evolving Fuzzy\nSystems, 2006, pp. 26\u201332.\n[25] L. Breiman, J. Friedman, C. Stone, and R. Olshen, Classification and\nRegression Trees. Boca Raton: Chapman and Hall, 1993.\n[26] P. Wasserman, Advanced Methods in Neural Computing. New York:\nVan Nostrand Reinhold, 1993.\n368\n"}