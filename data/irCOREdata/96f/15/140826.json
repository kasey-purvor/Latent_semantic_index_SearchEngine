{"doi":"10.1016\/j.ssci.2011.02.008","coreId":"140826","oai":"oai:dspace.lib.cranfield.ac.uk:1826\/6498","identifiers":["oai:dspace.lib.cranfield.ac.uk:1826\/6498","10.1016\/j.ssci.2011.02.008"],"title":"Human reliability analysis: A critique and review for managers","authors":["French, Simon","Bedford, Tim","Pollard, Simon J. T.","Soane, Emma"],"enrichments":{"references":[{"id":37919430,"title":"A \u2018Swiss cheese\u2019 model analysis of the risk management failures in the fatal Walkerton outbreak. IWA world water congress and exhibition,","authors":[],"date":"2006","doi":null,"raw":"Hrudey, S.E., E.J. Hrudey, J.W.A. Charrois, S.J.T. Pollard. 2006. A \u2018Swiss cheese\u2019 model analysis of the risk management failures in the fatal Walkerton outbreak. IWA world water congress and exhibition, Beijing, China.- 25 -International Atomic Energy Agency. 1991. The International Chernobyl Project: Technical Report. IAEA, Vienna.","cites":null},{"id":37919522,"title":"A leader's framework for decision making.","authors":[],"date":"2007","doi":null,"raw":"Snowden, D., M. Boone. 2007. A leader's framework for decision making. Harvard Business Review 68-76.","cites":null},{"id":37919497,"title":"A methodology to model causal relationships in offshore safety assessment focusing on human and organisational factors.","authors":[],"date":"2008","doi":"10.1016\/j.jsr.2007.09.009","raw":"Ren, J., I. Jenkinson, J. Wang, D.L. Xu, J.B. Yang. 2008. A methodology to model causal relationships in offshore safety assessment focusing on human and organisational factors. Journal of Safety Research 39 87-100.","cites":null},{"id":37919442,"title":"A recognition primed decision model (RPM) of rapid decision","authors":[],"date":"1993","doi":null,"raw":"Klein, G. 1993. A recognition primed decision model (RPM) of rapid decision making. G. Klein, ed. Decision Making in Action: Models and Method. Ablex.","cites":null},{"id":37919475,"title":"A socio-technical approach to systems design.","authors":[],"date":"2000","doi":"10.1007\/pl00010345","raw":"Mumford, E. 2000. A socio-technical approach to systems design. Requirements Engineering 5 125-133.","cites":null},{"id":37919478,"title":"A socio-technical perspective on the use of RODOS","authors":[],"date":"2005","doi":null,"raw":"Niculae, C. 2005. A socio-technical perspective on the use of RODOS in nuclear emergency management, The University of Manchester.","cites":null},{"id":37919541,"title":"An analysis of factors affecting software reliability.","authors":[],"date":"2000","doi":"10.1016\/s0164-1212(99)00075-8","raw":"Zhang, X., H. Pham. 2000. An analysis of factors affecting software reliability. Journal of Systems and Software 50(1) 43-56- 28 -","cites":null},{"id":37919445,"title":"Are organisations too complex to be integrated in technical risk assessment and current safety auditing?","authors":[],"date":"2005","doi":"10.1016\/j.ssci.2005.06.005","raw":"Le Coze, J.-C. 2005. Are organisations too complex to be integrated in technical risk assessment and current safety auditing? Safety Science 43(8) 613-638.","cites":null},{"id":37919496,"title":"Assessment of complex socio-technical systems \u2013 theorerical issues concerning the use of organisational culture and organisational core task concepts.","authors":[],"date":"2007","doi":"10.1016\/j.ssci.2006.07.010","raw":"Reiman, T., P. Oedewald. 2007. Assessment of complex socio-technical systems \u2013 theorerical issues concerning the use of organisational culture and organisational core task concepts. Safety Science 45 745-768.","cites":null},{"id":37919385,"title":"Attention and Self-Regulation: a Control Theory Approach to Human Behavior.","authors":[],"date":"1981","doi":"10.1007\/978-1-4612-5887-2_7","raw":"Carver, C.S., M.F. Scheier. 1981. Attention and Self-Regulation: a Control Theory Approach to Human Behavior. Springer Verlag, New York.","cites":null},{"id":37919372,"title":"Automaticity of social behavior: direct effects of trait construct and stereotype activation on action.","authors":[],"date":"1996","doi":"10.1037\/\/0022-3514.71.2.230","raw":"Bargh, J.A., M. Chen, L. Burrows. 1996. Automaticity of social behavior: direct effects of trait construct and stereotype activation on action. Journal of Personality and Social Psychology 71 230-244.","cites":null},{"id":37919395,"title":"Bayesian belief networks for safety assessment of computer-based systems.","authors":[],"date":"2000","doi":"10.1007\/978-1-4471-1480-2_5","raw":"Courtois, P.-J., B. Littlewood, L. Strigini, D. Wright, N. Fenton, M. Neil. 2000. Bayesian belief networks for safety assessment of computer-based systems. E. Gelenbe, ed. System Performance Evaluation: Methodologies and Applications. CRC Press, 349-363.- 24 -Fadier, E. 2008. Editorial of the Special Issue: Design Process and Human Factors Integration. Cognition, Technology and Work 10(1) 1-5.","cites":null},{"id":37919413,"title":"Believe in the Model: Mishandle the Emergency.","authors":[],"date":"2005","doi":"10.2202\/1547-7355.1108","raw":"French, S., C. Niculae. 2005. Believe in the Model: Mishandle the Emergency. Journal of Homeland Security and Emergency Management 2(1).","cites":null},{"id":37919439,"title":"Choices, Values and Frames.","authors":[],"date":"2000","doi":"10.1002\/pam.10145","raw":"Kahneman, D., A. Tversky, eds. 2000. Choices, Values and Frames. Cambridge University Press, Cambridge.","cites":null},{"id":37919423,"title":"Cognitive Reliability and Error Analysis Method \u2013 CREAM.","authors":[],"date":"1998","doi":"10.1016\/b978-008042848-2\/50005-1","raw":"Hollnagel, E. 1998. Cognitive Reliability and Error Analysis Method \u2013 CREAM. Elsevier Science, Oxford.","cites":null},{"id":37919447,"title":"Collective dimensions of reliability: some lines of research.","authors":[],"date":"1994","doi":"10.1080\/13594329408410489","raw":"Leplat, J. 1994. Collective dimensions of reliability: some lines of research. European Work and Organizational Ppsychologist 4(3) 271-295.","cites":null},{"id":37919529,"title":"Collective mind in organizations: heedful interrelating on flight decks.","authors":[],"date":"1993","doi":"10.2307\/2393372","raw":"Weick, K.E., K.H. Roberts. 1993. Collective mind in organizations: heedful interrelating on flight decks. Administrative Science Quarterly 38 357-381.","cites":null},{"id":37919394,"title":"Commission on the Three Mile Island Accident.","authors":[],"date":"1979","doi":"10.2172\/373890","raw":"Commission on the Three Mile Island Accident. 1979. Report of The President's Commission on the Accident at Three Miles Island. US GPO, Washington DC.","cites":null},{"id":37919520,"title":"Complex acts of knowing - paradox and descriptive self-awareness.","authors":[],"date":"2002","doi":"10.1108\/13673270210424639","raw":"Snowden, D. 2002. Complex acts of knowing - paradox and descriptive self-awareness. Journal of Knowledge Management 6 100-111.","cites":null},{"id":37919499,"title":"Complexity, tight-coupling and reliability: connecting normal accidents theory and high reliability theory.","authors":[],"date":"1997","doi":"10.1111\/1468-5973.00033","raw":"Rijpma, J.A. 1997. Complexity, tight-coupling and reliability: connecting normal accidents theory and high reliability theory. Journal of Contingencies and Crisis Management, 5(1).","cites":null},{"id":37919411,"title":"Decision Behaviour, Analysis and Support.","authors":[],"date":"2009","doi":"10.1017\/cbo9780511609947.001","raw":"French, S., A.J. Maule, K.N. Papamichail. 2009. Decision Behaviour, Analysis and Support. Cambridge University Press, Cambridge.","cites":null},{"id":37919487,"title":"Developing a risk management culture \u2014 \u2018mindfulness\u2019 in the international water utility sector (Report TC3184). Water Research Foundation,","authors":[],"date":"2009","doi":null,"raw":"Pollard, S., R. Bradshaw, D. Tranfield, J.W.A. Charrois, N. Cromar, D. Jalba. 2009. Developing a risk management culture \u2014 \u2018mindfulness\u2019 in the international water utility sector (Report TC3184). Water Research Foundation, Denver, CO.","cites":null},{"id":37919392,"title":"Drs Pangloss and Strangelove meet organizational theory: high reliability organizations and nuclear weapons accidents.","authors":[],"date":"1993","doi":"10.1007\/bf01115218","raw":"Clarke, L. 1993. Drs Pangloss and Strangelove meet organizational theory: high reliability organizations and nuclear weapons accidents. Sociological Forum 8 675-689.","cites":null},{"id":37919383,"title":"Dynamic human reliability analysis: benefits and challenges of simulating human performance","authors":[],"date":"2007","doi":null,"raw":"Boring, R.L. 2007. Dynamic human reliability analysis: benefits and challenges of simulating human performance European Safety and Reliability Conference (ESREL 2007). INL\/CON-07-12773, Idaho National Laboratory.","cites":null},{"id":37919485,"title":"Emotion and cognition: Insights from studies of the human amygdala.","authors":[],"date":"2006","doi":"10.1146\/annurev.psych.56.091103.070234","raw":"Phelps, E.A. 2006. Emotion and cognition: Insights from studies of the human amygdala. Annual Review of Psychology 57 27-53.","cites":null},{"id":37919467,"title":"Encyclopedia of Quantitative Risk Analysis and Assessment.","authors":[],"date":"2008","doi":"10.1002\/9780470061596.risk0518","raw":"Melnick, E.L., B.S. Everitt, eds. 2008. Encyclopedia of Quantitative Risk Analysis and Assessment. John Wiley and Sons, Chichester.","cites":null},{"id":37919365,"title":"Foundation of Risk Analysis: a Knowledge and Decision Oriented Perspective.","authors":[],"date":"2003","doi":null,"raw":"Aven, T. 2003. Foundation of Risk Analysis: a Knowledge and Decision Oriented Perspective. John Wiley and Sons, Chichester.","cites":null},{"id":37919381,"title":"Fractured pipe with loss of primary containment in the THORP feed clarification cell. British Nuclear Fuels Limited.","authors":[],"date":"2005","doi":null,"raw":"Board of Inquiry. 2005. Fractured pipe with loss of primary containment in the THORP feed clarification cell. British Nuclear Fuels Limited.","cites":null},{"id":37919523,"title":"Handbook of Human Reliability Analysis with Emphasis on Nuclear Power Plant Applications.","authors":[],"date":"1983","doi":"10.2172\/5752058","raw":"Swain, A.D., H.E. Guttmann. 1983. Handbook of Human Reliability Analysis with Emphasis on Nuclear Power Plant Applications. NUREG\/CR-1278, USNRC.","cites":null},{"id":37919457,"title":"Handbook of Software Reliability Engineering.","authors":[],"date":"2005","doi":"10.1109\/fose.2007.24","raw":"Lyu, M.R. 2005. Handbook of Software Reliability Engineering. IEEE Computer Society Press and McGrawHill Publishing Company.","cites":null},{"id":37919535,"title":"HEART \u2013 A proposed method for achieving high reliability in process operation by means of human factors engineering technology","authors":[],"date":"1985","doi":null,"raw":"Williams, J.C. 1985. HEART \u2013 A proposed method for achieving high reliability in process operation by means of human factors engineering technology Proceedings of a Symposium on the Achievement of Reliability in Operating Plant, Safety and Reliability Society, NEC, Birmingham.","cites":null},{"id":37919390,"title":"Heuristic and systematic information processing within and beyond the persuasion context.","authors":[],"date":"1989","doi":null,"raw":"Chaiken, S., A. Liberman, A.H. Eagly. 1989. Heuristic and systematic information processing within and beyond the persuasion context. J.S. Uleman, J.A. Bargh, eds. Unintended Thought. Guilford, New York, 212-252.","cites":null},{"id":37919444,"title":"High reliability organizations: unlikely, demanding and at risk.","authors":[],"date":"1996","doi":"10.1111\/j.1468-5973.1996.tb00078.x","raw":"La Porte, T.R. 1996. High reliability organizations: unlikely, demanding and at risk. Journal of Contingencies and Crisis Management 4 60-71.","cites":null},{"id":37919397,"title":"How to integrate safety in design: methods and models.","authors":[],"date":"1999","doi":"10.1002\/(sici)1520-6564(199923)9:4<367::aid-hfm4>3.3.co;2-1","raw":"Fadier, E., J. Ciccotelli. 1999. How to integrate safety in design: methods and models. Human Factors and Ergonomics in Manufacturing & Service Industries 9(4) 367-379.","cites":null},{"id":37919418,"title":"Human cognitive reliability model for PRA analysis.","authors":[],"date":"1984","doi":null,"raw":"Hannaman, G.W., A.J. Spurgin, Y.D. Lukic. 1984. Human cognitive reliability model for PRA analysis. Draft Report NUS-4531, EPRI Project RP2170-3. Electric Power and Research Institute, Palo Alto, CA.","cites":null},{"id":37919492,"title":"Human error: models and management.","authors":[],"date":"1990","doi":"10.1017\/cbo9781139062367","raw":"Reason, J. 1990b. Human error: models and management. British Medical Journal 320(7237) 768-770.","cites":null},{"id":37919538,"title":"Human reliability analysis has a role in preventing drinking water incidents Water Research(in press).","authors":[],"date":"2009","doi":"10.1016\/j.watres.2009.04.040","raw":"Wu, S., S.E. Hrudey, S. French, T. Bedford, E. Soane, S.J.T. Pollard. 2009. Human reliability analysis has a role in preventing drinking water incidents Water Research(in press).","cites":null},{"id":37919362,"title":"Human Reliability Analysis: A Review and Critique. Manchester Business School, Booth Street West,","authors":[],"date":"2008","doi":null,"raw":"Adhikari, S., C. Bayley, T. Bedford, J.S. Busby, A. Cliffe, G. Devgun, M. Eid, S. French, R. Keshvala, S. Pollard, E. Soane, D. Tracy, S. Wu. 2008. Human Reliability Analysis: A Review and Critique. Manchester Business School, Booth Street West, Manchester M15 6PB.","cites":null},{"id":37919422,"title":"Human Reliability Analysis: Context and Control.","authors":[],"date":"1993","doi":"10.1080\/00140139508928506","raw":"Hollnagel, E. 1993. Human Reliability Analysis: Context and Control. Academic Press, London.","cites":null},{"id":37919388,"title":"Importance of human contribution within the human reliability analysis (IJS-HRA).","authors":[],"date":"2008","doi":"10.1016\/j.jlp.2007.04.012","raw":"\u0108epin, M. 2008. Importance of human contribution within the human reliability analysis (IJS-HRA). Journal of Loss Prevention in the Process Industries 21(3) 268-276.","cites":null},{"id":37919441,"title":"Integrating human factors into process analysis.","authors":[],"date":"2007","doi":"10.1016\/j.ress.2007.01.002","raw":"Kariuki, S.G., K. Lowe. 2007. Integrating human factors into process analysis. Reliability Engineering and System Safety 92 1764-1773.","cites":null},{"id":37919449,"title":"Introduction to Reliability Engineering.","authors":[],"date":"1994","doi":"10.1007\/978-94-009-0757-7_1","raw":"Lewis, E.E. 1994. Introduction to Reliability Engineering. Johne Wiley and Sons, Chichester.","cites":null},{"id":37919517,"title":"Investigating linear and interactive effects of shared mental models on safety and efficiency in a field setting.","authors":[],"date":"2005","doi":"10.1037\/0021-9010.90.3.523","raw":"Smith-Jentsch, K.A., J.E. Mathieu, K. Kraiger. 2005. Investigating linear and interactive effects of shared mental models on safety and efficiency in a field setting. Journal of Applied Psychology 90(3) 523-525.","cites":null},{"id":37919510,"title":"Job demands, job resources, and their relationship with burnout and engagement: a multi-sample study.","authors":[],"date":"2004","doi":"10.1002\/job.248","raw":"Schaufeli, W.B., A.B. Bakker. 2004. Job demands, job resources, and their relationship with burnout and engagement: a multi-sample study. Journal of Organisational Behavior 25 293-315.","cites":null},{"id":37919436,"title":"Judgement under Uncertainty.","authors":[],"date":"1982","doi":"10.4337\/9781781001035.00010","raw":"Kahneman, D., P. Slovic, A. Tversky, eds. 1982. Judgement under Uncertainty. Cambridge University Press, Cambridge.","cites":null},{"id":37919536,"title":"Knowing the risks: theory and practice in financial market trading.","authors":[],"date":"2001","doi":"10.1177\/0018726701547005","raw":"Willman, P., M. Fenton-OCreevy, N. Nicholson, E. Soane. 2001. Knowing the risks: theory and practice in financial market trading. Human Relations 54(1) 887-910.","cites":null},{"id":37919473,"title":"L\u2019intelligence de la complexit\u00e9. L\u2019Harmattan (The intelligence of complexity)- 26","authors":[],"date":"1999","doi":null,"raw":"Morin, E., J.L. Lemoigne. 1999. L\u2019intelligence de la complexit\u00e9. L\u2019Harmattan (The intelligence of complexity)- 26 -Mosleh, A., Y.H. Chang. 2004. Model-based human reliability analysis: prospects and reliability. Reliability Engineering and System Safety 83 241-253.","cites":null},{"id":37919470,"title":"La method \u2013 tome I, La nature de la nature. Ed du seuil (coll point), Paris (The method \u2013 Vol I, the nature of nature).","authors":[],"date":"1977","doi":null,"raw":"Morin, E. 1977. La method \u2013 tome I, La nature de la nature. Ed du seuil (coll point), Paris (The method \u2013 Vol I, the nature of nature).","cites":null},{"id":37919489,"title":"Les lois de chaos. Flammarion (The laws of chaos).","authors":[],"date":"1994","doi":null,"raw":"Prigogine, I. 1994. Les lois de chaos. Flammarion (The laws of chaos).","cites":null},{"id":37919427,"title":"Looking for errors of omission and commission or The Hunting of the Snark revisited Reliability Engineering and System","authors":[],"date":"2000","doi":"10.1016\/s0951-8320(00)00004-1","raw":"Hollnagel, E. 2000a. Looking for errors of omission and commission or The Hunting of the Snark revisited Reliability Engineering and System Safety 68 135-145.","cites":null},{"id":37919428,"title":"Looking for errors of omission and commission or The Hunting of the Snark revisited.","authors":[],"date":"2000","doi":"10.1016\/s0951-8320(00)00004-1","raw":"Hollnagel, E. 2000b. Looking for errors of omission and commission or The Hunting of the Snark revisited. Reliability Engineering and System Safety 68 135\u2013145.","cites":null},{"id":37919375,"title":"Managerial Decision Making, 6th ed.","authors":[],"date":"2006","doi":null,"raw":"Bazerman, M. 2006. Managerial Decision Making, 6th ed. John Wiley and Sons, New York.","cites":null},{"id":37919495,"title":"Managing the Risks of Organisational Accidents.","authors":[],"date":"1997","doi":null,"raw":"Reason, J. 1997. Managing the Risks of Organisational Accidents. Ashgate, Aldershot, UK.","cites":null},{"id":37919415,"title":"Models of ecological rationality: the recognition heuristic.","authors":[],"date":"2002","doi":"10.1093\/acprof:oso\/9780199744282.003.0003","raw":"Goldstein, D.G., G. Gigerenzer. 2002. Models of ecological rationality: the recognition heuristic. Psychological Review 109(1) 75-90.","cites":null},{"id":37919453,"title":"Moving from cognition to action \u2013 a control theory perspective. Applied Psychology - An International Review (Psychologie appliquee -","authors":[],"date":"1994","doi":"10.1111\/j.1464-0597.1994.tb00828.x","raw":"Lord, R.G., P.E. Levy. 1994. Moving from cognition to action \u2013 a control theory perspective. Applied Psychology - An International Review (Psychologie appliquee - Revue Internationale) 43(3) 335-398.","cites":null},{"id":37919481,"title":"Normal accidents: living with high-risk technologies.","authors":[],"date":"1984","doi":"10.2307\/2392945","raw":"Perrow, C. 1984. Normal accidents: living with high-risk technologies. Basic Books, New York.","cites":null},{"id":37919462,"title":"Nuclear power in the former USSR: historical and contemporary perspectives","authors":[],"date":"1997","doi":null,"raw":"Marples, D.R. 1997. Nuclear power in the former USSR: historical and contemporary perspectives D.R. Marples, M.J. Young, eds. Nuclear Energy and Security in the Former Soviet Union. Westview Press.","cites":null},{"id":37919524,"title":"Nuclear Regulatory Commission.","authors":[],"date":"2002","doi":"10.2172\/570062","raw":"United States Nuclear Regulatory Commission. 2002. Review of Findings for Human Performance Contribution to Risk in Operating Events (NUREG\/CR-6753). US GPO, Washington, DC.","cites":null},{"id":37919373,"title":"NUREG-1624: Technical basis and implementation guidelines for a technique for human event analysis (ATHEANA). US Nuclear Regulatory Commission.","authors":[],"date":"2000","doi":null,"raw":"Barriere, M., D. Bley, S. Cooper, J. Forester, A. Kolaczkowski, W. Luckas, G. Parry, A. Ramey-Smith, C. Thompson, D. Whitehead, J. Wreathall. 2000. NUREG-1624: Technical basis and implementation guidelines for a technique for human event analysis (ATHEANA). US Nuclear Regulatory Commission.","cites":null},{"id":37919404,"title":"NUREG-1842: Evaluation of human reliability analysis methods against good practices. US Nuclear Regulatory Commission,","authors":[],"date":"2006","doi":"10.1115\/1.802442.paper163","raw":"Forester, J.A., A. Kolaczkowski, E. Lois, D. Kelly. 2006. NUREG-1842: Evaluation of human reliability analysis methods against good practices. US Nuclear Regulatory Commission, Washington, DC.","cites":null},{"id":37919420,"title":"On error management: lessons from aviation.","authors":[],"date":"2000","doi":"10.1136\/bmj.320.7237.781","raw":"Helmreich, R.L. 2000. On error management: lessons from aviation. British Medical Journal 320(7237) 781\u2013 785.","cites":null},{"id":37919459,"title":"On the edge: heeding the warnings of unusual events.","authors":[],"date":"1999","doi":"10.1287\/orsc.10.4.482","raw":"Marcus, A., M.L. Nichols. 1999. On the edge: heeding the warnings of unusual events. Organisazational Science 10(4) 482-499.","cites":null},{"id":37919527,"title":"Organisational culture as a source of high reliability.","authors":[],"date":"1987","doi":"10.2307\/41165243","raw":"Weick, K.E. 1987. Organisational culture as a source of high reliability. California Management Review 29 112-127.","cites":null},{"id":37919530,"title":"Organizing for high reliability: processes of collective mindfulness.","authors":[],"date":"1999","doi":null,"raw":"Weick, K.E., K.M. Sutcliffe, D. Obstfield. 1999. Organizing for high reliability: processes of collective mindfulness. Research in Organizational Behavior 21 81-123.","cites":null},{"id":37919380,"title":"Probabilistic Risk Analysis: Foundations and Methods.","authors":[],"date":"2001","doi":"10.1017\/cbo9780511813597.002","raw":"Bedford, T., R. Cooke. 2001. Probabilistic Risk Analysis: Foundations and Methods. Cambridge University Press, Cambridge.","cites":null},{"id":37919406,"title":"Problem Structuring Methods","authors":[],"date":"2007","doi":"10.1057\/palgrave.jors.2602366","raw":"Franco, A., D. Shaw, M. Westcombe. 2007. Problem Structuring Methods II Journal of the Operational Research Society, 545- 682.","cites":null},{"id":37919469,"title":"Problem Structuring Methods in Action.","authors":[],"date":"2004","doi":"10.1016\/s0377-2217(03)00056-0","raw":"Mingers, J., J. Rosenhead. 2004. Problem Structuring Methods in Action. European Journal of Operational Research 152 530-554.","cites":null},{"id":37919505,"title":"Rational Analysis for a Problematic World Revisited.","authors":[],"date":"2001","doi":"10.1002\/sres.491","raw":"Rosenhead, J., J. Mingers, eds. 2001. Rational Analysis for a Problematic World Revisited. John Wiley and Sons, Chichester.","cites":null},{"id":37919377,"title":"Reviews on decision making.","authors":[],"date":"1999","doi":"10.2307\/2667037","raw":"Bazerman, M.H. 1999. Reviews on decision making. Administrative Science Quarterly 44(1) 176-180.","cites":null},{"id":37919515,"title":"Risk as analysis and risk as feelings: some thoughts about affect, reason, risk and rationality.","authors":[],"date":"2004","doi":"10.1111\/j.0272-4332.2004.00433.x","raw":"Slovic, P., M.L. Finucane, E. Peters, D.G. MacGregor. 2004. Risk as analysis and risk as feelings: some thoughts about affect, reason, risk and rationality. Risk Analysis 24(2) 311-322.","cites":null},{"id":37919450,"title":"Risk as feelings.","authors":[],"date":"2001","doi":"10.1037\/0033-2909.127.2.267","raw":"Loewenstein, G., E.U. Weber, C.K. Hsee, N. Welch. 2001. Risk as feelings. Psychological Bulletin 127(2) 267-286.","cites":null},{"id":37919361,"title":"Risk homeostasis and the purpose of safety regulation.","authors":[],"date":"1988","doi":"10.1080\/00140138808966688","raw":"Adams, J. 1988. Risk homeostasis and the purpose of safety regulation. Ergonomics. Ergonomics 31(4) 407 -428.","cites":null},{"id":37919534,"title":"Risk homeostasis theory: an overview.","authors":[],"date":"1998","doi":"10.1136\/ip.4.2.89","raw":"Wilde, G.J.S. 1998. Risk homeostasis theory: an overview. Injury Prevention 4 89-91.","cites":null},{"id":37919417,"title":"Risk mitigation in virtual organizations.","authors":[],"date":"1999","doi":"10.1287\/orsc.10.6.704","raw":"Grabowski, M., K.H. Roberts. 1999. Risk mitigation in virtual organizations. Organization Science 10 704-721.","cites":null},{"id":37919433,"title":"Safe drinking water: critical components of effective inter-agency relationships. Environment International (in press -doi:10.1016\/j.envint.2009.1009.1007).","authors":[],"date":"2009","doi":"10.1016\/j.envint.2009.09.007","raw":"Jalba, D., N. Cromar, S. Pollard, J.W.A. Charrois, R. Bradshaw, E. Hrudey. 2009. Safe drinking water: critical components of effective inter-agency relationships. Environment International (in press -doi:10.1016\/j.envint.2009.1009.1007).","cites":null},{"id":37919398,"title":"Safety design: towards a new philosophy.","authors":[],"date":"2006","doi":"10.1016\/j.ssci.2005.09.008","raw":"Fadier, E., C. De la Garza. 2006. Safety design: towards a new philosophy. Safety Science 44 55\u201373.","cites":null},{"id":37919507,"title":"Shared situation awareness as a contributor to high reliability performance in railroad operations.","authors":[],"date":"2006","doi":"10.1177\/0170840606065705","raw":"Roth, E.M., J. Multer, T. Raslear. 2006. Shared situation awareness as a contributor to high reliability performance in railroad operations. Organization Studies 27 967-987.","cites":null},{"id":37919501,"title":"Some characteristics of one type of high reliability organisation.","authors":[],"date":"1990","doi":"10.1287\/orsc.1.2.160","raw":"Roberts, K.H. 1990. Some characteristics of one type of high reliability organisation. Organization Science 1(2) 160-176.","cites":null},{"id":37919429,"title":"System Reliability Theory.","authors":[],"date":"1994","doi":"10.2307\/1268910","raw":"H\u00f8yland, A., M. Rausand. 1994. System Reliability Theory. John Wiley and Sons, New York.","cites":null},{"id":37919491,"title":"The contribution of latent human failures to the breakdown of complex systems.","authors":[],"date":"1990","doi":"10.1093\/acprof:oso\/9780198521914.003.0003","raw":"Reason, J. 1990a. The contribution of latent human failures to the breakdown of complex systems.","cites":null},{"id":37919465,"title":"The influence of shared mental models on team process and performance.","authors":[],"date":"2000","doi":"10.1037\/0021-9010.85.2.273","raw":"Mathieu, J.E., T.S. Heffner, G.F. Goodwin, E. Salas, J.A. Cannon-Bowers. 2000. The influence of shared mental models on team process and performance. Journal of Applied Psychology 85(2) 273-283.","cites":null},{"id":37919508,"title":"The Limits of Safety: Organizations, Accidents, and Nuclear Weapons.","authors":[],"date":"1993","doi":"10.2307\/20045733","raw":"Sagan, S.D. 1993. The Limits of Safety: Organizations, Accidents, and Nuclear Weapons. Princeton University Press, Princeton, NJ.- 27 -Sagan, S.D. 1994. Toward a political theory of organizational reliability. Journal of Contingencies and Crisis Management 2 228-240.","cites":null},{"id":37919483,"title":"The limits of safety: the enhancement of a theory of accidents.","authors":[],"date":"1994","doi":"10.1111\/j.1468-5973.1994.tb00046.x","raw":"Perrow, C. 1994. The limits of safety: the enhancement of a theory of accidents. Journal of Contingencies and Crisis Management 2 212-220.","cites":null},{"id":37919509,"title":"The problem of redundancy problem [sic]: why more nuclear security forces may produce less nuclear security.","authors":[],"date":"2004","doi":"10.1111\/j.0272-4332.2004.00495.x","raw":"Sagan, S.D. 2004. The problem of redundancy problem [sic]: why more nuclear security forces may produce less nuclear security. Risk Analysis 24 935-946.","cites":null},{"id":37919539,"title":"The relation of strength of stimulus to rapidity of habit-formation.","authors":[],"date":"1908","doi":"10.1002\/cne.920180503","raw":"Yerkes, R.M., J.D. Dodson. 1908. The relation of strength of stimulus to rapidity of habit-formation. . Journal of Comparative Neurological Psychology 18 459-482.","cites":null},{"id":37919513,"title":"The Sciences of the Artificial.","authors":[],"date":"1996","doi":null,"raw":"Simon, H. 1996. The Sciences of the Artificial. MIT Press.","cites":null},{"id":37919504,"title":"The self-designing high reliability organization: aircraft carrier operations at sea.","authors":[],"date":"1987","doi":null,"raw":"Rochlin, G.I., T.R. La Porte, K.H. Roberts. 1987. The self-designing high reliability organization: aircraft carrier operations at sea. Naval War College Review 40 76-90.","cites":null},{"id":37919480,"title":"The Swiss cheese model of safety incidents: are their holes in the metaphor.","authors":[],"date":"2005","doi":null,"raw":"Perneger, T.V. 2005. The Swiss cheese model of safety incidents: are their holes in the metaphor. BMC Health Services Research 5 71-77.","cites":null},{"id":37919532,"title":"The theory of risk homeostasis: implications for safety and health.","authors":[],"date":"1982","doi":"10.1111\/j.1539-6924.1982.tb01384.x","raw":"Wilde, G.J.S. 1982. The theory of risk homeostasis: implications for safety and health. Risk Analysis 2 209-225.","cites":null},{"id":37919368,"title":"The unbearable automaticity of being.","authors":[],"date":"1999","doi":"10.1037\/\/0003-066x.54.7.462","raw":"Bargh, J.A., T.L. Chartrand. 1999. The unbearable automaticity of being. American Psychologist 54 462-479.","cites":null},{"id":37919402,"title":"Thinking, feeling and deciding: The influence of emotions on the decision making and performance of traders Academy of Management Conference,","authors":[],"date":"2008","doi":"10.1002\/job.720","raw":"Fenton-O\u2019Creevy, M., E. Soane, N. Nicholson, P. Willman. 2008. Thinking, feeling and deciding: The influence of emotions on the decision making and performance of traders Academy of Management Conference, Anaheim, California. . Finucane, M.L., A. Alhakami, P. Slovic, S.M. Johnson. 2000. The affect heuristic in judgments of risks and benefits. Journal of Behavioral Decision Making 13 1-17.","cites":null},{"id":37919400,"title":"Trading on illusions: unrealistic perceptions of control and trading performance.","authors":[],"date":"2003","doi":"10.1348\/096317903321208880","raw":"Fenton-O\u2019Creevy, M., N. Nicholson, E. Soane, P. Willman. 2003. Trading on illusions: unrealistic perceptions of control and trading performance. Journal of Occupational and Organizational Psychology 76(1) 53-68.","cites":null},{"id":37919494,"title":"Understanding adverse events: human factors.","authors":[],"date":"1995","doi":"10.1136\/qshc.4.2.80","raw":"Reason, J. 1995. Understanding adverse events: human factors. Quality Health Care 4 80-89.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2011-07-01T00:00:00Z","abstract":"In running our increasingly complex business systems, formal risk analyses and\nrisk management techniques are becoming a more important part of a manager's\ntool-kit. Moreover, it is also becoming apparent that human behaviour is often a\nroot or significant contributing cause of system failure. This latter\nobservation is not novel; for more than 30 years it has been recognised that the\nrole of human operations in safety critical systems is so important that they\nshould be explicitly modelled as part of the risk assessment of plant\noperations. This has led to the development of a range of methods under the\ngeneral heading of human reliability analysis (HRA) to account for the effects\nof human error in risk and reliability analysis. The modelling approaches used\nin HRA, however, tend to be focussed on easily describable sequential, generally\nlow-level tasks, which are not the main source of systemic errors. Moreover,\nthey focus on errors rather than the effects of all forms of human behaviour. In\nthis paper we review and discuss HRA methodologies, arguing that there is a need\nfor considerable further research and development before they meet the needs of\nmodern risk and reliability analyses and are able to provide managers with the\nguidance they need to manage complex systems safely. We provide some suggestions\nfor how work in this area should develop","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/140826.pdf","fullTextIdentifier":"http:\/\/dx.doi.org\/10.1016\/j.ssci.2011.02.008","pdfHashValue":"12924ca223fb93d5bdf612ad649eb2e9ac2ec809","publisher":"Elsevier Science B.V., Amsterdam.","rawRecordXml":"<record><header><identifier>\noai:dspace.lib.cranfield.ac.uk:1826\/6498<\/identifier><datestamp>2011-12-07T11:32:24Z<\/datestamp><setSpec>hdl_1826_24<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>Human reliability analysis: A critique and review for managers<\/dc:title><dc:creator>French, Simon<\/dc:creator><dc:creator>Bedford, Tim<\/dc:creator><dc:creator>Pollard, Simon J. T.<\/dc:creator><dc:creator>Soane, Emma<\/dc:creator><dc:description>In running our increasingly complex business systems, formal risk analyses and\nrisk management techniques are becoming a more important part of a manager's\ntool-kit. Moreover, it is also becoming apparent that human behaviour is often a\nroot or significant contributing cause of system failure. This latter\nobservation is not novel; for more than 30 years it has been recognised that the\nrole of human operations in safety critical systems is so important that they\nshould be explicitly modelled as part of the risk assessment of plant\noperations. This has led to the development of a range of methods under the\ngeneral heading of human reliability analysis (HRA) to account for the effects\nof human error in risk and reliability analysis. The modelling approaches used\nin HRA, however, tend to be focussed on easily describable sequential, generally\nlow-level tasks, which are not the main source of systemic errors. Moreover,\nthey focus on errors rather than the effects of all forms of human behaviour. In\nthis paper we review and discuss HRA methodologies, arguing that there is a need\nfor considerable further research and development before they meet the needs of\nmodern risk and reliability analyses and are able to provide managers with the\nguidance they need to manage complex systems safely. We provide some suggestions\nfor how work in this area should develop.<\/dc:description><dc:publisher>Elsevier Science B.V., Amsterdam.<\/dc:publisher><dc:date>2011-10-20T23:09:39Z<\/dc:date><dc:date>2011-10-20T23:09:39Z<\/dc:date><dc:date>2011-07-01T00:00:00Z<\/dc:date><dc:type>Article<\/dc:type><dc:identifier>Simon French, Tim Bedford, Simon J.T. Pollard, Emma Soane, Human reliability analysis: A critique and review for managers, Safety Science, Volume 49, Issue 6, July 2011, Pages 753-763.<\/dc:identifier><dc:identifier>0925-7535<\/dc:identifier><dc:identifier>http:\/\/dx.doi.org\/10.1016\/j.ssci.2011.02.008<\/dc:identifier><dc:identifier>http:\/\/dspace.lib.cranfield.ac.uk\/handle\/1826\/6498<\/dc:identifier><dc:rights>NOTICE: this is the author\u2019s version of a work that was accepted for publication in Safety Science. Changes resulting from the publishing process, such as peer review, editing, corrections, structural formatting, and other quality control mechanisms may not be reflected in this document. Changes may have been made to this work since it was submitted for publication. A definitive version was subsequently published in Safety Science, VOL 49, ISSUE 6, (2011) DOI:10.1016\/j.ssci.2011.02.008<\/dc:rights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":["issn:0925-7535","0925-7535"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2011,"topics":[],"subject":["Article"],"fullText":"- 1 -\nHuman Reliability Analysis:\na critique and review for managers\nSimon French1\nManchester Business School,\nUniversity of Manchester,\nManchester, M15 6PB\nTim Bedford\nDepartment of Management Science,\nUniversity of Strathclyde,\nGlasgow, G1 1QE\nSimon J.T. Pollard\nThe Collaborative Centre of Excellence\nin Understanding and Managing\nNatural and Environmental Risks,\nCranfield University,\nCranfield, MK43 0AL\nEmma Soane\nEmployment Relations and\nOrganisational Behaviour Group,\nDepartment of Management,\nLondon School of Economics and\nPolitical Science,\nLondon, WC2A2AE\nAbstract\nIn running our increasingly complex business systems, formal risk analyses and risk management\ntechniques are becoming a more important part of a manager\u2019s tool-kit. Moreover, it is also becoming\napparent that human behaviour is often a root or significant contributing cause of system failure. This\nlatter observation is not novel; for more than 30 years it has been recognised that the role of human\noperations in safety critical systems is so important that they should be explicitly modelled as part of\nthe risk assessment of plant operations. This has led to the development of a range of methods under\nthe general heading of human reliability analysis (HRA) to account for the effects of human error in\nrisk and reliability analysis. The modelling approaches used in HRA, however, tend to be focussed on\neasily describable sequential, generally low-level tasks, which are not the main source of systemic\nerrors. Moreover, they focus on errors rather than the effects of all forms of human behaviour. In this\npaper we review and discuss HRA methodologies, arguing that there is a need for considerable further\nresearch and development before they meet the needs of modern risk and reliability analyses and are\nable to provide managers with the guidance they need to manage complex systems safely. We provide\nsome suggestions for how work in this area should develop.\nKeywords: Cynefin model of decision contexts; high reliability organisations; human reliability\nanalysis (HRA); management of risk.\n1 Address for correspondence: Simon French, Manchester Business School, Booth Street West, Manchester,\nM15 6PB, UK. Email: simon.french@mbs.ac.uk\n- 2 -\n1. Introduction\nComplex systems are never 100% reliable: they fail, sometimes catastrophically, more usually\nreparably. Perrow (1984, 1994) has argued that failures are an inevitable consequence of the\nincreasing complexity of our systems. Whatever the case, inevitable or not, failures undoubtedly\noccur. Even in systems that appear to be largely technological rather than human, we find that in the\nmajority of cases there is a human element involved. Maybe some erroneous or even malicious\nbehaviour initiates the failure; maybe the human response to some event is insufficient to avoid\nsystem failure; or maybe the original design of the system did not anticipate a potential failure or\nunfavourable operating conditions.\nStatistics show human error is implicated in (see also Hollnagel 1993):\n\uf0b7 over 90% of failures in the nuclear industry (Reason 1990a), see also (United States Nuclear\nRegulatory Commission 2002);\n\uf0b7 over 80% of failures in the chemical and petro-chemical industries (Kariuki and Lowe 2007);\n\uf0b7 over 75% of marine casualties (Ren et al. 2008);\n\uf0b7 over 70% of aviation accidents (Helmreich 2000);\n\uf0b7 over 75% of failures in drinking water distribution and hygiene (Wu et al. 2009).\nIn addition to highly technological industries, there are other complex systems involving\napplications of technology in which we include complex mathematical modelling, software\nnd web-based systems. The growth of service industries with new business models implies an\neven greater dependence of businesses, organisations and even economies on reliable human\ninteractions. For instance, recently human checks and balances failed to detect dubious\ninvestment behaviour of a trader at Soci\u00e9t\u00e9 G\u00e9n\u00e9rale and led to a loss of some \u20ac4.9bn, large\nenough to have economic and financial effects beyond the bank. The current \u2018credit crunch\u2019\nowes not a little to misjudgement and error in the banking and finance sectors, indicating the\ngrowing interdependence of many disparate parts of the modern global economy. It also\nowes a lot to a loss of investors\u2019 confidence and trust, both of which inform human\nbehaviour. These data indicate how vulnerable our systems are, even after many years of\nrefinement and improvement; and how important an understanding of human behaviour is if\nwe are to reduce the risk to systems. Another high profile example is the leak in the THORP\nplant at Sellafield (Thermal Oxide Reprocessing Plant) that was discovered in 2005: see (BNFL,\n2005). This relatively modern plant had been designed to a high standard of safety, but\ninformation indicating a system problem was available for some months and yet went unnoticed.\n- 3 -\nDespite previous incidents in 1998 and earlier in 2005, the information that should have\nsuggested a leak, or at least a problem requiring investigation, was misinterpreted. The prevailing\nattitude was that the system was error-free and hence information that could suggest the contrary\nwas ignored or dismissed.\nManagerial processes are critical to successful operation of any complex system; and the quality of\nmanagement processes depends on their understanding of the import and limitations of the results of\nanalyses that are provided to them. In this article, we examine current and past approaches to human\nreliability analysis (HRA). We discuss its assumptions, limitations and potential in qualitative terms so\nthat managers can better assess the value of the information that it provides them and so manage risks\nmore effectively. We also suggest that further development of HRA methodologies should take more\naccount of the managerial practices that could be applied to reduce the failures that occur at the\ninterface of human behaviour and technology.\nManagers understand human behaviour; good managers understand human behaviour extremely well.\nTo bring out the best in a team one needs to know how each will respond to a request, an instruction,\nan incentive or a sanction. Yet only the most foolhardy and overconfident of managers would claim\nthat they can predict human behaviour perfectly all the time \u2013 or even 95% of the time. The problem\nis that we often need to design systems with very high reliabilities, many times with overall failure\nrates of less than 1 in 10 million (i.e. 1 in 10-7). To design and analyse such systems we need a deep\nunderstanding of human behaviour in all possible circumstances that may arise in their management\nand operation. And that is the challenge facing HRA. Our current understanding of human behaviour\nis not sufficiently comprehensive: worse, current HRA methodologies seldom use all the\nunderstanding that we do have.\nOf course, there is a trivial mathematical answer to this. If we are to achieve an overall system\nreliability of 10-7, we do not need humans to be perfectly reliable. We simply need to know how\nreliable they are and then ensure that we arrange and maintain sufficient safety barriers around the\nsystem to ensure that overall system failure probabilities are as low as required. Suppose we construct\nseven independent safety barriers perhaps some involving humans, some purely technological and\nsuppose each has a probability of 1 in 10 of failing, then arranging them (conceptually) in sequence so\nthat the whole system fails if and only if every one of the seven fails gives an overall probability of\nsystem failure of\n710\n10\n1\n10\n1\n10\n1\n10\n1\n10\n1\n10\n1\n10\n1 \uf02d\uf03d\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4\uf0b4 .\nThe problem with this is that there are few barriers that are truly independent, most systems offer\nopportunities to \u2018bypass\u2019 these barriers. Moreover, human behaviour tends to introduce significant\ncorrelations and dependencies which invalidate such calculations, reducing the benefit that each extra\n- 4 -\nsafety barrier brings; such problems with protective redundancy are well known (for example, Sagan\n2004). So the simplistic calculation does not apply, and we shall argue that we have yet to develop\nsufficiently complex mathematical modelling techniques to describe human behaviour adequately for\nrisk and reliability analyses.\nIn many ways the roles of risk and reliability analysis in general and of HRA in particular are often\nmisunderstood by system designers, managers and regulators. In a sense they believe in the models\nand the resulting numbers too much and fail to recognise the potential for unmodelled and possibly\nunanticipated behaviours \u2013 physical or human \u2013 to lead to overall system breakdown (cf. French and\nNiculae 2005). Broadly there are two ways in which such analyses may be used.\n\uf0b7 When HRA is incorporated into a summative analysis, its role is to help estimate the overall\nfailure probabilities in order to support decisions on, e.g., adoption, licensing or maintenance.\nSuch uses require quantitative modelling of human reliability; and overconfidence in these\nmodels can lead to overconfidence in the estimated probabilities and poor appreciation of the\noverall system risks.\n\uf0b7 There are also formative uses of HRA in which recognising and roughly ranking the potential for\nhuman error can help improve the design of the system itself and also the organisational\nstructures and processes by which it is operated. Effective HRA not only complements sound\ntechnical risk analysis of the physical systems, but also helps organisations develop their safety\nculture and manage their overall risk. Indeed, arguably it is through this that HRA achieves its\ngreatest effect.\nThese uses are not independent \u2013 in designing, licensing and managing a system one inevitably\niterates between the two \u2013 they do differ, however, fundamentally in philosophy. In summative\nanalysis the world outside the system in question learns from the outcome of an analysis; in formative\nanalysis the world inside the system learns from the process of analysis. In summative analysis the\nideal is almost to be able to throw away the process and deal only with the outcome; in formative\nanalysis the ideal is almost to throw away the outcome and draw only from the process. While we\nbelieve that HRA has a significant potential to be used more in formative ways; we are concerned at\nits current ability to fulfil a summative role, providing valid probabilities of sequences of failure\nevents in which human behaviour plays a significant role. We believe that there is scope for\nconsiderable overconfidence in the summative power of HRA currently and that management,\nregulators and society in general need to appreciate this, lest they make poorly founded decisions on\nregulating, licensing and managing systems.\nThe four of us were part of a recent UK EPSRC funded multi-disciplinary project Rethinking Human\nReliability Analysis Methodologies to survey and critique HRA methodologies (Adhikari et al. 2008).\nOur purpose in this paper is to draw out the relevant conclusions from this project for the management\n- 5 -\ncommunity and, perhaps as well, for our political masters who create the regulatory context in which\ncomplex systems have to operate. Overall we believe that current practices in and uses of HRA are\ninsufficient for the complexities of modern society. We argue that the summative outputs of risk and\nreliability analyses should be taken with the proverbial pinch of salt. But not all our conclusions will\nbe negative. There is much to be gained from the formative use of HRA to shape management\npractices and culture within organisations and society which can lead to better, safer and less risky\noperations.\nIn the next section we briefly survey the historical development underlying concepts of HRA and its\nrole in risk and reliability analyses. We reflect on the widely quoted Swiss Cheese Model (Reason\n1990b), which seeks to offer a qualitative understanding of system failure \u2013 though we shall argue that\nit may actually lead to systematic misunderstandings! In Section 3 we turn to modern theories of\nhuman behaviour, particularly those related to judgement and decision. A key issue is that HRA\nfocuses on human errors, whereas many systems failures may arise not just despite, but sometimes\nbecause of fully appropriate and rational behaviour on the part of those involved. Thus we need a\nbroader understanding of human behaviour than that relating to human error. We also need to\nrecognise that cultural, organisational, social and other contexts influence behaviour, perhaps\ncorrelating behaviour across a system, thus invalidating assumptions of independence commonly\nmade in risk and reliability analyses. One of the flaws common to many current HRA methodologies\nis that they tend to focus on easily describable, sequential, generally low-level operational tasks. Yet\nthe human behaviour that is implicated in many system failures may occur in other quite different\ncontexts, maybe in developing higher level strategy or during the response to an unanticipated\ninitiating failure event. In recent years there have been many studies of organisational forms which\nseem to be more resilient to system failures than might be expected and we discuss such studies of\nhigh reliability organisations (HROs) briefly in Section 4. Another flaw common to many current\nHRA methodologies is the lack of specification of the domain of applicability \u2013 hence making it\ndifficult to select appropriate methods for a given problem. Therefore in Section 5, we use Snowden\u2019s\nCynefin classification of decision contexts (Snowden 2002; Snowden and Boone 2007) to categorise\ndifferent circumstances in which human behaviour may be involved in system failure. We believe\nthat the use of Cynefin \u2013 or a similar categorisation of decision contexts \u2013 can help in delineating\nwhen different HRA methodologies are appropriate. Moreover, it points to areas in which we lack a\nreally sound, appropriate HRA methodology. Our final two sections draw our discussion to a close,\nsuggesting that:\n\uf0b7 by drawing together current understandings from HRA with other domains of knowledge in\nbehavioural, management and organisational theories, we can make better formative use of HRA\nin designing systems, process and the organisations that run these;\nbut that:\n- 6 -\n\uf0b7 the state of the art in quantitative HRA is too poor to make the summative assessments of risk and\nreliability that our regulators assume, and that society urgently needs to recognise this.\n2. HRA methodologies and the Swiss cheese model\nReliability analysis and risk analysis are two subjects with a great deal of overlap (Aven 2003; Barlow\nand Proschan 1975; Bedford and Cooke 2001; H\u00f8yland and Rausand 1994; Melnick and Everitt 2008).\nThe former is generally narrower in scope and tends to deal with engineered systems subject to\nrepeated failures and the need for preventative maintenance policies to address these. Key concepts in\nreliability engineering include component availability, reliability and maintainability; mean times to,\nand between failure; the use of specific fault tree and failure mode tools; and the concepts of system\nredundancy. Reliability engineering owes a significant amount to advances in manufacturing\nengineering and the desire to improve production quality and optimise output (Lewis 1994). Risk\nanalysis is a much broader term and tends to deal with more one-off failures that may write-off a\nsystem with concomitant impacts elsewhere. It is not necessarily restricted to technical systems and\nhas developed into a broad interdisciplinary field with important inputs from the social sciences,\nalongside applied mathematics and decision science. But both reliability engineering and risk\nanalysis are essentially are concerned with anticipating possible failures and assessing their likelihood.\nHRA specifically relates to methodologies for anticipating and assessing the effect of those failures\nwhich relate to human action or inaction, and not the failure of some physical component.\nThere are many reasons why one might undertake a risk or reliability analysis. In broad terms the first\nthree items in our list relate to formative uses of risk and reliability analysis and the last two to\nsummative uses2.\n1. The designers of a system may be concerned with \u2018designing out\u2019 the potential for system failure.\nPart of this involves analysing how human behaviour may affect the system in its potential both to\ncompromise its reliability and to avoid the threat of imminent failure.\n2. Sometimes an organisation wants to restructure and change its reporting structures. In such\ncircumstances, it may wish to understand how its organisational design may affect the reliability\nand safety of its systems; and in turn that understanding may inform the development of its\noperating practices and safety culture.\n3. There may be a need to modify a system in which case there are needs to design the modification\nand the project to deliver the modification.\n2 We make no claims that this list is exhaustive, just sufficient for our discussion.\n- 7 -\n4. During licensing discussions between a government regulator and the system operator there may\nbe a need to demonstrate that a system meets a safety target. An assessment of the risks arising\nfrom human behaviour will be an integral part of this.\n5. There may be a need to choose which of several potential systems to purchase and the risk of\nsystem failure may be a potential differentiator between the options. Such differences may not be\npurely technical, since some systems may be more or less at risk from some human behaviours.\nAs a component of a full risk or reliability analysis, HRA may be used in any of these ways.\nThe origins of HRA lie in the early probabilistic risk assessments performed as part of the US nuclear\nenergy development programme in the 1960s (Bedford and Cooke 2001; United States Nuclear\nRegulatory Commission 1975) Early first generation HRA methods such as the Technique for Human\nError Rate Prediction (THERP) (Swain and Guttmann 1983) were very similar to those in other areas\nof reliability analysis: namely, the probability of a human error is assessed via a simple event tree\nanalysis. The event tree simply listed an initiating event\u2019, which might be a system error reaching the\nhuman operator, and then considered a series of tasks that which had to be correctly carried out to\nprevent unwanted consequences. Essentially, in these early models, the human operator is treated as\nanother component in the system. Hollnagel (1993) referred to this general approach as\ndecomposition. A variety of other first generation methods have been developed with broadly similar\nfeatures to THERP \u2013 the use of task analysis, use of nominal probabilities for task failure, adjustment\nfactors to take account of different performance conditions, error factors and so on. The Human\nReliability Analysis Event Tree method (HEART) (Williams 1985) is a good example of a method that\naims to use many of the same features but in a simplified setting to give a more straightforward\napproach. Recognizing that many tasks have an associated time for completion, the Human Cognitive\nReliability method (HCR) (Hannaman et al. 1984) modelled the time to successful completion. A wider\nreview of these and many other methods is given in (Kirwan, 1994).\nMuch of the discussion around these models focussed on the issue that errors of omission (failures to\nrespond to events appropriately) were considered easier to model than errors of commission, i.e.\ninappropriate human actions. However, this simplistic dichotomy now appears too stark in light of our\ncurrent, richer, qualitative understandings of human cognition, motivation and decision making,\nincluding the effects of stress, emotion, training, group interactions, organisational structures, cultures\nand so forth (Bazerman 2006; Bazerman 1999; French et al. 2009; Kahneman et al. 1982; Kahneman\nand Tversky 2000). Research in these fields has shown that there are systematic influences on\ndecision making and behaviour that cannot be categorised simply as omissions or commissions: see\nSection 3 below. Human failure is far more complex than the failure of, say, a steel support beam or a\n- 8 -\nhard disk. To be fair, second3 generation HRA methods (Barriere et al. 2000; Hollnagel 1993)\nattempted to incorporate contextual effects such as tiredness, stress and organisational culture on an\noperator\u2019s proneness to error; and third generation HRA methods (Boring 2007; Mosleh and Chang\n2004) have sought to allow for the potential variation in response and recovery actions once an error\nchain has begun. Notwithstanding this, we argue that far more development is needed before any\nmethod takes account of all our current understandings of human behaviour.\nSurveys of current HRA methodologies may be found in Adhikari et al. (2008), Forrester et al. (2006)\nand Hollnagel (1993, 1998). For other recent research and developments in HRA, see the special issue\nof the Journal of Loss Prevention in the Process Industries (2008, 21, 225-343). Software reliability\nanalysis also has a large literature (Courtois et al. 2000; Lyu 2005; Zhang and Pham 2000). Software\nengineering is largely an endeavour of human design and thus subject to all the risks that HRA seeks\nto explore and assess. To date, software reliability assessment has, by and large, also adopted a\nmechanistic or empirical modelling of human error similar in methodology to current quantitative\nHRA.\nReason (1990b) offered a metaphor for system failure involving human error likening failure\nprocesses to movements of slices of Swiss cheese relative to each other: see Figure 1. Essentially this\nsuggested that systems do not fail because of a single failure, but because several elements fail near\nsimultaneously, as if the holes in slices of Swiss cheese have aligned. Although it is clear from his\nwritings that Reason knew the limitations of metaphors (Reason 1995, 1997), his readers have often\ninterpreted the model too mechanistically. There has been a dominant tendency to imagine a fixed\nnumber of slices, sliding backwards and forwards independently of each other until a series of holes\nalign. In safety studies one talks of the number of safety barriers (the multi-barrier concept) or layers\nbetween normal operation and system failure; and, in a sense, the slices of Swiss Cheese mirror these.\nSystems are designed with a set number of safety barriers and these barriers are intended to be\nindependent: cf. the simplistic calculation of a failure rate of 1 in 107 above. But human behaviour\ncan correlate the risks of failure of two or more barriers, and most systems also harbour the\nopportunity for the \u2018bypass\u2019 of these barriers. Human behaviour and propensity to failure varies in\ncomplex ways with, e.g., their tiredness, stress and general emotional state, which may well be\ninfluenced by external events leading to a common cause and which may disrupt several safety\nbarriers simultaneously. For instance, the Chernobyl Accident (International Atomic Energy Agency\n1991; Marples 1997) was in large measure caused by the imperative to conduct an engineering\nexperiment within a fixed time, leading to stress in the operators and behaviour that compromised\nseveral of the safety barriers simultaneously. Another potential unsafe behaviour is to discover an\n3 One should not take too chronological perspective on first, second and third generation HRA methods.\nSome of those developed earliest did make attempts to account for contextual effects (Adhikari et al. 2008).\n- 9 -\nindication of a \u2018hole\u2019 in one layer and to defer\nfurther investigation, relying on the \u2018cover\u2019 offered\nby other layers: such behaviour occurred during a\nrecent leak of radioactivity at Sellafield (Adhikari et\nal. 2008). Hrudey et al. (2006) describe similar\nbehaviour during the Walkerton drinking water\ntragedy in Ontario, where latent and active flaws left\nunaddressed exacerbated the impact of agricultural\nrun-off infiltrating a town\u2019s shallow groundwater\nsupply. On the positive side, humans have the ability to recover, to respond to the unexpected, to\nthink \u2018out of the box\u2019, and so on, effectively repairing a compromised layer or even introducing a new\none \u2013 the latter is, of course, the principle of preventative risk management.\nIn terms of the Swiss Cheese model, many of these failings correspond to varying the size of the holes,\nperhaps in a correlated fashion, and maybe varying the number of layers over time. Reason himself\ndiscusses similar criticisms (Reason 1995, 1997); but the simpler mechanistic thinking implicit in\nFigure 1 still pervades thinking in much of reliability engineering (Perneger 2005). The model\nvisually emphasises a reductionist approach to HRA and may thus \u2018wrong-foot\u2019 the users of reliability\nanalysis methodologies leading them to miss some of the key factors and mechanisms that should be\nbuilt into their models; and, perhaps, put too much trust in the combined effect of several safety\nbarriers. For example, it could be argued that the model struggles to fully represent the motives that\nmight accompany deliberate violations of procedure, the creeping loss of vigilance with respect to a\nsafety culture or the very real opportunities for the bypass of barriers in most technological systems.\nWe note that there is an established literature stemming from a range of work in France on the need to\nmoderate reductionist, decomposable approaches to human reliability \u2013 or as they sometimes term it,\n\u2018human factors of reliability\u2019 \u2013 with an understanding of organisational, management and process\ncontexts which can introduce dependencies (Fadier 2008; Fadier and Ciccotelli 1999; Fadier and De\nla Garza 2006; Leplat 1994).\n3. Human behaviour and human error\nHuman behaviour is complex and often non rational. For instance, it seems sensible to use modern\ntechnological advances to make the physical components of a system safer. But there is some\nevidence that making subsystems safer could make the overall system less safe because of the\npropensity of humans to take less care personally when a system takes more care (Adams 1988;\nHollnagel 1993). In this section we survey some recent findings from behavioural decision studies\nand consider how this area of theory and research can add to HRA. We do not focus on error\nbehaviours per se, but take a more holistic approach. We do this for three reasons.\nFigure 1 Reason's Swiss Cheese model (Reason\n1990b)\n- 10 -\nFirst, the error focus of HRA models may be too narrow (Hollnagel 1998, 2000a). Errors are just one\nof a range of behavioural products of a number of individual and organisational precursors; they are\nnot a class of behaviours that are entirely distinct from other behaviours and thus should not be\nconsidered in isolation. In the organisational context, it is often an external system or judgement that\ncategorises a behaviour as an error rather than the behaviour itself being inherently and indisputably\nwrong.\nSecond, models of HRA that explicitly include human factors typically focus on cognitive aspects of\ndecision making. Recent developments in the modelling of decision making emphasise the dual\ninfluences of cognition and emotion on decision outcomes (French et al. 2009; Loewenstein et al.\n2001; Slovic et al. 2004). The integration of emotions and cognition models of decision making has\nimproved the ability of such models to understand and predict behaviour (Phelps 2006). Furthermore,\nsuch an integrated approach is highly relevant to the risk-related decision making typically found\nwithin safety critical industries (Fenton-O\u2019Creevy et al. 2008; Finucane et al. 2000).\nThird, the use of high reliability systems designed and engineered to minimise errors and hazards has\nboth benefits and disadvantages. It is of course important that systems are designed to be as safe as\npossible. However, the reliance on such systems can cause biases and flaws in decision making. The\nrisk thermostat model suggests there is a dynamic interaction between actors\u2019 perceptions and\nbehaviours and their environment (Adams 1988; Wilde 1982, 1998). People will adjust their\nbehaviour to be more or less risky, as appropriate for their preferences and their situation, perhaps\nrelying on one safety system to protect them from the risk of failing to operate another. A high profile\nexample is the leak in a modern plant at Sellafield mentioned previously. There was a belief that such\na modern plant could not suffer from leaks or other failures. In the context of the \u2018new plant\u2019 culture\nand other management imperatives, it was too easy to ignore inconclusive but pertinent readings and\nobservations. It is also noteworthy that this \u2018new plant\u2019 culture was implicated in two previous\nsmaller incidents at Sellafield (Adhikari et al. 2008; Board of Inquiry 2005). Marcus and Nichols\n(1999) discuss similar behaviours in which warning signs were not heeded and suggest that other\npriorities for limited resources make it too easy to drift towards what they term the \u2018safety border\u2019.\nReal human judgement and decision making is not as rational and analytic as one might wish. Since\nthe early 1980s, psychologists have distinguished between two different forms of thinking (Chaiken et\nal. 1989)4:\n4 There is an unfortunate conflict of terminology here between our use of \u2018system\u2019 to mean the entire plant\nand processes which is at risk and \u2018systems of thinking\u2019 as referred to in the psychological literature. We\nuse the phrases \u2018System 1 (or 2) thinking\u2019 to distinguish the latter.\n- 11 -\n\uf0b7 System 1 thinking, often referred to as \u2018intuition\u2019 or \u2018gut reaction\u2019 that involves a superficial\nanalysis\/interpretation of the relevant information based on much simpler forms of thinking on the\nfringes or outside of consciousness;\n\uf0b7 System 2 thinking, characterised by conscious analytical thought that involves a detailed\nevaluation of a broad range of information, often based on a rule that is assumed to provide the\n\u2018correct\u2019 answer or solution.\nWhile formal risk assessment techniques have the characteristics of System 2 thinking, system\noperators may use System 1 thinking in their day-to-day operations and responses to events. For\nexample, a nuclear power plant is the outcome of considerable complex analysis, research and design,\ni.e. System 2 thinking. The operators of such a plant, however, do not typically engage in the same\nkind of analytical thinking as the system engineers and designers. The operators\u2019 work comprises\nmuch more routine procedures and, where complex problems are faced, there is potential for operators\nto make them more manageable through system 1 heuristics. It has become common to refer to much\nof System 1 thinking as involving \u2018heuristics and biases\u2019, because of its deviation from the more\nrational, analytic System 2 thinking, though that terminology is as pejorative as the constant use of the\nterm \u2018human error\u2019 in HRA which we reject in this article.\nThere is an extensive literature on decision making heuristics and biases (French et al. 2009;\nKahneman et al. 1982; Kahneman and Tversky 2000). Numerous studies have demonstrated the\nexistence of systematic and robust cognitive biases, and are well summarised by Bazerman (2006).\nFor example, emotionally-laden or otherwise individually salient information is recalled easily and\nlikely to be considered as significant to a decision when more objective evidence shows that other\ntypes of information are more important to a decision. The processes that drive such biases have not\narisen without reason \u2013 we cannot take into account all the information that surrounds us and so we\nneed to select information to attend to in order for any action to be taken. The work of Gigerenzer and\ncolleagues has shown that some heuristics can improve decision making by providing rapid\nmechanisms for recall of salient information and execution of choice behaviours (Goldstein and\nGigerenzer 2002). However, such biases can be problematic. For example, Willman et al. (2001) and\nFenton-O\u2019Creevy et al. (2003) explored the dislocation between pure financial theories and the\ncollective and individual behaviours of market traders. Their research showed that biases led to\nineffective decision making and reduced performance.\nIt is of concern that very little use of this extensive, often empirically based literature has been made\nin developing HRA methodologies. Indeed, the mechanistic approach common to many such\nmethodologies based on fault tree representations of human action assumes that the operators are\nusing System 2 thinking when in all probability their intuitive responses and actions are guided by\n- 12 -\nSystem 1 thinking (Bargh et al. 1996) 5 . HRA methodologies should model the thinking and\nbehaviours that are likely to occur rather than more rational, analytic actions and responses that one\nshould like to think would occur.\nIn fairness to some current approaches to quantitative HRA, their proponents would not claim to be\nmodelling actual behaviour, whether it be driven by System 1 or System 2 thinking; nor to be seeking\na \u2018correct\u2019 answer to a quantitative problem. When risk analysis is used formatively, its purpose is to\nunderstand better systems and identify the key drivers of risk, rather than chase quantified estimates\nper se. Current HRA methods may help identify the key drivers relating to human behaviour,\nirrespective of what is going on inside people\u2019s heads and whatever organisational and environment\ncontexts that surround them. However, such approaches do need data: and while there is generally no\ngreat problem in finding data relating to normal operations, appropriate data are \u2013 fortunately! \u2013\nsparse in most contexts relating to serious system failures.\nIf we are to model actual behaviour in a variety of circumstances, then the concept of self-regulation\nmay be needed. Individual self-regulation is defined as the internal and behavioural adjustments that\nfunction to maintain factors such as cognitions, emotions and performance within acceptable limits\n(Lord and Levy 1994). This approach to modelling behaviour proposed that behaviour is goal\norientated and there are internal, hierarchical processes that enable people to put thoughts into actions\nthrough activation and inhibition of decision making processes (Carver and Scheier 1981). Some of\nthe decision processes take place at a subconscious level and never reach conscious deliberation, a\nprocess called automaticity (Bargh and Chartrand 1999). Thus, there is a dynamic interaction between\npeople and their environment that is designed for effective behaviour. Models of decision making and\nbehaviour that incorporate optimal levels of functioning have a long history and a range of\norganisational applications. For example, Yerkes and Dodson (1908) introduced an inverted U model\nof the association between performance and arousal. More recent models of work performance show\nsimilar patterns: some effort and pressure can be effective, too much of either leads to burnout\n(Schaufeli and Bakker 2004). The organisational context must be considered both as an influence on\nindividual level decision making and as an integral outcome of individual and group decision making\nprocesses. Choices are made at all levels of organisational design that are potentially subject to the\nsame processes of automaticity, flawed biases and self-regulation as individual decision making.\nThis recognition that human behaviour is complex and driven by a range of internal and external\nfactors leads us to question the value of terminology such as \u2018error\u2019, \u2018slip\u2019 or \u2018failure\u2019 within HRA.\nHuman errors and faults are socially defined events: a perfectly reasonable action to one person may\n5 Of course, one might hope that if operators have been subject to many training exercises, then their\nresponses may be closer to those that would arise from system 2 thinking.\n- 13 -\nbe an unreasonable failure to another (Hollnagel 2000b). Furthermore, however well judged a\ndecision may be a priori, it may through \u2018ill fortune\u2019 lead to unwanted outcomes. Hence what may\nseem an error in hindsight may not be the outcome of irrational or erroneous choice. We should focus\nmore on human behaviour in individual, group and organisational contexts and recognise its potential\ninvolvement in system failure \u2013 without the pejorative judgement of whether that behaviour is\naberrant in any sense. For example, in the Three Mile Island Incident (Commission on the Three Mile\nIsland Accident 1979) the initiating event \u2013 the formation of a hydrogen bubble which forced down\ncooling water exposing the core \u2013 had not been anticipated in the reactor\u2019s design or safety studies.\nThe operators not only did not recognise what was happening, but also had never anticipated that it\nmight. It was an incident beyond their experience and imagination, in a very real sense outside of\nscientific and engineering knowledge as it stood then. The operators behaved entirely sensibly and in\naccordance with their mental models of what they believed was happening. There was no error in\ntheir behaviour in this respect, not at least in the sense of human error within HRA theory. As we\nbuild and operate more and more complex systems, we should recognise that it is inevitable that we\nwill encounter unanticipated events and conditions. Risk and reliability analyses need to take account\nof human responses to these and, although those responses may indeed lead to untoward outcomes, it\nis far from clear that they should be dubbed errors.\n4. High reliability organisations\nThe past 20 years has seen several studies of high reliability organisations (HROs), which Roberts\n(1990) defined as organisations failing with catastrophic consequences less than one time in 10,000.\nThese studies recognise that certain kinds of social organisation are capable of making even\ninherently vulnerable technologies reliable enough for a highly demanding society.\nAn HRO encourages a culture and operating style which emphasises the need for reliability rather than\nefficiency (Weick 1987). As organisations, HROs emphasise a culture of learning, although they\nclearly do not rely in any sense of learning from mistakes! Instead, HROs resort to learning from\nimagination, vicarious experience, stories, simulations and other symbolic representations (Weick\n1987). They emphasise a culture of sharing of learning and knowledge, of mental models: \u2018heedful\ninter-relating\u2019 (Weick and Roberts 1993), \u2018collective mindfulness\u2019 (Weick et al. 1999),\n\u2018extraordinarily dense\u2019 patterns of cooperative behaviour (La Porte 1996) and \u2018shared situation\nawareness\u2019 (Roth et al. 2006). Usually HROs apply a strategy of redundancy (Rochlin et al. 1987)\nwith teams of operators \u2018watching each others backs\u2019. As noted, it is suggested that teams share\ncommon mental models of both their internal organisational processes and the external world\n(Mathieu et al. 2000; Smith-Jentsch et al. 2005). Redundancy may increase complexity of operations\nas it makes the operations system less easily understood or opaque (Perrow 1984; Sagan 1993).\nHowever, redundancy also increases the probability or chance of getting adequate information to\n- 14 -\nsolve probable dangers, consequently reducing the risks arising from complexity rather than\nincreasing them. When necessary, HROs try to decentralise the authority of senior teams or\nmanagement responsible for decision making. Rijpma (1997) suggests that HROs use decentralisation\nto enable those working closest to any problems to address and solve them as they emerge or become\napparent. Using this method rapid problem solving is achieved, resulting in an increase in reliability\nand reduction of the risk of accidents occurring in highly critical situations. This decentralisation may\nincrease the complexity of the organisation as knowledge and lines of authority need to be distributed,\nbut La Porte (1996) suggests the balance of these opposing effects can lie in the direction of higher\nreliability.\nThere are several challenges that have been mounted to the HRO line of work. First, some suggest that\nHRO perspectives are heavily functionalist and neglect politics and group interests (Perrow 1994;\nSagan 1993, 1994). A second criticism relates to the absence of validation for the empirical studies\nunderpinning HRO theory (Clarke 1993; Perrow 1994; Sagan 1993). Critics argue that the context of\nsome of the most important HRO studies, e.g. on the flight decks of aircraft carriers, is misleading,\nwith evidence of safety only in simulated rather than actual operations. Others argue that the\nmechanisms and qualities that are said to underlie the achievement of high reliability are neither\nparticularly characteristic of HROs nor unequivocally good for reliability. But the HRO work has\ngiven us an insight into the way in which error and failure is managed by social organisations, and\nhow collective, rather than individual, phenomena like collective mindfulness (Weick et al. 1999) are\nwhat produce reliability in the face of supposedly unreliable individuals and unreliable technologies.\nThe emphasis of HRA on individuals and on atomised tasks therefore misses the probability that\ncollective actions and behaviours might lead to or avert system failure.\nThere would seem to be considerable potential for formative uses of HRA to influence the\ndevelopment of HRO theory, at least in so far as it can be applied in system and organisational design;\nand vice versa, complementing the work of, e.g., Grabrowski and Roberts (1999).\n5. Decision contexts\nThere is a further aspect of context that HRA should consider: decision context. The judgements and\ndecisions needed of humans in a system can vary from those needed to perform mundane repetitive\noperational tasks through more complex circumstances in which information needs to be sought and\nevaluated to identify appropriate actions to the ability to react to and deal with unknown and\nunanticipated. Decision processes will vary accordingly. Design decisions can inadvertently\nintroduce further risks to the system that arise from limitations inherent in human foresight. This\nmeans that the appropriate HRA methodology to assess the risks associated with the human decision\nmaking behaviour may vary with the details of that context.\n- 15 -\nCynefin is a conceptual framework developed by\nSnowden which, among other things, offers a\ncategorisation of decision contexts (Snowden\n2002; Snowden and Boone 2007). The Cynefin\nmodel roughly divides decision contexts into\nfour spaces: see Figure 2. In the known space, or\nthe Realm of Scientific Knowledge, the\nrelationships between cause and effect are well\nunderstood. All systems and behaviours can be\nfully modelled. The consequences of any course\nof action can be predicted with near certainty. In\nsuch contexts, decision making tends to take the\nform of recognising patterns and responding to\nthem with well rehearsed actions. Klein (1993) discusses such situations as recognition primed\ndecision making. In the knowable space, the Realm of Scientific Inquiry, cause and effect\nrelationships are generally understood, but for any specific decision there is a need to gather and\nanalyse further data before the consequences of any course of action can be predicted with any\ncertainty. Decision making can be proceduralised with clear guidance decided a priori. In the\ncomplex space, often called the Realm of Social Systems though such complexity can arise in\nenvironmental, biological and other contexts, decision making situations involve many interacting\ncauses and effects. Knowledge is at best qualitative: there are simply too many potential interactions\nto disentangle particular causes and effects. Before decisions can be made, it is necessary to think\nwidely, explore issues, frame the problem and develop broad strategies that are flexible enough to\naccommodate changes as the situation evolves. Much judgement and expertise will be needed in\nmaking the decision itself. Finally, in the chaotic space, situations involve events and behaviours\nbeyond our current experience and there are no obvious candidates for cause and effect. Decision\nmaking cannot be based upon analysis because there are no concepts of how separate entities and\npredict their interactions. Decision makers will need to take probing actions and see what happens,\nuntil they can make some sort of sense of the situation, gradually drawing the context back into one of\nthe other spaces. The boundaries between the four spaces should not be taken as hard. The\ninterpretation is much softer with recognition that there are no clear cut boundaries and, say, some\ncontexts in the knowable space may well have a minority of characteristics more appropriate to the\ncomplex space.\nThe Cynefin framework provides a structure in which to articulate some concerns about the use if\nHRA in risk and reliability analysis and in relation to HRO studies.\nCause and effect can\nbe determined with\nsufficient data\nKnowable\nThe Realm of\nScientific Inquiry\nComplex\nThe Realm of Social Systems\nCause and effect may be\ndetermined after the event\nChaotic\nCause and effect\nnot discernable\nKnown\nThe Realm of Scientific\nKnowledge\nCause and effect understood\nand predicable\nFigure 2: Cynefin\n- 16 -\n\uf0b7 First generation HRA methodologies and arguably most of second and third generation ones focus\non repetitive, operational tasks that lie in the known or, perhaps, knowable spaces. Yet many of\nthe perceived risks in modern systems arise because of their inherent complexity (Perrow 1984,\n1994). In other words, we need be concerned with human behaviour as managers and operators\nstrive to deal with events happening in the complex or even chaotic spaces. The Chernobyl\nAccident was initially managed as if it were in the known and knowable spaces, yet it was one of\nthe most complex socio-technical accidents that have occurred (French and Niculae 2005). In the\nThree Mile Island Accident initially there was no conceptual understanding of the processes by\nwhich a hydrogen bubble might form and hence decision making in the first hours and days of\nhandling the incident took place in the chaotic space.\n\uf0b7 It is informative to read HRO studies from the perspective of Cynefin. For instance, Weick\u2019s\n(1987) discussion moves from discussions of how air traffic controllers manage flights in a highly\nreliable way \u2013 a repetitive task in the known\/knowable spaces \u2013 and uses these to discuss how\nteams might react to complex events such as Bhopal, the decision to launch Challenger and the\nThree Mile Island Accident. It is far from clear that organisational practices that enable repetitive,\nintrinsically dangerous operations to be carried out safely can be used to develop organisational\npreparedness dealing with complex situations that bring many risks, some quite unanticipated.\n(For discussions of the tension between operational risk management practice, and incident\npreparedness and management, see, e.g., Jalba et al. 2009; Pollard et al. 2009).\nThe appropriateness of any HRA methodology may depend on the context that is being assessed. As\nis the case with all risk methodologies, the characteristics of the risk and the availability of data to\nsupport the application of specific tools and techniques has a forceful influence on their feasible use.\nAre we considering a repetitive task that an operator performs in the normal course of events? In this\ncase we need modelling approaches that fit with behaviours in the known domain. Or are we looking\nat the response of an operator to something unexpected that may herald an unanticipated departure of\nthe system from its normal operating characteristics? In this case we need modelling behaviours for\nthe knowable, complex or even chaotic domain. For repetitive events the key contextual pressures on\noperators that may modify their behaviour are likely to relate to complacency and organisational\nissues such as excessive workloads or requirements to work at the same task too long. External\npressures and distractions such family problems or a national sporting event are more likely to affect\nbehaviour in repetitive normal operations than in responding to the unexpected. In responding to\nevents ranging from an indication of departure from normal operations to a full blown crisis,\nadrenaline, the importance of the matter, as well as cognitive interest are likely to focus the mind. So\nthe operators\u2019 performance is more likely to be affected by issues such as cognitive overload,\nmiscommunication between several operations and a range of behaviours that we commonly call\npanic! Organisational contexts that affect the operators\u2019 responses relate to, inter alia, the provision\n- 17 -\nof training, including emergency simulations in a variety of scenarios, and the establishment of\ncommon mental models among response teams and, more generally, of supportive team behaviours.\nOur contention is that the variety of tasks that HRA is called upon to perform and the range of contexts\nin which it is applied are so great that it would be optimistic in the extreme to expect one\nmethodology to be sufficient to meet these requirements. Hollnagel (1998) recognised this, though\nhis suggestion of two methods probably does not take us much further forward, particularly as his\nbasic method is more of a screening method for his extended approach rather than appropriate to a\ndifferent set of circumstances. What we believe is needed is a portfolio of HRA methods. The\ncharacteristics of each need to be well understood so that we can determine the appropriate contexts\nfor its application and appreciate its accuracy. It is also important to work out a way of integrating\nthem so that we do not perpetuate the fallacy of thinking tasks can be divided up and broken down,\nand methods can be selected in isolation.\n6. Toward an extended model of HRA\nSummative HRA and related approaches emphasise quantification and prediction. While cognitive\nunderstanding of people and cultural perspectives on organisations are acknowledged, the gulf\nbetween these and quantitative risk models is generally considered too significant to be bridged. Yet\nthe conjoining of these approaches could yield a superior model of safety critical organisations and\nthe people working within them. In the short term, exploring the interfaces between HRA and\nbehavioural, organisational and related studies is likely to benefit formative analyses to support the\ndesign and operation of complex systems. The barriers to the quantification needed in summative\nanalyses are currently too substantial \u2013 we do not have sufficiently developed and validated models of\nbehaviour and organisations to provide the precision needed. Moreover, progress in improving and\ndeveloping quantitative HRA methods is likely to proceed most quickly in relation to tasks and\nactivities falling in the known and, perhaps, knowable Cynefin spaces. Successful quantitative\nmodelling of such tasks and activities depends on having sufficient data to develop and validate\nmodels. For systems that are long established or straightforward developments thereof, we are likely\nto have useful data. For novel systems we might generate such data by involving operators in\nsimulations of component tasks and activities in known and knowable spaces.  \u0108epin (2008) has \nsuggested as much, though without the language of Cynefin.  \u0108epin\u2019s modelling, as might be \nexpected from our discussion, is focused on probabilistic assessments of errors of omission and\ncommission. His proposed development focuses on manufactured situations with tight parameters.\nWhile additional data gathered within such a paradigm would add considerable utility to HRA models,\nthere remains the issue of scope. The approach cannot be easily extended to tasks and activities in the\nother spaces. By definition, in the complex space we have neither sufficient qualitative understanding\n- 18 -\nnor relevant data to develop quantitative HRA models that predict individual, group and organisational\nhuman behaviour and how these may impact overall system reliability and safety.\nThus we believe that the dominant HRA paradigm, suited as it is for the known and knowable spaces,\nneeds to be complemented by paradigms developed specifically for the complex space. In achieving\nthis, we will need to move away from many systems engineering approaches in which hazards are\npurportedly designed out of a system. Complex systems involve some human activity if only in their\ndesign and hence are susceptible to some risk arising from human behaviour. Such systems\nengineering approaches may work in the known or knowable spaces \u2013 and there the question is moot.\nEven the simplest systems in the known space need to be designed and that is a human activity.\nMoreover the risk homeostasis model suggests that there can be an over-reliance the safety promised\nby the system and a concomitant increase in overall risk. But in the complex space, we have no such\nhopes that current approaches can design hazards out of the system.\nIt will not be easy to develop models for new HRA paradigms suitable for the complex space. For\ninstance, organisational behavioural studies can be useful in identifying the individual, cultural and\norganisational factors relevant to system safety; but they do not lend themselves to simple\nquantification; indeed, it is the very nature of the complex space that quantification is difficult if not\nimpossible on current knowledge. We are also limited both by the availability of data from real\nincidents and from the generalisability of laboratory based studies. Some commentators, notably Le\nCoze (2005), consider the question of whether current forms of organisations can lend themselves to\neffective modelling. Furthermore, linear models of cause and effect cannot be simply applied (Morin\n1977). Le Coze provides a useful analysis of organisational theories, and their limitations. He\nproposes that approaches from complexity theory6 (Morin and Lemoigne 1999; Prigogine 1994;\nSimon 1996) could assist in integrating methodologies. One of the contributions of complexity theory\nis the guiding philosophy that complex problems cannot be meaningfully decomposed and retain\nutility since the whole is greater than the sum of its parts. Le Coze concludes by emphasizing the need\nfor holistic approaches to organisations with additional data from both organisational events and\nempirical studies.\nAnother family of approaches that might lead to a broadened conceptualisation of HRA in the complex\nspace are the socio-technical (Mumford 2000). For instance, Reiman and Oedewald (2007) propose\nthat safe and effective organisations can arise only when there is integration of organisational culture\nand organisational activities. Their model includes a range of qualitative and quantitative methods\ndesigned to elicit descriptions of the cultural features and the organisational core tasks resulting in a\n6 There are differences between complexity theory and Snowdon\u2019s notion of the complex space in Cynefin,\nbut there are also similarities and these link Le Coze\u2019s and our arguments.\n- 19 -\nthorough understanding of alternative ways to approach organisational thinking, strengths and\nweaknesses of practices and opportunities to create dialogue regarding the effectiveness of work.\nReiman and Oedewald\u2019s paper represents a useful contribution to the development of the field of\nsocio-technical systems and their potential links with more quantitative approaches to error and risk.\nWhat they do not encompass are individual approaches to understanding organisational behaviour.\nTheir work represents another step in the right direction but there is a long way to go before human\nactivities and behaviours in complex space can be modelled sufficiently for quantitative HRA.\nNone of the above addresses activities and behaviours which might arise if through some\nunanticipated event the system \u2018moves into\u2019 the chaotic space: e.g. the unanticipated formation of a\nhydrogen bubble in the Three Mile Island Incident (Commission on the Three Mile Island Accident\n1979; Niculae 2005). By definition these characteristics cannot be represented in a model \u2013 certainly\nnot in anything other than a schematic manner \u2013 simply because the chaotic space represents that part\nof our environment that we do not understand yet and so cannot predict.\nSo to take stock: current quantitative HRA methodologies seem applicable to behaviours and activities\nin the known and knowable spaces. There are the barest hints of how some quantitative models might\nbe developed to predict the impacts of human activities and behaviours in the complex spaces; and, by\ndefinition, it is logically inconceivable that we can develop quantitative models for the chaotic space.\nThus it is not currently possible to perform summative risk and reliability analyses for any system in\nwhich human behaviour and activity can enter the complex or chaotic spaces. Governments and\nregulators should be concerned because this accounts for the majority of the technological systems\ncurrently being operated and commissioned. This does not mean that they are unreliable or unsafe;\nonly that we cannot assure their reliability or safety to within some negligibly small probability. But\nthere are ways forward.\nFirstly and most immediately, we can look to formative uses of HRA, the behavioural and\norganisational sciences and many other related disciplines to inform the design of organisational and\nmanagement structures and the establishment of appropriate safety cultures to improve the systems\nthat we have and are designing. This will not be easy because the imperatives that drive this approach\nfly in the face of the dominant reductionist thinking in risk and reliability communities. One cannot\nsimply decompose systems into smaller subsystems, focus on these in turn and expect these to\nrepresent the total system, because culture, organisational structures and other drivers of human\nbehaviour correlate actions, judgements and decision making in the different subsystems. Modern\nperspectives on risk demand a systemic rather than an atomised perspective of the technical, human\nand organisational features of systems. Further, because many systems have shared, and arguably,\noften fragmented responsibilities for management and risk management (e.g. flood defence, social\ncare, biosecurity in the food chain), one needs to take a more holistic perspective. The\nconceptualisation offered by Cynefin may again give us a way forward. The simple visual\n- 20 -\ncategorisation of different decision contexts has proven very successful in one of the author\u2019s\nexperiences in helping in problem formulation and issue structuring (Franco et al. 2006, 2007;\nMingers and Rosenhead 2004; Rosenhead and Mingers 2001). The managers who decide on the\nchoice of managerial system, its components and operational processes could map these onto a\nCynefin diagram. The discussions and deliberations that would occur as they undertook this would\nnaturally surface many issues that their design and management decisions would need to address. In\nother words, we propose a careful use and reflection upon a Cynefin mapping would augment current\nhazard identification procedures and make clearer some of the issues relating to human behaviour that\nmanagement will face in operating the system. When they identify that issue although important lies\nin the known or knowable space they can look to current HRA \u2013 or, preferably, somewhat enhanced \u2013\nmodels to guide their thinking and planning. But when discussion identifies an issue as lying in the\ncomplex space then they will to rely much more on judgement and put into place management\nprocesses that can deal with behaviours more subtly than seeking to police against \u2018slips, errors, and\nomissions\u2019.\nWe also believe that in time it will be possible to develop better quantitative HRA methodologies to\ngive wider assurance at the summative level. But it is unlikely that this will lead to single\nmethodology. Rather we will need a multi-faceted approach that combines empirically validated HRA\nmodels for the known and knowable spaces with more judgementally based methods for the complex\nspace. The Cynefin model suggests a broad framework with which to categorise the human tasks and\nactivities in system to determine which form of HRA modelling would be most appropriate; but it is\nonly a broad framework. To develop this methodology it will probably need extending to recognise,\namong other things:\n\uf0b7 whether the human behaviours and activities take place at the individual, group, organisational\nlevel;\n\uf0b7 the wider organisational context \u2013 including strategic and economic imperatives \u2013 in which the\nteams and local management structures are embedded;\n\uf0b7 the team and local management structures which set the local context in which the operators work;\n\uf0b7 the cultural context and \u2013 including misplaced trust in other safety barriers in the system \u2013 in\nwhich the operators find themselves;\n\uf0b7 external influences, particularly those arising from larger external and societal pressures;\n\uf0b7 the historical context, including perhaps the lack of recent incidents leading to a growth of\ncomplacency.\n- 21 -\nIn Adhikari et al.(2008) we outline a programme of research and benchmarking that may help us\ndevelop such a multi-faceted portfolio of HRA methodologies that may eventually provide much\nbetter summative guidance on the risks inherent in complex systems.\nNone of this will be easy and it will only be possible if we can break the current mechanistic\nparadigms that permeate the risk and reliability communities. We need to move on from the Swiss\nCheese model.\n7. Conclusion: a message for managers\nThe key point that we have been trying to convey in this paper is the current dislocation between the\nmechanistic reductionist assumptions on which current HRA methodologies are primarily built and\nour current understandings of human and organisational behaviour. We must bring these into better\nregister. Managers, regulators, politicians and the public need to beware of this lest they believe the\nnumbers that are sometimes touted about the safety of our systems. This should not be read as a\nmanifesto for Luddism. We are not against the development of more and more complex systems,\nproviding that they bring benefits, of course. Nor are we against risk per se. Rather we are concerned\nat the prevalence of overconfidence in our ability to assess the risks that arise from human behaviour.\nWe need to take the numbers with that \u2018pinch of salt\u2019, recognising that when we build complex\nsystems our uncertainty is greater than the raw numbers suggest and we need to monitor and watch for\nthe unanticipated. As is often the case with the application of risk and reliability tools, the valuable\ninsight comes from a systemic and often qualitative understanding of which systems features \u2018drive\u2019\nthe risk, rather than from the risk estimates per se.\nWe in the research community have much to do. But so does the management community. It is too\neasy to trust the assurances of current risk and reliability analyses which promise that the chance of an\nuntoward event is small, to believe in the cumulative effect of \u2018independent\u2019 safety barriers and to\nmanage the subsystems separately unaware of the interconnections between them that organisational\nculture and human behaviour bring. Human reliability has too long been treated as something that\nrelates to individuals. It needs to be seen and managed at the organisational level. The key question\nis not how likely is an individual\u2019s behaviour is to impact a system, but how well the organisational\nstructures around and within that system enable the system to run safely and reliability, and how well\nthey will recover if an untoward event threatens or happens.\nAcknowledgements\nThis work was supported by the Engineering and Physical Sciences Research Council (Contract\nnumber: EP\/E017800\/1). We are grateful to our co-investigators and colleagues on this: Sondipon\nAdhikari, Clare Bayley, Jerry Busby, Andrew Cliffe, Geeta Devgun, Moetaz Eid, Ritesh Keshvala,\n- 22 -\nDavid Tracy and Shaomin Wu. We are also grateful for many helpful discussions with Ronald Boring,\nRoger Cooke and John Maule.\n- 23 -\nReferences\nAdams, J. 1988. Risk homeostasis and the purpose of safety regulation. Ergonomics. Ergonomics 31(4) 407 -\n428.\nAdhikari, S., C. Bayley, T. Bedford, J.S. Busby, A. Cliffe, G. Devgun, M. Eid, S. French, R. Keshvala, S.\nPollard, E. Soane, D. Tracy, S. Wu. 2008. Human Reliability Analysis: A Review and Critique. Manchester\nBusiness School, Booth Street West, Manchester M15 6PB.\nAven, T. 2003. Foundation of Risk Analysis: a Knowledge and Decision Oriented Perspective. John Wiley and\nSons, Chichester.\nBargh, J.A., T.L. Chartrand. 1999. The unbearable automaticity of being. American Psychologist 54 462-479.\nBargh, J.A., M. Chen, L. Burrows. 1996. Automaticity of social behavior: direct effects of trait construct and\nstereotype activation on action. Journal of Personality and Social Psychology 71 230-244.\nBarlow, R.E., F. Proschan. 1975. Statistical Theory of Reliability and Life Testing. Holt, Reinhart and Winston,\nNew York.\nBarriere, M., D. Bley, S. Cooper, J. Forester, A. Kolaczkowski, W. Luckas, G. Parry, A. Ramey-Smith, C.\nThompson, D. Whitehead, J. Wreathall. 2000. NUREG-1624: Technical basis and implementation guidelines\nfor a technique for human event analysis (ATHEANA). US Nuclear Regulatory Commission.\nBazerman, M. 2006. Managerial Decision Making, 6th ed. John Wiley and Sons, New York.\nBazerman, M.H. 1999. Reviews on decision making. Administrative Science Quarterly 44(1) 176-180.\nBedford, T., R. Cooke. 2001. Probabilistic Risk Analysis: Foundations and Methods. Cambridge University\nPress, Cambridge.\nBoard of Inquiry. 2005. Fractured pipe with loss of primary containment in the THORP feed clarification cell.\nBritish Nuclear Fuels Limited.\nBoring, R.L. 2007. Dynamic human reliability analysis: benefits and challenges of simulating human\nperformance European Safety and Reliability Conference (ESREL 2007). INL\/CON-07-12773, Idaho National\nLaboratory.\nCarver, C.S., M.F. Scheier. 1981. Attention and Self-Regulation: a Control Theory Approach to Human\nBehavior. Springer Verlag, New York.\n\u0108epin, M. 2008. Importance of human contribution within the human reliability analysis (IJS-HRA). Journal of\nLoss Prevention in the Process Industries 21(3) 268-276.\nChaiken, S., A. Liberman, A.H. Eagly. 1989. Heuristic and systematic information processing within and\nbeyond the persuasion context. J.S. Uleman, J.A. Bargh, eds. Unintended Thought. Guilford, New York, 212-\n252.\nClarke, L. 1993. Drs Pangloss and Strangelove meet organizational theory: high reliability organizations and\nnuclear weapons accidents. Sociological Forum 8 675-689.\nCommission on the Three Mile Island Accident. 1979. Report of The President's Commission on the Accident at\nThree Miles Island. US GPO, Washington DC.\nCourtois, P.-J., B. Littlewood, L. Strigini, D. Wright, N. Fenton, M. Neil. 2000. Bayesian belief networks for\nsafety assessment of computer-based systems. E. Gelenbe, ed. System Performance Evaluation: Methodologies\nand Applications. CRC Press, 349-363.\n- 24 -\nFadier, E. 2008. Editorial of the Special Issue: Design Process and Human Factors Integration. Cognition,\nTechnology and Work 10(1) 1-5.\nFadier, E., J. Ciccotelli. 1999. How to integrate safety in design: methods and models. Human Factors and\nErgonomics in Manufacturing & Service Industries 9(4) 367-379.\nFadier, E., C. De la Garza. 2006. Safety design: towards a new philosophy. Safety Science 44 55\u201373.\nFenton-O\u2019Creevy, M., N. Nicholson, E. Soane, P. Willman. 2003. Trading on illusions: unrealistic perceptions\nof control and trading performance. Journal of Occupational and Organizational Psychology 76(1) 53-68.\nFenton-O\u2019Creevy, M., E. Soane, N. Nicholson, P. Willman. 2008. Thinking, feeling and deciding: The influence\nof emotions on the decision making and performance of traders Academy of Management Conference, Anaheim,\nCalifornia. .\nFinucane, M.L., A. Alhakami, P. Slovic, S.M. Johnson. 2000. The affect heuristic in judgments of risks and\nbenefits. Journal of Behavioral Decision Making 13 1-17.\nForester, J.A., A. Kolaczkowski, E. Lois, D. Kelly. 2006. NUREG-1842: Evaluation of human reliability\nanalysis methods against good practices. US Nuclear Regulatory Commission, Washington, DC.\nFranco, A., D. Shaw, M. Westcombe. 2006. Problem Structuring Methods I Journal of the Operational\nResearch Society, 757-878.\nFranco, A., D. Shaw, M. Westcombe. 2007. Problem Structuring Methods II Journal of the Operational\nResearch Society, 545- 682.\nFrench, S., A.J. Maule, K.N. Papamichail. 2009. Decision Behaviour, Analysis and Support. Cambridge\nUniversity Press, Cambridge.\nFrench, S., C. Niculae. 2005. Believe in the Model: Mishandle the Emergency. Journal of Homeland Security\nand Emergency Management 2(1).\nGoldstein, D.G., G. Gigerenzer. 2002. Models of ecological rationality: the recognition heuristic. Psychological\nReview 109(1) 75-90.\nGrabowski, M., K.H. Roberts. 1999. Risk mitigation in virtual organizations. Organization Science 10 704-721.\nHannaman, G.W., A.J. Spurgin, Y.D. Lukic. 1984. Human cognitive reliability model for PRA analysis. Draft\nReport NUS-4531, EPRI Project RP2170-3. Electric Power and Research Institute, Palo Alto, CA.\nHelmreich, R.L. 2000. On error management: lessons from aviation. British Medical Journal 320(7237) 781\u2013\n785.\nHollnagel, E. 1993. Human Reliability Analysis: Context and Control. Academic Press, London.\nHollnagel, E. 1998. Cognitive Reliability and Error Analysis Method \u2013 CREAM. Elsevier Science, Oxford.\nHollnagel, E. 2000a. Looking for errors of omission and commission or The Hunting of the Snark revisited\nReliability Engineering and System Safety 68 135-145.\nHollnagel, E. 2000b. Looking for errors of omission and commission or The Hunting of the Snark revisited.\nReliability Engineering and System Safety 68 135\u2013145.\nH\u00f8yland, A., M. Rausand. 1994. System Reliability Theory. John Wiley and Sons, New York.\nHrudey, S.E., E.J. Hrudey, J.W.A. Charrois, S.J.T. Pollard. 2006. A \u2018Swiss cheese\u2019 model analysis of the risk\nmanagement failures in the fatal Walkerton outbreak. IWA world water congress and exhibition, Beijing, China.\n- 25 -\nInternational Atomic Energy Agency. 1991. The International Chernobyl Project: Technical Report. IAEA,\nVienna.\nJalba, D., N. Cromar, S. Pollard, J.W.A. Charrois, R. Bradshaw, E. Hrudey. 2009. Safe drinking water: critical\ncomponents of effective inter-agency relationships. Environment International (in press -\ndoi:10.1016\/j.envint.2009.1009.1007).\nKahneman, D., P. Slovic, A. Tversky, eds. 1982. Judgement under Uncertainty. Cambridge University Press,\nCambridge.\nKahneman, D., A. Tversky, eds. 2000. Choices, Values and Frames. Cambridge University Press, Cambridge.\nKariuki, S.G., K. Lowe. 2007. Integrating human factors into process analysis. Reliability Engineering and\nSystem Safety 92 1764-1773.\nKlein, G. 1993. A recognition primed decision model (RPM) of rapid decision making. G. Klein, ed. Decision\nMaking in Action: Models and Method. Ablex.\nLa Porte, T.R. 1996. High reliability organizations: unlikely, demanding and at risk. Journal of Contingencies\nand Crisis Management 4 60-71.\nLe Coze, J.-C. 2005. Are organisations too complex to be integrated in technical risk assessment and current\nsafety auditing? Safety Science 43(8) 613-638.\nLeplat, J. 1994. Collective dimensions of reliability: some lines of research. European Work and Organizational\nPpsychologist 4(3) 271-295.\nLewis, E.E. 1994. Introduction to Reliability Engineering. Johne Wiley and Sons, Chichester.\nLoewenstein, G., E.U. Weber, C.K. Hsee, N. Welch. 2001. Risk as feelings. Psychological Bulletin 127(2) 267-\n286.\nLord, R.G., P.E. Levy. 1994. Moving from cognition to action \u2013 a control theory perspective. Applied\nPsychology - An International Review (Psychologie appliquee - Revue Internationale) 43(3) 335-398.\nLyu, M.R. 2005. Handbook of Software Reliability Engineering. IEEE Computer Society Press and McGraw-\nHill Publishing Company.\nMarcus, A., M.L. Nichols. 1999. On the edge: heeding the warnings of unusual events. Organisazational\nScience 10(4) 482-499.\nMarples, D.R. 1997. Nuclear power in the former USSR: historical and contemporary perspectives D.R.\nMarples, M.J. Young, eds. Nuclear Energy and Security in the Former Soviet Union. Westview Press.\nMathieu, J.E., T.S. Heffner, G.F. Goodwin, E. Salas, J.A. Cannon-Bowers. 2000. The influence of shared mental\nmodels on team process and performance. Journal of Applied Psychology 85(2) 273-283.\nMelnick, E.L., B.S. Everitt, eds. 2008. Encyclopedia of Quantitative Risk Analysis and Assessment. John Wiley\nand Sons, Chichester.\nMingers, J., J. Rosenhead. 2004. Problem Structuring Methods in Action. European Journal of Operational\nResearch 152 530-554.\nMorin, E. 1977. La method \u2013 tome I, La nature de la nature. Ed du seuil (coll point), Paris (The method \u2013 Vol I,\nthe nature of nature).\nMorin, E., J.L. Lemoigne. 1999. L\u2019intelligence de la complexit\u00e9. L\u2019Harmattan (The intelligence of complexity)\n- 26 -\nMosleh, A., Y.H. Chang. 2004. Model-based human reliability analysis: prospects and reliability. Reliability\nEngineering and System Safety 83 241-253.\nMumford, E. 2000. A socio-technical approach to systems design. Requirements Engineering 5 125-133.\nNiculae, C. 2005. A socio-technical perspective on the use of RODOS in nuclear emergency management, The\nUniversity of Manchester.\nPerneger, T.V. 2005. The Swiss cheese model of safety incidents: are their holes in the metaphor. BMC Health\nServices Research 5 71-77.\nPerrow, C. 1984. Normal accidents: living with high-risk technologies. Basic Books, New York.\nPerrow, C. 1994. The limits of safety: the enhancement of a theory of accidents. Journal of Contingencies and\nCrisis Management 2 212-220.\nPhelps, E.A. 2006. Emotion and cognition: Insights from studies of the human amygdala. Annual Review of\nPsychology 57 27-53.\nPollard, S., R. Bradshaw, D. Tranfield, J.W.A. Charrois, N. Cromar, D. Jalba. 2009. Developing a risk\nmanagement culture \u2014 \u2018mindfulness\u2019 in the international water utility sector (Report TC3184). Water Research\nFoundation, Denver, CO.\nPrigogine, I. 1994. Les lois de chaos. Flammarion (The laws of chaos).\nReason, J. 1990a. The contribution of latent human failures to the breakdown of complex systems.\nPhilosophical Transactions of the Royal Society of London B327(1241) 475-484.\nReason, J. 1990b. Human error: models and management. British Medical Journal 320(7237) 768-770.\nReason, J. 1995. Understanding adverse events: human factors. Quality Health Care 4 80-89.\nReason, J. 1997. Managing the Risks of Organisational Accidents. Ashgate, Aldershot, UK.\nReiman, T., P. Oedewald. 2007. Assessment of complex socio-technical systems \u2013 theorerical issues concerning\nthe use of organisational culture and organisational core task concepts. Safety Science 45 745-768.\nRen, J., I. Jenkinson, J. Wang, D.L. Xu, J.B. Yang. 2008. A methodology to model causal relationships in\noffshore safety assessment focusing on human and organisational factors. Journal of Safety Research 39 87-100.\nRijpma, J.A. 1997. Complexity, tight-coupling and reliability: connecting normal accidents theory and high\nreliability theory. Journal of Contingencies and Crisis Management, 5(1).\nRoberts, K.H. 1990. Some characteristics of one type of high reliability organisation. Organization Science 1(2)\n160-176.\nRochlin, G.I., T.R. La Porte, K.H. Roberts. 1987. The self-designing high reliability organization: aircraft carrier\noperations at sea. Naval War College Review 40 76-90.\nRosenhead, J., J. Mingers, eds. 2001. Rational Analysis for a Problematic World Revisited. John Wiley and\nSons, Chichester.\nRoth, E.M., J. Multer, T. Raslear. 2006. Shared situation awareness as a contributor to high reliability\nperformance in railroad operations. Organization Studies 27 967-987.\nSagan, S.D. 1993. The Limits of Safety: Organizations, Accidents, and Nuclear Weapons. Princeton University\nPress, Princeton, NJ.\n- 27 -\nSagan, S.D. 1994. Toward a political theory of organizational reliability. Journal of Contingencies and Crisis\nManagement 2 228-240.\nSagan, S.D. 2004. The problem of redundancy problem [sic]: why more nuclear security forces may produce\nless nuclear security. Risk Analysis 24 935-946.\nSchaufeli, W.B., A.B. Bakker. 2004. Job demands, job resources, and their relationship with burnout and\nengagement: a multi-sample study. Journal of Organisational Behavior 25 293-315.\nSimon, H. 1996. The Sciences of the Artificial. MIT Press.\nSlovic, P., M.L. Finucane, E. Peters, D.G. MacGregor. 2004. Risk as analysis and risk as feelings: some\nthoughts about affect, reason, risk and rationality. Risk Analysis 24(2) 311-322.\nSmith-Jentsch, K.A., J.E. Mathieu, K. Kraiger. 2005. Investigating linear and interactive effects of shared\nmental models on safety and efficiency in a field setting. Journal of Applied Psychology 90(3) 523-525.\nSnowden, D. 2002. Complex acts of knowing - paradox and descriptive self-awareness. Journal of Knowledge\nManagement 6 100-111.\nSnowden, D., M. Boone. 2007. A leader's framework for decision making. Harvard Business Review 68-76.\nSwain, A.D., H.E. Guttmann. 1983. Handbook of Human Reliability Analysis with Emphasis on Nuclear Power\nPlant Applications. NUREG\/CR-1278, USNRC.\nUnited States Nuclear Regulatory Commission. 1975. Reactor safety study: an assessment of the accident risks\nin US commercial nuclear power plants.\nUnited States Nuclear Regulatory Commission. 2002. Review of Findings for Human Performance Contribution\nto Risk in Operating Events (NUREG\/CR-6753). US GPO, Washington, DC.\nWeick, K.E. 1987. Organisational culture as a source of high reliability. California Management Review 29 112-\n127.\nWeick, K.E., K.H. Roberts. 1993. Collective mind in organizations: heedful interrelating on flight decks.\nAdministrative Science Quarterly 38 357-381.\nWeick, K.E., K.M. Sutcliffe, D. Obstfield. 1999. Organizing for high reliability: processes of collective\nmindfulness. Research in Organizational Behavior 21 81-123.\nWilde, G.J.S. 1982. The theory of risk homeostasis: implications for safety and health. Risk Analysis 2 209-225.\nWilde, G.J.S. 1998. Risk homeostasis theory: an overview. Injury Prevention 4 89-91.\nWilliams, J.C. 1985. HEART \u2013 A proposed method for achieving high reliability in process operation by means\nof human factors engineering technology Proceedings of a Symposium on the Achievement of Reliability in\nOperating Plant, Safety and Reliability Society, NEC, Birmingham.\nWillman, P., M. Fenton-OCreevy, N. Nicholson, E. Soane. 2001. Knowing the risks: theory and practice in\nfinancial market trading. Human Relations 54(1) 887-910.\nWu, S., S.E. Hrudey, S. French, T. Bedford, E. Soane, S.J.T. Pollard. 2009. Human reliability analysis has a role\nin preventing drinking water incidents Water Research(in press).\nYerkes, R.M., J.D. Dodson. 1908. The relation of strength of stimulus to rapidity of habit-formation. . Journal of\nComparative Neurological Psychology 18 459-482.\nZhang, X., H. Pham. 2000. An analysis of factors affecting software reliability. Journal of Systems and Software\n50(1) 43-56\n- 28 -\n"}