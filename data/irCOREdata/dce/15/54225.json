{"doi":"10.1109\/TSMCB.2010.2042444","coreId":"54225","oai":"oai:eprints.lincoln.ac.uk:2684","identifiers":["oai:eprints.lincoln.ac.uk:2684","10.1109\/TSMCB.2010.2042444"],"title":"Invariant set of weight of perceptron trained by perceptron training algorithm","authors":["Ho, Charlotte Yuk-Fan","Ling, Bingo Wing-Kuen","IU, Herbert Ho-Ching"],"enrichments":{"references":[{"id":929227,"title":"A combined self-organizing feature map and multilayer","authors":[],"date":"1992","doi":"10.1109\/78.165652","raw":"Zezhen Huang and Anthony Kuh, \u201cA combined self-organizing feature map and multilayer IEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics perceptron for isolated word recognition,\u201d IEEE Transactions on Signal Processing, vol. 40, no. 11, pp. 2651-2657, 1992.","cites":null},{"id":933379,"title":"A constructive approach for nonlinear system identification using multilayer perceptrons,\u201d","authors":[],"date":"1996","doi":"10.1109\/3477.485881","raw":"Ju-Yeop Choi, Hugh F. VanLandingham and Stanoje Bingulac, \u201cA constructive approach for nonlinear system identification using multilayer perceptrons,\u201d IEEE Transactions on Systems, IEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics Man, Cybernetics\u23afPart B: Cybernetics, vol. 26, no. 2, pp. 307-312, 1996.","cites":null},{"id":932689,"title":"A joint compression-discrimination neural transformation applied to target detection,\u201d","authors":[],"date":"2005","doi":"10.1117\/12.382924","raw":"Alex Lipchen Chan, Sandor Z. Der and Nasser M. Nasrabadi, \u201cA joint compression-discrimination neural transformation applied to target detection,\u201d IEEE Transactions on Systems, Man, Cybernetics\u23afPart B: Cybernetics, vol. 35, no. 4, pp. 670-681, 2005.","cites":null},{"id":929914,"title":"Adaptive Ho-Kashyap rules for perceptron training,\u201d","authors":[],"date":"1992","doi":"10.1109\/72.105417","raw":"Mohamad H. Hassoun and Jing Song, \u201cAdaptive Ho-Kashyap rules for perceptron training,\u201d IEEE Transactions on Neural Networks, vol. 3, no. 1, pp. 51-61, 1992.","cites":null},{"id":932459,"title":"An adaptive high-order neural tree for pattern recognition,\u201d","authors":[],"date":"2004","doi":"10.1109\/tsmcb.2003.818538","raw":"G. L. Foresti and T. Dolso, \u201cAn adaptive high-order neural tree for pattern recognition,\u201d IEEE Transactions on Systems, Man, Cybernetics\u23afPart B: Cybernetics, vol. 34, no. 2, pp. 988-996, 2004.","cites":null},{"id":931970,"title":"An analysis of a class of neural networks for solving linear programming problems,\u201d","authors":[],"date":"1999","doi":"10.1109\/9.802909","raw":"Edwin K. P. Chong, Stefen Hui and Stanislaw H. \u017bak, \u201cAn analysis of a class of neural networks for solving linear programming problems,\u201d IEEE Transactions on Automatic Control, vol. 44, no. 11, pp. 1995-2006, 1999.","cites":null},{"id":934208,"title":"An introduction to dynamical systems: continuous and discrete,","authors":[],"date":"2004","doi":"10.3934\/dcds.2006.14.2i","raw":"R. Clark Robinson, An introduction to dynamical systems: continuous and discrete, Pearson Prentice Hall, 2004. BIOBIOGRAPHY Yuk-Fan Ho received the B.Eng. (Hons) degree from the department of Electrical and Electronic Engineering, the THong Kong University of Science and TechnologyT in 2000, and the MPhil. Degree from the department of Electronic and Information Engineering, theT Hong Kong Polytechnic University,T in 2003. She is pursuing her PhD study at the Queen Mary, University of London. Her research interests include image processing, filter banks and wavelets theory, functional inequality constrained optimization problems, symbolic dynamics as well as fuzzy and impulsive control theory. Bingo Wing Kuen Ling received the B.Eng. (Hons) and M.Phil. degrees from the department of Electrical and Electronic Engineering, the THong Kong University of Science and TechnologyT, in 1997 and 2000, respectively, and the Ph.D. degree from the department of Electronic and Information Engineering from theT Hong Kong Polytechnic UniversityT in 2003. In 2004, he joined the King\u2019s College London as a Lecturer. He has served as a technical committee member of several IEEE international conferences as well as an organizer of a special session in the International Symposium on Communication Systems, Networks and Digital Signal Processing at 2008 and 2010. He is the author of the textbook titled \u201cNonlinear Digital Filters: Analysis and Applications\u201d and an editor of the book titled \u201cControl of Chaos in Nonlinear Circuits and Systems\u201d. He has also served as a guest editor of IEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics a special issue on nonlinear circuits and systems in Circuits, Systems and Signal Processing. His research interests include investigations of applications of continuous constrained optimizations, symbolic dynamics, filter banks and wavelets as well as fuzzy and impulsive control theory. Herbert Ho-Ching Iu (S\u201998\u2013M\u201900\u2013SM\u201906) received the B.Eng.(Hons.) degree in electrical and electronic engineering from the University of Hong Kong, Hong Kong, in 1997, and the Ph.D. degree from the Hong Kong Polytechnic University, Hong Kong, in 2000. Since 2002, he has been with the School of Electrical, Electronic and Computer Engineering, University of Western Australia, Perth, Australia, where he was initially a Lecturer and is currently an Associate Professor. He was a Visiting Lecturer with the University of Reims Champagne-Ardenne, Reims, France, in 2004, and a Visiting Assistant Professor with the Hong Kong Polytechnic University in 2006. He is the author of more than 100 published papers. He currently serves as an Editorial Board Member for the Australian Journal of Electrical and Electronics Engineering. He is also an Associate Editor for the IEEE Circuits and Systems Society Newsletter and a Guest Associate Editor for International Journal of Bifurcation and Chaos. He is a Co-editor of Control of Chaos in Nonlinear Circuits and Systems (World Scientific, 2009). His research interests include power electronics, renewable energy, nonlinear dynamics, current sensing techniques, TCP dynamics, and computational intelligence.","cites":null},{"id":933861,"title":"Analysis of a four-layer series-coupled perceptron II,\u201d","authors":[],"date":"1962","doi":"10.1103\/RevModPhys.34.135","raw":"H. D. Block, B. W. Knight, Jr. and F. Rosenblatt, \u201cAnalysis of a four-layer series-coupled perceptron II,\u201d Reviews of Modern Physics, vol. 34, no. 1, pp. 135-142, 1962.","cites":null},{"id":929458,"title":"Attractor-based trust-region algorithm for efficient training of multilayer perceptron,\u201d","authors":[],"date":"2003","doi":"10.1049\/el:20030498","raw":"Jaewook Lee, \u201cAttractor-based trust-region algorithm for efficient training of multilayer perceptron,\u201d Electronics Letters, vol. 39, no. 9, pp. 727-728, 2003.","cites":null},{"id":933955,"title":"Bingo Wing-Kuen Ling, Hak-Keung Lam","authors":[],"date":"2008","doi":"10.1109\/CSNDSP.2008.4610830","raw":"Charlotte Yuk-Fan Ho, Bingo Wing-Kuen Ling, Hak-Keung Lam and Muhammad H U Nasir, \u201cGlobal convergence and limit cycle behavior of weights of perceptron,\u201d IEEE Transactions on Neural Networks, vol. 19, no. 6, pp. 938-947, 2008.","cites":null},{"id":931736,"title":"Chaoqun Ma and Lihong Huang, \u201cInvariance principle and complete stability for cellular neural networks,\u201d","authors":[],"date":"2006","doi":"10.1109\/tcsii.2005.857086","raw":"Xuemei Li, Chaoqun Ma and Lihong Huang, \u201cInvariance principle and complete stability for cellular neural networks,\u201d IEEE Transactions on Circuits and Systems\u23afII: Express Briefs, vol. 53, no. 3, pp. 202-206, 2006.","cites":null},{"id":931314,"title":"Cheng Lv and Kok Kiong Tan, \u201cConvergence analysis of a deterministic discrete time system of Oja\u2019s PCA learning algorithm,\u201d","authors":[],"date":"2005","doi":"10.1109\/tnn.2005.852236","raw":"Zhang Yi, Mao Ye, Jian Cheng Lv and Kok Kiong Tan, \u201cConvergence analysis of a deterministic discrete time system of Oja\u2019s PCA learning algorithm,\u201d IEEE Transactions on Neural Networks, vol. 16, no. 6, pp. 1318-1328, 2005.","cites":null},{"id":930145,"title":"Enhanced training algorithms, and integrated training\/architecture selection for multilayer perceptron networks,\u201d","authors":[],"date":"1992","doi":"10.1109\/72.165589","raw":"Martin G. Bello, \u201cEnhanced training algorithms, and integrated training\/architecture selection for multilayer perceptron networks,\u201d IEEE Transactions on Neural Networks, vol. 3, no. 6, pp. 864-875, 1992.","cites":null},{"id":932247,"title":"Existence of binary invariant sets in feedback neural networks with application to synthesis,\u201d","authors":[],"date":"1993","doi":"10.1109\/72.182709","raw":"Renzo Perfetti, \u201cExistence of binary invariant sets in feedback neural networks with application to synthesis,\u201d IEEE Transactions on Neural Networks, vol. 4, no. 1, pp. 153-156, 1993.","cites":null},{"id":929687,"title":"Fast parallel off-line training of multilayer perceptrons,\u201d","authors":[],"date":"1997","doi":"10.1007\/978-1-4471-3066-6_9","raw":"Se\u00e1n McLoone and George W. Irwin, \u201cFast parallel off-line training of multilayer perceptrons,\u201d IEEE Transactions on Neural Networks, vol. 8, no. 3, pp. 646-653, 1997.","cites":null},{"id":931079,"title":"Forecasting chaotic cardiovascular time series with an adaptive slope multilayer perceptron neural network,\u201d","authors":[],"date":"1999","doi":"10.1109\/10.804572","raw":"Nick Stamatis, Dimitris Parthimos and Tudor M. Griffith, \u201cForecasting chaotic cardiovascular time series with an adaptive slope multilayer perceptron neural network,\u201d IEEE Transactions on Biomedical Engineering, vol. 46, no. 12, pp. 1441-1453, 1999.","cites":null},{"id":933235,"title":"Genetic design of biologically inspired receptive fields for neural pattern recognition,\u201d","authors":[],"date":"2003","doi":"10.1109\/tsmcb.2003.810441","raw":"Claudio A. Perez, Cristian A. Salinas, Pablo A. Est\u00e9vez and Patricia M. Valenzuela, \u201cGenetic design of biologically inspired receptive fields for neural pattern recognition,\u201d IEEE Transactions on Systems, Man, Cybernetics\u23afPart B: Cybernetics, vol. 33, no. 2, pp. 258-270, 2003.","cites":null},{"id":930381,"title":"H\u00e9l\u00e8ne Tap-B\u00e9teille and Marc Lescure, \u201cAnalog neural network implementation for a real-time surface classification application,\u201d","authors":[],"date":"2008","doi":"10.1109\/jsen.2008.920713","raw":"Laurent Gatet, H\u00e9l\u00e8ne Tap-B\u00e9teille and Marc Lescure, \u201cAnalog neural network implementation for a real-time surface classification application,\u201d IEEE Sensors Journal, vol. 8, no. 8, pp. 1413-1421, 2008.","cites":null},{"id":933583,"title":"Identification of nonlinear dynamic systems using artificial neural networks,\u201d","authors":[],"date":"1999","doi":"10.1109\/3477.752797","raw":"Jagdish C. Patra, Ranendra N. Pal, B. N. Chatterji and Ganapati Panda, \u201cIdentification of nonlinear dynamic systems using artificial neural networks,\u201d IEEE Transactions on Systems, Man, Cybernetics\u23afPart B: Cybernetics, vol. 29, no. 2, pp. 254-262, 1999.","cites":null},{"id":928955,"title":"Multilayer perceptron, fuzzy sets, and classification,\u201d","authors":[],"date":"1992","doi":"10.1016\/0165-0114(95)00192-1","raw":"Sankar K. Pal and Sushmita Mitra, \u201cMultilayer perceptron, fuzzy sets, and classification,\u201d IEEE Transactions on Neural Networks, vol. 3, no. 5, pp. 683-697, 1992.","cites":null},{"id":931509,"title":"Multiperiodicity and attractivity of delayed recurrent neural networks with unsaturating piecewise linear transfer functions,\u201d","authors":[],"date":"2008","doi":"10.1109\/tnn.2007.904015","raw":"Lei Zhang, Zhang Yi and Jiali Yu, \u201cMultiperiodicity and attractivity of delayed recurrent neural networks with unsaturating piecewise linear transfer functions,\u201d IEEE Transactions on Neural IEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics Networks, vol. 19, no. 1, pp. 158-167, 2008.","cites":null},{"id":930847,"title":"Nonlinear dynamic system identification using Chebyshev functional link artificial neural networks,\u201d","authors":[],"date":"2002","doi":"10.1109\/TSMCB.2002.1018769","raw":"Jagdish C. Patra and Alex C. Kot, \u201cNonlinear dynamic system identification using Chebyshev functional link artificial neural networks,\u201d IEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics, vol. 32, no. 4, pp. 505-511, 2002.","cites":null},{"id":928720,"title":"The fractional correction rule: a new perspective,\u201d","authors":[],"date":"1998","doi":"10.1016\/S0893-6080(98)00055-0","raw":"Mitra Basu and Quan Liang, \u201cThe fractional correction rule: a new perspective,\u201d Neural Networks, vol. 11, pp. 1027-1039, 1998.","cites":null},{"id":930616,"title":"Training multiple-layer perceptrons to recognize attractors,\u201d","authors":[],"date":"1997","doi":"10.1109\/4235.687884","raw":"Garrison W. Greenwood, \u201cTraining multiple-layer perceptrons to recognize attractors,\u201d IEEE Transactions on Evolutionary Computation, vol. 1, no. 4, pp. 244-248, 1997.","cites":null},{"id":932990,"title":"Yunzin Zhao and Xinhua Zhuang, \u201cA general model for directional associative memories,\u201d","authors":[],"date":"1998","doi":"10.1109\/3477.704290","raw":"Hongchi Shi, Yunzin Zhao and Xinhua Zhuang, \u201cA general model for directional associative memories,\u201d IEEE Transactions on Systems, Man, Cybernetics\u23afPart B: Cybernetics, vol. 28, no. 4, pp. 511-519, 1998.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-03-01","abstract":"In this paper, an invariant set of the weight of the perceptron trained by the perceptron training algorithm is defined and characterized. The dynamic range of the steady state values of the weight of the perceptron can be evaluated via finding the dynamic range of the weight of the perceptron inside the largest invariant set. Also, the necessary and sufficient condition for the forward dynamics of the weight of the perceptron to be injective as well as the condition for the invariant set of the weight of the perceptron to be attractive is derived","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/54225.pdf","fullTextIdentifier":"http:\/\/eprints.lincoln.ac.uk\/2684\/1\/Invariant_Set_of_Weight_of_Perceptron.pdf","pdfHashValue":"053fe6467e6b7a10e876516684daea963bfd9266","publisher":"IEEE","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lincoln.ac.uk:2684<\/identifier><datestamp>\n      2013-06-05T15:19:54Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F48:6A6163735F48333130<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lincoln.ac.uk\/2684\/<\/dc:relation><dc:title>\n        Invariant set of weight of perceptron trained by perceptron training algorithm<\/dc:title><dc:creator>\n        Ho, Charlotte Yuk-Fan<\/dc:creator><dc:creator>\n        Ling, Bingo Wing-Kuen<\/dc:creator><dc:creator>\n        IU, Herbert Ho-Ching<\/dc:creator><dc:subject>\n        H310 Dynamics<\/dc:subject><dc:description>\n        In this paper, an invariant set of the weight of the perceptron trained by the perceptron training algorithm is defined and characterized. The dynamic range of the steady state values of the weight of the perceptron can be evaluated via finding the dynamic range of the weight of the perceptron inside the largest invariant set. Also, the necessary and sufficient condition for the forward dynamics of the weight of the perceptron to be injective as well as the condition for the invariant set of the weight of the perceptron to be attractive is derived.<\/dc:description><dc:publisher>\n        IEEE<\/dc:publisher><dc:date>\n        2010-03-01<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lincoln.ac.uk\/2684\/1\/Invariant_Set_of_Weight_of_Perceptron.pdf<\/dc:identifier><dc:identifier>\n          Ho, Charlotte Yuk-Fan and Ling, Bingo Wing-Kuen and IU, Herbert Ho-Ching  (2010) Invariant set of weight of perceptron trained by perceptron training algorithm.  IEEE Transaction on Systems, Man, and Cybernetics, 40  (6).   pp. 1521-1530.  ISSN 0018-9472  <\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/TSMCB.2010.2042444<\/dc:relation><dc:relation>\n        10.1109\/TSMCB.2010.2042444<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lincoln.ac.uk\/2684\/","http:\/\/dx.doi.org\/10.1109\/TSMCB.2010.2042444","10.1109\/TSMCB.2010.2042444"],"year":2010,"topics":["H310 Dynamics"],"subject":["Article","PeerReviewed"],"fullText":"IEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics \n 1\nInvariant Set of Weight of Perceptron Trained by \nPerceptron Training Algorithm \n \nCharlotte Yuk-Fan Ho \nTelephone: +44 (0)20 7882 5555 ext. 4333 Fax: +44 (0)20 7882 7997 Email: c.ho@qmul.ac.uk \nSchool of Mathematical Sciences, Queen Mary, University of London, Mile End Road, London, E1 4NS, United \nKingdom. \n*Bingo Wing-Kuen Ling \nTelephone: +44 (0)20 7848 2294 Fax: +44 (0)20 7848 2932 Email: HTwing-kuen.ling@kcl.ac.ukTH \nDepartment of Electronic Engineering, Division of Engineering, King\u2019s College London, Strand, London, WC2R 2LS, \nUnited Kingdom. \nHerbert H. C. Iu \nTelephone: +61 8 6488 7989 Fax: +61 8 6488 1065 Email: herbert@ee.uwa.edu.au \nSchool of Electrical, Electronic and Computer Engineering, The University of Western Australia, Crawley, Perth, \nWestern Australia, WA 6009, Australia. \n \nABSTRACT \nIn this paper, an invariant set of the weight of the perceptron trained by the perceptron \ntraining algorithm is defined and characterized. The dynamic range of the steady state values of the \nweight of the perceptron can be evaluated via finding the dynamic range of the weight of the \nperceptron inside the largest invariant set. Also, the necessary and sufficient condition for the \nforward dynamics of the weight of the perceptron to be injective as well as the condition for the \ninvariant set of the weight of the perceptron to be attractive is derived. \n \nIndex Terms\u23afPerceptron training algorithm, neurodynamics, symbolic dynamics, chaos, invariant \nset. \n \nI. INTRODUCTION \nPattern recognitions, such as speech recognitions [3], infra red imagery military vehicle \ndetections [18], English letter recognitions [19] and facial recognitions [20], play an important role \nIEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics \n 2\nin our daily life. The existing pattern recognition methods could be mainly categorized into three \ndifferent approaches, namely, the statistical approaches, the neural network approaches and the \nstructural approaches. The structural approaches are problem dependent and these approaches are \ndifficult for solving general pattern recognition problems. The statistical approaches require \ninformation on the prior probabilities of each class and the condition probabilities of the feature \nvectors in which this information is usually not available in many practical pattern recognition \nproblems. Hence, the neural network approaches are the most practical approaches for solving \ngeneral pattern recognition problems. The simplest neural network is a perceptron. A perceptron is a \nsingle neuron that applies the single bit quantization function to the inner product of its weight and \nits input [1], [2], [23]. As the output of the perceptron is either 1 or -1 [1], [2], the output of the \nperceptron is used for representing two different classes of objects of pattern recognition systems. \nHence, perceptrons are widely employed for solving general pattern recognition problems [17]. \nTo implement the perceptron, the weight of the perceptron is required to be known a prior and \nit is usually trained by perceptron training algorithms [1], [2], [4]-[7], [23]. There are many different \nperceptron training algorithms [1], [2], [4]-[7], [23], in which the one proposed in [1] and [2] is the \ncommonest perceptron training algorithm employed in industries (First, an arbitrary weight is \ninitialized. Then the new weight is obtained by adding the old weight to the product of its input and \nthe half difference between the desirable output and the perceptron output. By computing the new \nweight again and again, if the new weight converges, then the converged weight is employed as the \nweight of the perceptron [1], [2].). Many efficient hardware and software packages [8] have been \ndeveloped for the implementation of the perceptron training algorithm [1], [2]. \nIt is well known from the perceptron training algorithm [1], [2] that the weight of the \nperceptron would converge if the set of input vectors is linearly separable. When the set of input \nvectors is nonlinearly separable, the weight of the perceptron could exhibit chaotic behaviors \n(Chaotic behavior is a kind of nonlinear system behaviors in which the system is sensitive to its \ninitial condition, topological transitive and with dense periodic orbits [25]. It is worth noting that in \ngeneral non-converging behaviors may not be chaotic behaviors. For an example, an impulse \nIEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics \n 3\nresponse of an unstable linear system is diverging, but this diverging behavior is not a chaotic \nbehavior. Also, a limit cycle behavior is not a chaotic behavior too because the system response \nconsists of a finite number of periodic orbits. Hence, in this paper chaotic behaviors are not \nreferring to the non-converging behaviors.). Recent researches [9]-[11] show that the exhibition of \nchaotic behaviors of the weight of the perceptron could be applied for the recognition of chaotic \nattractors [9], nonlinear dynamical systems [10], [21], [22] and cardiovascular time series [11]. \nHowever, there are some fundamental questions remained unaddressed when the weight of \nthe perceptron exhibits chaotic behaviors. For examples, what is the dynamic range of the steady \nstate values of the weight of the perceptron when it exhibits chaotic behaviors? Are there any \nattractive regions that the weight of the perceptron will eventually move to and stay inside once the \nweight of the perceptron enters these regions? These two fundamental questions are important from \na practical point of view because the dynamic range of the steady state values of the weight of the \nperceptron has to be within a certain range for an implementation and safety reason. Also, as the \nexistence of the attractive regions implies that the weights of the perceptron will be stayed inside \nthese attractive regions if the initial weight of the perceptron is inside these attractive regions, and \nthe existence of these attractive regions implies the weights of the perceptron will move to these \nattractive regions, the existence of these attractive regions would guarantee the robust local stability \nof the perceptron. The objective of this paper is to address these two issues. \nTo investigate the dynamic range of the steady state values of the weight of the perceptron, an \ninvariant set approach [12]-[16] is proposed. The dynamic range of the steady state values of the \nweight of the perceptron could be evaluated by characterizing the largest invariant set and finding \nthe dynamic range of the weight of the perceptron inside the largest invariant set. To investigate \nwhether there exist attractive regions that the weight of the perceptron will eventually move to, it is \nequivalent to investigate whether the invariant set is attractive or not. \nHowever, it is very challenging to characterize an invariant set of the weight of the perceptron. \nThere are mainly two reasons. First, no existing result has been reported on the characterization of \nan invariant set of the weight of the perceptron. Since conventional perceptrons are usually operated \nIEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics \n 4\nwith a set of linearly separable input vectors, existing results are not applicable for the \ncharacterization of an invariant set of the weight of the perceptron when the weight of the \nperceptron exhibits chaotic behaviors. Second, as the forward dynamics of the weight of the \nperceptron depends on the output of the perceptron, in which it is obtained by applying the single \nbit quantization function on the inner product of the weight and the input of the perceptron, the \nforward dynamics of the weight of the perceptron is governed by a nonlinear map. Moreover, as the \ninput vectors keep multiplying to the weight of the perceptron, the input of the perceptron is \nperiodically time varying with the period equal to the total number of the input vectors. Hence, the \nforward dynamics of the weight of the perceptron is governed by a time varying map. Overall, the \nforward dynamics of the weight of the perceptron is governed by a nonlinear time varying map. \nThis results to a very difficult characterization of an invariant set and the corresponding invariant \nmap of the weight of the perceptron. \nTo address these difficulties, this paper proposes to downsample the weight of the perceptron \nwith the downsampling rate equal to the total number of the input vectors. Here, the set of the \ndownsampled weights of the perceptron refers to the set of the weights of the perceptron with the \ntime indices equal to an integer multiple of the total number of the input vectors. Since the next \nweight depends on the current weight, the current input vector and the current desirable output, the \nsystem map relating the current weight and the next weight is time variant. However, as all input \nvectors and desirable outputs are sum up for the calculation of the next downsampled weight, the \nnext downsampled weight only depends on the current weight. As a result, the system map relating \nthe current downsampled weight and the next downsampled weight is time invariant. Hence, the \nforward dynamics of the downsampled weight of the perceptron is now governed by a nonlinear \ntime invariant map. An invariant set of the weight of the perceptron is defined as a set of the \ndownsampled weights that maps to itself. \nBesides, it is also challenging to investigate whether an invariant set of the weight of the \nperceptron is attractive or not. Since if the invariant set is attractive, then some weights outside the \ninvariant set will map to a weight inside the invariant set. As the weight inside the invariant set will \nIEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics \n 5\nalso map to a weight inside the invariant set, there exist at least two different weights, one inside the \ninvariant set and another one outside the invariant set, that will map to the same weight inside the \ninvariant set. In other words, there exist at least two different backward dynamics of the weight of \nthe perceptron that will map the weight inside the invariant set to the weights both inside and \noutside the invariant set. As the backward dynamics of the weight of the perceptron is not uniquely \ndefined, the analysis of the attractive property of the invariant set of the weight of the perceptron is \nvery challenging. To address this difficulty, first it is required to define a backward dynamics of the \nweight of the perceptron so that the weight inside the invariant set will map (based on the defined \nbackward dynamics of the weight of the perceptron) to a weight inside the invariant set. The \nobtained result will be discussed in Lemma 1. Second, it is required to investigate the injective \nproperty of the forward dynamics of the weight of the perceptron. Here, the injective property of the \nforward dynamics of the weight of the perceptron refers to whether the forward dynamics is one to \none or many to one. This result will determine whether the backward dynamics of the weight of the \nperceptron is uniquely defined or not. The result derived in Lemma 1 will be applied for this \ninvestigation and the obtained result will be discussed in Lemma 2 and Corollary 1. Third, it is \nrequired to define an invariant set and the corresponding invariant map of the weight of the \nperceptron so that the corresponding invariant map is bijective. Hence, the weights of the \nperceptron will be stayed within the invariant set if the initial weight is inside the invariant set. The \nresult derived in Lemma 2 and Corollary 1 will be applied for this investigation and the obtained \nresult will be discussed in Theorem 1. By the way, it is worth investigating whether the invariant set \nis empty or not. Lemma 3 is addressing this issue. Now, it is ready to evaluate the dynamic range of \nthe steady state values of the weight of the perceptron by finding the dynamic range of the weight of \nthe perceptron inside the largest invariant set. The corresponding result will be discussed in \nCorollary 2. Fourth, it is required to investigate the dynamics of the weights of the perceptron \noutside the invariant set. The obtained result will be discussed in Lemma 4 and Theorem 2. Fifth, it \nis required to investigate the surjective property of the forward dynamics of the weight of the \nperceptron. The obtained result will be discussed in Theorem 3. Based on the obtained results, it can \nIEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics \n 6\nbe concluded whether there exist some weights outside the invariant set that will eventually move to \nthe invariant set or not. In other words, it can be concluded whether the invariant set is attractive or \nnot. Finally, all possible output sequences of the perceptron in which the initial weights outside the \ninvariant set will eventually move to the invariant set will be identified. The obtained result will be \ndiscussed in Lemma 5. An interesting property of the phase diagram will be discussed in Lemma 6. \nThe outline of this paper is as follows. Notations used throughout this paper are introduced in \nSection II. In Section III, an invariant set of the weight of the perceptron is defined and \ncharacterized. Some numerical computer simulation results are illustrated. Finally, a conclusion is \ndrawn in Section IV. \n \nII. NOTATIONS \nDenote N  as the total number of bounded training feature vectors and d  as the dimension \nof these training feature vectors. Denote the elements of these training feature vectors as ( )kxi  for \ndi ,,2,1 L=  and for 1,,1,0 \u2212= Nk L . Define the input vectors as ( ) ( ) ( )[ ]Td kxkxk ,,,1 1 L\u2261x  \nfor 1,,1,0 \u2212= Nk L , in which the superscript T  denotes the transposition operator. In this paper, \nwe assume that ( ) 0x \u2260k  for 1,,1,0 \u2212= Nk L . Define ( ) ( )kkNn xx \u2261+  { }0\\Zn\u2208\u2200  and for \n1,,1,0 \u2212= Nk L  so that ( )kx  is periodic with period N . Denote the weights of the perceptron as \n( )nwi  for di ,,2,1 L=  and Zn\u2208\u2200 . Denote the threshold of the perceptron as ( )nw0  Zn\u2208\u2200  \nand the activation function of the perceptron as ( ) \u23a9\u23a8\n\u23a7\n<\u2212\n\u2265\u2261\n01\n01\nz\nz\nzQ . Define \n( ) ( ) ( ) ( )[ ]Td nwnwnwn ,,, 10 L\u2261w  Zn\u2208\u2200  and denote the output of the perceptron as ( )ny  \nZn\u2208\u2200 , then ( ) ( ) ( )( )nnQny T xw=  Zn\u2208\u2200 . Denote the desirable output of the perceptron \ncorresponding to ( )nx  as ( )nt  Zn\u2208\u2200 . Assume that the perceptron training algorithm proposed in \n[1], [2] and [23] is employed for the training, then the forward dynamics of the weight of the \nperceptron is governed by the following equation: \n( ) ( ) ( ) ( ) ( )( ) ( )nnnQntnn T xxwww\n2\n1 \u2212+=+  Zn\u2208\u2200 , (1) \nIEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics \n 7\nand denoted as 11:~ ++ \u211c\u2192\u211c\u2111 ddFk , that is ( )( ) ( )1~ +\u2261\u2111 kkFk ww  Zk\u2208\u2200 . In order to investigate a \nbackward dynamics of the weight of the perceptron, the most direct approach is to characterize a \nsystem map such that the current weight ( )kw  will be moved to the previous weight ( )1\u2212kw . \nThat means, it is required to find an equation expressing ( )1\u2212kw  in terms of ( )kw . Define \n11:~ ++ \u211c\u2192\u211c\u2111 ddBk  such that \n( )( ) ( ) ( ) ( ) ( )( ) ( )1\n2\n11~ \u2212\u2212\u2212\u2212\u2212\u2261\u2111 kkkQktkk\nT\nB\nk x\nxwww  Zk\u2208\u2200 . (2) \nIt is worth noting that the time index of the weight in the activation function in 11:~ ++ \u211c\u2192\u211c\u2111 ddBk  \nZk\u2208\u2200  is not equal to 1\u2212k . It will be shown in Section III that 11:~ ++ \u211c\u2192\u211c\u2111 ddBk  is the \nbackward dynamics of the weight of the perceptron that will map the weight inside the invariant set \nto the weight inside the invariant set. \nIt will be shown in Section III that there exist at least two different initial weights (one inside \nthe invariant set and another one outside the invariant set) that will map to the same weight inside \nthe invariant set. Denote ( )0w  and ( )0w\u2032  as these two initial weights, respectively, and ( )jw\u2032  \nZj\u2208\u2200  as the weight of the perceptron at the time index j  based on the initial weight ( )0w\u2032 , that \nis ( ) ( )( )jj Fj ww \u2032\u2111\u2261+\u2032 ~1  Zj\u2208\u2200 . Denote ( )jy\u2032  as the corresponding output of the perceptron, that \nis ( ) ( ) ( )( )jjQjy T xw\u2032\u2261\u2032  Zj\u2208\u2200 . Suppose that Zk\u2208\u2203  such that these two initial weights of the \nperceptron will map to the same weight at the time index k , that is ( ) ( )kk ww =\u2032 . \nA set S  is called an invariant set under an invariant map T  if ( ) SST = . Denote the \nabsolute value of a real number as \u22c5  and the 2-norm of a vector as \u2211\n=\n\u2261\nd\ni\niv\n0\n2v , where \n[ ]Tdvv ,,0 L\u2261v . \n \nIII. DEFINITION AND CHARACTERIZATION OF AN INVARIANT SET OF THE \nWEIGHT OF THE PERCEPTRON \nIEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics \n 8\nIt has been discussed in Section II that 11:~ ++ \u211c\u2192\u211c\u2111 ddFk  Zk\u2208\u2200  is the forward dynamics \nof the weight of the perceptron. The following lemma reveals that 11:~ ++ \u211c\u2192\u211c\u2111 ddBk  Zk\u2208\u2200  is \none of the possible backward dynamics of the weight of the perceptron. \nLemma 1 \n( )( )( ) ( )kkBkFk ww =\u2111\u2111 \u2212 ~~ 1  Zk\u2208\u2200 . \nProof: \n( )( )( )\n( ) ( ) ( ) ( )( ) ( )\n( )( ) ( ) ( ) ( )( )\n( ) ( )( ) ( ) ( ) ( )( )\n( ) ( )( ) ( ) ( ) ( )( )\n( ) ( ) ( ) ( )( ) ( ) ( ) ( ) ( )( )\n( ) ( ) ( ) ( ) ( ) ( )( ) ( ) ( ) ( ) ( )( )\n( ) ( ) ( ) ( ) ( ) ( )( ) ( ) ( ) ( ) ( )( )\n( ) ( ) ( ) ( )( )\n( ) ( ) ( ) ( ) ( ) ( )( )\n( ) ( ) ( ) ( ) ( ) ( )( )\n( )k\nkkQktkkk\nkkQktkkk\nkkQktk\nkkQktk\nkkkQkt\nkk\nkkQktk\nkkkQkt\nkk\nkkQktkkkQktk\nkkQktkk\nkkQktkk\nkkQktk\nkkkQktk\nk\nT\nT\nT\nT\nT\nT\nT\nT\nT\nTF\nk\nTF\nk\nTF\nk\nT\nF\nk\nB\nk\nF\nk\nw\nxwxxw\nxwxxw\nxww\nxwx\nxxw\nxw\nxwx\nxxw\nxw\nxwxxww\nxwxw\nxwxw\nxww\nxxww\nw\n=\n\u23aa\u23a9\n\u23aa\u23a8\n\u23a7\n=\u2212\u2212=\u2212\u2212\u2212\u2212+\n\u2212=\u2212=\u2212\u2212+\u2212\u2212\n\u2212=\u2212\n=\n\u23aa\u23aa\n\u23aa\n\u23a9\n\u23aa\u23aa\n\u23aa\n\u23a8\n\u23a7\n=\u2212\u2212=\u2212\u2212\u2212+\u2212\u2212\u2212+\u2212+\n\u2212=\u2212=\u2212\u2212\u2212\u2212\u2212\u2212\u2212+\u2212\u2212\n\u2212=\u2212\u2212\u2212\u2212\u2212+\n=\n\u23aa\u23a9\n\u23aa\u23a8\n\u23a7\n=\u2212\u2212=\u2212\u2212+\u2111\n\u2212=\u2212=\u2212\u2212\u2212\u2111\n\u2212=\u2212\u2111\n=\n\u239f\u239f\u23a0\n\u239e\n\u239c\u239c\u239d\n\u239b \u2212\u2212\u2212\u2212\u2212\u2111=\n\u2111\u2111\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n11 and 1111\n11 and 1111\n11\n11 and 111\n2\n111\n1\n11 and 111\n2\n111\n1\n111\n2\n11\n11 and 111~\n11 and 111~\n11~\n1\n2\n11~\n~~\n2\n2\n1\n1\n1\n1\n1\n(3) \nZk\u2208\u2200 . This completes the proof. \u0084 \nLemma 1 states that the weight of the perceptron will map to itself if it is first mapped \naccording to 11:~ ++ \u211c\u2192\u211c\u2111 ddBk  Zk\u2208\u2200  and then mapped according to 111 :~ ++\u2212 \u211c\u2192\u211c\u2111 ddFk  \nZk\u2208\u2200 . This implies that 11:~ ++ \u211c\u2192\u211c\u2111 ddBk  Zk\u2208\u2200  is one of the possible backward dynamics of \nthe weight of the perceptron. \nIt is worth noting that although ( )( )( ) ( )kkBkFk ww =\u2111\u2111 \u2212 ~~ 1  Zk\u2208\u2200 , the inverse of \n11\n1 :\n~ ++\n\u2212 \u211c\u2192\u211c\u2111 ddFk  Zk\u2208\u2200  may not exist. In other words, the backward dynamics of the weight of \nIEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics \n 9\nthe perceptron may not be uniquely defined, and the forward dynamics of the weight of the \nperceptron may be many to one. Hence, it is required to investigate the injective property of the \nforward dynamics of the weight of the perceptron and the result is summarized below: \nLemma 2 \nAssume that ( ) ( ) ( )kkk T xwx \u22602 . Then Fk\u2111~  is not injective if and only if \n( ) ( ) ( )kkk T xwx >2 . \nProof: \nFor the necessity, ( ) ( ) ( )kkk T xwx >2  implies that ( ) ( ) ( ) ( )kkkk TT xwxx >  for \n( ) ( ) 0\u2265kkT xw  and ( ) ( ) ( ) ( )kkkk TT xwxx \u2212>  for ( ) ( ) 0<kkT xw . This implies that \n( ) ( )( ) ( ) ( ) ( ) ( )( )kkkkQkkQ TTT xxxwxw \u2212\u2212=  for ( ) ( ) 0\u2265kkT xw  (4) \nand \n( ) ( )( ) ( ) ( ) ( ) ( )( )kkkkQkkQ TTT xxxwxw +\u2212=  for ( ) ( ) 0<kkT xw . (5) \nThis further implies that \n( ) ( )( ) ( ) ( ) ( ) ( ) ( )( ) ( )( )kkkQkkkQkkQ TTTT xxwxxwxw \u2212\u2212= . (6) \nAs ( ) ( ) ( )( )kkQky T xw= , we have \n( ) ( ) ( ) ( ) ( ) ( )( ) ( ) ( ) ( )( ) ( )( )kkkykQkkykkkQky TTT xxwxxxw \u2212\u2212=\u2212\u2212= . (7) \nDefine ( ) ( ) ( ) ( )kkykk xww \u2212\u2261\u2032\u2032  and ( ) ( ) ( )( )kkQky T xw \u2032\u2032\u2261\u2032\u2032 . Then ( ) ( ) ( )( ) ( )kykkQky T \u2032\u2032\u2212=\u2032\u2032\u2212= xw  \nand \nIEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics \n 10\n( )( )\n( ) ( ) ( ) ( )( ) ( )\n( ) ( ) ( ) ( )\n( ) ( ) ( ) ( ) ( ) ( )\n( ) ( ) ( ) ( )\n( ) ( ) ( ) ( )( ) ( )\n( )( )k\nkkkQktk\nkkyktk\nkkyktkkyk\nkkyktk\nkkkQktk\nk\nF\nk\nT\nT\nF\nk\nw\nxxww\nxw\nxxw\nxw\nxxww\nw\n\u2111=\n\u2212+=\n\u2212+=\n++\u2212=\n\u2032\u2032\u2212+\u2032\u2032=\n\u2032\u2032\u2212+\u2032\u2032=\n\u2032\u2032\u2111\n~\n2\n2\n2\n2\n2\n~\n. (8) \nObviously, ( ) ( )kk ww \u2260\u2032\u2032  because ( ) 0\u2260ky  and ( ) 0x \u2260k . Hence, Fk\u2111~  is not injective. This \nproves the necessity. \nTo prove the sufficiency, if Fk\u2111~  is not injective, then there exists ( ) ( ) 1, +\u211c\u2208\u2032\u2032 dkk ww  such \nthat ( ) ( )kk ww \u2032\u2032\u2260  and ( ) ( ) ( ) ( )( ) ( ) ( ) ( ) ( ) ( )( ) ( )kkkQktkkkkQktk TT xxwwxxww\n22\n\u2032\u2032\u2212+\u2032\u2032=\u2212+ . This \nimplies that ( ) ( ) ( ) ( )( ) ( ) ( )( ) ( )kkkQkkQkk TT xxwxwww\n2\n\u2212\u2032\u2032=\u2212\u2032\u2032 . This further implies that \n( ) ( )( ) ( ) ( )( )kkQkkQ TT xwxw \u2212=\u2032\u2032  and ( ) ( ) ( ) ( )( ) ( )kkkQkk T xxwww \u2212=\u2032\u2032 . (9) \nConsequently, we have \n( ) ( )( ) ( ) ( ) ( )( ) ( )( ) ( )( ) ( ) ( ) ( ) ( ) ( )( ) ( )( )kkkQkkkQkkkkQkQkkQ TTTTTT xxwxxwxxxwwxw \u2212=\u2212=\u2032\u2032 . (10) \nAs ( ) ( )( ) ( ) ( )( )kkQkkQ TT xwxw \u2212=\u2032\u2032 , if ( ) ( ) 0>kkT xw , then ( ) ( ) ( ) ( ) 0<\u2212 kkkk TT xxxw . This \nimplies that ( ) ( ) ( )kkk T xwx >2 . This further implies that ( ) ( ) ( )kkk T xwx >2 . If ( ) ( ) 0<kkT xw , \nthen ( ) ( ) ( ) ( ) 0\u2265+ kkkk TT xxxw . This implies that ( ) ( ) ( )kkk T xwx \u2212\u22652 . This further implies that \n( ) ( ) ( )kkk T xwx \u22652 . Since ( ) ( ) ( )kkk T xwx \u22602 , we have ( ) ( ) ( )kkk T xwx >2 . If ( ) ( ) 0=kkT xw , \nsince we assume that ( ) 0x \u2260k , then we have ( ) ( ) ( )kkk T xwx >2 . Hence, this proves the \nsufficiency and it completes the proof. \u0084 \nLemma 2 states that the necessary and sufficient condition for the forward dynamics of the \nweight of the perceptron being not injective is the square of the 2-norm of the input vectors being \nIEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics \n 11\nlarger than the absolute value of the inner product of the weight and the input of the perceptron. \nWhen ( ) ( ) ( )kkk T xwx >2 , the forward dynamics of the weight of the perceptron is not injective. \nHence, Fk\u2111~  is not invertible and the backward dynamics of the weight of the perceptron is not \nuniquely defined. \nThis lemma also implies that the weight of the perceptron has to be within some \nneighborhood around the origin in order for the forward dynamics of the weight of the perceptron \nbeing not injective, and the sizes of the neighborhood depend on the magnitudes of the input vectors. \nIf an invariant set exists and is attractive, then the invariant set has to be located within some \nneighborhood around the origin. \nCorollary 1 \nAssume that ( ) ( ) ( )111 2 \u2212\u2212\u2260\u2212 kkk T xwx  and Fk\u2111~  is not injective, then \n( ) ( ) ( )111 2 \u2212\u2212>\u2212 kkk T xwx , (11) \n( ) ( )( ) ( ) ( )( )1111 \u2212\u2212\u2212=\u2212\u2212\u2032 kkQkkQ TT xwxw  (12) \nand \n( ) ( ) ( ) ( )( ) ( )11111 \u2212\u2212\u2212\u2212\u2212=\u2212\u2032 kkkQkk T xxwww . (13) \nProof: \nThe result follows directly from Lemma 2, so the proof is omitted here. \u0084 \nCorollary 1 states that if there exist two weights ( )1\u2212\u2032 kw  and ( )1\u2212kw  that will map to the \nsame weight ( ) ( )kk ww =\u2032 , then the relationship between these two weights is governed by \n( ) ( ) ( ) ( )( ) ( )11111 \u2212\u2212\u2212\u2212\u2212=\u2212\u2032 kkkQkk T xxwww . Also, as the output of the perceptron corresponding \nto these two weights ( )1\u2212\u2032 kw  and ( )1\u2212kw  are ( ) ( )( )11 \u2212\u2212\u2032 kkQ T xw  and ( ) ( )( )11 \u2212\u2212 kkQ T xw , \nrespectively, and the output of the perceptron is either \u201c1\u201d or \u201c-1\u201d, Corollary 1 implies that the \noutputs of the perceptron corresponding to these two weights ( )1\u2212\u2032 kw  and ( )1\u2212kw  are different. \nMoreover, as there exist two weights ( )1\u2212\u2032 kw  and ( )1\u2212kw  that will map to the same weight \n( ) ( )kk ww =\u2032 , this implies that the forward dynamics of the weight of the perceptron is not injective. \nIEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics \n 12\nAccording to Lemma 2 and Corollary 1, the square of the 2-norm of the input vectors is larger than \nthe absolute value of the inner product of the weight and the input of the perceptron. \nNow, it is ready to define an invariant set of the weight of the perceptron. It has been \ndiscussed in Section I that an invariant set of the weight of the perceptron is defined as the set of the \ndownsampled weights that will map to itself. Define ( ){ qNw\u2261\u2118  Zq\u2208\u2200  such that Zj\u2208\u2200  and \nZn\u2208\u2200   \n( ) ( ) ( )( ) ( )( ) ( )( ) ( )( ) ( )\n\u23ad\u23ac\n\u23ab+\u2212++\u2260 \u2211\u2212\n=\nppnNpQpjNpQnNjN\nN\np\nTT\nxxwxwww\n1\n0 2\n. (14) \nDefine \u2118\u2192\u2118\u2111 :F  such that ( )( ) ( )( )qNqN FFNF ww 01 ~~ \u2111\u2111\u2261\u2111 \u2212 oLo  ( ) \u2118\u2208\u2200 qNw . The following \ntheorem reveals that the above definitions on \u2118 and \u2118\u2192\u2118\u2111 :F  actually correspond to an \ninvariant set and an invariant map of the weight of the perceptron, respectively. \nTheorem 1 \nF\u2111  is bijective and \u2118  is an invariant set under the map F\u2111 . \nProof: \nAs ( ) \u2118\u2208\u2200 qNw , ( )( ) ( )( ) ( )( ) \u2118\u2208+=\u2111\u2111=\u2111 \u2212 NqqNqN FFNF 1~~ 01 www oLo  Zq\u2208\u2200 , we have \n( ) \u2118\u2286\u2118\u2111F . As ( ) \u2118\u2208\u2200 qNw , ( )( ) \u2118\u2208\u2212\u2203 Nq 1w  such that \n( )( )( ) ( )( )( ) ( ) \u2118\u2208=\u2212\u2111\u2111=\u2212\u2111 \u2212 qNNqNq FFNF www 1~~1 01 oLo , (15) \nwe have ( ) \u2118\u2287\u2118\u2111F  and F\u2111  is surjective. Consequently, ( ) \u2118=\u2118\u2111F  and \u2118 is an invariant set \nunder the map F\u2111 . \nAssume that ( ) ( )nNjN ww \u2260  such that ( )( ) ( )( )jNnN FF ww \u2111=\u2111 . This implies that \n( ) ( ) ( )( ) ( )( ) ( ) ( ) ( ) ( )( ) ( )( ) ( )ppnNpQptnNppjNpQptjN N\np\nTN\np\nT\nxxwwxxww \u2211\u2211 \u2212\n=\n\u2212\n=\n+\u2212+=+\u2212+\n1\n0\n1\n0 22\n. (16) \nThis further implies that \n( ) ( ) ( )( ) ( )( ) ( )( ) ( )( ) ( )ppnNpQpjNpQnNjN N\np\nTT\nxxwxwww \u2211\u2212\n=\n+\u2212++=\n1\n0 2\n. (17) \nIEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics \n 13\nHowever, there is a contradiction. Consequently, F\u2111  is injective. As a result, F\u2111  is bijective and \nthis completes the proof. \u0084 \nTheorem 1 states that the above definitions on \u2118 and \u2118\u2192\u2118\u2111 :F  actually correspond to \nan invariant set and an invariant map of the weight of the perceptron, respectively. This implies that \nthe weight of the perceptron inside the invariant set will map to a weight inside the invariant set. In \nother words, the weights of the perceptrons are stayed within the invariant set if the initial weight is \ninside the invariant set. Hence, the local stability of the perceptron is guaranteed even though the set \nof the input vectors is nonlinearly separable. Besides, any weights inside the invariant set are \nguaranteed to be mapped by some weights inside the invariant set. \nAlthough an invariant set is defined and proved in (14) and Theorem 1, respectively, it is \nworth to see if this invariant set would be empty or not. The following lemma addresses this issue. \nLemma 3 \n\u2118 is nonempty. \nProof: \n( ) 10 +\u211c\u2208\u2200 dw , there always exists a sequence of vectors ( ){ qNw  }Zq\u2208\u2200  and this \nsequence of vectors ( ){ qNw  }Zq\u2208\u2200  consists of an infinite number of vectors. As an invariant \nset is a set defined as ( ){ qNw\u2261\u2118  Zq\u2208\u2200  such that Zj\u2208\u2200  and Zn\u2208\u2200  \n( ) ( ) ( )( ) ( )( ) ( )( ) ( )( ) ( )\n\u23ad\u23ac\n\u23ab+\u2212++\u2260 \u2211\u2212\n=\nppnNpQpjNpQnNjN\nN\np\nTT\nxxwxwww\n1\n0 2\n, if \u00d8=\u2118 , then this implies \nthat there exists different two time indices j  and n  such that nj \u2260 , \n( ) ( )( ) ( )nNNjjN www \u2260+\u2260 1 , ( )( ) ( )( ) ( )( )NjnNjN FFNFFN 1~~~~ 0101 +=\u2111\u2111=\u2111\u2111 \u2212\u2212 www oLooLo , \nnj <+1  and ( )( )( ) ( )jNNjFFNFFN ww =+\u2111\u2111\u2111\u2111 \u2212\u2212 1~~~~ 0101 oLooLooLo . Otherwise, all the vectors in \nthe subsequence of vectors ( ){ qNw  }nq \u2264\u2200  or in the subsequence of vectors ( ){ qNw  }jq \u2264\u2200  \ncould be removed from the original sequence of vectors and the new sequence of vectors forms a \nnon-empty invariant set. However, as the forward dynamics of the weight of the perceptron is well \ndefined, it is impossible that there exists two different vectors ( )( )Nj 1+w  with one leads to the \nIEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics \n 14\nvector ( )jNw  and the other one leads to the vector ( )nNw . Hence, \u2118 is nonempty and this \ncompletes the proof. \u0084 \nLemma 3 clearly states that the invariant set defined by (14) is nonempty. \nNow, it is ready to evaluate the dynamic range of the steady state values of the weight of the \nperceptron. The following corollary addresses this issue. \nCorollary 2 \nThe dynamic range of the steady state values of the weight of the perceptron is bounded by \n( ) ( ) ( ) ( )nNjNjNnN wwww \u2212\u2118\u2208\u2118\u2208 minmax . \nProof: \nThis result is trivial, so the proof is omitted here. \u0084 \nCorollary 2 gives the bound on the dynamic range of the steady state values of the weight of \nthe perceptron, so it can be checked easily whether the perceptron satisfies the implementation and \nsafety constraints or not. \nThe next question is whether the weight of the perceptron outside the invariant set will \neventually move to the invariant set or not. In other words, is the invariant set attractive? The \nfollowing lemma and theorem reveal that the invariant set is actually attractive. \nLemma 4 \nkjN <\u2200  and knN <\u2200 , \n( ) ( ) ( ) ( )( ) ( ) ( ) ( ) ( )( ) ( ) \u2118\u2209+\u2032\u2212\u2212+\u2212+ \u2211\u2211 \u2212\u2212\n=\n\u2212\u2212\n=\njNk\np\nTnNk\np\nT\nppjNpQptppnNpQptnN\n1\n0\n1\n0 22\nxxwxxww . (18) \nProof: \nSince Zk\u2208\u2203  such that ( ) ( )kk ww =\u2032 , we have kjN <\u2200  and knN <\u2200 ,  \n( ) ( ) ( ) ( ) ( )( ) ( ) ( ) ( ) ( )( ) ( )\u2211\u2211 \u2212\u2212\n=\n\u2212\u2212\n=\n+\u2032\u2212\u2212+\u2212+=\u2032\njNk\np\nTnNk\np\nT\nppjNpQptppnNpQptnNjN\n1\n0\n1\n0 22\nxxwxxwww .(19) \nSuppose that ( ) \u2118\u2208\u2032 jNw , then { }0, U+\u2208\u2203 Zqp  and { }1,,1,0 \u2212\u2208\u2203 Nm L  such that \n( )( ) \u2118\u2208+\u2032 Npjw , ( )( ) \u2118\u2208+ Nqnw  and ( )( ) ( )( ) ( )kmNqnmNpj www =++=++\u2032 . This \nimplies that ( )( ) ( )( ) \u2118\u2208++=++\u2032 NqnNpj 11 ww . However, it contradicts to Theorem 1. Hence, \nIEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics \n 15\n( ) \u2118\u2209\u2032 jNw  and this completes the proof. \u0084 \nLemma 4 states that if there exist two weights ( )jNw\u2032  and ( )nNw  that will eventually \nmap to the same weight at the time index k , that is ( ) ( )kk ww =\u2032 , and if ( )nNw  is inside the \ninvariant set of the weight of the perceptron, then ( )jNw\u2032  is outside the invariant set. This lemma \nis important because it excludes some weights of the perceptron outside the invariant set so that it \nguarantees that the invariant map is bijective. The weights outside the invariant set will eventually \nmove to the invariant set. \nDefine 11:~ ++ \u211c\u2192\u211c\u2111 ddF  such that ( )( ) ( )( )qNqN FFNF ww \u2032\u2032\u2111\u2111\u2261\u2032\u2032\u2111 \u2212 01 ~~~ Lo  ( ) 1+\u211c\u2208\u2032\u2032\u2200 dqNw . \nTheorem 2 \nF\u2111~  is not injective. \nProof: \nAs ( ) ( )kk ww =\u2032 , { }1,,1,0 \u2212\u2208\u2203 Nm L , ( ) \u2118\u2209\u2032\u2203 jNw  and ( ) \u2118\u2208\u2203 nNw  such that \n( ) ( ) ( )kmnNmjN www =+=+\u2032 . Obviously, ( ) ( )nNjN ww \u2260\u2032  and \n( )( ) ( )( ) ( )( ) ( )( )nNNnNjjN FF wwww \u2111=+=+\u2032=\u2032\u2111 ~11~ . (20) \nHence, F\u2111~  is not injective and this completes the proof. \u0084 \nTheorem 2 states that F\u2111~  is not injective. This implies that some initial weights outside the \ninvariant set of the weight of the perceptron will eventually move to the invariant set. Hence, the \ninvariant set is attractive. As if the weights are inside an invariant set, then they will stay inside the \ninvariant set forever. If the weights are outside an invariant set, then these weights will move to the \ninvariant set after certain iterations. Hence, a logic diagram can be used to represent the dynamics \nof the weight of the perceptron and the logic diagram is shown in Figure 1. \n \n \n \n \n \n \nFigure 1. Logic diagram for the representation of the weight of the perceptron. The symbol \u201c0\u201d and \nw(0)\u00a0\n1\u00a0\nw(k)\u00a0\n0\u00a0\nIEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics \n 16\nthe symbol \u201c1\u201d represent whether the weights are outside and inside the invariant set, respectively. \nTheorem 3 \nF\u2111~  is surjective. \nProof: \n1+\u211c\u2208\u2200 dw , define ( )wv BNB \u2111\u2111= ~~1 oLo . Obviously, 1+\u211c\u2208 dv . By Lemma 1, we have \n( ) ( ) wwv =\u2111\u2111\u2111\u2111=\u2111 \u2212 BNBFFNF ~~~~~ 101 oLoooLo . Hence, F\u2111~  is surjective and this completes the proof. \u0084 \nTheorem 3 states that F\u2111~  is surjective. This implies that for any arbitrary weight of the \nperceptron in the 1+d  dimensional real-valued space, there always exist some weights in the same \nspace that will map to that weight. \nSince there exist initial weights outside the invariant set of the weight of the perceptron that \nwill eventually move to the invariant set, it is important to identify these initial weights. The \nfollowing lemma is to identify all possible output sequences of the perceptron that the initial weight \nis outside the invariant set but will eventually move to the invariant set. \nLemma 5 \n( ) ( ) ( )\n( ) ( )( )\n( ) ( )( )\n( ) ( )( )\n( ) ( )( )\n( ) ( )\n( ) ( ) \u239f\u239f\n\u239f\u239f\n\u23a0\n\u239e\n\u239c\u239c\n\u239c\u239c\n\u239d\n\u239b\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\u239f\u239f\n\u239f\u239f\n\u23a0\n\u239e\n\u239c\u239c\n\u239c\u239c\n\u239d\n\u239b\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\u2212\n\u2212\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\u2212\u2032\n\u2032\n+=\u2032\n01\n00\n11\n00\n11\n00\n2\n1000\nxx\nxx\nxw\nxw\nxw\nxw\nxw\nkkkQ\nQ\nkkQ\nQ\nQy\nT\nTT\nT\nT\nT\nT\nT MMM  (21) \nand \n( ) ( ) ( )\n( )\n( )\n( ) ( )( )\n( ) ( )( )\n( ) ( )( )\n( ) ( )( )\n( ) ( )\n( ) ( )\n\u239f\u239f\n\u239f\u239f\n\u239f\u239f\n\u239f\u239f\n\u239f\n\u23a0\n\u239e\n\u239c\u239c\n\u239c\u239c\n\u239c\u239c\n\u239c\u239c\n\u239c\n\u239d\n\u239b\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\n\u239f\u239f\n\u239f\u239f\n\u239f\u239f\n\u239f\u239f\n\u23a0\n\u239e\n\u239c\u239c\n\u239c\u239c\n\u239c\u239c\n\u239c\u239c\n\u239d\n\u239b\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\u2212\n\u2212\n\u23a5\u23a5\n\u23a5\u23a5\n\u23a5\u23a5\n\u23a5\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\u23a2\n\u23a2\u23a2\n\u23a2\u23a2\n\u23a3\n\u23a1\n\u2212\u2212\u2032\n\u2032\n\u2212+=\u2032\njk\nj\nkkQ\nQ\nkkQ\njjQ\njt\nt\njQjy\nT\nT\nT\nT\nT\nT\nT\nT\nxx\nxx\nxw\nxw\nxw\nxw\nxw\n1\n0\n11\n00\n11\n1\n0\n2\n10 MM\nM\nM\n for 1,,2,1 \u2212= kj L .(22) \nProof: \nSince ( ) ( ) ( )( )jjQjy T xw\u2032\u2261\u2032  and ( ) ( )( )jj Fj ww \u2032\u2111\u2261+\u2032 ~1  Zj\u2208\u2200  as well as Zk\u2208\u2203  such \nthat ( ) ( )kk ww =\u2032 , we have \nIEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics \n 17\n( ) ( ) ( )[ ]\n( ) ( ) ( )( )\n( ) ( ) ( )( )\n( ) ( ) ( )[ ]\n( ) ( ) ( )( )\n( ) ( ) ( )( )\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\u2212\u2212\u2212\n\u2212\n\u2212+=\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\u2212\u2032\u2212\u2212\n\u2032\u2212\n\u2212+\u2032\n111\n000\n1,,0\n2\n10\n111\n000\n1,,0\n2\n10\nkkQkt\nQt\nk\nkkQkt\nQt\nk\nT\nT\nT\nT\nxw\nxw\nxxw\nxw\nxw\nxxw\nML\nML\n. (23) \nThis further implies that \n( ) ( ) ( ) ( )[ ]\n( ) ( )( ) ( ) ( )( )\n( ) ( )( ) ( ) ( )( )\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\u2212\u2212\u2212\u2212\u2032\n\u2212\u2032\n\u2212+=\u2032\n1111\n0000\n1,,0\n2\n100\nkkQkkQ\nQQ\nk\nTT\nTT\nxwxw\nxwxw\nxxww ML  (24) \nand \n( ) ( ) ( ) ( )[ ]\n( )\n( )\n( ) ( )( )\n( ) ( )( )\n( ) ( )( )\n( ) ( )( )\n\u239f\u239f\n\u239f\u239f\n\u239f\u239f\n\u239f\u239f\n\u23a0\n\u239e\n\u239c\u239c\n\u239c\u239c\n\u239c\u239c\n\u239c\u239c\n\u239d\n\u239b\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\u2212\n\u2212\n\u23a5\u23a5\n\u23a5\u23a5\n\u23a5\u23a5\n\u23a5\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\u23a2\n\u23a2\u23a2\n\u23a2\u23a2\n\u23a3\n\u23a1\n\u2212\u2212\u2032\n\u2032\n\u2212\u2212+\u2261\u2032\n11\n00\n11\n1\n0\n1,,0\n2\n10\nkkQ\nQ\nkkQ\njjQ\njt\nt\nkj\nT\nT\nT\nT\nxw\nxw\nxw\nxw\nxxww M\nM\nM\nL  for 1,,2,1 \u2212= kj L .(25) \nAs ( ) ( ) ( )( )jjQjy T xw\u2032\u2261\u2032  Zj\u2208\u2200 , the result follows directly and this completes the proof. \u0084 \nTo evaluate ( )1\u2212\u2032 ky , as \n( ) ( ) ( )\n( )\n( )\n( ) ( )( )\n( ) ( )( )\n( ) ( )( )\n( ) ( )\n( ) ( ) \u239f\u239f\n\u239f\u239f\n\u239f\n\u23a0\n\u239e\n\u239c\u239c\n\u239c\u239c\n\u239c\n\u239d\n\u239b\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\u2212\n\u2212\n\u239f\u239f\n\u239f\u239f\n\u239f\n\u23a0\n\u239e\n\u239c\u239c\n\u239c\u239c\n\u239c\n\u239d\n\u239b\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\u2212\n\u2212\n\u23a5\u23a5\n\u23a5\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\u23a2\n\u23a3\n\u23a1\n\u2212\u2212\u2032\n\u2212+\u2212=\u2212\u2032\n11\n10\n11\n00\n11\n2\n0\n2\n1101\nkk\nk\nkkQ\nQ\nkkQ\nkt\nt\nkQky\nT\nT\nT\nT\nT\nT\nT\nxx\nxx\nxw\nxw\nxw\nxw MMM ,(26) \nit can be seen easily that the above equation is satisfied if ( ) ( )11 \u2212=\u2212\u2032 kyky . However, the above \nequation may also be satisfied when ( ) ( )11 \u2212\u2212=\u2212\u2032 kyky . Once all the possible values of ( )1\u2212\u2032 ky  \nare determined, then ( )2\u2212\u2032 ky  can also be determined as follows. As \n( ) ( ) ( )\n( )\n( )\n( ) ( )( )\n( ) ( )( )\n( ) ( )( )\n( ) ( )( )\n( ) ( )\n( ) ( )\n\u239f\u239f\n\u239f\u239f\n\u239f\u239f\n\u239f\n\u23a0\n\u239e\n\u239c\u239c\n\u239c\u239c\n\u239c\u239c\n\u239c\n\u239d\n\u239b\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\u2212\n\u2212\n\u239f\u239f\n\u239f\u239f\n\u239f\u239f\n\u23a0\n\u239e\n\u239c\u239c\n\u239c\u239c\n\u239c\u239c\n\u239d\n\u239b\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\u2212\n\u2212\n\u23a5\u23a5\n\u23a5\u23a5\n\u23a5\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\u23a2\n\u23a2\u23a2\n\u23a3\n\u23a1\n\u2212\u2212\u2032\n\u2212\u2212\u2032\n\u2212+\u2212=\u2212\u2032\n21\n20\n11\n00\n11\n22\n3\n0\n2\n1202\nkk\nk\nkkQ\nQ\nkkQ\nkkQ\nkt\nt\nkQky\nT\nT\nT\nT\nT\nT\nT\nT\nxx\nxx\nxw\nxw\nxw\nxw\nxw MM\nM\n,(27) \nall possible values of ( )1\u2212\u2032 ky  have already been determined and ( ) { }1,12 \u2212\u2208\u2212\u2032 ky , all possible \nIEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics \n 18\nvalues of ( )2\u2212\u2032 ky  could be determined accordingly. Similarly, all possible values of ( )jy\u2032  for \n1,,1,0 \u2212= kj L  could be determined accordingly. Hence, all possible output sequences of the \nperceptron that the initial weight is outside the invariant set but will eventually move to the \ninvariant set could be identified. \nPlotting the state trajectory on the phase diagram is a very important technique for the \nunderstanding of the dynamics of nonlinear systems. The following lemma describes an interesting \nproperty of the state trajectory of the weight of the perceptron. \nLemma 6 \n( ) ( ) Zwnw \u2208\u2212 000  Zn\u2208\u2200 . \nProof: \nSince ( ) ( ) ( ) ( ) ( )kkyktn n\nk\nxww \u2211\u2212\n=\n\u2212+=\n1\n0 2\n0  Zn\u2208\u2200 , the first element of ( )nx  is 1 Zn\u2208\u2200  and \n( ) ( ) { }1,0,1\n2\n\u2212\u2208\u2212 nynt  Zn\u2208\u2200 , the result follows directly and this completes the proof. \u0084 \nLemma 6 states that the difference of the thresholds of the weight between any time indices \nand the initial time index is always an integer. This implies that the weight occurs only at certain \nhyperplanes and no weight can be found between these hyperplanes. \nTo illustrate the developed theory, three different types of examples are shown below. The \nfirst type of examples illustrates the exhibition of the fixed point behavior, the second type of \nexamples illustrates the exhibition of the limit cycle behavior, while the last type of examples \nillustrates the exhibition of the chaotic behavior. For the first type of examples, in order for the \nweights to exhibit the fixed point behavior, the necessary and sufficient condition is that the sets of \nthe input vectors are linearly separable. Actually, this necessary and sufficient condition does not \ndirectly relate to the values of the input vectors (on the condition that the sets of the input vectors \nare linearly separable). However, in terms of the illustration purpose, simple input vectors, such as \nthe elements of the input vectors are either 1 or 1\u2212 , are employed for the illustration. Consider \nIEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics \n 19\nthe following set of the input vectors \n\u23aa\u23ad\n\u23aa\u23ac\n\u23ab\n\u23aa\u23a9\n\u23aa\u23a8\n\u23a7\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\n\u2212\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\u23a5\n\u23a5\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n1\n1\n1\n,\n1\n1\n1\n,\n1\n1\n1\n,\n1\n1\n1\n. Assume that the corresponding set of \nthe desirable outputs is { }1,1,1,1 \u2212\u2212 . Also, assume that ( ) [ ]T0,2,00 =w . It can be verified that \n( ) [ ]Tk 0,2,0=w  Zk \u2208\u2200 . Hence, the set of the weights of the perceptron only contains a single \nweight, that is [ ]{ }T0,2,0 , and the dynamics of the weight of the perceptron exhibits a fixed \npoint behavior. The invariant set of the weight of the perceptron also consists of a single weight, \nthat is [ ]{ }T0,2,0=\u2118 . It is trivial to see that the invariant map \n[ ]{ } [ ]{ }TTF 0,2,00,2,0: \u2192\u2111  is bijective because the invariant set only contains a single \nelement and the mapping is just a one to one mapping. However, the map 33:~ \u211c\u2192\u211c\u2111F  is not \ninjective because ( ) ( ) ( )kkk T xwx >2  Zk \u2208\u2200 . Hence, some weights outside the invariant set, \nsuch as [ ]T1,1,1 \u2212 , would converge to the invariant set. In other words, the invariant set is \nattractive. \nNow, consider another example that the weight of the perceptron also exhibits a fixed point \nbehavior. Suppose that the set of the input vectors is \n\u23aa\u23ad\n\u23aa\u23ac\n\u23ab\n\u23aa\u23a9\n\u23aa\u23a8\n\u23a7\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\n\u2212\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\u23a5\n\u23a5\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n1\n1\n1\n,\n1\n1\n1\n,\n1\n1\n1\n,\n1\n1\n1\n and the corresponding \nset of the desirable outputs is { }1,1,1,1 \u2212\u2212 . Also, assume that ( ) [ ]T1,1,10 \u2212=w . It can be verified \nthat ( ) ( ) ( ) [ ]T1,1,1210 \u2212=== www  and ( ) [ ]Tk 0,2,0=w  3\u2265\u2200k . Hence, the set of the \ndownsampled weights of the perceptron is [ ] [ ]{ }TT 0,2,0,1,1,1\u2212 . As both the weights \n[ ]T1,1,1\u2212  and [ ]T0,2,0  map to the same weight [ ]T0,2,0 , according to (14), the weight \n[ ]T1,1,1\u2212  is removed from the set [ ] [ ]{ }TT 0,2,0,1,1,1\u2212  and the new set [ ]{ }T0,2,0  \nforms an invariant set of the weight of the perceptron. As this invariant set is the same as that in the \nIEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics \n 20\nprevious example, it can be seen easily that this invariant set is attractive. \nFor the second type of examples, in order for the weights to exhibit the limit cycle behavior, \nthe most common well known example is the XOR example. Hence, the elements of the input \nvectors are selected as either 1 or 1\u2212  and the corresponding desirable outputs are chosen in such \na way that the input vectors and the corresponding desirable outputs correspond to the XOR truth \ntable. Consider the following example. Suppose that the set of the input vectors is \n\u23aa\u23ad\n\u23aa\u23ac\n\u23ab\n\u23aa\u23a9\n\u23aa\u23a8\n\u23a7\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\n\u2212\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\u23a5\n\u23a5\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n1\n1\n1\n,\n1\n1\n1\n,\n1\n1\n1\n,\n1\n1\n1\n. Assume that the corresponding set of the desirable outputs is { }1,1,1,1 \u2212\u2212 . \nAlso, assume that ( ) [ ]T1,1,10 \u2212\u2212\u2212=w . It can be verified that the set of the weights of the \nperceptron is \n\u23aa\u23ad\n\u23aa\u23ac\n\u23ab\n\u23aa\u23a9\n\u23aa\u23a8\n\u23a7\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\n\u2212\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\n\u2212\n\u2212\n0\n2\n0\n,\n1\n1\n1\n,\n0\n0\n0\n,\n1\n1\n1\n, that is ( ) [ ]Tk 1,1,14 \u2212\u2212\u2212=w , ( ) [ ]Tk 0,0,014 =+w , \n( ) [ ]Tk 1,1,124 \u2212\u2212=+w  and ( ) [ ]Tk 0,2,034 \u2212=+w  Zk \u2208\u2200 . The dynamics of the weight of \nthe perceptron exhibits a limit cycle behavior with period 4. The set of the downsampled weights of \nthe perceptron consists of a single weight, which is \n\u23aa\u23ad\n\u23aa\u23ac\n\u23ab\n\u23aa\u23a9\n\u23aa\u23a8\n\u23a7\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\n\u2212\n\u2212\n1\n1\n1\n. As the invariant set of the weight of \nthe perceptron is defined as the set of the downsampled weights that maps to itself, the invariant set \nof the weight of the perceptron also consists of a single weight, that is \n\u23aa\u23ad\n\u23aa\u23ac\n\u23ab\n\u23aa\u23a9\n\u23aa\u23a8\n\u23a7\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\n\u2212\n\u2212\n=\u2118\n1\n1\n1\n. It is trivial to \nsee that the invariant map \n\u23aa\u23ad\n\u23aa\u23ac\n\u23ab\n\u23aa\u23a9\n\u23aa\u23a8\n\u23a7\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\n\u2212\n\u2212\n\u2192\n\u23aa\u23ad\n\u23aa\u23ac\n\u23ab\n\u23aa\u23a9\n\u23aa\u23a8\n\u23a7\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\n\u2212\n\u2212\n\u2111\n1\n1\n1\n1\n1\n1\n:F  is bijective because the invariant set only \ncontains a single element and the map is just a one to one mapping. However, the map \n33:~ \u211c\u2192\u211c\u2111F  is not injective because ( ) ( ) ( )kkk T xwx \u22652  Zk \u2208\u2200 . Hence, some weights outside \nIEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics \n 21\nthe invariant set, such as [ ]T0,0,0 , would converge to the invariant set. In other words, the \ninvariant set is attractive. \nNow, consider another example that the weight of the perceptron also exhibits a limit cycle \nbehavior. Suppose that the set of the input vectors is \n\u23aa\u23ad\n\u23aa\u23ac\n\u23ab\n\u23aa\u23a9\n\u23aa\u23a8\n\u23a7\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\n\u2212\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\u23a5\n\u23a5\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n1\n1\n1\n,\n1\n1\n1\n,\n1\n1\n1\n,\n1\n1\n1\n and the corresponding \nset of the desirable outputs is { }1,1,1,1 \u2212\u2212 . Also, assume that ( ) [ ]T0,0,00 =w . It can be verified \nthat ( ) ( ) [ ]T0,0,010 == ww , ( ) [ ]T1,1,12 \u2212\u2212=w , ( ) [ ]T0,2,03 \u2212=w , \n( ) [ ]Tk 1,1,14 \u2212\u2212\u2212=w  1\u2265\u2200k , ( ) [ ]Tk 0,0,014 =+w  1\u2265\u2200k , ( ) [ ]Tk 1,1,124 \u2212\u2212=+w  \n1\u2265\u2200k  and ( ) [ ]Tk 0,2,034 \u2212=+w  1\u2265\u2200k . Hence, the set of the downsampled weights of the \nperceptron is [ ] [ ]{ }TT 1,1,1,0,0,0 \u2212\u2212\u2212 . As both the weights [ ]T0,0,0  and \n[ ]T1,1,1 \u2212\u2212\u2212  map to the same weight [ ]T1,1,1 \u2212\u2212\u2212 , according to (14), the weight \n[ ]T0,0,0  is removed from the set [ ] [ ]{ }TT 1,1,1,0,0,0 \u2212\u2212\u2212  and the new set \n[ ]{ }T1,1,1 \u2212\u2212\u2212  forms an invariant set of the weight of the perceptron. As this invariant set is the \nsame as that in the previous example, it can be seen easily that this invariant set is attractive. \nFinally, the last example is to illustrate the exhibition of the chaotic behavior of the weight of \nthe perceptron. As ( ) ( ) ( ) ( )( ) ( )( ) ( )nnnQntnn T xxwww\n2\n1 \u2212+=+  and the values of \n( ) ( )( ) ( )( )\n2\nnnQnt T xw\u2212  are in the set { }1,0,1\u2212 , ( )nw  is the sum of ( )0w  and the integer \ncombinations of ( )nx . In order for the weights to exhibit the chaotic behavior, the weights could \nnot exhibit a periodic behavior. One way to achieve this condition is that the elements of ( )0w  and \n( )nx  are irrational numbers and relatively prime. In this case, ( )nw  could not exhibit the limit \ncycle behavior. As it is shown in [24] that the weights of the perceptron are bounded, so in this case \nthe weights will most likely exhibit the chaotic behavior. Hence, the elements of ( )0w  and ( )nx  \nIEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics \n 22\nare chosen as irrational numbers rounded by certain numbers of significant figures. Assume that the \nset of the input vectors is \n\u23aa\u23ad\n\u23aa\u23ac\n\u23ab\n\u23aa\u23a9\n\u23aa\u23a8\n\u23a7\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\u23a5\n\u23a5\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\u23a5\n\u23a5\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212 0668.1\n1139.0\n1\n,\n1364.0\n1832.2\n1\n,\n5883.0\n7258.0\n1\n,\n1867.0\n1746.0\n1\nand the corresponding set \nof the desirable outputs is { }1,1,1,1 \u2212\u2212 . Also, assume that ( ) [ ]T2133.0,7923.0,10 \u2212\u2212=w . It can \nbe verified that the set of the weights of the perceptron consists of three hyperplanes as shown in \nFigure 2. The dynamics of the weight of the perceptron exhibits a chaotic behavior. The invariant \nset of the weight of the perceptron also consists of these three hyperplanes. It can be checked easily \nthat the map from the invariant set to itself is bijective but the map 33:~ \u211c\u2192\u211c\u2111F  is not injective \nbecause Zk \u2208\u2203  such that ( ) ( ) ( )kkk T xwx >2 . Hence, some weights outside the invariant set, \nsuch as [ ]T0,1,1 \u2212\u2212 , would converge to the invariant set. In other words, the invariant set is \nattractive. \n-2\n-1.5\n-1\n-0.5\n0\n-1\n0\n1\n2\n3\n4\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\nw0\nw1\nw\n2\n \nFigure 2. Phase diagram of the weights of the perceptron when \nIEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics \n 23\n( )\n\u23aa\u23ad\n\u23aa\u23ac\n\u23ab\n\u23aa\u23a9\n\u23aa\u23a8\n\u23a7\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u23a5\u23a5\n\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\u23a5\n\u23a5\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\u23a5\n\u23a5\u23a5\n\u23a6\n\u23a4\n\u23a2\u23a2\n\u23a2\n\u23a3\n\u23a1\n\u2212\n\u2208\n0668.1\n1139.0\n1\n,\n1364.0\n1832.2\n1\n,\n5883.0\n7258.0\n1\n,\n1867.0\n1746.0\n1\nkx , ( ) [ ]T2133.0,7923.0,10 \u2212\u2212=w  and \n( ) { }1,1,1,1 \u2212\u2212\u2208kt . \n \nIV. CONCLUSIONS \nIn this paper, an invariant set of the weight of the perceptron is defined as a set of the \ndownsampled weights that maps to itself. In order to investigate the dynamic range of the steady \nstate values of the weight of the perceptron, first a backward dynamics of the weights of the \nperceptron is defined. Based on the definition of the backward dynamics of the weights of the \nperceptron, it is shown in this paper that the forward dynamics of the weight of the perceptron is in \ngeneral not injective and the necessary and sufficient condition for the forward dynamics of the \nweight of the perceptron to be injective is characterized. As a result, the set of the weight of the \nperceptron that the forward dynamics is injective is characterized and it is shown that this set of the \nweight of the perceptron is actually a nonempty invariant set in which the map that maps this \ninvariant set to itself is a bijective map. Consequently, the dynamic range of the steady state values \nof the weight of the perceptron can be evaluated via finding the dynamic range of the weight of the \nperceptron inside the largest invariant set of the weight of the perceptron. Finally, all possible output \nsequences of the perceptron in which the initial weights outside the invariant set will eventually \nmove to the invariant set are identified. \n \nREFERENCES \n[1] Mitra Basu and Quan Liang, \u201cThe fractional correction rule: a new perspective,\u201d Neural \nNetworks, vol. 11, pp. 1027-1039, 1998. \n[2] Sankar K. Pal and Sushmita Mitra, \u201cMultilayer perceptron, fuzzy sets, and classification,\u201d IEEE \nTransactions on Neural Networks, vol. 3, no. 5, pp. 683-697, 1992. \n[3] Zezhen Huang and Anthony Kuh, \u201cA combined self-organizing feature map and multilayer \nIEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics \n 24\nperceptron for isolated word recognition,\u201d IEEE Transactions on Signal Processing, vol. 40, \nno. 11, pp. 2651-2657, 1992. \n[4] Jaewook Lee, \u201cAttractor-based trust-region algorithm for efficient training of multilayer \nperceptron,\u201d Electronics Letters, vol. 39, no. 9, pp. 727-728, 2003. \n[5] Se\u00e1n McLoone and George W. Irwin, \u201cFast parallel off-line training of multilayer perceptrons,\u201d \nIEEE Transactions on Neural Networks, vol. 8, no. 3, pp. 646-653, 1997. \n[6] Mohamad H. Hassoun and Jing Song, \u201cAdaptive Ho-Kashyap rules for perceptron training,\u201d \nIEEE Transactions on Neural Networks, vol. 3, no. 1, pp. 51-61, 1992. \n[7] Martin G. Bello, \u201cEnhanced training algorithms, and integrated training\/architecture selection \nfor multilayer perceptron networks,\u201d IEEE Transactions on Neural Networks, vol. 3, no. 6, pp. \n864-875, 1992. \n[8] Laurent Gatet, H\u00e9l\u00e8ne Tap-B\u00e9teille and Marc Lescure, \u201cAnalog neural network implementation \nfor a real-time surface classification application,\u201d IEEE Sensors Journal, vol. 8, no. 8, pp. \n1413-1421, 2008. \n[9] Garrison W. Greenwood, \u201cTraining multiple-layer perceptrons to recognize attractors,\u201d IEEE \nTransactions on Evolutionary Computation, vol. 1, no. 4, pp. 244-248, 1997. \n[10] Jagdish C. Patra and Alex C. Kot, \u201cNonlinear dynamic system identification using Chebyshev \nfunctional link artificial neural networks,\u201d IEEE Transactions on Systems, Man, and \nCybernetics\u23afPart B: Cybernetics, vol. 32, no. 4, pp. 505-511, 2002. \n[11] Nick Stamatis, Dimitris Parthimos and Tudor M. Griffith, \u201cForecasting chaotic cardiovascular \ntime series with an adaptive slope multilayer perceptron neural network,\u201d IEEE Transactions \non Biomedical Engineering, vol. 46, no. 12, pp. 1441-1453, 1999. \n[12] Zhang Yi, Mao Ye, Jian Cheng Lv and Kok Kiong Tan, \u201cConvergence analysis of a \ndeterministic discrete time system of Oja\u2019s PCA learning algorithm,\u201d IEEE Transactions on \nNeural Networks, vol. 16, no. 6, pp. 1318-1328, 2005. \n[13] Lei Zhang, Zhang Yi and Jiali Yu, \u201cMultiperiodicity and attractivity of delayed recurrent neural \nnetworks with unsaturating piecewise linear transfer functions,\u201d IEEE Transactions on Neural \nIEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics \n 25\nNetworks, vol. 19, no. 1, pp. 158-167, 2008. \n[14] Xuemei Li, Chaoqun Ma and Lihong Huang, \u201cInvariance principle and complete stability for \ncellular neural networks,\u201d IEEE Transactions on Circuits and Systems\u23afII: Express Briefs, \nvol. 53, no. 3, pp. 202-206, 2006. \n[15] Edwin K. P. Chong, Stefen Hui and Stanislaw H. \u017bak, \u201cAn analysis of a class of neural \nnetworks for solving linear programming problems,\u201d IEEE Transactions on Automatic \nControl, vol. 44, no. 11, pp. 1995-2006, 1999. \n[16] Renzo Perfetti, \u201cExistence of binary invariant sets in feedback neural networks with \napplication to synthesis,\u201d IEEE Transactions on Neural Networks, vol. 4, no. 1, pp. 153-156, \n1993. \n[17] G. L. Foresti and T. Dolso, \u201cAn adaptive high-order neural tree for pattern recognition,\u201d IEEE \nTransactions on Systems, Man, Cybernetics\u23afPart B: Cybernetics, vol. 34, no. 2, pp. 988-996, \n2004. \n[18] Alex Lipchen Chan, Sandor Z. Der and Nasser M. Nasrabadi, \u201cA joint \ncompression-discrimination neural transformation applied to target detection,\u201d IEEE \nTransactions on Systems, Man, Cybernetics\u23afPart B: Cybernetics, vol. 35, no. 4, pp. 670-681, \n2005. \n[19] Hongchi Shi, Yunzin Zhao and Xinhua Zhuang, \u201cA general model for directional associative \nmemories,\u201d IEEE Transactions on Systems, Man, Cybernetics\u23afPart B: Cybernetics, vol. 28, \nno. 4, pp. 511-519, 1998. \n[20] Claudio A. Perez, Cristian A. Salinas, Pablo A. Est\u00e9vez and Patricia M. Valenzuela, \u201cGenetic \ndesign of biologically inspired receptive fields for neural pattern recognition,\u201d IEEE \nTransactions on Systems, Man, Cybernetics\u23afPart B: Cybernetics, vol. 33, no. 2, pp. 258-270, \n2003. \n[21] Ju-Yeop Choi, Hugh F. VanLandingham and Stanoje Bingulac, \u201cA constructive approach for \nnonlinear system identification using multilayer perceptrons,\u201d IEEE Transactions on Systems, \nIEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics \n 26\nMan, Cybernetics\u23afPart B: Cybernetics, vol. 26, no. 2, pp. 307-312, 1996. \n[22] Jagdish C. Patra, Ranendra N. Pal, B. N. Chatterji and Ganapati Panda, \u201cIdentification of \nnonlinear dynamic systems using artificial neural networks,\u201d IEEE Transactions on Systems, \nMan, Cybernetics\u23afPart B: Cybernetics, vol. 29, no. 2, pp. 254-262, 1999. \n[23] H. D. Block, B. W. Knight, Jr. and F. Rosenblatt, \u201cAnalysis of a four-layer series-coupled \nperceptron II,\u201d Reviews of Modern Physics, vol. 34, no. 1, pp. 135-142, 1962. \n[24] Charlotte Yuk-Fan Ho, Bingo Wing-Kuen Ling, Hak-Keung Lam and Muhammad H U Nasir, \n\u201cGlobal convergence and limit cycle behavior of weights of perceptron,\u201d IEEE Transactions \non Neural Networks, vol. 19, no. 6, pp. 938-947, 2008. \n[25] R. Clark Robinson, An introduction to dynamical systems: continuous and discrete, Pearson \nPrentice Hall, 2004. \n \nBIOBIOGRAPHY \nYuk-Fan Ho received the B.Eng. (Hons) degree from the department of \nElectrical and Electronic Engineering, the THong Kong University of Science \nand TechnologyT in 2000, and the MPhil. Degree from the department of \nElectronic and Information Engineering, theT Hong Kong Polytechnic \nUniversity,T in 2003. She is pursuing her PhD study at the Queen Mary, \nUniversity of London. Her research interests include image processing, filter \nbanks and wavelets theory, functional inequality constrained optimization \nproblems, symbolic dynamics as well as fuzzy and impulsive control theory. \n \nBingo Wing Kuen Ling received the B.Eng. (Hons) and M.Phil. degrees \nfrom the department of Electrical and Electronic Engineering, the THong \nKong University of Science and TechnologyT, in 1997 and 2000, respectively, \nand the Ph.D. degree from the department of Electronic and Information \nEngineering from theT Hong Kong Polytechnic UniversityT in 2003. In 2004, \nhe joined the King\u2019s College London as a Lecturer. He has served as a \ntechnical committee member of several IEEE international conferences as \nwell as an organizer of a special session in the International Symposium on Communication \nSystems, Networks and Digital Signal Processing at 2008 and 2010. He is the author of the \ntextbook titled \u201cNonlinear Digital Filters: Analysis and Applications\u201d and an editor of the book \ntitled \u201cControl of Chaos in Nonlinear Circuits and Systems\u201d. He has also served as a guest editor of \nIEEE Transactions on Systems, Man, and Cybernetics\u23afPart B: Cybernetics \n 27\na special issue on nonlinear circuits and systems in Circuits, Systems and Signal Processing. His \nresearch interests include investigations of applications of continuous constrained optimizations, \nsymbolic dynamics, filter banks and wavelets as well as fuzzy and impulsive control theory. \n \nHerbert Ho-Ching Iu (S\u201998\u2013M\u201900\u2013SM\u201906) received the B.Eng.(Hons.) \ndegree in electrical and electronic engineering from the University of Hong \nKong, Hong Kong, in 1997, and the Ph.D. degree from the Hong Kong \nPolytechnic University, Hong Kong, in 2000. Since 2002, he has been with \nthe School of Electrical, Electronic and Computer Engineering, University of \nWestern Australia, Perth, Australia, where he was initially a Lecturer and is \ncurrently an Associate Professor. He was a Visiting Lecturer with the \nUniversity of Reims Champagne-Ardenne, Reims, France, in 2004, and a Visiting Assistant \nProfessor with the Hong Kong Polytechnic University in 2006. He is the author of more than 100 \npublished papers. He currently serves as an Editorial Board Member for the Australian Journal of \nElectrical and Electronics Engineering. He is also an Associate Editor for the IEEE Circuits and \nSystems Society Newsletter and a Guest Associate Editor for International Journal of Bifurcation \nand Chaos. He is a Co-editor of Control of Chaos in Nonlinear Circuits and Systems (World \nScientific, 2009). His research interests include power electronics, renewable energy, nonlinear \ndynamics, current sensing techniques, TCP dynamics, and computational intelligence. \n"}