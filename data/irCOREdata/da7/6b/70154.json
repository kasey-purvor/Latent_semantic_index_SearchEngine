{"doi":"10.1145\/1057237.1057239","coreId":"70154","oai":"oai:eprints.lancs.ac.uk:13019","identifiers":["oai:eprints.lancs.ac.uk:13019","10.1145\/1057237.1057239"],"title":"Expected, sensed, and desired: A framework for designing sensing-based interaction","authors":["Benford, Steve","Schnadelbach, Holger","Koleva, Boriana","Anastasi, Rob","Greenhalgh, Chris","Rodden, Tom","Green, Jonathan","Ghali, Ahmed","Pridmore, Tony","Gaver, Bill","Boucher, Andy","Walker, Brendan","Pennington, Sarah","Schmidt, Albrecht","Gellersen, Hans","Steed, Anthony"],"enrichments":{"references":[{"id":16309919,"title":"A morphological analysis of the design space of input devices.","authors":[],"date":"1991","doi":"10.1145\/123078.128726","raw":"CARD,S .K., MACKINLAY,J .D . ,AND ROBERTSON,G .G . 1991. A morphological analysis of the design space of input devices. ACMT rans. Inform. Sys. 9,2 ,99\u2013122.","cites":null},{"id":16309984,"title":"A semantic analysis of the design space of input devices.","authors":[],"date":"1990","doi":"10.1207\/s15327051hci0502&3_2","raw":"MACKINLAY,J .D ., CARD,S .K., AND ROBERTSON,G .G . 1990. A semantic analysis of the design space of input devices. Hum.-Comput. Interact. 5, 23, 145\u2013190.","cites":null},{"id":16309980,"title":"A two-ball mouse affords three degrees of freedom.","authors":[],"date":"1997","doi":"10.1145\/1120212.1120405","raw":"MACKENZIE,I .S ., SOUKOREFF,R .W . ,AND PAL,C . 1997. A two-ball mouse affords three degrees of freedom. In Proceedings of the Conference on Human Factors in Computing Systems Conference Companion. New York, NY. 303\u2013304.","cites":null},{"id":16309997,"title":"Affordances, conventions and design.","authors":[],"date":"1999","doi":null,"raw":"NORMAN,D .A . 1999. Affordances, conventions and design. Interact. 6,3 ,38\u201343.","cites":null},{"id":16309933,"title":"Alternatives: Exploring information appliances through conceptual design proposals.","authors":[],"date":"2000","doi":"10.1145\/332040.332433","raw":"GAVER,W .AND MARTIN,H . 2000. Alternatives: Exploring information appliances through conceptual design proposals. In Proceedings of the Conference on Human Factors in Computing Systems. The Hague, Netherlands. 209\u2013216.","cites":null},{"id":16309944,"title":"Camping in the digital wilderness: Tents and \ufb02ashlights as interfaces to virtual worlds.","authors":[],"date":"2002","doi":"10.1145\/506443.506594","raw":"GREEN,J . ,S CHN\u00a8 ADELBACH, H., KOLEVA,B . ,B ENFORD S., PRIDMORE,T . ,M EDINA, K., HARRIS, E., AND SMITH, H. 2002. Camping in the digital wilderness: Tents and \ufb02ashlights as interfaces to virtual worlds. In Proceedings of the Conference on Human Factors in Computing Systems Extended Abstracts. Minneapolis, MN. 780\u2013781.","cites":null},{"id":16310017,"title":"Context acquisition based on load sensing.","authors":[],"date":"2002","doi":"10.1007\/3-540-45809-3_26","raw":"SCHMIDT, A., STROHBACH, M., VAN LEERHOVEN, K., FRIDAY, A., AND GELLERSEN,H . 2002. Context acquisition based on load sensing. In Proceedings of the International Conference on Ubiquitous Computing. Goteborg, Sweden. 333\u2013350.","cites":null},{"id":16309921,"title":"Developing a contextaware electronic tourist guide: Some issues and experiences.","authors":[],"date":"2000","doi":"10.1145\/332040.332047","raw":"CHEVERST, K., DAVIES,N . ,M ITCHELL, K., FRIDAY, A., AND EFSTRATIOU C. 2000. Developing a contextaware electronic tourist guide: Some issues and experiences. In Proceedings of the Conference on Human Factors in Computing Systems. The Hague, Netherlands. 17\u201324.","cites":null},{"id":16309988,"title":"Exertion interfaces: Sports over a distance for social bonding and fun,","authors":[],"date":"2003","doi":"10.1145\/642611.642709","raw":"MUELLER,F . ,A GAMANOLIS,S . ,AND PICARD,R . 2003. Exertion interfaces: Sports over a distance for social bonding and fun, In Proceedings of the Conference on Human Factors in Computing Systems.F or Lauderdale. FL. 651\u2013568.","cites":null},{"id":16309949,"title":"Faltering from ethnography to design.","authors":[],"date":"1992","doi":"10.1145\/143457.143469","raw":"HUGHES,J .A., RANDALL,D . ,AND SHAPIRO,D . 1992. Faltering from ethnography to design. In Proceedingsofthe1992ACMConferenceonComputer-SupportedCooperativeWork.T oronto,Canada. 115\u2013122.","cites":null},{"id":16310011,"title":"Go go interaction technique: Non-linear mapping for direct manipulation in VR.","authors":[],"date":"1996","doi":"10.1145\/237091.237102","raw":"POUPYREV, I., BILLINGHURST, M., WEGHORST,S . ,AND ICHIKAWA,T . 1996. Go go interaction technique: Non-linear mapping for direct manipulation in VR. In Proceedings of the 10th Annual ACM Symposium on User Interface Software and Technology. Banff, Canada. 79\u201380.","cites":null},{"id":16309964,"title":"Integrality and separability of input devices.","authors":[],"date":"1994","doi":"10.1145\/174630.174631","raw":"JACOB,R .J .K., SIBERT,L .E., MCFARLANE,D .C . ,AND MULLEN,M .P . 1994. Integrality and separability of input devices. ACMT rans. Comput.-Hum. Interact. 1,1 ,3\u201326.","cites":null},{"id":16309991,"title":"Interacting at a distance: Measuring the performance of laser pointers and other devices.","authors":[],"date":"2002","doi":"10.1145\/503376.503383","raw":"MYERS,B .,B HATNAGER,R.,NICHOLS,J .,P ECK,C .,K INGD., MILLER,R., ANDLONG,C . 2002. Interacting at a distance: Measuring the performance of laser pointers and other devices. In Proceedings of the Conference on Human Factors in Computing Systems. Minneapolis, MN. 33\u201340.","cites":null},{"id":16310002,"title":"Laser pointer interaction.","authors":[],"date":"2001","doi":"10.1145\/365024.365030","raw":"OLSEN,D .AND NIELSEN,T . 2001. Laser pointer interaction. In Proceedings of the Conference on Human Factors in Computing Systems. Seattle, WA. 17\u201322.","cites":null},{"id":16309915,"title":"Lexical and pragmatic considerations of input structures.","authors":[],"date":"1983","doi":"10.1145\/988584.988586","raw":"BUXTON,W . 1983. Lexical and pragmatic considerations of input structures. Comput. Graph. 17, 1, 31\u201337.","cites":null},{"id":16309946,"title":"Litefoot\u2014a \ufb02oor space for recording dance and controlling media.","authors":[],"date":"1998","doi":null,"raw":"GRIFFITH,N .AND FERNSTROM,M . 1998. Litefoot\u2014a \ufb02oor space for recording dance and controlling media. In Proceedings of the International Computer Music Conference. Ann Arbor, MI. 475\u2013481.","cites":null},{"id":16309929,"title":"LumiPoint: Multi-user location-based interaction on large tiled displays. Elsevier Science 23,5 . EHN,P .AND KYNG,M .Cardboard computers: Mocking-it-up or hands-on the future.","authors":[],"date":"2002","doi":null,"raw":"DAVIS,J .AND CHEN,X . 2002. LumiPoint: Multi-user location-based interaction on large tiled displays. Elsevier Science 23,5 . EHN,P .AND KYNG,M .Cardboard computers: Mocking-it-up or hands-on the future. In Design at Work: Cooperative Design of Computer Systems.J .Greenbaum and M. Kyng, Eds. Lawrence Erlbaum Ass., Hillsdale, CA, 169\u2013197.","cites":null},{"id":16309898,"title":"Making sense of sensing systems: Five questions for designers and researchers.","authors":[],"date":"2002","doi":"10.1145\/503376.503450","raw":"BELLOTTI,V . ,B ACK, M., EDWARDS,W .K., GRINTER,R .E., HENDERSON, A., AND LOPES,C . 2002. Making sense of sensing systems: Five questions for designers and researchers. In Conference on Human Factors in Computing Systems. Minneapolis, MN. 415\u2013422.","cites":null},{"id":16310023,"title":"mediaBlocks: Physical containers, transports, and controls for online media.","authors":[],"date":"1998","doi":"10.1145\/280814.280940","raw":"ULLMER,B . ,I SHII, H., AND GLAS,D . 1998. mediaBlocks: Physical containers, transports, and controls for online media. In Proceedings of the 25th Annual Conference on Computer Graphics and Interactive Techniques. Orlando, FL. 379\u2013386.","cites":null},{"id":16309960,"title":"PingPongPlus: Design of an athletictangible interface for computer-supported cooperative play.","authors":[],"date":"1999","doi":null,"raw":"ISHII, H., ORBANES,J . ,C HUN,B . ,AND PARADISO,J . 1999. PingPongPlus: Design of an athletictangible interface for computer-supported cooperative play. In Proceedings of the Conference on Human Factors in Computing Systems. Pittsburgh, PA. 394\u2013401.","cites":null},{"id":16309937,"title":"Projected realities: Projected design for cultural effect.","authors":[],"date":"1999","doi":"10.1145\/302979.303168","raw":"GAVER,W .AND DUNNE,A . 1999. Projected realities: Projected design for cultural effect. In Proceedings of the Conference on Human Factors in Computing Systems. Pittsburgh, PA. 600\u2013607.","cites":null},{"id":16310005,"title":"PRop: Personal roving presence.","authors":[],"date":"1998","doi":"10.1145\/274644.274686","raw":"PAULOS,E .AND CANNY,J . 1998. PRop: Personal roving presence. In Proceedings of the Conference on Human Factors in Computing Systems. Los Angeles, CA. 296\u2013303.","cites":null},{"id":16310031,"title":"revised","authors":[],"date":"2003","doi":null,"raw":"Received February 2003; revised August 2003, April 2004; accepted February 2004 by Shumin Zhai and Victoria Bellotti ACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.","cites":null},{"id":16309935,"title":"Sensed, and Desired: Designing Sensing-Based Interaction","authors":[],"date":"2005","doi":"10.1145\/1057237.1057239","raw":"ACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.Expected, Sensed, and Desired: Designing Sensing-Based Interaction \u2022 29 GAVER,W . ,B EAVER,J . ,AND BENFORD S. 2003. Ambiguity as a resource for design. In Proceedings of the Conference on Human Factors in Computing Systems.F or Lauderdale, FL. 233\u2013240.","cites":null},{"id":16309908,"title":"Simple interfaces to complex sound in improvised electronic music.InProceedingsoftheConferenceonHumanFactorsinComputingSystems,Supplementary Proceedings. The Hague,","authors":[],"date":"2000","doi":"10.1145\/633292.633364","raw":"BOWERS,J .AND HELLSTROM,S . 2000. Simple interfaces to complex sound in improvised electronic music.InProceedingsoftheConferenceonHumanFactorsinComputingSystems,Supplementary Proceedings. The Hague, Netherlands. 125\u2013126.","cites":null},{"id":16309911,"title":"Talking through design: Requirements and resistance in cooperative prototyping.","authors":[],"date":"1994","doi":"10.1145\/259963.260388","raw":"BOWERS,J .AND PYCOCK,J . 1994. Talking through design: Requirements and resistance in cooperative prototyping. In Proceedings of the Conference on Human Factors in Computing Systems, Supplementary Proceedings. New York, NY. 299\u2013305.","cites":null},{"id":16309955,"title":"Tangible bits: Towards seamless interfaces between people, bits and atoms.","authors":[],"date":"1997","doi":"10.1145\/258549.258715","raw":"ISHII,H .AND ULLMER,B . 1997. Tangible bits: Towards seamless interfaces between people, bits and atoms. In Proceedings of the Conference on Human Factors in Computing Systems. Atlanta, GA. 234\u2013241.","cites":null},{"id":16309936,"title":"The affordances of media spaces.","authors":[],"date":"1992","doi":"10.1145\/143457.371596","raw":"GAVER,W . 1992. The affordances of media spaces. In Proceedings of the 1992 ACM Conference on Computer-Supported Cooperative Work.T oronto, Canada. 1, 7\u201324.","cites":null},{"id":16309975,"title":"The Augurscope: A mixed reality interface for outdoors.","authors":[],"date":"2002","doi":"10.1145\/503376.503379","raw":"KOLEVA,B . ,S CHN\u00a8 ADELBACH,H ,F LINTHAM,F RASER M., IZADI S., CHANDLER P., FOSTER M., BENFORD S., GREENHALGH C. AND RODDEN T. 2002. The Augurscope: A mixed reality interface for outdoors. In Proceedings of the Conference on Human Factors in Computing Systems. Minneapolis, MN. 9\u201316.","cites":null},{"id":16309924,"title":"The cave-audio visual experience virtual environment.","authors":[],"date":"1992","doi":"10.1145\/129888.129892","raw":"CRUZ-NEIRA,C . ,S ANDIN,D .J . ,D EFANT,T .A., KENYON,R .V . ,AND HART,J .C . 1992. The cave-audio visual experience virtual environment. Comm. ACM 35,6 ,65\u201372.","cites":null},{"id":16309968,"title":"The GOMS family of user interface analysis techniques: Compare and contrast.","authors":[],"date":"1996","doi":"10.1145\/235833.236054","raw":"JOHN,B .AND KIERAS,D . 1996. The GOMS family of user interface analysis techniques: Compare and contrast. ACMT rans. Comput.-Hum. Interact. 3,4 ,320\u2013351.","cites":null},{"id":16309932,"title":"The human factors of computer graphics interaction techniques.","authors":[],"date":"1984","doi":null,"raw":"FOLEY,J .D . ,W ALLACE,V .L . ,AND CHAN,P . 1984. The human factors of computer graphics interaction techniques. IEEE Comput. Graph. Appl. 4, 11, 13\u201348.","cites":null},{"id":16309995,"title":"The Psychology of Everyday Things.","authors":[],"date":"1988","doi":"10.5860\/choice.26-1204","raw":"NORMAN,D .A . 1988. The Psychology of Everyday Things. Basic Books, New York, NY.","cites":null},{"id":16309896,"title":"The Rockin\u2019Mouse: Integral 3D manipulation on a plane.","authors":[],"date":"1997","doi":"10.1145\/258549.258778","raw":"BALAKRISHNAN, R., BAUDEL,T . ,K URTENBACH,G . ,AND FITZMAURICE,G . 1997. The Rockin\u2019Mouse: Integral 3D manipulation on a plane. In Proceedings of the Conference on Human Factors in Computing Systems. Atlanta, GA. 311\u2013318.","cites":null},{"id":16309942,"title":"The Theory of Affordances. In Perceiving,","authors":[],"date":"1977","doi":null,"raw":"GIBSON,J .J . 1977. The Theory of Affordances. In Perceiving, Acting and Knowing.R .E .Sahw & J. Bransford, Eds. Lawrence Erlbaum Ass. Hillsdale, CA. 67\u201382.","cites":null},{"id":16310015,"title":"ToolStone:Effectiveuseofthephysicalmanipulationvocabularies of input devices.","authors":[],"date":"2000","doi":"10.1145\/354401.354421","raw":"REKIMOTO,J .ANDSCIAMMARELLA,E. 2000. ToolStone:Effectiveuseofthephysicalmanipulationvocabularies of input devices. In Proceedings of the 13th Annual ACM Symposium on User Interface Software and Technology. San Diego, CA. 109\u2013117.","cites":null},{"id":16309901,"title":"Toto: A tool for selecting interaction techniques.","authors":[],"date":"1990","doi":"10.1145\/97924.97940","raw":"BLESER,T .W .AND SIBERT,J .L . 1990. Toto: A tool for selecting interaction techniques. In Proceedings of the 3rd annual ACM SIGGRAPH Symposium on User interface Software and Technology. Snowbird, UT. 135\u2013142.","cites":null},{"id":16310027,"title":"Urp:Aluminous-tangibleworkbenchforurbanplanningand design.","authors":[],"date":"1999","doi":null,"raw":"UNDERKOFFLER,J .ANDISHII,H. 1999. Urp:Aluminous-tangibleworkbenchforurbanplanningand design. In Proceedings of the Conference on Human Factors in Computing Systems. Pittsburgh, PA. 386\u2013393.","cites":null},{"id":16309939,"title":"Visually tracked \ufb02ashlights as interaction devices.","authors":[],"date":"2003","doi":null,"raw":"GHALI, A., BOUMI,S . ,B ENFORD,S . ,G REEN,J . ,AND PRIDMORE,T . 2003. Visually tracked \ufb02ashlights as interaction devices. In Proceedings of Interact. Zurich, Switzerland. 487\u2013494.","cites":null},{"id":16309927,"title":"Water lamp and pinwheels: Ambient projection of digital information into architectural space.","authors":[],"date":"1998","doi":"10.1145\/286498.286750","raw":"DAHLEY, A., WISNESKI,C . ,AND ISHII,H . 1998. Water lamp and pinwheels: Ambient projection of digital information into architectural space. InProceedings of the Conference on Human Factors in Computing Systems. Los Angeles, CA. 269\u2013270.","cites":null},{"id":16310020,"title":"Ways of the Hand: The Organization of Improvised Conduct.","authors":[],"date":"1978","doi":"10.2307\/2065076","raw":"SUDNOW,D . 1978. Ways of the Hand: The Organization of Improvised Conduct. Harvard University Press, Cambridge, MA.","cites":null},{"id":16309972,"title":"Where do web sites come from?: Capturing and interacting with design theory.","authors":[],"date":"2002","doi":"10.1145\/503376.503378","raw":"KLEMMER,S .R., THOMSEN, M., PHELPS-GOODMAN, E., LEE, R., AND LANDAY,J .A . 2002. Where do web sites come from?: Capturing and interacting with design theory. In Proceedings of the Conference on Human Factors in Computing Systems. Minneapolis, MN. 1\u20138.","cites":null},{"id":16309904,"title":"Workbook one: Ideas, scenarios and proposals for the home, available at www.interaction.rca.ac.uk\/equator\/papers\/workbook1.pdf.","authors":[],"date":"2003","doi":null,"raw":"BOUCHER, A., GAVER,W . ,P ENNINGTON,S . ,AND WALKER,B . 2003. Workbook one: Ideas, scenarios and proposals for the home, available at www.interaction.rca.ac.uk\/equator\/papers\/workbook1.pdf.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2005","abstract":"Movements of interfaces can be analyzed in terms of whether they are expected, sensed, and desired. Expected movements are those that users naturally perform; sensed are those that can be measured by a computer; and desired movements are those that are required by a given application. We show how a systematic comparison of expected, sensed, and desired movements, especially with regard to how they do not precisely overlap, can reveal potential problems with an interface and also inspire new features. We describe how this approach has been applied to the design of three interfaces: pointing flashlights at walls and posters in order to play sounds; the Augurscope II, a mobile augmented reality interface for outdoors; and the Drift Table, an item of furniture that uses load sensing to control the display of aerial photographs. We propose that this approach can help to build a bridge between the analytic and inspirational approaches to design and can help designers meet the challenges raised by a diversification of sensing technologies and interface forms, increased mobility, and an emerging focus on technologies for everyday life","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/70154.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/13019\/1\/2005%2DExpected.pdf","pdfHashValue":"749546a24a65e23f4f2748ab3b878e6935778261","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:13019<\/identifier><datestamp>\n      2018-01-24T02:23:40Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Expected, sensed, and desired: A framework for designing sensing-based interaction<\/dc:title><dc:creator>\n        Benford, Steve<\/dc:creator><dc:creator>\n        Schnadelbach, Holger<\/dc:creator><dc:creator>\n        Koleva, Boriana<\/dc:creator><dc:creator>\n        Anastasi, Rob<\/dc:creator><dc:creator>\n        Greenhalgh, Chris<\/dc:creator><dc:creator>\n        Rodden, Tom<\/dc:creator><dc:creator>\n        Green, Jonathan<\/dc:creator><dc:creator>\n        Ghali, Ahmed<\/dc:creator><dc:creator>\n        Pridmore, Tony<\/dc:creator><dc:creator>\n        Gaver, Bill<\/dc:creator><dc:creator>\n        Boucher, Andy<\/dc:creator><dc:creator>\n        Walker, Brendan<\/dc:creator><dc:creator>\n        Pennington, Sarah<\/dc:creator><dc:creator>\n        Schmidt, Albrecht<\/dc:creator><dc:creator>\n        Gellersen, Hans<\/dc:creator><dc:creator>\n        Steed, Anthony<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        Movements of interfaces can be analyzed in terms of whether they are expected, sensed, and desired. Expected movements are those that users naturally perform; sensed are those that can be measured by a computer; and desired movements are those that are required by a given application. We show how a systematic comparison of expected, sensed, and desired movements, especially with regard to how they do not precisely overlap, can reveal potential problems with an interface and also inspire new features. We describe how this approach has been applied to the design of three interfaces: pointing flashlights at walls and posters in order to play sounds; the Augurscope II, a mobile augmented reality interface for outdoors; and the Drift Table, an item of furniture that uses load sensing to control the display of aerial photographs. We propose that this approach can help to build a bridge between the analytic and inspirational approaches to design and can help designers meet the challenges raised by a diversification of sensing technologies and interface forms, increased mobility, and an emerging focus on technologies for everyday life.<\/dc:description><dc:date>\n        2005<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:relation>\n        http:\/\/dx.doi.org\/10.1145\/1057237.1057239<\/dc:relation><dc:identifier>\n        Benford, Steve and Schnadelbach, Holger and Koleva, Boriana and Anastasi, Rob and Greenhalgh, Chris and Rodden, Tom and Green, Jonathan and Ghali, Ahmed and Pridmore, Tony and Gaver, Bill and Boucher, Andy and Walker, Brendan and Pennington, Sarah and Schmidt, Albrecht and Gellersen, Hans and Steed, Anthony (2005) Expected, sensed, and desired: A framework for designing sensing-based interaction. ACM Transactions on Computer-Human Interaction, 12 (1). pp. 3-30. ISSN 1073-0516<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/13019\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1145\/1057237.1057239","http:\/\/eprints.lancs.ac.uk\/13019\/"],"year":2005,"topics":["QA75 Electronic computers. Computer science"],"subject":["Journal Article","PeerReviewed"],"fullText":"Expected, Sensed, and Desired:\nA Framework for Designing\nSensing-Based Interaction\nSTEVE BENFORD, HOLGER SCHN \u00a8ADELBACH, BORIANA KOLEVA,\nROB ANASTASI, CHRIS GREENHALGH, TOM RODDEN,\nJONATHAN GREEN, AHMED GHALI, and TONY PRIDMORE\nThe University of Nottingham\nBILL GAVER, ANDY BOUCHER, BRENDAN WALKER,\nand SARAH PENNINGTON\nThe Royal College of Art\nALBRECHT SCHMIDT, and HANS GELLERSEN\nLancaster University\nand\nANTHONY STEED\nUniversity College London\nMovements of interfaces can be analyzed in terms of whether they are expected, sensed, and desired.\nExpected movements are those that users naturally perform; sensed are those that can be measured\nby a computer; and desired movements are those that are required by a given application. We show\nhow a systematic comparison of expected, sensed, and desired movements, especially with regard to\nhow they do not precisely overlap, can reveal potential problems with an interface and also inspire\nnew features. We describe how this approach has been applied to the design of three interfaces:\npointing flashlights at walls and posters in order to play sounds; the Augurscope II, a mobile\naugmented reality interface for outdoors; and the Drift Table, an item of furniture that uses load\nsensing to control the display of aerial photographs. We propose that this approach can help to build\na bridge between the analytic and inspirational approaches to design and can help designers meet\nThis work has been supported by the EPSRC funded Equator Interdisciplinary Research Collabora-\ntion [Gr-N-15986]. We also gratefully acknowledge the support of Getmapping.com and Nottingham\nCastle Museum.\nAuthors\u2019 addresses: S. Benford, H. Schna\u00a8delbach, B. Koleva, R. Anastasi, C. Greenhalgh, T. Rodden,\nJ. Green, A. Ghali and T. Pridmore, School of Computer Science, Jubilee Campus, The University of\nNottingham, Nottingham, NG8 1BB, UK; email: sdb@cs.nott.ac.uk; B. Gaver, A. Boucher, B. Walker\nand S. Pennington Department of Interaction Design, The Roya College of Art, Kensington Gore,\nLondon, SW7 2EU, UK; A. Schmidt, and H. Gellersen, Computing Department, Lancaster Uni-\nversity, Lancaster, LA1 4WA, UK; A. Steed, Department of Computer Science, University College,\nLondon WC1E 6BT, UK.\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is\ngranted without fee provided that copies are not made or distributed for profit or direct commercial\nadvantage and that copies show this notice on the first page or initial screen of a display along\nwith the full citation. Copyrights for components of this work owned by others than ACM must be\nhonored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers,\nto redistribute to lists, or to use any component of this work in other works requires prior specific\npermission and\/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 1515\nBroadway, New York, NY 10036 USA, fax: +1 (212) 869-0481, or permissions@acm.org.\nC\u00a9 2005 ACM 1073-0616\/05\/0300-0003 $5.00\nACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005, Pages 3\u201330.\n4 \u2022 S. Benford et al.\nthe challenges raised by a diversification of sensing technologies and interface forms, increased\nmobility, and an emerging focus on technologies for everyday life.\nCategories and Subject Descriptors: H.5.2 [Information Interfaces and Presentation]: User\nInterfaces\u2014User-centered design; interaction styles; theory and methods\nGeneral Terms: Human Factors, Design, Theory\nAdditional Key Words and Phrases: Sensing, mixed reality, augmented reality, mobile and wireless\napplications, interactive furniture\n1. INTRODUCTION\nThis article introduces a design framework for sensing-based interfaces in\nwhich designers are encouraged to compare expected physical movements with\nthose that can be sensed by a computer system and those that are desired by a\nparticular application. They are asked to treat the boundaries between these as\ninteresting areas of the design space, both in terms of problems to be solved and\nalso opportunities to be exploited. Our framework is motivated by four recent\ntrends in human computer interaction (HCI).\nFirst is the growth of interest in sensing technologies that enable interfaces\nto actively respond to a wide variety of user behaviors. Video and audio track-\ning, electronic tagging, load sensing, light sensing, physiological sensing, and\nother kinds of sensing underpin proposals for new styles of interface such as\nlocation-based and context-aware interfaces, smart environments, tangible in-\nterfaces, ambient interfaces, and affective interfaces. Instead of users directly\nmanipulating the interface with their hands on the controls, these interfaces\noften autonomously react to users who are neither directly tethered to them\nor necessarily always in control or even actively engaged. However, such inter-\nfaces still need to be interpretable and to some extent predictable, raising new\nchallenges for interface designers [Bellotti 2002].\nSecond, the physical forms of interfaces are diversifying. They are simulta-\nneously getting smaller (e.g., wearable, portable, and embedded displays) and\nalso larger (e.g., immersive displays such as CAVE-like systems [Cruz-Neira\n1992]). There is also a trend towards more purpose-designed and specialized\none-off appliances in contrast to the general purpose \u201cone size fits all\u201d PC. Con-\nsequently, designers increasingly mix and match technologies to create new\ninterfaces, requiring them to be aware of the boundary conditions that result\nfrom attaching different sensors to different physical forms.\nThird is an increase in mobility. The increasing power of handheld comput-\ners and mobile phones, coupled with the spread of wide-area positioning tech-\nnologies such as GPS and cellular positioning, means that the nature of our\nphysical interaction with computers is changing. We now see interfaces that\nrequire users to walk over large areas [Cheverst 2000], carry objects [Rekimoto\n2000; Ullmer 1998], run [Flintham 2003], kick footballs [Mueller 2003], play\ntable tennis [Ishii 1999], dance [Griffith 1998], and otherwise engage in phys-\nical movements that could be considered extreme when compared to using a\nkeyboard and mouse. This requires designers to take a broader view of how an\ninterface might potentially be used, more carefully considering the possibilities\nof surprising or even bizarre physical interactions.\nACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.\nExpected, Sensed, and Desired: Designing Sensing-Based Interaction \u2022 5\nFourth, the nature of applications is changing. Looking beyond traditional\nproductivity oriented workplace technologies where performance is a key ob-\njective, HCI is increasingly considering applications for everyday life. Inter-\nface design now encompasses leisure, play, culture, and art, and in some cases\nthe design of computer interfaces is merging with the design of everyday ap-\npliances. Consequently, there is a shift in emphasis towards interfaces that\nare pleasurable, aesthetic, expressive, creative, culturally relevant, and even\nprovocative. This trend requires designers to take a fresh perspective on ap-\nplication \u201crequirements\u201d, adopting new approaches to design, including those\nthat stimulate imaginative thinking and even exploit ambiguity.\nThese four trends raise significant new challenges for interface designers.\nSome of these are concerned with how users interact with sensing systems.\nOthers are more concerned with envisioning new kinds of interaction, opening\nup new design possibilities and considering how interfaces might potentially be\n(mis)treated and (ab)used in future situations. Together, they suggest designing\nsystems in which physical input-output (I\/O) devices are no longer treated as\nspecialized and separate components, but rather are seen as an integral part of\nwhat the thing \u201cis\u201d. In turn, this requires a holistic approach to design in which\nthe mechanics of interaction and new design possibilities are combined.\n2. BUILDING ON PREVIOUS TAXONOMIES AND METHODS\nThere is already a wide variety of taxonomies, methods, and guidelines avail-\nable to interface designers to support different aspects of the design process.\n2.1 Taxonomies for Input Devices\nVarious taxonomies have been proposed to help designers reason about the de-\ntailed mechanics of how users interact with different input devices, several of\nwhich have considered how physical movements map onto the sensing capabil-\nities of the interface.\nBuxton [1983] reviews some early user interface taxonomies and concludes\nthat there is not a sufficient focus on the pragmatic aspects of interaction, for\nexample, on how the choice of particular input devices affects interaction. He\nintroduces a taxonomy that classifies input devices (mostly for desktop direct-\nmanipulation interfaces) according to the input property sensed (position, mo-\ntion, or pressure) and the number of dimensions sensed (one, two, or three),\nenabling designers to reason about pragmatic choices with regard to input de-\nvices. Foley et al. [1984] focus on the range of tasks that are undertaken in\nan application\u2014selection, positioning, orienting, path specification, quantifi-\ncation, and text input\u2014and how these can be supported with different input\ndevices. Bleser and Sibert [1990] introduce a tool that uses heuristics to sug-\ngest interaction methods given a task description. Card et al. [1991] produce a\nvery wide-ranging review of input devices used in desktop interfaces, charac-\nterizing individual one-dimensional sensors in terms of force verses position,\nlinear verses rotary, and absolute verses relative. Finally, Jacob et al. [1994] ar-\ngue that such taxonomies should also consider which actions can be expressed\nsimultaneously with a given device.\nACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.\n6 \u2022 S. Benford et al.\nTaxonomies such as these offer designers detailed insights into the relation-\nship between users\u2019 physical manipulations of input devices and the capabil-\nities of sensors. However, they are limited with regard to addressing the four\ntrends noted in our introduction. They assume as a starting point that the\nuser wants to interact, knows how to interact, and has a goal in mind, and\nthen helps the designer make this interaction more efficient. This may not be\na good assumption for smart environments, location-based services, and other\ninterfaces that actively engage passing users and push information at them,\nand where users\u2019 intentions may be less clear. Furthermore, although such tax-\nonomies can inspire new classes of device as both Buxton [1983] and Card et al.\n[1991] demonstrate, they are not primarily focused on generating new design\nideas. Rather, they are analytic tools for refining an interface once its function-\nality has been nailed down, typically by matching the right input device to each\ninteraction task.\nThese taxonomies also tend to focus on relatively direct and precise sensors.\nIn this article, we are interested in extending their analysis to less precise\nsensors such as video tracking and GPS which involve a much higher level of\nuncertainty. With less precise sensors, the areas of potential mismatch between\nactions and sensing become broader and, we argue, more interesting as design\nspaces.\nFinally, previous taxonomies tend to assume that the user\u2019s focus is on the\ncomputer interface, and that physical I\/O devices are peripherals, that is, they\nare tools to get at what you\u2019re interested in, and not the focus of interest in\nand of themselves. As such, their form can justifiably be determined almost\nentirely by their function as I\/O devices. In contrast, a focus on individually\ndesigned appliances and augmented physical artifacts brings the design of the\nartifact itself more into focus. The forms of \u201cdesigner\u201d devices will be strongly\ninfluenced by preexisting functionality and cultural connotations and the fact\nthat they are I\/O devices is only a part of their meaning to users. This naturally\nleads us to our second thread of related research, inspirational design methods.\n2.2 Inspirational Design Methods\nThere is a long and extensive history of user-centered design methods in HCI,\nincluding task-analysis techniques that draw on cognitive psychology in order\nto understand how individuals plan and carry out detailed interactions with\nparticular interfaces, for example, GOMs [John 1996], the use of ethnogra-\nphy to inform system design with an understanding of the social and situated\nuse of technologies in particular environments [Hughes 1992] and participa-\ntory design methods that directly involve users as partners in the design pro-\ncess, sometimes through working with low-tech physical prototypes (e.g., Ehn\n[1991]).\nOf particular relevance to this article are insprational methods such as cul-\ntural probes [Gaver 1999] whose primary aim is to inspire new design ideas\nand that are targeted at designing products for everyday life rather than the\nwork place, focusing on creative, engaging, and playful applications of com-\nputer technologies. In one example, a community of seniors in Amsterdam was\nACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.\nExpected, Sensed, and Desired: Designing Sensing-Based Interaction \u2022 7\nmailed activity packs that included digital cameras, maps, and postcards which\nthey could use to record snapshots of their lives and feelings about their envi-\nronment. The completed cultural probes were then used by the design team to\ncreate fictional scenarios as the basis for new inspirations. An underlying idea\nhere is that working with physical artifacts allows people to tap their embodied\nunderstanding of things, from their affordances [Gaver 1992; Norman 1999] to\ntheir cultural connotations, in reasoning about how new designs might work.\nA related approach that also draws upon the disciplines of art and design is to\nrecognize the potentially positive role of ambiguity in creating interfaces that\nstimulate engagement and provoke reflection [Gaver 2003].\nThese inspirational design methods support our framework in two ways.\nFirst is the importance of deliberately undermining assumptions. In our case,\nwe wish designers to explicitly consider the unexpected\u2014unlikely patterns of\nuse that might lead to extreme movements or might result in unlikely sensor\ndata. Second, is the idea of looking at boundary conditions, the ambiguous area\nwhere physical movement may not precisely match the capabilities of sensors,\nas a new source of design opportunities.\nHowever, these methods suffer from their own limitations. In particular,\nthey do not support the kinds of detailed analysis of design trade-offs that were\nthe focus of the interface taxonomies that we reviewed previously. In response\nto these observations and the four trends noted in our introduction, we now\nintroduce a design framework that aims to build a bridge between the idea-\ngeneration phase of design (supported by ethnography, participatory design,\nand inspirational design methods) and the refinement phase where detailed\ntrade-offs are explored (supported by analytic frameworks and taxonomies),\nand that encourages designers to focus on extraordinary or quirky behaviors\nand boundary conditions for sensor technologies. We now introduce our frame-\nwork, beginning with definitions of expected, sensed, and desired movements.\n3. EXPECTED MOVEMENTS\nThe physical form of an interface fundamentally shapes the kinds of interac-\ntions that users can and will perform. We define expected movements as being\nthose movements that users might be expected to carry out; they are natural\nmovements for a given combination of user, interface, and environment. Besides\nexpected movements, there are less expected movements. These are unusual,\nalthough certainly possible movements, and when they occur, they indicate that\nthe interface is being used in an atypical manner or context. Outside the realm\nof these movements are nonsensical movements that are impossible without\nbreaking the interface or the laws of physics. We are interested in identifying\nunexpected and nonsensical movements as well as expected ones. We briefly\nillustrate this idea in relation to existing interfaces.\nHandheld Computer (PDA). Expected movements include holding the PDA\nin one or two hands while standing still and looking at the screen. Movements\nof the interface can then be expected to follow the principal axes of rotation\nof the human body (about wrists, elbows, shoulders, spine, etc.). Examples of\nless expected (but possible) movements might be carrying the device above\nACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.\n8 \u2022 S. Benford et al.\nyour head, interacting while running, throwing the device from one person to\nanother, or attaching it to a balloon, although note that Paulos [1999] presents\na system in which a small computer, camera, and microphone are attached to\na remote controlled blimp in order to create a Personal ROving Proxy (PROP).\nAn example of a nonsensical movement is moving through a solid wall.\nTangible Interface Object Moved Across a Surface. (e.g., a block on a table\n[Underkoffler 1999] or a post-it note on a drawing board [Klemmer 2002]. It\nis expected movement to place the device on the surface in an orientation sug-\ngested by its shape. It is also expected that users will carry objects between\ndifferent surfaces, a possibility exploited in mediaBlocks [Ullmer 1998] and\nthe work of Rekimoto et al. [2000] that treat physical objects as containers for\ndigital information. It may be less expected to stack objects, turn them upside\ndown, raise them into the air, or change their shapes and colors.\nLaser Pointer Used to Interact With a Screen [Olsen 2001; Myers 2002]. Here\nit is expected to hold the laser pointer in one hand and point it at targets for\nshort periods. Again, movements will typically follow natural arm movements.\nIt is less expected to wave it about wildly, or to hold the beam perfectly still\nand point at an object for many minutes (this could be achieved by resting the\npointer on a surface with the switch taped down). It is impossible to move the\nbeam instantly from one surface to another.\nA Virtual Reality Head-Mounted Display (HMD). Normal head movements\nwill be slow and will not feature extreme pitch and roll rotations and the hands\nwill stay within arm-extension distance of the head\/body. The user will not move\nfar in physical space due to connecting cables and the possibility of colliding with\nobjects that they cannot see. Less expected movements are rapid and extreme\nhead movements (but perhaps the head-mount is being held in the hands) or\na large separation between head and hands (but perhaps several people are\nholding the equipment).\nThe Common Mouse. It is expected to move a mouse horizontally on its mouse\nmat. It is common to lift it off the surface, move it through the air, and then\nplace it down on the surface again, and also to rotate it. It may be less expected\nto move the mouse entirely away from its mouse mat or surface, turn it over,\nand use it as a trackball, or carry it away altogether (though rollerball and\nwireless mice offer different possibilities here).\nWe offer some general observations on these examples. First, they concern\ndifferent properties of movement:\n\u2014degrees of freedom: which combinations of translations and rotations are\nexpected?\n\u2014range: how far is the device expected to move in each degree of freedom?\n\u2014speed: how quickly is it expected to move in each degree of freedom?\n\u2014accuracy: how precisely is it expected to move in each degree of freedom?\n\u2014stability and maintainability: how stable will expected movement be over\ntime?\nSecond, distinctions between expected and less expected movements emerge\nfrom a combination of other factors. The physical form of the interface (its\nACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.\nExpected, Sensed, and Desired: Designing Sensing-Based Interaction \u2022 9\nsize, shape, texture, weight, joints, supports, handles, etc.) suggests particular\nmovements. Drawing on the ideas of Norman [1999] and also Gaver [1992], both\nbased upon Gibson [1977], the relationship between the user and the interface\naffords certain movements. The form of the interface also constrains possible\nmovements [Norman 1988], for example, through size, weight, shape, tethering\nby physical cables, and physical joints that constrain rotation and extension.\nFurthermore, the human body imposes constraints on movement in terms of\nreach, natural postures, and rotations.\nThird, the surrounding environment implies and constrains expected move-\nments through its size and shape and through the presence and absence of\nobstacles, including boundaries. This relationship between the environment\nand the movement of an interface has not featured strongly in previous tax-\nonomies and frameworks but is one that takes on an increased significance as\ninterfaces become mobile.\nFinally, the designer will hold assumptions about how the interface will be\nmoved based upon their own experience and vision of how the interface is in-\ntended to be used. A key feature of our approach is encouraging designers to\ndeliberately question these assumptions by imagining extreme and bizarre sce-\nnarios in which unexpected movements could occur.\n4. SENSED MOVEMENTS\nNext, we turn our attention to an interface\u2019s sensed movements, defined as\nthose that can actually be measured by a computer. These are determined by the\nparticular combination of sensing technologies that are used with the interface.\nThere is an increasingly wide range of such technologies to chose from, each\nwith its own capabilities and limitations. The following list considers a few\nrepresentative examples.\nGlobal Positioning System (GPS). This is a versatile technology for sensing\nposition on and above the Earth\u2019s surface that can be integrated into PDAs and\nwearables. However, a GPS does not generally work indoors or underground,\nat extreme northerly or southerly latitudes, and can be problematic in built-\nup urban environments or in poor weather. Compared to the transducers used\nin devices for traditional direct manipulation interfaces, GPSs can suffer from\nconsiderable inaccuracy which varies over space and time.\nVideo Tracking. Video tracking can be readily combined with interfaces such\nas laser pointers, flashlights, and tangible objects. This technology can track\nthe presence, identities, number, position, orientation, and movement of known\nobjects, including people. However, the number of cameras and their fields of\nview limit the extent of the surfaces that can be tracked. Stereo or mono deploy-\nment determines the ability to track depth. Camera resolution and the rate at\nwhich frames can be processed limit accuracy. Systems are also usually tailored\nto follow specific objects in particular environments and may be unable to cope\nwith different objects, multiple objects, occlusion, and changes in lighting.\nElectro-Magnetic Tracking. This technology is widely used with immersive\nvirtual reality (VR) and usually trades off tracking range for accuracy, with\ntypical examples providing roughly a centimeter of accuracy over a couple of\nACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.\n10 \u2022 S. Benford et al.\nFig. 1. Expected versus sensed movements.\nmeters range, or only several centimeters accuracy over several meters. Sys-\ntems also suffer from interference and accuracy and stability decays towards\nthe edge of the tracking volume.\nRadio Frequency Identification (RFID). Widely used to recognize the identi-\nties of objects that are placed on surfaces RFID is characterized by different\nreading ranges, responsiveness (limiting how quickly an object can pass by),\nand by the number of tags that may be read simultaneously.\nMechanical Tracking. Mechanical tracking involves instrumenting the mov-\ning parts of an interface such as the joints of a moving arm or the rollerball\nof a mouse. This typically provides accurate and stable readings, but for lim-\nited degrees of freedom. For example, the rotation of a normal mouse is not\nsensed, although there are designs such as the two-ball [MacKenzie 1997] and\nRockin\u2019Mouse [Balakrishnan 1997] that overcome this limitation.\nAs with expected movement, we can consider many different properties of\nsensed movement including degrees of freedom, range, speed, accuracy, and\nstability. It is also worth drawing attention to the wide variety of factors that\nlead to their limitations including, inherent limitations in the technologies,\nmanufacturing cost (budget models may be less instrumented or accurate), en-\nvironmental conditions (weather, lighting, and interference), computing power\n(requiring trade-offs between accuracy and responsiveness), and political con-\ntrol (e.g., the accuracy of civilian versus military GPS).\n5. EXPECTED VERSUS SENSED\nA key point of this article is that the expected and sensed movements of a given\ninterface may only partially overlap with interesting consequences emerging at\nthe boundaries. Figure 1 shows the four possible relationships between expected\nand sensed movements. Designers should consider what could happen in each\nof the four areas.\nExpected and Sensed. These are natural movements that can be sensed and\ndefine the \u2018normal\u2019 operation of the interface. This area has been the domi-\nnant focus for previous frameworks and taxonomies that have been mainly ori-\nented towards achieving the best possible match between expected and sensed\nmovements.\nACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.\nExpected, Sensed, and Desired: Designing Sensing-Based Interaction \u2022 11\nExpected But Not Sensed. These are natural physical movements that can-\nnot be sensed by the computer. Consider as examples: taking a PDA equipped\nwith a GPS indoors, tilting it, or moving it more precisely than the GPS can\nfollow; using a laser pointer to point at an object that is outside video tracking\nrange; stepping outside tracking range in an HMD; and, rotating a conventional\nmouse. The potential problem with such movements is that they may confuse\nusers. For example, an interface may appear to stop working as it moves out\nof sensing range. The user is performing natural movements but suddenly is\ngetting no response. Several options are open to the designer at this point.\n\u2014Improve the sensing by adding additional sensors or sensor technologies so\nthat sensed movement matches the expected movement. This adds additional\ncost and may not be possible.\n\u2014Constrain expected movements so that they match sensed movements, for\nexample, by adding a physical constraint or tether to prevent such movement.\nThis may be appropriate for already jointed or tethered displays where, for\nexample, rotations can be limited, but is less so for wireless interfaces that\ncan be moved freely.\n\u2014Change the application to work in a more static or less spatially precise\nmode when out of sensor range. When no sensor information is available,\nthe display can present static information that is clearly not expected to\nrespond to movement. When less precise sensor data is available, the display\ncan present information in a way that is less suggestive of a precise location\nor orientation, an approach demonstrated by the Guide tourist information\nsystem [Cheverst 2000].\n\u2014Communicate the limits of sensed movement to the user, either in software\n(e.g., messages on the interface to indicate that they are now out of tracking\nrange), or in the physical design of the environment (e.g., clearly delineating\nthe extent of video tracking in a room through visible markers, barriers, and\nfurniture).\n\u2014Ignore the issue and assume that users will adapt to the interface (e.g., we\nsoon learn that rotating a mouse has no effect on the cursor).\nHowever, we further suggest that movements that are expected but not\nsensed can present designers with opportunities as well as problems. They\nenable the user to reposition the interface without making input to the com-\nputer. Perhaps the most familiar example here is lifting a mouse off the edge of\na mouse mat so as to reposition it back to the center without affecting the cur-\nsor. Moving out of sensing range might be used as a way of deliberately pausing\nan application.\nRelated to this, deliberate pauses allow the user to take a rest by disen-\ngaging from the interface and entering a state where their physical actions no\nlonger trigger (now unwanted) effects. This is typically not a major concern for\nthe kinds of direct manipulation interface that have been the focus of previous\nframeworks since with these the user can often simply take their hands off the\ncontrols. It is much more of a concern with ubiquitous sensing-based interfaces\nwhere users may not be able to disengage, or where it may not be clear how\nACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.\n12 \u2022 S. Benford et al.\nto disengage. We suggest that rather than aiming for full sensor coverage, de-\nsigners should consider deliberately building rest spaces into experiences and\nshould make it clear to users how to enter them, especially where those expe-\nriences involve prolonged or extreme physical activity such as virtual sports,\ndance, or other performance.\nSuch movements also allow physical preparation for, and follow through\nafter, the point of interaction. The principle that a moment of interaction is\nactually embedded in an entire gesture that determines its timing and feel\nis familiar from sports (e.g. a golfer\u2019s swing). Physical movement around an\ninterface also facilitates expressive interaction during public performance as\nseen with traditional musical instruments such as pianos [Sudnow 1978]. In\ndiscussing electronic instruments, Bowers and Hellstrom [2000] refer to \u201cex-\npressive latitude\u201d\u2014designing interfaces to not sense every movement so as to\nleave space for physical performance. It seems that, far from being a \u201cdead\nzone\u201d, expected but not sensed movements may actually provide an important\nspace of opportunities for readjustment, rest, preparation, follow-through, and\nperformance\u2014important features of physical movement.\nSensed and Not Expected. These are movements that can be sensed but not\nnaturally or easily physically carried out. Perhaps the interface cannot easily be\nmoved through all of the available sensing range, or it is being used in a bizarre\nway or an unanticipated context. For example, GPS can sense when our example\nPDA is raised several hundreds of meters above the ground (perhaps the user is\nhang-gliding) or is moving faster than walking speed (perhaps they are running\nor are in a vehicle). Video tracking can detect a laser pointer that is being held\nperfectly still for many minutes (perhaps it has been left switched on, resting on\na table and the battery is in danger of running out). Electromagnetic trackers\ncan sense full 360-degree rotations of an HMD (perhaps it is in the user\u2019s hands\ninstead of on their head). Again, these may be problems or opportunities.\nTreating these movements as problems, the designer can extend the expected\nrange of movement, although this may involve a radical physical redesign, or\napplications can monitor and react to sensor data that is outside the expected\nnorm. Such data could indicate that the sensors are erroneous, that the device\nis physically broken (e.g., a part has become detached), or that someone is\nbehaving inappropriately with it (e.g., moving more quickly with it than they\nshould) and, as a result, the application might raise a warning or alarm.\nTreating the movements as opportunities, especially where the physical\nmovement involved is safe for both the user and the technology, but just not\nnormally expected, the designer might trigger special application functionality\nthat is rarely used or not generally available, for example, resetting or recon-\nfiguring a system or swapping into another mode of operation. Designers might\nreward users with an equally \u201codd\u201d experience, for example, revealing mys-\nterious information, or offering strange perspectives. In this way, sensed but\nnonexpected movements create a space for playful or mysterious uses of in-\nterfaces that otherwise behave conventionally, potentially a useful strategy for\nentertainment, performance, and artistic applications.\nNeither Expected or Sensed. These movements cannot be achieved easily and\ncannot be sensed anyway. In practice, the interface cannot distinguish them\nACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.\nExpected, Sensed, and Desired: Designing Sensing-Based Interaction \u2022 13\nFig. 2. Expected, sensed, and desired movements.\nfrom movements that are expected but not sensed, and so designers may best\ntreat them in the same way.\nAs a final note, although our discussion has focused primarily on the range of\nmovement, previous taxonomies of input devices show that designers can also\ncompare expected and sensed movement across other properties of movement\nincluding speed, accuracy, and stability. Even if the same basic physical move-\nments are possible, there may be mismatches in other properties (e.g., the user\nmay move an interface more quickly than its sensors can follow, or sensors may\nbe less accurate than physical movements).\n6. DESIRED MOVEMENTS\nSo far, we have discussed the design of interfaces independently of particular\napplications. For interfaces that are used with a variety of different applica-\ntions there is a further issue to be considered\u2014how does the range of available\nexpected and sensed movements relate to those that are needed for the appli-\ncation? What is it that we want the application to do? This leads us to the third\ncomponent of our framework, movements that are desired, or conversely that\nare possible but not desired. Understanding of desired movements emerges\nfrom the kinds of participatory, observational, and inspirational design meth-\nods that we reviewed earlier (more than they emerge from analytic frameworks\nfor analyzing input devices).\nDesired movements may only partially overlap with expected and\/or sensed\nmovements. In other words, there may be movements that are desired for the\napplication but that are not expected and\/or sensed (it might be very desirable\nfor users to be able to fly in some 3D entertainment applications), and other\nmovements that are expected and\/or sensed but that are not desired. Figure 2,\ntherefore, extends Figure 1 to include desired movements.\nOnce again, the designer can consider each of the outlying regions as a space\nof design problems or opportunities. We would like to raise one particularly\ninteresting design strategy, the idea of compensating between movements that\nare desired, but that are not expected and\/or sensed, and those that are expected\nACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.\n14 \u2022 S. Benford et al.\nand sensed, but not desired. For example, in the Go Go immersive VR interface,\nthe physical movement of extending one\u2019s arm right has the effect of extending\none\u2019s reach in the virtual world beyond its normal range [Poupyrev 1996]. This\nnonlinear mapping is taking one action that is expected and sensed, but not\nparticularly desired, and making it more desired.\n7. APPLYING THE FRAMEWORK\nWe propose that our framework can help with refining an outlined design con-\ncept or sketch towards a more detailed design specification, evaluating how dif-\nferent sensing technologies match proposed application requirements, or iden-\ntifying detailed potential problems or opportunities with a prototype. It might\nalso support the repurposing of an existing artifact by suggesting unusual or\nplayful ways in which it might be used. Applying the framework of Figure 2 to\nthe design of an interface involves the following steps.\n(1) Analyze expected movements, exploring the impacts of physical form, en-\nvisaged users, and environments. Consider for each degree of freedom, the\nrange, speed, accuracy, and stability of expected movements. Spend time\nimagining scenarios that could result in less-expected movements. Consider\nwhich movements are genuinely impossible (rather than just unlikely). This\nstep can draw on existing analytic frameworks but we would also encourage\ndesigners to deliberately imagine and discuss the extreme boundaries of un-\nexpected physical movement, envisaging situations in which the interface\nmight be accidentally or even willfully misused.\n(2) Analyze sensed movements by identifying all of the known limitations of\nthe sensing technologies. Again consider the range, speed, accuracy, and\nstability of sensing for each degree of freedom. Deliberately try to imagine\nhow you could fool sensing systems. This step can also utilize existing ana-\nlytic frameworks but we emphasize the importance of explicitly identifying\nthe extremes of what can be sensed.\n(3) Analyze desired movements for your application. Apply inspirational design\nmethods to determine how your ideal interface would move if unconstrained\nby the limitations of the physical world and available sensing technologies.\n(4) Consider each of the different regions of Figure 2, trying to find possible\nmovements to fit each. Consider whether each issue raised represents a\nproblem to be solved or an opportunity to be exploited. In each case, consider\nthe design options outlined previously and whether users will require rest\nor will perform with an interface.\nWe now describe how our framework has been applied to the design of three\ncontrasting interfaces: the use of flashlights as interaction devices in under-\nground caves and with public wall displays; the design of a wheeled, mobile\n3D display for use outdoors at a museum; and the design of a piece of inter-\nactive domestic furniture. Between them, these examples span a variety of\nphysical forms (handheld flashlight, display attached to wheeled base, and a\ntable); include both purpose-designed physical objects (the 3D display) and the\naugmentation of everyday objects (flashlights and the table); use different\nACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.\nExpected, Sensed, and Desired: Designing Sensing-Based Interaction \u2022 15\nsensing technologies with different degrees of precision (video tracking, a com-\nbination of GPS and other sensors, and load sensing); are intended for use in\ndifferent environments (museums and the home); and are more or less \u201ctask ori-\nented\u201d, ranging from the defined task of exploring a historical 3D recreation to a\nmuch more open-ended style of engagement with an item of domestic furniture.\nIt should be noted from the outset that development of the framework and\ndevelopment of the three examples have occurred in parallel in such a way that\nthere has been a flow of ideas from the framework to the designs and back again.\nThis is particularly true of the first two examples that we present which both\ninformed the framework, and were informed by it, across several iterations.\n8. EXAMPLE 1: INTERACTIVE FLASHLIGHTS\nOur first example focuses on the use of flashlights for interacting with surfaces\nsuch as walls and posters. In this case, the sensing technology is based on vi-\nsual tracking; a video camera captures an image of the surface onto which a\nuser directs the flashlight beam. Image processing software extracts key fea-\ntures of the beam including its position, shape, and extent in real time and uses\nthese to trigger events, for example playing a sound whenever the beam illu-\nminates a designated target area. This is similar in principle to using visually\ntracked laser pointers to interact with large displays [Olsen 2000; Myers 2002;\nDavis 2002], although there are significant differences, too; most notably that a\nflashlight casts a pool rather than a point of light whose size and shape varies\naccording the user\u2019s position relative to the surface and the kind of flashlight\nbeing used. Consequently, flashlight beams can select areas of a surface and\ncan overlap, potentially enabling different kinds of collaboration where several\nbeams are brought together.\nOur visually-tracked flashlights technology is targeted at museums, exhibi-\ntions, tradeshows, and even classrooms since it involves the use of everyday\ndevices that are familiar, cheap, fun, and safe. We have explored three appli-\ncations to date. In the first, children used flashlights to control objects in a\nvirtual environment that was projected onto the tent-like immersive interface\nshown in Figure 3 [Green 2002]. In the second, visitors to Nottingham Castle\nMuseum used flashlights to trigger ghostly voices when exploring a series of\nunderground caves [Ghali 2003]. In the third, flashlights were used to create in-\nteractive posters such as the solar system poster, also shown in Figure 3 (right),\nthat replays audio descriptions of each planet as it is illuminated [Ghali 2003].\nExperience with these applications was one of the motivating factors for\ndeveloping our framework, and in the following discussion, we distinguish be-\ntween those design issues that inspired the framework (i.e., cases where we\nfirst encountered an issue which led us to make a generalization) and those\nthat were directly inspired by the framework (i.e., where we then applied the\ngeneralization to redesign the technology).\n8.1 Summary of Expected, Sensed, and Desired Movements for Flashlights\nExpected. Considering expected movements, flashlights can be carried or worn;\nthey can be handheld, head-mounted (e.g., when caving), stand-mounted or\nACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.\n16 \u2022 S. Benford et al.\nFig. 3. Using flashlights with the Storytent (left) and with an interactive poster (right).\neven vehicle-mounted and vary greatly in size and scale, ranging from small\nhandheld flashlights to large directional spotlights. We expect visitors to point\nflashlights at a sequence of target features on a surface in a systematic way\nand then listen to the resulting sounds and sweep across a surface in order\nto find targets. Flashlights also serve their traditional purpose of illuminating\ndark spaces so that we expect visitors to point them into dark areas in order to\nfind out what is there or see where they are going. We have observed that it is\ndifficult to hold larger flashlights perfectly still and that visitors will sometimes\nshine multiple flashlights onto a single surface causing their beams to overlap.\nBattery life is an issue, with the intensity of a beam becoming noticeably weaker\nas the batteries begin to run out, and it is expected that a person would switch a\nflashlight on and off, if only to conserve battery life. We can also expect ambient\nlighting levels to change in some environments, for example, as people switch\nlights on and off, or open and close doors.\nLess expected, but still possible, movements include not looking where you\nare pointing a flashlight (but people may do this when talking to one another),\nor waving a flashlight around very rapidly (children have been observed doing\nthis). We consider it to be a less expected action to defocus a flashlight (where\nit has a variable focus) or to obscure the beam in some way although this may\nbe unavoidable and people may even deliberately cast shadows. Repeatedly\nswitching the flashlight on and off may be less expected, unless perhaps sig-\nnaling to others. Also, leaving the flashlight switched on and pointing at one\nspot for a long time is less expected, although perhaps the user has put it down\nso that they can work with both hands. It is also less expected to shine the\nflashlight directly into the video camera (although we observed some people\ndoing this when they first noticed the camera), or to shine it into people\u2019s eyes\n(although this happens when wearing a head-mounted flashlight as people face\none another to talk).\nSensed. Turning to sensed movements, our tracking software extracts the\nposition of the centroid and extent of the area of the image of the flashlight\nACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.\nExpected, Sensed, and Desired: Designing Sensing-Based Interaction \u2022 17\nbeam on the surface. However, this is only possible when the beam image falls\nwithin the camera\u2019s field of view which may only cover a part of the total en-\nvironment, especially when deployed in a large area. This was apparent in our\ncave experience where, even with three cameras, we could only cover a fraction\nof the total surface area of the cave and where the boundaries of the interactive\nparts of the surface were not clearly visible as they would be if interacting with\na projection screen. Second, the sensing technology can be fooled by changes in\nambient lighting conditions which effectively change the background image (a\nwell known problem for visual tracking technologies in general). Objects that\nobscure the camera\u2019s view (e.g., people walking in front of it) can also confuse\nthe tracking software.\nDesired. Finally, we consider general characteristics of desired movements.\nIt should be easy to reach the targets with the flashlight beam and to hold the\nbeam on a target once found. It will be necessary to support a variety of target\nsizes, shapes, and placements. Groups of visitors may wish to share an experi-\nence, and this has implications for how they can position themselves to view the\nsurface and also how they can share audio output. It may also be necessary to\ndeal with potential interference between groups of visitors, for example, shield-\ning them from the sound that is triggered by other groups, avoiding conflicting\nuse of flashlights on a single surface, and generally managing visitor flow.\n8.2 Comparison of Expected, Sensed, and Desired Movements for Flashlights\nWe now compare these expected, sensed, and desired movements. The first two\nissues below arose from our initial experience and inspired us to develop the\nframework in the first place. The framework is therefore playing an explanatory\nrole in these cases.\nShining the beam outside tracking range (expected, and maybe desired, but\nnot sensed). Users can be expected to point the beam outside of the camera\nview. Indeed, this may be desired if they are finding their way around a dark\nenvironment. This observation inspired an option in the system to play a back-\nground sound whenever the flashlight beam is recognized as being in tracking\nrange but is not currently on a target in order to confirm to the user that they\nare in the right area. This technique also allows the user to understand when\nthey can safely use a flashlight for other purposes such as general illumination,\nor signaling without accidentally triggering targets. Early experience with this\ntechnique has suggested a further refinement. It is often the case that the extent\nof the camera viewpoint does not precisely match the relevant visual features\nof the surface. For example, it was not feasible to position the camera so that\nits field of view exactly matched the edge of the poster shown in Figure 3 (in\nthis case, the camera could see an area of the wall on either side of the poster).\nOne implication is to constrain the active tracking range to be the subset of the\nfield of view that matches the desired tracking range (e.g., specifying the edge\nof the poster as being the active region).\nWobbly flashlights (expected, sensed, but not desired). The observed wobble\nof a flashlight beam as a user tries to hold it on a target may be both expected\nand sensed, but it is not desired if it produces an annoying effect where the\nACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.\n18 \u2022 S. Benford et al.\nassociated action is repeatedly stopped and retriggered. Our solution has been\nto refine the mechanism for triggering a target. Early implementations trig-\ngered a target whenever the centroid of the beam entered the defined target\narea based on the idea that the user is pointing with the beam, rather like they\nwould with a laser pointer. Later implementations use a revised mechanism\nthat measures the proportion of the target that is illuminated by the area of\nthe beam. The target is triggered when this exceeds a critical threshold. This\napproach assumes that the user is illuminating the target rather than pointing\nat it which we suggest is more in line with the expected use of flashlights, and\nhas proved to be more accommodating to wobble.\nOur next two issues arose from reapplying the framework back to the design\nof the technology and provide examples of how it can help to generate new\ndesign possibilities.\nDetecting a very static beam (sensed, but maybe not expected or desired). The\ntracking system can potentially detect when a beam is held precisely still in one\nspot for an extended period of time (say several minutes). Given the tendency\nto wobble when handheld, this would suggest that the flashlight has been put\ndown on a surface while still switched on and might indicate a potential problem\n(perhaps it has been left behind and forgotten and perhaps the batteries will\nrun down) which might, in turn, generate a warning. Again, this might also be\nseen as an opportunity. Perhaps users could leave flashlights in position for a\nwhile in order to achieve special effects, for example, metaphorically \u201cdrilling\u201d\ninto a surface to reveal new content.\nUsing a defocused flashlight (not expected, potentially not sensed, and not\ndesired). Initially the idea of defocusing the beam seems problematic as it can\nno longer be tracked. This problem can be solved by physical constraint, using\nflashlights that can\u2019t be refocused, or by jamming the refocusing mechanism.\nHowever, this might also be an opportunity. It might sometimes be desired\nto be able to use a flashlight for general illumination without triggering any\ntargets. This could be achieved by deliberately defocusing the flashlight so that\nthe beam is no longer visible to the sensing system. This demonstrates an\ninteresting approach to disengaging from a sensing system without leaving the\nphysical sensing area.\n9. EXAMPLE 2: THE AUGURSCOPE II\nOur second example, the Augurscope II, is a portable mixed-reality display for\nviewing 3D historical reconstructions when outdoors. Users wheel it around a\nphysical site and rotate and tilt its display in order to view a 3D model as it\nwould appear from different physical vantage points. Our design, shown in\nFigure 4, responds to issues that were raised by public trials of an earlier\nprototype, particularly limited mobility [Koleva 2002]. It supports two modes\nof use: stand-mounted in which the top is attached to the wheeled-base by\na mounting that allows rotation, tilting, and various other adjustments; and\nhand-held in which the top can be detached and moved more freely. The design\nof the stand features two handles, one attached to the base and one to the\nrotating mount for the top, so that users can rotate the top while pushing the\ndisplay along.\nACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.\nExpected, Sensed, and Desired: Designing Sensing-Based Interaction \u2022 19\nFig. 4. The Augurscope II, stand-mounted and hand-held.\n9.1 Summary of Expected, Sensed, and Desired Movements for the Augurscope\nExpected. As designers, we expected individuals or small groups to wheel our\ninterface across relatively flat terrain at slow walking pace, occasionally stop-\nping to rotate, and tilt it in order to explore a particular viewpoint. More ex-\nperienced users may be able to study the display as they wheel it, combining\npushing, rotating, and tilting movements. Some may detach the top and use\nit in hand-held mode in which case we would expect relatively cautious move-\nments and probably no long, sustained poses due to its weight. They might also\nlay the top flat on the ground, especially if tired, or try to take it indoors. In\nterms of less-expected movements, we would not expect users to run quickly\nwith the device, to spin the top around rapidly, to turn it over and over in their\nhands, to move through solid walls, to take it underground, or to fly above the\nground.\nAs well as clarifying our general expectations of use and potential misuse,\nwe carried out a more systematic analysis of how the proposed physical form\nof our design would afford and constrain expected movements. Table I summa-\nrizes expected movements for the six degrees of freedom of possible movement:\ntranslate sideways (\u2191X), raise and lower (\u2191Y), push and pull forwards and back-\nwards (\u2191Z), tilt forwards and backwards (\u03b8X), rotate around vertical axis (\u03b8Y),\nand tilt sideways (\u03b8Z).\nAs noted by Foley at al. [1984], it is possible to combine two or more degrees\nof freedom into a single movement. Rotating (\u03b8Y) while tilting (\u03b8X) the top is\nexpected as this can easily be done with one handle. Using two hands to rotate\nand tilt the top while pushing the base (\u2191Z) might be expected. Rotating and\ntilting the top while raising and lowering it (in unlocked mode) is possible, but\nis much harder and hence less expected. Raising and lowering while pushing\nis possible, but only with great difficulty.\nACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.\n20 \u2022 S. Benford et al.\nTable I. Details of Expected Movements of the Augurscope II\nAccuracy, Speed,\nDoF Range of Movement Stability\n\u2191X Not expected (the user would have to lift it off of the\nground and carry sideways)\n\u2191Y Expected height adjustment involves unlocking ad-\njusting screw, raising or lowering, and locking again\n(range of 70cm). More extreme movement not ex-\npected. Less expected to raise and lower dynamically\nwhile unlocked.\nMillimeter accuracy. Takes\nseconds in locking mode,\nbut is stable. Quicker but\nless stable when unlocked\n\u2191Z Expected to push forwards and pull backwards. Un-\nconstrained in range (unless by obstacles).\ncm accuracy. Expected at\nwalking pace.\n\u03b8X Freely and indefinitely tilt the top forwards and\nbackwards. Not expected to rotate through many\nloops.\nBetter than 1\u25e6 accuracy.\nTakes seconds. Stable.\n\u03b8Y Can freely and indefinitely rotate the top unit on the\nstand. May not be expected to rotate through many\nloops.\nBetter than 1\u25e6 accuracy.\nTakes seconds. Stable.\n\u03b8Z Not physically possible on when stand mounted\nUsing the top in hand-held mode affects this analysis in several ways. \u2191X and\n\u03b8Z are now possible. The range of \u2191Y is extended down to ground level and up to\nthe maximum height to which a user can lift the screen and still see it. The speed\nof movement can also be increased. However, stability will be reduced because\nthe user has to hold the display in position rather than it resting on a supporting\nstand. It is also easier to combine degrees of freedom in hand-held mode.\nSensed. Turning now to sensed movements, the Augurscope II uses two sens-\ning technologies. A Trimble GPS receiver provides global position and a Honey-\nwell HMR3000 digital compass measures rotation and tilt. Both are integrated\ninto the top unit which communicates wirelessly with the base using 802.11b\nnetworking. Table II summarizes the range, accuracy, and delay associated with\nsensing each degree of freedom of movement. Several aspects need emphasiz-\ning. First, the range of GPS sensing extends a long way above ground, but not\nbelow it. Second, the digital compass can only sense up to 45\u25e6 of tilt downwards\nor upwards. Third, the top unit can stray out of communications range of the\nbase unit when in hand-held mode.\nDesired. When considering desired movements, we focus on the example\napplication of viewing a 3D recreation of a historic castle when exploring its\npresent day site, the same application that was used for testing the first proto-\ntype [Koleva 2002]. The interesting detail of the 3D model is in buildings as well\nas in an underground cave section. Conversely, there is no significant detail on\nthe surface of the ground or high in the sky. The model also covers a restricted\ngeographical area. It is therefore desired to move around this area and to look\nat objects at building height. It is less desired to look at the ground or high into\nthe sky or to move outside of the geographical area that is modeled. At some\npoints it would be desired to be able to fly under the ground. Our experience\nshows that it is also desired for users to be able to see a bird\u2019s eye view of such\nmodels, both to be able to orientate themselves and for the novel perspective\nthat this brings.\nACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.\nExpected, Sensed, and Desired: Designing Sensing-Based Interaction \u2022 21\nTable II. Details of Sensed Movements for the Augurscope II\nDoF Sensed by Sensed range Accuracy Delay\n\u2191X, \u2191Y, \u2191Z GPS receiver not indoors, in black\nspots, underground or\nextreme latitudes\ncm to meters\nvaries\n1 hz.\n\u03b8X\n\u03b8Z\ncompass +45\u25e6 to \u221245\u25e6 < 1\u25e6 20 hz\n\u03b8Y compass 360\u25e6 < 1\u25e6 20 hz\n9.2 Comparison of Expected, Sensed, and Desired Movements for\nthe Augurscope\nExtreme tilting (expected, but not sensed or desired). The User may tilt the\ninterface beyond its sensed range of 45\u25e6 up and down, especially in hand-held\nmode, in which case it will appear to suddenly stop responding. However, given\nthat there is no interesting detail on the floor or sky of the 3D model, this is not\nan especially desired movement.\nFlying (sensed and desired, but not expected). The GPS can sense the desired\nmovement of flying into the air (although looking down for a bird\u2019s eye view is\nnot sensed as noted above), but the user cannot lift the device off the ground.\nThese two issues have been addressed together through a single extension\nto the design. We have altered the mapping between the tilt of the top and the\nmovement of the virtual camera in the 3D model. The tilt has been exaggerated\nso that for every sensed degree of tilt, two degrees are rendered. Additionally\nbetween 20\u25e6 to 45\u25e6, the camera pulls upwards. At 45\u25e6, the limit of sensed move-\nment, the virtual camera has tilted to 90\u25e6 (i.e., is looking straight down) and has\nraised several tens of meters into the air to give a bird\u2019s eye view as shown in\nFigure 5. The view remains static beyond 45\u25e6. This provides an example of the\ncompensation strategy described earlier in which a sensed, expected, but not\nespecially desired movement (20\u25e6 to 45\u25e6 tilt) is remapped to support desired,\nbut not expected and\/or sensed movements.\nExploring caves (desired, but not expected or sensed). The desired act of\nexploring the caves in the 3D model requires taking the interface underground.\nHowever, there is no suitable physical access and GPS will not work there. Our\nframework suggests that we might exploit a similar compensation strategy\nas previously mentioned, using extreme upward tilting to drop the viewpoint\nbelow ground level into a virtual cave. However, two further issues have to\nbe addressed. First, the caves only exist at limited locations under the castle\ngrounds, and so it is appropriate to trigger this mechanism only when the\nAugurscope is above a cave, requiring an additional indication that there are\ncaves below. Second, it would not be appropriate for the viewpoint to remain\nfixed in the cave ceiling once underground. A better solution might be for an\nextreme upward tilt to trigger a downward navigation of the viewpoint, after\nwhich the Augurscope could be rotated as normal to explore a panoramic view\nof a cave (but not translated as there is no GPS) and for a subsequent extreme\ndownward tilt to take the viewpoint back up above ground.\nRunning with the Augurscope II (sensed, but not expected or desired). Run-\nning while looking at the screen is not expected and not desired, and is debatably\nACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.\n22 \u2022 S. Benford et al.\nFig. 5. Comparing normal with exaggerated tilt.\nsensed (the GPS can follow, although with some latency). One possibility here\nis to replace the view of the 3D model with a warning message. In terms of the\nframework, this deliberately makes the device less desired when it is not being\nused sensibly.\nMoving outside the model (expected and sensed, but not desired). The user\nmight wheel the augurscope outside the castle grounds and hence outside of the\nscope of the virtual model. There is no benefit associated with this since there is\nno virtual model to explore, but there is certainly a risk (the augurscope might\nbe stolen!). Our proposal is to raise an alarm and to encourage the user to take\nthe device back into range.\nAreas of poor GPS (desired and expected, but not sensed). We are concerned\nabout the effects of variable GPS (inside buildings, undercover, or by a wall).\nOne proposed solution is to switch to a mode in which there is a dialogue with\nthe user to confirm their location, perhaps using a touch-screen. Another is\nto present information in a less precise way, for example, turning off the exact\nmapping between GPS and the 3D viewpoint and instead exploring a panorama\nfrom a predetermined viewpoint.\n10. EXAMPLE 3: THE DRIFT TABLE\nOur third example is the Drift Table, an interactive coffee table (Figure 6) that\nallows its owners to take an aerial trip over Great Britain. The Drift Table is an\nexample of a load-sensing interactive surface [Schmidt 2002]. It uses four load\nsensors to determine the total weight and center of gravity of the objects on its\nsurface. These two measures control a viewpoint that looks down onto a series\nof aerial photographs that are drawn from a database that covers the whole of\nACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.\nExpected, Sensed, and Desired: Designing Sensing-Based Interaction \u2022 23\nFig. 6. The Drift Table.\nEngland and Wales at 25cm resolution (kindly provided by Getmapping.com).\nThese are stitched together so that the viewer appears to be smoothly and seam-\nlessly traveling across the entire country and are viewed on a small circular\ndisplay that is sunk into the center of the table. The direction of movement\nis given by the direction of the center of gravity relative to the center of the\ntable, and the speed of movement is a function of the total weight on the ta-\nble multiplied by the distance of the center of gravity from the center of the\ntable. Finally, the more weight there is on the table, the lower its apparent\naltitude.\nThe Drift Table is primarily a coffee table, but one that is also intended to\nprovide its users with an engaging, stimulating, and provoking experience that\nis deliberately designed to be open-ended rather than focused on achieving\na particular task. The Drift Table experience is also open-ended in terms of\nits duration. At one extreme, users might move objects on its surface to see\nimmediate changes in the viewpoint. At the other, they might leave objects and\nreturn to the table after hours or maybe even days to see changes.\n10.1 Summary of Expected, Sensed, and Desired Movements for the Drift Table\nAn initial design was in place and the construction of a prototype underway\nwhen our framework was introduced to the project through a series of brain-\nstorming sessions in which designers tried to envisage a wide variety of scenar-\nios of use. The Drift Table has since been completed and tested.\nExpected. It is expected to place everyday objects such as books, magazines,\nand drinks on the Drift Table, to take them off again, and to move them around\non its surface, including translating, rotating, and stacking them. It is expected\nmovement to clean the table and to occasionally move it to clean around or\nunderneath it. Moving the table might involve rolling (it has castors) or lifting\nit. Objects with changing or shifting weight such as candles or plants might\nalso find their way onto the table. People can be expected to lean on the table\n(e.g., to look into the display) or even write or sit on it.\nACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.\n24 \u2022 S. Benford et al.\nLess expected, but certainly possible, are standing and jumping on the table.\nPets such as cats might walk over the table from time to time. People might\nalso lift one or two legs (perhaps for a quick clean underneath) or jog the table\nwhen passing by or using a vacuum cleaner. Less expected also is using the\ntable as a support when hammering nails or putting a plank or trestle between\ntwo tables when decorating (although this might be more expected with less\n\u201cdesigner\u201d tables). It is also not expected to turn the table over or to stand\nit on its side. Another unlikely possibility is placing the table in a moving\nenvironment where it might be subject to external forces or might change its\norientation with respect to the outside world, on a yacht for example.\nSensed. There are two key factors to be considered with regard to the Drift\nTable\u2019s sensed movements: the characteristics of each load sensor, and the way\nin which the four sensors combine to measure overall weight and center of\ngravity.\nThe load sensors are industrial precision load cells based on resistive strain\ngauges. The physical characteristics of the load cells used in the design for the\nDrift Table provide a response for a force from 0 to 500N. If the load cells are\nused in a horizontal scale, this is the gravitational force created by a weight\nof about 50Kg. Between 500N and 1000N, the response is not linear anymore\nbut there is no damage to the load cell. If a force greater than 1000N is ap-\nplied, it might damage the load cell permanently. It takes on the order of\n500ms for load measurements to reach a stable reading because the surface\nof the table wobbles and then settles down due to the materials used, its con-\nstruction, and the force with which load is applied (e.g., throwing a book onto\nthe table causes a greater and longer period of instability than gently placing\nit).\nEach sensor measures one degree of freedom\u2014the load acting on it from\nabove. However, the use of four sensors allows two degrees of freedom to be\nderived\u2014the overall weight and the eccentricity of the center of gravity. If only\none object is moved at a time, it is possible to infer its movement across the\nsurface, for example a finger can be used as a drawing tool [Schmidt 2002]\nalthough moving multiple objects at a time would fool such an interpretation.\nIt might be possible to identify which objects have been added to, or removed\nfrom, the table if each has a unique weight, although again, a person could\neasily be fooled if different objects turn up with the same weight or objects\nchange their weight over time (e.g., a burning candle).\nThe surface of the table has a weight, and so the whole sensing system has to\nbe calibrated to output an effective weight of zero whenever just the surface is\npresent. This raises the possibility of sensing \u201cnegative weight\u201d, for example, if\nthe surface is somehow lifted. Calibration is done automatically when switching\non the table. For a period of 5 seconds, the average of each sensor is taken and\nstored as a base weight for this sensor. All further calculations are then relative\nto this base weight. If during this initialization process, there is additional\nweight on the table that is then taken off, it would lead to the empty table\nregistering negative weight.\nDesired. Our initial design was guided by a general principle that using the\nDrift Table should be analogous to riding in a hotair balloon. Movements that\nACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.\nExpected, Sensed, and Desired: Designing Sensing-Based Interaction \u2022 25\nbreak this metaphor (e.g., zooming and displaying extraneous information) are\ngenerally undesired.\nThe range of movement needs to cover the whole area of Britain and altitude\nalso needs to vary, but within a fixed range, so that the image remains inter-\nestingly visible at the highest altitude and yet readable without being grainy\nat the lowest.\nIn terms of speed, movement should be responsive enough to quickly and\nvisibly react to a sudden change in load and yet should move slowly enough to\nmake a gradual journey across the country over the course of several hours or\nmaybe even days. The maximum speed of movement will be about 150 km\/hr\nso that an appreciable amount of time will be required to traverse Britain even\nat its narrowest point.\nAlthough one of the primary functions of the Drift Table is to get lost over\nthe British landscape, it should be possible to \u201cfind\u201d oneself from time to time\nor frustration might ensue. Thus it is desired to know the current (virtual)\nlocation of the table, checked against a map if necessary. In addition, it is desired\nthat the orientation of the image corresponds to the orientation of the user\u2019s\nenvironment so that heading north from a starting point over one\u2019s home will\ncause the image to move in the appropriate direction. Finally, in order to avoid\nbeing in less desired parts of the country for days at a time, it is desired to\nreset the table\u2019s location to be over the user\u2019s home on rare occasions\u2014the only\ndiscontinuous movement deemed to be acceptable.\n10.2 Comparison of Expected, Sensed, and Desired Movements for\nthe Drift Table\nWe now describe how a comparison of these expected, sensed, and desired move-\nments through a series of face to face meetings and email exchanges among the\ndesign team inspired new design ideas. The overall goal of keeping the table\nopen-ended rather than too task specific, combined with the desire to achieve\na strong and quite minimal aesthetic, has meant that many of these new ideas\nwill not be included in the final prototype. However, we discuss them here as\nthey demonstrate the use of the framework to raise new possibilities and might\nalso be relevant to other load-sensing surfaces.\nKeeping the display oriented to North when the table is turned (desired and\nexpected, but not sensed). A discussion of the possibility of the table being\nmoved (expected) raised the question of what should happen to the orientation\nof the maps being displayed. The design team felt that they should always\norient correctly to the North whatever the orientation of the table and hence its\ndisplay. However, this would require being able to sense the rotation of the table\n(currently not sensed). As a result, the team extended the sensing capability of\nthe table by adding a digital compass to its hardware.\nResetting the viewpoint (desired, but not sensed or expected). The initial\ndesign required a way of resetting the viewpoint to a default position. The\napplication of our framework revealed the possibility of generating negative\nweight by pulling upwards on the load sensors, and it was felt that this might\nbe an appropriate way of triggering a viewpoint reset. Given the boxed-in design\nACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.\n26 \u2022 S. Benford et al.\nof the nearly completed prototype, there was no easy way to achieve this (i.e., it\nis not currently expected) and a reset button, hidden away near the floor, was\nprovided. However, an option for the future is to build a new housing so that\nthe surface stretches a few centimeters away from the base whenever the table\nis lifted, allowing negative weight to be generated.\nJourney objects (sensed, expected, and desired). Our discussions of objects\nthat change weight or position over time led to a new idea: journey objects that\ntake the viewpoint on a predictable journey. For example, a burning candle\nloses weight in a predictable way. This appears to the Drift Table as a gradual\nreduction in overall weight and a shift in the center of weight away from the\ncandle back towards the center of the table. Users could \u201cprogram\u201d the table\nto undertake a journey of a given distance in a given direction by placing can-\ndles of specific weights at specific locations on the table. A \u201crotating compass\ncandelabra\u201d might even allow the user to place a candle at a set distance from\nthe center in a particular compass direction. Different candles could then be\nmanufactured to travel set distances.\nLimit speed in order to limit weight (expected, sensed, and not desired). It was\ndecided to introduce a maximum speed limit for the movement of the viewpoint,\nbeyond which adding more weight to the table wouldn\u2019t make any difference.\nThe aim here was to deliberately discourage less expected behaviors such as\nstanding or jumping on the table, or loading it heavily in order to see how fast it\nmight go. This is an example of deliberately constraining sensed movement in\norder to avoid less expected physical behavior. If this strategy fails, then more\ndrastic measures might be called for such as removing the images altogether\n(making the table useless during nonexpected use).\nReducing sensitivity to sudden weight change (expected, sensed, but not de-\nsired). A related issue concerns managing acceleration. While it is desired for\nthe table to be visibly responsive to the movement of objects, sudden and large\nchanges in velocity (due to heavy weight being placed on the table) can cause\nproblems. In particular, a software caching mechanism needs to predict which\naerial photographs to preload from the database into the rendering software\nin order to ensure a smooth viewing experience. This prediction becomes dif-\nficult when velocity changes rapidly. Our solution here is to treat any sensed\nchange in the total weight and center of gravity as defining a target velocity\nto which the table gradually accelerates over a configurable period of time so\nthat some change is immediately noticeable, but that the caching mechanism\nhas time to adapt to it. This effectively reduces the (apparent) sensitivity of the\ntable (i.e., increases its sensing delay) in order to achieve a tradeoff between\ntwo potentially conflicting desired movements.\n11. SUMMARY AND REFLECTION\nDriven by four trends in interfaces\u2014the growth of sensor-based interaction, the\ndiversification of physical forms, increasing mobility, and a focus on playful,\nengaging and creative applications\u2014we believe that interface designers will\nincreasingly have to wrestle with the complex problem of matching physical\nform to the capabilities of sensors and the shifting requirements of applications.\nACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.\nExpected, Sensed, and Desired: Designing Sensing-Based Interaction \u2022 27\nThere is already a range of existing frameworks and methods available to\nsupport them. Existing taxonomies of input devices support detailed analysis\nof how the physical form of a device matches its sensing capabilities. Existing\ndesign methods such as participatory design, ethnography, and emerging in-\nspirational methods can be used to generate new design ideas. However, we\nhave argued that none of these existing frameworks and methods is in itself\nsufficient to address the entire problem. First, they have not been focused on\nthe specific challenges raised by working with imprecise sensor technologies\nand augmented everyday artifacts. Second, we believe that successful design\nneeds to combine both analytic and inspirational perspectives.\nWe have introduced a new framework that encourages designers to tackle\nthis problem head-on by analyzing and comparing expected, sensed, and de-\nsired movements. Our framework focuses on the boundaries between these,\ndrawing on analytic and inspirational approaches, and treating mismatches\nas opportunities as well as problems. We have applied our framework to three\nexample interfaces.\nFor the interactive flashlights, the framework was used to help understand\ninitial experiences. Indeed, development of the framework was in part inspired\nby these experiences. We then reapplied the framework back to the technology,\nwhich generated further insights in areas such as defocusing a flashlight in\norder to disengage from the sensing system; the potential for setting up a very\nstatic beam to trigger special effects; and a reconsideration of the relationship\nbetween the expected extent of users\u2019 movements with a flashlight, the extent\nof sensed movement in terms of the video camera\u2019s view, and the extent of\ndesirable movement in terms of the interactive content (e.g., a poster).\nFor the Augurscope II, the framework was introduced relatively late in the\ndesign process to refine and extend the design of a second generation prototype.\nIn this example, which is closest to designing a new physical input device,\nwe combined an analytic approach in reasoning about expected movements\nwith a deliberate attempt to question assumptions about users\u2019 likely actions.\nApplying the framework led to several new design ideas including our extended\ntilt mechanism.\nFor the Drift Table, the framework was employed earlier in the design pro-\ncess to help inspire design ideas in moving from a general concept to a first\nconcrete prototype. Our analysis here focused on brainstorming a wide variety\nof potential uses of a table and considering how these matched the capabil-\nities of the load-sensing technology as well as the more open-ended goals of\nthe \u201capplication\u201d. Several new insights emerged from this discussion including\nthe use of journey objects, the need to maintain orientation to the North, and\nintroducing constraints on the speed and acceleration of apparent movement.\nAcross all three examples, the framework has helped us clarify design trade-\noffs, identify and explain likely problems with interaction, and has sometimes\nhelped inspire new interaction possibilities. We believe that these examples\nshow that the framework has the potential to support both the analysis of de-\nsigns and the generation of new ideas. However, it is not a silver bullet. There is\nno guarantee that the resulting ideas are good ones. Applying the framework to\nthe Drift Table generated many new possibilities, most of which were rejected\nACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.\n28 \u2022 S. Benford et al.\nby the design team as over-complicating the design. Indeed, one of the main\nuses of the framework was to strongly focus discussion on the question of what\nwas desired.\nFuture work will focus on applying the framework to a broader set of de-\nsigns in order to better understand its role within the design process. This will\ninvolve packaging the framework in a way that makes it easier for others to\napply. We are intrigued by the possibility that focusing on mismatches between\nexpected, sensed, and desired movements can lead to new design opportunities\nand believe that this will be a useful design tactic as interfaces become more\ngraceful, sporting, artistic, and playful. With this in mind, we are particularly\ninterested in applying the framework to emerging applications that involve\nextreme physical interaction such as sports and performance.\nREFERENCES\nBALAKRISHNAN, R., BAUDEL, T., KURTENBACH, G., AND FITZMAURICE, G. 1997. The Rockin\u2019Mouse: In-\ntegral 3D manipulation on a plane. In Proceedings of the Conference on Human Factors in Com-\nputing Systems. Atlanta, GA. 311\u2013318.\nBELLOTTI, V., BACK, M., EDWARDS, W. K., GRINTER, R. E., HENDERSON, A., AND LOPES, C. 2002. Making\nsense of sensing systems: Five questions for designers and researchers. In Conference on Human\nFactors in Computing Systems. Minneapolis, MN. 415\u2013422.\nBLESER, T. W. AND SIBERT, J. L. 1990. Toto: A tool for selecting interaction techniques. In Proceed-\nings of the 3rd annual ACM SIGGRAPH Symposium on User interface Software and Technology.\nSnowbird, UT. 135\u2013142.\nBOUCHER, A., GAVER, W., PENNINGTON, S., AND WALKER, B. 2003. Workbook one: Ideas, scenarios and\nproposals for the home, available at www.interaction.rca.ac.uk\/equator\/papers\/workbook1.pdf.\nBOWERS, J. AND HELLSTROM, S. 2000. Simple interfaces to complex sound in improvised electronic\nmusic. In Proceedings of the Conference on Human Factors in Computing Systems, Supplementary\nProceedings. The Hague, Netherlands. 125\u2013126.\nBOWERS, J. AND PYCOCK, J. 1994. Talking through design: Requirements and resistance in coop-\nerative prototyping. In Proceedings of the Conference on Human Factors in Computing Systems,\nSupplementary Proceedings. New York, NY. 299\u2013305.\nBUXTON, W. 1983. Lexical and pragmatic considerations of input structures. Comput. Graph. 17,\n1, 31\u201337.\nCARD, S. K., MACKINLAY, J. D., AND ROBERTSON, G. G. 1991. A morphological analysis of the design\nspace of input devices. ACM Trans. Inform. Sys. 9, 2, 99\u2013122.\nCHEVERST, K., DAVIES, N., MITCHELL, K., FRIDAY, A., AND EFSTRATIOU C. 2000. Developing a context-\naware electronic tourist guide: Some issues and experiences. In Proceedings of the Conference on\nHuman Factors in Computing Systems. The Hague, Netherlands. 17\u201324.\nCRUZ-NEIRA, C., SANDIN, D. J., DEFANT, T. A., KENYON, R. V., AND HART, J. C. 1992. The cave-audio\nvisual experience virtual environment. Comm. ACM 35, 6, 65\u201372.\nDAHLEY, A., WISNESKI, C., AND ISHII, H. 1998. Water lamp and pinwheels: Ambient projection of\ndigital information into architectural space. InProceedings of the Conference on Human Factors\nin Computing Systems. Los Angeles, CA. 269\u2013270.\nDAVIS, J. AND CHEN, X. 2002. LumiPoint: Multi-user location-based interaction on large tiled\ndisplays. Elsevier Science 23, 5.\nEHN, P. AND KYNG, M. Cardboard computers: Mocking-it-up or hands-on the future. In Design at\nWork: Cooperative Design of Computer Systems. J. Greenbaum and M. Kyng, Eds. Lawrence\nErlbaum Ass., Hillsdale, CA, 169\u2013197.\nFOLEY, J. D., WALLACE, V. L., AND CHAN, P. 1984. The human factors of computer graphics interac-\ntion techniques. IEEE Comput. Graph. Appl. 4, 11, 13\u201348.\nGAVER, W. AND MARTIN, H. 2000. Alternatives: Exploring information appliances through concep-\ntual design proposals. In Proceedings of the Conference on Human Factors in Computing Systems.\nThe Hague, Netherlands. 209\u2013216.\nACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.\nExpected, Sensed, and Desired: Designing Sensing-Based Interaction \u2022 29\nGAVER, W., BEAVER, J., AND BENFORD S. 2003. Ambiguity as a resource for design. In Proceedings\nof the Conference on Human Factors in Computing Systems. For Lauderdale, FL. 233\u2013240.\nGAVER, W. 1992. The affordances of media spaces. In Proceedings of the 1992 ACM Conference on\nComputer-Supported Cooperative Work. Toronto, Canada. 1, 7\u201324.\nGAVER, W. AND DUNNE, A. 1999. Projected realities: Projected design for cultural effect. In Pro-\nceedings of the Conference on Human Factors in Computing Systems. Pittsburgh, PA. 600\u2013607.\nGHALI, A., BOUMI, S., BENFORD, S., GREEN, J., AND PRIDMORE, T. 2003. Visually tracked flashlights\nas interaction devices. In Proceedings of Interact. Zurich, Switzerland. 487\u2013494.\nGIBSON, J. J. 1977. The Theory of Affordances. In Perceiving, Acting and Knowing. R. E. Sahw &\nJ. Bransford, Eds. Lawrence Erlbaum Ass. Hillsdale, CA. 67\u201382.\nGREEN, J., SCHNA\u00a8DELBACH, H., KOLEVA, B., BENFORD S., PRIDMORE, T., MEDINA, K., HARRIS, E., AND SMITH,\nH. 2002. Camping in the digital wilderness: Tents and flashlights as interfaces to virtual\nworlds. In Proceedings of the Conference on Human Factors in Computing Systems Extended\nAbstracts. Minneapolis, MN. 780\u2013781.\nGRIFFITH, N. AND FERNSTROM, M. 1998. Litefoot\u2014a floor space for recording dance and controlling\nmedia. In Proceedings of the International Computer Music Conference. Ann Arbor, MI. 475\u2013481.\nHUGHES, J. A., RANDALL, D., AND SHAPIRO, D. 1992. Faltering from ethnography to design. In Pro-\nceedings of the 1992 ACM Conference on Computer-Supported Cooperative Work. Toronto, Canada.\n115\u2013122.\nISHII, H. AND ULLMER, B. 1997. Tangible bits: Towards seamless interfaces between people, bits\nand atoms. In Proceedings of the Conference on Human Factors in Computing Systems. Atlanta,\nGA. 234\u2013241.\nISHII, H., ORBANES, J., CHUN, B., AND PARADISO, J. 1999. PingPongPlus: Design of an athletic-\ntangible interface for computer-supported cooperative play. In Proceedings of the Conference on\nHuman Factors in Computing Systems. Pittsburgh, PA. 394\u2013401.\nJACOB, R. J. K., SIBERT, L. E., MCFARLANE, D. C., AND MULLEN, M. P. 1994. Integrality and separa-\nbility of input devices. ACM Trans. Comput.-Hum. Interact. 1, 1, 3\u201326.\nJOHN, B. AND KIERAS, D. 1996. The GOMS family of user interface analysis techniques: Compare\nand contrast. ACM Trans. Comput.-Hum. Interact. 3, 4, 320\u2013351.\nKLEMMER, S. R., THOMSEN, M., PHELPS-GOODMAN, E., LEE, R., AND LANDAY, J. A. 2002. Where do web\nsites come from?: Capturing and interacting with design theory. In Proceedings of the Conference\non Human Factors in Computing Systems. Minneapolis, MN. 1\u20138.\nKOLEVA, B., SCHNA\u00a8DELBACH, H, FLINTHAM, FRASER M., IZADI S., CHANDLER P., FOSTER M., BENFORD S.,\nGREENHALGH C. AND RODDEN T. 2002. The Augurscope: A mixed reality interface for outdoors.\nIn Proceedings of the Conference on Human Factors in Computing Systems. Minneapolis, MN.\n9\u201316.\nMACKENZIE, I. S., SOUKOREFF, R. W., AND PAL, C. 1997. A two-ball mouse affords three degrees of\nfreedom. In Proceedings of the Conference on Human Factors in Computing Systems Conference\nCompanion. New York, NY. 303\u2013304.\nMACKINLAY, J. D., CARD, S. K., AND ROBERTSON, G. G. 1990. A semantic analysis of the design space\nof input devices. Hum.-Comput. Interact. 5, 23, 145\u2013190.\nMUELLER, F., AGAMANOLIS, S., AND PICARD, R. 2003. Exertion interfaces: Sports over a distance\nfor social bonding and fun, In Proceedings of the Conference on Human Factors in Computing\nSystems. For Lauderdale. FL. 651\u2013568.\nMYERS, B., BHATNAGER, R., NICHOLS, J., PECK, C., KING D., MILLER, R., AND LONG, C. 2002. Interacting\nat a distance: Measuring the performance of laser pointers and other devices. In Proceedings of\nthe Conference on Human Factors in Computing Systems. Minneapolis, MN. 33\u201340.\nNORMAN, D. A. 1988. The Psychology of Everyday Things. Basic Books, New York, NY.\nNORMAN, D. A. 1999. Affordances, conventions and design. Interact. 6, 3, 38\u201343.\nOLSEN, D. AND NIELSEN, T. 2001. Laser pointer interaction. In Proceedings of the Conference on\nHuman Factors in Computing Systems. Seattle, WA. 17\u201322.\nPAULOS, E. AND CANNY, J. 1998. PRop: Personal roving presence. In Proceedings of the Conference\non Human Factors in Computing Systems. Los Angeles, CA. 296\u2013303.\nPOUPYREV, I., BILLINGHURST, M., WEGHORST, S., AND ICHIKAWA, T. 1996. Go go interaction technique:\nNon-linear mapping for direct manipulation in VR. In Proceedings of the 10th Annual ACM\nSymposium on User Interface Software and Technology. Banff, Canada. 79\u201380.\nACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.\n30 \u2022 S. Benford et al.\nREKIMOTO, J. AND SCIAMMARELLA, E. 2000. ToolStone: Effective use of the physical manipulation vo-\ncabularies of input devices. In Proceedings of the 13th Annual ACM Symposium on User Interface\nSoftware and Technology. San Diego, CA. 109\u2013117.\nSCHMIDT, A., STROHBACH, M., VAN LEERHOVEN, K., FRIDAY, A., AND GELLERSEN, H. 2002. Context\nacquisition based on load sensing. In Proceedings of the International Conference on Ubiquitous\nComputing. Goteborg, Sweden. 333\u2013350.\nSUDNOW, D. 1978. Ways of the Hand: The Organization of Improvised Conduct. Harvard Univer-\nsity Press, Cambridge, MA.\nULLMER, B., ISHII, H., AND GLAS, D. 1998. mediaBlocks: Physical containers, transports, and con-\ntrols for online media. In Proceedings of the 25th Annual Conference on Computer Graphics and\nInteractive Techniques. Orlando, FL. 379\u2013386.\nUNDERKOFFLER, J. AND ISHII, H. 1999. Urp: A luminous-tangible workbench for urban planning and\ndesign. In Proceedings of the Conference on Human Factors in Computing Systems. Pittsburgh,\nPA. 386\u2013393.\nReceived February 2003; revised August 2003, April 2004; accepted February 2004 by Shumin Zhai and Victoria\nBellotti\nACM Transactions on Computer-Human Interaction, Vol. 12, No. 1, March 2005.\n"}