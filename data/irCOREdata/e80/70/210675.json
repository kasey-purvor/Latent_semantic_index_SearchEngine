{"doi":"10.1111\/j.1600-0870.2008.00333.x","coreId":"210675","oai":"oai:eprints.lse.ac.uk:22220","identifiers":["oai:eprints.lse.ac.uk:22220","10.1111\/j.1600-0870.2008.00333.x"],"title":"From ensemble forecasts to predictive distribution functions","authors":["Br\u00f6cker, Jochen","Smith, Leonard A."],"enrichments":{"references":[{"id":17208795,"title":"A new interpretation of information rate.","authors":[],"date":"1956","doi":"10.1002\/j.1538-7305.1956.tb03809.x","raw":"Kelly, J. L. Jr. 1956. A new interpretation of information rate. Bell Syst. Tech. J. 35, 917\u2013926.","cites":null},{"id":17208790,"title":"Bayesian model averaging: a tutorial.","authors":[],"date":null,"doi":"10.1214\/ss\/1009212814","raw":"Bayesian model averaging: a tutorial. Stat. Sci. 14(4), 382\u2013417. Jewson,S.2003a.Comparingtheensemblemeanandtheensemblestandard deviation as inputs for probabilistic medium\u2013range temperature forecasts. arXiv:physics\/0310059v1 [physics.ao-ph].","cites":null},{"id":17208800,"title":"Combining dynamical and statistical ensembles.","authors":[],"date":"2003","doi":"10.3402\/tellusa.v55i1.12082","raw":"Roulston, M. S. and Smith, L. A. 2003. Combining dynamical and statistical ensembles. Tellus 55A, 16\u201330.","cites":null},{"id":17208822,"title":"Comparison of ensemble\u2013MOS methods using GFS reforecasts.","authors":[],"date":"2007","doi":"10.1175\/mwr3402.1","raw":"Wilks, D. S. and Hamill, T. M. 2007. Comparison of ensemble\u2013MOS methods using GFS reforecasts. Mon. Weather Rev. 6(14), 2379\u2013 2390. Tellus 60A (2008), 4","cites":null},{"id":17208796,"title":"Deterministic non-periodic \ufb02ow.","authors":[],"date":"1963","doi":null,"raw":"Lorenz, E. N. 1963. Deterministic non-periodic \ufb02ow. J. Atmos. Sci. 20, 130\u2013141.","cites":null},{"id":17208798,"title":"Economic value and skill Chapter 8. (eds","authors":[],"date":"2003","doi":null,"raw":"Richardson, D. S. 2003a. Economic value and skill Chapter 8. (eds I. T. Jolliffe and D. B. Stephenson). 165\u2013187.","cites":null},{"id":17208801,"title":"Evaluating probabilistic forecasts using information theory.","authors":[],"date":"2002","doi":"10.1175\/1520-0493(2002)130<1653:epfuit>2.0.co;2","raw":"Roulston, M. S. and Smith, L. A. 2002. Evaluating probabilistic forecasts using information theory. Mon. Weather Rev. 130, 1653\u2013 1660.","cites":null},{"id":17208784,"title":"Expected information as expected utility.","authors":[],"date":"1979","doi":"10.1214\/aos\/1176344689","raw":"Bernardo, J. M. 1979. Expected information as expected utility. Ann. Stat. 7(3), 686\u2013690.","cites":null},{"id":17208792,"title":"Forecast Veri\ufb01cation: A Practicioner\u2019s Guide in Athmospheric Science.","authors":[],"date":"2003","doi":"10.1002\/9781119960003","raw":"Jolliffe, I. T. and Stephenson, D. B. (eds) 2003. Forecast Veri\ufb01cation: A Practicioner\u2019s Guide in Athmospheric Science. John Wiley & Sons, Ltd, Chichester.","cites":null},{"id":17208818,"title":"Improvement of ensemble reliability with a new dressing kernel.","authors":[],"date":"2004","doi":"10.1256\/qj.04.120","raw":"Wang, X. and Bishop, C. H. 2004. Improvement of ensemble reliability with a new dressing kernel. Q. J. R. Meteorol. Soc. 131, 965\u2013986.","cites":null},{"id":17208793,"title":"Indistinguishable states I: the perfect model scenario.","authors":[],"date":"2001","doi":"10.1016\/s0167-2789(04)00182-4","raw":"Judd, K. and Smith, L. A. 2001. Indistinguishable states I: the perfect model scenario. Physica 151, 125\u2013141.","cites":null},{"id":17208789,"title":"Interpretation of rank histograms for verifying ensemble forecasts.","authors":[],"date":"2001","doi":"10.1175\/1520-0493(2001)129<0550:iorhfv>2.0.co;2","raw":"Hamill, T. M. 2001. Interpretation of rank histograms for verifying ensemble forecasts. Mon. Weather Rev. 6(14), 550\u2013560.","cites":null},{"id":17208797,"title":"Introduction to the Theory of Statistics.","authors":[],"date":"1974","doi":"10.2307\/2286195","raw":"Mood, A. M., Graybill, F. A. and Boes, D. C. 1974. Introduction to the Theory of Statistics. McGraw-Hill Series in Probability and Statistics, McGraw-Hill, New York.","cites":null},{"id":17208791,"title":"Moment based methods for ensemble assessment and calibration.","authors":[],"date":"2003","doi":null,"raw":"Jewson, S. 2003b. Moment based methods for ensemble assessment and calibration. arXiv:physics\/0309042v1 [physics.ao-ph]. Johnson,R.A.andWichern,D.W.1992.AppliedMultivariateStatistical Analysis 3rd Edition. Prentice Hall, Englewood Cliffs, NJ.","cites":null},{"id":17208786,"title":"Practical Optimization.","authors":[],"date":"1982","doi":"10.2307\/3616583","raw":"Gill, P. E., Murray, W. and Wright, M. H. 1982. Practical Optimization. Academic Press, London.","cites":null},{"id":17208805,"title":"Predictability and chaos. In: Encyclopedia of Atmospheric Sciences.","authors":[],"date":"2002","doi":"10.1016\/b0-12-227090-8\/00323-7","raw":"Smith, L. A. 2002. Predictability and chaos. In: Encyclopedia of Atmospheric Sciences. (eds J. Holton, J. Pyle and J. Curry). Academic Press, 1777\u20131785. Smith,L.A.2007.AVeryShortIntroductiontoChaos.OxfordUniversity Press, Oxford.","cites":null},{"id":17208799,"title":"Predictability and Economic Value.","authors":[],"date":"2003","doi":"10.1017\/cbo9780511617652.026","raw":"Richardson, D. S. 2003b. Predictability and Economic Value. Technical report, European Centre for Medium Range Weather Forecast.","cites":null},{"id":17208788,"title":"Rational decisions.","authors":[],"date":"1952","doi":"10.1007\/978-1-4612-0919-5_23","raw":"Good, I. J. 1952. Rational decisions. J. R. Stat. Soc. XIV(1), 107\u2013114.","cites":null},{"id":17208785,"title":"Scoring probabilistic forecasts: the importance of being proper.","authors":[],"date":"2007","doi":"10.1175\/waf966.1","raw":"Br\u00a8 ocker, J. and Smith, L. A. 2007. Scoring probabilistic forecasts: the importance of being proper. Weather Forecast. 22(2), 382\u2013388.","cites":null},{"id":17208820,"title":"Smoothing forecast ensembles with \ufb01tted probability distributions.","authors":[],"date":"2002","doi":"10.1256\/qj.01.215","raw":"Wilks, D. S. 2002. Smoothing forecast ensembles with \ufb01tted probability distributions. Q. J. R. Meteorol. Soc. 128, 2821\u20132836.","cites":null},{"id":17208812,"title":"Statistical Learning Theory.","authors":[],"date":"1998","doi":"10.1007\/978-1-4757-3264-1","raw":"Vapnik, V. N. 1998. Statistical Learning Theory. John Wiley & Sons, Inc., New York.","cites":null},{"id":17208787,"title":"Strictly proper scoring rules, prediction, and estimation.","authors":[],"date":"2007","doi":"10.1198\/016214506000001437","raw":"Gneiting, T. and Raftery, A. 2007. Strictly proper scoring rules, prediction, and estimation. J. Am. Stat. Assoc. 102, 359\u2013378. Gneiting,T.,Raftery,A.,Westveld,A.H.IIIandGoldmann,T.2005.Calibrated probabilistic forecasting using ensemble model output statistics and minimum CRPS estimation. Mon. Weather Rev. 133, 1098\u2013 1118.","cites":null},{"id":17208819,"title":"The Classical Groups 2nd Edition. Princeton Mathematical Series.","authors":[],"date":"1946","doi":null,"raw":"Weyl, H. 1946. The Classical Groups 2nd Edition. Princeton Mathematical Series. Princeton University Press, Princeton, NJ.","cites":null},{"id":17208803,"title":"The maintenance of uncertainty. In:","authors":[],"date":"1997","doi":null,"raw":"Smith, L. A. 1997. The maintenance of uncertainty. In: Proceedings of the International School of Physics \u2018Enrico Fermi\u2019, Vol. CXXXIII, Societ` a Italiana di Fisica, Bologna, Italy, 177\u2013246.","cites":null},{"id":17208810,"title":"The outlook: scattered showers.","authors":[],"date":"1988","doi":"10.1175\/1520-0477(1988)069<0368:toss>2.0.co;2","raw":"Tennekes, H. 1988. The outlook: scattered showers. Bull. Am. Meteorol. Soc. 69(4), 368\u2013372.","cites":null},{"id":17208802,"title":"Using medium range weather forecasts to improve the value of wind energy producton.","authors":[],"date":"2003","doi":"10.1016\/s0960-1481(02)00054-x","raw":"Tellus 60A (2008), 4678 J. BR\u00a8 OCKER AND L. A. SMITH Roulston, M. S., Kaplan, D. T., Hardenberg, J. and Smith, L. A. 2003. Using medium range weather forecasts to improve the value of wind energy producton. Renew. Energy 28, 585\u2013602. Selten,R.1998.Axiomaticcharacterisationofthequadraticscoringrule.","cites":null},{"id":17208808,"title":"Using weather ensemble predictions in electricity demand forecasting.","authors":[],"date":"2003","doi":"10.1016\/s0169-2070(01)00123-6","raw":"Taylor, J. W. and Buizza, R. 2003. Using weather ensemble predictions in electricity demand forecasting. Int. J. Forecast. 19, 57\u201370.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2008-08","abstract":"The translation of an ensemble of model runs into a probability distribution is a common task in model-based prediction. Common methods for such ensemble interpretations proceed as if verification and ensemble were draws from the same underlying distribution, an assumption not viable for most, if any, real world ensembles. An alternative is to consider an ensemble as merely a source of information rather than the possible scenarios of reality. This approach, which looks for maps between ensembles and probabilistic distributions, is investigated and extended. Common methods are revisited, and an improvement to standard kernel dressing, called 'affine kernel dressing' (AKD), is introduced. AKD assumes an affine mapping between ensemble and verification, typically not acting on individual ensemble members but on the entire ensemble as a whole, the parameters of this mapping are determined in parallel with the other dressing parameters, including a weight assigned to the unconditioned (climatological) distribution. These amendments to standard kernel dressing, albeit simple, can improve performance significantly and are shown to be appropriate for both overdispersive and underdispersive ensembles, unlike standard kernel dressing which exacerbates over dispersion. Studies are presented using operational numerical weather predictions for two locations and data from the Lorenz63 system, demonstrating both effectiveness given operational constraints and statistical significance given a large sample","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/210675.pdf","fullTextIdentifier":"http:\/\/eprints.lse.ac.uk\/22220\/1\/From%20ensemble%20forecasts%20to%20predictive%20distribution%20%28lsero%29.pdf","pdfHashValue":"ebd130c47aa0e2de7d58e05bac9719cab43fb809","publisher":"Blackwell Publishing on behalf of the International Meteorological Institute in Stockholm","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lse.ac.uk:22220<\/identifier><datestamp>\n      2014-03-13T12:32:37Z<\/datestamp><setSpec>\n      74797065733D43454E54524553:4C53455F52435F3136<\/setSpec><setSpec>\n      74797065733D4445505453:4C53452D5354<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lse.ac.uk\/22220\/<\/dc:relation><dc:title>\n        From ensemble forecasts to predictive distribution functions<\/dc:title><dc:creator>\n        Br\u00f6cker, Jochen<\/dc:creator><dc:creator>\n        Smith, Leonard A.<\/dc:creator><dc:subject>\n        GE Environmental Sciences<\/dc:subject><dc:subject>\n        QA Mathematics<\/dc:subject><dc:subject>\n        QC Physics<\/dc:subject><dc:description>\n        The translation of an ensemble of model runs into a probability distribution is a common task in model-based prediction. Common methods for such ensemble interpretations proceed as if verification and ensemble were draws from the same underlying distribution, an assumption not viable for most, if any, real world ensembles. An alternative is to consider an ensemble as merely a source of information rather than the possible scenarios of reality. This approach, which looks for maps between ensembles and probabilistic distributions, is investigated and extended. Common methods are revisited, and an improvement to standard kernel dressing, called 'affine kernel dressing' (AKD), is introduced. AKD assumes an affine mapping between ensemble and verification, typically not acting on individual ensemble members but on the entire ensemble as a whole, the parameters of this mapping are determined in parallel with the other dressing parameters, including a weight assigned to the unconditioned (climatological) distribution. These amendments to standard kernel dressing, albeit simple, can improve performance significantly and are shown to be appropriate for both overdispersive and underdispersive ensembles, unlike standard kernel dressing which exacerbates over dispersion. Studies are presented using operational numerical weather predictions for two locations and data from the Lorenz63 system, demonstrating both effectiveness given operational constraints and statistical significance given a large sample.<\/dc:description><dc:publisher>\n        Blackwell Publishing on behalf of the International Meteorological Institute in Stockholm<\/dc:publisher><dc:date>\n        2008-08<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc<\/dc:rights><dc:identifier>\n        http:\/\/eprints.lse.ac.uk\/22220\/1\/From%20ensemble%20forecasts%20to%20predictive%20distribution%20%28lsero%29.pdf<\/dc:identifier><dc:identifier>\n          Br\u00f6cker, Jochen and Smith, Leonard A.  (2008) From ensemble forecasts to predictive distribution functions.  Tellus Series A: Dynamic Meteorology and Oceanography, 60 (4).  pp. 663-678.  ISSN 0280-6495     <\/dc:identifier><dc:relation>\n        http:\/\/onlinelibrary.wiley.com\/journal\/10.1111\/%28ISSN%291600-0870<\/dc:relation><dc:relation>\n        10.1111\/j.1600-0870.2008.00333.x<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lse.ac.uk\/22220\/","http:\/\/onlinelibrary.wiley.com\/journal\/10.1111\/%28ISSN%291600-0870","10.1111\/j.1600-0870.2008.00333.x"],"year":2008,"topics":["GE Environmental Sciences","QA Mathematics","QC Physics"],"subject":["Article","PeerReviewed"],"fullText":"  \nJochen Br\u00f6cker, Leonard A. Smith \nFrom ensemble forecasts to predictive \ndistribution functions  \n \nArticle (Published version) \n(Refereed) \n \n \n \n \nOriginal citation: \nBr\u00f6cker, Jochen and Smith, Leonard A. (2008) From ensemble forecasts to predictive \ndistribution functions. Tellus series A: dynamic meteorology and oceanography, 60 (4). pp. 663-\n678. \nDOI: 10.1111\/j.1600-0870.2008.00333.x  \n \n\u00a9 2008 The Authors. Creative Commons Attribution-Noncommercial 3.0 Unported License \n \nThis version available at: http:\/\/eprints.lse.ac.uk\/22220\/ \nAvailable in LSE Research Online: October 2012 \n \nLSE has developed LSE Research Online so that users may access research output of the \nSchool. Copyright \u00a9 and Moral Rights for the papers on this site are retained by the individual \nauthors and\/or other copyright owners. Users may download and\/or print one copy of any \narticle(s) in LSE Research Online to facilitate their private study or for non-commercial research. \nYou may not engage in further distribution of the material or use it for any profit-making activities \nor any commercial gain. You may freely distribute the URL (http:\/\/eprints.lse.ac.uk) of the LSE \nResearch Online website.  \nTellus (2008), 60A, 663\u2013678 C\u00a9 2008 The Authors\nJournal compilation C\u00a9 2008 Blackwell Munksgaard\nPrinted in Singapore. All rights reserved\nT E L L U S\nFrom ensemble forecasts to predictive distribution\nfunctions\nBy JOCHEN BR \u00a8OCKER 1\u2217 and LEONARD A. SMITH 2, 1Max\u2013Planck\u2013Institut fu\u00a8r Physik komplexer\nSysteme, 01187 Dresden, Germany; 2Centre for the Analysis of Time Series, London School of Economics,\nLondon WC2A 2AE, United Kingdom\n(Manuscript received 26 February 2007; in final form 25 March 2008)\nABSTRACT\nThe translation of an ensemble of model runs into a probability distribution is a common task in model-based prediction.\nCommon methods for such ensemble interpretations proceed as if verification and ensemble were draws from the same\nunderlying distribution, an assumption not viable for most, if any, real world ensembles. An alternative is to consider an\nensemble as merely a source of information rather than the possible scenarios of reality. This approach, which looks for\nmaps between ensembles and probabilistic distributions, is investigated and extended. Common methods are revisited,\nand an improvement to standard kernel dressing, called \u2018affine kernel dressing\u2019 (AKD), is introduced. AKD assumes\nan affine mapping between ensemble and verification, typically not acting on individual ensemble members but on the\nentire ensemble as a whole, the parameters of this mapping are determined in parallel with the other dressing parameters,\nincluding a weight assigned to the unconditioned (climatological) distribution. These amendments to standard kernel\ndressing, albeit simple, can improve performance significantly and are shown to be appropriate for both overdispersive\nand underdispersive ensembles, unlike standard kernel dressing which exacerbates over dispersion. Studies are presented\nusing operational numerical weather predictions for two locations and data from the Lorenz63 system, demonstrating\nboth effectiveness given operational constraints and statistical significance given a large sample.\n1. Introduction\nEnsemble forecasts consist of several simulations of the future\nevolution of the dynamical process under concern (see e.g. Toth\net al., 2003). In principle, ensemble forecasts allow us to convey\nadditional information on forecast uncertainty (Tennekes, 1988),\nwhich is invaluable for informed decision making (Taylor and\nBuizza, 2003; Richardson, 2003a,b; Roulston et al., 2003). In\nboth scientific studies as well as practical applications, distribu-\ntion functions are often more convenient to manipulate than a\nset of point values. The question then arises how to transform an\nensemble into such a distribution function, a task often referred\nto as statistical post-processing of ensemble forecasts in Raftery\net al. (2005); Wilks (2006); Wilks and Hamill (2007) or ensem-\nble interpretation in Jewson (2003a), the latter term being used\nin this paper. Any particular method for interpreting ensembles\nwill be referred to as an ensemble interpretation method (other\nauthors, e.g. Wilks, 2006; Wilks and Hamill, 2007, use the term\nensemble MOS method).\n\u2217Corresponding author.\ne-mail: broecker@pks.mpg.de\nDOI: 10.1111\/j.1600-0870.2008.00333.x\nEnsemble interpretation methods generally differ due to the\ndifferent families of distribution functions employed in building\nthe ensemble interpretation and the way it is actually built. Both\naspects are discussed in this paper. As to the different families\nof distribution functions, two particular approaches are consid-\nered here. The first one is referred to as kernel dressing and\nconsists of replacing individual ensemble members by kernel\nfunctions. In the second approach, the ensemble is replaced by\na parametrized distribution function, where the parameters of\nthe distribution function have to be represented as functions\nof the original ensemble. This approach will be referred to as\ndistribution fit or DF interpretation.1,2 Both approaches typi-\ncally involve parameters which have to be determined.\nApproaches to build the ensemble interpretation method differ\nin what the ensemble is taken to represent. In the simplest case,\n1In fact, kernel dressing and DF interpretation are not really distinct, as\na sum of kernel functions can be interpreted as a special family of distri-\nbution functions, the centres of the kernel being part of the parameters.\nBut when speaking of DF interpretations, we usually have somewhat\nmore common families of distributions in mind, like Gaussian, Weibull\nor exponential distributions.\n2The term distribution fitting is used by, for example, Wilks (2006).\nTellus 60A (2008), 4 663\nP U B L I S H E D  B Y  T H E  I N T E R N A T I O N A L  M E T E O R O L O G I C A L  I N S T I T U T E  I N  S T O C K H O L M\nSERIES A\nDYNAMIC \nMETEOROLOGY\nAND OCEANOGRAPHY\n664 J . BR \u00a8OCKER AND L. A. SMITH\nthe ensemble is considered a collection of equally likely scenar-\nios of reality, drawn from the same distribution as the verification\n(a perfect ensemble). This approach suggests that ensemble in-\nterpretation is accomplished by approximating this underlying\ndistribution, for example, by parametric estimation techniques\n(see e.g. Mood et al., 1974, Chapter VII) or kernel estimates\n(Silverman, 1986).\nAlthough ensembles have been used to great effect even when\nassumed to be perfect (Wilks, 2002), we argue that a different\nparadigm is available which naturally includes the case where\nensemble members and verifications do not share the same dis-\ntribution. Nor need we assume that any one of the models in hand\nis true in any sense. Here, we are interested in a distribution of\nthe verification given the information contained in the ensemble.\nA formalism for constructing such distributions could take into\naccount that ensembles and corresponding verifications are not\ndraws from the same or at least fairly similar distributions, but\nentirely different ones.\nThis paradigm defines ensemble interpretation in a much\nbroader sense than just interpolating a distribution function un-\nderlying the ensemble. In fact, there is no need to assume that\nensembles are draws from distributions at all. As a simple exam-\nple, it will be demonstrated that a mere linear transformation of\nthe ensemble already brings about a significant improvement in\na predictive performance of kernel dressing. Finding this linear\ntransformation will be neither a preliminary nor a subsequent\nstep to dressing, but integral part of it. Inasmuch as dressing in-\nvolves finding unspecified parameters of the dressing method, we\nconsider dressing a generalization of statistical learning (Hastie\net al., 2001).\nThe performance of forecast distributions is evaluated using\nscoring rules (Selten, 1998; Gneiting and Raftery, 2007). Some\nscores can be applied to the raw ensemble itself (e.g. continuous\nranked probability score (CRPS) score Gneiting and Raftery,\n2007), while others can be applied to smoother probability\nassignments only, as provided, for example, by ensemble in-\nterpretation methods. Thus, ensemble interpretations render the\napplication of those scores to ensemble forecasts feasible. In this\npaper, we focus attention on the Ignorance score (Good, 1952;\nRoulston and Smith, 2002). Strengths and weaknesses of this\nscore are clarified as well.\nTechniques for ensemble interpretation are the subject of Sec-\ntion 2, where state-of-the-art ensemble interpretation methods\nare revisited and a new affine kernel dressing (AKD) method is\npresented. A comparison of these ensemble interpretation meth-\nods in terms of their mathematical properties is subject to Section\n3. Scoring rules are discussed briefly in Section 4, along with the\ndetails of how to optimize the performance of ensemble inter-\npretation methods, while questions of robust estimation and the\nvalue of blending in the climatological distribution are discussed\nin Section 5. In Section 6, we apply the ensemble interpretation\ntechniques to temperature forecasts at London Heathrow and\nHeligoland (German Bight) as well as to the Lorenz63 system.\nThe AKD method is shown to be capable of dealing with the\nimperfect ensembles more adequately than common ensemble\ninterpretation methods in these cases. Furthermore, the Lorenz63\nexample demonstrates the insufficiency of Gaussian DF\ninterpretations.\n2. Interpreting Ensemble Forecasts\nThis section introduces a new dressing method referred to as\nAKD in the context of three well-known methods, namely Gaus-\nsian DF interpretation (GDF), standard kernel dressing methods\n(SKD) and Bayesian model averaging (BMA) (see e.g. Hoeting\net al., 1999; Roulston and Smith, 2003; Raftery et al., 2005;\nWang and Bishop, 2004; Wilks, 2006). We use the following\nnotation throughout the paper. By\nx = [x1, . . . , xd ], (1)\nwe denote an ensemble with d ensemble members. Typically,\ndifferent ensemble members have different dynamical and statis-\ntical properties, depending on the ensemble generation scheme.\nIn this paper though, we treat all ensemble members equally,\nor in other words, the ensemble interpretation methods consid-\nered in this paper do not depend on the ordering of the ensemble\nmembers. If some of the xi need to be treated differently than\nothers, for example if they come from different models3, a super-\nscript x(J )i should be used. This case is to be distinguished from\nan ensemble in a higher dimensional space. Neither multimodel\nensembles nor ensembles in high-dimensional spaces are consid-\nered in this paper. In general, the ensemble is a function of time,\nwhich we denote by x(t), while we write y(t) for the verifica-\ntion, that is, the quantity to be forecast. The number of ensemble\nmembers d might even change over time. The ensemble has a\nmean and a variance, which are defined as\nm(x) = 1\nd\n\u2211\ni\nxi , (2)\nv(x) = 1\nd\n\u2211\ni\n[xi \u2212 m(x)]2, (3)\nrespectively. Finally, p(y; x, \u03b8 ) is a probability density function\nderived from the ensemble x, where \u03b8 denotes further parameters.\nIn other words, p(y; x, \u03b8 ) denotes the interpreted ensemble as a\nprobability density function, given the original ensemble. In fact,\na probability density function need not be the goal, as will be\ndiscussed at the end of Section 4.\nWe first consider Gaussian DF interpretations (GDF) which\ncan be written as\np(y; x, \u03b8 ) := 1\u221a\n\u03bd\nK\n(\ny \u2212 \u03bc\u221a\n\u03bd\n)\n, (4)\n3The unperturbed ensemble member (the \u2018control\u2019) could be treated dif-\nferently, which we will not do in this paper though.\nTellus 60A (2008), 4\nFROM ENSEMBLE FORECASTS TO PREDICTIVE DISTRIBUTION FUNCTIONS 665\nwhere K is a standard Gaussian density. Depending on the prob-\nlem, other distributions can be more appropriate, for example\nWeybull or \u0004 distributions. The parameters \u03bc and\n\u221a\n\u03bd are the\nmean and the standard deviation of the distribution, respectively.\nSetting \u03bc and\n\u221a\n\u03bd equal to the mean and the standard deviation\nof the ensemble is a possible choice (Wilks, 2002), but by do-\ning so we would approximate the distribution of the ensemble,\nrather than the distribution of the verification given the ensem-\nble, which is our goal. A conceptually different approach is to\ndetermine\n\u221a\n\u03bd and \u03bc by functions of the ensemble and some free\nparameters \u03b8 , so that the DF interpretation shows good forecast\nperformance. A variant of Gaussian DF interpretation following\nthis philosophy was presented by Jewson (2003a,b), who sug-\ngested a mean \u03bc and standard deviation\n\u221a\n\u03bd depending on the\nraw ensemble x as follows\n\u03bc = r1 + r2 \u00b7 m(x), (5)\n\u221a\n\u03bd = s1 + s2 \u00b7\n\u221a\nv(x). (6)\nThus,\n\u221a\n\u03bd and \u03bc are determined by linear functions of the stan-\ndard deviations and the mean of the ensemble, respectively. A\nvery similar interpretation method was suggested by Gneiting\net al. (2005), who replaced eq. (6) by \u03bd = s1 + s2 \u00b7 v(x). The\nparameters \u03b8 = [r1, r2, s1, s2] are free parameters, for which\nr1 = 0, r2 = 1, s1 = 0, s2 = 1 are reasonable initial choices. The\nlinear relationships in eqs. (5) and (6) might be unable to cope\nwith ensembles which are grossly different from the verification.\nThe key insight of Jewson (2003a,b) and Gneiting et al. (2005) is\nthat the parameters r1, r2, s1, s2 have to be determined according\nto forecast performance, rather than to represent the distribution\nof the ensemble members. Determining the parameters r1, r2,\ns1, s2 thus hinges on what counts as \u2018good performance\u2019. Both\nthe issue of finding the parameters as well as precise definitions\nof performance will be discussed in Section 4. This approach\nis distinctly different from, for example, Wilks (2002), where\nthe probability distribution is fitted to the ensemble, without any\nreference to the verification.\nAn obvious shortcoming of Gaussian DF interpretation is that\nthe shape of the dressed ensemble is invariably Gaussian. A\nmore versatile method is provided by kernel dressing. Various\nversions of kernel dressing have been considered in the literature\n(Roulston and Smith, 2003; Raftery et al., 2005; Wang and\nBishop, 2004; Wilks, 2006). A general way to present the kernel\ndressing approach reads as follows\np(y; x, \u03b8 ) := 1\nd\u03c3\n\u2211\ni\nK\n(\ny \u2212 axi \u2212 \u03c9\n\u03c3\n)\n. (7)\nHence, a kernel-dressed ensemble is a sum of bumps, with one\nbump replacing each ensemble member. The shape of the bumps\nis determined by the kernel K. Each bump is centred at axi + \u03c9,\nwhere xi is the ith ensemble member. Thus, a scales the ensemble,\nwhile \u03c9 acts as an offset. The width of each bump is determined\nby the bandwidth \u03c3 . As with GDF, a, \u03c3 and \u03c9 are quantities that\nmight depend on the ensemble and on a parameter vector \u03b8 in\na way we have to specify. Note that the bandwidth \u03c3 has to be\npositive. For simplicity, throughout this paper, the kernel K will\nbe a standard Gaussian density\nK (\u03be ) := 1\u221a\n2\u03c0\nexp\n(\n\u22121\n2\n\u03be 2\n)\n. (8)\nHence, kernel dressing results in a sum of d Gaussians, in contrast\nto GDF, which gives a single Gaussian. Possible advantages of\nusing different kernels with finite support like the Epanechnikov\nkernel (Silverman, 1986) are discussed in Section 5.\nA wide variety of different kernels have been employed in\nsimilar or related circumstances (Roulston and Smith, 2003;\nSilverman, 1986). All results below apply to kernels which are\nnormalized and positive, and furthermore have mean zero and\nunit variance.4 We remark that the Gaussian kernel employed\nhere is furthermore symmetric, but this property is not used in\nthis paper.\nFrom the properties of the kernel immediately follows that\nthe ensemble interpretation p(y; x, \u03b8 ) in eq. (7) is a positive\nand normalized probability density function. It is illustrative to\ncompute the mean\n\u03bc :=\n\u222b\ny p(y; . . .) dy (9)\nand the variance\n\u03bd :=\n\u222b\n(y \u2212 \u03bc)2 p(y; . . .) dy (10)\nof the ensemble interpretation (eq. 7). We will now prove the\nfollowing two identities on \u03bc and \u03bd, which we will need later:\n\u03bc = \u03c9 + a 1\nd\n\u2211\ni\nxi = \u03c9 + am(x), (11)\n\u03bd = \u03c3 2 + a2 1\nd\n\u2211\ni\n[xi \u2212 m(x)]2 = \u03c3 2 + a2v(x). (12)\nThe first eq. (11) states that the mean value of the ensemble inter-\npretation is equal to the mean value m(x) of the ensemble, scaled\nby the parameter a and shifted by the parameter \u03c9. The second\neq. (12) states that the variance of the ensemble interpretation is\nlikewise equal to the variance v(x) of the ensemble, scaled by the\nparameter a2 and shifted by the parameter \u03c3 2. Note, however,\nthat a, \u03c3 and \u03c9 might depend on the ensemble as well, as men-\ntioned above. To prove eq. (11), note that by substituting from\n4As long as the kernel has a mean m and a variance s at all, we can al-\nways obtain mean zero and unit variance by using the kernel 1\u221a\ns\nK ( \u03be\u2212m\u221a\ns\n)\ninstead of K. The Cauchy kernel provides an example of a kernel having\nneither a mean nor a variance.\nTellus 60A (2008), 4\n666 J . BR \u00a8OCKER AND L. A. SMITH\neq. (7) into eq. (9), we get\u222b\ny p(y; . . .) dy\n= 1\nd\u03c3\n\u2211\ni\n\u222b\nyK\n(\ny \u2212 axi \u2212 \u03c9\n\u03c3\n)\ndy\n= 1\nd\n\u2211\ni\n\u222b\n(z + axi + \u03c9)K (z) dz\n= a 1\nd\n\u2211\ni\nxi + \u03c9\n= \u03c9 + am(x),\nwhere we first substituted z for y\u2212axi \u2212\u03c9\n\u03c3\n, then used that the kernel\nis normalized and has zero mean and finally employed the def-\ninition eq. (2) of the ensemble mean. To derive eq. (12), again\nsubstituting from eq. (7), we get along similar lines\u222b\ny2 p(y; . . .) dy\n= 1\nd\u03c3\n\u2211\ni\n\u222b\ny2 K ( y \u2212 axi \u2212 \u03c9\n\u03c3\n) dy\n= 1\nd\n\u2211\ni\n\u03c3 2 + (axi + \u03c9)2\n= \u03c3 2 + 1\nd\n\u2211\ni\n(axi + \u03c9)2. (13)\nFurthermore, we expand\n1\nd\n\u2211\ni\n[axi \u2212 am(x)]2\n= 1\nd\n\u2211\ni\n(axi + \u03c9 \u2212 am(x) \u2212 \u03c9)2\n= 1\nd\n\u2211\ni\n(axi + \u03c9)2 \u2212 [\u03c9 + am(x)]2. (14)\nNow employing eqs. (13), (11) and then (14), we get\n\u03bd =\n\u222b\ny2 p(y; . . .) dy \u2212 \u03bc2\n= \u03c3 2 + 1\nd\n\u2211\ni\n(axi + \u03c9)2 \u2212 [\u03c9 + am(x)]2\n= \u03c3 2 + a2 1\nd\n\u2211\ni\n[xi \u2212 m(x)]2,\nwhich establishes eq. (12). For constant a, \u03c3 , \u03c9, these equations\nfollow from eqs. (4) and (7) in Raftery et al. (2005). All these\nidentities are special instances of the well-known fact that the\noverall variance of a model which is itself an average is given\nby the average of the individual variances plus the dispersion of\nthe models.\nThe kernel dressing methods discussed in this paper (and in\nfact most other kernel dressing methods we know of) differ only\nin how the parameters \u03c3 , \u03c9 and a are determined as functions of\nx and \u03b8 . For AKD, \u03c3 and \u03c9 are set to\n\u03c9 = r1 + r2 \u00b7 m(x), (15)\n\u03c3 2 = h2S \u00b7 [s1 + s2 \u00b7 a2v(x)]. (16)\nHere, hS is Silverman\u2019s factor (see Silverman, 1986)\nhS = 0.5 \u00b7 [4\/(3d)]1\/5,\nthe meaning of which will be explained below. Substituting\neq. (15) for \u03c9 in eq. (11) and eq. (16) for \u03c3 in (12), we get\nthe relations:\n\u03bc = r1 + (a + r2) \u00b7 m(x), (17)\n\u03bd = h2Ss1 + a2\n(\nh2Ss2 + 1\n) \u00b7 v(x). (18)\nThe dressing approach as presented in eqs. (15) and (16) leaves\nthe free parameter vector \u03b8 := [r1, r2, s1, s2, a] to be determined.\nThere is a different way to write eqs. (15) and (16) which reveals\nmore about the structure of AKD and the role of Silverman\u2019s\nfactor. Combining eqs. (15) and (16), it is easy to see that the\ndressed ensemble eq. (7) reads as\np(y; x, \u03b8 ) := 1\nd\u03c3\n\u2211\ni\nK\n(\ny \u2212 zi\n\u03c3\n)\n, (19)\nwhere\nzi = axi + r2m(x) + r1, (20)\n\u03c3 2 = h2S \u00b7 [s1 + s2 \u00b7 v(z)]. (21)\nThe relations (19)\u2013(21) allow for the interpretation of AKD as\ndressing the ensemble z, which is obtained from the original\nensemble x through the transformation in eq. (20). This trans-\nformation will henceforth be referred to as an affine ensemble\ntransform. Hence, also the name AKD.5 Further possible gener-\nalizations of dressing could be obtained by replacing the affine\nensemble transform (i.e. eq. 20) by more general ensemble trans-\nforms, which are discussed in Appendix B. Note that the affine\nensemble transform acts on the ensemble as a whole and cannot\nbe represented as a function acting on each ensemble member\nindividually. We stress that the ensemble transformation (eq. 20)\nas well as the dressing (eq. 19) are both integral parts of the entire\nmethod, and they should not be considered as separate steps. In\nother words, the parameters in eqs. (19) and (20) will generally\ndepend on each other.\nFrom the theory of kernel density estimates (Silverman, 1986),\nwe take the ansatz eq. (21) for the bandwith \u03c3 . In the highly\nidealized situation that the transformed ensemble z is Gaussian\nand perfect, \u03c3 2 = h2S \u00b7 v(z) is a close to optimal choice for the\nbandwidth. Although we do not assume z to be either Gaussian\nor perfect, using Silverman\u2019s factor conveniently scales s1 and\ns2 to ranges around 1.\nIn Section 6, AKD will be compared to Gaussian dressing as\nwell as a more standard version of kernel dressing, henceforth\nreferred to as SKD, which obtains by setting a = 1, r2 = 0 and\n5Which should, in fact, be \u2018affine ensemble transform kernel dressing\u2019.\nTellus 60A (2008), 4\nFROM ENSEMBLE FORECASTS TO PREDICTIVE DISTRIBUTION FUNCTIONS 667\ns1 = 0. That is, standard kernel dressing allows for a fixed offset\nr1 to all ensemble members as well as a bandwidth correction\nfactor of s2.\nAnother special case emerges by setting r2 = 0, s2 = 0. This\nensemble interpretation method was studied by Wilks (2006),\nwho introduced it as a special case of Bayesian model averaging\n(BMA, Raftery et al., 2005). As was pointed out in Wilks (2006),\nthe general BMA technique might be justified if the ensemble\nmembers are expected to have significantly different error statis-\ntics, as for example in ensembles of different numerical weather\nmodels. For the initial condition ensembles considered below,\nhowever, the ensemble members are expected to have quite sim-\nilar statistics, whence a general BMA approach would be overly\ncomplex.\n3. Properties OF AKD, SKD and GDF\nIn this section, a brief look is taken upon the advantages and\nshortcomings to be expected of the four dressing methods pre-\nsented. It is plausible that any kernel dressing is better than Gaus-\nsian dressing if (but not only if) the ensemble x(t) and the ver-\nification y(t) are independent draws from the same underlying\ndistribution (perfect ensemble) and the ensemble is sufficiently\nlarge. The reason is that with increasing ensemble size (and suit-\nable choice of the bandwidth \u03c3 ), the kernel-dressed ensemble\nwill approach the underlying density. Although we did not ven-\nture to find a proof, analogy to density estimation problems (Sil-\nverman, 1986) suggest that a necessary criterion would seem to\nbe \u03c3 (d) \u2192 0 if the ensemble size d goes to infinity, but slow\nenough so that still d \u00b7 \u03c3 (d) \u2192 \u221e, that is \u03c3 (d) shrinks slower\nthan d. This is expected, for example, in best member dressing\n(Roulston and Smith, 2003). Hence, we would expect that, if the\nensemble is perfect, yet not Gaussian but, for example, bimodal\n(Smith, 1997, 2002), kernel dressing will eventually outperform\nGaussian dressing. Even if the perfect ensemble is actually a\ndraw from a Gaussian, it is not clear that Gaussian dressing is\nbetter than kernel dressing, since the parameters \u03c9 and \u03c3 in eqs.\n(5) and (6) still need to be estimated from the ensemble. It can be\nshown (J. Penzer, personal communication, 2006) that maximum\nlikelihood estimates of these parameters are suboptimal, and a\nt-distribution should be used rather than a Gaussian (Johnson\nand Wichern, 1992). This effect is essentially due to the small\nensemble size.\nGaussian dressing, on the other hand, is expected to beat stan-\ndard kernel dressing when the ensemble x(t) is reasonably Gaus-\nsian but overdispersive, or in other words, the ensemble mem-\nbers are further away from each other than from the verification.\nSince \u03c3 2 is positive, eq. (12) reflects the basic result (see e.g.\nWilks, 2006) that the variance of the standard kernel-dressed en-\nsemble (i.e. if a = 1) is always larger than the variance of the\nraw ensemble, no matter how \u03c3 is determined. AKD, in con-\ntrast, allows for the variance of the dressed ensemble to be a\nlinear function of the variance of the raw ensemble, a feature\nit shares with Gaussian dressing and BMA. In operational nu-\nmerical weather prediction, the ensemble spread is typically too\nsmall on average, leading to convex Talagrand diagrams (Wilks,\n1995; Hamill, 2001). Nevertheless, eq. (12) is a relation for each\nindividual ensemble. Independent of whether the ensemble vari-\nance is too large or too small on average, affine kernel dressing\nallows for a more flexible relationship between the variance of\nthe ensemble and the variance of the dressed ensemble than\nstandard dressing in either case. A distinct advantage of AKD\nover BMA emerges from the relations (17, 18). For AKD, these\ntwo relations are independent. This would, in principle, permit\nto debiase the ensemble mean and simultaneously optimize the\nspread\u2013skill relationship. The relations (eqs. 5 and 6) show that\nthe same is true for GDF. For BMA though, r2 = 0 and s2 = 0, in\nwhich case the linear part in both the relations (eqs. 17 and 18) is\ndetermined by a. In other words, having debiased the ensemble,\nthere remains little which can be done for a better spread\u2013skill\nrelationship. As demonstrated in Section 6, AKD offers signifi-\ncant benefits when applied to numerical weather predictions for\nwhich the square error of the ensemble mean is not well repre-\nsented by the ensemble variance. To the extent that it is Bayesian,\nBMA provides a principled framework for constructing proba-\nbility forecasts. This comes with the cost of assuming that one\nof the models is true (Hoeting et al., 1999) or alternatively that\nthe available model class admits a perfect model.\nWhile all variants of kernel dressing borrow from and bear\nsome resemblance to Kernel Estimation (KE), a technique em-\nployed to estimate probability density functions (Silverman,\n1986), we stress that kernel dressing (and in fact ensemble inter-\npretation in general) rests on different assumptions than kernel\nestimation. The latter attempts to fit a probability density function\nto a single and unchanging archive of points. These points are si-\nmultaneously forecasts and verifications. Future points, although\nnot expected to be equal to any point in the archive, are never-\ntheless assumed to be drawn from the same source. Thereby, in\nKE, the ensemble and the verification are draws from the desired\ndistribution. For kernel dressing of ensemble forecast, there is\nbut one verification for every ensemble, and typically, the ver-\nification is not drawn from the ensemble, that is, the ensemble\nis demonstrably not perfect. The improved dressing method as\npresented in eqs. (19), (20) and (21) looks superficially similar\nto a kernel estimator applied to the transformed ensemble z. It\nshould be kept in mind though that eventually all parameters\nof kernel dressing are determined simultaneously and depend\non each other, thus the ensemble transform (eq. 20), the choice\nof the bandwidth (eq. 21) and the dressing (eq. 19) cannot be\nseparated.\n4. Scoring and Training\nThe ensemble interpretation methods presented in the Section 2\ndepend on the as yet unspecified parameters \u03b8 . We con-\nsider the problem of determining the parameters of ensemble\nTellus 60A (2008), 4\n668 J . BR \u00a8OCKER AND L. A. SMITH\ninterpretations to be similar to the learning problem of statistics\n(Vapnik, 1998; Hastie et al., 2001). In the latter problem, the ob-\njective is to fit a functional relationship between certain inputs\nand verifications, based on a training set of input-verification\npairs. The functional relationship is picked from a range of func-\ntions or model class according to performance. An algorithmic\nprocedure that tunes the parameters according to performance\nover a training set will be referred to as a training algorithm.\nAt the core of most training algorithms lies an iterative proce-\ndure which optimizes the expected performance as a function of\nthe parameters. A more classical term for training algorithm is\n\u2018estimation technique\u2019. The difference is only a linguistic one,\nbut estimation might imply the existence of a true parameter\n(like a physical quantity) that is to be estimated. The parame-\nters of ensemble interpretation methods though need not to have\nany physical interpretation, whence the term training algorithm\nseems more appropriate here.\nWhen interpreting ensembles, the objective is to find a proba-\nbilistic relationship between the inputs and verifications, where\nthe model class consists of sums of kernel functions, and the\ntraining set consists of ensemble-verification pairs (hence, the\ntraining set is often referred to as forecast archive). The un-\nspecified parameters should be determined solely by forecast\nperformance, not by any a priori assumptions, like, for exam-\nple, that the ensemble and the verification are draws form one\nand the same underlying distribution. This obviously involves\nfinding appropriate performance measures or scoring rules for\nprobabilistic forecasts, which we will turn to now.\nA scoring rule is a function S(p(y), Y), where p(y) is a prob-\nability density and Y is the verification. In this paper, scoring\nrules are defined like cost functions: small scores indicate better\nforecast skill. For example, the Ignorance Score is defined by\nthe scoring rule\nS(p(y), Y ) = \u2212 log[p(Y )].\nThe Ignorance score is related to the log likelihood (Mood et al.,\n1974; Bro\u00a8cker and Smith, 2007) and plays an important role in\ngambling theory. Another interesting scoring rule (although not\nused in this paper) is the Proper Linear Score. It is defined as\nS(p(y), Y ) =\n\u222b\np2(z) dz \u2212 2p(Y ). (22)\nIt should be noted that the Ignorance depends only on the single\nnumber p(Y), while the Proper Linear Score depends on the entire\nfunctional form of p(y). This particular property of the Ignorance\nis called locality. Local scores are typically cheaper to evaluate\nthan non-local scores. Computing functionals of the probability\ndensity (such as the integral in eq. 22) are often very costly. As\nnoted by Gneiting et al. (2005), similar reasons have hampered\nthe use of the CRPS score.\nIt turns out that not all conceivable candidates for scoring\nrules yield useful scores. An indispensable property of scores is\npropriety. Roughly speaking, a score is proper if p(y) achieves\nan optimal (i.e. minimal) expected score whenever the verifica-\ntion is drawn from p(y). A scoring rule is strictly proper if that\nhappens only if the verification is drawn from p(y). Propriety is\na property only of the scoring rule itself. The Ignorance and the\nProper Linear Score are proper (for a proof of this fact as well as\na discussion of the notion of propriety see Bro\u00a8cker and Smith,\n2007). A general result due to Bernardo (1979) states that all\nsmooth, proper and local scores are affine functions of the Ig-\nnorance. Proper scores, in general, have been characterized by\nGneiting and Raftery (2007).\nIn evaluating forecast systems, one is not only concerned with\na single probability density function p(y) but also with a sequence\npn(x) of probability density functions and corresponding verifi-\ncations Yn which can be employed to estimate the performance of\nthe forecast system, in other words, the expected score (with re-\nspect to a proper scoring rule S). To this end, define the empirical\nscore:\nSN := 1N\nN\u2211\nn=1\nS[pn(x), Yn]. (23)\nThe empirical score values the average performance of the fore-\ncast system over all samples in the archive. In the case of dressed\nensembles, the probability density functions are time depend\nthrough the ensemble x(n), that is pn(y) = p[y; x(n), \u03b8 ] where\n\u03b8 denotes the ensemble interpretation parameters. In the case\nof AKD, for example, \u03b8 = [a, r1, r2, s1, s2]. Replacing the ex-\npression for pn(y) in eq. (23) and using the Ignorance score, we\nobtain\nSN (\u03b8 ) = 1N\nN\u2211\nn=1\n\u2212 log [p(Yn, x(n), \u03b8 )] . (24)\nIn eq. (24), the empirical score of the ensemble (which essen-\ntially reflects the performance of the forecast system) can be\nregarded as a function of the free ensemble interpretation pa-\nrameters \u03b8 . Minimizing the score (and thereby optimizing the\nperformance of the dressed ensemble) with respect to the pa-\nrameters \u03b8 provides a means to choose these parameters, i.e. a\nmeans of training, reminiscent of statistical learning. In statis-\ntical learning, a functional relationship is picked from a range\nof functions according to its performance, which is often (but\nnot always) the quadratic error. In ensemble interpretation, a re-\nlationship between ensembles and probability density functions\nis picked from a range of functions according to performance,\nwhich in this paper is measured by the Ignorance score. The\napproach to minimize performance measures (such as the Igno-\nrance score) to determine the parameters of forecast interpreta-\ntion methods for continuous events was, to our knowledge, first\nconsidered by Jewson (2003a,b) and (apparently independently)\nGneiting et al. (2005). In so far as minimizing the Ignorance can\nbe considered as maximum likelihood, it is, of course, a very old\nconcept.\nA thorough theoretical investigation of the minimum-score\ntraining strategy and the properties of the obtained parameters\nTellus 60A (2008), 4\nFROM ENSEMBLE FORECASTS TO PREDICTIVE DISTRIBUTION FUNCTIONS 669\nwould be invaluable, but is not subject to this paper. We used\nan optimization algorithm that solves a sequence of constrained\nquadratic optimization problems (Gill et al., 1982). Other options\nare the EM algorithm employed by Raftery et al. (2005). Both\nalgorithms are only guaranteed to find local rather than global\nminima. We are ignorant as to whether the EM algorithm could\nbe applied to other scores, while preliminary studies indicate\nthat sequential quadratic optimization works equally well with\nthe proper linear score. The Ignorance of kernel dressing can\ndisplay multiple minima with rather poor performance. Robust\nsolutions with good performance, however, are obtained in prac-\ntice by a regularization strategy, discussed in Section 5, along\nwith a careful initialization of the minimization algorithm. The\nresults reported in this paper were obtained using the following\nmethodology for finding the initial conditions. The mean of the\ndressed ensemble (as described by eq. 5 in case of Gaussian DF\ninterpretation, respectively, eq. 11 in case of kernel dressing) is\nfitted to the verification in a mean-square error sense. The vari-\nance of the dressed ensemble (as described by eq. 6 in case of\nGaussian DF interpretation, respectively, eq. 12 in case of kernel\ndressing) then should roughly correspond to the squared resid-\nuals of the fitted mean. Thus, fitting the variance of the dressed\nensemble to the squared residuals gives a further condition to\nfind initialization parameters. As it turns out, this allows for\nfinding complete initial conditions for Gaussian DF interpreta-\ntion and standard kernel dressing. For AKD, this strategy leaves\ns2 unspecified, which is set to 1. The structure of the problem as\npresented in eqs. (20), (21) and (19) and the use of Silverman\u2019s\nfactor guarantee that setting s2 = 1 is a reasonable choice unless\nthe transformed ensemble (eq. 20) is extremely poor.\n5. Robustness Issues\nObtaining robust estimates for the parameters of ensemble in-\nterpretation methods can be difficult, especially if forecast busts\nare numerous or when the ensemble is small. This problem is of-\nten traced back to the empirical score showing a large variance.\nRecently, several authors (Gneiting and Raftery, 2007; Selten,\n1998) criticized the Ignorance for being particularly prone to\nlarge variation. The Ignorance is a quite unforgiving score in\nthat it extremely severely penalizes low-probability assignment\nto verifications that actually obtain. Indeed, assigning vanishing\nprobability to a verification yields an Ignorance of infinity. Even\nif the assigned probabilities are never exactly zero, a few \u2018bad\nforecasts\u2019 can render the variance of the empirical Ignorance un-\ndesirably large, resulting in parameters obviously useless (this\nmay be a positive attribute in decision support). It should be noted\nthat the Ignorance has a clear interpretation in terms of gambling\nreturns (Good, 1952; Kelly, 1956; Roulston and Smith, 2002).\nUnder a certain betting scenario (\u2018Kelly Betting\u2019, Kelly, 1956),\nthe Ignorance describes the rate at which the forecaster\u2019s fortune\nincreases with time. The properties of the Ignorance hence can\nbe defended as representing properties of a game. Furthermore,\nlarge variations in the empirical score are always to be expected\nif the forecasts are poor and should adequately be dealt with,\nespecially as the score might not even be a matter of choice. So\nhow can large variations in the empirical score be avoided?\nIt was suggested by Gneiting and Raftery (2007) that the sum-\nmands in eq. (24) could be censored, that is, a certain percentage\nof the data could be rejected as outliers. Another option could\nbe to use a truncated logarithm, which would be reminiscent of\n\t-insensitive loss functions in regression (Vapnik, 1998). This\nseems inadvisable in cases where such \u2018outliers\u2019 have a firm\nphysical interpretation and are expected to become more rele-\nvant in the future dynamics, for example in seasonal forecasting.\nThese and other means to combat the influence of outliers on\nthe score and subsequently the parameters are often referred to\nas regularization. It has to be kept in mind though that the Ig-\nnorance (or whichever score is employed) is used both to train\nthe ensemble interpretation parameters and also to evaluate the\ninterpreted forecast. During training, any kind of regularization\nis permissible and even recommended. For evaluation, however,\ncensoring or truncating of the score would require it to be rein-\nterpreted. Important properties and interpretations of the score\nmight not hold for the regularized score. For example, common\ninterpretations of the Ignorance in terms of gambling return rates\ncease to apply if the sum in eq. (24) is censored, which essentially\nwould be tantamount to cancelling the highest winnings and to\ndefault on the worst bankruptcies. In practice, certain scoring\nprocedures (e.g. in sailing, ski jumping or ice skating) actually\nallow to retrospectively discount the worst results (sometimes re-\nquiring the best results to be cancelled too), but this is certainly\nnot the case in \u2018games\u2019 as for example casinos, energy markets\nor air traffic control. Hence, in general, it seems to depend on\nthe particular problem whether a censored (or truncated) score\nis an appropriate measure of forecast performance.\nIn situations where a regularization of the problem is necessary\nduring training, but where the problem statement does not allow\nfor any censoring or otherwise altering of the score, it seems\ninevitable to apply a slightly different (i.e. regularized, respec-\ntively, not regularized) scoring methodology during the training\n(respectively, evaluation) period. In this paper, the logarithm was\neffectively truncated by replacing all pn which were equal to zero\n(up to numerical precision) by the smallest non-zero pn . For eval-\nuation though, the Ignorance was neither censored nor truncated.\nSuch discrepancies (which are inherent to all regularization ap-\nproaches) might seem disturbing at first sight. Currently, we lack\na full theoretical justification of this approach, but as an ad hoc\nscheme we found it to give superior results, presumably because\nof smaller variance in the dressing parameters.\nTo account for forecast failures during evaluation, the dressed\nensemble was blended with an estimate of the climatology of the\nverification, thereby circumventing the problem of large vari-\nances in the empirical score. For a finite ensemble size, this is\njustifiable even in the case of a perfect ensemble. More specifi-\ncally, let pn(y) be the interpreted ensemble and q(y) be an estimate\nTellus 60A (2008), 4\n670 J . BR \u00a8OCKER AND L. A. SMITH\nof the climatology of the verification. We use a mixture of both,\nlike\nrn(y) := \u03b1 pn(y) + (1 \u2212 \u03b1)q(y), 0 \u2264 \u03b1 \u2264 1, (25)\nas the forecast distribution. The weight \u03b1 is determined so as to\nminimize the Ignorance (i.e. to optimize the performance) of the\ncombination, and hence must be involved in the optimization.\nThe resulting probability assigned to a verification Y is never\nsmaller than (1 \u2212 \u03b1) q(Y). The effect therefore is that a small,\nyet non-vanishing probability is assigned to the verification, as\nlong as the latter does not fall outside the range of the data record\nemployed to estimate the climatology. Forecast performance is\noften stated in relation to the performance of climatology as a\nreference. This means that the (mean of the) difference in per-\nformance between pn(y) and the climatology q(y) is reported.\nThus, the climatology acts as a reference forecast, itself yielding\na score of zero. In case of the Ignorance, this can be written as\nSN [p] \u2212 SN [q] := 1N\n\u2211\n\u2212 log\n[\nrn(Yn)\nq(Yn)\n]\n. (26)\nReplacing rn(y) from eq. (25), we get for every summand\nrn(Yn)\nq(Yn)\n= \u03b1 pn(Yn) + (1 \u2212 \u03b1)q(Yn)\nq(Yn)\n= \u03b1 pn(Yn)\nq(Yn)\n+ (1 \u2212 \u03b1),\n\u2265 (1 \u2212 \u03b1),\nfrom which we can conclude\n\u2212 log [rn(Yn)] \u2264 \u2212 log [q(Yn)] \u2212 log(1 \u2212 \u03b1).\nHence, the empirical Ignorance of a forecast combined with\nclimatology relative to climatology is never larger (i.e. worse)\nthan \u2212log (1 \u2212 \u03b1). Blending in climatology thus acts as a hedge\nagainst forecast busts. Another way to interpret a blend with cli-\nmatology is to play cancelling bets. The Ignorance of a forecast\nrelative to climatology describes the rate at which the forecast-\ners fortune increases in a betting scenario where the odds are\nset according to climatology.6 Mixing in a proportion 1 \u2212 \u03b1 of\nclimatology hence is equivalent to staking a proportion \u03b1 of the\nfortune according to the forecast and a proportion 1 \u2212 \u03b1 accord-\ning to the odds given, which guarantees a certain return of at\nleast a proportion 1 \u2212 \u03b1 of the stake. The forecaster thus avoids\nbeing infinitely worse off than the house.\nOnly few forecast busts are sufficient to render a good cli-\nmatology worth being blended with the forecast proper. As\nan example, Fig. 1 shows \u2212log [rn(Yn)], combined with clima-\ntology, versus \u2212log [q(Yn)], that is the climatology itself. The\nensemble forecast was from ECMWF\u2019s medium range 51 mem-\nber ensemble prediction system. The lead time was 10 d. The\n6Or alternatively, relative Ignorance between two forecasts A and B\ndescribes the rate at which the fortune of forecaster A exceeds that of\nforecaster B.\n0 1 2 3 4 5 6 7 8 9\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nn\n)\nn\n)\nFig. 1. The Ignorance \u2212 log [rn(Yn)] (ECMWF ensemble and\nclimatology) versus \u2212 log [q(Yn)] (only climatology) for temperature at\nHeligoland, German Bight (WMO 10015), lead time ten days. The\ndressing method here is AKD. Obviously, \u2212 log [rn(Yn)] is never larger\nthan \u2212 log [q(Yn)] \u2212 log (1 \u2212 \u03b1). The weight assigned to the\nclimatology is 1 \u2212 \u03b1 = 0.051, whence \u2212 log (1 \u2212 \u03b1) \u2248 3.\n24 48 72 96 120 144 168 192 216 240\n0\n0.02\n0.04\n0.06\n0.08\n0.1\nLead Time\nW\ne\nig\nht\n A\nss\nig\nne\nd \nto\n C\nlim\nat\nol\nog\ny \n(\u03b1\n)\nFig. 2. The weight assigned to the climatology over lead time. The rest\nis as in Fig. 1. The confidence bars display variations of the weight\nestimate obtained through cross-validation (see Appendix A).\nweight assigned to the climatology is 1 \u2212 \u03b1 = 0.051. It is\nobvious from the plot that \u2212log [rn(Yn)] is never larger than\n\u2212log [q(Yn)] \u2212 log (1 \u2212 \u03b1) = \u2212log [q(Yn)] + 2.97 at every\nverification (not just in the mean). Fig. 2 shows the weight as-\nsigned to the climatology over lead time. The confidence bars\ndisplay variations of the weight estimate obtained through cross-\nvalidation (see Appendix A).\nIt might prove difficult to determine \u03b1 robustly, as the op-\ntimal combination of \u03b1 and kernel bandwith (i.e. \u03c3 in eq. 21)\nfor the training set might be a local (and very poor) minimum\nby suggesting a very wide bandwith to compensate for forecast\nbusts, instead of employing the climatology for that purpose.\nThis could be addressed by using a kernel function with a limited\ndomain (like the quadratic Epanechnikov kernel, see e.g. Silver-\nman, 1986), which yields infinite Ignorance for all points outside\nits domain. Alternatively, large kernel bandwidths \u03c3 could be pe-\nnalized. Taking into account the finite ensemble size and prob-\nably a known rate of forecast busts, it should even be possible\nto derive an upper bound on \u03b1 (i.e. a lower bound on the weight\nassigned to climatology). The suggested precautions were, how-\never, not necessary for the data sets considered in this paper (see\nSection 6).\nTellus 60A (2008), 4\nFROM ENSEMBLE FORECASTS TO PREDICTIVE DISTRIBUTION FUNCTIONS 671\nAnother interesting interpretation of the weight 1\u2212\u03b1 assigned\nto the climatology could be to quantify of belief in or uncertainty\nof our forecast. The question arises if and how uncertainty of\nprobabilistic forecasts could be quantified more generally, for\nexample if the climatology is unknown or is known to be chang-\ning. If the predictive distribution is interpreted as a probability,\nwe are now speaking about assigning an uncertainty to what is\nalready a probability, thus introducing the idea of second-order\nprobabilities, that is quantifying statements like \u2018the probability\nthat it rains tomorrow at London Heathrow is evenly distributed\nbetween 10 and 20%\u2019. The second-order probabilities lead to\nodd forecasts (K. Judd, personal communication, 2006), that is,\nforecasts with a total mass larger than one, the excess represent-\ning uncertainty in the forecast (Smith, 2007). Although it is not\nyet clear how uncertainty in probabilistic forecasts in general or\nodds, in particular, could be assigned or used, such a framework\nrequires ensemble interpretation methods that focus on informa-\ntion content in the ensembles to hand, while the assumption that\nthe resulting predictive distributions can be interpreted or acted\non as if they are (decision relevant) probability distributions has\nto be dropped.\n6. Comparative Studies\nThis section analyses the performance of standard kernel dress-\ning (SKD), AKD, and Gaussian DF interpretation (GDF). Short-\ncomings of SKD and GDF, which originally motivated the de-\nvelopment of AKD, are illustrated. AKD was compared to BMA\ntoo, albeit less comprehensively. All ensemble interpretation\nmethods were blended with climatology, with the exception of\nGaussian DF interpretation (GDF). AKD is shown to be superior\nto all other methods for the problems considered. As far as we are\naware, previous implementations of BMA do not blend in clima-\ntology, leading to significantly larger variations in performance\nand often inferior skill.\nResults are presented for three different data sets. The first\nand second data sets consist of forecasts of the 2 m temperature\nat London Heathrow Airport (WMO station Nr.03772) and He-\nligoland, German Bight (WMO station Nr.10015), respectively.\nThe forecasts consist of ECMWF\u2019s 51 member ensemble (as for\nFigs. 1 and 2). The verifications consist of station data, kindly\nprovided by ECMWF as well. Forecasts were available for the\nyears 2001\u20132005, featuring lead times from 1 to 10 d. Verifica-\ntions were available as far back as 1981. The years 1981\u20132000\nwere used to build a climatology. For any given day, the clima-\ntology is calculated only from data falling into the same annual\nperiod, defined by a window of \u00b120 d. Thus, the climatology\ndepends as well on the season. All data verified at noon. The\nresults for the weather data are shown in Figs 3 and 4 and are\ndiscussed below.\nThe third data set was generated using the Lorenz63 system\n(Lorenz, 1963). The ensemble, comprising 50 members, was\ngenerated from observations of the full state of the system, cor-\nrupted with 15 dB noise.7 The sampling interval was 0.05. For\ndata assimilation, a variant of the indistinguishable states im-\nportance sampler (Judd and Smith, 2001) was employed. Data\nassimilation is necessary here, since we have but noisy mea-\nsurements of the true underlying state of the system. Although\nensembles could also be generated by perturbing the true ini-\ntial condition, this option would, of course, not be available in\nreal applications. Hence, using a data assimilation scheme cor-\nresponds much more to realistic circumstances. Forecasts were\nconsidered at 10 lead times [0.1, 0.2, . . . , 1]. The same model\nwas used to generate both forecasts and the verifications. More-\nover, the verifications formed a single trajectory. In general, the\nAKD significantly outperforms SKD and GDF, especially for\nthe Lorenz63 system. The AKD method also appears to be the\nmost robust method among the three, in the sense that the perfor-\nmance of AKD showed the least variability. The results for the\nLorenz63 system are shown in Fig. 5, and are discussed below.\nFig. 3a shows the performance in terms of Ignorance of AKD\nrelative to climatology for the London Heathrow data set. The\nx-axis shows the lead time. The confidence bars (in fat line style)\nmark the 10\u201390% range obtained from a 10-fold cross-validation.\nThe thin line shows the Ignorance of the out-of-train (OOT) out-\nput. The corresponding thin confidence bars show the \u00b12 \u03c3 range\n(see Appendix A). Cross-validation is known to have a large vari-\nance (Hastie et al., 2001), while the variance of the OOT output\n(see Appendix A) on the other hand tends to be too small. In\nany case, AKD gives a significantly higher skill than the clima-\ntology under both validation methods. In order to compare the\nperformance of AKD, SKD and GDF interpretation, we plot the\ndifference of the Ignorance (eq. A7) directly, rather than leave it\nto the reader to compare performances across multiple graphs.\nThis allows for confidence bars of the relative performance,\nas the uncertainty in the relative performance does not fol-\nlow from the uncertainties of the absolute performances (see\nAppendix A). The axis scaling has been set so as to allow for\neasy comparison across different graphs.\nFig. 3b shows the performance of GDF versus AKD. The OOT\nconfidence bars overlap the zero line slightly for lead time 24, 48\nand 72 h, but sees AKD significantly ahead of GDF beyond lead\ntime 72 h. The cross-validation assessment indicates essentially\nthe same, the bars being wider though.\nFig. 3c shows the performance of SKD versus AKD for\nLondon Heathrow. Up to lead time 120 h, the AKD method\noutperforms SKD substantially, at least according to OOT cal-\nculation. For higher lead times, AKD still appears to be better\nfor a large fraction of cross-validation runs.\n7The dB scale measures the ratio between the variances of two signals.\nA signal-to-noise ratio of d dB indicates that d = 10 \u00b7 log10( vsvn ), where\nvs (respectively, vn) is the variance of the clean signal (respectively, the\nnoise)\nTellus 60A (2008), 4\n672 J . BR \u00a8OCKER AND L. A. SMITH\n24 48 72 96 120 144 168 192 216 240\n4\n2\n8\n6\n4\n2\n0\nLead Time (hours)\nIg\nno\nra\nnc\ne\nAffine Kernel Dressing vs Climatologya)\n24 48 72 96 120 144 168 192 216 240\n0\n0.025\n0.05\n0.075\n0.1\n0.125\n0.15\nLead Time (hours)\nIg\nno\nra\nnc\ne\nGaussian Fit vs Affine Kernel Dressingb)\n24 48 72 96 120 144 168 192 216 240\n0\n0.025\n0.05\n0.075\n0.1\n0.125\n0.15\nLead Time (hours)\nIg\nno\nra\nnc\ne\nStandard Kernel Dressing vs Affine Kernel Dressingc)\n24 48 72 96 120 144 168 192 216 240\n0\n0.025\n0.05\n0.075\n0.1\n0.125\n0.15\nLead Time (hours)\nIg\nno\nra\nnc\ne\nStandard Kernel Dressing vs Gaussian Fitd)\n24 48 72 96 120 144 168 192 216 240\n0\n0.025\n0.05\n0.075\n0.1\n0.125\n0.15\nLead Time (hours)\nIg\nno\nra\nnc\ne\nBMA vs Affine Kernel Dressingf)\n24 48 72 96 120 144 168 192 216 240\n4\n2\n8\n6\n4\n2\n0\nLead Time (hours)\nIg\nno\nra\nnc\ne\nBMA vs Climatologye)\nFig. 3. The relative Ignorance of the investigated ensemble interpretation methods and climatology for London Heathrow over lead time. The fat\nconfidence bars are from tenfold cross-validation (10\u201390% range). The thin confidence bars correspond to the OOT performance (\u00b12\u03c3 range).\nFig. 3d shows the performance of SKD versus GDF. From\nlead time 96 h onwards, the two are essentially similar. The\npotential advantage of SKD when dealing with strongly non-\nGaussian ensembles seems to play a little role for temperature\nat lead times up to 100 h.\nThe comparison between BMA and AKD (Fig. 3f) remains\nsomewhat inconclusive, although AKD is certainly better than\nBMA for medium and larger lead times. In terms of OOT per-\nformance, AKD is significantly better than BMA. Note that our\nimplementation of BMA includes blending with the climatology.\nThis blending is not a common part of BMA, and some Bayesians\nmight object to it on principle, but it allows for a better com-\nparison between BMA and AKD. Without climatology, BMA\nshows considerably larger variation in performance (not shown).\nThe findings for Heligoland (Fig. 4) are very similar to the\nresults obtained for London Heathrow, a notable exception being\nthat AKD wins over GDF by an even wider margin. Furthermore,\nthe superior performance of AKD over BMA occurs for higher\nlead times when compared to London Heathrow (cf. Fig. 3f with\nFig. 4f).\nIt is interesting to look at the non-Gaussianity of the ensemble\nfor these two data sets, especially in connection to the perfor-\nmance of AKD versus GDF (Figs. 3b and 4b), as we expect AKD\nto outperform GDF if the ensembles deviate from Gaussianity.\nAs a measure of non-Gaussianity, we employ the kurtosis of the\nensemble, that is the centred moment of fourth order,\nk(x) = 1\nd\n\u2211\n[xi \u2212 m(x)]4,\nwhere m(x) is, as before, the ensemble mean. For Gaussian distri-\nbutions, the fourth-centred moment is expected to be three times\nthe variance, hence we expect for Gaussian ensembles\n\u03ba(x) := k(x)\n3v(x) \u2212 1 \u2248 0.\nThe distribution of this statistic \u03ba for Gaussian ensembles can\nbe simulated through bootstrapping and subsequently compared\nwith the distribution of \u03ba for the actual ensembles. In Figs. 6\nand 7, the 10\u201390% range of the actual values of \u03ba is indicated\nby a black bar, for London Heathrow and Heligoland, respec-\ntively. The y-axis is calibrated in terms of quantiles of \u03ba for\nTellus 60A (2008), 4\nFROM ENSEMBLE FORECASTS TO PREDICTIVE DISTRIBUTION FUNCTIONS 673\n24 48 72 96 120 144 168 192 216 240\n4\n2\n8\n6\n4\n2\n0\nLead Time (hours)\nIg\nno\nra\nnc\ne\nAffine Kernel Dressing vs Climatologya)\n24 48 72 96 120 144 168 192 216 240\n0\n0.025\n0.05\n0.075\n0.1\n0.125\n0.15\nLead Time (hours)\nIg\nno\nra\nnc\ne\nGaussian Fit vs Affine Kernel Dressingb)\n24 48 72 96 120 144 168 192 216 240\n0\n0.025\n0.05\n0.075\n0.1\n0.125\n0.15\nLead Time (hours)\nIg\nno\nra\nnc\ne\nStandard Kernel Dressing vs Affine Kernel Dressingc)\n24 48 72 96 120 144 168 192 216 240\n0\n0.025\n0.05\n0.075\n0.1\n0.125\n0.15\nLead Time (hours)\nIg\nno\nra\nnc\ne\nStandard Kernel Dressing vs Gaussian Fitd)\n24 48 72 96 120 144 168 192 216 240\n0\n0.025\n0.05\n0.075\n0.1\n0.125\n0.15\nLead Time (hours)\nIg\nno\nra\nnc\ne\nBMA vs Affine Kernel Dressingf)\n24 48 72 96 120 144 168 192 216 240\n4\n2\n8\n6\n4\n2\n0\nLead Time (hours)\nIg\nno\nra\nnc\ne\nBMA vs Climatologye)\nFig. 4. As in Fig. 3, but for Heligoland.\nGaussian ensembles. If the actual ensembles were Gaussian, all\nbars should extend from 0.1 to 0.9.8 It emerges that at both lo-\ncations, the ensembles tend to be particularly non-Gaussian at\nlead times around 96 h. Interestingly, for larger lead times at\nLondon Heathrow, the \u03ba-statistic indicates again a more\nGaussian ensemble. For Heligoland, the ensembles are also par-\nticularly non-Gaussian at lead times around 96 h, but contrary to\nLondon Heathrow, the ensembles stay fairly non-Gaussian out\nto lead time 240 h. This provides a possible explanation for the\nbetter performance of AKD in relation to GDF at Heligoland. It\nis worth noting that the better performance of AKD versus GDF\nfurthermore indicates that the non-Gaussian ensembles carry\ninformation beyond the second moment. The AKD interpreta-\ntion outperforms GDF not only because the ensembles are non-\nGaussian, but also because this non-Gaussianity actually carries\ninformation.\n8The scale of the y-axis is not linear in p but in log( p1\u2212p ). For small\n(respectively, large p), this renders the plot effectively logarithmic in p\n(respectively, 1 \u2212 p).\nAs to the reasons why AKD outperforms the other discussed\nmethods, further investigation is necessary. There is some evi-\ndence though that the mechanisms discussed in Section 3 are in\nfact responsible. We investigated the parameters for both BMA\nand AKD for London Heathrow at lead time 120 h. Note that\nAKD is particularly strong here, and that the ensembles are par-\nticularly non-Gaussian. The parameters were substituted into\neqs. (17) and (18). For AKD, these relations read\n\u03bc = 0.0 + 0.99 m(x), (27)\n\u03bd = 1.93 + 0.53 v(x). (28)\nFor BMA, these relations read\n\u03bc = 0.003 + 1.0 m(x), (29)\n\u03bd = 0.17 + 1.0 v(x). (30)\nThe cross-validation approach (see Appendix A) yields an un-\ncertainty of less than 10\u22123 for all these coefficients. Since eqs.\n(27) and (29) agree to a high degree, AKD and BMA always\nhave very similar means. The eqs. (28) and (30) though differ.\nTellus 60A (2008), 4\n674 J . BR \u00a8OCKER AND L. A. SMITH\n0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1  \n1\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\nLead Time\nIg\nno\nra\nnc\ne\nStandard Kernel Dressing vs Gaussian Fit\n0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1  \n1\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\nLead Time\nIg\nno\nra\nnc\ne\nStandard Kernel Dressing vs Affine Kernel Dressingb)\n0  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1  \n1\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\nLead Time\nIg\nno\nra\nnc\ne\nGaussian Fit vs Affine Kernel Dressingd)c)\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nLead Time \nIg\nno\nra\nnc\ne\nAffine Kernel Dressing vs Climatologya)\nFig. 5. The relative Ignorance of the investigated ensemble interpretation methods for the Lorenz63 data set over lead time. Confidence bars are as\nin Fig. 3.\n24 48 72 96 120 144 168 192 216 240\n0.001\n0.01 \n0.1  \n0.5  \n0.9  \n0.99 \n0.999\nPlot of Nongaussianity Over Lead Time (station: wmo03772).\nLead Time\nFig. 6. The 10\u201390% range of the \u03ba-statistic for London Heathrow. The\ny-axis is calibrated in terms of quantiles of the \u03ba-statistic for Gaussian\nensembles and plot on log( p1\u2212p ) scale.\n24 48 72 96 120 144 168 192 216 240\n0.001\n0.01 \n0.1  \n0.5  \n0.9  \n0.99 \n0.999\nPlot of Nongaussianity Over Lead Time (station: wmo10015).\nLead Time\nFig. 7. The 10\u201390% range of the \u03ba-statistic for Heligoland. The y-axis\nis calibrated in terms of quantiles of the \u03ba-statistic for Gaussian\nensembles and plot on log( p1\u2212p ) scale.\nAs was mentioned already in Section 3, for BMA the slope of\nthe variance relation (eq. 30) is always the square of the slope\nof the mean relation (eq. 29), whence it is impossible for BMA\nto have mean and variance relations like eqs. (27) and (28). It\nappears though that the variance relation of AKD (eq. 28) gives\nthe better performance. It is interesting to note that the two vari-\nance relations intersect at v(x) = 3.74, as this is almost exactly\nthe temporal average of v(x), which is 3.76. This means that on\naverage over time, BMA and AKD feature the same variance\n(3.93), which is in fact the ensemble variance, slightly inflated.\nFor individual ensembles though, their variances generally dif-\nfer. In particular, the variations of the variance (i.e. the variance\nof \u03bd) is larger for BMA than for AKD. The lead times 48 and\n216 h (for Heathrow and Heligoland) were investigated along\nthe same lines, with similar findings. Finally, we would like to\nmention that for AKD, BMA and SKD, the weight assigned to\nclimatology behaves roughly as in Fig. 2.\nThe experiments carried out using the Lorenz63 data confirm\nthe general picture already obtained from the weather data ex-\nperiments, thereby confirming that any positive results are not\nonly due to limited counting statistics. AKD is the best perform-\ning and most robust method. The performance of AKD versus\nclimatology is shown in Fig. 5a. AKD and SKD perform roughly\nequal (Fig. 5b). We suspect that this is due to the high quality\nof the ensemble. If the ensembles were either overdispersive or\nunderdispersive, we would expect AKD to perform better than\nSKD. Talagrand diagrams (not shown), however, indicate that\nthe ensembles are very reliable (i.e. neither overdispersive nor\nunderdispersive), which explains the similar performance of both\nTellus 60A (2008), 4\nFROM ENSEMBLE FORECASTS TO PREDICTIVE DISTRIBUTION FUNCTIONS 675\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n0.001\n0.01 \n0.1  \n0.5  \n0.9  \n0.99 \n0.999\nPlot of Nongaussianity Over Lead Time (Lorenz63).\nLead Time\nFig. 8. The 10\u201390% range of the \u03ba-statistic for Lorenz63. The y-axis is\ncalibrated in terms of quantiles of the \u03ba-statistic for Gaussian\nensembles and plot on log( p1\u2212p ) scale.\nAKD and SKD. Inspection of the AKD models (not shown) indi-\ncates that the parameter a (see eqs. 20 and 21) is close to one, in\nparticular for small lead times, rendering AKD and SKD essen-\ntially equal. Kernel dressing (i.e. AKD and SKD) significantly\noutperform GDF for higher lead times (Figs. 5c and d). A main\nreason for this is certainly the increasingly non-Gaussian en-\nsembles for higher lead times, as is obvious from a plot of the\n\u03ba-statistic (Fig. 8). Again, by comparing the variance of the per-\nformance across different graphs, it can be concluded that AKD\nfeatures not only the best, but also the most robust performance.\n7. Conclusion\nThere is valuable information in ensemble weather forecasts;\nextracting this information requires interpreting the ensemble.\nComparing different methods for interpreting ensembles shows\nthat the AKD technique introduced in this paper yields promising\nresults for operational temperature forecasts using the ECMWF\nensemble; its strengths are also illustrated in the context of\nperfect models and large forecast-verification archives with\nthe Lorenz 63 system. In terms of the ignorance score, AKD\noutperforms the other methods in all cases considered; both\ncross-validation and OOT evaluation confirm the results. The\nimportance of blending climatology into the probability distri-\nbution function is shown.\nOur approach aims at extracting information from an ensem-\nble without making assumptions regarding the perfection of the\nmodel or the ensemble. There is no assumption that the ver-\nification represents \u2018just another draw\u2019 from the distribution\nthat generated the ensemble, nor any assumption that the model\nclass available admits a \u2018true\u2019 model. There is abundant evi-\ndence that such assumptions are not justified in operational fore-\ncast systems. We furthermore touch on the question of whether\nor not probability distributions functions are indeed the best\nrepresentation of the valuable information contained in these\nsystems.\nTo the extent that operational forecasts are made to be used,\nthe ensemble interpretation is a critical component contribut-\ning to the value of an ensemble prediction system. By aiming\nmerely to extract information from the model simulations and\nother available distributions (e.g. climatology), AKD has been\nshown to improve this critical component, and may contribute to\nenhancing the value of ensemble-based prediction, particularly\nin applications like weather forecasting at all lead times.\n8. Acknowledgments\nWe are grateful to Renate Hagedorn and ECMWF for pro-\nviding ensemble forecast as well as station data for London\nHeathrow and Heligoland. The forecasts and verifications for\nthe Lorenz63 system were provided by Hailiang Du. We grate-\nfully acknowledge the fruitful discussions with Jeremy Penzer,\nAntje Weisheimer and Liam Clarke. Questions and suggestions\nby two anonymous referees led to further improvements of the\nmanuscript.\n9. Appendix A: On Out-Of-Train Evaluation\nand Cross-Validation\nPerformance evaluation of forecast systems aims to provide a\nsound estimate of the future or out-of-sample performance, or\nmore specifically on data the forecast system will encounter\nwhile in operation. Estimating the performance on data which\nwere already used to build or select the forecast system or any\nparts of it, including the ensemble interpretation methods, is\nlikely to give overoptimistic results. Ideally, the ensemble in-\nterpretation methods are trained on one part of the available\ndata, while the other part is left aside as test data. To get reli-\nable estimates of the out-of-sample performance, the test data\nset has to be sufficiently large. But typically, as the total amount\nof data available is already limited, we cannot afford to sac-\nrifice large proportions of the data for out-of-sample perfor-\nmance assessment, as a small training set is expected to provide\ninferior parameter values. We apparently face the problem of\nhaving either unrealistic parameters or unreliable estimates of\nperformance.\nA way around this apparent circulus vitiosus is cross-\nvalidation (see e.g. Hastie et al., 2001). The price to be paid\nthough is having to train the ensemble interpretation method a\nnumber of times rather than only once. More specifically, cross-\nvalidation works as follows. The training set T = {(x (n), Yn),\nn = 1 . . . N} is partitioned into J partitions T j of equal length\nN\/J. Let \u03b8 (j) be the parameter vector obtained by training the\nensemble interpretation method on T \\ T j , that is the training set\nwithout partition j. The score Sj for this particular \u03b8 (j) is eval-\nuated only on T j (i.e. the data that had been left out for finding\n\u03b8 (j)) and is given by\nSj := JN\n\u2211\nn\u2208Tj\n\u2212 log {p[Yn ; x(n), \u03b8 ( j)]}. (A1)\nTellus 60A (2008), 4\n676 J . BR \u00a8OCKER AND L. A. SMITH\nThe mean of all Sj is called the cross-validation estimate of the\nscore\nSCV := 1J\n\u2211\nj\nS j . (A2)\nThe standard error of SCV can be estimated thus\n\fSCV :=\n\u221a\n1\nJ (J \u2212 1)\n\u2211\nj\nS2j \u2212 S2CV. (A3)\nIn similar fashion, quantiles of the Sj can be computed to give\nconfidence intervals for the score. In the figures of Section 6,\nwe plotted the median score along with the 10\u201390% range as\nconfidence bars. In Hastie et al. (2001), using the standard error\nis recommended, but this gives obscure results if the distribution\nof the Sj is rather non-Gaussian.\nAn another way to estimate the likely variations of the score,\nreferred to as the OOT estimate, works as follows. Using the pa-\nrameters \u03b8 (j) obtained through cross-validation, we can compute\nthe OOT output by\n\u03c0n := p\n(\nYn ; x(n), \u03b8 ( jn )\n)\n, (A4)\nwhere jn denotes the index of the partition containing [Yn ,\nx(n)]. Recall that the sample [Yn, x(n)] was not used during the\ntraining of the particular parameter \u03b8 ( jn ). Using the OOT output,\nthe expected score:\nSOOT := 1N\n\u2211\nn\n\u2212 log(\u03c0n), (A5)\nand its standard error\n\fSOOT :=\n\u221a\n1\nN (N \u2212 1)\n\u2211\nn\nlog(\u03c0n)2 \u2212 S2OOT (A6)\ncan be computed. An easy calculation [comparing eqs. (A4, A5)\nwith (A1, A2)] reveals that SOOT is actually equal to SCV. The\nstandard errors, however, generally differ. For the OOT method,\nthe usual \u00b12 \u00b7 \fSOOT confidence intervals were employed. It\ndoes not make sense to use quantiles of \u2212log(\u03c0n) as confidence\nintervals for SOOT. It is hard to say which of the two methods\nis to be preferred, whence both were used for performance as-\nsessment. The CV method explicitly takes into account model\nvariations, but as the individual CV partitions are shorter than\nthe training set, the model variations are likely to be overesti-\nmated. The OOT technique uses the entire data set to estimate\nthe variations, but both model variations as well as performance\nvariations are compounded. Furthermore, the individual outputs\n\u03c0n are assumed to be independent, an idealization that leads to\nunderestimation of the variations.\nIt is often necessary to consider the improvement of the Igno-\nrance obtained by pn(y) over q(y). This improvement is naturally\nmeasured by the increase in Ignorance (also often referred to as\nthe relative Ignorance of pn(y) with respect to q(y)):\nSp \u2212 Sq = 1N\nN\u2211\nn=1\n\u2212 log [pn(Yn)] + log [q(Yn)] . (A7)\nThis quantity, as the estimate of the Ignorance proper, carries\nan uncertainty. It is important to realize that there is no simple\nrelationship between the uncertainty in Sp \u2212 Sq and the individual\nuncertainties in Sp and Sq , since both are highly dependent. In\nother words, the standard error of Sp \u2212 Sq is not in any simple way\nrelated to the individual standard errors of Sp and Sq . To estimate\nthe standard error of relative ignorance\u2019s through either cross-\nvalidation or OOT technique, the eq. (A3) (respectively, A6) has\nto be applied to the differences in the performance Sj between\nthe forecasts on each partition [respectively, the differences of\n\u2212 log (\u03c0n)].\nAll performance plots (Figs. 3\u20135) show relative Ignorance\n(either with respect to another forecast or with respect to cli-\nmatology). The cross-validation estimates are plotted in fat-line\nstyle, while the OOT estimates are in thin-line style. As noted\nabove, cross-validation and OOT differ only in their estimates\nof the standard error.\n10. Appendix B: On Ensemble Transforms\nIn this paper, we considered the interpretation of ensembles as\nthe problem of finding a map from a series of ensembles on to a\nseries of distribution functions for a corresponding series of ver-\nifications. The method of AKD provides a special class of such\nmappings by combining a simple kernel estimator (eq. 19) with\nwhat we termed an affine ensemble transform (eq. 20). This idea\ncould be generalized by using different ensemble transforms,\nprobably involving non-linear elements. A particular linear en-\nsemble transform was used in this paper, and there is the possibil-\nity that the concept is of wider applicability in post-processing\nensemble forecasts. To this end, ensemble transforms need to\nbe properly understood and classified first. At this point, we are\nnot even sure if the ensemble transform used in this paper is the\nmost general linear ensemble transform. In this appendix, some\nnecessary conditions will be formulated that we deem general\nensemble transforms should obey and are hopefully sufficient\nfor a conclusive analysis of the aforementioned question.\nThe key property of an ensemble, which distinguishes it from\na vector, is that it is still considered the same ensemble if some\nmembers are interchanged either across parts of or the entire\nensemble. For example, although the 50 perturbed members of\nthe ECMWF ensemble are distinguishable by the initial pertur-\nbations used to compute them, they can be considered indistin-\nguishable for the purpose of many applications. For the numeri-\ncal studies in Section 6, even the unperturbed (\u2018control\u2019) forecast\nwas considered indistinguishable from the perturbed ensemble\nmembers. Such an ensemble of mutually interchangeable mem-\nbers will be called a pure ensemble. Ensembles consisting of a\ncollection of pure ensembles (say, if we combine pure ensem-\nbles produced by different models) might be called compound\nensembles. All ensemble interpretation methods studied in this\npaper tread the ensembles as pure, as they are invariant to any\npermutation of the ensemble members.\nTellus 60A (2008), 4\nFROM ENSEMBLE FORECASTS TO PREDICTIVE DISTRIBUTION FUNCTIONS 677\nAn ensemble transform f is defined simply as a mapping be-\ntween ensembles (not necessarily having the same number of\nmembers). The key property of a (pure) ensemble, namely that\nthe ordering of the ensemble members is irrelevant, imposes cer-\ntain restrictions on f, which we are going to formulate. Let (as\nbefore) x = [x1 . . . xd ] be the original ensemble (consisting of d\nmembers) and\nz = f (x)\nbe the transformed ensemble (of d\u2032 members). If we now permute\nthe elements in x, then z must remain the same ensemble, which\nmeans, as we have seen, that at most some permutation of the\nelements of z should take place. In other words, if \u03c0 denotes a\npermutation of d elements and \u03c0x denotes the permuted original\nensemble, there must be a permutation \u03ba of the members of the\ntransformed ensemble z so that\n\u03baz = f (\u03c0x) (B1)\nholds.\nThe permutation \u03ba so obtained obviously depends on \u03c0 , or in\nother words, the relation (B1) defines a mapping \u03ba(\u03c0 ) between\npermutations. If \u03b9 is the identity, that is the permutation of d\nelements that actually keeps all elements the same, then likewise\n\u03ba(\u03b9) is the identity permutation (of d\u2032 elements). This relation can\n(with a slight abuse of notation) be written as\n\u03ba(\u03b9) = \u03b9. (B2)\nFurthermore, if \u03c01, \u03c0 2 are two permutations, a third permu-\ntation \u03c01 \u25e6 \u03c02 arises through composition of \u03c01, \u03c02. It follows\nimmediately from eq. (B1) that\n\u03ba(\u03c01 \u25e6 \u03c02) = \u03ba(\u03c01) \u25e6 \u03ba(\u03c02). (B3)\nProperties (B2) and (B3) state that any ensemble transform gives\nrise to a representation \u03ba of the group of permutations of d\nsymbols in the group of permutations of d\u2032 symbols.\nThe transformed ensemble z in eq. (B1) is not necessarily a\npure ensemble though, as it might be possible to split the mem-\nbers of z into two subensembles, z = [z1, z2], so that for any\npermutation \u03c0 , the corresponding permutation \u03ba(\u03c0 ) permutes,\nin fact, only the members of z1 and z2 among each other, but\ndoes not interchange members of z1 with members of z2. If this\nis the case, we have created a compound ensemble consisting of\n(at least) two pure ensembles. In order to exclude this behaviour,\nwe have to require that for any two indices i, j in the set [1 . . . d\u2032],\nthere is at least one permutation \u03c0 so that \u03ba(\u03c0 ) permutes i into\nj. Groups of permutations with this property are called transi-\ntive. Hence, the conclusion of this appendix can be summarized,\nthus:\nVia eq. (B1), an ensemble transform induces a transitive repre-\nsentation of the group of permutations of d symbols in the group\nof permutations of d\u2032 symbols.\nTransitive representations of the permutation groups have\nbeen widely studied and classified. Hence, by means of group\ntheory (Weyl, 1946), it should be possible to address questions\nlike whether the affine ensemble transform as presented in eq.\n(20) is the most general class of ensemble transforms which can\nbe obtained by linear operations.\nReferences\nBernardo, J. M. 1979. Expected information as expected utility. Ann.\nStat. 7(3), 686\u2013690.\nBro\u00a8cker, J. and Smith, L. A. 2007. Scoring probabilistic forecasts: the\nimportance of being proper. Weather Forecast. 22(2), 382\u2013388.\nGill, P. E., Murray, W. and Wright, M. H. 1982. Practical Optimization.\nAcademic Press, London.\nGneiting, T. and Raftery, A. 2007. Strictly proper scoring rules, predic-\ntion, and estimation. J. Am. Stat. Assoc. 102, 359\u2013378.\nGneiting, T., Raftery, A., Westveld, A. H. III and Goldmann, T. 2005. Cal-\nibrated probabilistic forecasting using ensemble model output statis-\ntics and minimum CRPS estimation. Mon. Weather Rev. 133, 1098\u2013\n1118.\nGood, I. J. 1952. Rational decisions. J. R. Stat. Soc. XIV(1), 107\u2013114.\nHamill, T. M. 2001. Interpretation of rank histograms for verifying en-\nsemble forecasts. Mon. Weather Rev. 6(14), 550\u2013560.\nHastie, T., Tibshirani, R. and Friedman, J. 2001. The Elements of Statis-\ntical Learning 1st Edition. Springer, New York.\nHoeting, J. A., Madigan, D., Raftery, A. and Volinsky, C. T. 1999.\nBayesian model averaging: a tutorial. Stat. Sci. 14(4), 382\u2013417.\nJewson, S. 2003a. Comparing the ensemble mean and the ensemble stan-\ndard deviation as inputs for probabilistic medium\u2013range temperature\nforecasts. arXiv:physics\/0310059v1 [physics.ao-ph].\nJewson, S. 2003b. Moment based methods for ensemble assessment and\ncalibration. arXiv:physics\/0309042v1 [physics.ao-ph].\nJohnson, R. A. and Wichern, D. W. 1992. Applied Multivariate Statistical\nAnalysis 3rd Edition. Prentice Hall, Englewood Cliffs, NJ.\nJolliffe, I. T. and Stephenson, D. B. (eds) 2003. Forecast Verification: A\nPracticioner\u2019s Guide in Athmospheric Science. John Wiley & Sons,\nLtd, Chichester.\nJudd, K. and Smith, L. A. 2001. Indistinguishable states I: the perfect\nmodel scenario. Physica 151, 125\u2013141.\nKelly, J. L. Jr. 1956. A new interpretation of information rate. Bell Syst.\nTech. J. 35, 917\u2013926.\nLorenz, E. N. 1963. Deterministic non-periodic flow. J. Atmos. Sci. 20,\n130\u2013141.\nMood, A. M., Graybill, F. A. and Boes, D. C. 1974. Introduction to the\nTheory of Statistics. McGraw-Hill Series in Probability and Statistics,\nMcGraw-Hill, New York.\nRaftery, A. E., Gneiting, T., Balabdaoui, F. and Polakowski, M. 2005.\nUsing Bayesian model averaging to calibrate forecast ensembles. Mon.\nWeather Rev. 133(5), 1155\u20131174.\nRichardson, D. S. 2003a. Economic value and skill Chapter 8. (eds I.\nT. Jolliffe and D. B. Stephenson). 165\u2013187.\nRichardson, D. S. 2003b. Predictability and Economic Value. Technical\nreport, European Centre for Medium Range Weather Forecast.\nRoulston, M. S. and Smith, L. A. 2003. Combining dynamical and sta-\ntistical ensembles. Tellus 55A, 16\u201330.\nRoulston, M. S. and Smith, L. A. 2002. Evaluating probabilistic\nforecasts using information theory. Mon. Weather Rev. 130, 1653\u2013\n1660.\nTellus 60A (2008), 4\n678 J . BR \u00a8OCKER AND L. A. SMITH\nRoulston, M. S., Kaplan, D. T., Hardenberg, J. and Smith, L. A. 2003.\nUsing medium range weather forecasts to improve the value of wind\nenergy producton. Renew. Energy 28, 585\u2013602.\nSelten, R. 1998. Axiomatic characterisation of the quadratic scoring rule.\nExp. Econ. 1, 43\u201362.\nSilverman, B. W. 1986. Density Estimation for Statistics and Data Anal-\nysis 1st Edition. Chapman and Hall, London.\nSmith, L. A. 1997. The maintenance of uncertainty. In: Proceedings of\nthe International School of Physics \u2018Enrico Fermi\u2019, Vol. CXXXIII,\nSocieta` Italiana di Fisica, Bologna, Italy, 177\u2013246.\nSmith, L. A. 2002. Predictability and chaos. In: Encyclopedia of At-\nmospheric Sciences. (eds J. Holton, J. Pyle and J. Curry). Academic\nPress, 1777\u20131785.\nSmith, L. A. 2007. A Very Short Introduction to Chaos. Oxford University\nPress, Oxford.\nTaylor, J. W. and Buizza, R. 2003. Using weather ensemble predictions\nin electricity demand forecasting. Int. J. Forecast. 19, 57\u201370.\nTennekes, H. 1988. The outlook: scattered showers. Bull. Am. Meteorol.\nSoc. 69(4), 368\u2013372.\nToth, Z., Talagrand, O., Candille, G. and Zhu, Y. 2003. Probability\nand Ensemble Forecasts chapter 7. (eds I. T. Jolliffe and D. B.\nStephenson). 137\u2013163.\nVapnik, V. N. 1998. Statistical Learning Theory. John Wiley & Sons,\nInc., New York.\nWang, X. and Bishop, C. H. 2004. Improvement of ensemble reliability\nwith a new dressing kernel. Q. J. R. Meteorol. Soc. 131, 965\u2013986.\nWeyl, H. 1946. The Classical Groups 2nd Edition. Princeton Mathemat-\nical Series. Princeton University Press, Princeton, NJ.\nWilks, D. S. 1995. Statistical Methods in the Athmospheric Sciences,\n1st Edition, International Geophysics Series Vol. 59. Academic Press,\nNew York.\nWilks, D. S. 2002. Smoothing forecast ensembles with fitted probability\ndistributions. Q. J. R. Meteorol. Soc. 128, 2821\u20132836.\nWilks, D. S. 2006. Comparison of ensemble\u2013MOS methods in the\nLorenz\u201996 setting. Meteorol. Appl. 13(3), 243\u2013256.\nWilks, D. S. and Hamill, T. M. 2007. Comparison of ensemble\u2013MOS\nmethods using GFS reforecasts. Mon. Weather Rev. 6(14), 2379\u2013\n2390.\nTellus 60A (2008), 4\n"}