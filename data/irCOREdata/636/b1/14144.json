{"doi":"10.1080\/09687760601129539","coreId":"14144","oai":"oai:generic.eprints.org:712\/core5","identifiers":["oai:generic.eprints.org:712\/core5","10.1080\/09687760601129539"],"title":"Beyond model answers: learners\u2019 perceptions of self-assessment materials in e-learning applications","authors":["Handley, Karen","Cox, Benita"],"enrichments":{"references":[{"id":195705,"title":"A review of computer-assisted assessment,","authors":[],"date":"2005","doi":"10.1080\/0968776042000339772","raw":"Conole, G. & Warburton, B. (2005) A review of computer-assisted assessment, ALT-J Research in Learning Technology, 13(1), 17\u201331.","cites":null},{"id":1042333,"title":"Accretion, tuning and restructuring: three modes of learning, in:","authors":[],"date":"1978","doi":null,"raw":"Rumelhart, D. & Norman, D. (1978) Accretion, tuning and restructuring: three modes of learning, in: J. W. Cotton & R. L. Klatzky (Eds)  Semantic factors in cognition  (Hillsdale, NJ, Lawrence Erlbaum Associates).","cites":null},{"id":449091,"title":"Artificial intelligence and tutoring system","authors":[],"date":"1987","doi":"10.1145\/49103.1046398","raw":"Wenger, E. (1987) Artificial intelligence and tutoring system (Los Altos, CA, Morgan Kaufmann).","cites":null},{"id":1042332,"title":"Assessing students: how shall we know them? (2nd edn)","authors":[],"date":"1987","doi":"10.2307\/1981340","raw":"Rowntree, D. (1987) Assessing students: how shall we know them? (2nd edn) (London, Kogan Page).","cites":null},{"id":1042325,"title":"Contextual issues in the construction of computer-based learning programs,","authors":[],"date":"2003","doi":"10.1046\/j.0266-4909.2003.00053.x","raw":"Leung, A. C. K. (2003) Contextual issues in the construction of computer-based learning programs, Journal of Computer Assisted Learning, 19, 501\u2013516.","cites":null},{"id":448524,"title":"Educational blogging,","authors":[],"date":"2004","doi":null,"raw":"Downes, S. (2004) Educational blogging, EDUCAUSE Review, 39(5), 14\u201326.","cites":null},{"id":449084,"title":"Engines for education (Hillsdale, NJ, Lawrence Erlbaum Associates).","authors":[],"date":"1995","doi":null,"raw":"Schank, R. C. & Cleary, C. (1995)  Engines for education  (Hillsdale, NJ, Lawrence Erlbaum Associates).","cites":null},{"id":448526,"title":"Epilogue, in: The mind\u2019s new science: a history of the cognitive revolution (New York, BasicBooks) (Original text published","authors":[],"date":"1987","doi":null,"raw":"Gardner, H. (1987) Epilogue, in: The mind\u2019s new science: a history of the cognitive revolution (New York, BasicBooks) (Original text published 1985).","cites":null},{"id":195703,"title":"Feedback and self-regulated learning: a theoretical Synthesis,","authors":[],"date":"1995","doi":"10.3102\/00346543065003245","raw":"Butler, L. B. & Winne, P. H. (1995) Feedback and self-regulated learning: a theoretical Synthesis, Review of Educational Research, 65(3), 245\u2013281.","cites":null},{"id":195697,"title":"Feedback for learning","authors":[],"date":"2000","doi":null,"raw":"Askew, S. (Ed.) (2000) Feedback for learning (London, Routledge).","cites":null},{"id":448528,"title":"Feedback in written instruction\u2014the place of response certitude,","authors":[],"date":"1989","doi":"10.1007\/bf01320096","raw":"Kulhavy, R. W. & Stock, W. A. (1989) Feedback in written instruction\u2014the place of response certitude, Educational Psychology Review, 1(4), 279\u2013308.","cites":null},{"id":1042331,"title":"Formative assessment: a cybernetic viewpoint, Assessment","authors":[],"date":"2005","doi":"10.1080\/0969594042000333887","raw":"Roos, B. & Hamilton, D. (2005) Formative assessment: a cybernetic viewpoint, Assessment in Education: Principles, Policy and Practice, 12(1), 7\u201320.","cites":null},{"id":1042329,"title":"Forms of intellectual and ethical development in the college years","authors":[],"date":"1970","doi":"10.1080\/03075078912331377723","raw":"Perry, W. G. (1970) Forms of intellectual and ethical development in the college years (New York, Holt, Rinehart & Winston).","cites":null},{"id":448529,"title":"Interviews: an introduction to qualitative research interviewing (Thousand Oaks, CA,","authors":[],"date":"1996","doi":"10.1016\/s1098-2140(99)80208-2","raw":"Kvale, S. (1996) Interviews: an introduction to qualitative research interviewing (Thousand Oaks, CA, Sage Publications).","cites":null},{"id":448527,"title":"Lay epistemics and human knowledge: cognitive and motivational bases","authors":[],"date":"1989","doi":"10.1007\/978-1-4899-0924-4","raw":"Kruglanski, A. W. (1989) Lay epistemics and human knowledge: cognitive and motivational bases (New York, Plenum Press).","cites":null},{"id":1042330,"title":"Learning through assessment, in:","authors":[],"date":"1991","doi":null,"raw":"Race, P. (1991) Learning through assessment, in: S. Brown & P. Dove (Eds) Self and peer assessment, Standing Conference on Educational Development, Birmingham, SCED Paper 63.","cites":null},{"id":1042327,"title":"Managers not MBAs: a hard look at the soft practice of managing and management development (London, FT\/Prentice Hall).","authors":[],"date":"2004","doi":"10.5465\/ame.2004.15268779","raw":"Mintzberg, H. (2004) Managers not MBAs: a hard look at the soft practice of managing and management development (London, FT\/Prentice Hall).","cites":null},{"id":449090,"title":"Mind in society: the development of higher mental processes","authors":[],"date":"1978","doi":"10.1525\/aa.1979.81.4.02a00580","raw":"Vygotsky, L. S. (1978) Mind in society: the development of higher mental processes (Cambridge, MA, Harvard University Press).","cites":null},{"id":448531,"title":"Multimedia and the learner\u2019s experience of narrative,","authors":[],"date":"1998","doi":"10.1016\/s0360-1315(98)00041-4","raw":"Laurillard, D. (1998) Multimedia and the learner\u2019s experience of narrative, Computers & Education, 31, 229\u2013242.36 K. Handley and B. Cox Laurillard, D. (2002) Rethinking university teaching: a framework for the effective use of educational technology (2nd edn) (New York, Routledge).","cites":null},{"id":195702,"title":"Opening mouths to change feet: some views on self and peer assessment, in:","authors":[],"date":"1991","doi":null,"raw":"Brown, S. & Dove, P. (1991) Opening mouths to change feet: some views on self and peer assessment, in: S. Brown & P. Dove (Eds) Self and peer assessment, Standing Conference on Educational Development, Birmingham, SCED Paper 63.","cites":null},{"id":449085,"title":"Process consultation: lessons for managers and consultants","authors":[],"date":"1987","doi":"10.5465\/ame.1987.4275845","raw":"Schein, E. H. (1987)  Process consultation: lessons for managers and consultants  (Reading, MA, Addison-Wesley).","cites":null},{"id":448530,"title":"Rethinking university teaching: a framework for the effective use of educational technology (1st edn)","authors":[],"date":"1993","doi":"10.4324\/9780203304846","raw":"Laurillard, D. (1993) Rethinking university teaching: a framework for the effective use of educational technology (1st edn) (New York, Routledge).","cites":null},{"id":448532,"title":"Rethinking university teaching: a framework for the effective use of educational technology (2nd edn)","authors":[],"date":"2002","doi":"10.4324\/9780203304846","raw":null,"cites":null},{"id":1042324,"title":"Situated learning: legitimate peripheral participation (Cambridge,","authors":[],"date":"1991","doi":"10.1017\/cbo9780511815355","raw":"Lave, J. & Wenger, E. (1991)  Situated learning: legitimate peripheral participation  (Cambridge, Cambridge University Press).","cites":null},{"id":195698,"title":"Social foundations of thought and action: a social cognitive theory (Englewood Cliffs,","authors":[],"date":"1986","doi":null,"raw":"Bandura, A. (1986) Social foundations of thought and action: a social cognitive theory (Englewood Cliffs, NJ, Prentice-Hall).","cites":null},{"id":449083,"title":"Survey research (London,","authors":[],"date":"1999","doi":null,"raw":"Sapsford, R. (1999) Survey research (London, Sage Publications).","cites":null},{"id":449086,"title":"Teaching machines,","authors":[],"date":"1958","doi":"10.1126\/science.1535-b","raw":"Skinner, B. F. (1958) Teaching machines, Science, 128, 969\u2013977.","cites":null},{"id":449087,"title":"The black hole of cyberspace. Available online at: www.rider.edu\/suler\/psycyber\/ psycyber.html (accessed 28","authors":[],"date":"1997","doi":null,"raw":"Suler, J. R. (1997) The black hole of cyberspace. Available online at: www.rider.edu\/suler\/psycyber\/ psycyber.html (accessed 28 July 2006).","cites":null},{"id":449089,"title":"The fundamentals of learning","authors":[],"date":"1932","doi":"10.1037\/10976-000","raw":"Thorndike, E. L. (1932) , The fundamentals of learning (New York, Teachers College Press).","cites":null},{"id":195700,"title":"The future of rational\u2013critical debate in online public spheres. Available online at: http:\/\/collegewriting.us\/barton\/Shared%20Documents\/RevisedArticle.doc (accessed 17","authors":[],"date":"2004","doi":null,"raw":"Barton, M. (2004) The future of rational\u2013critical debate in online public spheres. Available online at: http:\/\/collegewriting.us\/barton\/Shared%20Documents\/RevisedArticle.doc (accessed 17 July 2006) Black, P. & Wiliam, D. (2003) \u2018In praise of educational research\u2019: formative assessment, British Educational Research Journal, 29(5), 623\u2013637 Boud, D. (1995) Enhancing learning through self assessment (London, Kogan Page).","cites":null},{"id":195699,"title":"The instructional effect of feedback in test-like events,","authors":[],"date":"1991","doi":"10.3102\/00346543061002213","raw":"Bangert-Drowns, R. L., Kulik, C. L., Kulik, J. A. & Morgan, M. (1991) The instructional effect of feedback in test-like events, Review of Educational Research, 61(2), 213\u2013237.","cites":null},{"id":195704,"title":"The nature of expertise (Hillsdale, NJ, Lawrence Erlbaum Associates).","authors":[],"date":"1988","doi":null,"raw":"Chi, M. T. H., Glaser, R. & Farr, M. J. (Eds) (1988)  The nature of expertise  (Hillsdale, NJ, Lawrence Erlbaum Associates).","cites":null},{"id":448523,"title":"There\u2019s no confidence in multiple-choice testing, in:","authors":[],"date":"2002","doi":null,"raw":"Davies, P. (2002) There\u2019s no confidence in multiple-choice testing, in:  Proceedings of the 6th International CAA Conference, Loughborough University, 4\u20135 July, pp. 117\u2013130. Available online at: http:\/\/www.caaconference.com\/pastConferences\/2002\/proceedings\/davies_p1.pdf (accessed 11 August 2006).","cites":null},{"id":449088,"title":"To feedback or not to feedback in student self-assessment,","authors":[],"date":"2003","doi":"10.1080\/02602930301678","raw":"Taras, M. (2003) To feedback or not to feedback in student self-assessment,  Assessment and Evaluation in Higher Education, 28(5), 549\u2013565.","cites":null},{"id":195701,"title":"Transforming qualitative data: thematic analysis and code development (Thousand Oaks, CA,","authors":[],"date":"1998","doi":null,"raw":"Boyatzis, R. E. (1998) ,  Transforming qualitative data: thematic analysis and code development (Thousand Oaks, CA, Sage Publications).","cites":null},{"id":1042326,"title":"Up to the mark: a study of the examination game,","authors":[],"date":"1974","doi":null,"raw":"Miller, C. M. L. & Parlett, M. (1974) Up to the mark: a study of the examination game, Monograph 21 (Guildford, Society for Research in Higher Education).","cites":null},{"id":195696,"title":"Using information technology in learning: case studies in business and management education programs,","authors":[],"date":"2003","doi":"10.5465\/amle.2003.9901667","raw":"Alavi, M. & Gallupe, R. (2003) Using information technology in learning: case studies in business and management education programs, Academy of Management Learning and Education, 2(2), 139\u2013153.","cites":null},{"id":1042328,"title":"What does research say about the learners using computer-mediated communication in distance learning?,","authors":[],"date":"2002","doi":"10.1207\/s15389286ajde1602_1","raw":"Moore, M. G. (2002) What does research say about the learners using computer-mediated communication in distance learning?, The American Journal of Distance Education, 16(2), 61\u201364.","cites":null},{"id":448525,"title":"Wiki pedagogy. Available online at:","authors":[],"date":"2005","doi":null,"raw":"Fountain, R. (2005) Wiki pedagogy. Available online at: http:\/\/www.profetic.org:16080\/dossiers\/ rubrique.php3?id_rubrique=110 (accessed 20 July 2006).","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2007-03","abstract":"The importance of feedback as an aid to self\u2010assessment is widely acknowledged. A common form of feedback that is used widely in e\u2010learning is the use of model answers. However, model answers are deficient in many respects. In particular, the notion of a \u2018model\u2019 answer implies the existence of a single correct answer applicable across multiple contexts with no scope for permissible variation. This reductive assumption is rarely the case with complex problems that are supposed to test students\u2019 higher\u2010order learning. Nevertheless, the challenge remains of how to support students as they assess their own performance using model answers and other forms of non\u2010verificational \u2018feedback\u2019. To explore this challenge, the research investigated a management development e\u2010learning application and investigated the effectiveness of model answers that followed problem\u2010based questions. The research was exploratory, using semi\u2010structured interviews with 29 adult learners employed in a global organisation. Given interviewees\u2019 generally negative perceptions of the model\u2010answers, they were asked to describe their ideal form of self\u2010assessment materials, and to evaluate nine alternative designs. The results suggest that, as support for higher\u2010order learning, self\u2010assessment materials that merely present an idealised model answer are inadequate. As alternatives, learners preferred materials that helped them understand what behaviours to avoid (and not just \u2018do\u2019), how to think through the problem (i.e. critical thinking skills), and the key issues that provide a framework for thinking. These findings have broader relevance within higher education, particularly in postgraduate programmes for business students where the importance of prior business experience is emphasised and the profile of students is similar to that of the participants in this research","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/14144.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/712\/1\/ALT_J%2DVol15_No1_2007_Beyond_model_answers%2D__learners.pdf","pdfHashValue":"2c02dc8b916450fedd79476de50895c4cb51ad3a","publisher":"Taylor and Francis Ltd","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:712<\/identifier><datestamp>\n      2011-04-04T09:02:16Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/712\/<\/dc:relation><dc:title>\n        Beyond model answers: learners\u2019 perceptions of self-assessment materials in e-learning applications<\/dc:title><dc:creator>\n        Handley, Karen<\/dc:creator><dc:creator>\n        Cox, Benita<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        The importance of feedback as an aid to self\u2010assessment is widely acknowledged. A common form of feedback that is used widely in e\u2010learning is the use of model answers. However, model answers are deficient in many respects. In particular, the notion of a \u2018model\u2019 answer implies the existence of a single correct answer applicable across multiple contexts with no scope for permissible variation. This reductive assumption is rarely the case with complex problems that are supposed to test students\u2019 higher\u2010order learning. Nevertheless, the challenge remains of how to support students as they assess their own performance using model answers and other forms of non\u2010verificational \u2018feedback\u2019. To explore this challenge, the research investigated a management development e\u2010learning application and investigated the effectiveness of model answers that followed problem\u2010based questions. The research was exploratory, using semi\u2010structured interviews with 29 adult learners employed in a global organisation. Given interviewees\u2019 generally negative perceptions of the model\u2010answers, they were asked to describe their ideal form of self\u2010assessment materials, and to evaluate nine alternative designs. The results suggest that, as support for higher\u2010order learning, self\u2010assessment materials that merely present an idealised model answer are inadequate. As alternatives, learners preferred materials that helped them understand what behaviours to avoid (and not just \u2018do\u2019), how to think through the problem (i.e. critical thinking skills), and the key issues that provide a framework for thinking. These findings have broader relevance within higher education, particularly in postgraduate programmes for business students where the importance of prior business experience is emphasised and the profile of students is similar to that of the participants in this research.<\/dc:description><dc:publisher>\n        Taylor and Francis Ltd<\/dc:publisher><dc:date>\n        2007-03<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/712\/1\/ALT_J-Vol15_No1_2007_Beyond_model_answers-__learners.pdf<\/dc:identifier><dc:identifier>\n          Handley, Karen and Cox, Benita  (2007) Beyond model answers: learners\u2019 perceptions of self-assessment materials in e-learning applications.  Association for Learning Technology Journal, 15 (1).  pp. 21-36.  ISSN 0968-7769 (print)\/1741-1629 (online)     <\/dc:identifier><dc:relation>\n        10.1080\/09687760601129539<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/712\/","10.1080\/09687760601129539"],"year":2007,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"ALT-J, Research in Learning Technology\nVol. 15, No. 1, March 2007, pp. 21\u201336\nISSN 0968-7769 (print)\/ISSN 1741-1629 (online)\/07\/010021\u201316\n\u00a9 2007 Association for Learning Technology\nDOI: 10.1080\/09687760601129539\nBeyond model answers: learners\u2019 \nperceptions of self-assessment \nmaterials in e-learning applications\nKaren Handleya* and Benita Coxb\naOxford Brookes University, UK;  bImperial College London, UK\nTaylor and Francis LtdCALT_A_212885.sgm10.1080\/09687760601129539ALT-J, Research in Learning Technology0968 7769 (pri t)\/174 -1629 (onli e)Original Article2 07 & Fran is5 000Ma ch 2007K renHandleykhandley@b ookes.ac.uk\nThe importance of feedback as an aid to self-assessment is widely acknowledged. A common form\nof feedback that is used widely in e-learning is the use of model answers. However, model answers\nare deficient in many respects. In particular, the notion of a \u2018model\u2019 answer implies the existence of\na single correct answer applicable across multiple contexts with no scope for permissible variation.\nThis reductive assumption is rarely the case with complex problems that are supposed to test students\u2019\nhigher-order learning. Nevertheless, the challenge remains of how to support students as they assess\ntheir own performance using model answers and other forms of non-verificational \u2018feedback\u2019. To\nexplore this challenge, the research investigated a management development e-learning application\nand investigated the effectiveness of model answers that followed problem-based questions. The\nresearch was exploratory, using semi-structured interviews with 29 adult learners employed in a\nglobal organisation. Given interviewees\u2019 generally negative perceptions of the model-answers, they\nwere asked to describe their ideal form of self-assessment materials, and to evaluate nine alternative\ndesigns. The results suggest that, as support for higher-order learning, self-assessment materials that\nmerely present an idealised model answer are inadequate. As alternatives, learners preferred materials\nthat helped them understand what behaviours to avoid (and not just \u2018do\u2019), how to think through the\nproblem (i.e. critical thinking skills), and the key issues that provide a framework for thinking. These\nfindings have broader relevance within higher education, particularly in postgraduate programmes\nfor business students where the importance of prior business experience is emphasised and the profile\nof students is similar to that of the participants in this research.\nIntroduction\nFormative feedback is a vital part of learners\u2019 efforts to apply and practice the principles\nthat they learn: as Laurillard (1993, p. 61) has said, \u2018action without feedback is\ncompletely unproductive for a learner\u2019. Feedback is usually interpreted as material that\nassesses, verifies and comments on the learner\u2019s response to assignment questions. In\n*Business School, Oxford Brookes University, Wheatley, Oxford, OX33 1HX, UK. Email:\nkhandley@ brookes.ac.uk\n22 K. Handley and B. Cox\nsome educational settings, however, the possibilities for giving verificational feedback,\nfollowed by adaptive comments on errors or areas for improvement, is limited. This\nis the case both in self-paced e-learning (traditionally known as computer-aided\ninstruction [CAI]) and also occasionally in large-scale lectures where lecturers may\nresort to rhetorical question-and-answer sessions addressed to the entire audience.\nIn e-learning, the desire to provide verification has tended to produce one of two\napproaches: either a proliferation of multiple choice questions (MCQs) amenable to\ncomputer-aided assessment; or the transfer of \u2018complex\u2019 forms of assessment to elec-\ntronic discussion forums where academic staff can respond to individual postings\nwith the benefit of their expertise and understanding. Increasingly, these approaches\nare combined as newer pedagogic technologies become integrated with the old, and\napproaches are blended to balance the advantages of one with the disadvantages of\nothers. Furthermore, recent technologies such as blogging and wikis have expanded\nthe repertoire of communication tools to promote student reflection and knowledge\nintegration (for example, Downes, 2004; Fountain, 2005). Nevertheless, there\ncontinues to be a reliance on MCQ quizzes (despite pedagogic limitations) and on\nstaff-moderated discussion forums (despite resource constraints).\nWith respect to MCQs and similar types of binary exercises, Schank and Cleary\n(1995) have derided the over-use of this method of assessment because it risks\n\u2018dumbing down\u2019 students\u2019 learning experience. The reliability and validity of MCQs\nhave also been questioned (Davies, 2002; Conole & Warburton, 2005). Validity is a\nparticular problem when assessing students\u2019 higher-order learning that has tradi-\ntionally relied on essays or short-answer questions (Rowntree, 1987). However, the\nlimitations of artificial intelligence technologies such as natural language processing\nmeans that computational assessment of free-text continues to be problematic\n(Gardner, 1987).\nThe second approach, which we have broadly categorised as the use of discussion\nforums, is not without its own problems. The principal one concerns resourcing: staff\n(and indeed students themselves; Moore, 2002) often feel they do not have the time\nto respond carefully and individually to each posting because of other demands on\ntheir time. This means that students may be left without feedback\u2014a \u2018black hole\nexperience\u2019 in cyberspace (Suler, 1997).\nThe twin problems of limited staff resources to moderate discussion forums, and\nthe deficiencies of MCQs and artificial intelligence technologies, continue to create\nchallenges for staff engaged in designing materials for students. A conventional\nresponse to these challenges has been to offer students \u2018model answers\u2019 to enable\nthem to self-assess their own work (for example, Laurillard, 1998) rather than give\nadaptive feedback that provides an external assessment. The proposed benefits of\nself-assessment are that students are encouraged and empowered to develop their\nown critical faculties rather than rely on the authoritative judgement of external\nsources such as academic staff (Boud, 1995). However, model answers are deficient\nin many ways, particular in their presumption of a single correct answer regardless of\ncontext, aims, personal characteristics, and so on. This reductive assumption is rarely\nthe case with complex problems designed to test students\u2019 higher-order learning.\nLearners\u2019 perceptions of self-assessment materials 23\nNevertheless the challenge remains of how to develop students if the conventional\nform of verificational, adaptive feedback cannot be computer generated.\nTo address this challenge, the research presented in this article investigates the\neffectiveness (and deficiencies) of model-answer feedback, and identifies alternative\nmaterials that can facilitate students\u2019 self-assessment. The research was exploratory,\nusing semi-structured interviews with 29 adult learners who used a seven-hour CAI\napplication to help develop their project management skills. The application included\ntutorials and a number of activities, which combined open questions with model\nanswers as \u2018feedback\u2019. The model answers did not provide verification in the form of\n\u2018correct\u2019 or \u2018incorrect\u2019. Dissatisfaction with model answers quickly became apparent\nduring early interviews, and so the research re-focused its efforts on two questions:\nwhat made feedback \u2018useful\u2019 (whether verificational or not), and what alternatives to\nmodel-answer feedback could be provided.\nLiterature review\nThe importance of formative feedback is widely acknowledged as a critical input to\nthe process of learning (Butler & Winne, 1995; Askew, 2000; Laurillard, 2002; Black\n& Wiliam, 2003; Taras, 2003; Roos & Hamilton, 2005). However, educational theory\ndiffers in its articulation of the function of adaptive feedback and self-assessment mate-\nrials such as model answers. In her review of contemporary feedback paradigms,\nAskew (2000, pp. 3\u201315) distinguishes between three paradigms: receptive-transmis-\nsion, constructivist, and co-constructivist. In the receptive-transmission paradigm, the\n\u2018expert\u2019 gives corrective or explanatory information to students to help them learn \u2018the\ntruth\u2019. In the constructivist paradigm, experts engage in an \u2018expanded discourse\u2019 to\nhelp students gain new understandings, but cannot dictate what those understandings\nwill be since students are influenced by a number of sources. In the co-constructivist\nparadigm, the dichotomy of expert and novice dissipates and there is recognition that\nthe \u2018teacher\u2019 also learns from the \u2018student\u2019 through dialogue and participation in\nshared practices (Lave & Wenger, 1991). In an older literature on \u2018programmed\ninstruction\u2019 inspired by behaviourists such as Skinner (1958) and Thorndike (1932),\nfeedback was viewed as a reinforcement mechanism: either rewarding desirable behav-\niour, or \u2018punishing\u2019 or withdrawing rewards if the student made errors. Emphasis was\ngiven to behaviour that could be observed and possibly shaped through a schedule of\nfeedback interventions.\nThe cognitive science revolution of the 1970s and early 1980s firmly swept aside\nthe behaviourist paradigm and refocused attention on the mind and its purported\nparallels with computational and symbolic processes. Inspired by these new ideas, the\ne-learning literature on intelligent tutoring systems and CAI became firmly grounded\nin the receptive-transmission feedback theories (for example, Wenger, 1987). As a\nconsequence, intelligent tutoring systems and CAI designers tended\u2014and to some\nextent still do\u2014to espouse the MCQ and its binary equivalents because they could be\nused to \u2018diagnose\u2019 and then \u2018correct\u2019 students\u2019 faulty knowledge. This assumption is\nembedded in seminal feedback models such as that of Kulhavy and Stock (1989) and\n24 K. Handley and B. Cox\nBangert-Drowns et al. (1991). The problem with \u2018model\u2019 answers as a form of self-\nassessment is that they, too, imply a receptive-transmission pedagogy that is consid-\nerably out of favour in current academic debates.\nIn the context of self-paced learning, a more appropriate way to conceptualise e-\nlearning feedback and self-assessment materials may therefore be one that\nacknowledges the role of self-regulated learning, and that then investigates the type\nof self-assessment material that facilitate that process. In this regard, a classic\nmodel of self-regulated learning that can be applied to e-learning is that of Butler\nand Winne (1995). They argue that although learners tend to seek out external\nsources of feedback (e.g. tutor comments), the most effective students also gener-\nate internal feedback by monitoring their performance against self-generated or\ngiven criteria. It is in this connection that self-assessment becomes particularly\nimportant.\nThe pedagogic benefits of self-assessment have been widely discussed. Brown and\nDove (1991, p. 59), for example, cite the advantages of student motivation,\nexchange of ideas, transferable personal skills and the development of a community\nof learning. However, self-assessment comes with its own cautions and challenges;\nfor example, it can be demanding and time-consuming, and students may lack the\ncapability to understand and apply assessment criteria (Brown & Dove, 1991, p. 61).\nSelf-assessment \u2018in its more rigorous form\u2019 involves learners not only in evaluating\ntheir work, but also in identifying and specifying the criteria and standards which\nshould be applied to it (Race, 1991, p. 7). Here we see the developmental potential\nof self-assessment methods, which is \u2018more transformational, elusive and confronting\nto conventional teaching than it is normally expedient to recognise\u2019 (Boud, 1995,\np. 1). This is because the process of identifying criteria\u2014especially if these are\ndebated openly with tutors\u2014acknowledges the socio-constructivist process of scaf-\nfolding student learning to broadly fit the norms of the educational community to\nwhich they belong (see also Vygotsky, 1978).\nThe experience of many students using stand-alone CAI, WebCT quizzes, and so\non (albeit as one among many elements of learning and teaching) is that the predom-\ninant forms of activity are the closed question plus MCQ, or the open question plus\nmodel answer. However, given the deficiencies of MCQs and model answers, which\nhave been discussed above, the need for alternative ways to facilitate student self-\nassessment is particularly important.\nResearch study aims and objectives\nIn view of the relative lack of prior empirical research on alternatives to model\nanswers following students\u2019 short-answer responses, this research aimed to investi-\ngate learners\u2019 perspectives on what constitutes effective non-verificational e-feedback.\nTo address this aim, three research objectives were identified: \n1. Obtain learners\u2019 perceptions of examples of model answer e-feedback taken from\nan e-learning application they have recently completed.\nLearners\u2019 perceptions of self-assessment materials 25\n2. Solicit their views on what constitutes \u2018useful\u2019 e-feedback.\n3. Solicit their perceptions and evaluations of alternatives to model-answer feedback.\nResearch methods\nThe research adopted an exploratory approach relying mainly on semi-structured\ninterviews, asking learners about specific examples of feedback as well as hypothetical\nexamples. The research participants were selected from a global consulting organisa-\ntion, \u2018ProfCo\u2019. ProfCo was selected because it produced high-quality, innovative CD-\nROM applications for its employees.\nIt was not feasible (nor an intention) to make statistical generalisations covering the\nwhole population in ProfCo, because interviews were in depth and were with a\nsmaller (n = 29) number of people than would be the case in large-scale hypothetico-\ndeductive research. Nevertheless, the risk of systematic bias was reduced by adopting\na time-based strategy (Sapsford, 1999) whereby all learners based in the UK and US\noffices who had completed Zentoria within three weeks prior to requesting an inter-\nview were selected for interview. The 29 interviewees were then stratified according\nto their level of expertise, because prior research (Chi et al., 1988) indicates that\nexpertise is an important mediator of learner expectations and therefore also of their\nevaluations of educational materials. Level of expertise was attributed to learners\nusing the proxy of their managerial \u2018grade\u2019, where one represented \u2018new entrant\u2019 and\nseven represented \u2018partner\u2019. New entrants were typically in their early 20s while\nconsultants were often in the early 30s. Thus the profile of younger research partici-\npants resembles that of postgraduate Masters-level programmes (e.g. M.Sc. in\nManagement), while the profile of consultants resembles that of Executive Masters\nand short-course programmes, which are a growing area of higher education.\nA mixed methodology was used, and methods were selected for their appropriate-\nness to each of the three objectives. Given the small sample size, no statistical gener-\nalisations have been made; nevertheless, we present some numerical data for\ninformational purposes, recognising that the data are indicative only.\nObtaining learners\u2019 perceptions of specific examples of model answer e-feedback\nThe first objective was to ascertain learners\u2019 perceptions of \u2018real\u2019 model-answer feed-\nback, based on their prior experience of an authentic e-learning application. The appli-\ncation selected for this purpose was Zentoria, which was designed specifically for the\nparticipating organisation with the aim of developing learners\u2019 project management\nskills. The storyline running through Zentoria was of a project team of management\nconsultants helping their client to develop an environmentally friendly car. The\napplication was created using Macromedia Director and Adobe Photoshop by a team\nof information technology developers and educational designers. The seven-hour\ncourse could be taken at the learner\u2019s own pace, and was structured around two\nelements: tutorials, which provided a grounding in project management principles; and\nactivities (MCQs and open questions), which portrayed problems typical of project\n26 K. Handley and B. Cox\nmanagement. The application included a \u2018Scratchpad\u2019\u2014a notebook facility in which\nlearners could type their short-answer responses to open questions. On finishing their\nresponses, learners were presented with model answers that varied in several ways.\nThe 29 interviewees were asked about their experiences of completing three open-\nquestion activities. These activities (scenario > task > type of \u2018feedback\u2019) are\nsummarised in Table 1. Semi-structured interviews were conducted to elicit inter-\nviewees\u2019 views on the activities, and particularly on the model-answer e-feedback.\nInterviews were analysed using \u2018meaning interpretation\u2019 and \u2018meaning categorisation\u2019\napproaches (Kvale, 1996), as well as open-coding, constant comparative methods,\ndeviant case analysis, tabulations and a modified form of inter-rater cross-checks.\nAnalysis was facilitated through the use of Microsoft Excel, and NVivo.1\nThe qualitative data analysis package NVivo was used to collate and then analyse\ncomments from interviewees. Analysis was facilitated by developing a \u2018coding\ntemplate\u2019 to categorise interviewees\u2019 comments according to theme. Some codes were\nidentified from the theoretical literature (e.g. the importance of \u2018student engage-\nment\u2019), but most were developed inductively by reading and re-reading the tran-\nscripts, or by listening to the audio-tapes. Selected extracts were discussed with two\ncolleagues engaged in similar research in order to clarify and improve consistency of\ninterpretations. This strategy provided a balance between seeking full inter-rate reli-\nability (which is difficult if not undesirable in qualitative research because of the\n\u2018tyranny of the lowest common denominator\u2019; Kvale, 1996, p. 181), and reliance on\none person\u2019s subjective interpretations.\nMicrosoft Excel was used to analyse numerical data. This was done in the final\npiece of research where learners were asked to comment on nine self-assessment\ndesigns. Comments were transformed into a rating scale of \u22122 to +2 according to the\nstrength and direction (favourable or not towards the design) of those comments\n(Boyatzis, 1998).\nSoliciting learners\u2019 views on what constitutes \u2018useful\u2019 e-feedback\nAs will be discussed later, many learners expressed dissatisfaction with the model\nanswer e-feedback in Zentoria. From the perspective of this research, a different\nTable 1. Summary of activities used to ascertain learners\u2019 perceptions of model answers in \nZentoria\nActivity reference Scenario Task Feedback\nZEN1 Unauthorised extension to \nproject scope\nOpen question Model answer\nZEN2 Loss of confidence by project \ndirector in the calibre of the \nproject team\nOpen question, \naccompanied by a \nfinished example of a \nsimilar open question\nModel answer\nZEN3 Preparation for the first \nmeeting of a new project team\nOpen question Presentation of some of \nthe issues to consider\nLearners\u2019 perceptions of self-assessment materials 27\ninterviewing strategy was required if alternative feedback designs were to be identi-\nfied. Therefore the interview schedule was supplemented with new questions, asking\ninterviewees what made feedback \u2018useful\u2019, and what they perceived was generally\n\u2018missing\u2019 from the Zentoria feedback. These qualitative data were analysed using the\nmethods described above.\nSoliciting learners\u2019 evaluations of alternative non-verificational e-feedback designs\nThe third research objective was developed once it became apparent that learners\nwere dissatisfied with model answers. In addition to asking for their general opinions\nabout \u2018useful\u2019 feedback, the remaining interviewees were presented with further\nmaterial in the form of descriptions of nine hypothetical feedback designs that could\nfollow short-answer open questions (Table 2). The designs were presented to inter-\nviewees for evaluation and comment. Qualitative responses were transformed into a\nrating scale of \u22122 to +2 as described above. Where no comments were made, a zero\nranking was recorded.\nResults\nLearners\u2019 perceptions of specific examples of model answer e-feedback\nWhen asked about the model-answer feedback, many learners recalled their disap-\npointment about the lack of assessment and verification. Most, however, also recog-\nnised that automated verificational feedback on free-text answers was an unrealistic\nexpectation. Over and above these generally shared sentiments, learners varied some-\nwhat in the way they responded to the model answers: as a new piece of instruction,\nas reinforcement for existing knowledge, or as a prompt to \u2018tune\u2019 existing mental\nschemata.\nTable 2. Nine hypothetical non-verificational feedback designs evaluated by interviewees\nShort-hand name Description given to interviewees\nMA Model answer\nKD&D Key dos and don\u2019ts\nExmples Several real-life examples, each describing a project situation, the \nintervention and the outcome\nBP Description of the formal, best practice intervention\nIssues Explanation of the underlying issues behind the problem\nExp. thnk proc Description of how an expert would think through the problem\nConsequences Video showing the consequences of several problem interventions\nExp. P&C Video showing an expert comparing the pros and cons of several problem \ninterventions\nPeer debate Video showing peers in an informal setting debating the merits of several \nproblem interventions\n28 K. Handley and B. Cox\ne-Feedback as a new piece of instruction.   Many learners stopped typing free-text\nanswers when they realised that the e-feedback in Zentoria was non-verificational.\nInstead, they read the e-feedback as a new piece of instructional text.\ne-Feedback as reinforcement.   For some interviewees, non-verificational feedback was\nuseful in re-enforcing their views. For example: \n\u2026 it helped to crystallise some ideas that weren\u2019t fully formed in my mind. (Z8)\ne-Feedback as a prompt for reflection and self-assessment.   For a few interviewees, and\nmainly those already experienced in project management, the model answers acted as\na prompt for further reflection and self-assessment. By thinking through the solutions\nand explanations given in the model answer, learners felt that they \u2018tuned\u2019 and\nimproved their existing knowledge schemata (Rumelhart & Norman, 1978). The\nfeedback helped them to achieve a sense of closure to the questions raised in their\nminds by doing the task, while at the same time validating and modifying their current\nways of thinking about the type of problem represented.\nHowever, only a few learners experienced this sense of reflection and closure. An\nimportant differentiating factor seemed to be their tolerance of the ambiguity of\nthe feedback. Some interviewees were tolerant of feedback that did not verify their\nown answers. These learners seemed satisfied with feedback that offered \u2018food for\nthought\u2019\u2014new ideas, perspectives and possibilities that allowed them to assess the\nfeasibility and applicability of their own \u2018solution\u2019 to the problem task and that\nadded richness to their existing ways of thinking about the problem. Although the\nmodel answers gave no \u2018feed back\u2019 in the true cybernetic sense, they did promote\nself-generated learning.\nLearners\u2019 views about what constitutes useful feedback\nHaving asked learners about three specific instances of model-answer feedback, they\nwere asked two supplementary questions and invited to talk about their generic\nexpectations of e-learning feedback following MCQs or short-answer questions. The\nfirst question was \u2018what makes feedback useful?\u2019 This was asked in recognition of the\nvital importance of understanding student expectations of e-learning feedback; indeed,\nAlavi and Gallupe (2003) argue that the management of student expectations is a key\nprinciple of technology-mediated learning. In response to the question, interviewees\ncommented mainly on the instructional function of feedback\u2014what they wanted it to\n\u2018do\u2019. The responses were analysed and grouped into six categories, which are\npresented in Table 3 using the terminology of instructional design, enriched by inter-\nviewees\u2019 own comments. The prevalence of each category of comment is given in the\nfinal column.\nMany interviewees cited several functions of e-feedback, and thus the total for\ncomments exceeds the number of interviewees. Comments indicate that e-feedback\nLearners\u2019 perceptions of self-assessment materials 29\nwas more frequently perceived as useful for verifying and reinforcing existing knowl-\nedge or explaining concepts, rather than facilitating more radical knowledge\n(re)construction by offering alternative insights, reflections and perspectives.\nHowever, a different picture emerged when interviewees were asked what was \u2018miss-\ning\u2019 from the model-answer feedback they received in Zentoria, and what they would\nhave gained from a (human) tutor. A wide range of responses were given, including\n\u2018what-if scenarios\u2019, \u2018behaviours to avoid\u2019, \u2018sense of realism\u2019, \u2018alternative perspectives\u2019\nand \u2018opportunity for debate\u2019.\nAn analysis of results that address the first two objectives of the research study\nsuggests that, overall, learners held two key expectations of e-feedback, which at\nfirst sight seem contradictory: firstly, the expectation for more verificational feed-\nback (implying a \u2018right\u2019 answer); and secondly, the expectation for more discursive\nand relativistic feedback. The latter expectation was indicated by comments recom-\nmending a move away from a theoretically \u2018model\u2019 answer, towards feedback that\nexemplified different ways of applying theory to practice. Interviewees\u2019 desire for\nalternative perspectives and \u2018what-ifs\u2019 suggested an expectation for less dualistic,\ncausal-explanatory feedback. By comparison, the primary expectation\u2014for verifica-\ntional feedback\u2014seemed somewhat of a paradox. This apparent paradox becomes\nunderstandable if one considers the mediating variable of \u2018experience\u2019; for example,\nthat less experienced students seemed most likely to want to know the behaviours\nthey should avoid.\nTable 3. Illustrations of interviewees\u2019 expectations of \u2018useful\u2019 feedback\nInstructional function \nof feedback Illustration of interviewees\u2019 expectations\nCited by \ninterviewees (n)\nVerify \u2018\u2026 confirms what you in your own mind mentally \nhave come up with\u2019 (Z6)\n34\nReinforce \u2018[Feedback] is very useful for confirming your ideas, \nand helping you -, you know if you think of something \non your own and then it gets confirmed, it helps you \nremember that point\u2019 (Z7)\n26\nCorrect errors, or state \ncorrect behaviours\n\u2018\u2026 straight answers \u2026 like \u201cdo this in this situation\u201d, \nfor example. In the real world of course it may not \nturn out that way but maybe [it\u2019d be nice to have] a \nclassic case like \u201cthis is what you would do\u201d. That\u2019d \nbe nice so that we don\u2019t have to figure it out on our \nown\u2019 (Z17)\n21\nInform \u2018\u2026 contains all the points. It\u2019s more of a checklist\u2019 \n(Z18)\n10\nExplain \u2018\u2026 if it\u2019s possible to get the \u201cwhy\u201d in there. If you\u2019re \nnot right, are you on the right track? or \u2026 or \u201cgee \nthat\u2019s a common mistake, but \u2026\u201d\u2019 (Z20)\n31\nPromote reflection \u2018\u2026 not rehash the things that are basic, but really to \ngive a more stretching analysis\u2019 (Z19)\n14\n30 K. Handley and B. Cox\nLearners\u2019 evaluations of alternative non-verificational feedback designs\nTo probe the apparent contradictions in the results, 26 of the 29 interviewees were\nasked to comment on nine hypothetical non-verificational e-feedback designs that\nwere developed following analysis of early interviews. Interviewees were asked how\neffective the designs would be as a follow-on from learners\u2019 short-answer responses to\nopen questions. The nine designs are described in Table 2. Interviewees\u2019 extrapolated\nratings are summarised in Figure 1, which show that, overall, the highest average\nrating was given to materials giving \u2018several real-life examples\u2019. The lowest average\nrating was given to \u2018model answer\u2019. Given the exploratory nature of this part of the\nresearch, numerical analysis does not generate statistical generalisations; neverthe-\nless, we provide some quantitative indicators where this may help the reader gauge\nthe range of responses (Table 4). We also present a summary of interviewees\u2019\ncomments for each design.\nFigure 1. Averaged and ranked interviewee ratings for hypothetical non-verificational feedback designs ( n = 26)\u2018Real-life examples\u2019 had the highest overall ranking of the nine designs, with learners\npreferring \u2018real-life examples\u2019 over feedback, which was \u2018too textbook-like\u2019 (Z22).\nLearners wanted \u2018authenticity\u2019 (see also Leung, 2003). However, some interviewees\n(e.g. Z26) tempered their positive comments with qualifying remarks. \nWith the real-life examples, they\u2019re kind of a fairy tale and it\u2019s oftentimes hard to give\nenough background information \u2026 to present a complex issue in the time available. (Z26)\nAn \u2018expert\u2019s process of thinking through the problem\u2019 was ranked highly by most inter-\nviewees, but was ranked only sixth by grade-two junior consultants. This result is not\nsurprising if one considers that an emphasis on process and on reasoning skills is\nExmples Several real-life examples\nExp proc Description of how an expert would\nthink through the problem\nKD&D Key do's and don'ts\nCons Video: consequences of several\nproblem interventions\nIssues Explanation of the underlying\nissues behind the problem\n \nExp P&C Video: expert comparing the pros\nand cons of several problem\ninterventions\nBP Description of the formal, best\npractice intervention\nPeers Video: peers debating the merits of\nseveral problem interventions\nModel answer\n \n0.0\n0.5\n1.0\n1.5\n2.0\nE\nxm\npl\ns\nE\nxp\n p\nro\nc\nK\nD\naD\nC\non\ns\nIs\nsu\nes\nE\nxp\n P\n&\nC\nB\nP\nP\nee\nrs M\nA\nNon-verificational feedback designs\nA\nve\nra\nge\n r\nat\nin\ng\nMA\nFigure 1. Averaged and ranked interviewee ratings for hypothetical non-verificational feedback \ndesigns (n = 26)\nLearners\u2019 perceptions of self-assessment materials 31\nembedded in the ethos of many management consultancy firms. Many firms\nsubscribe to what Schein (1987) referred to as \u2018process consulting\u2019: they apply stan-\ndard methodologies to evaluate the effectiveness of processes in client operations, and\ngenerally provide solutions that impact at the level of those client processes. Findings\nindicate that experts were admired because they were credited with understanding\nthese processes. However, inexperienced interviewees, perhaps because they had not\nyet been socialised into the organisation and had not yet adopted a processual\napproach to problems, ranked the \u2018expert-thinking process\u2019 feedback only sixth.\n\u2018Key dos and don\u2019ts\u2019 were perceived as useful by experienced as well as inexperi-\nenced learners. An advantage of this type of feedback is that it explicitly warns of\nbehaviours to avoid. \nThe key dos and don\u2019ts would be very good because then you know what is wrong, and\nthose types of things stay with you. (Z23, emphasis in original speech)\n\u2018Consequences\u2019 feedback was highly valued by the more experienced interviewees\n(grade five, ranked third; grade four, ranked third) and modestly valued by the less\nexperienced (grade three, ranked fifth; grade two, ranked fourth). This type of feed-\nback seemed to fulfil two main functions. The first was to promote social learning\n(Bandura, 1986) through vicarious experience. There was also recognition that,\nunless well done, consequence feedback could be \u2018cheesy\u2019 (Z7) (if using video) or\nTable 4. Description, average ratings and overall ranking of nine non-verificational feedback \ndesigns\nReference Description\nAverage rating between \n\u20132 and +2 (total sample \nn = 26)\nOverall \nranking\nExmpls Several real-life examples, each describing a \nproject situation, the intervention and the \noutcome\n1.24 First\nExp proc Description of how an expert would think \nthrough the problem\n1.19 Second\nKD&D Key dos and don\u2019ts 1.00 Third\nCons Video showing the consequences of several \npossible interventions\n0.88 Fourth\nIssues Explanation of the underlying issues behind \nthe problem\n0.65 Fifth\nExp P&C Video showing an expert comparing the pros \nand cons of several possible interventions\n0.62 Sixth\nBP Description of the formal, best practice \nintervention\n0.46 Seventh\nPeers Video showing peers in an informal setting \ndebating the merits of several possible \ninterventions\n0.29 Eighth\nMA Model answer 0.26 Ninth\n32 K. Handley and B. Cox\n\u2018something to laugh about\u2019 (Z15). Several interviewees suggested that the value of\nconsequence feedback would be enhanced if supplemented by comments from an\nexpert.\n\u2018Issues\u2019 underlying the given problem, provided by an expert, was ranked fifth over-\nall. For inexperienced learners it was ranked last, for example, because they saw \u2018issues\u2019\nas being too dependent on the circumstances of the specific problem. Interviewees with\nmore knowledge and experience were more accommodating, and saw issue-based\nfeedback as a useful prompt for reflection and for understanding the rationale behind\ndecisions.\nIt is possible that interviewees who were positive about \u2018issues\u2019 feedback recognised\nthat issues tend to be repeated across multiple problems. They become a way of\nindexing experiences of problems and their solutions, and of recalling them when\nanalogous problems arise. Issues-based thinking is one example of a more structured,\n\u2018critical\u2019 method of thinking that students develop during their time at university.\n\u2018Experts discussing pros and cons\u2019 was ranked low by most interviewees. This was\nsurprising if compared with the other \u2018expert\u2019 feedback, which related to the process\nof solving problems. This discrepancy was possibly because although experts were\ncredited with using effective processes for thinking through problems, the culmination\nof their thinking process\u2014the solution\u2014was not credited with being necessarily correct.\n\u2018Best practice\u2019 scored a low average rating, for reasons similar to those given for the\nmodel answer. For some interviewees, however, best practice feedback was welcomed\nprecisely because it established the organisation\u2019s \u2018policy\u2019 against which actual prac-\ntice could be benchmarked.\nFor some interviewees, \u2018peer debate\u2019 was perceived as useful because peers would\nadvocate new ideas and perspectives that could be reflected on. However, several\ninterviewees suggested that peer-debate feedback would create confusion unless\n\u2018controlled or even scripted to be sure to get the appropriate points across\u2019. This\nsuggests that interviewees were uncomfortable having to deal with divergent opinions.\n\u2018Model-answer feedback\u2019 was ranked the lowest of the nine designs. Interviewees\nimplied that model answers were inappropriate for complex questions or topics:\n\u2018Getting the perfect answer is not always great, because it\u2019s not a perfect world\u2019 (Z16).\nDiscussion\nAnalysis shows that this group of learners were generally unsatisfied with model\nanswers as a response to their short-answer responses, and that only the more reflec-\ntive learners used them for self-assessment. As alternatives to model answers, inter-\nviewees gave a number of suggestions about the sort of responses they would prefer\nin the context of a stand-alone element of a course; for example, learning what to\n\u2018avoid\u2019 and not just what to \u2018do\u2019; learning about process (e.g. how to think through a\nproblem) and not just action (e.g. what to do in a specific situation); and learning by\nhearing of other people\u2019s experiences and not just from theory. While the importance\nof these situated aspects are recognised in the educational literature, the principles are\nrarely applied to the design of e-feedback in CAI applications.\nLearners\u2019 perceptions of self-assessment materials 33\nWhat this group of learners seemed to desire at the end of each exercise was a sense\nof \u2018closure\u2019. A definitive single, correct answer was not necessarily required (see also\nKruglanski, 1989) nor deemed appropriate if learners recognised the complex and\nsituated nature of the problem. In those circumstances, a programmed response that\naddressed the task in terms of relevant issues, alternative perspectives, or the process\nof resolving the problem were sometimes considered more valuable because they\nprompted reflection leading to a deeper understanding of the problem. However, the\nnumber of learners who adopted this perspective was relatively few. Those who did\ntended to have sufficient expertise and self-confidence to rely on their own judge-\nments about whether their response was \u2018right\u2019. They would then reflect on what else\nthey could learn from the model-answer feedback. Inexperienced learners, on the\nother hand, wanted first and foremost to know whether they were \u2018right\u2019, because they\ndid not have the judgemental maturity with which to make a self-assessment. Here\nwe see parallels with the work of Miller and Parlett (1974) drawing on the research\nby Perry (1970) on the intellectual development of students. These authors identified\nstudent transitions from a \u2018cue-deaf\u2019 (dualist) orientation in which they believed there\nto be a body of \u2018right\u2019 knowledge to be accumulated in order to pass examinations,\nthrough \u2018cue-consciousness\u2019 (multiplicitous\/relativist) and leading (for some) to a\n\u2018cue-seeking\u2019 (committed) orientation. Zentoria students who were cue-seeking\nrecognised the relativity of knowledge and of the tasks they were given to solve, but\nwere also secure in their own values and self-knowledge, which meant they sometimes\ndisagreed with a solution imposed in a model answer. Indeed, one could argue that\nthe advanced students were not only questioning the validity of the model answers,\nbut were also questioning the appropriateness of the questions themselves. In other\nwords, their suggestions implied a rejection of questions of the type \u2018what would you\ndo?\u2019 and their replacement by more nuanced questions such as \u2018how would you think\nthrough the options?\u2019 or \u2018what are the key issues?\u2019. Going forward, this indicates a\npossible progression from involving students in self-assessment via criteria develop-\nment to their involvement via question development. This is an important area for\nfurther research.\nBearing in mind the range of student experiences, there may be some benefit in\ndeveloping a multi-layered exercise that seeks to promote reflection in those students\nwho are ready for it, while also providing a verificational element for those needing\nthe security of benchmark answers. The proposed exercise is depicted in Figure 2 and\nincludes a divergent-reflective cycle as well as a convergent-verificational cycle, lead-\ning, ideally, to the students\u2019 sense of closure.\nFigure 2. Proposed structure for divergent:convergent activityAnother alterative would be to exploit the newer technologies that facilitate conver-\nsation between students as they collaboratively debate the assignment questions and\ndevise a repertoire of solutions for them. One possibility is the use of wiki technologies\nthat enable the co-production and co-editing of a single text (Fountain, 2005). The\nlimitation, however, is that wikis may inhibit inexperienced or shy students who are\nstruggling to develop their own voice and authority (Barton, 2004). An alternative,\ntherefore, would be the use of blogging technologies to encourage individuals to\nengage in conversations with each other while maintaining their sense of identity (for\n34 K. Handley and B. Cox\nexample, Downes, 2004). These are important areas for future development and\nresearch.\nConclusion\nThis research has investigated learners\u2019 perceptions of model-answer e-feedback,\nincluding their evaluation of alternatives. Alternatives to the conventional model-\nanswer format need to be developed if e-learning is to move away from an over-reliance\non MCQs and model answers as the standard method of assessment, particularly where\nstudents are expected to meet higher-order learning objectives.\nUsing semi-structured interviews, the research found that learners are generally\ndissatisfied with model-answer e-feedback for a variety of reasons, which vary accord-\ning to their level of experience and prior knowledge in the subject taught. For\ncomplex topics requiring judgement and experience, many of the learners interviewed\nfelt that model answers were inappropriate because they failed to indicate the nuances\nof the given problem. Instead, alternative non-verificational feedback designs were\nconsidered more effective. These included \u2018examples from real-life\u2019, and \u2018issues\u2019 to\nhelp students structure their thinking of the problem.\nThe findings of this research have broader relevance within higher education,\nparticularly in postgraduate programmes for business students where the impor-\ntance of prior business experience is emphasised (Mintzberg, 2004), and where the\nprofile of students is similar to that of the participants in this research. These\nstudents need to be supported in their efforts to build on existing knowledge and\nexperiences, and to reconsider what they perceive to be the relevant conceptual\nand practical issues. The topic of project management, which was the basis of the\napplication used for this research, is an ideal vehicle for achieving this because it\nencourages an integrative approach to business studies that combines theory and\npractice.\nTASK\n(Open question)\nFEEDBACK\n(Non-verificational)\nLearner\u2019s\nOUTCOME\n(sense of closure)\nTASK\n(Multiple-choice\nquestion)\nFEEDBACK\n(Verificational)\n3. Learner\u2019s\n sense of inquiry\nprompting\nconvergent thinking\n2. Learner\u2019s\n interaction with\nfeedback\n1. Learner\u2019s\n sense of inquiry\nprompting divergent\nthinking\n4. Learner\u2019s\n interaction with\nfeedback\n \nFigure 2. Proposed structure for divergent:convergent activity\nLearners\u2019 perceptions of self-assessment materials 35\nNote\n1. Nvivo is a qualitative data analysis package owned by QSR International (http:\/\/\nwww.qsrinternational.com\/).\nReferences\nAlavi, M. & Gallupe, R. (2003) Using information technology in learning: case studies in business\nand management education programs, Academy of Management Learning and Education, 2(2),\n139\u2013153.\nAskew, S. (Ed.) (2000) Feedback for learning (London, Routledge).\nBandura, A. (1986) Social foundations of thought and action: a social cognitive theory (Englewood\nCliffs, NJ, Prentice-Hall).\nBangert-Drowns, R. L., Kulik, C. L., Kulik, J. A. & Morgan, M. (1991) The instructional effect of\nfeedback in test-like events, Review of Educational Research, 61(2), 213\u2013237.\nBarton, M. (2004) The future of rational\u2013critical debate in online public spheres. Available online at:\nhttp:\/\/collegewriting.us\/barton\/Shared%20Documents\/RevisedArticle.doc (accessed 17 July\n2006)\nBlack, P. & Wiliam, D. (2003) \u2018In praise of educational research\u2019: formative assessment, British\nEducational Research Journal, 29(5), 623\u2013637\nBoud, D. (1995) Enhancing learning through self assessment (London, Kogan Page).\nBoyatzis, R. E. (1998) , Transforming qualitative data: thematic analysis and code development\n(Thousand Oaks, CA, Sage Publications).\nBrown, S. & Dove, P. (1991) Opening mouths to change feet: some views on self and peer assess-\nment, in: S. Brown & P. Dove (Eds) Self and peer assessment, Standing Conference on Educational\nDevelopment, Birmingham, SCED Paper 63.\nButler, L. B. & Winne, P. H. (1995) Feedback and self-regulated learning: a theoretical Synthesis,\nReview of Educational Research, 65(3), 245\u2013281.\nChi, M. T. H., Glaser, R. & Farr, M. J. (Eds) (1988) The nature of expertise (Hillsdale, NJ,\nLawrence Erlbaum Associates).\nConole, G. & Warburton, B. (2005) A review of computer-assisted assessment, ALT-J Research in\nLearning Technology, 13(1), 17\u201331.\nDavies, P. (2002) There\u2019s no confidence in multiple-choice testing, in: Proceedings of the 6th\nInternational CAA Conference, Loughborough University, 4\u20135 July, pp. 117\u2013130. Available\nonline at: http:\/\/www.caaconference.com\/pastConferences\/2002\/proceedings\/davies_p1.pdf\n(accessed 11 August 2006).\nDownes, S. (2004) Educational blogging, EDUCAUSE Review, 39(5), 14\u201326.\nFountain, R. (2005) Wiki pedagogy. Available online at: http:\/\/www.profetic.org:16080\/dossiers\/\nrubrique.php3?id_rubrique=110 (accessed 20 July 2006).\nGardner, H. (1987) Epilogue, in: The mind\u2019s new science: a history of the cognitive revolution (New\nYork, BasicBooks) (Original text published 1985).\nKruglanski, A. W. (1989) Lay epistemics and human knowledge: cognitive and motivational bases\n(New York, Plenum Press).\nKulhavy, R. W. & Stock, W. A. (1989) Feedback in written instruction\u2014the place of response\ncertitude, Educational Psychology Review, 1(4), 279\u2013308.\nKvale, S. (1996) Interviews: an introduction to qualitative research interviewing (Thousand Oaks, CA,\nSage Publications).\nLaurillard, D. (1993) Rethinking university teaching: a framework for the effective use of educational\ntechnology (1st edn) (New York, Routledge).\nLaurillard, D. (1998) Multimedia and the learner\u2019s experience of narrative, Computers & Education,\n31, 229\u2013242.\n36 K. Handley and B. Cox\nLaurillard, D. (2002) Rethinking university teaching: a framework for the effective use of educational\ntechnology (2nd edn) (New York, Routledge).\nLave, J. & Wenger, E. (1991) Situated learning: legitimate peripheral participation (Cambridge,\nCambridge University Press).\nLeung, A. C. K. (2003) Contextual issues in the construction of computer-based learning\nprograms, Journal of Computer Assisted Learning, 19, 501\u2013516.\nMiller, C. M. L. & Parlett, M. (1974) Up to the mark: a study of the examination game, Monograph\n21 (Guildford, Society for Research in Higher Education).\nMintzberg, H. (2004) Managers not MBAs: a hard look at the soft practice of managing and manage-\nment development (London, FT\/Prentice Hall).\nMoore, M. G. (2002) What does research say about the learners using computer-mediated commu-\nnication in distance learning?, The American Journal of Distance Education, 16(2), 61\u201364.\nPerry, W. G. (1970) Forms of intellectual and ethical development in the college years (New York, Holt,\nRinehart & Winston).\nRace, P. (1991) Learning through assessment, in: S. Brown & P. Dove (Eds) Self and peer assess-\nment, Standing Conference on Educational Development, Birmingham, SCED Paper 63.\nRoos, B. & Hamilton, D. (2005) Formative assessment: a cybernetic viewpoint, Assessment in\nEducation: Principles, Policy and Practice, 12(1), 7\u201320.\nRowntree, D. (1987) Assessing students: how shall we know them? (2nd edn) (London, Kogan Page).\nRumelhart, D. & Norman, D. (1978) Accretion, tuning and restructuring: three modes of learn-\ning, in: J. W. Cotton & R. L. Klatzky (Eds) Semantic factors in cognition (Hillsdale, NJ,\nLawrence Erlbaum Associates).\nSapsford, R. (1999) Survey research (London, Sage Publications).\nSchank, R. C. & Cleary, C. (1995) Engines for education (Hillsdale, NJ, Lawrence Erlbaum\nAssociates).\nSchein, E. H. (1987) Process consultation: lessons for managers and consultants (Reading, MA,\nAddison-Wesley).\nSkinner, B. F. (1958) Teaching machines, Science, 128, 969\u2013977.\nSuler, J. R. (1997) The black hole of cyberspace. Available online at: www.rider.edu\/suler\/psycyber\/\npsycyber.html (accessed 28 July 2006).\nTaras, M. (2003) To feedback or not to feedback in student self-assessment, Assessment and\nEvaluation in Higher Education, 28(5), 549\u2013565.\nThorndike, E. L. (1932) , The fundamentals of learning (New York, Teachers College Press).\nVygotsky, L. S. (1978) Mind in society: the development of higher mental processes (Cambridge, MA,\nHarvard University Press).\nWenger, E. (1987) Artificial intelligence and tutoring system (Los Altos, CA, Morgan Kaufmann).\n"}