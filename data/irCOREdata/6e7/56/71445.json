{"doi":"10.1109\/FUZZY.2005.1452543","coreId":"71445","oai":"oai:eprints.lancs.ac.uk:945","identifiers":["oai:eprints.lancs.ac.uk:945","10.1109\/FUZZY.2005.1452543"],"title":"Simpl_eTS: a simplified method for learning evolving Takagi-Sugeno fuzzy models","authors":["Angelov, Plamen","Filev, Dimitar"],"enrichments":{"references":[],"documentType":{"type":null}},"contributors":[],"datePublished":"2005-05-22","abstract":"This paper deals with a simplified version of the evolving Takagi-Sugeno (eTS) learning algorithm - a computationally efficient procedure for on-line learning TS type fuzzy models. It combines the concept of the scatter as a measure of data density and summarization ability of the TS rules, the use of Cauchy type antecedent membership functions, an aging indicator characterizing the stationarity of the rules, and a recursive least square algorithm to dynamically learn the structure and parameters of the eTS model. (c) IEEE Pres","downloadUrl":"","fullTextIdentifier":null,"pdfHashValue":null,"publisher":"IEEE","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:945<\/identifier><datestamp>\n      2018-01-24T02:07:28Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141:51413735<\/setSpec><setSpec>\n      74797065733D626F6F6B5F73656374696F6E<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Simpl_eTS: a simplified method for learning evolving Takagi-Sugeno fuzzy models<\/dc:title><dc:creator>\n        Angelov, Plamen<\/dc:creator><dc:creator>\n        Filev, Dimitar<\/dc:creator><dc:subject>\n        QA75 Electronic computers. Computer science<\/dc:subject><dc:description>\n        This paper deals with a simplified version of the evolving Takagi-Sugeno (eTS) learning algorithm - a computationally efficient procedure for on-line learning TS type fuzzy models. It combines the concept of the scatter as a measure of data density and summarization ability of the TS rules, the use of Cauchy type antecedent membership functions, an aging indicator characterizing the stationarity of the rules, and a recursive least square algorithm to dynamically learn the structure and parameters of the eTS model. (c) IEEE Press<\/dc:description><dc:publisher>\n        IEEE<\/dc:publisher><dc:date>\n        2005-05-22<\/dc:date><dc:type>\n        Contribution in Book\/Report\/Proceedings<\/dc:type><dc:type>\n        NonPeerReviewed<\/dc:type><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/FUZZY.2005.1452543<\/dc:relation><dc:identifier>\n        Angelov, Plamen and Filev, Dimitar (2005) Simpl_eTS: a simplified method for learning evolving Takagi-Sugeno fuzzy models. In: Fuzzy Systems, 2005. FUZZ '05. The 14th IEEE International Conference on. IEEE, pp. 1068-1073. ISBN 0-7803-9159-4<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/945\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1109\/FUZZY.2005.1452543","http:\/\/eprints.lancs.ac.uk\/945\/"],"year":2005,"topics":["QA75 Electronic computers. Computer science"],"subject":["Contribution in Book\/Report\/Proceedings","NonPeerReviewed"],"fullText":"Simpl_eTS: A Simplified Method for Learning \nEvolving Takagi-Sugeno Fuzzy Models \nPlamen Angelov1, Dimitar Filev2\nAbstract This paper deals with a simplified version \nof the Evolving Takagi-Sugeno (eTS) learning \nalgorithm \u2013 a computationally efficient procedure for \non-line learning TS type fuzzy models.  It combines \nthe concept of the scatter as a measure of data \ndensity and summarization ability of the TS rules, the \nuse of Cauchy type antecedent membership \nfunctions, an aging indicator characterizing the \nstationarity of the rules, and a recursive least square \nalgorithm to dynamically learn the structure and \nparameters of the eTS model.   \nI. INTRODUCTION\nTakagi-Sugeno (TS) fuzzy models are a well \nestablished tool for dealing with complex systems \nexperiencing multiple operating modes.  Most of the \nresearch in the area of learning TS models has been \nfocused on off-line identification methods.  Among \nthose are methods based on back-propagation and multi-\nlayer perceptron [7], learning vector quantization and \nradial-basis functions [10], genetic algorithms [1,12], \ncombined clustering the input data space and consequent \nparameters estimation [7, 15].  Recently, the problem of \non-line learning TS models has also been addressed in \nconjunction with model identification [2, 9], control [1, \n8], fault detection [11], and signal processing [9]. The \nproblem of structure (adaptation of the number of rules) \nand parameter (adaptation of antecedent and consequent \nmodel parameters) identification have been introduced in \nthe framework of evolving connectionists models [9] and \nevolving TS (eTS) models [1, 2], and further extended to \nthe MIMO case in [4]. The eTS model is a TS fuzzy \nmodel whose rule-base and parameters continually \nevolve by adding new rules with more summarization \npower, and by modifying existing rules and parameters. \nThe eTS learning algorithm [4] is based on a recursive \nevaluation of the information potential of the new data \npoints and the focal points of the rules.  The algorithm \ncontinuously evaluates the relationship between those \npotentials and dynamically updates the number of rules \nand their antecedent parameters. This process is \ncombined with recursive updating of the consequent \nparameters. \n1 Department of Communication Systems, Lancaster Univeristy, \nLancaster, LA1 4WA, UK; e-mail: p.angelov@lancaster.ac.uk ;\n2 Ford Motor Co., Glendale Ave 24500,  Detroit, USA; e-mail: \ndfile@ford.com   \nIn this paper we propose a simplified version of the \neTS learning algorithm, called the simpl_eTS.\nComplexity of the original eTS learning method [2] is \nsignificantly reduced by replacing the notion of the \ninformation potential with the concept of the scatter that \nprovides a similar but computationally effective \ncharacteristic of the density of input\/output data.  We \nfurther replace the Gaussian exponential membership \nfunctions of the rule antecedents with a Cauchy type \nfunction. We also introduce of the concept of \nrules\/clusters age determined by the relative time of \nappearance of the data samples that belong to a \nparticular rule\/cluster to improve the ability of the \nalgorithm to deal with non-stationary models.  We also \npresent results of testing the simpl_eTS algorithm with \nBox-Jenkins gas-furnace data [6]. \nII. SIMPLIFIED EVOLVING TS FUZZY MODEL\nThe simpl_eTS algorithm deals with TS models [13]: \n\u0083 i: IF (x1 is i1\u0081 ) AND \u2026 AND (xn is \ni\nn\u0081 )    \nTHEN (yi= iTex S )   i=1,2,...,R     (1) \nwhere \u0083i denotes the ith fuzzy rule; R is the number of \nfuzzy rules; ex is the extended input \nvector, TTe xx ],1[ ,which is formed by appending the \ninput vector Tnxxxx ],...,,[ 21 with 1 (allowing a free \nparameter); i\nj\u0081 denotes the antecedent fuzzy sets, \nj=1,2,...,n; yi denotes the multidimensional vector output \nof the ith linear sub-system, ],...,,[ 21\ni\nm\niii yyyy  \n\u00bb\n\u00bb\n\u00bb\n\u00bb\n\u00bb\n\u00bc\n\u00ba\n\u00ab\n\u00ab\n\u00ab\n\u00ab\n\u00ab\n\u00ac\n\u00aa\n \ni\nnm\ni\nn\ni\nn\ni\nm\nii\ni\nm\nii\ni\naaa\naaa\naaa\n...\n............\n...\n...\n21\n11211\n00201\nS\n are its parameters. \nAt the heart of the TS method for fuzzy modeling is the \nsegmentation of the data space into fuzzily defined \nregions. The fuzzy regions are parameterized and each \nregion is associated with a linear sub-system. As a result, \nthe nonlinear system forms a collection of loosely \n(fuzzily) coupled (blended) multiple linear models. The \ndegree of firing of each rule is proportional to the level \n0-7803-9158-6\/05\/$20.00 \u00a9 2005 IEEE. The 2005 IEEE International Conference on Fuzzy Systems1068\nof contribution of the corresponding linear model to the \noverall output of the TS model. \nInstead of the conventionally used Gaussian \nmembership functions we consider the so called Cauchy \ntype membership functions to represent rule antecedents   \n\u0096\n \u00b8\n\u00b8\n\u00b8\n\u00b9\n\u00b7\n\u00a8\n\u00a8\n\u00a8\n\u00a9\n\u00a7\n\u00b8\n\u00b8\n\u00b9\n\u00b7\n\u00a8\n\u00a8\n\u00a9\n\u00a7 \u0010\n\u000e\n P\nn\n1j\n2*i\njj\ni\nr\n)xx(2\n1\n1\n)x(\n;i=1,2,..,R (2) \nwhere r is a positive constant, which defines the zone of \ninfluence of the ith fuzzy rule antecedent ; *i\njx is the j\nth\ncoordinate of the focal point of the ith  rule. \nThe radius r is one of the few parameters of the \nalgorithm, which needs to be pre-defined. Its value is an \nimportant leverage for a trade-off between the model \ncomplexity and precision. As a general guideline, \nexcessively large values of r lead to averaging, \nexcessively small - to over-fitting.  Our preference for \nthe Cauchy type function, which is a first order Taylor \nseries approximation of the Gaussian function, is based \non its simpler calculation, especially in real time \nimplementations. \nThe TS model output is calculated by weighted \naveraging of individual rules' contributions: \n\u00a6\u00a6\n  \nSO O \nR\n1i\niT\ne\ni\nR\n1i\nii x)x(y)x(y       (3) \nwhere \u00a6\n \n \nR\nj\njii xxx\n1\n)(\/)()( PPO  is the \nnormalized firing level of the ith rule; \nThe output can be represented in a vector form as:  \nT\\ Ty               (4) \nwhere > @TTRTT )(,...,)(,)( 21 SSST  is a vector \ncomposed of the linear model parameters; \nTT\ne\nRT\ne\nT\ne xxx ],...,,[\n21 OOO\\  is a vector of the inputs that \nare weighted by the normalized firing levels of the rules. \nAs we mentioned above, the procedure for clustering \nthe input\/output data into fuzzy regions, each of which is \nrepresented by a linear model, is the kernel of the eTS \nlearning algorithm.  The on-line clustering procedure of \nthe simpl_eTS models is based on the notion of the \nscatter of the data points rather than the potential.  We \ndefine global scatter as the average distance from a data \nsample to all other data samples: \n\u000b \f\u00a6\u00a6\n \n\u000e\n \n\u0010\n\u000e\n \nN\nl\nmn\nj\njj\nG\nk kzlzmnN\nkzS\n1 1\n2\n)()(\n)(\n1\n))((     (5) \nwhere \u000b \f)(kzSGk denotes the global scatter at the data \nsample Tkykxkz )](),([)(  ; mnRkz \u000e\u008f)( at time k=2,3,\u2026,N.   \nThe scatter is as intuitive as the potential, but is \ncomputationally more efficient. Note that the division is \nover integer numbers (N, n and m), which can be \nrealized effectively on hardware using shifting registers, \nwhile the expression for the potential involves a division \nby a real number. \nThe range of possible values of the scatter measured at \ncertain sample is obviously [0;1] with 0 meaning all of \nthe data samples coincide (which is extremely \nimprobable) and 1 meaning that all of the data points are \non the vertices of the hypercube formed as a result of the \nnormalization of the data. \nIn a similar way as the potential is used in subtractive \nclustering approach off-line [7, 15] and on-line evolving \nclustering [2] the scatter can be used to select the focal \npoints of fuzzy rules\/centers of clusters when the scatter\nis minimum.  \nWe call population the number of data samples which \nare \u2018assigned\u2019 to a cluster\/rule. We denote the population\nof the ith cluster rule\/cluster at the time instant k\nby )(kNi . Once the focal points of the rules are \ndetermined each data sample is \u2018assigned\u2019 to the nearest \nof the existing focal points unless it forms a new \nrule\/cluster: \n2*\n1\n)()(minarg;1)()( kxkxikNkN i\nR\ni\nii \u0010 \u000e \n \n   (6) \nIn the latter case the population of the newly formed rule \nis set to 1: 1)(  kNR . It should be noted that each data \nsample is assigned to one cluster\/rule only and thus the \nfollowing equation holds: \n\u00a6\n \n \nR\ni\nl kkN\n1\n)(              (7) \nWe introduce the notion of the local scatter as the\naverage distance from a data sample to the data samples\nthat belong to the same ith cluster, where i=1,2,\u2026,R:\n\u000b \f\u00a6\u00a6\n \n\u000e\n \n\u0010\n\u000e\n \n)(\n1 1\n2\n)()(\n))((\n1\n))((\nkN\nl\nmn\nj\njjl\nL\nk\ni\nkzlz\nmnkN\nkzS (8) \nwhere ))(( kzSLk denotes the local scatter at the data \nsample z(k) calculated at time k=2,3,\u2026,N.\nIII. ON-LINE LEARNING OF THE SIMPL_E TS MODEL\nIn on-line mode, the training data are collected \ncontinuously, rather than being a fixed set. On-line\nlearning of the simpl_eTS models includes recursive\nclustering under assumption of a gradual change of the \nrule-base and RLS method [5]. The RLS method for \nestimation of the consequents parameters can be used, \nbecause once the antecedent parameters are determined \nand fixed, the model is linear in parameters [2].   \nAn important step in the practical implementation of \nthe algorithm is the on-line standardization of the \ninput\/output data. While this task is an ordinary step in a \nbatch type learning process (i.e. subtraction of the mean \nand normalization over the standard deviation), its \nThe 2005 IEEE International Conference on Fuzzy Systems1069\npractical realization in on-line environment requires \nsome additional efforts.   \nIn order to standardize the data we recursively \nestimate the mean )(kzj  and the variance )(\n2 kjV  for each \nelement zj of the input\/output vector z \n)(\n1\n)1(\n1\n)( kz\nk\nkz\nk\nk\nkz jjj \u000e\u0010\n\u0010       (11) \n\u000b \f222 )()(\n1\n1\n)1(\n1\n)( kzkz\nk\nk\nk\nk\nk jjjj \u0010\u0010\n\u000e\u0010\u0010 VV (12) \nwith initial values 0)1(  jz  and 0)1(\n2  jV .\nThen the standardized input\/output value is: \n    \u000b \f )(\/)()()( kkzkzkz jjjst V\u0010       (13) \nSince the standard deviation is zero for k=1, the \nrecursive normalization process starts at the second step, \ntherefore we miss the first observation if we apply this \nstandardization procedure and the first standardized \ninput\/output data is calculated from the second actual \nsample.  In order to simplify the notations we assume in \nthe next steps of the actual simpl_eTS that the \ninput\/output vector z has been already standardized. \nThe actual learning algorithm starts (stage 1) with the \nfirst standardized data sample that establishes the focal \npoint of the first cluster (i=1). Its coordinates are used to \nform the antecedent part of the fuzzy rule (1) using for \nexample the Cauchy membership functions (2). Its \nscatter, S is assumed equal to 0.\n0:)();(:;1:;1: *11\n*1     zSkxxRk     (14) \nwhere *1z is the first cluster center; *1x is focal point of \nthe first rule being a projection of *1z on the axis x.\nAt this stage we also initialize the parameters of the \nRLS algorithm, which is used for on-line estimation of \nthe consequent parameters of the simp_eTS model: \nIC :   )1(;0)1()1( ST        (15) \nStart a loop, which continues until there are data (for \nkt1. Read the next input data sample (x(k+1)) at stage 2.\nEstimate the output at the next time step (stage 3) by: \n)()1()1(\n^^\nkkky T T\\ \u000e \u000e      (16) \nAt the next time step (k:=k+1) we can read the true \nvalue of the output of the new data point, y(k),stage 4. \nAt stage 5 the scatter of the new data points (z(k)) is \ncalculated recursively. Note that the summation is over \n(k-1). That means all data samples up to the previous \none: \n\u000b \f \u000b \f\u00a6\u00a6\n\u0010\n \n\u000e\n \n\u0010\n\u000e\u0010\n \n1\n1 1\n2\n)()(\n))(1(\n1\n)(\nk\nl\nmn\nj\nlj\nG\nk kzlzmnk\nkzS (17) \nwhere ))(( kzSGk denotes the global scatter at a data \nsample z(k) calculated at time instant k=2,3,\u2026.\nThe expression for the local scatter is similar. Further \nwe will consider the global scatter only without loss of \ngenerality. Instead of using (17) we introduce its \nrecursive version. To arrive at this expression we make \nthe following simple transformation: \n\u00a6\u00a6\n\u0010\n \n\u000e\n \n\u0010\n\u000e\u0010\n \n1\n1 1\n2))()((\n))(1(\n1\n))((\nk\nl\nmn\nj\njj\nG\nk kzlzmnk\nkzS    (18) \nThe scatter at the moment of time k can be expressed \nrecursively as: \n\u000b \f \u00b8\u00b8\n\u00b9\n\u00b7\n\u00a8\u00a8\n\u00a9\n\u00a7\n\u000e\u0010\u0010\n\u000e\u0010\n \u00a6\u00a6\n\u000e\n \n\u000e\n \nmn\nj\njj\nmn\nj\njk kkkzkzkmnk\nkzS\n11\n2 )()()(2)()1(\n))(1(\n1\n)( JE  (19) \nwhere \u00a6\u00a6\n\u0010\n \n\u000e\n \n \n1\n1 1\n2 )()(\nk\nl\nmn\nj\nj lzkO\n; \u00a6\n\u0010\n \n \n1\n1\n)()(\nk\nl\njj lzkE \u00a6\u00a6\n\u0010\n \n\u000e\n \n \n1\n1 1\n2 )()(\nk\nl\nmn\nj\nj lzkJ \u00a6\n\u0010\n \n \n1\n1\n)()(\nk\nl\njj lzkE\nIt should also be noted that the division is by integer \nnumbers (k, n, and m are integers), while the respective \nrecursive formula in the original eTS algorithm involves \ndivision by a real number. Parameters )(kjE  and \n)(kJ  are recursively updated as follows: \n\u00a6\n\u000e\n \n\u0010\u000e\u0010 \nmn\nj\nj kzkk\n1\n2 )1()1()( JJ ; )1()1()( \u0010\u000e\u0010 kzkk jjj EE  (20) \nAfter the new data are available in on-line mode, they \ninfluence the scatter at the centers of the clusters ( *iz ,\ni=1,2,...,R), which are respective to the focal points of \nthe existing rules ( *ix , i=1,2,...,R). The reason is that by \ndefinition the global scatter depends on the distance to \nall data points, including the new ones. \nAt stage 6 the scatter at the focal points of the existing \nclusters are updated recursively:\nIn order to find the recursive expression for the update \nof the scatter at the focal point, zi* we can compare the \nexpressions for the scatter at the focal point calculated at \ntwo consecutive steps (k-1) and k. The scatter calculated \nat time steps k and k-1 at this centre will depend on the \naveraged accumulated sum of distances between this \ncentre\/focal point and all previous points (from 1 to k-1\nand k-2 respectively) according to the definition of the \nscatter given in (16): \n\u000b \f\u00a6\u00a6\n\u0010\n \n\u000e\n \n\u0010 \u0010\u0010\u000e\u0010\n \n2\n1 1\n2*\n1 )1()())(2(\n1\n)(\nk\nl\nmn\nj\njj\ni\nk kzlzmnk\nzS    (21) \n\u00a6\u00a6\n\u0010\n \n\u000e\n \n\u0010\n\u000e\u0010\n \n1\n1 1\n2* ))()((\n))(1(\n1\n)(\nk\ni\nmn\nj\njj\ni\nk kzlzmnk\nzS      (22) \nFrom (21) we have: \n)())(2())1()(( *1\n2\n1 1\n2 i\nk\nk\nl\nmn\nj\njj zSmnkkzlz \u0010\n\u0010\n \n\u000e\n \n\u000e\u0010 \u0010\u0010\u00a6\u00a6 (21a) \nFrom (22) we have: \nThe 2005 IEEE International Conference on Fuzzy Systems1070\n))(1(\n))()1((\n))(1(\n))1()((\n)( 1\n2\n2\n1 1\n2\n*\nmnk\nkzkz\nmnk\nkzlz\nzS\nmn\nj\njj\nk\nl\nmn\nj\njj\ni\nk \u000e\u0010\n\u0010\u0010\n\u000e\n\u000e\u0010\n\u0010\u0010\n \n\u00a6\u00a6\u00a6\n\u000e\n \n\u0010\n \n\u000e\n (22a) \nSubstituting (21a) into (22a) we have the following \nrecursive expression: \n\u00b8\u00b8\n\u00b9\n\u00b7\n\u00a8\u00a8\n\u00a9\n\u00a7\n\u0010\u0010\u000e\u000e\u0010\n\u000e\u0010\n \u00a6\n\u000e\n \n\u0010\nmn\nj\njj\ni\nkik kzkzzSmnkmnk\nzS\n1\n2*\n1\n* ))1()(()())(2(\n))(1(\n1\n)( (23) \nThis can be re-written as: \n\u00a6\n\u000e\n \n\u0010 \u0010\u0010\u000e\u0010\n\u0010 \nmn\nj\njj\ni\nk\ni\nk kzkzzSk\nk\nzS\n1\n2*\n1\n* ))1()(()(\n1\n2\n)( (23a) \nwhere )( *ik zS is the scatter at the i\nth center *iz , which is \na prototype of the ith rule at time k.\n It should be noted that in this recursive update of the \nscatter at focal points there is practically no division \nexcept (k-2)\/(k-1) factor, which leads to computational \nsimplifications comparing with the original eTS \nalgorithm. \nAt stage 7 the scatter of the new data sample is \ncompared to the updated scatter of the centers of the \nexisting clusters. The decision whether to modify or up-\ngrade the rule-base is taken based on the following rule:  \nIF the scatter at the new data sample, z(k) is lower than \nthe scatter at all of the existing rule centers or higher \nthan the scatter at all of the existing centers [3]: \n)(max))((...)...(min))(( *\n1\n*\n1\ni\nk\nR\ni\nk\ni\nk\nR\ni\nk zSkzSORzSkzS   \n!\u001f  (24) \nAND z(k)  is close to an existing rule center given by: \nrk 5.0)(min \u001fG           (25) \nwhere 2*\n1\nmin )(min)(\ni\nR\ni\nxkxk \u0010 \n \nG  denotes the distance from \nthe new data point to the closest of the existing rule \ncenters (let us suppose that it has index l).  \nTHEN the new data point, z(k) replaces this center: \n)(:* kxxl  ; ))((:)( * kzSzS k\nl\nk       \nASIl(k):=ASIl(k)+k ;Nl(k) =Nl(k)+1    (26) \nELSEIF only (22) is satisfied but not (23)\nTHEN the new data sample is added to the rule-base as \na new center and a new rule is formed with a focal \npoint based on the projection of this center on the axis x:\nR :=R+1; )(* kxxR  ; ))(()( * kzSzS k\nR\nk       \nASI R(k):=k; N R(k):=1         (27) \nELSEIF ((23) is NOT satisfied)  \nTHEN the data sample is assigned to the nearest existing \ncluster centre\/focal point of a rule: \n2*\n1\n)(minarg;1)()(;)()( i\nR\ni\niiii xkxikNkNkkASIkASI \u0010 \u000e \u000e \n \n(28) \nEND\nIt should be noted that we calculate the distance, \nminG over the input data only by differ from [2] where it \nhas been calculated as an overall measure over both the \nantecedents and consequents. As a result we disallow \nrules which have similar antecedents to co-exist. Such \nrules could have entered the rule base according to the \ndefinition of \nminG used in [2] if their consequents are \ndifferent. Such rules, however, lead to contradiction in \ntheir linguistic interpretation and we adopted the \nstronger formulation 2*\n1\nmin )(min)(\ni\nR\ni\nxkxk \u0010 \n \nG  In such a \nway, we have the same effect as if we remove at a later \nstage a rule based on its linguistic similarity to a \npreviously accepted rule as it is done in [14].  \n Instead of \u2018linguistic-based\u2019 simplification of the rule-\nbase we perform \u2018population-based\u2019 one. Because the \nrule base evolve incrementally, at the moment of \nappearance of a rule it is judged by the existing at that \nmoment of time data samples, but not the future ones. \nThe data pattern can change and therefore, rules ones \naccepted in the rule-base can go out of favor or be less \nand less relevant in the future. We monitor the \npopulation of each cluster and if it amounts to less than \n1% of the total data samples at that moment the \ncluster\/rule is ignored from the rule base by setting its \nfiring level to 0:\nIF (N(l)\/N <0.01) THEN ( iO :=0)      (29) \nThis population-based rule-base reduction means that \nthe rules will be representative. \nIn simpl_eTS the rule-base gradually evolves [2]. \nTherefore the normalized firing strengths of the rules (Oi)\nchange, which affects all the data (including the data \ncollected before time of the change). Therefore, the \nstraightforward application of the RLS is not correct [5]. \nA resetting of the covariance matrices and parameters \ninitialization of the RLS is made at each time a new \n(R+1)th rule is added to the rule base estimating them as \na weighted average of the respective covariance and \nparameters of the remaining R rules [2].  \n In simpl_eTS we assume a globally optimal objective \nfunction: \n\u000b \f\u00a6\n\u0010\n \n\u0010 \n1\n1\n2\n)()()()(\nk\nl\nT\nG lllykJ T\\      (30)\nA local optimization which leads to a more interpretable \nlocally sub-models is also possible as in [2] for the \noriginal eTS. For the sake of simplicity, however, we \nconsider the global optimality here without loss of \ngenerality. It leads to better overall performance of the \nmodels, though the local interpretability can suffer [2]. It \ncan be minimized by the following RLS procedure \n(stage 8) [5]: \n\u00b8\n\u00b9\n\u00b7\n\u00a8\n\u00a9\n\u00a7 \u0010\u0010\u000e\u0010 )1()()()()()1()(\n^^^\nkkkykkCkk T T\\\\TT (31) \nThe 2005 IEEE International Conference on Fuzzy Systems1071\n)()1()(1\n)1()()()1(\n)1()(\nkkCk\nkCkkkC\nkCkC\nT\nT\n\\\\\n\\\\\n\u0010\u000e\n\u0010\u0010\n\u0010\u0010  (32) \nwhere 0)(,...,)(,)()1(\n^^\n2\n^\n1\n^\n \u00bb\n\u00bc\n\u00ba\n\u00ab\n\u00ac\n\u00aa\n \nT\nTRTT SSST ; IC : )1(  k=2,3,\u2026\nWhen a new rule is added to the rule-base, the RLS is \nreset as shown in [5]. \nThe recursive procedure for on-line learning of the \nsimpl_e TS models can be represented by the following \npseudo-code: \nBegin simpl_e TS \n \/*stage 1*\/ \n Initialize the rule-base: \n Read z(1);\n Set S(1)=0; z1*:=z(1);\nR:=1; K:=1; \nDO for kt1\nRead x(k+1);  \/*stage 2*\/ \n\/*stage 3*\/ \n  Estimate the output: )1(\n^\n\u000eky by(16);\nk:=k+1; \/*stage 4*\/  \nRead y(k);\n\/*stage 5*\/  \nCalculate Sk recursively by (19)-(20)\n\/*stage 6*\/  \nUpdate S(zi*)at each centre by (23a); \nCompare S(k) with Sk(z\ni\n*);\/*stage 7*\/ \nIF (24) AND (25) holds THEN\n   \/*replace a rule\/cluster (26)*\/ \n   xi*:=x(k); Sk(z\ni\n*):=Sk(z(k));\nASI\ni\n(k):=ASI\ni\n(k)+k; N\ni\n(k):=N\ni\n(k)+1\nELSEIF (24) holds THEN\n\/*add new rule\/cluster (23)*\/ \nR:=R+1; x\nR\n*:=x(k); ASI\ni\n(k):=k; \nNR(k):=1; Sk(z\nR\n*):= Sk(z(k));\nEND\n\/*assign the data sample to the nearest cluster *\/ \nEND DO \nIF (29) THEN Reduce the rule base; \n\/*stage 8*\/ \nEstimate the parameters of local sub-\nmodels by RLS (31)-(32); \nEnd (simpl_eTS) \nIV. EXPERIMENTAL RESULTS\nThe proposed approach has been tested on a benchmark \nproblem - Box-Jenkins gas furnace data. \nThe Box-Jenkins data set is one of the well established \nbenchmark problems. It consists of 290 pairs of \ninput\/output data taken from a laboratory furnace [6]. \nEach data sample consists of the methane flow rate, the \nprocess input variable, u(k), and the CO2 concentration \nin off gas, the process output, y(k). From different \nstudies the best model structure for this system is: \n\u000b \f)4(),1()( \u0010\u0010 kukyfky       (35)\nThe experiments include: i) applying eTS models to all\n(290) data samples; note, that the eTS model evolves all \nthe time; ii) applying simpl_eTS1 (using scatter instead \nof potential) to 200 data samples, then fix the model and \nvalidate it with the remaining 90 samples; iii) do the \nsame but using Cauchy function instead of Gaussian one \n(simpl_eTS2); iv) do the same plus population-based \nreduction of the rule-base. \nTo evaluate the performance of the models we use the \nRMSE and the NDEI (Non-Dimensional Error Index), \nand the variance accounted for (VAF). NDEI is the ratio \nof the root mean square error over the standard deviation \nof the target data: \n\u000b \f)(tystd\nRMSE\nNDEI  . The VAF represent \nthe ratio between the variance of the real data and the \nmodel output: \u000b \f\u00bb\u00bc\n\u00ba\n\u00ab\u00ac\n\u00aa\n\u00b8\n\u00b9\n\u00b7\u00a8\n\u00a9\n\u00a7 \u0010\u0010 yyyVAF r var\/var1%100\n^\n. is \n100% when the signals \n^\ny and y are identical.\nThe values of the performance measures were calculated \nseparately for the training and testing data. The value of the \ncluster radii was r=0.4, and initialization parameter for \nthe RLS algorithm 750 : . These are the only \nparameters of the algorithm that need to be pre-specified. \nThe results are shown in Figs. 1-4 and Table 1. \nTable 1 Results for Box-Jenkins Fata Set \nalgorithm r R RMSE \nTesting\nNDEI \nTesting\nVAF\neTS 0.4 5 0.04904 0.30571 92.134 \nsimpl_eTS1 0.4 3 0.04850 0.30232 92.225 \nsimpl_eTS2 0.4 3 0.04849 0.30041 92.315 \nsimple_TS3 0.4 3 0.04849 0.30041 92.315 \nFig.1 Box-Jenkins data (dots); simpl_eTS model (solid line); \ndata samples that originate new rule (circles); data samples that \nreplace existing centers (triangles); final position of the focal \npoint (asterisk) \nThe 2005 IEEE International Conference on Fuzzy Systems1072\nThe results illustrate that practically the same or slightly \nhigher performance can be achieved with smaller \nnumber of rules (3 instead of 5). This is directly linked \nto the interpretability and simplicity. In addition, the \nsimpl_eTS is computationally simpler and more \nconvenient for practical realization on dedicated \nhardware. \nFig.2 Scatter evolution in the simpl_eTS \nFig.3 Evolution of the population of rules\/clusters\nFig.4 Evolution of the age of rules\/clusters \nV. CONCLUDING REMARKS\nWe demonstrated the advantages of the eTS algorithm as \na tool for on-line learning of TS fuzzy model can be \npreserved while the actual learning algorithm can be \nsignificantly simplified.  The new concepts introduced in \nthis paper \u2013 Cauchy type membership function instead of \nGaussian, scatter as a measure of density and ability for \nsummarization instead of potential, rule age as a \nrepresentation of the stationarity of the rules, etc., didn\u2019t \nnegatively affect the performance of the algorithm but \ncontributed substantially towards its simplification.   \nREFERENCES \n[1] Angelov P. P. (2002) Evolving Rule-based Models: A Tool for \nDesign of Flexible Adaptive Systems, Springer-Verlag, \nHeidelberg, New York, ISBN 3-7908-1457-1. \n[2] Angelov, P., D. Filev, \"An approach to on-line identification of \nevolving Takagi-Sugeno models\", IEEE Trans. on Systems, Man \nand Cybernetics, part B, vol.34, No.1, 2004, 484-498 \n[3] Angelov P., J. Victor, A. Dourado, D. Filev, On-line evolution of \nTakagi-Sugeno Fuzzy Models, 2nd IFAC Workshop on \nAdvanced Fuzzy\/Neural Control, 16-17 September 2004, Oulu, \nFinland, pp.67-72. \n[4] Angelov P., C. Xydeas, D. Filev, On-line Identification of MIMO \nEvolving Takagi-Sugeno Fuzzy Models, Proc. IJCNN-FUZZ-\nIEEE, Budapest, Hungary, July 2004, ISBN 0-7803-8354-0 \n[5] Astrom K., B. Wittenmark (1984) Computer Controlled Systems: \nTheory and Design, Prentice Hall, NJ USA. \n[6] Box G., G, Jenkins (1976) Time Series Analysis: Forecasting and \ncontrol, 2nd edition, Holden-Day, San Francisco, CA, USA. \n[7] Chiu, S. L., \"Fuzzy model identification based on cluster \nestimation\", J. of Intell. and Fuzzy Syst.vol.2, 267-278, 1994. \n[8] Filev D.P., T. Larsson, L. Ma, Intelligent Control, for automotive \nmanufacturing-rule-based guided adaptation, in Proc. IEEE \nConf. IECON\u201900, Nagoya, Japan, Oct. 2000, pp.283-288. \n[9] Kasabov N., Q. Song (2002) DENFIS: Dynamic Evolving Neural-\nFuzzy Inference System and Its Application for Time-Series \nPrediction, IEEE Trans. on Fuzzy Systems 10(2) 144-154. \n[10] Lin, F.-J., C.-H. Lin, P.-H. Shen, Self-constructing fuzzy neural \nnetwork speed controller for permanent-magnet synchronous \nmotor drive, IEEE Trans. on Fuzzy Systems 9 (5) 751-759, 2001. \n[11] Lughofer E., E.-P. Klement, J.M. Lujan, C. Guardiola, Model-\nBased Fault Detection in Multi-Sensor Measurement Systems, \nIEEE Symp. on Intelligent Systems, Varna, Bulgaria, June 2004. \n[12] Setnes M., J.A. Roubos (1999) Transparent Fuzzy Modeling using \nClustering and GA's, Proc. of the NAFIPS Conference, New \nYork, USA, pp.198-202. \n[14] Shing J., R. Jang (1993) ANFIS: Adaptive Network-based Fuzzy \nInference Systems, IEEE Transactions on Systems, Man & \nCybernetics, v.23 (3), pp.665-685. \n[13] Takagi, T., M. Sugeno, \"Fuzzy identification of systems and its \napplication to modeling and control\", IEEE Trans. on Syst., Man \n& Cybernetics, vol. 15, pp. 116-132, 1985. \n[14] Victor Ramos J., A. Dourado (2004) On-Line Interpretability by \nFuzzy Rule-base simplification and Reduction, EUNITE 2004, \nAachen, Germany, pp.430-442. \n[15] Yager R.R., D.P. Filev (1993) Learning of Fuzzy Rules by \nMountain Clustering, Proc. of SPIE Conf. on Application of \nFuzzy Logic Technology, Boston, MA, USA, pp.246-254.\nThe 2005 IEEE International Conference on Fuzzy Systems1073\n"}