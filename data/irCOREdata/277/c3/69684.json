{"doi":"10.1007\/s10985-009-9133-5","coreId":"69684","oai":"oai:eprints.lancs.ac.uk:23532","identifiers":["oai:eprints.lancs.ac.uk:23532","10.1007\/s10985-009-9133-5"],"title":"Computation of the asymptotic null distribution of goodness-of-fit tests for multi-state models.","authors":["Titman, Andrew C."],"enrichments":{"references":[{"id":1023946,"title":"A chi-square statistic with random cell boundaries.","authors":[],"date":"1971","doi":"10.1214\/aoms\/1177693502","raw":"Moore D.S. A chi-square statistic with random cell boundaries. The Annals of Mathematical Statistics 1971; 42: 147-156.","cites":null},{"id":1024776,"title":"A general goodness-of-\ufb01t test for Markov and hidden Markov models. Statistics in Medicine","authors":[],"date":"2008","doi":"10.1002\/sim.3033","raw":"Titman A. C, Sharples L. D. A general goodness-of-\ufb01t test for Markov and hidden Markov models. Statistics in Medicine 2008; 27: 2177-2195.","cites":null},{"id":1021756,"title":"A Pearson-type goodness-of-\ufb01t test for stationary and time-continuous Markov regression models. Statistics in Medicine","authors":[],"date":"2002","doi":"10.1002\/sim.1152","raw":"Aguirre-Hernandez R, Farewell V. T. A Pearson-type goodness-of-\ufb01t test for stationary and time-continuous Markov regression models. Statistics in Medicine 2002; 21:1899-1911.","cites":null},{"id":1022064,"title":"Applications of continuous time hidden Markov models to the study of misclassi\ufb01ed disease outcomes. Statistics in Medicine","authors":[],"date":"2003","doi":"10.1002\/sim.1270","raw":"Bureau A, Shiboski S, Hughes J. P. Applications of continuous time hidden Markov models to the study of misclassi\ufb01ed disease outcomes. Statistics in Medicine 2003; 22: 441-462.","cites":null},{"id":1023354,"title":"Hidden Markov models for the onset and progression of bronchiolitis obliterans syndrome in lung transplant recipients. Statistics in Medicine","authors":[],"date":"2002","doi":"10.1002\/sim.886","raw":"Jackson C.H, Sharples L.D. Hidden Markov models for the onset and progression of bronchiolitis obliterans syndrome in lung transplant recipients. Statistics in Medicine 2002; 21: 113-128 Kalb\ufb02eisch J.D, Lawless J.F. The analysis of panel data under a Markov assumption. Journal of the American Statistical Association 1985; 80:863-871.","cites":null},{"id":1024195,"title":"Markov chains with measurement error: estimating the \u2018true\u2019 course of a marker of the progression of Human Immunode\ufb01ciency Virus disease.","authors":[],"date":"1996","doi":"10.2307\/2986089","raw":"Satten G.A, Longini I.M. Markov chains with measurement error: estimating the \u2018true\u2019 course of a marker of the progression of Human Immunode\ufb01ciency Virus disease. Journal of the Royal Statistical Society Series C 1996, 45: 265-309.","cites":null},{"id":1023095,"title":"msm: Multi-state Markov and hidden Markov models in continuous time. R package version 0.8.1","authors":[],"date":"2008","doi":null,"raw":"Jackson C.H. msm: Multi-state Markov and hidden Markov models in continuous time. R package version 0.8.1 2008.","cites":null},{"id":1022876,"title":"Note on the inversion theorem. Biometrika","authors":[],"date":"1951","doi":"10.2307\/2332598","raw":"Gil-Pelaez J. Note on the inversion theorem. Biometrika 1951; 38: 481-482.","cites":null},{"id":1024511,"title":"Testing departures from time homogeneity in multistate Markov processes.","authors":[],"date":"1988","doi":"10.2307\/2347343","raw":"11Stavola B. L de. Testing departures from time homogeneity in multistate Markov processes. Journal of the Royal Statistical Society. Series C 1988; 37:242-250.","cites":null},{"id":1023625,"title":"The advanced theory of statistics.","authors":[],"date":"1961","doi":"10.2307\/2985818","raw":"Kendall M.G, Stuart A. The advanced theory of statistics. Vol.2 London, 1961. Lystig T.C, Hughes J.P. Exact computation of the observed information matrix for hidden Markov models.","cites":null},{"id":1022592,"title":"The theory of stochastic processes.","authors":[],"date":"1965","doi":"10.2307\/1266653","raw":"Cox D.R, Miller H.D. The theory of stochastic processes. Chapman and Hall, London, 1965. Fisher R. A. The conditions under which \u03c72 measures the discrepancy between observation and hypothesis.","cites":null},{"id":1022331,"title":"The use of maximum likelihood estimates in \u03c72 tests for goodness-of-\ufb01t. The Annals of Mathematical Statistics","authors":[],"date":"1954","doi":"10.1214\/aoms\/1177728726","raw":"Cherno\ufb00 H, Lehmann E.L. The use of maximum likelihood estimates in \u03c72 tests for goodness-of-\ufb01t. The Annals of Mathematical Statistics 1954; 25:576-586.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2009-12","abstract":"We develop an improved approximation to the asymptotic null distribution of the goodness-of-fit tests for panel observed multi-state Markov models (Aguirre-Hernandez and Farewell, Stat Med 21:1899-1911, 2002) and hidden Markov models (Titman and Sharples, Stat Med 27:2177-2195, 2008). By considering the joint distribution of the grouped observed transition counts and the maximum likelihood estimate of the parameter vector it is shown that the distribution can be expressed as a weighted sum of independent X^2_1 random variables, where the weights are dependent on the true parameters. The performance of this approximation for finite sample sizes and where the weights are calculated using the maximum likelihood estimates of the parameters is considered through simulation. In the scenarios considered, the approximation performs well and is a substantial improvement over the simple X^2_1 approximation","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/69684.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/23532\/1\/nulltechnical.pdf","pdfHashValue":"a557df1f090197129b0fe3f074d172016f4f76ca","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:23532<\/identifier><datestamp>\n      2018-01-24T02:43:41Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5141<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Computation of the asymptotic null distribution of goodness-of-fit tests for multi-state models.<\/dc:title><dc:creator>\n        Titman, Andrew C.<\/dc:creator><dc:subject>\n        QA Mathematics<\/dc:subject><dc:description>\n        We develop an improved approximation to the asymptotic null distribution of the goodness-of-fit tests for panel observed multi-state Markov models (Aguirre-Hernandez and Farewell, Stat Med 21:1899-1911, 2002) and hidden Markov models (Titman and Sharples, Stat Med 27:2177-2195, 2008). By considering the joint distribution of the grouped observed transition counts and the maximum likelihood estimate of the parameter vector it is shown that the distribution can be expressed as a weighted sum of independent X^2_1 random variables, where the weights are dependent on the true parameters. The performance of this approximation for finite sample sizes and where the weights are calculated using the maximum likelihood estimates of the parameters is considered through simulation. In the scenarios considered, the approximation performs well and is a substantial improvement over the simple X^2_1 approximation.<\/dc:description><dc:date>\n        2009-12<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/23532\/1\/nulltechnical.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1007\/s10985-009-9133-5<\/dc:relation><dc:identifier>\n        Titman, Andrew C. (2009) Computation of the asymptotic null distribution of goodness-of-fit tests for multi-state models. Lifetime Data Analysis, 15 (4). pp. 519-533. ISSN 1380-7870<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/23532\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1007\/s10985-009-9133-5","http:\/\/eprints.lancs.ac.uk\/23532\/"],"year":2009,"topics":["QA Mathematics"],"subject":["Journal Article","PeerReviewed"],"fullText":"Computation of the asymptotic null distribution of goodness-of-fit\ntests for multi-state models\nA. C. Titman \u2020\nAbstract\nAn approach for testing goodness-of-fit for parametric continuous-time panel observed multi-state\nMarkov models is to use Pearson-type \u03c72 tests. When observations are balanced and there are no\ncontinuous covariates, a test with an asymptotic \u03c72 null distribution can be found. Aguirre-Hernandez\nand Farewell (2002) constructed a Pearson-type test by grouping observations with similar time intervals\nand covariate values, which is appropriate in more general cases. Titman and Sharples (2008) proposed\nan extension to accommodate models where the observed states can be subject to misclassification.\nWhile in some cases the chi-squared distribution can give a good approximation to the null distribution\nof these statistics, more generally, and particularly if there are continuous covariates in the model, the\nnull distribution of the statistic has a higher mean than the \u03c72 approximation. Use of the parametric\nbootstrap can yield a good approximation of the null distribution, however this method requires multiple\nrefitting of the model. For many models this may be prohibitively time consuming, limiting the practical\napplicability of the tests. In this paper, a better approximation to the asymptotic distribution of the\ntests, as a weighted sum of independent \u03c721 random variables, is given.\n1 Introduction\nParametric continuous-time multi-state models are a widely used method to model the progression of a\ncategorical response variable over time. In medical applications, the response may refer to a disease state,\nand this state may only be observed at discrete, irregular intervals. Such an observation scheme is referred\nto as panel observation. The multi-state process is often assumed to be Markov. This greatly simplifies\nthe computation of the likelihood. For data of the form of a series of observations xi0, . . . , xini at times\nti0, . . . , xini , for patients i = 1, . . . , N , with covariate vectors zi, the log-likelihood can be expressed as\nl(\u03b8) =\nN\u2211\ni=1\nni\u2211\nj=1\nlog\n(\npxi(j\u22121)xij (ti(j\u22121), tij ; zi, \u03b8)\n)\n,\nwhere\nprs(t0, t1; z, \u03b8) = P(X(t1) = s|X(t0) = r; z, \u03b8)\n\u2020Address for correspondence: Department of Mathematics and Statistics, Lancaster University, UK. Email:\na.titman@lancaster.ac.uk\n1\nis the (r, s) entry of the transition probability matrix which can be found by solving the Kolmogorov Forward\nequations (Cox and Miller (1965))\ndP (t0, t)\ndt\n= P (t0, t)Q(t), (1.1)\nsubject to initial condition P (t0, t0) = I. When the process is time homogeneous, the solution is given by a\nmatrix exponential. In this situation, a closed-form solution to (1.1) can be found\nP (t0, t1) = P (s) = exp (sQ0) =\n\u221e\u2211\nn=0\nsn\nn!\nQn0 ,\nwhere s = t1 \u2212 t0. The matrix exponential in this equation can be calculated using the eigen-decomposition\nof Q0. Let D be a diagonal matrix of the eigenvalues and U the matrix with the corresponding eigenvectors\nas columns. Provided the eigenvalues are distinct, U is invertible and Q0 = UDU\n\u22121. Then\nexp (sQ) = U exp (sD)U\u22121.\nKalbfleisch and Lawless (1985) gave a numerical Fisher scoring algorithm for computing the maximum\nlikelihood estimate.\nIn some observational studies, the diagnosed state at a particular time may be subject to misclassification.\nA method of attempting to account for this is to make the assumption that the observed states, oi0, . . . , oini\nare independent conditional on the values of xi0, . . . , xini . The observable process is then a continuous time\nhidden Markov model (HMM). The observed states relate to the true underlying states through classification\nprobabilities,\npirs = P(O(t) = s|X(t) = r).\nThese probabilities may be assumed unknown and to be estimated from the data.\nApproaches to computation of the maximum likelihood estimates for misclassification HMMs are either to\napply a continuous-time generalization of the Forward-Backward algorithm for discrete-time HMMs (Bureau\net al (2003)). Alternatively the likelihood can be computed directly and maximized using derivative free\noptimization algorithms (Jackson and Sharples (2002); Satten and Longini (1996)).\n2 Goodness-of-fit tests\nIn order to assess the appropriateness of the Markov assumption, as well as possible stationarity and subject\nhomogeneity assumptions, goodness-of-fit procedures are required. When the observation times are balanced,\nso that all subjects are observed at the same times, t1j = . . . = tNj , and there are no covariates or the\ncovariates have a small number of unique values, then a Pearson chi-squared test or asymptotically equivalent\nlikelihood ratio test can be applied (Kalbfleisch and Lawless (1985); de Stavola (1988)). The likelihood ratio\ntest has statistic\n\u039b = 2\n\u2211\nj\n\u2211\nc\nR\u2211\nr=1\nR\u2211\ns=1\nohcrs log (\nojcrs\nejcrs\n)\nwhile the Pearson \u03c72 test has\nX2 =\n\u2211\nj\n\u2211\nc\nR\u2211\nr=1\nR\u2211\ns=1\n(ojcrs \u2212 ejcrs)\n2\nejcrs\n2\nwhere\nojcrs =\n\u2211\n1[Xi(tj) = s,Xi(tj\u22121) = r] (2.1)\nejcrs = prs(tj\u22121, tj ; \u03b8\u02c6)njcr (2.2)\nwhere njcr is the number of individuals with covariate value c observed in state r at time tj\u22121, who have an\nobservation at time tj .\nEach statistic has an asymptotic null distribution which is \u03c72 with degrees of freedom given by C\u2212|\u03b8|, where\nC is the number of independent cells from the resultant contingency table and |\u03b8| is the number of unknown\nparameters fitted from the data.\nSuch tests cannot be applied when subjects are observed at irregularly spaced intervals or when the number\nof unique covariate values is high. Aguirre-Hernandez and Farewell (2002) (AH\/F) proposed a Pearson-type\ngoodness-of-fit test which accommodates irregular observation times and continuous covariates. Observations\nare categorized by observation number into observation categories, h, and, within each observation category,\nby time interval category, lh. Additionally, observations are categorized by covariate category, c, according\nto quantiles of the estimated transition intensity qrs. Then, for each transition type, r \u2192 s for a patient\nwith observations at times tj , j = 1, . . . , n, we calculate:\nohlhrsc =\n\u2211\n1[(X(tj+1) = s,X(tj) = r)] (2.3)\nehlhrsc =\n\u2211\nP(X(tj+1) = s|X(tj) = r)1[X(tj\u22121) = r] (2.4)\nwhere the summation is over the set of observations:\n\u2200patients, j : tj+1 \u2212 tj \u2208 lh, q(z) \u2208 c (2.5)\nwhere z is the vector of covariates for a patient.\nThe statistic is then given by\nT =\n\u2211\nh\n\u2211\nlh\n\u2211\nc\n\u2211\nr\n\u2211\ns\n(ohlhrsc \u2212 ehlhrsc)\n2\nehlhrsc\n. (2.6)\nTitman and Sharples (2008) (T\/S) proposed an extension to this test to accommodate misclassification\nhidden Markov models. The same grouping method as the AH\/F test applies. However, the observed and\nexpected counts are based on the observed states, but the observed state does not have the Markov property.\nTherefore, it is necessary to consider all observations up to the current time in order to estimate the next\nobserved state. o and e are then given by\nohlhrsc =\n\u2211\n1[(O(tj+1) = s,O(tj) = r)] (2.7)\nehlhrsc =\n\u2211\nP(O(tj+1) = s|O(tj) = r,O(t1), . . . , O(tj\u22121))1[O(tj) = r]. (2.8)\nIn addition, Titman and Sharples showed that a modified test was required when the data consisted of a\nmixture of discrete observations and exact death times. However, in this paper we only consider the case of\npanel observation of all transitions.\n3\n3 The null distribution of the AH\/F statistic\nBoth the AH\/F test and the T\/S test have null distributions which can be naively approximated by \u03c72 with\ndegrees of freedom \u03c72 with degrees of freedom C \u2212 |\u03b8| as defined above. However, even asymptotically, this\napproximation is usually a lower bound. In general the statistic will have an inflated mean compared to the\nnaive degrees of freedom. It will tend to have a lower variance compared to the mean than a \u03c72 distribution.\nThe null distribution is also dependent on the choice of categorization for observations, even if the number\nof independent cells C remains the same.\nThe exact asymptotic null distribution is intractable. This is because it depends both on the unknown true\nvalue of the parameter vector \u03b8 and the observation times. Aguirre-Hernandez and Farewell proposed a\nparametric bootstrap to obtain an approximation of the null distribution. Obtaining one bootstrap sample\ninvolves generating realizations of the Markov process using the maximum likelihood estimate for the original\ndata at the sampling times from the original data, refitting the Markov model to the new data and applying\nthe goodness-of-fit test to the data. Such a method does give a test of the required size. However, Markov\nmodels and particularly hidden Markov models can be time consuming to fit, especially if there are a large\nnumber of observations or many unknown parameters. Ideally, at least 1000 bootstrap samples are required\nto get a reasonable approximation of the 95% or 99% point of the null distribution. Bootstrapping becomes\nundesirable if the model takes more than a few seconds to fit.\nThe non-\u03c72 null distribution is due to the lack of efficiently of the maximum likelihood estimate to minimize\nthe statistics, and the cell counts being the sum of non-identical multinomials rather than multinomial.\nIt is well known been that a chi-squared test based on grouped data will result in a distribution which\nlies between \u03c72\nd\u2212|\u03b8| and \u03c7\n2\nd where d is the number of independent cells in the contingency table and |\u03b8| the\nnumber of unknown parameters fitted from the data (Chernoff (1954); Kendall and Stuart (1961)).\nBy considering the joint distribution of the observed counts in the contingency table and the score function of\nthe log-likelihood, better asymptotic approximations to the null distributions of the AH\/F and T\/S statistics\nmay be derived.\n4 Derivation of the approximation\nSuppose we have panel observed data from a Markov or hidden Markov process. For notational convenience,\nwe suppose each observation is arbitrarily categorized into category c = 1, . . . , C, where c encompasses\nobservation quantile, time quantile, covariate and categorization by previous observed state.\nEach observation from a panel observed Markov model can be considered as a multinomial random variable.\nAn observation, xc,i, i = 1, . . . , nc within category c is non-identical multinomial, such that\nxc,i \u223c Multinomial(1, (p1(zc,i, \u03b8), . . . , pR(zc,i, \u03b8))) (4.1)\nwhere zc,i is the covariate vector corresponding to that observation (which we allow to include both the last\nobserved state and the time between observations) and \u03b8 the parameter vector of length M . Similarly, for\n4\nhidden Markov models, since the likelihood for an individual can be factorized as\nLi = P (O(t1))P (O(t2)|O(t1)) . . . P (O(tn)|O(t1), . . . , O(tn\u22121))\nthe observed states can be again considered Multinomial as in (4.1), provided zc,i also includes all the\nobserved states up to that time.\nWe can therefore write either the AH\/F or T\/S statistic as\nT (x, \u03b8\u02c6) =\nC\u2211\nc=1\nR\u2211\nr\n(orc \u2212 erc(\u03b8\u02c6))\n2\nerc(\u03b8\u02c6)\n(4.2)\nwhere or,c =\n\u2211\ni 1{xc,i = \u03b4r}, er,c(\u03b8\u02c6) =\n\u2211\ni pr(zc,i, \u03b8\u02c6) and \u03b4r is a vector of length R with rth entry 1 and all\nother entries zero.\nFor a standard chi-squared test on multinomial data, O, the vector of length RC with entries orc, is a\nsufficient statistic for \u03b8 and so \u03b8\u02c6 is a deterministic function of O. However, this isn\u2019t the case when O are\nnot multinomial. The statistic is instead a function of both O and \u03b8\u02c6.\nTo proceed we first define \u03c5(\u03b8) to be a vector of length RC with entries\n\u03c5rc(\u03b8) =\n(orc \u2212 erc(\u03b8))\nerc(\u03b8)\n1\n2\n.\nAs\nT (x, \u03b8\u02c6) = \u03c5(\u03b8\u02c6)T\u03c5(\u03b8\u02c6),\nthe aim is then to determine the distribution of \u03c5(\u03b8\u02c6).\nWe firstly assume that the maximum likelihood estimate gives a consistent estimate such that \u03b8\u02c6\np\n\u2212\u2192 \u03b8, and\nthat we may therefore Taylor expand \u03c5(\u03b8\u02c6) about \u03b8. This gives\n\u03c5(\u03b8\u02c6) = \u03c5(\u03b8) +B(\u03b8\u02c6 \u2212 \u03b8) + op(1), (4.3)\nwhere B is a RC \u00d7 M matrix with entries \u2202erc(\u03b8)\n\u2202\u03b8m\n1\nerc(\u03b8)\n1\n2\n. Also, from standard asymptotic results, (\u03b8\u02c6 \u2212\n\u03b8) = (EI(\u03b8))\u22121U(\u03b8) + op(1), where U(\u03b8) =\n\u2202l(\u03b8)\n\u2202\u03b8\nis the score function and EI(\u03b8) = \u2212E( \u2202\n2l(\u03b8)\n\u2202\u03b8T \u2202\u03b8\n). Hence,\nasymptotically, \u03c5(\u03b8\u02c6) can be viewed as a linear function of \u03c9 = (U(\u03b8), v(\u03b8)), a vector of length M + RC.\nHence\n\u03c5(\u03b8\u02c6)\nd\n\u2212\u2192 A\u03c9, (4.4)\nwhere\nA =\n[\nBE(I(\u03b8))\u22121 I\n]\nand I is a RC \u00d7RC identity matrix.\nWe therefore seek the covariance matrix of \u03c9.\nFirstly note that\nV ar(U(\u03b8)) = EI(\u03b8). (4.5)\n5\nThe vector O is made up of blocks of length R which are the sum of independent, non-identical multinomials\nof size 1. O therefore has expectation e(\u03b8) and covariance matrix which is block diagonal,\n\u03a3 =\n\uf8ee\n\uf8ef\uf8ef\uf8f0\n\u03a31\n. . .\n\u03a3C\n\uf8f9\n\uf8fa\uf8fa\uf8fb\nwhere\n(\u03a3c)rs =\n\uf8f1\uf8f2\n\uf8f3\n\u2211nc\ni pr(zc,i, \u03b8)(1 \u2212 pr(zc,i, \u03b8)) r = s\u2211nc\ni pr(zc,i, \u03b8)ps(zc,i, \u03b8) r 6= s\nfor c = 1, . . . , C. \u03c5(\u03b8) therefore has mean zero and covariance matrix given by P\u03a3PT , where P is a RC\u00d7RC\ndiagonal matrix with elements (erc(\u03b8))\n\u2212 12 . Thus Cov(\u03c5(\u03b8)) = P\u03a3PT .\nWe then need to find Cov(U(\u03b8), \u03c5(\u03b8)). Since both U(\u03b8) and O are functions of the full data x, we can\ncondition on x.\nCov(U(\u03b8),O) = Cov[E(U(\u03b8)|x),E(O|x)] +E[Cov(U(\u03b8),O|x)] (4.6)\nThe second term of the right hand side of equation 4.6 is zero because given x, U(\u03b8) and O are fully\ndetermined, moreover\nCov(U(\u03b8),O) = Cov(U(\u03b8)|x,O|x).\nNote that the mth component of U(\u03b8) can be written as\nUm(\u03b8) =\n\u2211\nc\nnc\u2211\ni=1\n(\n\u2202p(zc,i, \u03b8)\n\u2202\u03b8m\n1\np(zc,i, \u03b8)\n)Txc,i,\nthat the (r, c) entry of O is just\n\u2211nc\ni x(c,i)(r) where x(c,i)(r) denotes the rth entry of the vector xc,i, and that\neach of the individual observations xc,i are independent. Then\nCov(Um(\u03b8), orc) =\nnc\u2211\ni\nCov((\n\u2202p(zc,i, \u03b8)\n\u2202\u03b8m\n1\np(zc,i, \u03b8)\n)Txc,i, \u03b4\nT\nr xc,i)\nwhere \u03b4r is a vector of length R with rth entry 1 and all other entries zero. Further\nCov(Um(\u03b8), orc) =\nnc\u2211\ni\nE\n[((\n\u2202p(zc,i, \u03b8)\n\u2202\u03b8m\n1\np(zc,i, \u03b8)\n)T\nxc,i\n)(\n\u03b4Tr xc,i\n)]\n\u2212E\n[\n(\n\u2202p(zc,i, \u03b8)\n\u2202\u03b8m\n1\np(zc,i, \u03b8)\n)Txc,i\n]\nE\n[\n\u03b4Tr xc,i\n]\n.\n(4.7)\nThe ith observation only contributes a non-zero value for both Um and orc if the observation is in cell r with\nprobability pr(zc,i, \u03b8). Moreover, the expected contribution to Um from observation i is 0. Hence equation\n4.7 reduces to\nCov(Um(\u03b8), orc) =\n\u2211\nic\n\u2202pr(zc,i, \u03b8)\n\u2202\u03b8m\n.\nIf we denote \u03a8 = Cov(U(\u03b8),O), then it follows that Cov(U(\u03b8), \u03c5(\u03b8)) = \u03a8P and moreover that \u03c9 has\ncovariance\n\u2126 =\n[\nE(I(\u03b8)) PT\u03a8T\n\u03a8P P\u03a3PT\n]\n.\n6\nCombining this result with (4.4), it follows that \u03c5(\u03b8\u02c6) has an asymptotic multivariate normal distribution with\nmean vector 0 and covariance matrix V = A\u2126AT . T (x, \u03b8\u02c6) can therefore be expressed as a scalar product of\nsuch a multivariate normal.\nEquivalently,\nT\nd\n\u2212\u2192\nM\u2211\ni=1\n\u03bbiX\n2\ni\nwhere \u03bb1, . . . , \u03bbM are the eigenvalues of covariance matrix V and X\n2\n1 , . . . , X\n2\nM are i.i.d. \u03c7\n2\n1.\nThe null distribution of T therefore tends towards a distribution with characteristic function given by\n\u03c8(u) =\nR\u220f\nj=1\n(1 \u2212 2i\u03bbju)\n\u22120.5.\nThe p-value of a particular point can then be found numerically by applying the Gil-Pelaez formula (Gil-\nPelaez (1951)). This gives\nP(T > x) =\n1\n2\n+\n\u222b \u221e\n\u2212\u221e\nIm(\n\u03c8(u) exp (\u2212iux)\n2piu\n)du.\nOf course, \u03bb1, . . . , \u03bbM , and therefore P(T > x), depend on the unknown true value of \u03b8. In the same way\nthat the parametric bootstrap approach takes \u03b8 = \u03b8\u02c6 when simulating new datasets, in our approximation we\nalso take \u03b8 = \u03b8\u02c6.\nIn order to compute P(T > x) for a given \u03b8, we need\n\u2202prc(zc,i,\u03b8)\n\u2202\u03b8m\nand the expected Fisher information matrix\nE(I(\u03b8)). Details of how to calculate these quantities, particularly for hidden Markov models, are given in\nAppendix A.1.\n5 Simulation study\nTo validate the method on data with realistic sample sizes and observation schemes, we consider data on\npost-heart transplantation patients. 596 patients received heart transplants between 1979 and 2000. Patients\nwere followed up until March 2005. Periodically patients had angiograms to assess the presence of cardiac\nallograpt vascopathy. Patients may be classified into one of three states. State 1, referring to absence of\nCAV, state 2, meaning mild CAV and state 3 referring to severe CAV. This dataset is available in the R\npackage msm (Jackson (2008)) where data on exact death times also available. In this paper our aim is only\nto get a realistic observation scheme for a panel observed dataset on which to base simulations, so we remove\nthe mortality data.\nIt is assumed that observation of disease free status or mild CAV is subject to classification error but that state\n3 is an absorbing state. A three-state misclassification HMM was fitted to the data via maximum likelihood,\ngiving estimate \u03b8\u02dc. Donor age and whether the patient had ischaemic heart disease before transplantation\nare both significant covariates affecting CAV onset rates (1\u2192 2 transition intensity). Donor age varied from\n6 to 61 years. Ischaemic heart disease was present in 50% of subjects.\n7\nNew data were simulated from a process with \u03b8 = \u03b8\u02dc using the same sampling times as in the original\ndataset. This involves sampling both a trajectory of the Markov model and subjecting the true states to\nmisclassification error.\nTo calculate the goodness-of-fit statistic we choose to stratify by time quantiles of the times between obser-\nvations and also by covariate values. To avoid sparse cell counts, we limit the number of covariate groups\nto two. This is achieved by choosing covariate groupings based on whether q12(zi, \u03b8\u02c6) for observation i, lies\nabove or below the median for the whole sample. This means that the cell boundaries are themselves a\nfunction of the data. This contradicts an assumption made in the derivation of section 4, that the groups\ninto which each observation is placed is fixed in advance. Such random cell boundaries are known to have\nsome effect on the theoretical distribution of \u03c72 tests (Moore (1971)). The naive \u03c72 approximation has 18\ndegrees of freedom in this case.\n10000 datasets were generated and the goodness-of-fit statistic was computed for each. Additionally, using\nthe methods of section 4, the p-value for the particular dataset was calculated. The 95% point for the\noriginal dataset was calculated as 30.46 using the asymptotic approximation. After obtaining 10000 bootstrap\nsamples, the 95% point was found to be at 30.78 and 5.42% of samples exceeded 30.46. In contrast the naive\n\u03c7218 approximation gives the 95% point at 28.87 and has size 8.12%. Figure 1 shows a comparison of the\nthree estimates of the null distribution.\nFigure 1: Comparison of empirical bootstrap null distribution with naive \u03c7218 approximation and improved\napproximation.\n10 20 30 40\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\nT\nF\nBootstrap\nImproved\nNaive\nFor each bootstrap sample we also calculated the p-value of the goodness-of-fit test, based on the refitted\nmaximum likelihood estimate and the simulated dataset, using the improved approximation. This gave\nsimilar results, with 5.44% of samples having a p-value below 0.05. Figure 2 shows the distribution of upper\n8\np-values for the naive and improved approximations.\nFigure 2: Distribution of p-values for the naive \u03c7218 and improved approximations.\n0.0 0.2 0.4 0.6 0.8 1.0\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\np\nF\nNaive\nImproved\nIdeal\n6 Discussion\nAn asymptotic approximation to the null distribution of the AH\/F and T\/S statistics has been derived. The\nmethod allows a more accurate p-value to be calculated in cases where the naive \u03c72\nC\u2212|\u03b8| approximation is poor\nand bootstrapping is prohibitively time consuming. This significantly improves the practical applicability of\nthe AH\/F and T\/S tests of goodness-of-fit.\nThe simulated example showed that the approximation is quite accurate for relative small sample sizes and\ncell frequencies. For instance, some cell categories only had 8 observations in them. The effect of small\ncell counts seems to be that the approximation is slightly anti-conservative. We would expect the degree\nof inaccuracy to increase for datasets that give tests with very sparse contingency tables. In these cases a\nparametric bootstrap may still be required. It is not clear to what extent cell boundaries that depend on the\ndata have on the accuracy of the approximation, but the effect seemed minimal in the simulated example.\nA goodness-of-fit suite has recently been incorporated into the R package msm (Jackson (2008)), including\nthe AH\/F and T\/S goodness-of-fit tests. Code to compute more accurate p-values for models fitted using\nmsm, based on the results of this paper, has been developed and is available from the author.\n9\nA Appendix\nA.1 Calculation of first derivatives and expected Fisher information\nFor Markov models, prs(zc,i, \u03b8) just refers to a particular entry, e.g. (r, s) of some transition probability\nmatrix P (t0, t1; \u03b8). Kalbfleisch and Lawless (1985) showed, for time homogeneous Markov models, where\nP (t0, t1) = P (t),\n\u2202P (t)\n\u2202\u03b8m\n= UVmU\n\u22121\nwhere Vm is a M \u00d7M matrix with (i, j) entry\uf8f1\uf8f2\n\uf8f3\ng\n(m)\nij\n(exp (dit)\u2212exp (djt))\n(di\u2212dj)\ni 6= j\ng\n(m)\nii t exp (dit) i = j\nwhere g\n(m)\nij is the (i, j) entry of the matrix Gm = U\n\u22121( \u2202Q\n\u2202\u03b8m\n)U and d1, . . . , dR are the eigenvalues of the\nintensity matrix Q(\u03b8), which are assumed to be distinct. Derivatives can also be calculated in the presence of\na singular matrix U , but this is unlikely to arise at the maximum likelihood estimate in practical applications.\nFor hidden Markov models, prs(zc,i, \u03b8) = P(ok = s|o1, . . . , ok\u22121). If we define a vector \u03be, with rth entry\n\u03ber = P (xk\u22121 = r|o1, . . . , ok\u22121), then\nP(ok = l|o1, . . . , ok\u22121) =\nR\u2211\nj=1\nR\u2211\ns=1\n\u03bejpjs(tk\u22121, tk)pisl, (A.1)\nwhere tk\u22121 and tk are the (k \u2212 1)th and kth observation times for the subject, pjs(tk\u22121, tk) is the (j, s)\nentry of transition probability matrix P (tk\u22121, tk; \u03b8) and pisl is the (s, l) entry in the matrix of classification\nprobabilities. The derivative of A.1 can therefore be found using the product rule.\nCalculation of\n\u2202\u03bej\n\u2202\u03b8m\ncan be done iteratively. Lystig and Hughes (2002) showed that the Forward algorithm\nfor computing the likelihood of a HMM can be generalized to compute the first and second derivatives. Let\n\u03b1k(i) = P(X(tk) = i, o1, . . . , ok)\nand\n\u03c6k(\u03b8m, i) =\n\u2202\n\u2202\u03b8m\nP(X(tk) = i, o1, . . . , ok).\nThen\n\u03b1k(j) =\nR\u2211\nj=1\n\u03b1k\u22121(i)pij(tk \u2212 tk\u22121)pijok\nand\n\u03c6k(\u03b8m, i) =\nR\u2211\ni=1\npij(tk \u2212 tk\u22121)\n(\n\u03c6k\u22121(\u03b8m, i)pijok + \u03b1k\u22121(i)\n\u2202pij,ok\n\u2202\u03b8m\n)\n+\u03b1k\u22121(i)pijok\n\u2202pij(tk \u2212 tk\u22121)\n\u2202\u03b8m\nSince \u03bei =\n\u03b1k(i)P\nR\nj=1 \u03b1k(i)\n,\n\u2202\u03bei\n\u2202\u03b8m\n=\n(\n\u2211R\nj=1 \u03b1k(j))\u03c6k(\u03b8u, i)\u2212 \u03b1k(i)\n\u2211T\nj=1 \u03c6k(\u03b8m, j)\n(\n\u2211R\nj=1 \u03b1k(j))\n,\n10\nand can therefore be computed for each k by iteratively computing \u03b1 and \u03c6.\nFinally, the expected Fisher information for both Markov and hidden Markov models is given by\nE(\u2212\n\u22022l\n\u2202\u03b8u\u2202\u03b8v\n) =\n\u2211\ni,c\nR\u2211\nl=1\n1\npl(zc,i, \u03b8)\n\u2202pl(zc,i, \u03b8)\n\u2202\u03b8u\n\u2202pl(zc,i, \u03b8)\n\u2202\u03b8v\nso only requires calculation of first derivatives.\nReferences\nAguirre-Hernandez R, Farewell V. T. A Pearson-type goodness-of-fit test for stationary and time-continuous\nMarkov regression models. Statistics in Medicine 2002; 21:1899-1911.\nBureau A, Shiboski S, Hughes J. P. Applications of continuous time hidden Markov models to the study of\nmisclassified disease outcomes. Statistics in Medicine 2003; 22: 441-462.\nChernoff H, Lehmann E.L. The use of maximum likelihood estimates in \u03c72 tests for goodness-of-fit. The\nAnnals of Mathematical Statistics 1954; 25:576-586.\nCox D.R, Miller H.D. The theory of stochastic processes. Chapman and Hall, London, 1965.\nFisher R. A. The conditions under which \u03c72 measures the discrepancy between observation and hypothesis.\nJournal of the Royal Statistical Society 1924; 87: 442-450.\nGil-Pelaez J. Note on the inversion theorem. Biometrika 1951; 38: 481-482.\nJackson C.H. msm: Multi-state Markov and hidden Markov models in continuous time. R package version\n0.8.1 2008.\nJackson C.H, Sharples L.D. Hidden Markov models for the onset and progression of bronchiolitis obliterans\nsyndrome in lung transplant recipients. Statistics in Medicine 2002; 21: 113-128\nKalbfleisch J.D, Lawless J.F. The analysis of panel data under a Markov assumption. Journal of the American\nStatistical Association 1985; 80:863-871.\nKendall M.G, Stuart A. The advanced theory of statistics. Vol.2 London, 1961.\nLystig T.C, Hughes J.P. Exact computation of the observed information matrix for hidden Markov models.\nJournal of Computational and Graphical Statistics 2002; 11: 678-689.\nMoore D.S. A chi-square statistic with random cell boundaries. The Annals of Mathematical Statistics 1971;\n42: 147-156.\nSatten G.A, Longini I.M. Markov chains with measurement error: estimating the \u2018true\u2019 course of a marker of\nthe progression of Human Immunodeficiency Virus disease. Journal of the Royal Statistical Society Series\nC 1996, 45: 265-309.\n11\nStavola B. L de. Testing departures from time homogeneity in multistate Markov processes. Journal of the\nRoyal Statistical Society. Series C 1988; 37:242-250.\nTitman A. C, Sharples L. D. A general goodness-of-fit test for Markov and hidden Markov models. Statistics\nin Medicine 2008; 27: 2177-2195.\n12\n"}