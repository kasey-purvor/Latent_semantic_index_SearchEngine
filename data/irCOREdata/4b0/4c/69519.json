{"doi":"10.1109\/TNS.2004.829977","coreId":"69519","oai":"oai:eprints.lancs.ac.uk:26345","identifiers":["oai:eprints.lancs.ac.uk:26345","10.1109\/TNS.2004.829977"],"title":"Second Level Trigger of the ATLAS Experiment at CERN's LHC.","authors":["ATLAS, TDAQ authorlist","Smizanska, Maria"],"enrichments":{"references":[{"id":1008194,"title":"Proposal for a General-Purpose pp Experiment at the Large Hadron Collider at CERN,\u201d ATLAS Collaboration, European Centre for Particle Physics,","authors":[],"date":"1994","doi":null,"raw":"\u201cATLAS: Technical Proposal for a General-Purpose pp Experiment at the Large Hadron Collider at CERN,\u201d ATLAS Collaboration, European Centre for Particle Physics, CERN, Geneva, Switzerland, CERN\/LHCC\/94\u201343, 1994.","cites":null},{"id":1008771,"title":"Studies for a common selection software environment in ATLAS: From level-2 trigger to offline reconstruction,\u201d presented at the Nuclear Sciences Symp.,","authors":[],"date":"2003","doi":null,"raw":"W. Wiedenmann et al., \u201cStudies for a common selection software environment in ATLAS: From level-2 trigger to offline reconstruction,\u201d presented at the Nuclear Sciences Symp., Portland, OR, Oct. 2003. Authorized licensed use limited to: Lancaster University Library. Downloaded on May 1, 2009 at 11:32 from IEEE Xplore.  Restrictions apply.","cites":null},{"id":1008478,"title":"The baseline dataflow system of the ATLAS trigger and DAQ,\u201d presented at the 9th Workshop Electronics for LHC Experiments,","authors":[],"date":"2003","doi":null,"raw":"M. Abolins et al., \u201cThe baseline dataflow system of the ATLAS trigger and DAQ,\u201d presented at the 9th Workshop Electronics for LHC Experiments, Amsterdam, The Netherlands, Oct. 2003.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2004-06","abstract":"The ATLAS trigger reduces the rate of interesting events to be recorded for off-line analysis in three successive levels from 40 MHz to \u223c100 kHz, \u223c2 kHz and \u223c200 Hz. The high level triggers and data acquisition system are designed to profit from commodity computing and networking components to achieve the required performance. In this paper, we discuss data flow aspects of the design of the second level trigger (LVL2) and present results of performance measurements","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/69519.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/26345\/1\/getPDF3.pdf","pdfHashValue":"afc5c63dec4d6a7c7b8b6194f03b6f4a20f9432b","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:26345<\/identifier><datestamp>\n      2018-01-24T02:48:06Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5143<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Second Level Trigger of the ATLAS Experiment at CERN's LHC.<\/dc:title><dc:creator>\n        ATLAS, TDAQ authorlist<\/dc:creator><dc:creator>\n        Smizanska, Maria<\/dc:creator><dc:subject>\n        QC Physics<\/dc:subject><dc:description>\n        The ATLAS trigger reduces the rate of interesting events to be recorded for off-line analysis in three successive levels from 40 MHz to \u223c100 kHz, \u223c2 kHz and \u223c200 Hz. The high level triggers and data acquisition system are designed to profit from commodity computing and networking components to achieve the required performance. In this paper, we discuss data flow aspects of the design of the second level trigger (LVL2) and present results of performance measurements.<\/dc:description><dc:date>\n        2004-06<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/26345\/1\/getPDF3.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/TNS.2004.829977<\/dc:relation><dc:identifier>\n        ATLAS, TDAQ authorlist and Smizanska, Maria (2004) Second Level Trigger of the ATLAS Experiment at CERN's LHC. IEEE Transactions on Nuclear Science, 51 (3 Part). pp. 909-914. ISSN 0018-9499<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/26345\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1109\/TNS.2004.829977","http:\/\/eprints.lancs.ac.uk\/26345\/"],"year":2004,"topics":["QC Physics"],"subject":["Journal Article","PeerReviewed"],"fullText":"IEEE TRANSACTIONS ON NUCLEAR SCIENCE, VOL. 51, NO. 3, JUNE 2004 909\nThe Second Level Trigger of the ATLAS Experiment\nat CERN\u2019s LHC\nA. dos Anjos, M. Abolins, S. Armstrong, J. T. Baines, M. Barisonzi, H. P. Beck, C. P. Bee, M. Beretta, M. Biglietti,\nR. Blair, A. Bogaerts, V. Boisvert, M. Bosman, H. Boterenbrood, D. Botterill, S. Brandt, B. Caron, P. Casado,\nG. Cataldi, D. Cavalli, M. Cervetto, M. Ciobotaru, G. Comune, A. Corso-Radu, E. Palencia Cortezon, R. Cranfield,\nG. Crone, J. Dawson, B. Di Girolamo, A. Di Mattia, M. Diaz Gomez, R. Dobinson, J. Drohan, N. Ellis, M. Elsing,\nB. Epp, Y. Ermoline, F. Etienne, S. Falciano, A. Farilla, M. L. Ferrer, D. Francis, S. Gadomski, S. Gameiro,\nS. George, V. Ghete, P. Golonka, S. Gonz\u00e1lez, B. Gorini, B. Green, M. Grothe, M. Gruwe, S. Haas, C. Haeberli,\nY. Hasegawa, R. Hauser, C. Hinkelbein, R. Hughes-Jones, P. Jansweijer, M. Joos, A. Kaczmarska, K. Karr,\nA. Khomich, G. Kieft, E. Knezo, N. Konstantinidis, K. Korcyl, W. Krasny, A. Kugel, A. Lankford, G. Lehmann,\nM. LeVine, W. Li, W. Liu, A. Lowe, L. Luminari, T. Maeno, M. Losada Maia, L. Mapelli, B. Martin, R. McLaren,\nC. Meessen, C. Meirosu, A. G. Mello, G. Merino, A. Misiejuk, R. Mommsen, P. Morettini, G. Mornacchi, E. Moyse,\nM. M\u00fcller, Y. Nagasaka, A. Nairz, K. Nakayoshi, A. Negri, N. Nikitin, A. Nisati, C. Padilla, I. Papadopoulos,\nF. Parodi, V. Perez-Reale, J. Petersen, J. L. Pinfold, P. Pinto, G. Polesello, B. Pope, D. Prigent, Z. Qian, S. Resconi,\nS. Rosati, D. A. Scannicchio, C. Schiavi, J. Schlereth, T. Schoerner-Sadenius, E. Segura, J. M. Seixas, T. Shears,\nM. Shimojima, S. Sivoklokov, M. Smizanska, R. Soluk, R. Spiwoks, S. Stancu, C. Stanescu, J. Strong, S. Tapprogge,\nL. Tremblet, F. Touchard, V. Vercesi, V. Vermeulen, A. Watson, T. Wengler, P. Werner, S. Wheeler, F. J. Wickens,\nW. Wiedenmann, M. Wielers, Y. Yasu, M. Yu, H. Zobernig, and M. Zurek\nAbstract\u2014The ATLAS trigger reduces the rate of interesting\nevents to be recorded for off-line analysis in three successive levels\nfrom 40 MHz to 100 kHz, 2 kHz and 200 Hz. The\nhigh level triggers and data acquisition system are designed to\nprofit from commodity computing and networking components to\nachieve the required performance. In this paper, we discuss data\nflow aspects of the design of the second level trigger (LVL2) and\npresent results of performance measurements.\nIndex Terms\u2014ATLAS, cluster, data acquisition, high-energy\nphysics, network, object oriented, performance, testbed, trigger.\nI. INTRODUCTION\nTHE ATLAS experiment [1] will use very high energyproton-proton collisions provided by CERN\u2019s LHC for a\nwide research program including a search for Higgs bosons.\nThe detector is composed of specialized subdetectors to register\nthe properties of the particles produced: an inner detector inside\na magnetic field of 2 Tesla measuring tracks, a calorimeter to\nmeasure energy and finally a muon detector.\nDue to the high event rate and noisy background conditions,\nthe experiment will be equipped with a powerful trigger system\nsubdivided into three levels [2] as shown in Fig. 1.\nThe first level trigger (LVL1) is directly connected to the de-\ntector front-end electronics of the calorimeter and muon de-\ntectors. Data of accepted events are stored in pipeline memo-\nries connected to readout drivers (RODs) and made available to\nthe high-level triggers (HLT) through\nManuscript received May 27, 2004.\nA. dos Anjos is with the Signal Processing Laboratory (LPS),\nCOPPE\/PEE\/UFRJ, Cidade Universit\u00e1ria, RJ 21945-970 Rio de Janeiro,\nBrazil (e-mail: Andre.dos.Anjos@cern.ch).\nM. Zurek (on behalf of the remaining authors) is with the ATLAS High-Level\nTriggers Groups, CERN-CH1211, Geneva 23, Switzerland.\nDigital Object Identifier 10.1109\/TNS.2004.829977\nFig. 1. Principal components of the Data Flow and HLT systems.\n(ROBs). The LVL1 will be built using custom hardware com-\nponents in order to cope with the high input rate of 40 MHz and\nwill deliver a maximum output rate of 75 kHz, upgradeable to\n100 kHz.\nThe output of the LVL1 trigger defines regions in the detector\nwhere the signal exceeds programmable thresholds. These so\ncalled Regions of Interest (RoI) are used as seeds for the Second\nLevel Trigger (LVL2). By only looking at data in the LVL1 RoIs,\nit is also possible to reduce the amount of data transferred into\nLVL2 processors to less than 2% of the total event data (1.5 MB)\nwhile still keeping efficient classification algorithms. LVL2 se-\n0018-9499\/04$20.00 \u00a9 2004 IEEE\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on May 1, 2009 at 11:32 from IEEE Xplore.  Restrictions apply.\n910 IEEE TRANSACTIONS ON NUCLEAR SCIENCE, VOL. 51, NO. 3, JUNE 2004\nFig. 2. Baseline implementation of the data flow and HLT systems. Networks and switches around the respective subsystems are represented by bubbles (L2N \u2013\nthe Level 2 Network, EBN \u2013 the event builder network, EFN \u2013 the event filter network and SW, minor switches) while processors as boxes.\nlection algorithms request data from variable number of RoIs,\ntypically 1 or 2. The average spans .\nIt is expected that the average processing time per event to\nbe at most 10 ms 0. This will require 500 dual-CPU processing\nnodes. At this configuration each node should deliver a trigger\ndecision at a rate of , requiring an input bandwidth of\n.\nThe last trigger level is the event filter (EF). After a LVL2\naccept, the full event data is assembled and redirected into spe-\ncialized processing farms, where elaborated filtering algorithms\nare applied. This level will still reduce the trigger rate to no more\nthan . If the event is accepted, it is recorded to perma-\nnent storage for later off-line analysis.\nII. THE SECOND LEVEL TRIGGER\nThe LVL2 system is part of the ATLAS data flow [3] as illus-\ntrated in Fig. 2. The main components are the readout system\n(ROS), the LVL2 system, the event builder (EB) and the EF.\nData for events accepted by the LVL1 trigger are sent to the\nROSs and, in parallel, information on the location of RoIs iden-\ntified by LVL1 is sent to the LVL2 supervisor. The LVL2 super-\nvisor sends the LVL1 Result to a LVL2 Processor (L2P), where\nthe event selection is performed. Using the LVL1 Result as guid-\nance, specialized LVL2 algorithms request a subset of the event\ndata from the ROSs to perform the selection. This task consists\nof determining if the event generated by the LHC contains char-\nacteristics classifying it as an event of interest through energy\nsums, particle shower containment, track-finding and track-fit-\nting algorithms among other methods [2]. For events accepted\nby LVL2, details are sent to a \u201cpseudo\u201d ROS (pROS) to be in-\ncluded in the event. The L2P sends the LVL2 Decision back to\nthe LVL2 supervisor (L2SV), which forward it to the EB.\nA. The Event Processing Framework\nThe LVL2 components are applications based on the ATLAS\ndata flow framework that provides a set of tools that enable\nthese applications to exchange event data, to be remotely con-\ntrolled, to report messages and to be monitored. Applications in\nthis framework exchange data through a set of predefined mes-\nsages that may run over standard TCP\/IP, UDP, or raw Ethernet.\nBoth the framework and the applications are designed using ob-\nject-oriented techniques [4] and fully implemented in C++.\nFig. 3 shows the abstract design of the software running in an\nL2P. It is composed of an input task that receives data from the\nL2SV and a set of concurrent worker tasks each processing one\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on May 1, 2009 at 11:32 from IEEE Xplore.  Restrictions apply.\nDOS ANJOS et al.: SECOND LEVEL TRIGGER OF ATLAS EXPERIMENT AT CERN\u2019S LHC 911\nFig. 3. UML collaboration diagram of the application running on the L2P.\nevent. The synchronization mechanism between the input and\nthe workers is an event queue, which is protected against con-\ncurrent access. Each worker, after extracting one LVL1 result\nout of the input queue, will start the event selection chain.\nThe L2P tasks, including workers, have been implemented as\nthreads. This approach is interesting because symmetric multi-\nprocessor architectures, computers with multiple processors of\nthe same type sharing a common memory bank, can be more ef-\nficiently exploited to achieve the required LVL2 trigger rate at a\nreduced cost with a smaller farm as opposed to a process-based\napproach. In addition, the L2PU becomes insensitive to laten-\ncies as the ESS may run in one thread whilst waiting for data in\nanother thread.\nMultithreading imposes a few restrictions to the software\nthat runs concurrently. Initially it needs to be made thread-safe,\nwhich means that concurrent write access to globally visible\nvariables should be protected by process locks. Software that\nruns in multiple threads of execution also needs to be made\nthread-efficient which means that the number of locks should\nbe minimized to avoid thread inter-locking. Excessive locking\nmay slow down the processing considerably by enforcing\nsequential execution of large parts of the threads.\nIt is by far not evident to make C++ code thread-safe and\neven less to make it thread-efficient. For example, by using\nthe latest GNU C++ compiler, locks are systematically used in\nmemory allocators for standard container objects. Naive use of\nlists, maps or even vectors in parallel threads of execution may\nconsiderably reduce the performance of applications. The so-\nlution here is to change the standard memory allocator by an\nallocator without locks. However, this method will fail when li-\nbraries pre-compiled with standard allocators are used.\nThe ESS is dynamically loaded. It is developed in an off-line\nframework, restricted to use only services also available on-line\nand respecting the programming guidelines implied by the L2P\napplication design. Many of the existing off-line tools available\nare also implanted into the application running at an L2P [5]. For\ntesting, there is also a possibility to emulate the complex ESS.\nMeasurements presented below were made with the dummy ver-\nsion of the ESS. The dummy ESS implementation emulates data\nprocessing, including the multistep analysis and data requests.\nIII. MEASUREMENTS\nA. Critical Parameters of the Readout System\nSome measurements varying parameters that are critical for\nthe performance of the RoI data collection have been made.\nThough the number and average size of the RoIs depend on\nphysics, the ROB concentration factor (the number of ROBs\ncontained within one ROS which may be read in one opera-\ntion) is a parameter that may be chosen to optimize the readout.\nThe optimum number of parallel worker threads depends on the\nnumber of CPUs in one node and the idle time incurred when\nwaiting for ROSs to send their data. The latter depends obvi-\nously on the size of the RoI as well as the ROB concentration\nfactor. In the following measurements, the L2P collects only the\nRoI data, i.e., it does not execute any LVL2 algorithm.\nThe contribution of RoI data collection to the L2P event pro-\ncessing time as a function of the RoI size and the number of\nROSs over which the RoI is distributed is shown in Fig. 4.\nThe range of measurements presented in this figure corresponds\nto the currently expected RoI sizes and their distribution over\nROBs, e.g., an RoI in the Liquid Argon (the electromag-\nnetic ATLAS calorimeter section) detector is expected to be dis-\ntributed over 13 to 16 ROBs [2] and have a size of approximately\n16 kB. It can be observed from these results that the time taken\nto collect RoI data contributes, in the worst case, to less than\n10% to the average event processing time budget.\nFig. 5 shows the performance of a single L2P as the inverse\nof the LVL1 accept rate sustained versus the number of worker\nthreads. In this measurement, the RoI has a fixed size of 16 kB.\nThe different curves represent the RoI being collected from 2,\n4, 8, or 16 ROSs. For example, the upper curve represents the\nresults of collecting a 16 kB RoI from 16 ROSs, each of which\ncontributes 1 kB to the RoI. The results indicate that the op-\ntimum number of worker threads, in this setup, is approximately\nthree and is independent of the number of ROSs from which the\nRoI is being collected. In addition, the results show that for the\nconditions of this measurement and for three worker threads, the\ncollection of RoI data contributes less than 10% to the average\nevent processing time of 10 ms.\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on May 1, 2009 at 11:32 from IEEE Xplore.  Restrictions apply.\n912 IEEE TRANSACTIONS ON NUCLEAR SCIENCE, VOL. 51, NO. 3, JUNE 2004\nFig. 4. Summary of the performance of the RoI data collection for various combinations of RoI sizes (in bytes) and worker threads. The plot shows the inverse\nof the input L2SV rate as a function of the number of ROSs that contributes with RoI data.\nFig. 5. The inverse of the LVL1 rate in a LVL2 system as a function of the number of worker threads for different RoI sizes.\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on May 1, 2009 at 11:32 from IEEE Xplore.  Restrictions apply.\nDOS ANJOS et al.: SECOND LEVEL TRIGGER OF ATLAS EXPERIMENT AT CERN\u2019S LHC 913\nFig. 6. Event rate per L2P as a function of the number of L2Ps in the system for a different number of ROBs per RoI. Each ROB contributes with an equal amount\nof data for the RoI in every test.\nFig. 7. L2SV sustained LVL1 accept rate versus the number of L2PUs.\nB. Scalability\nThe measurements so far presented have shown the rate at\nwhich a single L2P, with a dummy ESS, can collect RoI data.\nThe achieved performance depends on the RoI size, the ROB\nconcentration factor and the number of worker threads. For the\nsystem, however, it is also necessary to demonstrate that when\nmany L2Ps are requesting data at the same time from the same\nset of ROSs the performance of the RoI collection does not de-\ngrade unacceptably.\nFor this test, L2Ps with dummy ESS were again used, al-\nthough it should be noted that the request rate for data per L2P is\nmuch higher that it would be if the real classification algorithms\nwere included. Thus, in these measurements, each L2P gener-\nates more than ten times the normal request rate. Similarly, the\nrequests are sent to a small number of ROSs due to machinery\navailability, so that again it is important to ensure that the total\nrequest rate did not exceed the capability of these components.\nThe tests were run in a testbed consisting of three L2SVs, four\nROSs and up to eleven L2Ps. All the nodes in this testbed were\nPCs in the same configuration as before, interconnected via a\nGigabit Ethernet switch. For this test each ROS was configured\nto emulate 12 readout links (ROBs), to give a total of 48 ROBs\nacross the four ROSs. For each request the L2P chose one of\nthe 48 ROBs at random, and in the case of 6 ROBs per RoI and\n24 ROBs per RoI requested the following 5 and 23 ROBs, re-\nspectively, wrapping round across the ROSs as necessary. Fig. 6\npresents the results of these measurements. It shows the rate of\nevents handled by each L2P and the rate of requests to each ROS\nas a function of the number of L2Ps used in the testbed for an\nRoI distributed over 1, 6, or 24 ROBs (with 1.5 kB per ROB).\nThe plot in Fig. 6 also shows that the rate per ROB decreases\nas the number of L2PUs is increased, by for 1 ROB per\nRoI and for 24 ROBs per RoI. However, with 11 L2PUs\nin the test the request rate per ROS is 17 kHz for 1 ROB per RoI\nand 8 kHz for 24 ROBs per RoI, both much higher demands on a\nROS compared to the final system. In addition, the total RoI re-\nquest rate in the small test setup is already 70 kHz and 11 kHz for\nthe two RoI sizes, respectively. Given these extreme conditions\nthe fall from scaling can be seen to be relatively modest, and it\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on May 1, 2009 at 11:32 from IEEE Xplore.  Restrictions apply.\n914 IEEE TRANSACTIONS ON NUCLEAR SCIENCE, VOL. 51, NO. 3, JUNE 2004\nseems reasonable to assume that with more typical conditions\na full-sized system will show acceptable scaling. This interpre-\ntation is reinforced by the fact that running similar tests with\nthe ROSs replaced by custom hardware ROS emulators shows\nscaling within better than 10% over the same range of RoI data\ntraffic.\nFig. 7 shows the LVL1 rate sustained by the L2SV as a func-\ntion of the number of L2Ps that it controls. In this measurement\nthe computer running the L2SV is also connected to a gigabit\nEthernet switch on which a number of PCs are also connected,\neach of which were executing the L2P code equipped with a\ndummy ESS. The L2SV was not connected to the LVL1 proto-\ntype; instead it emulated the receiving of events from this sub-\nsystem. As can be seen from the figure, the L2SV can sustain a\nLVL1 accept rate of 32 kHz when distributing RoI information\nto a single L2PU and the dependency on the number of L2PUs is\n1%. Thus, based on today\u2019s prototype implementation and PCs,\nten L2SVs are sufficient to support a LVL1 accept rate of 100\nkHz.\nIV. CONCLUSION\nWe have presented a LVL2 system that uses a farm of con-\nventional PCs running ESS guided by results from the LVL1\ntrigger. Standard Gigabit Ethernet is used to connect the indi-\nvidual processors to the readout system. All measurements were\nmade using a connection-less protocol, UDP, or raw Ethernet.\nAlthough not explicitly shown in the measurements, the perfor-\nmance differences were minimal.\nIt has been shown that the I\/O capacity of each node is largely\nsufficient. The CPU resources devoted to RoI data collection\nuse a modest fraction of the allocated time budget for a large\nrange of RoI sizes. Though concentration of multiple ROBs in\none ROS (resulting in the transfer of larger data slices if the\nROB to detector mapping is optimized) is more efficient, even\nan architecture with a concentration factor of one would still\ngive acceptable results.\nEvent parallelism using multiple threads is effective, al-\nlowing the use of multi CPU nodes to save on cost, cooling and\nfloor space. Additional threads are necessary to compensate\nthe readout latency.\nThe scalability has been measured for a system with a small\nnumber of nodes. It has been shown that, with unrealistically\nhigh data rates into each processor and a small number of ROSs,\nLVL2 rates as high as 70 kHz can be obtained with a relatively\nmodest fall of scaling.\nREFERENCES\n[1] \u201cATLAS: Technical Proposal for a General-Purpose pp Experiment\nat the Large Hadron Collider at CERN,\u201d ATLAS Collaboration,\nEuropean Centre for Particle Physics, CERN, Geneva, Switzerland,\nCERN\/LHCC\/94\u201343, 1994.\n[2] (2003) ATLAS High-Level Triggers, DAQ and DCS Technical De-\nsign Report. ATLAS Trigger and Data Acquisition Collaboration, Euro-\npean Centre for Particle Physics, CERN, Geneva, Switzerland. [Online].\nAvailable: http:\/\/cern.ch\/atlas-proj-hltdaqdcs-tdr\n[3] M. Abolins et al., \u201cThe baseline dataflow system of the ATLAS trigger\nand DAQ,\u201d presented at the 9th Workshop Electronics for LHC Experi-\nments, Amsterdam, The Netherlands, Oct. 2003.\n[4] S. Gadomski et al., \u201cExperience with multi-threaded C++ applications\nin the ATLAS dataflow software,\u201d presented at the Computing in High-\nEnergy Physics, La Jolla, CA, Mar. 2003.\n[5] W. Wiedenmann et al., \u201cStudies for a common selection software en-\nvironment in ATLAS: From level-2 trigger to offline reconstruction,\u201d\npresented at the Nuclear Sciences Symp., Portland, OR, Oct. 2003.\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on May 1, 2009 at 11:32 from IEEE Xplore.  Restrictions apply.\n"}