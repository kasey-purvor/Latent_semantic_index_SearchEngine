{"doi":"10.1109\/TNS.2004.829548","coreId":"69518","oai":"oai:eprints.lancs.ac.uk:26346","identifiers":["oai:eprints.lancs.ac.uk:26346","10.1109\/TNS.2004.829548"],"title":"Studies for a common selection software environment in ATLAS : from the Level-2 trigger to the offline reconstruction.","authors":["ATLAS, TDAQ authorlist","Smizanska, Maria"],"enrichments":{"references":[{"id":1010811,"title":"and Physics Performance Tech. Design Rep.,\u201d","authors":[],"date":"1999","doi":null,"raw":"\u201cATLAS Detector and Physics Performance Tech. Design Rep.,\u201d ATLAS Collaboration, CERN\/LHCC\/99-014 and 99-015, 1999.","cites":null},{"id":1010523,"title":"Proposal for a General-Purpose Experiment at the Large Hadron Collider at","authors":[],"date":"1994","doi":null,"raw":"\u201cATLAS: Technical Proposal for a General-Purpose Experiment at the Large Hadron Collider at CERN,\u201d ATLAS Collaboration, CERN\/LHCC\/94-43, 1994.","cites":null},{"id":1011114,"title":"The second level trigger of the ATLAS experiment at CERN\u2019s LHC,\u201d in","authors":[],"date":"2003","doi":null,"raw":"S. Armstrong et al., \u201cThe second level trigger of the ATLAS experiment at CERN\u2019s LHC,\u201d in Proc. Nuclear Science Symp., Portland, OR, Oct., 19\u201325 2003.","cites":null},{"id":1010833,"title":"Trigger Data Acquisition and Controls,","authors":[],"date":"2003","doi":null,"raw":"\u201cATLAS High-Level Trigger Data Acquisition and Controls, Tech. Design Rep.,\u201d ATLAS Collaboration, CERN\/LHCC\/2003-022, 2003.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2004-06","abstract":"The ATLAS High Level Trigger's (HLT) primary function of event selection will be accomplished with a Level-2 trigger farm and an event filter (EF) farm, both running software components developed in the ATLAS offline reconstruction framework. While this approach provides a unified software framework for event selection, it poses strict requirements on offline components critical for the Level-2 trigger. A Level-2 decision in ATLAS must typically be accomplished within 10 ms and with multiple event processing in concurrent threads. To address these constraints, prototypes have been developed that incorporate elements of the ATLAS data flow, high level trigger, and offline framework software. To realize a homogeneous software environment for offline components in the HLT, the Level-2 Steering Controller was developed. With electron\/gamma- and muon-selection slices it has been shown that the required performance can be reached, if the offline components used are carefully designed and optimized for the application in the HLT","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/69518.pdf","fullTextIdentifier":"http:\/\/eprints.lancs.ac.uk\/26346\/1\/getPDF4.pdf","pdfHashValue":"cf6a6489c6eb003bfd4e226364105e065c7ae648","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lancs.ac.uk:26346<\/identifier><datestamp>\n      2018-01-24T02:48:07Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D51:5143<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Studies for a common selection software environment in ATLAS : from the Level-2 trigger to the offline reconstruction.<\/dc:title><dc:creator>\n        ATLAS, TDAQ authorlist<\/dc:creator><dc:creator>\n        Smizanska, Maria<\/dc:creator><dc:subject>\n        QC Physics<\/dc:subject><dc:description>\n        The ATLAS High Level Trigger's (HLT) primary function of event selection will be accomplished with a Level-2 trigger farm and an event filter (EF) farm, both running software components developed in the ATLAS offline reconstruction framework. While this approach provides a unified software framework for event selection, it poses strict requirements on offline components critical for the Level-2 trigger. A Level-2 decision in ATLAS must typically be accomplished within 10 ms and with multiple event processing in concurrent threads. To address these constraints, prototypes have been developed that incorporate elements of the ATLAS data flow, high level trigger, and offline framework software. To realize a homogeneous software environment for offline components in the HLT, the Level-2 Steering Controller was developed. With electron\/gamma- and muon-selection slices it has been shown that the required performance can be reached, if the offline components used are carefully designed and optimized for the application in the HLT.<\/dc:description><dc:date>\n        2004-06<\/dc:date><dc:type>\n        Journal Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/eprints.lancs.ac.uk\/26346\/1\/getPDF4.pdf<\/dc:identifier><dc:relation>\n        http:\/\/dx.doi.org\/10.1109\/TNS.2004.829548<\/dc:relation><dc:identifier>\n        ATLAS, TDAQ authorlist and Smizanska, Maria (2004) Studies for a common selection software environment in ATLAS : from the Level-2 trigger to the offline reconstruction. IEEE Transactions on Nuclear Science, 51 (3 Part). pp. 915-920. ISSN 0018-9499<\/dc:identifier><dc:relation>\n        http:\/\/eprints.lancs.ac.uk\/26346\/<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/dx.doi.org\/10.1109\/TNS.2004.829548","http:\/\/eprints.lancs.ac.uk\/26346\/"],"year":2004,"topics":["QC Physics"],"subject":["Journal Article","PeerReviewed"],"fullText":"IEEE TRANSACTIONS ON NUCLEAR SCIENCE, VOL. 51, NO. 3, JUNE 2004 915\nStudies for a Common Selection Software\nEnvironment in ATLAS: From the Level-2 Trigger to\nthe Offline Reconstruction\nS. Armstrong, J. T. Baines, C. P. Bee, M. Biglietti, A. Bogaerts, V. Boisvert, M. Bosman, S. Brandt, B. Caron,\nP. Casado, G. Cataldi, D. Cavalli, M. Cervetto, G. Comune, A. Corso-Radu, A. Di Mattia, M. Diaz Gomez,\nA. dos Anjos, J. Drohan, N. Ellis, M. Elsing, B. Epp, F. Etienne, S. Falciano, A. Farilla, S. George, V. Ghete,\nS. Gonz\u00e1lez, M. Grothe, A. Kaczmarska, K. Karr, A. Khomich, N. Konstantinidis, W. Krasny, W. Li, A. Lowe,\nL. Luminari, C. Meessen, A. G. Mello, G. Merino, P. Morettini, E. Moyse, A. Nairz, A. Negri, N. Nikitin,\nA. Nisati, C. Padilla, F. Parodi, V. Perez-Reale, J. L. Pinfold, P. Pinto, G. Polesello, Z. Qian, S. Resconi, S. Rosati,\nD. A. Scannicchio, C. Schiavi, T. Schoerner-Sadenius, E. Segura, J. M. de Seixas, T. Shears, S. Sivoklokov,\nM. Smizanska, R. Soluk, C. Stanescu, S. Tapprogge, F. Touchard, V. Vercesi, A. Watson, T. Wengler, P. Werner,\nS. Wheeler, F. J. Wickens, W. Wiedenmann, M. Wielers, and H. Zobernig\nAbstract\u2014The ATLAS High Level Trigger\u2019s (HLT) primary\nfunction of event selection will be accomplished with a Level-2\ntrigger farm and an event filter (EF) farm, both running software\ncomponents developed in the ATLAS offline reconstruction\nframework. While this approach provides a unified software\nframework for event selection, it poses strict requirements on\noffline components critical for the Level-2 trigger. A Level-2\ndecision in ATLAS must typically be accomplished within 10\nms and with multiple event processing in concurrent threads.To\naddress these constraints, prototypes have been developed that\nincorporate elements of the ATLAS data flow, high level trigger,\nand offline framework software. To realize a homogeneous\nsoftware environment for offline components in the HLT, the\nLevel-2 Steering Controller was developed. With electron\/gamma-\nand muon-selection slices it has been shown that the required\nperformance can be reached, if the offline components used are\ncarefully designed and optimized for the application in the HLT.\nIndex Terms\u2014Programming environments, software\nreusability, triggering.\nI. INTRODUCTION\nTHE Large Hadron Collider (LHC) currently under con-struction at CERN will produce -collisions with a center\nof mass energy of TeV at a design luminosity of\ncm s . With a bunch crossing rate of 40 MHz and about\n23 interactions per bunch crossing, it requires, however, highly\nselective trigger systems to reduce the expected interac-\ntions per second to an acceptable rate of a few hundred Hz.\nATLAS [1] is one of the two large general purpose experiments\nat the LHC and covers a widely diversified physics program [2],\nranging from discovery physics to precision measurements of\nStandard Model parameters. ATLAS has an inner detector for\nprecision tracking mounted inside a superconducting magnet\nwith 2 T field strength. Outside the solenoid follow electromag-\nManuscript received November 14, 2003; revised February 3, 2004.\nThe authors are with THE ATLAS HIGH LEVEL TRIGGER\nGROUP. Information is available online at: http:\/\/atlas.web.cern.ch\/Atlas\/\nGROUPS\/DAQTRIG\/HLT\/AUTHORLISTS\/nss2003.pdf.\nDigital Object Identifier 10.1109\/TNS.2004.829548\nnetic and hadronic calorimeters. Muon identification is achieved\nwith a high precision muon spectrometer. The total number of\nreadout channels is about .\nGiven the required large selectivity of the ATLAS trigger\nand the rare nature of the most interesting physics sig-\nnatures at the LHC collider, it is essential to understand the ef-\nficiencies at each step of the event selection process. Sharing a\nlarge number of software components across all platforms from\nthe trigger event selection software to the offline physics anal-\nysis and reconstruction environment helps in achieving this goal\nand allows for a common development and run environment.\nII. THE ATLAS TRIGGER\nThe ATLAS trigger is based on three levels of online selec-\ntion: Level-1, Level-2, and Event Filter (EF). The second and\nthird level triggers, together known as the high level trigger\n(HLT) [3], [4], are implemented on PCs running the Linux op-\nerating system.\nThe Level-1 trigger [5] is implemented in custom hardware\nand will reduce the initial event rate to about 75 kHz. The\nLevel-1 decision is based on data from the calorimeters and the\nmuon detectors. For accepted events small localized regions in\npseudo rapidity and azimuthal angle centered on the high\nobjects identified by the Level-1 trigger are determined.\nEach region of interest (RoI) contains the type and the thresh-\nolds passed of the associated high candidate objects.\nThe Level-2 trigger\u2019s selection process is guided by the\nRoI information supplied by the Level-1 trigger and uses full\ngranularity event data within a RoI from all detectors. In this\nway, only 2% of the full event data are needed for the decision\nprocess at Level-2, thus reducing the required aggregate band-\nwidth to serve the Level-2 trigger. The selection algorithms\nrequest data from the readout buffers (ROB) for specific\ndetectors in a Level-1 defined RoI for each processing step.\nThe data are held in the ROBs until the Level-2 trigger accepts\nor rejects the event. The Level-2 event selection algorithms\nare controlled by the HLT selection framework and run inside\n0018-9499\/04$20.00 \u00a9 2004 IEEE\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on May 1, 2009 at 11:47 from IEEE Xplore.  Restrictions apply.\n916 IEEE TRANSACTIONS ON NUCLEAR SCIENCE, VOL. 51, NO. 3, JUNE 2004\nthe Level-2 Processing Units [6], [7] (L2PU) in concurrent\nworker-threads, each processing one event. The multithreaded\napproach minimizes overheads from context-switching and\navoids stalling the CPU when waiting for requested RoI\ndata to arrive from the ROBs. Asynchronous services, like\ninput of data and application monitoring, are executed in\nseparate threads. This allows an efficient use of multi-CPU\nprocessors but requires also all software running in the L2PU\nto be thread-safe. The technical aspects of multithreading are\nhandled by the dataflow software itself, including creation and\ndeletion of threads and any locking mechanism that may be\nrequired. The Level-2 output rate is about 2 kHz with typical\nevent decision times of 10 ms.\nIf an event is accepted by Level-2, the Event Builder collects\nall the event data fragments from the ROBs. The complete event\nis then made available to the EF for the final stage of trigger\nprocessing. Here, more complex algorithms provide a further\nrate reduction to about 200 Hz with typical event decision times\nof 1\u20132 s. While the Level-2 reconstructs localized regions, the\nbaseline for the EF is a full offline-like event reconstruction\nguided by the Level-2 Result. It will also use more complete\ncalibration, alignment and magnetic field data.\nTo achieve a fast rejection, the event processing in the HLT\nselection proceeds in steps for feature extraction and hypothesis\ndecisions. At the end of each step, the step results are checked\nagainst abstract physics signatures defined in trigger menus.\nIII. HIGH LEVEL TRIGGER SELECTION SOFTWARE\nThe HLT selection framework [8] constitutes the run environ-\nment for the trigger algorithms. It is common to Level-2 and EF\nand is composed of four main components. The HLT Steering\nschedules the HLT Algorithms corresponding to the input seed,\nso that all necessary data for a trigger decision are produced.\nInformation about event specific quantities is exchanged via\ncomponents of the event data model (EDM). During event pro-\ncessing data are stored and accessed through a data manager.\nThis allows to hide platform- and storage technology-specific\ndetails of event data access from the algorithms. The HLT algo-\nrithms either reconstruct new event quantities or check trigger\nhypotheses with previously computed event features.\nAs the main purpose of the HLT selection software is event\nselection, it has to run efficiently and reliably in the online\nenvironment. In addition, critical selection components must\nbe transferable to the offline environment for development\nand testing purposes. By providing a common code base for\nthe online and the offline software, the HLT guarantees the\nconsistency of trigger performance evaluations. It also provides\na \u201cphysicist-friendly\u201d environment for trigger algorithm devel-\nopment. In addition, studies have already shown [9] that great\ncost savings can be obtained with the proper global optimiza-\ntion of the trigger. Having a single common framework, where\nthe different trigger levels can be cross optimized, greatly\nfacilitates these studies.\nSince the EF provides an offline-like environment, the HLT\nselection software is naturally based on the ATLAS offline re-\nconstruction and analysis environment ATHENA [10], which it-\nself is based on the GAUDI [11] framework. This allows for the\nFig. 1. The L2PU finite state machine. All algorithms and services are\ncreated at initialization time in the Configure step. The Level-2 selection code\nis executed in parallel in multiple worker-threads.\nreuse of the storage manager, the EDM, the detector descrip-\ntion and many algorithms, which are already developed by the\noffline community. Only the HLT Steering framework and cer-\ntain algorithms remain as HLT specific developments. In the\ncase of the Level-2 trigger, a similar ansatz is more difficult\ndue to the multithreaded selection process and the more severe\nperformance requirements. Even though Level-2 algorithms are\nspecially developed to meet the tight timing requirements, they\nuse the same EDM- and detector description objects present in\nthe EF and offline. A transparent use of such components is pos-\nsible and a common implementation of the HLT framework for\nboth Level-2 and EF can be realized if the same interfaces are\navailable at Level-2. This is provided by the Level-2 steering\ncontroller (SC).\nIV. THE LEVEL-2 STEERING CONTROLLER\nThe SC [12] is the software component that interfaces the\nL2PU, the dataflow application which provides access to the\nevent data stored in ROBs and the HLT event selection soft-\nware. The purpose of the SC is threefold: to allow the L2PU to\nhost and control the selection software; to allow the reuse of the\nsame trigger algorithm steering software as in the EF; and to\nprovide a mechanism for transmitting the results of Level-1 and\nLevel-2 processing between the data acquisition system and the\nevent selection software. All dataflow applications follow for\ncontrol a state model implemented in form of finite state ma-\nchines (FSM). The key to the SC design is to place this inter-\nface where the functionality of the dataflow and event selection\nframeworks can be cleanly separated. The location chosen is the\nFSM of the L2PU.\nThe SC provides the means for forwarding state changes from\nthe dataflow software (Fig. 1) to the event selection software. An\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on May 1, 2009 at 11:47 from IEEE Xplore.  Restrictions apply.\nARMSTRONG et al.: COMMON SELECTION SOFTWARE ENVIRONMENT IN ATLAS 917\nFig. 2. The sequence of interactions of the SC with the L2PU and the Event Selection Software. Three states are shown: configure, start, and stop. The gray area\nshows the interactions which happen in multiple Worker Threads. The pROS is the component that forwards the Level-2 Result to the EF.\nimportant aspect of this approach is that the Level-2 event data\naccess is managed entirely by the dataflow. The SC then does\nnot need to interact directly with the data input threads or other\ndataflow components.\nFig. 2 illustrates the sequence of interactions of the SC with\nthe L2PU and the event selection software. The figure shows\nthree states: configure, start, and stop. During the configure\nphase, configuration, and conditions data are obtained from ex-\nternal databases via an HLT-online interface. These data are then\nused to configure the selection software and all associated com-\nponents.\nAfter a start the SC receives an \u201cexecute event\u201d directive with\na Level-1 Result as argument. The result of event processing is\ndirectly returned as Level-2 Result to the L2PU. A stop com-\nmand terminates algorithm execution and produces run sum-\nmary information.\nSince the trigger event selection software is being developed\nin the ATLAS offline framework, which is itself based on the\nGAUDI framework, the SC also has been designed to reuse the\nframework interfaces defined in GAUDI. In this way, there is a\nunified environment for event reconstruction and selection from\nthe second level trigger to the offline analysis.\nSince the event selection software executes in multiple\nworker threads, the SC must provide a thread-safe environment.\nAt the same time, and in order to provide an easy-to-use frame-\nwork for offline developers, the SC must hide all technical\ndetails of thread handling and locks. Thread safety has been\nimplemented in the SC by using GAUDI\u2019s name-based object\nand service bookkeeping system. Copies of components that\nneed to be thread-safe are created for each worker thread\nwith different labels. The labels incorporate the thread-ID of\nthe worker thread, as obtained from the dataflow software.\nThe number of threads created by the dataflow software is\ntransferred to the SC, which transparently creates the number\nof required copies. In this scheme, the same configuration can\nbe used in the offline and in the Level-2 environments; the\nthread-ID collapses to null in the offline environment, as it is\nnot needed there.\nIn contrast to the EF and offline environment, all Level-2 al-\ngorithm- and service instances need to be created and initialized\nin the configuration state of the L2PU FSM, since only in this\nphase a L2PU has access to external databases. Later in the event\nloop, only access to information stored locally on the processor\nis possible. The configure step is still executed in a single thread\nand only later after the transition to the start state of the FSM\nthe thread specific copies of the algorithms and services are at-\ntached to the worker threads.\nThe implementation of the SC consists of three components.\n\u2022 An interface class, which forward the L2PU state changes\nto the algorithm execution framework. Different interface\nimplementations, e.g., for dataflow testing without algo-\nrithms, can be specified in the L2PU configuration and\nare loadable as shared libraries. This implementation also\nhelps to minimize cross dependencies in the respective\ndataflow and offline repositories.\n\u2022 The multithreaded algorithm execution environment. The\nnecessary changes for multithreading support have been\nincorporated in the GAUDI base libraries and are available\nwith the recent official releases of the GAUDI framework.\n\u2022 A modified ATHENA event loop handler. Contrary to of-\nfline, the event loop is controlled by the dataflow software.\nThe modified event loop handler makes the Level-1 Result\navailable to the HLT selection software, executes the algo-\nrithms for a given event and forward the Level-2 decision\nto the L2PU interface class.\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on May 1, 2009 at 11:47 from IEEE Xplore.  Restrictions apply.\n918 IEEE TRANSACTIONS ON NUCLEAR SCIENCE, VOL. 51, NO. 3, JUNE 2004\nFig. 3. A typical development and prototype setup. GAUDI with support for multiple threads is the basic algorithm execution environment. It is shared by the\nonline and the offline environments. Algorithms are developed in the offline environment and tested with the L2PU emulator athenaMT. The same binary libraries\ncontaining the event selection algorithms are then used by the real L2PU running in a dataflow setup.\nThe implementation is complemented by special utility ser-\nvices, which connect, e.g., the GAUDI framework messaging to\nthe corresponding dataflow implementations.\nV. SOFTWARE DEVELOPMENT MODEL\nHLT software developers follow a typical edit, compile and\nrun cycle in the ATHENA offline environment, when creating\nnew software components. For running an application, the\nATHENA main program together with a job configuration file\nwould be used: athena job-configuration .\nSince the same interfaces are available in the EF and the\nL2PU environment the code developed in the offline environ-\nment can be directly downloaded in binary form to the pro-\ncessors. For Level-2 the developer has to follow however a set\nof simple coding rules [13] to produce thread safe code and to\nmake its algorithms or services compatible with the automatic\ncreation of multiple copies in the L2PU. Furthermore, it should\nbe not necessary to use locks or mutexes to adapt the code to the\nmultithreaded environment. To meet the timing requirements for\nthe second level trigger, the number and type of available ser-\nvices is restricted to the necessary minimum, which is a subset\nof the services available in the EF and in offline. In this way,\nit should be always possible to move a Level-2 component to\nthe EF and offline environment. The other direction, however,\nis only possible if the software component finds all its necessary\nresources in the restricted L2PU environment.\nThe dataflow software can be configured to run either as\nsingle or multinode system. The single-node system starts a\nRead-Out Subsystem (ROS) emulator, a Level-2 supervisor,\nand a L2PU in the same processing node, while the multinode\nsystem distributes these applications over various nodes. The\nsetup of a complex dataflow system for application testing\nis in both cases a nontrivial task and most HLT developers\nlack also the necessary hardware resources. A multithreaded\ntest application called athenaMT was therefore created, which\npresents internally the same run environment as a L2PU,\nbut can be started as simple as the normal ATHENA main\nprogram: athenaMT number of worker-threads\nTABLE I\nRATES ON A DUAL-PROCESSOR 1.533-GHz AMD ATHLON MACHINE\njob-configuration . athenaMT uses the SC and most\nof the dataflow components that are also used in a standard\nL2PU. It differs, however, in the supervision aspect of the\nL2PU and in the way detector raw data are made available to\nthe processing unit. The application can be used on single- or\nmulti-CPU machines. HLT developers need not to be familiar\nwith detailed technical aspects of the dataflow software and are\nalso shielded from changes in the dataflow part of the software.\nThey can concentrate exclusively on the HLT software and\nare able to perform a large variety of useful tests, from thread\nsafety to performance measurements, in a realistic L2PU\nenvironment. Fig. 3 shows the relation between an online\ndataflow setup and an offline development environment for\nLevel-2 software. In this way the development effort for HLT\nand dataflow can be widely parallelized. It is clear, however,\nthat the final certification of the HLT software has to be done\non a large distributed system.\nVI. PERFORMANCE MEASUREMENTS\nAfter integrating the SC with the dataflow software, both\nperformance and robustness tests were carried out on a dual-\nprocessor 1.533-GHz AMD Athlon machine. The SC ran for\nover 50 h with three threads with an early version of the selec-\ntion software prototype. The prototype ran successfully on both\nsingle-CPU and double-CPU machines, showing it to be thread\nsafe. A direct measurement of the SC overhead yielded 13 s per\nevent. The overhead was estimated by comparing the number of\nevents per second a L2PU can handle when running without or\nwith the SC and executing a simple algorithm (first and second\nrow in Table I). Table I shows also the obtained rate, when in\naddition the Level-1 Result is transfered to the data manager.\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on May 1, 2009 at 11:47 from IEEE Xplore.  Restrictions apply.\nARMSTRONG et al.: COMMON SELECTION SOFTWARE ENVIRONMENT IN ATLAS 919\nFig. 4. The main contributions to the latency shown as curves of integrals for the \u0016 selection slice. The slice used offline software components for data conversion,\ndetector description, data manager, and steering. The contributions in order of decreasing importance are data preparation and conversion (Data conversion), HLT\nframework overheads (Steering), algorithmic processing (muFast), and network access time (Network latency). The curves show, e.g., that in this setup algorithmic\nprocessing is terminated for 95% of the events in less than 2 ms. The data sample consisted of 200 GeV muons at high luminosity. The muon background from the\nATLAS cavern was boosted by a factor of 2, so that the results give a conservative estimate of the processing times.\nThe quoted overhead includes all GAUDI framework steps to\nschedule and execute algorithms and to execute the used base\nservices. The measurements were based on runs of at least 100\n000 events and almost perfect scaling of the measured latency\nwas observed when the algorithm execution time was varied\nwith a CPU burning loop from 0 to 8 ms in the different con-\nfigurations, indicating that the overheads per event introduced\nby the SC are independent of the algorithm execution time.\nMore tests with increasing complexity were performed on a\nthree-node dataflow system build with a dual-processor Intel\nXEON 2.2 GHz machine hosting the L2PU, a dual-processor\n1.533-GHz AMD Athlon machine hosting the ROS emulator\nand a single-processor Intel Pentium 4 machine running the\nLevel-2 Supervisor. The dual -processor machines were con-\nnected via a Gigabit Ethernet network. All machines were run-\nning the Redhat 7.3 Linux operating system.\nIn sequence, the offline data manager StoreGate [14] the HLT\nsteering and a fast inner tracker feature extraction algorithm\n[15] were included in the setup. The algorithm used an early\nversion of the event data model and the raw data conversion\nprocess from byte stream format to EDM classes. More com-\nplex tests were recently done with complete and selection\nslices. They contained the complete HLT steering framework\nwith decoding of the Level-1 Result, scheduling of the feature\nextraction algorithms, and sending the results of the Level-2 pro-\ncessing to the EF. The data manager, the event data model, the\ndetector description for the calorimeters, and the muon system\nand services for conversion from raw data byte stream format\nto high-level data containers were used from offline. The fea-\nture extraction algorithms were specially developed for Level-2\nbut used the aforementioned offline services. In the case of the\nslice, over 95% of the events were processed within 5 ms\nfor a sample of di-jet events at low luminosity and a RoI size of\n[4]. Fig. 4 shows similar results for the\nslice.\nThe overheads of the SC are negligible compared to the pro-\ncessing times of the HLT event selection software. For both se-\nlection slices the data preparation and conversion step domi-\nnates the processing times. It contains all contributions from un-\npacking of the raw data, the application of the complex detector\ndescription and calibration schemes to the creation of fully cali-\nbrated high level physics objects suited for event reconstruction\nand selection in the algorithms. The data preparation and con-\nversion time is a direct function of the number of handled input\nraw data items and is therefore also sensitive, e.g., to the noise\nlevel in the calorimeters and the occupancy of the muon cham-\nbers due to cavern background. Appropriate data preprocessing\nand noise reduction schemes can help to reduce the time spend\nin this step [4]. The contribution from the HLT steering and the\nnetwork latency are similar for both selection slices. Measure-\nments on larger test beds [7] and detailed simulations of the full\nsystem have shown [4], that at most a contribution of 1 ms from\nthe RoI data collection over the network is expected.\nVII. EXPERIENCES\nDuring the tests it was observed that the event throughput in\na L2PU did not scale in the expected way with the number of\nworker-threads. This was due to the use of a common memory\npool for container objects in the default memory allocation\nscheme of the standard template library (STL). The event pro-\ncessing model of Level-2 favors a scheme where every thread\nallocates its own memory pool. Such an allocation scheme\nis available in the STL. After carefully optimizing the code\nwith it, the expected scaling behavior was observed. Such an\noptimization poses limitations on the offline components used\nand their external dependencies. For instance, external utility\nlibraries may not be available with this allocation scheme. To\navoid frequent memory allocation during event processing,\nall large containers that hold detector data were designed to\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on May 1, 2009 at 11:47 from IEEE Xplore.  Restrictions apply.\n920 IEEE TRANSACTIONS ON NUCLEAR SCIENCE, VOL. 51, NO. 3, JUNE 2004\nallocate memory only during initialization time and to reset\ntheir data for each new event.\nThe reconstruction of full events in offline favors a processing\nmodel where utility services and external data are retrieved on\ndemand. In the case of Level-2 this model cannot be applied\nsince during event processing only locally stored meta-data are\naccessible. All required data need to be known at configuration\ntime and need to be prefetched by the processor. Furthermore,\nthe creation of large configuration objects on demand may lead\nto time-outs during event processing. These restrictions required\na redesign and alternative initialization methods for some offline\ncomponents, especially for detector description and raw data\nconversion.\nThe use of offline components in multiple worker threads and\nthe requirement to avoid locks in the event selection code limits\ncertain design and implementation choices, e.g., the use of sin-\ngletons. These restrictions were communicated to the HLT de-\nvelopers at an early design stage.\nVIII. CONCLUSION\nThe presented implementation of the SC for the ATLAS\nLevel-2 trigger enables the reuse of offline software compo-\nnents throughout the ATLAS High Level Triggers. It realizes\na homogeneous software and development environment from\nthe Level-2 trigger to offline. Realistic prototypes have shown\nthat the required performances can be reached if the offline\ncomponents are carefully optimized and designed for reuse\nin the triggers. This may limit architectural, design, and\nimplementation choices that are otherwise available in a pure\noffline environment. An understanding of these restrictions is\nnecessary for all contributing developers.\nACKNOWLEDGMENT\nThe authors would like to acknowledge the help and support\nof the ATLAS Data Acquisition Group and the ATLAS Offline\nand Detector software groups.\nREFERENCES\n[1] \u201cATLAS: Technical Proposal for a General-Purpose Experiment\nat the Large Hadron Collider at CERN,\u201d ATLAS Collaboration,\nCERN\/LHCC\/94-43, 1994.\n[2] \u201cATLAS Detector and Physics Performance Tech. Design Rep.,\u201d\nATLAS Collaboration, CERN\/LHCC\/99-014 and 99-015, 1999.\n[3] \u201cATLAS High-Level Triggers, DAQ and DCS Tech. Proposal,\u201d ATLAS\nCollaboration, CERN\/LHCC\/2000-017, 2000.\n[4] \u201cATLAS High-Level Trigger Data Acquisition and Controls, Tech. De-\nsign Rep.,\u201d ATLAS Collaboration, CERN\/LHCC\/2003-022, 2003.\n[5] \u201cATLAS Level-1 Trigger Tech. Design Rep.,\u201d ATLAS Collaboration,\nCERN\/LHCC\/98-014, 1998.\n[6] A. Bogaerts and F. Wickens, \u201cLVL2 Processing Unit Application De-\nsign,\u201d, ATL-DH-ES-0009, 2001. EDMS Note.\n[7] S. Armstrong et al., \u201cThe second level trigger of the ATLAS experiment\nat CERN\u2019s LHC,\u201d in Proc. Nuclear Science Symp., Portland, OR, Oct.,\n19\u201325 2003.\n[8] M. Elsing, Ed., \u201cAnalysis and Conceptual Design of the HLT Selection\nSoftware,\u201d PESA Software Group, ATL-DAQ-2002-013, 2002. ATLAS\nInternal Note.\n[9] J. T. M. Baines, S. Gonz\u00e1lez, R. K. Mommsen, A. Radu, T. G. Shears,\nS. Sivoklokov, and M. Wielers, \u201cFirst Study of the LVL2-EF Boundary\nin the High-pT e\/gamma High-Level Trigger,\u201d, ATL-DAQ-2000-045,\n2000. ATLAS Internal Note.\n[10] Athena: User Guide and Tutorial. [Online]. Available:\nhttp:\/\/atlas.web.cern.ch\/Atlas\/GROUPS\/SOFTWARE\/OO\/architecture\/\nGeneral\/Tech.Doc\/Manual\/2.0.0-DRAFT\/AthenaUserGuide.pdf\n[11] Gaudi Project [Online]. Available: http:\/\/cern.ch\/proj-gaudi\/\n[12] S. Gonz\u00e1lez, A. Radu, and W. Wiedenmann, \u201cUse of Gaudi in\nthe LVL2 Trigger: The Steering Controller,\u201d, ATL-DH-EN-0001,\nATL-DAQ-2002-012, 2002, 2002. EDMS Note, and ATLAS Internal\nNote.\n[13] A. Bogaerts, A. dos Anjos, S. Gonz\u00e1lez, and W. Wiedenmann. Guide-\nlines for Offline Preparation and Testing of LVL2 Code. [Online]. Avail-\nable: http:\/\/www.cern.ch\/steve.armstrong\/algorithms\/guidelines\n[14] P. Calafiura, H. Ma, M. Marino, S. Rajagopalan, and\nD. R. Quarrie. StoreGate: A data model for the ATLAS\nsoftware architecture. presented at Proc. Computing in High\nEnergy and Nuclear Physics Conf.. [Online]. Available:\nhttp:\/\/atlas.web.cern.ch\/Atlas\/GROUPS\/SOFTWARE\/OO\/architec-\nture\/EventDataModel\/Tech.Doc\/StoreGate\/CHEP01.pdf\n[15] J. T. M. Baines, A. Bogaerts, M. Bosmann, A. dos Anjos, S. Gonz\u00e1lez, K.\nKarr, W. Li, G. Merino, J. Schlereth, and W. Wiedenmann, \u201cInitial LVL2\nTests with the Si Tree Algorithm,\u201d, ATL-DH-TN-0001, 2003. EDMS\nNote.\nAuthorized licensed use limited to: Lancaster University Library. Downloaded on May 1, 2009 at 11:47 from IEEE Xplore.  Restrictions apply.\n"}