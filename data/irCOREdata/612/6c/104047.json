{"doi":"10.1109\/EST.2010.14","coreId":"104047","oai":"oai:eprints.lincoln.ac.uk:3867","identifiers":["oai:eprints.lincoln.ac.uk:3867","10.1109\/EST.2010.14"],"title":"Vision-based landing of a simulated unmanned aerial vehicle with fast reinforcement learning","authors":["Shaker, Marwan","Smith, Mark N. R.","Yue, Shigang","Duckett, Tom"],"enrichments":{"references":[{"id":18446257,"title":"A vision-based automatic landing method for \ufb01xed-wing uavs.","authors":[],"date":"2010","doi":"10.1007\/s10846-009-9382-2","raw":"S. Huh and D. H. Shim. A vision-based automatic landing method for \ufb01xed-wing uavs. J. Intell. Robotics Syst., 57(1-4):217\u2013231, 2010.","cites":null},{"id":18446255,"title":"Active reinforcementlearning.","authors":[],"date":"2008","doi":"10.1145\/1390156.1390194","raw":"A. Epshteyn, A. Vogel, and G. DeJong. Active reinforcementlearning. InProceedingsofthe25thInternationalConference on Machine learning (ICML), pages 296\u2013303, New York, NY, USA, 2008.","cites":null},{"id":18446263,"title":"Adaptive critic autopilot design of bank-to-turn missiles using fuzzy basis function networks.","authors":[],"date":"2005","doi":"10.1109\/tsmcb.2004.842246","raw":"C.-K. Lin. Adaptive critic autopilot design of bank-to-turn missiles using fuzzy basis function networks. IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, 35(2):197 \u2013 207, 2005.","cites":null},{"id":18446271,"title":"Adaptive routing for sensor networks using reinforcement learning.","authors":[],"date":"2006","doi":"10.1109\/cit.2006.34","raw":"P. Wang and T. Wang. Adaptive routing for sensor networks using reinforcement learning. In CIT \u201906: Proceedings of the Sixth IEEE International Conference on Computer and Information Technology, page 219, Washington, DC, USA, 2006.","cites":null},{"id":18446258,"title":"Altitude control of aircraft using coef\ufb01cient-based policy method.","authors":[],"date":"2008","doi":"10.1109\/ccece.2008.4564557","raw":"J. Jiang, H. Gong, J. Liu, H. Xu, and Y. Chen. Altitude control of aircraft using coef\ufb01cient-based policy method. In Proc. Canadian Conference on Electrical and Computer Engineering, (CCECE), pages 361 \u2013364, 4-7 2008.","cites":null},{"id":18446267,"title":"altURI-a thin middleware for simulated robot vision applications. http:\/\/webpages. lincoln.ac.uk\/mnsmith\/altURI.htm,","authors":[],"date":"2010","doi":null,"raw":"M. N. R. Smith. altURI-a thin middleware for simulated robot vision applications. http:\/\/webpages. lincoln.ac.uk\/mnsmith\/altURI.htm, 2010.","cites":null},{"id":18446250,"title":"Exploration and apprenticeship learning in reinforcement learning.","authors":[],"date":"2005","doi":"10.1145\/1102351.1102352","raw":"P. Abbeel and A. Y. Ng. Exploration and apprenticeship learning in reinforcement learning. In ICML \u201905: Proceedings of the 22nd international conference on Machine learning, pages 1\u20138, New York, NY, USA, 2005.","cites":null},{"id":18446264,"title":"Fast reinforcement learning for vision-guided mobile robots. In","authors":[],"date":"2005","doi":"10.1109\/robot.2005.1570760","raw":"T. Martinez-Marin and T. Duckett. Fast reinforcement learning for vision-guided mobile robots. In Proc. IEEE International Conference on Robotics and Automation (ICRA), pages 4170\u20134175, 2005.","cites":null},{"id":18446269,"title":"Improved adaptive reinforcement learning control for morphing unmanned air vehicles.","authors":[],"date":"2008","doi":"10.2514\/6.2005-7159","raw":"J. Valasek, J. Doebbler, M. Tandale, and A. Meade. Improved adaptive reinforcement learning control for morphing unmanned air vehicles. IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, 38(4):1014 \u2013 1020, aug. 2008.","cites":null},{"id":18446262,"title":"Least-squares policy iteration.","authors":[],"date":"2003","doi":null,"raw":"M. G. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of Machine Learning Research, 4:2003, 2003.","cites":null},{"id":18446261,"title":"Model-free least squares policy iteration.","authors":[],"date":"2001","doi":null,"raw":"M. G. Lagoudakis and R. Parr. Model-free least squares policy iteration. In Proc. Advances in Neural Information Processing Systems (NIPS-14), 2001.","cites":null},{"id":18446251,"title":"Quadrocopter UAV Model AR100.","authors":[],"date":null,"doi":null,"raw":"AirRobot. Quadrocopter UAV Model AR100. http:\/\/ www.airrobot-uk.com\/index.htm.","cites":null},{"id":18446260,"title":"Reinforcement learning: A survey.","authors":[],"date":"1996","doi":"10.1007\/978-3-642-79629-6_5","raw":"L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement learning: A survey. Journal of Arti\ufb01cal Intelligence Research, 4:237\u2013285, 1996.","cites":null},{"id":18446268,"title":"Reinforcement Learning: An Introduction.","authors":[],"date":"1998","doi":"10.1109\/tnn.1998.712192","raw":"R. Sutton and A. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.","cites":null},{"id":18446256,"title":"Robotic camera control for remote exploration.","authors":[],"date":"2004","doi":"10.1145\/985692.985757","raw":"S. Hughes and M. Lewis. Robotic camera control for remote exploration. In Proceedings of the SIGCHI conference on Human factors in computing systems (CHI), pages 511\u2013517, New York, NY, USA, 2004.","cites":null},{"id":18446254,"title":"The OpenCV Library.","authors":[],"date":"2000","doi":"10.1109\/mra.2009.933612","raw":"G. Bradski. The OpenCV Library. Dr. Dobb\u2019s Journal of Software Tools, 2000.","cites":null},{"id":18446249,"title":"Unreal Tournament.","authors":[],"date":"2004","doi":"10.1109\/itw.2010.5593348","raw":"Epic Games, Unreal Tournament. http:\/\/www. unrealtournament2004.com, 2004.","cites":null},{"id":18446252,"title":"Vision-based landing of \ufb01xed-wing miniature air vehicles.","authors":[],"date":"2009","doi":"10.2514\/6.2007-2984","raw":"B. Barber, T. McLain, and B. Edwards. Vision-based landing of \ufb01xed-wing miniature air vehicles. Journal of Aerospace computing, Information, and Communication, 6, March 2009.","cites":null},{"id":18446266,"title":"Vision-based reinforcement learning using approximate policy iteration.","authors":[],"date":"2009","doi":null,"raw":"M. R. Shaker, S. Yue, and T. Duckett. Vision-based reinforcement learning using approximate policy iteration. In Proc. 14th IEEE International Conference on Advanced Robotics (ICAR), 2009.","cites":null},{"id":18446265,"title":"Visionbased autonomous landing of an unmanned aerial vehicle.","authors":[],"date":"2002","doi":"10.1109\/robot.2002.1013656","raw":"S. Saripalli, J. F. Montgomery, and G. S. Sukhatme. Visionbased autonomous landing of an unmanned aerial vehicle. In Proc. IEEE International Conference on Robotics and Automation (ICRA), pages 2799\u20132804, 2002.","cites":null},{"id":18446253,"title":"Visual servoing of an airplane for auto-landing.","authors":[],"date":"2007","doi":"10.1109\/iros.2007.4399216","raw":"O. Bourquardez and F. Chaumette. Visual servoing of an airplane for auto-landing. In IEEE\/RSJ Int. Conf. on Intelligent Robots and Systems, (IROS), pages 1314\u20131319, San Diego, CA France, 2007.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-08-25","abstract":"Landing is one of the difficult challenges for an unmanned\\ud\naerial vehicle (UAV). In this paper, we propose a vision-based landing approach for an autonomous UAV using reinforcement learning (RL). The autonomous UAV learns the landing skill from scratch by interacting with the environment. The reinforcement learning algorithm explored and extended in this study is Least-Squares Policy Iteration (LSPI) to gain a fast learning process and a smooth landing trajectory. The proposed approach has been tested with a simulated quadrocopter in an extended version of the USARSim Unified System for Automation and Robot Simulation) environment. Results showed that LSPI learned the landing skill very quickly, requiring less than 142 trials","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/104047.pdf","fullTextIdentifier":"http:\/\/eprints.lincoln.ac.uk\/3867\/1\/PID1374145.pdf","pdfHashValue":"047435952cf6c1a1371b963f3996d2bc6cdc1e73","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lincoln.ac.uk:3867<\/identifier><datestamp>\n      2014-02-17T16:24:20Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F48:6A6163735F48363730<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F48:6A6163735F48363731<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47343030<\/setSpec><setSpec>\n      74797065733D636F6E666572656E63655F6974656D<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lincoln.ac.uk\/3867\/<\/dc:relation><dc:title>\n        Vision-based landing of a simulated unmanned aerial vehicle with fast reinforcement learning<\/dc:title><dc:creator>\n        Shaker, Marwan<\/dc:creator><dc:creator>\n        Smith, Mark N. R.<\/dc:creator><dc:creator>\n        Yue, Shigang<\/dc:creator><dc:creator>\n        Duckett, Tom<\/dc:creator><dc:subject>\n        H670 Robotics and Cybernetics<\/dc:subject><dc:subject>\n        H671 Robotics<\/dc:subject><dc:subject>\n        G400 Computer Science<\/dc:subject><dc:description>\n        Landing is one of the difficult challenges for an unmanned\\ud\naerial vehicle (UAV). In this paper, we propose a vision-based landing approach for an autonomous UAV using reinforcement learning (RL). The autonomous UAV learns the landing skill from scratch by interacting with the environment. The reinforcement learning algorithm explored and extended in this study is Least-Squares Policy Iteration (LSPI) to gain a fast learning process and a smooth landing trajectory. The proposed approach has been tested with a simulated quadrocopter in an extended version of the USARSim Unified System for Automation and Robot Simulation) environment. Results showed that LSPI learned the landing skill very quickly, requiring less than 142 trials.<\/dc:description><dc:date>\n        2010-08-25<\/dc:date><dc:type>\n        Conference or Workshop contribution<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        <\/dc:rights><dc:identifier>\n        http:\/\/eprints.lincoln.ac.uk\/3867\/1\/PID1374145.pdf<\/dc:identifier><dc:identifier>\n          Shaker, Marwan and Smith, Mark N. R. and Yue, Shigang and Duckett, Tom  (2010) Vision-based landing of a simulated unmanned aerial vehicle with fast reinforcement learning.  In: International Symposium on Learning and Adaptive Behaviour in Robotics Systems (LAB-RS 2010), 6-7 September 2010, Canterbury, UK.  <\/dc:identifier><dc:relation>\n        http:\/\/www.est-2010.info\/download\/EST%202010%20preliminary%20Programme.pdf<\/dc:relation><dc:relation>\n        10.1109\/EST.2010.14<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lincoln.ac.uk\/3867\/","http:\/\/www.est-2010.info\/download\/EST%202010%20preliminary%20Programme.pdf","10.1109\/EST.2010.14"],"year":2010,"topics":["H670 Robotics and Cybernetics","H671 Robotics","G400 Computer Science"],"subject":["Conference or Workshop contribution","PeerReviewed"],"fullText":"Vision-Based Landing of a Simulated Unmanned Aerial Vehicle with Fast\nReinforcement Learning\nMarwan Shaker, Mark N.R. Smith, Shigang Yue and Tom Duckett\nLincoln University, Lincoln, UK, LN6 7TS.\n{mshaker,mnsmith,syue,tduckett}@lincoln.ac.uk\nAbstract\nLanding is one of the difficult challenges for an un-\nmanned aerial vehicle (UAV). In this paper, we propose\na vision-based landing approach for an autonomous UAV\nusing reinforcement learning (RL). The autonomous UAV\nlearns the landing skill from scratch by interacting with\nthe environment. The reinforcement learning algorithm ex-\nplored and extended in this study is Least-Squares Policy It-\neration (LSPI) to gain a fast learning process and a smooth\nlanding trajectory. The proposed approach has been tested\nwith a simulated quadrocopter in an extended version of the\nUSARSim (Unified System for Automation and Robot Simu-\nlation) environment. Results showed that LSPI learned the\nlanding skill very quickly, requiring less than 142 trials.\n1 INTRODUCTION\nIt is well known that landing is one of the most problem-\natic stages for both manned and unmanned airplanes [9].\nLanding an airplane is a complex task because it requires a\nlarge amount of kinetic and potential energy of the airplane\nin the presence of various dynamic constraints, such as sud-\nden changes in winds, carried weight, height and velocity\nat each landing [9]. In unmanned air vehicles (UAVs), a\nsignificant number of accidents happens during the landing\nphase due to inexperience of pilots or sudden changes in the\nweather dynamics, such as winds. Thus, automatic landing\nsystems are required to land UAVs safely.\nIn autonomous control of systems, the system to be con-\ntrolled requires an accurate model of the environment and\nthe agent in order to create controllers with optimal perfor-\nmance. However, obtaining an accurate model of the en-\nvironment and UAV is difficult, as the dynamics and aero-\ndynamics of the system are non-linear and complex. Fur-\nthermore, if for some reason the modelled system changes,\ne.g. due to heavy air turbulence in the case of aircraft, the\nmodel will no longer represent the actual aircraft. An al-\nternative to enable UAV agents to learn and adapt their be-\nhavior is required. One of the most common and general\nframeworks for this type of learning and adaptation is re-\ninforcement learning (RL). Reinforcement learning enables\nan agent to learn from scratch by interacting with the envi-\nronment [19]. We will use a RL algorithm as the learning\nalgorithm for the UAV to learn landing skill.\nTo land an UAV on the target area, a solution based on\nvisual servoing [15] is applied, where the camera is used\nto keep track of the target while the UAV is steered to\nthe desired configuration. An image-based visual servoing\nmethod is used, where the control law is computed directly\nfrom visual features, without explicit pose estimation. In\nour case, we used image processing techniques to extract\nfeatures that represent the current state of the system, then\nwe applied a reinforcement learning algorithm known as\nLeast-Squares Policy Iteration (LSPI) [12] to obtain the de-\nsired control law. Least-Squares Policy Iteration is designed\nto solve control problems [12, 13], and uses value function\napproximation to cope with large state spaces and batch pro-\ncessing for efficient use of the training data. In addition,\nLSPI converges faster with fewer samples than Q-learning\nand no initial tuning of parameters is required [21, 12]. In\nthis paper, we extend LSPI so that it will work on a contin-\nuous state-action space.\nPrior to investigations with real robots, it is very useful to\nimplement any algorithm in simulation. The benefits of sim-\nulations include the fact that simulations are easier to setup,\nless expensive, faster and more convenient to use. Some\nlearning algorithms, such as genetic algorithms, are compu-\ntationally expensive, and it would take a very long time to\ncompute the learning on a real robot. Simulation provides\nthe facility of transferring a learned controller from simula-\ntion and then applying it on a real robot. In our experiments,\nwe used an extended version of the USARSim (Unified Sys-\ntem for Automation and Robot Simulation) environment as\nthe testing environment.\nThe rest of the paper is organized as follows. After dis-\ncussing related work in Section 2, we explain the simulation\nsetup and the visual servoing process in Section 3. Then, we\ndescribe the methodology of the implemented approach for\nthe learning process in Section 4. In Section 5 the experi-\nmental results are presented, followed by our conclusions in\nSection 6.\n2 Related Work\nMany approaches have been introduced to make the\nlanding task safer in unmanned aerial vehicles. Jiang et\nal. [10] applied reinforcement learning to altitude control\nfor airplanes, in particular the Boeing 747 with 20 state vari-\nables. They used a coefficient-based policy search method\ncombined with genetic algorithms to learn altitude control\nfor airplanes, where reinforcement feedback is the only in-\nformation used to update the fitness value of each chromo-\nsome. Lin [14] introduced an approach for learning control\nof missiles (much faster and more mission-oriented tasks).\nThey provided a framework to control bank-to-turn mis-\nsiles, which combined a fuzzy basis function network and\nan adaptive critic. Saripalli et al. [16] designed and im-\nplemented a hierarchical behavior-based landing algorithm\nfor an autonomous helicopter. They used an AVATAR he-\nlicopter to navigate from an initial position to a final posi-\ntion in a partially known environment based on GPS and\nvision. Valasek et al. [20] developed an adaptive reinforce-\nment learning control methodology for the morphing air ve-\nhicle control problem. A structured adaptive model inver-\nsion was used as the controller for tracking trajectories and\nhandling time-varying properties, parametric uncertainties,\nun-modeled dynamics, and disturbances. In addition, a re-\ninforcement learning module using Q-learning was used to\nlearn how to produce the optimal shape at every flight con-\ndition. Barber et al. [4] proposed vision-based landing for\nsmall fixed-wing UAVs, where a visual marker is used to\ngenerate the roll and pitch commands to the flight controller.\nBourquardez and Chaumette [5] introduced a visual servo-\ning algorithm to control an airplane during landing. Visual\nfeatures are used to build the landing control providing a\nlinearized model of the airplane dynamics. The landing ma-\nnoeuvre is divided into three phases for the purpose of sim-\nplification. Huh and Shim [9] introduced an automatic land-\ning algorithm for a blended wing body shaped fixed-wing\nUAV based on a vision system.\nOur approach provides a learning controller that adapts\nits behavior through direct interaction with the environment.\nThe proposed approach converges faster than Q-learning\nwith fewer samples [21, 17, 12].\n3 Simulation\n3.1 USARSim Simulation\nThe USARSim (Unified System for Automation and\nRobot Simulation) system [8] was created to provide a re-\nalistic low cost simulation tool for robots in real environ-\nments. It is based on the commercial games platform Un-\nreal Tournament [1] which is customized to provide mod-\nels of various robots and environments. Additional soft-\nware components are provided to support image and sensor\ndata acquisition and implement robot actions. In our ex-\nperiments, we used an extended version of the USARSim\nsoftware known as altURI, which was developed by one of\nthe authors [18].\nThe altURI software makes it easy to use the USAR-\nSim [8] robot simulation environment. Furthermore, the\naltURI software provides images through the widely used\nOpenCV vision processing library [6], as well as networked\nweb images in several formats. Many robots can be si-\nmultaneously controlled in the simulator environment. The\naltURI software consist of two components and two sup-\nport programs. The vision component captures images from\ngraphics engines and makes them available as OpenCV, web\nimages or a Matlab array. This component is loaded into\nthe graphics process and, on request, intercepts calls made\nby the simulator to display a frame. The vision compo-\nnent supplies images at the same resolution as the USAR-\nSim game environment but using parameters can provide\nsmaller or partial images (multiview). The control compo-\nnent of altURI controls an instance of a robot using infor-\nmation specified in a configuration file. This component\nallows the action commands to be called using calls to a\nsimple programming interface. A support program to load\nthe vision component into a graphics process is provided.\n(a) The environment used for\nlearning the UAV landing skill.\n(b) The landing target, detected\nby OpenCV.\nFigure 1. Simulated environment\nThe modular approach allows the vision and command\ncomponents to be substituted for real robot vision and con-\ntrol components, or components that interact with other\nsimulator environments. The system removes the require-\nment for any direct simulator programming and makes it\npossible to have a working robot control program in just a\nfew minutes. The environment used in our experiments is\nshown in Fig. 1(a)\n3.2 Visual Servoing\nTo make the automatic landing possible, the UAV must\nrely on special sensors, such as vision, GPS or laser. How-\never, a single GPS is not useful because single GPS without\na differential correction typically provides position accuracy\nof at most a few meters from the ground, which makes the\nlast few meters in the landing process uncontrolled. By con-\ntrast, a laser sensor gives an accurate distance measure to the\nground. However, it consumes too much energy. Thus, we\nselected a vision sensor to obtain the state variable for the\nlanding stage.\nxy\nz\nRobot\nTarget\nr\n\u03b8\n\u03c6\nFigure 2. State space representation.\nA camera is fixed at the central bottom of the UAV, and\nprovides images of the landing target. In our experiments,\nthe landing target is a full black circle surrounded by circles\nwith grey color range (starting from black and ending with\nwhite) as shown in Fig. 1(b). The target was selected be-\ncause a circle will be detected as a target even the lighting\nis changed. The state space variables are extracted from the\nimage of the target.\nThe process of detecting the target is done in three steps\nusing OpenCV. The first step is to capture a frame and im-\nplement a Canny edge detection operator. The second step\nis to detect a number of circles using a Hough transform and\ncalculating their center. The third step is to use the original\nimage to select the circle that has a black center and white\ncircumference.\n3.3 Experiments Setup\nIn our experiments, we used a quadrocopter UAV to test\nour proposed approach on the landing task as shown in\nFiq. 2. The landing station is shown in Fig. 1(b). The\nstate space is represented by three state variables (r, \u03b8,\n\u03c6), where r is radial distance of that agent in the space\nfrom the center of the target, \u03b8 represents the azimuthal\nangle in the xy-plane from the x-axis of the target with\n0 \u2264 \u03b8 < 360, and \u03c6 represents the polar angle from the\nz-axis with \u221290 \u2264 \u03c6 \u2264 90. All the state variables are esti-\nmated from the captured image. Before starting the experi-\nments, we captured two images with the UAV at a distance\nof 2 and 4 meters from the target, respectively. We used\nthese images to interpolate the relative radial distance, r,\nduring subsequent processing. The state space is shown in\nTable 1.\n4 Methodology\nMany RL methods are time consuming, especially for\nlearning a complex task with a large state-action space from\nscratch [7, 2]. Nevertheless, many methods have been tried\nTable 1. State Space Parameters\nParameter Parameters Range\nr [0, 10]meters\n\u03b8 [0o, 360o]\n\u03c6 [\u221290o, 90o]\nReward\n(1) 120 if it gets to the goal,\n(2) -1500 if it finishes outside state space,\n(3) equal to a value, this value is decrease\nas \u03c6 or \u03b8 increase\nGoal State r = [0, 0.1]m, \u03b8 = [0o, 10o], \u03c6 = [\u22125o, 5o]\nControl Lateral Velocity: [\u22125, 5]\nLinear Velocity: [\u22125, 5]\nto accelerate the reinforcement learning process by combin-\ning it with different methods, such as neural networks, plan-\nning, etc. However, the high computational complexity or\ntuning of the initial parameters has limited the potential of\nsuch techniques to solve many problems. We propose to\naddress this challenges by using and extending a reinforce-\nment learning algorithm called Least-Squares Policy Itera-\ntion (LSPI) [12].\n4.1 Least-Squares Policy Iteration\nIn this work, we applied least-squares policy iteration\n(LSPI) [12, 13] as the learning algorithm. LSPI converges\nfaster with fewer samples than traditional approaches, since\nthe samples are used more efficiently. This property comes\nfrom the fact that LSPI evaluates the policy with a single\npass over the set of samples, and all the samples can be\nused in each iteration to evaluate the policy. LSPI is par-\nticularly suited to mobile robot applications because it does\nnot require careful tuning of initial parameters, e.g., learn-\ning rate. As it has no learning rate parameters to tune, and\ndoes not take gradient steps, there is no risk of overshooting,\noscillation or divergence, which are difficulties many other\nalgorithms have to face. This property comes from the fact\nthat the policy is evaluated over a history of samples. Thus,\nLSPI is insensitive to initial parameter settings. In the next\nparagraphs, we will explain the theoretical part of LSPI and\nhow it was extended to work on a continuous action space.\nLSPI approximates Q-values, Qpi , for a given policy, pi,\nwith a parametric function approximation instead of eval-\nuating the optimal state-action values function directly to\nfind the optimal policy. More precisely, the value function\nis approximated as a linear weighted combination of k basis\nfunctions (features) as:\nQ(s, a) \u2248 Q\u02c6pi(s, a, w) =\nk\u2211\ni=1\n\u03c6i(s, a)wi = \u03a6(s, a)TW,\n(1)\nwhere \u03c6i is the ith basis function and wi is its weight in\nthe linear equation, k is the number of basis functions (fea-\ntures). The k basis functions (features) represent informa-\ntion extracted from the state-action pairs, and were designed\nmanually.\nWith the help of Eq. 1, the TD update equation given\nin [19] can be re-written as \u03a6W \u2248 R+\u03b3Ppi\u03a6W , where \u03a6 is\na (|S||A|\u00d7k) matrix, representing the basis functions for all\nstate-action pairs. This equation can be reformulated [12]\nas follows: \u03a6T (\u03a6 \u2212 \u03b3Ppi\u03a6)wpi = \u03a6TR , where P is a\nstochastic matrix that contains the transition model of the\nprocess, and R is a vector that contains the reward values..\nThe weights W of the linear function Q\u02c6pi can be ex-\ntracted by solving the following linear system of equa-\ntions [12]:\nW = A\u22121b, (2)\nA = \u03a6T (\u03a6\u2212 \u03b3Ppi\u03a6), (3)\nb = \u03a6TR, (4)\nbut the values of P and R will be unknown or too large to\nbe used in practice. To overcome this problem, LSPI learns\nA and b by sampling from the environment. A sample is\ndefined as {s, a, s\u2032, r}, where s, a, s\u2032, r are the current state,\naction, next state, and immediate reward respectively. Given\na set of samples, D = {{si, ai, s\u2032i, ri}|i = 1, 2, . . . , L}, an\napproximate form of \u03a6, Ppi\u03a6 and R can be constructed as\nfollows:\nQ\u02c6 =\n\uf8eb\uf8ed \u03c6(s1, a1)T. . .\n\u03c6(sL, aL)T\n\uf8f6\uf8f8 , (5)\n\u02c6Ppi\u03a6 =\n\uf8eb\uf8ed \u03c6(s\u20321, pi(s\u20321))T. . .\n\u03c6(s\u2032L, pi(s\n\u2032\nL))\nT\n\uf8f6\uf8f8 , (6)\nR\u02c6 =\n\uf8eb\uf8ed r1. . .\nrL\n\uf8f6\uf8f8 . (7)\nWith \u03a6, Ppi\u03a6 and R, the optimal weights can be found\nusing Eq. 2. Thus, by combining the policy-search effi-\nciency of the approximate policy iteration with the data effi-\nciency of approximate estimation of the Q-value function,\nwe obtain the Least-Square Policy Iteration (LSPI) algo-\nrithm [13]. The aim of LSPI is to learn a policy, pi, that\nmaximizes the corresponding Q-function by taking advan-\ntage of the efficient search of the approximate policy itera-\ntion.\nThe policy evaluation step of the approximate policy iter-\nation depends on the Q-value function estimation described\nin Eq. 1. So, whenever a new sample is collected, the\nweights of the approximation are updated. After the pol-\nicy evaluation phase has finished processing, the policy im-\nprovement starts by selecting a policy that maximizes the\napproximate representation of the Q-value, as follows:\npi(s|w) = arg max\na\n\u03c6(s, a)Tw, (8)\nLagoudakis and Parr [12, 13] showed that these estimates\nconverge on the optimal weights of the linear function ap-\nproximation as the number of samples increases.\nIn traditional LSPI [12, 13], the action space was repre-\nsented by a fixed number of actions (discrete action space).\nIn this work, we extend the approach to work with a con-\ntinuous action space (controlling the lateral and linear ve-\nlocity of the UAV). The agent has to learn the correct di-\nrection (right and left for the lateral velocity, or forward\nand backward for the linear velocity) and optimal value for\nturning. The optimal value for the lateral and linear veloc-\nity is calculated from the optimal wpi . After wpi is calcu-\nlated from solving the linear system of Eqs. 2, we calculate\nQ\u02c6pi = \u03c6\u00d7wpi for each \u03c6 in \u03a6. Then, the action value will be\nequal to the average value of wpi that gives maximum Q\u02c6pi .\n4.2 Computational Complexity\nA standardized measurement of the computational time\ncomplexity of an algorithm is the number of elementary\ncomputer operations it takes to solve a problem, in the worst\ncase. The number of computer operations depends on the\n\u201csize\u201d of the problem. In the following table, we will give\nthe time complexity of some basic reinforcement learning\napproaches and the implemented approach, where the equa-\ntions for calculating the number of elementary computer op-\nerations required by value iteration (VI) and policy iteration\n(PI) is taken from [11].\nValue iteration and policy iteration approaches depend\non the total number of states and actions, which make these\napproaches inapplicable for large or continuous state-action\nspace. In our approach, the cost of each iteration of the\napproximate policy iteration in LSPI with continuous state-\naction space (LSPI-CSA) is:\nO(NB3+NB2+(NB2 \u2217hm)+(NB \u2217hm)+NB+hm)\n(9)\nwhereNB is the number of basis function used andNB \u001c\n|S| \u00d7 |A|, where |S| and |A| represents the total number\nof states and actions respectively, and hm represents the\nnumber of samples collected. In our experiments, we in-\nvestigated the number of samples, hm, required to reach\nthe optimal behavior. We tested for hm = 50, 100 or un-\nlimited and observed that hm = 100 or unlimited does not\nmake a major difference compared to hm = 50. Therefore,\nwe used hm = 50 in our experiments (meaning that at each\nstep, we collect one sample and 49 samples from the previ-\nous learning). Thus, the time complexity of the approximate\npolicy iteration is less than the value iteration and policy it-\neration approaches, and it is not affected by the number of\nstates.\n(a) 1st Sequence (b) 2nd Sequence (c) 3rd Sequence\n(d) 4th Sequence (e) 5th Sequence (f) 6th Sequence\nFigure 3. Image Sequence of the Landing.\nTable 2. UAV Landing Complexity\nComplexity\nparameter VI PI LSPI-CSA\nNo. Of \u03b8max\u2212\u03b8min5 \u00d7 \u03b8,\nState \u03c6max\u2212\u03c6min5 \u00d7 \u03c6 and\nCalculation (rmax \u2212 rmin) r\nNo. Of State 25920 Continuous state\nAction\nFour actions Four actions Continuous action\n(right or left)\n(Forward or Backward)\nNB 20\nComplexity Worst Case: Worst Case: Worst Case:\n(Number 10,749,542,400 17,425,008,230,400 for hm= 50 is 29,470\nof Computer for hm= 100 is 50,520\nOperations)\n5 Experiments\nIn our experiments, we used the quadrocopter UAV\nmodel AR100 [3]. Quadrocopters belong to the class of\nvertical take-off and landing (VTOL) aerial vehicles. A ma-\njor advantage of this type of aerial vehicle is the ability to\nlaunch from the stowing position without the need for com-\nplex launching facilities. Furthermore, quadrocopters have\na large number of degrees of freedom, and offer highly dy-\nnamic flight capabilities.\nIn our experiments, we implemented the modified LSPI\nusing two different types of basis functions, i.e. polynomial\nbasis functions (PBF) and radial basis functions (RBF) on\nthe UAV landing task. Both approximations gave a smooth\nlanding trajectory. However, to compare the performance of\nthe proposed improvement using RBF and PBF, a number\nof experiments were done to check the number of iterations\nrequired by LSPI to converge to the optimal policy. We\nperformed 100 experiments and the average results of the\nrequired number of iterations is shown in Table 3.\nTable 3. Comparison of RBF and PBF\nBasis Order Number of Average Number of\nFunction Iteration Iterations over\n100 experiments\nPBF 2 23-231 27\nPBF 4 3-9 4.8\nRBF N\/A 2-3 2.1\nIn the case of using PBF, we tested using polynomials of\norder 2 and 4. For PBF with order 2, LSPI required more\nthan 27 iterations to converge for each state, while for the\npolynomial of order 4, LSPI required between 3 and 9 it-\nerations to converge for each step. Thus, we used PBF of\norder 4 in our simulated experiments. In the case of using\nRBF, we used the Gaussian function. Furthermore, LSPI\nwith PBF requires from 3 to 9 iterations with an average of\n4.8 iterations to converge on the optimal policy, while LSPI\nwith RBF requires from 2 to 3 iterations with an average of\n2.1 iterations. We used PBF or RBF because we believe that\nit is a simpler notion and provides a more intuitive explana-\ntion than other function approximation schemes.\nUSARSim software is used for simulating our problem\nsetup. A bounded environment with no physical obstacles\nwas chosen for clarity of results. We used a simple circular\nshape in the environment to represent the landing station.\nUsing the experimental setup described in Section 3.3, the\nUAV can control the altitude, lateral, linear and rotational\nvelocities, where altitude velocity controls the up and down\nmovement of the UAV, lateral velocity controls the right and\nleft velocity of the UAV, linear velocity controls the forward\nand backward movement of the UAV, and rotational veloc-\nxy\nz\nTarget\nL1\nL2\nL3\nL4\nFigure 4. Samples of the learned path\nity controls the rotation movement of the UAV. In our ex-\nperiments, the UAV learns to control the lateral and linear\nvelocities, while the UAV\u2019s altitude velocity was set to de-\ncrease at a fixed rate. If the UAV falls below a certain height\nthat causes it to lose track of the target, the RL controller in\nthe UAV is stopped for safety purposes.\nThe learned path is shown in Fig. 4, where L1, L2, L3,\nL4 are different starting locations of the UAV. The UAV\ncan land on the target from different starting locations with\nthe learned visual servoing approach. Figure 3 shows a se-\nquence of images displaying the landing process with the\nlearned controller. With the extended version of LSPI, the\nUAV can learn the landing skill in less than 142 trials.\n6 Conclusion\nThis paper describes the implementation of a visual ser-\nvoing approach based on reinforcement learning to enable\na UAV learn and improve the landing skill. Simulation re-\nsults showed that, with LSPI as the learning algorithm, the\nquadrocopter UAV learned the landing skill very quickly,\ngenerating a smooth landing trajectory. Future work will\ninclude investigating the effects of wind to achieve a more\nfaithful simulation, and transferring the learned skill from\nsimulation to a real UAV.\nReferences\n[1] Epic Games, Unreal Tournament. http:\/\/www.\nunrealtournament2004.com, 2004.\n[2] P. Abbeel and A. Y. Ng. Exploration and apprenticeship\nlearning in reinforcement learning. In ICML \u201905: Proceed-\nings of the 22nd international conference on Machine learn-\ning, pages 1\u20138, New York, NY, USA, 2005.\n[3] AirRobot. Quadrocopter UAV Model AR100. http:\/\/\nwww.airrobot-uk.com\/index.htm.\n[4] B. Barber, T. McLain, and B. Edwards. Vision-based\nlanding of fixed-wing miniature air vehicles. Journal of\nAerospace computing, Information, and Communication, 6,\nMarch 2009.\n[5] O. Bourquardez and F. Chaumette. Visual servoing of an air-\nplane for auto-landing. In IEEE\/RSJ Int. Conf. on Intelligent\nRobots and Systems, (IROS), pages 1314\u20131319, San Diego,\nCA France, 2007.\n[6] G. Bradski. The OpenCV Library. Dr. Dobb\u2019s Journal of\nSoftware Tools, 2000.\n[7] A. Epshteyn, A. Vogel, and G. DeJong. Active reinforce-\nment learning. In Proceedings of the 25th International Con-\nference on Machine learning (ICML), pages 296\u2013303, New\nYork, NY, USA, 2008.\n[8] S. Hughes and M. Lewis. Robotic camera control for remote\nexploration. In Proceedings of the SIGCHI conference on\nHuman factors in computing systems (CHI), pages 511\u2013517,\nNew York, NY, USA, 2004.\n[9] S. Huh and D. H. Shim. A vision-based automatic landing\nmethod for fixed-wing uavs. J. Intell. Robotics Syst., 57(1-\n4):217\u2013231, 2010.\n[10] J. Jiang, H. Gong, J. Liu, H. Xu, and Y. Chen. Altitude\ncontrol of aircraft using coefficient-based policy method. In\nProc. Canadian Conference on Electrical and Computer En-\ngineering, (CCECE), pages 361 \u2013364, 4-7 2008.\n[11] L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforce-\nment learning: A survey. Journal of Artifical Intelligence\nResearch, 4:237\u2013285, 1996.\n[12] M. G. Lagoudakis and R. Parr. Model-free least squares pol-\nicy iteration. In Proc. Advances in Neural Information Pro-\ncessing Systems (NIPS-14), 2001.\n[13] M. G. Lagoudakis and R. Parr. Least-squares policy itera-\ntion. Journal of Machine Learning Research, 4:2003, 2003.\n[14] C.-K. Lin. Adaptive critic autopilot design of bank-to-turn\nmissiles using fuzzy basis function networks. IEEE Trans-\nactions on Systems, Man, and Cybernetics, Part B: Cyber-\nnetics, 35(2):197 \u2013 207, 2005.\n[15] T. Martinez-Marin and T. Duckett. Fast reinforcement learn-\ning for vision-guided mobile robots. In Proc. IEEE Inter-\nnational Conference on Robotics and Automation (ICRA),\npages 4170\u20134175, 2005.\n[16] S. Saripalli, J. F. Montgomery, and G. S. Sukhatme. Vision-\nbased autonomous landing of an unmanned aerial vehicle. In\nProc. IEEE International Conference on Robotics and Au-\ntomation (ICRA), pages 2799\u20132804, 2002.\n[17] M. R. Shaker, S. Yue, and T. Duckett. Vision-based rein-\nforcement learning using approximate policy iteration. In\nProc. 14th IEEE International Conference on Advanced\nRobotics (ICAR), 2009.\n[18] M. N. R. Smith. altURI-a thin middleware for simu-\nlated robot vision applications. http:\/\/webpages.\nlincoln.ac.uk\/mnsmith\/altURI.htm, 2010.\n[19] R. Sutton and A. Barto. Reinforcement Learning: An Intro-\nduction. MIT Press, 1998.\n[20] J. Valasek, J. Doebbler, M. Tandale, and A. Meade. Im-\nproved adaptive reinforcement learning control for morph-\ning unmanned air vehicles. IEEE Transactions on Systems,\nMan, and Cybernetics, Part B: Cybernetics, 38(4):1014 \u2013\n1020, aug. 2008.\n[21] P. Wang and T. Wang. Adaptive routing for sensor networks\nusing reinforcement learning. In CIT \u201906: Proceedings of\nthe Sixth IEEE International Conference on Computer and\nInformation Technology, page 219, Washington, DC, USA,\n2006.\n"}