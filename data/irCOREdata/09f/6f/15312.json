{"doi":"10.1142\/S0129065710002346","coreId":"15312","oai":"oai:dro.dur.ac.uk.OAI2:7752","identifiers":["oai:dro.dur.ac.uk.OAI2:7752","10.1142\/S0129065710002346"],"title":"Data compression and regression through local principal curves and surfaces.","authors":["Einbeck, Jochen","Evers, Ludger","Powell, Benedict"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2010-06-01","abstract":"We consider principal curves and surfaces in the context of multivariate regression modelling. For predictor spaces featuring complex dependency patterns between the involved variables, the intrinsic dimensionality of the data tends to be very small due to the high redundancy induced by the dependencies. In situations of this type, it is useful to approximate the high-dimensional predictor space through a low-dimensional manifold (i.e., a curve or a surface), and use the projections onto the manifold as compressed predictors in the regression problem. In the case that the intrinsic dimensionality of the predictor space equals one, we use the local principal curve algorithm for the the compression step. We provide a novel algorithm which extends this idea to local principal surfaces, thus covering cases of an intrinsic dimensionality equal to two, which is in principle extendible to manifolds of arbitrary dimension. We motivate and apply the novel techniques using astrophysical and oceanographic data examples","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/15312.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/7752\/1\/7752.pdf","pdfHashValue":"6aa6f24e06fb4e43afb2d9251de11fadb1dd9ed1","publisher":"World Scientific","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:7752<\/identifier><datestamp>\n      2011-01-14T15:51:58Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Data compression and regression through local principal curves and surfaces.<\/dc:title><dc:creator>\n        Einbeck, Jochen<\/dc:creator><dc:creator>\n        Evers, Ludger<\/dc:creator><dc:creator>\n        Powell, Benedict<\/dc:creator><dc:description>\n        We consider principal curves and surfaces in the context of multivariate regression modelling. For predictor spaces featuring complex dependency patterns between the involved variables, the intrinsic dimensionality of the data tends to be very small due to the high redundancy induced by the dependencies. In situations of this type, it is useful to approximate the high-dimensional predictor space through a low-dimensional manifold (i.e., a curve or a surface), and use the projections onto the manifold as compressed predictors in the regression problem. In the case that the intrinsic dimensionality of the predictor space equals one, we use the local principal curve algorithm for the the compression step. We provide a novel algorithm which extends this idea to local principal surfaces, thus covering cases of an intrinsic dimensionality equal to two, which is in principle extendible to manifolds of arbitrary dimension. We motivate and apply the novel techniques using astrophysical and oceanographic data examples.<\/dc:description><dc:subject>\n        Dimension reduction<\/dc:subject><dc:subject>\n         Smoothing<\/dc:subject><dc:subject>\n         Iocalized PCA<\/dc:subject><dc:subject>\n         Mean shift.<\/dc:subject><dc:publisher>\n        World Scientific<\/dc:publisher><dc:source>\n        International journal of neural systems, 2010, Vol.20(3), pp.177-192 [Peer Reviewed Journal]<\/dc:source><dc:date>\n        2010-06-01<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:7752<\/dc:identifier><dc:identifier>\n        issn:0129-0657<\/dc:identifier><dc:identifier>\n        issn: 1793-6462 <\/dc:identifier><dc:identifier>\n        doi:10.1142\/S0129065710002346<\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/7752\/<\/dc:identifier><dc:identifier>\n        http:\/\/dx.doi.org\/10.1142\/S0129065710002346<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/7752\/1\/7752.pdf<\/dc:identifier><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":[{"title":null,"identifiers":[" 1793-6462","0129-0657","issn: 1793-6462","issn:0129-0657"]}],"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2010,"topics":["Dimension reduction","Smoothing","Iocalized PCA","Mean shift."],"subject":["Article","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n14 January 2011\nVersion of attached file:\nAccepted Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nEinbeck, Jochen and Evers, Ludger and Powell, Benedict (2010) \u2019Data compression and regression through\nlocal principal curves and surfaces.\u2019, International journal of neural systems., 20 (3). pp. 177-192.\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1142\/S0129065710002346\nPublisher\u2019s copyright statement:\nAdditional information:\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\nInternational Journal of Neural Systems, Vol. 0, No. 0 (April, 2000) 00\u201300\nc\u00a9 World Scientific Publishing Company\nDATA COMPRESSION AND REGRESSION THROUGH\nLOCAL PRINCIPAL CURVES AND SURFACES\nJOCHEN EINBECK\nDepartment of Mathematical Sciences, Durham University\nDurham DH1 3LE, England\nE-mail: jochen.einbeck@durham.ac.uk\nLUDGER EVERS\nDepartment of Statistics, University of Glasgow\nGlasgow G12 8QQ, Scotland\nE-mail: ludger@stats.gla.ac.uk\nBENEDICT POWELL\nDepartment of Mathematical Sciences, Durham University\nDurham DH1 3LE, England\nE-mail: benedict.powell@durham.ac.uk\nReceived (to be inserted\nRevised by Publisher)\nWe consider principal curves and surfaces in the context of multivariate regression modelling. For\npredictor spaces featuring complex dependency patterns between the involved variables, the intrinsic\ndimensionality of the data tends to be very small due to the high redundancy induced by the dependencies.\nIn situations of this type, it is useful to approximate the high-dimensional predictor space through a low-\ndimensional manifold (i.e., a curve or a surface), and use the projections onto the manifold as compressed\npredictors in the regression problem. In the case that the intrinsic dimensionality of the predictor space\nequals one, we use the local principal curve algorithm for the the compression step. We provide a\nnovel algorithm which extends this idea to local principal surfaces, thus covering cases of an intrinsic\ndimensionality equal to two, which is in principle extendible to manifolds of arbitrary dimension. We\nmotivate and apply the novel techniques using astrophysical and oceanographic data examples.\n1. Introduction\nNowadays, we are confronted with data of ever in-\ncreasing complexity. There are three main manifes-\ntations of this complexity. Firstly, it is not unusual\nto observe sample sizes of formerly unthinkable mag-\nnitudes. Although this never posed a methodological\nproblem, such data sets could not be handled in the\npast due to data storage and computational limita-\ntions; however with advances in modern technology\nthe sample size in itself does not constitute a problem\nany more.\nThe second manifestation of complexity is more\nsevere. Often, not only the number of observations\ncollected is large, but also the number of variables\ninvolved. This problem, sometimes referred to as\n\u201cp \u226b n\u201d, is challenging not only from a computa-\ntional point of view, but also from a methodological\npoint of view. Consider the example of variable se-\nlection: the number of possible subsets of a set of p\nvariables is 2p, which is even for a moderately large\nnumber like p = 20 already more than a million.\nThe third manifestation of complexity has to do\nwith the intrinsic structure of the data themselves.\nData Compression and Regression through Local Principal Curves and Surfaces\nAdvances in science and modern technology have en-\nabled us to look deeper than ever into formerly in-\naccessible structures, yielding data with complex de-\npendency patterns. Whilst this might appear as a\ncurse at first sight, it can actually be a blessing: the\ncomplexity of high-dimensional data is often due to\nthe high redundancy of the variables involved. Ex-\nploiting this redundancy allows avoiding many of the\npitfalls of high-dimensional data analysis.\nIn Astrophysics for example, an issue of current\nresearch is to extract information on stellar param-\neters from photon counts collected at many differ-\nent wavelengths, paired with huge numbers (thou-\nsands or millions) of observations.1,2 Figure 1 shows\na scatterplot matrix of photon counts recorded at a\nsubset of 16 different wavelengths. Most variables\nare very strongly related. However, this relationship\nis non-linear, so that the association between these\nvariables would not be captured using the correla-\ntion coefficient. We will show that exploiting this\nlower-dimensional latent structure of the data allows\nfor building better models for predicting the stellar\nparameters.\nClearly, for situations of this type \u2014 but also\nfor much simpler problems \u2014 it is inefficient to op-\nerate with a full interaction model of type Y =\nm(X1, . . . , Xp) + noise. Here, Y is the response\nvariable, for instance the stellar temperature, and\n(X1, . . . , Xp) are the predictors, corresponding here\nto the photon counts at different wavelengths. Statis-\nticians have developed a huge range of tools in order\nsimplify the full interaction model so that it is more\ntractable. Common simplifications are, in decreas-\ning order of complexity, project pursuit regression,\nthe additive model, the partially linear model, or,\nmost simply, the multivariate linear model.3 Due to\nthe exponentially increasing difficulty of the model\nselection process mentioned above, a second string\nof research has looked for alternative ways of sim-\nplifying the model, and this family of methods is\nknown under the term dimension reduction. These\nmethods aim to compress the space of predictors\nX = (X1, . . . , Xp) before the actual model is fitted,\ni.e. we have a two-stage strategy:\n1 Find a dimension-reducing mapping f : Rp \u2212\u2192\nR\nd, with d < p, giving compressed data T =\nf(X) \u2208 Rd\n2 Base further inference on a regression model\nfor Y using T instead of the X as covariates.\nThe best-known example of a dimension-reducing\nmapping is principal component analysis (PCA).\nOther examples of such a technique include auto-\nassociative neural networks and self-organizing\nmaps.4 In this article we will explain how princi-\npal curves and surfaces can be used as a dimension-\nreducing mapping.\nWe will start with reviewing principal compo-\nnents, which, in combination with linear regression,\nis often referred to as principal component regres-\nsion (PCR). Here, the function f projects X \u2208 Rp\nonto the d-dimensional space spanned by the princi-\npal components corresponding to the largest d eigen-\nvalues \u03bb1, . . . , \u03bbd of \u03a3 = Cov(X):\nf : Rp \u2212\u2192 Rd, X 7\u2192 (\u03b31, \u00b7 \u00b7 \u00b7 , \u03b3d)\nT (X \u2212m),\ni = 1, . . . , n, where m = E(X) and \u03b31, . . . , \u03b3d are the\ncorresponding eigenvectors.\nSeveral alternative mappings have been pro-\nposed, which have in common with PCA that they\ncan be written as an affine transformation of X , i.e.\nthere is some d \u00d7 p matrix B and a d-dimensional\nvector c such that f(X) = BX + c. Members of this\nfamily of methods include sliced inverse regression5\nand parametric inverse regression.6\nIn our context, the word \u201ccompressing\u201d means\nnothing else than \u201cprojecting\u201d. That is, each data\npoint will be projected onto the nearest point on\nthe dimension-reduced subspace. In projecting data\nonto this subspace, we have to be prepared to lose\nsome information compared to the original \u201craw\u201d\ndata, which may impact on the accuracy of our fitted\nmodel. However, there is also a huge potential gain\ncompared to the model based on the raw data: if we\nhave reduced the dimension in step 1, we may be able\nto use a far more flexible and accurate model in step\n2. For instance, instead of a linear model with many\nvariables, we may use a one or two-dimensional non-\nparametric smoother. In other words, there is some\ntrade-off to be made between the loss of information\nin the projection step and the gain in precision in the\nestimation step. What the best trade-off will be, will\nlargely depend on how meaningful the projections in\nstep 1 are. If the predictor space features a strongly\nnon-linear shape, then the projections onto a linear\nsubspace (such as in PCA) may be of limited use. To\nillustrate this point more clearly, assume we are given\nData Compression and Regression through Local Principal Curves and Surfaces\nspec1\n0.78\n0.790.780.79\n0.76\n0.77\n0.760.77\nspec20.005\n0.0100.005.010\n0.000\n0.0050.000.005\nspec30.010\n0.0150.0200.010. 15.020\n0.0000.005\n0. 10\n0.000. 05.010\nspec40.015\n0.0200.0250.015.020.025\n0.0050.010\n0.015\n0.005.010.015\nspec50.04\n0.06.040.06\n0.00\n0.020.00.02\nspec60.06\n0.080.100.06.08.10\n0.000.02\n0.04\n0.00.02.04\nspec70.020.03\n0.040.02.03.04\n0.000.01\n0.02\n0.00.01.02\nspec80.010\n0.015.0100. 15\n0.000\n0.0050.0000. 05\nspec90.010\n0.015\n0.020.0100. 150.020\n0.000\n0.005\n0. 10\n0.0000. 050.010\nspec100.015\n0.0200.025. 15.020.025\n0.0000.005\n0.0\n0.000.005. 1\nspec11\n0.0060.0080.0060.008\n0.0020.0040.0020. 04\nspec12\n0.03\n0.040.030.04\n0.01\n0.02\n0.010.02\nspec130.015\n0.0200.0250.015.020.025\n0.0050.0 0\n0.015\n0.005.010.015\nspec140.015\n0.0200.0250.015.020.025\n0.0050.0 0\n0.015\n0.005.01.015\nspec150.03\n0.040.05.03.04.05\n0.000.01\n0.020.00.01.02\nspec160.04\n0.060.080.040.060.08\n0.000.02\n0.04\n0.000.020.04\nFig. 1. Pairwise matrix scatterplot of photon counts (fluxes) obtained at 16 different wave-\nlengths. The data were simulated through computer models within the Gaia project.2\na spiral-shaped bivariate predictor space as in figure\n2 (left panel). The dashed line shows the first prin-\ncipal component line through this data cloud, which\nexplains about 54% of the total variance. Clearly, the\nprojection indices (PIs) of the data projected orthog-\nonally onto this line will be uninformative for the ac-\ntual position of the data point within this cloud, just\nas it would be the case for any other linear approxi-\nmation of this data. In order to capture the intrinsic\nstructure of this data, one has to fit a curve through\nit nonparametrically. The statistical term for such a\nsmooth curve \u201cthrough the middle of a data cloud\u201d\nis a principal curve.7 The solid line in Figure 2 (left\npanel) shows such a curve fitted using the technique\nof local principal curves.8 Visually, the curve provides\na good one-dimensional summary of this data set. In\norder to use this curve for dimension reduction pur-\nposes, one has to be able to parametrize this curve,\nor at least to project data points onto it. The pro-\njections onto the local principal curve are shown in\nFigure 2 (right panel), and the resulting projection\nindices are informative for the position of the data\npoints within the cloud. Whether these projection\nindices are more informative for a (hypothetical) re-\nsponse variable than the straight line projections, is,\nof course, a question that we cannot answer in this\nexample, but we would hope that this will be the\ncase. We will see three examples in Section 3 where\nthis turns out to be the case.\nAn important concept that we will refer to is that\nof intrinsic dimensionality. We consider this term\nas being equivalent to the topological dimensionality,\nwhich is the basis dimension of the local linear ap-\nproximation of the hypersurface on which the data\nresides, i.e. the tangent space.9\nFor instance, the data in figure 2 appear to have\na topological dimension of one as they could be lo-\ncally approximated by a tangent to the curve in each\nData Compression and Regression through Local Principal Curves and Surfaces\nlocal neighborhood along the curve. This paper will\nfocus on data which feature a topological dimension\nof one or two, in which cases we will use local prin-\ncipal curves and surfaces, respectively, in the com-\npression step. These terms should be separated from\nthe notion of structural dimensionality as advocated\nfor instance by Cook10, which is the dimension of\nthe central subspace, i.e. the smallest linear sub-\nspace which contains all relevant information about\nthe response.\nWe proceed in the following section with setting\nup the local principal curve methodology that we\nshall be using to handle situations with intrinsic di-\nmensionality equal to 1. We provide several real data\nexamples and a comparison with other dimension\nreduction techniques in Section 3, and extend our\nmethodology towards two-dimensional nonparamet-\nric data summaries (in form of principal surfaces) in\nSection 4. We finish with a conclusion in Section 5.\n2. Dimension reduction via principal curves\n2.1. Local principal curves\nLocal principal curves (LPC)8 are based on the\nidea that, at each point x \u2208 Rp along a princi-\npal curve, the localized first principal component\nline forms the best one-dimensional linear approxi-\nmation to the curve. They can be seen as a sim-\nple and fast approximation to the mathematically\nand computationally more demanding concept de-\nveloped earlier by Delicado.11 Assume we are given\ndata x1, . . . , xn \u2208 R\np of which we think as n inde-\npendent replicates drawn from the random vector\nX = (X1, . . . , Xp)\nT , i.e. xi = (xi1, . . . , xip)\nT .\nBeginning at some starting point x = x0 \u2208 R\np,\nLPCs proceed through the data cloud, alternating\nbetween the following two steps:\n(i) Calculate a localized center of mass \u00b5x =\u2211n\ni=1 w\nx\ni xi, where\nwxi = KH(xi \u2212 x)Xi\/\n\u2211n\nj=1KH(xj \u2212 x).\n(ii) Compute the first local eigenvector \u03b3x of \u03a3x =\n(\u03c3xjk)(1\u2264j,k\u2264p), where \u03c3\nx\njk =\n\u2211n\ni=1 w\nx\ni (xij \u2212\n\u00b5xj )(xik \u2212 \u00b5\nx\nk) and \u00b5\nx\nj denotes the j\u2212th com-\nponent of \u00b5x. Using a predetermined step size\nz, step from \u00b5x to x := \u00b5x + z\u03b3x.\nThe sequence of the local centers of mass \u00b5x makes\nup the local principal curve. Here, KH(\u00b7) =\n|H |\u22121\/2K(H\u22121\/2\u00b7), with a multivariate kernel K\nand a positive definite bandwidth matrix H =\ndiag(h21, . . . , h\n2\np). Just as for usual PCA, it is\nrecommendable to use input variables X1, . . . , Xp\nwhich are operating on similar scales, which can be\nachieved e.g. by dividing by their range or standard\ndeviation. In this case, it is common to use band-\nwidths h \u2261 h1 = h2 = . . . = hp, and to choose z = h\nas well. The LPC algorithm has been extended to\ndisconnected8 and branched12 curves, which can be\neasily implemented using suitable multiple starting\npoints. Crossings can be handled conveniently us-\ning an angle penalization.8 As in each iteration only\npoints in the local neighborhood are considered, the\nalgorithm is quite flexible, and, at the same time,\nrobust to outliers.\n2.2. Parametrization, projection, and feature\nextraction\nFor a fitted LPC consisting of L local centers of\nmass \u00b5x\u2113 \u2261 \u00b5\u2113 = (\u00b5\u21131, . . . , \u00b5\n\u2113\np)\nT , \u2113 = 1, . . . , L, we\nseek a curve {g(t), t \u2208 Ig} which interpolates the lo-\ncal centers of mass. This curve can be parametrized\nby a function\ng : Ig \u2212\u2192 R\np, t 7\u2192 (g1(t), . . . , gp(t))\nT\n,\nwhere Ig \u2282 R denotes the domain of g. The param-\neter t corresponds to the projection index. Firstly,\none end point is chosen to be the origin correspond-\ning to t = 0. This is an arbitrary choice and we will\nuse the convention that t increases in the direction of\n\u03b3x0 . Technically, the curve is parametrized in three\nsteps:\n(i) Compute a discrete, preliminary parametriza-\ntion (s\u2113)(1\u2264\u2113\u2264L), with the same origin as t,\nby adding up the Euclidean distances between\nsubsequent \u00b5\u2113, \u2113 = 1, . . . , L.\n(ii) For each dimension of the covariate space j =\n1, . . . , p, interpolate the points (s\u2113, \u00b5\n\u2113\nj)1\u2264\u2113\u2264L\nby a cubic spline, yielding graphs (s, \u00b5j(s)).\nPutting them together, one obtains a con-\ntinuous and differentiable spline function\n(\u00b51, . . . , \u00b5p)\nT (s) \u2261 \u00b5(s).\n(iii) For each value of s within the support of the\nspline function, recalculate the parameter us-\ning the arc length,\nt =\n\u222b s\n0\n\u221a\n(\u00b5\u20321(u))\n2 + . . .+ (\u00b5\u2032p(u))\n2 du,\nData Compression and Regression through Local Principal Curves and Surfaces\n++\n+\n++\n+ ++\n++++\n++++ +\n++\n+\n+\n++\n+++\n+\n++++\n+\n+\n++\n+\n+\n++++\n+++\n++\n+\n+++++\n+\n+\n+ +\n+\n+\n+\n++\n+ ++ +\n+\n+\n++\n++\n+ ++++\n+ ++\n+++\n+\n++++ +\n+\n++++\n+\n++++\n+ +\n+\n+++ +\n++\n+++ +\n++\n+\n+\n+\n+\n+ ++ +\n++\n++ +\n+\n+\n++\n+\n+++\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n++\n++\n+\n+\n+\n+\n+\n+\n++ +\n+\n+\n+\n+\n+\n+\n+ +++\n+\n++ +\n+\n+++\n++ +\n+\n++\n+\n++\n+\n++\n+\n+\n++\n+++\n+ +\n+\n+\n+++++\n+ ++\n+\n+\n+\n+\n+\n+++\n++\n++\n+\n++\n+ +\n+\n+++++\n+\n+\n+\n+\n+++ +\n++\n++\n++++++++\n+\n++\n+\n+\n+ ++\n+\n+\n+\n+++++ +\n++++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++++\n+\n+++\n+\n+ +++++\n+\n+\n+\n+++++\n+\n+\n+\n+\n+\n+\n+\n++\n+\n++\n+\n++\n++\n++\n+\n++\n+\n++\n++\n++\n++\n+\n+\n++\n+ +++ ++\n+\n+++\n+\n++++\n+++\n++ +\n++ +\n+\n+\n+\n+\n+\n+++\n++\n++ ++\n+\n+\n++\n++++ ++++\n+\n+\n+\n+\n+ ++\n+\n++\n++ ++++ ++++ ++ ++\n+\n+ ++\n+ +++ +\n+\n+\n++++++ ++\n+++\n+\n+\n+\n+\n++\n+\n+\n+\n+++++ +\n++\n+\n+\n+\n+++\n++\n+\n++++\n+\n+\n+++\n+\n+\n+\n+\n+++++\n+\n++\n+++ ++\n+ +\n+\n++\n+\n+++\n+\n+\n+\n+\n+ +\n+\n+\n++++\n++++ +\n+\n+\n+\n+\n++ ++\n+\n+\n++\n+++\n+\n+++\n++\n+\n+\n+\n+\n+ +++\n++++\n+++\n++\n++\n+\n++ +\n+\n++++\n+++\n++\n++++++\n+\n+\n+\n+\n++++\n+\n++++\n++ +++\n++\n+\n+\n+\n+\n+\n+\n+ +++\n+\n+++\n++\n+\n+++\n+\n+\n+\n+\n+ +\n++ +\n+\n+\n++\n+ +\n+++\n+\n+\n+\n+ ++ ++\n+\n+\n+\n+ +\n++\n+++\n+\n+\n+\n+\n++ +\n++\n+ +++ +\n+\n++\n+\n++\n+\n+ +\n\u22121.0 \u22120.5 0.0 0.5\n\u2212\n1.\n0\n\u2212\n0.\n5\n0.\n0\n0.\n5\n1.\n0\nx1\nx 2\n++\n+\n++\n+ ++\n++++\n++++ +\n++\n+\n+\n++\n+++\n+\n++++\n+\n+\n++\n+\n+\n++++\n+++\n++\n+\n+++++\n+\n+\n+ +\n+\n+\n+\n++\n+ ++ +\n+\n+\n++\n++\n+ ++++\n+ ++\n+++\n+\n++++ +\n+\n++++\n+\n++++\n+ +\n+\n+++ +\n++\n+++ +\n++\n+\n+\n+\n+\n+ ++ +\n++\n++ +\n+\n+\n++\n+\n+++\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n++\n++\n+\n+\n+\n+\n+\n+\n++ +\n+\n+\n+\n+\n+\n+\n+ +++\n+\n++ +\n+\n+++\n++ +\n+\n++\n+\n++\n+\n++\n+\n+\n++\n+++\n+ +\n+\n+\n+++++\n+ ++\n+\n+\n+\n+\n+\n+++\n++\n++\n+\n++\n+ +\n+\n+++++\n+\n+\n+\n+\n+++ +\n++\n++\n++++++++\n+\n++\n+\n+\n+ ++\n+\n+\n+\n+++++ +\n++++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++++\n+\n+++\n+\n+ +++++\n+\n+\n+\n+++++\n+\n+\n+\n+\n+\n+\n+\n++\n+\n++\n+\n++\n++\n++\n+\n++\n+\n++\n++\n++\n++\n+\n+\n++\n+ +++ ++\n+\n+++\n+\n++++\n+++\n++ +\n++ +\n+\n+\n+\n+\n+\n+++\n++\n++ ++\n+\n+\n++\n++++ ++++\n+\n+\n+\n+\n+ ++\n+\n++\n++ ++++ ++++ ++ ++\n+\n+ ++\n+ +++ +\n+\n+\n++++++ ++\n+++\n+\n+\n+\n+\n++\n+\n+\n+\n+++++ +\n++\n+\n+\n+\n+++\n++\n+\n++++\n+\n+\n+++\n+\n+\n+\n+\n+++++\n+\n++\n+++ ++\n+ +\n+\n++\n+\n+++\n+\n+\n+\n+\n+ +\n+\n+\n++++\n++++ +\n+\n+\n+\n+\n++ ++\n+\n+\n++\n+++\n+\n+++\n++\n+\n+\n+\n+\n+ +++\n++++\n+++\n++\n++\n+\n++ +\n+\n++++\n+++\n++\n++++++\n+\n+\n+\n+\n++++\n+\n++++\n++ +++\n++\n+\n+\n+\n+\n+\n+\n+ +++\n+\n+++\n++\n+\n+++\n+\n+\n+\n+\n+ +\n++ +\n+\n+\n++\n+ +\n+++\n+\n+\n+\n+ ++ ++\n+\n+\n+\n+ +\n++\n+++\n+\n+\n+\n+\n++ +\n++\n+ +++ +\n+\n++\n+\n++\n+\n+ +\n\u22121.0 \u22120.5 0.0 0.5\n\u2212\n1.\n0\n\u2212\n0.\n5\n0.\n0\n0.\n5\n1.\n0\nx1\nx 2\nFig. 2. Left: illustrating example comparing the principal component line (dashed) to a\nprincipal curve (solid) as dimension-reducing mapping; right: orthogonal projections onto\nthe principal curve.\nand set g(t) = \u00b5(s).\nIt should be noted that no smoothing is involved in\n(ii) \u2014 the \u00b5\u2113 are just interpolated.\nOnce this parametrization is established, each\ndata point xi, i = 1, . . . , n, can be projected onto\nthe curve by finding the point on the curve which is\nnearest to it (in terms of Euclidean distances), yield-\ning the projection index ti. More formally, the di-\nmension reducing mapping is given by\nT \u2261 f(X) = sup\nt\u2208Ig\n{||x\u2212g(t)|| = inf\n\u03c4\u2208Ig\n||x\u2212g(\u03c4)||}. (1)\nThis definition goes back to the original principal\ncurve paper7: Hastie and Stuetzle use the projec-\ntion indices both in the definition of principal curves\nand in the algorithm for fitting them. However\nthey did not make any further use of the projec-\ntion indices. More recently, Ming-Ming et al.13 em-\nphasized the significance of the function f(\u00b7) as a\nfeature extractor for X . The logical next step is\nto base further inference about the response vari-\nable of a regression model on the extracted features\nti \u2261 f(xi), i = 1, . . . , n.\n2.3. Regression and prediction\nIn order to link the extracted feature T to the\nresponse Y , we proceed by fitting a univariate re-\ngression model\nyi = m(ti) + \u01ebi, i = 1, . . . , n.\nThe function m : R \u2212\u2192 R could in principle be\nspecified parametrically, for instance m(ti) = a+bti.\nAn example for this will be provided in Section 3.1.\nHowever, in the vast majority of situations where\nwe have to cope with data structures which are suf-\nficiently complex to justify application of the tech-\nniques mentioned above, we will also expect the re-\nsponse to be non-trivially related to the extracted\nfeature, so that typically m(\u00b7) will need to be mod-\nelled nonparametrically. Univariate nonparametric\nsmoothing is a standard procedure and well-studied\nroutines performing this job are readily available.\nFor instance, smoothing splines, local polynomials,\nbut also feed-forward neural networks could be used\nhere.\nAssume finally that we have a new observation\nxnew \u2208 R\nd available and wish to predict the yet un-\nobserved response ynew. This is now achieved in two\nsteps:\n(i) Using (1), project xnew onto the LPC g. This\ngives a projection index tnew.\n(ii) Compute y\u02c6new = m\u02c6(tnew) from the fitted non-\nparametric smoother.\nData Compression and Regression through Local Principal Curves and Surfaces\nWe will give some examples illustrating these\ntechniques in the next section.\n3. Data examples\n3.1. New Zealand Horse mussels\nWe consider data consisting of measurements of\nthe shell height (H), shell length (L), shell width\n(W ), shell mass (S), and the edible muscle mass of\nthe mussels in gram (M) of 172 horse mussels. We\nwill use the edible muscle mass (M) as the response\nvariable. These data were repeatedly analyzed in the\ncontext of dimension reduction.10,6 The latter refer-\nence also performs a test based on the singular values\nof the standardized matrix of inverse regression coef-\nficients to demonstrate that the structural dimension\nof the predictor space can be taken to be equal to one.\nThere is no theoretical justification which would al-\nlow us to conclude that the topological and structural\ndimension should necessarily be the same. Neverthe-\nless, visual inspection of the four-dimensional mussel\ncharacteristics (figure 3, top panel), seems to give\nsufficient evidence to allow us to work with an in-\ntrinsic dimension of d = 1. A local principal curve\nis fitted, with the result shown in figure 3 (bottom\npanel): it matches closely the appearance of the raw\ndata.\nWe proceed with projecting the predictors onto\nthis curve, and plotting the response against the pro-\njection indices. The resulting scatterplot is shown\nin figure 4 (left), which shows clearly a linear re-\nlationship between muscle mass and the projection\nindex. The resulting linear regression line y =\n1.037+0.113T has a residual standard error of 4.108\non 80df, and the coefficient of determination R2\ntakes the value 0.879. For comparison, Bura and\nCook6 derived another one-dimensional summary of\nthe predictor space via parametric inverse regression.\nSpecifically, they propose to define a new variable,\nsay C, as\nC = 0.028H\u22120.029L\u22120.0593 log(S)+0.804 log(W ).\nFrom the right plot in figure 4 it is evident that a\nsimple linear regression of M against C is not ade-\nquate here. Therefore, we employ a quadratic model,\nyielding the regression curve y = \u22122.230\u2212 3.832C+\n0.964C2 with a residual standard error of 6.051 on\n79df and corresponding R2 = 0.7401. This curve is\nshown in figure 3 (right). Clearly, the fit based on\nthe LPC performs superior in all aspects, and, in con-\ntrast to parametric inverse regression6, the method\ndoes not require \u201cvisual inspection of the scatterplot\nmatrix\u201d in order to \u201cdecide what functions of Y fit\nthe data best\u201d.\nOne may have doubts on the stability of the LPC-\nbased result, as the fitted local principal curve de-\npends (slightly) on the position of the starting point\nx0. To check this, we ran the LPC algorithm 100\ntimes, each time selecting a starting point at random\nfrom the cloud. The mean of the residual standard\nerrors of the 100 linear regression models was 4.1159\nwith a standard deviation of 0.0515, indicating that\nthe estimated line is very stable and that the differ-\nences in the fitted local principal curve only play a\nmarginal role. More care is, of course, needed if the\npredictors are highly scattered in space. An example\nfor such a situation will be provided below.\n3.2. Gaia data\nGaia is an astrophysics mission of the European\nSpace Agency (ESA) which will undertake a detailed\nsurvey of over 109 stars in our Galaxy and extra-\ngalactic objects. A satellite is to be launched in 2012,\nwhich will collect spectra (photon counts at certain\nwavelengths) from objects all over the universe. The\naims of the mission, among others, are to classify ob-\njects (as star, galaxy, quasar,...), and to learn about\nstellar properties in form of certain astrophysical pa-\nrameters (\u201cAPs\u201d: temperature, metallicity, gravity,\netc.).\nUntil the satellite will be launched, one has to\nwork with simulated data generated by a complex\ncomputer model. In total, 68 different wavelengths\nare considered in the scope of the Gaia project, but\nfor simplicity, we will consider in this paper only\na subset of 16 different wavelengths showing vari-\nance in the three astrophysical parameters temper-\nature, metallicity and gravity. Temperature is a\n\u201cstrong\u201d parameter: it accounts for most of the vari-\nance across the data set.14 Gravity and metallicity,\nin contrast, are \u201cweak\u201d parameters. The parameters\nhave a correlated impact on the data, e.g. at high\ntemperatures, varying the metallicity has a much\nsmaller impact on spectra than it does at low temper-\natures. The data are simulated to the typical noise\nData Compression and Regression through Local Principal Curves and Surfaces\nH\n150 200 250 300 20 30 40 50 60\n80\n10\n0\n12\n0\n14\n0\n16\n0\n15\n0\n20\n0\n25\n0\n30\n0\nL\nS\n0\n50\n15\n0\n25\n0\n35\n0\n80 100 120 140 160\n20\n30\n40\n50\n60\n0 50 100 150 200 250 300 350\nW\nH\n150 200 250 300 20 30 40 50 60\n80\n10\n0\n12\n0\n14\n0\n15\n0\n20\n0\n25\n0\n30\n0\nL\nS\n0\n50\n15\n0\n25\n0\n35\n0\n80 100 120 140\n20\n30\n40\n50\n60\n0 50 100 150 200 250 300 350\nW\nFig. 3. Scatterplot matrix of horse mussel data (top panel); local principal curves (bottom\npanel). It should be emphasized the fitted LPC is one curve through four-dimensional\nspace; what we are seeing here are the two-dimensional pairwise projections onto the\nrespective coordinate axes.\nData Compression and Regression through Local Principal Curves and Surfaces\n0 100 200 300 400\n0\n10\n20\n30\n40\n50\nprojection index T\nM\n\u22125 \u22124 \u22123 \u22122 \u22121\n0\n10\n20\n30\n40\n50\nC\nM\nFig. 4. Left: plot of mussel muscle mass M vs. projection indices T with regression\nline through the origin; right: plot of M versus the values of Bura and Cook\u2019s6 linear\ncombination of predictors (C) obtained via parametric inverse regression.\nproperties for such data, in our case Gaussian white\nnoise.\nIn our setting, the photon counts form the predic-\ntor space and the APs form the response space. Note\nthat this is opposite to the direction of simulation.\nA consequence is that the regression problem may\nbe degenerate, i.e., one set of photon counts may be\nassociated with two different APs. We focus here on\nthe temperature, which features the least amount of\ndegeneracy.\nApproaching the data naively, one could consider\nfitting a multiple linear regression model, with the\nphoton fluxes at the 16 wavelengths as regressors.\nHowever, this leads to a useless model due to the\nmulti-collinearity induced by the high redundancy\nof the photon counts.15 Obviously there is the po-\ntential for dimension reduction in this data set. To\nget a deeper insight into the structure of the data,\nwe plotted the first three principal component scores\nagainst each other, yielding the data cloud depicted\nin figure 5 (a). Data points corresponding to higher\ntemperatures are shaded in red. One can see that the\nposition within the curved data cloud is informative\nfor the temperature. Next we will fit the local prin-\ncipal curve, which is shown in figure 5 (b) as a solid\nline, with the local centers of mass represented as sky\nblue squares. The fitted spline function is depicted in\nfigure 5 (c). It is clear that it is almost indistinguish-\nable from the original LPC (and precisely coincides\nwith it at the position of the local centers of mass).\nProjections onto the curve are illustrated in figure\n5 (d). A scatterplot of the temperature against the\nprojection indices is provided in figure 6, and the fit-\nted smoothing spline is shown as a green solid curve.\nThis spline curve provides the fitted output of the\noriginally 16-dimensional regression problem.\nNext, we perform a small simulation study to\nget an impression of the relative performance of the\nproposed technique. We sample n\u2032 = 1000 test\ndata from the remaining 8286 \u2212 1000 observations\nand observe the prediction errors, \u03b5\u02c6i = \u201ctrue mi-\nnus predicted temperature\u201d. The average prediction\nerror of the test data as well as the training data\nare summarized in Table 1. Considering firstly the\nData Compression and Regression through Local Principal Curves and Surfaces\n(a)\n\u22120.08 \u22120.06 \u22120.04 \u22120.02  0.00  0.02  0.04\n\u2212\n0.\n02\n\u2212\n0.\n01\n \n0.\n00\n \n0.\n01\n \n0.\n02\n \n0.\n03\n \n0.\n04\n\u22120.10\n\u22120.05\n 0.00\n 0.05\n 0.10\nComp.2\nCo\nm\np.\n1\nCo\nm\np.\n3\n+\n+\n+\n+\n++\n++\n+\n+\n+\n+\n+\n+\n+\n+\n++++\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+++\n+++\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n++\n++\n++\n++\n+\n+++\n+++\n+\n+ +\n+\n+\n++\n++\n+++\n+\n+ ++\n+\n+\n+\n+\n+\n+\n+\n+\n+++\n+\n++\n++\n+++\n++ +\n+ ++\n+\n+\n++++\n++ +\n+\n+\n++++\n++++\n+ ++\n+\n+\n+ ++\n++\n+\n+\n+\n++\n+\n++\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n++\n+\n+\n+\n++\n++\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+ +\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n++\n+\n+\n++\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n++\n+\n++\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n++\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n++\n+\n+\n++\n+\n+\n++\n+\n++\n++\n++++\n+\n+\n+\n+\n+\n+\n+++++\n++\n+\n+++\n++++\n++\n+++\n++\n++++\n+++\n+\n++\n+\n+++\n++++\n+\n+++++\n+\n+++++++\n+++++++\n++++++++++\n+\n+++++++++\n++\n++\n+\n+\n++++++++++++\n+++++\n+\n++++++++++\n(b)\n\u22120.08 \u22120.06 \u22120.04 \u22120.02  0.00  0.02  0.04\n\u2212\n0.\n02\n\u2212\n0.\n01\n \n0.\n00\n \n0.\n01\n \n0.\n02\n \n0.\n03\n \n0.\n04\n\u22120.10\n\u22120.05\n 0.00\n 0.05\n 0.10\nComp.2\nCo\nm\np.\n1\nCo\nm\np.\n3\n+\n+\n+\n+\n++\n++\n+\n+\n+\n+\n+\n+\n+\n+\n++++\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+++\n+++\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n++\n++\n++\n++\n+\n+++\n+++\n+\n+ +\n+\n+\n++\n++\n+++\n+\n+ ++\n+\n+\n+\n+\n+\n+\n+\n+\n+++\n+\n++\n++\n+++\n++ +\n+ ++\n+\n+\n++++\n++ +\n+\n+\n++++\n++++\n+ ++\n+\n+\n+ ++\n++\n+\n+\n+\n++\n+\n++\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n++\n+\n+\n+\n++\n++\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+ +\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n++\n+\n+\n++\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n++\n+\n++\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n++\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n++\n+\n+\n++\n+\n+\n++\n+\n++\n++\n++++\n+\n+\n+\n+\n+\n+\n+++++\n++\n+\n+++\n++++\n++\n+++\n++\n++++\n+++\n+\n++\n+\n+++\n++++\n+\n+++++\n+\n+++++++\n+++++++\n++++++++++\n+\n+++++++++\n++\n++\n+\n+\n++++++++++++\n+++++\n+\n++++++++++\n(c)\n\u22120.08 \u22120.06 \u22120.04 \u22120.02  0.00  0.02  0.04\n\u2212\n0.\n02\n\u2212\n0.\n01\n \n0.\n00\n \n0.\n01\n \n0.\n02\n \n0.\n03\n \n0.\n04\n\u22120.10\n\u22120.05\n 0.00\n 0.05\n 0.10\nComp.2\nCo\nm\np.\n1\nCo\nm\np.\n3\n+\n+\n+\n+\n++\n++\n+\n+\n+\n+\n+\n+\n+\n+\n++++\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+++\n+++\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n++\n++\n++\n++\n+\n+++\n+++\n+\n+ +\n+\n+\n++\n++\n+++\n+\n+ ++\n+\n+\n+\n+\n+\n+\n+\n+\n+++\n+\n++\n++\n+++\n++ +\n+ ++\n+\n+\n++++\n++ +\n+\n+\n++++\n++++\n+ ++\n+\n+\n+ ++\n++\n+\n+\n+\n++\n+\n++\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n++\n+\n+\n+\n++\n++\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+ +\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n++\n+\n+\n++\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n++\n+\n++\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n++\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n++\n+\n+\n++\n+\n+\n++\n+\n++\n++\n++++\n+\n+\n+\n+\n+\n+\n+++++\n++\n+\n+++\n++++\n++\n+++\n++\n++++\n+++\n+\n++\n+\n+++\n++++\n+\n+++++\n+\n+++++++\n+++++++\n++++++++++\n+\n+++++++++\n++\n++\n+\n+\n++++++++++++\n+++++\n+\n++++++++++\n(d)\n\u22120.08 \u22120.06 \u22120.04 \u22120.02  0.00  0.02  0.04\n\u2212\n0.\n02\n\u2212\n0.\n01\n \n0.\n00\n \n0.\n01\n \n0.\n02\n \n0.\n03\n \n0.\n04\n\u22120.10\n\u22120.05\n 0.00\n 0.05\n 0.10\nComp.2\nCo\nm\np.\n1\nCo\nm\np.\n3\n+\n+\n+\n+\n++\n++\n+\n+\n+\n+\n+\n+\n+\n+\n++++\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+++\n+++\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n++\n++\n++\n++\n+\n+++\n+++\n+\n+ +\n+\n+\n++\n++\n+++\n+\n+ ++\n+\n+\n+\n+\n+\n+\n+\n+\n+++\n+\n++\n++\n+++\n++ +\n+ ++\n+\n+\n++++\n++ +\n+\n+\n++++\n++++\n+ ++\n+\n+\n+ ++\n++\n+\n+\n+\n++\n+\n++\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n++\n+\n+\n+\n++\n++\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+ +\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n++\n+\n+\n++\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n+\n+\n++\n+\n++\n+\n++\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n++\n++\n+\n+\n+\n+\n++\n+\n+\n+\n+\n+\n+\n++\n+\n+\n++\n+\n+\n++\n+\n++\n++\n++++\n+\n+\n+\n+\n+\n+\n+++++\n++\n+\n+++\n++++\n++\n+++\n++\n++++\n+++\n+\n++\n+\n+++\n++++\n+\n+++++\n+\n+++++++\n+++++++\n++++++++++\n+\n+++++++++\n++\n++\n+\n+\n++++++++++++\n+++++\n+\n++++++++++\nFig. 5. (a) 3D scatterplot of principal component scores. Red data points correspond to\nhigh temperatures and blue data points to low temperatures. (b) The same plot with a\nlocal principal curve (solid), and local centers of mass plotted as light blue squares. (c)\nThe cubic spline constructed via the algorithm in Section 2.3 is overlaid over the LPC. (d)\nProjections (black) onto the cubic spline (green) through PC scores (grey).\nparametric methods, we observe that, unsurprisingly,\nPC\/LM performs almost as well as LM. The additive\nmodel PC\/AM beats the parametric models signifi-\ncantly, which is particularly evident for the medians\nof squared residuals. Next, we turn to LPC-based\nregression techniques. Note that PC\/LPC stands\nfor extracting the principal components (PC), fitting\nthe LPC, and smoothing the response vs. the pro-\njection index, where the third step is notationally\nomitted for convenience. As the starting point of\nthe LPC algorithm, we choose the point of highest\ndensity. Comparing PC\/AM and PC\/LPC to each\nother, we observe that the latter performs generally\nbetter than the former, where the improvement is\nlarger for the medians than for the means. This can\nbe explained through the very hot points at the left\nboundary of figure 6 which impact more severely on\nthe mean than on the medians. We will attempt\nto improve these results even further in Section 4.\nWe also compare our results to nonparametric re-\ngression based on a local principal curve fitted di-\nrectly through the 16-dimensional space of spectra.\nThe corresponding test errors given in Table 1 in the\n0.00 0.05 0.10 0.15 0.20\n10\n00\n0\n20\n00\n0\n30\n00\n0\n40\n00\n0\n50\n00\n0\nProjection indices (t)\nTe\nm\npe\nra\ntu\nre\nSmoothing Spline\nFig. 6. Scatterplot of stellar temperatures versus PIs.\nData Compression and Regression through Local Principal Curves and Surfaces\ncolumn \u2018LPC\u2019 indicate a very slightly improved re-\nsult compared to the two-step compression PC\/LPC.\nHowever, it has recently been reported that direct\nlocal principal curve regression for high-dimensional\npredictor spaces (say, p > 4 or 5) should better\nbe avoided, or at least performed with care.15 The\nreason for this is that the dependence of the LPC\non the starting point, and, at the same time, the\nrisk of missing remote data patterns, increases for\ndata of high dimension. To shed some light on this\nstatement, we repeated the two LPC-based regres-\nsion approaches each a 100 times, but now choosing\neach time a starting point at random from the data\ncloud. The interquartile range of the 100 test errors\nobserved is provided in the squared brackets. It is\nclearly seen that direct LPC regression behaves far\nless reliably than the regression based on the com-\npressed scores.\n3.3. Sea water temperature\nThe oceanographic data comes via the World\nOcean Database16, held by the American National\nOceanographic Datacenter, whose data is publicly\navailable online\u2217. The sample studied here consists of\nobservations over nine days in May 2000 taken by the\nGerman vessel, Gauss, in the North Atlantic. The\nshape of the temperature vs. depth plot is well doc-\numented in introductory oceanographic literature.17\nIt shows high temperature and high variability near\nthe surface, and a pronounced drop typically from\naround 1000m to 2000m known as the pycnocline: a\ntransition stage between surface waters and bottom\nwaters. The oxygen levels near the ocean surface\nalso tend to be high, due to photosynthetically active\nplant-life there. Further down sunlight is reduced so\noxygen is not produced but is still absorbed by respir-\ning organisms. An oxygen peak at 2000m coincides\nwith the upper surface of the previously mentioned\nfresh cold deep water whose presence largely is due\nto sea ice melt water from the poles.\nOne can see that the simple trends between the\nvariables tend to break down at the surface, because\nof disturbances from the atmosphere, and also at the\npycnocline. The variability in the second region is\npartially explained by considering contours of wa-\nter density given its temperature and salinity. At\nthis boundary we have a meeting of warmer saltier\nwater (from evaporation at the surface) and colder\nfresher water. Whether the change in temperature,\nor change in salinity, dominates in its effect on the\ndensity gradient, and therefore whether the layers\nmix, is dependent on the water properties at the\nboundary.\nAs all variables operate on different scales, we\nfirst standardize the data by dividing each variable\nby their range. An LPC is fitted through the data\ncloud using the bandwidth h = 0.11. The local prin-\ncipal curve (as interpolated by splines) is depicted\nalong with projections in figure 8. The curve seems\nto do a fairly good job, though variation around it\nstill appears to be quite high. The question relevant\nfor our developments is whether the projection index\nis informative for the target variable, water temper-\nature. Therefore, we coloured the segments repre-\nsenting the projections by their associated (true, ob-\nserved) temperature values. If the projection indices\nare meaningful for the temperature, then the colour\nsaturation of red and blue colours should vary con-\ntinuously and smoothly with the projection index.\nOne observes that this is largely the case for the\nblue (cold) branch of the cloud, but something less\nclear occurs in the red (warm) part. Here \u201cpurple\u201d\n(moderately warm) segments from one side of the\ncurve project closely to red (warm) segments from\nthe other side of the curve. Obviously, there is rel-\nevant information on the temperature which is not\ncaptured through the projection indices. The con-\nsequence of this can be observed in Figure 9: For\nthe warmer regions, the plot of water temperature\nagainst the projection index features two almost par-\nallel strings, with the upper and lower one corre-\nsponding to data on each side of the LPC. The black\nline is a fitted local-linear smoother, which describes\nthe right part of the curve very well, but does not de-\nscribe the left part equally well. This suggests that\na (one-dimensional) curve cannot capture all the rel-\nevant information, which appears to reside in a two-\ndimensional surface.\nOne approach which allows for accessing the in-\nformation orthogonal to a principal curve was pro-\nposed very recently by Ming-Ming et al.13 They\ndefine a \u201csecond-order feature extractor\u201d through\nthe directed distance (i.e., distances on one side of\nthe curve are counted negatively, and on the other\n\u2217http:\/\/www.nodc.noaa.gov\/OC5\/WOD\/pr_wod.html\nData Compression and Regression through Local Principal Curves and Surfaces\nLM PC\/LM PC\/AM PC\/LPC LPC PC\/LPS\n\u03bb = 0.1 \u03bb = 1 \u03bb = 10\nTraining mean 3845 4227 1199 821 793 669 753 9573\nerror median 982 1073 100 46 45 22 36 1355\nTest mean 4593 4967 1732 1359 [91] 1320 [211] 1064 1227 10666\nerror median 1049 1124 104 44 [3] 43 [23] 35 47 1339\nTable 1. Squared prediction error\/103 for temperature. (LM= Linear Model,\nPC=Principal Components, AM=Additive Model, LPC=Local Principal Curve,\nLPS=Local Principal Surface). The results under PC\/LPS will be explained in Section\n4. For all reported test errors, the starting point of the LPC or LPS was chosen to be\nthe highest density point. The IQR of the test errors obtained through LPCs using 100\nrandom starting points are provided in squared brackets.\nside of the curve positively), which gives together\nwith the first order features (the projection) a two-\ndimensional feature space, onto which the response\ncan be regressed. We do not pursue this approach\nfurther in this paper, firstly because the concept of\n\u201cdifferent sides of a curve\u201d is potentially ambiguous,\nand secondly as we are aiming for a more general\nhandling of this problem by extending local principal\ncurve methodology directly to higher-dimensional\nnonparametric data summaries which could be gen-\nerally described as \u201clocal principal manifolds\u201d. A\nfirst but essential step to this is the extension to-\nwards local principal surfaces, which is the topic of\nthe next section.\n4. Local principal surfaces\nBefore we generalize local principal curves to sur-\nfaces, let us first of all go back to the local prin-\ncipal curve algorithm presented above. It had two\nimportant building blocks: the local first eigenvec-\ntor, which is responsible for extrapolating the cur-\nrent curve, and the local mean, which is responsible\nfor adjusting this extrapolated value. We will refer to\nthis second step as mean shift.18 It turns out, as we\nwill explain below, that this mean shift is the much\nmore important of the two steps.\nThe first local principal component at x is the\nline through \u00b5x which minimizes the weighted dis-\ntance between data and line, with weights wxi as de-\nfined in part (i) of the algorithm. In other words, \u03b3x\ndefines the locally optimal line, i.e. the most relevant\ndirection to which one can turn from \u00b5x. However,\nthis choice is, despite its optimality properties, by no\nmeans the only possible option.8 In turns out that it\nis only important that a movement is made \u201cinto the\ndirection of the data cloud\u201d, and the mean shift will\nsubsequently do the job of adjusting the principal\ncurve again towards the \u201cmiddle\u201d of the (local) dis-\ntribution of the data cloud. Most importantly, if we\nwere to replace the first local eigenvector \u03b3x by the\ndirection of the previous step \u00b5\u2113\u2212\u00b5\u2113\u22121, we would ob-\ntain an algorithm very similar to the local principal\ncurve algorithm. This modified algorithm has, just\nlike the original local principal curve algorithm, line\nsegments as geometric building blocks. Continuing\nthis geometric interpretation, the modified algorithm\ncan be viewed as extending the curve by attaching a\nnew line segment obtained by extending (or reflect-\ning over) the last line segment and adjusting its free\nvertex by applying the mean shift.\nWe exploit this geometric view for the extension\nof local principal curves to local principal surfaces\n(LPS). The basic building block of the local princi-\npal surface algorithm is a triangle (or, if we want to\nestimate a r-dimensional manifold, a simplex with\nr+1 vertices). Given a triangle \u2206 on the boundary,\nwe extend the surface by attaching new triangles to\nits \u201cfree\u201d edges. The triangles are obtained by re-\nflecting the current triangle \u2206, or to be more precise\nby reflecting it at the \u201cfree\u201d edge. In more detail, we\ndetermine the new triangle using the following steps.\nSuppose that the current triangle has the vertices \u03b41,\n\u03b42, and \u03b43, and suppose that the edge (\u03b42, \u03b43) is a free\nedge beyond which we want to extend the surface:\n(i) A preliminary vertex \u03b4\u02dc4 is obtained by attach-\ning an equilateral triangle to the edge (\u03b42, \u03b43)\nsuch that \u03b41, \u03b42, \u03b43, and \u03b4\u02dc4 all lie on the same\nplane. Figure 10 (a) illustrates this initial step,\nthe preliminary vertex \u03b4\u02dc4 is shown in red.\n(ii) Compute \u03b44 from \u03b4\u02dc4 by carrying out a con-\nData Compression and Regression through Local Principal Curves and Surfaces\ntempg\n35.0 35.2 35.4 35.6 4.5 5.0 5.5 6.0 6.5 7.0\n4\n6\n8\n10\n12\n14\n35\n.0\n35\n.2\n35\n.4\n35\n.6\nsalg\ndepthg\n0\n10\n00\n30\n00\n50\n00\n4 6 8 10 12 14\n4.\n5\n5.\n0\n5.\n5\n6.\n0\n6.\n5\n7.\n0\n0 1000 2000 3000 4000 5000\noxyg\nFig. 7. Scatterplot matrix of the pre-standardized oceanographic data. Salinity is mea-\nsured according to the PSS (practical salinity scale) as the ratio of the electrical conduc-\ntivity against a standard solution; oxygen in millilitres per litre of water; temperature in\ndegrees Celsius; and depth in metres. Variables are suffixed with the letter g for conve-\nnience of coding.\nstrained mean shift which enforces that the\ntriangle with vertices \u03b42, \u03b43, and \u03b44 is equilat-\neral. Figure 10 (b) shows the weights of the ob-\nservations (darker grey corresponds to higher\nweights) in the mean shift, with the circle in\nFigure 10 (c) representing the constraint. The\nnewly obtained vertex \u03b44 is shown in purple.\nThe use of an angle penalty8 can be beneficial\nin this step.\n(iii) The newly-created triangle is dismissed if the\nDelaunay condition is violated, which is the\ncase if an already existing vertex lies in the\ncircumsphere of the newly created triangle or\nif the new vertex \u03b44 lies in the circumsphere of\nan existing triangle. In the former case \u03b44 is\nreplaced by the already existing offending ver-\ntex. Figure 10 (d) illustrates this check. The\nnewly-created triangle is also dismissed if the\nnew vertex falls into a region of small density.\nStep (iii) is an important ingredient of the algo-\nrithm, as these checks make sure that the branching\ntriangles \u201cmeet\u201d again and form a single surface in-\nstead of many parallel surfaces. Checking the den-\nsity at the new vertex \u03b44 is the only stopping crite-\nrion used by the algorithm and ensures that the algo-\nrithm does not extend in directions in which there is\nonly little, or even no data. Enforcing the Delaunay\ncondition can occasionally yield to neighbouring tri-\nangles not being connected. Thus a post-processing\nstep is used to connect neighbouring triangles with\nfree edges, which are not already connected. These\nData Compression and Regression through Local Principal Curves and Surfaces\n34.8 35.0 35.2 35.4 35.6 35.8\n4.\n0\n4.\n5\n5.\n0\n5.\n5\n6.\n0\n6.\n5\n7.\n0\n   0\n1000\n2000\n3000\n4000\n5000\nsalg\nde\npt\nhg\nox\nyg\nFig. 8. Spline representation of LPC (green curve) through oceanographic data (grey cir-\ncles), with the latter projected onto the former (the more \u201cred\u201d the colour of the segments,\nthe higher the temperature associated to the data).\n0.0 0.5 1.0 1.5 2.0 2.5\n4\n6\n8\n10\n12\n14\nPI\nte\nm\npg\nFig. 9. Water temperature versus projection indices with local linear smoother (black\ncurve). Again, red data points correspond to higher temperatures.\nData Compression and Regression through Local Principal Curves and Surfaces\n(a) The preliminary vertex \u03b4\u02dc4 (red) is obtained by extending\nthe current triangle (blue).\n(b) The observations are weighted based on their distance to\nthe preliminary vertex \u03b4\u02dc4.\n(c) The vertex \u03b44 (purple) is obtained using a constrained mean-\nshift.\n(d) Checking of the Delaunay condition.\nFig. 10. Illustration of the local principal surface algorithm for a three-dimensional toy\nproblem.\ntriangles are then not necessarily equilateral.\nThe algorithm is initialized like the local princi-\npal curve algorithm. The first two local principal\ncomponents are computed based on a (manually or\nrandomly chosen) starting value x0. The initial tri-\nangle is placed in the plane spanned by the first two\nlocal principal components. We now apply this algo-\nrithm to the oceanographic data. The fitted surface\nis shown in figure 11: it nicely captures the shape of\nthe data cloud.\nTo demonstrate how powerful the information\ncontained in the surface is, we combine it with a very\nsimple local kernel regression with a discrete bivari-\nate kernel. More precisely, for each pair of triangles\nwe define the (discrete) \u201cdistance\u201d d between them\nas the smallest number of triangle borders that need\nto be crossed to proceed from one triangle on the\nsurface to the other one. This distance is cheap to\ncompute and can for example be obtained by apply-\ning Dijkstra\u2019s algorithm to the neighborhood graph.\nIn order to assign local weights, we define the dis-\ncrete distance-based kernel \u03ba(d) = e\u2212d\/\u03bb, where \u03bb is\na smoothing parameter. Important special cases are\n\u03bb = 0, in which case \u03ba(0) = 1 and \u03ba(d) = 0 for d > 1,\ni.e. no smoothing at all, and \u03bb \u2212\u2192\u221e, in which case\n\u03ba(d) = 1 for all d \u2265 0, i.e. the estimated response\nfunction is constant.\nThe smoothed response value y\u02c6\u2206 on triangle \u2206 is\nthen given by\ny\u02c6\u2206 =\n\u2211\n\u2206\u2032 \u03ba(d\u2206,\u2206\u2032)y\u00af\u2206\u2032\u2211\n\u2206\u2032 \u03ba(d\u2206,\u2206\u2032)\n,\nwhere y\u00af\u2206\u2032 is the mean of all observations for which\n\u2206\u2032 is the closest triangle, and d\u2206,\u2206\u2032 is the discrete\ndistance between the triangles \u2206 and \u2206\u2032.\nData Compression and Regression through Local Principal Curves and Surfaces\nFig. 11. LPS for the oceanographic data.\n4 6 8 10 12 14\n2\n4\n6\n8\n10\n12\nLPC\ntemperature\nfit\nte\nd\n4 6 8 10 12 14\n4\n6\n8\n10\n12\nLPS\ntemperature\nfit\nte\nd\nFig. 12. Top: LPC-based fitted values vs. true\ntemperature values; bottom: LPS-based fitted values\nvs. true temperature values.\nThis model is admittedly rather crude, but has\nthe advantage that it does not require finding a\nparametrization of the fitted local principal surface.\nWe will however see that, despite its simplicity, this\nmodel allows us to improve our predictions obtained\nusing the LPC algorithm.\nFor the oceanographic data, we obtain the LPS\nshown in figure 11, which features 177 triangles with\nan average count of 3.63 data points per triangle. We\ncompute the fitted values as outlined above and plot\nthem versus the true temperatures in figure 12. It\nis clearly seen that, when using the projections onto\nthe LPS, the inconvenient branched structure which\nwas observed for the LPCs disappears.\nWe also fitted the surface for the stellar tempera-\nture data with smoothing parameters \u03bb = 0.1, \u03bb = 1,\n\u03bb = 10. The result is provided in Table 1. The mes-\nsage to be taken from this is that the prediction error\ndoes improve (compared to the LPC-based method)\nwhen accounting for the two-dimensional nature of\nthe shape of the data. However this new technique\nis sensitive to the choice of the smoothing parameter\n\u03bb. For too small \u03bb, overfitting is inevitably present.\nThis can be alleviated by increasing \u03bb, which de-\ncreases effectively the degrees of freedom used for the\nregression fit. Note that, for the data at hand, over-\nfitting does not seem to constitute much of a prob-\nlem since the average test errors are even for small\nsmoothing parameters almost of the same magnitude\nas the average training errors.\n5. Conclusion\nIn this article we have presented a novel approach\nto regression based on exploiting the structure of the\ncovariate space by fitting a local principal curve or\nsurface to the covariate space. The data examples\nstudied showed that such a strategy can be very suc-\ncessful. In all the examples the method based on\nlocal principal curves and surfaces outperformed the\ncompeting methods.\nHowever this does not always need to be the case.\nFirstly, the data might not exhibit a manifold struc-\nture at all. But even if the data lies to a large extent\non a low-dimensional structure, it might be that the\ninformation relevant to the response variable of in-\nterest is not represented in the manifold structure.\nData Compression and Regression through Local Principal Curves and Surfaces\nFrom this point of view local principal curves and\nsurfaces are no different to principal components.\nFor instance, when replacing the \u201cstrong\u201d response\nvariable temperature by the \u201cweak\u201d variable metal-\nlicity in the Gaia example, all methods considered in\nthis paper give relatively poor results, with values of\nR2 around 0.2. This is simply a very hard estima-\ntion problem and any form of dimension reduction\ncannot do much about this. An entirely different ap-\nproach to this problem based on forward modelling\nwas recently provided by Bailer-Jones.14\nWe conclude with pointing out a connection to\nthe elastic net algorithm of Gorban and Zinovyev.19\nBoth the local principal curve algorithm and the lo-\ncal principal surface algorithm cannot update the lo-\ncation of an already created line segment or trian-\ngle. However one can view both the local principal\ncurve and the edges of the local principal surface as\nsome sort of elastic net and thus postprocess the esti-\nmated curve or surface with the elastic net algorithm.\nThis could be beneficial in order to smooth out mi-\nnor irregularities on the fitted surface as visible for\ninstance in the bottom of figure 11. Furthermore\nthis allows for estimating the local principal curve or\nmanifold in a low-dimensional \u201cpilot\u201d space and us-\ning the elastic net algorithm for embedding the curve\nor surface in the original data space.\nAcknowledgments\nWe are grateful to Coryn Bailer-Jones, MPIA Hei-\ndelberg, for providing the Gaia data and explaining\ntheir astrophysical context.\nReferences\n1. C.A.L. Bailer-Jones. Determination of stellar param-\neters with GAIA. Astrophysics and Space Science,\n280:21\u201329, 2002.\n2. C.A.L. Bailer-Jones, K.W. Smith, C. Tiede, R. Sordo,\nand A. Vallenari, Finding rare objects and building\npure samples: Probabilistic quasar classification from\nlow resolution Gaia spectra. Monthly Notices of the\nRoyal Astronomical Society, 391:1838-1853, 2008.\n3. J. Fan and I. Gijbels. Local Polynomial Modelling and\nits Applications. Chapman and Hall, London, 1996.\n4. N. Keeratipranon, F. Maire, and H. Huang. Manifold\nlearning for robot navigation. International Journal\nof Neural Systems, 16: 383-392, 2006.\n5. K. Li. Sliced inverse regression. J. Amer. Statist. As-\nsoc., 86:316\u2013327, 1991.\n6. E. Bura and R. D. Cook. Estimating the structural\ndimension of regressions via parametric inverse regres-\nsion. Journal of the Royal Statistical Society, Series\nB, 63:393\u2013410, 2001.\n7. T. Hastie and W. Stuetzle. Principal curves.\nJ. Amer. Statist. Assoc., 84:502\u2013516, 1989.\n8. J. Einbeck, G. Tutz, and L. Evers. Local principal\ncurves. Statistics and Computing, 15:301\u2013313, 2005.\n9. F. Camastra. Data dimensionality estimation meth-\nods: a survey. Pattern recognition, 36:2945\u20132954,\n2003.\n10. R.D. Cook. Regression Graphics \u2014 Ideas for studying\nregressions through graphics. Wiley, New York, 1998.\n11. P. Delicado. Another look at principal curves and\nsurfaces. Journal of Multivariate Analysis, 77:84\u2013116,\n2001.\n12. J. Einbeck, G. Tutz, and L. Evers. Exploring multi-\nvariate data structures with local principal curves. In\nC. Weihs and W. Gaul, editors, Classification - The\nUbiquitous Challenge, pages 257\u2013263, Springer, Hei-\ndelberg, 2005.\n13. S. Ming-Ming, Y. Jian, L. Chuan-Cai, and Y. Jing-\nYu. Similarity preserving principal curve: an optimal\none-dimensional feature extractor for data represen-\ntation. IEEE Transactions on Neural Networks, to\nappear, 2010.\n14. C.A.L. Bailer-Jones. The ILIUM forward modelling\nalgorithm for multivariate parameter. Monthly No-\ntices of the Royal Astronomical Society, to appear,\n2010.\n15. J. Einbeck, L. Evers, and K. Hinchliff. Data compres-\nsion and regression based on local principal curves.\nIn A. Fink, B. Lausen, W. Seidel, and A. Ultsch, ed-\nitors, Advances in Data Analysis, Data Handling and\nBusiness Intelligence, pages 701\u2013712, Springer, Hei-\ndelberg, 2009.\n16. T. Boyer, J. Antonov, H. Garcia, D. Johnsonn, R. Lo-\ncarnini, A. Mishonov, M. Pitcher, O. Baranova, and\nI. Smolyar. (2006). World ocean database 2005. In\nNOAA Atlas NESDIS 60. Washington, D.C.\n17. T. Garrison. Essentials of Oceanography (Fifth Edi-\ntion). Brooks\/Cole, Belmont, Canada, 2009.\n18. D. Comaniciu and P. Meer. Mean shift: A robust ap-\nproach toward feature space analysis. IEEE Trans.\nPattern Anal. Machine Intell., 24:603\u2013619, 2002.\n19. A.N. Gorban and A.Y. Zinovyev. Elastic principal\ngraphs and manifolds and their practical applications.\nComputing, 75:359\u2013379, 2005.\n"}