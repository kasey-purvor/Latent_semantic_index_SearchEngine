{"doi":"10.1214\/10-AOS819","coreId":"217069","oai":"oai:eprints.lse.ac.uk:31709","identifiers":["oai:eprints.lse.ac.uk:31709","10.1214\/10-AOS819"],"title":"Identifying the finite dimensionality of curve time series","authors":["Bathia, Neil","Yao, Qiwei","Ziegelmann, Flavio"],"enrichments":{"references":[{"id":17328484,"title":"Common functional principal components.","authors":[],"date":"2009","doi":null,"raw":"BENKO,M . ,H ARDLE,W .a n dK NEIP, A. (2009). Common functional principal components. Ann. Statist. 37 1\u201334. MR24883433386 N. BATHIA, Q. YAO AND F. ZIEGELMANN BESSE,P .a n dR AMSAY, J. O. (1986). Principal components analysis of sampled functions. Psychometrika 51 285\u2013311. MR0848110 BOSQ, D. (2000). Linear Processes in Function Spaces. Springer, New York. MR1783138 DAUXOIS,J . ,P OUSSE,A .a n dR OMAIN, Y. (1982). Asymptotic theory for the principal component analysis of a vector random function: Some applications to statistical inference. J. Multivariate Anal. 12 136\u2013154. MR0650934 DUNFORD,N .a n dS CHWARTZ, J. T. (1988). Linear Operators. Wiley, New York.","cites":null},{"id":17328486,"title":"HALL,P .a n dV IAL,","authors":[],"date":"2006","doi":null,"raw":"HALL,P .a n dV IAL, C. (2006). Assessing the \ufb01nite dimensionality of functional data. J. R. Stat. Soc.","cites":null},{"id":17328485,"title":"Nonparametric Functional Data Analysis.","authors":[],"date":"2006","doi":null,"raw":"FERRATY,F .a n dV IEU, P. (2006). Nonparametric Functional Data Analysis. Springer, New York. MR2229687 GUILLAS,S .a n dL AI, M. J. (2010). Bivariate splines for spatial functional regression models. J. Nonparametr. Stat. 22 477\u2013497.","cites":null},{"id":17328487,"title":"Robust forecasting of mortality and fertility rates: A functional data approach.","authors":[],"date":"2007","doi":"10.1016\/j.csda.2006.07.028","raw":"Ser. B Stat. Methodol. 68 689\u2013705. MR2301015 HYNDMAN,R .J .a n dU LLAH, M. S. (2007). Robust forecasting of mortality and fertility rates: A functional data approach. Comput. Statist. Data Anal. 51 4942\u20134956. MR2364551 LEE, A. J. (1990). U-Statistics. Dekker, New York. MR1075417 LI,W .K .a n dM CLEOD, A. I. (1981). Distribution of the residual autocorrelations in multivariate ARMA time series models. J. Roy. Statist. Soc. Ser. B 43 231\u2013239. MR0626770 KNEIP,A .a n dU TIKAL, K. J. (2001). Inference for density families using functional principal component analysis (with discussion). J. Amer. Statist. Assoc. 96 519\u2013542. MR1946423 MAS,A .a n dM ENNETEAU, L. (2003). Perturbation approach applied to the asymptotic study of random operators. In High Dimensional Probability III (J. Hoffmann-Jorgensen, M. B. Marcus and J. A. Wellner, eds.) 127\u2013134. Birkh\u00e4user, Boston. MR2033885 PAN,J .a n dY AO, Q. (2008). Modelling multiple time series via common factors. Biometrika 95 365\u2013379. MR2521589 PE\u00d1A,D .a n dB OX, G. E. P. (1987). Identifying a simplifying structure in time series. J. Amer.","cites":null},{"id":17328488,"title":"Some tools for functional data analysis (with","authors":[],"date":"1991","doi":null,"raw":"Statist. Assoc. 82 836\u2013843. MR0909990 RAMSAY,J .O .a n dD ALZELL, C. J. (1991). Some tools for functional data analysis (with discussion). J. Roy. Statist. Soc. Ser. B 53 539\u2013572. MR1125714 RAMSAY,J .O .a n dS ILVERMAN, B. W. (2005). Functional Data Analysis. Springer, New York. MR2168993 RICE,J .A .a n dS ILVERMAN, B. W. (1991). Estimating the mean and covariance structure nonparametrically when the data are curves. J. Roy. Statist. Soc. Ser. B 53 233\u2013243. MR1094283 SEN, P. K. (1972). Limiting behavior of regular functionals of empirical distributions for stationary \u2217-mixing processes. Probab. Theory Related Fields 25 71\u201382. MR0329003 TIAO,G .C .a n dT SAY, R. S. (1989). Model speci\ufb01cation in multivariate time series (with discussion). J. Roy. Statist. Soc. Ser. B 51 157\u2013213. MR1007452 N. BATHIA Q. YAO DEPARTMENT OF STATISTICS LONDON SCHOOL OF ECONOMICS LONDON,W C 2 A2 A E UNITED KINGDOM E-MAIL: n.bathia@lse.ac.uk q.yao@lse.ac.uk F. ZIEGELMANN DEPARTMENT OF STATISTICS FEDERAL UNIVERSITY OF RIO GRANDE DO SUL 91509-900 PORTO ALEGRE BRAZIL E-MAIL: \ufb02avioaz@mat.ufrgs.br","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2010","abstract":"The curve time series framework provides a convenient vehicle to accommodate some nonstationary features into a stationary setup. We propose a new method to identify the dimensionality of curve time series based on the dynamical dependence across different curves. The practical implementation of our method boils down to an eigenanalysis of a finite-dimensional matrix. Furthermore, the determination of the dimensionality is equivalent to the identification of the nonzero eigenvalues of the matrix, which we carry out in terms of some bootstrap tests. Asymptotic properties of the proposed method are investigated. In particular, our estimators for zero-eigenvalues enjoy the fast convergence rate n while the estimators for nonzero eigenvalues converge at the standard \u221an-rate. The proposed methodology is illustrated with both simulated and real data sets","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/217069.pdf","fullTextIdentifier":"http:\/\/eprints.lse.ac.uk\/31709\/1\/Identifying_the_finite_dimensionality_of_curve_time_series_%28LSE_RO%29.pdf","pdfHashValue":"be7b6b1fd59e195987133581fb06713aa4f87922","publisher":"Institute of Mathematical Statistics","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lse.ac.uk:31709<\/identifier><datestamp>\n      2017-10-26T10:38:52Z<\/datestamp><setSpec>\n      74797065733D4445505453:4C53452D5354<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lse.ac.uk\/31709\/<\/dc:relation><dc:title>\n        Identifying the finite dimensionality of curve time series<\/dc:title><dc:creator>\n        Bathia, Neil<\/dc:creator><dc:creator>\n        Yao, Qiwei<\/dc:creator><dc:creator>\n        Ziegelmann, Flavio<\/dc:creator><dc:subject>\n        QA Mathematics<\/dc:subject><dc:description>\n        The curve time series framework provides a convenient vehicle to accommodate some nonstationary features into a stationary setup. We propose a new method to identify the dimensionality of curve time series based on the dynamical dependence across different curves. The practical implementation of our method boils down to an eigenanalysis of a finite-dimensional matrix. Furthermore, the determination of the dimensionality is equivalent to the identification of the nonzero eigenvalues of the matrix, which we carry out in terms of some bootstrap tests. Asymptotic properties of the proposed method are investigated. In particular, our estimators for zero-eigenvalues enjoy the fast convergence rate n while the estimators for nonzero eigenvalues converge at the standard \u221an-rate. The proposed methodology is illustrated with both simulated and real data sets.<\/dc:description><dc:publisher>\n        Institute of Mathematical Statistics<\/dc:publisher><dc:date>\n        2010<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lse.ac.uk\/31709\/1\/Identifying_the_finite_dimensionality_of_curve_time_series_%28LSE_RO%29.pdf<\/dc:identifier><dc:identifier>\n          Bathia, Neil and Yao, Qiwei and Ziegelmann, Flavio  (2010) Identifying the finite dimensionality of curve time series.  Annals of Statistics, 38 (6).  pp. 3352-3386.  ISSN 0090-5364     <\/dc:identifier><dc:relation>\n        http:\/\/www.imstat.org\/aos\/<\/dc:relation><dc:relation>\n        10.1214\/10-AOS819<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lse.ac.uk\/31709\/","http:\/\/www.imstat.org\/aos\/","10.1214\/10-AOS819"],"year":2010,"topics":["QA Mathematics"],"subject":["Article","PeerReviewed"],"fullText":"  \nNEIL BATHIA, QIWEI YAO AND FLAVIO ZIEGELMANN \nIdentifying the finite dimensionality of curve \ntime series \n \nArticle (Published version) \n(Refereed) \n \n \nOriginal citation: \nBathia, Neil and Yao, Qiwei and Ziegelmann, Flavio (2010) Identifying the finite dimensionality of \ncurve time series. The annals of statistics, 38 (6). pp. 3352-3386. ISSN 0090-5364 \n \nDOI: 10.1214\/10-AOS819  \n \n\u00a9 2010 Institute of Mathematical Statistics \n \nThis version available at: http:\/\/eprints.lse.ac.uk\/31709\/ \nAvailable in LSE Research Online: November 2011 \n \nLSE has developed LSE Research Online so that users may access research output of the \nSchool. Copyright \u00a9 and Moral Rights for the papers on this site are retained by the individual \nauthors and\/or other copyright owners. Users may download and\/or print one copy of any \narticle(s) in LSE Research Online to facilitate their private study or for non-commercial research. \nYou may not engage in further distribution of the material or use it for any profit-making activities \nor any commercial gain. You may freely distribute the URL (http:\/\/eprints.lse.ac.uk) of the LSE \nResearch Online website.  \nThe Annals of Statistics\n2010, Vol. 38, No. 6, 3352\u20133386\nDOI: 10.1214\/10-AOS819\n\u00a9 Institute of Mathematical Statistics, 2010\nIDENTIFYING THE FINITE DIMENSIONALITY\nOF CURVE TIME SERIES1\nBY NEIL BATHIA, QIWEI YAO AND FLAVIO ZIEGELMANN\nLondon School of Economics, London School of Economics and\nFederal University of Rio Grande do Sul\nThe curve time series framework provides a convenient vehicle to ac-\ncommodate some nonstationary features into a stationary setup. We propose\na new method to identify the dimensionality of curve time series based on\nthe dynamical dependence across different curves. The practical implemen-\ntation of our method boils down to an eigenanalysis of a finite-dimensional\nmatrix. Furthermore, the determination of the dimensionality is equivalent to\nthe identification of the nonzero eigenvalues of the matrix, which we carry\nout in terms of some bootstrap tests. Asymptotic properties of the proposed\nmethod are investigated. In particular, our estimators for zero-eigenvalues en-\njoy the fast convergence rate n while the estimators for nonzero eigenvalues\nconverge at the standard\n\u221a\nn-rate. The proposed methodology is illustrated\nwith both simulated and real data sets.\n1. Introduction. A curve time series may consist of, for example, annual\nweather record charts, annual production charts or daily volatility curves (from\nmorning to evening). In these examples, the curves are segments of a single long\ntime series. One advantage to view them as a curve series is to accommodate some\nnonstationary features (such as seasonal cycles or diurnal volatility patterns) into\na stationary framework in a Hilbert space. There are other types of curve series\nthat cannot be pieced together into a single long time series; for example, daily\nmean-variance efficient frontiers of portfolios, yield curves and intraday asset re-\nturn distributions. See also an example of daily return density curves in Section 4.2.\nThe goal of this paper is to identify the finite dimensionality of curve time series\nin the sense that the serial dependence across different curves is driven by a finite\nnumber of scalar components. Therefore, the problem of modeling curve dynamics\nis reduced to that of modeling a finite-dimensional vector time series.\nThroughout this paper, we assume that the observed curve time series, which\nwe denote by Y1(\u00b7), . . . , Yn(\u00b7), are defined on a compact interval I and are subject\nto errors in the sense that\nYt (u) = Xt(u)+ \u03b5t (u), u \u2208 I,(1.1)\nReceived October 2009; revised February 2010.\n1Supported in part by the Engineering and Physical Sciences Research Council of the UK.\nAMS 2000 subject classifications. Primary 62M10, 62H30; secondary 60G99.\nKey words and phrases. Autocovariance, curve time series, dimension reduction, eigenanalysis,\nKarhunen\u2013Lo\u00e9ve expansion, n convergence rate, root-n convergence rate.\n3352\nCURVE TIME SERIES 3353\nwhere Xt(\u00b7) is the curve process of interest. The existence of the noise term \u03b5t (\u00b7)\nreflects the fact that curves Xt(\u00b7) are seldom perfectly observed. They are often\nonly recorded on discrete grids and are subject to both experimental error and\nnumerical rounding. These noisy discrete data are smoothed to yield \u201cobserved\u201d\ncurves Yt (\u00b7). Note that both Xt(\u00b7) and \u03b5t (\u00b7) are unobservable.\nWe assume that \u03b5t (\u00b7) is a white noise sequence in the sense that E{\u03b5t (u)} = 0 for\nall t and Cov{\u03b5t (u), \u03b5s(v)} = 0 for any u, v \u2208 I provided t \u0004= s. This is guaranteed\nsince we may include all the dynamic elements of Yt (\u00b7) into Xt(\u00b7). Likewise, we\nmay also assume that no parts of Xt(\u00b7) are white noise since these parts should be\nabsorbed into \u03b5t (\u00b7). We also assume that\u222b\nI\nE{Xt(u)2 + \u03b5t (u)2}du < \u221e,(1.2)\nand both\n\u03bc(u) \u2261 E{Xt(u)}, Mk(u, v) \u2261 Cov{Xt(u),Xt+k(v)}(1.3)\ndo not depend on t . Furthermore, we assume that Xt(\u00b7) and \u03b5t+k(\u00b7) are uncorre-\nlated for all integer k. Under condition (1.2), Xt(\u00b7) admits the Karhunen\u2013Lo\u00e9ve\nexpansion\nXt(u)\u2212\u03bc(u) =\n\u221e\u2211\nj=1\n\u03betj\u03d5j (u),(1.4)\nwhere \u03betj = \u222bI{Xt(u)\u2212\u03bc(u)}\u03d5j (u) du with {\u03betj , j \u2265 1} being a sequence of scalar\nrandom variables with E(\u03betj ) = 0, Var(\u03betj ) = \u03bbj and Cov(\u03beti , \u03betj ) = 0 if i \u0004= j . We\nrank {\u03betj , j \u2265 1} such that \u03bbj is monotonically decreasing as j increases.\nWe say that Xt(\u00b7) is d-dimensional if \u03bbd \u0004= 0 and \u03bbd+1 = 0, where d \u2265 1 is\na finite integer; see Hall and Vial (2006). The primary goal of this paper is to\nidentify d and to estimate the dynamic space M spanned by the (deterministic)\neigenfunctions \u03d51(\u00b7), . . . , \u03d5d(\u00b7).\nHall and Vial (2006) tackle this problem under the assumption that the curves\nY1(\u00b7), . . . , Yn(\u00b7) are independent. Then the problem is insoluble in the sense that\none cannot separate Xt(\u00b7) from \u03b5t (\u00b7) in (1.1). This difficulty was resolved in Hall\nand Vial (2006) under a \u201clow noise\u201d setting which assumes that the noise \u03b5t (\u00b7)\ngoes to 0 as the sample size goes to infinity. Our approach is different and it\ndoes not require the \u201clow noise\u201d condition, since we identify d and M in terms\nof the serial dependence of the curves. Our method relies on a simple fact that\nMk(u, v) = Cov{Yt (u), Yt+k(v)} for any k \u0004= 0, which automatically filters out the\nnoise \u03b5t (\u00b7); see (1.3). In this sense, the existence of dynamic dependence across\ndifferent curves makes the problem tractable.\nDimension reduction plays an important role in functional data analysis. The\nmost frequently used method is the functional principal component analysis in\nthe form of applying the Karhunen\u2013Lo\u00e9ve decomposition directly to the observed\n3354 N. BATHIA, Q. YAO AND F. ZIEGELMANN\ncurves. The literature in this field is vast and includes Besse and Ramsay (1986),\nDauxois, Pousse and Romain (1982), Ramsay and Dalzell (1991), Rice and Sil-\nverman (1991) and Ramsay and Silverman (2005). In spite of the methodological\nadvancements with independent observations, the work on functional time series\nhas been of a more theoretical nature; see, for example, Bosq (2000). The available\ninference methods focus mostly on nonparametric estimation for some character-\nistics of functional series [Part IV of Ferraty and Vieu (2006)]. As far as we are\naware, the work presented here represents the first attempt on the dimension reduc-\ntion based on dynamic dependence, which is radically different from the existing\nmethods. Heuristically, our approach differs from functional principal components\nanalysis in one fundamental manner; in principal component analysis the objec-\ntive is to find the linear combinations of the data which maximize variance. In\ncontrast, we seek for the linear combinations of the data which represent the serial\ndependence in the data. Although we confine ourselves to square integrable curve\nseries in this paper, the methodology may be extended to a more general functional\nframework including, for example, a surface series which is particularly important\nfor environmental study; see, for example, Guillas and Lai (2010). A follow-up\nstudy in this direction will be reported elsewhere.\nThe rest of the paper is organized as follows. Section 2 introduces the pro-\nposed new methodology for identifying the finite-dimensional dynamic structure.\nAlthough the Karhunen\u2013Lo\u00e9ve decomposition (1.4) serves as a starting point, we\ndo not seek for such a decomposition explicitly. Instead the eigenanalysis is per-\nformed on a positive-definite operator defined based on the autocovariance func-\ntion of the curve process. Furthermore, computationally our method boils down\nto an eigenanalysis of a finite matrix thus requiring no computing of eigenfunc-\ntions in a functional space directly. The relevant theoretical results are presented\nin Section 3. As our estimation for the eigenvalues are essentially quadratic, the\nconvergence rate of the estimators for the zero-eigenvalues is n while that for the\nnonzero eigenvalues is standard\n\u221a\nn. Numerical illustration using both simulated\nand real datasets is provided in Section 4. Given the nature of the subject con-\ncerned, it is inevitable to make use of some operator theory in a Hilbert space. We\ncollect some relevant facts in Appendix A. We relegate all the technical proofs to\nAppendix B.\n2. Methodology.\n2.1. Characterize d and M via serial dependence. Let L2(I) denote the\nHilbert space consisting of all the square integrable curves defined on I equipped\nwith the inner product\n\u3008f,g\u3009 =\n\u222b\nI\nf (u)g(u)du, f, g \u2208 L2(I).(2.1)\nNow Mk defined in (1.3) may be viewed as the kernel of a linear operator acting\non L2(I), that is, for any g \u2208 L2(I), Mk maps g(u) to g\u02c7(u) \u2261 \u222bI Mk(u, v)g(v) dv.\nCURVE TIME SERIES 3355\nFor notational economy, we will use Mk to denote both the kernel and the operator.\nAppendix A lists some relevant facts about operators in Hilbert spaces.\nFor M0 defined in (1.3), we have a spectral decomposition of the form\nM0(u, v) =\n\u221e\u2211\nj=1\n\u03bbj\u03d5j (u)\u03d5j (v), u, v \u2208 I,(2.2)\nwhere \u03bb1 \u2265 \u03bb2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 0 are the eigenvalues and \u03d51, \u03d52, . . . are the corresponding\northonormal eigenfunctions (i.e., \u3008\u03d5i, \u03d5j \u3009 = 1 for i = j , and 0 otherwise). Hence,\u222b\nI\nM0(u, v)\u03d5j (v) dv = \u03bbj\u03d5j (u), j \u2265 1.\nFurthermore, the random curves Xt(\u00b7) admit the representation (1.4). We assume\nin this paper that Xt(\u00b7) is d-dimensional (i.e., \u03bbd+1 = 0). Therefore,\nM0(u, v) =\nd\u2211\nj=1\n\u03bbj\u03d5j (u)\u03d5j (v), Xt(u) = \u03bc(u)+\nd\u2211\nj=1\n\u03betj\u03d5j (u).(2.3)\nIt follows from (1.1) that\nYt (u) = \u03bc(u)+\nd\u2211\nj=1\n\u03betj\u03d5j (u)+ \u03b5t (u).(2.4)\nThus, the serial dependence of Yt (\u00b7) is determined entirely by that of the d-\nvector process \u03be t \u2261 (\u03bet1, . . . , \u03betd)\u2032 since \u03b5t (\u00b7) is white noise. By the virtue of the\nKarhunen\u2013Lo\u00e9ve decomposition, E\u03be t = 0 and Var(\u03be t ) = diag(\u03bb1, . . . , \u03bbd).\nFor some prescribed integer p, let\nM\u0302k(u, v) = 1\nn\u2212 p\nn\u2212p\u2211\nj=1\n{Yj (u)\u2212 Y\u00af (u)}{Yj+k(v)\u2212 Y\u00af (v)},(2.5)\nwhere Y\u00af (\u00b7) = n\u22121\u22111\u2264j\u2264n Yj (\u00b7) and k = 1, . . . , p. The reason for truncating\nthe sums in (2.5) at n \u2212 p as opposed to n \u2212 k is to ensure a duality op-\neration which simplifies the computation for eigenfunctions; see Remark 2 at\nthe end of Section 2.2.2. The conventional approach to estimate d and M =\nspan{\u03d51(\u00b7), . . . , \u03d5d(\u00b7)} is to perform an eigenanalysis on M\u03020 and let d\u0302 be the num-\nber of nonzero eigenvalues and M\u0302 be spanned by the d\u0302 corresponding eigenfunc-\ntions; see, for example, Ramsay and Silverman (2005) and references therein.\nHowever, this approach suffers from complications due to fact that M\u03020 is not a\nconsistent estimator for M0, as Cov{Yt (u), Yt (v)} = M0(u, v)+Cov{\u03b5t (u), \u03b5t (v)}.\nTherefore, M\u03020 needs to be adjusted to remove the part due to \u03b5t (\u00b7) before the eige-\nnanalysis may be performed. Unfortunately, this is a nontrivial matter since both\nXt(\u00b7) and \u03b5t (\u00b7) are unobservable. An alternative is to let the variance of \u03b5t (\u00b7) decay\nto 0 as the sample size n goes to infinity; see Hall and Vial (2006).\n3356 N. BATHIA, Q. YAO AND F. ZIEGELMANN\nWe adopt a different approach based on the fact that Cov{Yt (u), Yt+k(v)} =\nMk(u, v) for any k \u0004= 0, which ensures that M\u0302k is a legitimate estimator for Mk ;\nsee (1.3) and (2.5).\nLet \u0003k = E(\u03be t\u03be \u2032t+k) \u2261 (\u03c3 (k)ij ) be the autocovariance matrix of \u03be t at lag k. It is\neasy to see from (1.3) and (2.3) that Mk(u, v) =\u2211di,j=1 \u03c3 (k)ij \u03d5i(u)\u03d5j (v). Define a\nnonnegative operator\nNk(u, v) =\n\u222b\nI\nMk(u, z)Mk(v, z) dz =\nd\u2211\ni,j=1\nw\n(k)\nij \u03d5i(u)\u03d5j (v),(2.6)\nwhere Wk = (w(k)ij ) = \u0003k\u0003\u2032k is a nonnegative definite matrix. Then it holds for any\ninteger k that \u222b\nI\nNk(u, v)\u03b6(v) dv = 0 for any \u03b6(\u00b7) \u2208 M\u22a5,(2.7)\nwhere M\u22a5 denotes the orthogonal complement of M in L2(I). Note (2.7) also\nholds if we replace Nk by the operator\nK(u, v) =\np\u2211\nk=1\nNk(u, v),(2.8)\nwhich is also a nonnegative operator on L2(I).\nPROPOSITION 1. Let the matrix \u0003k0 be full-ranked for some k0 \u2265 1. Then the\nassertions below hold.\n(i) The operator Nk0 has exactly d nonzero eigenvalues, and M is the linear\nspace spanned by the corresponding d eigenfunctions.\n(ii) For p \u2265 k0, (i) also holds for the operator K .\nREMARK 1. (i) The condition that rank(\u0003k) = d for some k \u2265 1 is implied\nby the assumption that Xt(\u00b7) is d-dimensional. In the case where rank(\u0003k) < d for\nall k, the component with no serial correlations in Xt(\u00b7) should be absorbed into\nwhite noise \u03b5t (\u00b7); see similar arguments on modeling vector time series in Pe\u00f1a\nand Box (1987) and Pan and Yao (2008).\n(ii) The introduction of the operator K in (2.8) is to pull together the informa-\ntion at different lags. Using single Nk may lead to spurious choices of d\u0302 .\n(iii) Note that \u222bI K(u, v)\u03b6(v) dv = 0 if and only if \u222bI Nk(u, v)\u03b6(v) dv = 0 for\nall 1 \u2264 k \u2264 p. However, we cannot use Mk directly in defining K since it does not\nnecessarily hold that\n\u222b\nI\n\u2211\n1\u2264k\u2264p Mk(u, v)g(v) \u0004= 0 for all g \u2208 M. This is due to\nthe fact that Mk are not nonnegative definite operators.\n2.2. Estimation of d and M.\n2.2.1. Estimators and fitted dynamic models. Let \u03c81, . . . ,\u03c8d be the orthonor-\nmal eigenfunctions of K corresponding to its d nonzero eigenvalues. Then they\nCURVE TIME SERIES 3357\nform an orthonormal basis of M; see Proposition 1(ii) above. Hence, it holds that\nXt(u)\u2212\u03bc(u) =\nd\u2211\nj=1\n\u03betj\u03d5j (u) =\nd\u2211\nj=1\n\u03b7tj\u03c8j (u),\nwhere \u03b7tj = \u222bI{Xt(u) \u2212 \u03bc(u)}\u03c8j(u)du. Therefore, the serial dependence of\nXt(\u00b7) [and also that of Yt (\u00b7)] can be represented by that of the d-vector process\n\u03b7t \u2261 (\u03b7t1, . . . , \u03b7td)\u2032. Since (\u03betj , \u03d5j ) cannot be estimated directly from Yt (see Sec-\ntion 2.1 above), we estimate (\u03b7tj ,\u03c8j ) instead.\nAs we have stated above, Mk for k \u0004= 0 may be directly estimated from the\nobserved curves Yt ; see (2.5). Hence, a natural estimator for K may be defined as\nK\u0302(u, v) =\np\u2211\nk=1\n\u222b\nI\nM\u0302k(u, z)M\u0302k(v, z) dz\n= 1\n(n\u2212 p)2\nn\u2212p\u2211\nt,s=1\np\u2211\nk=1\n{Yt (u)\u2212 Y\u00af (u)}{Ys(v)\u2212 Y\u00af (v)}(2.9)\n\u00d7 \u3008Yt+k \u2212 Y\u00af , Ys+k \u2212 Y\u00af \u3009,\nsee (2.8), (2.6), (2.5) and (2.1).\nBy Proposition 1, we define d\u0302 to be the number of nonzero eigenvalues of K\u0302 (see\nSection 2.2.3 below) and M\u0302 to be the linear space spanned by the d\u0302 corresponding\northonormal eigenfunctions \u03c8\u03021(\u00b7), . . . , \u03c8\u0302d\u0302 (\u00b7). This leads to the fitting\nY\u0302t (u) = Y\u00af (u)+\nd\u0302\u2211\nj=1\n\u03b7\u0302tj \u03c8\u0302j (u), u \u2208 I,(2.10)\nwhere\n\u03b7\u0302tj =\n\u222b\nI\n{Yt (u)\u2212 Y\u00af (u)}\u03c8\u0302j (u) du, j = 1, . . . , d\u0302.(2.11)\nAlthough \u03c8\u0302j are not the estimators for the eigenfunctions \u03d5j of M0 defined in\n(2.2), M\u0302 = span{\u03c8\u03021(\u00b7), . . . , \u03c8\u0302d(\u00b7)} is a consistent estimator of M = span{\u03d51(\u00b7),\n. . . , \u03d5d(\u00b7)} (Theorem 2 in Section 3 below).\nIn order to model the dynamic behavior of Yt (\u00b7), we only need to model the\nd\u0302-dimensional vector process \u03b7\u0302t \u2261 (\u03b7\u0302t1, . . . , \u03b7\u0302t d\u0302 )\u2032; see (2.10) above. This may be\ndone using VARMA or any other multivariate time series models. See also Tiao\nand Tsay (1989) for applying linear transformations in order to obtain a more\nparsimonious model for \u03b7\u0302t .\nThe integer p used in (2.5) may be selected in the same spirit as the maximum\nlag used in, for example, the Ljung\u2013Box\u2013Pierce portmanteau test for white noise.\nIn practice, we often choose p to be a small positive integer. Note that k0 fulfilling\nthe condition of Proposition 1 is often small since serial dependence decays as the\nlag increases for most practical data.\n3358 N. BATHIA, Q. YAO AND F. ZIEGELMANN\n2.2.2. Eigenanalysis. To perform an eigenanalysis in a Hilbert space is not a\ntrivial matter. A popular pragmatic approach is to use an approximation via dis-\ncretization, that is, to evaluate the observed curves at a fine grid and to replace\nthe observed curves by the resulting vectors. This is an approximate method; ef-\nfectively transform the problem to an eigenanalysis for a finite matrix. See, for\nexample, Section 8.4 of Ramsay and Silverman (2005). Below we also transform\nthe problem into an eigenanalysis of a finite matrix but not via any approximations.\nInstead we make use of the well-known duality property that AB\u2032 and B\u2032A share\nthe same nonzero eigenvalues for any matrices A and B of the same sizes. Further-\nmore, if \u03b3 is an eigenvector of B\u2032A, A\u03b3 is an eigenvector of AB\u2032 with the same\neigenvalue. In fact, this duality also holds for operators in a Hilbert space. This\nscheme was adopted in Kneip and Utikal (2001) and Benko, Hardle and Kneip\n(2009).\nWe present a heuristic argument first. To view the operator K\u0302(\u00b7, \u00b7) defined in\n(2.9) in the form of AB\u2032, let us denote the curve Yt (\u00b7) \u2212 Y\u00af (\u00b7) as an \u221e \u00d7 1 vector\nYt with Y\u2032tYs = \u3008Yt \u2212 Y\u00af , Ys \u2212 Y\u00af \u3009; see (2.1). Put Yk = (Y1+k, . . . ,Yn\u2212p+k). Then\nK\u0302(\u00b7, \u00b7) may be represented as an \u221e \u00d7 \u221e matrix\nK\u0302 = 1\n(n\u2212 p)2 Y0\np\u2211\nk=1\nY \u2032kYkY \u20320.\nApplying the duality with A = Y0 and B\u2032 =\u22111\u2264k\u2264p Y \u2032kYkY \u20320, K\u0302 shares the same\nnonzero eigenvalues with the (n\u2212 p)\u00d7 (n\u2212 p) matrix\nK\u2217 = 1\n(n\u2212 p)2\np\u2211\nk=1\nY \u2032kYkY \u20320Y0,(2.12)\nwhere the (t, s)th element of Y \u2032kYk is Y\u2032t+kYs+k = \u3008Yt+k \u2212 Y\u00af , Ys+k \u2212 Y\u00af \u3009 and\nk = 0,1, . . . , p. Furthermore, let \u03b3 j = (\u03b31j , . . . , \u03b3n\u2212p,j )\u2032, j = 1, . . . , d\u0302 , be the\neigenvectors of K\u2217 corresponding to the d\u0302 largest eigenvalues. Then\nn\u2212p\u2211\nt=1\n\u03b3tj {Yt (\u00b7)\u2212 Y\u00af (\u00b7)}, j = 1, . . . , d\u0302,(2.13)\nare the d\u0302 eigenfunctions of K\u0302(\u00b7, \u00b7). Note that the functions in (2.13) may not be\northogonal with each other. Thus, the orthonormal eigenfunctions \u03c8\u03021(\u00b7), . . . , \u03c8\u0302d\u0302 (\u00b7)\nused in (2.10) may be obtained by applying a Gram\u2013Schmidt algorithm to the\nfunctions given in (2.13).\nThe heuristic argument presented above is justified by result below. The formal\nproof is relegated to Appendix B.\nPROPOSITION 2. The operator K\u0302(\u00b7, \u00b7) shares the same nonzero eigenvalues\nwith matrix K\u2217 defined in (2.12) with the corresponding eigenfunctions given in\n(2.13).\nCURVE TIME SERIES 3359\nREMARK 2. The truncation of the sums in (2.5) at (n \u2212 p) for different k is\nnecessary to ensure the applicability of the above duality operation. If we truncated\nthe sum for M\u0302k at (n\u2212 k) instead, Y \u2032kYk would be of different sizes for different k,\nand K\u2217 in (2.12) would not be well defined.\n2.2.3. Determination of d via statistical tests. Although the number of\nnonzero eigenvalues of operator K(\u00b7, \u00b7) defined in (2.8) is d [Proposition 1(ii)],\nthe number of nonzero eigenvalues of its estimator K\u0302(\u00b7, \u00b7) defined in (2.9) may\nbe much greater than d due to random fluctuation in the sample. One empirical\napproach is to take d\u0302 to be the number of \u201clarge\u201d eigenvalues of K\u0302 in the sense\nthat the (d\u0302 + 1)th largest eigenvalue drops significantly; see also Theorem 3 in\nSection 3 and Figure 1 in Section 4.1. Hyndman and Ullah (2007) proposed to\nchoose d by minimizing forecasting errors. Below, we present a bootstrap test to\ndetermine the value of d .\nLet \u03b81 \u2265 \u03b82 \u2265 \u00b7 \u00b7 \u00b7 \u2265 0 be the eigenvalues of K . If the true dimensionality is d =\nd0, we expect to reject the null hypothesis \u03b8d0 = 0, and not to reject the hypothesis\n\u03b8d0+1 = 0. Suppose we are interested in testing the null hypothesis\nH0 : \u03b8d0+1 = 0,(2.14)\nwhere d0 is a known integer, obtained, for example, by visual observation of the\nestimated eigenvalues \u03b8\u03021 \u2265 \u03b8\u03022 \u2265 \u00b7 \u00b7 \u00b7 \u2265 0 of K\u0302 . Hence, we reject H0 if \u03b8\u02c6d0+1 > l\u03b1 ,\nwhere l\u03b1 is the critical value at the \u03b1 \u2208 (0,1) significance level. To evaluate the\ncritical value l\u03b1 , we propose the following bootstrap procedure.\n1. Let Y\u0302t (\u00b7) be defined as in (2.10) with d\u0302 = d0. Let \u03b5\u0302t (\u00b7) = Yt (\u00b7)\u2212 Y\u0302t (\u00b7).\n2. Generate a bootstrap sample from the model\nY \u2217t (\u00b7) = Y\u0302t (\u00b7)+ \u03b5\u2217t (\u00b7),\nwhere \u03b5\u2217t are drawn independently (with replacement) from {\u0302\u03b51, . . . , \u03b5\u0302n}.\n3. Form an operator K\u2217 in the same manner as K\u0302 with {Yt } replaced by {Y \u2217t },\ncompute the (d0 + 1)th largest eigenvalue \u03b8\u2217d0+1 of K\u2217.\nThen the conditional distribution of \u03b8\u2217d0+1, given the observations {Y1, . . . , Yn}, is\ntaken as the distribution of \u03b8\u02c6d0+1 under H0. In practical implementation, we repeat\nSteps 2 and 3 above B times for some large integer B , and we reject H0 if the\nevent that \u03b8\u2217d0+1 > \u03b8\u02c6d0+1 occurs not more than [\u03b1B] times. The simulation results\nreported in Section 4.1 below indicate that the above bootstrap method works well.\nREMARK 3. The serial dependence in Xt could provide an alternative method\nfor testing hypothesis (2.14). Under model (2.4), the projected series of the curves\nYt (\u00b7) on any direction perpendicular to M is white noise. Put Ut = \u3008Yt , \u03c8\u0302d0+1\u3009, t =\n1, . . . , n. Then Ut would behave like a (scalar) white noise under H0. However, for\nexample, the Ljung\u2013Box\u2013Pierce portmanteau test for white noise coupled with the\n3360 N. BATHIA, Q. YAO AND F. ZIEGELMANN\nstandard \u03c72-approximation does not work well in this context. This is due to the\nfact that the (d + 1)th largest eigenvalue K\u0302 is effectively the extreme value of the\nestimates for all the zero-eigenvalues of K . Therefore, \u03c8\u0302d0+1 is not an estimate for\na fixed direction, which makes the \u03c72-approximation for the Ljung\u2013Box\u2013Pierce\nstatistic mathematically invalid. Indeed some simulation results, not reported here,\nindicate that the \u03c72-approximation tends to underestimate the critical values for\nthe Ljung\u2013Box\u2013Pierce test in this particular context.\n3. Theoretical properties. Before presenting the asymptotic results, we first\nsolidify some notation. Denote by (\u03b8j ,\u03c8j ) and (\u03b8\u0302j , \u03c8\u0302j ) the (eigenvalue, eigen-\nfunction) pairs of K and K\u0302 , respectively [see (2.8) and (2.9)]. We always arrange\nthe eigenvalues in descending order, that is, \u03b8j > \u03b8j+1. As the eigenfunctions of K\nand K\u0302 are unique only up to sign changes, in the sequel, it will go without saying\nthat the right versions are used. Furthermore, recall that \u03b8j = 0 for all j \u2265 d + 1.\nThus, the eigenfunctions \u03c8j are not identified for j \u2265 d +1. We take this last point\ninto consideration in our theory. We always assume that the dimension d \u2265 1 is a\nfixed finite integer, and p \u2265 1 is also a fixed finite integer.\nFor simplicity in the proofs, we suppose that E{Yt (\u00b7)} = \u03bc(\u00b7) is known and\nthus set Y\u00af (\u00b7) = \u03bc(\u00b7). Straightforward adjustments to our arguments can be made\nwhen this is not the case. We denote by \u2016L\u2016S the Hilbert\u2013Schmidt norm for any\noperator L; see Appendix A. Our asymptotic results are based on the following\nregularity conditions:\nC1. {Yt (\u00b7)} is strictly stationary and \u03c8-mixing with the mixing coefficient de-\nfined as\n\u03c8(l) = sup\nA\u2208F0\u221e,B\u2208F\u221el ,P (A)P (B)>0\n|1 \u2212 P(B|A)\/P (B)|,\nwhere F ji = \u03c3 {Yi(\u00b7), . . . , Yj (\u00b7)} for any j \u2265 i. In addition, it holds that\n\u2211\u221e\nl=1 l \u00d7\n\u03c81\/2(l) < \u221e.\nC2. E{\u222bI Yt (u)2 du}2 < \u221e.\nC3. \u03b81 > \u00b7 \u00b7 \u00b7 > \u03b8d > 0 = \u03b8d+1 = \u00b7 \u00b7 \u00b7 , that is, all the nonzero eigenvalues of K\nare different.\nC4. Cov{Xs(u), \u03b5t (v)} = 0 for all s, t and u, v \u2208 I .\nTHEOREM 1. Let conditions C1\u2013C4 hold. Then as n \u2192 \u221e, the following as-\nsertions hold:\n(i) \u2016K\u0302 \u2212K\u2016S = Op(n\u22121\/2).\n(ii) For j = 1, . . . , d , |\u03b8\u0302j \u2212 \u03b8j | = Op(n\u22121\/2) and(\u222b\nI\n{\u03c8\u0302j (u)\u2212\u03c8j(u)}2 du\n)1\/2\n= OP (n\u22121\/2).\nCURVE TIME SERIES 3361\n(iii) For j \u2265 d + 1, \u03b8\u0302j = Op(n\u22121).\n(iv) Let {\u03c8j : j \u2265 d + 1} be a complete orthonormal basis of M\u22a5, and put\nfj (\u00b7) =\n\u221e\u2211\ni=d+1\n\u3008\u03c8i, \u03c8\u0302j \u3009\u03c8i(\u00b7).\nThen for any j \u2265 d + 1,(\u222b\nI\n{\nd\u2211\ni=1\n\u3008\u03c8i, \u03c8\u0302j \u3009\u03c8i(u)\n}2\ndu\n)1\/2\n=\n(\u222b\nI\n{\u03c8\u0302j (u)\u2212 fj (u)}2 du\n)1\/2\n= Op(n\u22121\/2).\nREMARK 4. (a) In the above theorem, assertions (i) and (ii) are standard. (In\nfact, those results still hold for d = \u221e.)\n(b) Assertion (iv) implies that the estimated eigenfunctions \u03c8\u0302d+j , j \u2265 1, are\nasymptotically in the orthogonal complement of the dynamic space M.\n(c) The fast convergence rate n in assertion (iii) deserves some further expla-\nnation. To this end, we consider a simple analogue: let A1, . . . ,An be a sample of\nstationary random variables, and we are interested in estimating \u03bc2 = (EAt)2 for\nwhich we use the estimator A\u00af2 = (n\u22121\u2211nt=1 At)2 = n\u22122\u2211ns,t=1 AsAt . Then under\nappropriate regularity conditions, it holds that\n|A\u00af2 \u2212\u03bc2| \u2264 |\u03bc||A\u00af\u2212\u03bc| + |A\u00af2 \u2212 A\u00af\u03bc| = |\u03bc| \u00b7Op(n\u22121\/2)+Op(n\u22121)(3.1)\nas |A\u00af \u2212 \u03bc| = Op(n\u22121\/2) and |A\u00af2 \u2212 A\u00af\u03bc| = Op(n\u22121). The latter follows from a\nsimple U -statistic argument; see Lee (1990). It is easy to see from (3.1) that |A\u00af2 \u2212\n\u03bc2| = Op(n\u22121\/2) if \u03bc \u0004= 0, and |A\u00af2 \u2212\u03bc2| = Op(n\u22121) if \u03bc = 0. In our context, the\noperator K\u0302 =\u2211pk=1 \u222bI Mk(u, r)Mk(v, r) = (n\u2212 p)\u22122\u2211pk=1\u2211n\u2212ps,t=1 ZikZ\u2217jk(u, v),\nwhere Ztk(u, v) = {Yt (u)\u2212\u03bc(u)}{Yt+k(v)\u2212\u03bc(v)} and ZikZ\u2217jk(u, v) =\n\u222b\nI Zik(u,\nr)Zjk(v, r) dr , is similar to A\u00af2, and hence the convergence properties stated in\nTheorem 1(iii) [and also (ii)]. The fast convergence rate, which is termed as \u201csuper-\nconsistent\u201d in econometric literature, is illustrated via simulation in Section 4.1\nbelow; see Figures 4\u20137. It makes the identification of zero-eigenvalues easier; see\nFigure 1.\nWith d known, let M\u02dc = span{\u03c8\u03021(\u00b7), . . . , \u03c8\u0302d(\u00b7)}, where \u03c8\u03021(\u00b7), . . . , \u03c8\u0302d(\u00b7) are the\neigenfunctions of K\u0302 corresponding to the d largest eigenvalues. In order to mea-\nsure the discrepancy between M and M\u02dc, we introduce the following metric. Let\nN1 and N2 be any two d-dimensional subspaces of L2(I). Let {\u03b6i1(\u00b7), . . . , \u03b6id(\u00b7)}\nbe an orthonormal basis of Ni , i = 1,2. Then the projection of \u03b61k onto N2 may\nbe expressed as\nd\u2211\nj=1\n\u3008\u03b62j , \u03b61k\u3009\u03b62j (u).\n3362 N. BATHIA, Q. YAO AND F. ZIEGELMANN\nIts squared norm is\n\u2211d\nj=1(\u3008\u03b62j , \u03b61k\u3009)2 \u2264 1. The discrepancy measure is defined as\nD(N1,N2) =\n\u221a\u221a\u221a\u221a\u221a1 \u2212 1\nd\nd\u2211\nj,k=1\n(\u3008\u03b62j , \u03b61k\u3009)2.(3.2)\nIt is clear that this is a symmetric measure between 0 and 1. It is independent of\nthe choice of the orthonormal bases used in the definition, and it equals 0 if and\nonly if N1 = N2. Let Z be the set consisting of all the d-dimensional subspaces\nin L2(I). Then (Z,D) forms a metric space in the sense that D is a well-defined\ndistance measure on Z (see Lemma 4 in Appendix B below).\nTHEOREM 2. Let the conditions of Theorem 1 hold. Suppose that d is known.\nThen as n \u2192 \u221e, it holds that D(M\u02dc,M) = Op(n\u22121\/2).\nREMARK 5. Our estimation of M is asymptotically adaptive to d . To this\nend, let d\u0302 be a consistent estimator of d in the sense that P(d\u0302 = d) \u2192 1, and\nM\u0302 = span{\u03c8\u03021, . . . , \u03c8\u0302d\u0302} be the estimator of M with d estimated by d\u0302 . Since d\u0302 may\ndiffer from d , we use the modified metric D\u02dc, defined in (4.1) below, to measure\nthe difference between M\u0302 and M. Then it holds for any constant C > 0 that\nP {n1\/2|D\u02dc(M\u0302,M)\u2212D(M\u02dc,M)| >C}\n\u2264 P {n1\/2|D\u02dc(M\u0302,M)\u2212D(M\u02dc,M)| >C|d\u0302 = d}P(d\u0302 = d)+ P(d\u0302 \u0004= d)\n\u2264 P {n1\/2|D\u02dc(M\u0302,M)\u2212D(M\u02dc,M)| >C|d\u0302 = d} + o(1).\nNote that when d\u0302 = d , M\u0302 = M\u02dc and thus D\u02dc(M\u0302,M) = D(M\u0302,M). Hence the\nconditional probability on the RHS of the above expression is 0. This together\nwith Theorem 2 yield D\u02dc(M\u0302,M) = Op(n\u22121\/2).\nOne such consistent estimator of d may be defined as d\u0302 = #{j : \u03b8\u0302j \u2265 \u000e}, where\n\u000e = \u000e(n) > 0 satisfies the conditions in Theorem 3 below.\nTHEOREM 3. Let the conditions of Theorem 1 hold. Let \u000e \u2192 0 and \u000e2n \u2192 \u221e\nand as n \u2192 \u221e. Then P(d\u0302 \u0004= d) \u2192 0.\n4. Numerical properties.\n4.1. Simulations. We illustrate the proposed method first using the simulated\ndata from model (1.1) with\nXt(u) =\nd\u2211\ni=1\n\u03beti\u03d5i(u), \u03b5t (u) =\n10\u2211\nj=1\nZtj\n2j\u22121\n\u03b6j (u), u \u2208 [0,1],\nwhere {\u03beti , t \u2265 1} is a linear AR(1) process with the coefficient (\u22121)i(0.9 \u2212\n0.5i\/d), the innovations Ztj are independent N(0,1) variables and\n\u03d5i(u) =\n\u221a\n2 cos(\u03c0iu), \u03b6j (u) =\n\u221a\n2 sin(\u03c0ju).\nCURVE TIME SERIES 3363\nWe set sample size n = 100,300 or 600, and the dimension parameter d = 2,4\nor 6. For each setting, we repeat the simulation 200 times. We use p = 5 in defining\nthe operator K\u0302 in (2.9). For each of the 200 samples, we replicate the bootstrap\nsampling 200 times.\nThe average of the ordered eigenvalues of K\u0302 obtained from the 200 replications\nare plotted in Figure 1. For a good visual illustration, we only plot the ten largest\neigenvalues. It is clear that drop from the dth largest eigenvalue to the (d + 1)st\nFIG. 1. The average estimated eigenvalues over the 200 replications with sample sizes n = 100\n(solid lines), 300 (dotted lines) and 600 (dashed lines).\n3364 N. BATHIA, Q. YAO AND F. ZIEGELMANN\nFIG. 2. The boxplots of the P -values for the bootstrap tests of the hypothesis that (a) the d th largest\neigenvalue of K is 0, and (b) the (d + 1)th largest eigenvalue of K is 0. The horizontal lines mark\nthe 1% (dotted line), 5% (solid lines) and 10% (dashed lines) significance levels, respectively.\nis very pronounced. Furthermore, the estimates for zero-eigenvalues with different\nsample size are much closer than those for nonzero eigenvalues. This evidence is\nin line with the different convergence rates presented in Theorem 1(ii) and (iii).\nWe apply the bootstrap method to test the hypothesis that the dth or the (d + 1)st\nlargest eigenvalue of K (\u03b8d and \u03b8d+1, resp.) are 0. The results are summarized\nin Figure 2. The bootstrap test cannot reject the true null hypothesis \u03b8d+1 = 0.\nThe false null hypothesis \u03b8d = 0 is routinely rejected when n = 600 or 300; see\nFigure 2(a). However, the test does not work when the sample size is as small\nas 100.\nTo measure the accuracy of the estimation for the factor loading space M, we\nneed to modify the metric D defined in (3.2) first, as d\u0302 may be different from d .\nLet N1,N2 be two subspaces in L2(I) with dimension d1 and d2, respectively. Let\n{\u03b6i1, . . . , \u03b6idi } be an orthonormal basis of Ni , i = 1,2. The discrepancy measure\nCURVE TIME SERIES 3365\nFIG. 3. The boxplots for the estimated error D\u02dc defined in (4.1).\nbetween the two subspaces is defined as\nD\u02dc(N1,N2) =\n\u221a\u221a\u221a\u221a\u221a1 \u2212 1\nmax(d1, d2)\nd1\u2211\nk=1\nd2\u2211\nj=1\n(\u3008\u03b62j , \u03b61k\u3009)2.(4.1)\nIt can be shown that D\u02dc(N1,N2) \u2208 [0,1]. It equals 0 if and only if N1 = N2, and 1\nif and only if N1 \u22a5N2. Obviously, D\u02dc(N1,N2) = D(N1,N2) when d1 = d2 = d .\nWe computed D\u02dc(M\u0302,M) in the 200 replications for each setting. Figure 3 presents\nthe boxplots of those D\u02dc-values. It is noticeable that the D\u02dc measure decreases as\nthe sample size n increases. It is interesting to note too that the accuracy of the\nestimation is independent of the dimension d .\nTo further illustrate the different convergence rates in estimating nonzero and\nzero eigenvalues, as stated in Theorem 1, we generate 10,000 samples with dif-\nferent sample sizes from model (1.1) with d = 1, \u03bet = 0.5\u03bet\u22121 + \u03b7t , where\n\u03b7t \u223c N(0,1), \u03d5(u) =\n\u221a\n2 cos(\u03c0u), and \u03b5t (\u00b7) is the same as above. In defining the\noperator K , we let p = 1. Then the operator K has only one nonzero eigenvalue\n\u03b8 = 2. Figure 4 depicts the standardized histograms and the kernel density esti-\nmators of\n\u221a\nn(\u03b8\u03021 \u2212 \u03b8), computed from the 10,000 samples. It is evident that those\ndistributions resemble normal distributions when the sample size is 200 or greater.\nThis is in line with Theorem 1(ii) which implies that \u221an(\u03b8\u03021 \u2212 \u03b8) converges to a\nnondegenerate distribution.\nFigure 5 displays the distribution of\n\u221a\nn\u03b8\u03022, noting \u03b82 = 0. It is clear that \u221an\u03b8\u03022\nconverges to zero as n increases, indicating the fact that the normalized factor\n3366 N. BATHIA, Q. YAO AND F. ZIEGELMANN\nFIG. 4. Standardized histograms overlaid by kernel density estimators of \u221an(\u03b8\u03021 \u2212 \u03b8).\n\u221a\nn is too small to stabilize the distribution. In contrast, Figure 6 exhibits that\nthe distribution of n\u03b8\u03022 stabilizes from the sample size as small as n = 50; see\nTheorem 1(iii). In fact, the profile of the distribution with n = 10 looks almost the\nsame as that with n = 2000.\nFigure 7 displays boxplots of the absolute estimation errors of the eigenvalues.\nWith the same sample size, the estimation errors for the nonzero eigenvalue are\nconsiderably greater that those for the zero eigenvalue.\nCURVE TIME SERIES 3367\nFIG. 5. Standardized histograms overlaid by kernel density estimators of \u221an\u03b8\u03022.\n4.2. A real data example. To further illustrate the methodology developed in\nthis paper, we set upon the task of modeling the intraday return densities for the\nIBM stock in 2006. To this end, we have obtained the intraday prices via the\nWRDS database. We only use prices between 09:30\u201316:00 since the market is not\nparticularly active outside of these times. There are n = 251 trading days in the\nsample and a total of 2,786,650 observations. The size of this dataset is 73.7 MB.\nSince high frequency prices are not equally spaced in time, we compute\nthe returns using the prices at the so-called previous tick times in every 5\nminute intervals. More precisely, we set the sampling times at \u03c41 = 09:35, \u03c42 =\n3368 N. BATHIA, Q. YAO AND F. ZIEGELMANN\nFIG. 6. Standardized histograms overlaid by kernel density estimators of n\u03b8\u03022.\n09:40, . . . , \u03c4m = 16:00 with m = 78. Denote by Xi(tij ) the stock price on the ith\nday at the time tij , j = 1, . . . , ni and i = 1, . . . , n. The previous tick times on the\nith day are defined as\n\u03c4il = max{tij : tij \u2264 \u03c4l, j = 1, . . . , ni}, l = 1, . . . ,m.\nThe lth return on the ith day is then defined as Zil = log{Xi(\u03c4il)\/Xi(\u03c4i,l\u22121)}.\nCURVE TIME SERIES 3369\nFIG. 7. Boxplots of estimation errors: (a) Errors for nonzero eigenvalue |\u03b8\u03021 \u2212 \u03b8 |; (b) Errors for\nzero-eigenvalue \u03b8\u03022. To add clarity to the display, the outliers are not plotted.\nWe then estimate the intraday return densities using the standard kernel method\nYi(u) = (mhi)\u22121\nm\u2211\nj=1\nK\n(\nZij \u2212 u\nhi\n)\n, i = 1, . . . , n,(4.2)\nwhere K(u) = (\u221a2\u03c0)\u22121 exp(\u2212u2\/2) is a Gaussian kernel and hi is a bandwidth.\nWe set I = [\u22120.002,0.002] as the support for Yi(\u00b7). Let \u03c3\u0302i be the sample stan-\ndard deviation of {Zij , j = 1, . . . ,m} and h\u0302i = 1.06\u03c3\u0302im\u22121\/5 be Silverman\u2019s rule\nof thumb bandwidth choice for day i. Then for each i, we employ three levels of\nsmoothness by setting hi in (4.2) equal to 0.5h\u0302i , h\u0302i and 2h\u0302i . Figure 8 displays the\nobserved densities for the first 8 days of the sample.\nTo identify the finite dimensionality of Yt (\u00b7), we apply the methodology devel-\noped in this paper. We set p = 5 in (2.8). Figure 9 displays the estimated eigenval-\nues. With all three bandwidths used, the first two eigenvalues are much larger than\nthe remaining ones. Furthermore, there is no clear cut-off from the third eigenvalue\nonwards. This suggests to take d\u0302 = 2. The bootstrap tests, reported in Table 1, lend\nfurther support to this assertion. Indeed for all levels of smoothness adopted, the\nbootstrap test rejects the null H0 : \u03b82 = 0 but cannot reject the hypothesis \u03b8j = 0\nfor j = 3,4 or 5. Note that it is implied by \u03b83 = 0 that \u03b83+k = 0 for k \u2265 1. Indeed,\nwe tested \u03b83+k = 0 only for illustrative purposes.\nTable 2 contains the P -values from testing the hypothesis that the estimated\nloadings, \u03b7\u0302tj in (2.11) are white noise using the Ljung\u2013Box\u2013Pierce portmanteau\n3370 N. BATHIA, Q. YAO AND F. ZIEGELMANN\nFIG. 8. Estimated densities, Yi(\u00b7), using bandwidths hi = h\u0302i (solid lines), 0.5h\u0302i (dashed lines) and\n2h\u0302i (dotted lines).\nFIG. 9. Estimated eigenvalues \u03b8\u0302j using bandwidths ht = 0.5h\u0302t (solid lines), h\u0302t (dashed lines) and\n2h\u0302t (dotted lines).\nCURVE TIME SERIES 3371\nTABLE 1\nP -values from applying the bootstrap test in Section 2.2.3 to the\nintraday return density example\nht = 0.5\u0302ht ht = \u0302ht h= 2\u0302ht\nH0 : \u03b81 = 0 0.00 0.00 0.00\nH0 : \u03b82 = 0 0.00 0.00 0.00\nH0 : \u03b83 = 0 0.35 0.15 0.18\nH0 : \u03b84 = 0 0.62 0.73 0.74\nH0 : \u03b85 = 0 0.68 0.91 0.93\ntest. Although we should interpret the results of this test with caution (see Re-\nmark 3 in Section 2.2.3), they provide further evidence that there is a considerable\namount of dynamic structure in the two-dimensional subspace corresponding to\nthe first two eigenvalues \u03b81 and \u03b82, and there is little or none dynamic structure in\nthe directions corresponding to \u03b83 and \u03b84. Collating all the relevant findings, we\ncomfortably set d\u0302 = 2 in our analysis.\nFigure 10 displays the first d\u0302(= 2) estimated eigenfunctions \u03c8\u0302j in (2.13). Al-\nthough the estimated curves Yt (\u00b7) in Figure 8 are somehow different for different\nbandwidths, the shape of the estimated eigenfunctions is insensitive to the choice\nof bandwidth.\nFigure 11 displays time series plots of the estimated loadings \u03b7\u0302t1 and \u03b7\u0302tj . Again\nthe estimated loadings with three levels of bandwidth are almost indistinguishable\nfrom each other. Furthermore, the ACF and PACF of the series \u03b7\u0302tj = (\u03b7\u0302t1, \u03b7\u0302t2)\u2032 are\nalso virtually identical for all three choices of h. These graphics are displayed in\nFigures 12 and 13.\nTABLE 2\nP -values from testing the hypothesis H0 : \u03b7\u0302tj is white noise using the Ljung\u2013Box\u2013Pierce\nportmanteau test. The test statistic is given by Qj = n(n+ 2)\u2211qk=1 sj (k)2\/(n\u2212 k), where sj (k) is\nthe sample autocorrelation of \u03b7\u0302tj at lag k. Under H0, Qj has an asymptotic \u03c72q -distribution\nht 0.5\u0302ht \u0302ht 2\u0302ht\nq 1 3 5 1 3 5 1 3 5\n\u03b7\u0302t1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n\u03b7\u0302t2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n\u03b7\u0302t3 0.11 0.08 0.04 0.09 0.08 0.02 0.07 0.06 0.02\n\u03b7\u0302t4 0.05 0.25 0.28 0.22 0.33 0.47 0.53 0.56 0.63\n\u03b7\u0302t5 0.22 0.19 0.39 0.30 0.47 0.58 0.73 0.77 0.81\n3372 N. BATHIA, Q. YAO AND F. ZIEGELMANN\nFIG. 10. Estimated eigenfunctions (a) \u03c8\u03021 and (b) \u03c8\u03022 using bandwidths ht = 0.5h\u0302t (solid lines),\nh\u0302t (dashed lines) and 2h\u0302t (dotted lines).\nFIG. 11. Estimated loadings (a) \u03b7\u0302t1 and (b) \u03b7\u0302t2 using bandwidths ht = 0.5h\u0302t (solid lines),\nh\u0302t (dashed lines) and 2h\u0302t (dotted lines).\nCURVE TIME SERIES 3373\nFIG. 12. ACF of \u03b7\u0302tj using bandwidthsht = 0.5h\u0302t (solid lines), h\u0302t (dashed lines) and 2h\u0302t (dotted\nlines).\nWe now fit a VAR model to the estimated loadings, \u03b7\u0302t :\n\u03b7\u0302t =\n\u03c4\u2211\nk=1\nAk\u03b7\u0302t\u2212k + et .(4.3)\nSince the estimated loadings \u03b7\u0302tj , as defined in (2.11), have mean zero by construc-\ntion, there is no intercept term in the model. We choose the order \u03c4 in (4.3) by\nminimizing the AIC. The AIC values for the order \u03c4 = 0,1, . . . ,10 are given in\nTable 3. With all three bandwidths used, the AIC chooses \u03c4 = 3, and the multi-\nvariate portmanteau test (with lag values 1, 3 and 5) of Li and McLeod (1981) for\nthe residual of the fitted VAR models are insignificant at the 10% level. The Yule\u2013\nWalker estimates of the parameter matrices, Ak = (ak,ij ) in (4.3), with the order\n\u03c4 = 3 are given in Table 4.\nTo summarize, we found that the dynamic behavior of the IBM intraday return\ndensities in 2006 was driven by two factors. These factors series are modeled well\nby a VAR(3) process. We note that with all the three levels of smoothness adopted\nin the initial density estimation, these conclusions were unchanged.\nFinally, we make a cautionary remark on the implied true curves Xt(\u00b7) in the\nabove analysis. We take the unknown true daily densities as Xt(\u00b7). We see those\ndensities as random curves, as the distribution of the intraday returns tomorrow\n3374 N. BATHIA, Q. YAO AND F. ZIEGELMANN\nFIG. 13. PACF of \u03b7\u0302tj using bandwidths ht = 0.5h\u0302t (solid lines), h\u0302t (dashed lines) and 2h\u0302t (dotted\nlines).\ndepends on the distributions of today, yesterday and so on, but is not entirely de-\ntermined by them. Now in model (1.1), E{\u03b5t (u)} = E{Yt (u)}\u2212E{Xt(u)} \u0004\u2261 0. But\nthis does not affect the analysis performed in identifying the dimensionality of the\ncurves; see also Pan and Yao (2008). Note that (2.10) provides an alternative esti-\nmator for the true density Xt(\u00b7) based on the dynamic structure of the curve series.\nIt can be used, for example, to forecast the density for tomorrow. However, an ob-\nvious normalization should be applied since we did not make use the constraint\u222b\nI Xt(u)du = 1 in constructing (2.10).\nTABLE 3\nAIC values from fitting the VAR model in (4.3). The figures in this table have been centered at the\nminimum AIC value\n\u03c4 = 0 \u03c4 = 1 \u03c4 = 2 \u03c4 = 3 \u03c4 = 4 \u03c4 = 5\nht = 0.5h\u0302t 131.33 40.39 9.98 0.00 7.86 10.38\nht = h\u0302t 133.04 41.32 9.53 0.00 7.47 10.08\nht = 2h\u0302t 135.47 40.83 9.58 0.00 7.00 8.94\nCURVE TIME SERIES 3375\nTABLE 4\nEstimated parameter matrices Ak = (ak,ij ) from fitting the VAR model in (4.3)\nj 1 2\nht 0.5\u0302ht \u0302ht 2\u0302ht 0.5\u0302ht \u0302ht 2\u0302ht\na1,1j 0.08 0.07 0.01 \u22120.14 \u22120.16 \u22120.22\na1,2j \u22120.08 \u22120.05 0.03 0.24 0.26 0.33\na2,1j 0.35 0.39 0.38 0.06 0.09 0.08\na2,2j \u22120.36 \u22120.43 \u22120.43 \u22120.05 \u22120.10 \u22120.11\na3,1j 0.08 0.05 0.02 \u22120.13 \u22120.15 \u22120.18\na3,2j \u22120.16 \u22120.13 \u22120.11 0.14 0.15 0.17\nAPPENDIX A\nIn this section, we provide the relevant background on operator theory used in\nthis work. More detailed accounts may be found in Dunford and Schwartz (1988).\nLet H be a real separable Hilbert space with respect to some inner product \u3008\u00b7, \u00b7\u3009.\nFor any V \u2282 H, the orthogonal complement of V is given by\nV\u22a5 = {x \u2208 H : \u3008x, y\u3009 = 0,\u2200y \u2208 V}.\nNote that V\u22a5\u22a5 = V where V denotes the closure of V . Clearly, if V is finite dimen-\nsional then V\u22a5\u22a5 = V .\nLet L be a linear operator from H to H. For x \u2208 H, denote by Lx the image of\nx under L. The adjoint of L is denoted by L\u2217 and satisfies\n\u3008Lx,y\u3009 = \u3008x,L\u2217y\u3009, x, y \u2208 H.(A.1)\nL is said to be self adjoint if L\u2217 = L and nonnegative definite if\n\u3008Lx,x\u3009 \u2265 0 \u2200x \u2208 H.\nThe image and null space of L are defined as Im(L) = {y \u2208 H :y = Lx,x \u2208 H}\nand Ker(L) = {x \u2208 H :Lx = 0}, respectively. Note that Ker(L\u2217) = (Im(L))\u22a5,\nKer(L) = (Im(L\u2217))\u22a5 and Ker(L\u2217) = Ker(LL\u2217). We define the rank of L to be\nr(L) = dim(Im(L)) and we say that L is finite dimensional if r(L) < \u221e.\nA linear operator L is said to be bounded if there exists some finite constant\n\u0011> 0 such that for all x \u2208 H\n\u2016Lx\u2016 <\u0011\u2016x\u2016,\nwhere \u2016 \u00b7 \u2016 is the norm induced on H by \u3008\u00b7, \u00b7\u3009. We denote the space of bounded\nlinear operators from H to H by B = B(H,H) and the uniform topology on B is\ndefined by\n\u2016L\u2016B = sup\n\u2016x\u2016\u22641\n\u2016Lx\u2016, L \u2208 B.\n3376 N. BATHIA, Q. YAO AND F. ZIEGELMANN\nNote that all bounded linear operators are continuous, and the converse also\nholds.\nAn operator L \u2208 B is said to be compact if there exists two orthonormal se-\nquences {ej } in {fj } of H and a sequence of scalars {\u03bbj } decreasing to zero such\nthat\nLx =\n\u221e\u2211\nj=1\n\u03bbj \u3008ej , x\u3009fj , x \u2208 H,\nor more compactly\nL =\n\u221e\u2211\nj=1\n\u03bbjej \u2297 fj .(A.2)\nNote that if H = L2(I) equipped with the inner product defined in (2.1), then\n(Lx)(u) =\n\u221e\u2211\nj=1\n\u03bbj \u3008ej , x\u3009fj (u).\nClearly, Im(L) = sp{fj : j \u2265 1} and Ker(L) = sp{ej : j \u2265 1}\u22a5.\nThe Hilbert\u2013Schmidt norm of a compact linear operator L is defined as \u2016L\u2016S =\n(\n\u2211\u221e\nj=1 \u03bb2j )1\/2. We will let S denote the space consisting of all the operators with a\nfinite Hilbert\u2013Schmidt or nuclear norm. Clearly, we have the inequalities \u2016 \u00b7 \u2016S \u2265\n\u2016\u00b7\u2016B , and thus the inclusions S \u2282 B. Note that B is a Banach space when equipped\nwith their respective norms. Furthermore, S is a Hilbert space with respect to the\ninner product\n\u3008L1,L2\u3009S =\n\u221e\u2211\ni,j=1\n\u3008L1gi, hj \u3009\u3008L2gi, hj \u3009, L1,L2 \u2208 S,\nwhere {gi} and {hj } are any orthonormal bases of H.\nAPPENDIX B\nIn this section, we provide the proofs for the propositions in Section 2 and the\ntheorems in Section 3. Throughout the proofs, we may use C to denote some\n(generic) positive and finite constant which may vary from line to line. We in-\ntroduce some technical lemmas first.\nLEMMA 1. Let L be a finite-dimensional operator such that for some se-\nquences of orthonormal vectors {ej }, {fj }, {gj } and {hj } and some sequences\nof decreasing scalars {\u03b8j } and {\u03bbj }, L admits the spectral decompositions L =\u2211d\nj=1 \u03b8j ej \u2297 fj =\n\u2211d \u2032\nj=1 \u03bbjgj \u2297 hj . Then it holds that d \u2032 = d .\nCURVE TIME SERIES 3377\nPROOF. Note that if d \u0004= d \u2032 then both Im(L) and Im(L\u2217k) will be of different\ndimensions under the alternative characterizations due to linear independence of\n{ej }, {fj }, {gj } and {hj }. Thus, it must hold that d = d \u2032. \u0002\nLEMMA 2. Let L be a linear operator from H to H, where H is a separable\nHilbert space. Then it holds that Im(LL\u2217) = Im(L).\nPROOF. Using the facts about inner product spaces and linear operators stated\nin Appendix A, we have\nIm(LL\u2217) = (Im(LL\u2217))\u22a5\u22a5 = (Im((LL\u2217)\u2217))\u22a5\u22a5\n= (Ker(LL\u2217))\u22a5 = (Ker(L\u2217))\u22a5\n= (Im(L))\u22a5\u22a5 = Im(L),\nwhich concludes the proof. \u0002\nFor the sake of the simplicity in presentation of the proofs, we adopt the the\nstandard notation for Hilbert spaces. For any f \u2208 L2(I), we write \u2016f \u2016 = \u221a\u3008f,f \u3009\n[see (2.1)], and denote Mkf \u2208 L2(I) the image of f under the operator Mk in the\nsense that\n(Mkf )(u) =\n\u222b\nI\nMk(u, v)f (v) dv.\nThe operators Nk,K, M\u0302k and K\u0302 may be expressed in the same manner. Note now\nthat the adjoint operator of Mk is\n(M\u2217k f )(u) =\n\u222b\nI\nMk(v,u)f (v) dv.\nSee (A.1). Furthermore, Nk = MkM\u2217k in the sense that Nkf = MkM\u2217k f ; see (2.6).\nSimilarly, K\u0302 =\u2211pk=1 M\u0302kM\u0302\u2217k ; see (2.9).\nPROOF OF PROPOSITION 1. (i) To save notational burden, we set k \u2261 k0. We\nonly need to show Im(Nk) = M. Since Nk = MkM\u2217k , it follows from Lemma 2\nthat Im(Nk) = Im(MkM\u2217k ) = Im(Mk) as Nk and Mk are finite dimensional and\nthus their images are closed.\nNow, recall from Section 2.1 that Mk may be decomposed as\nMk =\nd\u2211\ni,j=1\n\u03c3\n(k)\nij \u03d5i \u2297 \u03d5j .(B.1)\nSee also (A.2). Thus, from (B.1), we may write\nMk =\nd\u2211\ni=1\n\u03bb\n(k)\ni \u03d5i \u2297 \u03c1(k)i ,(B.2)\n3378 N. BATHIA, Q. YAO AND F. ZIEGELMANN\nwhere\n\u03c1ik =\n\u2211d\nj=1 \u03c3\n(k)\nij \u03d5j\n\u2016\u2211dj=1 \u03c3 (k)ij \u03d5j\u2016 , \u03bb\n(k)\ni =\n\u2225\u2225\u2225\u2225\u2225\nd\u2211\nj=1\n\u03c3\n(k)\nij \u03d5i\n\u2225\u2225\u2225\u2225\u2225.\nFrom (B.1), it is clear that Im(Mk) \u2286 M, which is finite dimensional. Thus, Mk is\ncompact and therefore admits a spectral decomposition of the form\nMk =\ndk\u2211\nj=1\n\u03b8\n(k)\nj \u03c8\n(k)\nj \u2297 \u03c6(k)j(B.3)\nwith (\u03c6(k)j ,\u03c8\n(k)\nj ) forming the adjoint pair of singular functions of Mk correspond-\ning to the singular value \u03b8(k)j . Clearly, dk \u2264 d . Thus, if dk < d , Im(Mk) \u2282 M since\nfrom (B.3), Im(Mk) = span{\u03c6(k)j : j = 1, . . . , dk} and any subset of dk < d linearly\nindependent elements in a d-dimensional space can only span a proper subset of\nthe original space.\nNow to complete the proof, we only need to show that the set of {\u03c1(k)j } in (B.2)\nis linearly independent for some k. If this can be done, then we are in a position to\napply Lemma 1. Let \u03b2 be an arbitrary vector in Rd and put \u03d5 = (\u03d51, . . . , \u03d5d)\u2032 and\n\u03c1k = (\u03c1(k)1 , . . . , \u03c1(k)d )\u2032, then the linear independence of the set {\u03c1(k)i } can easily be\nseen as the equation\n\u03b2\u03c1k = \u03b2\u0003k\u03d5 = 0\nhas a nontrivial solution if and only if \u03b2\u0003k = 0. However, since \u0003k is of full rank\nby assumption, it follows that it is invertible and the only solution is the trivial one\n\u03b2 = 0. Thus, Lemma 1 implies dk = d and the result follows from noting that any\nlinearly independent set of d elements in a d-dimensional vector space forms a\nbasis for that space.\n(ii) Similar to the proof of part (i) above, we only need to show Im(K) = M.\nNote that for any f \u2208 L2(I), \u3008MkM\u2217k f, f \u3009 = \u3008M\u2217k f,M\u2217k f \u3009 = \u2016M\u2217k f \u20162 \u2265 0, thus\nthe composition Nk = MkM\u2217k is nonnegative definite which implies that K is\nalso nonnegative definite. Therefore, Im(K) = \u22c3pk=1 Im(Nk). From here, the re-\nsult given in part (i) of the proposition concludes the proof. \u0002\nPROOF OF PROPOSITION 2. Let \u03b8\u0302j be a nonzero eigenvalue of K\u2217, and\n\u03b3 j = (\u03b31j , . . . , \u03b3n\u2212p,j )\u2032 be the corresponding eigenvector, that is, K\u2217\u03b3 j = \u03b3 j \u03b8\u0302j .\nWriting this equation component by component, we obtain that\n1\n(n\u2212 p)2\nn\u2212p\u2211\ni,s=1\np\u2211\nk=1\n\u3008Yt+k \u2212 Y\u00af , Ys+k \u2212 Y\u00af \u3009\u3008Ys \u2212 Y\u00af , Yi \u2212 Y\u00af \u3009\u03b3ij = \u03b3tj \u03b8\u0302j(B.4)\nCURVE TIME SERIES 3379\nfor t = 1, . . . , n\u2212 p; see (2.12). For \u03c8\u0302j defined in (2.13),\n(K\u0302\u03c8\u0302j )(u) =\n\u222b\nI\nK\u0302(u, v)\u03c8\u0302j (v) dv\n= 1\n(n\u2212 p)2\nn\u2212p\u2211\nt,s=1\np\u2211\nk=1\n{Yt (u)\u2212 Y\u00af (u)}\u3008Ys \u2212 Y\u00af , \u03c8\u0302j \u3009\n\u00d7 \u3008Yt+k \u2212 Y\u00af , Ys+k \u2212 Y\u00af \u3009\n= 1\n(n\u2212 p)2\nn\u2212p\u2211\nt,s,i=1\np\u2211\nk=1\n{Yt (u)\u2212 Y\u00af (u)}\u03b3ij \u3008Ys \u2212 Y\u00af , Yi \u2212 Y\u00af \u3009\n\u00d7 \u3008Yt+k \u2212 Y\u00af , Ys+k \u2212 Y\u00af \u3009;\nsee (2.9). Plugging (B.4) into the right-hand side of the above expression, we ob-\ntain that\n(K\u0302\u03c8\u0302j )(u) =\nn\u2212p\u2211\nt=1\n{Yt (u)\u2212 Y\u00af (u)}\u03b3tj \u03b8\u0302j = \u03c8\u0302j (u)\u03b8\u0302j ,\nthat is, \u03c8\u0302j is an eigenfunction of K\u0302 corresponding to the eigenvalue \u03b8\u0302j . \u0002\nAs we shall see, the operator K\u0302 =\u2211pk=1 M\u0302kM\u0302\u2217k may be written as a functional\nof empirical distributions of Hilbertian random variables. Thus, we require an aux-\niliary result to deal with this form of process. To this end, we extend the V -statistic\nresults of Sen (1972) to the setting of Hilbertian valued random variables. Further\ndetails about V -statistics may be found in Lee (1990).\nLet H be a real separable Hilbert space with norm \u2016 \u00b7 \u2016 generated by an inner\nproduct \u3008\u00b7, \u00b7\u3009. Let Xt \u2208 X be a sequence of strictly stationary and Hilbertian ran-\ndom variables whose distribution functions will be denoted by P(x), x \u2208 H. Note\nthat the spaces X and H may differ. Let \u03c6 :Xm \u2192 H be Bochner integrable and\nsymmetric in each of its m(\u22652) arguments. Now consider the functional\n\u03b8(P ) =\n\u222b\nX m\n\u03c6(x1, . . . , xm)\nm\u220f\nj=1\nP(dxj ),\ndefined over P = {P :\u2016\u03b8(P )\u2016 < \u221e}. As an estimator of \u03b8(P ), consider the V -\nstatistic defined by\nVn = n\u2212m\nn\u2211\ni1=1\n\u00b7 \u00b7 \u00b7\nn\u2211\nim=1\n\u03c6(Xi1, . . . ,Xim).\nNow for c = 0,1, . . . ,m, we define the functions\n\u03c6c(x1, . . . , xc) =\n\u222b\nX m\u2212c\n\u03c6(x1, . . . , xc, xc+1, . . . , xm)\nm\u220f\nj=c+1\nP(dxj )\n3380 N. BATHIA, Q. YAO AND F. ZIEGELMANN\nand\ngc(x1, . . . , xc) =\nc\u2211\nd=0\n(\u22121)c\u2212d \u2211\n1\u2264j1<\u00b7\u00b7\u00b7<jd\u2264c\n\u03c6d(Xj1, . . . ,Xjd ).\nIn order to construct the canonical decomposition of Vn, we use Dirac\u2019s \u03b4-measure\nto define the empirical measure Pn as follows:\nPn(A) = n\u22121(\u03b4X1(A)+ \u00b7 \u00b7 \u00b7 + \u03b4Xn(A)), A \u2208 X .\nThen for c = 1, . . . ,m, we set\nVnc =\n\u222b\nXc\n\u03c6c(x1, . . . , xc)\nc\u220f\nj=1\n(\nPn(dxj )\u2212 P(dxj ))\n= n\u2212c\nn\u2211\ni1=1\n\u00b7 \u00b7 \u00b7\nn\u2211\nic=1\ngc(Xi1, . . . ,Xic),\nthen we have\nVn \u2212 \u03b8(P ) =\nm\u2211\nc=1\n(\nm\nc\n)\nVnc.(B.5)\nIn particular, note that\nVn1 = 1\nn\nn\u2211\ni=1\ng1(Xi).\nDecomposition (B.5) is the Hoeffding representation of the statistic Vn. It plays a\ncentral role in the proof of Lemma 3 below. We are now in a position to state some\nregularity conditions which form the basis of the result.\n\u2022 A1. {Xt } is strictly stationary and \u03c8-mixing with \u03c8-mixing coefficients satisfy-\ning the condition\n\u2211\u221e\nl=1 lm\u22121\u03c81\/2(l) < \u221e.\n\u2022 A2. \u222bX m \u2016\u03c6(x1, . . . , xm)\u20162\u220fmj=1 P(dxj ) < \u221e.\n\u2022 A3. E\u2016g1(X1)\u20162 + 2\u2211\u221ek=2 E\u3008g1(X1), g1Xk\u3009 \u0004= 0.\nLEMMA 3. Let conditions A1\u2013A3 hold. Then for c = 1, . . . ,m it holds that\nE\u2016Vnc\u20162 = O(n\u2212c).\nPROOF. We make use of (B.5). Let {ej : j \u2265 1} be an orthonormal basis of H.\nThen\nE\u2016Vnc\u20162 =\n\u221e\u2211\nj=1\nE\u3008ej ,Vnc\u30092,(B.6)\nCURVE TIME SERIES 3381\nwhere \u3008ej ,Vnc\u3009 is the R valued V -statistic\n\u3008ej ,Vnc\u3009 = n\u2212c\nn\u2211\ni1=1\n\u00b7 \u00b7 \u00b7\nn\u2211\nic=1\n\u3008ej , gc(Xi1, . . . ,Xic)\u3009.\nNow under conditions A1\u2013A3, Lemma 3.3 in Sen (1972) yields\nE\u3008ej ,Vnc\u30092 \u2264 Cn\u2212c\n\u222b\nX c\n\u3008ej , \u03c6c(x1, . . . , xc)\u30092\nc\u220f\nj=1\nP(dxj )(B.7)\nfor all j \u2265 1. Now inserting the estimate in (B.7) into (B.6) yields\nE\u2016Vnc\u20162 \u2264 Cn\u2212c\n\u221e\u2211\nj=1\n\u222b\nX c\n\u3008ej , \u03c6c(x1, . . . , xc)\u30092\nc\u220f\nj=1\nP(dxj )\n\u2264 Cn\u2212c\n\u222b\nX c\n\u2016\u03c6c(x1, . . . , xc)\u20162\nc\u220f\nj=1\nP(dxj )\n\u2264 Cn\u2212c\n\u221e\u2211\nj=1\n\u222b\nX c\n\u2016\u03c6(x1, . . . , xm)\u20162\nm\u220f\nj=1\nP(dxj )\n= O(n\u2212c)\nas required. \u0002\nPROOF OF THEOREM 1. (i) Since p is fixed and finite, we may set n \u2261 n\u2212p.\nLet Ztk = (Yt \u2212 \u03bc) \u2297 (Yt+k \u2212 \u03bc) \u2208 S . Now consider the kernel \u03c1 :S \u00d7 S \u2192 S\ngiven by\n\u03c1(A,B) = AB\u2217, A,B \u2208 S.(B.8)\nNow note that from (B.8),\nM\u0302kM\u0302k = n\u22122\nn\u2211\ni=1\nn\u2211\nj=1\n\u03c1(Zik,Zjk),\nwhich in light of the preceding discussion is simply a S valued von Mises func-\ntional. Then d \u2265 1 it holds that Mk \u0004= 0, an application of Lemma 3 yields\nE\u2016M\u0302kM\u0302\u2217k \u2212MkM\u2217k \u20162S = O(n\u22121).(B.9)\nNote that if d = 0, the rate in (B.9) would be n\u22122, that is, the kernel \u03c1 would\npossess the property of first order degeneracy. Now by (B.9) and the Chebyshev\ninequality, we have\n\u2016K\u0302 \u2212K\u2016S \u2264\np\u2211\nk=1\n\u2016M\u0302kM\u0302k \u2212MkM\u2217k \u2016S = Op(n\u22121\/2).\n(ii) Given \u2016K\u0302 \u2212 K\u2016S = Op(n\u22121\/2), Lemma 4.2 in Bosq (2000) implies the\nsupj\u22651 |\u03b8\u0302j \u2212 \u03b8j | \u2264 \u2016K\u0302 \u2212 K\u2016S = Op(n\u22121\/2). Condition C3 ensures that \u03c8j is an\n3382 N. BATHIA, Q. YAO AND F. ZIEGELMANN\nidentifiable statistical parameter for j = 1, . . . , d . From here, Lemma 4.3 in Bosq\n(2000) implies \u2016\u03c8\u0302j \u2212\u03c8j\u2016 \u2264 C\u2016K\u0302 \u2212K\u2016S = Op(n\u22121\/2).\n(iii) First, note that by Lemma 3 we have\nE\u2016M\u0302kM\u0302\u2217k \u2212 M\u0302kM\u2217k \u20162S = O(n\u22122).(B.10)\nPut K\u02dc =\u2211pk=1 M\u0302kMk . Then by (B.10) and the Chebyshev inequality, we have\n\u2016K\u0302 \u2212 K\u02dc\u2016S \u2264\np\u2211\nk=1\n\u2016M\u0302kM\u0302\u2217k \u2212 M\u0302kM\u2217k \u2016S = Op(n\u22121).(B.11)\nThe estimate in (B.11) will prove to be crucial in deriving the results for \u03b8\u0302j when\nj \u2265 d + 1.\nNow, extend \u03c81, . . . ,\u03c8d to a complete orthonormal basis of H. Then it holds\nthat\nn\u2211\nj=1\n\u03b8\u0302j =\n\u221e\u2211\nj=1\n\u3008\u03c8j , K\u0302\u03c8j \u3009,(B.12)\nand by recalling that \u03b8j = 0 for j > d\nd\u2211\nj=1\n\u03b8j =\nd\u2211\nj=1\n\u3008\u03c8j ,K\u03c8j \u3009.(B.13)\nNote that span{\u03c8j : j > d} = M\u22a5 and K\u03c8j = 0 for all j > d since Ker(K) =\nM\u22a5. Thus, from (B.12) and (B.13), we have\nn\u2211\nj=1\n\u03b8\u0302j \u2212 \u03b8j =\n\u221e\u2211\nj=1\n\u3008\u03c8j , (K\u0302 \u2212K)\u03c8j \u3009.(B.14)\nNow we will show that\n\u03b8\u0302j \u2212 \u03b8j = \u3008\u03c8j , (K\u0302 \u2212K)\u03c8j \u3009 +Op(n\u22121), j = 1, . . . , d.(B.15)\nLet Kj = \u3008\u03c8j , (K\u0302 \u2212 K)\u03c8\u0302j \u3009. Then using the relations K\u03c8j = \u03b8j\u03c8j and K\u0302\u03c8\u0302j =\n\u03b8\u0302j \u03c8\u0302j along with the fact that K is self adjoint, we have\n|Kj \u2212 (\u03b8\u0302j \u2212 \u03b8j )| = |\u3008\u03c8j , K\u0302\u03c8\u0302j \u3009 \u2212 \u3008K\u03c8j, \u03c8\u0302j \u3009 \u2212 (\u03b8\u0302j \u2212 \u03b8j )|\n= |(\u03b8\u0302j \u2212 \u03b8j )(\u3008\u03c8j , \u03c8\u0302j \u3009 \u2212 1)|(B.16)\n= |\u03b8\u0302j \u2212 \u03b8j ||\u3008\u03c8j , \u03c8\u0302j \u3009 \u2212 1|.\nNote that\n|\u3008\u03c8j , \u03c8\u0302j \u3009 \u2212 1| = |\u3008\u03c8j , \u03c8\u0302j \u2212\u03c8j \u3009| \u2264 \u2016\u03c8j\u2016\u2016\u03c8\u0302j \u2212\u03c8j\u2016 = \u2016\u03c8\u0302j \u2212\u03c8j\u2016.(B.17)\nThus, from the results in (b) above (B.16) and (B.17), we have |Kj \u2212 (\u03b8\u0302j \u2212 \u03b8j )| \u2264\n|\u03b8\u0302j \u2212 \u03b8j |\u2016\u03c8\u0302j \u2212\u03c8j\u2016 = Op(n\u22121) for j = 1, . . . , d .\nCURVE TIME SERIES 3383\nNext, we have\n|\u3008\u03c8j , (K\u0302 \u2212K)\u03c8j \u3009 \u2212Kj | = |\u3008\u03c8j \u2212 \u03c8\u0302j , (K\u0302 \u2212K)\u03c8j \u3009|\n\u2264 \u2016\u03c8j \u2212 \u03c8\u0302j\u2016\u2016(K\u0302 \u2212K)\u03c8j\u2016\n\u2264 \u2016\u03c8j \u2212 \u03c8\u0302j\u2016\u2016K\u0302 \u2212K\u2016S,\nfrom which the results in (i) and (ii) |\u3008\u03c8j , (K\u0302 \u2212 K)\u03c8j \u3009 \u2212 Kj | = Op(n\u22121), thus\nproving (B.15).\nNow from (B.15) we have\nd\u2211\nj=1\n\u03b8\u0302j \u2212 \u03b8j =\nd\u2211\nj=1\n\u3008\u03c8j , (K\u0302 \u2212K)\u03c8j \u3009 +Op(n\u22121),\nand thus from (B.11) and (B.14)\nn\u2211\nj=d+1\n\u03b8\u0302j =\n\u221e\u2211\nj=d+1\n\u3008\u03c8j , (K\u0302 \u2212K)\u03c8j \u3009 +Op(n\u22121)\n=\n\u221e\u2211\nj=d+1\n\u3008\u03c8j , (K\u02dc \u2212K)\u03c8j \u3009 +Op(n\u22121).\nBy noting that \u03c8j \u2208 M\u22a5 for j \u2265 d +1 and Ker(Mk) = Ker(K\u02dc) = Ker(K) = M\u22a5,\nit holds that\n\u2211\u221e\nj=d+1\u3008\u03c8j , (K\u02dc \u2212K)\u03c8j \u3009 = 0. Thus,\n\u2211n\nj=d+1 \u03b8\u0302j = Op(n\u22121) and the\nresult follows from noting that \u03b8\u0302i \u2264\u2211nj=d+1 \u03b8\u0302j for i = 1, . . . , d .\n(iv) Let \u0015M and \u0015M\u22a5 denote the projection operators onto M and M\u22a5, re-\nspectively. Since x = \u0015M(x)+\u0015M\u22a5(x) for any x \u2208 L2(I), we have\n\u2016\u0015M(\u03c8\u0302i)\u20162 = \u2016\u03c8\u0302i \u2212\u0015M\u22a5(\u03c8\u0302i)\u20162 =\nd\u2211\nj=1\n\u3008\u03c8\u0302i,\u03c8j \u30092(B.18)\nfor all i \u2265 1. Now note that for i \u2265 d + 1\n\u2016K(\u03c8\u0302i)\u2016 = \u2016(K \u2212 K\u0302)(\u03c8\u0302i)+ \u03c8\u0302i \u03b8\u0302i\u2016\n\u2264 \u2016(K \u2212 K\u0302)(\u03c8\u0302i)\u2016 + |\u03b8\u0302i |\u2016\u03c8\u0302i\u2016(B.19)\n\u2264 2\u2016K \u2212 K\u0302\u2016B,\nwhere the final inequality follows from the definition of \u2016 \u00b7 \u2016B and Lemma 4.2 in\nBosq (2000) by noting that \u03b8i = 0 for all i \u2265 d + 1.\nNext, we have for i \u2265 d + 1\n\u2016K(\u03c8\u0302i)\u20162 =\n\u221e\u2211\nj=1\n\u3008K(\u03c8\u0302i),\u03c8j \u30092 =\n\u221e\u2211\nj=1\n\u03b82j \u3008\u03c8\u0302i,\u03c8j \u30092\n(B.20)\n=\nd\u2211\nj=1\n\u03b82j \u3008\u03c8\u0302i,\u03c8j \u30092 \u2265 \u03b82d\nd\u2211\nj=1\n\u3008\u03c8\u0302i,\u03c8j \u30092,\n3384 N. BATHIA, Q. YAO AND F. ZIEGELMANN\nsince \u03b81 > \u00b7 \u00b7 \u00b7 > \u03b8d . Combining (B.18), (B.19) and (B.20) yields\n\u2016\u0015M(\u03c8\u0302d0+1)\u20162 = \u2016\u03c8\u0302d0+1 \u2212\u0015M\u22a5(\u03c8\u0302d0+1)\u20162 \u2264 C\u2016K \u2212 K\u0302\u2016B,\nfrom which (i) yields the result. \u0002\nLEMMA 4. The function D defined in (3.2) is a well-defined distance measure\non ZD .\nPROOF. Nonnegativity, symmetry and the identity of indiscernibles are ob-\nvious. It only remains to prove the subadditivity property. For any L \u2208 S , note\nthat \u2016L\u2016S = \u221atr(L\u2217L), where tr denotes the trace operator. Now, for any Xi \u2208 Z ,\ni = 1,2,3, let \u0015Xi denote its corresponding d-dimensional projection operators\ndefined as follows:\n\u0015Xi =\nd\u2211\nj=1\n\u03b6ij \u2297 \u03b6ij ,\nwhere {\u03b6ij : j = 1, . . . , d} is some orthonormal basis of Xi . Now the triangle in-\nequality for the Hilbert\u2013Schmidt norm yields\n\u2016\u0015X1 \u2212\u0015X3\u2016S \u2264 \u2016\u0015X1 \u2212\u0015X2\u2016S + \u2016\u0015X2 \u2212\u0015X3\u2016S .\nSince the projection operators are self adjoint, we have\u221a\ntr(\u00152X1)+ tr(\u00152X3)\u2212 2 tr(\u0015X1\u0015X3)\n\u2264\n\u221a\ntr(\u00152X1)+ tr(\u00152X2)\u2212 2 tr(\u0015X1\u0015X2)\n+\n\u221a\ntr(\u00152X2)+ tr(\u00152X3)\u2212 2 tr(\u0015X2\u0015X3).\nNow tr(\u00152Xi ) = tr(\u0015Xi ) = d and tr(\u0015Xi\u0015Xj ) =\n\u2211d\nk,l=1\u3008\u03b6ik, \u03b6jl\u30092 for i, j = 1,2,3.\nThese last facts along with the definition of D in (3.2) give\nD(X1,X3) \u2264 D(X1,X2)+D(X2,X3),\nwhich concludes the proof. \u0002\nPROOF OF THEOREM 2. From the definition of D in (3.2), note that\n\u221a\n2dD(M\u0302,M) = \u2016\u0015M\u0302 \u2212\u0015M\u2016S,(B.21)\nwhere \u0015M\u0302 =\n\u2211d\nj=1 \u03c8\u0302j \u2297 \u03c8\u0302j and \u0015M =\n\u2211d\nj=1 \u03c6j \u2297 \u03c6j with \u03c61, . . . , \u03c6d form-\ning any orthonormal basis of M. Now if \u00151M and \u00152M are any projection op-\nerators onto M, then by virtue of Lemma 4 it holds that \u2016\u00151M \u2212 \u00152M\u2016S =\u221a\n2dD(M,M) = 0. Thus, we may proceed as if \u0015M in (B.21) was formed with\neigenfunctions of K , that is, \u03c6j = \u03c8j for j = 1, . . . , d .\nCURVE TIME SERIES 3385\nNow, we have\u2225\u2225\u2225\u2225\u2225\nd\u2211\nj=1\n\u03c8\u0302j \u2297 \u03c8\u0302j \u2212\nd\u2211\nj=1\n\u03c8j \u2297\u03c8j\n\u2225\u2225\u2225\u2225\u2225S \u2264\nd\u2211\nj=1\n\u2016\u03c8\u0302j \u2297 \u03c8\u0302j \u2212\u03c8j \u2297\u03c8j\u2016S,(B.22)\nthat is, \u03c8\u0302j \u2297 \u03c8\u0302j (resp., \u03c8j \u2297 \u03c8j ) is the projection operator onto the eigensub-\nspace generated by \u03b8\u0302j (resp., \u03b8j ). Now by part (i) of Theorem 1, \u2016K\u0302 \u2212 K\u2016S =\nOp(n\n\u22121\/2). Thus, Theorem 2.2 in Mas and Menneteau (2003) implies that \u2016\u03c8\u0302j \u2297\n\u03c8\u0302j \u2212 \u03c8j \u2297 \u03c8j\u2016S = Op(n\u22121\/2) for j = 1, . . . , d . This last fact along with (B.21)\nand (B.22) yield D(M\u0302,M) = Op(n\u22121\/2). \u0002\nPROOF OF THEOREM 3. We first note that from (B.9), the triangle inequality\nand the cr inequality, we have\nE\u2016K\u0302 \u2212K\u20162S = O(n\u22121).(B.23)\nAs \u03b8\u03021 \u2265 \u03b8\u03022 \u2265 \u00b7 \u00b7 \u00b7 \u2265 0 (with strict inequality holding with probability one), it holds\nthat {d\u0302 > d} = {\u03b8\u0302d+1 > \u000e}. Now since \u03b8d+1 = 0, it holds that \u03b8\u0302d+1 = |\u03b8\u0302d+1 \u2212\n\u03b8d+1| \u2264 \u2016K\u0302 \u2212 K\u2016S by Lemma 4.2 in Bosq (2000). Collecting these last few facts\nand applying the Chebyshev inequality yields\nP(d\u0302 > d) \u2264 \u000e\u22122E\u2016K\u0302 \u2212K\u20162S = O((\u000e2n)\u22121)(B.24)\nby (B.23). Next, we turn to P(d\u0302 < d). Due to the ordering of the eigenvalues, it\nholds that {d\u0302 < d} = {\u03b8\u0302d\u22121 < \u000e}. Therefore,\nP(d\u0302 < d) = P(\u03b8\u0302d\u22121 < \u000e)\n= P(\u03b8d\u22121 \u2212 \u03b8\u0302d\u22121 > \u03b8d\u22121 \u2212 \u000e)(B.25)\n\u2264 P(|\u03b8d\u22121 \u2212 \u03b8\u0302d\u22121| > \u03b8d\u22121 \u2212 \u000e)\n\u2264 P(\u2016K\u0302 \u2212K\u2016S > \u03b8d\u22121 \u2212 \u000e),\nwhere the final inequality follows from Lemma 4.2 in Bosq (2000). Now since\n\u03b8d\u22121 > 0 and \u000e \u2192 0 as n \u2192 \u221e, it holds that \u03b8d\u22121 \u2212 \u000e > 0 for large enough n.\nThus, by (B.24) and an application of the Chebyshev inequality to (B.23), we have\nP(d\u0302 < d) \u2264 (\u03b8d\u22121 \u2212 \u000e)\u22122E\u2016K\u0302 \u2212K\u20162S = O(n\u22121).(B.26)\nFrom (B.24) and (B.25), it follows that\nP(d\u0302 \u0004= d) = P(d\u0302 < d)+ P(d\u0302 > d) = O((\u000e2n)\u22121) \u2192 0.\nThis completes the proof. \u0002\nAcknowledgments. We are grateful to the Associate Editor and the two ref-\nerees for their helpful comments and suggestions.\nREFERENCES\nBENKO, M., HARDLE, W. and KNEIP, A. (2009). Common functional principal components. Ann.\nStatist. 37 1\u201334. MR2488343\n3386 N. BATHIA, Q. YAO AND F. ZIEGELMANN\nBESSE, P. and RAMSAY, J. O. (1986). Principal components analysis of sampled functions. Psy-\nchometrika 51 285\u2013311. MR0848110\nBOSQ, D. (2000). Linear Processes in Function Spaces. Springer, New York. MR1783138\nDAUXOIS, J., POUSSE, A. and ROMAIN, Y. (1982). Asymptotic theory for the principal component\nanalysis of a vector random function: Some applications to statistical inference. J. Multivariate\nAnal. 12 136\u2013154. MR0650934\nDUNFORD, N. and SCHWARTZ, J. T. (1988). Linear Operators. Wiley, New York.\nFERRATY, F. and VIEU, P. (2006). Nonparametric Functional Data Analysis. Springer, New York.\nMR2229687\nGUILLAS, S. and LAI, M. J. (2010). Bivariate splines for spatial functional regression models.\nJ. Nonparametr. Stat. 22 477\u2013497.\nHALL, P. and VIAL, C. (2006). Assessing the finite dimensionality of functional data. J. R. Stat. Soc.\nSer. B Stat. Methodol. 68 689\u2013705. MR2301015\nHYNDMAN, R. J. and ULLAH, M. S. (2007). Robust forecasting of mortality and fertility rates:\nA functional data approach. Comput. Statist. Data Anal. 51 4942\u20134956. MR2364551\nLEE, A. J. (1990). U-Statistics. Dekker, New York. MR1075417\nLI, W. K. and MCLEOD, A. I. (1981). Distribution of the residual autocorrelations in multivariate\nARMA time series models. J. Roy. Statist. Soc. Ser. B 43 231\u2013239. MR0626770\nKNEIP, A. and UTIKAL, K. J. (2001). Inference for density families using functional principal com-\nponent analysis (with discussion). J. Amer. Statist. Assoc. 96 519\u2013542. MR1946423\nMAS, A. and MENNETEAU, L. (2003). Perturbation approach applied to the asymptotic study of\nrandom operators. In High Dimensional Probability III (J. Hoffmann-Jorgensen, M. B. Marcus\nand J. A. Wellner, eds.) 127\u2013134. Birkh\u00e4user, Boston. MR2033885\nPAN, J. and YAO, Q. (2008). Modelling multiple time series via common factors. Biometrika 95\n365\u2013379. MR2521589\nPE\u00d1A, D. and BOX, G. E. P. (1987). Identifying a simplifying structure in time series. J. Amer.\nStatist. Assoc. 82 836\u2013843. MR0909990\nRAMSAY, J. O. and DALZELL, C. J. (1991). Some tools for functional data analysis (with discus-\nsion). J. Roy. Statist. Soc. Ser. B 53 539\u2013572. MR1125714\nRAMSAY, J. O. and SILVERMAN, B. W. (2005). Functional Data Analysis. Springer, New York.\nMR2168993\nRICE, J. A. and SILVERMAN, B. W. (1991). Estimating the mean and covariance structure nonpara-\nmetrically when the data are curves. J. Roy. Statist. Soc. Ser. B 53 233\u2013243. MR1094283\nSEN, P. K. (1972). Limiting behavior of regular functionals of empirical distributions for stationary\n\u2217-mixing processes. Probab. Theory Related Fields 25 71\u201382. MR0329003\nTIAO, G. C. and TSAY, R. S. (1989). Model specification in multivariate time series (with discus-\nsion). J. Roy. Statist. Soc. Ser. B 51 157\u2013213. MR1007452\nN. BATHIA\nQ. YAO\nDEPARTMENT OF STATISTICS\nLONDON SCHOOL OF ECONOMICS\nLONDON, WC2A 2AE\nUNITED KINGDOM\nE-MAIL: n.bathia@lse.ac.uk\nq.yao@lse.ac.uk\nF. ZIEGELMANN\nDEPARTMENT OF STATISTICS\nFEDERAL UNIVERSITY\nOF RIO GRANDE DO SUL\n91509-900 PORTO ALEGRE\nBRAZIL\nE-MAIL: flavioaz@mat.ufrgs.br\n"}