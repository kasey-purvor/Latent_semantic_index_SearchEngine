{"doi":"10.1109\/IJCNN.2000.857893","coreId":"55643","oai":"oai:eprints.lincoln.ac.uk:1901","identifiers":["oai:eprints.lincoln.ac.uk:1901","10.1109\/IJCNN.2000.857893"],"title":"Training feedforward neural networks using orthogonal iteration of the Hessian eigenvectors","authors":["Hunter, Andrew"],"enrichments":{"references":[{"id":18437548,"title":"Classification of radar returns from the ionosphere using neural networks.","authors":[],"date":"1989","doi":null,"raw":". Sigillito, V.G., Wing, S.P., Hutton, L.V. and Baker, K.B. (1989). Classification of radar returns from the ionosphere using neural networks. John Hopkins APL Technical Digest, 10,262-266. 0-7695-0619-4\/00\/$10.00 (C) 2000 IEEE Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks (IJCNN'00)","cites":null},{"id":18437547,"title":"Elementary Linear Algebra, ft\u2019 Edition,","authors":[],"date":"1994","doi":"10.11120\/msor.2002.02030050","raw":"Anton, H. and Rorres, C. (1994). Elementary Linear Algebra, ft\u2019 Edition, Wiley, New York.","cites":null},{"id":18437546,"title":"FasEr-learning variations on back-propagation: an empirical study.","authors":[],"date":"1988","doi":null,"raw":"Fahlmann, S.E. (1988). FasEr-learning variations on back-propagation: an empirical study. In D. Touretsky, G.E, Hintorzand T.J. Sejnowski (Eds.), Proceedings of the 1988 ConnectionistModels Summer School, 38-51. San Mateo, CA Morgan-Kaufmazm.","cites":null},{"id":18437545,"title":"Fast exact multiplication by the Hessian.Neural","authors":[],"date":"1994","doi":"10.1162\/neco.1994.6.1.147","raw":"Perlmutter, B.A. (1994). Fast exact multiplication by the Hessian.Neural Computation 6(1), 147-160.","cites":null},{"id":18437539,"title":"Neural Networksfor Pattern Recognition.","authors":[],"date":"1995","doi":null,"raw":"Bishop, C. (1995), Neural Networksfor Pattern Recognition. Oxford University Press.","cites":null},{"id":18437542,"title":"Numerical Recipes in C: TheArt of Scientzjic Computing (Seconded.).","authors":[],"date":"1992","doi":"10.1016\/s0003-2670(00)82860-3","raw":"Press, W.H., Teukolsky, S.A., Vetterling, W.T. and Flannery, B.P. (1992). Numerical Recipes in C: TheArt of Scientzjic Computing (Seconded.). Cambridge University Press.","cites":null},{"id":18437544,"title":"Partial BFGS Update and Efficient Step-Length Calculation for Three-Layer Neural Networks,","authors":[],"date":"1996","doi":"10.1162\/neco.1997.9.1.123","raw":"Saito, K. and Ryohei, N. (1996). Partial BFGS Update and Efficient Step-Length Calculation for Three-Layer Neural Networks, Neuron Computation 9 (1), 123-141. Moller, M (1993). A scaled conjugate gradient algorithm for fast supervised learning. Neural Networks 6 (4), 525-533.","cites":null},{"id":18437541,"title":"Second-Order Methodsfor Neural Networks.","authors":[],"date":"1997","doi":"10.1007\/978-1-4471-0953-2","raw":"Shepherd, A.J. (1997). Second-Order Methodsfor Neural Networks. New York, Springer.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"2000-07-24","abstract":"Introduction\\ud\nTraining algorithms for Multilayer Perceptrons optimize the set of W weights and biases, w, so as to minimize an\\ud\nerror function, E, applied to a set of N training patterns. The well-known back propagation algorithm combines an\\ud\nefficient method of estimating the gradient of the error function in weight space, DE=g, with a simple gradient\\ud\ndescent procedure to adjust the weights, Dw = -hg. More efficient algorithms maintain the gradient estimation\\ud\nprocedure, but replace the update step with a faster non-linear optimization strategy [1].\\ud\nEfficient non-linear optimization algorithms are based upon second order approximation [2]. When sufficiently\\ud\nclose to a minimum the error surface is approximately quadratic, the shape being determined by the Hessian matrix.\\ud\nBishop [1] presents a detailed discussion of the properties and significance of the Hessian matrix. In principle, if\\ud\nsufficiently close to a minimum it is possible to move directly to the minimum using the Newton step, -H-1g.\\ud\nIn practice, the Newton step is not used as H-1 is very expensive to evaluate; in addition, when not sufficiently close\\ud\nto a minimum, the Newton step may cause a disastrously poor step to be taken. Second order algorithms either build\\ud\nup an approximation to H-1, or construct a search strategy that implicitly exploits its structure without evaluating it;\\ud\nthey also either take precautions to prevent steps that lead to a deterioration in error, or explicitly reject such steps.\\ud\nIn applying non-linear optimization algorithms to neural networks, a key consideration is the high-dimensional\\ud\nnature of the search space. Neural networks with thousands of weights are not uncommon. Some algorithms have\\ud\nO(W2) or O(W3) memory or execution times, and are hence impracticable in such cases. It is desirable to identify\\ud\nalgorithms that have limited memory requirements, particularly algorithms where one may trade memory usage\\ud\nagainst convergence speed.\\ud\nThe paper describes a new training algorithm that has scalable memory requirements, which may range from O(W)\\ud\nto O(W2), although in practice the useful range is limited to lower complexity levels. The algorithm is based upon a\\ud\nnovel iterative estimation of the principal eigen-subspace of the Hessian, together with a quadratic step estimation\\ud\nprocedure.\\ud\nIt is shown that the new algorithm has convergence time comparable to conjugate gradient descent, and may be\\ud\npreferable if early stopping is used as it converges more quickly during the initial phases.\\ud\nSection 2 overviews the principles of second order training algorithms. Section 3 introduces the new algorithm.\\ud\nSecond 4 discusses some experiments to confirm the algorithm's performance; section 5 concludes the paper","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/55643.pdf","fullTextIdentifier":"http:\/\/eprints.lincoln.ac.uk\/1901\/1\/EQUALOrthogonalIteration.PDF","pdfHashValue":"80c28fe075e80a32d25740415609207bbd4e951d","publisher":null,"rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lincoln.ac.uk:1901<\/identifier><datestamp>\n      2013-03-13T08:32:50Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D6A6163735F47:6A6163735F47373330<\/setSpec><setSpec>\n      74797065733D636F6E666572656E63655F6974656D<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lincoln.ac.uk\/1901\/<\/dc:relation><dc:title>\n        Training feedforward neural networks using orthogonal iteration of the Hessian eigenvectors<\/dc:title><dc:creator>\n        Hunter, Andrew<\/dc:creator><dc:subject>\n        G730 Neural Computing<\/dc:subject><dc:description>\n        Introduction\\ud\nTraining algorithms for Multilayer Perceptrons optimize the set of W weights and biases, w, so as to minimize an\\ud\nerror function, E, applied to a set of N training patterns. The well-known back propagation algorithm combines an\\ud\nefficient method of estimating the gradient of the error function in weight space, DE=g, with a simple gradient\\ud\ndescent procedure to adjust the weights, Dw = -hg. More efficient algorithms maintain the gradient estimation\\ud\nprocedure, but replace the update step with a faster non-linear optimization strategy [1].\\ud\nEfficient non-linear optimization algorithms are based upon second order approximation [2]. When sufficiently\\ud\nclose to a minimum the error surface is approximately quadratic, the shape being determined by the Hessian matrix.\\ud\nBishop [1] presents a detailed discussion of the properties and significance of the Hessian matrix. In principle, if\\ud\nsufficiently close to a minimum it is possible to move directly to the minimum using the Newton step, -H-1g.\\ud\nIn practice, the Newton step is not used as H-1 is very expensive to evaluate; in addition, when not sufficiently close\\ud\nto a minimum, the Newton step may cause a disastrously poor step to be taken. Second order algorithms either build\\ud\nup an approximation to H-1, or construct a search strategy that implicitly exploits its structure without evaluating it;\\ud\nthey also either take precautions to prevent steps that lead to a deterioration in error, or explicitly reject such steps.\\ud\nIn applying non-linear optimization algorithms to neural networks, a key consideration is the high-dimensional\\ud\nnature of the search space. Neural networks with thousands of weights are not uncommon. Some algorithms have\\ud\nO(W2) or O(W3) memory or execution times, and are hence impracticable in such cases. It is desirable to identify\\ud\nalgorithms that have limited memory requirements, particularly algorithms where one may trade memory usage\\ud\nagainst convergence speed.\\ud\nThe paper describes a new training algorithm that has scalable memory requirements, which may range from O(W)\\ud\nto O(W2), although in practice the useful range is limited to lower complexity levels. The algorithm is based upon a\\ud\nnovel iterative estimation of the principal eigen-subspace of the Hessian, together with a quadratic step estimation\\ud\nprocedure.\\ud\nIt is shown that the new algorithm has convergence time comparable to conjugate gradient descent, and may be\\ud\npreferable if early stopping is used as it converges more quickly during the initial phases.\\ud\nSection 2 overviews the principles of second order training algorithms. Section 3 introduces the new algorithm.\\ud\nSecond 4 discusses some experiments to confirm the algorithm's performance; section 5 concludes the paper.<\/dc:description><dc:date>\n        2000-07-24<\/dc:date><dc:type>\n        Conference or Workshop contribution<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lincoln.ac.uk\/1901\/1\/EQUALOrthogonalIteration.PDF<\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lincoln.ac.uk\/1901\/2\/06192173.pdf<\/dc:identifier><dc:identifier>\n          Hunter, Andrew  (2000) Training feedforward neural networks using orthogonal iteration of the Hessian eigenvectors.  In: International Joint Conference of Neural Networks, 24-27 July 2000, Como, Italy.  <\/dc:identifier><dc:relation>\n        http:\/\/doi.ieeecomputersociety.org\/10.1109\/IJCNN.2000.857893<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/eprints.lincoln.ac.uk\/1901\/","http:\/\/doi.ieeecomputersociety.org\/10.1109\/IJCNN.2000.857893"],"year":2000,"topics":["G730 Neural Computing"],"subject":["Conference or Workshop contribution","PeerReviewed"],"fullText":"Training Feedforward Neural Networks using Orthogonal Iteration of\nthe Hessian Eigenvectors\nAndrew Hunter\nDepartment of Computing and Engineering Technology\nUniversity of Sunderland, St+Peter\u2019s Campus, Sunderland, Tyne and Wear, England.\nAndrew.Huntcr@sundcrland.ac.uk\nIntroduction\nTraining algorithms for Multilayer Perceptions optimize the set of Wweights and biases, w, so as to minimize au\nerror t%nction,E, applied to a set of N training patterns. The well-known back propagation algorithm combines an\nefficient method of estimating the gradient of the error function in weight space, AE=g, with a simple gradient\ndescent procedure to adjust the weighb, Aw = \u2013qg. More efficient algorithms maintain the gradient estimation\nprocedure, but replace the update step with a faster non-linear optimization strategy [1].\nEfficient non-linear optimization algorithms are based upon second order approximation [2]. When sufficiently\nclose to a minimum the error surface is approximately quadratic, the shape being determined by the Hessian matrix.\nBishop [1] presents a detailed discussion of the properties and significance of the Hessian matrix. In principle, if\nsufficiently close to a minimum it is possible to move dwectly to the minimum using the Newton step, - K1g.\nIn practice, the Newton step is not used as K1 is very expensive to evaluate; in addition, when not sufficiently close\nto a minimum, the Newton step may cause a dkastmusly poor step to be taken. Second order algorithms either build\nup an approximation to H-l, or construct a search strategy that implicitly exploits its structure without evaluating iu\nthey also either take precautions to prevent steps that lead to a deterioration in error, or explicitly reject such steps.\nIn applying non-linear optimization algorithms to neural networks, a key consideration is the high-dimensional\nnature of the search space. Neural networks with thousands of weights are not uncommon. Some algorithms have\n0(W2) or O(#) memory or execution times, and are hence impracticable in such cases. It is desirable to identify\nalgorithms that have limited memory requirements, particularly algorithms where one may trade memory usage\nagainst convergence speed.\nThe paper describes a new training algorithm that has scalable memory requirements, which may range horn O(W)\nto O(W2),although in practice the useful range is limited to lower complexity levels. The algorithm is based upon a\nnovel iterative estimation of the principal eigen-subspace of the Hessian, together with a quadratic step estimation\nprocedure.\nIt is shown that the new algorithm has convergence time comparable to conjugate gradient desceng and maybe\npreferable if early stopping is used as it converges more quickly during the initial phases.\nSection 2 overviews lhe principles of second order training algorithms. Section 3 introduces the new algorithm.\nSecond 4 discusses some experiments to confirm the algorithm\u2019sperformance; section 5 concludes the paper.\nOverview of Second Order training concepts\nSecond order training algorithms are based upon a local quadratic approximation of the error surface [21.Given a\nquadratic error function, the error surface has hyper-ellipsoid contours of equal error. The axes are aligned with the\neigenvectors of the Hessian, ej, with the length of each axis inversely proportional to the comespon~lng eigenv~ue,\n~, Gradient descent is unacceptably slow since the grtilent vector, -g, tends to point across the hyper-ellipsoid in\nthe direction of axes with large eigenwdues, and convergence speed is limited by the condition number of the\nHessian, & \/Aw. In contrast to the gradient vector, the Newton direction, \u2013Hg, points directly to the minimum,\n0-7695-0619-4\/00\/$10.00 (C) 2000 IEEEProceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks (IJCNN'00)\nQuasi-Newton methods [3] explicitly build up an approximation to the inverse Hessian and line search in the\ndirection of the estimated Newton step. Quasi-Newton algorithms are very effective, but have 0(W2) memory\nrequirements. This sugges~ using approximations that require less storage. For example, the Limited Memory\nQuasi-Newton algorithm uses only O(W)storage but maintains a relatively poor approximation [1]. Saito and\nRyohei[4]recentlysuggestedamodificationthatallowstheamountofmemoryusedintheapproximationtobe\nseated arbitmrily.\nConjugate gradient methods conduct a series of line searches along \u201cnon-interfering\u201d directions that are constructed\nto exploit the Hessian structure without explicitly storing it; they consequently have O(W) storage requirements.\nHowever, they tend to be somewhat slower than Quasi-Newton,\nMoller [5] suggested a modified conjugate gradient descent algorithm that exploits an interesting face the line\nsearch is used to calculate the step length, for which there is an analytic formula involving the Hessian. The Hessian\nenters this formula only in the form Hdj.An efficient modification of the Back Propagation algorithm, the %{.)\noperator technique [6], calculates the product of the Hessian and any vector in O(W)operations, without having to\nexplicitly store or evaluate H. Moller uses this to generate a step size in a single operation, avoiding the line search.\nThe EQUAL algorithm\nThe algorithm described in this paper uses quadratic estimation in a very direct way. To introduce it, it is helpful to\ndiscuss Fahlmarm\u2019sQuick Propagation [7] algorithm.\nConsider optimization of a quadratic function in one dimension, w. Evaluate the gra&ent goat point wO,then move a\nsmatl distance AWO(typically using the gradient descent formulation, \u2013hgo) and evatuate a second grtilen~ g], at\nWZ=WO+AWO.By linear interpolation of the gradients, the minimum is found at w1+AwI,Dwj=AwO.gJ(go-gJ.\nThis formula can be modified to produce an iterative update step for minimization of a non-linear function, as\nfollows: Awi=Awi.l.gi(gi-~\u2013gi). Once sufficiently close to the minimum, this formula converges extremely quickly,\nalthough some additional checks are required to prevent numerical problems on a poorly behaved curve.\nIn quick propagation, the formula is applied separately to each weight in the neural network, amounting to an\nassumption that the weights are independent (i.e. that the principal axes of the hyper-ellipsoids of the error surface\nare aligned with the weights). This suggests modifying the algorithm to operate along the eigenvectors, rather than\nalong the weights. The eigenvectors could in principle be calculated using standard techniques such as Householder\nreduction and the QR algorithm [8]; however, this requires O(w) operations and 0(W2) storage.\nThe approach taken in this paper is to iteratively estimate the leading subset of the eigenvectors. The size of this\nsubset is user-configurable, and can reflect available memory. The quick propagation formula is applied along the\naxes of this estimated eigen-subspace, and a separate step is made in the orthogonal subspace.\nThe estimated eigen-subspace is calculated using the Orthogomd Iteration technique [8]. An initial estimate is\nformed using the original gradient, go,as the first eigenestimate, eo,with subsequent eigenestimates el,. .e, formed\nusing the standard Gram-Schmidt orthogonalization procedure with the standard unit vectors, uj. On subsequent\niterations, the estimated eigenvectors are multiplied by the Hessian, then re-orthogonalized using Gram-Schmidt.\nThe Hejarecalculated using 9({.} operator technique [1,2,6].\nOrthogonal iteration isolates the eigenvectors corresponding to the kwgesteigenvalues: that is, the eigenvectors\naligned across the narrow part of the hypcr-ellipsoid. These are precisely the directions that most limit the search\nstep size, and so isolating even one can significantly improve convergence speed. Al first the approximations will be\nrelatively poor, and as the Hessian changes on anon-linear error surface, they may take time to settte down.\nHowever, an arbitrary rotation of the original axes does not cause a deterioration in quick propagation\u2019s\nperformance, so we can expect the algorithm at least to match that level of performance, even before the\neigenestimates are stabilized.\nAs with atl second-order algorithms, poor steps maybe generated. A simple model-trust procedure is therefore\napplied the step generated by the algorithm is accepted only if it causes a reduction in the error; otherwise, a\nstandard gradient descent step (with a low learning rate) is substituted. The algorithm (call EQUAL, for\nEigenvector-based QUAdratic Learning) is described in detail in figure 2. The Gram-Schmidt orthogonalization\nprocedure is described in any introductory linear algebra text(e.g.[91).\n0-7695-0619-4\/00\/$10.00 (C) 2000 IEEEProceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks (IJCNN'00)\nTerms\nAw(t)\nAw(t)E,Aw(t-w\nAl%\u2019(tp[ei]\n~\ng(t)\nEb(ej]\nUj\nCVi,Vj>\nGSN(E)\nThe weight update vector at time t.\nWeight update in\/orthogonal to eigen-subspace\nComponent of AW(,)Ealong vector e,\nThe learning rate to kick off search, =0.01\nThe gradient of the error function at time t\nThe set of V\u201ceigenestimate\u201d vectors\nA standard unit vector (1 inj\u2019th component Oin others)\nThe dot product of two vectors\nApply the Gram-Schmidt procedure to E, then normalize all ej\nI First iteration I\nAw[o) = \u2013Izg[o)\nel = \u2013g(o) ej=uj, Vl<j SV\nE = GSN(E)\nAll other iterations\nE = GSN(EH) calculateEH using 9?{.}, each ej\nAw[,)~[ej] = QP(c Aw(,_L)~,ej>,< g(,.l)~,ej >,< g(t)~,ej >) Aw(t)~=~Aw(,).[ejl\n~\nAw(f-l)El =Aw(t_,) -~< Aw(t.,),ej ~j\nJ\nEL!(t-l)EL = !J(,-l)E \u2013 , < g (t-l) y Je.>ej\ng(,)fl= g(,)E-~<g(t),ej Yej\n1\u00b0uick-mo~a~ation stet) I\nI lAw(i-l)g(f-:%\u2018tieWise\nFigure 1: The EQUAL Algorithm\n0-7695-0619-4\/00\/$10.00 (C) 2000 IEEEProceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks (IJCNN'00)\nExperiments\nThe algorithm has been tested on two network%The first is for Fischer\u2019sclassic Iris data set, with Sepal Length\nomitted. The network has three inputs, two hidden units and three outputs: a total of 17 weights (biases included);\nthere are 150 cases. The second is for Sigillito\u2019sIonosphere problem [10]. It has 33 inputs, 15 hidden units and one\noutpu~ a total of 477 weights; there are 351 cases. Logistic activation functions and sum-squared error function\nwere used: all inprm were normalized into the range [0,11.All cases were used for training with both data sets and\nno cross-verification was performed, as we are purely interested in optimization speed, not prevention of over-\nfitting.\nThe standard quick propagation and conjugate gradient descent algorithms were used a benchmarks. Versions of\nEQUAL were run with the dimension of the eigen-subspace set to 1,5 and 17 (in the case of the first problem, the\nlast of these implies a full eigen-decomposition of the search space). Each algorithm was executed twenty times, and\nthe results shown are the mean across these runs. As any of the algorithms may converge to significantly inferior\nsolutions on some occasions, up to two test runs were omitted from each average. Although insufficient runs were\nconducted to reach a reliable conclusion on the issue, conjugate gradient decent and EQUA.IWseem to be more\nprone to hit local minima than the simpler algorithms.\nThe computational requirements of the algorithms are best expressed using the number of propagations, expressed\nbelow as n props. Executing a network on each patterns costs one prop, and evaluating the gradient requires two\n(one forward and one backward). A single prop has O(WV)cost.\nAn iteration of quick propagation requires 2 props. An iteration of conjugate gradient descent has variable\nrequirements, depending on the length of the line search: the average is 12 props per iteration.\nAn iteration of EQUAL requires 2Vprops (the gradient calculation ca be combined with the first X {.] operator\napplication). There is also an overhead in the application of Gram-Schmidt. This has O(W2) costs, and hence\nbecomes significant unless V2<c N. In both the test problems, which have a relatively small number of cases, this\nimplies that Gram-Schmidt contributes marginally to the requirements of EQUALIT,and can be ignored for\nEQUAL5and EQUAL1,The figures used are 2 props\/iteration, 10 props\/iteration and 37 props\/iteration for\nEQUAL1,EQUAL5and EQUALITrespectively.\nFigure 2 shows the performance of the algorithms on the Iris test set. Initial convergence speed is faster with the\nsimpler algorithms, with EQU-% being particularly effective. Terminal convergence is superior in the conjugate\ngradient descent and EQUALU algorithms, with quick propagation proving noticeably inferior to the others. In the\nmid-range, the EQUAL1 and EQUAL5algorithms develop a noticeable lead on the others.\n0.55\n0.45\n0,35\n0.25\n0.15\n0.05\n0 400 800 1200 1600 2000 2400 2800 3200 3600\nNumber of Propagations\nFigure 2: Algorithm performance on Iris data set\n\u2014 EQLJAL1\n--- EQUALS\nEQUALI 7\n--- CGD\n--- QUICKP\n0-7695-0619-4\/00\/$10.00 (C) 2000 IEEEProceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks (IJCNN'00)\nEQUAL1 EQUALS EQUA147 CGD QuickProp\n0.0943 0.0947 0.0933 0.0923 0.0990\nTable 1: Average final error after 4000 props, Iris problem\nThe results indicate that the performance of EQUAL1and EQUALSis superior to that of quick propagation. It is also\ncomparable with that of conjugate gradient descent, although somewhat inferior during terminal convergence.\nHowever, we note that it is common practice to halt training algorithms before full minimization on the training set\noccurs, as this commonly leads to over-fitting, and EQUAL\u2019s fast early and mid-range convergence coincides with\nthe stage in tminiug when halting is likely. In practice, EQUAL\u2019s convergence speed maybe better than that of\nconjugate grfllent descent. The results on the Ionosphere data set were comparable.\nConclusion\nThe paper describes EQUAL, a novel second order training algorithm for feedforward neural networks. The\nalgorithm iteratively builds up an estimate of the leading eigen-subspace of the Hessian matrix, by applying the\nX{. } operator method to perform the orthogonal iteration algorithm. A simple quadratic estimation procedure is then\napplied along the axes of the eigen-subspace. The standard quick propagation procedure is applied to the portion of\nthe gradient orthogonal to this subspace, and then added to the subspace delta. By exploiting the independence of the\nquadratic function along eigenvectors, the algorithm accelerates convergence in comparison to quick propagation.\nIn comparison with the conjugate gradient descent algorithm EQUAL has superior performance in the early and\nmiddle stages of convergence, but inferior performance during terminal convergence. However, we not that the\nlatter stage is irrelevant in small- to medium-sized data sets, where excessive optimization on the training set merely\ninvites over-learning.\nThe algorithm is noteworthy in being scalable in terms of memory requirements - the dimension of the eigen-\nsubspace can be arbitrarily selected. Increasing the dimensionality reduces initial convergence, while improving\nterminal convergence, which invites selection of a good \u201ctrade-off\u2019 dimension. However, the algorithm has an\nunfortunate drawback in that the Gram-Schmidt orthogonalization procedure needs to be perfonm.d on each\niteration, and the computational efficiency of thk scales with the square of the dimension. Hence, the procedure is\nefficient only for a fairly small subspace.\nEQUAL presents a very novel approach to optimization, which maybe developed further to produce alternative\nimproved algorithms. For example, although quadratic estimation is natural in the eigen-subspace, it maybe\npossible to exploit a different technique in the orthogonal subspace. A particularly compelling possibility is to use\nQuasi-Newton in the orthogonal subspace, as this algorithm benefits greatly from even very modest reductions in\ndimensionality (each removed dimension halves the space requirements), and EQUAL is particularly efficient if a\nsmall number of dimensions are used. This will be the subject of further research.\nFinally, we note that the eigenestimates stabilize overtime. Consequently, the algorithm could be modified to search\nfor a single eigenestimate per epoch, progressively adding more as they stabilize. This variation would require only\n2 props per iteration, and might combine EQUAL1\u2019Sfast initial convergence with the superior terminal convergence\nof the higher order versions.\n0-7695-0619-4\/00\/$10.00 (C) 2000 IEEEProceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks (IJCNN'00)\nReferences\n[1] Bishop, C. (1995), Neural Networks for Pattern Recognition. Oxford University Press.\n[2] Shepherd, A.J. (1997). Second-Order Methodsfor Neural Networks. New York, Springer.\n[3] Press, W.H., Teukolsky, S.A., Vetterling, W.T. and Flannery, B.P. (1992). Numerical Recipes in C: The Art of\nScientzjic Computing (Seconded.). Cambridge University Press.\n[4] Saito, K. and Ryohei, N. (1996). Partial BFGS Update and Efficient Step-Length Calculation for Three-Layer\nNeural Networks, Neuron Computation 9 (1), 123-141.\nMoller, M (1993). A scaled conjugate gradient algorithm for fast supervised learning. Neural Networks 6 (4), 525-\n533.\n[6]Perlmutter, B.A. (1994). Fast exact multiplication by the Hessian. Neural Computation 6(1), 147-160.\n[7] Fahlmann, S.E. (1988). FasEr-learning variations on back-propagation: an empirical study. In D. Touretsky, G.E,\nHintorzand T.J. Sejnowski (Eds.), Proceedings of the 1988 Connectionist Models Summer School, 38-51. San\nMateo, CA Morgan-Kaufmazm.\n[8] Golub, G.H. and Van Loan, C.F. (1989). Matrix Computations. John Hopkins University Press, Baltimore.\n[9] Anton, H. and Rorres, C. (1994). Elementary Linear Algebra, ft\u2019 Edition, Wiley, New York.\n[10]. Sigillito, V.G., Wing, S.P., Hutton, L.V. and Baker, K.B. (1989). Classification of radar returns from the\nionosphere using neural networks. John Hopkins APL Technical Digest, 10, 262-266.\n0-7695-0619-4\/00\/$10.00 (C) 2000 IEEEProceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks (IJCNN'00)\n"}