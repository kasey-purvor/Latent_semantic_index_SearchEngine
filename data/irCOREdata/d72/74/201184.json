{"doi":"10.1111\/j.1368-423X.2008.00229.x","coreId":"201184","oai":"oai:eprints.lse.ac.uk:5398","identifiers":["oai:eprints.lse.ac.uk:5398","10.1111\/j.1368-423X.2008.00229.x"],"title":"Estimating GARCH models: when to use what?","authors":["Huang, Da","Wang, Hansheng","Yao, Qiwei"],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2008-03","abstract":"The class of generalized autoregressive conditional heteroscedastic (GARCH) models has proved particularly valuable in modelling time series with time varying volatility. These include financial data, which can be particularly heavy tailed. It is well understood now that the tail heaviness of the innovation distribution plays an important role in determining the relative performance of the two competing estimation methods, namely the maximum quasi-likelihood estimator based on a Gaussian likelihood (GMLE) and the log-transform-based least absolutely deviations estimator (LADE) (see Peng and Yao \\ud\n2003Biometrika,90, 967\u201375). A practically relevant question is when to use what. We provide in this paper a solution to this question. By interpreting the LADE as a version of the maximum quasilikelihood estimator under the likelihood derived from assuming hypothetically that the log-squared innovations obey a Laplace distribution, we outline a selection procedure based on some goodness-of-fit type statistics. The methods are illustrated with both simulated and real data sets. Although we deal with the estimation for GARCH models only, the basic idea may be applied to address the estimation procedure selection problem in a general regression setting","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/201184.pdf","fullTextIdentifier":"http:\/\/eprints.lse.ac.uk\/5398\/1\/Yao_etal_Estimating-GARCH-models-when-to-use-what_2008.pdf","pdfHashValue":"8b390f0193f28a25dc62b6b6c5681b4a48ec8fc3","publisher":"Wiley on behalf of the Royal Economic Society","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:eprints.lse.ac.uk:5398<\/identifier><datestamp>\n      2017-10-30T11:54:13Z<\/datestamp><setSpec>\n      74797065733D4445505453:4C53452D5354<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/eprints.lse.ac.uk\/5398\/<\/dc:relation><dc:title>\n        Estimating GARCH models: when to use what?<\/dc:title><dc:creator>\n        Huang, Da<\/dc:creator><dc:creator>\n        Wang, Hansheng<\/dc:creator><dc:creator>\n        Yao, Qiwei<\/dc:creator><dc:subject>\n        HA Statistics<\/dc:subject><dc:description>\n        The class of generalized autoregressive conditional heteroscedastic (GARCH) models has proved particularly valuable in modelling time series with time varying volatility. These include financial data, which can be particularly heavy tailed. It is well understood now that the tail heaviness of the innovation distribution plays an important role in determining the relative performance of the two competing estimation methods, namely the maximum quasi-likelihood estimator based on a Gaussian likelihood (GMLE) and the log-transform-based least absolutely deviations estimator (LADE) (see Peng and Yao \\ud\n2003Biometrika,90, 967\u201375). A practically relevant question is when to use what. We provide in this paper a solution to this question. By interpreting the LADE as a version of the maximum quasilikelihood estimator under the likelihood derived from assuming hypothetically that the log-squared innovations obey a Laplace distribution, we outline a selection procedure based on some goodness-of-fit type statistics. The methods are illustrated with both simulated and real data sets. Although we deal with the estimation for GARCH models only, the basic idea may be applied to address the estimation procedure selection problem in a general regression setting.<\/dc:description><dc:publisher>\n        Wiley on behalf of the Royal Economic Society<\/dc:publisher><dc:date>\n        2008-03<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:identifier>\n        http:\/\/eprints.lse.ac.uk\/5398\/1\/Yao_etal_Estimating-GARCH-models-when-to-use-what_2008.pdf<\/dc:identifier><dc:identifier>\n          Huang, Da and Wang, Hansheng and Yao, Qiwei  (2008) Estimating GARCH models: when to use what?  Econometrics Journal, 11 (1).  pp. 27-38.  ISSN 1368-4221     <\/dc:identifier><dc:relation>\n        http:\/\/www.blackwellpublishing.com\/journals\/ectj\/<\/dc:relation><dc:relation>\n        10.1111\/j.1368-423X.2008.00229.x<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":null,"relations":["http:\/\/eprints.lse.ac.uk\/5398\/","http:\/\/www.blackwellpublishing.com\/journals\/ectj\/","10.1111\/j.1368-423X.2008.00229.x"],"year":2008,"topics":["HA Statistics"],"subject":["Article","PeerReviewed"],"fullText":"  \nDa Huang, Hansheng Wang and Qiwei Yao \nEstimating GARCH models: when to use \nwhat? \n \nArticle (Accepted version) \n(Refereed) \n \n \n \nOriginal citation: \nHuang, Da, Wang, Hansheng and Yao, Qiwei (2008) Estimating GARCH models: when to use \nwhat? Econometrics Journal, 11 (1). pp. 27-38. ISSN 1368-423X  \nDOI: 10.1111\/j.1368-423X.2008.00229.x  \n \n\u00a9 2008 Royal Economic Society \n \nThis version available at: http:\/\/eprints.lse.ac.uk\/5398\/ \nAvailable in LSE Research Online: July 2014 \n \nLSE has developed LSE Research Online so that users may access research output of the \nSchool. Copyright \u00a9 and Moral Rights for the papers on this site are retained by the individual \nauthors and\/or other copyright owners. Users may download and\/or print one copy of any \narticle(s) in LSE Research Online to facilitate their private study or for non-commercial research. \nYou may not engage in further distribution of the material or use it for any profit-making activities \nor any commercial gain. You may freely distribute the URL (http:\/\/eprints.lse.ac.uk) of the LSE \nResearch Online website.  \n \nThis document is the author\u2019s final accepted version of the journal article. There may be \ndifferences between this version and the published version.  You are advised to consult the \npublisher\u2019s version if you wish to cite from it. \n \n \n \nEstimating GARCH Models: When to Use What?\nDa Huang1 Hansheng Wang1 Qiwei Yao2,1\n1Guanghua School of Management, Peking University, Beijing 100871, China\n2Department of Statistics, London School of Economics, London, WC2A 2AE, UK\nAbstract\nThe class of GARCH models has proved particularly valuable in modelling time series with\ntime varying volatility. These include financial data, which can be particularly heavy tailed. It\nis well understood now that the tail heaviness of the innovation distribution plays an important\nrole in determining the relative performance of the two competing estimation methods, namely\nthe maximum quasilikelihood estimator based on a Gaussian likelihood (GMLE) and the log-\ntransform based least absolutely deviations estimator (LADE); see Peng and Yao (2003). A\npractically relevant question is when to use what. We provide in this paper a solution to this\nquestion. By interpreting the LADE as a version of the maximum quasilikelihood estimator\nunder the likelihood derived from assuming hypothetically that the log squared innovations\nobey a Laplace distribution, we outline a selection procedure based on some goodness-of-fit\ntype statistics. The methods are illustrated with both simulated and real data sets. Although\nwe deal with the estimation for GARCH models only, the basic idea may be applied to address\nthe estimation procedure selection problem in a general regression setting.\nSome key words: Estimation procedure selection; GARCH; Gaussian likelihood; Heavy tail; Laplace dis-\ntribution; Least absolute deviations estimator; Maximum quasilikelihood estimator; Time series.\n1\n1 Introduction\nSeveral methods exist for estimating parameters in GARCH models with unknown innovation\ndistributions. The maximum quasilikelihood estimator facilitated by hypothetically assuming the\ninnovation distribution to be Gaussian is arguably the most frequently used estimator in prac-\ntice, which we simply call the Gaussian maximum likelihood estimator (GMLE). The asymptotic\nproperties of the GMLE is fully understood now. In fact, it is a well behaved estimator when the\ninnovation distribution has finite fourth moment. However when the innovation distribution is\nheavy tailed with an infinite fourth moment, the estimators may not be asymptotically normal,\nthe range of possible limit distributions is extraordinarily large, and the convergence rate is slower\nthan the standard rate of n1\/2; see, e.g. Hall & Yao (2003). To overcome the drawbacks due to the\npossible slow convergence rates of the GMLE, Peng & Yao (2003) propose a log-transform based\nleast absolute deviations estimator (LADE) as an alternative which is robust with respect to the\nheavy tails of the innovation distribution. In fact the LADE is asymptotically normal with the\nstandard convergence rate n1\/2 under the assumption that the second moment of the innovation\ndistribution is finite. Monte Carlo experiments reported in Peng & Yao (2003) indicate that the\nrelative performance of the two estimators hinges critically on the tail heaviness of the innova-\ntion distribution. Indeed LADE is preferred for the processes with very heavy tailed innovation\ndistributions.\nIn practice the innovation distribution is unknown. A practically relevant question is how to\nchoose an appropriate estimator for a given practical situation. In this paper we put forward a\nproposal to choose between the GMLE and the LADE based on some goodness-of-fit measures.\nTo this end, we view the LADE also as a maximum quasilikelihood estimator based on the\nhypothesis that the log squared innovations follow a Laplace distribution. Our approach is based\non the intuition that we should use the GMLE if the innovation distribution is close to a normal\ndistribution, and use the LADE if the distribution of the log squared innovations is close to a\nLaplace distribution. Some goodness-of-fit statistics are defined to measure the closeness of those\ndistributions; see section 2.3 below. We have shown that our selection procedure is consistent in\nthe sense that the probability of choosing the \u2018correct\u2019 estimator converges to 1. The numerical\nexperiments illustrate that the proposed procedure exhibits desirable finite sample performance.\nAlthough we only deal with the estimation for GARCH models in this paper, the general idea\n2\nmay be applied for selecting, for example, between L1 and L2 estimator in a general regression\nsetting; see the relevant discussion in section 4.\nThe asymptotic properties of the GMLE have been studied initially by Weiss (1986) for pure\nARCH(p) processes, by Lee & Hansen (1994) and Lumsdaine (1996) for GARCH(1,1) processes,\nunder the assumption that the innovation distribution has finite fourth moment. Further stud-\nies for general GARCH(p, q) processes without the condition of fourth finite moment may be\nfound in Hall & Yao (2003), Berkes, Horva\u00b4th & Kokoszka (2003), Straumann & Mikosch (2006),\nand Mikosch & Straumann (2006). See also Straumann (2005). Complex asymptotic proper-\nties were also observed from a Whittle estimator by Giraitis & Robinson (2001) for heavy tailed\nGARCH(1,1) models. The asymptotic properties of Lp-estimators for ARCH(p) models were\nestablished by Horvath and Liese (2004).\nThe rest of the paper is organized as follows. The methodology is presented in section 2. It\nalso contains a consistency result. Section 3 reports numerical illustrations with both simulated\nand real data sets. Miscellaneous remarks are given in section 4. The technical proof is relegated\nto the Appendix.\n2 Methodology\n2.1 Model\nA generalized autoregressive conditional heteroscedastic, GARCH, model with orders p \u2265 1 and\nq \u2265 0 is defined as\nXt = \u03c3t\u03b5t, and \u03c3\n2\nt \u2261 \u03c3t(\u03b8)\n2 = c+\np\u2211\ni=1\nbiX\n2\nt\u2212i +\nq\u2211\nj=1\naj\u03c3\n2\nt\u2212j , (2.1)\nwhere c > 0, bj \u2265 0 and aj \u2265 0 are unknown parameters, \u03b8 = (c, b1, \u00b7 \u00b7 \u00b7 , bp, a1, \u00b7 \u00b7 \u00b7 , aq)\nT, {\u03b5t} is a\nsequence of independent and identically distributed random variables with mean 0 and variance 1,\nand \u03b5t is independent of {Xt\u2212k, k \u2265 1} for all t. The distribution of \u03b5t is unknown. When q = 0,\n(2.1) reduces to an autoregressive conditional heteroscedastic, ARCH, model. The necessary and\nsufficient condition for (2.1) to define a unique strictly stationary process {Xt, t = 0,\u00b11,\u00b12, \u00b7 \u00b7 \u00b7 }\nwith EX2t <\u221e is that\np\u2211\ni=1\nbi +\nq\u2211\nj=1\naj < 1. (2.2)\n3\nFurthermore, for such a stationary solution, EXt = 0 and var(Xt) = c\/(1 \u2212\n\u2211p\ni=1 bi \u2212\n\u2211q\nj=1 aj);\nsee Giraitis et al. (2000), and also Theorem 4.4 of Fan & Yao (2003). Under condition (2.2),\n\u03c32t = \u03c3t(\u03b8)\n2 may be expressed as\n\u03c3t(\u03b8)\n2 =\nc\n1\u2212\n\u2211q\nj=1 aj\n+\np\u2211\ni=1\nbiX\n2\nt\u2212i +\np\u2211\ni=1\nbi\n\u221e\u2211\nk=1\nq\u2211\nj1=1\n\u00b7 \u00b7 \u00b7\nq\u2211\njk=1\naj1 \u00b7 \u00b7 \u00b7 ajkX\n2\nt\u2212i\u2212j1\u2212\u00b7\u00b7\u00b7\u2212jk\n, (2.3)\nwhere the multiple sum vanishes if q = 0; see Hall & Yao (2003).\n2.2 Two estimators\nThe GMLE is defined as\n\u03b8\u0302 = argmin\n\u03b8\nn\u2211\nt=\u03bd+1\n[ X2t\n\u03c3\u02dct(\u03b8)2\n+ log{\u03c3\u02dct(\u03b8)\n2}\n]\n, (2.4)\nwhere \u03c3\u02dct(\u03b8)\n2 is a truncated version of \u03c3t(\u03b8)\n2 defined as\n\u03c3\u02dct(\u03b8)\n2 =\nc\n1\u2212\n\u2211q\nj=1 aj\n+\nmin(p,t\u22121)\u2211\ni=1\nbiX\n2\nt\u2212i +\np\u2211\ni=1\nbi\n\u221e\u2211\nk=1\nq\u2211\nj1=1\n\u00b7 \u00b7 \u00b7\nq\u2211\njk=1\naj1 \u00b7 \u00b7 \u00b7 ajk (2.5)\n\u00d7 X2t\u2212i\u2212j1\u2212\u00b7\u00b7\u00b7\u2212jkI(t\u2212 i\u2212 j1 \u2212 \u00b7 \u00b7 \u00b7 \u2212 jk \u2265 1),\nwhich depends on the observations Xt\u22121, \u00b7 \u00b7 \u00b7 , X1 only; c.f. (2.3), and \u03bd \u2265 1 is an integer which\ncontrols the effect of the truncation. Note for a purely ARCH model (i.e. q = 0), we choose \u03bd = p.\nThe GMLE can be motivated by temporarily assuming that \u03b5t \u223c N(0, 1). Given {Xk, k \u2264 \u03bd}\nwith \u03bd \u2265 max(p, q), the conditional density function of X\u03bd+1, \u00b7 \u00b7 \u00b7 , Xn is then proportional to\n{ n\u220f\nt=\u03bd+1\n\u03c3t(\u03b8)\n2\n}\u22121\/2\nexp\n{\n\u2212\n1\n2\nn\u2211\nt=\u03bd+1\nX2t\n\u03c3t(\u03b8)2\n}\n.\nMaximizing this (conditional) likelihood with \u03c3t(\u03b8)\n2 replaced by \u03c3\u02dct(\u03b8)\n2 leads to the GMLE esti-\nmator \u03b8\u0302; see (2.4).\nThe LADE, proposed by Peng and Yao (2003), requires a different parametrization as follows.\nLet C0 > 0 be a constant such that the median of e\n2\nt is equal to 1, where et = C\n1\/2\n0 \u03b5t. Then (2.1)\nmay now be expressed as\nXt = stet, and s\n2\nt \u2261 st(\u03b1)\n2 = \u03b3 +\np\u2211\ni=1\n\u03b2iX\n2\nt\u2212i +\nq\u2211\nj=1\najs\n2\nt\u2212j , (2.6)\nwhere s2t = \u03c3\n2\nt \/C0, \u03b3 = c\/C0, \u03b2i = bi\/C0, and \u03b1 = (\u03b3, \u03b21, \u00b7 \u00b7 \u00b7 , \u03b2p, a1, \u00b7 \u00b7 \u00b7 , aq)\nT . Note that now\nlog(X2t ) = log{st(\u03b1)\n2}+ log(e2t ), (2.7)\n4\nand the median of log(e2t ) is 0. Thus the true value of \u03b1 minimizes E| log(X\n2\nt ) \u2212 log{st(\u03b1)\n2}|.\nThis motivates the LADE\n\u03b1\u0302 = argmin\n\u03b1\nn\u2211\nt=\u03bd+1\n| log(X2t )\u2212 log{s\u02dct(\u03b1)\n2}|, (2.8)\nwhere s\u02dct(\u03b1)\n2 is a truncated version of st(\u03b1)\n2 defined as\ns\u02dct(\u03b1)\n2 =\n\u03b3\n1\u2212\n\u2211q\nj=1 aj\n+\nmin(p,t\u22121)\u2211\ni=1\n\u03b2iX\n2\nt\u2212i +\np\u2211\ni=1\n\u03b2i\n\u221e\u2211\nk=1\nq\u2211\nj1=1\n\u00b7 \u00b7 \u00b7\nq\u2211\njk=1\naj1 \u00b7 \u00b7 \u00b7 ajk (2.9)\n\u00d7 X2t\u2212i\u2212j1\u2212\u00b7\u00b7\u00b7\u2212jkI(t\u2212 i\u2212 j1 \u2212 \u00b7 \u00b7 \u00b7 \u2212 jk \u2265 1),\nwhich directly follows from (2.5).\nIn fact the LADE may also be viewed as a maximum quasilikelihood estimator by temporarily\nassuming log(e2t ) having a Laplace distribution with density to 0.5\u03bb exp(\u2212\u03bb|x|), where \u03bb > 0 is a\nconstant. By (2.7), the (conditional) likelihood function based on the observations X\u03bd+1 \u00b7 \u00b7 \u00b7 , Xn\n(given {Xk, k \u2264 \u03bd}) is then proportional to\nexp\n[\n\u2212 \u03bb\nn\u2211\nt=\u03bd+1\n| log(X2t )\u2212 log{st(\u03b1)\n2}|\n]\n.\nMaximizing this with st(\u03b1)\n2 replaced by s\u02dct(\u03b1)\n2 leads to the LADE \u03b1\u0302; see (2.8). Note E(\u03b52t ) <\u221e\nif log(e2t ) has the above Laplace distribution with \u03bb < 1.\n2.3 Selecting an estimation procedure\nThe performance of \u03b8\u0302 and \u03b1\u0302 hinges critically on the tail heaviness of the innovation distribution.\nWhen E(|\u03b5t|\n4\u2212\u03b4) <\u221e for any \u03b4 > 0, \u03b8\u0302 is asymptotically normal. Furthermore the convergence rate\nis the standard n1\/2 provided E(\u03b54t ) <\u221e. When \u03b5t is heavy tailed in the sense that E(|\u03b5t|\nd) =\u221e\nfor some 2 < d < 4, the asymptotic distribution of \u03b8\u0302 is no longer normal with a convergence\nrate slower than n1\/2, and it depends on infinite many unknown parameters of the underlying\ndistribution. Those asymptotic results have been established under different settings by, for\nexample, Lee and Hansen (1994), Lumsdaine (1996), Hall and Yao (2003), Berkes et al. (2003),\nStraumann and Mikosch (2006), and Mikosch and Straumann (2006). On the other hand, the\nLADE \u03b1\u0302 is always asymptotically normal with the convergence rate n1\/2 provided E(\u03b52t ) < \u221e.\nSimulation studies also indicate that the finite sample performance of the LADE is better than\nthat of the GMLE when, for example, E(|\u03b5t|\n3) =\u221e. See Peng and Yao (2003).\n5\nSince the distribution of \u03b5t is unknown in practice, it is rather difficult, if not impossible, to\ninference on how many moments \u03b5t has. A pertinent question is which estimator, between the\nGMLE and the LADE, we should use in practice. We provide an answer to this question below.\nIf we knew the distribution of innovations \u03b5t, the genuine maximum (conditional) likelihood\nestimator would be used. Intuitively we would expect that the GMLE is a better option when the\ndistribution of \u03b5t is close to N(0, 1), and the LADE is better when the distribution of log(e\n2\nt ) is\napproximately a Laplace distribution; see the discussion at the end of section 2.2. This suggests\nthat we may compare the closeness of those two pair distributions to select a good estimation\nprocedure.\nDenoted by \u03a6(\u00b7) the N(0, 1) distribution function, and by G(\u00b7) the distribution function with\nthe density function 0.25 exp(\u2212|x|\/2). Let \u03b5\u0302t = Xt\/\u03c3\u02dct(\u03b8\u0302) be the residuals derived from the GMLE.\nIn practice, we standardize \u03b5\u0302t such that the first two sample moments are 0 and 1. Let e\u0302t =\nXt\/s\u02dct(\u03b1\u0302) be the residuals derived from the LADE. In practice, we \u2018standardize\u2019 e\u0302t such that\nthe sample median of e\u03022t is 1 and the sample mean of | log(e\u0302\n2\nt )| is 2. This may be achieved by\nletting log(e\u03022t ) = C1 log{C2X\n2\nt \/s\u02dct(\u03b1\u0302)\n2} for appropriate positive constants C1 and C2. Note that\n\u03a6(\u03b5t) \u223c U(0, 1) when \u03b5t \u223c N(0, 1), and G{log(e\n2\nt )} \u223c U(0, 1) when G(\u00b7) is the distribution\nfunction of log(e2t ). Let F\u0302n,1(\u00b7) be the empirical distribution of {\u03a6(\u03b5\u0302t), \u03bd < t \u2264 n}, and F\u0302n,2(\u00b7)\nthe empirical distribution of [G{log(e\u03022t )}, \u03bd < t \u2264 n]. We define the goodness-of-fit statistics\nbelow to measure the distances between F\u0302n,i and the uniform distribution on (0, 1).\nTMLE =\n\u222b 1\n0\n|F\u0302n,1(x)\u2212 x|dx, TLADE =\n\u222b 1\n0\n|F\u0302n,2(x)\u2212 x|dx. (2.10)\nObviously these statistics are reminiscent of the Crame\u00b4r-von Mises goodness-of-fit statistics. In\npractical implementation, we use the Riemann approximations of these integrals:\nTMLE =\nn\u2211\nt=\u03bd+1\n\u2223\u2223 t\u2212 \u03bd\nn\u2212 \u03bd\n\u2212 ut\n\u2223\u2223(ut \u2212 ut\u22121), TLADE = n\u2211\nt=\u03bd+1\n\u2223\u2223 t\u2212 \u03bd\nn\u2212 \u03bd\n\u2212 vt\n\u2223\u2223(vt \u2212 vt\u22121), (2.11)\nwhere u\u03bd+1 \u2264 u\u03bd+2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 un are the order statistics of {\u03a6(\u03b5\u0302t), \u03bd < t \u2264 n}, and v\u03bd+1 \u2264 v\u03bd+2 \u2264\n\u00b7 \u00b7 \u00b7 \u2264 vn the order statistics of [G{log(e\u0302\n2\nt )}, \u03bd < t \u2264 n].\nSelection rule: we use the LADE if TMLE > TLADE, and the GMLE otherwise.\nLet F1 and F2 denote, respectively, the distribution function of \u03a6(\u03b5t) andG{log(e\n2\nt )}. Theorem\n1 below indicates that the selection role defined above is consistent. Its proof is given in the\nAppendix.\n6\nTheorem 1. Let {Xt} be defined by (2.1) for which condition (2.2) holds. Let \u03bd \u2192 \u221e and\n\u03bd\/n\u2192 0 as n\u2192\u221e. Suppose that for some constant \u03ba1, \u03ba2 > 0,\n||\u03b8\u0302 \u2212 \u03b8|| = OP (n\n\u2212\u03ba1), ||\u03b1\u0302\u2212 \u03b1|| = OP (n\n\u2212\u03ba2). (2.12)\nFurthermore, for any constant \u03b40 > 0, there exists \u03b4 > 0 for which\nsup\n0\u2264x\u22641\n|F1(x+ \u03b4)\u2212 F1(x\u2212 \u03b4)| < \u03b40, sup\n0\u2264x\u22641\n|F2(x+ \u03b4)\u2212 F2(x\u2212 \u03b4)| < \u03b40. (2.13)\nThen as n\u2192\u221e, P (TMLE > TLADE)\u2192 1 provided\u222b 1\n0\n|F1(x)\u2212 x|dx >\n\u222b 1\n0\n|F2(x)\u2212 x|dx. (2.14)\nCondition (2.12) requires that both the GMLE and the LADE are, respectively, n\u03ba1 and\nn\u03ba2 consistent, which has been established under certain regularity conditions. For the LADE,\n\u03ba2 = 1\/2 (Peng & Yao 2003). For the GMLE, the value of \u03ba1 is related to the tail heaviness of the\ndistribution of \u03b5t. In fact such a positive \u03ba1 always exists for the GMLE when E(\u03b5\n2\nt ) < \u221e (Hall\n& Yao 2003, Mikosch & Straumann 2006, and Straumann 2005). Condition (2.13) is fulfilled if,\nfor example, both F1 and F2 admit bounded probability density functions.\n3 Numerical illustration\nIn this section, we first illustrate the proposed selection procedure with the data simulated from\nGARCH(1,1) and ARCH(2) models. In both cases we took the errors \u03b5t to beN(0, 1), standardized\nt or skewed t with d = 3 or 6 degrees of freedom. A skewed t random variable is defined as\n(0.8|V0|+ 0.6V1)\/(V2\/d)\n1\/2,\nwhere V0 and V1 are N(0, 1) random variables, V2 \u223c \u03c7\n2(d), and V0, V1 and V2 are independent with\neach other. See Azzalini and Capitanio (2003). We also used \u03b5t such that log(\u03b5\n2\nt ) is of a Laplace\ndistribution. We further experimented with the semi-strong ARCH\/GARCH models defined in\nterms of martingale difference innovations\n\u03b5t = sgn(\u03bet)[1 + (\u03b7\n2\nt \u2212 1)\/{1 + exp(\u03c3\n2\nt )}]\n1\/2,\nwhere \u03bet and \u03b7t are independent N(0, 1) random variables. We used c = 1, (b1, b2) = (0.7, 0.2) for\nthe ARCH(2) model, and (b1, a1) = (0.2, 0.7) for the GARCH(1,1) model. Setting the sample size\n7\nTable 1: Simulation results for GARCH(1,1) model \u2013 relative frequencies for the oc-\ncurrences of the events {TLADE < TMLE} and {ELADE < EMLE} in 200 replications.\nDistribution of \u03b5t n TLADE < TMLE ELADE < EMLE\nN(0, 1) 250 0.000 0.290\n500 0.000 0.235\n1000 0.000 0.295\nt(6) 250 0.030 0.360\n500 0.000 0.425\n1000 0.000 0.450\nskewed t(6) 250 0.065 0.430\n500 0.065 0.470\n1000 0.005 0.490\nt(3) 250 0.575 0.655\n500 0.680 0.650\n1000 0.790 0.695\nskewed t(3) 250 0.805 0.710\n500 0.925 0.725\n1000 0.965 0.775\nLaplace 250 1.000 0.745\n500 1.000 0.765\n1000 1.000 0.845\nMartingale 250 1.000 0.070\ndifference 500 1.000 0.850\n1000 1.000 0.730\nn = 250, 500 and 1000, we drew 200 samples for each setting. We used \u03bd = 20 in the estimation\nfor GARCH models.\nThe relative frequencies for the occurrence of the event {TLADE < TMLE} in the 200 replications\nare listed in Table 1 for GARCH(1,1) model, and in Table 2 for ARCH(2) model. We also included\nin the tables the relative frequencies of the occurrence of the event {ELADE < EMLE}, where the\nestimation errors are defined as\nELADE =\np\u2211\ni=1\n|\u03b2\u0302i\/\u03b3\u0302 \u2212 bi\/c|+\nq\u2211\nj=1\n|a\u0302j \u2212 aj |, EMLE =\np\u2211\ni=1\n|\u0302bi\/c\u0302\u2212 bi\/c|+\nq\u2211\nj=1\n|a\u0302j \u2212 aj |,\nsee (2.6) and (2.1). For the models with normal innovations, the GMLE is the genuine MLE, and\nis always the preferred estimator according to our selection procedure. On the other hand, the\nLADE is always selected when log(e2t ) has the Laplace distribution. For the models with t(d)-\ninnovations, the results are less clear-cut. Overall the GMLE is preferred when d = 6 while the\nLADE is preferred when d = 3. Furthermore the preference for the GMLE when d = 6 and that\n8\nTable 2: Simulation results for ARCH(2) model \u2013 relative frequencies for the occur-\nrences of the events {TLADE < TMLE} and {ELADE < EMLE} in 200 replications.\nDistribution of \u03b5t n TLADE < TMLE ELADE < EMLE\nN(0, 1) 250 0.000 0.290\n500 0.000 0.260\n1000 0.000 0.260\nt(6) 250 0.015 0.415\n500 0.000 0.470\n1000 0.000 0.450\nskewed t(6) 250 0.075 0.510\n500 0.055 0.460\n1000 0.010 0.505\nt(3) 250 0.570 0.605\n500 0.675 0.625\n1000 0.770 0.725\nskewed t(3) 250 0.840 0.700\n500 0.890 0.730\n1000 0.975 0.785\nLaplace 250 1.000 0.795\n500 1.000 0.825\n1000 1.000 0.800\nMartingale 250 1.000 0.170\ndifference 500 1.000 0.930\n1000 1.000 0.995\nfor the LADE when d = 3 increase when the sample size n increases. The models with skewed\nt-innovations tend to be in favour of LADE more often than those with (centered) t-innovations\nwith the same degrees of freedom. For semi-strong GARCH\/ARCH models with martingale\ndifference innovations, the LADE is preferred. This may be due to the fact that the innovation\ndistribution is very different from normal and L1 estimation is more robust. Overall there is a\nclear synchrony between the occurrences of the two events {TLADE < TMLE} and {ELADE < EMLE};\nindicating that overall the preferred method by the T -measures leads to more accurate estimates\nfor the parameters.\nNow we apply the method to two centered daily return series: the Switzerland stock index\n(SWI) in 2 January 1991 \u2013 31 December 1998, and the B Share of the Shanghai Stock Exchange\n(SHB) in 2 January 2001 \u2013 31 December 2004. The length of the series are, respectively, 1859\nand 946. The P -value of the Jarque-Bear Test is 0.000 for both the series, and the kurtosis\nis 5.72665 for SWI and 5.761476 for SHB. For each of those two series, we fit the first half\n9\nseries with GARCH(1,1) models using both the GMLE and the LADE. The sample size used in\nthe estimations is n = 930 for SWI, and 473 SHB. The values of the goodness-of-fit statistics\n(TMLE, TLADE) are (0.026, 0.057) for SWI, and (0.044, 0.041) for SHB. Thus our selection rule\nprefers the GMLE for SWI, and the LADE for SHB.\nWith the sample size fixed at n = 930 for SWI and n = 473 for SHB, we also perform one-\nstep ahead prediction of the squared returns for each of the second half series. The prediction is\nbased on the fitted GARCH(1,1) models using both the GMLE and the LADE. With LADE, the\npredicted squared returns are of the form s\u03022tSe, where Se is the sample variance of the residuals\ne\u0302j \u2261 Xj\/s\u0302j (j < t); see (2.6). The root mean squares error of the prediction based on the GMLE\nis 1.750 for SWI, and 4.757 for SHB. The root mean squares error based on the LADE is 2.715\nfor SWI, and 2.894 for SHB. Thus the GMLE provided the more accurate prediction for SWI\nwhile the LADE predicted SHB better. This shows that the estimation method preferred by our\nselection rule also provided better prediction.\n4 Miscellaneous remarks\nAlthough we deal with the estimation for GARCH models only in this paper, the idea may apply\nto select an appropriate estimation method in, for example, a general regression model\ny = f(X) + \u03b5. (4.1)\nWhen f is known up to some unknown parameters, it is a parametric regression model. When\nf is completely unknown, it is a nonparametric regression problem. Nevertheless both the least\nsquares estimation (LSE) and least absolutely deviations estimation (LADE) are well-developed in\nboth parametric and nonparametric setting. Intuitively LSE should be used when the distribution\nof \u03b5 is close to a normal distribution while LADE should be used when the distribution of \u03b5 is\nclose to a Laplace distribution. The procedure presented in section 3.2 is readily applicable for\nthe selection between those two estimation methods.\nThe above problem may be seen as to choose the most relevant distribution from the union\nof the normal distribution family and the Laplace distribution family. In this sense it is a kind\nof model selection problem. However we argue that such an estimation-selection problem is\ndifferent from the conventional model-selection problems often featured in statistical literature.\nThe standard information criteria such as the AIC are designed to select the most relevant model\n10\nfrom a given smooth parametric family under the assumption that the family contains the true\nmodel as one of its members. When the truth is not in the family, the criteria such the TIC\n(Takeuchi 1976, Konishi and Kitagawa 1996) may be used to select the \u2018best\u2019 approximation for\nthe truth within the given family. However to our best knowledge, no criteria may be applied to\nselect an \u2018optimum\u2019 approximate model for the truth across two or more parametric families. The\nlack of such a criterion is due to the fact that the maximum likelihood principle may not apply\nacross different distribution families.\nWe may embed the two distribution families into one via, for example, a convex combination.\nThis is to consider, for the regression model (4.1), the error distribution family\npiN(0, \u03c32) + (1\u2212 pi)L(0, \u03bb), pi \u2208 [0, 1], \u03c32 > 0, \u03bb > 0,\nwhere L(0, \u03bb) denotes the Laplace distribution centered at 0 and with scale parameter \u03bb. Now\nthe MLE for pi is typically neither 0 nor 1. Consequently the MLE for f(\u00b7) is neither LSE nor\nLADE. Therefore this approach, though legitimate on its own, would not provide an answer to\nthe problem concerned.\nAcknowledgment\nH. Wang acknowledges the support of the Chinese NSF grant #10771006. Q. Yao\u2019s work was\npartially supported by the U.K. Engineering and Physical Sciences Research Council. We thanks\ntwo referees for helpful comments and suggestions.\nAppendix: Proof of Theorem 1\nWe use the same notation as in section 2. Put Ut = \u03a6(\u03b5t), U\u0302t = \u03a6(\u03b5\u0302t), Vt = G{log(e\n2\nt )} and\nV\u0302t = G{log(e\u0302\n2\nt )}. Let An = {||\u03b8\u0302 \u2212 \u03b8|| < n\n\u2212\u03ba1\/2} and Bn = {||\u03b1\u0302 \u2212 \u03b1|| < n\n\u2212\u03ba2\/2}. It follows from\n(2.12) that P (An) \u2192 1 and P (Bn) \u2192 1. Denote by \u2018\nP\n\u2212\u2192\u2019 the convergence in probability, and\nC,C1 and C2 some generic positive constants which may be different at different places. We split\nthe proof into several lemmas. We assume that the conditions of Theorem 1 always hold in this\nappendix.\nLemma 1. As n\u2192\u221e, it holds that\n\u2211\nt>\u03bd E\n{\n|Xt|\n\u2223\u2223\u03c3t(\u03b8)\u2212 \u03c3\u02dct(\u03b8)\u2223\u2223}\u2192 0.\n11\nProof. It follows from (2.3) and (2.5) that for any t > p,\nE|\u03c3t(\u03b8)\n2 \u2212 \u03c3\u02dct(\u03b8)\n2| \u2264 E(X2t )\np\u2211\ni=1\nbi\n\u2211\nk\u2265(t\u2212p)\/q\nq\u2211\nj1=1\n\u00b7 \u00b7 \u00b7\nq\u2211\njk=1\naj1 \u00b7 \u00b7 \u00b7 ajk\n\u2264 E(X2t )\np\u2211\ni=1\nbi\n(a1 + \u00b7 \u00b7 \u00b7+ aq)\n(t\u2212p)\/q\n1\u2212 (a1 + \u00b7 \u00b7 \u00b7 aq)1\/q\n,\nsee also (2.2). Hence\n\u2211\nt>\u03bd\n[E|\u03c3t(\u03b8)\n2\u2212 \u03c3\u02dct(\u03b8)\n2|]1\/2 \u2264 C\n\u2211\nt>\u03bd\n(a1+ \u00b7 \u00b7 \u00b7+aq)\n(t\u2212p)\/(2q) \u2264 C\n(a1 + \u00b7 \u00b7 \u00b7+ aq)\n(\u03bd\u2212p)\/(2q)\n1\u2212 (a1 + \u00b7 \u00b7 \u00b7 aq)1\/(2q)\n\u2192 0. (A.1)\nNote that E(X2t ) <\u221e, which is ensured by (2.2). By (A.1), it holds that\nn\u2211\nt=\u03bd+1\nE\n{\n|Xt|\n\u2223\u2223\u03c3t(\u03b8)\u2212 \u03c3\u02dct(\u03b8)\u2223\u2223} \u2264 C\u2211\nt>\u03bd\n[E{|\u03c3t(\u03b8)\u2212 \u03c3\u02dct(\u03b8)|\n2}]1\/2 \u2264 C\n\u2211\nt>\u03bd\n[E{|\u03c3t(\u03b8)\n2\u2212 \u03c3\u02dct(\u03b8)\n2|}]1\/2 \u2192 0.\nThis completes the proof. \u0004\nLemma 2. As n\u2192\u221e, (n\u2212 \u03bd)\u22121\n\u2211\n\u03bd<t\u2264nE\n\u2223\u2223Xt{\u03c3\u02dct(\u03b8\u0302)\u2212 \u03c3\u02dct(\u03b8)}I(An)\u2223\u2223\u2192 0.\nProof. It holds on the set An that\n\u2211\n1\u2264j\u2264q a\u0302j is bounded from the above by a constant smaller\nthan 1 for all sufficiently large n.\nReplace the sum over 1 \u2264 k < \u221e in the third term on the RHS of (2.5) by the sum over\n1 \u2264 k \u2264 n\u03ba1\/4, and denote by \u03c3\u02c7t(\u03b8)\n2 the resulting function on the RHS. Then similar to Lemma\n1, we may show that\nn\u2211\nt=\u03bd+1\nE[|Xt|{|\u03c3\u02dct(\u03b8\u0302)\u2212 \u03c3\u02c7t(\u03b8\u0302)|+ |\u03c3\u02dct(\u03b8)\u2212 \u03c3\u02c7t(\u03b8)|}I(An)]\u2192 0. (A.2)\nOn the other hand,\n1\nn\u2212 \u03bd\nn\u2211\nt=\u03bd+1\nE{|Xt||\u03c3\u02c7t(\u03b8\u0302)\u2212 \u03c3\u02c7t(\u03b8)|I(An)} \u2264\nC\nn\u2212 \u03bd\nn\u2211\nt=\u03bd+1\n[E{|\u03c3\u02c7t(\u03b8\u0302)\u2212 \u03c3\u02c7t(\u03b8)|\n2I(An)}]\n1\/2\n\u2264\nC\nn\u2212 \u03bd\nn\u2211\nt=\u03bd+1\n[E{|\u03c3\u02c7t(\u03b8\u0302)\n2 \u2212 \u03c3\u02c7t(\u03b8)\n2|I(An)}]\n1\/2 \u2264 C1{E(X\n2\nt )}\n1\/2n\u2212\u03ba1\/2n\u03ba1\/4 \u2192 0.\nThe result required follows from this and (A.2). \u0004\nLemma 3. For any given constant x,\n(i) sup0\u2264x\u22641\n1\nn\u2212\u03bd\n\u2211\n\u03bd<t\u2264n |I(U\u0302t \u2264 x)\u2212 I(Ut \u2264 x)|\nP\n\u2212\u2192 0, and\n(ii) sup0\u2264x\u22641\n1\nn\u2212\u03bd\n\u2211\n\u03bd<t\u2264n |I(V\u0302t \u2264 x)\u2212 I(Vt \u2264 x)|\nP\n\u2212\u2192 0.\n12\nProof. We prove (i) first. Since the standard normal density function is bounded, it holds that\n|U\u0302t \u2212 Ut| \u2264 C|\u03b5\u0302t \u2212 \u03b5t| \u2264 C1|Xt||\u03c3\u02dct(\u03b8\u0302)\u2212 \u03c3t(\u03b8)|\/\u03c3\u02dct(\u03b8\u0302).\nNote that 1\/\u03c3\u02dct(\u03b8\u0302) is bounded from above by a finite constant on the set An for all sufficiently\nlarge n. It follows from Lemmas 1 and 2 that (n \u2212 \u03bd)\u22121\n\u2211\n\u03bd<t\u2264nE{|U\u0302t \u2212 Ut|I(An)} \u2192 0. This\nimplies that for any \u03b4 > 0,\n1\nn\u2212 \u03bd\nn\u2211\nt=\u03bd+1\nI(|U\u0302t \u2212 Ut| > \u03b4, An)\nP\n\u2212\u2192 0. (A.3)\nNote that\n|I(U\u0302t \u2264 x)\u2212 I(Ut \u2264 x)| \u2264 I(U\u0302t \u2264 x, Ut > x) + I(U\u0302t > x,Ut \u2264 x)\n\u2264 I(|U\u0302t \u2212 Ut| > \u03b4) + I(Ut \u2208 [x\u2212 \u03b4, x+ \u03b4]).\nTherefore\nsup\nx\n1\nn\u2212 \u03bd\nn\u2211\nt=\u03bd+1\n|I(U\u0302t \u2264 x)\u2212 I(Ut \u2264 x)| \u2264\n1\nn\u2212 \u03bd\nn\u2211\nt=\u03bd+1\nI(|U\u0302t \u2212 Ut| > \u03b4,An) (A.4)\n+ sup\nx\n1\nn\u2212 \u03bd\nn\u2211\nt=\u03bd+1\nI(Ut \u2208 [x\u2212 \u03b4, x+ \u03b4]) + I(A\nc\nn).\nFor any given \u03b40 > 0, P (A\nc\nn) < \u03b40 for all sufficiently large n. Note that\nsup\nx\n1\nn\u2212 \u03bd\nn\u2211\nt=\u03bd+1\nI(Ut \u2208 [x\u2212 \u03b4, x+ \u03b4])\n\u2264 sup\nx\n\u2223\u2223 1\nn\u2212 \u03bd\nn\u2211\nt=\u03bd+1\nI(Ut \u2208 [x\u2212 \u03b4, x+ \u03b4])\u2212 F1(x+ \u03b4) + F1(x\u2212 \u03b4)\n\u2223\u2223+ sup\nx\n|F1(x+ \u03b4)\u2212 F1(x\u2212 \u03b4)|.\nBy the Glivenko-Cantelli Theorem (p.284 of Chow and Teicher 1997), the first term on the RHS\nof the above expression converges to 0 almost surely. Condition (2.13) implies that the second\nterm may be smaller than \u03b40 by choosing \u03b4 sufficiently small. Now the result follows from (A.4)\nand (A.3).\nNow we prove (ii). Since a Laplace density function is bounded,\n|V\u0302t \u2212 Vt| \u2264 C| log(e\u0302\n2\nt )\u2212 log(e\n2\nt )| = C| log{st(\u03b1)\n2\/s\u02dct(\u03b1\u0302)\n2}|\n\u2264 C[| log{st(\u03b1)\n2\/s\u02dct(\u03b1)\n2}|+ | log{s\u02dct(\u03b1)\n2\/s\u02dct(\u03b1\u0302)\n2}|].\n13\nNote that for x \u2265 0, log(1 + x) \u2264 x, and st(\u03b1)\n2 \u2265 s\u02dct(\u03b1)\n2 \u2265 \u03b3 > 0. Hence the above expression\nimplies that\n|V\u0302t \u2212 Vt| \u2264 C\n[ |st(\u03b1)2 \u2212 s\u02dct(\u03b1)2|\ns\u02dct(\u03b1)2\n+ |s\u02dct(\u03b1)\n2 \u2212 s\u02dct(\u03b1\u0302)\n2|\n{I{s\u02dct(\u03b1)2 > s\u02dct(\u03b1\u0302)2}\ns\u02dct(\u03b1\u0302)2\n+\nI{s\u02dct(\u03b1)\n2 \u2264 s\u02dct(\u03b1\u0302)\n2}\ns\u02dct(\u03b1)2\n}]\n\u2264\nC\n\u03b3\n[|st(\u03b1)\n2 \u2212 s\u02dct(\u03b1)\n2|+ |s\u02dct(\u03b1)\n2 \u2212 s\u02dct(\u03b1\u0302)\n2|{1 + \u03b3\/s\u02dct(\u03b1\u0302)\n2}].\nWhen n is sufficiently large, s\u02dct(\u03b1\u0302)\n2 is bounded from below by a positive constant on the set Bn.\nThus it holds on Bn that\n|V\u0302t \u2212 Vt| \u2264 C1|st(\u03b1)\n2 \u2212 s\u02dct(\u03b1)\n2|+ C2|s\u02dct(\u03b1)\n2 \u2212 s\u02dct(\u03b1\u0302)\n2|.\nNow using the similar arguments as in the proofs of Lemmas 1 and 2, we may show that\n\u2211\nt>\u03bd\nE|st(\u03b1)\n2 \u2212 s\u02dct(\u03b1)\n2| \u2192 0, and\n1\nn\u2212 \u03bd\nn\u2211\nt=\u03bd+1\nE{|s\u02dct(\u03b1)\n2 \u2212 s\u02dct(\u03b1\u0302)\n2|I(Bn)} \u2192 0.\nNow proceeding as the proof for (i) above, we may obtain the required result. \u0004\nProof of Theorem 1. Let Fn,1 and Fn,2 be, respectively, the empirical distribution of {Ut, \u03bd <\nt \u2264 n} and {Vt, \u03bd < t \u2264 n}. By Lemma 3 and the Glivenko-Cantelli Theorem, it holds that\nsup\nx\n|F\u0302n,i(x)\u2212 Fi(x)| \u2264 sup\nx\n|F\u0302n,i(x)\u2212 Fn,i(x)|+ sup\nx\n|Fn,i(x)\u2212 Fi(x)|\nP\n\u2212\u2192 0,\nfor i = 1, 2. This and condition (2.14) entail the required result. \u0004\nReferences\nAzzalini, A. and Capitanio, A. (2003). Distributions generated by perturbation of symmetry\nwith emphasis on a multivariate skew-t distribution. Journal of the Royal Statistical Society,\nB, 65, 367-389.\nBerkes, I., Horva\u00b4th, L. and Kokoszka, P. (2003). GARCH processes: structure and\nestimation. Bernoulli 9, 201\u2013227.\nFan, J. & Yao, Q. (2003) Nonlinear Time Series: Nonparametric and Parametric Methods.\nSpringer, New York.\nGiraitis, L. & Robinson, P.M. (2001). Whittle estimation of ARCH models. Econometric\nTheory 17, 608-23.\nHall, P. & Yao, Q. (2003). Inference in ARCH and GARCH models with heavy-tailed errors.\nEconometrica 71, 285-317.\n14\nHorvath, L. & Liese, F. (2004). Lp-estimators in ARCH models. Journal of Statistical\nPlanning and Inference 119, 277-309.\nKonishi, S. & Kitagawa, G. (1996). Generalized information criteria in model selection. Biometrika\n83, 875-890.\nLee, A.W. & Hansen, B.E. (1994). Asymptotic theory for GARCH(1,1) quasi-maximum\nlikelihood estimator. Econometric Theory 10, 29-52.\nLumsdaine, R. (1996). Consistency and asymptotic normality of the quasi-maximum likelihood\nestimator for IGARCH(1,1) and covariance stationary GARCH(1,1) models. Econometrica\n16, 575-96.\nMikosch, T. and Straumann, T. (2006). Stable limits of martingale transforms with appli-\ncation to the estimation of GARCH parameters. Annals of Statistics, 34, 493-522.\nPeng, L. and Yao, Q. (2003). Least absolute deviations estimation for ARCH and GARCH\nmodels. Biometrika 90, 967\u2013975.\nStraumann, D. (2005). Estimation in Conditionally Heteroscedastic Time Series Models.\nSpringer, Heidelberg.\nStraumann, D. and Mikosch, T. (2006). Quasi-MLE in heteroscedastic times series: a\nstochastic recurrence equations approach. A preprint. Annals of Statistics, 34, 2449-95.\nTakeuchi, K. (1976). Distribution of information statistics and criteria for adequacy of models.\nMathematical Sciences 153, 12-18 (in Japanese).\nWeiss, A. (1986). Asymptotic theory for ARCH models: estimation and testing. Econometric\nTheory 2, 107-31.\n15\n"}