{"doi":"10.1109\/SCAM.2002.1134103","coreId":"66630","oai":"oai:dro.dur.ac.uk.OAI2:659","identifiers":["oai:dro.dur.ac.uk.OAI2:659","10.1109\/SCAM.2002.1134103"],"title":"Evaluating clone detection tools for use during preventative maintenance.","authors":["Bailey,  J. O.","Burd,  E. L."],"enrichments":{"references":[],"documentType":{"type":1}},"contributors":[],"datePublished":"2002-10","abstract":"This paper describes the results of a process whereby the detection capability of 5 code replication detection tools for large software application are evaluated. Specifically this work focuses on the benefits of identification for preventative maintenance that is with the aim to remove some of the identified clones from the source code. A number of requirements are therefore identified upon which the tools are evaluated. The results of the analysis processes show that each tool has its own strengths and weaknesses and no single tool is able to identify all clones within the code. The paper proposes that it may be possible to use a combination of tools to perform the analysis process providing that adequate means of efficiently identifying false matches is found","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/66630.pdf","fullTextIdentifier":"http:\/\/dro.dur.ac.uk\/659\/1\/659.pdf","pdfHashValue":"5acaeef4dadcddca2562606a93895a209299fd31","publisher":"IEEE","rawRecordXml":"<record><header><identifier>\n  \n    \n      oai:dro.dur.ac.uk.OAI2:659<\/identifier><datestamp>\n      2010-11-08T14:14:44Z<\/datestamp><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:title>\n    \n      \n        Evaluating clone detection tools for use during preventative maintenance.<\/dc:title><dc:creator>\n        Bailey,  J. O.<\/dc:creator><dc:creator>\n        Burd,  E. L.<\/dc:creator><dc:description>\n        This paper describes the results of a process whereby the detection capability of 5 code replication detection tools for large software application are evaluated. Specifically this work focuses on the benefits of identification for preventative maintenance that is with the aim to remove some of the identified clones from the source code. A number of requirements are therefore identified upon which the tools are evaluated. The results of the analysis processes show that each tool has its own strengths and weaknesses and no single tool is able to identify all clones within the code. The paper proposes that it may be possible to use a combination of tools to perform the analysis process providing that adequate means of efficiently identifying false matches is found. <\/dc:description><dc:subject>\n        Software application<\/dc:subject><dc:subject>\n         Source code.<\/dc:subject><dc:publisher>\n        IEEE<\/dc:publisher><dc:source>\n         2nd IEEE International Workshop on Source Code Analysis and Manipulation, SCAM\u201902, 1 October 2002, Montreal, Canada ; proceedings. Los Alamitos, CA: IEEE, pp. 36-43<\/dc:source><dc:date>\n        2002-10<\/dc:date><dc:type>\n        Book chapter<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:identifier>\n        dro:659<\/dc:identifier><dc:identifier>\n        doi:10.1109\/SCAM.2002.1134103  <\/dc:identifier><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/659\/<\/dc:identifier><dc:identifier>\n        http:\/\/dx.doi.org\/10.1109\/SCAM.2002.1134103  <\/dc:identifier><dc:format>\n        application\/pdf<\/dc:format><dc:identifier>\n        http:\/\/dro.dur.ac.uk\/659\/1\/659.pdf<\/dc:identifier><dc:rights>\n        \u00a9 2002 IEEE. Personal use of this material is permitted. However, permission to reprint\/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must be obtained from the IEEE.\\ud\n<\/dc:rights><dc:accessRights>\n        info:en-repo\/semantics\/openAccess<\/dc:accessRights><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":[],"year":2002,"topics":["Software application","Source code."],"subject":["Book chapter","PeerReviewed"],"fullText":"Durham Research Online\nDeposited in DRO:\n01 November 2010\nVersion of attached file:\nPublished Version\nPeer-review status of attached file:\nPeer-reviewed\nCitation for published item:\nBailey, J. O. and Burd, E. L. (2002) \u2019Evaluating clone detection tools for use during preventative\nmaintenance.\u2019, in 2nd IEEE International Workshop on Source Code Analysis and Manipulation, SCAM02, 1\nOctober 2002, Montreal, Canada ; proceedings. Los Alamitos, CA: IEEE, pp. 36-43.\nFurther information on publisher\u2019s website:\nhttp:\/\/dx.doi.org\/10.1109\/SCAM.2002.1134103\nPublisher\u2019s copyright statement:\n2002 IEEE. Personal use of this material is permitted. However, permission to reprint\/republish this material for\nadvertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists,\nor to reuse any copyrighted component of this work in other works must be obtained from the IEEE.\nAdditional information:\nUse policy\nThe full-text may be used and\/or reproduced, and given to third parties in any format or medium, without prior permission or charge, for\npersonal research or study, educational, or not-for-profit purposes provided that:\n\u2022 a full bibliographic reference is made to the original source\n\u2022 a link is made to the metadata record in DRO\n\u2022 the full-text is not changed in any way\nThe full-text must not be sold in any format or medium without the formal permission of the copyright holders.\nPlease consult the full DRO policy for further details.\nDurham University Library, Stockton Road, Durham DH1 3LY, United Kingdom\nTel : +44 (0)191 334 3042 \u2014 Fax : +44 (0)191 334 2971\nhttp:\/\/dro.dur.ac.uk\n Evaluating Clone Detection Tools for Use during Preventative \nMaintenance \n \nElizabeth Burd, John Bailey \n \nThe Research Institute in Software Evolution \nUniversity of Durham \nSouth Road \nDurham, DH1 3LE, UK \n \nAbstract \nThis paper describes the results of a process whereby \nthe detection capability of 5 code replication detection \ntools for large software application are evaluated. \nSpecifically this work focuses on the benefits of \nidentification for preventative maintenance that is with \nthe aim to remove some of the identified clones from \nthe source code. A number of requirements are therefore \nidentified upon which the tools are evaluated. The \nresults of the analysis processes show that each tool has \nits own strengths and weaknesses and no single tool is \nable to identify all clones within the code. The paper \nproposes that it may be possible to use a combination of \ntools to perform the analysis process providing that \nadequate means of efficiently identifying false matches \nis found. \n \n1. Introduction \nSoftware systems provide vital support for the smooth \nrunning of an organisation\u2019s business. It is the \nresponsibility of maintainers to keep the system up-to-\ndate and functioning correctly. It is reported by Burd \n[Bur97] that \u201cduring the maintenance of legacy code, it \nis common to identify areas of replicated code\u201d. These \nduplicate or near duplicate functionalities are termed \nclones [Lag97]. Within this paper a clone is recognized \nto be where a second or more occurrences of source \ncode is repeated with or without minor modifications. \nSoftware cloning because of its ad hoc nature it is not \nconsidered reuse, quite the opposite, Mayrand argues \nthat the need for such cloning indicates an organisation \n\u201cdoes not have a good re-use process in place\u201d \n[May97]. \n \nFrom the analysis of software application it appears that \nthe inclusion of these clones results from the addition of \nsome extra functionality which is similar but not \nidentical to some existing logic within a system. Its \nseems that when presented with the challenge of adding \nnew functionality the natural instinct of a programmer is \nto copy, paste and modify the existing code to meet the \nnew requirements and thus creating a software clone \n[Bur97]. While the basis behind such an approach is \nuncertain, one possible reason is due to time restrictions \non maintainers to complete the maintenance change. \nDuccase [Duc99] points out that \u201cmaking a code \nfragment is simpler and faster than writing from \nscratch\u201d and that if a programmer\u2019s pay is related to the \namount of code they produce then the proliferation of \nsoftware clones will continue. \n \nIt is generally recognised that the majority of effort \nduring maintenance is classified as perfective changes \ni.e. \u201cexpanding the existing requirements of a system\u201d \n[Tak96]. Software cloning complicates the maintenance \nprocess by giving the maintainers unnecessary code to \nexamine. \n \nOnce a clone is created it is effectively lost within the \nsource code and so both clones must therefore be \nmaintained as separate units despite their similarities. If \nerrors are identified within one clone then it is likely \nthat modifications may be necessary to the other \ncounter-part clones [Kom01]. Detection is therefore \nrequired if any of the clones are to be re-identified to \nassist the maintenance process. Further potential also \nexists for clone detection to assist the maintenance \nprocess. If clones can be detected then the similarities \ncan be exploited and replaced during preventative \nmaintenance with a new single code unit this will \neliminate the problems identified above. \n \nOne major problem in detecting a clone is that it is \nimpossible to be absolutely certain that one section of \ncode has been copied and pasted from another. Short \nsections of code like wrapper methods in Java can have \nan almost identical structure to countless others. \nFurthermore, there are two types of cloning identified in \n[Bur97] \u201cReplication Within Programs\u201d describing \nsituations where code has been copied and pasted once \nor more within the same file. Secondly \u201cReplication  \n \nProceedings of the Second IEEE International Workshop on Source Code Analysis and Manipulation (SCAM\u201902) \n0-7695-1793-5\/02 $17.00 \u00a9 2002 IEEE \nAcross Programs\u201d is introduced as the cloning of code \nbetween files. Duccase [Duc97] defines \u201ccloned files\u201d \nas \u201cfiles that have a very high duplication ratio between \neach other\u201d.  \n \nCloned code can constitute a significant proportion of a \nlegacy system\u2019s code. Estimates regarding the scale of \nthis problem vary depending on the domain and origin \nof the source code. For instance, Baxter [Bax98] has \nidentified up to 12.7% of code being clones; Baker \n[Bak95] has identified between 13% - 20% of code \nbeing cloned and Lague [Lag97] between 6.4% - 7.5%. \nTaking these points into account it follows that any \nreduction of redundant code is beneficial to the \nmaintenance of a system.  \n \nHowever, the problem is not solely restricted to the \nissue of the increasing size of an application. When \ncode is copied and pasted systematic renaming of \nvariables can lead to  \u201cunintended aliasing, resulting in \nlatent bugs\u201d [Joh94]. Thus, Johnson establishes that \ncloning is a form of \u201csoftware ageing\u201d or making even \nminor changes to the system\u2019s design very difficult.  \n \n There are a good number of clone detection tools \navailable both commercially and within academia. \nWithin these tools several different approaches to \nsoftware clone detection have been implemented, \nincluding string analysis, program slicing, metric \nanalysis and abstract tree comparisons. The aim of this \npaper is to compare a set of clone detection tools on \nsome large software applications. The results of the \nanalysis process will then be used to investigate which \nof the tools are best suited to assist the process of \nsoftware maintenance in general and specifically  \n \n \n \nTable 1: clone detection tools under investigation \n \npreventative maintenance. The investigation will \nexamine results gained from each tool using two \nmetrics; that of precision and recall. Also of interest is \ninvestigating how similar the results achieved are in \ndetecting replication within a single program and \nreplication across distinct programs. Of further interest \nis how similar the results from the different categories \nof detection tools for example JPlag and Moss will only \ndetect replication across programs because they are \nsearching for cheating and copying and modifying one's \nown code is not plagiarism.  \n \nThe following section will describe some of the existing \ntools in the field of clone detection. \n \n2. Clone Detection Technique \n \nFive established detection tools will be used in the \nevaluation process; JPlag, MOSS, Covet, CCFinder and \nCloneDr. JPlag and MOSS are web-based academic \ntools for detecting plagiarism in student's source code. \nCloneDr and CCFinder are stand alone tools looking at \ncode duplication in general. Also included in the  \nexperiment is a prototype tool created Covet. Covet \nuses metrics by Mayrand [May96] and compares the \nmetrics of each function to look for potential clones. \n \nTable 1 summarizes the clone detection tools under \ninvestigation. The languages supported by the analysis \nprocess are highlighted, as is the analysis approach. The \ncolumn labeled domain highlights the main purpose of \nthe tools for either clone detection or for plagiarism \ndetection.  \n \nTool  Author Supported \nLanguages \nDomain Approach Category Background \nCCFinder T.Kamiya C, C++, \nCOBOL, Java, \nEmacs Lisp, \nPlain Text \nClone \nDetection \nTransformation \nfollowed by token \nmatching \nAcademic \nCloneDr I. Baxter C, C++, \nCOBOL, Java, \nProgress \nClone \nDetection \nAbstract Syntax Tree \ncomparison \nCommercial \nCovet \n \nJ. Bailey  \nJ. Mayrand \n \nJava Clone \nDetection \nComparison of \nFunction Metrics \nAcademic \nJPlag G. Malpohl C, C++, Java, \nScheme \nPlagiarism \nDetection \nTransformation \nfollowed by token \nmatching \nAcademic \nMoss A. Aiken Ada, C, C++, \nJava, Lisp, ML, \nPascal, Scheme \nPlagiarism \nDetection \nUnpublished Academic \nProceedings of the Second IEEE International Workshop on Source Code Analysis and Manipulation (SCAM\u201902) \n0-7695-1793-5\/02 $17.00 \u00a9 2002 IEEE \nFurther details of the different approaches taken to \nexamining source code by the clone detection \ntechniques are now given. \n  \nCCFinder [Kam01] focuses on analyzing large-scale \nsystems with a limited amount of language dependence. \nIt transforms the source code into tokens. CCFinder \naims to identify \"portions of interest (but syntactically \nnot exactly identical structures)\". After the string is \ntokenised a token-by-token matching algorithms is \nperformed. CCFinder also provides a dotplotting \nvisualisation tool that allows visual recognition of \nmatches within large amounts of code.  \n \nCloneDr [Bax98] analyses software at the syntactic \nlevel to produce abstract syntax tree (AST) \nrepresentations. A series of algorithms are then applied \nto the tree to detect clones. The first algorithm searches \nfor sub-tree matches within the ASTs. Then a \u201csequence \ndetection\u201d algorithm attempts to detect \u201cvariable size \nsequences of sub-tree clones\u201d. A third algorithm uses \ncombinations of previously detected clones and looks \nfor \u201cmore complex near-miss clones\u201d. The final clone \nset includes the clones detected in the second and third \nalgorithms. CloneDr can automatically replace cloned \ncode by producing a functionally equivalent subroutine \nor macro. \n \nCovet uses a number of the metrics as defined by \nMayrand [May96]. These metrics were selected by \ntaking known clones and identifying which of the Datrix \nmetrics best highlighted the known clone set. Covet \ndoes not apply the same scale of clone likelihood \nclassification as Mayrand. Rather within Covet this is \nsimplified; there is no scale of clone, functions are \neither classed as clones or distinct. The tool is still in \nthe prototype stages and is not capable of processing \nindustrial sized programs. \n \nJPlag [Pre00] uses tokenised substring matching to \ndetermine similarity in source code. Its specific purpose \nis to detect plagiarism within academic institutions. \nFirstly the source code is translated into tokens (this \nrequires a language dependent process). JPlag aims to \ntokenise in such way that the \"essence\" of a program is \ncaptured and so can be effective for catching copied \nfunctionality.  Once converted the tokenised strings are \ncompared to detect the percentage of matching tokens \nwhich is used as a similarity value. JPlag is an online \nservice freely available to academia. \n \nMOSS [Aik02] Aiken does not publish the method \nMOSS uses to detect source code plagiarism, as its \nability to detect plagiarism may be compromised. Moss \nlike JPlag is an online service provided freely for \nacademic use. Source code is submitted via a perl script \nand then the results are posted on the MOSS\u2019s webpage. \nUsers are emailed a url of the results. \n \n \n3. Approach \n \nIf different approaches to clone detection are taken then \nthe results achieved will vary considerably. How much \nvariation is to be addressed by comparing the results of \nseveral clone detection tools. Both commercial and non-\ncommercial tools are used in the experiment, including \ntwo that use clone detection techniques in order to find \nplagiarism in academia.  \n \nThe experiment will involve running a set of clone \ndetection tools over source code of GraphTool. \nGraphTool is a graph layout tool developed in 1999 at \nthe University Of Durham. It is used internally within \nthe computer science department. GraphTool was \nwritten in Java by a postgraduate and currently consists \nof 63 individual source files, 16335 lines of code totally \n660KB. It was chosen because it is a medium size \napplication. Also GraphTool is written in Java and so \nwill be \"understood\" by the majority of detection \nsoftware. \n \nEach tool produces a set of matches (clone \nrelationships) these results will be analyzed to assess the \nsimilarity between the resulting sets. In order to \ncompare the results of each tool some translation will be \nrequired to allow comparison of intersection and \ndifference of the result sets. To standardize the results \nof the different tool and to perform a comparison the \nstart and end lines from each function is taken as the \nindices. This is necessary due to the different approach \nutilized by the different tools, for instance, Covet looks \nfor cloned functions rather than disparate segments of \ncode. Whereas the other tools provide start and end line \nindices showing exactly where the clones appear, this is \nnot possible with Covet. \n \nIn order to establish the coverage of each tool their \noutput was translated into a single data structure, a \nGeneralPair. This GeneralPair holds two matched \nsections of code (either within the same file or from \nseparate source files). Implemented in Java it holds the \nname of the file(s) involved and the code regions that \nhave been matched. Each code region is identified by a \nstart and end index (line numbers). Each tool's \nGeneralPairs are stored in a set. If two GeneralPairs \noverlap then it is considered a match. For example \n \nFileA.java(110-130) & FileB.java(340-360) GeneralPair_One \nFileB.java(310-350) & FileA.java(115-150) GeneralPair_Two \n \nProceedings of the Second IEEE International Workshop on Source Code Analysis and Manipulation (SCAM\u201902) \n0-7695-1793-5\/02 $17.00 \u00a9 2002 IEEE \nGeneralPair_One and GeneralPair_Two are considered \nas a match because the code region in both FileA and \nFileB overlap.  \n \nAn initial comparison of the tools is made through the \nuse of two metrics that of Precision and Recall. \nPrecision is the number of clones of the identified set \nthat are also in the clone base. Thus precision is the \nmeasure to the extent to which the clones identified by \nthe tool are extraneous or irrelevant. Recall is the \nnumber of clones in the clone base that are also in the \nidentified set. Recall is therefore a measure to the extent \nto which the clones identified by the tool matches the \nclone base within the GraphTool application. In order to \nestablish the clone base (the total number of clones \nwithin GraphTool) the results of clone identification \nfrom all tools were merged and then manually verified. \nThis verification process is currently still subjective but \nthe rejection of clones has been based on their \nunsuitability to assist the preventative maintenance \nprocess. Below is a description of the attributes \nconsidered during verification. \nGraphTool was originally developed by a single \npostgraduate. As a result GraphTool's source code is not \noverly large with consistent naming conventions and \nformating. This consistency allowed familiarity with the \ncode to develop fairly quickly and spotting replication \neasier and thus helped verification \n \nSimilar \/ Identical control flow and layout Series of \nof repeated layout blocks could often to point to a copy \nof another piece of code elsewhere in the system. For \nexample if two functions both contained the same \nnumber of if-statements testing similar conditions. \n \nSimilar \/ Identical method names These usually took \nthe form of a verb-noun combination with the verb \nremaining constant and the noun being changed. (for \nexample saveGraph and saveGINGraph) \n \nSimilar \/ Identical variables Clusters of indentical \nvariables and assignments were often a good indication \nthat the code originated somewhere else. \n \nSimilar \/ Identical comments Occasionally the same or \nver similar comment blocks were interspersed in the \ncode. This is quite obviously a legacy of cloning within \nthe source. \n \n4. Results \n \nIn total the GraphTool case study identified 1463 initial \nclones. Not all clones were found by each tool, in fact \nno single tool identified all clones. The initial clone \nnumbers identified by each tool are shown within Table \n2 below.  \n \n \nCCFinder CloneDr Covet JPlag Moss\n \nIdentified \nclones by \ntool 1128 84 278 131 120 \nTable 2: Total clone numbers identified by each tool \n \nFrom Table 2 it can be seen that CCFinder identified the \nlargest number of clones, a total of 1128, whereas \nCloneDr identified the smallest number only 84. \n \nIn order to examine the differences between the output \nof the two tools the difference between the sets of \nclones identified has been established. These are \nrepresented within Table 3.  \n \n \nCCFinder CloneDr Covet JPlag Moss\n \nCCFinder  1090 1089 989 1025 \n \nCloneDr 43  265 120 111 \n \nCovet 251 70  120 109 \n \nJPlag 44 73 273  67 \n \nMoss 19 76 268 81  \nTable 3: The difference between the set of clones \noutput between each tool \n \nThus Table 3 shows that CCFinder identified 1090 \nclones that were not identified by CloneDr, however \nCloneDr identified a further 43 that were not identified \nby CCFinder. \n \nIn addition the intersection between each of the tools \nhas also been established. The intersections, i.e. those \nclones that were identified by more than one tool, are \nshown in Table 4. Thus, CCFinder and CloneDr \nidentified 38 clones in common.\nProceedings of the Second IEEE International Workshop on Source Code Analysis and Manipulation (SCAM\u201902) \n0-7695-1793-5\/02 $17.00 \u00a9 2002 IEEE \n \n \n \nCloneDr Covet JPlag Moss \n \nCCFinder 38 27 87 101 \n \nCloneDr  13 11 9 \n \nCovet   15 10 \n \nJPlag    50 \nTable 4: The intersection of identified clones from \neach tool \n \nDue to the different domains of the detection tools that \nhave been analyzed it is interesting to investigate for the \nset of all clones what proportion are identified within a \nsingle file and the proportion of clones that are \nidentified across files within the GraphTool application. \nDue to the nature of their implementation, plagiarism \ndetectors do not investigate clones within files, since \nthis is not plagiarism. It is therefore of interest to see \nwhat potential disadvantages to clone identification for \nmaintenance such restrictions would bring. The results \nof this analysis are represented within Table 5. \n \n \n \nWithin files Across files \n \nCCFinder 44 66 \n \nCloneDr 79 21 \n \nCovet 31 69 \n \nJPlag N\/A 100 \n \nMoss N\/A 100 \nTable 5: The percentage of identified clones \nidentified within \/ across files \n \nThe results show that CCFinder identifies the greatest \nproportion of its clones across files; that is CCFinder \nimplies that most clones appears to be copied between a \nnumber of application files. However, the results of \nCloneDr seems to show the opposite in that the clones \nthat it identifies are mostly copied within files. These \nresults show that the plagorism detection software, due \nto only investigating replication across files, failed to \nidentify over 500 clones. \n \nWhat conducting the analysis process on only the total \nnumbers of clones identified may obscure, is the overall \nproportion of the code that is cloned. For instance, the \nlarge numbers gained could be due to the relatively \nsmall size of the clones found. Therefore to gain an \nindication of the proportion of clones, mean size of the \nclone set has been analyzed for each tool. The results of \nthis analysis process are shown within Table 6. \n \n Largest identified\nclone (LOC) \nMean size of \nclone (LOC) \n \nCCFinder \n \n94 17 \n \nCloneDr \n \n100 16 \n \nCovet \n \n123 21 \n \nJPlag \n \n78 23 \n \nMoss \n \n57 15 \nTable 6: The Lines of Code (LOC) of clones \nidentified \n \nTable 6 shows some interesting results. Is is surprising \nthat the mean size of the clones that are identified do not \nsignificantly vary for each tool, with the exception of \nMoss where both the maximum and mean sizes are \nslightly smaller than the others. \n \nA further interesting aspect of the clone identification \nprocess is the number of times each clone is replicated \nwithin the code. The data presented so far identifies the \nnumbers of total clones; that is all duplicate instances of \nthe code. Table 7 shows the frequencies of replication \nidentified for each unique clone identified by each tool. \n \nFrequency CCFinder CloneDr Covet JPlag Moss \n1 569 66 40 95 104 \n2 98 6 34 10 8 \n3 33 2 13 4 0 \n4 14 0 6 1 0 \n5 16 0 5 0 0 \n6 19 0 5 0 0 \n7 2 0 1 0 0 \n8 0 0 1 0 0 \n9 0 0 0 0 0 \n10 0 0 0 0 0 \n11 0 0 0 0 0 \n12 0 0 2 0 0 \n13 0 0 1 0 0 \nTable 7: Total occurrences of each clone per tool \n \nTable 7 shows that it is Covet that recognizes the largest \nnumber of replications within the application. The table \nshows that Covet identified 3 cases where a clone has \nProceedings of the Second IEEE International Workshop on Source Code Analysis and Manipulation (SCAM\u201902) \n0-7695-1793-5\/02 $17.00 \u00a9 2002 IEEE \nbeen replicated 12 or more times within the source \ncode, the other tools recognize no more than 7 \ninstances. \n \n \nCCFinder CloneDr Covet JPlag Moss \n \nRecall 72 9 19 12 10 \n \nPrecision 72 100 63 82 73 \nTable 8: Precision and recall \n \nThe results up to now have been based on all the clones \nidentified by the tools. However, not all automatically \nidentified clones are likely to be actual clones. Thus, a \nsubjective assessment was then made of the clones that \nwere identified by the tools which could be considered \nactual clones for the purpose of preventative \nmaintenance. From a total of 1463, from the initial \nremoval criteria 563 clones were rejected as being false \nmatches clones. Thus, based on this analysis the values \nof recall and precision could then be calculated. Recall \nis the percentage of true clones actually identified by \neach tool. This is shown within Table 8. CCFinder has \nby far the greatest recall identifying a total of 72% of \nthe true clone base. \n \nA measure of precision has also been made. The table \nshows that Covet identified the greatest percentage of \nfalse clones and therefore had the worst precision with \n37% of the clones being identified not actually being \nclones. Conversely, CloneDr had the best precision with \nnone of the clones later being identified as false \nmatches.  \n \n5. Evaluation \nThe aim of this paper has been to identify which of the \nevaluated tools are best suited to support the process of \nsoftware maintenance. In order to address this issues it \nneeds to be considered what requirements are required \nof such a support tool. The requirements such a tool to \nsupport maintenance are considered to be the: \n \noutput of a high proportion or all of the clones present \nwithin the code \noutput of a low proportion or no incorrectly identified \nclones \nmatching and output of clones that have high \nfrequencies of replication \noutput of clones that are large in terms of lines of code \noutput of clones that can be modified or removed with \nminimum impact to the application \nease of usability of the tool \n \nThese points are now considered in relation to the \nresults obtained. \n \n5.1 Output of a high proportion of the clones  \n \nThe identification of a high proportion of the clones is \nimportant for maintenance so that the severity of the \nmodification problem can be addressed prior to \nmaintenance and that proper consideration can be given \nto the selection and attributing of a priority for the \nremoval of the clones. Within this paper this \nrequirement have been investigated in a number of \nways. CCFinder identified more clones than the other \ntools but the greater proportion of these clones \nidentified was across files. Proportionally CloneDr \nidentified more clones that were internally replicated \nwithin a file. However, the most predictive assessment \nof this requirement is the metric of recall being to \npercentage of the clones identified from the total known \nset. CCFinder identified the greatest total number of \nclones, thus resulting in the highest level of recall 72%. \n \nOverall each tool identified some clones that were not \nidentified by any other tool and that each tool \noverlapped those that it identified with other tools. In all \ninstances these overlaps were different. Only through \nusing all the tools would it have been possible to \nidentify the total set of clones. \n \nOutput of a low proportion of incorrectly identified \nclones \nThe output of a low proportion of incorrectly identified \nclones is important to ensure that the maintenance \nprocess is efficient. In most instances false positives will \nhave to be verified manually. This provides a cost in \nterms of the maintainer\u2019s time. For this reason good \nprecision is required of the tools. CloneDr was the only \ntool who provided perfect precision, thereby identifying \nno false positive matches, and therefore not resulting in \nthe incurring of wasted maintenance effort. This is due \nto the automation process for clone removal; if it can\u2019t \nbe automatically removed then its not identified as a \nclone. Thus in this instance a tradeoff has been applied \nto forsake high recall for perfect precision. \n \nAll other tools outputs were found contain at least 1 in 5 \nclones to be false positives. Of course the greater the \nnumber of clones that each tool identifies so the total \nnumber of false positives rises and thereby the potential \nfor wasted effort.  \n \nIn some uses of clone detection for maintenance the \nidentification of false-positives will not be an issue. For \ninstance, consider a scenario when a change is required \nto a specific portion of code, a search for clones could \nthen be made that match, and only match, the \nProceedings of the Second IEEE International Workshop on Source Code Analysis and Manipulation (SCAM\u201902) \n0-7695-1793-5\/02 $17.00 \u00a9 2002 IEEE \nspecifically identified portion of the code to be changed. \nSince modification are only considered for the matched \ncode it is unlikely that false positives will be identified \nin this instance. \n \n5.3 Matching and output of clones with a high \nfrequencies of replication \n \nClones represent the potential for wasted maintenance \neffort. One way in which preventative maintenance can \nassist is through the removal of these clones. Therefore, \nthe clones that are replicated most frequently within the \ncode potentially offer the greatest payback for \nconducting preventative maintenance. Tools that are \nable to match the largest sets of duplicate functionality \npotentially offer the greatest payback to maintainers. \nThe results of the analysis process showed that Covet \nfollowed by CCFinder best satisfied this requirement. \nHowever, the benefits CloneDr\u2019s ability to \nautomatically conduct an automated clone replacement \nprocess should be not underestimated. \n \n5.4 Output of clones that are large in terms of lines \nof code \n \nAs for the same reasons as indicated above the more \ncode that can potentially be removed per change the \ngreater the potential payback for maintenance. Thus the \nsize of the clones identified is important. The largest \nclone identified was by Covet at 123 LOC, but the tools \ngenerating the largest mean for all clones was JPlag. \nOverall, however, all tools showed fairly similar \nperformance levels. \n \nOutput of clones that can be modified or removed \nwith minimum impact  \n \nImpact of change is effectively the maintenance cost for \nremoving each clone. Due to the costs involved in \nconducting this analysis this requirement was not \npossible to assess except where the process is known to \nhave been automated as in CloneDr. However, what was \npossible to assess was the change impact to an \napplication. Where removal of the clone was focused \nwithin a source file the program comprehension costs \nare likely to be less when more files are involved. As \nindicated above CloneDr identified a very high \npercentage of clones that were internally replicated \nwithin a file. \n \n5.6 Ease of usability of the tool \n \nWhen running an analysis process other factors need to \nbe taken into account such as ease of use, speed and \nlanguage support. No subjective measures of the \nusability of these tools have yet been made, but an \nindication of factors such as language support was \nincluded in table 1. \n \n6. Conclusions and Further Work \n \nThe results have identified that there is no single and \noutright winner for clone detection for preventive \nmaintenance. Each tool had some factors that may \nultimately prove useful to the maintainer. Furthermore \nthe ultimate choice is most likely to differ under the \ncircumstances at which the change proposal is made. \nFor instances, whether precision or recall is the most \nhighly desirable requirement or any combination \nthereof.  \n \nWhat this analysis process has identified is need to be \nable to accurately define requirements for the \nidentification and removal process and this paper has \nthus identified a set of criteria upon which this \nassessment can be based. Furthermore, it has also \nidentified areas of strengths and weaknesses in each tool \nthat may ultimately lead to their improvement. \nHowever, due to the plagiarism tools only considering \nacross file duplication these are of less use than the \ndedicated clone detection tools. \n \nOne way in which a more definitive analysis could be \nperformed may be, based on the clone set identified, to \ninvestigate a priority for those clones which it would be \nmost beneficial to remove. From this analysis it may be \npossible to make a more definitive selection of clone \nidentification tool.   \n \nA further way in which this process could be improved \nwould be to automate the collation process and to be \nable to pool the results of using each tool. This work has \nalready been started though this project but the removal \nof false positives still needs to be addressed. \n \nOne way of more effectively dealing with false positives \nis to improve the process by which they can be \nidentified. Currently the output of most clone detection \ntools is a simple textual representation. Applying a \ngraphical representation will allow the user to browse \nsummaries of each source code file detailing the clones \ndetected across and within file structures. Plotting such \ngraphical representation will allow maintenance\u2019s to \nmore efficiently evaluate and plan the preventative \nmaintenance process of clone removal.  \nReferences \n \nProceedings of the Second IEEE International Workshop on Source Code Analysis and Manipulation (SCAM\u201902) \n0-7695-1793-5\/02 $17.00 \u00a9 2002 IEEE \n[Aik02]  Aiken, A., A System for Detecting Software \nPlagiarism (Moss Homepage), Last visited \n11th April 2002 \n[Bak95]  Baker B. S., On Finding Duplication and \nNear-Duplication in Large Software \nSystems, 2nd Working Conference on \nReverse Engineering 1995 \n[Bax98]  Baxter I.D., Yahin A., Moura L., Sant\u2019Anna \nM., Bier L., Clone Detection Using Abstract \nSyntax Trees, International Conference on \nSoftware Maintenance 1998 \n[Bur97]  Burd E.L., Munro M., Investigating the \nMaintenance Implications of the Replication \nof Code, International Conference on \nSoftware Maintenance 1997 \n[Duc99]  Ducasse S., Rieger M., Demeyer S., A \nLanguage Independent Approach for \nDetecting Duplicated Code, International \nConference on Software Maintenance 1999 \n[Joh94]  Johnson J. H., Substring Matching For \nClone Detection and Change Tracking, \nInternational Conference on Software \nMaintenance 1994. \n[Kam01]  Kamiya T., Ohata F., Kondou K., Kusumoto \nS., Inoue K., Maintenance Support Tools for \nJava Programs: CCFinder and JAAT, \nInternational Conference on Software \nEngineering 2001 \n[Kom01]  Komondoor R., Horwitz S., Using Slicing to \nIdentify Duplication in Source Code,  \nSymposium on Static Analysis 2001 \n[Lag97]  Lague B., Proulx D., Mayrand J., Merlo E., \nHudepohl J., Assessing the Benefits of \nIncorporating Function Clone Detection in \na Development Process, International \nConference on Software Maintenance 1997 \n[May96]  Mayrand J., Leblanc C., Merlo E., Automatic \nDetection of Function Clones in a Software \nSystem Using Metrics, International \nConference on Software Maintenance 1996 \n[Nie97]  Niessink F., van Vliet H., Predicting \nMaintenance Effort with Function Points, \nInternational Conference of Software \nMaintenance 1997 \n[Pre00]  Prechelt L., Malpohl G., Philippsen M., \nJPlag: Finding plagiarisms among a set of \nprograms, Technical Report 2000-1 \n[Tak96]  Takang A., Grubb P., Software Maintenance \n: Concepts and Practice, Thomson Computer \nPress 1996, ISBN 1-85032-192-2 \n \n \n \n \n \nProceedings of the Second IEEE International Workshop on Source Code Analysis and Manipulation (SCAM\u201902) \n0-7695-1793-5\/02 $17.00 \u00a9 2002 IEEE \n"}