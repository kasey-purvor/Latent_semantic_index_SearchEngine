{"doi":"10.1080\/0968776960040306","coreId":"6817","oai":"oai:generic.eprints.org:221\/core5","identifiers":["oai:generic.eprints.org:221\/core5","10.1080\/0968776960040306"],"title":"Computer\u2010based laboratory simulation: Evaluations of student perceptions","authors":["Edward, Norrie"],"enrichments":{"references":[{"id":1041889,"title":"An evaluation of some common CAL and CBT authoring styles',","authors":[],"date":"1989","doi":"10.1080\/1355800890260303","raw":"Whiting, J. (1989), 'An evaluation of some common CAL and CBT authoring styles', ETTI, 26 (3), 186-200. S3","cites":null},{"id":193277,"title":"Assessment of undergraduate electrical engineering laboratory studies',","authors":[],"date":"1980","doi":null,"raw":"Carter, G., Amour, D. G., Lee, L. S. and Sharpies, R. (1980), 'Assessment of undergraduate electrical engineering laboratory studies', IEE Proceedings, A.460.","cites":null},{"id":193285,"title":"Can CAL biology packages be used to replace the teacher?',","authors":[],"date":"1993","doi":"10.1080\/00219266.1993.9655336","raw":"Phillips, T. and Moss, G. (1993), 'Can CAL biology packages be used to replace the teacher?', Journal of Biological Education, 27 (3), 213\u201416.","cites":null},{"id":1041888,"title":"Case study: the use of a HyperCard simulation to aid in the teaching of laboratory apparatus operation',","authors":[],"date":"1994","doi":"10.1080\/0954730940310405","raw":"Waddick, J. (1994), 'Case study: the use of a HyperCard simulation to aid in the teaching of laboratory apparatus operation', ETTI, 31 (4), 295-301.","cites":null},{"id":193284,"title":"Computer-based assessment to support the learner not the assessor',","authors":[],"date":"1993","doi":null,"raw":"Pengelly, M. (1993), 'Computer-based assessment to support the learner not the assessor', Teaching and Learning Technology Programme Workshop on Assessment of Learning in Higher Education, Workshop Papers, Sheffield, November 1993 (ISBN 1-85889-086-1).","cites":null},{"id":1041886,"title":"Computer-based laboratory tutorials',","authors":[],"date":"1995","doi":"10.1080\/0968776950030107","raw":"Ritchie, G. and Garner, P. (1995), 'Computer-based laboratory tutorials', IEE Colloquium: Computer-Based Learning in Electronic Education, Digest No. 1995\/098, p. 13\/1-2, London: IEE.","cites":null},{"id":193280,"title":"Computer-based teaching and evaluation of introductory statistics for health science students: some lessons learned',","authors":[],"date":"1994","doi":"10.1080\/0968776940020207","raw":"Colgan, N., McClean, S. and Scotney, B. (1994), 'Computer-based teaching and evaluation of introductory statistics for health science students: some lessons learned', Association for Learning Technology Journal, 2 (2), 68-74. 52ALT-J Volume 4 Number 3 Edward, N. 'Evaluation of computer-based laboratory simulation, Computers and Education (in press).","cites":null},{"id":193283,"title":"Computer-simulated experiments and computer games: a method of design analysis',","authors":[],"date":"1995","doi":"10.1080\/0968776950030106","raw":"Leary, J. (1995) 'Computer-simulated experiments and computer games: a method of design analysis', Association for Learning Technology Journal, 3 (1), 57-61.","cites":null},{"id":193276,"title":"Enhancing student experience of computer-aided learning packages',","authors":[],"date":"1994","doi":null,"raw":"Adumbra, C. (1994), 'Enhancing student experience of computer-aided learning packages', Proceedings of the Conference on Computer Aided Learning in Engineering, Sheffield, September 1994, Sheffield: University of Sheffield.","cites":null},{"id":1041885,"title":"Evaluating interactive multimedia',","authors":[],"date":"1992","doi":null,"raw":"Reeves, T. (1992), 'Evaluating interactive multimedia', Educational Technology, May, 47-52.","cites":null},{"id":193282,"title":"Formative Evaluation for Educational Technologies,","authors":[],"date":"1990","doi":null,"raw":"Flagg, B. (1990), Formative Evaluation for Educational Technologies, Hillsdale NJ: Lawrence Erlbaum.","cites":null},{"id":193281,"title":"Screen presentations in laboratory simulations as perceived by students',","authors":[],"date":"1996","doi":null,"raw":"Edward, N. (1996), 'Screen presentations in laboratory simulations as perceived by students', Proceedings of the SERA (1995) Conference, Glasgow: University of Strathclyde.","cites":null},{"id":1041887,"title":"Simulation training builds teams through experience',","authors":[],"date":"1993","doi":null,"raw":"Solomon, C. (1993), 'Simulation training builds teams through experience', Personnel Journal, June.","cites":null},{"id":193279,"title":"Teaching in a third of the time: a successful application of computer-aided learning in degree-level electronics',","authors":[],"date":"1994","doi":"10.1109\/13.704542","raw":"Coleman, J. N., Kinniment, D., Burns, F., and Butler, T. (1994), 'Teaching in a third of the time: a successful application of computer-aided learning in degree-level electronics', Proceedings of the Conference on Computer Aided Learning in Engineering, Sheffield, September 1994, Sheffield: University of Sheffield.","cites":null},{"id":193278,"title":"The development of a low technology marketing CBT',","authors":[],"date":"1994","doi":null,"raw":"Catterall, M. and Ibbotson, P. (1994), 'The development of a low technology marketing CBT', Account, (6) 1.","cites":null}],"documentType":{"type":1}},"contributors":[],"datePublished":"1996","abstract":"Providing resources to meet the needs of oil workers who miss blocks of an engineering course was the motivation for producing computer\u2010based simulations of laboratory equipment. This paper reports on student perceptions of various aspects of the package. The factors are grouped into (i) motivation and support, and (ii) presentations and interaction. A schematic representation of the controls and instrumentation was used. Two classes, engineers and non\u2010engineers, were the pilot groups. The engineers clearly preferred laboratories, whereas the non\u2010engineers were just as happy with the simulation. The results of the survey suggest that while computer\u2010based simulation may be an alternative to laboratories, even the best alternative, much is lost as a result. Practical appreciation and team\u2010working skills are not well developed The schematic presentation is easy to use, but gives the student little \u2018feel\u2019 for the operation of a real plant","downloadUrl":"https:\/\/core.ac.uk\/download\/pdf\/6817.pdf","fullTextIdentifier":"http:\/\/repository.alt.ac.uk\/221\/1\/ALT_J_Vol4_No3_1996_Computer_based_laboratory_simu.pdf","pdfHashValue":"1037352a267c3f8f6e2f77e2f5b666d65f47805d","publisher":"Universit of Wales Press","rawRecordXml":"<record><header><identifier>\n    \n    \n      oai:generic.eprints.org:221<\/identifier><datestamp>\n      2011-04-04T09:22:34Z<\/datestamp><setSpec>\n      7374617475733D707562<\/setSpec><setSpec>\n      7375626A656374733D4C:4C42<\/setSpec><setSpec>\n      7375626A656374733D4C:4C43:4C4331303232<\/setSpec><setSpec>\n      74797065733D61727469636C65<\/setSpec><\/header><metadata><oai_dc:dc xmlns:oai_dc=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/\" xmlns:dc=\"http:\/\/purl.org\/dc\/elements\/1.1\/\" xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\" xsi:schemaLocation=\"http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc\/ http:\/\/www.openarchives.org\/OAI\/2.0\/oai_dc.xsd\" ><dc:relation>\n    \n      \n        http:\/\/repository.alt.ac.uk\/221\/<\/dc:relation><dc:title>\n        Computer\u2010based laboratory simulation: Evaluations of student perceptions<\/dc:title><dc:creator>\n        Edward, Norrie<\/dc:creator><dc:subject>\n        LB Theory and practice of education<\/dc:subject><dc:subject>\n        LC1022 - 1022.25 Computer-assisted Education<\/dc:subject><dc:description>\n        Providing resources to meet the needs of oil workers who miss blocks of an engineering course was the motivation for producing computer\u2010based simulations of laboratory equipment. This paper reports on student perceptions of various aspects of the package. The factors are grouped into (i) motivation and support, and (ii) presentations and interaction. A schematic representation of the controls and instrumentation was used. Two classes, engineers and non\u2010engineers, were the pilot groups. The engineers clearly preferred laboratories, whereas the non\u2010engineers were just as happy with the simulation. The results of the survey suggest that while computer\u2010based simulation may be an alternative to laboratories, even the best alternative, much is lost as a result. Practical appreciation and team\u2010working skills are not well developed The schematic presentation is easy to use, but gives the student little \u2018feel\u2019 for the operation of a real plant.<\/dc:description><dc:publisher>\n        Universit of Wales Press<\/dc:publisher><dc:date>\n        1996<\/dc:date><dc:type>\n        Article<\/dc:type><dc:type>\n        PeerReviewed<\/dc:type><dc:format>\n        application\/pdf<\/dc:format><dc:language>\n        en<\/dc:language><dc:rights>\n        cc_by_nc_nd<\/dc:rights><dc:identifier>\n        http:\/\/repository.alt.ac.uk\/221\/1\/ALT_J_Vol4_No3_1996_Computer_based_laboratory_simu.pdf<\/dc:identifier><dc:identifier>\n          Edward, Norrie  (1996) Computer\u2010based laboratory simulation: Evaluations of student perceptions.  Association for Learning Technology Journal, 4 (3).  pp. 41-53.  ISSN 0968-7769     <\/dc:identifier><dc:relation>\n        10.1080\/0968776960040306<\/dc:relation><\/oai_dc:dc><\/metadata><\/record>","journals":null,"language":{"code":"en","id":9,"name":"English"},"relations":["http:\/\/repository.alt.ac.uk\/221\/","10.1080\/0968776960040306"],"year":1996,"topics":["LB Theory and practice of education","LC1022 - 1022.25 Computer-assisted Education"],"subject":["Article","PeerReviewed"],"fullText":"Computer-based laboratory\nsimulation: evaluations of student perceptions\nNorrie S. Edward\nSchool of Mechanical and Offshore Engineering, The Robert Gordon University,\nAberdeen\nProviding resources to meet the needs of oil workers who miss blocks of an engineering course was the\nmotivation for producing computer-based simulations of laboratory equipment. This paper reports on\nstudent perceptions of various aspects of the package. The factors are grouped into (i) motivation and\nsupport, and (ii) presentations and interaction. A schematic representation of the controls and\ninstrumentation was used. Two classes, engineers and non-engineers, were the pilot groups. The\nengineers clearly preferred laboratories, whereas the non-engineers were just as happy with the\nsimulation. The results of the survey suggest that while computer-based simulation may be an\nalternative to laboratories, even the best alternative, much is lost as a result. Practical appreciation and\nteam-working skills are not well developed The schematic presentation is easy to use, but gives the\nstudent little feel'for the operation of a real plant.\nIntroduction\nLaboratory experimentation in engineering is an essential part of the three main\ncomponents in an engineer's formation. The theoretical constructs and models are\nimparted in lectures and tutorials. Workshop hands-on activity allows the student to\nacquire an understanding of the interaction of design and manufacture, and the\nconstraints both impose. Characteristics of plant are investigated through experiment,\nand this aids the learner's understanding of the limitation of models in predicting\nperformance. The learner also gains an appreciation of the nature of errors and of the\nconstruction of plant. But while the oil industry has brought prosperity to the North-\nEast, it has also brought unique educational demands: the working arrangements place\nsevere restrictions on part-time student attendance. Technicians work a block of two to\nfour weeks offshore, followed by a similar period of leave. Different companies have\ndifferent arrangements, and shift-change days.\nA review of the literature (see Boud et al, 1984) suggests that the main aim of laboratory\nwork should be to teach inquiry methodology and experimental design. It is often also\n41\nNome S. Edward Computer-based laboratory simulation: evaluations of student perceptions\nreported that students do not enjoy the activity. In contrast, our staff at the School of\nMechanical and Offshore Engineering are clear that the main aims of the work are to\nestablish the links between theory and practice, to aid visualization, and to develop team-\nworking skills, and our students, in common with Carter's (1980) findings, enjoy this\naspect of their education. Whatever the true outcomes of laboratories, the activity is seen\nas essential by our staff and students alike. Alternative provision, such as directed reading\nto allow part-time students to stay abreast of theory, is not difficult to arrange, and most\nof them gain a knowledge of plant construction through daily contact in their working\nlives. But this is not to say that they are afforded the opportunity to investigate the\ncharacteristics of the equipment. Rather, the objective will often be to maintain a steady-\nstate operational condition. We therefore felt that the provision of an alternative to\nlaboratory work was essential.\nThe evaluation reported in this and another paper (Edward, in press) set out to test the\ncontention that in comparison with laboratories, cognitive learning would be as high with\na simulation, but that practical appreciation would suffer. The opportunity was taken to\ncarry out what Flagg (1990) describes as 'formulative evaluation' based on our students'\nperceptions of the package, i.e. 'the systematic collection of information for the purpose\nof informing decisions to design and improve the product'. Reeves (1992) avers that the\neffectiveness of a package is constrained by (i) the design of the user-interface, and (ii) the\nmotivation and expertise of the user. It is on these and related factors that this paper\nreports.\nSince students may miss either or both of the laboratory experiment and the relevant\ntheory, it was decided that any alternative should provide both. The final package was\nmultiple media. The theory was covered in an interactive text-based workbook. A\nseparate laboratory procedure sheet was also provided so that students who had attended\nthe lectures did not have to work through the book. As an aid to visualization, a short\nvideo showing the plant in operation was produced. The heart of the package was a\ncomputer-based operational simulation. This was interactive, and allowed users the same\ntype of control as they would have on the actual plant. Similarly, the readings provided\nwere based on actual experimental performance data from the machine, a centrifugal\npump being the example examined here. Using real data allowed representative\nsystematic error to be included. Random error was approximated by analysing repeated\nreadings and applying an appropriate randomization factor to each of the displayed\noutputs.\nA desirable package would be 100% faithful to actual performance, would include effects\nsuch as sound, would have a photographically realistic interface, and would be controlled\nin a manner identical to the real plant. The first of these would have required an\nenormous database, but the inaccuracy introduced by limiting the database is very small.\nSound, though desirable, was not felt to add much, and at the time would not have been\navailable on most computers. The realism is important. Does the display have to look like\nthe equipment for the user to gain the most from the experience? Must a dial gauge be so\nrepresented, or can it be replaced by a digital output? Must a valve be opened by clicking\nand dragging a lever, or can an indexing button be used? In the interests of reducing the\nproduction time* and of ensuring rapid response and portability to relatively low-\nperformance machines, compromises had to be made. A diagrammatic interface was used\n42\nALT-J Volume 4 Number 3\n(Figure 1). (In passing, it is worth noting that our gas turbine simulator uses a much more\nrealistic interface.) These compromises were not made lightly and, as discussed below, I\nhave separately been investigating their importance.\nPresentation Window\nFile\nValve Opening Control\nValve - .5\n+ 10 - 10 Speed Control\nFigure \/: Screen representation of the pump controls and instruments\nEvaluation of the centrifugal-pump simulation was conducted by running the package\nwith a pilot group of students. Students were drawn from two classes. One class (30\nstudents) was composed of mainstream engineers. The other (26 students) was composed\nof cross-disciplinary technology and business undergraduates. Means were compared\nusing an independent t-test with a <.O5 level considered to be significant. Few significant\ndifferences were detected between the two classes, and they are generally reported here as\na single cohort. They were divided into two groups of 28, one of which carried out the\nactual laboratory experiment while the other obtained their results from the computer-\nbased simulation (CBS). Most of the aspects of the experience reported here relate only to\nthe simulation, and for them the relevant sample size is 28. Evaluation was by means of\nquestionnaires administered to all students, and follow-up interviews. The questionnaires\n43\nNome S. Edward Computer-based laboratory simulation: evaluations of student perceptions\nused five-point Likert scales. Space was provided for free-form comments, with\nsuggestions being made that respondents might wish to comment on matters such as ease\nof use, help facilities, difficulties with operation, and so on. The response rate was 84%\noverall, with the CBS group return-rate at 89%. Ten formal audio-taped interviews were\nconducted, and around the same number of ad-hoc informal interviews were written up\nafterwards. In the formal interviews, a semi-structured questionnaire was used.\nQuite a range of factors was investigated, and I have grouped these factors into three\nmain categories. These are the learning outcomes, the presentation and interface, and\nfinally motivation and support. I have reported elsewhere on the learning outcomes\n(Edward, in press). In this paper, I report on the findings on the remaining factors. Table\n1 gives the means and range for each of the factors discussed in this paper.\nFACTOR\nEase of use of the package (general section)*\nEase of use of the package (simulator section)\nRealism of the simulation\nAttractiveness of the screen presentation\nLength of the package\nEffectiveness of simulation as a laboratory alternative\nSufficiency of interaction provided\nGuidance on use of the package (general section)\nGuidance on use of the package (simulator section)\nGuidance on writing report\nSufficiency of feedback on performance\nMotivational impact of the simulation\nPreference for independence\nMEAN\n3.91\n3.88\n3.56\n3.41\n2.71\n3.18\n2.67\n3.18\n2.76\n2.95\n2.95\n3.29\n3.28\n(n = 28 for all factors. * indicates low to high transposition of results)\nRANGE\n1-5\n1-5\n2-4\n1-5\n1-4\n1-5\n1-4\n1-5\n2-4\n1-5\n1-4\n2-5\n1-5\nTable I: Analysis of student perceptions of the computer-based simulation package\nPresentation and interface\nEase of use\nStudents rated how easy they had found the simulation to operate, both in the general\nquestionnaire and in the one on the CBS itself. The scales were reversed, but after\ntransposing one set so that a low score indicates that it was found difficult to use, the two\nmean scores were almost identical at 3.88 and 3.91. A total of 46% of the respondents\nrated the package easy to use. Students did report some difficulties, but these generally\nwere due to faults in the program which were readily remedied. Their most compelling\ncriticism was of the quality of on-line help. The laboratory group found the actual plant\nslightly more difficult to use (mean 3.68), but interviews suggested that this was\nwelcomed. The plant is fairly complex and the students enjoyed operating it.\n44\nALT-J Volume 4 Number 3\nScreen presentation\n% 20\n10\nvery poor 2.00 3.00 4.00\nScreen presentation\nfigure 2: Students' appraisal of the representation of the plant on screen\nvery good\nBecause of the compromises made in the representation of the apparatus, as reported\nabove, I was anxious to see user-reaction to the result. It was something of a relief that\nthe mean rating was 3.41, and only 16% of students selected a rating below the mid-point.\nBut the students were somewhat more circumspect about the realism of the simulation.\nNo student in either class used the extremes of the scale. Here also the overall mean of\n3.56 suggests that a suitable approach has been adopted. Complacency must be avoided,\nhowever, as one or two comments were either unfavourable or uninformed. In the latter\ncategory, one student welcomed the simulator's ability to repeat the results 'without\nerror', having failed to appreciate the distinction between random and systematic error.\nOne student who had previously used a simulation said that he preferred its approach\nwhich featured optional controls offering different levels of realism from alpha-numeric\nto a fully realistic representation of the plant.\nLength\nAlthough length was included in the assessment of the simulation, some students\nobviously treated it as relating to the overall experience. Even for the simulation, a\ndefinition of this may have proved difficult because users had latitude in use of the\nvarious sections, repeat tests and self-assessment questions (SAQs). Some students said\nthey felt that even more choices should have been available. Some wished to be free to\nescape from an SAQ if they felt it was inappropriate. Another said he would have liked\nmore depth to the theory and more testing SAQs. It was pointed out that these were\nprovided in the workbook. He said: 'I did read the workbook but I'd have preferred to\nhave it all in one place when I was at the computer. Also I would have liked to be able to\ngo back to the theory when I got stuck.' The laboratory group reported that on average\nthey had spent 100 minutes on the test, while those using the CBS approach spent only 74\n45\nNome S. Edward 'Computer-based laboratory simulation: evaluations of student perceptions\nminutes (see Figure 3). Overall, despite the shorter times recorded as being spent on the\nCBS than on the laboratory, students tended to feel that the package was rather long.\nThey rated it from too short to much too long, but the mean of 3.9 shows that generally it\nwas found slightly long. It suggests that more options on routing would be welcomed by\nthe students.\n40,\n30,\n20-\n10<\n0 I fin n\n\u2022\n. 1In\nExperimental method\n\u2022laboratory\n\u2022 CBS\n30 35 45 50 60 70 80 85 90 100 120130180\nTime spent on activity (minutes)\nfigure 3: Length of time spent by students of both groups on the activity\nEffectiveness of the simulation\nThe group were asked how effective they had found the pump simulation, and how\neffective they felt the general approach would be as a laboratory substitute. This was one\narea in which there was a significant different between the two classes who formed the\npilot group, though the small sample sizes make interpretation of the difference\nspeculative. The mechanical engineers did not in general feel that the simulation was an\neffective alternative. As one put it: 'It [CBS] wouldn't give you practice of hardware and\nsetting things up properly, running it and taking good readings.' Another said: 'It rather\ndefeats the purpose of doing lab work.' The cross-disciplinary technology and business\nclass were more appreciative. One called it the 'perfect alternative', and another described\nit as 'semi hands-on'. The difference may reflect the engineer's identification of working\nwith plant as 'what an engineer does'.\nInteraction\nThe appropriateness of the degree of interaction to students' needs is related to routing. A\nlow score indicated that students would have liked more interaction, and as the overall\nmean was 2.67, this appears to have been the case. Most students rated the factor between\n'much too little' and the mid-point. Their comments suggest that they were referring\nmainly to the theory section where a few students said they would have welcomed more\ndepth being available. One more ambitious individual said: 'I would have liked to have\n46\nAu-j Volume 4 Number 3\nPercent\nmuch too little 2.00 3.00\nExtent of interaction\nFigure 4: Student perceptions of the adequacy of the interaction provided\n4.00\ntried other pump speeds so I could have done an NDG (Non Dimension Group)\ncalculation'. The conclusion is that more and better detail and interaction would be\ndesirable.\nMotivation and support\nGuidance on use and report writing, and feedback on performance\nAs these topics are obviously related they will be considered together: they gauge the level\nof support the students felt they had received. The simulation was run in somewhat\ncontrived circumstances, but every effort was made to confine support to dealing with\noperational problems. It must also be recognized that the so-called Hawthorne Effect\nmay have been invoked: the students were aware that they were the pilot group for a\nnovel methodology.\nA rather important factor if the simulation were to be used independently is guidance on\nthe use of the package. Again, a question on this was included in the general section of the\nquestionnaire, and in the section specific to the simulator. Responding to the general\nquestion, many of the simulator group felt better guidance could have been beneficial,\nalthough the overall mean ranking of 3.18 was moderately favourable. (The laboratory\ngroup felt that guidance was very good, with a mean of 3.63.)\nThere were teething problems with getting into the package which will be readily resolved,\nbut of more concern were expressed difficulties in running the simulation: 'I went off in\nthe wrong direction at the start, and didn't have time to complete it. If it was a bit clearer\nas to what to expect and what to do . . . '\nIn response to the question in the simulator section, rather strangely the rating for the\ncombined group dropped from 3.18 to 2.76 (significant at the 5% level). Admittedly this\nwas due to many of the students rating guidance as 'too little' but none as 'much too\n47\nNome S. Edward Computer-based laboratory simulation: evaluations of student perceptions\nlittle'. The most frequently cited reason given was shortage of on-line help, particularly\nduring the operational simulation.\n\u2022\nThe responses on guidance on report writing are rather disappointing, since the\nworkbook activities were intended to lead the user through the report. Laboratory groups\nfrom both classes actually rated the guidance on report writing higher than did their\ncounterparts on the CBS. One CBS user said: 'I knew what the procedures were on the\nday, but wasn't sure how to apply them or what they were for'. The overall means were\n3.32 for the laboratory groups, and 2.95 for the CBS groups. (What the implications are is\nunclear, but the topic is discussed below.)\nThis contrasted with the responses on feedback on performance where the respondents\nreported better feedback from the simulation than the laboratory. The mechanical\nengineers were only slightly higher at a mean of 3.00 as against 2.92, but 89.7% of the\ncombined CBS group rated feedback at or above the mid-point. The non-engineering\nlaboratory group gave a significantly lower rating in the experiences, with a mean of 1.86.\nOnly one student in this group even rated feedback at the mid-point. The technology and\nbusiness simulation group mean at 2.86 was significantly higher than the laboratory\ngroup, but still more than a quarter of the students rated this factor as 'poor' or 'very\npoor'. It seems that more can be done to enhance feedback, a point addressed below.\nMotivation\nAsked how motivational the experience was, the two classes responded rather differently\n(see Figure 5). Overall, the mean for the laboratory group was similar to that for the CBS\ngroup (3.21 as against 3.29). In the case of the mechanical engineers, there was little\ndifference between the two mean ratings, with the laboratory rating being slightly the\nhigher. The non-engineers, however, produced a mean of only 2.71 for the laboratory,\ncompared with a mean of 3.25 for the simulation.\nCount\n.5\nExperimental method\n\u2022 laboratory\n\u2022 CBS\n2.00 3.00 4.00\nFigure 5: Student perceptions of motivation of method\n48\nAdJ-j Volume 4 Number 3\nTo an extent at least, this may again reflect the engineers' self-identification. They were\nmuch more adamant that the missing element in the CBS was hands-on, which they felt\nwas both necessary and enjoyable. Overall, however, both the ratings and the students'\nwritten and oral comments make it clear that the non-engineers did not find the\nlaboratory experience motivational. This calls into question the nature of this type of\nactivity for this less technical class. Are the right objectives being pursued? Is the right\napproach being adopted? Is enough time being given? Further, the question of how\nmotivational students had found the experience begs the question of what motivated\nthem. Were they motivated by the experience per se, by the opportunity to gain\nknowledge, or by the need to satisfy an assessment criterion? This factor will warrant\nfuller investigation, but might be related to the students' motivation for taking the course.\nIndependence\nA feature of laboratory work is that there is always a lecturer supervising who will offer\nadvice or answer queries. The simulation is intended to be used without this support.\nStudents were asked whether they preferred to have a lecturer present or to work\nindependently. The results were rather surprising. Those who had done the actual\nlaboratory declared a significantly higher preference for having a lecturer present (mean\n2.5 as against 3.28). Analysis of the figures produced more surprises. Both groups of non-\nengineers had a mean of 3.0, the mid-point of the scale. The significant difference was due\nto a very marked difference between the two groups of engineers. The laboratory group\nmuch preferred the support of a lecturer (mean 2.00), whereas the CBS group declared a\npreference for independent working (mean 3.45). Interviews did little to explain this\ndifference. Students generally said that the information they were given about the\nlaboratory was sufficient, and that they rarely needed to consult the lecturer: 'We usually\nknow what we have to do. We just get on with it and take our readings. The lecturer helps\nif we ask, but mostly we don't need to.' The CBS group, on the other hand, made little\ncomment, though one student did say that he felt he had to find out how the simulation\nwas operated by trial and error. Probably, although asked about their general preference,\nstudents responded in the context of the activity they had experienced.\nHelp\nOne question not specifically asked was the students' opinion of the adequacy of on-line\nhelp. In retrospect, this important issue should have been included: it was the most\nquoted topic of concern to the users. Help provision is an important issue which is seldom\ngiven the attention it deserves; both the strategy and the presentation of assistance may\ngreatly affect the effectiveness of a package. Students generally felt that insufficient help\nwas available through the computer (although they agreed that most of it was available in\nthe accompanying workbook). One student said: \"The instructions weren't clear. You sit\ndown and you do something. You think: what am I to do now? Nothing tells me. It's trial\nand error, like varying the speeds and things. See what happens.' Student comments have\nalerted me to the crucial role which the help provision plays in the success of a package. I\nplan to enhance the provision in the present packages, and to devote significantly more\nattention to it in future designs.\n49\nNome S. Edward Computer-based laboratory simulation: evaluations of student perceptions\nCorrelations\nFew significant correlations were found which related to the factors considered in this\npaper. With regard to factors related to support, the feedback the students felt they had\nreceived proved to be significantly correlated (p > 0.05) with quite a number of other\nfactors. It was very strongly related to the motivational influence on the student\n(p. =.000), but negatively correlated with the time they spent on the package. This tends\nto suggest that feedback led to the desire to get more out of the experience, and hence to\nspending longer investigating the scope of the package. Respondents had been asked\nto rank their objectives in pursuing the degree course. The options offered were broadly in\ntwo groups: (i) extrinsic, e.g. pursuit of a qualification, improving employment prospects,\nand (ii) intrinsic, e.g. increasing knowledge, challenge. Feedback also tended to be rated\nhigher by those who had ranked the pursuit of a qualification lowest, although whether\nthis suggests that the extrinsically motivated students merely wanted results with the\nminimum of effort is debatable; it could equally be argued, given an opposite correlation,\nthat those seeking knowledge had proved to have been less easily satisfied with the\nfeedback given. Guidance on use, and guidance on the report and ease of use, were\npositively correlated. Guidance on use was negatively correlated with the desire to gain\nknowledge as a motivational factor. Taken together with the fact that the easier students\nfound the package to use, the more time they spent on it, it could be suggested that those\nseeking knowledge benefited from feeling better guided and finding the package easier to\nuse in seeking deeper understanding. A more jaundiced connotation could be that, since it\nis not correlated with their knowledge gain, either their expectations were higher, or they\nfound their search fruitless.\nDisappointingly, few correlations were detected in any of the measures on the simulation\nitself. The effectiveness of the simulation correlated positively with the usefulness of the\nworkbook, which suggests that those who found the workbook most informative may\nthen have been better prepared for the simulation. Students were asked a general question\non the suitability of CBS as a laboratory alternative. This factor, as might be expected,\ncorrelated positively with their perceptions of the effectiveness of the pump simulation.\nMuch less to be expected was the negative correlation between perceptions of the general\nsuitability of simulation and of the effectiveness of guidance on this simulation: a positive\ncorrelation would have evoked no surprise. The only other correlation of note was\nbetween the length of the simulation and the ranking of seeking a qualification as the\nstudents' reason for study, which again suggests that the more extrinsically motivated\nwere looking for the shortest route to satisfying the assessment criteria.\nDiscussion\nA number of students commented on the value they placed on the summary theory\nsection. They felt that the SAQs helped to reassure them that they understood the topic.\nThis section, they felt, could usefully be extended. This corresponds with the findings of\nColeman et al (1994) when evaluating an application of CAL with Electronics students.\nMotivation is promoted by keeping learners informed of their progress (see, for example,\nRitchie and Garner, 1995).\nMotivation has also been related to prior preparation. Colgan et al (1994) found that only\n50\nAa-j Volume 4 Number 3\n31% of respondents wanted to use CBT again, and inferred that preparation must be\nenhanced. My finding, that those who made most effective use of the workbook which\ncontained the preparation also performed best in simulation, perhaps supports this.\nMotivation is likely to be a crucial factor in an open-learning context. K. Bateson and W.\nSimpson, of the University of Surrey, reported in 1995 (in a paper given at the CAL95\nConference Learning to Succeed at the University of Cambridge) that allowing\nElectronics students unrestricted freedom to explore systems behaviour was intensely\nmotivating. Catteral and Ibbotson (1994), however, found that students' motivation\ndeclined over time, and concluded that CAL is useful for preparatory work but too time-\nconsuming to remain motivating. I feel that a well-presented package without\nunnecessary time-consuming effects, and with what Phillips and Moss (1993) have\ndescribed as 'an attractive interactive manner', can be stimulating and can remain\nmotivating. I used a multiple-media approach, and found that motivation was related to\nfeedback and the effectiveness of the workbook. As I mentioned above, on-line help was\npinpointed by a number of students as an important motivational factor. Whiting (1989)\nfound that an easily used screen presentation reduced the need for what in his application\nwas largely off-line help. He suggests optional presentations to cater for different learning\napproaches.\nA thought-provoking paper by Pengelly (1993) on motivation and on-line guidance is\nconcerned with developing domain reasoning, monitoring, and reflection levels of\nlearning. He suggests a broad model for developing support by monitoring the search\npaths of the users, and by analysis inferring their reasoning processes. He points out that\na human teacher becomes sensitive to a student's moods, motivation, compliance, etc.,\nand modifies his or her approach accordingly. In some way, the computer must emulate\nthis if effective prospective help is to be provided.\nScreen presentation is an important issue. A declared objective of laboratories is to\nprovide an appreciation of the plant, and presentation obviously has a bearing on this.\nThe attractiveness of the display, ease of interpretation and use of effects are all likely to\ninfluence a user's approach to and evaluation of the product. Respondents would have\nwelcomed more realism: 'The gauges and that sort of thing. Sound and sight has a lot to\ndo with things. If you're looking at things, it does make a difference from seeing it on\nscreen.' In general, however, the students found the display clear and attractive, and were\nat ease using it, something which they welcomed.\nFew if any researchers have published systematic evaluations of the importance of realism\nin simulation. My own work (Edward, 1996) revealed that cognitive learning was\nperceived to be lower and practical appreciation was poor in a diagrammatic presentation\ncompared to a realistic one. Waddick (1994) found that students could as effectively learn\nspectrophotometry on a realistic simulation as on real equipment. Leary (1995) exploited\nthe potential to the full by allowing the internal operation of plant to be viewed by the\nuser. Although there are some benefits in using, as in the pump, a restricted diagrammatic\npresentation, I must concede that for engineers no simulation can take the place of\nlaboratories. That said, if a simulation is required for whatever use, I now believe that it is\nworth making it as realistic as possible.\nAdumbra (1994) also advocates interaction which he avers keeps the learner 'awake'. Our\nSi\nNome S. Edward Computer-based laboratory simulation: evaluations of student perceptions\nsimulation is, of course, very interactive, all controls being operated by the student.\nStudents also appreciated the interaction and feedback in the theory section, and wished\nthis to be expanded. This corresponds with the findings of Catterall and Ibbotson (1994),\nalthough the need diminished as their students gained confidence (they wanted more\ncontrol over the interaction). Catterall and Ibbotson reported at ALT-C '94 in Hull that\ntheir students sought control over routing and number of attempts at questions. This is an\naspect which I propose to address in future simulations. Whiting (1989) found that where\nease of use and interaction were highly rated, the learners had less need of external\nsupport. Whether the workbook can eventually be absorbed as an interactive section of\nthe package is debatable, but it is a useful long-term objective.\nConclusions\nI am reluctant to use the word 'successful' about the results because at best the simulation\nwas a qualified success. I feel that a sound start has been made, but that my evaluations\nhave revealed scope for improvement. My objective was to produce an alternative for\nstudents who miss laboratories. I do not feel it provides an equivalent experience.\nAlthough the simulation was clear and easy to use, practical appreciation was diminished.\nDespite completing the CBS more quickly, students still wished it to be shorter This\nsuggests that, in contrast to laboratories which engineers enjoy, it was viewed as a means\nof satisfying an assessment criterion. My aim now is to enhance the package, and produce\nwhat Solomon (1993) describes as a simulation which 'take[s] the best of the experiential\nand combinefs] it with more traditional learning methods [. . .] [to] enhance but not\nreplace traditional learning techniques'.\nReferences\nAdumbra, C. (1994), 'Enhancing student experience of computer-aided learning\npackages', Proceedings of the Conference on Computer Aided Learning in Engineering,\nSheffield, September 1994, Sheffield: University of Sheffield.\nBoud, D., Dunn, J. and Hegarty-Hazel, E. (1984), Teaching in Laboratories, Guildford:\nSRHE & NFER Nelson.\nCarter, G., Amour, D. G., Lee, L. S. and Sharpies, R. (1980), 'Assessment of\nundergraduate electrical engineering laboratory studies', IEE Proceedings, A.460.\nCatterall, M. and Ibbotson, P. (1994), 'The development of a low technology marketing\nCBT', Account, (6) 1.\nColeman, J. N., Kinniment, D., Burns, F., and Butler, T. (1994), 'Teaching in a third of\nthe time: a successful application of computer-aided learning in degree-level electronics',\nProceedings of the Conference on Computer Aided Learning in Engineering, Sheffield,\nSeptember 1994, Sheffield: University of Sheffield.\nColgan, N., McClean, S. and Scotney, B. (1994), 'Computer-based teaching and\nevaluation of introductory statistics for health science students: some lessons learned',\nAssociation for Learning Technology Journal, 2 (2), 68-74.\n52\nALT-J Volume 4 Number 3\nEdward, N. 'Evaluation of computer-based laboratory simulation, Computers and\nEducation (in press).\nEdward, N. (1996), 'Screen presentations in laboratory simulations as perceived by\nstudents', Proceedings of the SERA (1995) Conference, Glasgow: University of\nStrathclyde.\nFlagg, B. (1990), Formative Evaluation for Educational Technologies, Hillsdale NJ:\nLawrence Erlbaum.\nLeary, J. (1995) 'Computer-simulated experiments and computer games: a method of\ndesign analysis', Association for Learning Technology Journal, 3 (1), 57-61.\nPengelly, M. (1993), 'Computer-based assessment to support the learner not the assessor',\nTeaching and Learning Technology Programme Workshop on Assessment of Learning in\nHigher Education, Workshop Papers, Sheffield, November 1993 (ISBN 1-85889-086-1).\nPhillips, T. and Moss, G. (1993), 'Can CAL biology packages be used to replace the\nteacher?', Journal of Biological Education, 27 (3), 213\u201416.\nReeves, T. (1992), 'Evaluating interactive multimedia', Educational Technology, May,\n47-52.\nRitchie, G. and Garner, P. (1995), 'Computer-based laboratory tutorials', IEE\nColloquium: Computer-Based Learning in Electronic Education, Digest No. 1995\/098,\np. 13\/1-2, London: IEE.\nSolomon, C. (1993), 'Simulation training builds teams through experience', Personnel\nJournal, June.\nWaddick, J. (1994), 'Case study: the use of a HyperCard simulation to aid in the teaching\nof laboratory apparatus operation', ETTI, 31 (4), 295-301.\nWhiting, J. (1989), 'An evaluation of some common CAL and CBT authoring styles',\nETTI, 26 (3), 186-200.\nS3\n"}